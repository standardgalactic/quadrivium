Lu Fang
Jian Pei
Guangtao Zhai
Ruiping Wang (Eds.)
 123
LNAI 14473
Third CAAI International Conference, CICAI 2023 
Fuzhou, China, July 22–23, 2023 
Revised Selected Papers, Part I
Artificial Intelligence

Lecture Notes in Computer Science
Lecture Notes in Artiﬁcial Intelligence
14473
Founding Editor
Jörg Siekmann
Series Editors
Randy Goebel, University of Alberta, Edmonton, Canada
Wolfgang Wahlster, DFKI, Berlin, Germany
Zhi-Hua Zhou, Nanjing University, Nanjing, China

The series Lecture Notes in Artiﬁcial Intelligence (LNAI) was established in 1988 as a
topical subseries of LNCS devoted to artiﬁcial intelligence.
Theseries publishes state-of-the-art researchresults at ahighlevel. As withtheLNCS
mother series, the mission of the series is to serve the international R & D community
by providing an invaluable service, mainly focused on the publication of conference and
workshop proceedings and postproceedings.

Lu Fang · Jian Pei · Guangtao Zhai ·
Ruiping Wang
Editors
Artiﬁcial Intelligence
Third CAAI International Conference, CICAI 2023
Fuzhou, China, July 22–23, 2023
Revised Selected Papers, Part I

Editors
Lu Fang
Tsinghua University
Beijing, China
Guangtao Zhai
Shanghai Jiao Tong Univeristy
Shanghai, China
Jian Pei
Duke University
Durham, NC, USA
Ruiping Wang
Chinese Academy of Sciences
Beijing, China
ISSN 0302-9743
ISSN 1611-3349 (electronic)
Lecture Notes in Artiﬁcial Intelligence
ISBN 978-981-99-8849-5
ISBN 978-981-99-8850-1 (eBook)
https://doi.org/10.1007/978-981-99-8850-1
LNCS Sublibrary: SL7 – Artiﬁcial Intelligence
© The Editor(s) (if applicable) and The Author(s), under exclusive license
to Springer Nature Singapore Pte Ltd. 2024
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the
editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors
or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd.
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, Singapore
Paper in this product is recyclable.

Preface
The present book includes extended and revised versions of papers selected from the
second CAAI International Conference on Artiﬁcial Intelligence (CICAI 2023), held in
Fuzhou, China, during July 22–23, 2023.
CICAI is a summit forum in the ﬁeld of artiﬁcial intelligence and the 2023 forum was
hosted by Chinese Association for Artiﬁcial Intelligence (CAAI). CICAI aims to estab-
lish a global platform for international academic exchange, promote advanced research
in AI and its afﬁliated disciplines, and promote scientiﬁc exchanges among researchers,
practitioners, scientists, students, and engineers in AI and its afﬁliated disciplines in
order to provide interdisciplinary and regional opportunities for researchers around the
world, enhance the depth and breadth of academic and industrial exchanges, inspire new
ideas, cultivate new forces, implement new ideas, integrate into the new landscape, and
join the new era. The conference program included invited talks delivered by four dis-
tinguished speakers, Chenghu Zhou, Zhihua Zhou, Marios M. Polycarpou, and Xuesong
Liu, as well as 17 tutorials on 8 themes, followed by an oral session of 13 papers, a poster
session of 72 papers, and a demo exhibition of 16 papers. Those papers were selected
from 376 submissions using a double-blind review process, and on average each submis-
sion received 2.9 reviews. The topics covered by these selected high-quality papers span
the ﬁelds of AI-generated content, computer vision, machine learning, nature language
processing, application of AI, and data mining, amongst others.
These two volumes contain 100 papers selected and revised from the proceedings of
CICAI 2023. We would like to thank the authors for contributing their novel ideas and
visions that are recorded in this book.
The proceedings editors also wish to thank all reviewers for their contributions and
Springer for their trust and for publishing the proceedings of CICAI 2023.
October 2023
Lu Fang
Jian Pei
Guangtao Zhai
Ruiping Wang

Organization
General Chairs
Lu Fang
Tsinghua University, China
Jian Pei
Duke University, USA
Guangtao Zhai
Shanghai Jiao Tong University, China
Program Chair
Ruiping Wang
Chinese Academy of Sciences, China
Publication Chairs
Xiaohui Chen
University of Illinois at Urbana-Champaign, USA
Mengqi Ji
Beihang University, China
Presentation Chairs
Xun Chen
University of Science and Technology of China,
China
Jiantao Zhou
University of Macau, China
Demo Chairs
Jiangtao Gong
Tsinghua University, China
Can Liu
City University of Hong Kong, China
Tutorial Chairs
Jie Song
ETH Zurich, Switzerland
Tao Yu
Tsinghua University, China

viii
Organization
Grand Challenge Chairs
David Brady
University of Arizona, USA
Haozhe Lin
Tsinghua University, China
Advisory Committee
C. L. Philip Chen
University of Macau, China
Xilin Chen
Institute of Computing Technology, Chinese
Academy of Sciences, China
Yike Guo
Imperial College London, China
Ping Ji
City University of New York, USA
Licheng Jiao
Xidian University, China
Ming Li
University of Waterloo, Canada
Chenglin Liu
Institute of Automation, Chinese Academy of
Sciences, China
Derong Liu
University of Illinois at Chicago, USA
Hong Liu
Peking University, China
Hengtao Shen
University of Electronic Science and Technology
of China, China
Yuanchun Shi
Tsinghua University, China
Yongduan Song
Chongqing University, China
Fuchun Sun
Tsinghua University, China
Jianhua Tao
Institute of Automation, Chinese Academy of
Sciences, China
Guoyin Wang
Chongqing University of Posts and
Telecommunications, China
Weining Wang
Beijing University of Posts and
Telecommunications, China
Xiaokang Yang
Shanghai Jiao Tong University, China
Changshui Zhang
Tsinghua University, China
Lihua Zhang
Fudan University, China
Song-Chun Zhu
Peking University, China
Wenwu Zhu
Tsinghua University, China
Yueting Zhuang
Zhejiang University, China
Program Committee
Boxin Shi
Peking University, China
Can Liu
City University of Hong Kong, China

Organization
ix
Feng Xu
Tsinghua University, China
Fu Zhang
University of Hong Kong, China
Fuzhen Zhuang
Beihang University, China
Guangtao Zhai
Shanghai Jiao Tong University, China
Hao Zhao
Tsinghua University, China
Haozhe Lin
Tsinghua University, China
Hongnan Lin
Chinese Academy of Sciences, China
Jian Zhao
Institute of North Electronic Equipment, China
Jian Zhang
Peking University, China
Jiangtao Gong
Tsinghua University, China
Jiannan Li
Singapore Management University, Singapore
Jiantao Zhou
University of Macau, China
Jie Song
ETH Zurich, Switzerland
Jie Wang
University of Science and Technology of China,
China
Jinshan Pan
Nanjing University of Science and Technology,
China
Junchi Yan
Shanghai Jiao Tong University, China
Le Wu
Hefei University of Technology, China
Le Wang
Xi’an Jiaotong University, China
Lei Zhang
Chongqing University, China
Liang Li
Institute of Computing Technology, Chinese
Academy of Sciences, China
Lijun Zhang
Nanjing University, China
Lu Fang
Tsinghua University, China
Meng Yang
Sun Yat-sen University, China
Meng Wang
BIGAI, China
Mengqi Ji
Beihang University, China
Nan Gao
Tsinghua University, China
Peng Cui
Tsinghua University, China
Qi Liu
University of Science and Technology of China,
China
Qi Dai
Microsoft Research, China
Qi Ye
Zhejiang University, China
Qing Ling
Sun Yat-sen University, China
Risheng Liu
Dalian University of Technology, China
Ruiping Wang
Institute of Computing Technology, Chinese
Academy of Sciences, China
Shuhui Wang
VIPL, ICT, Chinese Academic of Sciences, China
Si Liu
Beihang University, China
Tao Yu
Tsinghua University, China
Wu Liu
AI Research of JD.com, China

x
Organization
Xiaoyan Luo
Beihang University, China
Xiaoyun Yuan
Tsinghua University, China
Xiongkuo Min
Shanghai Jiao Tong University, China
Xun Chen
University of Science and Technology of China,
China
Ying Fu
Beijing Institute of Technology, China
Yubo Chen
Institute of Automation, Chinese Academy of
Sciences, China
Yuchen Guo
Tsinghua University, China
Yue Gao
Tsinghua University, China
Yue Deng
Beihang University, China
Yue Li
Xi’an Jiaotong-Liverpool University, China
Yuwang Wang
Tsinghua University, China

Contents – Part I
Computer Vision
MARS: An Instance-Aware, Modular and Realistic Simulator
for Autonomous Driving
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen,
Hongmin Xiao, Chao Hou, Haozhe Lou, Yuantao Chen, Runyi Yang,
Yuxin Huang, Xiaoyu Ye, Zike Yan, Yongliang Shi, Yiyi Liao,
and Hao Zhao
Concealed Object Segmentation with Hierarchical Coherence Modeling . . . . . . .
16
Fengyang Xiao, Pan Zhang, Chunming He, Runze Hu, and Yutao Liu
ViT-MPI: Vision Transformer Multiplane Images for Surgical Single-View
View Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
Chenming Han, Ruizhi Shao, Gaochang Wu, Hang Shao, and Yebin Liu
Dual-Domain Network for Restoring Images from Under-Display Cameras . . . .
41
Di Wang, Zhuoran Zheng, and Xiuyi Jia
Sliding Window Detection and Distance-Based Matching for Tracking
on Gigapixel Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
Yichen Li, Qiankun Liu, Xiaoyong Wang, and Ying Fu
Robust Self-contact Detection Based on Keypoint Condition
and ControlNet-Based Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
He Zhang, Jianhui Zhao, Fan Li, Chao Tan, and Shuangpeng Sun
Explicit Composition of Neural Radiance Fields by Learning an Occlusion
Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
Xunsen Sun, Hao Zhu, Yuanxun Lu, and Xun Cao
LEAD: LiDAR Extender for Autonomous Driving . . . . . . . . . . . . . . . . . . . . . . . . .
91
Jianing Zhang, Wei Li, Ruigang Yang, and Qionghai Dai
Fast Hierarchical Depth Super-Resolution via Guided Attention . . . . . . . . . . . . . .
104
Yusen Hou, Changyi Chen, Gaosheng Liu, Huanjing Yue, Kun Li,
and Jingyu Yang

xii
Contents – Part I
A Hybrid Approach for Segmenting Non-ideal Iris Images Using CGAN
and Geometry Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
Shanila Azhar, Ke Zhang, Xiaomin Guo, Shizong Yan, Guohua Liu,
and Shan Chan
3D-B2U: Self-supervised Fluorescent Image Sequences Denoising . . . . . . . . . . .
130
Jianan Wang, Hesong Li, Xiaoyong Wang, and Ying Fu
Equivariant Indoor Illumination Map Estimation from a Single Image . . . . . . . . .
143
Yusen Ai, Xiaoxue Chen, Xin Wu, and Hao Zhao
Weakly-Supervised Grounding for VQA with Dual Visual-Linguistic
Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
Yi Liu, Junwen Pan, Qilong Wang, Guanlin Chen, Weiguo Nie,
Yudong Zhang, Qian Gao, Qinghua Hu, and Pengfei Zhu
STU3: Multi-organ CT Medical Image Segmentation Model Based
on Transformer and UNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
170
Wenjin Zheng, Bo Li, and Wanyi Chen
Integrating Human Parsing and Pose Network for Human Action
Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
Runwei Ding, Yuhang Wen, Jinfu Liu, Nan Dai, Fanyang Meng,
and Mengyuan Liu
Lightweight Rolling Shutter Image Restoration Network Based
on Undistorted Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
Binfeng Wang, Yunhao Zou, Zhijie Gao, and Ying Fu
An Efﬁcient Graph Transformer Network for Video-Based Human Mesh
Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
Tao Tang, Yingxuan You, Ti Wang, and Hong Liu
Multi-scale Transformer with Decoder for Image Quality Assessment . . . . . . . . .
220
Shuai Zhang and Yutao Liu
Low-Light Image Enhancement via Unsupervised Learning . . . . . . . . . . . . . . . . .
232
Wenchao He and Yutao Liu
GLCANet: Context Attention for Infrared Small Target Detection . . . . . . . . . . . .
244
Rui Liu, Qiankun Liu, Xiaoyong Wang, and Ying Fu
Fast Point Cloud Registration for Urban Scenes via Pillar-Point
Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
256
Siyuan Gu and Ruqi Huang

Contents – Part I
xiii
PMPI: Patch-Based Multiplane Images for Real-Time Rendering of Neural
Radiance Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
269
Xiaoguang Jiang, You Yang, Qiong Liu, Changbiao Tao, and Qun Liu
EFPNet: Effective Fusion Pyramid Network for Tiny Person Detection
in UAV Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
Ruichen Zhang, Qiong Liu, and Kejun Wu
End-to-End Object-Level Contrastive Pretraining for Detection
via Semantic-Aware Localization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
293
Long Geng and Xiaoming Huang
PointerNet with Local and Global Contexts for Natural Language Moment
Localization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
305
Linwei Ye, Zhi Liu, and Yang Wang
Self-supervised Meta Auxiliary Learning for Actor and Action Video
Segmentation from Natural Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
317
Linwei Ye and Zhenhua Wang
RsMmFormer: Multimodal Transformer Using Multiscale Self-attention
for Remote Sensing Image Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
Bo Zhang, Zuheng Ming, Yaqian Liu, Wei Feng, Liang He,
and Kaixing Zhao
Fashion Label Relation Networks for Attribute Recognition . . . . . . . . . . . . . . . . .
340
Tongyang Wang, Yan Huang, and Jianjun Qian
A Modiﬁed Fuzzy Markov Random Field Incorporating Multiple Features
for Liver Tumor Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
352
Laquan Li and Yan Jiang
Weakly Supervised Optical Remote Sensing Salient Object Detection
Based on Adaptive Discriminative Region Suppression . . . . . . . . . . . . . . . . . . . . .
364
Xingyu Li, Jieyu Wu, Yuan Zhou, Jingwei Yuan, and Yanwen Chen
SPCTNet: A Series-Parallel CNN and Transformer Network for 3D
Medical Image Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
376
Bin Yu, Quan Zhou, and Xuming Zhang
LANet: A Single Stage Lane Detector with Lightweight Attention . . . . . . . . . . . .
388
Qiangbin Xie, Xiao Zhao, and Lihua Zhang

xiv
Contents – Part I
Visible and NIR Image Fusion Algorithm Based on Information
Complementarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
400
Zhuo Li and Bo Li
Data Mining
End-to-End Optimization of Quantization-Based Structure Learning
and Interventional Next-Item Recommendation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
415
Kairui Fu, Qiaowei Miao, Shengyu Zhang, Kun Kuang, and Fei Wu
Multi-trends Enhanced Dynamic Micro-video Recommendation . . . . . . . . . . . . .
430
Yujie Lu, Yingxuan Huang, Shengyu Zhang, Wei Han, Hui Chen,
Wenyan Fan, Jiangliang Lai, Zhou Zhao, and Fei Wu
Parameters Efﬁcient Fine-Tuning for Long-Tailed Sequential
Recommendation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
442
Zheqi Lv, Feng Wang, Shengyu Zhang, Wenqiao Zhang, Kun Kuang,
and Fei Wu
Heterogeneous Link Prediction via Mutual Information Maximization
Between Node Pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
460
Yifan Lu, Zehao Liu, Mengzhou Gao, and Pengfei Jiao
Explainability, Understandability, and Veriﬁability of AI
ADAPT: Action-Aware Driving Caption Transformer . . . . . . . . . . . . . . . . . . . . . . .
473
Bu Jin and Haotian Liu
Structural Recognition of Handwritten Chinese Characters Using
a Modiﬁed Part Capsule Auto-encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
478
Xin-Jian Wu, Xiang Ao, Rui-Song Zhang, and Cheng-Lin Liu
Natural Language Processing
Sequential Style Consistency Learning for Domain-Generalizable Text
Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
493
Pengcheng Zhang, Wenrui Liu, Ning Wang, Ran Shen, Gang Sun,
Xinghua Jiang, Zheqian Chen, Fei Wu, and Zhou Zhao
MusicGAIL: A Generative Adversarial Imitation Learning Approach
for Music Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
505
Yusong Liao, Hongguang Xu, and Ke Xu

Contents – Part I
xv
Unsupervised Traditional Chinese Herb Mention Normalization
via Robustness-Promotion Oriented Self-supervised Training . . . . . . . . . . . . . . . .
517
Wei Li, Zheng Yang, and Yanqiu Shao
Feature Fusion Gate: Improving Transformer Classiﬁer Performance
with Controlled Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
529
Yu Xiang and Lei Bai
Multi-round Dialogue State Tracking by Object-Entity Alignment
in Visual Dialog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
541
Wei Pang
Multi-modal Dialogue State Tracking for Playing GuessWhich Game . . . . . . . . .
554
Wei Pang, Ruixue Duan, Jinfu Yang, and Ning Li
Diagnosis Then Aggregation: An Adaptive Ensemble Strategy
for Keyphrase Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
566
Xin Jin, Qi Liu, Linan Yue, Ye Liu, Lili Zhao, Weibo Gao, Zheng Gong,
Kai Zhang, and Haoyang Bi
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
579

Contents – Part II
AI Ethics, Privacy, Fairness and Security
FairDR: Ensuring Fairness in Mixed Data of Fairly and Unfairly Treated
Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Yuxuan Liu, Kun Kuang, Fengda Zhang, and Fei Wu
Blind Adversarial Training: Towards Comprehensively Robust Models
Against Blind Adversarial Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
Haidong Xie, Xueshuang Xiang, Bin Dong, and Naijin Liu
AI Generated Content
TOAC: Try-On Aligning Conformer for Image-Based Virtual Try-On
Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
Yifei Wang, Wang Xiang, Shengjie Zhang, Dizhan Xue,
and Shengsheng Qian
GIST: Transforming Overwhelming Information into Structured
Knowledge with Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
Meng Wu, Xinyu Zhou, Gang Ma, Zhangwei Lu, Liuxin Zhang,
and Yu Zhang
AIGCIQA2023: A Large-Scale Image Quality Assessment Database
for AI Generated Images: From the Perspectives of Quality, Authenticity
and Correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
Jiarui Wang, Huiyu Duan, Jing Liu, Shi Chen, Xiongkuo Min,
and Guangtao Zhai
Applications of Artiﬁcial Intelligence
Space Brain: An AI Autonomous Spatial Decision System . . . . . . . . . . . . . . . . . .
61
Jiachen Du, Boyang Jia, and Xinyi Fu
TST: Time-Sparse Transducer for Automatic Speech Recognition . . . . . . . . . . . .
68
Xiaohui Zhang, Mangui Liang, Zhengkun Tian, Jiangyan Yi,
and Jianhua Tao

xviii
Contents – Part II
Enhancing Daily Life Through an Interactive Desktop Robotics System . . . . . . .
81
Yuhang Zheng, Qiyao Wang, Chengliang Zhong, He Liang,
Zhengxiao Han, and Yupeng Zheng
Model Distillation for Lane Detection on Car-Level Chips . . . . . . . . . . . . . . . . . . .
87
Zixiong Wei, Zerun Wang, Hui Chen, Tianyu Shao, Lihong Huang,
Xiaoyun Kang, and Xiang Tian
A Weakly Supervised Learning Method for Recognizing Childhood Tic
Disorders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
Ruizhe Zhang, Xiaojing Xu, Zihao Bo, Junfeng Lyu, Yuchen Guo,
and Feng Xu
Detecting Software Vulnerabilities Based on Hierarchical Graph Attention
Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
Wenlin Xu, Tong Li, Jinsong Wang, Tao Fu, and Yahui Tang
Domain Speciﬁc Pre-training Methods for Traditional Chinese Medicine
Prescription Recommendation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
Wei Li, Zheng Yang, and Yanqiu Shao
Ensemble Learning with Time Accumulative Effect for Early Diagnosis
of Alzheimer’s Disease . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
Zhou Zhou, Hong Yu, and Guoyin Wang
LTUNet: A Lightweight Transformer-Based UNet with Multi-scale
Mechanism for Skin Lesion Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
Huike Guo, Han Zhang, Minghe Li, and Xiongwen Quan
A Novel Online Multi-label Feature Selection Approach
for Multi-dimensional Streaming Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
Zhanyun Zhang, Chuan Luo, Tianrui Li, Hongmei Chen, and Dun Liu
M2Sim: A Long-Term Interactive Driving Simulator . . . . . . . . . . . . . . . . . . . . . . . .
172
Zhengxiao Han, Zhijie Yan, Yang Li, Pengfei Li, Yifeng Shi, Nairui Luo,
Xu Gao, Yongliang Shi, Pengfei Huang, Jiangtao Gong, Guyue Zhou,
Yilun Chen, Hang Zhao, and Hao Zhao
Long-Term Interactive Driving Simulation: MPC to the Rescue . . . . . . . . . . . . . .
177
Zhengxiao Han, Zhijie Yan, Yang Li, Pengfei Li, Yifeng Shi, Nairui Luo,
Xu Gao, Yongliang Shi, Pengfei Huang, Jiangtao Gong, Guyue Zhou,
Yilun Chen, Hang Zhao, and Hao Zhao

Contents – Part II
xix
Text-Oriented Modality Reinforcement Network for Multimodal
Sentiment Analysis from Unaligned Multimodal Sequences . . . . . . . . . . . . . . . . .
189
Yuxuan Lei, Dingkang Yang, Mingcheng Li, Shunli Wang, Jiawei Chen,
and Lihua Zhang
PSDD-Net: A Dual-Domain Framework for Pancreatic Cancer Image
Segmentation with Multi-scale Local-Dense Net . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
Dongying Yang, Cong Xia, Ge Tian, Daoqiang Zhang, and Rongjun Ge
Airport Boarding Bridge Pedestrian Detection Based on Spatial Attention
and Joint Crowd Density Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
Xu Han, Hao Wan, Wenxiao Tang, and Wenxiong Kang
A Novel Neighborhood-Augmented Graph Attention Network
for Sequential Recommendation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
Shuxiang Xu, Qibu Xiang, Yushun Fan, Ruyu Yan, and Jia Zhang
Reinforcement Learning-Based Algorithm for Real-Time Automated
Parking Decision Making . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
242
Xiaoyi Wei, Taixian Hou, Xiao Zhao, Jiaxin Tu, Haiyang Guan,
Peng Zhai, and Lihua Zhang
FAI: A Fraudulent Account Identiﬁcation System . . . . . . . . . . . . . . . . . . . . . . . . . .
253
Yixin Tian, Yufei Zhang, Fangshu Chen, Bingkun Wang, Jiahui Wang,
and Xiankai Meng
An Autonomous Recovery Guidance System for USV Based on Optimized
Genetic Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258
Lulu Zhou, Xiaoming Ye, Pengzhan Xie, and Xiang Liu
UAV Path Planning Based on Enhanced PSO-GA . . . . . . . . . . . . . . . . . . . . . . . . . .
271
Hongbo Xiang, Xiaobo Liu, Xinsheng Song, and Wen Zhou
Machine Learning
How to Select the Appropriate One from the Trained Models
for Model-Based OPE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
Chongchong Li, Yue Wang, Zhi-Ming Ma, and Yuting Liu
A Flexible Simplicity Enhancement Model for Knowledge Graph
Completion Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
Yashen Wang, Xuecheng Zhang, Tianzhu Chen, and Yi Zhang

xx
Contents – Part II
A Novel Convolutional Neural Network Architecture with a Continuous
Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
310
Yao Liu, Hang Shao, and Bing Bai
Dynamic Task Subspace Ensemble for Class-Incremental Learning . . . . . . . . . . .
322
Weile Zhang, Yuanjian He, and Yulai Cong
Energy-Based Policy Constraint for Ofﬂine Reinforcement Learning . . . . . . . . . .
335
Zhiyong Peng, Changlin Han, Yadong Liu, and Zongtan Zhou
Lithology Identiﬁcation Method Based on CNN-LSTM-Attention: A Case
Study of Huizhou Block in South China Sea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
347
Zhikun Liu, Xuedong Yan, Yanhong She, Fan Zhang, Chongdong Shi,
and Liupeng Wang
Deployment and Comparison of Large Language Models Based on Virtual
Cluster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
Kai Li, Rongqiang Cao, Meng Wan, Xiaoguang Wang, Zongguo Wang,
Jue Wang, and Yangang Wang
Social Network Community Detection Based on Textual Content
Similarity and Sentimental Tendency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
366
Jie Gao, Junping Du, Zhe Xue, and Zeli Guan
IvyGPT: InteractiVe Chinese Pathway Language Model in Medical Domain . . .
378
Rongsheng Wang, Yaofei Duan, ChanTong Lam,
Jiexin Chen, Jiangsheng Xu, Haoming Chen, Xiaohong Liu,
Patrick Cheong-Iao Pang, and Tao Tan
Network Pruning via Explicit Information Migration . . . . . . . . . . . . . . . . . . . . . . .
383
Jincheng Wu, Dongfang Hu, and Zhitong Zheng
Multidisciplinary Research with AI
Attention-Based RNA Secondary Structure Prediction . . . . . . . . . . . . . . . . . . . . . .
399
Liya Hu, Xinyi Yang, Yuxuan Si, Jingyuan Chen, Xinhai Ye,
Zhihua Wang, and Fei Wu
A Velocity Controller for Quadrotors Based on Reinforcement Learning . . . . . . .
411
Yu Hu, Jie Luo, Zhiyan Dong, and Lihua Zhang
DACTransNet:AHybridCNN-TransformerNetworkforHistopathological
Image Classiﬁcation of Pancreatic Cancer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
422
Yongqing Kou, Cong Xia, Yiping Jiao, Daoqiang Zhang, and Rongjun Ge

Contents – Part II
xxi
YueGraph: A Prototype for Yue Opera Lineage Review Based
on Knowledge Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
435
Songjin Yang, Fuxiang Fu, Chenxi Zhu, Hao Zeng, Youbing Zhao,
Hao Xie, Xuxue Sun, Xi Guo, Bin Han, Guofen Tao, and Shengyou Lin
An Integrated All-Optical Multimodal Learning Engine Built
by Reconﬁgurable Phase-Change Meta-Atoms . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
442
Yuhao Wang, Jingkai Song, Penghui Shen, Qisheng Yang, Yi Yang,
and Tian-ling Ren
Brainstem Functional Parcellation Based on Spatial Connectivity Features
Using Functional Magnetic Resonance Imaging . . . . . . . . . . . . . . . . . . . . . . . . . . . .
452
Meiyi Wang, Zuyang Liang, Cong Zhang, Yuhan Zheng, Chunqi Chang,
and Jiayue Cai
Other AI Related Topics
A Reinforcement Learning Approach for Personalized Diversity in Feeds
Recommendation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
463
Li He, Kangqi Luo, Zhuoye Ding, Hang Shao, and Bing Bai
Domain Incremental Learning for EEG-Based Seizure Prediction
. . . . . . . . . . . .
476
Zhiwei Deng, Tingting Mao, Chenghao Shao, Chang Li, and Xun Chen
Multitask Learning-Based Early MTT Partition Decision for Versatile
Video Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
488
Wu Liu, Yue Li, and Mingxing Nie
EEG Extended Source Imaging with Variation Sparsity and Lp-Norm
Constraint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
500
Shu Peng, Feifei Qi, Hong Yu, and Ke Liu
Perceptual Quality Assessment of Omnidirectional Audio-Visual Signals . . . . . .
512
Xilei Zhu, Huiyu Duan, Yuqin Cao, Yuxin Zhu, Yucheng Zhu, Jing Liu,
Li Chen, Xiongkuo Min, and Guangtao Zhai
Research on Tongue Muscle Strength Measurement and Recovery System . . . . .
526
Xiaotian Pan, Ling Kang, Qia Zhang, Hang Liu, Jin Ai, Jianxiong Zou,
Donghong Qiao, Menghan Hu, Yue Wu, and Jian Zhang
Updates and Experiences of VenusAI Platform . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
532
Meng Wan, Rongqiang Cao, Kai Li, Xiaoguang Wang, Zongguo Wang,
Jue Wang, and Yangang Wang

xxii
Contents – Part II
SyncRec: A Synchronized Online Learning Recommendation System . . . . . . . . .
539
Yixuan Zhang, Wenkang Zhang, Linqi Liu, Xuxue Sun, Hao Zeng,
Weijuan Zhao, Youbing Zhao, and Weifan Chen
Interpretability-Based Cross-Silo Federated Learning . . . . . . . . . . . . . . . . . . . . . . .
545
Wenjie Zhou, Zhaoyang Han, Chuan Ma, Zhe Liu, and Piji Li
Nature-Inspired and Nature-Relevant UI Models: A Demo from Exploring
Bioluminescence Crowdsourced Data with a Design-Driven Approach . . . . . . . .
557
Yancheng Cao, Keyi Gu, Ke Ma, and Francesca Valsecchi
Lightening and Growing Your Stromatolites: Draw and Dive into Nature . . . . . .
562
Yancheng Cao, Xinghui Chen, Siheng Feng, and Jing Liang
Robotics
A CNN-Based Real-Time Dense Stereo SLAM System on Embedded
FPGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
569
Qian Huang, Yu Zhang, Jianing Zheng, Gaoxing Shang, and Gang Chen
Land-Air Amphibious Robots: A Survey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
575
Bo Hu, Zhiyan Dong, and Lihua Zhang
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
587

Computer Vision

MARS: An Instance-Aware, Modular
and Realistic Simulator for Autonomous
Driving
Zirui Wu1,2, Tianyu Liu1,3, Liyi Luo1,4, Zhide Zhong1,5, Jianteng Chen1,5,
Hongmin Xiao1,6, Chao Hou1,7, Haozhe Lou1,8, Yuantao Chen1,9,
Runyi Yang1,10, Yuxin Huang1,5, Xiaoyu Ye1,5, Zike Yan1, Yongliang Shi1,
Yiyi Liao11, and Hao Zhao1(B)
1 AIR, Tsinghua University, Beijing, China
zhaohao@air.tsinghua.edu.cn
2 System Hub, HKUST(GZ), Guangzhou, China
3 HKUST, Hong Kong SAR, China
4 McGill University, Montreal, Canada
5 Beijing Institute of Technology, Beijing, China
6 National University of Singapore, Singapore, Singapore
7 HKU, Pokfulam, Hong Kong
8 University of Wisconsin Madison, Madison, USA
9 Xi’an University of Architecture and Technology, Xi’an, China
10 Imperial College London, London, UK
11 Zhejiang University, Hangzhou, China
Abstract. Nowadays, autonomous cars can drive smoothly in ordinary
cases, and it is widely recognized that realistic sensor simulation will play
a critical role in solving remaining corner cases by simulating them. To
this end, we propose an autonomous driving simulator based upon neural
radiance ﬁelds (NeRFs). Compared with existing works, ours has three
notable features: (1) Instance-aware. Our simulator models the fore-
ground instances and background environments separately with indepen-
dent networks so that the static (e.g., size and appearance) and dynamic
(e.g., trajectory) properties of instances can be controlled separately.
(2) Modular. Our simulator allows ﬂexible switching between diﬀer-
ent modern NeRF-related backbones, sampling strategies, input modal-
ities, etc. We expect this modular design to boost academic progress
and industrial deployment of NeRF-based autonomous driving simula-
tion. (3) Realistic. Our simulator set new state-of-the-art photo-realism
results given the best module selection. Our simulator will be open-
sourced while most of our counterparts are not. Project page: https://
open-air-sun.github.io/mars/.
H. Zhao—Sponsored by Tsinghua-Toyota Joint Research Fund (20223930097).
Supplementary Information The online version contains supplementary material
available at https://doi.org/10.1007/978-981-99-8850-1 1.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 3–15, 2024.
https://doi.org/10.1007/978-981-99-8850-1_1

4
Z. Wu et al.
Keywords: Autonomous Driving Simulator · Neural Radiance Fields
1
Introduction
Autonomous driving [10–12,14,22,28] is arguably the most important applica-
tion of modern 3D scene understanding [4,23] techniques. Nowadays, Robotaxis
can run in big cities with up-to-date HD maps, handling everyday driving sce-
narios smoothly. However, once a corner case that lies out of the distribution of
an autonomous driving algorithm happens on the road unexpectedly, the lives
of passengers are put at risk. The dilemma is that while we need more training
data about corner cases, collecting them in the real world usually means dan-
ger and high expenses. To this end, the community believes that photorealistic
simulation [5,9,15,25] is a technical path of great potential. If an algorithm can
experience enormous corner cases in a simulator with a small sim-to-real gap,
the performance bottleneck of current autonomous driving algorithms can be
potentially addressed.
Existing autonomous driving simulation methods have their own limitations.
CARLA [7] is a widely used sensor simulator based upon traditional graphics
engines, whose realism is restricted by asset modeling and rendering qualities.
AADS [15] also exploits traditional graphics engines but demonstrates impressive
photorealism using well-curated assets. On the other hand, GeoSim [5] introduces
a data-driven scheme for realistic simulation by learning an image enhancement
network. Flexible asset generation and rendering can be achieved through image
composition with promisingly good geometry and realistic appearance.
In this paper, we take advantage of the realistic rendering ability of NeRFs
for autonomous driving simulation. Training data captured from real-world envi-
ronments guarantees a small sim-to-real gap. Several works also exploit NeRFs
to model cars [18] and static backgrounds [9] in outdoor environments. However,
the inability to model complex dynamic scenes that are composed of both mov-
ing objects and static environments limits their practical use for real-world sen-
sor simulation. Recently, Neural Scene Graph (NSG) [19] decomposes dynamic
scenes into learned scene graphs and learns latent representations for category-
level objects. However, its multi-plane-based representation for background mod-
eling cannot synthesize images under large viewpoint changes.
To be speciﬁc, our central contribution is the very ﬁrst open-source NeRF-
based modular framework for photorealistic autonomous driving simulation. The
proposed pipeline models foreground instances and background environments
in a decomposed fashion. Diﬀerent NeRF backbone architectures and sampling
methods are incorporated in a uniﬁed manner with multi-modal inputs sup-
ported. The best module combination of the proposed framework achieves state-
of-the-art rendering performance on public benchmarks with large margins, indi-
cating photorealistic simulation results.
2
Method
Overview. As illustrated in Fig. 1, we aim to provide a modular framework
for constructing compositional neural radiance ﬁelds, where realistic sensor

MARS
5
Fig. 1. Pipeline. Left: We ﬁrst calculate the ray-box intersection of the queried ray
r and all visible instance bounding boxes {Bij}. For the background node, we directly
use the selected scene representation model and the chosen sampler to infer point-
wise properties, as in conventional NeRFs. For the foreground nodes, the ray is ﬁrst
transformed into the instance frame as ro before being processed through foreground
node representations (Sect. 2.1). Right: All the samples are composed and rendered
into RGB images, depth maps, and semantics (Sect. 2.2).
simulation can be conducted for outdoor driving scenes. A large unbounded
outdoor environment with plenty of dynamic objects is taken into consideration.
The input to the system consists of a set of RGB-images {Ii}N (captured by
vehicle-side or roadside sensors), sensor poses {Ti}N (calculated using IMU/GPS
signals), and object tracklets (including 3D bounding boxes {Bij}N×M, cate-
gories {typeij}N×M, and instance IDs {idxij}N×M). N is the number of input
frames and M is the number of tracked instances {Oj}M across the whole
sequence. An optional set of depth maps {Di}N and semantic segmentation
masks {Si}N can also be adopted as extra supervision signals during training.
By constructing a compositional neural ﬁeld, the proposed framework can sim-
ulate realistic sensor perception signals (including RGB images, depth maps,
semantic segmentation masks, etc.) at given sensor poses. Instance editing on
object trajectories and appearances is also supported.
Pipeline. Our framework model each foreground instance and the background
node compositionally. As shown in Fig. 1, when querying properties (RGB, depth,
semantics, etc.) of a given ray r, we ﬁrst calculate its intersection with all visible
objects’ 3D bounding boxes to get the entering and leaving distances [tin, tout].
Afterward, both the background node (Fig. 1 left-top) and the foreground object
nodes (Fig. 1 left-bottom) are queried, where each node samples a set of 3D points
and uses its speciﬁc neural representation network to obtain point properties
(RGB, density, semantics, etc.). Speciﬁcally, to query foreground nodes, we con-
vert the ray origins and directions from world space into instance frames accord-
ing to the object tracklets. Finally, all the ray samples from background and

6
Z. Wu et al.
foreground nodes are composed and volume-rendered to produce pixel-wise ren-
dering results (Fig. 1 right, Sect. 2.2).
We observe that the nature of background nodes (typically unbounded large-
scale scenes) diﬀers from the object-centric foreground nodes, while current
works [13,19] in sensor simulation use uniﬁed NeRF models. Our framework
provides a ﬂexible and open-sourced framework that supports diﬀerent design
choices of scene representations for background and foreground nodes and can
easily incorporate new state-of-the-art methods of static scene reconstruction
and object-centric reconstructions.
2.1
Scene Representation
We decompose the scene into a large-scale unbounded NeRF (as the background
node) and multiple object-centric NeRFs (as independent foreground nodes).
Conventionally, a neural radiance ﬁeld maps a given 3D point coordinate x =
(x, y, z), x ∈R3 and a 2D viewing direction d ∈S2 to its radiance c and volume
density σ shown in Eq. 1. Based upon this seminal representation, many variants
have been proposed for diﬀerent purposes, so we take a modular design.
f(x, d) = (c, σ) : [R3, S2] →[R3, R+]
(1)
The challenge of modeling unbounded background scene photo-realistically lies in
accurately representing far regions, so we utilize the unbounded scene warping [2]
to contract the far region. For foreground nodes, we support both the code-
conditioned representation f(x, d, z) = (c, σ) (z ∈Rk denotes the instance-wise
latent code) and the conventional ones, which will be explained as follows.
Architectures. In our modular framework, we support various NeRF back-
bones, which can be roughly categorized into two hyper-classes: MLP-based
methods [1,2,16], or grid-based methods that store spatially-variant features
in their hash grid voxel vertices [17,21]. Although these architectures diﬀer from
each other in details, they follow the same high-level formulation of Eq. 1 and
are capsuled in modules under a uniﬁed interface in MARS.
While the MLP-based representations are simple in mathematical form, we
give a formal exposition of grid-based methods. The speciﬁc implementation of
a multi-resolution feature grid {Gl
θ}L
l=1 has layer-wise resolutions Rl := ⌊Rmin ·
bl⌋, b = exp
ln Rmax −ln Rmin
L −1

, where Rmin , Rmax are the coarsest and the
ﬁnest resolution [17,26]. The coordinates x are ﬁrst scaled to each resolution
before being processed by the ceiling and ﬂooring operations to ⌈x · Rl⌉, ⌊x · Rl⌋
and hashed to obtain table indexes [17]. The extracted feature vectors are then
tri-linearly interpolated and decoded through a shallow MLP.
(c, σ) = fθ

interp(hash and lookup(x, {Gl
θ}L
l=1)), d

.
(2)
Sampling. We support various sampling strategies, including the recently pro-
posed proposal network [2], which distills a density ﬁeld from a radiance-free

MARS
7
NeRF model to generate ray samples and other sampling schemes like coarse-
to-ﬁne sampling [16] or uniform sampling [8] for ﬂexibility.
Foreground Nodes. For rendering foreground instances, we ﬁrst transform
the projected rays into per-instance coordinate space and then infer the object-
centric NeRFs in each instance-wise canonical space. The default setting of our
framework uses code-conditioned models that exploit latent codes to encode
instance features and shared category-level decoders to encode class-wise pri-
ors, allowing the modeling of many long tracklets with compact memory usage.
Meanwhile, the conventional ones without code conditions are also supported in
our framework. We detailed our modiﬁed foreground representation (denoted as
‘Ours’ in Sect. 3) in supplementary materials.
Fig. 2. Illustration on the compositional rendering. Some of the static vehicles in the
far region are considered as background objects.
2.2
Compositional Rendering
Figure 2 demonstrates the compositional rendering results. To render an image
at a given camera pose Ti, we cast a ray r = o + td at each rendered pixel. For
each ray r, we ﬁrst calculate the intersection interval [tin, tout] with all visible
foreground nodes Oij (Fig. 3) and transform the samples {P obj-j
k
} along the ray
from world space into each foreground canonical space. We also sample a set
of 3D points along the ray ({P bg
k } as background samples. Samples in all nodes
are ﬁrst passed through their corresponding networks to obtain point-wise colors
{cbg, obj
k
}, densities {σbg, obj
k
}, and foreground semantic logits {sbg
k }. Considering
that the semantic properties of foreground samples are actually their category
label, we create a one-hot vector as:
sobj-j
k
[l] =

σobj-j
k
if l = category of j’s instance
0
otherwise
, for l in category.
(3)
To aggregate the point-wise properties, we sort all the samples by their ray
distance in world space and use the standard volume rendering process to render
pixel-wise properties:

8
Z. Wu et al.
ˆc(r) =

Pi
Tiαici + (1 −accum) · csky,
Ti = exp(−
i−1

k=1
σkδk),
(4)
ˆd(r)=

Pi
Tiαiti + (1 −accum) · inf, ˆs(r)=

Pi
Tiαisi + (1 −accum) · ssky, (5)
where Pi ∈sorted({P bg, obj
i
}), αi = 1 −exp(−σiδi), δi = ti+1 −ti, accum =

Pi Tiαi, csky is the rendered color from the Sky model (Sect. 2.3), inf is the
upper bound distance, and ssky is the one-hot semantic logits of the sky category.
Fig. 3. Visual demonstration on our conﬂict-free sampling process. We use uniform
sampling in all nodes for illustration.
2.3
Towards Realistic Rendering
Sky Modeling. In our framework, we support the usage of a sky model to deal
with appearances at inﬁnite distance, where an MLP-based spherical environ-
ment map [20] is leveraged to model the inﬁnitely far regions that never intersect
opaque surfaces:
fsky(d) = csky : S2 →R3
(6)
However, na¨ıvely blending the sky color csky with background and foreground
rendering (Eq. 4) leads to potential inconsistency. Therefore, we introduce a BCE
semantic regularization to alleviate this issue:
Lsky = BCE(1 −accum, Ssky).
(7)
Resolving Conﬂict Samples. Due to the fact that our background and fore-
ground sampling are done independently, there is a chance that background
samples fall within the foreground bounding box (Fig. 3 Background Truncated
Samples). The compositional rendering may mistakenly classify foreground sam-
ples as background (referred to later as background-foreground ambiguity). As
a result, after removing the foreground instance, artifacts will emerge in the
background area (Fig. 4). Ideally, with suﬃcient multi-view supervision signal,
the system can automatically learn to distinguish between foreground and back-
ground during the training process. However, for a data-driven simulator, obtain-
ing abundant and high-quality multi-view images is challenging for users as vehi-
cles move fast on the road. The ambiguity is NOT observed in NSG [19] as NSG

MARS
9
only samples a few points on the ray-plane intersections, and is unlikely to have
much background truncated samples.
Fig. 4. We show that the background truncated samples cause background-foreground
ambiguity without our regularization.
To address this issue, we devise a regularization term that minimizes the
density sum of background truncated samples to minimize their inﬂuence during
the rendering process as:
Laccum =

P (tr)
i
σi,
(8)
where {P (tr)
i
} denotes background truncated samples.
2.4
Optimization
To optimize our system, we minimize the following objective function:
L = λ1Lcolor + λ2Ldepth + λ3Lsem + λ4Lsky + λ5Laccum,
(9)
where λ1−5 are weighting parameters. Lsky and Laccum are explained in Eq. 7
and 8.
Color Loss: we adopt a standard MSE loss that minimizes the photo-metric
errors as:
Lcolor = ||c(r) −ˆc(r)||2
2.
(10)
Depth Loss: We introduce a depth loss to address textureless regions and
regions that are observed from sparse viewpoints. We have devised two strategies
for supervising the geometry. Given depth data, we utilize a ray distribution loss
derived from [6]. On the other hand, if the depth data is not available, we utilize
a mono-depth network and apply mono-depth loss following [26].
Ldepth =

Lsensor depth
if depth data is available
Lmono depth if depth data is not available
(11)
Semantic Losses: We follow SemanticNeRF [29] and use a cross-entropy seman-
tic loss Lsem = CrossEntropy(s(r), S(r)).

10
Z. Wu et al.
3
Experiments
In this section, we provide extensive experimental results to demonstrate the
proposed instance-aware, modular, and realistic simulator for autonomous driv-
ing. We evaluate our method on scenes from the KITTI [10] dataset and the
Virtual KITTI-2 (V-KITTI) [3] dataset. In the following, we use “our default
setting” to denote a grid-based NeRF with proposal sampler for the background
node, and our modiﬁed category-level representation with coarse-to-ﬁne sampler
for foreground nodes.
Table 1. Quantitative results on image reconstruction task & Comparisons on the
settings with baseline methods. The dataset used for evaluation is KITTI.
NeRF [16] NeRF+Time NSG [19] PNF [13] SUDS [24] Ours
PSNR ↑
23.34
24.18
26.66
27.48
28.31
29.06
SSIM ↑
0.662
0.677
0.806
0.870
0.876
0.885
Instance-aware ×
×
✓
✓
×
✓
Modular
×
×
×
×
×
✓
Open-sourced
✓
–
✓
×
✓
✓
3.1
Photorealistic Rendering
We validate the photorealistic rendering performance of our simulator by evalu-
ating image reconstruction and novel view synthesis (NVS) following [19,24].
Table 2. Quantitative results on novel view synthesis
KITTI-75%
KITTI-50%
KITTI-25%
PSNR ↑SSIM ↑LPIPS ↓PSNR ↑SSIM ↑LPIPS ↓PSNR ↑SSIM ↑LPIPS ↓
NeRF [16]
18.56
0.557
0.554
19.12
0.587
0.497
18.61
0.570
0.510
NeRF+Time
21.01
0.612
0.492
21.34
0.635
0.448
19.55
0.586
0.505
NSG [19]
21.53
0.673
0.254
21.26
0.659
0.266
20.00
0.632
0.281
SUDS [24]
22.77
0.797
0.171
23.12
0.821
0.135
20.76
0.747
0.198
Ours
24.23
0.845
0.160
24.00
0.801
0.164
23.23
0.756
0.177
+1.46
+0.048
-0.011
+0.88
-0.020
+0.029
+2.47
+0.009
-0.021
VKITTI-75%
VKITTI-50%
VKITTI-25%
PSNR ↑SSIM ↑LPIPS ↓PSNR ↑SSIM ↑LPIPS ↓PSNR ↑SSIM ↑LPIPS ↓
NeRF [16]
18.67
0.548
0.634
18.58
0.544
0.635
18.17
0.537
0.644
NeRF+Time
19.03
0.574
0.587
18.90
0.565
0.610
18.04
0.545
0.626
NSG [19]
23.41
0.689
0.317
23.23
0.679
0.325
21.29
0.666
0.317
SUDS [24]
23.87
0.846
0.150
23.78
0.851
0.142
22.18
0.829
0.160
Ours
29.79
0.917
0.088
29.63
0.916
0.087
27.01
0.887
0.104
+5.92
+0.071
-0.062
+5.85
+0.065
-0.055
+4.83
+0.058
-0.056
Baselines. We conduct qualitative and quantitative comparisons against other
state-of-the-art methods: NeRF [16], NeRF with timestamp input (denoted as
NeRF+Time), NSG [19], PNF [13], and SUDS [24]. Note that none of them
simultaneously meet all three standards mentioned in Table 1.

MARS
11
Implementation Details. Our model is trained for 200,000 iterations with
4096 rays per batch, using RAdam as optimizers. The learning rate of the back-
ground node is assigned 1 ∗10−3 decaying to 1 ∗10−5, while that of 5 ∗10−3
decaying to 1 ∗10−5 in object nodes (Fig. 5).
Fig. 5. Qualitative image reconstruction results on KITTI dataset.
Experiment Settings. The training and testing image sets in the image recon-
struction setting are identical, while in the NVS task, we render the frames that
are not included in the training data. Speciﬁcally, we hold out every 4th frames,
every 2nd and 4th frames, and training with only one in every four frames, namely
25%, 50%, and 75%.
We follow the standard evaluation protocol in image synthesis and report
Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), and Learned
Perceptual Image Patch Similarity (LPIPS) [27] of our default setting for quan-
titative evaluations. Results are shown in Table 1 for image reconstruction and
Table 2 for NVS, which indicate that our method outperforms baseline methods
in both settings. We can achieve 29.79 PSNR on V-KITTI using 75% training
data, while the best result previously published is 23.87 (Fig. 6).
Fig. 6. Gallery of diﬀerent rendering channels.

12
Z. Wu et al.
3.2
Instance-Wise Editing
Our framework separately models background and foreground nodes, which
allows us to edit the scene in an instance-aware manner. We qualitatively present
our capability to remove instances, add new instances, and edit vehicle trajec-
tories. In Fig. 7, we show some editing examples of rotating and translating a
vehicle, though more results can be found in our video clip.
Fig. 7. Rendering results on the edited scene.
3.3
The Blessing of Modular Design
We use diﬀerent combinations of background and foreground nodes, samplers,
and supervision signals for evaluation, which is credited to our modular design.
Note that some of the baseline methods in the literature actually correspond
to an ablation entry in this table. For instance, PNF [13] uses NeRF as back-
ground node representation and instance-wise NeRF as foreground node repre-
sentation with semantic losses. NSG [19] uses NeRF as background node rep-
resentation and category-level NeRF as foreground representation, but with a
multi-plane sampling strategy. Our default setting uses grid-based background
node representation, and our proposed category-level method for foreground
node representation.
3.4
Ablation Results
In this section, we analyze diﬀerent experiment settings, verifying the necessity of
our design. We reveal the impact of diﬀerent design choices in background node
representation, foreground node representation, etc. Speciﬁcally, we present all
experiments with 50,000 iterations. Unlike prior works [13,19,24] that evaluate
their method on a short sequence of 90 images, we use the full sequence from
the dataset for all evaluation. Since they are not open-sourced and their exact
evaluation sequences are not known, we hope our new benchmarking would stan-
dardize this important ﬁeld. Quantitative evaluation can be found in Table 3.
For background and foreground nodes, we substitute our default model (ID
1 in Table 3) with MLP-based and grid-based model and list their metrics in row

MARS
13
2, 7–12. In the 3rd–6th row, we show the eﬀectiveness of our model components.
For model and sampler, selected modules for background and foreground nodes
are noted before and after the slash, respectively.
Table 3. Quantitative evaluation for ablation studies
Settings
KITTI
V-KITTI
ID
Model
Sampler
Category Lsky Ldepth Lsem Laccum PSNR ↑SSIM ↑LPIPS ↓PSNR ↑SSIM ↑LPIPS ↓
1* Grid / Ours
prop / c2f †
25.04
0.782
0.175
28.37
0.907
0.108
2
MLP / Ours
c2f / c2f
20.14
0.589
0.476
22.19
0.664
0.409
3
Grid / Ours
prop / c2f
×
21.35
0.713
0.242
27.30
0.881
0.130
4
Grid / Ours
prop / c2f
×
×
23.68
0.774
0.181
27.32
0.881
0.129
5
Grid / Ours
prop / c2f
×
23.66
0.769
0.184
27.30
0.880
0.128
6
Grid / Ours
prop / c2f
×
20.07
0.723
0.251
27.42
0.863
0.148
7
Grid / MLP
prop / c2f
20.46
0.709
0.255
26.46
0.875
0.132
8
Grid / Grid
prop / prop
22.23
0.741
0.211
25.22
0.871
0.134
9
Grid / MLP
prop / c2f
×
20.98
0.699
0.257
27.27
0.881
0.130
10
Grid / Grid
prop / prop
×
23.71
0.763
0.193
26.65
0.882
0.125
11* MLP / MLP
c2f / c2f
20.42
0.592
0.472
21.77
0.659
0.410
† prop stands for proposal sampler, and c2f stands for coarse-to-ﬁne sampler.
* ID 1 is our default setting. ID 11 is similar to the setting of NSG [19] with coarse-
to-ﬁne sampler instead.
4
Conclusion
In this paper, we present a modular framework for photorealistic autonomous
driving simulation based on NeRFs. Our open-sourced framework consists of
a background node and multiple foreground nodes, enabling the modeling of
complex dynamic scenes. We demonstrate the eﬀectiveness of our framework
through extensive experiments. The proposed pipeline achieved state-of-the-art
rendering performance on public benchmarks. We also support diﬀerent design
choices of scene representations and sampling strategies, oﬀering ﬂexibility and
versatility in the simulation process.
Limitations. Our method requires hours to train and is not capable of rendering
in real-time. Besides, our method fails to consider the dynamic specular eﬀects on
glasses or other reﬂective materials that may cause artifacts in rendered images.
Improving simulation eﬃciency and view-dependent eﬀects will be our future
work.
References
1. Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srini-
vasan, P.P.: Mip-NeRF: a multiscale representation for anti-aliasing neural radiance
ﬁelds. In: 2021 IEEE/CVF International Conference on Computer Vision (ICCV),
pp. 5835–5844 (2021)
2. Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-
NeRF 360: unbounded anti-aliased neural radiance ﬁelds. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. arXiv (2022)

14
Z. Wu et al.
3. Cabon, Y., Murray, N., Humenberger, M.: Virtual KITTI 2. http://arxiv.org/abs/
2001.10773
4. Chen, X., Zhao, H., Zhou, G., Zhang, Y.Q.: PQ-transformer: jointly parsing 3D
objects and layouts from point clouds. IEEE Robot. Autom. Lett. 7(2), 2519–2526
(2022)
5. Chen, Y., et al.: GeoSim: realistic video simulation via geometry-aware composition
for self-driving. http://arxiv.org/abs/2101.06543
6. Deng, K., Liu, A., Zhu, J.Y., Ramanan, D.: Depth-supervised NeRF: fewer views
and faster training for free. In: 2022 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 12872–12881 (2022)
7. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: CARLA: an open
urban driving simulator. In: Proceedings of the 1st Annual Conference on Robot
Learning, pp. 1–16. PMLR (2017)
8. Fridovich-Keil, S., Meanti, G., Warburg, F., Recht, B., Kanazawa, A.: K-planes:
explicit radiance ﬁelds in space, time, and appearance. In: Computer Vision and
Pattern Recognition (2023)
9. Fu, X., et al.: Panoptic NeRF: 3D-to-2D label transfer for panoptic urban scene
segmentation. In: 2022 International Conference on 3D Vision (3DV), pp. 1–11
(2022)
10. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: the KITTI
dataset. Int. J. Robot. Res. 32(11), 1231–1237 (2013)
11. Hu, Y., et al.: Planning-oriented autonomous driving. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17853–
17862 (2023)
12. Jin, B., et al.: ADAPT: action-aware driving caption transformer. In: 2023 IEEE
International Conference on Robotics and Automation (ICRA), pp. 7554–7561
(2023)
13. Kundu, A., et al.: Panoptic neural ﬁelds: a semantic object-aware neural scene
representation. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 12871–12881 (2022)
14. Li, P., et al.: LODE: locally conditioned eikonal implicit scene completion from
sparse LiDAR. In: 2023 IEEE International Conference on Robotics and Automa-
tion (ICRA). arXiv (2023)
15. Li, W., et al.: AADS: augmented autonomous driving simulation using data-driven
algorithms. Sci. Robot. 4(28), eaaw0863 (2019)
16. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.: NeRF: representing scenes as neural radiance ﬁelds for view synthesis. In:
Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol.
12346, pp. 405–421. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-
58452-8 24
17. M¨uller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives
with a multiresolution hash encoding. ACM Trans. Graph. 41(4), 1–15 (2022)
18. Niemeyer, M., Geiger, A.: GIRAFFE: representing scenes as compositional gener-
ative neural feature ﬁelds. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pp. 11453–11464 (2021)
19. Ost, J., Mannan, F., Thuerey, N., Knodt, J., Heide, F.: Neural scene graphs for
dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. arXiv (2021)
20. Rematas, K., et al.: Urban radiance ﬁelds. In: 2022 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 12922–12932 (2022)

MARS
15
21. Tancik, M., et al.: Nerfstudio: a modular framework for neural radiance ﬁeld devel-
opment. ACM Trans. Graph. 1(1) (2023)
22. Tian, B., Liu, M., Gao, H.A., Li, P., Zhao, H., Zhou, G.: Unsupervised road
anomaly detection with language anchors. In: 2023 IEEE International Confer-
ence on Robotics and Automation (ICRA), pp. 7778–7785 (2023)
23. Tian, B., Luo, L., Zhao, H., Zhou, G.: VIBUS: data-eﬃcient 3D scene parsing
with VIewpoint Bottleneck and Uncertainty-Spectrum modeling. J. Photogramm.
Remote Sens. 194, 302–318 (2022)
24. Turki, H., Zhang, J.Y., Ferroni, F., Ramanan, D.: SUDS: scalable urban dynamic
scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. arXiv (2023)
25. Yang, Z., et al.: UniSim: a neural closed-loop sensor simulator. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
1389–1399 (2023)
26. Yu, Z., Peng, S., Niemeyer, M., Sattler, T., Geiger, A.: MonoSDF: exploring monoc-
ular geometric cues for neural implicit surface reconstruction. In: Advances in Neu-
ral Information Processing Systems (2022)
27. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable eﬀec-
tiveness of deep features as a perceptual metric. In: 2018 IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 586–595 (2018)
28. Zheng, Y., et al.: STEPS: joint self-supervised nighttime image enhancement and
depth estimation. In: 2023 IEEE Conference on Robotics and Automation (ICRA
2023) (2023)
29. Zhi, S., Laidlow, T., Leutenegger, S., Davison, A.J.: In-place scene labelling
and understanding with implicit scene representation. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision (2021)

Concealed Object Segmentation
with Hierarchical Coherence Modeling
Fengyang Xiao1
, Pan Zhang1
, Chunming He2(B)
, Runze Hu3
,
and Yutao Liu4
1 Sun Yat-sen University, Zhuhai 510275, China
xiaofy5@mail2.sysu.edu.cn
2 Tsinghua Shenzhen International Graduate School, Tsinghua University,
Shenzhen 518055, China
chunminghe19990224@gmail.com
3 Beijing Institute of Technology, Beijing 100086, China
4 School of Computer Science and Technology, Ocean University of China,
Qingdao 266000, China
Abstract. Concealed object segmentation (COS) is a challenging task
that involves localizing and segmenting those concealed objects that are
visually blended with their surrounding environments. Despite achiev-
ing remarkable success, existing COS segmenters still struggle to achieve
complete segmentation results in extremely concealed scenarios. In this
paper, we propose a Hierarchical Coherence Modeling (HCM) segmenter
for COS, aiming to address this incomplete segmentation limitation. In
speciﬁc, HCM promotes feature coherence by leveraging the intra-stage
coherence and cross-stage coherence modules, exploring feature corre-
lations at both the single-stage and contextual levels. Additionally, we
introduce the reversible re-calibration decoder to detect previously unde-
tected parts in low-conﬁdence regions, resulting in further enhancing
segmentation performance. Extensive experiments conducted on three
COS tasks, including camouﬂaged object detection, polyp image seg-
mentation, and transparent object detection, demonstrate the promising
results achieved by the proposed HCM segmenter.
Keywords: Concealed object segmentation · Hierarchical coherence
modeling · Edge reconstruction
1
Introduction
Concealed object segmentation (COS) is a challenging task with the purpose of
localizing and segmenting those objects visually blended in their surrounding
F. Xiao—First Author
This work was supported by the National Science Foundation of China under
grant 62201538 and Natural Science Foundation of Shandong Province under grant
ZR2022QF006.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 16–27, 2024.
https://doi.org/10.1007/978-981-99-8850-1_2

Concealed Object Segmentation with Hierarchical Coherence Modeling
17
Fig. 1. Results of UGTR [33], SegMaR [14], and the proposed HCM. It is observed
that our HCM can generate more accurate and complete results.
scenarios [3,8]. COS is a general task encompassing various applications, such
as camouﬂaged object detection (COD) [3], polyp image segmentation (PIS) [4],
and transparent object detection (TOD) [22].
COS poses signiﬁcant challenges due to the intrinsic similarity between fore-
ground objects and their corresponding scenarios, making it diﬃcult to iden-
tify discriminative cues for complete and accurate foreground-background sep-
aration. To cope with this challenge, existing COS segmenters have employed
various strategies, e.g., drawing inspiration from human vision [21,25], incorpo-
rating frequency clues [7], and adopting joint modeling strategies across multiple
tasks [19,34]. Despite their notable achievements, existing segmenters still strug-
gle to achieve precise results in some extremely concealed scenarios. As shown
in Fig. 1, while UGTR [33] and SegMaR [14] manage to ﬁnd the rough regions
for the concealed objects, the prediction results are still incomplete.
To overcome this limitation, we propose a Hierarchical Coherence Model-
ing (HCM) segmenter for the COS task, which aims to generate more com-
plete segmentation maps by promoting feature coherence. HCM incorporates
two key components, namely, intra-stage coherence (ISC) and cross-stage coher-
ence (CSC), to explore feature correlations at both single stage and contextual
levels. Additionally, we develop the reversible re-calibration decoder (RRD) to
detect previously undetected parts in those low-conﬁdence regions and thus fur-
ther improve segmentation performance.
Our contributions are summarized as follows:
– We propose the Hierarchical Coherence Modeling (HCM) segmenter for the
COS task. HCM encourages feature coherence and thus alleviating the incom-
plete segmentation problem.
– We introduce RRD to detect previously undetected parts in low-conﬁdence
regions, thus improving segmentation performance.
– The proposed HCM signiﬁcantly outperforms the state-of-the-art methods on
three COS tasks by a large margin, i.e., camouﬂaged object detection, polyp
image segmentation, and transparent object detection.

18
F. Xiao et al.
x
ISC
ISC
ISC
ISC
CSC
CSC
ASPP
E0
E1
E2
E3
E4
0f
1f
2f
3f
4f
1f
2f
3f
Hierarchical Coherence Modeling
CSC
1
if
2
if
3
if
4
if
4
cf
3
cf
2
cf
RRD
D1
D2
D3
D4
1
cf
2
cf
3
cf
4
cf
1p
2p
3p
4p
5p
5p
4p
3p
2p
ISC
CSC
RRD
Intra-stage Coherence
Cross-stage Coherence
Reversible Re-calibration Decoder
1p
Fig. 2. Architecture of the proposed HCM.
2
Related Works
Concealed Object Segmentation. Traditional COS techniques heavily rely
on manually designed feature extraction operators, which inherently suﬀer from
limited feature extraction capacity and struggle to handle extremely complex
scenarios. In contrast, learning-based approaches, facilitated by the rapid devel-
opment of deep learning, have achieved remarkable success in this ﬁeld. For
instance, MGL [34] introduces an auxiliary edge reconstruction task and con-
structs a mutual graph learning strategy to generate prediction maps with clear
boundaries. Inspired by human vision principles [21], PFNet leverages distraction
mining techniques to achieve accurate concealed object segmentation. Recogniz-
ing the limitations of human vision, FEDER [7] introduces an adaptive decom-
position approach to extract subtle yet discriminative clues that may have been
overlooked. Unlike existing COS solvers, we propose HCM segmenter for the
COS task, which encourages feature coherence and thus alleviates the incomplete
segmentation problem. Additionally, we introduce the reversible re-calibration
decoder (RRD) to detect previously undetected parts in low-conﬁdence regions,
further enhancing the segmentation performance (Fig. 2).

Concealed Object Segmentation with Hierarchical Coherence Modeling
19
3
Methodology
3.1
Concealed Feature Encoder
Follow [3], we employ ResNet50 [6,11] as the default backbone of the basic
encoder E for feature extraction. Given a concealed image X, we obtain a series
of feature maps {fs}4
s=0. Considering that f4 has abundant semantic information,
we further feed this feature map into an astrous spatial pyramid pooling (ASPP)
module As [12] to generate a coarse segmentation map p5: p5 = As(f4).
Fig. 3. Details of ISC, CSC, and RRD.
3.2
Hierarchical Coherence Modeling
Due to the inherent similarity between concealed objects and their surrounding
contexts, obtaining accurate and complete segmentation results poses a signiﬁ-
cant challenge for the segmenter. To address this issue, we propose a hierarchical
coherence modeling strategy that explicitly promotes feature coherence to facil-
itate more comprehensive predictions. Illustrated in Fig. 3, this strategy consists
of two parts: intra-stage coherence (ISC) and cross-stage coherence (CSC). These
parts work in conjunction to enhance the coherence of features at diﬀerent stages,
promoting better segmentation outcomes.
Intra-stage Coherence. ISC aims to discover feature correlations by fusing
multi-scale features with diﬀerent receptive ﬁelds within a single stage. This
allows the aggregated features to capture scale-invariant information. As illus-
trated in Fig. 3, ISC comprises two primary branches with residual connections.
Given the input feature fs, we initially apply a 1 × 1 convolution to reduce the
channel dimension. Subsequently, we process these features with two parallel
convolutions using diﬀerent kernel sizes, resulting in features f 3
s and f 5
s :
f 3
s = conv3(conv1(fs)), f 5
s = conv5(conv1(fs)),
(1)

20
F. Xiao et al.
where conv1, conv3, conv5 denote 1 × 1, 3 × 3, 5 × 5 convolutions, respectively.
We proceed by merging the features f 3
s and f 5
s , which are obtained from the pre-
vious step. These merged features are then further processed using two parallel
convolutions. Finally, we multiply the outputs of these convolutions in a resid-
ual connection structure to extract the scale-invariant information. This process
yields the aggregated features {f i
s}4
s=1:
f i
s = conv1(fs) + conv3(conv3(f 3
s ⊕f 5
s ) ⊗conv5(f 3
s ⊕f 5
s )),
(2)
where ⊕and ⊗denote pixel-level summation and multiplication. This design
facilitates the extraction of features at multiple scales and enhances the ability
to capture diverse contextual information.
Cross-Stage Coherence. CSC explores the contextual feature correlations by
selectively interacting cross-stage information with joint attention JA(•), com-
prising spatial attention and channel attention [2,13]. Additionally, we employ
position normalization PN(•) to highlight the contextual similarity and eliminate
discrepancy interference information across diﬀerent stages, getting {f c
s}4
s=1:
f c
s = PN(JA(f i
s, up(f i
s+1))),
(3)
where up(•) denotes up-sampling operator.
3.3
Reversible Re-calibration Decoder
Due to the complexity of concealed object scenes, segmenters produce predic-
tion maps that contain low-conﬁdence and ambiguous regions. To tackle this
challenge, we propose a novel module called Reversible Re-calibration Decoder
(RRD). As shown in Fig. 3, RRD leverages both the previous decoder’s predic-
tion map as prior information and reverses the prediction map to extract cues
from the low-conﬁdence regions. This allows the segmenter to eﬀectively detect
previously undetected parts in these regions, leading to improved segmentation
performance. Consequently, the prediction map {ps}4
s=1 is deﬁned as follows:
ps = conv3 (RCAB (cat (f s
k ⊙S (rp (ps+1)) , f s
k ⊙rv (S (rp (ps+1)))))) ,
(4)
where rp(•), S(•), rv(•), ⊙, and conv3(•) denote repeat, Sigmoid, reverse (element-
wise subtraction with 1), Hadamard product, and 3×3 convolution. RCAB(•) is
the residual channel attention block [15,31] and we employ this block to empha-
size the noteworthy information.
3.4
Loss Functions
We follow the practice in [3,7] and employ the weighted binary cross-entropy
loss Lw
BCE [32] and weighted intersection-over-union loss Lw
IoU [26] to supervised
our HCM with the ground truth Y in a multiscale manner ,which is deﬁned as
follows:
L =
5

s=1
1
2s−1 (Lw
BCE (ps, Y) + Lw
BCE (ps, Y)) + LOH.
(5)

Concealed Object Segmentation with Hierarchical Coherence Modeling
21
Table 1. Quantitative comparisons of our method and other 9 ResNet50-based SOTAs
on COD. The best two results are in red and blue fonts.
CAMO (250 images)
COD10K (2,026 images)
NC4K (4,121 images)
Methods
Pub.
M ↓
Fβ ↑
Eφ ↑
Sα ↑
M ↓
Fβ ↑
Eφ ↑
Sα ↑
M ↓
Fβ ↑
Eφ ↑
Sα ↑
PFANet [37]
CVPR19
0.132
0.607
0.701
0.695
0.074
0.478
0.729
0.716
0.095
0.634
0.760
0.752
CPD [29]
CVPR19
0.113
0.675
0.723
0.716
0.053
0.578
0.776
0.750
0.072
0.719
0.808
0.787
EGNet [36]
ICCV19
0.109
0.667
0.800
0.732
0.061
0.526
0.810
0.736
0.075
0.671
0.841
0.777
SINet [3]
CVPR20
0.092
0.712
0.804
0.745
0.043
0.667
0.864
0.776
0.058
0.768
0.871
0.808
PFNet [21]
CVPR21
0.085
0.751
0.841
0.782
0.040
0.676
0.877
0.800
0.053
0.779
0.887
0.829
MGL-R [34]
CVPR21
0.088
0.738
0.812
0.775
0.035
0.680
0.851
0.814
0.053
0.778
0.867
0.833
MGL-S [34]
CVPR21
0.089
0.733
0.806
0.772
0.037
0.666
0.844
0.811
0.055
0.771
0.862
0.829
LSR [20]
CVPR21
0.080
0.756
0.838
0.787
0.037
0.699
0.880
0.804
0.048
0.802 0.890
0.834
UGTR [33]
ICCV21
0.086
0.747
0.821
0.784
0.036
0.670
0.852
0.817 0.052
0.778
0.874
0.839
SegMaR [14] CVPR22 0.072 0.772 0.861 0.805 0.035 0.699 0.890
0.813
0.052
0.767
0.885
0.835
HCM (Ours)
—
0.070 0.782 0.873 0.806 0.032 0.736 0.902 0.820 0.046 0.816 0.900 0.846
Fig. 4. Qualitative analysis on the COD task.
4
Experiments
Implementation Details. Our HCM is implemented on two RTX3090TI GPUs
and is optimized by Adam with the momentum terms (0.9, 0.999). Following [3],
our encoder is initialized with the model pre-trained on ImageNet. During the
training phase, the batch size is set to 32. The learning rate is initialized to 0.0001
and is divided by 10 every 80 epochs. The images are resized as 352 × 352.
4.1
Camouﬂaged Object Segmentation
Datasets and Metrics. Following [14], three datasets are utilized for evalua-
tion, including CAMO, COD10K, and NC4K. CAMO comprises 1,250 camou-
ﬂaged images with 8 categories. COD10K have 10 super-classes, containing 5,066
images. NC4K is the largest testing set which contains 4,121 images. Same as
existing methods [3,14], our training set comprises 1,000 images from CAMO
and 3,000 images from COD10K, and our test set integrates the rest of images.

22
F. Xiao et al.
Table 2. Quantitative comparisons on two benchmarks in polyp image segmentation.
The best two results are in red and blue fonts.
Methods
CVC-ColonDB (380 images)
Kvasir (100 images)
mDice ↑mIoU ↑
M ↓
F w
β ↑Emax
φ
↑
Sα ↑
mDice ↑mIoU ↑
M ↓
F w
β ↑Emax
φ
↑
Sα ↑
U-Net [27]
0.512
0.444
0.061
0.498
0.776
0.712
0.818
0.746
0.055
0.794
0.893
0.858
Atten-UNet [24]
0.466
0.385
0.071
0.431
0.724
0.670
0.769
0.683
0.062
0.730
0.859
0.828
SFA [5]
0.469
0.347
0.094
0.379
0.765
0.634
0.723
0.611
0.075
0.670
0.849
0.782
PraNet [4]
0.709
0.640
0.045
0.696
0.869
0.819
0.898
0.840
0.030
0.885
0.948
0.915
MSNet [39]
0.755
0.678
0.041
0.737
0.883
0.836
0.907
0.862
0.028
0.893
0.944
0.922
TGANet [28]
0.722
0.661
0.043
0.711
0.875
0.823
0.902
0.845
0.030
0.891
0.952
0.920
LADK [35]
0.764
0.683
0.039 0.739
0.862
0.834
0.905
0.852
0.028
0.887
0.947
0.918
M2SNet [38]
0.758
0.685
0.038 0.737
0.869
0.842
0.912
0.861
0.025 0.901
0.953
0.922
HCM (Ours)
0.775
0.687
0.038 0.741
0.885
0.845
0.910
0.868
0.025 0.903
0.956
0.924
Fig. 5. Qualitative analysis on the PIS task.
Following [3,9], we utilize four metrics, namely mean absolute error M, adap-
tive F-measure Fβ, mean E-measure Eφ, and structure measure Sα. Smaller M
means better performance, yet this is reversed on Fβ, Eφ, and Sα.
Quantitative Analysis. We compare the proposed Hierarchical Coherence
Model (HCM) with nine other state-of-the-art ResNet50-based segmenters on
the COD task and present the segmentation results in Table 1. As shown in the
table, our HCM outperforms all other segmenters and achieves the top rank-
ing. It surpasses the second-best COD segmenter, SegMaR [14], by a margin of
3.6%. This remarkable performance clearly showcases the superior capability of
our HCM in enhancing feature coherence.
Qualitative Analysis. As shown in Fig. 4, our HCM demonstrates the capa-
bility to generate more complete and comprehensive segmentation results and
reduces those uncertainty regions. This improvement can be attributed to our
novel hierarchical coherence modeling strategy, which enhances feature coher-

Concealed Object Segmentation with Hierarchical Coherence Modeling
23
Table 3. Quantitative comparisons on two benchmarks in transparent object detec-
tion. The best two results are in red and blue fonts.
GDD (936 images)
GSD (810 images)
Methods
mIoU ↑F max
β
↑
M ↓
BER ↓mIoU ↑F max
β
↑
M ↓
BER ↓
PMD [17]
0.870
0.930
0.067
6.17
0.817
0.890
0.061
6.74
GDNet [22]
0.876
0.937
0.063
5.62
0.790
0.869
0.069
7.72
GlassNet [16]
0.881
0.932
0.059
5.71
0.836
0.901
0.055
6.12
EBLNet [10]
0.870
0.922
0.064
6.08
0.817
0.878
0.059
6.75
CSNet [1]
0.773
0.876
0.135
11.33
0.666
0.805
0.135
14.76
PGNet [30]
0.857
0.930
0.074
6.82
0.805
0.897
0.068
7.88
GDNet-B [23]
0.878
0.939
0.061
5.52
0.792
0.874
0.066
7.61
ESRNet [18]
0.901
0.942
0.046
4.46
0.854
0.911
0.046
5.74
HCM (Ours)
0.908
0.946
0.045
4.42
0.858
0.922
0.045
5.52
Fig. 6. Qualitative analysis on the TOD task.
ence within the segmenter. Furthermore, the proposed reversible re-calibration
decoder plays a crucial role in enabling the segmenter to eﬀectively identify
previously undetected parts within these regions.
4.2
Polyp Image Segmentation
Datasets and Metrics. In line with the methodology employed in [38], we
evaluate the segmentation performance on two widely-used benchmark datasets:
CVC-ColonDB and Kvasir. 900 images from Kvasir make up the training set and
the testing set comprises the remaining images. Additionally, Consistent with
[38], we adopt six metrics for quantitative evaluation: namely mean dice (mDice),
mean IoU (mIoU), M, weighted F-measure (F w
β ), max E-measure (Emax
φ
), and

24
F. Xiao et al.
Table 4. Ablation study on COD10K of the COD task, where “w/o” denotes without.
The best results are marked in bold.
Metrics w/o HCM component w/o RRD Ours
M ↓
0.035
0.033
0.032
Fβ ↑
0.702
0.725
0.736
Eφ ↑
0.866
0.893
0.902
Sα ↑
0.803
0.815
0.820
(a) Break down ablations of HCM.
Metrics w/o ISC w/o CSC Ours
M ↓
0.033
0.033
0.032
Fβ ↑
0.725
0.727
0.736
Eφ ↑
0.895
0.888
0.902
Sα ↑
0.812
0.815
0.820
(b) Eﬀect of the HCM strategy.
Sα. For mDice and mIoU, higher values indicate better performance, whereas
for the remaining four metrics, higher values indicate poorer performance.
Quantitative Analysis. Table 2 presents the quantitative comparisons on two
benchmarks in polyp image segmentation. As exhibited in Table 2, our proposed
HCM outperforms the second-best techniques in 0.6%. This improvement can
be attributed to the introduction of our novel modules, namely the Inter-Scale
Coherence (ISC) and Contextual Scale Coherence (CSC) modules, which enable
the exploration of feature correlations at both single-stage and contextual levels.
Qualitative Analysis. As shown in Fig. 5, our method can capture polyp more
accurately because the proposed HCM method encourages feature coherence,
which enables ﬁner discrimination of the gap between foreground and back-
ground, leading to improved accuracy in polyp segmentation.
4.3
Transparent Object Detection
Datasets and Metrics. In accordance with the experimental setup in [18],
we conduct our evaluations on two datasets: GDD and GSD. To assess the
segmentation results, we employ four widely-used metrics: mean intersection
over union (mIoU), maximum F-measure (F max
β
), M, and balance error rate
(BER). The training set consists of 2,980 images from GDD and 3,202 images
from GSD, while the remaining images are allocated to the testing set. It is
important to note that a smaller value for M or BER, or a higher value for IoU
and F max
β
indicates superior segmentation performance.
Quantitative Analysis. Table 3 demonstrates the superior performance of our
HCM in transparent object detection (TOD). Our method outperforms the
second-best TOD solver, ESRNet, by 1.5%. This substantial improvement fur-
ther validates the eﬀectiveness and advancement of our proposed HCM seg-
menter in addressing the challenges of the TOD task.
Qualitative Analysis. As depicted in Fig. 6, our HCM segmenter achieves more
accurate and complete segmentation results compared to the other methods. In
contrast, the comparison methods often produce incomplete segments or exhibit

Concealed Object Segmentation with Hierarchical Coherence Modeling
25
blurred parts. These visual comparisons provide compelling evidence of the supe-
riority of our method in addressing the challenges of incomplete segmentation
with low-conﬁdence regions.
4.4
Ablation Study and Further Analysis
We conduct ablation studies about our HCM on COD10K of the COD task.
Break Down Ablations of HCM. As demonstrated in Table 4a, when exam-
ining the individual components of our HCM, namely the HCM component or
RRD, the performance of HCM decreases. This observation highlights the supe-
riority of our proposed components in contributing to the overall performance
of the proposed segmenter.
Eﬀect of the Hierarchical Coherence Modeling Strategy. We conducted
additional experiments to validate the eﬀectiveness of each component in our
proposed hierarchical coherence modeling strategy. As shown in Table 4b, our
results demonstrate the superiority of the ISC and CSC. These components
work in synergy to form a powerful hierarchical coherence modeling strategy.
5
Conclusions
In this paper, we present a novel segmenter called HCM for COS with the aim of
addressing the existing limitation of incomplete segmentation. The HCM method
focuses on promoting feature coherence by utilizing both intra-stage coherence
and cross-stage coherence modules, which explore feature correlations at both
the single-stage and contextual levels. Moreover, we introduce the reversible
re-calibration decoder to identify previously undetected parts in regions with
low-conﬁdence, thereby further improving the segmentation performance. The
eﬀectiveness of the proposed HCM segmenter is demonstrated through extensive
experiments conducted on three diﬀerent COS tasks: camouﬂaged object detec-
tion, polyp image segmentation, and transparent object detection. The results
obtained from these experiments show promising outcomes, aﬃrming the eﬃcacy
of the HCM approach.
References
1. Cheng, M.M., Gao, S.H., Borji, A., Tan, Y.Q., Lin, Z., Wang, M.: A highly eﬃcient
model to study the semantics of salient object detection. IEEE Trans. Pattern Anal.
Mach. Intell. 44(11), 8006–8021 (2021)
2. Deng, L., He, C., Xu, G., Zhu, H., Wang, H.: PcGAN: a noise robust conditional
generative adversarial network for one shot learning. IEEE Trans. Intell. Transp.
Syst. 23(12), 25249–25258 (2022)
3. Fan, D.P., Ji, G.P., Sun, G., Cheng, M.M., Shen, J., Shao, L.: Camouﬂaged object
detection. In: CVPR, pp. 2777–2787 (2020)

26
F. Xiao et al.
4. Fan, D.P., et al.: PraNet: parallel reverse attention network for polyp segmentation.
In: Martel, A.L., et al. (eds.) MICCAI 2020, Part VI. LNCS, vol. 12266, pp. 263–
273. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-59725-2 26
5. Fang, Y., Chen, C., Yuan, Y., Tong, K.: Selective feature aggregation network
with area-boundary constraints for polyp segmentation. In: Shen, D., et al. (eds.)
MICCAI 2019. LNCS, vol. 11764, pp. 302–310. Springer, Cham (2019). https://
doi.org/10.1007/978-3-030-32239-7 34
6. He, C., et al.: HQG-Net: unpaired medical image enhancement with high-quality
guidance. arXiv preprint: arXiv:2307.07829 (2023)
7. He, C., et al.: Camouﬂaged object detection with feature decomposition and edge
reconstruction. In: CVPR (2023)
8. He, C., et al.: Weakly-supervised concealed object segmentation with SAM-based
pseudo labeling and multi-scale feature grouping. arXiv preprint: arXiv:2305.11003
(2023)
9. He, C., Wang, X., Deng, L., Xu, G.: Image threshold segmentation based on GLLE
histogram. In: CPSCom, pp. 410–415. IEEE (2019)
10. He, H., et al.: Enhanced boundary learning for glass-like object segmentation. In:
ICCV, pp. 15859–15868 (2021)
11. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR, pp. 770–778 (2016)
12. Hu, R., Liu, Y., Gu, K., Min, X., Zhai, G.: Toward a no-reference quality metric
for camera-captured images. IEEE Trans. Cybern. (2021)
13. Hu, R., Liu, Y., Wang, Z., Li, X.: Blind quality assessment of night-time image.
Displays 69, 102045 (2021)
14. Jia, Q., Yao, S., Liu, Y., Fan, X., Liu, R., Luo, Z.: Segment, magnify and reiterate:
detecting camouﬂaged objects the hard way. In: CVPR, pp. 4713–4722 (2022)
15. Ju, M., He, C., Liu, J., Kang, B., Su, J., Zhang, D.: IVF-Net: an infrared and
visible data fusion deep network for traﬃc object enhancement in intelligent trans-
portation systems. IEEE Trans. Intell. Transp. Syst. 24, 1220–1234 (2022)
16. Lin, J., He, Z., Lau, R.W.: Rich context aggregation with reﬂection prior for glass
surface detection. In: CVPR, pp. 13415–13424 (2021)
17. Lin, J., Wang, G., Lau, R.W.: Progressive mirror detection. In: CVPR, pp. 3697–
3705 (2020)
18. Lin, J., Yeung, Y.H., Lau, R.: Exploiting semantic relations for glass surface detec-
tion. NIPS 35, 22490–22504 (2022)
19. Lu, Y., He, C., Yu, Y.F., Xu, G., Zhu, H., Deng, L.: Vector co-occurrence mor-
phological edge detection for colour image. IET Image Process. 15(13), 3063–3070
(2021)
20. Lv, Y., et al.: Simultaneously localize, segment and rank the camouﬂaged objects.
In: CVPR, pp. 11591–11601 (2021)
21. Mei, H., Ji, G.P., Wei, Z., Yang, X., Wei, X., Fan, D.P.: Camouﬂaged object seg-
mentation with distraction mining. In: CVPR, pp. 8772–8781 (2021)
22. Mei, H., et al.: Don’t hit me! glass detection in real-world scenes. In: CVPR, pp.
3687–3696 (2020)
23. Mei, H., Yang, X., Yu, L., Zhang, Q., Wei, X., Lau, R.W.: Large-ﬁeld contextual
feature learning for glass detection. IEEE Trans. Pattern Anal. Mach. Intell. 45,
3329–3346 (2023)
24. Oktay, O., et al.: Attention U-Net: learning where to look for the pancreas. arXiv
preprint: arXiv:1804.03999 (2018)
25. Pang, Y., Zhao, X., Xiang, T.Z., Zhang, L., Lu, H.: Zoom in and out: a mixed-scale
triplet network for camouﬂaged object detection. In: CVPR, pp. 2160–2170 (2022)

Concealed Object Segmentation with Hierarchical Coherence Modeling
27
26. Rahman, M.A., Wang, Y.: Optimizing intersection-over-union in deep neural net-
works for image segmentation. In: Bebis, G., et al. (eds.) ISVC 2016. LNCS, vol.
10072, pp. 234–244. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
50835-1 22
27. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomed-
ical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.
(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-24574-4 28
28. Tomar, N.K., Jha, D., Bagci, U., Ali, S.: TGANet: text-guided attention for
improved polyp segmentation. In: Wang, L., Dou, Q., Fletcher, P.T., Speidel, S., Li,
S. (eds.) Medical Image Computing and Computer Assisted Intervention – MIC-
CAI 2022. Lecture Notes in Computer Science, vol. 13433, pp. 151–160. Springer,
Cham (2022). https://doi.org/10.1007/978-3-031-16437-8 15
29. Wu, Z., Su, L., Huang, Q.: Cascaded partial decoder for fast and accurate salient
object detection. In: CVPR, pp. 3907–3916 (2019)
30. Xie, C., Xia, C., Ma, M., Zhao, Z., Chen, X., Li, J.: Pyramid grafting network for
one-stage high resolution saliency detection. In: CVPR, pp. 11717–11726 (2022)
31. Xu, G., He, C., Wang, H., Zhu, H., Ding, W.: DM-Fusion: deep model-driven
network for heterogeneous image fusion. IEEE Trans. Neural Netw. Learn. Syst.
(2023)
32. Xu, L., et al.: Multi-modal sequence learning for Alzheimer’s disease progression
prediction with incomplete variable-length longitudinal data. Med. Image Anal.
82, 102643 (2022)
33. Yang, F., et al.: Uncertainty-guided transformer reasoning for camouﬂaged object
detection. In: ICCV, pp. 4146–4155 (2021)
34. Zhai, Q., Li, X., Yang, F., Chen, C., Cheng, H., Fan, D.P.: Mutual graph learning
for camouﬂaged object detection. In: CVPR, pp. 12997–13007 (2021)
35. Zhang, R., et al.: Lesion-aware dynamic kernel for polyp segmentation. In: Wang,
L., Dou, Q., Fletcher, P.T., Speidel, S., Li, S. (eds.) Medical Image Computing
and Computer Assisted Intervention - MICCAI 2022. Lecture Notes in Computer
Science, vol. 13433, pp. 99–109. Springer, Cham (2022). https://doi.org/10.1007/
978-3-031-16437-8 10
36. Zhao, J.X., Liu, J.J., Fan, D.P., Cao, Y., Yang, J., Cheng, M.M.: EGNet: edge
guidance network for salient object detection. In: ICCV, pp. 8779–8788 (2019)
37. Zhao, T., Wu, X.: Pyramid feature attention network for saliency detection. In:
CVPR, pp. 3085–3094 (2019)
38. Zhao, X., et al.: M2SNet: multi-scale in multi-scale subtraction network for medical
image segmentation. IEEE Trans. Med. Imag. (2023)
39. Zhao, X., Zhang, L., Lu, H.: Automatic polyp segmentation via multi-scale subtrac-
tion network. In: de Bruijne, M., et al. (eds.) MICCAI 2021. LNCS, vol. 12901, pp.
120–130. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-87193-2 12

ViT-MPI: Vision Transformer Multiplane
Images for Surgical Single-View View
Synthesis
Chenming Han1
, Ruizhi Shao2
, Gaochang Wu1(B)
, Hang Shao3
,
and Yebin Liu2
1 State Key Laboratory of Synthetical Automation for Process Industries,
Northeastern University, Shengyang, China
wugc@mail.neu.edu.cn
2 Department of Automation, Tsinghua University, Beijing, China
3 Zhejiang Future Technology Institute, Jiaxing, China
Abstract. In this paper, we explore the use of a single imaging device
to acquire immersive 3D perception in endoscopic surgery. To solve the
heavily ill-posed problem caused by the unknown depth and unseen
occlusion, we introduce a Vision Transformer (ViT)-based Multiplane
Images (MPI) representation, termed as ViT-MPI, for the continuous
novel view synthesis using single-view input. The MPI representation
provides layered depth images to explicitly decode positional relation-
ships between tissues. Instead of using the existing full convolutional
network as the backbone of our MPI representation, we exploit the ViT
architecture to collect tokens output from all stages of the transformer
and combine them into feature representations with diﬀerent resolutions.
The interactions between tokens in the ViT provide accurate predictions
of local and global positional relations, ensuring reliable view synthesis of
occluded regions with ﬁne-grained details. Experiments on real-captured
endoscopic surgery images from the da Vinci Surgical Robot System
demonstrate that our proposed approach enables the prediction of multi-
view images from a single-view input. Moreover, our method produces
reasonable depth maps, further enhancing its practical applicability.
Keywords: View synthesis · Vision transformer · MPI
representation · Endoscopic surgery
1
Introduction
Stereo endoscopy provides surgeons with immersive 3D perception, enabling
complex surgical procedures in minimally invasive surgery. Such systems, for
example, the Da Vinci surgical robot system [7], have achieved a great success
in general surgery, urologic surgery, cardiac surgery, etc. The system captures
binocular video with a certain baseline, thus enabling surgeons to construct 3D
information in their mind while watching the binocular video.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 28–40, 2024.
https://doi.org/10.1007/978-981-99-8850-1_3

ViT-MPI for Surgical Single-View View Synthesis
29
However, the multiple imaging devices will inevitably lead to a larger surgical
incision and aﬀect the postoperative recovery of patients. Therefore, it is of
great signiﬁcance and necessity to provide 3D experience with a single imaging
device, but it comes with additional challenges. On the one hand, it is diﬃcult
to disentangle 3D appearance of the surgery scene from the rigid movement of
surgical instruments and the non-rigid deformation of tissues during the surgical
process, simply by using conventional Structure from Motion (SfM). On the
other hand, predicting depth information using a single image can be a heavily
ill-posed problem. Multiple depth solutions can be consistent with a monocular
view.
Recently, researches of depth estimation [6] and view synthesis [27] using
single view input based on the deep learning technique [12] have attracted lots
of attentions, making it possible to generate 3D vision from monocular input in
surgical scenes. To overcome the ill-posed problem described above, these meth-
ods learn to predict positional relationships between objects in the scene rather
than simply memorizing the training data, achieving generalization to unseen
scenarios [11]. Among these methods, the Multiplane Images (MPIs) [20,30,31]
is a more sophisticated learning-based 3D representation. It decomposes the
input view(s) into multiple layered depth images, which can be further applied
to rendering novel views or depth maps.
In this paper, we set our sights on the MPI representation to promote single
image into immersive 3D vision for endoscopic surgery. In endoscopic applica-
tions, global contextual information may be particularly important because key
structures (such as lesions) in endoscopic images may be located at any location
in the image, and their reconstruction may require understanding the context
of the entire image. Therefore, for better learning the positional relationships
between tissues, a Vision Transformer (ViT) [3] backbone is adopted to predict
the layered depth images as well as a background image for inpainting occluded
regions. We term the proposed method ViT-MPI. The ViT-MPI ﬁrst divides the
input image into blocks and maps them into tokens, and then uses a multi-head
attention mechanism to extract the local and global positional relations. Finally,
a multiscale convolutional decoder is applied to fuse features of transformer lay-
ers from coarse to ﬁne. The network is trained through end-to-end optimization
using the perception loss [10] to prevent blurry artifacts caused by the mean
square error loss. Experiments on dataset from a da Vinci surgical robot system
demonstrates that the proposed ViT-MPI is visually and quantitatively superior
to existing single-view view synthesis methods.
The contributions can be summarized as follows:
– An end-to-end solution for continuous novel view synthesis for endoscopic
surgery.
– A ViT-based MPI representation for accurately predicting local and global
positional relations, ensuring reliable view synthesis for the heavily ill-posed
problem.
– Both visual and quantitative results demonstrate the performance of the pro-
posed ViT-MPI.

30
C. Han et al.
2
Related Work
Traditional View Synthesis Methods. Traditional view synthesis methods
mostly focus on interpolation of densely-sampled views, such as light ﬁeld render-
ing [28,29], or reconstructing scene geometry from sparse views [8]. In addition,
some methods involving local geometric proxies [9,15,32] have been explored.
Although the ﬁnal results of these methods are often satisfactory, this kind of
method is only applicable to the input of multiple views.
Learning-Based View Synthesis Methods. The main advantage of learn-
ing-based methods lies in their capability to learn and generalize patterns from
training data, which enables them to handle diverse and complex scenarios.
Flynn et al. [5] introduced a deep learning approach to create new scene views
from sparse input views, using a deep Convolutional Neural Network (CNN) to
generate novel view pixel colors. Srinivasan et al. [21] introduced an innovative
deep-learning algorithm employing light ﬁelds, which turns a 2D RGB image
into a synthesized 4D light ﬁeld. These two methods compute a representation
in the coordinate of target views, requiring the execution of the trained network
for each target view, thereby posing real-time rendering as a hurdle. In contrast,
our approach computes the scene representation once and leverages it to produce
a multitude of output views in real time.
Other techniques predict a solitary scene representation, allowing the render-
ing of multiple output views. Take, for example, layered depth images are ini-
tially conceived to tackle stereo-matching issues [23], they have recently gained
traction in learning view interpolation and extrapolation as MPI representation
[4,14,20,30]. In addition, Tucker et al. [24] extended these methods so that they
can predict an MPI from a single input image.
Model Architecture. Full convolutional networks [13,17] have been widely
employed for view synthesis task, and are often considered as a standard archi-
tecture. Modern architectures simultaneously handle high-resolution and lower-
resolution representations throughout the network [22,26], adeptly capturing
details at multiple scales, vital for realistic view synthesis.
While view synthesis has signiﬁcantly beneﬁted from convolutional networks,
attention-based mechanisms, particularly transformers [25], have emerged as a
powerful alternative. Originally conceived for natural language processing (NLP)
tasks [2], transformers utilize a set-to-set operation paradigm built upon the self-
attention mechanism. Several studies [1,18] have begun adapting these attention
mechanisms for image-related tasks, including view synthesis. Intriguingly, it has
been demonstrated that transformer architectures, originally successful in NLP,
can also yield comparable performance in image-related tasks including image
classiﬁcation [3]. Therefore, our method applies the vision transformer as the
backbone to predict an MPI, using only multiple views as supervision.

ViT-MPI for Surgical Single-View View Synthesis
31
Source image
Target image
Reassemble-12
Reassemble-6
Reassemble-48
Reassemble-24
Fusion
Fusion
Fusion
Fusion
MPI representation
Target prediction
Disparity
Perception loss
Transformer 
Transformer
Transformer
Transformer
Decoder
Encoder
Reconstructor
Patch 
Warping
Fig. 1. Model overview. Our network based on ViT backbone generates an MPI from
a single image input. The input image is decomposed into patches and ﬂattened, fol-
lowed by a linear projection and position embedding to obtain a set of tokens (white).
A readout token (red) that is not grounded in the input image is added to aggregate
global information. All tokens interact in pairs through multiple transformer stages.
Our decoder decodes tokens from diﬀerent stages into an MPI representation for sub-
sequent rendering operations. (Color ﬁgure online)
3
Approach
3.1
MPI Representation and Rendering Using Single-View
We apply MPI [30] as a surgical scene representation with single-view input,
and further to the rendering of multi-view images. We use MPI representation
because of their realistic rendering of novel views and their ability to predict
occluded elements, producing reliable stereo-visual eﬀects for surgical proce-
dures. Moreover, in our application of endoscopic scenes, this may help generate
richer and more complete views from a single or small number of images. For
example, it may help doctors better understand and judge the spatial structure
of the lesion area.
An MPI consists of a set of D fronto-parallel planes. Every plane at ﬁxed
depths from a reference camera coordinate frame contains four channels, which
are RGB channels and α channel. Here, we use ci for the color and αi for the
alpha channels of layer i. Then the MPI can be considered as a set of RGBA
layers {(c1, α1), . . . . . . , (cD, αD)}.
In this paper, we use a network f to learn the MPI representation. Consider
an image Is at viewpoint vs is the input image of our network, then the network
f outputs are a set of RGBA layers {(c1, α1), . . . . . . , (cD, αD)} of the image Is
at viewpoint vs:
f : Is →{(cs
1, αs
1), . . . , (cs
D, αs
D)}.
(1)
Each layer’s color is set by a blend of foreground (input image Is) and back-
ground Ib (predicted) following Zhou et al. [30]. From the source viewpoint, we
consider a point pixel to be dominated by the foreground pixel if it is visible,
and conversely, if it is not visible, then the pixel is dominated by the background
pixel. We use 1 −α as the visibility for each pixel also called opacity, and the

32
C. Han et al.
visibility indicates how much each pixel is occluded by its corresponding pixel in
front of it (i.e., pixels with the same (x, y) position in the subsequent layers). The
blending weight wi for each layer is calculated as the product of the opacities of
all layers that are positioned above it. Therefore, colors can be represented as
follows:
cs
i = wiIs + (1 −wi)Ib,
wi =

j>i
(1 −αs
j).
(2)
We adopt the method of warping each MPI layer to render a novel image.
This warp operation W can transform an image from source viewpoint vs to
target viewpoint vt:
(ct
d, αt
d) = Wvs→vt(δd, (cs
d, αs
d)),
(3)
where δd denotes the amount of warping for the mth plane.
The warping operation W shifts the sampled pixels for each MPI layer from
source viewpoint to the target viewpoint using bilinear interpolation.After the
warping operation, we obtained the MPI {(ct
1, αt
1), . . . . . . , (ct
D, αt
D)} at target
viewpoint vt.
Then the image ˆIt at target viewpoint is composited by the MPI at target
viewpoint vt using the over operation. The over operation uses alpha softly
blending MPI layer at diﬀerent depths. The speciﬁc cover operation is as follows:
ˆIt =
D

i=1
(ct
iαt
i
D

j=i+1
(1 −αt
j)).
(4)
3.2
Vision Transformer Backbone
Our network is based on the Vision Transformer (ViT) as the backbone [16]. We
still use the encoder-decoder structure since view synthesis and depth estimation
are inherently dense prediction tasks. A schematic overview of the complete
architecture is shown in Fig. 1. Please refer to the supplementary materials for
the implementation details.
Transformer Encoder. The Vision Transformer (ViT) encoder converts input
sequence into a ﬁxed vector, preserving spatial resolution via tokens, which cor-
relate to patch positions. Each processed patch is referred to as a token, which
takes on the role of a word [3]. In the ViT encoder, the token count is preserved,
ensuring consistent spatial resolution for each patch across diﬀerent transformer
stages. The Multi-Head Self-Attention (MHSA) of ViT captures varying context
within the sequence, with each attention head computing self-attention indepen-
dently, enabling focus on diﬀerent sequence parts. ViT achieves global receptive-
ness at each stage, unlike convolutional networks due to token interaction.
Furthermore, the ViT accepts position embedding obtained by processing all
non-overlapping patches of size p × p. The patches are initially ﬂattened, then

ViT-MPI for Surgical Single-View View Synthesis
33
linearly projected. To compensate for the loss of spatial information from ﬂat-
tening, we incorporate location details through a position embedding operation.
We also added a special token following the vanilla ViT in [3], which is dubbed
cls token. This token serves as a sequence representation for classiﬁcation task,
aggregating global context information from the entire input sequence. In our
view synthesis task, we refer to this special token as the readout token [16], which
is useful to capture global information.
Project
reshape
Resample-S
Reassemble-S
Fusion
Residual Conv Unit
Residual Conv Unit
Bilinear upsampling
Reconstructor
Conv 3×3
Deconv 3×3
Conv 3×3
Conv 3×3
Fig. 2. Left: We reassemble tokens into feature maps with two spatial dimensions.
Center: The fusion modules continuously upsample feature maps and fuse them in
diﬀerent scales. Right: The reconstructor is designed to convert the feature maps into
the ﬁnal MPI representation.
In the MPI representation, two pixels that are far apart may have the same
disparity, i.e. they lie in the same disparity (depth) plane. The readout token can
perform a self-attention operation with all the other tokens, which can better
help establish a relationship between two pixels that are far apart, whereas other
tokens usually pay more attention to the token information in their vicinity. The
introduction of the readout token helps us construct a more reliable MPI repre-
sentation. Suppose the input image size is H × W pixels, after the processing,

HW
p2 + 1

tokens are obtained. The feature dimension of each token including
the readout token is C.
Convolutional Decoder. Inspired by [16], we apply the convolutional decoder
to our MPI representation task. The decoder collects the token sets output from
all the transformer encoder layers to generate an MPI representation, i.g., RGBα
maps. We ﬁrst use linear mapping to handle the special token (see Fig. 2. (left)).
Speciﬁcally, the readout token is concatenated to all other tokens along the
channel dimension and fused through linear mapping to transmit information to
other tokens. This step maps the tokens into image-like feature representations:
Project : R
 HW
p2 +1

×C →R
HW
p2 ×C.
(5)
Post special token processing, we use patch position data to reposition tokens,
restoring an image-like representation. Speciﬁcally, a reshape operation generates

34
C. Han et al.
feature maps with dimensions of H
p × W
p × C:
Reshape : R
HW
p2 ×C →R
H
p × W
p ×C.
(6)
We then perform spatial resampling on this feature representation and scale it
into a H
s × W
s feature representation with ˆD channels, i.e.:
Resample −s : R
H
p × W
p ×C →R
H
s × W
s × ˆ
D.
(7)
After the reshape operation, we map the feature representation to the ˆD dimen-
sion through 1×1 convolution. Then we set s = {48, 24, 12, 6} to resample the
above features for each decoder layer. A strided convolution is used for spatial
downsampling when s ≥p, while a transposed convolution is used for spatial
upsampling when s < p. The deep transformer layer outputs are resampled at a
lower resolution, whereas the shallow ones are at a higher resolution. Through
the reassemble operation, our ViT is able to handle input of diﬀerent resolutions.
Please see the supplementary ﬁle for more details.
Using a residual structure, we sequentially fuse diﬀerent resolution features
over four stages, each time upsampling the input by a factor of two using bilinear
interpolation (see Fig. 2. (center)). The size of feature map by the last fusion
module is 1/3 of the original input image. To generate an MPI representation,
we employ a reconstructor (see Fig. 2. (right)) to upscale these features back to
the initial image size. The transpose convolution in the projection operation is
used for this upsampling, and subsequent convolution layers resize the feature
map’s channel to 34.
The ﬁnal output includes two types of maps, i.e., 31 alpha maps that each
correspond to a disparity (depth) plane, and one background RGB image. Each
type of image is scaled to match its corresponding eﬀective range (e.g. [0, 1]
for alpha images). The 31 alpha channels of the output give us α2, . . . , αM. The
back layer is always opaque, so α1 = 1 and need not be output from the network.
4
Experiments
We evaluate and demonstrate our used method on a real-captured surgical
dataset by the da Vinci Surgical Robot System. We use PSNR, SSIM and LPIPS
as evaluation metrics for quantitative comparisons.
4.1
Dataset and Training Objective
We evaluate our proposed method on the endoscopic stereo video clip of the
distal gastric wall. We extracted a total of 2 clips, comprising 2295 frames of left
and right views, with a spatial resolution of 624 × 480 for each view. We use the
left view for the network input and the right view for the target prediction, i.e.,
label. We divided the dataset into a training set of 2254 frames and a test set
of 41 frames. During the training, patches with a spatial size of 384 × 384 are

ViT-MPI for Surgical Single-View View Synthesis
35
extracted from the images, constructing 9016 training samples. We conduct an
ablation study on our ViT modules through qualitative comparison.
For the training objective, we apply the perception loss [10] using the pre-
trained VGG-19 [19] to prevent blurry artifacts caused by the mean square error
loss. More implementation details can be referred in the supplementary ﬁle.
4.2
Comparison
We compare two State-Of-The-Art (SOTA) MPI-based view synthesis methods
by Tucker et al. [24] and Zhou et al. [30]. These two methods apply U-net
backbones. For a fair comparison, we use the U-net network of the roughly
equal parameters to our ViT backbone and retrain them on the same surgical
training set.
Fig. 3. Visual comparison of the synthesized results at target viewpoint with Zhou et
al. [30] and Tucker et al. [24]. The results also demonstrate temporal changes within
the surgical scene.
Figures 3 and 4 illustrates the qualitative results of the view synthesis of our
method and the compared method. In the cases of Figs. 3 and 4, when rendering
some ﬁner blood vessels, there may be artifacts such as blurring and interruption
of blood vessels. However, our method can capture global information to create
a more reliable MPI representation, which alleviates these problems to a certain

36
C. Han et al.
Fig. 4. Visual comparison of the synthesized results around regions with subtle tex-
tures. The comparison underscores the ability of each method to capture, retain, and
reproduce the subtle textures integral.
extent. In contrast, the methods by Tucker et al. [24] and Zhou et al. [30] do not
handle these thin structures well.
As shown in Table 1, our model generalizes well on the surgical dataset with-
out any ﬁne-tuning, and our ViT network performs better than Zhou et al.
[30] (0.7892 dB increase in PSNR and 10% LPIPS drop) and Tucker et al. [24]
(0.5209 dB increase in PSNR and 3% LPIPS drop). Since our ViT does not have
these speciﬁc inductive biases as convolutional networks do, the convolutional
networks are not as capable of building a global relationship between two dis-
tant pixels as our ViT model. The ViT learns the relationship between any two
points in an image, independent of spatial distance, which means that it is able
to determine pixels located in the same depth plane more accurately, regardless
of whether they are far apart or close together.
4.3
Ablation Studies
In this section, we present the ablation studies on our loss function, readout
token in the ViT model, and the setting of the number of depth planes.

ViT-MPI for Surgical Single-View View Synthesis
37
Table 1. Quantitative comparison with SOTA methods.
Methods
PSNR ↑
SSIM ↑
LPIPS ↓
Zhou et al. [30]
26.4868±0.4904
0.8712±0.0173
0.1312±0.0355
Tucker et al. [24] 26.7531±0.5569
0.8716±0.0147 0.1218±0.0249
Ours
27.2740±0.4715 0.8599±0.0132
0.1181±0.0293
Table 2. Quantitative comparison with diﬀerent settings of loss functions and numbers
of depth planes in the MPI representation.
PSNR ↑
SSIM ↑
LPIPS ↓
L1 loss
25.8060±1.2192
0.8515±0.0122
0.1947±0.0537
Ours w/o readout token 24.4255±2.7232
0.8537±0.0168
0.1238±0.0174
Ours (D = 8)
25.0054±0.8435
0.7683±0.0296
0.1692±0.0465
Ours (D = 16)
25.4128±0.2761
0.8074±0.0113
0.1432±0.0346
Ours (D = 32)
27.2740± 0.4715 0.8599±0.0132 0.1181±0.0293
Perceptual Loss. The inﬂuence of the perceptual loss is underscored through a
comparative analysis between ViT using perceptual loss within the feature space
and ViT using L1 loss within the RGB pixel space. As demonstrated in Table 2,
the ViT model educated with perceptual loss demonstrates superior performance
of preserving object structure and texture nuances in the view synthesis. Table 2
further provides quantitative evidence of the beneﬁts associated with training
using perceptual loss.
Readout Token in ViT. We present diﬀerent ways of handing the readout
token, with or without adding the readout token as shown in Table 2. From the
results, it can be seen that adding a readout token greatly improves the results.
The readout token can capture global information to build a more reliable MPI
representation and tends to produce sharper results in the synthesized views.
Number of Depth Planes. Table 2 demonstrates that the eﬃcacy of our
model increases with the addition of more depth planes in the computed MPI
representation. At present, we can only utilize 32 planes because of memory
limitations, but this could be surpassed with upcoming hardware advancements
or diﬀerent network designs.
5
Conclusion
We present ViT-MPI, a novel vision synthesis method that combines the spa-
tial geometric advantage of Multiplane Images (MPIs) with the powerful Vision
Transformer (ViT) to provide an eﬀective solution for single-view synthesis for

38
C. Han et al.
surgical scenes. Experiment results show that our ViT-MPI outperforms exist-
ing single-view view synthesis methods in terms of visual quality and numerical
metrics, and demonstrates preponderances in handling complex structures and
nuance textures. Our approach also demonstrates how the Transformer struc-
tures can be eﬀectively applied to the ﬁeld of medical image analysis, opening
up new research directions.
However, despite the remarkable progress of our approach, there are still
some challenges to overcome. For example, for images with extreme perspective
changes or complex backgrounds, our method may need further improvement.
In the future, we will continue to explore more powerful view synthesis methods
and try to apply ViT-MPI to other medical image processing tasks.
Acknowledgment. This work was supported by the Natural Science Foundation of
China under Grants No. 61827805 and No. 62103092, and the Fundamental Research
Funds for Central University under Grants No. N2108001.
References
1. Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V.: Attention augmented convo-
lutional networks. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 3286–3295 (2019)
2. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of
deep bidirectional transformers for language understanding. arXiv preprint:
arXiv:1810.04805 (2018)
3. Dosovitskiy, A., et al.: An image is worth 16x16 words: transformers for image
recognition at scale. arXiv preprint: arXiv:2010.11929 (2020)
4. Flynn, J., et al.: DeepView: view synthesis with learned gradient descent. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 2367–2376 (2019)
5. Flynn, J., Neulander, I., Philbin, J., Snavely, N.: DeepStereo: learning to predict
new views from the world’s imagery. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5515–5524 (2016)
6. Fu, H., Gong, M., Wang, C., Batmanghelich, K., Tao, D.: Deep ordinal regression
network for monocular depth estimation. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 2002–2011 (2018)
7. Giulianotti, P.C., et al.: Robotics in general surgery: personal experience in a large
community hospital. Arch. Surg. 138(7), 777–784 (2003)
8. Hedman, P., Alsisan, S., Szeliski, R., Kopf, J.: Casual 3D photography. ACM Trans.
Graph. (TOG) 36(6), 1–15 (2017)
9. Hedman, P., Philip, J., Price, T., Frahm, J.M., Drettakis, G., Brostow, G.: Deep
blending for free-viewpoint image-based rendering. ACM Trans. Graph. (TOG)
37(6), 1–15 (2018)
10. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer
and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV
2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). https://doi.org/10.
1007/978-3-319-46475-6 43
11. Laina, I., Rupprecht, C., Belagiannis, V., Tombari, F., Navab, N.: Deeper depth
prediction with fully convolutional residual networks. In: 2016 Fourth International
Conference on 3D Vision (3DV), pp. 239–248. IEEE (2016)

ViT-MPI for Surgical Single-View View Synthesis
39
12. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444
(2015)
13. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3431–3440 (2015)
14. Mildenhall, B., et al.: Local light ﬁeld fusion: practical view synthesis with pre-
scriptive sampling guidelines. ACM Trans. Graph. (TOG) 38(4), 1–14 (2019)
15. Penner, E., Zhang, L.: Soft 3D reconstruction for view synthesis. ACM Trans.
Graph. (TOG) 36(6), 1–11 (2017)
16. Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision,
pp. 12179–12188 (2021)
17. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: OverFeat:
integrated recognition, localization and detection using convolutional networks.
arXiv preprint: arXiv:1312.6229 (2013)
18. Shao, R., Wu, G., Zhou, Y., Fu, Y., Fang, L., Liu, Y.: LocalTrans: a multiscale local
transformer network for cross-resolution homography estimation. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision, pp. 14890–14899
(2021)
19. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint: arXiv:1409.1556 (2014)
20. Srinivasan, P.P., Tucker, R., Barron, J.T., Ramamoorthi, R., Ng, R., Snavely, N.:
Pushing the boundaries of view extrapolation with multiplane images. In: Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 175–184 (2019)
21. Srinivasan, P.P., Wang, T., Sreelal, A., Ramamoorthi, R., Ng, R.: Learning to
synthesize a 4D RGBD light ﬁeld from a single image. In: Proceedings of the IEEE
International Conference on Computer Vision, pp. 2243–2251 (2017)
22. Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution representation learn-
ing for human pose estimation. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 5693–5703 (2019)
23. Szeliski, R., Golland, P.: Stereo matching with transparency and matting. Int. J.
Comput. Vision 32(1), 45–61 (1999)
24. Tucker, R., Snavely, N.: Single-view view synthesis with multiplane images. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 551–560 (2020)
25. Vaswani, A., et al.: Attention is all you need. In: Advances in Neural Information
Processing Systems, vol. 30 (2017)
26. Wang, J., et al.: Deep high-resolution representation learning for visual recognition.
IEEE Trans. Pattern Anal. Mach. Intell. 43(10), 3349–3364 (2020)
27. Wiles, O., Gkioxari, G., Szeliski, R., Johnson, J.: SynSin: end-to-end view synthesis
from a single image. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 7467–7477 (2020)
28. Wu, G., Liu, Y., Fang, L., Chai, T.: Revisiting light ﬁeld rendering with deep anti-
aliasing neural network. IEEE Trans. Pattern Anal. Mach. Intell. 44(9), 5430–5444
(2021)
29. Wu, G., Liu, Y., Fang, L., Dai, Q., Chai, T.: Light ﬁeld reconstruction using con-
volutional network on epi and extended applications. IEEE Trans. Pattern Anal.
Mach. Intell. 41(7), 1681–1694 (2018)

40
C. Han et al.
30. Zhou, T., Tucker, R., Flynn, J., Fyﬀe, G., Snavely, N.: Stereo magniﬁcation:
learning view synthesis using multiplane images. arXiv preprint: arXiv:1805.09817
(2018)
31. Zhou, Y., Wu, G., Fu, Y., Li, K., Liu, Y.: Cross-MPI: cross-scale stereo for image
super-resolution using multiplane images. In: Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp. 14842–14851 (2021)
32. Zitnick, C.L., Kang, S.B., Uyttendaele, M., Winder, S., Szeliski, R.: High-quality
video view interpolation using a layered representation. ACM Trans. Graph.
(TOG) 23(3), 600–608 (2004)

Dual-Domain Network for Restoring
Images from Under-Display Cameras
Di Wang, Zhuoran Zheng, and Xiuyi Jia(B)
School of Computer Science and Engineering, Nanjing University of Science and
Technology, Nanjing, China
jiaxy@njust.edu.cn
Abstract. With the increasing popularity of full-screen devices, phone
manufacturers have started placing cameras behind screens to increase
the percentage of the displays. However, this innovative approach, known
as under-display camera (UDC) technology, can lead to certain image
degradations. We introduce the dual-domain network (DNUDC) for
the purpose of UDC image restoration. We decompose the input into
reﬂection and illumination. An amplitude-phase mutual guided block is
designed to reconstruct the reﬂection and recover the attenuated high fre-
quency features present in the frequency domain. Additionally, a multi-
scale hybrid dilated convolution block is also proposed to handle degra-
dations at various scales and capture features with wide spatial exten-
sion. Our approach has been evaluated through extensive experiments,
demonstrating that our model consistently achieves superior results while
employing a relatively small number of parameters.
Keywords: Under-display camera · Retinex · Dual-domain network
1
Introduction
The UDC imaging system has emerged as a crucial technology for full-screen
smartphones. This method enables screen expansion, providing users with a
wider ﬁeld of view and ultimately enhancing the user experience. Popular choices
for displays in this type of imaging systems include T-OLED and P-OLED. How-
ever, there are certain challenges associated with these displays. For instance,
T-OLED images often suﬀer from blur and noise
[17], P-OLED images may
encounter issues such as color shift, low light, and noise. The degradation pro-
cess of UDC images can be simulated by considering the convolution with a point
spread function (PSF), which represents the blurring eﬀect, followed by the addi-
tion of noise. Based on the above modeling, some traditional deconvolution algo-
rithms, such as Wiener Filter [4], can be applied to enhance the UDC images.
Such algorithms rely on pre-estimation of the PSFs, and ﬁnding a PSF with
universal applicability can be challenging. This diﬃculty arises because UDC
systems exhibit greater light diﬀusion compared to ordinary cameras, resulting
in more spatially dispersed PSFs [2]. Consequently, deconvolution-based algo-
rithms often struggle to perform eﬀectively on UDC tasks due to their limited
receptive ﬁeld.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 41–52, 2024.
https://doi.org/10.1007/978-981-99-8850-1_4

42
D. Wang et al.
Fig. 1. (a) Clear image. (b) The amplitude information. (c) The phase information.
(d) The spectrum of the image.
Recently, various computer vision processing models are continually evolving
and advancing [8,13,22,27]. Nonetheless, UDC image restoration poses a greater
level of complexity and challenge. Firstly, previous image-to-image mapping
approaches have diﬃculty specializing the processing of diﬀerent components.
Secondly, some studies incorporate modules commonly utilized in other image
enhancement tasks [23,29], without taking into account the speciﬁc degrada-
tions in UDC Systems. Thirdly, some methods introduce the domain knowledge
of UDC into their models [2,29], which increases the complexity. In this paper,
we introduce a novel DNUDC approach. We adopt a decomposition approach
based on Retinex
[10] to separate the input into illumination and reﬂection.
The frequency domain features contain rich semantic information, as illustrated
in Fig. 1. The amplitude is primarily responsible for encoding lightness and color
information, while the phase provides insights into the inner structure [7], and
the spectrum of the clear image exhibits sharp changes. Based on these obser-
vations, we enhance the spectrum of the reﬂection under the mutual guidance
of amplitude and phase. In addition, it is also necessary to reﬁne the features
in the spatial domain and handle the pixel-level diﬀerences. We thus propose a
multi-scale hybrid dilated convolution block to jointly restore the illumination
and reﬂection. The dilated convolution with large receptive ﬁeld can handle the
degradations that span wide spatial extension. We set multi-scale processing
branches to extract similar pattern information across scales. Our contributions
can be outlined as follows:
– We introduce an innovative DNUDC approach, which can eﬀectively restore
degraded images captured by UDC imaging systems, and DNUDC is
lightweight compared to previous models.
– An amplitude-phase mutual guided (APMG) block is introduced to recon-
struct the reﬂection in the frequency domain, and restore the texture and
color information.
– A multi-scale hybrid dilation convolution (MHDC) block is utilized to jointly
enhance the illumination and reﬂection in the spatial domain.
2
Related Work
2.1
UDC Image Enhancement
Currently, there are relatively few studies on UDC image enhancement, and a
comprehensive review [28] of UDC enhancement approaches has been provided

DNUDC
43
by Zhou et al. Many of the proposed methods improve on the U-Net struc-
ture [15,23] to restore UDC images. In addition, HrishikeshP et al. [6] utilize
dilated convolution and demonstrate its strong competitiveness. Zhou et al. [29]
use an MCIS [16] to capture paired images and design a modiﬁed version of the
U-Net model to recover the degraded images. Nevertheless, the obtained images
from their system can be unrealistic due to the lack of high dynamic range in
the monitors. Feng et al. [2] consider high dynamic range to ensure accurate rep-
resentation of the data, and incorporate the domain knowledge into the model.
This approach may limit the application scenarios and increase the complexity
of the network.
2.2
Retinex-Based Visual Models
Retinex is a color vision model
[10], which models the source image S as a
combination of reﬂection and the illumination. Mathematically, this process can
be modeled as:
S = R ◦I,
(1)
where R refers to the reﬂection map, and contains the physical properties of the
object. R should ideally contain textures and details. I denotes the illumina-
tion map, which encapsulates information regarding the brightness and contrast
resulting from external lighting conditions impacting the structure. ◦represents
the multiplication operation. This theory has been widely used in image enhance-
ment tasks. Fu et al. [3] use reﬂection to enhance the illumination, assuming that
the underlying structure of the illumination remains consistent across diﬀerent
lighting conditions. Guo et al. [5] mine the illumination structure to reﬁne the
enhanced results. Wei et al. [21] propose a framework that incorporates multiple
losses inspired by Retinex theory to facilitate the reconstruction process. These
methods all exploit the structure information in the degraded images, but the
artiﬁcial constraints may destroy the naturalness of the images.
2.3
Frequency Domain Enhancement Methods
Researches on combining frequency domain processing with deep learning have
been continuously raised. Mao et al. [14] connect the real and imaginary parts,
and design a residual module to capture diverse interactions. Lee-Thorp et al. [11]
use Fourier transform to replace the self-attention sublayer within Transform-
ers [19]. Although many approaches use frequency domain information to assist
in image processing, they do not consider the semantic information provided by
phase and amplitude. Yu et al. [24] reconstruct the phase under the guidance of
amplitude, and obtain results with better texture details. Their reconstruction
process is one-way, while we further use the phase information to feed back the
enhancement of the amplitude.

44
D. Wang et al.
Fig. 2. Network structure of DNUDC. (a) is the amplitude-phase mutual guided block
and (b) is the multi-scale hybrid dilation convolution block.
3
Methodology
We now present an overview of the architectural design of DNUDC. The overall
structure is shown in Fig. 2. Given a degraded UDC image, we ﬁrst decompose
it into reﬂection R and illumination I. Next, we perform parallel frequency and
spatial domain reconstruction of the reﬂection and illumination. Finally, the
output is derived by multiplying the outputs of both branches.
3.1
Retinex-Based Image Decomposition
The structure of the decomposition network is depicted within the dashed box
in Fig. 2. It uses multiple convolutional layers with the rectiﬁed linear units
(ReLU) activation function to process the input. In the ﬁnal convolutional layer,
the channels are adjusted to 4, with the ﬁrst three channels being taken as the
reﬂection and the last channel as the illumination. We use the Sigmoid function
to force the pixel intensity to be limited within the range [0,1].
3.2
Amplitude-Phase Mutual Guided Block
The frequency domain of an image encompasses both high-frequency and low-
frequency components. The former contains valuable information pertaining to
edges and ﬁne details, while the latter is the approximate information of the
image. Furthermore, the phase carries more detailed information about the
structure and texture compared to the amplitude. It is less susceptible to noise
interference and contrast distortions, ensuring the preservation of ﬁne-scale fea-
tures. The amplitude capturing the overall intensity and chromatic variations

DNUDC
45
present [7]. We thus propose an amplitude-phase mutual guided (APMG) block
to enhance the UDC images. For the reﬂection R obtained in the previous stage,
APMG block ﬁrst transforms it utilizing the fast Fourier transform (FFT) F(·)
: Rf = F(R) = R + jI, where R refers to the real part, I is the imaginary part.
Then we can get the amplitude A and phase P as follows:
A(p, q) = [R2(p, q) + I2(p, q)]1/2,
(2)
P(p, q) = arctan[ I(p, q)
R(p, q)].
(3)
Then, we reﬁne the amplitude and phase by convolution, and obtain the corre-
sponding residual information:
P
′ = Conv(P),
(4)
Pres = P
′ −P,
(5)
here, Conv denotes the convolution operator, P
′ is the reﬁned phase, Pres is
the residual information. We apply the same process to amplitude to obtain the
reﬁned amplitude A
′ and the corresponding residual information Ares. We use
the amplitude residual to compensate for the variations in color and brightness
that may occur, and the phase residual serves as a guide in restoring the structure
during the reconstruction of amplitude. Since the operations are similar, we only
show the process of amplitude:
PoolA = Softmax(Pooling(Pres)),
(6)
Aout = A
′ ⊙PoolA,
(7)
Rout(p, q) = Aout(p, q)cosPout(p, q),
(8)
Iout(p, q) = Aout(p, q)sinPout(p, q),
(9)
where Pooling(·) refers to the global average pooling, ⊙denotes the element-wise
product. Finally, we transform Rf
′ = Rout + jIout by the inverse fast Fourier
transform (IFFT) to get the reﬂection component R
′ after the amplitude-phase
mutual guided enhancement. We present the degraded and the restored images,
along with the corresponding frequency domain features in Fig. 3. It can be seen
that our network can eﬀectively reconstruct the phase and amplitude, and the
restored spectrum is much clearer.

46
D. Wang et al.
Fig. 3. The input and the output are in the ﬁrst column. The remaining three columns
are amplitude information, phase information, and spectrum, respectively.
Fig. 4. Parameter settings for the original scale HDC block. The kernel size is 7 × 7,
and the dilation rates are 7, 3, and 1, respectively.
3.3
Multi-scale Hybrid Dilation Convolution Block
The structure of MHDC block is shown at the bottom of Fig. 2. We adopt the
multi-scale architecture to reﬁne the reﬂection and the illumination, and the
width and height ratio is maintained during processing ( 2048×1024). We down-
sample the input to 512 × 256 and 256 × 128, respectively. As mentioned in [12],
the conventional dilation convolution has the drawback of sparsely sampling the
feature map, which can result in grid eﬀects and limited representation capacity.
Therefore, we adopt the hybrid dilation convolution strategy [20], and design a
hybrid dilation convolution (HDC) block for each scale. Figure 4 shows the archi-
tectural design of the HDC block at the original scale. For diﬀerent scales, the
kernel size and the dilation rates are set diﬀerently. The kernel size for medium
scale (512 × 256) is 5 × 5, with dilation rates of 5, 3, and 1, respectively. The
parameters for the small scale (256 × 128) are 3 × 3 and 3, 2, 1, respectively.
There are residual connections between each layer in HDC block, and the saw-
tooth dilation rates achieve complete coverage of the receptive ﬁeld. Therefore,
DNUDC eﬀectively expands the receptive ﬁeld without information loss. We
illustrate the visualizations of the input and output of MHDC block in Fig. 5,
our network can enhance the ﬁne-grained color and contrast using the multi-scale
complementary information.

DNUDC
47
Fig. 5. Visualization of the intermediate results. (a) and (c) represent the input and
output of the MHDC module, respectively. (b) and (d) are the corresponding heatmaps.
3.4
Training Loss
During training, DNUDC is trained using three loss functions. The ﬁrst is the
reconstruction loss Lrec:
Lrec = ||Y −ˆY ||1,
(10)
where ˆY is the ground truth and Y is the output. The second is the frequency
domain loss Lfre:
Lfre = ||P(Y ) −P( ˆY )||1 + ||A(Y ) −A( ˆY )||1,
(11)
where P(·) denotes the phase, A(·) is the amplitude. The third is the Retinex
loss to satisfy the fundamental constraints of Retinex theory:
LRet = ||R −ˆR||1 + ||I −ˆI||1 + ||∇I||1,
(12)
where the ﬁrst term is to ensure the reﬂection consistency, the third term aims to
further enhance the smoothness of the illumination by considering the horizontal
and vertical gradients (represented as ∇I). Thus, the total loss L is:
L = Lrec + αLfre + βLRet,
(13)
α is 0.1 and β is 0.01 in this paper.
4
Experiments
4.1
Datasets and Training Procedure
We train and evaluate the performance of the proposed DNUDC on two speciﬁc
datasets: P-OLED and T-OLED [28]. During training, we use the Adam opti-
mizer. Our DNUDC is trained with batch size of 4. We conﬁgure the number
of epochs to be 3000 for the P-OLED and 2000 for T-OLED. We apply a decay
strategy where the learning rate is reduced by a factor of 0.8 every 300 epochs for
the P-OLED and every 200 epochs for T-OLED dataset with an initial learning
rate set to 0.001. This decay schedule helps in ﬁne-tuning the model over the
training process to achieve better convergence and performance.

48
D. Wang et al.
Table 1. Quantitative evaluations on two datasets. The symbol “↑” means that higher
values are preferable, “↓” suggests that lower values are preferred.
Dataset
P-OLED
T-OLED
Method
PSNR↑SSIM↑
LPIPS↓DISTS↓PSNR↑SSIM↑
LPIPS↓DISTS↓PARAM
PDCRN
30.68
0.9256
0.2217
0.2057
35.27
0.9637
0.1455
0.1372
4.7M
DAGF
31.19
0.9429
0.2185
0.1972
35.63
0.9679
0.1428
0.1353
1.1M
DE-UNet
29.49
0.9208
0.2359
0.2248
33.40
0.9456
0.1546
0.1428
8.9M
Restormer 30.23
0.9251
0.2307
0.2125
33.67
0.9538
0.1509
0.1417
25.31M
BNUDC
32.04
0.9560 0.1971
0.1876
36.40
0.9754 0.1362
0.1304
4.6M
DNUDC
31.82
0.9547
0.1926
0.1829
36.12
0.9742
0.1343
0.1298
0.85M
Fig. 6. Test results on P-OLED dataset. Our method eﬀectively restores ﬁne details,
resulting in visually pleasing outcomes.
Fig. 7. Test results on T-OLED dataset.
4.2
Results
We evaluate our DNUDC by conducting a series of experiments on the two
datasets mentioned above. We select several representative methods as our com-
parison: PDCRN [6], DAGF [18], DE-UNet [29], BNUDC [9]. We also compare
our network with the general image restoration model Restormer [25]. Several

DNUDC
49
metrics are employed to assess the performance of DNUDC, including PSNR,
SSIM, LPIPS [26] and DISTS [1]. All the quantitative results are reported in
Table 1, and the corresponding visual comparisons can be found in Fig. 6 and 7.
Evaluation on P-OLED Dataset. According to Table 1, DNUDC has
improved signiﬁcantly compared to other methods, and only BNUDC achieves
performance comparable to ours. DNUDC performs slightly lower in terms of
PSNR and SSIM compared to BNUDC, but shows greater competitiveness in
terms of LPIPS and DISTS, since the two metrics measure the perceptual simi-
larity between textures of two images, and our network speciﬁcally enhances the
texture details for UDC images.
Evaluation on T-OLED Dataset. All the models achieve superior results on
the T-OLED dataset, which can be attributed to the slighter degradations in
the T-OLED images compared to those in the P-OLED images. BNUDC and
our DNUDC still achieve the best performance. PDCRN slightly changes the
original color distribution of the image, DAGF still has blurred texture details,
and the output of DE-UNet is not smooth, with some of the original contrast
information lost during the recovery process.
Table 2. Results of the ablation experiments.
Decom Pha-gudied Amp-guided APMG Block MHDC Block Lfre LRet PSNR
✓
✓
✓
30.84
✓
✓
✓
31.05
✓
✓
30.68
✓
✓
29.84
✓
✓
30.23
✓
✓
✓
31.19
✓
✓
✓
✓
31.46
✓
✓
✓
✓
✓
31.82
4.3
Ablation Study
To showcase and analyze the validity of each module incorporated in DNUDC,
we perform a series of experiments in the following manner:
Ablation Experiment of APMG Block. To verify the validity of APMG, we
perform three ablation experiments. In the ﬁrst ablation experiment we remove
the phase-guided branch. For the second ablation experiment, we remove the
amplitude-guided branch. In the third experiment, we remove the APMG block
entirely.

50
D. Wang et al.
Ablation Experiment of MHDC Block. We remove the MHDC block in
this ablation experiment, and use the normal spatial domain convolution layers
to replace the MHDC block.
Ablation Experiment of Decomposition Module. To verify the validity of
our decomposition strategy, we do not decompose the input image and directly
enhance it in the frequency and spatial domains in parallel with APMG block
and MHDC block.
We also perform ablation experiments on the frequency domain loss Lfre
and Retinex loss LRet. The results shown in Table 2 indicate that each module
of our proposed DNUDC is eﬀective.
Table 3. Results of diﬀerent scale settings in MHDC block.
Medium / Small
PSNR↑SSIM↑
LPIPS↓DISTS↓
768×384 / 512×256 31.89
0.9541
0.1959
0.1906
512×256 / 256×128 31.82
0.9547 0.1926
0.1829
256×128 / 128×64
31.47
0.9440
0.2008
0.1965
128×64
/ 64×32
30.98
0.9329
0.2140
0.2076
Impact of Scale Settings in MHDC Block. To explore the impact of dif-
ferent scale settings, we down-sample the features to diﬀerent resolutions in the
medium-scale branch and the small-scale branch of MHDC block, as shown in
Table 3. The experimental results indicate that DNUDC achieves optimal per-
formance when the medium and small scales are set to 512 × 256 and 256 × 128,
respectively. Importantly, this setup maintains the original width-to-height ratio
of the image, inconsistent aspect ratio may corrupt the spatial structure in the
multi-scale processing.
5
Conclusion
This paper introduces a novel dual-domain network designed speciﬁcally for
the restoration of images captured using UDC imaging systems, and eﬀectively
addresses various degradations present in UDC images. Rather than learning
the nonlinear mapping from the input to the output directly, we decompose the
degraded image into illumination and reﬂection, and subsequently perform par-
allel dual domain enhancement of the components. We introduce the amplitude-
phase mutual guided block to reconstruct the reﬂection, and jointly enhance the
illumination and reﬂection utilizing the multi-scale hybrid dilation convolution
block. Our DNUDC has undergone on various datasets, and the performance
consistently demonstrates its extreme potential across diﬀerent metrics.

DNUDC
51
Acknowledgements. This work was supported by National Natural Science Founda-
tion of China (62176123) and Postgraduate Research & Practice Innovation Program
of Jiangsu Province (KYCX22 0461).
References
1. Ding, K., Ma, K., Wang, S., Simoncelli, E.P.: Image quality assessment: unifying
structure and texture similarity. IEEE Trans. Pattern Anal. Mach. Intell. 44, 2567–
2581 (2022)
2. Feng, R., Li, C., Chen, H., Li, S., Loy, C.C., Gu, J.: Removing diﬀraction image
artifacts in under-display camera via dynamic skip connection network. In: CVPR,
pp. 662–671 (2021)
3. Fu, X., Zeng, D., Huang, Y., Zhang, X.P., Ding, X.: A weighted variational model
for simultaneous reﬂectance and illumination estimation. In: CVPR, pp. 2782–2790
(2016)
4. Goldstein, J.S., Reed, I.S., Scharf, L.L.: A multistage representation of the wiener
ﬁlter based on orthogonal projections. IEEE Trans. Inf. Theory 44, 2943–2959
(1998)
5. Guo, X., Li, Y., Ling, H.: Lime: low-light image enhancement via illumination map
estimation. IEEE Trans. Image Process. 26, 982–993 (2017)
6. HrishikeshP., S., Puthussery, D., Kuriakose, M., Jiji, C.V.: Transform domain pyra-
midal dilated convolution networks for restoration of under display camera images.
In: ECCV, pp. 364–378 (2020)
7. Huang, J., et al.: Deep fourier-based exposure correction network with spatial-
frequency interaction. In: ECCV. vol. 13679, pp. 163–180 (2022)
8. fan Jiang, Y., et al.: Enlightengan: Deep light enhancement without paired super-
vision. IEEE Trans. Image Process. 30, 2340–2349 (2021)
9. Koh, J., Lee, J., Yoon, S.: Bnudc: a two-branched deep neural network for restoring
images from under-display cameras. In: CVPR, pp. 1950–1959 (2022)
10. Land, E.H.: The retinex theory of color vision. Sci. Am. 237, 108–129 (1977)
11. Lee-Thorp, J., Ainslie, J., Eckstein, I., Ontanon, S.: FNet: Mixing tokens with
fourier transforms. arXiv preprint arXiv:2105.03824 (2021)
12. Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W.: Multi-level wavelet-cnn for image
restoration. In: CVPR, pp. 773–782 (2018)
13. Liu, Y., Qin, Z., Anwar, S., Ji, P., Kim, D., Caldwell, S., Gedeon, T.: Invertible
denoising network: a light solution for real noise removal. In: CVPR, pp. 13360–
13369 (2021)
14. Mao, X., Liu, Y., Shen, W., Li, Q., Wang, Y.: Deep residual fourier transformation
for single image deblurring. arXiv preprint arXiv:2111.11745 (2021)
15. Oh, Y., Park, G.Y., Chung, H., Cho, S., Cho, N.I.: Residual dilated u-net with
spatially adaptive normalization for the restoration of under display camera images.
Asia-Paciﬁc Signal and Information Processing Association, pp. 151–157 (2021)
16. Peng, Y., Sun, Q., Dun, X., Wetzstein, G., Heidrich, W., Heide, F.: Learned large
ﬁeld-of-view imaging with thin-plate optics. ACM Trans. Graph. 38, 1–14 (2019)
17. Qin, Z., Yeh, Y.W., Tsai, Y.H., Cheng, W.Y., Huang, Y.P., Shieh, H.P.D.: See-
through image blurring of transparent oled display: diﬀraction analysis and oled
pixel optimization. In: SID Symposium Digest of Technical Papers. vol. 47, pp.
393–396 (2016)
18. Sundar, V., Hegde, S., Kothandaraman, D., Mitra, K.: Deep atrous guided ﬁlter
for image restoration in under display cameras. In: ECCV pp. 379–397 (2020)

52
D. Wang et al.
19. Vaswani, A., et al.: Attention is all you need. arXiv preprint arXiv:1706.03762
(2017)
20. Wang, P., et al.: Understanding convolution for semantic segmentation. In: WACV,
pp. 1451–1460 (2018)
21. Wei, C., Wang, W., Yang, W., Liu, J.: Deep retinex decomposition for low-light
enhancement. arXiv preprint arXiv:1808.04560 (2018)
22. Wu, H., et al.: Contrastive learning for compact single image dehazing. In: CVPR,
pp. 10546–10555 (2021)
23. Yang, Q., Liu, Y., Tang, J., Ku, T.: Residual and dense u-net for under-display
camera restoration. In: ECCV, pp. 398–408 (2020)
24. Yu, H., Zheng, N., Zhou, M., Huang, J., Xiao, Z., Zhao, F.: Frequency and spatial
dual guidance for image dehazing. In: ECCV. Lecture Notes in Computer Science,
vol. 13679, pp. 181–198 (2022)
25. Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H.: Restormer:
eﬃcient transformer for high-resolution image restoration. In: CVPR, pp. 5728–
5739 (2022)
26. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
eﬀectivaeness of deep features as a perceptual metric. In: CVPR, pp. 586–595
(2018)
27. Zheng, Z., et al.: Ultra-high-deﬁnition image dehazing via multi-guided bilateral
learning. In: CVPR, pp. 16185–16194 (2021)
28. Zhou, Y., Kwan, M., Tolentino, K., Emerton, N.: Udc 2020 challenge on image
restoration of under-display camera: Methods and results. In: ECCV, pp. 337–351
(2020)
29. Zhou, Y., Ren, D., Emerton, N., Lim, S., Large, T.A.: Image restoration for under-
display camera. In: CVPR, pp. 9175–9184 (2021)

Sliding Window Detection
and Distance-Based Matching
for Tracking on Gigapixel Images
Yichen Li1, Qiankun Liu1, Xiaoyong Wang1, and Ying Fu1,2(B)
1 MIIT Key Laboratory of Complex -ﬁeld Intelligent Sensing, Beijing Institute of
Technology, Beijing 100081, China
{liyichen,liuqk3,wangxiaoyong,fuying}@bit.edu.cn
2 Yangtze Delta Region Academy of Beijing Institute of Technology, Jiaxing 314019,
China
Abstract. Object detection and tracking are representative tasks in
the ﬁeld of computer vision. Existing methods have achieved commend-
able results on common datasets, yet they struggle to adapt to gigapixel
images that demand higher spatio-temporal resolution and oﬀer a greater
spatial visibility range. In this paper, we propose a novel method for
object detection and tracking dedicated designed for gigapixel images.
Speciﬁcally: 1) We devise a multi-scale sliding window for object detec-
tion, eﬀectively tackling the constraints of hardware conditions and the
wide range of object scales present in the images; 2) We introduce a
region proposal-based dense crowd detection algorithm within the sliding
window, signiﬁcantly enhancing the detection performance in crowded
and occlusion-rich scenes; 3) We propose a distance-based strategy in the
online tracking algorithm, enabling the tracker to maintain high tracking
accuracy and identity consistency. The experimental results demonstrate
that our proposed method signiﬁcantly outperforms the baseline meth-
ods in terms of both detection and tracking performance.
Keywords: Gigapixels · Object detection and tracking · Slide
windows · Dense crowd detection · Online tracking
1
Introduction
Object Detection and Multi-Object Tracking (MOT) are classic tasks in the
ﬁeld of computer vision. Object detection aims to recognize and locate objects
of interest in images or videos using bounding boxes. MOT aims at estimating
the locations of interested objects in the given video while maintaining their
identities consistently. Over the past few decades, thanks to the advancements
in object detection, tracking-by-detection has become the mainstream approach
for MOT task, dividing the MOT task into two stages: detection and tracking.
The existing datasets for object detection and tracking exhibit an inherent
trade-oﬀbetween wide-ﬁeld coverage and high resolution, resulting in a lack of
long-term analysis of crowd activities with clear local details within large-scale
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 53–65, 2024.
https://doi.org/10.1007/978-981-99-8850-1_5

54
Y. Li et al.
Detector
Previous Trajectories
Detection Results
NMS
Apperence
Maching
Multi-scale Sliding Detectors
26753 15052
Tracking Results
Position
Maching
Distance-based Tracking Strategy
Fig. 1. Overall architecture. We divide MOT task into detection and tracking stages.
In the ﬁrst stage, we employ a multi-scale sliding window approach for detection, this
process generates detection results for each window, which are merged through a simple
post-processing step. In the second stage, we propose a distance-based tracking strategy
to associate the detection results with previous trajectories.
spatio-temporal ranges. To address this limitation, the PANDA [28] dataset was
introduced to advance related research in this ﬁeld. Simultaneously, the introduc-
tion of the PANDA [28] dataset poses new challenges to traditional detection
and tracking algorithms: 1) In terms of detection, algorithms are limited by
hardware condition, and they struggle to handle gigapixel images that contain
large-scale spatio-temporal information. In addition, the distribution of objects
in gigapixel images is imbalanced and exhibits signiﬁcant scale variations, mak-
ing object detection diﬃcult; 2) In terms of tracking, fast-moving objects with
a wide range of motion are challenging to capture. These issues are particu-
larly evident in low frame-rate gigapixel image videos, leading to object loss and
identity inconsistency problems in MOT.
In response to the aforementioned diﬃculties faced in gigapixel images, we
propose a tracking-by-detection method to address these challenges. As depicted
in Fig. 1, we use sliding window for images detection, and the detection results are
fed into the distance-based tracker for object trajectory association and match-
ing. Speciﬁcally, we ﬁrstly propose a multi-scale sliding window detection method
to address the issues of insuﬃcient memory and varying object scales, than we
employ a naive non-maximum suppression (NMS) pipeline for post-processing
to avoid redundant predictions. Then we propose a distance-based online track-
ing strategy for associating object trajectories, utilizing the Euclidean distance
to constrain the object trajectories and employing a lightweight online tracking
method. This approach improves tracking accuracy while reducing computa-
tional costs. The contributions of this work are as follows:
– We propose a multi-scale sliding window method and introduce a dense crowd
detection approach for the sliding window, which enables accurate object
detection in gigapixel images.

Detection and Tracking on Gigapixel Images
55
– We propose a distance-based tracking strategy that incorporates both appear-
ance and location information in the data association approach, which is more
suitable for gigapixel image data.
– Experimental results demonstrate that the proposed method signiﬁcantly out-
performs the baseline methods. Particularly, our method exhibits higher recall
and achieves higher accuracy in gigapixel images.
2
Related Work
This section brieﬂy reviews related works from diﬀerent aspects, including region
proposal convolutional detectors and multi-object tracking.
2.1
Region Proposal Convolutional Detectors
The current mainstream methods for object detection can be categorized into
one-stage [12,20,23] and two-stage [4,9,21,22,24,30] approaches. Among them,
the two-stage methods strike a balance between model speed and accuracy, with
Faster R-CNN [24] being the most classic example. It primarily performs object
regression and classiﬁcation based on region proposal networks (RPN) generated
from the image. Subsequent improvements mainly focus on enhancing the per-
formance and accuracy of the RPN. One method involves the introduction of an
attention mechanism, such as CBAM [30], to improve the precision and quality
of region proposals. Another approach is to combine the RPN with other tech-
niques, such as the Cascade R-CNN [4], Libra R-CNN [22], and Grid R-CNN [21],
all of which employ diﬀerent strategies to improve RPN performance.
In the object detection on gigapixel images, to address the issue of compu-
tational eﬃciency associated with sliding window methods, we employ more
lightweight region proposal convolutional detectors. Although the previous
region proposal convolutional detectors [4,9,21,22,24,30] have achieved remark-
able results in the ﬁeld of object detection, existing methods lack research on
scenarios with dense object occlusion. In this paper, we focus on dense crowd
scenes and ultra-high resolution in the gigapixel images. We propose multi-scale
sliding window and introduce a detector speciﬁcally designed for crowded sce-
narios, aiming to address the challenges posed by the gigapixe images.
2.2
Multi-object Tracking
Multi-Object Tracking (MOT) has emerged as a popular research area and
has been dominated by tracking-by-detection paradigm. Existing detection and
tracking methods primarily focus on data association of object detection results,
and can be divided into oﬄine and online methods. Oﬄine methods [13,14,25,26]
process the video in a batch way and can utilize the whole video information
including the future frames to better handle the data association problem. While
online methods [1–3,5,8,29,32] process the video frame-by-frame and generate
trajectories only utilize the positional information and appearance features of

56
Y. Li et al.
the current frame’s objects, resulting in lower computational complexity and
reduced computational requirements compared to oﬄine methods.
Tracking-by-detection methods primarily focus on the data association prob-
lem, and classical approaches often utilize graph-based algorithms to address
such problems, including the Hungarian algorithm, network ﬂow [7,25,31], and
graph multicut [13,14,18,26]. In recent years, with the development of deep
learning, lightweight online tracking methods have garnered increasing atten-
tion. Online methods typically employ the Hungarian algorithm for data asso-
ciation but emphasize joint learning of target detection with useful priors such
as object motion [1,19,34], appearance features [5,17], occlusion maps [17], and
object pose [27], these priors serve as conditions for data association.
Despite the commendable performance achieved by current MOT methods
in speciﬁc scenarios, MOT tasks on gigapixel images are still limited by the
signiﬁcant movement range of objects. In this paper, we address this challenge
by focusing on optimizing the association matching stage of the tracker to achieve
improved tracking performance.
3
Methodology
The overview of the proposed detection and tracking network is shown in Fig. 1.
Its workﬂow follows the conventional approach of tracking-by-detection. In order
to address the issue of memory overﬂow caused by ultra-high resolution, we
introduce a sliding window detection method , incorporating diﬀerent scales of
sliding windows based on the distribution pattern of object scales in the image.
Furthermore, we propose a distance-based tracking strategy, these enhancements
result in improved tracking accuracy and identity consistency.
3.1
Preliminary
Due to the challenge of highly overlapped dense objects in images, previous
methods [4,9,21,22,24,30] have struggled to address this issue eﬀectively. To
tackle this problem, we introduce a dedicated detector called CrowdDet [6],
which performs well in detecting dense crowds and can also generalize to less
crowded scenes. The overall network structure of CrowdDet [6] closely resembles
that of Faster R-CNN [24], comprising a backbone network, a region proposal
network (RPN), and a classiﬁcation-regression network.
Considering that multiple objects can overlap with each other, such as
crowded areas, if each region proposal corresponds to single object, a set of
overlapping region proposals can only be predicted as single object, leading to
severe missed detections. To overcome this, CrowdDet [6] introduces instance
set prediction, wherein each region proposal bi is predicted as a set of K
objects P(bi) rather than a single object. To minimize the distance between
the predicted set and the ground truth, CrowdDet [6] further introduces the
EarthMover′sDistance (EMD) loss as

Detection and Tracking on Gigapixel Images
57
Fig. 2. Multi-scale Slide Window. (a) Distribution of object scales in gigapixel images.
(b) Illustration of four scale sliding windows (Including the original image).
L(bi) = min
π∈Π
K

k=1
[Lcls(c(k)
i
, gπk) + Lreg(l(k)
i
, gπk)],
(1)
where π represents a certain permutation of (1, 2, ..., K) whose kth item is πk;
gπk is the πk-th groundtruth box; Lcls(·) and Lreg(·) are classiﬁcation loss and
box regression loss respectively.
To mitigate the risk of duplicate predictions, CrowdDet [6] introduces two
post-processing modules. The Set NMS is proposed to ﬁlter out duplicate pre-
dictions from diﬀerent region proposals. Additionally, an optional Reﬁnement
module is incorporated in the network to eliminate duplicate predictions within
the same region proposal. These post-processing modules aim to improve the
precision and eﬃciency of the detection results by reducing duplicate outputs.
3.2
Multi-sclae Sliding Window
Diﬀerent from conventional object detection, we are unable to directly input
gigapixel images into the network due to computational limitations while resizing
the images to ﬁt the network’s computation scale would result in a signiﬁcant
loss of image information. To address this challenge, we propose a multi-scale
sliding window detection method.
In addition, the wide-ﬁeld and ultra-high resolution also introduce signiﬁcant
scale variations in the objects, as depicted in Fig. 2(a). We have conducted an
analysis of the object scale distribution in the PANDA dataset. The majority
of objects are distributed within the range of 100×100 to 3000 × 3000 pixels.
The largest objects in terms of scale can reach up to 3926×5068 pixels, while
the smallest objects have dimensions as small as 116 × 155 pixels. Based on
the aforementioned distribution pattern of object scales, we have designed four
diﬀerent scales of sliding windows, as shown in Fig. 2(b). The sliding windows
are set to 2000 × 1200, 6000 × 3600, 10000 × 6000, and the original image size

58
Y. Li et al.
Fig. 3. Failure cases of IoU distance matching. The image shows the results of trajec-
tory association using Euclidean distance (bottom row) and IoU distance (top row).
When object (red boxes) undergo large-scale movements, the IoU distance matching
fails, while the Euclidean distance provides a better solution to this problem. The dif-
ferent colored bounding boxes in the image represent the trajectory IDs of the objects,
and a color change indicates a switch in object identity. (Color ﬁgure online)
respectively. This selection ensures that sliding windows of diﬀerent scales can
encompass complete objects of various sizes.
Furthermore, only using small-scale window to traverse the entire image, it
would result in many sub-images without any objects and longer processing time.
Hence, considering the depth variations in gigapixel images, we adopt a depth-
based strategy, utilizing small-scale sliding windows in regions with higher depth
and large-scale sliding windows in regions with lower depth. This approach aims
to strike a balance between eﬃciency and accuracy while preserving the detailed
features of the image to the maximum extent. Finally, we merge all the detection
results from the sliding windows and perform post-processing with naive NMS
to eliminate duplicate objects across diﬀerent sliding windows.
3.3
Distance-Based Tracking Strategy
In online tracking algorithms, the association matching of object trajectories
involves computing the aﬃnity matrix between diﬀerent objects and setting an
aﬃnity threshold to prevent incorrect associations. In this paper, we propose a
distance-based tracking strategy, where trajectory association is performed by
appearance features and positional information of the objects under the con-
straint of Euclidean distance threshold.
We have found that Euclidean distance allows for better handling of high-
speed and large-scale object movements, therefore, we propose a distance-based
two-stage matching approach. In the ﬁrst stage, we assign detection results to
existing trajectories by appearance features. Considering the possibility of large-
scale object movements, we employ Euclidean distance as a constraint instead
of IoU distance. This approach not only reduces computational complexity but

Detection and Tracking on Gigapixel Images
59
also improves tracking performance compared to global appearance matching.
To mitigate the risk of appearance matching failures, we introduce a second
stage of positional information matching, where potential objects are classiﬁed
as unconﬁrmed trajectories by Euclidean distance. The computation is
DEuclidean =

(xtrack −xdet)2 + (ytrack −ydet)2
Ddiag
,
(2)
where xtrack and ytrack is the center coordinates of the current trajectory in
the previous frame, while Ddiag denotes its diagonal length. xdet and ydet is the
center coordinates of the detections in current frame. Additionally, we create a
simpliﬁed re-identiﬁcation dataset from PANDA [28], which yields appearance
feature extraction models that are more compatible with gigapixel images in
PANDA [28]. As illustrated in Fig. 3, our distance-based tracking strategy helps
to mitigate issues associated with object loss.
4
Experiment
4.1
Datasets and Metrics
We utilize the PANDA [28] for both training and testing purposes. PANDA [28]
is the ﬁrst gigapixel image dataset, comprises 21 real-world scenes . The videos
in PANDA [28] dataset have a resolution of 25k×14k, and cover an area exceed-
ing 1km2. PANDA [28] provides 15974.6k bounding boxes, 111.8k ﬁne-grained
attribute labels and 12.7k trajectories. We select 20 scenes from the PANDA [28]
and divide them equally into train and test set. Additionally, we ﬁne-tune the
ReID model using pedestrian trajectories from PANDA [28]. Following the set-
ting of Market1501 [33], we select 100 pedestrian trajectories from each scene
in the training data, with a maximum of 25 frames per trajectory, and images
are resized to 128×256 pixels. In total, we obtain approximately 20,000 images,
matching the scale of Market1501 [33]. For model ﬁne-tuning, we evenly split
the cropped images into train and test set.
We adopt the standard metrics of multi-object tracking for evaluation, includ-
ing: Multi-Object Tracking Accuracy (MOTA), Multi-Object Tracking Precision
(MOTP). Some other metrics, including Average Precision with IoU threshold
(AP@0.5 and AP@0.5:0.95) and Average Recall (AR) are also introduced for the
evaluation of object detection.
4.2
Implementation Details
We employ ResNet50 [11] as the backbone network and adopt the FPN [15] with
RoIAlign [10] as the detection framework in CrowdDet [6], utilizing the same
anchor scales as FPN [15]. During the training phase, we split the gigapixel
images into sub-images using the multi-scale sliding window approach described
in Sect. 3.2. We ﬁlter out images without any objects and bounding boxes with
an overlap less than 50% with the ground truth at the edges of the window.

60
Y. Li et al.
Table 1. Detection and Tracking Results Comparison.
Detector
Tracker
AP@0.5↑AP@0.5:0.95↑AR↑
MOTA↑MOTP↑
Faster R-CNN [24]
DeepSORT [29] 50.5%
28.2%
34.7%
25.53%
76.67%
Cascade R-CNN [4] DeepSORT [29] 50.4%
28.1%
35.3%
24.24%
76.31%
RetinaNet [16]
DeepSORT [29] 48.1%
26.2%
32.5%
15.57%
78.0%
Ours(single-scale)
DeepSORT [29] 53.3%
31.4%
39.2%
29.81%
80.17%
Ours
DeepSORT [29] 70.1%
49.7%
54.3% 35.17%
80.33%
Ours
Ours
70.1%
49.7%
54.3% 40.85% 80.42%
Fig. 4. Comparison of detection qualitative results under diﬀerent test scenes in
PANDA [28], We mark the densely crowded scenes with a yellow box and zoom in
to enhance clarity. (Best viewed on screen with zoom)
In the detection experiments, we set the cardinality of the instance set K as
2. The threshold for Set NMS is set to 0.5, while the threshold for naive NMS
is set to 0.7. The conﬁdence threshold for the objects is set to 0.85. During the
training process of the detection model, the batch size is set to 2, and the initial
learning rate is set to 1e10−4. In the tracking experiments, we set the threshold
for appearance aﬃnity to 0.2 and the threshold for position aﬃnity to 0.5. The
detection model is trained for 30 epochs, and the decay factor is set to 0.1 at
the 11th, 20th, and 30th epoch.
4.3
Comparsion with Other Method
We compare our method with the benchmark provided by PANDA [28], which
selects Faster R-CNN [24], Cascade R-CNN [4], and RetinaNet [16] as the detec-
tors and selects DeepSORT [29] as tracker. These detectors employed a single-
scale sliding window approach and are ﬁne-tuned on the PANDA [28] train set
using pre-trained models based on COCO, while evaluation is performed on the

Detection and Tracking on Gigapixel Images
61
test set. Experimental results are shown in Table 1. It can be observed that uti-
lizing CrowdDet [6] for sliding window detection yields signiﬁcant improvements.
The metric AP@0.5 improved by 22%, AP@0.5:0.95 improved by 23.5%, and the
AR improved by 21.8%. Compared to [4,16,24], employing a detector speciﬁcally
designed for dense crowds is undoubtedly more suitable for capturing the local-
ized and dense distribution of objects in gigapixel images. The higher recall rate
also indicates that our method is capable of handling dense occlusion scenarios
more eﬀectively. In terms of the tracking task, the improved tracker outperforms
the original DeepSORT [29], exhibiting a notable increase of 15.32% in MOTA,
demonstrating the eﬀectiveness of our proposed distance-based tracking strategy.
The qualitative detection results are shown in Fig. 4. The top two rows depict
the results using single-scale sliding windows. Compared to Faster R-CNN [24],
CrowdDet! [6] is less aﬀected by crowding and occlusion. However, our proposed
multi-scale sliding window approach is able to locate more interesting objects
with higher precision, indicating the eﬀectiveness of our approach. The qualita-
tive results of MOT are presented in Fig. 5, which further demonstrate that our
method ensures excellent tracking precision and identity consistency in scenarios
with normal object motion and large-scale fast movements.
4.4
Ablation Study
To validate the eﬀectiveness of the multi-scale sliding window approach, we con-
ducted ablation experiments on detection using diﬀerent scales of sliding win-
dows, as shown in Table 2. Using small windows leads to excessively long detec-
tion times and the inability to detect large-scale objects, and even results in the
segmentation of objects by multiple bounding boxes. On the other hand, using
large windows reduces the detection time but sacriﬁces the detection of small-
scale objects. Although using medium-scale windows can locate the majority of
objects, it still fails to handle some extreme cases. Therefore, we combine multi-
scale windows based on image depths, achieving the best detection results while
maintaining a moderate processing time.
Table 2. Ablation experiments on multi-scale sliding window. The ﬁrst four rows rep-
resent single-scale window detection experiments, while the last row represents multi-
scale windows blended according to image depths. Time represents the average process-
ing time on single NVIDIA RTX 3060 GPU, while Window Num denotes the average
number of sliding windows per image.
Window Scale
AP@0.5↑
AR↑
Time(s)
Window Num
2000×1200
45.6%
28.4%
83.77
241
6000×3600
53.3%
31.46%
16.60
43
10000×6000
39.7%
25.1%
19.22
26
origin size
28.4%
22.3%
7.39
1
4 scale(no depth)
65.7%
52.1%
108.68
309
4 scale
70.1%
54.3%
49.8
145

62
Y. Li et al.
Fig. 5. Comparison of visual tracking results between our method and Faster R-
CNN+DeepSORT under diﬀerent testing scenes in PANDA. The box color corresponds
to the object ID. (Best viewed on screen with zoom)
Table 3. Ablation experiments on association matching. Note that we utilize the same
set of detection results and perform tracking experiments by sequentially incorporating
distance-based tracking strategy and ﬁne-tuning of the ReID model.
Positional Info
Appearnce Info
MOTA↑
MOTP↑
Distance
Threshold
ReID model
IoU
0.2
pretrain
35.17%
80.33%
Euclidean
2
pretrain
37.31%
79.99%
Euclidean
1
pretrain
38.01%
80.27%
Euclidean
0.5
pretrain
38.17%
80.35%
Euclidean
0.5
ﬁne tune
40.85%
80.42%
The association matching is an important component of our distance-based
tracking strategy. We conducted ablation experiments on the appearance and
position information mentioned in the Sect. 3.3. The results in Table 3 (row
1st −2th) show that using Euclidean distance for position information asso-
ciation is beneﬁcial for object tracking in gigapixel images compared to using
IoU distance, and Table 3 (row 2nd −4th) show that the optimal threshold for
position information aﬃnity is found to be 0.5. Additionally, the comparison in
Table 3 (row 4th −5th) demonstrates the eﬀectiveness of ﬁne-tuning on the ReID
model.

Detection and Tracking on Gigapixel Images
63
5
Conclusion
In this paper, we focus on object detection and tracking in gigapixel images. We
propose a multi-scale sliding window detection approach and utilize a crowd-
aware detection algorithm as the window detector, eﬀectively addresses the issue
of insuﬃcient GPU memory in current hardware devices, and achieves improved
detection performance in gigapixel images. In terms of tracking, we propose the
distance-based tracking strategy, optimize the aﬃnity calculation in the asso-
ciation matching stage and utilize the Euclidean distance for association con-
straints, resulting in more accurate and stable tracking performance, while sig-
niﬁcantly reduces redundant computations and overheads. Experimental results
demonstrate that our proposed method achieves signiﬁcantly superior tracking
and detection results compared to the baseline methods.
Acknowledgement. This work was supported by National Key R&D Program of
China (2022YFC33 00704), and the National Natural Science Foundation of China
under Grants (62171038, 62171042, and 62088101).
References
1. Bergmann, P., Meinhardt, T., Leal-Taixe, L.: Tracking without bells and whistles.
In: Proceedings of the IEEE International Conference on Computer Vision, pp.
941–951 (2019)
2. Bewley, A., Ge, Z., Ott, L., Ramos, F., Upcroft, B.: Simple online and realtime
tracking. In: Proceedings of the IEEE International Conference on Image Process-
ing, pp. 3464–3468 (2016)
3. Bras´o, G., Leal-Taix´e, L.: Learning a neural solver for multiple object tracking. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 6247–6257 (2020)
4. Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object detec-
tion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 6154–6162 (2018)
5. Chu, P., Ling, H.: Famnet: joint learning of feature, aﬃnity and multi-dimensional
assignment for online multiple object tracking. In: Proceedings of the IEEE Inter-
national Conference on Computer Vision, pp. 6172–6181 (2019)
6. Chu, X., Zheng, A., Zhang, X., Sun, J.: Detection in crowded scenes: one proposal,
multiple predictions. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 12214–12223 (2020)
7. Dehghan, A., Tian, Y., Torr, P.H., Shah, M.: Target identity-aware network ﬂow
for online multiple target tracking. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1146–1154 (2015)
8. Fang, K., Xiang, Y., Li, X., Savarese, S.: Recurrent autoregressive networks for
online multi-object tracking. In: Proceedings of IEEE Winter Conference on Appli-
cations of Computer Vision, pp. 466–475 (2018)
9. Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference on
Computer Vision, pp. 1440–1448 (2015)
10. He, K., Gkioxari, G., Doll´ar, P., Girshick, R.: Mask R-CNN. In: Proceedings of the
IEEE International Conference on Computer Vision, pp. 2961–2969 (2017)

64
Y. Li et al.
11. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778 (2016)
12. Hong, Y., Wei, K., Chen, L., Fu, Y.: Crafting object detection in very low light.
In: Proceedings of the British Machine Vision Conference, p. 3 (2021)
13. Hornakova, A., Henschel, R., Rosenhahn, B., Swoboda, P.: Lifted disjoint paths
with application in multiple object tracking. In: Proceedings of the IEEE Interna-
tional Conference on Machine Learning, pp. 4364–4375 (2020)
14. Keuper, M., Levinkov, E., Bonneel, N., Lavou´e, G., Brox, T., Andres, B.: Eﬃcient
decomposition of image and mesh graphs by lifted multicuts. In: Proceedings of
the IEEE International Conference on Computer Vision, pp. 1751–1759 (2015)
15. Lin, T.Y., Doll´ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramid networks for object detection. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2117–2125 (2017)
16. Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll´ar, P.: Focal loss for dense object
detection. In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 2980–2988 (2017)
17. Liu, Q., et al.: Online multi-object tracking with unsupervised re-identiﬁcation
learning and occlusion estimation. Neurocomputing 483, 333–347 (2022)
18. Liu, Q., Chu, Q., Liu, B., Yu, N.: Gsm: graph similarity model for multi-object
tracking. In: Proceedings of the Twenty-Ninth International Joint Conference on
Artiﬁcial Intelligence, pp. 530–536 (2020)
19. Liu, Q., Liu, B., Wu, Y., Li, W., Yu, N.: Real-time online multi-object tracking in
compressed domain. arXiv preprint arXiv:2204.02081 (2022)
20. Liu, W., et al.: SSD: single shot multibox detector. In: Proceedings of European
Conference on Computer Vision, pp. 21–37 (2016)
21. Lu, X., Li, B., Yue, Y., Li, Q., Yan, J.: Grid r-cnn. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 7363–7372 (2019)
22. Pang, J., Chen, K., Shi, J., Feng, H., Ouyang, W., Lin, D.: Libra r-cnn: Towards
balanced learning for object detection. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 821–830 (2019)
23. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: uniﬁed,
real-time object detection. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 779–788 (2016)
24. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detec-
tion with region proposal networks. In: Proceedings of Advances in Neural Infor-
mation Processing Systems, vol. 28 (2015)
25. Roshan Zamir, A., Dehghan, A., Shah, M.: Gmcp-tracker: global multi-object
tracking using generalized minimum clique graphs. In: Proceedings of European
Conference on Computer Vision, pp. 343–356 (2012)
26. Tang, S., Andriluka, M., Andres, B., Schiele, B.: Multiple people tracking by lifted
multicut and person re-identiﬁcation. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 3539–3548 (2017)
27. Wang, M., Tighe, J., Modolo, D.: Combining detection and tracking for human
pose estimation in videos. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 11088–11096 (2020)
28. Wang, X., et al.: Panda: a gigapixel-level human-centric video dataset. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3268–3278 (2020)

Detection and Tracking on Gigapixel Images
65
29. Wojke, N., Bewley, A., Paulus, D.: Simple online and realtime tracking with a deep
association metric. In: Proceedings of IEEE International Conference on Image
Processing, pp. 3645–3649 (2017)
30. Woo, S., Park, J., Lee, J.Y., Kweon, I.S.: Cbam: convolutional block attention
module. In: Proceedings of European Conference on Computer Vision, pp. 3–19
(2018)
31. Zhang, L., Li, Y., Nevatia, R.: Global data association for multi-object tracking
using network ﬂows. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1–8 (2008)
32. Zhang, Y., et al.: Bytetrack: Multi-object tracking by associating every detection
box. In: Proceedings of European Conference on Computer Vision, pp. 1–21 (2022)
33. Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian, Q.: Scalable person re-
identiﬁcation: A benchmark. In: Proceedings of the IEEE International Conference
on Computer Vision, pp. 1116–1124 (2015)
34. Zhou, X., Koltun, V., Kr¨ahenb¨uhl, P.: Tracking objects as points. In: Proceedings
of European Conference on Computer Vision, pp. 474–490 (2020)

Robust Self-contact Detection Based
on Keypoint Condition
and ControlNet-Based Augmentation
He Zhang1, Jianhui Zhao1, Fan Li1(B), Chao Tan3, and Shuangpeng Sun2
1 Beihang University, Beijing, China
{zhaojianhui,lifan}@buaa.edu.cn
2 Tsinghua University, Beijing, China
3 Weilan Tech Company, Beijing, China
Abstract. Existing
self-contact
detection
methods
have
diﬃculty
detecting dense per-vertex self-contact. Dataset collection for existing
self-contact detection methods is costly and ineﬃcient, as it requires
diﬀerent subjects to mimic the same pose. In this paper, we propose
a generation-to-generalization approach by utilizing ControlNet to aug-
ment existing datasets. Based on that we develop a keypoint-conditioned
neural network that can successfully infer per-vertex self-contact from a
single image. With the extended dataset synthesized by ControlNet, our
network requires only one real subject training data to achieve satis-
factory individual generalization ability. Experiments verify the eﬀec-
tiveness of our proposed method and the improvement of the network’s
generalization with synthetic data.
Keywords: Self-Contact · Generation · Generalization
1
Introduction
Human self-contact plays a crucial role in various applications such as pose
estimation, motion analysis, and behavior understanding. Human body parts
inevitably touch each other in everyday life, yet self-contact has received limited
attention in the 3D vision ﬁeld. For example, current methods for pose detection
only focus on aligning the human body model with the observation, neglecting
the constraints of self-contact on pose ambiguity. As a result, accurately detect-
ing self-contact is of paramount importance.
Self-contact detection faces two main challenges, the ﬁrst being the insuﬃ-
cient availability of datasets. While existing human pose datasets [11,12,15,23]
contain a limited amount of self-contact poses. Furthermore, there are few ded-
icated datasets for self-contact. Attempts to address this issue, Fieraru et al.
[6] proposed HumanSC3D and FlickrSC3D, while Muller et al. [16] developed
3DCP and MTP. Collecting images of diﬀerent people in varied scenes is a time-
consuming task. These datasets still suﬀer from inaccurate labels and limited
diversity.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 66–77, 2024.
https://doi.org/10.1007/978-981-99-8850-1_6

RSCD Based on Keypoint Condition and ControlNet-Based Augmentation
67
With continuous development of AI Generated Content (AIGC), diﬀusion
models have attracted widespread attention recently. Among them, Stable Dif-
fusion [19], an inﬂuential image generation method, can synthesize high-quality
images based on text descriptions. Based on Stable Diﬀusion, Zhang et al. [24]
proposed ControlNet, a neural network structure to control pre-trained large dif-
fusion models to support additional input conditions. The images synthesized by
ControlNet can be conditioned by keypoints, depth map, normal map, edges, seg-
mentation and so on. Hence, we proposed Generation to Generalization (G2G),
which uses ControlNet instead of real subjects to mimic poses.
Constructing the mapping relationship between observations (color/depth
images) and self-contact presents another challenge. Fieraru et al. have proposed
the SCP method [6] for detecting contact region pairs in the human body mesh.
While this method has shown promising results, it is only capable of detecting
contact at a coarse level. Dense self-contact, which is deﬁned on body mesh
vertices, is required to more accurately describe the contact between body parts.
However, dense self-contact is both sparse and high-resolution, making it diﬃcult
to establish a relationship with the image. As such, there is currently no method
that can eﬀectively detect dense self-contact with the required accuracy.
In order to address the aforementioned challenge, we present a novel app-
roach called Robust Self-Contact Detection (RSCD). Our key insight is that
self-contact is intricately linked to human pose. The sparsity and high resolution
properties of self-contact necessitate the use of a high-resolution image feature
space. Therefore, we introduce HR-Net [22] as an image encoder to obtain high-
resolution features. The decoder is conditioned by 2D keypoints to associate
images with self-contact constructs in a high-dimensional feature space. Cooper-
ated with G2G, our method can achieve satisfactory generalization with training
data of only one subject.
In summary, our contributions are:
– We propose G2G, which enhances the generalization of the network on the
limited collected dataset.
– We propose RSCD, a novel network that detects per-vertices self-contact
through keypoints-condition.
2
Related Work
2.1
Contact Detection
In recent years, contact detection has received more and more attention in com-
puter vision. Contact is mainly divided into human-object contact, human-scene
contact, and self-contact.
Human-object contact helps to analyze human behavior by understanding
the interaction between individuals and objects. Gkioxari et al. [7] proposed
detecting related objects while detecting the human body, while Chen et al.
proposed HOT [5] for detecting full-body human-object contact.

68
H. Zhang et al.
Fig. 1. Two examples of HumanSC3d. Each column contains multiview images of the
same subject with the same pose. This dataset asks subjects to mimic the same poses
with self-contact.
Human-scene contact reﬂects the interactions between individuals and their
surrounding environment. In this regard, a series of methods have been proposed
to detect the per-vertex contact, such as POSA [9], BSTRO [11], and HULC
[20]. In pose estimation, leveraging human-scene contact plays a great role in
mitigating penetration and optimizing human position.
Self-contact is an important factor in reducing pose ambiguity, as it is con-
cerned with the intricacies of one’s body language. Unlike the previous meth-
ods mentioned that required input from the surrounding scene, self-contact is
solely focused on the human body itself. To detect self-contact, Fieraru et al.
proposed SCP [6] where the body mesh is divided into several regions to help
identify self-contact region pairs. M¨uller et al. [16] utilized both geodesic and
Euclidean distances to calculate discrete self-contact labels, which they then
used to extend SMPLify into SMPLify-DC to handle poses in self-contact. They
also introduced TUCH, a method of regressing the human pose in self-contact
situations. Together, these advancements in self-contact detection oﬀer a more
comprehensive approach to accurately capturing the nuances of the human pose
within a given environment.
2.2
3D Body Dataset
There are many datasets available for 3D human body modeling, such as
Human3.6m [12], FAUST [3], People-Snapshot [2], AMASS [15], THUman [23],
ZJU-MoCap [18], PROX [8], and RICH [11], etc. However, most of these datasets
lack information on body contacts, which is an important aspect of human body
modeling. To address this issue, Fieraru et al. [6] created HumanSC3D and
FlickrSC3D. The HumanSC3D dataset encompasses a collection of multiview

RSCD Based on Keypoint Condition and ControlNet-Based Augmentation
69
videos featuring 172 distinct human actions. These actions are performed by a
diverse group of three men and three women, as shown in Fig. 1. HumanSC3D
uses 3D body scanners and 3D markers to accurately capture body shape and
pose. They divided the surface of the human body into 75 regions and annotated
self-contact in body part region pairs. However, this method is time-consuming
and labor-intensive. FlickrSC3D collected 969 images from Flickr for further
extension. But this is not able to provide satisfactory ground truth. To get a
more accurate self-contact label, Muller et al. [16] created 3DCP and MTP.
They annotated the per-vertex self-contact label on SMPL-X [17]. 3DCP needs
to collect high-quality 3D scans to register SMPL-X. MTP asked subjects to
mimic the pose as accurately as possible.
These data sets usually require diﬀerent subjects to mimic diﬀerent poses
or high-quality 3D scans, as illustrated in Fig. 1. But even so, the clothing and
scenes in the dataset are still relatively simple.
2.3
Generation for Generalization
A large amount of data is necessary for the model’s generalization. There has
been a growing interest in synthetic data to enhance the eﬃciency and accuracy
of data collection and labeling. Sun et al. presented PersonX [21], which uses
a pedestrian data synthesis engine to create images of individuals with various
backgrounds, viewpoints, lighting conditions, and poses. For semantic segmenta-
tion tasks, SAIL-VOS [10] extracted datasets from GTA-V. Li et al. [13] veriﬁes
the improvement of the generalization ability of autonomous driving by using
generated scenes. In 2022, NVIDIA introduced DRIVE Replicator [1], a tool for
synthesizing autonomous driving data. Nevertheless, the eﬀects of data synthe-
sis and generation on the generalization ability of self-contact detection remain
unclear.
3
Method
3.1
Deﬁnition
Similar to TUCH [16], given a mesh M with vertices MV and two vertices
vi ∈MV and vj ∈MV , their Euclidean distance and geodesic distance are
denoted as Euclid(V m
b , V n
b ) and geo(V m
b , V n
b ), respectively. These two vertices
are in contact if Euclid(V m
b , V n
b ) ≤teuclid and geo(V m
b , V n
b ) ≥tgeo. Otherwise,
these two vertices are not in contact.
We deﬁne self-contact on the SMPL [14] surface and denote self-contact as a
vector C ∈R6890 (6890 is the number of SMPL vertices). When the vertex vi is
in self-contact, its contact label ci is 1, otherwise it is 0.
3.2
Self-contact Data Generation
The diversity of subjects, scenes, and poses within datasets signiﬁcantly restricts
the generalizability of self-contact detection. A majority of self-contact datasets

70
H. Zhang et al.
Fig. 2. ControlNet with OpenPose. ControlNet can generate images with diﬀerent
clothes, diﬀerent scenes, and the same pose as the input image. All results are achieved
with prompts: “diﬀerent clothes, diﬀerent scenes”.
ask diﬀerent subjects to perform the same poses, as seen in HumanSC3D and
MTP. Such datasets present limited variability, as depicted in Fig. 1. Collecting
images of subjects performing distinct actions in dissimilar surroundings is both
labor-intensive and time-consuming. However, the advent of Stable Diﬀusion [19]
and ControlNet [24] oﬀers a viable solution for boosting detection generalization
through data expansion and generation.
Stable Diﬀusion is capable of producing high-quality images from textual
descriptions, while ControlNet enhances this large diﬀusion model through its
ability to regulate pre-trained models based on a variety of input conditions such
as normal maps, edge maps, segmentation maps, depth maps, and keypoints. To
mimic a pose, we utilize the generation model instead of subjects for our task.
More speciﬁcally, ControlNet operates on input images from the existing self-
contact dataset. We employ the keypoints-condition mode to identify keypoints
within the inputted image and generate new images with the same pose. This
methodology allows us to produce a wide array of images depicting diﬀerent
scenes, various subjects, and the same pose, as illustrated in Fig. 2, with signif-
icant reductions in data collection costs as it no longer requires intensive pose
mimicking, as was the case with practices in HumanSC3d and MTP.
To ensure high-quality source images, we have opted to use the HumanSC3d
dataset. As ControlNet relies on OpenPose detection, the generation of hand
poses may be suboptimal. As such, the generated images tend to better preserve
limb contact semantics but may fall short in the preservation of ﬁnger contact.
To ensure ample resources for our task, we generated over 10k+ images and
further curated a set of 2,000 images that showcased superior contact semantics,
as shown in Fig. 2.

RSCD Based on Keypoint Condition and ControlNet-Based Augmentation
71
Fig. 3. Pipline. Given an input image, we ﬁrst detect keypoints by OpenPose. Sub-
sequently, the image and the keypoints are encoded by HR-Net and a convolutional
neural network (CNN). The features extracted from the image and keypoints are fused.
⊕is a concatenation operator. Finally, a multilayer perceptron (MLP) is employed to
predict dense per-vertex contact.
3.3
Method: RSCD
In this section, we proposed a novel keypoint-conditioned self-contact detection
method called RSCD. The self-contact region exclusively exists in the invisible
area and has a robust correlation with the human body’s keypoint position. We
assert that these correlations can be learned implicitly by the network. We aim
to predict the self-contact vector C from a single image I.
As the self-contact is sparse, high-resolution features are essential for per-
vertex contact label prediction. Therefore, we employ HR-Net [22] as the image
encoder N1. The visual features f1 are extracted from the input image I by
network N1. The human body’s 2D keypoints depict the body’s posture to a
certain degree and have a strong association with self-contact. Thus, we detected
the keypoints kp using OpenPose [4]. Network N2 extracts feature f2 from kp.
By utilizing f1 and f2, an MLP is employed to predict the per-vertex self-contact
label vector C ∈R6890. We use the UV map of SMPL to mark the self-contact
more clearly. The pipeline is illustrated in Fig. 3.
We train our pipeline using the mean squared error loss (MSE).
4
Experiments
4.1
Implementation Details
For network training, the whole pipeline is implemented with PyTorch. We
trained the networks on two RTX3090 for 1400 epochs. The optimizer is Adam.
The initial learning rate is 1e-4, which decreases every 200 epochs. The image
encoder is initialized with pre-trained HR-Net weights.

72
H. Zhang et al.
Fig. 4. Results of unseen subjects. We trained on only one subject and generated
images and tested on unseen subjects to evaluate the generalization. The two images
in the ﬁrst row are from HumanSC3D. The two images in the second row are real-world
data.
We ﬁnally generated a total of 2662 images of 30 poses as the generation
dataset. Due to the generative capability of ControlNet, the number of images
for each pose is not the same. We selected 510 multi-view images of subject s06
in the HumanSC3D. The image in Fig. 3 shows one of the examples of subject
s06. We merge these data with the generation dataset as the training set. We test
the generalization of the network on subjects s01, s02, and s03 in HumanSC3D.
In addition, we also did the test on real data.
We chose F1, recall, and precision as the quantitative metrics for evaluation.
However, most body vertices do not have contact, and their labels are assigned as
0, the network may still produce relatively high scores even when contact is not
detected. Hence we have also incorporated the contact area IOU as an additional
quantitative metric, as it can be more representative of contacted area overlap
accuracy.
4.2
Results
For our task, we only need images of one subject (s06 of HumanSC3D). Then
ControlNet is applied to generate images with the same pose but diﬀerent clothes
and scenes. Our network is trained on the above images. As no prior approach has
been found capable of detecting dense self-contact, our method was compared
solely with the ground truth. The outcomes for previously unseen subjects are
depicted in Fig. 4. Images in Fig. 4 include other subjects in HumanSC3D and
real captured data. This ﬁgure serves as evidence that our algorithm demon-
strates strong generalization across diﬀerent individuals, regardless of cloth-
ing color or background environment. For further qualitative and quantitative
results, please refer to the following sections.

RSCD Based on Keypoint Condition and ControlNet-Based Augmentation
73
4.3
Evaluation on Keypoint Condition
We present an evaluation of the impact of the keypoint condition on our method.
The qualitative and quantitative results are shown in Fig. 5 and Table 1, respec-
tively. Table 1 shows that with keypoint condition, all four metrics achieve better
performances. In particular, the IOU metric achieves a quite signiﬁcant improve-
ment, which demonstrates that the contacted areas overlapped with the ground
truth more accurately after keypoint conditioning. Moreover, this improvement
can also be illustrated with several typical cases in Fig. 5. In which, we highlight
the failed results without keypoint conditions with blue circles. This improve-
ment can be attributed to the fact that self-contact is closely related to human
poses, and the keypoint condition enables the construction of the relationship
between the input image and the pose. The supervision of key points makes it
easier for the network to ﬁt onto training data, and it also plays an important
role in helping network generalization.
Table 1. Quantitative comparison. Evaluation on keypoint condition.
Metrics
Overﬁtting on training data
Generalization
Precision Recall
F1
IOU
Precision Recall
F1
IOU
W/o
0.9997
0.9998
0.9997
0.9887
0.9646
0.9610
0.9593
0.6637
W (RSCD) 0.9999
0.9999 0.9999 0.9928 0.9753
0.9634 0.9682 0.7251
5
Ablation Studies
5.1
Generation to Generalization
In this subsection, we discuss the impact of the generation dataset. We compared
ﬁve diﬀerent proportions:0%, 25%, 50%, 75%, 100%. Figure 6 shows the quantita-
tive results of diﬀerent proportions. With the increase of generation data, the
growth of F1, recall, precision, and IOU curves gradually slows down. With the
increase of generation data, the values of F1, recall, precision, and IOU generally
grow. Especially for IOU, when the generated data is not included at all (0%),
the IOU value is approximately 0.2, which means the network hardly has any
generalization performance. But it is boosted to 0.6 after the ﬁrst 25% gener-
ated data is included. As the generated data ratio increases, the generalization
becomes more and more eﬀective. The same evidence can be also demonstrated
in Fig. 7.
In our experiment, we found that by utilizing generated data, we were able to
improve the generalization of the network when given images of only one subject.
The experiment veriﬁed the eﬀectiveness of generated data in improving the
network’s generalization capabilities. G2G makes it possible for us to improve
network generalization under limited data.

74
H. Zhang et al.
Fig. 5. Qualitative comparison. Evaluation on keypoint condition.
Fig. 6. Quantitative results. Ablation on generation dataset proportion. (a)–(d) repre-
sents F1, recall, precision, and IOU, respectively.
6
Limitations
Motion blur signiﬁcantly impacts the accuracy of OpenPose keypoint detection,
subsequently inﬂuencing the outcomes of our approach. Furthermore, it is imper-

RSCD Based on Keypoint Condition and ControlNet-Based Augmentation
75
Fig. 7. Qualitative results. Ablation on generation dataset proportion.
ative to note that our method is not applicable in scenarios where body parts
are excessively obstructed.
7
Conclusions
In this paper, given the scarcity of datasets, we utilize ControlNet to synthesize
images for augmenting the training data. Additionally, we propose a robust self-
contact detection technique, conditioned on keypoints, for identifying dense per-
vertex self-contact. The generated images signiﬁcantly improve generalization,
enabling our network to perform well even when the training data only contains
data from a single subject. Our experiments demonstrate the eﬃcacy of our
approach, highlighting the beneﬁts of the generated data for generalization. We
anticipate that our method will prove valuable for motion analysis and behavior
recognition.

76
H. Zhang et al.
References
1. https://developer.nvidia.com/zh-cn/drive/drive-sim
2. Alldieck, T., Magnor, M., Xu, W., Theobalt, C., Pons-Moll, G.: Video based recon-
struction of 3d people models. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 8387–8397 (2018)
3. Bogo, F., Romero, J., Loper, M., Black, M.J.: Faust: dataset and evaluation for
3d mesh registration. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 3794–3801 (2014). https://doi.org/10.1109/
CVPR.2014.491
4. Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y.: Openpose: realtime multi-
person 2d pose estimation using part aﬃnity ﬁelds. IEEE Trans. Pattern Anal.
Mach. Intell. 43(1), 172–186 (2021)
5. Chen, Y., Dwivedi, S.K., Black, M.J., Tzionas, D.: Detecting human-object contact
in images. arXiv preprint arXiv:2303.03373 pp. 17100–17110 (2023)
6. Fieraru, M., Zanﬁr, M., Oneata, E., Popa, A.I., Olaru, V., Sminchisescu, C.: Learn-
ing complex 3d human self-contact. Proc. AAAI Conf. Artif. Intell. 35, 1343–1351
(2021)
7. Gkioxari, G., Girshick, R., Doll´ar, P., He, K.: Detecting and recognizing human-
object interactions. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 8359–8367 (June 2018)
8. Hassan, M., Choutas, V., Tzionas, D., Black, M.J.: Resolving 3d human pose ambi-
guities with 3d scene constraints. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 2282–2292 (2019)
9. Hassan, M., Ghosh, P., Tesch, J., Tzionas, D., Black, M.J.: Populating 3d scenes by
learning human-scene interaction. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 14708–14718 (2021)
10. Hu, Y.T., Chen, H.S., Hui, K., Huang, J.B., Schwing, A.G.: Sail-vos: semantic
amodal instance level video object segmentation - a synthetic dataset and baselines.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 3100–3110 (June 2019)
11. Huang, C.H.P., et al.: Capturing and inferring dense full-body human-scene con-
tact. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pp. 13274–13285 (Jun 2022)
12. Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6m: large scale
datasets and predictive methods for 3d human sensing in natural environments.
IEEE Trans. Pattern Anal. Mach. Intell. 36(7), 1325–1339 (2014)
13. Li, Q., Peng, Z., Zhang, Q., Qiu, C., Liu, C., Zhou, B.: Improving the gen-
eralization of end-to-end driving through procedural generation. arXiv preprint
arXiv:2012.13681 (2020)
14. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: Smpl: a skinned
multi-person linear model. ACM Trans. Graph. (TOG) 34(6), 248:1–248:16 (2015)
15. Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.: Amass: archive
of motion capture as surface shapes. In: Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pp. 5441–5450 (2019)
16. M¨uller, L., Osman, A.A.A., Tang, S., Huang, C.H.P., Black, M.J.: On self-contact
and human pose. In: Proceedings IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 9990–9999 (Jun 2021)
17. Pavlakos, G., et al.: Expressive body capture: 3d hands, face, and body from a
single image. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 10975–10985 (2019)

RSCD Based on Keypoint Condition and ControlNet-Based Augmentation
77
18. Peng, S., et al.: Neural body: implicit neural representations with structured
latent codes for novel view synthesis of dynamic humans. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9050–
9059 (2021)
19. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diﬀusion models. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10674–
10685 (2022)
20. Shimada, S., Golyanik, V., Li, Z., P´erez, P., Xu, W., Theobalt, C.: Hulc: 3d human
motion capture with pose manifold sampling and dense contact guidance. In: Pro-
ceedings of the European Conference on Computer Vision, pp. 516–533 (Jun 2022)
21. Sun, X., Zheng, L.: Dissecting person re-identiﬁcation from the viewpoint of view-
point. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pp. 608–617 (2019). https://doi.org/10.1109/CVPR.2019.00070
22. Wang, J., et al.: Deep high-resolution representation learning for visual recognition.
IEEE Trans. Pattern Anal. Mach. Intell. 43(10), 3349–3364 (2020)
23. Yu, T., Zheng, Z., Guo, K., Liu, P., Dai, Q., Liu, Y.: Function4d: real-time human
volumetric capture from very sparse consumer RGBD sensors. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
5742–5752 (June 2021)
24. Zhang, L., Agrawala, M.: Adding conditional control to text-to-image diﬀusion
models (2023)

Explicit Composition of Neural Radiance
Fields by Learning an Occlusion Field
Xunsen Sun, Hao Zhu(B), Yuanxun Lu, and Xun Cao
Nanjing University, Nanjing, Suzhou, China
zh@nju.edu.cn
Abstract. The neural radiance ﬁeld (NeRF) is an implicit representa-
tion of the appearance and shape of objects based on neural networks.
While numerous studies have shown the great performance of NeRF on
tasks like free-view synthesis, it remains a challenge to composite mul-
tiple objects depicted by NeRF models. Diﬀerent from previous works
that focus on decomposing separate scenes from a compositional NeRF,
we study how to naturally composite two trained NeRF models without
combined scene images for supervision. Speciﬁcally, we propose a novel
framework that learns an occlusion ﬁeld (OCF) to model the occupancy
property of the object. A dedicated compositional rendering equation
and loss functions are then designed to learn an accurate occlusion ﬁeld.
With the trained occlusion ﬁeld, the source object represented by NeRF
can be explicitly scaled, moved, and merged into the target NeRF. Our
method can synthesize plausible merged results with accurate occlusion
relation and natural transition at occluding boundaries, and even work
for challenging objects like hairs and leaf clusters. Experiments show
that our method is superior to the previous methods, providing a new
solution for the eﬃcient and eﬀective composition of NeRF models.
Keywords: Neural Radiance Fields · Neural Rendering · Composition
1
Introduction
Novel view synthesis from a set of calibrated images is a signiﬁcant task that can
be used in many applications like virtual reality and 3D simulation. Recently,
neural radiance ﬁeld (NeRF) [14] and its variants achieve impressive perfor-
mance in synthesizing high-quality free-view images. NeRF represents the scene
as a continuous volumetric scene function that predicts the volume density and
radiance on the condition of spatial location and viewing direction. The follow-
up works further improve its performance in rendering quality [1,7,25], speed
[3,10,15,21] and feasibility [4,9,19,22,33,34]. Though huge improvements are
made, the approach to merging two trained NeRF models is rarely explored,
preventing NeRF from being used in interactive applications.
The straightforward way to composite two NeRF models is weighted sum-
ming the value of density and color before the volume rendering [11,27,29], or
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 78–90, 2024.
https://doi.org/10.1007/978-981-99-8850-1_7

Explicit Composition of NeRFs by Learning an OCF
79
Fig. 1. Density-sum-based methods generate cloud-like artifacts in composited images
and surface-extraction approaches cannot produce natural transitions at the occluding
boundaries, while our OCF can synthesize photo-realistic transitions at the boundaries.
extracting surface in advance for occlusion check [32]. However, these strate-
gies may lead to unnatural transitions at the edges and cause obvious artifacts
for scenes containing complex occlusion, as shown in Fig. 1. Another strategy is
to train a neural network to combine two NeRF models by learning from the
combined images [11,28,29], while these models require to be trained with the
composited images and cannot be generalized to arbitrary trained NeRF models.
In this paper, we study how to composite two arbitrary pre-trained NeRF
models realistically and eﬃciently. We propose to learn an occlusion ﬁeld (OCF)
to composite two NeRF models without knowing composited images as super-
vision. Given a source and target NeRF model, the MLP-based OCF could be
trained by minimizing the loss function that is composed of a photometric loss,
matting loss, geometric loss, and a regularization term. A novel compositional
rendering equation is presented to render natural and realistic merged images
from the two NeRF inputs and OCF.
Our contributions can be summarized as follows:
– We explore how to composite two arbitrary pre-trained NeRF models realis-
tically without knowing composited images as supervision.
– The occlusion ﬁeld is proposed to model the occupancy property of objects,
which enables high rendering quality for complex objects like hairs and leaves.
– A novel compositional rendering equation and loss functions are designed to
learn an accurate occlusion ﬁeld. The experiments show the eﬀectiveness of
our method on both synthetic scenes and real-captured scenes.
2
Related Work
Neural Implicit 3D Representations: The representative works of neu-
ral implicit 3D representations include signed distance functions (SDF) [8,18],
occupancy ﬁelds [5,13], and neural radiance ﬁeld (NeRF) [1,11,14,30]. The 3D
objects represented by implicit models are diﬃcult to be spatially composite,
while explicit models can easily do so, which is the problem that our paper
focuses on. Next, we will review prior works that enhance the compositional and
disentangling capabilities of NeRF.

80
X. Sun et al.
Compositional NeRF: Compositional NeRF aims to model multiple editable
objects or scenes within one integrated implicit function. Guo et al. [6] proposed
to learn object-centric neural scattering functions based on NeRF, which models
per-object light transport implicitly using lighting- and view-dependent neural
networks. Similarly, Smith et al. [23] proposed a light ﬁeld compositor module
that enables reconstructing the global light ﬁeld from a set of object-centric
light ﬁelds. Yang et al. [28] proposed a two-pathway architecture, in which the
scene and the object are encoded by dual branches, and each standalone object
is conditioned on learnable activation codes. Niemeyer et al. [16] proposed com-
positional generative neural feature ﬁelds to disentangle the objects from the
background by learning from unstructured and unposed image collections via
self-supervised learning. Zhang et al. [31] proposed a recurrent framework that
is trained across scenes and reconstructs a radiance ﬁeld of a large indoor scene
sequentially.
Decompositional NeRF: Decompositional NeRFs focus on parsing diﬀerent
factors inside the scenes separately. Rebain et al. [20] propose to spatially decom-
pose a scene and dedicate smaller networks for each decomposed part, achieving
eﬃcient and GPU-friendly rendering. Tancik et al. [24] proposed to decompose
the scene into individually trained NeRFs so that a city-level scene can be learned
with NeRF. Guo et al. [7] modiﬁed NeRF to model scenes with reﬂections,
which splits a scene into transmitted and reﬂected components and models the
two components with separate neural radiance ﬁelds. To model street scenarios
with moving vehicles, Ost et al. [17] proposed to decompose dynamic scenes into
scene graphs and encode object transformation and radiance to eﬃciently render
novel arrangements and views of the scene. Martin et al. [11] proposed to sepa-
rately render the static scene and transient elements for modeling a high-ﬁdelity
static scene. Similarly in the task of modeling scenes with dynamic objects,
Wu et al. [27] proposed to decompose the targets into the moving objects and
the static background, each of which is represented by separate neural radiance
ﬁelds with one allowing for temporal changes. Yuan et al. [29] presented a self-
supervised framework for tracking and reconstruction of dynamic scenes with
the rigid motion from multi-view RGB videos.
Diﬀerent from previous work, our method focuses on synthesizing photo-
realistic boundaries and complex occlusion when merging two pre-trained NeRF.
An additional occlusion ﬁeld is learned from the pre-trained source NeRF and
helps synthesize anti-aliasing boundaries between the foreground object and the
occluded, and even works for complex objects like hairs and trees. Once the
occlusion ﬁeld is learned, the objects to be merged can be moved, rotated, and
scaled with high rendering quality maintained.
3
Method
3.1
Preliminaries
Neural Radiance Field (NeRF) represents a scene as a continuous volume ﬁeld
with a multilayer perceptron (MLP), which takes a 3D location x = (x, y, z) and

Explicit Composition of NeRFs by Learning an OCF
81
Fig. 2. We plot the distribution of weights wi (solid red curve) and accumulated trans-
mittance Ti (solid green curve) for each sample point along a ray in the original NeRF,
density-sum-based NeRF and our OCF. NeRF predicts sample points with lower den-
sity values at semi-transparent pixels, which causes cloud-like artifacts in the combined
image, while our OCF can eliminate such artifacts and results in high-quality images.
2D viewing direction d = (θ, φ) as input and outputs a volume density σ and a
directional emitted color c = (r, g, b). To compute the color of a certain pixel,
the density σ and radiance of the sample points on the projective ray are ﬁrst
predicted. The projective ray is formulated as r(t) = o + td, emitted from the
camera center o and going through the given pixel on the image plane. The color
ˆC(r) and opacity ˆA(r) of this pixel is estimated by volume rendering equation
with quadrature approximation [12] used:
ˆC(r) =
N

i=1
Tiαici,
ˆA(r) =
N

i
Tiαi,
(1)
where the accumulated transmittance Ti = exp(−i−1
j=1 σjδj); αi = 1 −
exp(−σiδi); δi = ti+1 −ti, which is the interval distance between adjacent sam-
ples. The contribution of each sample point to the ﬁnal accumulated color is
formulated as:
wi = Tiαi.
(2)
To improve the performance of representing high-resolution and complex
scenes, positional encoding is adopted to map the inputs to a higher dimensional
space. The NeRF is trained with the photometric loss:
Lphoto =

r∈R
 ˆC(r) −C(r)

2
2 .
(3)
where R is the set of rays in each batch.
3.2
Occlusion Field
As shown in Fig. 2, previous methods to merge two NeRF models lead to unnat-
ural transitions at the occlusion boundaries. To address this issue, we propose

82
X. Sun et al.
Fig. 3. Our pipeline consists of a pre-trained source NeRF, pre-trained target NeRF,
and occlusion ﬁeld (OCF). The OCF is trained in a self-supervised manner and can
help synthesize the image of the combined scene in the inference phase.
an occlusion ﬁeld (OCF) specialized in modeling an accurate object occupancy
property, which helps disentangle the occlusion relationships when compositing
two NeRFs, as shown in Fig. 3. Given a 3D position x and 2D viewing direction
d, the OCF learns the occlusion value o at a sampled point i:
FO
φ : (x, d) →(oi).
(4)
where the sampling strategy of oi is the same as that in vanilla NeRF.
3.3
Composition Rendering Equation
After obtaining the occlusion value o, a novel compositional rendering equation
will be employed to generate realistic merging renderings. For clarity, we denote
the foreground object as source S and the background object as target T. Their
corresponding NeRF FS
θ and FT
θ could be written as follows:
FS
θ : (x, d) →(σS
i , cS
i ),
(5)
FT
θ : (x, d) →(σT
i , cT
i ),
(6)
where x ∈R3 is the 3D spatial coordinate, and d ∈R2 is the viewing direc-
tion. Diﬀerent from Equation (1), we deﬁne the color of the source NeRF by
compositing the accumulated color ˆ
CS(r) and the background color CS
bg:
ˆCS
f (r) = ˆCS(r) + (1 −ˆAS(r))CS
bg,
(7)
where the accumulated color
ˆ
CS(r) represents the object color and the back-
ground color CS
bg could be simply set as an input or learned from another net-
work [26]. With the help of this disentangled representation, the fused color C(r)
of the composited NeRF could be calculated:

Explicit Composition of NeRFs by Learning an OCF
83
C(r) =
N

i=1
T c
i (αT
i cT
i + αS
i cS
i ),
(8)
where
T c
i = exp(−
i−1

j=1
(σT
j + σS
j oj)δj),
(9)
and wc
i = T c
i (αT
i + αS
i oi) determines the contribution of each sampling point
to the ﬁnal accumulative color of the ray.
3.4
Model Training
The OCF is represented as a multilayer perceptron consisting of 10 fully con-
nected layers. We follow the architecture of NeRF to introduce skip connections
in our network. Positional encoding is also leveraged to map the input of x and
d to a higher dimensional space. We carefully design the loss function in a self-
supervised manner, which consists of four components: photometric loss Lpm,
matting loss Lmat, geometric loss Lgeo, and geometric regularization Lreg.
Photometric Loss. Photometric loss is used to constrain the rendering results
of the updated source NeRF from being changed, which is essential for correctly
compositing the source and target NeRF and rendering realistic images of the
combined scene. The photometric loss is formulated as:
Lpm =

r∈R
 ˆCS
f (r) −CS(r)

2
2 ,
(10)
where CS(r) is the ground truth color of source scene.
Matting Loss. Matting loss is designed to extract the objects from the source
NeRF. The challenge is to disentangle the blurred transition at the occlusion
boundaries, which is requisite to seamlessly composite source NeRF and target
NeRF. Here we introduce occlusion map ˆO(r), which can be rendered based on
the geometry of the source NeRF via volume rendering:
ˆOS(r) =
N

i=1
T S
i αS
i oi.
(11)
Our rendering equation is based on that of vanilla NeRF to incorporate the
geometric information of the occlusion ﬁeld when integrating the color:
ˆCS′(r) =
N

i=1
T o
i αS
i cS
i ,
(12)
where
T o
i = exp(−
i−1

j=1
σS
j ojδj).
(13)

84
X. Sun et al.
Then the matting loss is applied to extract the foreground objects with the
properties of transparency or edge-excessive in the source NeRF model:
Lmat =

r∈R
 ˆCS′(r) −ˆCS(r)

2
2 ,
(14)
where ˆCS(r) is the accumulated color of source NeRF deﬁned in Equation (7).
Geometric Loss. Considering that both OCF and source NeRF contain the
geometric information of the source object, we use their corresponding rendering
equations to obtain an occlusion mask and a source mask respectively. The
geometric loss is formulated as:
Lgeo =

r∈R
 ˆOS(r) −AS(r)

2
2 .
(15)
Occlusion Regularization. We observed that only a small subset of sampled
points along the rendering ray hit objects or background in a scene, while the
rest of the points are mostly useless. And by minimizing the sum of o along
each ray, the useless points are largely suppressed. Based on this observation,
occlusion regularization is introduced and is formulated as:
Lreg =
N

i=1
oi.
(16)
The overall loss function used in our model is given by:
L = Lpm + λgeoLgeo + λmatLmat + λregLreg,
(17)
where λgeo, λmat, λreq are the weight of proposed supervision, respectively.
4
Experiment
4.1
Data Preparation
We generate 5 tuple of synthetic data with various objects and occlusion relation,
including Hair-Black, Hair-Blond, Horse&Saddle, Box&Fruit, Table&Basin. The
data of each tuple contains multi-view images of the source scene, the target
scene, and the composited scene. The resolution of the rendered images is 512 ×
512. And the images are randomly divided into 80% as the training set and 20%
as the testing set. We also captured multi-view images for the scene Tree&Star
to validate our method on real-world objects. By compositing these pre-trained
neural radiance ﬁeld models, we qualitatively and quantitatively evaluate our
method and compare it with previous methods.

Explicit Composition of NeRFs by Learning an OCF
85
4.2
Qualitative Evaluation
As shown in Fig. 4, given a source NeRF model and a target NeRF model, our
method can explicitly composite the two NeRFs with a learned occlusion ﬁeld.
The scenes of Horse&Saddle, Box&Fruit, Table&Basin contain complex occlu-
sion relation, while our method accurately composites the two models and main-
tains the high rendering quality of the original NeRF. The scenes of Hair-Black
and Hair-Blond contain a head and hair, which is extremely challenging to com-
posite. There are noticeable translucency transitions at the boundaries of wisps
of hair, and our method successfully synthesizes them. The scene of Tree&Star
is a real-captured scene, which contains clusters of pine leaves. Our method can
explicitly scale the star-shaped object and put it in the pine leaves cluster. In
all these scenes, our method composite the two input NeRF model with correct
occlusion relation and natural transition at the boundaries. The intermediate
results including the occlusion map and the extracted source object are shown
in the right two columns, which demonstrates that OCF is able to resolve an
accurate occlusion relation. It is worth noting that our method composites the
source and target neural radiance ﬁeld without knowing the image of the merged
scene.
Fig. 4. Visualized results of our method on synthetic and real-captured data.

86
X. Sun et al.
4.3
Comparison
Previous approaches for merging multiple NeRFs fall into two categories:
• (a) Density-Sum Strategy: merging two NeRF models by weighted sum-
ming the density σ, as adopted by NeRF-W [11], STaR [29], D2NeRF [27].
• (b) Surface-Extraction Strategy: to obtain the surface points, then judge
the occlusion relation with the surface-to-camera distances [32].
We quantitatively evaluate the performance of our method and the fusion of
these two categories of methods, as shown in Table 1. Our method outperforms
two previous methods in PSNR, SSIM, and LPIPS, which indicates that our
method composites two NeRFs more accurately. As shown in Fig. 5, the density-
sum strategy tends to synthesize obvious cloud-like artifacts. We consider this
to be due to the fact that the density ﬁeld in NeRF is not suitable for direct
weighted accumulation. Surface-extraction strategy tends to produce unnatural
transitions at the occluding boundaries of the foreground object, which is con-
spicuous in the test of Hair-Black and Hair-Blond. If the multi-view constraints
of the background in the source scene are not enough, a wrong background is
obtained in the combined scene because the surface of the background cannot be
calculated accurately, as shown in Table&Basin. By comparison, our methods
can composite two neural radiance ﬁelds with accurate occlusion relation and
natural transition at occluded boundaries.
4.4
Ablation Studies
To validate the eﬀectiveness of each proposed module, we conduct the experi-
ments with the following settings:
• (a) w/o mat. loss: The matting loss is removed only.
• (b) w/o geo. loss: The geometric loss is removed only.
• (c) w/o reg. term: The occlusion regularization term is removed only.
• (d) w/o view dep: The 2D viewing direction is removed from the input of
OCF.
The results are visualized in Fig. 6 and quantitatively evaluated in Table 1.
Our method without matting loss (a) fails to extract the source object from
source NeRF accurately, resulting in cloud-like artifacts in the merged image
and wrong background in the Table&Basin case. In the result of our method
without geometric loss (b), the source object is somehow transparent with partial
background fused in (see Hair-Black, Box&Fruit case), and the background of the
target component is covered by the source’s background (see Table&Basin case).

Explicit Composition of NeRFs by Learning an OCF
87
Table 1. Quantitative evaluation of comparison experiments.
PSNR↑
Method
Hair-Black Hair-Blond Box&Fruit Horse&Saddle Table&Basin
Density-sum
20.45
17.82
20.53
28.36
6.780
Surface-extraction 22.11
19.22
31.92
13.72
6.478
w/o mat. loss
21.30
19.43
29.39
33.10
7.950
w/o geo. loss
21.89
21.78
29.48
28.82
6.774
w/o reg. term
22.92
21.96
31.54
30.73
25.76
w/o view dep
22.99
22.16
30.60
29.95
25.47
OCF
23.04
22.10
32.32
33.46
26.13
SSIM↑
Method
Hair-Black Hair-Blond Box&Fruit Horse&Saddle Table&Basin
Density-sum
0.899
0.842
0.849
0.966
0.505
Surface-extraction 0.911
0.863
0.960
0.674
0.510
w/o mat. loss
0.907
0.857
0.955
0.978
0.526
w/o geo. loss
0.891
0.879
0.954
0.978
0.529
w/o reg. term
0.917
0.883
0.962
0.977
0.869
w/o view dep
0.915
0.882
0.960
0.972
0.860
OCF
0.918
0.877
0.967
0.981
0.880
LPIPS↓
Method
Hair-Black Hair-Blond Box&Fruit Horse&Saddle Table&Basin
Density-sum
0.242
0.270
0.288
0.226
0.565
Surface-extraction 0.220
0.265
0.157
0.467
0.566
w/o mat. loss
0.211
0.242
0.158
0.131
0.551
w/o geo. loss
0.211
0.239
0.160
0.130
0.555
w/o reg. term
0.193
0.238
0.153
0.133
0.202
w/o view dep
0.246
0.294
0.159
0.140
0.215
OCF
0.203
0.231
0.152
0.128
0.182
Our method without occlusion regularization (c) cannot model the translucent
attributes of the scene accurately. As shown in Hair-Black, the unnatural edge
transition of objects exists in the combined image. Our method without view-
dependency (d) fails to synthesize natural and realistic results, especially at the
edges of objects (see Hair-Black case for detail). By comparison, our complete
method produces realistic rendering results and leads in the scoreboard.

88
X. Sun et al.
Fig. 5. The comparison of density-sum method, surface-extraction method, and ours.
Fig. 6. Qualitative evaluation of ablation study.
5
Conclusion
We propose to composite multiple objects represented by NeRFs by learning
an occlusion ﬁeld. The proposed framework solves accurate occlusion relation
and natural transition at occluding boundaries in a self-supervised manner, and
greatly expands the application scenarios of trained NeRF models. There are still
some limitations to our approach. The merging process still requires a training
phase, which means it cannot be completed instantly. We only verify the per-
formance on vanilla NeRF, and more experiments regarding recent NeRF-based
frameworks like Triplane [2] and InstantNGP [15] should be studied.

Explicit Composition of NeRFs by Learning an OCF
89
Acknowledgement. This work was supported by NSFC grant 62001213, 62025108,
gift funding from Huawei, and Tencent Rhino-Bird Research Program.
References
1. Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srini-
vasan, P.P.: MIP-NERF: a multiscale representation for anti-aliasing neural radi-
ance ﬁelds. In: CVPR, pp. 5855–5864 (2021)
2. Chan, E.R., et al.: Eﬃcient geometry-aware 3d generative adversarial networks.
In: CVPR, pp. 16123–16133 (2022)
3. Deng, K., Liu, A., Zhu, J.Y., Ramanan, D.: Depth-supervised nerf: fewer views
and faster training for free. In: CVPR, pp. 12882–12891 (2022)
4. Gafni, G., Thies, J., Zollhofer, M., Nießner, M.: Dynamic neural radiance ﬁelds for
monocular 4d facial avatar reconstruction. In: CVPR, pp. 8649–8658 (2021)
5. Genova, K., Cole, F., Sud, A., Sarna, A., Funkhouser, T.: Local deep implicit
functions for 3d shape. In: CVPR, pp. 4857–4866 (2020)
6. Guo, M., Fathi, A., Wu, J., Funkhouser, T.: Object-centric neural scene rendering.
arXiv preprint arXiv:2012.08503 (2020)
7. Guo, Y.C., Kang, D., Bao, L., He, Y., Zhang, S.H.: Nerfren: neural radiance ﬁelds
with reﬂections. In: CVPR, pp. 18409–18418 (2022)
8. Jiang, C., Sud, A., Makadia, A., Huang, J., Nießner, M., Funkhouser, T., et al.:
Local implicit grid representations for 3d scenes. In: CVPR, pp. 6001–6010 (2020)
9. Kim, M., Seo, S., Han, B.: Infonerf: ray entropy minimization for few-shot neural
volume rendering. In: CVPR, pp. 12912–12921 (2022)
10. Liu, L., Gu, J., Zaw Lin, K., Chua, T.S., Theobalt, C.: Neural sparse voxel ﬁelds.
NIPS 33, 15651–15663 (2020)
11. Martin-Brualla, R., Radwan, N., Sajjadi, M.S., Barron, J.T., Dosovitskiy, A., Duck-
worth, D.: Nerf in the wild: neural radiance ﬁelds for unconstrained photo collec-
tions. In: CVPR, pp. 7210–7219 (2021)
12. Max, N.: Optical models for direct volume rendering. TVCG 1(2), 99–108 (1995)
13. Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy
networks: learning 3d reconstruction in function space. In: CVPR, pp. 4460–4470
(2019)
14. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.: Nerf: representing scenes as neural radiance ﬁelds for view synthesis. ECCV
65(1), 99–106 (2021)
15. M¨uller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives
with a multiresolution hash encoding. ToG 41(4), 1–15 (2022)
16. Niemeyer, M., Geiger, A.: Giraﬀe: representing scenes as compositional generative
neural feature ﬁelds. In: CVPR, pp. 11453–11464 (2021)
17. Ost, J., Mannan, F., Thuerey, N., Knodt, J., Heide, F.: Neural scene graphs for
dynamic scenes. In: CVPR, pp. 2856–2865 (2021)
18. Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: Deepsdf: learning
continuous signed distance functions for shape representation. In: CVPR, pp. 165–
174 (2019)
19. Pumarola, A., Corona, E., Pons-Moll, G., Moreno-Noguer, F.: D-nerf: neural radi-
ance ﬁelds for dynamic scenes. In: CVPR, pp. 10318–10327 (2021)
20. Rebain, D., Jiang, W., Yazdani, S., Li, K., Yi, K.M., Tagliasacchi, A.: Derf: decom-
posed radiance ﬁelds. In: CVPR, pp. 14153–14161 (2021)

90
X. Sun et al.
21. Reiser, C., Peng, S., Liao, Y., Geiger, A.: Kilonerf: speeding up neural radiance
ﬁelds with thousands of tiny MLPS. In: CVPR, pp. 14335–14345 (2021)
22. Roessle, B., Barron, J.T., Mildenhall, B., Srinivasan, P.P., Nießner, M.: Dense
depth priors for neural radiance ﬁelds from sparse input views. In: CVPR, pp.
12892–12901 (2022)
23. Smith, C., et al.: Unsupervised discovery and composition of object light ﬁelds.
arXiv preprint arXiv:2205.03923 (2022)
24. Tancik, M., et al.: Block-nerf: scalable large scene neural view synthesis. In: CVPR,
pp. 8248–8258 (2022)
25. Verbin, D., Hedman, P., Mildenhall, B., Zickler, T., Barron, J.T., Srinivasan,
P.P.: Ref-nerf: structured view-dependent appearance for neural radiance ﬁelds.
In: CVPR, pp. 5481–5490. IEEE (2022)
26. Wang, Z., et al.: Learning compositional radiance ﬁelds of dynamic human heads.
In: CVPR, pp. 5704–5713 (2021)
27. Wu, T., Zhong, F., Tagliasacchi, A., Cole, F., Oztireli, C.: D2nerf: self-supervised
decoupling of dynamic and static objects from a monocular video. In: NIPS (2022)
28. Yang, B., Zhang, Y., Xu, Y., Li, Y., Zhou, H., Bao, H., Zhang, G., Cui, Z.: Learning
object-compositional neural radiance ﬁeld for editable scene rendering. In: ICCV,
pp. 13779–13788 (2021)
29. Yuan, W., Lv, Z., Schmidt, T., Lovegrove, S.: Star: self-supervised tracking and
reconstruction of rigid objects in motion with neural rendering. In: CVPR, pp.
13144–13152 (2021)
30. Zhang, K., Riegler, G., Snavely, N., Koltun, V.: Nerf++: analyzing and improving
neural radiance ﬁelds. arXiv preprint arXiv:2010.07492 (2020)
31. Zhang, X., Bi, S., Sunkavalli, K., Su, H., Xu, Z.: Nerfusion: fusing radiance ﬁelds
for large-scale scene reconstruction. In: CVPR, pp. 5449–5458 (2022)
32. Zhang, X., Srinivasan, P.P., Deng, B., Debevec, P., Freeman, W.T., Barron, J.T.:
Nerfactor: neural factorization of shape and reﬂectance under an unknown illumi-
nation. TOG 40(6), 1–18 (2021)
33. Zhuang, Y., et al.: Neai: pre-convoluted representation for plug-and-play neural
ambient illumination. arXiv preprint arXiv:2304.08757 (2023)
34. Zhuang, Y., Zhu, H., Sun, X., Cao, X.: MoFaNeRF: morphable facial neural radi-
ance ﬁeld. In: Avidan, S., Brostow, G., Ciss´e, M., Farinella, G.M., Hassner, T. (eds.)
ECCV 2022. LNCS, vol. 13663, pp. 268–285. Springer, Cham (2022). https://doi.
org/10.1007/978-3-031-20062-5 16

LEAD: LiDAR Extender for Autonomous
Driving
Jianing Zhang1,5, Wei Li2(B), Ruigang Yang2, and Qionghai Dai1,3,4
1 Department of Automation, Tsinghua University, Beijing, China
2 Inceptio Technology, Shanghai, China
wei.li@inceptio.ai
3 Beijing National Research Center for Information Science and Technology (BNRist),
Beijing, China
4 Institute for Brain and Cognitive Science, Tsinghua University (THUIBCS), Beijing, China
5 Department of Electronic Engineering, Fudan University, Shanghai, China
Abstract. 3D perception using sensors under vehicle industrial standards is the
rigid demand in autonomous driving. MEMS LiDAR emerges with irresistible
trend due to its lower cost, more robust, and meeting the mass-production stan-
dards. However, it suffers small ﬁeld of view (FoV), slowing down the step
of its population. In this paper, we propose LEAD, i.e., LiDAR Extender for
Autonomous Driving, to extend the MEMS LiDAR by coupled image w.r.t both
FoV and range. We propose a multi-stage propagation strategy based on depth
distributions and uncertainty map, which shows effective propagation ability.
Moreover, our depth outpainting/propagation network follows a teacher-student
training fashion, which transfers depth estimation ability to depth completion net-
work without any scale error passed. To validate the LiDAR extension quality, we
utilize a high-precise laser scanner to generate a ground-truth dataset. Quantita-
tive and qualitative evaluations show that our scheme outperforms SOTAs with a
large margin.
Keywords: 3D perception · Autonomous Driving · LiDAR Extender
1
Introduction
Autonomous driving (AD) is one of the most challenging problems in computer vision
and artiﬁcial intelligence, which has attracted considerable attention in recent years.
Among all of the advances the AD system should achieve for mass production, 3D per-
ception using sensors under vehicle industrial standards is the rigid demand in the near
future. To meet this goal, more and more types of MEMS LiDAR emerge, with lower
cost, more robust performance, and most importantly, meeting the mass production
standards comparing with traditional mechanical LiDAR sensors. However, researches
on 3D perception still focus on data from mechanical LiDAR sensor. In this paper, we
tackle the problem of depth completion/estimation based on the MEMS LiDAR.
Despite the irresistible trend of using MEMS LiDAR, it suffers small ﬁeld of view
(FoV). Typical mechanical LiDAR sensors are always with 360◦FoV, while MEMS
LiDAR such as the one adopted in our paper is just 14.5◦× 16.2◦. A promising direc-
tion is utilizing a coupled camera with larger FoV to extend the point cloud/depth from
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 91–103, 2024.
https://doi.org/10.1007/978-981-99-8850-1_8

92
J. Zhang et al.
Fig. 1. (a) and (b) are the input pair, i.e., reference image from RGB camera and partial depth map
from MEMS LiDAR. (c) is the ground truth from our LEAD dataset. (d) is our depth extension
result.
MEMS LiDAR. A natural question to ask is: why do not we directly use cameras for
depth estimation if an extra camera is introduced? It is true that depth estimation from
monocular or stereo cameras, especially with the power of deep learning, achieves com-
pelling results [3,6,8,9,18,27]. Nevertheless, the depth maps estimated in this line of
works suffer from scale ambiguities. They may yield a plausible relative depth map, but
hard to deploy in real AD systems due to the incredible scale.
Another family of works avoids the scale ambiguities with the guidance of LiDAR
even only partial observations w.r.t the FoV of camera. The problem then becomes how
to complete the depth maps with images [19,22]. Regarding existing depth completion
approaches, the input depth maps are projected from raw point clouds from mechanical
LiDAR. Thus, the depth pattern is sparse but covers the whole space of the target view
(the FoV of the image). Those methods focus on inpainting holes of depth map using
interpolation and propagation mechanism [1,17]. Oppositely, as shown in Fig. 1, our
MEMS LiDAR could only produce very limit observations in a small area, which usu-
ally lies on the center of coupled images. In other words, we need to extend/outpainting
signiﬁcantly from partial depth to ﬁll the whole space of the target image. Actually,
there are few works of depth completion online sensors [12], and they are only vali-
dated on indoor scenes. To the best of our knowledge, our approach is the ﬁrst one of
extending such limited depth observation to large FoV with high accuracy.
The key insight of the proposed approach is to propagate partial depth from MEMS
LiDAR to a larger area with the guidance of image. We introduce a multi-stage propa-
gation to deal with this problem gradually. Eventually, we can get full of accurate depth
after several stages. However, an effective extending operation between two stages is
not trivial. Technically, in our approach, the output of one stage is a depth distribution
set and an uncertainty map instead of one simple depth map. Then we use a novel prob-
abilistic selection and combination operator to yield the depth for the next state. With
these mechanisms, accurate depth could effectively propagate out. However, the net-
work stays fragile even with a well-designed training strategy or parameters. Thus, we
propose a teacher-student training fashion to learn depth estimation ability. We build
a teacher network based on Mono2 [9] for our core depth outpainting/propagation
network. Qualitative and quantitative experiments show that the proposed method

LEAD: LiDAR Extender for Autonomous Driving
93
Fig. 2. Overview of our method. The pipeline consists of a self-supervised teacher network (STN),
propagative probabilistic generating (PPG) module and probabilistic derivation and composi-
tion (PDC) module. STN provides the initial depth and stabilizes the training processing of PPG.
PPG generates the depth probabilistic distributions for different propagation levels. With these
distributions and the input partial depth map, PDC derives the ﬁnal depth map with correspond-
ing uncertainty.
outperforms SOTAs with a large margin. Additionally, to have accurate metrics, we
use a 3D high-accurate long-range laser scanner to collect our dataset. In summary, the
technical contributions of our work are as follows:
– We introduce a new setup focusing on extending the MEMS LiDAR by coupled
image w.r.t both FoV and range. We propose a multi-stage propagation strategy
based on depth distributions and uncertainty map, which shows effective propaga-
tion ability.
– We introduce a teacher-student strategy to combine monocular depth estimation and
completion networks. We experimentally ﬁnd such a strategy could transfer depth
estimation ability to depth completion network without any scale error passed.
– To validate the LiDAR extension quality, we utilize a high-precise laser scanner to
generate a ground-truth dataset. We hope such carefully collected data could beneﬁt
the community w.r.t depth researches (Fig. 2).
2
Related Work
2.1
Depth Estimation
The target of monocular depth estimation is to estimate the depth map with a single
camera. This problem is ill-posed since an image can project to many plausible depths.
So in practice, these methods can only provide an estimation or relative depth map.
Eigen et al. [3] proposed the ﬁrst learning-based monocular depth estimation algorithm
that relies on the dense ground truth. After this work, a lot of supervised learning-
based monocular depth estimation merged, such as [5] and [10]. However, acquisition
of the accurate and dense ground truth is tedious. [16] tried to explore the potential
of synthetic training data but the complexity of synthetic data is still not comparable

94
J. Zhang et al.
Fig. 3. Propagative probabilistic generating module. For each propagation stage, the network can
fuse the output depth from last stage to generate depth map for next stage. On each stage, there
is a discriminator to judge the quality of generated depth map.
with the real data. To break through the limitation of insufﬁcient training data, Gard et
al. [6] proposed a self-supervised learning framework that uses the stereo pair and con-
structs the stereo photometric reprojection warping loss to train the network. After that,
Godard et al. [8] improved the stereo-based framework by adding a left-right consis-
tency loss. In addition to stereo pair, Zhou et al. [27] only used the video sequences
to estimate both the camera pose and depth map and the assumption of this method
is that the scene is static so the network had to predict a mask to ﬁlter out the mov-
ing objects. Godard et al. [9] also improved sequence-based self-supervised framework
with an auto-mask, besides, in [9], the two kinds of the self-supervised framework were
used together, which also improved the monocular depth estimation result. To improve
the quality of the depth map, some other tasks can be added and trained together, such
as optical ﬂow estimation [24] and image segmentation [28] [23]. To ﬁnd the uncertain
area, some algorithms [4] [11] [18] generate the uncertainty map while estimating the
depth map.
2.2
Depth Completion
Depth completion is a relevant topic with our task, which focuses on yielding dense
depth from sparse or noisy point cloud data [19]. The classical depth completion meth-
ods take only sparse or noisy depth sample from LiDAR or SLAM/SfM systems as
input, which fall into the concept of depth super-resolution [13], depth inpainting [25],
depth denoising [20]. In this category, Zhang et al. [26] proposed to predict the surface
normal to estimate the dense depth map for indoor scenes in NYUv2 dataset [21]. Ma et
al. [15] used sparse depth maps and images to ﬁrst calculate the camera poses, and then
train a depth completion network based on predicted poses. Those works achieve com-
pelling results of depth densiﬁcation, but they only deal with partial depth maps in the
same resolution with RGB images, i.e.sparse depth samples are in the complete spatial
space. So, the network only needs to inpaint the empty part of the depth map. However,
in our setup, the input depth is only a small part w.r.t the vertical and horizontal space
of our ﬁnal estimated depth. Our task is more a depth outpainting problem with both the
FoV and range extension are required. Instead of mechanical LiDAR, Liao et al. [12]
adapt a line sensor to get the partial depth measurement and generate a dense depth map
but limited by the setup, this method can only be applied indoor.

LEAD: LiDAR Extender for Autonomous Driving
95
3
Method
3.1
Overview
Given the RGB image I and partial depth map Ds generated from MEMS LiDAR sen-
sor, our goal is to estimate full depth map Df with the same FoV of a camera, in other
words, the same resolution of image I. The key insight of the proposed method is to
propagate the partial depth to larger areas with the guidance of the image. Digging into
the inputs, the partial depth Ds provides some accurate depth value, and the structure
in RGB image I encodes plentiful cues for depth distribution so we want to learn a
mapping G : (I, Ds) →p.
However, training such a probabilistic generator faces some challenges. First, gen-
erating dense outdoor depth maps needs to face the challenge that acquiring the ground
truth is hard. Even using the expensive LiDAR, there are still holes in the depth maps.
Second, the training process is not stable, especially without ground truth. Besides, how
to make full use of the partial depth and propagate the partial depth is also a problem.
3.2
Self-supervised Teacher Network (STN)
To stabilize the training process without ground truth, we ﬁrst train a teacher network
to guide the probabilistic generator. The teacher network can learn by itself to inference
an initial depth map for next stage. Note that the probabilistic generator, which shares
the network structure and initialization weights with teacher network, can distill the
knowledge learned by the teacher network. The inputs of our teacher net are RGB and
partial depth map.
Without ground truth, we adopt the self-supervised training fashion for STN. In
addition to the network for depth estimation, we construct a network for pose esti-
mation. With the estimated pose and depth, we can synthesis the target image from
another viewpoint. By minimizing the photometric reprojection error, the network can
be trained to generate plausible depth maps.
3.3
Propagative Probabilistic Generator (PPG)
Based on the guidance of STN, we build probabilistic generator to maximize the poten-
tial of the partial depth map in terms of full FoV depth estimation.
Multi Stage Propagation. In our experiments, we found that the straightforward map-
ping of I, Ds →p is unsatisfactory. This is reasonable, as Ds could provide relatively
accurate depth values but only in a very limited area. Thus depth values in the remaining
area heavily rely on the inference from the monocular image, which cannot guarantee
accuracy. Considering the small FoV depth cannot affect the whole depth map but it
can improve the nearby depth value, here we gradually expand the propagation area.
During the fusion step, we adjust the scale of the unreﬁned larger cropped depth maps
based on the median ratio,
Ds
i = median(Di−1 > 0)/median(Db
i > 0) · Db
i,
Dm
i = (1 −sgn(Di−1))Ds
i + sgn(Di−1)Di−1,
(1)

96
J. Zhang et al.
where Db
i is the blur depth map cropped from the output of teacher network, Di−1 is
the reﬁned depth map in stage i −1, in stage i, Di−1 will be padded to the same size
as Db
i, Ds
i is the depth after scale adjustment and Dm
i
is the mixed depth map. Then
reﬁne depth of this stage is generated with the concatenation of cropped RGB features
and depth features extracted from the Dm
i . In the next stage, the last stage depth maps
will be propagated into a larger cropped depth map. Repeating this step can propagate
the small FoV depth into the whole depth map. For each stage, the generated depth map
is based on the reﬁned depth map of the last stage.
Probabilistic Generating. After propagation, to generate the depth distribution, we
introduce a distribution generating block to generate the distributions of each prop-
agation stage. Figure 3 shows the details of our distribution generating block. While
training, at each propagation stage, the generator can generate the depth map of this
stage and the discriminator will judge the quality of the generated depth map. Like
other generative adversarial networks, we introduce some noise to the input feature
by randomly dropping out. While testing, we still enable the function of dropping out
and generate probabilistic distribution by multiple forward sampling. Additionally, we
can derive the uncertainty map Umap using the standard deviation of distribution set
p(D|I, Ds):
Umap = σ(p) =



 1
N
N

k=1
(dk −μ(p)), dk ∈p,
(2)
where N is the size of distribution set p. μ(p) = 1
N
N
k=1 dk is the mean of p.
However, using the given small FoV RGBD values merely cannot train the discrim-
inator well due to the limited ﬁeld of view. We thus adopt the propagation strategy to
train the GAN step by step. At each stage, the reﬁned depth at the previous stage is
regarded as the true samples, while the cropped depth in this stage which aligns with
RGB images of the previous stage is regarded as the false samples. Besides, for better
training of the discriminator, we randomly change the scale of depth maps, i.e., in stage
i, for the discriminator
Dt
i = S · Di−1, Df
i = S · Di,
(3)
where S follows uniform distribution S ∼U[0.8,1.2], we can change the size of the
RGBD image by a random scale that follows uniform distribution Ssize ∼U[0.5,1.8].
The data augment forces the discriminator to learn the structural mapping relation
between RGB and depth while ignoring the different scales and sizes among different
levels.
3.4
Probabilistic Derivation and Composition
With the probabilistic depth distribution, the simple way to generate the ﬁnal depth map
is to calculate the mean of these distributions, however, this way cannot take advantage
of the partial depth map. With our special propagative probabilistic generator, we can
derive the ﬁnal depth map without complex post-processing. Since we generate the
depth distribution at each propagation stage, we can follow the same idea to derive

LEAD: LiDAR Extender for Autonomous Driving
97
and compose the ﬁnal depth map stage by stage. The depth map Di in stage i can be
acquired by
Di = min
dk,i∈pi(∥(dk,i −Di−1) · sgn(Di−1)∥
+λ∥(dk,i −Ds,i) · sgn(Ds,i)∥),
(4)
where pi is the distribution set in i-th stage and in our experiment, at stage i, we run the
generator for 5 times to generate the distribution set pi, Di−1 is the reﬁned depth map
in stage i −1, dk is the k-th depth map in pi, λ is the weight coefﬁcient and Ds,i is
the cropped partial depth map whose size is the same as Di. So in each stage, we make
sure that the reﬁned depth map is similar to the previous stage reﬁned depth map and
the partial ground truth. With Di, the distribution of the next stage can be generated
by the next generator and we use the same way to generate Di+1. In this way, we can
gradually propagate the partial depth information into the ﬁnal result.
3.5
Network Training
For self-supervised learning, given the camera intrinsic and the relative pose between
the two frames, we can use the depth map to synthesis the image of a novel view and
calculate the photo-metric loss.
Lpe = F(π(I(t′), K, R|t, D, I(t))),
(5)
F(I′, I) = α
2 (1 −SSIM(I′, I)) + ∥I′ −I∥.
(6)
For STN, we adopt multi-scale output and use three frames to train the network. While
training with three frames,
Lpe =
4

s=0
wi min
i∈−1,1 F(I(t + i), I(t)).
(7)
The partial depth maps can supervise the output depth maps,
Lpart =
4

s=0
∥Ds −sgn(Dpart)∥.
(8)
Besides, we use edge-aware smooth loss to improve the smoothness
Lsmooth = |∂xD|e−|∂xI| + |∂yD|e−|∂yI|.
(9)
The ﬁnal loss function for STN is
Lt = wpeLpe + wpLpart + wsLsmooth.
(10)
When training the PPG, different from the STN, the output of the network are ﬁve
different size depth maps corresponding to ﬁve propagation stages. The PPG follows

98
J. Zhang et al.
the GAN structure, the loss function contains a GAN loss and an appearance loss. The
GAN loss is
LGAN =
5

i=0
EIt
i ,Dt
i[logD(It
i, Dt
i)]
+EIt
i ,Dt
i[log(1 −D(G(It
i, Dt
i)))].
(11)
For appearance loss, since the STN has generated Dblur, we adjust the scale of the
depth map to supervise the PPG
s = median(Dpart > 0)/median(Dt · sgn(Dpart)),
(12)
Dpseudo = s · Dblur,
(13)
Lpseudo = F(crop(Dpseudo), Ds),
(14)
The partial depth can adjust the scale of Dblur to generate Dpseudo. We construct the
photo-metric loss of generator denoted as Lpeg, then the loss function of PPG is given
by
Ls = wpegLpeg + wpseLpseudo + wGLGAN.
(15)
4
Experiment
4.1
Hardware and Evaluation Dataset
Hardware. We use the Tele-15 MEMS LiDAR sensor from Livox Inc.1 as the partial
depth sensor. The FoV of the MEMS LiDAR is 14.5◦× 16.2◦. The effective range is
from 3 m to 200 m during our practical usage. In addition, we use an industrial camera
from FLIR Inc. with 12mm focal length, 41.3◦×31.3◦FoV and 1536×1024 resolution.
Our LEAD Dataset. We captured around 100 h of data, in 50 streets over 500 km from
the urban area in Shanghai, China. Then, we semi-automatically pick 16792 pairs of
partial depth maps and their corresponding images, which are used as training and val-
idation sets. To quantitatively evaluate the extended depth map from our setup, we uti-
lize an additional high-accurate long-range laser scanner to capture additional 120 pairs
with ground truth depth. We use Riegl Inc.’s vz-20002 3D terrestrial laser scanner. The
scanner has a wide FoV of 100◦× 360◦with up to 2500 m sensing capability and 5 mm
accuracy. Figure 4 shows one sample of the testing set in our LEAD dataset.
KITTI Eigen Split. To evaluate the generalizability of our proposed method, we conduct
experiments on the KITTI dataset [7]. We resample a small region from the ground truth
to simulate partial depth input. Note that we use the improved ground truth depth maps
provided by KITTI in our experiments.
1 https://www.livoxtech.com/tele-15.
2 http://www.riegl.com/nc/products/terrestrial-scanning/produktdetail/product/scanner/58/.

LEAD: LiDAR Extender for Autonomous Driving
99
Fig. 4. Collection area and samples in LEAD dataset. The map in bottom row shows the data
collection area. The point clouds in dotted box are captured from MEMS LiDAR (red) and high-
accurate laser scanner (grey). (Color ﬁgure online)
Fig. 5. Qualitative results on LEAD and KITTIdataset .
Table 1. Quantitative results on LEAD dataset.
lower is better
higher is better
Method
RMSE RMSE log δ1.25
δ1.252 δ1.253
STN
15.718
0.633
0.070 0.174
0.603
Ours
6.299
0.171
0.859 0.963
0.991
STN (< 80)
14.446
0.633
0.072 0.176
0.604
Ours (< 80)
6.053
0.172
0.858 0.963
0.991
STN (> 80)
22.687
0.330
0.720 0.743
0.848
Ours (> 80)
6.968
0.077
0.959 0.999
1.000
4.2
Qualitative Results

100
J. Zhang et al.
Results on LEAD Dataset. Figure 5(a) shows the qualitative results on our LEAD
dataset. The second row shows the depth map and its the error map for the STN module.
The third row shows the results for our full pipeline.
Results on KITTI Dataset. Figure 5(b) shows the qualitative results on the KITTI
dataset. Since our network is based on MonoDepth2 [9], we use it as our baseline
method. We train the MonoDepth2 supervised by the input narrow FoV depth map.
MonoDepth2 suffers from the problem of sparse and uneven depth distribution. The
input narrow FoV depth map is not able to correct the scale even after using the ratio of
the median to correct the monocular depth scale. By contrast, our propagation and dis-
tribution output modules are designed to solve this problem, resulting in more accurate
wide range of depth maps.
Table 2. Quantitative results on KITTI Eigen Split with improved ground truth. “SC” means
different scale correction methods: “M” means using different scales for each individual test case.
The scale is the ratio between the median depth of the ground truth depth map and the median
depth of the resulting depth map. “F” means using a ﬁxed scale that is the mean scale. “P” means
using the scale estimated from the input partial depth.
lower is better
higher is better
Method
SC Abs Rel Sq Rel RMSE RMSE log δ1.25
δ1.252 δ1.253
EPC++[14]
M
0.120
0.789
4.755
0.177
0.856 0.961
0.987
Geonet[24]
M
0.132
0.994
5.240
0.193
0.833 0.953
0.985
MonoDepth2[9]
M
0.090
0.545
3.942
0.137
0.914 0.983
0.995
EPC++
F
0.153
0.998
5.080
0.204
0.805 0.945
0.982
Geonet
F
0.202
1.521
5.829
0.244
0.707 0.913
0.970
MonoDepth2
F
0.109
0.623
4.136
0.154
0.873 0.977
0.994
MonoDepth2
P
0.104
0.560
3.898
0.48
0.889 0.980
0.995
Ours (res18)
P
0.093
0.387
3.073
0.133
0.918 0.985
0.996
Ours (res50)
P
0.090
0.424
3.419
0.133
0.916 0.984
0.996
4.3
Quantitative Results
Results on LEAD Dataset. Table 1 shows the quantitative results on our LEAD dataset.
We compare results for the STN module and our full pipeline.
Results on KITTI Dataset. Table 2 shows the quantitative results on the KITTI Eigen
Split [2]. The ﬁrst and second row blocks show results for the unsupervised monocular
depth estimation methods, which need scale correction based on ground truth depth
information and it is not a practical assumption. By contrast, our method only needs
partial depth maps. Hence, the third-row block shows results for MonoDepth2 and our
method using a partial depth map for scale correction. The results show that our method
outperforms the baseline methods.

LEAD: LiDAR Extender for Autonomous Driving
101
Ablation Study. Table 3 shows the ablation study results. We compare the performances
among three methods: STN, STN+PPG, and STN+PPG+PDC. It can be shown that the
proposed PPG and PDC modules improve the performance effectively.
Table 3. Ablation study on KITTI dataset.
lower is better
higher is better
Method
Abs Rel Sq Rel RMSE RMSE log δ1.25
δ1.252 δ1.253
STN
0.103
0.504
3.713
0.148
0.886 0.979
0.995
STN+PPG
0.106
0.486
3.635
0.144
0.904 0.983
0.996
STN+PPG+PDC
0.101
0.456
3.475
0.140
0.909 0.984
0.996
5
Conclusion
We propose a novel self-supervised learning-based method to use the small ﬁeld of view
solid-state MEMS LiDAR to estimate a deeper and wider depth map, a new hardware
system, and a corresponding dataset for this kind of LiDAR. The experiment shows
that our new setup and algorithm make full use of the depth generated by the MEMS
LiDAR and generate a deeper and wider depth map. We believe that our setup and
method can promote the wide application of the low-cost and miniaturized solid-state
MEMS LiDAR in the ﬁeld of autonomous driving.
References
1. Cheng, X., Wang, P., Yang, R.: Learning depth with convolutional spatial propagation net-
work. IEEE Trans. Pattern Anal. Mach. Intell. (2019)
2. Eigen, D., Fergus, R.: Predicting depth, surface normals and semantic labels with a common
multi-scale convolutional architecture. In: Proceedings of the IEEE International Conference
on Computer Vision, pp. 2650–2658 (2015)
3. Eigen, D., Puhrsch, C., Fergus, R.: Depth map prediction from a single image using a multi-
scale deep network. In: Advances in Neural Information Processing Systems, pp. 2366–2374
(2014)
4. Eldesokey, A., Felsberg, M., Holmquist, K., Persson, M.: Uncertainty-aware cnns for depth
completion: uncertainty from beginning to end. In: IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) (June 2020)
5. Fu, H., Gong, M., Wang, C., Batmanghelich, K., Tao, D.: Deep ordinal regression network for
monocular depth estimation. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 2002–2011 (2018)
6. Garg, R., Vijay Kumar, B.G., Carneiro, G., Reid, I.: Unsupervised CNN for single view
depth estimation: geometry to the rescue. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) ECCV 2016. LNCS, vol. 9912, pp. 740–756. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-46484-8_45

102
J. Zhang et al.
7. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti vision
benchmark suite. In: 2012 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3354–3361. IEEE (2012)
8. Godard, C., Mac Aodha, O., Brostow, G.J.: Unsupervised monocular depth estimation with
left-right consistency. In: Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, pp. 270–279 (2017)
9. Godard, C., Mac Aodha, O., Firman, M., Brostow, G.J.: Digging into self-supervised monoc-
ular depth estimation. In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 3828–3838 (2019)
10. Guo, X., Li, H., Yi, S., Ren, J., Wang, X.: Learning monocular depth by distilling cross-
domain stereo networks. In: Proceedings of the European Conference on Computer Vision
(ECCV), pp. 484–500 (2018)
11. Johnston, A., Carneiro, G.: Self-supervised monocular trained depth estimation using self-
attention and discrete disparity volume. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 4756–4765 (2020)
12. Liao, Y., Huang, L., Wang, Y., Kodagoda, S., Yu, Y., Liu, Y.: Parse geometry from a line:
monocular depth estimation with partial laser observation. In: 2017 IEEE International Con-
ference on Robotics and Automation (ICRA), pp. 5059–5066. IEEE (2017)
13. Lu, J., Forsyth, D.: Sparse depth super resolution. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 2245–2253 (2015)
14. Luo, C., et al.: Every pixel counts++: joint learning of geometry and motion with 3d holistic
understanding. arXiv preprint arXiv:1810.06125 (2018)
15. Ma, F., Cavalheiro, G.V., Karaman, S.: Self-supervised sparse-to-dense: self-supervised
depth completion from lidar and monocular camera. In: 2019 International Conference on
Robotics and Automation (ICRA), pp. 3288–3295. IEEE (2019)
16. Mayer, N., et al.: What makes good synthetic training data for learning disparity and optical
ﬂow estimation? Int. J. Comput. Vision 126(9), 942–960 (2018)
17. Park, J., Joo, K., Hu, Z., Liu, C.K., Kweon, I.S.: Non-local spatial propagation network for
depth completion. In: Proceedings of European Conference on Computer Vision (ECCV)
(2020)
18. Poggi, M., Aleotti, F., Tosi, F., Mattoccia, S.: On the uncertainty of self-supervised monocu-
lar depth estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 3227–3237 (2020)
19. Qiu, J., et al.: Deep surface normal guided depth prediction for outdoor scene from sparse
lidar data and single color image. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3313–3322 (2019)
20. Shen, J., Cheung, S.C.S.: Layer depth denoising and completion for structured-light rgb-d
cameras. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-
tion, pp. 1187–1194 (2013)
21. Silberman, N., Hoiem, D., Kohli, P., Fergus, R.: Indoor segmentation and support inference
from RGBD images. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.)
ECCV 2012. LNCS, vol. 7576, pp. 746–760. Springer, Heidelberg (2012). https://doi.org/
10.1007/978-3-642-33715-4_54
22. Uhrig, J., Schneider, N., Schneider, L., Franke, U., Brox, T., Geiger, A.: Sparsity invariant
CNNs. In: 2017 International Conference on 3D Vision (3DV), pp. 11–20. IEEE (2017)
23. Wang, L., Zhang, J., Wang, O., Lin, Z., Lu, H.: SDC-depth: semantic divide-and-conquer
network for monocular depth estimation. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 541–550 (2020)
24. Yin, Z., Shi, J.: Geonet: unsupervised learning of dense depth, optical ﬂow and camera pose.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
1983–1992 (2018)

LEAD: LiDAR Extender for Autonomous Driving
103
25. Zhang, H.T., Yu, J., Wang, Z.F.: Probability contour guided depth map inpainting and super-
resolution using non-local total generalized variation. Multim. Tools Appl. 77(7), 9003–9020
(2018)
26. Zhang, Y., Funkhouser, T.: Deep depth completion of a single RGB-D image. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 175–185 (2018)
27. Zhou, T., Brown, M., Snavely, N., Lowe, D.G.: Unsupervised learning of depth and ego-
motion from video. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1851–1858 (2017)
28. Zhu, S., Brazil, G., Liu, X.: The edge of depth: explicit constraints between segmentation
and depth. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 13116–13125 (2020)

Fast Hierarchical Depth Super-Resolution
via Guided Attention
Yusen Hou1
, Changyi Chen1
, Gaosheng Liu1
, Huanjing Yue1
,
Kun Li2
, and Jingyu Yang1(B)
1 School of Electrical and Information Engineering,
Tianjin University, Tianjin, China
{hys20,2019234356,gaoshengliu,huanjing.yue,yjy}@tju.edu.cn
2 College of Intelligence and Computing, Tianjin University, Tianjin, China
lik@tju.edu.cn
Abstract. Depth maps captured by mainstream depth sensors are still
of low resolution compared with color images. The main diﬃculties in
depth super-resolution lie in the recovery of tiny textures from severely
undersampled measurements and texture-copy artifacts due to depth-
texture inconsistency. To address these problems, we propose a simple
and eﬃcient convolutional ﬁltering approach based on guided atten-
tion, named HDSRnet-light, for high quality depth super-resolution. In
HDSRnet-light, a guided attention scheme is proposed to fuse features of
the pyramidal main branch with complementary features from two side-
branches associated with the auxiliary high-resolution color image and
a bicubic upsampled version of the input depth map. In this way, high-
resolution features are progressively recovered from multi-scale informa-
tion from both the depth map and the color image. Experimental results
show that our method achieves state-of-art performance for depth map
super-resolution.
Keywords: depth map super-resolution · guided ﬁltering ·
convolutional neural network
1
Introduction
High-resolution (HR) depth maps are highly desirable in the ﬁeld of 3D vision.
In recent years, with rapid development of aﬀordable and portable consumer
depth cameras such as Microsoft Kinect and Time-of-Flight (TOF), consumer-
level depth sensors have entered the daily life of the public. However, current
depth sensors can only acquire low-resolution depth maps, which limits their
applications.
Depth super-resolution methods have been developed to bridge the resolution
gap between depth maps and color images. However, the super-resolution of
depth maps is an under-determined problem. Fine structures are lost or distorted
in LR depth maps due to under-sampling. Traditional interpolation methods
would introduce blurry artifacts. So, numerous methods have been proposed
to address these problems [1,2]. However, artifacts are still easily observed in
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 104–115, 2024.
https://doi.org/10.1007/978-981-99-8850-1_9

Fast Hierarchical Depth Super-Resolution via Guided Attention
105
many cases. Recently, convolution neural network (CNN) had been introduced
into the area of depth map super-resolution, and achieved promising results
[3,4]. However, it still confronts some challenges: Due to the excessive use of
color image information, texture copy artifacts still exists in some tough cases.
After training to a certain level, edge information with minor variation becomes
extremely diﬃcult to reconstruct which also the main diﬃculty of depth map
super-resolution. Besides, recent works tend to construct heavy CNN structures
with huge amount of model parameters to boost objective metrics such as RMSE
or MAD. The improvement in terms of such metrics are marginal with increase
of model complexity. But for industrial applications, heavy models are diﬃcult
to deploy in embedded systems, and model compression would in turn sacriﬁce
prediction precision.
Based on above observations, we propose a light and eﬃcient CNN-based
depth super-resolution approach, named HDSRnet-light, which has achieved
state-of-the-art performance with a compact and fast model. Standing on the
pyramidal framework, our HDSRnet-light structure has three branches: main
branch for depth super-resolution, the Structure side-branch extracting depth
features from Dbicubic (Upsampling from LR depth maps via bicubic), and Detail
side-branch extracting color features from HR color image. In the main branch,
feature maps of the LR depth map are enhanced by an attention module with
complementary features from the two side-branches, followed by an upsampling
unit to magnify the size of the feature maps. The attention-and-upsampling
structure is repeated twice before reaching the adder of the residual structure.
The two side-branches ﬁrst extract features from the auxiliary input, either
bicubic depth map and color image respectively, and go through two down-
sampling units to have compatible sizes with the associated attention modules.
The HDSRnet-light is trained using ℓ1 loss. Experimental results show that our
method outperforms several state-of-the-art methods for most cases due to the
eﬀectiveness of the hierarchical attention mechanism. HDSRnet-light has only
287.9k parameters and is able to run at a speed of more than 300 fps under
NVIDIA GeForce GTX 1080Ti with state-of-the-art performance for 8× super-
resolution to the size of 1920 × 1080.
2
Related Work
2.1
Traditional Depth SR Method
Early depth super-resolution methods do not assume the RGB-D input, and are
mainly based on various image ﬁltering schemes. Lei et al. [5] improved the super-
resolution performance by using an up-sampling ﬁlter to strengthen the view
synthesis quality. Xie et al. [6] proposed an edge-guided method, which generates
a HR edge map from LR depth map by Markov random ﬁeld optimization.
Park et al. [7] incorporated a non-local means term to preserve local structure
and remove outliers.
Recent mainstream depth sensors usually output RGB-D pairs, and the asso-
ciated RGB color images have a higher resolution than the depth maps. So most

106
Y. Hou et al.
depth super-resolution use the HR color image or intensity image as guidance
to enhance the quality of the depth map. He et al. [8] proposed a guided ﬁlter
as a generic edge-preserving smoothing operator for depth SR. Joint Bilateral
Filter [9] further applied addition guidance to improve the SR quality.
Zuo et al. [10] proposed a explicit edge inconsistency method for depth SR.
Yang et al. [11] fused the median and bilateral ﬁltering to construct a new
edge-preserving depth SR ﬁlter. Yang et al. [12] proposed a pixel-wise adaptive
autoregressive model to utilize non-local correlation. Jiang et al. [13] proposed
a dual domain based depth super-resolution methods, where a multi-directional
total variation (MTV) prior was proposed to characterize the spatially vary-
ing geometrical structures. These methods prove that utilize HR color image
as guided information can improve performance of depth SR and show strong
robustness.
2.2
Deep Learning-Based Depth SR Method
Thanks to the powerful learning capability of CNN, many deep learning-based
depth map SR methods have achieved some improvements. Anisotropic total
generalized variation network (ATGV-Net) [14], the priority work introduced
the learning framework to the variational optimization approach. Hui et al. [3]
proposed a multi-scale guided convolutional network (MSGnet) for depth map
super-resolution with RGB-D inputs. MSGnet uses multi-scale fusion strategy
to capture the complementary features of LR depth map and HR color image
in several scales. Riegler et al. [14] designed a deep convolutional network with
a variational method to recover accurate HR depth map. Song et al. [15] uti-
lized pixelshuﬄe in their method to super-resolve depth maps. Ye et al.
[16]
introduced the Progressive Multi-Branch Aggregation Network (PMBANet).
PMBANet utilizes attention-based error feed-forward/backward modules in a
multi-scale branch to reﬁne depth maps and designs a guidance branch as pri-
ority knowledge to recover depth details.
As a departure of devising large models to boost performance, we propose a
lightweight network which uses guided attention to fuse information. It achieves
state-of-the-art performance with only 287.9k parameters.
3
Proposed Method
In this part, we ﬁrst brieﬂy introduce the framework of our method, then illus-
trate the details of our method. The diﬃculties of depth SR is how to recover
the boundaries of depth map and make the model easy to deploy in the indus-
trial environment. To address these problems, we propose a lightweight method,
named HDSRnet-light, which achieves state-of-the-art performance with only
signiﬁcantly fewer parameters than previous models.
3.1
Framework Overview
In Fig. 1, we show the architecture of the proposed method. To be speciﬁc,
HDSRnet-light consists of three branches, the top side-branch extracts structure

Fast Hierarchical Depth Super-Resolution via Guided Attention
107
Fig. 1. Overall architecture of the proposed HDSRnet-light, consisting of three
branches, i.e., the main branch to reconstruct residual component from the input LR
depth map, the structure side-branch to learn structural depth information from bicu-
bic upsampled version of the depth map, and the detail side-branch to extract HR
details from the HR color image.
information from the bicubic interpolated version of the LR depth map, the
middle main-branch extracts depth features from the input LR depth map, and
the bottom side-branch extracts edge detail information from the color image
to compensate the loss of ﬁne granular details in the LR depth map. We utilize
a few number of convolutional layers to extract simple features and propose
an attention scheme named guided attention (GA) to fuse features (shown in
Fig. 2). The details are described in the following subsections.
3.2
Structures of Main and Side Branches
We utilize a multi-scale structure to super-resolve depth maps. Besides main-
branch and detail branch, we add a structure side-branch to extract auxiliary
features. Because we found that Dbicubic (Upsampling from LR depth map via
bicubic interpolation) contains structure information, which makes the training
more stable and convergence faster, especially when the size of LR depth map is
much lower than HR depth map. Then, we propose to learn the residual result
between the interpolated depth map and the HR depth map. Moreover, we use
a few number of deconvolution and the size of convolution kernel to reduce
computational complexity. We show HDSRnet-light in Fig. 1.
The downsampling unit can be described as:
f = σ(W1 ∗input + b1),
(1)
fpool = Maxpool(f),
(2)
output = σ(W2 ∗fpool + b2).
(3)

108
Y. Hou et al.
The Upsample Unit can be described as:
fdeconv = σ(Deconv(input)),
(4)
output = σ(W ∗fdeconv + b).
(5)
where σ represents Rectiﬁed Linear Unit (ReLU), Deconv represents transposed
convolution operator and “*” represents convolutional operator. W and b stand
for the weight and bias in the convolution operation. W1, W2, b1 and b2 are the
weight and bias that from diﬀerent convolutional layers. The three branches of
HDSRnet-light are built by these units and attention module, which transform
HR color image, preprocessed depth map and LR depth map into several scales:
– Main-Branch that achieves multiple level receptive ﬁelds and fuses features
from other branches;
– Structure Side-Branch that extracts the basic shape information of depth
map in several scales;
– Detail Side-Branch that extracts details from HR color image in several
scales, and transfer useful structure to reﬁne depth map.
Instead of directly concatenating, we propose a guided-based attention mecha-
nism to fuse these features, which are extracted by these branches.
Fig. 2. Structure of the proposed attention module: the module consists of two guided
attention blocks to fuse information from the main branch, structure side-branch, and
the detail side-branch.
3.3
Guided Attention
Existing depth super-resolution methods have proved the eﬀectiveness of using
the color image as reference information. Inspired by He et al. [8], we propose
guided attention to utilize the color image as guided image to regulate depth
maps in feature domain. Guided ﬁlter [8] considered the pixel of guided image
Ii and the pixel of output image Yi exists a linear relationship, which can be
described as
Yi = αiIi + βi.
(6)

Fast Hierarchical Depth Super-Resolution via Guided Attention
109
According to the characteristics of RGB-D, we consider a similar relationship
between color images and depth maps exists in feature domain. So we design
a guided attention to learn such a linear relationship adaptively. The proposed
attention is shown in Fig. 2. It contains two steps: weight learning and bias
learning.
Weight Learning: each attention module takes the features extracted from the
previous convolutional layers as inputs. Taking the ﬁrst guided attention block
in Fig. 2 as an example, structure side-branch input (denoted by input2) is used
to learn a series of weights via several convolutional layers for main branch input
(denoted by input1). This operation makes sure every element in main branch
input has an adaptive weight. The weight adaptive learning can be described as:
α = σ(W1 ∗input2 + b1),
(7)
where α represents the weight matrix.
Bias Adaptive Learning: we also use structure side-branch input to learn the
bias via another convolutional layer. It can be described as:
β = σ(W2 ∗input2 + b2),
(8)
where β represents the bias matrix.
The whole operation of the guided attention can be deﬁned as:
output = α · σ(W3 ∗input1 + b3) + β.
(9)
The proposed attention block helps aggregate information by adaptively
attending importance to the features from multiple branches, which uses the
relationship between HR color image and depth map eﬀectively.
4
Experiments and Results
This section shows the performance of our method by comparison with state-
of-the-art (SOTA) methods, attention analysis, running time and generalization
ability of our method experiment on real data.
4.1
Implementation Details
We collected 58 RGB-D images from MPI Sintel [23] depth dataset, and 34
RGB-D images from Middlebury dataset (6, 10 and 18 images from 2001, 2006
and 2014 datasets, respectively) [24–26]. To evaluate the performance of our
method, we test six standard depth maps (Art, Books, Moebius, Dolls, Laundry,
Reindeer) from Middlebury 2005 dataset.
The patch size is set to 128 × 128, and we augment data by introducing
random rotation by [0◦, 90◦, 180◦, 270◦]. We get totally 100, 000 training paired
patches. LR depth maps are obtained by downsampling the HR depth maps with
bicubic interpolation [4] [16].

110
Y. Hou et al.
Table 1. Quantitative depth SR results in MAD ↓and PE ↓.
Art
Books
4×
8×
16×
4×
8×
16×
JGF[1]
0.47/3.25
0.78/7.39
1.54/14.31
0.24/2.14
0.43/5.41
0.81/12.05
CDLLC[17]
0.53/2.86
0.76/4.59
1.41/7.53
0.19/1.34
0.46/3.67
0.75/8.12
EG[18]
0.48/2.48
0.71/3.31
1.35/5.88
0.15/1.23
0.36/3.09
0.70/7.58
MSG[3]
0.46/2.31
0.76/4.31
1.53/8.78
0.15/1.21
0.41/3.24
0.76/7.85
DGDIE[19]
0.48/2.34
1.20/13.18
2.44/26.32
0.30/3.21
0.58/7.33
1.02/14.25
DEIN[20]
0.40/2.17
0.64/3.62
1.34/6.69
0.22/1.68
0.37/3.20
0.78/8.05
CCFN[21]
0.43/2.23
0.72/3.59
1.50/7.28
0.17/1.19
0.36/3.07
0.69/7.32
GSRPT[22]
0.48/2.53
0.74/4.18
1.48/7.83
0.21/1.77
0.38/4.23
0.76/7.67
PMBANet[16]
0.26/1.95
0.51/3.45
1.22/6.28
0.15/1.13
0.26/2.87
0.59/6.79
HDSRnet-light 0.18/1.01 0.37/2.44 0.97/10.39
0.18/1.53
0.30/2.74
0.57/7.95
Dolls
Laundry
4×
8×
16×
4×
8×
16×
JGF[1]
0.33/3.23
0.59/7.29
1.06/15.87
0.36/2.60
0.64/4.54
1.20/8.69
CDLLC[17]
0.31/4.61
0.53/5.94
0.79/12.64
0.30/2.08
0.48/3.77
0.96/8.25
EG[18]
0.27/2.72
0.49/5.59
0.74/12.06
0.28/1.62
0.45/2.86
0.92/7.87
MSG[3]
0.25/2.39
0.51/4.86
0.87/9.94
0.30/1.68
0.46/2.78
1.12/7.62
DGDIE[19]
0.34/4.79
0.63/9.44
0.93/11.66
0.35/2.03
0.86/3.69
1.56/16.72
DEIN[20]
0.22/1.73
0.38/3.38
0.73/9.95
0.23/1.70
0.36/3.27
0.81/7.71
CCFN[21]
0.25/1.98
0.46/4.49
0.75/9.84
0.24/1.39
0.41/2.49
0.71/7.35
GSRPT[22]
0.28/2.84
0.48/4.61
0.79/10.12
0.33/1.79
0.56/4.55
1.24/8.98
PMBANet[16]
0.19/1.35
0.32/3.22
0.59/8.92
0.17/0.27
0.34/2.41
0.71/6.88
HDSRnet-light 0.17/1.00 0.31/2.39
0.60/7.62
0.15/1.00
0.28/2.19
0.69/9.37
Moebius
Reindeer
4×
8×
16×
4×
8×
16×
JGF[1]
0.25/3.36
0.46/6.45
0.80/12.33
0.38/2.27
0.64/5.17
1.09/11.84
CDLLC[17]
0.27/1.98
0.46/4.59
0.79/7.89
0.43/2.09
0.55/5.39
0.98/11.49
EG[18]
0.23/1.88
0.42/4.29
0.75/7.63
0.36/1.97
0.51/4.31
0.95/9.27
MSG[3]
0.21/1.79
0.43/4.05
0.76/7.48
0.31/1.73
0.52/2.93
0.99/7.63
DGDIE[19]
0.28/1.98
0.58/8.11
0.98/16.22
0.35/1.76
0.73/7.82
1.29/15.83
DEIN[20]
0.20/1.89
0.35/3.02
0.73/7.42
0.26/1.40
0.40/2.76
0.80/5.88
CCFN[21]
0.23/2.18
0.39/3.91
0.73/7.41
0.29/1.51
0.46/2.79
0.95/6.58
GSRPT[22]
0.24/2.02
0.49/4.70
0.80/8.38
0.31/1.58
0.61/5.90
1.07/10.35
PMBANet[16]
0.16/1.21
0.28/2.87
0.67/6.73
0.17/1.28
0.34/2.40
0.74/5.66
HDSRnet-light 0.14/1.16 0.24/2.22
0.50/6.43
0.16/0.77 0.27/1.56 0.57/5.58
We use Adam as the optimizer with momentum = 0.9, β1 = 0.9, β2 = 0.99,
eps = 10−8. Our initial learning rate is set to 0. 0001, and the decay rate is
1/10 for every 10 epochs. We train HDSRnet-light for about 80 epochs with ℓ1

Fast Hierarchical Depth Super-Resolution via Guided Attention
111
loss. Our network is implemented with the PyTorch framework on a NVIDIA
1080Ti GPU. Mean Absolute Diﬀerence (MAD), and percentage of error pixels
(PE) are used to measure the diﬀerence between the predicted depth map and
its ground-truth.
4.2
Comparison with SOTA Methods
We compared our method with 9 depth SR methods at three upsampling rates:
4×, 8× and 16×. Three of the compared methods are using advanced ﬁlter-
ing or functional optimization: JGF [1], CDLLC [17], and EG [18]. Others are
CNN-based SOTA methods: MSG [3], DGDIE [19], DEIN [20], CCFN [21],
GSRPT [22], and PMBANet [16]. Table 1 reports objective results in terms of
MAD and PE. The results show that CNN-based methods have better perfor-
mance comparing to traditional ﬁltering or optimization based methods. Our
method achieves better results for most cases than the second best performer
PMBANet, signiﬁcantly outperforming other competing methods. Particularly,
our method reduces the MAD reconstruction error below 1.0 for 16× upsampling
of “Art”, which is the most challenging case in the ﬁeld of depth upsampling.
Fig. 3. Visual comparison of 8 × upsampling results on “Art” and “Reindeer”: (a) GT,
(b) Bicubic, (c) Depth SR [4], (d) PMBANet [16], (e) DEIN [20], and (f) our method.
Figure 3 compares the visual results of our method and other state-of-the-
art methods. Comparing with the ground-truth, all these methods reconstruct
similar structures in general, and our methods achieves the best visual quality.
Particularly for 8× and 16× cases, the results show obvious diﬀerences around
the reconstructed tiny structures. In Fig. 3, we notice some tiny objects like
the tip and outline of the pencil in “Art” and the protuberance of the neck in
“Reindeer” are diﬃcult to recover.

112
Y. Hou et al.
To test generalization ability of our method, we run our model on NYU real
dataset. The comparison results of other competing methods are quoted from
Ye et al. [16], and they are trained with images from the NYU dataset. However,
the training data of our HDSRnet-light does not include any image from the NYU
dataset. The results in Table 2 show that our results are far superior than others,
which demonstrates that the proposed HDSRnet-light can be well generalized
to unseen depth maps.
Table 2. Quantitative depth SR results in RMSE on NYU dataset.
Bicubic
EDEG [7]
DJF [27]
DGDIE [19]
GbFT [28]
4×
8.16
5.21
3.54
1.56
3.35
8×
14.22
9.56
6.2
2.99
5.73
16× 22.32
18.1
10.21
5.24
9.01
PAC [29] SVLRM [30] DKN [31] PMBANet [16] Our method
4×
2.39
1.74
1.62
1.06
0.76
8×
4.59
5.59
3.26
2.28
1.26
16× 8.09
7.23
6.51
4.98
2.32
4.3
Attention Analysis
To show the eﬀectiveness of attention module, we conduct ablation study with
the following conﬁgurations:
1) HDSRnet-light - A + ℓ1: keep the structure of HDSRnet-light but use
concatenation to replace the attention module, and train via ℓ1 loss.
2) HDSRnet-light + ℓ1: add guided attention and train the HDSRnet-light
via ℓ1 loss.
Table 3 presents ablation results in terms of MAD and PE on ﬁve Middlebury
images at 8× up-sampling rate. It can be observed that our attention mechanism
can signiﬁcantly decrease MAD and PE eﬀectively.
Table 3. Ablation results in MAD/PE of our attention mechanism
Art
Dolls
Laundry
Moebius
Reindeer
Books
Ours wo A
0.4/2.71
0.33/2.59 0.31/2.55 0.25/2.36 0.29/1.66 0.30/2.71
Ours w A
0.37/2.44 0.31/2.39 0.28/2.19 0.24/2.22 0.27/1.56 0.30/2.74

Fast Hierarchical Depth Super-Resolution via Guided Attention
113
4.4
Running Time
We compared the running time of several methods on our workstation with
Intel(R) Core(TM) i5-9400F CPU, 32GB RAM, and a NVIDIA GeForce GTX
1080Ti GPU. The code of Bicubic comes from Python Image Library (PIL).
MSGnet [3] is implemented by Caﬀe, DepthSRnet [4] is implemented by Tensor-
Flow, and our method is implemented by Pytorch.
Consider the 8× SR of a low-resolution depth map of size 135×240, the
computation complexity (in FLOPs) and the running time (in ms) are reported
in Table 4. The second performer PMBANet [16] has the highest complexity, and
our HDSRnet-light has the lowest complexity. However, our HDSRnet-light still
outperforms PMBANet (shown in Table 1). Our HDSRnet-light achieves a speed
of 360 frames/second, which is comparable to the bicubic interpolation (CPU
implementation).
Table 4. The results of the running time and ﬂops
Times (ms) FLOPs
MSGnet [3]
405.72
4.85e11
DepthSR [4]
1639.43
4.43e12
PMBANet [16] 30.01
6.92e13
HDSRnet-light 2.61
4.62e11
5
Conclusion
In this paper, we have proposed an eﬀective depth map super-resolution method
(HDSRnet-light) with guided attention mechanism. In HDSRnet-light, we pro-
pose a guided attention scheme to fuse features of the main branch with comple-
mentary features from two side-branches associated with the auxiliary high-
resolution texture information and the low-resolution depth information. In
the proposed network, high-resolution features are progressively recovered from
multi-scale information from both the depth map and the color image. Exper-
imental results on benchmark datasets demonstrate that our method achieves
superior performance in comparison with other state-of-the-art methods. Our
HDSRnet-light has only 287.9k parameters and achieves a speed of more than
300 fps under NVIDIA GeForce GTX 1080Ti with state-of-the-art performance
for the 8× super-resolution to the size of 1920×1080.
Acknowledgement. This work was supported in part by the National Natural Sci-
ence Foundation of China under Grant 62231018, Grant 62171317, and Grant 62072331.

114
Y. Hou et al.
References
1. Liu, M.Y., Tuzel, O., Taguchi, Y.: Joint geodesic upsampling of depth images. In:
2013 IEEE Conference on Computer Vision and Pattern Recognition, pp. 169–176
(2013)
2. Ferstl, D., Reinbacher, C., Ranftl, R., Ruther, M., Bischof, H.: Image guided depth
upsampling using anisotropic total generalized variation. In: 2013 IEEE Interna-
tional Conference on Computer Vision, pp. 993–1000 (2013)
3. Hui, T.-W., Loy, C.C., Tang, X.: Depth map super-resolution by deep multi-scale
guidance. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS,
vol. 9907, pp. 353–369. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
46487-9 22
4. Guo, C., Li, C., Guo, J., Cong, R., Fu, H., Han, P.: Hierarchical features driven
residual learning for depth map super-resolution. IEEE Trans. Image Process.
28(5), 2545–2557 (2019)
5. Lei, J., Li, L., Yue, H., Wu, F., Ling, N., Hou, C.: Depth map super-resolution
considering view synthesis quality. IEEE Trans. Image Process. 26(4), 1732–1745
(2017)
6. Xie, J., Feris, R.S., Sun, M.: Edge-guided single depth image super resolution.
IEEE Trans. Image Process. 25(1), 428–438 (2016)
7. Park, J., Kim, H., Tai, Y.W., Brown, M.S., Kweon, I.: High quality depth map
upsampling for 3D-TOF cameras. In: 2011 International Conference on Computer
Vision, pp. 1623–1630 (2011)
8. He, K., Sun, J., Tang, X.: Guided image ﬁltering. IEEE Trans. Pattern Anal. Mach.
Intell. 35(6), 1397–1409 (2013)
9. Tomasi, C., Manduchi, R.: Bilateral ﬁltering for gray and color images. In: Inter-
national Conference on Computer Vision (2002)
10. Zuo, Y., Qiang, W., Zhang, J., An, P.: Explicit edge inconsistency evaluation model
for color-guided depth map enhancement. IEEE Trans. Circuits Syst. Video Tech-
nol. 28(2), 439–453 (2018)
11. Yang, Q., et al.: Fusion of median and bilateral ﬁltering for range image upsam-
pling. IEEE Trans. Image Process. 22(12), 4841–4852 (2013)
12. Yang, J., Ye, X., Li, K., Hou, C., Wang, Y.: Color-guided depth recovery from
RGB-D data using an adaptive autoregressive model. IEEE Trans. Image Process.
23(8), 3443–3458 (2014)
13. Jiang, Z., Hou, Y., Yue, H., Yang, J., Hou, C.: Depth super-resolution from RGB-
D pairs with transform and spatial domain regularization. IEEE Trans. Image
Process. 27(5), 2587–2602 (2018)
14. Riegler, G., R¨uther, M., Bischof, H.: ATGV-Net: accurate depth super-resolution.
In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9907,
pp. 268–284. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46487-
9 17
15. Song, X., Dai, Y., Qin, X.: Deeply supervised depth map super-resolution as novel
view synthesis. IEEE Trans. Circuits Syst. Video Technol. 29(8), 2323–2336 (2019)
16. Ye, X., et al.: PMBANet: progressive multi-branch aggregation network for scene
depth super-resolution. IEEE Trans. Image Process. 29, 7427–7442 (2020)
17. Xie, J., Chou, C.C., Feris, R., Sun, M.T.: Single depth image super resolution and
denoising via coupled dictionary learning with local constraints and shock ﬁltering.
In: 2014 IEEE International Conference on Multimedia and Expo (ICME), pp. 1–6
(2014)

Fast Hierarchical Depth Super-Resolution via Guided Attention
115
18. Xie, J., Feris, R.S., Sun, M.: Edge guided single depth image super resolution. In:
2014 IEEE International Conference on Image Processing (ICIP), pp. 3773–37777
(2014)
19. Gu, S., Zuo, W., Guo, S., Chen, Y., Chen, C., Zhang, L.: Learning dynamic guid-
ance for depth image enhancement. In: 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 712–721 (2017)
20. Ye, X., Duan, X., Li, H.: Depth super-resolution with deep edge-inference net-
work and edge-guided depth ﬁlling. In: 2018 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 1398–1402 (2018)
21. Wen, Y., Sheng, B., Li, P., Lin, W., Feng, D.D.: Deep color guided coarse-to-
ﬁne convolutional network cascade for depth image super-resolution. IEEE Trans.
Image Process. 28(2), 994–1006 (2019)
22. Lutio, R.D., D’aronco, S., Wegner, J.D.: Guided super-resolution as a learned pixel-
to-pixel transformation (2019)
23. Butler, D.J., Wulﬀ, J., Stanley, G.B., Black, M.J.: A naturalistic open source movie
for optical ﬂow evaluation. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y.,
Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7577, pp. 611–625. Springer, Heidelberg
(2012). https://doi.org/10.1007/978-3-642-33783-3 44
24. Scharstein, D., Szeliski, R., Zabih, R.: A taxonomy and evaluation of dense two-
frame stereo correspondence algorithms. In: Proceedings IEEE Workshop on Stereo
and Multi-Baseline Vision (SMBV 2001), pp. 131–140 (2001)
25. Scharstein, D., Pal, C.: Learning conditional random ﬁelds for stereo. In: 2007
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8 (2007)
26. Scharstein, D., et al.: High-resolution stereo datasets with subpixel-accurate ground
truth. In: Jiang, X., Hornegger, J., Koch, R. (eds.) GCPR 2014. LNCS, vol. 8753,
pp. 31–42. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-11752-2 3
27. Li, Y., Huang, J.-B., Ahuja, N., Yang, M.-H.: Deep joint image ﬁltering. In: Leibe,
B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9908, pp.
154–169. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46493-0 10
28. AlBahar, B., Huang, J.B.: Guided image-to-image translation with bi-directional
feature transformation. In: 2019 IEEE/CVF International Conference on Com-
puter Vision (ICCV), pp. 9015–9024 (2019)
29. Su, H., Jampani, V., Sun, D., Gallo, O., Learned-Miller, E., Kautz, J.: Pixel-
adaptive convolutional neural networks. In: 2019 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 11158–11167 (2019)
30. Pan, J., Dong, J., Ren, J.S., Lin, L., Tang, J., Yang, M.H.: Spatially variant lin-
ear representation models for joint ﬁltering. In: 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 1702–1711 (2019)
31. Kim, B., Ponce, J., Ham, B.: Deformable kernel networks for guided depth map
upsampling (2019). arXiv:abs/1903.11286

A Hybrid Approach for Segmenting
Non-ideal Iris Images Using CGAN
and Geometry Constraints
Shanila Azhar1, Ke Zhang1, Xiaomin Guo2, Shizong Yan1, Guohua Liu1,
and Shan Chan1(B)
1 Donghua University, Shanghai, China
{415029,2171768,1222040}@mail.dhu.edu.cn, {ghliu,changshan}@dhu.edu.cn
2 Shaanxi Provincial People’s Hospital, Xi’an, Shaanxi, China
guoxiaomin@xjtu.edu.cn
Abstract. The prevalence of personal mobile devices makes iris authen-
tication being more and more popular. Accurate iris segmentation is
critical for authentication. However, it is very challenging, due to iris
images captured by mobile and handheld devices may exhibit occlu-
sion, low resolution, blur, unusual glint, ghost eﬀect, and oﬀ-angles.
Moreover, mobile devices may be equipped with visible light cameras
rather than near-infrared (NIR) light cameras, which makes iris segmen-
tation susceptible to the noise of visible light. We propose an accurate
iris image segmentation approach, which takes advantages of both Con-
ditional Generative Adversarial Network (CGAN) and geometry-based
optimization. First, we design a CGAN which force the generator to pro-
duce better segmentation corresponds to the original image, a compara-
tively accurate prediction of iris region can be obtained. Second, a series
of geometry-based optimization schemes is introduced to reﬁne the pre-
diction results, where elliptical Hough transform and boundary piecewise
ﬁtting are performed on the inner and outer boundary of predicted iris
regions, respectively. We performed experiments on three non-ideal iris
datasets of visible light and NIR environments. The segmentation accu-
racy is evaluated using error rate, intersection over union and F-score.
Experimental results demonstrate that the proposed approach provides
signiﬁcant performance improvements comparing with the state-of-art
methods, OSIRIS and IrisSeg.
Keywords: Mobile authentication · Iris segmentation · CGAN
1
Introduction
Iris recognition is one of the promising biometric modalities and has received
much attention. The performance of iris recognition is largely dependent on the
accurate isolation of iris regions from the rest of eye images [11,17]. In tradi-
tional iris recognition systems, the acquisition of iris images is commonly in
near-infrared illumination environment, and requires a high degree of user coop-
eration. However, for mobile and handheld device authentication, non-ideal eye
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 116–129, 2024.
https://doi.org/10.1007/978-981-99-8850-1_10

A Hybrid Approach for Segmenting Non-ideal Iris Images
117
images are captured in uncontrolled environmental conditions. A large amount of
environmental noise can aﬀect the quality of eye images, including reﬂections of
visible light, motion blur, specular reﬂection, etc., which poses a severe challenge
to iris recognition technology [1,22].
Previous works on segmenting non-ideal iris can be divided into two cat-
egories. One is to use image processing related techniques to retrieve the iris
region the image based on features such as gradients and geometric relation-
ships, and use curves to ﬁt the contour of the iris. In this way, the anti-noise iris
segmentation method based on a modiﬁed fast Hough transform is proposed,
and a multi-arc and multi-line iris boundary strategy is used to deﬁne the iris
boundary [6]. Subsequently, the geometric active contours are used to reﬁne iris
boundaries, and eyelash noise is suppressed by the open operation [20]. But the
anti-noise ability of these methods is limited. Another is to use a deep learning
approach to segment non-ideal iris images. For example, multi-scale and hierar-
chical convolutional neural network is proposed to segment non-ideal iris images
[15]. To enhance the ability of the network to extract feature features, a deeper
convolutional network has been proposed to improve segmentation accuracy [3].
Limited by the distribution of trained datasets, the segmentation results of neu-
ral network are not stable enough.
Inspired by two diﬀerent ways of segmentation, we propose a combination
method of learning-based and edge-based processing for segmenting non-ideal iris
images. We design a conditional generative adversarial network (CGAN) in which
a U-Net and a VGG network are employed as the generator and discriminator,
respectively, and original iris images are fed into the generator, acting as the
condition of GAN, which force the generator to make predictions of iris regions
indistinguishable from real iris regions, and consistent with its original iris image
as well. The prediction results of CGAN will be further reﬁned by exploiting the
natural geometry properties of iris. The outer and inner boundaries of predicted
iris regions are re-ﬁtted and smoothed, and redundant areas will be removed.
Our contributions are as follows:
– We design a hybrid approach, where the CGAN-based learning procedure
and the geometry characteristic-based boundary optimization methods are
aggregated together, to enable accurate and robust segmenting for non-ideal
iris images captured by either NIR or visible light cameras.
– We demonstrate that introducing training constrains to the discriminator of
a GAN, and geometry constrains to the boundaries of an iris region can both
improve the performance of iris segmentation.
– We evaluate our approach in both ideal and non-ideal datasets using three well
accepted metrics, i.e., error rate, IoU, and F1-score, and compare it with two
state-of-the-art methods. Experimental results show that, for non-ideal iris
images, our approach signiﬁcantly outperforms both methods in all metrics.
2
Related Work
Previous iris segmentation methods usually aim at ideal iris images acquired
in an ideal environment. Hough transform is used for detecting circular

118
S. Azhar et al.
boundaries from iris images, and then the edge map voting based on the con-
straints of the iris or pupil radius is performed to determine the iris image
boundary [23]. Thereafter, A new method using integral diﬀerential operators
is also carried out to detect circular iris boundaries in images [7]. [10] propose
an iris segmentation method based on radial suppression edge detection. They
employ wavelet transform to extract the wavelet transform modulus of the iris
image. According to the designed radial non-maximum suppression method, the
annular edge is retained and the radial edge is removed. Finally, the threshold
edge processing is used to remove the isolated edges and generate a binary edge
map.
Subsequent works note the interference caused by noise on the iris segmen-
tation, and begin to study the iris segmentation under non-ideal conditions. A
novel noise detection model is designed for accurate iris segmentation. In the
model, three conditions are used to determine the pixels in the eyelashes, which
solves the reﬂection noise [14]. Alternatively, the Adaboost eye detection is per-
formed to compensate for iris detection errors caused by two circular edge detec-
tion operations. The color segmentation method is used to solve the ghost eﬀect
noise, and the eyelid and eyelash detection is performed to reduce error [12].
Fig. 1. The overall structure of the proposed method. Our model ﬁrst feeds the input
image into the neural network to produce the segmentation prediction, and then applies
the reﬁnement method to obtain the ﬁnal segmentation result.
To detect iris boundaries more accurately, there are some methods using ﬂex-
ible contours. The active contours can be adapted to various shapes and segment
multi-object at the same time, further improving the accuracy and eﬃciency of
segmentation [21]. Considering the eﬀects of noise, a new method combining
ﬂexible contour with Hough transform is used for iris segmentation [13].
The deep learning approaches provide a new idea for the segmentation of
non-ideal iris images. Hierarchical CNN and multi-scale CNN are proposed for
iris segmentation [15]. Then, a two-step segmentation method based on deep
learning is proposed. Hough transform is used to locate the rough boundary,
and the deep learning model is used to detect the iris image region in the rough

A Hybrid Approach for Segmenting Non-ideal Iris Images
119
boundary [2]. Later, [3] propose a deeper dense CNN to improve segmentation
performance. Meanwhile, a new segmentation method using the graph theory to
optimize the hierarchical network structure is proposed [5].
3
Methods
In our approach, we design a CGAN to generate a prediction of iris region for a
non-ideal iris image. Then we use a set of boundary optimization methods based
on geometry constraints to reﬁne the prediction result, leading to an accurate
segmentation of iris region. The overview of our approach is shown in Fig. 1.
3.1
Iris Segmentation Using CGAN
The small diﬀerence in boundary and gradient features in non-ideal iris images
poses a challenge for iris segmentation. It is diﬃcult to deal with complex noise
interference using only image processing techniques based on gradients and geo-
metric relationships. Since neural networks have powerful feature extraction
capabilities, we choose neural network as the primary segmentation model for
non-ideal iris images. In our model, we use the U-Net [19] to segment iris image.
The U-Net uses a cascade structure in the up-sampling process to combine shal-
low features with deep features. Among them, deep features are used to locate
the iris area, and shallow features are used for accurate segmentation, which is
suitable for segmentation of non-ideal iris images. In the image segmentation
task, the CNN constructs the loss function using only the diﬀerence between
the prediction and ground truth pixels, ignoring the potential spatial continuity
in the segmentation image. It is diﬃcult to construct a loss function to force
CNN to learn such constraints. However, GAN can automatically learn such a
loss function that satisﬁes the constraint, because the goal of GAN is to train a
generator that can generate predictions that are indistinguishable from ground
truth. Therefore, we train an CGAN segmentation model to enable the genera-
tor to learn the statistical distribution of iris data, making the iris segmentation
prediction is indistinguishable from the iris label. And we use original image as
the condition, making the segmentation result corresponds to the original image
under the constraint of condition.
3.2
Objective
The generator G is trained to learn a map from the original iris image x to
the segmentation prediction ˆy, G(x) →ˆy. We use G(x) to denote the class
probability map over 2 classes of size H × W × 2 that the segmentation model
produces given an input iris image x of size H × W × 3. Unlike the standard
CGAN, we do not use the noise z together as the input to the generator, the
network can still use the iris image x to get the predicted segmentation result ˆy.
The original image x is also used as input of the discriminator, the discriminator
D learns to classify between fake (original iris image x, segmentation prediction

120
S. Azhar et al.
ˆy) and real (original iris image x, iris labels y) images. We concatenate the two
images of the same spatial Resolution along the depth axis. Both the generator
and discriminator observe the original iris image diﬀerent from an unconditional
GAN.
Adversarial Loss: We deﬁne the following adversarial loss in our model.
Ladv(G, D) = Ex,y[log D(x, y)] + Ex[log(1 −D(x, G(x))]
(1)
where the generator G tries to minimize this objective to fool the discrim-
inator D. Conversely, the discriminator D tries to maximize it to distinguish
between “real” and “fake” images.
Segmentation Loss: We use binary cross-entropy loss term to represent segmen-
tation loss.
Lseg = −Ex,y[y log G(x) + (1 −y) log(1 −G(x))]
(2)
Training the Generator: Our goal is to locate the iris and non-iris area from
a non-ideal iris image. Therefore, we employ segmentation loss to encourage
the generator to correctly predict the pixel location of the iris region. And the
adversarial loss is used to help the generator’s predicted output closer to the
real segmentation label. The objective function for the generator is deﬁned as
follows:
LG = Ladv + Lseg
(3)
Training the Discriminator: Since the adversarial loss is large if discriminator
can discriminate the output of the generator from ground-truth label maps. We
deﬁne the objective function for the discriminator as follows:
LD = −Ladv
(4)
3.3
Network Architecture
Our network uses the architecture of U-Net as the generator, which is a con-
volutional encoder and decoder with skip connections. The encoder consists of
a 3×3 convolution (we only use one convolution on each layer), each followed
by a rectiﬁed liner unit (ReLU) and a 2×2 max pooling operation with stride
2. The decoder consists of consists of an upsampling of the Feature map fol-
lowed by a 2×2 convolution, a concatenation with the correspondingly cropped
Feature map from the contracting path, and a 3×3 convolution, each followed
by a ReLU. The architecture of the VGG network is used in the discriminator,
it contains eight convolutional layers with 3×3 ﬁlter kernel. The feature maps
are followed by two dense layers and a ﬁnal sigmoid activation function, which
produce a probability for sample classiﬁcation.

A Hybrid Approach for Segmenting Non-ideal Iris Images
121
Fig. 2. Illustration of designed reﬁnement method. The operation steps are performed
in alphabetical order, and the iris prediction from neural network is as the input
of reﬁnement method. (A-B) remove the abnormal component, (C) is performed to
smooth boundary, (D-O) optimize boundary shape.
3.4
Reﬁnement Using Geometry Constraints
We observe the segmentation results from neural network appear small anoma-
lous segmentation components and the iris boundary is rough. To solve these
problems, we designed a new reﬁnement method based on geometry constraints.
The illustration of reﬁnement method is shown in the Fig. 2.
Remove Anomalous Component. Since the anomalous segmentation area is small
and discontinuous with the iris region, we ﬁrst perform connected component
analysis on the predicted segmentations of the neural network. Speciﬁcally, we
use the 8-connected components labeling algorithm to obtain the connected
regions in the graph, then calculate the size of each connected region, retain
only the connected component of the largest area, and then delete other con-
nected components. In this way, the anomalous segmentation component can be
eﬀectively removed.
Boundary Smoothing. We use the opening operation in morphology to smooth
the iris boundary. The open operation consists of two operations, corrosion and
expansion. The corrosion operation can eliminate the prominent boundary points
and enable the boundary to shrink toward the inside. Otherwise, the expansion
operation can ﬁll the gap in the boundary and expands the entire boundary
to the outside. The opening operation is ﬁrst performed by etching and then
expanding to remove isolated points and burrs on the boundary without causing
boundary deformation. Therefore, we eﬀectively smooth the iris boundary by
the opening operation.
Boundary Shape Optimization. The boundary shape optimization is performed
to further improve segmentation results. Due to the diﬀerent shapes of the outer
and inner boundaries, we deal with the inner and outer boundaries separately.

122
S. Azhar et al.
We ﬁrst use canny edge detection algorithm to detect the inner and outer
boundary curves of the iris segmentation image. After acquiring the iris boundary
images, we use the connected component labeling algorithm to mark the inner
and outer boundaries with diﬀerent colors. The inner and outer boundaries are
regarded as diﬀerent connected components, and are separated according to
diﬀerent color markings.
The normal pupil shape of a person is nearly circular, and we hope to optimize
the inner boundary by constraining the shape of the pupil boundary. Therefore,
we use the elliptical Hough transform for the inner boundary image of the iris,
and an ellipse on the inner boundary to replace the original pupil boundary.
Such a boundary is more reasonable.
Since the outer boundary of the iris is blocked by the upper and lower eyelids,
the shape of the outer contour consists of a curved portion and a straight portion.
In order to smooth the outer boundary of the iris, we want to ﬁnd those curved
portions and then use the boundary piecewise ﬁtting method to optimize the
outer boundary. Therefore, we design an algorithm using the slope information
to ﬁnd the demarcation point, and optimize the shape of outer boundary.
Algorithm 1. Boundary piecewise ﬁtting
Input: outer boundary
Output: optimized outer boundary
1: function Findcontour(pic)
2:
SIM ←InitialSIM()
3:
DP ←[]
4:
direct = [lower left, upper left, lower right, upper right]
5:
for i = 0; i < 4; i + + do
6:
switch direct[i] do
7:
case lower left
8:
x, y ←FindInitialPoint(pic, upper left)
9:
case upper left
10:
x, y ←FindInitialPoint(pic, lower left)
11:
case lower right
12:
x, y ←FindInitialPoint(pic, upper right)
13:
case upper right
14:
x, y ←FindInitialPoint(pic, lower right)
15:
SIM ←SlopeUpdate(pic, direct[i], x, y, SIM)
16:
P start, P end ←SelectPoint(pic, direct[i], x, y, SIM)
17:
DP ←P start, P end
18:
optimized outer boundary ←Polyfit(pic, DP )
19:
return optimized outer boundary
As shown in Algorithm 1, we ﬁrst initialize a slope information matrix SIM
with input image size to store the slope information of each pixel on the contour
line. Initially all values Set to 0. We use the four points on the left and right ends
of the top and bottom contour lines as the starting points for scanning in each
direction, and whether there are valid pixels points (when scanning to lower left,
the contour pixels in the left, down, and lower left directions are valid pixels) in
each scanning direction as the condition for judging the termination of scanning.
On the ﬁrst scan, we update the slope value of SIM. For example, when scanning
to the lower left, if there is a valid pixel in lower left direction, the value of the

A Hybrid Approach for Segmenting Non-ideal Iris Images
123
current position is updated to 1. And the other directions also update the slope
value of the SIM according to similar rules. After the ﬁrst scan, we need perform
a second scan to update the SIM value again. The second update is to ﬁnd the
start and end points of the curve part in each direction. During the scanning
process, We ﬁrst check if the horizontal coordinate interval between two adjacent
points with a SIM value of 1 is too large. If it is large, it is considered that the
contour between the two points does not belong to the curve part. Then another
pair of adjacent points continues to be judged until a small interval is found,
the ﬁrst of two adjacent points is set to 2 and is used as the starting point for
the curve part. After the starting point is found, a new judgment is started. We
judge whether the ordinate gap between two adjacent points with a SIM value
of 1 is too large. And if so, the ﬁrst of two adjacent points is set to 2 and the end
point is found. After the scanning ends, if there are no adjacent points where
the ordinate gap is large, the last point with a SIM value of 1 is used as the end
point. After ﬁnding the start and end points of the curve section, a polynomial
is used to ﬁt the points between the start and end points.
After acquiring the new inner and outer boundaries of the iris, we need
recover an iris mask image from these two boundaries images. First, we use
the hole ﬁlling algorithm to ﬁll the inside of the inner and outer boundaries,
respectively. Then we inversely convert the ﬁlled inner boundary region. Finally,
we integrate the two inner and outer boundary images into one iris segmentation
image.
4
Experiments
In this section, we describe our experiments from datasets, evaluation metrics,
implementation details, and evaluation results.
4.1
Datasets
CASIA v4 Interval. This database is a subset of CASIA Iris Image Database,
which is provided by NLPR Lab [4]. It contains 2,639 iris images from 249
subjects with an image resolution of 320∗280. This dataset is obtained through
the camera equipped a circular NIR LED array, with suitable luminous ﬂux.
Due to the lack of corresponding ground truth in this dataset, we use the EP
dataset generated by WaveLab [9], which contains the segmentation labels for
the CASIA v4 Interval dataset.
UBIRIS v2. This dataset contains a total of 11,102 iris images from 261 subjects,
which are 400∗300 in size [18]. We select 2,250 iris images as the dataset used
in our model. The iris images are captured by SOCIA Lab on non-constrained
conditions, including at-a-distance, on-the-move, and on the visible wavelength.
Therefore, the dataset has realistic noise factors. Similarly, the dataset contains
only iris images, and the ground truth of UBIRIS v2 is from EP dataset.

124
S. Azhar et al.
Perturbed CASIA v4 Interval. The texture feature of CASIA-Iris-Interval is clear
and it does not meet non-ideal condition. To obtain the various types of non-
ideal iris dataset, we use the augmentation method proposed in the Shabab’s
study [5]. The augmentation method simulates non-ideal iris images acquired in
real-life environments by reducing eye socket resolution, reducing image contrast,
shading images, and blurring images. We use the perturbed CASIA dataset to
verify the performance of the proposed method for non-ideal iris images.
4.2
Evaluation Metrics
We select several evaluation metrics to analyze the performance of our segmen-
tation method for non-ideal iris image. The evaluation metrics are deﬁned as
follows:
Error Rate. We refer to the evaluation criteria of the NICE I competition to
measure the performance of our proposed segmentation method. The segmenta-
tion error is represented as follows:
error =
FP + FN
TP + TN + FP + FN
(5)
where TP, TN, FP, FN represent the True Positive, True Negative, False Posi-
tive, False Negative, respectively.
Intersection Over Union. IOU is often used to assess the accuracy of predictions
in target detection tasks. Here we use IOU to measure the diﬀerence between
the mask predictions and ground truth. The deﬁnition of IOU is as follows:
IOU =
TP
FP + TP + FN
(6)
F1 Score. F1 score is an indicator used to measure the accuracy of classiﬁcation
model in statistics. It considers both the accuracy and the recall rate of the
classiﬁcation model. The F1 score is denoted as follows:
F1 score =
2 TP
2 TP + FP + FN
(7)
4.3
Implementation Details
Our network model is implemented in TensorFlow, and we implement reﬁnement
algorithms in Python. Our model is trained using Adam optimization algorithm,
and the momentum is 0.99. We use a random normal distribution initialization
to initialize the weight of the network. In experiments, the batch size is set to 10.
The learning rate is 0.0001 for the ﬁrst 10 epochs and linearly decay the learning
rate to 0 over the next 10 epochs. The generator and the discriminator perform
iterative alternation training, in which the generator performs one update after
ﬁve discriminator updates. A GTX1080Ti GPU server is utilized to train the

A Hybrid Approach for Segmenting Non-ideal Iris Images
125
networks. For all datasets, we use 60% of the dataset as training set, 20% as
validation set, and the remaining 20% as test set. To make up for the lack of
data, the data augmentation is done by horizontally ﬂipping the dataset. In
addition, we cut the input image into a 128∗96 size and feed it into network,
which keeps the image size of diﬀerent iris datasets consistent.
4.4
Comparision with the State of the Art
To compare the segmentation results of our method with other methods, we
select two state-of-the-art iris segmentation methods, including the iris recogni-
tion system OSIRIS v4.1 [16] and the iris segmentation framework IrisSeg [8].
The OSIRIS is an open source iris recognition framework integrating multiple
image processing methods. This framework does not require training and per-
forms well on ideal iris images. In the experiments, we only use its iris segmenta-
tion module. To ensure that the performance of OSIRIS is not aﬀected by unre-
lated factors, we set the maximum and minimum parameters of the iris radius
to the appropriate values before the experiment. The IrisSeg is a segmentation
framework for non-ideal iris images. This method allows for fast segmentation
and does not require parameter adjustment for diﬀerent iris datasets. In the
experiments, we directly use the framework to test on three diﬀerent datasets.
Quantitative Evaluation. In the quantitative experiment, we compare the seg-
mentation results with the other two methods on three evaluation indicators.
Table 1 is the experimental comparison results of the three datasets. In the table,
we can see that our proposed method achieves the best segmentation eﬀect on
the perturbed CASIA and UBIRIS datasets. Although our segmentation results
on the CASIA dataset are worse than IrisSeg, we can ﬁnd it that the diﬀerence
between the two is less than 1%. Meanwhile, on the other two datasets, the
results of the three indicators of our method are signiﬁcantly higher than the
other two methods. Moreover, since the CASIA dataset is a near-infrared dataset
without noise interference, and the other two datasets are non-ideal datasets con-
taining noise. Therefore, our approach has signiﬁcant advantages in segmenting
non-ideal iris images. In addition, we also calculated the standard deviation of
the segmentation results on each indicator. It can be seen from these ﬁgures
that the standard deviation of our method is the lowest on all three datasets,
indicating that the performance of our method is the most stable. Diﬀerent from
the other two methods, there are not many poor segmentation results in our
method.
Qualitative Evaluation. We also qualitatively compare our method with the
other two methods on the three datasets, which visually compares the quality
of the segmentation. We randomly select a test example from each dataset to
compare. Figure 3 shows the segmentation results of three diﬀerent methods.
Similarly, we can see that there is not much diﬀerence among the three segmen-
tation methods for CASIA dataset, but for the other two non-ideal iris datasets,

126
S. Azhar et al.
Table 1. The quantitative comparison
results of three segmentation methods.
Dataset
Method Err%
IOU%
F1-score%
UBIRIS
OSIRIS 23.91±3.88 14.58±3.60
19.36±5.54
IrisSeg
16.78±1.96 17.71±5.08
22.33±7.18
Ours
0.74±0.01 88.86±5.37 93.95±4.35
Perturbed CASIA OSIRIS 4.43±0.16
85.93±8.99
92.02±6.82
IrisSeg
6.99±0.65
72.40±24.28 77.96±24.35
Ours
2.06±0.02 92.81±2.35 96.25±1.33
CASIA
OSIRIS 3.54±0.07
86.91±7.12
92.77±4.80
IrisSeg
0.97±0.01 95.23±4.76 97.48±2.89
Ours
1.44±0.01 94.94±1.56 97.40±0.85
Table
2. The quantitative comparison
results of ablation study.
Dataset
Method
Err%
IOU%
F1-score%
UBIRIS
w/o CGAN
0.92±0.01
86.65±6.39
92.63±5.18
w/o reﬁnement 0.84±0.01
87.58±7.01
93.09±6.59
Ours
0.74±0.01 88.86±5.37 93.95±4.35
Perturbed CASIA w/o CGAN
3.05±0.06
89.60±5.32
94.40±3.46
w/o reﬁnement 2.23±0.02
92.29±2.83
95.97±1.63
Ours
2.06±0.02 92.81±2.35 96.25±1.33
CASIA
w/o CGAN
2.00±0.01 92.97±2.26
96.34±1.27
w/o reﬁnement 1.46±0.01 94.86±1.52 97.48±2.89
Ours
1.44±0.01 94.94±1.56 97.36±0.83
Fig. 3.
The
qualitative
comparison
results of three segmentation meth-
ods. These results are from randomly
selected
test
samples
on
three
iris
datasets.
Fig. 4. The qualitative results on three
iris datasets: the best (bottom) and
worst (top) cases in terms of error.
Green and red pixels represent the False
Positives and False Negatives, respec-
tively. (Color ﬁgure online)
the segmentation result of our method is closest to ground truth. It can eﬀec-
tively demonstrate that our method has the ability to segment the non-ideal iris
images. Figure 4 shows the worst and best segmentation results for our method
on each dataset. It can be seen that our worst results still have good accuracy
and the overall results are stable.
Ablation Study. To explore the eﬀect of the proposed CGAN model and reﬁne-
ment method on iris segmentation, we also design the experiments of ablation
studies, and analyze the performance of our proposed method by eliminating the
conditional adversarial network (w/o CGAN) and removing reﬁnement opera-
tions (w/o reﬁnement), separately. In the w/o CGAN setting, we choose U-Net
as a benchmark. We use only the generator for iris segmentation, no longer use
the discriminator and reﬁnement method. This veriﬁes the segmentation per-
formance of the network model when the CGAN is not used. We retain the
same hyperparameter conﬁguration as CGAN in our experiment. In addition,
we remove the entire reﬁnement method and use the designed CGAN model
to obtain the iris segmentation results, which helps to analyze the performance
improvements brought by reﬁnement method. Table 2 shows the comparison of
ablation experiments. We can see that our method achieves the best perfor-
mance on all three datasets. Although the standard deviation of our method
on the CASIA dataset is slightly higher than w/o reﬁnement method, the low-

A Hybrid Approach for Segmenting Non-ideal Iris Images
127
est standard deviation is presented on the other two datasets. It shows that
the CGAN model and the designed reﬁnement method can eﬀectively improve
the iris segmentation performance. From the comparison between the three, we
can ﬁnd that the segmentation model using CGAN is better than the segmen-
tation model without CGAN, which shows that segmentation performance can
be improved by CGAN structure and reﬁnement method. Figures 5, 6, and 7
illustrate CDFs for error rate, IOU, and F1 score, respectively. Similar to the
previous experiments, although our method is slightly worse than IrisSeg on
CASIA dataset, the performance gap on CASIA dataset is small enough. Con-
versely, We have great advantages in non-ideal iris image segmentation, which
demonstrates our iris segmentation method is best on the whole.
Fig. 5. CDFs of error rate.
Fig. 6. CDFs of IOU.
Fig. 7. CDFs of F1 score.

128
S. Azhar et al.
5
Conclusion
In this paper, we propose a hybrid approach based on CGAN and geometry
constraints to accurately segment non-ideal iris images. Real iris regions of eye
images are introduced into a CGAN, which can guide the generator to evolve
towards a stronger ability of iris region prediction. Meanwhile, we use the geome-
try constraints of iris shape to reﬁne the prediction results of the CGAN. Exten-
sive experiments on three diﬀerent iris datasets demonstrate that the proposed
approach achieves superior performance over the state-of-the-art methods.
Acknowledgement. This work was supported in part by the National Natural Sci-
ence Foundation of China (Grant No. 61972081), and the Natural Science Foundation of
Shanghai (Grant No. 22ZR1400200), the RGC RIF grant under the contract R602120,
and RGC GRF grants under the contracts 16209120, 16200221 and 16207922, and Key
R&D Plan of Shaanxi Province (Grant No. 2023-ZDLSF-20).
References
1. Alonso-Fernandez, F., Bigun, J.: Quality factors aﬀecting iris segmentation and
matching. In: 2013 International Conference on Biometrics (ICB), pp. 1–6. IEEE
(2013)
2. Arsalan, M., et al.: Deep learning-based iris segmentation for iris recognition in
visible light environment. Symmetry 9(11), 263 (2017)
3. Arsalan, M., Naqvi, R., Kim, D., Nguyen, P., Owais, M., Park, K.: IrisDenseNet:
robust iris segmentation using densely connected fully convolutional networks in
the images by visible light and near-infrared light camera sensors. Sensors 18(5),
1501 (2018)
4. of Sciences Institute of Automation, C.A.: Casia-irisv4 (2005).
5. Bazrafkan, S., Thavalengal, S., Corcoran, P.: An end to end deep neural network
for iris segmentation in unconstrained scenarios. Neural Netw. 106, 79–95 (2018)
6. Chen, Y., Adjouadi, M., Barreto, A., Rishe, N., Andrian, J.: A computational
eﬃcient iris extraction approach in unconstrained environments. In: 2009 IEEE
3rd International Conference on Biometrics: Theory, Applications, and Systems,
pp. 1–7. IEEE (2009)
7. Daugman, J.: How iris recognition works. In: The Essential Guide to Image Pro-
cessing, pp. 715–739. Elsevier (2009)
8. Gangwar, A., Joshi, A., Singh, A., Alonso-Fernandez, F., Bigun, J.: IrisSeg: a
fast and robust iris segmentation framework for non-ideal iris images. In: 2016
International Conference on Biometrics (ICB), pp. 1–8. IEEE (2016)
9. Hofbauer, H., Alonso-Fernandez, F., Wild, P., Bigun, J., Uhl, A.: A ground truth for
iris segmentation. In: 2014 22nd International Conference on Pattern Recognition,
pp. 527–532. IEEE (2014)
10. Huang, J., You, X., Tang, Y.Y., Du, L., Yuan, Y.: A novel iris segmentation using
radial-suppression edge detection. Signal Process. 89(12), 2630–2643 (2009)
11. Jain, A.K., Nandakumar, K., Ross, A.: 50 years of biometric research: accomplish-
ments, challenges, and opportunities. Pattern Recogn. Lett. 79, 80–105 (2016)
12. Jeong, D.S., et al.: A new iris segmentation method for non-ideal iris images. Image
Vis. Comput. 28(2), 254–260 (2010)

A Hybrid Approach for Segmenting Non-ideal Iris Images
129
13. Koh, J., Govindaraju, V., Chaudhary, V.: A robust iris localization method using
an active contour model and hough transform. In: 2010 20th International Confer-
ence on Pattern Recognition, pp. 2852–2856. IEEE (2010)
14. Kong, W., Zhang, D.: Accurate iris segmentation based on novel reﬂection and eye-
lash detection model. In: Proceedings of 2001 International Symposium on Intel-
ligent Multimedia, Video and Speech Processing. ISIMP 2001 (IEEE Cat. No.
01EX489), pp. 263–266. IEEE (2001)
15. Liu, N., Li, H., Zhang, M., Liu, J., Sun, Z., Tan, T.: Accurate iris segmentation in
non-cooperative environments using fully convolutional networks. In: 2016 Inter-
national Conference on Biometrics (ICB), pp. 1–8. IEEE (2016)
16. Othman, N., Dorizzi, B., Garcia-Salicetti, S.: Osiris: an open source iris recognition
software. Pattern Recogn. Lett. 82, 124–131 (2016)
17. Proen¸ca, H., Alexandre, L.A.: Iris recognition: analysis of the error rates regarding
the accuracy of the segmentation stage. Image Vis. Comput. 28(1), 202–206 (2010)
18. Proenca, H., Filipe, S., Santos, R., Oliveira, J., Alexandre, L.A.: The UBIRIS. v2: a
database of visible wavelength iris images captured on-the-move and at-a-distance.
IEEE Trans. Pattern Anal. Mach. Intell. 32(8), 1529–1535 (2009)
19. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomed-
ical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.
(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-24574-4 28
20. Roy, K., Bhattacharya, P., Suen, C.Y.: Unideal iris segmentation using region-
based active contour model. In: Campilho, A., Kamel, M. (eds.) ICIAR 2010.
LNCS, vol. 6112, pp. 256–265. Springer, Heidelberg (2010). https://doi.org/10.
1007/978-3-642-13775-4 26
21. Shah, S., Ross, A.: Iris segmentation using geodesic active contours. IEEE Trans.
Inf. Forensics Secur. 4(4), 824–836 (2009)
22. Thavalengal, S., Bigioi, P., Corcoran, P.: Iris authentication in handheld devices-
considerations for constraint-free acquisition. IEEE Trans. Consum. Electron.
61(2), 245–253 (2015)
23. Wildes, R.P.: Iris recognition: an emerging biometric technology. Proc. IEEE 85(9),
1348–1363 (1997)

3D-B2U: Self-supervised Fluorescent
Image Sequences Denoising
Jianan Wang1, Hesong Li1, Xiaoyong Wang1(B), and Ying Fu1,2
1 MIIT Key Laboratory of Complex-Field Intelligent Sensing,
Beijing Institute of Technology, Beijing 100081, China
wangxiaoyong@bit.edu.cn
2 Yangtze Delta Region Academy of Beijing Institute of Technology,
Jiaxing 314019, China
Abstract. Fluorescence imaging can reveal the spatiotemporal dynam-
ics of life activities. However, ﬂuorescence image data suﬀers from photon
shot noise due to a limited photon budget. Therefore, denoising ﬂuo-
rescence image sequences is an important task. Existing self-supervised
methods solve the problem of complex parameter tuning of non-learning
methods and the problem of requiring a large number of noisy-clean
image pairs for supervised learning and become state-of-the-art methods
for ﬂuorescent image sequences denoising. However, they aim at 2D data,
which cannot make good use of the increased time dimension informa-
tion of ﬂuorescence data compared with single image data. Besides, they
still use paired noisy data to train models, and the strong prior infor-
mation brought by paired data may lead to the overﬁtting of the model.
In this work, we extend existing self-supervised methods to 3D and pro-
pose a 3D global masker that introduces a visible blind-spot structure
based on 3D convolutions to avoid identity mapping while fully utilizing
the input data information. Our method makes reasonable use of time
dimension information and enables the task of self-supervised denoising
on ﬂuorescent images to mine information from the input data itself.
Experimental results show that our method achieves a better denoising
eﬀect for ﬂuorescent image sequences.
Keywords: Self-Supervised Learning · Fluorescence Image Sequences
Denoising · 3D Global Masker
1
Introduction
Life activities are coordinated by a myriad of cellular and subcellular struc-
tures with complex spatiotemporal dynamics. Fluorescence microscopy has good
molecular speciﬁcity and high spatiotemporal resolution. It is an important tool
for studying cell and subcellular structures as it uses ultraviolet light or other
light sources of speciﬁc wavelengths to excite objects to emit ﬂuorescence, and
then observes the shape, position, and distribution of objects.
However, the use of ﬂuorescence microscopy also faces some challenges and
problems. Due to the insuﬃcient eﬃciency of existing ﬂuorescent indicators and
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 130–142, 2024.
https://doi.org/10.1007/978-981-99-8850-1_11

Self-supervised Fluorescent Sequences Denoising
131
the susceptibility of life activities to light-induced interference [14], low photon
counts inevitably generate shot noise. It greatly aﬀects the quality of ﬂuorescence
imaging, reducing the image signal-to-noise ratio.
To improve the image quality, some researchers start with experimental
equipment to obtain more photons by developing high-performance ﬂuorescent
indicators [28] or improving the performance of detection equipment [16]. How-
ever, these methods depend on the speciﬁc situation of the researcher and are
diﬃcult to be universal. In contrast, calculation-based denoising has its own
merit. It does not have high requirements for researchers’ equipment, so it has
good compatibility and great development potential.
At present, the image denoising algorithm has been greatly developed. The
earliest non-learning methods based on iterative calculation, such as NLM [4],
BM3D [7], etc., obtain the approximate result of denoising by looking for sim-
ilar blocks of the target. However, these traditional methods usually involve
complicated optimization, requiring manual adjustment of relevant parameters
according to the noise situation of the data during the experiment. Therefore,
although traditional methods can eﬀectively deal with some noisy signals, such
methods often require a high time cost, and it is usually diﬃcult to achieve
optimal denoising performance.
With the development of deep learning, supervised learning methods such as
DNCNN [26] and FFDNet [27] have also shown good performance on denoising
tasks. However, this method requires a large amount of labeled data to train
the network, and the cost of obtaining noise-free images is often very high,
which makes it diﬃcult to collect eﬀective training data sets. In order to solve
the problem that supervised learning methods require a large amount of clean
data, self-supervised learning represented by Noise2Noise [15] began to develop.
Noise2Void [12], Neighbor2Neighbor [11], Blind2Unblind (B2U) [23] and other
methods have achieved performance close to or even reached the performance of
supervised learning methods.
However, current self-supervised learning methods are mainly aimed at 2D
images. In the ﬂuorescence image data, since most of the ﬂuorescence image
sequences record continuous activity information, there is a strong spatiotem-
poral correlation between the data frames. Therefore, self-supervised denoising
algorithms in 2D are diﬃcult to handle and exploit the spatiotemporal infor-
mation inherent in ﬂuorescence image sequences. At the same time, although
DeepCAD-RT [17] uses 3D-Unet [6] to utilize the spatiotemporal information
of the ﬂuorescence image sequence, it directly uses the adjacent frames of the
input data as the input and target of the model, which does not dig deep into the
image information. In addition, the information correlation between the input
frame and the target frame is too high, and too strong prior knowledge may also
cause the model to overﬁt the training set.
In this work, we extend existing self-supervised methods to 3D and propose
a 3D global masker that introduces a visible blind-spot structure based on 3D
convolutions to avoid identity mapping while fully utilizing the input data infor-
mation. Our method makes reasonable use of time dimension information and

132
J. Wang et al.
enables the task of self-supervised denoising on ﬂuorescent images to mine infor-
mation from the input data itself. Experimental results show that our method
achieves better denoising eﬀect for ﬂuorescence data.
2
Related Work
In this section, we brieﬂy describe the development of image denoising methods
and the study of self-supervised denoising algorithms on ﬂuorescence image data.
2.1
Image Denoising
Non-learning Image Denoising. Traditional non-learning denoising methods,
such as NLM [4], BM3D [7], CBM3D [8], WNNM [9], etc., perform denoising
iteratively by using similar mean values as a strategy. However, these methods
are often constrained by complex optimization and hyperparameter tuning, as
well as long denoising inference time and limited performance improvement.
Supervised Image Denoising. In recent years, many supervised image denois-
ing methods have been proposed. Zhang et al. ﬁrst proposed DnCNN [26], which
uses noise-clean pairs as supervision to deal with unknown noise levels. Sub-
sequently, they further proposed FFDNet [27] with improved adaptability to
diﬀerent noise levels. At the same time, as a general-purpose denoiser based on
multi-scale features, U-Net [21] has also achieved good results in image denoising
tasks. Besides that, many improvements have also been contributed to supervised
methods [1,5,10,25]. However, supervised denoising methods require a large
amount of labeled data to train the network, however, the cost and diﬃculty
of obtaining noise-free images are often very high, which makes it diﬃcult to
collect high-quality datasets and increases the diﬃculty of model training.
Self-supervised Image Denoising. Supervised deep learning denoising meth-
ods usually require a large number of clean images as training labels, which are
diﬃcult or cost-prohibitive to obtain in most scenarios. Therefore, a challeng-
ing and practical question is how to train eﬀective denoising models without
clean data. Noise2Noise [15] was the ﬁrst to use paired noisy data to train the
model, thereby alleviating the need for data. Then, Noise2Self [2] and Noise2Void
[12] proposed a masking scheme for denoising a single noisy image. Since the
mask covers some areas of the noisy data, this practice will cause the loss of
information, thus aﬀecting the ability to denoising. Noisier2Noise [19] intro-
duces additional noise, but this method requires a known noise distribution,
making it diﬃcult to handle data with unknown noise distributions. Neigh-
bor2Neighbor [11] obtains noise pairs for training by subsampling noise images,
but subsampling destroys the integrity of pixel results. Recently, Wang et al. pro-
posed Blind2Unblind [23], which eﬀectively solves the information loss caused
by the blind spot structure by introducing a globally aware mask mapper and
re-visibility loss on top of the blind structure.

Self-supervised Fluorescent Sequences Denoising
133
2.2
Self-supervised Denoising on Fluorescence Images
As an advanced signal processing technique, deep learning has been adopted by
microscopists and achieved good performance in ﬂuorescence imaging [3,20,24].
However, the highly dynamic nature of physiological activities makes it no longer
feasible to collect high-quality supervised images. Li et al. proposed DeepCAD
[18], which introduced the Noise2Noise [15] method into the ﬂuorescence image
denoising task. Subsequently, DeepCAD-RT [17] was further developed, and the
speed of denoising processing was greatly accelerated by methods such as prun-
ing. However, existing ﬂuorescence image self-supervised algorithms still rely on
paired noisy images for training, which may provide image prior information to
the model due to the high similarity between input and target images, which
may make it easy for the model to learn the identity mapping, making it diﬃcult
to achieve better performance [23]. In this work, our proposed 3D global masker
can not only maintain the model’s ability to process 3D ﬂuorescence data, but
also eﬀectively avoid the model from being limited by the noise prior and identity
mapping, thereby obtaining stronger information digging ability.
3
Method
In this section, we comprehensively introduce our research motivation, the overall
framework of our method and the 3D global masker.
3.1
Motivation
Since the Noise2Noise [15] method was proposed, self-supervised denoising meth-
ods have been greatly developed and improved. For example, some methods
[2,12,13] introduce blind spot structure, so that the model no longer needs paired
data for training. On this basis, the Blind2Unblind method goes a step further.
Through the introduction of a global-aware mask mapper and re-visibility loss,
the model can eﬀectively alleviate the information loss problem caused by the
blind spot structure, while mining information from the input data itself.
However, we note that in the task of denoising ﬂuorescence image data,
the recently proposed DeepCAD-RT [17] method is still designed following the
Noise2Noise method, using adjacent frames of the input image sequence as the
input and target of the model. Therefore, we decided to propose a 3D global
masker, which introduces a visible blind spot structure based on 3D convolution
to avoid identity mapping while fully using the input data information, so that
the model is free from information loss and identity mapping, thus improving
the ability of the model to mine information from the input data.
3.2
Framework
In this work, we obtain the interpolated sequence Ω(y) through the operation
of splitting-mask-interpolation-merging from the original image sequence y. We

134
J. Wang et al.
let the interpolated sequence Ω(y) through the denoising network f(the 3D-
Unet model is used in this work) to get f(Ω(y)). The f(Ω(y)) and the mask
sequence are operated under the global mask mapper h to obtain the prediction
result h(f[Ω(y)]). We subtract the original image sequence from the inference
result to get the diﬀerence d, and use d to calculate the regular constraint loss
of the model. The calculation method is
Lreg = α mean(d2).
(1)
At the same time, the original image sequence y directly passes through the
denoising network f, and the prediction result of the non-blind spot sequence
f(y) can be obtained, which provides the missing pixel information under the
blind spot structure for the learning of the model. By making the diﬀerence
between f(y) and y, we get drev, the revisible results (r) obtained by the
weighted combination of drev and d can be used to calculate the revisible loss of
the model, the calculation method of r is
r = d + β drev,
(2)
and the speciﬁc calculation method of the revisible loss is
Lrev = mean(r2).
(3)
The denoising network f adds the regularized constraint loss and the revisit
constraint loss to obtain the loss function used to update the parameter weights.
The speciﬁc calculation method is
Lall = Lreg + Lrev.
(4)
Among them, α and β are the hyperparameters preset by the experimenter, and
mean is the mean value operation.
The pipeline of the whole model is shown in Fig. 1. For an input image
sequence y, we mask it with a 3D global masker Ω according to a ﬁxed order,
and sample the masked result using a 3D global-aware mask mapper h to
obtain h(f[Ω(y)]) through a denoising network f. Besides, the unmasked image
sequence y directly passes through the denoising network f to obtain a prediction
result sequence f(y) without loss of information. The function of ‘⊕’ is to weigh
the interpolation prediction result and the direct prediction result to obtain
the prediction result with heavy visible information. Through the weighted cal-
culation results of h(f[Ω(y)]) and f(y), a denoising prediction sequence with
re-visibility information is obtained. For h(f[Ω(y)]) and revisible results, the
regularization loss and the revisible loss are computed respectively with y to
guide the parameter iteration of the model.
3.3
3D Global Masker
Through the analysis of DeepCAD-RT [17], it can be found that by applying
3D-Unet to the denoising of ﬂuorescent image sequences and referring to the

Self-supervised Fluorescent Sequences Denoising
135
self-supervised learning idea of Noise2Noise [15], DeepCAD-RT has achieved
good results in the task of denoising ﬂuorescent image sequences. However, the
idea of self-supervised learning denoising is to mine information from the unla-
beled data itself. During the training process, DeepCAD-RT directly uses the
adjacent frames of the image sequence as the input and target of the model.
In addition to the information on the ﬂuorescence image sequence itself, the
additional supervision information brought by the high correlation between the
input and the target will also guide the training of the model, which may lead
to the occurrence of overﬁtting. To address these issues, We propose a 3D global
masker.
Fig. 1. An overview of our overall pipeline, where the 3D global masker uses 3D con-
volution to interpolate the input image sequence y. The role of ‘⊕’ is to weigh the
interpolated predictions with the direct predictions. Revisibile Results are the fusion
of the prediction result of the interpolated sequence and the direct prediction result.
Speciﬁcally, we made the following improvements to the 3D global masker:
we rewrote the mask module in the Blind2Unblind method so that it can
handle the input tensor of [N, C, D, H, W], and Returns the Mask input for
[N × c2, C, D, H, W], where c is the side length of the unit that divides the
image in the Blind2Unblind [23] method. In the process of interpolating the
image sequence, we use 3D convolution to interpolate the input image sequence,

136
J. Wang et al.
and the pixel value of the interpolated position is provided by the neighboring
pixel values of this frame and its adjacent frames. Unlike the 2D-B2U method,
which interpolates only one image at a time, we interpolate the same position
of the entire image sequence at a time and generate the corresponding interpo-
lation sequence results, then stitch the interpolated image sequences together as
the input of the denoising model. The speciﬁc details of our 3D Global Masker
are shown at the bottom of Fig. 1.
4
Experimental Results
In this section, we comprehensively introduce the datasets, comparison methods
and metrics used in our experiments, as well as the denoising results in diﬀerent
noisy data scenarios.
4.1
Experimental Settings
Comparison Methods. In order to demonstrate the eﬀectiveness of our
method, we choose a variety of comparative methods in the experiment, includ-
ing a non-learning method (BM3D [7]), a supervised method (3D-Unet [6]) and
two self-supervised learning methods. Self-supervised learning methods include
the latest Blind2Unblind [23] method and the DeepCAD-RT [17] method for
denoising on ﬂuorescent image sequences. Among them, all methods are pub-
licly available. To ensure fairness, the input size of the DeepCAD-RT method
is set to be the same as our 3D-B2U, 3D-Unet and 3D-B2U use the same net-
work as DeepCAD-RT, and all other parameters are consistent with the relevant
original papers.
Datasets. The data set used in this work was produced by Li et al. [17], the
data set is divided into two parts, including synthetic calcium imaging data
with diﬀerent signal-to-noise ratios (SNR) and 10 sets of high-low SNR image
sequence pairs of real images. Among them, synthetic calcium imaging data
was produced with silicon Neural Anatomy and Optical Microscopy (NAOMi)
[22], through the use of analog methods to create images, it can provide noise-
free ground truth images that cannot be obtained during normal experimental
shooting. The simulated data has very similar spatio-temporal characteristics
to the experimentally obtained data. By adding Poisson-Gaussian mixed noise
with Poisson noise as the main noise source to the noise-free data, diﬀerent SNR
caused by diﬀerent relative photon numbers can be simulated. Real-noise data
are captured synchronously by a two-photon microscopy system. The data sce-
narios are real physiological activity data, including mouse dendritic activity,
zebraﬁsh brain activity, and ATP release in mouse brain, etc. For each physio-
logical activity, this dataset provides high-low SNR sequence pairs. The complete
data set is about 250G in total and can be downloaded from the public link.

Self-supervised Fluorescent Sequences Denoising
137
Metrics. We follow the metrics of Li et al. [17], using SNR to evaluate denoising
results on synthetic data, and Pearson correlation coeﬃcient (R) to evaluate
denoising results on real data.
For synthetic data, since it can provide the ground truth, we can directly
calculate the SNR between the denoised result and the ground truth. For the
real data, due to the lack of ground truth, we complete the denoising process on
the low SNR data, and calculate the Pearson correlation coeﬃcient between the
denoising results and the corresponding high SNR data. The Pearson correlation
coeﬃcient is a statistical concept which is used to measure the degree of linear
correlation between two variables, and its value range is [−1, +1]. The higher the
result, the stronger the correlation between the two data. Its speciﬁc deﬁnition is
R = E[(x −μx)(y −μy)]
σxσy
,
(5)
where x is the data to be measured, and y is the corresponding high SNR data.
μx and μy are the mean values of x and y, respectively. σx and σy are the
standard deviations of x and y, respectively.
4.2
Synthetic Data Denoising
Due to memory limitations, we control DeepCAD-RT to use the same scale input
as 3D-B2U to ensure fairness. Besides, we train the model with default settings
and compare the denoising results of diﬀerent denoising methods on synthetic
data. All methods perform denoising inference on the last 2000 frames on the
−2.51 dB data and compute the SNR with the corresponding ground truth. It is
worth noting that the 3D-Unet uses the same network model as DeepCAD-RT,
using noisy data and ground truth values as the input and target of the model.
The comparison of inference results of each method on synthetic data is shown
in Fig. 2. Table 1 shows the SNR of the inference results of each method.
For the DeepCAD-RT method, paired image pairs are used for training dur-
ing training. During the training process, the model fails to fully utilize the
information of the input data, moreover, the obtained information also includes
strong prior information between the target and the input. When we control the
size of the input data of the DeepCAD-RT method to be the same as that of
3D-B2U, the additional information between the ”input-target” pairs that the
model can obtain becomes less. The 3D-B2U method uses a global mask based
on 3D convolution, which can focus more on mining information from the input
data itself, so it can still maintain a strong denoising ability under the premise
of small input data. The experimental results conﬁrmed our inference.

138
J. Wang et al.
Fig. 2. Denoising results of diﬀerent methods for synthetic data with the same SNR
level (−2.51 dB).
Table 1. Comparison of the denoising results of each method in the scene of synthetic
noise data. The denoising task is performed on the data with an initial SNR of −2.51 dB,
and the denoising eﬀect is evaluated by calculating the SNR between the denoising
result and the ground truth. The higher the SNR, the better the denoising eﬀect.
BM3D [7]
3DUNet [6]
B2U [23]
DeepCAD-RT [17]
3D-B2U (ours)
Params (K)
0
1020
1099
1020
1020
Flops (G)
N/A
29.68
315.47
29.68
298.33
SNR (dB)↑
6.21
21.45
17.88
19.76
21.89
4.3
Real Data Denoising
We selected four real noise scenes such as the Zebraﬁsh multi brain dataset to
compare the denoising eﬀects of diﬀerent methods. Since the real scene data has
no ground truth, we measure the quality of the denoising eﬀect by calculating the
Pearson correlation coeﬃcient of the high SNR data and the denoising results.
The comparison of inference results of each method on synthetic data is shown
in Fig. 3. Table 2 shows the inference performance of diﬀerent methods on the
real data. It can be seen from the results that the 3D-B2U method basically
achieves the same denoising ability as the state-of-the-art method.
Fig. 3. Denoising results of diﬀerent methods on real data scenarios. The ﬁrst row is the
Zebraﬁsh multiple brain regions data, and the second row is the mouse neurites data.

Self-supervised Fluorescent Sequences Denoising
139
Table 2. Comparison of the denoising results of each method in the real noise data
scene. The denoising eﬀect was evaluated by calculating the Pearson correlation coeﬃ-
cient (R) between the denoising results and the corresponding high SNR data. A higher
R indicates that the denoising result is more correlated with the corresponding real
high SNR data.
Data
BM3D [7] 3D-UNet [6] B2U [23] DeepCAD-RT [17] 3D-B2U (ours)
Zebraﬁsh Multiple Brain
0.1449
0.7907
0.7142
0.8044
0.8019
Mouse Dendritic Spines
0.2227
0.4560
0.3171
0.4526
0.4677
Drosophila Mushroom Body 0.0437
0.8431
0.7847
0.8427
0.8474
ATP in Mouse Brain
0.3791
0.5133
0.4722
0.4982
0.5128
5
Ablation
In this work, we extend the B2U [23] method to handle 3D data, and design a
3D Global Masker to more eﬃciently utilize the spatiotemporal information in
3D data. We conduct ablation experiments to verify the eﬀectiveness of our 3D
extension method and the proposed 3D Global Masker.
Speciﬁcally, in the 3D extension of the B2U method, for a single frame of
an image sequence, we interpolate according to a given c. After we have all the
interpolated sub-images for that image, we put these interpolated results in an
image stack and repeat the operation for the next image. After completing the
interpolation of all the pictures in the input sequence, the obtained results are
spliced together to become the input sequence Ω(y) of the model. The 3D Global
Masker we proposed goes a step further and realizes a 3-dimensional visible blind
spot structure through 3D convolution. To better utilize the information of the
input data, we consider the input data batch as a whole. We interpolate all
frames of the input sequence simultaneously according to the given c and put
the result in an image stack. After all position interpolation is completed, the
image sequence in the stack is the input sequence Ω(y) of the model.
3D Extension Method. Since the ﬂuorescence data sequences we deal with
have the characteristics of high dynamics and high complexity. In the image
sequence, the information of adjacent frames is actually an important feature of
the data itself, while the 3d extension of B2U only interpolates a single frame on
the image sequence and stitches it, and the interpolation result is only provided
by the pixel neighborhood in this frame. In contrast, after we extend B2U to
3D, since we use 3D-Unet as the denoising network, the interpolation results are
inspired by 3D ﬁlters, and the larger data receptive ﬁeld increases the ability of
data to mine its own information.
3D Global Masker. Compared with the method of interpolating a single
frame of the image sequence and then splicing it, the interpolation operation
of the entire image sequence can better preserve the spatiotemporal information
between adjacent frames of the input data. This is helpful for the model to make
better use of data for training. Although the end of the previous interpolation
part sequence is connected to the head of the current interpolation part at the

140
J. Wang et al.
splicing of the image sequence, for the model as a whole, most of the spatiotem-
poral information of the input data is “ordered”. We believe that this ordered
spatiotemporal information can make up for the disadvantage of unordered spa-
tiotemporal information at the splicing, and help to improve the denoising ability
of the model.
The quantitative results of the ablation experiments are shown in Table 3.
From the experimental results, we can see that by extending B2U to 3D, we
achieve better performance on 3D data. Besides, the introduction of 3D Global
Masker further improves the model’s ability to mine data information.
Table 3. Results of ablation experiments. Veriﬁed the eﬀectiveness of B2U’s 3D exten-
sion and 3D Global Masker. Among them, B2U+3D represents the 3D extension of
B2U, and 3D-B2U represents the 3D extension of B2U after the introduction of 3D
Global Masker.
Data
Metric
B2U
B2U+3D 3D-B2U
Synthetic Calcium Data (−2.51dB) [17] SNR (dB) 17.88
21.77
21.89
Mouse Dendritic Spines [17]
R
0.3171 0.4668
0.4677
Mouse Neurites [17]
R
0.3676 0.4506
0.4528
6
Conclusion
In this work, we extend existing self-supervised methods to 3D and propose a
3D global masker that introduces a visible blind-spot structure based on 3D
convolutions to avoid identity mapping while fully utilizing the input data infor-
mation. Our method makes reasonable use of time dimension information and
enables the task of self-supervised denoising on ﬂuorescent images to mine infor-
mation from the input data itself. Experimental results show that our method
can achieve better denoising eﬀect for ﬂuorescent image sequences.
Acknowledgement. This work was supported by National Key R&D Program of
China (2022YFC 3300704), and the National Natural Science Foundation of China
under Grants (62171038, 62171042, and 62088101).
References
1. Anwar, S., Barnes, N.: Real image denoising with feature attention. In: Proceedings
of the International Conference on Computer Vision, pp. 3155–3164 (2019)
2. Batson, J., Royer, L.: Noise2self: blind denoising by self-supervision. In: Proceed-
ings of the International Conference on Machine Learning, Proceedings of Machine
Learning Research, vol. 97, pp. 524–533 (2019)
3. Belthangady, C., Royer, L.A.: Applications, promises, and pitfalls of deep learning
for fuorescence image reconstruction. Nat. Methods 1215–1225 (2019)

Self-supervised Fluorescent Sequences Denoising
141
4. Buades, A., Coll, B., Morel, J.M.: A non-local algorithm for image denoising. In:
Proceedings of the Conference on Computer Vision and Pattern Recognition, vol.
2, pp. 60–65 (2005)
5. Chang, M., Li, Q., Feng, H., Xu, Z.: Spatial-adaptive network for single image
denoising. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020.
LNCS, vol. 12375, pp. 171–187. Springer, Cham (2020). https://doi.org/10.1007/
978-3-030-58577-8 11
6. C¸i¸cek, ¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3D U-net:
learning dense volumetric segmentation from sparse annotation. In: Ourselin, S.,
Joskowicz, L., Sabuncu, M.R., Unal, G., Wells, W. (eds.) MICCAI 2016. LNCS,
vol. 9901, pp. 424–432. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
46723-8 49
7. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image restoration by sparse
3d transform-domain collaborative ﬁltering. In: Image Processing: Algorithms and
Systems. SPIE Proceedings, vol. 6812, p. 681207 (2008)
8. Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.O.: Color image denoising
via sparse 3d collaborative ﬁltering with grouping constraint in luminance-
chrominance space. In: Proceedings of the International Conference on Image Pro-
cessing, ICIP, pp. 313–316 (2007)
9. Gu, S., Zhang, L., Zuo, W., Feng, X.: Weighted nuclear norm minimization with
application to image denoising. In: Proceedings of the Conference on Computer
Vision and Pattern Recognition, pp. 2862–2869 (2014)
10. Guo, S., Yan, Z., Zhang, K., Zuo, W., Zhang, L.: Toward convolutional blind
denoising of real photographs. In: Proceedings of the Conference on Computer
Vision and Pattern Recognition, pp. 1712–1722 (2019)
11. Huang, T., Li, S., Jia, X., Lu, H., Liu, J.: Neighbor2neighbor: self-supervised
denoising from single noisy images. In: Proceedings of the Conference on Com-
puter Vision and Pattern Recognition, pp. 14781–14790 (2021)
12. Krull, A., Buchholz, T.O., Jug, F.: Noise2void-learning denoising from single noisy
images. In: Proceedings of the Conference on Computer Vision and Pattern Recog-
nition, pp. 2129–2137 (2019)
13. Krull, A., Vicar, T., Prakash, M., Lalit, M., Jug, F.: Probabilistic noise2void:
unsupervised content-aware denoising. Front. Comput. Sci. 2, 5 (2020)
14. Laissue, P.P., Alghamdi, R.A., Tomancak, P., Reynaud, E.G., Shroﬀ, H.: Assessing
phototoxicity in live ﬂuorescence imaging. Nat. Methods 14(7), 657–661 (2017)
15. Lehtinen, J., et al.: Noise2noise: learning image restoration without clean data.
In: Proceedings of the 35th International Conference on Machine Learning, ICML.
Proceedings of Machine Learning Research, vol. 80, pp. 2971–2980 (2018)
16. Li, B., Wu, C., Wang, M., Charan, K., Xu, C.: An adaptive excitation source for
high-speed multiphoton microscopy. Nat. Methods 17(2), 163–166 (2020)
17. Li, X., et al.: Real-time denoising of ﬂuorescence time-lapse imaging enables high-
sensitivity observations of biological dynamics beyond the shot-noise limit. Nat.
Biotechnol. 282–292 (2023)
18. Li, X., et al.: Reinforcing neuron extraction and spike inference in calcium imaging
using deep self-supervised denoising. Nat. Methods 1395–1400 (2021)
19. Moran, N., Schmidt, D., Zhong, Y., Coady, P.: Noisier2noise: learning to denoise
from unpaired noisy data. In: Proceedings of the Conference on Computer Vision
and Pattern Recognition, pp. 12061–12069 (2020)
20. Ouyang, W., Aristov, A., Lelek, M., Hao, X., Zimmer, C.: Deep learning mas-
sively accelerates super-resolution localization microscopy. Nat. Biotechnol. 460–
468 (2018)

142
J. Wang et al.
21. Ronneberger, O., Fischer, P., Brox, T.: U-net: convolutional networks for biomed-
ical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.
(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-24574-4 28
22. Song, A., Gauthier, J.L., Pillow, J.W., Tank, D.W., Charles, A.S.: Neural anatomy
and optical microscopy (NAOMI) simulation for evaluating calcium imaging meth-
ods. J. Neurosci. Methods 358, 109173 (2021)
23. Wang, Z., Liu, J., Li, G., Han, H.: Blind2unblind: self-supervised image denoising
with visible blind spots. In: Proceedings of the Conference on Computer Vision
and Pattern Recognition, pp. 2017–2026 (2022)
24. Weigert, M., et al.: Content-aware image restoration: pushing the limits of ﬂuores-
cence microscopy. Nat. Methods 1090–10970 (2018)
25. Yue, Z., Yong, H., Zhao, Q., Meng, D., Zhang, L.: Variational denoising network:
toward blind noise modeling and removal. In: Advances in Neural Information Pro-
cessing Systems 32: Annual Conference on Neural Information Processing Systems,
pp. 1688–1699 (2019)
26. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:
residual learning of deep CNN for image denoising. IEEE Trans. Image Process.
26(7), 3142–3155 (2017)
27. Zhang, K., Zuo, W., Zhang, L.: Ffdnet: toward a fast and ﬂexible solution for CNN-
based image denoising. IEEE Trans. Image Process. 27(9), 4608–4622 (2018)
28. Zheng, Q., et al.: Ultra-stable organic ﬂuorophores for single-molecule research.
Chem. Soc. Rev. 43(4), 1044–1056 (2014)

Equivariant Indoor Illumination Map
Estimation from a Single Image
Yusen Ai1, Xiaoxue Chen2, Xin Wu1, and Hao Zhao2(B)
1 Key Laboratory of Machine Perception(MOE), School of AI,
Peking University, Beijing, China
ysai@pku.edu.cn
2 Institute for AI Industry Research,
Tsinghua University, Beijing, China
zhaohao@air.tsinghua.edu.cn
Abstract. Thanks to the recent development of inverse rendering, pho-
torealistic re-synthesis of indoor scenes have brought augmented reality
closer to reality. All-angle environment illumination map estimation of
arbitrary locations, as a fundamental task in this domain, is still challeng-
ing to deploy due to the requirement of expensive depth input. As such,
we revisit the appealing setting of illumination estimation from a sin-
gle image, using a cascaded formulation. The ﬁrst stage predicts faithful
depth maps from a single RGB image using a distortion-aware architec-
ture. The second stage applies point cloud convolution operators that
are equivariant to SO(3) transformations. These two technical ingredi-
ents collaborate closely with each other, because equivariant convolution
would be meaningless without distortion-aware depth estimation. Using
the public Matterport3D dataset, we demonstrate the eﬀectiveness of our
illumination estimation method both quantitatively and qualitatively.
Code is available at https://github.com/Aitensa/Img2Illum.
1
Introduction
Understanding the physical properties that can be used to generate an image,
which is often referred to as inverse rendering [13,32], is not only a fundamental
computer vision problem but also an enabling technique of emerging A/VR
applications. If this goal is ﬁnally achieved with high accuracy, we can insert
any objects into captured photos without human-perceptible artifacts. But this
is very challenging as the inverse rendering problem involves many sub-tasks
that are diﬃcult on their own. Among them, estimating the lighting condition
is an indispensable module.
While there exist other lighting parameterizations, we choose panoramic illu-
mination map [21,30] among alternatives due to its simplicity and expressiveness.
Speciﬁcally, the task is to infer a panoramic illumination map for a certain pixel
in a perspective RGB image as shown in Fig. 1. We ﬁrst recap two highly related
prior works as follows: (1) Neural Illumination [21] uses exactly the same setting
as ours, but it’s quite complicated, consisting of a geometry estimation module, a
diﬀerentiable warping module and an HDR reconstruction module. (2) PointAR
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 143–155, 2024.
https://doi.org/10.1007/978-981-99-8850-1_12

144
Y. Ai et al.
Fig. 1. The task is to infer a panoramic illumination map from a single perspective
RGB image and the ﬁrst step of our method is to infer a point cloud from RGB images.
We want the algorithm to be equivariant to SO(3) transformations. For example, for
the same point (e.g., a point on the ﬂoor) in two viewpoints shown in the left panel, we
want the illumination map to be equivariant to viewpoint changes. As such, depth map
distortion calibration becomes important because imposing equivariant convolution on
point clouds (middle panel) is meaningless.
[30] assumes that an RGB-D image, which can be converted into a point cloud, is
available as input. Then a point convolutional network directly extracts spherical
harmonics that approximates the illumination map, from input point clouds.
We note that PointAR is not applicable to most cellphones without depth
cameras, so we revisit the more generic single-image setting of Neural Illumina-
tion. Unfortunately, the network of Neural Illumination involves several dense
prediction modules that are also ineﬃcient for deployment. As such, we propose
a new cascaded formulation that ﬁrstly predicts depth from a single RGB image
and then applies a PointAR-like architecture to regress spherical harmonics from
the predicted point cloud.
This new formulation is conceptually simple but successfully pushing it to
the state-of-the-art performance level needs speciﬁc designs. The ﬁrst design is
introducing equivariant point convolution of [3]. The illumination map of the
same point (e.g., a point on the ﬂoor in the scene shown in Fig. 1) should be
equivariant to SO(3) transformation like the viewpoint changes in two rows of
Fig. 1. To clarify, since the predicted point cloud needs to be re-centered to
the point of interest as PointAR does, we only need to concern about SO(3)
equivariance instead of SE(3) equivariance. The second design is introducing
the distortion calibration technique proposed in [26]. It is widely known that
single-view depth estimation is troubled by incorrect depth scale and bias. As
shown in the middle panel of Fig. 1, without (scale/bias) distortion calibration

Equivariant Indoor Illumination Map Estimation from a Single Image
145
the point clouds generated from two viewpoints are completely diﬀerent. In this
case, SO(3) equivariance becomes meaningless so using calibrated piont clouds
(Fig. 1 right panel) is the right choice. To summarize, in this study:
– We propose a new framework that estimates panoramic illumination maps
from a single RGB image, which cascades a depth estimation network and a
network that estimates spherical harmonics from predicted point clouds.
– We introduce SO(3) equivariant point convolution and depth calibration into
the framework. Although they are existing techniques, we are the ﬁrst to show
their collaboration and signiﬁcant impact on illumination estimation.
– We benchmark on the large-scale public dataset Matterport3D, achieving
state-of-the-art results. Through ablations, we demonstrate the impact of
newly introduced modules. Codes are publicly available.
2
Related Work
Lighting Estimation has been a long-standing challenge in computer vision,
and is critical for real-world AR applications like realistic relighting and object
replacement. A direct way of capturing the illumination of an environment is to
use a physical probe [4]. This process, though accurate, can be expensive and
unsuitable for lighting estimation of diﬀerent locations. Another line of works
estimates illumination as a sub-task of inverse rendering [16,32], whose goal is to
jointly estimate intrinsic properties of the scene, e.g. geometry, reﬂectance, and
lighting from the input image. Classical methods formulate inverse rendering as
an energy optimization problem with heuristic priors [13]. With the rapid devel-
opment of deep neural networks, we could also learn generalizable models directly
from single images in a data-driven fashion. Existing works estimate environ-
ment lighting in simpliﬁed problem settings, such as outdoor scenes [12,29] and
objects [1,17]. In this work, we focus on more complex indoor environments,
where the spatially-varying eﬀects are not negligible.
For indoor scenes, Karsch et al. [13] recover parametric 3D lighting from a
single image assuming known geometry. Gardner et al. [10] propose to learn the
location and intensity of light sources in an end-to-end manner. However, their
models don’t handle spatially-varying lighting, i.e., diﬀerent locations within the
scene can have diﬀerent lighting. [9] improves it by representing lighting as a set
of discrete 3D lights with geometric and photometric parameters. Song et al. [21]
decompose illumination prediction into several simpler diﬀerentiable sub-tasks,
but suﬀer from spatial instability. Lighthouse [22] further proposes a multi-scale
volumetric lighting representation. Wang et al. [24] leverage a holistic inverse
rendering framework to guarantee physically correct HDR lighting prediction. Li
et al. [16] use 360◦panoramic images to obtain high-deﬁnition spatially-varying
lighting. Zhan et al. [28] solve illumination estimation via spherical distribution
approximation.
Meanwhile, to enable real-time AR applications on modern mobile devices,
[11,30] use the spherical harmonics (SH) lighting model for fast estimation. In
this work, we predict both the SH coeﬃcients and the irradiance map.

146
Y. Ai et al.
Equivariance is a promising property of feature representation. Compared
to invariance, equivariance maintains the inﬂuence of diﬀerent transformations,
ensuring stable and reasonable performance. In the ﬁeld of computer vision, since
neural networks are often sensitive to rotation transformations, a large body of
work has been proposed for rotation equivariance. Existing techniques could be
roughly divided into spectral and non-spectral methods.
Spectral methods usually design intrinsically rotation-equivariant basis func-
tions [23], and develop special network architectures with these basis func-
tions [5,14,20]. Tensor-ﬁeld based networks [7,8,23] implement convolutional
kernels in the spherical harmonics domain to make the features equivariant to
rotations and translations. However, spherical harmonics leads to high space and
time complexity. Deng et al. [5] propose a general framework built on vector acti-
vations to enable SO(3)-equivariance. Due to the nature of linear combination,
[5] fails to conduct ﬂexible vector transformations. Luo et al. [19] introduce ori-
entations for each point to achieve equivariance based on graph neural network
schemes in a fully end-to-end manner. As for non-spectral methods [3,6,15,27],
they discretize the rotation group and construct a set of kernels for the group
equivariant computation. EPN [3] introduces a tractable approximation to SE(3)
group equivariant convolution on point clouds. [27] further transfers this frame-
work to object-level equivariance for 3D detection. Du et al. [6] proposes to
construct SE(3) equivariant graph neural networks with complete local frames,
approximating the geometric quantities eﬃciently.
Estimated Depth
RGB
  
Predicted 
PointCloud
PointConv Layer 
SHC Feature Extraction
PointConv Layer 
BN-ReLu
SE(3) point conv
BN-ReLu
SPConv Block
SE(3) point conv
SE(3) point conv
BN-ReLu
SE(3) point conv
BN-ReLu
SPConv Block
SE(3) point conv
BN-ReLu
SE(3) point conv
BN-ReLu
SPConv Block
PointNet
Normalization
Equivariant Features
Equivariant Feature Extraction
Shift -Refinement
Point Cloud
Generation
Raw Depth Estimation
Concatenate
Sum
raw depth
d
SHC Feature
LDR/HDR
Irradiance Map
Reconstruction
Fig. 2. The overview of our framework. We propose a cascaded illumination estima-
tion formulation of two stages, composed of a point cloud generation module and an
equivariant illumination estimation module.
3
Method
3.1
Overall Architecture
Our goal is to estimate illumination from a single perspective RGB image. This
is an extremely ill-posed problem since diﬀerent lighting might lead to the same

Equivariant Indoor Illumination Map Estimation from a Single Image
147
appearance. Therefore, we choose to leverage geometric priors by predicting
depth from the image ﬁrst and then generating the corresponding point cloud at
the rendered position. Following [30], we formulate the illumination estimation
as a spherical harmonic coeﬃcients regression problem, and regress spherical
harmonics from the predicted point cloud.
However, such a framework still has potential issues. As shown in Fig. 1,
the images of the same scenario from various perspectives may lead to diﬀerent
distortion of predicted depth and mislead the limited illumination estimation,
especially in indoor scenes, let alone all-angle environment illumination map
estimation at arbitrary locations. Moreover, there is a basic fact for image-based
estimation: with light sources ﬁxed, the illumination is consistent when the ren-
dered point rotates or the viewpoint changes, which is namely the equivariance
of illumination. Recently, the depth estimation from a single RGB image has
made great progress [26], with a distortion-ware depth estimation paradigm and
oﬀers a precise depth estimation. In this case, the equivariance of SO(3) trans-
formations do make sense. The precise distortion-aware depth estimation can
not only guarantee the reliability of the combination of RGB and its predicted
depth, but also unleash the potential of equivariance in lighting estimation.
To this end, we propose a cascaded network in Fig. 2. It contains two stages,
generating point clouds with distortion-aware depth estimation and equivariant
illumination estimation from diﬀerent viewpoints. The model is composed of
a point cloud generation module D, an equivariant feature extraction module
E, a PointConv model P and a lighting estimation module R. r denotes the
rendered point. The formulation of our equivariant indoor illumination map
estimation from a single image is deﬁned as a mapping:
F : L(R(E(D(S, r)), P(D(S, r)))) →I
(1)
where S is the source image, I is the target illumination map, and L is the
transformation from SH coeﬃcients to illumination map. Speciﬁcally,
L(Ishc) = I,
R(er, Sf) = Ishc,
(2)
E(Pr) = er,
P(Pr) = Sf,
(3)
D(S, r) = Sample(S, g(S) ◦f(S), r) = Pr.
(4)
Pr is the generated point cloud, Ishc is the predicted spherical harmonics,
and er and Sf are both point features. g(S) and f(S) will be elaborated later.
Point Cloud Generation. In the ﬁrst stage, we generate the point cloud based
on the rendered point leveraging the distortion-aware depth estimation [26],
which is formulated as Eq. 4. Firstly, g(S) maps from an RGB image to a raw
depth estimation by a Depth Prediction Model (DPM), then f(S) takes the
raw depth as input and estimates a reﬁned shift for the raw depth through a
Point cloud Module (PCM), g(S)◦f(S) make reﬁnement shift on raw depth and
output a corrected depth image. Sample(S, g(S)◦f(S), r) make a point cloud Pr
centered at the render position r for subsequent process on the input estimated
depth and corresponding image, which will be described in Sect. 3.2.

148
Y. Ai et al.
Equivariant Illumination Estimation. In the second stage, we formulate
the equivariant illumination map estimation as a composite point cloud-based
learning problem L that takes a predicted point cloud Pr as input and outputs a
scene-consistent equivariant irradiance map. In this stage, we ﬁrst simultaneously
compute the raw estimate sphere harmonic coeﬃcient (SHC) feature Sf through
PointConv module P and extract the structure-aware equivariant feature er
through the equivariant feature extraction module E, then concatenate both
SHC and equivariant feature to acquire a 2nd order SH coeﬃcients Ishc through
Eq. 2. Finally, L(Ishc) inputs the SHC and outputs the target illumination map.
More details about the modules will be described in Sect. 3.3.
3.2
Point Cloud Generation
In this section, we will describe the details of generating a point cloud recentered
on the rendered point from a single RGB image.
Distortion-Aware Depth Estimation. Given an RGB image, there are two
modules in order for processing. The DPM module based on PVCNN [18], gen-
erates a depth image with unknown scale and shift, and the PCM module takes
the distorted point cloud as input and predicts shift reﬁnement to the depth
image.
Recentered Point Cloud Generation. Given the distortion-aware depth
image Z and camera intrinsic matrix, we can easily transform the depth image
into a point cloud P centered on the camera origin through:
x = (u −cx)z
fx
, y = (u −cy)z
fy
,
(5)
where u and v are the photo pixel coordinates, z is the corresponding depth value
of pixel (u,v), fx and fy are the vertical and horizontal camera focal length, cx
and cy are the photo-optical center, and (x,y,z) is the corresponding point of
pixel (u,v).
With the rendered point r, we apply a linear translation T to P and transform
the point cloud center to the observation point Pr:
Pr = T(P) = P −r.
(6)
Unit-Sphere Downsampling. Accounting for the eﬃciency of reserving the
spatial structure, we exert a new technique of sphere sampling – Unit-sphere
Downsampling [31]. At downsampling, for each input point cloud Pinput, we will
project the point cloud on a unit surface, and accumulate the area of the uniform
surface anchor’s coverage. Theoretically, let Pdata and Panchor correspondingly
be the input point cloud’s and the uniform surface anchors’ distribution, the com-
pleteness of observation was measured by the joint entropy H(Pdata, Panchor),
H(Pdata, Panchor) = −

i∈S
i

j=1
P(p′
ij, pi) log2[P(p′
ij, pi)]
(7)

Equivariant Indoor Illumination Map Estimation from a Single Image
149
where P(p′
ij, pi) s the joint probability of projecting points into a unit sphere
with i anchor point, and S is the set of possible anchors,

2k|1 ≤k ≤12

can
be accepted. In this work, we set 1280 points as the target for downsampling.
3.3
Equivariant Illumination Estimation
Equivariant Feature Extraction. For this module, we were inspired by recent
works on Equivariance [3,15,19,20,27], and based on [3], we put forward the cur-
rent network. For every RGB and corresponding point cloud input, we use three
basic SPConv blocks whose layer channel settings are [32,32],[64,128],[128,128]
and a Pointnet with channel [128,64] to output a 64-dimension Equivariant fea-
ture, which uses 60 SO(3) bases in this work. As to the other conﬁguration of
this module, the initial radius ratio = 0.2, sampleing ratio = 0.4.
SHC Feature Extraction. The module is a PointConv-based [25] backbone
derived from [30]. It takes downsampled RGB and the corresponding point cloud
as input, and outputs a 256-dimensional tensor, which we regard as an SHC
feature because it numerically describes SHC. The two PointConvs’ channel
numbers are set as [64,128] and [128,256] respectively.
Concatenating the above two features, we then use a fully connected layer
as the prediction head to get the predicted spherical harmonic coeﬃcients Ishc
and transform them into the illumination map.
3.4
Loss
To provide an auxiliary constraint on the outputs, we supervise both SH coeﬃ-
cients and the irradiance map, and the total loss function L is deﬁned as:
L = Lsh + Lir
(8)
Lsh is the L2 loss of SH coeﬃcients, which is deﬁned in Eq. 9:
Lsh = 1
9
3

c=l
2

l=0
l

m=−l
(im∗
l,c −im
l,c)
(9)
where c is the color channel (RGB), l and m are the degree and order of SH
coeﬃcients. Lir is L2 loss for irradiance map, and deﬁned in Eq. 10.
Lir =
1
Nenv
Nenv

p=0
(i∗
p −ip)2
(10)
where Nenv is the number of pixels of the target image, and i is the value of the
corresponding pixel.

150
Y. Ai et al.
4
Experiment
4.1
Datasets and Preprocessing
We carry out our experiments on the Matterport3D dataset [2], with illumina-
tion ground truth generated by Neural Illumination. Matterport3D is a large-
scale dataset that contains RGB-D images and panoramic views for indoor
scenes. Each RGB-D scene contains undistorted color and depth images (of
size 1028 × 1024) of 18 viewpoints. The Neural Illumination dataset [21] was
derived from Matterport3D and packed with additional information that asso-
ciates images and the relationship between images at observation and rendering
locations. We derive depth images from Matterport3D as Fig. 3 shows, whose
depth images come from the DepthEst module and Unit-Sphere Downsampling
module.
Fig. 3. Depth estimation results. The RGB images are from Matterpord3D, each col-
umn in order is RGB, predicted depth image from our DepthEst module and depth
image captured from the device.
4.2
Comparison in Quality and Quantity
Quantitative Results. As shown in Table 1, We carried out our experiments
based on the Matterport Dataset, we compare our solution with other illumi-
nation estimation solutions using L2 loss of SH coeﬃcients, Irradiance map on
the test set. For baseline solutions targeted at RGB-D, we keep the same setting
in experiments. And we achieved superior performance compared with previ-
ous arts. We credit this to our Equivariant feature extraction module which has
learned how to adjust to the transforms of the scenes and combined multiple
rotation cases’ tradeoﬀs during training, and ﬁnally calibrated and enhanced
the sources for SHC regression. And the robustness will be seen in qualitative
results.

Equivariant Indoor Illumination Map Estimation from a Single Image
151
Table 1. Comparison to state-of-the-art networks. Our approach achieved the
lowest loss for both spherical harmonics coeﬃcients l2 and irradiance map l2.
Method
SH coeﬃcients l2 Loss Irradiance map l2 Loss United l2 Loss
Song et al. [21]
N/A
0.619
N/A
Garon et al. [11] 1.10 (±0.1)
0.63 (±0.03)
N/A
PointAR [30]
0.31 (±0.03)
0.434 (± 0.02)
0.32 (±0.02)
Ours
0.11 (±0.05)
0.31 (±0.04)
0.20 (±0.04)
Also, we compare the complexity of the networks of illumination estimation
stage with PointAR [30] in Table 3. Account for the diﬀerence in target task
between PointAR and Ours, the increments of complexity are acceptable, so
that we believe that our model can still be applied on mobile platforms.
Table 2. Comparison of loss coeﬃcients.
Model
α
β
Valid Loss
PointAR [30] 1
10 9.04
5
10 29.60
10 1
5.77
Ours
1
10 0.20
5
10 0.17
10 1
1.36
Table 3. Comparison of model complexity.
Method
Parameters(M)
PointAR [30] 1.42
Ours
2.42
Qualitative Results. Here we demonstrate the quality of our method. Both
PointAR and our method take RGB-D input, use unite loss as supervision for
training and train 10 epochs. At test, we input the RGB-PD pairs and obtain
illumination estimation results as shown in Fig. 4. It is evident that our method
exhibits more detailed results. There are two main reasons for this improvement.
Firstly, This optimization by DPM eﬀectively reduces the impact of noise and
enhances the level of detail in our results. Secondly, our method leverages equiv-
ariance, which enables it to eﬀectively overcome slight perturbations. For further
generalization, incorporating additional equivariance bases becomes crucial.
4.3
Ablation Study
In this section, we describe the ablation experiments performed on the model set-
tings, sampling methods and loss function coeﬃcients. Among them, the model
setup needs to be especially noted that PointAR [30] is the degenerate model
and also the method of this paper, so the ablation experiments of the equivari-
ant module proposed in this paper will be obtained by comparing the method of
this paper with PointAR. As displayed in Table 1, compared with PointAR, our

152
Y. Ai et al.
Fig. 4. Comparison of illumination estimation. Each row is a partial image of a scene
and the column in order is the RGB image, GT illumination map, results of PointAR
and our method.
method attained a comparative result than PointAR, which shows the eﬃciency
of the equivariance in quantity.
Table 4. Ablation study of the sampling method in diﬀerent measurements.
Method
SHC coeﬃcients Loss Irradiance Map Loss Unite Loss
PointAR(w/o) 0.417
1.698
0.45
PointAR
0.914
0.009
0.32
Ours(w/o)
0.210
1.02
0.396
Ours
0.11
0.012
0.204
In Table 4, we conducted the comparative test on the use of sphere sample,
which is regarded as more geometry information. Obviously, both PointAR and
our method with the sphere sampling achieve better results, for instance, the SH
coeﬃcients loss decreased from 0.21 to 0.11 with sphere sampling.
As shown in Table 2, in our proposed method, the ﬁnal result varies from the
coeﬃcients of the loss function. We separately conduct experiments on diﬀerent
parameter settings, and measures on united valid loss. It needs to be mentioned
that the results are diﬀerent from the Table 1. Then we can arrive at that α =
5, β = 10 is a relatively optimal setting.
4.4
Application on AR
End-to-End Generation. As shown in Fig. 5, we build a pipeline for captur-
ing, uploading and attaining the irradiance map in Fig. 5. It demonstrates the
feasibility of the proposed framework.

Equivariant Indoor Illumination Map Estimation from a Single Image
153
Fig. 5. Demonstration from a snapshot RGB Image to its corresponding Irradiance
map.
Rendering. As shown in Fig. 6, the rabbit is rendered at the user’s preferred
location. Although the rendering result seems reasonable in human’s observation,
it still lacks some details.
Fig. 6. AR applications. Our method is tested in indoor scenes.
5
Conclusion
In this study, we propose a novel cascaded approach for estimating illumination
from a single RGB image. The ﬁrst stage utilizes a distortion-aware architecture
to accurately predict depth maps. In the second stage, a PointAR-like architec-
ture is employed to regress spherical harmonics from the predicted point clouds.
To ensure equivariance of the illumination map under SO(3) transformation,
we introduce equivariant point convolution for estimating spherical harmonics
coeﬃcients based on distortion-aware single-frame depth estimation. These two
techniques work closely together as distortion calibration is crucial to gener-
ate consistent point clouds from single images captured from diﬀerent view-
points. Experimental results demonstrate that our method achieves state-of-the-
art (SOTA) performance on the large-scale Matterport3D dataset, highlighting
the eﬀectiveness of the cascaded formulation and equivariance in illumination
modeling.

154
Y. Ai et al.
References
1. Boss, M., Jampani, V., Kim, K., Lensch, H., Kautz, J.: Two-shot spatially-varying
BRDF and shape estimation. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 3982–3991 (2020)
2. Chang, A., et al.: Matterport3D: learning from RGB-D data in indoor environ-
ments. arXiv preprint arXiv:1709.06158 (2017)
3. Chen, H., Liu, S., Chen, W., Li, H., Hill, R.: Equivariant point network for 3D
point cloud analysis. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 14514–14523 (2021)
4. Debevec, P.: Rendering synthetic objects into real scenes: bridging traditional and
image-based graphics with global illumination and high dynamic range photogra-
phy. In: ACM SIGGRAPH 2008 Classes, pp. 1–10 (2008)
5. Deng, C., Litany, O., Duan, Y., Poulenard, A., Tagliasacchi, A., Guibas, L.J.: Vec-
tor neurons: a general framework for so (3)-equivariant networks. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision, pp. 12200–12209
(2021)
6. Du, W., et al.: Se (3) equivariant graph neural networks with complete local frames.
In: International Conference on Machine Learning, pp. 5583–5608. PMLR (2022)
7. Esteves, C., Allen-Blanchette, C., Makadia, A., Daniilidis, K.: Learning SO(3)
equivariant representations with spherical CNNs. In: Ferrari, V., Hebert, M., Smin-
chisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11217, pp. 54–70. Springer,
Cham (2018). https://doi.org/10.1007/978-3-030-01261-8 4
8. Fuchs, F., Worrall, D., Fischer, V., Welling, M.: Se (3)-transformers: 3D roto-
translation equivariant attention networks. In: Advances in Neural Information
Processing Systems, vol. 33, pp. 1970–1981 (2020)
9. Gardner, M.A., Hold-Geoﬀroy, Y., Sunkavalli, K., Gagn´e, C., Lalonde, J.F.: Deep
parametric indoor lighting estimation. In: Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pp. 7175–7183 (2019)
10. Gardner, M.A., et al.: Learning to predict indoor illumination from a single image.
arXiv preprint arXiv:1704.00090 (2017)
11. Garon, M., Sunkavalli, K., Hadap, S., Carr, N., Lalonde, J.F.: Fast spatially-varying
indoor lighting estimation. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pp. 6908–6917 (2019)
12. Hold-Geoﬀroy, Y., Athawale, A., Lalonde, J.F.: Deep sky modeling for single image
outdoor lighting estimation. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pp. 6927–6935 (2019)
13. Karsch, K., Hedau, V., Forsyth, D., Hoiem, D.: Rendering synthetic objects into
legacy photographs. ACM Trans. Graph. (TOG) 30(6), 1–12 (2011)
14. Keriven, N., Peyr´e, G.: Universal invariant and equivariant graph neural networks.
In: Advances in Neural Information Processing Systems, vol. 32 (2019)
15. Li, J., Bi, Y., Lee, G.H.: Discrete rotation equivariance for point cloud recognition.
In: 2019 International Conference on Robotics and Automation (ICRA), pp. 7269–
7275. IEEE (2019)
16. Li, J., Li, H., Matsushita, Y.: Lighting, reﬂectance and geometry estimation from
360 panoramic stereo. In: 2021 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 10586–10595. IEEE (2021)
17. Li, Z., Xu, Z., Ramamoorthi, R., Sunkavalli, K., Chandraker, M.: Learning to recon-
struct shape and spatially-varying reﬂectance from a single image. ACM Trans.
Graph. (TOG) 37(6), 1–11 (2018)

Equivariant Indoor Illumination Map Estimation from a Single Image
155
18. Liu, Z., Tang, H., Lin, Y., Han, S.: Point-voxel CNN for eﬃcient 3D deep learning.
In: Advances in Neural Information Processing Systems, vol. 32 (2019)
19. Luo, S., et al.: Equivariant point cloud analysis via learning orientations for message
passing. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 18932–18941 (2022)
20. Shen, W., Zhang, B., Huang, S., Wei, Z., Zhang, Q.: 3D-rotation-equivariant
quaternion neural networks. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-
M. (eds.) ECCV 2020. LNCS, vol. 12365, pp. 531–547. Springer, Cham (2020).
https://doi.org/10.1007/978-3-030-58565-5 32
21. Song, S., Funkhouser, T.: Neural illumination: lighting prediction for indoor envi-
ronments. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 6918–6926 (2019)
22. Srinivasan, P.P., Mildenhall, B., Tancik, M., Barron, J.T., Tucker, R., Snavely,
N.: Lighthouse: predicting lighting volumes for spatially-coherent illumination. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, pp. 8080–8089 (2020)
23. Thomas, N., et al.: Tensor ﬁeld networks: rotation-and translation-equivariant neu-
ral networks for 3D point clouds. arXiv preprint arXiv:1802.08219 (2018)
24. Wang, Z., Philion, J., Fidler, S., Kautz, J.: Learning indoor inverse rendering with
3D spatially-varying lighting. In: Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pp. 12538–12547 (2021)
25. Wu, W., Qi, Z., Fuxin, L.: Pointconv: deep convolutional networks on 3D point
clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 9621–9630 (2019)
26. Yin, W., et al.: Learning to recover 3D scene shape from a single image. In: Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 204–213 (2021)
27. Yu, H.X., Wu, J., Yi, L.: Rotationally equivariant 3D object detection. In: Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 1456–1464 (2022)
28. Zhan, F., et al.: Emlight: lighting estimation via spherical distribution approxima-
tion. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 35, pp.
3287–3295 (2021)
29. Zhang, J., Sunkavalli, K., Hold-Geoﬀroy, Y., Hadap, S., Eisenman, J., Lalonde,
J.F.: All-weather deep outdoor lighting estimation. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10158–
10166 (2019)
30. Zhao, Y., Guo, T.: PointAR: eﬃcient lighting estimation for mobile augmented
reality. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020.
LNCS, vol. 12368, pp. 678–693. Springer, Cham (2020). https://doi.org/10.1007/
978-3-030-58592-1 40
31. Zhao, Y., Guo, T.: Xihe: a 3D vision-based lighting estimation framework for
mobile augmented reality. In: The 19th ACM International Conference on Mobile
Systems, Applications, and Services (2021)
32. Zhu, R., Li, Z., Matai, J., Porikli, F., Chandraker, M.: Irisformer: dense vision
transformers for single-image inverse rendering in indoor scenes. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
2822–2831 (2022)

Weakly-Supervised Grounding for VQA
with Dual Visual-Linguistic Interaction
Yi Liu1,2, Junwen Pan1, Qilong Wang1, Guanlin Chen1, Weiguo Nie2,
Yudong Zhang2, Qian Gao2, Qinghua Hu1, and Pengfei Zhu1(B)
1 College of Intelligence and Computing, Tianjin University, Tianjin, China
lly liuyi@tju.edu.cn
2 Baidu Inc, Beijing, China
Abstract. Visual question answer (VQA) grounding, aimed at locat-
ing the visual evidence associated with the answers while answering
questions, has attracted increasing research interest. To locate the evi-
dence, most existing methods extract attention maps in an unsupervised
manner from pretrained VQA models. As only the text-related objec-
tive is considered during training, the attention map coarsely depicts
the grounding region, resulting in poor interpretability. A straightfor-
ward solution for improving grounding accuracy is leveraging pixel-wise
masks as strong supervision. However, precise per-pixel annotation is
time-consuming and labor-intensive. To address above issues, this paper
presents the weakly-supervised grounding for VQA, which learns an end-
to-end Dual Visual-Linguistic Interaction (DaVi) network in a uniﬁed
architecture with various low-cost annotations, such as click-, scribble-
and box-level grounding labels. Speciﬁcally, to enable the visual mask
prediction, DaVi proposes a language-based visual decoder that extends
the previous VQA network. Since the visual decoder is guided with weak
labels, we also present a Pseudo Grounding Reﬁnement Module (PGRM)
to reﬁne the relatively coarse predictions as an additional constraint.
Extensive experiments demonstrate that our weakly supervised DaVi
signiﬁcantly improves grounding performance even under the click-level
supervision with one pixel annotation. Scribble-level supervision achieves
92% performance at a dramatically reduced annotation cost compared
to its fully supervised counterpart. More essentially, weak visual ground-
ing usually boosts the accuracy of text answers despite using inaccurate
supervision.
Keywords: Weakly-Supervised Learning · Visual Question
Answering · Visual Grounding
1
Introduction
Visual Question Answer (VQA) is a challenging task [27] that requires machines
to understand visual information and questions to provide accurate answers. It
has real-world applications, including aiding visually impaired individuals [7].
However, current high-performing models are complex black-box systems, which
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 156–169, 2024.
https://doi.org/10.1007/978-981-99-8850-1_13

Weakly-Supervised Grounding for VQA
157
raises concerns about answer reasoning and performance assessment [1]. VQA
grounding is crucial for addressing these challenges [7], as it provides explana-
tions for answer generation and helps evaluate answer quality and improve model
performance.
Fig. 1. VQA Grounding with diﬀerent supervision types. 1) Unsupervised grounding:
without any grounding labels during training. 2) Weakly supervised grounding: with
click-level, scribble-level or box-level annotations. 3) Fully supervised grounding: with
pixel-level ground-truth.
Previous answer grounding [12] extracted attention maps from the pretrained
VQA transformers which trained with only answering loss. The absence of visual
localization supervision in training leads to messy localization and clutter bound-
aries. To address this issue, recent advances introduced pixel-level grounding
masks to provide strong supervision with spatial information. However, fully-
supervised VQA grounding requires per-pixel annotations for training [3], which
is expertise-demanding, labor-intensive and time-consuming. In contrast, this
work introduces a weakly supervised scheme for answer grounding which lever-
ages various low-cost grounding labels, including click-level, scribble-level, and
box-level as illustrated in Fig. 1.
Meanwhile, despite the success of the vision-language Transformers has made
a signiﬁcant leap in the performance of VQA, there has been less improvement
in the interpretability for these black-box Transformers. A typical architecture
implements VQA with a visual encoder, a text encoder and a text decoder.
Since the VQA model can only predict text answers but not generate grounding
results, existing vision-language model can only apply post-hoc means for VQA
grounding, e.g. Grad-CAM [21] is one of the most used answer grounding tech-
niques [11]. To directly predict the grounding mask (i.e., self-explanation), we
extend the existing VQA architecture with a Language-based Visual Decoder
(LVD) which is learned from the additional weak labels [15]. The LVD module
decodes visual features into grounding masks through cross attending to lin-
guistic features. The LVD module along with the existing visual-based linguistic
encoder form our overall model structure, named Dual Visual-Linguistic Inter-
action (DaVi), which generates both visual and linguistic predictions through
two visual-linguistic interactions.
Since the weakly supervised LVD can only produce coarse ground masks,
we propose a pseudo supervision scheme to constrain the model for cleaner

158
Y. Liu et al.
predictions. Speciﬁcally, a Pseudo Grounding Reﬁnement Module (PGRM) is
designed to produce pseudo-labels, which eﬀectively improves the performance
of VQA Grounding. Our contributions are summarized as follows: i) the weakly-
supervised framework learns from low-cost grounding labels, achieving remark-
able grounding performance with limited budgets. ii) We propose an end-to-end
Dual Visual-Linguistic Interaction (DaVi) framework for eﬃcient execution of
vision and language tasks in a single architecture. iii) Our VQA Grounding
method outperforms state-of-the-art methods by over 2x IoU on the WizViz-
VQA-Grounding dataset, demonstrating superior performance in IoU, accuracy,
and composite metrics.
2
Related Work
2.1
Visual Question Answering
Visual Question Answering (VQA) [2] is a vital task for computer vision with
real-world applications such as aiding the visually impaired [7]. Best-performing
VQA models are criticized for complexity and lack of transparency, but answer
grounding provides clear explanations and accurate visual evidence [3] for rea-
soning, enhancing interpretability and transparency. Answer grounding makes
VQA models valuable tools for real-world applications.
2.2
VQA Grounding
The study of weakly supervised grounding in image phrase-grounding has
received signiﬁcant attention, but the equally critical task of visual question
answer Grounding (VQA Grounding) has been largely overlooked. With the
introduction of the concept of scene graphs, datasets containing Grounding
began to be proposed, and the Visual Genome [10] and CLEVR [8] datasets con-
tain scene graphs that represent the relationships between objects and attributes,
as well as a number of clusters of objects that are typically clustered together.
These representations are typically generated manually by humans for real
images [16] and automatically for rendered images [8]. However, a method to
automatically generate such scene graphs has recently been proposed [26]. This
method uses a state-of-the-art object detection algorithm (Faster R-CNN [17]) to
detect objects in images; instead of predicting local relational predicates between
objects, it passes messages between diﬀerent regions of the image to capture the
global scene context.
AU Khan et al. [24] proposed a unsupervised approach(No pixel supervision)
to localizing relevant visual entities in VQA tasks. In addition to the proposed
module, [9] utilizes the visual encoder [25] to group visual tokens in the capsule,
while also introducing text-guided selection using activations from the linguistic
self-focus layer. The selection module masks the capsules before forwarding them
to the next layer.

Weakly-Supervised Grounding for VQA
159
Existing methods use attention or gradient maps for answer grounding, while
our weakly supervised framework utilizes multimodal interactions to simultane-
ously generate answers and answer grounding, improves the performance of VQA
Grounding and overcomes the limitations of existing methods that ignore spatial
modeling. These advances are critical for achieving robust and accurate VQA
models for key applications.
3
Method
Our method, depicted in Fig. 2, takes as input the training images and questions
and produces predictions for answer grounding and answer. To obtain pseudo-
labels for the answer grounding, we select the largest class in the prediction map
Fig. 2. Our weakly supervised VQA grounding framework, featuring four primary
components: the visual encoder (VE), the visual-based language encoder (VLE),
the language-based visual decoder (LVD), and the language model (LM), or answer
decoder.
Fig. 3. Pseudo grounding reﬁnement module(PGRM). To guide Reﬁne Module to iter-
atively reﬁne the answer grounding mask, we use Guide Module to capture pixel-level
similarity information with multi-scale adaptive convolution.

160
Y. Liu et al.
generated by our network and reﬁne it using our Pseudo Grounding Reﬁnement
Module (PGRM), shown in Fig. 3. This approach allows us to obtain supervised
labels for training our model despite the lack of fully labeled data.
3.1
Architecture
The overall process is based on four primary steps: (1) Visual Encoder: We use
a Vision Transformer (ViT) [25] to extract image features. The text encoder
adopts the BERT [5] approach and includes the [CLS] token at the start of
the text input to summarize the sentence. (2) Vision-based Language Encoder:
image-question pairs are processed and visual features are linearly mapped to
Kvl, Vvl and natural language features are linearly mapped to Qvl features.
Finally, multimodal fusion is performed to align these visual features to text
features to facilitate subsequent answer generation. (3) Language-based Visual
Encoder: Using the language-based visual decoder, we linearly map the visual
cascade features Fv to Qlv and the linguistic features to Klv, Vlv, and ﬁnally
perform multimodal fusion to align the text features to the visual features to
generate the pseudo answer grounding. (4) Answer Decoder: we use a language
generation model Ld similar to the bert structure, and predict the correct answer
from the answer set by a classiﬁer.
As shown in Fig. 4 Visual-based Linguistic Encoder (VLE) leverages visual
features for cross-modal understanding. Our encoder maps visual features,
extracted by a visual encoder, to visual feature vectors Kvl and Vvl through
a separate linear mapping layer. Language features are ﬁrst extracted using self-
attention, and then mapped to language feature vectors Qvl. A Cross-Attention
(CA) layer is interleaved between the self-attention layer and the feedforward
network of BERT. The CA layer attends to visual references using language
features as queries and generates feature vectors F after 12 layers. These visual-
Fig. 4. Illustration of Dual Visual-Linguistic Interaction (DaVi) model, which consists
of Visual Encoder, Visual-based Linguistic Encoder, Answer Decoder for answering
and Linguistic-based Visual Decoder for answer grounding.

Weakly-Supervised Grounding for VQA
161
linguistic interactions in V Le provide multimodal evidence for answer decoding.
We present V Le as a novel approach to enhance cross-modal understanding by
integrating vision and language in a uniﬁed framework.
These visual-linguistic interactions in V Le provide multimodal evidence for
answer decoding in Ld, enhancing the ability of cross-modal understanding.
3.2
Language-Based Visual Decoder
To predict the answer grounding mask, the Linguistic-based Visual Decoder
(LVd) focuses on evidence-relevant regions of the visual features for answer
grounding. In this implementation, visual features are mapped to image features
Qlv through a linear mapping layer, while language features Fvl are mapped
to Klv and Vlv. Cross-attention is then used to align the visual features. This
approach provides an eﬀective way to ground the answer to the visual evidence,
enabling cross-modal understanding between language and vision.
3.3
Pseudo Grounding Reﬁnement
In this section, we will describe how we utilize the Pseudo Grounding Reﬁnement
Module (PGRM) to produce high-quality supervision signals. As shown in Fig. 3
We input the training images into the network to obtain the prediction maps.
Pseudo-labels are generated by selecting the largest class of the prediction map.
The core is that the neighboring pixels with similar appearance should be
grounded consistently. Our implementation is derived from pixel adaptive con-
volution (PAC) [22]. The idea is to update the pixel labels m :, i, j using convex
combinations of the labels of its neighbors N(i, j) iteratively, and in tth iterations
mt
:,i,j =

(l,n)∈N(i,j)
αi,j,l,n · mt−1
:,l,n
(1)
To compute α, we use the Reﬁne Core on the pixel-level intensity I, the
k (Ii,j, Il,n) = −|Ii,j −Il,n|
σ2
i,j
(2)
where we deﬁne σ(i,j) as the standard deviation of the image intensity com-
puted locally for the Reﬁne Core. We apply softmax to obtain the ﬁnal distance
α(i,j,l,n) for each neighbor (l, n) of (i, j). For this module, we do not perform
backpropagation, so it is always in “evaluation” mode,
LP seudo = −1
N

i∈n
(ˆyc
i )log(yc
i )
(3)
In summary, the model is trained in each iteration by the network using the
overall loss L, which is deﬁned as,
L = LAnswer + LCE + LP seudo
(4)

162
Y. Liu et al.
3.4
Training Objectives
During training, we optimize two objectives simultaneously: one based on visual
tasks and the other based on language tasks. We use diﬀerent functions to cal-
culate the two losses, as described below.
The Language Modeling Loss (LM) activates the image-grounded text
decoder to generate a text description of a given image. We optimize the cross-
entropy loss and train the model to maximize the likelihood of the text in an
autoregressive manner. To calculate the loss, we apply label smoothing of 0.1.
Compared to the widely used Masked Language Modeling (MLM) loss in Visual
Language Pre-Training (VLP), LM provides the model with the ability to gen-
eralize and convert visual information into coherent captions. Let us assume a
dataset C, which includes a continuous input token x1, ..., xm, and an answer
label y. The maximization objective for training the network is deﬁned as:
LAnswer(C) =

(x,y)
log(y|x1, ..., xm)
(5)
The answer grounding Loss activates a Visual-based Linguistic Encoder to
generate answer using fused joint-level features with a standard U-Net [18] net-
work structure of the generative model. To calculate the loss, we train the U-Net
network by minimizing the partial cross-entropy loss LCE with labeled n pixels
i, which is deﬁned as:
LCE = −1
n

i∈n
(ˆyc
i )log(yc
i )
(6)
Table 1. Composite metrics under weak supervision: Accuracy (50/75) measures VQA
accuracy with IoU greater than 50/75, while IoU (True/False) measures IoU of cor-
rect/incorrect VQA.
Supervision
Accuracy
Accuracy (50)
Accuracy (75)
IoU
IoU (True)
IoU (False)
Unsupervised
42.97
47.16
47.05
30.29
30.41
30.16
Click-level
43.23
57.84
58.53
60.79
71.77
49.18
Scribble-level
43.76
55.71
58.50
68.92
76.23
60.08
Box-level
44.38
53.11
56.23
66.78
72.84
58.59
Full pixel
45.80
57.22
60.70
73.21
79.00
64.68

Weakly-Supervised Grounding for VQA
163
Table 2. Performance comparison of diﬀerent click-level labels: Φ30 pixels, 1 pixel,
and 1% pixels representing dots of 30 pixels in diameter, a single pixel, and 1% of the
total area of the label, respectively.
Supervision
Accuracy
Accuracy (50)
Accuracy (75)
IoU
IoU (True)
IoU (False)
Φ30pixels
43.76
55.92
57.79
63.02
76.90
58.20
1 pixel
43.23
57.84
58.53
60.79
71.77
49.18
1%pixels
42.97
55.04
59.12
66.75
76.14
56.04
5%pixels
43.76
55.92
57.79
68.56
76.90
58.20
25%pixels
44.12
54.31
59.27
72.91
79.74
64.16
Full pixel supervision
45.80
57.22
60.70
73.21
79.00
64.68
Table 3. Linguistic and visual features for the evaluation of model eﬀects. Contrast
learning is pre-trained using the LAION dataset and VQA is pre-trained using the
VQA-v2 dataset
freeze
Contrast Learning VQA
Visual Text IoU
Accuracy
IoU
Accuracy
Baseline [11]
-
35.45
–
48.18
Ours
70.65 39.16
73.21 45.8
Ours
✓
69.33
39.08
71.74
47.3
Ours
✓
✓
68.09
35.98
70.45
48.27
4
Experiments
4.1
Training Details
We use the same pre-trained model as [11] for ﬁne-tuning. In we pre-trained the
model for 40 periods using a batch of 8. We use the AdamW [14] optimizer with
a weight decay of 0.05. The learning rate is pre-trained to 2e−4 and decreases
linearly at a rate of 0.85, and the image resolution is increased to 512 × 512. In
addition, we evaluate our model in the VizWiz-VQA-Grounding dataset [3]. In
this dataset, the validation set is publicly available, while the test set is not.
In order to evaluate the test set (Test-Dev and Test-Standard), the predictions
need to be submitted to the test server. But here, we use 6494 images from the
original training set as the training set and 1131 images from the validation set
as the test set for our experiments and compare them with other methods.
We generate supervised data using diﬀerent methods: random positioning
for scribble set supervision, horizontal and vertical lines for long line generation,
and external rectangles for box-level supervision. Our approach trains the model
end-to-end under diﬀerent weakly supervised signals.

164
Y. Liu et al.
Table 4. Dual Visual-Linguistic Interaction module. LE is language encoder, VE is
visual encoder, LD is language decoder, VD is visual decoder, LVD is language-based
visual decoder, and VLE is visual-based language encoder.
Method
Modules
Accuracy IoU
LE + VE + LD + VD
17.59
60.03
LE + VE + LVD + VD
+LVD
17.77
70.31
VE + VLE + LD + VD
+VLE
35.72
61.23
VE + VLE + LVD + LD +LVD,VLE 39.16
70.65
Mac-Caps [24]
15.76
17.42
Table 5. Eﬀectiveness of PGRM model using diﬀerent weak supervision labels.
Supervision
PGRM model Accuracy IoU
Unsupervised
42.97
30.29
✓
46.59
23.67
Click-level
43.23
60.79
✓
45.8
63.52
Scribble-level
43.76
68.92
✓
47.83
70.44
Box-level
44.38
66.78
✓
47.48
67.34
4.2
Ablation Study
Eﬀect of Diﬀerent Supervisory Information. To investigate the eﬀect of dif-
ferent annotation signals on VQA Grounding tasks, we conduct ablation exper-
iments and list composite index performance under diﬀerent levels of supervi-
sion in Table 1. Full pixel supervision outperforms weakly supervised informa-
tion. Among weakly supervised information, scribble-level supervision improves
answer grounding, while box-level supervision improves VQA Grounding.
Eﬀect of Diﬀerent Percentage Labels. In order to reﬁne more the impact of
the labeling method on the performance in all directions, the click-level labels are
reﬁned in Table 2. As can be seen from the table, the greater the label coverage,
the better the performance in terms of accuracy and intersection ratio, where it
approaches pixel supervision in the case of 25% pixels. By looking at the two
composite indices of accuracy (50) and accuracy (75) for the VQA, we can see
that the two indices are not very correlated, especially the accuracy (50) for 1
pixel in accuracy (50) reaches 57.84. For the IoU, IoU(True) and IoU(False) for
the answer grounding index, we can see that the model is inﬂuenced by the label
coverage while is also aﬀected by the accuracy of the VQA.
Truncated
Gradient. As shown in Table 3. Controlling gradient back-
propagation can tune the performance of answer grounding and visual VQA.

Weakly-Supervised Grounding for VQA
165
While our proposed method outperforms previous SOTA on LAION [20] pre-
training, it underperforms Baseline on VQA pre-training. Freezing gradient back
to language encoder only decreases IoU but increases accuracy, while freezing
both language and visual encoder reaches optimal performance. Gradient back-
propagation can thus be adjusted to tune task performance.
Table 6. Results of question and answer grounding with diﬀerent pixel supervision
information on the VizWiz-VQA-Grounding validation set. * indicates frozen gradient
back-propagation of visual coder and vision-based language encoder while computing
answer grounding.
Method
Pretrained
Supervision
Accuracy
IoU
IoU (True)
BLIP [11](Baseline)
LAION [19]
Unsupervised
37.01
–
–
BLIP(Baseline)
VQA-v2 [6]
Unsupervised
48.18
–
–
LAVT [28]
ImageNet-22K [4]
Full pixel
–
69.64
–
Ours
LAION
Unsupervised
36.78
27.72
23.30
Scribble-level
36.78
66.15
75.70
Box-level
37.75
64.56
71.51
Full pixel
39.16
70.65
77.65
Ours
VQA-V2
Unsupervised
42.97
30.29
30.41
Click-level
43.23
60.79
71.77
Scribble-level
43.76
68.92
76.23
Box-level
44.38
66.78
72.84
Full pixel
45.80
73.21
79.00
Ours∗
Full pixel
48.27
70.45
77.55
Table 7. Results on the VizWiz-VQA-Grounding validation and test set.
Set
Method
Pretrained
Accuracy IoU
test OSCAR [13]
VQA-v2
29.2
15.48
LXMERT [23]
VizWiz-VQA 38.17
22.09
Mac-Caps [24]
VQA-v2
15.76
17.42
Mac-Caps [24]
VizWiz-VQA 14.83
27.43
val
BLIP [11]
LAION
37.01
–
BLIP [11]
VQA-v2
48.18
–
Ours
LAION
31.12
57.11
Ours
VQA-V2
42.97
30.29
Ours + click-level VQA-V2
43.23
60.79
Reﬁnement Model. As shown in Table 5, we demonstrate the eﬀectiveness
of the Reﬁne Module in improving pseudo answer grounding masks and self-
correction. Performance evaluations show signiﬁcant improvements in both

166
Y. Liu et al.
visual question answering and answer grounding tasks under weak supervisory
signals, even under unsupervised. The IoU reaches 70.44 under scribble-level
supervision, close to fully supervised VQA pre-training. Deterioration of answer
grounding task performance under unsupervised signals may be due to optimiza-
tion failure caused by poor quality of generated answer grounding maps.
Eﬀect of Dual Visual-Linguistic Interaction Module. As shown in Table 4,
ablation experiments on DaVi show low accuracy and IoU without interac-
tion module, with signiﬁcant improvements after adding language-based visual
decoder or visual-based language encoder. Using both modules together achieves
high VQA and answer grounding accuracy. Results demonstrate DaVi’s ability to
improve single-task performance and complement downstream tasks of diﬀerent
modalities.
Fig. 5. This ﬁgure visualizes attention maps and answer grounding produced by models
trained with diﬀerent weakly supervised labels, including unsupervised.
Visualization. As can be observed from the graph Fig. 5We visualize the visual-
verbal interaction feature map to validate the model’s behavior. Scores are higher
on regions related to linguistic expressions, guiding the model to recognize images
and answer questions. The model accurately recognizes object edges and text
through pixel-level answer grounding, indicating strong multimodal interaction
and spatial learning capability.
5
Comparison with State-of-the-Arts
In this section, we evaluates the existing methods and our proposed method on
the VizWiz-VQA-Grounding dataset, with results shown in Table 7 and Table 6.
Our method outperforms others in IoU and accuracy under diﬀerent weakly

Weakly-Supervised Grounding for VQA
167
supervised signals, reﬂecting the complementary nature of VQA and answer
grounding tasks. While our method has higher IoU, accuracy drops due to insuﬃ-
cient training data and catastrophic forgetting. Interrupting gradient back prop-
agation can prevent catastrophic forgetting.
6
Conclusion
In this paper, we have proposed a weakly supervised approach for the VQA
grounding task, which uses low-cost annotations, such as click-, scribble-, and
box-level grounding labels. Moreover, we also proposed an end-to-end framework
which is Dual Visual-Linguistic Interaction (DaVi) network with a language-
based visual decoder and Pseudo Grounding Reﬁnement Module (PGRM). Our
approach signiﬁcantly improves the performance on the VQA grounding task,
achieving 92% performance with scribble-level supervision and reducing anno-
tation cost compared to fully supervised methods. Furthermore, our approach
boosts text answer accuracy despite using inaccurate supervision. These results
demonstrate the eﬀectiveness of our approach and suggest promising directions
for future research on VQA grounding.
References
1. Agrawal, A., Batra, D., Parikh, D., Kembhavi, A., Don’t just assume; look and
answer: overcoming priors for visual question answering. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 4971–4980
(2018)
2. Antol, S., et al.: VQA: visual question answering. In: Proceedings of the IEEE
International Conference on Computer Vision, pp. 2425–2433 (2015)
3. Chen, C., Anjum, S., Gurari, D.: Grounding answers for visual questions asked
by visually impaired people. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 19098–19107 (2022)
4. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.: Imagenet: a large-scale
hierarchical image database. In: 2009 IEEE Conference on Computer Vision and
Pattern Recognition, pp. 248–255. IEEE (2009)
5. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: pre-training of deep bidi-
rectional transformers for language understanding. In: North American Chapter of
the Association for Computational Linguistics (2018)
6. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in
VQA matter: elevating the role of image understanding in visual question answer-
ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 6904–6913 (2017)
7. Gurari, D., et al.: Vizwiz-priv: a dataset for recognizing the presence and purpose
of private visual information in images taken by blind people. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 939–948
(2019)
8. Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Zitnick, C.L., Gir-
shick, R.: ClevR: a diagnostic dataset for compositional language and elementary
visual reasoning. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2901–2910 (2017)

168
Y. Liu et al.
9. Khan, A.U., Kuehne, H., Gan, C., Da Vitoria Lobo, N., Shah, M.: Weakly super-
vised grounding for VQA in vision-language transformers. In: Avidan, S., Bros-
tow, G., Ciss´e, M., Farinella, G.M., Hassner, T. (eds.) ECCV 2022. LNCS, vol.
13695, pp. 652–670. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-
19833-5 38
10. Krishna, R., et al.: Visual genome: connecting language and vision using crowd-
sourced dense image annotations. Int. J. Comput. Vision 123(1), 32–73 (2017)
11. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: bootstrapping language-image pre-training
for uniﬁed vision-language understanding and generation. In: ICML (2022)
12. Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before
fuse: Vision and language representation learning with momentum distillation. In:
Advances in Neural Information Processing Systems, vol. 34, pp. 9694–9705 (2021)
13. Li, X., et al.: Oscar: object-semantics aligned pre-training for vision-language
tasks. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020.
LNCS, vol. 12375, pp. 121–137. Springer, Cham (2020). https://doi.org/10.1007/
978-3-030-58577-8 8
14. Loshchilov, I., Hutter, F.: Fixing weight decay regularization in Adam (2017)
15. Pan, J., et al.: Tell me the evidence? Dual visual-linguistic interaction for answer
grounding. arXiv preprint arXiv:2207.05703 (2022)
16. Ramakrishnan, S.K., Pal, A., Sharma, G., Mittal, A.: An empirical evaluation of
visual question answering for novel objects. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 4392–4401 (2017)
17. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object
detection with region proposal networks. In: Advances in Neural Information Pro-
cessing Systems, vol. 28 (2015)
18. Ronneberger, O., Fischer, P., Brox, T.: U-net: convolutional networks for biomed-
ical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.
(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-24574-4 28
19. Schuhmann, C., et al.: Laion-5b: an open large-scale dataset for training next
generation image-text models. arXiv preprint arXiv:2210.08402 (2022)
20. Schuhmann, C., et al.: Laion-400m: open dataset of clip-ﬁltered 400 million image-
text pairs. In: NeurIPS Workshop Datacentric AI, number FZJ-2022-00923. J¨ulich
Supercomputing Center (2021)
21. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-
CAM: visual explanations from deep networks via gradient-based localization. In:
Proceedings of the IEEE International Conference on Computer Vision, pp. 618–
626 (2017)
22. Su, H., Jampani, V., Sun, D., Gallo, O., Learned-Miller, E., Kautz, J.: Pixel-
adaptive convolutional neural networks. In: Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp. 11166–11175 (2019)
23. Tan, H., Bansal, M.: LxMERT: learning cross-modality encoder representations
from transformers. In: Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), pp. 5100–5111 (2019)
24. Urooj, A., Kuehne, H., Duarte, K., Gan, C., Lobo, N., Shah, M.: Found a reason
for me? Weakly-supervised grounded visual question answering using capsules.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 8465–8474 (2021)
25. Vaswani, A., et al.: Attention is all you need. In: Advances in Neural Information
Processing Systems, vol. 30 (2017)

Weakly-Supervised Grounding for VQA
169
26. Xu, D., Zhu, Y., Choy, C.B., Fei-Fei, L., Scene graph generation by iterative mes-
sage passing. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5410–5419 (2017)
27. Xu, K., et al.: Show, attend and tell: neural image caption generation with
visual attention. In: International Conference on Machine Learning, pp. 2048–2057.
PMLR (2015)
28. Yang, Z., Wang, J., Tang, Y., Chen, K., Zhao, H., Torr, P.H.S.: LAVT: language-
aware vision transformer for referring image segmentation. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18155–
18165 (2022)

STU3: Multi-organ CT Medical Image
Segmentation Model Based on Transformer
and UNet
Wenjin Zheng1, Bo Li1(B), and Wanyi Chen2
1 College of Computer Science and Engineering, Chongqing University of Technology,
Chongqing 400054, China
libo@cqut.edu.cn
2 College of Science, Chongqing University of Technology, Chongqing 400054, China
Abstract. With the popularity of artiﬁcial intelligence applications in the med-
ical ﬁeld, U-shaped convolutional neural network (CNN) has garnered signiﬁ-
cant attention for their efﬁcacy in medical image analysis tasks. However, the
intrinsic limitations of convolutional operation, particularly in the receptive ﬁeld
since it is an end-to-end learning method, impede the establishment of long-
term semantic feature dependence and holistic context information connection.
This results in the edge contour details insensitive during the image segmenta-
tion task. To mitigate these shortcomings, Transformer architectures equipped
with self-attention mechanism offer a potential alternative for encoding long-term
semantic features and capturing global contextual information. Motivated by these
insights, this paper proposes a novel U-shaped Transformer architecture, denoted
as STU3, speciﬁcally engineered for medical image segmentation. Initially, a
parallel training paradigm is employed that distinguishes between global ﬁne-
grained and local coarse-grained image features, optimizing the feature extrac-
tion process. Secondly, to alleviate the restrictions on ﬁne-grained feature fusion
due to peer skip connections, we propose a Residual Full-scale Feature Fusion
module (RFFF) as the global decoder component. Lastly, a Global-Local Feature
Fusion Block (GLFB) is implemented to seamlessly integrate the ﬁne-grained and
coarse-grained features, thereby constructing a comprehensive global information
dependency network. This ensures a high level of accuracy in medical image seg-
mentation tasks. Experimental evaluations conducted on abdominal and cervical
multi-organ CT datasets substantiate the superiority of the proposed STU3 model
over most current models, particularly in terms of the Dice Similarity Coefﬁcient
evaluation metric.
Keywords: Medical Image Segmentation · Swin Transformer · UNet
1
Introduction
Automatic medical image segmentation is a crucial step in medical image analysis,
offering valuable diagnostic evidence in clinical practice. This technology is especially
impactful for enhancing the accuracy of computer-aided diagnosis and image-guided
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 170–181, 2024.
https://doi.org/10.1007/978-981-99-8850-1_14

STU3: Multi-organ CT Medical Image Segmentation Model
171
surgeries [1]. In recent years, CNN has emerged as the dominant paradigm in various
medical image segmentation tasks [2, 3]. Notable architectures such as UNet, featuring
with encoder-decoder symmetric structure [4], and its variants have illustrated supe-
rior performance. For instance, UNet++ [5] incorporates extensive skip connections to
minimize the semantic gaps between feature maps, while UNet3+ [6] employs deep
supervision, full-scale skip connections and classiﬁcation guidance to optimize training
efﬁciency and reduce model complexity. Similarly, DenseUNet [7] introduces the con-
cepts of residual connection and dense connection to help information ﬂow better in the
network and promote the propagation of gradients. KiUNet [8] achieves faster conver-
gence andcaptures a moresubtle structure through asmaller receptive ﬁled. Despite these
advancements, U-shaped architecture suffers from the inherent limitations of convolu-
tional operations. Speciﬁcally, the convolutional kernel can only focus on one sub-region
[9]. This nature restricts their ability to capture long-term semantic dependencies and
global context, rendering them less sensitive to edge and contour details in segmentation
tasks [10], consequently affecting the quality of segmentation outcomes.
In the realm of natural language processing (NLP), the Transformer architecture [11]
has demonstrated robust capabilities for sequence feature learning and long-term depen-
dency modeling. This inspired the development of Vision Transformer (ViT) [12] and
its adaptation from NLP to Computer Vision (CV) tasks. To mitigate training challenge
associated with ViT, Touvron et al., introduced a data-efﬁcient image Transformer (DeiT)
[13], using Knowledge Distillation [14] to enhance model performance and robustness.
Liu et al., proposed Swin Transformer, a hierarchical visual Transformer leveraging mov-
ing window self-attention mechanism [15]. It also implemented W-MSA and SW-MSA
serial Transformer modules to reduce the computational complexity.
Given the limitations of both CNNs and Transformers in medical image segmen-
tation, the emergence of hybrid models becomes increasingly important. TransUNet,
as the ﬁrst hybrid network model of Transformer and CNN [9], incorporated a global
self-attention mechanism in the encoder, and beneﬁted from jump connection and up-
sampling of UNet architecture in the decoder. Other approaches such as U-shaped Swin
Transformer-SwinUnet [16], DSTransUNet [17] and SwinUNETR [19] also utilize vari-
ation of Transformer modules, targeting different aspects like dual-scale feature extrac-
tion (TIE), dual-scale feature fusion (TIF) and feature up-sampling. However, there
are two major obstacles, which impedes the migration of application of Transformer
to medical image segmentation tasks. First [16], Transformer training requires signif-
icant computing resources and large-scale datasets. Second, Transformers, inherently
being sequence-to-sequence models, exhibits limitations in capturing pixel-level spatial
features and positional information compared with convolutional methods.
To address the conﬁned receptive ﬁeld in convolutional operations and the challenges
in capturing pixel-level positional information inherent to Transformers, this paper pro-
poses STU3, a novel U-shaped Transformer model for automatic medical image seg-
mentation that leverages the strengths of both the Swin Transformer Blocks (SWTB)
and the UNet architecture.

172
W. Zheng et al.
Fig. 1. The overall architecture of STU3.
2
Method
2.1
Architecture Overview
The architecture of the proposed STU3 model is depicted in Fig. 1. Due to the high
grayscale level of original CT images O ∈RH×W×C, this paper will preprocess the
original image ﬁrst. The local part and the global part are both U-shaped network archi-
tectures. In the local part, the encoder initially conducts two successive down-sampling
operations for feature extraction, followed by the integration of SWTB to enhance the
remote dependence of the feature map, which is then fused with low-scale features.
Subsequently, spatial resolution and local positional information are recovered by up-
sampling twice. In the global component, the input image is partitioned into uniform
pixel patches, and global semantic features are extracted via four consecutive down-
samplings. A Residual Full-scale Feature Fusion module (RFFF) serves as the decoder,
performing four consecutive up-sampling steps to reinstate both spatial resolution and
thoroughgoingcontextofthefeaturegraph.Finally,aGlobal-LocalFeatureFusionBlock
(GLFB) is employed to amalgamate features from both the local and global outputs.
2.2
SWTB
The conventional Transformer encoder comprises multiple identical, standard Trans-
former modules, as shown in Fig. 2a. There are two core components: the Multi-Head
Self-Attention mechanism (MSA) and the Multi-Layer Perceptron (MLP). Each and
every component is predated by a Normalized Layer (LN) and followed by a residual
link. The output of l-level standard Transformer can be mathematically expressed as:
ˆM l = MSA(LN(M l−1)) + M l−1

STU3: Multi-organ CT Medical Image Segmentation Model
173
M l = MLP(LN( ˆM l)) + ˆM l
(1)
Here, ˆM l and M l correspond the outputs of the MSA and the MLP, respectively.
LN
MSA
LN
MLP
ˆ l
M
l
M
1
l
M −
a
LN
W-MSA
LN
MLP
LN
SW-MSA
LN
MLP
1
l
Z −
l
Z
ˆ l
Z
l
Z
1
ˆ l
Z +
1
l
Z +
b
Fig. 2. (a) The architecture of the standard Transformer module; (b) The structure of SwinTrans-
former module.
While the standard Transformer computes relationships between all tokens in a
feature map [11], its computational complexity scales quadratically with the size of the
image, making it suboptimal for high-resolution feature maps. As shown in Fig. 2b,
the Swin Transformer consists of two sub-modules [20], each containing the similar
components and steps, except for their distinct multi-head self-attention mechanism
component (W-MSA\SW-MSA). The features output by LN are divided into equal-sized
and non-overlapping window sequences. Each encompasses M × M patches. W-MSA
conducts intra-window multi-head self-attention mechanism, while SW-MSA shifts the
original window partition to the right and down
 M
2 , M
2

window sizes, respectively,
followed by multi-head self-attention mechanism operation inside the new windows.
This design mitigates the disadvantage of information isolation across windows without
increasing additional computational overhead [15]. The mathematical representation of
the Swin Transformer module can be expressed as:
ˆSl = W −MSA(LN(Sl−1)) + Sl−1
Sl = MLP(LN(ˆSl)) + ˆSl
ˆSl+1 = SW −MSA(LN(Sl)) + Sl
Sl+1 = MLP(LN(ˆSl+1)) + ˆSl+1
(2)
where ˆSl correspond the output of the W-MSA component or the SW-MSA component,
and Sl is the output of the MLP. The main function of the self-attention mechanism
[21] is to compute dependencies between disparate elements of the input sequence. The
output of this mechanism can be formally represented as:
Attention(Q, K, V) = SoftMax
QKT
√
d
+ B

V
(3)
Among them, B ∈R(2M −1)2 represents the relative position coding [22], M is the window
size,Qrepresentsthepositioninformationofthecurrentconcernedelement,Krepresents

174
W. Zheng et al.
the position information of other elements, and V represents the content feature and
information meaning of each element.
Fig. 3. Global decoder RFFF.
2.3
Local Part
The preprocessed 2D slice X ∈RH×W×C is used as the input of the local part. This
part has two levels, each level consists of an encoder, a skip connection, and a decoder
module. The encoder downsamples the feature twice and projects the channel to K
4
dimensions. The two-stage encoder outputs X i
LEn, i = 1, 2 resolutions of H
2 × W
2 × K
4
and H
4 × W
4 × K
2 , respectively. The feature output of each level encoder will interact
with the local feature information through the corresponding level of the peer-to-peer
skip connection module (SWTB) [15], thereby enhancing the global information of the
local features. During this phase, the size of the feature map remains invariable, and
then the feature output will be merged with the output features of the lower decoder, and
then deliver to the decoder to sample twice. The output X i
LDe, i = 1, 2 resolutions of the
two-stage decoder are H
2 , W
2 , K
4 and H
2 × W
2 × K
2 respectively, which can be expressed
as:
X i
LDe = U(H[SWTB(D(X i−1
LEn)), X i+1
LDe]), i = 1, . . . N
(4)
Among them, H[.] represents the use of convolution, batch normalization and activation
function to achieve feature fusion. U(.) and D(.) represent up-sampling and down-
samplingoperations,respectively.SWTBrepresentstheuseofSwinTransformermodule
for feature extraction and fusion, and N represents the total series.
2.4
Global Part and RFFF
Both the global and the local parts of the model share the same image as their input
X ∈RH×W×C. There are four hierarchical levels. The ﬁrst three levels are composed

STU3: Multi-organ CT Medical Image Segmentation Model
175
of an encoder and a decoder, while the fourth level solely consists of a bottleneck layer
featuring a single encoder. The ﬁrst-level encoder is composed of a patch embedding
layer [15] coupled with a SWTB. The patch embedding layer divides X into uniform
and non-overlapping patches Xa ∈R
H
p × W
p ×

p2×C

, where H
p × W
p and p2 × C are its
resolution and number of channels, respectively. p2 is the resolution of the patch. Then,
Xa is projected into a K-dimensional embedding space, with its spatial resolution pre-
served unchanged at Xb = R
H
p × W
p ×K, and ﬁnally Xb will be fed into SWTB for further
processing. The encoders in the latter three levels are constructed from a patch merging
layer and an SWTB module. The patch merging layer down-samples the number of
patches of the feature map output from the previous encoder by 4 times, while simulta-
neously doubling the number of channels in the feature map. The resolution and number
of channels of the output X i
GEn, i = 1, 2, 3, 4 of each stage encoder are H
p × W
p × K,
H
2p × W
2p × 2K, H
4p × W
4p × 4K and H
8p × W
8p × 8K, respectively.
In the decoder part, the RFFF module is employed. The conﬁguration for the third-
level decoder is shown in Fig. 3. Firstly, the decoder accepts feature vectors X 1
GEn and
X 2
GEn from the low-level encoder, followed by the peer-to-peer jump connection from the
same-level encoder, and ﬁnally receive the feature information X 4
GDe from a high-level
decoder. To clarify using the third level as an example, within the RFFF module, all
feature maps with different levels of resolution are made consistent with the resolution
and channel number of the corresponding level decoder output X 3
GDe ∈R
H
4P × W
4P ×4K.
For the low-level features, 3-i, where i is the corresponding series, feature acquisition
components are implemented for down-sampling. For same level features, the peer-to-
peer jump connection is used for feature extraction. High-level features are up-sampled
through i-3 feature acquisition components. In order to reduce redundancy during multi-
level feature fusion, a feature aggregation component is used, effectively reducing the
number of fused feature channels to 4K. Then these fused features are combined with
the output features from the encoder through a residual fusion mechanism. The decoder
output X i
GDe, i = 1, 2, 3 can be expressed as:
X i
GDe = {
X i
GEn,i=N
H

C
	
D
	
X j
GEn


i−1
j=1,C

X i
GEn

,C
	
U
	
X j
GDe


N−1
j=i+1

,i=1,...,N−1
(5)
where, H[.] represents the use of convolution, batch normalization and activation func-
tion to achieve feature fusion. U(.) and D(.) are up-sampling and down-sampling oper-
ations, respectively. C(.) stands for the use of ﬁlters to extract features, and N represents
the total series.
2.5
Glfb
The GLFB is shown in Fig. 4. It receives ﬁve features with resolutions of 2H
p × 2W
p × K
2 ,
H
p × W
p ×K, H
2p × W
2p ×2K, H
4p × W
4p ×4K and H
8p × W
8p ×8K, and uses feature the acquisition
component (FAC) to up-sample the four global features, making their resolution consis-
tent with X 1
LDe. Subsequently, the global features along with the local features undergo

176
W. Zheng et al.
axial concatenation and are passed through a sequence of four SWTB modules. Follow-
ing this, a feature aggregation group is employed to upscale the resolution by a factor of
2, while simultaneously decreasing the channel count to K
4 . To complete the architecture,
a segmentation head projects aggregated features into a J-dimensional embedding space,
thereby providing the ﬁnal segmented output. The GLFB output XGLFB ∈RH×W×J can
be expressed as:
XGLFB = E(SWTB([U(X i
GDe)N
i=1, X 1
LDe]))
(6)
where E(.) represents the use of convolution, batch normalization, and activation
functions to achieve extended resolution and compression channels. U(.) and [.] are
up-sampling and axial connection, respectively. N stands for the total number of stages.
Fig. 4. Global-local feature fusion module GLFB.
3
Experiments
3.1
Implementation Details
In this paper, Dice Similarity Coefﬁcient [26] (DSC) evaluation was performed on two
distinct medical CT datasets: the abdominal multi-organ CT dataset (BTCV) and the
cervical multi-organ CT dataset (CRD). BTCW focused on the six most difﬁcult organs:
gallbladder, esophagus, portal vein and splenic vein, pancreas, right adrenal gland and
left adrenal gland, and CRD included four organs: bladder, uterus, rectum, and small
intestine. The experiments were implemented using Python3.9 and Pytorch1.7.1. For
training, rudimentary data augmentation technologies such as random ﬂipping, rotation,
and cropping were utilized, without any pre-training on external datasets. The input
image and patch dimensions were set to 448 × 448 and 4, respectively. The learning
rate is 1e−3, the batch size is 6, and a pre-deﬁned iteration count of 100.

STU3: Multi-organ CT Medical Image Segmentation Model
177
3.2
Evaluating Metric
The evaluation metric employed in this study is the DSC [26], also known as the
Sorensen–Dicecoefﬁcient.Thismetriciswidelyusedtoquantifytheoverlaporsimilarity
between two sets and is particularly pertinent for comparing image segmentation results.
For each dimension of the feature map, the DSC is computed, and subsequently, the mean
value is obtained to represent the resultant DSC for the given image segmentation task.
The formula for DSC is provided as follows:
DSC = 2 ∗|A ∩B|
|A| + |B|
(7)
Table 1. Accuracy comparison on BTCV
Framework
Gall
Eso
Veins
Pan
Rag
Lag
Avg
ASPP [23]
0.6358
0.7264
0.7149
0.7429
0.6361
0.6555
0.6853
TransUNet [9]
0.6549
0.7319
0.7147
0.7426
0.6376
0.6575
0.6899
SwinUnet [16]
0.6902
0.7415
0.7300
0.7496
0.6473
0.6695
0.7047
TransBTS [24]
0.685
0.7559
0.7425
0.7512
0.6795
0.6795
0.7156
UNETR [25]
0.7698
0.7401
0.7505
0.8012
0.6184
0.6361
0.7194
nnU-Net [24]
0.6658
0.7571
0.7831
0.7917
0.6665
0.6960
0.7267
Swin UNETR [19]
0.7712
0.7414
0.7508
0.8102
0.6396
0.6600
0.7289
CLIP [27]
0.7952
0.7655
0.7754
0.8317
0.689
0.7214
0.7630
STU3
0.7586
0.7750
0.7684
0.7933
0.7094
0.7401
0.7575
3.3
Experimental Result
So as to validate the validity of proposed STU3 model, a comprehensive comparison
against SOTA is conducted on the abdominal multi-organ dataset as shown in Table 1.
Table 1 presents segmentation outcomes from eight diverse models based on BTCV
dataset, including ASPP, TransUNet, SwinUnet, SwinUNETR and the current optimal
CLIP. It is noteworthy that STU3 achieves a DSC of 75.75%, outperforming most of
the existing methods, especially on more challenging organs. Furthermore, the average
DSC scores speciﬁcally for the adrenal gland is superior to that of the state-of-the-art
methods. This result underscores resilience of STU3 to data imbalance and its focused
attention to individual organs. Figure 5 visually substantiates these claims, displaying
that the segmentation output of STU3 closely approximates the ground truth.
For the cervical multi-organ dataset, STU3 records an average DSC of 65.55%,
out classing the performance of U-Unet, TransUNet, and SwinUnet models. Figure 6
reinforces the robustness generalization capabilities of STU3, as indicated by its supe-
rior segmentation outcomes.The empirical evaluations afﬁrm the high efﬁcacy and
generalizability of the STU3 model for medical image segmentation tasks (Table 2).

178
W. Zheng et al.
Table 2. Accuracy comparison of CRD
Framework
Bladder
Uterus
Rectum
Small bowel
Avg
Unet [4]
0.7764
0.6092
0.5656
0.3123
0.5659
TransUNet [9]
0.7961
0.6766
0.6209
0.3326
0.6066
SwinUnet [16]
0.8303
0.7515
0.6472
0.3523
0.6453
STU3
0.8375
0.7241
0.6315
0.4289
0.6555
(a)Input
(b)GT
(c)STU3
(d)SwinUnet
(e)TransUNet
(f)ASPP
Lag
Rag
Eso
Gall
Fig. 5. Segmentation results of different methods on BTCV
As shown in Table 3, this paper takes the basic model STU without RFFF module and
GLFB module as the benchmark model, and conducts ablation tests on the abdominal
multi-organ CT dataset to evaluate the performance of RFFF module and GLFB module.
The DSC segmentation accuracy of STU is 69.19%, the segmentation accuracy of STU
+ RFFF is 73.03%, which is increased by 3.84%, and the segmentation accuracy of STU
+ GLFB is 73.8%, which is increased by 4.61%. The segmentation accuracy of each
organ and tissue has been improved to varying degrees, which proves that RFFF and
GLFB modules can effectively extract global context information and local structural
features.
Table 3. Accuracy comparison of ablation experiments on BTCV
Framework
Gall
Eso
Veins
Pan
Rag
Lag
Avg
STU
0.6882
0.6938
0.6973
0.7027
0.6725
0.6967
0.6919
STU + RFFF
0.7395
0.7414
0.7422
0.7753
0.6813
0.7023
0.7303
STU + GLFB
0.7223
0.751
0.7611
0.7628
0.6926
0.7383
0.738
STU3
0.7586
0.775
0.7684
0.7933
0.7094
0.7401
0.7575

STU3: Multi-organ CT Medical Image Segmentation Model
179
(a)Input
(b)GT
(c)STU3
(d)SwinUnet
(e)TransUNet
(f)Unet
Rec
Ute
Bla
Fig. 6. Segmentation results of different methods on CRD
4
Conclusion
This paper introduces STU3, a novel model for automatic medical image segmentation
that leverages the strengths of the Swin Transformer module and the UNet architec-
ture. A parallel training scheme for global and local features is devised to optimally
capture both global semantic attributes and local positional information within images.
Combined with the GLFB, the model further amalgamates global coarse-grained and
local ﬁne-grained features, thereby establishing a robust link for both global information
and local structural dependencies. Employing the RFFF module as the global decoder
accomplishes dual objectives: it mitigates the limitations of ﬁne-grained feature fusion
and bridges the semantic gap across features at varying scales, thereby augmenting the
global context across all feature maps. Experimental validation on the BTCV and CRD
datasets indicates that STU3 outperforms existing SOTA in terms of accuracy. Future
research will focus on the applicability to different segmentation tasks, such as tumors,
polyps, and cancerous tissues. Transitioning the model from 2D to 3D to enhance seg-
mentation precision in medical imaging also constitutes a pivotal avenue for future
exploration.
References
1. Razzak, M.I., Naz, S., Zaib, A.: Deep learning for medical image processing: overview,
challenges and the future. In: Classiﬁcation in BioApps: Automation of Decision Making,
pp. 323–350 (2018)
2. Taghanaki, S.A., Abhishek, K., Cohen, J.P., Cohen-Adad, J., Hamarneh, G.: Deep semantic
segmentation of natural and medical images: a review. Artif. Intell. Rev. 54(1), 137–178
(2020)
3. Minaee, S., Boykov, Y., Porikli, F., Plaza, A., Kehtarnavaz, N., Terzopoulos, D.: Image
segmentation using deep learning: a survey. CoRR, abs/2001.05566, pp. 1–22 (2020)
4. Ronneberger, O., Fischer, P., Brox, T.: U-net: convolutional networks for biomedical image
segmentation. In: International Conference on Medical Image Computing and Computer-
Assisted Intervention, pp. 234–241 (2015). https://doi.org/10.1007/978-3-319-24574-4_284

180
W. Zheng et al.
5. Zhou, Z., Siddiquee, R., Tajbakhsh, N., et al.: UNet++: redesigning skip connections to exploit
multiscale features in image segmentation. IEEE Trans. Med. Imaging 39(6), 1856–1867
(2019)
6. Huang, H., Lin, L., Tong, R., et al.: Unet 3+: a full-scale connected UNet for medical image
segmentation. In: IEEE International Conference on Acoustics, Speech and Signal Processing
(2020)
7. Li, X., Chen, H., Qi, X., Dou, Q., Fu, C.W., Heng, P.-A.: H-Denseunet: hybrid densely
connected UNet for liver and tumor segmentation from ct volumes. IEEE Trans. Med. Imaging
37(12), 2663–2674 (2018)
8. Valanarasu,J.M.J.,Sindagi,V.A.,Hacihaliloglu,I.,etal.:Kiu-net:overcompleteconvolutional
architectures for biomedical image and volumetric segmentation. IEEE Trans. Med. Imaging
41(4), 965–976 (2021)
9. Chen,J.,etal.:Transunet:transformersmakestrongencodersformedicalimagesegmentation.
CoRR, abs/2102.04306 (2021)
10. Gu, Z., et al.: CE-net: context encoder network for 2d medical image segmentation. IEEE
Trans. Med. Imaging 38(10), 2281–2292 (2019). https://doi.org/10.1109/TMI.2019.2903562
11. Vaswani, A., Shazeer, N., Parmar, N.: Attention is all you need. In: Advances in Neural
Information Processing Systems, pp. 5998–6008 (2017)
12. Yuan, L., et al.: Tokens-to-token vit: training vision transformers from scratch on imagenet.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 558–567
(2021). https://doi.org/10.48550/arXiv.2101.11986
13. Touvron, H., Cord, M., Douze, M., et al.: Training data-efﬁcient image transformers & distilla-
tion through attention. In: International Conference on Machine Learning, pp. 10347–10357.
PMLR (2021)
14. Gou, J., Yu, B., Maybank, S.J., et al.: Knowledge distillation: a survey. Int. J. Comput. Vision
129, 1789–1819 (2021)
15. Liu,Z.,etal.:Swintransformer:hierarchicalvisiontransformerusingshiftedwindows.CoRR,
abs/2103.14030 (2021). https://arxiv.org/abs/2103.14030
16. Cao, H., et al.: Swin-unet: unet-like pure transformer for medical image segmentation. arXiv
preprint arXiv:2105.05537 (2021)
17. Lin, A., Chen, B., Xu, J., et al.: DS-transunet: Dual swin transformer u-net for medical image
segmentation. IEEE Trans. Instrum. Meas. 71, 1–15 (2022)
18. Atek, S., Mehidi, I., Jabri, D., et al.: SwinT-Unet: hybrid architecture for medical image seg-
mentation based on swin transformer block and dual-scale information. In: 2022 7th Interna-
tional Conference on Image and Signal Processing and their Applications (ISPA), pp. 1–6.
IEEE (2022)
19. Hatamizadeh A, Nath V, Tang Y, et al. Swin UNETR: swin transformers for semantic seg-
mentation of brain Tumors in MRI images. In: Crimi, A., Bakas, S. (eds.) BrainLes 2021.
LNCS, vol. 12962, pp. 272–284. Springer, Cham (2021). https://doi.org/10.1007/978-3-031-
08999-2_22
20. Bojesomo, A., Al-Marzouqi, H., Liatsis, P.: Spatiotemporal swin transformer network for
short time weather forecasting. In: 1st Workshop on Complex Data Challenges in Earth
Observation, 01 November 2021 (2021)
21. Shaw, P., Uszkoreit, J., Vaswani, A.: Self-attention with relative position representations.
arXiv preprint arXiv:1803.02155 (2018)
22. Wu, K., Peng, H., Chen, M., et al.: Rethinking and improving relative position encoding for
vision transformer. In: Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 10033–10041 (2021)
23. Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with atrous
separable convolution for semantic image segmentation, arXiv:1802.02611 (2018)

STU3: Multi-organ CT Medical Image Segmentation Model
181
24. Isensee, F., Jaeger, P.F., Kohl, S.A.A., Petersen, J., Maier-Hein, K.H.: NNU-net: a self-
conﬁguring method for deep learning-based biomedical image segmentation. Nat. Methods
18(2), 203–211 (2021)
25. Hatamizadeh, A., et al.: UNETR: transformers for 3D medical image segmentation. In:
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
pp. 574–584 (2022)
26. Dice, L.R.: Measures of the amount of ecologic association between species. Ecology 26(3),
297–302 (1944). https://doi.org/10.2307/1932
27. Liu, J., Zhang, Y., Chen, J.N., et al.: Clip-driven universal model for organ segmentation and
Tumor detection. arXiv preprint arXiv:2301.00785 (2023)

Integrating Human Parsing and Pose
Network for Human Action Recognition
Runwei Ding1, Yuhang Wen2, Jinfu Liu2, Nan Dai3, Fanyang Meng4,
and Mengyuan Liu1(B)
1 Shenzhen Graduate School, Peking University, Shenzhen, China
nkliuyifang@gmail.com
2 Sun Yat-sen University, Shenzhen, China
3 Changchun University of Science and Technology, Changchun, China
4 Peng Cheng Laboratory, Shenzhen, China
Abstract. Human skeletons and RGB sequences are both widely-
adopted input modalities for human action recognition. However, skele-
tons lack appearance features and color data suﬀer large amount of irrel-
evant depiction. To address this, we introduce human parsing feature
map as a novel modality, since it can selectively retain spatiotemporal
features of the body parts, while ﬁltering out noises regarding outﬁts,
backgrounds, etc. We propose an Integrating Human Parsing and Pose
Network (IPP-Net) for action recognition, which is the ﬁrst to leverage
both skeletons and human parsing feature maps in dual-branch approach.
The human pose branch feeds compact skeletal representations of diﬀer-
ent modalities in graph convolutional network to model pose features.
In human parsing branch, multi-frame body-part parsing features are
extracted with human detector and parser, which is later learnt using a
convolutional backbone. A late ensemble of two branches is adopted to
get ﬁnal predictions, considering both robust keypoints and rich semantic
body-part features. Extensive experiments on NTU RGB+D and NTU
RGB+D 120 benchmarks consistently verify the eﬀectiveness of the pro-
posed IPP-Net, which outperforms the existing action recognition meth-
ods. Our code is publicly available at https://github.com/liujf69/IPP-
Net-Parsing.
Keywords: Action recognition · Human parsing · Human skeletons
1
Introduction
Human action recognition is an essential task in the ﬁeld of computer vision,
which has great research value and broad application prospects in human-robot
R. Ding and Y. Wen—Contributed equally to this work.
This work was supported by the Basic and Applied Basic Research Foundation of
Guangdong (No. 2020A1515110370) and the National Natural Science Foundation of
China (No. 62203476).
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 182–194, 2024.
https://doi.org/10.1007/978-981-99-8850-1_15

IPP-Net
183
Description: 
Geometric Location of Joints
Connectivity
Description: 
Body-part Appearance
Shapes & Contours
Preliminary Judgment:
Clapping? Snap Fingers? Drinking? 
Preliminary Judgment:
Clutching palm-sized items
Comprehensive Judgment: 
More likely to be Drinking
Complementarity: 
Robust Key Points (Pose)
Rich Semantic Feature (Parsing)
Fig. 1. An example of the modality complementarity between the human pose and
the human parsing map. A more comprehensive prediction can be made by integrating
robust key points from poses and rich semantic features from parsing feature maps.
interaction [26] and visual media [29,32]. Most action recognition methods take
human skeletons sequences [11,13,14,24,30] or color images [23] as the input
modality for the following reasons. A human skeleton can be conceptualized as
an inherent topological graph, which can accurately portray body movements
and is highly robust to environmental changes, thereby adopted in many stud-
ies using graph convolutional neural networks (GCNs) [2,4,5,24,28,33]. On the
other hand, color images (RGB data) have rich appearance information, therefore
some prior studies in action recognition employ Convolutional Neural Networks
(CNNs) [6,23] to learn spatiotemporal features from video frames. In recent
years, methods [6] of integrating skeletal and RGB data have emerged to make
eﬀective use of multimodal features for better action recognition.
However, both skeletal and RGB data have their respective limitations in
representing human actions. Skeletons lack the capacity to convey the visual
appearance of individual body parts, while RGB features are susceptible to dis-
turbances from diverse sources, such as background interference, irrelevant ele-
ments, and changes in illumination. This naturally leads to the question: Can
we explore a new modality that incorporates body-part appearance depiction
while remaining noiseless and robust?
Our aﬃrmative response is inspired by Human Parsing, a vision task that
holds signiﬁcant importance in video surveillance and human behavior analysis.
This task facilitates the recognition of distinct semantic parts, such as limbs,
within the human body [9]. By focusing on these semantic parts, human pars-
ing explicitly and eﬀectively eliminates action-irrelevant details, while retaining
crucial extrinsic features of the human body. We believe this can eﬀectively
complement the skeletal data, as illustrated in Fig. 1.

184
R. Ding et al.
In this work, we advocate to integrate human parsing feature map as a novel
modality into action recognition framework, and propose an Integrating Human
Parsing and Pose Network (IPP-Net). Speciﬁcally, our IPP-Net consists of two
trunk branches. In the ﬁrst branch, referred to as the human pose branch, pose
data is transformed into diﬀerent skeleton representations and subsequently fed
into a graph convolutional neural network to obtain predictions. In the second
branch, dubbed the human parsing branch, the human parsing features from
multiple frames are sequentially combined to construct a feature map. This fea-
ture map is subsequently inputted into a convolutional neural network to derive
recognition results. The results from both branches are integrated via a softmax
layer to make ﬁnal predictions. By leveraging the proposed IPP-Net, we eﬀec-
tively integrate pose data and human parsing feature maps to achieve better
human action recognition.
The contributions of our work are summarized as follows:
1. We advocate to leverage human parsing feature map as a new modality for
human action recognition task, which is appearance-oriented depictive and
also action-relevant.
2. We propose a framework called Integrating Human Parsing and Pose Network
(IPP-Net), which is the ﬁrst to eﬀectively integrates human parsing feature
maps and pose data for robust human action recognition. Speciﬁcally, pose
feature (representing body-part positions and connections) and human pars-
ing feature (representing body-part contours and appearance) are learnt in
two-stream approach and integrated via a late ensemble, to give comprehen-
sive judgements about actions.
3. Extensive experiments on benchmark NTU RGB+D and NTU RGB+D 120
datasets verify the eﬀectiveness of our IPP-Net, which outperforms most exist-
ing action recognition methods.
2
Related Work
2.1
Human Action Recognition
Prior approaches for human action recognition usually leverage skeletal data,
which is a compact and suﬃcient representation for human actions. A signiﬁcant
body of work deals with more eﬀective and eﬃcient model architecture design
for skeleton sequences [2,4,5,16,19,22,26,28,33], with the aim of exploiting more
informative joints. Beyond skeleton data, several of works use multi-modal fea-
tures of human actions, including RGB sequences [6] and text descriptions [27],
to achieve robust recognition results. VPN [6] embeds 3D skeletons with their
corresponding RGB videos, and feed them into an attention network to learn
spatiotemporal relations. Empowered by Large Language Model (LLM), LST
[27] conditions GCN training using the generated text descriptions of body-
part motions. Compared to single modality, Multi-modal inputs provide unique
information for each type of action, albeit in diﬀerent forms, thereby enhanc-
ing human action understanding. However, it may contain irrelevant features

IPP-Net
185
The Human Pose Branch
BN
B1 (3,64)
B2 (64,64)
B3 (64,64)
B4 (64,64)
B5 (64,128)
B6 (128,128)
B7 (128,128)
B8 (128,256)
B9 (256,256)
B10 (256,256)
GCN Block
Linear (256,NC)
Block1
Block2
B1ock3
Linear (2024,NC)
Drink Water
Pose Estimation
OR
Sensor Capture
Human Parsing
Feature Map
Generation
CNN Block
The Human Parsing Branch
Ensemble
Integration
Modalities
Detector
Parser
: Weights
Fig. 2. Framework of our proposed Integrating Human Parsing and Pose Network.
(e.g. speciﬁc outﬁts, backgrounds). Our IPP-Net also leverages multi-modalities,
and we argues to utilize pose data and feature map sequences of human pars-
ing. Compared with the above approaches, human parsing ﬁlters out irrelevant
information regarding illumination and backgrounds, while selectively retaining
spatiotemporal features of all body parts.
2.2
Human Parsing
Human parsing involves the segmentation of a human image into ﬁnely detailed
parts, including the limbs, the head, the torso, etc. Several benchmarks have
been proposed for human parsing task, providing large-scale annotations of body
parts [9,10]. A number of works concentrated on this problem and proposed novel
models for better semantic parsing. The majority are based on ResNet architec-
ture [8,9,17,31], while some are based on Transformer architecture [1]. Inspired
by the human parsing task, we exploit the advantages of human parsing to get
noiseless and concise representations, thus proposing a framework to ensemble
parsing results and skeletons.
3
IPP-Net
As depicted in Fig. 2, in our proposed Integrating Human Parsing and Pose Net-
work, we incorporate two primary branches, speciﬁcally the human pose branch
and the human parsing branch. The human pose branch (Sect. 3.1) utilizes GCN
to model the skeleton data, while the human parsing branch (Sect. 3.2) employs
CNN to learn spatiotemporal features of the human parsing feature maps. Subse-
quently, the outcomes from these two branches are integrated via a late ensemble
to get action predictions (Sect. 3.3).

186
R. Ding et al.
3.1
Human Pose Learning
Skeleton Data. Skeleton data ﬁnds extensive applications in action recognition
frameworks. The pose branch leverages 3D skeleton data collected by sensors. In
a conceptual sense, the skeletal sequence can be likened to a naturally occurring
topological graph, with joints serving as the graph’s vertices and bones as its
connecting edges. The graph is represented as G = {V , E}, where V represents
a collection of N joints and E comprises the set of skeletal bones. When working
with 3D skeletal data, a joint v i is represented as {x i, y i, z i}, with components
specifying the three-dimensional coordinates of point v i within Euclidean space.
Skeleton data can be categorized into four distinct modalities: joint (J),
bone (B), joint motion (JM ) and bone motion (BM ). The bone modality can
be derived by calculation between two physically connected joints, while joint
motion and bone motion modalities can be calculated separately using joint and
bone data from two successive frames respectively. We acquire the four skeleton
modalities following the formula deﬁned in [11].
Backbone. GCN-based approaches have emerged as predominant in the domain
of skeleton-based action recognition for their unique advantages in modeling
graph-structured data. Our IPP-Net also embraces GCN as backbone to learn
pose features. A Graph Convolutional Network (GCN) consists of both graph
convolutions and temporal convolutions. The typical form of a graph convolution
can be formulated as:
Hl+1 = σ

D−1
2 AD−1
2 HlW l
,
(1)
where Hl is the joint features at layer l and σ is the activation function. D ∈
RN×N is the degree matrix of N joints and Wl is the learnable parameter of
the l-th layer. A denotes the adjacency matrix that signiﬁes the connections
between joints and can be obtained in static (predeﬁned manually) or dynamic
(initialized manually but learnable) ways. Our IPP-Net feeds the normalized four
skeleton modalities deﬁned above into the ten dynamic GCN blocks for feature
extraction, followed by a linear head to obtain the recognition result.
3.2
Human Parsing Learning
Human Parsing Feature Map Generation. Inspired by the human parsing
task, we propose human parsing feature maps as one novel modality represent-
ing body-part movements with appearances. The following steps shows how we
generate this new modality from raw RGB videos. Given an RGB video stream
F = {f 1, f 2, · · · , f N} with N frames, frame-level feature is extracted by
I i = E(D(f i)),
(2)
where 1 ≤i ≤N, i ∈N, then D and E denote an object detector and a fea-
ture extractor respectively. In implementation, we use YoloV5 [25] as the target

IPP-Net
187
Construction
Detection
Parsing
(a)
(b)
Sampling
Human Action
Human Paring
Feature Map
Fig. 3. (a) Illustration of human parsing feature map generation from RGB videos. (b)
Visualization of human parsing feature maps for diﬀerent action samples.
detector of our model, and feed the detected human maps into Resnet101 [7] to
extract the features I i of each frame.
The frame-level feature I i gets downsampled and upsampled in PSPNet [31]
for human parsing, formulated as
ˆ
Pi = argmaxclass(PSPNet(I i)),
(3)
where 1 ≤i ≤N, i ∈N. ˆ
Pi retrieves the index of the most possible body part
category for each pixel.
Then the frame-level maps get resized to a standard shape [h, w]. We sample
total Nsample frames from N frame-level maps with the random equidistant
sampling strategy, denoted as
˜
Pk = ˆ
P1+δ×(k−1)
(4)
where 1 ≤k ≤Nsample, k ∈N. Suppose Nsample ≤N, then δ is a random
positive integer between 1 and ⌊N/Nsample⌋. If Nsample > N, then we repeat
the N maps until Nsample frames are sampled. The chosen Nsample feature maps
are chronologically arranged to construct the ﬁnal feature map P . Figure 3 (b)
visualizes the feature maps of three action samples, namely drink water, hand
waving and salute.
Backbone. Intuitively the human parsing feature maps repeated in channel
dimension can be viewed as 3-channel grayscale images. Therefore, a convolu-
tional backbone is adopted to extract deep parsing features for its strong percep-
tion for locality, which is essential to percept graphic structures, such as edges
and shapes in parsing feature maps P .

188
R. Ding et al.
3.3
Integration
The outcomes from the human pose branch and the human parsing branch are
fused via an ensemble layer, which is formulated as:
S = softmax(α · C(V ) + β · M(P)),
(5)
where V and P denote the skeleton data and human parsing feature maps
respectively. The skeleton data V and feature maps P are respectively fed into
a GCN C and a CNN M. The parameters α and β represent the ensemble
weights for the linear combination. After a softmax layer, we could acquire the
prediction S.
4
Experiments
4.1
Datasets
Experiments are conducted on two widely-adopted large-scale human action
recognition datasets, illustrated as follows:
NTU-RGB+D [18], also referred as NTU 60, is a extensively employed 3D
action recognition dataset comprising over 56,000 video samples. These actions
have been executed by 40 individuals and classiﬁed into 60 distinct categories.
The original paper [18] suggests two benchmark scenarios for evaluation: (a)
Cross-View (X-View), where the training set originates from cameras at 0◦and
45◦, and the testing set is sourced from another camera at 45◦. (b) Cross-Subject
(X-Sub), where the training set comprises samples from 20 subjects, while the
remaining 20 subjects are reserved for testing.
NTU-RGB+D 120 [12], also referred as NTU 120, is derived from the NTU-
RGB+D dataset. A total of 114,480 video clips across 120 daily action classes
performed by 106 volunteers are recorded using 3 Kinect V2 TOF cameras. The
original work [12] also suggests two criteria: (a) Cross-subject (X-Sub), where
similarly the entire dataset is divided into two halves based on the number of
subjects for both training and testing. (b) Cross-setup (X-Set), where the entire
dataset is divided into two halves based on the parity of ID numbers for both
training and testing.
4.2
Implementation Details
All experiments were carried out using four Tesla V100-PCIE-32 GB GPUs and
two NVIDIA GeForce RTX 3070 GPUs. The pose branch adopts CTR-GCN [2]
as the backbone. We use SGD for model training, conducting 65 epochs with a
batch size of 64. The initial learning rate decayed from 0.1 by a factor of 0.1 at
epochs 35 and 55. The parsing branch adopts InceptionV3 [21] as the backbone.

IPP-Net
189
Table 1. Top-1 accuracy comparison with state-of-the-art methods on NTU-RGB+D
and NTU-RGB+D 120 dataset.
Type
Method
Source
NTU 60
(%) NTU 120 (%)
X-Sub X-View X-Sub
X-Set
Pose
Shift-GCN [4]
CVPR’20
90.7
96.5
85.9
87.6
DynamicGCN [28]
MM’20
91.5
96.0
87.3
88.6
DSTA-Net [19]
ACCV’20
91.5
96.4
86.6
89.0
MS-G3D [15]
CVPR’20
91.5
96.2
86.9
88.4
MST-GCN [3]
AAAI’21
91.5
96.6
87.5
88.8
CTR-GCN [2]
ICCV’21
92.4
96.8
88.9
90.6
GS-GCN [33]
CICAI’22
90.2
95.2
84.9
87.1
PSUMNet [22]
ECCV’22
92.9
96.7
89.4
90.6
InfoGCN [5]
CVPR’22
93.0
97.1
89.8
91.2
STSA-Net [16]
Neurocomputing’23
92.7
96.7
88.5
90.7
Multi-Modality
VPN [6]
ECCV’20
93.5
96.2
86.3
87.8
LST [27]
arXiv’22
92.9
97.0
89.9
91.1
Ours (J+B+P)
93.4
96.8
89.4
91.2
Ours
93.8
97.1
90.0
91.7
Similarly SGD is used to optimize the model for 30 epochs with the same batch
size. The initial learning rate decayed from 0.1 by a factor of 0.0001 at epochs 10
and 25. The cross-entropy loss is employed as the training loss. During training
we select 5 frames randomly to construct the feature maps of human parsing,
while 5 frames at equal intervals in testing. When generating the human parsing
feature map, we resize the map of each frame to a designated size of [480, 96],
ﬁve frames of which are arranged in chronological order to construct the ﬁnal
feature with a size of [480, 480].
4.3
Comparison with Related Methods
Table 1 compares our IPP-Net with the existing human action recognition meth-
ods on the NTU 60 and NTU 120 datasets. In both of these extensive action
recognition datasets, our model excels and surpasses all existing methods across
virtually all evaluation metrics. Notably, our IPP-Net is the ﬁrst one that com-
bines human parsing and pose data for action recognition.
In the NTU 60 dataset, the top-1 accuracy reaches 93.8% and 97.1% on the
benchmark of X-Sub and X-View respectively, which outperforms CTR-GCN [2]
by 1.4% and 0.3%. On the tougher benchmark namely X-Sub, our IPP-Net out-
performs LST [27] by 0.9%, even though the LST model additionally introduces
texts as a new modality. In the NTU 120 dataset, the top-1 accuracy is 90.0% and
91.7% on the benchmark of X-Sub and X-View respectively, which outperforms
CTR-GCN by 1.1% and 1.1%. The most related method to our work is VPN [6],
which introduces RGB features besides skeletons for action recognition. On two
benchmarks of NTU 120 database, our IPP-Net outperforms VPN by 3.7% and

190
R. Ding et al.
Fig. 4. Visual explanation with class activation maps on how the convolutional back-
bone judges the human action by the given parsing feature maps.
Table 2. Accuracy of diﬀerent modalities on NTU-RGB+D and NTU-RGB+D 120.
Modality
NTU 60 (%)
NTU 120 (%)
J
B
JM BM P
X-Sub X-View X-Sub X-Set

90.2
95.0
85.0
86.7

90.5
94.7
86.2
87.5

88.1
93.2
81.2
83.0

87.3
92.0
81.7
82.9
 73.5
74.2
53.6
65.8

 91.7
95.9
86.1
88.8

 91.8
95.9
87.5
89.8


92.2
96.2
88.7
90.2

 

92.4
96.5
89.0
90.5


 93.4
96.8
89.4
91.2

 

 93.8
97.1
90.0
91.7
3.9% respectively. Figure 4 visually illustrates how human parsing feature maps
in our IPP-Net help recognize actions by providing semantic information about
body parts. The class activation maps indicates that the human parsing branch
can focus on the most informative body parts.
4.4
Ablation Study
Modality. Table 2 reports the recognition accuracy of four skeleton modalities
and human parsing feature map in NTU60 and NTU120 datasets respectively.
When the joint, bone and human parsing feature maps are integrated together
(J+B+P), the ensemble recognition accuracy of two benchmarks in the NTU60
dataset is 93.4% and 96.8%, which outperforms the setting that combining joint

IPP-Net
191
Table 3. Comparison of diﬀerent numbers of frames in feature map construction.
#Frame Parsing (%) Ensemble (%)
3
46.7
89.8
4
50.4
89.9
5
53.6
90.0
6
55.6
89.9
Table 4. Accuracy of diﬀerent CNN backbones in human parsing branch.
Backbone
Parsing (%) Ensemble (%)
VGG11 [20]
49.0
89.8
VGG13 [20]
48.7
89.8
ResNet18 [7]
50.5
90.0
InceptionV3 [21] 53.6
90.0
and bone (J+B) by 1.2% and 0.6% respectively. Similarly, the setting (J+B+P)
outperforms the setting (J+B) by 0.7% and 1.0% on the two benchmarks of the
NTU120 dataset respectively. It can conclude from the experimental results that
integrating human parsing as a modality into the framework can improve action
recognition performance, which is attributed to the action-relevant body-part
appearance provided by human parsing feature maps.
Frames for Human Parsing Feature Maps. We explore how numbers of
frames for constructing human parsing feature maps aﬀect the accuracy on
NTU120 X-Sub benchmark. As depicted in Table 3, we use 3-frame, 4-frame,
5-frame and 6-frame settings respectively to make up feature maps. The shapes
of the four maps are all generated as [480,480] and then resized to [299,299] to
meet InceptionV3’s demand. We observe that the ensemble accuracy is 90.0%
when using 5-frame parsing maps, which is the highest among the four settings.
Backbones to Learn Parsing Features. On NTU120 X-Sub benchmark, we
implement four diﬀerent CNN backbones to learn parsing features. Results in
Table 4 indicate that InceptionV3 gets the highest ensemble accuracy among all
the CNN backbones.
5
Conclusions
This work proposes an Integrating Human Parsing and Pose Network (IPP-Net)
for human action recognition, which introduces the human parsing feature maps
as a new modality to represent human actions. Human parsing can selectively
preserve spatiotemporal body-part features while ﬁltering out action-irrelevant

192
R. Ding et al.
information in RGB data. As a multi-modal action recognition framework, our
IPP-Net is the ﬁrst to leverage both skeletons and human parsing feature maps,
considering both robust keypoints and rich semantic body-part features. The
eﬀectiveness of IPP-Net is veriﬁed on the NTU-RGB+D and NTU-RGB+D 120
datasets, where our IPP-Net outperforms most existing methods.
References
1. Chen, W., et al.: Beyond appearance: a semantic controllable self-supervised learn-
ing framework for human-centric visual tasks. In: Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR) (2023)
2. Chen, Y., Zhang, Z., Yuan, C., Li, B., Deng, Y., Hu, W.: Channel-wise topology
reﬁnement graph convolution for skeleton-based action recognition. In: Proceedings
of the IEEE International Conference on Computer Vision (CVPR) (2021)
3. Chen, Z., Li, S., Yang, B., Li, Q., Liu, H.: Multi-scale spatial temporal graph
convolutional network for skeleton-based action recognition. In: Proceedings of the
AAAI Conference on Artiﬁcial Intelligence (AAAI) (2021)
4. Cheng, K., Zhang, Y., He, X., Chen, W., Cheng, J., Lu, H.: Skeleton-based action
recognition with shift graph convolutional network. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2020)
5. Chi, H.G., Ha, M.H., Chi, S., Lee, S.W., Huang, Q., Ramani, K.: InfoGCN: rep-
resentation learning for human skeleton-based action recognition. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
(2022)
6. Das, S., Sharma, S., Dai, R., Br´emond, F., Thonnat, M.: VPN: learning video-
pose embedding for activities of daily living. In: Vedaldi, A., Bischof, H., Brox, T.,
Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12354, pp. 72–90. Springer, Cham
(2020). https://doi.org/10.1007/978-3-030-58545-7 5
7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2016)
8. Li, P., Xu, Y., Wei, Y., Yang, Y.: Self-correction for human parsing. IEEE Trans.
Pattern Anal. Mach. Intell. 44(6), 3260–3271 (2020)
9. Liang, X., Gong, K., Shen, X., Lin, L.: Look into person: joint body parsing &
pose estimation network and a new benchmark. IEEE Trans. Pattern Anal. Mach.
Intell. 41(4), 871–885 (2019)
10. Liang, X., et al.: Deep human parsing with active template regression. IEEE Trans.
Pattern Anal. Mach. Intell. 37(12), 2402–2414 (2015)
11. Liu, J., Wang, X., Wang, C., Gao, Y., Liu, M.: Temporal decoupling graph convo-
lutional network for skeleton-based gesture recognition. IEEE Trans. Multimedia
(2023)
12. Liu, J., Shahroudy, A., Perez, M., Wang, G., Duan, L.Y., Kot, A.C.: NTU RGB+D
120: a large-scale benchmark for 3D human activity understanding. IEEE Trans.
Pattern Anal. Mach. Intell. 42(10), 2684–2701 (2020)
13. Liu, M., Meng, F., Chen, C., Wu, S.: Novel motion patterns matter for practical
skeleton-based action recognition. In: AAAI Conference on Artiﬁcial Intelligence
(AAAI) (2023)
14. Liu, M., Meng, F., Liang, Y.: Generalized pose decoupled network for unsupervised
3D skeleton sequence-based action representation learning. Cyborg Bionic Syst.
2022, 0002 (2022)

IPP-Net
193
15. Liu, Z., Zhang, H., Chen, Z., Wang, Z., Ouyang, W.: Disentangling and unifying
graph convolutions for skeleton-based action recognition. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020)
16. Qiu, H., Hou, B., Ren, B., Zhang, X.: Spatio-temporal segments attention for
skeleton-based action recognition. Neurocomputing 518, 30–38 (2023)
17. Ruan, T., Liu, T., Huang, Z., Wei, Y., Wei, S., Zhao, Y.: Devil in the details:
towards accurate single and multiple human parsing. In: Proceedings of the AAAI
Conference on Artiﬁcial Intelligence (AAAI) (2019)
18. Shahroudy, A., Liu, J., Ng, T.T., Wang, G.: NTU RGB+D: a large scale dataset for
3D human activity analysis. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2016)
19. Shi, L., Zhang, Y., Cheng, J., Lu, H.: Decoupled spatial-temporal attention net-
work for skeleton-based action-gesture recognition. In: Proceedings of the Asian
Conference on Computer Vision (ACCV) (2020)
20. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition (2015)
21. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the incep-
tion architecture for computer vision. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (2016)
22. Trivedi, N., Sarvadevabhatla, R.K.: PSUMNet: uniﬁed modality part streams are
all you need for eﬃcient pose-based action recognition. In: Karlinsky, L., Michaeli,
T., Nishino, K. (eds.) Computer Vision – ECCV 2022 Workshops. ECCV 2022.
LNCS, vol. 13805. Springer, Cham (2023). https://doi.org/10.1007/978-3-031-
25072-9 14
23. Tu, Z., Li, H., Zhang, D., Dauwels, J., Li, B., Yuan, J.: Action-stage emphasized
spatiotemporal VLAD for video action recognition. IEEE Trans. Image Process.
28(6), 2799–2812 (2019)
24. Tu, Z., Zhang, J., Li, H., Chen, Y., Yuan, J.: Joint-bone fusion graph convolutional
network for semi-supervised skeleton action recognition. IEEE Trans. Multimedia
25, 1819–1831 (2022)
25. Ultralytics: ultralytics/yolov5: v7.0 - YOLOv5 SOTA Realtime Instance Segmen-
tation (2022)
26. Wen, Y., Tang, Z., Pang, Y., Ding, B., Liu, M.: Interactive spatiotemporal token
attention network for skeleton-based general interactive action recognition. In:
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
(2023)
27. Xiang, W., Li, C., Zhou, Y., Wang, B., Zhang, L.: Language supervised training
for skeleton-based action recognition. arXiv:2208.05318 (2022)
28. Ye, F., Pu, S., Zhong, Q., Li, C., Xie, D., Tang, H.: Dynamic GCN: context-
enriched topology learning for skeleton-based action recognition. In: Proceedings
of the ACM International Conference on Multimedia (ACM MM) (2020)
29. Zhang, F.L., Wu, X., Li, R.L., Wang, J., Zheng, Z.H., Hu, S.M.: Detecting and
removing visual distractors for video aesthetic enhancement. IEEE Trans. Multi-
media 20(8), 1987–1999 (2018)
30. Zhang, J., Jia, Y., Xie, W., Tu, Z.: Zoom transformer for skeleton-based group
activity recognition. IEEE Trans. Circuits Syst. Video Technol. 32(12), 8646–8659
(2022)
31. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2017)

194
R. Ding et al.
32. Zheng, Z.H., Zhang, H.T., Zhang, F.L., Mu, T.J.: Image-based clothes changing
system. Comput. Visual Media 3, 337–347 (2017)
33. Zhu, S., Zhan, Y., Zhao, G.: Multi-model lightweight action recognition with group-
shuﬄe graph convolutional network. In: Proceedings of the CAAI International
Conference on Artiﬁcial Intelligence (CICAI) (2022)

Lightweight Rolling Shutter Image
Restoration Network Based
on Undistorted Flow
Binfeng Wang1, Yunhao Zou1, Zhijie Gao1,2(B), and Ying Fu1,2
1 MIIT Key Laboratory of Complex-ﬁeld Intelligent Sensing,
Beijing Institute of Technology, Beijing 100081, China
2 Yangtze Delta Region Academy of Beijing Institute of Technology,
Jiaxing 314019, China
gzj@bit.edu.cn
Abstract. Rolling shutter(RS) cameras are widely used in ﬁelds such
as drone photography and robot navigation. However, when shooting a
fast-moving target, the captured image may be distorted and blurred
due to the feature of progressive image collection by the rs camera. In
order to solve this problem, researchers have proposed a variety of meth-
ods, among which the methods based on deep learning perform best, but
it still faces the challenges of poor restoration eﬀect and high practical
application cost. To address this challenge, we propose a novel lightweight
rolling image restoration network, which can restore the global image at
the intermediate moment from two consecutive rolling images. We use a
lightweight encoder-decoder network to extract the bidirectional optical
ﬂow between rolling images. We further introduce the concept of time
factor and undistorted ﬂow, calculate the undistorted ﬂow by multiplying
the optical ﬂow by the time factor. Then bilinear interpolation is per-
formed through the undistorted ﬂow to obtain the intermediate moment
global image. Our method achieves the state-of-the-art results in several
indicators on the RS image dataset Fastec-RS with only about 6% of
that of existing methods.
Keywords: Rolling Shutter Image Restoration · Undistorted Flow ·
Optical Flow
1
Introduction
Depending on the type of shutter, cameras can be divided into two types, rolling
shutter(RS) camera and global shutter(GS) camera. As shown in Fig. 1(left),
the GS camera scans each line of the image in parallel when shooting, while
the RS camera scans line by line when shooting. The RS camera can obtain
images with a higher frame rate and higher deﬁnition at a lower cost and has
a wide range of applications in the ﬁelds of motion capture, machine vision,
and consumer electronics. However, due to the progressive imaging mechanism
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 195–206, 2024.
https://doi.org/10.1007/978-981-99-8850-1_16

196
B. Wang et al.
of the RS camera, when the subject or the camera is moving rapidly, the images
captured will be distorted and blurred. Consequently, as shown in Fig. 1(right),
in the ﬁrst row of images, the door in the RS image is heavily skewed compared
to the GS image.
Fig. 1. Our model inputs two consecutive RS images and outputs the GS image corre-
sponding to the intermediate moment. As can be seen from the second row of results,
the restoration results of our method have higher geometric similarity to the real GS
image than other models.
In order to solve the RS eﬀect, researchers have proposed many solu-
tions, which can be roughly divided into two categories: hardware-based and
calculation-based. Hardware-based solutions implement global functionality by
placing additional memory nodes on the RS sensor to store charge, voltage, and
even pixels in the digital domain. But hardware-based solutions have signiﬁcant
disadvantages in terms of sensor size, cost, and noise.
The calculation-based method corrects the captured RS image through statis-
tics, formulas, models and other methods. Through the combination of image
deblurring, video frame interpolation, optical ﬂow and other methods in the ﬁeld
of computer vision, the RS image correction based on deep learning has made
great breakthroughs. Liu et al. [1] ﬁrst proposed DSUN, which inputs two consec-
utive RS images, restores the GS image corresponding to the second frame, and
constructs the ﬁrst synthetic datasets Fastec-RS for the RS image correction.
Zhong et al. [8] constructed the ﬁrst real-world dataset BS-RSCD for the RS
image correction problem, and proposed a deep learning model JCD combined
with image deblurring. Fan et al. [2–4,10] combined RS image correction and
video frame interpolation to predict the GS image at the intermediate moment
from two consecutive rolling images. Other models including CVR [4], SUNet
[3], RSSR [2] predict the bidirectional optical ﬂow and undistorted ﬂow between
the RS images, and then use the undistorted ﬂow to warp the RS images into
the GS. VRS [10] uses trained video frame interpolation models to interpolate
a large number of RS image interpolation frames between two RS images, and

LRSIRN-UF
197
combines these frames into a GS image. Among them, CVR [4] and VRS [10] per-
form best, which is the current state-of-the-art. However, to predict a GS image,
VRS [10] needs to generate thousands of candidate frames, which wastes a lot of
memory and computing power. CVR [4] uses multiple U-Nets for superposition,
yet the model occupies too much memory, making it diﬃcult to deploy in practi-
cal applications. Cao et al. [7] constructed another real-world RS image dataset
BS-RSC, and proposed adaptive warping and multiple displacement ﬁelds to
capture complex motions of objects. Zhou et al. [5,6,9] modiﬁed the form of
input data to obtain additional information, Wang et al. [6] proposed RS global
features, and built an optical instrument to acquire datasets RSGR-GS. Zhong
et al. [9] restores the intermediate GS image through two RS images scanned
from top to bottom and bottom to top, respectively, on the synthetic dataset
RS-GOPRO. Zhou et al. [5] uses the event camera to obtain the event signal of
the RS image, and corrects the RS image through the additional event signal.
EvUnroll [5], IFED [9], RSGR [6] require the use of new types of camera that is
expensive, making it diﬃcult to popularize.
Considering the above situation, we design a lightweight rolling shutter image
restoration model that restores the GS image at the intermediate moment from
two consecutive RS images with undistorted ﬂow. Our method achieves the best
results in several indicators on the RS image dataset Fastec-RS while the number
of parameters of our model is only about 6% of that of CVR [4].
2
Related Works
In this section, we present related works on our method, the most important of
which is video frame interpolation.
Video frame interpolation [17–19] increases the frame rate of a video by
inserting a new frame between two known frames. With recent advances in opti-
cal ﬂow estimation [11–13,15], ﬂow-based VFI methods have been actively stud-
ied to exploit motion information explicitly. Some methods employ oﬀ-the-shelf
ﬂow models [20,21] for interpolation, and estimate task-speciﬁc ﬂows as guidance
for pixel-level motion [14,22,23]. In addition, in order to improve the practica-
bility of the VFI model, some methods [16] supervise the intermediate results
of the interpolation, and realize eﬃcient video frame interpolation with fewer
parameters. VFI is often interpolated between two GS images. We have learned
its idea and interpolated GS image from RS image.
3
Approach
3.1
Problem Formulation
When a RS camera acquires an image, all its scanlines are exposed sequentially
with diﬀerent timestamps. Therefore, each scan line corresponds to a diﬀerent
GS image frame. Let the number of lines in the image be h and the constant

198
B. Wang et al.
inter-line delay time be td, the rolling shutter imaging model can be obtained as
follows:
[Ir(x)]s = [Ig
s(x)]s,
(1)
where s is the row number corresponding to the extracted pixel in the rolling
shutter image, and []s means to extract the pixel from the sth row of the image.
Assuming that the acquisition time corresponding to the ﬁrst row of the image
is 0, and the total number of rows of the image is h, then, Ig
s is the GS image
at time td · s.
Reversing the above process, we can use the RS image to restore the GS
image.
Ig(x) = Ir(x + U g→r(x)),
(2)
where U g→r is the displacement ﬁeld from each pixel in the GS image to the
corresponding pixel in the RS image, called undistorted ﬂow. However, the undis-
torted ﬂow is diﬃcult to predict directly, in this paper, we combine optical ﬂow
and temporal factors to predict the undistorted ﬂow.
Our task is to restore the GS image at the intermediate moment from two
consecutive RS images. And we make the following assumptions: 1. The acquisi-
tion time interval of each line of the rolling screen image is constant. 2. After the
last line of the RS image of the previous frame is collected, the ﬁrst line of the RS
image of the next frame will be collected immediately. In practical applications,
the speed of the RS camera can even start to capture the next frame before the
previous frame is captured. Therefore, this assumption is reasonable and easy to
implement.
3.2
Architecture Overview
We propose a lightweight rolling shutter image restoration network based on
undistorted ﬂow. Our model uses a lightweight encoder-decoder network to
extract the bidirectional optical ﬂow between two consecutive frames of RS
images, and then multiplies the optical ﬂow by a time factor to calculate the
undistorted ﬂow. Then, we use the undistorted ﬂow to warp the RS image to
calculate the candidate frames. Finally, we multiply the candidate frames by the
mask to obtain the GS image at the intermediate moment. We will introduce
these parts separately next. The overall structure of our model is shown in Fig. 2.
3.3
Extract Feature Pyramid
To obtain a contextual representation from each input rolling image frame, our
model ﬁrst extracts a feature pyramid with a compact encoder network. It should
be noted that we use the same set of encoder parameters for each RS image,
which greatly reduces the parameter amount of the encoder network.

LRSIRN-UF
199
The encoder network inputs a RS image and outputs a four-level feature
pyramid of the image. For the feature extraction of each layer, two 3 · 3 con-
volution blocks are used. The step sizes of the convolution blocks are 2 and 1,
respectively, and the activation function used is PReLU.
Fig. 2. The overall structure of our model.
3.4
Generate Bidirectional Optical Flow
To reconstruct the bidirectional optical ﬂow between rolling images from the fea-
ture pyramid, we use a multi-level decoder network for decoding. The decoder
network inputs the feature pyramids of two RS images and outputs the bidirec-
tional optical ﬂow, mask and residuals.
The main body of the decoder network is composed of four levels of pro-
gressively increasing decoders. The ﬁrst level decoder inputs the features at the
bottom of the two feature pyramids, and outputs the bidirectional optical ﬂow
and intermediate features of the next level. The second and third levels decoders
respectively input the intermediate features, bidirectional optical ﬂow, and con-
text features of this level, and outputs the intermediate features and bidirectional
optical ﬂow residuals of the next level. By upsampling the bidirectional optical
ﬂow of this layer, added with bidirectional optical ﬂow residuals, we can calculate
the bidirectional optical ﬂow of the next level, this method makes our training
process more stable. The fourth layer decoder does not generate intermediate
features, instead, it generates a GS image residual and a mask, which are used
to synthesize a GS image from candidate frames later. The formulation of the
decoders are:
(ΔF j+1
i
, Oj+1) = Dj(Gj
i); i = 0, 1; j = 1,
(3)

200
B. Wang et al.
(ΔF j+1
i
, Oj+1) = Dj(F j
i, Oj, Gj
i); i = 0, 1; j = 2, 3,
(4)
(ΔF i, M, Res) = Dj(F j
i, Oj, Gj
i); i = 0, 1; j = 4,
(5)
where F j
i is the optical ﬂow from image i to image 1 −i at level j, Oj is the
intermediate feature at level j, Gj
i is the feature extracted from RS image at
level j, M and Res are mask and residual used to merge candidate frames in
next step.
3.5
Time Factor and Undistortion Flow
Theoretically speaking, if two RS images are directly warped using bidirectional
optical ﬂow between RS images, the result of warping should be the RS image
corresponding to the middle frame, not the GS image. In order to obtain the GS
image corresponding to the intermediate frame, it is necessary to ﬁrst generate
the undistorted ﬂow from the optical ﬂow.
Fig. 3. Comparison of the time span of optical ﬂow and undistorted ﬂow.
In a RS image, the acquisition time of pixels in the same row is the same.
Figure 3 is a comparison of the time span of optical ﬂow and undistorted ﬂow.
The green dotted line in the left ﬁgure is the time period spanned by the optical
ﬂow of each row of pixels. Since the optical ﬂow between two RS images is
predicted, the length of each time period is the same. But their start times are
diﬀerent, the start time of the ﬁrst row is the earliest, and the start time of the
last row is the latest. Assuming that the image has h lines in total, the acquisition
time interval of each line of image is 1/h, the acquisition time of the middle line
of the ﬁrst frame image is t = 0, and the acquisition time of the middle line of
the second frame image is t = 1, then for the sth line image, the start time of its
optical ﬂow span is (s−h/2)/h, and the end time is (s+h/2)/h. And if the ﬁrst
frame image is to be warped to the GS image at the middle moment, the actual
optical ﬂow span end time should be 1/2. As shown in the right ﬁgure, the black
line is the time span of undistorted ﬂow. The length of he undistorted ﬂow span
should be (1/2−((s−h/2)/h) = 1−s/h). From this, the transformation formula
between undistorted ﬂow and optical ﬂow can be derived:
U s
r0→0.5 = F s
r · T s,
(6)

LRSIRN-UF
201
T s = 1 −s/h,
(7)
where F s
r is the optical ﬂow corresponding to the sth row, U s
r→0.5 is the undis-
torted ﬂow corresponding to the sth row, T s is the time factor corresponding to
the sth row.
Similarly, the reverse undistorted ﬂow formula is:
U s
r→0.5 = F s
r · (s/h).
(8)
After obtaining the undistorted ﬂow, we can use the undistorted ﬂow to warp
the two RS images respectively to obtain two candidate frames. The warping
method we use is bilinear interpolation.
When synthesizing two candidate GS images into a global image at an inter-
mediate moment, we use the mask and residual combination method commonly
used in video frame interpolation this method can eﬀectively reduce the occur-
rence of synthesizing GS frames pixel overlap, black hole problem. The formula
for this method is as follows:
ˆI
g
0.5 = M 0 · ˆI
g
0→0.5 + M 1 · ˆI
g
1→0.5 + Res.
(9)
3.6
Losses
In terms of loss function, we use two losses: image reconstruction loss and undis-
torted ﬂow smoothing loss.
The image reconstruction loss limits the ﬁnal image generated by the model
to be as close as possible to the GS image at the intermediate moment. The
performance of the traditional L1 loss in this task is relatively poor. Therefore,
we use a new image reconstruction loss. The loss is the sum of the two parts of
the loss, and the calculation formula is:
Lr = σ(ˆI
g
0.5 −Igt
0.5) + Lcen(ˆI
g
0.5, Igt
0.5),
(10)
where σ(x) = (x2 + ϵ2)α, where α = 0.5, ϵ = 10−3, this loss is an alternative
to the traditional L1 loss called Charbonnier loss, Lcen is a statistical loss for
Computes the soft Hamming distance between census-transformed image patches
of size 7 · 7.
In order to add a smoothing term to the estimated undistorted ﬂow to make
the restoration result smoother, a smoothing loss function Ls can be deﬁned
based on the idea of full variational regularization:
Ls = (∥∇ˆU 0→0.5∥2 + ∥∇ˆU 1→0.5∥2),
(11)
where λs is the hyperparameter controlling the weight of the smoothing term,
∇is the gradient operator, and U is the undistorted ﬂow.
The overall losses function are:
L = λ1Lr + λ2Ls,
(12)
where λ1, λ2 are parameters that can be adjusted manually. According to the
experiment, we set these two parameters to 1 and 0.3 respectively.

202
B. Wang et al.
4
Experiment
In this section, we compare with the baseline approaches and provide analysis
and insight into our method.
4.1
Datasets
In the experiment, we use Fastec-RS [1], which is a real-world RS image synthe-
sized by a high frame rate GS camera installed on a ground vehicle dataset to
evaluate all compared methods. The dataset is divided into three parts: train,
val, and test. Each part of data consists of several segments of video. Each video
has 34 continuous time images corresponding to the RS image. The dataset pro-
vides the GS image annotation signals of the ﬁrst row and the central scanning
line of the RS image, so in two consecutive frames of RS images, the GS anno-
tation image at the time t = 0.5 can be obtained, We uses it as labeled data to
train the network.
In this experiment, We use the training set to train all deep learning based
methods. Both the veriﬁcation set and the test set are given, which are used to
evaluate the performance of the model. In order to make evaluation of the model
more convincing, we use the results on both the validation set and the test set
as the evaluation index.
4.2
Evaluation Strategies
We use two traditional evaluation indicators PSNR (Peak Signal-to-Noise Ratio)
and SSIM (Structural Similarity Index Measurement) in the ﬁeld of RS image
restoration. In addition, the parameter amount of the model is also one of our
evaluation indicators.
4.3
Comparison with SOTA Methods
We cmopare our model with CVR [4], SUNet [3], RSSR [2] and DSUN [1] on
the Fastec-RS-test dataset and Fastec-RS-val dataset. SUNet did not provide
the original code, and the experimental data was the original data in the paper
without parameter information. Our model can ﬂexibly tune parameters accord-
ing to performance and memory footprint requirements. LRRN L, LRRN, and
LRRN S in the table correspond to the models with encoder-decoder network
parameters from large to small.
The comparison results are shown in Table 1. In the Fastec-RS-val data, the
PSNR and SSIM of LRRN L reached the highest while LRRN reached the second
highest and LRRN S were the third highest. On the Fastec-RS-test dataset, the
PSNR performance of the model in this paper is second only to CVR, and the
SSIM index is still the top three. In terms of parameter quantity, the parameters
of LRRN S, LRRN, and LRRN L in this paper are 10.1, 19.0 and 79.2. Under
the condition of achieving similar PSNR and SSIM indicators, the parameter

LRSIRN-UF
203
Table 1. Comparison with the state-of-the-art Methods.
model
PSNR (test) SSIM (test) PSNR (val) SSIM (val) param(M)
DSUN [1]
26.52
0.792
30.77
0.851
14.9
SUNet [3]
28.34
0.837
-
-
-
CVR [4]
28.72
0.847
32.85
0.868
166
RSSR [2]
21.23
0.776
26.32
0.789
101.5
LRRN S(ours) 28.03
0.848
33.11
0.881
10.1
LRRN(ours)
28.26
0.851
33.35
0.902
19.0
LRRN L(ours) 28.35
0.852
33.52
0.912
79.2
quantity of our model is only 6% of the CVR model, which means our model
has greater practical application value.
We use our large-parameter model LRRN L and small parameter model
LRRN S to restore the RS image respectively, and visualize the restoration
results, and compare the restoration results with the previous state-of-the-art
method CVR. The visualization results are shown in Fig. 4. In the face of com-
plex situations such as occlusion, the image restored by our method is closer
to the real GS image, and a good restoration result can also be obtained when
using a model with small parameters. This further illustrates the practicability
and superiority of our method.
RS frame 0
RS frame 1
CVR
LRRN_S(ours)
LRRN_L(ours)
GS frame
DSUN
Fig. 4. In this situation that the window is framed in the ﬁrst RS image and blocked
in the second RS image, our large-parameter model LRRN L can still restore images
very similar to the GS image, and the small-parameter model LRRN S restores better
results, while the CVR model for comparison is seriously distorted.
4.4
Ablation Studies
Loss Function. We ﬁrst conducted an ablation experiment on the loss function.
Results are shown in Table 2. LRRN, ΔLr, and ΔLs are the cases where the loss
function parameter λ1, λ2 are set to 1, 0.3; 0, 1; 1, 0 respectively. It can be seen
that the image reconstruction loss Lr has the greatest impact on the model. And
the ﬂow smoothing loss Ls can also improve the performance of the model.

204
B. Wang et al.
Undistorted Flow. We compare undistorted ﬂow to a method that reduces
the steps of multiplying the time factor with optical ﬂow with the original model.
ΔU in Table 2 is the result of using only optical ﬂow. It can be seen that the
PSNR and SSIM indicators of the model using the undistorted ﬂow method
are much higher, which veriﬁes eﬀectiveness of undistorted ﬂow in RS image
restoration.
Mask. We compare our method with the method that directly uses the linear
weighted average of the values of the two candidate frames to obtain the GS
image. ΔM in Table 2 is the result of the linear weighting method. Compared
with the method of linear weighted average, the method of using mask has
achieved a very large improvement in PSNR and SSIM indicators, which proves
that when combining two candidate GS images into one, the mask can eﬀectively
improve the quality of the composite result.
Table 2. Ablation Studies.
model
PSNR (test) SSIM (test) PSNR (val) SSIM (val)
LRRN 28.26
0.851
33.35
0.902
Δlr
24.13
0.688
28.21
0.712
Δls
27.99
0.844
33.14
0.869
ΔU
27.87
0.838
32.14
0.854
ΔM
27.16
0.801
31.45
0.823
5
Conclusion
In this paper, we propose a lightweight rolling shutter image restoration model,
which can eﬀectively extract the undistorted ﬂow from two consecutive frames
of rolling shutter images, and then restore the global image at the intermediate
moment. For the calculation of distortion-free ﬂow, we innovatively proposed
the method of multiplying the optical ﬂow by the time factor, and achieved
eﬀective results. Our method achieves SOTA results on multiple indicators on
the Fastec-RS dataset with parameter greatly reduced. Through further research
on the method in this paper, there will be great hope that it will be extended
to practical applications in the future.
Acknowledgments. This work was supported by the National Natural Science Foun-
dation of China under Grants (62171038, 62171042, and 62088101), and the R&D
Program of Beijing Municipal Education Commission (Grant No. KZ202211417048).

LRSIRN-UF
205
References
1. Peidong, L., Zhaopeng. C., Viktor. L.: Deep shutter unrolling network. In: CVPR,
pp. 10–22 (2020)
2. Bin, F., Yuchao, D.: Deep shutter unrolling network. In: ICCV, pp. 4228–4237
(2021)
3. Bin, F., Yuchao, D.: Sunet: symmetric undistortion network for rolling shutter
correction. In: ICCV, pp. 4541–4550 (2021)
4. Bin, F., Yuchao, D.: Context-aware video reconstruction for rolling shutter cam-
eras. In: CVPR, pp. 17572–17582 (2022)
5. Xinyu, Z., Peiqi, D., Yi, M.: EvUnroll: neuromorphic events based rolling shutter
image correction. In: CVPR, pp. 17751–17784 (2022)
6. Zhixiang, W., Xiang, J., Jia-Bin H.: Neural global shutter: learn to restore video
from a rolling shutter camera with global reset feature. In: CVPR, pp. 17794–17803
(2022)
7. Mingdeng, C., Zhihang, Z., Jiahao, W.: Learning adaptive warping for real-world
rolling shutter correction. In: CVPR, pp. 17785–17793 (2022)
8. Zhihang, Z., Yinqiang, Z., Imari, S.: Towards rolling shutter correction and deblur-
ring in dynamic scenes. In: CVPR, pp. 9219–9228 (2020)
9. Zhihang, Z., Mingdeng, C., Xiao, S.: Bringing rolling shutter images alive with
dual reversed distortion. In: Avidan, S., Brostow, G., Ciss´e, M., Farinella, G.M.,
Hassner, T. (eds.) ECCV 2022. LNCS, vol. 13667, pp. 223–249. Springer, Cham
(2022). https://doi.org/10.1007/978-3-031-20071-7 14
10. Naor, E., Antebi, I., Bagon, S., Irani, M.: Combining internal and external con-
straints for un-rolling shutter in videos. In: Avidan, S., Brostow, G., Ciss´e, M.,
Farinella, G.M., Hassner, T. (eds.) ECCV 2022. LNCS, vol. 13677, pp. 119–134.
Springer, Cham (2022). https://doi.org/10.1007/978-3-031-19790-1 8
11. Alexey, D., Philipp, F., Eddy, I.: FlowNet: learning optical ﬂow with convolutional
networks. In: ICCV, pp. 2758–2766 (2015)
12. Deqing, S., Xiaodong, Y., Ming-Yu, L.: PWC-Net: CNNs for optical ﬂow using
pyramid, warping, and cost volume. In: CVPR, pp. 8934–8943 (2018)
13. Teed, Z., Deng, J.: RAFT: recurrent all-pairs ﬁeld transforms for optical ﬂow. In:
Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol.
12347, pp. 402–419. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-
58536-5 24
14. Junheum, P., Chul, L., Chang-Su, K.: Asymmetric bilateral motion estimation for
video frame interpolation. In: ICCV, pp. 3703–3712 (2021)
15. Shihao, J., Dylan, C., Yao, L.: Learning to estimate hidden motions with global
motion aggregation. In: ICCV, pp. 9772–9781 (2021)
16. Zhewei, H., Tianyuan, Z., Wen, H.: Real-time intermediate ﬂow estimation for
video frame interpolation. In: Avidan, S., Brostow, G., Ciss´e, M., Farinella, G.M.,
Hassner, T. (eds.) ECCV 2022. LNCS, vol. 13674, pp. 624–642. Springer, Cham
(2022). https://doi.org/10.1007/978-3-031-19781-9 36
17. Zou, Y., Zheng, Y., Takatani, T., Fu, Y.: Learning to reconstruct high speed and
high dynamic range videos from events. In: CVPR, pp. 2024–2033 (2021)
18. Zeng, Y., Zou, Y., Fu, Y.: 3D2Unet:3D deformable Unet for low-light video
enhancement. In: PRCV, pp. 66–77 (2021)
19. Zhang, F., Li, Y., You, S., Fu, Y.: Learning temporal consistency for low light
video enhancement from single images. In: CVPR, pp. 4967–4976 (2021)

206
B. Wang et al.
20. Niklaus, S., Liu, F.: Context-aware synthesis for video frame interpolation. In:
CVPR, pp. 1701–1710 (2018)
21. Xu, X., Siyao, L., Sun, W., Yin, Q., Yang, M.H.: Quadratic video interpolation.
In: NeurIPS, pp. 1645–1654 (2019)
22. Jiang, H., et al.: Super slomo: High quality estimation of multiple intermediate
frames for video interpolation. In: CVPR, pp. 9000–9008 (2018)
23. Liu, Z., Yeh, R.A., Tang, X., Liu, Y., Agarwala, A.: Video frame synthesis using
deep voxel ﬂow. In: ICCV, pp. 4473–4481 (2017)

An Efﬁcient Graph Transformer Network
for Video-Based Human Mesh Reconstruction
Tao Tang1,2, Yingxuan You1(B), Ti Wang1, and Hong Liu1
1 Key Laboratory of Machine Perception, Shenzhen Graduate School, Peking University,
Shenzhen, China
{taotang,youyx,tiwang}@stu.pku.edu.cn, hongliu@pku.edu.cn
2 School of Computer Science and Engineering, Central South University, Changsha, China
Abstract. Although existing image-based methods for 3D human mesh recon-
struction have achieved remarkable accuracy, effectively capturing smooth
human motion from monocular video remains a signiﬁcant challenge. Recently,
video-based methods for human mesh reconstruction tend to build more com-
plex networks to capture temporal information of human motion, resulting in a
large number of parameters and limiting their practical applications. To address
this issue, we propose an Efﬁcient Graph Transformer network to Reconstruct
3D human mesh from monocular video, named EGTR. Speciﬁcally, we present
a temporal redundancy removal module that uses 1D convolution to eliminate
redundant information among video frames and a spatial-temporal fusion mod-
ule that combines Modulated GCN with transformer framework to capture human
motion. Our method achieves better accuracy than the state-of-the-art video-
based method TCMR on 3DPW, Human3.6M and MPI-INF-3DHP datasets while
only using 8.7% of the parameters, indicating the effectiveness of our method for
practical applications.
Keywords: Transformer · Graph Convolutional Network · Temporal
Redundancy Removal · Video-based 3D Human Mesh Reconstruction
1
Introduction
Estimating 3D human pose and shape from a single image or video has signiﬁcant prac-
tical applications in computer graphics, virtual reality, physical therapy and so on [1].
This task typically involves taking an image or a sequence of video frames as input and
generating pose and shape parameters of human model [2] as output.
While existing image-based methods [3–7] can generate reasonably accurate 3D
human mesh from individual images, they often struggle to estimate smooth 3D human
motion from videos due to the absence of temporal information. To tackle this chal-
lenge, some methods [8–11] have extended image-based methods to video scenarios.
These approaches primarily leverage Recurrent Neural Networks (RNNs) [12] to cap-
ture temporal information (i.e., the continuity of human motion) and achieve coherent
3D human motion estimation. However, these methods have several limitations. For
instance, VIBE [9] uses a bidirectional Gated Recurrent Unit (GRU) that lacks sufﬁ-
cient temporal information because GRU only transmits temporal information between
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 207–219, 2024.
https://doi.org/10.1007/978-981-99-8850-1_17

208
T. Tang et al.
adjacent frames, resulting in excessive motion jitter in the reconstructed human mesh
sequence. TCMR [11] employs three GRUs to capture past, future, and global temporal
Fig. 1. Comparision between accuracy (MPJPE) and parameters (left), FLOPs (right) of video-
based methods. All methods are evaluated on the 3DPW dataset.
information. It constrains neighboring three frames to the intermediate frame, causing
the network to reconstruct the average human mesh within adjacent frames and resulting
in over-smoothed 3D human mesh reconstruction with insufﬁcient accuracy. Neverthe-
less, methods based on RNNs suffer from difﬁculties in parallelization and have a large
number of model parameters, signiﬁcantly limiting their practical applications.
To address the above issues, we propose an Efﬁcient Graph Transformer network
for video-based human mesh Reconstruction (EGTR). Our method mainly consists of
three modules: Temporal Redundancy Removal (TRR), Spatial-Temporal Fusion (STF),
and Multi-Branch Integration (MBI). As shown in Fig. 1, EGTR has better 3D human
body reconstruction accuracy than other video-based methods while signiﬁcantly reduc-
ing the parameters and FLOPs. Our main contributions are as follows:
– We propose an efﬁcient network EGTR for 3D human mesh reconstruction from
a monocular video that combines Modulated GCN with transformer framework.
EGTR not only improves reconstruction accuracy, but also signiﬁcantly reduces the
parameters and FLOPs.
– We introduce novel modules namely Temporal Redundancy Removal (TRR) which
decreases the local redundancies present in video frames and Spatial-Temporal
Fusion (STF) which uses GCN to replace multi-head attention in transformer frame-
work, signiﬁcantly reducing the model parameters.
– Experiments show that our EGTR achieves better accuracy with fewer parame-
ters. For instance, despite having only 8.7% of the parameters compared to TCMR,
EGTR achieves a 2.2% improvement in MPJPE on the 3DPW dataset.
2
Related Work
2.1
Human Mesh Reconstruction
Reconstructing 3D human mesh from an image or video is a challenging task, which has
gained signiﬁcant attention in recent years due to its efﬁciency and convenience. Many

EGTR
209
previous methods utilize parametric human models, such as SCAPE [13], SMPL [2],
SMPL-X [14], and STAR [15], to reconstruct human mesh by regressing human pose
and shape parameters.
Image-Based Human Mesh Reconstruction. For methods that use a single image as
input, HMR [3] proposed an end-to-end trainable 3D human reconstruction network
that introduces an adversarial training approach and minimizes the reprojection loss of
2D keypoints, allowing the network can be trained using in-the-wild images with only
2D annotations. SPIN [4] introduced a self-improving network that consists of an SMPL
parameter regressor and an iterative ﬁtting framework, then trained a ResNet [16] to
extract static features of the human body from images. Some methods [5–7] used prior
knowledge such as joint interdependencies, body parts segmentation, and mesh align-
ment to assist the regression of SMPL parameters. However, when applying image-
based methods to videos, reconstruction errors between different frames lead to incon-
sistent human motion (i.e., motion jitter), due to the absence of temporal information.
Video-Based Human Mesh Reconstruction. For methods that use a sequence of
frames as input, HMMR [8] introduced using acceleration error to evaluate the smooth-
ness of the reconstructed human motion. VIBE [9] employed a bidirectional GRU-based
motion generator and an adversarial motion discriminator. It trained the discriminator
with an actual human motion dataset AMASS [17], assisting the generator in gener-
ating more realistic human motion. MEVA [10] also used a bidirectional GRU-based
generator, and it added a residual estimation to estimate the details of person-speciﬁc
motion. However, VIBE and MEVA still suffer from inadequate temporal information
extracted by bidirectional GRU, resulting in the persistence of motion jitter. Therefore,
TCMR [11] proposed a three-branch network that utilizes a bidirectional GRU and two
unidirectional GRUs to extract temporal information from past and future frames to
constrain the target frame. However, the large number of parameters in the network
composed of three GRUs makes it hard to deploy on practical applications.
2.2
Graph Convolutional Networks
Graph Convolution Networks (GCNs) generalize the capabilities of CNNs by perform-
ing convolution operations on graph-structured data. Recently, GCNs have been widely
applied to 3D human pose and mesh reconstruction [18–23], primarily for extracting
and integrating information from various body joints or meshes. Thanks to the natural
graph structures of the human skeleton and body mesh, GCNs are appropriate to model
human topology. GraphCMR [18] utilized GCNs to regress the position of mesh ver-
tices without using the parametric model (SMPL [2]). Pose2Mesh [19] employed GCNs
to estimate human mesh from 2D and 3D poses. GATOR [21] utilized GCN to assist
in capturing the multiple relations between the skeleton and human mesh. Modulated
GCN [22] addressed the issue of sharing feature transformations among different nodes
while keeping relatively small parameters. Modulated GCN can be expressed as:
Y = sigmoid(D−1
2 ˜AD−1
2 X(W ⊙M)),
(1)

210
T. Tang et al.
where X represents the input features of all nodes in the graph, ˜A donates the learn-
able adjacency matrix, D is the degree matrix, W is the fusion matrix that needs to be
learned, M represents modulation vectors, and Y is the output of Moudulated GCN.
3
Method
Fig. 2. The overview of efﬁcient graph transformer network for video-based human mesh recon-
struction (EGTR). Given a video sequence, ResNet [16] is utilized to extract the static features
and TRR reduces the redundancy of neighboring frames. Then, the static features are separated
into three sequences and respectively fused by STF into temporal features of the past, future and
all frames. After that, MBI integrates the temporal features into a feature of the target frame.
Finally, the SMPL parameter regressor outputs the human mesh.
3.1
Overview of EGTR
The overview of EGTR is shown in Fig. 2. Given a video sequence V = {It}T
t=1
with T frames, we ﬁrstly use ResNet [16] pretrained by SPIN [4] to extract static
features sequence F
= {ft}T
t=1 of all frames, where ft ∈R2048. We consider
(⌊T/2⌋+ 1)th frame as the target frame, 1, ..., (⌊T/2⌋)th frames as past frames
and (⌊T/2⌋+ 2)th, ..., Tth frames as future frames. Then, the Temporal Redundancy
Removal (TRR) module and three branches of Spatial-Temporal Fusion (STF) mod-
ules are sequentially applied to obtain the temporal feature of past frames gpast, future
frames gfuture, and global frames gall, where g∗∈R2048. Multi-Branch Integration
(MBI) module integrates the extracted temporal features and gets gint ∈R2048. Finally,
gint is fed to the pretrained SMPL regressor to obtain the human mesh vertices. We
introduce each module in EGTR as follows.
3.2
Temporal Redundancy Removal
Due to the small temporal interval between consecutive video frames, the variations in
human body actions are subtle. Consequently, the static features extracted by ResNet

EGTR
211
exhibit similarities, impeding the subsequent network focus on crucial human motion
information. This draws a challenge in acquiring comprehensive temporal features from
videos. The Temporal Redundancy Reduction (TRR) module employs 1D convolu-
tion to capture the differential features among neighboring frames in the static features
sequence. This effectively decreases the local redundancies in video frames and enables
the following network to capture global dependencies. Speciﬁcally, a 1D convolution
layer with a kernel size of 3, a stride of 1, and padding of 1 is utilized within this module,
which considers the static features of neighboring three frames, effectively reducing the
redundancy in the static features of neighboring frames.
3.3
Spatial-Temporal Fusion
Fig. 3. Pipeline of the Spatial-Temporal Fusion module.
Given a static features sequence F = {ft}T
t=1 after TRR, we extract temporal fea-
tures for the target frame from the past and future frames by employing three branches
of STF with the same architecture. For the reconstruction of the target frame, the static
features of T = 16 frames consisting of 8 frames before and 7 frames after the target
frame are input to the three branches of STF module. Previous methods for video-based
human mesh reconstruction [9–11] use GRU to capture temporal features by sequen-
tially inputting video frames into the network, preserving the temporal order of the
frames. As we simultaneously take the spatial features of all frames as input, result-
ing in the loss of the inherent temporal order of the frames in the original sequence.
Consistent with the transformer [24], we employ sine and cosine functions for position
encoding of the input sequence. A 2048-dimensional position encoding (PE) is gener-
ated for each frame.
For each branch, as shown in Fig. 3, We employ Modulated GCN [22] within the
transformer framework to fuse the static features for the temporal feature of the target
frame. Instead of the multi-head attention in the transformer encoder, Modulated GCN
is used in EGTR to add temporal constraints, which enhances the smoothness of recon-
structed 3D human motion. Speciﬁcally, we consider each video frame as a graph node
and the adjacency matrix of the graph is a learnable matrix obtained by training on large

212
T. Tang et al.
mixed 3D and 2D human motion datasets. This signiﬁcantly reduces the parameters and
FLOPs of the network. The STF can be expressed as the following equations:
f
′ = f + PE,
(2)
m = Norm(Modulated GCN(f
′)),
(3)
g = Norm(FFN(m) + m),
(4)
where Norm demotes the layer normalization [25], FFN denotes feedforward net-
works.
3.4
Multi-branch Integration
Fig. 4. Pipeline of the Multi-Branch Integration module.
After separating the original 16-frame sequence into three subsequences: past 8
frames, future 7 frames, and the global 16 frames, we input each subsequence to the
STF to generate gpast, gfuture and gall. As shown in Fig. 4, the multi-branch integration
module passes the temporal information through four fully connected layers with the
Tanh activation function, and utilizes a softmax to output weights that represent the
inﬂuence of past, future, and global temporal information on the target frame: wpast,
wfuture and wall. Finally, the temporal features of three branches are fused by weighted
summation to obtain the temporal feature of the target frame, which can be calculated
as the following equation:
gint = wpastgpast + wallgall + wfuturegfuture.
(5)
By separating into three sub-sequences, this module signiﬁcantly reduces the train-
ing difﬁculty of the highly non-linear reconstruction task. It assists the STF in focusing
on the accuracy and smoothness of the reconstructed human motion.

EGTR
213
3.5
Loss Function
For training, we utilize the following loss functions: 3D joint position loss L3D, 2D
joint reprojection loss L2D, SMPL body shape parameter loss Lshape and SMPL pose
parameter loss Lpose. L2 losses between predicted and ground-truth SMPL parameters
and 2D/3D joint coordinates are used. Due to each loss function having a different
scale, the total loss is calculated as the weighted sum of the four losses above. This can
be formulated as:
L = λ1L3D + λ2L2D + λ3Lshape + λ4Lpose.
(6)
The experimental settings for the hyperparameters are λ1 = 300, λ2 = 300, λ3 =
0.06 and λ4 = 60.
4
Experiments
4.1
Experimental Settings
Implementation Details. Following previous methods [9,11] we conﬁgure the input
sequence length (T) to be 16. The ResNet and SMPL parameter regressor are initialized
by the pretrained SPIN [4]. We employ the Adam optimizer [26] for weight updates.
Static features are precomputed from the images using the ResNet [16] to optimize
training time and memory consumption. We set the initial learning rate to 5 × 10−5,
with a reduction factor of 10 applied when accuracy has no improvement after 5 epochs.
The network is trained for 50 epochs using a single NVIDIA RTX 2080Ti GPU. The
PyTorch [27] framework is utilized for code implementation.
Evaluation Metrics and Datasets. To evaluate reconstruction accuracy, we consider
the Mean Per Joint Position Error (MPJPE), Procrustes-Aligned MPJPE (PA-MPJPE),
and Mean Per Vertex Position Error (MPVPE). These metrics measure the positional
differences between predicted and ground truth in millimeters (mm). To evaluate recon-
struction smoothness, we utilize the acceleration error proposed in HMMR [9], which
is measured in (mm/s2). Our training dataset includes 3DPW [28], Human3.6M [29],
MPI-INF-3DHP [30], PoseTrack [31], InstaVariety [9], and Penn Action [32]. 3DPW is
the only dataset that provides accurate ground truth SMPL parameters. Following previ-
ous methods [9,11], 3DPW, Human3.6M, and MPI-INF-3DHP are used for evaluation,
which have annotation for 3D keypoints.
4.2
Comparison with State-of-the-Art Methods
To compare our approach with state-of-the-art video-based 3D human mesh recon-
struction methods that report the acceleration error in Table 1. The result shows that
EGTR outperforms the current state-of-the-art video-based 3D human reconstruction
methods in most metrics and datasets.

214
T. Tang et al.
Table 1. Evaluation of state-of-the-art methods on 3DPW, Human3.6M, and MPI-INF-3DHP
Datasets. All methods use 3DPW training set while training, but do not use the SMPL parameters
of Human3.6M from Mosh [33]. The top two best results are highlighted in bold and underlined,
respectively.
Method
3DPW
Human3.6M
MPI-INF-3DHP
input frames
PA-MPJPE↓MPJPE↓MPVPE↓ACC-ERR↓PA-MPJPE↓MPJPE↓ACC-ERR↓PA-MPJPE↓MPJPE↓ACC-ERR↓
HMMR [8]
72.6
116.5
139.3
15.2
56.9
–
–
–
–
–
16
VIBE [9]
57.6
91.9
–
25.4
53.3
78.0
27.3
68.9
103.9
27.3
16
MEVA [10]
54.7
86.9
–
11.6
53.2
76.0
15.3
65.4
96.4
11.1
90
TCMR [11]
52.7
86.5
102.9
6.8
52.0
73.6
3.9
63.5
97.3
8.5
16
EGTR (Ours) 52.3
84.6
100.2
9.1
47.5
68.9
5.1
62.2
97.5
10.6
16
Comparison of Accuracy and Smoothness. In terms of reconstruction accuracy,
EGTR achieves results closest to the ground truth compared to other methods. The
Spatial-Temporal Fusion (STF) module extract information from adjacent frames for
human body reconstruction, signiﬁcantly improving the robustness and stability of
human mesh reconstruction from images. While MEVA outperforms all other methods
in MPJPE on MPI-INF-3DHP dataset, it requires at least 90 frames for reconstruction,
limiting its applicability for short videos.
Regarding the smoothness of reconstructed human motion, although our approach is
not as smooth as the TCMR, it shows substantial improvement compared to the previous
methods such as HMMR, VIBE and MEVA. Moreover, qualitative evaluations indicate
that TCMR seems to be too focused on generating over-smoothed 3D human motion,
which limits its reconstruction accuracy.
Table 2. Comparison of the network parameters and FLOPs.
Method
Parameters (M) FLOPs (M)
VIBE [9]
15.01
351.19
MEVA [10]
39.70
415.43
TCMR [11]
50.43
464.80
EGTR (Ours)
4.39
274.43
Comparison of Parameters and FLOPs. As shown in Table 2, we compare the model
sizes of the previous methods. “Parameters” represents the number of model parame-
ters, which indicates the model size. “FLOPs” represents the ﬂoating-point operations,
which indicates the computational complexity of method. Compared to TCMR, EGTR
achieves a reduction of 91.3% (from 50.43 M to 4.39 M) in parameters and 41.0% (from
464.80 M to 274.43 M) in FLOPs. Furthermore, on the 3DPW dataset, EGTR shows
a further decrease of 2.2% (from 86.5 mm to 84.6 mm) in MPJPE and 2.6% (from
102.9 mm to 100.2 mm) in MPVPE. These results indicate that EGTR has better real-
time performance and broader applications.
4.3
Qualitative Evaluation
In addition to quantitative evaluation, the qualitative evaluation of human mesh recon-
struction methods is also an important consideration in comparison.

EGTR
215
Fig. 5. Qualitative comparison of TCMR and EGTR on 3DPW dataset.
Comparison on the Continuous Video. As shown in Fig. 5, we utilize the EGTR and
TCMR to reconstruct the video from the 3DPW dataset. The ﬁrst column represents
some input frames, and the second and third columns show the reconstructed human
meshes of TCMR and EGTR, respectively. We can observe that compared to TCMR,
our EGTR achieves signiﬁcantly higher reconstruction accuracy in terms of limb posi-
tions, body shape, and back torso. Moreover, we ﬁnd that TCMR tends to produce
over-smoothed human motion, resulting in smaller body proportions and insufﬁcient
accuracy.
Fig. 6. Qualitative results of EGTR on a stitched video.

216
T. Tang et al.
Analysis on the Stitched Video. To provide a more intuitive reﬂection of the strong
temporal feature extraction capability of our EGTR, we conduct an experiment using a
stitched video. We repeat two different frames 20 times respectively to create a stitched
video. As shown in Fig. 6, frames 18th to 23nd are displayed. The 20th frame introduces
a sudden change in body shape and pose. We can note that the motion of the hands and
legs changes gradually over time in the reconstructed human mesh of EGTR rather
than abruptly changing between the 20th and 21st frames. This demonstrates that our
EGTR effectively alleviates the problem of motion jitter in the video-based human mesh
reconstruction.
4.4
Ablation Analysis
Table 3. Ablation results for different modules of EGTR on 3DPW dataset.
TRR PE MBI STF 3DPW
PA-MPJPE ↓MPJPE ↓MPVPE ↓ACC-ERR ↓
✗
✓
✓
✓
54.2
87.9
103.8
10.1
✓
✗
✓
✓
53.5
86.9
102.9
9.5
✓
✓
✗
✓
55.7
90.2
105.9
7.4
✓
✓
✓
✗
54.1
91.1
106.7
27.5
✓
✓
✓
✓
52.3
84.6
100.2
9.1
As shown in Table 3, the results demonstrate the effects of modules in Sect. 3 on
estimating accurate and smooth human meshes. Without TRR module, The redundant
information from neighboring frames prevents the subsequent network from focusing
on crucial human motion information, leading to decreased reconstruction accuracy
and smoothness. Moreover, position encoding also plays a crucial role in achieving
better reconstruction results because it aids the network in learning the positional infor-
mation of each frame within the video sequence. The MBI module can signiﬁcantly
reduce reconstruction accuracy errors, albeit at the cost of a slight decrease in ACC-
ERR. Additionally, paying attention to 4th row, directly using static features extracted
by ResNet treats the reconstruction of each video frame as independent tasks. Similar
to image-based methods, without using STF module not only signiﬁcantly decreases
the accuracy but also results in a large decrease in smoothness.

EGTR
217
Fig. 7. Ablation results for different number of input frames of EGTR on 3DPW dataset.
We also conduct ablation experiments to investigate the impact of different number
of input frames on the performance of EGTR. As shown in Fig. 7, we conduct experi-
ments by setting the input length to 4, 8, 16, 32, and 64. The results indicate that EGTR
achieves the best performance when the input length is 16 frames. When the input is
fewer than 16 frames, it fails to capture sufﬁcient temporal information. In addition,
when the input exceeds 16 frames, the lightweight Graph Transformer network strug-
gles to model excessively long temporal dependencies.
5
Conclusions
In this paper, we propose an efﬁcient graph transformer network for video-based human
mesh reconstruction (EGTR). We introduce the temporal redundancy removal module
to remove redundant spatial features from adjacent video frames. We also propose the
spatial-temporal fusion module, which utilizes Modulated GCN combined with trans-
former framework to propagate and fuse temporal features. Compared to TCMR, we
achieve a reduction of 91.3% in model parameters and 41.0% in FLOPs, while further
improving the MPJPE and MPVPE by 2.2% and 2.6% on the 3DPW dataset. Exper-
iments evaluate the efﬁciency of our EGTR for human mesh reconstruction, which
shows its potential for practical applications.
Acknowledgement. This paper is funded by National Natural Science Foundation of China
(No.62073004), National Key R&D Program of China (No.2020AAA0108904), and Shenzhen
Fundamental Research Program (No. GXWD20201231165807007-20200807164903001).
References
1. Tian, Y., Zhang, H., Liu, Y., Wang, L.: Recovering 3D human mesh from monocular images:
a survey. arXiv preprint arXiv:2203.01923 (2022)

218
T. Tang et al.
2. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned multi-
person linear model. ACM Trans. Graphics (TOG) 34(6), 1–16 (2015)
3. Kanazawa, A., Black, M.J., Jacobs, D.W., Malik, J.: End-to-end recovery of human shape and
pose. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 7122–7131 (2018)
4. Kolotouros, N., Pavlakos, G., Black, M.J., Daniilidis, K.: Learning to reconstruct 3D human
pose and shape via model-ﬁtting in the loop. In: Proceedings of the IEEE International Con-
ference on Computer Vision (ICCV), pp. 2252–2261 (2019)
5. Georgakis, G., Li, R., Karanam, S., Chen, T., Koˇseck´a, J., Wu, Z.: Hierarchical kinematic
human mesh recovery. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV
2020. LNCS, vol. 12362, pp. 768–784. Springer, Cham (2020). https://doi.org/10.1007/978-
3-030-58520-4 45
6. Kocabas, M., Huang, C. H. P., Hilliges, O., Black, M. J.: PARE: part attention regressor
for 3D human body estimation. In: Proceedings of the IEEE International Conference on
Computer Vision (ICCV), pp. 11127–11137 (2021)
7. Zhang, H., Tian, Y., Zhou, X., Ouyang, W., Liu, Y., Wang, L., Sun, Z.: Pymaf: 3d human
pose and shape regression with pyramidal mesh alignment feedback loop. In: Proceedings of
the IEEE International Conference on Computer Vision (ICCV), pp. 11446–11456 (2021)
8. Kanazawa, A., Zhang, J.Y., Felsen, P., Malik, J.: Learning 3D human dynamics from video.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 5614–5623 (2019)
9. Kocabas, M., Athanasiou, N., Black, M.J.: VIBE: video inference for human body pose and
shape estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 5253–5263 (2020)
10. Luo, Z., Golestaneh, S.A., Kitani, K.M.: 3D human motion estimation via motion compres-
sion and reﬁnement. In: Ishikawa, H., Liu, C.-L., Pajdla, T., Shi, J. (eds.) ACCV 2020. LNCS,
vol. 12626, pp. 324–340. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-69541-
5 20
11. Choi, H., Moon, G., Chang, J.Y., Lee, K.M.: Beyond static features for temporally consis-
tent 3D human pose and shape from a video. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 1964–1973 (2021)
12. Cho, K., Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H.: Learning
phrase representations using RNN encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078 (2014)
13. Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: Scape: shape com-
pletion and animation of people. In: ACM SIGGRAPH, pp. 408–416 (2005)
14. Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A.A., Tzionas, D.: Expressive
body capture: 3d hands, face, and body from a single image. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10975–10985 (2019)
15. Osman, A.A.A., Bolkart, T., Black, M.J.: STAR: sparse trained articulated human body
regressor. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS,
vol. 12351, pp. 598–613. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-58539-
6 36
16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770–778 (2016)
17. Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: AMASS: archive of
motion capture as surface shapes. In: Proceedings of the IEEE International Conference on
Computer Vision (ICCV), pp. 5442–5451 (2019)

EGTR
219
18. Kolotouros, N., Pavlakos, G., Daniilidis, K.: Convolutional mesh regression for single-image
human shape reconstruction. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 4501–4510 (2019)
19. Choi, H., Moon, G., Lee, K.M.: Pose2Mesh: graph convolutional network for 3D human pose
and mesh recovery from a 2D human pose. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-
M. (eds.) ECCV 2020. LNCS, vol. 12352, pp. 769–787. Springer, Cham (2020). https://doi.
org/10.1007/978-3-030-58571-6 45
20. Lin, K., Wang, L., Liu, Z.: Mesh graphormer. In: Proceedings of the IEEE International
Conference on Computer Vision (ICCV), pp. 12939–12948 (2021)
21. You, Y., Liu, H., Li, X., Li, W., Wang, T., Ding, R.: Gator: graph-aware transformer with
motion-disentangled regression for human mesh recovery from a 2D Pose. In: IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1–5 (2023)
22. Zou, Z., Tang, W.: Modulated graph convolutional network for 3D human pose estima-
tion. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV),
pp. 11477–11487 (2021)
23. Wang, T., Liu, H., Ding, R., Li, W., You, Y., Li, X.: Interweaved graph and attention network
for 3D human pose estimation. In: IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pp. 1–5 (2023)
24. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.: Attention is all
you need. In: Conference on Neural Information Processing Systems (NIPS) (2017)
25. Ba, J.L., Kiros, J.R., Hinton G.E.: Layer normalization. arXiv preprint arXiv:1607.06450
(2016)
26. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
27. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z.: Automatic differentia-
tion in pytorch (2017)
28. Marcard, T., Henschel, R., Black, M.J., Rosenhahn, B., Pons-Moll, G.: Recovering accurate
3d human pose in the wild using imus and a moving camera. In: Proceedings of the European
Conference on Computer Vision (ECCV), pp. 601–617 (2018)
29. Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6m: large scale datasets and
predictive methods for 3D human sensing in natural environments. IEEE Trans. Pattern Anal.
Mach. Intell. (TPAMI) 36(7), 1325–1339 (2013)
30. Mehta, D., Rhodin, H., Casas, D., Fua, P., Sotnychenko, O., Xu, W.: Monocular 3D human
pose estimation in the wild using improved CNN supervision. In: International Conference
on 3D Vision (3DV), pp. 506–516 (2017)
31. Andriluka, M., Iqbal, U., Insafutdinov, E., Pishchulin, L., Milan, A., Gall, J.: Posetrack: a
benchmark for human pose estimation and tracking. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 5167–5176 (2018)
32. Zhang, W., Zhu, M., Derpanis, K.G.: From actemes to action: A strongly-supervised repre-
sentation for detailed action understanding. In: Proceedings of the IEEE International Con-
ference on Computer Vision (ICCV), pp. 2248–2255 (2013)
33. Loper, M., Mahmood, N., Black, M.J.: MoSh: motion and shape capture from sparse mark-
ers. ACM Trans. Graphics (TOG) 33(6), 220:1-220:13 (2014)

Multi-scale Transformer with Decoder
for Image Quality Assessment
Shuai Zhang and Yutao Liu(B)
School of Computer Science and Technology, Ocean University of China,
Qingdao 266100, China
zhangshuai8775@stu.ouc.edu.cn, liuyutao@ouc.edu.cn
Abstract. Blind image quality assessment (BIQA) is of great signiﬁ-
cance in image processing ﬁeld. However, due to diverse image content
and complex types of distortions, the issue of BIQA has not been fully
resolved. To address this issue more eﬀectively, in this paper, we pro-
pose a framework based on Vision Transformer for BIQA called MSIQT.
This model aims to extract image features more eﬀectively and achieve
a more accurate representation of quality. Speciﬁcally, at the input end,
we adopt a multi-scale input approach to enrich the image features and
utilize ResNet-50 for feature extraction. At the output end, a decoder is
introduced to interpret quality-aware vectors obtained from image fea-
tures. Experiments on four image quality assessment datasets prove that
the proposed method outperforms or is comparable to state-of-the-art
approaches.
Keywords: blind image quality assessment · vision transformer ·
multi-scale
1
Introduction
Image quality assessment is critical in both production and living. Its purpose
is to allow machines to automatically assess image quality from a human per-
ception perspective. Accurate and eﬃcient image quality assessment can drive
other tasks in image processing towards more advanced directions, such as image
enhancement and image dehaze. Image quality evaluation can be roughly classi-
ﬁed according to whether there are reference images auxiliary evaluation, includ-
ing FR-IQA [18], RR-IQA [19] and NR-IQA [20]. Both FR-IQA and RR-IQA
require reference images which are generally high-quality images corresponding
to the distorted images. However, it can be luxury to obtain reference images
on many occasions, and FR-IQA and RR-IQA methods are subject to various
limitations. On the contrary, NR-IQA methods, which do not need reference
images, have a wider range of applications.
Traditional blind image quality assessment methods evaluate image quality
by detecting speciﬁc types of distortions, including blur, block artifacts, vari-
ous forms of noise, etc. NIQE [1], extracts quality features from the test images
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 220–231, 2024.
https://doi.org/10.1007/978-981-99-8850-1_18

MSIQT
221
and ﬁts them to a multivariate Gaussian model to evaluate quality of the test
images. The advent of deep learning has provided many diﬀerent solutions to
BIQA. In recent years, CNN and ViT have shined brilliantly, and most advanced
BIQA algorithms are based on these two networks. CNN-based methods primar-
ily extract features from images through operations like convolution and pooling.
These features are mapped to ﬁnal quality scores using several fully connected
layers. Le et al. [2] designed a ﬁve-layer CNN, including one convolution layer,
one pooling layer, and three fully connected layers, to predict image scores. Zhang
et al. [3] proposed two CNN structures speciﬁcally designed for synthetic and
real distortion images, respectively. CNN-based blind image quality assessment
methods performed well, however, the local perception characteristic of CNNs
may make it challenging to extract global features from images, which can poten-
tially impact the ﬁnal evaluation results. Methods based on Vision Transformer
divide images into several ﬁxed-size patches and encode them. These encoded
patches are then fed into a Transformer for attention computation, and the ﬁnal
evaluation result is obtained using the CLS token. BIQA methods based on ViT
can be generally categorized into two types in terms of implementation. One is
to combine with CNN, ﬁrst use CNN to get feature map, and then input the
feature map into ViT. Golestaneh et al. [21] used CNN to extract local features
of images, and input the features into ViT to obtain global representation of
images, and used the ranking between images to improve the monotonic correla-
tion between diﬀerent evaluation indicators. Another ViT-based implementation
method is to directly divide the image into ﬁxed-size patches, input them into
ViT, and extract the global features of the image for evaluation [10].
Since there are no reference images to compare in BIQA, it is necessary to
extract as many features in images as possible. Existing methods generally use
CNN to extract features or directly input the original image, but these features
may not be rich, or the model may ignore some features during processing,
making the ﬁnal evaluation biased. We use a multi-scale method when inputting
images, zoom the original image while maintaining the aspect ratio, and input
images of multiple scales, so that the features that the model can analyze are
more abundant. Recently, BIQA methods based on ViT have achieved good
results. However, most of the methods only use the encoder. We introduce a
decoder to further analyze and process the output of encoder, so that the quality
evaluation results more precise.
This paper’s key contributions can be summed into three points:
1. We designed a multi-scale image quality assessment transformer (MSIQT)
that enables multi-scale feature extraction.
2. We introduced a decoder and attention panels into the model to help the
Transformer better map features to the score domain.
3. We tested MSIQT on four large-scale quality evaluation datasets, including
TID2013 [14], LIVE [12], LIVEC [15], and CSIQ [13]. Our suggested model’s
results are comparable to advanced quality evaluation approaches.

222
S. Zhang and Y. Liu
The subsequent chapters of this paper are organized as follows. The second
section covers relevant work on BIQA, the third section explains our suggested
method framework, the fourth section gives experimental data and analysis, and
the ﬁfth section is a conclusion.
2
Related Work
Traditional BIQA approaches and deep learning BIQA methods are the two
types of BIQA approaches. Deep learning methods are further classiﬁed into
CNN-based methods and ViT-based methods. Several types of blind image qual-
ity evaluation methods are introduced in detail below.
2.1
Traditional Blind IQA
Traditional BIQA methods mainly extract color, contrast and other features
in images through mathematical or physical modeling, and performs the ﬁnal
quality evaluation [27]. Mathematical methods and machine learning algorithms
have played a big role in traditional ﬁelds. Anish et al. [22] proposed a BIQA
model based on natural scene statistics operating in spatial domain. They cal-
culated the naturalness by counting the local brightness diﬀerences of pictures
in a large number of scenes. Zhang et al. [25] built a Gaussian model for picture
patches using a set of raw natural images, then utilized the acquired model to
assess the quality of each image patch and average pooling to produce a global
score. Xu et al. [26] proposed a BIQA algorithm based on high-order statistical
aggregation. Local image patches are extracted as local features and codebooks
are constructed, which are subsequently used to construct global quality-aware
image representations. Finally, a regression algorithm in machine learning is used
to calculate the correlation between human subjective perception and objective
indicators.
2.2
CNN-Based Blind IQA
Beneﬁting from the feature extraction and representation capabilities of
CNN [17], CNN-based methods have achieved considerable success. Some CNN-
based image quality assessment methods leverage the remarkable feature expres-
sion abilities of CNN. Le et al. [2] proposed a ﬁve-layer CNN, employs simple
convolution and pooling operations to extract image features. Bosses et al. [4]
designed an end-to-end CNN-based BIQA method that includes convolution lay-
ers, ﬁve pooling layers, and two fully connected layers. This architecture makes
it deeper compared to other coherent IQA models. Su et al. [9] proposed an
adaptive hypernetwork architecture for evaluating the quality of real-world out-
door photographs. The architecture ﬁrst utilizes ResNet-50 to extract features
from images. Subsequently, a content-aware hypernetwork is employed to train
diﬀerent weights and biases for each image, and ﬁnally, a simple network is used
to obtain the ﬁnal score.

MSIQT
223
BIQA is often limited by the size of datasets. Some CNN methods address this
issue by tackling the problem of small datasets to improve model performance.
Liu et al. [5] suggested a ranking-based NR-IQA approach. They used a Siamese
network to rank photos according to their quality by using synthetic distortions
generated with known relative image quality. The feature representation from the
Siamese network was then sent to a CNN for individual image quality evaluation.
CNN models often require pretraining on large datasets and then further ﬁne-
tuning on downstream tasks. However, these pretrained networks are widely used
in other vision tasks, which can lead to generalization issues when evaluating
diﬀerent types of distortions. Zhu et al. [6] developed a NR-IQA metric that
learns shared meta-knowledge from people while evaluating the quality of photos
with varied distortions. This enables the model to adapt easily to unknown
distortions.
2.3
ViT-Based Blind IQA
Transformer [7] was initially introduced and widely applied in the ﬁeld of nat-
ural language processing (NLP). Its outstanding performance in various NLP
tasks, such as machine translation and language modeling, drew attention from
the computer vision community. Vision Transformer (ViT) [8] is one of the most
representative examples of applying the Transformer model in the ﬁeld of com-
puter vision. It has performed admirably in image classiﬁcation tasks. ViT sep-
arates the input picture into numerous patches, each of which is projected onto
a ﬁxed-length vector. The Transformer model is then given these vectors. To
allow image classiﬁcation, a speciﬁc token indicating the image classiﬁcation job
is introduced to the input sequence. The ﬁnal projected class corresponds to the
output corresponding to this token.
The global attention of Vision Transformer on images helps capture global
features of images, which may be advantageous for evaluating image quality.
Manri et al. [24] proposed a framework with ViT as backbone for FR-IQA. The
method ﬁrst uses CNN to extract perceptual features of the image, which are
then input to ViT to compare distorted image with reference image. Currently,
BIQA algorithms based on ViT can be mainly categorized into two types: hybrid
methods and pure ViT-based methods. Hybrid methods ﬁrst extract image fea-
tures using pre-trained CNN models and then process the feature maps using
ViT. You et al. [10] ﬁrst utilize ResNet-50 to extract image features. The feature
maps are then projected and pooled before being inputted into the ViT struc-
ture. Finally, a MLP head is employed to obtain the quality distribution. Pure
ViT-based methods directly divide the input image into patches and feed them
into the ViT for processing. Ke et al. [11] focused on addressing the limitation of
CNNs, which can only handle ﬁxed-size images. They achieved this by inputting
images of diﬀerent scales into the ViT structure.

224
S. Zhang and Y. Liu
Fig. 1. Model overview of MSIQT. We employ multi-scale inputs to enrich the image
features, including the original size of the image and two smaller sizes obtained through
downsampling. Each image is fed into ResNet-50 to extract feature maps, which are
then fed into a patch generation module to segment them into ﬁxed-size patches. The
generated patches of diﬀerent scales are augmented with a CLS token and position
embeddings. Subsequently, these tokens are inputted into a Transformer encoder for
attention computation. Afterwards, the CLS tokens from the three scales are summed
up to form a single token, and the remaining vectors are concatenated and inputted
together into the decoder. The decoder further processes the cls token and feeds it into
MLP to obtain the ﬁnal quality score.
3
Proposed Method
3.1
Overall Architecture
ViT divides the input image into ﬁxed-sized patches, which may overlook some
distortions across patches and aﬀect the ﬁnal evaluation results. To address this
issue and extract image features more comprehensively, we use multi-scale input
in our model. Most previous ViT-based methods only include an encoder, where
the encoder’s output, the CLS token, is directly fed into an MLP to obtain the
ﬁnal output. We incorporate a decoder into our structure to provide further
interpretation of the output vectors. Figure 1 depicts the general design of our
suggested model.

MSIQT
225
3.2
Multi-scale Input
Using multi-scale inputs is beneﬁcial for enriching image features and strength-
ening the representation of cross-patch features that may be overlooked in single-
scale inputs. In our approach, we employ three scales of inputs, including the
original image and two downsampled scales, scale 1 and scale 2. According to
human visual perception, resizing an image while changing its aspect ratio can
result in a degradation of visual quality. Therefore, when downsampling the
image, we maintain its original aspect ratio to preserve the maximum amount
of inherent features on the smaller scales. Speciﬁcally, we start by adjusting the
width of the image to a given size. Then, based on the original aspect ratio of
the image, we calculate the corresponding height after the width modiﬁcation.
The new image height is calculated as follows:
H = h ∗(W/w)
(1)
where h is the original image’s height, w is the original image’s width, W is the
image’s width in the new scale, and H is the image’s new height in the new scale.
Finally, we perform downsampling on the image according to the generated new
dimensions. By preserving the original aspect ratio and maintaining proportional
adjustments, we aim to retain the maximum amount of original features on the
downscaled image.
3.3
Attention Aggregation in Transformer Encoder
Given an image I ∈RC×H×W , we use ResNet-50 [16] to extract image features
at three diﬀerent scales, resulting in three sets of feature maps with a dimension
of d = 2048 × h × w, where h and w are proportional to the original image size.
C represents the number of channels. The patch embedding module is used to
generate embeddings corresponding to the three scales. Assuming each image
is divided into N patches, each patch is transformed into embedding having a
dimension of D. We appended a CLS token Tcls ∈R1×D to each of the N embed-
dings yielding to N + 1 embeddings, like the original ViT. The learnable CLS
token is used to aggregate both global and local quality-aware features of images,
which are used for subsequent quality assessment. A positional embedding, used
to encode position information, is also added to these N+1 embeddings. The
multi-scale inputs of the image are eventually transformed into three sequences
as follows:
T = {Tcls, T1, T2, . . . TN}
(2)
These sequences are then input into the encoder for attention computation. The
encoder consists of several multi-head attention modules (MHSA). T is ﬁrst
transformed into three matrices: Q, K, and V . The computation process of the
encoder is as follows:
MHSA(Q, K, V ) =cat(Attention(Q1, K1, V1), . . . ,
Attention(Qg, Kg, Vg))WH
YM = MHSA(Q, K, V ) + T
YO = MLP(Norm(YM)) + YM,
(3)

226
S. Zhang and Y. Liu
where Attention(Qg, Kg, Vg) = softmax(
QgKT
g
√
d )Vg, WH represents the weights
of the linear projection layer and Norm() refers to the layer normalization.
Fig. 2. The speciﬁc structure of the Attention panel. The cls token output from the
encoder is expanded into N shares, and N depends on the number of panel members.
Each cls token is combined with a panel member, and then sent to the decoder for
processing.
3.4
Decoder and Quality Score Generation
The output YO obtained from the encoder can be represented as:
YO = {YO[0], YO[1], . . . , YO[N]} ∈R(N+1)×D
(4)
Since we input images of three scales, we get Z = {YO−original, YO−scale1,
YO−scale2} from encoder. Then, we merge the CLS tokens in three scales as
follows:
ZO[O] = YO−original[0] + YO−scale1[0] + YO−scale2[0],
(5)
and concatenate the remaining vectors, so that the information in all scales can
be processed in the decoder.
The new CLS token, ZO[0], is ﬁrst put through a multi-head attention mod-
ule to represent the relationship of one element with the others. The multi-
head attention output is then followed by a residual connection to construct the
decoder’s query. We use the remaining part of ZO as the key and value of the
decoder, use them and the query to calculate the cross-attention, and send the
output to MLP to calculate the ﬁnal score. In order to obtain a more robust eval-
uation score and reduce possible model prediction bias, inspired by DEIQA [23],

MSIQT
227
we add an attention panel at the decoder side. The structure is shown in Fig. 2.
The CLS token at the input of the decoder is copied N copies and added to the
learnable vector. The number of N is a hyperparameter. This vector participates
in the calculation of multi-head attention, and the output of cross-attention is
also expanded to N. We feed N outputs into MLP and average the outputs to
get the ﬁnal score.
Fig. 3. Some images in LIVEC.
4
Experimental Results
4.1
Datasets and Evaluation Protocols
MSIQT’s performance was examined using four datasets, which comprised
LIVE [12], CSIQ [13], TID2013 [14] and LIVEC [15]. LIVE consists of a total of
779 distorted images, which were generated by applying 5 to 6 levels of degra-
dation using ﬁve distortion operations on reference images. CSIQ contains 30
original images and 866 synthetic distorted images, encompassing six diﬀerent
types of distortions. TID2013 comprises 25 reference photos without distortion,
and every reference image is distorted with 24 diﬀerent types of distortions,
each having ﬁve levels of distortion. Common distortion types in the synthetic
datasets include Gaussian blur, JPEG compression, and random noise, among
others. LIVEC consists of 1162 images captured using modern mobile devices
under a wide range of real-world distortion conditions. These photos are taken by

228
S. Zhang and Y. Liu
diﬀerent photographers using various cameras, resulting in images with complex
and realistic distortions. Figure 3 shows some images in LIVEC.
As a model for evaluating image quality, the accuracy of evaluation results
is undoubtedly the most important indicator. In our experiments, we utilized
two commonly used metrics for evaluation, namely SRCC and PLCC. These
evaluation metrics are used to evaluate the accuracy of our model due to its
wide application in other domains as well as BIQA. And their absolute values
range from 0 to 1, with values closer to 1 indicating superior performance.
4.2
Implementation Details
The number of multi-scale inputs of MSIQT is set to 3, and the usual scale sizes
are the original image size, 384, 224. The image patch size is 16 and the token
dimension is 384. The encoder depth is 12 and the number of heads is also 12.
The depth of the decoder is 1 and the size of attention panel is set to 6.
MSIQT was trained for 100 epochs on each of the four quality assessment
datasets. The learning rate is set to 1e-4, and the batch size is generally 16.
The datasets were randomly split into two sections for training and testing, with
four-ﬁfths of the data used as the training set, and one-ﬁfth of the data used as
the test set. We perform the training on each dataset ten times to increase the
accuracy of the ﬁndings, and the mean values of the evaluation index are used
as the experimental result.
4.3
Comparison of Quality Evaluation Results
Table 1 compares our suggested method to several state-of-the-art BIQA meth-
ods on four datasets. These advanced methods include both traditional methods
and some methods based on deep learning. Our proposed method outperforms or
matches state-of-the-art methods on four datasets, especially on TID2013. Our
proposed model achieved better results on TID2013 dataset. The reason may
be that TID2013 dataset has more pictures than other datasets, and our model
extracts more suﬃcient features when facing with more images, thus exhibit-
ing superior performance over other methods. These experimental ﬁndings show
that our proposed strategy is eﬀective. To a certain extent, it shows that the
multi-scale input and the use of decoder can extract rich features in images and
perform eﬀective representation and mapping. As a non-synthetic image dataset,
LIVEC imposes stricter requirements on the model’s ability to extract features
and ﬁt scores. It can be seen that compared to the three synthetic datasets,
the test results of the model on LIVEC are slightly lower. This illustrates some
shortcomings of our model in the face of non-synthetic datasets, and points the
way for future research, that is, to make the model better evaluate images from
non-synthetic datasets.
4.4
Ablation Experiment
We conducted ablation tests to conﬁrm the precise role of each component of
the model. First, we replaced the decoder part of the model with ordinary MLP.

MSIQT
229
Table 1. MSIQT’s SRCC and PLCC results on four datasets, and comparisons with
other state-of-the-art methods. The red mark is the ﬁnest among them, and the blue
mark is the second best.
Method
LIVE
CSIQ
TID2013
LIVEC
SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC
BRISQUE [22] 0.926
0.935
0.866
0.893
0.367
0.475
0.316
0.369
WaDIQaM [4]
0.885
0.887
0.851
0.842
0.837
0.851
0.680
0.674
DBCNN [3]
0.919
0.918
0.946
0.959
0.816
0.865
0.837
0.863
HyperIQA [9]
0.964
0.962
0.926
0.942
0.859
0.874
0.846
0.873
TReS [21]
0.949
0.954
0.932
0.943
0.850
0.882
0.852
0.880
MSIQT(ours)
0.951
0.955
0.956
0.955
0.947
0.956
0.849
0.879
Table 2. Ablation experiments on LIVEC and CSIQ. The red mark is the ﬁnest among
them, and the blue mark is the second best.
Method
LIVEC
CSIQ
SRCC PLCC SRCC PLCC
MSIQT(without decoder)
0.824
0.841
0.919
0.921
MSIQT(without multi-scale input) 0.840
0.860
0.955
0.953
MSIQT(ours)
0.849
0.879
0.956
0.955
After comparing the two results, we found that the results on both datasets
decreased after replacing it with MLP, indicating that the decoder does play a
role in further processing and mapping the features obtained from the encoder.
Then, we remove the multi-scale input and replace it with a single-scale original
image input, and the rest of the model is the same as the original model. The
ﬁndings reveal that the model’s performance deterioration is not noticeable,
indicating that the impact of multi-scale input on the performance of the model
is not as large as imagined. The results predicted by the model are slightly lower.
This indicates that multi-scale input plays a role in enriching image features. The
speciﬁc experimental results are shown in the Table 2.
5
Conclusion
In this paper, we propose a BIQA transformer (MSIQT). It aims to extract
image features more comprehensively for quality evaluation purposes. We explore
quality-aware features from the multi-scale representation of the image and fur-
ther extract them using a pre-trained model. The transformer encoder is utilized
for aggregating attention mechanisms, while the decoder is employed to reﬁne
perceptual information, thereby achieving a more accurate assessment of image
quality. The results of experiments on four datasets show that MSIQT is superior
in terms of prediction accuracy.

230
S. Zhang and Y. Liu
Acknowledgement. This work was supported by the National Science Foundation
of China under grant 62201538 and Natural Science Foundation of Shandong Province
under grant ZR2022QF006.
References
1. Mittal, A., Soundararajan, R., Bovik, A.C.: Making a “Completely Blind” image
quality analyzer. IEEE Signal Process. Lett. 20(3), 209–212 (2013). https://doi.
org/10.1109/LSP.2012.2227726
2. Kang, L., Ye, P., Li, Y., Doermann, D.: Convolutional neural networks for no-
reference image quality assessment. In: 2014 IEEE Conference on Computer Vision
and Pattern Recognition, Columbus, OH, USA, 2014, pp. 1733–1740 (2014).
https://doi.org/10.1109/CVPR.2014.224
3. Zhang, W., Ma, K., Yan, J., Deng, D., Wang, Z.: Blind image quality assess-
ment using a deep bilinear convolutional neural network. IEEE Trans. Circuits
Syst. Video Technol. 30(1), 36–47 (2020). https://doi.org/10.1109/TCSVT.2018.
2886771
4. Bosse, S., Maniry, D., M¨uller, K.-R., Wiegand, T., Samek, W.: Deep neural net-
works for no-reference and full-reference image quality assessment. IEEE Trans.
Image Process. 27(1), 206–219 (2018). https://doi.org/10.1109/TIP.2017.2760518
5. Liu, X., Van De Weijer, J., Bagdanov, A.D.: RankIQA: learning from rankings for
no-reference image quality assessment. In: 2017 IEEE International Conference on
Computer Vision (ICCV), Venice, Italy, 2017, pp. 1040–1049 (2017). https://doi.
org/10.1109/ICCV.2017.118
6. Zhu, H., Li, L., Wu, J., Dong, W., Shi, G.: MetaIQA: deep meta-learning for no-
reference image quality assessment. In: 2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 2020, pp. 14131–
14140 (2020). https://doi.org/10.1109/CVPR42600.2020.01415
7. Vaswani, A., et al.: Attention is all you need. In: NIPS (2017)
8. Dosovitskiy, A., et al.: An image is worth 16x16 words: transformers for image
recognition at scale. In: International Conference on Learning Representations.
Virtual Event, Austria (2021)
9. Su, S., et al.: Blindly assess image quality in the wild guided by a self-adaptive
hyper network. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), Seattle, WA, USA, 2020, pp. 3664–3673 (2020). https://doi.
org/10.1109/CVPR42600.2020.00372
10. You, J., Korhonen, J.: Transformer for image quality assessment. In: 2021 IEEE
International Conference on Image Processing (ICIP), Anchorage, AK, USA, 2021,
pp. 1389–1393 (2021). https://doi.org/10.1109/ICIP42928.2021.9506075
11. Ke, J., Wang, Q., Wang, Y., et al.: MUSIQ: multi-scale image quality transformer.
In: International Conference on Computer Vision (2021). https://doi.org/10.1109/
ICCV48922.2021.00510
12. Sheikh, H.R., Sabir, M.F., Bovik, A.C.: A statistical evaluation of recent full ref-
erence image quality assessment algorithms. IEEE Trans. Image Process. 15(11),
3440–3451 (2006). https://doi.org/10.1109/TIP.2006.881959
13. Larson, E.C., Chandler, D.M.: Most apparent distortion: full-reference image qual-
ity assessment and the role of strategy. J. Electron. Imaging 19(1), 011006 (2010)

MSIQT
231
14. Ponomarenko, N., et al.: Image database TID2013: peculiarities, results and per-
spectives. Signal Process. Image Commun. 30, 57–77 (2015)
15. Ghadiyaram, D., Bovik, A.C.: Massive online crowdsourced study of subjective
and objective picture quality. IEEE Trans. Image Process. 25(1), 372–387 (2016).
https://doi.org/10.1109/TIP.2015.2500021
16. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
Las Vegas, NV, USA, 2016, pp. 770–778 (2016). https://doi.org/10.1109/CVPR.
2016.90
17. Lecun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to
document recognition. Proc. IEEE 86(11), 2278–2324 (1998). https://doi.org/10.
1109/5.726791
18. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
from error visibility to structural similarity. IEEE Trans. Image Process. 13(4),
600–612 (2004). https://doi.org/10.1109/TIP.2003.819861
19. Liu, Y., Zhai, G., Gu, K., Liu, X., Zhao, D., Gao, W.: Reduced-reference image
quality assessment in free-energy principle and sparse representation. IEEE Trans.
Multimedia 20(2), 379–391 (2018). https://doi.org/10.1109/TMM.2017.2729020
20. Moorthy, A.K., Bovik, A.C.: Blind image quality assessment: from natural scene
statistics to perceptual quality. IEEE Trans. Image Process. 20(12), 3350–3364
(2011). https://doi.org/10.1109/TIP.2011.2147325
21. Golestaneh, S.A., Dadsetan, S., Kitani, K.M.: No reference image quality assess-
ment via transformers, relative ranking, and self-consistency. In: Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1220–1230
(2022)
22. Mittal, A., Moorthy, A.K., Bovik, A.C.: No-reference image quality assessment in
the spatial domain. IEEE Trans. Image Process. 21(12), 4695–4708 (2012). https://
doi.org/10.1109/TIP.2012.2214050
23. Qin, G., et al.: Data-eﬃcient image quality assessment with attention-panel
decoder. arXiv:abs/2304.04952 (2023)
24. Cheon, M., Yoon, S.-J., Kang, B., Lee, J.: Perceptual image quality assessment with
transformers. In: 2021 IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW), Nashville, TN, USA, 2021, pp. 433–442 (2021).
https://doi.org/10.1109/CVPRW53098.2021.00054
25. Zhang, L., Zhang, L., Bovik, A.C.: A feature-enriched completely blind image qual-
ity evaluator. IEEE Trans. Image Process. 24(8), 2579–2591 (2015). https://doi.
org/10.1109/TIP.2015.2426416
26. Xu, J., Ye, P., Li, Q., Du, H., Liu, Y., Doermann, D.: Blind image quality assess-
ment based on high order statistics aggregation. IEEE Trans. Image Process. 25(9),
4444–4457 (2016). https://doi.org/10.1109/TIP.2016.2585880
27. Liu, Y., et al.: Unsupervised blind image quality evaluation via statistical measure-
ments of structure, naturalness, and perception. IEEE Trans. Circuits Syst. Video
Technol. 30(4), 929–943 (2020). https://doi.org/10.1109/TCSVT.2019.2900472

Low-Light Image Enhancement
via Unsupervised Learning
Wenchao He and Yutao Liu(B)
School of Computer Science and Technology, Ocean University of China,
Qingdao 266100, China
hwc@stu.ouc.edu.cn, liuyutao@ouc.edu.cn
Abstract. The models based on unsupervised learning methods have
achieved prominent achievement in several low-level tasks such as image
restoration and low-light enhancement. Many of them are based on gen-
erative adversarial networks such as EnlightenGAN. Although Enlight-
enGAN can be trained without the need for paired images, there are
still existing some issues such as insuﬃcient illumination and color dis-
tortion. Inspired by the achievement in visual tasks made by Vision
Transformer(ViT), we propose a discriminator based on ViT to replace
the original fully convolutional network to solve this problem. Further-
more, to improve the illumination enhancement eﬀect, we devise a new
loss function enlightened by the luminance in SSIM and multi-scale
SSIM. Our method surpasses the state-of-the-art on mainstream test-
ing datasets.
Keywords: Low-Light image Enhancement · Unsupervised Learning ·
Vision Transformer · Generative Adversarial Network
1
Introduction
In the real-world environment, capturing high-quality images in insuﬃcient illu-
mination is challenging. This is due to the poor shooting environment, the lim-
itation of the photographic devices, and incorrect operation by the photogra-
pher. It will cause low ISO, high noise, and poor visibility that will not only
impact the visual experience but also diminish the performance of numerous
downstream high-level vision algorithms. To suppress the degradation, there
are various algorithms that have been proposed for decades, such as Histogram
equalization(HE) [1,2]and Retinex theory methods [3–6].
With the tremendous progress in deep neural networks, methods based on
deep learning have made brilliant success in a series of low-level tasks, including
super-resolution [9], denoising [7,8], and dehazing [10]. However, various deep
learning-based methods heavily depend on the low-normal image pairs, which
raises a series of challenges [11,12]. For the low/normal image pairs gather-
ing, capturing them in the same place is extremely diﬃcult and impracticable
[13,14]. Although the synthesized dataset obtained by adjusting gamma values
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 232–243, 2024.
https://doi.org/10.1007/978-981-99-8850-1_19

Low-Light Image Enhancement via Unsupervised Learning
233
to normal-light images can alleviate the problem, the dataset obtained in this
way does not take into account the speciﬁc situation of the real world [15,16].
To mitigate the problem of lacking paired images, unsupervised learning
methods are proposed. EnlightenGAN [17] is the ﬁrst work that introduced
unsupervised learning to the low-light enhancement domain with Generative
adversarial network(GAN) [18]. EnlightenGAN is formed of two components:
U-net [19] architecture generator, global-local discriminator. Although Enlight-
enGAN achieves outstanding enhancement eﬀects on low-light images, there are
still existing some issues with it, such as somewhere lack of enhancement and
color distortion.
Beneﬁting by the global attention mechanism, Vision Transformer(ViT) can
eﬀectively extract image features for classiﬁcation [20]. We consider what dis-
criminators do to be essentially a classiﬁcation task. Inspired by this motivation,
we propose to replace the vanilla discriminator with the pure ViT to extract
the features to discriminate the image whether real normal-light image or not.
And it will help the generator to improve itself for enhancing. Moreover, to
address the issue of insuﬃcient enhancement in some areas, we propose single-
scale and multi-scale illumination loss for enhancement. We ﬁnd that the illu-
minance is extremely similar between the normal-light image and the enhanced
image. Extensive experiments are conducted on mainstream datasets and out-
comes indicate that our method outperforms other methods we compared.
2
Related Work
2.1
Traditional Method
In the past few decades, research on low-light enhancement has a long tradi-
tion, various algorithms have been proposed to boost the visual quality for both
subjective and objective. Histogram equalization(HE) [1,2] is a famous classic
technique in image enhancement, which is wide-used in several tasks. By redis-
tributing the pixel intensities of an image’s histogram, HE and its variants make
the distribution more uniform across the entire intensity range and thus can
improve the image’s contrast eﬃcaciously.
Another category of low-light image enhancement technique is Retinex the-
ory [3,4], which assumes that the image is composed of two components, illumi-
nation, and reﬂectance. The ﬁrst algorithm of Retinex theory was Single Scale
Retinex (SSR), which only manipulates the reﬂectance. And then, Multi-Scale
Retinex (MSR) was proposed by Jobson et al. to suppress the degradation of the
image by linearly combining multi-scale enhancement results [5,6]. The outputs
from those early attempts look somewhere over-exposure and strange. Wang
et al. proposed an improved algorithm called NPE, which keeps the balance
between naturalness and enhancement by manipulating the illumination [21].
Fu et al. proposed a weighted variational algorithm named SRIE to simulta-
neously provide an estimation of both reﬂectance and illumination and then
adjust the illumination [22]. Guo et al. proposed a method named LIME, which
accelerates the illumination map estimation through ALM [23]. Those works

234
W. He and Y. Liu
substantially suppose that the images don’t exist color distortion and all are
noise-free. However, the real circumstance is a bit more complex.
2.2
Deep Learning Method
Full Supervised Method. With the achievement made by deep learning in
other vision tasks, deep learning is introduced into low-light enhancement. Lore
et al. proposed the LLNet to utilize Sparse Autoencoder to enhance the con-
trast and denoise simultaneously, which was the ﬁrst work applying convolu-
tional neural networks in the low-light enhancement domain [24]. Inspired by the
Retinex theory, Chen et al. proposed an end-to-end framework RetinexNet that
combines Retinex theory with deep network [25]. GladNet divided the enhance-
ment task into two steps–illumination estimation and detail reconstruction [26].
MBLLEN proposed using 3D convolution instead of 2D convolution for video
low-light image enhancement [27]. However, training these models extremely
requires low/normal image pairs, which are diﬃcult for collecting.
Unsupervised Method. Unsupervised learning had achieved progress in other
low-level tasks. EnligthenGAN was the ﬁrst work to introduce unsupervised
learning to low-light enhancement, which was an adversarial network includ-
ing global-local discriminator [17]. Guo et al. presented an unsupervised model
named Zero-DCE, which formulated the enhancement task as an illumination
curve learning [28]. Although those methods solve the problem of requiring paired
dataset training, they still remain the drawbacks of color distortion and image
noise.
3
Proposed Method
As shown in the model in Fig. 1, the architecture of the proposed work is com-
posed of an attention-guided generator and global-local discriminators. The U-
Net style generator is the same as the generator in EnlightenGAN [17]. And each
attention module of the generator is to multiply the feature map with a (resized)
attention map. However, we proposed a pure Vision Transformer to replace the
vanilla discriminators. Moreover, we propose illumination loss and MSI loss as
supplements for the loss function.
3.1
Vision Transformer Discriminator
Same as the EnlightenGAN, we also adopt global-local discriminator structure.
However, instead of the vanilla full convolutional network, we adopt the Vision
Transformer(ViT) for image discrimination.
Intuitively speaking, identifying whether an image is an enhanced image by
a discriminator is essentially an image classiﬁcation problem. Because both real
and fake can be analogized as two labels for image classiﬁcation. Although both

Low-Light Image Enhancement via Unsupervised Learning
235
vanilla global and local discriminators employ the same PatchGAN [29] archi-
tecture for real/fake discrimination, limited by the convolutional neural network
itself, the vanilla PatchGAN can’t extract the features to direct the generator
to improve itself better. Inspired by the huge success made by ViT in image
classiﬁcation, we propose replacing the vanilla full convolutional network with
pure ViT. The structure of the pure ViT discriminators is shown on the right of
Fig. 1.
Fig. 1. Total structure of our model. The generator is on the left side. Each convolu-
tional block of the generator consists two 3 × 3 convolutional layers and then through
the operation of the batch normalization and the LeakyRelu. On the right side is
global-local discriminator, which consists of transformer blocks. The blue dashed line
indicates the transformer encoder. (Color ﬁgure online)
For the global ViT discriminator, we apply ﬁve transformer attention blocks
to build up the network. The original standard function of the discriminator is
the same as the one in EnlightenGAN [17]:
DRe(xr, xf) = σ((R(xr)) −Exf ∼Pfake[R(xf)])
(1)
DRe(xf, xr) = σ((R(xf)) −Exr∼Preal[R(xr)])
(2)
where R denotes the discriminator, xr and xf are sampled from the real and fake
distribution, σ stands for the sigmoid function. As for the generative adversarial
loss of our model, we also adopt the least-square GAN (LSGAN) to take the
place of the original sigmoid function.
LGlobal
D
= Exr∼Preal[(DRe(xr, xf) −1)2] + Exf ∼Pfake[DRe(xf, xr)]2
(3)
LGlobal
G
= Exf ∼Pfake[(DRe(xf, xr) −1)2] + Exr∼Preal[DRe(xr, xf)]2
(4)

236
W. He and Y. Liu
Also, we utilize four attention blocks to build up the local discriminator. We
also randomly crop 5 patches from both the real images and the outputs each
time, here is the local adversarial loss:
LLocal
D
= Exf ∼Pfake−patches[(D(xf) −0)2] + Exr∼Preal−patches[(D(xr) −1)2] (5)
LLobal
G
= Exf ∼Pfake−patches[(D(xf) −1)2]
(6)
3.2
Loss Function
Single-Scale and Multi-scale Illumination Loss.
Structure Similarity
Index Measure(SSIM) [30] is one of the well-known visual quality metrics, which
mainly considers three key features, luminance, contrast, and structure. The
original SSIM function is:
SSIM(x, y) = [l(x, y)α · c(x, y)β · s(x, y)γ]
(7)
where exponents α,β,γ are all set to 1 by default.
Usually, when we determine the similarity between two images, we need to
combine those three variables into SSIM. In our task, we should improve the
illumination of the low-light image intuitively. We ﬁnd that the two normal-light
images are almost identical in luminance, while there is a signiﬁcant diﬀerence
between a low-light image and a normal-light image. Therefore, the Single-scale
illumination loss(SSI) LSSI is deﬁned as:
LSSI = 2μxμy + c1
μ2x + μ2y + c1
(8)
where μx,μy denote the mean value of the two input images’ pixels separately.
By default, we set c1 = 1e-4.
The illumination perception of an image also depends on the distance from
the image to the observer. Based on this, we used Multi-scale illumination(MSI)
loss as a constraint, which is the illuminance in MS-SSIM [31]. We deﬁne it as:
LMSI = [LSSI]αM
(9)
where the αM depends on the scale you choose. By default, we choose M = 5
and the αM = 0.1333.
Self Feature Preserving Loss. Johnson et al. [32] proposed perceptual loss
by modeling the feature space distance between images using a pre-trained
VGG model for maintaining the perceptual similarity of images before and after

Low-Light Image Enhancement via Unsupervised Learning
237
enhancement. Brandon et al.
[33] observed that pre-trained VGG models are
not susceptible when adjusting the input image’s pixel intensity range. For the
sake of this, our self feature preserving loss is the same as the EnlightneGAN’s
loss:
LSF P (IL) =
1
Wi,jHi, j
Wi,j

x=1
Hi,j

y=1
(Φi,j(IL) −Φi,j(G(IL)))2
(10)
where IL represents the input images for enhancement. G(IL) represents results
through the generator. Φi, j(·) represents the feature maps extracted from images
by the pre-trained VGG-16 model. Wi,j denotes the width of the extracted fea-
ture maps and the Hi,j denotes the height of the extracted feature maps. i
represents the VGG-16’s i-th max pooling layer, and j represents the j-th con-
volutional layer after the maxpooling layer. By default, we choose i = 5, j = 1.
Same to the generative adversarial loss, we also deﬁne the self feature pre-
serving loss into two parts. For the local discriminator, the local self feature
preserving Loss is deﬁned as LLocal
SF P , while the global one is deﬁned as LGlobal
SF P .
The total loss function for training the whole model is described as follows:
Loss = LLocal
SF P + LGlobal
SF P
+ LGlobal
G
+ LLocal
G
+ LSSI + LMSI
(11)
4
Experiment
4.1
Datasets and Implementation Details.
Unpaired Enhancement Dataset Jiang et al. [17] have collected an unpaired
enhancement dataset for training the model. The training dataset contains 1016
normal-light images and 914 low-light images which are obtained from public
datasets. All of them are in PNG format and then resized to 400 × 600 for
training and testing.
Testing Dataset. For the testing images, we choose the LOL dataset [25]
and VE-LOL [34] dataset to measure the model performance. The LOL dataset
includes 15 low/normal light image pairs and the VE-LOL contains 100
low/normal light image pairs. All of them perform the same operations as the
training dataset.
Implementation Details. Input images are randomly cropped to 256 × 256
for training while the testing images retain the initial size. In the ﬁrst 100 epochs,
the initial learning rate is 1e-4, then gradually decays to 0 in the last 100 epochs.
Adam is used as the optimizer and the training batch size is set to 32. The model
is trained under two Nvidia RTX 3090 for nearly two hours.

238
W. He and Y. Liu
Table 1. Objective results in VE-LOL and LOL datasets among all the methods
Method
VE-LOL
LOL
SSIM ↑MS-SSIM↑PSNR↑SSIM↑MS-SSIM↑PSNR ↑
LIME
0.4674
0.4598
16.9714 0.4822
0.4747
17.1812
SRIE
0.5204
0.5203
14.4505 0.4937
0.4942
11.8552
NPE
0.4617
0.4543
17.3324 0.4818
0.4744
16.9697
Dong et al.
0.4757
0.4692
17.2553 0.4792
0.4722
16.7165
RetinexNet
0.4092
0.4007
16.0971 0.3373
0.3268
14.9774
GladNet
0.6871
0.6820
19.8200 0.5636
0.5570
16.9419
MBLLEN
0.6947
0.6943
17.8690 0.7270
0.7266
17.8583
EnlightenGAN 0.6794
0.6755
18.6396 0.5337
0.5284
15.3080
Zero-DCE
0.5770
0.5736
18.0587 0.5615
0.5588
14.8607
ours
0.7393
0.7359
19.5355 0.5928
0.5866
16.6105
4.2
Performance Evaluation
Objective Measurement. For objective measurement, we choose three visual
quality metrics: SSIM [30], MS-SSIM [31], and PSNR to assess the perfor-
mance of our model. As the results shown in Table 1, we compare several
competing models ranging from traditional to deep learning-based methods:
NPE [21], SRIE [22], LIME [23], dong et al. [35], RetinexNet [25], GladNet [26],
MBLLEN [27], EnligthenGAN [17] and Zero-DCE [28]. In the LOL dataset, our
method’s performance is only behind the MBLLEN in SSIM and MS-SSIM and
the performance in PSNR also falls behind many methods. However, among all
the methods, our method performs best in both SSIM and MS-SSIM on the
VE-LOL dataset, and only below GladNet on PSNR.
Subjective Measurement. As shown in Fig. 2, although our method falls
behind the MBLLEN and performs mediocrely in objective visual metrics in
LOL dataset, our method greatly suppresses color distortion and is more natu-
ral than other methods. As we can see, SRIE and Zero-DCE can not suﬃciently
enhance the brightness of images. In addition, from LIME to EnlightenGAN, all
of them make the background of the enhanced image greener than the normal-
light image, which means that they still exist color distortion issues. In contrast,
our method not only increases higher brightness but also suppresses color dis-
tortion.
Figure 3 displays the enhancement outcomes in the VE-LOL dataset. Our
method’s results are more approaching the ground-truth and are looking more
natural. The results of RetinexNet exists severe image noise. LIME and NPE
lead to over-exposure results, while Zero-DCE and SRIE lead to under-exposure.
Also as the bounding box shows in Fig. 3, the outcome of EnlightenGAN occurs
color distortion, because the leaves turn yellow, not green. Compared to other

Low-Light Image Enhancement via Unsupervised Learning
239
enhancement methods, our method provides a more natural and realistic visual
performance on those two datasets.
(a) normal-light
(b) low-light
(c) NPE [21]
(d) SRIE [22]
(e) LIME [23]
(f) dong et al. [35]
(g) RetinexNet [25]
(h) GladNet [26]
(i) MBLLEN [27]
(j) EnlightenGAN [17]
(k) Zero-DCE [28]
(l) Ours
Fig. 2. Visual comparison with other enhancement methods in LOL dataset
Table 2. Objective visual quality results in VE-LOL dataset for ablation study
pure Vision Transformer SSI Loss MSI Loss VE-LOL
SSIM↑MS-SSIM↑PSNR↑



0.7393
0.7359
19.5355



0.7257
0.7213
19.6525



0.7374
0.7335
19.5075



0.6553
0.6499
18.0113



0.6673
0.6649
17.0985

240
W. He and Y. Liu
Table 3. Objective visual quality results in LOL dataset for ablation study
pure Vision Transformer SSI Loss MSI Loss LOL
SSIM↑MS-SSIM↑PSNR↑



0.5928
0.5866
16.6105



0.5554
0.5503
15.5048



0.5855
0.5791
16.7426



0.6553
0.6499
18.0113



0.5823
0.5764
16.3229
(a) normal-light
(b) low-light
(c) NPE [21]
(d) SRIE [22]
(e) LIME [23]
(f) dong et al. [35]
(g) RetinexNet [25]
(h) GladNet [26]
(i) MBLLEN [27]
(j) EnlightenGAN [17]
(k) Zero-DCE [28]
(l) ours
Fig. 3. Visual comparison with other enhancement methods in VE-LOL dataset

Low-Light Image Enhancement via Unsupervised Learning
241
4.3
Ablation Study
A group of ablation experiments are conducted to test the eﬀectiveness of the
modules we proposed. Firstly, we focus on the visual quality measurements,
which are shown in Tables 2 and 3. For each table, the ﬁrst three columns list
our proposed modules and the last three are the visual quality metrics. Accord-
ing to Table 2 and Table 3, the SSIM and MS-SSIM signiﬁcantly drop while
without illumination or MS-SSIM loss, especially the former. Although replac-
ing the vanilla discriminator with ViT will lead to a decrease in visual metrics,
as shown in Fig. 4, the ViT discriminator structure can eﬀectively suppress color
distortion. As the image is shown in Fig. 4(c), the cabinet color is slightly green
without the ViT, but in Fig. 4(f) the cabinet color remains white.
(a) low-light
(b) normal-light
(c) without ViT
(d) without MSI loss
(e) without SSI loss
(f) ours
Fig. 4. Visual comparison from the ablation study of our method
5
Conclusion
In this paper, a new unsupervised low-light image enhancement model is pro-
posed. By replacing the fully convolutional network with ViT, it cultivates both
subjective and objective visual qualities. Moreover, with the illumination loss
and MS-SSIM constraint, our model shows superiority among the compared
models.
However, the computational complexity of pure ViT greatly limits the speed
of model training. In the future, we’d upgrade the structure of the discriminator
for reducing computational complexity and improving visual quality.
Acknowledgements. This work was supported by the National Science Foundation
of China under grant 62201538 and Natural Science Foundation of Shandong Province
under grant ZR2022QF006.

242
W. He and Y. Liu
References
1. Pizer, S.M., et al.: Adaptive histogram equalization and its variations. Comput.
Vis. Graphics Image Process. 39(3), 355–368 (1987)
2. Zuiderveld, K.: Contrast Limited Adaptive Histogram Equalization, p. 474–485.
Academic Press Professional Inc, USA (1994)
3. Land, E.H., McCann, J.J.: Lightness and retinex theory. JOSA 61(1), 1–11 (1971)
4. Land, E.H.: The retinex. Am. Sci. 52(2), 247–264 (1964)
5. Rahman, Z.U., Jobson, D.J., Woodell, G.A.: Multi-scale retinex for color image
enhancement. In: Proceedings of 3rd IEEE International Conference on Image
Processing, vol. 3, pp. 1003–1006. IEEE (1996)
6. Jobson, D.J., Rahman, Z.U., Woodell, G.A.: A multiscale retinex for bridging the
gap between color images and the human observation of scenes. IEEE Trans. Image
Process. 6(7), 965–976 (1997)
7. Xie, J., Xu, L., Chen, E.: Image denoising and inpainting with deep neural net-
works. In: Advances in Neural information Processing Systems, vol. 25 (2012)
8. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser:
residual learning of deep CNN for image denoising. IEEE Trans. Image Process.
26(7), 3142–3155 (2017)
9. Dong, C., Loy, C.C., He, K., Tang, X.: Image super-resolution using deep convo-
lutional networks. IEEE Trans. Pattern Anal. Mach. Intell. 38(2), 295–307 (2015)
10. Cai, B., Xu, X., Jia, K., Qing, C., Tao, D.: DehazeNet: an end-to-end system for
single image haze removal. IEEE Trans. Image Process. 25(11), 5187–5198 (2016)
11. Liu, Y.: Unsupervised blind image quality evaluation via statistical measurements
of structure, naturalness, and perception. IEEE Trans. Circuits Syst. Video Tech-
nol. 30(4), 929–943 (2020)
12. Liu, Y., Zhai, G., Gu, K., Liu, X., Zhao, D., Gao, W.: Reduced-reference image
quality assessment in free-energy principle and sparse representation. IEEE Trans.
Multimedia 20(2), 379–391 (2018)
13. Liu, Y., Gu, K., Wang, S., Zhao, D., Gao, W.: Blind quality assessment of camera
images based on low-level and high-level statistical features. IEEE Trans. Multi-
media 21(1), 135–146 (2019)
14. Liu, Y., Gu, K., Li, X., Zhang, Y.: Blind image quality assessment by natural
scene statistics and perceptual characteristics. ACM Trans. Multimedia Comput.
Commun. Appl. (TOMM) 16(3), 1–91 (2020)
15. Hu, R., Liu, Y., Gu, K., Min, X., Zhai, G.: Toward a no-reference quality metric for
camera-captured images. IEEE Trans. Cybern. 53(6), 3651–3664 (2023). https://
doi.org/10.1109/TCYB.2021.3128023
16. Min, X., Zhai, G., Gu, K., Liu, Y., Yang, X.: Blind image quality estimation via
distortion aggravation. IEEE Trans. Broadcast. 64(2), 508–517 (2018)
17. Jiang, Y., et al.: Enlightengan: deep light enhancement without paired supervision.
IEEE Trans. Image Process. 30, 2340–2349 (2021)
18. Goodfellow, I., et al.: Generative adversarial networks. Commun. ACM 63(11),
139–144 (2020)
19. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomed-
ical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.
(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-24574-4 28
20. Dosovitskiy, A., et al.: An image is worth 16x16 words: transformers for image
recognition at scale. arXiv preprint arXiv:2010.11929 (2020)

Low-Light Image Enhancement via Unsupervised Learning
243
21. Wang, S., Zheng, J., Hu, H.M., Li, B.: Naturalness preserved enhancement algo-
rithm for non-uniform illumination images. IEEE Trans. Image Process. 22(9),
3538–3548 (2013)
22. Fu, X., Zeng, D., Huang, Y., Zhang, X.P., Ding, X.: A weighted variational model
for simultaneous reﬂectance and illumination estimation. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 2782–2790
(2016)
23. Guo, X., Li, Y., Ling, H.: Lime: low-light image enhancement via illumination map
estimation. IEEE Trans. Image Process. 26(2), 982–993 (2016)
24. Lore, K.G., Akintayo, A., Sarkar, S.: LLNet: a deep autoencoder approach to nat-
ural low-light image enhancement. Pattern Recogn. 61, 650–662 (2017)
25. Wei, C., Wang, W., Yang, W., Liu, J.: Deep retinex decomposition for low-light
enhancement. arXiv preprint arXiv:1808.04560 (2018)
26. Wang, W., Wei, C., Yang, W., Liu, J.: GladNet: low-light enhancement network
with global awareness. In: 2018 13th IEEE International Conference on Automatic
Face & Gesture Recognition (FG 2018), pp. 751–755. IEEE (2018)
27. Lv, F., Lu, F., Wu, J., Lim, C.: MBLLEN: low-light image/video enhancement
using CNNs. In: BMVC, vol. 220, p. 4 (2018)
28. Guo, C., et al.: Zero-reference deep curve estimation for low-light image enhance-
ment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 1780–1789 (2020)
29. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation
using cycle-consistent adversarial networks. In: Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pp. 2223–2232 (2017)
30. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
from error visibility to structural similarity. IEEE Trans. Image Process. 13(4),
600–612 (2004)
31. Wang, Z., Simoncelli, E.P., Bovik, A.C.: Multiscale structural similarity for image
quality assessment. In: The Thrity-Seventh Asilomar Conference on Signals, Sys-
tems & Computers, 2003. vol. 2, pp. 1398–1402. IEEE (2003)
32. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and
super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016,
Part II. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). https://doi.org/10.
1007/978-3-319-46475-6 43
33. RichardWebster, B., Anthony, S.E., Scheirer, W.J.: Psyphy: a psychophysics driven
evaluation framework for visual recognition. IEEE Trans. Pattern Anal. Mach.
Intell. 41(9), 2280–2286 (2018)
34. Liu, J., Xu, D., Yang, W., Fan, M., Huang, H.: Benchmarking low-light image
enhancement and beyond. Int. J. Comput. Vision 129, 1153–1184 (2021)
35. Dong, X., Pang, Y., Wen, J.: Fast eﬃcient algorithm for enhancement of low light-
ing video. In: ACM SIGGRAPH 2010 Posters, pp. 1–1 (2010)

GLCANet: Context Attention
for Infrared Small Target Detection
Rui Liu1,2, Qiankun Liu2, Xiaoyong Wang2(B), and Ying Fu1,2
1 Yangtze Delta Region Academy of Beijing Institute of Technology,
Jiaxing 314019, China
2 MIIT Key Laboratory of Complex-ﬁeld Intelligent Sensing,
Beijing Institute of Technology, Beijing 100081, China
{liurui20,liuqk3,wangxiaoyong,fuying}@bit.edu.cn
Abstract. Infrared small target detection (IRSTD) refers to extracting
small targets from infrared images with noisy interference and blurred
background. Due to their small size and low contrast in the image,
infrared targets are easily overwhelmed, which requires the network to
have a wider receptive ﬁeld for images and better ability to process local
information. How to extract contextual information simply and eﬃciently
remains challenging. In this paper, we propose a global and local context
attention network (GLCANet), where the global context extraction mod-
ule (GCEM) and the local context attention module (LCAM) are devised
to address this problem. Speciﬁcally, GCEM transforms the feature map
from the spatial domain to the frequency domain for feature extraction.
Since updating a single value in the frequency domain aﬀects all raw data
globally, GCEM enables the network to consider the global context at an
early stage and obtain a wider receptive ﬁeld. LCAM fuses multiple lay-
ers of features, where we devise a local context-oriented down-sampling
block (LCDB). LCDB transforms the planar dimension of the original
feature map into the spatial dimension, which can extract more local
contextual information while down-sampling the feature. Experiments
on public datasets demonstrate the superiority of our method over rep-
resentative state-of-the-art IRSTD methods.
Keywords: infrared image · target detection · contextual information
1
Introduction
Infrared small target detection (IRSTD) is widely used in various ﬁelds, includ-
ing military reconnaissance [1,2], security monitoring [3], ﬁre early warning [4,5]
and so on. Diﬀerent from general visible light target detection, there are several
diﬃculties in infrared small target detection: 1)Dim: Since infrared images usu-
ally have a lot of noise and background clutter, small targets are not obvious
in the image and are easily overwhelmed by the background. 2)Small: The size
of the target is very small, accounting for less than 0.15% of the entire image.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 244–255, 2024.
https://doi.org/10.1007/978-981-99-8850-1_20

GLCANet: Context Attention for Infrared Small Target Detection
245
3)Changeable shape: The shape and size of infrared small targets will change in
diﬀerent scenes.
To detect small infrared targets, various IRSTD methods have been pro-
posed. Typical methods include ﬁlter based methods [6–8], local information
based methods [9,10] and low-rank based methods [11,12]. These methods have
some limitations in complex backgrounds, target changes, low contrast and noise
processing, etc.
Diﬀerent from these typical methods, deep learning based methods achieve
better performance due to their unique characteristics,e.g., data-driven learn-
ing, automatic feature learning, and end-to-end learning. Among them, CNN-
based methods are developing most rapidly. Liu et al. [13] proposed the pio-
neer CNN-based method for IRSTD. After this, the work of Wang et al. [14]
achieved a balance between miss detection and false alarm. However, small and
dim infrared targets are easily overwhelmed by noise, background interference,
and other similar hotspots, which requires the network to have a wider recep-
tive ﬁeld for images and a better ability to process local information. Dai et al.
proposed ACM-Net [15] and ALCNet [20] to leverage contextual information,
which achieved good performance. Subsequently, lots of methods focusing on the
extraction of contextual information have been proposed [16–19,21–24]. When
extracting global contextual information, the receptive ﬁeld of these module-
based methods depends too much on the deepening of the network level, and
cannot eﬀectively extract global contextual information in the early stage, lead-
ing to the failure to achieve good performance. Meanwhile, global contextual
information is easily lost during feature conduction in deep networks, leading to
the inaccurate positioning of small targets. As for the extraction of local context
information, deepening the network tends to lead to information loss, and com-
plex modules also perform poorly. How to extract contextual information simply
and eﬃciently remains challenging.
To address this problem, we propose a global and local context attention
network (GLCANet) with two key modules. First, we devise a global context
extraction module (GCEM) to eﬃciently extract global contextual information
at an early stage. Speciﬁcally, GCEM transforms the feature map from the spa-
tial domain to the frequency domain for feature extraction. Since updating a
single value in the frequency domain aﬀects all raw data globally, GCEM can
eﬀectively extract global information at any layer in the network, which enables
the network to consider the global context from early layers and obtain a wider
receptive ﬁeld. Then, we devise a local context attention module (LCAM) to
extract more local contextual information. LCAM fuses multiple layers of fea-
tures, where we devise a local context-oriented down-sampling block (LCDB).
LCDB transforms the planar dimension of the original feature map into the
spatial dimension, which can extract more local contextual information while
down-sampling the feature. By combining GCEM and LCAM in the network,
the contextual information of the target is eﬃciently extracted. Results on the
two public datasets of IRSTD-1k [29] and NUAA-SIRST [15] demonstrate that

246
R. Liu et al.
our GLCANet outperforms other SOTA methods on metrics intersection over
union (IoU) ratio, probability detection (Pd) rate and false-alarm (Fa) rate.
The main contributions of this paper are summarized as follows:
– We propose a global context extraction module which can extract global
contextual information at any layer in the network and more accurately locate
small infrared targets.
– We propose a local context attention module to eﬀectively extract more local
contextual information and obtain a more complete target shape.
– Experiments on the two public datasets of IRSTD-1k and NUAA-SIRST
demonstrate the superior performance of our method.
2
Related Work
2.1
Infrared Small Target Detection
So far, various IRSTD methods have been proposed. Typical methods include
ﬁlter based methods [6–8], local information based methods [9,10] and low-rank
based methods [11,12]. These traditional methods have some limitations in com-
plex backgrounds, target changes, low contrast and noise processing, etc.
In contrast, deep learning based methods are characterized by automatic fea-
ture learning, high-level feature representation, large-scale data training, con-
textual information utilization, and end-to-end learning. This leads to better
performance of CNN-based methods on IRSTD. Liu et al. [13] proposed a multi-
layer perception (MLP) network, which is the pioneer CNN-based method for
IRSTD. After this, the work of Wang et al. [14] achieved a balance between miss
detection and false alarm.
Although these methods improve the detection eﬀect of infrared small targets,
the loss of small target information remains, which is limited by noise and clutter
interference in infrared images and information loss during feature extraction in
CNN-based methods.
2.2
Global Contextual Information
Researchers have done a lot of work to extract the global contextual informa-
tion of infrared small targets. An asymmetric contextual modulation module is
proposed by Dai et al. [15] to extract multi-layer contextual information. Zhang
et al. [16] computed global associations between semantics through joint work
between multiple modules. However, the receptive ﬁeld of these methods depends
too much on the deepening of the network level, and cannot eﬀectively extract
global contextual information in the early stage, leading to the failure to achieve
good performance. Wu et al. [18] proposed a network with a skip-connected fea-
ture pyramid network (SCFPN) to fuse small object features and contextual
multi-scale features. Ju et al. [19] proposed an end-to-end CNN-based target
detector ISTDet to achieve a better trade-oﬀbetween speed and accuracy. As
a result, global contextual information is easily lost during feature conduction,
leading to inaccurate positioning of small targets.

GLCANet: Context Attention for Infrared Small Target Detection
247
Fig. 1. Overview of the proposed GLCANet and the structure of GCEM and LCAM.
2.3
Local Contextual Information
Existing methods for extracting local contextual information are mainly clas-
siﬁed into module based methods and network based methods. Dai et al. [20]
proposed a cross-layer bottom-up local attentional modulation (BLAM) module.
Inspired by this, Tong et al. [21] proposed a BAA block to achieve dynamic per-
ception of ﬁne details. Yu et al. [22] proposed a simpliﬁed bilinear interpolation
attention module (SBAM) to speed up inference. However, when the network
level deepens, small infrared targets are easily overwhelmed by noise and clut-
ter, and these speciﬁc modules achieve poor performance. There are also some
works trying to design networks for local information. Qi et al. [23] usesd a U-
Net with skip connections to obtain low-level local details of small targets. Lv et
al. [24] proposed a specially crafted feature pyramid aggregation module to pro-
cess local information. Unfortunately, these complex modules do not eﬃciently
extract local information after greatly increasing the amount of computation,
not achieving the expected good performance.
3
Method
3.1
Overall Architecture
We show an overview of our GLCANet in Fig. 1. As can be seen, the network
backbone is a U-Net structure [25]. After expanding the number of channels, the

248
R. Liu et al.
infrared image is input to the global context encoder part. The features of each
layer in the encoder are input to the LCAM for processing. In the global decoder
part, the features processed by LCAM are fused with the features of each layer.
3.2
Global Context Extraction Module
Infrared images have a lot of noise and clutter, and small targets are often
submerged. In addition, most of the high-resolution feature maps are useless
backgrounds, and the recognition targets only occupy a small part of them, thus
wasting a lot of computation. All of these require the network to extract global
contextual information earlier. To solve this problem, we propose a global con-
text extraction module (GCEM). As shown in Fig. 1, we introduce fast Fourier
convolution [26] (FFC) to replace the convolution in the residual network, so that
the module has stronger global context acquisition capabilities. In addition, we
introduce the convolution block attention module [27] (CBAM) before resid-
ual addition. Channel attention and spatial attention in CBAM can suppress
unnecessary regional features.
Considering that updating a single value in the frequency domain aﬀects all
raw data globally, FFC is proposed to use global contextual information at early
feature layers. FFC divides features into local and global branches at the channel
level for processing. The local branch is processed by conventional convolutions,
while the global branch is processed by Fourier transform through a spectral
transformer module (STM). The operation steps of STM are as follows:
a) Apply a 2-D FFT operation to the input tensor L:
RH× W
2 ×C, IH× W
2 ×C = FFT(LH×W ×C),
(1)
where FFT(·) denotes the fast Fourier transform. R and I are the real part and
imaginary part of the result.
b) Apply 1×1 convolution, batch normalization and ReLU on L1 obtained by
concatenating the real part and the imaginary part along the channel dimension:
L
H× W
2 ×2C
2
= ReLU(Bn(Conv(L
H× W
2 ×2C
1
))),
(2)
where ReLU (·) denotes the ReLU function. Bn(·) and Conv(·) denote batch
normalization and convolutional layers, respectively.
c) Apply the inverse 2-D FFT operation to the re-split real part R′ and imaginary
part I′ from L2:
L′H×W ×C = iFFT(R′H× W
2 ×C, I′H× W
2 ×C),
(3)
where iFFT(·) denotes the inverse fast Fourier transform.
After FFC processing, the feature map integrates more contextual informa-
tion, but not all information is conducive to the detection of small targets. To
this end, we introduce a convolution block attention module (CBAM) to com-
bine the channel and spatial attention of the feature map, so as to strengthen
the eﬀective feature contribution while weakening the invalid.

GLCANet: Context Attention for Infrared Small Target Detection
249
3.3
Local Context Attention Module
GCEM’s extraction of global context provides more accurate positioning for
small targets, but good detection results also require a more complete target
shape. The simple skip connection in the U-Net structure cannot eﬀectively pay
attention to the local context for small and dim infrared targets, leading to
the loss of shape information. To solve the problem, a local context attention
module (LCAM) is proposed to extract more local contextual information. As
shown in Fig. 1, the LCAM fuses multiple layers of features, where we devise
a local context-oriented down-sampling block (LCDB). LCDB transforms the
planar dimension of the original feature map into the spatial dimension, which
can extract more local contextual information while down-sampling the feature.
The operation of LCDB can be expressed as:
R′ = ReLU(SPD(R) + Short(R)),
(4)
where ReLU (·) denotes the ReLU function. Short(·) stands for a 1×1 convolution
with a stride of 2 to retain more discriminative feature information. SPD(·) [28]
represents a structure with two convolutional layers to convert the plane infor-
mation of the feature map into depth information, and can be expressed as:
R∗= Conv1(R),
(5)
SPD(R) = Conv2(ReLU(Cat(R∗
0,0, R∗
0,1, R∗
1,0, R∗
1,1))),
(6)
where Conv1(·) and Conv2(·) denote convolutional layer. Cat(·) denotes the
concatenation operation. R∗
0,0, R∗
0,1, R∗
1,0, R∗
1,1 ∈R∗H×W ×C, and R∗
0,0 can be
obtained as:
R∗=
⎡
⎢⎢⎢⎣
a11 a12 · · · a1j
a21 a22 · · · a2j
...
...
...
...
ai1 ai2 · · · aij
⎤
⎥⎥⎥⎦, R∗
0,0 =
⎡
⎢⎢⎢⎣
a11
a13
· · ·
a1(j−1)
a33
a33
· · ·
a3(j−1)
...
...
...
...
a(i−1)1 a(i−1)3 · · · a(i−1)(j−1)
⎤
⎥⎥⎥⎦.
(7)
Compared with interpolation, this operation can retain more local context
information of small targets. As can be seen, although LCDB is a learnable
block, we use the same LCDB block at the same resolution layer. Such a setting
is inspired by the attention mechanism. When down-sampling the feature map
of the same layer, the features that need to be paid attention are roughly the
same. Therefore we choose to use the same LCDB to down-sample the same
layer features.
4
Experiment
4.1
Datasets and Evaluation Metrics
Datasets. We choose the IRSTD-1k [29] and NUAA-SIRST [15] as experimental
datasets. IRSTD-1k includes 1,001 infrared images, while NUAA-SIRST contains
427 infrared images. For each dataset, we take 50% of the images as the training
set, 30% of them as the validation set, and 20% of them as the test set.

250
R. Liu et al.
Table 1. Quantitative comparisons with state-of-the-art methods.
Method
IRSTD-1k
NUAA-SIRST
IoU
Pd
Fa
IoU
Pd
Fa
Top-Hat [8]
10.06
75.11
1432
7.143
79.84
1012
Max-Median [30] 6.998
65.21
59.73
4.172
69.20
55.33
WSLCM [31]
3.452
72.44
6619
1.158
77.95
5446
TLLCM [32]
3.311
77.39
6738
1.029
79.09
5899
IPI [11]
27.92
81.37
16.18
25.67
85.55
11.47
NRAM [33]
15.25
70.68
16.93
12.16
74.52
13.85
RIPT [34]
14.11
77.55
28.31
11.05
79.08
22.61
PSTNN [35]
24.57
71.99
35.26
22.40
77.95
29.11
MSLSTIPT [36]
11.43
79.03
1524
10.30
82.13
1131
MDvsFA [14]
49.50
82.11
80.33
60.30
89.35
56.35
ACM [15]
60.97
90.58
21.78
72.33
96.33
9.325
ALCNet [20]
62.05
92.19
31.56
74.31
97.34
20.21
Ours
67.97 93.88 12.22 78.24 99.98 10.29
Evaluation Metrics. We evaluate the proposed GCEM and LCDM modules using
several common metrics as follows:
a) Intersection over Union (IoU): It is the ratio of the intersecting area to the
union area:
IoU = Ainter
Aunion
,
(8)
where Ainter and Aunion denote the intersection areas and union areas.
b) Probability of Detection ( Pd): It is the ratio of the number of correctly pre-
dicted targets Npred to the total number of targets Ntotal:
Pd = Npred
Ntotal
.
(9)
c) False-Alarm Rate ( Fa): It is the ratio of the false predicted target pixels
Pfalse to all pixels in the image Pall:
Fa = Pfalse
Pall
.
(10)
4.2
Implementation Details
AdaGrad is adopted as the optimizer and we set the learning rate to be constant
at 0.05. We set the training epoch and batch size of the model to 500 and
4, respectively. ALCNet [20], ACMNet [15], and MDvsFA [14] are selected as
representative CNN-based methods for comparison. For traditional methods,
Top-Hat [8], Max-Median [30], WSLCM [31], TLLCM [32], IPI [11], NRAM [33],
RIPT [34], PSTNN [35], and MSLSTIPT [36] are selected.

GLCANet: Context Attention for Infrared Small Target Detection
251
Fig. 2. Visual comparison of detection performance on several representative infrared
images. The box in the upper right corner is a close-up view of the target. Correctly
detected targets, missed detected targets, and falsely detected targets are framed by
red, blue, and yellow boxes, respectively. (Color ﬁgure online)
4.3
Quantitative Results
As can be seen from Table. 1, compared to SOTA methods, our method performs
best on most metrics on both datasets. Traditional methods have limited ability
in challenging cases and thus perform poorly. Moreover, as can be seen from the
results, other CNN-based methods do not pay enough attention to contextual
information, resulting in inaccurate positioning and shape detection of the target.
In contrast, the IoU of our method reaches as high as 67.97% on the IRSTD-
1k dataset, which means that our method can detect the object shape more
completely. The performance on Pd and Fa also demonstrates that our method
is more accurate for target localization.
4.4
Visual Results
We selected several representative infrared images in the NUAA-SIRST dataset
to compare the visual eﬀects of diﬀerent methods. As shown in Fig. 2, traditional
methods frequently produce false detection and missed detection, other CNN-
based methods cannot eﬀectively use contextual information, which leads to

252
R. Liu et al.
insuﬃcient target localization accuracy and loss of edge information. However,
our method can accurately locate the target and detect a more complete target
shape. We attribute this success to the unique modules, i.e. the GCEM and
LCAM, where the former can eﬀectively extract global contextual information
and fuse it into features to accurately locate small targets, and the latter can
take advantage of local contextual information after localization to maintain the
details of small targets.
Table 2. Ablation study of the GCEM and the LCAM.
Modules
IoU
Pd
Fa
UNet
64.98
89.46
15.71
UNet+GCEM
66.89
90.82
9.03
UNet+LCAM
66.71
89.22
9.11
UNet+GCEM+LCAM 67.97 93.88 12.22
Table 3. Ablation study of FFC blocks in GCEM.
Blocks
IoU
Pd
Fa
Conv+Conv 65.76
90.13
12.98
FFC+Conv
66.48
90.14
9.34
Conv+FFC
65.95
91.50 13.06
FFC+FFC
66.89 90.82
9.03
Table 4. Ablation study of CBAM in GCEM.
Methods
IoU
Pd
Fa
GCEM w/o CBAM 64.99
88.44
12.45
GCEM
66.89 90.82 9.03
4.5
Ablation Study
To investigate the eﬀectiveness of the proposed GCEM and LCAM, we conduct
several ablation studies on the IRSTD-1k dataset. As can be seen from Table 2,
GCEM and LCAM each improve the model performance, and using both of them
achieves the best balance of detection metrics.
Impact of GCEM. We investigate the inﬂuence of diﬀerent numbers and positions
of FFC blocks in GCEM. As shown in Table 3, the FFC block has a signiﬁcant
improvement eﬀect. Speciﬁcally, GCEM achieves the best balance among all
metrics when using two FFC blocks at the same time. Moreover, as shown in
Table 4, when not using CBAM, GCEM produces more false predictions, and
the IoU is signiﬁcantly reduced. We choose two FFC blocks with CBAM as the
default setting.

GLCANet: Context Attention for Infrared Small Target Detection
253
Impact of LCAM. We also performe ablation study on the proposed LCAM. As
shown in Table 5, when not using GCEM, LCAM produces more false predictions
and achieves poor performance on the IoU metric. The IoU is also signiﬁcantly
reduced when not using LCDB. The best balance is delivered when using both
of them. Meanwhile, as can be seen from Table 6, the one-to-many connection
of LCDB achieves the best balance among all metrics. We choose to use GCEM
to extract features and use LCDB in a one-to-many manner in LCAM as the
default setting.
Table 5. Ablation study of the blocks in LCAM.
Methods
IoU
Pd
Fa
LCAM w/o GCEM 64.67
91.84 24.82
LCAM w/o LCDB
65.21
89.45
6.91
LCAM
66.71 89.22
9.11
Table 6. Ablation study of the connection mode of LCDB in LCAM.
Modes
IoU
Pd
Fa
One-to-One
66.06
90.14 8.20
One-to-Many 66.71 89.22
9.11
5
Conclusion
In this paper, we propose a novel GLCANet to accurately locate small infrared
targets while preserving more shape details. Speciﬁcally, we devise two novel
modules, i.e., the global context extraction module and local context attention
module to better exploit contextual information, where the former uses Fourier
transform to extract global contextual information at an early stage and the
latter fuses multiple layers of features paying more attention to local contextual
information. Extensive experiments on public datasets verify the eﬀectiveness
and superiority of our GLCANet over SOTA methods.
Acknowledgment. This work was supported by the National Natural Science Foun-
dation of China under Grants (62171038, 62171042, and 62088101), and the R&D
Program of Beijing Municipal Education Commission (Grant No. KZ202211417048).
References
1. Hudson, R.D., Hudson, J.W.: The military applications of remote sensing by
infrared. Proc. IEEE 63(1), 104–128 (1975)

254
R. Liu et al.
2. Harney, R.C.: Military applications of coherent infrared radar. In: Society of Photo-
Optical Instrumentation Engineers on Physics and Technology of Coherent Infrared
Radar I (1982)
3. Huang, H., Yu, H., Xu, H., et al.: Near infrared spectroscopy for on/in-line mon-
itoring of quality in foods and beverages: a review. J. Food Eng. 87(3), 303–313
(2008)
4. Robinson, J.M.: Fire from space: global ﬁre evaluation using infrared remote sens-
ing. Int. J. Remote Sens. 12(1), 3–24 (1991)
5. Arrue, B.C., Ollero, A., De Dios, J.R.M.: An intelligent system for false alarm
reduction in infrared forest-ﬁre detection. IEEE Intell. Syst. Appl. 15(3), 64–73
(2000)
6. Jia-xiong, P., Wen-lin, Z.: Infrared background suppression for segmenting and
detecting small target. Acta Electron. Sin. 27(12), 47–51 (1999)
7. Azimi-Sadjadi, M.R., Pan, H.: Two-dimensional block diagonal LMS adaptive ﬁl-
tering. IEEE Trans. Signal Process. 42(9), 2420–2429 (1994)
8. Bai, X., Zhou, F.: Analysis of new top-hat transformation and the application for
infrared dim small target detection. Pattern Recogn. 43(6), 2145–2156 (2010)
9. Chen, C.L.P., Li, H., Wei, Y., et al.: A local contrast method for small infrared
target detection. IEEE Trans. Geosci. Remote Sens. 52(1), 574–581 (2013)
10. Deng, H., Sun, X., Liu, M., et al.: Infrared small-target detection using multiscale
gray diﬀerence weighted image entropy. IEEE Trans. Aerosp. Electron. Syst. 52(1),
60–72 (2016)
11. Gao, C., Meng, D., Yang, Y., et al.: Infrared patch-image model for small target
detection in a single image. IEEE Trans. Image Process. 22(12), 4996–5009 (2013)
12. Wang, X., Peng, Z., Kong, D., et al.: Infrared dim and small target detection based
on stable multisubspace learning in heterogeneous scene. IEEE Trans. Geosci.
Remote Sens. 55(10), 5481–5493 (2017)
13. Liu, M., Du, H., Zhao, Y., et al.: Image small target detection based on deep
learning with SNR controlled sample generation. Curr. Trends Comput. Sci. Mech.
Autom. 1, 211–220 (2017)
14. Wang, H., Zhou, L., Wang, L.: Miss detection vs. false alarm: adversarial learning
for small object segmentation in infrared images. In: Proceedings of the IEEE/CVF
on International Conference on Computer Vision, pp. 8508–8517 (2019)
15. Dai, Y., Wu, Y., Zhou, F., et al.: Asymmetric contextual modulation for infrared
small target detection. In: Proceedings of the IEEE/CVF on Winter Conference
on Applications of Computer Vision, pp. 950–959(2021)
16. Zhang, T., Li, L., Cao, S., et al.: Attention-guided pyramid context networks for
detecting infrared small target under complex background. IEEE Trans. Aerosp.
Electron. Syst. 59, 1–13 (2023)
17. Hong, Y., Wei, K., Chen, L., et al.: Crafting object detection in very low light. In:
Proceedings of the British Machine Vision Virtual Conference, pp. 3 (2021)
18. Wu, D., Cao, L., Zhou, P., et al.: Infrared small-target detection based on radiation
characteristics with a multimodal feature fusion network. Remote Sens. 14(15),
3570 (2022)
19. Ju, M., Luo, J., Liu, G., et al.: ISTDet: an eﬃcient end-to-end neural network for
infrared small target detection. Infrared Phys. Technol. 114, 103659 (2021)
20. Dai, Y., Wu, Y., Zhou, F., et al.: Attentional local contrast networks for infrared
small target detection. IEEE Trans. Geosci. Remote Sens. 59(11), 9813–9824
(2021)
21. Tong, X., Sun, B., Wei, J., et al.: EAAU-Net: enhanced asymmetric attention U-
Net for infrared small target detection. Remote Sens. 13(16), 3200 (2021)

GLCANet: Context Attention for Infrared Small Target Detection
255
22. Yu, C., Liu, Y., Wu, S., et al.: Pay attention to local contrast learning networks for
infrared small target detection. IEEE Geosci. Remote Sens. Lett. 19, 1–5 (2022)
23. Qi, M., Liu, L., Zhuang, S., et al.: FTC-Net: fusion of transformer and CNN features
for infrared small target detection. IEEE J. Sel. Top. Appl. Earth Observations
Remote Sens. 15, 8613–8623 (2022)
24. Lv, G., Dong, L., Liang, J., et al.: Novel asymmetric pyramid aggregation network
for infrared dim and small target detection. Remote Sens. 14(22), 5643 (2022)
25. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomedi-
cal image segmentation. In: International Conference on Medical Image Computing
and Computer-Assisted Intervention, pp. 234–241 (2015)
26. Chi, L., Jiang, B., Mu, Y.: Fast fourier convolution. Adv. Neural. Inf. Process.
Syst. 33, 4479–4488 (2020)
27. Woo, S., Park, J., Lee, J.Y., et al.: CBAM: convolutional block attention module.
In: Proceedings of the European Conference on Computer Vision, pp. 3–19 (2018)
28. Sunkara, R., Luo, T.: No more strided convolutions or pooling: a new CNN build-
ing block for low-resolution images and small objects. In: European Conference
on Machine Learning and Principles and Practice of Knowledge Discovery in
Databases, pp. 443–459 (2022)
29. Zhang, M., Zhang, R., Yang, Y., et al.: ISNET: shape matters for infrared small tar-
get detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 867–876 (2022)
30. Deshpande, S.D., Er, M.H., Venkateswarlu, R., et al.: Max-mean and max-median
ﬁlters for detection of small targets. In: Society of Photo-Optical Instrumentation
Engineers on Signal and Data Processing of Small Targets (1999)
31. Han, J., Moradi, S., Faramarzi, I., et al.: Infrared small target detection based
on the weighted strengthened local contrast measure. IEEE Geosci. Remote Sens.
Lett. 18(9), 1670–1674 (2020)
32. Han, J., Moradi, S., Faramarzi, I., et al.: A local contrast method for infrared
small-target detection utilizing a tri-layer window. IEEE Geosci. Remote Sens.
Lett. 17(10), 1822–1826 (2019)
33. Zhang, L., Peng, L., Zhang, T., et al.: Infrared small target detection via non-
convex rank approximation minimization joint l 2, 1 norm. Remote Sens. 10(11),
1821 (2018)
34. Dai, Y., Wu, Y.: Reweighted infrared patch-tensor model with both nonlocal and
local priors for single-frame small target detection. J. Sel. Top. Appl. Earth Obser-
vations Remote Sens. 10(8), 3752–3767 (2017)
35. Zhang, L., Peng, Z.: Infrared small target detection based on partial sum of the
tensor nuclear norm. Remote Sens. 11(4), 382 (2019)
36. Sun, Y., Yang, J., An, W.: Infrared dim and small target detection via multiple
subspace learning and spatial-temporal patch-tensor model. IEEE Trans. Geosci.
Remote Sens. 59(5), 3737–3752 (2020)

Fast Point Cloud Registration for Urban
Scenes via Pillar-Point Representation
Siyuan Gu and Ruqi Huang(B)
Tsinghua Shenzhen International Graduate School, Tsinghua -Berkeley Shenzhen
Institute, Shenzhen, China
ruqihuang@sz.tsinghua.edu.cn
Abstract. Eﬃcient and robust point cloud registration is an essential
task for real-time applications in urban scenes. Most methods introduce
keypoint sampling or detection to achieve real-time registration of large-
scale point clouds. Recent advances in keypoint-free methods have suc-
ceeded in alleviating the bias and error introduced by keypoint detection
via coarse-to-ﬁne dense matching strategies. Nevertheless, the running
time performance of such a strategy turns out to be far inferior to key-
point methods. This paper proposes a novel framework that adopts a
pillar-point representation based feature extraction pipeline and a three-
stage semi-dense keypoint matching scheme. The scheme includes global
coarse matching, anchor generation and local dense matching for eﬃcient
correspondence matching. Experiments on large-scale outdoor datasets,
including KITTI and NuScenes, demonstrate that the proposed feature
representation and matching framework achieve real-time inference and
high registration recall.
Keywords: Point cloud registration · Pillar-point representation ·
Semi-dense keypoint matching
1
Introduction
Point cloud registration plays a fundamental role in various applications includ-
ing indoor scene reconstruction, drone mapping, autonomous driving, to name
a few. In general, it aims to estimate the optimal rigid transformation between
a pair of unaligned point clouds. Early axiomatic approaches cast point regis-
tration as minimizing certain types of geometric residuals (e.g., Euclidean dis-
tance [21], or normal distribution [18]). On the other hand, recent trends in
learning-based methods [2,6,10,16] follow a generalized correspondence-based
workﬂow, which consists of feature extraction, keypoint sampling, correspon-
dence matching, outlier rejection, and pose estimation.
This work was supported in part by the National Natural Science Foundation of China
under contract No. 62171256, in part by Shenzhen Key Laboratory of next-generation
interactive media innovative technology (No. ZDSYS20210623092001004).
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 256–268, 2024.
https://doi.org/10.1007/978-981-99-8850-1_21

Registration via Pillar-Point
257
When dealing with large-scale point clouds, such as those generated by scan-
ning devices like LiDARs at a rate of hundreds of thousands of points per second,
most axiomatic methods and learning-based methods alleviate computational
burden by utilizing keypoint sampling. In particular, modern learning-based
methods typically obtain keypoints through either plain Furthest Point Sam-
pling (FPS) [16] or learned saliency [2]. However, keypoint-based methods are
not only inﬂuenced by the sampling scheme but also often rely on the assump-
tion that a suﬃcient number of repeatable keypoints across input point clouds
are available. Methods like [2,12] attempt to enhance keypoint repeatability by
employing deep learning-based keypoint detectors, but they still face challenges
associated with detection errors.
By contrast, the recent keypoint-free paradigm [19,27] proposes to perform
dense point matching in a coarse-to-ﬁne manner. This paradigm tackles the
challenge of dense matching in large-scale point clouds by breaking point clouds
down into local dense matching within multiple matched patches (i.e., super-
points). The keypoint-free approach has demonstrated excellent performance in
terms of accuracy and robustness. Despite the employment of hierarchical match-
ing strategies and the avoidance of time-consuming RANSAC pose estimators,
these keypoint-free methods have not yet achieved runtime eﬃciency compara-
ble to methods that utilize keypoint sampling or detection [7,16] in large-scale
outdoor scenes.
Motivated by the aforementioned observations, we propose a pipeline that
incorporates a novel matching scheme, which involves sampling anchor points
on only one side of input point cloud pairs, referred to as semi-dense keypoint
sampling. The corresponding candidates to anchor points are searched in fea-
ture space and the coarse matching stage can help to reduce the search space
to ensure robustness. Virtual corresponding points as weighted sum of candi-
dates are generated and used as centers of mini patches in local ﬁne matching.
Compared to keypoint-free methods, anchor correspondence generation narrows
the distance between mini patches, leading to a higher inlier ratio and smaller
neighborhood size.
Experimental results demonstrate that our pipeline achieves comparable
registration performance with the state-of-the-art methods, including both
keypoint-based and keypoint-free approaches. Additionally, our pipeline exhibits
excellent generalization performance when applied to unseen scenes. Further-
more, as a pipeline that eliminates the need for keypoint detection, we signif-
icantly improve the runtime eﬃciency by a large margin, typically performing
four times faster than the keypoint-free counterpart [19].
The main contributions of our work are listed as follows:
– An eﬃcient and robust registration network for large-scale urban point clouds
based on pillar-point representations.
– A three-stage correspondence matching scheme with semi-dense anchor cor-
respondence generation.
– Extensive experiments on large-scale urban datasets have proved the high
eﬃciency of our proposed network with comparable registration accuracy.

258
S. Gu and R. Huang
2
Related Works
Learned 3D Feature Descriptor. Early works in 3D feature learning most belong
to patch-based methods. [28] proposes the representative 3DMatch benchmark
and a neural network utilizing voxel-based Truncated Distance Function as local
patch descriptors. [1] proposes 3D cylindrical convolution layers to extract fea-
tures from spherically voxelized point cloud. [6] ﬁrst suggests to learn dense
descriptors (FCGF) from full voxelized point cloud via sparse 3D convolutions
and gains enormous speedup. [26] proposes KPConv which deﬁnes convolution
on precomputed kernel points to adapt to irregularity of point clouds. KPConv
is utilized by several registration methods including [2,19,27].
Works in object detection area also propose some generic represent learning
concepts. [14,23,25] construct feature extract modules with keypoint-to-voxel
set abstraction. This kind of modules ﬁrst query keypoint features via trilinear
interpolation from multi-scale voxel features or directly from independent point
MLP and then fuse all the features. [11] and related [22] divide scenes into
pillar structure with inﬁnite length in vertical direction and achieve signiﬁcant
breakthrough in inference speed.
Correspondence-based Registration Methods. Point cloud registration methods
which utilize a common correspondence step can be summarized into two main
categories, i.e., with or without keypoints. The former, such as [13,16], use uni-
form grid sampling or farthest point sampling to extract a few interest points
and construct a keypoint set for training. Similar to our work, [7] suggests a
pillar-based method to construct patches based on ﬁltered salient keypoints and
utilizes graph neural network to search correspondences. These methods reduce
storage for interest points from the beginning but are easily inﬂuenced by key-
point sampling strategies.
On the other hand, keypoint-free methods including [6,27], inspired by clas-
sical image matching method [20], perform hierarchical matching on point cloud.
They utilize patch overlaps to supervise superpoint matching and solve patch-
wise point matching with optimal transport techniques. Though they can achieve
more stable results than the keypoint-based counterparts, they suﬀer from eﬃ-
ciency issue when the input point clouds become larger.
3
Methodology
Given a pair of input clouds denoted as P ∈Rm×3 and Q ∈Rn×3 respectively, we
aim to estimate rigid rotation R ∈SO(3) and translation t ∈R3 that align P and
Q. As shown in Fig. 1, our pipeline ﬁrst learns a two-scale feature representation
with a shared encoder-decoder, then solves for point correspondences and ﬁnally
predicts rigid transformation.

Registration via Pillar-Point
259
3.1
Pillar-Point Based Feature Extractor
Pillar Feature Embedding.
Common scenes perceived from ground LiDAR
have such uniqueness that the variances of the roll and pitch components of
transformations are signiﬁcantly smaller than that of the yaw component, or
V ar(θz) ≫V ar(θx), V ar(θy) under Euler angle form. Therefore the pillar-based
feature embedding, previously utilized in [11], is introduced into our registration
pipeline for eﬃciency.
A pillar structure is organized by uniformly dividing the whole scene into reg-
ular grids with full height in z-axis. For each pillar with center xi
c = (xi
c, yi
c, zi
c),
point set {xi
j} and its arithmetic mean xi}, we calculate a 6-dimensional vec-
tor Fpt = {xi
j −xi
c, yi
j −yi
c, zi
j −zi
c, xi
j −xi, yi
j −yi, zi
j −zi} as the initial input
for every single point in the pillar. Then a two-layer shared MLP is applied to
encode the statistics into pillar features Fpl = MLP(Fpt). In the embedding pro-
cess, dynamic voxelization strategy from [30] is adopted in our pillar partition
step and hence no points are truncated.
Fig. 1. The proposed point cloud registration pipeline is mainly composed a shared
feature extractor, a hierarchical feature scheme and a pose estimator. The shared pillar-
point based feature extractor extracts coarse-scale feature (marked in red) in downsam-
pling and ﬁne-scale feature (marked in purple) in upsampling. The feature matching
scheme is composed of three stages, namely coarse matching, anchor generation and
ﬁne matching, which generate correspondences at diﬀerent levels. The details of pro-
posed feature matching scheme is stated in Sect. 3.2. Finally, a pose estimator based
on an iterative proposal and veriﬁcation scheme ﬁlters out outliers and outputs ﬁnal
transformation. (Color ﬁgure online)
Convolutional Backbone. We adopt a Residual U-Net backbone like that in [6],
which is built with sparse 2D convolution block to aggregate pillar features. The
backbone has two output features, coarse-level features Fc at the bottleneck of
backbone and ﬁne-level features Ff. It should be noticed that a proposed point
feature decoder is appended to last convolutional layer to revert tensors located
on 2D grids to point-wise features by interpolation.

260
S. Gu and R. Huang
Point Feature Decoder. In our case, a pillar contains multiple points, incon-
sistent to the output feature map Fbev of last convolutional layer. To con-
vert the feature map into point-wise features Ff suitable for ﬁne matching
among points, we incorporate the concept of set abstraction in [14] to further
increase the separability of points. The ﬁnal point feature Ff is calculated by
Ff = MLP(Concat[x, Fpl, Fbev]). The grid set abstraction procedure is illus-
trated in Fig. 2. It simultaneously reserves the scalability of voxel/pillar-based
methods and the compatibility with point-based methods.
3.2
Hierarchical Matching Scheme
Coarse Matching. Following [20], we adopt a transformer to learn patch features
at a coarse-level. The transformer structure consists of a sequence of self- and
cross- attention modules.
Fig. 2. Grid set abstraction based decoder. The middle results from convolution back-
bone are in the form of sparse 2D vectors. We convert the vectors into dense BEV maps
and retract point features via bilinear interpolation. The decoder also includes a stan-
dalone point branch for compensation. In our pipeline, the point branch is integrated
with MLP in pillar embedding layer.
We ﬁrst aggregate coarse features Fc for each point cloud with a geometric-
aware self-attention block. For attention mechanism, the query Q, key
K and value V by projecting input xi,xj with learnable weight matrixs
WQ, WK, WV ∈Rb×b. The self attention matrix ASA can be expressed as
ASA = Q(K + G)T /
√
b, where G = g · WG is projected geometric embedding.
The output feature F P
SA is calculated by F P
SA = ASA · V, where ASA is ﬁrst
normalized by softmax function as in [19]. The updated feature F Q
SA is calculated
by the same formula. The proposed geometric embedding g serves as positional
hints to enhance the self attention. For each point x, its neighborhood embedding
g is constructed by its nearest k neighbours {x1, ..., xk} .
g = MLP(Concat[∥xi −x∥, ∠(xi −x, x −x)/(2π)]).
(1)
The geometric embedding vector g is concatenated by Euclidean distance
between x and its neighbours and angles with neighbourhood center x as refer-
ence point.

Registration via Pillar-Point
261
The attention matrix of cross attention block is similarly computed as AP
CA =
QP(KQ)T /
√
b, ,where we use subscripts :P,:Q to distinguish variables from two
point clouds. The output cross attention feature F P
CA is calculated as F P
CA =
AP
CA · VQ, where attention matrix is also ﬁrst normalized by softmax function.
The attention matrix AQ
CA and cross attention feature F Q
CA is calculated in the
symmetric formula.
After performing inter- and intra- point cloud attention aggregation, we got
aggregated coarse-level feature F ′P
c
and F ′Q
c . The entries of similarity matrix
S′ ∈R|P|×|Q| is computed as dual Softmax function of patch features. We refer
readers to [19] for details. We select top Nc entries of similarity matrix S′ with
minimum values as conﬁdent sparse correspondences.
Anchor Correspondence Generation. Points in overlapping pillars tend to have
larger spatial distribution compared to points in the knn-grouped node stated
in previous works [19,27]. Therefore, we propose to sample anchor points inside
sparse correspondences and adopt virtual correspondence generation inspired
by HRegNet [16]. Figure 3 shows the scheme for generating a pair of anchors.
First for each selected coarse pair (P P, P Q) in the coarse matching stage, we
sample max to M0 anchor points Xq ⊂P Q per pair. Then for each anchor point
xq ∈Xq, we search for M1 nearest points in feature space as candidates.
Next, we incorporate original point feature Fp, feature similarity Fd and
structure similarity Fs to learn the attentive weights of candidate points. For
calculation of feature similarity Fd, we use common cosine similarity Fd =<
Fp, F i
p > /(|Fp| · |F i
p|). For calculation of structure similarity, we search for K
nearest neighbours of anchor points xi ∈Xq in sampled anchor points, denoted
as {xk}K
k=1 ⊂Xq. We calculate the distances between xi and xj
i and also dis-
tances between their candidate points xm
i , xn
k. The entry dimk of ﬁnal structure
similarity Fd is calculated by
dimk = min
n |∥xi −xk∥2 −∥xm
i −xn
k∥2|.
(2)
Finally, we concatenate Fp, Fd, Fs and input them into a three-layer shared-MLP
as HRegNet and SDMNet [15,16]. A maxpool and softmax layer is then applied
to get ﬁnal weights wik of candidate points. The ﬁnal virtual correspondence
x′
i ∈Xv is calculated as weighted sum of candidate points x′
i = 
k wikxk. In
addition, MLP and sigmoid function is applied to learned weights wik to get
conﬁdence score ci for each pair of (xi, x′
i). In testing stage, we utilize learned
conﬁdence score to ﬁlter anchor correspondences with conﬁdence score below a
predeﬁned threshold c0.
The output coordinates and conﬁdence score of anchor correspondence gen-
eration module is respectively regularized by classiﬁcation loss and probabilistic
distance.

262
S. Gu and R. Huang
Fig. 3. Scheme for anchor correspondence generation.
Fine Matching. At the ﬁne-scale level, we seek for learning point-wise correspon-
dence using optimal transport algorithm [20].
For each anchor correspondence (x, x′), we group knn neighbours of both
into mini patches and carry out local ﬁne matching. The initial batched sim-
ilarity matrix is constructed by calculation of unnormalized cosine similarity
s′
ij =< Fx,i, Fx′,j > /
√
b where Fx,i denotes feature of ith neighbour of x and
likewise for Fx′,j, b is the feature dimension length of Fx. The feature similar-
ity matrix is enhanced with an additional row and column serving as dustbins
as in [20] and denoted as ˜S′. Then Sinkhorn algorithm [24] is applied to solve
the extended assignment matrix ˜Z on ˜S′. The ﬁnal assignment matrix Z is
obtained by discarding the last row and column of ˜Z. Finally, same as coarse-
scale matcher, entries of S′
ij with top k maximum similarity are chosen as ﬁnal
point correspondences. We refer readers to [20] for more details.
3.3
Pose Estimator
Finally, we adopt parametric-free weighted SVD algorithm [3] as our pose esti-
mator in a iterative proposal and veriﬁcation scheme. We calculate a set of Ri, ti
for each mini-patch generated from conﬁdent anchor correspondences as N ′
c pro-
posals. For each iteration in pose veriﬁcation, we select Ri, ti with most inliers
whose residuals are less than a predeﬁned distance threshold d0 and use the inlier
points as input of next iteration. Experiments show that the pose estimator can
guarantee accuracy while outperforming RANSAC on eﬃciency.
3.4
Loss Function
For the supervision of coarse matching stage, an overlap-aware circle loss function
from [19] is utilized. Each pair of patch correspondence is seen as positive sample
only if point correspondences account for at least 10%. We denote patch from
point cloud P as P P and its positive and negative correspondence from point
cloud Q as P Q+ and P Q−. The overlap-aware circle loss is deﬁned as following:
LP
c =
1
|N|

P P∈N
log[1 +

P Q+
e
√
λβ+(d−Δ+)) ·

P Q−
eβ−(Δ−−d))],
(3)

Registration via Pillar-Point
263
where λ represents the overlap ratio, d is feature space distance. β+ = γ(d−Δ+)
and β−= γ(Δ+ −d) are weights for positive samples and negative samples
respectively. Loss LQ
c is calculated in the same way and ﬁnal loss is Lc = (LP
c +
LQ
c )/2.
For anchor generation stage, the coordinates (xq, xv) and conﬁdence c of gen-
erated anchor correspondences is supervised by distance loss and classiﬁcation
loss borrowed from [15].
La1 =

i
(ln (α −ci) +
d
α −ci
), d = ∥Rgtxq + tgt −xv∥2,
(4)
La2 = BCE(c, exp(−d/d0)), d = ∥Rgtxq + tgt −xv∥2.
(5)
Fine-level loss is a negative log-likelihood loss deﬁned on ground truth cor-
respondence set M with matching radius τ and unmatched point sets I,J from
two point clouds as in [19,20].
Lf = −

(i,j)∈M
log ˜Zij −

i∈I
log ˜Zi,N+1 −

j∈J
log ˜ZM+1,j,
(6)
where ˜Z ∈R(M+1)×(N+1).
The overall loss is L = λcLc + λa1La1 + λa2La2 + λfLf.
4
Experiments
4.1
Implementation Details
We implement our pipeline with a GTX 2080Ti GPU and a Intel Xeon Platinum
8255C @ 2.50 GHz CPU. We train our model for 15 epochs using an Adam opti-
mizer with an initial learning rate 1e −4 and an exponential decaying rate 0.9.
The batch size is set to 1 for all experiments. The width for pillar embedding is
set to 0.3m. In both training and testing phases, we select up to 128 coarse cor-
respondences but accept those only with overlap ratios > 0.1 for training phase.
We randomly sample 12 anchor points per patch and select 4 candidates for each
anchor points in generation of virtual correspondence. For data augmentation,
we apply random rotation within 4◦in pitch and roll and within 360◦in yaw,
and random translation within 0.5m and random scale within [0.95, 1.05].
We evaluate our pipeline and the baselines on two large-scale urban datasets:
KITTI Odometry dataset [9] and NuScenes dataset [4]. We follow the split set-
tings of [16] to organize point cloud pairs and reﬁne the point cloud pair poses
from original KITTI Odometry dataset with ICP registration.
4.2
Results on KITTI and NuScenes
We evaluate registration result with three metrics following [6,27]. The ﬁrst is
Relative Rotation Error (RRE), i.e., the geodesic distance between estimated

264
S. Gu and R. Huang
and ground truth rotation matrices ˆR and Rgt, which is computed as RRE =
arccos(Tr(R−1
gt ˆR −I)/2). The second is Relative Translation Error (RTE), the
Euclidean distance between estimated and ground truth translation ˆt and tgt,
which is RTE = ∥tgt −ˆt∥2. And the last is Registration Recall (RR), deﬁned as
the fraction of point cloud pairs with RRE and RTE below certain thresholds.
Following prior works [2,6,10,19,27], we set the threshold as RRE< 5◦and
RTE< 2m throughout.
Regarding baselines, we compare our methods with both axiomatic methods
FGR [29], RANSAC [8] and learning-based methods DGR [5], HRegNet
[16], and GeoTrans [19]. Part of results are borrowed from [16]. Apart from
the three metrics, we also report the mean inference time (in seconds) on the
test point cloud pairs. Note that DGR and HRegNet distinct from the rest as
they explicitly take ground truth transformations into loss function and regress
transformation.
Table 1. Registration results on KITTI and Nuscenes dataset.
Model
KITTI dataset
NuScenes dataset
RRE(◦)
RTE(m)
RR(%) T(s)
RRE(◦)
RTE(m)
RR(%) T(s)
FGR
0.96± 0.81
0.93± 0.59
39.43
0.506
1.01± 0.92
0.71± 0.62
32.24
0.285
RANSAC 0.54± 0.40
0.13± 0.70
91.90
0.550
0.74± 0.70
0.21± 0.19
60.86
0.268
DGR
0.37± 0.30
0.32± 0.32
98.71
1.497
0.48± 0.43
0.21± 0.18
98.4
0.523
HRegNet
0.18± 0.08 0.056± 0.075
99.77
0.106
0.27± 0.20 0.12± 0.11 100.0
0.087
GeoTrans 0.18± 0.13
0.063± 0.045
100.0
0.368
OOMa
Ours
0.26± 0.20
0.046± 0.034 100.0
0.092 0.31± 0.25
0.14± 0.13
100.0
0.082
a Out of memory during training phase.
Table 2. Generalized registration results on Apollo SouthBay Dataset.
Sequence
Model
RRE(◦)
RTE(cm)
RR(%) T(s)
SanJoseDowntown
HRegNet
0.15± 0.34
9.9± 22.2
90.0
0.108
GeoTrans 0.13± 0.20
5.5± 8.9
99.9
0.321
Ours
0.13± 0.15 3.2± 4.8
100.0
0.092
BaylandsToSeafood HRegNet
0.37± 0.67
29.6± 42.9 81.4
0.108
GeoTrans 0.11± 0.14 8.8± 10.2 93.6
0.293
Ours
0.17± 0.25
9.9± 13.9
93.3
0.092
Table 1 shows that though relative rotation error of our work is in mid-
stream, our method achieves comparable performance to state-of-art methods
in registration recall and relative translation error. Compared with the state-of-
art keypoint-free approach [19], our runtime is signiﬁcantly smaller. Moreover,
in the case of NuScenes dataset, we observe an out-of-memory breakdown in
implementing GeoTrans, while Ours deliver the best or second best results in
comparison. Compared with the state-of-art keypoint-free approach HRegNet,

Registration via Pillar-Point
265
the recall rate of the proposed method is higher on KITTI dataset and it is
proved in Sect. 4.3 that our performance downgrades less than HRegNet on
unseen datasets. The runtime of our method achieves best on both datasets.
4.3
Generalization to Apollo Southbay Dataset
We further test the generalization ability of our method and the baselines on the
’SanJoseDowntown’ and ’BaylandsToSeafood’ sequence from Apollo SouthBay
Dataset [17]. The test point cloud pairs are constructed in the same way as
NuScenes. For each method, we directly apply pretrained weight trained on
KITTI Odometry dataset.
Table 2 reports the generalization performance. When encountering unseen
dataset, HRegNet suﬀers a signiﬁcant drop in registration recall while
detection-free works like GeoTrans and ours still perform well.
4.4
Ablation Study
We conduct ablation experiments to study the eﬀectiveness of the key compo-
nent within our pipeline. Table 3 shows ablation result on diﬀerent modules. We
directly initialize the candidates points for anchors in the whole point cloud range
to ablate coarse matching. We test the setting similar to [19] where no anchor
points are required and dense points are randomly sampled inside patches to
ablate anchor generation. We ablate ﬁne matching by comparing the estimated
pose using ﬁltered anchor correspondences with original estimated poses. Results
show that our individual modules keep the our performance the best.
Table 3. Comparison of our modules on KITTI dataset.
Model
IR(%) RRE(◦) RTE(cm) RR(%) T(s)
ours
96.1
0.26
4.6
100
0.092
w/o coarse
86.7
1.18
23.6
73
0.101
w/o anchor 87.8
0.36
5.9
100
0.109
w/o ﬁne
92.4
0.31
8.2
100
0.081
Table 4. Comparison of diﬀerent sampling density on KITTI dataset.
Hyperparameters
PIR(%) IR(%) RRE(◦) RTE(cm) RR(%) T(s)
Nc = 256, Na = 12 97.7
95.5
0.246
4.2
100
0.103
Nc = 128, Na = 12 98.9
96.1
0.258
4.6
100
0.092
Nc = 64, Na = 12
99.2
96.5
0.284
5.1
100
0.086
Nc = 128, Na = 16 98.9
95.9
0.260
4.6
100
0.098
Nc = 128, Na = 8
98.9
96.2
0.259
4.6
100
0.084

266
S. Gu and R. Huang
The selection of hyperparameters, particularly selected sparse correspon-
dence numbers Nc and max anchor points per pair Na has inﬂuence on ﬁnal
result. We change Nc and Na to search for a balanced setting. Table 4 shows the
impact of sampling density. The accuracy increases with density while runtime
decreases.
We also ablate the choice of superpoint representation in feature extractor.
Table 5 shows that superpoint in pillar form achieves a little inferior performance
in exchange for 30% improvement in inference time.
Table 5. Comparison of diﬀerent representations on KITTI dataset.
Setting Length RRE(◦) RTE(cm) RR(%) T(s)
Model Size
Voxel
0.3m
0.244
4.3
100
0.131
11.2M
Pillar
0.3m
0.259
4.6
100
0.092
5.4M
Pillar
0.5m
0.291
5.1
100
0.086 5.4M
5
Conclusion
We have proposed an eﬃcient network for point cloud registration in the urban
scenes. By utilizing eﬃcient feature extraction modules and hierarchical match-
ing strategy, we make a step towards real-time robust deep learning based point
cloud registration. Experiments show that our pipeline, being compact and light,
achieves comparable inference speed and accuracy with state-of-art methods in
urban scenes.
In the future work, we will further look into works like [13] and study the
potential of our pipeline on multi-task learning. Our pipeline is suitable for more
universal scene understanding by integration with other downstream task such
as object detection and semantic segmentation.
References
1. Ao, S., Hu, Q., Yang, B., Markham, A., Guo, Y.: SpinNet: learning a general surface
descriptor for 3d point cloud registration. In: IEEE Conference on Computer Vision
and Pattern Recognition, pp. 11748–11757 (2021)
2. Bai, X., Luo, Z., Zhou, L., Fu, H., Quan, L., Tai, C.L.: D3Feat: joint learning
of dense detection and description of 3D local features. In: IEEE Conference on
Computer Vision and Pattern Recognition, pp. 6358–6366 (2020)
3. Besl, P.J., McKay, N.D.: A Method for registration of 3-D shapes. IEEE Trans.
Pattern Anal. Mach. Intell. 14, 239–256 (1992)
4. Caesar, H., et al.: NuScenes: a multimodal dataset for autonomous driving. In:
IEEE Conference on Computer Vision and Pattern Recognition, pp. 11618–11628
(2020)
5. Choy, C.B., Dong, W., Koltun, V.: Deep global registration. In: IEEE Conference
on Computer Vision and Pattern Recognition, pp. 2511–2520 (2020)

Registration via Pillar-Point
267
6. Choy, C.B., Park, J., Koltun, V.: Fully convolutional geometric features. In: IEEE
International Conference on Computer Vision, pp. 8957–8965 (2019)
7. Fischer, K., Simon, M., Olsner, F., Milz, S., Gross, H.M., Mader, P.: Stickypillars:
robust and eﬃcient feature matching on point clouds using graph neural networks.
In: IEEE Conference on Computer Vision and Pattern Recognition, pp. 313–323
(2021)
8. Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model
ﬁtting with applications to image analysis and automated cartography. Commun.
ACM 24(6), 381–395 (1981)
9. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the KITTI
vision benchmark suite. In: IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3354–3361 (2012)
10. Huang, S., Gojcic, Z., Usvyatsov, M.M., Wieser, A., Schindler, K.: PREDATOR:
registration of 3D point clouds with low overlap. In: IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4265–4274 (2021)
11. Lang, A.H., Vora, S., Caesar, H., Zhou, L., Yang, J., Beijbom, O.: PointPillars: fast
encoders for object detection from point clouds. In: IEEE Conference on Computer
Vision and Pattern Recognition, pp. 12689–12697 (2019)
12. Li, J., Lee, G.H.: USIP: unsupervised stable interest point detection from 3D point
clouds. In: IEEE International Conference on Computer Vision, pp. 361–370 (2019)
13. Liu, C.J., Guo, J., Yan, D., Liang, Z., Zhang, X., Cheng, Z.L.: SARNet: semantic
augmented registration of large-scale urban point clouds (2022), arXiv preprint
arXiv:2206.13117
14. Liu, Z., Tang, H., Lin, Y., Han, S.: Point-voxel CNN for eﬃcient 3D deep learning.
In: Advances in Neural Information Processing Systems, pp. 963–973 (2019)
15. Lu, F., et al.: Sparse-to-dense matching network for large-scale LiDAR point cloud
registration. IEEE Trans. Pattern Anal. Mach. Intell. 1–13 (2023)
16. Lu, F., et al.: HRegNet: a hierarchical network for large-scale outdoor LiDAR
point cloud registration. In: IEEE International Conference on Computer Vision,
pp. 15994–16003 (2021)
17. Lu, W., Zhou, Y., Wan, G., Hou, S., Song, S.: L3-net: towards learning based lidar
localization for autonomous driving. In: IEEE Conference on Computer Vision and
Pattern Recognition, pp. 6382–6391 (2019)
18. Magnusson, M., Lilienthal, A.J., Duckett, T.: Scan registration for autonomous
mining vehicles using 3D-NDT. J. Field Rob. 24(10), 803–827 (2007)
19. Qin, Z., Yu, H., Wang, C., Guo, Y., Peng, Y., Xu, K.: Geometric transformer for
fast and robust point cloud registration. In: IEEE Conference on Computer Vision
and Pattern Recognition, pp. 11133–11142 (2022)
20. Sarlin, P.E., DeTone, D., Malisiewicz, T., Rabinovich, A.: SuperGlue: learning
feature matching with graph neural networks. In: IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4937–4946 (2020)
21. Segal, A.V., H¨ahnel, D., Thrun, S.: Generalized-ICP. In: Robotics: Science and
Systems (2009)
22. Shi, G., Li, R., Ma, C.: PillarNet: real-time and high-performance pillar-based 3D
object detection. In: European Conference on Computer Vision. Springer, Heidel-
berg (2022). https://doi.org/10.1007/978-3-031-20080-9 3
23. Shi, S., et al.: PV-RCNN: point-voxel feature set abstraction for 3D object detec-
tion. In: IEEE Conference on Computer Vision and Pattern Recognition, pp.
10526–10535 (2020)
24. Sinkhorn, R., Knopp, P.: Concerning nonnegative matrices and doubly stochastic
matrices. Pac. J. Math. 21, 343–348 (1967)

268
S. Gu and R. Huang
25. Tang, H., Liu, Z., Zhao, S., Lin, Y., Lin, J., Wang, H., Han, S.: Searching eﬃ-
cient 3D architectures with sparse point-voxel convolution. In: Vedaldi, A., Bischof,
H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12373, pp. 685–702.
Springer, Cham (2020). https://doi.org/10.1007/978-3-030-58604-1 41
26. Thomas, H., Qi, C., Deschaud, J.E., Marcotegui, B., Goulette, F., Guibas, L.J.:
KPConv: ﬂexible and deformable convolution for point clouds. In: IEEE Interna-
tional Conference on Computer Vision, pp. 6410–6419 (2019)
27. Yu, H., Li, F., Saleh, M., Busam, B., Ilic, S.: CoFiNet: reliable coarse-to-ﬁne corre-
spondences for robust point cloud registration. In: Advances in Neural Information
Processing Systems, vol. 29, pp. 23872–23884 (2021)
28. Zeng, A., Song, S., Nießner, M., Fisher, M., Xiao, J., Funkhouser, T.A.: 3DMatch:
learning local geometric descriptors from RGB-D reconstructions. In: IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 199–208 (2017)
29. Zhou, Q.-Y., Park, J., Koltun, V.: Fast global registration. In: Leibe, B., Matas, J.,
Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 766–782. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-46475-6 47
30. Zhou, Y., et al.: End-to-End multi-view fusion for 3D object detection in LiDAR
point clouds. In: Conference on Robot Learning, vol. 100, pp. 923–932 (2019)

PMPI: Patch-Based Multiplane Images
for Real-Time Rendering of Neural
Radiance Fields
Xiaoguang Jiang1, You Yang1, Qiong Liu1(B), Changbiao Tao2, and Qun Liu2
1 Huazhong University of Science and Technology, Wuhan, China
q.liu@hust.edu.cn
2 ZTE Corporation, Shenzhen, China
Abstract. Neural radiance ﬁelds (NeRFs) have made it possible to
synthesize novel views in a photo-realistic manner. However, real-time
view synthesis with superior quality and low consuming remains a chal-
lenge due to the dense but uniform sampling of NeRFs. This paper pro-
poses Patch-based Multiplane Images (PMPIs) for real-time view syn-
thesis. PMPI is an adaptive combination of 3D patches, each encodes an
implicit 2D neural radiance ﬁeld. We then propose a method to learn our
PMPI. The structure of our PMPI is periodically updated during train-
ing. Patches of PMPI are thus assembled around visible contents. We
compare our method with six the state-of-the-art techniques, including
other plane-based methods. The proposed method achieves the highest
PSNR, SSIM and LPIPS scores and enables real-time over 50fps render-
ing. We also prove the adaptability of PMPI with an ablation study on
the number of sampling points.
Keywords: View synthesis · Scene representation · Multiplane
images · Neural radiance ﬁeld
1
Introduction
Novel view synthesis techniques are crucial to a variety of fascinating applica-
tions, including virtual reality and metaverse. Recently, one representative work
by Mildenhall et al. [1] learned a neural radiance ﬁeld (NeRF) from input images.
NeRF encodes the geometry and appearance of the entire space of a scene. NeRF
dramatically outperforms previous work and makes photo-realistic results pos-
sible. Despite the signiﬁcant progress in rendering quality, NeRF in practice
still shows artifacts when it comes to extremely complex geometry and detailed
textures.
To improve the rendering quality, subsequent works tried to use explicit mod-
els as guidance for computing implicit representations. Explicit 3D geometric
models are generally used, such as voxel grids [6,8,9], point clouds [2,10,11] and
meshes [5,12,13]. Explicit plane-based models are also used to extend NeRF for
detailed results and real-time rendering. Instead of slow volume rendering used
by original NeRF, plane-based model is naturally suitable for real-time rendering
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 269–280, 2024.
https://doi.org/10.1007/978-981-99-8850-1_22

270
X. Jiang et al.
via fast warping and alpha compositing. One typical plane-based model is mul-
tiplane images (MPIs) [14–18]. For example, NeX [15] is a plane-based NeRF. It
represents a scene with an MPI and then learns a NeRF which is only ﬁne-tuned
in the planes. Although the plane-based NeRFs [7,15] are eﬃcient in learning
and rendering, the plane-based models suﬀer from ineﬃcient modeling of scenes.
For NeX, repeated texture artifacts are frequently seen when it comes to detailed
textures. The planes in an MPI are ﬁxed and placed equidistantly in the depth
space or inverse depth space. As a result, many pixels of an MPI are placed in
empty space and have no contribution to rendering results. The ineﬃcient sam-
pling of MPIs leads to repeated texture artifacts [18]. Real-time view synthesis
with superior quality and low consuming remains a challenge.
To solve this challenge, we propose a new representation model, Patch-
based Multiplane Images (PMPIs) for real-time novel view synthesis. Our PMPI
describes a collection of fronto-parallel patches. The direction of each patch is
ﬁxed, while the position of each patch in our PMPI changes for better surface ﬁt-
ting. In this way, more dense and eﬀective samplings are produced without using
more computational resources by assembling patches around visible contents.
The collection of patches is semi-structured. To learn a PMPI from images for
each scene, we have addressed three problems. First, we propose an end-to-end
method. Unlike previous approaches which got 3D geometry priors in advance
[2–4,7], we extract coarse geometry during training because extra geometry pri-
ors are often too noisy and hard to obtain. Second, we take the extracted coarse
geometry as a guidance and propose a strategy to optimize the structure of our
PMPI via feedback updating. The feedback style ensures the convergence of the
position of each patch. Third, we employ a simpliﬁed rendering process for PMPI
to speed up the training process and enable real-time rendering.
The contributions of this work can be concluded as follows:
– We propose Patch-based Multiplane Images (PMPIs), a new 3D scene rep-
resentation with a ﬂexible structure to depict 3D surfaces and appearance
eﬃciently.
– To make full use of the ﬂexibility of our PMPI for diﬀerent scenes, we pro-
pose a end-to-end method to optimize the structure and the values of the
proposed PMPI simultaneously. The experimental results show that a PMPI
with a ﬂexible structure can eﬀectively reduce repeated texture artifacts and
recover more complete geometry. Extensive ablation experiments validate the
eﬃciency of the proposed PMPI.
2
Related Work
2.1
Neural Implicit Representations for View Synthesis
Mildenhall et al. [1] proposed neural radiance ﬁelds (NeRFs) for view synthe-
sis. NeRF is implicit and continuous, parameterized as a multi-layer perceptron
(MLP). This method represents a scene as a 5D radiance ﬁeld and directly
regresses the volume density and RGB values. NeRF was then enhanced in two

PMPI
271
Fig. 1. Illustration of our PMPI in normalized device coordinate (NDC) space. For each
target scene given (a), we ﬁrst initialize an MPI for scene representing (b). We take a
step forward to split each plane of the MPI into patches whose depths are variable (c).
The patches in each grid are then updated and gathered around the visible content
(d).
major areas: greater visual quality and faster rendering. Some works improve
the visual quality by using additional 3D supervision or sampling more densely
around surfaces with the help of geometry information. For example, depth-
supervised NeRF [3] learned a NeRF under not only color supervision, but also
depth supervision from a corresponding sparse point cloud. NSVF [8] and Point-
nerf [2] use a voxel grid and a point cloud respectively, which are combined
with local implicit features to preserve ﬁne details. The above depth-supervised
methods require very dense sampling points to show sharp results with detailed
thin structures, which result in extremely high computational complexity. NeX
[15] learns an enhancement of multiplane images(MPIs), which can be rendered
eﬃciently through simple warping and alpha compositing.
2.2
Representations of Multiplane Images
Multiplane images (MPIs) is a scene representation that consists of a set of
fronto-parallel planes at a ﬁxed range of depths with respect to a reference view.
Each sampling point in an MPI contains RGB and alpha values representing
the scene appearance and geometry respectively. Compared to neural implicit
representations, an MPI can be rendered in real time via planar warping and
alpha compositing.
Original MPI [14] is proposed for view synthesis from small-baseline stereo
and learned with a convolutional neural network (CNN). Mildenhall et al. [19]
learned an MPI for each input observation via a CNN. Novel views are then
rendered by blending adjacent MPIs. Flynn et al. [16] produced MPIs using
learned gradient descent. One recent work based on MPI is NeX [15] by Sutti-
sak Wizadwongsa et al. This method enables view-dependent eﬀects modeling
by parameterizing the RGB values as a function of the viewing direction and
approximates this function with a linear combination of learnable basis functions.
NeX views an MPI as discrete samplings of implicit, volumetric representation
like NeRF and directly regresses the values of each sampling through a implicit
modeling framework.

272
X. Jiang et al.
3
Representation of Patch-Based Multiplane Images
To solve the issue caused by ineﬃcient sampling of MPIs, we propose a new
scene representation, called patch-based multiplane image (PMPI). Our PMPI
contains a set of fronto-parallel patches in a reference frustum, as shown in (d) of
Fig. 1. Similar to the planes of an MPI, each patch of our PMPI contains an RGB
image and an alpha map. At the position of the corresponding pixel, the RGB
image and alpha map encode the texture and transparency respectively. Figure 1
illustrates an example of generating a PMPI. Given the images of a scene and a
reference view of our PMPI, we construct a PMPI from an initialized MPI via
splitting and rearrangement. We ﬁrst place a set of planes to initialize an MPI
by placing fronto-parallel planes in the reference viewing frustum. Then we split
every plane into patches to obtain an initialized PMPI. As shown in Fig. 1, each
patch is a rectangle part of the original plane. The splitting of planes can also be
viewed as splitting the reference viewing frustum into small grids. The patches
in each grid form an minor MPI. Note that the depth range of the minor MPI
in each grid can change in accordance with visible scene contents.
Speciﬁcally, assume that an initialized MPI M has NM planes. The dimension
of each plane is HM × WM × 4, where the last dimension contains the RGB and
alpha values. We then split every plane into patches, as shown in Fig. 1. Each
patch is a rectangular part of the original plane and the dimension of each patch
is HP × WP × 4. Thus the number of patches of our PMPI P is:
NP = NM × HM × WM
HP × WP
.
(1)
Let ci ∈RHP×WP×3 and αi ∈RHP×WP denote the RGB image and alpha
map respectively of the ith patch. Let (ui, vi, di) describe the position of the
ith patch, where (ui, vi) is the reference image coordinates of the center of the
ith patch and di is the depth of the ith patch from the reference camera. Note
that (ui, vi) is ﬁxed while di changes in accordance with the scene’s content. A
PMPI can thus be described as a collection of RGBA patches {P1, P2, ..., PNP},
where Pi = (ci, αi, ui, vi, di) denotes the ith patch and NP is the total number of
patches. In relation to MPI, the patches in each grid of our PMPI can be thought
of as a minor MPI with dynamic depth range, or an MPI can be thought of as
a special case of a PMPI.
4
Training of Adaptive PMPI
4.1
Initialization of Patches
As illustrated in Fig. 2, initialization takes a given reference view and the spec-
iﬁed resolution of patches HP × WP as input. This process ends up with an
initialized PMPI as the scene representation. Figure 1 illustrates this process in
detail. We ﬁrst initialize the structure of an MPI by placing planes in the ref-
erence viewing frustum. Then the resolution of patches HP × WP is taken as

PMPI
273
Fig. 2. An overview of our end-to-end method. (a) We initialize a PMPI by splitting
each plane of an MPI into NP patches. Each patch Pi = (ci, αi, ui, vi, di) contains an
RGB image ci and an alpha map αi. The position of patch Pi is denoted as ui, vi, di. (b)
In Learning process, we calculate the loss between rendered image and ground truth.
Then the weights of neural networks and base color b are learned via backpropagation.
(c) We update the structure of a PMPI periodically.
Fig. 3. An Illustration of Learning PMPI. We obtain alpha value αi(x, y) and RGB
color ci(x, y, v) of each pixel Pi(x, y) via two MLPs and explicitly stored base color.
“PosEn” indicates positional encoding [1];
a guidance for splitting the planes. Each plane in an MPI is split into patches,
each with a resolution of HP × WP × 4. Each patch Pi contains a RGB color
image ci ∈RHP×WP×3 and an alpha map αi ∈RHP×WP.
4.2
Learning Appearance and Geometry
In the learning process of a PMPI, the alpha value and RGB color vector of each
pixel are learned through multilayer perceptrons(MLPs). Speciﬁcally, we follow
the learning process used by NeX [15]. Let Pi(x, y) denote a pixel in patch Pi
with a patch coordinate (x, y). To allow for view-dependent modeling, we also
parameterize the color of each pixel Pi(x, y) in a PMPI as a function of position
and viewing direction v = (vx, vy, 1) in the camera coordinate system [15]. The

274
X. Jiang et al.
function is approximated with a base color vector b ∈R3 and a linear combina-
tion of learnable basis functions {Hn(v) : R3 →R} over the spherical domain
described by vector v:
ci(x, y, v) = bPi(x,y) +
N

n=1
kPi(x,y)
n
Hn(v)
(2)
where ci(x, y, v) ∈R3 is a vector of RGB color values in pixel Pi(x, y) over
the viewing direction v, bPi(x,y) ∈R3 is a vector of base color values in pixel
Pi(x, y), kPi(x,y)
n
∈R3 is a vector of the nth RGB coeﬃcients of N global basis
functions in pixel Pi(x, y), Hn(v) ∈R is the nth learnable basis function under
viewing direction v and N is the number of basis functions. In this way, the
color in each pixel varies when changing viewing directions.
For pixel Pi(x, y), base color bPi(x,y) is stored explicitly while RGB coeﬃ-
cients kPi(x,y)
n
, alpha value αi(x, y) and basis functions Hn(v) are learned using
two MLPs:
Fθ : (Pi(x, y)) →(αi(x, y), (kPi(x,y)
1
, kPi(x,y)
2
, ..., kPi(x,y)
N
))
(3)
Gφ : (v) →(H1(v), H2(v), ..., HN(v))
(4)
where Pi(x, y) is a pixel in the patch coordinate system and v is the normalized
viewing direction. The process of learning PMPI is illustrated in Fig. 3.
To optimize the base color and the weights of MLPs, we render our PMPI
into target views and compute loss for backpropagation. Speciﬁcally, we employ
a variant of Axis Aligned Bounding Box intersection test [20] instead of planar
warping to ﬁnd out the accurate intersections of rays with the patches. Then
the color and alpha values (RGBA) in intersecions are evaluated using bilinear
interpolation and inference of MLPs. The color of target pixel is calculated by
compositing the color and alpha values of intersections:
ˆIt =
NP

i=1

ciαi
NP

j=i+1
(1 −αj)

(5)
where NP is the number of patches of a PMPI, ci ∈RHP×WP×3 and αi ∈
RHP×WP denote the color image and alpha map in the ith patch Pi, respectively.
After rendering, we then calculate the loss between rendered results and
ground truth. The weights of the two MLPs and base colors k are optimized by
minimizing the following loss function:
L =
ˆIt −It
2 + ω
∇ˆIt −∇It

1 + γTV(b)
(6)
where ˆIt, It are the rendered image and the ground-truth image respectively, ∇
denotes the gradient operator, TV(b) denotes the total variation loss of base
color images, ω and γ are balancing weights.

PMPI
275
Fig. 4. An illustration of the ﬁrst structure updating of a PMPI in the vertical view
of normalized device coordinates. (a) An initialized PMPI with patches (black). (b)
Patches are determined to be preserved (black) or pruned (gray). (c) Updated depth
ranges. (d) Updated PMPI with rearranged patches. (Color ﬁgure online)
4.3
Updating Structure of PMPI
In order to gather the sampling points around the space with relevant scene con-
tents, the depth of each patch needs to be optimized during training. However,
gradient-based optimization procedure is computationally expensive. Besides, it
is hard to ﬁnd a proper loss function which is convex over the depth of each
patch.
Inspired by neural sparse voxel ﬁelds [8], we update the structure of PMPI
periodically during training based on learned coarse geometry. Instead of 3D
priors or implicit alpha values, we reason coarse geometry from the explicitly
stored base color k. We ﬁnd that, in practice, the normalized base color values are
optimized extremely close to B = (0.5, 0.5, 0.5) for the sampling points which are
invisible in all observations. Therefore, we deﬁne the ith patch Pi to be invisible
under the condition as follows:
if
max
0≤x<WP
0≤y<HP
bPi(x,y) −B
2 < δ
(7)
where bPi(x,y) ∈R3 is a vector of base color values in pixel Pi(x, y) and δ is a
threshold (δ = 6 × 10−4 in all our experiments).
The above formula enables us to identify the invisible patches eﬃciently. After
a certain period of training, a patch is pruned if both itself and its neighbors
are invisible. Then the ranges of depths are updated according to the depths
of the nearest and farthest patches. Depths of all the patches in each grid are
then reset equidistantly in inverse depth space, according to the updated range
of depths. The operation is illustrated in Fig. 4.
5
Real-Time Rendering with Customized CUDA Kernel
After training, we inference MLPs and compute the color and alpha values on all
pixels of PMPI. Then the color images and alpha maps are stored for real-time
rendering. Explicitly stored images can be fast rendered. To further acceler-
ate the rendering process, we implement a custom CUDA kernel for ray-plane

276
X. Jiang et al.
intersection and alpha composition. We achieve real-time rendering of captured
scenes.
6
Experiments
We evaluate our method on Real Forward-Facing dataset. Our method is com-
pared to a bunch of state-of-the-art methods. In addition, we conduct an ablation
study to demonstrate the advantage of PMPI over MPI in a more direct way.
To validate the eﬃciency of PMPI, we render the a baked PMPI model using an
NVIDIA RTX 3090. We reduce the number of planes NM in the initialized MPI
and compare our method to NeX [15].
Fig. 5. Qualitative comparisons between Ours, NeRF [1], Point-nerf [2], NeX [15],
LLFF [19], NeurMips [7] and MobileNeRF [5].
Table 1. Quantitative comparisons between our method, NeRF [1], Point-nerf [2], NeX
[15], LLFF [19], NeurMips [7] and MobileNeRF [5]. All scores are averaged across the
test images of Real Forward-Facing dataset. Bold numbers represent the best scores.
The results shows that our model is rendered in real-time.
Ours
NeRF Point-nerf NeX
LLFF NeurMips MobileNeRF
PSNR↑
27.33 26.76
20.61
27.26 24.41
21.52
25.91
SSIM↑
0.905 0.883
0.738
0.904 0.863
0.718
0.825
LPIPS↓
0.174 0.246
0.358
0.178 0.212
0.291
0.183
Real-time 







PMPI
277
6.1
Training Details of Our Method
The number of planes NM is set to 192. Each plane contains a RGB image and
an alpha map with a resolution of 1440 × 1188 pixels. The planes are then split
into patches, each with a resolution of 36×36. In order to model view-dependent
eﬀects, the RGB values in each pixel of a PMPI are parameterized as a function of
the viewing direction, as shown in Formula 2, where N is set to 8. The weights
of the two MLPs and base color k are optimized using Adam optimizer [21]
by minimizing the loss function as shown in Formula 6. To compute the loss,
we randomly sample and render 8000 pixels in the training view and compare
them to the corresponding pixels in the ground truth. Same as NeX [15], we set
ω = 0.05, γ = 0.03 and train the networks and base color for 4000 epochs. The
learning rates of base color and networks are set to 0.01 and 0.001, respectively.
The learning rates decrease with a decay factor of 0.1 every 1333 epochs. We
update the structure of PMPI at 1k, 1.5k, 2k and 2.5k epochs.
6.2
Comparisons
Quantitative Evaluation. To show the superior quality of our method, we
evaluate our method and the state-of-the-art methods using the common PSNR,
SSIM [22] and LPIPS [23] metrics. We report the quantitative results in Table 1.
The results show that our method achieves the highest average scores across all
metrics. By the way, we implement a rendering software for our PMPI model.
We achieve real-time over 50fps rendering using an NVIDIA RTX 3090 at this
high quality.
Qualitative Evaluation. The qualitative results are presented in Fig. 5. Our
results have sharper boundaries and more details than the other methods, espe-
cially in the backgrounds. For the ﬁrst row in Fig. 5, our method recovers greater
texture of the petal. For the second and third row, our method preserves sharp
boundaries while other methods show repeated texture or much noise.
Table 2. Quantitative evaluation of our method and NeX [15] on diﬀerent numbers of
planes. All scores are averaged across the test images of Real Forward-Facing dataset.
Bold numbers represent the best scores. NM is the number of planes in the initialized
MPI.
NM=192
NM=128
NM=64
Ours
NeX
Ours
NeX
Ours
NeX
PSNR↑
27.33
27.26 27.03
26.94 26.57
26.11
SSIM↑
0.905
0.904 0.902
0.899 0.890
0.880
LPIPS↓0.174
0.178 0.180
0.184 0.202
0.225

278
X. Jiang et al.
Fig. 6. Qualitative comparisons between our method and NeX [15] while varying the
number of planes. Our results stay at high quality but NeX shows much more blurriness
and broken boundaries when the number of planes decreases, such as on the pattern
on the skull of a dinosaur and on the boundaries of the chair in the second column.
6.3
Ablation Study
To demonstrate the advantage of PMPI over MPI in a more direct way, we
compare our method to NeX [15] and set the number of planes NM in the
initialized MPI to 192, 128 and 64. The MPI used by NeX shares the same
number and resolution of planes as our initialized MPI. In this way, our method
and NeX share the same number of sampling points. Quantitative results are
presented in Table 2 and qualitative comparisons are illustrated in Fig. 6.
The results prove that updating structures improves the performance of
PMPI greatly. When the number of plane decreases, NeX has a much lower
PSNR and produces increasingly blurriness and broken boundaries while our
results stay at high quality. For example, our method preserves straight bound-
aries while NeX shows bent boundaries, as shown in Fig. 6. By gathering the
patches around visible contents in scenes, our method reduces the negative eﬀects
of decreasing planes.
7
Conclusions
We propose a new scene representation, patch-based multiplane images (PMPIs)
for novel view synthesis. Based on the proposed model, we construct a method for
fast and photo-realistic rendering. Our PMPIs capture and produce detailed tex-
tures with less repeated texture artifacts and allow real-time rendering. We com-
pare our method to other state-of-the-art methods in real scenes and our method
outperforms other state-of-the-art methods in both qualitative and quantitative
metrics. Besides, we conduct extensive studies on the eﬀects of varying numbers
of planes. Our PMPIs remain eﬀective when the number of planes decreases.

PMPI
279
References
1. Mildenhall, B., Srinivasan, P., Tancik, M., Barron, J., Ramamoorthi, R., Ng, R.:
Nerf: representing scenes as neural radiance ﬁelds for view synthesis. In: European
Conference on Computer Vision (2020)
2. Xu, Q., et al.: Point-nerf: Point-based neural radiance ﬁelds. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5438–
5448 (2022)
3. Deng, K., Liu, A., Zhu, J.-Y., Ramanan, D.: Depth-supervised nerf: fewer views and
faster training for free. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 12882–12891 (2022)
4. Roessle, B., Barron, J.T., Mildenhall, B., Srinivasan, P.P., Nießner, M.: Dense
depth priors for neural radiance ﬁelds from sparse input views. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
12892–12901 (2022)
5. Chen, Z., Funkhouser, T., Hedman, P., Tagliasacchi, A.: Mobilenerf: exploiting
the polygon rasterization pipeline for eﬃcient neural ﬁeld rendering on mobile
architectures. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 16569–16578 (2023)
6. Hedman, P., Srinivasan, P.P., Mildenhall, B., Barron, J.T., Debevec, P.: Bak-
ing neural radiance ﬁelds for real-time view synthesis. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 5875–5884 (2021)
7. Lin, Z.-H., Ma, W.-C., Hsu, H.-Y., Wang, Y.-C.F., Wang, S.: Neurmips: neural
mixture of planar experts for view synthesis. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 15702–15712 (2022)
8. Liu, L., Gu, J., Zaw Lin, K., Chua, T.-S., Theobalt, C.: Neural sparse voxel ﬁelds.
In: Advances in Neural Information Processing Systems, vol. 33, pp. 15651–15663
(2020)
9. Fridovich-Keil, S., Yu, A., Tancik, M., Chen, Q., Recht, B., Kanazawa, A.: Plenox-
els: radiance ﬁelds without neural networks. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 5501–5510 (2022)
10. Aliev, K.-A., Ulyanov, D., Lempitsky, V.S.: Neural point-based graphics. In: ECCV
(2020)
11. R¨uckert, D., Franke, L., Stamminger, M.: ADOP: approximate diﬀerentiable one-
pixel point rendering. ACM Trans. Graph. 41(4), 1–14 (2022)
12. Riegler, G., Koltun, V.: Free view synthesis. In: Vedaldi, A., Bischof, H., Brox, T.,
Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12364, pp. 623–640. Springer, Cham
(2020). https://doi.org/10.1007/978-3-030-58529-7 37
13. Riegler, G., Koltun, V.: Stable view synthesis. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 12216–12225 (2021)
14. Zhou, T., Tucker, R., Flynn, J., Fyﬀe, G., Snavely, N.: Stereo magniﬁcation: learn-
ing view synthesis using multiplane images. ACM Trans. Graph. 37(4), 1–12 (2018)
15. Wizadwongsa, S., Phongthawee, P., Yenphraphai, J., Suwajanakorn, S.: Nex: real-
time view synthesis with neural basis expansion. In: 2021 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 8530–8539 (2021)
16. Flynn, J., et al.: DeepView: view synthesis with learned gradient descent. In: 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 2362–2371 (2019)
17. Tucker, R., Snavely, N.: Single-view view synthesis with multiplane images. In: 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.
548–557 (2020)

280
X. Jiang et al.
18. Srinivasan, P.P., Tucker, R., Barron, J.T., Ramamoorthi, R., Ng, R., Snavely, N.:
Pushing the boundaries of view extrapolation with multiplane images. In: 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 175–184 (2019)
19. Mildenhall, B., et al.: Local light ﬁeld fusion: practical view synthesis with pre-
scriptive sampling guidelines. ACM Trans. Graph. 38(4), 1–14 (2019)
20. Haines, E.: Essential ray tracing. Glas 89, 33–77 (1989)
21. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. CoRR, vol.
abs/1412.6980 (2015)
22. Wang, Z.: Image quality assessment: from error visibility to structural similarity.
IEEE Trans. Image Process. 13(4), 600–612 (2004)
23. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
eﬀectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 586–595 (2018)

EFPNet: Eﬀective Fusion Pyramid
Network for Tiny Person Detection
in UAV Images
Ruichen Zhang, Qiong Liu(B), and Kejun Wu
School of Electronic Information and Communications,
Huazhong University of Science and Technology, Wuhan, China
q.liu@hust.edu.cn
Abstract. Unmanned Aerial Vehicles (UAVs) have found extensive
applications in the ﬁeld of rescue and navigation scenarios. The objects in
UAV images are generally with small sizes, which rises a serious challenge
of object detection. Most existing methods address this issue by con-
structing multi-scale feature pyramids to integrate deep semantic infor-
mation with shallow layer, but these networks fail to eﬀectively extract
and learn features of tiny objects in the shallow layer. In this paper, we
propose an Eﬀective Fusion Pyramid Network (EFPNet) for tiny per-
son detection in UAV images. EFPNet consists of a Multi-Dimensional
Attention Module (MDAM) and an Eﬀective Feature Fusion Module
(EFFM). The MDAM learns the weighted combination of features in
both channel and spatial dimensions, which generates attention maps. It
enriches semantic information in features. The EFFM utilizes the infor-
mation from attention maps of diﬀerent layers, which guides feature
fusion between adjacent layers. It maintains consistency between deep
and shallow features. Our proposed model achieves an Average Preci-
sion (AP) of 60.72% on the TinyPerson dataset, which demonstrate our
model outperforms other state-of-the-art detectors.
Keywords: Tiny object detection · Self-attention · Feature pyramid
network · Unmanned aerial vehicles
1
Introduction
Due to their exceptional mobility, large ﬁeld-of-view, and cost-eﬀectiveness,
unmanned aerial vehicles (UAVs) have found widespread application in various
tasks such as environmental monitoring [25] and maritime rescue [3,28]. UAVs
are typically deployed at a distance from the objects of interest, enabling them to
capture images with a large ﬁeld-of-view. Consequently, these images encompass
targets exhibiting signiﬁcant scale variations, including a substantial number of
tiny objects(less than 20 pixels), thus tiny objects often suﬀer from low signal-
to-noise ratios, making it challenging to detect them in the background [27].
As a result, the performance of most detectors in detecting such tiny objects is
unsatisfactory, the detection precision of tiny objects in many public datasets is
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 281–292, 2024.
https://doi.org/10.1007/978-981-99-8850-1_23

282
R. Zhang et al.
less than half that of larger objects [20]. Hence, there exists signiﬁcant scope for
improvement in the ﬁeld of tiny object detection.
Generally, detectors continuously perform downsampling and pooling opera-
tions on input images. In this process, the features of tiny objects gradually get
overwhelmed and drown in the background [2], resulting in the detector’s inabil-
ity to detect tiny objects eﬀectively. To solve the issue of feature disappearance,
some methods generate additional small targets in the dataset [5,13] to achieve a
better balance between positive and negative samples. However, the performance
of these methods varies signiﬁcantly on diﬀerent datasets, indicating limited gen-
eralization capability. Most existing detectors utilize Feature Pyramid Network
[16] (FPN) to combine deep and shallow features, though it allows the shallow
features to acquire richer semantic information for performance enhancement,
it fails to fully address the semantic conﬂict arising from the disappearance of
tiny object features. As the feature information of tiny objects gradually gets
submerged within the background information during forward propagation, the
majority of anchor points in deep layers are mistakenly assigned as negative sam-
ples. This results in semantic conﬂict between deep and shallow features, leading
to inconsistent gradient computation within the network [19]. Consequently, deep
features do not enrich the semantics of shallow features but instead have a neg-
ative impact, ultimately causing the detector to fail in detecting tiny objects.
Guo et al. [9] proposed a consistency supervision method to reduce the semantic
gap between diﬀerent layers, Gong et al. [8] proposed a statistical-based feature
fusion method to alleviate the negative inﬂuence of deep features on shallow
features. Nevertheless, these methods are limited in improving precision when
detecting tiny objects due to the noisier semantic information and inadequate
mitigation of semantic conﬂict during feature fusion, resulting in minimal accu-
racy improvement.
In this paper, we propose an Eﬀective Fusion Pyramid Network (EFPNet)
to improve the detection precision of tiny person. To address the issue of tiny
object features being lost during downsampling and pooling processes in the net-
work, we propose a Multi-Dimensional Attention Module (MDAM). By learning
the weighted combination of features in both channel and spatial dimensions,
MDAM generates attention maps that carry rich semantic information. By uti-
lizing the self-attention mechanism and maintaining a higher internal resolution
during attention map generation, MDAM eﬀectively strengthens the semantic
information of tiny objects across diﬀerent layers, preventing them from being
lost in background noise. To resolve the semantic conﬂict and inconsistent gradi-
ent computation in FPN, we propose a Eﬀective Feature Fusion Module (EFFM).
EFFM guides the fusion of deep and shallow features by statistically analyzing
the number of responsive pixels and connected regions in the attention maps
from diﬀerent layers. It utilizes the eﬀective information from deep layers while
suppressing conﬂicting information from shallow features. This method eﬀec-
tively prevents semantic conﬂicts and enables the network to learn tiny features
accurately. The experimental results demonstrate that our method signiﬁcantly
improves the accuracy of tiny person detection.

Eﬀective Fusion Pyramid Network for Tiny Person Detection in UAV Images
283
Our contribution can be summarized as follows:
• We propose EFPNet for tiny person detection in UAV images. The proposed
Multi-Dimensional Attention Module (MDAM) utilizes both channel and spa-
tial information and maintain a high internal resolution to avoid semantic
information loss. Eﬀective Feature Fusion Module (EFFM) guidies feature
fusion in the FPN using pixel-level information to mitigate semantic conﬂicts
and relieve the inconsistency in gradient computation.
• The proposed EFPNet signiﬁcantly improves the performance of the detector
due to the introduction of the proposed MDAM and EFFM. It outperforms
state-of-the-art detectors on the TinyPerson benchmark.
2
Related Works
Multi-scale Feature Fusion. Objects in UAV images often exhibit tiny sizes
and extreme scale variations, causing signiﬁcant challenges for object detection.
Lower-level features in deep networks lack semantic information but contain rich
texture and geometric details, while higher-level features exhibit the opposite
characteristics. Fusion of multi-scale features can transfer high-level semantic
information to lower-level features, improving the expressive ability of features.
FPN [16] constructs a feature pyramid by using outputs from diﬀerent layers of
the backbone and performs feature fusion through a top-down pathway. NAS-
FPN [7] designs a novel connection method based on neural architecture search.
CE-FPN [23] designs a context enhancement method for extracting stronger
features. To address the semantic conﬂict issue in the fusion of higher-level and
lower-level features, FaPN [12] employs deformable convolutions for implicit com-
pensation to enhance feature consistency. A2-FPN [11] aggregates information
from adjacent features. These works have partially alleviated the feature loss
problem for small objects, but the detection performance is still not ideal for
extremely tiny objects.
Attention Mechanism. Attention mechanisms are considered a potential
means of optimizing features by selectively focusing on the most important
regions in an image and ignoring other regions. In object detection tasks, channel
attention and spatial attention are frequently used. In channel attention meth-
ods, SK-Net [15] employs channel attention to adaptively adjust the receptive
ﬁeld size of diﬀerent feature channels, ABNet [21] captures cross-channel interac-
tion to extract ﬁne-grained features. In spatial attention methods, Dai et al. [4]
proposes a deformable convolution approach to adapt to diﬀerent objects. Some
methods combine channel attention and spatial attention. CBAM [26] introduces
a module that cascades channel attention and spatial attention to compute atten-
tion maps, while Liu et al. [18] propose a module that can cascade and parallelize
both types of attention. These attention modules have shown improvements in
general visual tasks, but their eﬀectiveness is limited for tiny object detection
tasks where features are hard to extract.

284
R. Zhang et al.
3
The Proposed Method
Fig. 1. Overview of our EFPNet. (a)EFPNet architecture, Multi-Dimensional Atten-
tion Module (MDAM) generates attention maps with rich semantics, Eﬀective Fea-
ture Fusion Module (EFFM) guides the fusion of features from adjacent layers.
(b)Visualization of detection result and attention heatmaps.
3.1
Overview of the Proposed EFPNet
This chapter will introduce the architecture of our EFPNet. In order to obtain
higher-quality feature maps, we adopt Swin-T [22] as the backbone of our net-
work. As shown in Fig. 1. {C2, C3, C4, C5} denotes the feature maps of diﬀer-
ent levels after input images being down-sampled by {4, 8, 16, 32} times, these
features are used to construct the feature pyramid. Then, they are input into
the MADM to generate semantically enriched attention maps {A2, A3, A4, A5}.
{P2, P3, P4} are generated from the attention maps of adjacent layers by EFFM,
which are further used to enhance the features of tiny objects. P5 is obtained by
applying convolution to A5. The EFPNet is mainly composed of the MADM and
EFFM. MADM learns the weighted combination of features in both the channel
and spatial dimensions to generate enhanced attention maps. EFFM guides the
fusion of features from adjacent layers based on the information of tiny objects,
it prevents the loss of tiny objects information. In the following chapters, we will
describe the speciﬁc implementation details of the main modules.
3.2
Multi-dimensional Attention Module
It is challenging for general detectors to extract high-quality features for tiny
objects due to their limited amount of pixels. Inspired by the attention model
used in pixel-level regression tasks, we designed MADM to generate attention
maps containing high-quality features of tiny objects. The structure of MADM

Eﬀective Fusion Pyramid Network for Tiny Person Detection in UAV Images
285
is illustrated in Fig. 2. This module divides the input feature map into two
branches to further highlight features in diﬀerent dimension: one branch learns
the channel-only weighted combination, and the other branch learns the spatial-
only weighted combination. The output of two branches are then multiplied with
the original feature and added to obtain the ﬁnal attention map.
Fig. 2. The structure of Multi-Dimensional Attention Module (MDAM). Input features
are divided into two branches to generate channel-only attention tensor and spatial-
only attention tensor.
To
calculate
the
channel-only
attention
tensor,
the
input
feature
(C × H × W) is split in the channel dimension, 1D horizontal global pooling
and 1D vertical global pooling is performed on the H and W dimensions of each
split. This pooling strategy in diﬀerent dimensions preserves more positional
information. Then we use global pooling and convolution to make the concate-
nated feature tensor match the channel dimension of input. After normalization
and sigmoid activation, the channel-only attention tensor Ach is obtained.
For spatial-only attention tensor calculation, we also split the input along the
channel dimension. The two split branches undergo global pooling and reshaping,
and then their element-wise product generates the feature tensor. This approach
allows for higher internal resolution preservation without signiﬁcantly increasing
computational complexity, thereby preserving more tiny object features. After
reshaping and sigmoid activation, the spatial-only feature tensor Asp is obtained.
The channel-only feature tensor Ach and spatial feature tensor Asp are mul-
tiplied to the original feature map and added to obtain the ﬁnal attention map
Ai according to the following equation:
Ai = Cch
i
⊙Ci + Csp
i
⊙Ci,
(1)

286
R. Zhang et al.
where Cch
i
and Csp
i
denotes channel-only feature tensor and spatial-only feature
tensor at ith layer, ⊙is element-wise multiplication, and Ci is the input feature
at ith layer.
3.3
Eﬀective Feature Fusion Module
Fig. 3. Visualization of attention maps. (a) The input image p(1280 × 720) and one
patch p′(200×200) from p. (b) Attention maps of diﬀerent layers, the red points denotes
pixels with values above 20% of the maximum value of the input attention map. (Color
ﬁgure online)
The downsampling and pooling processes in the network cause the features
of tiny objects to gradually drown in the background noise. As shown in Fig. 3,
we visualized the attention maps at diﬀerent layers, and it is clear that the
features of tiny objects are visible in the lower layers. However, in the higher
layers, most of the features of tiny objects vanish. These deep features tend
Fig. 4. (a) The structure of EFFM, feature fusion is guided by a factor α calculated
by statistical analysis. (b) The calculation of NAi and RAi.

Eﬀective Fusion Pyramid Network for Tiny Person Detection in UAV Images
287
to indicate the absence of tiny objects in the image, while the corresponding
positions in the shallow features still contain the features of tiny objects. When
fusing features from diﬀerent layers, a semantic conﬂict arises, where the features
of tiny objects cannot be distinguished from the background features, resulting
in poor performance in tiny object detection.
To alleviate the semantic conﬂicts during the feature fusion process, we
designed EFFM to guide the fusion of adjacent layers’ features. EFFM allows the
appropriate target features from deeper layers to propagate to shallower layers
while suppressing potentially conﬂicting features. This selective feature fusion
approach preserves the features of tiny objects in the lower layers while reducing
the negative impact of higher-layer features on the lower-layer features. In the
fusion process, we introduced a fusion factor, α, to control the weights of diﬀer-
ent layers. This factor enhances the guiding role of EFFM. The calculation of α
is illustrated in Fig. 4.
We ﬁrst compute two pixel-level statistics, NAi and RAi, based on the atten-
tion maps Ai generated at diﬀerent layers using training images. NAi represents
the number of pixels with values above 20% of the maximum value of the input
attention map, and RAi represents the number of connected regions formed by
these pixels. These statistics reﬂect the quantity of tiny objects at diﬀerent lay-
ers. The formulas for calculating NAi and RAi are as follows:
NAi =

Aiinp

(x,y)inAi
LN
xy,
(2)
RAi =

Aiinp

(x,y)inAi
LR
xy,
(3)
where p denotes input images form dataset X, (x, y) denotes coordinate in Ai,
LN
xy is the activated pixels which value is above 20% of the maximum value of
Ai, and LR
xy is the connected regions formed by LN
xy. This approach of separately
counting pixels and connected regions enables a reasonable allocation of weights
during the fusion of neighboring layers’ features and avoids incorrect calculations
for partially occluded targets. The fusion factor αi+1
i
, which is derived from NAi
and RAi, is calculated using the following fonction:
αi+1
i
= NAi+1
NAi
+ RAi+1
RAi
,
(4)
where NAi+1 and NAi denotes the number of activated pixels in diﬀerent layers,
RAi+1 and RAi denotes the number of connected regions in diﬀerent layers.
To ensure consistent gradient computation and mitigate the eﬀects of seman-
tic conﬂicts, we multiply the attention maps by the upsampled features from
higher layers. This strengthens the regions that are simultaneously detected in
adjacent layers while suppressing the negative impact of regions that only appear
in the higher layers on the lower layers. This approach ensures that more objects
can be detected in each layer during subsequent multi-scale detection and also
helps alleviate issues caused by occlusion. The feature fusion process in EFFM
can be represented as follows:

288
R. Zhang et al.
Pi = (Ai + αi+1
i
fup(Ai+1)) ⊙(Ai ⊙fup(Ai+1)),
(5)
where ⊙denotes element-wise multiplication, fup is the nearest upsampling oper-
ation, and Pi is the output of EFFM.
4
Experiments and Results
4.1
Experimental Settings
Datasets and Evaluation Metrics. Our EFPNet is evaluated on TinyPer-
son [27], which is a public dataset for seaside tiny person detection. There are
around 1600 images with more than 70k bounding boxes are annotated. The size
range of person is divided into 5 intervals: tiny[2, 20], tiny1[2, 8], tiny2[8, 12],
tiny3[12, 20], and small[20, 32]. In order to keep consistent with the TinyPerson
benchmark [27], we also adopt AP (average precision) as evaluation metrics.
Implementation Details. We choose Swin-T and ResNet-50 pre-trained on
ImageNet as the backbone respectively. To prevent GPU being out of memory
during training, we cropped the input images into patches of size 680×510 with a
40 pixel overlap. Our network was developed based on the MMDetection toolkit
[1]. All models were trained for 12 epochs on two 24GB RTX 3090 GPUs. We
employed the stochastic gradient descent optimizer with an initial learning rate
of 0.002, which was decreased by a factor of 0.1 after 6 epochs. For anchor
generation, we used aspect ratios of 0.5, 1.2, and 2.0.
4.2
Visualization Analysis
Fig. 5. Qualitative comparison of attention maps generated by diﬀerent method. Sub-
image(200×200) is cropped from the input image, The A2, A3, and A4 are the attention
maps in diﬀerent layers and result is the detection result of the sub image.

Eﬀective Fusion Pyramid Network for Tiny Person Detection in UAV Images
289
To demonstrate the eﬀectiveness of MDAM, we compared visualization of the
attention heatmaps generated by MDAM with those generated by state-of-the-
art method (SSPNet). As shown in Fig. 5, the attention heatmaps generated
by MDAM exhibit higher quality and fewer noise compared to other methods.
These high-quality feature maps enable more accurate detection of tiny person
and reduce the number of missed targets. Thus, our approach is highly eﬀective
for detecting tiny person.
Fig. 6. Visualization of detection results.
To further validate the eﬀectiveness of EFPNet, we present the detection
results of several images in Fig. 6. These images contain densely crowded scenes,
signiﬁcant scale variations among crowds and sparsely scattered person. It can be
observed that results retain the features of correct targets while suppressing the
features of incorrect targets. Our EFPNet is also capable of accurately detecting
partially occluded objects. While there are occasional misclassiﬁcations of tiny
objects such as buoys, we believe that these issues will be resolved as the dataset
becomes more abundant. Overall, our EFPNet can accurately detect tiny targets
while maintaining the performance in detecting larger targets.
4.3
Comparison to State-of-the-arts
In Table 1, we present a comparison of our EFPNet with state-of-the-art
methods on TinyPerson. We measure the performance using the standard crite-
rion Average Precision (AP). The superscript of AP represents the IOU thresh-
old, while the subscript represents size of tiny objects. A higher AP indicates
better performance of the detector. During the training process, we applied the
same conﬁguration to all models and utilized consistent data augmentation and
multi-scale training methods for all detectors. From the table, we can observe

290
R. Zhang et al.
Table 1. APs of diﬀerent methods on TinyPerson. The best result in each AP is
highlighted in bold.
Detector
AP tiny
50
AP tiny1
50
AP tiny2
50
AP tiny3
50
AP small
50
AP tiny
25
AP tiny
75
RetinaNet [17]
51.19
35.44
54.53
60.89
66.83
70.68
6.21
Faster RCNN-FPN [24]
51.46
35.65
55.96
61.51
67.02
71.05
6.38
FoveaNet [14]
54.18
37.97
57.27
64.84
71.63
74.81
7.12
RetinaNet+SM with S-α [8] 54.64
41.32
57.35
64.22
67.43
73.59
6.78
Swin-T [22]
55.55
42.39
58.64
64.71
68.91
75.56
7.24
Faster RCNN-SSPNet [10]
58.86
47.25
61.82
66.47
72.14
77.98
8.92
Swin-T-AEFNet [6]
59.82
47.59
61.74
68.43
72.23
78.09
9.21
Faster RCNN-EFPNet
60.35
48.33
62.96
69.14
72.20
78.16
9.16
Cascade RCNN-EFPNet
59.19
47.82
61.77
68.35
72.14
77.96
8.99
Swin-T-EFPNet
60.72
48.85
63.18
69.43
72.52
78.48
9.34
that while using the Faster-RCNN framework, our EFPNet improves the perfor-
mance by 8.89% compared to FPN of AP tiny
50
, 1.49% compared to the latest SSP-
Net. Moreover, while employing the Swin-T framework, our method outperforms
Swin-T by 5.17% and AEFNet by 0.90%. These results strongly demonstrate the
eﬀectiveness and superiority of EFPNet in tiny object detection task.
4.4
Ablation Experiments
To demonstrate the eﬀectiveness of MADM and EFFM, we conducted abla-
tion experiments by integrating MADM and EFFM separately on Swin-T. The
experimental results are shown in Table 2, it can be observed that both MADM
and EFFM contribute to performance gains. Speciﬁcally, EFFM achieves larger
improvements due to its ability to address more severe semantic conﬂicts in fea-
ture fusion. The combination of MADM and EFFM allows the two modules to
complement each other and leads to signiﬁcant performance gains.
Table 2. Ablation experiments on TinyPerson.
Detector
AP tiny
50
AP tiny1
50
AP tiny2
50
AP tiny3
50
AP small
50
AP tiny
25
AP tiny
75
Swin-T [22]
55.55
42.39
58.64
64.71
68.91
75.56
7.24
Swin-T+MADM
58.26
45.15
60.85
67.08
70.11
77.02
8.44
Swin-T+EFFM
58.92
46.07
61.54
67.73
70.74
77.46
8.59
Swin-T+MADM+EFFM 60.72
48.85
63.18
69.43
72.52
78.48
9.34
5
Conclusions
We propose a feature pyramid network to accurately detect tiny objects in large-
scale scenes by generating high-quality attention maps and guiding feature fusion

Eﬀective Fusion Pyramid Network for Tiny Person Detection in UAV Images
291
eﬀectively. Two major components are designed to improve the detection perfor-
mance, namely the Multi-Dimensional Attention Module (MDAM) and the Eﬃ-
cient Feature Fusion Module (EFFM). MDAM extracts information from both
the channel and spatial dimensions to generate attention maps with rich seman-
tic information. EFFM guides feature fusion by statistically analyzing the pixel-
level object information in the attention maps, eﬀectively alleviating semantic
conﬂicts and ensuring gradient consistency during the fusion process. Visual-
ization results and comparative experiments demonstrate that our network can
eﬀectively extract features of tiny objects and achieve high-precision detection
for complex tiny objects, with average precision of 60.72% on the public dataset
TinyPerson. In future work, we will explore the application of our proposed
model in occluded scenes and multi-class tiny object detection tasks.
References
1. Chen, K., et al.: MMDetection: Open MMLab detection toolbox and benchmark.
arXiv:1906.07155 (2019)
2. Cheng, G., Yuan, X., Yao, X., Yan, K., Zeng, Q., Han, J.: Towards large-scale
small object detection: Survey and benchmarks. arXiv:2207.14096 (2022)
3. Cheng, Y., Xu, H., Liu, Y.: Robust small object detection on the water surface
through fusion of camera and millimeter wave radar. In: 2021 IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV), pp. 15243–15252 (2021)
4. Dai, J., et al.: Deformable convolutional networks. In: 2017 IEEE International
Conference on Computer Vision (ICCV), pp. 764–773 (2017)
5. Duan, C., Wei, Z., Zhang, C., Qu, S., Wang, H.: Coarse-grained density map guided
object detection in aerial images. In: 2021 IEEE/CVF International Conference on
Computer Vision Workshops (ICCVW), pp. 2789–2798 (2021)
6. Gao, S., Liu, C., Zhang, H., Zhou, Z., Qiu, J.: Multiscale attention-based detection
of tiny targets in aerial beach images. Front. Mar. Sci. 9, 1073615 (2022)
7. Ghiasi, G., Lin, T.Y., Pang, R., Le, Q.V.: NAS-FPN: learning scalable feature
pyramid architecture for object detection. In: 2019 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 7029–7038 (2019)
8. Gong, Y., Yu, X., Ding, Y., Peng, X., Zhao, J., Han, Z.: Eﬀective fusion factor in
FPN for tiny object detection. In: 2021 IEEE Winter Conference on Applications
of Computer Vision (WACV), pp. 1159–1167 (2020)
9. Guo, C., Fan, B., Zhang, Q., Xiang, S., Pan, C.: AugFPN: improving multi-scale
feature learning for object detection. 2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 12592–12601 (2019)
10. Hong, M., Li, S., Yang, Y., Zhu, F., Zhao, Q., Lu, L.: SSPNet: scale selection
pyramid network for tiny person detection from UAV images. IEEE Geosci. Remote
Sens. Lett. 19, 1–5 (2021)
11. Hu, M., Li, Y., Fang, L., Wang, S.: A2-FPN: attention aggregation based feature
pyramid network for instance segmentation. In: 2021 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 15338–15347 (2021)
12. Huang, S., Lu, Z., Cheng, R., He, C.: FaPN: feature-aligned pyramid network for
dense image prediction. In: 2021 IEEE/CVF International Conference on Com-
puter Vision (ICCV), pp. 844–853 (2021)
13. Kisantal, M., Wojna, Z., Murawski, J., Naruniec, J., Cho, K.: Augmentation for
small object detection. arXiv:1902.07296 (2019)

292
R. Zhang et al.
14. Kong, T., Sun, F., Liu, H., Jiang, Y., Li, L., Shi, J.: FoveaBox: beyound anchor-
based object detection. IEEE Trans. Image Process. 29, 7389–7398 (2019)
15. Li, X., Wang, W., Hu, X., Yang, J.: Selective kernel networks. In: 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 510–519
(2019)
16. Lin, T.Y., Doll´ar, P., Girshick, R.B., He, K., Hariharan, B., Belongie, S.J.: Feature
pyramid networks for object detection. In: 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 936–944 (2016)
17. Lin, T.Y., Goyal, P., Girshick, R.B., He, K., Doll´ar, P.: Focal loss for dense object
detection. IEEE Trans. Pattern Anal. Mach. Intell. 42, 318–327 (2017)
18. Liu, H., Liu, F., Fan, X., Huang, D.: Polarized self-attention: towards high-quality
pixel-wise regression. arXiv:2107.00782 (2021)
19. Liu, S., Huang, D., Wang, Y.: Learning spatial fusion for single-shot object detec-
tion. arXiv:1911.09516 (2019)
20. Liu, W., et al.: SSD: single shot multibox detector. In: European Conference on
Computer Vision (2015)
21. Liu, Y., Li, Q., Yuan, Y., Du, Q., Wang, Q.: ABNet: adaptive balanced network
for multi-scale object detection in remote sensing imagery. IEEE Trans. Geosci.
Remote Sens. 60, 1–14 (2021)
22. Liu, Z., et al.: Swin transformer: hierarchical vision transformer using shifted win-
dows. In: 2021 IEEE/CVF International Conference on Computer Vision (ICCV),
pp. 9992–10002 (2021)
23. Luo, Y., et al.: CE-FPN: enhancing channel information for object detection. Mul-
timedia Tools Appl. 81, 30685–30704 (2021)
24. Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: towards real-time object
detection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell.
39, 1137–1149 (2015)
25. Varga, L.A., Kiefer, B., Messmer, M., Zell, A.: SeaDronesSee: a maritime bench-
mark for detecting humans in open water. In: 2022 IEEE/CVF Winter Conference
on Applications of Computer Vision (WACV), pp. 3686–3696 (2021)
26. Woo, S., Park, J., Lee, J.Y., Kweon, I.S.: CBAM: convolutional block attention
module. In: European Conference on Computer Vision (2018)
27. Yu, X., Gong, Y., Jiang, N., Ye, Q., Han, Z.: Scale match for tiny person detection.
In: 2020 IEEE Winter Conference on Applications of Computer Vision (WACV),
pp. 1246–1254 (2019)
28. Zhang, R., et al.: Automatic detection of earthquake-damaged buildings by inte-
grating UAV oblique photography and infrared thermal imaging. Remote. Sens.
12, 2621 (2020)

End-to-End Object-Level Contrastive
Pretraining for Detection via Semantic-Aware
Localization
Long Geng and Xiaoming Huang(B)
Computer School, Beijing Information Science and Technology University, Beijing 100029,
People’s Republic of China
huangxm0556@163.com
Abstract. Pretraining on a large dataset is the ﬁrst stage of many computer vision
tasks such as classiﬁcation, detection, and segmentation. A conventional pretrain-
ing approach is performed on large datasets with human annotation. In this con-
text, self-supervised learning, which uses unlabeled datasets to pretrain models,
shows increasing promise for applications. Throughout the development of self-
supervised learning, image-level contrastive representation learning has emerged
as a highly effective approach for general transfer learning. However, it may lack
speciﬁcity when applied to a speciﬁc downstream task, compromising perfor-
manceinthattask.Recently,anobject-levelself-supervisedpretrainingframework
called SoCo is proposed for object detection. To achieve object-level pretraining,
they adopt the traditional selective search algorithm to generate object proposals,
which needs high space and time cost and also hinders end-to-end training to
achieve global optimization. In this work, we propose an end-to-end object-level
contrastive pretraining for detection, which obtains object proposals using the pre-
training network itself. Speciﬁcally, we adopt the heat map from the features at
the last backbone convolutional layer as semantic information to roughly local-
ize objects, then generate promised proposals with center-suppressed sampling
and multiple cropping strategies. The experimental results show that our method
displays better performance with signiﬁcantly less training space and time cost.
Keywords: Object Detection · Self-Supervised Learning · Semantic-aware
Localization · Contrastive Learning · Pretraining
1
Introduction
Object detection aims at ﬁnding objects in images or videos, and labeling them with
bounding boxes and the corresponding categories. Object detection methods based on
deep learning can be categorized into two classes, namely single-stage and two-stage
approaches, based on the generation of proposal windows [1]. Examples of single-stage
methods include YOLO [2] and SSD [3], while Faster R-CNN [4] is an example of a two-
stage method. This classiﬁcation is based on whether these methods generate proposal
windows or not. Both single-stage and two-stage object detection algorithms require
pretraining on large datasets, such as ImageNet, to initialize the network effectively.
Pretraining on such extensive datasets is a crucial step for both types of algorithms.
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 293–304, 2024.
https://doi.org/10.1007/978-981-99-8850-1_24

294
L. Geng and X. Huang
A conventional pretraining approach is carried out on large labeled datasets, while
self-supervised learning methods alleviate the dependence of model pretraining on
labeled data. The visual representations learned by image-level contrastive learning
are generalizable and can perform quite well in many downstream tasks. However, it is
also this generality that makes the initialization of the model sacriﬁce the speciﬁcity it
deserves when we focus on a speciﬁc downstream task. Recent research has observed
that the representations learned through image-level contrastive learning may not be
optimal for dense labeling tasks, such as object detection and instance segmentation.
This indicates a signiﬁcant representation gap between image-level contrastive learning
and object-level bounding boxes in the context of object detection. In detection tasks,
it is common to use bounding boxes to represent the detected object and to reﬂect the
relevant properties of object detection by the position and size of the bounding boxes.
To address the representation gap, a novel object-level self-supervised pre-training
framework called Selective Object COntrastive learning (SoCo) has been proposed [5].
SoCo introduces a design principle that emphasizes aligning the self-supervised pretext
task with the downstream task, aiming to bridge the representation gap effectively. SoCo
uses selective search [6], a traditional algorithm, to complete the generation of object
proposals, thus introducing an object-level representation, while treating each object
proposal in an image. SoCo performs scale and position data enhancement on the same
image to obtain multiple augmented views, and learns object-level visual representa-
tions by maximizing similarity through contrastive learning, enabling the acquisition
of important properties like translation invariance and scale invariance, which are cru-
cial for object detection tasks. Additionally, SoCo performs pretraining on all network
modules used in the detector, ensuring well-initialized layers throughout the detector.
SoCo [5] should be the ﬁrst to achieve pretraining of the entire object detection
network, and the task alignment approach used to achieve this goal is highly innovative
and inspiring. However, the object proposals in SoCo pretraining are generated ofﬂine
using selective search [6], which needs high space and time cost. The traditional pro-
posal generation usually shows worse performance than deep learning-based methods.
Besides, it does not allow end-to-end training to achieve global optimization.
To conquer the shortcomings of SoCo [5], we consider the object proposals gener-
ation using the pretraining network itself. Speciﬁcally, we adopt the heat map from the
features at the last convolutional layer of the backbone network as semantic information
to roughly localize objects in the image, then generate promised proposals with center-
suppressed sampling and multiple cropping strategies. To evaluate the effectiveness of
our method, we conducted pretraining on the ImageNet dataset [7] followed by ﬁne-
tuning on the MS COCO dataset [8]. This two-step process allowed us to validate the
performance and efﬁcacy of our approach.
The main contributions of this work can be summarized as follows:
(1) Introduction of an end-to-end object-level contrastive pretraining approach for object
detection. This approach leverages the pretraining network itself to obtain object
proposals, eliminating the need for separate proposal generation methods.
(2) Comparative evaluation against the SoCo method [5] on the ImageNet dataset,
demonstrating that our proposed approach achieves superior performance while
requiring signiﬁcantly less training space and time.

End-to-End Object-Level Contrastive Pretraining
295
Section2providesacomprehensivereviewofrelevantself-supervisedlearningmeth-
ods, establishing the context for our proposed approach. In Sect. 3, we present our method
along with the enhancements made to improve its effectiveness. The experimental evalu-
ation of our approach is detailed in Sect. 4, showcasing its performance and comparative
analysis against existing methods. Finally, Sect. 5 concludes the paper, summarizing the
key ﬁndings and contributions of our work.
2
Related Work
Learning effective visual representations in the absence of supervision is a long-standing
topic. In this section, we begin by reviewing self-supervised learning techniques for
classiﬁcation and object detection tasks, then introduce the recent object-level self-
supervised pretraining framework SoCo [5].
2.1
Self-supervised Learning for Classiﬁcation
In the ﬁeld of image classiﬁcation, self-supervised learning has been a powerful
alternative to supervised pretraining, especially in image-rich but sparsely annotated
domains.
The backbone pretraining strategies can be categorized into contrastive, clustering-
based, and self-distillation [9]. Contrastive learning approaches (e.g., MoCo [10] and
SimCLR [11]) use InfoNCE loss to compare sample pairs. In general, a common practice
involves obtaining multiple views of each image through data augmentations. This is
done by maximizing the similarity between different augmented views of the same image
while simultaneously minimizing the similarity between augmented views of different
images. This approach aims to enhance the robustness and generalization capabilities
of the learned representations by encouraging consistency across different views of the
same image while emphasizing the differences between different images. Clustering-
based approaches generate pseudo-labeling by unsupervised clustering techniques. The
paper introduced a method called Swapping Assignments between multiple Views of the
same image (SwAV) [12]. SwAV aims to predict cluster assignments for one view based
on an augmented view of the same image. Unlike contrastive learning, self-distillation
methods focus on maximizing the similarity between predictions made by a teacher and
student model. In the BYOL (Bring Your Own Latent) [13] method, the student model is
optimized using gradient descent, while the teacher model’s weights are updated using an
exponential moving average of the student weights. This approach facilitates knowledge
transfer and improves the performance of the student model.
Conceptually,severalproblemsexistintransferringaclassiﬁcation-basedrepresenta-
tion to detection. The presence of an untrained detection head is one aspect to consider.
Model architecture differences lead to the omission of pre-training for the detection
heads used in object detection. Conversely, there exists a task mismatch between Ima-
geNet Top-1 classiﬁcation accuracy and object detection performance. Although certain
attributes are utilized in classiﬁcation, they may hinder accurate object localization,
which can lead to imperfect positive correlation.

296
L. Geng and X. Huang
2.2
Self-supervised Learning for Object Detection
Self-supervised object detection methods can be broadly classiﬁed into three categories:
predictive, contrastive, and self-distillation.
In predictive methods (e.g., UP-DETR [14]), pretraining of the detection head is
achieved by having the head re-predict the location of the autogenerated “ground truth”
crop. Contrastive methods (e.g., DenseCL [155]) attempt to train the box head of the
Faster RCNN by minimizing the InfoNCE loss such that the output of positive sample
pairs is close and the output of negative sample pairs is far. Self-distillation methods,
such as SoCo, are signiﬁcantly different from contrastive methods because they do not
require the use of negative samples. SoCo employs a simultaneous training approach to
pretrain the backbone, feature pyramid, and RoI head of the Faster R-CNN model. This
involves training two networks in parallel, allowing for comprehensive pretraining of
these key components in object detection.
These methods not only try to compensate for the shortcomings of classiﬁcation
methods but also attempt pretraining the dedicated modules used for object detection on
unsupervised pretext tasks as well.
2.3
Selective Object COntrastive Learning (SoCo)
Fig. 1. Overview of SoCo. SoCo utilizes selective search for object proposals and creates multi-
ple views with varying scales and locations. It uses a backbone network with FPN and RoIAlign
for feature encoding. Contrastive learning is performed at the object level to learn invariant
representations. The target network is updated using exponential moving average.
The main ﬂowchart of SoCo is illustrated in Fig. 1. In this section, we will cover it
in detail in several parts.
Object Proposal Generation. To generate object proposals, SoCo employs the use of
selective search [6], an unsupervised algorithm that takes into consideration factors such
as color similarity. This allows SoCo to generate accurate and diverse object proposals
for further processing.
View Construction. SoCo uses three views, namely V1, V2, and V3, where V1 is
obtained by resizing the original image, V2 is obtained by randomly cropping V1, and V3

End-to-End Object-Level Contrastive Pretraining
297
is obtained by downsampling V2. Each view undergoes independent and random aug-
mentation. By introducing variations in scale and location for the same object proposal
across the augmented views, the model can effectively learn translation-invariant and
scale-invariant object-level representations.
Scale-Aware Assignment. To facilitate the learning of scale-invariant object-level
representations, SoCo assigns object proposals with areas falling within speciﬁc
ranges

0 −482, 492 −962, 972 −1922, 1932 −2242
to different pyramid levels
{P2, P3, P4, P5} respectively. This approach ensures that object proposals of vary-
ing scales are encouraged to learn scale-invariant representations through contrastive
learning. By considering proposals at different scales, SoCo can effectively capture
object-level information across different size ranges.
Architecture. Object proposals address the architectural discrepancy between pretrain-
ing and downstream detection ﬁnetuning. Mask R-CNN [16] evaluates transfer perfor-
mance, utilizing ResNet-50 with FPN as the image-level feature encoder. RoIAlign
extracts RoI features from the feature map. SoCo employs both the online and target
networks, sharing a common architecture but with different weights.
3
Method
In this section, we present the proposed end-to-end object-level contrastive pretrain-
ing for detection, which obtains object proposals using the pretraining network itself,
includingsemantic-aware localization, center-suppressed sampling, and multiple crop
strategies.
From the Introduction, it can be seen that the current self-supervised learning method
SoCo [5], which performs well in object detection, has certain shortcomings: object
proposals generation by selective search [6] wastes a lot of time and storage space,
traditional proposal generation usually shows worse performance than deep learning-
based methods, and it cannot achieve end-to-end training. By observing the excellent
performance of the semantic-aware localization strategy [17] to obtain heat maps and
the multiple cropping idea [12] in self-supervised learning, this paper innovatively uses
the above strategies for object proposal generation to achieve end-to-end self-supervised
object detection. We refer to the improved method as SoCo-E2E.
As shown in Fig. 2, SoCo-E2E uses the semantic-aware localization strategy to obtain
the bounding box, followed by several object proposal boxes by the center-suppressed
sampling strategy. During each training step, k proposals are randomly chosen to cre-
ate three views that exhibit variations in scale and location. The backbone network is
employed to encode image-level features, while RoI Align is utilized to extract object-
level features. Based on the area of the object proposals, they are assigned to different
pyramid levels, and contrastive learning is conducted at the object-level to facilitate the
learning of translation invariance and scale invariance for detection. The target network
is updated by performing an exponential moving average of the online network. This
approach helps stabilize the learning process and ensures that the target network beneﬁts
from the accumulated knowledge of the online network.

298
L. Geng and X. Huang
Fig. 2. Overview of SoCo-E2E.
3.1
Semantic-Aware Localization
The semantic-aware localization strategy [17] enables the model to learn semantic infor-
mation in the image in an unsupervised manner so that the generated object proposals
are reasonably framed to objects. In this research paper, the heat map is computed by
aggregating features at the ﬁnal convolutional layer across channels and subsequently
normalizing them within the range of [0,1]. Figure 3 provides visual representations of
the heat maps generated at different training milestones, including the 20th, 40th, 60th,
and 80th epochs.
Fig. 3. Heat map for different training epochs
Based on the above analysis, we use the information in the heat map to locate objects
during the training process. Speciﬁcally, a random generation method is used to initialize
the object proposals at the early stage of training, as a way to make the network learn
the semantic information of the whole image. Then, the bounding box of object B is
obtained from the heat map by a threshold function, as shown in Eq. 1.
B = L(I[M > thresh])
(1)
where M denotes the heat map, thresh ∈[0,1] is the threshold of activation, I is the
indicator function, and L computes the rectangular closure of the activation position.
Throughout the training process, the object proposals undergo periodic and progres-
sive updates to leverage the latest learned features from the mode l. It is important to
note that the objective of this method is not precise localization, but rather guiding the
bounding box generation by identifying the object of interest. The threshold parameter,
denoted as thresh ∈[0, 1], controls the proportion of the bounding box.

End-to-End Object-Level Contrastive Pretraining
299
3.2
Center-Suppressed Sampling
The semantic-aware localization strategy generates a bounding box containing the
objects of interest. To avoid that the object proposals generated based on this bounding
box are too concentrated and contain approximate semantic content, in this subsection,
we introduce the center-suppressed sampling strategy [17] to solve this problem.
The primary objective is to minimize the concentration of object proposals around
the center by dispersing them. To achieve this, the paper proposes the utilization of a
beta distribution β(α, α) with identical parameters α. By adjusting α, the shape of the
distribution can be controlled effectively. We expand the variance of the object proposals
by setting α < 1 so that the probability is lower near the center and higher at other
locations. In this way, the object proposals are more easily dispersed near the boundary
line of the operable region and the overlap can be largely avoided.
Fig. 4. Example of center-suppressed sampling
As shown in Fig. 4, based on the bounding box obtained by the semantic-aware local-
ization strategy in Sect. 3.1, random sampling leads to too much similarity among object
proposals, which is not conducive to model learning. When generating object proposals
in this paper, the central suppression sampling strategy will be used to make the object
proposals overlap as little as possible to obtain differentiated semantic information, so
that the model can be learned better.
3.3
Multiple Crop
In SwAV [12], the data enhancement strategy of multiple-crop was ﬁrst proposed. In
previous self-supervised contrastive learning methods, such as MoCo [10], and SimCLR
[11], all of them crop two random views of 224 × 224 size from the original image,
while the multi-crop strategy includes two standard random crops and V small views.
The larger area crops reﬂect the features of the whole scene and only enable the
model to learn the global features. If we want the model to learn the local features of
these objects, we need to add multiple additional small area crops. To keep the model
from paying extra computational cost, the authors of SwAV [12] proposed using 2 crops
of 160 × 160 size to learn the image’s global features. Also to add additional positive
samples to learn local features, 4 crops of size 96 × 96 were selected. By adjusting the
size of the crops, a trade-off is made between the area of the crops and the number of

300
L. Geng and X. Huang
crops to add additional views to learn local features without increasing the computational
cost.
Fig. 5. Multi-crop sampling example
As shown in Fig. 5, when generating object proposals in this paper, the multiple
cropping strategy will be used to control the size of the object proposals so that they can
extract global features and local features as much as possible to better learn the semantic
information of the images.
4
Experiment
4.1
Pretraining Settings
Similar to SoCo [5], we also use Mask R-CNN to evaluate transfer performance, ResNet-
50 with FPN for image-level feature encoding, and RoIAlign to extract RoI features.
The difference lies in the generation phase of object proposals, where we obtain the
convolutional layer output of the backbone network during training. Same to SoCo [5],
ImageNet [7] was used as the dataset for self-supervised pretraining. Data enhancement
was performed on all constructed views using the process used in BYOL [13], including
random level ﬂipping, color dithering, Gaussian blur, exposure, and grayscale operations.
In all experiments, we adopt a training schedule consisting of 100 epochs, and the
reported results in the comparisons with SoCo methods are based on 100 epochs. The
LARS optimizer is utilized, along with a cosine decay learning rate schedule. The weight
decay is set to 1.0 × e−5, and the total batch size is conﬁgured as 256 across 4 Nvidia
V100 GPUs.
4.2
Finetuning Settings
For transfer learning, the COCO datasets are employed. Speciﬁcally, we use the COCO
train2017 set, which consists of approximately 118,000 images.
In our approach, we utilize the Mask R-CNN detector [166] with the R50-FPN
backbone. The evaluation metrics include APbb, APbb
50 and APbb
75 for object detection, as
well as APmk, APmk
50 and APmk
75 for instance segmentation.

End-to-End Object-Level Contrastive Pretraining
301
To ensure consistency, synchronized batch normalization is implemented across all
layers, including the newly initialized batch normalization layers during ﬁnetuning,
following the approach in [18].
For the COCO datasets, we adopt stochastic gradient descent with a batch size 8 on
4 GPUs. Weight decay and base learning rate are selected as of 2.5 × e−5 and 0.02. The
learning rate increases linearly for the ﬁrst 1000 iterations and then decreases twice by
a factor of 10 after 2/3 and 8/9 of the total training time, as described in [18].
4.3
Training Time and Space Cost
We carry a series of experiments to verify the feasibility of the object proposal gener-
ation scheme proposed in this paper, including runtime and storage space comparison,
performance evaluation, and ablation study.
Table 1. Comparison of runtime and storage space
Method
Time
Storage space
selective search
pretrain
ﬁne-tune
total
SoCo [5]
36 h
13.5 h
10 h
59.5 h
291 G
SoCo-E2E
(Ours)
-
14 h
10 h
24 h
1 G
As shown in Table 1, with a 10% ImageNet dataset, the SoCo method requires about
36 h of selective search, 13.5 h of pretraining and 10 h of ﬁne-tuning. The total time
required is 59.5 h. The SoCo-E2E method proposed in this paper requires only 14 h of
pretraining and 10 h of ﬁne-tuning. The total time required is 24 h. The total training time
is 60% lower than that of the SoCo method. In terms of storage space, SoCo requires
291 G, while the SoCo-E2E proposed requires less than 1 G. Therefore, the SoCo-E2E
method has a great improvement in both runtime and storage space.
4.4
Performance Comparison
Table 2 shows the results for Mask R-CNN with R50-FPN backbone. We compare
the proposed method with SoCo [5] on the COCO 1 × schedules. Due to hardware
limitations, we use batch size 256 over 4 Nvidia V100 16G GPUs in the proposed
method (denoted as SoCo-E2E#). For a fair comparison, we also run the author’s code
with batch size 256 over 4 Nvidia V100 16G GPUs (denoted as SoCo#).
As shown in the bottom two rows of Table 2, compared with SoCo# [5], the proposed
SoCo-E2E# shows about 1%-2% performance improvement, reﬂecting the effectiveness
of our work. Additionally, the proposed method can achieve end-to-end training, which
is more convenient to use, and improve efﬁciency, avoiding a large amount of time and
storage space occupied when using the selective search, and also reducing the hardware
requirements when applying the method to other datasets.

302
L. Geng and X. Huang
Table 2. Comparison with SoCo [5] on COCO by using Mask R-CNN with R50-FPN.
Methods
Epoch
APbb
APbb
50
APbb
75
APmk
APmk
50
APmk
75
SoCo# [5]
100
32.9
51.4
35.3
30.1
48.6
32.1
SoCo-E2E#
(Ours)
100
34.2
52.7
37.1
31.1
49.9
33.2
4.5
Ablation Study
To further analyze the advantages of SoCo-E2E, we conduct an ablation study that exam-
ines the effectiveness of the semantic-aware localization strategy. We carry pretraining
experiments on 0%, 10% and 100% ImageNet datasets, the results are shown in Table 3.
The result with 0% ImageNet datasets which means object detection without pretraining,
shows signiﬁcantly worse performance and indicates the importance of pretraining in
object detection tasks. Our work shows a little worse performance than SOCO [5] on
pretraining with 10% ImageNet datasets while achieving better results on pretraining
with 100% ImageNet datasets.
We analyze that the semantic-aware localization strategy has the property of beneﬁt-
ing from larger datasets, while the performance of traditional selective search algorithms
is independent of dataset size. This also indicates that semantic-aware localization meth-
ods have more performance advantages in the ﬁeld of self-supervised learning using large
datasets.
Table 3. Evaluation results on COCO by using Mask R-CNN with R50-FPN.
method
ImageNet
APbb
APbb
50
APbb
75
APmk
APmk
50
APmk
75
None
0%
26.65
43.15
28.11
24.65
40.59
26.08
SoCo# [5]
10%
32.09
50.34
34.72
29.16
47.24
30.90
100%
32.89
51.35
35.29
30.14
48.63
32.08
SoCo-E2E#
(Ours)
10%
31.85
49.74
34.46
29.03
47.06
31.03
100%
34.15
52.72
37.10
31.14
49.89
33.19
5
Conclusion
In this paper, we propose an end-to-end object-level contrastive pretraining for detec-
tion. To this end, we use semantic-aware localization to learn semantic information for
generating bounding boxes during training, then apply center-suppressed sampling and
multiple cropping to increase the diversity and plausibility of object proposals. The com-
parative experiments demonstrate that the proposed method shows better performance
with signiﬁcantly less training space and time cost. By observing the experimental results

End-to-End Object-Level Contrastive Pretraining
303
on datasets of different sizes, it is found that the semantic-aware localization strategy
can learn better semantic information on larger datasets. The phenomenon reﬂects that
semantic-aware localization has a good prospect in the ﬁeld of self-supervised learning
on large datasets.
Acknowledgments. This work was supported by Scientiﬁc Research Project of Beijing Educa-
tional Committee (KM202011232014).
References
1. Zaidi, S.S.A., Ansari, M.S., Aslam, A., Kanwal, N., Asghar, M., & Lee, B.: A survey of
modern deep learning based object detection models. Digital Signal Process. 126 103514.
(2022)
2. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: uniﬁed, real-time
object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 779–788 (2016)
3. Liu, W., et al.: SSD: single shot multibox detector. In: Leibe, B., Matas, J., Sebe, N., Welling,
M. (eds.) ECCV 2016. LNCS, vol. 9905, pp. 21–37. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-46448-0_2
4. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object detection with
region proposal networks. In: Advances in Neural Information Processing Systems, vol. 28
(2015)
5. Wei, F., Gao, Y., Wu, Z., Hu, H., Lin, S.: Aligning pretraining for detection via object-level
contrastive learning. Adv. Neural. Inf. Process. Syst. 34, 22682–22694 (2021)
6. Uijlings, J.R., Van De Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search for object
recognition. Int. J. Comput. Vision 104(2), 154–171 (2013)
7. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual
object classes (VOC) challenge. Int. J. Comput. Vis. 88(2), 303–338 (2010)
8. Lin, T.-Y., et al.: Microsoft coco: common objects in context. In: Fleet, D., Pajdla, T., Schiele,
B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp. 740–755. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-10602-1_48
9. Huang, G., Laradji, I., Vazquez, D., Lacoste-Julien, S., Rodriguez, P.: A survey of self-
supervised and few-shot object detection (2021). arXiv preprint arXiv:2110.14711
10. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised visual
representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 9729–9738 (2020)
11. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learn-
ing of visual representations. In: International Conference on Machine Learning (PMLR),
pp. 1597–1607 (2020)
12. Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised learning
of visual features by contrasting cluster assignments. Adv. Neural. Inf. Process. Syst. 33,
9912–9924 (2020)
13. Grill, J.B., et al.: Bootstrap your own latent-a new approach to self-supervised learning. Adv.
Neural. Inf. Process. Syst. 33, 21271–21284 (2020)
14. Dai, Z., Cai, B., Lin, Y., Chen, J.: UP-DETR: unsupervised pretraining for object detection
with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 1601–1610 (2021)

304
L. Geng and X. Huang
15. Wang, X., Zhang, R., Shen, C., Kong, T., Li, L.: Dense contrastive learning for self-supervised
visual pretraining. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 3024–3033 (2021)
16. He, K., Gkioxari, G., Dollár, P., Girshick, R. Mask R-CNN. In: Proceedings of the IEEE
International Conference on Computer Vision, pp. 2961–2969 (2017)
17. Peng, X., Wang, K., Zhu, Z., Wang, M., You, Y.: Crafting better contrastive views for siamese
representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 16031–16040 (2022)
18. Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive
learning (2020). arXiv preprint arXiv:2003.04297

PointerNet with Local and Global
Contexts for Natural Language Moment
Localization
Linwei Ye1(B)
, Zhi Liu2
, and Yang Wang3
1 College of Computer Science and Artiﬁcial Intelligence, Wenzhou University,
Wenzhou, China
ylw@wzu.edu.cn
2 School of Communication and Information Engineering, Shanghai University,
Shanghai, China
3 Department of Computer Science and Software Engineering, Concordia University,
Montreal, Canada
Abstract. We consider the problem of natural language moment local-
ization. Given an untrimmed video and a natural language query, we aim
to automatically retrieve a semantically relevant moment in the video
referred by the query sentence. Most existing methods work by projecting
visual and linguistic data into feature embedding space, then matching
the semantic similarity or ranking a set of pre-deﬁned segments to select
the moment. In this paper, we propose a novel PointerNet with local and
global contexts to solve this problem. Our proposed model ﬁrst uses a
recurrent network over words to interact visual and linguistic features
in a ﬁne-grained fashion. The word recurrence represents each clip as a
multimodal feature that captures the ﬁne-grained interaction of each clip
with all words in the query sentence. It then uses another bi-directional
recurrent network that processes all clips in the video. The clip recur-
rence reﬁnes the local context information of each clip and produces a
global context representation of the entire video. Finally, the global video
context and the local context of each clip are jointly used to determine
the start and the end positions of the moment. Extensive experimental
results demonstrate the eﬀectiveness of our proposed method.
Keywords: Vision and Language · Deep Learning · Moment
Localization
1
Introduction
Video understanding is an important problem in computer vision with many
potential applications, such as intelligent video retrieval, browsing, recommen-
dation, etc. A lot of existing work in video understanding focuses on activity
detection. There has been fruitful progress [2,22,24] in recent years. However,
previous work in video activity detection requires a set of pre-deﬁned activity
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 305–316, 2024.
https://doi.org/10.1007/978-981-99-8850-1_25

306
L. Ye et al.
classes such as eating, dancing and washing. This has limited its applications
in the real world. In addition, a video with long duration may contain complex
activities that cannot be described as a simple action class. For example, Fig. 1
shows an example of a complex video with diﬀerent activities (“open refrigera-
tor” and “pour milk”). It is more appealing to use the free-form natural language
as the query for video understanding. In this paper, we address the problem of
natural language moment localization in unconstrained videos. Speciﬁcally, given
a video and a natural language sentence, our goal is to automatically determine
the start and end positions of a moment in the video that corresponds to seman-
tic meaning of the query.
Some works [5,8,13,18–21] have been proposed to solve the natural language
moment localization problem. These methods usually treat the video and lan-
guage modalities individually and extract visual and linguistic features sepa-
rately from each modality. Then they use diﬀerent fusion strategies to integrate
these features to obtain multimodal feature representation by addition and mul-
tiplication [5], concatenation [16], pooling [9] or attentive weights [19], etc.
Most existing methods [1,5,8,19,23] for this task also require generating
candidate segments in the form of video segments. The moment localization is
then treated as a matching or ranking problem. In this way, they are able to
reduce the search space of all clips in the video to the candidate video segments
and utlize the multimodal features within these video segments. However, in
order to precisely localize the moment, these works usually need to generate
dense segments. This is time-consuming for long videos in practice. In addition,
the multimodal feature representations for these generated segments have lim-
ited contexts since the feature extraction process only considers the information
within the segment. The independent matching of each candidate segment is
suboptimal without considering the global context of the whole video.
Query: Person opens a refrigerator to pour milk.
Fig. 1. An illustration of natural language moment localization. Given an input video
and a natural language query sentence, the goal is to localize the moment in the video
referred by the query sentence. The moment (shown in blue) can potentially involve
complex activities, such as “opens a refrigerator” shown in red and “pour milk” shown
in green. (Color ﬁgure online)
What is the strategy that humans use in order to solve the natural language
moment localization problem? Given an input video and a sentence query, people
often follow a vision-language-vision reading order and go back-and-forth sev-
eral times between the video and sentence before making the prediction [7,14].
Motivated by this observation, we propose a novel PointerNet-based model [15]
for moment localization in this paper. Our model produces an eﬀective mul-
timodal representation for every clip in the video. It also captures the global

PointerNet for NLML
307
video context of the entire video and the local context of each clip. Speciﬁcally,
we introduce word recurrence that computes multimodal feature for each clip in
the video by taking into account the interaction of this clip with each word in
the query sentence. This multimodal clip feature representation captures ﬁne-
grained interaction between words in the sentence and clips in the video. In
addition, we develop temporal clip recurrence of all clips to reﬁne local context
information for each clip and build global video context by going forward and
backward over the whole video. Then the generated global video context is used
together with the local context of each clip to localize the moment. Finally, we
use PointerNet to predict soft positions for the start and the end positions of the
moment. Our network does not require segment proposals and can be trained in
an end-to-end fashion.
Our contributions can be summarized as follows. (1) We propose a novel
PointerNet with local and global video contexts for natural language moment
localization. (2) We introduce word recurrence to recurrently interact visual and
linguistic information for an eﬀective multimodal representation in a sequential
order. (3) We further introduce clip recurrence to reﬁne local context information
for each clip to build relationships of individual clips and generate global video
context of the whole video to directly predict the start and end positions of the
moment of interest.
2
Our Approach
Let X denote an input video with a sequence of T clips as X = {xt}T
t=1. A set
of moment and natural language annotations {S, ps, pe} is associated with the
video X for a speciﬁc moment, where S is a natural language sentence describing
the content of a moment that we want to retrieve. The sentence S consists of a
sequence of L words. Here ps and pe are the start position and the end position
of the corresponding moment in the video, respectively. Speciﬁcally, the input
to our network is a video X and a natural language sentence S. Our task is to
predict the start position ps and the end position pe of the moment according to
the query sentence. The overall architecture of our proposed model is illustrated
in Fig. 2.
2.1
Word Recurrence for Multimodal Clip Features
We propose a word recurrence network for multimodal clip feature extraction in
this section. The goal is to represent each clip in the video as a feature vector
that captures the information about the query sentence.
Visual Encoder: For a given clip xt in the long video X, we ﬁrst extract the
visual feature of this clip using the inﬂated 3D ConvNet (I3D) [3]. I3D inﬂates
ﬁlters and kernels of 2D ConvNet to 3D and uses these ﬁlters to extract spatio-
temporal features on a clip xt. We then summarize raw spatio-temporal I3D

308
L. Ye et al.
...
GRU
GRU
GRU
person   opens   a   laptop
...
I3D
...
Glove
...
...
...
Word Recurrence
Video Encoder
Language Encoder
Clip Recurrence
Pointer Prediction
...
start
end
start
prob.
end
prob.
Fig. 2. An overview of our approach. The proposed network consists of several com-
ponents, including video and language encoders, word recurrence for multimodal clip
features, clip recurrence, and pointer prediction for moment localization. Our model
ﬁrst extracts visual features for clips in the video and linguistic features for words
in the sentence. Then a multimodal feature representation for each clip is generated
by the word recurrence. Video clip recurrence is further used to reﬁne local context
information for each clip and build global video context of the whole video. Finally, a
PointNet uses the global video context and the local context of each clip to predict the
start and the end positions of the moment in the video.
features over spatial regions as a visual feature vector vt by a global average
pooling.
vt = SpatialPool(I3D(xt)),
t = 1, 2, ..., T
(1)
where I3D(xt) extracts the spatial-temporal I3D features from the clip xt and
SpatialPool(·) denotes the global average pooling over spatial regions. We use
dv to denote the dimension of vt, i.e. vt ∈Rdv.
Language Encoder: In order to capture ﬁne-grained word-level representa-
tions, we represent each word by its Glove [11] word embedding vector and then
use a fully connected layer to transform the original word embedding to the ﬁnal
word-level representations. The query sentence is represented as {wl}L
l=1 ∈Rdw,
where L is the total number of words in the sentence and dw is the dimension
of the word vector representation.
Multimodal Clip Features: When people solve the moment localization task,
they usually examine the language words and the video in a back and forth
manner and gradually correlate linguistic semantics with visual contents [14].
Inspired by this, we propose to combine the visual feature of each clip with the
linguistic feature of every word using a sequential process. Concretely, we ﬁrst
concatenate every clip feature vt and every word feature wl along the channel
dimension to result in a multimodal representation as follows:
ft,l = [vt, wl] ∈Rdv+dw, ∀t ∈{1, 2, ..., T} ∀l ∈{1, 2, ..., L}
(2)
The feature vector ft,l contains both the visual information of the t-th clip vt and
the linguistic information of the l-th word wl. For a particular clip vt, we use

PointerNet for NLML
309
gated recurrent units (GRU) [4] to recurrently aggregate multimodal features
over all words in the sentence as:
zt,l = σ(Wzft,l + Uzht,l−1 + bz)
(3a)
rt,l = σ(Wrft,l + Urht,l−1 + br)
(3b)
ht,l = zt,l ⊙ht,l−1 + (1 −zt,l) ⊙
(3c)
tanh(Whft,l + Uh(rt,l ⊙ht,l−1) + bh)
(3d)
where
l = 1, 2, ..., L
(3e)
where Wz, Uz, Wr, Ur, Wh, Uh, bz, bz and bh are parameters of the GRU. σ
denotes the sigmoid function and ⊙is the Hadamard product. zt,l and rt,l are the
update gate and reset gate, respectively. ht,l is the hidden state also known as the
output after ﬁnishing reading the l-th word in the sentence. When sequentially
taking the multimodal features over words, the hidden state ht,l is updated for
every clip t in the video at the same time according to the current input ft,l
and the previous hidden state ht,l−1 by controlling the information ﬂow through
gates zt,l and rt,l. The recurrence of GRU is performed over words to make each
clip interact with each word in order to capture ﬁne-grained interaction between
each clip in the video and each word in the query sentence. The ﬁnal output of
the GRU ht,L at the last time step L has read all words in the sentence and is
used as the multimodal feature representation for the clip. We use ct = ht,L to
denote this multimodal feature for the clip t.
2.2
Clip Recurrence and Global Video Context
The generated multimodal clip representation in Sect. 2.1 only depends on the
local context within a single clip. However, people tend to look over the entire
video to get a comprehensive context of the video ﬁrst in order to localize the
moment associated with the query sentence. Inspired by this observation, In this
section, we ﬁrst employ clip recurrence of all clips in the entire video to reﬁne
local context information for each clip and obtain a global context representation
of the entire video. Then the global video context as well as the reﬁned local
context of each clip is jointly used to accurately localize the moment as shown
in Fig. 3.
For a sequence of multimodal clip representations {ct}T
t=1 over clips in the
video, we ﬁrst encode all clips of the video via clip recurrence by a temporal
GRU which updates the hidden state along the video clip order as:
−→
z′
t = σ(−−→
Wz′ct + −→
Uz′−−→
h′
t−1 + −→
bz′)
(4a)
−→
r′
t = σ(−−→
Wr′ct + −→
Ur′−−→
h′
t−1 + −→
br′)
(4b)
−→
h′
t =
−→
z′
t ⊙
−−→
h′
t−1 + (1 −
−→
z′
t) ⊙
(4c)
tanh(−−→
Wh′ct + −→
Uh′(
−→
r′
l ⊙
−−→
h′
t−1) + −→
bh′)
(4d)
where
t = 1, 2, ..., T
(4e)

310
L. Ye et al.
Similarly, −−→
Wz′, −→
Uz′, −−→
Wr′, −→
Ur′, −−→
Wh′, −→
Uh′, −→
bz′ and −→
bh′ are parameters of this GRU.
−−→
z′
t,L and −−→
r′
t,L are the update and reset gates of the temporal GRU. The ﬁnal
hidden state at the last time step T (i.e. −→
h′
T ) has gone over all clips in the
forward direction.
We use another temporal GRU to produce backward hidden state ←−
h′
t in the
opposite direction in the video as follows:
←−
z′
t = σ(←−−
Wz′ct + ←−
Uz′←−−
h′
t+1 + ←−
bz′)
(5a)
←−
r′
t = σ(←−−
Wr′ct + ←−
Ur′←−−
h′
t+1 + ←−
br′)
(5b)
←−
h′
t =
←−
z′
t ⊙
←−−
h′
t+1 + (1 −
←−
z′
t) ⊙
(5c)
tanh(←−−
Wh′ct + ←−
Uh′(
←−
r′
l ⊙
←−−
h′
t+1) + ←−
bh′)
(5d)
where
t = T, T −1, ..., 1
(5e)
The multimodal clip feature with local context is updated by the combination
of the forward and backward hidden states at each clip t as h′
t = [−→
h′
t, ←−
h′
t]. The
encoded h′
t contains the progressive context information over time steps, which
is essential for moment boundary identiﬁcation [10,17]. Then the global video
context can be obtained as G = [−→
h′
T , ←−
h′
1]. G gathers the ﬁnal hidden states in
both directions for global contextual multimodal information through all clips
and words.
c2
ct-1
ct
...
Bi-GRU
Bi-GRU
Bi-GRU
c1
Bi-GRU
...
...
ht
ht
h'2
h't-1
h't
...
h'1
G
Concat
Multimodal clip features
Global video context
Predicted start position
Predicted end position
Fig. 3. Clip Recurrence and global video context. The multimodal features of clips in
the video recurrently interact with other clips via a bi-directional GRU to reﬁne local
context information for each clip. The ﬁnal hidden states of the bi-directional GRU are
used as the global video context representation.

PointerNet for NLML
311
2.3
PointerNet-Based Moment Localization
PointerNet [15] is introduced to predict discrete positions based on the input
sequence. It ﬁrst proposes to use a pointer to indicate the potential position over
the input sequence to solve tasks such as convex hull and Delaunay triangulation.
Inspired by the pointer mechanism, we utlize the global video context of the video
and the local context of each clip to produce two pointers (i.e. start position and
end position) corresponding to the localized moment. In the following, we use
the process of the start position prediction as an example. The same technique
can also be applied for the end position prediction. To localize the start position,
the pointer function is calculated as follows:
st = wT tanh(Wah′
t + WbG),
∀t
(6)
where Wa, Wb and w are learnable parameters of the pointer function. st denotes
the score of a speciﬁc clip t being the start position of the moment. We then
calculate the probability of the clip being the start position by normalizing the
responses predicted by Eq. 6 across all clips in the video by:
et =
exp(st)
T
t′=1 exp(s′
t)
,
∀t
(7)
We propose to use a soft position prediction instead of the hard selection
strategy for position prediction. Speciﬁcally, we generate a sequence of integers
as the same length as the video, i.e., {1, 2, ..., T}, which represent possible clip
positions in the video. We can then calculate the weighted average for a start
position as follows:
ps =
T

t=1
t · et
(8)
where ps can be interpreted as a “soft pointer” indicating the start position of the
moment. We use another pointer function (with its own learnable parameters)
similar to Eq. 6 to generate the end position scores of all clips. We then predict
the end position pe by Eq. 7 and Eq. 8.
2.4
Training
Given a pair of predicted positions (ps, pe) and ground truth positions (pgt
s ,
pgt
e ), we design a normalized mean square error (MSE) loss to directly regress
the predicted positions deﬁned as: Lmse =
(ps−pgt
s )2+(pe−pgt
e )2
pgt
e −pgt
s
. The MSE loss
measures the absolute diﬀerence at the start and the end positions between the
prediction and the ground truth. However, only using MSE loss may fail to
evaluate a moment that has a reasonable overlap with ground truth. Therefore,
we additionally deﬁne an IoU loss: Liou = −log

[ps,pe]∩[pgt
s ,pgt
e ]
[ps,pe]∪[pgt
s ,pgt
e ]
2
, to measure
whether the predicted moment matches with the ground truth. where [ps, pe]
denotes the temporal interval with ps and pe as the start and the end positions,

312
L. Ye et al.
respectively. [pgt
s , pgt
e ] is similarly deﬁned. The overall loss of our network is
composed of these two loss functions with a weight parameter α to balance
between two losses as: L = Lmse + αLiou.
3
Experiments
We evaluate our approach on three publicly available natural language moment
localization datasets, including Charades-STA [5], ActivityNet Captions [6], and
TACoS [12]. We follow the evaluation metrics used in [20] (“R@1 IoU@σ” and
“mean IoU”) to measure the performance of the proposed network.
Implementation Details: We truncate input sentences to keep the maximum
length as 20 words and embed each word into a 300 dimensional Glove word
vector. Then each word vector is transformed to dw = 512 with a fully con-
nected layer and concatenated with the visual feature of each clip of dimension
dv = 1024 to result in a 1536 dimensional multimodal feature representation.
The whole network is trained in an end-to-end fashion by Adam optimization
algorithm with an initial learning rate at 1e−4 and a weight decay of 1e−3.
Table 1. Ablation study on the Charades-STA dataset.
Methods
R@1, IoU@0.3
R@1, IoU@0.5
R@1, IoU@0.7
mIoU
Baseline
62.82
46.07
21.34
41.57
Baseline+WR
64.49
47.23
23.06
42.93
Baseline+GVC
65.00
48.41
25.08
44.10
Baseline+WR+GVC (Ours)
69.87
55.75
30.22
47.83
Ours w/iou
55.12
36.52
19.17
36.33
Ours w/mse
65.23
48.11
23.06
43.54
Ours w/mse+iou
69.87
55.75
30.22
47.83
3.1
Ablation Study
To verify the eﬀectiveness of each component of our network, we conduct ablation
experiments on the Charades-STA dataset with diﬀerent variants of the proposed
method as follows:
– Baseline: this model does not have the word recurrence for multimodal clip
features and global video context. Instead, it uses a LSTM to encode the whole
sentence as a single sentence vector and then concatenates the sentence vector
with the visual feature of each clip for multimodal representation. It predicts
the start and end positions based on the local context of each video clip only.
– Baseline+WR: this model adds the word recurrence (WR) to the baseline
model.

PointerNet for NLML
313
– Baseline+GVC: this model adds the global video context (GVC) to the base-
line model.
– Baseline+WR+GVC (i.e. ours): this is our proposed model that uses both
WR and GVC.
The experimental results are shown in Table 1. It can be observed that either
word recurrence for multimodal clip features or global video context improves the
moment localization performance on its own compared with the baseline. The
complete model with both WR and GVC further improves the performance. The
results demonstrate the eﬀectiveness of the proposed multimodal representation
and the global video context for the moment localization problem. Meanwhile,
the network trained with both MSE loss and IoU loss achieves the best perfor-
mance compared with using either one.
3.2
Comparison with State-of-the-art
We compare our network with several competitive methods including moment
contextual network (MCN) [1], cross-model temporal regression localizer
(CTRL) [5], multilevel integration and multi-task loss (MIML) [19], moment
alignment network (MAN) [21], attention based localization (ABLR) [20], seman-
tic matching reinforcement learning (SMRL) [16], Boundary proposal network
(BPnet) [18] and span-QA net (VSLNet) [22].
Table 2. Comparison of diﬀerent methods on the Charades-STA dataset, the Activi-
tyNet Captions dataset and the TACoS dataset, respectively.
Methods
Charades-STA
ActivityNet Caption
TACoS
R@1, IOU@0.5 R@1, IOU@0.7 R@1, IoU@0.3 R@1, IoU@0.5 R@1, IOU@0.3 R@1, IOU@0.5
MCN [1]
17.46
8.01
21.37
9.58
10.06
5.70
CTRL [5]
21.42
7.15
28.70
14.00
17.40
12.90
ABLR [20]
24.36
9.01
55.67
36.79
19.50
9.40
SMLR [16]
24.36
11.17
–
–
20.25
15.95
MIML [19]
35.60
15.80
45.30
27.70
–
–
MAN [21]
46.53
22.72
–
–
–
–
BPNet [18]
50.75
31.64
42.07
24.69
25.96
20.96
VSLNet [22] 54.19
35.22
43.22
26.16
29.61
24.27
Ours
55.75
30.22
56.12
34.53
31.46
17.59
Table 2 show the comparisons on the Charades-STA dataset, the ActivityNet
Captions dataset and the TACoS dataset, respectively. Our network outper-
forms these approaches in most cases. Our method obtains better results in
R@1,IoU@0.5 on the Charades-STA dataset and in R@1,IoU@0.3 on the TACoS
dataset, but worse results with higher IOU compared with VSLNet. It is probably
because the VSLNet incorporates the concept of multi-paragraph QA for boost-
ing their performance on long videos. For the results on the ActivityNet Captions
dataset, our network achieves superior performance in terms of R@1,IoU@0.3 but

314
L. Ye et al.
slightly behind ABLR in terms of R@1,IoU@0.5. However, their method is much
more unstable and the results varies dramatically over diﬀerent datasets where
the performance degenerates on the Charades-STA dataset and TACoS dataset.
Fig. 4. Qualitative examples of our proposed network. The two curve graphs show the
probabilities of the start and the end positions of clips over a video, respectively. The
moment in red is our predicted moment and the moment in grey is the ground truth
for the given query shown at the top of the video. (Color ﬁgure online)
Some qualitative examples are presented in Fig. 4. In the top example, our
proposed network correctly localizes the moment involving two activities “open
a mobile” and “another person comes running”. In the second example, our net-
work localizes an entire process of a long continuous activity “undressed out of
jacket”. The third example shows the situation where the background of the
scene remains unchanged after the moment “walks through the doorway”. Our
model successfully localizes the precise moment once the activity ends. In addi-
tion, we visualize the probabilities deﬁned in Eq. 7 of the start and the end
positions of clips over a video, respectively. The two curve graphs below each
corresponding video in Fig. 4 show how our network responses to the input and
identiﬁes the localized moment.
4
Conclusion
In this paper, we have presented a novel PointNet-based model with local and
global contexts for natural language moment localization. The proposed network
uses word recurrence or an eﬀective multimodal feature representation of each
clip. It then uses clip recurrence to reﬁne local context information for each clip

PointerNet for NLML
315
and build global video context. Finally, the PointerNet is adopted to produce
start and end positions over clips for the moment of interest. Experimental results
on three datasets shows that the supervisor performance of the proposed method.
Acknowledgments. This work was supported in part by the National Natural Sci-
ence Foundation of China (Grant No. 62102289) and in part by the Zhejiang Provincial
Natural Science Foundation (Grant No. LQ22F020005).
References
1. Anne Hendricks, L., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.:
Localizing moments in video with natural language. In: IEEE International Con-
ference on Computer Vision, pp. 5803–5812 (2017)
2. Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: ActivityNet: a
large-scale video benchmark for human activity understanding. In: IEEE Confer-
ence on Computer Vision and Pattern Recognition, pp. 961–970 (2015)
3. Carreira, J., Zisserman, A.: Quo vadis, action recognition? A new model and the
kinetics dataset. In: IEEE Conference on Computer Vision and Pattern Recogni-
tion, pp. 6299–6308 (2017)
4. Cho, K., et al.: Learning phrase representations using RNN encoder-decoder for
statistical machine translation. In: Conference on Empirical Methods in Natural
Language Processing (2014)
5. Gao, J., Sun, C., Yang, Z., Nevatia, R.: TALL: temporal activity localization via
language query. In: IEEE International Conference on Computer Vision, pp. 5267–
5275 (2017)
6. Krishna, R., Hata, K., Ren, F., Fei-Fei, L., Niebles, J.C.: Dense-captioning events
in videos. In: International Conference on Computer Vision (2017)
7. Liu, C., Lin, Z., Shen, X., Yang, J., Lu, X., Yuille, A.L.: Recurrent multimodal
interaction for referring image segmentation. In: IEEE International Conference
on Computer Vision (2017)
8. Liu, M., Wang, X., Nie, L., He, X., Chen, B., Chua, T.S.: Attentive moment
retrieval in videos. In: ACM SIGIR Conference on Research & Development in
Information Retrieval, pp. 15–24 (2018)
9. Liu, M., Wang, X., Nie, L., Tian, Q., Chen, B., Chua, T.S.: Cross-modal moment
localization in videos. In: ACM International Conference on Multimedia, pp. 843–
851 (2018)
10. Ma, S., Sigal, L., Sclaroﬀ, S.: Learning activity progression in LSTMs for activ-
ity detection and early detection. In: IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1942–1950 (2016)
11. Pennington, J., Socher, R., Manning, C.: GloVe: global vectors for word represen-
tation. In: Conference on Empirical Methods in Natural Language Processing, pp.
1532–1543 (2014)
12. Regneri, M., Rohrbach, M., Wetzel, D., Thater, S., Schiele, B., Pinkal, M.: Ground-
ing action descriptions in videos. Trans. Assoc. Comput. Linguist. 1, 25–36 (2013)
13. Tunguturi, M.: Moment localization using multi-scale 2D temporal adjacent net-
works and natural language. Int. J. Mach. Learn. Sustain. Dev. 4(3), 1–10 (2022)
14. Underwood, G., Jebbett, L., Roberts, K.: Inspecting pictures for information to
verify a sentence: Eye movements in general encoding and in focused search. Q. J.
Exp. Psychol. Sect. A 57(1), 165–182 (2004)

316
L. Ye et al.
15. Vinyals, O., Fortunato, M., Jaitly, N.: Pointer networks. In: Advances in Neural
Information Processing Systems, pp. 2692–2700 (2015)
16. Wang, W., Huang, Y., Wang, L.: Language-driven temporal activity localization:
a semantic matching reinforcement learning model. In: IEEE Conference on Com-
puter Vision and Pattern Recognition,
17. Wei, Z., et al.: Sequence-to-segment networks for segment detection. In: Advances
in Neural Information Processing Systems, pp. 3507–3516 (2018)
18. Xiao, S., et al.: Boundary proposal network for two-stage natural language video
localization. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence. vol.
35, pp. 2986–2994 (2021)
19. Xu, H., He, K., Plummer, B.A., Sigal, L., Sclaroﬀ, S., Saenko, K.: Multilevel lan-
guage and vision integration for text-to-clip retrieval. In: AAAI Conference on
Artiﬁcial Intelligence. vol. 33, pp. 9062–9069 (2019)
20. Yuan, Y., Mei, T., Zhu, W.: To ﬁnd where you talk: Temporal sentence localization
in video with attention based location regression. In: AAAI Conference on Artiﬁcial
Intelligence. vol. 33, pp. 9159–9166 (2019)
21. Zhang, D., Dai, X., Wang, X., Wang, Y.F., Davis, L.S.: MAN: moment alignment
network for natural language moment retrieval via iterative graph adjustment. In:
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1247–1257
(2019)
22. Zhang, H., Sun, A., Jing, W., Zhen, L., Zhou, J.T., Goh, R.S.M.: Natural language
video localization: A revisit in span-based question answering framework. IEEE
Trans. Pattern Anal. Mach. Intell. 44(8), 4252–4266 (2021)
23. Zhang, L., Radke, R.J.: Natural language video moment localization through
query-controlled temporal convolution. In: Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision, pp. 682–690 (2022)
24. Zheng, M., Huang, Y., Chen, Q., Liu, Y.: Weakly supervised video moment local-
ization with contrastive negative sample mining. In: AAAI Conference on Artiﬁcial
Intelligence. vol. 36, pp. 3517–3525 (2022)

Self-supervised Meta Auxiliary Learning
for Actor and Action Video Segmentation
from Natural Language
Linwei Ye(B)
and Zhenhua Wang
College of Computer Science and Artiﬁcial Intelligence, Wenzhou University,
Wenzhou, China
ylw@wzu.edu.cn
Abstract. This paper addresses the problem of actor and action video
segmentation from natural language. Given a video and a language
query, the goal is to segment the actor and its action described by the
query. Existing methods focus on exploring elaborated multimodal fea-
ture fusion networks to combine visual and linguistic features for an
eﬀective multimodal representation directly learnt from this labeled seg-
mentation task. In this paper, we propose a novel self-supervised meta
auxiliary learning method to improve the primary segmentation task by
adding an auxiliary task for better generalization. The auxiliary task is
established to reconstruct the input sentence representation so that the
multimodal representation can be adapted to a speciﬁc query. In addi-
tion, the auxiliary task does not require additional labels. It can also be
used in test time to update a multimodal representation according to a
speciﬁc query in a self-supervised way.
Keywords: Language and vision · Auxiliary learning · Actor and
action video segmentation
1
Introduction
Understanding actors and their actions in a video is a fundamental problem in
video understanding. There has been lots of existing work in action detection
[2,19] and segmentation [17]. However, these studies rely on a list of predeﬁned
action classes to recognize speciﬁc human activities such as “jumping”, “walking”
and “standing”. This greatly restricts the potential applications of these methods
in the real world. In contrast, a natural language sentence provides a richer and
more ﬂexible way of specifying actors and actions in a video. In this paper,
we consider the problem of actor and action video segmentation from natural
language as [4]. The goal is to segment a speciﬁc actor and his/her action referred
by an arbitrary natural language query. Figure 1 shows an example of this task.
A general way of solving vision and language segmentation problem is to
design an elaborated fusion strategy for a multimodal representation. These
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 317–328, 2024.
https://doi.org/10.1007/978-981-99-8850-1_26

318
L. Ye and Z. Wang
Fig. 1. Illustration of actor and action video segmentation from natural language.
Given an input video and a natural language query. The language query refers an
actor and his/her action. The goal is to generate the pixel-level segmentation mask
speciﬁed by the query. In this ﬁgure, the masks colored with red and green correspond
to the query sentences with the same color at the top of each video. (Color ﬁgure
online)
methods generally use convolutional neural networks to extract visual features
and recurrent neural networks to encode linguistic features. Then they combine
the visual features with linguistic features for a multimodal representation by dif-
ferent strategies such as concatenation [1,5], dynamic ﬁlters [4] or cross-guided
attention [14,20]. Afterwards, a segmentation mask is produced based on the
multimodal representation. The linguistic information is implicitly learned along-
side the segmentation task. Diﬀerent from these previous methods, we argue that
the knowledge of visual and linguistic representations should be able to transfer
between both visual and linguistic domains in order for an eﬀective multimodal
representation.
Learning to reconstruct the language sentence from the input has been shown
to be eﬀective for constructing a more robust multimodal representation in vision
and language tasks (e.g. [10,12]). In addition, the recent progress of auxiliary
learning shows its ability of improving the generalization of a primary task
[9,13]. The input language query in our task can potentially serve as a natu-
ral supervision signal without any additional labeling.
Motivated by these observations, we propose a novel self-supervised auxiliary
learning method for actor and action video segmentation from natural language.
The proposed model consists of a feature extraction network, a primary segmen-
tation network and an auxiliary language reconstruction network. The feature
extraction network is a backbone network used to extract visual and linguistic
features from the input. The primary segmentation network leverages the visual
and linguistic features to build the spatial attention and language-context vision
attention, which drives the network to focus on important spatial regions corre-
sponding to the natural language query. The auxiliary language reconstruction
network integrates visual contexts with word-level linguistic features to gener-
ate vision-context language attention. The visually attentive features are then
used to reconstruct the input sentence representation as the auxiliary task. This
self-supervised auxiliary task allows the network to optimize the shared fea-

Meta Auxiliary Learning
319
ture extraction parameters for a better multimodal representation. Then the
updated shared parameters can be used for the primary segmentation task. The
self-supervised learning strategy does not require additional labels and can also
be deployed during test time to adapt the model parameters to a speciﬁc test
example. An overview of our approach is presented in Fig. 2.
The main contributions of our work are threefold: (1) We propose a self-
supervised meta auxiliary learning method to improve the primary segmenta-
tion task for the problem of actor and action video segmentation from natural
language. (2) We introduce spatial attention and language-context vision atten-
tion to generate eﬀective multimodal feature representation. We also introduce
vision-context language attention to reconstruct the sentence representation for
the auxiliary task in a self-supervised fashion. (3) The experimental results on
two actor and action video segmentation datasets demonstrate that the eﬀec-
tiveness of the proposed method.
2
Our Approach
CNN
Natural language query:
man climbing
rocks in the center
Glove
man
climbing
center
...
Bi-LSTM
...
Decoder
...
Segmentation
Loss
Reconstruction
Loss
Feature Extraction Network
Primary Segmentation Network
Auxiliary Reconstruction Network
visual feature
spatial attention
feature
language-context vision
attention feature
spatial coordinate
feature 
vision-context language
attention feature
Fig. 2. Overview of our approach. The proposed model consists of a feature extraction
network, a primary segmentation network and an auxiliary language reconstruction
network. The auxiliary language reconstruction network is used to reconstruct the
input sentence representation in a self-supervised way. It adapts the multimodal repre-
sentation and the network parameters to the speciﬁc input query in the meta auxiliary
training ( green dash lines). The updated parameters will be used for training the pri-
mary segmentation task (red dash lines) or producing segmentation masks in testing.
(Color ﬁgure online)
2.1
Feature Extraction Network
Let I denote an input frame from a video. We ﬁrst use a convolutional neu-
ral network (i.e. DeepLab [3]) to extract visual features from I represented as

320
L. Ye and Z. Wang
f v ∈R(H·W )×Cv, where H · W is the spatial size of the feature map and Cv is
the feature channel dimension. We use f v
i ∈RCv to denote the feature vector
corresponding to a particular spatial grid location i where i ∈{1, 2, ..., H · W}.
For a given natural language query with L words, we represent each word in
the query with a pre-trained Glove word embedding vector [11] as {el}L
l=1 where
el ∈RCe is the word vector of the l-th word and Ce is the dimension of the word
embedding. Then a bidirectional LSTM is adopted to encode both forward and
backward relations between words.
We use f w
l
∈RCw to denote the ﬁnal word-level feature representation of
the l-th word, which is the concatenation of the bidirectional hidden vectors of
LSTM at the corresponding time step. Finally, we use f w ∈RL×Cw to denote
the feature representation of all L words in the query sentence.
2.2
Primary Segmentation Network
The visual and linguistic features in Sect. 2.1 are only extracted in their own
modalities separately. For the problem of actor and action video segmentation
from natural language, the task requires more detailed understanding of visual
and linguistic inputs. Self-attention mechanism [15] has been proven useful to
capture meaningful task-relevant features and suppress unimportant ones from
redundant multimodal features. Speciﬁcally, we propose spatial attention (as
shown in Fig. 3(a)) and language-context visual attention (as shown in Fig. 3(b))
to eﬀectively extract long-range relations over spatial regions and language-
context visual features.
Concretely, with the visual feature vector f v
i at each spatial grid location i,
we ﬁrst use three linear transformation layers to produce a set of representations
for query qi = Wqf v
i , key ki = Wkf v
i and value vi = Wvf v
i , where Wq, Wk, Wv ∈
RCa×Cv are parameters of the linear transformation layers. Then for any spatial
location pair (i, j) in the feature map f v, the similarity between key ki and
query qj can be calculated by qT
j ki. Afterwards, we use a softmax function to
normalize the similarity and aggregate similarity values over all spatial locations.
The operation can be summarized as:
f sa
i
=

j
softmax(qT
j ki)vj
(1)
The resultant spatial attention features f sa
i
capture correlations between diﬀer-
ent spatial locations in the visual features.
In order to combine the linguistic context information with the visual fea-
ture representation for guiding the segmentation, we further propose language-
context vision attention to explore correlations between visual features and corre-
sponding words. Given the visual feature map f v ∈R(H·W )×Cv and the linguistic
features f w ∈RL×Cw, we project the visual and linguistic features to the same
semantic space using a 1 × 1 convolution layer and a linear layer, respectively. It
results in ˜f v ∈R(H·W )×Ca and ˜f w ∈RL×Ca. Then we transpose ˜f w and multiply
it with ˜f v to generate a similarity matrix of size (H ·W)×L. Similarly, a softmax

Meta Auxiliary Learning
321
function is applied along the L axis to measure the correlations of each word to
diﬀerent spatial regions. Finally, we collect the weighted correlation values over
all words as follows:
f va = softmax

˜f v( ˜f w)T 
˜f w
(2)
where f va ∈R(H·W )×Ca denotes the language-context vision attention features.
The overall process is visualized in Fig. 3(a).
Following previous work in referring segmentation [5,8], we also extract spa-
tial coordinate features f sc ∈R(H·W )×8 to capture the spatial information which
is crucial for handling query sentences with relative position words, such as “left”
and “right”. To produce the ﬁnal segmentation mask, we concatenate the visual
features f v, spatial attention features f sa, the language-context vision attention
features f va and the spatial coordinate features f sc to form a multimodal feature
representation f m = Concat(f v, f sa, f va, f sc).
In addition, we extract visual features at three levels from ResNet-based
DeepLab blocks (Res3, Res4 and Res5) and use above spatial attention and
language-context vision attention methods to get multimodal feature represen-
tations {f m
s }S
s=1(s = 1, 2, 3) corresponding to three diﬀerent levels. Then a con-
volutional LSTM (ConvLSTM) [16] is adopted to reﬁne multi-level features as
follows:
(Hm
s , Cm
s ) = ConvLSTM(f m
s , Hm
s−1, Cm
s−1)
(3)
where Hm
s and Cm
s represent the hidden state and the cell state of each time step
over diﬀerent level s. The ﬁnal hidden state Hm
S of ConvLSTM is then used to
as the ﬁnal multimodal feature representation. Finally, we feed Hm
S to another
convolutional layer to produce a segmentation mask M for the input frame. We
learning the primary segmentation network using a binary cross entropy loss
with a ground truth segmentation mask.
2.3
Auxiliary Language Reconstruction Network
In this section, we propose an auxiliary task to help improve the segmentation
performance. Here we propose to reconstruct the linguistic sentence representa-
tion from the multimodal feature representation as the auxiliary task.
We align the channel dimension of visual and linguistic features by applying
a 1 × 1 convolution layer to transform f v to ¯f v ∈R(H·W )×Ca and applying a
linear layer to transform f w to ¯f w ∈RL×Ca as shown in Fig. 3(c). A vision-
context similarity matrix can be calculated by ¯
f w( ¯f v)T . We then use a softmax
function along the H · W axis to normalize the correlation between each spatial
grid location of the visual features and each word. The vision-context language
attention features are obtained by aggregating all weighted visual information as
f wa = softmax

¯f w( ¯f v)T 
¯f v, where f wa
l
∈R1×Ca to denote the corresponding
feature for the l-th word in the sentence.
We use the vision-context language attention features to reconstruct the sen-
tence representation by averaging over words and projecting it into the same

322
L. Ye and Z. Wang
L x Ca
(H * W) x Ca
L x Ca
(H * W) x Ca
(H * W) x Ca
(H * W) x L
(H * W) x L
L x (H * W)
fva
fsa
L x (H * W)
(H * W) x Ca
(H * W) x Cv
Ca x L 
L x Ca
L x Cw
(H * W) x (H * W)
(H * W) x Ca
(H * W) x (H * W)
Ca x (H * W)
(H * W) x Ca
key
value
fv
(H * W) x Ca
Trans.
(a) Spatial Attention
Mul.
Softmax
query
Mul.
fv
fw
Linear
Trans.
Mul.
Softmax
Mul.
(H * W) x Cv
Ca x (H * W) 
L x Cw
fw
fv
Conv.
Trans.
Mul.
Softmax
Mul.
(b) Language-Context Vision Attention (c) Vision-Context Language Attention
fwa
Conv.
Linear
Fig. 3. Illustration of the proposed attention methods. (a) Spatial attention and (b)
Language-context vision attention are used to eﬀectively extract long-range relations
over spatial regions and language-context visual features for the primary segmentation
network. (c) Vision-context language attention incorporate visual features into linguis-
tic features to reconstruct the input sentence representation for learning an adaptive
multimodal representation.
dimension of Ce with a linear layer as:
Sr = W
 1
L
L

l=1
f wa
l

+ b
(4)
where Sr ∈RCe is the reconstructed sentence representation and W ∈RCe×Ca.
We also average the word embedding vectors {el}L
l=1 as a ground truth sentence
representation S ∈RCe. The loss function of the auxiliary reconstruction net-
work can be written as Laux(I, S) = ||Sr −S||2, where ||·|| denotes the L2 norm.
The mean square error loss trains the network to learn a multimodal represen-
tation that can be used to reconstruct the original language query sentence.
2.4
Meta Auxiliary Training
Assume that we have a set of training instances (I(n), S(n), Y (n)) (n = 1, 2, ..., N),
where I(n) and S(n) are a video frame and a sentence representation of a language
query, respectively. Note that S(n) represents an averaged pre-trained Glove word
vectors here for brevity instead of raw text. Y (n) is a ground truth segmentation
mask. Let θm denote the parameters of the feature extraction network. The
output of the feature extraction network is used as the input to both the primary
and auxiliary tasks. Let θpri and θaux represent the task-speciﬁc parameter sets
for the primary segmentation network and auxiliary language reconstruction

Meta Auxiliary Learning
323
network, respectively. We use Θ = {θm, θpri, θaux} to denote the entire model
parameters. A straightforward way of learning the parameters is to optimize one
of the following two loss functions:
min
Θ
1
N
N

n=1
Lpri(I(n), Y (n); Θ)
(5)
min
Θ
1
N
N

n=1

Lpri(I(n), Y (n); Θ) + Laux(I(n), S(n); Θ)

(6)
We call Eq. 5 and Eq. 6 the primary training and the joint training, respec-
tively. Note that the primary training does not consider the network branch for
the auxiliary task, so it only learns the parameters {θm, θpri} corresponding to
the feature extraction network and the primary segmentation network. The joint
training can be seen as a form as multi-task learning.
In this paper, we propose a meta auxiliary training approach to learn the
model parameters. During testing, the model parameters are ﬁne-tuned by the
network given a speciﬁc input (i.e. video frame and language query).
We treat the language query as a self-supervised label for the auxiliary task
(i.e. language reconstruction). During testing, we use this auxiliary task to adapt
the model parameters for each test example. This gives our model more ﬂexibility
to adapt to each test example. During training, we use a meta-learning paradigm
to learn model parameters that facilitate such adaptation during testing.
We ﬁrst describe how to adapt the learned model parameters Θ during test-
ing. Let I and S denote a frame and a language representation during testing.
We perform a small number of gradient updates of the model parameters Θ
using the loss Laux, i.e.
Θ ←Θ −α∇Θ(Laux(I, S); Θ)
(7)
Note that since Laux(I, S; Θ) only involves {θm, θpri}, the update in Eq. 7 will
only change {θm, θaux}, but not θpri. We use θm and θaux to denote the cor-
responding parameters after this update. We can consider Θ from Eq. 7 as the
model parameters speciﬁcally tuned to the test example (I, S). We then predict
the segmentation mask for (I, S) using Θ.
During meta-training, our goal is to learn Θ that has the following prop-
erty. For a given training example (I(n), S(n), Y (n)), we ﬁrst apply Eq. 7 on
(I(n), S(n)) so Θ becomes Θ(n). We would like Θ(n) to produce a segmentation
mask that matches Y (n). In other words, the performance of Θ is measured by
Lpri(I(n), Y (n); Θ). During meta-training, the model parameters Θ are trained
by optimizing Lpri(I(n), Y (n); Θ) across all training examples as follows:
min
Θ
N

n=1
Lpri(I(n), Y (n); Θ(n))
(8)

324
L. Ye and Z. Wang
Note that the optimization in Eq. 8 is performed over Θ, but Lpri(I(n),
Y (n); Θ(n)) is deﬁned in terms of Θ(n) which are the parameters obtained after
performing the update in Eq. 7 on (I(n), S(n)) of the n-th training example.
Algorithm 1 shows an outline of the meta auxiliary learning.
Algorithm 1: Meta Auxiliary Training
Data: Network parameters: Θ = {θm, θpri, θaux}; learning rate: α, β
while not done do
Sample a batch of training data {(I(n), S(n), Y (n))}N
n=1
for each training sample n do
Compute adapted parameters Θ(n) from Θ as:
Θ(n) ←Θ −α∇ΘLaux(I(n), S(n); Θ)
Update: Θ ←Θ −β 
n ∇ΘLpri(I(n), Y (n); Θ(n))
end
end
3
Experiments
Datasets: We perform comprehensive evaluations on two public actor and
action video segmentation datasets including Actor-Action Dataset (A2D)
dataset [17] and Joint- annotated HMDB (JHMDB) dataset [6]. They are orig-
inally collected for actor and action video segmentation. The nature language
descriptions are complemented by [4] for the task in this paper.
Evaluation Metrics: To fairly compare with other methods, we follow previous
work [4,14] to use overall and mean intersection-over-union (Overall IoU and
Mean IoU) and Precision@X (prec@X) to evaluate the performance of methods.
Implementation Details: We truncate input sentences to keep the maximum
length as 20 words and embed each word into a pre-trained Glove word vector
with Ce = 300. Then the hidden size of the bidirectional LSTM used to encode
words is 500, which results in Cw = 1000 dimensional feature for each word.
The dimension of the visual feature is also set to Cv = 1000 and the dimension
of the projected space for attention is Ca = 500. We set the learning rate α for
auxiliary reconstruction network as 5e−5 and the learning rate β of training for
the primary segmentation network as 2.5e−4.
3.1
Ablation Study
We verify the eﬀectiveness of the proposed meta auxiliary learning. We com-
pare our method with two alternative learning methods: (1) training only with

Meta Auxiliary Learning
325
Table 1. Ablation study of auxiliary learning on A2D Sentences dataset in terms of
prec@X, meanIoU and Overall IoU.
Method
Overlap
IoU
prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9 Overall Mean
Primary
51.5
44.3
34.3
19.5
2.8
61.2
44.9
Primary+Auxiliary (joint train)
53.5
46.2
34.9
20.1
3.0
62.1
46.8
Primary+Auxiliary (meta train only)
54.0
46.2
35.1
20.3
3.0
62.3
46.9
Primary+Auxiliary (meta train and test) 55.0
47.4
35.8
20.7
3.4
63.0
47.3
the primary segmentation network as Eq. 5, denoted as “Primary”; (2) training
primary and auxiliary networks jointly as multi-task learning in Eq. 6, denoted
as “Primary+Auxiliary (joint learning)”. Table 1 presents the comparisons of
diﬀerent learning approaches. It can be observed that adding the self-supervised
auxiliary task can improve the primary segmentation task. In addition, our meta
auxiliary training consistently outperforms the primary only method and the
joint training method with the same training network and data.
3.2
Comparison with the State-of-the-art
Table 2. Comparison of segmentation performance with the state-of-the-art methods
on A2D Sentences dataset in terms of prec@X, Overall IoU and Mean IoU.
Method
Overlap
mAP
IoU
prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9 0.5:0.95 Overall Mean
Hu et al. [5]
34.8
23.6
13.3
3.3
0.1
13.2
47.4
35.0
Li et al. [7]
38.7
29.0
17.5
6.6
0.1
16.3
51.5
35.4
Gavrilyuk et al. [4] 50.0
37.6
23.1
9.4
0.4
21.5
55.1
42.6
Wang et al. [14]
55.7
45.9
31.9
16.0
2.0
27.4
60.1
49.0
Ye et al. [20]
48.7
43.1
35.8
23.1
5.2
–
61.8
43.2
Yang et al. [18]
65.2
58.0
44.5
20.8
1.6
34.8
58.8
51.3
Bellver et al. [1]
49.5
–
–
–
6.4
–
59.9
43.0
Ours
55.0
47.4
35.8
20.7
3.4
29.6
63.0
47.3
Quantitative Results: We compare our method with several state-of-the-
art approaches in actor and action video segmentation from natural language,
including [1,4,14,18,20] with RGB input for visual features. Following previous
methods [4,14], we also involve one referring image segmentation model [5] and
one lingual speciﬁcation model [7] for comparison.
The experimental results on the A2D Sentences dataset and the JHMDB
sentences dataset are shown in Table 2 and Table 3, respectively. Although our
method only achieves best result in overall IoU, it still generates comparable

326
L. Ye and Z. Wang
Table 3. Comparison of segmentation performance with the state-of-the-art methods
on JHMDB Sentences dataset in terms of prec@X, Overall IoU and Mean IoU.
Method
Overlap
mAP
IoU
prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9 0.5:0.95 Overall Mean
Hu et al. [5]
63.3
35.0
8.5
0.2
0.0
17.8
54.6
52.8
Li et al. [7]
57.8
33.5
10.3
0.6
0.0
17.3
52.9
49.1
Gavrilyuk et al. [4] 69.9
46.0
17.3
1.4
0.0
23.3
54.1
54.2
Wang et al. [14]
75.6
56.4
28.7
3.4
0.0
28.9
57.6
58.4
Ye et al. [20]
76.4
62.5
38.9
9.0
0.1
–
62.8
58.1
Yang et al. [18]
74.6
57.3
25.6
1.5
0.0
27.8
55.6
56.3
Ours
78.4
63.8
37.9
7.3
0.0
33.3
62.9
59.7
results with other methods in the other metrics. Note that our method does
not require temporal proposals as [18] which relies on an external detector. In
addition, the proposed method shows superior performance on the JHMDB Sen-
tences dataset over most metrics. It should be noted that this dataset is used
only for testing. We use the network trained on A2D Sentences dataset without
ﬁne-tuning as previous methods. The remarkable improvement on the JHMDB
Sentences dataset shows the better generalization ability of our method from
our meta auxiliary learning.
Fig. 4. Qualitative examples of our method. For each example, we show the language
query, video frames, our segmentation result and the ground truth from top to bottom.
Diﬀerent colored segmentation masks correspond to queries with the same color.
Qualitative Results: Figure 4 shows qualitative examples produced by our
network to present the scenarios where actors are speciﬁed by diﬀerent sentence
queries. The moving actor (“car is ﬂying”) and static actor (“guy in black sit-
ting”) are accurately identiﬁed in the left example. In the right example, the
ﬁne-grained segmentation masks are produced for two neighboring and moving
actors (“cat” and “ball”).

Meta Auxiliary Learning
327
4
Conclusion
We have presented a novel self-supervised auxiliary learning method for actor
and action video segmentation from natural language. The proposed approach
produce spatial attention, language-context vision attention and vision-context
language attention for this task. We also design a meta auxiliary training method
in a self-supervised way, to improve the primary segmentation task.
Acknowledgments. This work was supported in part by the National Natural Sci-
ence Foundation of China (Grant No. 62102289) and in part by the Zhejiang Provincial
Natural Science Foundation (Grant No. LQ22F020005).
References
1. Bellver, M., Ventura, C., Silberer, C., Kazakos, I., Torres, J., Giro-i Nieto, X.:
A closer look at referring expressions for video object segmentation. Multimedia
Tools Appl. 82(3), 4419–4438 (2023)
2. Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet: a
large-scale video benchmark for human activity understanding. In: IEEE Confer-
ence on Computer Vision and Pattern Recognition, pp. 961–970 (2015)
3. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab:
semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell. 40(4), 834–848
(2017)
4. Gavrilyuk, K., Ghodrati, A., Li, Z., Snoek, C.G.: Actor and action video segmen-
tation from a sentence. In: IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5958–5966 (2018)
5. Hu, R., Rohrbach, M., Darrell, T.: Segmentation from natural language expres-
sions. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS,
vol. 9905, pp. 108–124. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
46448-0 7
6. Jhuang, H., Gall, J., Zuﬃ, S., Schmid, C., Black, M.J.: Towards understanding
action recognition. In: IEEE International Conference on Computer Vision, pp.
3192–3199 (2013)
7. Li, Z., Tao, R., Gavves, E., Snoek, C.G., Smeulders, A.W.: Tracking by natu-
ral language speciﬁcation. In: IEEE Conference on Computer Vision and Pattern
Recognition, pp. 6495–6503 (2017)
8. Liu, C., Lin, Z., Shen, X., Yang, J., Lu, X., Yuille, A.L.: Recurrent multimodal
interaction for referring image segmentation. In: IEEE International Conference
on Computer Vision (2017)
9. Liu, S., Davison, A., Johns, E.: Self-supervised generalisation with meta auxiliary
learning. In: Advances in Neural Information Processing Systems, pp. 1677–1687
(2019)
10. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A.L., Murphy, K.: Generation
and comprehension of unambiguous object descriptions. In: IEEE Conference on
Computer Vision and Pattern Recognition (2016)
11. Pennington, J., Socher, R., Manning, C.: Glove: global vectors for word represen-
tation. In: Conference on Empirical Methods in Natural Language Processing, pp.
1532–1543 (2014)

328
L. Ye and Z. Wang
12. Rohrbach, A., Rohrbach, M., Hu, R., Darrell, T., Schiele, B.: Grounding of textual
phrases in images by reconstruction. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) ECCV 2016. LNCS, vol. 9905, pp. 817–834. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-46448-0 49
13. Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A.A., Hardt, M.: Test-time training
for out-of-distribution generalization. arXiv:1909.13231 (2019)
14. Wang, H., Deng, C., Yan, J., Tao, D.: Asymmetric cross-guided attention network
for actor and action video segmentation from natural language query. In: IEEE
International Conference on Computer Vision, pp. 3939–3948 (2019)
15. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: IEEE
Conference on Computer Vision and Pattern Recognition, pp. 7794–7803 (2018)
16. Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.C.: Convo-
lutional lstm network: a machine learning approach for precipitation nowcasting.
In: Advances in Neural Information Processing Systems, pp. 802–810 (2015)
17. Xu, C., Hsieh, S.H., Xiong, C., Corso, J.J.: Can humans ﬂy? action understanding
with multiple classes of actors. In: IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2264–2273 (2015)
18. Yang, J., Huang, Y., Niu, K., Huang, L., Ma, Z., Wang, L.: Actor and action
modular network for text-based video segmentation. IEEE Trans. Image Process.
31, 4474–4489 (2022)
19. Yang, Y., Deng, C., Gao, S., Liu, W., Tao, D., Gao, X.: Discriminative multi-
instance multitask learning for 3d action recognition. IEEE Trans. Multimedia
19(3), 519–529 (2016)
20. Ye, L., Rochan, M., Liu, Z., Zhang, X., Wang, Y.: Referring segmentation in images
and videos with cross-modal self-attention network. IEEE Trans. Pattern Anal.
Mach. Intell. 44(7), 3719–3732 (2021)

RsMmFormer: Multimodal Transformer
Using Multiscale Self-attention
for Remote Sensing Image Classiﬁcation
Bo Zhang1, Zuheng Ming2, Yaqian Liu1, Wei Feng3, Liang He1(B),
and Kaixing Zhao1(B)
1 School of Software, Northwestern Polytechnical University, Xi’an, China
{2021050018,kaixing.zhao}@nwpu.edu.cn
2 Laboratoire L2TI, Institut Galil´ee, Universit´e Sorbonne Paris Nord, Villetaneuse,
France
3 School of Electronic Engineering, Xidian University, Xi’an, China
Abstract. Remote Sensing (RS) has been widely utilized in various
Earth Observation (EO) missions, including land cover classiﬁcation
and environmental monitoring. Unlike computer vision tasks on natu-
ral images, collecting remote sensing data is more challenging. To fully
exploit the available data and leverage the complementary information
across diﬀerent data sources, we propose a novel approach called Multi-
modal Transformer for Remote Sensing (RsMmFormer) for image classi-
ﬁcation, which utilizes both Hyperspectral Image (HSI) and Light Detec-
tion and Ranging (LiDAR) data. In contrast to the conventional Vision
Transformer (ViT), which does not incorporate the inherent biases and
assumptions of convolutions, we improve our RsMmFormer model by
incorporating convolutional layers. This allows us to integrate the favor-
able characteristics of convolutional neural networks (CNNs). Next, we
introduce the Multi-scale Multi-head Self-Attention (MSMHSA) module,
which enables learning detailed representations, facilitating the detection
of small targets occupying only a few pixels in the remote sensing image.
The proposed MSMHSA module facilitates the integration of Hyper-
spectral Imaging (HSI) and LiDAR data in a progressive and detailed
manner, eﬀectively attending to both global and local contexts using self-
attention mechanisms. Comprehensive experiments conducted on popu-
lar benchmarks such as Trento and MUUFL showcase the eﬀectiveness
and superiority of our proposed RsMmFormer model for remote sensing
image classiﬁcation.
Keywords: Multimodal Transformer · Multi-Scale Multi-head
self-attention · Convolutions · Remote sensing image classiﬁcation
1
Introduction
Remote Sensing (RS) has been signiﬁcantly contributing to a wide range of Earth
Observing (EO) tasks, playing a crucial role in their execution because of its
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 329–339, 2024.
https://doi.org/10.1007/978-981-99-8850-1_27

330
B. Zhang et al.
rapid imaging feature and wide application prospect. Generally, RS can be used
(but not limited) in diﬀerent tasks, such as landcover classiﬁcation [1–3], mineral
and forest resources exploration [4], object/target detection [5,6], environmental
monitoring [7], urban planning [8], biodiversity conservation, as well as disas-
ter response and management. As Remote Sensing (RS) data becomes increas-
ingly accessible, research in RS has transitioned towards data-driven approaches,
employing a variety of Machine Learning (ML) and Deep Learning (DL) models
in RS systems. However, in the past few years, most studies focused only on
single EO sensors, such as HSI sensors, rather than combining diﬀerent types
of sensor data. Although HSI acquired from diﬀerent sensors can provide more
rich spectral information, It is unable to distinguish land cover objects, such as
roads and roofs, that share similar materials. [9]. On the other hand, LiDAR
data provides valuable elevation information, allowing for the discrimination of
objects with similar spectral signatures but diﬀerent elevations. This capability
enables the distinction between features like roads and roofs that are constructed
using materials like cement [10]. Integrating multimodal data for remote sens-
ing classiﬁcation can serve as a solution to this dilemma. Over the past few
years, DL methods have gained signiﬁcant popularity in the fusion of multi-
modal data for RS image classiﬁcation [11,12]. More recently, Vision Trans-
formers (ViT) [13] model has emerged as a new leading approach in RS image
classiﬁcation. It leverages self-attention mechanisms to capture both local and
global features of an image or image sequence, gaining traction in the ﬁeld. Spec-
tralFormer [14] learned the spectral representation of neighboring bands using a
cross-layer encoder module of ViT. However, it neglected spatial information and
only uses spectral information. Swalpa et al. proposed MFT [10] which incorpo-
rates HSI and other source data to construct a multimodal ViT for RS image
classiﬁcation. Nevertheless, MFT did not consider the compatibility problem
when fusing the data with a large resolution gap such as HSI and LiDAR data
which may aﬀect the landcover classiﬁcation in complex scenes [15]. To leverage
the complementary information between the diﬀerent modalities, we propose a
multimodal transformer using self-attention between two modalities HSI and
LiDAR data for RS image classiﬁcation. Instead of using a vanilla ViT as in
MFT or SpectralForm, we propose a MSMHSA module using diﬀerent heads
of transformer aiming to better fuse the feature representations with diﬀerent
resolutions of heterogeneous data from diﬀerent modalities. Besides, we also
introduce convolutions to our RsMmFormer using Convolutional Tokenization
to tokenize HSI and LiDAR, instead of employing linear projection to generate
the Query, Key, and Value feature tensors for self-attention.
The primary contributions of this paper can be summarized as follows: 1)
We design a Multimodal Transformer (RsMmFormer) for RS image classiﬁcation
using HSI and LiDAR data. 2) A MSMHSA module implemented on a single
transformer allows better fusing of multimodal data of diﬀerent resolutions. 3)
To incorporate the desirable properties of convolutions into RsMmFormer, we
introduce convolutions as a means of integration. 4) The exceptional performance

RsMmFormer for RS Image Classiﬁcation
331
achieved by the proposed RsMmFormer validates its eﬀectiveness as a state-of-
the-art backbone for multimodal RS image classiﬁcation.
2
Related Works
Over the past few decades, various traditional methods have been investigated
to derive more relevant and informative features from multi-modal RS data,
such as random forest (RF), morphological proﬁles (MPs) [16], attribute proﬁles
(APs) [17], extinction proﬁles (EPs) [18] and subspace learning [19]. Ham et al.
investigated hierarchical classiﬁers based on RF to improve the generalization
in analysis of quantity limited hyperspectral data [21]. Recently, deep learning
techniques have gained signiﬁcant attention and have been extensively applied in
the ﬁeld of multi-modal data fusion for RS data classiﬁcation. These techniques
have demonstrated remarkable capabilities in extracting and learning informa-
tive features from RS data. In [11], Makantasis et al. exploited a CNN2D based
network to encode pixels’ spectral and spatial information. Vision Transformers
methods used attention mechanism to learn the local and global features of an
HSI data. SpectralFormer [14] used a cross-layer encoder module of ViT to learn
the spectral features between HSI bands. And Swalpa et al. proposed MFT [10]
which leverage the complementary information between the diﬀerent modalities
to construct a multi-modal ViT for RS image classiﬁcation. Inspired by the for-
mer methods, we propose a MSMHSA transformer to improve the overall fusion
and classiﬁcation accuracy.
3
Methodology
3.1
Overall Architecture
An overview of the architecture of RsMmFormer is illustrated in Fig. 1. To bal-
ance the performance and the parameters of model, we set the proposed trans-
former depth to 2.
Convolutional Tokenization. Unlike the vanilla Transformer, we use con-
volutional embedding to tokenize Query/Key/Value ∈RHA×WA×CA feature
tensors. Firstly, the 11*11 data cubes are padding to 16*16. And sequential
layers Conv3D [12] and HetConv2D [20] are used to extract and learn the infor-
mative features of HSI cube and reduce the HSI spectral channels down to 64.
A Conv2D layer is used to extract LiDAR cube’s feature map and expend the
band to 64. After that, the feature maps, concatenated in the last dimension,
act as the input of convolutional embedding to get the Q, K and V.
Subsequently the Q, K and V obtained from the previous step are passed
into the MSMHSA module to facilitate the learning of both local and global
dependencies in the fused data. In the output layer of the MSMHSA module,
we replace the linear layer and layer normalization with a convolution layer,

332
B. Zhang et al.
HSI
LiDAR
11 11 B
11 11 C
Upsampling
Conv2D
Conv3D
HetConv2D
16 16 64
16 16 64
Concatenation
Norm
Convolutional 
Projection
Q
K
V
...
Concatenation
Head1
Head2
Head3
Multi-Scale
MHSA
Norm
Feed Forward
MLP
Categories
QKV
QKV
Q
K
V
....
Softmax
Q
K
V
....
....
....
....
Head 1
Concatenation
Multi-Scale MHSA
Head 2
Head 3
Q
K
V
(a)
(b)
Z
Fig. 1. (a) The overall architecture of the Multimodal Transformer Using Multiscale
Self-Attention for RS image classiﬁcation (RsMmFormer) is presented, which intro-
duces convolutions to the transformer. (b) The proposed MSMHSA module integrated
into a single transformer architecture.
utilizing a kernel size of 3 and padding of 1. Additionally, a LeakyReLU activa-
tion function is applied. The mentioned modules could be deﬁned as follow:
Q, K, V = Conv2D(Xin, k = (1, 1))
(1)

Xout = Conv2D(Xin, k = (3, 3), p = (1, 1))
X = LeakyReLU(Xout, 0.2)
(2)
A comprehensive description of the MSMHSA module can be located in
Sect. 3.2. Finally, at the conclusion of the transformer, we introduce a Feed-
Forward Network (FFN) with Norm layers. In this study, we further enhance
the FFN by substituting the linear projection layers with convolution layers.
Similar to the approach used in ViT [22], After the transformer architecture, we
incorporate a residual MLP module to encode the classiﬁcation embeddings.
3.2
Multi-scale Multi-Head Self-Attention (MSMHSA)
The objective of MSMHSA, illustrated in Fig. 1(b), is to introduce a pyramid
structure into the self-attention module. This structure facilitates the generation
of multi-scale feature maps, enabling pixel-level feature fusion with complemen-
tary features. The proposed MSMHSA is applied to diﬀerent heads within each
layer of the transformer. All the heads adhere to a uniform procedure for com-
puting self-attention.
Speciﬁcally, the feature tensors Query, Key, and Value are evenly split among
these heads along the corresponding dimension prior to being fed into the
MSMHSA module.

RsMmFormer for RS Image Classiﬁcation
333
In the context of a transformer model, there exists a set of heads whose
number is determined by the scales’ number, represented by N. And the shape
of i-th scale is Hi × Wi. These heads are responsible for processing fed fea-
ture tensors Query, Key and Value of size HA × WA × CA, and the feature
tensors for each head are Qi/Ki/Vi ∈RHA×WA× CA
N . The feature maps will
be divided into ni patches, where ni is determined by HA
Hi × WA
Wi . For the ﬁrst
head Head1, with W1 and H1 being equal to WA and HA respectively, we uti-
lize a full-size patch q1/k1/v1 ∈RHA×WA× CA
N to compute the global attention
feature tensor. By following the aforementioned steps, we can generate the self-
attention feature map h1 ∈RHA×WA× CA
n
of Head1. Then for Head2, we divide
Q2/K2/V2 into n2 patches, where n2 equals HA
H2 × WA
W2 , each patch q2/k2/v2 of
size n2 × H2 × W2 × CA
N
and then obtain h2 ∈Rn2×H2×W2× CA
N
of Head2. We
continue to divide Q3/K3/V3 into n3 patches, where n3 equals HA
H3 × WA
W3 , to gen-
erate the self-attention feature map h3 ∈Rn3×H3×W3× CA
N of Head3. The remain-
ing heads are also processed in the same way as before. Finally, we concatenate
the resulting attention feature tensors of each scale {h1, h2, h3, ...} together to
create the resultant multi-scale attention feature tensor H ∈RHA×WA×CA (We
need to transform the shape of hi≥2 to be match the same format as h1 for
consistency):
H = Concat(h1, Reshape(h2), Reshape(h3), ..., Reshape(hi)).
(3)
The attention feature tensor hi is deﬁned as follows:
hi = ΣC
mΣC
n Softmax(
qi,mkT
i,n
dheadi,n
)vi,n,
(4)
Here, the attention feature tensor hi is calculated using the following compo-
nents: qi,m represents the m-th patch obtained from feature tensor Qi for the
i-th head (Headi), while ki,n and vi,n denotes the n-th patches obtained from
the feature tensors Ki and Vi respectively for the i-th head (Headi). Then,
just like what was mentioned earlier, qi,m, ki,n and vi,n ∈RHi×Wi× CA
N . dheadi,n
represents the dimension of the qi,m. In every head Headi, we join the obtained
patches qi, ki and vi from all pictures together to generate the attention feature
map of a single scale hi. Thus, The self-attention mechanism of each head always
incorporates both local attention, which focuses on local spatial information,
simultaneously., i.e., utilizing qi,m and ki,n generated from the neighborhood
to compute self-attention, and global attention focusing on the global context
information calculated by qi,m/ki,n from far regions within the image. In this
work, rather than appending an additional token like ViT does for image clas-
siﬁcation, we utilize a sequence-based representation, denoted as the result of
residual MLP Head Z, which learns from all the tokens in the input. We also
employ a cross-entropy loss for the ﬁnal land-cover classiﬁcation task.

334
B. Zhang et al.
4
Experiments and Analysis
4.1
Experimental Setup
(1) Data Description In order to assess the eﬀectiveness of our proposed
RsMmFormer, we perform experiments on two commonly employed datasets
that involve the integration of HSI and LiDAR data.
Trento Dataset: This dataset consists of a pair of data, namely hyperspectral
imaging and LiDAR, collected from a rural area located Trento, Italy. The HSI
data comprises 63 bands, while the LiDAR data consists of a single band. Both
the hyperspectral imaging and LiDAR data in this dataset have a size of 166 ×
600 pixels, with a spatial resolution of 1 m, encompassing a total of 6 diﬀerent
classes.
MUUFL Dataset: This dataset was gathered at the University of Southern
Mississippi Gulf Park. Both the hyperspectral imaging and LiDAR data in the
dataset encompass 325 × 220 pixels. The hyperspectral imaging initially com-
prised 72 bands, but four bands at the beginning and end were discarded due
to noise problems. Thus, the experiment utilized the remaining 64 bands. The
LiDAR data consisted of 2 bands. The dataset comprises 11 distinct classes.
(2) Experimental Setup The experiments were conducted on a CentOS Linux
server (release 7.9.2009) with a single Nvidia 3090 GPU, which has 24576 MB of
VRAM.
The models under consideration were trained and tested with batch sizes of
64 for training phase and 500 for testing phase. The models were trained using
an Adam optimizer with a learning rate of 5e–4 and weight decay of 5e–3. A step
scheduler with a step size of 50 and a gamma value of 0.9 was also employed.
Each experiment was performed for 500 epochs, repeated three times, and the
results reported include the average values and standard deviations. The source
code was implemented using PyTorch 1.12.1 and Python 3.8.7.
(3) Evaluation metrics The classiﬁcation performance of our model is evalu-
ated quantitatively using three commonly used metrics: overall accuracy (OA),
average accuracy (AA), and statistical Kappa (κ) coeﬃcients.
4.2
Quantitative Analysis
Table 1 and Table 2 report the quantitative OA, AA, kappa and each class accu-
racy on two widely used datasets Trento and MUUFL to compare the pro-
posed RsMmFormer with other state-of-art methods, i.e., RF [21], CNN2D [11],
ViT [22], SpectralFormer [14] and MFT [10]. Our model RsMmFormer obtains
the best performance on all three indices on both two benchmarks, such as
99.18% and 94.73% OA, 97.91% and 84.57% AA and 98.90% and 93.02% Kappa
on Trento and MUUFL datasets. Our model has also achieved the best perfor-
mance for almost each class accuracy, and even it has gained 20% improvement
on landcover Yellow-Curb (class 10) compared to the latest MFT on MUUFL
data.

RsMmFormer for RS Image Classiﬁcation
335
Table 1. Classiﬁcation Performance on Trento Data (HSI and LiDAR) - OA, AA, and
Kappa Values (in %). The Best is shown in bold.
Class No. RF [21]
CNN2D [11]
ViT [22]
Spectral-Former [14] MFT [10]
RsMmFormer
1
83.73 ± 00.06 96.98 ± 00.21
90.87 ± 00.77
96.76 ± 01.71
98.23 ± 00.38
99.71 ± 0.25
2
96.30 ± 00.06 97.56 ± 00.14
99.32 ± 00.77
97.25 ± 00.66
99.34 ± 00.02 98.06 ± 0.80
3
70.94 ± 01.55 55.35 ± 00.00
92.69 ± 01.53
58.47 ± 11.54
89.84 ± 09.00
94.47 ± 1.77
4
99.73 ± 00.07 99.66 ± 00.03 100.0 ± 00.00 99.24 ± 00.21
99.82 ± 00.26
99.96 ± 0.02
5
95.35 ± 00.25 99.56 ± 00.07
97.77 ± 00.86
93.52 ± 01.75
99.93 ± 00.05 99.90 ± 0.07
6
72.63 ± 00.90 76.91 ± 00.15
86.72 ± 02.02
73.39 ± 06.78
88.72 ± 00.94
95.34 ± 1.32
OA
92.57 ± 00.07 96.14 ± 00.03
96.47 ± 00.49
93.51 ± 01.27
98.32 ± 00.25
99.18 ± 0.02
AA
86.45 ± 00.32 87.67 ± 00.04
94.56 ± 00.57
86.44 ± 02.96
95.98 ± 01.64
97.91 ± 0.25
κ
90.11 ± 00.09 94.83 ± 00.04
95.28 ± 00.65
91.36 ± 01.67
97.75 ± 00.00
98.90 ± 0.02
Table 2. Classiﬁcation Performance on MUUFL Data (HSI and LiDAR) - OA, AA,
and Kappa Values (in %). The Best is shown in bold.
Class No. RF [21]
CNN2D [11]
ViT [22]
Spectral-Former [14] MFT [10]
RsMmFormer
1
95.42 ± 00.09 95.79 ± 00.11 97.85 ± 00.29 97.30 ± 00.83
97.90 ± 00.39
98.88 ± 0.15
2
74.03 ± 00.11 72.76 ± 00.58 76.06 ± 02.40 69.35 ± 05.16
92.11 ± 01.58 88.84 ± 1.66
3
75.81 ± 00.38 78.92 ± 00.52 87.58 ± 03.46 78.48 ± 03.41
91.80 ± 00.82 90.00 ± 0.80
4
68.59 ± 00.77 83.59 ± 00.99 92.05 ± 02.31 82.63 ± 03.68
91.59 ± 02.25
95.19 ± 0.24
5
88.17 ± 00.18 78.29 ± 01.12 94.73 ± 00.60 87.91 ± 02.97
95.60 ± 01.21 95.28 ± 0.48
6
77.28 ± 00.93 50.34 ± 02.13 82.02 ± 01.13 58.77 ± 02.76
88.19 ± 03.49
88.48 ± 0.97
7
64.83 ± 00.97 79.70 ± 00.26 87.11 ± 01.54 85.87 ± 00.62
90.27 ± 02.13
92.94 ± 1.14
8
93.29 ± 00.27 71.95 ± 01.10 97.60 ± 00.16 95.60 ± 01.26
97.26 ± 00.53
97.84 ± 0.53
9
19.15 ± 01.37 43.92 ± 01.24 57.83 ± 04.45 53.52 ± 04.32
61.35 ± 03.80
65.02 ± 1.79
10
04.41 ± 00.72 12.45 ± 00.27 31.99 ± 08.86 08.43 ± 02.22
17.43 ± 04.63
36.97 ± 3.39
11
71.88 ± 00.84 26.82 ± 02.60 58.72 ± 03.85 35.29 ± 06.00
72.79 ± 09.25
80.85 ± 5.58
OA
85.32 ± 00.09 83.40 ± 00.04 92.15 ± 00.19 88.25 ± 00.56
94.34 ± 00.07
94.73 ± 0.20
AA
66.62 ± 00.16 63.14 ± 00.21 78.50 ± 01.28 68.47 ± 01.44
81.48 ± 00.70
84.57 ± 0.35
κ
80.39 ± 00.12 77.94 ± 00.06 89.56 ± 00.27 84.40 ± 00.77
92.51 ± 00.10
93.02 ± 0.26
4.3
Ablation Study
Ablation studies are performed exclusively on the Trento dataset. The tables
highlight the best results, which are presented in bold.
Evaluation of the Eﬀectiveness of Multimodal Fusion. In order to evalu-
ate the eﬀectiveness of multimodal fusion, we trained the proposed model respec-
tively on single modality and multimodalities. Only one branch has been used
to input the data when training the model in single modality as shown in Fig. 1.
In Table 3, we can see that the performance of multimodal model is superior
to single-modal either HSI or LiDAR for all three indices, i.e., OA, AA, κ and
almost for each class accuracy, which demonstrate the eﬀectiveness of our mul-
timodal transformer RsMmFormer for RS image classiﬁcation.

336
B. Zhang et al.
Table 3. Multimodal v.s. single-modal RsMmFormer.
Class LiDAR only
HSI only
Multimodal
1
97.93 ± 0.56 99.21 ± 0.27
99.71 ± 0.25
2
74.47 ± 4.60 91.20 ± 2.48
98.06 ± 0.80
3
58.11 ± 5.98 93.58 ± 2.18
94.47 ± 1.77
4
93.79 ± 0.43 99.83 ± 0.20
99.96 ± 0.02
5
96.67 ± 0.56 99.96 ± 0.03 99.90 ± 0.07
6
67.03 ± 3.18 77.34 ± 4.60
95.34 ± 1.32
OA
90.29 ± 0.45 96.56 ± 0.60
99.18 ± 0.02
AA
81.33 ± 1.47 93.52 ± 1.35
97.91 ± 0.25
κ
86.99 ± 0.61 95.39 ± 0.81
98.90 ± 0.02
Table 4. Eﬀectiveness of multiscale RsMmFormer.
16*16 8*8 4*4 2*2 OA
AA
κ
√
99.04
97.29
98.71
√
98.46
97.03
97.94
√
98.89
97.50
98.51
√
98.97
96.93
98.62
√
√
99.06
96.72
98.74
√
√
99.18
97.61
98.90
√
√
99.17
97.58
98.89
√
√
98.78
96.96
98.36
√
√
98.70
96.11
98.26
√
√
99.06
98.09
98.74
√
√
√
99.13
97.21
98.83
√
√
√
99.05
97.42
98.73
√
√
√
99.18 97.91 98.91
√
√
√
98.71
97.28
98.27
√
√
√
√
99.14
97.22
98.85
Evaluation of the Eﬀectiveness of Multi-scale MHSA. In Table 4, it can
be observed that the model using MSMHSA performs always better than the one
using single-scale MHSA, e.g., the model using 16×16, 4×4, 2×2 gains 0.3%, 0.7%
and 0.5% improvement in terms OA, AA and κ respectively.

RsMmFormer for RS Image Classiﬁcation
337
4.4
Visualization
Figure 2 demonstrates a qualitative assessment by visually presenting the classi-
ﬁcation maps generated by diﬀerent models on the Trento dataset, utilizing HSI
and LiDAR data. The MSMHSA module helps to learn the features in a ﬁne-
to-coarse manner, which obtains a classiﬁcation map with less noise and ﬁner
details.
(d) RF
(e) CNN2D
(c) GT
(f) ViT
(g) SpectralFormer
(h) MFT
(i) RsMmFormer
background
buildings
woods
roads
apples
ground
vineyard
(a) Pseudo-color Map 
(b) LiDAR Map 
Fig. 2. The visual representations for the Trento dataset include: (a) Pseudo-color
Map, (b) LiDAR, (c) Ground truth, along with the classiﬁcation maps inferred by
various models: (d) RF, (e) CNN2D, (f) ViT, (g) SpectralFormer [14], (h) MFT, and
(i) RsMmFormer, based the HSI and LiDAR data on the Trento dataset.

338
B. Zhang et al.
5
Conclusion
We have designed a Multi-modal Transformer (RsMmFormer), which allows us
to explore the complementary information between the spectral information in
HSI and the spatial information in LiDAR for RS image classiﬁcation. The pro-
posed MSMHSA module in the Transformer aims to better fuse the multi-modal
data with very diﬀerent resolutions. Additionally, we have incorporated convolu-
tions into our model to leverage the favorable properties of Convolutional Neural
Networks (CNNs) and achieve a favorable trade-oﬀbetween computation and
accuracy. The demonstrated eﬀectiveness and state-of-the-art performance high-
light that the model RsMmFormer can serve as a valuable backbone for Remote
Sensing (RS) image classiﬁcation.
References
1. Ahmad, M., Shabbir, S.: Hyperspectral image classiﬁcation-traditional to deep
models: a survey for future prospects. IEEE J. Sel. Topics Appl. Earth Obs. Remote
Sens. 15, 968–999 (2021)
2. Bartholome, E., Belward, A.S.: GLC2000: a new approach to global land cover
mapping from Earth observation data. Int. J. Remote Sens. 26(9), 1959–1977
(2005)
3. Roy, S.K., Kar, P.: Revisiting deep hyperspectral feature extraction networks via
gradient centralized convolution. IEEE Trans. Geosci. Remote Sens. 60, 1–19
(2021)
4. Koetz, B., Morsdorf, F.: Multi-source land cover classiﬁcation for forest ﬁre man-
agement based on imaging spectrometry and LiDAR data. Forest Ecol. Manag.
256, 263–271 (2008)
5. Wu, X., Hong, D.: ORSIm detector: a novel object detection framework in opti-
cal remote sensing imagery using spatial-frequency channel features. IEEE Trans.
Geosci. Remote Sens. 57, 5146–5158 (2019)
6. Wu, X., Hong, D.: Fourier-based rotation-invariant feature boosting: an eﬃcient
framework for geospatial object detection. IEEE Geosci. Remote Sens. Lett. 17,
302–306 (2019)
7. Ustin, S.L.: Manual of Remote Sensing, Remote Sensing for Natural Resource
Management and Environmental Monitoring. John Wiley & Sons, Hoboken (2004)
8. Chen, C., Yan, J.: Classiﬁcation of urban functional areas from remote sensing
images and time-series user behavior data. IEEE J. Sel. Topics Appl. Earth Obs.
Remote Sens. 14, 1207–1221 (2020)
9. Ghamisi, P., Benediktsson, J.A., Phinn, S.R.: Land-cover classiﬁcation using both
hyperspectral and LiDAR data. Int. J. Image Data Fusion 6, 189–215 (2015)
10. Roy, S.K., Deria, A.: Multimodal fusion transformer for remote sensing image
classiﬁcation. arXiv preprint arXiv:2203.16952 (2023)
11. Makantasis, K., Karantzalos, K., Doulamis, A., Doulamis, N.: Deep supervised
learning for hyperspectral data classiﬁcation through convolutional neural net-
works. In: International Geoscience and Remote Sensing Symposium (2015)
12. Hamida, A.B., Benoit, A., Lambert, P., Amar, C.B.: 3-D deep learning approach
for remote sensing image classiﬁcation. IEEE Trans. Geosci. Remote Sens. 56(8),
4420–4434 (2018)

RsMmFormer for RS Image Classiﬁcation
339
13. Vaswani, A., Shazeer, N.: Attention is all you need. Adv. Neural Inf. Process. Syst.
30 (2017)
14. Hong, D., et al.: SpectralFormer: rethinking hyperspectral image classiﬁcation with
transformers. In: Computer Vision and Pattern Recognition (2021)
15. Gao, L., Hong, D., Yao, J., Zhang, B., Gamba, P., Chanussot, J.: Spectral super-
resolution of multispectral imagery with joint sparse and low-rank learning. IEEE
Trans. Geosci. Remote Sens. 59, 2269–2280 (2021)
16. Benediktsson, J.A., Palmason, J.: Classiﬁcation of hyperspectral data from urban
areas based on extended morphological proﬁles. IEEE Trans. Geosci. Remote Sens.
43, 480–491 (2005)
17. Dalla Mura, M., Benediktsson, J.A.: Morphological attribute proﬁles for the analy-
sis of very high resolution images. IEEE Trans. Geosci. Remote Sens. 48, 3747–3762
(2010)
18. Ghamisi, P., Souza, R.: Extinction proﬁles for the classiﬁcation of remote sensing
data. IEEE Trans. Geosci. Remote Sens. 54, 5631–5645 (2016)
19. De La Torre, F., Black, M.J.: A framework for robust subspace learning. Int. J.
Comput. Vision 54, 117–142 (2003)
20. Singh, P., Verma, V.K., et al.: Hetconv: heterogeneous kernel-based convolutions
for deep CNNs. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 4835–4844 (2019)
21. Ham, J., Chen, Y.: Investigation of the random forest framework for classiﬁcation
of hyperspectral data. IEEE Trans. Geosci. Remote Sens. 43, 492–501 (2005)
22. Dosovitskiy, A., Beyer, L.: An image is worth 16x16 words: transformers for image
recognition at scale. arXiv preprint arXiv:2010.11929 (2010)

Fashion Label Relation Networks
for Attribute Recognition
Tongyang Wang, Yan Huang, and Jianjun Qian(B)
PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional
Information of Ministry of Education, and Jiangsu Key Lab of Image and Video
Understanding for Social Security, School of Computer Science and Engineering,
Nanjing University of Science and Technology, Nanjing, China
{tongyangwang,arvohy,csjqian}@njust.edu.cn
Abstract. Clothes attribute recognition has played a distinguished role
in the fashion industry. A large number of attributes and the intrin-
sic relations between attributes make this task challenging. Most prior
methods employ the landmark detection to improve performance. How-
ever, there has been little research devoted to analyzing the relationships
between fashion labels. To address the above problem, this paper pro-
poses an eﬀective Fashion Label Relation (FLR) network, including the
local label relation (LLR) and the global label relation (GLR). LLR uses
an attention mechanism to focus on local label relations in conjunction
with a predicted label correlation matrix for reﬁning the label repre-
sentation. To fully characterize the relationships among all attributes,
GLR performs a pairwise label co-occurrence prediction task in label co-
occurrence prediction (LCP) module and constructs a new label feature
distribution in label relation smoothing (LRS) module to avoid over-
ﬁtting. The comprehensive experimental results strongly demonstrate
that our proposed framework outperforms state-of-the-art algorithms.
Keywords: Fashion recognition · label relation · attention mechanism
1
Introduction
Fashion, as a visual domain, has gained signiﬁcant interest from computer vision
researchers in recent years. In the ﬁeld of computer vision, there are various
areas of research related to fashion, such as fashion recommendation [1], clothing
classiﬁcation [2], and fashion trend prediction [2,3]. Among these areas, our
primary focus is on traditional fashion attribute recognition.
As an outﬁt has multiple attributes, fashion attribute recognition is essen-
tially a multi-label classiﬁcation task. The previous works combine auxiliary
tasks together to achieve better performance. Liu et al. [4] proposed a Fashion-
Net to jointly predict attributes and the location of landmarks. [5,6] also learn
jointly with landmark detection and category classiﬁcation. However, these works
expand the model complexity and increase the amount of computation load.
Meanwhile, the most previous methods ignore the implicit associations between
labels.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 340–351, 2024.
https://doi.org/10.1007/978-981-99-8850-1_28

Fashion Label Relation Networks for Attribute Recognition
341
In the ﬁeld of conventional multi-label classiﬁcation, researchers have started
to focus on modeling the correlation between labels. Recently, several works
have employed Graph Convolutional Network (GCN) to map label representa-
tions [7,8]. However, these methods still lack the ability to establish correlations
in many complex scenarios. Transformers have shown success in various com-
puter vision tasks, as we all known it include object detection [9,10] and image
classiﬁcation [11]. The attention mechanism, a key component of transformers,
models relationships within data and has been used in many multi-label clas-
siﬁcation works [12,13]. Previous studies have achieved signiﬁcant results but
typically considered fewer than 100 labels in their tasks. In contrast, our task
involves hundreds to thousands of labels.
To overcome this diﬃculty, we wander to build label relationships from local-
wise and global-wise orientations, respectively. The local-wise part mainly aims
to model label relations by focusing on several labels with high correlation and
ignoring other irrelevant labels. And the other part is responsible to improve
global correlation learning by using a label distribution constraint and a label
co-occurrence prediction constraint.
Based on the ideas mentioned above, we propose a Fashion Label Relation
Network for fashion attribute recognition. This network consists of an LLR
module and a GLR module. In LLR, we predict a correlation matrix to reﬁne
label embedding according to its local guidance and attention mechanism. After
that, we perform cross-attention to pool image features for downstream steps.
In our approach, we utilize the VGG-16 model [14] to extract image features.
The co-occurrence relationships among labels play a signiﬁcant role in revealing
label correlations, and the advantage is that it doesn’t require additional man-
ual annotation and has low computational complexity. In the proposed method,
called GLR (Graph-based Label Relationship), we go beyond just considering co-
occurrence and capture label correlations by leveraging a given partially relevant
label set to predict the relevance of unknown labels. This helps in incorporating
the relationships between labels into the classiﬁcation process. In multi-label
classiﬁcation tasks, true labels are often represented using a multi-hot vector,
where each label is indicated by a binary value (0 or 1) denoting its presence or
absence. However, relying solely on the multi-hot vector may not fully capture
the intricate relationships between objects and labels. Therefore, if we heavily
rely on the multi-hot vector during training, it can lead to overconﬁdence of
the model and potentially aﬀect its performance. Therefore, a new label distri-
bution is generated in GLR to replace the original multi-hot vector. This new
distribution is then compared with the predicted distribution to compute the
loss. Extensive experiments show that our model signiﬁcantly outperforms the
compared methods. The main contributions are listed as follows:
– A local label relation (LLR) module is proposed to predict a correlation
matrix, combing attention mechanism to build local-wise attribute relation-
ships.

342
T. Wang et al.
– A global label relation (GLR) module is developed with a co-occurrence pre-
diction constraint and a label distribution constraint, which can model the
global-wise attribute relationships dynamically.
– Based on LLR and GLR, the proposed Fashion Label Relation (FLR) network
achieves state-of-the-art performance on four fashion datasets.
2
Related Work
Fashion Image Recognition. In recent years, because of the improvement
of deep learning and the appearance of large-scale fashion datasets [4,15,16],
many convolutional neural networks have been introduced to dig more discrimi-
native representation and obtain more superior performance. In 2016 the paper
FashionNet [4] is proposed, and it contained a deep model designed for joint pre-
diction of clothing category classiﬁcation and landmark localization prediction.
Instead, [5,6] tended to combine a-priori information like grammar knowledge
when training models for achieving better performance. However, although these
methods achieved good performance, they ignored the relationship between the
fashion attributes, thereby hindering the further improvement of the recognition
accuracy.
Label Relationship Modeling. For multi-label classiﬁcation tasks, from the
previous study we ﬁnd that building label relations is a critical problem, which
further motivates us to model the relationship between labels. Attention mech-
anism strategy [17] has been widely used in the deep-learning ﬁelds, such as
natural language processing (NLP) task and computer vision (CV) task. It was
ﬁrst brought to vision in Vision Transformer (ViT) [18] when the study use it to
split an image into a sequence of visual tokens. Many works [9,10,19] introduces
it as an eﬀective architecture for relationship digging and have demonstrated
better results. Though the attention mechanism is expert in obtaining the rela-
tionship, it will not work well when the number of entities is huge. With the
objective of reducing the eﬀects mentioned, we apply a correlation matrix to
assist with it. Nevertheless, label co-occurrence characteristics were analyzed
as primary in multi-label classiﬁcation problems [20], due to it can help with
enhancing label correlation learning. Label smoothing(LS) is another relation
modeling method and it was ﬁrst introduced in the image classiﬁcation task
[21], they take it as a regularization technique to stop the model from predicting
the training inputs too conﬁdently, and has been applied widely in many other
works [17,22]. Naturally, it is not hard for us to attain some inspiration from
these mentioned works, that is, we can forge several constraints to dynamically
model the correlation between labels from a global perspective.
3
Methodology
In this section, we will introduce our Fashion Label Relation (FLR) network in
detail. The overview of FLR is shown in Fig. 1.

Fashion Label Relation Networks for Attribute Recognition
343
3.1
Framework
Suppose we have a dataset consisting of images that can be classiﬁed into C
diﬀerent classes. The label space is denoted as y = [y1, y2, ..., yC], where each
yc represents whether the image belongs to the c-th class or not. If the image
contains the c-th category label, then yc is 1, otherwise, it is 0. Our task is to
learn a predictive function that can assign predicted probabilities to each class
for a given input image I. We aim to obtain the probabilities p = [p1, p2, ..., pC]
corresponding to each class. These probabilities represent the model’s conﬁdence
in predicting the presence or absence of each class in the image.
Fig. 1. The overall architecture of our proposed Fashion Label Relation (FLR) network
consists of two essential structures. The LLR structure enables the label embeddings
to reﬁne themselves and capture image features. The GLR structure uses the output
from LLR to predict label co-occurrence probability and a new label distribution.
Feature Extraction. Let’s consider an input image denoted as I ∈RH
′×W
′×3.
To extract spatial features from this image, we can use the VGG-16 network [14]
as the backbone network. The spatial feature f ∈RH×W ×d, where H
′ × W
′
represent the height and weight of feature map respectively, and d represents
the dimension of the extracted features.
Local Label Relation. An illustration of local relation construction is shown in
Fig. 2(a). Firstly, we utilize a random initialization label embedding E ∈RC×D
to predict K relation positions, so that attain a correlation matrix Ap reﬂecting
the labels that each label is interested in. Then, both E and Ap are feed to the
label attention block and acquire a reﬁned label representation E
′. In practical

344
T. Wang et al.
Fig. 2. a) Illustration of LLR structure. b) Illustration of GLR structure, which is
composed of a basic Label Co-occurrence Prediction(LCP) module on the left and a
Label Relation Smooth(LRS) module on the right.
terms, we deﬁne the formula as follows.
Ap = MLP(E),
A = softmax( EWQ(EWK)T
√
C
∗Ap),
E
′ = AEWV ,
(1)
where Ap is the correlation matrix, and W{Q}, W{K},W{V } are represented as
the learnable weights of query, key and value projections. A is the attention
matrix. Subsequently, we adopt the cross-attention mechanism to encourage the
label to query interested class-related region features for the subsequent binary
classiﬁcation. After the LLR construction processed the inputs via two consec-
utive attention stages, the original label embedding E obtains an appropriate
class-related representation yattr and updates itself iteratively. To address the
class imbalance issue in the classiﬁcation task, we employ the asymmetric loss
[23] as our classiﬁcation loss function:
LCLs = 1
C
C

i=1
(1 −pi)γ+log(pi),
yi
gt = 1.
(pi)γ−log(1 −pi),
yi
gt = 0.
(2)
where p = [p1, ..., pC] is the predicted results of yattr. ygt is the ground truth.
Global Label Relation.
Label Co-occurrence Prediction. In order to enhance the label correlation learn-
ing globally, we add an auxiliary label co-occurrence prediction branch named

Fashion Label Relation Networks for Attribute Recognition
345
LCP. Firstly, let’s assume that the label set for the dataset is denoted as U,
and the attribute set corresponding to each image is denoted as U +, where U +
belongs to U. The strategy begins by randomly selecting s attributes from the
label set U + and form a new set L+. These selected attributes are guaranteed
to exist in the corresponding images. Then, we pair the remaining labels from
the total label set with these s selected labels L+. In this approach, the average
feature vector serves as a representation of the selected s labels. It is combined
with each of the remaining label vectors one by one to form pairs for further
processing. Ultimately, the goal is to predict whether these pairs of labels will
appear together in an image. From Fig. 2(b), we can see the overview of GLR. We
use the binary cross-entropy loss to calculate the label co-occurrence prediction
loss:
LLCP = −
C−s

i=1
[yi
pair ln(yi
lcp) + (1 −yi
pair) ln(1 −yi
lcp)]
(3)
we denote yi
pair as the ground-truth, yi
pair = 1 only if the i-th label pair will be
co-occurrence.
Label Relation Smooth. Moreover, the LRS module overcomes the limitations of
inﬂexible techniques like uniform label smoothing, which treat all labels equally.
By considering the individual similarities between objects and labels, the module
provides a more adaptable and ﬁne-grained approach to capturing label depen-
dencies. We employ a Multi-Layer Perceptron (MLP) layer to construct the
similarity layer shown in Fig. 2 (b). Speciﬁcally, this similarity layer takes the
output of LLR named yattr and the pooled image features f
′ as the inputs, and
then computes their similarity values. In order to preserve the primary label dis-
tribution information, we add the multi-hot vector ground truth to help with the
new label distribution building. So far, we have received the ultimate predicted
new label distribution ykl. To measure the dissimilarity between the predicted
classiﬁcation result p and the ground truth label distributionykl, we employ the
Kullback-Leibler divergence (KL-divergence) [24] as the loss function. The loss
can be formulated as follows:
LLRS = KL(ykl||p),
= C
i=1 yi
kl log yi
kl
pi ,
(4)
3.2
Learning Objective
In this subsection, three constraints term deﬁned above are employed to improve
the robustness of the recognition task. The total training loss function is as
follows:
Lsum = LCLs + λLLCP + αLLRS
(5)
where λ and α are hyperparameters, which denote the weight of LLCP and LLRS
loss respectively. We minimize the loss by using the adaptive moment estimation
[25] optimization.

346
T. Wang et al.
4
Experiment
4.1
Benchmark Dataset and Evaluation Metrics
DeepFashion-C [4] is a comprehensive and big clothing dataset that contains
rich annotations for clothing items. Each image in this dataset is extensively
labeled with 1,000 attributes. The attribute labels set are divided into ﬁve dis-
tinct groups, which characterize various aspects of the clothing items. These
groups include ‘texture’, ‘fabric’, ‘shape’, ‘part’, and ‘style’. In terms of dataset
split, 209,222 fashion images from dataset are allocated for training purposes.
Another set of 40,000 images is designated for validation, while the rest 40,000
images serve as test samples.
iFashion (iMaterialist Fashion Attribute) [16] is a large-scale fashion
dataset constructed from a collection of over one million fashion images. It boasts
a comprehensive label space consisting of eight groups, encompassing a total of
228 ﬁne-grained attributes. These attribute groups are categorized as ‘category’,
‘color’, ‘gender’, ‘material’, ‘neckline’, ‘pattern’, ‘sleeve’, and ‘style’.
Evaluation Metrics. In accordance with the evaluation protocol established
in [4], the performance of the model is assessed using the top-k recall rate met-
ric. This metric measures the accuracy of attribute predictions by ranking the
classiﬁcation scores and determining the number of attributes that are cor-
rectly matched within the top-k predictions. Speciﬁcally, in iFashion dataset,
the attributes amount of ‘gender’ and ‘sleeve’ are less than 5, so we choose to
evaluate them with accuracy.
4.2
Implementation Details
We adopt the settings described below for all experiments. Following [6], we lever-
age vgg-16 [14] pre-trained on ImageNet [26] as our backbone. In the training
stage, we resize the input images into 224 × 224. And the vgg-16 output feature’s
size is deﬁned as H × W × d = 14 × 14 × 512, the hidden dimension is seted as
D = 512 and d = 512. Moreover, we use image normalization technology to nor-
malize the input images, and set mean as [0, 0, 0] and set std as [1, 1, 1]. And then
for data augmentation, we use RandAugment [27]. We set λ = 0.5, α = 0.5 in
Eq.(5). All the experiments are implemented on PyTorch using NVIDIA TITAN
RTX GPU. We trained the model for 30 epochs with batch size=64, and employ
the Adam [25] as the optimizer with the learning rate equals to 1×10−4, and with
True-weight-decay [28] of 1e −2, β1 = 0.9 and β2 = 0.999.
4.3
Comparison with the Benchmarking Methods
Comparisons on DeepFashion-C. As shown in Table 1, we compare our app-
roach on DeepFahsion-C benchmark with some state-of-the-art methods. For fair

Fashion Label Relation Networks for Attribute Recognition
347
Table 1. Comparisons with state-of-the-art methods on the Deepfashion-C dataset.
Best:bold.
Methods
Texture
Fabric
Shape
Part
Style
All
Top-3
Top-5
Top-3
Top-5
Top-3
Top-5
Top-3
Top-5
Top-3
Top-5
Top-3
Top-5
WTBI [29]
24.21
32.65
25.38
36.06
23.39
31.26
26.31
33.24
49.85
58.68
27.46
35.37
DARN [30]
36.15
48.15
36.64
48.52
35.89
46.93
39.17
50.17
66.11
71.36
42.35
51.95
FashionNet [4]
37.46
49.52
39.30
49.84
39.47
48.59
44.13
54.02
66.43
73.16
45.52
54.61
BCRNN [5]
50.31
49.52
40.31
48.23
53.32
61.05
40.65
56.32
68.70
74.25 51.53
60.95
Lu et al. [31]
56.17
65.83
43.20
53.52
58.28
67.80
46.97
57.42
68.82 74.13
54.69 63.74
re-impl of BCRNN [6] 56.48
65.85
44.10
54.40
61.30
70.30
49.24
59.36
33.58
42.44
49.19
58.80
Ts-fashionNet [6]
58.52 68.19
46.44 57.02 61.86 70.81
49.82 60.36
34.40
43.44
50.58
60.43
re-impl of [31]
56.49
66.31
43.69
53.96
57.74
67.21
46.40
56.80
31.59
40.25
26.80
34.84
re-impl of [6]
59.16
68.79
46.28
56.75
60.05
69.31
48.58
59.16
35.50
44.22
28.87
37.28
ML-GCN [7]
58.09
67.40
44.54
54.75
56.86
65.98
45.64
55.96
33.95
42.16
27.75
35.72
Vgg16 [14]
57.02
66.51
42.58
52.78
54.98
64.48
41.37
51.69
30.25
39.29
25.97
33.40
Ours
62.66 72.31
51.08 61.48 63.30 72.14
54.10 64.29
43.27 51.38 32.43 42.09
Table
2. Comparisons with state-of-the-art methods on the ifashion dataset.
Best:bold.
Methods
category
color
gender material
neckline
pattern
sleeve
style
All
Top-3
Top-5
Top-3
Top-5
Acc
Top-3
Top-5
Top-3
Top-5
Top-3
Top-5
Acc
Top-3
Top-5 Top-3
Top-5
Lu et al. [31]
82.52
90.50
72.75
85.78
90.77
85.87
93.25
87.72
95.91
88.38
93.47
82.03
77.96
88.13
38.25
53.36
Ts-FashionNet [6] 82.92
90.67
74.41
86.97
91.06
86.51
93.54
89.18
96.42
87.69
93.08
82.66
79.28
88.98
39.63
55.35
Vgg16 [14]
80.55
88.71
69.84
83.74
90.49
84.67
92.21
88.36
95.97
85.02
91.14
82.10
77.42
87.81
38.85
53.81
ML-GCN [7]
80.65
88.68
73.43
86.05
90.67
85.01
92.45
88.53
96.07
86.18
91.84
82.06
77.85
87.84
39.00
54.41
Ours
85.07 92.08 76.38 88.53 91.65
88.76 94.63 90.11 96.78 90.09 94.35 83.49 81.09 90.3
40.44 56.96
comparisons, we follow their resolution settings [5,6] and use the same backbone.
From the top half of Table 1, we ﬁnd that the results within the ‘style’ group are
quite diﬀerent. According to [5,6], the previous works utilized additional fashion-
related data or the attributes’ annotations might have been modiﬁed or updated
over time. In addition, the all top-3 and top-5 recall rates also have discrepancies
between the previous and our experiments. We infer they may directly average
the results of all attribute groups before separately. So for fairness and reality, we
imitate [6] and utilize the re-implementation experiment results for comparison,
the results are presented in the bottom half of the Table 1. Our FLR signiﬁcantly
outperforms all the benchmarking methods. On average, FLR produces better
recognition results than the best benchmarking results (shown in bold) around
4.73% and 4.70% on top-3 recall and top-5 recall respectively.
Comparisons on iFashion. Furthermore, we utilize the same experimental
methods as described above to train on the iFashion dataset, and the results are
presented in Table 2. It can be seen from the table that our method exceeds all
other methods. According to the above results, it is easy to ﬁnd that our method
is very eﬀective for fashion image attribute recognition task, which achieves state-
of-the-art performance and can generalize to other database.

348
T. Wang et al.
Table 3. Ablation experiments on the Deepfashion-C dataset. Best:bold.
Model
Texture
Fabric
Shape
Part
Style
All
Avg
Top-3
Top-5
Top-3
Top-5
Top-3
Top-5
Top-3
Top-5
Top-3
Top-5
Top-3
Top-5
FLR
62.66 72.31
51.08 61.48
63.30
72.14 54.10 64.29
43.27 51.38
32.43 42.09 55.88
w/o GLR
62.34
72.35 50.83
61.55 63.37
72.06
53.91
64.42 42.50
50.94
32.33
41.86
55.70
w/o LLR
62.11
71.76
50.73
61.26
63.40 72.06
53.85
63.78
43.25
51.51 32.18
41.61
55.62
w/o GLR & LLR 61.82
71.50
50.32
60.76
63.00
71.72
53.70
63.65
42.13
50.41
32.15
41.46
55.21
Table 4. Results on clothes retrieval datasets. Best:bold.
Methods
Inshop
Consumer-to-shop
Top-30 Top-50 Top-20 Top-50
BCRNN [5]
74.91
83.86
67.07
76.51
TS-FashionNet [6]
78.45
86.69
70.40
79.71
TS-FashionNet (pre-trained) [6] 79.04
87.13
69.33
78.66
Ours
85.62
92.03
71.33
80.45
4.4
Performance Analysis
Ablation Experiment. To deeply evaluate the eﬀectiveness of our proposed
GLR module and LLR module, we reconstruct our model with diﬀerent ablation
factors in Table 3 on the DeepFashion-C dataset. First, we study the baseline
framework (i.e. FLR without LLR and GLR) with the identical training pro-
tocol, and the results, presented in the last row, show that it reaches a high
baseline performance. Though all key components are deleted, our proposed
baseline framework still outperforms the aforementioned SOTA methods, which
shows the adaptability and superiority of our base model. Second, to intuitively
compare results, we average each line to add an extra ‘Avg’ column. Note that
each component in our study has its own beneﬁts, particularly the proposed LLR
module, which brings the most substantial performance improvement. On aver-
age, it achieves a gain of 0.49% compared to the baseline model. Furthermore,
our proposed FLR module brings the best performance. This obviously demon-
strates that the advantages of the individual components are complementary,
and their combination leads to superior overall results.
Visualization and Analysis. To further demonstrate the role of our relational
understanding network FLR. We utilize Grad-CAM [32] to exhibit the visual-
ization of cross-attention maps in Fig. 3. The ﬁrst column contains the input
images, while the rest display the results of object location, with each attribute
accurately located. It validates that our approach could precisely perceive small
objects such as ‘V-neck’ and ‘zip’ while capturing the whole big objects like ‘life’
and ‘sheath’.

Fashion Label Relation Networks for Attribute Recognition
349
4.5
Results on Clothes Retrieval Datasets
Experimental Setup. DeepFashion provides two benchmarks for evaluat-
ing clothing retrieval tasks: In-shop Clothes Retrieval and Consumer-to-Shop
Clothes Retrieval. Every clothing item in both datasets has around 7 images.
As for In-shop Clothes Retrieval, it has 7,982 clothing items and contains
52,712 images together with 463 attributes in total. Consumer-to-Shop Clothes
Retrieval has 33,881 clothing items and 239,557 images in all and contains 303
attributes. Exactly as [6], we use the split methods like them. Similarly, We fairly
choose the top-30 and top-50 recall rates for In-shop Clothes Retrieval and the
top-20 and top-30 recall rate for Consumer-to-Shop Clothes Retrieval to align
with [6].
Performance Evaluation. The experiment results are shown in Table 4, where
the bold values indicate the best performance. We can see that our method
clearly outperforms all the others across all of the evaluation metrics. Due to the
numerous attributes in the In-Shop dataset, our model is required to eﬀectively
mine label correlations to achieve optimal performance. As a result, our model
plays a more signiﬁcant role in improving performance on the In-Shop dataset
when compared to that on the Consumer-to-Shop Clothes dataset.
Fig. 3. Visualization of cross-attention maps.
5
Conclusion
In this paper, we propose the Fashion Label Relation (FLR) network for fash-
ion image attribute recognition. Our method utilizes attention mechanisms to

350
T. Wang et al.
extract correlations between labels based on a correlation matrix. To further
improve global label relation learning, we introduce a label co-occurrence pre-
diction branch and create a new smooth label distribution, which served as two
loss constraints to make the model more robust. Experimental results on multi-
ple datasets demonstrate the eﬀectiveness of our approach. In the future, we plan
to further explore label relationships in depth to achieve higher performance.
Acknowledgments. This work was supported by the National Science Fund of China
under Grant Nos. 62176124,61876083.
References
1. Vaccaro, K., Shivakumar, S., Ding, Z., Karahalios, K., Kumar, R.: The elements
of fashion style. In: Symposium (2016)
2. Al-Halah, Z., Stiefelhagen, R., Grauman, K.: Fashion forward: Forecasting visual
style in fashion. IEEE (2017)
3. Mall, U., Matzen, K., Hariharan, B., Snavely, N., Bala, K.: Geostyle: discover-
ing fashion trends and events. In: 2019 IEEE/CVF International Conference on
Computer Vision (ICCV) (2020)
4. Liu, Z., Luo, P., Qiu, S., Wang, X., Tang, X.: Deepfashion: powering robust clothes
recognition and retrieval with rich annotations. In: IEEE (2016)
5. Wang, W., Xu, Y., Shen, J., Zhu, S.C.: Attentive fashion grammar network for fash-
ion landmark detection and clothing category classiﬁcation. In: 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (2018)
6. Zhang, Y., Zhang, P., Yuan, C., Wang, Z.: Texture and shape biased two-stream
networks for clothing classiﬁcation and attribute recognition. In: 2020 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) (2020)
7. Chen, Z.M., Wei, X.S., Wang, P., Guo, Y.: Multi-label image recognition with graph
convolutional networks. In: 2019 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) (2019)
8. Ye, J., He, J., Peng, X., Wu, W., Qiao, Y.: Attention-driven dynamic graph convo-
lutional network for multi-label image recognition. CoRR, abs/2012.02994 (2020)
9. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-
to-end object detection with transformers. CoRR, abs/2005.12872 (2020)
10. Zhang, H., et al.: Dino: Detr with improved denoising anchor boxes for end-to-end
object detection. arXiv e-prints (2022)
11. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Houlsby, N.: An image
is worth 16 × 16 words: transformers for image recognition at scale (2020)
12. Lanchantin, J., Wang, T., Ordonez, V., Qi, Y.: General multi-label image classiﬁ-
cation with transformers. In: Computer Vision and Pattern Recognition (2021)
13. Liu, S., Zhang, L., Yang, X., Su, H., Zhu, J.: Query2label: a simple transformer
way to multi-label classiﬁcation (2021)
14. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. Computer Science (2014)
15. Ge, Y., Zhang, R., Wang, X., Tang, X., Luo, P.: Deepfashion2: a versatile bench-
mark for detection, pose estimation, segmentation and re-identiﬁcation of clothing
images. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR) (2019)
16. Guo, S., et al.: The imaterialist fashion attribute dataset (2019)

Fashion Label Relation Networks for Attribute Recognition
351
17. Vaswani, A., et al.: Attention is all you need. arXiv (2017)
18. Dosovitskiy, A., et al.: An image is worth 16 × 16 words: transformers for image
recognition at scale. In: International Conference on Learning Representations
(2021)
19. Sun, C., Liu, F., Xiang, T., Hospedales, T.M., Yang, W.: Semantic regularisation
for recurrent image annotation (2016)
20. Xue, X., Wei, Z., Jie, Z., Wu, B., Yao, L.: Correlative multi-label multi-instance
image annotation. In: International Conference on Computer Vision (2011)
21. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures
for scalable image recognition (2017)
22. Chorowski, J., Jaitly, N., Chorowski, J., Jaitly, N.: Towards better decoding and
language model integration in sequence to sequence models (2017)
23. Ben-Baruch, E., Ridnik, T., Zamir, N., Noy, A., Zelnik-Manor, L.: Asymmetric loss
for multi-label classiﬁcation (2020)
24. Kullback, S., Leibler, R.A.: On information and suﬃciency. Ann. Math. Stat. 22(1),
79–86 (1951)
25. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: Interna-
tional Conference on Learning Representations (2014)
26. Jia, D., Wei, D., Socher, R., Li, L.J., Kai, L., Li, F.F.: Imagenet: a large-scale
hierarchical image database, pp. 248–255 (2009)
27. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: practical automated
data augmentation with a reduced search space. In: 2020 IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops (CVPRW) (2020)
28. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization (2017)
29. Chen, H., Gallagher, A., Girod, B.: Describing clothing by semantic attributes.
In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV
2012. LNCS, vol. 7574, pp. 609–623. Springer, Heidelberg (2012). https://doi.org/
10.1007/978-3-642-33712-3 44
30. Huang, J., Feris, R.S., Chen, Q., Yan, S.: Cross-domain image retrieval with a dual
attribute-aware ranking network. IEEE (2015)
31. Liu, J., Lu, H.: Deep fashion analysis with feature map upsampling and landmark-
driven attention. In: Leal-Taix´e, L., Roth, S. (eds.) ECCV 2018. LNCS, vol. 11131,
pp. 30–36. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-11015-4 4
32. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-
cam: visual explanations from deep networks via gradient-based localization. In:
IEEE International Conference on Computer Vision (2017)

A Modiﬁed Fuzzy Markov Random Field
Incorporating Multiple Features for Liver
Tumor Segmentation
Laquan Li(B) and Yan Jiang
School of Science, Chongqing University of Posts and Telecommunications, Chongqing 400065,
People’s Republic of China
lilq@cqupt.edu.cn
Abstract. Automated segmentation of liver tumors from computerized tomogra-
phy (CT) images plays a crucial role in computer-aided pathological diagnosis,
surgical planning, and postoperative assessment. However, liver tumors exhibit
signiﬁcant variations in size, shape, and location, often low contrast and blurry
boundaries with surrounding tissues, making the segmentation task highly chal-
lenging. To address this problem, this study proposes a novel and powerful seg-
mentation method based on fuzzy Markov Random Fields(fMRF). Without the
need for preprocessing steps, the method utilizes superpixel blocks to form coarse-
grainedfeaturesandintensity,gradientinformation,andtextureinformationtocre-
ate enhanced feature vectors representing the tumors. Meanwhile, a new potential
function is designed by combining the afﬁliation distance and multi-feature infor-
mation in the prior energy function of the model, as a way to further improve the
judgment performance of the method for label classiﬁcation. Finally, morpholog-
ical processing is applied to reﬁne the segmentation results and obtain the ﬁnal
tumor segmentation outcome. The proposed method is applied to the 3Dircadb
dataset. The results demonstrate that this method achieves superior overall seg-
mentation performance compared to many existing approaches, particularly in
scenarios involving low contrast and blurry boundaries in tumor segmentation.
Keywords: fuzzy Markov Random Field · tumors segmentation · CT images
1
Introduction
Hepatocellular carcinoma (HCC) represents a highly prevalent malignancy on a global
scale, exerting a substantial toll in terms of mortality [1, 2]. Timely identiﬁcation, pre-
cise diagnosis, and effective management of hepatocellular carcinoma play a pivotal
role in enhancing patient survival outcomes and optimizing their overall well-being. In
the realm of clinical application, computed tomography (CT) has emerged as a widely
utilized modality for the detection and characterization of hepatic neoplasms owing to
its favorable attributes such as superior signal-to-noise ratio, exceptional spatial reso-
lution, expedited scanning capabilities, and favorable cost-effectiveness. By furnishing
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 352–363, 2024.
https://doi.org/10.1007/978-981-99-8850-1_29

A Modiﬁed Fuzzy Markov Random Field Incorporating Multiple Features
353
meticulous details encompassing tumor morphology, dimensions, and spatial coordi-
nates, CT empowers clinicians with invaluable data to discern and deliberate upon opti-
mal therapeutic approaches. Nonetheless, the process of segmentation is commonly
executed through manual intervention by radiologists, a practice characterized by its
protracted duration, laborious nature, and signiﬁcant reliance on individual operators.
Consequently, the imperative to develop automatic or semi-automatic methodologies for
liver tumor segmentation has become exceedingly urgent, garnering escalating research
interest and scrutiny [3].
Precisely and reliably delineating liver tumors from CT images continues to pose
formidable challenges attributable to the following factors: (1) substantial anatomical
heterogeneity exhibited by tumors in terms of their spatial distribution, dimensions, and
morphologies; (2) indistinct demarcation of tumor boundaries with adjacent hepatic tis-
sues; and (3) inherent noise artifacts stemming from CT acquisition and contrast injection
protocols. To mitigate these challenges, a plethora of methodologies have been devised
and introduced, aiming to offer effective solution. The methodologies employed for seg-
mentation can be classiﬁed into distinct categories, namely traditional image segmenta-
tion approaches [4–7], theory-driven methodologies [8, 9] and machine learning-based
techniques [10–12]. In the realm of medical image processing, machine learning has
assumed a progressively pivotal role in recent years, manifesting as a potent paradigm
for advanced data analysis and decision-making. A substantial body of research endeav-
ors has been dedicated to the development of machine learning-based methodologies for
liver tumor segmentation. These approaches, broadly classiﬁed into two primary cate-
gories, encompass traditional learning techniques and deep learning paradigms. Each of
these approaches has its own characteristics, but there is still much room for improvement
in their performance in tumor segmentation.
Markov Random Field (MRF) model is a machine learning method with a solid theo-
retical foundation. It is widely used in image segmentation due to their ability to describe
the spatial information of an image well [13, 14]. However, classical MRF models are
deﬁned only on deterministic classes, which poses challenges in CT image segmentation
such as grayscale overlap, grayscale blurring, low contrast, and partial volume effects.
For the problems existing in medical images, many scholars use fuzzy logic and statistics
to describe the uncertainty of segmentation to accomplish the segmentation task [15–
17]. The fuzzy MRF model is the result of introducing fuzzy set theory into the model,
which can deal with both uncertainties in medical image segmentation - ambiguity and
randomness - without losing the spatial information of the image and overcoming the
effects of uncertainty due to grey-scale overlap, blurring and partial volume effects, thus
improving the image segmentation accuracy.
We are aware that hepatic lesions in CT images often exhibit highly fuzzy bound-
aries. In such cases, we consider applying a fuzzy MRF-based approach for liver tumor
segmentation of CT images. However, the conventional fuzzy MRF method utilizes only
the pixel intensity information of the image. Therefore, we construct an enhanced fea-
ture vector of the tumor from the perspective of characterizing the tumor properties,
using intensity, gradient, texture information, and coarse-grained features. Based on the
enhanced vector, a new potential function is proposed by combining the afﬁliation dis-
tance metric. It makes the method not only consider the dissimilarity judgment on the

354
L. Li and Y. Jiang
labels, but also combine the difference distance on the features, which further enhances
the accuracy of the tumor segmentation. In addition, our segmentation process was
subjected to morphological post-processing to reﬁne the tumor results of the proposed
method. We performed the validation of our method on the public dataset 3Dircadb, and
its evaluation demonstrated the effectiveness of the proposed method.
2
Proposed Method
The proposed method is a probabilistic model based on fuzzy Markov Random
Field(fMRF) theory. Figure 1 illustrates the ﬂowchart of the segmentation method. A
detailed description of the proposed method is provided below.
Fig. 1. Flowchart of the proposed liver tumor segmentation pipeline.
2.1
Feature Extraction
We traverse the CT scans of cases and select the ROI containing tumors. Then, the simple
linear iterative clustering technique (SLIC) [18] is used to generate region blocks to form
coarse-grained information and calculate the average of the region blocks as coarse-
grained features. The Kirsch operator is used to obtain the image gradient information.
The grey scale co-generation matrix (GLCM) is calculated in parallel. To reduce the

A Modiﬁed Fuzzy Markov Random Field Incorporating Multiple Features
355
computational effort, we visualize each feature map of the GLCM and select the feature
that most clearly represents tumors, i.e. the correlation feature. In order to incorporate
features in different scales, all the features are extracted in both 11 × 11 patches. These
features are incorporated into the feature vector as input to the next step.
2.2
The Modiﬁed Fuzzy Markov Random Field Model
Fuzzysegmentationofanimageisbasedonallowingeachpixeltobelongsimultaneously
to more than one class. Then, the fuzzy segmentation problem is to associate to the pixel
at location a vector xs = (us1, us2, ..., usk, ..., usK) with usk ∈[0, 1], 
k usk = 1, where
usk represents the degree of membership of the pixel s to class k, and K is the number
of classes.
In the modiﬁed multi-feature fuzzy MRF(MFMRF), three ﬁelds are used for the
description. Let S = {(i, j)|1 ≤i ≤M , 1 ≤j ≤N} be an input image where M ×
N denotes the size of the image. Y = {ys|s ∈S} is the observed random ﬁeld which
represents the observed image. X = {xs|s ∈S} is the fuzzy random ﬁeld deﬁned on S.
Z = {zs|s ∈S} is the label random ﬁeld deﬁned on S, which corresponds to the ﬁnal
segmentation results and takes value from the set  = {1, 2, ..., K}. X, as a fuzzy random
ﬁeld, is introduced to replace a classical random variable. When X loses its fuzziness, it
degenerates to the classical Markov random ﬁeld Z.
We model the above three random ﬁelds to achieve segmentation of liver tumors. The
prior knowledge of the image is obtained by fuzzy random ﬁelds, updated by maximum
the posterior probability(MAP), and ultimately the tumor segmentation is achieved by
eliminating the ambiguity for X. From MRF theory, it is clear that the image segmentation
problem based on MRF model will eventually be transformed into a combinatorial
optimization problem of the following equation:
X ∗= argmax
x P(Y|X )P(X )
(1)
P(Y|X ) is the likelihood function, and P(X ) is a joint priori distribution. Therefore,
the MFMRF model can be obtained by giving the prior energy and the likelihood energy.
Determination of the Degree of Afﬁliation. We proceed the classical fuzzy MRF
approach [17] by deﬁning the afﬁliation at pixel point s as
˜U : K →[0, 1]
k →˜U(k) =
|Nk|

k |Nk|
(2)
|Nk| is the number of pixels in its neighborhood that belong to class k. ˜U(k) is the
afﬁliation of pixel s to class k.
Establishment of the Distribution of Fuzzy Prior. X is the random ﬁeld that blurs the
label ﬁeld, and the value of each pixel in X is only relevant to its neighborhood. According
to the Hammersley–Clifford theorem [13], P(X ) obeys the Gibbs distribution which is
given by
P(X ) =
1
Z(X )exp{−U(X )
T
}
(3)

356
L. Li and Y. Jiang
where Z(X ) is the normalizing factor and U(X ) is called the fuzzy priori energy function
deﬁned on the potential group, T can be considered as a constant and is usually taken as
1.
In MRF prior distribution, its potential function is typically deﬁned in terms of
the Potts model [13], which only makes use of the dissimilarity of the labels in the
neighborhood. For the tumor characteristics in CT images, we introduce the distance of
afﬁliation in soft segmentation theory and incorporate the multi-feature information in
the image. So a novel fuzzy prior energy function U(X ) is proposed as follows
U(X ) =

s
β
2 (||xs −xt||)(Yc + σ 2
k )
σ 2
k
(4)
Here,
||xs −xt|| =

t∈Ns

k
|usk −utk|
(5)
And
Yc =

t∈Ns
yfeat
s
−yfeat
t

(6)
where yfeat
s
= (ypixel
s
, ygradi
s
, ytext
s
, ycgrain
s
) denotes the grayscale feature, gradient fea-
ture, correlation feature and coarse-grained feature vector at pixel point s. Yc represents
the feature difference between a point and its neighboring points. This distance func-
tion delineates subtle distinctions between pixels and provides a more reﬁned descrip-
tion of prior information, whereas the classical model only considers the similarity and
dissimilarity between pixels.
Establishment of the Fuzzy Likelihood Distribution. As for the likelihood term
P(Y|X ), we assume that different Ys are independent given the labels and each P(ys|xs)
obeys a Gaussian distribution, that is
P(Y|X ) = 
s P(ys|xs)
= 
s
1
√
2πσ exp{−(ys−μ)2
2σ 2
}
(7)
Compared to the classical MRF model, the likelihood distribution energy in this
approach incorporates fuzzy weights, denoted as
μ = 
kuskμk
σ 2 = 
kuskσ 2
k
(8)
μk and σ 2
k are the mean and variance of the kth class. Applying a negative logarithmic
transformation to Eq. (8), the likelihood energy function can be deﬁned as
U(Y|X ) =

s[(ys −μ)2
2σ 2
+ lnσ]
(9)

A Modiﬁed Fuzzy Markov Random Field Incorporating Multiple Features
357
By taking the negative logarithm of Eq. (1), it can be simpliﬁed into a form
of energy function minimization, which is expressed as X ∗= argmin
x U(X , Y) =
argmin
x {U(Y|X )+U(X )}. Correspondingly, the optimization objectives of the modiﬁed
multi-featured fuzzy MRF model is denoted as
X ∗= argmin
x U(X , Y)
= argmin
x {
s[ (ys−μ)2
2σ 2
+ lnσ] + 
s
β
2 (||xs −xt||) (Yc+σ 2
k )
σ 2
k
}
(10)
In this paper, we use the iterated conditional modes (ICM) approach to perform
the ﬁnal solution to Eq. (10). Algorithm 1 shows the steps of proposed practical
implementation.
Algorithm 1. The Proposed Method
Input: the observed image, Parameters {K, β, , the Maximum number of iteraƟons t_max}
Output: the segmentaƟon label Z
1. ExtracƟon features;
2. Get the iniƟal label ﬁeld Z by using the k-means algorithm to cluster the features into K
classes;
3. FuzziﬁcaƟon of Z. The aﬃliaƟon is calculated according to Eq. (2) to obtain the fuzzy ﬁeld 
X;
4. DefuzziﬁcaƟon of the fuzzy ﬁeld X. According to the principle of maximum subordinaƟon, 
the algorithm obtain an updated determinisƟc label ﬁeld Z;
5. From the label ﬁeld Z, updaƟng the mean and variance according to Eq. (8);
6. Calculate the characterisƟc ﬁeld energy from Eq. (9) and bring Eq. (5)(6) into Eq.(4) to ﬁnd 
the fuzzy random ﬁeld energy;
7. Update the label ﬁeld Z according to equaƟon (10) using the ICM;
8. Determining iteraƟon terminaƟon. If 
, the iteraƟon is terminated. Other-
wise, repeat steps 3)-7) unƟl the maximum number of iteraƟons is saƟsﬁed.
2.3
Post-processing
To obtain the tumor region, morphological post-processing was performed. In order to
eliminate isolated points and smooth the segmentation results, an opening operation
with a disk-shaped structuring element of width 1 pixel was applied to reﬁne the tumor
segmentation. This morphological operation helps in removing small isolated regions
and improving the overall segmentation accuracy by enhancing the connectivity and
smoothness of the segmented tumor region.
3
Experimental Results
3.1
Dataset and Evaluation Measures
Like most scholars, we perform the application and comparison of segmentation methods
in the public dataset 3Dircadb and LiTs. They are widely recognized and utilized within
the research community for benchmarking and validating liver-related algorithms and

358
L. Li and Y. Jiang
techniques. The LiTs dataset is composed of 131 contrast-enhanced 3D abdominal CT
scans obtained from different scanners and protocols at six different clinical sites. The
3Dircadb dataset consists of 20 contrast-enhanced CT volumes obtained from different
European hospitals. It is worth mentioning that the LiTs dataset contains 3Dircadb data.
To evaluate the performance of the proposed method quantitatively, four metrics
were computed to measure the volumetric overlap of the segmentation result compared
to ground truth. The four metrics are dice similarity coefﬁcient (DICE), volumetric
overlap error (VOE), relative volume difference (RVD) and average symmetric surface
distance(ASD). For the detailed deﬁnitions of these statistic measures, please refer to
[19]. The value of DICE ranges from 0 to 1, with a value of 0 indicating no overlap and 1
representing perfect segmentation. For the other metrics, a value of 0 represents perfect
segmentation. A negative value of RVD indicates under-segmentation.
3.2
Experimental Details
In our experiments, the parameter settings for generating coarse-grained blocks using
the SLIC method were automatically set to the total number of image pixels divided by
100. For the setting of category K, we set K to 3 as the selected ROIs usually contain
tumors, liver tissue and background areas. The parameter β in the priori energy function
is empirically set to 0.5. Additionally, we set the maximum number of iterations to 400
to ensure timeliness.
In the comparison experiments, we refer to the setup of [3, 20]. Among the several
methods compared, the deep learning based method is applied on LiTs data. These
methods split the LiTs dataset into a dataset containing 3Dircadb and the remaining data
as test and training sets, respectively. Therefore, our method only utilizes the 3Dircadb
dataset for comparison.
3.3
Results and Discussion
We perform the implementation and analysis of the proposed segmentation method on
the public dataset 3Dircadb. Figure 2 shows some example results of our proposed
method with and without post-processing. Its numerical evaluation results are displayed
in Table 1, and these results show that our method has good tumor results without post-
processing. After the post-processing step, our tumor segmentation results are more
reliable and stable.
Table 1. Performance evaluation of segmentation methods without post-processing on 3Dircadb
dataset
Method
Dice
VOE
RVD
ASD
fMRF[17]
0.73 ± 0.10
0.40 ± 0.15
0.15 ± 0.25
1.90 ± 1.05
Ours
0.77 ± 0.04
0.36 ± 0.08
−0.02 ± 0.16
1.65 ± 1.24
Figure 3 gives some segmentation results of fMRF and the modiﬁed fMRF. As seen
in the ground truth contour in the third column, the boundary between the tumor and

A Modiﬁed Fuzzy Markov Random Field Incorporating Multiple Features
359
the liver tissue is blurred. In such a context, the comparison between the result of fMRF
in the ﬁrst column and the result of our method in the second column demonstrates the
superiority of our method. The segmentation results of liver tumors at vague boundaries
are closer to the gold standard. Figure 4 demonstrates the overlap between our method
and ground truth tumor contours. It can be seen that the proposed method is also very
close to the ground truth in low contrast, especially in the case of the fourth column.
Table2presentsthesegmentationoutcomesonthe3Dircadbpublicdataset,evaluated
using the Dice coefﬁcient along with three error measures. As anticipated, the proposed
method consistently surpasses the fMRF approach across all indices, substantiating the
efﬁcacy of the modiﬁed fMRF framework for liver tumor segmentation. Reference to the
setup and comparison of [3, 20], we performed quantitative evaluations with currently
available deep learning methods U-Net [21], Deeplabv3+ [22], Trans-Unet [23], H-
DUNET [24], mU-Net [25] and two other automatic methods [3, 26], whose results are
also presented in Table 2. The table reveals that the averaged Dice values attained by the
alternative approaches are comparatively lower. However, the enhanced fMRF model,
which integrates multiple features, exhibits signiﬁcantly improved overall performance
in liver tumor segmentation, as evidenced by higher Dice and lower VOE, RVD, ASD
scores. Also, the comparison of the variance data results in the table shows that our
method has better stability. The numerical results of these four indicators are almost
always better than the compared methods. Combining the results of these indicators,
we can know the effectiveness of the proposed method. From the visualization results
presented in Figs. 2, 3, and 4, we can see that the CT image tumor regions have different
intensities and vary greatly in shape and size, yet our method still demonstrates good
segmentation results on these morphologically diverse tumors. Again, as far as training
time(Tr.t) and test time(Te.t) analysis are concerned, we refer to the comparison of [3],
Table 2. Comprehensive evaluation of the segmentation performance of our method compared
to other existing approaches using the 3Dircadb dataset.
Method
Dice
VOE
RVD
ASD
Tr.t(h)
Te.t(s)
U-Net [21]
0.68 ± 0.11
0.40 ± 0.13
−0.09 ± 0.31
2.78 ± 1.05
22.3
4.5
Deeplabv3 +
[22]
0.67 ± 0.18
0.47 ± 0.19
−0.12 ± 0.39
2.94 ± 0.88
26.7
3.7
TransUnet [23]
0.70 ± 0.13
0.44 ± 0.14
−0.19 ± 0.27
4.04 ± 3.20
30.2
7.3
H-DUNET
[24]
0.65 ± 0.02
0.49 ± 0.05
0.33 ± 0.10
5.29 ± 6.15
−
−
mU-Net [25]
0.68 ± 0.06
0.36 ± 0.14
−0.01 ± 0.18
1.58 ± 0.51
−
−
Di et al. [3]
0.71 ± 0.07
0.31 ± 0.12
−0.35 ± 0.14
1.74 ± 1.24
0.8
25.1
Moghbel et al.
[26]
0.75 ± 0.15
0.23 ± 0.12
0.09 ± 0.19
−
−
−
fMRF [17]
0.75 ± 0.13
0.38 ± 0.05
0.12 ± 0.30
1.84 ± 1.15
−
10.8
Ours
0.78 ± 0.02
0.35 ± 0.03
−0.02 ± 0.14
1.62 ± 1.20
−
12.2

360
L. Li and Y. Jiang
whose results are placed in the last two columns of Table 2. In contrast to automated
methods for deep learning, we do not require training time. The experimental results
data show that our method spends an average of 12.2 s per tumor. Although higher than
the comparison method, we do not need training time. So, the time consumed is within
acceptable limits.
Fig. 2. (a) Original image, (b) Tumor results without post-processing, (c) Tumor results with
post-processing. Segmentation results and Ground truth are shown as red and green outlines,
respectively (Color ﬁgure online)
Fig. 3. Visual comparison between fMRF and the improved method. The tumors are shown in
blue, red and green outlines in each column, respectively. (Color ﬁgure online)

A Modiﬁed Fuzzy Markov Random Field Incorporating Multiple Features
361
Fig. 4. Segmentation results for some tumors. The ﬁrst row shows the original CT images. The
second row shows a partial enlargement of the segmentation of the proposed method, and is shown
with a red outline. The third row shows a comparison of the proposed method with ground-truth,
where the split contours are in red, the ground-truth contours in green and the overlapping part of
contours in yellow. (Color ﬁgure online)
4
Conclusion
In this paper, we have presented a modiﬁed multi-feature fuzzy MRF model for hepatic
lesion segmentation. It improves upon the fMRF by incorporating the pixel feature,
the coarse-grained feature, the gradient information and the correlation feature that can
adequate representation of tumor characteristics and friendly to classiﬁcation of pixels
with blurred boundaries for segmenting hepatic lesions or liver tumors. The proposed
method eliminates the need for pre-segmentation of the liver and avoids the use of
complex and cumbersome training procedures. By incorporating enhanced characteristic
information, we have developed an improved fuzzy Markov Random Field model that
effectively minimizes and accurately identiﬁes tumor regions.
The improved method was tested on the 3Dircadb dataset and compared with exist-
ing methods. The results substantiate the effectiveness of the proposed method in accu-
rately segmenting tumors with challenging characteristics such as low contrast, blurred
boundaries, and variations in shape, location, and intensity. It surpasses existing meth-
ods in terms of segmentation accuracy and reduces the need for manual interaction, thus
achieving superior overall performance.

362
L. Li and Y. Jiang
Acknowledgement. This work was supported in part by the National Natural Science Foundation
of China (Grant No. 61901074) and the Science and Technology Research Program of Chongqing
Municipal Education Commission (Grant Nos. KJQN201900636 and KJZD-K202200606) and
the Natural Science Foundation of Chongqing (Grant No. 2022NSCQ-MSX3746) and China
Postdoctoral Science Foundation (Grant No. 2021M693771).
References
1. Camilla, M., Giuseppe, L.: Cancer statistics: a comparison between world health organization
(WHO) and global burden of disease (GBD). Eur. J. Publ. Health 30(5), 1026–1027 (2020)
2. Pilleron, S., Soto-Perez-De-Celis, E., Vignat, J., et al.: Estimated global cancer incidence in
the oldest adults in 2018 and projections to 2050. Int. J. Cancer 148(3), 601–608 (2021)
3. Di, S., Zhao, Y., Liao, M., et al.: Automatic liver tumor segmentation from ct images using
hierarchical iterative superpixels and local statistical features. Expert Syst. Appl. 203(3),
117347 (2022)
4. Moghe, A., Singhai, J., Shrivastava, S., et al.: Automatic threshold based liver lesion
segmentation in abdominal 2D-CT images. Int. J. Image Process. 5(2), 2011–2166 (2011)
5. Choudhary A., Moretto N., Ferrarese F.P., et al.: An entropy based multi-thresholding method
for semi-automatic segmentation of liver tumors. In: MICCAI Workshop, vol. 41(43), pp. 43–
49(2008)
6. Anter, A.M., Azar, A.T., Hassanien, A.E., et al.: Automatic computer aided segmentation
for liver and hepatic lesions using hybrid segmentations techniques. In: 2013 Federated
Conference on Computer Science and Information Systems, pp. 193–198. IEEE, Krakow
(2013)
7. Krishnakumar, S., Manivannan, K.: Effective segmentation and classiﬁcation of brain tumor
using rough k means algorithm and multi kernel svm in mr images. J. Ambient. Intell.
Humaniz. Comput. 12(6), 6751–6760 (2020)
8. Li, C., Wang, X., Eberl, S., et al.: A likelihood and local constraint level set model for liver
tumor segmentation from ct volumes. IEEE Trans. Biomed. Eng. 60(10), 2967–2977 (2013)
9. Siriapisith, T., Kusakunniran, W., Haddawy, P.: Pyramid graph cut: Integrating intensity and
gradient information for grayscale medical image segmentation. Comput. Biol. Med. 126,
103997 (2020)
10. Pesapane, F., et al.: Abdominal imaging. In: Amalou, H., Suh, R.D., Wood, B.J. (eds.) The
Radiology Survival Kit, pp. 95–146. Springer, Cham (2021). https://doi.org/10.1007/978-3-
030-84365-6_5
11. Aghamohammadi, A., Ranjbarzadeh, R., Naiemi, F., et al.: Tpcnn: two-path convolutional
neural network for tumor and liver segmentation in ct images using a novel encoding approach.
Expert Syst. Appl. 183, 115406 (2021)
12. Gul, S., Khan, M.S., Bibi, A., et al.: Deep learning techniques for liver and liver tumor
segmentation: a review. Comput. Biol. Med.. Biol. Med. 147, 105620 (2022)
13. Li, S.Z.: Markov random ﬁeld modeling in image analysis, 3rd edn. Springer-Verlag, Berlin
(2009)
14. Chen, S.Y., Tong, H., Cattani, C.: Markov models for image labeling. Math. Probl. Eng.Probl.
Eng. 2012, 1–18 (2012). https://doi.org/10.1155/2012/814356
15. Salzenstein, F., Pieczynski, W.: Parameter estimation in hidden fuzzy Markov random ﬁelds
and image segmentation. Graph. Models Image Process. 59(4), 205–220 (1997)
16. Ruan S., Moretti B., Fadili J., et al.: Segmentation of magnetic resonance images using fuzzy
markov random ﬁelds. In: Proceedings 2001 International Conference on Image Processing,
vol. 3, pp. 1051–1054, IEEE, Piscataway (2001)

A Modiﬁed Fuzzy Markov Random Field Incorporating Multiple Features
363
17. Liu, X., Langer, D.L., Haider, M.A., et al.: Prostate cancer segmentation with simultaneous
estimation of markov random ﬁeld parameters and class. IEEE Trans. Med. Imaging 28(6),
906–915 (2009)
18. Achanta, R., Shaji, A., Smith, K., et al.: Slic superpixels compared to state-of-the-art
superpixel methods. IEEE Trans. Pattern Anal. Mach. Intell. 34(11), 2274–2282 (2012)
19. Heimann, T., Van Ginneken, B., Styner, M.A., et al.: Comparison and evaluation of methods
for liver segmentation from ct datasets. IEEE Trans. Med. Imaging 28(8), 1251–1265 (2009)
20. Wu, W., Wu, S., Zhou, Z., Zhang, R., Zhang, Y.: 3D liver tumor segmentation in CT images
using improved fuzzy C-means and graph cuts. BioMed Res. Int. 2017, 1–11 (2017). https://
doi.org/10.1155/2017/5207685
21. Ronneberger, O., Fischer, P., Brox, T.: U-net: convolutional networks for biomedical image
segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F. (eds.) Medical Image
ComputingandComputer-AssistedIntervention—MICCAI2015.LNCS,vol.9351,pp.234–
241. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-24574-4_28
22. Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with Atrous
separable convolution for semantic image segmentation. In: Ferrari, V., Hebert, M., Sminchis-
escu, C., Weiss, Y. (eds.) Computer Vision – ECCV 2018. LNCS, vol. 11211, pp. 833–851.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01234-2_49
23. Chen, J., Lu, Y., Yu, Q., et al.: Transunet: Transformers make strong encoders for medical
image segmentation. arXiv preprint, arXiv:210204306 (2021)
24. Li, X., Chen, H., Qi, X., et al.: H-denseunet: hybrid densely connected unet for liver and tumor
segmentation from CT volumes. IEEE Trans. Med. Imaging 37(12), 2663–2674 (2018)
25. Seo, H., Huang, C., Bassenne, M., et al.: Modiﬁed u-net (mu-net) with incorporation of object-
dependent high level features for improved liver and liver-tumor segmentation in CT images.
IEEE Trans. Med. Imaging 39(5), 1316–1325 (2019)
26. Moghbel, M., Mashohor, S., Mahmud, R., et al.: Automatic liver tumor segmentation on
computed tomography for patient treatment planning and monitoring. EXCLI J. 15, 406
(2016)

Weakly Supervised Optical Remote
Sensing Salient Object Detection Based
on Adaptive Discriminative Region
Suppression
Xingyu Li1, Jieyu Wu1,2(B), Yuan Zhou1, Jingwei Yuan1, and Yanwen Chen1,2
1 School of Computer Science and Technology, Anhui University, Hefei 230601,
China
2 Anhui Provincial Key Laboratory of Multimodal Cognitive Computation,
Anhui University, Hefei 230601, China
jieyu wuu@163.com
Abstract. Salient object detection in optical remote sensing images
aims to detect attractive objects from optical remote sensing images,
providing important prior information for many remote sensing tasks,
which have received more and more attention in recent years. The exist-
ing convolutional neural network-based salient object detection networks
mostly rely on pixel-level labelling. Although their detection accuracy
is high, annotation cost for the data is high. In addition, it is always
a diﬃcult problem that the scales of salient objects in optical remote
sensing images change signiﬁcantly. To address these problems, a new
weakly supervised salient object detection method for optical remote
sensing images is proposed. Speciﬁcally, ﬁrstly, we introduce image-level
labelling as the weakly supervised information for remote sensing image
salient object detection, obtaining pseudo labels to train the saliency
detection network. Secondly, we propose the Local Activation Suppres-
sion module, including the Discriminative Region Suppression module
and Receptive Field Block, which can eﬀectively spread the high response
region of the object to the neighbouring low response region, improving
the quality of large objects pseudo labels. Finally, the Adaptive Fusion
module is proposed to raise the accuracy of pseudo labels of large and
small objects, which aims to reduce the noise caused by small objects.
Many experiments on a public dataset show that the proposed method is
better than the existing weakly supervised learning methods for salient
object detection, with better detection accuracy.
Keywords: Optical remote sensing image · Salient object detection ·
Weakly supervised learning · Image-level labelling
1
Introduction
Salient object detection(SOD) [16,27] methods aim to mimic the human
visual attention mechanism to detect and segment attractive objects in images
X. Li and J. Wu—Contributed equally to this work.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 364–375, 2024.
https://doi.org/10.1007/978-981-99-8850-1_30

Weakly Supervised ORSI SOD Based on ADRS
365
accurately. The salient objects in optical remote sensing images(ORSI) have
many categories and rich information. Recently ORSI SOD has been applied to
many remote sensing vision tasks, such as ship detection, remote sensing scene
classiﬁcation and change detection.
However, there are still many challenges in ORSI SOD. For example, ORSI
are taken by satellites or sensors at high altitudes, and the scale of salient objects
varies greatly due to the variability of shooting altitudes and angles; since there
are often multiple objects in remote sensing images, it is diﬃcult to detect multi-
ple objects more accurately; ORSI is easily aﬀected by weather and environment
when they are taken, so the background of ORSI is often complex and noisy.
Most of the existing deep-learning-based methods for ORSI SOD are strongly
supervised methods that rely on many pixel-level annotations and are costly. To
weaken the reliance of method models on accurate pixel-level annotations and
reduce the cost of dataset construction, some low-cost annotations have been
introduced for SOD, such as image-level or graﬃti tagging. For example, Piao et
al. [21] propose a multi-pseudo-label framework MFNet, which includes multiple
directive ﬁlters, aiming to extract more comprehensive and accurate saliency
cues from multiple pseudo labels and improve the accuracy of SOD by learning
and integrating the resulting accurate cues.
Considering that image-level labelling of ORSI is simple and eﬃcient, we use
image-level labelling as weakly supervised information for ORSI SOD. We train
the classiﬁcation network using image-level labelling to obtain a class activation
map(CAM) [34] that can locate the object’s local regions and obtain pseudo
labels through the post-processing process to train the saliency detection net-
work. However, the CAM only focuses on the most discriminative region. It
cannot cover the whole object, while the remote sensing image changes at a
large scale, leading to a sparse CAM for large objects. Thus, in this paper, we
design the Local Activation Suppression module(LAS), including the Discrimi-
native Region Suppression (DRS) [10] and the Receptive Field Block (RFB) [31].
DRS suppresses the high response in the CAM and spreads it to adjacent regions,
activating more regions and thus expanding the segmentation area. RFB is used
to expand the receptive ﬁeld and improve the quality of large object pseudo
labels, enhancing the eﬀectiveness of classiﬁcation network detection. Therefore,
introducing RFB and DRS modules will result in more accurate localization of
large objects in the saliency maps. For the problem that small objects will acti-
vate the background region and thus introduce much noise, we introduce the
Adaptive Fusion (AF) module [15]. This module learns a set of reliable weights
for the features before and after suppression, thus adaptively performs feature
selection and obtains fused features, improving the pseudo labels accuracy of
both large and small objects. The comparison and ablation experiments reveal
that the method in this paper outperforms existing weakly supervised methods
of SOD.
The contributions of our work are as follows: We introduce a new model
(denoted as ADRS) for weakly supervised ORSI SOD, which introduces image-
level labeling as the weakly supervised information. We design the LAS module

366
X. Li et al.
and the AF module to simultaneously improve the pseudo labels accuracy of both
small and large targets. Extensive experiments on a public dataset demonstrate
the superiority of our method.
2
Related Work
2.1
Salient Object Detection in Optical Remote Sensing Images
In recent years, deep learning-based approaches methods have achieved great
progress in the ﬁeld of computer vision [5,8,9,30]. The research of ORSI SOD
has also attracted remote sensing scholars’ attention and in-depth study. Li et
al. [14] propose a two-stream pyramid module to extract a set of complementary
multiscale information. The encoder-decoder module with nested connections
concatenates the multiscale features to predict the salient maps. Li et al. [13]
propose a novel parallel down-up fusion network for ORSI SOD to identify salient
objects of diﬀerent sizes and eliminate background confusion. Tu et al. [25] pro-
pose a joint learning framework to simultaneously optimize multi-scale salient
regions and boundary features to generate exact saliency maps.
2.2
Weakly Supervised Salient Object Detection
Although the fully supervised methods have shown some performance improve-
ment, these methods still rely on pixel-level annotations, which is expensive.
Many recent studies have used weakly supervised information for object local-
ization [18,33]. For SOD, Wang et al. [26] pioneer a network using image-level
labels. It uses the joint training strategy consisting of a foreground inference
network and a fully convolutional network. Pinheiro et al. [23] propose a weakly
supervised segmentation model, which puts more weight on pixels which are
important for image classiﬁcation during training. Tian et al. [24] use com-
plementary image-level labels to train the SOD network by weakly supervised
learning. It detects target boundaries and locates instance prime centres through
collaborative learning.
3
Method
3.1
Overall Structure
To better mine the saliency cues in pseudo labels, this paper uses MFNet [21]
as the baseline. The architecture consists of a classiﬁcation and SOD networks,
as shown in Fig. 1. In the initial phase, we label the dataset at the image level
and train the model for classiﬁcation using a network architecture, as shown in
Fig. 2. Using the classiﬁcation network, class activation maps are acquired to
determine the initial regions of salient objects within the image, and after post-
processing, the class activation maps are converted into pseudo labels. We use

Weakly Supervised ORSI SOD Based on ADRS
367
Fig. 1. The overall structure of the ADRS
the proposed LAS and AF to train the classiﬁcation network better so that the
CAM locates more salient regions and thus obtains high-quality pseudo labels.
In the second stage, we use the pseudo label to train the sali network, which
includes a saliency encoder-decoder and two directive ﬁlters. The directive ﬁlters
are utilized to extract and reﬁne highly accurate saliency cues from the noisy
pseudo labels, eﬀectively enhancing the quality and reliability of the extracted
information. The accurate saliency cues are passed to the saliency decoder using
the multicollinearity loss function to produce the ﬁnal saliency map.
3.2
Local Activation Suppression Module
In the initial stage, classiﬁcation networks are employed to generate CAM, which
are subsequently processed to derive pixel-level pseudo-labels. The CAM can
only provide discriminative regions of each target, which are usually very sparse
and cover only a small part of the object. The application to remote sensing
images will lead to the problem that the CAM can only recognise the local part
of the object and produce the missing information of a large target pseudo label.
Therefore, we improve the classiﬁcation network by designing a local activa-
tion suppression module (LAS), which is used to expand the sensory ﬁeld and
suppress the CAM’s very high local response to extend the target’s activation
region. This module consists of two parts, DRS [10] and RFB [31], to generate
dense localisation maps simply and eﬃciently.
DRS aims to suppress discriminative regions, spread attention to neighbour-
ing non-discriminative regions, eﬀectively extend the target region, and produce
a dense localisation map. DRS consists of three components: a maximum ele-
ment extractor, a suppression controller, and a suppressor. Let P ∈M W ×H×N
be the input feature map.
The maximum element extractor uses global max pooling to extract max-
imum values from each channel in the intermediate feature map P. Finally, N
maximum values are obtained, and the output composition vector is noted as

368
X. Li et al.
Fig. 2. Classiﬁcation network
Pmax ∈M 1×1×N . These N maximal elements are used as the basis for the dis-
criminant region. The suppression controller determines the degree of suppres-
sion of the discriminant part. It generates an inhibition factor for each channel
in P, which results in N inhibition factors that form vector Q ∈[0, 1]1×1×N. The
suppressor uses N maxima and N control factors to suppress the discriminative
region, speciﬁcally, Multiplying Q element by element. The upper limit of the
pixel value for each channel is obtained:
δ = Pmax × Q ∈M 1×1×N
(1)
The part above this upper bound is the discriminant region to be suppressed.
The suppression controller in this paper uses an unlearnable controller with no
additional training parameters, and each element in Q is set to a constant value
of 0.55. Next, the suppressor expands δ from M 1×1×N to M W ×H×N. After
developing the upper bound δ to P, an element-by-element minimum operation
˜P = min(P, δ) applies to P and δ to suppress the discriminant region such that
the value of P at each pixel point does not exceed the value of the corresponding
end of δ.
Large convolution kernels eﬀectively capture larger targets, whereas small
convolution kernels are more suitable for smaller targets. However, employing a
single-scale kernel is suboptimal in the case of objects with various types and
scales in ORSI. Dilated convolution with diﬀerent dilation rates can expand
the perceptual ﬁeld and extend the discriminative region to adjacent uncertain
areas. Therefore, this paper introduces RFB in the classiﬁcation network to
obtain multi-scale information. As shown in Fig. 3(a), the module incorporates a
multi-branch architecture comprising convolutional kernels of diverse sizes and
convolutional layers with varying dilation rates. Ultimately, the outputs from
multiple layers are fused to combine multi-scale features and expand the activa-
tion region, resulting in dense predictions.
The feature vector generated by convolution is passed through the sensory
ﬁeld module RFB to develop a new feature vector, denoted as ˜
M. The feature
vector generated by the discriminative region suppression module DRS immedi-
ately afterwards is noted as ˆ
M.
3.3
Adaptive Fusion Module
With the introduction of the judgment suppression region module, the local-
isation eﬀect of large targets is improved; however, small marks activate the

Weakly Supervised ORSI SOD Based on ADRS
369
background region and introduce a lot of noise information. Therefore, AF [15]
is presented in this paper, as shown in Fig. 3(b). The input of the AF module
is the feature vector ˜
M output from the RFB module and the feature vector
ˆ
M generated by the DRS module, and they are concatenated in the channel
dimension, noted as M.
Fig. 3. Receptive Field Block and Adaptive Fusion Module
The global information is ﬁrst compressed onto a one-dimensional vector
using global average pooling (GAP). Then the compressed features are passed
through two fully connected layers FC1 and FC2 to learn the non-linear inter-
actions between channels, thus obtaining a feature vector x ∈1 × R2c on a
channel.
V = FC2(FC1(GAP(M)))
(2)
where FC is the fully connected layers, after the feature vector is obtained, it
is mapped between 0 and 1 by the softmax function, transforming them into a
weighted feature map.
S = softmax(V )
(3)
Finally, the weighted feature map is split into vectors S1 and S2 by channel
dimension. The two attention vectors are multiplied and summed with the two
feature vectors
˜
M and
ˆ
M entered at the beginning of the module.The ﬁnal
feature map F is obtained:
F = S1 ˜
M + S2 ˆ
M
(4)
In conclusion, the adaptive fusion module can eﬀectively suppress background
noise while improving the localization of large and small targets in the CAM,
improving the accuracy of pseudo labels.
3.4
Multi-ﬁlter Directive Network
After obtaining the class activation maps in the ﬁrst stage, we use two diﬀer-
ent reﬁnement algorithms, including pixel-level [4] and superpixel-level [17], to
synthesize two diﬀerent pseudo labels that provide a more comprehensive cue to
supervise the training of the second stage saliency network. The saliency net-
work consists of two directive ﬁlters and an encoder-decoder saliency prediction

370
X. Li et al.
network. Both directive ﬁlters and decoders use a U-Net-type jump-connected
structure. The primary objective of the two directive ﬁlters is to extract and
reﬁne highly accurate saliency cues from the respective pseudo-labels. Utilizing
the features of the shared encoder as input, saliency cues are extracted from
the pseudo-labels using multiple convolutional layers supervised by the pseudo-
labels. Simultaneously, the convolutional layers within the two directive ﬁlters
gradually rectify the cues present in the pseudo-labels. The corresponding loss
functions L1 and L2 of the two directive ﬁlters are described as follows [21]:
Lm(Pm, Qm) = −

i
qmi ∗log pmi −(1 −qmi) ∗log(1 −pmi), m = 1, 2
(5)
where Pm and Qm denote the directive ﬁlter prediction and its pseudo-label,
respectively. We transmit these ﬁltered saliency messages to the decoder via
multicollinearity loss Lmg [21]:
Lmg(Ps, Qs) = −

i
(1 −qi) ∗log(1 −psi) −qi ∗log psi
(6)
where Ps and Qs are the decoder prediction and the average prediction of the
two ﬁlters reﬁned by the pixel level.
Furthermore, a self-supervised strategy is used between the two ﬁlters,
prompting the two ﬁlters to extract similar saliency cues, resulting in more
accurate saliency information. The loss Lss [21] of the self-supervised term is
deﬁned as:
Lss(P1, P2) = −

i
(p1i −p2i)2
(7)
The ﬁnal loss function, denoted as L, is derived by integrating the aforemen-
tioned individual loss functions:
L = L1 + L2 + Lmg + δLss
(8)
where δ is a hyperparameter that controls the weights of the self-supervised
terms.
4
Experiments
4.1
Implementation Details
The network uses DenseNet-169 as the backbone, like the latest work MFNet [21].
The network is trained on two TITAN Xp using a two-stage training approach.
The input ORSI size is set to 256 × 256. We use the Adam optimization algo-
rithm [11] to optimize the network parameters. The learning rate and batch size
are set to 1e−4 and 12.In the inference phase, the CAM is generated using a
multi-inference strategy according to the settings of AﬃnityNet [3]. Speciﬁcally,
the input image is ﬂipped and adjusted to four scales. The ﬁnal output cam is
the average of the corresponding eight CAM. For the SOD network, we only take
the images and the corresponding pseudo labels for training. The learning rate
and the maximum number of iterations are set to 3e−6 and 26000.

Weakly Supervised ORSI SOD Based on ADRS
371
4.2
Datasets and Evaluation Metrics
To validate our method denoted as ADRS, a series of experiments are conducted
on the ors-4199 dataset, which contains 4199 images with diﬀerent scenes and
their truth values: the training set contains 2000 samples from the dataset.
For quantitative evaluations, three widely used SOD evaluation indicators
are used in this paper, including mean absolute error(MAE [20]), maximum
F-measure(MaxFm [1]), and S-measure(Sm) [7] for comparison.
4.3
Comparison with State-of-the-Art Methods
Table 1. Quantitative Comparison
methods
MAE
MaxFm Sm
CPD
0.0391
0.8616
0.8562
JRBN
0.0391
0.8631
0.8596
MINet
0.0378
0.8640
0.8595
SCRN
0.0368
0.8743
0.8632
GCPANe
0.0340 0.8727
0.8624
MSW
0.0957
0.5664
0.6951
NSALWSS
0.0937
0.6220
0.6856
MFNet
0.0641
0.7231
0.7702
Ours
0.0589 0.7409
0.7758
As shown in Table
1, we compare our method with eight state-of-the-art
SOD models, including ﬁve fully supervised methods: CPD [28], JRBN [25],
MINet [19], SCRN [29], GCPANet [6], and three weakly supervised methods:
MSW [32], NSALWSS [22], MFNet [21]. Our method consistently outperforms
other weakly supervised methods under the three evaluation metrics compared.
Speciﬁcally, compared to the sub-optimal method [21], our method improves
MaxFm 1.78%. These results further validate our method has higher accuracy
and stability than other weakly supervised methods. This indicates that our
method has great potential to solve the weakly supervised SOD problem.
The visualization comparison is shown in Fig. 4. By comparing the visualiza-
tion results, we can observe our method’s superior performance on the SOD task.
Compared with other weakly supervised methods, the method of this paper can
accurately capture the objects in the image and eﬀectively separate them from
the background. In contrast, other methods have some limitations in extracting
salient objects. For example, NSALWSS [22] misclassiﬁes the backgrounds as
salient objects and accurately outlines the boundaries of objects; MFNet [21]
loses a part of the object in the large target challenge.

372
X. Li et al.
Fig. 4. Visual comparisons between our results and other methods
4.4
Ablation Studies
We apply both pixel-level and superpixel-level approaches, speciﬁcally PAMR [4]
with CRF [12] and SLIC [2] with CRF [12] to reﬁne the class activation maps
and generate two diﬀerent pseudo-labels. As shown in Table 2, our method per-
forms better in all evaluation metrics. After introducing the LAS module, the
experimental results show a slight decline compared to the baseline [21] due to
the activation of background regions by small objects, which introduces noise.
To improve the accuracy of the pseudo labels, we incorporate the AF module to
eliminate the noise information, resulting in improvements in all metrics. Speciﬁ-
cally, compared to using PAMR with CRF post-processing, our method improves
the MAE metric by 14.7%, the MaxFm metric by 4.4%, and the Sm metric by
3%. For SLIC with CRF, the improvement is 24.7% on the MAE metric, 7%
on the MaxFm metric, and 13.0% on the Sm metric, further demonstrating the
eﬀectiveness of our method.
Figure 5 shows a visual comparison between the original CAM and the CAM
obtained by the method of this paper. From the results, compared to the orig-
inal CAM, our method can cover a complete range of target regions instead
of focusing only on the most discriminative regions, possessing a more accu-
rate localization capability and eﬀectively improving the problem of sparse large
target CAM. It also shows the pseudo labels obtained in both post-processing
processes, and it can be seen that our method achieves better results than the
baseline [21] and obtains higher-quality pseudo labels, especially in large target
localization.

Weakly Supervised ORSI SOD Based on ADRS
373
Fig. 5. Visual comparison of CAM and pseudo labels
Table 2. Comparison of pseudo labels
methods
PAMR+CRF
SLIC+CRF
MAE
MaxFm Sm
MAE
MaxFm Sm
Baseline
0.0572
0.7618
0.7856
0.0838
0.6988
0.6885
+LAS
0.0928
0.6851
0.7464
0.0886
0.6295
0.6995
+LAS+AF
0.0488 0.7955
0.8095 0.0631 0.7488
0.7777
5
Conclusion
This paper introduces image-level annotation as weak supervision for ORSI SOD,
and the adaptive discriminative region suppression model is proposed. In the
proposed network, considering the problem that the class activation maps of large
targets are very sparse, we design the local suppression activation module and the
adaptive fusion module in the classiﬁcation network to eﬀectively enhance the
semantic information of pseudo labels of large targets while improving the pseudo
labels accuracy of small targets, which overcomes the problem of large target
scale variation. Quantitative and qualitative experimental results on publicly
available datasets show that this method outperforms existing related methods
for weakly supervised salient object detection.
Acknowledgement. This work was supported in part by the Natural Science Foun-
dation of Anhui Higher Education Institution of China under Grant KJ2020A0033,
in part by Anhui Provincial Natural Science foundation under Grant 2108085MF211,
in part by University Synergy Innovation Program of Anhui Province under Grant
GXXT-2022-014.
References
1. Achanta, R., Hemami, S., Estrada, F., Susstrunk, S.: Frequency-tuned salient
region detection. In: 2009 IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1597–1604. IEEE (2009)

374
X. Li et al.
2. Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., S¨usstrunk, S.: SLIC super-
pixels compared to state-of-the-art superpixel methods. IEEE Trans. Pattern Anal.
Mach. Intell. 34(11), 2274–2282 (2012)
3. Ahn, J., Kwak, S.: Learning pixel-level semantic aﬃnity with image-level super-
vision for weakly supervised semantic segmentation. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 4981–4990 (2018)
4. Araslanov, N., Roth, S.: Single-stage semantic segmentation from image labels.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 4253–4262 (2020)
5. Chen, S., et al.: Transzero++: cross attribute-guided transformer for zero-shot
learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022)
6. Chen, Z., Xu, Q., Cong, R., Huang, Q.: Global context-aware progressive aggrega-
tion network for salient object detection. In: Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, vol. 34, pp. 10599–10606 (2020)
7. Fan, D.P., Cheng, M.M., Liu, Y., Li, T., Borji, A.: Structure-measure: a new way to
evaluate foreground maps. In: Proceedings of the IEEE International Conference
on Computer Vision, pp. 4548–4557 (2017)
8. Jin, L., Wang, X., Nie, X., Liu, L., Guo, Y., Zhao, J.: Grouping by center: pre-
dicting centripetal oﬀsets for the bottom-up human pose estimation. IEEE Trans.
Multimed. 25, 3367–3374 (2022)
9. Jin, L., et al.: Single-stage is enough: Multi-person absolute 3d pose estimation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 13086–13095 (2022)
10. Kim, B., Han, S., Kim, J.: Discriminative region suppression for weakly-supervised
semantic segmentation. In: Proceedings of the AAAI Conference on Artiﬁcial Intel-
ligence, vol. 35, pp. 1754–1761 (2021)
11. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
12. Kr¨ahenb¨uhl, P., Koltun, V.: Eﬃcient inference in fully connected CRFs with gaus-
sian edge potentials. In: Advances in Neural Information Processing Systems, vol.
24 (2011)
13. Li, C., et al.: A parallel down-up fusion network for salient object detection in
optical remote sensing images. Neurocomputing 415, 411–420 (2020)
14. Li, C., Cong, R., Hou, J., Zhang, S., Qian, Y., Kwong, S.: Nested network with
two-stream pyramid for salient object detection in optical remote sensing images.
IEEE Trans. Geosci. Remote Sens. 57(11), 9156–9166 (2019)
15. Li, X., Wang, W., Hu, X., Yang, J.: Selective kernel networks. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2019)
16. Li, Z., et al.: Dense attentive feature enhancement for salient object detection.
IEEE Trans. Circuits Syst. Video Technol. 32(12), 8128–8141 (2021)
17. Liu, N., Han, J.: DHSNet: deep hierarchical saliency network for salient object
detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 678–686 (2016)
18. Meng, M., Zhang, T., Yang, W., Zhao, J., Zhang, Y., Wu, F.: Diverse comple-
mentary part mining for weakly supervised object localization. IEEE Trans. Image
Process. 31, 1774–1788 (2022)
19. Pang, Y., Zhao, X., Zhang, L., Lu, H.: Multi-scale interactive network for salient
object detection. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 9413–9422 (2020)

Weakly Supervised ORSI SOD Based on ADRS
375
20. Perazzi, F., Kr¨ahenb¨uhl, P., Pritch, Y., Hornung, A.: Saliency ﬁlters: contrast based
ﬁltering for salient region detection. In: 2012 IEEE Conference on Computer Vision
and Pattern Recognition, pp. 733–740. IEEE (2012)
21. Piao, Y., Wang, J., Zhang, M., Lu, H.: MFNEt: multi-ﬁlter directive network for
weakly supervised salient object detection. In: Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pp. 4136–4145 (2021)
22. Piao, Y., Wu, W., Zhang, M., Jiang, Y., Lu, H.: Noise-sensitive adversarial learning
for weakly supervised salient object detection. IEEE Trans. Multimed. 25, 2888–
2897 (2022)
23. Pinheiro, P.O., Collobert, R.: From image-level to pixel-level labeling with con-
volutional networks. In: Proceedings of the IEEE Computer Vision and Pattern
Recognition, pp. 1713–1721 (2015)
24. Tian, X., Xu, K., Yang, X., Yin, B., Lau, R.W.: Learning to detect instance-level
salient objects using complementary image labels. Int. J. Comput. Vision 130(3),
729–746 (2022)
25. Tu, Z., Wang, C., Li, C., Fan, M., Zhao, H., Luo, B.: ORSI salient object detection
via multiscale joint region and boundary model. IEEE Trans. Geosci. Remote Sens.
60, 1–13 (2021)
26. Wang, L., et al.: Learning to detect salient objects with image-level supervision. In:
Proceedings of the IEEE Computer Vision and Pattern Recognition, pp. 136–145
(2017)
27. Wang, W., Lai, Q., Fu, H., Shen, J., Ling, H., Yang, R.: Salient object detection
in the deep learning era: An in-depth survey. IEEE Trans. Pattern Anal. Mach.
Intell. 44(6), 3239–3259 (2021)
28. Wu, Z., Su, L., Huang, Q.: Cascaded partial decoder for fast and accurate salient
object detection. In: Proceedings of the IEEE/CVF Computer Vision and Pattern
Recognition, pp. 3907–3916 (2019)
29. Wu, Z., Su, L., Huang, Q.: Stacked cross reﬁnement network for edge-aware salient
object detection. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 7264–7273 (2019)
30. Xiao, Y., et al.: QueryPose: sparse multi-person pose regression via spatial-aware
part-level query. Adv. Neural. Inf. Process. Syst. 35, 12464–12477 (2022)
31. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. arXiv
preprint arXiv:1511.07122 (2015)
32. Zeng, Y., Zhuge, Y., Lu, H., Zhang, L., Qian, M., Yu, Y.: Multi-source weak super-
vision for saliency detection. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 6074–6083 (2019)
33. Zhao, F., Li, J., Zhao, J., Feng, J.: Weakly supervised phrase localization with
multi-scale anchored transformer network. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 5696–5705 (2018)
34. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep fea-
tures for discriminative localization. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2921–2929 (2016)

SPCTNet: A Series-Parallel CNN
and Transformer Network for 3D Medical
Image Segmentation
Bin Yu, Quan Zhou, and Xuming Zhang(B)
Department of Biomedical Engineering, College of Life Science and Technology,
Huazhong University of Science and Technology, Wuhan, China
zxmboshi@hust.edu.cn
Abstract. Medical image segmentation is crucial for lesion localization and sur-
gical navigation. Recent advancements in medical image segmentation have been
driven by Convolutional Neural Networks (CNNs) and Transformers. However,
CNNs have limitations in capturing long-range dependencies due to their weight
sharing and localized receptive ﬁelds, posing challenges in handling varying organ
shapes. While Transformers offer an alternative with global receptive ﬁelds, their
spatial and computational complexity is particularly high, especially for 3D med-
ical images. To address this issue, we propose a novel series-parallel network that
combines convolution and self-attention for 3D medical image segmentation. We
utilize a serial 3D CNN as the encoder to extract multi-level feature maps, which
are fused via a feature pyramid network. Subsequently, we adopt four parallel
Transformer branches to capture global features. To efﬁciently model long-range
information, we introduce patch self-attention, which divides the input into non-
overlapping patches and computes attention between corresponding pixels across
patches. Experimental evaluations on 3D MRI prostate and left atrial segmentation
tasks conﬁrm the superior performance of our network compared to other CNN and
Transformer-based networks. Notably, our method achieves higher segmentation
accuracy and faster inference speed.
Keywords: Deep learning · 3D Medical image segmentation · Transformer
1
Introduction
Medical image segmentation plays a vital role in computer-aided lesion localization,
treatmentplanning,andsurgicalnavigation.Accuratedelineationofstructuresinmedical
images is essential for tasks such as quantitative analysis and personalized intervention
[1]. However, the complexity, variability, and noise inherent in medical images pose
signiﬁcant challenges for achieving accurate and robust segmentation.
Over the past decade, deep learning (DL) has revolutionized medical image analysis
by addressing the limitations of traditional segmentation methods [1, 2]. Unlike tradi-
tional approaches that heavily rely on manual feature engineering and heuristic algo-
rithms like region growing, watershed and active contour [3–5], these DL-based meth-
ods employ automatic feature extraction from raw image data. The dominant paradigm
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 376–387, 2024.
https://doi.org/10.1007/978-981-99-8850-1_31

SPCTNet: A Series-Parallel CNN and Transformer Network
377
in medical image segmentation has been U-shaped fully convolutional neural networks
(CNNs) since the inception of U-Net [2, 6–9]. The architecture enabling effective feature
fusion across different semantic levels [2]. Attention U-Net [6] extends this concept by
employing attention modules to dynamically focus on relevant image regions, thereby
facilitating capture of intricate details in complex segmentation scenarios. To further
increase U-Net’s representation capability, researchers have introduced progressive fea-
ture fusion methods. U-Net++ [7] integrates dense skip connections and multi-scale
feature aggregation to ensure better integration of contextual information at different
scales. Furthermore, to enhance feature extraction capabilities or improve the intercon-
nections within the U-shaped architecture, several variants such as Res-UNet [8] and
UNet3+ [9] have been proposed.
Despite the improved efﬁciency and accuracy of medical image segmentation
achieved by CNN-based methods, their ability to capture long-range information remains
limited due to localized receptive ﬁelds [10]. This limitation is particularly critical in the
segmentation of anatomically diverse organs and tumors. Recently, Transformer [11] has
emerged as a compelling alternative due to its achievements in natural language process-
ing and computer vision. Chen et al. [12] have designed TransUNet, which incorporates
Transformer to capture global contexts by encoding tokenized CNN feature maps. These
encoded features are later fused with detailed CNN feature maps in the decoder module.
Similarly, UNetR [13] utilizes a Transformer encoder to tokenize the image patches and
performs upsampling and feature fusion at different layers using CNN blocks. Despite
the improved performance of TransUNet and UNetR over the traditional CNN meth-
ods, the standard self-attention mechanism poses signiﬁcant computational challenges,
especially in 3D image. To reduce the computational and spatial complexity, Xie et al.
[14] have introduced a deformable self-attention mechanism into Transformer, focusing
on a limited number of key points in the sequence. This attention mechanism effectively
improves efﬁciency while maintaining performance. Inspired by Swin Transformer [15],
Swin-Unet [16] has combined the hierarchical Transformer blocks within the U-Net
framework. Nonetheless, more in-depth research is required to explore the effective
strategies for enhancing feature interaction and optimizing the fusion of convolution
and self-attention mechanisms.
In this paper, we propose a series-parallel CNN and Transformer network (SPCTNet)
that combines convolution and self-attention for volumetric segmentation. Speciﬁcally,
we utilize a serial 3D CNN structure to extract multi-level feature maps in the encoder.
The extracted features are preliminarily fused through the feature pyramid network
(FPN) [17], enhancing the encoder’s capacity to learn features across multiple levels.
These features are then fed into parallel Transformer blocks, enabling the network to cap-
ture global context. Inspired by MobileViT [18] and Swin Transformer, we introduce
patch self-attention (PSA) to realize the 3D image segmentation. Similar to window
attention [15], PSA partitions the image into non-overlapping patches, computes atten-
tion between corresponding pixels across patches. This approach signiﬁcantly reduces
the computation of Transformer so that it can process feature maps at higher resolutions.
Our method has been extensively evaluated on three 3D medical image datasets,
including the ProstateX Challenge dataset [21], Atrial Segmentation Challenge dataset
[22] and a private prostate dataset from Zhongnan Hospital of Wuhan University. By

378
B. Yu et al.
leveraging the series-parallel architecture, multi-scale feature fusion, and patch self-
attention mechanism, our method exhibits higher segmentation accuracy compared to
existing algorithms. Additionally, the parallel computing mechanism employed in our
method enables faster inference speed.
2
Method
2.1
Architecture
The overall framework of the proposed SPCTNet is shown in Fig. 1. SPCTNet consists
of three primary components: a serial CNN encoder, parallel Transformer blocks, and a
segmentation head. The encoder utilizes 3D ResNeXt [19], a highly effective network
architecture for extracting features from the input 3D volume. Speciﬁcally, we denote
the features from the last four residual blocks as {X1, X2, X3, X4}, corresponding to
conv1, conv2, conv3, and conv4 layers with respective strides of {4, 8, 16, 32}. Notably,
conv0 is excluded from the feature pyramid to prevent memory overﬂow.
Fig. 1. Network architecture of SPCTNet.
To optimize the interaction of features across various scales and semantic levels, we
utilize a feature pyramid network (FPN), incorporating a top-down pathway to upsample
higher-level semantic features to align with the resolution of lower-level features. Lateral
connections are utilized to fuse the {X1, X2, X3, X4} with the corresponding lower-level
ones. The preliminary fused feature maps {Y1, Y2, Y3, Y4} that capture multi-scale
information are obtained via FPN. These feature maps are then fed in parallel to the
Transformer blocks, enabling the model to learn long-range dependencies and model
complex relationships between different regions of the input.

SPCTNet: A Series-Parallel CNN and Transformer Network
379
Additionally, the pixel attention [6] is employed between two Transformer blocks to
selectively enhance multi-scale contextual information. Following the ﬁrst Transformer
block, the feature maps from different branches are upsampled to a uniﬁed resolution.
Next, these features are concatenated and processed through a convolution module to
generate a fused feature representation. We selectively enhance the features from the
four branches with the fused feature via pixel attention. The features are further pro-
cessed by the second Transformer block, producing outputs denoted as {Z1, Z2, Z3,
Z4}. The outputs are then concatenated and passed through a cascaded convolution
block before undergoing trilinear upsampling to obtain the ﬁnal prediction. To incorpo-
rate deep supervision, we also perform upsampling on {Z1, Z2, Z3, Z4} to obtain the
auxiliary predictions.
2.2
Transformer Block
Although FPN is capable of integrating features from different receptive ﬁelds, it is
limited by the inherent locality of convolution operations. To address this constraint,
we propose the patch self-attention (PSA) module for efﬁcient modeling of long-range
contextual dependencies.
Fig. 2. The structure of the Transformer block with the patch self-attention. The volume is ﬂat-
tened and reshaped at the input and output. Here, the patch size is 2 × 2 × 2, and self-attention is
computed within pixels of the same color across different patches.
As illustrated in Fig. 2, the feature tensor I ∈RH×W×D×C with (H, W, D) spatial
dimensions and C channels is ﬂattened and reshaped at the input and output. Speciﬁcally,
the input is divided into non-overlapping patches, where each patch has a resolution of
(ph, pw, pd). However, unlike previous approaches [13] that directly represent these
patches as tokens, we ﬂatten them into a 3D tensor, I ∈RH×W×D×C →I′ ∈RS×N×C,
where S = ph × pw × pd represents both the volume of a patch and the number of
subsequences, N = H × W × D/S is the length of each subsequence. Self-attention is
computed within pixels across different patches. In our task, the patch sizes for the ﬁrst
four PSA blocks are 4 × 4 × 4, 2 × 2 × 2, 2 × 2 × 2, and 1 × 1 × 1 while the patch size
for the remaining four blocks is consistently set to 4 × 4 × 4. The Transformer blocks
are computed as:
ˆzi = PSA(Norm(zi−1) + zi−1
zi = MLP(Norm(ˆzi)) + ˆzi
(1)
The output features of the PSA and the MLP block for layer i are denoted as ˆzi and
zi, , respectively. Layer normalization (Norm) is applied, and the MLP module consists
of two linear layers with non-linear activation functions.

380
B. Yu et al.
A PSA block is composed of S parallel self-attention submodules which can effec-
tively learn long-range dependencies within a three-dimensional space with a receptive
ﬁeld of the entire image. The computation method for self-attention remains unchanged
and can be formulated as:
Attention(Q,K,V) = Softmax(QKT/
√
d) V
(2)
where Q, K, V ∈RS×N×d are the query, key and value matrices; d is the dimension of
key. Positional encoding is not utilized in the sublayer due to the inherent characteristics
of zero padding, which is used in the convolutional layers to maintain the consistent
feature map dimensions.
The computational complexity of the global MSA and PSA modules is:
(MSA) = (H · W · D)2C,
(PSA) = (H · W · D)2C/(ph · pw · pd)
(3)
By setting the patch size as ph = pw = pd = 2, the computational complexity of the
PSA becomes only 1/8 of that of the multi-head self-attention (MSA) [11]. Moreover, our
model effectively learns image representations across multiple feature spaces without
the need for artiﬁcial partitioning in the MSA mechanism.
2.3
Multi-scale Feature Fusion (MSF)
The ﬁrst Transformer block operates in parallel to generate features {Z1, Z2, Z3, Z4}
at four different scales. These scale-speciﬁc features are upsampled to a uniform size
and concatenated along the channel dimension. Subsequently, a reﬁnement process is
performed by passing the concatenated features through a convolutional module with
1 × 1 × 1 and 3 × 3 × 3 kernels, resulting in a reﬁned fused feature representation
denoted as Zf . To compute the pixel-wise attention as shown in Fig. 3, Zf is individually
used in conjunction with each of four scale-speciﬁc features.
Fig. 3. Scheme of the pixel attention module. By applying convolution, fusion, and activation
functions, the module generates attention weight αi for each pixel.
The pixel-wise attention is given by:
αi = Sigmoid(Wm(Relu(WiZi + Wf Zf )))
(4)
where Wi, Wf are 1 × 1 × 1 convolutions used to align Zi and Zf in channels, and Wm
is a 1 × 1 × 1 convolution that reduces the channels to 1. These operations adapt the

SPCTNet: A Series-Parallel CNN and Transformer Network
381
feature maps for the pixel attention mechanism. αi is the attention coefﬁcient selectively
enhancing the feature responses of Zi with respect to Zf .
Next, the attention coefﬁcient αi is multiplied with Zf in an element-wise way, result-
ing in a weighted fusion of the reﬁned feature. This fused representation is then concate-
nated with Zi along the channel dimension, yielding an integrated feature representation.
By incorporating the pixel attention, our method adaptively adjusts the contribution of
each scale in Zi based on its relevance to Zf .
2.4
Loss Function
We utilize a hybrid of loss functions to reduce training time and optimize the segmen-
tation accuracy. The Dice loss [20], commonly used for evaluating the overlap between
predictions and labels, captures ﬁne-grained details and ensures spatial consistency.
Additionally, the incorporation of cross-entropy loss encourages pixel-wise classiﬁ-
cation. To further enhance learning and address the vanishing gradient problem, we
introduce deep supervision by calculating the loss between the four intermediate feature
representations. The loss function is computed as:
ℓ= 1−2
M
M

j=1
N
i=1 Pi,jGi,j
N
i=1 P2
i,j + N
i=1 G2
i,j
−1
N
M

j=1
N

i=1
Gi,j log Pi,j
(5)
where M is the num of classes and N is the number of voxels in the input volume; At
voxel i, Pi,j ∈[0.0, 1.0] is the network’s prediction for class j, while Gi,j ∈{0.0, 1.0}
is corresponding segmentation label. The deep supervision loss is calculated as the
summation of losses from both the segmentation head and the four parallel branches:
ℓtotal = wf ℓf +
n

i=1
wiℓi
(6)
where ℓf and ℓi represent the losses of the segmentation head and the i-th branch, wf
and wi are denote the corresponding weights. To ensure a balanced contribution from
each branch during the learning process, we set the weights (wf , wi=1,2,3,4) as (1.0, 0.2,
0.3, 0.4, 0.5) according to experience.
3
Experiments
3.1
Datasets
We have selected two public datasets and one private dataset for experiments, including
the SPIE-AAPM-NCI ProstateX Challenge dataset [21], the 2018 Atria Segmentation
Challenge dataset [22], and a private prostate MRI dataset from Zhongnan Hospital of
Wuhan University. The ProstateX Challenge dataset consists of multiparametric mag-
netic resonance imaging (mpMRI) data and clinical information of patients suspected of
prostate cancer. For the mpMRI data, we employ a subset of the dataset containing 914
cases of T2-weighted imaging (T2WI) with a size of 256 × 256 × 60 and a resolution of

382
B. Yu et al.
0.66 × 0.66 × 1.50 mm3. Among the dataset, we randomly partitioned the data into train-
ing, validation, and test sets, consisting of 639, 137, and 138 cases, respectively. The
atrium dataset includes 154 gadolinium-enhanced magnetic resonance imaging scans
with a resolution of 0.625 × 0.625 × 0.625 mm3. We use 100 scans for training and 27
scans each for validation and testing. Additionally, our in-house prostate MRI dataset
reviewed by the Ethics Committee of Zhongnan Hospital. The dataset consists of 200
3D scans with a size of 320 × 320 × 24 and a resolution of 0.56 × 0.56 × 3.00 mm3.
We split it into140 cases for training and reserve 30 cases each for validation and testing.
3.2
Experimental Settings and Evaluation Metrics
In our experimental setup, we use common online data augmentation methods including
random ﬂipping, rotation, and cropping. The ProstateX and in-house prostate datasets
are randomly cropped to sizes of 192 × 192 × 48 and 256 × 256 × 24, respectively.
Similarly, for the left atrium dataset, we choose a crop size of 128 × 128 × 80. The
framework is based on MindSpore Lite tool [23]. In order to accelerate training process,
NVIDIA GeForce A100 GPU with 40 GB memory is used. The network is optimized
using AdamW with a weight decay of 0.01. The learning rate is initialized at 0.001 and
changes to 0.0001 using the cosine decay during the training. Our method is trained
on the ProstateX, the atrium and our in-house prostate datasets for 90, 60, 60 epochs,
respectively. Following the reference [24], we utilize a sliding inference technique with
a stride of 16 × 16 × 4 to segment the entire image for model testing.
To evaluate the efﬁcacy of our method and enable comparative analysis, we adopt
four quantitative metrics, i.e., Jaccard index (Jaccard), Dice coefﬁcient (Dice), 95%
Hausdorff Distance (95HD), and average symmetric surface distance (ASD) [25, 26].
3.3
Ablation Study
We conduct ablation experiments to examine the individual contribution of patch self-
attention (PSA) and multi-scale fusion (MSF) in our network. Table 1 presents Jaccard
and Dice of our network with/without PSA and MSF on the ProstateX dataset. Clearly,
after removing the MSF component (Model 2), the Jaccard of our method decreases by
0.97%. To ensure fairness in terms of parameters, we replace the PSA module with two
convolution layers during its removal (Model 1) and the corresponding Jaccard shows a
reduction of 2.57%. Ablation results clearly demonstrate the signiﬁcant impact of both
MSF and PSA on the performance of our network.
Table 1. Ablation experiments on ProstateX dataset.
Methods
PSA
MSF
Jaccard(%)
Dice(%)
Model 1
×
×
82.61
90.37
Model 2
√
×
84.21
91.32
SPCTNet
√
√
85.18
91.92

SPCTNet: A Series-Parallel CNN and Transformer Network
383
3.4
Quantitative Evaluation
To validate the superiority of the SPCTNet, we perform a quantitative evaluation with six
CNN and Transformer-based segmentation approaches: 1) U-Net; 2) Attention U-Net;
3) TransBTS [27]; 4) Swin-UNETR [28]; 5) CoTr [14]; 6) nnFormer [29]. As U-Net is
originally designed for 2D segmentation, we have modiﬁed its architecture to enable 3D
segmentation. Table 2, Table 3 and Table 4 represent the quantitative results on the three
datasets. On the ProstateX dataset, our method achieves a Jaccard index of 85.18% and
a Dice coefﬁcient of 91.92%, which are higher than other methods. As regards inference
time, our method achieves the fastest inference speed.
Similarly, on the private prostate, our method outperforms the most competitive
method with a 1.73% higher Jaccard index, 1.11% higher Dice coefﬁcient, 0.39 lower
95HD, and 0.23 smaller ASD. On the left atrium dataset, our method outperforms other
methods with at least a 1.08% higher Jaccard index.
Table 2. Quantitative results of different methods on the ProstateX dataset.
Methods
Jaccard(%)
Dice(%)
95HD(voxel)
Inference Time(s)
U-Net
83.49
90.91
3.33
6.24
Attention U-Net
83.64
91.10
3.37
6.41
TransBTS
83.88
91.14
3.07
8.45
Swin-UNETR
84.30
91.39
2.98
12.81
CoTr
83.97
91.20
3.41
10.72
nnFormer
84.12
91.28
3.02
12.51
SPCTNet
85.18
91.92
2.88
5.52
Table 3. Quantitative results of different methods on the in-house prostate dataset.
Methods
Jaccard(%)
Dice(%)
95HD(voxel)
ASD(voxel)
U-Net
64.85
77.89
6.35
1.94
Attention U-Net
66.00
78.72
5.80
1.99
TransBTS
66.77
79.24
9.76
2.52
Swin-UNETR
67.59
79.98
4.21
1.16
CoTr
66.91
79.56
3.84
1.37
nnFormer
68.43
80.71
3.92
1.34
SPCTNet
70.16
81.82
3.53
1.11
3.5
Qualitative Evaluation
In Fig. 4, we present the 2D and 3D views of segmented results for the various methods on
the two prostate datasets. Our method provides more consistent segmented results with
the segmentation ground truths compared with other methods. The superiority of our

384
B. Yu et al.
method may stem from its successful integration of deep supervision loss and the efﬁcient
utilization of multi-feature interaction fusion. The former can facilitate capturing more
discriminative features across different depths by introducing intermediate supervision
signals at multiple layers. The latter can facilitate capturing and integrating information
from multiple scales to provide more comprehensive representations of prostate structure
and enhanced boundary localization.
Table 4. Quantitative results of different methods on the left atrium dataset.
Methods
Jaccard(%)
Dice(%)
95HD(voxel)
ASD(voxel)
U-Net
82.94
90.67
4.36
1.57
Attention U-Net
84.33
91.45
5.40
1.46
TransBTS
83.59
91.06
5.83
1.62
Swin-UNETR
85.19
91.94
4.66
1.40
CoTr
84.50
91.60
4.58
1.23
nnFormer
84.90
91.77
4.82
1.39
SPCTNet
86.27
92.63
4.58
1.19
To further validate the efﬁcacy of our proposed MSF, the attention maps in Fig. 5
provide valuable insights into the allocation of attention within the fused feature for each
layer.Clearly,theattentionisconcentratedinregionsclosetotheprostatecontours,align-
ingwiththeobservationsinFig.4.Thisindicatesthatourmethodeffectivelyconcentrates
Fig. 4. Segmented results of various methods on ProstateX (top) and private datasets (bottom).
Green color represents ground truths and red color represents predictions.

SPCTNet: A Series-Parallel CNN and Transformer Network
385
on relevant information within the fused feature, leading to improved representation of
individual layers and enhanced segmentation accuracy.
Fig. 5. The visualization of the multi-scale feature fusion block. Feature maps {f1, f2, f3, f4}
are upsampled to a consistent size and fused together. Pixel attention maps {a1, a2, a3, a4} are
calculated using the fused feature and the former feature maps.
4
Conclusion
In this paper, we have introduced SPCTNet, a series-parallel network that combines
CNN and Transformer for volumetric segmentation. Distinctively, the presented method
extracts multi-scale features effectively using 3D CNN and captures global information
using Transformer. The integration of the patch self-attention mechanism, multi-scale
information, and deep supervision loss contributes to improved segmentation result.
Indeed, the SPCTNet can deliver superior segmentation performance by effectively
leveraging the strengths of both CNN and Transformer. Extensive evaluations on three
MRI datasets have conﬁrmed its advantage over six baseline approaches in Jaccard,
Dice, 95HD, and ASD. Future directions include exploring its applicability to other
segmentation tasks and extending our method to additional medical modalities.
Acknowledgment. This work is sponsored by the National Natural Science Foundation of China
(Grant No. 61871440), China Postdoctoral Science Foundation (Grant No. 2023M731204), and
CAAI-Huawei MindSpore Open Fund.

386
B. Yu et al.
References
1. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmenta-
tion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3431–3440 (2015)
2. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomedical image
segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F. (eds.) Medical Image
ComputingandComputer-AssistedIntervention—MICCAI2015.LNCS,vol.9351,pp.234–
241. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-24574-4_28
3. Adams, R., Bischof, L.: Seeded region growing. IEEE Trans. Pattern Anal. Mach. Intell.,
641–647 (1994)
4. Beucher, S., Meyer, F.: The morphological approach to segmentation: the watershed trans-
formation. In: Mathematical Morphology in Image Processing, pp. 433–481. CRC Press
(2018)
5. Kass, M., Witkin, A., Terzopoulos, D.: Snakes: active contour models. Int. J. Comput. Vis.,
321–331 (1988)
6. Oktay, O., Schlemper, J., et al.: Attention U-Net: learning where to look for the pancreas.
arXiv preprint arXiv:1804.03999 (2018)
7. Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J.: Unet++: a nested U-Net archi-
tecture for medical image segmentation. In: Stoyanov, D., et al. (eds.) Deep Learning in
Medical Image Analysis and Multimodal Learning for Clinical Decision Support. LNCS,
vol. 11045, pp. 3–11. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-00889-5_1
8. Xiao, X., Lian, S., Luo, Z., et al.: Weighted res-unet for high-quality retina vessel segmen-
tation. In: 2018 9th International Conference on Information Technology in Medicine and
Education (ITME), pp. 327–331. IEEE (2018)
9. Huang, H., Lin, L., et al.: UNet 3+: a full-scale connected U-Net for medical image seg-
mentation. In: IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 1055–1059. IEEE (2020)
10. Prajit, R., Niki, P., et al.: Standalone self-attention in vision models. arXiv preprint arXiv:
1906.05909 (2019)
11. Vaswani, A., Shazeer, N., Parmar, N., et al.: Attention is all you need. In: Advances in Neural
Information Processing Systems, pp. 5998–6008 (2017)
12. Chen, J., Lu, Y., Yu, Q., et al.: TransUNet: transformers make strong encoders for medical
image segmentation. arXiv preprint arXiv:2102.04306 (2021)
13. Hatamizadeh, A., Tang, Y., Nath, V., et al.: UNETR: transformers for 3D medical image seg-
mentation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision, pp. 574–584 (2022)
14. Xie, Y., Zhang, J., Shen, C., Xia, Y.: CoTr: efﬁciently bridging CNN and transformer for 3D
medical image segmentation. In: de Bruijne, M., et al. (eds.) Medical Image Computing and
Computer Assisted Intervention – MICCAI 2021. LNCS, vol. 12903, pp. 171–180. Springer,
Cham (2021). https://doi.org/10.1007/978-3-030-87199-4_16
15. Liu, Z., Lin, Y., Cao, Y., et al.: Swin transformer: hierarchical vision transformer using shifted
windows. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 10224–10233 (2021)
16. Cao, H., Wang, Y., Chen, J., et al: Swin-Unet: unet-like pure transformer for medical image
segmentation. In: Karlinsky, L., Michaeli, T., Nishino, K. (eds.) European Conference on
Computer Vision, pp. 205–218. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-
25066-8_9
17. Lin, T.Y., Dollár, P., Girshick, R., et al.: Feature pyramid networks for object detection. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2117–
2125 (2017)

SPCTNet: A Series-Parallel CNN and Transformer Network
387
18. Mehta, S., Rastegari, M.: MobileViT: light-weight, general-purpose, and mobile-friendly
vision transformer. arXiv preprint arXiv:2110.02178 (2021)
19. Xie, S., Girshick, R., Dollár P., et al.: Aggregated residual transformations for deep neural net-
works. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1492–1500 (2017)
20. Yang, X., Bian, C., Yu, L., Ni, D., Heng, P.-A.: Hybrid loss guided convolutional networks for
whole heart parsing. In: Pop, M., et al. (eds.) Statistical Atlases and Computational Models
of the Heart. ACDC and MMWHS Challenges. LNCS, vol. 10663, pp. 215–223. Springer,
Cham (2018). https://doi.org/10.1007/978-3-319-75541-0_23
21. Geert,L.,Oscar,D.,Jelle,B.,Nico,K.,Henkjan,H.:ProstateXchallengedata.CancerImaging
Arch. (2017). https://doi.org/10.7937/K9TCIA.2017.MURS5CL
22. Xiong, Z., et al.: A global benchmark of algorithms for segmenting the left atrium from late
gadolinium-enhanced cardiac magnetic resonance imaging. Med. Image Anal. 67, 101832
(2021)
23. MindSpore. https://www.mindspore.cn
24. Yu, L., Wang, S., Li, X., Fu, C.-W., Heng, P.-A.: Uncertainty-aware self-ensembling model
for semi-supervised 3D left atrium segmentation. In: Shen, D., et al. (eds.) Medical Image
Computing and Computer Assisted Intervention – MICCAI 2019. LNCS, vol. 11765, pp. 605–
613. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-32245-8_67
25. Chang, H.H., Zhuang, A.H., Valentino, D.J., et al.: Performance measure characterization for
evaluating neuroimage segmentation algorithms. Neuroimage 47(1), 122–135 (2009)
26. Litjens, G., Toth, R., et al.: Evaluation of prostate segmentation algorithms for MRI: the
PROMISE12 challenge. Med. Image Anal. 18(2), 359–373 (2014)
27. Wang, W., Chen, C., Ding, M., Yu, H., Zha, S., Li, J.: TransBTS: multimodal brain tumor
segmentation using transformer. In: de Bruijne, M., et al. (eds.) Medical Image Computing and
Computer Assisted Intervention – MICCAI 2021. LNCS, vol. 12901, pp. 109–119. Springer,
Cham (2021). https://doi.org/10.1007/978-3-030-87193-2_11
28. Hatamizadeh, A., Nath, V., Tang, Y., et al.: Swin unetr: swin transformers for semantic seg-
mentation of brain tumors in mri images. In: Crimi, A., Bakas, S. (eds.) International MICCAI
Brainlesion Workshop, pp. 272–284. Springer, Cham (2021). https://doi.org/10.1007/978-3-
031-08999-2_22
29. Zhou, H., Guo, J., Zhang, Y., et al.: nnFormer: interleaved transformer for volumetric
segmentation. arXiv preprint arXiv:2109.03201 (2021)

LANet: A Single Stage Lane Detector
with Lightweight Attention
Qiangbin Xie, Xiao Zhao, and Lihua Zhang(B)
Intelligent Perception and Autonomous Systems Laboratory (IPASS),
Academy for Engineering and Technology, Fudan University,
Shanghai 200433, China
21210860087@m.fudan.edu.cn, lihuazhang@fudan.edu.cn
Abstract. Currently,
lane
detection
is
one
of
the
key
tasks
in
autonomous driving. Numerous lane detection methods have achieved
high accuracy. However, there is still ample room for improvement in
developing techniques that can meet both the accuracy and real-time
requirements of driving scenarios. In our work, we proposed an anchor-
based deep lane detection method named LANet. It meets the needs
of lane detection in the case of insuﬃcient information, and achieves a
balance between accuracy and eﬃciency. Since the feature extraction of
slender lane is extremely challenging, obtaining enough lane information
is crucial for accurate lane detection. Therefore, we propose to absorb the
indirect learning surplus of lane representation, by analyzing the latent
distribution of lane features in other insuﬃciently informative scenarios.
At the same time, we also use a more lightweight attention mechanism
to improve the co-detection ability of anchors. Our approach also yields
promising outcomes on two publicly available datasets, striking a favor-
able balance between detection accuracy and eﬃciency, which is crucial
for practical lane detection in driving scenarios.
Keywords: Lane detection · self-driving · attention
1
Introduction
The emergence of deep learning technology has promoted the development of
many traditional computer vision. In the ﬁeld of intelligent driving, many deep
learning methods are used in its perception system [4,12]. Especially, it is crucial
for vehicles to leverage mathematical modeling based on deep learning in order to
perceive its surrounding environment. The initial work focused on solving simple
scenarios such as highways. However, with the rapid development of road traﬃc,
eﬀectively dealing with the long-tail eﬀect(such as severe shading, marking wear,
extreme weather, etc.) in lane detection has become a more concerned issue [23].
Early work [5,27,28] relied on the assumption of parallel geometry between
pairs of traﬃc lines, and employed handcrafted ﬁlters [2,6,10,17] to extract line
segment features. These segments were then clustered into diﬀerent traﬃc lines.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 388–399, 2024.
https://doi.org/10.1007/978-981-99-8850-1_32

LANet: A Single Stage Lane Detector with Lightweight Attention
389
However, many recent works [14,15,19,22] have adopted CNNs to implement the
aforementioned two-stage solution. The current two-stage approach [2,14,15,19,
22] is limited, and the clustering operation conducted during post-processing fails
to preserve the global information of lanes. As a result, distinguishing between
diﬀerent lanes becomes challenging, particularly in scenarios involving shadows
or severe occlusion.
The lane itself has strong shape characteristics, such as thin, long and
straight. And it occupies only a few pixels in the overall driving image. For lane
detection tasks, accurately capturing the valid information among the abundant
interference information is crucial to improve eﬃciency. However, the majority
of methods ignore it. In addition, when there exists a series of problems such as
extreme lighting and occlusion, it makes more diﬃcult to capture information.
With this condition, model can’t directly learn this lane representation from
the information in this scenario. In some cases the local information may not
be enough to predict the lane. Some work [24] propose to use self-attention for
aggregate global information. However, the computational complexity of self-
attention is quadratic and its scope is limited to a single self feature. Inspired by
human perception, mining the potential correlations among frames can facilitate
the model’s collaborative prediction of lane, and enhances the model’s robust-
ness.
In this work, we present a method for lane detection that takes into account
both eﬃciency and accuracy in a balanced manner, with a focus on lane fea-
ture detection based on strong shape priors of the lanes. In the particularly, we
introduce deformable convolution operation in feature extraction to enhance the
adaptive ability of the feature extraction network to lane shape. We employ a
novel anchor-frame attention mechanism, which is more lightweight than conven-
tional attention mechanism, to improve the generalization perception of model.
Our method conducts extensive experiments on two public datasets, and makes
a trade-oﬀbetween accuracy and eﬃciency in the design details of the module.
In summary, our main contributions are:
• A feature extraction network that is more suitable for lane tasks is introduced
with deformable convolution, enabling the network to adapt better to object
shape and extract more precise features;
• The independent memory unit is utilized to acquire the latent relationship
between anchors and datasets, for the collaborative detection ability of the
model in the lack of information;
• Our approach demonstrates excellent performance in terms of both speed and
accuracy.
2
Related Work
In this section, we present the mainstream detection methods based on deep
learning: classiﬁcation-based [13,19–21,30], regression modeling-based [9,25],
and transformer architecture-based methods [3,8,11,29].

390
Q. Xie et al.
Classiﬁcation-Based Methods. Since the segmentation method is a two-stage
method of pixel-level classiﬁcation in nature, we consider it as a class of clas-
siﬁcation methods. For it, a segmentation map is generated from original pixel
features. Then, it is decoded into distinct lane through post-processing steps.
SCNN [19] exhibits strong spatial relationship for lane. However, its eﬃciency
is weaken by repeated convolution operation. Later, SAD [13] incorporated self-
attention distillation to signiﬁcantly enhance detection speed. When it is faced
with the dazzle environment, its performance is lacking. Another approach based
on classiﬁcation is through dividing the rows or columns of the input image into
grids, and then classiﬁes each grid cell. The concept was initially introduced in
E2E-LMD [30]. In UFLD [20], the authors achieve fast detection speed with a
loss of detection accuracy. However, the later version of UFLD (UFLD-V2) [21]
improves this problem by adopting a mixed classiﬁcation method based on both
rows and columns. Nonetheless, their FDR and FNR are at a relatively high
level.
Regression-Based Modeling Methods. PolyLaneNet [25] uses polynomial
representation to ﬁt the lane expression instead of a set of marked points. Since
B´ezierLaneNet [9] considers polynomial coeﬃcients diﬃcult to optimize, it pro-
poses to use third-order B´ezier curves to ﬁt the lanes globally. B´ezier curve
regression only needs a few control points to describe the curve.
Transformer-Based Methods. Beneﬁting from the vision transformer [3,8,18]
on vision tasks, many work [11,26,29] start using it. RCLane [29] is compati-
ble with the bifurcated lane locally, and considers the lane structure globally.
Laneformer [11] puts the feature map into transformer-encoder and transformer-
decoder, while using the information of object detection to capture the connec-
tion between the lane and the driving vehicle. They are excellent in detection
accuracy. However, the use of a transformer results in a decrease in detection
eﬃciency.
3
Proposed Method
LANet is a single-stage lane detection method that utilizes discrete ray points
(referred to as anchors) for improved accuracy. Figure 1 provides an overview
of the method, which takes RGB images (Foriginal ∈R3∗H∗W ) captured by the
front-facing camera as input and outputs precise boundary detection for lanes.
Section 3.2 describes backbone and pooling. A neural network with both regular
convolution and deformable convolution [7,31] is employed as the backbone.
The resulting feature map is then used for subsequent feature pooling operation
based on anchors that are discrete ray points. The pooled features will serve as
query in our anchor-frame attention module (see Sect. 3.3), generating panoramic
semantic that integrate both self-feature and collaboration-feature (referred to as
co-feature). In Sect. 3.4, the panoramic features are fed into two fully connected
layers to generate classiﬁcation and localization prediction.

LANet: A Single Stage Lane Detector with Lightweight Attention
391
Fig. 1. Overview of the proposed method. A backbone generates feature maps from an
input images. The anchor-frame attention is used for the anchor-based pooling feature
to obtain the collaboration features. Finally, the panoramic features were used for
classiﬁcation and regression, which push the ﬁnal prediction.
3.1
Anchor Representation
A lane is represented by a set of 2D coordinate points that are equidistant
longitudinally and remain in a ﬁxed and ordered sequence. For any point Pi =
(xi, yi) from that, its vertical coordinate yi can be calculated as i ∗
H
Nstrip−1,
where H represents the number of longitudinal pixels in the picture and Nstrip
represents the number of sampling strips. Assuming Y = {yi}Nstrip−1
i=0
represents
the sequence of ordinates, each xi will have unique one yi corresponding to it.
Hence, a lane can be uniquely determined by its corresponding lateral coordinate
sequence X = {xi}Nstrip−1
i=0
, as shown in Fig. 2. The majority of lane points are
expected to appear a speciﬁc range on the vertical axis. Therefore, our method
uses the starting and ending indices to deﬁne a continuous and valid sequence
of lateral coordinates, thereby reducing unnecessary computations.
The concept of anchor line was initially proposed by Line-CNN [16]. Our
method utilizes it during both inference and training. An anchor is a line segment
of ﬁnite length that originates from a point Porigin on the boundary, and extends
to the boundary of the image with a certain slope. The system allows for the
multiple anchor based on various angles and starting positions, thereby catering
to the vast majority of lane markings encountered in real-world scenarios.
3.2
Backbone and Feature Pooling
First of all, model extract high-level features from raw features. However, we
observe that the feature distribution of lane across the entire image exhibits

392
Q. Xie et al.
Fig. 2. The Examples of anchors based on diﬀerent starting points or angles
strong shape properties. Therefore, we incorporate oﬀset and scaling modulation
mapping into the process of feature extraction. The oﬀset was used to adjust
the mapping oﬀset, and the scaling modulation was used to control the size of
the oﬀset to prevent its oﬀset from being too large. Finally, the regular-shaped
convolution kernel was replaced with an adaptive one, to adapt to the elongated
and straight shape characteristics of lane. The schematics of oﬀset and scaling
modulation are shown in “Backbone” from Fig. 1.
To avoid exploding caculation, we only use them into high-level seman-
tic. In the oﬀset process, the pixel point coordinates are transformed from an
integers to an ﬂoats, requiring the use of bilinear interpolation to obtain fea-
ture values at these non-integer coordinates. So this stage generates the fea-
ture map Fbackbone ∈RC′∗HF ∗HF from the original feature Forigin ∈R3∗H∗W .
Moreover, we reduce the feature dimensionality of Fbackbone, getting the ﬁnal
Fmap ∈RC∗HF ∗WF .
Feature pooling based anchors can facilitate feature utilization. For any point
PF = (xF , yF ) located on the feature map Fmap, mapping based on Porigin and
α, a unique xF can be obtained for each value of yF in the set {0, 1, 2, ..., HF −1}.
xF = [xorigin
σ
+ yF −yorigin
σ
tanα
]
(1)
The backbone produces an σ-fold downsize Fbackbone from Forigin. After map-
ping based on Eq. 1, an anchor-based feature vector fi ∈RC∗HF can be obtained.
In cases where apart of the anchor is outside the boundaries of Fmap, fi ∈RC∗HF
is zero-padded. The vector will serve as the query for anchor-frame attention in
the next stage.
3.3
Anchor-Frame Attention
The computational complexity of self-attention and its limitation between single
self-feature show possibility of improvement. Therefore, our method proposes an
anchor-frame attention mechanism that is linear complexity. Two shared external
memory units is independent of feature information to capture potential collab-
orative relationships between diﬀerent anchor and all of frame from datasets.

LANet: A Single Stage Lane Detector with Lightweight Attention
393
The anchor-frame attention can be formulated as:
F(key′, query) ∗value′ = attention feature
(2)
where F(key′, query)i,j is the similarity between the i−th anchor and the j −th
row of memory key. Compared with the self-attention mechanism, the key and
value in anchor-frame attention are no longer generated by feature projection.
Using two linear layers as memory units simpliﬁes the computational complexity.
Because external memory units is much fewer than input features. A separate
Softmax is generally used as the normalization layer, but it may lead to the
corresponding co-feature being particularly large or small when self-feature value
is particularly large or small, thereby compromising the original meaning of
attention. So it is necessary to use a double normalization that adding an L1
norm after Softmax.
3.4
Prediction
For each predeﬁned anchor, their feature vectors aggregated through anchor-
frame attention and self-feature are fed into two parallel fully connected layers,
one performing classiﬁcation and the other performing regression, which together
generate the ﬁnal proposal. The classiﬁcation layer is responsible for predicting
the probability distribution (K + 1 conﬁdence scores) of each anchor belonging
to one of K lane types, denoted as Ci = {c0, c1, ...cK} . The regression layer
is responsible for predicting the horizontal oﬀset of each point based on the
current anchor and valid indexes. It includes the number of lane points and
their corresponding horizontal oﬀsets within a given range, denoted as Ri =
(n, {o0, o1, ...}).
3.5
Non-maximum Supression (NMS)
In addition to using a substantial number of anchors as proposals for lane, we
also incorporate deformable convolution and anchor-frame attention mechanism
in the feature extraction and aggregation stages, respectively. Therefore, our
method also requires NMS to ﬁlter most repeat detection. We utilize a distance
metric (Line-CNN) to quantify the separation of common range between two
anchors or anchors and a lane, as shown in Fig. 2. And the calculated metric
is used in both training and testing phases. Anchor A1 and A2, for example,
they can be respectively by the corresponding sequence of lateral coordinates
to determine the abscissa, namely A1 = {x1i}end1
i=start1, A2 = {x2i}end2
i=start2. The
start index of the common range is then deﬁned as the larger of the two anchor’s
start indexes, while the end index is deﬁned as the smaller of their end indexes.
In other words, start′ = max(start1, start2) and end′ = min(end1, end2).
D(A1, A2) =

+∞,
otherwise
1
end′−start′+1
end′
i=start′ |x1i −x2i|,
end′ ≥start′
(3)

394
Q. Xie et al.
3.6
Model Training
Based on the aforementioned lane distance metric, it is necessary to allocate
positive and negative samples for the anchors. The distance between the anchors
remaining after NMS ﬁltering and the ground-truth is calculated one by one.
Among them, anchors whose distance from the ground-truth is less than Dp
are regarded as positive anchors, while anchors whose distance is more than Dn
are negative anchors. Anchors that fall halfway between these two thresholds
are disregarded as their contribution to the model’s training for this task is
negligible. The positive anchor AP and negative anchor AN will be utilized
to compute the loss for classiﬁcation and regression tasks, thereby guiding the
model during training. The total loss function is deﬁned as:
L({Ci, Ri}AP &N−1
i=0
) = λ ∗Losscls + Lossreg
(4)
Losscls =

i
FL(Ci, C∗
i ),
Lossreg =

i
L1(Ri, R∗
i )
(5)
where Ci, Ri are the prediction results of classiﬁcation layer and regression layer
for anchori, and where C∗
i , R∗
i are the classiﬁcation and regression targets of
the model for anchori. The parameter λ is used to balance classiﬁcation and
regression for the total loss. Due to the imbalanced distribution of positive and
negative samples in the datasets, Focal Loss is employed for loss calculation
in classiﬁcation tasks. For the regression task’s loss calculation, Smooth L1 is
utilized. It is worth noting that if anchori is considered as a negative anchor
during sample assignment, it will only be taken into account for the classiﬁcation
loss calculation. For positive anchors, the regression loss only calculates the error
between each pair of horizontal oﬀsets within the common index interval shared
by both the predicted valid index range and ground truth index range.
4
Experiments
This section mainly describes the rules of the evaluation metrics and the details
of the experiment execution. Our approach will be evaluated on two of the most
widely used public lane detection datasetss. Tusimple [1] is only contains high-
way scenes, CULane [19] is divided nine categories, such as crowded, night,
absence of visible lines, etc. Their main parameters are presented in Table 1. All
of our experiments use default metrics set by the creators of the datasets. The
two ﬁrst subsection compare the method with the existing excellent methods
(see Table 2 and Table 3), and the two ﬁnal subsection conduct ablation analysis
of the characteristic parts of this method.
The speed of detection is an important factor to judge the real-time perfor-
mance of lane detection method. This method uses frames-per-second (FPS) to
evaluate its speed of detection. Our method will pick 72 and 128 starting points
from the left and right boundary and the bottom boundary, respectively, and
then take 6 and 15 diﬀerent angles based on them for anchors. But we picked
out 1,000 of anchors that have a high frequency of being assigned as positive

LANet: A Single Stage Lane Detector with Lightweight Attention
395
Table 1. Detail of the datasets about Tusimple and CULane.
datasets
#Frame Tra.
Val.
Test
Resolution #Max #Scenarios
Tusimple 6,480
3,268
358
2,782
1280 × 720 5
1
CULane
133,235
88,880 9,675 34,680 1640 × 590 4
9
samples in the training phase and the testing phase, for improving eﬃciency.
The model parameters were Nstrip = 72, Dp = 15, Dn = 20, K = 1.
By changing the number of doformable convolution and where they are used
in our experiments, we found that using it only for high-level semantic features
can play a great role. The last two layers are saturated with deformable convo-
lutions, so we don’t need to add them more than once. Otherwise, it will bring
a large number of calculations and limit the detection speed. In anchor-frame
attention mechanism, we set the dimension of the attention matrix of the mem-
ory unit to 64, and the performance improvement brought by continually adding
the number of unit is actually not worth the loss.
Table 2. The results of mainstream methods on Tusimple. (* represents the Source-
code is unavaliable)
Method
F1(%)↑Acc(%)↑FDR(%)↓FNR(%)↓FPS↑
Line-CNN [16]∗
96.79
96.87
4.42
1.97
30
E2E [30]∗
96.40
96.04
3.11
4.09
-
Laneformer [11]∗
95.58
96.56
5.39
3.37
61
RCLane [29]∗
97.64
96.58
2.28
2.27
43.8
SCNN [19]
95.97
96.53
6.17
1.80
7.5
ENet-SAD [13]
95.92
96.64
6.02
2.05
75
UFLD-V2 [21]
96.16
95.65
3.06
4.61
330
LaneATT [24]
96.77
95.63
3.53
2.92
171
B´ezierLaneNet [9] 95.03
95.41
5.30
4.60
-
LANet
96.81
95.59
3.41
2.98
105
4.1
Tusimple
Evaluation Metrics. On TuSimple, the three standard metrics are false dis-
covery rate (FDR), false negative rate (FNR), and accuracy. The accuracy is
deﬁned as:
Acc =

i Cframei

i Sframei
(6)
where Cframei is the number of lane points predicted correctly (the diﬀerence
between the ground-truth point and the predicted point is less than 20 pixels)

396
Q. Xie et al.
in the i-th frame, and Sframei is the total number of points in the i-th frame.
For a lane prediction to be considered a true positive (for the FDR and FNR
metrics), its number of correct predicted points has to be greater than 85%.
4.2
CULane
Evaluation Metrics. The only metric is the F1, which is based on the inter-
section over union (IoU) of lane areas. The datasets’s oﬃcial metric considers
the lanes as 30-pixels-thick lines. If a prediction has an IoU greater than 0.5 with
a ground-truth lane, it is considered a true positive (Fig. 3).
Table 3. The results of mainstream methods on CULane. (* represents the Source-code
is unavaliable)
Category
E2E [30]∗
Laneformer [11]∗RCLane [29]∗
SCNN [19]
Normal
90.00
88.60
93.59
90.60
Crowded
69.70
69.02
78.77
69.70
Dazzle
60.20
64.07
72.44
58.50
Shadow
62.50
65.02
84.37
66.90
No-line
43.20
45.00
52.77
43.40
Arrow
83.20
81.55
90.31
84.10
Curve
70.30
60.46
78.39
64.40
Cross↓
2296
25
907
1990
Night
63.30
64.76
73.96
66.10
Total
70.80
71.71
80.03
71.60
FPS
-
61
43.8
7.5
ENet-SAD [13] UFLD-V2 [21] LaneATT [24]
B´ezierLaneNet [9] LANet
90.10
91.70
92.14
90.22
91.50
68.80
73.00
75.03
71.55
74.02
60.20
64.60
66.47
62.49
64.87
65.90
74.70
78.15
70.91
79.12
41.60
47.20
49.39
45.30
47.06
84.00
87.60
88.38
87.16
88.91
65.70
67.70
67.72
58.98
63.29
1998
1998
1330
996
1170
66.00
70.20
70.72
68.70
69.82
70.80
74.7
76.68
73.67
75.59
75
330
171
-
105

LANet: A Single Stage Lane Detector with Lightweight Attention
397
Fig. 3. LANet qualitative results on TuSimple (the ﬁrst row), CUlane (the second row).
Blue lines are GT, while green and red lines are TP and FP, respectively. (Color ﬁgure
online)
4.3
Ablation Study
This experiment evaluates the impact of each major part of the proposed method:
deformable convolution and anchor-frame attention. The results are shown in
Table 4. In the second row, the anchor-frame attention mechanism was removed.
In the third row, the deformable convolution was replaced with conventional
convolution. The drop of performance when anchor-frame attention mechanism
is removing shows its importance of enhance collaborative detection ability.
And it has little impact on FPS due to its linear complexity computation. The
deformable convolution increase the model performance relatively small. How-
ever, our method achieves a balance between eﬃciency and accuracy, and does
not compromise one with the other.
Table 4. Ablation study results on CUlane
Method
F1(%) FPS
LANet
75.59
105
-anchor-frame attention
74.07
117
-deformable convolution 75.23
143
5
Conclusion
We proposed an anchor-based single-stage deep lane detection model to balance
accuracy with eﬃciency. In order to balance them, we use an attention mecha-
nism with linear complexity. On the one hand, we use the potential connections
in diﬀerent scenarios to collaborate with anchors for better lane prediction, and
on the other hand, we control the computing cost of the attention mechanism
to achieve the purpose of improving detection eﬃciency. In the phase of feature

398
Q. Xie et al.
extraction and aggregation, our method makes full use of the strong lane shape
prior condition that is ignored by most work, and improves the generalization
ability of anchor in diﬀerent scenes through a novel attention mechanism. The
experimental results on two large public datasets show that our method achieved
a high balance between accuracy and eﬃciency. However, due to the inherent
characteristics of anchor itself, our method performs poorly in curved lane, bifur-
cating lane and high degree of freedom lane. This may be the next direction that
anchor-based methods should strive for.
References
1. Tusimple benchmark (2017). https://github.com/TuSimple/tusimple-benchmark
2. Aly, M.: Real time detection of lane markers in urban streets. In: 2008 IEEE
Intelligent Vehicles Symposium, pp. 7–12. IEEE (2008)
3. Ashish, V.: Attention is all you need. In: Advances in Neural Information Process-
ing Systems, vol. 30, I (2017)
4. Badue, C., et al.: Self-driving cars: a survey. Expert Syst. Appl. 165, 113816 (2021)
5. Bertozzi, M., Broggi, A.: Gold: a parallel real-time stereo vision system for generic
obstacle and lane detection. IEEE Trans. Image Process. 7(1), 62–81 (1998)
6. Chiu, K.Y., Lin, S.F.: Lane detection using color-based segmentation. In: IEEE
Proceedings. Intelligent Vehicles Symposium, pp. 706–711. IEEE (2005)
7. Dai, J., et al.: Deformable convolutional networks. In: Proceedings of the IEEE
International Conference on Computer Vision, pp. 764–773 (2017)
8. Dosovitskiy, A., et al.: An image is worth 16x16 words: transformers for image
recognition at scale. arXiv preprint arXiv:2010.11929 (2020)
9. Feng, Z., Guo, S., Tan, X., Xu, K., Wang, M., Ma, L.: Rethinking eﬃcient lane
detection via curve modeling. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 17062–17070 (2022)
10. Gonzalez, J.P., Ozguner, U.: Lane detection using histogram-based segmentation
and decision trees. In: ITSC2000. 2000 IEEE Intelligent Transportation Systems.
Proceedings (Cat. No. 00TH8493), pp. 346–351. IEEE (2000)
11. Han, J., et al.: Laneformer: object-aware row-column transformers for lane detec-
tion. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 36, pp.
799–807 (2022)
12. Hong, Z., et al.: Semantic compression embedding for generative zero-shot learning.
In: IJCAI, Vienna, Austria, vol. 7, pp. 956–963 (2022)
13. Hou, Y., Ma, Z., Liu, C., Loy, C.C.: Learning lightweight lane detection CNNs by
self attention distillation. In: Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pp. 1013–1021 (2019)
14. Huval, B., et al.: An empirical evaluation of deep learning on highway driving.
arXiv preprint arXiv:1504.01716 (2015)
15. Lee, S., et al.: VPGNet: vanishing point guided network for lane and road marking
detection and recognition. In: Proceedings of the IEEE International Conference
on Computer Vision, pp. 1947–1955 (2017)
16. Li, X., Li, J., Hu, X., Yang, J.: Line-CNN: end-to-end traﬃc line detection with
line proposal unit. IEEE Trans. Intell. Transp. Syst. 21(1), 248–258 (2019)
17. McCall, J.C., Trivedi, M.M.: An integrated, robust approach to lane marking detec-
tion and lane tracking. In: IEEE Intelligent Vehicles Symposium, pp. 533–537.
IEEE (2004)

LANet: A Single Stage Lane Detector with Lightweight Attention
399
18. Pan, W., Wu, H., Zhu, J., Zeng, H., Zhu, X.: H-ViT: hybrid vision transformer
for multi-modal vehicle re-identiﬁcation. In: Fang, L., Povey, D., Zhai, G., Mei,
T., Wang, R. (eds.) Artiﬁcial Intelligence: Second CAAI International Conference,
CICAI 2022, Beijing, China, 27–28 August 2022, Revised Selected Papers, Part
I, pp. 255–267. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-20497-
5 21
19. Pan, X., Shi, J., Luo, P., Wang, X., Tang, X.: Spatial as deep: spatial CNN for
traﬃc scene understanding. In: Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, vol. 32 (2018)
20. Qin, Z., Wang, H., Li, X.: Ultra fast structure-aware deep lane detection. In:
Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol.
12369, pp. 276–291. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-
58586-0 17
21. Qin, Z., Zhang, P., Li, X.: Ultra fast deep lane detection with hybrid anchor driven
ordinal classiﬁcation. IEEE Trans. Pattern Anal. Mach. Intell. (2022)
22. Revilloud, M., Gruyer, D., Rahal, M.C.: A lane marker estimation method for
improving lane detection. In: 2016 IEEE 19th International Conference on Intelli-
gent Transportation Systems (ITSC), pp. 289–295. IEEE (2016)
23. Romera, E., Alvarez, J.M., Bergasa, L.M., Arroyo, R.: ERFNet: eﬃcient residual
factorized convnet for real-time semantic segmentation. IEEE Trans. Intell. Transp.
Syst. 19(1), 263–272 (2017)
24. Tabelini, L., Berriel, R., Paixao, T.M., Badue, C., De Souza, A.F., Oliveira-Santos,
T.: Keep your eyes on the lane: real-time attention-guided lane detection. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 294–302 (2021)
25. Tabelini, L., Berriel, R., Paixao, T.M., Badue, C., De Souza, A.F., Oliveira-Santos,
T.: PolyLaneNet: lane estimation via deep polynomial regression. In: 2020 25th
International Conference on Pattern Recognition (ICPR), pp. 6150–6156. IEEE
(2021)
26. Wang, Y., Qian, W., Li, M., Zhang, X.: A transformer-based network for
deformable medical image registration. In: Fang, L., Povey, D., Zhai, G., Mei,
T., Wang, R. (eds.) Artiﬁcial Intelligence: Second CAAI International Conference,
CICAI 2022, Beijing, China, 27–28 August 2022, Revised Selected Papers, Part
I, pp. 502–513. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-20497-
5 41
27. Wang, Y., Shen, D., Teoh, E.K.: Lane detection using spline model. Pattern
Recogn. Lett. 21(8), 677–689 (2000)
28. Wang, Y., Teoh, E.K., Shen, D.: Lane detection and tracking using B-Snake. Image
Vis. Comput. 22(4), 269–280 (2004)
29. Xu, S., et al.: RCLane: relay chain prediction for lane detection. In: Avidan, S.,
Brostow, G., Ciss´e, M., Farinella, G.M., Hassner, T. (eds.) Computer Vision-
ECCV 2022: 17th European Conference, Tel Aviv, Israel, 23–27 October 2022,
Proceedings, Part XXXVIII, pp. 461–477. Springer, Cham (2022). https://doi.
org/10.1007/978-3-031-19839-7 27
30. Yoo, S., et al.: End-to-end lane marker detection via row-wise classiﬁcation. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition Workshops, pp. 1006–1007 (2020)
31. Zhu, X., Hu, H., Lin, S., Dai, J.: Deformable ConvNets V2: more deformable,
better results. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 9308–9316 (2019)

Visible and NIR Image Fusion Algorithm
Based on Information Complementarity
Zhuo Li and Bo Li(B)
Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China
{lizhuo320,boli}@buaa.edu.cn
Abstract. Visible and near-infrared (NIR) band sensors provide images
that capture complementary spectral radiations from a scene. And the
fusion of the visible and NIR image aims at utilizing their spectrum
properties to enhance image quality. However, currently visible and NIR
fusion algorithms cannot well take advantage of spectrum properties,
as well as lack information complementarity, which results in color dis-
tortion and artifacts. Therefore, this paper designs a complementary
fusion model from the level of physical signals. First, in order to dis-
tinguish between noise and useful information, we use two layers of the
weight-guided ﬁlter and guided ﬁlter to obtain texture and edge layers,
respectively. Second, to generate the initial visible-NIR complementar-
ity weight map, the diﬀerence maps of visible and NIR are ﬁltered by
the extend-DoG ﬁlter. After that, the signiﬁcant region of NIR night-
time compensation guides the initial complementarity weight map by
the arctanI function. Finally, the fusion images can be generated by the
complementarity weight maps of visible and NIR images, respectively.
The experimental results demonstrate that the proposed algorithm can
not only well take advantage of the spectrum properties and the informa-
tion complementarity, but also avoid color unnatural while maintaining
naturalness, which outperforms the state-of-the-art.
Keywords: Image Fusion · Near-Infrared · Low Light · Color
Distortion · Signal Complementarity
1
Introduction
Recent studies have demonstrated various strategies for concurrently acquiring
visible and Near-infrared (NIR) images, utilizing silicon-based sensors as imaging
technology advances [14,18]. Especially for low-light enhancement, utilizing two
image sensors with specialized optical components, one of which takes visible
spectra while the other captures near-infrared spectra by adding near-infrared
light correction, as shown in Fig. 1. The information of the two spectra can be
combined by image fusion to generate high-quality images in various applications
for night vision systems [1,13,31,32].
For combining NIR information with visible images, numerous image fusion
techniques have been developed, including traditional [12,20,22,25] and deep-
learning methods [14,17]. The traditional methods focus on the intensity channel
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 400–411, 2024.
https://doi.org/10.1007/978-981-99-8850-1_33

Fusion Algorithm Based on Information Complementarity
401
of the source images. The color information of visible spectra is retained by
determining the chroma terms, ﬁnally used to reform the greyscale fused image
into a color image.
(a)
(b)
Fig. 1. Visible and NIR image captured by adding near-infrared light compensation
In [21], bilateral and weighted least squares ﬁlters were employed by Sharma
et al. to produce images that were upgraded with greater detail information
and higher contrast. By taking into account the structural variations between
visible and NIR images, Elliethy et al. [7] method maintained the essential detail
and edge information of a visible image. [24] proposed a mapping model that
maintains local contrast in the NIR image while changing the NIR image values
to correspond to the visible image corresponding pixels in the luminance plane.
In [23], the relative diﬀerence of the local contrasts in the visible and NIR image
was used to estimate a fusion map initially. In order to create the enhanced
fusion image, the approach ﬁrst extracted spatial features from the NIR image.
Then the details were weighted in accordance with the fusion map and combined
with the visible image. Additionally, Connah et al. proposed the image fusion
technique based on the Spectral Edge (SpE) [5] is a promising algorithm for
combining images in the gradient domain. The kind of fusion method, however,
requires a lot of computation and iteration.
Some data-driven methods were developed based on the deep learning theory
[6,10,17]. For the fusion of visible and infrared images, Li et al. [10] suggested a
deep learning system with an encoder, fusion layer, and decoder model. And the
Dark Vision Net (DVN) [11], a near-infrared visible light fusion method based
on Deep Structure and Deep Inconsistency Prior (DIP) was proposed. Deep
Structure is used to extract structural information from multi-scale features of
extracted depth, avoiding introducing a lot of noise directly from the original
input. Then based on this structural information, DIP uses structural incon-
sistency to guide the fusion of visible-NIR. The above method based on deep
learning has made remarkable achievements, but its defect is that it requires a
large number of training samples and computing power. The quality of image
fusion depends on the training label material, model capacity, loss function selec-
tion, super parameter selection, and many other aspects.

402
Z. Li and B. Li
Overall, the above-mentioned schemes require a color compensation process
to generate a natural color-looking image. For many applications, it is desirable
to retain the characteristic colors of the scene. However, modifying the chroma
terms often leads to undesirable false-color image renderings [29]. Therefore, we
think more attention should be paid to the analysis of complementary at the
level of image data, which is a display of physical signals. This paper designs a
complementary fusion model from the level of physical signals. First, in order
to distinguish between noise and useful information, we use two-scale image
ﬁlters to obtain texture and edge layers, by the weight-guided ﬁlter and guided
ﬁlter in proper order. Second, to generate the initial complementarity weight
map, the diﬀerence maps of visible and NIR are ﬁltered by the extend-DoG
ﬁlter. After that, the signiﬁcant region of NIR guides the initial complementarity
weight map by the arctanI function. Finally, the fusion images can be generated
by the complementarity weight maps of visible and NIR images, respectively.
The experimental results demonstrate that the proposed algorithm can not only
well take advantage of the properties of diﬀerent spectra and the information
complementarity, but also avoid color distortion while preserving chromaticity
information of visible images, which outperforms the state-of-the-art.
The rest of this paper is organized as follows. The complementary theory
and related work above the visible and near-infrared (NIR) are both covered in
Sect. 2. The speciﬁcs of the proposed fusion algorithm are described in Sect. 3.
Section 4 assesses both the objective and subjective performance of the suggested
algorithm. Finally, Sect. 5 concludes the paper.
2
Related Work
As we have discussed in [16], each spectral band has diﬀerent spectrum proper-
ties, providing diﬀerent kinds of physical information. The similarity and diﬀer-
ence between visible and NIR images have been addressed in [1,9]. The authors
computed the correlation between visible and NIR gradients, and use the gra-
dients of the visible image in reconstructing NIR only where the gradients are
highly correlated. Meanwhile, many researchers often assumed that the high-
frequency information of the visible-NIR channels is strongly correlated in [3,19].
Moreover, according to the theory of spectral reﬂection, the diﬀerence
between visible and NIR spectra is the reﬂection-dependent molecular clus-
ters [19,31]. The visible spectral reﬂection is based on the structural system in
molecules called chromophores, therefore the visible images are colorful and suit-
able for human visual perception. However, the NIR spectral reﬂects the compo-
sition and molecular structure information of most types of organic compounds,
namely NIR radiation is material dependent and with no color information.
It can be found that there are signiﬁcant similarities between visible and
near-infrared information that can be transformed, however, there are also some
that cannot be obtained through transformation. In this paper, the information
of one spectral cannot be transformed from the other one is called complemen-
tarity information. We think the color information of visible bands could not be

Fusion Algorithm Based on Information Complementarity
403
replaced by NIR [8], therefore, the R, G, and B three bands of visible are com-
bined with NIR band, respectively. Meanwhile, in low-light conditions utilizing
two image sensors to capture visible spectra and near-infrared spectra by adding
near-infrared light compensation, the NIR image contains a high signal-to-noise
ratio, which can be supplemented for the visible image.
Fig. 2. The overall structure of the proposed fusion algorithm.
3
The Proposed Algorithm
3.1
Two-Scale Guided Image Decomposition
The proposed fusion algorithm is shown in Fig. 2, in order to better distinguish
between noise and useful information, as well as maximize the utilization of high-
frequency information, we use two-scale image ﬁlters to obtain texture layers and
edge layers, by weighted guided and guided ﬁlter respectively. As shown in Fig. 3,
we can see that the texture layer has more noise than the edge layer, which is
more clear.
Considering the dark and uneven illumination conditions at night, the high-
frequency noise of visible images in the texture layer is high. In order to distin-
guish noise and texture information in the ﬁrst layer, we adopt the weight-guided
ﬁlter (WGIF) proposed in [15], which can preserve sharp edges like the global
ﬁlters, similar to the guided ﬁlter (GIF) in [22]. Besides that, the WGIF also
avoids gradient reversal, therefore the halo artifacts can be avoided by the WGIF.
After the ﬁrst layer of ﬁltering decomposition, the second layer ﬁltered by the
GIF will contain clear edge information. To some extent, the ﬁrst layer of ﬁlter-
ing can reduce the gradient of large-scale edges, which will also reduce the halo
phenomenon of the second layer of ﬁltering. The two-scale ﬁlters process is as
follows:

404
Z. Li and B. Li
(a)
(b)
Fig. 3. The texture and edge layers of the Red channel of the visible image. (Color
ﬁgure online)
R1
p = WGuidFτ,ε(R0
p, R0
p),
R2
p = GuidFω,θ(R1
p, R1
p)
(1)
where p represents the R, G, B channels of visible and NIR channel index,
p ∈{r, g, b, n}; R1
p are the ﬁrst base layers calculated by the weight-guided ﬁlter;
R2
p are the second base layers from the guided ﬁlter. The reason for using the
weight-guided ﬁlter in the ﬁrst layer is that it can better distinguish noise and
preserve the gradients of edges. The τ, ε, ω, and σ are the parameters of the
ﬁlter functions.
Di+1
p
= Ri
p −Ri+1
p
(2)
where Di+1
p
means the detail and edge layers of the image, p is the channel
index, represents R, G, and B of visible and NIR channels, and i in this paper
is i = 0, 1. That is to say, D1
p means the detail layer.
3.2
Inter-band Information Complementarity Map Estimation
The relationship between the visible and NIR spectral bands has been discussed
in Sect. 2, particularly in the context of the reﬂectivity of vegetation and other
natural scene features. It can be clearly seen that the original near-infrared is
signiﬁcantly diﬀerent from all three original visible bands.
According to [30], the diﬀerence maps of visible and NIR images can include
information from two modalities. The smaller the diﬀerence, the closer the indi-
cation is. The larger the diﬀerence, the more diﬃcult. And it is also applied to
the extraction of the DOG ﬁlter of diﬀerence maps. Therefore, We use the fea-
ture of diﬀerence maps to estimate the initial visible-NIR complementarity map,
as well as add the signiﬁcant region of near-infrared night-time compensation to
guide the complementary information.
Additionally, the fundamental layer’s potential texture and edge guide map
are obtained using the extend-DoG ((XDog)) [2,27] method, the main goal of

Fusion Algorithm Based on Information Complementarity
405
which is to expand the edge information. The edge line width of the generated
edge graph is usually only 12 pixels, however, the general edges of images should
not be so thin. The color weight map is obtained by using extend-DoG and the
complementary information is obtained from the near-infrared light supplement
area at night. The extend-DoG ﬁlter is applied to estimate the diﬀerence maps
as the initial complementarity weight maps as follows:
RN i
c = Normal(Ri
c −Ri
n),
(3)
dogi
c = XDoG(RN i
c),
dogi
n = XDoG(Ri
n),
(4)
where Normal() means the normalization processing of the base layers of
the visible and NIR images processed by two-scale ﬁlters, i = 0, 1 represents the
texture and edge layer, c is the R, G, and B of visible channels, n is the NIR
channel.
(a)
(b)
(c)
Fig. 4. The arctanI function used to ﬁt the information complementary
3.3
Information Complementary Weight Model
Due to the low illumination data, when the brightness value in the image area
is relatively low, the noise will be higher and the dog value will also be higher;
So only when the brightness value of the image area is relatively high and the
dog value is large, will useful information be obtained. So, we believe that the
XDog value of an image, combined with its pixel value size, reﬂects whether it
is a texture with a high signal-to-noise ratio or information with a low signal-
to-noise ratio. As a preliminary estimate of fusion weights.
Considering that at night, the signal-to-noise ratio of near-infrared images
that rely on supplementary lighting will be relatively high in the supplementary
area, but the signal-to-noise ratio of areas that do not receive supplementary
lighting is still relatively low. So, we use the extend-DoG ﬁlter and the value of
near-infrared as a variable factor for adjustment.
Theorem 1. When the signal-to-noise ratio of near-infrared is high, the weight
value of R, G, and B of visible channels needs to be changed within a lower range
based on the signal-to-noise ratio of visible light.

406
Z. Li and B. Li
Theorem 2. When the signal-to-noise ratio of near-infrared is low, the weight
value of R, G, and B of visible channels needs to be changed within a higher
range based on the signal-to-noise ratio of visible channels.
Inspired by [27], the arctanI function is used for adjustment to ﬁt the infor-
mation complementary between visible and NIR images as follows:
fuwti
c = arctanI(dogi
c, dogi
n)
(5)
where arctanI is the function used to control the complementary based on
the signal-to-noise of NIR, it is deﬁned as follows:
arctanI(x, y) =
atan(x)
atan(
x
1−y) + α
(6)
As shown in the Fig. 4, when the y value of NIR dogi
n is a range of 0.5 approaches
1, the x value of visible bands dogi
c varies between 0 and 0.5; When the y value
of dogi
n is less than 0.5 and close to 0, the x value of visible bands dogi
c changes
between 0.5 and 1.
After the weights fuwti
c of visible channels have been generated, the fused
detail and edge layers can be implemented from each visible channel and NIR
according to their own weights as follows:
fuDi+1
c
= Di+1
c
. ∗fuwti+1
c
+ Di+1
n
. ∗(1 −fuwti+1
c
))
(7)
Finally, combining the fused detail and edge layers and the correspondent
base layer of RGB, the fusion Fc is deﬁned as:
Fc = R2
c + fuD2
c + fuD1
c
(8)
4
Experimental Results
In this paper, by taking advantage of the spectrum properties and the informa-
tion complementarity in fusion processing, the proposed fusion algorithm can not
only improve image quality but also avoid color distortion while maintaining the
natural color look. To verify the advantages of our fusion algorithm, We compare
the proposed method with traditional state-of-the-art fusion methods including
Joint Scale Map Restoration fusion (JSM) [28], Bilateral Filter fusion (BF) [12],
Color Preserve fusion (CP) [1], Laplacian Pyramid fusion (Lap) [25] and Spectral
Edge fusion (SE) [5]. And the group of test images is pairs of visible and NIR
images captured by Hikvision Black Light Camera, which can simultaneously
capture visible and NIR images with two CCDs through the same optical path.
And we collected data on diﬀerent conditions of lighting and noise, indoors and
outdoors respectively.

Fusion Algorithm Based on Information Complementarity
407
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Fig. 5. Qualitative comparison of the outdoor image (a) Visible image. (b) Near-
infrared image. The fusion images are obtained by (c) BF [12] (d) CP [1] (e) JSM
[28] (f) Lap [25] (g) SE [5] (h) our algorithm.
4.1
Objective Comparison
In our experiments, in order to value the authenticity and naturalness color
of fusion images, we employ two metrics. To conﬁrm the spectrum properties
preservation as well as the chromaticity information of the visible image, the no-
reference spectrum distortion index (SDI) is adopted [16], it is used to determine
whether the correlation between visible and near-infrared spectra has changed.
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Fig. 6. Qualitative comparison of the indoor image. (a) Visible image. (b) Near-infrared
image. The fusion images are obtained by (c) BF [12] (d) CP [1] (e) JSM [28] (f) Lap
[25] (g) SE [5] (h) our algorithm.

408
Z. Li and B. Li
Besides that, we also adopt a simple metric of color distance (CD) [4]. This
metric is a combination of both weighted Euclidean distance functions, where
the weight factors depend on how signiﬁcant the “red” component of the color
is. This formula has results that are more stable algorithms, the selection of the
closest color is subjective. And the smaller both values of the two metrics are,
the less the correlations between R, G, and B of visible and near-infrared bands
have changed. And the CD is deﬁned as follows:
r = Cc,r + Cf,r
2
,
(9)
R
′ = Cc,r −Cf,r, G
′ = Cc,g −Cf,g, B
′ = Cc,b −Cf,b,
(10)
CD =

(2 +
r
256) × R
′2 + 4 × G
′2 + (2 + 255 −r
256
) × B
′2
(11)
Meanwhile, to verify that the fused information comes from the original
image, rather than artiﬁcially adding false information, we used SSIM [26] to
measure the structural similarity between the source image and the fusion image.
In addition, the peak signal-to-noise ratio (PSNR) is used to quantify the quality
of the fusion image, as well as the visual information ﬁdelity (VIF) [14] is used
to measure information ﬁdelity by calculating the distortion between the source
image and the fusion image.
Table 1 demonstrates the comparisons of test images with corresponding
algorithms, including 5 metrics. The greater value of the VIF, SSIM, and PSNR,
the better the performance. As expected, the SE and Lap algorithms have high
scores of SDI and CD, respectively. As shown in Fig. 5(g) the color is not real,
as well as the other values of metrics are not well. These algorithms, in general,
aim on maintaining more details, nonetheless, the colors are oversaturated and
unrealistic.
Although the SE and JSM algorithms have lower values of SDI and CD, the
proposed algorithm can have the best PSNR value and the second VIF value.
That is to say, the proposed algorithm can not have a color appearance suitable
for the human eye, but also ensure the clarity of the fusion image, avoiding
high-frequency noise. Meanwhile, the highest value of SSIM of the proposed
algorithm indicates the fused information has the greatest correlation with the
source information.
With a comprehensive examination of indicator data, our algorithm can take
advantage of the correlation and complementarity between visible and NIR spec-
tra, resulting in a natural color look. While the color appearance of the SE and
JSM algorithms is closest to the original viewable image, the edges and dark
areas are not obvious.
4.2
Subject Comparison
As shown in Fig. 5 and Fig. 6, the fusion results with the SE, and the Lap algo-
rithms all have color distortion, such as the clothes of people, and the areas

Fusion Algorithm Based on Information Complementarity
409
Table 1. Qualitative Comparision
Metrics COMPARISION ALGORITHM
JSM [28] SE [5]
Lap [25] BF [12] CP [1] Ours
CD
0.98
5.00
5.72
1.11
0.76
0.73
SDI
0.0021
0.0777 0.0421
0.0321
0.0020 0.0018
VIF
0.41
0.38
0.36
0.39
0.57
0.46
SSIM
0.48
0.38
0.41
0.56
0.63
0.65
PSNR
39.83
37.46
27.07
37.46
37.88
40.93
of trees. Although the details in some dark areas can be seen with their algo-
rithms, the color is unrealistic, such as the ground steps areas in Fig. 5. These
algorithms, in general, emphasize maintaining more details, however, the col-
ors are over-saturated and artiﬁcial, particularly in places with large variations
between visible and NIR spectra. Furthermore, the noise on the black garments
of the guy in the scene has been reduced in our fusion results. This demon-
strates that our algorithm is capable of distinguishing useful information from
meaningless noise input.
More importantly, due to overexposure, the details of the visual acuity chart
in the visible image are invisible, which can be seen in the NIR image, as shown
in Fig. 6. However, in the fusion images, our visual acuity chart features are
the most evident, whereas the others are either indistinct or not visible. This
comparison demonstrates that our algorithm can proﬁt from the information
complementarity between visible and NIR images, as well as distinguish valuable
information from noise. Overall, our technology generates high-quality images
with natural-looking color appearances.
5
Conclusion
In this paper, the fusion model based on the level of physical signals is proposed,
with information complementarity between visible and NIR images. First, to
distinguish between noise and useful texture and edge information, we use two
layers of the weight-guided ﬁlter and guided ﬁlter to obtain texture and edge
layers, respectively. Second, the extend-DoG ﬁlter is applied to estimate the
diﬀerence maps as the Visible and NIR complementarity map. After that, to
obtain the signal complementary weight, the signiﬁcant region of NIR night-
time compensation guides the complementarity map by the arctanI function we
deﬁned. Finally, the fusion images can be generated by the weights of three
bands of visible and NIR images, respectively. Based on the physical optics
imaging theory, the proposed algorithm analyzes the complementarity of physical
information reﬂected in diﬀerent light bands and designs a reasonable fusion
model. Experiment ﬁndings show that our proposed fusion method outperforms
state-of-the-art algorithms in both subjective and objective measures.

410
Z. Li and B. Li
References
1. Awad, M., Elliethy, A., Aly, H.A.: Adaptive near-infrared and visible fusion for fast
image enhancement. IEEE Trans. Comput. Imaging 6, 408–418 (2020). https://
doi.org/10.1109/TCI.2019.2956873
2. Boerner, H.: Feature extraction by grayscale morphological operations-a compari-
son to dog ﬁlters. In: International Workshop on Industrial Applications of Machine
Intelligence and Vision, pp. 112–117 (1989). https://doi.org/10.1109/MIV.1989.
40534
3. Brown, M., S¨usstrunk, S.: Multi-spectral sift for scene category recognition. In:
IEEE Conference on Computer Vision and Pattern Recognition (2011)
4. Chulhee, P., Kang, M.: Color restoration of RGBN multispectral ﬁlter array sensor
images based on spectral decomposition. Sensors 16(5), 719 (2016)
5. Connah, D., Drew, M.S., Finlayson, G.D.: Spectral edge image fusion: theory and
applications. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014.
LNCS, vol. 8693, pp. 65–80. Springer, Cham (2014). https://doi.org/10.1007/978-
3-319-10602-1 5
6. Deng, X., Dragotti, P.L.: Deep convolutional neural network for multi-modal image
restoration and fusion. IEEE (10) (2021)
7. Elliethy, A., Aly, H.A.: Fast near infrared fusion-based adaptive enhancement of
visible images. In: 2017 IEEE Global Conference on Signal and Information Pro-
cessing (GlobalSIP), pp. 156–160 (2017). https://doi.org/10.1109/GlobalSIP.2017.
8308623
8. Fredembach, C., S¨usstrunk, S.: Colouring the near-infrared. In: Proceedings IS
T/SID 16th Color Imaging Conference (2008)
9. Helou, M.E., Sadeghipoor, Z., Susstrunk, S.: Correlation-based deblurring leverag-
ing multispectral chromatic aberration in color and near-infrared joint acquisition.
In: IEEE International Conference on Image Processing, pp. 1402–1406 (2017)
10. Jiang, J., Feng, X., Liu, F., Xu, Y., Huang, H.: Multi-spectral RGB-NIR image clas-
siﬁcation using double-channel CNN. IEEE Access 7, 20607–20613 (2019). https://
doi.org/10.1109/ACCESS.2019.2896128
11. Jin, S., Yu, B., Jing, M., Zhou, Y., Liang, J., Ji, R.: DarkVisionNet: low-light
imaging via RGB-NIR fusion with deep inconsistency prior. CoRR abs/2303.06834
(2023). https://doi.org/10.48550/arXiv.2303.06834
12. Kumar, B.K.S.: Image fusion based on pixel signiﬁcance using cross bilateral ﬁlter.
Signal Image Video Process. 9(5), 1193–1204 (2015)
13. Kwon, H.J., Lee, S.H.: Visible and near-infrared image acquisition and fusion for
night surveillance. Chemosensors 9 (2021)
14. Li, S., Kang, X., Fang, L., Hu, J., Yin, H.: Pixel-level image fusion: a survey of the
state of the art. Inf. Fusion 33, 100–112 (2017)
15. Li, Z., Zheng, J., Zhu, Z., Yao, W., Wu, S.: Weighted guided image ﬁltering. IEEE
Trans. Image Process. 24(1), 120–129 (2015)
16. Li, Z., Hu, H.M., Zhang, W., Pu, S., Li, B.: Spectrum characteristics preserved
visible and near-infrared image fusion algorithm. IEEE Trans. Multimedia 23,
306–319 (2020)
17. Lv, Y., Xiong, W., Zhang, X., Cui, Y.: Fusion-based correlation learning model for
cross-modal remote sensing image retrieval. IEEE Geosci. Remote Sens. Lett. 19,
1–5 (2022). https://doi.org/10.1109/LGRS.2021.3131592

Fusion Algorithm Based on Information Complementarity
411
18. Perconti, P.: Part task investigation of multispectral image fusion using gray scale
and synthetic color night-vision sensor imagery for helicopter pilotage. In: Pro-
ceedings of SPIE - The International Society for Optical Engineering, vol. 3062,
pp. 88–100 (1997)
19. Salamati, N., S¨usstrunk, S.: Material-based object segmentation using near-
infrared information. In: Proceedings 18th Color Imaging Conference (2010)
20. Sappa, A.D., Carvajal, J.A., Aguilera, C.A., Oliveira, M., Romero, D., Vintimilla,
B.X.: Wavelet-based visible and infrared image fusion: a comparative study. Sensors
16(6), 861 (2016)
21. Sharma, V., Hardeberg, J., George, S.: RGB-NIR image enhancement by fusing
bilateral and weighted least squares ﬁlters. J. Imaging Sci. Technol. 61 (2017).
https://doi.org/10.2352/J.ImagingSci.Technol.2017.61.4.040409
22. Shutao, L., Xudong, K., Jianwen, H.: Image fusion with guided ﬁltering. IEEE
Trans. Image Process. 22(7), 2864–2875 (2013)
23. Son, C., Zhang, X.: Near-infrared fusion via color regularization for haze and color
distortion removals. IEEE Trans. Circuits Syst. Video Technol. 28(11), 3111–3126
(2018). https://doi.org/10.1109/TCSVT.2017.2748150
24. Son, C.H., Zhang, X.P.: Near-infrared coloring via a contrast-preserving mapping
model. IEEE Trans. Image Process. 26(11), 5381–5394 (2017)
25. Vanmali, A.V., Gadre, V.M.: Visible and NIR image fusion using weight-map-
guided Laplacian-Gaussian pyramid for improving scene visibility. S¯adhan¯a 42(7),
1063–1082 (2017)
26. Wang, Z., Simoncelli, E., Bovik, A.: Multiscale structural similarity for image qual-
ity assessment. In: The Thrity-Seventh Asilomar Conference on Signals, Systems
Computers, vol. 2, pp. 1398–1402 (2003). https://doi.org/10.1109/ACSSC.2003.
1292216
27. Winnem¨oller, H., Kyprianidis, J.E., Olsen, S.C.: XDoG: an extended diﬀerence-
of-Gaussians compendium including advanced image stylization. Comput. Graph.
36(6), 740–753 (2012). 2011 Joint Symposium on Computational Aesthetics (CAe),
Non-Photorealistic Animation and Rendering (NPAR), and Sketch-Based Inter-
faces and Modeling (SBIM)
28. Yan, Q., et al.: Cross-ﬁeld joint image restoration via scale map. In: 2013 IEEE
International Conference on Computer Vision, pp. 1537–1544, December 2013.
https://doi.org/10.1109/ICCV.2013.194
29. Yang, W., Cai, J., Zheng, J.: Solving the out-of-gamut problem in image composi-
tion. In: 2010 IEEE International Conference on Image Processing, pp. 3977–3980
(2010). https://doi.org/10.1109/ICIP.2010.5650293
30. Zhang, Y.: Understanding image fusion. Photogram. Eng. Remote Sens. 70(6),
657–661 (2004)
31. Zheng, Y.: An overview of night vision colorization techniques using multispectral
images: from color fusion to color mapping. In: International Conference on Audio,
pp. 134–143 (2012)
32. Zheng, J., Jung, C., Yu, S.: Low light image enhancement by multispectral fusion
of RGB and NIR images. In: 2020 IEEE International Conference on Image Pro-
cessing (ICIP) (2020)

Data Mining

End-to-End Optimization
of Quantization-Based Structure Learning
and Interventional Next-Item
Recommendation
Kairui Fu1, Qiaowei Miao1, Shengyu Zhang1(B), Kun Kuang1,4(B),
and Fei Wu1,2,3
1 Institute of Artiﬁcial Intelligence, Zhejiang University, Hangzhou, China
{fukairui.fkr,qiaoweimiao,sy zhang,wufei}@zju.edu.cn
2 Shanghai Institute for Advanced Study of Zhejiang University, Shanghai, China
3 Shanghai AI Laboratory, Shanghai, China
4 Key Laboratory for Corneal Diseases Research of Zhejiang Province, Hangzhou,
China
kunkuang@zju.edu.cn
Abstract. With the development of deep learning, more and more
related techniques are used in recommender system, making it more eﬀec-
tive and reliable. However, due to the various distribution of real-time
data, those deep-learning-based methods can merely learn the correla-
tion between data rather than the actual causal eﬀect, decreasing the
performance of recommenders when a distribution shift occurs. There-
fore, causal structure learning, which has been proposed to search for
causal relationships between variables, is applied in recommender sys-
tems. However, existing methods assume that the recommender system
is a non-interventional environment, making the causal graph learned not
entirely correct. In this paper, we propose to decouple the recommender
module and the causal module to consider the intervention of recom-
mender system when building a causal graph. We utilize vector quan-
tization to learn a cluster-level graph rather than an item-level graph
to guarantee an acceptable training time. With an adjustable number of
clusters, our model can adapt datasets of any size and be trained within a
certain period. We conduct extensive experiments on both real-world and
synthetic OOD datasets to demonstrate that our model is more eﬀective
than other state-of-the-art sequential recommenders.
Keywords: Sequential recommendation · Causal structure learning ·
Vector quantization
1
Introduction
Due to the extraordinary ability of sequential recommendation to model user’s
long and short-term interests, many sequential recommenders [10,13,17,29,32,
33] have been proposed to dynamically and accurately capture user preferences,
achieving remarkable success in many scenes.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 415–429, 2024.
https://doi.org/10.1007/978-981-99-8850-1_34

416
K. Fu et al.
Despite their astonishing performance, they all assume that the datasets are
independent and identically distributed, while the actual scenarios are pretty
diﬀerent, reducing the eﬀectiveness of these methods. We ﬁgure out that this is
because the relation learned is just correlation, not causality. [6] As exampled in
Fig. 1(d), users in the training set are mostly programmers who always purchase
oﬃce supplies, plaid shirts, and digital gadgets. In contrast, users in the validation
set contain those addicted to games. For these programmers who bought a printer
before, he would buy a ink cartridge for the printer, and this is the expected
causal relation. However, some sequential recommenders may tend to attribute
the purchase of ink cartridges to the purchase of the mouse, then they will learn
a spurious relation. Thus they will recommend the ink cartridge due to the
mouse in history to those internet addiction teenagers, leading to a fallacious
recommendation.
Fig. 1. (a) real causal relations in recommender systems. (b) causal structure learning
under the intervention of recommender system. (c) confounder relations during causal
structure learning. (d) spurious relationship in out-of-distribution environments.
Towards this end, we propose to learn the underlying causal graph, which
reveals the data generation mechanism [2] and will always remain invariant in
diﬀerent environments [24]. However, applying causal structure learning in rec-
ommender system presents three main challenges: i) As illustrated in Fig. 1(b),
user’s mind would be changed due to the item list provided by recommender,
prompting him to click the diaper rather than the ink he wishes to purchase
initially. That is, the recommender intervention will change the user’s mind and
lead to a wrong causal graph where we try to model causal relations between
origin interactions and diaper. ii) As shown in Fig. 1(c), the reason for this phe-
nomenon is that the historical item beer and the target item diaper are both

Quantization-Based Structure Learning in Recommendation
417
associated with a confounder C. Those newly married dads with child would buy
beer and diaper simultaneously, thus constituting a confounder, making beer and
diaper form a confounder relation, then changing user’s mind. Our model must
capture those relations properly. iii) We need to maintain massive causal func-
tions whose quantity is equal to the number of items, denoted as n, whose time
complexity is O(n), and it will generate huge overhead for some e-commerce rec-
ommender systems with more than ten billion items. Apart from this, due to the
long-tail item distribution in recommender, most items only appear a few times
during user interaction sequences, making it a big problem to train satisfactory
causal functions.
In our paper, we propose an end-to-end optimization of quantization-based
structure learning and interventional next-item recommendation to address the
abovementioned problems. For the ﬁrst two challenges, we decouple the causal
module and the sequential recommender to use the causal module to score if
the user is not aﬀected by the recommender else the recommendation module,
thereby eliminating the eﬀect of the intervention of recommenders and bene-
ﬁting from the powerful sequential modeling capabilities of recommenders to
learn the confounder relations when the causal relations are weak as exampled
in Fig. 1(c). Another core idea for the third challenge in our paper is taking
advantage of vector quantization [18] to build a cluster-level graph rather than
the origin item-level graph, from which our model can: i)
get a more bene-
ﬁcial alternative than the metadata in the dataset for training. ii) make the
number of causal functions maintained adjustable with an adjustable number of
meta-items, thereby decreasing time complexity from O(n) above to O(k) where
k ≪n. iii) get enough data to train because each cluster corresponds to multiple
items.
Overall, the main contributions of our paper are summarized below:
– We analyze the inconsistent distribution of users in recommender system and
the diﬃculty in causal structure learning accompanied by the intervention of
recommender system.
– We propose an eﬀective method for end-to-end optimization of quantization-
based structure learning and interventional next-item recommendation,
improving the performance of the recommender.
– We conduct extensive experiments on three real-world datasets and their
corresponding synthetic OOD datasets. The comparison with other models
validates the eﬀectiveness of our model.
2
Related Works
Sequential
Recommendation. Owing to the eﬀectiveness of capturing
dynamic user preference through history interaction, various sequential recom-
mendation methods have been proposed in industrial and research ﬁelds. Tradi-
tional methods include sequential pattern-based [9,20] and Markov chain-based
models [7]. To learn complicated relations and long-term interests, deep learn-
ing methods are widely used to capture users’ long and short-term interests.

418
K. Fu et al.
RNN-based methods [10] leverages gate recurrent unit to capture the long-term
dependencies in user interactions. Since each item in the historical sequence inﬂu-
ences the target diﬀerently, some models [13,29,31] start exploiting the attention
mechanism [19] in recommenders. CNN-based methods, like Caser and PAUP,
have also been investigated to consider more sequential patterns [17,33]. Mean-
while, causal inference has also been used to improve the interpretability and
reduce diﬀerent kinds of bias [21–23,25–28].
Causal Structure Learning. Causal structure learning plays a vital role in
understanding the underlying data generating process while learning from obser-
vational data is not easy. Existing methods mainly consist of two parts: score-
based and constraint-based methods. The constraint-based method [12] will ﬁrst
utilize the independence test, learn the undirected graph of the causal relation-
ship between variables, and then determine the direction of each causal edge
based on the collision structure. But this method is hard to learn the directions
of all edges in the causal graph, and only a set of Markov equivalence classes
can be obtained in the end. The score-based method assigns a score to each edge
of the causal graph, maximizing the obtained score while ensuring the directed
acyclic property of the graph. Typical models include Notears [30] and graph-
based approach [16], both solving this problem by converting it to a constraint
continuous-optimization problem.
3
Methodology
Fig. 2. The structure diagram of our method. (a): The overall architecture of the
model. (b): Quantization module in the model. (c): Implementation process of the
item encoder. (d): Implementation of the causal module. (e): Implementation process
of user encoder

Quantization-Based Structure Learning in Recommendation
419
In this section, we will ﬁrst give a brief problem formulation of sequential
recommendation and then introduce our proposed method in detail. The model
we propose is shown in Fig. 2:
3.1
Problem Formulation
In the setting of sequential recommendation, suppose we have an item set I =
{i1, i2, ..., in} and a user set U = {u1, u2, ..., um}. Our task is to predict the next
clicked item xt ∈I correctly for each user ui ∈U with interaction sequence
(x1, x2, ..., xt−1) where xi ∈I for any i ∈[1, t −1]. Formally, the parameter θ in
the model f is optimized by minimizing the following function:
L(θ) =
i=m

i=1
j=t−1

j=1
f(xt|x1, x2, ..., xt−1; θ)
(1)
3.2
Causal Structure Learning in Recommendation
Due to decoupling the two modules, we should carefully select the causal or rec-
ommender module as the scoring source for each item. Because the intervention
in recommender system is quite diﬀerent from other ﬁelds like medical diagnosis,
where the outcome of the intervention can be inferred from the patient’s con-
dition, we don’t know whether the user is intervened successfully or not. The
user will anyway click an item, and we have no idea about the source of the
clicked item. In causal structure learning, a node given its parent nodes does not
change and will always remain unchanged in the causal mechanism. Under this
intuition, we choose to use causal mechanism if there is any parent node of the
target node in the history interactions. We get a relatively good result from this
approach compared to simply constant weighting shown in Sect. 4.3.
For each item x in the item set I, there is exactly one cluster c corresponding
to it. Then the user interaction can be rewritten as (cu
1, cu
2, ..., cu
t−1), cu
i is the
corresponding clusters of xi for each i ∈[1, t −1]. When predicting the score of
cluster c′, we ﬁrst ﬁlter out those less associated clusters in the sequence, that
is, (C ⊙A′), where C is a multi-hot vector of (cu
1, cu
2, ..., cu
t−1), cu
i with length
Nc. For each cluster, we train a causal function using a multi-layer perceptron,
which accepts (C ⊙A′) as input and outputs the predicted score of the cluster
given its parents in the causal graph. Thus the ranking loss can be described as
follows:
Lranking =
i=m

i=1
j=t−1

j=1
[p(xt|x1, x2, ..., xt−1)R · fxt(−→
C ⊙Act
u)1−R]
(2)
where R describes whether there exists any parent node in history and is deter-
mined by the sample edges and the cluster that the target item belongs to.

420
K. Fu et al.
3.3
Quantization-Based Structure Learning
In order to get a quantity adjustable and better input, we propose to learn the
corresponding cluster for each item in the training phase and use the learned clus-
ters as input for the causal module. We are inspired by VQ-VAE [18], which takes
advantage of quantization to build an end-to-end discrete latent VAE model. By
adding an item-quantization part, that is, for each item embedding ie, ﬁnd the
nearest discrete latent representation ci of it, we successfully get the meta-item
and use it as the input of the causal part. During training, the meta-item of
each item will gradually reach the optimum, and the causal function can then
be optimized. Considering that the user embedding u may beneﬁt from quantiza-
tion, we also perform quantization for each user embedding iu. The quantization
process can be described as follows:
p(i = k|e) =

1
i = argmini dist(ci, e)
0
otherwise
(3)
After applying quantization, we get latent representations ci and cu for each
item and user representation, respectively. Then we concatenate all the three
embeddings above and get the ﬁnal representation eui for the feed-forward net-
work.
3.4
End-to-End Optimization
Optimization of the Quantization Part. Although we have adopted vector
quantization to do clustering ourselves, there are still deﬁciencies that gradi-
ents from eui can only backpropagate to the codebook while the item and user
embeddings receive nothing useful, not conducive to parameter updates in the
model. We ﬁx this problem with the straight-through estimator used in the orig-
inal paper, namely, adjust the concatenation embeddings ci and cu to propagate
the gradient to the item embedding. Take the item embedding ie as an example,
we replace ci with c′
i:
c′
i = sg[ci −ie] + ie
(4)
where sg stands for the stop-gradient method. Because the item embedding ie
is close to the latent representation ci in the embedding space, we can simul-
taneously optimize the codebook by minimizing the distance between the item
embedding and the latent representations. Thus, we get an extra objective loss:
Lquantization = ∥sg[c′
i] −ci∥2 + β · ∥sg[ci] −c′
i∥2
(5)
the second part is used to prevent items from ﬂuctuating between each cluster,
providing a stable input for the causal module.
Optimization of the Causal Module. Because our method decouples causal
structure learning and sequential recommender, getting the causal graph through

Quantization-Based Structure Learning in Recommendation
421
a single activation function is impractical. In that case, some causal functions
won’t be updated because the parameters in the graph are so small, and the
recommender module results will be used to rate all items.
Therefore, we propose to sample each value in the causal graph G to overcome
this problem and give each causal function a chance to update during training.
For each edge Aij, we sample it with probability pij:
Aij =

1
Bernoulli(pij) = 1
0
otherwise
(6)
Nevertheless, we can notice the sample operation is non-diﬀerential, and the
graph can never be updated. To address this challenge, we utilize the Gumbel-
Softmax trick and Straight-Through estimation method used in GDAS [4]:
Aij = I(σ(s + pij) ≥0.5) + σ(s + pij) −sg[σ(s + pij)]
(7)
where I is indicator function and σ is the sigmoid function. s is an independent
sample from the standard Logistic distribution.
As for the directed acyclic graph constraints, we adopt the augmented
Lagrangian in Notears [30] with a dual ascent method:
LDAG = ρ
2|h(G)|2 + αh(G)
(8)
Until now, we have ﬁxed all the optimization problems in our model. Then
the log-likelihood loss function can be written as:
L = Lranking + Lquantization + LDAG
=
i=m

i=1
j=t−1

j=1
[p(xt|x1, x2, ..., xt−1)R · fxt(C ⊙A′)1−R]+
∥sg[c′
i] −ci∥2 + β · ∥sg[ci] −c′
i∥2 +
λ ∥G∥2 + ρ
2|h(G)|2 + αh(G)
(9)
4
Experiments
To demonstrate the eﬀectiveness of our method, we compare it with some sequen-
tial recommenders on three real-world datasets and corresponding synthetic
OOD datasets. All results are the average of ﬁve experiments.
4.1
Experimental Setup
Datasets. We conduct experiments on three state-of-art benchmarks Movie-
lens1M1, Netﬂix2 and Amazon-Music3 and construct corresponding OOD
datasets with the paradigm of DESMIL [15].
1 https://grouplens.org/datasets/movielens/1m/.
2 https://www.kaggle.com/code/laowingkin/netﬂix-movie-recommendation.
3 https://nijianmo.github.io/amazon/index.html.

422
K. Fu et al.
Baselines. For the implementation of our method, we choose YoutubeDNN
[3] as the base model. We select YoutubeDNN [3], Caser [17], AttRec [29],
PAUP [33], GRU4Rec [10], NeuMF [8], SASRec [13] and FMLP-Rec [32]
as baselines.
Evaluation Metrics. The three evaluation metrics we used are the same as the
framework in ComiRec [1]: Recall, Hit Rate, and NDCG. The higher the scores
of the three indicators, the better the eﬀect of the recommenders.
More details about the settings can be found in Appendix A.
4.2
Experiments Results
The results are displayed in Table 1, from which we can conclude that our model
outperforms other models on almost all metrics. The metric Recall on Amazon
is slightly worse than Caser. However, we ﬁnd that our model is way ahead of
Caser and other models on NDCG, a more convincing metric that still proves the
eﬀectiveness of our model. Regardless of the dataset, our model constantly sig-
niﬁcantly improved over the base model YoutubeDNN. Not surprisingly, among
the other baselines, these models with self-attention mechanisms perform better
in all three datasets compared to those without, which in line with that attention
mechanism generalizes well in most deep learning tasks [11,19].
Table 1. Comparative performance between our model and the other baselines. We
use bold font to denote the best-performing model and underline the next-best model.
Datasets
Movielens
Netﬂix
Amazon
Metric
Recall
NDCG
Hit
Recall
NDCG
Hit
Recall
NDCG
Hit
YoutubeDNN 0.0878
0.1564
0.6510
0.2848
0.2241
0.5649
0.4100
0.4512
0.8536
GRU4Rec
0.0912
0.1440
0.6508
0.2895
0.1736
0.5827
0.2618
0.2246
0.5772
NeuMF
0.07927 0.1446
0.6268
0.2362
0.1767
0.4677
0.2208
0.2360
0.4684
SASRec
0.0909
0.1521
0.6661
0.3028
0.2287
0.6012
0.3494
0.3395
0.7382
Caser
0.0995
0.1474
0.6987
0.4818
0.3296
0.9550
0.4276 0.3751
0.9219
AttRec
0.0952
0.1169
0.6225
0.4460
0.3021
0.8843
0.4110
0.3502
0.8839
PAUP
0.0934
0.1644
0.6919
0.2804
0.2132
0.5591
0.3086
0.2603
0.6422
FMLP-Rec
0.0904
0.1533
0.6715
0.2698
0.2043
0.5378
0.2930
0.2332
0.6065
ours
0.1051 0.1773 0.7098 0.4921 0.5713
0.9801 0.4239
0.5212 0.9556
Improv
19.76%
13.40%
9.03%
72.78%
154.93% 73.51%
3.40%
15.50%
1.95%
4.3
Parameter Analysis
Inﬂuence of the Cluster Num. To further demonstrate the validity of the
causal module in our model, we adjust the cluster num Nc used in our model.
Logically, suppose we reduce the value to less than a certain degree. In that
case, the performance between ours and the base model YoutubeDNN will be

Quantization-Based Structure Learning in Recommendation
423
approximately 0 because, in that case the clustering information of the model
will be rough, making the causal module invalid. We study its inﬂuence through
tuning Nc in the range of [1, 2, 4, 8, 16, 32, 64, 128] and present the result
in Fig. 3. We can conclude that our model performs almost identically to the
base model when Nc equals 1. And the performance keeps increasing when we
increase the value until a certain threshold is reached.
Fig. 3. Inﬂuence of the hyperparameter Nc on the model performance.
Inﬂuence of the Dynamic Selection. In this section, we replace this dynamic
allocation with constant weighting to validate the necessity of our selective
method mentioned in Sect. 3.2. Suppose we assign wRS for recommendation mod-
ule and wcausal for causal module, where wRS +wcausal = 1. We tune the wRS in
the range of [0.25, 0.5, 0.75] and conduct experiments on Netﬂix and Amazon. We
present the results in Fig. 4. For both datasets, our adaptive method can achieve
the best performance than those with constant weighting. This agrees with our
above analysis. By making choices adaptively, our model can better make better
use of those strong causal relations and utilize the powerful sequential prediction
capability when the causal relation is small.
Fig. 4. performance comparison between our adaptive allocation strategy model and
those with constant weighting.left panel: metrics comparison on Amazon dataset. right
panel: metrics comparison on Netﬂix dataset.

424
K. Fu et al.
4.4
Ablation Study
Since some datasets have their own category information, it is critical to conduct
an ablation study to demonstrate the eﬀectiveness of learned cluster information
and the necessity of combining causal structure learning and recommender. The
ablated models are as follows: i) w.o. (without) causal removes the causal
module from our model, degenerating into the base model YoutubeDNN. ii)
w.o. RS removes recommender from our model and will give the same logits to
those items belonging to the same categories. iii) w.o. quantization removes
vector quantization and uses the origin metadata as input of the causal module.
Relevant results are shown in Table 2, from which we can draw the following
conclusions: i)
Combining causal structure learning and recommendation is
necessary for making the recommender works better, our model has relatively
large improvements over models that only use a single module. ii) Simply using
those semantic prior, namely, the original category information as input won’t
always improve our model. It can be inferred from the comparison between the
base model and w.o. quantization, which makes use of the knowledge in the
original dataset but gets worse results.
Table 2. Ablation studies on Movielens and Amazon.
Datasets
Movielens
Amazon
Metric
Recall
NDCG Hit
Recall
NDCG Hit
w.o. causal
0.0878 0.1563
0.6510 0.4100 0.4512
0.8536
w.o. RS
0.0112 0.0232
0.1967 0.0100 0.0068
0.0267
w.o. quantization 0.0837 0.1289
0.6444 0.4096 0.4498
0.8536
ours
0.1051 0.1773
0.709
0.4239 0.5212
0.9556
4.5
OOD Generalization
In the above sections, we have demonstrated our model’s eﬀectiveness and under-
stood the hyperparameter’s inﬂuence by adjusting them through ablation study
and comparative experiments. However, these experiments are carried out in
the IID environment, which does not always hold in real scenarios. Hence we
further investigate the OOD dataset to verify our model’s stability under out-
of-distribution environments. We use the same baselines and OOD datasets
described in Sect. 4.1, and the results are shown in Table 3, from which we can
observe that some methods, such as Caser and AttRec drop a lot in OOD envi-
ronment while our model still gets the best results, proving the stability of our
method.

Quantization-Based Structure Learning in Recommendation
425
Table 3. Comparative performance on OOD datasets between our model and the other
baselines. We use the same symbols as in Table 1
Datasets
Movielens
Netﬂix
Metric
Recall
NDCG
Hit
Recall
NDCG
Hit
YoutubeDNN 0.0931
0.1582
0.6634
0.3056
0.2354
0.5515
GRU4Rec
0.0814
0.1347
0.6114
0.1830
0.1053
0.3625
NeuMF
0.07911 0.1392
0.6079
0.2363
0.1737
0.4485
SASRec
0.0995
0.1552
0.6740
0.3375
0.2430
0.5814
Caser
0.0522
0.0932
0.490
0.1761
0.1185
0.3481
AttRec
0.0311
0.0584
0.3763
0.1038
0.0588
0.2038
PAUP
0.0979
0.1662
0.6924
0.2814
0.2131
0.5485
FMLP-Rec
0.0963
0.1594
0.6879
0.2672
0.1996
0.5221
ours
0.1115 0.1824 0.7190 0.4470 0.4871
0.8224
Improv
19.76%
15.30%
8.38%
46.27%
106.92% 49.12%
5
Conclusion
In our paper, we propose an end-to-end optimization of quantization-based
structure learning and interventional next-item recommendation, which not only
reconstructs the underlying causal graph in an interventional environment caused
by RS but also adaptively allocate logits for each target item according to its
parent node in the causal graph. Moreover, with an adjustable hyperparameter
Nc, the time consumption can be guaranteed regardless of the size of the dataset.
Extensive experiments conducted on three benchmark datasets and correspond-
ing synthetic OOD datasets show that our method outperforms other sequential
models by a large margin in all three metrics, validating the eﬀectiveness and
eﬃciency of our model. Additional parameter analysis and ablation study also
prove the rationality of our method.
While an item exactly corresponds to a cluster, it can be extend to a more
realistic scene where an item corresponds to many clusters with diﬀerent weights.
The distance between items and meta-items in the embedding space can be used
as the weight criterion. We leave this as a future work to better improve the
eﬀect of the model.
Acknowledgement. This work was supported in part by National Natural Sci-
ence Foundation of China (62006207, U20A20387), Young Elite Scientists Sponsorship
Program by CAST (2021QNRC001), Zhejiang Province Natural Science Foundation
(LQ21F020020), and the Fundamental Research Funds for the Central Universities
(226-2022-00142, 226-2022-00051).

426
K. Fu et al.
A
Appendix
A.1
Experimental Setup
Datasets. We conduct experiments based on the following benchmark datasets,
of which the statistics are shown in Table 4:
– Movielens-1M4 contains anonymous ratings of movies made by MovieLens
users who joined MovieLens in 2000. Besides, it also includes the metadata
of each movie, such as the categories it belongs to.
– Amazon-Music5 is an e-commerce dataset collected from Amazon.com,
which also includes the category information of a product.
– Netﬂix6 is another movie rating ﬁle constructed to support participants in
the Netﬂix Prize. In this experiment, We use the preprocessed dataset pro-
vided in NATR [5].
In order to keep dataset quality, We discard items with fewer than 20 related
appearances, while for user ﬁltering, we adopt 5-core settings for Amazon and
Netﬂix and 20-core settings for Movielens due to their diﬀerent sparsity.
For the three datasets above, we sort the user-item interactions in chronolog-
ical order and divide the training and testing sets according to the ratio of 8:2.
In our experiments, the validation set is consistent with the test set. For each
interaction, We regard those feedback with positive ratings as positive.
OOD Datasets. Following the data splitting paradigm of DESMIL [15], we
build the OOD datasets by adjusting the distribution of user groups in the
training set and test set. We randomly select a user and calculate the Jaccard
similarity7 between other users and the selected user. Then we choose 80% users
with the smallest similarity as the training set and use the whole dataset for
testing, where the training set and testing set contain diﬀerent user groups.
Table 4. Statistics of the datasets.
Dataset
#Users #Items #Interactions #SeqLen #Sparsity
Movielens 6,040
3,012
994,852
164.71
94.52%
Netﬂix
14,630
5,569
166,836
11.40
99.79%
Amazon
9,923
8,902
96,505
11.58
99.89%
Baselines. For the implementation of our method, we choose YoutubeDNN [3]
as the base model, which is a classic deep learning recommendation system.
Since our model is a sequential recommendation model, we ignore the graph
4 https://grouplens.org/datasets/movielens/1m/.
5 https://nijianmo.github.io/amazon/index.html.
6 https://www.kaggle.com/code/laowingkin/netﬂix-movie-recommendation.
7 https://www.learndatasci.com/glossary/jaccard-similarity/.

Quantization-Based Structure Learning in Recommendation
427
recommendation model and focus on the sequential recommendation model. As
for those collaborative ﬁltering (CF) recommendation models, we adjust them
to apply the historical sequence to the model. The compared models are shown
as the following:
– YoutubeDNN [3]. As one of the industry’s most commonly used recom-
mender systems, it represents each user with his historical interaction data
and combines them to get the ﬁnal predictive score.
– GRU4Rec [10]. GRU4Rec is a representative recommendation model in ses-
sion based recommendation which adopts recurrent neural networks to train
the relationship between target items and item sequences.
– NeuMF [8]. NeuMF combines traditional matrix decomposition and multi-
layer perceptron to extract both low- and high-dimensional features simulta-
neously.
– SASRec [13]. SASRec applies self-attention mechanisms in sequential rec-
ommendation to help learn more valuable information.
– Caser [17]. Caser is a state-of-the-art model that utilizes vertical and hor-
izontal convolutional networks to extract short-term preferences from user
interaction sequences.
– AttRec [29]. Similar to SASRec, AttRec makes use of self-attention mech-
anisms to capture long-term preference. Moreover, it uses metric learning to
model the consequences of short-term interests versus long-term preferences.
– PAUP [33]. PAUP proposes a down-sampling convolution module and an
unsymmetrical positional encoding strategy to eﬀectively and eﬃciently cap-
ture both short- and long-term patterns.
– FMLP-Rec [32]. FMLP-Rec makes use of ﬁltering algorithms in signal pro-
cessing instead of the traditional transformer to relieve the noise in the orig-
inal dataset.
Evaluation Metrics. The three evaluation metrics we used are the same as the
framework in ComiRec [1]. In recommendation system, accuracy, that is, whether
what the user wants to see is correctly recommended to the user, is crucial. Thus
we use Recall and Hit Rate to evaluate the accuracy of a model. Another metric
used in our experiment is NDCG(Normalized Discounted Cumulative Gain),
which pays more attention to the ratings in the recommendation list of items
desired by users and is more convincing than recall. The higher the scores of the
three indicators, the better the eﬀect of the recommendation model.
Implementation Details. For all the models mentioned above, we use Adam
[14] as the optimizer and 0.001 as the learning rate. All of them are implemented
with an item embedding with size d = 64. The number of horizontal and vertical
convolution ﬁlters are 4 and 16 in Caser, respectively. For each sequence in
Caser, AttRec, and SASRec, we randomly sample 10 items not interacted with
by the user as negative samples. While for GRU4Rec, we follow the method
used in the original paper, namely, using other items in the same batch except
for the target item as negative samples. For the NeuMF baseline, we create an
embedding for each user with the history of interactions over time to adapt
sequential recommendations.

428
K. Fu et al.
References
1. Cen, Y., Zhang, J., Zou, X., Zhou, C., Yang, H., Tang, J.: Controllable multi-
interest framework for recommendation. In: Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pp. 2942–2951
(2020)
2. Chen, W., Wu, Y., Cai, R., Chen, Y., Hao, Z.: CCSL: a causal structure learning
method from multiple unknown environments. arXiv preprint arXiv:2111.09666
(2021)
3. Covington, P., Adams, J., Sargin, E.: Deep neural networks for youtube recommen-
dations. In: Proceedings of the 10th ACM Conference on Recommender Systems,
pp. 191–198 (2016)
4. Dong, X., Yang, Y.: Searching for a robust neural architecture in four GPU hours.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 1761–1770 (2019)
5. Gao, C., et al.: Cross-domain recommendation with bridge-item embeddings. ACM
Trans. Knowl. Discov. Data (TKDD) 16(1), 1–23 (2021)
6. Gao, C., Zheng, Y., Wang, W., Feng, F., He, X., Li, Y.: Causal inference in recom-
mender systems: a survey and future directions. arXiv preprint arXiv:2208.12397
(2022)
7. He, R., Kang, W.C., McAuley, J.J., et al.: Translation-based recommendation: a
scalable method for modeling sequential behavior. In: IJCAI, pp. 5264–5268 (2018)
8. He, X., Liao, L., Zhang, H., Nie, L., Hu, X., Chua, T.S.: Neural collaborative
ﬁltering. In: Proceedings of the 26th International Conference on World Wide
Web, pp. 173–182 (2017)
9. Herawan, T., Noraziah, A., Abdullah, Z., Deris, M.M., Abawajy, J.H.: IPMA: indi-
rect patterns mining algorithm. In: Advanced Methods for Computational Collec-
tive Intelligence, vol. 285, pp. 159–166 (2013)
10. Hidasi, B., Karatzoglou, A., Baltrunas, L., Tikk, D.: Session-based recommenda-
tions with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015)
11. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 7132–7141
(2018)
12. Kalisch, M., B¨uhlman, P.: Estimating high-dimensional directed acyclic graphs
with the PC-algorithm. J. Mach. Learn. Res. 8(3) (2007)
13. Kang, W.C., McAuley, J.: Self-attentive sequential recommendation. In: 2018 IEEE
International Conference on Data Mining (ICDM), pp. 197–206. IEEE (2018)
14. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
15. Liu, Q., Liu, Z., Zhu, Z., Wu, S., Wang, L.: Deep stable multi-interest learning for
out-of-distribution sequential recommendation. arXiv preprint arXiv:2304.05615
(2023)
16. Ng, I., Zhu, S., Chen, Z., Fang, Z.: A graph autoencoder approach to causal struc-
ture learning. arXiv preprint arXiv:1911.07420 (2019)
17. Tang, J., Wang, K.: Personalized top-n sequential recommendation via convolu-
tional sequence embedding. In: Proceedings of the Eleventh ACM International
Conference on Web Search and Data Mining, pp. 565–573 (2018)
18. Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning. In:
Advances in Neural Information Processing Systems, vol. 30 (2017)

Quantization-Based Structure Learning in Recommendation
429
19. Vaswani, A., et al.: Attention is all you need. In: Advances in Neural Information
Processing Systems, vol. 30 (2017)
20. Wang, S., Cao, L.: Inferring implicit rules by learning explicit and hidden item
dependency. IEEE Trans. Syst. Man Cybern. Syst. 50(3), 935–946 (2017)
21. Wang, X., Zhang, R., Sun, Y., Qi, J.: Doubly robust joint learning for recommen-
dation on data missing not at random. In: International Conference on Machine
Learning, pp. 6638–6647. PMLR (2019)
22. Wang, Z., Chen, X., Dong, Z., Dai, Q., Wen, J.R.: Sequential recommendation
with causal behavior discovery. arXiv preprint arXiv:2204.00216 (2022)
23. Xu, S., et al.: Causal structure learning with recommendation system. arXiv
preprint arXiv:2210.10256 (2022)
24. Zhang, K., Glymour, M.R.: Unmixing for causal inference: thoughts on mccaﬀrey
and danks. Br. J. Philos. Sci. (2020)
25. Zhang, S., et al.: Personalized latent structure learning for recommendation. IEEE
Trans. Pattern Anal. Mach. Intell. (2023)
26. Zhang, S., et al.: Video-audio domain generalization via confounder disentangle-
ment. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 37,
pp. 15322–15330 (2023)
27. Zhang, S., et al.: Devlbert: learning deconfounded visio-linguistic representations.
In: Proceedings of the 28th ACM International Conference on Multimedia, pp.
4373–4382 (2020)
28. Zhang, S., Yao, D., Zhao, Z., Chua, T.S., Wu, F.: Causerec: counterfactual user
sequence synthesis for sequential recommendation. In: Proceedings of the 44th
International ACM SIGIR Conference on Research and Development in Informa-
tion Retrieval, pp. 367–377 (2021)
29. Zhang, S., Tay, Y., Yao, L., Sun, A.: Next item recommendation with self-attention.
arXiv preprint arXiv:1808.06414 (2018)
30. Zheng, X., Aragam, B., Ravikumar, P.K., Xing, E.P.: DAGs with no tears: con-
tinuous optimization for structure learning. In: Advances in Neural Information
Processing Systems, vol. 31 (2018)
31. Zhou, G., et al.: Deep interest network for click-through rate prediction. In: Pro-
ceedings of the 24th ACM SIGKDD International Conference on Knowledge Dis-
covery & Data Mining, pp. 1059–1068 (2018)
32. Zhou, K., Yu, H., Zhao, W.X., Wen, J.R.: Filter-enhanced MLP is all you need for
sequential recommendation. In: Proceedings of the ACM Web Conference 2022,
pp. 2388–2399 (2022)
33. Zhu, Y., Huang, B., Jiang, S., Yang, M., Yang, Y., Zhong, W.: Progressive self-
attention network with unsymmetrical positional encoding for sequential recom-
mendation. In: Proceedings of the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval, pp. 2029–2033 (2022)

Multi-trends Enhanced Dynamic
Micro-video Recommendation
Yujie Lu1, Yingxuan Huang2, Shengyu Zhang3(B), Wei Han4, Hui Chen4,
Wenyan Fan3, Jiangliang Lai5, Zhou Zhao3(B), and Fei Wu3
1 University of California, Santa Barbara, USA
2 The University of Hong Kong, Pokfulam, Hong Kong
3 Zhejiang University, Hangzhou, China
zhaozhou@zju.edu.cn
4 Singapore University of Technology and Design, Singapore, Singapore
5 The Information Center of the Supreme People’s Court of the People’s Republic
of China, Beijing, China
Abstract. The explosively generated micro-videos on content sharing
platforms call for recommender systems to permit personalized micro-
video discovery with ease. Recent advances in micro-video recommenda-
tion have achieved remarkable performance in mining users’ current pref-
erence based on historical behaviors. However, most of them neglect the
dynamic and time-evolving nature of users’ preference, and the prediction
on future micro-videos with historically mined preference may deteriorate
the eﬀectiveness of recommender systems. In this paper, we devise the
DMR framework, which comprises: 1) the implicit user network mod-
ule which identiﬁes sequence fragments from other users with similar
interests and extracts the sequence fragments that are chronologically
behind the identiﬁed fragments; 2) the multi-trend routing module which
assigns each extracted sequence fragment into a trend group and update
the corresponding trend vector; 3) the history-future trend prediction
module jointly uses the history preference vectors and future trend vec-
tors to yield the ﬁnal click-through-rate. We validate the eﬀectiveness of
DMR over multiple state-of-the-art micro-video recommenders on two
publicly available real-world datasets. Relatively extensive analysis fur-
ther demonstrate the superiority of modeling dynamic multi-trend for
micro-video recommendation.
Keywords: Micro-video Recommendation · Multi-trend Routing ·
Personalization
1
Introduction
In recent years, the amount of searchable micro-videos has increased dramat-
ically and exacerbated the need for recommender systems that can eﬀectively
mine users’ preference and identify potentially interested micro-videos in a per-
sonalized manner. Due to the powerful representation learning capacity, the rapid
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 430–441, 2024.
https://doi.org/10.1007/978-981-99-8850-1_35

Multi-trends Enhanced Dynamic Micro-video Recommendation
431
development of deep learning techniques has nourished the research ﬁeld of rec-
ommendation [11,16]. Such a development also gives rise to diverse models for
video recommendation, which can be roughly categorized to collaborative ﬁlter-
ing [2,13], content-based ﬁltering [8,10,18,19,23], and hybrid ones [5,6,22].
Recent years have witnessed much progress to confront the above challenges
in this vein. THACIL [7] employs temporal block splitting and hierarchical multi-
head attention to model diverse interests across blocks. ALPINE [15] models
users’ dynamic interests by constructing temporal behavior graph and devising
the temporal graph-based LSTM. MTIN [14] considers personalized importance
decay over time and diverse interests using item-level temporal mask and group
routing mechanism, individually. In spite of the great advances of these works,
we argue that solely modeling the historical behaviors deteriorates the capacity
of user modeling capturing diverse and dynamic users’ interests. For example,
MTIN [14] assigns historically interacted items to one of six interest groups
and accordingly updates the six interest vectors. Since users’ interests are by
nature dynamic, the interests learned from the logged data might be out-of-date
or at least limited to the history, falling short to recommend fresh items and
hurting the recommendation diversity. Therefore, capturing dynamic interest
trends based on (but not limited to) historical items can be an indispensable
function for high-quality recommender systems.
To this end, DMR framework makes predictions based on both the history
interests implied by the historical behaviors as well as multi-trends implied in
similar users, which helps to capture even more diverse and dynamic interests
compared with existing micro-video recommenders. We validate the eﬀectiveness
of DMR on micro-video recommendation benchmarks. The substantial improve-
ment over state-of-the-art comparison methods and in-depth model analysis
demonstrate the superiority of modeling multi-trend for micro-video recommen-
dation.
2
Our Approach
In this section, we ﬁrst formulate the micro-video recommendation problem,
and then introduce the proposed framework in detail. As illustrated in Fig. 1,
our proposed DMR framework for dynamic micro-video recommendation mainly
comprises of three modules: 1) Pearson Correlation Coeﬃcient enhanced implicit
user network module; 2) A history-future multi-trend joint routing module; 3)
A multi-level time-aware attention module.
2.1
Problem Formulation
In a typical micro-video recommendation scenario, we have a set of users and
a set of micro-videos, which can be denoted as U = {u1, u2, u3, ..., u|U|} and
V = {v1, v2, v3, ..., v|V |} respectively. Let Iu = {xu
1, xu
2, ..., Iu
|Iu|} represent the
sequence of interacted micro-videos x ∈Iu of user u ∈U, which is sorted in
a chronological order according to the timestamp of each interaction, and xu
t

432
Y. Lu et al.
Fig. 1. Network Architecture of DMR. DMR is composed of an implicit user network
module, a multi-trend routing module, a multi-level time attention layer and a predic-
tion layer.
denote the micro-video that the user u has interacted with at timestamp t. The
interaction sequence Iu is split into I+ and I−which represent the micro-videos
clicked by the user and the ones not clicked respectively. Given the user’s histor-
ical micro-video interaction behaviors, the investigated goal of the micro-video
recommendation task in this paper is to predict the probability that the new
candidate micro-video will be clicked by user u.
To model diverse user preferences dynamically, DMR learns a function f for
mapping history trend set T h
u and future trend set T f
u into user representations,
which can be formulated as −→
eu = f(T h
u , T f
u ), where −→
eu ∈Rd×1 denotes the rep-
resentation vector of user u, d the dimension. Besides, the representation vector
of target micro-video i is obtained by an embedding function g as −→
ei = g(Ai),
where −→
ei ∈Rd×1 denotes the representation vector of target micro-video i.
Based on the learned user representation vector and micro-video represen-
tation vector, the probability of candidate micro-video is calculated using the
likelihood function P as p(i|U, V, X) = P(−→
eu, −→
ei ), where −→
ei is the embedding
of target item i from set of micro-videos V . Our framework outputs the click
probabilities of the candidate micro-video to rank the personalized recommen-
dation list. Then the system provide precise and diversiﬁed recommendation for
each user, which entails potential preference of the speciﬁc user as they are most
likely to interact with the recommended micro-videos.
The objective function for training our model is described in Sect. 2.6 We use
the Adam optimizer to train our method.
2.2
Overview
The overall structure of our proposed framework DMR is illustrated in Fig. 1,
which is composed of an implicit user network module, a multi-trend routing
module, a multi-level time-aware attention module and a prediction layer. As

Multi-trends Enhanced Dynamic Micro-video Recommendation
433
the relative future sequence for current user is actually the history sequence for
the neighbors, the multi-trend routing algorithm is applied on both the future
and history sequences using shared parameters in parallel. The framework takes
the user historical interactions set X as input. We use Xu
1,N−K and Xu
N−K+1,N to
represent training and testing data of interactions sequence of user u respectively.
N and K denotes the selected total length of interaction sequence of each user
u and the length of training sequence respectively. For micro-videos from the set
of Xu
1,N−K, embeddings are presented as −→e Xu
1,N−K.
Neighbor Candidate 
Selecon
Chronological (Train/Test) Set 
Paron
Ranking and Neighbor 
Decision
1
1
2
1
−+1
1
1
1
2
2
2
−+1
2
2
1
3
2
3
−+1
3
3
1
4
2
4
−+1
4
4
Test (last K)
Train
Candidate
User
hop=1
(item)
hop=2
(user)
Unselected Item/User 
Candidate Neighbor
Current User
Query Item
Top 
Correlaon
Computaon
User            Score
0.93
0.88
0.76
Fig. 2. Architecture of the implicit user network module. The leftmost part stands for
the neighbor candidate selection process based on user-item graph with the interactions
by edge, user and micro-videos by node. User behaviors of the selected neighbors are
then split into train and test set to compare their similarity to the current user. The
relative future sequence of the most similar users are utilized to generate the future
sequence as the input of multi-trend routing module, which output the future trend
representation.
2.3
Implicit User Network
As shown in Fig. 2, the implicit user network is constructed based on user-item
heterogeneous graph, which contains both the user nodes and item nodes. The
current user node is in the center, the linked nodes are query items, which extend
to candidate neighbors based on the items’ interaction records. The query items
are selected in a multi-hop manner. The user nodes connected to the selected
query items are considered as the candidate neighbor nodes of the current user.
An edge in the graph represents the interaction between the user and the item.
The weight of the edge indicates the temporal weight of each interacted item in
a chronological order.
We compare the similarity among users via collaborative ﬁltering implicitly
based on the historical interactions with micro-videos. As the Pearson Correla-
tion Coeﬃcient (PCC) is a widely used similarity measure, we adopt Pearson
Correlation Coeﬃcient [4] to compute a linear correlation between the user and

434
Y. Lu et al.
each candidate neighbor as:
sij =

k∈I(i)∩I(j)
(rik −ri) · (rjk −rj)


k∈I(i)∩I(j)
(rik −ri)2 ·


k∈I(i)∩I(j)
(rjk −rj)2
(1)
where I(i) is a set of micro-videos user i interacted with, rik and ri represents
the level (click or not click) of interaction of user i over micro-video k and the
average level of action of user i. The user similarity si is ranging from [−1, 1],
and the similarity between users i and j is proportional to the value according to
this deﬁnition. Following [17], we employ a mapping function f(x) = (x + 1)/2
to bound the range of PCC similarities into [0, 1].
In the case of users with only one common micro-video in history, PCC simi-
larity gets 1 when the users’ preferences over the common micro-video are similar
and −1 when not, which encourages diversity of neighbors while damaging the
fairness of similarity calculation. To tackle this issue, we only kept less than 20%
of such neighbor nodes to seek the balance.
In addition to the PCC method, we also design a ﬁlter with simple schema
to extract similar users. For each user, if the historical interactions Iu is split
into two pieces, Iu
1:t1 for training data, and Iu
t1:t2 for testing data, the item ˆIu
k is
deﬁned as the last k micro-videos, k could be any value less than or equal to |Iu|,
while in practice k = 1 can achieve good enough performance with simplicity.
We extracted a list of neighbors N = {n1, n2, ..., n|N|} according to the query
item. Furthermore, we constructed the future sequence of user u as:
Fu = {nf, nf ∈In, TI(nf) ≥TI(Iu
|Iu|−k)}
(2)
where Timestamp is denoted as TI and the query item is denoted as I|Iu|−k. In
represents the interaction set of neighbor n
2.4
Multi-trend Routing
To capture the trend information lies in both history sequence and future
sequence, we devised a multi-trend routing module into a two-stage manner to
generate trend represent parallelly. Speciﬁcally, we group each micro-video from
both the user’s historical sequence and extracted relative future sequence into
diverse trends in the ﬁrst stage. The micro-videos that are grouped into the same
trend are considered to be similar according to users’ interactions over them and
their own basic features. In the second stage, the micro-videos from historical
sequence and relative future sequence are utilized to generate the representation
of history and future trend group in parallel.

Multi-trends Enhanced Dynamic Micro-video Recommendation
435
Algorithm 1. Multi-trend Routing Algorithm
Require:
User’s historical interaction sequence Iu;
Matching scores P = ∪l0
g=1Pj = p1←j, p2←j, ..., ps←j;
Iteration number δ;
Ensure:
Interest groups X = ∪s
g=1g = ∪s
g=1i(g)
1 , i(g)
2 , ..., i(g)
l
;
1: for each ij ∈Hpos do
2:
Sj = −1, ϵSj ←∅
3: end for
4: for each iteration do
5:
for each ij ∈I+ do
6:
εg = logb(b + max(avgϵg −pg←j, 0)), g ∈[1, s];
7:
p(d)
g←j = pg←j/εg, g∗= arg maxg(p(d)
g←j);
8:
if g∗̸= Sj ∧p(d)
g←j > ϵ then
9:
εSj ←POP(εSj, ij), Sj = g∗, εg∗) ←ADD(εg∗, ij)
10:
end if
11:
end for
12: end for
13: return ε = ∪s
g=1εg
Based on the positive historical interaction sequence I+ of user u, we rep-
resent each micro-video x in I+ as an embedding vector −→x ∈Rd, where d is
the embedding size. And we initialize positive history trend group as T h
u ∈Rs×d
for user u, where s denotes the number of trend groups indicated from histor-
ical sequence and d denotes the embedding dimension of each history trend.
Speciﬁcally, each trend embedding is represented as −→t ∈Rd.
Similarly, based on the extracted future sequence F+ from the implicit user
network. The positive future trend group is denoted as T f
u ∈Rs×d for user u,
where s denotes the number of trend groups indicated from future sequence and
d denotes the embedding dimension of each future trend.
In order to ﬁne-tune the representation of each trend, we apply attention
mechanism over each micro-video and the initialized trend group. Given the
micro-video embedding −→x ∈Rd and the trend embedding −→t ∈Rd, we calculate
the weight between the micro-video and the trend based on a co-attention mem-
ory matrix. The micro-video from the history sequence and the future sequence
are put into history trend and future trend separately. As the history sequence
and future sequence is processed separately, our module is capable of capturing
timeliness of trends which indicates evolved user interest.
2.5
Multi-level Time Attention Mechanism
As for the item-level, we use the weighted sum of historical micro-video features
to obtain the current micro-video representation. Finally, we get the represen-
tation of each trend by attention mechanism on each micro-video in the trend

436
Y. Lu et al.
group. As for the trend-level, we utilize the time-aware attention to activate the
weight of diverse trends to capture the timeliness of each trend. Speciﬁcally, the
attention function takes the interaction time of item i, the interaction time of
trends and trend embeddings as the query, key and value respectively. We com-
pute the ﬁnal representation of trend representation future sequence of user u
as:
HFu = ATT(−→
TIi, −−→
TItr, −→
tu) = −→
tuσ(pow(−→
TIi, −−→
TItr))
(3)
where ATT denotes the attention function, σ denotes the softmax function, TIi
represents the interaction time of micro-video i, TItr represents the average
interaction time of micro-videos related to the trend group, −→
tu represents the
embedding of the speciﬁc trend group.
The trend group generated from the user’s historical sequence and future
sequence are then eventually updated by adding the corresponding trend group
in T h
u and T f
u with the aggregation of history trend and future trend represen-
tation respectively.
2.6
Prediction
After computing the trend embeddings from activated trends through time-aware
attention layer, we apply sumpooling to both history and future trend represen-
tations.
eh
u = sumpooling(T h1
u , ..., T hs
u ),
ef
u = sumpooling(T f1
u , ..., T fs
u )
(4)
And then we concatenate the history trend representation vector eh
u and
future trend representation vector ef
u to form a user preference embedding −→
eu as:
−→
eu = eh
u ⌢ef
u
(5)
Given a training sample u, i with the user preference embedding −→
eu and
micro-video embedding −→
ei as well as the micro-video set V , we can predict the
possibility of the user interacting with the micro-video as
p(i|U, V, I) =
exp(−→
euT −→
ei )

v∈V exp(−→
euT −→
ev)
(6)
In the same way, we calculate the prediction score P(x|H−) based on the
negative interaction sequence, which aims to maximize the distance between the
new micro-video embedding and user’s negative trend embeddings.
The ﬁnal recommendation probability ˆpij is represented by the linear com-
bination of p(x|H+) and p(x|H−). And the objective function of our model is as
follows:
L = −

i∈U
⎛
⎝
i∈H+
log σ(pui) +

i∈H−
log(1 −σ(ˆpui))
⎞
⎠
(7)
where ˆpui denotes the prediction score of micro-video i for user u, σ represents
the sigmoid activation function.

Multi-trends Enhanced Dynamic Micro-video Recommendation
437
3
Experiments
3.1
Dataset
MicroVideo-1.7M [7] and KuaiShou were used as micro-video benchmark
datasets in our experiments. Micro-video data and user-video interaction infor-
mation can be found in each of these datasets. Each micro-video is represented
by its features in these two datasets, and each interaction record includes the
userID, micro-video ID, visited timestamp, and whether the user clicked the
video.
Table 1. Overall Performance Comparision. The model performance of our model
and several state-of-the-art baselines on two public datasets: MicroVideo-1.7M and
KuaiShou-Dataset. The best results are highlighted in bold.
Model
MicroVideo-1.7M
KuaiShou-Dataset
AUC@50 Precision@50 Recall@50 F1-score@50 AUC@50 Precision@50 Recall@50 F1-score@50
BPR
0.583
0.241
0.181
0.206
0.595
0.290
0.387
0.331
LSTM
0.641
0.277
0.205
0.236
0.731
0.316
0.420
0.360
CNN
0.650
0.287
0.214
0.245
0.719
0.312
0.413
0.356
NCF
0.672
0.316
0.225
0.262
0.724
0.320
0.420
0.364
ATRank
0.660
0.297
0.221
0.253
0.722
0.322
0.426
0.367
THACIL 0.684
0.324
0.234
0.269
0.727
0.325
0.429
0.369
ALPINE 0.713
0.300
0.460
0.362
0.739
0.331
0.436
0.376
MTIN
0.729
0.317
0.476
0.381
0.752
0.341
0.449
0.388
DMR
0.731
0.323
0.478
0.385
0.742
0.343
0.442
0.386
Table 2. Eﬀect analysis of Neighbors. The model performance with diﬀerent Neighbor
Number setting on two datasets: MicroVideo-1.7M and KuaiShou-Dataset. The metrics
are @50. Here we set Neighbor Number to 5, 20, 50.
Model
MicroVideo-1.7M
KuaiShou-Dataset
AUC@50 Precision@50 Recall@50 F1-score@50 AUC@50 Precision@50 Recall@50 F1-score@50
DMR-N5
0.689
0.319
0.425
0.364
0.674
0.333
0.439
0.378
DMR-N20 0.731
0.323
0.478
0.385
0.742
0.343
0.442
0.386
DMR-N50 0.668
0.280
0.282
0.281
0.652
0.329
0.404
0.362
3.2
Implementation Details
We used TensorFlow on four Tesla P40 GPUs to train our model with Adam
optimizer. The following are the hyper-parameters: The micro-video embedding
is 512-dimensional vectors, while the user embedding is 128-dimensional vectors.
The batch size was set to 32, the optimizer was Adam, the learning rate was set
to 0.001, and the regularization factor was set to 0.0001. To ﬁnd the user’s similar
neighbors, we used the Pearson Correlation Coeﬃcient (PCC) described earlier.
In the ablation analysis, we set neighbor numbers as 5, 20, and 50. As for the
future sequences, we cut oﬀeach neighbor’s at most 100 interacted micro-videos
after the current user’s query items.

438
Y. Lu et al.
3.3
Evaluation Metrics
To compare the performance of diﬀerent models, we use Precision@N,
Recall@N, F1-score@N and AUC, where N is set to 50 as metrics for evalu-
ation.
3.4
Competitors
To validate the eﬀectiveness of our proposed DMR framework, we conducted
experiments on two publicly available real-world datasets. The comparision
to other state-of-the-art micro-video recommenders (BPR [21], ALPINE [15],
MTIN [14], etc.) are summarized in Table 1. To intelligently route micro videos
to target users, ALPINE proposed an LSTM model based on a temporal graph,
which is encoded by user’s historical interaction sequence. And MTIN is a multi-
scale time-aware user interest modeling framework, which learns user interests
from ﬁne-grained interest groups.
3.5
Results
The model performance on the two datasets is summarized in Table 1. We run
experiments to dissect the eﬀectiveness of our recommendation model. We com-
pare the performance of DMR with several commonly used and state-of-the-art
models: BPR, LSTM, CNN, NCF, ATRank, THACIL, ALPINE and MTIN. All
these models are running on the two datasets introduced above: MicroVideo-
1.7M and KuaiShou-Dataset. According to the results shown in Table 1, our
model DMR achieve better performance on precision over KuaiShou dataset
and performs better in terms of AUC, Recall and F1-score over MicroVideo-
1.7M dataset.
We investigate the performance of our model DMR in diﬀerent parameter
setting by changing number of trends T. We set the two datasets setting T to 2,
6 and 12. Our model achieves improvements on T = 6 over T = 2, which may
caused by insuﬃcient trends for the dataset. However, it did not show much
improvement when we change T from 6 to 12 showing clustering sequences to 12
trends in these datasets is redundant and to 6 trends is just suitable. Thus we
ﬁx T as 6 in our following experimental reports. We also compares the result of
setting query item from the ﬁrst to last item, the third to last item and the ﬁfth
to last one. The largest improvements appear on increasing K from 1 to 3. This
demonstrates that by adding the number of query items, our model can cap-
ture more trend information and be more powerful to predict future sequences.
Increasing query item number from 3 to 5 does not gain much improvement.
This implies our model is eﬃcient to capture much trend information by few
historical items of the user. Thus we ﬁx the last K number as 1 in our following
experimental reports. Table 2 compares the result of diﬀerent neighbor number
setting of 5, 20 and 50. Considering more neighbors could result in more diver-
sity, but too many neighbors would dilute interest trends’ embedding. Our model
achieves improvements on neighbor number equals 20 over 5. Besides, it shows

Multi-trends Enhanced Dynamic Micro-video Recommendation
439
reduction if setting neighbor number from 20 to 50. This means the number of
neighbors also play a crucial part in model performance.
The computational complexity of sequence layer modeling user and neighbors
is O(knd2), where k denotes the number of extracted neighbors, n denotes the
average sequence length and d denotes the dimension of item’s representation.
Capsule layer’s computational complexity depends on kernel size and number of
trends. Average time complexity of capsule layer scales O(nTr2), where r denotes
kernel size of capsule layer and T denotes the number of trends. For large-scale
applications, our proposed model could reduce computational complexity by
two measures: (1) encode neighbors with a momentum encoder [12].(2) adopt a
light-weight Capsule network.
3.6
Recommendation Diversity
Aside from achieving high recommendation accuracy, diversity is also essential
for the user experience. With little information of historical interactions between
the users and the micro-videos, recommendation systems learned to assist users
in selecting micro-videos that would be of interest to them. Recommender sys-
tems keep track of how users interacted with the micro-videos they’ve chosen.
Table 3. Model Recommendation Diversity Comparision on Micro-video Dataset.
MicroVideo-1.7M THACIL MTIN DMR
Diversity@10
1.9112
1.9940 1.9948
Diversity@50
1.9104
1.9948 1.9956
Diversity@100
1.9436
1.9950 1.9954
Many research works [1,3,9,20] have been undertaken to propose novel diver-
siﬁcation algorithms. Our proposed module can learn the diverse trends of user
preference and provide recommendation with diversity. We deﬁne the individual
diversity as below:
D@N =
N
j=1
N
k=j+1 δ(C(ˆiu,j) ̸= C(ˆiu,k))
N × (N −1)/2
(8)
where C represents the category of the item. ˆiu denotes item recommended
for user u, j and k represents the order of the recommended items. δ(·) is an
indicator function.
Table 3 presents comparisons with THACIL and MTIN over the recommen-
dation diversity metric on Micro-video dataset, which provides category informa-
tion of micro-videos. We adopt the setting of six historical trend and six future
trend evolved from 5 neighbors for our model. From the table, our module DMR
achieve the optimum diversity metric indicating the recommendation it provide
can eﬀectively take neighbors’ interests into account.

440
Y. Lu et al.
4
Conclusion
In this work, we propose to capture even more diverse and dynamic interests
beyond those implied by the historical behaviors for micro-video recommenda-
tion. We refer to the future interest directions as trends and devise the DMR
framework. DMR employ an implicit user network module to extract future
sequence fragments from similar users. A mutli-trend routing module assigns
these future sequences to diﬀerent trend groups and updates the corresponding
trending memory slot in a dynamic read-write manner. Final predictions are
made based on both future evolved trends and history evolved trends with a
history-future trends joint prediction module.
Acknowledgments. This work was supported Key R & D Projects of the Ministry
of Science and Technology (2020YFC0832503).
References
1. Adomavicius, G., Kwon, Y.: Improving aggregate recommendation diversity using
ranking-based techniques. IEEE Trans. Knowl. Data Eng. 24(5), 896–911 (2012)
2. Baluja, S., et al.: Video suggestion and discovery for youtube: taking random walks
through the view graph. In: Proceedings of the 17th International Conference on
World Wide Web, WWW 2008, Beijing, China, 21–25 April 2008 (2008)
3. Boim, R., Milo, T., Novgorodov, S.: Diversiﬁcation and reﬁnement in collaborative
ﬁltering recommender. In: Proceedings of the 20th ACM International Conference
on Information and Knowledge Management, CIKM 2011, pp. 739–744. Association
for Computing Machinery, New York (2011)
4. Breese, J.S., Heckerman, D., Kadie, C.: Empirical analysis of predictive algorithms
for collaborative ﬁltering (2013)
5. Chen, B., Wang, J., Huang, Q., Mei, T.: Personalized video recommendation
through tripartite graph propagation. In: Proceedings of the 20th ACM Multi-
media Conference, MM 2012, Nara, Japan, 29 October–02 November 2012 (2012)
6. Chen, J., Song, X., Nie, L., Wang, X., Zhang, H., Chua, T.-S.: Micro tells macro:
predicting the popularity of micro-videos via a transductive model. In: Proceedings
of the 2016 ACM Conference on Multimedia Conference, MM 2016, Amsterdam,
The Netherlands, 15–19 October 2016 (2016)
7. Chen, X., Liu, D., Zha, Z.J., Zhou, W., Xiong, Z., Li, Y.: Temporal hierarchical
attention at category- and item-level for micro-video click-through prediction. In:
2018 ACM Multimedia Conference on Multimedia Conference, MM 2018, Seoul,
Republic of Korea, 22–26 October 2018 (2018)
8. Cui, P., Wang, Z., Su, Z.: What videos are similar with you?: learning a com-
mon attributed representation for video recommendation. In: Proceedings of the
ACM International Conference on Multimedia, MM 2014, Orlando, FL, USA, 03–
07 November 2014 (2014)
9. Di Noia, T., Ostuni, V.C., Rosati, J., Tomeo, P., Di Sciascio, E.: An analysis of
users’ propensity toward diversity in recommendations. In: Proceedings of the 8th
ACM Conference on Recommender Systems, RecSys 2014, pp. 285–288. Associa-
tion for Computing Machinery, New York (2014)

Multi-trends Enhanced Dynamic Micro-video Recommendation
441
10. Dong, J., Li, X., Xu, C., Yang, G., Wang, X.: Feature re-learning with data aug-
mentation for content-based video recommendation. In: 2018 ACM Multimedia
Conference on Multimedia Conference, MM 2018, Seoul, Republic of Korea, 22–26
October 2018 (2018)
11. Du, X., Wang, X., He, X., Li, Z., Tang, J., Chua, T.-S.: How to learn item rep-
resentation for cold-start multimedia recommendation? In: MM 2020: The 28th
ACM International Conference on Multimedia, Virtual Event/Seattle, WA, USA,
12–16 October 2020 (2020)
12. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.B.: Momentum contrast for unsuper-
vised visual representation learning. CoRR, abs/1911.05722 (2019)
13. Huang, Y., Cui, B., Jiang, J., Hong, K., Zhang, W., Xie, Y.: Real-time video
recommendation exploration. In: Proceedings of the 2016 International Conference
on Management of Data, SIGMOD Conference 2016, San Francisco, CA, USA, 26
June–01 July 2016 (2016)
14. Jiang, H., Wang, W., Wei, Y., Gao, Z., Wang, Y., Nie, L.: What aspect do you like:
Multi-scale time-aware user interest modeling for micro-video recommendation.
In: MM 2020: The 28th ACM International Conference on Multimedia, Virtual
Event/Seattle, WA, USA, 12–16 October 2020, pp. 3487–3495. Association for
Computing Machinery, New York (2020)
15. Li, Y., Liu, M., Yin, J., Cui, C., Xu, X.-S., Nie, L.: Routing micro-videos via a
temporal graph-guided recommendation system. In: Proceedings of the 27th ACM
International Conference on Multimedia, MM 2019, Nice, France, 21–25 October
2019, pp. 1464–1472. Association for Computing Machinery, New York (2019)
16. Lu, Y., et al.: Future-aware diverse trends framework for recommendation. CoRR
(2020)
17. Ma, H.: An experimental study on implicit social recommendation, pp. 73–82
(2013)
18. Mei, T., Yang, B., Hua, X.-S., Li, S.: Contextual video recommendation by multi-
modal relevance and user feedback. ACM Trans. Inf. Syst. 29(2), 1–24 (2011)
19. Park, J.: An online video recommendation framework using view based tag cloud
aggregation. IEEE Multimedia (2010)
20. Premchaiswadi, W., Poompuang, P., Jongswat, N., Premchaiswadi, N.: Enhancing
diversity-accuracy technique on user-based top-n recommendation algorithms. In:
Proceedings of the 2013 IEEE 37th Annual Computer Software and Applications
Conference Workshops, COMPSACW 2013, pp. 403–408. IEEE Computer Society,
USA (2013)
21. Rendle, S., Freudenthaler, C., Gantner, Z., Schmidt-Thieme, L.: BPR: Bayesian
personalized ranking from implicit feedback. CoRR, abs/1205.2618 (2012)
22. Yan, M., Sang, J., Xu, C.: Uniﬁed youtube video recommendation via cross-network
collaboration. In: Proceedings of the 5th ACM on International Conference on
Multimedia Retrieval, Shanghai, China, 23–26 June 2015 (2015)
23. Zhou, X., Chen, L., Zhang, Y., Cao, L., Huang, G., Wang, C.: Online video rec-
ommendation in sharing community. In: Proceedings of the 2015 ACM SIGMOD
International Conference on Management of Data, Melbourne, Victoria, Australia,
31 May–4 June 2015 (2015)

Parameters Eﬃcient Fine-Tuning
for Long-Tailed Sequential
Recommendation
Zheqi Lv1
, Feng Wang2
, Shengyu Zhang3(B)
, Wenqiao Zhang3
,
Kun Kuang1(B)
, and Fei Wu1(B)
1 College of Computer Science and Technology, Zhejiang University,
Hang Zhou, China
{zheqilv,kunkuang,wufei}@zju.edu.cn
2 School of Software Technology, Zhejiang University, Hang Zhou, China
3 DAMO Academy, Alibaba Group, Hang Zhou, China
{sy zhang,wenqiaozhang}@zju.edu.cn
Abstract. In an era of information explosion, recommendation systems
play an important role in people’s daily life by facilitating content explo-
ration. It is known that user activeness, i.e., number of behaviors, tends
to follow a long-tail distribution, where the majority of users are with
low activeness. In practice, we observe that tail users suﬀer from sig-
niﬁcantly lower-quality recommendation than the head users after joint
training. We further identify that a model trained on tail users sepa-
rately still achieve inferior results due to limited data. Though long-tail
distributions are ubiquitous in recommendation systems, improving the
recommendation performance on the tail users still remains challenge
in both research and industry. Directly applying related methods on
long-tail distribution might be at risk of hurting the experience of head
users, which is less aﬀordable since a small portion of head users with
high activeness contribute a considerate portion of platform revenue. In
this paper, we propose a novel approach that signiﬁcantly improves the
recommendation performance of the tail users while achieving at least
comparable performance for the head users over the base model. The
essence of this approach is a novel Gradient Aggregation technique that
learns common knowledge shared by all users into a backbone model,
followed by separate plugin prediction networks for the head users and
the tail users personalization. As for common knowledge learning, we
leverage the backward adjustment from the causality theory for decon-
founding the gradient estimation and thus shielding oﬀthe backbone
Z. Lv and F. Wang—These authors equally contributed to this study.
This work was supported in part by National Natural Science Foundation of
China (62006207, 62037001, U20A20387), Young Elite Scientists Sponsorship Pro-
gram by CAST (2021QNRC001), Zhejiang Province Natural Science Foundation
(LQ21F020020), Project by Shanghai AI Laboratory (P22KS00111), Program of Zhe-
jiang Province Science and Technology (2022C01044), the StarryNight Science Fund
of Zhejiang University Shanghai Institute for Advanced Study (SN-ZJU-SIAS-0010),
and the Fundamental Research Funds for the Central Universities (226-2022-00142,
226-2022-00051).
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 442–459, 2024.
https://doi.org/10.1007/978-981-99-8850-1_36

Fine-Tuning for Long-Tailed Sequential Recommendation
443
training from the confounder, i.e., user activeness. We conduct extensive
experiments on two public recommendation benchmark datasets and a
large-scale industrial datasets collected from the Alipay platform. Empir-
ical studies validate the rationality and eﬀectiveness of our approach.
Keywords: Recommendation System · Long-tail · Gradient
Aggregation · Collaborative Training
1
Introduction
With the rapid development of the Internet in recent years, recommendation
systems play a vital role in people’s daily life. Since users’ behaviors naturally
show chronological dependencies, many sequencial recommendation algorithms
have been proposed, including, but not limited to, GRU4Rec [4], DIN [30], SAS-
Rec [7], etc. One of the most enticing merits of sequential models is that we
can dynamically update the recommendation list as long as users interact with
new items. Many existing researches on recommender systems focus on person-
alization [12,13], disentanglement [26], Despite their great advances, we iden-
tify that existing methods still achieve less satisﬁed recommendation quality for
non-active users. In industrial recommendation systems, it is known that user
activeness tends to form a long-tail distribution. For example, as shown in Fig. 1,
the majority of users have signiﬁcantly less number of behaviors than head users.
Apparently, to improve recommendation quality of most users, it is of paramount
importance to pay attention to the tail users with low activeness.
(a)
(b)
Fig. 1. The long-tail eﬀect
on Alipay dataset.
Investigating long-tail distributions has gained
much attention in the literature [27], especially in
computer vision [5,22,29]. However, these methods
cannot be directly applied to the recommendation
domain due to its unique challenges. Speciﬁcally, in
the recommendation system, the behaviors of tail
users are quite limited, leading to less adequate pref-
erence understanding. Moreover, we identify that
models are easily biased towards head users during
joint training and thus make the problem even more
severe.
As for separate training for tail users, we con-
duct an empirical study and ﬁnd that the separately
trained model still achieves inferior results due to
the limited amount of data. At present, studying tail
users in recommendation systems is still a nascent
research ﬁeld, such as Grad-Transfer [24]. Despite
the performance improvement on tail users, most
of them are at risk of sacriﬁcing the performance
of head users. Note that hurting the experiences of
head users is less aﬀordable in industrial environments due to the large amount

444
Z. Lv et al.
of platform revenue contributed by active users. Another line of related works
is cold-start recommendation [2,9,18], which aims to improve the performance
of users that are not observed during training. We diﬀer from these works fun-
damentally by improving the performance of observed tail users during joint
training.
We argue that improving the model’s performance on the tail user base does
not necessarily degrade the model’s performance on the short-head user base.
Similar users (no matter how active they are) should all beneﬁt from joint train-
ing through collaborative ﬁltering. Besides these common knowledge shared by
similar users, each user has their personalized preferences. Therefore, we argue
that one possible solution is to extracting common knowledge that is shared
by all users and is independent from user activeness while retaining the per-
sonalized information for diﬀerent users. Upon the above analysis, we devise a
gradient aggregation framework for long-tailed sequential user behavior mod-
eling. To shield oﬀthe common knowledge learning from user activeness, we
borrow back-door adjustment [16] technique from the causality theory and uti-
lize it like [26]. In particular, we view the user activeness as a confounder and
deconfound the model training via causal intervention. To instantiate back-door
adjustment, a grouping strategy and a gradient aggregation strategy are pro-
posed. We group each group of data according to the activity divided by the
time window, and at the same time ensure that the amount of data in each
group is as equal as possible. During training, we intervene the gradient esti-
mation via back-door adjustment, leading to a activeness-independent training.
Upon the causally learned trunk that encodes the common knowledge, we devise
multiple plugin networks for group-speciﬁc personalization. Through the archi-
tecture of the trunk network and the plugin network, the model not only has
strong general reasoning ability, but also has good individual reasoning ability.
Moreover, we conduct extensive experiments on industrial datasets to demon-
strate the eﬀectiveness of our method. In this paper, our main contributions are
as follows:
– We investigate how to improve the recommendation quality of tail non-active
users without sacriﬁcing the performance on active users. We identify dis-
entangle the common knowledge learning and personalization is a plausible
solution to the problem.
– We learn a trunk model that encodes the common knowledge via back-door
adjustment borrowed from causality theory. With causal intervention, the
estimated gradient should be independent from user activeness. Upon the
trunk, we devise several plugin networks for group-speciﬁc personalization.
– We completed experiments on Movielens, Amazon, and Alipay datasets.
Experiments show that our method is practical and eﬀective.
2
Related Work
2.1
Long-Tail
Deep long-tail learning is one of the most challenging problems in deep learning,
and its goal is to train well-performing deep learning models from data that

Fine-Tuning for Long-Tailed Sequential Recommendation
445
follow a long-tailed class distribution. Long-tail class imbalance tends to limit
the usefulness of deep learning-based models in real-world applications, as they
tend to be biased towards dominant classes and perform poorly on tail classes.
[27] In order to solve this problem, researchers have carried out a lot of research
in recent years, and have made gratifying progress in the ﬁeld of deep long-
tail learning. Existing research on deep long-tail learning is mainly divided into
three categories, namely class rebalancing [28,29], information augmentation [11,
22,24], and module improvement [5,17]. Although some other work is more or
less related to the long tail, they cannot be directly transferred to the long tail
learning [2,6,9,10,18,21,31].
2.2
Gradient Surgery
Gradient surgery is usually used in ﬁelds such as multi-task learning and domain
generalization. It can use the knowledge learned on one task during the train-
ing process to improve the generalization performance of the model on other
tasks. To achieve this, GradNorm [1] dynamically scales the gradients to ensure
that the gradients produced by each task contribute similarly to model training.
PCGrad [25] achieves soft fusion of two gradients with conﬂicting components
by projecting between gradients of diﬀerent tasks. GradVac [23] sets a separate
gradient similarity objective for each task pair, making better use of inter-task
correlations through adaptive gradient similarity. Agr-Rand [14] solves domain
generalization through gradient surgery. It determines how gradients are updated
by comparing the sign bits between gradients across domains.
3
Method
In this paper, we focus on the problem of long-tail distribution in recommenda-
tion in real industrial applications. To address this problem, we propose a novel
framework to disentangle the common knowledge learning and personalization
for improving the recommendation quality of the tail non-active users without
sacriﬁcing the performance on the head active users. Figure 2 shows the archi-
tecture of our proposed method. Next, we will detail each component of the
proposed method.
3.1
Preliminaries
The sequence recommendation model can generally be expressed as: supposing
we have a user set U = {u1, u2, ..., un}, and an item set I = {i1, i2, ..., im}.
The click sequence S = {su1, su2, ..., sun} represents the L items that the user
u has clicked recently so far. We randomly select some user-item pairs and the
corresponding user behavior sequence Su from the data set and put them into
the model to get a prediction score ˆyu,i between 0 and 1, that is,
ˆyu,i = f(u, i, su|θ)
(1)

446
Z. Lv et al.
Fig. 2. Architecture of the proposed method. (1) During training, our method consists
of two stages. The ﬁrst stage is to use all of the data for training and aggregate the
gradients of each group to obtain a trunk network that can extract generalization
information. The second stage is to insert the plugin network into the trunk network.
The plugin network for each group is trained using only the data of this group. At
the same time, the trunk network is still undergoing updates similar to the ﬁrst stage.
In general, the trunk network extracts the common information of all groups, and
the plugin network extracts the personalized information of each group. (2) During
inference, the plugin network is plugged to the trunk network to realize the fusion of
general information inference and individual information inference.
f(·) denotes the forward propagation function and θ represents the parameters
of the model. The ground-truth yu,i ∈{0, 1} is a binary variable, so we can use
the binary cross entropy loss as the loss function which is commonly used in
binary classiﬁcation problems to calculate the loss. We use D to represent the
entire dataset, then we can get the loss function L by
L = −1
|D|
|D|

i=1
yi log ˆyu,i + (1 −yi) log(1 −ˆyu,i)
(2)
Then, we backpropagate the loss and update f, repeat several times to obtain
the optimal parameter θ, and ﬁnally a sequential recommendation model can be
obtained.
θ = arg min
θ
L
(3)
3.2
Gradient Aggregation
In the recommendation system, although the diﬀerence in user activity also
aﬀects the model’s judgment of their preferences, users with diﬀerent activity
levels also have a lot of common information that can be extracted by the model.

Fine-Tuning for Long-Tailed Sequential Recommendation
447
These common information can not only help the model perform better on the
tail user group, but also help the model to better learn the preferences of high-
active users. Since the common information is implicit and highly entangled
with personalized information, it is hard to identify and extract them directly.
To bridge the gap, we borrow the back-door adjustment technique [16] from
the causal literature and deconfound the model training. Deconfounding means
shielding the model training/prediction from possible confounders. Since the
major focus is user activeness, i.e., the number of users’ behaviors, we regard it
as the confounder. Formally, by the Bayes rule, the original training gradients
can be obtained by:
P(g | u, i, yu,i) =

z
P(g, z | u, i, yu,i)
(4)
=

z
P(g | u, i, yu,i, z)P(z | u, i, yu,i),
(5)
where g denotes the model gradients. z denotes the confounder which is user
activeness here. P(·) denotes the forward function that takes u, i as the input
and a deterministic backward function that estimates the gradient based on the
prediction of the model and the ground-truth yu,i. In the language of causality,
the gradient estimation is confounded by the user activeness, leading to biased
training that is favor of active users. Backward adjustment deconfounds the
estimation by blocking the direct eﬀect z →u, i, yu,i:
P(g | do(u, i, yu,i)) =

z
P(g|u, i, yu,i, z)P(z)
(6)
where the proof can be found in [3,19,20]. In practice, the essence of the above
adjusted equation is to estimate the gradient P(g|u, i, yu,i, z) for each z value,
and sum all gradients from diﬀerent z values with the prior probabilities.
To estimate the gradients per z value, we divide users into n groups according
to the level of activeness. During training, each batch can be divided into n mini-
batches. When the j-th mini-batch in the b-th batch is trained with the data
from the j-th group, we can get the gradient g|k|
i,j produced by this mini-batch
at the k-th layer of the model. Similarly, when we train all mini-batches in this
batch, we can get a gradient set G|k|
i
= {g|k|
i,2, g|k|
i,2, . . . , g|k|
i,n}. When a mini-batch
in a batch completes backpropagation, we do not directly update the gradient
generated by the mini-batch but put the gradient into a gradient memory queue
Q|k|. After all mini-batches in this batch have completed backpropagation, we
aggregate these gradients using Eq. 6. The prior distribution of diﬀerent z values
is empirically set to a uniform distribution. This is because we divide the users
into equally-numbered groups and diﬀerent user groups are treated as equally
important so as to deal with the long-tail problems. Formally, we obtain the
deconfounded gradients as follows:
g|k|
b
= 1
n
n

j=1
N
nj
g|k|
b,j.
(7)

448
Z. Lv et al.
Among them, N represents the total number of samples in the dataset D, nj
represents the number of samples in the j-th user group. We use ft(·) denotes
the forward propagation function.
3.3
Plugin Network
After gradient aggregation, we can obtain a trunk network containing the com-
mon knowledge of all user groups. However, the group-speciﬁc knowledge is less
modeled, which may cause degraded performance. To alleviate the problem, we
introduce the plugin network for each group to learn the personalized knowledge
from the data of this group.
Naive Plugin Network. A naive method is training a plugin network of the
same size as the trunk network for each user group, with its model parameters
denoted Δθ. When predicting users belonging to the j-th group, it is necessary
to add the plugin network parameters Δθj and the trunk network parameters θ
dedicated to the j-th group. The forward propagation function is,
ˆyu,i = f(u, i, j, su|θ + Δθ)
(8)
Furthermore, we split the model into embedding layers, feature extractors
and classiﬁers. We use E to denote the parameters of the embedding layer in
the trunk network. Since the parameters of the embedding layer are very sparse,
only using the data of a certain user group for training will lead to inaccurate
mapping of the data in the feature space, we remove the embedding layer from
the plugin network. Therefore, we only update the parameters of the feature
extractor ΔΦj and the classiﬁer ΔWj when we train the plugin network for the
j-th user group. Formally, we predict the user-item pair of the j-th user group
as follows,
ˆyu,i = f(u, i, j, su|E, Φ + ΔΦj, W + ΔWj)
(9)
We compute the loss function as follows, where Lj denotes the data of the j-th
group of users,
Lj = −1
|Dj|
|Dj|

i=1
(yi log ˆyu,i + (1 −yi) log(1 −ˆyu,i)))
(10)
Then, we minimize the loss by,
ΔΦj, ΔWj = arg
min
ΔΦj,ΔWj Lj
(11)
Lightweight Plugin Network. The above plugin network is similar to the
residual module in ResNet, the diﬀerence is that we use the residual of the net-
work parameters instead of the residual of the feature. Such a method can stably
extract the personalized information of each group into the residual parameters

Fine-Tuning for Long-Tailed Sequential Recommendation
449
ΔΦ and ΔW. However, in order to avoid overﬁtting, the learning rate needs
to be set very low, which leads to a stable eﬀect but limited improvement. So
we improve the plugin network. Speciﬁcally, we insert plugin networks between
the embedding layer and the feature extractor and between the feature extrac-
tor and the classiﬁer, whose parameters are denoted by Φ
′ and W
′, respectively.
Speciﬁcally for the j-th user group, their parameters are Φ
′
j and W
′
j respectively.
ˆyu,i = f(u, i, j, su|θ, Φ
′
j, W
′
j)
(12)
The loss function is same to Eq. 10. Then, we backpropagate the loss on the
lightweight plugin network and minimize the loss by,
Φ
′
j, W
′
j = arg min
Φ′
j,W ′
j
Lj
(13)
At the same time, we continue to update the trunk network using Eq. 7 with
a lower learning rate. To summarize brieﬂy, we update Φ
′
j and W
′
j immediately
after the j-th mini-batch backpropagation of each batch. The trunk network is
updated after all mini-batches of each batch have completed backpropagation.
The plugin network can ﬁne-tune the intermediate features inside the trunk
network to meet the needs of model personalization. The trunk network with
continuous gradient aggregation can not only continuously improve the trunk
network’s ability to capture common information, but also prevent the bias of
the plugin network from being too large.
4
Experiments
In this section, we conduct extensive experiments to show the superiority of the
proposed framework. Speciﬁcally, we will answer the following questions.
1. RQ1: How eﬀective is the proposed method compared with the state-of-the-
art (SOTA) competitors?
2. RQ2: Does the proposed method actually improve the performance of tail
users?
3. RQ3: How do the gradient aggregation and plugin network aﬀect the perfor-
mance of the proposed method?
4. RQ4: Is the proposed method eﬀective for other mainstream recommendation
models?
4.1
Experiment Settings
Datasets. We conduct experiments on the following publicly accessible datasets:
MovieLens, Amazon, and Alipay. Details can be referred to Appendix.
Evaluation Metrics. In the experiments, we use the widely adopted AUC,
HitRate and NDCG as the metrics to evaluate model performance. They are
deﬁned by the following equations. Details can be referred to Appendix.

450
Z. Lv et al.
Baselines. To verify the applicability, the following representative sequential
modeling approaches are implemented and compared with the counterparts com-
bined with the proposed method. GRU4Rec [4], DIN [30], and SASRec [7] are
three of the most widely used sequential recommendation models. Agr-Rand [14],
PCGrad [25], and Grad-Transfer [24] are three gradient surgery methods. For
details, please refer to the Appendix.
Table 1. Group level performance comparison on each group data.
Dataset
Group
DIN
Agr-Rand Grad-Transfer PCGrad Ours
Movielens Group 1 0.8262 0.8246
0.8443
0.8375
0.8434
Group 2 0.9026 0.8828
0.9040
0.8964
0.9131
Group 3 0.9072 0.8950
0.9133
0.9097
0.9247
Group 4 0.9059 0.8899
0.9110
0.9033
0.9227
Group 5 0.9105 0.8813
0.9094
0.9021
0.9251
Amazon
Group 1 0.9196 0.9139
0.9196
0.9205
0.9233
Group 2 0.9218 0.9175
0.9211
0.9220
0.9249
Group 3 0.9237 0.9192
0.9246
0.9247
0.9261
Group 4 0.9240 0.9182
0.9240
0.9259
0.9275
Group 5 0.9272 0.9212
0.9270
0.9303
0.9285
Alipay
Group 1 0.7253 0.6881
0.7159
0.7269
0.7304
Group 2 0.7381 0.7045
0.7296
0.7390
0.7432
Group 3 0.7508 0.7169
0.7415
0.7511
0.7548
Group 4 0.7635 0.7297
0.7544
0.7625
0.7658
Group 5 0.8025 0.7615
0.7889
0.8002
0.8059
4.2
Experiments and Results
Overall Comparison (RQ1, RQ2). The comparison between our model and
three SOTA models on the group level and the user level are shown in Table 1
and Table 3, respectively.
Group-Level Analysis. We are interested in how diﬀerent user groups with
diﬀerent activeness beneﬁt from the propose technique. Towards this end, we
explicitly present the performance of these groups across three datasets. Accord-
ingly to the results in Table 1, we have the following observations:
– Overall, our technique yields the best performance across diﬀerent datasets
in most cases. Remarkably, the proposed method outperforms the best-
performing SOTA by +.0091, +.0029, and +.0042 w.r.t. AUC on the Movie-
Lens, Amazon, and AliPay datasets, respectively. Note that the improvement
+.0042 AUC is signiﬁcant for the large-scale industrial dataset, Alipay. These
results demonstrate the merits of our technique for recommendation.

Fine-Tuning for Long-Tailed Sequential Recommendation
451
– Our method achieves consistent improvement across diﬀerent user groups.
Although many SOTA methods achieve performance gains on some user
groups, they might fail to improve the base model in some other groups.
For example, Grad-Transfer achieves signiﬁcant performance gains on Group1
(+.0181, AUC), but achieves inferior results on Group5 (-.0011, AUC). These
results demonstrate that some SOTA baselines might be at risk of hurting the
experiences of some user groups. These results basically indicate the merit of
our plugin components, which focus on learning personalized patterns and
prevent over-correction for some users.
– The performance gain on tail user groups are larger than that of the head user
groups in our method. Note that the major argument of this paper is that
joint training in current recommendation models might be at risk of hurting
head user groups. These results reveal that our method could oﬀer all-user
groups better recommendation quality that they deserve. Importantly, with-
out sacriﬁcing the recommendation quality of the others. These are notable
merits in industrial environments where the non-active users might be poten-
tial loyal users and the active users currently contribute the most revenue.
User-Level Analysis. Note that the AUC in the above analysis is computed
in the group level. To further reveal the performance gain in the user level,
we compute the AUC metric for each user and take the average as the ﬁnal
performance. The results are shown in Table 3.
Method
Movielens
Amazon
Alipay
DIN
0.8909
0.9165
0.6905
Agr-Rand
0.8728
0.9124
0.6754
Grad-Transfer
0.8958
0.9177
0.6853
PCGrad
0.8886
0.9177
0.6897
Ours
0.9052
0.9202
0.6924
Fig. 3. User level performance compar-
ison on all data.
We observe that the proposed tech-
nique consistently outperforms the base
model DIN and three SOTA methods.
It is noteworthy that although the some
baselines can achieve performance gains
over DIN in the group-level, they cannot
beat DIN in the user level in most cases.
Remember that tail users are the major-
ity in many recommender systems. These
results basically indicate that these base-
lines are less optimal to deal with long-
tail problems, and might fail to improve the average recommendation quality
of all users. On the contrary, the consistent improvement of our technique fur-
ther reveals the rationality and eﬀectiveness of our problem analysis and model
design for tail user behavior modeling.

452
Z. Lv et al.
Table 2. Performance comparison in terms of AUC to demonstrate the eﬀects of
gradient aggregation and plugin model.
Dataset
Group
GRU4Rec +GA
+GA+PN DIN
+GA
+GA+PN SASRec +GA
+GA+PN
Movielens Group 1 0.8587
0.8595
0.8633
0.8262 0.8384 0.8434
0.8539
0.8591 0.8638
Group 2 0.9264
0.9210
0.9250
0.9026 0.8977 0.9131
0.9242
0.9221 0.9258
Group 3 0.9354
0.9338
0.9385
0.9072 0.9023 0.9247
0.9337
0.9345 0.9393
Group 4 0.9379
0.9393
0.9424
0.9059 0.9017 0.9227
0.9360
0.9358 0.9399
Group 5 0.9400
0.9371
0.9425
0.9105 0.8973 0.9251
0.9387
0.9355 0.9408
Amazon
Group 1 0.9306
0.9338
0.9340
0.9196 0.9198 0.9233
0.9287
0.9329 0.9337
Group 2 0.9329
0.9366
0.9367
0.9218 0.9211 0.9249
0.9315
0.9353 0.9357
Group 3 0.9381
0.9401
0.9407
0.9237 0.9216 0.9261
0.9367
0.9374 0.9384
Group 4 0.9407
0.9438
0.9452
0.9240 0.9228 0.9275
0.9399
0.9412 0.9417
Group 5 0.9479
0.9487 0.9470
0.9272 0.9270 0.9285
0.9458
0.9444 0.9421
Alipay
Group 1 0.7312
0.7318
0.7372
0.7253 0.7270 0.7304
0.7283
0.7313 0.7353
Group 2 0.7432
0.7432
0.7485
0.7381 0.7397 0.7432
0.7411
0.7428 0.7467
Group 3 0.7569
0.7565
0.7620
0.7508 0.7507 0.7548
0.7547
0.7558 0.7609
Group 4 0.7700
0.7694
0.7745
0.7635 0.7622 0.7658
0.7674
0.7682 0.7737
Group 5 0.8103
0.8081
0.8135
0.8025 0.7970 0.8059
0.8053
0.8067 0.8125
Ablation Study (RQ3, RQ4). We are interested in whether diﬀerent building
blocks all contribute to the proposed method. To have a comprehensive analysis,
we progressively add the Gradient Aggregation component (GA) and the Plugin
Network (PN) onto three base models (GRU4Rec, DIN, and SASRec) and test
the constructed architectures on three datasets. The results are shown in Table 2,
which also reveal the performance change on diﬀerent user groups since it is one
of the primary focuses in this paper. According to the results,
– The proposed technique (+GA+PN) demonstrates consistent performance
improvement over diﬀerent base recommendation architectures across diﬀer-
ent user groups and diﬀerent datasets in most cases. These comprehensive
results basically indicate that the eﬀectiveness of the proposed method is
model-agnostic and dataset-agnostic. These results again verify the merits of
the proposed method in long-tail sequential behavior modeling with similar
ﬁndings on ﬁve user groups to those illustrated in Section “Overall Compar-
ison”.
– Not surprisingly, the Gradient Aggregation (GA) module mainly improves
the recommendation performance for tail user groups while achieving compa-
rable or sometimes inferior results compared to the base model. For example,
+GA achieves AUC +.0122 on Group1 and AUC -.0132 on Group5 com-
pared to the DIN base model on the MovieLens dataset. These results are
reasonable in the sense that GA might neglect the personalization by mainly

Fine-Tuning for Long-Tailed Sequential Recommendation
453
focusing on learning common information shared by all users. Nevertheless,
learning common information boosts the performance on tail users, which is
our primary focus, and lays the foundation for further group-speciﬁc person-
alization. These results basically reveal the rationality of our analysis and the
eﬀectiveness of the GA module.
– Based on the common information learned by the GA module, the PN mod-
ule constructs an additional prediction network per group for group-speciﬁc
personalization. PN achieves consistent performance improvement over dif-
ferent base models for ﬁve groups. These results demonstrate the necessity
of personalization and the eﬀectiveness of our design. Compared to GA, the
PN component mostly improves the head users than tail users. For example,
+GA+PN outperforms the +GA by AUC +.0278 and AUC +.005 on Group5
and Group1, respectively. These are reasonable results since tail users have
signiﬁcantly less number of interactions than head users. By jointly analyzing
these results and those of GA, we can ﬁnd that both GA and PN components
are essential for bringing higher-quality recommendation to most users.
5
Conclusion
In this paper, we propose a gradient aggregation strategy. We group each data
set according to activity. During training, the gradients generated by each group
of data are aggregated to encourage the integration of knowledge from the per-
spective of optimization, and greatly improve the trunk model’s ability to extract
common information. In addition, we design the plugin network to guarantee the
extraction of personalized information in each user group. Through the architec-
ture of the backbone network and the plugin network, the model has both strong
general reasoning ability and good individual reasoning ability. Experiments on
real-world datasets demonstrate the eﬀectiveness of the proposed method by
comparing with state-of-the-art baselines.
A
Pseudo Code
The pseudo code of our proposed method is summarized in Algorithm 1.

454
Z. Lv et al.
Algorithm 1: Two-stage training strategy
Divide the dataset D into D1 ∼Dn according to the level of users’ activeness.
Stage I: ▷Train the Trunk Network
1) Initialize Trunk Network.
2) Feed each group of samples into Eq. (1) for forward propagation.
3) Compute loss ˆyu,i via Eq. (2).
4) Backpropagate via Eq. (3) and put the gradient into the gradient memory
queue.
5) Aggregate the gradients via Eq. (7) and update θ
Output the parameters θ of the Trunk Network.
Stage II: ▷Train the whole network
1) Initialize Plugin Network with small values.
2) Feed each group of samples into Eq. (12) for forward propagation.
3) Compute loss ˆyu,i via Eq.(10).
4) Backward the loss in Eq. (10) and update fp(·) and preserve the gradients
on ft(·).
5) Backpropagate via Eq. (13), update Φ
′ and W
′, put the gradient into the
gradient memory queue.
6) Aggregate the gradients via Eq. (7) and update θ
Output the parameters θ of the Trunk Network and the parameters Φ
′ and
W
′ of the Plugin Network.
B
Experiments
B.1
Experiments Settings
Datasets
Movielens1. MovieLens is a widely used public benchmark on movie ratings.
In our experiments, we use movielens-1M which contains one million samples.
Amazon2. Amazon Review dataset [15] is a widely-known recommendation
benchmark. We use the Amazon-Books dataset for evaluation.
Alipay. We collect a larger-scale industrial dataset for online evaluation from
the AliPay platform3. Applets such as mobile recharge service are treated as
items. For each user, clicked applets are treated as positives and other applets
exposed to the user are negatives.
The detailed statistics of these datasets are summarized in Table 3.
1 http://grouplens.org/datasets/movielens/.
2 http://jmcauley.ucsd.edu/data/amazon/.
3 https://www.alipay.com/.

Fine-Tuning for Long-Tailed Sequential Recommendation
455
Table 3. Statistics of the evaluation datasets.
Dataset
#Users #Items #Interactions #Records/#user #Records/#item Density
Movielens 6,040
3,706
1,000,209
165.57
269.89
4.468%
Amazon
69,168
81,473
3,137,442
45.36
38.51
0.056%
Alipay
262,446 4,545
27,849,672
106.12
6,127.54
2.334%
Evaluation Metrics
AUC =

x0∈DT

x1∈DF 1[f(x1) < f(x0)]
|DT ||DF |
,
HitRate@K = 1
|U|

u∈U
1(Ru,gu ≤K),
(14)
where 1(·) is the indicator function, f is the model to be evaluated, Ru,gu is the
rank predicted by the model for the ground truth item gu and user u, and DT ,
DF is the positive and negative testing sample set respectively.
Baselines
GRU4Rec [4] is one of the early works that introduce recurrent neural networks
to model user behavior sequences in recommendation.
DIN [30] introduces a target-attention mechanism for historically interacted
items aggregation for click-through-rate prediction.
SASRec [7] is a representative sequential modeling method based on self-
attention mechanisms. It simultaneously predicts multiple next-items by masking
the backward connections in the attention map.
To evaluate the eﬀectiveness on tail user modeling, the following competing
methods are introduced for comparison.
Agr-Rand [14] introduced a gradient surgery strategy to solve the domain gen-
eralization problem by coordinating inter-domain gradients to update neural
weights in common consistent directions to create a more robust image classi-
ﬁer.
PCGrad [25] is a very classic gradient surgery model that mitigates the nega-
tive cosine similarity problem by projecting the gradients of one task onto the
normal components of the gradients of the other task by removing the disturbing
components to mitigate gradient conﬂicts.
Grad-Transfer [24] adjusts the weight of each user during training through
resample and gradient alignment, and adopts an adversarial learning method to
avoid the model from using the sensitive information of user activity group in
prediction to solve the long-tail problem.

456
Z. Lv et al.
Implementation Details
Preprocessing. On the Alipay dataset, the dates of all samples in the dataset
are from 2021-5-19 to 2021-7-10. In order to simulate the real a/b testing envi-
ronment, we use the date to divide the dataset. We take the data before 0:00
AM in 2021-7-1 as the training set, and vice versa as the test set. On Movielens
and Amazon datasets, we treat the labels of all user-item pairs in the dataset
as 1, and the labels of user-item pairs that have not appeared as 0. We take
the user’s last sample as the test set. On Movielens, we use positive samples in
the training set: the ratio of negative samples = 1:4 to sample negative samples.
In the test set, we refer to [8], so we use all negative samples of a user as the
test set. In Amazon’s training set, we sample negative samples with the ratio of
positive samples: negative samples = 1:4, and this ratio becomes 1:99 in the test
set. We also ﬁlter out all users and items in Amazon with less than 15 clicks to
reduce the dataset. On Alipay and Amazon datasets, we group by the number
of samples of users. On the Movielens dataset, we group by the length of the
user’s click sequence
Implementation. In terms of hardware, our models are trained on workstations
equipped with NVidia Tesla V100 GPUs. For all datasets and all models, the
batch size is set to 512. The loss function is optimized by the Adam optimizer
with a learning rate of 0.001 for the gradient aggregation learning stage and
0.0001 for the plugin model learning stage. The training is stopped when the
loss converges on the validation set.
B.2
Results
Fig. 4. Performance of Hit@1 on vali-
dation set with diﬀerent epochs.
According to Fig. 4, the larger the group
number, the more active the user is, that
is, the ﬁrst group is the least active user
group, and the ﬁfth group is the most
active user group. Training the plugin net-
work for relatively inactive groups of users
requires only a small number of epochs to
be optimal (such as groups 1 and 2). The
training curve of the user group with rela-
tively high activity level has a more stable
upward trend with the increase of epoch
(such as group 3, group 4 and group 5).
This is mainly due to the diﬀerence in the
amount of personalized information in user groups with diﬀerent levels of activ-
ity. For a user group with a large amount of data, more personalized information
is required, and more epochs are needed to learn the personalized information.
Otherwise, only a few epochs are required.
Toy Example. In Fig. 5, we give a toy example of long-tail eﬀect of a real case
in Alipay platform and show the improvement brought by our propose method.

Fine-Tuning for Long-Tailed Sequential Recommendation
457
In this case, there are two groups of women, one is young women with high
activity and the other is middle-aged women with low activity. They have com-
mon preferences such as clothes and shoes but also some diﬀerent preferences.
Due to the long-tail eﬀect, the preferences of low-activity women are diﬃcult
to capture, and the model will recommend some popular products for them.
Fig. 5. A toy example of the long-tail eﬀect and
the improvement brought by our method.
To
address
this
problem,
we
extract
generalization
informa-
tion via the gradient aggregation
module so that the model can
recommend common preferences
such as clothes and shoes to low-
activity women, although some-
times not her favorite style. The
model’s recommendation for the
high-activity women and the low-
activity women are more simi-
lar, so the performance of the
model on high-activity women has
decreased. Next, we train a plugin
network for each group of women. The plugin network captures the group-speciﬁc
personalization information such as diﬀerent styles of clothes and shoes and other
unpopular preference.
References
1. Chen, Z., Badrinarayanan, V., Lee, C.Y., Rabinovich, A.: Gradnorm: gradient nor-
malization for adaptive loss balancing in deep multitask networks. In: International
Conference on Machine Learning, pp. 794–803. PMLR (2018)
2. Dong, M., Yuan, F., Yao, L., Xu, X., Zhu, L.: Mamo: memory-augmented meta-
optimization for cold-start recommendation. In: Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
688–697 (2020)
3. Glymour, M., Pearl, J., Jewell, N.P.: Causal Inference in Statistics: A Primer.
Wiley, Hoboken (2016)
4. Hidasi, B., Karatzoglou, A., Baltrunas, L., Tikk, D.: Session-based recommenda-
tions with recurrent neural networks. In: International Conference on Learning
Representations 2016 (2016)
5. Huang, C., Li, Y., Loy, C.C., Tang, X.: Learning deep representation for imbalanced
classiﬁcation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5375–5384 (2016)
6. Huang, R., et al.: Audiogpt: understanding and generating speech, music, sound,
and talking head. arXiv preprint arXiv:2304.12995 (2023)
7. Kang, W.C., McAuley, J.: Self-attentive sequential recommendation. In: 2018 IEEE
International Conference on Data Mining (ICDM), pp. 197–206. IEEE (2018)
8. Krichene, W., Rendle, S.: On sampled metrics for item recommendation. In: Pro-
ceedings of the 26th ACM SIGKDD International Conference on Knowledge Dis-
covery & Data Mining, pp. 1748–1757 (2020)

458
Z. Lv et al.
9. Lee, H., Im, J., Jang, S., Cho, H., Chung, S.: MeLU: meta-learned user prefer-
ence estimator for cold-start recommendation. In: Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
1073–1082 (2019)
10. Li, M., et al.: Winner: weakly-supervised hierarchical decomposition and alignment
for spatio-temporal video grounding. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 23090–23099 (2023)
11. Li, M., et al.: End-to-end modeling via information tree for one-shot natural lan-
guage spatial video grounding. In: Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 8707–8717
(2022)
12. Lv, Z., et al.: Ideal: toward high-eﬃciency device-cloud collaborative and dynamic
recommendation system. arXiv preprint arXiv:2302.07335 (2023)
13. Lv, Z., et al.: Duet: a tuning-free device-cloud collaborative parameters generation
framework for eﬃcient device model generalization. In: Proceedings of the ACM
Web Conference 2023 (2023)
14. Mansilla, L., Echeveste, R., Milone, D.H., Ferrante, E.: Domain generalization via
gradient surgery. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 6630–6638 (2021)
15. McAuley, J.J., Targett, C., Shi, Q., Hengel, A.V.D.: Image-based recommenda-
tions on styles and substitutes. In: Proceedings of the 38th International ACM
SIGIR Conference on Research and Development in Information Retrieval, Santi-
ago, Chile, 9–13 August 2015 (2015)
16. Neuberg, L.G.: Causality: models, reasoning, and inference, by Judea Pearl, Cam-
bridge University Press, 2000. Econom. Theory 19(4), 675–685 (2003)
17. Ouyang, W., Wang, X., Zhang, C., Yang, X.: Factors in ﬁnetuning deep model for
object detection with long-tail distribution. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 864–873 (2016)
18. Pan, F., Li, S., Ao, X., Tang, P., He, Q.: Warm up cold-start advertisements:
improving CTR predictions via learning to learn id embeddings. In: Proceedings
of the 42nd International ACM SIGIR Conference on Research and Development
in Information Retrieval, pp. 695–704 (2019)
19. Pearl, J.: Causal diagrams for empirical research. Biometrika 82(4), 669–688 (1995)
20. Pearl, J.: Causality. Cambridge University Press, Cambridge (2009)
21. Tong, Y., et al.: Quantitatively measuring and contrastively exploring heterogene-
ity for domain generalization. In: Proceedings of the 29th ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining (2023)
22. Wang, Y.X., Ramanan, D., Hebert, M.: Learning to model the tail. In: Proceedings
of the 31st International Conference on Neural Information Processing Systems,
pp. 7032–7042 (2017)
23. Wang, Z., Tsvetkov, Y., Firat, O., Cao, Y.: Gradient vaccine: investigating and
improving multi-task optimization in massively multilingual models. arXiv preprint
arXiv:2010.05874 (2020)
24. Yin, J., Liu, C., Wang, W., Sun, J., Hoi, S.C.: Learning transferrable parameters
for long-tailed sequential user behavior modeling. In: Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
359–367 (2020)
25. Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., Finn, C.: Gradient surgery
for multi-task learning. arXiv preprint arXiv:2001.06782 (2020)

Fine-Tuning for Long-Tailed Sequential Recommendation
459
26. Zhang, S., Yao, D., Zhao, Z., Chua, T., Wu, F.: Causerec: counterfactual user
sequence synthesis for sequential recommendation. In: SIGIR 2021: The 44th Inter-
national ACM SIGIR Conference on Research and Development in Information
Retrieval, Virtual Event, Canada, 11–15 July 2021, pp. 367–377. ACM (2021)
27. Zhang, Y., Kang, B., Hooi, B., Yan, S., Feng, J.: Deep long-tailed learning: a survey.
arXiv preprint arXiv:2110.04596 (2021)
28. Zhang, Y., et al.: Online adaptive asymmetric active learning for budgeted imbal-
anced data. In: Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pp. 2768–2777 (2018)
29. Zhang, Z., Pﬁster, T.: Learning fast sample re-weighting without reward data. In:
Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
725–734 (2021)
30. Zhou, G., et al.: Deep interest network for click-through rate prediction. In: Pro-
ceedings of the 24th ACM SIGKDD International Conference on Knowledge Dis-
covery & Data Mining, pp. 1059–1068 (2018)
31. Zhu, D., et al.: Bridging the gap: neural collapse inspired prompt tuning for gen-
eralization under class imbalance. arXiv preprint arXiv:2306.15955 (2023)

Heterogeneous Link Prediction
via Mutual Information Maximization
Between Node Pairs
Yifan Lu1, Zehao Liu1, Mengzhou Gao2(B), and Pengfei Jiao2,3
1 Zhuoyue Honors College, Hangzhou Dianzi University, Hangzhou, China
{lyfcan,zehaoliu}@hdu.edu.cn
2 School of Cyberspace, Hangzhou Dianzi University, Hangzhou, China
{mzgao,pjiao}@hdu.edu.cn
3 Data Security Governance Zhejiang Engineering Research Center, Hangzhou, China
Abstract. Heterogeneous graphs (HGs), possessing various node and
edge types, are essential in capturing complex relationships in networks.
Link prediction on heterogeneous graphs has wide applications in real-
world. Although existing methods for learning representations of HGs
have made substantial progress in link prediction tasks, they primarily
focus on the heterogeneous attributes of nodes when capturing the het-
erogeneity of heterogeneous graphs, therefore, it performs poorly in main-
taining pairwise relationships in HG. To address this limitation, we pro-
pose a simple yet eﬀective model for link prediction on HGs via Mutual
Information Maximization between Node Pairs (MIMNP). We use an
Multi-Layer Perceptron as a node encoder to learn node embeddings and
maximizes the mutual information between node pairs. Our model eﬀec-
tively preserves the pairwise relationships between nodes, resulting in
enhanced link prediction performance. Extensive experiments conducted
on three real-world datasets consistently demonstrate that MIMNP out-
performs state-of-the-art baselines in link prediction.
Keywords: Heterogeneous Graphs · Mutual Information
Maximization · Link Prediction
1
Introduction
Heterogeneous graphs (HGs), also referred to as heterogeneous information net-
works (HINs), are extensively present in real-world scenarios, ranging from bib-
liographic networks [1], social networks [35] to recommendation systems [27].
Unlike homogeneous graphs, which contain only one type of node and edge,
heterogeneous graphs encompass multiple types of nodes and edges, eﬀectively
capturing complex and diverse relationships within a given network. In hetero-
geneous graphs, nodes of diverse types can signify diﬀerent entities or concepts,
such as literature, authors, themes, among others. Edges represent associations
between nodes, which may encompass various types of relationships like cita-
tion connections, author relationships, or thematic relationships. These diﬀerent
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 460–470, 2024.
https://doi.org/10.1007/978-981-99-8850-1_37

Heterogeneous Link Prediction via MIMNP
461
types of edges encapsulate the diverse interconnections among diﬀerent entities
within the heterogeneous graph. Performing link prediction on heterogeneous
graphs, with the aim of predicting whether a connection exists between a pair
of nodes, constitutes an important task. The outcomes of link prediction can
be leveraged for recommending relevant items [2], discovering new drugs [3], or
detecting risks [5].
Many link prediction methods on HGs are based on heterogeneous represen-
tation learning [6–8], that is, they learn representations of HGs and determine
the likelihood of link existence based on the similarity between nodes. Methods
based on random walks [9,10] typically deﬁne a metapath and perform random
walks based on the metapath to generate node sequences. These sequences are
then used to learn node representations. Heterogeneous Graph Neural Networks
(HGNNs) like R-GCN [11], HeteHG-VAE [23], HGT [12] amalgamate message-
passing on HGs [34,35] with deep neural networks [33,36] to study node embed-
dings, and they have demonstrated outstanding performance in link prediction
tasks. However, many representation learning methods may not be suitable for
the task of link prediction. This is because these methods, in their attempt to
capture heterogeneity in HGs, mainly focus on the heterogeneous attributes of
nodes, thereby overlooking the importance of graph structure. For link predic-
tion, similarity in graph structure is crucial, particularly the similarity between
pairs of nodes, which indicates the probability of link existence. There have been
relevant explorations of structural information learning in homogeneous graphs.
Heuristic methods [14–17] represent a category of algorithms that are relatively
simplistic, but nonetheless eﬀective, in determining predeﬁned graph structural
data as nodal similarity indices, thereby establishing the probability of executing
link prediction. Despite the acknowledged eﬃcacy and impressive interpretabil-
ity of heuristic methods, they frequently possess robust presuppositions about
graph structures, which consequently limits their applicability across a broader
spectrum of contexts [17,18]. SEAL [18] framework provides both a theoreti-
cal justiﬁcation and empirical validation for the eﬀectiveness of acquiring graph
structural information by employing graph neural networks. N2N [19] learns
node representations by maximizing the mutual information from nodes to their
neighborhoods, thereby aggregating information from neighbors. However, as it
is designed for node classiﬁcation tasks, directly aggregating higher-order neigh-
bor information for link prediction tasks can lead to link loss or confusion, thus
reducing the eﬀectiveness of link prediction. Moreover, these methods can only
address homogeneous graphs and are unable to capture the rich semantics in
HGs.
In this paper, we propose a simple yet eﬀective model for link prediction on
HGs via Mutual Information Maximization between Node Pairs, named MIMNP.
We use an Multi-Layer Perceptron (MLP) as a node encoder to learn node
embeddings and maximize the mutual information between nodes and their
neighboring nodes. Then, we optimize the node embeddings speciﬁcally for link
prediction, thereby further enhancing the performance of link prediction. By
maximizing the mutual information between node pairs, we can maintain the

462
Y. Lu et al.
pairwise structure of the graph, which is a key aspect in link prediction. Our
model can achieve competitive results in link prediction on HGs without any
special design. Our contributions are as follows:
– We introduce mutual information into heterogeneous graph learning, main-
taining pairwise relationships of nodes by maximizing the mutual information
between node pairs.
– We propose a simple yet eﬀective model named MIMNP. It only uses MLP as
node encoder and can handle link prediction in heterogeneous graphs without
the need for speciﬁc heterogeneity design.
– We conduct extensive experiments on three heterogeneous graph datasets to
validate our proposed model. Our model consistently outperforms state-of-
the-art baselines in link prediction.
2
Related Work
In this section, we brieﬂy review related work on Heterogeneous Graph Embed-
ding and Link Prediction.
2.1
Heterogeneous Graph Embedding
The objective of heterogeneous graph embedding is to transpose nodes sourced
from a HG into a corresponding vector space which has low dimension. Random
walk-based techniques employ meta-path-based random walks to amalgamate
structural and semantic information, as exempliﬁed by methods like Metap-
ath2vec [10], HIN2vec [9], and HeteEdgeWalk [13]. As Graph Neural Networks
(GNNs) have advanced, numerous studies have employed them in the realm
of HG embedding learning, such as HetGNN [8], HAN [6], MAGNN [7] and
HDGI [20]. In the encoder-decoder-based technique, a neural network serves as
the encoder to process node attributes, while a decoder is formulated to preserve
the attributes of HGs, such as TaPEm [21], SHNE [22] and HeteHG-VAE [23].
However, all these methods lack attention to the pairwise relationships between
nodes, thereby limiting the expressive capability of the model.
2.2
Link Prediction
Link prediction has garnered substantial scholarly interest, with an Multitude
of methods being proposed by researchers in the ﬁeld. Heuristic methods rely on
structural information concerning pairs of nodes, including metrics like short-
est path, degree, common neighbors, and more. Relevant works include Com-
mon Neighbors [14], Adamic-Adar [16], and Resource Allocation [15]. With the
progression of Graph Neural Networks (GNNs), certain models have integrated
them into link prediction tasks. For instance, GAE [24] and VGAE [25] employ
autoencoders to learn node embeddings, leveraging GCN [26] as a foundation.
SEAL [18] takes a diﬀerent approach by extracting closed subgraphs around

Heterogeneous Link Prediction via MIMNP
463
target links and aggregating node information to enhance link prediction per-
formance. In the context of link prediction in HGs, the focus primarily revolves
around heterogeneous representation learning. Several HG-based methods have
been developed using GNNs, including R-GCN [11], HGT [12], and HERec [27].
However, these methods exhibit a deﬁciency in utilizing graph structural infor-
mation, which subsequently impacts link prediction performance.
3
Preliminaries
Heterogeneous Graph. A HG is formally deﬁned as G = (V, E), where it com-
prises multiple types of nodes and edges. Here, V represents the set of nodes, and
E represents the set of edges. Each edge, denoted as eij = (vi, vj) ∈E, signiﬁes
a connection between node vi and node vj. Additionally, a HG is equipped with
two mapping functions, φ(v) : V →A for nodes and ϕ(e) : E →R for edges.
Here, A and R respectively denote the sets of node types and edge types, with
the constraint that |A| + |R| > 2 to account for the heterogeneity in the graph.
The adjacency matrix, denoted as A, is deﬁned as follows: Aij = 1 if the edge
eij ∈E. The node features is deﬁned as X.
4
Methodology
Our model aims to learn node embeddings based on mutual information max-
imization between node pairs in a heterogeneous graph and apply it for link
prediction. The overall framework is shown in Fig. 1.
Fig. 1. The overall framework of our MIMNP. MIMNP ﬁrst inputs the features of
the nodes into an MLP, optimizing the model via mutual information maximization
between node pairs during this process. Then, the learned node embeddings go through
a similarity model to obtain the likelihood of links. In particular, rows of diﬀerent colors
in X represent diﬀerent types of nodes.
MIMNP ﬁrst inputs the node features X into an MLP, which is as follows:
H = f(W · X + b)
(1)

464
Y. Lu et al.
where H is the learned node embeddings. f(·) is a non-linear activation function
such as ReLU(x) = max(0, x) or Tanh(x) =
2
1+e−2x −1. W and b are the
learned weight and bias. Each row of H represents the representation of each
node, we use hk to represent the embedding of the k−th node.
Then given a link (i, j), we calculate the similarity score ˆyij between the node
pair based on their embeddings:
ˆyij = Sim(hi, hj)
(2)
where the function Sim(·) is responsible for computing similarity scores, such as
metrics like Euclidean distance and cosine similarity.
During the learning process of node embeddings, we optimize them by max-
imizing the mutual information between node pairs. N2N [19] introduces Proba-
bility Density Function into graph representation learning to obtain the mutual
information between a node and its neighborhood through the node mapping
function H(·) from x to hi and the neighbor mapping function S(·) from x to si.
The mutual information between the node representations and their respective
neighborhood representations is deﬁned as follows:
I(S(x); H(x)) =

D
p(S(x), H(x)) · log
p(S(x), H(x))
p(S(x)) · p(H(x))dx
(3)
where D is the feature space. Then, to calculate mutual information in a high-
dimensional and continuous space, MINE [28] convets converts mutual informa-
tion maximization into minimizing the InfoNCE loss [29]:
LInfoNCE = −Evi∈V(log
exp(Sim(si, hi))/τ

vk∈V exp(Sim(hk, hi))/τ )
(4)
In our paper, we extend the loss function to node pairs:
L1 = −Evi∈V(log
exp(Sim(hj, hi))/τ

vk∈V exp(Sim(hk, hi))/τ )
(5)
where the exp(·) function donates the exponential function, and τ is the tem-
perature parameter. (hj, hi) is the positive pair and (hk, hi)i̸=k is the negative
pair. Wherein, node j is the ﬁrst-order neighbor of node i.
To further enhance the eﬃciency of the model in link prediction, we introduce
a binary cross-entropy loss to optimize the model, and the loss function is as
follows:
L2 =

(i,j)∈N
BCE(ˆyij, yij)
(6)
where y represents the existence label of edges derived from the adjacency matrix
A of the heterogeneous graph. For a pair of nodes (vi, vj), if eij = 1, then yij = 1,
otherwise, yij = 0.
Finally, the loss function in our proposed MIMNP can be represented as
follows:
L = L1 + L2
(7)

Heterogeneous Link Prediction via MIMNP
465
Table 1. Statistics of Datasets.
Dataset Node
Edge
V1
V2
V3
V
E1
E2
E
DBLP
P
A
V
18,405 P-A
P-V
33,973
14,328 4,057 20
19,645 14,328
IMDB
M
A
D
11,616 M-A
M-D
17,106
4,278
5,257 2,081
12,828 4,278
ACM
P
A
S
11,800 P-A
P-S
17,426
4,057
7,723 20
4,019
13,407
5
Experiments
5.1
Experiment Settings
Datasets. We evaluate the performance of MIMNP on three real-world datasets:
DBLP, IMDB, and ACM. The details of the datasets are provided in Table 1.
– DBLP is a computer science bibliography network consisting of three distinct
node types: paper (P), author (A), and venue (V). The authors within this
network belong to four research domains, including Data Mining, Information
Retrieval, Database, and Artiﬁcial Intelligence.
– IMDB is a network related to movies, which depicts user evaluations of movies
and directors. It comprises three types of nodes: movie (M), actor (A), and
director (D). In this network, movies are categorized into three genres: Action,
Comedy, and Drama.
– ACM is a bibliographic network that encompasses papers published at con-
ferences like KDD, SIGMOD, SIGCOMM, MobiCOMM, and VLDB. Within
the network, there are three types of nodes: paper (P), author (A), and sub-
ject (S). The papers in this network are divided into three categories: Wireless
Communication, Data Mining, and Database.
Baselines. We compare our MIMNP with eight baselines, including four meth-
ods for homogeneous graphs, and four methods for heterogeneous graphs:
– Methods for homogeneous graphs: GraphSAGE [30], node2vec [31], GAE [24],
and DGI [32].
– Methods for heterogeneous graphs: Metapath2vec [10], R-GCN [11], HGT [12]
and HeteHG-VAE [23].
Implementation Details. In our paper, we utilized the Adam algorithm for
optimizing our model, with a learning rate set to 0.001. The relevant parameter
settings in the paper are as follows: the temperature parameter τ is set to 5,
and the ﬁnal embedding dimension is 128. During the model training process,
we randomly removed 20% to 80% of the edges as a test set, using the remaining

466
Y. Lu et al.
edges for training. Additionally, we allocated 5% of the edges as a validation set
to evaluate the model’s performance. We employed the Tanh function as the non-
linear activation function and used cosine similarity as the similarity metric. The
meta-path conﬁgurations in our research are as follows: In the DBLP dataset, we
employed APA and APTPA meta-paths. In the IMDB dataset, we utilized MAM
and MDM meta-paths. In the ACM dataset, we used PAP and PSP meta-paths.
For each diﬀerent method, we conducted multiple experiments and selected the
best-performing meta-path for that method. In this paper, we evaluate the link
prediction performance of various models using standard evaluation metrics,
including the AUC score (Area Under the Receiver Operating Characteristic
Curve) and the AP score (Average Precision).
Table 2. Link prediction performances (%) of our MIMNP and baselines on three
real-world datasets. Bold indicates the best performance.
Datasets Metrics Train SAGE nd2vec GAE DGI
Mp2vec R-GCN HGT Hete-VAE MIMNP
DBLP
AUC
20%
50.33
63.11
57.31 71.42 61.39
76.48
69.15 81.32
84.31
40%
52.98
65.65
58.48 84.58 65.32
80.33
75.16 82.71
85.31
60%
54.45
68.35
61.04 85.30 71.41
82.54
82.66 83.15
86.21
80%
58.65
76.75
68.51 87.73 73.65
82.98
84.21 83.46
87.88
AP
20%
50.61
61.27
59.77 73.65 60.81
76.65
70.36 82.37
85.59
40%
50.91
64.82
61.89 86.04 64.74
81.33
76.84 83.56
86.73
60%
52.69
64.29
65.02 87.01 70.69
82.93
81.63 84.26
89.67
80%
55.95
71.19
72.14 89.61 71.51
83.52
85.93 85.39
90.22
IMDB
AUC
20%
55.22
53.77
56.38 73.80 52.77
64.02
55.36 54.21
75.57
40%
58.31
59.26
60.21 75.20 51.94
64.31
55.81 60.33
77.15
60%
60.09
63.82
65.52 77.03 53.84
65.27
57.32 54.53
78.13
80%
69.01
69.21
74.29 76.56 53.28
65.89
56.19 61.32
80.51
AP
20%
52.07
51.90
54.12 73.59 54.03
67.69
59.97 55.18
74.38
40%
53.27
55.07
57.31 76.31 53.71
67.83
56.47 61.46
74.96
60%
55.98
57.45
62.35 76.61 61.39
68.51
61.17 57.73
76.89
80%
64.92
60.01
71.96 76.11 62.98
68.92
57.6
62.50
78.75
ACM
AUC
20%
61.52
55.49
64.29 74.20 51.56
60.27
53.59 65.54
78.31
40%
63.17
57.34
66.87 74.33 51.90
61.39
52.22 69.98
78.67
60%
65.30
58.69
67.35 74.92 53.24
61.94
54.93 66.72
80.65
80%
67.34
63.14
68.79 74.94 52.47
61.73
52.59 70.39
82.18
AP
20%
65.77
50.69
65.21 76.31 61.33
63.13
58.61 67.03
75.28
40%
67.59
51.23
66.24 77.06 66.67
63.63
59.76 71.95
75.43
60%
69.09
53.85
67.63 77.39 66.27
63.94
63.12 67.11
82.45
80%
69.88
56.02
68.95 77.79 71.62
66.4
64.59 72.19
83.55
5.2
Results on Link Prediction
Table 2 shows link prediction performances (%) of our MIMNP and baselines on
three real-world datasets. We can observe that MIMNP consistently achieve
state-of-the-arts performance across all datasets. Compared to all baselines,

Heterogeneous Link Prediction via MIMNP
467
MIMNP improved the AUC scores by 3% to 27% and the AP scores by 3% to
21% on the three datasets. Especially, MIMNP show signiﬁcant improvements
on IMDB and ACM, where the improvements of MIMNP over the best baseline
are 27% (AUC) and 12% (AUC), respectively. We noticed that DGI outperforms
other baselines in link prediction, which is because DGI can eﬀectively learn the
structural information of the graph through the contrastive learning method
based on mutual information. This further demonstrates the eﬀectiveness of our
designed method based on maximizing mutual information between node pairs.
We can see that when only considering features, the performance of the meth-
ods for HGs is better than that of the methods for homogeneous graphs. This is
because the methods for HGs can better learn the information of nodes and use
it for link prediction.
5.3
Ablation Studies
We present ablation experiments to identify the beneﬁcial components of
MIMNP. We evaluate our MIMNP without the specially designed module for
maximizing mutual information between nodes. Diﬀerent from choosing neigh-
bors with maximum mutual information, in the ablation study, we directly ran-
domly select neighbor nodes adjacent to the node as positive samples. Table 3
demonstrates the eﬀectiveness of the design of maximizing mutual information
between node pairs.
5.4
Parameters Experiments
In this section, we explore the impact of varying the dimension of the node
embedding H, and the results are illustrated in Fig. 2. It is evident that as the
embedding dimension increases, performance initially improves before gradually
declining. This behavior can be attributed to the fact that MIMNP necessitates
an appropriate dimension to eﬀectively encode semantic information, and an
excessively large dimension may introduce redundancies.
Table 3. Ablation study analyzing the signiﬁcance of MIMNP on the DBLP, IMDB,
and ACM datasets for link prediction.
Dataset MIMNP (w/ MIM) MIMNP (w/o MIM) Improve
DBLP
87.88%
84.37%
4.16%
IMDB
80.51%
75.35%
6.85%
ACM
82.18%
75.21%
9.27%

468
Y. Lu et al.
Fig. 2. The performance of various datasets at diﬀerent embedding dimensions.
6
Conclusion
In this paper, we propose a simple yet eﬀective model for link prediction on HGs
via Mutual Information Maximization between Node Pairs, named MIMNP. We
use an Multi-Layer Perceptron (MLP) as a node encoder to learn node embed-
dings and maximize the mutual information between nodes and their neighbor-
ing nodes. Extensive experiments conducted on three real-world datasets consis-
tently demonstrate superior performance of our model. In future work, we intend
to investigate better ways of generating graph structural information to enable
the model to reach better performance. One promising application scene is the
recommendation system.
Acknowledgement. This work was supported in part by the Zhejiang Provincial
Natural Science Foundation of China under Grant LDT23F01015F01 and Grant
LDT23F01012F01, in part by the Fundamental Research Funds for the Provincial Uni-
versities of Zhejiang Grant GK229909299001-008 and in part by the National Natural
Science Foundation of China under Grant 62003120.
References
1. Tang, J., et al.: Arnetminer: extraction and mining of academic social networks. In:
Proceedings of the 14th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (2008)
2. El-Kishky, A., et al.: Twhin: embedding the twitter heterogeneous information net-
work for personalized recommendation. In: Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (2022)
3. Shao, K., Zhang, Y., Wen, Y., et al.: DTI-HETA: prediction of drug-target inter-
actions based on GCN and GAT on heterogeneous graph. Brief. Bioinform. 23(3),
bbac109 (2022)

Heterogeneous Link Prediction via MIMNP
469
4. Jin, D., et al.: A survey of community detection approaches: from statistical mod-
eling to deep learning. IEEE Trans. Knowl. Data Eng. 35(2), 1149–1170 (2021)
5. Tao, X., et al.: Mining health knowledge graph for health risk prediction. World
Wide Web 23, 2341–2362 (2020)
6. Wang, X., et al.: Heterogeneous graph attention network. In: The World Wide Web
Conference (2019)
7. Fu, X., et al.: MAGNN: metapath aggregated graph neural network for heteroge-
neous graph embedding. In: Proceedings of the Web Conference 2020 (2020)
8. Zhang, C., et al.: Heterogeneous graph neural network. In: Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining
(2019)
9. Fu, T.-Y., Lee, W.-C., Lei, Z.: Hin2vec: explore meta-paths in heterogeneous infor-
mation networks for representation learning. In: Proceedings of the 2017 ACM on
Conference on Information and Knowledge Management (2017)
10. Dong, Y., Chawla, N.V., Swami, A.: metapath2vec: scalable representation learning
for heterogeneous networks. In: Proceedings of the 23rd ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining (2017)
11. Schlichtkrull, M., Kipf, T.N., Bloem, P., van den Berg, R., Titov, I., Welling,
M.: Modeling relational data with graph convolutional networks. In: Gangemi, A.,
et al. (eds.) ESWC 2018. LNCS, vol. 10843, pp. 593–607. Springer, Cham (2018).
https://doi.org/10.1007/978-3-319-93417-4 38
12. Hu, Z., et al.: Heterogeneous graph transformer. In: Proceedings of the Web Con-
ference 2020 (2020)
13. Liu, Z., et al.: HeteEdgeWalk: a heterogeneous edge memory random walk for
heterogeneous information network embedding. Entropy 25(7), 998 (2023)
14. Barab´asi, A.-L., Albert, R.: Emergence of scaling in random networks. Science
286(5439), 509–512 (1999)
15. Zhou, T., L¨u, L., Zhang, Y.-C.: Predicting missing links via local information. Eur.
Phys. J. B 71, 623–630 (2009)
16. Adamic, L.A., Adar, E.: Friends and neighbors on the web. Soc. Netw. 25(3),
211–230 (2003)
17. Zhang, M., Chen, Y.: Weisfeiler-Lehman neural machine for link prediction. In:
Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (2017)
18. Zhang, M., Chen, Y.: Link prediction based on graph neural networks. In: Advances
in Neural Information Processing Systems, vol. 31 (2018)
19. Dong, W., et al.: Node representation learning in graph via node-to-neighbourhood
mutual information maximization. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (2022)
20. Ren, Y., Liu, B.: Heterogeneous deep graph infomax. In: Workshop of Deep Learn-
ing on Graphs: Methodologies and Applications co-located with the Thirty-Fourth
AAAI Conference on Artiﬁcial Intelligence (2020)
21. Park, C., et al.: Task-guided pair embedding in heterogeneous network. In: Pro-
ceedings of the 28th ACM International Conference on Information and Knowledge
Management (2019)
22. Zhang, C., Swami, A., Chawla, N.V.: SHNE: representation learning for semantic-
associated heterogeneous networks. In: Proceedings of the Twelfth ACM Interna-
tional Conference on Web Search and Data Mining (2019)
23. Fan, H., et al.: Heterogeneous hypergraph variational autoencoder for link predic-
tion. IEEE Trans. Pattern Anal. Mach. Intell. 44(8), 4125–4138 (2021)

470
Y. Lu et al.
24. Schulman, J., et al.: High-Dimensional Continuous Control Using Generalized
Advantage Estimation. CoRR abs/1506.02438 (2015)
25. Kipf, T.N., Welling, M.: Variational graph auto-encoders. In: NIPS Workshop on
Bayesian Deep Learning (2016)
26. Kipf, T.N., Welling, M.: Semi-supervised classiﬁcation with graph convolutional
networks. In: International Conference on Learning Representations (2016)
27. Shi, C., et al.: Heterogeneous information network embedding for recommendation.
IEEE Trans. Knowl. Data Eng. 31(2), 357–370 (2018)
28. Belghazi, M.I., et al.: Mutual information neural estimation. In: International Con-
ference on Machine Learning. PMLR (2018)
29. van den Oord, A., Li, Y., Vinyals, O.: Representation learning with contrastive
predictive coding. arXiv preprint arXiv:1807.03748 (2018)
30. Hamilton, W., Ying, Z., Leskovec, J.: Inductive representation learning on large
graphs. In: Advances in Neural Information Processing Systems, vol. 30 (2017)
31. Grover, A., Leskovec, J.: node2vec: scalable feature learning for networks. In: Pro-
ceedings of the 22nd ACM SIGKDD international conference on Knowledge Dis-
covery and Data Mining (2016)
32. Veliˇckovi´c, P., et al.: Deep graph infomax. In: International Conference on Learning
Representations (2018)
33. Jiao, P., et al.: Role discovery-guided network embedding based on autoencoder
and attention mechanism. IEEE Trans. Cybern. 53(1), 365–378 (2021)
34. Gao, M., et al.: Inductive link prediction via interactive learning across relations
in multiplex networks. IEEE Trans. Comput. Soc. Syst. (2022)
35. Jiao, P., et al.: HB-DSBM: modeling the dynamic complex networks from commu-
nity level to node level. IEEE Trans. Neural Netw. Learn. Syst. (2022)
36. Jiao, P., et al.: A survey on role-oriented network embedding. IEEE Trans. Big
Data 84, 933–952 (2021)

Explainability, Understandability,
and Veriﬁability of AI

ADAPT: Action-Aware Driving Caption
Transformer
Bu Jin(B) and Haotian Liu
Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China
jinbu18@mails.ucas.ac.cn
Abstract. Beneﬁting from precise perception, real-time prediction and
reliable planning, autonomous driving systems have exhibited excep-
tional performance in research. However, the high complexity and opac-
ity prevent its application in practice. To introduce a user-friendly
autonomous driving system, we propose a driving captioner to generate
real time description and explanation of self-driving systems in natural
language. Speciﬁcally, we unify the end-to-end autonomous driving and
video captioning tasks into a single yet eﬀective framework by introduc-
ing an additional captioning head to describe the action of the vehicle
and explain the reasons. Besides, we exploit an eﬀective accelerating
method to accelerate the inference process, which decreases the average
inference time from 0.670 s to 0.298 s. Through extensive experiments on
both simulation datasets and real-world datasets, we show the superior
generalization ability and robustness of the proposed framework.
Keywords: Autonomous driving · Driving caption · Interpretability
1
Introduction
Autonomous driving has achieved signiﬁcant success in research and devel-
opment over recent years. Most state-of-the-art methods [4] decompose the
autonomous driving task into a series of sub-tasks, including perception, pre-
diction and planning. Despite their superior performance, the multifarious and
intricate designing makes it too complicated for common passengers to under-
stand, for whom the safety of such vehicles and their controllability is the top
priority. How to explain the behavior of self-driving vehicles to passengers has
been an increasingly important issue considering its industrial adoption in prac-
tice.
Currently, the interpretability of autonomous driving mainly focus on visual
interpretation, like attention map [4] or cost volume [8]. Although such vision-
based approaches provide promising results, the lack of linguistic interpretation
makes them too complicated for passengers like the elderly to understand.
To bridge the gap, we propose an end-to-end driving caption framework
named ADAPT (Action-aware Driving cAPtion Transformer), which uniﬁes the
Supported by Institute for AI Industry Research (AIR), Tsinghua University.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 473–477, 2024.
https://doi.org/10.1007/978-981-99-8850-1_38

474
B. Jin and H. Liu
two tasks of autonomous driving and textual generation. It empowers the model
with the ability to both accurately predict the control signal of driving vehi-
cles and provide sound descriptions and explanations of the action in natural
language.
Speciﬁcally, ADAPT is a multi-task learning framework, which jointly trains
the driving captioning task and control signal predicting task with a shared
video representation to eliminate the discrepancy between two tasks. We argues
that the cues from the other task help regularize the deviation of single task
and improve the ﬁnal performance of both. We demonstrate the robustness of
ADAPT with extensive experiments on diverse domains, like diﬀerent dataset
styles, various weather et al.
The contributions are summarized as follows:
• We propose ADAPT, the ﬁrst end-to-end action-aware driving caption trans-
former, which can provide linguistic interpretation for autonomous driving
vehicles.
• We propose to unify the two pipelines of driving captioning and control signal
predicting, which is beneﬁcial to both tasks.
• Through extensive experiments on diverse driving scenarios, we demonstrate
the eﬀectiveness and robustness of ADAPT. With a fast inference time,
ADAPT is feasible for applications in real-time autonomous driving system.
2
Method
2.1
Overview
As illustrated in Fig. 1, ADAPT comprises two distinct tasks: driving caption
generation and control signal prediction. Speciﬁcally, a sequence of front-view
frames are fed into the shared visual encoder. In driving caption generation
task, the video features are passed as input to the language decoder, resulting in
two natural language sentences: one describes the vehicle’s action and another
provides the underlying reasoning. The Control Signal Prediction task takes the
same video features as input and produces a sequence of control signals, such as
speed, course, or acceleration.
2.2
Model Design
Video Encoder. We employ Video Swin Transformer (Vidswin) [6] as our visual
feature extractor. Speciﬁcally, for a front-view vehicular video, we perform uni-
form sampling to obtain T frames with size of H ×W ×3. These frames serve as
inputs to the Vidswin, resulting in video features FV with size of T
2 × H
32 × W
32 ×8C.
Note that C represents the channel dimension deﬁned in Vidswin. The result-
ing video features are then fed into diﬀerent prediction heads for the respective
tasks.

ADAPT: Action-Aware Driving Caption Transformer
475
Video
Video Swin
Transformer
T  frames
Autoregression
Contronl Signal:{S2,S3, ...,ST-1,ST}
speed
course
acceleraon
...
Narraon:  the car is merging into 
the lane to its le
Reasoning:  because there is a gap 
in traﬃc.
Output
Prediction Head
...
Fig. 1. Overview of the model framework. Input is a vehicle-front-view video, and out-
puts are predictions for the vehicle’s control signals as well as description and expla-
nation for the current action.
Text Generation Head. Text generation head aims to produce two sentences
that describe the vehicle’s action and the rationale behind it. With previous
extracted video features, the video tokens are created by tokenizing along the
channel dimension. Then the video tokens are utilized in a language decoder to
generate words autoregressively.
Control Signal Prediction Head. The target of control signal prediction head
is to predict the control signals based on the video frames. Given the previous
extracted features, the prediction head generates a sequence of predicted control
signals ˆS = { ˆs2, ..., ˆ
sT }. Note that each control signal si or ˆsi is an n-tuple,
where n represents the number of sensor types utilized. The loss function in this
process is deﬁned as below:
LCSP =
1
T −1
T

i=2
(si −ˆsi)2
(1)
Joint Training. In our work, we argue that the driving caption generation
and control signal prediction are aligned on the semantic level of the video rep-
resentation, and jointly training these tasks in a single network can improve
performance by leveraging the inductive biases between diﬀerent tasks. Thus
during training, the two tasks are jointly performed, and the ﬁnal loss function
is obtained by adding the individual task losses, as follows:
L = LDCG + LCSP
(2)
3
Experiment
3.1
Implementation Details
Datasets. We conduct experiments on 3 autonomous driving datasets, detailed
below.

476
B. Jin and H. Liu
BDDX. BDD-X [5] is a large dataset designed for driving-related captioning
tasks, consisting of approximately 7000 videos paired with control signals. The
videos, sourced from the BDD100K dataset [7], capture various vehicle behaviors
and are annotated with textual explanations.
NUSCENES. NUSCENES [2] dataset is a collection of 1,000 scenes from diﬀer-
ent cities worldwide, featuring diverse road types, traﬃc conditions, and weather
conditions. Each scene provides high-resolution 360-degree images, radar data,
and precise vehicle positions and motion trajectories.
CARLA. CARLA [3] is an open-source simulator that oﬀers a highly customiz-
able virtual city environment for researchers and developers in autonomous driv-
ing, with a dataset providing detailed vehicle behavior information and simula-
tion of diverse driving scenarios.
Speciﬁcally, We conduct experiments on videos that encompass multifarious
scenarios, including various driving behaviors (e.g., stopping, turning, or merg-
ing), weather conditions (e.g., sunny, rainy, or snowy) or diﬀerent time of day
(e.g., daytime, dusk or nighttime).
3.2
Main Results
We show some qualitative results on three datasets (detailed in Demo). For
BDD-X, our model’s performance is impressive across various scenarios, includ-
ing left/right turn, traﬃc light (red/green), stop sign, lane merging, pedestrian,
and so on. Regardless of diﬀerent weather conditions (sunny or rainy), the pre-
dictions of our model are aligned with the ground truth labels. Moreover, even
in dark environment, our model can still produce sound predictions, indicating
its eﬀectiveness and robustness.
Furthermore, our model demonstrates excellent zero-shot performance on
unseen datasets, like NUSCENES or CARLA. As is shown in our Demo, the
selected NUSCENES data consists of videos captured in both sunny and rainy
conditions, and the CARLA dataset includes videos from daytime and night-
time scenarios. The generated descriptions and explanations for these videos are
essentially reasonable, highlighting the strong robustness of our model.
3.3
Accelerate the Inference Process
We utilize the Open Neural Network Exchange (ONNX) [1] to accelerate the
inference process of our model. ONNX is an open format for deep learning mod-
els that allows model conversion between frames and enables inference on various
platforms. We convert our model into the ONNX format to accelerate the infer-
ence process. The results show that the accelerated model decreases the average
inference time from 0.670 s to 0.298 s, which suggests the possible applications
of ADAPT on real-time autonomous driving system.

ADAPT: Action-Aware Driving Caption Transformer
477
4
Conclusion
Linguistic interpretability is essential for the social acceptance of self-driving
vehicles. We present ADAPT (Action-aware Driving cAPtion Transformer), a
new end-to-end transformer-based framework for generating action description
and explanation for self-driving vehicles. Through extensive experiments on
diverse datasets, we show the superior generalization ability and robustness of
the proposed framework. Eﬀective accelerating methods enable the deployment
of ADAPT in industrial practice.
References
1. Bai, J., Lu, F., Zhang, K., et al.: ONNX: open neural network exchange. GitHub
repository, p. 54 (2019)
2. Caesar, H., et al.: nuScenes: a multimodal dataset for autonomous driving. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 11621–11631 (2020)
3. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: CARLA: an open
urban driving simulator. In: Conference on Robot Learning, pp. 1–16. PMLR (2017)
4. Kim, J., Canny, J.: Interpretable learning for self-driving cars by visualizing causal
attention. In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 2942–2950 (2017)
5. Kim, J., Rohrbach, A., Darrell, T., Canny, J., Akata, Z.: Textual explanations
for self-driving vehicles. In: Proceedings of the European Conference on Computer
Vision (ECCV), pp. 563–578 (2018)
6. Liu, Z., et al.: Video swin transformer. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 3202–3211 (2022)
7. Yu, F., et al.: BDD100K: a diverse driving dataset for heterogeneous multitask
learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 2636–2645 (2020)
8. Zeng, W., et al.: End-to-end interpretable neural motion planner. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
8660–8669 (2019)

Structural Recognition of Handwritten
Chinese Characters Using a Modiﬁed
Part Capsule Auto-encoder
Xin-Jian Wu1,2, Xiang Ao1,2, Rui-Song Zhang1,2, and Cheng-Lin Liu1,2(B)
1 State Key Laboratory of Multimodal Artiﬁcial Intelligence Systems,
Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China
2 School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences,
Beijing 100049, China
{wuxinjian2020,aoxiang2017,zhangruisong2019}@ia.ac.cn,
liucl@nlpr.ia.ac.cn
Abstract. Handwritten Chinese character recognition has achieved high
accuracy using deep neural networks (DNNs), but the structural recogni-
tion (which oﬀers structural interpretation, e.g., stroke and radical compo-
sition) is still a challenge. Existing DNNs treat character image as a whole
and perform classiﬁcation end-to-end without perception of the structure.
They need a large amount of training samples to guarantee high general-
ization accuracy. In this paper, we propose a method for structural recog-
nition of handwritten Chinese characters based on a modiﬁed part capsule
auto-encoder (PCAE), which explicitly considers the hierarchical part-
whole relationship of characters, and leverages extracted structural infor-
mation for character recognition. Our PCAE is improved based on stacked
capsule auto-encoder (SCAE) so as to better extract strokes and perform
classiﬁcation. By the modiﬁed PCAE, the character image is ﬁrstly decom-
posed into primitives (stroke segments), with their shape and pose infor-
mation decoupled. The transformed primitives are aggregated into higher-
level parts (strokes) guided by prior knowledge extracted from writing
rules. This process enhances interpretability and improves the discrimi-
nation ability of features. Experimental results on a large dataset demon-
strate the eﬀectiveness of our method in both Chinese character recogni-
tion and stroke extraction tasks.
Keywords: Handwritten Chinese character recognition · Structural
recognition · Part-whole hierarchical relationship · Self-supervised
learning · Modiﬁed part capsule auto-encoder
1
Introduction
Handwritten Chinese character recognition (HCCR) has been extensively stud-
ied in the ﬁeld of artiﬁcial intelligence for many years and plays an essential
This work has been supported by the National Key Research and Development Program
under Grant No. 2018AAA0100400, the National Natural Science Foundation of China
(NSFC) grants 61836014 and U20A20223.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 478–490, 2024.
https://doi.org/10.1007/978-981-99-8850-1_39

Structural Recognition of HCC Using a Modiﬁed PCAE
479
Images
Learned Part Templates
Fig. 1. The primitives discovered by SCAE [10] on digits and Chinese characters.
Even though SCAE can capture primitives on simple digits, it tends to fail on more
complicated objects like Chinese characters, learning holistic representations.
role in many applications. With over thousands of commonly used characters
in the Chinese language, the ability to recognize and understand these charac-
ters is crucial for communication, education, business, and other areas of daily
life. Traditional methods [3,12,15] on HCCR mainly rely on hand-crafted fea-
tures and their performance is limited. With the rapid development of deep
learning, the performance of HCCR has been promoted largely by using convo-
lutional neural networks (CNNs) for end-to-end feature extraction and classiﬁ-
cation [4,11,24,25]. However, these end-to-end methods, based on feature vector
classiﬁcation, yield predictions with low interpretability, and with the inability
to understand the character structure (stroke and radical composition).
Studies in cognitive psychology [1,4,14] reveal that humans have a natu-
ral ability to parse objects into their hierarchical component parts and con-
sider their spatial relationships. Building on these ﬁndings, computer vision
researchers have explored methods for HCCR based on their radicals [16,17] and
strokes [9,15]. However, both radical-based and stroke-based methods rely on
massive manual annotations, and the performance of stroke and radical extrac-
tion is still not satisfactory. To further enhance the hierarchical structure under-
standing of visual objects and align it with human intelligence, a promising
approach is to learn parsing directly from data. The model is learned to discover
visual part concepts with minimal human intervention while maintaining seman-
tic consistency across diﬀerent samples. Nonetheless, learning semantic parts in
an unsupervised manner remains a challenging problem.
Capsule networks (CapsNets) were proposed as a promising alternative
method for part detection and structural interpretability. CapsNets are speciﬁ-
cally designed to achieve translation equivariance and model the part-whole hier-
archical relationships of objects, making it feasible for our parsing tasks. Among
the CapsNet variants proposed in recent years [6,7,10,13], the stacked capsule
auto-encoder (SCAE) [10] is particularly well-suited for part-whole analysis, as it
explicitly utilizes geometric relationships between parts to reason about objects in
a self-supervised manner. However, SCAE is limited to learning a small number
of simple objects such as handwritten digits. When applied to large-scale Chi-
nese character datasets, SCAE fails to capture character primitives, generating

480
X.-J. Wu et al.
pseudo-holistic representations in the part capsules, as shown in Fig. 1. And thus,
the performance of character recognition will collapse in such a scenario.
Fig. 2. Illustration of the proposed method. The model understands characters with
two stages. The ﬁrst stage learns the feature extractor and a set of primitives (stroke
segments) in a self-supervised manner. The second stage performs post-processing to
aggregate strokes and classiﬁcation using a linear classiﬁer.
In this paper, we ﬁnd that using only the part capsule auto-encoder (PCAE)
in SCAE for character primitive extraction results in better convergence. Moti-
vated by this observation, to enhance the ability of primitive extraction, we
enforce some modiﬁcations to the original PCAE: adjusting the backbone net-
work architecture, constraining speciﬁc pose parameters, and eliminating redun-
dant modules. To address the issue of lacking stroke-level labels, the PCAE is
learned in a self-supervised manner: a set of primitives (stroke segment tem-
plates) shared across the entire Chinese character dataset are learned based on
a reconstruction task. Based on primitive extraction, a post-processing merging
algorithm is designed to aggregate transformed stroke segments and generate
more complete strokes. The whole framework of the proposed modiﬁed PCAE-
based method is shown in Fig. 2. Our approach has the potential to signiﬁcantly
improve stroke-based Chinese character recognition accuracy, particularly in sce-
narios where labeled data is limited.
The contributions of this work can be summarized as follows:
• We propose a method based on modiﬁed PCAE for stroke-based Chinese
character recognition, which extends the application of CapsNets to large-
scale Chinese character recognition problems.

Structural Recognition of HCC Using a Modiﬁed PCAE
481
• In our method, stroke segments are learned in a self-supervised manner by
the modiﬁed PCAE, and a post-processing algorithm is designed to generate
more complete strokes during the inference stage.
• Experimental results on a large dataset show that our method improves the
interpretability of handwritten Chinese character recognition and yields com-
petitive accuracy, particularly in scenarios where labeled data is limited.
2
Related Work
Chinese Character Recognition. HCCR has been studied since the 1990s
intensively. Existing works can be divided into three groups: character based,
radical based, and stroke based, corresponding semantic levels from high to low
respectively. Character based approaches use hand-crafted features like Gabor
features [15], directional features [8], vector features [2], and use diﬀerent clas-
siﬁers such as modiﬁed quadratic discriminate function (MQDF) [12]. Recent
methods use DNNs such as CNNs to perform end-to-end feature extraction and
classiﬁcation, and have achieved high accuracies by training with large amounts
of samples. ART-CNN [18] trains a relaxation CNN through alternative train-
ing and achieved ﬁrst place in ICDAR2013 competition [20]. However, these
methods overlook the structural information of characters and rely on mas-
sive training data. Traditional radical based approaches relied on hand-crafted
rules for radical segmentation, while recent methods also resort to deep learn-
ing techniques. DenseRAN [16] treats the recognition task as image captioning
by considering each character as a sequence of radicals. Based on DenseRAN,
STN-DenseRAN [17] employs a rectiﬁcation block for distorted character images.
However, these methods still rely on radical deﬁnitions given by humans. Stroke
based approaches usually rely on rule-based stroke extraction. In [9], the authors
proposed a stroke-guided pixel matching method that can tolerate errors caused
by stroke extraction. The stroke extraction method of [15] is based on a direc-
tional ﬁltering technique. These rule-based methods are hard to extract complex
strokes and achieve competitive recognition accuracy. Our method uses a PCAE
to learn stroke primitives in an unsupervised manner and achieves signiﬁcant
improvement.
Capsule Networks. CapsNet [13] was ﬁrst proposed to parse object parts with
dynamic parse trees, inspired by the perception functions of the human brain. As
a promising alternative model for improving the robustness and interpretability
of CNNs, CapsNet is speciﬁcally designed to focus on translation equivariance
and model the part-whole hierarchical relationships of objects. Architecturally
diﬀerent from CNNs, CapsNet replaces scalar neurons and pooling operations in
CNNs with vector capsules and routing mechanisms, respectively. Over the past
few years, a series of variants have been proposed to reﬁne initial CapsNet [13].
EM-Routing CapsNet [7] modiﬁes the capsule representation and proposes an
improved routing mechanism based on the Expectation-Maximization algorithm.
Self-routing CapsNet [6] further improves the performance by combining the

482
X.-J. Wu et al.
Mixture-of-Experts. The recent stacked capsule auto-encoder (SCAE) [10] was
introduced as an unsupervised CapsNet that explicitly analyzes the hierarchical
structure of objects. Although recent works have demonstrated the eﬀective-
ness of CapsNets in a various tasks [5,19,21–23], they were only evaluated on
small-scale datasets, containing a small number of object categories. As the num-
ber of categories increases, CapsNets may suﬀer from catastrophic performance
degradation. In this paper, we explore the application of CapsNets to large-scale
Chinese character datasets.
Images
Learned Part Templates
(a) The partial primitives templates discovered by SCAE
(c) The partial primitives templates discovered by modified PCAE
(b) The partial primitives templates discovered by PCAE
Fig. 3. The primitives discovered on Chinese characters by SCAE, PCAE, and modiﬁed
PCAE respectively. The modiﬁed PCAE shows better performance in learning clear
primitives.
3
Methodology
3.1
Overall Framework
The proposed method uses a PCAE based model to extract stroke segments
from character images, aggregate the segments to complete strokes and extract
features for classiﬁcation. The PCAE model is learned in an unsupervised man-
ner from a collection of characters images from a large dataset without stroke-
level labels. The model contains three modules, i.e., primitive extraction mod-
ule, stroke aggregation module, and character recognition module. The overall
framework is shown in Fig. 2. In training, the PCAE is trained to learn a set of
fundamental stroke segments (primitives). In testing, based on the transformed
primitives, the complete strokes are aggregated to give a better interpretation of
character structure and a feature encoder extracts structural features from the
primitives for classiﬁcation.

Structural Recognition of HCC Using a Modiﬁed PCAE
483
3.2
Primitive Extraction
Primitives are a set of basic parts that make up objects. In the context of Chi-
nese character etymology, primitives refer to strokes or stroke segments that
form Chinese characters. Since Chinese characters are composed of many com-
plex strokes, including curved and multi-line strokes, which are hard to extract
directly, we choose to extract stroke segments as primitives using PCAE.
The recent SCAE [10] has shown potential in unsupervised part discovery,
but it tends to fail on large-scale Chinese character dataset, as shown in Fig. 1.
The main reason behind this is that the large number of categories poses a sig-
niﬁcant barrier to the convergence of the entire model. In contrast, using only
the PCAE (a part of the SCAE) for part discovery, the model demonstrated bet-
ter convergence. In addition, we proposed to modify the PCAE by substituting
the feature encoder with more sophisticated CNN, eliminating noise in the part
capsules, removing the alpha channel of the templates, and imposing constraints
on the speciﬁc pose parameters. The modiﬁed PCAE demonstrates the ability to
acquire sparse and well-deﬁned part templates as illustrated in Fig. 3. Inspired
by the observation, we design a Chinese character primitive extractor based on
the modiﬁed PCAE.
The modiﬁed PCAE formulates the part discovery problem as auto-encoding:
the encoder infers the part capsules which contain the presence probabilities
pk ∈[0, 1], the poses θk ∈R1×6, and special features fk ∈R1×d of diﬀerent
parts. More formally, given an input image I, the network initially employs a
feature extractor to encode the image into K part capsules:
E(I) = Θ1, Θ2, ..., ΘK,
Θk : {pk, θk, fk}.
(1)
While the decoder learns a set of pixel-level primitive templates to reconstruct
the input. Firstly, the learnable templates T are transformed by the parameters
θ and the their gray level C is decoded from f:
ˆTk = AﬃneTrans(Tk, θk),
Ck = MLP(fk).
(2)
Then the transformed templates ˆT are arranged in the image, and a spatial Gaus-
sian mixture model is used to deﬁne the image likelihood. The model employs
pixels of the transformed part templates as centers of isotropic Gaussian compo-
nents with constant variance. The mixing probabilities of diﬀerent components
are proportional to the product of the presence probabilities of part capsules
and the value of the template:
pk,i,j ∝pk ˆTk,i,j,
L(I) =

i,j
K

k=1
pk,i,jN(Ii,j|Ck · ˆTk,i,j; σ2),
(3)
where the (i, j) means the coordinates in image. This feature extractor and the
primitive templates are learned by maximizing the image likelihood of L.

484
X.-J. Wu et al.
3.3
Stroke Aggregation
As shown in Fig. 4, the part templates discovered by PCAE are primarily stroke
segments (horizontal, vertical, left-slanted, and right-slanted), which coincides
with many manually deﬁned basic strokes [3,15], reﬂecting some extent the abil-
ity to understand the structure of Chinese characters. To extract more intuitive
structural information from these ﬁne stroke segments, we further aggregate
them to obtain complete strokes. Speciﬁcally, we introduce certain writing rules
of Chinese characters as priors. For example, a complete stroke should satisfy the
collinearity rule. As a result, we systematically examine the continuity between
neighboring stroke segments and merge the segments that should belong to the
same stroke. This process allows us to extract higher-level complete strokes at
the semantic level for the current input Chinese character, thereby enhancing
the model’s ability to understand the character’s structure. Moreover, to avoid
redundant computations during the continuity checking process, we adopt a
greedy algorithm to optimize the entire stroke aggregation process. Algorithm 1
details the post-processing algorithm for stroke aggregation.
Algorithm 1. Post-processing algorithm for strokes aggregation
Input: A set of stroke segment images ˆT1, ˆT2, ..., ˆTK
Output: A set of stroke images S1, S2, .., SM
for all k ∈{1, 2, ..., K} do
calculates the center location of stroke segment mk
calculates the slope of stroke segment sk
end for
merge list = [ ]
// store merged stroke segments
for i ∈{1, 2, ..., K −1} and i /∈merge list do
for j ∈{i + 1, ..., K}and j /∈merge list do
calculate the slope sij between mi and mj
if max(si, sj, sij) −min(si, sj, sij) < sth then
merge ˆTi and ˆTj
merge list.append(j)
end if
end for
end for
3.4
Character Recognition
For character recognition, we train a linear classiﬁer with supervision given the
part capsules, which are encoded into a structural feature representation. This
learns a (K × D) × C weights and C biases without modiﬁcation of the feature
encoder, where K and D denote the number and the dimension of part capsules,
respectively.

Structural Recognition of HCC Using a Modiﬁed PCAE
485
4
Experiments
4.1
Implementation Details
In our model, we replace the four-layer CNN in PCAE with a seven-layer CNN
as the encoder, and the number of capsules is set as 40 considering that the prim-
itives are stroke segments. For optimization, we employ the RMSProp optimizer
with 3×10−4 learning rate. All the experiments are implemented on TensorFlow
with a single NVIDIA TiTan Xp GPU, and the throughput of our model is 304
images/s with batch size ﬁxed to 256.
4.2
Datasets
HWDB1.0-1.1. We train our model on the HWDB1.0-1.1 from CASIA-
HWDB [12] datasets. It contains 2,678,424 oﬄine handwritten Chinese character
images in 3881 classes, collected from 720 writers. We use the samples of 3,755
GB level-1 Chinese characters in our experiments for training.
Fig. 4. The structural understanding of Chinese character by our framework. For each
input image, the model automatically adjusts the basic stroke segments to describe
current character and aggregates them to get more complete strokes with the stroke
aggregation algorithm, constructing a bottom-up hierarchy.
ICDAR2013. We evaluate the performance of our model on the ICDAR2013
[20], which contains 224,419 oﬄine handwritten Chinese character images in
3755 classes, collected from 60 writers.
4.3
Eﬀects of Primitive and Stroke Extraction
For each input Chinese character image, the modiﬁed PCAE can encode
character-aware part capsules. The parameters in these capsules can activate

486
X.-J. Wu et al.
a subset of primitives and adjust their pose and shape to describe the current
character. However, these fragmented stroke segments are not intuitive enough
for understanding the structure of Chinese characters. The post-processing algo-
rithm to aggregate complete strokes. Figure 4 shows the bottom-up hierarchical
character strokes extraction by our model. It can be observed that our model
eﬀectively reconstructs the original images and parses them into strokes that are
comprehensible to humans. This demonstrates its strong capability in under-
standing the structural aspects of Chinese characters.
4.4
Performance of Chinese Character Recognition
We test the classiﬁcation performance of the model under diﬀerent settings,
including diﬀerent numbers of categories, as well as a few-shot recognition sce-
nario. We compared the performance of our model with the original SCAE and
two popular DNN architectures: Conv-Auto-Encoders (Conv-AE) and CNN. For
a fair comparison, the backbone of all models remains the same structure.
Fig. 5. TSNE Visualization of the feature distribution for diﬀerent categories. Subﬁg-
ures (a-c) correspond to the 10/20/100 categories samples from testset.
Diﬀerent Category Numbers. We ﬁrst investigate the distribution of part
capsules in the feature space. Here we show TSNE embeddings of part capsules
for 10/20/100 categories in Fig. 5. We can see that the feature distribution of
samples belonging to the same class is relatively compact while those belong-
ing to diﬀerent classes are relatively scattered, indicating the separability of the
learned features. The recognition accuracies on test dataset of diﬀerent meth-
ods under diﬀerent category numbers are shown in Table 1. With the category
number increasing, the recognition accuracy of these models tends to decrease,
while the SCAE even fails to converge during training. The performance of our
modiﬁed PCAE is signiﬁcantly better than that of other unsupervised feature
extraction methods, but slightly inferior to CNN trained in a supervised manner.

Structural Recognition of HCC Using a Modiﬁed PCAE
487
Table 1. Accuracies (%) of Chinese character recognition under the diﬀerent number
of categories. HWDB-η means to test the accuracy with η categories.
Method
HWDB-10
HWDB-100
HWDB-3755
Conv-AE
97.6
88.1
43.8
CNN
99.8
97.7
91.7
SCAE
96.8
45.2
-
PCAE
98.4
93.6
62.5
Modiﬁed PCAE
99.7
95.8
74.7
Few-Shot Recognition. Considering that our model extracts features in a
self-supervised manner, we further investigate its recognition performance in
a scenario where the classiﬁer is trained with a limited number of samples.
As shown in Table 2, despite the decrease in classiﬁcation accuracy caused by
few-shot learning, our model exhibits better performance than other methods,
demonstrating that the structural information extracted by our model has a
stronger generalization ability in the few-shot scenario.
Table 2. Accuracies (%) of Chinese character recognition results with diﬀerent num-
bers of labeled samples per class. HWDB-3755/ρ means ρ labels for each category of
samples.
Method
HWDB-3755/1
HWDB-3755/10
HWDB-3755
Conv-AE
0.03
35.8
43.8
CNN
0.83
46.3
91.7
PCAE
8.4
40.6
62.5
Modiﬁed PCAE
10.73
48.6
74.7
Fig. 6. Impact of the number of part capsules
in our framework.
Table 3. Accuracies (%) of Chinese
character recognition with diﬀerent
classiﬁcation features selected from
the parameters of part capsules.
Classiﬁcation Features
Accuracy
presences
3.32
patches presences
64.3
pose
70.8
presences+pose
71.8
presences+pose+features 74.7

488
X.-J. Wu et al.
4.5
Ablation Study
We conduct ablation studies to explore the eﬀectiveness of each component in
our method.
Impact of Part Capsules Number. It is important to set an appropriate
number of part capsules in PCAE. Insuﬃcient number of capsules results in
suboptimal reconstruction of input images, whereas an excessive number of cap-
sules may lead to fragmented and repetitive primitive templates. As shown in
Fig. 6, we observe the part capsule number is 40 leads to higher accuracy.
Eﬀectiveness of Classiﬁcation Features. We also investigate the impact of
using diﬀerent classiﬁcation features on the performance of Chinese character
recognition. The presence, pose, and features are decoupled parameters in part
Capsules respectively, and the patches presences are enhanced presences fea-
tures obtained by dividing the transformed primitive templates into blocks. As
shown in Table 3, the integration of all these features gives optimal classiﬁcation
performance.
5
Conclusion
In this work, we bring a new perspective of primitive extraction for Chinese
character recognition by modifying the PCAE and introducing a pose-processing
stroke aggregation algorithm. Our method captures the structural information of
Chinese characters by decomposing them into stroke segments based on a set of
primitives and aggregating these segments into relatively complete strokes that
align with human cognition. The PCAE is trained in an unsupervised manner
to learn primitives, so, in addition to the stroke extraction ability, the model
can generalize well even when a small number of samples are used to train the
classiﬁer. Our method performs comparably to CNN on medium/small-scale
datasets and outperforms them in label-limited scenarios. The proposed method
provides a potential solution to large-scale Chinese character recognition with
structural interpretation. Our future work will seek better part capsule learning
and feature encoding scheme to further improve the recognition performance.
References
1. Biederman, I.: Recognition-by-components: a theory of human image understand-
ing. Psychol. Rev. 94(2), 115 (1987)
2. Chang, F.: Techniques for solving the large-scale classiﬁcation problem in Chinese
handwriting recognition. In: Doermann, D., Jaeger, S. (eds.) SACH 2006. LNCS,
vol. 4768, pp. 161–169. Springer, Heidelberg (2008). https://doi.org/10.1007/978-
3-540-78199-8 10
3. Chiu, H.-P., Tseng, D.-C.: A novel stroke-based feature extraction for handwritten
Chinese character recognition. Pattern Recogn. 32(12), 1947–1959 (1999)
4. Cire¸san, D., Meier, U.: Multi-column deep neural networks for oﬄine handwritten
Chinese character classiﬁcation. In: 2015 International Joint Conference on Neural
Networks (IJCNN), pp. 1–6. IEEE (2015)

Structural Recognition of HCC Using a Modiﬁed PCAE
489
5. Duarte, K., Rawat, Y., Shah, M.: VideoCapsuleNet: a simpliﬁed network for action
detection. In: Advances in Neural Information Processing Systems, vol. 31 (2018)
6. Hahn, T., Pyeon, M., Kim, G.: Self-routing capsule networks. In: Advances in
Neural Information Processing Systems, vol. 32 (2019)
7. Hinton, G.E., Sabour, S., Frosst, N.: Matrix capsules with EM routing. In: Inter-
national Conference on Learning Representations (2018)
8. Jin, L.-W., Yin, J.-X., Gao, X., Huang, J.-C.: Study of several directional fea-
ture extraction methods with local elastic meshing technology for HCCR. In: Pro-
ceedings of the Sixth International Conference for Young Computer Scientist, pp.
232–236 (2001)
9. Kim, I.-J., Liu, C.-L., Kim, J.-H.: Stroke-guided pixel matching for handwritten
Chinese character recognition. In: Proceedings of the Fifth International Confer-
ence on Document Analysis and Recognition, pp. 665–668. IEEE (1999)
10. Kosiorek, A., Sabour, S., Teh, Y.W., Hinton, G.E.: Stacked capsule autoencoders.
In: Advances in Neural Information Processing Systems, vol. 32 (2019)
11. Lai, S., Jin, L., Yang, W.: Toward high-performance online HCCR: a CNN app-
roach with DropDistortion, path signature and spatial stochastic max-pooling.
Pattern Recogn. Lett. 89, 60–66 (2017)
12. Liu, C.-L., Yin, F., Wang, D.-H., Wang, Q.-F.: Online and oﬄine handwritten
Chinese character recognition: benchmarking on new databases. Pattern Recogn.
46(1), 155–162 (2013)
13. Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. In:
Advances in Neural Information Processing Systems, pp. 3856–3866 (2017)
14. Singh, M., Hoﬀman, D.D.: Part-based representations of visual shape and impli-
cations for visual cognition. In: Advances in Psychology, vol. 130, pp. 401–459.
Elsevier (2001)
15. Yih-Ming, S., Wang, J.-F.: A novel stroke extraction method for Chinese characters
using Gabor ﬁlters. Pattern Recogn. 36(3), 635–647 (2003)
16. Wang, W., Zhang, J., Du, J., Wang, Z.-R., Zhu, Y.: Denseran for oﬄine hand-
written Chinese character recognition. In: 2018 16th International Conference on
Frontiers in Handwriting Recognition (ICFHR), pp. 104–109. IEEE (2018)
17. Wu, C., Wang, Z.-R., Du, J., Zhang, J., Wang, J.: Joint spatial and radical analysis
network for distorted Chinese character recognition. In: 2019 International Con-
ference on Document Analysis and Recognition Workshops (ICDARW), vol. 5, pp.
122–127. IEEE (2019)
18. Wu, C., Fan, W., He, Y., Sun, J., Naoi, S.: Handwritten character recognition by
alternately trained relaxation convolutional neural network. In: 2014 14th Inter-
national Conference on Frontiers in Handwriting Recognition, pp. 291–296. IEEE
(2014)
19. Xin, J., Wang, N., Jiang, X., Li, J., Gao, X., Li, Z.: Facial attribute capsules for
noise face super resolution. In: Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, vol. 34, pp. 12476–12483 (2020)
20. Yin, F., Wang, Q.-F., Zhang, X.-Y., Liu, C.-L.: ICDAR 2013 Chinese handwriting
recognition competition. In: 12th International Conference on Document Analysis
and Recognition, pp. 1464–1470. IEEE (2013)
21. Yu, C., Zhu, X., Zhang, X., Wang, Z., Zhang, Z., Lei, Z.: HP-Capsule: unsupervised
face part discovery by hierarchical parsing capsule network. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4022–
4031 (2022)

490
X.-J. Wu et al.
22. Yu, C., Zhu, X., Zhang, X., Zhang, Z., Lei, Z.: Graphics capsule: learning hierar-
chical 3D face representations from 2D images. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 20981–20990 (2023)
23. Zhang, X., Li, P., Jia, W., Zhao, H.: Multi-labeled relation extraction with attentive
capsule network. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 33, pp. 7484–7491 (2019)
24. Zhang, X.-Y., Bengio, Y., Liu, C.-L.: Online and oﬄine handwritten Chinese char-
acter recognition: a comprehensive study and new benchmark. Pattern Recogn.
61, 348–360 (2017)
25. Zhang, X.-Y., Yin, F., Zhang, Y.-M., Liu, C.-L., Bengio, Y.: Drawing and recogniz-
ing Chinese characters with recurrent neural network. IEEE Trans. Pattern Anal.
Mach. Intell. 40(4), 849–862 (2017)

Natural Language Processing

Sequential Style Consistency Learning
for Domain-Generalizable Text
Recognition
Pengcheng Zhang1, Wenrui Liu1, Ning Wang1, Ran Shen2, Gang Sun2,
Xinghua Jiang3, Zheqian Chen3, Fei Wu1, and Zhou Zhao1(B)
1 Zhejiang University, Hangzhou, China
{zhangpengcheng1218,liuwenrui,ningxin99,zhaozhou}@zju.edu.cn,
wufei@cs.zju.edu.cn
2 State Grid Zhejiang Electric Power Co., Ltd., Hangzhou, China
{shen ran,sun gang}@zj.sgcc.com.cn
3 Yiwise AI Technology Co., Ltd., Hangzhou, China
{jiangxinghua,chenzheqian}@yiwise.com
Abstract. As a task aiming to recognize text from images, text recog-
nition is of great signiﬁcance in both industry and academia. The vast
majority of existing text recognition methods use text images with the
same styles as training and testing samples. However, when these mod-
els encounter images with new styles, their recognition accuracy will
be signiﬁcantly reduced. In this paper, we mainly explore Domain-
Generalizable Text Recognition (DGTR), a challenging but meaningful
setting focusing on enhancing the generalization ability of text recogni-
tion models. For this reason, we propose a practical framework called
Sequential Style Consistency Learning (SSC), disentangling the style-
speciﬁc and task-speciﬁc representation. Speciﬁcally, our SSC ﬁrst con-
structs samples of augmented visual feature sequences, then disentan-
gles the original and augmented feature sequences into style-speciﬁc fea-
tures and task-speciﬁc features. To better separate the task-speciﬁc rep-
resentation from the style-speciﬁc representation, the Style-Consistency
Learning (SCL) is designed for learning the style consistency between
original and augmented sequences. The disentangled module and style-
consistency learning could provide complementary information for each
other. Besides, our SSC is encouraged to meta-learn the style-speciﬁc
and task-speciﬁc features during training based on text images with seen
styles, generalizing better to text images with other styles. Numerous
experiments and analyses conducted on the benchmark dataset MSDA
have shown that SSC can achieve very competitive experimental results
compared to state-of-the-art methods.
Keywords: Text Recognition · Domain Generalization · Style
Consistency Learning
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 493–504, 2024.
https://doi.org/10.1007/978-981-99-8850-1_40

494
P. Zhang et al.
1
Introduction
As a task of reading text from natural images, text recognition has drawn
the interest of many researchers. Numerous deep-learning-based text recogni-
tion models [2,4,10,25,31] have been proposed, and they have shown exciting
performance on benchmark text recognition datasets [12,13,15,16,20,24,27,30].
These datasets usually consist of text images with only a few styles, and
these text recognition models use text images with the same styles as training
and testing samples. Unfortunately, when these models encounter images with
new styles, such as diﬀerent fonts, diﬀerent angles, etc., their performance will
deteriorate, which is also known as domain shift [23]. Obviously, these paradigms
of text recognition research do not meet the needs of practical applications. A
viable text recognition method should accurately recognize the text for images
with unseen styles, instead of only images with speciﬁc styles.
In this paper, we mainly explore DGTR, a challenging but meaningful set-
ting to enhance the generalizability of text recognition models. The previous
methods [5,9,14,18,19,33,34] of domain generalization are mainly designed to
eliminate the domain shift in image classiﬁcation. However, text recognition
is vastly diﬀerent from image classiﬁcation. Therefore, the above domain gen-
eralization methods are hard to be directly applied for text recognition. We
argue that the sequentiality of text recognition must be considered to achieve
DGTR. Motivated by the above considerations, we propose Sequential Style
Consistency Learning (SSC), a practical framework for DGTR. Compared with
original text recognition models, Our SSC has made several improvements in
terms of deep architecture and training modes. Because style-speciﬁc informa-
tion in the extracted visual feature sequence is trivial or even interferes with
DGTR, style-speciﬁc and task-speciﬁc representation should be decoupled from
the original representation. Speciﬁcally, our SSC ﬁrst constructs a correspond-
ing augmented feature sequence with our sequential augmentation methods for
each extracted feature sequence. Then a disentangled module is implemented to
decouple the style-speciﬁc and task-speciﬁc representation. Besides, we designed
a style-consistency module for learning the style consistency between original
and augmented samples to better separate the task-speciﬁc representation from
the style-speciﬁc representation. We argue that both the sequential augmen-
tation methods and the style-consistency module are designed to enhance the
disentangled modules. As for the training mode, our SSC is encouraged to meta-
learn the style-speciﬁc and task-speciﬁc features during training based on text
images with seen styles, generalizing better to text images with other styles. We
conducted numerous experiments on the dataset MSDA [26], and the analysis
of experimental results show that our SSC can achieve very competitive exper-
imental results compared to state-of-the-art methods. Speciﬁcally, Gulrajani et
al. [11] stated in DomainBed that model selection is non-trivial for DG yet aﬀects
results. Following [11], we use training-domain validation as the mechanism in
our experiments. In a word, the contribution points of our paper are summarized
as follows:

SSC for Domain-Generalizable Text Recognition
495
– We propose domain-generalizable text recognition, a challenging but mean-
ingful setting, enhancing the generalization performance of text recognition
models. We are the ﬁrst attempt to investigate the domain-generalizable set-
ting of text recognition.
– We propose SSC for the challenging setting, disentangling style-speciﬁc and
task-speciﬁc representation from the original representation.
– SSC constructs augmented visual features sequences, and both the original
and augmented features are decoupled into style-speciﬁc and task-speciﬁc
features. Besides, we design the style-consistency module to better separate
the task-speciﬁc representation from the style-speciﬁc representation.
– Numerous experiments and comprehensive analyses on the text recognition
dataset MSDA show that our SSC method can achieve very competitive
experimental results compared to state-of-the-art text recognition methods.
Feature 
Extractor
Prediction
Original Extracted
Feature Sequence
Task-Specific 
 Sequence 
Encoder
Augmented Feature
Sequence
Disentangled
Module
Original Task-Specific
Feature Sequence
Augmented Task-Specific
Feature Sequence
Style-Specific 
Sequence 
Encoder
Encoded Augmented
Style-Specific Feature
Sequence
Encoded Original
Task-Specific Feature
Sequence
Encoded Original
Style-Specific Feature
Sequence
Encoded Augmented
Task-Specific Feature
Sequence
Minimax Entropy
Original Style-Specific
Feature Sequence
Augmented Style-Specific
Feature Sequence
Style-Consistency
Learning
data flow during training
data flow during inference
Fig. 1. Overview framework of our SSC.
2
Method
2.1
Base Network
Several methods for text recognition [4,10,25,31] have been proposed, which are
mainly variants of [2]. In our paper, we focus on the general framework [2] and
use it as the base network of our SSC. The framework [2] consists of the four
stages:
1. Transformation: The input text image is standardized by a TPS transforma-
tion [28]. This stage is optional for a text recognition model.

496
P. Zhang et al.
2. Feature extraction: A CNN is utilized to extract the visual feature sequence
of the text image. The quality of the extracted visual features has an essential
impact on the eﬀect of subsequent recognition.
3. Sequence modeling: This stage utilizes a Bidirectional LSTM (BiLSTM) to
model the extracted visual feature sequence.
4. Prediction: At this stage, the model predicts the ﬁnal character sequence
based on the sequence-modeled feature sequence.
2.2
Sequential Style Consistency Learning
The visual feature sequences extracted in stage Feature extraction usually con-
tain both style-speciﬁc and task-speciﬁc information in traditional models. To
provide eﬀective feature sequences containing only task-speciﬁc information to
stage Prediction, we propose SSC for DGTR. Figure 1 presents the overall frame-
work of SSC, which consists of three major parts: Sequential Feature Augmenta-
tion, Disentanglement and Minimax Entropy, and Style-Consistency Learning.
0
1
2
3
4
5
1
3
5
0
2
4
0
1
2
3
4
5
0
1
2
3
4
5
3
4
5
0
1
2
0
3
4
Original Feature
Sequence
Augmented
Feature Sequence
(a) Random Rearrangement
(b) Slice Replacement
(c) Random Masking
Fig. 2. Considered sequential feature augmentation types in our SSC.
Sequential Feature Augmentation. Previous data augmentation methods
[1,7,8] for images have shown eﬀective results. Inspired by these image augmen-
tation methods [1,7,8], we propose the sequential feature augmentation method
for DGTR. Figure 2 depicts diﬀerent sequential feature augmentation types con-
sidered in our work, including Random Rearrangement (RR) Slice Replacement
(SR) and Random Masking (RM).
The simple idea is if a feature sequence has a speciﬁc property (style), the
augmented samples corresponding to this sequence should have the same prop-
erty (style). Therefore, we construct an augmented sample for each visual fea-
ture sequence extracted in stage Feature extraction. Suppose the original visual
feature sequence is denoted S ∈RT ×d, we construct its augmented sample as
˜S ∈RT ×d, where T is the number of features and d is the dimension of features.
Disentangled Module and Minimax Entropy. The Disentangled Module
is designed to decouple S into the task-speciﬁc feature sequence St and the
style-speciﬁc feature sequence Ss by:
St = αS,
Ss = (1 −α)S
(1)

SSC for Domain-Generalizable Text Recognition
497
where St ∈RT ×d and Ss ∈RT ×d. α ∈Rd is the weight used to calculate St and
Ss. α is modeled as a two-layer perceptual structure:
α = σ(W2ReLU(W1pool(S)))
(2)
where pool is the average pool operation along the time dimension, W1 ∈Rd×2d
and W2 ∈R2d×d are learnable transformation weights.
Similarly, the augmented sample ˜S is disentangled into augmented task-
speciﬁc feature sequence ˜St
∈RT ×d and augmented style-speciﬁc feature
sequence ˜Ss ∈RT ×d.
Inspired by the structure of existing text recognition models, we perform
sequence modeling on these decoupled feature sequences, as shown in Fig. 1.
Speciﬁcally, two BiLSTMs, Task-Speciﬁc Sequence Encoder and Style-Speciﬁc
Sequence Encoder, are used to model task-speciﬁc and style-speciﬁc feature
sequences, respectively. Let the encoded feature sequences corresponding to
these disentangled feature sequences be denoted as S′
t ∈RT ×d′, S′
s ∈RT ×d′,
˜S′t ∈RT ×d′ and ˜S′s ∈RT ×d′, respectively, where d′ is the dimension after
sequence modeling. Note that only S′
t as the valid semantic representation is
provided to the stage Prediction to predict the ﬁnal character sequence.
To better disentangle S′
t, S′
s and ˜S′t, ˜S′s, we devise the Minimax Entropy
strategy, employing label-based and entropy-based constraints. In detail, for
style-speciﬁc information, S′
s and ˜S′s are utilized to predict the correspond-
ing style categories, which is simpliﬁed into a multi-class classiﬁcation problem.
The speciﬁc process is designed as follows:
ps = softmax(W3pool(S′
s)),
˜ps = softmax(W3pool( ˜S′s))
(3)
Ls = −
N

i=1
qilogps,i,
˜Ls = −
N

i=1
˜qilog˜ps,i
(4)
where q is the corresponding one-hot label, ps and ˜ps denote the probability
distributions of all N style categories, W3 ∈Rd′×M is a learnable transformation
weight, Ls and ˜Ls represent the cross-entropy loss function.
For task-speciﬁc information, an intuitive idea is that the corresponding style
category should not be determined according to task-speciﬁc information. There-
fore, we utility S′
t and ˜S′t to calculate the probability distributions pt and ˜pt of
all N style categories. Then we leverage reverse entropy loss to regularize S′
t and
˜S′t to discard style discrimination, the process is as follows:
pt = softmax(W4pool(S′
t)),
˜pt = softmax(W4pool( ˜S′t))
(5)
Lt =
N

i=1
pt,ilogpt,i,
˜Lt =
N

i=1
˜pt,ilog˜pt,i
(6)
where pt and ˜pt denote the probability distributions of all N style categories,
W4 ∈Rd′×M is a learnable transformation weight, Lt and ˜Lt are the reverse
entropy loss function which we need to minimize in our SSC.

498
P. Zhang et al.
Pooling
Pooling
Encoded Original Style-Specific
Feature Sequence
Encoded Augmented Style-Specific
Feature Sequence
Encoded Augmented Task-Specific
Feature Sequence
Encoded Original Task-Specific
Feature Sequence
close
distant
Fig. 3. Overall of our Style-Consistency Learning.
Style-Consistency Module. Style-consistency module aims to learn the style
consistency between the original feature sequence and the corresponding aug-
mented sample while reducing the correlation between the style-speciﬁc features
and corresponding task-speciﬁc features, thereby decoupling better style-speciﬁc
and task-speciﬁc features. As shown in Fig. 3, style-consistency learning is per-
formed by maximizing the similarity between the pooling style features and
style-speciﬁc feature sequences and minimizing the similarity between the pool-
ing style features and task-speciﬁc feature sequences.
Inspired by [6], we ﬁrst use a MLP to obtain the projected style-speciﬁc and
task-speciﬁc features:
Sp
i = g(S′
i) = σ(Wp(S′
i)),
˜Sp
i = g( ˜S′i) = σ(Wp( ˜S′i))
(7)
where i ∈{s, t} and Wp ∈Rd′×dp is a learnable parameter.
Style-consistency learning is performed in two directions: original to aug-
mented (o2a) and from augmented to original (a2o). The direction o2a is per-
formed as follows:
We ﬁrst deﬁne a feature set Sall = { ˜Sp
s, Sp
t , ˜Sp
t } = {s1, . . . , sN}. And we
calculate the pooling original style-speciﬁc feature sp as follows:
sp = pool(Sp
s)
(8)
where pool is is the mean pooling operation along temporal dimension.
The soft nearest neighbor ˆs is deﬁned as follows:
ˆs =
N

si∈Sall
βisi,
βi =
exp(sim(sp, si))
N
si∈Sall exp(sim(sp, si))
(9)
where βi is the the similarity distribution which signiﬁes the proximity between
sp and each si ∈Sall.
Then we utilize the style-pairs (sp, sj) between sp and ˜Sp
s to apply with
contrastive loss, which is deﬁned similar to InfoNCE [22] as follows:
Lo2a = −log
exp(sim(sp, sj)/τ)
N
si∈Sall exp(sim(sp, si)/τ)
(10)

SSC for Domain-Generalizable Text Recognition
499
Table 1. Comparison to recent DG methods for DGTR on MSDA.
Method
Ω →C
Ω →H
Ω →D
Ω →Sy
Ω →St
Average
WER
CER
WER
CER
WER
CER
WER
CER
WER
CER
WER
CER
ERM [29]
82.88
24.89
96.95
83.15
72.33
23.87
77.90
34.56
90.99
76.66
84.21
48.63
MLDG [17]
79.89
23.36
96.87
82.69
69.64
22.56
76.22
33.07
88.83
73.27
82.29
46.99
Reptile [21]
80.24
24.44
96.84
82.77
70.35
22.69
76.46
33.48
87.89
73.09
82.36
47.29
MetaReg [3]
80.03
24.37
97.02
82.55
69.86
22.03
76.84
32.99
89.02
74.01
82.55
47.19
MixStyle [34] 81.89
24.77
97.99
83.34
71.36
23.11
77.49
34.02
89.74
73.87
83.69
47.82
DTN [9]
80.17
23.85
96.90
83.02
69.71
22.17
76.83
32.46
88.61
72.99
82.44
46.90
SFA [18]
82.02
24.33
96.93
83.21
71.22
23.24
77.03
33.83
89.91
75.61
83.42
48.04
SSC
78.51 22.54 96.45 81.71 67.57 20.00 74.07 31.27 85.92 71.99 80.50 45.50
Minimizing Lo2a encourages the network to make the pooling original style-
speciﬁc feature sp close to ˜Sp
s, and distant to Sp
t and ˜Sp
t .
Similarly, we can obtain loss La2o for the direction a2o. Then we obtain the
style-consistency learning loss Lsc as follows:
Lscl = Lo2a + La2o
(11)
2.3
Training and Inference
Let the original loss function used for the base network of text recognition train-
ing be Lbase, and the ﬁnal loss function L can be obtained by combining losses
of the disentangled module and style-consistency module as follows:
L = Lbase + Ls + ˜Ls + Lt + ˜Lt + λLscl
(12)
where λ is a hyper-parameter.
Our method is encouraged to meta-learn the style-speciﬁc and task-speciﬁc
features during training based on text images with multiple seen styles, gener-
alizing better to text images with unseen styles.
As shown in Fig. 1, the data ﬂow of our SSC model during training and
inference is diﬀerent, and the computational cost of our model during inference
is much less than that during training.
3
Experiments
3.1
Dataset and Metrics
We evaluated our SSC method on the latest large-scale multi-domain text recog-
nition dataset MSDA [26]. We utilize two commonly used text recognition met-
rics CER and WER. For both CER and WER, lower scores indicate better
performance.

500
P. Zhang et al.
Table 2. Ablation studies of SSC for DGTR.
Method Ω →C
Ω →H
Ω →D
Ω →Sy
Ω →St
Average
WER
CER
WER
CER
WER
CER
WER
CER
WER
CER
WER
CER
ERM
82.88
24.89
96.95
83.15
72.33
23.87
77.90
34.56
90.99
76.66
84.21
48.63
+D
82.03
24.01
96.94
82.04
72.20
23.92
77.24
33.63
90.27
75.03
83.74
47.73
+D,A
81.61
23.87
96.84
82.35
70.61
22.22
76.03
32.29
88.94
74.36
82.81
47.02
+D,A,S 80.04
22.82
96.85
82.01
69.43
21.24
75.24
31.88
87.61
72.90
81.83
46.17
SSC
78.51 22.54 96.45 81.71 67.57 20.00 74.07 31.27 85.92 71.99 80.50 45.50
Table 3. Experiment results of SSC with 3 diﬀerent augmentation methods for DGTR.
Method
Ω →C
Ω →H
Ω →D
Ω →Sy
Ω →St
Average
WER CER
WER CER
WER CER
WER CER
WER CER
WER CER
ERM
82.88
24.89 96.95
83.15 72.33
23.87 77.90
34.56 90.99
76.66 84.21
48.63
with RR
78.84
22.84 96.68
81.87 68.33
20.11 74.21
31.83 86.44
72.81 80.90
45.89
with RM 79.35
23.03 96.90
82.14 68.22
20.32 75.31
31.64 87.03
72.67 81.36
45.96
with SR
78.51
22.54 96.45
81.71 67.57
20.00 74.07
31.27 85.92
71.99 80.50
45.50
3.2
Implementation Details
Our experiment is carried out on the PyTorch framework. We used a single
NVIDIA GeForce GTX 1080Ti for training and inference. Following the work
[2], we choose the AdaDelta optimizer [32], and the corresponding decay rate is
set as ρ = 0.95. The training batch size in our experiment is set as 96, and the
number of iterations is set as 300K. We set the hyper-parameter λ = 1.0 in the
Eq. 12 and τ = 0.1 in the Eq. 10. The learning rates of meta-learning γ and η are
set to 5e −4. Following [26], the TPS module is not used in our model. Other
details of the network structure are the same as [2].
3.3
Model Selection
Gulrajani et al. [11] stated in DomainBed that model selection is non-trivial for
DG yet aﬀects results. Following the work [11], we use training-domain valida-
tion as the strategy in our experiments. Speciﬁcally, we use the 4 testing sets
corresponding to the 4 training sets as the choose the model on the validation
set for inference.
3.4
Comparison Baselines
We compare our SSC model with recent state-of-the-art domain generalization
methods [3,9,17,18,21,34] that can be directly applied to text recognition. These
DG methods include: ERM [29], MLDG [17], Reptile [21], MetaReg [3], MixStyle
[34], DTN [9], and SFA [18]. For a fair comparison, we directly apply these
methods to the same base text recognition network [2] as our SSC, and the
model selection is also same as SSC.

SSC for Domain-Generalizable Text Recognition
501
Table 4. Experiment results of ablation studies on the disentangled module.
Method
Average
WER
CER
ERM
84.21
48.63
w/o. Ls
81.47
46.22
w/o. Ls and ˜Ls
82.39
46.65
w/o. Lt
81.73
46.61
w/o. Lt and ˜Lt
82.56
47.12
w/o. Ls and Lt
82.33
47.35
w/o. ˜Ls and ˜Lt
81.57
46.76
w/o. Ls, ˜Ls, Lt and ˜Lt
83.61
47.81
w/o. Lscl
81.97
46.66
Full Model
80.50 45.50
3.5
Comparison Results
Table 1 compares our SSC model with recent domain generalization models on
the MSDA dataset. Obviously, our SSC outperforms these recent domain gener-
alization methods on both metrics, WER and CER. Previous approaches (such
as MLDG [17], Reptile [21], MetaReg [3], and DTN [9]) consider improving the
generalization ability of the original model from the training mode, which is a
common approach, but more speciﬁc designs are needed for speciﬁc tasks. Other
DG methods (such as MixStyle [34] and SFA [18]) are only designed for image
classiﬁcation and do not take into account the properties of the sequence recog-
nition task. When these methods are used directly for the text recognition task,
their eﬀect is not ideal. Comparatively, our proposed sequential style consistency
learning method for DGTR, which disentangles the visual feature sequences and
learns the style consistency between original and augmented samples. Our SSC
method is designed for text recognition task, which also signiﬁcantly improves
the generalization ability of the text recognition method further.
Fig. 4. Experiment results of diﬀerent hyperparameters λ and τ.

502
P. Zhang et al.
3.6
Ablation Studies
The Roles of Diﬀerent Modules. Our SSC method can be divided into four
diﬀerent parts: the disentangled module, the sequential augmentation, style-
consistency learning, and meta-learning. Table 2 shows the detailed ablation
experimental result on these modules. As can be seen in Table 2, these four
parts in SSC are all important, and it is the combination of these four parts that
obtains the ﬁnal eﬀect of our SSC model.
The Inﬂuence of Diﬀerent Augmentation Methods. As shown in Table 3,
we conducted experiments with 3 diﬀerent sequential augmentation methods.
Compared to the baseline model, all augmentation methods are eﬀective, and
SSC model with SR performs the best.
The Role of the Speciﬁc Content of the Disentangled Module. As shown
in Table 4, we remove the parts of the loss function L that are related to the
disentangled module for experiments. As shown in Table 4, the performance of
the SSC model is weakened when any part is removed. Speciﬁcally, as shown in
Table 4, in the Minimax Entropy part, the loss function of the original sample
and the augmented sample are complementary, and when the loss function of
the original sample is removed, the loss function of the augmented sample can
be supplemented to a certain extent.
The inﬂuence of diﬀerent values λ and τ. As shown in Fig. 4, we also show
the experiment results of inﬂuence of diﬀerent values of hyper-parameters λ and
τ on the average WER and CER. According to the results, we ﬁnd λ = 1.0 and
τ = 0.1 are the relatively appropriate values.
4
Conclusion
In our paper, we propose domain-generalizable text recognition, a challenging
setting, and we attack this setting with a novel framework called Sequential Style
Consistency Learning (SSC). Since style-speciﬁc information in the extracted
visual feature sequence is trivial, our model leverages a disentangled module to
decouple the task-speciﬁc and style-speciﬁc feature sequences. Besides, sequen-
tial feature augmentation methods and style-consistency learning are designed
to better separate task-speciﬁc features from style-speciﬁc features. As for the
training mode, our SSC is encouraged to meta-learn the style-speciﬁc and task-
speciﬁc features during training based on text images with multiple seen styles,
generalizing better to text images with unseen styles. Extensive experiments and
comprehensive analyses on the text recognition dataset MSDA demonstrate the
eﬀectiveness of our approach.
Acknowledgments. This work was supported in part by National Key R&D Program
of China under Grant No.2022ZD0162000, National Natural Science Foundation of

SSC for Domain-Generalizable Text Recognition
503
China under Grant No. 62222211, National Natural Science Foundation of China under
Grant No.61836002, and National Natural Science Foundation of China under Grant
No.62072397.
References
1. Bachman, P., Hjelm, R.D., Buchwalter, W.: Learning representations by maximiz-
ing mutual information across views. In: Advances in neural information processing
systems, vol. 32 (2019)
2. Baek, J., et al.: What is wrong with scene text recognition model comparisons?
Dataset and model analysis. In: Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pp. 4715–4723 (2019)
3. Balaji, Y., Sankaranarayanan, S., Chellappa, R.: Metareg: Towards domain gener-
alization using meta-regularization. In: Advances in neural information processing
systems, vol. 31 (2018)
4. Bhunia, A.K., Sain, A., Kumar, A., Ghose, S., Chowdhury, P.N., Song, Y.Z.: Joint
visual semantic reasoning: multi-stage decoder for text recognition. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision, pp. 14940–14949
(2021)
5. Bui, M.H., Tran, T., Tran, A., Phung, D.: Exploiting domain-speciﬁc features to
enhance domain generalization. In: Advances in Neural Information Processing
Systems, vol. 34 (2021)
6. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-
trastive learning of visual representations. In: International conference on machine
learning, pp. 1597–1607. PMLR (2020)
7. Chen, T., Kornblith, S., Swersky, K., Norouzi, M., Hinton, G.E.: Big self-supervised
models are strong semi-supervised learners. Adv. Neural. Inf. Process. Syst. 33,
22243–22255 (2020)
8. Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum con-
trastive learning. arXiv preprint arXiv:2003.04297 (2020)
9. Du, Z., Li, J., Lu, K., Zhu, L., Huang, Z.: Learning transferrable and interpretable
representations for domain generalization. In: Proceedings of the 29th ACM Inter-
national Conference on Multimedia, pp. 3340–3349 (2021)
10. Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: autonomous,
bidirectional and iterative language modeling for scene text recognition. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 7098–7107 (2021)
11. Gulrajani, I., Lopez-Paz, D.: In search of lost domain generalization. In: Interna-
tional Conference on Learning Representations (2021)
12. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in nat-
ural images. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2315–2324 (2016)
13. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and
artiﬁcial neural networks for natural scene text recognition. arXiv preprint
arXiv:1406.2227 (2014)
14. Jeon, S., Hong, K., Lee, P., Lee, J., Byun, H.: Feature stylization and domain-aware
contrastive learning for domain generalization. In: Proceedings of the 29th ACM
International Conference on Multimedia, pp. 22–31 (2021)

504
P. Zhang et al.
15. Karatzas, D., et al.: ICDAR 2015 competition on robust reading. In: 2015 13th
International Conference on Document Analysis and Recognition (ICDAR), pp.
1156–1160. IEEE (2015)
16. Karatzas, D., et al.: ICDAR 2013 robust reading competition. In: 2013 12th Inter-
national Conference on Document Analysis and Recognition, pp. 1484–1493. IEEE
(2013)
17. Li, D., Yang, Y., Song, Y.Z., Hospedales, T.M.: Learning to generalize: meta-
learning for domain generalization. In: Thirty-Second AAAI Conference on Artiﬁ-
cial Intelligence (2018)
18. Li, P., Li, D., Li, W., Gong, S., Fu, Y., Hospedales, T.M.: A simple feature augmen-
tation for domain generalization. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 8886–8895 (2021)
19. Liu, C., Wang, L., Li, K., Fu, Y.: Domain generalization via feature variation decor-
relation. In: Proceedings of the 29th ACM International Conference on Multimedia,
pp. 1683–1691 (2021)
20. Mishra, A., Alahari, K., Jawahar, C.: Scene text recognition using higher order
language priors. In: BMVC-British Machine Vision Conference. BMVA (2012)
21. Nichol, A., Achiam, J., Schulman, J.: On ﬁrst-order meta-learning algorithms.
arXiv preprint arXiv:1803.02999 (2018)
22. Van den Oord, A., Li, Y., Vinyals, O., et al.: Representation learning with con-
trastive predictive coding. arXiv preprint arXiv:1807.03748 (2018)
23. Pan, S.J., Yang, Q.: A survey on transfer learning. IEEE Trans. Knowl. Data Eng.
22(10), 1345–1359 (2009)
24. Phan, T.Q., Shivakumara, P., Tian, S., Tan, C.L.: Recognizing text with per-
spective distortion in natural scenes. In: Proceedings of the IEEE International
Conference on Computer Vision, pp. 569–576 (2013)
25. Qiao, Z., et al.: PIMNet: a parallel, iterative and mimicking network for scene
text recognition. In: Proceedings of the 29th ACM International Conference on
Multimedia, pp. 2046–2055 (2021)
26. Qiu, S., Zhu, C., Zhou, W.: Meta self-learning for multi-source domain adapta-
tion: a benchmark. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 1592–1601 (2021)
27. Risnumawan, A., Shivakumara, P., Chan, C.S., Tan, C.L.: A robust arbitrary text
detection system for natural scene images. Expert Syst. Appl. 41(18), 8027–8048
(2014)
28. Shi, B., Wang, X., Lyu, P., Yao, C., Bai, X.: Robust scene text recognition with
automatic rectiﬁcation. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4168–4176 (2016)
29. Vapnik, V.: Statistical Learning Theory. Wiley (1998)
30. Wang, K., Babenko, B., Belongie, S.: End-to-end scene text recognition. In: 2011
International Conference on Computer Vision, pp. 1457–1464. IEEE (2011)
31. Wang, Y., Lian, Z.: Exploring font-independent features for scene text recognition.
In: Proceedings of the 28th ACM International Conference on Multimedia, pp.
1900–1920 (2020)
32. Zeiler, M.D.: ADADELTA: an adaptive learning rate method. arXiv preprint
arXiv:1212.5701 (2012)
33. Zhang, G., Zhao, H., Yu, Y., Poupart, P.: Quantifying and improving transferability
in domain generalization. In: Advances in Neural Information Processing Systems,
vol. 34 (2021)
34. Zhou, K., Yang, Y., Qiao, Y., Xiang, T.: Domain generalization with mixstyle. In:
International Conference on Learning Representations (2021)

MusicGAIL: A Generative Adversarial
Imitation Learning Approach for Music
Generation
Yusong Liao1(B), Hongguang Xu1,2, and Ke Xu3
1 Harbin Institude of Technology, Shenzhen, China
21s152107@stu.hit.edu.cn
2 Peng Cheng Laboratory, Shenzhen, China
3 Shenzhen Polytechnic, Shenzhen, China
Abstract. Deep learning based automatic music generation has received
signiﬁcantly attention and became an attractive research topic in recent
years. However, most existing methods face three major challenges: (1)
Models based on the Recurrent Neural Networks (RNNs) trained by
Maximum Likelihood Estimation (MLE) suﬀer from the exposure bias
problem, while some Generative Adversarial Networks (GANs) based
methods have been proposed to alleviate it, they still suﬀer from the
problems of reward sparsity and mode collapse. (2) Deep Reinforcement
Learning (DRL) based models encounter the reward sparsity problem
and it is impossible to manually specify a completely reasonable reward
function. (3) Although Transformer can be used as an alternative to show
the better parallelization and performance over the origin RNNs, it is still
extremely diﬃcult to optimize for the standard transformer structure in
RL setting. In this paper, we propose MusicGAIL, a generative model for
generating the online countermelody for folk melodies under the frame-
work of generative adversarial imitation learning (GAIL) that directly
learn the optimal policy from dataset without laboriously deﬁning the
reward. As the standard Transformer is hard to be applied in RL set-
ting, we adopt Gated Transformer-XL (GTrXL) to stabilize the model
training. Experimental results show that the melody pieces generated
from MusicGAIL achieve better quality and diversity. Moreover, their
subjective scores signiﬁcantly higher than baselines in terms of melody,
harmony, rhythm, folk style and emotion.
Keywords: Music Generation · GAIL
1
Introduction
As an important form of art to express human feelings and emotions, music has
played an increasingly essential role in people’s daily life. In recent years, with
the advances of machine learning technology, deep learning and neural networks
This work is supported by the Major Key Project of PCL (PCL2021A03-1).
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 505–516, 2024.
https://doi.org/10.1007/978-981-99-8850-1_41

506
Y. Liao et al.
have been implemented in symbolic music generation domain. Most existing
deep learning based generative model adopt the variants of Recurrent Neural
Networks (RNNs) [1] such as Long Short-Term Memory (LSTM) [2] or Gated
Recurrent Unit (GRU) [3], and are trained by Maximum Likelihood Estimation
(MLE) [4] to learn the probability distribution of next note based on the previous
note sequence.
However, generating a piece of reasonable music that have clear harmonic
and rhythmic structure has always been an interesting and challenging task.
These existing methods still face three major challenges.
Firstly, these autoregressive models trained by MLE are not always ideal since
they suﬀer from the exposure bias problem [5] and mismatch problem between
training criterion and generation objectives [6]. Meanwhile, MLE only focuses
on the prediction of each note and greedily choose the note with the highest
probability as the next prediction, but ignoring the long-term global consistency
of the whole music. Therefore, the two major defects will signiﬁcantly lower
the quality of generated music. Although some Generative Adversarial Networks
(GAN) [7] based methods have been proposed to alleviate the exposure bias
problem, these adversarial models still suﬀer from the mode collapse problems
[8], which means the distribution of generated music samples cannot cover the
global distribution of expert samples, causing a lack of diversity and creativity
for generated samples.
Secondly, some deep reinforcement learning (DRL) based models (e.g. [9–
11]) are employed as an alternative to show the superior performance over most
GAN based music generation models, among which RL-Tuner [9] is the ﬁrst
attempt to train model to generate monophonic melody by combining MLE and
RL training. On this basis, Bach2Bach [10] adopt a two layers LSTM model with
a convolutional kernel to generate polyphonic harmonies and achieve the supe-
rior performance than RL-Tuner [9]. However, they also encounter the reward
sparsity problem similar to GAN, which makes the model training signiﬁcantly
diﬃcult to converge. Sequence Tutor [11] adopt a pair of RNNs and manually
deﬁne a domain-speciﬁc reward function to ensure the high-quality of generated
melodies. These reward models manually speciﬁed in terms of a reward function
as composition rules based on music theory in [9–11] have greatly limitations
and it is almost impossible to manually specify a completely reasonable reward
function for this complex task like music generation. While RL-Duet [6] used a
set of neural networks to represent reward instead of manually specifying reward
models based on composition rules and designed four types of reward models to
achieve the global coherence in whole music pieces, it still has not completely
got rid of its dependence on composition rules.
Thirdly, the original RNNs have poor performance in parallelization and are
likely to suﬀer from vanishing or exploding gradients problem. One new net-
work architecture Transformer [12] can handle longer temporal horizons better
than RNNs. On this basis, MusicTransformer [13] modiﬁed the relative attention
mechanism in Transformer and became the ﬁrst successful case of using Trans-
formers to generate music with long-term structure. Nevertheless, it is extremely
diﬃcult to optimize for the standard transformer structure in RL setting, which

MusicGAIL
507
results in a number of transformers used in Natural Language Processing (NLP)
domain still cannot be successfully applied to the RL setting.
To address these problems in existing music generation model, we propose
using Generative Adversarial Imitation Learning (GAIL) [14] with high sam-
ple eﬃciency to directly learn the optimal policy for generating each note from
giving expert musical pieces without laboriously specifying the reward function.
Speciﬁcally, we design MusicGAIL based on [6,11,15], a novel model to gener-
ate the online countermelody for folk melodies via GAIL, which consists of a
melodic generator, a style discriminator and a cooperation rewarder. To over-
come the limitation that the standard Transformer is diﬃcult to be applied to
RL setting, a novel architectural variant, the Gated Transformer-XL (GTrXL),
has been proposed in [16] to provide an easy-to-train alternative that improve
the stability and learning speed of initial transformer. Therefore, MusicGAIL
leverages guidance from GTrXL in our work.
To test and verify the MusicGAIL model, we compare it with three baselines:
MLE, DuetLSTM and RL-Duet [6] and evaluate these models from the objective
and subjective aspects. The experimental results of objective evaluation show
that MusicGAIL model achieves signiﬁcant improvement in performance, gener-
ates the more harmonious note sequences with reasonable intervals and imitate
Chinese folk style better than baselines. The experimental results of user listen-
ing evaluation also show that the subjective scores of our proposed MusicGAIL
are signiﬁcantly higher than baselines in terms of melody, harmony, rhythm, folk
style and emotion, especially folk style.
The main contributions of our work are summarized in: (1) We propose
a generative adversarial imitation learning framework based music generation
model MusicGAIL which directly learn the optimal policy from dataset without
laboriously deﬁning the reward. (2) Experimental results show that MusicGAIL
can eﬀectively alleviate the exposure bias problem in MLE and the mode collapse
problem in most adversarial models to a certain extent. (3) The melody samples
generated from MusicGAIL achieve better quality and diversity in objective
evaluation and higher scores in subjective evaluation compared to baselines.
2
Musical Data Representation and Preprocessing
In order to make music data preprocessing more convenient and eﬃcient, we
implement it by an open source Python library MusPy [17] which has provided
easy-to-use methods for data collection, data preprocessing and model evaluating
in symbolic music generation system. In general, we pick up the required dataset
from music database and ﬁlter out some music that have non-acceptable dura-
tions or cannot be successfully read by MusPy. In order to eliminate the inﬂuence
of rhythm and tone on the music, we transpose music into Cmajor/Aminor.
2.1
Interactive Duet Model
It is extremely important to deﬁne the music representation for our subsequent
work. According to RL-Duet [6], we take an interactive duet improvisation model

508
Y. Liao et al.
as consisting of the human player part and the machine part. More speciﬁcally,
denote the token representations of each music piece by
N1:K =

n(h)
1:K
n(m)
1:K

=

n(h)
1
n(h)
2
· · · n(h)
K
n(m)
1
n(m)
2
· · · n(m)
K

,
(1)
where n(h)
1:K =

n(h)
1 , n(h)
2 , · · · , n(h)
K

represents the music melody which is com-
posed by human player and the n(m)
1:K =

n(m)
1
, n(m)
2
, · · · , n(m)
K

represents the
countermelody which is generated by machine. The n(h)
i
and n(m)
i
are the i −th
tokens in the human player part and machine part respectively.
2.2
Pitch and Duration Encodings
Following BachDuet [18], each token n(·)
i
encodes the pitch and duration infor-
mation of a note. Pitch is encoded with the range of [36, 96] in MIDI number.
The same as most of existing researches, we assume a 4/4-time signature and
quantize music timing into 16th notes, which is the shortest duration of notes
in this work. It means that each beat is subdivided into 4 equal parts. For those
notes with longer duration, we use “_” symbols for held notes. In this case, each
note is represented by a pitch-onset token followed by some “_” tokens. For
example, a C4 quarter note can be represented as 4 time-steps: [“60”, “_”, “_”,
“_”] (the MIDI number of C4 is 60). In addition, we use “R” symbol to represent
rest.
In order to represent the rhythmic information of music and achieve a nat-
ural synchronization between human player part and machine part, we follow
FolkDuet and adopt a subdivision [b1, b2, · · · , bK] containing the within-measure
position of each note to encode the beat information. Where bi can be obtained
by modulating the onset time ti of each note by 16. However, this is not the ﬁnal
representation of music data and we are unable to feed non-numeric notes into
a neural network. We need to convert these symbols to integers by constructing
a mapping table from pitch and duration to integer numbers.
MusPy is used to extract the corresponding sequence of notes, which are
applied to create mapping tables that map from each of possible pitches and
durations to the unique integer. Since the pitch and duration sequences are all
non-numeric, it is further necessary to convert these sequences into corresponding
integer sequences based on the mapping tables. As shown in Fig. 1
3
Generative Adversarial Imitation Learning
In this work, we extend the generative adversarial imitation learning (GAIL)
to music generation. This framework applies GAN to solving imitation learning
problems and can directly learn a policy from giving dataset. Speciﬁcally, it
consists of a generator Gθ which aims to generate note sequences similar to real
note sequences and a discriminator Dφ which aims to distinguish the real note

MusicGAIL
509
Fig. 1. Illustration of music data preprocessing procedure.
sequences from the generated sequences. They are parameterized with θ and φ,
respectively.
What is more important, GAIL directly takes the optimal strategy as the
learning objective, the strategy and reward function are represented by neural
networks. Therefore, GAIL simpliﬁes the steps of iterative training in IRL and
more eﬃcient than general IRL algorithm.
In GAIL, generator and discriminator satisfy the following objective function:
min
G max
D Epreal[Dφ(s, n)] + EGθ[1 −Dφ(s, n)],
(2)
where (s, n) denotes state-action pairs. n denotes each note generated from gen-
erator.
3.1
MusicGAIL Framework
For our task of music generation, we proposed MusicGAIL, a generative adver-
sarial imitation learning framework base on the Sequence Tutor [11] proposed by
Jaques, RL-Duet [6] and later expanded upon by Jiang’s FolkDuet [15] model.
Just like RL-Duet and FolkDuet, our framework consists of melodic generator,
style-discriminator, cooperation-rewarder and critic-network. In order to apply
the GAIL algorithm and achieve a globally coherent and rhythmic structure in
music, we need to model generating corresponding machine counterparts accord-
ing to human input as a RL problem. Figure 2 shows the overall framework of
MusicGAIL. The novelty of our work is using GAIL algorithm to learn a pol-
icy from given expert demonstrations during the training process and adopt
the Transformer [12], replacing the recurrent neural network most commonly
used in music generative model. More speciﬁcally, our approach is that style-
discriminator is trained to distinguish generated melody samples from the real
expert samples and cooperation-rewarder in our model is the same as [15]. It is
trained on two pre-trained models, Bach-HM and Bach-M [15], to make a judge-
ment on the degree of interaction between human player and machine part, in

510
Y. Liao et al.
Fig. 2. GAIL framework for music generation.
this case the melodic generator is trained by the actor-critic algorithm with
rewards provided by style-discriminator and cooperation-rewarder to learn an
optimal policy to generate notes, which aims to generate melodies whose style
is as similar as possible to the real melody samples. Critic network is a feedback
network that can score each note generated by melodic generator and the reward
function in critic network consists of the probability output of style-discriminator
and output of cooperation-rewarder. The two training phases of melodic gener-
ator and style-discriminator are executed alternately. It is worth noting that
during the training of melodic generator, cooperation-rewarder is ﬁxed.
3.2
Melodic Generator
For the melodic generator, the probability of the note sequence is deﬁned as the
joint probability of all the tokens:
Gθ (n1:K) =
K−1

k=1
Gθ (ak = nk+1 |sk = n1:k ) ,
(3)
where n1:K is the note sequence. ak is the action to select the next note nk+1,
sk is the state of the previous prediction n1:k and K is the length of sequence.
We obtain the generated note sequence by sampling from the distribution. The
melodic generator is trained by the Actor-Critic algorithm with generalized
advantage estimator(GAE) [19] to learn a policy π

n(m)
k
|Sk

and a value Vk to
predict the next note, which aims to maximize the expected long-term discounted
reward of note n(m)
k
.
Rk =
K
	
i=0
γi

R

s

i+k + βR(c)
i+k

,
(4)
where R(s) and R(c) are two rewards provided by style-discriminator and coop-
eration rewarder, γ is the discount factor, and β is the weight factor.

MusicGAIL
511
We randomly select one melody from training dataset as human player part
n(h)
1:K =

n(h)
1 , n(h)
2 , · · · , n(h)
K

in each training iteration. The melodic generator
acquired the observation of the current state sk from the already played notes
in the human player part

n(h)
1 , n(h)
2 , · · · , n(h)
k′−1

, the already generated machine
part

n(m)
1
, n(m)
2
, · · · , n(m)
k−1

, and generate the new note nk base on sk.
In this work, GTrXL is used to form the melodic generator and discriminator,
replacing RNNs. More details about GTrXL can be found in [16]. The main
improvements of this architecture compared to the original transformer are as
follows:
– Reordered the layer normalization modules in transformer block.
– The standard residual connections after Multi-Head Attention and Position-
Wise MLP are replaced with gating layers.
3.3
Style-Discriminator
The purpose of the style-discriminator is to capture the style of folk melodies in
expert samples and strive to classify the style between the real note sequences
and the generated sequences from melodic generator. It takes the current state
sk, generated action ak (i.e. state-action pairs (sk, ak)) and the state-action
pairs of expert samples (sE, aE) as the input. The outputs of style-discriminator
represent the probability of discriminator makes a judgment on (s, a) generated
from the expert policy or agent policy, which satisﬁed the following expression:
PG, PE = soft max (Wt [D (sk, ak) , D (sE, aE)]) ,
(5)
where Wt is the trainable weight matrix, PE is the probability of judging note
sequences are generated from the expert samples and it will be maximized during
optimizing the style-discriminator with cross-entropy loss, PG is the probability
of judging note sequences are generated from the melodic generator and it can
be interpreted as the reward function to provide the learning signal to train
generator policy [14].
When the values of PG and PE approach 0.5, style-discriminator will not be
able to distinguish between the generated samples and real expert samples. In
this case, the samples generated by melodic generator can perfectly ﬁt the real
expert samples distribution (i.e. the style of the generated melodies are extremely
similar to the real melodies).
4
Experimental Results
In the experiment section, we implemented the code of our MusicGAIL model
for experiments by programming language Python 3.10 under the deep learn-
ing framework “Pytorch” and trained our model on a server with 40GB video
memory NVIDIA A100 GPU. We used the default parameters in FolkDuet [15]

512
Y. Liao et al.
and GTrXL [16]. After the training of MusicGAIL model, we can obtain some
high-quality melody samples1
4.1
Dataset
In order to test and evaluate our MusicGAIL model better, folksong dataset in
Essen Associative Code and Folksong Database2 is employed on our experiment.
It mainly consists of folksongs from Germany and China and their folksongs
account for about 67% and 28% respectively. Due to the Chinese folksongs con-
tained relatively complete information on area, style and phrase, we select the
2241 folksongs from China as our training dataset instead of Germany.
4.2
The Training Process of MusicGAIL
The training procedure of MusicGAIL consists of three main stages: First of all,
we randomly select folksongs in training dataset, shuﬄe or randomly transpos-
ing pitch of each melody and take them as the corresponding countermelody to
form fake duets, then warm up the melodic generator with fake duets. Next, we
obtain the cooperation reward on Bach-HM and Bach-M. The melodic gener-
ator and style-discriminator are trained alternatively. Finally, we update style-
discriminator and update the melodic generator respectively after normalizing
all the rewards. It is worth noting that learning rate in our model will be updated
every 10 training epoch. We repeat the aforementioned model training process
until convergence.
4.3
Comparison and Evaluation
In this work, we compare the performance of our MusicGAIL model with three
baselines: a maximum likelihood estimation (MLE) model, a DuetLSTM model
and a RL-Duet [6] model. Among which, the DuetLSTM3 model is our implemen-
tation of an open source project that generating melodies with LSTM network.
For a fair comparison between DuetLSTM model and our MusicGAIL model,
DuetLSTM needs to be trained on the duets dataset and all settings should
be the same as our MusicGAIL except LSTM network. On the other hand,
following the previous work on music generation, we evaluate our MusicGAIL
model from the subjective and objective aspects. Speciﬁcally, we implement it
on quantitative metrics evaluation and user listening evaluation. It is noted that
we only compare our model with MLE and DuetLSTM in the user listening
evaluation since RL-Duet didn’t publish their results in subjective evaluation.
1 Melody samples are available at https://github.com/AcademicLoser/MusicProject.
2 Database is available at http://www.esac-data.org.
3 Code is available at https://github.com/musikalkemist/generating-melodies-with-
rnn-lstm.

MusicGAIL
513
Quantitative Metrics Evaluation. We will evaluate the features of music
generated from our model on several conventional objective metrics proposed in
[20]: PC/bar, PI, IOI, NLH, PCH. These metrics can evaluate the generative
model by comparing the statistical diﬀerence between the real dataset and the
generated samples, the closer the values of these metrics are to the dataset, the
more similar the style of generated melody is to Chinese folk style. Note that,
for NLH and PCH, we adopt the earth moving distance between the generated
melody and folk dataset in RL-Duet [6] to measure the similarity between the
style of generated melody and Chinese folk style. We calculated the mean values
and the standard deviations of these metrics on 10 runs.
Table 1. Results of quantitative metrics evaluation of MLE, DuetLSTM, RL-Duet and
MusicGAIL. Results of RL-Duet with * are showed in [15].
PC/bar
PI
IOI
NLH
PCH
Folk Dataset 3.97
2.86
2.41
–
–
MLE
4.36±0.14
3.02±0.09 2.91±0.09
0.032±0.009
0.015±0.001
DuetLSTM
3.34±0.17
3.16±0.11
3.05±0.07
0.068±0.001
0.021±0.002
RL-Duet
3.23±0.01* 4.02±0.01* 3.64±0.02* 0.055±0.002*
0.017±0.001*
MusicGAIL
3.79±0.15 2.64±0.08
2.13±0.12 0.018±0.004 0.01±0.001
We can observe in Table 1 that note sequences generated from MLE model
are extremely messy and dense because of the exposure bias problem, its PC/bar
is the highest, 4.36.
Furthermore, it is a challenging task for RL-Duet to imitate Chinese folk
style due to it does not contain style information and its PI, IOI, NLH and PCH
signiﬁcantly deviate from the folk dataset. By contrast, these quantitative met-
rics of our model are closer to folk dataset than three baselines, which indicated
MusicGAIL can generate the more harmonious note sequences with reasonable
intervals and imitate Chinese folk style better.
Users’ Evaluation. For the subjective evaluation, we recruited 215 volunteers
from the Internet via our social circles to establish an expert review panel and
conduct a listening test evaluation for the generated melody samples. Most of
these volunteers have learned music theory or have learned to play at least one
musical instrument. Following the previous work, we designed a simple online
survey4.
Speciﬁcally, we randomly select 4 music samples from each of the MLE,
DuetLSTM, MusicGAIL and a piece of original music from dataset. Each volun-
teer needs to listen to the origin music and score the melody samples generated
from diﬀerent models in a range of 0 to 10, among which “0” means the worst
4 The survey is available at https://github.com/AcademicLoser/MusicProject.

514
Y. Liao et al.
Fig. 3. The scores of subjective evaluation are divided into 2 groups based on question
1 in the survey: with (The selection in question 1 is “Yes”) or without (The selection
in question 1 is “No”) a certain musical foundation.
0
2
4
6
8
10
Melody
Harmony
Rhythm
Coherence Folk Style
Emotion
Overall
rating
MLE
DuetLSTM
MusicGAIL
0
2
4
6
8
10
Melody
Harmony
Rhythm
Coherence Folk Style
Emotion
Overall
rating
MLE
DuetLSTM
MusicGAIL
0
2
4
6
8
10
Melody
Harmony
Rhythm
Coherence Folk Style
Emotion
Overall
rating
MLE
MusicLSTM
MusicGAIL
(a) Less than 0.5 hour
(b) 0.5-1 hour
(c) More than 1 hour
Fig. 4. The scores of subjective evaluation are divided into 3 groups based on question
2 in the survey: (a) less than 0.5 h of listening to music every day, (b) 0.5–1 hour of
listening to music every day and (c) more than 1 h of listening to music every day.
and “10” means the best. Next, we collect the survey of all volunteers, ﬁltered
out outliers that are too high or too low scores compared to other scores and
averaged all the scores. Figure 3 and Fig. 4 show the scores of subjective evalu-
ation from 2 aspects: whether the volunteers have a certain musical foundation
and how long they listen to music every day. In general, the subjective scores of
MusicGAIL are signiﬁcantly higher than baselines in terms of melody,harmony,
rhythm, folk style and emotion, especially folk style. Interestingly, the subjective
scores of the three models in coherence are very close. It indicated that although
MLE and DuetLSTM are diﬃcult to imitate the style of music due to their own
defects, these defects do not directly aﬀect the coherence of generated music. On
the other hand, we can observe that the subjective scores from the group that
listened to music for a longer time each day are lower overall. A potential reason
may be that volunteers who listen to music for a longer time every day will have
higher requirements for music quality and diversity, hence it is not surprising
that they rate lower subjective scores.
However, it is reasonable for us to believe the melody sequences generated
from MusicGAIL shows a clearer and more natural style and diversity, indicating
that our proposed model is capable of capturing music style.

MusicGAIL
515
5
Conclusion
In this paper, we have designed a GAIL framework for generating the counter-
melody for folk melodies based on the previous research, which avoids labori-
ously specifying the reward in RL based music generation models. We evalu-
ate the MusicGAIL model from the objective and subjective aspects and the
experimental results of objective evaluation show that MusicGAIL can gener-
ate the harmonious melody pieces with reasonable intervals and achieve better
style imitation of Chinese folk than baselines. The volunteers’ evaluation show
that MusicGAIL obtain the signiﬁcantly higher scores than baselines in terms
of melody, harmony, rhythm, folk style and emotion, especially folk style. For
future work, we would like to continue to explore and extend application scope
of our model to other more styles of music.
References
1. Sperduti, A.: An overview on supervised neural networks for structures. In: Inter-
national Conference on Neural Networks (1997)
2. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
1735–1780 (1997)
3. Cho, K., et al.: Learning phrase representations using RNN encoder-decoder for
statistical machine translation. Comput. Sci. (2014)
4. Eck, D., Schmidhuber, J.: A ﬁrst look at music composition using LSTM recurrent
neural networks. Istituto Dalle Molle Di Studi Sull Intelligenza Artiﬁciale (2008)
5. Bengio, S., Vinyals, O., Jaitly, N., Shazeer, N.: Scheduled sampling for sequence
prediction with recurrent neural networks (2015)
6. Jiang, N., Jin, S., Duan, Z., Zhang, C.: RL-Duet: online music accompaniment gen-
eration using deep reinforcement learning. In: Proceedings of the AAAI Conference
on Artiﬁcial Intelligence (2020)
7. Goodfellow, I.J., et al.: Generative adversarial networks (2014)
8. Bau, D., Zhu, J.Y., Wulﬀ, J., Peebles, W., Torralba, A.: Seeing what a GAN cannot
generate (2019)
9. Jaques, N., Gu, S., Turner, R.E., Eck, D.: Tuning recurrent neural networks with
reinforcement learning (2016)
10. Kotecha, N.: Bach2Bach: generating music using a deep reinforcement learning
approach (2018)
11. Jaques, N., Gu, S., Bahdanau, D., Hernández-Lobato, J.M., Turner, R.E., Eck,
D.: Sequence tutor: conservative ﬁne-tuning of sequence generation models with
KL-control (2016)
12. Vaswani, A., et al.: Attention is all you need. arXiv preprint arXiv:1706.03762
(2017)
13. Huang, C., et al.: Music transformer (2018)
14. Ho, J., Ermon, S.: Generative adversarial imitation learning (2016)
15. Jiang, N., Jin, S., Duan, Z., Zhang, C.: When counterpoint meets Chinese folk
melodies. In: Neural Information Processing Systems (2020)
16. Parisotto, E., et al.: Stabilizing transformers for reinforcement learning. In: Inter-
national Conference on Machine Learning (2020)

516
Y. Liao et al.
17. Dong, H.W., Chen, K., Mcauley, J., Berg-Kirkpatrick, T.: MusPy: a toolkit for
symbolic music generation (2020)
18. Benetatos, C., VanderStel, J., Duan, Z.: BachDuet: a deep learning system for
human-machine counterpoint improvisation. In: Proceedings of the International
Conference on New Interfaces for Musical Expression (2020)
19. Schulman, J., Moritz, P., Levine, S., Jordan, M., Abbeel, P.: High-dimensional
continuous control using generalized advantage estimation. Comput. Sci. (2015)
20. Yang, L.C., Lerch, A.: On the evaluation of generative models in music. Neural
Comput. Appl. 32(9), 4773–4784 (2020)

Unsupervised Traditional Chinese Herb
Mention Normalization
via Robustness-Promotion Oriented
Self-supervised Training
Wei Li1
, Zheng Yang2
, and Yanqiu Shao1(B)
1 School of Information Science, Beijing Language and Culture University,
Beijing 100081, China
{liweitj47,shaoyanqiu}@blcu.edu.cn
2 School of Traditional Chinese Medicine, Beijing University of Chinese Medicine,
Beijing 100029, China
yangzheng@bucm.edu.cn
Abstract. Herbal prescriptions are a vital aspect of Traditional Chi-
nese Medicine (TCM) treatment. The textual representations of herbs
can vary signiﬁcantly across various TCM documents and records. To
enhance the utilization of this valuable knowledge in contemporary set-
tings, we propose the objective of Traditional Chinese Herb Mention
Normalization by associating them with standardized modern names.
However, supervised approaches face the challenge of data sparsity, as
they require a substantial amount of labeled data, which is particularly
expensive to acquire in the context of TCM. Previous self-alignment
methods solely focus on the mentions and names in the gazetteer, over-
looking crucial contextual information. Drawing from the observation
that mentions often exhibit shared characters with canonical names and
possess similar contextual information related to the targeted symptoms
and co-occurring herbs, we propose an unsupervised method focusing on
promoting robustness. This is achieved by training the model with a self-
supervised objective of recovering the original standard herb mentions
from the perturbed ones, while leveraging a pretrained language model
to capture context information. We argue that the model can develop
the alignment ability by making the representation immune to the possi-
ble perturbations. To evaluate the eﬀectiveness of the proposed method,
we construct a dataset an ancient TCM record dataset. We then enlist
TCM professionals to manually annotate the most prevalent aliases of
the herbs. Our method achieves an accuracy of 89.79, which is practi-
cable in the real-life scenarios. Extensive analysis further validate the
eﬃcacy of the proposed unsupervised method.
Keywords: Traditional Chinese Medicine · Mention Normalization ·
Self-supervised
W. Li and Z. Yang—Equal Contribution.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 517–528, 2024.
https://doi.org/10.1007/978-981-99-8850-1_42

518
W. Li et al.
Fig. 1. An illustration of a prescription-
symptom pair extracted from the TCM
record in our proposed task of herb men-
tion normalization.
Fig. 2. A preliminary depiction of the
eﬃcacy
of
the
robustness-promotion
method in the herb mention normaliza-
tion task. Following self-supervised train-
ing, the semantic space of the herb
expands to encompass its mentions.
1
Introduction
Traditional Chinese Medicine (TCM) holds a signiﬁcant position within the med-
ical system of China and its neighboring regions. One widely practiced TCM
treatment involves the compositionally cooking of TCM soups with various
herbal medicines. The TCM records from ancient documents contain numerous
prescriptions and their corresponding targeted symptoms, serving as an invalu-
able knowledge source for TCM practitioners. However, one herb can exhibit
various expression forms, related to factors such as historical background, place
of production, and method of processing. This can lead to confusion for both
machines and humans, impeding the utilization of historical TCM knowledge,
particularly in situations where the names of the herbs have largely been stan-
dardized in modern TCM. Hence, there is signiﬁcant value in normalizing the
non-standard mentions of TCM herbs found in the TCM records to align with
commonly used canonical names.
Figure 1 presents an example of the TCM record under study. The upper part
displays the prescription, while the lower part exhibits the corresponding symp-
toms. The prescription comprises multiple herbs. Among them, the herb “roasted
aucklandiae radix” (煨木香) should be normalized to “aucklandiae radix” (木
香), and “raw Radix glycyrrhizae” (生甘草) should be normalized to “Radix gly-
cyrrhizae” (甘草). These are instances where the same herb is represented with
diﬀerent processing methods. One challenging case involves the herb “byname of
radix et rhizoma rhei” (川军), which literally translates to the “army of Sichuan
province” in Chinese but refers to the herb “radix et rhizoma rhei” (大黄) within
the TCM context. In such cases, the contextual information, including both the
co-occurring herbs and the targeted symptoms, must be considered to determine
the intended meaning.
Previous works on medical terminology normalization (medical entity link-
ing) have predominantly employed supervised methods [1,13,19]. However, these
supervised methods require a substantial amount of labeled data, which can be
costly to acquire, particularly in specialized ﬁelds such as medicine. This issue
is further exacerbated in the case of TCM scenario, as it not only demands

Unsupervised TCM Mention Normalization
519
expertise in TCM knowledge but also necessitates proﬁciency in reading ancient
Chinese text. Liu et al. [10] propose a self-alignment pretraining method that
leverages the aliases deﬁned in UMLS1, which, unfortunately, is not applicable
to the TCM domain.
In the natural language processing (NLP) community, researchers aim to
deceive the model by manipulating the input text in a way that remains recog-
nizable to human beings [3,4], thereby identifying vulnerabilities in the targeted
models. This process can be viewed, in our observation, as an inverse opera-
tion of aligning unseen mentions to their canonical names, because many of the
mentions can be generated from the canonical names through text perturbation
operations, such as ﬂipping, mask insertion, and deletion. Furthermore, given the
widespread use of pre-trained language models, perturbing the input embeddings
is another promising method to deceive the model [6,8,15]. On the other hand,
incorporating perturbed samples as augmented training data can enhance the
model’s robustness against unseen noise in the inference data [18], enabling the
model to handle variations in the unseen representation forms of the same herb
in our task.
Building upon the aforementioned observations, we propose a self-alignment
method based on perturbation. This method involves creating pseudo positive
samples by perturbing the canonical names found in the records. On one hand,
the unaligned mentions can be considered as a variation of the perturbed (canon-
ical) names, eﬀectively simulating the potential mentions. On the other hand, the
normalization operation serves to restore the canonical names from the perturba-
tion. This transforms the objective of normalization into the goal of enhancing
the model’s robustness against perturbation. (Refer to Fig. 2 for an illustra-
tion.) By instructing the model on recovering the original canonical form from
the perturbed form, we anticipate the model to acquire the capability to nor-
malize unseen mentions. Additionally, as context information, encompassing co-
occurring herbs and targeted symptoms, is valuable for accurate predictions, we
propose a BERT-based model that treats the prescription text and the symptoms
as a sequence pair. This enables the model to leverage the contextual information
surrounding the target mention.
In order to evaluate the eﬀectiveness of the proposed method, we create a
dataset for TCM herb mention normalization using TCM records from ancient
TCM books. A TCM professional is enlisted to label the mentions with their
corresponding canonical herb names. The experimental results demonstrate that
our proposed method achieves an accuracy of 89.79%, outperforming all baseline
methods. Further analysis conﬁrms the eﬀectiveness of our proposed method.
We conclude our contributions as follows,
– We propose a Robustness-promotion Oriented Self-supervised Training
method that learns the ability of TCM mention normalization in an unsuper-
vised manner. The model is subjected to both text-level and embedding-level
1 short for Uniﬁed Medical Language System, available at https://www.nlm.nih.gov/
research/umls/index.html.

520
W. Li et al.
perturbation methods, with the expectation to learn the normalization abil-
ity by restoring the original names from their perturbed representations. The
context of the target mentions is modeled using pretrained language models.
– We create a dataset for evaluating the normalization ability of TCM herb
mentions, which includes labels provided by TCM professionals.
– Extensive experiments and analysis are conducted on the proposed method.
We discover some intriguing and counter-intuitive ﬁndings. For example,
despite the “ﬂipping” operation being less similar to real mentions, it is more
eﬀective than other perturbation methods. This suggests that enhancing the
model’s robustness by appropriately expanding the semantic space can indeed
improve herb mention normalization performance.
2
Approach
This section presents our proposed method, the Robustness-promotion Oriented
Self-supervised Training. Our method comprises three components: the context-
aware Pretrained Language Model (PLM) for TCM records, text-level pertur-
bation, and embedding-level perturbation. The inference process relies on the
restoration ability trained by our method.
Context-Aware Pretrained Language Model Framework
The context information, which includes both surrounding herbs and associated
symptoms, serves as crucial indicators for predicting the correct herb. In fact, the
model can occasionally predict the correct herb even without the mention text,
resembling a ﬁll-in-the-blank herb recommendation task. This objective closely
aligns with the masked language model in BERT [2]. Hence, we propose using
BERT as the backbone model to leverage the context information. To prepare
the TCM record as input for BERT, we suggest converting the set of herbs into
a sequence of characters, separated by commas, and concatenating it with the
symptom text. To diﬀerentiate between symptoms and herbs, distinct segment
IDs are assigned to the symptom sequence and herb sequence. Additionally, a
special“[SEP]” token is inserted between the symptoms and the herbs.
Text Level Perturbation
Input perturbation techniques have been extensively utilized in the ﬁeld of com-
puter vision (CV) [7,17], where the pixel values of the input are subtly altered
within an imperceptible range to deceive the model. In the task of TCM herb
mention normalization, the perturbation operation serves to simulate various
possible forms of herb mentions, while simultaneously expanding the semantic
space of the herb. This expansion enables the herb representation to encom-
pass unseen mentions, thereby enhancing the model’s robustness (as depicted in
Fig. 2). Drawing inspiration from similar endeavors in the ﬁeld of NLP [3,4], we
propose incorporating character-level perturbations to the canonical herb names

Unsupervised TCM Mention Normalization
521
Fig. 3. Illustration of the text-level perturbation method, showcasing insertion masking
and replacement masking on the left, and ﬂipping on the right.
found in TCM records. Subsequently, the model is trained to be invariant to
these perturbations through a self-supervised approach.
The perturbed names are considered as pseudo positive samples, with the
objective of recovering the original standard herb names from the perturbed
mentions. This approach allows us to train the model in a purely unsupervised
manner, ensuring its invariance to unseen mentions that are simulated through
the perturbation operations. Speciﬁcally, we propose the utilization of two types
of character-level perturbations, namely masking and ﬂipping. We refrain from
employing cropping, a commonly used technique, due to the brevity of many
herb names which consist of only two to three Chinese characters. Cropping
such names would result in signiﬁcant information loss. Figure 3 provides a visual
representation of masking and ﬂipping through a rough illustration.
While this approach deviates from the typical representation pattern of most
mentions (as the forged names may not closely resemble actual herb mentions
due to disorder), it expands the semantic space of the representation, thereby
enhancing the model’s robustness. In the experimental section, we will demon-
strate that despite its seemingly less intuitive nature compared to masking, this
method is highly eﬀective.
Dynamic Perturbation Strategy. To introduce more variation in the per-
turbation patterns and ensure the model’s invariance to diﬀerent types of per-
turbations, we propose combining the masking and ﬂipping strategies mentioned
earlier with a dynamic random perturbation strategy. Speciﬁcally, for each mini-
batch of record data, we randomly select a perturbation strategy from inserting
masking, replacing masking, and ﬂipping. To center the semantic space of a
herb around its canonical name, we deﬁne a perturbation ratio ξ that deter-
mines whether the batch of data will be perturbed. If the ratio is met, the
input text will be perturbed, requiring the model to predict the standard herb
name given the correct standard herb names. Inspired by the masking strategy
of Roberta [11] in comparison to the original BERT [2], we dynamically per-
turb the input text at each iteration, ensuring that each mini-batch encounters
diﬀerent perturbation methods.
Embedding Level Perturbation
Similar to CV, the embeddings in NLP can also be perturbed in the continuous
space. In our study, we propose three embedding level perturbation methods:
adversarial perturbation, dropout perturbation, and Gaussian perturbation.

522
W. Li et al.
Adversarial Perturbation. Taking inspiration from the successful FGSM
method in CV [7], we propose to incorporate this gradient-based adversarial
attack method into our perturbation approach. Given the target canonical herb
denoted as y, the objective function of our PLM model as J, the model’s param-
eters as θ, and the input embeddings as x, we introduce a small perturbation
vector η that is added to the original input embeddings, calculated as,
η = ϵsign(∇xJ(x, y, θ))
(1)
where η acts as a perturbation vector that aims to push the model to a point
where it would make incorrect predictions while incurring a small perturbation
cost that is imperceptible to humans (the model makes correct predictions before
the perturbation). The magnitude of the perturbation is restricted by the L2
norm of the noise |η|.
Dropout Perturbation. Inspired by the success of SIMCSE [5], which utilizes
dropout [16] to generate positive samples for contrastive learning, we propose
the incorporation of dropout as a perturbation technique applied to the embed-
ding layer of the BERT model speciﬁcally designed for the TCM herb mention
normalization task.
Gaussian Perturbation. Apart from the two perturbation methods above,
we also propose to add random noise to the embeddings of the sequence. The
random noise is sampled from the Gaussian distribution with an average of 0 and
a standard deviation of ϵ. The purpose of introducing random noise is similar to
that of dropout, aiming to expand the semantic space of herb representation.
Prediction Objective
To predict the speciﬁc standard herb from the perturbed herb representations,
we propose to use the sum of the representation of the ﬁrst and the last character
of the herb from the last hidden layer of the PLM as the herb representation,
which is then matched against the standard herbs after a linear transformation,
calculated as,
hherb = hi + hi+k
(2)
p = softmax(Wouthherb)
(3)
where i indicates the starting position of the herb, k is the character-level length
of the herb, Wout ∈(C × H) is the parameters for the linear transformation,
C is the size of the canonical herb list, while H is the dimension of the hidden
vectors. The objective function used is cross entropy, which is minimized using
the AdamW optimizer [12].

Unsupervised TCM Mention Normalization
523
Inference
During inference, instead of using the perturbed herb name, we utilize the repre-
sentations of the ﬁrst and last characters of the actual mention (without pertur-
bation) to predict the canonical herb name. The representation that corresponds
to the highest probability is matched with the canonical herb, and this matched
herb is chosen as the ﬁnal prediction. In cases where the mention is infrequently
encountered in the extensive record database used to train the PLM, the model
may struggle to capture the relationship between the mention and its context.
To address this, we draw inspiration from Kim et al. [9] and suggest integrating
the Jaccard string matching method with our neural model. This integration
involves combining the prediction probability from our model with the Jaccard
matching score. By doing so, we aim to provide more reasonable responses when
the conﬁdence of the underlying deep learning model is low for a given mention.
Experiment
In this section, we introduce the experiment results.
Data and Setting
To evaluate the eﬀectiveness of our proposed method, we enlisted the assistance
of TCM professionals to annotate the frequently occurring herb mentions and
associate them with their corresponding standard herb names. The test dataset
consisted of 76,712 records, comprising 533 standard herb names and 1,469 non-
standard mentions (each labeled with its corresponding standard herb name). We
omit the records with no attached prescription, and construct regular expressions
to extract herbs from the original records. We only consider the frequently seen
mentions (≥5 times) to mitigate the inﬂuence of noise from the automated
regular expression extraction process.
We use guwen-bert2 as the backbone model. The model is ﬁrst pretrained
with the MLM objective for 5 epochs and then trained with the proposed per-
turbation oriented objective for another 5 epochs. The dropout rate is set to 0.1.
The batch size is set to 14 to accommodate the memory limitations of our GPUs,
which are 2 Nvidia GTX 1080Ti. To ensure the model learns the fundamental
prediction ability, we preserve 50% of the examples without perturbation. Fol-
lowing the approach of FGSM [7], we set ϵ to 1e-5. To mitigate the impact of
randomness, we conduct each experiment ﬁve times and report the average accu-
racy and standard deviation. Accuracy serves as the primary evaluation metric
to assess performance.
Baseline Methods
In this sub-section, we introduce the unsupervised normalization methods that
we take as baselines. Because most of the previous works need labelled data, we
2 GuwenBERT https://github.com/ethan-yt/guwenbert.

524
W. Li et al.
take the unsupervised method from Kim et al. [9] and Yan et al. [20], and adapt
the supervised method from Mondal et al. [14] to the unsupervised setting.
– MARIE [9]: this method incorporates both string matching method (Jaccard,
R/O or editdistance) and BioBERT to calculate the similarity between the
candidate and standard herbs. We use the BERT trained on our TCM record
instead of BioBERT.
– BEL [20]: a basic embedding based entity linking method that calculates
the similarity between the candidate and standard herb names with average
character embeddings.
– EMEL [20]: adapts multi-instance learning paradigm that applies BEL to
detect potential positive candidates and then uses contrastive based multi-
instance learning objective to make the representation of the mentions closer
to the corresponding standard herb.
– TripletCNN [14]: this model adapts convolution based triplet network that
applies contrastive learning to reduce the distance between the mention and
its positive candidates, which are selected with Jaccard from the mention
and the standard herb names. Either dynamic or static word2vec character
embeddings are used by triplet network.
2.1
Overall Result
We present the overall results in Table 1. The ﬁndings demonstrate that pre-
vious unsupervised methods generally fail to achieve satisfactory outcomes,
whereas our proposed method exhibits signiﬁcantly improved results. Interest-
ingly, among the baseline methods, the relatively straightforward BEL model
outperforms others. This observation highlights the fact that the pseudo pos-
itive candidates obtained through heuristic matching methods are inadequate
for guiding the model to learn the matching patterns between mentions and
canonical herbs.
2.2
Ablation Study
we evaluate the accuracy of our proposed method by systematically removing
each designed module in Table 2. It shows that all three modules contribute to the
enhancement of prediction accuracy. Notably, the text level perturbation module
exhibits the most substantial improvement, elevating the accuracy from 71.04
to 86.19. Moreover, without the text level perturbation module, the standard
deviation across the ﬁve rounds signiﬁcantly increases from 1.77 to 6.28. This
indicates that the text level perturbation eﬀectively stabilizes the representation,
resulting in greater stability. Similarly, the embedding level perturbation module
also contributes to stability. Although the accuracy improvement (from 86.19 to
87.03) achieved by the embedding level perturbation is not as signiﬁcant as that
of the text level perturbation, it signiﬁcantly reduces the standard deviation
from 1.77 to 0.31, thereby enhancing performance stability.

Unsupervised TCM Mention Normalization
525
Table 1. Accuracy of the baseline meth-
ods and our proposed method.
Model
ACC
MARIE (Jaccard)
58.07
MARIE (R/O)
57.86
MARIE (Editdistance) 47.11
TripletCNN (dynamic) 55.59 ± 0.48
TripletCNN (static)
54.51 ± 0.87
BEL
79.58 ± 0.11
EMEL
22.95 ± 2.26
Proposal
89.79 ± 0.44
Table 2. Ablation study results. Accuracy
and standard deviation are reported.
Method
Acc
Full
89.79 ± 0.44
w/o. Jaccard
87.03 ± 0.31
w/o. Embedding Level 86.19 ± 1.77
w/o. Text Level
71.04 ± 6.28
2.3
Analysis
Performance of Individual Perturbation Method. We show the accu-
racy of diﬀerent perturbation methods for embedding and text levels in Fig. 4a.
Observing the results, we ﬁnd that the embedding level perturbation meth-
ods generally yield similar performance, with the widely-used dropout operation
slightly outperforming the other two methods.
In contrast, the performance of text level perturbation methods exhibits
larger variation. Intuitively, we initially expect that the “replace” and “insert”
methods, which involve replacing a character in the canonical names with a mask
token and inserting a mask token among the characters, respectively, would yield
better results than ﬂipping. This assumption is based on the notion that manip-
ulating mask tokens on the herb characters aligns with the pattern of expected
unseen mentions. However, surprisingly, the ﬂipping operation actually achieves
the best result. We postulate that this outcome arises because our model empha-
sizes the semantic space rather than the superﬁcial text. The ﬂipping operation
can be viewed as an eﬀective approach to expand the semantic space around the
canonical name of the herb without signiﬁcantly distorting the original meaning.
This proximity of the expanded semantic space to unseen mentions contributes
to the superior performance of the ﬂipping operation.
Eﬀect of Symptom Context. Figure 4b displays the accuracy comparison
between inputs with and without symptoms as context under the text level per-
turbation scenario. The results clearly demonstrate that including symptoms as
input signiﬁcantly enhances the accuracy, improving it from 81.78 to 86.19. We
posit that this improvement arises from the strong association between symp-
tom text and the corresponding herb. A herb typically exhibits its own symptom
matching pattern, which aligns with the targeted symptoms for the entire pre-
scription. Consequently, incorporating the symptom information provides valu-
able context that aids in accurately predicting the associated herb.

526
W. Li et al.
(a)
(b)
(c)
Fig. 4. (a) The accuracy of diﬀerent perturbation methods (embedding level and text
level). (b) The accuracy of input with and without symptoms as context. (c) The
accuracy of diﬀerent polluting ratios of text level perturbation. (Color ﬁgure online)
Eﬀect of Polluting Ratio. In Fig. 4c, we present the accuracy results for
diﬀerent polluting ratios. We propose that preserving a portion of the canoni-
cal names from pollution helps maintain the semantic space of a herb centered
around the original canonical name in the hyper-space. The experimental results
corroborate this assumption, as the accuracy initially increases with the pollut-
ing ratio and then declines once the ratio exceeds 0.5. The initial increase can be
attributed to the expansion of the semantic space around the herb through text
level perturbation, facilitating the inclusion of unseen mentions within the herb’s
semantic space. However, beyond a certain pollution threshold, the centroid of
the space becomes inﬂuenced by the perturbation, causing the canonical herb
name to deviate from the centroid of the herb’s semantic space. Consequently,
this shift in the centroid adversely impacts the prediction accuracy.
3
Conclusion
In this paper, we introduce the task of herb mention normalization in TCM
records and release a corresponding dataset to facilitate further research in
this area. We propose a novel approach that leverages unsupervised learning
and perturbation-based recovery to address this normalization task without the
need for manual labeling. To capture the contextual information, including sur-
rounding herbs and targeted symptoms, we employ a pretrained language model
when processing the textual TCM records. Experimental results demonstrate the
eﬀectiveness of our method, achieving an accuracy of 89.79. Through extensive
analysis, we uncover interesting ﬁndings, such as the surprising eﬀectiveness of
the“ﬂipping” perturbation operation, which outperforms other methods despite
not closely resembling real mentions. This highlights the importance of enhanc-
ing model robustness by appropriately expanding the semantic space, leading to
improved herb mention normalization performance.

Unsupervised TCM Mention Normalization
527
Acknowledgements. This research project is supported by National Key R&D Pro-
gram of China (2020YFC2003100, 2020YFC2003102), Innovation Team and Talents
Cultivation Program of National Administration of Traditional Chinese Medicine. (No:
ZYYCXTD-C-202001), Science Foundation of Beijing Language and Culture Univer-
sity (supported by “the Fundamental Research Funds for the Central Universities”)
(No. 21YBB19)
References
1. Bhowmik, R., Stratos, K., de Melo, G.: Fast and eﬀective biomedical entity linking
using a dual encoder. In: Proceedings of the 12th International Workshop on Health
Text Mining and Information Analysis, pp. 28–37 (2021)
2. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirec-
tional transformers for language understanding. In: Burstein, J., Doran, C., Solorio,
T. (eds.) Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2–7, 2019, Volume 1 (Long and
Short Papers), pp. 4171–4186. Association for Computational Linguistics (2019).
https://doi.org/10.18653/v1/n19-1423, https://doi.org/10.18653/v1/n19-1423
3. Ebrahimi, J., Rao, A., Lowd, D., Dou, D.: Hotﬂip: white-box adversarial examples
for text classiﬁcation. In: Proceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short Papers), pp. 31–36 (2018)
4. Gao, J., Lanchantin, J., Soﬀa, M.L., Qi, Y.: Black-box generation of adversarial text
sequences to evade deep learning classiﬁers. In: 2018 IEEE Security and Privacy
Workshops (SPW), pp. 50–56. IEEE (2018)
5. Gao, T., Yao, X., Chen, D.: Simcse: simple contrastive learning of sentence embed-
dings. In: Moens, M., Huang, X., Specia, L., Yih, S.W. (eds.) Proceedings of the
2021 Conference on Empirical Methods in Natural Language Processing, EMNLP
2021, Virtual Event / Punta Cana, Dominican Republic, 7–11 November, 2021,
pp. 6894–6910. Association for Computational Linguistics (2021). https://doi.
org/10.18653/v1/2021.emnlp-main.552, https://doi.org/10.18653/v1/2021.emnlp-
main.552
6. Garg, S., Ramakrishnan, G.: BAE: bert-based adversarial examples for text clas-
siﬁcation. In: Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,
Online, November 16–20, 2020, pp. 6174–6181. Association for Computational Lin-
guistics (2020). https://doi.org/10.18653/v1/2020.emnlp-main.498, https://doi.
org/10.18653/v1/2020.emnlp-main.498
7. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial
examples. In: Bengio, Y., LeCun, Y. (eds.) 3rd International Conference on Learn-
ing Representations, ICLR 2015, San Diego, CA, USA, May 7–9, 2015, Conference
Track Proceedings (2015). https://arxiv.org/abs/1412.6572
8. Jin, D., Jin, Z., Zhou, J.T., Szolovits, P.: Is BERT really robust? a strong baseline
for natural language attack on text classiﬁcation and entailment. In: The Thirty-
Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second
Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth
AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020,
New York, NY, USA, February 7–12, 2020, pp. 8018–8025. AAAI Press (2020).
https://ojs.aaai.org/index.php/AAAI/article/view/6311

528
W. Li et al.
9. Kim, H.K., et al.: Marie: a context-aware term mapping with string matching and
embedding vectors. Appl. Sci. 10(21), 7831 (2020)
10. Liu, F., Shareghi, E., Meng, Z., Basaldella, M., Collier, N.: Self-alignment pretrain-
ing for biomedical entity representations. In: Proceedings of the 2021 Conference
of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pp. 4228–4238 (2021)
11. Liu, Y., et al.: Roberta: a robustly optimized BERT pretraining approach. CoRR
abs/1907.11692 (2019). https://arxiv.org/abs/1907.11692
12. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: 7th Interna-
tional Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6–9, 2019. OpenReview.net (2019). https://openreview.net/forum?
id=Bkg6RiCqY7
13. Miftahutdinov, Z., Kadurin, A., Kudrin, R., Tutubalina, E.: Medical concept nor-
malization in clinical trials with drug and disease representation learning. Bioin-
formatics 37(21), 3856–3864 (2021)
14. Mondal, I., et al.: Medical entity linking using triplet network. In: Proceedings of
the 2nd Clinical Natural Language Processing Workshop, pp. pp. 95–100 (2019)
15. Sato, M., Suzuki, J., Shindo, H., Matsumoto, Y.: Interpretable adversarial per-
turbation in input embedding space for text. In: Lang, J. (ed.) Proceedings
of the Twenty-Seventh International Joint Conference on Artiﬁcial Intelligence,
IJCAI 2018, July 13–19, 2018, Stockholm, Sweden, pp. 4323–4330. ijcai.org (2018).
https://doi.org/10.24963/ijcai.2018/601, https://doi.org/10.24963/ijcai.2018/601
16. Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:
Dropout: a simple way to prevent neural networks from overﬁtting. J. Mach. Learn.
Res. 15(1), 1929–1958 (2014). https://doi.org/10.5555/2627435.2670313, https://
dl.acm.org/doi/10.5555/2627435.2670313
17. Szegedy, C., et al.: Intriguing properties of neural networks. In: Bengio, Y., LeCun,
Y. (eds.) 2nd International Conference on Learning Representations, ICLR 2014,
Banﬀ, AB, Canada, April 14–16, 2014, Conference Track Proceedings (2014).
https://arxiv.org/abs/1312.6199
18. Wang, Y., Bansal, M.: Robust machine comprehension models via adversarial train-
ing. In: Walker, M.A., Ji, H., Stent, A. (eds.) Proceedings of the 2018 Conference
of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June
1–6, 2018, Volume 2 (Short Papers), pp. 575–581. Association for Computational
Linguistics (2018). 10.18653/v1/n18-2091, https://doi.org/10.18653/v1/n18-2091
19. Xu, D., Zhang, Z., Bethard, S.: A generate-and-rank framework with semantic type
regularization for biomedical concept normalization. In: Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics, pp. 8452–8464
(2020)
20. Yan, C., Zhang, Y., Liu, K., Zhao, J., Shi, Y., Liu, S.: Enhancing unsupervised
medical entity linking with multi-instance learning. BMC Med. Inform. Decis. Mak.
21(9), 1–10 (2021)

Feature Fusion Gate: Improving
Transformer Classiﬁer Performance
with Controlled Noise
Yu Xiang and Lei Bai(B)
School of Information Science and Technology, Yunnan Normal University,
Juxian Street 768, 650500 Kunming, China
{xiangyu,bailey}@ynnu.edu.cn
Abstract. The pre-trained model based on the Transformer architec-
ture is currently the most widely used model in the ﬁeld of Natural
Language Processing (NLP), and feature fusion technology is the pro-
cess of aggregating features from diﬀerent sources to form an augmented
feature representation that contains more information. In multi-modal or
multi-branch NLP models, feature fusion is a commonly used technique,
but for models with only a single feature source, feature fusion tech-
nology can be diﬃcult to apply. Therefore, this paper proposes a new
probabilistic-controlled late fusion encoder-decoder architecture, called
the Feature Fusion Gate (FFG), based on both feature fusion technology
and Mixup technology to aggregate the feature representations from the
last two layers of the NLP pre-trained model to better capture seman-
tic information in samples. During the aggregation process, FFG utilizes
controlled noise as a regularization technique to help the model achieve
better generalization performance. Experimental results on eight NLP
benchmark datasets show that FFG outperforms three other baseline
methods and consistently achieves signiﬁcant performance improvements
across DistilBERT, BERT and RoBERTa.
Keywords: Feature Fusion · Controlled Noise · Mixup · Natural
Language Processing · Transformer architecture
1
Introduction
In the ﬁeld of deep learning, Feature Fusion Technology (FFT) can utilize the
features extracted from multiple data sources for weighted combination to obtain
more accurate prediction results [1]. In a multi-modal network model, the data
of each modality, such as image, audio, text, etc., can be treated as an indepen-
dent feature source after speciﬁc processing [2]. In a multi-branch (multi-stream)
network model, each branch is usually an independent sub-network that can be
used to process diﬀerent types or formats of input data, thereby providing dif-
ferent feature information [3]. It is obvious that FFT plays a crucial role in the
performance of these network models. How to adjust FFT properly according to
speciﬁc needs to make it adapt to the structure of diﬀerent network models is a
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 529–540, 2024.
https://doi.org/10.1007/978-981-99-8850-1_43

530
Y. Xiang and L. Bai
topic worthy of in-depth research and discussion. For example, in a multi-modal
network model, commonly used feature fusion methods include element-wise
addition, element-wise multiplication, and concatenation of modality features
from diﬀerent modules [4]. In multi-branch (multi-stream) network models, fea-
ture fusion methods generally involve weighted averaging, weighted voting, and
concatenation of the feature data output from diﬀerent feature extraction struc-
tures such as parallel branches, serial branches, or cascading branches [5]. It can
be seen that the number of feature sources, the shape of features, and the speciﬁc
location of feature extraction can all aﬀect the model’s choice of FFT.
On the other hand, for network models with only one feature source, if we
want to use FFT to improve their performance, we ﬁrst need to split or combine
the input sample features from diﬀerent perspectives to obtain multiple new fea-
tures. Then, we can follow the ideas of multimodal and multi-branch models to
feed these new features to speciﬁc modules or sub-networks, and ﬁnally fuse the
output features [6]. The biggest drawback of this approach is that it adds a lot
of additional computational overhead, while the improvement in model perfor-
mance is very limited. Therefore, the starting point of this paper is to not change
the structure of the backbone network, but to relatively simply process its output
features, so that FFT can be seamlessly applied to various mainstream pre-trained
transformer models [7]. In this way, we can expand the application scope of FFT
as much as possible while minimizing the cost of computational resources.
The main contributions of this paper include the following four aspects:
First, inspired by the gating structure of the LSTM network and the feature
Mixup method, a feature fusion gate with adaptive weights named FFG is pro-
posed. Second, based on the diﬀerent ways of extracting sequence classiﬁcation
encodings from the pre-trained model under the transformer architecture, the
model output is split and combined, and FFG is used for the initial Mixup to
achieve feature augmentation. Third, inspired by the idea of Mixup technique,
FFG is used for secondary fusion between diﬀerent text feature sequences within
the same mini-batch to augment the model’s robustness to noise and reduce over-
ﬁtting. Fourth, the proposed downstream decoder network can be seamlessly
integrated with any transformer-based backbone network without the need for
any adjustment or modiﬁcation of their structure.
2
Related Works
2.1
The Gating Mechanism of LSTM
The LSTM network controls the transmission of information through three gates:
the forget gate, the input gate, and the output gate [8]. These gates can be in one
of three states - open, closed, or partially open - and the amount of information
that can pass through the gates in each state can be precisely controlled by
a probability σ. When σ is 0, the gate is closed and no information can pass
through. When σ is 1, the gate is fully open and all information can pass through.
When σ is between 0 and 1, the gate is partially open and information can pass
through with a probability of σ [9].

FFG: Improving Transformer Classiﬁer Performance with Controlled Noise
531
Fig. 1. The recurrent neural network structure of LSTM, where the three linear layers
from left to right correspond to the probability control mechanisms of the forget gate,
input gate, and output gate, respectively, and are marked as σ.
In neural networks, the output of the sigmoid non-linear function can be used
to satisfy the requirements of σ, so it is introduced to implement the control pro-
cess of information transmission. In the LSTM network structure shown in Fig. 1,
taking the input gate as an example, a copy of the feature X = cat (Xt−1, Xt)
is sigmoid activated after passing through a fully connected layer σ and obtains
Xs. Then another copy of X is also passing through a fully connected layer and
activated by tanh, resulting in Xt. Thus, Xs and xt are multiplied element-wise
to implement the corresponding gate control. The formula for this probability
control mechanism is shown below:
Xst = tanh (fc (X)) ⊗sigmoid (fc (X))
(1)
2.2
Mixup
The standard Mixup method was ﬁrst proposed in 2017 as a simple and eﬀective
image augmentation technique [10]. It aggregates the features of two images and
their corresponding labels using linear interpolation to generate an augmented
image and its pseudo-label. The main advantage of the standard Mixup method
is that it generates beneﬁcial noise by using the weighted average of features
in the original samples, reducing the sensitivity of the model to harmful noise
in the training samples and improving its generalization ability and robustness.
Additionally, Mixup can aggregate diﬀerent original sample pairs to generate
augmented samples, even at high data augmentation magnitudes, without gen-
erating duplicate results. This greatly ampliﬁes the diversity of training samples,
eﬀectively improving the model’s prediction performance while reducing the risk
of overﬁtting.
The basic idea of standard Mixup is to linearly aggregate two sample label
pairs (xi, yi) and (xj, yj) in the training set Dtrain, to generate a new sample

532
Y. Xiang and L. Bai
pseudo-label pair (xmix, ymix) as an augmented input to train the network,
where x is the sample and y is its label. This aggregation process can be expressed
by the following formulae:
xmix = λ · xi + (1 −λ) · xj
ymix = λ · yi + (1 −λ) · yj
(2)
where λ is the weight coeﬃcient sampled from the Beta distribution, then λ ∈
[0, 1] and λ ∼Beta (α, α).
Fig. 2. A classiﬁcation encoder-decoder architecture based on FFG and Transformer
layer, which uses a three-level probability control gate mechanism. Additionally, both
feature augmentation branches each generate three controlled noise Mixup branches,
and these six feature branches are ultimately weighted and aggregated under FFG
control and fed to the classiﬁcation head. In general, it is a structure of 1-2-6-1.

FFG: Improving Transformer Classiﬁer Performance with Controlled Noise
533
3
Proposed Methodology
Our research approach is mainly to borrow the gating mechanism of the LSTM
network to control the transmission of feature information. In addition, we also
borrow the standard Mixup technique to perturb the feature information with
beneﬁcial noise. As shown in Fig. 2, to achieve these goals, we ﬁrst constructed
a simple FFG structure, consisting of one fully connected layer (In most cases,
this layer has twice the number of neurons as the input feature dimension size.)
and one softmax activation layer. It transforms the input feature tensor into a
probability distribution used to control the proportion of feature information in
Mixup, i.e. λ and 1 −λ in Formula 2, thereby achieving controlled noise per-
turbation of features. Secondly, we also constructed an ES module to rearrange
the features after multiple Mixup operations. It consists of two fully connected
layers (the ﬁrst layer has 384 neurons and the second layer has 192 neurons)
to achieve a certain degree of dimensionality reduction operation on the input
feature vector.
For the Transformer-based NLP model, there are two ways to obtain the
semantic encoding of the entire input sequence for downstream classiﬁcation
tasks. One way is to extract the feature vector Cln corresponding to the [CLS]
token from the hidden state HSn output by the last layer of the model. The
other way is to obtain the sequence feature vector An by taking the aver-
age of HSn along the sequence dimension by using the Γ operation, that is
An = Γ (HSn, dim = 1), i.e., an average pooling operation over the sequence
dimension. Therefore, in order to construct a dual-branch network structure, we
extract the feature vectors Cln−1 and An−1 of the hidden state HSn−1 output
by the second to last layer of the model, where An−1 = Γ (HSn−1, dim = 1).
Then, we concatenate them with their corresponding feature vectors Cln and
An by the C operation. Finally, a copy of each of the concatenated vectors are
fed into their respective FFGs to obtain two probability distributions {pCl, qCl}
and {pA, qA}. Next, we perform Mixup operation on the corresponding vectors
separately in the two branches. Then, in order to accelerate the convergence of
the model, the fused results are layer normalized to obtain the augmented fea-
ture vectors Clf and Af. This process is denoted as Σ and is represented by the
following formula:
Clf = LayerNorm (pCl · Cln + qCl · Cln−1)
Af = LayerNorm (pA · An + qA · An−1)
(3)
In the next stage, three copies of both Clf and Af are shuﬄed ran-
domly across the mini-batch dimension to obtain Clf
i and Af
j respectively, i.e.
Clf
i = Φ

Clf
ci, dim = 0

and Af
j = Φ

Af
cj, dim = 0

, where Clf
c i is a copy of
Clf, Af
cj is a copy of Af, and i, j = 1, 2, 3. Then Clf
i and Af
j are fed into their cor-
responding FFGs and the probability distributions

pcf
i , qcf
i

and

pAf
j , qAf
j

are obtained. Using the strategy shown in Eq. 4, Mixup is performed between
Clf and Clf
i , as well as between Af and Af
j , to obtain Cls
i and As
j respectively.

534
Y. Xiang and L. Bai
In this way, our method enhances the ability of the model to resist harmful noise
by treating Clf
i and Af
j c as controlled noise and fusing them into Clf and Af,
while avoiding the introduction of out-of-domain noise during Mixup.
Cls
i = LayerNorm

pcf
i
· Clf + qcf
i
· Clf
i

As
j = LayerNorm

pAf
j
· Af + qaf
j
· Af
j

(4)
In the third stage, all Cls
i and As
j are concatenated together after being
processed by their corresponding ES modules to obtain the global feature repre-
sentation Ow. As a result, a copy of Ow is fed into the last FFG for aggregating
features to obtain the probability distribution {pi, qj|i, j = 1, 2, 3}, and the out-
puts of all six feature branches are fused under its control using formula 5 to
derive the ﬁnal feature vector Om, which is subsequently fed to the classiﬁcation
head (It consists of two fully connected layers, with 96 and the number of clas-
siﬁcation categories neurons respectively.). Thus, the feature fusion under the
probability control at three levels is proposed and shown as Algorithm 1, where
the features output by the backbone network undergo a process of one-to-two
splitting, followed by six-way splitting, and ﬁnally merging into one. Therefore,
FFG can be regarded as an in-domain data augmentation technique in the mini-
batch space [11,12].
Om = LayerNorm
⎛
⎝
3

i=1
pi · Cls
i +
3

j=1
qj · As
j
⎞
⎠
(5)
4
Experiments and Results
4.1
Benchmark Datasets and Models
We conducted a detailed test of the eﬀectiveness of FFG based decoder on eight
NLP benchmark datasets using three diﬀerent pre-trained backbone networks.
The experimental results are shown in Table 1. These eight datasets include:
SST-2 [13], SST-5 [13], TREC-Coarse [14], TREC-Fine [14], MRPC (Microsoft
Research Paraphrase Corpus), RTE (The Recognizing Textual Entailment),
SUBJ [15] and AG-News [16], all of which come from HuggingFace Datasets
or the oﬃcial website of the dataset. The three backbone network models
include: DistilBERT-base-uncased [17], BERT-base-uncased [18] and RoBERTa-
base [19], all of which come from the HuggingFace Model Hub. The reason
for choosing these transformer-based pre-trained language models is that they
have achieved remarkable success in various NLP tasks, and any performance
improvement on them can result in signiﬁcant gains [20,21]. Due to the lack of
a dedicated test set in the RTE dataset, we randomly sampled 30% of the data
from its training set to construct a test set.

FFG: Improving Transformer Classiﬁer Performance with Controlled Noise
535
Algorithm 1: FFG Aggregation Method Based on Controlled Noise
Input: Hidden states HSn and HSn−1
Output: Aggregated feature vector Om
1 Cln ←HSn [0, :]
2 Cln−1 ←HSn−1 [0, :]
3 An ←Γ (HSn, dim = 1)
4 An−1 ←Γ (HSn−1, dim = 1)
5 {pCl, qcl} ←FFG (C (Cln, Cln−1))
6 {pA, qA} ←FFG (C (An, An−1))
7 Clf ←Σ (Cln, Cln−1, pCl, qCl)
8 Af ←Σ (An, An−1, pA, qA)
9 foreach i, j = 1, 2, 3 do
10
Clf
ci
copy
←−−−Clf
11
Af
cj
copy
←−−−Af
12
Clf
i ←Φ

Clf
ci, dim = 0

13
Af
j ←Φ

Af
cj, dim = 0

14

pcf
i , qcf
i

←FFG

C

Clf, Clf
i

15

pAf
j
, qAf
j

←FFG

C

Af, Af
j

16
Cls
i ←ES

Σ

Clf, Clf
i , pcf
i , qcf
i

17
As
i ←ES

Σ

Af, Af
j , pAf
j
, qAf
j

18 end
19 Ow ←C3
i,j=1

Cls
i , As
j

20 {pi, qj|i, j = 1, 2, 3} ←FFG (Ow)
21 Om ←Σ

Cls
i , As
j, pi, qj|i, j = 1, 2, 3

22 return Om
4.2
Baselines
In the experiments, we compared FFG with the following three baseline methods:
– CLS: The encoded sequence feature vector of the corresponding [CLS] token
in the hidden state HSn output by the last layer of the backbone network is
directly fed to the classiﬁcation head.
– Mean: the feature encoding of each token in the hidden state HSn is averaged
to obtain a feature vector for the entire sequence, which is then fed to the
classiﬁcation head.
– S-Mixup: Retaining the same downstream network structure as FFG, only
performing Σ operations on the feature vectors with the same probability,
which is equivalent to setting λ to 0.5 in Eq. 2.
4.3
Experimental Settings
To ensure consistency in the experimental environment, we only used AdamW
[22] as the optimizer for the models, and introduced a cosine learning rate [23]

536
Y. Xiang and L. Bai
with a warm-up step accounting for 10% of the total training steps [24]. The
batch size was set to 32, the maximum learning rate was set to 3e-5, with ϵ set to
1e-8. In addition, all experiments were conducted on a computer with an NVIDIA
RTX 4090 GPU. All pre-trained models were constructed and initialized using
the default settings of the HuggingFace Transformers library [25], and during
the training process of 30 epochs, their parameters were frozen, and only the
downstream network parameters were updated, that is to say, we do not ﬁne-
tune the backbone network. Each experiment was repeated three times with 3
diﬀerent random seeds (2, 42, and 882), and the average of the best performance
from the three predictions was taken as the experimental result.
4.4
Overall Results
From Table 1, it can be observed that regardless of the backbone network used,
FFG demonstrates signiﬁcant performance improvements compared to the base-
line methods across all datasets. Only in the case of using DistilBERT as the
backbone network, FFG shows slightly lower performance compared to S-Mixup
on the RTE dataset. This is because RTE does not provide a separate test
set, and we had to randomly split 30% of the samples from the training set as
the test set during the experiment. This resulted in a signiﬁcant domain shift
during training, and many out-of-domain features appeared during testing. As
a result, the S-Mixup method, which has more intense feature perturbations
during training, achieved the best adaptability. In addition, comparing CLS,
Mean, and S-Mixup, FFG only increases the parameter size by 2.10%, 2.10%,
and 0.28%, respectively. Therefore, the resource cost of FFG is quite limited,
it is a high cost-eﬀectiveness method to improve the prediction accuracy of the
pre-trained language model.
To further investigate the impact of the number of controlled noise Mixup
branches on model performance, we conducted detailed ablation experiments,
and the results are shown in Table 2. In order to maintain the symmetry structure
of the decoder network, the number of branches, denoted as b, was chosen to be
even numbers between 2 and 12 in our experiments. It is evident from the results
that b does have a noticeable impact on performance: as b increases, the model
performance gradually improves. However, once b exceeds a certain threshold,
which in our experiment is 6, the average performance gain diminishes, and in
some datasets, it even decreases. Considering the relationship between b and
the number of trainable parameters in the decoder, we ultimately used only 6
branches during the optimization of the decoder.

FFG: Improving Transformer Classiﬁer Performance with Controlled Noise
537
Table 1. Overall experimental results of diﬀerent downstream aggregation methods
on eight benchmark datasets. The values in the table represent the average prediction
accuracy (%) and variance of the model after running three times with three diﬀerent
random seeds. The best result of the experiment is highlighted in bold.
backbone
dataset
CLS
Mean
S-Mixup
FFG
DistilBERT
SST-2
84.77 ± 0.04
85.06 ± 0.06
83.12 ± 0.86
86.64 ± 0.08
SST-5
46.02 ± 0.01
46.85 ± 0.03
45.72 ± 0.14
48.75 ± 0.23
TREC-Coarse
81.00 ± 0.28
87.73 ± 0.01
85.93 ± 0.25
93.33 ± 0.09
TREC-Fine
55.33 ± 0.49
65.53 ± 0.65
72.87 ± 0.37
82.87 ± 1.85
MRPC
70.36 ± 0.05
71.65 ± 0.13
72.00 ± 0.10
72.73 ± 0.12
RTE
53.50 ± 7.00
54.80 ± 8.47
58.05 ± 1.94
58.01 ± 4.31
SUBJ
95.28 ± 0.01
95.28 ± 0.01
92.32 ± 0.04
95.28 ± 0.00
AG-News
90.49 ± 0.00
90.91 ± 0.03
88.47 ± 0.04
92.91 ± 0.02
model size
66,438,628
66,438,628
68,609,764
68,641,274
BERT
SST-2
86.75 ± 0.12
87.08 ± 0.03
85.17 ± 0.15
88.34 ± 0.10
SST-5
48.10 ± 0.04
48.01 ± 0.09
47.32 ± 0.06
49.46 ± 0.01
TREC-Coarse
87.00 ± 0.28
86.80 ± 0.04
87.47 ± 1.21
94.33 ± 0.05
TREC-Fine
60.07 ± 0.17
61.20 ± 0.52
73.73 ± 0.01
83.33 ± 0.09
MRPC
73.58 ± 0.18
72.75 ± 0.20
73.60 ± 0.28
73.99 ± 0.09
RTE
59.08 ± 8.97
56.40 ± 1.46
62.38 ± 4.96
63.54 ± 5.96
SUBJ
95.62 ± 0.00
95.50 ± 0.01
93.05 ± 0.01
96.08 ± 0.37
AG-News
89.93 ± 0.00
90.61 ± 0.00
87.57 ± 0.08
92.67 ± 0.07
model size
109,557,988
109,557,988
111,729,124
111,760,634
RoBERTa
SST-2
84.02 ± 0.00
83.91 ± 0.02
83.42 ± 0.84
88.32 ± 0.03
SST-5
40.59 ± 0.03
42.96 ± 0.38
45.69 ± 0.05
49.53 ± 0.18
TREC-Coarse
56.40 ± 5.32
73.33 ± 1.05
78.00 ± 2.44
91.67 ± 0.01
TREC-Fine
23.47 ± 1.21
46.73 ± 0.17
58.67 ± 0.69
75.80 ± 0.84
MRPC
69.23 ± 0.02
70.65 ± 0.00
71.98 ± 0.04
74.22 ± 0.16
RTE
54.53 ± 4.59
55.11 ± 8.64
56.72 ± 0.67
59.04 ± 4.59
SUBJ
94.02 ± 0.00
94.06 ± 0.00
90.97 ± 0.09
95.62 ± 0.00
AG-News
91.10 ± 0.00
91.39 ± 0.00
87.84 ± 0.01
93.27 ± 0.01
model size
124,721,380
124,721,380
126,892,516
126,924,026

538
Y. Xiang and L. Bai
Table 2. The impact of diﬀerent numbers of controlled noise Mixup branches in the
text classiﬁcation decoder on the accuracy (%) of the model. The random seeds in the
experiment is ﬁxed at 882, and other settings are unchanged. In addition, the trainable
size represents the total number of trainable parameters (kilo) in the model, and the
best result of the experiment is highlighted in bold.
backbone
dataset
b=2
b=4
b=6
b=8
b=10
b=12
DistilBERT SST-2
86.45
86.66
86.71
86.71
86.45
86.55
SST-5
48.87 47.83
48.73
47.96
48.64
48.01
TREC-Coarse 92.60
93.60
93.60
93.80
93.80
93.40
TREC-Fine
80.00
83.20
84.40
83.00
84.20
83.60
MRPC
71.77
72.58
72.35
72.17
72.35
72.64
RTE
61.04 60.24
60.11
60.51
59.84
60.51
SUBJ
95.80
96.10
96.10
95.90
96.05
96.05
AG-News
92.57
92.79
93.07
93.16
93.07
93.18
average acc
78.64
79.12
79.38
79.15
79.30
79.24
BERT
SST-2
88.30
89.02
88.52
88.41
88.19
88.80
SST-5
50.05
49.32
49.32
49.77
51.49
50.41
TREC-Coarse 94.40 93.60
94.20
93.60
94.20
94.40
TREC-Fine
81.00
82.00
83.60
82.00
82.80
84.00
MRPC
73.86
74.67
74.32
74.15
73.80
73.74
RTE
67.34 65.86
65.73
66.80
67.20
66.13
SUBJ
96.20
96.45
96.40
96.35
96.60
96.45
AG-News
91.92
92.36
92.47
92.84
93.30
93.03
average acc
80.38
80.41
80.57
80.49
80.95
80.87
RoBERTa
SST-2
88.63 88.36
88.25
88.25
88.25
88.25
SST-5
49.68
50.00
49.41
49.55
49.23
49.14
TREC-Coarse 91.20
92.00
91.80
91.40
92.00
92.20
TREC-Fine
73.40
73.60
76.00
77.20
76.00
76.20
MRPC
74.09
74.03
74.67
74.20
74.55
74.44
RTE
57.70
57.70
58.37
57.83
58.23
57.70
SUBJ
95.17
95.17
95.65
95.17
95.08
95.17
AG-News
92.67
92.99
93.37
93.38
93.20
93.59
average acc
77.82
77.98
78.44
78.37
78.32
78.33
trainable size
776.8
1526.7 2278.2 3031.2 3685.8 4541.9
5
Conclusion and Future Work
In this paper, we focus on applying commonly used feature fusion techniques
in multi-modal or multi-branch models to a network structure with only one
modality and one main branch. To achieve this, we propose an encoder-decoder
architecture that does not modify or adjust the structure of the pre-trained

FFG: Improving Transformer Classiﬁer Performance with Controlled Noise
539
backbone network. Speciﬁcally, ﬁrst, we split the outputs of the last two trans-
former layers of the backbone network into two feature branches using diﬀerent
sequence feature encoding methods. We then utilize probabilistic gating tech-
niques for initial feature augmentation fusion. Next, we employ controlled noise
to randomly augment the robustness of features in each feature branch within
the mini-batch space using sequence Mixup method, resulting in six disturbed
feature branches. Finally, we use probabilistic gating techniques again to fuse
these branches into one aggregated outcome, which is fed into the classiﬁcation
head to complete the entire late fusion decoding process.
During the experimentation process, we also discovered some issues that
are worth further investigation and discussion. In multi-classiﬁcation tasks, for
example, on Dataset TREC-Fine, RoBERTa exhibited a signiﬁcant performance
drop (below half of the normal value) when using traditional feature encoding
output methods (CLS). This abnormal behavior was observed regardless of the
random seed used for model initialization. Additionally, it would be interesting to
explore whether ideas such as gated linear units (GLU) [26] can be introduced
to optimize the component of FFG, upgrading the structure of its two fully
connected layers and one non-linear softmax activation layer to a more eﬃcient
form. Lastly, it would be valuable to assess whether FFG can achieve equally
outstanding performance in computer vision classiﬁcation tasks. In our future
work, we will focus on addressing these speciﬁc questions.
References
1. Gao, J., Li, P., Chen, Z., Zhang, J.: A survey on deep learning for multimodal
data fusion. Neural Comput. 32(5), 829–864 (2020). https://doi.org/10.1162/
neco a 01273
2. He, X., Deng, L., Rose, R., Huang, M., Trancoso, I., Zhang, C.: Introduction to the
special issue on deep learning for multi-modal intelligence across speech, language,
vision, and heterogeneous signals. IEEE J. Selected Topics Signal Process. 14(3),
474–477 (2020). https://doi.org/10.1109/JSTSP.2020.2989852
3. Li, C., Huang, X., Tang, J., Wang, K.: A multi-branch feature fusion network
for building detection in remote sensing images. IEEE Access 9, 168511–168519
(2021). https://doi.org/10.1109/ACCESS.2021.3091810
4. Domingues, I., Muller, H., Ortiz, A., Dasarathy, B.V., Abreu, P.H., Calhoun,
V.D.: Guest editorial: information fusion for medical data: early, late, and deep
fusion methods for multimodal data. IEEE J. Biomed. Health Inform. 24(1), 14–
16 (2020). https://doi.org/10.1109/jbhi.2019.2958429
5. Atrey, P.K., Hossain, M.A., El Saddik, A., Kankanhalli, M.S.: Multimodal fusion
for multimedia analysis: a survey. Multimedia Syst. 16(6), 345–379 (2010). https://
doi.org/10.1007/s00530-010-0182-0
6. Ma, S., Shan, L., Li, X.: Multi-window Transformer parallel fusion feature pyra-
mid network for pedestrian orientation detection. Multimedia Syst. 29(2), 587–603
(2023)
7. Chen, S., et al.: TransZero: attribute-guided transformer for zero-shot learning.
arXiv e-prints arXiv:2112.01683v1 (2021)
8. Zhao, F., Feng, J., Zhao, J., Yang, W., Yan, S.: Robust LSTM-Autoencoders for
Face De-Occlusion in the Wild. arXiv e-prints arXiv:1612.08534v1 (2016)

540
Y. Xiang and L. Bai
9. Greﬀ, K., Srivastava, R.K., Koutnik, J., Steunebrink, B.R., Schmidhuber, J.:
LSTM: a search space odyssey. IEEE Trans. Neural Netw. Learn. Syst. 28(10),
2222–2232 (2017). https://doi.org/10.1109/tnnls.2016.2582924
10. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: Mixup: Beyond Empirical
Risk Minimization. arXiv preprint arXiv:1710.09412 (2017)
11. Dong, Y., Hopkins, S., Li, J.: Quantum entropy scoring for fast robust
mean estimation
and improved outlier
detection. In: Advances in Neural
Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer,
F.
d’Alch´e-Buc,
E.
Fox,
and
R.
Garnett
(Eds.),
Vol.
32.
Curran
Asso-
ciates, Inc. (2019). https://proceedings.neurips.cc/paper ﬁles/paper/2019/ﬁle/
a4d92e2cd541fca87e4620aba658316d-Paper.pdf
12. Huang, Y., Zhang, Y., Zhao, Y., Shi, P., Chambers, A.J.: A novel outlier-robust
kalman ﬁltering framework based on statistical similarity measure. IEEE Trans.
Autom. Control 66(6), 2677–2692 (2021)
13. Socher, R., et al.: Recursive deep models for semantic compositionality over a
sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pp. 1631–1642, Seattle, Washington, USA (2013).
Association for Computational Linguistics
14. Li, X., Roth, D.: Learning question classiﬁers. In: COLING 2002: The 19th Inter-
national Conference on Computational Linguistics (2002)
15. Conneau, A., Kiela, D.: SentEval: An Evaluation Toolkit for Universal Sentence
Representations. arXiv preprint arXiv:1803.05449 (2018)
16. Zhang. X., Zhao, J., LeCun, Y.: Character-level convolutional networks for text
classiﬁcation. In: Advances in Neural Information Processing Systems, vol. 28
(NIPS 2015)
17. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: DistilBERT, a Distilled Version
of BERT: Smaller, Faster, Cheaper and Lighter. arXiv e-prints arXiv:1910.01108
(2019)
18. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: pre-training of deep bidi-
rectional transformers for language understanding. arXiv e-prints arXiv:1810.04805
(2018)
19. Liu, Y.: RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv
e-prints arXiv:1907.11692 (2019)
20. He, P., Gao, J., Chen, W.: DeBERTaV3: Improving DeBERTa using ELECTRA-
Style Pre-Training with Gradient-Disentangled Embedding Sharing. In: The
Eleventh International Conference on Learning Representations (2023)
21. OpenAI. ChatGPT: Optimizing Language Models for Dialogue. Open AI, blog
(2022)
22. Loshchilov, I., Hutter, F.: Decoupled Weight Decay Regularization. arXiv e-prints
arXiv:1711.05101 (2017)
23. Shazeer, N., Stern, M.: Adafactor: adaptive learning rates with sublinear memory
cost. In: International Conference on Machine Learning, PMLR, pp. 4596–4604
(2018)
24. Vaswani, A.: Attention Is All You Need. arXiv e-prints arXiv:1706.03762 (2017)
25. Wolf, T., et al.: HuggingFace’s Transformers: State-of-the-art Natural Language
Processing. arXiv e-prints arXiv:1910.03771 (2020)
26. Shazeer, N.: GLU Variants Improve Transformer. arXiv e-prints arXiv:2002.
05202v1 (2020)

Multi-round Dialogue State Tracking
by Object-Entity Alignment in Visual
Dialog
Wei Pang(B)
Beijing Information Science and Technology University, Beijing, China
pangweitf@bistu.edu.cn
Abstract. Visual Dialog (VD) is a task where an agent answers a series
of image-related questions based on a multi-round dialog history. How-
ever, previous VD methods often treat the entire dialog history as a
simple text input, disregarding the inherent conversational information
ﬂows at the round level. In this paper, we introduce Multi-round Dia-
logue State Tracking model (MDST), a framework that addresses this
limitation by leveraging the dialogue state learned from dialog history
to answer questions. MDST captures each round of dialog history, con-
structing internal dialogue state representations deﬁned as 2-tuples of
vision-language representations. These representations eﬀectively ground
the current question, enabling the generation of accurate answers. Exper-
imental results on the VisDial v1.0 dataset demonstrate that MDST
achieves a new state-of-the-art performance in generative setting. Fur-
thermore, through a series of human studies, we validate the eﬀectiveness
of MDST in generating long, consistent, and human-like answers while
consistently answering a series of questions correctly.
Keywords: Visual Dialog · Multi-round Dialogue State Tracking ·
Object-Entity Alignment
1
Introduction
Vision-language based multi-modal tasks have gained signiﬁcant attention at the
intersection of computer vision and natural language processing. Tasks such as
Visual Question Answering (VQA) [2], and Visual or Video Dialogue [11,18,27]
require the fusion of visual and textual information. Among these tasks, Visual
Dialog (VD) [11] poses a unique challenge that goes beyond simple question
answering grounded in an image. VD involves comprehending conversational
language, navigating through multi-round dialog history, and reasoning based
on visual and textual contents to generate coherent answers.
While previous methods in VD have made progress, they often overlook the
inherent information ﬂows and round-level interactions within the dialog history.
Existing models [5,7,8,12,15,20,21,28] commonly concatenate the entire dialog
history into a single text sequence, lacking explicit focus on the most relevant
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 541–553, 2024.
https://doi.org/10.1007/978-981-99-8850-1_44

542
W. Pang
history clues. Although attention mechanisms, such as sequential attention [19],
co-attention [1,31], dual-attention [9], and multi-view attention [25], have been
proposed, they still treat each round of the dialog history independently.
To address these limitations, we propose the multi-round dialogue state track-
ing model (MDST) for Visual Dialog. Unlike prior work, MDST explicitly models
the round-level interactions in dialog history. We deﬁne the dialogue state in VD
as a 2-tuple of vision and language states, where vision states capture object-level
representations and language states represent dialog entity-level representations.
In MDST, each round question from the dialog history is processed, ground-
ing the question in the dialogue state to yield question-guided visual-textual
clues. These clues are then used to decode accurate answers, while updating the
dialogue states accordingly. Notably, vision states remain unchanged through-
out the dialogue, while language states are updated in each round. We align
the vision and language representations in dialogue states in an object-entity
fashion, facilitating the grounding of follow-up questions.
Experimental results on the VisDial v1.0 dataset demonstrate that our pro-
posed model achieves state-of-the-art performance in generative setting. Further
examinations reveal that MDST consistently answers questions correctly, with
a joint answer accuracy (JACC) of 79.8% in the generative setting. Moreover,
MDST generates human-like responses, as validated through human studies. To
summarize, our contributions are three-fold:
– We propose a novel multi-round dialogue state tracking model (MDST) for
Visual Dialog. The MDST, including representations of image objects and
representations of dialog entities, models the inherent interactions in dialog
history at the round level.
– We achieve new state-of-the-art results on most evaluation metrics on VisDial
v1.0, and ﬁnd that the alignment of vision-language in dialogue states could
improve the ﬁnal performance signiﬁcantly.
– We introduce JACC to evaluate the answer quality, and ﬁnd that our MDST
can continuously generate correct answers as proved by JACC of 79.8% that
means about 8 rounds in 10 are correct on VisDial v1.0 val.
2
Related Work
Dealing with dialog history as a simple text input in Visual Dialog (VD) has
been a prevailing practice since the works of LF [11], HCIAE [19], and LTMI
[21]. However, recent research has highlighted the limitations of such approaches
in explicitly capturing round-level interactions between dialog rounds [23,24].
Existing work can be categorized into three groups based on their handling of
dialog history.
Firstly,
attention-based
models
[1,9,11,13,17,19,22,25,29,30]
typically
encode each round of history separately to obtain a set of history embeddings.
Sequential attention is applied in HCIAE [19] to attend to the history and image
sequentially. MCA [1] leverages modular co-attention to fuse visual and textual
modalities. DMRM [9] utilizes dual-attention mechanisms to resolve textual and

Multi-round Dialogue State Tracking by Object-Entity Alignment
543
visual co-references. MVAN [25] introduces multi-view attention to fuse the ques-
tion and history at both the sentence and word level.
Secondly, graph-based models [6,14,32] construct a graph representation of
the entire dialog history, where each node represents a question-answer (QA)
pair. KBGN [14] and LTMI-GoG [6] establish edges between nodes to indicate
coreference relations between QA pairs. However, these graph-based approaches
can suﬀer from scalability issues as the graph size grows with the dialogue.
Thirdly, concatenation-based models treat the entire dialog history as a single
sentence. DualVD [15] packs the dialog history into a long string encoded by an
LSTM. UTC [5], ICMU [8], and LTMI [21] concatenate each QA pair as a text
sequence, separated by a special token (e.g., [SEP]), and input them into a
transformer encoder.
In summary, prior approaches for handling dialog history in VD have not
explicitly modeled interactions at the round level of granularity. This limitation
hinders their ability to capture the nuanced dynamics of multi-round dialogues.
3
Model
Fig. 1. Overall structure of our proposed MDST model for Visual Dialog.
Figure 1 presents an overview of our Multi-Round Dialogue State Track-
ing (MDST) model, which consists of four main modules: Question Encoder
(QEncoder), Question Grounding on Dialogue State (QGDS), Answer Decoder
(ADer), and Postdiction on Dialogue State (PDS). In the remainder of this
section, we go into more detail on each module.
Problem Formulation. Given an image I and a multi-round dialogue history
H = C, (q(1), a(1)), . . . , (q(t−1), a(t−1)) up to round t −1, where C represents the
image caption and (q(·), a(·)) denotes the previously experienced question-answer
pairs, the dialogue agent aims to respond to the current question q(t) at round
t. This response involve generating a free-form natural language answer in a
generative setting.

544
W. Pang
We ﬁrst extract object-level image features using Faster-RCNN [3]. For each
image, we select the top-ranked N objects, each of them is of size 2048-dim and
projected to low-dimension features with size d through a linear layer as:
Of = RCNN(I),
(1)
O(0) = LayerNorm(ReLU(WoOf + bo)),
(2)
where LayerNorm is the layer normalization [4], Wo and bo are learnable param-
eters, O(0) ∈RN×d represents a set of object-level image features. Furthermore,
we insert two special pseudo-object features: NULL (ϵ) and ALL (χ), where
NULL is a zero vector of size d, and ALL denotes the representation of the
whole image by taking the mean of O(0). Thus, we get a new set of N + 2 object
features of O(0) ∈R(N+2)×d = O(0) ∪{ϵ} ∪{χ}. For clarity, we omit all the
biases in the remainder of this section.
Let <O(0), S(0)> denote the initial dialogue state, where S(0) ∈R(N+2)×d
is initialized as a set of zero vectors of the same size. In our approach, the
image caption is treated as the zeroth round QA pair C(0), which serves as the
initialization for S(0) at the beginning of the dialogue.
Question Encoder (QEncoder). For encoding both the question and the
image caption, we employ a standard Transformer encoder [26]. This encoder
generates contextual representations, denoted as q(t) and C(0), where l represents
the length of the question or caption. It is worth noting that, for simplicity, we
use the same symbol to represent both the textual string and its corresponding
representation.
Question Grounding on Dialogue State (QGDS). QGDS aims to ground
current question q(t) in dialogue state, yielding question-related textual clues on
language states and visual clues on vision states. To better associate question
with vision and language states, three probability distributions are designed:
word-entity alignment between question words and language states, word-object
alignment between question words and vision states, and switching probability.
Before going into detail, we introduce a notation to express a non-linear trans-
formation layer, to which dropout regularization and layer normalization are
applied:
MLP(x) = LayerNorm(Dropout(GELU(Wx))),
(3)
where x is the input: a vector or a matrix, with learnable weight W of the size
varying with the input.
Because many textual relations (e.g., co-reference) existing in question and
previous history [11], our model will associate current question words with its
most related dialog entities in language states using a learnable word-entity
alignment distribution π(t)
l
∈Rl×N in Eq. 4. To ground current question in an
image, we then calculate a cross-modal matching as in Eq. 5,

Multi-round Dialogue State Tracking by Object-Entity Alignment
545
π(t)
l
= softmax(MLP(q(t)) · MLP(S(t))T /
√
d),
(4)
π(t)
v
= softmax(MLP(q(t)) · MLP(O(0))T /
√
d),
(5)
where π(t)
v
∈Rl×N represents word-object alignment distribution between ques-
tion words and objects in vision states.
Switching probability is designed to 1) determine whether current question
is related to previous dialog history; 2) provide a weight to fuse two alignment
distributions because there is one-to-one correspondence (i.e., object-entity)
between vision and language states:
ϕ(t) = sigmoid(
w(MLP(q(t))MLP(S(t))T ) .mean
l→1
√N + 2
),
(6)
where .mean takes the mean on the l dimension with trainable parameter w ∈
R(N+2)×1. ϕ(t) ∈[0, 1] is a weight measured the relationship between question
and dialog history. The larger value of ϕ(t), the less relevant current question
is to dialog history. Experiment shows introducing ϕ(t) can contribute to better
the ﬁnal performance.
The question-guided textual context Δq(t)
l
is obtained by a weighted sum of
language stats over both word-entity and word-object alignment distributions,
as denoted in Eq. 7:
Δq(t)
l
= S(t)(π(t)
l
+ ϕ(t)π(t)
v ),
(7)
Δq(t)
v
= O(0)(π(t)
v
+ (1 −ϕ(t))π(t)
l ),
(8)
where Δq(t)
l
∈Rl×d represents history context relevant to current question com-
posed of two parts. The ﬁrst part consists of an explicit history attention directly
from question to language states, while the second part contains an aligned his-
tory attention indirectly from question, via vision states, to language states,
weighted by switching probability. Similarly, the question-guided visual context
is written as in Eq. 8, where Δq(t)
v
∈Rl×d represents the focused visual regions
relevant to current question from two parts, including an explicit visual atten-
tion from question to vision states and an implicit ones via the cross-modal
alignment.
Finally, we use the sum of the three components to denote the ﬁnal question
representation as in q(t) + Δq(t)
l
+ Δq(t)
v , which is decoded in next ADer module.
Answer Decoder (ADer).
In ADer, we utilize a standard Transformer
decoder as the backbone for the generative setting. It takes the ﬁnal question
representation, obtained by combining the question representation q(t) with the
question-guided textual context Δq(t)
l
and the question-guided visual context

546
W. Pang
Δq(t)
v , as input. The decoder autoregressively generates the next word one by
one until it encounters an end-of-sequence token, producing a free-form natural
answer. Formally, the ADer module can be expressed as:
a(t) = Decoder(q(t) + Δq(t)
l
+ Δq(t)
v ),
(9)
where a(t) represents the output of the decoder, which not only represents the
free-form natural answer of length ℓ, but also denotes its contextualized repre-
sentations over the words: a(t) ∈Rℓ×d. The decoder progressively generates each
word based on the input representation.
In the discriminative setting, we encode each of the 100 candidate answers
using another Transformer encoder. We score these candidate answers by com-
puting the dot product similarity with the ﬁnal question representation. The
candidate answer with the highest score is selected as the response.
Postdiction on Dialogue State (PDS). PDS is responsible for updating the
representation of previously experienced language states with the new question-
answer (QA) pair. This module incorporates the new QA pair as new information
into the dialogue history, reﬁning the dialogue state representation. It’s worth
noting that the language states are updated from S(0) to S(1) with the image
caption at the beginning, while the vision states remain unchanged throughout
the dialogue.
The PDS module leverages an alignment distribution to fuse the QA pair and
capture word-word interactions between the question and answer. The alignment
distribution is deﬁned as follows:
α(t) = softmax(MLP(q(t) + Δq(t)
l )MLP(a(t))T /
√
d),
h(t) = q(t) + α(t)a(t),
(10)
where α(t) ∈Rl×ℓrepresents the word-word alignment distribution between the
question and answer. h(t) ∈Rl×d denotes the ﬁnal representation of the QA pair,
which is used to update the previous language states, as follows:
β(t) = softmax(MLP(h(t) + Δq(t)
l )MLP(S(t))T /
√
d),
S(t+1) = S(t) + (β(t))T h(t),
(11)
where β(t) ∈Rl×(N+2) represents the assignment probability, indicating how
the new QA information is distributed among the language states. It provides a
word-entity alignment distribution for associating the new information with the

Multi-round Dialogue State Tracking by Object-Entity Alignment
547
language states. The language states are then updated to S(t+1) by adding the
assigned new information to S(t). The updated dialogue states <O(0), S(t+1)>
are used as input for the Question Grounding on Dialogue State (QGDS) module
in the next round, and this process continues iteratively.
4
Experiment
Datasets and Evaluation. We conduct our experiments on the VisDial v1.0
dataset, which consists of a standard train/val/test split. To evaluate the per-
formance, we employ NDCG and retrieval metrics, including MRR, Mean rank,
and R@1, 5, 10, following the conventions of previous studies [5,8]. Additionally,
we assess the quality of generated answers by generating 2064 dialogues for 10
rounds on the VisDial v1.0 validation set and calculating the following metrics:
Joint Answer Accuracy (JACC) measures the percentage of correct QA pairs
among all the generated QA pairs. It assesses whether the generated answers
are correct given the corresponding images. Average Answer Length (AvgLen)
calculates the average length of generated answers.
Implementation Details. We utilize a Transformer encoder-decoder architec-
ture as the backbone. The encoder and decoder consist of 12 layers with 12 heads
and 768 hidden states, respectively. For image features, we extract bottom-up
features of 36 objects using Faster-RCNN. We employ the Adamax optimizer
with an initial learning rate of 1e-3, which linearly decreases to 5e-5 following
a scheduled warmup of 0.2. The model is trained for 20 epochs with a batch
size of 32. The word embeddings, shared between encoders and decoders, are
set to 768 dimensions. For training, we compute the negative log-likelihood of
ground-truth and generated answer.
Comparison to State-of-the-Art Methods. We compare our approach to
several state-of-the-art methods, categorizing them based on how they utilize
the dialog history: 1) Attention-based models: MN [11], HCIAE [19], CoAtt
[29], DAM [16], DMRM [9], ReDAN [13], SeqIPN [30], MVAN [25], and MCA
[1]. 2) Graph-based models: KBGN [14], LTMI-GoG [6], and HKNet [32]. 3)
Concatenation-based models: LateFusion [12], DualVD [15], LTMI [21], LTMI-
LG [7], VDBERT [28], Visdial-BERT [20], ICMU [8], and UTC [5]. 4) Dialogue
State Tracking (DST) based model: Our MDST model. It’s important to note
that our MDST model for VisDial v1.0 is trained from scratch, without relying
on pretraining or ﬁne-tuning on additional large-scale datasets.

548
W. Pang
Table 1. Comparisons on VisDial v1.0 val in the generative setting.
Model
MRR↑
R@1↑
R@5↑
R@10↑
Mean↓
NDCG↑
LateFusion
46.57
36.20
56.40
63.40
19.44
54.21
MN
47.83
38.01
57.49
64.08
18.76
56.99
HCIAE
49.07
39.72
58.23
64.73
18.43
59.70
CoAtt
49.64
40.09
59.37
65.92
17.86
59.24
DAM
50.51
40.53
60.84
67.94
16.65
60.93
DMRM
50.16
40.15
60.02
67.21
15.19
-
ReDAN
49.60
39.95
59.32
65.97
17.79
59.41
SeqIPN
47.86
38.16
57.08
64.89
15.27
60.72
SeqMRN
49.22
38.75
59.62
68.47
13.00
63.01
SKANet
45.53
36.17
55.05
61.41
19.79
-
KBGN
50.05
40.40
60.11
66.82
17.54
60.42
LTMI-GoG
51.32
41.25
61.83
69.44
15.32
62.63
LTMI
50.38
40.30
60.72
68.44
15.73
61.61
LTMI-LG
51.30
41.34
61.61
69.06
15.26
63.23
LTMI-LG∗
51.43
41.68
61.96
69.87
14.89
63.53
UTC
52.22
42.56
62.40
69.51
15.67
63.86
MDST (Ours)
53.49
42.56
62.47 69.77
14.94
65.03
Table 1 presents the generative results on the VisDial v1.0 validation split.
Our proposed MDST model outperforms all the comparison methods on 4 out
of 6 metrics, establishing a new state-of-the-art. Speciﬁcally, we achieve an
NDCG of 65.03, MRR of 53.49, R@1 of 42.56, and R@5 of 62.47. Notably, when
compared to attention- and graph-based methods, our model shows signiﬁcant
improvements across all metrics, especially in NDCG and MRR. We improve
NDCG by 2.02 points (65.03 vs. SeqMRN’s 63.01) and MRR by approximately
2.17 points (53.49 vs. LTMI-GoG’s 51.32). When compared to concatenation-
based methods, our MDST model achieves similar or better results. Moreover,
it surpasses the previous best-performing method, UTC, by approximately 1.27
points in MRR and 1.17 points in NDCG. It’s important to note that UTC relies
on ViLBERT pretraining and utilizes VQA datasets.
Table 2. Main comparisons on VisDial v1.0 test.
Model
MRR↑R@1↑
R@5↑
R@10↑Mean↓NDCG↑
LTMI-LG
64.0
50.63
80.58
90.20
4.12
58.55
LTMI
64.08
50.20
80.68
90.35
4.05
59.03
VDBERT
65.44
51.63
82.23
90.68
3.90
59.96
LTMI-GoG
63.52
50.01
80.13
89.28
4.31
61.04
ICMU
66.82
53.50
83.05
92.05
3.59
61.30
UTC
66.27
52.25
83.55
92.23
3.48
62.65
MDST (Ours) 66.78
53.58 83.69 92.62
3.54
63.18

Multi-round Dialogue State Tracking by Object-Entity Alignment
549
Table 3. Ablation study on VisDial v1.0 val in the generative setting.
# Model
MRR R@1
R@5
R@10 Mean NDCG
1
MDST
53.49
42.56 62.47 69.77
14.94
65.03
2
-QGDS-PDS 50.80
40.55 60.79 67.46
15.73
61.15
3
-α(t)
52.79
41.65 61.43 69.05
15.20
63.64
4
-NULL-ALL
53.27
41.91 61.97 69.34
15.06
64.48
Table 2 displays the results on the VisDial v1.0 test split. Our MDST model
achieves a NDCG value of 63.18, outperforming other methods across various
metrics. Compared to UTC, MDST improves NDCG by 0.53 points. In summary,
our MDST model, despite being simpler and not relying on larger pre-trained
language models or extra datasets like UTC and ICMU, achieves signiﬁcant
improvements across most metrics, outperforming previous state-of-the-art mod-
els. These improvements highlight the eﬀectiveness of the dialogue state tracking
mechanism in VisDial.
Ablation Studies. Table 3 presents the results of ablation studies, which eval-
uate the importance of each module in the generative setting. The ﬁrst row
represents the performance of the full model, while the subsequent rows (2–4)
indicate the eﬀect of removing each module sequentially.
When removing the QGDS&PDS module (Row 2), we adopt a similar app-
roach to previous work, where we directly use the question to attend to image
features and original history embeddings. The fusion of the question, attended
image features, and history features is then fed into the ADer module. The results
show a signiﬁcant drop in NDCG by -3.88 points, MRR decreases to 50.80 by
-2.69 points, and R@1,5,10 exhibits a substantial decrease. These ﬁndings align
with the previous comparisons and further demonstrate the eﬀectiveness of the
dialogue state tracking mechanism. It highlights the ability of the model to cap-
ture information ﬂows in the dialogue history at the round level. It is important
to note that utilizing PDS without QGDS or vice versa is meaningless since the
dialogue states are updated in PDS but used in QGDS. The combination of the
QGDS and PDS modules provides strong support for the tracking mechanism.
When removing the switching probability α(t) in the QGDS module, we
observe a signiﬁcant decrease in overall performance. NDCG and MRR decrease
by 1.39 and 0.7 points, respectively. This result underscores the importance of
the switching probability in our model. Speciﬁcally, the switching probability
plays a crucial role in associating the two alignment distributions (π(t)l and
π(t)v), facilitating the alignment of vision-language dialogue states. In other
words, aligning vision-language states brings about a substantial improvement,
which aligns with ﬁndings from previous studies [10,25].
Furthermore, when removing the two pseudo-object features, NULL and ALL
(Row 4), we observe a slight decline in performance. This ﬁnding validates that
both pseudo-objects carry useful information about the image. The inclusion of

550
W. Pang
these pseudo-objects is valuable because the upcoming question may be unre-
lated to the input image or may involve the entire image.
Fig. 2. Answers generated by our MDST model. The correct answers are highlighted
in red, and blue highlights denote the incorrect answers. (Color ﬁgure online)
Human Studies. A series of human studies were conducted on VisDial v1.0
val to generate 2064 dialogues. The results are presented in Table 4, and we pro-
vide three examples of generated dialogues in Fig. 2. The ﬁndings reveal that
our proposed MDST model demonstrates the ability to consistently provide cor-
rect answers throughout a series of questions, while generating more human-
like responses. Speciﬁcally, MDST achieves a Joint Answer Accuracy (JACC) of
79.8% on VisDial v1.0 val split, indicating that approximately 8 out of 10 rounds
yield correct answers. In comparison, LateFusion achieves a signiﬁcantly lower
JACC of 53.4%.
Table 4. JACC (%) and AvgLen on VisDial v1.0 val.
Model
#QA pair Correct InCorrect JACC AvgLen
MDST
1000
798
202
79.8
3.57
LateFusion
534
466
53.4
1.81
Human
–
–
–
3.11

Multi-round Dialogue State Tracking by Object-Entity Alignment
551
In the ﬁrst example, MDST generates 9 correct answers. Notably, in the
6th and 8th rounds, MDST produces reasonable responses: “he looks like he is
happy” and “I can’t see his shoes”, which capture the semantics similar to the
ground truth: “he’s smiling so I would say yes” and “I can only see chest up so
I don’t know”. In the second example, MDST provides correct answers in all 10
rounds, with the response “1 is female and the other is male” being as accurate
and natural as the human-generated answer”appears to be 1 of each”. The third
example also showcases MDST’s ability to correctly predict all 10 questions,
with the response “the dog is brown and white” in the 4th round also deemed
correct based on the image.
Interestingly, contrary to the human and LateFusion models, MDST tends to
produce longer, more consistent answers. The average answer length (AvgLen)
of MDST reaches 3.57, surpassing the human-generated answer length of 3.11
and LateFusion’s length of 1.81. In terms of consistency, for the question “how
many dogs are around?” in the second example, our model responds with an
accurate answer “just 1”, which aligns with dialog history (i.e., image caption). In
contrast, LateFusion provides an inconsistent response of “there are no people”,
and in the last round, LateFusion produces a question-irrelevant answer.
5
Conclusions
In this paper, we introduce a novel approach called Multi-Round Dialogue State
Tracking Network (MDST) for the task of Visual Dialog (VD). Unlike previ-
ous methods that treat dialog history as a simple text input, MDST tracks and
updates dialogue states, which are 2-tuple aligned vision-language representa-
tions. By modeling the inherent interactions at the round level, MDST aims
to capture dynamics of the conversation more eﬀectively. Experimental results
on VisDial v1.0 dataset demonstrate that MDST achieves state-of-the-art per-
formance across most evaluation metrics. Additionally, extensive human studies
further validate MDST can generate long, consistent, and human-like answers
while maintaining the ability to provide correct responses to a series of questions.
Overall, our proposed MDST framework represents a signiﬁcant advancement in
visual dialog systems, showcasing the importance of modeling dialogue states in
capturing the complex nature of visual conversations.
Acknowledgements. We thank the reviewers for their comments and suggestions.
This paper was partially supported by the National Natural Science Foundation of
China (NSFC 62076032), Huawei Noah’s Ark Lab, MoECMCC “Artiﬁcial Intelli-
gence” Project (No. MCM20190701), Beijing Natural Science Foundation (Grant No.
4204100), and BUPT Excellent Ph.D. Students Foundation (No. CX2020309).
References
1. Agarwal, S., Bui, T., Lee, J.Y., Konstas, I., Rieser, V.: History for visual dialog:
Do we really need it? In: ACL, pp. 8182–8197 (2020)

552
W. Pang
2. Agrawal, A., et al.: Vqa: Visual question answering. In: ICCV, pp. 2425–2433
(2015)
3. Anderson, P., et al.: Bottom-up and top-down attention for image captioning and
visual question answering. In: CVPR, pp. 6077–6086 (2018)
4. Ba,
J.L.,
Kiros,
J.R.,
Hinton,
G.E.:
Layer
normalization.
arXiv
preprint
arXiv:1607.06450 11 (2016). https://doi.org/10.48550/arXiv.1607.06450
5. Chen, C., et al.: Utc: a uniﬁed transformer with inter-task contrastive learning for
visual dialog. In: CVPR, pp. 18103–18112 (2022)
6. Chen, F., Chen, X., Meng, F., Li, P., Zhou, J.: Gog: relation-aware graph-over-
graph network for visual dialog. In: Findings of ACL, pp. 230–243 (2021)
7. Chen, F., Chen, X., Xu, C., Jiang, D.: Learning to ground visual objects for visual
dialog. In: EMNLP Findings, pp. 1081–1091 (2021)
8. Chen, F., Chen, X., Xu, S., Xu, B.: Improving cross-modal understanding in visual
dialog via contrastive learning. In: ICASSP (2022)
9. Chen, F., Meng, F., Xu, J., Li, P., Xu, B., Zhou, J.: Dmrm: a dual-channel multi-
hop reasoning model for visual dialog. In: AAAI (2020)
10. Chen, F., Zhang, D., Chen, X., Shi, J., Xu, S., Xu, B.: Unsupervised and pseudo-
supervised vision-language alignment in visual dialog. In: ACM MM, pp. 4142–4153
(2022)
11. Das, A., et al.: Visual dialog. In: CVPR, pp. 326–335 (2017)
12. Desai, K., Das, A., Batra, D., Parikh, D.: Visual dialog challenge starter code.
https://github.com/batra-mlp-lab/visdial-challenge-starter-pytorch (2019)
13. Gan, Z., Cheng, Y., Kholy, A.E., Li, L., Liu, J., Gao, J.: Multi-step reasoning via
recurrent dual attention for visual dialog. In: ACL, pp. 6463–6474 (2019)
14. Jiang, X., Du, S., Qin, Z., Sun, Y., Yu, J.: Kbgn: Knowledge-bridge graph network
for adaptive vision-text reasoning in visual dialogue. In: ACM MM (2020)
15. Jiang, X., et al.: Dualvd: An adaptive dual encoding model for deep visual under-
standing in visual dialogue. In: AAAI, pp. 11125–11132 (2020)
16. Jiang, X., et al.: Dam: Deliberation, abandon and memory networks for generating
detailed and non-repetitive responses in visual dialogue. In: IJCAI (2020)
17. Kang, G.C., Lim, J., Zhang, B.T.: Dual attention networks for visual reference
resolution in visual dialog. In: EMNLP, pp. 2024–2033 (2019)
18. Le, H., Sahoo, D., Chen, N.F., Hoi, S.C.: Multimodal transformer networks for
end-to-end video-grounded dialogue systems. In: ACL, pp. 5612–5623 (2019)
19. Lu, J., Kannan, A., Yang, J., Parikh, D., Batra, D.: Best of both worlds: transfer-
ring knowledge from discriminative learning to a generative visual dialog model.
In: NeurIPS (2017)
20. Murahari, V., Batra, D., Parikh, D., Das, A.: Large-scale pretraining for visual
dialog: a simple state-of-the-art baseline. In: ECCV, pp. 336–352 (2020)
21. Nguyen, V.Q., Suganuma, M., Okatani, T.: Eﬃcient attention mechanism for visual
dialog that can handle all the interactions between multiple inputs. In: ECCV, pp.
223–240 (2020)
22. Niu, Y., Zhang, H., Zhang, M., Zhang, J., Lu, Z., Wen, J.R.: Recursive visual
attention in visual dialog. In: CVPR (2019)
23. Pang, W., Wang, X.: Guessing state tracking for visual dialogue. In: 16th European
Conference on Computer Vision - ECCV 2020, pp. 683–698 (2020)
24. Pang, W., Wang, X.: Visual dialogue state tracking for question generation. In:
AAAI (Oral), pp. 11831–11838 (2020)
25. Sungjin, P., Taesun, W., Yeochan, Y., Heuiseok, L.: Multi-view attention network
for visual dialog. Appl. Sci. 11(7) (2021). https://doi.org/10.3390/app11073009

Multi-round Dialogue State Tracking by Object-Entity Alignment
553
26. Vaswani, A., et al.: Attention is all you need. In: NeurIPS, pp. 5998–6008 (2017)
27. de Vries, H., Strub, F., Chandar, S., Pietquin, O., Larochelle, H., Courville, A.:
Guesswhat?! visual object discovery through multi-modal dialogue. In: CVPR, pp.
5503–5512 (2017)
28. Wang, Y., Joty, S., Lyu, M., King, I., Xiong, C., Hoi, S.C.: VD-BERT: a Uniﬁed
Vision and Dialog Transformer with BERT. In: EMNLP, pp. 3325–3338 (2020)
29. Wu, Q., Wang, P., Shen, C., Reid, I., van den Hengel, A.: Are you talking to
me? reasoned visual dialog generation through adversarial learning. In: CVPR,
pp. 6106–6115 (2018)
30. Yang, L., Meng, F., Liu, X., Wu, M.K.D., Ying, V., Xu, X.: Seqdialn: sequential
visual dialog networks in joint visual-linguistic representation space. In: 1st Work-
shop on Document-grounded Dialogue and Conversational Question Answering,
pp. 8–17 (2021)
31. Yang, T., Zha, Z.J., Zhang, H.: Making history matter: history-advantage sequence
training for visual dialog. In: ICCV, pp. 2561–2569 (2019)
32. Zhao, L., Li, J., Gao, L., Rao, Y., Song, J., Shen, H.T.: Heterogeneous knowledge
network for visual dialog. IEEE Trans. Circ. Syst. Video Technol. (TCSVT), pp.
1–1 (2022). https://doi.org/10.1109/TCSVT.2022.3207228

Multi-modal Dialogue State Tracking
for Playing GuessWhich Game
Wei Pang1(B), Ruixue Duan1, Jinfu Yang2, and Ning Li1
1 Beijing Information Science and Technology University, Beijing, China
{pangweitf,duanruixue}@bistu.edu.cn
2 Beijing University of Technology, Beijing, China
Abstract. GuessWhich is an engaging visual dialogue game that
involves interaction between a Questioner Bot (QBot) and an Answer Bot
(ABot) in the context of image-guessing. In this game, QBot’s objective
is to locate a concealed image solely through a series of visually related
questions posed to ABot. However, eﬀectively modeling visually related
reasoning in QBot’s decision-making process poses a signiﬁcant chal-
lenge. Current approaches either lack visual information or rely on a sin-
gle real image sampled at each round as decoding context, both of which
are inadequate for visual reasoning. To address this limitation, we pro-
pose a novel approach that focuses on visually related reasoning through
the use of a mental model of the undisclosed image. Within this frame-
work, QBot learns to represent mental imagery, enabling robust visual
reasoning by tracking the dialogue state. The dialogue state comprises
a collection of representations of mental imagery, as well as representa-
tions of the entities involved in the conversation. At each round, QBot
engages in visually related reasoning using the dialogue state to construct
an internal representation, generate relevant questions, and update both
the dialogue state and internal representation upon receiving an answer.
Our experimental results on the VisDial datasets (v0.5, 0.9, and 1.0)
demonstrate the eﬀectiveness of our proposed model, as it achieves new
state-of-the-art performance across all metrics and datasets, surpassing
previous state-of-the-art models.
Keywords: GuessWhich · Multi-Modal Dialogue State Tracking ·
Visual Dialogue
1
Introduction
In the future, visual conversational agents may engage in natural language con-
versations. Research in vision-language tasks is still in its early stages, progress-
ing from single-round VQA [1] to multi-round Visual Dialogue and multi-modal
multi-modal Video Dialogue [2,3,13,16]. Among these tasks, GuessWhich stands
out as a two-player image-guessing game with a QBot and ABot. QBot aims to
identify a hidden image by asking questions to ABot. While ABot has received
attention, research on QBot is relatively limited, which is the focus of this paper.
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 554–565, 2024.
https://doi.org/10.1007/978-981-99-8850-1_45

Multi-modal Dialogue State Tracking for Playing GuessWhich Game
555
Fig. 1. Illustration of three types of QBot involving four components: question (Q),
answer (A), history encoder (H) at round t, and Decoder for new question generation.
Specially, a) Non-visual QBot, no visual information is provided to QBot. b) Real-image
QBot, retrieves a real image per round from a pool to serve as visual information for
Decoder. c) Mental-imagery QBot (Ours), explores visually related reasoning based on
the QBot’s mental model of the secret image.
QBot starts by receiving a caption and formulating the ﬁrst question. Sub-
sequent questions are generated based on the caption and dialogue history. Per-
forming visual reasoning solely from textual information poses a challenge. Exist-
ing approaches, as illustrated in Fig. 1 (a) and (b), can be divided into non-visual
methods [3,4,6,8,14,19,20] that rely on language-based models and real-image
methods [21] that retrieve probable images as visual context. However, both
approaches have limitations in visual reasoning.
Firstly, QBot needs to generate questions that pertain to the image, linking
words to visual concepts, much like humans do [9]. Hence, QBot, which lacks
visual information in its modeling, is inadequate for this purpose. Secondly, QBot
samples a single real image from a large pool of images, often numbering in the
thousands, e.g., 9,628 candidate images per round. This approach is unnatural
in the context of a genuine game and introduces substantial sampling variance,
rendering the model’s reasoning process unstable.
Drawing on the Dual-coding theory [9], human cognition is based on two
interconnected modes: words and images. Mental imagery plays a signiﬁcant
role in word comprehension. Building on this theory, we propose aligning dia-
logue entities with mental visual concepts in QBot, as depicted in Fig. 1(c). By
constructing a mental model of the secret image through textual semantics, we
establish a dialogue state consisting of representations of dialogue entities and
mental objects denoted as ⟨words, images⟩states.
As the game progresses, the dialogue state evolves, encompassing represen-
tations of mental objects in QBot’s mind, and prompting QBot to pose visually
related questions. To the best of our knowledge, the problem of modeling visual

556
W. Pang et al.
reasoning in QBot remains relatively unexplored. In this paper, we present a
QBot model that incorporates mental imagery through dialogue state tracking
(DST) to address the aforementioned ideas. Our model consists of a cycle of
two primary procedures: Visual Reasoning on Dialogue State (VRDS) and State
Tracking (STrack).
VRDS facilitates three-hop reasoning in the dialogue state, progressing
through the path words→words→images→words and generating an internal rep-
resentation. A decoder utilizes this representation to generate new questions.
Upon receiving an answer, STrack is activated, involving two actions: 1) Addi-
tion, introducing new textual semantics to the dialogue states (e.g., “strawberry”
as a new entity and its associated mental object). 2) Update, incorporating new
textual features (e.g., colors, positions, counts) into the aligned dialogue states’
existing representation. Experimental results demonstrate our model’s superior
performance, achieving a new state-of-the-art level. In summary, our contribu-
tions are three-fold:
– We propose a novel QBot agent that is capable of performing visually related
reasoning based on mental imagery in one’s mind in dialog.
– We present dialogue state tracking based QBot model (DST), which learns to
form representations of mental imagery that support visually related reason-
ing. The dialogue states, composed of not only words states but also images
states, are tracked and updated through dialoguing with ABot.
– Achieving new state-of-the-art results on the GuessWhich game underlying
VisDial v0.5, v0.9 and v1.0 datasets. Compared with prior studies, this work
takes a step towards mimicking humans playing a series of visual dialogue
games (such as GuessWhich).
2
Related Work
Visual Dialogue is a key area of research in vision-language studies, with the aim
of developing conversational agents capable of human-like interactions. Recent
progress [10–12] has been made in various tasks, including GuessWhat!?, Guess-
Which, and Visual & Video Dialogue. GuessWhich speciﬁcally involves the chal-
lenge of QBot ﬁnding an undisclosed image from a large pool without sharing it
with ABot.
Existing QBot models can be categorized into non-visual and real-image
approaches. Non-visual models, represented in Fig. 1(a), do not utilize visual
information. For example, Das et al. [4] propose an encoder-decoder network
with a feature regression network. They use a hierarchical encoder-decoder archi-
tecture to generate questions based on history. Murahari et al. [8] introduce a
Smooth-L1 Penalty to mitigate repetitive questions. Zhao et al. [19] incorporate
an Attentive Memory Network, and Li et al. [6] propose an information-theoretic
model. These models solely rely on textual information.
Real-image models, shown in Fig. 1(b), provide physical images as input to
the QBot decoder. Zhou et al. [21] introduce an image-guesser module into the

Multi-modal Dialogue State Tracking for Playing GuessWhich Game
557
QBot model. While related works track real image objects, our approach focuses
on constructing and tracking mental imagery representations during the dialogue.
3
Model
Fig. 2. Overall structure of the proposed DST model. The oblong colorful strips denote
image state M (t)
v , and the rounded circle are word state M (t)
e .
The proposed Dialogue State Tracking based QBot (DST) model, depicted in
Fig. 2, consists of ﬁve modules: Recursive Self-Reference Equation (R-SRE),
Visual Reasoning on Dialogue State (VRDS), Question Decoder (QDer), QBot
Encoders (QEnc), and State Tracking (STrack). Detailed explanations of each
module will be provided in the subsequent sections.
Problem Setting. At the start of the game, QBot receives a caption C describ-
ing the target image I∗visible to ABot. This caption provides the initial textual
information for QBot to generate its ﬁrst question. In subsequent rounds, QBot
generates questions q(t) based on the accumulated dialogue history H, which
includes the caption C and previous question-answer pairs. QBot’s goal is to use
this information to guess the undisclosed image from a candidate pool.
We deﬁne an accumulated dialogue state <M (t)
e , M (t)
v > at round t. The words
state M (t)
e
represents the textual representation of discussed entities, while the
images state M (t)
v
represents mental imagery information derived from the words
in QBot’s mind. The initial dialogue state <M (1)
e
, M (1)
v > is constructed using
the caption C as input and the Adding action of the STrack module. Both M (1)
e
and M (1)
v
are 1 × d matrices.
Recursive Self-Reference Equation (R-SRE). To capture visually related
interactions within and between modalities in the dialogue state, we propose a
recursive self-reference equation (R-SRE) denoted as V ′ = R −SRE(Q, V ). The
R-SRE consists of two attention mechanisms that facilitate the update of the
matrix V based on the guidance provided by another matrix Q. The equation is
formulated as follows:

558
W. Pang et al.
α = softmax(wαQ),
(1)
q = αT Q,
(2)
β = softmax(wβ[r(k)(q); r(k)(q) ⊙V ; V ]),
(3)
V ′ = βV,
(4)
where Q, V ∈Rk×d are two input data matrix, wα ∈Rd×1 and wβ ∈R3d×1 are
trainable projection matrices. [;;] denotes the symmetric concatenation between
bi-modalities as in [11,12], ⊙is the element-wise product. We deﬁne a repeat
operation r(k)(q) that repeats q k times to form a matrix of size k × d with same
dimension as V . Specially, a self-attention on Q is ﬁrst performed and attention
scores α ∈Rk×1 is obtained, q ∈Rd×1 is the weighted sum of Q using α. Then,
q is referred to as a key again to query V to get a β ∈Rk×1 over V . Finally, V
is changed by multiplying the attention weight β to yield a new same-dimension
representation V ′. We omit bias where possible, for clarity.
Visual Reasoning on Dialogue State (VRDS). The VRDS process involves
three-hops reasoning using the R-SRE operation. Firstly, a self R-SRE operation
updates the words state (Eq. 5). Then, a cross-modal R-SRE operation generates
a visually related context vector (Eq. 6). Iterative cross-modal R-SRE operations
between the updated words state and the images state yield an intermediate
textual context vector (Eq. 7). Finally, the visually and textually related context
vectors are concatenated and processed by a linear layer to obtain the ﬁnal
context vector. Formally,

M (t)
e
= R−SRE(M (t)
e , M (t)
e ),
(5)

M (t)
v
= R−SRE(
M (t)
e , M (t)
v ), a(t)
v
= sum(
M (t)
v ),
(6)

M (t)
e
= R−SRE(
M (t)
v , 
M (t)
e ), a(t)
e
= sum(
M (t)
e ),
(7)
z(t) = Dropout(wv[a(t)
v ; a(t)
e ]),
(8)
where wv ∈R2d×d are learnable projection matrixes, sum is operated on the k-
dimension to compact the intermediate representation matrix to a vector, z(t) ∈
Rd is the ﬁnal context vector passed to QDer module.
From Eq. 5 to Eq. 8, VRDS performs textually and visually related interactions
upon dialogue state. It models the cross-modal interactions between two states
motivated from Dual-coding theory [9]: QBot can think with words as well as can
think with images based on mental imagery representations in one’s mind.
Question Decoder (QDer). We use a multi-layer Transformer decoder [15]
and employs softmax on its output. It takes z(t) and dialogue states as input.
The QDer module predicts next word by employing cross-attention with dialogue
states as conditioning, continuing this process until an end-of-sequence token
[EOS] is encountered. Formally,
h(t)
i+1 = transformer decoder(h(t)
i , M (t)
e , M (t)
v ),
(9)
w(t)
i+1 = argmax softmax(h(t)
i+1wder),
(10)

Multi-modal Dialogue State Tracking for Playing GuessWhich Game
559
where wder ∈Rd×|V| is trainable parameters, V is the number of vocabulary.
h(t)
i+1 denotes the representation of next token w(t)
i+1, which is selected by the
greedy algorithm. h(t)
0
is initialized as the element-wise addition of the starting
token [SOS] and z(t). We save a sequence of w(t)
i+1 to produce a new question q(t).
QBot Encoders (QEnc). QEnc utilizes a pre-trained vision-linguistic model
called ViLBERT [7]. The input to ViLBERT is structured as follows: [CLS]
q(t) [SEP] a(t) [SEP] M (t)e [SEP] M (t)v, with each segment separated by a
[SEP] token. The output of [CLS] token is considered as the fact representation
f (t) ∈Rd, which captures the information from new question-answer pair.
State Tracking (STrack). STrack oﬀers two actions: Adding and Updating.
The decision between these actions is determined by a diﬀerentiable binary choice
made using Gumbel-Softmax sampling [5]. This allows for end-to-end training.
We introduce a two-layer feedforward network denoted as FFN(·), which incorpo-
rates GELU activation, Dropout, and LayerNorm. For clarity, we apply FFN(·)
to the element-wise product of the fact representation f (t) and words state M (t)
e .
Subsequently, a Gumbel-Softmax operation is performed, yielding a probability
distribution representing the action type.
φ = Gumbel(FFN([r(k)(f (t)) ⊙M (t)
e ])),
(11)
where φ ∈R2 is 2-d one-hot vector for discrete decision. According to predicted
φ, one of the two actions is chosen for the STrack.
Adding Action on Words State. The Adding action is executed on the
current words state if φ[0] = 1. It takes the f (t) as input to FFN, resulting in a
new textual representation e(t+1)a, w ∈Rd. Subsequently, e(t+1)a, w is inserted
into the set of words states, leading to a new set of words states M (t+1)
e
∈
R(k+1)×d:
e(t+1)
a,w
= FFN(f (t)),
(12)
M (t+1)
e
= M (t)
e
∪{e(t+1)
a,w },
(13)
where ∪is an append operation. Note that the size of newly updated M (t+1)
e
is
increased to k + 1.
Adding Action on Images State. In the case of φ[0] = 1, the adding action
is performed on current images states. It produces a new mental object o(t+1)
a,v
∈
Rd, which denotes a new visual concept (e.g., “carpet”). We translate the fact
representation into images with FFN network, and get a new set of images states
M (t+1)
v
∈R(k+1)×d, in which its size is increased to k + 1. Formally,
o(t)
a,v = FFN(f (t)),
(14)
M (t+1)
v
= M (t)
v
∪{o(t)
a,v},
(15)
where fact representation f (t) is used for translation from words to images by
FFN.

560
W. Pang et al.
Updating Action on Words State. When φ[0] = 0, the updating action is
applied to the words state. It includes calculating an assignment distribution ψ
that determines how much the new fact representation can be merged into the
existing representation of the associated words state. This is achieved by passing
f (t) and the current words state M (t)
e
through a two-layer feedforward network
and a softmax classiﬁer.
ψ = softmax(FFN([r(k)(f (t)) ⊙M (t)
e ])),
(16)
M (t+1)
e
= M (t)
e
+ ψFFNψ(f (t)),
(17)
where ψ ∈Rk is the assignment distribution, FFNψ(·) ∈Rd denotes another
FFN network, M (t+1)
e
∈Rk×d is the newly updated words states. Note that the
number of words states in M (t+1)
e
remains unchanged in this case.
Updating Action on Images State. If φ[0] = 0, similar to the case of updat-
ing on words state, we compute an assignment distribution for associating cur-
rent fact representation with previous images state. Formally,
γ = softmax(FFN([r(k)(f (t)) ⊙M (t)
v ])),
(18)
M (t+1)
v
= M (t)
v
+ γFFNγ(f (t)),
(19)
where γ ∈Rk is the assignment distribution, FFNγ(·) ∈Rd is another FFN
network diﬀerent from in Eq. 18. It converts fact representation to visually
related representation, which accumulates further visual attributes (such as
object shape, color and position, e.g., a question “what color is the carpet?”
with an answer “red”) about the contents of the undisclosed image in the same
representation in M (t)
v . M (t+1)
v
∈Rk×d is the newly updated images state, its
size remains unchanged.
Model Training. Our model is optimized using supervised learning (SL) with
three loss functions: Cross-Entropy (CE) loss, Mean Square Error (MSE) loss,
and Progressive (PL) loss. The CE loss is computed based on ground truth
questions, while the MSE loss is calculated using the feature regression network
f(·) [4] to predict the image representation y(t)I. The MSE loss compares the
predicted representation y(t)I with the ground truth image representation y(t)
I∗
obtained from VGG19.
LCE = −1
l
l

j=1
log pj; LMSE = −1
T
T

t=1
||y(t)
I∗−y(t)
I ||2
2,
(20)
where l here denotes the total length of generated questions, T is the total dialog
rounds, pj is the probability of ground-truth word at step j in the dialogue.
Because of multi-round dialogue brings a series of MSE loss, we present a
progressive loss that is deﬁned as the diﬀerence of MSE loss in successive dialog
rounds, which encourages Questioner to progressively increase similarity towards
the target image, as written in: LPL = −
1
T −1
T
t=2 L(t)
MSE −L(t−1)
MSE . Overall, the
ﬁnal loss for supervised learning QBot is a sum of three losses as mentioned
above: LSL = LCE + LMSE + LPL.

Multi-modal Dialogue State Tracking for Playing GuessWhich Game
561
4
Experiment and Evaluation
Dataset Our GuessWhich model is evaluated on three benchmarks: VisDial
v0.5, v0.9, and v1.0. These datasets include various numbers of training, valida-
tion, and test images. VisDial v1.0 has 123,287 training images, 2,064 validation
images, and 8,000 test images. VisDial v0.9 includes 82,783 training images and
40,504 validation images. VisDial v0.5 consists of 50,729 training images, 7,663
validation images, and 9,628 test images. Dialogues in these datasets contain
a caption for the target image and multiple question-answer pairs. It’s worth
noting that only the test set of VisDial v1.0 has variable-length dialogues, while
the other dataset splits have ﬁxed 10-round dialogues.
Evaluation Metric. We follow the standard evaluation metrics [4,8] for QBot
in two parts: image guessing and question diversity. At image guessing, we report
retrieval metrics of target image, including mean reciprocal rank (MRR), Recall
@k (R@k) for k = 1, 5, 10, mean rank (Mean) and percentile mean rank (PMR). At
question diversity, we adopt six metrics like Novel Questions [8], Unique Questions
[8], Dist-n and Ent-n [17], Negative log-likelihood [8], and Mutual Overlap [18].
Implementation Details. Our model architecture consists of a cross-modal
Transformer decoder with 12 layers and a hidden state size of 768. It utilizes
12 attention heads. The base encoder is a pre-trained ViLBERT model with
12 layers and a hidden state size of 768. During training, we used a batch size
of 64 and trained the model for 30 epochs. A dropout rate of 0.1 was applied
after each linear layer. Early stopping was implemented on the validation split if
the performance metric (PMR) did not improve for 10 consecutive epochs. We
used the Adam optimizer with a base learning rate of 1e-3, which decayed to
1e-5 during training. For image representation, we used pre-extracted VGG19
features, where each image is represented by a 4096-dimensional vector.
Comparison to State-of-the-Art Methods. The comparing methods on
QBot can be regarded to have three types: 1) Non-visual based models, like SL-
Q [4], ReCap [14], ADQ [8], AMN [19], RL-Q [4], AQM+/indA [6], ReeQ-SL
(trained in SL) [20] and ReeQ-RL (ﬁne-tuned in reinforcement learning) [20]. 2)
Real-image based methods, such as SL-Q-IG [21]. and 3) Mental-imagery based
method: our DST.
Image Guessing. The results of image guessing are provided in Table 1. Our
DST model achieves signiﬁcant improvements over previous state-of-the-art mod-
els (SOTA) across all metrics and datasets. On the validation split of v1.0, DST
outperforms ReeQ and establishes new SOTA with a PMR of 99.60 and Mean of
17.52. On the test split of v1.0, DST consistently outperforms other strong models,
such as AMN, with a PMR of 98.10. Compared to the real-image based SL-QI-G
on the v0.5 test, DST achieves a PMR of 98.69, demonstrating the ineﬃciency of
image retrieval from larger pools. DST also performs well on the v0.9 validation
split with a PMR of 98.02. The trends in PMR are consistent across all datasets,
and only DST shows a continuous increase in PMR as the dialogue progresses,
highlighting its robustness and eﬀectiveness in diﬀerent dataset settings.

562
W. Pang et al.
Table 1. Result comparison of image guessing on VisDial datasets. Higher is bet-
ter for MRR, R@k, and PMR, and lower is better for Mean. Note that
† means
we roughly estimated the value of Mean by an approximate evaluation: Mean ≃
#Num of Image Pool × (1.0 −PMR) [14], and the results of ⋄are cited from [19].
Model
MRR↑R@1↑
R@5↑
R@10↑PMR↑Mean↓
Dataset
#Num of Image Pool
SL-Q [4]
–
–
–
–
91.19
848.2†
v0.5 test 9,628
RL-Q [4]
–
–
–
–
94.19
559.4†
SL-Q-IG [21]
–
–
–
–
96.09
376.5†
ReCap [14]
–
–
–
–
95.54
429.4†
AQM+/indA [6] –
–
–
–
94.64
516.1†
DST (Ours)
6.25
2.59
8.31
13.14
98.69
254.19
8.73
3.55
10.49
15.96
98.76
184.65
v0.5 val
7,663
ADQ⋄[8]
–
–
–
–
94.99
400.8†
v1.0 test 8,000
RL-Q⋄[4]
–
–
–
–
93.38
529.6†
AMN [19]
–
–
–
–
94.88
409.6†
DST (Ours)
33.49
17.47
30.62
33.97
99.44
161.19
SL-Q [4]
7.8
2.56
9.49
17.87
93.83
127.84
v1.0 val
2,064
ADQ [8]
10.73
3.39
14.82
25.29
95.73
87.92
ReeQ-SL [20]
31.21
17.78
45.01
59.98
99.00
20.60
ReeQ-RL [20]
33.65
19.91
48.50
62.94
99.13
18.05
DST (Ours)
34.01
24.99 49.49 63.92
99.60
17.52
4.10
2.27
5.88
8.13
98.02
1195.43 v0.9 val
40,504
Fig. 3. Comparison of generated dialogs with ADQ [8] and humans on VisDial v1.0
val. Our QBot agent converses with SL-ABOT [8] for a fair comparison to ADQ model.
Ablation Studies. Ablation studies on the v1.0 validation set (Table 2) were
performed to analyze the impact of each module. The full model’s performance
is reported in row 1, while subsequent rows correspond to the removal of speciﬁc
modules to assess their signiﬁcance.

Multi-modal Dialogue State Tracking for Playing GuessWhich Game
563
Table 2. Ablation studies on major modules by removing it from the full model on
VisDial v1.0 val.
# Model
MRR↑R@1↑
R@5↑
R@10↑Mean↓PMR↑
1
Full model 34.01
24.99 49.49 63.92
17.52
99.60
2
-VRDS
27.01
20.25
40.35
52.57
55.24
97.29
3
-STrack
25.45
19.08
38.47
49.82
75.80
96.40
4
-LMSE
31.59
23.09
46.53
60.45
29.30
98.55
5
-LPL
32.28
23.61
47.83
62.16
25.30
99.11
Removing the VRDS module resulted in a signiﬁcant drop in PMR by 2.30
points, an increase in Mean by 37.72 absolute points to 55.24, and a decrease in
MRR by approximately 7 points to 27.01. These ﬁndings highlight the impor-
tance of three-hop reasoning using the recursive self-reference equation (R-SRE)
for accurate image guessing. The role of the R-SRE operation is indirectly veri-
ﬁed as it enables visually related reasoning, providing more distinguishing clues
for generating visually related questions (refer to Fig. 3 for details).
Removing the STrack module, which relies solely on the image caption at the
0th round, resulted in a decrease in PMR to 96.40, emphasizing the importance
of caption information. The performance metrics of this conﬁguration were lower
than those of removing VRDS, indicating that the STrack module facilitates the
incorporation of additional textual semantics and visual concepts into dialogue
states. Additionally, the results from rows 4 and 5 demonstrate the eﬀectiveness
of both supervisions. Removing LPL led to a slight reduction in PMR, while
removing LMSE decreased PMR by nearly 1 point, highlighting the eﬃcacy of
the combined LPL and LMSE in training the model eﬃciently.
Case Studies. In Fig. 3, a comparison with recent ADQ [8] reveals two key
observations. Firstly, our model eﬀectively avoids repetition over the 10 rounds
by combining VRDS and STrack modules. This prevents repetitive context and
ensures that context vector z(t) remains distinctive and informative, leading to
non-repetitive questions generated. Secondly, our model generates a higher num-
ber of visually related questions. In the ﬁrst example, it asks three color-related
questions (highlighted in red), while [8] and humans ask questions that are not
focused on color. In the last two examples, our model initiates the dialogue with
four and ﬁve image-related questions, respectively. These ﬁndings indicate that
mental model of the unseen image enables the generation of image-like represen-
tations, prompting QBot to ask visually related questions.
5
Conclusion
This paper proposes DST, a novel dialogue state tracking approach for visual
dialog question generation in the GuessWhich game. DST maintains and updates

564
W. Pang et al.
dialogue states, including word and mental image representations, enabling men-
tally related reasoning. Unlike previous studies, DST performs visual reasoning
using mental representations of unseen images, achieving state-of-the-art perfor-
mance. Future work will focus on exploring and visualizing the image state in
DST.
Acknowledgements. We thank the reviewers for their comments and suggestions.
This paper was partially supported by the National Natural Science Foundation of
China (NSFC 62076032), Huawei Noah’s Ark Lab, MoECMCC “Artiﬁcial Intelli-
gence” Project (No. MCM20190701), Beijing Natural Science Foundation (Grant No.
4204100), and BUPT Excellent Ph.D. Students Foundation (No. CX2020309).
References
1. Agrawal, A., et al.: VQA: visual question answering. In: ICCV, pp. 2425–2433
(2015)
2. Das, A., et al.: Visual dialog. In: CVPR, pp. 326–335 (2017)
3. Das, A., Kottur, S., Moura, J.M., Lee, S., Batra, D.: Evaluating visual conversa-
tional agents via cooperative human-AI games. In: HCOMP (2017)
4. Das, A., Kottur, S., Moura, J.M., Lee, S., Batra, D.: Learning cooperative visual
dialog agents with deep reinforcement learning. In: ICCV, pp. 2951–2960 (2017)
5. Jang, E., Gu, S., Poole, B.: Categorical reparameterization with gumbel-softmax.
In: ICLR (2017)
6. Lee, S.W., Gao, T., Yang, S., Yoo, J., Ha, J.W.: Large-scale answerer in questioner’s
mind for visual dialog question generation. In: ICLR (2019)
7. Lu, J., Batra, D., Parikh, D., Lee, S.: ViLBERT: pretraining task-agnostic visiolin-
guistic representations for vision-and-language tasks. In: NeurIPS (2019)
8. Murahari, V., Chattopadhyay, P., Batra, D., Parikh, D., Das, A.: Improving gen-
erative visual dialog by answering diverse questions. In: EMNLP, pp. 1449–1454
(2019)
9. Paivio, A.: Imagery and verbal processes (1971)
10. Pang, W.: Multi-round dialogue state tracking by object-entity alignment in visual
dialog. In: CICAI (Oral) (2023)
11. Pang, W., Wang, X.: Guessing state tracking for visual dialogue. In: Vedaldi, A.,
Bischof, H., Brox, T., Frahm, J.-M. (eds.) ECCV 2020. LNCS, vol. 12361, pp.
683–698. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-58517-4 40
12. Pang, W., Wang, X.: Visual dialogue state tracking for question generation. In:
Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, pp. 11831–11838. AAAI
(Oral) (2020)
13. Shuster, K., Humeau, S., Bordes, A., Weston, J.: Image-chat: engaging grounded
conversations. In: ACL, pp. 2414–2429 (2020)
14. Testoni, A., Shekhar, R., Fern´andez, R., aella Bernardi, R.: The devil is in the
details: a magnifying glass for the guesswhich visual dialogue game. In: Proceedings
of the 23rd SemDial Workshop on the Semantics and Pragmatics of Dialogue, pp.
15–24 (2019)
15. Vaswani, A., et al: Attention is all you need. In: NeurIPS (2017)
16. de Vries, H., Strub, F., Chandar, S., Pietquin, O., Larochelle, H., Courville, A.:
GuessWhat?! visual object discovery through multi-modal dialogue. In: CVPR,
pp. 4466–4475 (2017)

Multi-modal Dialogue State Tracking for Playing GuessWhich Game
565
17. Zhang, Y., et al.: Generating informative and diverse conversational responses via
adversarial information maximization. In: NeurIPS (2018)
18. Zhang, Y., Galley, M., Gao, J., Schwing, A., Forsyth, D.: Fast, diverse and accurate
image captioning guided by part-of-speech. In: CVPR, pp. 10695–10704 (2019)
19. Zhao, L., Lyu, X., Song, J., Gao, L.: GuessWhich? visual dialog with attentive
memory network. Pattern Recogn. 114, 107823 (2021)
20. Zheng, D., Xu, Z., Meng, F., Wang, X., Wang, J., Zhou, J.: Enhancing visual
dialog questioner with entity-based strategy learning and augmented guesser. In:
Findings of EMNLP, pp. 1839–1851 (2021)
21. Zhou, M., Arnold, J., Yu, Z.: Building task-oriented visual dialog systems through
alternative optimization between dialog policy and language generation. In:
EMNLP, pp. 143–153 (2019)

Diagnosis Then Aggregation: An
Adaptive Ensemble Strategy
for Keyphrase Extraction
Xin Jin1,2, Qi Liu1,2(B), Linan Yue1,2, Ye Liu1,2, Lili Zhao1,2, Weibo Gao1,2,
Zheng Gong1,2, Kai Zhang1,2, and Haoyang Bi1,2
1 Anhui Province Key Laboratory of Big Data Analysis and Application,
University of Science and Technology of China, Hefei, China
2 State Key Laboratory of Cognitive Intelligence, Hefei, China
{kingiv,lnyue,liuyer,liliz,weibogao,gz70229,sa517494,
bhy0521}@mail.ustc.edu.cn, qiliuql@ustc.edu.cn
Abstract. Keyphrase extraction (KE) is a fundamental task in the
information extraction, which has recently gained increasing attention.
However, when facing text with complex structure or high noise, cur-
rent individual keyphrase extraction methods fail to handle capturing
multiple features and limit the performance of the keyphrase extraction.
To solve that, ensemble learning methods are employed to achieve bet-
ter performance. Unfortunately, traditional ensemble strategies rely only
on the extraction performance (e.g., Accuracy) of each algorithm on the
whole dataset for keyphrase extraction, and the aggregated weights are
commonly ﬁxed, lacking ﬁne-grained considerations and adaptiveness to
the data. To this end, in this paper, we propose an Adaptive Ensemble
strategy for Keyphrase Extraction (AEKE) that can aggregate individual
KE models adaptively. Speciﬁcally, we ﬁrst obtain the multi-dimensional
abilities of individual KE models by employing cognitive diagnosis meth-
ods. Then, based on the diagnostic abilities, we introduce an adaptive
ensemble strategy to yield an accurate and reliable weight distribution for
model aggregation when facing new data, and further apply it to improve
keyphrase extraction in the model aggregation. Extensive experimental
results on real-world datasets clearly validate the eﬀectiveness of AEKE.
Code is released at https://github.com/kingiv4/AEKE.
Keywords: Keyphrase Extraction · Ensemble Learning · Cognitive
Diagnosis
1
Introduction
How to extract the needed information from the huge amount of unstructured
knowledge is the fundamental problem in the ﬁeld of natural language process-
ing today [19,20,33]. Among the information extraction methods, keyphrase
extraction (KE) has garnered signiﬁcant attention [14,27,34] as it can enhance
the eﬃciency of natural language processing and beneﬁt numerous downstream
tasks, such as information retrieval [14] and document summarization [25].
c
⃝The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 566–578, 2024.
https://doi.org/10.1007/978-981-99-8850-1_46

An Adaptive Ensemble Strategy for Keyphrase Extraction
567
Fig. 1. Part (a) shows the traditional ensemble strategy based on Accuracy. Since
the three methods perform consistently across the whole dataset, they are aggregated
equally when encountering new data. However, part (b) shows that there are diﬀerences
among the methods from a ﬁne-grained perspective. When dealing with Sports news,
more emphasis should be placed on the two methods (i.e. Bert-Chunk and SpanBert-
Rank) that are more capable in Sports.
The goal of keyphrase extraction is to extract several keyphrases from docu-
ments that can represent the main information of the documents. For example,
given a text document “ The authors had given a method for the construction
of panoramic image mosaics with global and local alignment. ”, the keyphrase
extraction method can identify “panoramic image mosaics, global alignment,
local alignment” as the representative keyphrases. Finally, for evaluation, Accu-
racy, Precision, Recall and F1-score metrics are commonly employed to evaluate
the performance of keyphrase extraction algorithms [10,29,32].
Despite previous approaches achieving promising results, when facing text
with complex structure (e.g. long and diﬃcult sentences) or high noise (e.g.
text from diﬀerent domains), these individual approaches fail to capture var-
ious features in the above text and have limited performance. To this end, a
straightforward approach is to exploit the ensemble methods to aggregate dif-
ferent keyphrase extraction models to achieve better keyphrase extraction.
Figure 1(a) presents a traditional ensemble strategy that aggregates individ-
ual KE methods based on the Accuracy. However, unfortunately, in practice,
the traditional methods can not always achieve satisfying results and even cause
a negative impact on keyphrase extraction. Speciﬁcally, since SpanBert-Rank
[28], Bert-Chunk, [28] and Bert-TagKPE [28] perform consistently in Accuracy
on the overall dataset, we should aggregate these methods equally from the
perspective of traditional ensemble strategy when facing new data about Sports
topic. However, as shown in Fig. 1(b), from a more ﬁne-grained perspective, both
Bert-Chunk and Span-Rank outperform Bert-TagKPE on the Sports topic, while
they perform poorly on the Medical. Therefore, when facing new Sports data,
we should focus more on Bert-Chunk and SpanBert-Rank rather than dealing
with all three methods equally during the model aggregation. In this paper, we
deﬁne the extraction ability of the KE model for diﬀerent topic domains as the
multi-dimensional extraction ability of KE (See Sect. 3 for detail).
From the above observations, we can conclude that traditional ensemble
methods fail to consider the multi-dimensional extraction abilities of individ-
ual models, and instead focus only on the performance of individual models

568
X. Jin et al.
with a single metric (e.g., Accuracy), degrading the performance of the ensem-
ble. Therefore, we argue that this “ensemble pattern” can be further explored to
improve the keyphrase extraction.
Along this research line, in this paper, we propose an Adaptive Ensemble
strategy for Keyphrase Extraction (AEKE) based on the multi-dimensional abil-
ities of individual keyphrase extraction models. Speciﬁcally, inspired by the psy-
chometric theories [5,22] from human measurement, we ﬁrst diagnose the multi-
dimensional abilities of diﬀerent keyphrase extraction models by means of cogni-
tive diagnostic techniques. Then, based on the diagnostic abilities, we develop an
adaptive ensemble strategy. The strategy will adaptively adjust the aggregation
weights for diﬀerent samples to achieve better ensemble performance. Finally,
experiments over two datasets, including OpenKP [32] and Inspec [15], validate
the eﬀectiveness of our AEKE.
2
Related Work
Keyphrase Extraction aims to select a set of phrases that could summarize
the main topics discussed in the document [14]. The algorithms in keyphrase
extraction are commonly divided into supervised and unsupervised methods.
Speciﬁcally, unsupervised methods [2,3,24] mainly used diﬀerent features of the
document such as topic features, phrase frequency and so on to make keyphrase
extraction. In supervised methods [7,29], pre-trained language models have been
exploited and achieved competitive performance with annotation of the corpus.
Ensemble Learning can fuse the knowledge of individual models together
to achieve competitive performance via voting schemes based on some learned
features, which is widely used in machine learning tasks [8,25]. Traditional vot-
ing schemes include unweighted averaging and weighted voting. Among them,
unweighted averaging of the outputs of the base learners in an ensemble is the
most followed approach for fusing the outputs [11]. It considers the output results
of each learner equally but ignores the diﬀerences between learners. On the other
hand, weighted voting methods [11] tend to assign diﬀerent weights to diﬀerent
learners based on their unidimensional ability. Such ability is often assessed by
a single traditional metric on the history datasets. But the weights are constant
during the model aggregation. In ensemble strategies of keyphrase extraction,
mainstream methods employed unweighted averaging and weighted voting meth-
ods to aggregate individual KE models. However, these methods still suﬀered
from relying on the unidimensional ability (e.g., Accuracy, Precision) of individ-
ual KE models to achieve aggregation, resulting in limited performance in the
ensemble. To solve that, we develop an adaptive ensemble strategy for keyphrase
extraction from the perspective of multi-dimensional abilities.
Cognitive Diagnosis is a fundamental task in many real-world scenarios
(e.g., business [17] and education [12,13,31]). The main goal of cognitive diag-
nosis is to measure learners’ proﬁciency proﬁles of abilities to ﬁnish speciﬁc tasks
from their observed behaviors [31]. For instance in education, it can be used to
infer student (as learner) knowledge proﬁciency (as ability) by fully exploiting

An Adaptive Ensemble Strategy for Keyphrase Extraction
569
their responses of answering each exercise (as task). Most of the existing cog-
nitive diagnosis models (CDMs) [5,12,22] are well designed from psychometric
theories of human measurement. Among them, item response theory (IRT) [22]
is the most classic CDMs which assumes the probability of the learner si cor-
rectly ﬁnishing a task ej, i.e., rij = 1, increases with learner ability θi while
decreasing with task diﬃculty βj. Among them, the user ability and task diﬃ-
culty are trainable unidimensional parameters [18]. A typical formulation of IRT
is P(rij = 1) = sigmoid((θi −βj) · aj), where aj is an optional task discrimina-
tion item. Recently, some works extended the previous basic models to capture the
more complex relationships among users, tasks, and abilities. The typical model
is NeuralCD [31] which introduced neural networks F(·) to model high-level inter-
action between learners/abilities and tasks, i.e., P(rij = 1) = F(θi −βj).
Inspired by the psychometric theories from human measurement, the multi-
dimensional evaluation of KE algorithms can also beneﬁt from the more ﬁne-
grained assessment of human learning performance.
3
Problem Deﬁnition
Cognitive Diagnosis for Keyphrase Extraction. Following the NeuralCD
[31] which is a cognitive diagnostic model (CDMs), we introduce the deﬁnition
of the cognitive diagnosis problem for keyphrase extraction algorithms. First, we
denote the algorithms to be evaluated as learners and the CDMs as diagnosers.
Then, with the diagnoser, we can evaluate the multi-dimensional abilities of
learners on diﬀerent skills, which are used to describe how well an algorithm
performs on a particular category of samples.
Besides, in our work, since the topic of documents contains the main infor-
mation and represents the speciﬁc textual features of keyphrase [23], we take the
topics of documents as skills. For instance, topics on Sports and Medical convey
a totally diﬀerent message. Therefore, we deﬁne speciﬁc skills as speciﬁc topics
of documents and one topic for one skill.
To design our diagnoser, we consider a well-trained learner set S
=
{s1, ..., sN}, a sample set E = {e1, ..., eM} which is the dataset in our task,
and a skill (topic) set C = {c1, ..., cP }. N and M denote the number of learners
to be aggregated and samples in the dataset. P denotes the number of skills as
a hyper-parameter in our task. Then the learner’s output results on each sample
as response logs R, which are denoted as a set of triplet (s, e, rij), where s ∈S,
e ∈E and rij is the score that learner i got on sample j. The top 5 results
of keyphrase extraction are transferred to a score (0 or 1). We denote rij = 1
if learner i predicts more than one keyphrase correctly and rij = 0 otherwise.
Meanwhile, an explicitly pre-deﬁned sample-skill relevancy matrix Q should also
be given. Q = {Qij}M×P , where Qij = 1 if sample ei is related to skill pj and
Qij = 0 otherwise. Given the learner-sample response matrix R and the sample-
skill matrix relevancy Q, we could estimate the multi-dimensional abilities of
diﬀerent learners on diﬀerent skills through the diagnoser.

570
X. Jin et al.
Adaptive Ensemble Strategy. Figure 1(a) illustrates the problems encoun-
tered with traditional ensemble strategies. They only focus on the performance
of keyphrase extraction algorithms on a single metric, while ignoring the diﬀer-
ences in multi-dimensional abilities. To solve that, from the perspective of the
multi-dimensional abilities of the keyphrase extraction algorithms, we use the
results of cognitive diagnosis to design adaptive ensemble strategies.
With the cognitive diagnostic module, we ﬁrst obtain diagnostic results that
include the multi-dimensional abilities of each algorithm and the characteristics
(e.g., diﬃculty, discrimination, topic) of the data. Then, in the face of the new
document n, we design the ensemble strategy of adaptive weight adjustment
based on the above diagnostic results, including the multi-dimensional abilities,
diﬃculty, discrimination, and topic. Among them, the multi-dimensional abilities
represent the characteristics of the KE algorithms, while the diﬃculty, discrim-
ination, and topic represent the characteristics of the samples. The goal of our
strategy is to construct a relationship among diagnostic results and get more
reasonable voting weights w for algorithms adaptively to get a better ensemble
performance on every new document.
Problem Deﬁnition. Given the multi-dimensional abilities of KE algorithms
and features of the new document, our goal is to design an adaptive ensemble
strategy to adjust the aggregation weights to improve the keyphrase extraction.
4
Adaptive Ensemble Strategy via Cognitive Diagnosis
In this section, we present the details of AEKE for keyphrase extraction, which
contains two stages. First, in the cognitive diagnostic stage, we follow Neu-
ralCD [31] diagnostic approach and perform ﬁne-grained diagnostics on the
performance of various individual keyphrase extraction models to obtain their
multi-dimensional abilities. In the ensemble stage, we design an adaptive ensem-
ble strategy based on the diagnostic multi-dimensional abilities and document
characteristics to get a better ensemble performance.
4.1
Cognitive Diagnose for Keyphrase Extraction Algorithms
Learner and Sample Factors. In our task, since we only focus on the ability
of the diﬀerent skills, each learner is represented with a one-hot vector sz ∈
{0, 1}1×N as input, where N denotes the number of learners to be evaluated. In
the same way, we represent sample ed input as one-hot vector ed ∈{0, 1}1×M.
Skill Factors. We want to make the topics as skills, as topic information is
valuable in keyphrase extraction tasks. However, the published datasets do not
contain topic labels for documents. To this end, in this paper, we employ the
LDA [1] (Latent Dirichlet Allocation topic model) to obtain the topic labels
by unsupervised clustering of the documents. Especially, LDA has better inter-
pretability and the topical tokens for the clusters can be used as the explicit
description for skills, which is great of importance for cognitive diagnosis.

An Adaptive Ensemble Strategy for Keyphrase Extraction
571
After clustering the documents into P topics by LDA, we can obtain the
sample-skill matrix Q ∈{0, 1}M×P . By this method, the topic label of each
sample will be obtained.
Latent Factors. Following NeuralCD [31], with the model we can get the multi-
dimensional abilities of the learner ha and the diﬃculty hd and discrimination
hdisc
d
of the sample. Among them, the ha indicates the ability of the learner to
process samples on diﬀerent topics. The hd represents the degree of diﬃculty
the learner to solving the problem. Besides, the hdisc
d
indicates the capability of
samples to diﬀerentiate the proﬁciencies of learners. Samples with low discrimi-
nation mean that of low quality: they tend to have annotation errors or do not
make sense.
Interaction and Prediction. Here, we exploit neural networks to model the
relationship between learner ability factor ha and skill diﬃculty factor hd. The
probability Y is deﬁned as the ability compared with the sample in the covered
topic as Y = (ha −hd) × hdisc
d
. Then, we use the full connection layers F to
predict the score y of learner z on the sample d: y = σ(F(Y )). Finally, the whole
objective of the diagnoser is deﬁned with the cross entropy loss function:
L = −

i
(ri log yi + (1 −ri) log(1 −yi)),
(1)
where r is the true score. Based on Eq. (1), we can get the multi-dimensional
abilities of the keyphrase extraction algorithms.
4.2
Adaptive Ensemble Strategy
With the diagnostic module, we get the multi-dimensional abilities of each
keyphrase extraction algorithm. Based on such diagnostic results, we propose
an adaptive ensemble strategy to better aggregate the results of each extraction
algorithm in the face of new test samples.
Inputs for Adaptive Ensemble Strategy. The inputs to the adaptive ensem-
ble strategy include the abilities of individual KE algorithms and the features
of the new sample. Among them, the KE algorithms’ abilities are obtained from
the previous diagnostic module, indicating the multi-dimensional abilities of the
KE algorithms on diﬀerent topics.
Features of the new sample contain information about the topic, diﬃculty
and discrimination. Speciﬁcally, the topic information is associated with the
diagnosed algorithm ability. The diﬃculty and discrimination information can
reﬂect the implicit features of the algorithm in dealing with such problems to
some extent. The information of the new sample is adequately represented by
these three features.
To sum up, based on the topic model obtained in Sect. 4.1, the new sample is
input and its distribution over each implicit topic is obtained as its topic infor-
mation cn. Each of its dimensions represents the probability of its distribution
on that implicit topic.

572
X. Jin et al.
Besides, since unseen samples are not used as input to the diagnostic module,
the diﬃculty and discrimination of the samples cannot be directly obtained. To
this end, we design a non-parametric module to predict the diﬃculty and discrim-
ination. Speciﬁcally, as there is a close relationship between original texts and
the factors of samples including the diﬃculty and discrimination, we choose to
predict diﬃculty and discrimination based on semantic K-nearest neighbor [26]
methods. Here, given the token sequence of original texts of keyphrase extraction
samples Dw = {dw
1 , dw
2 , ..., dw
n }, we map each word of Dw into word embedding
by BERT [6], and get the document embedding by applying mean-pooling, where
nw is the length of the word sequence. We use the document embedding ed as
input representation for the new sample:
ed = MeanPool(BERT([dw
1 , dw
2 , ..., dw
nw])).
(2)
Then, we match and retrieve the textual representations of the new samples
with the representations of the samples entering in the diagnosis and ﬁnd the
K closest samples. These samples are able to get the corresponding diﬃculty
{d1, ..., dk} and discrimination {disc1, ..., disck} by diagnosis. Finally, we average
the diﬃculty and discrimination retrieved as the diﬃculty dn and discrimination
discn of the new sample.
Weight Prediction. After getting the above inputs, we need to get the most
appropriate ensemble weights for each new sample. To ensure the interpretability
of the weights, we design the ensemble strategy for the new samples by the
following calculation:
w = SoftMax(ha · cn × dn × discn),
(3)
where w ∈RN×1, ha ∈RN×P , cn ∈R1×P , dn and discn are single numbers.
5
Experiments
5.1
Experimental Setup
Dataset Description. We conduct experiments on two common keyphrase
extraction datasets, i.e., OpenKP [32] and Inspec [15]. OpenKP is an open-
domain keyphrase extraction dataset with various domains. In our settings, we
choose the valid set (6,600 documents) of OpenKP for experiments. Besides,
Inspec consists of short documents selected from scientiﬁc journal abstracts
which are labeled by the authors, we choose the test (500 documents) and valid
(1,500 documents) sets in this paper. The detailed statistics of the datasets are
shown in Table 1. In particular, in our task, it is necessary to divide the dataset
into two subsets, one for the diagnostician module and the other for the ensemble
experiments. Therefore, we split the two datasets according to 3:1.
Algorithms to be Aggregated. To better train the diagnoser module and
obtain the multi-dimensional abilities of each KE algorithm, we select 24 repre-
sentative KE algorithms as follows:

An Adaptive Ensemble Strategy for Keyphrase Extraction
573
– Unsupervised methods: Firstphrase1, YAKE [4], TextRank [24], SingleR-
ank [30], TopicRank [3], TopicalPageRank [21], PositionRank [9], Multipar-
titeRank [2], SIFRank [29].
– Supervised methods: BERT-RankKPE [28], SpanBERT-Variants*5 [28],
BERT-ChunkKPE [28], BERT-SpanKPE [28], BERT-JointKPE [28], BERT-
TagKPE [28], RoBERTa-Variants*5 [28].
Among them, supervised methods are trained on the OpenKP training set
(134k documents). We obtain the response logs of learners on all samples on the
datasets. Following the past research [31], we split the response logs into the
training set, validation set and test set as 7:1:2.
Table 1. Statistics of keyphrase
extraction datasets.
Statistics
OpenKP Inspec
Document Number
6,616
2,000
Document Len Average
900
128
Keyphrase Average
2.2
9.8
Keyphrase Len Average 2.0
2.5
Table 2. Evaluation of all diagnosers through
predicting learner performance on samples.
Methods
OpenKP
Inspec
AUC
Accuracy RMSE AUC
Accuracy RMSE
DINA
0.563
0.545
0.559
0.538
0.512
0.578
IRT
0.576
0.540
0.542
0.560
0.545
0.544
NeuralCD 0.914 0.869
0.340 0.883 0.762
0.379
Baselines. For the cognitive diagnosis, we evaluate the performance of Neu-
ralCD with other well-known CDMs (i.e., IRT [22] and DINA [5]). Among them,
IRT is the most popular cognitive diagnosis method, it models students’ latent
traits and the parameters of exercises like diﬃculty and discrimination with a
logistic-like function. DINA is the ﬁrst method to design the Q-matrix and it uses
binary variables to represent mastery of skills. NeuralCD [31] is a neural cogni-
tive diagnostic framework, which leverages multi-layers for modeling the complex
interactions of students and exercises, aiming to diagnose students’ cognition by
predicting the probability of the student answering the exercise correctly.
For the ensemble learning strategy, we choose to compare our approach with
the average strategy and the weighted voting strategy. The weights are con-
stant based on the performance of the history dataset evaluated on the tradi-
tional metrics (e.g., shape Precision). We also choose several individual keyphrase
extraction methods from the 24 representative KE algorithms described before
as baselines.
Experimental Settings. In our experiment, we use the pre-trained uncased
BERT-based [6] model with 768 dimensions hidden representation as our tool.
In our experiments, we set P = 10 for both two datasets. As the number of top-
ics P is the most important hyper-parameter in AEKE, we conduct sensitivity
experiments on it in Sect. 5.3. To set up the training process for the diagnos-
tic module, we initialize all network parameters with Xavier initialization. The
Adam optimizer [16] is used in the experiment while the learning rate is set to
1 https://github.com/boudinﬂ/pke..

574
X. Jin et al.
0.0002. We train all diagnosers for 20 epochs and select the best model on the
validation set for testing. All experiments are run on two NVIDIA A100 GPUs.
5.2
Evaluation Metrics
Learner Performance Prediction. Generally, the ground truth of the ability
of learners can’t be obtained, it’s diﬃcult to evaluate the performance of cogni-
tive diagnosis models. In most works, the prediction of learners’ performance is
an indirect way of evaluating the model. Evaluation metrics including Accuracy,
RMSE (Root Mean Square Error), and AUC (Area Under the Curve) are cho-
sen. Among them, better predictions have higher values in Accuracy and AUC,
while the lower RMSE value, the better the prediction is achieved.
Model Aggregation. We realize model aggregation based on each learner’s
proﬁciency in the topics. The aggregation is tested on both OpenKP and Inspec
datasets with several traditional keyphrase extraction metrics including Preci-
sion, Recall, and F1-score.
5.3
Experimental Results
Learner Performance Prediction. The experimental results are reported in
Table 2, we have several observations as follows. First, NeuralCD performs the
best on both OpenKP and Inspec, demonstrating NeuralCD can eﬀectively eval-
uate the ability of keyphrase extraction algorithms. Besides, the traditional mod-
els including IRT and DINA perform poorly, which reﬂects that the relationship
between learners’ ability and samples’ features is too diﬃcult to capture, and
Table 3. Model aggregation results of popular keyphrase extraction models. The top
part lists some unsupervised methods, the middle part lists the supervised methods,
and the bottom part lists the ensemble methods.
Methods
OpenKP
Inspec
P@5
R@5 F1@5 P@5
R@5 F1@5
Firstphrase
19.5
36.7
23.6
24.0
15.0
17.3
YAKE [4]
12.1
29.1
16.7
21.0
13.6
15.5
TextRank [24]
5.5
14.2
7.9
31.7
19.2
22.6
SingleRank [30]
14.4
34.5
19.7
33.0
20.2
23.6
TopicRank [3]
14.4
30.3
19.6
28.2
16.9
20.0
BERT-JointKPE [28]
22.7
57.1
30.3
37.9
24.3
27.9
SpanBERT-RankKPE [28]
23.2
61.8
33.9
38.7
24.9
28.6
RoBERTa-TagKPE [28]
23.0
58.9
31.8
36.9
23.7
27.2
Averaging
23.7
61.0
33.5
39.1
25.0
28.9
Weighted Voting (Precision) 24.0
61.4
33.7
39.7
25.2
29.4
AEKE
24.5 62.0 34.1
40.3 25.8 29.8

An Adaptive Ensemble Strategy for Keyphrase Extraction
575
indirectly proves the eﬀectiveness of neural networks. Through NeuralCD, we
can obtain highly reliable diagnostic results to be applied in the ensemble stage.
Model Aggregation. We compare our AEKE with the traditional aggregation
methods (i.e., weighted voting and averaging) to illustrate the eﬃciency of our
method as presented in Table 3. Among them, weights for weighted voting are
obtained based on the overall performance (precision) of the history datasets
of each keyphrase extraction algorithm. Such weights are constant during the
model aggregation. In general, ﬁrstly, compared to supervised and unsupervised
methods, both AEKE and the baseline ensemble strategy perform better than
individual methods, demonstrating the necessity of ensemble. Besides, our adap-
tive ensemble strategy outperforms the ensemble baseline on both datasets, indi-
cating the eﬀectiveness of aggregation according to multi-dimensional abilities.
Hyper-Parameter Sensitivity Study. In our work, the number of skills P
is a hyper-parameter, which determines how well the topics are clustered and
also inﬂuences the design of the assessment skills. Therefore, in this section, we
investigate the sensitivity of P. Figure 2 shows the performance of AEKE with
diﬀerent topic numbers P on the OpenKP dataset. The experiment shows a rising
trend followed by a falling trend in the eﬀectiveness of the ensemble result as the
number of P increases. 10 is the best topic clustering number for the OpenKP.
Speciﬁcally, when P is small, the result of document topic clustering is poor,
which further aﬀects the cognitive diagnosis of multi-dimensional abilities and
the ensemble procedure. While P > 10, the ensemble results tend to be stable.
Therefore, in this paper, we set P to 10 for our experiments.
Case Study. In this section, to further illustrate the eﬀectiveness of AEKE, we
show a high-quality sample in OpenKP and the ensemble results of weighted vot-
ing and AEKE in Fig. 3. Speciﬁcally, we aggregate the extraction results of three
KE methods (i.e. BERT-Chunk [28], RoBERTa-Rank [28] and RoBERTa-Span
[28]) by our strategy and traditional weighted voting strategy, respectively. In
Fig. 3(a), we illustrate a detailed procedure of our AEKE. First, the new sample
is entered into the diagnosis module and we can obtain the corresponding diag-
nosis results. It is obvious that this sample is a shooting report, which belongs to
Fig. 2. Hyper-parameter
Sensitivity Study.
Fig. 3. Visualized keyphrases extracted by AEKE (a) and
traditional strategy (b).

576
X. Jin et al.
the legal news topic. Its diﬃculty and discrimination indicate that this sample
has high text quality. It also shows the ability of the three methods on legal
news topics. Then, based on the diagnosis results, our AEKE can adaptively
adjust the weights of diﬀerent methods during aggregation to get good ensem-
ble results. In Fig. 3(b), the traditional method relies on the evaluation result
of the three methods on the history datasets evaluated on the single metric
Precision@5, and since the overall results on Precision@5 are similar, the same
weights are constant for all new samples. However, such weights do not achieve
satisfying ensemble results in this new sample. This case serves as a compelling
demonstration of the remarkable ﬂexibility and eﬃciency of AEKE.
It is worth noting that, unlike traditional methods whose ensemble weights
are ﬁxed during aggregation, the weights in AEKE are not constant. Speciﬁcally,
the above case belongs to the Legal topic, and when facing with samples of other
topics (e.g., Sports), AEKE will adjust the ensemble weights adaptively based
on the multi-dimensional abilities of KE methods and features of new sample.
6
Conclusion
In this paper, we proposed an adaptive ensemble strategy (AEKE) based on
cognitive diagnostic techniques in the keyphrase extraction task. To the best
of our knowledge, this is the ﬁrst attempt to aggregate machine learning algo-
rithms from a cognitive diagnostic perspective. To be speciﬁc, we ﬁrst carefully
employed the NeuralCD to evaluate the multi-dimensional abilities of keyphrase
extraction algorithms. Then, based on the diagnostic ability, we developed an
adaptive ensemble strategy to aggregate individual keyphrase extraction meth-
ods. Experimental results on both OpenKP and Inspec datasets demonstrated
the eﬀectiveness of AEKE.
Acknowledgements. This research was supported by grants from the National Key
Research and Development Program of China (Grant No. 2021YFF0901003).
References
1. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. J. Mach. Learn.
Res. 3, 993–1022 (2003)
2. Boudin, F.: Unsupervised keyphrase extraction with multipartite graphs. In: Pro-
ceedings of the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 2 (Short
Papers), pp. 667–672 (2018)
3. Bougouin, A., Boudin, F., Daille, B.: TopicRank: graph-based topic ranking for
keyphrase extraction. In: International Joint Conference on Natural Language Pro-
cessing (IJCNLP), pp. 543–551 (2013)
4. Campos, R., Mangaravite, V., Pasquali, A., Jorge, A.M., Nunes, C., Jatowt, A.:
A text feature based automatic keyword extraction method for single documents.
In: Pasi, G., Piwowarski, B., Azzopardi, L., Hanbury, A. (eds.) ECIR 2018. LNCS,
vol. 10772, pp. 684–691. Springer, Cham (2018). https://doi.org/10.1007/978-3-
319-76941-7 63

An Adaptive Ensemble Strategy for Keyphrase Extraction
577
5. De La Torre, J.: Dina model and parameter estimation: a didactic. Journal of
educational and behavioral statistics 34(1), 115–130 (2009)
6. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of
deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805 (2018)
7. Ding, H., Luo, X.: AttentionRank: unsupervised keyphrase extraction using self and
cross attentions. In: Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing, pp. 1919–1928 (2021)
8. Dong, X., Yu, Z., Cao, W., Shi, Y., Ma, Q.: A survey on ensemble learning. Front.
Comp. Sci. 14, 241–258 (2020)
9. Florescu, C., Caragea, C.: PositionRank: an unsupervised approach to keyphrase
extraction from scholarly documents. In: Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pp.
1105–1115 (2017)
10. Gallina, Y., Boudin, F., Daille, B.: Large-scale evaluation of keyphrase extraction
models. In: Proceedings of the ACM/IEEE Joint Conference on Digital Libraries
in 2020, pp. 271–278 (2020)
11. Ganaie, M.A., Hu, M., Malik, A., Tanveer, M., Suganthan, P.: Ensemble deep
learning: a review. Eng. Appl. Artif. Intell. 115, 105151 (2022)
12. Gao, W., et al.: RCD: relation map driven cognitive diagnosis for intelligent edu-
cation systems. In: Proceedings of the 44th International ACM SIGIR Conference
on Research and Development in Information Retrieval, pp. 501–510 (2021)
13. Gao, W., et al.: Leveraging transferable knowledge concept graph embedding for
cold-start cognitive diagnosis. In: Proceedings of the 46th International ACM
SIGIR Conference on Research and Development in Information Retrieval, pp.
983–992
14. Hasan, K.S., Ng, V.: Automatic keyphrase extraction: A survey of the state of the
art. In: Proceedings of the 52nd Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pp. 1262–1273 (2014)
15. Hulth, A.: Improved automatic keyword extraction given more linguistic knowl-
edge. In: Proceedings of the 2003 Conference on Empirical Methods in Natural
Language Processing, pp. 216–223 (2003)
16. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
17. Liu, C., Yang, L., Gao, W., Li, Y., Liu, Y.: MuST: an interpretable multidimen-
sional strain theory model for corporate misreporting prediction. Electron. Com-
mer. Res. Appl. 57, 101225 (2023)
18. Liu, Q.: Towards a new generation of cognitive diagnosis. In: IJCAI, pp. 4961–4964
(2021)
19. Liu, Y., et al.: Technical phrase extraction for patent mining: a multi-level app-
roach. In: 2020 IEEE International Conference on Data Mining (ICDM), pp. 1142–
1147. IEEE (2020)
20. Liu, Y., et al.: TechPat: technical phrase extraction for patent mining. ACM Trans.
Knowl. Disc. Data 17, 1–31 (2023)
21. Liu, Z., Huang, W., Zheng, Y., Sun, M.: Automatic keyphrase extraction via topic
decomposition. In: Proceedings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pp. 366–376 (2010)
22. Lord, F.: A Theory of Test Scores. Psychometric Monographs (1952)
23. Meng, R., Wang, T., Yuan, X., Zhou, Y., He, D.: General-to-speciﬁc transfer label-
ing for domain adaptable keyphrase generation. arXiv preprint arXiv:2208.09606
(2022)

578
X. Jin et al.
24. Mihalcea, R., Tarau, P.: TextRank: bringing order into text. In: Proceedings of
the 2004 Conference on Empirical Methods in Natural Language Processing. pp.
404–411 (2004)
25. Papagiannopoulou, E., Tsoumakas, G.: A review of keyphrase extraction. Wiley
Interdisc. Rev. Data Min. Knowl. Disc. 10(2), e1339 (2020)
26. Peterson, L.E.: K-nearest neighbor. Scholarpedia 4(2), 1883 (2009)
27. Song, M., Feng, Y., Jing, L.: A survey on recent advances in keyphrase extraction
from pre-trained language models. In: Findings of the Association for Computa-
tional Linguistics, EACL 2023, pp. 2108–2119 (2023)
28. Sun, S., Liu, Z., Xiong, C., Liu, Z., Bao, J.: Capturing global informativeness in
open domain keyphrase extraction. In: Wang, L., Feng, Y., Hong, Yu., He, R. (eds.)
NLPCC 2021. LNCS (LNAI), vol. 13029, pp. 275–287. Springer, Cham (2021).
https://doi.org/10.1007/978-3-030-88483-3 21
29. Sun, Y., Qiu, H., Zheng, Y., Wang, Z., Zhang, C.: SIFRank: a new baseline for
unsupervised keyphrase extraction based on pre-trained language model. IEEE
Access 8, 10896–10906 (2020)
30. Wan, X., Xiao, J.: Single document keyphrase extraction using neighborhood
knowledge. In: AAAI, vol. 8, pp. 855–860 (2008)
31. Wang, F., et al.: Neural cognitive diagnosis for intelligent education systems. In:
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 34, pp. 6153–
6161 (2020)
32. Xiong, L., Hu, C., Xiong, C., Campos, D., Overwijk, A.: Open domain web
keyphrase extraction beyond language modeling. In: Proceedings of the EMNLP-
IJCNLP 2019, pp. 5175–5184 (2019)
33. Yue, L., Liu, Q., Du, Y., An, Y., Wang, L., Chen, E.: DARE: disentanglement-
augmented rationale extraction. In: Advances in Neural Information Processing
Systems (2022)
34. Zhao, H., Lu, M., Yao, A., Guo, Y., Chen, Y., Zhang, L.: Physics inspired opti-
mization on semantic transfer features: an alternative method for room layout esti-
mation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 10–18 (2017)

Author Index
A
Ai, Jin
II-526
Ai, Yusen
I-143
Ao, Xiang
I-478
Azhar, Shanila
I-116
B
Bai, Bing
II-310, II-463
Bai, Lei
I-529
Bi, Haoyang
I-566
Bo, Zihao
II-100
C
Cai, Jiayue
II-452
Cao, Rongqiang
II-359, II-532
Cao, Xun
I-78
Cao, Yancheng
II-557, II-562
Cao, Yuqin
II-512
Chan, Shan
I-116
Chang, Chunqi
II-452
Chen, Changyi
I-104
Chen, Fangshu
II-253
Chen, Gang
II-569
Chen, Guanlin
I-156
Chen, Haoming
II-378
Chen, Hongmei
II-159
Chen, Hui
I-430, II-87
Chen, Jianteng
I-3
Chen, Jiawei
II-189
Chen, Jiexin
II-378
Chen, Jingyuan
II-399
Chen, Li
II-512
Chen, Shi
II-46
Chen, Tianzhu
II-298
Chen, Wanyi
I-170
Chen, Weifan
II-539
Chen, Xiaoxue
I-143
Chen, Xinghui
II-562
Chen, Xun
II-476
Chen, Yanwen
I-364
Chen, Yilun
II-172, II-177
Chen, Yuantao
I-3
Chen, Zheqian
I-493
Cong, Yulai
II-322
D
Dai, Nan
I-182
Dai, Qionghai
I-91
Deng, Zhiwei
II-476
Ding, Runwei
I-182
Ding, Zhuoye
II-463
Dong, Bin
II-15
Dong, Zhiyan
II-411, II-575
Du, Jiachen
II-61
Du, Junping
II-366
Duan, Huiyu
II-46, II-512
Duan, Ruixue
I-554
Duan, Yaofei
II-378
F
Fan, Wenyan
I-430
Fan, Yushun
II-229
Feng, Siheng
II-562
Feng, Wei
I-329
Fu, Fuxiang
II-435
Fu, Kairui
I-415
Fu, Tao
II-113
Fu, Xinyi
II-61
Fu, Ying
I-53, I-130, I-195, I-244
G
Gao, Jie
II-366
Gao, Mengzhou
I-460
Gao, Qian
I-156
Gao, Weibo
I-566
Gao, Xu
II-172, II-177
Gao, Zhijie
I-195
Ge, Rongjun
II-201, II-422
Geng, Long
I-293
Gong, Jiangtao
II-172, II-177
Gong, Zheng
I-566
Gu, Keyi
II-557
Gu, Siyuan
I-256
Guan, Haiyang
II-242
© The Editor(s) (if applicable) and The Author(s), under exclusive license
to Springer Nature Singapore Pte Ltd. 2024
L. Fang et al. (Eds.): CICAI 2023, LNAI 14473, pp. 579–583, 2024.
https://doi.org/10.1007/978-981-99-8850-1

580
Author Index
Guan, Zeli
II-366
Guo, Huike
II-147
Guo, Xi
II-435
Guo, Xiaomin
I-116
Guo, Yuchen
II-100
H
Han, Bin
II-435
Han, Changlin
II-335
Han, Chenming
I-28
Han, Wei
I-430
Han, Xu
II-213
Han, Zhaoyang
II-545
Han, Zhengxiao
II-81, II-172, II-177
He, Chunming
I-16
He, Li
II-463
He, Liang
I-329
He, Wenchao
I-232
He, Yuanjian
II-322
Hou, Chao
I-3
Hou, Taixian
II-242
Hou, Yusen
I-104
Hu, Bo
II-575
Hu, Dongfang
II-383
Hu, Liya
II-399
Hu, Menghan
II-526
Hu, Qinghua
I-156
Hu, Runze
I-16
Hu, Yu
II-411
Huang, Lihong
II-87
Huang, Pengfei
II-172, II-177
Huang, Qian
II-569
Huang, Ruqi
I-256
Huang, Xiaoming
I-293
Huang, Yan
I-340
Huang, Yingxuan
I-430
Huang, Yuxin
I-3
J
Jia, Boyang
II-61
Jia, Xiuyi
I-41
Jiang, Xiaoguang
I-269
Jiang, Xinghua
I-493
Jiang, Yan
I-352
Jiao, Pengfei
I-460
Jiao, Yiping
II-422
Jin, Bu
I-473
Jin, Xin
I-566
K
Kang, Ling
II-526
Kang, Wenxiong
II-213
Kang, Xiaoyun
II-87
Kou, Yongqing
II-422
Kuang, Kun
I-415, I-442, II-3
L
Lai, Jiangliang
I-430
Lam, ChanTong
II-378
Lei, Yuxuan
II-189
Li, Bo
I-170, I-400
Li, Chang
II-476
Li, Chongchong
II-285
Li, Fan
I-66
Li, Hesong
I-130
Li, Kai
II-359, II-532
Li, Kun
I-104
Li, Laquan
I-352
Li, Mingcheng
II-189
Li, Minghe
II-147
Li, Ning
I-554
Li, Pengfei
II-172, II-177
Li, Piji
II-545
Li, Tianrui
II-159
Li, Tong
II-113
Li, Wei
I-91, I-517, II-125
Li, Xingyu
I-364
Li, Yang
II-172, II-177
Li, Yichen
I-53
Li, Yue
II-488
Li, Zhuo
I-400
Liang, He
II-81
Liang, Jing
II-562
Liang, Mangui
II-68
Liang, Zuyang
II-452
Liao, Yiyi
I-3
Liao, Yusong
I-505
Lin, Shengyou
II-435
Liu, Cheng-Lin
I-478
Liu, Dun
II-159
Liu, Gaosheng
I-104
Liu, Guohua
I-116
Liu, Hang
II-526
Liu, Haotian
I-473
Liu, Hong
I-207
Liu, Jinfu
I-182
Liu, Jing
II-46, II-512
Liu, Ke
II-500
Liu, Linqi
II-539

Author Index
581
Liu, Mengyuan
I-182
Liu, Naijin
II-15
Liu, Qi
I-566
Liu, Qiankun
I-53, I-244
Liu, Qiong
I-269, I-281
Liu, Qun
I-269
Liu, Rui
I-244
Liu, Tianyu
I-3
Liu, Wenrui
I-493
Liu, Wu
II-488
Liu, Xiang
II-258
Liu, Xiaobo
II-271
Liu, Xiaohong
II-378
Liu, Yadong
II-335
Liu, Yao
II-310
Liu, Yaqian
I-329
Liu, Ye
I-566
Liu, Yebin
I-28
Liu, Yi
I-156
Liu, Yutao
I-16, I-220, I-232
Liu, Yuting
II-285
Liu, Yuxuan
II-3
Liu, Zehao
I-460
Liu, Zhe
II-545
Liu, Zhi
I-305
Liu, Zhikun
II-347
Lou, Haozhe
I-3
Lu, Yifan
I-460
Lu, Yuanxun
I-78
Lu, Yujie
I-430
Lu, Zhangwei
II-41
Luo, Chuan
II-159
Luo, Jie
II-411
Luo, Kangqi
II-463
Luo, Liyi
I-3
Luo, Nairui
II-172, II-177
Lv, Zheqi
I-442
Lyu, Junfeng
II-100
M
Ma, Chuan
II-545
Ma, Gang
II-41
Ma, Ke
II-557
Ma, Zhi-Ming
II-285
Mao, Tingting
II-476
Meng, Fanyang
I-182
Meng, Xiankai
II-253
Miao, Qiaowei
I-415
Min, Xiongkuo
II-46, II-512
Ming, Zuheng
I-329
N
Nie, Mingxing
II-488
Nie, Weiguo
I-156
P
Pan, Junwen
I-156
Pan, Xiaotian
II-526
Pang, Patrick Cheong-Iao
II-378
Pang, Wei
I-541, I-554
Peng, Shu
II-500
Peng, Zhiyong
II-335
Q
Qi, Feifei
II-500
Qian, Jianjun
I-340
Qian, Shengsheng
II-29
Qiao, Donghong
II-526
Quan, Xiongwen
II-147
R
Ren, Tian-ling
II-442
S
Shang, Gaoxing
II-569
Shao, Chenghao
II-476
Shao, Hang
I-28, II-310, II-463
Shao, Ruizhi
I-28
Shao, Tianyu
II-87
Shao, Yanqiu
I-517, II-125
She, Yanhong
II-347
Shen, Penghui
II-442
Shen, Ran
I-493
Shi, Chongdong
II-347
Shi, Yifeng
II-172, II-177
Shi, Yongliang
I-3, II-172, II-177
Si, Yuxuan
II-399
Song, Jingkai
II-442
Song, Xinsheng
II-271
Sun, Gang
I-493
Sun, Shuangpeng
I-66
Sun, Xunsen
I-78
Sun, Xuxue
II-435, II-539
T
Tan, Chao
I-66
Tan, Tao
II-378
Tang, Tao
I-207
Tang, Wenxiao
II-213
Tang, Yahui
II-113

582
Author Index
Tao, Changbiao
I-269
Tao, Guofen
II-435
Tao, Jianhua
II-68
Tian, Ge
II-201
Tian, Xiang
II-87
Tian, Yixin
II-253
Tian, Zhengkun
II-68
Tu, Jiaxin
II-242
V
Valsecchi, Francesca
II-557
W
Wan, Hao
II-213
Wan, Meng
II-359, II-532
Wang, Binfeng
I-195
Wang, Bingkun
II-253
Wang, Di
I-41
Wang, Feng
I-442
Wang, Guoyin
II-136
Wang, Jiahui
II-253
Wang, Jianan
I-130
Wang, Jiarui
II-46
Wang, Jinsong
II-113
Wang, Jue
II-359, II-532
Wang, Liupeng
II-347
Wang, Meiyi
II-452
Wang, Ning
I-493
Wang, Qilong
I-156
Wang, Qiyao
II-81
Wang, Rongsheng
II-378
Wang, Shunli
II-189
Wang, Ti
I-207
Wang, Tongyang
I-340
Wang, Xiaoguang
II-359, II-532
Wang, Xiaoyong
I-53, I-130, I-244
Wang, Yang
I-305
Wang, Yangang
II-359, II-532
Wang, Yashen
II-298
Wang, Yifei
II-29
Wang, Yue
II-285
Wang, Yuhao
II-442
Wang, Zerun
II-87
Wang, Zhenhua
I-317
Wang, Zhihua
II-399
Wang, Zongguo
II-359, II-532
Wei, Xiaoyi
II-242
Wei, Zixiong
II-87
Wen, Yuhang
I-182
Wu, Fei
I-415, I-430, I-442, I-493, II-3,
II-399
Wu, Gaochang
I-28
Wu, Jieyu
I-364
Wu, Jincheng
II-383
Wu, Kejun
I-281
Wu, Meng
II-41
Wu, Xin
I-143
Wu, Xin-Jian
I-478
Wu, Yue
II-526
Wu, Zirui
I-3
X
Xia, Cong
II-201, II-422
Xiang, Hongbo
II-271
Xiang, Qibu
II-229
Xiang, Wang
II-29
Xiang, Xueshuang
II-15
Xiang, Yu
I-529
Xiao, Fengyang
I-16
Xiao, Hongmin
I-3
Xie, Haidong
II-15
Xie, Hao
II-435
Xie, Pengzhan
II-258
Xie, Qiangbin
I-388
Xu, Feng
II-100
Xu, Hongguang
I-505
Xu, Jiangsheng
II-378
Xu, Ke
I-505
Xu, Shuxiang
II-229
Xu, Wenlin
II-113
Xu, Xiaojing
II-100
Xue, Dizhan
II-29
Xue, Zhe
II-366
Y
Yan, Ruyu
II-229
Yan, Shizong
I-116
Yan, Xuedong
II-347
Yan, Zhijie
II-172, II-177
Yan, Zike
I-3
Yang, Dingkang
II-189
Yang, Dongying
II-201
Yang, Jinfu
I-554
Yang, Jingyu
I-104
Yang, Qisheng
II-442
Yang, Ruigang
I-91
Yang, Runyi
I-3

Author Index
583
Yang, Songjin
II-435
Yang, Xinyi
II-399
Yang, Yi
II-442
Yang, You
I-269
Yang, Zheng
I-517, II-125
Ye, Linwei
I-305, I-317
Ye, Xiaoming
II-258
Ye, Xiaoyu
I-3
Ye, Xinhai
II-399
Yi, Jiangyan
II-68
You, Yingxuan
I-207
Yu, Bin
I-376
Yu, Hong
II-136, II-500
Yuan, Jingwei
I-364
Yue, Huanjing
I-104
Yue, Linan
I-566
Z
Zeng, Hao
II-435, II-539
Zhai, Guangtao
II-46, II-512
Zhai, Peng
II-242
Zhang, Bo
I-329
Zhang, Cong
II-452
Zhang, Daoqiang
II-201, II-422
Zhang, Fan
II-347
Zhang, Fengda
II-3
Zhang, Han
II-147
Zhang, He
I-66
Zhang, Jia
II-229
Zhang, Jian
II-526
Zhang, Jianing
I-91
Zhang, Kai
I-566
Zhang, Ke
I-116
Zhang, Lihua
I-388, II-189, II-242, II-411,
II-575
Zhang, Liuxin
II-41
Zhang, Pan
I-16
Zhang, Pengcheng
I-493
Zhang, Qia
II-526
Zhang, Ruichen
I-281
Zhang, Rui-Song
I-478
Zhang, Ruizhe
II-100
Zhang, Shengjie
II-29
Zhang, Shengyu
I-415, I-430, I-442
Zhang, Shuai
I-220
Zhang, Weile
II-322
Zhang, Wenkang
II-539
Zhang, Wenqiao
I-442
Zhang, Xiaohui
II-68
Zhang, Xuecheng
II-298
Zhang, Xuming
I-376
Zhang, Yi
II-298
Zhang, Yixuan
II-539
Zhang, Yu
II-41, II-569
Zhang, Yudong
I-156
Zhang, Yufei
II-253
Zhang, Zhanyun
II-159
Zhao, Hang
II-172, II-177
Zhao, Hao
I-3, I-143, II-172, II-177
Zhao, Jianhui
I-66
Zhao, Kaixing
I-329
Zhao, Lili
I-566
Zhao, Weijuan
II-539
Zhao, Xiao
I-388, II-242
Zhao, Youbing
II-435, II-539
Zhao, Zhou
I-430, I-493
Zheng, Jianing
II-569
Zheng, Wenjin
I-170
Zheng, Yuhan
II-452
Zheng, Yuhang
II-81
Zheng, Yupeng
II-81
Zheng, Zhitong
II-383
Zheng, Zhuoran
I-41
Zhong, Chengliang
II-81
Zhong, Zhide
I-3
Zhou, Guyue
II-172, II-177
Zhou, Lulu
II-258
Zhou, Quan
I-376
Zhou, Wen
II-271
Zhou, Wenjie
II-545
Zhou, Xinyu
II-41
Zhou, Yuan
I-364
Zhou, Zhou
II-136
Zhou, Zongtan
II-335
Zhu, Chenxi
II-435
Zhu, Hao
I-78
Zhu, Pengfei
I-156
Zhu, Xilei
II-512
Zhu, Yucheng
II-512
Zhu, Yuxin
II-512
Zou, Jianxiong
II-526
Zou, Yunhao
I-195


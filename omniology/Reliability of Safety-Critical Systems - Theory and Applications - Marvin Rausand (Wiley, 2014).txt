

RELIABILITY OF 
SAFETY-CRITICAL SYSTEMS 


RELIABILITY OF 
SAFETY-CRITICAL SYSTEMS 
Theory and Applications 
MARVIN RAUSAND 
Department of Production and Quality Engineering 
Norwegian University of Science and Technology 
Trondheim, Norway 
WILEY 

Cover Design: Wiley 
Cover Images: (background) © 0yvind Hagen, Statoil ASA; (inset) © Marvin Rausand 
Copyright © 2014 by John Wiley & Sons, Inc. All rights reserved. 
Published by John Wiley & Sons, Inc., Hoboken, New Jersey. 
Published simultaneously in Canada. 
No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or 
by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as 
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior 
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to 
the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax 
(978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission should 
be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 
07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permission. 
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in 
preparing this book, they make no representation or warranties with respect to the accuracy or 
completeness of the contents of this book and specifically disclaim any implied warranties of 
merchantability or fitness for a particular purpose. No warranty may be created or extended by sales 
representatives or written sales materials. The advice and strategies contained herein may not be suitable 
for your situation. You should consult with a professional where appropriate. Neither the publisher nor 
author shall be liable for any loss of profit or any other commercial damages, including but not limited 
to special, incidental, consequential, or other damages. 
For general information on our other products and services please contact our Customer Care 
Department within the United States at (800) 762-2974, outside the United States at (317) 572-3993 or 
fax (317) 572-4002. 
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print, 
however, may not be available in electronic formats. For more information about Wiley products, visit 
our web site at www.wiley.com. 
Library of Congress Cataloging-in-Publication Data: 
Rausand, Marvin. 
Reliability of safety-critical systems : theory and application / Marvin Rausand. 
pages cm 
Includes bibliographical references and index. 
ISBN 978-1-118-11272-4 (cloth) — ISBN 978-1-118-77635-3 — ISBN 978-1-118-55340-4 
(ePDF) — ISBN 978-1-118-55338-1 (ePub) — ISBN 978-1-118-55337-4 (eMOBI) 1. Reliability 
(Engineering) I. Title. 
TA169.R375 2013 
620'.00452dc23 
2013034448 
Printed in the United States of America. 
10 9 8 7 6 5 4 3 2 1 

To Hella, Guro, Idunn, 
and Emil 


CONTENTS 
Preface 
xii 
Acknowledgments 
xvii 
1 
Introduction 
1 
1.1 
Introduction 
1 
1.2 
Objectives and Scope 
10 
1.3 
Functional Safety Standards 
13 
1.4 
The Main Elements of a SIS 
17 
1.5 
A Brief History 
21 
1.6 
Structure of the Book 
22 
1.7 
Additional Reading 
24 
2 
Concepts and Requirements 
25 
2.1 
Introduction 
25 
2.2 
System Hardware Aspects 
25 
2.3 
Safety-Instrumented Functions 
29 
vii 

VIII 
CONTENTS 
2.4 
Modes of Operation 
29 
2.5 
Safe State 
31 
2.6 
Demands and Demand Rate 
31 
2.7 
Testing of Safety-Instrumented Functions 
32 
2.8 
Safety Integrity Levels (SILs) 
33 
2.9 
Safety Life Cycle 
39 
2.10 
Reliability of Safety-Instrumented Systems 
47 
2.11 
Functional Safety Certificates 
48 
2.12 
Safety Analysis Report 
48 
2.13 
Functional Safety Assessment 
49 
2.14 
Reliability and Decision-Making 
50 
2.15 
Additional Reading 
51 
Failures and Failure Analysis 
53 
3.1 
Introduction 
53 
3.2 
Failures and Failure Modes 
53 
3.3 
Failure Causes and Mechanisms 
58 
3.4 
Failure Effects 
58 
3.5 
Failure/Fault Classification 
59 
3.6 
FMECA 
71 
3.7 
FMEDA 
75 
3.8 
Additional Reading 
75 
Testing and Maintenance 
77 
4.1 
Introduction 
77 
4.2 
Testing 
78 
4.3 
Maintenance 
87 
4.4 
Additional Reading 
89 
Reliability Quantification 
91 
5.1 
Introduction 
91 
5.2 
Reliability Block Diagrams 
92 
5.3 
Fault Tree Analysis 
105 
5.4 
The Beta-Factor Model 
119 
5.5 
Markov Approach 
120 
5.6 
Petri Net Approach 
146 
5.7 
Additional Reading 
164 
3
4
5

CONTENTS 
IX 
6 
Reliability Data Sources 
165 
6.1 
Introduction 
165 
6.2 
Types of Data 
165 
6.3 
Failure Modes 
167 
6.4 
Generic Failure Rate Sources 
167 
6.5 
Plant-Specific Reliability Data 
170 
6.6 
Data Dossier 
172 
6.7 
Additional Reading 
174 
7 
Demand Modes and Performance Measures 
175 
7.1 
Introduction 
175 
7.2 
Mode of Operation According to the IEC Standards 
175 
7.3 
Functional Categories 
177 
7.4 
Operational Strategies 
179 
7.5 
Reliability Measures 
181 
7.6 
PFDavg versus PFH 
186 
7.7 
Placement of the SIF 
187 
7.8 
Analytical Methods 
188 
7.9 
Assumptions and Input Data 
188 
7.10 
Additional Reading 
190 
8 
Average Probability of Failure on Demand 
191 
8.1 
Introduction 
191 
8.2 
Reliability Block Diagrams 
195 
8.3 
Simplified Formulas 
196 
8.4 
The IEC 61508 Formulas 
223 
8.5 
The PDS Method 
233 
8.6 
Fault Tree Approach 
241 
8.7 
Markov Approach 
248 
8.8 
Petri Net Approach 
265 
8.9 
Additional Reading 
272 
9 
Average Frequency of Dangerous Failures 
273 
9.1 
Introduction 
273 
9.2 
Frequency of Failures 
274 
9.3 
Average Frequency of Dangerous Failures (PFH) 
280 
9.4 
Simplified PFH Formulas 
285 
9.5 
The IEC 61508 Formulas 

X 
CONTENTS 
9.6 
Alternative IEC Formulas 
301 
9.7 
The PDS Method 
302 
9.8 
Fault Tree Approach 
302 
9.9 
Markov Approach 
304 
9.10 
Petri Net Approach 
307 
9.11 
PFDavgorPFH? 
308 
9.12 
Additional Reading 
308 
10 
Common-Cause Failures 
309 
10.1 
Introduction 
309 
10.2 
Causes of CCF 
312 
10.3 
Defenses Against CCF 
314 
10.4 
Explicit Versus Implicit Modeling 
315 
10.5 
The Beta-Factor Model 
317 
10.6 
The Binomial Failure Rate Model 
330 
10.7 
Multiplicity of Faults 
333 
10.8 
The Multiple Beta-Factor Model 
335 
10.9 
CCF Modeling with Petri Nets 
340 
10.10 CCFs Between Groups and Subsystems 
340 
10.11 Additional Reading 
341 
11 
Imperfect Proof-Testing 
343 
11.1 
Introduction 
343 
11.2 
Proof Test Coverage 
344 
11.3 
Splitting the Failure Rate 
345 
11.4 
Adding a Constant PFDavg 
353 
11.5 
Nonconstant Failure Rates 
354 
11.6 
Markov Models 
355 
11.7 
Additional Reading 
358 
12 
Spurious Activation 
359 
12.1 
Introduction 
359 
12.2 
Main Concepts 
362 
12.3 
Causes of Spurious Activation 
365 
12.4 
Reliability Data for Spurious Operations 
368 
12.5 
Quantitative Analysis 
368 
12.6 
Additional Reading 
379 

CONTENTS 
XI 
13 
Uncertainty Assessment 
381 
13.1 
Introduction 
381 
13.2 
What Is Uncertainty? 
382 
13.3 
Completeness Uncertainty 
383 
13.4 
Model Uncertainty 
386 
13.5 
Parameter Uncertainty 
387 
13.6 
Concluding Remarks 
390 
13.7 
Additional Reading 
391 
14 
Closure 
393 
14.1 
Introduction 
393 
14.2 
Which Approach Should Be Used? 
394 
14.3 
Remaining Issues 
395 
14.4 
A Final Word 
397 
Appendix A 
Elements of Probability Theory 
399 
A. 1 
Introduction 
399 
A.2 
Probability 
401 
A.3 
Discrete Distributions 
406 
A.4 
Life Distributions 
410 
A.5 
Repairable Items 
418 
Acronyms 
423 
Symbols 
427 
Bibliography 
431 
Index 
443


PREFACE 
This book provides an introduction to reliability assessment of safety-critical systems 
with a focus on safety-related systems that are based on electrical, electronic, and/or 
programmable electronic (E/E/PE) technology. Several international standards give 
requirements for the reliability, or safety integrity, of such systems. The most impor-
tant of these standards is IEC 61508: Functional safety of electrical/electronic/pro-
grammable electronic safety-related systems. This standard introduces several new 
features, the most noticeable being (i) the life cycle approach with requirements for 
each phase of the life cycle, and (ii) the classification of requirements into four dis-
tinct safety integrity levels (SILs). 
The standard has seven parts, is very comprehensive, and may be difficult to fully 
understand. As a performance-based standard, methods and formulas are just sug-
gested and explanations and justifications are lacking. An objective of this book 
is therefore to introduce, describe, and extend these methods and formulas, explain 
how they can be used, and highlight their limitations. 
IEC 61508 gives general requirements for E/E/PE safety-related systems and was 
developed as a basis for more detailed sector-specific standards. Several sector-
specific standards have been developed, such as IEC 61511 for the process industry, 
IEC 62061 for machinery systems, IEC 61513 for the nuclear power industry, and 
ISO 26262 for the automotive industry. The intention was to obtain a unifying sys-
tem of standards, but the terminology and the suggested approaches are not fully 
XIII 

XIV 
PREFACE 
consistent across the various standards. This book focuses on IEC 61508 and the 
sector-specific standard for the process industry, IEC 61511. 
The book concentrates on quantitative reliability analysis of the hardware of E/E/-
PE safety-related systems. It does not treat software issues, human and organiza-
tional aspects, or the qualitative requirements of the standards. It is therefore not at 
all a replacement for IEC 61508 and its sector-specific standards, but I hope that the 
book will be regarded as a helpful supplement to the standards to meet the quantita-
tive safety integrity requirements. 
Many of the approaches described in this book are general and can be applied 
to analyze any safety-critical system, including those that are not based on E/E/PE 
technology. I therefore hope that this book will be of interest to a wide range of 
reliability engineers, also to those working outside the scope of IEC 61508. 
SIL analysis and verification of SIL are important topics in many industries and 
many engineers struggle to understand all the requirements and how to perform the 
required calculations. IEC 61508 requires in Part 1, paragraph 6.2 that all persons 
with responsibilities related to the development and use of of E/E/PE safety-related 
systems shall have a sufficient competence (see also HSE, 2007). It is my hope that 
this book contributes to obtaining this competence and makes life easier for reliabil-
ity engineers. The book is mainly aimed at engineers who are developing E/E/PE 
safety-related systems, with main roles as system designers, system integrators, and 
functional safety assessors. The book does not explicitly treat SIL in operation, but 
may all the same be useful for end-users of safety-related systems. 
The technical report ISO/DTR 12489 is currently being developed with a scope 
similar to this book. The technical report (TR) is written as a guideline for the 
petroleum, petrochemical, and natural gas industries. Unfortunately, I did not see 
a draft of this TR early enough to let it influence much of my presentation. 
Since the book is aimed primarily at reliability engineers who carry out reliability 
assessments of practical E/E/PE safety-related systems, I have refrained from too 
much mathematical rigor and included a high number of worked examples. I have 
also refrained from theory and methods that are too difficult to understand or will 
require too much efforts to use. I realize, however, that this may be a contested 
issue and that some readers will find the book too basic and others will find it far too 
advanced. Readers who find this book too basic may consult ISO/DTR 12489 for 
a more thorough treatment of some approaches, notably the Petri net approach and 
partly also the Markov approach. 
When I started writing this book, I thought that I had adequate knowledge on re-
liability assessment of E/E/PE safety-related systems. After having read and re-read 
hundreds of research papers and reports, I realize that my knowledge is rather shal-
low, and I have again proved that "The more you understand, the more you realize 
that you do not understand." 
To fully appreciate the book, you should have a basic knowledge of probability 
theory. I have tried to reduce the use of difficult theory, but you still need to under-
stand the basic concepts of probability theory. For this purpose, I have included a 
very brief introduction to probability theory as an appendix. 

PREFACE 
XV 
Comments to the Notation. 
IEC 61508 is the main generic standard in this field, and I have carefully follow the 
terminology in this standard; with two notable exceptions: 
(a) The term E/E/PE safety-related system that is used in IEC 61508 is awkward and I 
have therefore replaced it with the corresponding term safety-instrumented system 
(SIS) in IEC 61511. This is also motivated by the presumption that many of the 
readers of this book will come from the process industry. 
(b) The term safety-related function in IEC 61508 has further been replaced with the 
term safety-instrumented function (SIF) from IEC 61511. 
General information and some useful mathematical results are presented in framed text-
boxes, similar to the one you are reading now. 
A large number of abbreviations and symbols are used in the book, and brief 
explanations are found as appendices. 
I hope that you will enjoy reading this book as well as find it useful. I also hope 
that professors will find the book suitable as a textbook for courses in functional 
safety. 
If you have questions or comments, you will find my email address on the book's 
homepage h t t p : / / w w w . n t n u . e d u / r o s s / b o o k s / s i s . On this homepage, 
you will also find slides, problems, and additional information related to the book. 
M. RAUSAND 
Trondheim, Norway 
July 1, 2013 


ACKNOWLEDGMENTS 
This book started out as a joint project with Mary Ann Lundteigen. She has a 
master's degree in cybernetics and has been working with maintenance of safety-
instrumented systems in the Norwegian offshore oil and gas industry. She returned 
to the university and took a PhD degree in reliability of safety-instrumented systems 
with me as supervisor. Thereafter, she got a postdoc position, and we started to plan 
this book. After the postdoc period, she became my colleague. Unfortunately, she 
decided to leave the university to start working in a consulting company (DNV). 
In her new position, she was no longer able to continue participating in the book 
project, and I decided to carry on alone. With her thorough knowledge and experi-
ence on safety-instrumented systems, Mary Ann has had a significant influence on 
the book, and I am very grateful for her contribution. 
My colleague, Yiliu Liu, also started out as a PhD and a postdoc in reliability of 
safety-instrumented systems, under my supervision. Yiliu has helped me to write 
the sections in this book related to Petri net analysis and also inspired me through 
numerous related discussions. Another colleague, Professor J0rn Vatn, has tested a 
draft of the book in an industry course and given many helpful comments. 
During the book project, the cooperation with PhD students Hui Jin, Inger Lise 
Johansen, and Yukun Wang has been a great inspiration. They have raised many 
challenging questions and put forward many proposals. 
xvii 

XVÜi 
ACKNOWLEDGMENTS 
Many students have written their master's theses related to reliability assessment 
of safety-instrumented systems, often in cooperation with industry. It has been in-
spiring to work with all of them. 
During the book project, I read many scientific papers and reports. I have tried to 
process, combine, and reformulate the information obtained in these sources and to 
give proper references. I hope that I have understood the messages in these sources, 
and that I have presented them in an acceptable way. 
Many of the definitions used in the book are from the International electrotech-
nical vocabulary (IEV) h t t p : / / w w w . e l e c t r o p e d i a . o r g . 
I appreciate the 
initiative of the International Electrotechnical Commission (IEC) to make this vo-
cabulary freely available. References to the vocabulary are given in the text as IEV 
191-xx-yy, where 191 refers to the chapter Dependability and quality of service, and 
xx-yy is the number of the definition. 
The author thanks the International Electrotechnical Commission (IEC) for per-
mission to reproduce information from International Standards IEC 61508-4 ed.2.0 
(2010) and IEC 62551 ed.1.0 (2012). All such extracts are copyright of IEC, Geneva, 
Switzerland. All rights reserved. Further information on the IEC is available from 
h t t p : //www. i e c . ch. IEC has no responsibility for the placement and context 
in which the extracts and contents are reproduced by the author, nor is IEC in any 
way responsible for the other content or accuracy therein. 
I thank SINTEF for the permission to reproduce Table 8.8 and Statoil for the 
permission to use the picture "Melk0ya Sn0hvit Winter morning light" by 0yvind 
Hagen on the front cover of the book. 
Last, but not least, I am grateful to the editorial and production staff at John Wiley 
& Sons for their careful, effective, and professional work. 
M.R. 

CHAPTER 1 
INTRODUCTION 
1.1 
Introduction 
The title of this book, Reliability of Safety-Critical Systems, embraces a wide range 
of issues and may be too broad to truly represent the content of the book. Our 
intuitive understanding of a safety-critical system is a system whose failure may lead 
to harm to people, economic loss, and/or environmental damage. Some failures may 
lead directly to undesired consequences, while other failures may increase the risk 
of damage. 
Whether or not a system is considered to be safety critical depends on the possible 
consequences of its failure. If the failure can result in consequences that are judged 
to be unacceptable, we say that the system is safety-critical. 
Safety-critical systems are used in many products and application areas. The 
safety-critical systems that are considered in this book are technical systems and 
may, or may not, involve human operator actions. The scope is delimited to systems 
that are designed to perform one or more safety functions. A safety function is usu-
ally implemented to protect against a specific undesired event that can cause harm. 
The system that is protected by the safety-critical system is called equipment under 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
1 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

2 
INTRODUCTION 
control (EUC). When the safety-critical system is medical equipment, the EUC may 
be a person. 
Examples of safety-critical systems that may be assessed by the models and meth-
ods described in this book include: 
- Automobiles (e.g., airbag systems, brakes, steering, electronic stability program 
(ESP) systems) 
- Process industry (e.g., emergency shutdown (ESD) systems, fire and gas sys-
tems, gas burner management systems) 
- Machinery (e.g., guard interlocking systems, emergency stop systems) 
- Railway transport (e.g., signaling systems, automatic train stop (ATS) systems) 
- Nuclear power industry (e.g., turbine control systems, fire prevention systems) 
- Medical devices (e.g., heart pacemakers, insulin pumps, electronic equipment 
used in surgery) 
fl 
EXAMPLE 1.1 Interlock 
An interlock is a device that is used to prevent a technical system (e.g., a ma-
chine) from harming people or damaging itself by stopping the system. An 
interlock can be a strictly mechanical item, such as a switch, but can also be 
rather sophisticated and based on infrared beams and photodetectors. 
Consider an industrial robot that is used to stack boxes. The robot is often 
equipped with an interlocking system comprising a fence to avoid contact be-
tween moving parts of the robot and the human operator. If the operator opens 
the door, for example, to remove a misplaced box, the power is automatically 
isolated from the robot and the robot stops. Closing the door is normally not 
enough to re-power the robot. A reset button must also be pressed, to make 
sure that the operator has left the area inside the fence (e.g., see Department of 
Labour, 1987). 
0 
Another word in the title of the book is reliability. The reliability of an item is 
defined as "the ability of the item to perform a required function, under given en-
vironmental and operational conditions and for a stated period of time" (e.g., see 
Rausand & H0yland, 2004). The reliability of an item is always related to its re-
quired functions and it may therefore be more relevant to talk about the reliability 
of a function. In this book we are especially concerned about safety functions and 
the reliability of these functions. Several quantitative reliability measures for safety 
functions are defined and used in the following chapters. 
A safety function that is performed by a safety-critical system may be categorized 
as follows: 

INTRODUCTION 
3 
Safety control function. A safety function that is a normal part of the operation of 
the EUC and/or integrated into the EUC control system (e.g., a railway signaling 
system, the braking system of an automobile). 
Safety protective function. A dedicated safety function that is separate from the EUC 
control system and is only activated when the safety function is demanded (e.g., 
the ESD system in a process plant, the airbag system in an automobile). 
Many safety-critical systems are based on electrical, electronic, or programmable 
electronic (E/E/PE) technology. The development of programmable electronics and 
computers continues at a fast pace, and the new technology gets more functions and 
becomes steadily cheaper, and finds its way into more and more advanced safety-
critical systems. 
In this book, we mainly consider safety-critical systems where E/E/PE technology 
plays an important role, often together with mechanical or other technology items. 
The important standard IEC 61508 Functional safety of electrical/electronic/pro-
grammable electronic safety-related systems designates these systems by the term 
E/E/PE safety-related systems. This term is long and difficult to pronounce, and the 
author therefore prefers to use the term safety-instrumented system (SIS), which is 
the corresponding term used in the process industry. 
The IEC 61508 standard is introduced briefly in Section 1.3.1 and further dis-
cussed in Chapter 2. A notable feature of IEC 61508 is that it is risk-based, which 
means that reliability requirements for the E/E/PE safety-related systems (i.e., SISs) 
must be allocated based on the results from a risk analysis. We therefore start with a 
brief introduction to risk and risk analysis. 
1.1.1 
Risk and Risk Analysis 
The term risk is complex and has been given a wide range of definitions (e.g., see 
Rausand, 2011). In this book, we define risk as the combined answer to the following 
three questions: 
1. What can go wrong? 
2. How probable is it? 
3. What are the consequences? 
To answer the first question, we have to identify the possible undesired events1. 
Most undesired events are related to energy of some sort and occur when this en-
ergy is released. Examples of undesired events in the process industry are gas leaks, 
runaway reactions, fires, explosions, falling objects, and so on. To answer the sec-
ond question, we often need to study the causes of each undesired event and use 
'An undesired event is called a hazardous event in Rausand (2011), which is a more suitable term. The 
term undesired event is used in this book because the term hazardous event is used with a slightly different 
meaning (see Chapter 2) 

4 
INTRODUCTION 
Figure 1.1 
Bow-tie diagram. 
experience data and expert judgment to estimate the probability or frequency of the 
undesired event. Most EUCs are protected by one or more safety barriers that are 
installed to remove or mitigate the consequences of the undesired events. The an-
swers to questions two and three therefore depend on how well the safety barriers 
are functioning. 
The process of answering the three questions is called a risk analysis and is some-
times illustrated by a bow-tie diagram, as shown in Figure 1.1, where the safety bar-
riers are illustrated as gray rectangles. A thorough introduction to risk analysis is 
provided in Rausand (2011). 
1.1.2 
Safety Barriers 
Safety barrier is common term in most risk analyses and is partly overlapping with 
our definition of a safety-critical system. A safety barrier system may be a technical 
system or some dedicated human and organizational effort. Safety barrier is therefore 
not the same concept as safety-critical system. An emergency procedure may, for 
example, be a safety barrier but is not a safety-critical system. The concept of safety 
barrier is denned and further discussed by Sklet (2006). 
β 
EXAMPLE 1.2 Safety barriers in a process plant 
A process plant usually has a range of safety barriers. Among these are: 
- Fire and gas detection systems 
- Fire extinguishing systems 
- Emergency shutdown systems 
- Pressure relief systems 
- Fire and explosion walls 
- Fire and evacuation training 
- Passive fire protection 
Φ 
fl 
EXAMPLE 1.3 Safety barriers related to fires in buildings 
Several safety barriers may be used to reduce the risk related to fires in buildings. 
Among these are: 

INTRODUCTION 
5 
- Smoke detection and alert system 
- Thermal detection and alert system 
- Evacuation plans and emergency 
procedures 
- Exits, exit signs, and emergency 
lighting 
See Robinson & Anderson (2003) for a detailed discussion. 
Φ 
Safety barrier systems are also called defenses, safeguards, countermeasures, or 
protection layers. A safety barrier system may perform one or more safety barrier 
functions and may usually be split into several safety barrier subsystems and ele-
ments. 
Classification of Safety Barriers. Safety barriers may be classified according to 
whether they are active or passive, technical or human/organizational, how often they 
are demanded, and so on. We introduce briefly some of these classifications. 
Proactive Versus Reactive Safety Barriers. Proactive and reactive safety barriers 
are illustrated in the bow-tie diagram in Figure 1.1. 
Proactive safety barrier. A safety barrier that is installed to prevent one or more 
undesired events in the EUC from occurring. A proactive safety barrier is also 
called a frequency-reducing barrier because it should reduce the frequency of the 
undesired event(s). 
Reactive safety barrier. A safety barrier that is installed to remove or mitigate the 
consequences of one or more undesired events in the EUC (if they should happen). 
A reactive safety barrier is also called a consequence-reducing barrier. 
Passive Versus Active Safety Barriers. Safety barriers may also be categorized as 
passive or active safety barriers: 
Passive safety barrier. A barrier whose safety function is always available as an in-
herent property of the EUC or workplace. Examples of passive safety barriers are 
fire walls, means for physical separation (e.g., fences, shields), housing used to 
protect equipment from gas or water intrusion, and so on. 
Active safety barrier. The safety function of an active safety barrier is not always 
available, but will be performed in response to certain events. An ESD system in 
a process plant is an active safety barrier and is only activated when a dangerous 
situation occurs. 
Only active safety barriers are covered in this book. 
- Smoke control and air handling 
system 
- Fire doors and fire walls 
- Sprinkler systems 

6 
INTRODUCTION 
Table 1.1 
Demand modes for some selected safety barriers. 
Safety barrier 
Low-demand 
High-demand 
Emergency shutdown system (ESD) in a process plant 
x 
Fire and gas detection in a process plant 
x 
Signaling system for railway applications 
x 
Airbag system in an automobile 
x 
Antilock braking system in an automobile 
x 
Mode of Operation. Safety barriers may be categorized according to how often the 
barrier functions are demanded. We distinguish between 
Demanded mode. These safety barrier functions do not take active part in the control 
of the EUC and are only activated when a dangerous situation (i.e., a demand, 
undesired event) occurs. We often distinguish between 
- Low-demand mode. A safety barrier is said to operate in low-demand mode 
when its function is demanded no more often than once per year. The airbag 
system in an automobile is an example of a safety barrier operating in low-
demand mode. 
- High-demand mode. A safety barrier is said to operate in high-demand mode 
when it is exposed to distinct demands that occur more often than once per 
year. A presence-sensing safeguarding device for a moving robot is (usually) 
an example of a safety barrier operating in high-demand mode. 
Continuous mode. A safety barrier is said to operate in continuous mode when its 
function is always crucial. In this case, the safety barrier is integrated with the 
EUC control system, and an undesired event will occur when the safety barrier 
fails. Examples of safety barriers operating in continuous mode are (i) fly-by-wire 
systems for flight control of aircrafts and (ii) dynamic positioning systems (DPS) 
for ships and offshore platforms. 
Some examples of safety barrier systems that operate in low-demand and high-
demand mode are listed in Table 1.1. 
Technical Versus Human/Organizational Safety Barriers. Safety barriers may 
also be classified according to their nature. 
Technical safety barriers. A technical safety barrier is a safety barrier where the bar-
rier function is performed by a technical system. Technical safety barriers may 
partly be based on E/E/PE technology. 
Human and organizational safety barriers. A human barrier is a safety barrier where 
the barrier function is carried out by one or more persons, sometimes by using 
technical safety barrier elements. The term organizational safety barrier is used 

INTRODUCTION 
7 
COMMUNITY EMERGENCY RESPONSE 
PLANT EMERGENCY RESPONSE 
FIRE AND GAS SYSTEMS 
Deluge systems, fire sprinklers, 
toxic gas detection and alarm 
PHYSICAL BARRIERS 
Barricades, dikes 
MITIGATION 
Pressure relief valves 
Rupture discs 
PREVENTION 
Safety-critical process alarms 
Safety instrumented systems 
" 
CONTROL 
^ 
Basic process control system 
Process alarms, operator procedures 
PROCESS DESIGN 
Inherently safe design 
I sais 
Figure 1.2 
Protection layers for process plants (adapted from CCPS, 2007). 
to designate safety barriers in the form of laws, regulations, procedures, training, 
and so on. 
1.1.3 
Layers of Protection 
In the process industry, safety barriers are often called layers of protection or protec-
tion layers and are sometimes visualized as in Figure 1.2, where the layers are drawn 
in the sequence they are activated. Following this sequence, it is distinguished be-
tween: 
(a) Process design (by using inherently safe design principles). 
(b) Control, using basic control functions, alarms, and operator responses to keep 
the system in normal (steady) state. 

8 
INTRODUCTION 
(c) Prevention, using safety-instrumented systems (SISs) and safety critical alarms 
to act upon deviations from normal state and thereby prevent an undesired event 
from occurring. 
(d) Mitigation, using SISs or functions implemented by other technologies, to mit-
igate the consequences of the undesired event. Examples include the protection 
that is provided by pressure relief valves. 
(e) Physical protection, using permanent (and more robust) safety barriers to en-
hance the mitigation. Examples include the protection that is achieved by hav-
ing dikes and barricades in place. 
(f) Fire and gas detection and distinguishing, as a third strategy to mitigate the con-
sequences by avoiding ignition, and thereby an accident, in relation to explosive 
gases and mixtures. 
(g) Emergency response, using various means to limit the severity of the accident, 
locally as well as in the community. Examples include rescue procedures, mo-
bilization of rescue teams, and use of emergency exits. 
1.1.4 
Safety Performance Criteria 
A simplified demanded SIS or technical safety barrier is illustrated in Figure 1.3. The 
safety barrier is installed in an EUC to reduce the risk related to a specific type of 
demands that occurs with frequency λ^β (see Chapter 5). The objective of the safety 
barrier is to stop the demands or to reduce the frequency or consequences of the 
demands. In most cases, the safety barrier is not 100% effective and some demands 
may pass the safety barrier and have negative effects on the EUC. The frequency of 
these negative effects is denoted Aeffect· If the safety barrier were not installed, all the 
demands would have negative effects. We may therefore use the relative reduction of 
the demand frequency as a measure of the risk-reduction performance of the safety 
barrier as 
„ . , 
, 
. 
Ade — ^effect 
, 
^eff-ect 
Risk reductioni = 
= 1 
-Me 
*de 
Reactive safety barriers are installed in the EUC to remove or reduce the conse-
quences of demands. The risk-reduction performance of these safety barriers can be 
assessed based on the relative reduction of the consequences obtained. Let Cwjthout 
and Cwith be the assessed consequences of demands without and with the safety 
barrier, respectively. The risk-reduction obtained is then 
τ,. , 
, 
. 
^without 
^ w i t h 
, 
^ w i t h 
Risk reduction = 
— 
= 1 — ^without 
Categories of Safety Performance Criteria. The main performance criteria for an 
active safety barrier are related to: 

INTRODUCTION 
9 
Figure 1.3 
The risk-reduction of a safety barrier. 
Functionality/effectiveness. This criterion concerns how effectively the safety bar-
rier can reduce the risk related to a specific demand, and also the safety barrier's 
ability to handle different situations and variants of the demand. 
Reliability/availability. An active safety barrier can never be completely reliable and 
available. The reliability and availability (see Chapter 5) are therefore important 
performance measures. 
Response time. To reduce the risk, the safety barrier must often be activated quickly. 
Sometimes, a maximal response time is specified as part of the functional require-
ments. 
Robustness. The safety barrier must sometimes function in hazardous situations where 
it is exposed to external stresses. It is therefore important that the safety barrier 
is robust and not vulnerable to these stresses. This criterion is sometimes referred 
to as survivability (e.g., see NORSOKS-001, 2008). 
1.1.5 
Safety-Instrumented Systems 
A SIS consists of at least three subsystems: 
1. Sensor subsystem - detects a potential danger and produces an appropriate elec-
trical signal that is sent to the logic solver. Examples of sensors are pressure 
transmitters, level transmitters, temperature gauges, and so on. 
2. Logic solver subsystem - detects the electrical signal exceeding a given thresh-
old and sends a signal to the final elements. Logic solvers can be computers, 
programmable electronic controllers (PLCs), and relay circuits. 
3. Final element subsystem - performs the safety function. Examples of final ele-
ments are shutdown valves, circuit breakers, motors, fans, and so on. 
The three subsystems must act in concert to detect the deviation (i.e., demand) 
and bring the EUC into a safe state. In brief, a SIS shall detect, react, and avert. 
A sketch of a simple SIS that is used for pressure protection of a pipeline is shown 
in Figure 1.4. Three pressure transmitters monitor the pressure in the pipeline and 

10 
INTRODUCTION 
Logic solver 
subsystem 
SDV, 
SDV2 
Sensor subsystem 
Final element subsystem 
Figure 1.4 
Sketch of a simple SIS used as pressure protection system in a pipeline. 
send this information to the logic solver subsystem. The logic solver compares the 
received values with predefined set points and, when high pressure occurs, a signal 
is sent to the two shutdown valves (SDVs) to close the flow in the pipeline. 
Each subsystem can have one or more channels. The sensor subsystem in Fig-
ure 1.4 has three channels (i.e., pressure transmitters) and the final element subsys-
tem has two channels (i.e., shutdown valves). 
Functional Safety. Safety is often denned as a state where the risk has been re-
duced to, and is maintained at, a level that is as low as reasonably practicable 
(ALARP) and where the remaining risk is generally accepted. Most well-designed 
EUCs that are exposed to hazards have a control system and one or more safety bar-
riers that protect the EUC and the environment from being harmed by the hazards. 
The control and safety barrier functions are more and more often being carried out 
by E/E/PE technology, with increasingly complex software. 
The term functional safety is used in the title of the important standard IEC 61508, 
and this term is therefore used to denote the part of the overall system safety that 
depends on the correct functioning of active control and safety systems. Functional 
safety relies on active safety barriers, while passive safety barriers are not part of 
functional safety. 
IEC 61508 and associated standards are often called functional safety standards. 
1.2 Objectives and Scope 
This section outlines the objectives and the scope of the book. In addition, the author 
presents some views on the importance of the subject area. 
1.2.1 Objectives 
The main objective of this book is to provide a comprehensive introduction to relia-
bility assessment of SISs and the various parts of such systems. 
More specific objectives are: 
pressure 
transmitters © © © 
(a) To present the terminology used in reliability assessment of a SIS. 

OBJECTIVES AND SCOPE 
11 
(b) To identify and classify the possible failure modes of a SIS. 
(c) To define and discuss relevant reliability mearures for a SIS. 
(d) To present models and methods that can be used to analyze and quantify the 
reliability of a SIS and to discuss the adequacy of each method. 
(e) To discuss problematic issues, such as common-cause failures and imperfect 
proof-testing, and show how these issues can be incorporated into the reliability 
analysis. 
(f) To discuss negative side effects of a SIS in the form of spurious trips. 
(g) To discuss the uncertainty of the reliability measures that are produced by the 
various analyses. 
1.2.2 
Scope 
The book is directed towards suppliers, system integrators, and users of SISs, along 
with reliability analysts who carry out the required analyses in the design and devel-
opment stages of the systems. The terminology and the presentation in the book are 
adapted to IEC 61508. 
Although the focus is on the reliability of a SIS, most of the methods presented 
in the book are also relevant for safety systems based on other technologies, such as 
mechanical, hydraulic, and/or pneumatic devices. 
1.2.3 
Delimitation 
The book is limited to reliability analysis of the hardware of SISs. Software, human, 
and organizational issues are not treated in the book. 
The operational phase of a SIS is not addressed in any detail in this book. In this 
phase, the system operator (called the end-user in this book) has to verify that the 
required reliability of the SIS is maintained by testing, maintenance, modifications, 
and updating of reliability analyses. For this purpose, it is important that the end-user 
is familiar with the reliability analyses that have been used to prove compliance with 
IEC 61508 and the strengths and weaknesses of these analyses. The book should 
therefore also be of interest to SIS end-users. 
The reasons why software reliability is not treated in the book are twofold: (1) 
quantitative software reliability analyses are not required to claim compliance with 
IEC 61508, and (2) software reliability assessments are usually done by software 
specialists and not by traditional system reliability engineers, who are the intended 
readers of this book. 
1.2.4 
The Importance of Functional Safety 
Every day, people are injured and killed, large material and financial assets are lost, 
and the environment is polluted because of failures of safety-critical systems and 

12 
INTRODUCTION 
lack of functional safety. The accidents may range from single-person accidents 
up to disasters such as the Macondo accident in the Gulf of Mexico in 2010 and the 
Fukushima Daiichi nuclear power accident in Japan in 2011. If the safety-critical sys-
tems had functioned as intended, many of these accidents might have been avoided. 
The next section presents briefly a number of functional safety standards. The 
objective of these standards is to ensure that SISs are specified, designed, manu-
factured, installed, and operated such that they will reliably perform their intended 
safety functions. To achieve a sufficiently high reliability, a number of detailed relia-
bility analyses have to be performed, especially in the design phase. The intention of 
this book is to help reliability analysts to perform adequate reliability analyses that 
can contribute to improving the functional safety. 
System designers are trained to develop systems that are able to perform the de-
sired functions, but they often forget to consider how the systems can fail. This is the 
role of reliability engineers and reliability analysts who should be part of the design 
team. A number of analytical methods are available for identifying potential system 
failures and the causes of these failures. Some of the methods are qualitative, some 
are quantitative, and some are both qualitative and quantitative. The most impor-
tant output from the reliability analyses is the improved understanding of how the 
system may behave and how it can fail in the different operational situations. This 
knowledge can help the design team to improve the system reliability and to avoid 
failures. 
The quantitative reliability mearures that are produced from the reliability anal-
yses are important but sometimes get too much focus. The quantitative methods 
described in this book will, in many cases, give approximately the same reliability 
measures and we may therefore ask whether it is necessary to learn to use more 
than one method. The answer to this is that by using different methods, you will 
understand different aspects of the proposed design. In this book, you will, for ex-
ample, learn the two methods of fault tree analysis and Petri net analysis. By using 
fault tree analysis you will understand how combination of component failures can 
produce system failures and by Petri net analysis, you will better understand the dy-
namic features of the system. The quantitative results obtained by the two methods 
are, however, rather similar. 
Some people claim that reliability analysis is only playing with numbers and has 
no real value. The author disagrees with these statements. As reliability analysts, we 
can make a big difference; we can improve the reliability of safety-critical systems, 
avoid failures, and even prevent accidents. In many cases, we can contribute to 
saving lives - even the lives of our loved ones. 
What we, as reliability engineers or reliability analysts, have to do is to obtain 
a thorough knowledge of the tools and methods we are using and carry out our job 
with the seriousness it deserves. The author hopes that this book can help you to do 
an even better job. 

FUNCTIONAL SAFETY STANDARDS 
13 
1.3 
Functional Safety Standards 
This section gives a brief survey of some of the most important functional safety 
standards. 
1.3.1 
The Generic lEC 61508 Standard 
The international standard Functional safety of electrical/electronic/programmable 
electronic safety-related systems (IEC 61508, 2010) is a generic, performance-based 
standard for safety-related systems that involve E/E/PE technology. IEC 61508 pro-
vides a basis for specification, design, and operation of all types of SISs. The ob-
jective of the standard is to give overall requirements and to serve as a basis for 
development of sector-specific standards. 
IEC 61508 has several main characteristics. First is the life cycle approach that 
defines the necessary requirements for a SIS "from cradle to grave." Another main 
characteristic is that it is risk-based, such that requirements for the SIS have to be 
based on a risk assessment. 
The standard has seven parts (see box) and introduces 16 life cycle phases, which 
may be split into five main stages. 
1. Risk assessment (covering phases 1-5), the outcome of which is the formulation 
of the required safety functions and the associated reliability targets. 
2. Design and construction (covering phases 9-11), the outcome of which is a SIS 
comprising hardware and software elements. 
3. Planning for integration, overall validation, and operation and maintenance 
(covering phases 6-8). 
4. Operation and maintenance, including management of change (covering phases 
14-15). Any change to the SIS should initiate a return to the most appropriate 
life cycle phase when a modification has been requested. 
5. Disposal, which ends the life of the SIS. 
The life cycle phases are further described in Chapter 2. 
1.3.2 
Sector-Specific Standards 
Sector-specific standards related to IEC 61508 have been developed for several sec-
tors, such as process industry, machinery systems, nuclear power plants, railway ap-
plications, and automotive industry. This section gives a brief introduction to some 
of these standards. 
Process Industry. The standard Functional safety - Safety instrumented systems 
for the process industry sector (IEC 61511, 2003) is based on IEC 61508 and is the 

14 
INTRODUCTION 
IEC 61508: 
Functional safety of electrical/electronic/programmable electronic safety-related sys-
tems 
Normative parts: 
PART 1 : General requirements 
Defines the overall safety life cycle model. The standard employs qualitative or quantita-
tive techniques to identify the process risk to the safety-related system. These techniques 
focus on project management, quality assurance, and configuration management. 
PART 2: 
Requirements for electrical /electronic /programmable electronic safety-
related systems 
Provides objectives for the safety development of the E/E/PES. Software is further de-
fined in part 3. However, it should be noted that part 2 maintains jurisdiction. 
PART 3: Software requirements 
Provides objectives for the safety development of the software residing in the E/E/PES. 
PART 4: Definitions and abbreviations 
Contains definitions, abbreviations, and terminology used in the safety process that must 
be adhered to in order to establish and maintain consistency. 
Informative parts: 
PART 5: Examples of methods for the determination of safety integrity levels 
Provides the formal approach for determining the safety integrity level (SIL) of the safety 
system (SIL is described in Chapter 2). 
PART 6 : Guidelines on the application of IEC 61508-2 and IEC 61508-3 
Provides specific guidelines for applying IEC 61508 parts 2 and 3. 
PART 7: Overview of techniques and measures 
Provides details of the safety techniques and measures relevant to parts 2 and 3. 
Supplement: 
PART 0 : Functional safety and IEC 61508 
This is a technical report (TR) with number IEC/TR 61508-0 and is not formally a part 
of IEC 61508. Part 0 explains and gives comments to the standard. 
In this book, reference to the parts of the standard is given as IEC 61508-1, and so on. 
main standard for the application of SISs in the process industry, including the oil 
and gas industry. 
IEC 61511 is mainly concerned with SISs operating in low-demand mode, that is, 
where demands for the SIF are discrete events that occur rather infrequently. The SIS 
is consequently an independent protection layer in addition to the basic process con-
trol system (BPCS). The SIS does not play any active role during normal operation 
and is only activated if a demand should occur. 
IEC 61511 applies when a SIS is based on proven technology or technology 
whose design has been verified against the requirements in IEC 61508. Development 
of new technology is beyond the scope of IEC 61511. For this reason, IEC 61511 is 

FUNCTIONAL SAFETY STANDARDS 
15 
IEC 61511: 
Functional safety. Safety instrumented systems for the process industry sector 
PART 1 : Framework, definitions, system, hardware and software requirements 
PART 2: Guidelines for the application ofIEC61511-I 
PART 3 : Guidance for the determination of the required safety integrity levels 
IEC 61511 is the sector-specific standard for the process industry, including the oil and 
gas industry. In this, SISs are assumed to operate mainly in low-demand mode. 
sometimes called the end-user's and system integrator's standard, whereas IEC 61508 
is called the manufacturer's 
standard. 
Guidelines have been published to ease the application of IEC 61508 and IEC 61511. 
Two notable guidelines are: 
- Guidelines for Safe and Reliable Instrumented Protective Systems published by 
the Center for Chemical Process Safety (CCPS, 2007). 
- Application of IEC 61508 and IEC 61511 in the Norwegian Petroleum Industry 
published by the Norwegian Oil and Gas Association (NOG-070, 2004). 
► Remark: The process industry has traditionally separated control functions and 
safety functions. 
The aim is to avoid a failure in the control system having an im-
pact on the SIS. Despite this advantage, there are now many industry applications 
where control and safety functions are integrated, sometimes motivated by cost and 
efficiency. Merging control and safety may also be due to space limitations that pose 
restrictions on the design philosophy. 
Φ 
Machinery Systems. 
Machinery safety in Europe is regulated by the EU Machin-
ery Directive (EU-2006/42/EC, 2006). The first edition of this directive was ap-
proved in 1989 and it has been amended and updated several times. The EU Ma-
chinery Directive gives the essential health and safety requirements (EHSRs) related 
to design and use of machinery and leaves the details to harmonized standards. 
It 
is not mandatory to follow the standards, but if one complies with a harmonized 
standard, the associated EHSR is fulfilled. 
SISs have a high priority in the Machinery Directive and the EHSRs related to 
such systems are listed in § 1.2. The first few lines of these requirements are: 
Control systems must be designed and constructed in such a way as to prevent haz-
ardous situations from arising. Above all, they must be designed and constructed 
in such a way that: 
- they can withstand the intended operating stresses and external influences, 
- a fault in the hardware or the software of the control system does not lead to 
hazardous situations, 

16 
INTRODUCTION 
- errors in the control system logic do not lead to hazardous situations, 
- reasonably foreseeable human error during operation does not lead to haz-
ardous situations. 
The first standard that was developed for machinery control systems was EN 954-
1 (1997). As the requirements of the EU Machinery Directive have been accepted 
and implemented in the national laws of many countries around the world, the related 
EN standards have been transferred into international standards. The EN 954-1 was 
therefore transferred, in a slightly modified form, into ISO 13849-1 (2006). The 
EN 954-1 was developed before the IEC 61508 was made and consequently does not 
fully comply with IEC 61508. The same applies for ISO 13849-1. 
Another standard, IEC 62061 "Safety of machinery - Functional safety of safety-
related electrical, electronic and programmable electronic control systems" was there-
fore developed, based on IEC 61508. Today, both ISO 13849-1 and IEC 62061 are 
accepted as harmonized standards related to E/E/PE based control systems of ma-
chinery. The relationship between ISO 13849-1 and IEC 62061 is described in the 
technical report IEC TR 62061 -1. 
When discussing control of machinery systems, we will only refer to IEC 62061 
in the rest of this book. In this standard, a SIS is called a safety-related electrical con-
trol system (SRECS). A special standard has been developed for risk assessment of 
machinery. This is ISO 12100 "Safety of machinery - General principles for design 
- Risk assessment and risk reduction." SISs in machinery systems mainly operate 
in high-demand or continuous mode and are often integrated with the machinery 
control system. 
Nuclear Power Industry. The standard IEC 61513 (2004) was developed as a sector-
specific standard for the nuclear power industry, based on IEC 61508. In IEC 61513 
a SIS is called an instrumentation and control (I&C) system and is defined as a "sys-
tem, based on electrical and/or electronic and/or programmable electronic technol-
ogy, performing I&C functions as well as service and monitoring functions related 
to the operation of the system itself." IEC 61513 is not further discussed in this book. 
Automotive Industry. ISO 26262 (2011) is the sector-specific standard for road 
vehicles under IEC 61508. It was developed for electric and/or electronic systems 
installed in series production passenger automobiles with a maximum gross vehicle 
mass up to 3 500 kilograms. The standard has nine normative parts and a guideline 
for the use of ISO 26262 as part 10. The standard is not further discussed in this 
book. 
Railway Transport. Three European norms: EN 50126, EN 50128, and EN 50129, 
have been developed with a scope similar to IEC 61508. The three EN-norms have 
later been transferred into IEC-standards. 
- IEC 62278 (EN 50126). Railway applications - The specification and demon-
stration of reliability, availability, maintainability, and safety (RAMS). 
- IEC 62279 (EN 50128). Railway applications - Communications, signaling, 
and processing systems - Software for railway control and protection systems. 

THE MAIN ELEMENTS OF A SIS 
17 
- IEC 62425 (EN 50129 ). Railway applications - Communication, signaling, 
and processing systems - Safety-related electronic systems for signaling. 
The three standards do not have the format of sector-specific standards related 
to IEC 61508, but meeting the requirements in these standards is considered to be 
sufficient to ensure that the requirements in IEC 61508 are met. 
MODSafe (Modular urban transport safety and security analysis) is an EU re-
search project under the Seventh Framework Program. MODSafe developed knowl-
edge and methods for the analysis of safety-related systems in urban guided trans-
port systems (i.e., light trains, métros, and trams). A number of MODSafe reports 
discuss issues in the interface between the approach described in the three railway 
standards and in IEC 61508. The reports are available on the project's web page 
h t t p : / /www. modsaf e . eu. Several of these reports give additional insight to 
many chapters of this book. 
1.4 The Main Elements of a SIS 
The main elements of a SIS are input elements, logic solver, and final elements. 
These elements are briefly introduced in this section. A more detailed account of 
these elements and the technologies used in a SIS is given by Goble & Cheddie 
(2005); Macdonald (2004a,b); Gruhn & Cheddie (2006); CCPS (2007). 
1.4.1 
The Fail-Safe Principle 
A SIS element can be designed according to two different principles: 
Energize-to-trip. The SIS element is de-energized during normal operation and need 
to be energized (e.g., by electricity, hydraulic pressure, pneumatic pressure) to 
perform its safety function (i.e., to trip). Loss of energy will, by this principle, 
prevent the element from performing its safety function. 
De-energize-to-trip. The SIS element is energized during normal operation and re-
moval of the energy will cause a trip action. By this principle, loss of energy will 
cause a spurious (i.e., false) activation of the safety function. 
Many SIS elements are today designed according to the de-energize-to-trip prin-
ciple. This principle is also a basis for the fail-safe principle. 
Fail-safe. A design property that causes a SIS element to go to a predetermined safe 
state in the event of a specific failure or malfunction. 
An illustration of the fail-safe principle is given in Example 1.5. 
1.4.2 
Input Elements 
Input elements are used to monitor a certain process or EUC state, for example, 
temperature, pressure, level, or flow. Input elements may be based on a wide range of 

18 
INTRODUCTION 
Figure 1.5 
Pressure transmitter in a pipeline (simplified sketch). 
principles and may be designated using terms such as switches, sensors, transmitters, 
and transducers. A pressure transmitter in a pipeline, comprising a sensing element 
and a transducer, is illustrated in Figure 1.5. 
Many input elements have additional built-in electronics and software and are 
sometimes referred to as smart sensors. A smart sensor may be able to 
- Preprocess the readings (measurements) into meaningful quantities 
- Store previous readings and compare with current readings 
- Perform self-testing related to some possible failures of the sensor (referred to 
as diagnostic testing) 
- Communicate deviations to the logic solver 
- Remember configuration settings 
fl 
EXAMPLE 1.4 Fire and gas detectors in the process industry 
A variety of fire and gas detectors are use in the process industry. Some exam-
ples are: 
- Infrared (IR) flame detectors 
- Aspiration smoke detectors 
- Camera-based flame detectors 
- Camera-based smoke detectors 
- Ultraviolet (UV) flame detectors 
- Point heat detectors 
- Combined IR/UV flame detectors 
- Linear heat detectors 
- Ionization point smoke detectors 
- Infrared absorption combustible 
- Optical point smoke detectors 
gas detectors 
- Laser point smoke detectors 
~ Catalytic Sas detectors 
- Beam smoke detectors 

THE MAIN ELEMENTS OF A SIS 
19 
Power supply 
Inputs 
1 
i 
' 
T3 
O 
£ 
a 
c 
Logic module 
CPU I 
► Communication 
Outputs 
Figure 1.6 
The main elements of a logic solver. 
1.4.3 
Logic Solver 
The logic solver determines, based on signals from the input elements, whether an 
abnormal situation has occurred and initiates the required actions. The logic solver is 
the "brain" of the SIS and may be based on electrical relays, electronic components 
(e.g., printed circuit boards), programmable logic controllers (PLC), or computers. 
Programmable logic controller. A programmable logic controller (PLC) is a digital 
computer used for automation and safety of electromechanical processes, such as 
control of machinery, shutdown system, and so on. A PLC is typically designed 
for multiple input and output arrangements and is more robust than a normal com-
puter. 
A PLC comprises input cards, one or more central processing units (CPUs), out-
put cards, and associated cables for communication. The logic is mainly imple-
mented by software. The use of software reduces the hardware costs and makes it 
easier to implement modifications, but, at the same time, it leads to more complex 
systems with the added features that come with the software. The main elements of 
a logic solver are illustrated in Figure 1.6. 
A logic solver can also be relay-based, sometimes referred to as direct-wired 
logic, because the input elements interact directly with the final elements via electri-
cal relays. Printed circuit boards are sometimes called solid state logic, and have a 
fixed (printed) arrangement of electronic components, such as resistors, capacitors, 
transistors, diodes, and so on. 
The decision taken by the logic solver on how to act on the input signals is de-
termined by how the signals are voted. If the input signals are voted Ä>out-of-n, the 
safety function is performed when fc-out-of-n input elements raise an alarm. The vot-
ing may be implemented by software, hardware, or a combination of both depending 
on the technology being used. 
A SIS may use more than one logic solver to perform the safety functions. This 
approach is sometimes used in railway signaling systems, where two logic solvers 
(i.e., 2-out-of-2) have to agree on setting a green (go) signal, while it is sufficient that 
one of the two logic solvers (i.e., l-out-of-2) sets a red (stop) signal. 

20 
INTRODUCTION 
Figure 1.7 
Fail-safe gate valve used in a subsea oil/gas production (i.e., X-mas) tree. 
1.4.4 
Final Elements 
Final elements are also called actuating devices and may be valves, relays, circuit 
breakers capable of stopping flow and isolating electrical equipment, and many more. 
To improve safety and reliability, more than one final element is sometimes used 
to perform the same function. The physical installation may sometimes determine 
how the final elements are voted. If two shutdown valves are installed in the same 
pipeline, it is, for example, sufficient that l-out-of-2 valves closes to stop the flow. 
β 
EXAMPLE 1.5 Shutdown valve in a subsea oil/gas production tree 
Figure 1.72 is a sketch of a gate valve that is used as a shutdown valve in a subsea 
oil/gas production tree. The production tree is often called a X-mas tree3 and 
the valve is therefore also called a X-mas tree valve. The valve has two main 
parts: (1) a valve housing where the closing element, called the gate, is a solid 
steel block with a hole that has the same diameter as the flowtube (normally a 
diameter of 4.5 to 7 inches), and (2) a hydraulically operated fail-safe actuator 
where the fail-safe function is accomplished by a strong steel spring. In normal 
operation, the valve is open (i.e., the hole in the gate is in line with the flowtube) 
and is kept open by applying hydraulic pressure to the actuator. The hydraulic 
pressure compresses the steel spring. The valve is therefore said to be normally 
energized. When the hydraulic pressure is bled off, the valve will close by the 
spring force and fulfills the de-energize-to-trip principle. If there is a leakage in 
the hydraulic system, the pressure will also be bled off and the valve will go to 
the safe state "closed." The valve is therefore said to have a fail-safe design. 
2Figure 1.7 is reproduced from Rausand & H0yland (2004) with permission from John Wiley & Sons. 
3Read as Christmas tree. 

A BRIEF HISTORY 
21 
Solenoid valves are normally used to control the hydraulic supply to and from 
the actuator. A solenoid valve is also called a pilot valve. 
Θ 
1.5 
A Brief History 
Many safety initiatives only gain momentum after a major accident. One such ac-
cident happened in 1976 at a chemical plant in Seveso, Italy, producing pesticides 
and herbicides. The accident was triggered by an uncontrolled overheating reaction 
whose excess pressure destroyed a plant safeguard and released a large cloud of toxic 
dioxin to the environment. The reactor in question featured no automatic cooling 
systems and there were neither warning systems nor alarm plans in the installation. 
The Seveso accident was the main background for the EU directive on major 
accident hazards of certain industrial activities, also called the Seveso Directive. 
The Seveso Directive has been amended and updated several times and the most 
recent version is the Seveso III Directive that was approved in 2012 (EU, 2012). 
After the major accidents in Flixborough, UK (1974), Seveso (1976), Bhopal, In-
dia (1984), and Piper Alpha in the North Sea (1988), initiatives were taken in many 
countries to develop regulations and standards that would minimize the risk related to 
industrial accidents for citizens. In Germany, the important standard DIN V 19250, 
Control technology; fundamental safety aspects to be considered for measurement 
and control equipment, a forerunner to IEC 61508, was issued in 1989, and the 
Seveso I directive was adopted in German law with reference to DIN V 19250. 
According to DIN V 19250, safety systems should be designed to meet certain 
safety classes, class 1 through class 8. The class should be chosen based on the 
level of risk of the process (i.e., the EUC). The standard therefore forces the users 
to determine the hazards involved in their processes and the integrity of the safety-
related systems that would be required. The harmonized standard EN 954-1 to the 
EU Machinery Directive on control system of machinery was developed in parallel 
with DIN V 19250. After being adopted in IEC 61508, DIN V 19250 was withdrawn 
in 2004. 
As programmable electronic systems (PESs) made their entrance into safety sys-
tems, there was increased concern about how to determine whether the design of the 
PES was sufficiently reliable for the application and for the DIN V 19250 class. The 
standard DIN V VDE 0801 Principles for computers in safety-related systems was 
developed in 1990 to address these concerns. The two German standards provided a 
mechanism for relating risk to PES integrity, but it was always understood that risk 
reduction had to include the evaluation of the complete SIS. 
PESs have been in use for several decades but formal requirements for the relia-
bility of such systems are more recent. One of the first guidelines was the Guidelines 
for use ofPESfor Safety Related Applications in two parts that was published by the 
UK HSE in 1987. These guidelines highlighted the need for high levels of safety at 
every stage of the design, installation, and operation of PES. The safety life cycle 
concept was formalized whereby safety was not restricted to any particular phase of 

22 
INTRODUCTION 
a system's design process but spanned the entire design process and continued into 
commissioning, operation, and maintenance. 
In 1996, the US-based International Society of Automation published the standard 
ISA S84.01 Application of Safety Instrumented System for the Process Industries, 
and the standard also became an American National Standards Institute (ANSI) stan-
dard the same year. The term SIS was first introduced in the United States through 
this process industry standard. A second edition, ANSI/ISA 84.00.01 (IEC 61511 
Mod), was approved in 2004 and is similar to IEC 61511. 
The globalization of the process industry during the 1980s and 1990s resulted in 
a demand for international practices rather than national practices and guidelines for 
safety systems (CCPS, 2007). The main answer to this demand was IEC 61508. The 
first edition of the standard was issued in 1997, and the second edition came in 2010, 
following a thorough review and commenting process by national and international 
committees. 
IEC 61508 and the related sector-specific standards are not harmonized standards 
according to EU law and are not automatically mandatory unless they are referenced 
by national authority regulations. Such a reference is, for example, given in the 
Norwegian Petroleum Safety Authority (PSA) regulations. 
1.6 
Structure of the Book 
This book has 14 chapters, an appendix, and lists of acronyms and symbols used. 
Chapter 1 gives a brief introduction to the issues that are treated in the book, and 
presents the objectives and the scope of the book. The book is closely related to a 
number of international standards and a brief survey of these standards are therefore 
given with focus on the main "mother" standard, IEC 61508. The main subject of 
the analyses, a SIS, is briefly described together with its building blocks. A brief 
historical account of the development of the subject area is also given, mainly as 
seen from Europe and the United States. 
Chapter 2, Concepts and Requirements, presents the main concepts that are re-
lated to reliability assessment of a SIS. The quantitative requirements in IEC 61508 
and its sector-specific standards are described and put into a safety life cycle frame-
work. The concepts of safety integrity and safety integrity level (SIL) are presented 
and the reliability measures that are used to document compliance with a specific 
SIL are introduced. 
Chapter 3, Failures and Failure Analysis, defines the main concepts related to 
failures of a SIS. Among these are random hardware failures versus systematic fail-
ures, detected versus undetected failures, and so on. It is very important for the rest 
of the book to understand these concepts, so spending some time on this chapter is 
worthwhile. The failure analysis methods FMECA and FMEDA are also introduced 
briefly in this chapter. 
Chapter 4, Testing and Maintenance, is a brief chapter that introduces the ways a 
SIS is tested and maintained. It is important here to distinguish between proof testing 
and diagnostic testing. 

STRUCTURE OF THE BOOK 
23 
Chapter 5, Reliability Quantification, is a long chapter, which introduces models 
and methods that can be used to determine the reliability of a system. The presen-
tation is general, but most of the examples are related to SISs. Readers who are 
familiar with reliability analysis do not need to read the whole chapter, but should 
check that they are familiar with the terminology used. The following chapters are 
based on Chapter 5 and give many references to this chapter. To understand the next 
chapters, it is therefore important to understand the material in Chapter 5. 
Chapter 6, Reliability Data Sources, is a short chapter that introduces the data 
required for a reliability assessment of a SIS. Several generic data sources are de-
scribed and the quality of the available data is briefly discussed. The chapter ends 
with a discussion of how to obtain parameter estimates for a special application, 
so-called plant-specific reliability data. 
Chapter 7, Demand Modes and Performance Measures, is a short chapter that 
discusses the different demand modes and operational strategies or a SIS, and the 
implication for choice of reliability measure. A data set is supplied as a basis for the 
examples that are given in the following chapters. 
Chapter 8, Average Probability of Failure on Demand, shows how the models 
and methods introduced in Chapter 5 can be used to determine the PFDavg of a SIS. 
All the methods suggested in IEC 61508-6, are discussed and formulas are explained 
and extended to general systems. A number of worked examples are presented and 
discussed. This chapter is the most important chapter of the book and is long. 
Chapter 9, Frequency of Dangerous Failures per Hour, is the parallel to Chap-
ter 8, but applied to high-demand and continuous demanded systems. New approxi-
mation formulas, extending the formulas in IEC 61508-6, are presented and applied 
in worked examples. The chapter is shorter than Chapter 8 because some of the ap-
proaches that are used to determine the PFDavg can also be used to determine the 
PFH. 
Chapter 10, Common-Cause Failures, deals with common-cause failures (CCFs), 
how they influence the reliability of a SIS, and presents and discusses several ap-
proaches to include the effect of CCFs in the reliability assessments. The main focus 
is on the beta-factor model and the multiple beta-factor model that is an integral part 
of the PDS method (SINTEF, 2013b). Approaches to determine the beta-factor are 
outlined. 
Chapter 11, Imperfect Proof Testing, discusses possible causes of imperfect proof 
testing and how imperfect proof testing can be incorporated into reliability models. 
Chapter 12, Spurious Activation, discusses the undesired side effects a SIS has 
to give spurious activation or spurious trips. New formulas are developed for the 
spurious trip rate (STR). The formulas are illustrated in several worked examples. 
Chapter 13, Uncertainty Assessment, discusses the uncertainty of the PFDavg and 
PFH estimates that are determined by the methods described in this book. The chap-
ter introduces the various categories of uncertainty, such as aleatory and epistemic 
uncertainty, and classifies uncertainty as completeness, model, and parameter uncer-
tainty. 

2 4 
INTRODUCTION 
Chapter 14, Closure, gives some concluding remarks and presents the author's 
views of the application of the material presented in the book. Several topics that 
need further research are listed. 
The appendix section gives a brief introduction to probability theory and provides 
lists of acronyms and symbols used in the book. 
1.7 Additional Reading 
The following titles are recommended for further study related to Chapter 1 : 
- Safety Critical Systems Handbook (Smith & Simpson, 2011) gives a practical 
introduction to the IEC-standards and is a good supplement to the current book. 
- Safety Instrumented Systems: Design, Analysis, and Justification (Gruhn & 
Cheddie, 2006) gives more qualitative descriptions than this book. 
- Safety Instrumented Systems Verification: Practical Probabilistic Calculation 
(Goble & Cheddie, 2005) has more focus on reliability quantification than the 
two first books, but can still be considered a supplement to the current book. 

CHAPTER 2 
CONCEPTS AND REQUIREMENTS 
2.1 
Introduction 
This chapter introduces the main concepts and requirements related to safety-instru-
mented systems (SISs). Because the focus of the book is on quantitative reliability 
analysis of a SIS, we do not go deeply into qualitative requirements. Activities re-
quired in the various phases of the life cycle of the SIS are presented briefly as a 
background for understanding the required reliability analyses. 
2.2 System Hardware Aspects 
The terms used to describe the hardware elements of a SIS are not fully consistent in 
the standards of relevance for these systems. This book mainly uses the terminology 
in IEC 61508, but terms from IEC 61511 are also used when discussing SISs in the 
process industry. 
A SIS is installed to protect an equipment under control (EUC). Depending on 
the application, the EUC is also called process, machinery, and several other names. 
IEC 61508 does not give particular requirements for how to define an EUC, and 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
25 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

26 
CONCEPTS AND REQUIREMENTS 
Pressure 
transmitter 
-Channel 
Pressure 
transmitter 
Pressure 
transmitter 
Temperature 
switch 
Temperature 
switch 
Logic 
Solver 
-»· 
-►■ 
Shutdown 
valve 
Shutdown 
valve 
Circuit 
breaker 
Sensor subsystem 
Logic solver subsystem Final element subsystem 
Figure 2.1 
Subsystems, groups, and channels of a SIS. 
the user has therefore considerable freedom in defining the scope and boundaries of 
the EUC. Guidance on defining EUCs in the process industry is given in NOG-070 
(2004). 
2.2.1 
Subsystems, Groups, Channels, and Elements 
A SIS has, as mentioned in Section 1.1, at least three subsystems: 
1. A sensor subsystem with one or more sensors that are installed to detect a possi-
ble undesired event in the EUC and send signals to the logic solver subsystem. 
2. A logic solver subsystem with one or more logic solvers that receive the signals 
from the sensor subsystem, interpret these signals, and decide which actions 
should be taken. 
3. h final element subsystem with one or more actuating elements (e.g., valves, 
circuit breakers, motors) that take a prescribed action to prevent harm. 
The subsystems of a SIS are illustrated in Figure 2.1. Each subsystem may have 
one or more voted groups of channels. A channel is a structure of one or more el-
ements and can independently perform a channel safety function. In Figure 2.1, the 
block denoted "pressure transmitter" is a channel that should (i) detect when the 
pressure goes beyond acceptable limits and (ii) send a signal to the logic solver sub-
system. The block called "shutdown valve" in Figure 2.1 is also a channel, that upon 
signal from the logic solver subsystem shall shut and stop the flow in, for example, a 
pipeline. The channel "shutdown valve" has typically at least two elements: a pilot 
valve and the shutdown valve. 
The main elements of a channel of a logic solver subsystem are illustrated in Fig-
ure 2.2. A logic solver subsystem is usually a complex system with several channels, 

SYSTEM HARDWARE ASPECTS 
27 
Logic solver channel 
Input 
module 
— » 
Main 
processor — » 
Output 
module 
1 
1 
1 
1 
1 
1 
1 
Figure 2.2 
Elements of a channel. 
a range of elements, and a lot of software. The term element is the lowest level of 
indenture that is considered in this book. 
A subsystem may have several types of channels, with different functions, as 
indicated in Figure 2.1. Similar channels with the same function are often called a 
voted group. In Figure 2.1, the sensor subsystem has two voted groups: one voted 
group with three pressure sensor channels and one voted group with two temperature 
switch channels. When using the term voted group, it is, in most cases, presupposed 
that the channels of the voted group are identical. 
When discussing reliability concepts, the term item is sometimes used to denote 
any of these entities—ranging from a complete SIS, down to an element. 
2.2.2 
Redundancy 
Redundancy means having two or more items, such that if one item fails, the system 
can continue to function by using the other item(s). This design principle is also 
referred to as fault tolerance. Redundancy can be implemented in many different 
ways. Two main categories are: 
1. Active redundancy. All the redundant items are actively performing the duties. 
If the items carry a load, they share the load (e.g., pumps that should supply a 
given volume of a fluid). 
2. Standby redundancy. One or more items perform the duties, while the rest of 
the items are in standby, waiting to be put into operation if one of the active 
items fails. While in standby, the items can be in cold standby or partly loaded 
standby. Items in cold standby are usually considered to be as-good-as-new 
when activated. The items may sometimes be activated and de-activated on a 
scheduled basis. Standby redundancy is also called dynamic redundancy. 
Redundancy can further be categorized as: 
Hardware redundancy can be implemented by installing two or more items that can 
perform the same, or a similar, safety function. The redundancy can be imple-
mented on element level, channel level, voted group level, subsystem level, and 
even on SIS level. 
Software redundancy is sometimes implemented by having two or more software 
routines, each written by independent coding teams and developed to give the 
same output for the same input. If there is no fault, the modules produce identical 

2 8 
CONCEPTS AND REQUIREMENTS 
Pressure 
transmitter 
Pressure 
transmitter 
Pressure 
transmitter 
Figure 2.3 
Voting of three channels (pressure transmitters). 
Shutdown 
Shutdown Shutdown 
valve 1 
valve 2 
valve 3 
——D><]—ϊΆ—\>< 
Figure 2.4 
Three redundant shutdown valves. 
outputs at all times, but if one of the outputs differs, the software either has an 
undetected bug or the hardware it is running on has failed. In this case, the routine 
producing the deviating output is ignored and an error signal is sent. 
Several other types of redundancy are described and discussed by Boulanger (2010). 
2.2.3 Voting 
A group of n identical channels can be configured in several ways. One extreme 
is when the group is functioning only when all the n channels are functioning; the 
other extreme is when the group is functioning as soon as at least one channel is 
functioning. The first extreme case is called an n-out-of-n voted structure and the 
second a 1-out-of-n voted structure. In a general case, the group may be configured 
such that it is functioning when at least k of its n channels are functioning, that 
is a k-o\A-oî-n voted structure. Such a structure is often written koon and is said 
to be a koon voting. A 2oo3 voting of a group of three pressure transmitters is 
illustrated in Figure 2.3. In this figure, the voting is drawn as a circle with a V (for 
voting), but in most cases, the voting is done by the logic solver subsystem. The 
voted group of the three pressure transmitters is functioning when at least two of the 
transmitters are able to detect and transmit signal when the pressure goes beyond the 
acceptable limits. When the logic solver subsystem receives signals from at least two 
transmitters, the signals are treated and a decision about action is made. 
The channels may also be implemented to have a specific voting without any 
treatment by the logic solver subsystem. Such a system is illustrated in Figure 2.4 
where three shutdown valves are installed in physical series in a pipeline. Each of 
the valves can stop the flow in the pipeline and we therefore have a loo3 voting with 
respect to the safety function "stop flow.". 
2oo3 

SAFETY-INSTRUMENTED FUNCTIONS 
29 
2.2.4 
Hardware Fault Tolerance 
The concept hardware fault tolerance (HFT) is used in IEC61508 to denote the 
ability of a hardware subsystem to continue to perform a required function in the 
presence of faults or errors. The HFT is given as a digit, where HFT = 0 means that 
if there is one fault, the function (e.g., to measure pressure) is lost. HFT = 1 means 
that if a channel fails, there is one other channel that is able to perform the same 
function, or that the subsystem can tolerate one failure and still be able to function. 
A subsystem of three channels that are voted 2oo3 is functioning as long as two of 
its three channels are functioning. This means that the subsystem can tolerate that 
one channel fails and still function as normal. The hardware fault tolerance of the 
2oo3 voted group is, therefore, HFT = 1. 
A koon voted group is functioning if at least k of its n channels are functioning 
and can tolerate up to n — k channel failures without losing its ability to function. 
The HFT of a koon voted group is therefore n — k. Note that hardware in this 
context covers the entire channel, included embedded software (e.g., for a "smart" 
transmitter). 
2.3 Safety-Instrumented Functions 
A safety-instrumented function (SIF) is a function that has been intentionally de-
signed to protect the EUC against a specific demand (see Section 2.6). The SIF 
is implemented by a SIS and given a specific safety integrity level (SIL) [see Sec-
tion 2.8]. A SIS may perform one or more SIFs. The parts of a SIS that performs a 
particular SIF are called a safety loop. Observe that 
- All safety functions related to an EUC are not necessarily SIFs 
- There is no one-to-one relationship between a SIS and a SIF 
2.4 
Modes of Operation 
Modes of operation were introduced in Chapter 1. IEC 61508 defines three modes 
of operation: low-demand mode, high-demand mode, and continuous mode but com-
bines the two last modes and refers to these as the high-demand/continuous mode. 
IEC 61511, on the other hand, distinguishes between two modes of operation: de-
manded mode and continuous mode. 
There is a significant difference between a SIF that operates in demanded mode 
and one that operates in continuous mode. A SIF in demanded mode is passive in 
the sense that it does not perform any active function during normal operation but is 
an add-on to the EUC and is only called upon when something goes wrong, or starts 
to go wrong. 
A SIF that operates in continuous mode, on the other hand, plays an active role in 
the control of the EUC and a hazardous event will occur almost immediately when a 
dangerous failure of the SIF occurs. 

30 
CONCEPTS AND REQUIREMENTS 
Control system 
Safety-related 
fails? 
function fails? 
Yes 
Hazardous 
Yes 
Operational 
situation 
(Demand) 
No 
Figure 2.5 
Event tree for a SIS in low-demand mode. 
fl 
EXAMPLE 2.1 Dynamic positioning system 
A dynamic positioning system (DPS) is a computer-controlled system on a ship 
or a semi-submersible oil and gas platform that is installed to maintain the ves-
sel's position and heading by using propellers and thrusters. Position sensors, 
wind sensors, motion sensors, and gyro compasses continuously provide infor-
mation to the computer pertaining to the vessel's position and the magnitude and 
direction of environmental forces affecting its position. Many SIFs of the DPS 
system therefore operate in continuous mode. 
φ 
IEC 61508 splits the demanded mode into two submodes: 
Low-demand mode. For this mode, the SIF is only performed when a demand oc-
curs, in order to bring the EUC into a specified safe state, and where the demand 
rate (i.e., frequency of demands) is no more than once per year.1 When the system 
is in continuous operation, this means that the demand rate is Ade < 1.15 · 10~4 
per hour. In low-demand mode, the EUC is usually kept in a safe state by an EUC 
control system and the SIF is called upon only when the EUC control fails, as 
illustrated by a simple event tree in Figure 2.5. 
High-demand mode. For this mode, the SIF is only performed on demand, in order 
to transfer the EUC to a specified safe state or to keep the EUC in a safe state, 
and where the demand rate is greater than once per year. When the EUC is in 
continuous operation, this means that the demand rate is Ade > 1.15 · 10-4 per 
hour. The mean number of demands per year will be one or more. 
Some systems do not have control systems related to all hazards and must there-
fore rely on controls based on work procedures. This may, for example, be the case 
for machinery with moving parts, such as robots. Work procedures prevent the oper-
ators from coming near the moving parts. If the procedures are not followed and an 
operator comes too close, there may be sensors that detect the position and initiate a 
A year is equal to 8 760 hours in this book. 

SAFE STATE 
31 
SIF such that the moving parts are stopped. The demand rates for such systems often 
comply with high-demand mode. 
Modes of Operation Are Related to Safety-Instrumented Functions. 
It is the 
SIFs that operate in the various modes. Because a SIS can perform more than one 
SIF, it can, at least in principle, have one SIF that operates in low-demand mode and 
another that operates in high-demand mode. In some cases, we tacitly assume that 
the SIF is specified and say that a SIS, or a safety loop, operates in low-demand, 
high-demand, or continuous mode. 
2.5 Safe State 
An objective of a SIF is to bring the EUC into a safe state or to keep the EUC in 
a safe state when a demand occurs in order to protect people, the environment, and 
material assets. A safe state is a state of the EUC, whether the system is operating 
or shut down, such that an undesired event cannot occur. The safe state must be 
achieved in a timely manner. In the process industry, the time allowed to bring the 
process to a safe state is called the process safety time. If the total time elapsed 
exceeds the process safety time, the upset escalates to create a demand on the SIF, 
initiate a trip, or cause an accident. In the process industry, the safe state is often 
achieved by shutting down the process. 
2.6 
Demands and Demand Rate 
A demand is defined as follows: 
Demand: An event or a condition that requires a SIF to be activated (i) to prevent 
an undesired event from occurring or (ii) to mitigate the consequences of an un-
desired event. In the process industry, a demand is also called a process upset or 
a process deviation. 
How often a SIF is demanded varies from system to system. A safeguard of a 
machine may be activated several times a day, while an emergency shutdown (ESD) 
system in a process plant may be activated much less than once per year. 
The occurrence of demands is often modeled as a homogeneous Poisson process 
with demand rate X^e (see Appendix A, Section A.4). This implies that the demands 
are independent events and that the demand rate is constant, both over the year and 
over a long period of time. The demand rate can then be estimated as 
Ade = * f > 
(2.1) 
where Nde(t) is the number of demands that occur during a time interval of length 
t. The interval between two consecutive demands is a random variable and will 
therefore vary. According to Appendix A, the interval has an exponential distribution 

32 
CONCEPTS AND REQUIREMENTS 
with rate Xde- When the demand rate is known, we can therefore determine the 
probability that a demand occurs within a specified time interval. The mean time 
between demands is given by MTBDe = 1/Aae and is sometimes called the mean 
demand interval. 
For an EUC that is in continuous operation, the demanded modes may be defined 
as: 
Low-demand mode: Ade < 1.15· 10~4 per hour or MTBDe > 1 year 
High-demand mode: Ade > 1.15· 10~4 per hour or MTBDe < 1 year 
2.6.1 
Demand Duration 
Demands are most often regarded as shocks that occur without any significant du-
ration, but in some cases it is also relevant to assume a certain demand duration, 
either as a specific time interval or as a random variable. An example of a SIF with a 
prolonged duration is an automatic fire extinguishing system. To perform its safety 
function, the final element subsystem consisting of fire pumps must start and survive 
as long as the fire lasts. 
How important the demand duration is for the reliability assessment of the SIS 
depends on the situation. In some cases, the SIS performs its SIF and brings the 
EUC to a safe state instantly when the demand is detected and it may not be important 
whether or not it fails later in the duration of the demand. In other cases, the SIS must 
perform its SIF as long as the demand situation is present. It is therefore important 
to consider the demand handling requirements carefully in the reliability analyses. 
2.7 Testing of Safety-Instrumented Functions 
A SIS is often a passive system that is activated only when a demand occurs. Fail-
ures may therefore occur and remain hidden until the system is demanded or tested. 
Testing is further discussed in Chapter 4. Here, we briefly mention the two main 
categories of testing. 
Proof-Testing. To verify that a SIS is able to perform its SIFs, the system is usu-
ally proof-tested at regular intervals of length τ. The time interval between two 
consecutive proof tests is often called the proof test interval. Dangerous failures 
detected by proof-testing are called dangerous undetected (DU) failures. 
Diagnostic testing. A diagnostic test is an automatic partial test that uses built-in 
self-test features to detect failures. Dangerous failures detected by a diagnostic 
test are called dangerous detected (DD) failures. The identified faults are an-
nounced as alarms, locally at the equipment and in the control room. 

SAFETY INTEGRITY LEVELS (SILS) 
33 
2.8 Safety Integrity Levels (SILs) 
IEC 61508 uses safety integrity as a performance measure for a SIF. 
Safety integrity. Probability of a SIS satisfactorily performing the specified SIFs un-
der all the stated conditions within a stated period of time [IEC 61508-4 (2010), 
def. 3.5.4].2 
IEC 61508 does not specify detailed probability values but divides the require-
ments into four safety integrity levels, SIL1, SIL2, SIL3, and SIL4, with SIL4 
being the most reliable and SIL 1 being the least reliable. 
To demonstrate that the requirements for a specific SIL are fulfilled, we must 
verify that the requirements are met for: 
- Hardware safety integrity (see below) 
- Software safety integrity (not covered in this book) 
- Systematic safety integrity (not fully covered in this book, see below) 
It is important to note that a SIL is always related to a specific SIF and not to a 
SIS, and that a safety function is not a SIF unless a SIL is allocated to the safety 
function. 
2.8.1 
Hardware Safety Integrity 
The hardware safety integrity requirements concern the hardware reliability of the 
SIS and is the main focus of this book. The hardware safety integrity is split into two 
categories of requirements: 
Quantitative Reliability Requirements. 
This part requires that the reliability of 
the SIF is analyzed and quantified. Two different reliability measures are used: 
- The average probability of (dangerous) failure on demand (PFDavg) 
- The average frequency of dangerous failures per hour (PFH) 
The reliability measures are introduced briefly in Section 2.8.3 and discussed in de-
tail in Chapters 8 and 9. 
Architectural Constraints. 
The reliability quantification is connected with uncer-
tainty of several types, such as completeness uncertainty, model uncertainty, and 
parameter uncertainty (see Chapter 13). In addition to the requirements for the quan-
tified reliability, IEC 61508 also gives requirements for the robustness of the struc-
ture of the SIS. These requirements are given as architectural constraints and set 
2IEC 61508-4 ed.2.0 "Copyright © 2010 IEC Geneva, Switzerland, www.iec.ch." 

3 4 
CONCEPTS AND REQUIREMENTS 
restrictions to the designer's freedom to choose hardware architecture on the basis of 
PFDavg and PFH calculations alone. 
The architectural constraints specify the minimum hardware fault tolerance (i.e., 
the redundancy level) to claim compliance with a given SIL. IEC 61508 suggests two 
different routes to claim compliance: 
Route 1R: Based on the hardware fault tolerance and the safe failure fraction 
concepts. 
Route 2H: Based on component reliability data from field feedback, increased 
confidence levels, and hardware fault tolerance for specified SILs. 
The architectural constraints require consideration of the following: 
1. The complexity and type of each element of the SIS. IEC 61508-2 divides ele-
ments into two types. 
Type A. An element is said to be of type A if 
(a) The failure modes of all components of the element are well defined. 
(b) The behavior of the element under fault conditions can be completely 
determined. 
(c) There is dependable failure data to show that the claimed rates of DU 
and DD failures are met. 
Type B. An element is said to be of type B if at least one of the following 
statements is true: 
(a) The failure mode of at least one component of the element is not well 
defined. 
(b) The behavior of the element under fault conditions cannot be com-
pletely determined. 
(c) There is insufficient failure data to show the claimed rates of DU and 
DD failures are met. 
2. The safe failure fraction (SFF) of each element is calculated as 
The sum of the rate of safe and DD failures of the element 
SFF = The sum of the rate of safe and dangerous failures of the element 
The SFF is a measure of the inherent tendency of an element to fail towards a 
safe state. 
IEC 61508-2 provides tables where one, based on the type and the SFF, can de-
termine the maximum allowable SIL for a given hardware fault tolerance. If, for 
example, the SFF is determined to be 70%, the elements are of type B, and the hard-
ware fault tolerance is 1, then the maximum SIL that can be claimed is SIL 2. 
The requirements related to complexity and type of channels are not part of the 
scope of this book and are therefore not discussed any further. The reader is advised 
to consult IEC 61508-2 for details. 

SAFETY INTEGRITY LEVELS (SILS) 
35 
2.8.2 Systematic Safety Integrity 
The systematic safety integrity is specified by qualitative requirements. 
Systematic failure. Failure, related in a deterministic way to a certain cause, which 
can only be eliminated by a modification of the design or of the manufactur-
ing process, operational procedures, documentation or other relevant factors (IEV 
191-4-19). Systematic failures are further discussed in Chapter 3. 
Systematic safety integrity needs an extended examination of the design, pro-
duction, and test procedures of both hardware and software. The higher the SIL 
claimed, the more detailed the examination has to be, and suppliers have to provide 
the required evidence. This is, however, outside the scope of this book. 
2.8.3 
Reliability Measures 
Probability of Failure on Demand. The probability of (dangerous) failure on de-
mand, PFD(i), is specified for a SIF and is the probability that a dangerous fault is 
present such that the SIF cannot be performed at time t. 
PFD(i) = Pr(The SIF cannot be performed at time t) 
(2.2) 
► Remark: The notion of probability of failure on demand may indicate that we 
are dealing with a conditional probability, given that a demand has occurred. This is 
not correct and PFD(i) may be expressed by (2.2) irrespective of whether a demand 
occurs or not. 
φ 
In many cases, it is not necessary to determine the PFD as a function of time 
and an average value will suffice. If the SIF is proof-tested after regular intervals of 
length τ and the system is considered to be as-good-as-new after each proof test, the 
long-term average probability of failure on demand can be expressed as 
1 Γ 
PFDavg = - / PFD(r) dt 
(2.3) 
The PFDavg is a probability and has no unit. If, for example, PFDavg = 2 · 
10~3, this means that the corresponding SIF fails, on the average, in 2 out of 1000 
demands. It can also be interpreted as the percentage of time the EUC is unprotected 
by the SIF. If the SIF should be available on a continuous basis, we have 
v-3 
8 760 hours 
PFDavR = 2 ■ 10~J 
% 17.5 hours/year 
1 year 
The PFDavg concept is further discussed in Chapters 7 and 8, where several ap-
proaches to calculate PFDavg are presented. 

36 
CONCEPTS AND REQUIREMENTS 
Frequency of Dangerous Failures per Hour. For SIFs that are operated in high-
demand or continuous mode, IEC61508 requires that the reliability is specified by 
the average frequency of dangerous failures (PFH), where the frequency is given as 
number of dangerous failures per hour. The abbreviation PFH is retained from the 
first edition of IEC 61508 where the measure was called "probability of (dangerous) 
failure per hour." 
The idea behind using the PFH as a reliability measure is that demands occur so 
often that, when a dangerous failure of the SIF occurs, it is most likely that a demand 
occurs and a hazardous event is manifested before we can bring the EUC to a safe 
state. The PFH is further discussed in Chapters 7 and 9, where several approaches to 
calculate PFH are presented. 
► Remark: For the process industry, IEC 61511 (2003) indicates in paragraph 9.2.3 
that we may choose between PFDavg and PFH for all SIFs operating in demanded 
mode. In IEC 61508, however, it is required to use PFH when the demand rate is 
higher than once per year. 
® 
2.8.4 
Hazardous Event 
A hazardous event occurs when a SIF fails when a demand for the SIF occurs. We 
distinguish between SIFs that are operated in demanded mode and in continuous 
mode. 
The interpretation of a hazardous event related to a SIF operating in demanded 
mode is illustrated in Figure 2.6 where two safety barriers that perform SIFs are in-
stalled against a certain type of demands. Barrier 1 is an intermediate safety barrier, 
meaning that if the SIF is not able to stop the demand, there is at least one additional 
barrier that may stop the demand. Barrier 2 is called an ultimate safety barrier be-
cause demands that pass this safety barrier will affect assets and lead to an accident. 
As illustrated in Figure 2.6, Barrier 1 must handle demands with demand rate Ade.i 
and will stop some of these. A demand that passes Barrier 1 creates a hazardous 
event, called "Hazardous event 1" in Figure 2.6, that has to be handled as a demand 
by Barrier 2. The rate of demands to be handled by Barrier 2 is Ade,2 < ^de,i- If 
Barrier 2 fails, "Hazardous event 2" occurs. Because there are no additional safety 
barriers that can handle this event, it will usually lead to an accident where assets are 
harmed. 
For a SIF that is operated in demanded mode, a hazardous event can occur in two 
different ways: 
1. A demand occurs while the SIF has a dangerous fault (i.e., either a DD or a DU 
fault). 
2. A dangerous failure of the SIS occurs while a demand situation is present. 
For a SIF that is operated in continuous mode, a hazardous event occurs more or 
less immediately when a dangerous failure of the SIF occurs. The consequence of 
the hazardous event depends on: 

SAFETY INTEGRITY LEVELS (SILS) 
3 7 
Figure 2.6 
The interpretation of a hazardous event for a SIF operated in demanded mode. 
1. Whether the SIF is the ultimate safety barrier before assets are harmed, or there 
are other safety barriers that may prevent or mitigate the consequences. 
2. Whether or not failures can be detected fast enough to allow the EUC to be 
brought into a safe state before assets are harmed. This ability depends on the 
speed of the physical processes that lead to harm. 
Hazardous Event Frequency (HEF). A SIF that operates in demanded mode should 
ideally stop all demands that it has been installed to stop, but because it is not 100% 
reliable, some demands will not be stopped and will create hazardous events. The 
frequency of hazardous events will hence depend on the frequency of the demands 
and the reliability of the SIF, and is given by 
HEF = PFDavg · Ade 
(2.4) 
Formula (2.4) is only correct when the demand is a shock with a negligible dura-
tion. When the demand duration is not negligible and has a mean demand duration 
(MDD), the HEF is approximately given by 
HEF % (PFDavg + X*F · MDD) Ade 
(2.5) 
where AgF is the average dangerous failure rate of the SIF when the demand is 
present. During a demand situation, the SIF may be exposed to higher stresses and 
have a higher dangerous failure rate than during normal operation. 
Some authors use the term hazardous event rate (HER) instead of hazardous event 
frequency (HEF). 
2.8.5 
Reliability Measures and SIL 
SIL for Low-Demand Mode. To fulfill the requirements for a safety integrity level, 
a SIF in low-demand mode must have a PFDavg in the corresponding interval speci-
fied in Table 2.1. 

3 8 
CONCEPTS AND REQUIREMENTS 
Table 2.1 SIL target for low-demand SISs. 
Safety integrity 
level 
SIL 4 
SIL 3 
SIL 2 
SIL1 
Average probability of failure 
on demand (PFDavg) 
1(TS to 10 - 4 
1(T4 to 1(T3 
1(T3 to 1(T2 
10~2 to ΚΠ1 
Consider a SIF that operates in low-demand mode and assume that we have deter-
mined the PFDavg to be 5.0 · 10~3. Because this value is in the interval from 10~3 to 
10~2, the system may fulfill the requirements for SIL 2 if it also fulfills the require-
ments for architectural constraints and software and systematic integrity. A SIF will 
therefore not automatically fulfill the SIL 2 requirements when the PFDavg is within 
the interval for SIL 2. 
Risk-Reduction Factor. Without any safety barriers installed, any demands would 
result in a hazardous event, such that the hazardous event frequency would be equal 
to the demand rate; HEFwithout = ^de- With a SIF with PFDavg, the hazardous 
event frequency would be reduced to HEFw;th = PFDavg-Ade. The SIF has therefore 
reduced the HEF and thereby the risk with a factor k, given by 
HEFwjth = k ■ HEFwithout 
This factor k is called the risk-reduction factor (RRF) provided by the SIF and is 
given by 
RRF = H E F w i t h o u t = — — 
(2.6) 
HEFw;th 
PFDavg 
A SIF with PFDavg = 5.0 · 10~3 therefore has a risk-reduction factor, RRF = 200, 
related to the particular demand. 
The Importance of a Safety-Instrumented Function. Consider a SIF that has been 
installed to protect people from a specific type of critical hazardous events. Assume 
that the SIF is the ultimate safety barrier. If this safety barrier fails, people are 
immediately exposed to the hazardous energy. From (2.6), we notice that: 
- A SIF with SIL 3 means that if this function fails, then the risk of fatality is 
more than 1 000 times higher than when the SIF is functioning. 
- A SIF with SIL 2 means that if this function fails, then the risk of fatality is 
more than 100 times higher than when the SIF is functioning. 
SIL for High-Demand and Continuous Mode. For high-demand and continuous 
mode the SIL requirements are related to the PFH as given in Table 2.2. The PFH 

SAFETY LIFE CYCLE 
3 9 
Table 2.2 
SIL target for high-demand and continuous mode SISs. 
Safety integrity 
level 
SIL 4 
SIL 3 
SIL 2 
SIL1 
Average frequency of danger-
ous failures per hour (PFH) 
1(T9 to 1(T8 
1(T8 to 1(T7 
10"7 to 1(T6 
10~6 to 10 - 5 
must belong to the specified interval to claim a certain SIL, but, as for low-demand 
mode, this is not sufficient. The architectural constraints and the software and sys-
tematic integrity requirements must also be fulfilled. 
Important Terminology Issues. 
The terminology related to SIL and SIF is some-
times used in a confusing way. To avoid misunderstanding, we should be aware of 
the following: 
1. A SIL is not allocated to a SIS but to a SIF that is performed by the SIS. 
2. A SIL is not allocated to a subsystem function but to the whole safety loop 
(including sensors, logic solver, and final elements) that performs the SIF. 
3. A safety function is not a SIF unless a SIL is allocated to the safety function. 
4. A SIS may perform one or more SIFs. To say that the reliability of a SIF is 
the same as the reliability of the SIS (that is performing the SIF) is therefore at 
best imprecise. It would be better to say that the reliability of a SIF is the same 
as the reliability of the safety loop that is performing the SIF, but this is still 
imprecise, since the subsystems of the safety loop may perform functions that 
are irrelevant for the SIF. 
2.9 Safety Life Cycle 
The safety life cycle is an important concept in IEC 61508 and its sector-specific 
standards, and is defined as: 
Safety life cycle. An engineering process designed to obtain a SIS with a risk-based 
level of safety in all operating phases. The life cycle is a sequence of phases 
providing a logical path from specification through design, development, com-
missioning, operation, maintenance, and finally decommissioning of the SIS, that 
is, from "cradle" to "grave." 
The safety life cycle corresponds to many traditional product life cycle models, 
but is adapted to the special considerations that are needed to identify the desired 

4 0 
CONCEPTS AND REQUIREMENTS 
performance, to estimate the predicted performance, and to make decisions regarding 
design and follow-up that eventually will give an actual performance that complies 
with the desired performance of the SIFs. The life cycle of a SIS can, in many cases, 
be more than 20 years. 
► Remark: The safety life cycle activities must be integrated into the normal product 
or system development model used by the company; otherwise, it risks being an add-
on with limited influence on the main decisions that are made related to the SIS. Ideas 
on how to integrate reliability thinking and methods into the product development 
project are given by Murthy et al. (2008) and Lundteigen et al. (2009). 
φ 
A safety life cycle model in presented is IEC 61508 and several of the sector-
specific standards present modified versions of this safety life cycle. The safety life 
cycle model in Figure 2.7 is not identical with the safety life cycle models in the 
standards, but contains all the main elements of these. The safety life cycle model in 
Figure 2.7 has six main phases. 
(a) Preparation 
(b) Analysis 
(c) Planning and development 
(d) Installation 
(e) Operation and maintenance 
(f) Decommissioning 
Here, we suffice by mentioning the two first phases. Details about these and the 
remaining phases are found in IEC 61508 and in IEC 61511. 
2.9.1 
Preparation 
The objective of the preparation phase is to become familiar with 
- The technical aspects of the EUC 
- The functions and the performance criteria of the EUC 
- The environmental and operational conditions 
- The EUC control system 
- Existing or planned safety barriers based on non-E/E/PE technology 
- The type of demands the SIF is intended to protect against 
- Preliminary testing and maintenance requirements 

SAFETY LIFE CYCLE 
Figure 2.7 
Safety life cycle. 
- The possible safe state(s) of the EUC 
Planning of the development project (incl. scope, cost, and time) is also an 
portant part of the preparation phase. 
2.9.2 
Analysis 
This phase is also called the predesign phase and consists of: 
- Identification of hazards, and risk assessment 
- Allocation of SIFs to protection layers 
41

4 2 
CONCEPTS AND REQUIREMENTS 
- Safety requirement specification for each SIF 
Hazards and Risk Assessment. 
IEC 61508 prescribes a risk-based approach for 
the allocation of requirements for a SIF. A hazard and risk assessment of the EUC 
and its associated equipment (including the EUC control system) is therefore re-
quired. For a thorough introduction to identification of hazards, risk analysis, and 
risk assessment, the reader may consult Rausand (2011). 
The objectives of this step are: 
1. To identify all relevant undesired events that may affect the EUC and the EUC 
control system in all modes of operation, including fault conditions and fore-
seeable misuse 
2. To determine the causes and event sequences that can lead to each undesired 
event 
3. To determine the consequence chains and the risk associated with each unde-
sired event 
4. To determine the requirements for risk reduction 
5. To determine the safety functions required to achieve the necessary risk reduc-
tion (where reasonably practicable, the EUC should be designed to be inherently 
safe) 
6. To determine whether any of the safety functions should be implemented as a 
SIF 
This activity should start in the concept phase of the EUC development project, 
continue into detailed engineering, and conclude with an "as built" risk analysis 
report. When major design changes occur, the report shall be updated. Updates of 
the risk analysis are usually made at certain time intervals after the installation has 
come into operation [e.g., every five years] (NOG-070, 2004). A detailed outline of 
the hazard and risk assessment is not given in this book, some of the main concepts 
and approaches, however, will be noted. A specific standard, ISO 12100 (see box), 
has been developed for risk assessment and risk reduction of machinery systems. 
When dealing with machinery systems, it is recommended to follow this standard. 
For any other systems, Rausand (2011) presents stepwise procedures that are easy to 
follow. 
Hazards Analysis. 
A hazard analysis is carried out to identify all relevant hazards 
and the associated undesired events in the EUC or the plant. These two concepts are 
important in the remaining part of the book and are defined as: 
Hazard. A hazard is a potential source of harm and is often linked to some energy, 
such as high pressure and high temperature. It is sometimes beneficial to distin-
guish between mechanical hazards, electrical hazards, kinetic hazards, radiation 

SAFETY LIFE CYCLE 
43 
ISO 12100: 
Safety of machinery - General principles for design - Risk assessment and risk reduc-
tion 
ISO 12100 is a harmonized standard under the EU Machinery Directive (EU-
2006/42/EC, 2006) and recommends a risk reduction strategy based on five steps. The 
strategy shall cover all phases of the life cycle of the machinery. 
1. Determine the limits of the machinery, including the intended use and any reason-
ably foreseeable misuse of the machinery 
2. Identify the hazards and the associated hazardous situations (detailed checklists are 
provided in Annex B of the standard) 
3. Estimate the risk related to each identified hazard and hazardous situation 
4. Evaluate the risk and take decisions about the need for risk reduction 
5. Eliminate the hazard or reduce the risk associated with the hazard by means of 
protective measures. Three categories of protective measures are indicated. 
(a) Inherent safe design measures, i.e., remove the hazard by design changes 
(b) Safeguarding and complementary protective measures, i.e., introduce safe-
guards and other safety barriers to protect people 
(c) Information for use, i.e., introduce signs and/or alarms on the machine and/or 
in the instruction handbook 
hazards, and so on. Deliberate hazards, such as arson and sabotage, are sometimes 
called threats. 
Undesired event. An undesired event is the first significant deviation that will, if not 
stopped, develop into an accident. An undesired event may occur when a hazard 
is released by a triggering event, for example, a failure of the EUC control system. 
fl 
EXAMPLE 2.2 
Hazards related to machinery 
Common mechanical hazards related to machinery comprise: 
- Entanglement in rotating parts 
- Being caught between two parts moving past one another in a shearing ac-
tion 
- Being crushed between machinery or parts as they move towards a fixed 
part of the structure 
- Being hit by material that is ejected from the machinery 
- Being cut by sharp edges 
φ 

4 4 
CONCEPTS AND REQUIREMENTS 
Several methods can be used to identify hazards and undesired events. Among 
these are 
- Hazard identification (HAZID) 
- Preliminary hazard analysis 
- Hazard and operability analysis (HAZOP) 
- Structured what-if technique (SWIFT) 
- Failure modes, effects, and criticality analysis (FMECA) 
All of these methods are explained in detail in Rausand (2011). The most im-
portant output obtained by using these methods is a listing of all possible undesired 
events. Several other names are used for these events, among which are demand, 
process demand, process upset, and process deviation. In the process industry, a 
hazard analysis is sometimes (especially in the United States) called a process haz-
ard analysis (PHA). 
Risk Analysis. 
The process of risk analysis was introduced briefly in Chapter 1. 
Several methods are available for risk analysis. Two of the commonly used methods 
aie fault tree analysis (for causal analysis) and event tree analysis (for consequence 
chain analysis). These and several other methods are discussed in detail in Rausand 
(2011). Fault tree analysis is also discussed in Chapter 5. 
The results from a risk analysis are presented by using one or more risk metrics. 
Two commonly used risk metrics are: 
Individual risk per annum (IRPA). The probability that an individual will be killed 
due to a specific hazard or by performing a certain activity during one year's 
exposure. 
Fatal accident rate (FAR). The expected number of fatalities in a defined population 
per 100 million hours of exposure. (If 1 000 persons are working 2000 hours per 
year during 50 years, they are exposed in 100 million hours). 
Risk acceptance criteria for a system or an activity may be defined by using a risk 
metric. One such criterion is FAR < 10, which means that it is not acceptable that 
more than 10 out of 1 000 people are killed if they work their whole life (i.e., 2 000 
hours per year for 50 years) in this system. See Rausand (2011) for more risk metrics 
and more details about risk acceptance criteria. 
Allocation of Safety Functions to Protection Layers The objectives of this phase 
are: 
1. To allocate safety functions to protection layers 
2. To determine required SIFs 

SAFETY LIFE CYCLE 
45 
3. To determine the required SIL for each SIF 
IEC 61508 advocates a risk-based approach to allocation of requirements for the 
various protection layers. This means that decisions should be based on a consider-
ation of risk reduction. 
EUC Risk Tolerability Criteria. This activity should start by specifying risk accep-
tance criteria for the top system (i.e., plant) level. When comparing risk with the 
benefits obtained from the plant, we often refer to a tolerable risk. This is not a "pre-
ferred" level of risk, but a level that is tolerable in light of the benefits gained from 
the plant and also based on the current values of society. A condition for accepting 
this level of risk is that the risk-reducing measures have been implemented to a level 
such that the cost of introducing further measures is "grossly disproportionate to the 
benefits gained." This approach is based on the ALARP principle. 
The risk acceptance criteria on plant level must next be broken down to system 
modules and finally to the EUC level. This is a difficult task where a range of factors 
must be taken into account, such as cost of risk reduction and also an equality prin-
ciple. It may not be acceptable to expose a specific group of people to a much higher 
risk than for other groups. 
It is the end-user's responsibility to determine the acceptable level of risk to per-
sonnel, the environment, and capital assets based on company philosophy, insurance 
requirements, budgets, and a variety of other factors. A risk level that one end-
user determines to be tolerable may be unacceptable to another end-user. When risk 
acceptance criteria have been allocated to an EUC, we may define and allocate (ap-
portion) requirements for protection layers of the EUC. 
Definition of Safety-Instrumented Functions. 
All the potential undesired events 
(demands) that can occur related to the EUC have been identified in phase 1. For 
each demand, we have to define relevant and realistic SIFs. 
Allocation of Required SIL. Several methods are available for determining the re-
quired SIL for each SIF to be implemented by a SIS. Among these methods are: 
- Risk graph (e.g., see IEC 61508, IEC 62061, and Baybutt (2007)) 
- Calibrated risk graph 
- Layer of protection analysis (LOPA). This approach is described in IEC 61508 
and IEC 61511. More thorough descriptions are given by CCPS (2001); Rau-
sand(2011). 
NOG-070 (2004) recommends a different approach that is not directly risk-based. 
See also Summers (1998). 
Risk-Reducing Measures. 
To identify the necessary risk reduction, the risk asso-
ciated with the EUC is compared with the tolerable risk. The EUC risk is the risk 
that arises from the EUC itself and its interaction with the EUC control system, and 

4 6 
CONCEPTS AND REQUIREMENTS 
without any additional safety barriers in place. The risk may be expressed in terms 
of a frequency, for example, the number of accidents per year. 
The risk reduction may be allocated or apportioned to several safety barriers, 
such that the necessary risk reduction is distributed to one or more safety barriers. 
The use of several, rather than single, safety barriers or safety-critical systems to 
prevent accidents has been a key design philosophy in many industries with major 
accident potential. In the nuclear industry, this design principle is called defense-
in-depth (NUREG/CR-6303, 1998), whereas the process industry has adopted the 
concept of layers of protection (IEC61511, 2003; CCPS, 2001). In some regulations 
and standards, the need to have at least two independent safety barriers or layers 
of protection is explicitly mentioned. ISO 10418 requires primary and secondary 
means for protection against high pressures, for example, the use of an instrumented 
system for closing inlet flow and heat to a vessel and a self-actuating pressure relief 
valve. In the railway industry, it is required to have means in addition to railway 
signaling systems to ensure safe operation of trains. Upon a failure of the signaling 
system, all trains should stop and not change their position without permission from 
a centralized railway control center. 
SIL Budget. 
When the SIL requirement for a specific SIF has been allocated, the 
SIL requirement has to be broken down to requirements for the subsystems of the 
safety loop performing the SIF. The resulting SIL budget defines the percentage of 
the requirement that can be consumed by each subsystem. A commonly accepted 
SIL budget is (Hoekstra, 2005) 
Subsystem 
Sensor subsystem 
Logic solver subsystem 
Final element subsystem 
Percentage 
35% 
15% 
50% 
fl 
EXAMPLE 2.3 SIL budget 
Consider a SIF that is required to fulfill PFDavg < 10~3. With the SIL budget 
above, the safety loop should be designed such that PFDavg of the three subsys-
tems fulfill: 
PFDs,avg 
< 3.5 · 10-4 
PFDLS>avg 
< 1.5· 10-4 
PFDFE,avg 
<5.0·1(Π4 
This is further discussed in Chapter 8. 
Θ 
Several studies indicate that the final element subsystem consumes more than 
50% of the SIL budget, in some cases up to 80%. This underlines the importance of 
the final elements in the reliability assessment of a SIF. 

RELIABILITY OF SAFETY-INSTRUMENTED SYSTEMS 
4 7 
Safety Requirement Specification for SIF. The safety requirement specification 
(SRS) is a document where all the safety requirements for a SIF are stated clearly. 
The requirements may come from standards such as IEC 61508 and IEC61511 but 
may also be company- and application-specific requirements. The term SRS is not 
a new concept in engineering design. What is special about the SRS is the level of 
detail related to when the SRS should be developed and what the SRS should include. 
The SRS must address the two main categories of requirements associated with each 
SIF: 
Safety functional requirements. This part of the SRS describes the safety functions 
that are needed to prevent or act upon demands (i.e., undesired events) occur-
ring within the EUC. Performance criteria for each safety function must also be 
specified. 
Safety integrity requirements. This part states how reliable the safety functions must 
be in order to achieve adequate risk reduction. 
The SRS is not only a design-related document, but a document that must remain 
updated throughout the whole life cycle of the SIS. A proposed structure of an SRS 
is provided in Annex E of NOG-070 (2004). 
2.10 
Reliability of Safety-Instrumented Systems 
The reliability of a SIF quantifies the ability of the SIF to perform according to the 
requirements stated in the SRS, and is a key property that needs to be specified. The 
term reliability is defined as: 
Reliability. The ability of an item to perform a required function, under given envi-
ronmental and operational conditions, and for a stated period of time. 
A few comments may be made regarding this definition: 
- The item may be an element, a channel, a subsystem or the complete SIS, and 
it may include both hardware and software. 
- The required function may be a single function or a combination of several 
functions that are necessary to provide a specified service. 
- The stated period may be well defined, for example, to continue running for 
a certain number of hours, or more vague, such as available to perform the 
specified function when demanded. 
Reliability is closely related to similar concepts, such as quality, availability, 
dependability, safety, and security. Reliability and availability/dependability are 
achieved by: 
- Design 

4 8 
CONCEPTS AND REQUIREMENTS 
- System architecture, as redundancy 
- Proper type of elements (i.e., with "correct" functions), for example, trans-
mitter vs. switch 
- Elements with proper quality 
- Built-in self-testing/diagnostics 
- Installation - according to manufacturers's guidelines 
- Testing - both at start-up and at specified intervals or after any modification 
2.11 
Functional Safety Certificates 
Several agencies issue functional safety certificates for elements of a SIS, based on 
an examination of one or more samples of the element. The agency determines 
whether the element complies with the safety requirements of a specific functional 
safety standard. 
No element of a SIS needs to be certified, but the system integrator may pre-
fer to use certified elements inasmuch as reliability calculations for such elements 
have already been performed and are available. This can reduce the time required 
to document that the SIL is met and may improve the end-user's confidence in the 
documentation because a neutral third party has been involved. 
It is sometimes said that a particular element is, for example, a "SIL 3 elements." 
This is not correct because a SIL is only applicable for the whole safety function, and 
the whole control loop must be taken into account. Technically, it is correct to say 
that an element is "suitable for use within a given SIL environment," for example, 
"element X is certified for use in a SIL 3 environment." 
2.12 Safety Analysis Report 
A SIS usually comprises equipment from several equipment suppliers. The equip-
ment suppliers should prepare a safety analysis report (SAR) to document compli-
ance with the SRS. A suggested table of contents for the SAR and a flowchart de-
scribing the various steps of the preparation of the SAR are given in Annex E of 
NOG-070 (2004). 
A SAR is always based on a number of assumptions and it is important that all 
these assumptions are identified and properly stated in the SAR. The assumptions 
should be classified according to how they affect the performance of the SIS. This 
may typically include assumptions concerning: 
- Proof test intervals and how the safety functions are to be tested 
- Maximum allowable repair times for redundant equipment 
- Demand rates 

FUNCTIONAL SAFETY ASSESSMENT 
49 
- Functions that are not to be inhibited or overridden (or only for limited time 
periods) 
- Manual operator intervention or activation of safety functions 
- How an operational activity shall be carried out, for example, the opening se-
quence for valves or the use of written checklists 
- Allowable response times for the safety functions 
► Remark: It is sometimes seen that assumptions made in the SARs are in conflict 
with the requirements given in the SRS documents and/or the input given to the 
maintenance systems (e.g., the length of the proof test intervals). The importance of 
ensuring consistency between the various assumptions, requirements, and documents 
should be stressed. 
® 
2.13 
Functional Safety Assessment 
A functional safety assessment (FSA) is a systematic and independent examination 
of the adequacy of the functional safety achieved by the SIS within the particular 
environment. The requirements for the FSA are outlined in Chapter 8 of IEC 61508-
1. The FSA is carried out by one or more professionals in system safety and these 
shall have access to all relevant persons that have been involved in the design of the 
SIS and to all relevant documents. Whether the FSA can be carried out within the 
company or must be performed by an independent third party depends on the SIL 
required for the S IF. 
The FSA can be performed after each phase of the safety life cycle or after con-
cluding a specific number of phases. The FSA should, as a minimum, check that: 
- Hazard and risk assessment recommendations are implemented 
- The SRS is followed in design, construction, and implementation 
- Operating, emergency, maintenance, and safety procedures pertaining to the SIS 
are in place 
- The validation of the SIS is properly done 
- Employee training related to the SIS is completed 
- Recommendations from previous FSAs have been resolved 
See Chapter 8 of IEC 61508-1 for detailed requirements for the FSA. 

5 0 
CONCEPTS AND REQUIREMENTS 
2.14 
Reliability and Decision-Making 
The main purpose of a reliability assessment is to provide information as a basis 
for decision-making. The decision may concern technical design issues, design pro-
cesses, construction phases, and operation and maintenance issues after the SIS has 
been put into operation. A reliability assessment may give answers to the following 
questions: 
1. What should be the desired reliability of SIFs that are performed by the SIS? 
To answer this question, we need to understand the process of risk analysis and 
risk assessment and the translation of overall risk criteria down to the individual 
criteria for the SIFs. This translation is often called reliability allocation. 
2. What is the predicted reliability of the same SIFs? To answer this question, we 
need to understand many aspects of system reliability theory, such as failure 
identification and analysis, system reliability modeling, and reliability quantifi-
cation. 
3. What is the actual reliability for the same SIFs? To answer this question, we 
need to understand the same aspects as for predicted reliability. In addition, we 
need to understand how application-specific information or data may be used to 
improve our knowledge about the reliability performance in the real operating 
environment. 
4. Is the predicted or actual reliability adequate, in light of the desired perfor-
mance? 
5. How can we maintain the reliability performance throughout the life? To answer 
this question, we need to extend reliability assessment with reliability man-
agement activities. This means that we need to understand how operation and 
maintenance influence the reliability performance. 
Reliability assessments are always based on system models, such as graphical 
and/or mathematical representations of the system. The models are needed to trans-
form observable quantities, such as the number of failures, into reliability measures, 
such as the average probability of dangerous failure (PFDavg) or the mean time to 
failure (MTTF). Due to the complexity of a SIS, the mathematical descriptions are 
usually based on probabilistic rather than deterministic theory and methods. For any 
reliability model, two conflicting interests always apply (Rausand & H0yland, 2004): 
- The model should be sufficiently simple to be handled by available mathemati-
cal and statistical methods. 
- The model should be sufficiently "realistic" such that the deduced results are of 
practical relevance. 
The starting point should therefore always be to clearly understand what type of 
decisions the results from the reliability assessment should provide input to, and also 

ADDITIONAL READING 
51 
the required format of the input to the decision. Issues that may need to be raised in 
an initial phase are, for example (Rausand & H0yland, 2004): 
- Precisely which parts of the system are going to be included in the assessment 
and which parts are not? Are only the technical parts included, or also the 
human and organizational elements? 
- Precisely what are the objectives of the assessment? Different objectives may 
necessitate different approaches. 
- What are the system interfaces, and in what way do they interact with the sys-
tem? 
- What level of detail is required from the assessment? 
- In what context will the results be used? 
- Who are the users of the results? The results need to be presented so that they 
are fully understood by the users. 
- What operational phases are to be included in the assessment (e.g., start-up, 
steady state, maintenance, disposal)? 
- What are the environmental conditions for the system, and is it likely that they 
will change over time? 
- Which external stresses should be considered (e.g., sabotage, earthquakes, light-
ning strikes)? 
2.15 
Additional Reading 
The following titles are recommended for further study related to Chapter 2: 
- Safety Critical Systems Handbook: A Straightforward Guide to Functional 
Safety, IEC 61508, and Related Standards (Smith & Simpson, 2011) is a prac-
tical handbook in functional safety. It does not explain the background for the 
quantitative methods but can be a good supplement to the current book. 
- Safety Instrumented Systems Verification: Practical Probabilistic Calculations 
(Goble & Cheddie, 2005) has a similar focus as the book you are reading, but 
is more oriented towards physical details and does not explain the theory to the 
same detail as this book aims to do. 
- Guidelines for Safe and Reliable Instrumented Protective Systems (CCPS, 2007) 
discusses many of the same issues as this book but does not offer much help 
when it comes to reliability quantification. 
- Application of IEC 61508 and IEC 61511 in the Norwegian Petroleum Industry 
(NOG-070,2004) gives valuable advice on how to implement and use IEC 61508 
and IEC 61511, also for nonpetroleum applications. 


CHAPTER 3 
FAILURES AND FAILURE ANALYSIS 
3.1 Introduction 
This chapter gives an introduction to the main concepts that are used to describe and 
classify failures of a SIF. The failures are related to items, which can be elements, 
channels, subsystems, or safety loops. At the end of the chapter, two analytical meth-
ods that can be used to identify and study the possible failures of a SIF are described 
briefly: Failure modes, effects, and criticality analysis (FMECA) and Failure modes, 
effects, and diagnostic analysis (FMEDA). 
3.2 Failures and Failure Modes 
Failure and failure mode are the two most important concepts in any reliability anal-
ysis of a technical system. 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
5 3 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

5 4 
FAILURES AND FAILURE ANALYSIS 
3.2.1 
Failures and Faults 
A failure occurs when an item is no longer able to perform one or more of its required 
functions and is defined as: 
Failure. The termination of the ability of an item to perform its required function 
(IEV 191-04-01). 
Failure is therefore the event that takes place when a required function is termi-
nated. After a failure, the item will usually be in a failed state and we say that it has 
a fault. The transition (i.e., failure) from the functioning state to a failed state is not 
always clear and possible to observe, and there is sometimes a gradual drifting out 
of an acceptable state or performance. 
To be able to distinguish a functioning state from a failed state, we must have a 
clear understanding of the functional requirements that have been specified for the 
item. Functional requirements are also called performance requirements. 
dP EXAMPLE 3.1 Performance requirements for a pump 
Consider a pump that is required to pump 100 liters per minute with an accept-
able variation of ± 10 liters per minute. The pump is considered to be func-
tioning as long as the pumped volume is within the interval (90,110) liters per 
minute. The pump is said to have failed when the pumped volume is outside this 
interval. The criticality of the fault will usually increase with the deviation from 
the target value (i.e., 100 liters per minute). 
® 
A fault is defined as: 
Fault. The state of an item characterized by inability to perform a required function, 
excluding the inability during preventive maintenance or other planned actions, 
or due to lack of external resources (IEV 191-05-01). 
A fault is hence a state of an item and may have its origin in a (i) random fail-
ure event or in a (ii) deficiency related to the item, its location, or its application. 
IEC 61508 distinguishes between these two categories and calls the first (i) random 
hardware failures and the second (ii) systematic faults. The two categories are dis-
cussed in more detail in Sections 3.5.4 and 3.5.5. 
A failure may originate from an error, which is a "discrepancy between a com-
puted, observed or measured value or condition and the true, specified or theoret-
ically correct value or condition" (IEV 191-05-24). An error is not (yet) a fault 
because it is within the acceptable limits of deviation from the desired performance 
(target value). The concepts error, failure, and fault are illustrated in Figure 3.1. In 
Example 3.1, an error occurs when the pumped volume deviates from its target value 
and approaches, for example, 110 liters per minute. An error may develop into a 
failure if no further actions are taken. For this reason, an error is sometimes referred 
to as an incipient failure (e.g., see OREDA, 2009). 

FAILURES AND FAILURE MODES 
55 
Time 
Figure 3.1 Error, failure, and fault. 
► Remark: There are many different definitions of the term fault in the standards. 
In IEC 62425 (2007), for example, a fault is defined as "an abnormal condition that 
could lead to an error in the system." This definition is clearly in conflict with the 
definition given above. 
0 
3.2.2 
Failure Modes 
A failure mode is a description of a fault, telling how we can observe the inability 
of the item to perform a required function according to the functional requirements. 
For the terminology to be consistent, a failure mode should have been called a fault 
mode, as used in the International Electrotechnical Vocabulary (IEV), which defines 
a fault mode as: 
Fault mode. One of the possible states of a faulty item, for a given required function 
(IEV 191-05-22). 
The book uses the term failure mode since it is so well established in the industry 
that the introduction of a new term might lead to confusion. Some failure modes for 
a shutdown valve are defined and discussed briefly in Example 3.2. 
fl 
EXAMPLE 3.2 Failure modes of a shutdown valve 
Consider a shutdown valve that is installed in a gas pipeline feeding a production 
system. The shutdown valve is a final element of an ESD system, which is 
performing a SIF. If an emergency occurs in the production system (i.e., the 
EUC), the valve should close and stop the gas flow. The valve is a hydraulically 
operated fail-safe gate valve that is similar to the valve shown in Figure 1.7. 
When an emergency situation is detected in the EUC, an electric signal is sent 
to the valve control system and the pressure in the valve actuator is bled off. In 
this example, we consider only the valve. The main failure modes of the valve 
are: 
- Fail to close on command (FTC). This failure mode may be caused by a 
broken spring, blocked return line for the hydraulic fluid, too high friction 

56 
FAILURES AND FAILURE ANALYSIS 
between the stem and the stem seal, too high friction between the gate and 
the seats, or by sand, debris, or hydrates in the valve cavity. The failure 
mode is observed either as a failure event, "the valve does not close," or 
afterwards, as a failed state, "the valve is open, but should have been in 
closed position." 
Leakage (through the valve) in closed position (LCP). This failure mode is 
mainly caused by corrosion and/or erosion on the gate or the seat. It may 
also be caused by misalignment between the gate and the seat. The failure 
mode cannot be observed as a failure event and is only observed as a failed 
state: "the valve is leaking (more than an acceptable amount)." 
Spurious trip (ST). This failure mode occurs when the valve closes without 
a closing signal. It is caused by a failure in the hydraulic system or a leakage 
in the supply line from the control system to the valve. The failure mode is 
sometimes observed as a failure event, "valve is closing without a signal," 
or afterwards as a failed state, "the valve is in closed position, but should 
have been open." 
Closing too slowly (CTS). The process may require the valve to close within 
a certain time interval (e.g., 10 seconds) after the ESD signal has been given. 
Possible causes may be friction between the stem seal and the stem or a de-
graded or partly broken spring. If the closing time is monitored, the failure 
event may be observed. Afterwards, the failure cannot be observed because 
the valve is found in a normal, closed position after the shutdown action. 
Fail to open on command (FTO). When the valve is closed, it may fail to 
reopen. Possible causes may be leakage in the control line, too high friction 
between the stem seals and the stem, too high friction between the gate and 
the seats, and sand, debris, or hydrates in the valve cavity. Both the failure 
event and the failed state can normally be observed. 
φ 
Note that a failure mode is always related to a required function of the item. In 
Example 3.2, "corrosion on the stem" and "wear of the valve seal" are not failure 
modes but possible causes of failure modes of the valve. 
The failure mode concept may be rather confusing. In most cases, a failure results 
in a failure mode. An example is a light bulb that suddenly fails. Before the failure, 
the bulb is functioning as normal; whereas after the failure, the bulb has the failure 
mode "Cannot give light," which is a state. We may therefore say that the failure 
causes a failure mode. 
A second example is the failure/fault mode "Leakage (through the valve) in closed 
position" (LCP) in Example 3.2. A likely cause of this failure/fault is erosion in 
the gate sealing area, which is a progressive failure mechanism. When the erosion 
reaches a specified level, the valve is no longer able to perform its required functions 
according to the performance criteria and has failed. The failure event occurs at a 

FAILURES AND FAILURE MODES 
5 7 
time that is impossible to observe. After the failure, the valve has the failure mode 
"Cannot prevent leakage (through the valve) in closed position." Because the valve 
is operated in demanded mode, it is kept open for long periods and the failure mode 
may be present for a long time before the failure is manifested. The manifested 
failure occurs when the safety function of the valve is demanded or during a proof 
test (see Chapter 4). In this case, the failure mode is present both before and after 
the failure is manifested. 
A third example is the failure mode "closing too slowly" in Example 3.2. A likely 
cause of this failure mode is a sticking stem seal. The sticking effect will usually 
develop gradually, and at some time, the closing time will be too long. This means 
that a failure has occurred and the valve has a failure mode. It is, in most cases, 
not possible to observe the failure event and the failure mode without operating the 
valve. The manifested failure occurs when the valve function is demanded or during 
a proof test. The effect of the failure mode "closing too slowly" on the downstream 
system may be disastrous, but, in some cases, no fault can be observed on the failed 
item after it has been closed because the movement of the valve has removed the 
sticking seal problem. In this case, the failure mode is present before the failure 
is manifested, but not after. Partial stroke testing of valves is sometimes done by 
slightly moving the valve some few percentages of full stroke. This type of testing 
can be performed rather frequently without interrupting the flow through the valve 
and has at least two positive effects: (i) it reduces the likelihood of sticking stem 
seals, and (ii) sticking seals are detected earlier. 
The three examples show that we must distinguish between two types of failure 
events: (1) when the item is no longer able to perform a required function according 
to the performance requirements, and (2) when the failure is manifested or observed. 
In the first example, the two categories are merged, whereas they are distinct in 
the two next examples. Failure events of the first category are often not possible 
to observe whereas failure events of the second category are easier to observe, and 
these types of failures are recorded in reliability databases (see Chapter 6). 
3.2.3 
Failure Rate 
The failure rate concept is introduced in Appendix A, Section A4. Very simplified, 
we may say that the failure rate, λ, is the frequency of the occurrence of failures: 
Mean number of failures in a time interval of length t 
The failure rate with respect to failure mode A is often written as AA-
► Remark: Note that the term "fault rate" has no meaning, because it is not possible 
to define the rate or frequency of a state. A rate/frequency must always be related to 
some type of events that may occur. 
© 

5 8 
FAILURES AND FAILURE ANALYSIS 
3.3 
Failure Causes and Mechanisms 
Failure causes and failure mechanisms are essential for understanding why failures 
occur and how they can be avoided in the future. 
Failure cause. The circumstances during design, manufacture, or use that have led 
to a failure (IEV 191-04-17). 
Failure mechanisms. The physical, chemical, or other processes that have led to a 
failure (IEV 191-04-18). 
Examples of failure mechanisms are wear, corrosion, hardening, pitting, and oxi-
dation. This level of failure cause description is, however, not sufficient to evaluate 
possible remedies. Wear can, for example, be a result of incorrect material speci-
fication (design-related failure), usage outside specification limits (excessive stress 
failure), poor maintenance, such as inadequate lubrication (maintenance or opera-
tional failure), and so on. These fundamental causes are sometimes referred to as 
root causes, the causes upon which remedial actions can be decided. 
3.4 
Failure Effects 
Failure effects are the consequences the failure modes may have on the EUC, the 
SIS, a SIS subsystem, humans, and/or the environment. Not all failure consequences 
are equally important, and the main motivation for identifying failure effects is to 
distinguish the more severe failure modes from less severe ones. Such prioritizing 
may be useful for maintenance and test planning, identification of training needs, and 
for making decisions on upgrades and modification projects. The failure effect may 
depend on the operational status of the system when the failure occurs, as illustrated 
in Example 3.3. 
fl 
EXAMPLE 3.3 Airbag system 
A spurious release of an airbag in a parked car is usually safe, unless a small 
child is sitting in the seat where the airbag is released. A similar spurious release 
of the airbag when driving on a motorway is highly unsafe, as it can cause the 
driver to lose control of the car. 
φ 
Duration is also of importance when evaluating the failure effects. 
β 
EXAMPLE 3.4 Automobile engine control 
Consider an automobile engine control system (adapted from EASIS, 2006). 
A spurious full engine torque produced during 1 second is less critical than a 
spurious full engine torque produced during an interval of 10 seconds, as the 
speed increase will be very small in a short period. 
Θ 

FAILURE/FAULT CLASSIFICATION 
5 9 
3.5 
Failure/Fault Classification 
Failure and faults of a SIF can be classified according to many different criteria. 
Some main types of classifications are listed in the following. To distinguish clearly 
between failures and faults is sometimes difficult and may lead to confusion. The 
attentive reader may notice that the author has not been strictly consistent in the 
following classifications. 
3.5.1 
Classification Based on Fault Descriptors 
Duration of fault: Faults can be classified according the length of the time the fault 
is lasting. 
1. Intermittent faults: Faults that are present under specific conditions and that 
disappear without any repair action when the conditions change. An example 
of an intermittent fault is a fault that appears when the temperature or the 
humidity is high and disappears when the temperature/humidity is reduced. 
2. Extended faults: Failures that result in a prolonged fault state and continue 
until some part of the item is replaced or repaired. 
Complete versus partial faults: Extended faults may be further divided into: 
1. Complete fault: Fault with complete loss of a required function. 
2. Partial fault: Fault with loss of some functionality but not complete loss of a 
required function. 
Failures may also be classified as complete or partial. 
Sudden versus gradual failure: Both the complete failures and the partial failures 
may be further classified: 
1. Sudden failures: Failures that could not be forecasted by prior testing or ex-
amination. 
2. Gradual failures: Failures that could be forecasted by testing or examination. 
A gradual failure represents a gradual "drifting out" of the specified range of 
performance values. The recognition of gradual failures requires comparison 
of actual device performance with a performance specification and may in 
some cases be a difficult task. 
A failure that is both sudden and complete is sometimes called a catastrophic 
failure, whereas a failure that is both partial and gradual (such as the wear of the 
tires on a car) may be called a gradual failure. 

60 
FAILURES AND FAILURE ANALYSIS 
3.5.2 
Classification Based on Consequence and Detectability 
Hardware failures can be classified as: 
Dangerous (D) failure. A dangerous failure is a failure that brings the item into a 
state where it is not able to perform its safety function(s). When the item is in 
such a state, it is said to have a dangerous (D) fault. If a demand should occur 
when the item has D fault, the item is not able to respond to the demand (i.e., it is 
not able to bring the EUC to the defined safe state). 
Safe (S) failure. A safe failure is a failure that does not leave the item in a state where 
it is not able to perform its safety function(s). When the item is in such a state, it 
is said to have a safe (S) fault. 
A safe failure is often a spurious operation of the safety function that brings the 
EUC (or a part of the EUC) into a safe state. When a safe failure of a SIS occurs, 
the EUC either goes to a safe state or remains in a safe state. When a fuse blows, 
there is a failure but it is highly probable that the fault is to an open circuit that, 
in most cases, is a safe fault. Safe failures may sometimes have significant eco-
nomic consequences, for example, when the failure leads to a spurious production 
shutdown. 
► Remark: The classification of a failure/fault as dangerous or safe only relates to 
the specific safety function and a "safe failure" according to this definition may not 
be safe in a general context. Reconsider, for example, the airbag system in an au-
tomobile. Dangerous failure means that the airbag cannot blow up if a crash (i.e., a 
demand) should occur. If the airbag blows up spuriously while driving on a highway, 
this is a "safe" failure according to the definition, but may lead to even higher con-
sequences than a "dangerous" failure. 
φ 
In addition to dangerous and safe failures, IEC 61508 also considers no effect 
failures and no part failures: 
No effect failure. A failure of an item that is part of the safety function but has no 
direct effect on the safety function. 
No part failure. A failure of an item that plays no part in implementing the safety 
function. The item is, however, part of the circuit diagram and should be listed 
for completeness. 
Annunciation failure. A failure that has no effect on the safety function, but does 
affect the ability to detect future faults, for example, a failure of an internal diag-
nostic circuit of an item (exida, 2010). 
Hardware failures/faults may be categorized as detected or undetected: 
Detected. A fault that is detected by automatic diagnostic testing, internal in the 
item, or connected to a logic solver. 

FAILURE/FAULT CLASSIFICATION 
61 
Element 
Result of failure 
Revealed by 
Failure category 
Dangerous 
Safe 
Proof test 
Diagnostic 
Proof test 
Diagnostic 
Dangerous undetected (DU) 
Dangerous detected (DD) 
Safe undetected (SU) 
Safe detected (SD) 
Figure 3.2 
Failure categories in a tree structure. 
Undetected. A fault that is not detected (not diagnosed) by automatic diagnostic 
testing, internal in the item, or connected to a logic solver. Undetected faults are 
usually revealed in proof tests (see Chapter 4), or if a demand should occur. 
The following categories of hardware failure/faults can therefore be distinguished: 
Dangerous undetected (DU) faults. DU faults are preventing activation on demand 
and are revealed only by proof-testing or when a demand occurs. DU faults are 
sometimes called dormant or hidden faults. The DU faults are of vital impor-
tance when calculating the SIF reliability as they are a main contributor to SIF 
unavailability. 
Dangerous detected (DD) faults. DD faults are detected a short time after they oc-
cur, by automatic diagnostic testing. The average period of unavailability due to a 
DD failure is called the mean time to restoration (MTTR), the mean time elapsing 
from the DD failure occurs until the function is restored. 
Safe undetected (SU) failures. Nondangerous failures that are not detected by auto-
matic self-testing. 
Safe detected (SD) failures. Nondangerous failures that are detected by automatic 
self-testing. In some configurations, early detection of failures may prevent an 
actual spurious trip of the system. 
These failure categories are illustrated in Figure 3.2. 
Categories of Failure Rates. 
The failure rate concept was introduced briefly in 
Section 3.2.3. The failure rate symbols for the failure mode categories listed above 
are given in Table 3.1. 

62 
FAILURES AND FAILURE ANALYSIS 
Table 3.1 Failure rate symbols and their relationships. 
Undetected 
Detected 
Sum 
Dangi ;rous failures 
A-DU 
AD 
Safe failures 
Asu 
ASD 
As 
ÖP EXAMPLE 3.5 Periodically proof-tested shutdown valve 
Consider a shutdown valve that is proof-tested at regular intervals. An important 
issue is to determine which failure modes of the valve are dangerous. In many 
cases, there are at least three dangerous failure modes: 
1. Fail to close (FTC). With this failure mode, the valve is not able to close on 
command. 
2. Leakage in closed position (LCP). The valve is able to close on command, 
but there is a leakage through the closed valve that is higher than accepted 
leakages. 
3. Leakage to the environment (LTE). With this failure mode, there is a leakage 
out from the valve, through flanges, bonnet seals, and so on, such that the 
fluid in the pipeline reaches the environment. 
The failure modes FTC and LCP cannot be detected unless we close the 
valve, and are therefore undetected during normal operation. The failure mode 
LTE is usually detected rather quickly, by leak detectors or by visual inspection. 
The failure modes FTC and LCP are therefore DU failures, whereas LTE is a 
DD failure. It is also noticeable that the two DU failure modes, FTC and LCP, 
may have very different consequences. In some applications, FTC may lead to 
disaster whereas LCP may be more like a nuisance. 
® 
β 
EXAMPLE 3.6 Pressure transmitter 
A pressure transmitter is installed in a pipeline to monitor the pressure and raise 
an alarm when the pressure exceeds a threshold. The transmitter is calibrated 
so that the pressure reading corresponds to signals between 4mA and 20mA. A 
value below or above this range is treated as a faulty reading. In this case, the 
failure modes may be as follows: '. 
- Fail low (<4mA): Will not be able to measure pressure. Will be alarmed. 
This example is adapted from an example by exida 

FAILURE/FAULT CLASSIFICATION 
63 
Fail high (>20 mA): Will not be able to measure pressure. Will be alarmed. 
Too low reading: Set point may not be exceeded even if the actual pressure 
is higher (DU). 
Too high reading: Set point exceeded even if the actual pressure is lower. 
No effect: The failure has no effect on the safety function. 
Diagnostic annunciation undetected: The diagnostic system is not able to 
announce that it does not work. 
Diagnostic annunciation detected: The diagnostic system is able to an-
nounce that it does not work. 
φ 
3.5.3 
Failures/Faults Classified According to Cause 
Items may fail-to-function due to many different causes. Some causes result in latent 
faults that are present during a prolonged period, whereas other causes result in a 
failure event. The following categories are sometimes used: 
1. Design-related fault: A fault due to inadequate specification or design of an 
item. 
2. Aging failure: A failure whose probability of occurrence increases with the pas-
sage of time, as a result of deterioration processes inherent in the item. Aging 
failures occur under conditions within the design envelope of the item. Aging 
failures are also called primary failures. 
3. Software fault: A functional fault caused by an error in the integrated software. 
4. Manufacturing fault: A fault due to nonconformity during manufacture of an 
item, related to the design or to specified manufacturing processes. 
5. Installation fault: A fault due to erroneous or inadequate installation of the item. 
6. Maintenance or operational fault: A fault due to inadequate or erroneous testing 
and maintenance or errors made during operation. 
7. Excessive stress failure: A failure due to stresses during use that exceed the 
stated capabilities of the item. The stresses may be shocks from thermal, me-
chanical, electrical, chemical, magnetic, or radioactive energy sources. The 
stresses may be caused by neighboring components, the environment, or by 
system operators/plant personnel. The excessive stresses may give rise to mul-
tiple or common-cause failures. For some stresses, the multiple failures may 

6 4 
FAILURES AND FAILURE ANALYSIS 
occur at the same time. For other stresses, such as excessive humidity, the fail-
ures may occur during a rather long time interval. Excessive stress failures are 
also called secondary failures. 
8. Mishandling failure: A failure caused by incorrect handling or lack of care of 
the item. 
A failure or fault often have more than one cause and may therefore be classi-
fied into more than one of the categories above. An item that is degraded may, for 
example, have a higher tendency to fail if an excessive stress occurs. 
β 
EXAMPLE 3.7 Causes of experienced failures 
Several faults of control and safety-instrumented systems were investigated by 
the UK HSE and attributed to categories of failure causes. The following distri-
bution was found (HSE, 2003): 
Cause 
Percent 
Specifications 
44% 
Changes after commissioning 
20% 
Design and implementation 
15% 
Operations and maintenance 
15% 
Installation and commissioning 
6% 
3.5.4 
Random Hardware Failures 
IEC 61508 distinguishes between random hardware failures and systematic faults. 
The standard also treats software faults, but these may be considered a subclass of 
the systematic faults (e.g., see Note 3 on page 16 of IEC 61508-4). 
Random hardware failure. Failure, occurring at a random time, which results from 
one or more of the possible degradation mechanisms in the hardware (IEC 61508, 
2010, Part 4, par. 3.6.5).2 
A random hardware failure can result in a DD, DU, SD, or SU fault. Three fea-
tures of a random hardware failure can be deduced from the definition: 
1. A random hardware failure only applies to the hardware part of an item. 
2. The failure occurs at a random time, which implies that the time when the failure 
occurs can be described by a random variable with a probability distribution that 
may be more or less known. 
2IEC 61508-4 ed.2.0 "Copyright © 2010 IEC Geneva, Switzerland, www.iec.ch." 

FAILURE/FAULT CLASSIFICATION 
65 
3. The failure of the item results in a physical fault and this fault has to be repaired 
for the item to be able to function again. A random hardware failure is therefore 
also called a physical failure. 
What IEC 61508 means by "degradation mechanisms," is not quite clear to the au-
thor. Most analysts would classify a random failure of an electronic component as a 
random hardware failure, but electronic components can fail without any discernible 
degradation. A random hardware failure of an item can, for example, be caused by: 
- Degradation of the item, i.e., aging 
- Inadequate maintenance 
- Stresses and combination of stresses within the design envelope of the item 
- Internal stresses outside the design envelope (e.g., humidity, temperature, vi-
bration) 
- Excessive stresses from the environment (e.g., lightning, earthquake) 
- Human errors (some, but not all) 
Random hardware failure is a contested concept and several analysts do not agree 
with the author that hardware failures caused by human errors and excessive stresses 
(internal and external) should be classified as random hardware failures. 
3.5.5 
Systematic Faults 
A systematic failure is defined as follows: 
Systematic failure. Failure, related in a deterministic way to a certain cause, which 
can only be eliminated by a modification of the design or of the manufacturing 
process, operational procedures, documentation, or other relevant factors. 
Note 1 : 
Corrective maintenance of a systematic failure without modification will 
usually not eliminate the failure cause. 
Note 2: 
A systematic failure can be induced by simulating the failure cause 
Note 3: 
Examples of causes of systematic failures include human errors in: 
- The safety requirements specification 
- The design, manufacture, installation, operation of the hardware 
- The design, implementation, etc. of the software. 
(IEV 191-04-19). 
A systematic fault is a fault resulting from a systematic failure (IEV 191-05-21). 
For the author, systematic fault is a more relevant concept than systematic failure. A 
design fault that inhibits a channel from performing its safety function in a certain 
context is an example of a systematic fault. The error or failure that resulted in 

66 
FAILURES AND FAILURE ANALYSIS 
the design fault may have been done at some stage in the design process. When 
the channel fails to perform its safety function due to such a systematic fault, it is 
not always feasible—or, it does not help—to physically repair the channel. The 
channel has to be modified to be able to carry out its safety function in a similar 
context in the future. A systematic fault is therefore also called a non-physical fault. 
Systematic faults are discussed extensively and related to common-cause failures in 
IAEA (2009). See also Gentile & Summers (2006). 
► Remark: When a channel fails to perform its safety function due to a systematic 
fault, some analysts say that a systematic failure occurs. According to IEC 61508 
and IEV, this is incorrect use of the terminology. It is more correct to say that a sys-
tematic failure (or error) has resulted in a systematic fault (i.e., a state), and that this 
fault is a cause of the channel's failure to function. 
Θ 
A systematic fault of an item is a dormant fault that may cause a failure to per-
form the item's safety function when the item enters a particular state, encounters a 
particular environment, or receives erroneous inputs from interfacing items. Failures 
caused by systematic faults do not occur as random events, but rather appear in a 
deterministic way when the system reaches the situation for which the underlying 
systematic fault is existing (see IEC 61508, 2010, Part 6, Annex D, p. 80). A failure 
rate for item failures caused by systematic faults can therefore not be quantified. 
fl 
EXAMPLE 3.8 Systematic fault of gas detector 
A gas detector is installed in a production room. Gas leaks may occur at several 
places in the room. The room has an efficient ventilation system that prevents 
the gas from some possible leak locations from reaching the gas detector. The 
gas detector may therefore fail to carry out its safety function (i.e., to transmit 
signal about gas leak) even if the gas detector as such is functioning perfectly. 
The gas detector fails due to a systematic fault, which may be caused by the 
relation between the location of the gas detector and the layout or efficiency of 
the ventilation system. 
A systematic fault of this type is usually not revealed in a proof test (see 
Chapter 4), because the proof test is carried out by injecting test gas directly 
into the head of the detector. 
The failure mode of the gas detector is "does not transmit signal about gas 
leak." This failure mode can, in a general case, result from a random hardware 
failure of the detector or be due to a systematic fault. There can be many dif-
ferent systematic faults that prevent the correct functioning of the detector, such 
as wrong location, miscalibration, covering of the detector, and so on. This 
example has illustrated only one type of systematic faults. 
φ 

FAILURE/FAULT CLASSIFICATION 
6 7 
fl 
EXAMPLE 3.9 Systematic fault of pressure transmitter 
Consider a pressure transmitter that is installed on a pressure vessel at the end 
of a sensing tube. The sensing tube has a manual valve. To proof-test the pres-
sure transmitter, the manual valve is closed and a test gas is injected between the 
manual valve and the transmitter. Even if the pressure transmitter is functioning 
perfectly, it may fail to transmit signal about high pressure in the vessel due to 
at least two types of systematic faults: (i) the manual valve was not reopened 
when the test was completed, (ii) the sensing tube is plugged by dirt, scaling, or 
some other material. The first type can be attributed to human error, inadequate 
procedure, confusing closed/open signs on the manual valve, inadequate super-
vision, and several more. The second type may be due to inadequate filtering of 
the fluid, wrong material or inadequate design of the sensing tube. 
0 
Systematic faults can be introduced in all the life cycle phases of an item. Many 
systematic faults can be attributed to failures or errors in design, manufacturing, 
and installation, but many systematic faults are also due to failures and errors in the 
operating phase. Examples of systematic failures in the operating phase of a process 
system are: 
- A safety valve is bypassed during testing or maintenance, and the valve is not 
properly "reactivated" after the test/maintenance (e.g., the bypass valve is left 
open or partly open). 
- A flame detector is covered during a welding operation, and the cover is not 
removed when the welding operation is finished. 
- A pressure safety valve is blinded off during testing, and the blinding is not 
removed after the test. 
Due to their nature, systematic faults can often give rise to failure of multiple 
components, that is, common-cause failures, CCFs. CCFs are briefly introduced in 
Section 3.5.8 and are discussed in detail in Chapter 10. 
Systematic failure integrity is achieved by means of the quality management and 
safety management procedures specified in IEC 61508. Many systematic faults are 
the result of failure to follow best practices and efforts should therefore be taken to 
remove potential systematic faults in all life-cycle phases. Errors in software code 
are generally systematic, but can be eliminated by rigorous application of quality 
procedures and analysis during the software development. These topics are not pur-
sued in this book. 
Failure classification in the PDS method. 
The PDS method (SINTEF, 2013b) 
uses a failure classification that deviates slightly from the author's classification: 
(a) Random hardware failure - a failure that can be classified as: 
- Aging failure 

6 8 
FAILURES AND FAILURE ANALYSIS 
- Random failures due to natural (and foreseen) Stressors 
(b) Systematic failure - a failure that can be classified into one of five categories: 
- Software fault 
- Programming error 
- Compilation error 
- Error during software update 
- Hardware-related failure 
- Inadequate specification 
- Inadequate implementation 
- Design not suited to operational conditions 
- Installation failure 
- Gas detector cover left on after commissioning 
- Valve installed in wrong direction 
- Incorrect sensor location 
- Excessive stress failure 
- Excessive vibration 
- Too high temperature 
- Operational failure 
- Valve left in wrong position 
- Sensor calibration failure 
- Detector in override mode 
The main difference between the PDS method's classification and the author's 
classification is related to random hardware failures. Whereas the PDS method re-
stricts random hardware failures to be aging failures that occur when the item is 
exposed to stresses within the design envelope, the author also includes many types 
of excessive internal and external stresses and also many types of human errors. 
The offshore oil and gas industry has for more than 30 years collected data from 
operating installations as part of the OREDA project (see Chapter 6 for more de-
tails). OREDA maintains a huge database and has published several handbooks with 
reliability data. The data is extracted from the maintenance databases of the par-
ticipating oil companies, and covers all faults that have been repaired. By using 
the author's interpretation of a random hardware failure, all the failures recorded 
in OREDA are random hardware failures, whereas only a fraction of the failures 
recorded in OREDA are random hardware failures when using the PDS methods's 
interpretation. 
The author's interpretation of a random hardware failure can therefore be summa-
rized as: 
Random hardware failure. A hardware failure for which a maintenance workorder 
is issued to carry out a repair of the failure. 
If a human error results in a physical hardware fault that has to be repaired, the failure 
is classified as a random hardware failure. 

FAILURE/FAULT CLASSIFICATION 
69 
3.5.6 
Random Hardware Failures Versus Systematic Faults 
The borderline between random hardware failures and systematic faults is not fully 
clear and analysts often have different views on whether a given failure is a random 
hardware failure or due to a systematic fault. 
IEC 61508 takes a rather flexible view in Note 2 to the definition of a random 
hardware failure in Part 4 of the standard. This note can be interpreted such that a 
random hardware failure is a hardware failure whose failure rate "can be quantified 
with a reasonable accuracy." On the other hand, a systematic failure is a failure 
that cannot be "accurately statistically quantified because the events leading to them 
cannot easily be predicted." This interpretation is seen to comply with the author's 
interpretation above. 
fl 
EXAMPLE 3.10 Excessive stress as random hardware failure 
Consider a hardware item of a SIS that is vulnerable to the stresses that occur 
during lightning. Lightning occurs as a random process and the number of light-
ning strikes per year at a certain location can be predicted based on historical 
data. If the hardware item fails with a probability of, say, 0.1 % each time a 
lightning strike with a certain intensity occurs, then we are able to quantify the 
item's failure rate due to lightning with "reasonable accuracy" and the failure 
is a random hardware failure. The failure is an excessive stress failure and is 
therefore a systematic failure in the PDS method. 
® 
Important Borderline. The borderline between random hardware failures and sys-
tematic faults is important, because IEC 61508 requires only random hardware fail-
ures to be considered when the unavailability measures PFDavg and PFH are quanti-
fied. The interpretation of the borderline can therefore, in specific cases, determine 
whether or not a SIF can be accepted as, for example, SIL3. 
Many analysts do not have a clear view on this difference and use the failure 
rates they find in the available data sources. This means that they tacitly accept that 
random hardware failures are those failures for which failure rates are available in 
the data sources. 
3.5.7 Classification by Origin 
Failures of a SIS can be categorized according to the time of their origin (IEC 61508, 
2010, Part 2, Annex A): 
- Failures caused by faults originating before or during system installation (e.g., 
software faults include specification and program faults, hardware faults include 
manufacturing faults and incorrect selection of elements). 
- Failures caused by faults or human errors originating after system installation 
(e.g., random hardware failures, or failures caused by incorrect use). 

7 0 
FAILURES AND FAILURE ANALYSIS 
3.5.8 Common-Cause Failures 
A common-cause failure (CCF) is sometimes defined as follows: 
Common-cause failure. Failures of different items, resulting from a single event, 
where these failures are not consequences of each other (IEV 191-04-23). 
CCFs do not fit entirely into the classification of random hardware failure and 
systematic faults, but CCFs are often caused by systematic faults. Defense measures 
against systematic faults may therefore also be efficient means to defend against 
CCFs. CCFs are discussed in detail in Chapter 10. 
3.5.9 OREDA Failure Classification System 
OREDA is a data source for equipment used in offshore oil and gas production activ-
ities. The data source is further presented in Chapter 6. Several OREDA handbooks 
with generic data have been published (OREDA, 2009; ISO 14224, 2006). Here, 
failure causes are classified as either design-related, fabrication/installation-related, 
operation/maintenance-related, or miscellaneous (not identified or covered by the 
other categories). Failure effects are split into critical, degraded, and incipient: 
Critical failure: A critical failure is defined as a failure of an item that causes an 
immediate cessation of its ability to perform a required function. In this context, 
the term "required function" comprises two elements: The ability to activate on 
demand and the ability to maintain production when safe (no demands) (SINTEF, 
2013b). This failure category therefore includes failures that may prevent the 
execution of a SIF as well as unintended (spurious) activation failures. 
Degraded failure: A degraded failure is a partial failure, which means that the item 
has a degraded performance but that it is still able to perform its essential function. 
A hydraulic leakage in an actuator for a fail-safe (close) valve may, for example, 
lead to spurious closure of the valve, but will not, while in open position, prevent 
the valve from closing on demand. 
Incipient failure: An incipient failure is also a partial failure, but its degradation is 
barely noticeable. An incipient failure is more like a symptom that the item may 
soon get a degraded failure if no corrective action is taken. 
3.5.10 
Human Errors 
Human errors are often found to be among the causes of item failures, especially 
systematic failures. Human errors are not discussed in this book. 

FMECA 
71 
3.6 
FMECA 
3.6.1 
Introduction 
A Failure Modes, Effects, and Criticality Analysis (FMECA) is a structured, bottom-
up technique used to assess the effects on a system of each potential component 
failure. The FMECA is performed by analyzing each component within the system 
to identify how it might fail (failure mode) and what could be the consequences of 
such a failure on the system (failure effect). The analysis is usually documented in a 
specific FMECA worksheet as shown in Figure 3.4. 
3.6.2 
Objectives and Applications 
The objectives of an FMECA are to: 
(a) Identify how each of the system components can conceivably fail (i.e., what are 
the failure modes?). 
(b) Determine the causes of these failure modes. 
(c) Identify the effects each failure mode can have on the rest of the system. 
(d) Describe how the failure modes can be detected. 
(e) Determine how often each failure mode occurs. 
(f) Determine how serious the various failure modes are. 
(g) Assess the risk related to each failure mode. 
(h) Identify risk-reducing actions/features that may be relevant. 
FMECA is mainly used in the design phase of a SIS to identify and analyze poten-
tial failures. The analysis is qualitative but may have some quantitative elements that 
include specifying the failure rate of the failure modes and a ranking of the severity 
of the failure effects. 
FMECA can also be used in later phases of the life cycle of the SIS. The objective 
is then to identify parts of the system that should be improved to meet certain safety 
requirements, or as input to test and maintenance planning. 
3.6.3 Analysis Procedure 
The FMECA should preferably be performed by the design team and is based on de-
tailed knowledge about the hardware design and knowledge about the system compo-
nents. The FMECA produces several types of information. Related to the following 
quantitative analyses, the main outputs are 
- All the relevant failure modes for each component 

7 2 
FAILURES AND FAILURE ANALYSIS 
Hardware 
design 
Component 
information 
> 
» 
FMECA 
r 
>->■ 
Component 
failure modes 
Failure rate 
for each 
failure mode 
Severity 
of each 
failure mode 
Figure 3.3 
FMECA inputs and outputs related to quantitative assessment. 
- Failure rates for each failure mode 
- Severity ranking of each failure mode 
The relevant inputs and outputs are shown in Figure 3.3. 
A dedicated FMECA worksheet is used when performing the analysis. A typical 
FMECA worksheet is shown in Figure 3.4. The various columns of the worksheet 
are presented briefly below. More details are given, for example, by Rausand & 
H0yland (2004). 
Reference (col. 1) 
A unique reference to the component is given in this column. The reference can 
be to a drawing or some other documentation. 
Function (col. 2) 
The function(s) of the component is (are) described in this column. 
Operational mode (col. 3) 
The component may have various operational modes, for example, running or 
standby. A valve can, for example, be normally closed or normally open. 
For each component, the relevant failure modes and failure causes are identified. 
Experience data and generic failure mode checklists may provide useful help. 
Failure mode (col. 4) 
For each function and operational mode, the related failure modes are identified 
and recorded, one by one. When identifying failure modes, it is important to relate 
these to the functions and the performance requirements for the component. 
Failure cause (col. 5) 
For each failure mode in column 4, the possible failure causes and failure mecha-
nisms are recorded. 
Detection of failure (col. 6) 
The possible ways of detecting the identified failure modes are then recorded. 

Study object: Shutdown valve B4 
Reference: Process diagram 14.6-2013 
s m 
O > 
Figure 3.4 
Example of an FMECA worksheet. 
^, 
w 
Date: 
2013-07-01 
Name: Marvin Rausand 

7 4 
FAILURES AND FAILURE ANALYSIS 
These may involve diagnostic testing, proof-testing, human perception, and so 
on. Some failure modes are evident whereas other failures are hidden. A hidden 
failure is normally detected only during testing of the component. When FMECA 
is used in the design phase, this column records the designer's recommendations 
for diagnostics, functional testing, and so on. 
For each failure mode, the credible consequences are entered into columns 7 and 
8 in the FMECA worksheet in Figure 3.4. 
Local effects of failure (col. 7) 
Here, the consequences the failure mode have on the next higher level in the 
hierarchy are recorded. 
System effects of failure (col. 8) 
All the main effects of the identified failure mode on the primary function of the 
system are now recorded. The resulting operational status of the system after the 
failure may also be recorded, that is, whether the system is functioning or not. 
The risk is presented as the failure rate of each failure mode together with a brief 
assessment of the severity of the failure mode. 
Failure rate (col. 9) 
Failure rates for each failure mode are then recorded. Failure rates may be found 
in data sources, such as OREDA (2009), see Chapter 6. 
Severity (col. 10) 
The severity of a failure mode is usually interpreted as the worst credible conse-
quence of the failure. It is normally sufficient to rank the severity as HH, H, M, 
L, LL, where HH means extra high, H means high, M means medium, L means 
low, and LL extra low (i.e., negligible). 
Risk-reducing measures (col. 11) 
Possible actions to correct the failure and restore the function or prevent serious 
consequences are then recorded. Actions that are likely to reduce the frequency 
of the failure modes should also be recorded. 
Responsible (col. 12) 
Here, the name of the person who should be responsible for the follow-up of 
the failure mode and/or the risk-reducing measures that have been identified is 
recorded. 
Comments (col. 13) 
This column may be used to record pertinent information not included in the other 
columns. 
Several computer programs have been developed for FMECA. A suitable program 
can significantly reduce the workload of an FMECA and make it easier to update the 
analysis. 

FMEDA 
75 
exida 
exida is an international consulting company that provides product certification with 
IEC 61508 and related application specific standards, exida was established in 1999 
and today has offices in several countries around the world. The FMEDA technique 
was developed by exida, and the company provides FMEDA services to the industry. 
exida has developed several computer tools for SIL analysis, such as exSILentia. 
Further information: h t t p : / /www. e x i d a . com. 
3.7 
FMEDA 
A Failure Modes, Effects, and Diagnostic Analysis (FMEDA) is an extension of an 
FMECA that is tailor-made for a SIS (Goble & Brombacher, 1999; Goble & Cheddie, 
2005). The FMEDA is mainly developed by the company e x i d a (see box). The 
FMEDA worksheet has additional columns that cover the following: 
- Each failure mode is classified as either dangerous or safe. 
- The diagnostics related to each failure mode are identified and described. 
- The detectability of each failure mode by online diagnostics is assessed. A 
number 1 is entered to indicate detectability. A number 0 is entered if the failure 
mode is not detectable. 
- System specific failure rates related to the various categories are estimated, that 
is, ADU» ^ΌΌ, ^SU> a nd -^SD- Special spreadsheet programs have been devel-
oped for this purpose. 
- Diagnostic and proof test coverage. 
The FMEDA worksheet therefore provides traceable failure rates and failure mode 
distributions as a basis for calculations of PFD a v g, PFH, and SFF. The FMEDA ap-
proach is further described by Goble & Brombacher (1999) and Grebe & Goble 
(2007). The inputs to and the outputs from an FMEDA are shown in Figure 3.5. 
3.8 Additional Reading 
The following titles are recommended for further study related to Chapter 3: 
- System Reliability Theory; Models, Statistical Methods, and Applications (Rau-
sand & H0yland, 2004) gives a thorough introduction to the main concepts of 
failures and failure classification and how these concepts are used in reliability 
analyses. 

76 
FAILURES AND FAILURE ANALYSIS 
Hardware 
design 
Component 
information 
FMEDA 
Component 
failure modes 
Failure rate 
for each 
failure mode 
Severity 
of each 
failure mode 
Diagnostic 
coverage of 
failure modes 
Classified as 
dangerous 
or safe 
Figure 3.5 
FMEDA inputs and outputs. 
Guidelines for Safe and Reliable Instrumented Protective Systems (CCPS, 2007) 
has a chapter that discusses failure modes and failure classification. 
IEC 60812: Procedures for failure modes and effects analysis (FMEA) (IEC 60812, 
2006). This is the most authoritative international standard for FMEA/FMECA. 
SAE ARP 5580: Recommended failure modes and effects analysis (FMEA) 
practices for nonautomobile applications (SAE ARP 5580,2001). This standard 
is a good supplement to IEC 60812. 
SEM ATECH: Failure mode and effect analysis (FMEA): A guide for contin-
uous improvement for the semiconductor equipment industry (SEMATECH, 
1992). This guideline is rather old, but still one of the best FMEA guidelines 
you can find. 

CHAPTER 4 
TESTING AND MAINTENANCE 
4.1 
Introduction 
Testing and maintenance are key activities to ensure that a SIS achieves and main-
tains the desired performance. 
Testing. Execution of a function of a system, subsystem, or channel to confirm that 
the function can be performed according to the requirements stated in the SRS. 
Maintenance. The combination of all technical and administrative actions, including 
supervision actions, intended to retain an item in, or restore it to, a state in which 
it can perform a required function (IEV 191-07-01). 
The manufacturers of SIS elements and channels have to prepare a number of 
documents, such as technical descriptions, and manuals describing how the equip-
ment shall be installed, operated, tested, and maintained. In the context of this book, 
the most important of these documents is the Safety manual, which is also called a 
Functional safety manual. The safety manual can be a separate booklet or integrated 
into a more comprehensive user's manual. The contents of the safety manual may 
vary, but will normally cover procedures for installation, configuration and setup, 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
77 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

78 
TESTING AND MAINTENANCE 
startup, testing, maintenance, and shutdown. In addition, most safety manuals spec-
ify environmental limits for the equipment and give guidance on how to reveal faults. 
4.2 Testing 
A test may be designed to confirm correct performance and to confirm correct be-
havior in response to specific fault conditions, such as power loss. In the operational 
phase, the tests may be split into three main categories: (i) proof tests, (ii) partial 
tests, and (iii) diagnostic tests. 
4.2.1 
Proof-Testing 
A proof test is a carefully planned periodic test, which is designed to reveal all DU 
faults of each channel of a safety loop. A proof test is further designed to reveal all 
element faults that can influence the occurrence of DU faults such that the faults can 
be repaired. After the proof test and the associated repair, the safety loop is assumed 
to be as-good-as-new or as close as reasonably practicable to this condition. To make 
clear that test is designed to reveal all possible DU faults and element faults, the test 
is sometimes called a full proof test. 
A Function Test Is Not Always a Proof Test. Some channels have redundant 
elements, for example, a shutdown valve with two redundant solenoid valves. For 
such channels, it is not sufficient to test the function of the channel; the correct 
functioning of each and every element must be tested. A function test is therefore 
not always a proof test. 
Proof Test Procedure. 
A stepwise procedure for the proof test is usually described 
in the safety manual for the equipment. The proof test should reflect real operating 
conditions as accurately as possible. This may be problematic in some cases, as a 
fully realistic test may harm the equipment or induce unacceptable hazards. When-
ever possible, the safety function should be initiated by manipulating process vari-
ables without driving the process into a demand condition (HSE, 2002). The proof 
test of a SIF shall confirm the correct functioning of the entire safety loop that is 
performing the SIF, including the sensor elements, the logic solver, and the final el-
ements. In practice, a proof test is often split into sub-tests, such that the sensor 
subsystem, including the logic solver subsystem, and the final element subsystem 
are tested at different points in time (ISATR 84.00.02, 2002). 
A proof test is normally manual but may also be semi-automatic. This means 
that some type of human interaction is required. The human involvement may lead 
to errors and violations, where an error is an unintentional deviation from the test 
procedure and a violation is a deliberate deviation from a procedure. Both errors 
and violations can influence the effectiveness of proof tests and lead to undesirable 
outcomes (HSE, 2002). 

TESTING 
7 9 
State = = Unknown 
Pr 
te 
' 
DOf 
St 
No DU fault revealed 
i 
( 
DU fault /\S 
revealed 
State = DU fault 
* 
State = 
As-good-as-new 
^Repair 
Figure 4.1 
The process of proof-testing 
Proof-testing shall include, but not be limited to (NOG-070, 2004): 
- Operation of all input devices including sensing elements and input modules 
- Logic associated with each input device 
- Trip initiating values (set-points) for all inputs 
- Alarm functions 
- Response time for the SIF (when relevant) 
- Operating sequence of the logic program 
- Function of all final elements and output modules 
- Timing and speed of final elements 
- Function of manual trip to bring the EUC to a safe state (when relevant) 
- Function of user-initiated diagnostics 
- Complete system functionality 
- That the SIS is operational after the proof test 
The basic idea of proof-testing is illustrated in Figure 4.1. Before the proof test, 
the state of the item with respect to DU faults is unknown. A proof test is carried 
out to reveal possible DU faults. The result of the proof test is either that (i) no DU 
fault is revealed or that (ii) a DU fault is revealed. In the first case, the item is put 
into operation again and it is assumed that it is as-good-as-new. In the second case, a 
repair action is initiated to bring the item back to a functioning state. After the repair, 
the item is put into operation and assumed to be as-good-as-new. 

8 0 
TESTING AND MAINTENANCE 
Proof Test Consequences. 
Proof-testing improves the reliability of a safety sys-
tem, but may involve significant consequences: 
- Many proof tests require shutdown of the EUC. Shutdown and restart are some-
times hazardous operations and the EUC risk may therefore increase during 
these operations. 
- A proof test will often require several hours of EUC shutdown and the associ-
ated loss of production or service may be significant. 
The following aspects of proof-testing are important in quantitative analyses: 
Proof Test Interval. The time interval between the initiation of two consecutive 
proof tests is called the proof test interval and is denoted by τ. The length of the 
interval may range from some days to some years, depending on the system. The 
corresponding proof test frequency is the inverse of the proof test interval (i.e., 1/τ). 
The proof test interval is normally chosen to be at least an order of magnitude shorter 
than the mean time to a DU failure. In quantitative analyses, the proof test interval is 
usually assumed to be a constant value, but in practice, the interval may vary slightly. 
Perfect Versus Imperfect Proof-Testing. In reliability analyses, it is often as-
sumed that the proof test is a perfect proof test. This means that all DU faults are 
revealed and repaired as part of the proof test, and that the item is brought back to an 
as-good-as-new condition. The reasons why a proof test may not be perfect include: 
1. The proof test is inadequate and not able to reveal all types of DU faults. 
2. The proof test is carried out under conditions that differ from a real demand 
situation. 
A proof test is not perfect is called an imperfect proof test. Imperfect proof-testing 
is not a central concept in IEC 61508 (2010), but is referred to as non-perfect proof-
testing in IEC 61508-6. There are many reasons why a proof test is not performed 
under the same conditions as during a demand, as illustrated in Example 4.1. 
β 
EXAMPLE 4.1 Proof-testing pressure transmitters 
Proof-testing of pressure transmitters in a process plant is usually performed 
after the transmitters are isolated from die process. This is because pressurizing 
a pipeline/vessel to the trip pressure may, itself, lead to an unsafe situation. 
When such a proof test is performed, some DU failures, for example caused by 
contamination in the pressure-sensing lines, may remain hidden after the test. 
Θ 
Proof test coverage (PTC). The testing strategy is often a compromise between 
what is needed (with respect to revealing faults) and what is safe to do at the plant 
or with the system in question. As a result, some DU faults may remain hidden until 

TESTING 
81 
a real demand occurs. The ability of the proof test to reveal DU faults is called the 
proof test coverage. 
The proof test coverage (PTC) is the conditional probability that a DU fault will 
be detected by the proof test, given that the fault is present when initiating the proof 
test. 
PTC = Pr(Detect DU fault by proof test | DU fault is present) 
(4.1) 
The PTC can also be expressed as the percentage of DU faults that will be detected 
by the proof test among all the DU faults that are present when the proof test is 
initiated. The fraction of the DU faults that will not be detected by the proof test is 
therefore 1 — PTC. These faults may be revealed when a demand for the SIF occurs. 
A perfect proof test has PTC = 100%, since all DU faults are revealed, and an 
imperfect proof test has PTC < 100%. The higher the PTC, the better is the proof 
test. Imperfect proof-testing is discussed further in Chapter 11. 
Some manufacturers provide an estimate of the PTC in the safety manual. This 
estimate is obtained by a careful examination of each step of the proof test procedure 
and its ability to reveal DU faults, for example, by performing a detailed FMEDA. 
Mean test time. The mean test time is the mean time spent to perform the proof 
test. The test time is typically less than one hour, but may also be significantly longer 
in some applications. Leakage testing of shutdown valves in pipelines is most often 
carried out by closing a downstream valve and monitoring the pressure build-up in 
the pipe between the valves. When the volume between the two valves is large, this 
may take some time. The EUC may or may not be in a safe state during the test time. 
Mean repair time (MRT). The MRT is the mean time from when a DU fault is 
detected in a proof test until the fault is repaired and the item is put into function 
again. The EUC may or may not be in a safe state when the repair is carried out. 
Failures Induced by Proof-Testing. To be realistic, the proof test will sometimes 
expose a channel to hard stresses. These stresses can be so severe that the channel 
may fail, as illustrated in Example 4.2. 
fl 
EXAMPLE 4.2 Testing of downhole safety valves 
Consider a downhole safety valve (DHSV) that is installed as a final element in 
an oil-well. The safety function of the DHSV is to stop the flow in the tubing 
when a hazardous event occurs on its downstream side. The DHSV has two 
main dangerous failure modes: fail to close on command, FTC, and leakage in 
closed position, LCP. Both of these are DU faults. To detect these DU faults, the 
safety function is proof-tested at regular intervals. In a worst-case situation, the 
DHSV has to be closed against a flowing well. This is called slam-shut closure, 
and the valve is exposed to hard stresses. The valve cannot normally withstand 
more than a few slam-shut closures without failing. In this case, the DHSV is 
not proof-tested by slam-shut closure. Instead, the flow is stopped by one or 
more valves on its downstream side, the DHSV is closed against a static well, 

8 2 
TESTING AND MAINTENANCE 
and the DHSV is checked for a possible leakage. The proof test is not fully 
realistic but is considered to be adequate. 
0 
Proof-testing is mainly applied to SIFs that are operated in low-demand mode, but 
may also be used for items in high-demand mode. If a safety-related function in high-
demand or continuous mode is not proof-tested regularly, IEC 62061 recommends 
using the test interval τ = 20 years in quantitative reliability analyses. 
4.2.2 
Partial Proof-Testing 
A partial proof test is a planned test, which is designed to reveal one or more spe-
cific types of DU faults of a channel without significantly disturbing the EUC. Partial 
proof tests are sometimes carried out between full proof tests to improve the relia-
bility of a SIF or as a means to extend the (full) proof test interval. A partial proof 
test is sometimes carried out to reveal failure causes, rather than failure modes (i.e., 
faults). 
Partial Proof Test Coverage. 
The partial proof test coverage is the percentage of 
DU faults that can be revealed by a partial proof test. The reliability improvement 
gained by partial testing is determined by the partial proof test coverage and the 
partial proof test interval. 
Partial Stroke Testing of Valves. A common application of partial proof-testing 
is partial stroke testing (PST) of valves. This is done by partially closing the valve, 
and then returning it to the initial position. The valve movement is so small that the 
impact on the process flow or pressure is negligible, but the valve movement may 
still be sufficient to reveal several dangerous failure causes, such as sticking seals 
and broken signal paths. 
► Remark: The difference between a full proof test and a partial proof test lies in 
the scope of the test. A partial proof test is planned to reveal a specific fraction of 
the possible failure modes or failure causes. A full test is perfect if it reveals all the 
possible DU faults and returns the safety loop in an as-good-as-new condition. A 
partial proof test may also be perfect in the sense that all the requirements for the test 
are fulfilled. Both full and partial proof tests may therefore be perfect or imperfect, 
as illustrated in Figure 4.2. Some analysts do not agree with such a classification and 
say that a partial proof test is just a special case of an imperfect proof test and that 
there is only a semantic difference between the two categories. 
Θ 
4.2.3 Diagnostic Testing 
A diagnostic test is an automatic partial test that uses built-in self-test features to 
detect faults. Dangerous faults detected by a diagnostic test are called dangerous 
detected (DD) faults, and safe faults detected by a diagnostic test are called safe 
detected (SD) faults. When a DD fault is identified, the system can be configured to: 

TESTING 
8 3 
Proof test 
Full 
Partial 
Imperfect 
Perfect 
Imperfect 
Figure 4.2 
Full versus partial proof test. 
1. Raise an alarm, locally at the equipment and in the control room. This option 
requires an operator action to repair the fault or to bring the EUC into a safe 
state. 
2. Initiate an immediate action to bring the EUC to a safe state. 
The reaction to the DD fault should as far as possible be designed to avoid a 
spurious trip of the SIS (see Chapter 12). The time required to reach a safe state is 
an important input variable in quantitative reliability analyses of a SIS and may be 
significantly different for the two options. 
Diagnostic tests can seldom reveal all faults. Typical faults that can be detected 
by diagnostic testing are signal loss, impulse line pluggage, drifted analogue signal, 
signal out of range, and final element in wrong position. Features may also be built-
in to monitor the status, such as output signals from redundant channels, and to 
announce any discrepancy. 
Diagnostic testing requires additional hardware and/or programmed instructions 
and adds complexity to the channels. It is important to verify that the diagnostic 
function itself is not able to interfere adversely with the safety function. The follow-
ing aspects of diagnostic testing are important for the quantitative analysis: 
Diagnostic (test) coverage (DC). Most often, the DC is related to dangerous faults 
and may be written as DCD- The DCD can be expressed as the conditional proba-
bility that a dangerous fault is detected (i.e., becomes a DD fault) by the diagnostic 
test, given that a dangerous (D) fault is present when the diagnostic test is initiated, 
that is 
DCD = Pr(DD fault is revealed | D fault is present) 
This can again be expressed as the mean fraction of all D faults of an item that are 
detected by diagnostic self-testing. 
DCD = ^ 
=
 
A ; D 
(4.2) 
^D 
Λ-DD + A-DU 
The mean fraction of dangerous faults that are not revealed by the diagnostic testing 
is 1 — DCD- High diagnostic coverage usually requires that the channels have em-
bedded software. The DCD for the sensor elements and the logic solvers is therefore 

8 4 
TESTING AND MAINTENANCE 
rather high (often in the range of 50-99%), while the DCD for final elements is of-
ten low (typically < 30%). IEC 61508-2 classifies the diagnostic coverage into four 
categories: 
Category 
Interval 
1 
0% - 60% 
2 
60%-90% 
3 
90% - 99% 
4 
> 99% 
The diagnostic coverage of safe faults, DCg, can be defined as the fraction of all 
safe faults that are detected by diagnostic self-testing, but this concept is not used 
further in this book. The diagnostic coverage is often determined based on a detailed 
FMEDA (see Chapter 3). 
Diagnostic test frequency. 
Diagnostic tests may be run more or less frequently. 
The time interval between the initiation of two consecutive diagnostic tests is called 
the diagnostic test interval, TD, and may range from milliseconds up to several hours. 
Mean time to restore (MTTR). The MTTR is the mean time from when a DD 
failure occurs until the item is restored and functioning again. If the diagnostic test 
interval in non-negligible, the MTTR is the mean time from the DD failure occurs 
until it is detected plus the mean repair time. 
Diagnostic testing is relevant mainly for low-demand systems but can also be ben-
eficial for high-demand systems when the time between demands is at least an order 
of magnitude longer than the time between diagnostic tests. For systems operating 
in continuous mode, diagnostic testing may not give much benefit. 
fl 
EXAMPLE 4.3 Diagnostic Testing of Transmitters 
There are two main types of diagnostic testing of transmitters: 
1. Self-diagnostics. Self-diagnostic coverage is the percentage of transmitter faults 
that can be detected by the transmitter itself. The development of smart trans-
mitters has steadily improved this type of coverage to higher than 90%. 
2. Discrepancy alarms. Discrepancy alarms are deviation alarms between redun-
dant transmitters. When a difference is detected between two or more transmit-
ters , an alarm is sent to the control room and a maintenance action is carried out 
to resolve the discrepancy. In some cases, it is also possible to involve control 
transmitters in the discrepancy checking and thereby increase the coverage. 0 

TESTING 
85 
4.2.4 
Demands Serving as Testing 
For high-demand SIFs, demands occur rather frequently, typically several demands 
per year. Because the system has a high reliability, most of these demands are han-
dled successfully. A demand is real and may be more realistic than a proof test, and 
it has therefore been suggested to think of demands as if they were proof tests. 
NOG-070 (2004) accepts that an actual shutdown (i.e., a handling of a demand) 
may be given credit as a full proof test under the following conditions: 
- The shutdown must document equivalent information as registered during the 
corresponding described proof test. 
- The shutdown must cover all equipment covered by the corresponding described 
proof test; if not, the equipment not covered must be tested separately. 
- The shutdown must occur within the last half of the current test interval. 
4.2.5 Other Classifications of Tests 
A test may be associated with a high number of characteristics, each characteris-
tic indicating something about the scope of the test, the time at which the test is 
performed, or the resources involved in performing the test. We may, for example, 
distinguish between: 
Manual versus Automatic Tests. 
Automatic test: A test that is initiated and executed without human involvement. 
Semi-automatic test: A test that is partly automatic, either in terms of initiation or 
execution, but where human involvement is required to complete the test. 
Manual test: A test where all tasks, including initiation, are executed by humans 
alone. 
Proof tests are typically manual and semi-automatic, whereas diagnostic tests are 
automatic. 
Online versus Offline Tests. We may also distinguish between: 
Online test: A test performed while the EUC is operating and performing its in-
tended functions. 
Offline test: A test performed while the EUC is not operating. The EUC may be 
stopped in order to perform the test, or the test may be performed because the 
EUC is closed down for operational reasons. 
A proof test may be online or offline, depending on the possibility of isolating an 
element for testing. A diagnostic test is normally an online test. The EUC may 
sometimes have specific design features to facilitate testing, as illustrated in Exam-
ple 4.4. 

8 6 
TESTING AND MAINTENANCE 
IV-1 
SDV 
IV-2 
x 
MV 
Figure 4.3 
Valve layout to facilitate testing and repair. 
fl 
EXAMPLE 4.4 Design for testing and repair 
Figure 4.3 shows a typical layout for important valves in a process plant. In 
normal operation, the process is protected by the shutdown valve, SDV, and the 
isolation valves, IV-1 and IV-2, are open and the manual valve is closed. During 
testing and repair of the SDV, the valve is isolated by closing IV-1 and IV-2, 
and the flow is maintained through the manual valve MV. With this architecture, 
the process is not disrupted by the testing and repair of the SDV, but the safety 
function of SDV is only partly maintained, if at all. 
φ 
4.2.6 Test Scheduling 
The proof tests can be scheduled in several ways. Three main strategies are (i) si-
multaneous testing, (ii) sequential testing, and (iii) staggered testing. 
Simultaneous Testing. A simultaneous test tests all the channels of a subsystem 
at the same time. During the test, the EUC is unprotected (by the SIF in question) 
while the test is ongoing. This may be an unacceptable strategy at many plants, 
and simultaneous testing may require that the EUC be shut down while the test is 
ongoing. The downtime may be reduced by allocating additional test resources, such 
that all channels can be tested in parallel. 
Sequential Testing. During a sequential test, the channels are tested one after the 
other. Before the next channel is tested, the prior one is restored to an operating 
state. The advantage is that the SIS retains its ability (even if degraded) to respond 
to demands. 
Staggered Testing. Staggered testing means to test redundant channels at different 
times, but at the same time keeping a constant test interval. Staggered testing has 
similarities to sequential testing where the tests are spread out over the entire test 
interval τ. The point of testing for one channel or a small subset of channels is shifted 
with a time to compared to the other channels(s). The main benefits are improved 
availability of the SIF and reduced probability of CCFs. There are two main reasons 
for the reduced probability of CCFs: 

MAINTENANCE 
8 7 
Test 
Test 
Test 
Test 
Test 
channel 1 
channel 2 
channel 1 
channel 2 
channel 1 
i 
1 
1 
I T - * 
0 
t0 
τ 
T+t„ 
2τ 
Time 
Figure 4.4 
Staggered testing procedure for a subsystem of two channels. 
1. CCF causes may be introduced by the testing crew in the form of miscalibra-
tion of sensing elements, failures to reset equipment, and so on. By staggered 
testing, the likelihood of such errors is much lower inasmuch as the tests of re-
dundant channels are separated in time and often carried out by different testing 
crews. 
2. CCFs have a common cause, but the individual failures may often be spread out 
over an interval (see Chapter 10). Because tests are performed more frequently 
than for simultaneous testing, the likelihood of detecting and repairing a failed 
channel is higher. 
However, staggered testing requires more management, which may have a nega-
tive effect on maintenance and testing costs. 
fl 
EXAMPLE 4.5 Staggered testing of two channels voted loo2 
Figure 4.4 shows a staggered testing procedure for a subsystem with two chan-
nels voted loo2. We start observing the subsystem at time t = 0 when channel 
1 is proof-tested. The next proof test of channel 1 is at time τ. Channel 2 is first 
tested at time to and then again at time τ + in- This procedure is continued such 
that the time interval between the tests of channel 1 and channel 2 is always ίυ· 
If we know the failure rates of the two channels, we may now determine the time 
delay fo that gives the lowest PFDavg of the subsystem (see Chapter 8). When 
the two channels have the same DU failure rate, we can show that the optimal 
delay is in = τ/2. 
Θ 
Some pros and cons related to the three strategies are given in Table 4.1. The 
three test schedules are discussed in detail by Torres-Echeverria et al. (2009b). 
4.3 
Maintenance 
4.3.1 
Single Channel Maintenance 
Detailed maintenance procedures for each channel are usually supplied in the manu-
facturer's safety manual for the equipment. The procedures may be split into preven-
tive maintenance and corrective maintenance. Preventive maintenance may cover: 

88 
TESTING AND MAINTENANCE 
Table 4.1 
Pros and cons of different test schedules. 
Test method 
Pros 
Cons 
Simultaneous test 
Easy to conduct 
Sequential test 
Staggered test 
Partial protection while ongo-
ing 
Improved reliability. 
effect of CCFs 
Reduced 
Unprotected while test is ongoing 
(plant is still running) or loss of pro-
duction (plant shut down while test 
is ongoing) 
More time consuming due to addi-
tional isolation and restoration ac-
tivities. More vulnerable to hu-
man error due to more isolation and 
restoration activities 
More vulnerable to human errors? 
Difficult to implement in prac-
tice, due to the additional resources 
needed to perform? 
- Testing (proof-testing and diagnostic testing) 
- Inspection 
- Cleaning 
- Lubrication 
- Replacement of parts (e.g., batteries, seals in safety valves) 
- Condition monitoring 
- Overhauling 
The corrective maintenance part describes the repair actions that are required to 
restore a failed channel to a functioning state. 
4.3.2 Safety Loop Maintenance 
The maintenance program for a safety loop must be based on the maintenance pro-
cedures for each channel of the loop, but must also cover system aspects, such as 
scheduling of preventive maintenance actions, inhibiting of channels during testing 
and repair, bypassing of process flow during testing of final elements, and so on. 
The maintenance program must include plans and procedures for all testing and 
maintenance activities related to the SIF. The main objective of the maintenance 
program is to keep the performance of the SIF in accordance with the SRS throughout 
the entire life cycle. The maintenance program shall include, but not be limited to 
(NOG-070, 2004): 
- Scheduled proof-testing of the SIFs 

ADDITIONAL READING 
8 9 
- Regular inspections of the system elements to ensure that there is no observable 
deterioration, such as corrosion or mechanical damage, damaged cabling or 
terminations, blockage of detectors, and so on. 
- Scheduled preventive maintenance, such as replacement of batteries, lubrica-
tion, calibration, and so on. 
- Repair of detected faults, with appropriate testing after the repair. 
The time and resources required to perform preventive and corrective maintenance 
is influenced by the maintainability of the channels. 
Maintainability. The probability that a given active maintenance action, for an item 
under given conditions of use can be carried out within a stated time interval, 
when the maintenance is performed under stated conditions and using stated pro-
cedures and resources (IEV 191-13-01) 
4.3.3 
Human Errors and Job Safety 
The likelihood of human errors should be considered in all testing and maintenance 
interventions and supervisory checks should be implemented as required. 
Interventions, shutdowns, and startups of processes are often hazardous events 
and it is therefore important that the hazards are identified and that the operators 
are prepared, for example, through a Job safety analysis. These issues are further 
discussed by Rausand (2011). 
4.4 Additional Reading 
The following titles are recommended for further study related to Chapter 4: 
- ISA-TR84.00.03-2002: Guidance for Testing of Process Sector Safety Instru-
mented Functions /SIF) Implemented as or Within Safety Instrumented Systems 
(SIS). 
- Principles for proof-testing of safety instrumented systems in the chemical in-
dustry (HSE, 2002) 
- Guidelines for Safe and Reliable Instrumented Protective Systems (CCPS, 2007) 
- Application of IEC 61508 and IEC61511 in the Norwegian petroleum industry 
(NOG-070, 2004). This guideline is tailor-made for the offshore petroleum 
industry, but it gives insight that can also be useful within other application 
areas. 
- Test Engineering: A Concise Guide to Cost-Effective Design, Development and 
Manufacture (O'Connor, 2001). This book gives guidance to general testing in 
product design and development and is, as such, partly outside the scope of the 
chapter. The book provides, however, many helpful ideas for the type of testing 
described in this chapter. 


CHAPTER 5 
RELIABILITY QUANTIFICATION 
5.1 
Introduction 
This chapter gives an introduction to the analytical methods that can be used to assess 
the reliability of a SIF. If you have a strong background in reliability theory, you may 
not need to read the whole chapter in detail, but you should go through the chapter to 
refresh the main concepts and become familiar with the terminology used. A basic 
introduction to probability theory is given in Appendix A. 
Several analytical methods are suggested in IEC 61508-6: 
1. Approximation formulas 
2. Reliability block diagrams 
3. Fault tree analysis 
4. Markov approach 
5. Petri net approach 
All these are presented and discussed in this book. The current chapter gives an in-
troduction to methods 2-5. The introduction is general, but the examples are related 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
91 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

9 2 
RELIABILITY QUANTIFICATION 
to reliability assessment of a SIR The objective of the chapter is to establish a thor-
ough background for the specialized approaches that are presented in the following 
chapters. 
The approximation formulas are presented in IEC 61508-6 without any justifi-
cations or explanations and are limited to voted groups with no more than three 
channels. Many reliability engineers have problems understanding these formulas, 
and it is therefore an objective of this book to develop formulas that can be used for 
general voted groups and to carefully explain how they are developed. The formulas 
are based on reliability block diagrams, which are introduced in this chapter. The 
detailed formulas for PFDavg are developed in Chapter 8 and the formulas for PFH 
are developed in Chapter 9. 
Methods 2-5 all provide system models that describe the interactions between the 
voted groups and its channels. The models established as reliability block diagrams 
and fault trees are static, whereas the Markov and Petri net approaches can incorpo-
rate dynamic aspects related to testing and maintenance. 
5.2 
Reliability Block Diagrams 
A reliability block diagram (RBD) illustrates the state of a specified function of a 
system with several items. The diagram is made up of functional blocks that are de-
picted as rectangles or squares and connected by lines. The reliability block diagram 
has a single starting point (a) and a single end point (b), as shown in Figure 5.1. Each 
functional block can have two different states, a functioning state and a failed state. 
A functional block may represent an item or a specific function of an item. When the 
(specific) function of the item is available, we illustrate this by saying that we can 
pass through the functional block. If we can pass through sufficient functional blocks 
to come from (a) to (b), we say that the system is functioning (with respect to the 
specified function). Guidance to reliability block diagram construction and analysis 
is given in IEC 61078 (2006) and more details may be found in Rausand & H0yland 
(2004). 
A reliability block diagram is not a physical layout diagram but rather a logical 
diagram illustrating what is required for the system to function. In a reliability block 
diagram, the same functional block may occur at several places in the diagram. Some 
simple reliability block diagrams are discussed in the following. These are later used 
as elements in building more complex reliability block diagrams. 
5.2.1 
Structure Function 
Consider a system of n distinct items. Each item has two possible states: functioning 
or failed, for / = 1,2,... ,n. The state of item i, for i = 1,2,... ,n, can be 
represented by the state variable 
!
1 if item i is functioning 

RELIABILITY BLOCK DIAGRAMS 
9 3 
(a) 
(b) 
Figure 5.1 Reliability block diagram of a series system. 
(a) 
Sensor 
subsystem 
Logic solver 
subsystem 
Final element 
subsystem 
(b) 
Figure 5.2 
A safety loop modeled as a series system of three subsystems. 
x = (xi, Χ2,. ■ ■, Xn) is called the state vector of the system. 
The state of the system can be described by the binary function 
where 
φ(χ) 
φ(χ) = 
φ(Χι,Χ2,...,Χη) 
S
I 
if the system is functioning 
0 
otherwise 
(5.2) 
(5.3) 
The function φ(χ) is called the structure function of the system and a system is 
sometimes called a structure. 
Series Systems. 
A system that is functioning if and only if all of its n items are 
functioning is called a series system or a series structure (e.g., see Rausand & H0y-
land, 2004). The structure function is 
φ(χ) =χι·χ2···χ„ 
= Y[XÎ 
ί' = 1 
(5.4) 
.,η. The 
and we observe that φ(χ) — 1 if and only if x, = 1 for all i = 1,2,. 
series system is illustrated by a reliability block diagram in Figure 5.1. 
fl 
EXAMPLE 5.1 A safety loop with three subsystems 
Consider a safety loop with three subsystems: (1) sensor subsystem, (2) logic 
solver subsystem, and (3) final element subsystem. Because the corresponding 
SIF depends on the correct functioning of all these subsystems, the safety loop 
can be represented as a series system of the three subsystems, as illustrated in 
Figure 5.2. 
© 
Parallel System. 
A system that is functioning if at least one of its n items is func-
tioning is called a parallel system or a parallel structure (e.g., see Rausand & H0y-
land, 2004). A parallel system with n items may be illustrated by the reliability block 
diagram in Figure 5.3. 

9 4 
RELIABILITY QUANTIFICATION 
(a) 
1 
2 
n 
(b) 
Figure 5.3 
Reliability block diagram of a parallel system. 
Voted Systems and Notation 
In some applications, it is important to stress that the system is functioning (i.e., "good") 
when at least k of its n items are functioning and we therefore write koon:G. In other 
applications, we focus on failures, such that the system fails if at least k of its n items 
fail and write Â:OO«:F. Note that 
A:oon:G is equal to a (n — k + l)oon:F 
This means that: loo2:G = 2oo2:F, 2oo3:G = 2oo3:F, 2oo4:G = 3oo4:F, and so on. 
When writing koon, it is presupposed that we consider a koon:G system. 
The structure function of the parallel system can be written 
n 
φ(χ) = I - (1 - Xl)(l - x2)---(l 
- xn) = 1 -Y\(l 
- Xi) 
(5.5) 
( = 1 
and we observe that φ(χ) = 0 if and only if x, = 0 for all i = 1,2,...,«. The 
structure function φ(χ) is sometimes written φ(χ) = x\ u x2 u ... u xn), where 
II is the symbol for logical OR. The functioning of a parallel structure of two items 
is written as x\ u x2 = 1 — (1 — xi)(l — x2) = x\-\- x2— 
x\x2. 
k-out-of-n Systems. 
A k-out-of-n system is a system of« items where the system 
is functioning if at least k of its n items are functioning. Instead of fc-out-of-n, we 
sometimes write koon. A parallel system with n items is functioning if at least one 
of its items is functioning and is therefore a loon system. A series system of« items 
is only functioning if all its n items are functioning and is therefore a noon system. 
A koon system is sometimes called a koon voted system. 
The structure function of a koon system can be expressed as 
φ{χ) 
1 if Σ"=ι xt > k 
10 otherwise 
(5.6) 
This format of the structure function is suitable for further analysis when the items 
are independent and identical, but when the items are different, we need a detailed 

RELIABILITY BLOCK DIAGRAMS 
9 5 
structure function. We discuss this after having introduced the concepts of minimal 
path set and minimal cut set. 
5.2.2 
Minimal Path Sets 
A minimal path set of a system is defined as follows: 
Minimal path set. A path set is a set of item functions that by functioning secures 
that the system is functioning. A path set is said to be minimal if it cannot be 
reduced without losing its status as a path set. 
Consider a reliability block diagram with end points (a) and (b). If all the items 
in a minimal path set are functioning, we can "go" along the path from (a) to (b) and 
the system is functioning. 
β 
EXAMPLE 5.2 Series system 
A series system with n items is functioning if and only if all its n items are 
functioning, and has therefore a single minimal path set {1,2,..., n}, where i 
denotes item function i, for / = 1,2,..., n. 
Θ 
fl 
EXAMPLE 5.3 Parallel system 
A parallel system of n items is functioning if at least one of its items is func-
tioning. In the parallel system, any combination of one or more items is a path 
set, but because each set can be reduced to a single item and still be a path set, 
the parallel system has n different minimal path sets {1}, {2},..., {«}, where i 
denotes item function i, for i = 1,2,..., η. 
Θ 
ffl EXAMPLE 5.4 ft-out-of-n system 
A koon system is functioning when at least k of its n items are functioning. 
Any combination of at least k items is therefore a path set, but each set can be 
reduced to exactly k items and still be a path set, such that the minimal path sets 
are all different sets of exactly k items. Because k items can be chosen among 
n items in (£) different ways, the number of different minimal path sets is 
In \ 
n\ 
(5.7) 
\kj 
k\{n-k)\ 
Consider a 2oo3 system where the items are numbered 1, 2, and 3. This 
system has Q) — 3 minimal path sets and these are {1,2}, {1,3}, and {2,3}. If 
any two items are functioning, the system is functioning. 
® 

9 6 
RELIABILITY QUANTIFICATION 
5.2.3 
Minimal Cut Sets 
A minimal cut set of a system is defined as follows: 
Minimal cut set. A cut set is a set of item functions that by failing secures that the 
system is failed. A cut set is said to be minimal if it cannot be reduced without 
losing its status as a cut set. 
Again, consider a reliability block diagram with end points (a) and (b). If all the 
items in a minimal cut set are failed, it is not possible to come from (a) to (b) and 
the system is failed. 
fl 
EXAMPLE 5.5 Series system 
A series system fails if at least one of its n items fails and has therefore n 
minimal cut sets {1}, {2},..., {«}, where / denotes item function i, for i = 
1,2,...,«. 
Θ 
A 
EXAMPLE 5.6 Parallel system 
A parallel system of n items only fails when all its items fail and therefore has 
only one minimal cut set {1,2,...,«}, where / denotes item function /, for 
i — 1,2,. ..,n. 
φ 
β 
EXAMPLE 5.7 koon system 
A koon system is functioning when at least k of its n items are functioning and 
fails if at least n—k + l items fail. Any combination of at least n—k + l items 
is therefore a cut set, but each set can be reduced to exactly n—k + l items and 
still be a cut set, such that the minimal cut sets are all different sets of exactly 
n—k + l items. Because n—k + l items can be chosen among n items in 
(„_i+i) different ways, the number of different minimal path sets is 
n\ 
(fi-Jk+ ! ) ! ( * - ! ) ! 
Consider a 2oo3 system where the items are numbered 1, 2, and 3. This system 
has (3_2 . i) = (2) = 3 minimal cut sets, and these are {1,2}, {1,3}, and {2,3}. 
If any two components are failed, the system is failed. 
θ 
5.2.4 
Structure Function from Minimal Path/Cut Sets 
It can be shown (Rausand & H0yland, 2004) that any system can be represented as 
a parallel system of its minimal paths and also as a series system of its minimal cut 

RELIABILITY BLOCK DIAGRAMS 
97 
(a) 
1 
1 
2 
2 
3 
3 
(b) 
Figure 5.4 
Reliability block diagram for a 2oo3 system represented as a parallel system of 
its minimal paths. 
(a) 
1 
2 
1 
3 
2 
3 
(b) 
Figure 5.5 
Reliability block diagram for a 2oo3 system represented as a series system of its 
minimal cuts. 
parallel systems. For many systems, this representation is the easiest way to establish 
the structure function. 
2oo3 System. 
We illustrate the approach by a 2oo3 system. The reliability block 
diagram of a 2oo3 system can be drawn as a parallel system of its three minimal 
paths as shown in Figure 5.4. 
By the approach we introduced in Section 5.2.1, the structure function for the 
2oo3 system can be written as 
φ(χ) 
= X\X2 
U ·Χΐ·Χ3 II ·Χ2*3 = (*1*2 + *1*3 — x\x2xi) 
LI *2*3 
2 
2 
= X\X2 ~\~ -^1^3 — X^X2Xj 
~f~ -^2-^3 — X\X2X3 
1 
2 2 2 
X\X2X-i 
i X\ 
Χ2ΧΊ 
Inasmuch as Xj is a binary variable that can only take the values 0 and 1, we always 
have xf = x, for all integers k, and the structure function can therefore be reduced 
to 
φ(χ) 
= X\X2 + *1*3 + X2Xj, — 2x\X2X3 
(5.8) 
The reliability block diagram of a 2oo3 system can also be represented as a series 
system of the three minimal cuts, as shown in Figure 5.5. The structure function of 
this system can be written as 
φ{χ) = (χι 11 x2) ■ (xi u x3) ■ (x2 11 X3) 
= (Xl + Xl - ^1^2) ■ O l + X3- 
*1*3) · (*2 + Xi- 
X2X3) 
By multiplying and reducing this expression (by using xf = JQ), we obtain 
φ(χ) 
= X\X2 + X1X3 + ^2-^3 — 2X1^2^3 
which is the same expression we got in (5.8) by representing the 2oo3 system as a 
parallel system of the minimal paths. 

RELIABILITY QUANTIFICATION 
PT1 
PT1 
PT1 
PT1 
PT1 
PT1 
LS 
SDV1 
SDV2 
Figure 5.6 
Reliability block diagram for the HIPPS in Example 5.8. 
EXAMPLE 5.8 High-integrity pressure protection system (HIPPS) 
Consider the high-integrity pressure protection system (HIPPS) that is illustrated 
in Figure 1.4. Such a HIPPS can, for example, be installed to protect a subsea 
pipeline from a subsea well cluster to a processing unit, either on a floating plat-
form or on land. The pipeline may be long, often more than 50 kilometers, and it 
becomes very expensive if it has to be rated for full well pressure. When the fluid 
in the pipeline is flowing as normal, the pressure is not very high, but when the 
flow is blocked on the outlet end, for example, due to a shutdown at the process-
ing plant, the pressure increases rapidly. If the flow from the wellhead could be 
stopped immediately when the pressure starts to increase, a significantly cheaper 
pipeline with a lower pressure rating could be used. A HIPPS may be used for 
this purpose. The HIPPS in Figure 1.4 has three pressure transmitters, Pi, P2, 
and P3, which should detect any pressure increase in the pipeline. The pressure 
transmitters are connected to a logic solver with a 2oo3 voting. When at least 
two of the pressure transmitters report increased pressure, the logic solver sends 
a closing signal to the two HIPPS valves, SDVi and SDV2. It is sufficient that 
one of the two HIPPS valves closes to stop the flow. 
The system is functioning if at least two of the three pressure transmitters 
are functioning AND the logic solver is functioning AND at least one of the two 
shutdown valves is functioning. A reliability block diagram for the HIPPS is 
shown in Figure 5.6. 
The structure function for the HIPPS is given by 
φ(χ) 
— (.Χρτΐ·*ΡΤ2 + XPTl-XPTS + *PT2*PT3 ~ 2jCpTl-XPT2*PT3) 
Sensor subsystem 
*LS 
- (*SDV1 + *SDV2 — ^SDV1^SDV2) 
LS subsyst. 
Final element subsystem 
(5.9) 
The structure is a series system of the three subsystems. 

RELIABILITY BLOCK DIAGRAMS 
9 9 
5.2.5 
System Reliability 
In Section 5.2.1, the state variable *,· was considered to be a deterministic quantity 
that was used to describe the state of the system. In this section, we assume that the 
state variables are random variables. In some applications, we also assume that the 
state depends on the time t and write Xi (t). 
Because Xi(t) is a random variable that can only take the values 1 and 0, we 
introduce 
Pr(*,(0 = l) = />i(0 
Pv(Xi(t)=0) 
= 
l-pi(t) 
for i = 1,2,... ,n. We note that 
Pi(t) = Pr(Xj(t) — 1) = Pr(Item i is functioning at time t) 
This means that /?,· (/) is equal to the survivor function Ä, (?) when the item is 
nonrepairable, and is equal to the availability Λ,·(ί) when the item is being repaired 
upon failure. For brevity, we often say that p, (t) is the reliability of item i at time t. 
The mean value of X\ (t) is 
E[Xi(t)] = 0 · Pr(*/(f) = 0) + 1 · Pr(*,(0 = 1) 
= Pr(Xi(t) = l) = pt(t) 
(5.10) 
The reliability of item i at time t can therefore be represented by E[Xj(t)]. This 
result can be used to determine the system reliability, because the mean value has the 
following properties: 
E(X + Y) = E(X) + E(Y) for all random variables X and Y 
E(k-X)=k- 
E(X) for all constants k 
E(X -Y) — E{X)· E(Y) when the random variables X and Y are independent 
The structure function φ(Χ(ί)) is also a binary random variable and we define 
the probabilities 
Ρτ(φ(Χ(ΐ) 
= 1) = 
Ps(t) 
P r ( 0 ( * ( / ) = O ) = 1 - / * ( / ) 
In the same way as for items, ps(t) is called the system reliability at time t and 
can be represented as the mean value Ε[φ(Χ(ί)]. These properties can now be used 
to determine the reliability of systems with independent items. 
ffl EXAMPLE 5.9 Series system 
The structure function of a series system is 
n 
<KX(t)) = Xi(t)-X2(t)-Xn(t) 
= YlXi(t) 
1 = 1 

100 
RELIABILITY QUANTIFICATION 
If all the n items are independent, the system reliability is found as the mean 
value 
ps(t) = Ε[φ(Χ(ί))] 
= 
E[Xi(t))-E(X2(t))-E(X„(t)] 
n 
n 
= X[E[Xi(t)\ = Y[pi{t) 
(5.11) 
1 = 1 
( = 1 
This example shows that the expression for the system reliability has the same 
form as the expression for the structure function. The only thing that is necessary is to 
replace the state variable Xi(t) with the item reliability pt(t), for i = 1,2,...,«. It 
can be shown (e.g., see Rausand & H0yland, 2004) that this rule applies for structure 
functions where: 
- all logical symbols ( u ) have been "removed" from the structure function, and 
- the structure function has been reduced such that it does not contain any powers 
of the form Xi (t)k in the expression. These must be removed by using the rule 
Xt{t)k 
= *,(/) for all Â:. 
It is not necessary to multiply parentheses that do not contain the same variables. 
β 
EXAMPLE 5.10 Parallel system 
The structure function of a parallel system is 
n 
φ(Χ(ί)) = Χι(ΐ)η 
X2(t)u 
··· ii ^ ( ί ) = 1 - Π ( 1 - ^ ( 0 ) 
i = l 
When all the n items are independent, the system reliability is found as the mean 
value: 
ps(t) = E[<KX(t))] 
l-fl(l-*,.(*)) 
< = 
1 
= 1 - Π ( 1 - Λ ( 0 ) 
(5.12) 
ί = 1 
β 
EXAMPLE5.il 
2oo3 system 
The structure function of a 2oo3 system is 
φ(χ) = ΧιΧ2 + XiX3 + A'zA's - 2XiZ2^3 

RELIABILITY BLOCK DIAGRAMS 
101 
When the three items are independent, the system reliability is found as the mean 
value. 
ps(t) = E [X1(t)X2(t) 
+ XiiOX^t) 
+ X2(t)X3(t) 
-2Xl(t)X2(t)X3(t)] 
= pi(t)p2(t) 
+ pi(f)p3(t) 
+ p2(t)p3(t)-2Pl(t)p2(t)p3(t) 
(5.13) 
When all the three items are equal and have the same reliability pt(t) = p(t) 
for / = 1,2,3, the system reliability becomes: 
ps(t) = 3p(t)2-2p(t)3 
(5.14) 
Θ 
EXAMPLE 5.12 koon system with identical items 
Consider a koon system with n identical and independent items with reliability 
p(t). At a certain point of time t, we have the following situation: 
1. n independent items (that can be regarded as n independent trials). 
2. Each "trial" has one out of two possible outcomes: the item is functioning 
or not. 
3. The probability of the outcome, Pr(Functioning) = p(t), is the same for 
all trials/items. 
This is a binomial situation, and the variable N = number of items that are 
functioning is therefore binomially distributed. Because the system is function-
ing if at least k items are functioning, the system reliability is 
n 
ps(t) = Ρτ(Ν >k) = ^Pr(7V = i) 
i=k 
= Σ(")^),'[1-^ί)]""ί 
<
5·
15> 
Θ 
EXAMPLE 5.13 HIPPS (cont.) 
Reconsider the HIPPS in Example 5.8. If we assume that the subsystems and 
channels are independent, the reliability of the HIPPS can be determined from 
the structure function (5.9) as 
Ps(t) = [pPTl(0pPT2(t) 
+ PPTl(t)PFT3(t) 
+ 
PPT2(t)pPT3(t) 
-2ppT1(t)pPT2(t)ppT3(t)] 
■ PLs(t) ■ bsDVl(0 + />SDV2(0 ~ />SDVl(0/>SDV2(0] 

102 
RELIABILITY QUANTIFICATION 
When the three pressure transmitters are of the same type with the same relia-
bility /»ρτ(0 and the two shutdown valves are of the same type with reliability 
PsDv(t)> me system reliability becomes 
ps(t) = [3/>PT(02 - 2/>PT(03] · Phs(t) ■ [2/>SDV(02 - />SDV(02] 
To determine the channel reliabilities, we need to know the failure rate of the 
channels (see Appendix A), the test strategy, and the test intervals. This is further 
discussed in Chapter 8. 
0 
Nonrepairable System. 
Consider a system of n independent and nonrepairable 
items. Being nonrepairable does not necessarily mean that the items cannot be, or 
are not, repaired upon failure. It may just mean that we are only interested in consid-
ering them until a specified type of failure occurs. For nonrepairable items the item 
reliability is equal to the survivor function. 
Pi(t) — Ri(t) — Pr(7} > t) = Pr(Item / is still functioning at time t) 
In many applications the time to failure, 7}, is exponentially distributed with con-
stant failure rate λ,, such that 
Pi(t) = Rt{t) = e~kit 
fl 
EXAMPLE 5.14 Series system 
Consider a series system with n independent items with constant failure rates 
λ i, λ2,..., λ„. The survivor function of the series system is then 
n 
n 
Rs(t) = Y\ Ri(t) = Y[ e"Xit = <?-(Σ?-ι λ')' 
(5.16) 
( = 1 
i = l 
This survivor function can be written Rs(t) — e~*s' withAs = Σ " = 1 λ,·. This 
is recognized as the survivor function of an item with exponentially distributed 
time to failure with failure rate Xs- We have hence shown that a series system 
of independent items with constant failure rates has constant system failure rate 
λ5 = Σ > 
1 = 1 
The mean time to failure of the series system is 
MTTFs = -1- = 
l 
(5.17) 
When all the items are identical with the same failure rate A, = λ, for all i, the 
mean time to failure of the series system is MTTF = Ι/ηλ, which is 1/n'th of 
the MTTF of a single item. 
Θ 

RELIABILITY BLOCK DIAGRAMS 
103 
EXAMPLE 5.15 Parallel system 
Consider a parallel system of n independent items with constant failure rates 
Ai, À2,..., λ„. The survivor function of the series parallel system is 
n 
n 
Rs(t) = l - Π(ΐ - */(0) = i - Π 0 -*"λ") 
(5·18) 
i = i 
; = i 
This expression cannot be written in the form e~ks' and we can conclude that 
the failure rate of a parallel system does not have a constant failure rate even if 
all its items have constant failure rates. 
For a parallel system of two items, the survivor function can be written 
/?s(r) = 
l - ( l - e - À " ) - ( l - e - A 2 r ) 
= £>"λι' +β~λ2' 
- Ε Γ ^ Ι + ^ ) ' 
(5.19) 
The mean time to failure of this parallel system with two items is 
MTTFs = -L + -L - 
| 
(5.20) 
Ai 
A 2 
À1 + Λ.2 
For a parallel system of two identical items with failure rate A, we get 
Rs(t) = 2e-kt -e~2Xt 
(5.21) 
and the MTTFs becomes 
MTTFs = I-2H-X
 
( 5 ·
2 2 ) 
This means that introducing redundancy to an item with constant failure rate 
increases the item's MTTF by 50%. 
Θ 
EXAMPLE 5.16 2oo3 system 
Consider a 2oo3 system with three independent, identical, and nonrepairable 
items with constant failure rate A. The survivor function of the 2oo3 system is 
Rs(t) = 3e~2Xt - 2e~3Xt 
(5.23) 
The mean time to failure of the 2oo3 system is 
Μττρ*=έ-ΙΗ·ι
 
(5·
24) 
This means that the mean time to failure of a 2oo3 system is shorter than the 
mean time to failure of a single item. 
© 

104 
RELIABILITY QUANTIFICATION 
) 
1 
2 
3 
(b) 
Figure 5.7 
Reliability block diagram of a simple system. 
Repairable System. 
In Section A.5.1 of Appendix A the concepts of availability 
and average availability are introduced as reliability measures for repairable items. 
As for nonrepairable items, we may introduce pi(t) = Ai(t) for i — 1,2,... ,n 
where Ai (/) is the availability of item i at time t, and introduce these into the struc-
ture function. 
The same approach can also be used when considering average availabilities. 
fl 
EXAMPLE 5.17 Average system availability 
Consider the system illustrated by the reliability block diagram in Figure 5.7, 
with three independent items with average availabilities Αχ,Α2, and A3. The 
structure function is 
φ{Χ) = X1(X2 11 X3) = X1W2 + *3 - *2* 3) 
The average system availability is 
As = Ai(A2 + A3 - 
A2A3) 
Θ 
This approach can be used as long as the items are independent. This assump-
tion is, however, often not realistic inasmuch as it implies that all the other items 
operate as normal irrespective of whether one or more items are being taken out of 
service for repair. The approach in Example 5.17 may be sufficiently accurate as an 
approximation, but a more advanced approach, such as the Markov approach, may 
be needed when more accuracy is required. 
Repairable Series System. 
Consider a series system of two independent and re-
pairable items with failure rates λ\ and λ2, respectively. The failure rate As of the 
system is, from Example 5.14, As = λι + λ2. 
When one of the items fails, the system is "down" and will be repaired. If item 
1 fails (first), the mean downtime is MDTi, and if item 2 fails, the mean downtime 
is MDT2. Because the items are independent, they cannot fail at the same time. We 
further assume that when an item fails, the other item cannot fail during the downtime 
caused by the first failure. 

FAULT TREE ANALYSIS 
1 0 5 
Let Ti denote the time to failure of item i (i = 1,2). The probability that item 1 
fails first is 
/•OO 
PT(T2 > 7\) - 
/ 
ΡΓ(Γ 2 > r | 7Ί = f)/r, (0 dt 
Jo 
/•OO 
ΛΟΟ 
= / 
e-X2,Xie-Xlt 
dt = λι / 
e-^+^'dt 
Jo 
Jo 
λ ι 
(5.25) 
Al + λ2 
If component / fails first, the mean system downtime is MDT, (i = 1,2). The 
mean system downtime is, therefore, by the law of total probability 
MDTs = . 
,'. MDTi + 
, 2. MDT2 
(5.26) 
Ai + A2 
Ai + A2 
The series system can therefore be regarded as a "super item" with failure rate As 
and mean downtime MDTs, and we can determine the average unavailability from 
(A.47) as 
iavg % As · MDTg 
(5.27) 
This approach can easily be extended to any series system of independent compo-
nents. 
5.3 
Fault Tree Analysis 
5.3.1 
Introduction 
A fault tree is a top-down logic diagram that displays the interrelationships between 
a potential critical event in a system and the causes of this event. The causes at the 
lowest level are called basic events and may be component failures, environmental 
conditions, human errors, and normal events (i.e., events that are expected to occur 
during the life span of the system). 
Fault tree analysis (FTA) was introduced in 1962 at Bell Telephone Laboratories, 
in connection with a safety evaluation of the Minuteman intercontinental ballistic 
missile launch control system. An FTA may be qualitative, quantitative, or both, 
depending on the scope of the analysis. The main objectives of an FTA are: 
(a) To identify all possible combinations of basic events that may result in a critical 
event in the system. 
(b) To find the probability that the critical event occurs during a specified time 
interval or at a specified time t, or the frequency of the critical event. 
(c) To identify aspects (e.g., components, safety barriers, structure) of the system 
that need to be improved to reduce the probability of the critical event. 

106 
RELIABILITY QUANTIFICATION 
FTA has traditionally been applied to mechanical and electromechanical systems, 
but there is no fundamental reason why the methodology cannot be applied to any 
type of system. The standard IEC 61025 (2006), Fault Tree Analysis (FTA) defines 
the symbols used in FTA and gives guidance on how to construct and to perform 
qualitative and quantitative analyses of the fault tree. 
5.3.2 
Fault Tree Diagram 
FTA is a deductive method, which means that we reason backward in the causal 
sequence of a specific event. We start with a specified potential critical event in 
the system, called the TOP event of the fault tree. The immediate causal events 
E\,E2,..- 
that, either alone or in combination lead to the TOP event, are identified 
and connected to the TOP event through a logic gate (see Table 5.1). Next, all the 
potential causal events £,·,!, £,·>2,... that may lead to event £,· for / = 1,2,... are 
identified and connected to event £,· through a logic gate. The procedure is continued 
deductively until a suitable level of detail is reached. The events at the lowest level 
are called the basic events of the fault tree. 
Table 5.1 shows the most commonly used fault tree symbols together with a brief 
description of their interpretation. A number of more advanced fault tree symbols 
are available but are not covered in this book. A thorough description may be found, 
for example, in Stamatelatos et al. (2002). 
FTA is a binary analysis. All events, from the TOP event down to the basic events, 
are assumed to be binary events that either occur or do not occur. No intermedi-
ate states (e.g., the state is 80% good) are therefore allowed in the fault tree. The 
fault tree diagram is a deterministic model. This means that when the fault tree is 
constructed and we know the states of all the basic events, the TOP event and the 
states of all intermediate events are known. A fault tree is single eveni-oriented, and 
a separate fault tree must therefore be constructed for each potential TOP event in the 
system. 
β 
EXAMPLE 5.18 HIPPS (Cont.) 
Reconsider the HIPPS in Example 5.8 and assume that the production is running 
as normal when an emergency shutdown (ESD) is suddenly initiated in the pro-
cess system. As part of this ESD, the pipeline is closed by a shutdown valve and 
the pressure in the pipeline therefore increases rapidly. The sudden blockage of 
the pipeline is a demand for the safety function of the HIPPS, which is a SIS. 
The safety function of the HIPPS is to close the flow from the wellhead when 
a demand occurs. Performance requirements for the safety function are given 
with respect to the closing time of the HIPPS valves and the acceptable leakage 
through closed valves. 
If the flow from the wellhead is not stopped fast enough, the pressure in 
the pipeline will increase beyond the strength of the pipeline, and the pipeline 
will rupture, leading to a long production stop, expensive repair, and significant 
pollution. 

FAULT TREE ANALYSIS 
107 
Table 5.1 
Fault tree symbols (Reproduced from Rausand (2011 ) with permission from John 
Wiley & Sons). 
Symbol 
Description 
Logic gates 
OR-gate 
E, 
E2 E3 
AND-gate 
E, 
E2 E3 
The OR-gate indicates that the output 
event A occurs if any of the input events 
Ej occur 
The AND-gate indicates that the output 
event A occurs only when all the input 
events Ej occur at the same time 
Input events 
Basic event 
u 
The basic event represents a basic 
equipment failure that requires no fur-
ther development of failure causes 
Undeveloped event 
Description 
Transfer symbols 
Comment rectangle 
Transfer-out 
À 
Transfer-in 
ZV 
The undeveloped event represents an 
event that is not examined further be-
cause information is unavailable or be-
cause its consequence is insignificant 
The comment rectangle is for supple-
mentary information 
The transfer-out symbol indicates that 
the fault tree is developed further at 
the occurrence of the corresponding 
transfer-in symbol 

108 
RELIABILITY QUANTIFICATION 
Critical high pressure 
in pipeline when 
outlet blocked 
- OR-gate 
No signal about high 
Logic solver does not 
pressure from the 
pressure transmitters 
Transfer symbol 
transmit signal 
about high pressure 
Shutdown valves 
Figure 5.8 
Fault tree for the HIPPS in Example 5.18. 
We want to analyze the reliability of the HIPPS by fault tree analysis, and 
the first task is therefore to define the TOP event. If the safety function of the 
HIPPS fails, the following event sequence may occur: "critical high pressure in 
the pipeline" =>■ "pipeline ruptures" =>■ "oil/gas pollutes the sea." It is normally 
recommended to choose the first event in such a sequence as TOP event, and 
therefore: 
TOP event = Critical high pressure in pipeline 
A fault tree for this TOP event is illustrated in Figure 5.8 
The TOP structure of the fault tree corresponds to the modules of the SIS and 
the TOP event occurs if "the sensor subsystem fails to detect and signal increased 
pressure" OR "the logic solver subsystem fails to transmit the signal" OR "the 
final element subsystem fails to stop the flow in the pipeline." Each input event 
in the TOP structure are next analyzed separately, and Figure 5.8 shows the next-
level fault tree for the final element subsystem. The logic solver subsystem is 
not analyzed any further in this example and is presented as a basic event. 
When space is not available to draw the whole fault tree on the same page, we 
can use a transfer symbol and continue on a new page, as shown for the sensor 
(i.e., pressure transmitter) subsystem in Figure 5.9. 
The fault tree for a koon voting may be large and cumbersome, as illustrated 
in Figure 5.9. Many computer programs for fault tree analysis have therefore 
implemented a koon voting gate, as illustrated in Figure 5.10. The fault tree for 
the pressure transmitter subsystem is then drawn as shown in Figure 5.10, and 
transformed to the fault tree in Figure 5.9 internally in the fault tree program as 
part of the quantitative analysis. 
φ 

FAULT TREE ANALYSIS 
109 
No signal about high 
pressure from the 
pressure transmitters 
Figure 5.9 
Fault tree for the pressure transmitter subsystem in Example 5.18. 
No signal about high 
pressure from the 
pressure transmitters 
PT, fails to 
signal high pressure 
PTa fails to 
signal high pressure 
PT3 fails to 
signal high pressure 
Figure 5.10 
Alternative fault tree for the pressure transmitter subsystem in Example 5.IS 
using a 2oo3 voted OR-gate. 
Remark: Two issues should be noted in relation to the fault tree in Example 5.18: 
- A HIPPS valve fails when it is not able to function according to its performance 
requirements and can have several failure modes, such as "fail to close," "closes 
too slowly," and "leaks in closed position." The consequences of these failure 
modes may be quite different, and it is therefore important to consider carefully 
which failure mode to include in the fault tree analysis. 
- When we, in general, talk about a koon voting, we usually think in terms of 
function: a koon system is functioning when at least k of its n channels are 
functioning. In a fault tree, we focus on failures and the output event of a k/n 
OR-gate occurs when at least k of its n input events occur. In Figure 5.9, the 

110 
RELIABILITY QUANTIFICATION 
output event "No signal about high pressure from the pressure transmitters" 
occurs when at least two of the three pressure transmitters have failed to sig-
nal high pressure. As mentioned earlier in this chapter, we sometimes write 
k oon: G to make it clear that we think in terms of function [good (G)]; and 
koon:¥ when we think in terms of failure (F). Note that a koon:G is the same 
as a (n — k + l)oon:F voting. For the pressure transmitters in Example 5.8, the 
voting is "symmetric" in the sense that a 2oo3:G is the same as a 2oo3:F. 
The failure to distinguish A:oon:G from koon:F is a common error and the au-
thor has seen several practical fault tree analyses where this error has been done. 
Θ 
Relationship with Reliability Block Diagrams. A fault tree diagram (with only 
AND- and OR-gates) can always be converted to a reliability block diagram, and vice 
versa. This is illustrated in Figure 5.11. 
The reliability block diagram in Figure 5.11 (i) represents a series system that fails 
if item 1 fails, OR item 2 fails, OR item 3 fails. A series system always corresponds 
to an OR-gate in the fault tree when the basic events represent item failure. 
The reliability block diagram in Figure 5.11 (ii) is a parallel system that fails only 
when item 1 fails AND item 2 AND item 3 fails. It is therefore clear that the parallel 
system corresponds to an AND-gate. 
Note that to save space, we have omitted the rectangles describing the basic events 
in the fault trees in Figure 5.11. In practical applications, we should always give 
proper descriptions of the events in the fault tree. By comparing the fault tree of the 
HIPPS in Figures 5.8-5.10 with the reliability block diagram in Figure 5.6, we see 
that the reliability block diagram can be obtained from the fault tree and vice versa. 
Note that the blocks (i.e., the rectangles) in the reliability block diagram in Figure 5.6 
are said to be "functioning" when the corresponding basic events in the fault tree do 
not occur. 
Whether you establish the system model by a fault tree or a reliability block di-
agram will, in most cases, give the same result. The way of thinking when you 
establish the system model is, however, different for the two methods. When using 
a reliability block diagram, you repeatedly ask, "How can this subsystem or system 
function?" When using a fault tree, you repeatedly ask, "How can this subsystem 
or system fail?" By focusing on functions, you may tend to forget safety functions 
that are not required to perform the main system functions. Experience has shown 
that the failure focus of fault trees often gives a more complete system model. It is 
therefore recommended to always establish the system model by a fault tree. If you 
prefer to use reliability block diagrams in the further analysis, you may convert the 
fault tree to a reliability block diagram. 
5.3.3 Minimal Cut Sets 
In the same way as for reliability block diagrams, we can also define cut sets and 
path sets in a fault tree. Of these, the cut sets are the most important. 

FAULT TREE ANALYSIS 
111 
(i) 
TOP 
""H 
■ 
1 
Ô0Ô 
(ii) 
TOP 
d)0(b 
('") 
TOP 
o 
" 
^ 
2 
3 
1 
3 
i 
? 
3 
■ 
Figure 5.11 
Relationship between some simple fault trees and reliability block diagrams 
(Reproduced from Rausand (2011 ) with permission from John Wiley & Sons). 
Cut set. A cut set in a fault tree is a set of basic events whose (simultaneous) occur-
rence ensures that the TOP event occurs. 
The most interesting cut sets are those that are minimal: 
Minimal cut set. A cut set is said to be minimal if the set cannot be reduced without 
losing its status as a cut set. 
Let C\, C2, ■ ■ ■, Ck denote the k minimal cut sets of a fault tree. The number of 
distinct basic events in a minimal cut set is called the order of the cut set. A minimal 
cut set of order 1 has only one basic event, and the TOP event occurs as soon as this 
basic event occurs. A minimal cut set of order 2 has two basic events and both events 
have to occur for the TOP event to occur. It is obvious that minimal cut sets with a 
low order are more important than minimal cut sets with a high order. 

112 
RELIABILITY QUANTIFICATION 
Figure 5.12 
The TOP event occurs if at least one of the k minimal cut sets fails. 
A minimal cut set is said to fail when all the basic events of this cut set are occur-
ring at the same time.1 A minimal cut set can therefore be represented as a fault tree 
with a single AND-gate, as illustrated in Figure 5.11 (ii). In a reliability block dia-
gram, a minimal cut set can be represented as a single parallel system with r items, 
where r is the order of the minimal cut set. All the r items in this parallel system 
have to fail for the minimal cut set to fail. 
Let Cj(t) be the event where minimal cut set Cj is failed at time /, for j = 
1,2,... ,k. The TOP event occurs at time t when at least one of the minimal cut sets 
fails at t, and can therefore be expressed as 
TOP (0 = Ci (i) U C 2(0 U · ■ · U Ck(t) 
(5.28) 
The fault tree can therefore be represented by an alternative top structure, where the 
minimal cut set fault trees are connected through a single OR-gate, as illustrated in 
Figure 5.12. 
To save space, the rectangles describing the basic events are omitted in Fig-
ure 5.12. Each minimal cut set is drawn here with three basic events. The basic 
events in minimal cut set j are illustrated by the symbols j . \ , j.2, and j.3, for 
j = 1,2,... ,k. In a real fault tree, the minimal cuts sets will be of different or-
ders and the same basic event may be a member of several minimal cut sets. 
Identification of Minimal Cut Sets. 
The minimal cut sets of the HIPPS fault tree 
in Example 5.8 can, for example, be easily determined from the reliability block 
The term occurring may be somewhat misleading. The term does not imply that a basic event occurs 
exactly at time t ; it means that the state of the basic event is present at time t (e.g., a component is in a 
failed state at time t). 

FAULT TREE ANALYSIS 
113 
TOP 
2 
Figure 5.13 
Fault tree with a single AND-gate. 
diagram in Figure 5.6 to be 
Ci = {PT1,PT2} 
C2 = {PT1,PT3} 
C3 = {PT2, PT3} 
C4 = {LS} 
C5 = {SDV1,SDV2} 
This approach is, however, not feasible for large fault trees and an efficient al-
gorithm is therefore needed. Most computer programs for fault tree analysis have 
implemented effective algorithms to find the minimal cut sets. Many of these algo-
rithms are based on MOCUS (method for obtaining cut sets), which is described in 
Rausand & H0yland (2004). 
5.3.4 
Quantitative Fault Tree Analysis 
Formulas for quantitative FTA are developed in this section. 
Fault Tree with a Single AND-gate. Consider a fault tree with a single AND-gate, 
as illustrated in Figure 5.13. 
Let E,(t) denote that the event £, is occurring at time t, for i = 1,2,...,«. As 
the TOP event occurs if and only if all the basic events occur, the Boolean represen-
tation of the fault tree is 
TOP (t) = Ex (t) Π E2(t) Π · · · Π E„ (t) 
(5.29) 
The probability that the event is occurring at time t is denoted 
qi(t) = Pv[Ei(t)] 
If event Ei is a component failure, then qt(t) is the unreliability or unavailability of 
the component. We assume that the events E\ (t), E2(t),..., 
E„(t) are independent. 
The probability of the TOP event at time t, Qs{t), is then 

114 
RELIABILITY QUANTIFICATION 
TOP 
Ü 
W ö) ^J 
Figure 5.14 
Fault tree with single OR-gate. 
Q,(t) = Pr[£i(f) Π E2(t) n ··· Π En{t)\ 
= Pr[£ 1(0)-Pr(£ 2(0)-Pr(£n(0] 
= ίι(0·92(0···?ιι(0 
=n«--w 
/ = ! 
(5.30) 
Fau/f Tree w/f/i a Single OR-gate. Consider a fault tree with a single OR-gate, as 
illustrated in Figure 5.14. 
In this case, any of the basic events will cause the TOP event to occur, and the 
Boolean representation is 
TOP (0 = Ex(t) U E2(t) U ··· U En(t) 
(5.31) 
When all the events E\(t), E2{t),... 
,En(t) are independent, the probability of 
the TOP event at time t is 
Qs(t) = Pr[£i(i) U E2(t) U ··· U En(t)] 
= i-Pr[£*(r)n£2*(i)n...nis„*(i)] 
= 1-([1 - 9i (0Η1-?2(01-[1-?»(')]) 
n 
= ι-Π[1-^(/)ί 
i' = l 
(5.32) 
Input Data. The basic events in a fault tree can usually be classified into five cate-
gories: 
1. Failure of a nonrepairable technical item 
2. Failure of a repairable technical item (that is repaired when a failure occurs) 
3. Failure of a periodically tested technical item (i.e., an item with hidden failures 
that are only detected through periodic testing) 

FAULT TREE ANALYSIS 
115 
4. Events that occur with a specific frequency (e.g., natural events such as light-
ning, storm, and flooding) 
5. Events that occur in a specific situation, often called on-demand events (e.g., 
human error, ignition) 
1. Nonrepairable Item. Consider the basic event £,· (?) = "The nonrepairable item 
/ is in a failed state at time ?." The item is assumed to have a constant failure rate A, 
and is put into operation at time t = 0. The probability of the basic event at time t is 
then 
qi(t) = Pr[Ei(t)] = l-e-^t 
(5.33) 
When λ, is "small," the following approximation is often used: qi(f) «s λ,ί. 
More details may be found in Chapter 8. 
2. Repairable Item. Consider the basic event £,·(ί) = "The repairable item i is 
in a failed state at time /." Let MTTF,· be the mean time to failure of the item and 
MTTR, be the mean downtime after a failure. The probability of the basic event at 
time / is then 
qi{t) ^ qi = ΜΤ^Γ^ττκ,
 
( 5 ·
3 4 ) 
qi{t) is the unavailability of item i at time t. The unavailability approaches the 
average unavailability g,· when the time / increases, and we therefore often use qi 
instead of qi{t). In (5.34) we assume that failure of item i is detected immediately 
and that repair action is initiated. After a repair action is completed, the item is "as 
good as new," and there are therefore no trends in either MTTF, or MTTR,. 
Let λ, be the constant failure rate of the item, such that MTTF, = l/λ,. Because 
MTTR, « MTTF,·, the unavailability may be written as 
MTTR 
Qt = , , Γ Τ ^ 
T7—ϊΓ- % A,MTTR,· 
(5.35) 
H 
MTTF,· + MTTR,· 
K 
' 
3. Periodically Tested Item. Consider the basic event £,·(ί) = "The periodically 
tested item i is in a failed state at time t." In this case, item i may have an unde-
tected failure that is revealed only by proof testing. The time interval between two 
consecutive proof tests is denoted r and the constant failure rate with respect to the 
undetected failure mode is λ,·. After a proof test, the item is considered to be as-
good-as-new. If we can consider the testing time and the repair time (if required) 
to be negligible compared with the proof test interval, the basic event probability is 
approximately constant 
qi « ~y 
(5.36) 
Assume now that the function of the item is unavailable during an average repair 
time, MRT,, that is not negligible. The repair time needs to be included with proba-
bility 
Pr("Item is found in failed state in the test") = 1 — ε~λ'τ ?» λ,τ 

116 
RELIABILITY QUANTIFICATION 
In this case, the basic event probability is 
λ,·τ 
λ,-rMRT,· 
λ,-r 
?/(0 % 4- + 
L = 4 - + λ,-MRT; 
(5.37) 
2 
τ 
2 
Periodically tested items are discussed in detail in Chapter 8. 
4. Frequency. This category is used for events that occur from time to time, but 
with no duration. Although the probability of the basic event i at time t is qi (t ) = 0, 
the frequency of the event can be given by υ,. 
► Remark: An event with duration may be treated as a repairable item, where the 
failure rate is the frequency of the event and the repair time is the duration. 
φ 
5. On-Demand Probability. Consider the basic event £,-(i) = "Basic event / oc-
curs at time f." This category is used to describe events that may occur in a specific 
context, for example, "operator fails to activate manual shutdown system" and "re-
leased gas is ignited." The probability of the basic event is usually assumed to be 
independent of the time t. 
qi{t)=qt 
(5.38) 
TOP Event Probability. As indicated in (5.28) and Figure 5.12, any fault tree di-
agram can be represented as an alternative fault tree diagram with a single OR-gate 
with all the minimal cut set failures as input events. From (5.28), the probability, 
Qo(0 of the TOP event at time t can be written 
0o(O - Pr[TOP(r)] - Pr [d(t) U C2(t) U · ·· U Ck(t)] 
(5.39) 
where Cj(t) is the probability that minimal cut set j is failed at time t, for j = 
1,2,..., k. Minimal cut set j fails at time t when all the basic events Ejj in Cj 
occur at time t. The minimal cut set failure, C/(i), can therefore be represented as 
a fault tree with a single AND-gate. The probability that minimal cut set Cj fails at 
time t is denoted Qj (t). If all the basic events in minimal cut set Cj are independent, 
we get from (5.30) 
Qj (r) = ΡΓ[£ Λ 1 (0 n EJi2(t) n · · · n £,·,„, (*)] 
= π ««■(')
 
( 5·
4 0 ) 
ieCj 
where nj is the number of basic events in minimal cut set Cj, for j = 1,2,... 
,k. 
If all the minimal cut sets were independent, we could use (5.32) to determine the 
probability of the TOP event at time t as 
k 
ßo(0 = 1 - Π t1 - &<')] 

FAULT TREE ANALYSIS 
117 
A basic event is, however, often a member of several minimal cut sets, and the min-
imal cut sets are, therefore, generally not independent. This type of dependency 
gives a positive association2 between the minimal cut sets, and we can deduce the 
following approximation: 
k 
ôo(0 ^ i - Π t1 - &(')] 
<5·41) 
7=1 
This formula is called the upper bound approximation formula and is used by 
most of the fault tree analysis programs. Using the right-hand side of equation 
(5.41) generally gives an adequate approximation. The approximation is conser-
vative, meaning that the TOP event probability, Qo(t), is slightly less than the value 
calculated. 
For hand-calculation, the following formula may be used 
k 
Qo(t)%YiQj(t) 
(5.42) 
7 = 1 
It is obtained from (5.41) by neglecting simultaneous failures of two or more 
minimal cut sets. The probability of simultaneous failures is often very small such 
that the approximation may be adequate. 
The approximation (5.42) is seen to be more conservative than the upper bound 
approximation because 
Qo(t)<l-Y\[\-Qj(t)]<J2Qj(t) 
7 = 1 
7 = 1 
Binary Decision Diagrams. In this section, we have used minimal cut sets and 
Boolean logic to determine the TOP event probability in fault trees. Another, and 
more powerful approach is to use Binary decision diagrams (BDDs). The BDD ap-
proach requires more mathematical formalism and is considered to be more difficult 
to understand and few practical reliability analysts are, so far, familiar with the BDD 
approach. The author has therefore chosen not to present BDDs in this book. Some 
computer programs for fault tree analysis, such as GRIF Workshop,3 are performing 
quantitative fault tree analysis based on BDDs. 
5.3.5 
Sensitivity Analysis 
A sensitivity analysis (see Chapter 16) is carried out to determine how much the TOP 
event probability changes when one or more input parameters are changed. Ques-
tions of interest may, for example, concern the effect on the TOP event probability 
if: 
2Association is discussed further in Chapter 6 of Rausand & H0yland (2004). 
3See h t t p : / / g r i f - w o r k s h o p . c o m 

118 
RELIABILITY QUANTIFICATION 
- All failure rates of a special group of components were 50% higher than the 
nominal value 
- The test interval for a set of detectors was increased from 3 months to 6 months. 
In some cases, it may be of interest to see how uncertainties in the input parame-
ters influence the TOP event probability. Such an analysis is referred to as uncertainty 
or error propagation, and is usually carried out by Monte Carlo simulation. This is 
discussed further in Chapter 16. Most of the fault tree programs have a module for 
sensitivity analysis and/or uncertainty/error propagation. 
5.3.6 
Importance of Basic Events 
Several importance measures have been developed to measure the relative impor-
tance of a basic event, in comparison to other basic events, with respect to the TOP 
event probability. The importance of a basic event has two sources: (i) the prob-
ability of the basic event, and (ii) where the basic event is placed in the fault tree 
diagram. 
Birnbaum's Measure. 
Birnbaum (1969) proposed the following measure of the 
importance of basic event / in a fault tree: 
''<''"> = ΐ π £
 
(5·
43) 
for i = 1,2,... ,n. 
Birnbaum's measure is thus obtained by partial differentiation of the TOP event 
probability (2o(0 with respect to q, (t) and represents a classical sensitivity measure. 
If IB(i | /) is large, a small change in #,■(?) leads to a comparatively large change in 
the TOP event probability Qo(t). 
Let Q0(t | Ei) and Q0(t | E*) be the conditional probability of the TOP event 
when it is known that basic event i has occurred (Ei) and not occurred (£*), re-
spectively. We can now show (e.g., see Rausand & H0yland, 2004, Sec. 5.2) that 
Birnbaum's measure can be expressed as 
IB(i \ t) = Q0(t \ Ei) - Q0(t \ E*) 
(5.44) 
This formula is used by many fault tree programs to calculate Birnbaum's measure. 
First, the TOP event probability Qo(t | £,·) is calculated by assuming that basic event 
/ has occurred. Then, the TOP event probability Qo(t \ E*) is calculated when it is 
assumed that the basic event has not occurred. Compared to taking the derivative of 
Qo(t), the two calculations of the TOP event probability are usually performed much 
faster by a computer. 
Birnbaum's measure has another important feature, as IB(i | t) also is the prob-
ability that basic event / is critical for the fault tree (i.e., for the system) at time /. A 
basic event is said to be critical for the fault tree if all the other basic events are in 
such states that "everything" depends on basic event i, meaning that the TOP event if 

THE BETA-FACTOR MODEL 
119 
and only if basic event i occurs. This feature is derived and explained in Rausand & 
H0yland (2004). 
Fussell-Vesely's Measure. 
J.B. Fussell and W. Vesely suggested the following 
measure of the importance of basic event i in a fault tree: 
/ F V(/ | i) = Pr(at least one minimal cut set that contains basic event i is failed 
at time / | TOP event occurs at time t) 
(5.45) 
Let Cj denote the minimal cut set j that contains the basic event i. The probabil-
ity that this cut set fails at time t is denoted 
ßi-(0 = Pr(CJ(/)) 
(5.46) 
Fussell-Vesely's measure can now be expressed as (e.g., see Rausand & H0yland, 
2004, Sec. 5.7) 
ι-Π7ίι [i-Ö>(0] 
1 
('' I ') * 
Π(Λ
 
( 5 ·
4 7 ) 
Uov) 
where m, is the number of minimal cut sets that contain basic event i, for i = 
1,2,... ,n. The main strength of Fussell-Vesely's measure is that it can be calculated 
very efficiently. 
Application of Importance Measures. 
Very often, it is found that relatively few 
events contribute significantly to the TOP event probability. It is also commonly seen 
that the events cluster in groups that differ by orders of magnitude from one another. 
In these cases, their importance is so dramatically different that they are generally not 
dependent on the preciseness of the data used to calculate the TOP event probability 
(Stamatelatos et al., 2002). 
The main benefits of the importance measures are that they can: 
- Identify basic events with the greatest need to be improved, maintained, or con-
trolled. 
- Identify the basic events for which we need to obtain high-quality data. A basic 
event with low importance has a very low influence on the TOP event probability. 
Spending resources to get accurate data for such events may thus be a waste 
of money. A relevant approach is therefore first to calculate the TOP event 
probability and one or more importance measures based on approximate input 
parameters, and then concentrate the data acquisition resources on the most 
important basic events. 
5.4 The Beta-Factor Model 
A common-cause failure (CCF) is a failure of two or more items due to a single 
specific event or cause and within a specified time interval. A simple model, called 

120 
RELIABILITY QUANTIFICATION 
the beta-factor model was introduced by Fleming (1975) to incorporate CCFs into 
reliability models. The idea of the beta-factor model is to split the failure rate λ of 
an item in two parts: 
λ ^ - the rate of individual failures, i.e., failures that affect only the specific 
item and 
λ^ 
- the rate of failures that affect all the items in a voted group, that is, the 
rate of CCFs, 
such that λ = λν) 
+X(c). 
The parameter β is introduced as the fraction of all failures of an item that are 
CCFs. 
This means that we can express λ ^ and λ ^ as 
λ<''> =(1 - β)λ 
A(c) =βλ 
The beta-factor, β, can also be interpreted as a conditional probability: When a 
failure of the item is observed, β is the probability that this failure in fact is a CCF. 
A consequence of the beta-factor model is that any failure is either an individual 
failure affecting a single item, or a CCF affecting all the items of the voted group. 
When a failure occurs in, for example, in a 2oo4 voted group, it is either a single 
item failure or a failure affecting all the four items. A CCF affecting two or three 
items is not permitted when using the beta-factor model. 
CCFs are very important in reliability analyses of SIFs and are treated in detail in 
Chapter 10. 
5.5 
Markov Approach 
5.5.1 
Introduction 
The Markov approach4 is one of the recommended approaches in IEC 61508-6 for 
reliability assessment of a SIF. Guidance on the application of the Markov approach 
in reliability engineering is given in the standard IEC 61165. 
5.5.2 
Basic Concepts 
The Markov approach is used to analyze how the state of a system may change with 
time. The Markov approach described in this section requires the system to have a 
Named after the Russian statistician Andrey Markov (1856-1922). 

MARKOV APPROACH 
121 
Table 5.2 
Possible states of a single channel. 
State 
State description 
0 
The channel is functioning as specified 
1 
The channel has a safe fault 
2 
The channel has a dangerous fault 
finite number of discrete states, the time must be continuous, and the process must 
fulfill the Markov property. The collection of all possible states is called the state 
space and we denote it by X. 
Let the possible states in X be numbered as 0,1,2,..., r such that the system 
has r + 1 different states. Whenever feasible, we assume that 0 represents the "best" 
state and that r represents the "worst" state of the system. Let X(t) denote the state 
of the system at time t. For a future point of time t, the state X(t) is a random 
variable, and Pr(X(t) = i) = Pi(t) is the probability of being in state i at time t 
for i = 0 , 1 , . . . , r. The state X(t) changes with the time t and this process is called 
a stochastic process, or more specifically a continuous time Markov process. 
ffl EXAMPLE 5.19 Single channel 
Consider a single channel that can have two types of failures: safe failures and 
dangerous failures. All failures are repaired and the channel is brought back to 
the functioning state. The channel can have the three states listed in Table 5.2. 
We assume that the channel is started up at time t — 0 in the functioning 
state, A'(O) = 0. At some time t, the channel gets a safe failure and the state 
changes to X(t) = 1. This is called a transition from state 0 to state 1. A repair 
action is then started to bring the channel back to the functioning state 0 and we 
have a transition from state 1 to state 0. At some other time, the channel has 
a dangerous failure and a transition from state 0 to state 2, and a repair action 
brings the channel from state 2 to state 0. The possible transitions are illustrated 
in the state transition diagram in Figure 5.15. The diagram is also called a 
Markov model. In this case, it is assumed that it is not possible for a safe failure 
to develop directly into a dangerous failure, and vice versa; such that there are 
no direct transitions between the states 1 and 2. 
φ 
The Markov Property. 
Let Hs denote the "history" of the process up to time s. 
This history contains information about which states the system has visited from 
when it was put into operation at time 0 and until time s. 

122 
RELIABILITY QUANTIFICATION 
Functioning 
Dangerous 
fault 
Figure 5.15 
Markov model for the single channel in Example 5.19. 
A stochastic process is said to fulfill the Markov property and be a Markov process 
if for all times s, t > 0, all states i,j, and for all possible histories Hs, we have that 
Pv(X(s + t) = j I X(s) = i Π Us) = Pr(*(i + t) = j | X(s) = i) 
(5.49) 
This equation says that for a system that is in state i at time s, the probability that 
this system is in state j at time s + t is independent of what happened to the system 
before time s. This means that the past has no influence on what happens in the 
future, and we say that the process has the memoryless property. What happened to 
the system before time s is therefore of no interest when calculating the transition 
probability Pv(X(s + t) = j \ X(s) = i). 
In addition, we assume that the probability PT(X(S + t) = j | X(s) = i) 
is independent of the time s. Such a process is said to have stationary or time-
homogenous transition probabilities and we write 
Pu(t) 
= PT(X(S 
+ t) = j | X(s) 
= /) 
for all s 
(5.50) 
This means that if the system is in state i at time s, the probability that it is in state 
j t time units later is independent of the time s. 
The transition probabilities can be set up as a matrix called the transition proba-
bility matrix 
P(i) = 
fPoo(t) Λη(0 
Λο(0 Λι(0 
\ Λ ο ( 0 Λ ι ( 0 
Por(t)\ 
Λ Γ ( 0 
Prr(ß)J 
(5.51) 
The diagonal element Pa (t) is the probability that a process that is in state i at time 
0, also will be in state i t time units later. The matrix P(/) gives the probabilities of 
"jumping" from a state / to a state j in the time interval of length t, for any i and j . 
► Remark: The transition probability Pjj (t) does not imply that the process makes 
a single "jump" from state i to state j in the time interval (0, t). Pij(t) only gives 
the probability that a process in state i at time 0 will be in state j at time /. What 
happens in the interval is not expressed by this probability. 
Θ 

MARKOV APPROACH 
123 
In addition to the assumptions above, we assume that the transition probability is 
continuous. 
lim Pif (/)= I 1 
for 
' = y 
(5.52) 
f-o 
U W 
(0 for 
ίφ 
j 
fl 
EXAMPLE 5.20 Single channel (Cont. 1) 
Reconsider the channel in Example 5.19 and assume that the channel is function-
ing at time s, such that X(s) = 0. When we assume that the Markov property 
is fulfilled, the probability of having a dangerous failure / time units later is 
Pr(-Y(s +1) — 2 | X(s) = 0). How many times the channel has failed and been 
repaired prior to time s is of no interest when calculating this probability. 
Because the process has stationary transition probabilities, the probability is 
the same for all s; what is important is the time / available for the transition. 
This means, for example, that a failure is as likely in summer as in winter. 
φ 
Sojourn Time. When the Markov process enters a specific state i, it stays there for 
a period Γ, that is called the sojourn time in state i. The sojourn time is a random 
variable and we want to find its probability distribution expressed as ΡΓ(Γ, > t). 
First, consider the conditional probability PrCT, > / + s | 7} > s), the probability 
that the process stays in state i at least t time units more when we know that it already 
has been staying there for s time units. As the process has the Markov property, the 
probability of staying for t time units more is determined only by the current state i. 
The fact that it already has been staying there for s time units is irrelevant. Thus 
Pr(f, >t + s\Ti>s) 
= Pr(7} > t) for s, t > 0 
This means that the random variable T has the memoryless property and is therefore 
exponentially distributed (because the exponential distribution is the only continuous 
distribution with this property). Each time the system enters state i, the amount of 
time it spends in this state before making a transition to a different state is exponen-
tially distributed with some parameter a,. This means that when the system enters 
state i, the mean time it stays in this state is Ι/α,. When the time Γ, is "over," the 
system leaves state i and jumps to another state, the rate α,- may therefore be inter-
preted as the (total) departure rate from state /. Note that the higher the rate α;, the 
smaller is the mean time the system stays in state /. 
β 
EXAMPLE 5.21 Single channel (Cont. 2) 
Reconsider the single channel in Example 5.19. The paragraph above implies 
that the channel must have constant failure rates with respect to both safe and 
dangerous failures. When the channel enters the safe failure state (i.e., state 
1), the mean time it stays in this state is \/et\. This is the mean time to repair 
the safe failure, MTTRs, and therefore a\ = 1/MTTRS. The parameter a\ is 

124 
RELIABILITY QUANTIFICATION 
therefore the repair rate for a safe failure. In the same way, when the channel 
enters the dangerous failure state (i.e., state 2), the mean time it stays there is 
1/α2· This is the mean downtime MDTD the channel has after a dangerous 
fault, and «2 = 1/MDTD is the "repair rate" of a dangerous failure. When 
the channel enters the functioning state (i.e., state 0), the mean time it stays in 
this state is 1/ao. This is the mean time to failure MTTF, of the channel and 
a0 = 1/MTTF is the total failure rate of the channel (i.e., with respect to both 
safe and dangerous failures). 
► Remark: There are several problematic issues related to Example 5.21. If the 
channel is proof tested periodically, the Markov property is not fulfilled. Another 
problem is that for low-demand systems, the downtime related to a DU failure has 
two elements: the unknown downtime D\ from the DU failure occurs until the DU 
fault is detected in the proof test, and the repair time (MRT). The mean downtime 
related to a DU failure is therefore MDTDU = E(D\) + MRTDU· Formulas for 
E(D\) for general koon voted groups are developed in Chapter 8. 
Θ 
Embedded Markov Chain. If we consider the Markov process only at the mo-
ments at which the state of the system changes, and we number these instances 
n = 0,1,2,..., the process is a discrete Markov chain that is called the embed-
ded Markov chain of the process (e.g., see Taylor & Karlin, 1998). Xn = i means 
that the process is in state i at time (step) n. When the process is in state i, the 
probability that it, at the next step, is in state j is ptj is 
Pij = Pr(Z n + 1 
=j\Xn=i) 
Because the system cannot leave state i and then enter state i, we must have />,·,· = 
0, and because the system has to jump to one of the other states, then X) ,-έ, Pij = 1. 
The last expression simply says that the system has to stay within the r + 1 states; it 
is not possible to jump to any "external" state that has not been specified. 
The transition probabilities of the embedded Markov chain can be set up as a 
matrix 
ΪΡοο 
Ροι ■■■ Por\ 
Pio 
P\\ 
■■■ P\ 
\PrO 
Pr\ 
■■■ Prrj 
where />,·,· = 0 for all i = 0,1,..., r. 
We may also introduce the probability py based on a different argument. Con-
sider a process that is in state / at time t = 0, such that A^O) = i. The process stays 
in state i during the sojourn time 7;. The probability that it then jumps to another 
state j is 
Pij = Pr(Z(f,·) = j | *(0) = i) 

MARKOV APPROACH 
125 
Transition Rates. 
Let ay be defined by 
ay =ctiPij 
for all / φ j 
(5.53) 
Because a, is the (total) departure rate from state i and />y is the probability that 
when the departure takes place, it goes to state j , the aij is the transition rate from 
state i to state j . 
Because Σ ■ <; /?y = 1, it follows that 
r 
at = Y^au 
(5.54) 
y=0 
This equation says that the total departure rate from state i is equal to the sum of all 
the transition rates out of state i (i.e., to all states other than state /). 
Let 7y be the time the process spends in state i before entering into state j (Φ i). 
As the transition rate ay is constant, the time 7y is exponentially distributed with 
rate ay and we have 
Pr(*(f + Δί) = j | X(t) = i) = Pr(7y < At) = 1 - e~a'JAt «s ay At 
Consider a short time interval of length Δί. Because 7} and 7y are exponentially 
distributed, we have that 
-a; At 
Pu(At) = Pr(7} > At) = e _ a' A i % 1 - α , Δ ί 
Pij(At) = Pr(7y < Δί) = 1 - β-ανΑί 
« an At 
In the first equation, Pa (At) is the probability that if the item is in state / at some 
time, it is also in the same state Δί time units later. This means that the sojourn time 
in state i must be longer than At. In the second equation, Py (At) is the probability 
of an item being in state i at some time, jumps to another state j within Δί time 
units. This means that the transition time 7y from state i to state j must be shorter 
than Δί. 
When Δί is "small," we have 
l-Pn(At) 
Ρτφ < At) 
lim 
= lim 
= a.· 
(5.55) 
Δί^-O 
At 
At->0 
At 
Pii(At) 
, 
Pr(7y < Δί) 
lim -^i—- = lim —^ 
= a,·,· 
for i φ j 
(5.56) 
Δί^Ο 
Δ ί 
Δί^Ο 
At 
J 
From (5.56) the transition rate ay may be defined as 
r 
Prfflf + At) = j j X(t) = i) 
a,·,· = lim 
(5.57) 
J 
At^-0 
At 

126 
RELIABILITY QUANTIFICATION 
The Transition Rate Matrix. The transition rates can be set up in a matrix format 
to form the transition rate matrix A: 
/Ö00 
#01 
αιο 
an 
\aro 
ari 
aor\ 
air 
arr) 
(5.58) 
where (as before) a 
The transition rate matrix is also called the generator matrix or the infinitesimal 
generator matrix and contains all the "from-to" information in the Markov model. 
We can read a lot of information from the transition rate matrix A: 
- The diagonal element α,, = —α,, where a, is the total departure rate from state 
i. This implies that the sum of the elements in any row of A is equal to zero. 
Consequently, A does not have full rank and is not invertible. 
- When the process enters state i, it stays in this state for a time 7} that is ex-
ponentially distributed with rate a, = —a,/. The mean time it stays in state 
i is then 1/a, and the probability that it stays longer than t in the state is 
Pr(7} > 0 = <?""''· 
- When the process is in state / and its sojourn time in this state is over, it jumps 
to another state j with probability />,y = α^/α,- = —α^/α,-,-. 
It is worth noting that the probability pij from (5.53) and (5.54) is 
a; 
PU 
ILL 
ίφί 
By using (5.55) and (5.56) it is seen that the transition rate matrix A is given by 
" ( Δ 0 - Ι 
= lim 
Ar->o 
Δί 
(5.59) 
where I is the identity matrix 
/ l 
0 
0 1 
Vo o 
0 
1/ 

MARKOV APPROACH 
127 
5.5.3 
Kolmogorov Equations 
The Chapman-Kolmogorov Equation. The probability that a system that is in 
state i at time 0 is in state j at time t + s is Pjj (t + s). Assume that we observe the 
system at time t and find that it is in state k. This means that the system has had a 
transition from state / to state k in the time interval of length t. To be in state j at 
time t + s, it must have a transition from state k to state j in the remaining interval 
of length s. The probability of this particular transition from / to j is Pjk (t ) Pkj (s) 
because of the memoryless property. 
Because the state k can be any of the states, we get 
Pij{t+s) = 
YdPik{t)Pkj{s) 
(5.60) 
* = 0 
Equation (5.60) is called the Chapman-Kolmogorov equation.5 
By using the matrix notation in (5.51), the Chapman-Kolmogorov equation may 
be written as 
P ( f + ί ) = Ρ(ί)Ρ(ί) 
(5.61) 
Kolmogorov Equations. 
According to (5.56), limAr-*· Pij(^t)/ 
At — ay. This 
can also be written as 
Pjj (Δί) = ajj At +o(At) 
for / φ j 
where o(At) is a function such that limAr->-o o(At)/At 
= 0. The time derivative of 
Pjj (t) must therefore be 
Pu(0)=liPu(t) 
H m 
Pjj(0+At)-Pjj(0) 
t=o 
Δί->0 
At 
Similarly, we have 
Pii(Q) = an = -a,· 
By using matrix notation, we therefore have 
P(0) = 
(Poi(0) 
Ai(o) 
\Λι(ο) 
Ρθ2(0) 
■■ 
P12(0) 
■■ 
Prl(0) 
■■ 
■ 
Por(0)\ 
■ 
Λ Γ ( 0 ) 
• Prr(0)J 
By differentiating the Chapman-Kolmogorov equation (5.60) with respect to /, 
we get for s = 0 
AP(0 = P(f) 
5 Named after the British mathematician Sydney Chapman (1888-1970) and the Russian statistician An-
drey Kolmogorov (1903-1987). 

128 
RELIABILITY QUANTIFICATION 
Matrix multiplication 
Consider the two matrices A and B 
A = 
(an 
a2l 
Kanl 
Ö12 
··· 
aim^ 
Û22 
·· · 
«2m 
Uni 
■■■ 
dnmj 
, 
B = 
rbu 
b2\ 
\bm\ 
The matrix product of A and B is 
/ 
AB = 
V 
(ab)u 
(ab)2\ 
{ab)n\ 
(ab)l2 
■■■ 
(ab)lp^ 
(ab)22 
■ ■ ■ 
(ab)2p 
(ab)n2 
■■■ 
(ab)np/ 
where 
b\2 
■■■ 
b22 
... 
bmi 
■ ■ ■ 
(ab)ij 
= 
biP^ 
b2p 
bmp) 
m 
Σ 
aikhj 
k=l 
Note that A is an n x m matrix, B is an m x p matrix, and consequently, AB is an n x p 
matrix. 
Example: 
A = 
I , B = ( X 1 , and AB = 
[c d) 
\y) 
(ax + by 
\cx + dy ) 
This matrix equation is called the Kolmogorov backward equations. 
Similarly, by differentiating the Chapman-Kolmogorov equation with respect to 
s, we get 
P(f)A = P(i) 
and this matrix equation is called the Kolmogorov forward equations. 
Whether we use the Kolmogorov backward or forward equations is not important. 
In order not to confuse the reader, we will in the following use the Kolmogorov 
forward equations. 
The Kolmogorov equations are also called the state equations for the process. 
Known Starting Point. If we know that the Markov process started in state / at 
time 0, that is, X(Q) = /, we are mainly interested in the row of P(r) corresponding 
to state i. The vector P,(i) = [Pio(t), Pn(t),..., 
Pir(t)] then denotes the state 
distribution of the Markov process at time t, when we know that the process started 
in state / at time 0. 
Because we know that the Markov process starts in state i, the index / is often 
suppressed and we write the vector P(t) = [/Ό(0> Λ(0» · · ·. Λ·(01· ^ s there are 
only r + 1 possible states, 5^·_ 0 Pj(t) = 1· 

MARKOV APPROACH 
129 
The distribution P(t) can in matrix terms be found from the Kolmogorov forward 
equations. In matrix terms, this can now be written 
[P0(t),...,Pr(t)]· 
\ar0 
an 
aor\ 
arr) 
= [Ρο(0.···.Λ(θ] 
or in a more compact form as 
P(t)A = P(t) 
(5.62) 
Solving the State Equations. 
The state equations can, at least in principle, be 
solved by the following approach. 
In the same way as for scalar differential equations, the solution of (5.62) is given 
as 
P(i) = P(0)i>Ai 
Because P(0) = I (the identity matrix) this yields 
p(0 
where the matrix eAt is defined by 
1! 
2! 
^ 
n\ 
(5.63) 
(5.64) 
n=0 
and where A" is the (matrix) multiplication of A by itself n times. The correctness 
of the solution (5.63) can be checked by substitution into the Kolmogorov equation. 
The following approximation is sometimes used 
p(0 nr 
(5.65) 
when n is large (see Ross (1996) for details). 
If the evaluation of (5.64) is too complicated, we may also use Laplace transforms 
(see box). The Laplace transform of (5.62) is 
sP*(s)-P(0) = P*(s)A 
and P* (s) can be expressed as 
P*(i)=P(0)[5l-A] _ 1 
The vector P(r) can be obtained—at least in principle—from the inverse Laplace 
transforms. 
Several computer programs are available for solving the Markov state equations. 
Some of these programs use Monte Carlo simulation to find the solution. 

130 
RELIABILITY QUANTIFICATION 
Laplace transform 
Let /(f) be a function for t > 0. The Laplace transform" f*(s) of the function f(t) is 
defined by 
/•OO 
/*(*)= / 
e-"f{t)dt 
where s is a real number. In more advanced treatments of the Laplace transform, s is 
permitted to be a complex number. The Laplace transform is also denoted by C[f(t)]: 
/■OO 
£[/(')] = /*(*)= / 
e-"f(t)dt 
Jo 
to indicate the relation between the functions / and / * . When /(f) is a probability 
density function of a nonnegative random variable T, the Laplace transform of /(f) is 
seen to be equal to the expected value of the random variable e~ 
. 
E{e-sT) = j 
e-stf{t)dt 
= f*(s) 
The function /(f) is said to be the inverse Laplace transform of f*(s), 
and is written 
fit) = ΖΓ1 [/*(*)] 
Properties of the Laplace transform include: 
i. £[/ι(ο + /2(θ] = Λ*ω + /2*ω 
2. £[«/(*)] = α / * ( ί ) 
3. £[/'(/)] = i / * W - / ( 0 ) 
4. L \k\ = k/s 
5. £ [e- A f] = 1/(5 + A) 
Tables listing corresponding expressions for f(t) and /*($) are available. 
α Named after the French mathematician Pierre-Simon Laplace (1749-1827). 
β 
EXAMPLE 5.22 
Single item with only one failure mode 
Consider a single item. The item has two possible states: 
State 
State description 
0 
The item is functioning as specified 
1 
The item is in a failed state 
Transition from state 0 to state 1 means that the item fails, and transition 
from state 1 to state 0 means that the item is repaired. The transition rate αυι is 
therefore the failure rate of the channel, and the transition rate a\o is the repair 

MARKOV APPROACH 
131 
OK 
μ 
Figure 5.16 
Markov model for a single item (function-repair cycle). 
rate of the channel. We assume here that the failure is detected immediately and 
that a repair action is initiated. In this example we use the following notation: 
α0ι = λ the failure rate of the item 
a 10 = μ the repair rate of the item 
The mean sojourn time in state 1 is the mean time to failure, MTTF = 1/λ, 
and the mean sojourn time in state 0 is the mean repair time, MTTR = 1/μ. 
The Markov model for the single item is illustrated in Figure 5.16. 
The state equations are 
/>oo(0 
Ροι(ί)\ί-λ 
k\ÎPm(t) 
P01(t)\ 
\Pw(t) 
Ριι(ί))\μ 
-μ) 
\ Λ ο ( 0 
Λ ι ( 0 / 
If we assume that the item is started at time t = 0 in the functioning state, 
then 
Pr(A"(0) = 0) = 1 and Pr(X(0) = 1) = 0 
it is sufficient to consider only two of the equations in (5.66) and we can solve 
-XPoo(t) + μΡοι(ί) = Α)θ(ί) 
XPoo(t) - μΡοι(ΐ) 
= Ροι(0 
Because Ppo(0 + Λη(0 = 1, the time derivative is P00(t) + PoiU) = 0, 
such that PooiO = —Poi(t)· The two equations are therefore equal, and it is, 
therefore, sufficient to use only one of them, for example, 
-λΡ00(ί) 
+ μΡοι(() = PooiO 
and combine this equation with Poo(t) + Λ)ΐ(0 = 1. The solution is 
Poo(t) = - 4 y 
+ -^-j 
e-tW 
(5.67) 
μ + A 
μ + λ 
μ + λ 
μ + λ 

132 
RELIABILITY QUANTIFICATION 
Availability 
Figure 5.17 
The availability and the survivor function of a single item (λ = 1, μ = 10). 
These results can be readily obtained by solving (5.63) with the computer 
program MAPLE® and its Linear Algebra package. 
Because the item is started up in state 0 at time t = 0, this can be assumed as 
a known fact, and we may write Po(0 and P\(t) instead of PooU) and Po\(t), 
respectively, as we did in (5.62). 
Po(t) denotes the probability that the item is functioning at time /, that is, the 
availability of the item. The limiting availability Po = lim/_HX) Po(t) is, from 
(5.67), 
μ 
P0 = lim P0(t) = 
ί->·οο 
λ + μ 
(5.69) 
and is the same irrespective of whether the item started out in state 0 or state 1 
at time / = 0. 
The limiting availability may therefore be written as the well-known formula 
MTTF 
MTTF + MTTR 
(5.70) 
When there is no repair (μ = 0), the availability is Po(t) = e~kt, which coin-
cides with the survivor function of the item. The availability Po(t) is illustrated 
in Figure 5.17. 
Θ 
5.5.4 
Steady-State Solution 
In many applications, only the steady-state, or long-run probabilities are of interest, 
that is, the values of Pj(t) when t -> oo. In Example 5.22, the state probabilities 
Pj(t) (j = 0,1) approached a steady-state Pj when t ->■ oo. The same steady 
state value would have been found irrespective of whether the system started in the 
operating state or in the failed state. 
For the limiting state probabilities to exist, we have to assume that 
- All states in the Markov process communicate. This means that if a process 
starts in state i at some time t, it has a positive probability of reaching state j 

MARKOV APPROACH 
133 
some time in the future. This must apply for any combination i, j of states. A 
Markov process with this property is said to be irreducible. 
The Markov process must be positive recurrent, meaning that if the process 
starts in a specified state /, the expected time to return to this state is finite, for 
all states i. 
Steady-State Probability. For a Markov process that fulfills the criteria above, the 
limits 
lim Pj(t) = Pj for; = 0 , 1 , , . . . , r 
(5.71) 
exist and are independent of the initial state of the process (at time 0). For a proof, see 
Ross (1996, p. 251). These asymptotic probabilities are often called the steady-state 
probabilities for the Markov process. 
If Pj (t) tends to a constant value when t —► oo, then 
lim Pj(t) = 0 for; = 0 , 1 , . . . , r 
(5.72) 
The steady-state probabilities P = [Po, Pi, 
trix equation 
Pr] must therefore satisfy the ma-
[Po, Λ 
Pr] 
'ßoo 
floi 
iZlO 
o i l 
Wo 
an 
aor\ 
a\ 
arrJ 
= [0,0,...,0] 
(5.73) 
which may be abbreviated to 
where, as before, 
P A = 0 
7=0 
(5.74) 
To calculate the steady-state probabilities, PQ,P\,...,PT,OÎ 
such a process, we use 
r of the r + 1 linear algebraic equation from the matrix equation (5.74) and, in 
addition, the fact that the sum of the state probabilities is always equal to 1. The 
initial state of the process has no influence on the steady-state probabilities. Note 
that Pj may also be interpreted as the average, long-run proportion of time that the 
system spends in state j . 
fl 
EXAMPLE 5.23 2oo3 voted group of independent channels 
Consider a 2oo3 voted group of three identical and independent channels with 
failure rate λ. Because the channels are identical, it suffices to define the four 
states in Table 5.3. The 2oo3 voted group is functioning in states 0 and 1 and is 

134 
RELIABILITY QUANTIFICATION 
Table 5.3 
Possible states of the 2oo3 voted group in Example 5.23. 
State 
State description 
0 
Three channels are functioning 
1 
Two channels are functioning and one is failed 
2 
One channel is functioning and two are failed 
3 
Three channels are failed 
Figure 5.18 
Markov model for the 2oo3 voted group in Example 5.23. 
failed in states 2 and 3. A Markov model for the 2oo3 voted group is shown in 
Figure 5.18. 
In state 0, three independent channels can fail and the transition rate to state 
1 is therefore 3λ. Similarly, in state 1, two independent channels can fail and 
the transition rate to state 2 is therefore 2λ. In state 2, only one channel can cal 
and the transition rate to state 3 is therefore A. In state 1, one channel is failed 
and a repair action is initiated to bring the voted group back to state 0. The rate 
of this transition is μι = 1/MTTRi, where MTTRi is the mean repair time of 
one channel. In state 2, two channels are failed (and the 2oo3 voted group is 
failed) and our strategy is to repair both failures before the system is started up 
again. This means that we have a transition from state 2 to state 0, but not to 
state 1. The mean repair time of two channels is MTTR2, such that the repair 
rate from state 2 to state 0 is μ^ = I/MTTR2. When the voted group is in 
state 3, we use the same strategy and do not start up the system again until all 
the three channels are functioning. The mean repair time of three channels is 
MTTR3 and the repair rate from state 3 to state 0 is μ3 = I/MTTR3 (note that 
several other repair strategies may be used). 
The transition rate matrix for this 2oo3 voted group is 
3λ 
0 
0^ 
(μι + 2λ) 
2λ 
0 
0 
-{μι + λ) 
λ 
0 
0 
-μ3> 
ί-3λ 
μι 
-
μ2 
V Μ3 

MARKOV APPROACH 
135 
The steady-state distribution of the 2oo3 voted group is denoted 
P = 
[Po,Pi,P2,P3\ 
where f, is the long-term probability of finding the 2oo3 voted group in state 
i. The steady-state distribution can be determined from PA = 0. Inasmuch as 
the matrix does not have full rank, we can remove one of the four columns and 
replace it with the equation P0 + Pi + Pi + Pi = 1 · We then get the new matrix 
equation (when replacing a column, we usually choose one with few zeros). 
[Po,PuPi,P3] 
(l 
1 
1 
V 
3λ 
-(μ-ι + 2λ) 
0 
0 
0 
2λ 
-0*2 + λ) 
0 
o\ 
0 
λ 
-ß3j 
= [1,0,0,0] 
Assume the parameter values λ = 5.0 · 10~5 per hour (i.e., MTTF = 20000 
hours), μι = 1/8 per hour (i.e., MTTRi = 8 hours), μ2 = μ3 = 1/10 per hour 
(i.e., MTTR2=MTTR3=10 hours. We then obtain 
P0 = 
μ3(2λΐ + 2λμ2 
+ μιμ2 
+ λμι) 
% Q 
^ 
6λ3 + 11λ2μ3 + 5λμ2μ3 + μιβ2ΐ^3 + λμιμ 3 
ρ 
= 
3Αμ3(Α + μ2) 
% 1 12 · 10~3 
6λ3 + 11λ2μ3 + 5λμ2μ3 
+ μιμ2μ?, + 
λμιμ3 
ρ 
=
 
6 Α 2^3 
ssl.12-10 - 6 
6λ3 + 11λ2μ3 4- 5λμ2μ3 
+ μιμ2μ3 
+ λμιμ 3 
ρ3 
^ 
% 5.99· 10-10 
6λ3 + 11λ2μ3+5λμ2μ3 
+ μιμ2μ3 
+ 
λμιμ3 
A 2oo3 voted group is functioning in states 0 and 1 and is failed in states 2 
and 3. The long-term probability that the voted group is in a failed state (i.e., its 
average unavailability) therefore Äavg = P2 + F3 % 1.12 · 10~6. 
® 
With the four states in Example 5.23, it is still possible to find the state probabil-
ities by hand calculation, but this is a demanding task when the Markov model has 
five or more states. In Example 5.23, the author used MAPLE® to find the analytical 
and numerical solution for the state probabilities. For voted groups with many states, 
analytical expressions are often not feasible. In such cases, MATLAB®, or one of 
its clones, may be a better choice. For complicated Markov models, Monte Carlo 
simulation may be the best choice to determine the state probabilities. 
fl 
EXAMPLE 5.24 2oo3 voted group with CCFs 
Reconsider the 2oo3 voted group in Example 5.23, but assume that CCFs may 
occur due to external events. When an external event occurs, with rate λ ^ , all 

136 
RELIABILITY QUANTIFICATION 
Figure 5.19 
Markov model for the 2oo3 voted group with CCFs in Example 5.24. 
the channels fail and the voted group jumps to state 3. This occurs irrespective 
of the number of channels that are functioning when the external event occurs. 
The states in Table 5.3 still apply and the associated Markov model is shown in 
Figure5.19. The individual failure rate of a channel is X^'\ suchthatA = λ ^ + 
X^c\ The failure rates may be specified by a beta-factor, ß, such that λ^ 
= βλ 
and Α^ = (1 — β)λ. With the same repair strategy as in Example 5.23, the 
associated Markov model is given in Figure 5.19. 
The transition rate matrix for mis 2oo3 voted group is 
/-(3λ ( / ) + A<c>) 
\ 
Ms 
3λ('"> 
0 
XW\ 
-(μι + 2λ<η + λ^) 
2λ<''> 
X(c) 
0 
-(μ2 + λ) 
X 
0 
0 
- μ 3 / 
This transition rate matrix has fewer zeros than the matrix in Example 5.23 
and it is therefore more demanding to solve the state equations by hand. The 
author has again used MAPLE® to find the analytical expressions, but they are 
too complicated to include here, so we suffice with the numerical results. The 
results are based on the same input data as in Example 5.23. In addition, we use 
the beta-factor β = 0.10, and get 
P0 % 0.9989 
Λ % 1.08· 10"3 
P2 % 9.70 ■ 10"7 
P3 « 5.00 · 10~5 
The average unavailability of the 2oo3 voted group is in this case 
Aavg 
P2 + P3 %5.10-10 - 5. 
A Note on Series Systems. 
The number of states of the Markov model increases 
with the number of system components and we should therefore try to reduce the 

MARKOV APPROACH 
137 
(a) 
1 
3 
2 
(b) 
(a) 
(b) 
Figure 5.20 
Replacing a series system by a "super component" (S). 
number of states whenever possible. If the system has modules with a series config-
uration, one option is to model this module as a "super component." 
A simple example is illustrated in Figure 5.20, where the independent components 
1 and 2 have been replaced by a "super component" S. When the failure rate and 
mean downtime of component i are A, and MDT,, respectively, for i = 1,2, the 
failure rate of S is 
As = λι + λ2 
and the mean downtime of S is from (5.26) 
MDTs = 
MDTi + 
λι +λ2 
The repair rate of S is then approximately 
λ ι + λ 2 
MDT2 
1 
Ms 
MDTs 
(5.75) 
► Remark: The failure rate of independent components in series is the sum of the 
failure rates, such that As is correct. The distribution of a weighed sum of indepen-
dent exponential distributions (i.e., the basis for MDTs) is» however, not exponen-
tially distributed, unless all the components have the same repair rate. This means 
that the repair rate of S in (5.75) is not constant (it approaches, however, a constant 
value after some time, see Rausand & H0yland, 2004). 
This may not be a bigger problem than using the Markov approach in the first 
place, inasmuch as repair times very seldom (if at all) are exponentially distributed. 
Availability. The availability of a voted group is related to its essential function. Let 
X = {0,1,..., r} be the set of all the possible states of the voted group. For some of 
these states, the voted group is able to perform its essential function, while for other 
states the voted group is not able to perform its essential function. Let U denote the 
subset of X for which the voted group is functioning (i.e., is "up"). The subset for 
which the voted group is not functioning (i.e., is "down") is then V = X — U. 
The voted group is functioning and the essential function is available as long as 
the voted group is in one of the functioning states (i.e., in U). The average, or long-
term, availability of the voted group is therefore 
*avg 
(5.76) 

138 
RELIABILITY QUANTIFICATION 
The unavailability of the voted group is 
^avg 
= 
1 — ^avg 
— 
/ 
, "j 
(5.77) 
Visit Frequency. It is sometimes of interest to determine how often a particular 
state will be visited, for example, how many system failures that occur. 
The visit frequency Vj to state j can be expressed as 
Vj = Σ Pka/cj 
The rationale for this equation is: To have a transition to state j , the voted group 
must be in some other state k φ j . The probability of this is Pk. Then, there 
must be a transition from state k to state j and the rate of this transition is a^j. By 
summing over all possible k φ j , the visit frequency to state j is obtained. 
In the same way, the frequency of departures from state j can be expressed as 
vfp = PJ Σ «* = PJ«J 
*=0 
The rationale for this equation is: To depart from state j , the voted group must be in 
state j and the probability of this is Pj. Then, there must be a transition out of state 
j to a state k where k φ j . By summing over all possible k φ j , the departure 
frequency from state j is obtained. 
In a steady state situation, the long-term number of visits to a state must be equal 
to the number of departures. The visit frequency of state j may therefore be written 
as 
vj = Σ 
Pkakj - Pjotj 
(5.78) 
k=0 
k±j 
► Remark: The average availability can also expressed based on the visit frequency. 
During a time period of length t, the mean number of visits to state j is E(Nj (t) = 
Vjt. In each visit, the mean sojourn time in state j is 1/a/, and the total mean time 
spent in state j in the time period is Vjt/otj. The average availability in the time 
period of length t is 
Mean uptime in (0, i) 
J2j€UvJt/aJ 
v ^ 
As = 
= 
= > 
j&A 
which is the same result as obtained in (5.76). 

MARKOV APPROACH 
139 
fl 
EXAMPLE 5.25 Visit frequencies for a 2oo3 voted group with CCFs 
Reconsider the 2oo3 voted group with CCFs in Example 5.24. The visit fre-
quencies to the four states are: 
v0 = P0ao = Λ)(3λ(ί) + A(c)) % 1.40 · 10"4 per hour 
Vl = piai 
= Pi (/ii + 2λ(,) -I- Ac) % 1.35 · 1(Γ4 per hour 
y2 = P2a2 = Ρ2(2μ2 + λ) sa 9.70 · 10~8 per hour 
v3 = P3a3 = Ρ32μ3 % 5.00 · 10"6 per hour 
If the voted group is operated on a continuous basis (i.e., 8760 hours per year), 
a departure from state 0 takes place 8760vo hours ss 1.23 times per year, or on 
the average once every 1/vo s» 7151 hours. 
Θ 
Frequency of Voted Group Failures. To have a voted group failure, there must be 
a transition from a functioning state k e U to a state i e V. With the same arguments 
as for the visit frequency to state i, the frequency ωρ of voted group failures is 
COF = Σ Σ 
PkUki 
(5·79) 
keuiev 
fl 
EXAMPLE 5.26 Frequency of system failures for a 2oo3 voted group 
Reconsider the 2oo3 voted group with CCFs in Examples 5.24 and 5.25. A voted 
group failure occurs when a channel failure causes the voted group to enter state 
2 or state 3. A voted group failure to state 2 can only occur when the voted 
group is in state 1 and one of the two channels fail, while a voted group failure 
to state 3 occurs when the voted group is in state 0 or 1 and a CCF occurs (The 
transition from state 2 to state 3 does not represent a voted group failure). The 
frequency of voted group failures is then 
ω = Px2X(i) + (P0 + Pi)X{c) sa 5.10 · 10"6 per hour 
The voted group will, on the average, fail once per 22.40 years. 
φ 
Mean Duration of a Failure. The mean duration of a voted group failure ÎQE 
is the mean time from when a voted group enters into a failed state until it is re-
paired/restored and brought back into a functioning state. The unavailability of the 
voted group can obviously be expressed as the product of the frequency of voted 
group failures times the mean duration of a voted group failure 
As = (ûFtGE 
(5.80) 
This formula is used in IEC61508 to determine the unavailability of a low-demand 
SIF. 

140 
RELIABILITY QUANTIFICATION 
Table 5.4 
Possible states of the loo2 system in Example 5.27. 
State 
State description 
0 
Both channels are functioning 
1 
One channel is functioning and one is failed 
2 
Both channel are failed 
5.5.5 Time-Dependent Solution 
Time-dependent state probabilities P(t) can, at least in principle, be determined 
from (5.62)-(5.65). To determine P(t) by using an analytical approach is problem-
atic when the number of states goes beyond the capacity of computer programs, such 
as MAPLE®. In many practical cases, we therefore have to use Monte Carlo simu-
lation. Several computer programs are available for this purpose, but we will not go 
into details about these. 
Some reliability measures can still be determined by analytical methods, such as 
the MTTF. Before determining the MTTF through some simple examples, we need 
to define two new concepts. 
Absorbing and Transient States. 
A state i in a Markov model is called absorbing 
if it is impossible to leave state i. A Markov process is said to be absorbing if it 
has at least one absorbing state, and if it from every state is possible to go to an 
absorbing state (not necessarily in one transition). In an absorbing Markov model, 
the probability that the process will be absorbed is equal to 1. A state that is not 
absorbing is said to be transient. 
The states we define to be absorbing depend on the aim of the analysis. In some 
cases, the absorbing state is truly absorbing, but we may also define a specific state 
to be absorbing because we want to study the time until the voted group jumps into 
this state, for example until a system failure. 
Survivor Function and MTTF. The approach to finding the survivor function and 
the mean time to system failure is illustrated by Example5.27. 
fl 
EXAMPLE 5.27 
loo2 voted group of independent channels 
Consider a loo2 voted group of two identical and independent channels. As the 
channels are identical, it suffices to define the three states in Table 5.4. States 0 
and 1 are functioning states, while state 2 is a failed state. A Markov model for 
the loo2 voted group is shown in Figure 5.21. 
With No Repair. Assume that both channels have failure rate λ and that no 
repair is carried out. The voted group starts out at time t = 0 in state 0 such 

MARKOV APPROACH 
141 
Figure 5.21 
Markov model for the loo2 voted group in Example 5.27 with no repair. 
that Pr(X(0) = 0) = 1. A Markov model for the voted group is given in 
Figure 5.21. We note that state 2 is an absorbing state. When the voted group 
has come to this state, it cannot leave the state. 
The transition rate matrix for this loo2 voted group is 
Because we assume that the voted group is started up in state 0 (i.e., the fully 
functioning state) at time t = 0, we may omit the explicit reference to the 
starting state and use (5.62) to write 
[Po(0. Λ(0, Pi{t)\ A - [Po(0. Λ(0. hit)} 
As all the entries of row 3 of A are 0, Ρζ{ί) and ^ ( 0 do not appear in the state 
equations, and we can write 
[Ρο(Ο.Λ(0]ΑΜ = [Α>(Ο.Λ(0] 
where 
(-2X 
2X 
A" = ( o 
-x 
is the (truncated) transition rate matrix for the "up" states 0 and 1. 
Direct Solution. 
The state probabilities Po(t) and Pi (r) can in this simple case 
be obtained by solving the matrix equation with MAPLE® and its LinearAlge-
bra package, and we get 
P0(t) = e-2Xt 
Pi(t) = 2e-Xl 
-2e~2X' 
The voted group survives as long as it is state 0 or 1. The survivor function 
for the voted group is therefore 
R(t) = PQ(t) + Pl(t) = 2e~Xt 
-e~2kt 
which is the same result we got in Example 5.15 with a more direct approach. 
The mean time to voted group failure can be found in the usual way as 
f00 
2 
1
3 
MTTF = / o 
R(t)ät 
= J + - 
= 
-

142 
RELIABILITY QUANTIFICATION 
Solution With Laplace Transforms. We can also solve determine the state 
probabilities by using Laplace transforms. The Laplace transform of the reduced 
state equations is 
[P*(s), P*(s)] Au = [sP*(s) - />o(0), sP*(s) - Px(0)] 
Because the voted group is started in state 0, PQ{0) = 1 and P\ (0) = 0, and the 
equation becomes 
[P0*(s),P*(s)]Au 
= [sP0*(s)-
and the solution is 
P*(v\ 
— 
F°{S)-2X+s 
-uw] 
By taking the inverse Laplace transform, we get Λ>(0 and P\(t) as above. 
MTTF. The Laplace transform of the survivor function is 
R*(s) = / 
r>oo 
„St , 
e-stR(t)dt 
/o 
For s = 0, the Laplace transform of R(t) becomes 
R*(0) = / 
e°R(t)dt 
= / 
R(t)dt 
- MTTF 
Jo 
Jo 
The MTTF can therefore be determined by R*(0). Because R(t) = P0(t) + 
Λ ( 0 , the MTTF is 
MTTF = Ä*(0) = P0*(0) + P*(0) 
where P0*(0) and P*(0) are determined from 
[P*(0),P*(0)]Au 
= [-l,0] 
This matrix equation reduces to two linear equations and the solution is 
nm = έ 
Ρί(0) = 2ί>0·(0) = -ί 
The MTTF is therefore 
MTTF = R*(0) = P*(0) + P*(0) = ^ 
+ j = 
^ 

MARKOV APPROACH 
143 
Figure 5.22 
Markov model for the loo2 voted group in Example 5.27 with repair from 
state 1. 
With Repair. Assume now that a single failed channel is repaired with repair 
rate μ, but that state 2 is still an absorbing state. A Markov model for this case 
is given in Figure 5.22. 
The transition rate matrix for this loo2 voted group is 
<-2λ 
2λ 
θ\ 
μ 
-(μ + λ) 
λ 
0 
0 Oy 
As above, the reduced state equations (only involving "up-states") are 
[/>ο(0,Λ(0]ΑΜ = [Ρ0(0,Λ(0] 
where 
' -IX 
2λ 
μ 
-{μ + λ) 
By solving (5.63) with MAPLE® and its LinearAlgebra package, we obtain the 
expressions for Po(t) and Pi (i) (which are extensive and therefore not presented 
here). 
The MTTF for the voted group can be found by Laplace transforms (with 
5 = 0) from 
[P*(0),Pl*(0)]Au 
= [-l,0] 
giving 
ΛΓ(0) = 
Λ*(0) = 
λ + μ 
~2Ä2" 
The MTTF is now 
MTTF = Ä*(0) = PQ (0) + P*(0) = 3λ + μ 
3 
μ 
2λ + 2Ä2 
(5.81) 
The introduction of the repair has therefore increased the MTTF by an additional 
μ/2λ2. 

144 
RELIABILITY QUANTIFICATION 
A Numerical Example. 
Let λ = 5 · 10 4 per hour and let μ = 1/10 per hour, 
such that the MTTR of the channel is 10 hours. The MTTF in (5.81) is then 
MTTF = — + -^r = 3.00 ■ 103 + 2.00 · 105 = 2.03 · 105 
2A 
2Λ2 
* 
" 
' 
" 
» 
' 
1.48% 
98.52% 
The introduction of repair has therefore a dominating effect on the MTTF in this 
case. 
φ 
Procedure for Determining MTTF. It can be shown (e.g., se Rausand & H0yland, 
2004) that the approach we used to determine the system MTTF in Example 5.27 can 
also be used for more complicated systems. The following procedure can be used. 
1. Establish the transition rate matrix A, and let P(t) - [Po(0- Λ(0> · · · · pr(t)] 
be the distribution of the (r + 1) states at time /. 
2. Define the initial state distribution P(0) = [P0(0), Pi(0),..., Pr(0)] and ver-
ify that P(0) represents functioning state. 
3. Identify the failed states of the system, and define these states as absorbing. 
Assume that there are k absorbing states. 
4. Delete the rows and the columns of A corresponding to the k absorbing states. 
The remaining matrix , Α^ has dimension (r + 1 — k) x (r + 1 — k). 
5. Let P*{s) = [PQ(S),P*(S),...,P*(S),] 
denote the Laplace transform of 
P(t) and remove the entries of P*(s) corresponding to absorbing states. Let 
Pu(s) denote the remaining vector. 
6. Let [s P * (s)—P (0)] denote the Laplace transform of the time-derivative of P (t) 
and remove the entries corresponding to absorbing states. Let [sP*(s) — P(0)]u 
denote the remaining vector. 
7. Establish the matrix equation 
Pu(s)-Au 
= 
[sP*(S)-P(0)}u 
set s = 0 and determine P^(0). 
8. The MTTF is determined by 
MTTF= ^ P * ( 0 ) 
where the sum is taken over all j representing the (r + 1 — k) nonabsorbing 
states. 

MARKOV APPROACH 
145 
β 
EXAMPLE 5.28 
Reconsider the 2oo3 voted group with CCFs in Example 5.24. The voted group 
fails in states 2 and 3 in Table 5.3 and these two states are hence defined as 
absorbing. The reduced transition rate matrix is 
_ /-(3λ<''> + λ<Γ>) 
3λ( , )\ 
It is further assumed that the 2oo3 voted groups start out in state 0 (the perfect 
state) at time t = 0, such that Po(0) — 1. The matrix equation for the Laplace 
transform of the transient "up" states is therefore 
[P*(s), P*(s)] ■ Au = [sP*(s) - Ι , ί Λ ' ω ] 
For 5 = 0, the matrix equation is 
[P*(0),P*(0)]-Au 
= [-l,0] 
The equation can be solved by hand or by using MAPLE®. 
P*(0) 
= 
μ,+2Αω+Α<'> 
° 
6λ('")2 + 5λΟ')λΜ + ΧΜμι + 
λ^2 
Ρ*(0) 
3λ« 
>2 , , , / η ι Μ 
. 1 ΟΊ 
. ι Οΐ2 
6λ(') + 5A<'U<C> + χωμι 
+ X^r 
The MTTF for the voted group when starting out in state 0 is 
MTTF = R*(0) = Po*(0) + ^ ( 0 ) = PQ(0) 
3λ(,) +μι+ 
2λ<,-> + λ& 
6λ0)2 + 5Χ^χ(0 
+ XV μι + 
λ(02 
Numerical Example 
Using the same input data as in Example 5.24 yields 
P0*(0) = 1.9598 -105 hours 
P*(0) = 211.5 hours 
MTTF = 1.962 · 105 hours ss 22.40 years 
This is the same answer as in Example 5.26 and is because the system is always 
repaired to state 0. Another repair strategy would not have given the same result. 

146 
RELIABILITY QUANTIFICATION 
OHKJ1 ΘΜ^Ο* 
(a) 
(b) 
Figure 5.23 
Simple Petri net, without (a) and with (b) tokens. 
5.6 
Petri Net Approach 
The Petri net approach was introduced by Carl Adam Petri in the 1960s and is sug-
gested in IEC 61508-6 as a suitable approach for reliability analysis of SIFs. The 
standard IEC 62551 defines the terminology and gives requirements for the use of 
Petri net modeling in reliability analysis. The Petri net approach is, as the Markov 
approach, based on the possible system states and how they change when events 
(e.g., failures, repairs, proof tests) occur (Dutuit et al., 1997; IEC 62551, 2012). 
Petri nets have been studied and applied in many different fields, such as of 
telecommunication, software engineering, and transportation, but in this book, we 
only consider Petri net modeling in reliability analysis. More general information 
about Petri nets is given by, for example, Murata (1989) and David & Alia (2004). 
5.6.1 
Method Description 
Basic Concepts. 
A Petri net (PN) model has two basic nodes: places, drawn as 
circles, and transitions, drawn as bars, as shown in Figure 5.23 (a). Places are used to 
model local states or conditions (e.g., faulty or operating), while transitions are used 
to model local events (i.e., state changes, such as failure or restoration) (IEC 62551). 
Places and transitions are connected by directed arcs, drawn as arrows. A place /?, 
from which an arc directs to a transition tj is called an input place of the transition; 
a place to which an arc directs from a transition is called an output place of the 
transition. In Figure 5.23 (a), p\ is an input place for t\ and /?2 is an output place 
for t\. Similarly, t\ is called an output transition for p\, and an input transition to 
pi- A place can act as an input and output place of a transition at the same time, and 
vice versa. In other words, a directed arc from pi to i, can coexist with a directed 
arc from t, to p,. These names have no formal meaning, but they are important to 
improve the readability of the Petri net model. 
Tokens are dynamic elements in Petri net models, illustrated as black bullets and 
assigned to places as shown in Figure 5.23 (b). Petri nets with tokens, as shown 
in Figure 5.23 (b), are called marked Petri nets. A place /?,· can contain an integer 
number m, > 0 of tokens. The distribution of tokens in the places of a Petri net 
is called the marking (m) of the model. Firing of a transition can result in a new 
marking, and each marking represents a system state. 
This section is coauthored by Associate Professor Yiliu Liu, NTNU. 

PETRI NET APPROACH 
147 
In Figure 5.23(b), the marking of the Petri net is m — {ηΐι,ηΐτ} = {2,0}. The 
original state of the system is the initial marking (mo) of the model, and a marking 
is regarded as reachable from trio if a sequence of transition firings from the initial 
marking results in this marking. The reachability set of a Petri net is the set of all 
markings that are reachable from the initial marking (Malhotra & Trivedi, 1995). 
A multiplicity (or weight) is assigned to each arc as a digit. When the multiplicity 
is one, it is usually not specified in the Petri net. The multiplicity represents the 
number of tokens the arc delivers at a time. The dynamics of a system is described 
by the change of the distribution of tokens; and this change is determined by the 
firing of enabled transitions. A transition is enabled if the number of tokens in each 
of its input places is equal to or greater than the multiplicity of the associated arcs. 
Firing a transition means to remove tokens from its input places and deposit new 
tokens into its output places. The number of tokens removed or deposited must be 
equal to the multiplicity of the corresponding arcs. The distribution of tokens in the 
next state is determined by the structure of the net and the current marking. 
31 EXAMPLE 5.29 Petri net model of a simple chemical reaction 
Two hydrogen molecules (2H2) can under certain conditions react with one oxy-
gen molecule (O2) and create two water molecules (2H20). This reaction is de-
scribed by a Petri net model in Figure 5.24 where a token in place p\ means 
that an oxygen molecule is present, a token in place pi means that a hydro-
gen molecule is present, and a token in place p^ means that a water molecule 
is present. To fire transition t\, we must have at least one oxygen molecule in 
place p\ and two hydrogen molecules in place pi-
Transition t\ in Figure 5.24(a) cannot be fired because place Ρ2 does not 
have any tokens (i.e., no hydrogen molecules are present) such that the number 
of tokens in the place P2 is less that the multiplicity of the arc from Ρ2 to t\. 
The arc from pi to ii has multiplicity 2 to indicate that two hydrogen molecules 
must be combined with one oxygen molecule to create two water molecules. 
The transition in Figure 5.24(b) can be fired, because there are two tokens in 
both input places to transition t\, in fact, there are one oxygen molecule more 
that what is needed. Figure 5.24 (b) illustrates the state before transition t\ is 
fired, and Figure 5.24 (c) shows the state after the transition has been fired. In 
the resulting state, in Figure 5.24 (c), there are one oxygen molecule left in place 
p\, no hydrogen molecules in place p2, and two water molecules in place p3. 
The markings in the three subfigures are m& — {2,0,0}, m^ = {2,2,0}, and 
mc = {1,0,2}. A similar example is presented by Murata (1989). 
Θ 
As illustrated in Example 5.29, firing of a transition does not mean to transfer the 
tokens in the input places of a transition to its output places. It is important to note 
that two actions are included in this process: Removing or withdrawing tokens from 
the input places, and releasing or depositing tokens to the output places. 

148 
RELIABILITY QUANTIFICATION 
Enabled, 
before firing 
(a) 
(b) 
(c) 
Figure 5.24 
Petri net models for a chemical reaction. 
a 
Figure 5.25 
Petri net model with an inhibitor arc. 
Inhibitor Arcs. 
An inhibitor arc (or an inhibitor) is a special type of arcs in the 
Petri net model. An inhibitor is a directed arc from a place to a transition, and its end 
is marked by a small circle (David & Alia, 2004; IEC 62551, 2012). The function of 
an inhibitor is to block its output transition if the number of tokens in the input place 
is higher or equal to the weight of the arc. In other words, it is the absence of tokens 
in the input place, rather than the presence, that enables the transition. This function 
is illustrated in Figure 5.25, where the transition is blocked, or not enabled, because 
the number of tokens in p2 is equal to the multiplicity (weight) of the inhibitor. Even 
if the number of tokens in p\ is sufficient, the transition cannot be enabled until all 
of its input places contain appropriate numbers of tokens. 
Transition t\ in Figure 5.26 is enabled because the inhibitor from pi to t\ has 
multiplicity (weight) 2, which is higher than the number of tokens in pi- Firing a 
transition connected with inhibitors is different from normal transitions. As shown 
in Figure 5.26, after the transition t\ is fired, the number of tokens in the input place 
associated with an inhibitor remains unchanged. No token is withdrawn, and no 
token is added. 

PETRI NET APPROACH 
149 
Before firing 
After firing 
ft 
Figure 5.26 
Transition with an inhibitor arc. 
(a) 
(b) 
Figure 5.27 
(a) A structural conflict, (b) An effective conflict. 
Some Properties of Petri Nets. 
Petri net properties that are relevant for this book 
are presented below. More properties can be found in David & Alia (2004, chap. 2). 
Conflict. A conflict refers to a place that has two or more output transitions.6 A 
conflict is written as K={pi, {ti,t2, ■■■}} and is effective when the transitions are 
enabled, but the number of tokens in pi is less than the sum of weights of its output 
arcs. In Figure 5.27 (a), a (structural) conflict exists in firing transitions ii and i2, 
and in Figure 5.27 (b), the conflict is effective. With this marking, it is impossible to 
know which transition should be fired first. 
In some computer programs for Petri net analysis, the output transition to be fired 
is chosen at random. If a certain transition is required to be fired, the conflict structure 
should be avoided in the modeling of the Petri net. 
Boundedness. 
If there is an integer k such that the number of tokens in place /?, 
is < k for all markings that are reachable from the initial marking m0, pi is said to 
be bounded for /MO (David & Alia, 2004). Furthermore, if all places in a Petri net are 
bounded for nto, the Petri net is said to be bounded for ntQ. 
Figure 5.28(a) is a bounded model, with two possible markings: nto — {1.0} 
and »ii = {0,1}. The Petri net model in Figure 5.28 (b) shows that each time ?2 is 
fired, it releases tokens to p\ and pi, and the number of tokens in p2 continues to 
6David & Alia (2004) call such a conflict a structural conflict. 

150 
RELIABILITY QUANTIFICATION 
(a) 
(b) 
Figure 5.28 
(a) A bounded Petri net. (b) An unbounded Petri net. 
A 
Ί 
Pi 
OH—O 
(a) 
(b) 
Figure 5.29 
Liveness of Petri net models. 
increase. Therefore, p2 is not bounded, and the Petri net model is also unbounded, 
with reachable markings: {1,0}, {1,1}, {1,2}, {1,3},.... 
In stationery analysis of systems, it is necessary to ensure that the Petri net model 
is bounded. Otherwise, the model can have infinite markings. 
► Remark: To make the Petri net model more compact, the two arcs p2 —> t2 and 
t2 —*■ P2 in Figure 5.28(b) can be simplified by using a bi-directional arc between 
p2 and t2. 
® 
Liveness. 
Consider the two Petri net models in Figure 5.29, both of which can be 
used to model the failure of an item. When a token is in p\, the item is functioning, 
while a token in p2 means that the item is in a failed state. Transition t\ indicates the 
failure event. The difference between the two models lies in the post-failure phase. In 
model (a), place p2 has no output arcs, and the marking m\ = {0, 1} is a sink state? 
where no more transition is enabled. In model (b), on the other hand, transition t2 
is enabled when there is a token in p2. Firing of t2 indicates restoration of the item, 
and in the model language, one token is removed from p2 and a token is deposited 
to pi. 
According to David & Alia (2004), if a possibility remains for firing i, regardless 
the evolution of the model, such a transition is a live transition. A Petri net model is 
live if all its transitions are live. In model (a), transition t\ cannot be fired after the 
A sink state corresponds to an absorbing state in Markov modeling. 

PETRI NET APPROACH 
151 
Table 5.5 
Symbols used for transitions. 
Type of transition 
Deterministic 
Delay 
Parameter 
Symbol 
Delay 
is d 
I 
Stochastic 
Exponentially 
or geomet-
rically 
distributed 
Arbitrarily 
distributed 
φ Arbitrary 
distribution 
system enters marking m\ = {0,1}, and this model is hence not a live Petri net. In 
model (b), one transition is enabled at any time, implying a live Petri net. 
As shown in Figure 5.29, a live Petri net can be used to model a repairable item, 
while a Petri net with a sink state is suitable for modeling a nonrepairable item. 
5.6.2 
Timed Petri Nets 
In some cases, there may be a delay, d, from a transition is enabled until it is fired. 
The delay is also called the firing time of the transition. When d > 0, the transition 
is said to be a timed transition. A timed transition is fired when the delay (i.e., the 
firing time) is over. 
Different symbols may be used to illustrate different types of delays, as shown in 
Table 5.5 (adapted from IEC 62551). 
β 
EXAMPLE 5.30 Single Item 
Figure 5.30 shows a Petri net model for a single item. When a token is in place 
p\, the item is functioning, ii is a timed transition representing failure of the 
item. The transition is fired, and the item fails, after a random delay, which is 
the same as the time-to-failure of the item. In Figure 5.30, the symbol used for 
transition t\ means that the time-to-failure distribution has to be specified. When 
transition t\ has been fired, the item is in a nonfunctioning (i.e., fault) state and 
a repair action is initiated. The repair is represented by the timed transition tj 
that has a random delay, which is the repair time of the item. The repair time 
distribution has to be specified. When transition ti is fired, the item is repaired 
and functioning again. 
φ 

152 
RELIABILITY QUANTIFICATION 
Figure 5.30 
Timed Petri net model for a single item. 
(a) 
(b) 
(c) 
Figure 5.31 
Types of markings in a GSPN; tangible in (a) and (c), and vanishing in (b). 
A timed Petri net can have both untimed transitions and timed transitions and the 
delay can be deterministic or stochastic. An untimed transition can be regarded as 
a deterministic transition with zero firing time. For a stochastic transition, the delay 
can (i) be exponentially distributed with constant firing rate, or (ii) have a more 
general probability distribution with associated parameters. 
When all the transitions in a Petri net are stochastic, we have a stochastic Petri net 
(SPN), and this model can be extended to a generalized stochastic Petri net (GSPN) 
when transitions with zero firing time are allowed (Malhotra & Trivedi, 1995). Ac-
cording to IEC 62551, a GSPN contains two types of transitions: immediate transi-
tions and timed transitions with exponentially distributed firing times. In a GSPN, 
the immediate transitions have priority over timed transitions, such that enabled im-
mediate transitions are always fire before timed transitions. Furthermore, two types 
of markings exist in a GPSN: vanishing (at least one immediate transition is enabled 
in this marking, so the system leaves the state in no time after it enters the state), and 
tangible (no immediate transition is enabled, and the system sojourns in the state). 
Consider the Petri net model in Figure 5.31. This is a GSPN with three possible 
markings, as shown in subfigures (a), (b) and (c). The markings in (a) and (c) are 
tangible markings, while the marking in (b) is a vanishing marking, as the immediate 
transition ti is fired in no time after one token enters pj,. 
► Remark: A GSPN can be converted to a continuous time Markov chain (i.e., a 
Markov process), because all transitions between different tangible states have expo-
nentially distributed firing times. 
φ 

PETRI NET APPROACH 
153 
TOP 
0 
I 
1 
I 
2 
Figure 5.32 
A fault tree with a single OR-gate, transformed to a Petri net. 
TOP 
e^ 
1 
2 
Figure 5.33 
A fault tree with a single AND-gate, transformed to a Petri net. 
5.6.3 
Reliability Modeling with Petri Nets 
This section discusses modeling issues of events related to the reliability of a SIF, 
while analysis is discussed in the next section. 
Fault Trees versus Petri Nets. 
Any fault tree with only AND- and OR-gates can 
be transformed to a Petri net, by transforming the logical gates to a specific structure 
with places and transitions. Figure 5.32 shows the Petri net model corresponding to 
a fault tree with a single OR-gate. Here, the marking where a token resides in p^ 
corresponds to the occurrence of the TOP event in the fault tree, while the presence 
of a token in p\ or pi corresponds to a basic event. The OR logic is reflected by the 
fact that a token in p\ or pi can result in deposition of a token in pj. 
Figure 5.33 shows the Petri net model corresponding to a fault tree with a single 
AND-gate. Again, a token in p$ represents the occurrence of the TOP event, and the 
AND logic lies in the enabling condition of the transition t\. Such a transition cannot 
be enabled unless both pi and p$ hold tokens. 
With these transformations, it is straightforward to convert fault tree models and 
reliability block diagrams (RBDs) into Petri net models. 
Modeling of Failures. A failure is an event that changes the state of the system. 
If the cause of the failure is known, the failure process can be modeled as shown in 
Figure 5.34, where a failure occurs when the item is working, and the temperature 
is too high. When there are tokens in both the two places, the transition of failure is 
enabled. Firing the transition removes tokens from the input places, and deposits a 
token to the output place "item faulty." 

154 
RELIABILITY QUANTIFICATION 
Item 
working 
(iK 
■-(^v-
High 
temperature 
Failure 
J 
Item 
faulty 
~vJ 
\ 
) 
V 
Item 
working ex 
High 
temperature 
Failure 
JA 
Item 
faulty 
{ 
J 
v_y 
Figure 5.34 
Modeling of a failure. 
Figure 5.35 
Modeling of DU and DD failures. 
The occurrence of a failure is sometimes modeled as a random event with a prob-
ability distribution. In Figure 5.35, a SIF is considered with two types of failures, 
DU and DD failures. Both DU and DD failures are assumed to occur with constant 
firing rates ADU and ADD, respectively. In the Figure5.35, abbreviations rather than 
numbers are used in subscripts to denote places and transitions, in order to highlight 
the functions. 
When DU and DD failure are independent, they can be modeled by Figure 5.35 (a). 
The model has three parts: 
1. A model for the DU failure consisting of 
- A place puw· When this place has a token, the item is in a working state 
(with respect to DU fault). 
- A transition iou representing a DU failure of the item, with an exponentially 
distributed firing time with rate ADU· 
- A place /?UF· When this place has a token, the item has a DU fault. 
2. A model for the DD failure consisting of 
- A place pnw- When this place has a token, the item is in a working state 
(with respect to DD fault). 

PETRI NET APPROACH 
155 
- A transition ÎDD representing a DD failure of the item, with an exponentially 
distributed firing time with rate ADD-
- A place PUF- When this place has a token, the item has a DD fault. 
3. A model of system failure consisting of 
- A place psw- When this place has a token, the system is working (with no 
DU or DD fault). 
- A transition ?SDU representing a system DU failure. 
- A transition ÎSDD representing a system DD failure. 
- A place PSF- When this place has a token, the system has a dangerous (DU 
or DD) fault. 
Firing fnu removes the token in />uw and deposits a token to /?UF» which means 
that the item has a DU fault. The token in /?UF c a n ß r e ^SDU and absorb the token 
from the working place psw and release a token in /?SF> meaning that the system is 
faulty due to a DU failure. Similarly, the occurrence of a DD failure can also result in 
the deposit of a token into PSF- The function of the bi-directional arc between pup 
and ?SDU is to keep a token in /?UF while the transition is fired, such as to illustrate 
that the fault of the item results from a DU failure. Likewise, the bi-directional arc 
between pOF and ÎSDD is used to denote that a DD failure fails the item. 
As the DU part and DD part do not interact with each other in the model, the bi-
directional arcs connected with psw are used to ensure that the change in one of the 
above two parts does not impact the other one, by always keeping a token in psw-
If DU and DD failures are mutually exclusive, the system can be modeled by 
Figure 5.35 (b), where two inhibitors are used on ?DU and ?DD to block a DD failure 
when a DU failure occurs, and vice versa. In this case, bi-directional arcs connected 
with psw are not necessary, because the item can have only one type of failure at a 
time. In other words, if a DD failure occurs, it is not possible for the item to have a 
DU failure, so that the token in psw is absorbed, and the enabling condition of ÎSDU 
is stopped. 
Modeling a 1oo2 Voted Group Consider a loo2 voted group of two channels. 
First, assume that the two channels are nonrepairable. 
Nonrepairable 1oo2 Voted Group. A Petri net model for a nonrepairable loo2 
voted group is shown in Figure 5.36. In this model, piw and /?2w express the local 
state of channels 1 and 2, respectively, while the presence of a token in psw means 
that the voted group is working. When both pip and p2F hold tokens, ?SF is fired 
and represents failure of the voted group. 
Repairable 1oo2 Voted Group. Assume now that the loo2 voted group is re-
pairable. If the failures are detected immediately and then repaired, the voted group 
can be modeled by the Petri net shown in Figure 5.37. The firing of tm or ?2R means 
the repair of the corresponding channel, and the absence of a token in piF or /?2F 
can fire ism or isR2 together with the presence of a token in PSF-

156 
RELIABILITY QUANTIFICATION 
Psx Θ 
flw 
'IF 
P\f 
çyA-o 
PSF O 
Figure 5.36 
Petri net model for a nonrepayable loo2 voted group. 
Psvf7\*-
ff'sR2 
Figure 5.37 
Petri net model for a loo2 repairable system. 
Repairable 2oo3 Voted Group. Consider a 2oo3 voted group of identical channels. 
A Petri net model for this group is shown in Figure 5.38. 
In the 2oo3 voted group, the transitions ?SF; 0' = 1,2, 3) denote the voting mecha-
nism. When channels 1 and 2 both fail, for example, tokens are in pip and /?2F, and 
isFi is fired. Similarly, if two or more channels are restored, the system returns to a 
working state and deposits a token to psw by firing one of the transitions ?SR; 0' = 
1,2,3). 
5.6.4 
Reliability Block Diagram Driven Petri Net Modeling 
Reliability block diagram (RBD)-driven Petri net modeling is suggested by Signoret 
et al. (2013) to facilitate the use of Petri net modeling (the method is also mentioned 
in IEC 61508 (2010)). When two or more interacting items are modeled, many arcs 
are involved even for very simple logics. The principle of the RBD driven method is 
to draw the Petri net models in a modular way by adding some new elements. 
In Figure 5.37, two inhibitors from /?,w are used to replace the bi-directed arcs to 
is, and they have the same functions because the items have only two states (i.e., if 
the place /?,F holds a token, the place />,w has no token). This way, the model can 
be divided into three sub-Petri nets as shown in Figure 5.39: a model for item 1, a 

PETRI NET APPROACH 
157 
Figure 5.38 
Modeling of a 2oo3 repairable system. 
Figure 5.39 
Structure of the model for a loo2 repairable system. 
model for item 2, and a model for the system. The Petri net model for the loo2 voted 
group is then defined by the three modules and their interactions. 
In Figure 5.40, the arcs between the modules are removed to make the model 
more understandable. In such a figure, each module can be analyzed separately. A 
module can include two parts (Signoret et al., 2013): 
- An intrinsic part, which describes the behavior of the items in the module. 

158 
RELIABILITY QUANTIFICATION 
Figure 5.40 
Modules of the Petri net by using repeated places. 
- An extrinsic part, which describes the links with the outside. Some places be-
longing to other modules are repeatedly considered and plotted as boxes of solid 
lines, for example, those in the module S of Figure 5.40. 
Because the two sub-Petri nets (C1 and C2) for item 1 and 2 are almost identical, 
we use C/ to denote them in Figure 5.41. In addition, interpretations are introduced 
to replace those places occurring in more than one module. Such interpretations, are 
textual messages of the firing conditions of associated transitions and the changes of 
situations after the firings. In other words, in these Petri nets, a transition cannot be 
fired until its input places have enough tokens as well as its firing condition stated in 
the interpretation is satisfied. There are two types of interpretations (Signoret et al., 
2013): 
- Predicate (identified by "??") is a formula that can be true or false (e.g., 
"??1>2" is false, "V.x=y" 
is true when x is equal to v.) 
- Assertion (identified by "!!") is a formula used to update a variable after the 
transition is fired (e.g., "Wnb = nb + 1" updates the value of nb by adding 1.) 
Sometimes, short messages are used to simplify the above notations: 
- "?Mes" means "??Mes==true," while "?-Mes" means "??Mes==false" 
- "!Mes" means "!!Mes=true," while "!-Mes" means "!!Mes=false" 
In Figure 5.41, the boxes (repeated places) ptw are replaced by the description 
for the integer variable nb/?w (the number of items that are working). When the 
predicate "??nft pw==0" is true (the number of working items is zero), the system 
enters a failed state. And then when the predicate "??nè pw > 0" is true, we have 
at least one item that is working, and so the system returns to a functional state. 
The two modules communicate with these interpretations. In module i, firing of 
ί/F (the failure of item i) increases the variable nb ργ by 1, and reduces nb /?w by 

PETRI NET APPROACH 
159 
Figure 5.41 
Modules of the Petri net using predicates and assertions. 
1. On the other hand, Î,R is used to increase the value oînb pw by 1 in each firing. 
It is possible to reach a conclusion that a predicate can be regarded as the input of a 
module from others, and an assertion is the output of the module to the others. 
In reliability analysis of safety-related systems, a system is always regarded to 
have two states: working/failed. Here, we can introduce state variables Ci to rep-
resent these states: Ci is true when the item is working, whereas Ci is false when 
the item is failed. Then, global assertions are adopted to structure Petri net modules 
in a RBD way, and the underlying RBD of the system can be drawn with gray lines 
as shown in Figure 5.42. Global assertions are expressed as statements in rectan-
gles directed to state variables A, B, and S. The updates of global assertions are not 
dependent on specific transitions, but on the changes of state variables of modules. 
In Figure 5.42, S is the state variable of the system, while A and B can be regarded 
as intermediate variables transferring the logical values (true or false) of Cl and C2 
into quantities (1 orO). In the module 1, the firing of ÎIR can result in the value of Cl 
as true (i.e., "!C1"), while the firing of i i W put the value of Cl as false (i.e., "!-Cl"). 
The assertion "!!A=In.Cl" means that A is updated according to the value of Cl, for 
example, when Cl is true, the value of A is set as 1. 
Addition (+) and multiplication (·) signs are used to denote the logical OR and 
AND operations (Signoret et al., 2013). In Figure 5.42, the system state variable S is 
updated to the sum of values of A and B, for example, when tm, and ?2W are fired, 
the value of Cl is true, while C2 is false, therefore A is updated to 1 and B is 0, 
and finally S is 1 + 0 — 1. As a result, the system state can be achieved by using a 
reliability block diagram while taking dynamic characteristics in consideration. 
With the RBD-driven Petri net modeling method, a SIS can be modeled first by 
a reliability block diagram, and then the operation of each item is reflected by an 
interpreted Petri net model. The understandability of Petri net models is improved 
by removing intricate arcs, but the analysis methods for basic Petri nets are still 
valid. More details about RBD-driven Petri net modeling can be found in Signoret 
et al. (2013). 

160 
RELIABILITY QUANTIFICATION 
Figure 5.42 
An RBD-driven Petri net model for a loo2 voted group. 
5.6.5 
Analysis of Petri Net Models 
A Petri net model can be analyzed qualitatively and/or quantitatively, and both can 
use reachability graphs (RGs). The analytical method should be chosen based on 
the purpose of the analysis. Possible analytical methods are indicated in Figure 5.43, 
which is adapted from IEC 62551.8 
Qualitative analysis is used to answer questions concerning what are the possible 
states of a system. Such an analysis is often based on an untimed RG that describes 
reachable markings from the initial state (or marking TWO). The method works when 
the number of reachable markings is limited. If the number of reachable markings is 
too high, or the Petri net is not bounded, structural analyses can be used to check the 
influence of the invariant elements (places and transitions) on the whole model. 
For reliability assessment, a quantitative approach is required to determine to the 
state probabilities. Timed RGs can be used in this analysis and Figure 5.44 shows, 
for example, the RG of the Petri net model in Figure 5.37. Here, ellipses are states 
of the system, and the digits in the brackets are denote the number of tokens held 
by the associated places. In state I, component 1 and 2 are working, and the system 
is therefore working, meaning that piw, P2W and psw hold a token, respectively. 
Arrows describe the transitions between the states. Only tangible markings of the 
Petri net are shown in the RG. 
8IEC 62551 ed.1.0 "Copyright © 2012 IEC Geneva, Switzerland, www.iec.ch." 

PETRI NET APPROACH 
161 
Start 
Qualitative 
Quantitative 
RGis 
analyzable 
Size of RG 
is too big 
RG is 
analyzable 
Size of RG 
is too big 
Reachability analysis 
Exclusively 
exponential distr. 
Arbitrary 
distributions 
Monte Carlo _. 
Monte Carlo 
g. 
Markov 
Markov 
Monte Carlo 
Monte Carlo 
isient analysis 
tionary analysis 
Strationary analysis 
Transient analysis 
Strationary analysis 
Transient analysis 
Figure 5.43 
Analytical methods for Petri net models. 
PIWPIFP2WP2FPSWPSF 
fÖl1010p)|| 
(^ΤίθΟΙΐΟΤ^) iii 
(^foi oi oTp) IV 
Figure 5.44 
The RG of the Petri net model in Figure 5.37. 
The RGs for GSPN models have the same structure as the underlying continuous 
time Markov chains (CTMCs). Constant rates can therefore be specified to tran-
sitions between states, as illustrated in Figure 5.35, and we can use the methods 
introduced in Section 5.5 for the Markov approach to analyze the GSPN models. 
Computer Programs. 
Several computer programs can analyze GSPN models and 
give numerical results. Among these is SHARPE (Symbolic Hierarchical Automated 
Reliability and Performance Evaluator)9 that is developed by the Duke University. 
When using SHARPE, the user has to establish an appropriate Petri net model and 
specify one or more places for which the system performance is to be evaluated. In 
Figure 5.37, for example, the places psw and psr are used to quantify the availabil-
ity of the system. The probability that PSF is empty can be interpreted as the average 
availability of the system. Without these places, the unavailability is equal to the 
probability that both /^w and /?2w are empty, but these values are difficult to obtain 
in some programs. 
http://sharpe.pratt.duke.edu/ 

162 
RELIABILITY QUANTIFICATION 
For Petri net models with general, timed transitions, Monte Carlo simulation is 
recommended by IEC 62551. Many of the Petri net programs can support simula-
tion to obtain estimates. Among these are SPNP (Stochastic Petri Net Package),10 
developed by the Duke University, and GRIF Workshop.11 
The Monte Carlo simulation relies on repeated computations of random quanti-
ties, and provides confidence intervals for the estimated results (IEC 62551). The 
method is very flexible, and suitable for a variety of applications, but the time re-
quired for the simulation is relatively long, especially for rare events in highly reli-
able safety systems. Although some methods have been proposed for accelerating 
the simulation (e.g., see Marseguerra & Zio, 1996; Lagnoux, 2006), efforts are still 
needed to integrate them into Petri net programs. 
► Remark: Compared with Markov models, Petri nets can include more types of 
transitions, to facilitate more realistic modeling. Simulation can be used for achiev-
ing estimation of models. Moreover, a variety of software tools for Petri can support 
analysts who are not very familiar with the programming language. 
® 
5.6.6 
High-Level Petri Nets 
Petri net models may become very complicated. To make the models manageable, 
so-called high-level Petri nets have been introduced based on tokens with complex 
structured data, transitions with algebraic expressions, and so on (ISO/IEC 15909-1, 
2004). 
Several high-level Petri net variants have been developed in recent decades, but 
we suffice by mentioning briefly two of them: colored Petri nets and stochastic ac-
tivity networks both of which have wide ranges of applications and well-developed 
analytical tools. 
Colored Petri Nets. 
A colored Petri net (CPN) is a popular high-level Petri net. In 
the ordinary Petri net models, all tokens in a place are the same, and we always use 
black bullets to draw them. In CPNs, tokens in a place can be marked with different 
colors, such that tokens carrying different types of data are discernible. As a simple 
example, we can add a white token in /?w in Figure 5.37 to represent the second 
item, and then the model can be used to simulate the failure-repair characteristics of 
a parallel system with two items as modeled in Figure 5.37. 
In a CPN, tokens of different colors can be used in a place to represent the sys-
tem state (Jensen & Kristensen, 2009). The colors of the tokens that are removed 
are determined by means of the arc expressions, which are the textual inscriptions 
associated with the individual arcs. As described in Jensen & Kristensen (2009), in 
order to enable a transition: 
it must be possible to find a binding of the variables that appear in the surrounding 
arc expressions of the transition such that the arc expression of each input arc 
http ://people.ee.duke.edu/~kst/software_packages.html 
"http://grif-workshop.com/ 

PETRI NET APPROACH 
163 
evaluates to a multiset of token colors that is present on the corresponding input 
place. 
The CPN method is supported by the CPN Tools, which can analyze basic Petri 
nets, high-level Petri nets, and CPNs. CPN Tools was developed at Aarhus Univer-
sity, Denmark.12 More details about CPNs and analytical methods can be found in 
Jensen & Kristensen (2009). 
Stochastic Activity Networks. 
Stochastic activity networks (SANs) extend the 
stochastic Petri nets. In the SAN models, transitions are called activities, and two 
new elements are introduced: input gate and output gate. Associated with an input 
gate are enabling predicate and input function, and associated with an output gate is 
output function. Gates are introduced to permit more flexibility in defining enabling 
and completion rules (Sanders & Meyer, 2001). In addition, cases for activities are 
used to specify probabilistic choices. The inclusion of such elements is helpful to 
reduce the size of the models, but the tradeoff is the difficulty of understanding the 
model without reading the associated functions. 
Another important concept in SANs is reward. A reward may be accumulated 
in two ways: rate reward (values accumulated when the model spends time in a 
state) or impulse reward (values obtains when activities finish). With the concept of 
reward, it is possible to take costs into consideration when measuring the reliability 
of a system. 
UltraSAN and Mobius are two commonly used tools for modeling and analysis of 
SANs, and more details about these can be found in Sanders & Meyer (2001). 
12Since autumn of 2010, CPN Tools has been managed by the AIS group, Eindhoven University of Tech-
nology, The Netherlands, see h t t p : / / c p n t o o l s .org. 

164 
RELIABILITY QUANTIFICATION 
5.7 Additional Reading 
The following titles are recommended for further study related to Chapter 5: 
- Fault Tree Handbook with Aerospace Applications (Stamatelatos et al., 2002) 
is an authoritative text on fault tree construction and analysis. It is written for 
aerospace applications but is a valuable source also for other application areas. 
- System Reliability Theory: Models, Statistical Methods, and Applications (Rau-
sand & H0yland, 2004) provides a more detailed treatment of all the topics in 
this chapter, with approximately the same notation. It is a good source for fur-
ther study related to this chapter. 
- An Introduction to Stochastic Modeling (Taylor & Karlin, 1998) gives a good 
and easy-to-read introduction to stochastic processes. 
- Petroleum, petrochemical and natural gas industries - Reliability modeling and 
calculation of safety systems (ISO/DTR 12489, 2012) provides a more in-depth 
treatment of the Markov and Petri net approaches than what is presented in this 
chapter. 
- Processus Stochastique et Fiabilité des Systèmes (Cocozza-Thivent, 1997) gives 
an excellent treatment of stochastic processes in reliability assessment, but re-
quires a certain background knowledge in probability theory. The book is avail-
able in French only. 
- Discrete, Continuous, and Hybrid Petri Nets (David & Alia, 2004) gives a thor-
ough introduction on construction and properties of Petri net models. For people 
who are interested in the basics of Petri nets, it is beneficial to read the first two 
chapters of this book. 
- Petri Nets for Reliability Modeling (Schneeweiss, 1999) focuses in the field of 
engineering safety and dependability. This book contains a lot of modeling 
examples. 
- Stochastic Petri Nets: An Introduction to the Theory (Bause & Kritzinger, 2002) 
gives a more detailed description on stochastic process and associated Petri net 
models. Chapter 6 of this book also introduces CPNs. 
- Coloured Petri Nets: Modelling and Validation of Concurrent Systems (Jensen 
& Kristensen, 2009) presents basic concepts and modeling and analysis meth-
ods of CPNs. 
- Stochastic Activity Networks: Formal Definitions and Concepts (Sanders & 
Meyer, 2001) provides a good introduction to SANs. 

CHAPTER 6 
RELIABILITY DATA SOURCES 
6.1 
Introduction 
Provision of relevant input data is an essential part of any quantitative reliability 
analysis. It is also one of the most time-consuming tasks. This chapter gives a brief 
survey of some data sources that are relevant for quantitative reliability analyses of a 
SIS. The quality of reliability data is discussed in Rausand & H0yland (2004, chapt. 
10) and is not covered in this book. 
6.2 Types of Data 
The main types of data that are needed as input to a reliability analysis of a SIS are: 
Technical data, a variety of which is needed in order to understand all the functions 
of a SIS and to establish a reliability model. 
Operational data is needed to understand how elements, channels, and subsystems 
are operating and to establish system models. 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
165 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

166 
RELIABILITY DATA SOURCES 
Reliability data are predictions of failure rates, MTTFs, and so on (e.g., see ISO 
14224, 2006), and may be classified as: 
1. Generic data. This is mainly generic data that has been collected by an or-
ganization and published in a handbook or as a computerized database. A 
number of reliability handbooks/databases are available; some of these are 
listed in Section 6.4. When using the data, care should be taken as it is not 
always possible to ascertain the actual source of the data. The data can be a 
mixture of field and test data, and also data from other sources. The data hand-
books/databases are often issued by neutral organizations that do not have any 
business interest in manipulating the predicted values. 
ECSS-Q-HB-30-08A (2011) suggests a scoring approach that can be used to 
rank the quality of the various reliability data sources, and Cartwright et al. 
(1999) suggest that the reliability prediction methodology should be judged 
based on answers to ten questions. 
2. Manufacturer provided data. This is reliability data for a specific product pre-
pared by the manufacturer of the product or by a consultant, and can be based 
on testing, comparison with similar products, and/or some field experience. 
The consulting company exida has specialized in providing such data based 
on a detailed FMEDA. 
It is sometimes claimed that manufacturer data give significantly lower fail-
ure rates than comparable generic data. The reasons for this may be varying 
equipment quality, the failure modes that are included, and the definition of 
the equipment boundaries. Another reason may be that failures due to exces-
sive stress, mishandling, installation failures, and maintenance errors are not 
included in the manufacturer data. This is understandable inasmuch as manu-
facturers are in the business of selling and do not want to include failures that 
may be attributed to factors external to the equipment itself (ISO/DTR 12489, 
2012). 
3. User-provided data. These are reliability predictions based on recorded fail-
ures at a specific site or in a specific application. The predictions may be based 
on data from maintenance databases, shutdown reports, and so on. End-users 
of SISs are required to collect data from the operation of the SISs and use 
these to update the initial estimates of the PFDavg and/or the PFH for the 
systems in order to verify that the safety integrity level (SIL) is maintained. 
4. Expert judgment. Expert judgment may be the only option for new technol-
ogy, where experience data is not available. Expert judgment elicitation is a 
difficult process and is further discussed, for example, by Ayyub (2001). 
Test and maintenance data. A variety of test and maintenance data is required as 
input to a quantitative SIL analysis. These may, for each channel or subsystem, 
comprise: 
- Proof test interval τ 
- Duration of a test/inspection, MTTI (mean time to inspect) 

FAILURE MODES 
167 
- Mean repair-time, MRT, after a DU fault has been revealed in a proof test 
- The diagnostic test interval TD, that is, the time between two consecutive di-
agnostic tests 
- Mean time to restore, MTTR, a channel after a DD failure,that is, the mean 
time from when a DD failure occurs until its function is restored 
- The diagnostic coverage, DC 
- Coverage of the proof tests 
- Preventive maintenance data (intervals and tasks) 
6.3 
Failure Modes 
Failure modes are usually identified and defined as part of an FMECA (see Chap-
ter 3), but it can sometimes be useful to consult generic sources, such as: 
- FMD-2013 (2013): Failure Mode/Mechanism Distributions. This handbook is 
published by RIAC ( h t t p : / / t h e r i a c . o r g ) and provides lists of failure 
modes and failure mechanisms for a long range of electronic, electromechani-
cal, and mechanical parts. 
- IEC 62061 (2005): Safety of machinery - Functional safety of safety-related 
electrical, electronic, and programmable electronic control systems. Annex D 
of the standard provides a list of relevant failure modes of many electrical and 
electronic components, together with typical failure mode ratios. 
6.4 
Generic Failure Rate Sources 
Some data sources are listed in this section. The list is not comprehensive, nor is it 
set up in a prioritized sequence with respect to quality. Three sources are, however, 
indicated below by a «· as the most relevant for a SIS. 
- IEC 61709 (2011): Electric components - Reliability - Reference conditions 
forfaiture rates and stress models for conversion (ed. 2.0). The standard gives 
guidance on how failure rate data can be used for reliability prediction of electric 
components but does not provide specific failure rate estimates. Stress models 
and stress values are given in order to facilitate prediction of failure rates for 
specific operating conditions. 
- IEC TR 62380 (2004): Reliability data handbook - Universal model for reli-
ability prediction of electronic components, PCBs, and equipment. This tech-
nical report provides constant failure rate estimates, models, and influencing 
factors that can be used to predict application-specific failure rates of mounted 
electronic components. The technical report was formerly a French standard 

168 
RELIABILITY DATA SOURCES 
published by Union Technique d'Electricité (UTE) and known as UTE C 80-
810) 
rs- ISO 13849-1 (2006): Safety of machinery - Safety-related parts of control sys-
tems - Part 1: General principles for design. This is a harmonized stan-
dard for the EU Machinery Directive (EU-2006/42/EC, 2006) and gives in An-
nex C MTTF values for hydraulic, pneumatic, mechanical, and electromechan-
ical components. The reliability estimates are given as Bio values and is the 
mean number of work cycles items can sustain until 10% of them have failed. 
The Biod values are the same number of work cycles when only dangerous fail-
ures are considered. This means that if the time to failure is measured as work 
cycles, then Pi(T > Bio) = 0.90. Annex D gives guidance for estimating 
MTTF for channels. 
- MIL-HDBK-217F (1995) (Rev. F2): Reliability prediction of electronic equip-
ment. This military handbook is withdrawn and rather old, but is still used. 
It provides constant failure rate estimates for electronic equipment together 
with the Part stress method to account for influencing factors, such as for tem-
perature, vibration, and so on. The handbook also includes the Parts count 
method to determine the failure rate of systems with many components. An 
updated and extended version of MIL-HDBK-217F is available from RIAC 
( h t t p : / /www. t h e r i a c . org) and is called 217Plus. 
- OREDA (2009): Offshore Reliability Data. The OREDA project was initiated 
in 1981 to collect and analyze reliability data from offshore oil and gas activi-
ties. The project is still active, and a huge database of estimated failure rate and 
downtimes is now available to the OREDA participants. Several data handbooks 
have been published, containing generic data for specified failure modes. More 
information about OREDA may be found on http://www.oreda.com. 
The standard ISO 14224 (2006) Petroleum, petrochemical and natural gas in-
dustries - Collection and exchange of reliability and maintenance data for 
equipment was developed on the initiative from OREDA and is used as basis 
for data collection and analysis in OREDA. 
•s- SINTEF (2013a): Reliability data for safety instrumented systems, PDS data 
handbook. This data handbook is issued as a companion to the PDS method. 
It provides data dossiers for channels and elements of SISs, mainly used in 
the process and offshore oil and gas industry. In addition to failure modes and 
failure rates, the handbook also gives data for beta-factors and coverage values. 
The data provided mainly originate from OREDA and data supplied by vendors. 
More information may be found on h t t p : //www. s i n t e f .no/pds. 
•s· exida (2007): Safety Equipment Reliability Handbook. The handbook has three 
volumes: (1) Sensors, (2) Logic solvers and interface modules, and (3) Final 
elements. The handbook provides reliability estimates for both manufacturer-
specific and generic equipment. 

GENERIC FAILURE RATE SOURCES 
169 
- NPRD-2011: Nonelectronic Parts Reliability Data. This handbook is published 
by the Reliability Information Analysis Center (RIAC) and provides failure rate 
data for mechanical, electromechanical, and electrical items, sorted by type, 
quality level, and environment. The handbook is provided both in hardcopy and 
as a computerized database. 
- FIDES: Reliability methodology for electronic components. FIDES has been 
developed by a consortium of major French companies. The FIDES methodol-
ogy is based on physics of failures and is supported by the analysis of test data, 
field returns, and existing modeling. The FIDES guide presents a methodology 
for incorporating influencing factors into reliability predictions for electronic 
components. Part V of the FIDES guide is a "reliability process control and 
audit guide" and gives valuable guidance on how to integrate reliability into 
the design process of electronic systems. More information may be found on 
h t t p : / / f i d e s - r e l i a b i l i t y . o r g . 
- Siemens SN 29500: Failure rates of components and expected values method. 
This Siemens standard is based on IEC 61709 and provides failure rate data at 
reference conditions and stress models necessary for parts count and parts stress 
predictions. 
- Telcordia SR332: Reliability prediction procedure for electronic equipment. 
Failure rate predictions by the SR332-procedure are applicable for commer-
cial electronic products. Telcordia SR332 is included in several commercially 
available reliability software packages. 
- MechRel Handbook (NSWC-11, 2011 ): Handbook of reliability prediction pro-
cedures for mechanical equipment. This handbook is developed by the U.S. 
Naval Surface Warfare Center - Carderock Division, and provides failure rate 
models for selected types of mechanical components. More information may be 
found on h t t p : / / w w w . n a v s e a . n a v y . m i l / n s w c / c a r d e r o c k / p u b / 
m e c h r e l / p r o d u c t s / h a n d b o o k . a s p x . 
Additional reliability data sources may be found on h t t p : / /www. n t n u . edu/ 
r o s s / i n f o / d a t a . 
Beta-factors. 
Common-cause failures (CCFs) are very important in most reliabil-
ity analyses of a SIS. In most analyses, the CCFs are modeled by the traditional 
beta-factor model of the multiple beta-factor model (see Chapter 10). In both cases, 
we need to estimate the relevant beta-factor ß. The value of ß is strongly influenced 
by local, plant-specific conditions and it is not likely that we can find a relevant es-
timate for ß in any generic data source. We therefore have to estimate the /J-value 
based on checklists, such as the ones found in IEC 61508-6 and IEC 62061. 

170 
RELIABILITY DATA SOURCES 
6.5 
Plant-Specific Reliability Data 
The technological development of E/E/PE technology is progressing quickly and the 
reliability estimates found in generic data sources is therefore often old-fashioned, 
or only relevant for another environment. 
6.5.1 
MIL-HDBK 217 Approach 
The Part stress and Parts count approach that was introduced by MIL-HDBK-217F 
and used in several of the reliability data sources listed above presupposes that suf-
ficient reliability data for the actual operational and environmental conditions does 
not exist and that we need to predict the reliability based on a set of influencing fac-
tors. The Part stress method is a simple example of a proportional hazards model 
where the actual failure rate λρ for a specific operational and environmental context 
is determined by multiplying the basic failure rate Aß by a number of influencing 
coefficients called covariates and concomitant variables. 
The MIL-HDBK-217F approach may seem simple, but much research has been 
carried out to determine the various coefficients. If the temperature in the given con-
text, for example, is 90°C, the influencing coefficient given in MIL-HDBK-217F 
covers both the effect of the increased temperature and the importance of the tem-
perature as an influencing factor. Similar influencing coefficients have not been de-
veloped for mechanical, electro-mechanical, and more complex equipment. 
The general expression of the proportional hazards model when it is assumed that 
the basic failure rate is constant (i.e., with no wear-out effects) is 
λρ = λΒ·Α(π·ι,π·2,...,7Γ„) 
(6.1) 
where τΐ\, πι,..., 
nm are the influencing coefficients. In some cases, one or more of 
these coefficients may vary with time. 
6.5.2 Approach Proposed by Brissaud et al. (2010) 
In general, a large number of factors may influence the failure rate. Some examples 
of influencing factors are given in Table 6.1. 
A possible approach to estimating a plant-specific failure rate λρ for a given com-
ponent is the following: 
1. Estimate the failure rate λβ in a normal (basic) operating context. This estimate 
can sometimes be found from generic reliability databases. If not, λβ has to 
be estimated based on observed data or expert judgments. The estimate of λ# 
is assumed to reflect the average operating and environmental conditions in the 
relevant industry. 
2. Identify the factors (e.g., from Table 6.1) that are considered to have the highest 
influence on the component's failure rate. The various factors should be studied 
carefully to avoid dependencies, for example, that (i) the influence of a factor 

PLANT-SPECIFIC RELIABILITY DATA 
171 
Table 6.1 
Examples of influencing factors. 
Group 
Design 
Manufacture 
Installation 
Operation 
Maintenance 
Influencing factor 
System type 
Working principle 
Dimensions (size, length, volume, weight) 
Materials 
Component quality (quality requirements, controls) 
Special characteristics 
Manufacturer 
Manufacturing process (procedures, controls) 
Location (access facilities) 
Assembly/activation (procedures, control) 
Type of loading (cyclic, random) 
Frequency of use 
Loading charge/activation threshold 
Electrical loading (voltage, intensity) 
Mechanical constraints (vibration, friction, shocks) 
Temperature 
Corrosion/humidity 
Pollution 
Other stresses (electromagnetic, climate) 
Performance requirements 
Failure modes (recorded failures) 
Frequency of preventive maintenance 
Quality of preventive maintenance 
Quality of corrective maintenance 
Source: Adapted from Brissaud et al. (2010). 
takes place only through another factor and that (ii) it is the combined effect of 
two factors that is important and not the single factors. 
fl 
EXAMPLE 6.1 
Shutdown valve in an oil pipeline 
Consider a shutdown valve in an oil pipeline. The essential function of the 
valve is to stop the flow of oil in the pipeline if a hazardous event occurs 
on the downstream side of the valve. The ability to close a valve tightly 
is dependent on the wear of the valve seat and seals, and the failure rate 
of the failure mode "leakage through the valve in closed position" is found 

172 
RELIABILITY DATA SOURCES 
to be influenced by the flow rate through the valve and the content of sand 
particles in the fluid. A careful analysis shows that the main influencing 
factor is the combination of these two factors: high flow rate in combination 
with a high sand content in the fluid. When this combined factor is taken 
into account, the two single factors have a negligible influence. 
φ 
For the purpose of the remaining analyses, the number of influencing factors 
should be kept as small as possible. Let the k remaining influencing factors be 
denoted by y\, yi,..., 
y^, and let j?o,i > Jo,2. · · ·, yo,k denote the normal (i.e., 
industry average) level of the influencing factors. 
3. Weigh the influencing factors according to their importance for the failure rate. 
This has to be done by several experts based on physical and engineering knowl-
edge. Let ω, denote the weight of influencing factor y,>, for 1 = 1,2,..., k. The 
weights are allocated such that X), = 1 ω, = 1. 
4. Record the current (i.e., plant-specific) level of the influencing factors and de-
note these by yc,i,yc,2, · · ·. yc,k- Determine a score acj for the current level 
of influencing factor y,, for i = 1,2,... ,k, such that 
ac>i = 1, when>>c,; s» y0J 
acj < 1, when ycj is considered to be more benign than y0j 
ocj > 1, when ycj is considered to be more hostile than yo,i 
This means that when all the influencing factors are similar to the industry av-
erage, aCti = 1 for all i, and λρ = λβ, meaning that the basic (i.e., industry 
average) failure rate λβ can be used as plant-specific failure rate. 
5. The plant-specific failure rate λρ is then determined by 
k 
k 
λρ = λΒ ■ Y\ ω,- · ac,,· = λΒ · Υ\ πι 
(6.2) 
( = 1 
( = 1 
where π, — ω,- · ac,i denotes the influencing coefficient for factor j , . 
6.6 
Data Dossier 
When performing a reliability assessment of a SIS, it is important to document all the 
input parameters that are used in the calculations. It is therefore recommended that a 
data dossier be set up that presents and justifies the choice of data for each element 
or channel of the system. An example of such a data dossier is shown in Figure 6.1. 
The data dossier format used in the PDS data handbook (SINTEF, 2013a) is another 
good alternative. In many applications, a simpler data dossier may be used. 

DATA DOSSIER 
173 
Component: Hydraulically operated 
gate valve 
System: Pipeline into pressure vessel A1 
Description: The valve is a 5-inch gate valve with a hydraulic "fail safe" actuator. 
The fail safe-function is achieved by a steel spring that is compressed by hydraulic 
pressure. The valve is normally in the open position and is only activated when the pressure 
in the vessel exceeds 150 bar. The valve is function-tested once a year. After a function test, 
the valve is considered to be "as good as new." The valve is located in a sheltered area 
and is not exposed to frost/icing. 
Failure mode: 
Failure rate (per hour): 
- uoes not close on command 
- Leakage through the valve 
in closed position 
- External leakage from valve 
- Closes spuriously 
- Cannot be opened after closure 
3.3x10 6 
1.2 x10 6 
2.7x10 6 
4.2x10 7 
3.8x10 6 
7.8x10 6 
1/300 
Source: 
Source A 
Source B 
Source A 
Source A 
Source A 
Source B 
Expert judgment 
Assessment: 
The failure rates are based on sources A and B. The failure rate for the failure mode 
"cannot be opened after closure" is based on the judgments from three persons with 
extensive experience from using the same type of valves and is estimated to one such 
failure per 300 valve openings. Source B is considered to be more relevant than source A, 
but source B gives data for only two failure modes. Source B is therefore used for the failure 
modes "does not close on command" and "closes spuriously", while source A is used for the 
remaining failure modes. 
Testing and maintenance: 
The valve is function-tested after installation and thereafter once per year. The function 
test is assumed to be a realistic test, and possible failures detected during the test are 
repaired immediately such that the valve can be considered "as good as new" after the test. 
There are no options for diagnostic testing of the valve. 
Comments: 
The valve is a standard gate valve that has been used in comparable systems for a long 
time. The data used therefore have good validity and are relevant for the specified 
application. 
Figure 6.1 
Example of a reliability data dossier (Reproduced from Rausand (2011) with 
permission from John Wiley & Sons). 

174 
RELIABILITY DATA SOURCES 
6.7 Additional Reading 
The following titles are recommended for further study related to Chapter 6: 
- ECSS-Q-HB-30-08A (2011): Space product assurance - Components reliabil-
ity data sources and their use. This handbook discusses the quality of reliability 
data and suggest a scoring system to rank the various sources, and also gives a 
survey of some data sources, partly overlapping with the list provided in this 
chapter. 
- Petroleum, petrochemical and natural gas industries - Collection and exchange 
of reliability and maintenance data for equipment (ISO 14224, 2006) is an im-
portant standard for collection of reliability data. 
- EPSMA (2005): Guidelines to understanding reliability prediction. This guide-
line gives a brief introduction to failure rate prediction and the various predic-
tion models. 
- Rausand & H0yland (2004): System Reliability Theory: Models, Statistical 
Methods, and Applications. Chapter 10 of this book gives a survey of reliability 
data issues and discusses challenges related to data analysis and data quality. 

CHAPTER 7 
DEMAND MODES AND 
PERFORMANCE MEASURES 
7.1 Introduction 
This chapter explores the main assumptions that have to be clarified as a starting 
point for the quantitative reliability analyses in Chapters 8-12. The chapter further 
deals with the choice of reliability performance measure (i.e., PFDavg or PFH) for a 
SIF. This choice determines the analytical methods and formulas that may be used 
to quantify the reliability of the SIF and it is important that we have a thorough 
understanding of the options. The choice of analytical method depends on how often 
the SIF is demanded, the architecture of the SIS, and the testing and operational 
strategies. Finally, an input data set is presented as a basis for the worked examples 
in the following chapters. 
7.2 Mode of Operation According to the IEC Standards 
The main focus of the presentation in the remaining chapters is on the requirements 
in the generic standard, IEC 61508, and the sector-specific standards for the process 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
175 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

176 
DEMAND MODES AND PERFORMANCE MEASURES 
industry (IEC 61511) and for machinery systems (IEC 62061). The three IEC stan-
dards have different focus and interpretation of the operating mode of a SIF. 
7.2.1 IEC 61508: The Generic Standard 
IEC 61508 classifies a SIF into one out of three operating modes according to how 
often the safety function is demanded: 
(a) Low-demand mode of operation (i.e., demands occur no more often than once 
per year) 
(b) High-demand mode of operation (i.e., demands occur more often than once per 
year) 
(c) Continuous mode of operation (in this mode, the SIF continuously prevents the 
occurrence of a specific type of hazardous events) 
IEC 61508 does not distinguish clearly between the two last modes and refers to 
these as high-demand/continuous mode of operation. 
The boundary demand rate, Ade, of once per year (i.e., Ade
 % 1.15 ■ 10-4 per 
hour) that distinguish low-demand from high-demand mode is given in IEC 61508 
without any clear justification. Several authors have questioned the appropriateness 
of this boundary demand rate. 
IEC 61508 requires the reliability performance of a SIF that operates in low-
demand mode be expressed by the average probability of (dangerous) failure on 
demand, PFDavg, and the reliability performance of a SIF that operate in high-
demand/continuous mode be expressed by the average frequency of dangerous fail-
ures per hour, PFH. This distinction has been questioned by several authors (e.g., 
see SINTEF, 2013b) and it is straightforward to show that the two measures will lead 
to "strange results" around the boundary demand rate (i.e., once per year). 
7.2.2 IEC 61511 : For the Process Industry 
The sector-specific standard IEC 61511 deviates from IEC 61508 and distinguishes 
between only two modes of operation. 
(a) Demanded mode of operation (i.e., no distinction between low-demand and 
high-demand). 
(b) Continuous mode of operation. 
The standard states that most SIFs in the process industry are operated in de-
manded mode, and hence does not focus much on the continuous mode of operation. 
We may therefore say that IEC 61511 is a standard for SIFs that operate in demanded 
mode. The standard refers to the EUC as the process. 
IEC 61511 (2003) further states in paragraph 9.2.3 that the SIL requirements for 
demanded systems can be specified by either PFDavg or PFH. It is therefore up to 

FUNCTIONAL CATEGORIES 
177 
the user to decide which safety unavailability measure to use. If the PFH is chosen, 
paragraph 9.2.3 requires that neither the proof test interval τ nor the demand rate Ade 
be used in the quantification of PFH. 
7.2.3 
lEC 62061 : For Machinery Systems 
The sector-specific standard IEC 62061 (2005) is concerned with SIFs that operate in 
high-demand and continuous mode and uses PFH to express SIL requirements. The 
standard refers to a SIS as a safety-related electrical, electronic, and programmable 
electronic control system, SRECS. A safety-related function that is implemented by 
a SRECS is called a safety-related control function, SRCF. The EUC is called the 
machine or the machinery. 
Low-demand mode of operation is not considered to be relevant for SRECS used 
in machinery systems. 
7.2.4 The PDS Method 
The PDS method (SINTEF, 2013b) follows the recommendations in IEC 61508, but 
the most recent edition argues that PFDavg is the most suitable reliability perfor-
mance measure for all demanded safety functions, and that PFH should be used only 
for SIFs that are operated in continuous mode. 
► Remark: It is important to note that all the standards determine the demand mode 
solely by the demand rate and not the demand duration. 
Φ 
7.3 
Functional Categories 
Among the three IEC standards mentioned above, it is only IEC 61511 (2003) that 
distinguishes between demanded mode and continuous mode of operation. The other 
two standards consider high-demand and continuous mode of operation as one cate-
gory, and therefore do not discuss how to distinguish these two subcategories. After 
having made the classification, IEC 61511 states right away that continuous mode of 
operation has limited interest for the process industry and gives no further explana-
tion. 
It is not clear for all practitioners what is really meant by continuous mode of 
operation. Does the SIF have to operate all the time, or is it operating in continuous 
mode when it is used, say, every ten minutes? Are, for example, the brakes of an 
automobile operating in continuous mode, or is it high-demand mode? 
In line with IEC 61511, we suggest classifying SIFs in two categories: 
(a) Safety-related protective functions. These functions are dormant during normal 
operation and are only activated when a demand occurs. The safety-related sys-
tem that implements a safety protective function is an add-on to the EUC and 
is not required to perform other than safety-related functions. A safety-related 

178 
DEMAND MODES AND PERFORMANCE MEASURES 
protective function may be classified as a reactive safety-related function, inas-
much as its objective is to stop or mitigate the consequences of a deviation that 
has already occurred. 
A reactive SIF is operated in a demanded mode. Whether or not it is necessary 
to distinguish between low-demand and high-demand mode of operation may 
be discussed. 
(b) Safety-related control function. These functions are continuously controlling so 
that dangerous situations do not occur. The equipment that performs a safety-
related control function is often part of the EUC control system as a supplement 
taking care of safety-related issues. This equipment may therefore be called 
safety-related parts of the EUC-control system. A safety-related control system 
is always alert, but does not necessarily need to perform any physical control 
action on a continuous basis. A safety-related control function may be classified 
as a proactive safety-related function, as its objective is to prevent that safety-
critical deviations occur. A proactive safety-related control function is said to 
operate in continuous mode. 
7.3.1 
Difference Between Low-Demand and High-Demand 
Low-Demand Systems. 
Low-demand SIFs are proof-tested at regular intervals of 
length τ, usually in the order of 6 months to 2-3 years. The test interval is decided 
based on the dangerous failure rate of the system and the demand rate and is normally 
so short that the PFDavg becomes rather low. In this case, there is a fair chance that 
a DU fault is revealed and corrected before a demand occurs such that a hazardous 
event is avoided. This situation is illustrated in Figure 7.1 for a single channel that 
experiences a DU failure. The mean time the DU fault is present in the interval is 
E(Di) = r/2 (see Chapter 8), and the the (average) probability that the DU fault is 
revealed before a demand occurs is e~*-<ieT/2. 
β 
EXAMPLE 7.1 A single channel operating in low-demand mode 
Consider a single channel that is proof-tested with test interval τ = 8 760 hours 
and assume that demands occur on the average once per five years. Because five 
years is 5 · 8 760 = 43 800 hours, the demand rate is Aae = 1/43 800 hours, or 
2.28-10"5 per hour. 
The average probability that the DU fault is revealed before a demand occurs 
is approximately 
Pr(DU fault revealed before demand occurs | DU fault present) 
% e - ^ d e r / 2 
% 
0 
9 0 5 
Θ 

OPERATIONAL STRATEGIES 
179 
' DU failure 
Figure 7.1 In low-demand mode, a demand may occur while a DU fault is present. 
High-Demand Systems. 
For a SIF operating in high-demand mode, it is likely that 
a demand occurs before the DU fault is revealed by the proof test, and the rationale 
for proof-testing has therefore been questioned. Proof-testing can, however, be justi-
fied for voted groups with hardware fault tolerance HFT > 1 because DU faults and 
redundant element faults may be reveled even if the SIF is functioning. 
It is sometimes argued that successful handling of demands may be given credit 
as proof tests and that this may be an argument for skipping periodic proof-testing. 
This is briefly discussed in Chapter 4. 
7.4 Operational Strategies 
EUCs and SIFs can be operated in many different ways and this can influence our 
choice of method for reliability analysis of the SIF. The variety of operational strate-
gies makes it impossible to present detailed approaches and formulas that cover all 
possibilities. The user may therefore need to adapt the SIF reliability formulas pre-
sented in Chapters 8-12 to the actual operational conditions and procedures. Some 
operational issues are mentioned briefly in the following. 
7.4.1 Safe State 
The concept safe state was introduced in Chapter 2. One or more safe states must be 
defined for each EUC. If a hazardous event occurs in the EUC, a planned procedure 
must be followed to bring the EUC to a safe state. The time available for a safe 
transition to a safe state is sometimes called the process safety time. If the transition 
time exceeds the process safety time, an accident may occur. 
7.4.2 Detected Dangerous Faults 
End-users may use different strategies when a dangerous SIF fault is detected. A 
dangerous SIF fault means that the safety loop is no longer able to carry out the SIF 
and the EUC is not protected by the SIF. Among these strategies are the following: 
1. Immediately bring the EUC to a safe state when a dangerous SIF fault is de-
tected. 

180 
DEMAND MODES AND PERFORMANCE MEASURES 
2. Bring the EUC to a safe state when a dangerous SIF fault is detected, if the 
function cannot be restored within a specified time interval of length to. The 
length of the interval is determined based on the demand rate and the process 
safety time. The probability of successful restoration of the SIF within the 
allowed interval is dependent on the maintainability of the channels and the 
available maintenance resources. 
3. Bring the EUC to a safe state when it is detected that the SIF is in a specified 
degraded mode, but still functioning. This strategy may, for example, be fol-
lowed when two DD faults are detected in a 2oo4 voted group. In this case, the 
voted group is still functioning as a 2oo2 voted group, with a significantly lower 
reliability than the initial 2oo4 voted group. 
If a dangerous SIF fault is detected and the EUC is immediately brought to a safe 
state, the fault may not be dangerous for the EUC and detected dangerous SIF faults 
may therefore be disregarded in the SIF reliability analysis. 
What is meant by immediately in item 1 depends on several factors, among which 
is the demand rate. The diagnostic testing of modern sensor and logic solver subsys-
tems is usually carried out so often that the time to detect a DD fault is negligible. 
The time required to bring the EUC to a safe state varies significantly and may range 
from a few seconds up to several hours. We may distinguish between the following 
cases: 
(a) Low-demand mode. For a low-demand SIF, the probability that a demand will 
occur during the transition time to a safe state is normally negligible. 
(b) High-demand mode. The probability of a demand during the transition time de-
pends on the demand rate and it may, therefore, be necessary to include the 
effects of detected dangerous SIF faults in the SIF reliability assessments, espe-
cially for SIFs with very high demand rate. 
(c) Continuous mode. For a SIF operating in continuous mode, a hazardous event 
occurs almost immediately when a dangerous SIF failure occurs. For some 
EUCs, it may, however, be possible to intervene by other means and stop the 
EUC before an accident occurs. The time available and the likelihood of a 
successful intervention have to be evaluated in each particular case. 
7.4.3 Testing and Repair 
Proof-testing of a channel may be done offline or online. An offline proof test is 
carried out while the EUC is in a safe state. Some channels are designed or located 
such that offline testing is the only possibility. In an online proof test, the channel 
is isolated from the rest of the SIF and the EUC may be operated with the SIF in a 
degraded mode during the test. 
If a DU fault is revealed in an offline proof test, the repair action is usually also 
carried out offline. If, on the other hand, a DU fault is revealed in an online proof test, 
the repair action may be carried out online or offline depending on the system layout 

RELIABILITY MEASURES 
181 
and the risk related to the repair action. The voting may or may not be reconfigured 
to improve the SIF reliability during the proof test and possible repair action. 
fl 
EXAMPLE 7.2 Proof-testing of 2oo3 voted group 
Consider a 2oo3 voted group that is a sensor subsystem in a safety loop. If 
one channel is isolated for proof-testing, the two remaining channels will, as 
a default, operate as a 2oo2 voted group, which is an unreliable configuration. 
To improve the SIF reliability, the voting may be reconfigured to a loo2 voted 
group, and this configuration has a higher SIF reliability than the original 2oo3 
voting. 
® 
In many applications, it is assumed that the proof-testing is perfect and that the 
safety loop is as-good-as-new after each test. This assumption is often not realistic, 
and both the proof test and the repair actions may be partial or imperfect. In some 
cases, the end-user may deliberately postpone a repair of a partial failure till after the 
next proof test. This may, for example, be the case when the proof test reveals that 
a shutdown valve has a slightly too long closing time, but still close to the boundary 
of acceptable limits. 
In some applications, comprehensive overhauls of the EUCs and the SISs are 
carried out, for example, as part of turnarounds. These overhauls should be included 
in SIF reliability analyses. 
7.4.4 
Safety during Repair 
When a fault is revealed in a diagnostic test or a proof test, the failed items must be 
repaired. Several safety strategies may be followed in these tests and among these 
are the following: 
1. Bring the EUC to a safe state whenever a repair action is carried out. 
2. Bring the EUC to a safe state during the repair of a dangerous SIF fault, but not 
when the SIF is able to function. This strategy allows operating the EUC with 
the SIF in a degraded mode during repair of channel faults that do not constitute 
a dangerous SIF fault (see Example 7.2). 
3. Always continue EUC operation, irrespective of whether the SIF is functioning 
or not. 
These strategies are also discussed by SINTEF (2013b). 
7.5 
Reliability Measures 
Several quantitative measures can be used to judge the reliability of a SIF. The main 
measures are: 

182 
DEMAND MODES AND PERFORMANCE MEASURES 
- Average probability of (dangerous) failure on demand (PFDavg) 
- Average frequency (per hour) of dangerous failures (PFH) 
- Hazardous event frequency (HEF) 
- Risk-reducing factor (RRF) 
- Spurious trip rate (STR) 
- Safe failure fraction (SFF) 
Except for the spurious trip rate, all these measures were introduced briefly in Chap-
ter 2. As a basis for the remaining chapters of the book, we need to introduce the 
measures in more detail. 
7.5.1 Probability of Failure on Demand 
The average probability of failure on demand, PFDavg, is the most commonly used 
reliability measure for a SIF that operates in demanded mode, notably in low-demand 
mode, and is discussed in detail in Chapter 8. Here, giving a definition of PFDavg 
will suffice. 
The average probability of (dangerous) failure on demand, PFDavg, is the average 
probability that the item (SIS, subsystem, voted group, or channel) is not able 
to perform its specified safety function if a demand should occur. 
The PFDavg is the same as the average unavailability of the item (see Appendix 
A) and is equal to the long-term average proportion of time where the item is not able 
to perform its safety function. Consider a time interval of length t and let E[D(t)] 
be the average downtime of the item in this interval. The downtime is here the 
accumulated time where the item is not able to perform its safety function. We then 
have 
E[D(t)] 
PFDavg = 
L W J 
(7.1) 
The probability of failure on demand can also be given as a function of time, 
PFD(r). If the item can be considered to be as-good-as-new after each proof test, 
the long-term average of PFD(i) is equal to the average of PFD(i) over a proof test 
interval (0, r). 
PFDavg = - f PFD(f) dt 
(7.2) 
τ Jo 
E[D(x)] 
In this case, we have 
If 
t Jo 
PFD(i) dt = 
such that 
/ ' 
Jo 
E[D(T)] 
= / PFD(0 dt 
(7.3) 
Jo 
Detailed formulas for the PFDavg for various types of system configurations are 
provided in Chapter 8. 

RELIABILITY MEASURES 
183 
7.5.2 Average Frequency of a Dangerous Failure per Hour 
The average frequency of dangerous failures per hour (PFH) is a reliability measure 
for items operated in high-demand or continuous mode, and is further discussed in 
Chapter 9. Here, giving a definition of PFH will suffice. 
The average frequency of a dangerous failure per hour, PFH, is the average frequency 
of dangerous failures (of the SIF) to perform the specified safety function over a 
given period of time. It is presupposed that PFH is given per hour. 
PFH was called "probability of dangerous failure per hour" in the 1997 version of 
IEC 61508, and this name is the basis for the abbreviation PFH. The abbreviation is 
still used even though the name has changed. 
The PFH can be considered a function of time, PFH(/), but is most often given as 
an average value over a period of time. Formally, we should have written PFHavg, 
but the reference to the average is usually suppressed and implicitly assumed. 
PFH(i) is similar to the rate of occurrence of failures (ROCOF), which is used 
in the general reliability literature. The ROCOF is an unconditional failure rate of 
an item at time t and is often denoted by w(t). When the ROCOF is restricted to 
dangerous SIF failures, we have 
PFH(r) = w(t) 
(7.4) 
Consider a long time interval (0, fo). The average frequency (or rate) of dangerous 
failures in (0, to) is given by 
PFHavg(0, tQ) = - f ° w(t) dt 
(7.5) 
h Jo 
If the PFHaVg (0, h) tends to a limit when t0 -*■ oo, we may use PFHavg(0, îo) as 
an approximation for the long-term PFH. It is, however, not obvious that PFHavg(0, to) 
will tend to such a limit. 
7.5.3 
Hazardous Event Frequency 
When a demand for a SIF occurs and the SIF is not able to function, a hazardous 
event occurs. The hazardous event frequency (HEF) is defined as: 
Hazardous event frequency (HEF). The average frequency of hazardous events per 
hour. 
For a low-demand SIF where the demand has no duration, the HEF is given as 
HEF = PFDavgAde 
(7.6) 
where Aje denotes the demand rate. 
For a high-demand system, HEF will, in the same way, be a function of the de-
mand rate, and will generally fulfill 
HEF < PFH 
(7.7) 

184 
DEMAND MODES AND PERFORMANCE MEASURES 
It may be argued that HEF is a more relevant performance measure than the 
PFDavg and the PFH. HEF is, however, a problematic measure for the system inte-
grators since a precise estimate of the demand frequency is required, which is usually 
beyond the control of the system integrator. 
7.5.4 
Risk-Reduction Factor 
The risk-reduction factor (RRF) is a measure that can be used to assess the benefit 
of implementing a SIF, and is defined as 
Risk-reduction factor (RRF). RRF is the fraction of the frequency of hazardous events 
that will occur when a specified SIF is not implemented relative to the frequency 
of hazardous events that will occur with the specified SIF. 
R R F = H E F w i t h o u t S I F 
(7.8) 
HEFwith siF 
In general, RRF is greater than 1 and the SIF will be more beneficial the higher 
the value of RRF. If RFF = 1, the SIF has no risk-reducing effect. 
7.5.5 
Spurious Trip Rate 
A spurious trip is an unintended activation of a SIF without the presence of a specific 
demand. A spurious trip will sometimes but not always cause shutdown of the EUC. 
Spurious trips are often assumed to occur according to a homogeneous Poisson pro-
cess with rate STR, denoting the spurious trip rate. Let Ν$τ(0 denote the number 
of spurious trips in an interval of length /. The probability of exactly n spurious trips 
in an interval of length t is 
Pr(JVST(i) = n) = ( S T R ' ° V S T R - ' for n = 0,1,... 
n\ 
With this notation, the spurious trip rate is 
STR = ElNft)] 
(7.9) 
The mean number of spurious trips in an interval of length t is then 
E[NST(t)] = STR -t 
Spurious trips and spurious activations are barely mentioned in IEC 61508 (2010), 
but are treated in some detail in Chapter 12. There are several reasons why the 
spurious trip rate should be kept as low as possible. Among these are the following: 
- Spurious trips interrupt the production or service provided by the EUC and may 
imply high cost. 

RELIABILITY MEASURES 
185 
- Shutdown and start-ups of an EUC are sometimes hazardous events. The EUC 
risk may therefore increase with increasing STR. 
- Many spurious trips require manual intervention to restore the SIR Manual in-
terventions may lead to human errors and higher likelihood of common-cause 
failures (CCFs). 
- Many spurious trips may lead to loss of confidence in the SIF, and the EUC 
end-user may be tempted to remove the SIF. 
7.5.6 
Safe Failure Fraction 
The safe failure fraction (SFF) is introduced in IEC 61508 as a measure used to 
determine the required hardware fault tolerance (HFT). The SFF is defined as 
Safe failure fraction (SFF). The ratio of the failure rates of safe and DU failures of 
an element (of a SIS) relative to the average rate of all safe and dangerous failures 
of the same element. 
The SFF is calculated as 
S F F = A s + ADD 
( 7 1 0 ) 
As + AD 
IEC 61508 requires SFF be calculated for the subsystems of a SIS. The SFF takes 
into account the inherent tendency of a SIF to fail towards a safe state. For example, 
when a fuse blows, there is a failure but it is highly probable that the failure will be 
to an open circuit which, in most cases, would be a "safe" failure. It is important 
to realize that the only types of failures to be considered are those that could affect 
the safety function. This means that no part failures and annunciation failures (see 
Section 3.5.2) shall not be considered when calculating the SFF. 
The SFF is an element property and does not depend on the implementation (ex-
ida, 2010) and the SFF value will therefore normally be supplied by the manufacturer. 
Let λχ = As + AD denote the total failure rate of the item. Equation (7.10) can 
then be expressed as 
S F F = A T - A D U 
= 
I _ A D U 
( 7 U ) 
Λχ 
Λχ 
The SFF is therefore fully determined by the fraction of DU failures among all (rel-
evant) failures of the item. 
Equation (7.11) can also be written as 
ADU = ( 1 - S F F ) A T 
(7.12) 
such that (1 — SFF) is the fraction of all channel failures that are DU failures. 
Many electronic safety devices have built-in diagnostics such that most dangerous 
failures become DD failures and will therefore have a high SFF, often greater than 
90%. Mechanical safety devices, for which diagnostics is not feasible, will have a 
low SFF. 

186 
DEMAND MODES AND PERFORMANCE MEASURES 
IEC 61508-2 defines four categories of the SFF of an element as follows: 
Category 
Interval 
1 
< 60% 
2 
60 - 90% 
3 
90 - 99% 
4 
> 99% 
The SFF concept has been criticized by several authors, and some of these (e.g., 
Yoshimura & Sato, 2008; Lundteigen & Rausand, 2009) claim that the SFF should 
not be used, or at least only used with great care. 
7.6 
PFDavg versus PFH 
When a SIF is demanded, irrespective of the time between the demands, it seems 
natural to specify the reliability of the relevant safety-related function in terms of 
its ability to handle the demand. In this sense, the intuitive reliability performance 
measure for a safety-related protective function would be PFDavg. This has been 
discussed in several publications and is also the current view of the PDS method 
(SINTEF, 2013b). 
If the SIL requirement for a safety-related function is specified using either PFDavg 
of PFH, the same reliability performance measure should always be used by the sys-
tem integrator of the SIF. 
β 
EXAMPLE 7.3 SIL handling in the Norwegian petroleum industry 
The petroleum industry is a process industry and the industry can therefore use 
IEC 61508 and IEC 61511. The terminology in IEC 61511 has, however, be-
come the standard terminology in this industry. The SIL allocation is usually 
performed by the petroleum company and a system integrator is contracted to 
develop a SIS with one or more SIFs with specified SIL requirements that are 
included in the safety requirement specification (SRS). Let us assume that a SIF 
is specified with a SIL 3 requirement and that the petroleum company has es-
timated the demand rate to be once per 11 months. If the petroleum company 
decides to use IEC 61508, this means high-demand mode of operation, and the 
quantitative reliability requirement is PFH < 10~7 per hour. If, on the other 
hand, the petroleum company decides to use IEC 61511, they may choose be-
tween PFH and PFDavg. The quantitative reliability requirement can therefore 
be as above, or specified as PFDavg < 10~3. 
This choice is problematic since the requirements in the SIL tables are not 
calibrated. A specific design may fulfill the required SIL when one reliability 
measure is used, but not when the other measure is used. 
Θ 

PLACEMENT OF THE SIF 
187 
( OK 
V 0 
Barrier system 
Failure 
Restore 
D 
y 
1 J 
Demand 
Figure 7.2 
Ultimate safety barrier failure leading to an accident. 
f OK 
V 0 
Barrier system 
Failure 
T " " ^ 
^ 7 
D 
ï 
J^< 
-^V 1 J 
K 
Restore 
^ ^ ^ ^ ^ ^ 
Restore 
Demand 
Figure 7.3 
Intermediate safety barrier failure leading to a hazardous event. 
For a SIF that is operating in continuous mode, the safety-related control function 
is lost when the safety-related system has a dangerous failure and a hazardous event 
will, most likely, occur. In some cases, the failure of the SIF may be detected by 
an operator who is able to bring the EUC to a safe state before the hazardous event 
occurs. If we disregard this possibility, the hazardous event will occur whenever the 
SIF has a dangerous failure and PFH is therefore an intuitive reliability measure. 
7.7 
Placement of the SIF 
As shown in the bow-tie diagram in Figure 1.1, there may be several safety barriers 
between the hazard and a potential accident. If the SIF is the ultimate safety barrier 
in this sequence, a safety barrier failure will immediately lead to an accident if a 
demand occurs when the safety barrier is in a failed state as illustrated by a simplified 
Markov model in Figure 7.2. In this figure, the SIF is either a functioning (OK) state 
or in a dangerous failed (D) state. If a demand occurs when the SIF is in the D 
state, an accident occurs immediately. This accident is then an absorbing state in the 
Markov model. 
When the SIF is an intermediate safety barrier, there are additional safety barri-
ers that may stop the EUC from developing into an accident. This event does not 
represent an absorbing state and the system may, therefore, be restored to the fully 
functioning (OK) state. This is illustrated in Figure 7.3. 

188 
DEMAND MODES AND PERFORMANCE MEASURES 
7.8 Analytical Methods 
In the following chapters, seven approaches are used to analyze the SIF reliability: 
1. Reliability block diagram approach 
2. Simplified approximation formulas (based on reliability block diagram models) 
3. Approximation formulas provided in IEC 61508-6 
4. The PDS method (SINTEF, 2013b) 
5. Fault tree analysis 
6. Markov approachs 
7. Petri net approach 
It should be noted that IEC 61508 does not require the use of any particular approach 
or any particular set of formulas, but leaves it to the user to choose the most appro-
priate approach for quantifying the SIF reliability. Rouvroye & Brombacher (1999) 
and Rouvroye & van den Bliek (2002) compare and discuss the suitability of several 
of these methods for SIF reliability analysis. 
7.9 Assumptions and Input Data 
The analytical methods in Section 7.8 are presented and discussed in the following 
chapters and often illustrated by examples. 
7.9.1 
Assumptions 
The quantitative analyses in the following chapters are based on several assumptions. 
The main assumptions are as follows: 
(a) All channels have constant failure rates with respect to both DD and DU fail-
ures. The DD failure rate is denoted ADD and the DU failure rate is denoted 
ADU-
(b) All channels are proof-tested periodically with test interval τ. The testing is 
perfect such that all faults are detected in the proof test. All channels in a voted 
group are tested at the same time. 
(c) If a fault is revealed in the proof test, the channel is repaired to an as-good-as-
new condition and the mean repair time is MRT. 
(d) Each voted group has a single proof test interval and a single MRT. 
(e) The MRT is assumed to be so short that MRT «C Ο.ΐτ. 

ASSUMPTIONS AND INPUT DATA 
189 
(f) All channels are equipped with a diagnostic system that can reveal dangerous 
faults. The diagnostic test interval is short and considered to be negligible. 
(g) When a DD fault is revealed, the fault is repaired. The mean time from when the 
DD failure occurs until the item is functioning again is denoted MTTR (mean 
time to restoration), comprising both the time to detect the fault and the mean 
repair time. 
(h) All channels in a voted group have the same failure rates and diagnostic cover-
age. Some examples with diverse channels are, however, presented. 
(i) The expected time between demands is at least 10 times greater than the proof 
test interval τ. 
(j) Common cause failures (CCF) are included by using the beta-factor model, 
when relevant. When discussing the PDS method, the multiple beta-factor 
model is used instead of the beta-factor model. 
(k) Common-cause failures are either DD-failures and denoted DD-CCF, or DU-
failures and denoted DU-CCF. A CCF comprising both DD and DU failures is 
not considered to be possible. 
(1) The DD-CCF rate is denoted βΌ and the DU-CCF rate is denoted β. 
(m) Safe failures are disregarded in the SIF reliability analyses (except in Chap-
ter 12). 
Possible deviations from these assumptions are pointed out in each particular case. 
7.9.2 Symbols 
In addition to the symbols introduced as part of the assumptions above, the symbols 
in Table 7.1 are used. 
7.9.3 Input Data 
To be able to compare the results produced by the various methods, the examples are 
based on the common data set in Table 7.2. The data in Table 7.2 are typical values, 
but may not be realistic in practical cases. 

190 
DEMAND MODES AND PERFORMANCE MEASURES 
Table 7.1 
Symbols used in quantitative SIF reliability analyses. 
Symbol 
Description 
PFDavg for the sensor subsystem (i.e., comprising the input ele-
ments/channels). 
PFDavg for the logic solver subsystem. 
PFDavg for the final element subsystem. 
PFDavg for a voted group of channels. If the group is equal to a whole 
subsystem, PFDaVg is equal to the PFD of that subsystem. 
PFDavg for the whole SIF (i.e., comprising all the three subsystems). 
Channel-equivalent mean downtime for a channel of a voted group. 
This is the combined mean downtime for a channel. 
Voted group-equivalent mean downtime. This is the combined mean 
downtime for the group of channels. 
Table 7.2 
Input data for numerical calculations. 
Symbol 
Λ-DU 
■*DD 
τ 
ß 
ßD 
MRT 
MTTR 
Value 
1.0-10"6 per 
6.0-10"6 per 
8 760 hours 
0.10 
0.05 
10 hours 
8 hours 
hour 
hour 
Description 
DU failure rate 
DD failure rate 
Proof test interval 
Beta-factor for DU failures 
Beta-factor for DD failures 
Repair time of a DU fault 
Restoration time of a DD fault 
7.10 Additional Reading 
The following titles are recommended for further study related to Chapter 7: 
- Petroleum, Petrochemical and Natural Gas Industries - Reliability 
Modeling 
and Calculation of Safety Systems (ISO/DTR 12489, 2012). 
- Safety Instrumented 
Systems: 
Design, Analysis, and Justification (Gruhn & 
Cheddie, 2006). 
- Safety Instrumented Systems Verification: Practical Probabilistic 
Calculation 
(Goble & Cheddie, 2005). 
FFDäf 
PFDÎv^ 
PFDa?g 
P F D ^ f 
fGE 
FFDäf 
fGE 

CHAPTER 8 
AVERAGE PROBABILITY OF FAILURE 
ON DEMAND 
8.1 
Introduction 
This chapter deals with SIFs that are operated in demanded mode and where the 
average probability of (dangerous) failure on demand, PFDavg, is used to quantify 
the reliability of the SIF. 
In most of the chapter, it is assumed that the SIF operates in low-demand mode, 
such that the demand rate, Ade, is less than once per year. The SIF is independent of 
the EUC control system and is a separate and dormant protection layer that is only 
activated when a hazardous event in the EUC occurs. Some faults may therefore 
remain undetected until the SIF is demanded or proof-tested. 
A main application area for low-demand systems is the process industry and the 
sector-specific standard for this industry, IEC 61511, has been mentioned several 
times in earlier chapters of the book. 
A SIS that implements a SIF has two main functions: 
Perform the SIF on demand. This is the essential function of the SIS, that is, the 
reason why it has been installed. When a demand occurs, the SIS shall be able 
to carry out the SIF according to the performance criteria specified in the SRS. 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
191 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

192 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
' Proof test 
Figure 8.1 
Possible performance for a SIS item. No dangerous failure occurs. 
The probability of a failure of this function is usually quantified by the PFDavg 
and the main objective of this chapter is to develop formulas for the PFDavg for 
relevant SIS architectures. 
Do not activate the SIF without the presence of a demand. A failure of this function 
may lead to loss of production or service but may also have safety consequences. 
Such a failure is classified as a safe failure and is often called a false alarm or a 
spurious trip. Spurious activations and spurious trips are discussed in Chapter 12. 
8.1.1 
Safety versus Production 
Three Typical Performances. 
Consider a SIS item. The item may be a channel, a 
voted group, a subsystem, or an entire safety loop. Safe failures are disregarded. The 
SIS item is observed from time t = 0, just after a proof test has been performed. The 
item is as-good-as-new and the state is A'(O) = 1. The item may have three typical 
performances in a proof test interval (0, τ). 
1. No dangerous failure in the proof test interval. This performance is illustrated 
in Figure 8.1 and is the most typical performance for high reliability items. The 
item is able to perform its safety function during the whole proof test interval. 
At time r the proof test is initiated. The mean time required to perform the 
proof test is called the mean test-time and is often rather short, typically 1-2 
hours, but can in specific cases be significantly longer. When the proof test is 
completed, the item is put into operation again and is fully functioning. 
2. A DD failure in the proof test interval. This performance is illustrated in Fig-
ure 8.2. A DD failure occurs at time t in the interval and the item "jumps" to 
state X(t) = 0, meaning that it has lost the ability to perform its safety function. 
A repair team is called upon to bring the item back to an as-good-as-new state 
(i.e., to X(t) = 1). The total mean time to restore (MTTR) the DD fault has 
two parts: 
(a) The mean time from when the DD failure occurs until the DD fault is de-
tected. This is a fraction TD, the time interval between two consecutive 
diagnostic tests. The fraction depends on the architecture of the SIS item. 
(b) The mean time from when the DD fault is detected until the function has 
been fully restored. 

INTRODUCTION 
193 
X(t) A 
DD failure 
- Proof test 
MTTR 
Figure 8.2 
Possible performance for a SIS item. A DD failure occurs in the proof test 
interval. 
DU failure 
' Proof test 
Figure 8.3 
Possible performance for a SIS item. A DU failure occurs in the proof test 
interval. 
A typical value for MTTR is 5-10 hours. In many applications, the time be-
tween diagnostic tests is so short that the time until detection may be neglected. 
3. A DU failure in the proof test interval. This performance is illustrated in Fig-
ure 8.3 where a DU failure occurs at time t in the proof test interval and brings 
the item to state X(t) = 0. As the DU fault is undetected (i.e., hidden), the 
fault is not revealed until the proof test at time t = τ. The associated downtime 
(from the time when the DU failure occurs until the time when the proof test is 
initiated) is denoted D\ in Figure 8.3. The mean test-time is the same as for the 
two first performances. When a DU fault is revealed in the proof test, a repair 
action is initiated to bring the item back to an as-good-as-new state, and the 
mean repair time is denoted MRT. 
The probability of two or more dangerous failures of the same item in the proof test 
interval is usually so small that it can be neglected. 
Two Types of Unavailability. 
categories: 
The unavailability of a SIF can be classified in two 
1. Unknown safety unavailability. In this case, the safety function is lost when we 
believe that we are protected. This situation may occur (i) in the fraction of the 
diagnostic interval when a DD failure occurs, and (ii) in the unknown downtime 
D\ after a DU failure. For some architectures, we may also be unknowingly 
unprotected during the proof test when a DU fault is present. 

194 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
1 
Sensor 
subsystem 
2 
Logic solver 
subsystem 
3 
Final element 
subsystem 
Figure 8.4 
The three subsystems of a SIS. 
2. Known safety unavailability. In this case, we know that the safety function 
is unavailable due to testing, repair, and planned preventive maintenance, and 
we may be able to take precautions, introduce other safety barriers, or avoid 
hazardous operations. 
These two categories are distinguished in the PDS method (SINTEF, 2013b) but not 
inIEC61508. 
8.1.2 
PFDavg for a SIF 
As discussed in Chapter 1, the safety loop performing a SIF usually has three main 
subsystems: (1) a sensor subsystem, (2) a logic solver subsystem, and (3) a final 
element subsystem. The three subsystems are configured as a series system, as illus-
trated in Figure 8.4. 
Let Ei denote the event that subsystem i fails on demand, for / = 1,2,3. Because 
all three subsystems must function for the SIF to function on demand, the SIF fails 
on demand if any of the subsystems fail. Let PFD^g denote the PFDavg of the 
sensor subsystem, PFD^jp the PFDavg of the logic solver subsystem, and PFD^^ 
the PFDaVg of the final element subsystem. 
The average probability that the SIF fails on demand, P F D ^ F ) , is by the addition 
rule 
PFD^Ip = Pr(£i U E2 U E3) = Pr(£x) + Pr(£2) + Pr(£3) - Pr(£i Π E2) 
- Ρτ(Ει Π E3) - Pr(£ 2 n E3) + ΡΓ(£Ί Π E2 Π E3) 
When the three subsystems are independent and have high reliability, the probability 
that two or three subsystems fail at the same time is negligible, such that 
PFD^e
F) % Pr(£0 + Pr(£2) + Pr(£3) 
avg 
_ PFD ( S ) + PFD(LS) + PFD(FE) 
1 l *-^avg ' 
x x 
avg 
' 
l l 
avg 
(8.1) 
The PFDavg for a SIF can therefore be determined by adding the PFDavg's for the 
three subsystems. 
► Remark: In many cases, the three subsystems in Figure 8.4 depend on a power 
supply and are able to function only when the power supply functions. This can 
be modeled by including a power supply subsystem in series with the three other 
subsystems. The probability, PFD^^, that the power supply subsystem is in a failed 
state when a demand occurs can then be added to (8.1 ). 
φ 

RELIABILITY BLOCK DIAGRAMS 
195 
(a) 
* 
PT1 
PT1 
PT2 
PT2 
PT3 
PT3 
LS 
SV1 
SV2 
(b) 
Figure 8.5 
Reliability block diagram for a simple SIR 
Voted Group. The term voted group was introduced in Chapter 2 as a group of 
n identical channels voted koon. A subsystem may consist of one or more voted 
groups and a single channel may be referred to as a lool voted group. To simplify 
the presentation, the SIS item considered is, in the following, assumed to be a voted 
group of« identical channels. 
Analytical Methods. 
In Section 7.8, seven analytical methods are claimed to be 
suitable for determining the reliability of a SIF. The following sections introduce 
each of these methods and discuss their applicability for determining the PFDavg. 
The analyses are based on the assumptions in Section 7.9. Possible deviations from 
these assumptions are pointed out in each case. Several examples are presented and 
numerical results are based on the input data in Table 7.2. 
8.2 
Reliability Block Diagrams 
Reliability block diagrams are introduced in Section 5.2. Establishing a reliability 
block diagram for a SIF is usually straightforward. A reliability block diagram is 
shown in Figure 8.5 for a very simple SIF with three pressure transmitters (PT) voted 
2oo3, a logic solver (LS), and two safety valves (SV) voted loo2. All channels are, 
at this stage, assumed to be independent. 
The reliability of the system in Figure 8.5 can be determined by using structure 
functions (exact method) or by using approximation formulas based on the minimal 
cut sets. Both methods are described in Chapter 5. 
A problem arises, however, when determining the PFDavg for the system. In this 
case, we cannot treat the channels as separate items, determine the PFDavg of each 
channel, and use the structure function or the approximation formulas to combine the 
PFDavg for each channel to find the PFDavg for the SIF. Such a calculation would 
require repeated multiplications of average probabilities for combination of channels, 
but "the average of products is not equal to the product of averages" (see box). The 
simple approach described in Chapter 5, therefore, gives a wrong result, and the 
approximation is nonconservative. 
This means that the standard formulas for reliability analysis of reliability block 
diagrams cannot be used and an alternative approach is required. Such an approach 
is, however, straightforward and is described in Section 8.3. 

1 9 6 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Product of averages. 
The rule "the average of products is not equal to the product of averages" can be illus-
trated by a simple example. Consider two ordered sets of numbers {1,5} and {2,8}. The 
average of the products of these two ordered sets of numbers is to be determined. We 
have two options: 
1. We can take the average of each set and multiply these averages: 
(1 + 5)/2 x (2 + 8)/2 = 15 
2. We can multiply first and than determine the average: 
(1 x2 + 5x8)/2 = 21 
This shows that option 1 gives a lower result than option 2. You may try with other sets 
of numbers. 
8.3 Simplified Formulas 
The simplified formulas for PFDavg were introduced by Rausand & H0yland (2004). 
The formulas are easy to use and give adequate results for many systems. In the 
simplified formulas, DD failures and S failures are disregarded and only DU failures 
are considered. The proof-testing is assumed to be perfect such that the item is as-
good-as-new after each proof test and repair. The time required to repair a revealed 
DU fault is assumed to be so short that it can be neglected, and we set MRT = 0. 
This assumption also covers the case where the EUC is brought to a safe state during 
the repair action. The effect of a non-zero MRT can, however, be easily included in 
the formulas. 
Several of the concepts and formulas introduced here are used in the more ad-
vanced approaches and it is therefore important to study this part carefully before 
proceeding to the more advanced approaches. 
8.3.1 
Probability of Failure on Demand 
Consider a single item and let T denote the time to DU failure of the item. The item 
can be a channel, a voted group of channels, a subsystem, or a system. The time T 
is measured from time / = 0 and the distribution function is F(t) = Pr(T < /). 
As only DU failures are considered and these failures are undetected, no information 
about the status of the item during the proof test interval is obtained. The status is 
revealed only when the proof test is performed. The time-dependent PFD in the first 
proof test interval is therefore 
PFD(i) = Pr(a DU failure has occurred at, or before, time /) 
= Ρ Γ ( Γ < t) = F(t) 
(8.2) 
Because the item is assumed to be as-good-as-new after each proof test, the proof 
test intervals (0, r], (τ, 2τ],..., are all equal from a stochastic point of view. Hence, 
the safety unavailability PFD(/) of the item is as illustrated in Figure 8.6. Note that 

SIMPLIFIED FORMULAS 
197 
PFD(t) 
4τ Time 
Figure 8.6 
The PFD(i) of a periodically proof-tested item. 
PFD(i) is discontinuous for t — ητ, for n = 1,2, 
If a demand for the safety 
function of the item occurs at time t, the PDF(i) denotes the probability that the item 
fails to respond adequately to the demand. 
Average PFD. In many applications, we are not interested in the PFD as a function 
of time and it is sufficient to know the long-run average value PFDavg. Because of 
the periodicity of PFD(i) (see Figure 8.6), the long-run average PFDavg is equal to 
the average value of PFD(i) in the first proof test interval (0, r), 
-in 
τ Jo 
PFDavg = - / PFD(r) dt 
By introducing PFD(i) = F{t) from (8.2), this can be written as 
PFDavir = - f F(t) dt 
τ Jo 
'avg 
(8.3) 
We can also express PFDavg by the survivor function of the item with respect to DU 
failure, R(t) = 1 - F(t), and get 
PFDa -.-I/' 
τ Jo 
R(t)dt 
(8.4) 
The average PFDavg is illustrated in Figure 8.7. The figure shows that the PFD(i) 
is lower than PFDavg in the first part of a proof test interval and higher than PFDavg 
in the last part of the interval. If a demand occurs just before a proof test, the item 
fails to carry out its safety function with close to twice as high probability as PFDavg. 
Two Interpretations of PFDavg. The PFDavg can be interpreted in two different 
ways: 
1. If a demand for the safety function of the item occurs at a random time in the 
future, the PFDavg is the average probability that the item is not able to react 
and perform its safety function in response to the demand. 
2. The PFDaVg is equal to the mean proportion of time the item is not able to 
perform its safety function. 

198 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Figure 8.7 
The PFDavg for a periodically proof-tested item. 
Average Downtime. 
The second interpretation of PFDavg can be expressed as: 
PFDavg = E [ Z ^ ° ' r ) l 
(8.5) 
where 0(0, r) is the mean downtime of the item in the proof test interval (0, τ). By 
using (8.3), the mean downtime in (0, r) can also be expressed as 
E [D(0, τ)] = PFDavg · r = / 
F(t) dt 
(8.6) 
Jo 
The mean downtime E [D(0, t)} in a long time interval (0, t) (spanning several 
proof test intervals) is 
£[£>(0,/)]=PFDa v g-i 
(8.7) 
► Remark: The mean downtime E [D(Q, r)] in (8.5) is the mean downtime in a 
proof test interval where no information about the status of the item is available. 
E [0(0, r)] can therefore be interpreted as the average downtime in a long sequence 
of proof test intervals, in most of which the downtime is zero. This downtime should 
not be confused with the downtime £>i in Figure 8.3, which is the downtime when 
it is known that a DU failure has occurred. The mean downtime in Figure 8.3 can be 
expressed as 
£ ( o , ) = £ [ D ( 0 , r ) | I ( r ) = 0] 
where Χ(τ) = 0 means that a DU fault is present and the item is failed at time τ. ® 
fl 
EXAMPLE 8.1 Average downtime for a single item 
Consider an item for which the PFDavg has been determined to be 5 · 10~3. This 
means that the item, on the average, fails to perform its safety function once per 
200 demands. The mean downtime of the item during a proof test interval of 
one year (r. = 8 760 hours) is, according to (8.6), 
E [D(0, τ)] = PFDavg · τ = 5 · 10"3 · 8 760 hours = 43.8 hours 
This means that the EUC is "unprotected" by the item on the average 43.8 hours 
per year. 
Θ 

SIMPLIFIED FORMULAS 
199 
Consider a proof test interval, and let 7(0, τ) be the mean uptime in this proof 
test interval (i.e., the time the item is able to perform its safety function). Because 
7(0, τ) + Z)(0, r) = r, the mean up-time E [T(0, r)] in the proof test interval is 
= 
/
'
■ 
Jo 
E [7(0, τ)] = τ-Ε 
[D(0, T)] = / 
R(t) dt 
(8.8) 
Jo 
8.3.2 
PFDavg for Independent and Identical Channels 
Single Channel. Consider a single channel with constant DU failure rate λρυ· The 
channel is proof-tested at regular intervals of length x. The survivor function of the 
channel is R(t) = Γ λ ™' and the PFDavg is from (8.4) 
PFD<î,°ol) =1 - - f R(t)dt = 1 - - [ β'λΌυ' 
dt 
s 
τ Jo 
x J0 
= 1 - 
(l - β~λΌυτ) 
(8.9) 
λΌυχ 
V 
/ 
Replacing β~λουτ 
in (8.9) by its Taylor series (see box), gives 
p p n d o o i ) . 
! 
Λ 
T 
(^DUt)2 . (Apur)3 
(ADUr)4 
\ 
V 
2 
3! 
4! 
) 
When λπυ^ is small (i.e., < 0.10) 
PFDi>ol) « ^f- 
(8.10) 
This approximation is often used in practical calculations. The approximation is 
conservative, which means that the approximated value in (8.10) is slightly greater 
than the correct value in (8.9). 
► Remark: The result in (8.10) can also be obtained by a slightly different approxi-
mation. The probability distribution function of 7 is F(t) — 1 — exp(—ADUO· By 
using the Taylor series, F(t) % ADUÎ· Using this approximation in (8.3) gives the 
same result as (8.10). 
® 
fl 
EXAMPLE 8.2 A single fire detector 
In OREDA (2009), the failure rate of a specific type of fire detectors is ADU = 
2.1 · 10~7 DU failures per hour. If we use a proof test interval τ = 1 year 
« 8 760 hours, the PFDavg is 
™wioon 
λΡυτ 
2.1 · 10~7 · 8 760 
A 
pFD(iooi) K _}^_ 
= 
% 0 00092 = 9.2.10~4 
aVg 
^ 
ry 

200 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Taylor series 
The Taylor series of the function e~at in a neighborhood of t = 0 is 
p-at _ V 
( - " 0 " 
n=0 
at 
{at)2 
(at)3 
(at)4 
= 1 
r + —7 
^—T- + 
1! 
2! 
3! 
4! 
When at > 0 is small (e.g., at < 0.10), the addends of the sum become smaller and 
smaller, and we may use the approximation 
e~at « 1 - at 
or 1 - e~at « at 
at 
0.01 
0.05 
0.10 
0.15 
1 - e~at 
0.00995 
0.04877 
0.09516 
0.13929 
Difference 
0.5% 
2.5% 
5.1% 
7.7% 
If at is not so small, we get a better approximation by including more addends, for 
example, 
-at 
, 
, , (f l02 
, 
-at 
, 
(«0 2 
e 
«a 1 — at -\ 
or 1 — e 
«J at 
— 
at 
at - 
te*2i 
1 - e~at 
Difference 
2 
0.01 
0.00995 
0.00995 
« 0 % 
0.05 
0.04875 
0.04877 
0.04% 
0.10 
0.09500 
0.09516 
0.17% 
0.15 
0.13875 
0.13929 
0.39% 
Taylor series can be used to represent a wide range of functions. In this book, we only 
use the Taylor series for exponential functions, such as e~at, and the general formula is 
therefore not presented. Taylor series is named after the English mathematician Brook 
Taylor (1685-1731). 
If a demand for the fire detector occurs, the (average) probability that the de-
tector is not able to detect the fire is PFD^v°ol) % 0.00092. This means that 
approximately one out of 1 087 fires will not be detected by the fire detector. 
This also means that the fire detector is not able to detect a fire during 0.092% 
of the time, or approximately 8.1 hours per year, when we assume that the de-
tector is in continuous operation. 
Θ 

SIMPLIFIED FORMULAS 
2 0 1 
Series Structure. 
Consider two independent channels with failure rates ADu,i and 
^DU,2. respectively, with respect to DU failures. The channels are proof-tested at 
the same time with test interval r. The channels are voted 2oo2, which is a series 
structure where both channels have to function for the structure to function. The 
survivor function for the structure is 
R({) 
= 
g~ (^DU,1+^DU.2)< 
and the PFDavg is from (8.4) 
1 ι·τ 
PPQ(2OO2) _ J _ _ 
/ 
£-(λου.ι+λϋυ.2)ί 
di 
τ JO 
C^DU.l + ^DU,2) τ 
^ D U , l r , λΐ>υ,2τ 
, 0 , , , 
2 
2 
2 
K 
' 
when λπυ,ίΐ is small, for i = 1,2. This result can be easily extended to a series 
structure of« independent channels (i.e., voted noon). 
pFD0.oo„)ss£pFDavg. 
i' = l 
where PFDavgi, is the average PFD for item i, for i = 1,2,...,«. A similar result 
was shown with another approach in (8.1). 
1oo2 Voted Group of Identical Channels. Consider a voted group of two inde-
pendent and identical channels with failure rate λπυ with respect to DU failures. 
The channels are proof-tested at the same time with test interval r. The channels are 
voted loo2, which means that it is sufficient that one channel is functioning for the 
voted group to function. The survivor function for the voted group is 
R(t) = 2e~knvt 
- 
e~2XDut 
and the PFD<^°o2) is from (8.4) 
PFD^vg
o2) = 1 - - ί (2β-λτ>υΙ 
-ε-2λου') 
dt 
= 1 - 
(Ί - β~λουτ) 
+ — 
( 1 - β~2λουτ) 
(8.12) 
λΏυτ 
V 
/ 
2λΌυτ 
\ 
) 
By using the Taylor series approximation 1 — e~ax %ar + (ar)2 /2, PFDavg can be 
approximated by 
p F D ( l o o 2 ) % ( λ π ^ 
( 8 1 3 ) 
whenAou^ is small. 

202 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
β 
EXAMPLE 8.3 loo2 voted group of identical fire detectors 
Reconsider the fire detector in Example 8.2 and assume that two independent 
fire detectors of the same type voted loo2 have been installed. By using the 
same data as in Example 8.2: ADU = 2.1 ■ 10-7 hours-1 and τ = 1 year, the 
PFDavg of the parallel system is 
M r> 
(ADUT)2 
(2.1·1(Γ 7·8 760)2 
, 
p F D U o o 2 ) % i DU ) 
= 
V 
) _ 
Ä 
L 1 . 1 0 - 6 
If a demand for the loo2 voted group occurs, the average probability that the 
group is not able to detect the fire is PFDavg % 1.1 · 10~6, that is, a very high 
reliability. 
Comparing with Example 8.2 shows that adding an extra independent fire 
detector reduces the PFDavg from 9.2 · 10~4 to 1.1 · 10~6, that is, with a factor 
of 836. 
Θ 
► Remark: Because the voted group in Example 8.3 fails only when both of its 
independent channels fail, the probability, Qs(t), that the voted group is in a failed 
state at time t is equal to q\ (f )·<?2(0> where q, (t) is the probability that channel i is in 
a failed state at time t, for i = 1,2. Because the average probability that channel / is 
in a failed state is PFDavg,; =» ADUT/2, we might expect that the average PFD of the 
system would be approximately (λου τ/2) 2 = (ADU T) 2/4 instead of (Αουτ)2/3, 
as we found in (8.13). The result in (8.13) is the correct result. The reason for this 
difference is the fact that the average of a product is not the same as the product 
of averages. Several computer programs for fault tree analysis do this error. An 
unfortunate effect is that the wrong approach produces a nonconservative result (see 
also Section 8.2). 
® 
2oo3 Voted Group of Identical Channels. Consider a 2oo3 voted group of inde-
pendent channels of the same type with failure rate AQU with respect to DU failures. 
The three channels are proof-tested at the same time with test interval r. The survivor 
function for the voted group is 
R{t) = 2, β-2λΌυ' 
- 2 e " 
and the PFDavg is from (8.4) 
PFD^ o 3 ) = 1 - - ί 
( 3 e ~ 2 ; w - 2e"3ADU') 
-JL· 
(i _ β-2λΌυζ\ + _?_ 
Λ _ e-UDVA 
( 8 1 4 ) 
2λπυ^ ^ 
' 
3ADUT ^ 
' 
dt 
2 
= 1 
The Taylor series approximation 1 — β~ατ s» ατ + (ατ)2 /2 yields 
PFDi2°o3> « (λ ο υτ) 2 
(8.15) 

SIMPLIFIED FORMULAS 
203 
when λπυ^ is small. 
Section 5.2.4 shows that the reliability block diagram of a 2oo3 voted group can 
be represented as a series structure of three parallel (i.e., loo2) structures (see Fig-
ure 5.5). These are not independent, but when the probability of channel failure is 
"small," the PFDavg's of the three parallel structures can still be added to get the 
PFDavg of the 2oo3 structure. This is the reason why the PFDavg of the 2oo3 voted 
group in (8.15) is approximately three times as high as the PFDavg of a loo2 voted 
group. 
PFDi2°o3) % 3 · PFDÜ°o2) 
fl 
EXAMPLE 8.4 2oo3 voted group of identical fire detectors 
Consider a voted group of three independent and identical fire detectors of the 
same type and with the same data as in Example 8.2. The PFDavg of the 2oo3 
voted group is 
PFD^°o3) «a (ADUr)2 = (2.1 · 10~7 · 8 760)2 % 3.4 · 10-6 
If a demand for the fire detector group occurs, the average probability that the 
2oo3 voted group is not able to detect the fire is PFD^2°o3) % 3.4 · 10~6. 
φ 
8.3.3 
PFDaVg for Independent and Nonidentical Channels 
1oo2 Voted Group of Nonidentical Channels. Consider a loo2 voted group of 
independent and nonidentical channels with failure rates λου,ι and ^DU,2. respec-
tively. The survivor function for the voted group is 
R(t) = e~^OVjlt + e~^DU-2t — e~(^DV-lt~^^"Dl]·2'1 
and the PFDavg is from (8.4) 
Λ 
PT 
1 
PFDiv°o2) = 1 - - / R{t) dt = 1 - 
(Ί - β-*ι>υ.ιΛ 
τ Jo 
λου,ιτ V 
/ 
I 1 — e-^DU-2T | 
M — g-(^Du,i+^Du,2)r ) 
^DU,2t V 
/ 
(λου,Ι + ^DU,2) T V 
/ 
By using the Taylor series approximation 1 — e~aT ?» ax + (ατ)2 /2, the PFDavg 
can be approximated by 
PFD0OO2) «
 
( Α ρ υ '
ι Λ
3
Ρ υ ·
2 ) τ 2 
(8.16) 
When λου,ι = ADu,2. we get the same PFDavg as for identical channels in (8.13). 
Determining the PFDavg of n channels voted koon with this approach is time-
consuming. We therefore use a simplified approach and illustrate this approach for a 
2oo3 voted group. 

204 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
2oo3 Voted Group of Nonidentical Channels. 
Section 5.2.4 shows that a 2oo3 
voted group can be represented as a series structure of three loo2, minimal cut par-
allel structures. Each of these parallel structures has PFDavg % (λρυτ)2/3- When 
λου τ is small, the probability of two parallel structures being in a failed state at the 
same time is negligible, and the PFDavg of the 2oo3 voted group is be approximately 
the sum of the PFDavg's of the three parallel structures. 
The same argument can be used for a 2oo3 voted group of three different channels, 
and (8.16) gives 
(2oo3) 
(Αρυ,ιλρυ;2)τ 
(Αρυ,ιΑρυ^) τ 
(Αρυ^Αρυ^) τ 
^ U a v s 
* 
3 
+ 
3 
+ 
3 
[(Apu,iApu,2) + (Apu,iApu,3) + (Apu,2Apu,3)] τ 
= 
r 
(o-il) 
8.3.4 
PFDavg for a koon Voted Group of Independent Channels 
Consider a koon voted group of independent and nonidentical channels. The DU 
failure rate of channel j is Apuj and the channels are proof-tested at the same time 
with test interval r. The proof-testing is assumed to be perfect such that all DU faults 
are revealed by the test. 
A koon voted group is functioning when at least k of its n channels are function-
ing and fails to function when at least n — k + 1 channels fail. Each minimal cut 
set has n — k + 1 events (i.e., channels with DU failure) and the voted group has 
K = (n_£+1) minimal cut sets. Let the minimal cut sets be denoted 
C\,Ci,...,CK. 
Minimal cut set C, is a parallel structure of n — k + 1 channels and also a loo(n — 
k + 1) voted group, and its PFDavg is from (8.3) 
pFDUoog-,+1)] = ΙΓγ\ 
Fj(t)dt = i Γ Π 0 - e - W ) dt 
J0 Jed 
Jo 
jed 
J e W 
= (Πλ»νΛ 
τη~1+] 
(8.18) 
\jeCi 
Jn-k 
+ 2 
2oo4 Voted Group of Nonidentical Channels. 
Consider a 2oo4 voted group of 
independent and nonidentical channels. The channels are numbered 1, 2, 3, and 4, 
and have constant failure rates λρυ,ι, λρυ,2. Apu,3, and Apu,4, respectively. The 
group fails when at least three channels fail and, therefore has the four minimal 
cut sets Ci = {1,2,3}, C2 = {1,2,4}, C3 = {1,3,4}, and C4 = {2,3,4}. The 
PFDavg!c, of minimal cut C, is from (8.18) 
PFDavg,C; = 
Π λ°υ·; ) \ 
3 

SIMPLIFIED FORMULAS 
205 
The PFDavg for the 2oo4 voted group is the sum of the PFDavg for the minimal cuts, 
such that 
i = l \jeCi 
) 
— [^DU,1^DU,2^DU,3 + ^DU,1^DU,2^DU,4 + ^DU,1^DU,3^DU,4 
r3 
+ ^DU,2^DU,3^DU,4] ■ — 
The PFDavg of any koon voted group of independent and nonidentical channels can 
be determined by the same approach. 
Correction Factor for Fault Tree Analysis. 
In the remark after Example 8.3, we 
claimed that many fault tree analysis programs give a nonconservative answer be-
cause they disregard the fact that the average of a product is not the same as the 
product of averages. 
Consider a minimal cut set with n — k + 1 channels (i.e., a loo(n —k + l) voted 
group). All the channels are proof-tested at the same time with test interval τ and 
DU failure rates XDVJ for j = l,2,...,n—k 
+ I. The PFDavgj for channel j is 
PFDDUj % 
— ~ 
Because a minimal cut set fails when all its channels fail, several fault tree analysis 
programs say that the PFDavg of the minimal cut set is 
ΡΡ^,ΟΟ^+,)] w "fl1 ^2M1 = I π 
A D U J J Ξ
^ 
(8.19) 
By comparing this result with the correct result in (8.18), the correct result can be 
obtained by multiplying the fault tree result (8.19) by a correction factor CF given 
by 
ppr)[loo(«-fc+l)] 
<)n-k+l 
CF = 
avg 
= 
p F n[loo(«-*r + l)] 
„ _ k + 2 
r r l J a v g , FT 
For a minimal cut set with m(= n—k + 1) channels, the correction factor can be 
written 
1m 
CF = —— 
(8.20) 
m + 1 
The correction factor, CF, for some values of m is given in Table 8.1. The error is 
seen to increase with the order m of the minimal cut set. 
PFDavg for a koon Voted Group of Independent and Identical Channels. As-
sume that all the channels of a koon voted group are identical with DU failure rate 

2 0 6 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Table 8.1 
Correction factor to adjust nonconservative results from some fault tree analysis 
programs. 
Order m of 
minimal cut set 
2 
3 
4 
5 
6 
Correction 
factor (CF) 
4/3 
2 
16/5 
32/6 
64/7 
*s 1.33 
= 2.00 
= 3.20 
«5.33 
«9.14 
Table 8.2 
PFDavg of some koon voted groups of identical and independent channels with 
failure rate AQU and proof test interval r. 
k\n 
1 
ADUT 
(Aour)2 
(Août)3 
(λουτ)4 
2 
3 
4 
2 
-
-
-
3 
Aour 
-
-
4 
(Aour)2 
3ADUT 
2 
-
5 
(λουτ) 3 
2(ADUr) 2 
2Aour 
ADU- The minimal cut sets of this voted group are identical and the PFDavg of each 
minimal cut set is 
PFn[ioo(n-i+i)] ^ Κλ-ρυτ) 
P h D a ^ 
~ 
n-k+2 
Because all the κ = (n_^+1) minimal cut sets are identical, (8.18) can be used 
to conclude that the PFDavg of a koon voted group of independent and identical 
channels is 
PFD (toon) 
( 
n 
\ (A DUr)"-*+1 
\n-k-\-\) 
n-k+2 
(8.21) 
The PFDavg for some simple koon voted groups are listed in Table 8.2. 

SIMPLIFIED FORMULAS 
2 0 7 
8.3.5 
Mean Downtime of a DU Failure 
The mean downtime E [0(0, r)] in a proof test interval was found in (8.6) to be 
£[D(0,r)]= / 
F(t)dt 
Jo 
Suppose that an item is proof-tested at time τ and it is revealed that the item has a 
DU fault (i.e., X(z) = 0). Because a DU fault is a hidden fault, it is not possible 
to know exactly when the failure occurred, but we may determine the (conditional) 
mean downtime in the interval (0, z) when a DU fault is revealed at time z. 
By using double expectation, the mean downtime E [0(0, r)] can be written 
E[D(0,T)]=E(E[D(0,T)\X(T))]) 
=E [D(0, z) | Λ'(τ) = 0] · Ρτ(Χ(ζ) = 0) 
+ E [D(0, τ) I *(τ) = 1] · Pr(*(r) = 1) 
If the item is functioning at time z, the downtime E [D(0, z)] is equal to 0. Therefore 
E(D(0, z) | Χ(τ) = 1) = 0. Furthermore, 
Pr(X(r) = 0) = Pi(T < z) = F(z) 
Hence, 
E [D(0, z)] = E [£)(0, τ) | Λ'(τ) = 0] · F(T) 
Furthermore, 
F(T) 
F(T) J0 
r if 
τ Jo 
F(t)dt 
= ----PFOavs 
(8.22) 
F(z) 
τ Jo 
F(z) 
fl 
EXAMPLE 8.5 Single channel with constant failure rate 
Consider a single channel with DU failure rate A DU· When a proof test reveals 
that the channel has a DU fault, the conditional mean downtime in the proof test 
interval (0, τ) is from (8.22) approximately 
£[Z) 1|Z(r) = 0] = -^--PFDit° g
o l ) 
Z 
λ.Τ)\ιΖ 
Z 
% 
: 
— 
% - 
(8.23) 
1_ β-ληυτ 
2 
2 
The same result can also be found by an alternative argument: When a chan-
nel with constant failure rate is failed at time r, all possible failing times in (0, τ) 
have the same probability, and the time of failure is uniformly distributed over 
(0, r). The mean downtime in (0, τ) is therefore τ/2. 
φ 

AVERAGE PROBABILITY OF FAILURE ON DEMAND 
EXAMPLE 8.6 Parallel structure with constant failure rates 
Consider a loo2 voted group of two independent, and identical channels (i.e., a 
parallel structure). Each channel has DU failure rate Arju- When a DU fault (on 
voted group level) is revealed in a proof test, the mean time in the test interval 
where the voted group's safety function has been unavailable is from (8.22) 
£(D 1|Z(T) = 0) = - ^ - P F D ^ ° g
o 2 ) 
* 
~ 
=3 
( A D U T ) 2 » - 
(8.24) 
The last approximation follows because the distribution function of the parallel 
structure 1 —2β~λϋυΓ + β~2λΌυΤ can be approximated by (λου*)2 by using the 
Taylor series. 
This result can also be justified by the following argument: When both chan-
nels have a DU fault at time τ and the two failures are spread out in a "uniform" 
way, the first failure will, on the average, occur at time r/3, whereas the second 
failure on the average occurs (and the system fails) at time 2τ/3. [This statement 
is not well justified, but may help the reader to appreciate the result in (8.24).] 
EXAMPLE 8.7 2oo3 voted group 
Consider a 2oo3 voted group of three independent and identical channels. Each 
channel has DU failure rate AQU- The PFDavg was in (8.15) found to be (λου^)2 
and the distribution function is F(t) - 1 - 3ÉT 2 A D U Î + 2e_3ÀDU' 
The conditional mean downtime when a voted group DU fault is revealed in 
the proof test is therefore 
E(D1 
| Χ(τ) 
= 0) = - f - 
■ PFD£°°3) 
(λουτ) 
F ( T ) 
The Taylor series approximation 1 — e~UT % ατ + (ατ)2 /2 gives 
F(T) = 1 - 3e~2XDuT + 2ε-3λουτ 
% 3 (λ ο υτ) 2 
The conditional mean downtime for the 2oo3 voted group is hence 
E(Dl | *(τ) - 0) % 
T- 
(8.25) 
which is the same result we obtained for the loo2 voted group (because both the 
voted groups fail as soon as two channels fail). 
Θ 

SIMPLIFIED FORMULAS 
209 
β 
EXAMPLE 8.8 koon voted group 
Consider a koon voted group of n independent and identical channels. Each 
channel has DU failure rate ADU- The PFDavg was in (8.21) found to be 
P F D ( W , ) ; J 
n 
\ ( A D U r ) - ^ ' 
avg 
\n-k 
+ l) 
n-k 
+ 2 
The koon voted group fails when at least n—k + l of its channels fail. The prob-
ability of group failure in the proof test interval (0, τ) is by using the binomial 
distribution 
F ^ = 
Σ (" (ι-^
λ ο υ τ)' (<>-
ÀDur) 
i=n-k+\ 
\ / 
Σ 
(")(λουτ)' 
i=n-k+l 
- ( . . ; + , ) < w ) — + ( „ _ ; + 2 ) Ι Λ Ο „ Γ ) Μ « + -
where the first approximation is based on 1 —β~λΌυΤ ss λπυ^ and β~λουτ 
% 1. 
When λπυτ is small, the addends get smaller and smaller as the index ; 
increases, and we may approximate F(z) by the first addend in the sum, such 
that 
The conditional mean downtime is therefore 
E[D:\ 
Χ(τ) = 0] = - f - 
· PFD<*°°"> « 
1 — 
(8.26) 
F{T) 
ë 
n — k + 2 
The conditional mean downtime (given group DU fault in proof test) is given in 
Table 8.3 for some koon voted groups. 
8.3.6 
PFDavg with Common-Cause Failures 
The beta-factor model for common-cause failures (CCFs) was introduced briefly in 
Section 5.3 and is thoroughly discussed in Chapter 10. This section shows how the 
beta-factor model can be used to include potential CCFs into the simplified PFDavg 
formulas. The main feature of the beta-factor model is that the DU failure rate is 
split into two parts: 

210 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Table 8.3 
The conditional mean downtime (given group DU fault in proof test) for some 
koon voted groups. 
k\n 
r/2 
/3 
/2 
-
-
r/4 
τ/3 
τ/2 
-
r/5 
r/4 
r/3 
r/2 
(1-β)λ 
(a) 
1 
1 
2 
DU α-β) λ 
2 
3 
3 
DU 
H'^DU 
c 
(b) 
Figure 8.8 
model. 
Group of three channels voted 2oo3 with CCFs modeled with a beta-factor 
Λ0 
Ac) 
VDU 
This is the rate of DU failures that only affect the channel in question. It is 
also called the independent failure rate or the channel-specific failure rate. 
This is the rate of CCFs that affect all the channels of the voted group, and 
is often called the common-cause failure rate. 
such that 
ADU - ADU 
The parameter ß is introduced as 
+ 
ADU 
(8.27) 
ß 
Ac) 
lDU 
X DU 
and is the fraction of common-cause failures among all DU failures of a single chan-
nel. The independent failure rate then becomes ADy = (1 — β)ληυ. 
Because of (8.27), the group of voted channels can be represented as a series struc-
ture of two independent parts: (a) and "independent" part comprising a number of 
independent and identical channels with DU failure rate λ^υ, and (b) a virtual CCF 
element with failure rate Ap(j, as illustrated for a 2oo3 voted group in Figure 8.8. 
According to (8.11), the PFDavg of a series structure of independent items is 
approximately equal to the sum of the PFDavg's of the items of the series structure. 
The PFDavg of a voted group modeled with the beta-factor model can therefore be 

SIMPLIFIED FORMULAS 
211 
written as 
PFDavg * PFD«g + PFD<& 
where PFD^J is the PFDavg of the voted group of independent channels, each with 
failure rate (1 - β)λΌυ, 
and PFD^g is the PFDavg of the virtual CCF element with 
failure rate βλου- The PFD^ of the CCF element does not change when the archi-
tecture is changed. The beta-factor model will therefore always yield 
PFDavg « PFD^g + ^ψ^- 
(8.28) 
fl 
EXAMPLE 8.9 PFDavg of a loo2 voted group with potential CCFs 
Consider a loo2 voted group of identical channels and assume that the input data 
set in Table 7.2 is applicable. Each channel has DU failure rate ADU = 1 · 10~6 
per hour. The voted group is proof-tested once per year such that r = 8 760 
hours and the beta-factor has been estimated to be ß = 0.10. 
If the two channels had been independent, the PFDavg of the voted group 
would be 
5(1002) _ 
(ADU 
τΫ 
avg 
~ 
3 
By including the effect of potential CCFs, the PFDavg of this voted group is 
PFDi:°oz' « 
v""" '' 
= 2.56 · 10" 
PFDavg = PFD<£*'> + PFD& = [ ( 1 " ^ D U T ] 
+ 
^ ψ -
= 2.07 · 10"5 + 4.38 · 10"4 = 4.59 · 10"4 
v 
v 
' 
' 
v 
' 
(i):4.5% 
(c):95.5% 
This example shows that: 
Including the effect of potential CCFs increases the PFDavg by a factor 
k = 17.9 compared with the case when the two channels are assumed to be 
independent. 
When the effect of potential CCFs is included, the independent part of the 
structure only contributes with approximately 4.5% of the voted group's 
PFDavg. 
The contribution from the CCF element, PFDj$„ = 4.38 · 10~4 is constant 
and will not change if the degree of redundancy is increased. This part of 
the PFDaVg remains the same for all voted groups irrespective of the voting 
and number of channels. This again, means that the benefit to the PFDavg 
obtained by increasing the degree of redundancy is very limited. 
φ 

212 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Table 8.4 
Two types of PFD, 
Measure 
Description 
PFDavg,i 
This is the PFDavg within the proof test interval as determined earlier 
in this section. In this interval, we believe that we are protected by the 
SIF and this is therefore an unknown unavailability. 
PFDavg,2 
This is the unavailability of the SIF during the repair time of a revealed 
DU fault. In this period, we know that a channel (or the SIF) is not 
able to perform its safety function. PFDavgl2 is therefore a known 
unavailability. 
8.3.7 Non-Negligible Repair Time 
So far, the time required to repair a revealed DU fault has been considered to be 
negligible, but this may not always be the case in practice. Let MRT denote the 
mean repair time of a channel DU fault that is revealed in a proof test, but assume 
that the time to perform the proof test is negligible. If the proof test interval is kept 
constant, the total PFDavg can be written as the sum 
PFDavg = PFDavgil + PFDavg,2 
(8.29) 
where PFDavgii and PFDavgi2 are defined in Table 8.4. 
Single Channel. The probability that a DU fault of a single channel is revealed in 
a proof test is 
Pr(7bu < T) = 1 - β" λ ο υ τ «s λ ο υ τ 
The long-term average downtime per time unit due to repair of DU faults is 
pcn(iooi) 
Pr(7bu<r)-MRT 
PFDavg,2 = 
^ ^DUMRT 
The total PFDavg for a single channel is therefore 
p F D0ooi) = 
^DTJT 
+ 
A D U M R T 
( 8 
3 0 ) 
2. 
^ « — i . « . - ^ . 
— ' 
^ 
Known 
Unknown 
When it is known that the safety function is unavailable, it may sometimes be 
possible to take precautions. It may therefore be argued (e.g., se SINTEF, 2013b) 
that the known part of PFDavg is less important than the unknown part. 

SIMPLIFIED FORMULAS 
213 
β 
EXAMPLE 8.10 Downhole safety valve in a subsea oil/gas well 
Consider a downhole safety valve (DHSV) in an oil/gas subsea production well. 
The DHSV is an integral part of the production tubing and is located approx-
imately 100 meters below the sea bottom. The valve has a spring-loaded hy-
draulic fail-safe actuator and is held open by hydraulic pressure. The DHSV is 
proof-tested periodically, with a test interval of 6 months. 
Assume that a dangerous fail-to-close fault has been revealed in a proof test. 
Repairing a failed valve is a long, hazardous, and extremely costly operation. 
A semi-submersible intervention rig has to be moved out to the offshore field, 
the tubing string has to be pulled, and the well pressure has to be controlled 
during the intervention. The operation may last several weeks, depending on the 
system and the weather conditions. In addition, one may have to wait months 
before an intervention rig becomes available. In this case, the repair time is far 
from negligible. 
φ 
1oo2 Voted Group of Identical Channels. A loo2 voted group of independent 
and identical channels is able to perform its safety function if at most one channel 
has a DU fault. The proof test can result in: 
1. No DU fault is revealed. In this case, there is no channel to repair and the 
downtime is therefore zero. The probability of option 1 is p\ = 
β~2λ°υτ 
2. One DU fault is revealed. In this case, there are two options. 
(a) The voted group is out of function and is unavailable during the repair pe-
riod. 
(b) The failed channel is isolated during the repair period and the remaining 
channel is operated as a single channel. 
The probability of option 2 is p2 = 2e~XouT (l - 
e~XovT) 
3. Two DU faults are revealed. In this case, the group is out of function until both 
channels are repaired. The probability of option 3 is p-$ = (l — e~*DvT) 
Assume that the mean repair time, MRT, is the same irrespective of whether one 
or two channels are repaired. This is often a realistic assumption because most of the 
repair downtime is associated with mobilizing the repair crew and preparing for the 
repair. 
Option 2(a). With this option, the mean downtime due to repair is MRT(/?2 + ^3) 
and when the test period is not influenced by the repair time, the PFDavgi2 is 
JI002) _ MRT(p2 + p3) _ MRT (1 - 
e - 2 W ) 
PFD avg,2 
τ 
2 A D U T M R T 
—— 
= 2ADUMRT 

214 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
The total PFDavg for the loo2 voted group with repair option 2(a) is therefore 
P F DiÎ2(«) = ^ ψ ^ 
+ 2ADUMRT 
(8.31) 
Option 2(b). The mean downtime associated with option 3 is MRT/?3. With option 
2(b), the remaining single channel may get a DU failure during the repair period and 
the associated mean downtime is E{Dr). After a channel is proof-tested at time τ, 
and no DU fault is revealed, it is as-good-as-new. The probability of a DU failure 
during a relatively short repair time of the other channel is therefore rather small and 
is from (8.6) approximately 
/■MRT 
/.MRT 
E(Dr) = / 
F{t)dt = / 
U -e-XDvt) 
dt 
/ 
ADU/ at = 
Jo 
2 
The mean downtime related to the repair in option 2(b) is therefore 
MDT2(b) = MRT · p3 + £(£>,)· p2 
= MRT(l- e-W)V D UMRT* 
~MRT(A D Ur) 2 + 
A D U ^ R T 2 
The PFDaV;2 for option 2(b) is 
The total PFDavg for the loo2 voted group with repair option 2(b) is therefore 
P F D Ä , - ^ ψ 1 + MRTA^r + ^ M 5 î ! 
(8.32) 
IJi EXAMPLE 8.11 loo2 voted group of identical channels 
Consider the same loo2 voted group as above and assume that the data set in 
Table 7.2 is applicable. 
Option 2(a) gives, by using (8.31): 
pFD2(«) w (^ψΐ + 2ADUMRT 
= 2.56 · 10~5 + 2.00 · 10"5 = 4.56 · 10-5 
%
 
V 
' 
* 
v 
' 
Unknown 
Known 

SIMPLIFIED FORMULAS 
215 
Option 2(b) gives, by using (8.32): 
p F D2W % i ^ l l 
+ M R T A 2 
+ AOU MRT2 
'avg ~ 
3 
-TivirviA D ui. -r 
^ 
= 2.56 ·1(Γ5 + 8.76 · 10"8 + 5.71 · 10"9 = 2.57 · 10~5 
Unknown 
Known 
Partly unknown 
The unavailability due to a possible DU failure of the remaining active channel 
during the repair period of the channel that failed in the proof test interval is 
generally very low. The associated PFDavg is classified as "partly unknown" 
in the above expression because it is not possible to detect a DU failure of the 
active channel, while the group is running in a degraded mode. 
Θ 
koon Voted Group of Independent Channels. For a voted group with multiple 
channels, the safety function of the group may or may not be unavailable during 
repair actions. In some cases, it may be physically impossible to repair one channel 
while the other channels are active. This is, for example, the case when several 
shutdown valves are installed in the same pipeline without bypass options. In other 
cases, it may be possible to repair one or more channels while the system is online 
and operating in a degraded mode. 
In many cases, the mean repair time, MRT, is close to independent of how many 
channels that are repaired, and this assumption is made here. Further, assume that 
the proof test is perfect and that the time required to carry out the test is negligible. 
The contribution to the PFDavg due to DU failures of the part of the voted group 
running in degraded mode during repair actions is assumed to be very small and is 
therefore neglected (see Example 8.11). 
Probability of Dangerous (DU) Group Failure. Consider a koon voted group of n 
independent and identical channels. A dangerous (DU) group failure (DGF) when 
at least n — k + 1 of its channels have DU faults. Each channel has DU failure rate 
ADU- The probability that a DU failure of a channel occurs in a proof test interval is 
p = 1 — e~^OvT ss λου^. Let M denote the number of channels that fail during a 
proof test interval. The distribution of M is binomial (n, p) and the probability that 
a DGF is revealed in the proof test is 
Pr(M >n-jfc + l)= 
J^ 
i" )py(l - pf~y 
y=n-k+l 
V/ 
È 
(η)(λ0υτ)^(1-λουτ)"-
y=n-k+l 
V 

216 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
In most cases, the channels have high reliability, such that λ^υτ is very small. In 
that case 1 — λου^ % 1, such that the following approximation can be used 
Pr(Jlf > « - * + 1 ) * 
Σ 
( " p o u r ) ' 
y=n-k+\ 
V) 
Whent λου^ is small, (ληυτΥ 
^> ( A D U T F + 1 for all positive integers y. This 
means that Pr(M > n — A: + 1) is determined mainly by the first addend in the sum 
and the followinge approximation can therefore be used: 
Pr(M>n-k 
+ l)*( 
" 
) (ADUr)"-*+1 
\n—k + lj 
With Online Repair. When online repair is possible, the safety function is unavail-
able during the repair action only when a DGF is revealed in the proof test. It is 
assumed that the repair of failed channels will not influence the proof test interval. 
The contribution to the PFDavg from the repair action, PFDavg)2, is equal to the mean 
downtime of the safety function during the repair divided by the length of the proof 
test interval. 
The total PFDavg for the koon voted group is from (8.21) 
pFD(*oo„) 
/ 
n 
W D U T ) " - * + 1 
/ 
n 
W + i r „ - * . M R T 
avg 
\n-k + lj 
n-k + 2 
\n - k + \J DU 
(8.33) 
fl 
EXAMPLE 8.12 2oo4 voted group of identical channels 
Consider a 2oo4 voted group of independent and identical channels and assume 
that the data set in Table 7.2 is applicable. 
The total PFDavg is from (8.33) 
s» 6.72 · 10"7 + 3.07 · 10"9 = 6.75 ■ 10"7 
> 
„ 
' 
v 
< 
Unknown 
Known 
The contribution from the repair action is only 0.45% of the total PFDavg. 
® 

SIMPLIFIED FORMULAS 
217 
The Safety Function Is Unavailable during Repair. Assume now that if at least 
one channel fails in a proof test interval, the channel has to be repaired and the entire 
voted group has to be disconnected during the repair action. 
The probability of at least one channel DU failure during the proof test interval is 
Pr(M > 1) = 1 - Pr(M = 0) = 1 - (1 - p)n = 1 - (έΓ λ ϋ υ τ)" 
= i_ e-"A D UT 
κηχΌυτ 
The the contribution to the PFDavg of the koon voted group from this repair action 
ponOtoo«) 
Pr(M>l)-MRT 
P F Davg,2 
= 
% "^DU · MRT 
Any option between these two extremes is possible, such as to close down the safety 
function if two or more channels have failed—even if the voted group is able to 
function. 
Multiple Channels with CCF. Consider a koon voted group of identical channels. 
Each channel has DU failure rate ADU- The group is exposed to CCFs and we use a 
beta-factor model with parameter β to model the CCFs. The CCF rate is βλ^>υ and 
each channel has an independent DU failure rate (1 — /8)λου· 
A DGF can occur either because of n — k + 1 or more independent DU failures or 
because of a CCF. Let Mjnd denote the number of independent channel DU failures. 
The probability that DGF is revealed in a proof test is 
Pr(DGF in (0, r)) = Pr(Mind > n - k + 1) + Pr(CCF) 
* („ _ I + ,) K1 - /Ολουτ]"-^1 + (l - Γ ^ « 1 ) 
* ( 
! \ 
JK1-/0ADurr- f e + 1+/5AD Ur 
\n — k + 1 / 
by using the same approximations formulas as for independent channels. 
With Online Repair. When online repair is possible, the safety function is unavail-
able only when a DGF is revealed in the proof test. The long-term average downtime 
of the voted group per time unit due to repair actions is 
PPn(fcoo„) 
Pr[DGFin(0,r)]MRT 
P F Davg,2 
= 
The Safety Function is Unavailable During Repair. Assume now that if at least 
one DU failure occurs in a proof test interval, the channel has to be repaired and the 
safety function is unavailable during the repair action. 
The frequency of at least one DU failure in a proof test interval with CCF is 
n{\ — β)λου 
+ βλνν and the long-term average downtime of the voted group per 

218 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
time unit due to repair actions is 
Dp n(W.) 
Pr(M > 1)-MRT 
PFDavg,2 = 
^ ("v1 _ PMDU + pADU)MRT 
fl 
EXAMPLE 8.13 2oo4 voted group of identical channels with CCFs 
Consider a 2oo4 voted group of identical channels that are exposed to CCFs, 
and assume that the data set in Table 7.2 is applicable. 
The unknown PFDavgii in the proof test interval (0, τ) is from (8.33) and 
Example 8.12: 
pFD(2oo4) ^ M [ ( 1 - 1 ) A D U T ] 3 
βλΌυτ 
P^avg.l ~ ^ J 
4 
+
2 
4.90 · 1(T7 + 4.38 · 10"4 = 4.38 · 10"4 
~ 
* 
' 
x 
v 
' 
0.11% 
99.9% 
The probability that the voted group has failed in a proof test interval is 
Pr(DGF in (0, T)) = Pr(Mind > 3) + Pr(CCF) 
* \Ζ\[(1-β)λΌυτ]3 
+ 
βλΌυτ 
= 1.96· 1(Γ6 + 8.76 · 10~4 = 8.78 · 10- 4 
With online repair, the contribution to PFDavg from the repair action is 
P F D £ ? = Pr[paFm(0,r)]MRr % ^
. 
^ 
The total PFDavg with online repair policy is 
PFD£° O 4> = P F D ^ + PFDi2
v°g°4) 
ss 4.38 · 10"4 + 1.00 · 10~6 = 4.39 · 10 - 4 
When the safety function is unavailable during the repair of any DU fault, 
PFD^°g°4) * 4 [(1 - β)λΌυ + βλΌυ] MRT = 4.0 · 10~5 
The total PFDavg with this repair policy is 
pp n(2oo4) _ 0^x^(2004) , p p r n(2oo4) 
r r l J a v g 
— r r L , a v g , l 
f 
r r u a v g , 2 
% 4.38 · 10~4 + 4.00 · 10"5 = 4.78 · 10 - 4 
The two options are seen to give different results. 
Θ 

SIMPLIFIED FORMULAS 
219 
8.3.8 
Nonnegligible Test-Time 
So far, it has been assumed that that the time required for proof-testing is negligible. 
This may be a realistic assumption for most voted groups, but there are also exam-
ples where the test-time cannot be disregarded. A channel that has a DU fault when 
the test is initiated has the DU fault until the proof test is completed and the channel 
is repaired. In many cases, the unavailability during the proof test is not significant 
compared with the unavailability during the proof test interval. It may, however, be 
cases when the safety availability during the proof test is important. This is espe-
cially the case when the EUC is unstable because of the proof test and when several 
operators are present in the EUC performing the proof test. 
The test-time is the time from the proof test is initiated until the channels are put 
into operation again or handed over to the repair crew. Let the mean test-time be 
denoted by MTT. The MTT of a channel may range from some few minutes up to 
several hours. 
The probability that a single channel has a DU fault when the proof test is initiated 
is 1 — ε~λουτ 
and the associated downtime may range from a small fraction of the 
test-time up to the whole test-time, depending on how the proof test is performed. 
This can be illustrated by two different options for proof-testing a shutdown valve. 
1. In this option, the shutdown valve is installed in a pipeline with no possibility 
for online testing. The valve is normally in open position and the DU failure 
modes are "fail to close on command" and "leakage in closed position." To test 
the valve with respect to both failure modes, the flow in the pipeline must be 
closed by an upstream valve. Afterwards, a shutdown signal is sent to the shut-
down valve and the closing function is monitored, for example, by a position 
indicator. The failure mode "leakage in closed position" is normally controlled 
by monitoring the pressure in the pipe between the shutdown valve and the 
valve that was used to shut down the flow. If this volume is large, it may take 
a rather long time to identify a small leakage. If the shutdown valve has a DU 
fault when the proof test is initiated, it cannot perform its safety function during 
the test, but this is not important because the EUC is brought into a safe state 
before the test is initiated. 
2. In this option, the shutdown valve has a a bypass design for testing and repair, 
as illustrated in Figure 4.1. The proof test is then initiated by blocking off the 
shutdown valve and it will be unavailable as a safety barrier during the proof 
test and possible repair. With this option, the shutdown valve cannot perform 
its safety function during proof-testing and repair, irrespective of whether it had 
a DU fault when the proof test was initiated. 
The effect of non-negligible test-time on the PFDavg is difficult to quantify in a 
general case, but a similar approach as the one outlined for the repair actions can 
often be used. 
When the unavailability of the item increases during the proof test and/or the 
repair time, the PFD(?) may have a shape as illustrated in Figure 8.9. 

220 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
4τ Time 
Figure 8.9 
PFD(/) of item with non-negligible proof test-time and/or non-negligible repair 
time. 
8.3.9 
Effect of DD Failures on the PFDavg for a Single Channel 
Assume that DU and DD failures occur as independent events. The contribution 
from DU and DD failures to the PFDavg can then be evaluated separately for a koon 
for each channel. In this section, we only consider a single channel. 
Single Channel. Let NUD (r) be the number of DD failures in the proof test inter-
val (0, r). When the DD failure rate, ADD, is constant, DD failures occur according 
to a homogeneous Poisson process and the mean number of DD failures in (0, r) 
is E[NVO(Î)] 
= λγ)γ>τ. Each time a DD-failure occurs, the mean time the safety 
function of the channel is "down" is MTTR. The total mean downtime in the proof 
test interval, the safety function is "down" due to DD failures is 
E[Z>DD(0,T)] = ADDrMTTR 
The contribution from DD failures to the PFDavg of a single channel is therefore 
p F r ) ( l o o l ) 
_ 
r r L a v g , D D 
— E[DDO(0,r)] 
= A D D M T T R 
The Total PFDavg for a Single Channel. By including the contributions from both 
DU and DD failures and and the MRT, the "total" PFDavg is obtained. 
p F D ( l o o l ) 
r r i - a v g , MRT 
PFDÜooD = P F D O O O ^ + P F D ^ D + P F D ^ 
* p p 
+ A D D M T T R + λ 
— λου 
I D U M R T 
; (^ + MRT) + ADDMTTR 
(8.34) 
In Section 8.4, the PFDavg is determined by the formulas in IEC 61508-6, which we 
call the IEC formulas. The result for a single channel obtained with these formulas 
is the same as (8.34). To determine the total PFDavg for a koon voted group is 
complicated when using the simplified formulas, whereas it is easier with the IEC 
formulas. 

SIMPLIFIED FORMULAS 
221 
PFD(t) 
τ+ to 
2τ 
2τ+ to 
3τ 
Time t 
Figure 8.10 
PFD(i) of a parallel system of two channels with staggered proof-testing. 
Channel 1 (short dash) is proof-tested at times 0, r, 2r,..., while channel 2 (long dash) is 
proof-tested at times to, τ + 'ο. 2τ + ίυ 
The system PFD(i) is the fully drawn curve. 
8.3.10 
Staggered Proof-Testing 
The PFDavg of a voted group can be reduced by proof-testing the channels at dif-
ferent times. The approach is illustrated for a loo2 voted group of independent 
channels. 
1oo2 Voted Group of Independent Channels. 
A loo2 voted group has two inde-
pendent channels with constant failure rates λπυ,ι a nd ADU,2> respectively. Chan-
nel 1 is proof-tested at times 0, τ, 2τ,..., while channel 2 is proof-tested at times 
io. f + ίο, 2τ + ίο, 
This testing strategy is called staggered proof-testing with 
interval to- Assume that the time necessary for proof-testing and repair is so short 
that it can be neglected. Further, assume that the process has been running for some 
time and that time 0 is the time for a proof test of channel 1. 
The PFDavg of the two channels as a function of time is illustrated in Figure 8.10. 
In the first proof test interval (0, τ), the channels have the following unavailabili-
ties: 
qi(t) 
= 
l-e-^DU.i' 
for 
0 < / < τ 
q2(t) 
= 
1 - e~ADu,2(i + τ - /0) 
for 
0 < t < t0 
q3(t) = 1 -ί>-λου,2(ί-'ο) 
for 
t0 < t < τ 
The unavailability of channel 1, q\(t), is illustrated by a short-dashed line in Fig-
ure 8.10, while the unavailability of channel 2, <72(0> is illustrated by longer dashes. 
The system unavailability qs(t) = qi(t) ■ qi{t) is illustrated by a fully drawn line in 
Figure 8.10. 
?*(0 
= 
q\ (0-92(0 
for 
0 < i < / 0 
= 
qi(t)-q3(t) 
for t0 < t < τ 
The PFDavg in the proof test interval (0, r) is a function of io and is calculated by 
PFDavg(i, 
qs(t)dt 
JtQ 
qi(t)-q2(t)dt+ 
/ qx(t) ■ q3{t) dt 
(8.35) 

222 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
To solve these these integrals by hand is a tedious job and the final expression is 
not very nice. The solution is, however, straightforward by using MAPLE®. 
When the two channels have the same failure rate, λου,ι = ^DU,2, PFDavg(io) 
attains its minimum when to = τ/2, which is an intuitive result. 
β 
EXAMPLE 8.14 Staggered testing of loo2 voted group 
Consider the same loo2 voted group as discussed above. The aim of this ex-
ample is to illustrate the benefits of staggered testing by some numerical cal-
culations. The following parameters are used: λρυ,ι = 2.0 · 10"6 per hour. 
^DU,2 = ^λου,ι · The proof test interval is τ = 8 760 hours and the test delay 
of the test of channel 2 is to = τ/b. 
The PFDavg(i0) in (8.34) is entered into MAPLE® and the numerical calcu-
lations are performed by this program. To test the program, let k = 1, such that 
the DU failure rates are equal, and let b = 1, which means that the two channels 
are proof-tested at the same time. 
This is the same situation that has been studied earlier in this section. The 
simplified formula for the loo2 voted group gives 
(λητττ)2 
A 
PFDavg % v 
υ ' 
% 1.02 · 10~4 
By using MAPLE® based on (8.34), the solution becomes 
PFD^=1} % 1.01 · 10~4 
This is an expected result because the simplified formula is constructed to give 
a conservative approximation. 
When b = 2, t0 = τ/2 and channel 2 is tested in the middle of the proof test 
interval of channel 1. The result is 
PFD^=2) % 6.33 · 10~5 
This means that introducing staggered testing with delay io = r/2 reduces the 
PFDavg with close to 40% compared with using simultaneous testing. 
When b = 4, such that the delay is three months, the result is 
PFD<£g4) % 7.28 · 10 -5 
This PFDaVg is higher that the one obtained for to = r/2, because the optimal 
delay for identical channels is τ/2. 
Assume now that the two channels have different DU failure rate, for example 
k = 0.2. With simultaneous testing (b = 1), the result is 
PFD<£gU=0-2) «s2.02-10-5 
A careful analysis shows that the minimum PFDavg is obtained for to & 4170 
hours 
pFD£=2.1,fc=0.2) ^ j 2 4 . 10-5 

THE lEC 61508 FORMULAS 
223 
The optimal delay of the second test is therefore approximately 210 hours earlier 
than τ/2. 
Θ 
8.4 The lEC 61508 Formulas 
IEC 61508-6 provides approximation formulas for the PFDavg for simple configu-
rations with no more than three channels. We refer to these formulas as the IEC 
formulas. The IEC formulas are presented in IEC 61508-6 without any derivation or 
justification and many reliability analysts find them confusing. The purpose of this 
section is to derive similar PFDavg formulas for koon configurations and to try to 
make them understandable. 
The main idea of the IEC formulas is to calculate the PFDavg of a voted group 
(G) of channels as if the group were a single item. The calculation is based on the 
average dangerous group failure frequency, AD,G> and the group-equivalent mean 
downtime, ÎQE» and PFDavg for the group is calculated as 
PFD<°> - AD,GfGE 
(8.36) 
In the calculation of PFDavg, it is necessary to take the mean downtime, ÎQE. of a 
channel that has got a D failure, into account. The mean downtime, ÎQE, is called the 
channel-equivalent mean downtime. 
The IEC formulas take DU and DD failures into account, but safe failures are 
disregarded in this section. Assumptions for the derivations are given in Section 7.9. 
8.4.1 
PFDavg for Independent Channels 
To determine the PFDavg of a voted group, it is necessary to find 
1. The average rate, AQ.G, of dangerous group failures (DGFs). 
2. The mean downtime of the voted group when a DGF occurs. 
The main concepts related to rate of failures of an item (e.g., a group) are discussed 
in further detail in Section 9.2. 
Frequency of Dangerous Group Failures. We start with task 1 and consider two 
simple structures (loo2 and 2oo3) before presenting the frequency of DGFs of a 
koon voted group of identical channels. 
Average System Failure Rate for a 1oo2 Voted Group. Consider a loo2 voted 
group of independent and identical channels with D failure rate λο· Because the 
channels are independent, they will not fail at exactly the same time. A dangerous 
group failure must therefore start with a D failure of one of the two channels. The 
rate of this event is 2AD, as both channels can fail. When one channel has failed, 

224 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
it will be down for a period ÎCE· TO have a dangerous group failure, the remaining 
channel must fail within the downtime of the already failed channel, and this failure 
occurs with probability Pr(7b < ÎCE). where 7b is the time to dangerous failure 
of a channel. The average system failure rate for the voted group with respect to D 
failures is therefore 
AD,G = 2AD (l -e~XDtcE) 
% 2A DA DÎCE 
= 2(A D) 2ÎCE 
(8.37) 
Average System Failure Rate for a 2oo3 Voted Group. Consider a 2oo3 voted 
group of three independent and identical channels with D failure rate AD- Because 
the channels are independent, they will not fail at exactly the same time. As for 
the loo2 voted group, a dangerous group failure must start with a D failure of one 
of the three channels. The rate of this event is 3AD, because all the three channels 
can fail. When one channel has failed, it will be down for a period ÎCE- TO have a 
dangerous group failure, at least one of the remaining two channels must fail within 
the downtime of the already-failed channel. The average system failure rate for the 
voted group with respect to D failures is therefore 
AD,G = 3AD (l - e-2*D'cB J % 3AD2ADiCE 
= 6(A D) 2ÎCE 
(8.38) 
Average System Failure Rate for a koon Voted Group. Consider a koon voted 
group of n independent and identical with D failure rate AD · The group is functioning 
when at least k of the n channels are functioning and fails as soon as at least n—k + l 
channels fail. A dangerous group failure, must start with a D failure of one of the 
n channels. The frequency of the first D failure is «AD, because n independent 
channels can fail. When one channel has failed, it will be down for a period ÎCE· A 
dangerous group failure occurs if at least n — k of the remaining n — 1 channels fail 
while the first failed channel is down. Because all the n — 1 are identical, this is a 
binomial situation, and the probability of at lest n — k D failures is 
Pi(N>n-k)= 
Σ 
" y ' 
U
l
-
r
W
i
r
^
r 
i=n—k \ 
/ 
* Σ (" 7 *) (ADÎCE)Î 
=(;:;H->"H
!*+')
<w~*
+'
+'· 
Because AQÎCE is small, the addends get smaller and smaller. The second addend 
is, for example, only 0.15% of the first addend for a 2oo4 voted group when using 
the dataset in Table 7.2. The sum can therefore be approximated by the first addend 

THE IEC 61508 FORMULAS 
2 2 5 
Table 8.5 
Frequency of independent group D failures. Numerical values are calculated by 
using the data set in Table 7.2, such that ÎCE = 634 hours. 
koon 
Frequency of D failures 
Num. (per hour) 
lool 
AD 
7.00 ·1(Π6 
1οο2 
2À^fCE 
6.21 · 10 - 8 
1003 
3A-DfCE 
4.14 ■ 10 - 1 0 
2003 
6 A ^ C E 
1.86- 10-7 
1004 
4;t-D?CE
 
2 ·
4 5 - 1 0~ 1 2 
2004 
12λΕ)'θΕ 
1.65- 10 - 9 
such that 
P i ( N > n - k ) * 
| " _ M ( A D Î C E ) n—k 
The average system failure rate for the koon group, with respect to D failures be-
comes 
AD,G = ηλΏ ■ Ρτ(Ν >n-k)^n{n~_X\k"^k+l 
■ t^k 
(8.39) 
Because ("Zl) = (£ΙΪ)> (8.39) can also be given as 
ik-l. 
λο,ο«»(" 
:)As- t + i-iSi* 
Using (8.39) gives the results in Table 8.5. It is seen that the frequency of D failures 
for a 2oo3 voted group is three times as high as the frequency for a loo2 voted 
group. This is expected because the reliability block diagram of a 2oo3 structure can 
be represented as a series structure of three loo2 structures (see Section 5.2). In the 
same way, a 2oo4 structure can be represented as a series of four loo3 structures 
such that the frequency of D failures of a 2oo4 voted group should be four times the 
frequency of a loo3 voted group, which is reflected in Table 8.5. 
► Remark: Attentive readers may claim that the derivation above may not be correct 
because a second D failure that occurs within (0, ÎCE) may extend the downtime 
beyond this interval. The third D failure may extend the interval even further, and so 
on. This is a good observation, but as all realistic channels have high reliability, the 
effect of this error is negligible. For a DD failure, the repair time is short, typically 
around eight hours. The probability of one independent D failure in an interval of 
eight hours is very small, and the probability of two or more independent D failures 
within such an interval is negligible. DU failures are only revealed in the proof test 
and there can therefore be at most one group DU failure in a proof test interval. DU 
faults of multiple channels are usually repaired when the safety loop is disconnected 
and the EUC is in a safe state. 
© 

226 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Figure 8.11 
Reliability block diagram of a single channel regarded as a series system of 
two virtual elements. 
Equivalent Mean Downtime. 
The next main task is to determine the mean down-
time of the group when a DGF occurs. Inasmuch as the downtime following a DD 
failure is different from the downtime following a DU failure, we need to weigh 
the two downtimes. The weighing process is illustrated first for a single channel. 
Thereafter, the mean downtime for a group of two independent and identical chan-
nels voted loo2 is determined before we determine the mean downtime for a koon 
voted group of n independent and identical channels. 
Equivalent Mean Downtime for a Single Channel. 
Consider a single channel that 
can have both DU and DD failures. The channel can be regarded as a series structure 
of two virtual elements, one element that can have only DU failures and one element 
that can have only DD failures, as illustrated in Figure 8.11. 
If a D failure occurs, the probability that this failure is a DU failure is 
Pr(DU failure | D failure) = 
A p u 
= ^ΞΞ. 
A-DU + Λ-DD 
Ap 
and the probability that it is a DD failure is 
Pr(DD failure | D failure) = 
A p D 
= 
^°-
A-DU + Λ-DD 
Λ-D 
If the D failure is a DU failure, the mean downtime associated with this failure is 
Ε(ΏΌυ) 
=T-+ 
MRT 
A DU fault is revealed only in a proof test and has, on the average, been present 
for a period τ/2 (see Table 8.3). When the DU fault has been revealed in the proof 
test, the channel has to be repaired/restored and the associated downtime is the mean 
repair time (MRT). 
If the D failure is a DD failure, the fault is revealed by the diagnostic system and 
the mean downtime until the channel is restored in MTTR, which has two parts (i) 
the time from the failure until it is revealed and (ii) the time required to repair/restore 
the channel. Part (i) is equal to the half of the diagnostic test interval (usually less 
than some few minutes). 
The channel-equivalent mean downtime (ÎCE) for the channel is therefore 
'CE = ^ 
β + MRT) + ^ M T T R 
(8.40) 
AD 
V^ 
' 
AD 

THE IEC 61508 FORMULAS 
227 
Equivalent Mean Downtime for a 1 oo2 Voted Group. Consider a 1 oo2 voted group 
with two independent and identical channels that can have both DU and DD failures. 
Because the channels are independent, they cannot fail at the same time. To have 
a dangerous group failure, one of the channels must first get a D failure, and when 
this channel is down with a D fault, the other channel must get a D failure. If the 
second failure is a DU failure, the downtime of the loo2 group will, from Table 8.3, 
be approximately τ/3 + MRT. If the second failure is a DD failure, the downtime 
will be MTTR. 
The group-equivalent mean downtime for the loo2 voted group is therefore 
tGE = ^ ( 1 + MRT) + ^ M T T R 
(8.41) 
AD 
V3 
/ 
AD 
Equivalent Mean Downtime for a koon Voted Group. Consider a k oon voted group 
of n independent and identical channels that can have both DU and DD failures. 
With the same argument as above and by using (8.26), the group-equivalent mean 
downtime is 
— ^ D U ( 
AD 
\n 
IGE = ^ P ( 
y 
+ MRT j + ^ M T T R 
(8.42) 
k + 2 
) 
AD 
8.4.2 
Probability of Failure on Demand 
By the IEC formulas, the PFDavg of a voted group (G) is from (8.36) determined by 
PFD<°> = AD,G ?GE 
Another Argument. 
Equation (8.36) may also be justified by the following argu-
ments: The frequency of D failures, AD,G is constant such that the up-time (with 
respect to D failures) is exponentially distributed with mean up-time (MUT) equal to 
1/AD,G· Each time the group fails, the group will be down with a mean downtime 
(MDT) equal to ÎGE· Because the PFDavg denotes the average safety unavailability 
of the group, we may use the well-known formula for unavailability 
PFD 
(G) _ 
MDT 
_ 
?GE 
avg 
MUT + MDT 
i 
+ i G E 
A D . G 'GE 
, 
, , i 
; — * AD,G ÎGE 
(8.43) 
1 + AD,G 'GE 
The approximation is adequate when AD,G 'GE is small, that is, when ÎGE <&. MUT, 
which is fulfilled for all relevant SIFs. 

2 2 8 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
β 
EXAMPLE 8.15 Single channel 
For a single channel, ÎQE = *CE< and the PFD^) is 
PFD^°g
ol) = XDtGE = (ADU + ADD)/GE 
= ADU ( I + MRT) + ADDMTTR 
(8.44) 
Numerical Example. 
The data in Table 7.2 gives 
PFD^°ol) % 4.44 · 10"3 
The simplified formula gives 
ADUT 
p F D(simpl.) ^ J 3 U _ ^ 
4 3 g _ 1 0 - 3 
avg 
2 
The simplified formula gives a lower value because DD failures are disregarded 
in this formula. 
Θ 
fl 
EXAMPLE 8.16 
loo2 voted group 
By combining (8.36) and (8.40), the P F D ^ for a loo2 voted group becomes 
P F D (loo2) = A D , G i G E = 
2X2
DtcEtGE 
= 2 U D U ( x 
+ MRT) 
+ A D D M T T R ] 
• [λου ( J + MRT) + A D D M T T R ] 
(8.45) 
Numerical Example. 
The data in Table 7.2 gives 
PFD^02* «2.64-10 - 5 
The simplified formula gives 
p F D(simpl.) % wv-, 
% 2.56 · 10 
(simpl.) ^ 
( ADU T) 
- , 9 r, . ,n-5 
The simplified formula again gives a lower value because DD failures are disre-
garded in this formula. 
Θ 

THE IEC 61508 FORMULAS 
229 
fl 
EXAMPLE 8.17 2oo2 voted group 
A 2oo2 voted group is a series structure that fails when the first channel fails. 
The average group failure rate is here AD,G = 2AD and the group-equivalent 
mean downtime ÎQE is the same as the channel mean downtime ÎCE· The 
PFD^ofthis group is therefore 
avg 
P F D ( 2 O O 2 ) = 2 A D Î C E 
avg 
= 2 [ λ ο υ (^ + MRT) + A D D M T T R ] 
(8.46) 
fl 
EXAMPLE 8.18 2oo3 voted group 
A 2oo3 group fails when at least two of its three channels fail. The average 
system failure rate is from (8.38) 
A D , G = 6A D Î C E 
The group-equivalent mean downtime is the same as for the loo2 group (8.42). 
The PFD<£) for the 2oo3 group is therefore 
p F D ^ o o 3 ) = 6 A 2 ) i c E i G E 
= 6 [ A D U Q + MRT) 
+ ADDMTTR] 
• [λου (I + MRT) + ADDMTTR] 
(8.47) 
PFDavg for a koon Voted Group. The P F D ^ for a koon voted group of inde-
pendent and identical channels is determined from (8.42), where the frequency of 
dangerous group failures AD,G is given by (8.38) and the group-equivalent mean 
downtime, ?GE is given by (8.41). 
p F D(koon) = 
λ 
G E 
avg 
= n\ 
+ MRT) + ADDMTTRI 
(8.48) 

230 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
β 
EXAMPLE 8.19 3oo5 voted group 
Consider a 3oo5 voted group of independent and identical channels. With n = 5 
and k = 3, (8.49) becomes 
pFD(.3oo5) = 30A2 t2E [ A D U Q + MRT) + A D D M T T R ] 
By using the data set in Table 7.2, we obtain 
pFD(.3oo5) % U3.1Q-6 
► Remark: It is not straightforward to see how the IEC formulas can be adapted to 
voted groups with independent and nonidentical channels. It may be possible, but 
will be rather cumbersome. 
Θ 
8.4.3 
PFDavg with Common-Cause Failures 
The beta-factor model for common-cause failures (CCFs) was introduced in Sec-
tion 5.4 and is thoroughly discussed in Chapter 10. In Section 8.3, the beta-factor 
model was used together with the simplified formulas. This section shows how the 
beta-factor model can be used together with the IEC formulas. IEC 61508 applies 
two beta-factors, β for DU failures and βτ, for DD failures. It is generally acknowl-
edged that ßu < ß- The rationale for this difference is discussed in Chapter 10. In 
Section 8.3, a channel was represented as a series structure of an independent part 
and a virtual common-cause element. Further, Section 8.4.2 shows that a channel 
can be split into a virtual part that is only exposed to DU failures and another virtual 
part that is only exposed to DD failures. By combining these approaches, a channel 
can be split into four virtual parts: 
- An independent part that is exposed only to DU failures 
- An independent part that is exposed only to DD failures 
- A common-cause element for DU failures (DU-CCFs) 
- A common-cause element for DD failures (DD-CCFs) 
The channel can be represented as a series structure of these four parts, as illustrated 
in Figure 8.12. Following the approach in Section 8.3, a voted group of channels 
can be represented as a series structure of three parts: (a) an "independent" part 
comprising a number of independent and identical channels with failure rates (1 — 
/5)ADU anc^ 0 ~~ /?D)ADD, w^tn r e sP e c t t 0 DU and DD failures, respectively; (b) 
a virtual CCF element representing DU failures, with failure rate ßXuv\ and (c) a 

THE IEC61508 FORMULAS 
DLl 
DD,_ 
CCF„„ 
CCF„, 
Figure 8.12 
A channel represented as a series structure of four parts. 
Figure 8.13 
Reliability block diagram for a 2oo3 voted group with DU and DD-CCFs 
modeled with the beta-factor model. 
virtual CCF element representing DD failures with failure rate POADD. a s shown by 
the reliability block diagram for a 2oo3 voted group in Figure 8.13. 
We showed in (8.1) that the PFDavg for a series structure is approximately equal 
to the sum of the PFDavg's of the elements of the series structure. The PFD^g of a 
voted group modeled with the beta-factor model can therefore be written as 
PFD£) = P F D « g + PFD£g
u"CCF> + P F D ^ - C C F ) 
( j U 9 ) 
where, 
- PFD^g is the PFDavg of the voted group of the independent channels. 
- PFD^u-001") is the PFDavg of the virtual CCF element for DU failures. 
- PFD^ g
3- C C F ) is the PFDavg of the virtual CCF element for DD failures. 
In line with (8.30), 
P F D ( D U - C C F ) Ä βλΌυ 
(I 
+ M R T ) 
( 8 5 0 ) 
where βλΌν 
is the rate of DU-CCFs and (τ/2 + MRT) is the mean downtime for 
each DU-CCF. 
Similar for the DD-CCFs, 
P F D ^ D - C C F ) % £ D A D U M T T R 
( 8 
5 1 ) 
where POADU is the rate of DD-CCFs and MTTR is the mean downtime for each 
DD-CCF. 
2 3 1

AVERAGE PROBABILITY OF FAILURE ON DEMAND 
EXAMPLE 8.20 2oo3 voted group of identical channels 
Consider a 2oo3 voted group of identical channels and assume that the input data 
set in Table 7.2 is applicable. Further, assume that the repair time of a multiple 
channel fault (i.e., a CCF) is the same as the repair time of a single channel. The 
independent (i) failure rates are 
ADU = 0 _ß)ADU 
ADD = (1 — )ÖD)^DD 
l (0 _ I 0) 
, i (0 
A D 
— 
A D U "·" A D D 
By using (8.47), and the data set in Table 7.2, the PFDavg for the independent 
part of the 2oo3 voted group is 
PFD^g % 6.43 · 10~5 
The channel-equivalent mean downtime of the independent part is 
tCE = -ψ± ( - + MRT ) + -%%■ · MTTR = 605.5 hours 
A D 
A D 
The group-equivalent mean downtime for the independent part of the 2oo3 
voted group is 
«GE = -¥r ( - + MRT ) + - ? £ · MTTR = 425.1 hours 
A D 
A D 
The contribution to the PFDavg from DU-CCFs is 
10"4 
PFDÇDU-CCF) % βλΌν 
(Z + ΜΚή 
^ 4 3 9 . 
and the contribution to the PFDavg from DD-CCFs is 
PFD<£°-CCF) % ßDADDMTTR ss 2.40 · 10"6 
The total PFDavg for the 2oo3 voted group is therefore 
PFr>(G) = PFD« + P F T ) ( D U - C C F ) 4- P F n ( D D- C C F) 
1 * 
avg 
λ l 
avg ' x l 
avg 
' L l 
avg 
ss6.43-10" 5+ 4.39· 10-4 + 2.40 · 10~6 % 5.06 · 10"4 
Ind:12.7% 
DU-CCF:86.8% 
DD-CCF:0.4% 
The P F D ^ for the 2oo3 voted group for the input data in Table7.2 is seen 
to be dominated by the contribution from the DU-CCFs, whereas the influence 
from the DD-CCFs is almost negligible. 
φ 
2 3 2

THE PDS METHOD 
233 
► Remark: As mentioned in Section 7.4, the operational strategy for the EUC and 
the SIF may strongly influence the PFDavg. For SIFs operated in high-demand mode, 
IEC 61508-6 advocates the strategy to immediately bring the EUC to a safe state 
when a dangerous SIF fault is detected. This way, detected dangerous SIF faults are 
not dangerous for the EUC. If the same strategy were adopted for low-demand SIFs, 
DD-CCFs should be disregarded when calculating the PFDavg. 
This strategy will also have effect on the repair times, MRT and MTTR. If a 
dangerous SIF fault is detected, and the EUC is immediately brought to a safe state, 
the mean repair times will not be dangerous and should be disregarded in the PFDavg 
formulas. 
® 
8.5 The PDS Method 
The PDS method is developed by SINTEF, the Norwegian contract research or-
ganization, and deals with SIFs that are operated in both low-demand and high-
demand mode. The method complies with the main principles of IEC 61508 and 
IEC 61511 and is widely used in the Norwegian petroleum industry; it can also be 
used in other application areas. A detailed description of the method is provided 
in the PDS method handbook (SINTEF, 2013b), which, together with the PDS data 
handbook (SINTEF, 2013a), offers a practical approach towards implementing the 
quantitative aspects of the standards. 
The PDS method has some notable differences compared with the IEC standards: 
- There is a slightly different definition of failure mode categories (see Chapter 3). 
- A more detailed, and slightly deviating, set of safety performance measures is 
used. 
- Another model for CCFs, using the multiple beta-factor model (see Chapter 10), 
is used 
- The probability of systematic failures is quantified. 
- A more detailed modeling of several practical issues is provided. 
8.5.1 Safety Performance Measures 
The safety performance measures used in the PDS method are not the same as in 
the IEC standards. A central term in the PDS method is loss-of-safety and a SIS is 
said to have loss-of-safety when it is in such a state that it cannot perform its SIE 
Loss-of-safety can be due to a dangerous failure, or when the SIF is unavailable 
due to testing, repair, or preventive maintenance. Another central PDS term is test-
independent failure (TIF), which is a dangerous fault that is not detected in a proof 
test and that therefore remains in the system. The three main performance measures 
in the PDS method related to loss-of-safety are listed in Table 8.6. 

234 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Table 8.6 
PDS performance measures for loss of safety. 
Measure 
Description 
PFDavg,i 
This is the unavailability due to DU failures within the proof test in-
terval and is the most dangerous type of unavailability because it is 
unknown. We believe that we are protected by the SIF, but we may not 
be. In the PDS method, this unavailability has two elements: 
(a) Unavailability due to DU random hardware failures, which occur 
with rate ADU,RH 
(b) Unavailability due to DU systematic failures, which occur with 
rate ADU.SYST 
The symbol PFDavg,i is used because the PDS interpretation of 
PFDavg is the same concept we introduced in Table 8.4 and not the 
one obtained by the IEC formulas. 
DTU 
Downtime unavailability (DTU) is the unavailability due to known or 
planned downtime. This unavailability is caused by items taken out 
of function for testing and/or repair/maintenance. The unavailability 
is therefore known and precautions may be taken. The downtime un-
availability has two main contributors: 
(a) The known unavailability due to repair of dangerous failures (de-
tected either by diagnostics or in a proof test). 
(b) The planned and known unavailability due to downtime during 
proof tests and/or preventive maintenance. 
PTIF 
Unavailability due to test-independent failures (TIFs). This is unavail-
ability due to DU faults that are not revealed during proof tests but that 
will be manifested during a real demand. 
A notable feature of the PDS method is the distinction between unknown unavail-
ability and known unavailability. The unknown unavailability is measured by the 
PFDavgii in the proof test interval. The PFD avg;i denotes the average probability 
that we are unprotected by the SIF when we believe that we are protected. The 
known unavailability is the unavailability during repair, testing, and planned main-
tenance when we know that we are unprotected. These two concepts can easily be 
distinguished when using the simplified formulas, but it is not equally obvious how 
this can be done with the IEC formulas. 
8.5.2 
Critical Safety Unavailability 
The term critical safety unavailability (CSU) of a SIF is defined in the PDS method 
as the probability that the SIF cannot be performed if a demand should occur. The 

THE PDS METHOD 
235 
Table 8.7 
Two types of downtime unavailability. 
Measure 
Description 
DTUR 
The part of the downtime unavailability due to repair of dangerous (D) 
faults, resulting in a period when it is known that the SIF is unavailable. 
DTUT 
The part of the downtime unavailability resulting from planned activi-
ties, such a proof-testing and planned maintenance. 
CSU is calculated as 
CSU = PFDavg,i + DTU + PTÏF 
(8.52) 
The downtime unavailability, DTU, can be split in two parts as specified in Ta-
ble 8.7. Observe that PFDavgji+ DTUR corresponds to the PFDavg that is calculated 
by the IEC formulas. 
8.5.3 
Modeling of Common-Cause Failures 
The PDS method applies a CCF model based on the multiple beta-factor (MBF) 
model, which is described in Chapter 10. The MBF model is a generalization of the 
beta-factor model that was introduced in Section 5.4. The beta-factor model only 
distinguishes between failures of individual channels and total group failure (i.e., 
where all channels of the voted group fail due to a common cause). The PDS method 
argues that this is not realistic and that a common-cause may lead to an arbitrary 
number of channel failures. By this logic, a koon voted group may survive the 
common-cause incident when no more than n — k of the n channels fail. 
The main parameter of the MBF model is still ß, but with a slightly different 
definition compared with the traditional beta-factor model. In the MBF model, ß 
is the conditional probability of at least one more channel failure, given that one 
channel has failed, but in practice, the same beta-factor as used in the beta-factor 
model is applied. The dependency on the actual koon configuration is included by a 
configuration factor. 
By this approach, the PFDavgji can be split into two parts. 
PFDavgjl = PFD«g>1 + PFD<?g>1 
(8.53) 
where PFDa^ x is the PFDavg within the proof test interval (0, τ) due to independent 
channel failures, and PFD^ x is PFDavg within the proof test interval (0, τ) due to 
CCFs, similar to the traditional beta-factor model. 

236 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Table 8.8 
Configuration factors, C^oon, for selected koon architectures (Reproduced with 
permission from SINTEF). 
k/n 
k = 1 
k = 2 
k = 3 
k = A 
k = 5 
n=2 
1.00 
-
-
-
_ 
n = 3 
0.50 
2.00 
-
-
_ 
n = 4 
0.30 
1.10 
2.80 
-
_ 
n = 5 
0.20 
0.80 
1.60 
3.60 
— 
« = 6 
0.15 
0.60 
1.20 
1.90 
4.50 
The Configuration Factor Ckoon- The common-cause part, PFD^ x, is expressed 
as 
P F D i ^ C ^ / î ^ 
(8.54) 
where the configuration factor Ckoon has the recommended values given in Table 8.8. 
The values in Table 8.8 are partly supported by empirical data, but it seems obvious 
that the value, for example, for a 2oo3 system will not be the same for all types of 
channels and for all applications. The following assumptions are made as a basis for 
selecting Ckoon values (SINTEF, 2013b): 
- Given a common-cause failure of two redundant channels, the probability that a 
third similar channel will also fail due to the same cause is approximately 50%. 
- To account for the reduced effect of added redundancy, it is assumed that: 
- When three channels have failed due to a CCF, the probability that the fourth 
channel will also fail is approximately 60%. 
- When four channels have failed due to a CCF, the probability that the fifth 
channel will also fail is approximately 70%, and so on. 
- Finally, when seven or more channels have failed due to a CCF, the assumed 
effect of adding one more channel is negligible. This means that if seven 
channels have failed due to a CCF, the probability that the eighth channel 
will also fail is 100%. 
The suggested algorithm cannot be strictly followed to obtain the Ckoon values in 
Table 8.8. More details can be found in SINTEF (2013b, Appendix B). 
In the beta-factor model the channel DU failure rate ADU is split into two parts, 
an independent part λ^ν and a common-cause DU failure rate λ^υ, where 

THE PDS METHOD 
2 3 7 
In the PDS method, the CCF part of the DU failure rate is not necessarily subtracted 
from the total DU failure rate, and extra conservatism is therefore added.1 
8.5.4 
PDS Formulas for Low-Demand Voted Groups 
The PFDavg>i in (8.52) is illustrated in this section by some simple voted groups. In 
the basic version of the PDS method, the contribution from DD failures is not taken 
into account. 
Single Channel. For a single channel, the simplified formula (8.10) is used 
p F D(iooi) 
W r 
A single channel does not need any special treatment of CCFs because the DU-CCF 
rate is included in the total DU failure rate. 
1oo2 Voted Group of Identical Channels. For a loo2 voted group of identical 
channels, the independent part is determined by the simplified formula (8.13) 
p F D(loo2:i) ^ 
(ApTjr) 
As mentioned, the PDS method does not necessarily reduce the total DU failure 
rate when calculating the PFDavg,i for the independent part. This will add extra 
conservatism to the total PFDavg, but the difference may not be significant. 
The CCF contribution to the PFDavg!i is 
p p n(loo2:c) _ r 
R ^DU T 
F h D a v g , l 
- 
Cloo2 P — ^ ~ 
From Table 8.8, the configuration factor Ci002 = 1 and the total PFDavg;i for the 
loo2 voted group is 
pFn(ioo2) _ (ADUr)2 
ADUr 
^uavg,i ~ 
3 
+p^r 
koon Voted Group of Identical Channels. For a koon voted group of identical 
channels, the simplified formula (8.21) is used together with (8.53): 
^(koon) ^ ( 
n 
\ (ApuT)" 
ArjTjT 
{n—k + ln—k 
+ 2 
2 
PFD™ * L 
I , , I ■ l , . 
+ C*°°« ß - ^ 
(8·55) 
forfc < n. 
'The CCF part of the DU failure rate is not subtracted in the main part of the PDS method handbook, 
whereas it is subtracted in Appendix C of the handbook. 

238 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
0 
EXAMPLE 8.21 2oo3 voted group of identical channels 
Consider a 2oo3 voted group of identical channels and assume that the input 
data set in Table 7.2 is applicable. 
By using (8.54), the PFDavgil is 
PFDavg,l 
% I 2 I 
3 
+ C2oo3P ~^— 
From Table 8.8, C2003 = 2.0, and 
Ind 
CCF 
= 7.67 · 10-5 + 8.76 · 1(Γ4 = 9.53 ■ 10"4 
v 
' 
~ 
» 
' 
8.1% 
91.9% 
The PFDavgil of the 2oo3 voted group is seen to be dominated by the con-
tribution from CCFs. If the independent DU failure rate had been reduced to 
(1 - β)λΌυ, 
the PFD<£°S) would be reduced to 9.38 · 10"4, a reduction that, in 
most cases, will not be significant. 
Comment. The traditional beta-factor model gives 
PFDivS * K1 - ^ ο υ τ ] 2 + β ■ ϊψ- 
= 5.00 · 10-4 
which is a more "optimistic" result. By comparing with Example 8.20, it is noted 
that the PDS method gives a more conservative result than the IEC formulas for 
the 2oo3 voted group with the input data set in Table 7.2. 
® 
8.5.5 CCF Modeling for Nonidentical Channels 
Diversification is often recommended to reduce the susceptibility to CCFs. The chan-
nels of a koon may, according to the PDS method, have three types of differences: 
- Channels with different failure rates 
- Channels with different ß -factors 
- Channels with different proof test intervals 
Any combination of these types of differences may also be relevant. 
The PDS method suggests modeling the CCF part of the PFDavg)i as 
"'I'avg,! — (-'koon ßn I I ^ου,ί 
.;' = ! 
I/« _ 
I 
(8.56) 

THE PDS METHOD 
239 
where ßmin 
is the minimum of the estimated ß's for the different types of channels, 
1 In 
[Πί"=ι ^DU,( ] 
is the geometric average of the individual failure rates, and τ is the 
arithmetic average of the individual proof test intervals. 
fl 
EXAMPLE 8.22 2oo3 voted group of different channels 
Consider a 2oo3 voted group of different channels (e.g., a fire detection system 
with three different types of detectors; flame, smoke, and heat). The channels 
are numbered 1, 2, and 3 and the failure rates (per hour) have been estimated to 
be ADu,i = 5 · 10~7, ADU,2 = 5 · 10-6, and λ ο υ,3 = 8 · 10"7, the beta-factors 
are estimated to be β\ = 0.10, βι = 0.05, and β$ — 0.07, and the proof test 
intervals to be τ\ = r2 = 8 760 hours and TJ, = 4 380 hours. The elements of 
(8.55) become 
C2003 
= 2 . 0 
ßmin = 0.5 
3 
"I 1/ 3 
ΠΑΓ 
= 1.26· 10-6 per hour 
DU,; 
.i = l 
Ϊ = (n + r2 + τ3)/3 = 7300 hours 
The independent part is from (8.17) 
) + 
( A D U , 1 ^ D U , 3 ) + 
( A D U , 2 ^ D U , 3 ) ] 
%2.3-10 - 1 2 
and the CCF part of the PFDavg!i is 
p F D(2oo3:c) = 
4 6 0 . 1 0 _ 4 
The PFDavgjl obtained for the 2oo3 system is therefore 
p F D £ o o 3 ) = p F D^oo3:i) + p F D^oo3:c) % 
4 6 Q _ 1 Q - 4 
The contribution from the independent part is seen to be negligible in this case. 
8.5.6 
Downtime Unavailability 
The downtime unavailability (DTU) can be split into two types: 
1. The downtime related to repair of (dangerous) faults. 
2. The downtime (or inhibition time) caused by planned activities such as proof-
testing and preventive maintenance. 

2 4 0 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
The contribution of the two types of downtime to the safety unavailability of a SIF 
is affected by several issues and it is therefore not possible to provide formulas that 
are generally applicable. We therefore suffice with some illustrative examples. 
DTU Due to Repair of Dangerous Faults. The average downtime unavailability 
due to repair of dangerous faults is denoted by DTUR. The PDS method discusses 
three different strategies for repair actions. The same strategies are also discussed 
briefly in Section 7.4. 
Here, we consider only one of the strategies. By this strategy, the EUC is al-
ways operated during the repair action. During repair of channel faults that do not 
constitute a dangerous SIF fault, the SIF operates in a degraded mode, and when a 
dangerous SIF fault is repaired, the EUC continues operating without protection. 
The following assumptions are made: 
- The mean repair time (MRT) of DU faults does not depend on the number of 
failed channels, but the formulas can be easily adjusted to account for an MRT 
that increases with the number of channels being repaired. 
- Degraded operation takes place if possible. In case of a single channel fault in 
a koon voting, the group continues to operate as a (k — l)oo(« — 1) voted group 
(when n > 2 and k > 2) during the repair time.2 
- If all redundant channels have failed, the voted group has a dangerous group 
fault (DGF), and the operation of the EUC continues without protection. 
If a channel is taken out for repair in a koon voted group, the group can continue 
to operate as a (k — l)oo(n — 1) voted group, and the new configuration has a higher 
reliability than the original one. Consider, for example, a 2oo3 configuration, that 
becomes a loo2 configuration when a channel is repaired. The PFDavg for these 
configurations are 
PFD£° gf*(A D Ur) 2 + 2.0£ A D U r 
2 
(ioo2) ^ (λρυτ) 
ι ] na ADut 
~ 
3 
pFD(loo2) % 
V^J_ 
+ 
l Q ß 
by using the C^oon values in Table 8.8. 
A Single Channel. 
Consider a single channel with DU failure rate ADu and proof 
test interval τ, and assume that the proof test has 100% coverage. When a DU 
fault is revealed in a proof test, it is repaired and the mean repair-time is MRT. The 
probability that a DU fault is revealed is 1 — e" i D D t ss λου^. The mean downtime 
2The voting for the degraded operation can normally be set in the logic solver. The voting for degraded 
operation during the repair of a single DU fault is here set to (k — l)oo(n — 1). An alternative voting 
would be koo(n — 1), but this voting is less safe. 

FAULT THEE APPROACH 
241 
due to such repair is then approximately MRT-λουτ. and the mean downtime per 
time unit is 
D T U R % MRT-ADUt = MRT · λΌυ 
(8.57) 
τ 
Two Channels Voted 1oo2. Consider a loo2 voted group of two independent and 
identical channels. Each channel has DU failure rate ADU and is proof-tested at the 
same time with test interval τ. In a proof test, the following may be revealed: 
1. No DU failure, with probability px = Γ 2 ^ » 1 
2. One DU failure, with probability p2 = 2(1 - β~λ°υτ) 
e~kDvT % 2λ ο υτ 
3. Two DU failures, with probability p3 = (1 - 
β"λΌυτ)2 
When two DU faults are revealed and both channels are repaired together, the 
voted group is unavailable during the repair time. The contribution to the downtime 
unavailability is MRT3 · ρι/τ, 
where MRT3 is the mean repair time for option 3 
when both channels are repaired together and the voted group is not started again 
until both channels are ready for start-up. 
In option 2, where one DU fault is revealed, we have two options 
- Both channels are taken out of operation during the repair of the failed channel. 
The contribution to the downtime unavailability for this option is MRT ■ ρ2/τ, 
where MRT is the mean repair time of one channel. 
- Only the failed channel is taken out for repair, while the other channel (lool) 
remains in operation. The voted group then operates in degraded mode during 
the repair action. If the remaining channel gets a D failure during the repair, the 
group fails. The repair time is, however, usually so short that the probability of 
this option is negligible. 
It may sometimes be relevant to determine the proportion of time the voted 
group has a degraded operation (with only one channel functioning). For more 
details, see SINTEF (2013b). 
8.6 
Fault Tree Approach 
Fault tree analysis (FTA) is suggested in IEC 61508-6 as a relevant approach for 
reliability analysis of a SIF and is introduced briefly in Section 5.3. FTA has since the 
1960s been a common method in reliability and risk analyses and most SIF reliability 
analysts are therefore familiar with FTA. The FTA theory is well developed and 
several computer programs for FTA are available. FTA provides a graphical model 
that is easy to understand and is therefore a suitable tool for communication with 
decision-makers and other stakeholders who are not experts in reliability analysis. 
FTA is therefore a natural choice for many SIF reliability analysts. 

242 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Sensor subsystem 
fails-to-fu notion 
on demand 
SIF fails-to-function 
on demand 
Ü 
Logic solver subsystem 
fails-to-function 
on demand 
Final element subsystem 
fails-to-function 
on demand 
Figure 8.14 
Fault tree illustrating a "series" of three subsystems. 
8.6.1 
Fault Tree Construction 
A fault tree can be constructed at different levels of the system hierarchy, such as 
for the whole SIF, for a subsystem, or for a voted group of channels. Relevant TOP 
events for the whole SIF may be: 
TOPi : The SIF cannot be performed 
TOP2: The SIF is activated spuriously 
The SIF and the EUC must be specified together with the operation going on when 
the TOP event occurs (i.e., normal production, start-up). 
fl 
EXAMPLE 8.23 A SIS with three subsystems 
Consider a simple SIS with three subsystems. A SIF to be performed by this 
SIS can, on the top level, fail-to-function in three different ways: (i) the sen-
sor subsystem fails to detect the demand or fails to "inform" the logic solver 
subsystem about the demand, (ii) the logic solver subsystem fails to receive the 
message from the sensor subsystem or fails to decide on the appropriate actions, 
or (iii) the final element subsystem fails to perform the required actions. The top 
structure of a fault tree for the TOP event "SIF fails-to-function on demand" is 
shown in Figure 8.14. The triangle below the input events are transfer symbols 
and indicate that a new fault tree will be constructed to explain the causes of the 
fault. 
Assume that the sensor subsystem has three independent and identical chan-
nels of pressure transmitters (PT) that are voted loo3. This means that the sub-
system is functioning as long as at least one of its three channels is functioning, 
and will only fail when all three channels fail. A fault tree for the event "sensor 
subsystem fails-to-function on demand" is shown in Figure 8.15. The triangle 
to the left of the TOP event of the fault tree is a transfer-in symbol indicating 
that this fault tree shall be linked to the corresponding event in the fault tree in 
Figure 8.14. 

FAULT TREE APPROACH 
243 
fails-to-function 
on demand 
Sensor subsystem 
fails-to-function 
on demand 
7^\ 
PT 2 
fails-to-function 
on demand 
PT3 
fails-to-function 
on demand 
Figure 8.15 
voting. 
Fault tree for the sensor subsystem with three independent channels and loo3 
Sensor subsystem 
fails-to-function 
on demand 
X 
Independent 
PT failures 
on demand 
7^\ 
m 
PT, 
fails-to-function 
on demand 
PT
2 
fails-to-function 
on demand 
P T 3 
fails-to-function 
on demand 
Common-cause 
failure 
on demand 
Figure 8.16 
Fault tree for the sensor subsystem with three channels voted loo3 and where 
a common-cause failure may occur. 
The fault tree in Figure 8.15 has only one minimal cut set {PTi, PT2, PT3}. 
All the three events have to occur for the TOP event of the tree to occur. If we 
remove one of the events in the minimal cut set, the set is no longer a cut set. 
If the three pressure transmitters cannot be assumed to be independent, we 
include a virtual common-cause event into the fault tree as illustrated in Fig-
ure 8.16. In this case, the sensor subsystem will fail-to-function when either 
all the three pressure transmitters fail independently or when a common-cause 
failure (CCF) occurs. 
The fault tree in Figure 8.16 has two minimal cut sets: {PTi, PT2, PT3} and 
{CCF}. If all the pressure transmitters fail-to-function, or if the CCF occurs, the 
TOP event occurs. 
φ 

244 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Sensor subsystem 
fails-to-f unction 
on demand 
Minimal 
cut-set 1 
fails 
T*\ 
PT, 
fails-to-f unction 
on demand 
PT2 
fails-to-function 
on demand 
Minimal 
cut-set 2 
fails 
7~\ 
_n 
π 
PT, 
fails-to-function 
on demand 
IL 
PT, 
fails-to-function 
on demand 
Minimal 
cut-set 3 
fails 
7^\ 
ρη, 
fails-to-function 
on demand 
i 
PT3 
fails-to-function 
on demand 
Figure 8.17 
Fault tree for a 2oo3 voted group of sensors. 
Fault Tree for a koon Voted Group. Consider a voted group with n independent 
and identical channels that are voted koon. This means that the voted group is func-
tioning when at least k of the n channels are functioning, and fails-to-function when 
at least n — k + 1 channels fail. All the minimal cut sets therefore have exactly 
n — k + 1 identical events (channel failures). If a cut set has more than n — k + 1 
events, the set is not minimal because we can remove at least one event and still have 
a cut set. If we, on the other hand, remove an event from a minimal cut set (i.e., with 
n — k + 1 events), the set is no longer a cut set because the subsystem is able to 
function with n—k failed channels. 
n — k + 1 channels can be selected among n channels in (B_£+1) different ways. 
Because any system can be represented as a series structure of its minimal cut par-
allel structures (see Section 5.2.4), a fault tree for the koon voted group can be 
constructed with an OR-gate linking all the minimal cut fault trees. This is illustrated 
in Example 8.24 for a 2oo3 voted group of independent channels. 
fl 
EXAMPLE 8.24 Fault tree for a 2oo3 voted group 
Consider a voted group with three independent and identical channels of pres-
sure transmitters (PT). The channels are voted 2oo3, meaning that the group is 
functioning when at least two of the three channels are functioning and fails-to-
function when at least two channels fail-to-function. The voted group therefore 
has three minimal cut sets {PT!, PT2}, {PTi, PT3}, and {PT2, PT3}. The voted 
group fails-to-function if any of these minimal cut sets fail. The fault tree can 
be drawn as shown in Figure 8.17. 
Θ 
The fault tree for a koon voted group becomes very large when n > 3, and a 
special logic gate has therefore been made for such systems. This koon gate for a 
2oo3 voting is shown in Figure 8.18 for the same voted group as in Example 8.23. 

FAULT TREE APPROACH 
245 
Sensor subsystem 
fails-to-function 
on demand 
fails-to-function 
on demand 
PT2 
fails-to-function 
on demand 
PT3 
fails-to-function 
on demand 
Figure 8.18 
Fault tree for a sensor subsystem with three channels voted 2oo3 by using : 
2oo3 gate. 
Fault Trees versus Reliability Block Diagrams. As long as a fault tree is con-
structed solely by AND- and OR-gates, the fault tree can easily be converted to a 
reliability block diagram, and vice versa. When the fault tree or the reliability block 
diagram has been constructed, the two approaches should therefore give the same re-
sults. The approaches to construct the diagrams are, however, different. In an FTA, 
the focus is on failures and the fault tree diagram is constructed by repeatedly asking 
"how can this item fail"? For a reliability block diagram, the diagram is items that 
need to function for the system function to be performed. 
8.6.2 
Quantitative FTA - Simple Formulas 
To determine the probability Qo(t) of the TOP event at time t, we need to provide 
input data to all of the basic events in the fault tree. The data required depends 
on the type of basic events, our assumptions, and the limitations for the analysis. 
When the TOP event is fail-to-function of a SIF, the TOP event probability is usually 
Qo(t) = PFD(i) and we may, in many cases, suffice with DU failure rates, proof 
test intervals, mean repair times, beta-factors, and human error probabilities. 
The assumptions in Section 7.9 apply but a diagnostic system is not available such 
that ADD = 0. 
Let qi(t) denote the basic event probability at time t for basic event i. Most of 
the computer programs for FTA calculate q, (t) for a periodically tested channel by 
9/(0 
ADU.ÎT,- + λ DU,/' MRT, 
(8.58) 
The probability of the TOP event βο(0 is usually calculated by the upper bound 
approximation formula 
Qo(t)<l-Yl[l-Qj(t)] 
(8.59) 
J = l 

2 4 6 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
For hand calculation, this formula is sometimes simplified as 
k 
ßo(0;$£ß;(0 
(
8-
6°) 
j=i 
where Qj (0 K m e probability that minimal cut set j is failed at time t and is calcu-
lated as 
Qj(t) = Π 9i(t) 
(8.61) 
ieCj 
where Cj denotes minimal cut set j . For details, see Chapter 5. 
The approach in (8.59) and (8.61) implies that the events of a minimal cut set are 
treated as independent events. 
Importance of Minimal Cut Sets. 
The importance of the various minimal cut sets 
may be very different. The importance of minimal cut set j can be quantified as the 
contribution of minimal cut set j to the TOP event probability. 
ZT-« = f f 
(8-62) 
When considering SIF modifications and risk-reducing measures, the importance of 
the minimal cut sets may help in focusing efforts. 
β 
EXAMPLE 8.25 PFDavg of a 2oo3 voted group by FTA 
Reconsider the 2oo3 voted group in Figure 8.16. All three channels are inde-
pendent and identical with DU failure rate ADU a nd are proof-tested at the same 
time with test interval r. The repair time is so short that it is considered to be 
negligible (i.e., MRT = 0). Otherwise, the assumptions are as above. 
From (8.58) and (8.61), the average probability that minimal cut set 1 is failed 
is 
öi =qtqi = ( —^— J = 
^ 
(8.63) 
Because the channels are identical, the three minimal cut sets have the same 
failure probability. By using (8.61), we get the average PFD for the voted group: 
PFDs,avg < ö i + 03 + 03 = 3 ( λ Ρ
4
υ τ ) 
(8.64) 
This result is not compatible with the results we got by using the simplified 
formulas and the IEC approximation formulas in Sections 8.3 and 8.4, respec-
tively. A parallel structure of two channels (i.e., loo2 voting) is the same as 
the minimal cut set 1 in (8.62). In Section 8.3, we got PFDavg «a (ADU^) 2/3, 
which is different from the PFDs,avg (i-e., Q\) in (8.63). The same applies for 
the 2oo3 voting in (8.64). The number we get by using FTA is different from the 
PFDavg ^ (^DU^) 2 we got in Section 8.3. If the values we got in Section 8.3 
are approximately correct, the values we get by FTA are nonconservative! 
Θ 

FAULT TREE APPROACH 
247 
Table 8.9 
PFDaVg calculated by simplified formulas and FTA for some koon voted groups 
of independent and identical channels. 
Voting 
Approx. formula 
FTA formula 
Difference 
1οο2 
33% 
4 
1OO3 
v 
" ' 
■ „ ' 
100% 
Approx. formula 
(λουτ) 2 
3 
(λΌυτ)3 
4 
(A D Ur) 2 
(ληυτ) 4 
5 
(Aour)3 
2003 
(ÀDur) 
33% 
4 
loo4 
r ^ i ^ 
i ^ i l ! 
220% 
16 
2004 
(Aour)5 
(λΌυτ) 
lQQ% 
3oo4 
2(ADUT) 
33% 
PFDavg o' a koon Voted Group by FTA. 
Consider a koon voted group of n inde-
pendent and identical channels. Each of the ( η_£ + 1) minimal cut sets of the group 
has n — k + 1 channels. The PFD a v g of each channel is λουτ/2 
and the PFD a v g of 
a minimal cut set (mes) is by using the simple FTA formulas 
PFD m c s = ( * H H l ) 
By using (8.59), the PFD a v g of the koon voted group is 
™E U Î ( „ _ : + 1 ) ( ^ ) " + ' 
In (8.22), we determined the PFDfc00„ by the simplified formulas and got 
^(simpi.) < [ 
n 
\ ( Α ρ υ τ ) " 
k+l 
■k+2 
PFD™ $ I _ , , , 1 
,. . . 
(8-66) 
The two formulas (8.65) and (8.66) do not give the same result. The differences 
[in relation to (8.65)] for some simple koon voted groups are given in Table 8.9. It 
is seen that the FTA approach gives a lower value for PFDavg· The reason for this 
difference is that "the product of averages is not equal to average of products." A 
similar comparison is presented in Table 8.1. 

2 4 8 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
► Remark: Some computer programs for FTA have subroutines to handle common-
cause failures by the beta-factor model, but in many programs we have to model 
CCFs explicitly in the fault tree, as we did in Figure 8.16. 
Θ 
8.7 Markov Approach 
The methods described so far in this chapter are static and not able to handle dynamic 
systems.3 For dynamic systems, the most commonly used approaches are 
1. Markov approach 
2. Petri net approach 
The Markov approach is described in the current section, and the Petri net approach 
is treated in Section 8.8. The Markov approach was introduced in Section 5.4 and 
the reader is referred to that section for general aspects. A Markov model can be 
quantified to give either steady-state probabilities or time-dependent probabilities. 
In this section, we consider a koon voted group. To simplify the presentation, 
it is assumed that all the channels are identical with the same failure rates and re-
pair/restoration rates. It is, however, fully possible to have nonidentical channels, 
but this leads to many more states in the Markov models and more cumbersome 
solutions. 
Consider a group that can be in r + 1 distinct states, arrange the states in a se-
quence, and give them numbers in the state space X = {0,1,..., r}, where 0 is the 
"best" state and r is the "worst" state of the group.4 
At time t = 0, the voted group starts out in a state specified by the initial proba-
bility distribution P(0) = [P0(0), Λ(0),..., Pr(0)], where P,(0) is the probability 
that the group is in state / at time t = 0. We may, for example, assume that the voted 
group starts in the best state (i.e., X(0) = 0) at time t = 0, in which case the initial 
probability distribution is P(0) = (1,0,..., 0). 
When the voted group is put into operation, the transition rate matrix A and the 
initial probability distribution P (0) can be used to determine the probability distribu-
tion P(t) = [P0(t), Pi(t),..., 
Pr{t)], where Pt(t) is the probability that the voted 
group is in state i at time t > 0. 
In some states, the voted group is able to perform its safety function, in other 
states it is not. As in Chapter 5, let V denote the set of states where the voted group 
is "down" and not able to perform its SIR These states may represent that the voted 
group has a dangerous fault or that it is under repair. The probability that the group 
is not able to perform its safety function at time t, the PFD(i) is given by 
PFD(r) = J > i ( 0 
ieV 
3This is not entirely true and some options for dynamic analysis exist, but these require deep knowledge 
of probability theory. 
4This is just a convention, and we may arrange the states in the sequence we wish and give each state the 
symbol we like, be it numbers, letters, or a combination of these. 

MARKOV APPROACH 
249 
The average PFD over a test interval (0, r) is 
U'I> 
PFDavg = - / 
)Pi(t)dt 
(8.67) 
'avg 
In the cases where it is relevant to assume a steady-state situation, we get 
PFDavg = Σ 
pi 
(8-68> 
ieV 
where F, is the steady-state probability in state /. We should note that the steady-
state assumption is often not relevant. 
The next section gives some examples of how to determine the PFDavg for the 
voted group by using the steady-state approach, whereas the time-dependent ap-
proach is studied in Section 8.7.3. 
► Remark: Because a low-demand SIF is tested and, if necessary, repaired at regular 
intervals of length r, the Markov property is not fulfilled and the process is, strictly 
speaking, not a Markov process. The process never approaches a steady-state, but 
fluctuates according to the test interval. The steady-state approach is therefore not 
valid, but we use this approach anyway, because we can cheat a bit and obtain ade-
quate results. 
Θ 
8.7.1 Steady-State Solution 
We illustrate the steady-state approach for some simple cases, for some of which 
numerical results are also presented. To compare the results, the data set in Table 7.2 
is used in all the cases. 
Single Channel with only DU failures. Consider a single channel with DU failure 
rate ADU and repair rate μου· We assume perfect proof-testing and disregard both 
DD and S-failures. Input parameter values are given in Table 7.2. The single channel 
can have two possible states: 
State 
State description 
0 
The channel is able to function (i.e., does not have a DU fault) 
1 
The channel has a DU fault 
A Markov model for the single channel is shown in Figure 8.19. The correspond-
ing transition rate matrix is 
A = ( ~ A D U 
A D U ) 
\ MDU 
—MDU/ 
As shown in Chapter 5, the steady-state probabilities P = (P0, P\) can be found 
from the matrix equation 
P A = 0 
(8.69) 

250 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
HDU 
Figure 8.19 
Markov model (state transition diagram) for a single channel with only DU 
failures. 
The channel is not able to perform its SIF (i.e., is "down") in state 1, such that 
V = {1}. Because Pi is the long-term or average probability that the channel is in 
state 1 (i.e., is "down"), PFDavg = Pi. 
Because the matrix A does not have full rank, we also have to use that Po + ^ι = 
1 to find a unique solution for P0 and Pi. The steady-state probabilities can be 
determined from the two equations 
—ADU^O + MDU^I = 0 
P0 + Pi = l 
Solving these equations, we get 
P0 = 
■ 
and Pi = - 
■ 
(8.70) 
Λ-DU + μου 
Λ-DU + MDU 
The mean downtime (MDT) of the channel is the mean time that elapses from when 
the DU failure occurs until the fault has been repaired and the channel is functioning 
again. This is equal to the sojourn time in state 1 in Figure 8.19. The MDT has 
two parts: (1) the unknown downtime from when the DU failure occurs until it is 
detected in the proof test, and (2) the mean repair time (MRT). In Section 8.3.5, we 
found that the mean unknown downtime for a single channel is r/2, such that MDT 
= τ/2+ MRT. The sojourn time in state 1 is generally not exponentially distributed, 
but we approximate it by an exponential distribution and use the "repair" rate μου 
given by 
_ 
1 
_ 
1 
μ°υ 
~ MDT ~ f + M R T 
Using this value, we get 
p 
A D U(f+MRT) 
1 
ADU (f + MRT) + 1 
Because λου τ is very small, λπυ(τ/2 + MRT) is also very small, and the denom-
inator is s» 1 (With the data in Table 7.2, the denominator is % 1.0044). Using this 
approximation gives 
PFDavg K ^ ^ 
+ ADUMRT 
(8.71) 
which is the same result that was obtained by the simplified formulas. The input 
data set in Table 7.2 gives PFDavg = 4.390 · 10-3, whereas the "exact" solution is 
i>! = 4.371 · 10"3. 

MARKOV APPROACH 
251 
ΘΟΘΟ© 
r-DD 
M-DU 
Figure 8.20 
Markov model (state transition diagram) for a single channel with DD and DU 
failures. 
A channel failure occurs when the channel is in state 0 and then fails, and the 
frequency of channel failures, « F , is from (5.78) 
wF = Ρ0λΌν 
= 9.96 · 1(Γ7 per hour 
(8.72) 
This is a more correct failure frequency than the ADU w e u s ed in the IEC formulas, 
but the difference is usually insignificant. 
Because DD failures are not considered in this example, the channel-equivalent 
mean downtime, ÎCE> is 
icE = - + MRT = 4390 hours 
The IEC formula gives therefore 
PFDavg = ωΡίοΕ = 4.371 · 1(Γ3 
which is the same result (Pi) we obtained with the Markov approach. 
For a single channel with only DU failures, we conclude that the Markov approach 
and the IEC formulas give the same PFDavg. 
Single Channel with DD and DU failures. Consider a single channel with DD 
failure rate ADD» DU failure rate ADU. repair rate from DD fault μπϋ and repair rate 
from DU fault μου· As above, μου = l/( r/2 + MRT). A DD fault is assumed 
to be revealed immediately and the mean time to restore the function is MTTR, such 
that μοϋ = 1/MTTR. Input parameter values are given in Table 7.2. The single 
channel can have three possible states. 
State 
State description 
0 
The channel is able to function (i.e., does not have a DU fault) 
1 
The channel has a DD fault 
2 
The channel has a DU fault 
A Markov model for the channel is shown in Figure 8.20. In this model it is 
tacitly assumed that a DU fault cannot develop into a DD fault. The corresponding 
transition rate matrix is 
(
—(ADD + ADU 
^DD 
ADU \ 
MDD 
-μΌΏ 
0 
MDU 
0 
- M D U / 

252 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
The steady-state probabilities Po, P\, and P2 can be determined from the three 
equations 
P0 + P1+P2 
= l 
ADD^O — MDD-PI = 0 
λουΛ) ~~ MDU-P2 = 0 
Solving these equations yields 
1 
Po = Λ-DD 
I ^ . D U I 1 
MDD 
A D U 
By introducing the expressions for μοϋ and μου> Po can be expressed as 
1 
Po = 
A D D M T T R + ADU (f + MRT) + 1 
When Po is known, it is straightforward to determine Pi and P2 and we get 
F1 = ^ P
0 
and P2 = ^ P
0 
MDD 
MDU 
The channel is functioning when it is in state 1 and has a dangerous fault when 
it is in state 1 or 2. The "up"-state is therefore U = {1} and the "down"-states are 
V = {1,2}. This means that the PFDavg for the channel is 
PFDavg = Pl + P2 
By introducing the expressions we have derived, we get 
ADDMTTR + ADU (§ + MRT) 
PFDavg 
A D D M T T R + ADU ( | + MRT) + 1 
Because ADD and ADU are very small, the denominator is very close to 1 (with the 
supplied data, the denominator is % 1.0044) and we may use the approximation 
pFDapprox % 
A
D
D
M
T
T
R 
+ ^ 
Π 
+ 
M R T ) 
The input data set in Table 7.2 yields 
- The Markov solution: Λ + P2 = 4.418 · 10"3 
- The approximated solution: PFD*Pprox = 4.438 · 10-3 
By multiplying the approximated PFD*Pprox by AD/AD (i.e., by 1), where AD = 
^DD + ADU is the rate of D failures, we can write 
ADU / T , „„-A , ADD, 
pFDapprox % 
^ 
(- + MKl) + - ^ M T T R 
V2 
/ 
An 
AD ^2 
= AD ÎCE 
(8.73) 

MARKOV APPROACH 
253 
which is recognized as the IEC formula for the PFDavg for a single channel. The IEC 
formula for a lool system has therefore been developed by the Markov approach. 
By comparing with Example 8.25, note that the effect of DD failures on the PFDavg 
is small (i.e., «s 1.08%) in this case. 
pFDapprox % 4 39 . JQ-3 + 4 g . JQ-5 = 4 43g . JQ-3 
(g j ^ 
" v& 
j 
~- 
j 
DU failures 
DD failures 
Single Channel with DD and DU failures: Alternative Approach. 
The Markov 
model in Figure 8.20 for a single channel with both DD and DU failures can be 
simplified slightly, and we may use only two states. 
State 
State description 
0 
The channel is able to function (i.e., does not have a D fault) 
1 
The channel has a D fault 
The corresponding Markov model is obtained from the Markov model in Figure 8.19 
by replacing DU by D. As is (8.70), the PFDavg can be written as 
PFDavg = P, = 
A D 
(8.75) 
When a D failure occurs, it is either a DD failure or a DU failure. The proba-
bility that it is a DD failure is ADDMD anc^ t n e probability that it is a DU failure is 
ADUMD- If the D failure is a DD failure, the mean time to restore the fault is MTTR, 
and if it is a DU failure, the mean time the channel is available is τ/2 + MRT. The 
mean downtime, MDT, associated with a D failure is therefore 
ΛΠΤΙ / 1 
\ 
ΛΓ)Π 
MDT = - p - ( - + MRT) + -f^MTTR 
An ^2 
/ 
AQ 
The repair rate from state 1 is therefore μο = 1/MDT. Using this μο in (8.75) 
gives the same PFDavg as obtained by the previous approach. 
1oo2 Voted Group of Independent and Identical Channels. 
Consider a loo2 
voted group of independent and identical channels. The channels can only have DU 
failures and the DU failure rate is ληυ· This means that ADD — 0. The other input 
parameter values are given in Table 7.2. The states of the voted group are: 
State 
State description 
0 
Both channels are able to function (i.e., do not have a DU fault) 
1 
One channel is able to function and the other has a DU fault 
2 
Both channels have DU faults 
The voted group is functioning as long as it is in state 0 or state 1 and fails when 
it is in state 2 such that V = {2}, and PFDavg = P2. DU failures are only detected 

254 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Figure 8.21 
Markov model (transition diagram) for a loo2 voted group of two independent 
and identical channels, where only DU failures are considered. 
in proof tests. If one channel is found to have a DU fault, the channel is repaired to 
a fully functioning state. The mean time from when a DU failure occurs until the 
channel is restored is τ/2 + MRTi and the associated repair rate is /ii,DU· If both 
channels are found to have DU faults, they are both repaired to a fully functioning 
state (state 0) and the associated repair rate is μ2,Όν- The mean repair time of two 
channels at the same time is MRT2. MRT2 may be equal to MRTi, but may also be 
different, depending on the repair resources and the situation that occurs when both 
channels are "down." A Markov model for the voted group is shown in Figure 8.21 
and the corresponding transition rate matrix is 
'—2ADU 
2ADU 
0 I 
A = 
MI,DU 
— (ADU + MI.DU) 
ADU 
V /^2,DU 
0 
— /i2,DU> 
Because the matrix A does not have full rank, it is sufficient to use two of the three 
equations and add that P0 + Pi + P2 = 1. We therefore replace the first column of 
A with (1,1,1)' and the steady-state probabilities can then be determined from the 
three equations 
P0 + Pi + P2 = l 
2λουΛ) — (ADU + MI,DU) Pi = 0 
ADU^I — M2,DU^2 = 0 
Solving these equations, we obtain 
p 
_ 
Z A D U 
"2 
2^DU ~*~ 3ADUM2,DU + Ml,DUM2,DU 
As the DU failure rate is small compared with the repair rates, P2 can be approxi-
mated by 
P2 s« 
DU 
(8.76) 
Ml,DUM2,DU 
When one DU fault is found in the proof test, the voted group is brought to state 0 by 
a repair action and the total mean downtime of the channel is MDT = τ/2 + MRTi 

MARKOV APPROACH 
255 
such that the associated repair rate is 
1 
Ml.DU 
I + MRTi 
r + 2MRTj 
In the same way, when the voted group enters state 2, and two DU faults are 
detected in the proof test, both channels are repaired. According to Example 8.7, the 
mean downtime of the voted group is MDT = τ/3 + MRT2 such that 
1 
3 
M2,DU = ■ 
3 
MRT2 
r + 3MRT2 
By inserting these repair rates into (8.75), we obtain the average PFDavg for the 
voted group (PFDavg = P2) as 
λ?>ΙΤ(τ + 2MRT!)(T + 3MRT2) 
PFDavg « - ^ 
^ 
— 
(8.77) 
Assume that MRTi = MRT2 = MRT. With the parameter values in Table 7.2, 
the "exact" value for PFDavg is 
PFDavg = 2.54 · 10"5 
and the approximated value is 
PFDavg = 2.57 · 10"5 
The IEC formula gives 
PFDavg = 2(ADU)2ÎCEÎGE = 2 (ADU)2 Ç- + MRT) ( I + M R T ) 
= 2.57 · 10"5 
The simplified formula gives 
PFDavg = ( A D" T ) 
+ ADUrMRT = 2.57 · 10-5 
All approaches are seen to give approximately the same result for these input 
parameters. 
1oo2 Voted Group with CCFs Consider a voted group of two identical channels 
that may experience common-cause failures (CCFs) and also may have DD failures 
with rate A DD, but safe failures cannot occur. CCFs are modeled by a beta-factor 
model where β applies for DU failures and βο applies for DD failures. Repair rates 
for DU failures are as above and the corresponding repair rates for DD failures are 
μι,ΠΌ and /X2,DD for one and two DD failures, respectively. The states of the voted 
group are: 

256 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Figure 8.22 
Markov model (transition diagram) for a loo2 voted group of two identical 
channels. 
State 
State description 
0 
Both channels are able to function (i.e., do not have any dangerous failure) 
1 
One channel has a DD fault and the other is able to function 
2 
One channel has a DU fault and the other is able to function 
3 
Both channels have DD faults 
4 
One channel has a DD fault and the other has a DU fault 
5 
Both channels have DU faults 
The voted group is functioning as long as it is in states 0, 1, or 2, and fails to 
function when it enters states 3, 4, or 5, such that V = {3,4,5}. 
DU failures are only detected in proof tests. The mean time to repair one DU fault 
is MRTi and the mean repair time for two DU faults at the same time is MRT2. DD 
faults are detected almost immediately. The mean restoration time (i.e., time from 
the DD failure occurs until it is restored to a functioning state) is MTTRi, and the 
mean restoration time for two DD faults is MTTR2. In state 4, the voted group has 
one DU fault and one DD fault. In almost all cases, the DD fault will be detected 
by diagnostics and the only exception is when the the DD failure occurs so close 
to the proof test that the proof test is initiated before the DD fault is revealed. The 
probability of this situation is, however, negligible. 
When a DD fault of one channel is revealed, we have two options: (i) the DD 
fault is repaired without checking the other channel, or (ii) the other channel is 
checked/tested and a possible DU fault is repaired. Here, the first option is assumed. 
A Markov model for the voted group is shown in Figure 8.22. The corresponding 

MARKOV APPROACH 
257 
transition rate matrix A is 
/ 
aoo 
2(1— (SDMDD 
2(1— β)λου 
fo^DD 
0 
βληυ' 
β\.ΏΏ 
- ( M I . D D + A D D) 
0 
ADD 
0 
0 
μ-i.DU 
0 
—(/ti.DU + ADD + ADU) 
0 
ADD 
A D U 
i*2,DD 
0 
0 
-M2,DD 
0 
0 
0 
0 
Ml.DD 
0 
-Ml.DD 
0 
\ M 2 , D U 
0 
0 
0 
0 
- M 2 . D U / 
where a00 = - [(2 - /ÖD)ADD + (2 - 
β)λΌυ]. 
The steady-state probabilities P = (Po, P\,..., 
Ρ5) can now be determined from 
the matrix equation P ■ A = 0 combined with the condition Σί =ο ^' = 1 · Generally, 
we have problems with hand-calculation when the size of A is greater than 4 x 4 , 
in which case we need to use a computer. If we need the analytic expression for 
P, MAPLE® is a suitable tool. If we only need a numerical solution, MATLAB®, 
or one of its free "clones," may be more suitable. There are also several dedicated 
Markov reliability programs on the market. 
When P has been determined, the average PFDavg for the voted group is given 
by 
PFDavg = P3 + P4 + P5 
(8.78) 
In many practical applications, it is realistic to assume that the mean repair time 
is independent of the number of channels that are repaired, because most of the time 
is consumed by securing the EUC and making the repair team available. 
The matrix equations are solved by MAPLE® but the expressions obtained for 
Po,Pi,---,Ps 
are complicated and not shown here. With the data set in Table 7.2, 
the numerical result is: 
P3 = 2.38 · 1(Γ6 
P4 = 3.74 · 10"7 
P5 = 3.13-10-4 
The PFDavg for the loo2 voted group with CCFs is therefore 
PFD^°o2) = P3 + PA + P5 = 3.16 · 10"4 
The main contribution (i.e., 99.1%) to the PFDavg is from state 5 with two DU faults. 
State 3 contributes 0.75% whereas state 4 contributes 0.12% for the data set in Ta-
ble 7.2. 
When the matrix equations have been entered into a computer program (such as 
MAPLE®), several other parameters can be easily determined. 
Markov Models with Nonconstant Failure Rates. 
The standard Markov approach 
is based on the assumption that all channels have constant failure rates, but is possible 
to model simple types of deterioration by introducing intermediate states. This is 
illustrated in Figure 8.23 where a degraded state is introduced for a single channel. 
The model has three states: 

258 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Figure 8.23 
Markov model with increasing failure rate. 
State 
State description 
0 
The channel is able to function (i.e., does not have a DU fault) 
1 
The channel is in a degraded mode, but the degradation cannot be 
revealed by proof-testing 
2 
The channel has a DU fault that can be detected by proof-testing 
When the channel is in state 1 (degraded) it is still functioning and no fault is 
revealed by proof-testing. The time Jbu from the channel is started up in state 0 
at time t = 0, until it reaches state 2 (DU fault) is the sum of 7o,i, the time to 
reach state 1, plus T\£, the time the process spends in state 1 before jumping to state 
2. Γου is the sum of two exponentially distributed variables and therefore has a 
Gamma-type distribution with increasing failure rate function (e.g., see Rausand & 
H0yland, 2004). More intermediate states may be included to obtain a specific form 
of the failure rate function, but this gives a more complicated Markov model. 
The model in Figure 8.23 may be realistic for a channel with increasing failure 
rate that is replaced or refurbished to an as-good-as-new condition when a DU fault 
is revealed (which is the case, for example, for subsea oil and gas equipment). 
8.7.2 
Demands with Prolonged Duration 
Assume that demands occur according to a homogeneous Poisson process with de-
mand rate Xae. Each time a demand occurs, it has a demand duration with mean 
value MDD. We assume that the demand duration is exponentially distributed and 
introduce the demand duration rate μ^ 
= 1/MDD. A hazardous event (HE) can 
occur in two ways: 
1. A demand occurs while the voted group has a dangerous fault (i.e., DU 
or DD fault). 
2. A dangerous (D) failure occurs while the demand is active (i.e., within 
the demand duration). 

MARKOV APPROACH 
2 5 9 
Figure 8.24 
Markov model for a single channel with prolonged demand duration. 
di EXAMPLE 8.26 Single channel with demand 
Consider a subsystem of a SIS with a single channel that can have the following 
states: 
State 
State description 
Fully functioning state 
The channel has a DU failure 
Demand state 
Hazardous state 
In this case, the demands do not come as shocks but have a prolonged dura-
tion. Demands are assumed to occur as a homogeneous Poisson process with 
rate Ade- The subsystem may fail when the demand is present, even if it was 
functioning when the demand occurred. The Markov model for the subsystem 
is shown in Figure 8.24, where the transition rates symbols are also indicated. 
We assume that the subsystem is not an ultimate safety barrier and that the sub-
system will be able to function again after a certain delay. The mean delay is 
here 1/μτ. 
The transition rate matrix for this subsystem is 
-(ADU + Ade) 
ADU 
MDU 
—(MOU + ^de) 
Mde 
0 
βτ 
0 
o \ 
Ade 
0 
Ade 
-(Mde + ADU) 
ADU 
0 
- M T / 
When the channel is in state 1, it cannot respond to a demand and therefore 
PFDavs = Px 
(8.79) 
The steady-state probabilities P = (Po, Pi, Pi, P3) can be solved from P ■ 
A = 0, but the mathematical expressions are extensive and are therefore not 
presented here. 

260 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
For high-reliability channels with short demand duration we have ADU <S 
MDU <3C Mde· In this case, we get approximately 
PFDl!aVg SS 
; 
- — 
; 
-
(Ade + MdeA^de + MDUj 
When λου <3C Mde> the following approximation is also adequate 
ADU 
PFD 2,avg 
A-de + MDU 
Numerical Example. 
Assume that ADU = 2 · 10-6 per hour, the proof test 
interval τ = 8 760 hours, and the mean repair time of a DU failure, MRT=8 
hours, such that μΌυ 
= 1/(τ/2 + MRT) = 2.28 · 10"4 per hour. Further, 
assume that Ade = 1 · 10-4 (i.e., low-demand mode), and that the mean demand 
duration is 2 hours. In this case, we get 
PFDavg = 4.650 ■ 10"3 
PFDi,aVg = 4.672 · 10"3 
PFD2,avg = 4.674 · 10"3 
The first value is the exact solution obtained by using MATLAB®. Both of the 
approximations are seen to be adequate in this case. It is seen that the restoration 
time after a hazardous situation has very little influence on the PFDavg. 
φ 
8.7.3 Time-Dependent Solution 
The average PFD in the proof test interval (0, τ) is, according to (8.66), 
PFDavg(0,r) = - f 
YPi(t)dt 
where V is the set of "down" states for the voted group and P, (f ) is the probability 
that the voted group is in state i at time t. 
The states that are not "down" states are "up" states, such that the total state space 
X — U U V. In many cases, it is easier to determine the probability that the voted 
group is "up" and functioning instead of "down" and we have 
£/>,(0 = i-I>(0 
i€V 
i&A 
The average PFD in the proof test interval (0, τ) can therefore also be expressed as 
PFDavg(0, T) = 1 - - f Σ 
P'(0 dt 

MARKOV APPROACH 
261 
Figure 8.25 
Markov model for the single channel in Example 8.27. 
where U is the set of "up" states for the voted group. 
To determine P, (/) for i e V is often a hard job to accomplish by hand calcu-
lation, even for voted groups with few channels. In practice, one therefore needs to 
have access to an efficient computer program, such as GRIF Workshop. 
The mean cumulated time (MCT) the system spends in state i in the proof test 
interval (0, r) is 
M C T ( 0 , T ) = [ 
Pj(t)dt 
Jo 
The average probability of failure on demand can therefore also be determined as 
PFDavg(0,τ) = ~Σ 
MCT'(°·τ) 
(8·8°) 
ieS D 
fl 
EXAMPLE 8.27 Single channel 
Consider a subsystem with a single channel that can have DD and DU failures, 
but where the possibility of S failures is disregarded. The channel has the fol-
lowing states: 
State 
State description 
0 
The channel is functioning (OK) 
1 
The channel has a DD fault 
2 
The channel has a DU fault 
We consider the subsystem in the proof test interval (0, τ). In this interval, 
DU failures are not detected. If a DU failure occurs, the channel will have this 
fault until the end of the interval. This means that state 2 is absorbing in the 
interval considered. A Markov model for the channel is shown in Figure 8.25. 
The associated transition rate matrix is 

262 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
The transition rate matrix for transient states is 
. 
_ / - ( A D D + ^DU) 
Λ-DD \ 
\ 
βΌΌ 
-MUD/ 
If we assume that the channel is as-good-as-new (i.e., state 0) at time t = 0, 
we can solve the state equation P(t)-At 
= P (t) to obtain the state probabilities 
P(t) = (Po(t), P\(t))· In this case, the equations are so simple that we can 
solve them by hand, but the solution procedure does not give much additional 
knowledge, and the solution is lengthy and it is not shown. Another option is to 
use Laplace transforms, which give the following matrix equation: 
(w. w ) (~(ADD + ADU) 
ADD ) = K'w - ww) 
\ 
MDD 
—MDD/ 
From this matrix equation, we can find P0* (s) and P*(s) and thereafter P0(t) 
and P\{t) from the inverse Laplace transforms. This can be done, for example, 
by using the Inttrans package in MAPLE® or a dedicated Markov program. 
It is straightforward to determine Pç>(t) and P\{t), but the solution is too 
lengthy to present here. 
Average Probability of Failure on Demand. 
The subsystem is functioning in 
state 0 and has a dangerous fault in states 1 and 2. The probability of being 
"down" at time / is therefore [1 — Po(t)] and the instantaneous PFD at time t is 
therefore 
PFD(f) = 1 - Po(f) 
The PFDavg over the proof test interval (0, τ) is 
1 Γ 
PFDavg = 1 - - / 
P0(t)dt 
τ Jo 
Survivor Function. If we are interested in determining the survivor function 
for the channel, the Laplace transform is 
R*{s) = P0*(s) + P1*(s) 
and R(t) can be found by taking the inverse Laplace transform. These solution 
is not given here, but can easily be obtained by using MAPLE®. 
Mean Time to First DU Failure. The mean time to first DU failure is obtained 
by first setting s = 0 and then solve the equation. 
(ρ0*(0),Λ*(0))(-(λοο + λου) 
A D D W I , O ) 
\ 
MDD 
—MDD/ 

MARKOV APPROACH 
263 
We solve and get 
MTTF = Ä*(0) = Po*(0) + />,*(<)) = ^— + 
^ — 
(8.81) 
Λ-DU 
MOD · ADU 
From this result, we note that when the mean restoration time of a DD failure, 
MTTRDD> is very short, then the repair rate μοϋ = 1/MTTR DD is high. In this 
case, we have that MTTF % 1/ADU> the mean time to a DU failure, which is an 
obvious result. 
The result above can be illustrated by a simple numerical example. Let 
ADD = 8.0 · 10~6 per hour, ADU = 2.0 · 10~6 per hour (which means the 
the diagnostic coverage, DC, is 80%). Further, let MTTRDD = 10 hours, such 
that μΏΌ = 1/10 per hour. The MTTF in (8.80) is then 
MTTF = 
+ 
—^— = 5.0 · 105 + 40 = 5.0004 · 105 hours 
Λ-DU 
MDD · Λ-DU 
In this case, we clearly have that MTTF «s 1/λου· 
Θ 
8.7.4 
Demands with Prolonged Duration 
Reconsider the subsystem of a SIS with a single channel that was studied in Exam-
ple 8.27. In this case, we assume that the subsystem is part of an ultimate safety 
barrier (SIF) and that state 3 therefore is an absorbing state. The same states as in 
Example 8.27 can be used: 
State 
State description 
0 
Fully functioning state 
1 
The channel has a DU failure 
2 
Demand state 
3 
Hazardous state 
In this case, the demands do not come as shocks but have a prolonged duration. 
This means that the subsystem may fail when the demand is present, even if it was 
functioning when the demand occurred. The Markov model for the subsystem is 
shown in Figure 8.26, where the transition rates symbols are also indicated. 
We consider the subsystem in the proof test interval (0, τ). In this Markov model, 
state 3 is an absorbing state, and the transition rate matrix for the transient states is 
0 
-Ade 
0 
ßde 
0 
— (^de + Aou)y 

264 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Figure 8.26 
Markov model for a single channel with prolonged demand duration. 
When the subsystem is in state 1, it cannot respond to demands and the instan-
taneous PFD(i) for the subsystem is therefore PFD(i) = P\{t). The average PFD 
is 
PFDavg = - f 
PiiOdt 
(8.82) 
τ Jo 
Hazardous Event Frequency. What may be more interesting here is the frequency 
of hazardous events (HEF), which is the frequency of transitions into state 3. To 
come to state 3, the subsystem must be in either state 1 or state 2 and then make a 
"jump." The HEF at time t is therefore 
HEF(0 = P, (t) · Ade + P2(t) · ADU 
The average HEF in the proof test interval (0, r) is 
HEF = - / HEF(r) dt = - I (7^(0 · Ade + P 2(0 · ADu) dt 
(8.83) 
τ Jo 
τ J0 
With the notation we introduced in (8.76), the average HEF can also be written as 
H E F = ^ Γ Pl(t)dt + ^ 
Γ P2(t)dt 
τ Jo 
τ Jo 
M C T K O . T ) 
M C T 2 ( 0 , T ) 
= Ade · 
h A D U · 
(Ö.84) 
τ 
r 
Because MCT, (0, r) is the cumulated time the subsystem spends in state i in the 
proof test interval (0, r), the fraction MCT;(0, r)/r is the average probability of 
finding the subsystem in state i in the proof test interval. 

PETRI NET APPROACH 
265 
Figure 8.27 
Petri net model for proof-testing a channel. 
8.8 
Petri Net Approach 
The Petri net approach was introduced in Chapter 5. This section gives some exam-
ples of how Petri nets can be used to establish system reliability models for a SIR 
Quantitative analyses of Petri nets must usually to be carried out by Monte Carlo 
simulation, for which you need access to a dedicated computer program, such as 
GRIF Workshop.5 In this section, we suffice by discussing modeling issues and do 
not go into quantification of PFDavg. 
The Petri net approach is an efficient modeling tool that can solve several of the 
problems encountered in the Markov approach. The Petri net approach is more gen-
eral and creates more compact models. Everything that can be modeled in a Markov 
model can also be modeled in a Petri net. 
Reliability assessments are generally carried out to provide input to important de-
cisions made by people who are not experts in reliability analysis. An important 
feature of all the approaches we have discussed is how easy it is to explain the ap-
proach to non-experts. In this respect, the Petri net approach is not optimal because it 
requires a certain level of knowledge and experience to develop and fully understand 
a Petri net model. 
8.8.1 
Proof-Testing 
Periodic proof-testing cannot be adequately modeled by the Markov approach, but 
this can be accomplished by the Petri net approach. 
Modeling a Proof Test. Consider a channel (such as a valve) that is proof-tested. 
If the channel is in good condition when the test is initiated, the work is finished after 
the proof test. If the channel has a DU fault, the fault is reveled and then repaired. 
A simple Petri net model for the proof-testing process is shown in Figure 8.27. 
A token in p\ means that the channel has a DU fault, and a token in p2 means that 
the proof test is ongoing. If there is a token in p2 but no token in p\, transition t2 is 
enabled and fired, which means that the proof test is terminated without any repair. 
If, on the other hand, there is a token in p\, the inhibitor arc from p\ tot2 prevents t2 
This section is coauthored by Associate Professor Yiliu Liu, NTNU. 
See h t t p : / / g r i f - w o r k s h o p . c o m 

266 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
(b) 
Figure 8.28 
Petri net model for (a) proof-testing a channel for DU faults, (b) is a simplified 
Markov model for a proof-tested channel, whereas (c) shows the corresponding simplified 
Petri net model. 
from being fired. Transition t\ is therefore enabled and fired, meaning that a repair 
action is started. 
Modeling a Single Channel with Proof-Testing. A Petri net model for a channel 
that is proof-tested involves three parts, and these are illustrated in Figure 8.28(a). 
1. A part describing the failure of the channel. In Figure 8.28(a), this part involves 
the nodes p-w, ?F> and /?p and is used to model the dynamics of the channel. 
2. A part describing the proof test. This part involves ρς,ίτ, and ρτ and reflects 
the process of standby and proof-testing. 
3. A part describing the repair action(s). This part involves ίχ with a deterministic 
firing time. If the channel is proof-tested, for example, once per year, the firing 
time of ίχ is 8 760 hours. 
When a token is in ρτ, a proof test is performed. The firing logic is similar to the 
one in Figure 8.27: fire t\ if the channel has a DU fault, or fire ti if no DU fault is 

PETRI NET APPROACH 
267 
found. When either t\ or i2 is fired, a directed arc deposits a token to ps, to start the 
next proof test cycle. When t\ is fired, a token is also deposited to /?R, which is for 
repair. The repair time is assumed to be a random variable. After ÎR is fired, a token 
is deposited in p-w, denoting that the channel is restored and start functioning again. 
It is also possible to establish a Markov model with three states (working, DU 
fault, and repair) for the proof test, as shown in 8.28(b). The corresponding simpli-
fied Petri net model is shown in 8.28(c). It should be noted that these two models 
are established on the basis of identification of all possible states of the channel, and 
the proof test is not modeled directly. The reveal of a DU fault is approximated by 
a stochastic process with a constant rate of 2/r, which is also the firing rate of tran-
sition iT in 8.28(c). As the occurrence of the proof test is modeled by a transition 
rate, it is difficult to include conditions in the testing, such as unavailability of test-
ing resources due to the work on other channels. In addition, the Markov model also 
disregards the situation where a proof test does not reveal any fault. 
8.8.2 
Simultaneous and Sequential Proof-Testing 
Proof tests and repairs can be included in the models as separate parts that are in-
teracting with other parts. This is necessary when different proof-testing and repair 
strategies influence the performance of the system. Figure 8.29 describes two proof-
testing strategies for a loo2 voted group: simultaneous and sequential proof-testing. 
These two models can be regarded as extensions of the model in Figure 5.37. With 
simultaneous proof-testing, the test team tests the two channels at the same time at 
regular intervals, which means that the proof-testing resource must be divided into 
two parts such that the same proof-testing resource can be allocated to both channels. 
For this reason, the model in Figure 8.29(a) has an almost symmetrical structure. 
When the proof test begins, the transition ίχ is fired, and the places ριχ and ρ2τ 
each receives a token, such that the proof tests of channels 1 and 2 are carried out at 
the same time. Thereafter, only one token from the part of channel 2 returns to p$ 
for bounding the Petri net. 
With sequential proof-testing, it is assumed that channel 1 is proof-tested before 
channel 2. In this way, completion of the proof-testing of channel 1 initiates the 
proof-testing on channel 2. Therefore, when modeling (b), a token can be released 
to p2T after one of the transitions fix and t'1T is fired. 
In addition, two inhibitors from ριτ can be used to block transitions Î2X and ί'2Ύ 
until the token in the place is absorbed by ίχχ or t'1T. This modeling approach can be 
applied to more complex architectures, with three or more channels. Inhibitors from 
the place denoting the previous channel being proof-tested ensure that the proof test 
on the next channel cannot be conducted until the necessary work is finished. 
When CCFs are taken into consideration in the modeling of sequential tests, as 
shown in 8.30, a transition ÎCCF can be involved to absorb tokens both in piw and 
P2w at the same time when a CCF occurs. When a token falls into PCCF, it can be 
removed by ÎITC °r ?2TC when a token is in pn or ρ2τ, meaning that proof tests 
on any of the two channels can reveal the CCF. In this model, it is assumed that the 
restoration of CCFs is same as for independent failures. The two bi-directional arcs 

2 6 8 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Figure 8.29 
Petri net models for two proof test strategies for a loo2 voted group; (a) 
simultaneous proof-testing, and (b) sequential proof-testing. 
Figure 8.30 
Petri net model for sequential proof-testing of a loo2 voted group with CCFs. 
between r,xc and pn ensure that the tests for CCFs have no impact on the balancing 
of tokens in the model. 
For koon voted groups with CCFs, a transition can be introduced to remove all 
tokens in working places, and its firing time depends on the occurrence rate of the 
CCF. Such a transition is often followed by a place to express the CCF fault with 
holding a token. If the group needs a special repair to be restored from unavailability 
by the CCF fault, the output transitions enabled together with proof tests release 
tokens to the same repair place. After firing one transition for repair, tokens can be 
released to all working places. Otherwise, as shown in 8.30, CCF can be handled in 
me same way as independent faults, such mat the transitions representing proof tests 
deposit tokens to the places for independent repairs. 
► Remark: In Markov modeling, we need to list all possible states, and the igno-
rance of a state results in a wrong representation of the system. With the Petri net 

PETRI NET APPROACH 
2 6 9 
Figure 8.31 
Petri net model for staggered proof-testing of a loo2 voted group. 
approach, it is not necessary to identify all system states before the model is built. 
States of the system can be revealed in the process of running the model. Although 
some Petri net models for large systems look complex, they generally contain more 
information than corresponding Markov models. 
A Petri nets are useful for describing and analyzing systems with concurrency, 
parallelism, synchronization and resource sharing—properties that are difficult to 
deal with in Markov models. 
φ 
8.8.3 
Staggered Proof-Testing 
Consider a loo2 voted group, where the proof tests are carried out as staggered 
testing. Channel 1 is proof-tested at times 0, τ, 2τ,..., and channel 2 is proof-tested 
at times to, x + to, 2x + to, ■ ■., such that the delay is to- Further, assume that DU 
failures may occur in both channels with constant failure rates, and if a DU fault is 
revealed during a proof test, a repair action is initiated immediately. 
In Figure 8.31, the place psT2 is used for holding a token to set off the proof test 
of channel 2, and the token is released after ?ST2 is fired, which is enabled when the 
proof test and the repair of channel 1 is finished. The firing times of ÎSTI and /ST2 
depend on the proof test intervals. 
Two places, psw and psr, are used to model the state of the voted group. The 
probability of a token in psw (in a long term) denotes the average availability of the 
voted group. 
8.8.4 
Petri Net Modeling for Low-Demand Mode 
Figure 8.32 shows a Petri net model that is an extension of the model in Figure 5.37 
by including the effects of demands. The model has three new places: p^o, PDE, 
and PA- A token in /?w is used to indicate that the SIF is available, and that the plant 
is protected by the SIF. Similarly, a token in /?NO means that the SIF is not available, 
and the plant is not protected by the SIF. 

270 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
Figure 8.32 
Petri net model for a loo2 voted group with demands. 
The presence of a token in /?NO means that no demand is generated, whereas the 
occurrence of a demand is modeled by firing /DE and releasing a token into pr>E-
In this model, we assume that demands occur according to a homogeneous Poisson 
process. If the SIF is not available when a demand occurs, an accident happens in 
the plant (or the risk is increased) resulting in firing ?A and the deposit of a token in 
PA- Otherwise, if the SIF is available, ?N is fired and returns a token to p^o. 
It is assumed that the plant can be restored from the accident to normal operation, 
and that the restoration time (modeled by ÎRA) is exponentially distributed. 
In the low-demand mode, PFDavg is obtained by calculating the sojourn proba-
bility of a token in p-p. 
8.8.5 
DD Failures with Follow-up Proof-Testing 
If a DD failure occurs in a SIS, the maintenance team will repair the DD fault and 
may opt for a proof test for DU faults in conjunction with the repair action. The 
decision should be made based on the operating conditions and the available mainte-
nance resources. If such an "insert proof test" is conducted, it is necessary to decide 
whether the original proof test schedule needs to be adjusted, that is, whether to 
postpone the next proof test or perform it at the planned date. 
As a result, there are three possible strategies to be adopted when a DD failure 
occurs: 
1. Do not perform proof test for a DU fault. 
2. Perform proof test for a DU fault, and keep the proof test schedule unchanged. 
3. Perform proof test for the DU failure, and change the proof test schedule (mostly 
postpone the subsequent proof test). 
For modeling strategy 1, DD failures and associated repairs should be separate 
from DU failures and proof tests; for strategy 2, DD failures activate proof tests, but 

PETRI NET APPROACH 
271 
A>w 
'DD 
PDF 
PDVI 
Î D D 
PDF 
ΡΊ / / 
t[ 
Pm 
fjuR 
P\J?I / 
t\ 
PUR 
'UR 
Figure 8.33 
Petri net models for three DD failure follow-up proof test strategies [(a) strategy 
1, (b) strategy 2, and (c) strategy 3], 
they do not affect the "normal" proof test model; whereas for strategy 3, DD failures 
reinitialize the behavior of the maintenance team in preparing the proof tests. 
Figure 8.33 shows PN models for the three follow-up strategies after a DD failure 
is revealed. In all three models, the removal of the token in /?DW by the transition 

272 
AVERAGE PROBABILITY OF FAILURE ON DEMAND 
/DD and the deposition of a token to /?DF designate a DD failure. Meanwhile, the 
occurrence of a DU failure is described with />uw> ?DU and p\jF- The proof test is 
modeled by firing ίχ and depositing a token to p^. If a DU fault is revealed in the 
proof test, t\ is fired; otherwise, t-i is fired. Then, after repairing the DU fault, ?UR 
is fired, and the maintenance team returns to the standby state preparing for the next 
proof test. 
A place ρ'τ is added in Figure 8.33(b) and (c) as an output place of ?DR> which 
is a transition for restoring the system from a DD fault. A token in ρ'Ύ means that a 
proof test is ongoing triggered by the DD failure. As a result, the presence of a token 
in /?UF is checked similarly with the "normal" proof test. If it is present, a token is 
released to ρ'υκ and the system is restored by firing t{jR. 
The difference between (c) and (b) lies in the removal of the token in ps when 
releasing a token in p'T. In other words, the proof-testing resource is diverted when 
performing the triggered proof test in model (c). The enabling condition of /χ is 
stopped. Therefore, the subsequent proof test is performed after the firing time of /χ, 
which is enabled after the insert proof test and repair are finished. 
Liu (2013) discusses the models of follow-up proof-testing options for loo2 voted 
groups, and such models are helpful in order to evaluate the effect of different proof-
testing and maintenance strategies on the SIF reliability. 
8.9 Additional Reading 
The following titles are recommended for further study related to Chapter 8: 
- Safety Instrumented Functions (SIF) — Safety Integrity Level (SIL) Evaluation 
Techniques. Part 4: Determining the SIL of a SIF via Markov Analysis (ISA-
TR84.00.02-4, 2002) 
- Petroleum, Petrochemical and Natural Gas Industries - Reliability Modeling 
and Calculation of Safety Systems (ISO/DTR 12489, 2012). Draft to technical 
report. 
- Contribution to modelling safety instrumented systems and to assessing their 
performance. Critical analysis of the IEC 61508 standard (Innal, 2008). This 
PhD thesis covers many of the issues that are discussed in this chapter. Several 
problematic issues are treated better and in more detail in this thesis. 

CHAPTER 9 
AVERAGE FREQUENCY OF 
DANGEROUS FAILURES 
9.1 
Introduction 
The aim of this chapter is to determine the average frequency of dangerous failures 
(PFH) of a SIF. The PFH was introduced briefly in Section 7.5.2 and is the required 
reliability measure for a SIF operating in high-demand and continuous mode in both 
IEC 61508 and IEC 62061. 
Because PFH is a frequency of failures, this chapter begins with an introduction 
to the frequency concept. The reader should notice that the terms frequency and rate 
are used with the same meaning in this chapter. The presentation in this chapter 
focuses on PFH approximation formulas and two different approaches are presented. 
First, a set of simplified PFH formulas that are in line with the simplified PFDavg 
formulas in Chapter 8 are presented. Thereafter, the PFH formulas in IEC 61508-6 
are presented and slightly extended. Determination of the PFH by fault tree analysis, 
Markov analysis, and Petri net analysis is also discussed briefly. 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
273 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

2 7 4 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
t+Ät 
Time 
Figure 9.1 
The time axis. 
9.2 
Frequency of Failures 
This section introduces a set of formulas for the frequency of failures of an item 
that is periodically proof-tested. The item may be a single channel, a voted group, 
a subsystem, or a complete safety loop. The average rates are determined for a time 
interval (0, τ). This interval is referred to as the proof test interval, but can be any 
time interval, not necessarily a test interval. 
High-demand SISs are sometimes seldom proof-tested, if tested at all. If the SIS 
is proof-tested, the time interval (0, τ) is the proof test interval. If the SIS is not 
proof-tested, the interval (0, τ) is then chosen to be either the estimated lifetime of 
the SIS or the mission time of the EUC. 
9.2.1 
Nonrepairable Items 
Probability Density Function. The distribution of the time-to-failure, T, of an 
item that is not repaired can be given by its probability density function f(t) as 
Pr(i < T < t + At) ss f(t)At 
when Δί is small 
This can also be written as 
Pr(Failure in (t, t + At)) «a f(t)At 
when Δί is small 
(9.1) 
The probability density function can be explained from Figure 9.1. If we are 
"standing" at time t — 0 with a fully functioning item and look forward in time, 
fit) At is the (unconditional) probability that the item will fail (for the first time) in 
the interval (t,t + At). 
Failure Rate Function. The failure rate function z (t) of an item is given by 
Pr(f < Γ < ί + Δ ί | Γ > ί ) ^ z(t)At 
when At is small 
This can be rewritten as 
Pr(Failure in (t, t + At) \ T > t) ss z(t)At 
when At is small 
(9.2) 
Here, z(t)At is the conditional probability of failure in (t, t + At], given that the 
item has survived up to time t, the starting point of the interval (see Figure 9.1). 
The Average Failure Rate. 
Generally, an item has a nonconstant failure rate func-
tion z(t), but we will here approximate this failure rate function by an average failure 
rate. The average value of z(t) in the proof test interval (0, τ) is 
-'-{'■■ 
τ Jo 
z(0, τ) = - / 
z{t)dt 

FREQUENCY OF FAILURES 
275 
The survivor function for the item is given as (see Appendix A): 
R(t) = Pr(T > t) = e~& z(u)du 
= e'1^' 
(9.3) 
The probability that the item will fail in a test interval (0, r) is given by the prob-
ability distribution function 
F(T) = 1 - R(T) = 1 - β-^°'τ)τ 
% z(0, T)T 
(9.4) 
The average failure rate can therefore be approximated by 
F(T) 
z(0, r) « - ^ 
(9.5) 
T 
The approximation (9.5) is acceptable only for rather short intervals. For large 
values of r, the average failure rate z(0, τ), calculated by (9.5), approaches zero, 
which is not realistic. The formulas are illustrated in Example 9.1. 
fl 
EXAMPLE 9.1 Weibull distribution 
Consider an item for which the time-to-failure T is Weibull distributed (see 
Appendix A) with shape parameter a = 2. The failure rate function for this 
item is 
z(0=aAV - 1 =2λ2ί 
This failure rate function is a linear function of /, such that an item that has 
survived up to time It has twice as high probability of failing in the next interval 
of length At, compared to an item that has survived up to time t. 
The average failure rate over (0, r) is 
1 
ίτ 
ζ(Ο,τ) = - / z{t)dt =λ2τ 
(9.6) 
τ Jo 
From Appendix A, the distribution function of the item is 
F(t) = l-e-^')a 
= 
l-e-^t)2 
For t = τ, we therefore have that 
F(r) = 1 - e - ( / U ) 2 = 1 - 
e-si0'x)x 
The mean time-to-failure, MTTF, of the item is from Appendix A 
MTTF^Ir(i + l) 

2 7 6 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
N(t)A 
'e 
(7 
's la 
Ίο 
Time 
Figure 9.2 
The number of item failures N(t) plotted against the time t (the figure is not 
based on real data). 
Numerical Example. 
Let λ = 4.0 · 10-6 per hour and the proof test interval 
τ = 8 760 hours (i.e., one year). The probability that the item will fail in the 
proof test interval (0, r) is given by the distribution function at time t = r, 
F(x) RS 1.23 · 10~3. This means that the item will fail in one out of 815 proof 
test intervals. The MTTF of the item is Γ(1.50)/λ = 2.216 · 105 hours, which 
is approximately 25.3 years. 
Approximation (9.4) is here very good and the difference between the ex-
act value and the approximated value is only 7.53 · 10-7, which is approxi-
mately 0.06% of the exact value. The average failure rate in (0, r) is from (9.6), 
z(0, τ) % 8.76 · 10-7 per hour. Using (9.4) yields z(0, r) % 8.73 · 10-7 per 
hour. 
φ 
9.2.2 
Repairable Items 
Consider a repairable item that fails and is repaired many times and let N(t) denote 
the number of item failures in the time interval (0, t). Further, let E(N(t)) denote 
the expected (mean) number of item failures in the same interval (e.g., see Rausand 
& H0yland, 2004, chapt. 7). 
ROCOF. In Figure 9.2, the number of failures N(t) is plotted against the time t. 
The first failure occurs at time t\ such that N(t\) = 1. The second failure occurs 
at time t^ such that Nfa) 
= 2, and so on. The curve illustrates the mean value 
E(N(t)) as a function of f. In Figure 9.2, the frequency of failures (i.e., the number 
of failures per time unit) increases with the time t. The curve illustrating the mean 
number of failures, E(N(t)), is therefore convex. To better understand this behavior, 

FREQUENCY OF FAILURES 
277 
it is of interest to study the slope (i.e., the derivative) of the curve. 
w(t) = j-tE[N(t)} 
(9.7) 
By using the definition of the derivative, 
E[N(t + At) - N(t)] 
w(t) = lim 
Δί^Ο 
At 
When Δί is small, 
E[N(t + At) - N(t)] 
w(f) * 
Mean no. of failures in (t, t + At) 
= 
Δί 
which means that w(t) denotes the frequency of item failures in the time interval 
(t,t + At). 
All practical SIFs will fail at most once in a short interval of length Δί, which 
means that N(t + At) — N(t) is either 0 or 1. From the definition of the mean value, 
we must therefore have 
Pr[Failure in (t, t + At)] 
W(t) ~ 
► Remark: It should be noticed that 
- When the E(N(t)) curve is linear, the slope w(t) is constant and the frequency 
of failures is the same for all /. 
- When the E(N(t)) curve is convex (as in Figure 9.2), the slope w(t) increases 
with time and failures tend to occur more and more frequently. 
- When the E(N(t)) curve is concave, the slope w(t) decreases with time and 
failures tend to occur less and less frequently 
Θ 
In textbooks on system reliability theory, the frequency of failures w(t) is often 
called the rate of occurrence of failures, ROCOF, at time t, and can be expressed as 
Pr [Failure in (t, t + At)] % w(t)At 
(9.8) 
Observe that w(t) is an unconditional failure frequency (or rate) for the item. 
When the item is nonrepairable, w(t) = f(t). 

2 7 8 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
fl 
EXAMPLE 9.2 Item with constant failure rate A 
Consider the same situation as above and assume that the item has constant 
failure rate A. We do not distinguish between different failure modes. From 
Appendix A, we know that the failure process is a homogeneous Poisson process 
and that the distribution of N(t) is 
(Xt)n 
Pr(W(/) = n) = ^ - - e~Xt 
for n = 0,1,... 
(9.9) 
n! 
The mean number of failures in the interval (0, t) is 
oo 
E[N(t )] = J2 n Pr(N(t)) = n) = Xt 
«=o 
The ROCOF is in this case 
w(f) = wm=x 
(9,o) 
The ROCOF has the same value as the failure rate, but the two concepts have 
different interpretations. 
φ 
Average ROCOF. From (9.7), note that the average number of item failures in a 
test interval (0, τ) can be expressed as 
Ε[Ν(τ)] = f w{t)dt 
(9.11) 
Jo 
The average ROCOF in a test interval (0, r) is 
1 Γ 
W(0,T) = - / 
w(t)dt 
(9.12) 
τ Jo 
By using (9.11), the average ROCOF can also be expressed as 
* ( 0 . τ ) = * Μ ΐ > ϊ 
(9.13) 
r 
The expected number of failures in (0, τ) is 
Ε[Ν(τ)] = 0 · Ρι(Ν(τ) = 0) + 1 · Pr(JV(r) = 1) + 2 · Ρτ(Ν(τ) = 2) + · · · 
When the item has a high reliability, we should expect no more than one failure in 
(0, τ), and we therefore assume that Pr(./V = k) = 0 for k = 2,3,..., such that 
Ε[Ν(τ)] % Pr(JV(r) = 1) = F(T) 

FREQUENCY OF FAILURES 
2 7 9 
The average ROCOF in (0, r) is 
*((U)*ffiM^ 
(9.14) 
r 
τ 
Because F(r) = f0
T /(/) dt, we may also write 
«5(0, τ) % ^ 
= - f f(t) dt = /(O, T) 
(9.15) 
τ 
τ Jo 
where /(0, τ) is the average probability density over the interval (0, τ). 
Note that the approximations are adequate only when the time interval (0, r) is 
rather short. When r gets large, both the approximated values will tend to zero, 
which is not at all a realistic result. 
Vesely Failure Rate. 
Vesely ( 1970) suggests a "failure rate function" for repairable 
items that has later been called the Vesely failure rate, Xv(t), and is denned as 
Pr(Failure in (r, t + At) \ X(t) = 1) « λ ν(/)Δ/ 
when Δ/ is small 
(9.16) 
The Vesely failure rate can be explained with reference to Figure 9.1: we start with a 
functioning item at time t = 0, such that X(Q) — 1, where X(t) is the state variable 
for the item at time t. In this case, λ ν(ί)Δί is the conditional probability of failure 
in (i, t + At], given that the item is functioning at time t. In this expression, we do 
not say anything about the history of the item up to time /, such as how many times 
the item has failed and been repaired. We note that when the item is nonrepairable, 
thenAv(i) = z(t). 
Because X(t) = 1 is a prerequisite for having a failure in (t,t + At], we have 
that 
x, , . 
, 
, , 
, , 
, 
PriFailurein (t,t + At)] 
Pr(Failure in (t,t + At) \ X(t) = 1) = - L 
K 
" 
Pv(X(t) = 1) 
w(t) 
A(t) 
(9.17) 
where A(t) = Pr(X(t) = 1) denotes the availability of the item. The unconditional 
rate of failures (ROCOF) of the item can therefore be written 
w(t) = A{t)Xv(t) 
(9.18) 
β 
EXAMPLE 9.3 Items with constant failure rate 
When an item has constant failure rate X it is as-good-as-new as long as it is 
functioning and the history of the item is of no relevance. If the item is function-
ing at time t, it does not matter what happened before time t. This means that 

2 8 0 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
the statements T > t and X{t) = 1 say the same thing and the Vesely failure 
rate is therefore equal to the standard failure rate, such that λ ν(ί) = λ, and 
wit) = Ait)X 
(9.19) 
This result is used in the Markov approach to determine the frequency of failures 
and the PFH. 
φ 
9.3 Average Frequency of Dangerous Failures (PFH) 
9.3.1 
Definition and interpretation of PFH 
The frequency of dangerous failures (PFH) is the same concept as the ROCOF, which 
was introduced in Section 9.2.2. As the ROCOF, the PFH can be given as a time-
dependent frequency, PFH(i), or as an average value, PFH(ii, t2), in a time interval 
ihJi). 
The PFH at time t can, therefore, be denned as 
PriDangerous failure in Ü, t + At)] 
PFH(f) = wD(t) = lim — 
— 
(9.20) 
Δί->0 
Δ ί 
where IÜD(0 is the ROCOF at time t with respect to dangerous failures. 
IEC61508 requires the average PFH be used as reliability measure for high-
demand and continuous demanded SIFs, but does not specify for which time interval 
the average should be determined. 
The average PFH in the time interval (?i, t2) is 
1 
/": 
PÎ2 
PFHiti,t2) = wu(ti,t2) = — : — / 
wDiu)du 
(9.21) 
h 
Let M}(0 denote the number of dangerous failures in the time interval (0, /). By 
using (9.13), the average PFH in (ii, Î2) c a n De expressed as 
rmitl,t2)=E[NM]-E[N^)] 
(9.22) 
h — h 
and is the mean number of dangerous failures in the interval (fi, t2) per time unit. 
The PFH in the interval (/1, t2) is illustrated in Figure 9.3. 
Before proceeding, it is necessary to clarify some issues: 
The Meaning of Dangerous. 
PFH is defined as the frequency of dangerous failures 
per hour. The term dangerous can be interpreted in at least two different ways. 
1. A dangerous failure is a failure that terminates the ability of the SIS to carry out 
its safety function according to the performance requirements. This means that 
all failures to perform a SIF are dangerous failures. 

AVERAGE FREQUENCY OF DANGEROUS FAILURES (PFH) 
281 
wD(t)A 
Figure 9.3 
The relationship between the ROCOF and the PFH in the interval (ίι, t2). 
2. A dangerous failure is a SIF failure that can lead to a hazardous event in the 
EUC that is protected by the SIF, if a demand for the SIF should occur. 
Both interpretations can be defended, but IEC61508 has adopted the second option 
when determining the PFH. This means that if the SIF failure is detected immedi-
ately and the EUC is brought to a safe state (almost) immediately, the failure is not 
dangerous for the EUC. 
If a dangerous SIF failure is detected (as independent DD failures or as a DD-
CCF), IEC 61508 requires the EUC be immediately brought to a safe state. This 
principle can be reformulated as: 
The EUC is allowed to operate when one or more detected faults lead to degraded 
functioning of the SIF, but if a fault of the SIF (i.e., the SIF is not able to perform 
its safety function) is detected, the EUC must be brought to a safe state as soon as 
possible. 
Consider a SIS subsystem with independent channels and assume that the SIF is 
operated in a critically degraded mode such that the SIF will fail if a single extra dan-
gerous channel failure occurs. There are two different types of dangerous subsystem 
failures: 
(a) The final failure is a DU failure. This causes an undetected dangerous failure 
according to option 2 and the SIF downtime will, in most cases, last until the 
next proof test. 
(b) The final failure is a DD failure. This is a detected failure and an online repair 
action is started immediately. The mean restoration time, MTTR, is typically a 
few hours. When the restoration is finished, the SIF is able to perform its safety 
function again, although in degraded mode. 
There is obviously a big difference between these two types. For type (a), the SIF 
is unavailable for a relatively long time. The mean SIF downtime depends on the 
architecture of the subsystem that failed and the length of the proof test interval. The 
downtime will, in many cases, be in the order of months. The probability that a 
demand occurs in the SIF downtime may therefore be rather high. For type (b), the 
mean SIF downtime is MTTR and the probability of a demand in the SIF downtime 

282 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
Table 9.1 Probability of a hazardous event following a dangerous SIF failure where the final 
failure is a DD failure, for MTTR = 8 hours and selected mean demand intervals. 
Mean demand 
interval 
12 months 
8 months 
4 months 
2 months 
1 month 
14 days 
7 days 
1 day 
Probability of 
hazardous event 
0.09% 
0.14% 
0.27% 
0.55% 
1.09% 
2.17% 
4.65% 
28.35% 
is approximately (l — ^-^deMTTR^ % ^deMTTR, where Ade is the demand rate. In 
Table 9.1 the probability of a hazardous event is calculated for some selected mean 
intervals between demands for MTTR = 8 hours. 
Recommendations. 
1. For a SIF that is operated continuously mode, a hazardous event occurs immedi-
ately when the SIF fails to perform its safety function. In this case, a dangerous 
failure should be defined according to option 1 above, and the PFH should be 
calculated for this option. 
2. For a SIF that is operated in demanded mode, the choice of definition of a dan-
gerous failure should be made based on the demand rate. 
(a) When the mean time between demands is in the order of hours, the author 
recommends using the same approach as for continuously demanded SIFs. 
(b) When the mean time between demands is in the order of days and up to a 
month, the author recommends that the option 2 definition is used, but that 
type (b) failures are included in the calculation of the PFH. 
(c) When the mean time between demands is between one month and one year, 
the author recommends that option 2 is used and that type (b) failures can 
be disregarded in the calculation of the PFH. 
The PFH formulas that are presented in IEC 61508-6 and discussed in Section 9.5 
are only applicable for case 2 (c) in the above recommendations. 
The Time Interval (0, τ). The average PFH must be determined for a specific 
interval (0, τ). When the item is periodically proof-tested, r is the length of the 

AVERAGE FREQUENCY OF DANGEROUS FAILURES (PFH) 
2 8 3 
proof test interval. If the item is not proof-tested, (0, r) is either the time interval to 
an overhaul of the item, or the mission time of the item. IEC 62061 says that if the 
item is neither proof-tested nor overhauled, a mission time (or life time) of 20 years 
should be used in the calculation of PFH. 
To simplify the notation, (0, r) is called a proof test interval, but we should re-
member that it may sometimes be an overhaul interval or the mission time. 
Per Hour. In IEC 61508, PFH denotes the average frequency of dangerous failures 
per hour. A frequency of events is determined as the number of events per time unit. 
The time unit can be hours, weeks, months, and so on. A frequency can, for example, 
be given as two events per year. The term per hour in the expression for PFH is a 
measurement unit and should not be part of the definition of PFH. A more precise 
statement would be: PFH denotes the average frequency of dangerous failures, and 
the frequency shall be specified per hour. 
9.3.2 PFHofaSIF 
As described in Chapter 2, a SIF is performed by a safety loop with three indepen-
dent subsystems: a sensor subsystem (S), a logic solver subsystem (LS), and a final 
element subsystem (FE). Let Ni(t) denote the number of dangerous failures of sub-
system / (for i =S, LS, and FE) in the time interval (0, τ). The subsystems are based 
on different technologies and often placed in different locations. The likelihood of 
failures affecting more than one subsystem is therefore low. 
The total number of dangerous failures in (0, τ) is 
JVSIFM = NS(T) + NhS(r) + Ν¥Ε(τ) 
The expected number of dangerous failures is 
E[Nsw(r)] = E[NS(T)] 
+ E[NLa(r)] + E[NFE(r)] 
The average PFH of the SIF in (0, r) is obtained from (9.22) as 
PFH SIF(0, r) = PFHs(0, r) + PFHLS(0, r) + PFHFE(0, τ) 
(9.23) 
► Remark: 
1. Observe that (9.23) cannot be deduced directly from (9.10), because the failure 
rate of a subsystem is generally not constant. 
2. The subsystems are not always independent. Two categories of dependency are: 
(a) Common-cause failures (CCFs) may affect more than one subsystem. 
(b) During the repair of a failure, the other subsystems may be disconnected 
and cannot fail. 

2 8 4 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
CCFs and other types of positive dependency in a series structure are, however, 
not problematic because a conservative result is obtained by assuming indepen-
dent subsystems (at least when using the beta-factor model). 
φ 
In IEC 61508, the PFH requirement is related to a specific SIF. This requirement 
may be apportioned to subsystems, voted groups, and to single channels. In the 
following, the term PFH is used when discussing the frequency of dangerous failures 
of any item of the safety loop. 
9.3.3 
Assumptions 
In the remainder of this chapter, the average PFH is determined for voted groups and 
it is tacitly assumed that the voted group is a subsystem of a safety loop performing 
a specified SIF, such that when the group fails, the SIF also fails. 
The interpretation of a dangerous failure was discussed in the paragraph "The 
Meaning of Dangerous" in Section 9.3.1. As in IEC 61508, interpretation 2 of a dan-
gerous failure is adopted. A consequence of this interpretation is that when a group 
failure is detected, the EUC is immediately brought to a safe state and such failures 
are hence disregarded when determining the PFH of the group. The undetected group 
failures are, in the following, called dangerous group failures (DGF). The average 
PFH of a voted group (G) in the proof test interval is therefore 
Mean number of DGFs in (0, r) 
PFHG(0, T) = 
- 
-
τ 
To simplify the notation, PFH denotes the average PFH and the proof test interval 
(0, τ) is implicitly assumed. In the following, PFH means the average PFH in the 
proof test interval (0, τ). To specify the voting, the PFH is often written as PFHQ °°"' 
for a koon voted group (G). Further, the assumptions in Section 7.9 are assumed to 
apply-
In Section 9.4, simplified PFH formulas are derived for two separate cases for 
interpretation 2 of a dangerous failure. 
(A) In this case, the final failure (see Section 9.3.1) can be either a DU or a DD 
failure. This case is applicable for high-demand SIFs where the demand rate is 
so high that it is likely that a demand can occur during the restoration of the DD 
fault. 
(B) In this case, the final failure (see Section 9.3.1) must be a DU failure. This case 
presupposes that the demand rate is so low that the probability of a demand 
during the restoration of a DD fault is negligible. This is the same assumption 
that is used to derive the PFH formulas in IEC 61508-6. 

SIMPLIFIED PFH FORMULAS 
285 
9.4 Simplified PFH Formulas 
This section presents simplified formulas for the PFH of some simple voted groups 
of identical channels. Several PFH formulas are illustrated by numerical examples, 
for which the data set in Table 7.2 is used. Case A in Section 9.3.3 is treated first. 
9.4.1 
Voted Groups of Independent and Identical Channels: Case A 
In Case A, it is assumed that the demand rate is so high that it is likely that a demand 
may occur during the restoration time of a DD fault. The author recommends that this 
approach is used when the mean interval between demands is less than one month. In 
this section the channels are assumed to be independent and identical. Voted groups 
with common-cause failures are treated in Section 9.4.3 
PFH of a Single Channel. Consider a lool voted group with a single channel, 
for example, a final element subsystem with a single shutdown valve. When the 
channel gets a DD failure, this is at the same time a detected failure of the SIF and 
the EUC is immediately brought to a safe state according to the operating philoso-
phy in IEC 61508. DD failures of the channel are therefore not DGFs and can be 
disregarded when determining the PFH of the channel. 
If a DU fault is revealed in the proof test, this is a detected fault of the SIF and 
the EUC is immediately brought to a safe state. The repair time of the DU fault can 
therefore be disregarded when determining the PFH. A DGF is here a DU failure of 
the channel occurring in a proof test interval. Because a DU fault is undetected, there 
can be at most one DU failure in each proof test interval. The probability of a DU 
failure in a proof test interval of length r is 
ρ{τ) = Pr(r D U < τ) = 1 - <Τλ°υτ 
The mean number of DGFs (i.e., DU failures) in (0, τ) is 
E[NG(0, T)] = 0 · Pr(JVG(0, r) = 0) + 1 · Pr(yVG(0, τ) = 1) = ρ(τ) 
The PFH of a voted group (G) of a single channel with only DU failures is there-
fore 
ppugooi) = E[NG(0.r)] 
= ρψ 
= l_ ^ _ ^
j 
^ 
By using 1 - β~λ°υτ 
% λουτ, the PFH can be approximated by 
PFHG
lool) % ADU 
(9.25) 
The approximation is considered to be adequate when λου τ 5 0.10. This re-
quirement means that DU failures do not occur more often than once per ten proof 
test intervals. If the test interval is one year, DU failures should not occur more often 
than once per ten years (i.e., λου < 114 · 10-5 per hour). For some applications of 
high-demand SIFs, the proof test interval may be long, if the channels are tested at 
all. It may therefore be of interest to see what happens when τ increases. 

286 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
Numerical Example. 
Consider a channel with DU failure rate λου = 1.0-10 
per hour. The PFHpy0 
in (9.24) is calculated for some values (years) of x. 
x (years) 
pFHgool) 
(per hour) 
1 
0.996-10"6 
2 
0.991 · 10"6 
5 
0.978 · 10 - 6 
10 
0.957 · 10 - 6 
20 
0.917-10-6 
The average PFHQ°° 
goes down when the length of the interval increases. This 
is obvious because there can be at most one DU failure in each interval. The re-
duction with time is small and this is because the probability of a DU failure in an 
interval of? = 2 0 years is low, only 1 — e~^Ovt =» 17.1%. 
PFH of a Series Structure. 
A DD failure of one of the items of a series structure 
is a detected SIF failure and the EUC is immediately brought to a safe state. To 
determine the PFH of a series structure, it is therefore sufficient to consider only DU 
failures. Consider a series structure of n independent items with DU failure rates 
^DU,I.^DU,2, · · · ΛΌ\],Η· 
When a DU fault is revealed in the proof test, this is a 
detected SIF fault and the EUC is immediately brought to a safe state, such that the 
repair time may be disregarded. 
Because the failure rate of a series (s) structure of independent items is the sum 
of the failure rates of the individual items, the series structure can be considered a 
"super"-item with DU failure rate 
n 
^DU = 
/ 
,Λ-Ρυ,ί 
i = l 
In the same way as for a single channel, the PFH of the series structure (i.e., an 
noon voted group) for case A is 
PFH<ron) = - (l - e " A ^ r ) 
(9.26) 
(s} 
When A^(jT < 0.10, the PFH can be approximated by 
P F H g O M ) « A & = £A D U i I· 
(9.27) 
( = 1 
1oo2 Voted Group with Only DU Failures. Consider a loo2 voted group of inde-
pendent and identical channels that can only have DU failures. A loo2 voted group 
fails when both channels fail in the same proof test interval. The probability of a 
dangerous group failure (DGF) in a proof test interval (0, τ) is 
Pr[DGF in (0, τ)] = (l - 
ε~λΌυτ) 

SIMPLIFIED PFH FORMULAS 
2 8 7 
Because there can be at most one DGF (i.e., double DU failure) in a proof test inter-
val, the mean number, Να(τ) of DGFs in (0, τ) is 
E[NG(T)] 
= 0 · Ρτ(Ν(τ) = 0) + 1 · Ρν(Ν(τ) = 1) = (l - <Τ λ ο υ τ) 
The PFH in the proof test interval is from (9.22) 
pFHOoo2) = £[Λ|(τ)] = £ ^ _ β _ λ η υ τ ^ 
( 9 2 8 ) 
When λου^ is small (i.e., < 0.10), the approximation 1 - e~XouT sa ADU^ may 
be used to obtain 
p F Haoo2) % (λθ£Τ)2 = 
λ 2 υ Γ 
( 9 2 9 ) 
The average PFH is here a function of the length of the proof test interval τ and 
increases almost linearly when r increases. Note that we in (9.29) have assumed that 
λου^ is small (i.e., < 0.10) and the formula is therefore not valid for very long proof 
test intervals. 
di EXAMPLE 9.4 loo2 voted group, numerical example 
Consider a loo2 voted group of independent and identical channels and assume 
that the input data set in Table 7.2 is applicable. 
With this data, λ^,υτ «s 8.76 · 10-3, which means that the approximation 
should be sufficiently accurate. The probability that the voted group fails in 
(0, τ) is 
Pr[DGF in (0, τ)] = (l - β~λτ>υΛ 
as 7.61 · 10"5 
The approximation formula gives 
Pr[DGFin (0, r)] « (ADUr)2 % 7.67 · 10 -5 
which is 0.88% higher than the correct solution. 
The average PFH of the loo2 voted group is 
pFHgoo2) = _ ^ _ β-λουτ^ 
% g 6g . 10-9 p e r h Q u r 
If the proof test interval is extended to 10 years, the average PFH of the voted 
group becomes 
PFH 1 ° ^ = - i - (l - β-χ™10ή2 
% 8.02 · 10-8 per hour 
With such a long proof test interval, the approximation formula gives a result 
that is 9.1 % higher than the correct value. 
φ 

288 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
koon Voted Group with Only DU Failures. Consider a koon voted group of inde-
pendent and identical channels. The group fails as soon as at least n — k + 1 of its n 
channels fail. 
With n independent and identical channels, the probability that a channel fails in 
(0, r) is the same for all channels. The number Μ(τ) of channels that fail in the 
interval is therefore binomially distributed 
Pr(M(r) = j) = I") (l - β-λουτΥ 
(β-λΏντΥ 
J 
(9.30) 
A dangerous group failure occurs when at least n — k + 1 of the n channels have 
DU faults in the same proof test interval. 
Pr[DGF in (0, r)] = Pr(M(r) > n - k + 1) 
n 
= 
Σ 
Pr(M(r) = y) 
(9.31) 
j=n-k+l 
Let NQ(T) be the number of DGFs due to independent DU failures in (0, τ). 
Because DU faults are only revealed at time τ, Να(τ) can only take the values 0 and 
1, and the expected number of NQ(T) is 
E[NG(T)] 
= 0 · Pr(^ G(r) = 0) + 1 · PT(NG(T) 
= 1) 
= Pr(M(r) >n-k 
+ \) 
The PFH of a koon voted group with only independent DU failures is therefore 
pFH(i:oon) = 
E[NG(t)] 
G 
r 
= \ 
Σ 
( " ) ( l - e - A D u r ) y ( * - W ) " " y 
(9-32) 
Approximation Formulas. By using the approximations 1 - β~λΌΙ]Τ 
SB ADU^ and 
g-Aour % j^ (9 30) can be approximated by 
Pr(M(r) = y ) « r ) ( A D U r y 
When λου^ is small, 
(λ ο υτ)^ + 1 « (λΌυτΥ 
for all j > 1 
In this case, the average PFH is approximately 
jikoon) _ I 
J2 
PT(M^ 
- n ~ Pr(M(T) = n-k 
+ l) 
j=n-k+l 
p F H^oo n; = _·_ 
J2 
Pr(M(r) = j) 
n 
' - 
\n-k+\rn-k 
, , , (λ ο υ)"-* + 1τ"-* 
(9.33) 
[n — k + 1 / 

SIMPLIFIED PFH FORMULAS 
2 8 9 
For this approximation to be adequate, λου^ should be significantly smaller than 
0.10. 
β 
EXAMPLE 9.5 2oo3 voted group with only DU failures 
Consider a 2oo3 voted group of independent and identical channels and assume 
that the data set in Table 7.2 is applicable. The 2oo3 voted group fails when at 
least two of its three channels fail. 
The PFH of the 2oo3 voted group due to independent channel DU failures is 
from (9.32) 
T(2OO3) -7?0 (1-—)V°«r 
PFH> * 
r 7=2 
% 2.59· 10"8 per hour 
The approximation formula (9.33) gives 
pFH(2oo3) ^ 
3 A 2 ) u r = 
2 6 3 . 10-8 
p e r h o u r 
This approximated PFH is adequate because λου^ = 8.76 · 10-3 <5C 0.10. The 
average PFH of a 2oo3 voted group is seen to be three times as high as the aver-
age PFH of a parallel system with two channels (i.e., a loo2 voted group). This 
result can (as in Chapter 8) be explained by the fact that the 2oo3 voted group 
can be represented as a series structure of three minimal cut parallel structures, 
that is, as a series of three parallel structures with two channels in each. 
φ 
PFH of Voted Groups with Independent DU and DD Failures. In this section, 
the channels can have both DU and DD failures. The derivation is complicated for 
general koon voted groups, so we suffice with some simple cases. 
1oo2 Voted Group. Consider a loo2 voted group of two independent and identi-
cal channels. The voted group fails when both channels fail. Two combination of 
dangerous failures can give a DGF in the proof test interval (0, τ). 
(a) First, a DU failure occurs at some time t in (0, τ) and then a dangerous failure 
(DU or DD) occurs in the remaining part of the interval, that is, in (/, τ). 
The channel to fail first with a DU failure can be one out of two. When the 
first failure has occurred, there is only one option for the second failure. The 
probability of this option is 
PrfDGF in (0, r)] = f (l - β-λΌίτ~'Λ 
2λΌυβ-2λΏυ' 
dt 
^ (^DUT) + ^DU^DDÏ" 
(9.34) 

2 9 0 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
To obtain the result in (9.34), the approximation 1 — e x κ, χ — x2/2 is used. 
The contribution from this option to the PFH of the loo2 voted group is there-
fore 
D C O ( l o o 2 ) 
£ [ # G ( T ) ] 
I 2 
, i 
i 
mis\ 
PFH^,(a) = 
ss ADUr + A D U A D D T 
(9.35) 
(b) First, a DD failure occurs some time in (0, r) and then a D failure occurs before 
the DD failure is restored. 
The group has two identical channels and both of these can get a DD failure. 
The first DD failure therefore occurs with rate 2ADD- Because it is a single 
DD failure, the EUC continues to operate while the failed channel is restored. 
During the restoration time, the SIF is operating in degraded mode. To get 
a potentially dangerous SIF failure, the other channel must get a dangerous 
failure while the first DD failure is being restored. If this second D failure is a 
DD failure, the logic solver records a double DD fault and immediately initiates 
a transition of the EUC to a safe state. To get a DGF, the second failure must 
be a DU failure and this failure must occur within the MTTR of the first failure. 
The average frequency of DGFs (i.e., the PFH) is hence 
FFHg^f * 2ADD (l - r ^
M
m ) 
% 2ADDADUMTTR 
(9.36) 
The total PFH of the loo2 voted group is now 
p p o ( l o ° 2 ) _ pptr(loo2) . ρρχτ(1οο2) 
G 
G (a) 
G (b) 
% A D U T + A D U A D D T + 2 A D D A D U M T T R 
(9.37) 
Because MTTR <£ τ, the third addend of (9.37) is very small compared to the 
two first addends. The PFH may therefore be approximated by 
P F H £ O O 2 ) « ADUr + A D U A D D T 
(9.38) 
Observe that when ADD = 0, the same result as in (9.29) is obtained. 
Numerical Example. By using the data set in Table 7.2 and by solving the 
integral in (9.34) by MAPLE®, the "correct" contribution to the PFH of the loo2 
voted group from option (a) is 
PFHG(a)2) *
 5 ·
9 7 - 10~8 Per h o u r 
The approximation formula in (9.35) gives PFHç?°j % 6.13 · 10~8 per hour. 
The contribution from option (b) is P F H Q ? ^ 
«S 9.60 · 10-11 per hour and the 
contributions from the three parts of (9.38) are 
p F H ( i o o 2 ) ^ 8 7 6 10-9 + 526.10-8 
+ 9 6 0 . jQ-ii = 6 
1 4 . 1 0 - 8 
° 
' 
v 
' 
v 
' 
* 
v 
' 
»14.3% 
»85.7% 
%0% 

SIMPLIFIED PFH FORMULAS 
291 
The difference between (9.37) and (9.38) is therefore insignificant in this example. 
► Remark: Potential failures during proof-testing and repair of DU failures are not 
covered in (9.37). In some applications, the EUC is put into a safe state (often 
closed down) during proof-testing and repair, and additional failures in this period 
are therefore not relevant for the PFH. If a DGF is detected in the proof test, the 
EUC is immediately brought into a safe state according to the assumed operating 
philosophy. A DGF may occur if a single DU failure is revealed in the proof test, 
the repair is carried out online, and the other channel gets a DU failure during the 
repair time (MRT). The contribution from this case is normally smaller than the 
contribution from the MTTR in (9.37) and is therefore negligible. 
Θ 
2oo3 Voted Group. Consider a 2oo3 voted group of independent and identical 
channels. The voted group fails when at least two channels fail. The PFH of the 
group can be derived in the same way as for the loo2 voted group, by considering 
two options. 
(a) First, a DU failure occurs at some time t in (0, r) and then a dangerous failure 
(DU or DD) occurs in the remaining part of the interval, i.e., in (ί, τ). Each 
of the three channels can give the first DU failure. When a DU failure has 
occurred, there are two channels that can have a DU failure. The probability of 
a dangerous group failure in (0, τ) is hence 
Pr[DGF in (0, τ)] = ί 
(l - e " 2 ^ - ' ) ) 3χΌυ£-3λουΐ 
dt 
« 3 [(λ ο υτ) 2 + A D U A D D T 2 ] 
(9.39) 
The contribution from this option to the PFH of the 2oo3 voted group is 
PFH^° 3 ) * 3 [λ2
Ώυr + ADUADDτ] 
(9.40) 
(b) First, a DD failure occurs some time in (0, τ) and then a DU failure occurs 
before the DD failure is restored. 
The group has three identical channels and any of these can get a DD failure. 
The first DD failure therefore occurs with rate 3ADD· To get a DGF, one of 
the other two channels must get a DU failure while the first DD failure is being 
restored. The average frequency of DGFs (i.e., the PFH) is 
PFHg°°3) « 3ADD (l - e^DuMTTR^ 
ss 6ADDADUMTTR 
(9.41) 
The total PFH of the 2oo3 voted group is now 
pFH(2oo3) = pragma) + PFH<*°°3) 
% 3([λ2
Όυτ + λΌυλΌΌτ] 
+ 2ADDADUMTTR) 
(9.42) 

292 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
Numerical Example. By using the data set in Table 7.2 and by solving the 
integral in (9.39) by MAPLE®, the "correct" contribution to the PFH of the 2oo3 
voted group from option (a) is 
pFH(2oo3) % ! 7 5 .10-7 
p e r h o u r 
The approximation formula in (9.42) gives PFH^oo3) % 1.82 · 10~7 per hour. The 
contribution from option (b), when a DD failure comes first, is here 2.88· 1(T10 and 
is therefore negligible. 
(n-1)oon:F Voted Group. The same approach as outlined for the loo2 and 2oo3 
voted groups can be applied to any (n — l)oon voted groups. All these groups have 
hardware fault tolerance, HFT = 1. 
(a) The first failure is a DU failure. The voted group has n independent and iden-
tical channels and any of these can fail first. While the first channel has a DU 
fault, a DGF occurs as soon as one of the other n — 1 channels get a D failure. 
The probability of a DGF in (0, τ) is 
Pr[DGF in (0, τ)] = f (l - e-(«-m D(r-r)\ ηχΌΙ]β-»*-ουί dt 
« ί 2 1 [(λουτ)2 + ADUADDr2] 
(9.43) 
The contribution from this option to the PFH of the (n — l)oo« voted group is 
pFH[(„-i)oon] % M 
[ A2 ) u r + 
A D U A D D T ] 
(Q.44) 
(b) A DD failure occurs first. Because there are n independent channels, this DD 
failure occurs with rate «ADD- TO have a PGF, one of the remaining n — 1 
channels must get a DU failure while the first DD failure is being restored. The 
PFH for this option is therefore 
PFH^"1'00" 1 = ηλΌΏ (l - e-(«-DADUMTTRJ 
% n(n - 1)ADDADUMTTR 
= 2\2 UDDADUMTTR 
The total PFH of the (n — l)oon voted group is 
pFH[(»-i)oo»] % Γ | ( μ 2 υ Τ + A D U A D D T ] + 2ADDADUMTTR) 
(9.45) 

SIMPLIFIED PFH FORMULAS 
293 
koon:F Voted Group with n-k > 2. Consider a koon voted group of independent 
and identical channels. When n — k > 2, the group has hardware fault tolerance 
> 2. This means that at least three independent D-failures must occur in the proof 
test interval (0, τ) for a DGF to occur. The first observation is that the probability of 
additional failures while a DD fault is restored will be negligible. The probability of a 
DGF can be approximated by the probability of n —k DU failure and then a D failure 
in the same proof test interval. This probability is very low for all realistic input 
parameters, especially compared to the contribution from common-cause failures, 
which is discussed in the next section. General formulas for koon voted groups are 
developed by Jin et al. (2013), but the derivation is complicated and lengthy, and 
therefore not included here. 
9.4.2 Voted Groups of Independent and Identical Channels: Case B 
In case B (see Section 9.3.3), it is assumed that the demand rate is so low that the 
probability of a demand during the restoration of a DD fault is negligible. As dis-
cussed in Section 9.3.1, the final failure causing a DGF must therefore be a DU 
failure. It is still assumed that a detected group failure will initiate an immediate 
action to bring the EUC to a safe state. Detected group failures can therefore be 
disregarded when determining the PFH. The PFH of some simple voted groups are 
determined in the following and it is assumed that all channels can have both DD 
and DU failures. 
PFH of a Single Channel. The PFH of a single channel for case B is the same as 
the result (9.25) for case A. 
PFH£ O O 1 ) % A D U 
PFH of a Series Structure. 
The PFH of a series structure of n independent items 
with DU failure rates λου,ι . λου,2, ■ ■ ■, λου,η is as the result (9.27) for case A, 
n 
r r i i G 
s» ADU — / /DU,i 
( = 1 
PFH of a koon Voted Group. To derive the PFH of a koon voted group of inde-
pendent and identical channels, the DD failures may be disregarded, and the result 
for a koon group in (9.33) for only DU failures can be applied. 
i 
n 
/ 
\ 
1 
x—v 
/ n\ 
/ . 
i 
„W / 
i 
r\n~J 
ppjrifcoon) _ 
G 
r 
j=n-k+l 
(ADU)""* + 1T"-* 
(9.46) 
f 
n 
[n-k 
+ 1 
9.4.3 
PFH for Voted Groups with Common-Cause Failures 
Consider a koon voted group of identical channels and assume that the voted group 
is exposed to both individual failures and common-cause failures (CCFs). In line 

294 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
with the beta-factor model (see Section5.4), the PFH of the voted group can be split 
into two parts: 
1. P F H £ ° ° " \ that is, the PFH of the voted group due to independent (or individ-
ual) channel failures 
2. P F H £ ° ° " \ that is, the PFH of the voted group due to CCFs 
The PFH contributions from CCFs and individual failures are treated indepen-
dently, and the total PFH is calculated as 
p F H(*oon) = p F H ( W O 
+ ppHgoo») 
( 9 
4 ? ) 
Formulas for the independent part are derived in the previous section and CCFs 
are discussed in Chapter 10. In this section, the beta-factor model is used for the 
CCFs and it is assumed that the common-cause produces either DU-CCFs or DD-
CCFs, and not a mixture of DU and DD failures. In IEC 61508, ß is used to denote 
the beta-factor for DU-CCFs and ßo is used for DD-CCFs. Due to the frequent 
diagnostic testing, ß& is usually significantly lower than ß. 
When using the beta-factor model, the individual failure rate is λ^'υ 
= (1 — 
β)λνυ 
for DU failures and λ^Ό 
= (1 — βο)^ΏΏ for DD failures. The rate of 
individual D failures is hence λ^' = (1 — /Ö)ADU + (1 — βϋ)^ΌΌ-
Because a DD-CCF is a dangerous detected group failure (DGF), the EUC is 
immediately brought to a safe state. Only DU-CCFs are therefore taken into account 
when determining the PFH. The CCF-contribution to PFH is therefore: 
p F H(*oo») = 
β λ ο υ 
( 9 4 8 ) 
As discussed in Chapter 10, the CCF-contribution is the same for all possible con-
figurations. 
fl 
EXAMPLE 9.6 loo2 voted group with CCFs (Case A) 
Consider a loo2 voted group that is exposed to both individual failures and 
CCFs and assume that the data set in Table 7.2 is applicable. The notation and 
the assumptions are as above and the beta-factor model is used for CCFs. 
The average PFH in the time interval (0, τ) is from (9.37) and (9.48). 
p p u ( l o ° 2 ) _ ppT_j(l°°2) _i ρρτι(1°°2) 
Ci 
G, ί 
G, C 
% ([(1 - β)λΌυ]2τ 
+ [(1 - βΌ)λΌΌ] ■ [(1 - 
β)λΌυ]τ) 
+ 2 ([(1 - β)λΌυ] · [(1 - β0)λΌΏ]) 
MTTR + 
βλΌυ 
% 5.20 · 10~8 + 1.00 · 10-7 = 1.52 · 10-7 per hour 
* 
» 
' 
* 
« 
' 
Individual 
CCF 
This result shows that 66% of the PFH of the loo2 voted group is due to CCFs. 
Θ 

THE IEC 61508 FORMULAS 
295 
fl 
EXAMPLE 9.7 2oo3 voted group with CCFs (Case A) 
Consider a 2oo3 voted group that is exposed to both individual failures and 
CCFs and assume that the data set in Table 7.2 is applicable. The notation and 
the assumptions are as above and the beta-factor model is used for CCFs. 
The average PFH in the time interval (0, τ) is from (9.42) and (9.48). 
p F H(2oo3) = p F H(2oo3) + ppH(2oo3) 
% 3 ([(1 - β)λΌΙ]]2τ 
+ [(1 - βΌ)λΌΌ] ■ [(1 - 
β)λΌυ]τ) 
+ 6 ([(1 - β)λΌυ] · [(1 - βΒ)λΌΏ]) 
MTTR + 
βλΌυ 
ss 1.56 · 10"7 + 1.00 · 10~7 = 2.56 · 10"7 per hour 
Individual 
CCF 
This result shows that 39% of the PFH of the 2oo3 voted group is due to CCFs. 
Θ 
9.5 The IEC 61508 Formulas 
IEC 61508-6 provides approximation formulas for PFH for some voted groups with 
no more than three channels. The formulas are based on the same ideas as used for 
the IEC-formulas for PFDavg in Section 8.4. The assumptions in Section 9.3.3 still 
apply. 
The same notation as above is used and the channel-equivalent mean downtime is 
from (8.41) 
ÎCE = ^ ( I + M R T W ^ M T T R 
AD \L 
' 
AQ 
The group-equivalent downtime for a koon voted group is from (8.43) 
'<* = τ^ ( ^ π ^ + MRT) + τ^ MTTR 
The restoration time for DD failures is not covered in the IEC formulas because 
the contribution from hazardous events in the mean restoration time, MTTR, is con-
sidered to be negligible. 
9.5.1 
PFH of Some Simple Voted Groups 
The main PFH formulas given in IEC 61508-6 are presented in the following. 
Single Channel. A DD failure of a group with a single channel is a group failure 
and the EUC is immediately brought to a safe state. A dangerous group failure (DGF) 
therefore only occurs when a DU failure occurs and the PFH is consequently 
pFHaooi) = ADU 
( 9 4 9 ) 
This is the same result as for for the simplified PFH formulas. 

2 9 6 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
1oo2 Voted Group. A DGF of a 1 oo2 voted group can occur in two different ways: 
(a) First, an independent D failure occurs. This can be either a DU or a DD failure. 
The channel-equivalent downtime is ?CE- If the second failure is DD failure, 
this failure is detected and the function is restored within MTTR, such that the 
likelihood of a DGF in this short interval is negligible. The only option for a 
DGF is therefore that the second failure is a DU failure. The contribution to the 
PFH from this option is 
PFHg°°2) = 2λ£} (l _ e - U - / » W c E ) 
« 2 [(1 - βΌ)λΌΌ 
+ (1 - β)λΌν] (1 - |ß)ADUicE 
(b) A DU-CCF occurs. The contribution to the PFH from this option is 
PFHg°bf = βλΌυ 
The total PFH of the loo2 voted group is therefore 
PFH£ O O 2 ) % 2 [(1 - βΌ)λΌΌ 
+ (1 - β)λΌυ] (1 - ß)XOutCE 
+ βλΏυ 
(9.50) 
fl 
EXAMPLE 9.8 Numerical example for loo2 voted group 
Consider a loo2 voted group of two identical channels and assume that the data 
set in Table 7.2 is applicable. With this data set, the IEC formula (9.50) gives 
p F H g o o 2 ) 
= p F H(loo2) + 
p F H a o o 2 ) 
= 7.53 · 10"9 + 1.00 · 10"7 = 1.075 · 10~7 per hour 
* 
„ 
' 
* 
v 
' 
7.0% 
93.0% 
DU-CCFs are seen to be responsible for 93% of the total PFH for a loo2 voted 
group. The result is therefore not very sensitive to changes in the other parame-
ters (i.e., ADD, /?D, τ, MTTR, and MRT). 
Θ 
► Remark: If a loo2 voted group with only DU failures is considered (i.e., ADD = 
0), equation (9.50) becomes 
pFHaoo2) = 2 [(i _ /Ö)ADU]2 tcE + βλΌυ 
If the EUC is brought to a safe state during the repair action, ÎCE = τ/2, we get the 
familiar simplified formula (e.g., see (9.29) for the independent part) 
p F Haoo2) = 
[ ( 1 _ ß)Xm]2 
χ + 
βλΌυ 

THE IEC 61508 FORMULAS 
2 9 7 
2002 Voted Group. A 2oo2 voted group is in a dangerous situation when one of 
its channels gets a dangerous failure, but in the same way as for a single channel, the 
EUC is immediately brought to a safe state when a DD failure occurs. It is therefore 
only DU failures that are dangerous for the EUC. The PFH of a 2oo2 voted group is 
therefore 
p F H(2oo2) = 
2 A D U 
( 9 
5 1 ) 
2003 Voted Group. Consider a 2oo3 voted group of identical channels. As for the 
loo2 voted group, the 2oo3 voted group can have two types of dangerous failures. 
1. Independent failures: Assume that one of the three channels gets an independent 
D failure. The rate of this event is 3λ£, . To have a dangerous group failure, one 
of the remaining two channels must get an independent DU failure (with rate 
2(1 — /J)ADU) Def° re the first D failure is restored. 
2. Common-cause failure: A dangerous group failure takes place is a DU-CCF 
occurs. If a DD-CCF occurs, the EUC is immediately brought to a safe state 
and the DD-CCF is therefore not a dangerous group failure for the EUC. 
The IEC-formula for the 2oo3 voted group is therefore 
pFH(2oo3) = 
3 λ ( 0 fi _g-2(l-/l)ADU/cE) + 
βλΌχ] 
« 3 [(1 - jßD)ADD + (1 - β)λΌυ] 2(1 - ß)XDl]tCE 
+ βλΌυ 
= 6 [(1 - βΌ)λΌΌ 
+ (1 - β)λΌυ] (1 - β)λΌυίοΒ 
+ βλΌν 
(9.52) 
koon Voted Group. The IEC-formulas are only available for groups with no more 
than three channels. The following formulas can therefore not be claimed to be IEC-
formulas, but they are (hopefully) derived in the same spirit. 
We distinguish between two cases. 
1. Hardware fault tolerance, HFT = 1. In this case, k = n — 1, and the group of 
independent channels is voted (n — l)oon. This means that when one channel 
has a D failure, the group will have a dangerous failure for the EUC as soon as 
a following DU failure occurs. 
2. Hardware fault tolerance, HFT > 2. In this case k < n — 2, and when the first D 
failure has occurred, at least two more independent and dangerous failures need 
to occur before the group has a dangerous failure. 
The two situations are investigated separately. 

2 9 8 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
(n-1)oon Voted Group. The PFH of an (n — l)oon voted group can be derived by 
using the same arguments as for the 2oo3 voted group above. 
pFHK»-l)oo»] = 
ηχ(0 
^ _e-(»-l)(l-/J)ADU»cE^ + 
βλΌυ 
« n [(1 - βΌ)λΌΌ 
+ (1 - β)λΌυ] (n - 1)(1 - ß)XOl]tCE 
+ βλΌυ 
= n(n - 1) [(1 - βΌ)λΌΌ 
+ (1 - β)λΌν] (1 - /?)A D U; CE + 
βλΌυ 
= 2Î "2 )([(1 - βΌ)λΌΌ 
+ (1 - β)λΌυ] (1 - ß)XOl]tCE) 
+ βλΌυ 
(9.53) 
fl 
EXAMPLE 9.9 3oo4 voted group 
A 3oo4 voted group is an (n — l)oon voted group with n = 4. By using the data 
set in Table 9.1, the PFH becomes 
PFHgoo4) % 12 [(1 - βΌ)λΏΌ 
+ (1 - β)λΌυ] (1 - £)ADUiCE + βλΌν 
= 4.52 · 10"8 + 1.00 ■ 10~7 = 1.45 · 10-7 per hour 
31.1% 
68.9% 
For a 3oo4 voted group, there are (2) = 6 combinatons of two independent channel 
failures that may give a dangerous group failure. The contribution of individual 
failures to the PFH of an (n — l)oon voted group is seen to increase with n because 
the group fails as soon as two channels fail and the number of combinations of two 
channels increases with«. 
Θ 
koon Voted Group. Consider a koon voted group of identical channels where k < 
n — 2. The group has a hardware fault tolerance, HFT > 2. The first independent D 
failure of the group occurs with frequency ηλ^'. To get a dangerous group failure, 
at least two additional D failures have to occur before the first D failure is restored. 
The approach is illustrated by two simple cases, a loo3 voted group and a loo4 
voted group. 
1oo3 Voted Group. First, consider the contribution to the PFH from independent 
channel failures. A loo3 voted group of get a dangerous (for the EUC) group failure 
when the following events take place 
1. An independent dangerous channel failure occurs. The rate of this event is 3λ^ . 
The channel-equivalent mean downtime for the failure is ÎCE-
2. A second independent dangerous channel failure occurs in the downtime of 
the channel that failed first. Because there are two channels that can fail, the 
probability of the second dangerous failure is approximately 
1 _ ε - 2 λ ^ ί Ο Ε % 2 λ α ) 
D ?CE 

THE IEC 61508 FORMULAS 
299 
The joint mean downtime of the two failed channel is the (sub)group-equivalent 
mean downtime (of two channels) 
t<ßl = ~φ± (- + MRT) + - ^ M T T R 
(jh' 
i0) V3 
/ 
i0) 
3. The third independent dangerous channel failure must be a DU failure and must 
occur when both the other two channels are in a dangerous failed state. The 
probability of this event is 
1 _ e-^υ''αί % l ( 0 ,<2> 
I 
C 
~ 
A D T j i G E 
By adding the contribution from DU-CCFs, βλνυ, the PFH of the loo3 voted 
group due to independent channel failures is hence 
pFHOoo3) = J3 j [(1 _ ^ D ) A D D + ( 1 _ / 3 ) A D U ] 2 ( 1 _ β)λΌυί0Εΐ£1 
+ βλΌυ 
(9.54) 
The same formula is given in IEC 61508-6 without any justification. 
With the data set in Table 9.1, this PFH becomes 
pFH(ioo3) % 5 ? 9 
1 0_n + j 0 0 
10_7 = , 0 0 . 10-7 p e r h o u r 
The contribution from individual failures («a 0.058%) is seen to be totally negligible 
in this case. 
1oo4 Voted Group. Consider a loo4 voted group of identical channels. The group 
has hardware fault tolerance HFT = 3 and will fail when at all of its four channels 
fail. First, consider independent failures. A dangerous group failure (for the EUC) 
due to independent channel failures occurs when the the following events occur. 
1. An idependent dangerous channel failure occurs. The rate of this event is 4Ajy. 
The mean downtime of this channel is ÎCE· 
2. A second independent dangerous channel failure occurs in the downtime of 
the channel that failed first. Because there are three channels that can fail, the 
probability of the second dangerous failure is approximately 
The joint mean downtime of the two failed channel is the (sub)group-equivalent 
(2) 
mean downtime (of two channels) is ÎQ E and is the same as for the loo3 voted 
group. 
3. A third independent dangerous channel failure occurs while two channels are in 
a failed state. Because there are two channels that can fail, the probability of a 
third independent dangerous channel failure is 
1 -g-^v'CE 
~2λ ( 0/ ( 2 ) 
I
t 
~ Z A D i G E 

300 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
The joint mean downtime of the three dangerous failures is 
,<3> = % 
( I + MRT) + ^ M T T R 
AD 
AD 
4. The fourth independent dangerous channel failure must be a DU failure and 
must occur when both the other two channels are in a dangerous failed state. 
The probability of this event is 
ι_β-λΜ£ 
%A°V 3 ) 
l 
e 
~ ADU'GE 
The contribution to the PFH of a loo4 voted group from independent channel 
failures is therefore 
p F H(loo4) ^ 
A ) 0 ' ) o i ( 0 f 
9) (0,(2) j ( 0 ,(3) 
F ^ H G , i 
% 4 A D J A D f C E ^ A
D i GE ADU ?GE 
- 
9 A / J ( ' A 3 l(') 
t 
,(2) ,(3) 
- 
2 4 \ A D J 
ADU fCEf GE iGE 
= 24 ((1 - /3)ADU + (1 - £D)ADD)3 (1 - ^)A D UÎ CE4 2E4E 
The total PFH for the loo4 voted group is obtained by adding the contribution 
from CCFs, βλου-
PFHOoo4) = 2/4j 
[(1 _ / 6 ) A D U + (1 -βΌ)χΌΌγ 
{l-ß)XDUtCEt%ltgl 
+ ßXOU 
(9.55) 
With the data set in Table 9.1, this PFH becomes 
pFH(ioo4) = 4 6 9 . 1 0-i3 + i.oo· 10"7 = 1.00· 10-7 per hour 
The contribution from individual failures is seen to be totally negligible in this case. 
By using the same approach for a loon voted group where n > 4, the contribution 
from independent dangerous failures is even smaller. 
A koon voted group of identical channels can always be represented as a series 
structure of its minimal cut parallel structures. Each minimal cut set is a loo(« — 
k + 1) voted group and all the (n_£+1) minimal cut sets are equal. A koon with 
HFT = 2 is therefore a series structure of equal loo3 voted groups. Because it has 
been shown that the contribution to the PFH from independent dangerous channel 
failures of a loo3 voted group is negligible, the contribution to the PFH of the koon 
from independent dangerous channel failures is also negligible. When HFT > 2, this 
contribution is even more negligible, at least for relevant values of n and relevant 
input parameters. 
We therefore conclude that for koon voted groups with a hardware fault tolerance, 
HFT > 2, it is not worthwhile to determine the PFH contribution from independent 
dangerous failures. 

ALTERNATIVE IEC FORMULAS 
301 
The following guide is therefore given: 
1. For noon voted groups, use 
pFH(«oo«) = 
M A D U 
2. For (n — l)oon voted groups, use 
pFH[(n-l)oon] 
= 
2 / » j [ ( 1 _βΌ)λΌΌ 
+ (1-β)λΌυ](\-β)λΌυίαΕ 
+ 
βλΌυ 
3. For koon voted groups with (n — k) > 2, use 
p F H(too«) = 
( β λ β υ 
9.6 Alternative IEC Formulas 
The IEC formulas for PFDavg were derived in Section 8.4 based on the equation 
PFDavg = AD,GiGE 
(9.56) 
Here, AD,G denotes the frequency of dangerous group failures (DGF) and is the same 
concept as PFHG- It may therefore be relevant to determine PFHG from 
PFD; 
As in Section 9.5, PFHQ can be split into the two parts: an independent part 
and a CCF part. For the CCF contribution, we can again disregard the DD-CCFs, 
because these failures are detected and the EUC can be brought to a safe state before 
a hazardous event takes place. The contribution from DU-CCFs to the PFDavg is 
given in (8.51). The group-equivalent downtime for a DU-CCF is 
PFHG = 
avg 
(9.57) 
, (DU-ccF) = 
^ + M R T 
The contribution to the PFHQ from DU-CCFs is therefore 
ppD(DU-CCF) 
P F HG,c = 
, (DU-CCF) 
= ßkVV 
( 9- 5 8 ) 
rGE 
which is the same DU-CCF contribution that was used in Section 9.5. 
The contribution to PFHG from independent cannel failures for a &oon voted 
group is from (8.49) 
ppr)(0 
PFHG,i 
- 
—öj— 
'GE 
-(::i)(^r
+,(<sr 

302 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
This approach would give an adequate result if the PFDavg and the PFHQ were 
determined based on the same assumptions. The problem is that IEC 61508 does 
not give the same requirements for operational strategies for determining the two 
measures. Whereas IEC 61508 prescribes that the EUC shall be immediately brought 
to a safe state when a dangerous group failure is detected, for calculating PFHQ, 
the same is not required for calculating PFDavg. The difference is illustrated in 
Example 9.10. 
β 
EXAMPLE 9.10 2oo3 voted group 
Consider a 2oo3 voted group of identical channels. By using (9.59) the contri-
bution to the PFHQ of independent channel failures is 
PFH^ o 3 ) = 3 
6 [ ( 1 - , 8 D ) A D D + ( 1 - | 8 ) A D U ] 
( 1 - 0 D ) * D D + ( 1 - J 8 ) A D U 
DD failure 
r(/) 
' C E 
This is seen to be nearly the same result as obtained by using the standard IEC 
formulas in (9.52). The difference is that a DD failure cannot be the final failure 
in (9.52), whereas it is accepted in this example. 
φ 
9.7 The PDS Method 
The PDS method determines PFHQ from the simplified formulas in Section 9.4 but 
uses a configuration factor C^oon when modeling CCFs, such that the contribution 
to the PFH of a koon voted group from CCFs is 
PFH 
(koon) 
G,c 
Ckoonß^DV 
(9.60) 
The simplified formulas used by the PDS method are listed in Table 9.2. 
DD failures are not considered in the simplified formulas, but the PDS method 
also include some more advanced formulas where DD failures and MTTRs are con-
sidered. These formulas are presented in SINTEF (2013b), but not shown in this 
book. 
9.8 
Fault Tree Approach 
Fault tree analysis was introduced in Chapter 5 and applied to SIFs in Chapter 8. As-
sume that a fault tree has been established for a specific SIF with TOP event "The SIF 
has a D failure." When constructing the fault tree, it is important to incorporate the 
strategy that is adopted with respect to EUC transfer to a safe state when dangerous 

FAULT TREE APPROACH 
303 
Table 9.2 
PDS formulas for average PFH. 
Voting 
lool 
noon 
loo2 
loo3 
2oo3 
koon 
PFH formulas 
Contribution from 
Independent failures 
ADU 
nXuv 
A2 
T 
ADU T 
J 3 
T 2 
j A D U r 
/ 
n 
\-ln-k + l^n-k 
\n-k+V*DU 
τ 
in the PDS method 
+ 
+ 
+ 
+ 
Contribution from 
CCFs 
NA 
NA 
/M-DU 
Cloo3^DU 
C2oo3ßADU 
Q O O T I M D U 
SIF faults are detected. Further, assume that both individual faults and CCFs have 
been explicitly modeled as basic events in the fault tree. 
To get a dangerous SIF failure, a basic event must become critical and then occur. 
A basic event i is said to be critical for the SIF if the rest of the fault tree is in such a 
state that "everything" depends on basic event i. If basic event / does not occur, the 
TOP event does not occur, and if basic event / occurs, the TOP event occurs. 
The probability that basic event / is critical for the TOP event at time / is expressed 
by Bimbaum's measure of importance IB(i 
| t), which is available in almost all 
computer programs for fault tree analysis. Bimbaum's measure of importance was 
introduced in Chapter 5 and is discussed in detail in Rausand & H0yland (2004). 
Assume that all basic events in the fault tree are independent and remember that 
we have assumed that CCFs are modeled explicitly in the fault tree. By this approach, 
we may assume that the CCFs are independent from the individual failures. Since the 
property that basic event i is critical for the TOP event was defined by an expression 
about the other basic events in the fault tree, we may consider the two events related 
to basic event / : (a) Basic event / is critical, and (b) basic event i gets a dangerous, 
D, failure, to be independent. 
The frequency (rate) of the occurrence of the TOP event at time /, caused by basic 
event i, is therefore 
u>TOP,i(0 = / 0" 10 «>/(0 
(9.61) 
where if, (t) is the unconditional rate of occurrences of basic event i at time t. 
The instantaneous P F H Q ( 0 related to the specified SIF is therefore 
PFHG(O = ^ / B 0 ' IOw>/(0 
i' = l 
where n is the number of basic events in the fault tree. 

3 0 4 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
The average PFH over an interval, say (0, τ), is then given by 
1 Γ " 
PFHG(0, r) = - / 
V IB(i | t) Wj(t) dt 
(9.62) 
The rate tu/(i) is the unconditional rate of occurrence of basic event /. If basic 
event i denotes the event where a single channel (or a CCF) gets a D failure with 
D failure rate AD,/, then Wj(t) = AD,/· In many cases, Birnbaum's measure will 
be approximately constant, at least after some time. If we assume that IB(i 
\ t) — 
IB(i), 
and that all the n basic events have constant rates λο,<, then 
n 
PFHG = ^ / B 0 ) A D , , 
( = 1 
In some cases, one or more of the basic events represent a group of channels 
and the rate of occurrence of the basic event is therefore not constant. In this case, 
we have to use the formulas derived in Section 9.2. The fault tree approach for 
SIF reliability assessment is thoroughly discussed by Dutuit et al. (2008). See also 
Cacheuxetal. (2013). 
9.9 Markov Approach 
The Markov approach was introduced in Chapter 5 and used to determine the PFDavg 
in Chapter 8. The Markov approach used to determine the PFH is very similar to the 
one used for PFDavg. The main difference is that our focus here is to determine the 
frequency of dangerous failures, that is, the frequency of transitions from a nondan-
gerous state to a dangerous state. 
As before, let U denote the up-states (i.e., nondangerous states) and let V denote 
the dangerous (or down) states for a voted group. The nondangerous states can be 
split in two subsets Uc and Unc, where Uc are the critical states in the sense that a 
single transition can bring the voted group into a dangerous state in V. If the voted 
group is in a noncntical state in Unc, two or more transitions are required to bring the 
voted group into a dangerous state. The average PFH is hence the average frequency 
of transitions from states in Uc to a state in V. These concepts are illustrated in 
Example 9.11. 
9.9.1 
Steady-State Solution 
To determine PFHQ as a steady-state solution is straightforward and does not intro-
duce any new issues. The approach is illustrated in Example 9.11. 
β 
EXAMPLE 9.11 2oo4 voted group of independent channels 
Consider a 2oo4 voted group of identical and independent channels with dan-
gerous (D) failure rate λΏ. We disregard the possibility of safe failures and 

MARKOV APPROACH 
305 
Table 9.3 
The possible states of the 2oo4 voted group in Example 9.11. 
State 
State description 
0 
Four channels are functioning 
1 
Three channels are functioning and one is failed 
2 
Two channels are functioning and two are failed 
3 
One channel is functioning and three are failed 
4 
Four channels are failed 
4λ0 
3λ0 
2λ„ 
λΒ 
μ 
2μ 
3μ 
4μ 
ι 
Non-dangerous states 
1 
ι— Dangerous states — ι 
I 
1 
I 
1 
Non-critical 
Critical 
Figure 9.4 
Markov model for the 2oo4 voted group in Example 9.11. 
assume that four independent repair teams are available. Because the channels 
are identical, it suffices to define the five states in Table 9.3. 
A Markov model for the voted group is shown in Figure 9.4. From this figure, 
we note that Unc = {0,1}, Uc = {2}, and V = {3,4}. For this voted group, only 
one transition gives a dangerous group failure, that is the transition from state 2 
to state 3. No other transition will immediately result in a dangerous failure of 
the voted group. 
The PFH is therefore equal to the frequency of transitions from state 2 to 
state 3. 
PFH G(0,r) = />2-2AD 
Because there are no absorbing states in this system, the steady-state probability 
P2 can be used to determine P F H Q -
In this example, only one state is critical, but if CCFs had been taken into 
account, more states would be critical. 
® 
► Remark: Example 9.11 is made to illustrate the different categories of states and 
is not a fully realistic example because we do not distinguish between DU and DD 
failures and have not modeled any transitions to a safe state. 
φ 

306 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
Table 9.4 
The possible states for the loo2 voted group in Example 9.12. 
State 
State description 
0 
Both channels are functioning (OK) 
1 
One channel has a DD fault, one is OK 
2 
One channel has a DU fault, one is OK 
3 
Both channels have DD faults 
4 
Both channels have DU faults 
5 
One channel has a DD fault and the other a DU fault 
6 
The EUC is brought to a safe state 
9.9.2 Time-Dependent Solution 
The time-dependent solution is also rather straightforward and is illustrated in Ex-
ample 9.12. 
fl 
EXAMPLE 9.12 
loo2 configuration 
Consider a loo2 voted group of identical channels. Possible safe (S) failures 
are disregarded. The voted group can have seven different states as described in 
Table 9.4. 
Common-cause failures (CCFs) may occur and these are modeled by a beta-
factor model. For simplicity, the same beta-factor ß is used for both DU and 
DD failures. CCFs occur due to external shocks and will have the same effect 
in all the states {0,1,2}. We assume that two independent repair teams with the 
same resources are always available. We also assume that the voted group is 
proof-tested at regular intervals of length τ and that the proof test is perfect with 
100% coverage. 
A Markov model for the loo2 voted group is shown in Figure 9.5, where the 
transition rate symbols are also indicated. 
The voted group is functioning in states U = {0,1,2} and is failed in states 
V = {3,4,5}. A dangerous group failure (DGF) occurs each time there is a 
transition from a state in U to a state in V. When the voted group comes to state 
3, a DGF is detected and the EUC is almost immediately brought to the safe 
state (i.e., state 6). The mean time required to bring the EUC to a safe state is 
1/ju.ss- When the EUC is brought to a safe state, a repair action of the voted 
group is initiated. Assume that the voted group is always repaired to state 0 and 
that the mean repair time is l//*so· 
Because the voted group is as-good-as-new after each proof test, we may 
consider only one test interval (0, τ). In this interval, all the states are transient, 
except for state 4. Assume that the voted group is in state 0 at time t = 0. 

PETRI NET APPROACH 
307 
Figure 9.5 
Markov model for the loo2 voted group in Example 9.10. 
The methods described in Chapter 5 can now be used to determine the state 
probabilities P(t) = [P0(0, Pi(t),..., 
P6(0]. 
To have a transition from a state in U to a state in V, the voted group must be 
in a functioning state, and then "jump" to a failed state. The following transitions 
will give a dangerous failure of the voted group: 0 —► 4, 1 —*■ 5, 2 -> 4, and 
2 ^ 5 . 
The instantaneous PFH(i) at time t is therefore 
PFHG(i) = Ρ0(0βλΌν 
+ Λ ί θ λ ο υ + -Ρ2(0(λου + ADD) 
The average PFH over the test interval will in this case also be the long-term 
average and is given by 
1 
f 
P F H G ( 0 , T ) = - / 
P F H G ( 0 ^ 
τ Jo 
Note that the Vesely failure rate (see Section 9.2.2) is used when calculating the 
frequency of failures in this example. 
® 
9.10 
Petri Net Approach 
The Petri net approach was introduced in Chapter 5 and was used to determine 
PFDavg in Chapter 8. As for the Markov approach, the Petri net approach for de-
termining PFHG is also very similar to the one used to determine PFDavg. The Petri 
net approach for high-demand mode is not pursued further in this book. Interested 
readers may consult ISO/DTR 12489 (2012). 

3 0 8 
AVERAGE FREQUENCY OF DANGEROUS FAILURES 
9.11 PFDavgorPFH? 
When demand rate is close to once per year, one should be able to claim compliance 
with IEC 61508 by using either PFDavg or PFH as reliability measure. The problem 
is, however, that the two reliability measures may lead to different conclusion. If 
we use PFH and the associated IEC formulas, we may be able to claim that the SIF 
meets the requirements for, for example, SIL 3. If we, on the other hand, use PFDavg 
and the associated IEC formulas, we may not be able to claim that the SIF meets the 
same requirements. 
There may be at least three possible causes of this inconsistency: 
1. The intervals in the SIL tables for the different SILs are not calibrated ade-
quately. 
2. The IEC formulas for low-demand and high-demand mode of operation are 
based on different assumptions. 
3. The approximations the IEC formulas are based on are not fully adequate when 
the demand rate is around once per year. 
The causes of this problem have not been carefully enough examined into by the 
author, and he is yet not prepared to conclude on this matter. See Liu & Rausand 
(2011) and Jin et al. (2011) for further treatment of this issue. 
9.12 Additional Reading 
The following titles are recommended for further study related to Chapter 9: 
- Contribution to modeling safety instrumented systems and to assessing their 
performance. Critical analysis of IEC 61508 standard (Innal, 2008). 
- Petroleum, petrochemical and natural gas industries - Reliability modeling and 
calculation of safety systems (ISO/DTR 12489, 2012). 
- Reliability prediction methods for safety instrumented systems, PDS method 
handbook (SINTEF, 2013b). 
- New PFH-formulas for k-out-of-n:F-systems (Jin et al., 2013). This paper pro-
vides slightly more general approximation formulas compared to the formulas 
presented in this chapter. 

CHAPTER 10 
COMMON-CAUSE FAILURES 
10.1 
Introduction 
Modeling of common-cause failures (CCFs) was introduced in the nuclear power 
industry in the 1970s (e.g., see NUREG-75/014). This industry has had a continuous 
focus on CCFs and has been in the forefront of the development of CCF models, 
and on collection and analysis of data related to CCF. The aviation industry has 
also given these failures close attention, and the Norwegian offshore industry has 
since the 1990s focused on CCFs related to reliability assessment of SISs (e.g., see 
SINTEF, 2013b). 
IEC 61508 points at the need to control CCFs in order to maintain the safety 
integrity of SIFs. The standard suggests calculating the reliability of a SIF where the 
well-known beta-factor model is used to model the contribution ofCCFs. 
The concerns about CCFs may be expressed by the quotation: 
A primary concern is that a design that uses the same or shared hardware, software 
(included embedded software), and data may be susceptible to common-cause fail-
This chapter is partly based on an earlier joint work with Per Hokstad, SINTEF. 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
309 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

3 1 0 
COMMON-CAUSE FAILURES 
ures, thus defeating the defense in depth concept that is implemented in the design 
(DOE-STD-1195, 2011). 
10.1.1 
Dependent Failures 
When items fail, the failures cannot always be considered as independent events. 
Consider a system of two items, 1 and 2. Let Ej denote the event that item i is in 
fault state, for / = 1,2. The probability that both components are in fault state is 
Ρτ(Ε! ΓΊ E2) = Pr(£, | E2) ■ Pr(£2) = Pr(£ 2 I Ex) ■ Pr(£j) 
where ΡΓ(£Ί | E2) is the probability that item 1 is in fault state when it is known 
that item 2 is in fault state, and Ρ Γ ( £ 2 I E\)\& the probability that item 2 is in fault 
state when it is known that item 1 is in fault state. 
The two items are said to be independent when Pr(£i | E2) = Ρ Γ ( £ Ί ) and 
Pr(£ 2 I Ei) = Pr(£2), suchthat 
Pr(£i Π E2) = Pr(£i) · Pr(£2) 
The items have a positive dependency when Pr(£i | E2) > Pr(£i) and Pr(£ 2 | 
£ 0 > Pr(£2), such that 
P r ( £ 1 n £ 2 ) > P r ( £ 1 ) - P r ( £ 2 ) 
and the items have a negative dependency when Pr(£i | E2) < ΡΓ(£Ί), andPr(£2 | 
Ei) < Pr(£2), such that 
ΡΓ(£Ί Π EI) < Ρτ(Ει) · Pr(£ 2) 
In reliability analysis, positive dependency is usually the most relevant type of 
dependency. Negative dependency may, however, occur in practice. There are two 
main types of positively dependent failures: (i) cascading failures, where one item 
failure leads to higher stress and thereby higher probability that another item fails, 
and (ii) common-cause failures, where multiple failures occur due to a common-
cause. For SIFs, the last category is the most important. 
10.1.2 
Definition of Common-Cause Failures 
The term common-cause failure (CCF) has for a long time been discussed in relation 
to both risk and reliability analysis. Still, there is no generally accepted definition of 
a CCF that applies for all types of systems. 
Nuclear Industry. In the nuclear power industry, a CCF is defined as 
Common-cause failure (nuclear industry). Dependent failure in which two or more 
component fault states exist simultaneously, or within a short time interval, and 
are a direct result of a shared cause (NEA, 2004). 

INTRODUCTION 
311 
According to this definition, the components do not have to fail at the same time, 
but the components must be in a fault state at the same time, or nearly the same 
time. For a SIF where the sensor subsystem has n items, two or more DU failures 
can occur at different times in the same proof test interval. Because the DU failures 
are undetected, the items remain in DU fault states until the proof test is carried 
out. According to this nuclear CCF definition, a CCF of DU failures has therefore 
occurred if two or more DU faults (with a shared cause) are revealed in a proof test. 
It may also be important to note that the common-cause does not need to be an event. 
IEC61508. To claim that a SIF has a CCF implies that the SIF must have failed. 
A multiple fault in a redundant subsystem where the system is still able to function 
is therefore not a failure of the SIF; and consequently not a CCF of the SIF. This is, 
for example, the case for a double channel fault in an input subsystem with a 2-out-
of-4 voting. The subsystem is still functioning with two channels in fault state. The 
double fault has reduced the reliability of the SIF, but not caused a subsystem failure. 
On this basis, IEC 61508 (2010) gives the following definition of a CCF: 
Common-cause failure (IEC 61508). Failure, that is the result of one or more events, 
causing concurrent failures of two or more separate channels in a multiple channel 
system, leading to system failure IEC 61508 (2010, Part 4, def. 3.6.10).1 
There are two issues related to this definition. The first is that the common-cause 
is specified to be "one or more events." A condition, such as "higher humidity than 
specified," is not an event, and can therefore not be a common-cause according to the 
IEC 61508 definition. The other issue is related to the term "concurrent failures." A 
failure is an event that takes place at a specific time. The term used in the definition 
therefore implies that the failure events must occur concurrently, which can be inter-
preted as rather close in time. This is in opposition to the CCF definition used in the 
nuclear power industry. 
New Definition. By combining the two definitions above, the author proposes a 
new definition of a CCF of a SIF: 
Common-cause failure. Failure, that is the direct result of a shared cause, in which 
two or more separate channels in a multiple channel system are in fault state 
simultaneously, leading to system fault. 
When multiple DU faults are revealed in a proof test, it is often difficult to decide 
whether they are due to a common shared cause and thus represent a CCF, or they 
are merely due to independent failures. To claim that they are a true CCF must be 
based on a thorough investigation of the causes of the faults. 
The term CCF implies the existence of a cause-effect relationship that links the 
CCF event to some cause. Such a relationship is not, however, reflected in the CCF 
models that are presented later in this chapter (e.g., see Littlewood, 1996). 
'IEC61508-4 ed.2.0 "Copyright © 2010 IEC Geneva, Switzerland, www.iec.ch." 

312 
COMMON-CAUSE FAILURES 
Two Categories of CCFs. CCFs can occur 
(a) at the same time due to a shock, or 
(b) over a certain interval due to an increased stress (e.g., temperature, humidity, 
vibrations). 
The occurrence of shocks is often modeled by a homogeneous Poisson process. If 
a DU-CCF of category (a) is revealed in a proof test, the mean time the SIF has been 
unavailable in the proof test interval is equal to the half of the interval (i.e., τ/2). 
Similarly, if a DD-CCF occurs in a diagnostic test interval of length TD, the mean 
downtime in the interval is TD/2. If the EUC is brought to a safe state immediately 
after the fault is detected, the effect of DD-CCFs on the PFDavg of low-demand SIFs 
is small. 
DU-CCFs of category (b) have a similar effect as category (a), but the mean down-
time in the proof test interval is difficult to assess, as it is dependent on the time of 
occurrence of the common-cause, the architecture of the subsystem, and the degra-
dation processes due to the common-cause. 
► Remark: Increased stress may not only lead to a CCF of category (b), but may 
also increase the independent failure rates of the affected channels. In some cases, 
the increased stress may imply that the constant failure rate assumption is no longer 
valid and that the channels will have increasing failure rate functions. 
0 
DD failures are usually revealed short time after the failures occur. For category 
(b) failures, the EUC may therefore be brought into a safe state before the next failure 
occurs, such that a system DD-CCF is avoided. The likelihood of DD-CCFs of 
category (b) may therefore be negligible. 
Multiple Failures That Are Not a CCF. As mentioned above, a multiple failure with 
a shared cause does not need to be a CCF. It is sometimes useful to have a specific 
term for such a multiple failure in relation to a SIF. The following term is therefore 
introduced: 
Multiple failure with a shared cause (MFSC). Failure, that is a direct result of a shar-
ed cause, in which two or more items are in fault state simultaneously. 
An MFSC is also called a CCF event, but the author prefers the term MFSC be-
cause the term CCF event may be confused with a CCF. Note that when an MFSC 
leads to system failure, then the MFSC is a CCF of the system. CCFs have particu-
larly been focused in systems where there is a high risk for fatal accidents. Methods 
for controlling and preventing such failures have been developed during safety anal-
yses within the aviation and the nuclear power industry. 
10.2 Causes of CCF 
Many authors find it useful to split CCF causes into root causes and coupling factors 
(e.g., see Parry, 1991; Paula et al., 1991). A root cause is a basic cause of an item 

CAUSES OF CCF 
313 
Ei 
Root 
causes 
Coupling 
factors 
^^^ 
,( 
^
\ 
Figure 10.1 
Root causes in combination with coupling factors lead to CCFs (£,- denotes 
that item ; is failed, for ; = 1,2, 3). 
failure (e.g., a corrosive environment), whereas a coupling factor explains why sev-
eral items are affected by the same root cause (e.g., inadequate material selection for 
several valves). Root causes and coupling factors leading to CCFs are illustrated in 
Figure 10.1. 
10.2.1 
Root Causes 
A root cause of a failure is the most basic cause that, if corrected, would prevent 
recurrence of this and similar failures. There is often a series of causes that can be 
identified, one leading to another. This series of causes should be pursued until the 
fundamental, correctable cause has been identified (US DOE, 1992). The concept of 
root cause is tied to that of defense, because there are, in many cases, several possible 
corrective actions (i.e., defenses) that can be taken to prevent recurrence. Knowledge 
about root causes allows system designers to incorporate countermeasures for reduc-
ing the susceptibility to both single failures and CCFs. 
A number of studies have investigated the root causes of CCF events, and several 
classification schemes have been proposed and used to categorize these events (e.g., 
see Paula et al., 1991 ; NEA, 2004; US DOE, 1992; Rasmuson, 1991). Several studies 
of CCFs in complex systems have shown that the majority of the root causes are 
related to human actions and procedural deficiencies. A study of centrifugal pumps 
in nuclear power plants indicates that the root causes of about 70% of all CCFs are 
of this category (Miller et al., 2000). 
In practice, root causes of item failures can seldom be determined from failure 
reports. CCF root causes have to be identified through root cause analyses, supported 
by checklists of generic root causes (US DOE, 1992). The description of a CCF in 
terms of a single root cause is in many cases too simplistic (Parry, 1991). Cooper 
et al. (1993) advocate using the concept of common failure mechanism instead of 
root cause to take into account multiple root causes. 
10.2.2 Coupling Factors 
A coupling factor is a property that makes multiple items susceptible to failure from 
a single shared cause. Such properties include: 
- Same design 

3 1 4 
COMMON-CAUSE FAILURES 
- Same hardware 
- Same software 
- Same installation staff 
- Same maintenance or operation staff 
- Same procedures 
- Same environment 
- Same location 
A more detailed taxonomy of coupling factors is given in NEA (2004); NUREG/CR-
5485 (1998); Childs & Mosleh (1999). Studies of CCFs in nuclear power plants 
indicate that the majority of coupling factors contributing to CCFs are related to 
operational aspects (Miller et al., 2000). 
To save money and ease operation and maintenance, the technical solutions in 
many industries become increasingly standardized. This applies both to hardware 
and software and increases the presence of coupling factors. SINTEF, the Norwegian 
research organization, has recently carried out several studies of the impacts of this 
type of standardization on Norwegian offshore oil and gas installations, where new 
operational concepts and reduced manning levels are feeding this trend (Hauge et al., 
2006). 
10.3 Defenses Against CCF 
IEC 61508-6 suggests three overall measures that can be used to reduce the proba-
bility of dangerous CCFs. These are: 
(a) Reduce the overall number of random hardware and systematic failures. 
(b) Maximize the independence of the channels. 
(c) Reveal nonsimultaneous CCFs while only one, and before a second, channel 
has been affected. 
A high number of possible defense measures against CCFs have been proposed. 
Some main categories of defense measures are: 
- Separation or segregation: Separation/segregation can be both physical and 
electrical and enhances the independence of the channels and reduces the sus-
ceptibility to CCFs and Domino effects. 
- Diversity: Different components and different technologies reduce the likeli-
hood of coupling factors and the susceptibility to CCFs. 
- Robustness: A robust design has a higher ability to withstand environmental 
stresses (e.g., see DOE-STD-1195, 2011). 

EXPLICIT VERSUS IMPLICIT MODELING 
3 1 5 
- Channel reliability: High channel reliability reduces the number of both indi-
vidual and dependent failures, and thereby the number of maintenance actions 
and human interventions (which are generally recognized causes of CCFs) 
- Simplicity of design: A simple design is easier to understand and maintain and 
reduces the number of intervention errors. 
- Analysis: FMECA and other reliability analyses can identify causes of CCFs 
and propose measures to reduce the likelihood of CCFs 
- Procedures and human interface: Clear procedures and an adequate human-
machine interface reduce the likelihood of human errors. 
- Competence and training: Designers, operators, and maintainers can help to 
reduce CCFs by understanding root causes and coupling factors. 
- Environmental control: The susceptibility to CCFs can be reduced by weather 
proofing. 
- Diagnostics and coverage: A diagnostic system with high coverage can reveal 
the first non-simultaneous CCFs and bring the EUC to a safe state before the 
next failure occurs. 
For more defense measures, see IEC 61508-6 and Smith & Simpson (2011). 
10.4 
Explicit Versus Implicit Modeling 
Two different approaches can be applied to the modeling of CCFs, an explicit method 
and an implicit method. IEC 61508, Part 6, Annex D distinguishes between 
1. Dependent failures due to clear deterministic causes 
2. Residual potential multiple failure events not explicitly considered in the anal-
ysis because of not enough accuracy, no clear deterministic causes or impossi-
bility to gather reliability data. 
The standard further says that the first category should be modeled explicitly in the 
model. 
Assume that a specific cause of a CCF can be identified and defined. By the 
explicit method, this cause of dependency is included into the system logic models, 
for example, as a basic event in the fault tree model as illustrated in Figure 10.2, or 
as a functional block in a reliability block diagram. Examples of causes that may be 
modeled explicitly are: 
- Human errors 
- Utility failures (e.g., electricity, cooling, heating) 
- Environmental events (e.g., earthquake, lightning) 

316 
COMMON-CAUSE FAILURES 
Rest of the 
fault tree 
No signal about high 
pressure from the 
pressure switches 
Independent 
switch failures 
7^\ 
Common tap 
blocked with solids 
No signal from 
pressure switch 1 
No signal from 
pressure switch 2 
Figure 10.2 
Explicit modeling of a common-cause failure in a system of two pressure 
switches (ref. Example 5.8). 
A parallel system of two pressure switches can fail in two different ways, either 
as simultaneous individual failures or due to a common-cause. In this case, the 
common-cause may be that solids block the common tap to the pressure switches. 
Some causes of dependencies are difficult or even impossible to identify and 
model explicitly. These are called residual causes and are catered for in a so-called 
implicit model. The residual causes cover many different root causes and coupling 
factors, such as common manufacturer, common environment, and maintenance er-
rors. There are so many causes that an explicit representation of all of them in a fault 
tree or an event tree would be unmanageable. 
When establishing the implicit model, it is important to remember which causes 
were covered in the explicit model so that they are not counted twice. For small 
system modules it may be possible to use Markov techniques to model both explicit 
and implicit causes. 
Modeling and analysis of CCF as part of a risk or reliability study should, in 
general, comprise at least the following steps (see also Rasmuson, 1991; Johnston, 
1987): 
1. Development of system logic models. This activity comprises system familiar-
ization, system functional failure analysis, and establishment of system logic 
models (e.g., fault trees, reliability block diagrams, and event trees). 
2. Identification of common-cause component groups. The groups of components 
for which the independence assumption is suspected not to be correct are iden-
tified. 

THE BETA-FACTOR MODEL 
317 
3. Identification of root causes and coupling factors. The root causes and coupling 
factors are identified and described for each common-cause component group. 
Suitable tools are checklists and root cause analysis. 
4. Assessment of component defenses. The common-cause component groups are 
evaluated with respect to their defenses against the root causes that were identi-
fied in the previous step. 
5. Explicit modeling. Explicit CCF causes are identified for each common-cause 
component group and included into the system logic model. 
6. Implicit modeling. Residual CCF causes that were not covered in the previ-
ous step are included in an implicit model as discussed later in this section. 
The parameters of this model have to be estimated based on checklists (e.g., 
IEC 61508, part 6) or from available data. 
7. Quantification and interpretation of results. The results from the previous steps 
are merged into an overall assessment of the system. The step also covers im-
portance, uncertainty, and sensitivity analyses and reporting of results. In most 
cases, we are not able to find high-quality input data for the explicitly mod-
eled CCF causes. However, even with low-quality input data, or guesstimates, 
the result is usually more accurate than by including the explicit causes into a 
general (implicit) CCF model. 
The CCF models that are discussed in the rest of this section are limited to cover-
ing implicit causes of CCF. 
10.5 The Beta-Factor Model 
The beta-factor model was introduced by Fleming (1975) and is suggested as a suit-
able CCF model in IEC 61508-6. The idea of the beta-factor model is to split the 
failure rate, λ, for a channel into two parts, one part, λ^'\ covering the individual 
failures of the channel, and another part, λ ^ , covering CCFs. 
A = A(,)+A(C) 
(10.1) 
The beta-factor, β, is introduced as 
ß = — 
(10.2) 
and is the fraction of all the failures of a channel that are common-cause failures. 
The parameter β can also be interpreted as the conditional probability that a failure 
of a channel is in fact a common-cause failure. 
β = Pr (Common-cause failure | Failure of channel) 

318 
COMMON-CAUSE FAILURES 
Figure 10.3 
Part of a Markov model for a voted group of two channels. Only CCF states 
are shown. 
The individual failure rate and the common-cause failure rate can be expressed by 
the total channel failure rate λ and the factor β as 
A(c) = βλ 
λ<'"> = (1-/3)λ 
A consequence of the beta-factor model is that when a CCF occurs, it affects all 
the items of the system, such that we either have individual failures or a total failure 
affecting all items. The beta-factor can be different for the various categories of 
channel failures. Let β denote the CCF rate for DU failures, ß-Q for DD failures, and 
ßs for S failures. 
The overall rate of dangerous CCFs is therefore 
M D U + βΌλΌΌ 
(10.3) 
For systems with an adequate diagnostic system, we usually have that β& < β. 
The two types of dangerous CCFs are illustrated in a part of a Markov model in 
Figure 10.3. The figure is not a complete Markov model since only dangerous CCF 
states are shown. 
The beta-factor model may also be regarded as a shock model where shocks occur 
randomly according to a homogeneous Poisson process with rate X^c\ Each time 
a shock occurs, all the channels of the group fail, irrespective of the status of the 
channels. Each channel may hence fail due to two independent causes; shocks and 
channel-specific (individual) causes. The rate λ ^ is sometimes called the rate of 
individual failures of a channel. 
fl 
EXAMPLE 10.1 loo2 voted group of identical channels 
Consider a group of two identical channels voted loo2 with DU failure rate 
ADU- An external event may occur that causes DU failure in both channels 
of the voted group. This external event can be represented as a "hypothetical" 
component (CCF) that is in series with the rest of the voted group. The voted 
group is illustrated by a reliability block diagram in Figure 10.4. The group is 
proof-tested with proof test interval r and we assume that the proof tests are 
perfect. When using the beta-factor model, the failure rate of component CCF is 
λ^ 
= βλ, while the two channels in the parallel structure in Figure 10.4 may 
be considered as independent with individual failure rate λ^ = (1 — β)λ. 

THE BETA-FACTOR MODEL 
319 
CCF 
Figure 10.4 
Group of two identical channels voted loo2 and a CCF component 
(Example 10.1) 
1 
1 
2 
2 
3 
3 
CCF 
Figure 10.5 
A group of three identical channels voted 2oo3, and a CCF component 
(Example 10.2). 
The PFDavg of the voted group is, by using the results in Table 8.2: 
PFD avg 
[ ( l - l ) A p u r f 
βληυτ 
Indvidual 
2 
CCF 
where the part "Individual" is the PFDavg of a loo2 structure with individual 
i ( 0 
failure rate λ\^υ — (1 — ß)Xvv and the part "CCF" is the PFDavg of the single 
CCF component with failure rate λ (e) 
DU 
jßAou-
fl 
EXAMPLE 10.2 2oo3 voted group of identical channels 
Consider a 2oo3 voted group of identical channels with DU failure rate Arju-
The group is exposed to CCFs that are modeled by a beta-factor model. The 
voted group is illustrated by a reliability block diagram in Figure 10.5. The 
voted group is proof-tested with proof test interval r and we assume that the 
proof tests are perfect. 
The PFDavg of the voted group is, by using the results in Table 8.2: 
PFD, avg 
((1 - β)λΌυτ)2 
+ ßX-D\jT 
Individual 
The CCF part of PFDavg is seen to be the same as in Example 10.1 and is the 
same for all architectures. 
® 
► Remark: For a fixed ß, the rate of CCFs, λ^ 
= βλ, in the beta-factor model is 
seen to increase with the failure rate X. Systems with many failures will hence also 

320 
COMMON-CAUSE FAILURES 
CCF 
(a) 
1 
2 
3 
4 
(b) 
CCF 
Figure 10.6 Two groups of identical channels: (a) voted loo2 and (b) voted loo4 with CCFs 
modeled by the beta-factor model. 
have many CCFs. Because repair and maintenance is often claimed to be a prime 
cause of CCFs, it is relevant to assume that systems requiring a lot of repair also 
have many CCFs. 
φ 
Figure 10.6 shows the reliability block diagrams of two voted groups of identical 
channels that are exposed to CCFs, modeled with the beta-factor model with factor 
β. Group (a) is voted loo2 and group (b) is voted loo4. Assume that the DU failure 
rate of each channel is ADU> that the voted groups are proof-tested with proof test 
interval τ, and that the proof tests are perfect. 
The PFDavg of the two voted groups are (by using the results in Table 8.3) 
p p n 
_ [(1-β)λΌυτ}2 
βλΌυτ 
r r U a V g , a ~ 
- 
|-
D c n 
[ Q - D A D U T ] 4 
βλΌυτ 
r T U a v g ; b SS 
1 
With the data set in Table 7.2, we obtain 
PFDavg>a K 2.072 · 10"5 + 4.380 · 10"4 = 4.587 · 10-4 
7.727 · 10"10 4- 4.380 ■ 10~4 = 4.380 · 10-4 
PFD avg,b 
This means that the PFDavgja is reduced by approximately 4.5% when adding two 
extra channels. The CCF part is responsible for 95.4% of the PFDavgja for the loo2 
structure and very close to 100% of PFDavgib for the loo4 structure. 
For many high-reliability SIS subsystems the use of the beta-factor model results 
in 
JSADUT 
PFDa 
(10.4) 
When using the beta-factor model for high-reliability subsystem, the individual part 
has almost no influence on the PFDavg; everything depends on the CCF part! This 
feature may be considered a major weakness of the beta-factor model as we get 
almost no credit for increased redundancy. 

THE BETA-FACTOR MODEL 
321 
Arithmetic versus geometric average 
Consider a data set {01,02. · · · ,an}. 
The arithmetic average of the data set is ä = £ Σ" = 1
 ai 
1 
The geometric average of the datais α* = (Π/=ι ai)" 
= \/aia2 •••on 
For a data set of two entries: a\ — 1 and 02 = 10, we have 
â = (1 + 10)/2 = 5.5 
and 
a* = Vl · 10 = Λ/ΪΟ«;3.16 
10.5.1 The Beta-Factor Model for Nonidentical Channels 
The original beta-factor was defined for identical channels with the same constant 
failure rate ADU- Many systems are, however, diversified with channels that are 
nonidentical. In this case, it is more difficult to define and interpret the beta-factor. 
An approach that is sometimes used is to define β as a percentage of the geometric 
average (see box) of the failure rates of the various channels of the group (e.g., see 
SINTEF, 2013b). 
fl 
EXAMPLE 10.3 Parallel structure with n nonidentical channels 
Consider a group of n nonidentical channels voted loon (i.e., a parallel struc-
ture). The group is exposed to CCFs and we assume that this can be modeled 
by a beta-factor model. Let λου,; be the DU failure rate of channels i, for 
i = 1,2,...,«. The geometric average of the n DU failure rates is 
VDU 
(10.5) 
The beta-factor β can be determined as a fraction of this average DU failure 
rate ADU and the independent DU failure rate of channel i becomes 
(10.6) 
λ'ου,ί — ^Du,i - ρ"λου 
The survivor function of the voted group is 
Rs(t) = 
n 
\ _ T~\ M _ £-(λ ο υ ,-βλ ο υ)Λ 
ί' = 1 
. 
g-βλ-Όυ' 
This approach may be acceptable when all the DU failure rates are in the same 
order of magnitude. When the DU failure rates are very different, this approach can 
lead to unrealistic results, as illustrated in Example 10.4. 

322 
COMMON-CAUSE FAILURES 
β 
EXAMPLE 10.4 Beta-factor with very different DU failure rates 
Consider a group of two channels voted loo2. The DU failure rate of channel 1 
is λου,ι = 1 · 10-4 per hour, and the DU failure rate of channel 2 is λου,2 = 
1 · 10~8 per hour. The two channels are exposed to CCFs that can be modeled 
by a beta-factor model. The geometric average of the two DU failure rates is, 
according to (10.5), 
ADU = (ADU,IADU,2)1/2 = VlO"4 · 10"8 = 1 · 10"6 per hour 
If we suggest a β of 10%, the CCF rate becomes X^\j = βλ^υ 
= 10~5 per 
hour. This is clearly impossible as the total DU failure rate of the strongest 
channel is λου,2 = 10~8 per hour, and the rate of CCFs of an item can never be 
higher than the total DU failure rate. 
This example shows that the suggested approach cannot be suitable when the 
channels have very different DU failure rates. 
© 
Another problematic issue is illustrated in Example 10.5. 
fl 
EXAMPLE 10.5 2oo3 voted group with different DU failure rates 
Consider a group of three channels voted 2oo3. Channels 1 and 2 are identical, 
with DU failure rate ADU,12 = 5 · 10~7 per hour, and channel 3 is different with 
DU failure rate ADU,3 = 2 · 10~6 per hour (a possible example of such a system 
might be a system of two smoke detectors and one flame detector). 
If the third channel had been of the same type as channels 1 and 2, we would 
have used a beta-factor model with β\2 = 0.10 for the whole group. The third 
channel is, however, different from channels 1 and 2 and the likelihood of a 
CCF involving all the three channels is considered to be very low, such that a 
beta-factor model with βΆ\\ = 0.01 might be suggested. 
Because the group is voted 2oo3, it is sufficient that two channels fail for the 
group to fail. If a CCF involving channels 1 and 2 occurs, the group fails. The 
rate of group CCFs should consequently be 
4u,Syst > 4u,i2 · ßn = 5 · 10-8 per hour 
How this situation should be treated by the approach suggested above is far from 
obvious. 
® 
10.5.2 The C-Factor Model 
The C-factor model was introduced by Evans et al. (1984) and is essentially the 
same model as the beta-factor model but defines the fraction of CCFs in another 
way. In the C-factor model, the CCF rate is defined as A(c) = C · λ*'\ that is, as a 

THE BETA-FACTOR MODEL 
3 2 3 
Figure 10.7 
Markov model for the loo2 voted group in Example 10.6 with CCFs. 
fraction of the individual failure rate λ^'\ The total failure rate may then be written 
as λ = λ^ + C · λ^'\ In this model, the individual failure rate λ ^ is kept constant 
and the CCF rate is added to this rate to give the total failure rate. 
10.5.3 Markov Approach 
The Markov approach is more flexible than the approach that is presented above. 
Several Markov models involving CCFs were studied in Chapter 8 and we suffice by 
illustrating the approach for a loo2 voted group of identical channels. 
fl 
EXAMPLE 10.6 loo2 voted group with common-cause failures 
Consider a loo2 voted group where the channels are exposed to CCFs resulting 
from external shocks that occur with rate X^c\ When a shock occurs, all chan-
nels fail, irrespective of the state of the group. We only consider DU failures 
in this example. The group is proof-tested with proof test interval τ and the 
proof test is supposed to be perfect. After a proof test/repair, the voted group 
is assumed to be as-good-as-new. During the repair action, the group is discon-
nected and cannot perform its SIR When a single DU fault is revealed, the mean 
unknown downtime of this channel in the proof test interval is τ/2 and the mean 
repair time of the failed channel is MRTj. The associated repair rate is then 
μι = 1/(τ/2 + MRTi). When two DU faults are revealed in the proof test, 
the mean unknown downtime in the proof test interval is r/3 (see Chapter 8), 
the mean repair time of two channels is MRT2, and the associated repair rate is 
μ2 — l/( r/3 + MRT2). The individual D failure rate of a channel is denoted 
λ(/) 
The Markov model for the loo2 voted group is shown in Figure 10.7, and the 
transition rate matrix is 
/_ (2λωυ + λ(0) 
2λιου 
λ<ο\ 
A = 
μι 
-(μ,+2λ<& + λ<£>) λ<& + A<c> 
V 
ß2 
0 
- μ 2 / 
The steady-state distribution P — [P0, P\, P2] is determined in the same way 
as in Chapter 8. We replace the last column in the matrix A with the equation 

324 
COMMON-CAUSE FAILURES 
Ρο + Λ + Pi = 1, and get 
/-(2λ«+λ ( β )) 
P \ 
Mi 
V 
μ-2 
The solution is 
Z A D U 
0 
l \ 
1/ 
[0,0,1] 
Λ> = 
Pi = 
Pi = 
μ2{μι + 2Ag{j + A<e>) 
4 ( λο υ ) 2 + 4Ag{jAW + λ^μι 
+ (λΜ)2 + 4μ 2λ^υ + ΜιΜ2 + μ 2 λ « 
ι(0 
4(A D') 2 + 4AD , λ « + λ ^ μ ι + (λ('>)2 + 4μ 2λ 0' + Μ ι μ 2 + μ 2 λ « 
0) 
»('") 
4 ( A £ J ) 2 + 4 A £ U < C > + M I A < C > + (A<C>)2 
4 < υ ) 2 + 4λ^ )
υλω + λΜμι + (AW)2 + 4μ2λ^)
υ + μ ι μ 2 + μ2λ(<:) 
Assume the following input parameters: 
Symbol 
Value 
Comment 
id') 
ADU 
X 
MRTi 
MRT2 
μι 
μι 
5.0 ·10 - 6 
DU failure rate per hour 
3.5 · 10~7 
Shock/CCF rate per hour 
8760 
Proof test interval, hours 
10 
Repair time of one channel, hours 
25 
Repair time of two channels (hazardous event), 
hours 
= T/2 + MRTi 
Total downtime of one channel (i.e., unknown 
downtime plus the repair (hours)) 
= τ/3 + MRT2 
Total downtime for both channels (i.e., unknown 
downtime plus repair time (hours)) 
Using these input values, we obtain 
P 0 % 0.9576 
Pi «a 0.0402 
P 2 % 0.0022 
The loo2 voted group can perform its safety function in states 0 and 1, but is 
not able to perform its SIF in state 2. The PFD a v g for the voted group is 
PFD a v g = P 2 
0.0022 = 2.2 · 10"3 
With the given input data, the total DU failure rate of a channel is ADU 
ADU + A(c) = 5.35 · 10~6 per hour, and the beta-factor is ß = 
A ( C ) / A D U 
0.065. 

THE BETA-FACTOR MODEL 
325 
If we use the standard beta-factor model in this case, we get 
P F D : v g * I d " A W ] 2 + ^ψΐ 
+ (2 - β)λΌυΜΤΓΚ 
In this model, the probability of revealing at least one DU failure in the proof 
test is 
Pr(DU failure revealed) = 1 - 
β-(.2(ΐ-β)+β)λΌυτ 
% (2(1 -β) + β)λΌντ 
= (2 - 
β)λΌντ 
The MTTR is the average repair time when one or two DU failures are revealed 
in the proof test. We assume that MTTR= 15 hours. The average unavailability 
due to repairs after a revealed DU failure is then 
IT 
i K-rt H t 
· 
(2-l)A D UrMTTR 
Unavailability due to repair ss 
= (2 — P)ADUM1 IR 
τ 
The PFD*vg is then 
PFD*vg = 0.00233 
which is slightly higher than the PFDavg we got by the Markov approach. 
It should be noted that the two results PFDavg and PFD*vg are not directly 
comparable as the two approaches do not cover exactly the same situation. It 
should also be noted that the average repair time MTTR =15 hours is not a true 
average but a guesstimate made by the author. It is not a huge problem to derive 
a more exact estimate for MTTR, but the effect of the extra work would not be 
significant. 
φ 
10.5.4 
Plant Specific Beta-factors 
The beta-factor for a voted group of a SIS normally falls within the range from 1 to 
10 percent. The defenses against CCF events that are implemented in a plant affect 
the fraction of CCF events. Estimates of β based on generic data are therefore of 
limited value. 
There are several suggestions for how to choose a more "correct" β, based the 
actual system's vulnerability to possible causes of CCF events. Some of the methods 
that are designed to determine "plant-specific" (application-specific) ß's are briefly 
reviewed in this section. 
Humphreys' Method. One of the first methods to determine a plant specific ß was 
suggested by Humphreys (1987). He identifies eight factors that are considered to be 
important for the actual value of ß (grouped as design, operation, and environment). 
The various factors are weighted as shown in Table 10.1, based on expert judgment 
and discussions amongst reliability engineers. Some other potential factors are not 

326 
COMMON-CAUSE FAILURES 
Table 10.1 
Factors and weights in Humphreys' method (Humphreys, 1987). 
Factor 
Design 
Operation 
Environment 
Subfactor 
Separation 
Similarity 
Complexity 
Analysis 
Procedures 
Training 
Control 
Tests 
a 
2400 
1750 
1750 
1750 
3000 
1500 
1750 
1200 
b 
580 
425 
425 
425 
720 
360 
425 
290 
Weights 
c 
140 
100 
100 
100 
175 
90 
100 
70 
d 
35 
25 
25 
25 
40 
20 
25 
15 
e 
8 
6 
6 
6 
10 
5 
6 
4 
included because they were found to be too difficult to quantify. Each of the chosen 
eight factors is described in the appendix of Humphreys (1987) and classified into 
five categories from a (= worst) to e (= best). For a specific application, the eight 
subfactors are classified into categories a - e, and given weights (score) according 
to Table 10.1. A simple procedure is next used to produce an application-specific ß 
estimate: 
1. The sum of column "a" representing the worst possible case should correspond 
to ß = 0.3, a value that is considered to be a realistic worst-case scenario. 
2. The sum of column "e" representing the best possible case should correspond 
to ß = 0.001 (i.e., it is not realistic to attain any lower value of ß). 
3. To allow the convenience of whole numbers, a divisor of 50 000 was chosen, in 
which case the sum of column "a" should be 15 000, and the sum of column "e" 
should be 50. 
The application specific ß is determined by adding the weights for the chosen 
categories and dividing by 50 000. 
fl 
EXAMPLE 10.7 Identical channels 
Consider a subsystem of identical channels. This subsystem is a worst-case (i.e., 
category "a") with respect to the sub-factor "similarity," and get the weight 1750 
for this factor. If all the other factors are of the best category "e," the total weight 
is 1795, and the estimated value of ß is hence 0.036. This means that the lowest 
possible value of ß for a subsystem of identical channels is 3.6% when we are 
using the method of Humphreys (1987). 
Θ 

THE BETA-FACTOR MODEL 
3 2 7 
Humphreys' method was developed before diagnostic systems were commonly 
used and therefore does not take this issue into account. 
The IEC61508 Method. An alternative method is suggested in IEC 61508-6, An-
nex D. This method is called the IEC 61508 method and is based on a similar idea 
as Humphreys' method. The IEC 61508 method is only applicable for hardware fail-
ures (i.e., not for software faults) and /J-values must be calculated separately for each 
voted group of the SIS. 
The sensor subsystem, the logic solver subsystem, and the final element subsys-
tem are subject to different environmental stresses and usually have different types 
of diagnostic systems, and it is therefore likely that these subsystems have different 
/?-values. 
The IEC 61508 method is carried out by answering 37 predefined questions. The 
questions can be grouped into the following categories: 
1. Physical design (20 questions) 
- Separation/segregation (5) 
- Diversity/redundancy (9) 
- Complexity/design/application/maturity/experience (6) 
2. Analysis (3 questions) 
- Assessment/analysis and feedback of data 
3. Human/operator issues (10 questions) 
- Procedures/human interface (8) 
- Competence/training/safety culture (2) 
4. Environmental issues (4 questions) 
- Environmental control (3) 
- Environmental testing (1) 
Procedure. 
The application-specific beta-factor is found by the following steps: 
1. Choose one of the subsystems of a SIS [i.e., sensor subsystem (S), logic solver 
subsystem (LS), or final element subsystem (FE)]. 
The table in IEC 61508 has two double columns, one for the logic solver, de-
noted LS, and the other for sensors and final elements, denoted SF. In the further 
description, it is assumed that we have chosen sensors, and must therefore use 
the column SF. (The approach is similar for the two other subsystems.) 
2. For each of the 37 questions that is given the answer "yes," we have to choose 
between two cases presented as two columns ^ S F and KSF· If the answer is 

328 
COMMON-CAUSE FAILURES 
"no," no score is given. Column A'SF is chosen if diagnostic tests lead to im-
provement, while column Y$F is chosen if the issue for the question cannot be 
improved by diagnostic testing. The user must for each question decide which 
of the columns is relevant, and only one entry is used in the further analysis. 
For each question, the A'SF and FSF have been given scores, representing their 
effect on the CCFs. The scores are based on engineering judgment and indicate 
the contribution each measure makes in the reduction of the rate of CCFs. The 
ratio A'SF/ YSF for a question represents the benefit of diagnostic testing as a 
defense against CCF related to this particular question. 
Some of the questions do not have scores for A'SF and/or YSF, meaning that this 
question is not relevant for this subsystem. 
3. When all the questions are answered, the A'SF scores are summed for all ques-
tions that have been given A'SF scores. Similarly, all the YSF scores are also 
summed. Let Σ Α ^ and EFSF denote the sum of A'SF scores and the YSF 
scores, respectively. 
4. The β for undetected failures is determined based on the total score 
S = ΣΑ-gp + Σ7 3 Ρ 
(10.7) 
while ßD for detected failures is determined based on the total score 
SO = ΣΧ3¥(Ζ 
+ 1) + ΣΥ8ρ 
(10.8) 
where Z is a factor that is dependent on the frequency and coverage of the 
diagnostic tests and is determined from tables provided in IEC 61508, one table 
for the logic solvers (LS) and one for sensors and final elements (SF). The value 
of Z varies between 0.0 and 2.0 depending on the frequency and the coverage 
of the diagnostic test. 
5. Based on the obtained Z values for S and So, the corresponding values for β 
and ßD, respectively, are given by Table D.4 in IEC 61508, Part 6, Annex D. 
The approach provides a value of β between 0.5% and 5% for the logic solver 
(LS) and between 1% and 10% for the sensors and final elements (SF). 
By this approach, the beta-factor for a logic solver is never higher than 5% and 
never higher than 10% for sensors and final elements. The beta-factor is a step func-
tion of the score S (or (So), as illustrated in Figure 10.8. 
In practice, this means that we can make several improvements to the system 
without being credited with a lower value of the beta-factor. A more rewarding 
approach would therefore be to approximate the step function in Figure 10.8 by a 
smooth curve. 
Several of the 37 questions are ambiguous or difficult to answer even by SIS 
designers (e.g., "Are all devices/components conservatively rated (e.g., by a factor of 
2 or more)?" and "Does the system diagnostic tests report failures to the level of a 
replaceable module?") 

THE BETA-FACTOR MODEL 
329 
ß " 
0.10 
; 
0.05 
' 
; 
0.02 
: 
; 
0.01 
' 
0 
I 
*■ 
0 
20 
40 
60 
80 
100 
120 
g 
Figure 10.8 The beta-factor as a function of the score S as given in Table D.4 in IEC 61508, 
Part 6, Annex D. 
Some questions ask for practices that are not common for SIS design, at least not 
in the oil and gas industry (e.g., "Do the channels employ different technologies, for 
example, one electronic or programmable electronic and the other relay?" and "Are 
separate test methods and people used for each channel during commissioning?") 
The ßu is generally assumed to be 50% of ß due to the advantages of diagnostic 
technology. 
► Remark: 
1. It should be noted that Part 6 of IEC 61508 is only informative and other meth-
ods for determining an application-specific ß can be used. Even if other tech-
niques may be justified, the use of the IEC 61508 method makes sense, espe-
cially when we want to show compliance with IEC 61508. 
2. If a conservative design approach is adopted and the PFDavg requirement is 
fulfilled with the maximum values of ß (i.e., 59c for logic solvers and 10% for 
sensors and final elements), we do not need to use the IEC 61508 method to 
determine the beta-factor. 
0 
The IEC 62061 Method. The IEC 62061 method is based on the IEC 61508 method, 
but significantly simplified. The same categories of questions are used, but the num-
ber of questions is reduced to 14. The questions are given in IEC 62061, Annex 
F. 
Questions that are answered by "yes" are given a score according to Table F. 1 in 
IEC 62061, Annex F. If the answer is "no," no score is given. As opposed to the 
IEC 61508 method, all subsystems are treated in the same way. If another means of 
avoiding CCFs can be achieved by an alternative design measure (e.g., use of opto-
isolated devices rather than shielded cables), the related score can be claimed if this 
alternative provides a similar contribution to the avoidance of CCF. 
When all the questions are answered, the sum S of the scores is calculated and 
ß is determined from Table F.2 of IEC 62061, Annex F. The value of ß ranges from 

3 3 0 
COMMON-CAUSE FAILURES 
1% to 10% according to a similar step function as given in Figure 10.4, but the steps 
occur for different values of the total score S. 
The Unified Partial Method. The unified partial method (UPM) was developed for 
the British nuclear power industry (e.g., see Zitrou & Bedford, 2003) and is based on 
the beta-factor model. In the UPM framework, the defenses against CCFs are broken 
down to eight factors: 
1. Environmental control 
2. Environmental tests 
3. Analysis 
4. Safety culture 
5. Separation 
6. Redundancy and diversity 
7. Understanding 
8. Operator interaction 
The actual system is assigned one out of five possible levels x,;y for/ = 1,2,..., 5, 
across each defense, j = 1,2,..., 8, and scores Sj {xij) are given accordingly from 
generic tables that have been deduced from past research. The overall beta-factor is 
obtained as a scaled sum of these scores. 
The UPM model has been discussed by Zitrou & Bedford (2003); Zitrou et al. 
(2004) where a novel approach based on influence diagrams is suggested for assess-
ment of the plant-specific beta-factor. 
► Remark: By using the approach provided in IEC 61508-6, the greatest value of ß 
is 0.10. This may be the case in strictly controlled environments such as a nuclear 
power plant, but it is the conviction of the author that this value is not realistic in the 
process industry and in the offshore oil and gas industry. He does not have sufficient 
data to document this position, but he would all the same suggest that the largest ß 
should be in the order of 0.25. The PDS project has recently initiated a data collection 
exercise to estimate ß in the offshore oil and gas industry and preliminary findings 
point in the same direction as the author's conviction. If this is correct, the CCFs are 
even more important. 
Θ 
10.6 The Binomial Failure Rate Model 
The binomial failure rate model is suggested as a suitable CCF model in IEC 61508-
6, where it is described briefly in Annex D.7. The binomial failure rate model was 
introduced by Vesely (1977) with the following assumptions. A voted group has 

THE BINOMIAL FAILURE RATE MODEL 
331 
n identical channels. Each channel can fail at a random time, independent of each 
other, and they are all supposed to have the same individual (independent) failure rate 
λ('\ As for the beta-factor model, the failure rate must be specified for the failure 
category (e.g., DD and/or DU) that are of interest. 
The binomial failure rate model is based on the premise that CCFs result from 
shocks to the voted group (Evans et al., 1984). The shocks occur randomly according 
to a homogeneous Poisson process with rate v. Whenever a shock occurs, each of the 
individual channels is assumed to fail with probability p, independent of the states 
of the other channels. The number Z of channels failing as a consequence of the 
shock is thus binomially distributed (n, p). The probability that the multiplicity, Z, 
of failures due to a shock is equal to z is 
P r ( Z = z ) = r j / 7 z ( l - j p ) " - z 
(10.9) 
forz = 0,1,... ,n. 
The mean number of channels that fail in one shock is E(Z) = np. The following 
two conditions are assumed: 
- Shocks and independent failures occur independently of each other. 
- All failures are immediately discovered and repaired, with the repair time being 
negligible. 
As a consequence, the time between independent failures of a channel, in the 
absence of shocks, is exponentially distributed with failure rate λ''\ and the time 
between shocks is exponentially distributed with rate v. The number of independent 
failures in any time period of length to is therefore Poisson distributed with parameter 
λ ^ to , and the number of shocks in any time period of length to is Poisson distributed 
vt0. 
The channel failure rate caused by shocks thus equals pv, and the total failure 
rate of one channel equals 
λ = λ ( 0 + /?ν 
(10.10) 
By using this model, we have to estimate the independent failure rate λ(ί) and the two 
parameters v and p. The parameter v relates to the degree of "stress" on the group, 
while p is a function of the built-in channel protection against external shocks. Note 
that the binomial failure rate model is identical to the beta-factor model when the 
group has only two channels. 
The assumption that the channels fail independently when a shock occurs is a 
rather serious limitation, and this assumption is often not satisfied in practice. The 
problem can, to some extent, be remedied by denning one fraction of the shocks 
as being "lethal" shocks, that is, shocks that automatically cause all the channels to 
fail, that is p = 1. If all the shocks are "lethal," one is back to the beta-factor model. 
Observe that this case p — 1 corresponds to the situation in which there is no built-in 
protection against these shocks. 

3 3 2 
COMMON-CAUSE FAILURES 
Situations where independent failures may occur together with nonlethal as well 
as lethal shocks are often realistic. Such models are, however, rather complicated, 
even if the nonlethal and the lethal shocks occur independently of each other. 
[P EXAMPLE 10.8 2oo3 voted group of identical channels 
Consider a 2oo3 voted group of identical channels. This example considers only 
DU failures and the individual DU failure rate is assumed to be λ ^ = 5.0· 10-6 
per hour. The group is proof-tested with proof test interval τ = 8 760 hours, and 
the proof tests are assumed to be perfect. 
The voted group is exposed to random shocks that occur according to a homo-
geneous Poisson process with rate v that has been estimated to be v = 1.0· 10-5 
per hour. Each time a shock occurs, each channel gets a DU failure with proba-
bility p = 0.20. The channels are assumed to fail independently when a shock 
occurs such that the number Z of channels that fail due to a shock is binomially 
distributed. 
Pr(Z = 0) = Γ U°(l - />)3-° = (1 - ργ = 0.5120 
Pr(Z = 1) = [ \pl(l 
- p)3~l = 3/>(l - p)2 = 0.3840 
Pr(Z = 3 ) = \)p3(l-p)° 
= p3 =0.0080 
The voted group only fails due to shocks when Z = 2 and Z = 3. This means 
that the probability that a shock results in a group DU failure is ps = Pr(Z = 
2) + Pr(Z = 3) = 0.1040. Random shocks giving group DU failures therefore 
occurs according to a homogeneous Poisson process with rate vs = v ■ ps = 
1.04-10"6 per hour. 
If we disregard the possibility of both individual and shock failures in the 
same proof test interval, the PFDavg of the 2oo3 voted group is 
PFDavg = ( λ ^ τ ) 2 + ^j- ss 0.00192 + 0.00455 = 6.47 ■ 10"3 
Observe that shocks can occur without any failures (i.e., Z = 0). This makes it 
difficult to estimate v directly from failure data, because shocks may occur unnoticed 
when no channel fails. 

MULTIPLICITY OF FAULTS 
333 
10.7 Multiplicity of Faults 
10.7.1 
Model Assumption 
Let Z denote the number of failed channels when a multiple failure occurs in a 
voted group of n channels. When using the beta-factor model, Z can only take the 
values 1 and n. Various generalizations are suggested. Either we could assume some 
parametric distribution of Z, say a binomial distribution, or we could allow Z to 
have a more general distribution. 
Considering a voted group with n channels, the following symmetry assumptions 
apply for most parametric CCF models: 
- There is a complete symmetry in the n channels, and each channel has the same 
constant failure rate (i.e., independent of time). Further, all specific combina-
tions where k channels are failing and n — k channels are not failing, have the 
same probability to occur. 
- Removing j of the n channels has no effect on the probabilities of failure of the 
remaining n — j channels. 
These assumptions imply that we do not have to specify completely new param-
eters for each n. The parameters defined to handle CCF for n = 2 are retained for 
n = 3, and so on. 
► Remark: Beckman (1995) claims that the susceptibility to CCF is architecture 
sensitive and postulates that triple-redundant architectures are three times more sen-
sitive to CCF than dual architectures. He does not, however, give any formal justifi-
cation for his assertions. 
Θ 
10.7.2 Probability of a Specific Multiplicity 
A channel that has failed can be a single fault or one fault in a set of multiple faults. 
The number of channels that are simultaneous in fault state is called the multiplic-
ity of the fault. We are interested in studying this multiplicity and its distribution. 
Several measures of the multiplicity can be defined as illustrated below, for a voted 
group of three channels. 
Consider a voted group of three channels of the same type, and let E* denote that 
channel / is functioning and £, that it is failed, for i = 1,2,3. A specific channel, 
say channel 1, can be involved in four disjoint fault scenarios: 
- Channel 1 is failed, as a single fault, that is, £ι Π £J Π £3 
- Channel 1 is, together with channel 2, involved in a double fault, that is, E\ Π 
Ε2ηε; 
- Channel 1 is, together with channel 3, involved in a double fault, that is, E\ Π 
E* n E3 

334 
COMMON-CAUSE FAILURES 
- Channel 1 has, together with channels 2 and 3, a triple fault, that is, £Ί ΠΕ2Γ\Ε3 
Similar expressions can be established for channels 2 and 3. 
Let gic,n denote the probability of a specific combination of functioning and failed 
channels, such that (exactly) k channels are in fault state, and n — k channels are 
functioning. The probability of a specific single (individual) fault in a voted group 
of three channels is 
Si,3 = Pr(£i n E* Π £*) 
= Pr(£* Π E2 Π E*) = Pr(£* Π E* Π E3) 
(10.11) 
The probability of a specific double fault is 
g2,3 = Pr(£i Π E2 ΓΊ E*) 
= Pr(£i i l i 2 ' n E3) = Pr(£* Π Ε2 Π £3) 
(10.12) 
and the probability of a triple fault is 
g3,3 = Pr(Ei Π E2 Π £3) 
(10.13) 
Let β^:3 be the probability that a voted group of three identical channels has a (un-
specified) fault with multiplicity k, for k = 1,2,3. Faults of multiplicity one and 
two can occur in three different ways. 
(10.14) 
g3,3 = g3,3 
Conditional Probability of a Specific Multiplicity. Assume that we observe (test) 
a channel and find the channel in a fault state. Without loss of generality, we can 
assume that this is channel 1. Let Q denote the probability of this event. When such 
a fault is observed, we know that the multiplicity of faults is either 1,2, or 3. Let f^T, 
be the conditional probability that the failure has multiplicity k when we know that 
a specific channel has failed, for k = 1,2,3. For a triple fault, the fault of channel 1 
is included in the triple fault, and we have 
t 
n/r. n r 
n r i c i 
Pr(£i n £ 2 n £3) 
g3,3 
/ 3 ) 3 = Pr(£i Π Ε2 Π £3 I £ 0 = 
= — - 
(10.15) 
Pr(£i) 
Ô 
For a double fault, the fault of channel 1 is included in two of the three possible 
fault combinations in (10.12). By using the same argument as above, the conditional 
probability of a double fault involving channels 1 and 2 is 
f (1,2) _ 
J2,3 
Λ.3 
- 
Q 

THE MULTIPLE BETA-FACTOR MODEL 
335 
and the conditional probability of a double fault involving channels 1 and 3 is 
,.(1,3) _ 
g2,3 
Λ.3 
- 
Q 
The superscripts in f2:3' 
and / 2 3' 
indicate which channels are involved in the 
fault. 
The conditional probability of a double fault involving channel 1 and one of the 
other channels is 
For a single fault, the fault of channel 1 is included in only one fault combination in 
(10.11). The conditional probability that the fault of channel 1 is a single fault is 
/i,3 = η^ 
(10.17) 
Similar formulas can be established for voted groups of a general number n of iden-
tical channels. 
fl 
EXAMPLE 10.9 Beta-factor model 
When a failure occurs in a voted group of n channels, the beta-factor model 
requires that the multiplicity of the fault is either one or n. Intermediate values 
of the multiplicity are not possible. The conditional probabilities of the possible 
multiplicities are 
fk,n = 0 
fn,n = β 
for k = 2,3,...,n 
— I. 
This is illustrated in Figure 10.9 for a voted group of three identical channels, 
where £,· denotes failure of channel i, for i = 1,2,3. Assume that we have 
observed (tested) one of the channels, say channel 1, and found this channel to 
be in a failed state. We are hence within the circle £Ί and the numbers 0, β, and 
1 — ß in the circle indicate the conditional probabilities of involvement of the 
other channels in the failure. (1 — β) is the probability that the observed failure 
is a single (individual) failure, β is the probability that the observed failure is a 
complete failure with multiplicity three, and the zeros indicate that failures with 
multiplicity two are not possible. 
φ 
10.8 The Multiple Beta-Factor Model 
The multiple beta-factor model was developed as part of the PDS method (Hokstad 
& Corneliussen, 2004) and is still an integral part of this method (SINTEF, 2013b). 

3 3 6 
COMMON-CAUSE FAILURES 
Figure 10.9 Fractions of different multiplicities of failures for a voted group of three identical 
channels when using the beta-factor model. 
In the multiple beta-factor model, the probability that a group, voted koon, fails due 
to a common-cause, is given by 
Qkoon = Ckoon ■ ßQ 
(10.18) 
where Q is the probability that a channel is in a fault state, Ckoon is a correction 
factor that depends on the voting of the group, and ß is the conditional probability of 
exactly one extra failure when we know that one channel has failed. The parameter 
ß neither depends on the number n of channels nor on the configuration of the group. 
When n = 2, the parameter ß has the same interpretation as in the beta-factor model. 
interpretation of the Model in (10.18). Consider a koon voted group of identical 
channels, and assume that it is observed that a channel has a DU fault. Because this 
fault has been observed, the probability Q is equal to one. The parameter ß is the 
probability that we will find exactly one additional DU fault in the group. If we know 
that there are at least one additional DU fault, we may ask: What is the (conditional) 
probability that the koon voted group has a DU fault? The answer to this question is 
*~*koon' 
A loo2 voted group fails when there is one extra failure (in addition to the one 
we initially observed) and the correction factor is therefore C\002 = 1. 
A 2oo3 voted group fails when there is exactly one extra failure (in addition to 
the one we initially observed). In this case, there are two channels that can have the 
additional DU fault, and it can therefore be argued that the correction factor should 
be C2oo3 = 2. 
We now illustrate the multiple beta-factor model for a voted group of three iden-
tical channels. 
Voted Group of Three Identical Channels. 
Consider a voted group of three iden-
tical channels, and let E* denote that channel / is functioning, and £,· that channel 
i is in a dangerous failed state. Assume that we observe (test) one of the channels, 
chosen at random, and find that this channel is in a failed state. Let Q denote the 
unconditional probability of this event, that is, with no regard to the state of the other 

THE MULTIPLE BETA-FACTOR MODEL 
337 
channels. Without loss of generality, we can assume that the failed channel is chan-
nel 1. Let ß be the probability that a second channel (channel 2 or 3) is also failed. 
Further, let ß2 denote the conditional probability that the third channel fails, when 
we know that two channels have failed. 
The probability of a triple failure is from equation (10.13): 
g3,3 = Pr(£i n E2 ΓΊ E3) 
= Pr(£ 3 I E1 Π E2) ■ Pr(£ 2 | Ex) ■ Pr(£i) 
= 
ßiß-Q 
When we observe a channel (channel 1) and find it in a failed state, the conditional 
probability that the fault is a triple fault is 
/3>3 = Pr(£i ΓΊ E2 n E3 | Ei) = ^ 
= ß2ß 
(10.19) 
The probability of a specific double fault (e.g., fault of channels 1 and 2) is from 
equation (10.12): 
g2>3 = Pr(El Π E2 Π E*) 
= Pr(£* | E1 Π E2) · Pr(£ 2 | Ex) ■ Pr(£0 
= 
V-ßi)ßQ 
Because the channels are of the same type, we get the same result for all three com-
binations of double faults. 
When a channel (channel 1) has failed, the conditional probability that the fault is 
a double fault involving channels 1 and 2 is 
-(1,2) _ 
g2,3 
Q 
/2,3 
= ^ T = 
Q-ß2)ß 
and, similarly, the conditional probability that the fault is a double fault involving 
channels 1 and 3 is 
r-0,3) _ 
S2,3 
Q 
The conditional probability that the fault of channel 1 is a double fault involving one 
of the other channels is 
fir = *w- = Q-ß2)ß 
Al) _ 
f(l,2) 
,(1,3) _ 7 n 
a Λο 
h,3 - Λ.3 
+ /2,3 
- 2(l - ßl)ß 
The probability of a double fault without specifying which channels are involved 
is 
f 
_ 
f(1.2) , 
f(l,3) 
,-(2,3) _ 
%n 
o 
Λο 
/2,3 - 
/ 2 , 3 
+ /2,3 
+ J2,3 
~ ^ 1 ~ 
ß2)ß 
When a channel is found to be in a failed state, the multiplicity of faults must be 
either 1, 2, or 3, and we have /i > 3 + /2,3 + /3,3 = 1· The conditional probability 
that the fault of channel 1 is a single (individual) fault is thus 
/l,3 = 1 - /2,3 - /3,3 = 1 - (2 - ß2)ß 

3 3 8 
COMMON-CAUSE FAILURES 
Figure 10.10 
Fractions of different multiplicities of failures when using the multiple beta-
factor model for a voted group of three identical channels (Reproduced from Rausand (2011) 
with permission from John Wiley & Sons). 
When a channel fault is observed, the fractions of the possible multiplicity of 
faults are 
fi,3 = 
l-(2-ß2)ß 
/2,3 = 3(l-j82)j8 
(10.20) 
/3,3 = ßlß 
These fractions are illustrated in Figure 10.10. 
Let Qk.3 be the probability that a voted group of three identical channels has a 
(unspecified) fault with multiplicity k, for k = 1,2,3. As faults of multiplicity one 
and two can occur with three different combinations of channels, we get 
Qv.3 = 
3(l-(2-ß2)ß)-Q 
02:3 = 3(1 - ß2)ß ■ Q 
(10.21) 
03:3 
=ßlß-Q 
Possible Configurations of Three Channels. A group of three channels can have 
three different votings; a loo3 (parallel) voting, a 2oo3 voting, or as a 3oo3 (series) 
voting. 
A loo3 (parallel) voted group is functioning as long as at least one channel is 
functioning and fails only when all the channels fail. The probability of fault of a 
loo3 voted group is therefore 
Qloo3 = Q3:3=ß2ß-Q 
(10.22) 
A 2oo3 voted group is functioning as long as at least two of its three channels 
are functioning. The group fails when at least two of its three channels fail. The 
probability of fault of a 2oo3 voted group is 
Ô2oo3 = 02:3 + 03:3 = (3 - 2ß2)ß ■ Q 
(10.23) 
A 3oo3 (series) voted group fails as soon as one of its three channels fails, and the 
probability of fault of the 3oo3 voted group is 
Ô3oo3 = 01:3 + 02:3 + ß3:3 = (3 - (3 - ß2)ß) · Q 
(10.24) 

THE MULTIPLE BETA-FACTOR MODEL 
339 
The multiple beta-factor model was given by Qkoon = Qoon -ßQ ■ By comparing 
with the results above, we get the configuration factors 
Cloo3 = ßl 
C2003 = 3-2,ß 2 
(10.25) 
Voted Group of n Identical Channels. The approach presented above for a voted 
group of three channels can be extended to a voted group of n identical channels. By 
increasing the number of channels from m to m -I-1, we each time have to introduce 
a new parameter ßm, where ßm is the conditional probability of m + 1 faults when 
m channels have failed. This can be expressed as 
ßm = Pr(£i n E2 n · · · n Em+i | Ex n E2 n · · ■ n Em) 
(10.26) 
The probability that exactly m specified channels have failed out of n channels is 
gm>n = Pr(£! n E2 n · · · Em n Em+l n · · · n E*) 
(10.27) 
Due to the symmetry assumption in Section 10.7.1, we can interchange subscripts as 
long as there are m events £,· and n—m events E*. The probability of exactly m of 
the n channels being in a failed state is 
Qm:n = [\gm,n 
(10.28) 
By determining gm,n, we also get an expression for 
n 
Qkoon = Pr(At least n-k+l 
failed) = 
£ 
Qm:n 
(10.29) 
m=n— k+l 
We have defined Qkoon = Q:oo« -ßQ and can therefore use (10.29) to find a general 
expression for the configuration factor C^oon. 
To determine gmy„, observe that by direct argument 
gn,n = I Π ßj ) ■ Q 
(103°) 
From the symmetry assumption and the law of total probability, it follows that 
gm,n-i - gm,n + gm+ι,η, and we may hence find gm,n recursively from 
gm,n = gm,n-l 
- gm+l,n 
form 
= 1 , 2 , . . . , « 
(10.31) 
Starting with g2,2 = ß\Q (wnereßi = /J)andglj2 = (1— βι) Q, and then using the 
above expression for gn >n, we recursively find g„ _ ι <n, g„ _2 ιΠ,... for n = 3,4,5, 
Thus, we find the explicit expression 
gm 
n-m 
/ _ 
\ m _ 1+' 
,n = Ô - E ( - 1 ) T " m 
Π ßJ 
(10·32) 
<=0 
V ' 
/ 
j = \ 

3 4 0 
COMMON-CAUSE FAILURES 
In equation (10.32) the term Q ■ Π7=ι ßj ' s a c o m m o n factor. So for m > 2 we 
have ßQ, as a common factor, and introduce Gm>n = gm,n/ßQ- Thus 
n—m 
I 
\ m—l+i 
G ^ 
= j i - E ( -
i y [ i j 
0 
fr 
fi»m 
= 2,3,...,ii 
(10.33) 
and by (10.28), (10.29), and (10.33) it follows that 
Ckoon= 
Σ 
Γ W « 
for£ = l , 2 , . . . , n - l 
(10.34) 
m=n-k+\ \ 
/ 
All configuration factors may now be explicitly expressed by the ßj 's. 
10.8.1 The PDS Method 
The CCF model that is integrated into the PDS method (SINTEF, 2013b) is based on 
the multiple beta-factor model as described above. The PFDavg of a koon structure 
is determined as a sum of the P F D ^ due to individual failures and the PFD^y 
due to common-cause failures. In the PDS method, the CCF part is determined as 
PFD^ 
= 
Ckoon-ß.^ 
The value of ß that is used in the PDS method is usually taken to be the same 
as the ß used in the beta-factor model and is most often estimated by the approach 
provided in IEC 61508-6. The correction factor C^oon is estimated based on expert 
judgment and following a procedure outlined in Appendix B of SINTEF (2013b). 
The interested reader can consult this report. The resulting correction factors are 
presented in Table 8.8. 
10.9 CCF Modeling with Petri Nets 
Modeling of CCFs with the Petri net approach is not covered in this chapter, but 
some simple examples are shown in Chapter 8. For further information, the reader 
may study Annex M of ISO/DTR 12489 (2012). 
10.10 CCFs Between Groups and Subsystems 
A voted group is a set of identical (or similar) channels. Examples of voted groups 
are (1) a 2oo3 voted group of pressure transmitters and (2) a loo2 voted group of 
level transmitters. The methods described in this chapter are mainly focused on 
CCFs within a single group. 

ADDITIONAL READING 
341 
10.10.1 CCFs Between Voted Groups 
A subsystem of a safety loop (or a SIS) may sometimes have more than one voted 
group. An example is a shutdown function (SIF) on a pressure vessel, with a sensor 
subsystem of both pressure transmitters [group 1] and level transmitters [group 2]. 
These two groups may be configured either with loo2 voting or with 2oo2 voting. 
An intuitive approach would be to use the beta-factor model (or the PDS model) 
for each voted group, and determine factors ß\ and ß2 for group 1 and group 2, 
respectively, and thereafter, to determine a beta-factor ß\2 to model possible CCFs 
between the two voted groups. The two types of beta-factors are sometimes referred 
to as "inner" (i.e., within voted groups) and "outer" (i.e., between voted groups) 
beta-factors. 
A problem with this approach is that even if all channels have constant failure 
rates, the voted groups will generally not have constant failure rates. This means that 
a main assumption of the beta-factor model is not fulfilled. 
This issue is not discussed any further in this book. 
10.10.2 CCFs Between Subsystems 
The three main subsystems of a safety loop (or a SIS) may also be exposed to CCFs. 
The three subsystems are generally set up as a series structure. 
Consider a series structure of two identical items with constant failure rate λ. The 
items are exposed to CCF that is modeled by a beta-factor model with factor β. Since 
the failure rate of a series structure is the sum of the failure rates, the failure rate of 
the series structure is 
As = 2(1 - β)λ + βλ = 
2λ-βλ 
This means that a series structure that is exposed to CCFs has a lower failure rate, 
and a higher reliability, than a series structure of independent items, when using the 
beta-factor model. This also means that assuming independence gives a conservative 
result for series structures. 
This argument cannot be directly transferred to a series of subsystems, since 
the beta-factor model does not easily apply to non-identical subsystems with non-
constant and different failure rate functions. 
10.11 Additional Reading 
The following titles are recommended for further study related to Chapter 10: 
- Guidelines on Modeling Common-Cause Failures in Probabilistic Risk Assess-
ment (NUREG/CR-5485, 1998). 
- Probabilistic risk assessment procedures guide for NASA managers and prac-
titioners (Stamatelatos et al., 2002). Chapter 10 of this guideline gives a good 
survey of available CCF models. 

3 4 2 
COMMON-CAUSE FAILURES 
- Procedures for Treating Common-Cause Failures in Safety and Reliability Stud-
ies, volume 2: Analytical Background and Techniques. (NUREG/CR-4780, 
1989). 
- Common-Cause Failure Database and Analysis System: Event Data Collection, 
Classification, and Coding (NUREG/CR-6268, 2007). 
- Risk Analysis in Engineering: Techniques, Tools, and Trends (Modarres, 2006). 
Section 3.5 of this book gives a survey of models for CCFs. 
- Common-cause failure modeling: Status and trends (Hokstad & Rausand, 2008) 
gives a survey of CCF models, similar to this chapter, but also presents ideas on 
how to estimate the parameters of the models. 

CHAPTER 11 
IMPERFECT PROOF-TESTING 
11.1 
Introduction 
So far, it has been assumed that proof tests and the associated repair actions are 
perfect in the sense that 
1. The proof test is carried out under conditions that are identical with and covers 
all relevant demand conditions. 
2. All DU faults and all element faults that increase the likelihood of a DU fault 
are revealed by the proof test. 
3. All channels with a (revealed) DU fault are repaired and all channels are always 
restarted in an as-good-as-new condition. 
This is obviously not always realistic. Some demands are hazardous events and may 
occur in many different ways. To simulate a demand may also be hazardous and may 
need to be repeated several times to cover all aspects of the demand. Proof tests are 
often carried out under conditions that are different from real demand conditions and 
may, therefore, not be fully realistic. 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
343 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

3 4 4 
IMPERFECT PROOF-TESTING 
Some channels have redundant elements, such as a fail-safe shutdown valve with 
two redundant pilot valves. A function test is therefore not sufficient to verify that 
all elements are functioning. The third assumption about restarting in an as-good-
as-new condition presupposes that human errors are nonexistent and that there is no 
deterioration of the channels. 
The concepts of imperfect and partial proof-testing were introduced and discussed 
briefly in Chapter 4. A full proof test is a proof test that has been designed and 
planned to test for all dangerous failure modes and all element failures, under realistic 
demand conditions. A partial proof test, on the other hand, has been designed and 
planned to reveal one or more specific failure modes. Both tests are imperfect when 
they are not able to reveal the faults they were planned to reveal. 
This chapter does not distinguish between imperfect and partial proof-testing, but 
refers to both as imperfect proof-testing. This chapter suggests how imperfect proof-
testing can be interpreted and be included in reliability models. Several modeling 
approaches are outlined. 
11.2 Proof Test Coverage 
The concept proof test coverage was introduced in Chapter 4. As before, let A DU 
be the rate of DU failures. A proof test is carried out to reveal latent DU faults and 
element faults that can increase the likelihood of DU failures, in order to prevent a 
dangerous failure in a demand situation. In the following, the specific reference to 
element faults is skipped. 
A proof test is said to be perfect and have 100% coverage if it is able to reveal 
all the DU faults. In practice, many proof tests are not perfect and hence not able 
to "cover" all possible DU faults. The DU faults can therefore be divided into two 
types: 
(a) DU faults that can be revealed by the proof test (r-faults). 
(b) DU faults that cannot be revealed by the proof test (nr-faults). 
Let AQU be the rate of DU failures that can be revealed by proof-testing (i.e., that 
leads to r-faults), and let Ag1^ be the rate of DU failures that cannot be revealed by 
proof-testing, such that ADU = Ag"{j + ^u\]· 
The proof test coverage, PTC, is defined as 
A(r) 
PTC=-J2U- 
( H ! ) 
ADU 
PTC is therefore the fraction of all DU faults that are revealed by a proof test. 
It should be noted that the PTC is related to only DU faults resulting from random 
hardware failures and does not cover systematic faults. 

SPLITTING THE FAILURE RATE 
3 4 5 
The rate of r-failures and nr-failures can be expressed by the PTC and the DU 
failure rate. 
A ^ P T C - A D U 
(11.2) 
A ^ = (1-PTC)-A D U 
(11.3) 
A proof test with a PTC < 100% is said to be an imperfect proof test or a non-
perfect proof test. The lower the PTC, the more imperfect is the proof test. The PTC 
can therefore be used as a measure of the quality of the proof test. 
► Remark: Observe that detection of redundant channel element faults is not in-
cluded in the diagnostic coverage. 
Θ 
Based on a careful evaluation of the channels and the available testing methods, 
SINTEF (2013a) presents estimates of the PTC and the diagnostic coverage for dif-
ferent types of channels. Most of these estimates are in the order of 50-60% and 
several channels have PTC in the order of 10-15%. These estimates are not abso-
lutely correct, but give an indication of the order of magnitude and show clearly that 
the assumption of perfect testing is not always realistic. 
There are many reasons why a proof test may not be perfect and among these are: 
- Test equipment is not adequate 
- Test equipment is not calibrated 
- Test conditions are different from real demand conditions 
- Lack of training and competence of test crew 
- Test procedures are not followed or procedures are inadequate 
Imperfect proof-testing is discussed briefly in IEC 61508 (2010, Part 6, B.3.2.5). 
► Remark: As defined in IEC 61508, ADu is the rate of random hardware DU fail-
ures. Some analysts argue, however, that the PTC should be related to the total rate 
of DU faults that may be revealed in a real demand, including systematic faults that 
will be manifested in a demand situation. 
® 
11.3 Splitting the Failure Rate 
A common approach to model imperfect testing of a channel is to distinguish be-
tween r-failures and nr-failures as done in Section 11.2. This way, the channel can 
be considered as a series structure of two virtual and independent items 1 and 2, 
where item 1 can only have DU faults that are revealed by proof-testing (i.e., with 

3 4 6 
IMPERFECT PROOF-TESTING 
1 
1 
1 
1 
[ 
r-faults 
1 
n r-faults 
2 
l 
_ 
Figure 11.1 A channel with revealable (r) and nonrevealable (nr) faults modeled as a series 
structure. 
PFD(t) 
PFD(t) 
5τ 
Time 
Figure 11.2 
The PFD(f ) of a channel with imperfect proof-testing, modeled by splitting the 
DU failure rate. 
PTC = 100%) and item 2 can only have DU faults that cannot be revealed by proof-
testing. The series structure is illustrated in Figure 11.1. The time-dependent proba-
bility of failure on demand, PFD(i), of the channel can be written as 
PFD(f) = PFD(r)(i) + PFD(nr)(i) 
(11.4) 
and has the shape illustrated in Figure 11.2. 
PFD^(r) j s s e e n t 0 ^ 3 ν β me s a m e shape as in Chapter 8, while PFD^nr^(?) in-
creases with /. 
PFD(nr)(f) = 1 - β- λί?ύ·' 
The fully drawn curved lines in Figure 11.2 illustrate the shape of the resulting 
PFD(i) for the channel. It is seen that the PFD(r) has an increasing trend and it 
is therefore obvious that the average, PFDavg, may not be an adequate measure of 
the safety integrity. If PFD^nr^(i) is allowed to continue to increase, the PFD(i) will, 
after a while, "move out" of the specified SIL range. 
In some cases, it is technically impossible to carry out a proof test that can reveal 
all DU faults (e.g., see Example 11.1). In other cases, a perfect proof test would 
imply too high cost or too high risk and is therefore skipped. 
Even if an nr-fault is not revealed by proof-testing, it may be revealed when: (i) 
an r-fault is revealed in a proof test and repaired, (ii) a demand for the channel's 
safety function occurs, or (iii) other operational actions are performed (e.g., preven-
tive maintenance or overhaul). 

SPLITTING THE FAILURE RATE 
347 
PFD(t) 
Overhaul 
Figure 11.3 
PFD(i) with imperfect proof-testing and overhaul. 
fl 
EXAMPLE 11.1 Shutdown valve 
Consider a shutdown valve in a subsea pipeline. A DU failure may occur with 
two failure modes "fail to close on command" (FTC) and "leakage in closed 
position" (LCP). To test for LCP, the valve must be closed and the pressure 
build-up on the downstream side of the valve must be monitored. In cases where 
it is not possible to isolate a relatively small volume on the downstream side, it 
is not feasible to carry out a realistic LCP-test and an LCP-fault is therefore a 
nonrevealed DU fault. The likelihood of such an LCP-fault naturally increases 
with time. The model in Figure 11.1 may therefore be a realistic model. 
Θ 
11.3.1 Splitted Failure Rate Model with Overhaul 
Consider the same model as above but assume that a complete overhaul of the item 
is carried out with interval r. In practice, the overhaul interval is often equal to a 
specific number (m) of proof test intervals, such that r = mx. We assume that 
the overhaul is perfect such that all DU faults are revealed and repaired as part of 
the overhaul, and consider some simple architectures. This situation is illustrated in 
Figure 11.3. 
Single Channel. When considering only DU failures, a single channel ( 1 oo 1 ) with 
imperfect proof-testing and overhaul, the PFDavg is determined from (11.4). 
PFDavg = PFD«g 
PFD (nr) 
P T C - A D U T 
| (l-PTC)A D Ur 
(11.5) 
This formula is also used by the PDS method. Note that the repair-time after a 
revealed fault is disregarded in this formula. 
IEC Formula for a Single Channel. 
By using the IEC formula, both DU and DD 
failures can be incorporated. By using the same notation as earlier, we have AD = 
ADU + ADD- The approach to develop the IEC formulas is presented in Section 8.4. 

348 
IMPERFECT PROOF-TESTING 
The rate (frequency) of channel D-failures is 
AD,G = AD 
For a single channel, the channel mean equivalent downtime is equal to the group 
mean equivalent downtime. 
](') 
/ T 
. 
i(nr) 
/= 
\ 
I 
iGE = tcE = Ùm (I + MRT) + ψ- ( T-+ MRT ) + ψ.. MTTR 
AD 
V^ 
' 
AD ( 2 
/ 
AD 
This formula can be explained as follows: When a D-failure occurs, it is either 
an r-failure, an nr-failure, or a DD failure. The conditional probabilities (given that 
a D-failure has occurred) of these failures are A ^ / A D , 
ADUMD> and ADD/AD, 
respectively. When an r-failure occurs, the mean downtime of the channel is τ/2 + 
MRT, when it is an nr-failure, the mean downtime is τ/2 + MRT, and if it is a DD 
failure, the mean downtime is MTTR. 
By the IEC formula, the PFDavg is equal to 
PFDavg % AD,G · ^GE 
% ADU (I + M R T) + λτ>υ (I + M R T ) + ADDMTTR 
(11.6) 
By introducing the proof teat coverage, PTC, (11.6) can be written 
PFDavg % PTC · ADU (^ + MRT) + (1 - PTC)ADU (j + MRT^ + ADDMTTR 
If DD failures are disregarded (i.e., ADD = 0) and the mean repair times are 
negligible (i.e., MTTR and MRT are s» 0), the IEC formula for a single channel is 
identical to (11.5). 
fl 
EXAMPLE 11.2 PFDavg for a single channel with different PTCs 
Consider a single channel with imperfect proof-testing and overhaul. The aim of 
this example is to study the effect of the PTC on the PFDavg in (11.6). For this 
purpose, we use the input data in Table 7.2 and calculate the PFDavg for some 
selected values of PTC. The overhaul interval is r = 5τ. The results, given in 
Table 11.1, show that the PFDavg = 8.83 · 10~4 when the proof tests are perfect 
(i.e., when PTC = 100%) and 2.28 ■ 10-3 when PTC = 60%. 
Θ 
Group of n Independent and Identical Channels Voted noon. 
An noon voted 
group of n independent and identical channels with constant DU failure rate with 
imperfect proof-testing and overhaul, may be considered as a "super-component" 
with DU failure rate «ADU- Again, we disregard DD failures and assume that the 

SPLITTING THE FAILURE RATE 
3 4 9 
Table 11.1 
The PFDavg of a single channel for different values of the PTC. 
PTC 
PFDavg 
100% 
8.83 · 10""4 
90% 
1.23-10"3 
80% 
1.58 · 10"3 
60% 
2.28 · 10~3 
40% 
2.99· 10"3 
20% 
3.69· 10~3 
0% 
4.39 · 10-3 
mean repair times are negligible. The DU faults can be split into r-faults and nr-
faults as in Section 11.2, such that 
ηλΌυ = ηλ$υ + ηλ(£$ 
In the same way as for the single channel, the PFDavg of the noon architecture is 
PFDavg = PFD«g + PFD<$ 
^ P T C - W A D U T 
(l-PTC)nA D Ur 
~ 
2 
2 
This formula is also used by the PDS method. The IEC formula for the noon voted 
group can be derived in the same way as for a single channel by multiplying the 
failure rates by n. 
Group of 2 Identical Channels Voted 1oo2. We use the IEC approach to derive the 
formula for PFDavg for a loo2 voted group of two identical channels with imperfect 
repair and overhaul. The group can get a D-fault in five different ways. 
1. Two DU faults due to a common cause (modeled with beta-factor ß) 
2. Two DD faults due to a common cause (modeled with beta-factor β^, 
3. Two independent (DU) r-faults in the same proof test interval (of length r) 
4. Two independent (DU) nr-faults in the same overhaul interval (of length r 
5. One independent (DU) r-fault and one independent (DU) nr-fault in the same 
proof test interval 
If the two channels are independent, the frequency of group D-failures is accord-
ing to (8.37) 
ADjG % 2(AD)2iCE 

350 
IMPERFECT PROOF-TESTING 
where ÎQE is the channel mean equivalent downtime. 
i <r) 
,., 
. 
i ( n r) 
ÎCE = ψ± C- + MRT) + ψ - ( I + MRT ) + ^ M T T R 
AD ^2 
/ 
AD \ 2 
/ 
AD 
The group mean equivalent downtime is 
T (r) 
-i (nr) s ~ 
\ 
1 
*GE = ^ 
(\ +MRT) + ^ u ( I +MRT) + ^ M T T R 
AD ^3 
' 
AD \ J 
/ 
AD 
According to the IEC formulas, the PFDavg of a loo2 voted group of two inde-
pendent and identical channels is 
PFDavg = AD>G · rGE = 2(AD)2 · icE · ÎGE 
(11.8) 
Assume now that the channels are exposed to CCFs and that these are modeled by 
a beta-factor model with β for DU failures and ρΌ for DD failures. When using the 
beta-factor model, the group can be represented as a series structure of three items. 
(a) A parallel structure of two independent channels with failure rates representing 
the individual failures. The failure rates (1 — /5)ADU f°r DU failures and (1 — 
POMDD for DD failures. 
(b) A virtual CCF-item representing the DU-CCFs with failure rate βλου· 
(c) A virtual CCF-item representing the DD-CCFs with failure rate PO^DD-
In Chapter 8, it was shown that the PFDavg of a series structure is the sum of the 
PFDavg for the individual items. By introducing PTC, such that λ^υ — PTCADU 
and λ ^ = (1 - PTC)ADu, and by using (11.6) and (11.8), we get 
PFDavg = 2 [(1 - p-)ADu + (1 - βο)λΏΏ)2 
■ tCE · ÎGE 
+ PTC · βλΌυ Q + MRT) + (1 - PTQpOAou (^ + ΜΚτλ 
+ £ DADDMTTR] 
(11.9) 
In this derivation, it is assumed that it is not possible to have a CCF involving one 
r-failure and one nr-failure. A CCF is always either two r-failures or two nr-failures. 
We may rearrange (11.9) into three contributions to PFDavg. 
1. PFD^g; the contribution from independent DU and DD failures 
2. PFD£)g; the contribution from DU-CCF failures 
3. PFD^)g; the contribution from DD-CCF failures 
such that 
PFDavg = FFD<» + PFD£>g + PFD« 
This is illustrated in Example 11.3. 

SPLITTING THE FAILURE RATE 
351 
fl 
EXAMPLE 11.3 loo2 voted group with imperfect repair and overhaul 
Consider a loo2 voted group of two identical channels with imperfect repair and 
overhaul. The example is made to illustrate the quantities of the formulas above. 
For this purpose, we use the input data in Table 7.2 and assume the the overhaul 
interval is r = 5τ. With this data set, we obtain 
fCE = 7.543 
iGE = 5.032 
AD,G = 1.509 
P F D ^ = 6.607 
PFD^)g = 2.806 
PFD^g = 5.600 
103 hours 
103 hours 
10~8 hours-1 for independent channels 
10~5 withCCFs 
10-4 
withCCFs 
10-7 withCCFs 
The PFDavg of the loo2 group is 
PFDavg = PFDiVg + PFD^)g + PFD<3)
g = 3.482 · 10~4 
(11.10) 
19.3% 
80.6% 
0.1% 
It is seen that independent failures contribute 19% to the total PFDavg while 
DU-CCFs contribute 80.6%. The contribution from DD-CCFs is seen to be 
negligible with the data in Table 7.2. 
® 
Group of n Identical Channels Voted koon. 
To determine the PFDavg of a general 
koon voted group of identical channels with imperfect proof-testing and overhaul 
can be done by using the same approach as for a loo2 voted group. The derivation 
is, however, lengthy and is therefore not included here. The PDS method (SINTEF, 
2013b) suggests to approximate the PFDavg by using only the DU-CCF-contribution 
(i.e., PFD<2)g in Example 11.2). 
PFD a v g« 
^ J ^ + ^ 
" 
DU 
(ii.ii) 
Because the PDS method applies the multiple beta-factor model for CCFs (see 
Chapter 10), the following expression is used. 
p F D(PDS) % P T C ■ CfcoonftApuT 
(1 - 
PTC)CkoonßXOUT 
As indicated in Example 11.2, this approximation is rather nonconservative, which 
is also pointed out in SINTEF (2013b). 

352 
IMPERFECT PROOF-TESTING 
11.3.2 Partial Stroke Testing 
Partial stroke testing (PST) is sometimes used as a supplement to full proof-testing 
of safety valves. PST means to partially open or close a valve and return the valve 
to the initial position. A small valve movement may be sufficient to detect several 
types of DU faults without interrupting the process, and as a result, it is possible to 
run PST more frequently than proof-testing. For a shutdown valve, PST can reveal 
most causes for the failure mode "fail to close," but cannot reveal any "leakage in 
closed position." 
The PST can be considered as an imperfect proof test with coverage factor, PTC, 
which is sometimes called the PST coverage. Lundteigen & Rausand (2008a) outline 
an approach that can be used to estimate the PST coverage. The PST coverage is 
typically in the range of 60% to 70% (Summers & Zachary, 2000). 
The PFDavg is improved when PST is introduced, because a fraction of the DU 
faults are detected and corrected within a shorter time interval after their appearance, 
than by full proof-testing. Thus, if the full proof test interval is kept unchanged, the 
introduction of PST improves the reliability of the SIF. 
The assumptions related to the PST coverage should be made for the plant specific 
application rather than for an average valve performance (ISA TR 84.00.03, 2002; 
Goble, 2005). The factors that may influence the PST coverage are, for example: 
- Valve design: Different types of valves may have different failure properties. 
One type of valve may, for example, have more failures related to leakage in 
closed position than another type of valve where most failures are caused by a 
stuck stem. For the first valve, we expect a lower PST coverage than for the 
latter. The failure properties may be derived from an FMECA. 
- Functional requirements: PST is not able to verify all types of functional re-
quirements. A PST reveals whether or not a valve starts to move, but is not 
able to verify that the valve continues to a fully closed position and that it keeps 
tight in this position. ISA TR 84.00.03 suggests that PST is not used where tight 
shutoff is very important. The specified valve closing time may also impact the 
PST coverage. For valves where 2 to 3 seconds closing time is specified, one 
may obtain less information about the valve failures and performance deviations 
than for valves with longer closing times. 
- PST technology: The PST technology may affect which and to what extent 
failures are detected. While a PST implemented with simple readback of the 
valve position signal may detect that a valve fails to start closing, a more ad-
vanced PST technology solution with additional sensors may indicate if other 
failures are present by analyzing performance deviations (Ali & Goble, 2004; 
Ali, 2004). ISA TR 84.00.03 emphasizes that readback of position signal should 
confirm that the requested position has been reached. To simply verify that the 
limit switches have left and returned to their end position is not considered a 
valid test. 

ADDING A CONSTANT PFDAVG 
353 
PFD(t) 
5 τ Time 
Figure 11.4 
The PFD(i) of a channel with imperfect proof-testing, modeled by adding a 
constant PFD(a). 
- Operational and environmental conditions: Some operational and environmen-
tal conditions may lead to obstructions and build-up in the path towards the 
valve end position. Concern has been raised that build-up is more likely for 
valves that are moved to a fixed intermediate position (ISA TR 84.00.03, 2002). 
While a stuck valve may be detected by the PST, other obstructions or build-
up that impede the valve from reaching the end position, are not identified. 
If the operating or environmental conditions are likely to cause build-up or 
obstructions, (e.g., multi-phase flow, seawater flow), one may expect a lower 
PST coverage than if the valve is operated in a clean (e.g., gas) environment. 
ISA TR 84.00.03 suggests that PST is not performed for valves in unclean en-
vironment where, for example, dirt, polymerization products, deposition, crys-
tallization, corrosive chemicals are present. 
Based on the discussion above, PST seems most suited to detect that a valve is 
stuck and does not start to move. To what extent other dangerous faults may be 
detected, is influenced by the features of the selected PST technology. The PST 
coverage is not only influenced by what the PST technology is able to detect. It is 
just as important to consider how frequent these dangerous failures occur compared 
to the occurrence of other dangerous failures. Here, the functional requirements, the 
valve design, and the documented valve performance must be considered. 
11.4 Adding a Constant PFD avg 
Another possible approach to model imperfect proof-testing is to add a constant 
PFD^g for the nonrevealed DU faults. For this approach the time-dependent PFD(i) 
is written as 
PFD(r) = PFD(r)(/) + PFD^g 
avg 
and is illustrated in Figure 11.4. 
The resulting PFDavg is therefore 
PFDa 
>ω 
= PFDW + PFD(a) 
1 l 
avg ' * * 
avg 
PTC-A D Ur + PFD: (a) 
avg 

3 5 4 
IMPERFECT PROOF-TESTING 
This approach is used by the PDS method (SINTEF, 2013b) where the PFD^g is 
called /?TIF (probability of test-independent failures) to indicate that this contribution 
is related to faults that are not revealed by proof-testing. The PDS method argues that 
PFD<a? (or ^ T I F) should also cover systematic faults that may occur upon a demand. 
The PDS method provides in SINTEF (2013a) estimates of PFD^g (or pTW) for 
different types of channels. The values range from 5 · 10~6 to 1 · 10~3. 
fl 
EXAMPLE 11.4 Gas detectors 
Consider a gas detector that is installed in a process plant. To test the gas detector 
a non-poisonous test gas is injected directly into the detector. This is not a fully 
realistic test because a different type of gas is used in the test and because the 
proof test cannot reveal if gas is prevented from entering the detector (e.g., due to 
wind or fans). It is no reason to believe that the probability of nonrevealed faults 
increases with time and the model in Figure 11.4 may therefore be realistic. Θ 
11.5 
Nonconstant Failure Rates 
In IEC 61508 and IEC61511, failure rates are generally assumed to be constant, 
which means that all channels are as-good-as-new as long as they are functioning. 
This may be a relevant assumption for electronic items, but is not at all realistic for 
mechanical items, such as valves. 
Consider a channel that is deteriorating with an increasing failure rate. If no DU 
fault is revealed in a (perfect) proof test, the channel is in a functioning state just after 
the proof test, but the channel is not as-good-as-new. The PFD(/) of the channel in a 
proof test interval is different from the PFD(f) in the previous proof test interval. 
Let 7bu denote the time from t = 0 to the first DU failure and assume that 7bu 
has an increasing failure rate function ZDU(0· The PFD(i) of the channel in the first 
proof test interval (0, r) is 
PFDKO = Pr(7bu < t) = \ - e~& Z™(M)du 
ss / zOV(u)du 
for 0 < t < r 
Jo 
If the channel passes the first (perfect) proof test, the PFD2(0 in the second proof 
test interval (τ, 2τ) is 
PFD2(i) = Pr(7bu < t | 7bu > T) 
Pr(r D U < t) - Pr(rD U < τ) 
= 
Ρ,Γτν,.. -, ,Λ 
for r < / < 2r 
Pr(7DU > T) 

MARKOV MODELS 
3 5 5 
PFD(t) 
5τ 
Time 
Figure 11.5 
The PFD(i) of a channel with increasing failure rate and perfect proof-testing. 
λ
( Γ |,, 
λ
<η,|„, 
Figure 11.6 
Markov model. 
If the channel passes the second (perfect) proof test, the PFD3(?) in the third proof 
test interval (2τ, 3τ) is 
PFD3(0 = Pr(r D U < t | r D U > 2τ) 
P r ( r D U < 0 - P r ( 7 D U < 2 r ) 
= 
^ /rT, 
— 
for 2τ < t < 3τ 
Pr(7DU > 2τ) 
and so on. 
The PFD(;) of the channel is illustrated in Figure 11.5. This model is based on the 
assumption that if a DU fault is revealed in a proof test, a minimal repair is carried 
out (e.g., see Rausand & H0yland, 2004). A minimal repair corrects the DU fault, 
but does not reduce the wear or change any other properties of the channel. The 
other extreme is to replace the channel when a DU fault is revealed, in which case 
the PFD(i) is reset to t = 0 after the repair action. Intermediate repair strategies 
may also be relevant. 
This model is discussed in detail by Rausand & Vatn (1998). The model in Fig-
ure 11.5 may be combined with the models in Sections 11.2 and 11.3. 
11.6 Markov Models 
11.6.1 Standard Markov Approach 
The standard Markov approach may be used to establish a simple model for imper-
fect proof-testing. Such a Markov model is illustrated in Figure 11.6 for a single 
channel. 
The channel starts in state 0 at time t — 0. When a revealable DU failure occurs 
with rate λ^υ and is revealed in a proof test, the DU fault is repaired with rate μου· 

356 
IMPERFECT PROOF-TESTING 
When a nonrevealable DU fault occurs with rate λ^ 
, this DU fault remains until 
a demand occurs or the channel is overhauled or replaced. The transitions between 
states 0 and 1 will continue ignoring whether or not a nonrevealable DU fault is 
present. 
The down-states of this Markov process are V = {1,2} and the PFD(i ) is 
PFD(r) = Λ(ί) + Piif) 
This model is seen to be comparable to the model in Section 11.2. Markov models 
for groups of multiple channels can be established with the same approach, but are 
complicated when the number of channels increases. 
11.6.2 Multi-Phase Markov Models 
A group of channels with imperfect proof-testing and repair can be modeled by a 
modified Markov approach by the following steps: 
1. Specify the state of the group at time t — 0. The initial state is often state 0, but 
any other state can be chosen. 
2. Model the behavior in the first test interval (0, r) by a standard Markov model 
3. Describe the proof test and the repair by a transition probability, Rtj. 
If the 
state of the group is i immediately before the proof test, Rij is the probability 
that the group is in state j immediately after the proof test/repair. 
4. Model the behavior in the second test interval (τ, 2τ) by a standard Markov 
model (with distribution of starting states according to the result of step 3. 
5. Repeat the steps for each proof test interval. 
This approach is called a multi-phase Markov approach and is also known as a 
"piecewise Markov approach". We should note that the multi-phase Markov ap-
proach does not fulfill the Markov property and is therefore, strictly speaking, not a 
Markov approach. Each step fulfills the Markov property, but not the whole process. 
Repair Matrix. Imperfect testing and repair may be modeled by a repair matrix, R. 
As before, let X(t) denote the state of an item (channel, group, subsystem, system) at 
timei. Proof tests and repairs are carried out at times τ,2τ, 3τ, 
The time required 
to test and repair the item is rather short and will not influence the scheduling of the 
proof tests. 
Let Yn = Χ(ητ—) denote the state of the item immediately before the proof test 
at time ητ, that is, immediately before proof test n. If a DU fault is revealed in the 
proof test, a repair action is initiated, and changes the state of the item from Yn to 
Z„, where Z„ is the state of the item just after the proof test (and possible repair) n. 
When Yn is given, we assume that Z„ is independent of all transitions of the system 
before timenr. Let 
Pr(Z„ = j | Yn = i) = R,j 
(11.13) 

MARKOV MODELS 
3 5 7 
denote the transition probabilities, let R be the corresponding transition, or repair, 
matrix. We illustrate the repair matrix by a simple example. 
31 EXAMPLE 11.5 Repair matrix for a group of two channels 
Consider a loo2 voted group of independent and identical channels. When dis-
regarding DD and S failures, the group has three different states. 
State 
State description 
0 
Both channels are functioning (i.e., the channels do not have DU faults) 
1 
One channel is functioning and one has a DU fault 
2 
Both channels have DU faults 
Consider proof test n. If the state of the group immediately before this proof 
test is Yn = 0, both channels are functioning and should also be functioning just 
after the proof test is completed. In this case, we should have 
(floo,/?oi,*02) = (1,0,0) 
If, however, the proof test exposes the channels to high stress and/or a human 
error may harm the channels during the test or the following start-up, this may 
be incorporated into the model, for example as 
(R0o, Äoi, R02) = (0.99,0.008,0.002) 
Here, it is assumed that a DU failure of one of the two channels is introduced by 
the proof test in eight out of thousand proof tests, and that DU failures of both 
channels are introduced by the proof test in two out of thousand proof tests. 
Similar arguments can be used to determine /?,; when the group is in state 1 
or 2 immediately before the proof test. 
The repair matrix for a perfect proof test and repair, where all DU faults are 
revealed and repaired to an as-good-as-new state is 
(11.14) 
For imperfect testing and repair, other values for Ry are used to reflect the 
actual situation. 
® 
Multi-phase Markov models are further discussed by Felgner & Frey (2011), who 
illustrate how the models can be evaluated by the computer program Modelica.® 

3 5 8 
IMPERFECT PROOF-TESTING 
Figure 11.7 
Markov model. 
11.7 Additional Reading 
The following titles are recommended for further study related to Chapter 11 : 
- Partial stroke testing of process shutdown valves: How to determine the test 
coverage (Lundteigen & Rausand, 2008a) provides a thorough discussion re-
lated to the pros and cons of partial stroke testing and gives a detailed procedure 
for how to determine the partial stroke test coverage. 
- Reliability of safety-instrumented systems subject to partial testing and common-
cause failures (Jin & Rausand, 2014) 
- Reliability Prediction Method for Safety Instrumented Systems: PDS Method 
Handbook (SINTEF, 2013b) presents may interesting views on imperfect proof-
testing and how it should be modeled. 
- Petroleum, Petrochemical and Natural Gas Industries - Reliability Modeling 
and Calculation of Safety Systems (ISO/DTR 12489, 2012). Draft to technical 
report. 
- Safety Instrumented System Management (Baradits, 2010). Chapter 6 of this 
PhD-thesis presents many interesting views on imperfect proof testing and mod-
eling consequences. 

CHAPTER 12 
SPURIOUS ACTIVATION 
12.1 
Introduction 
An item that is designed to carry out a safety function may fail in two different ways: 
1. Fail to function. This failure occurs when the safety function is demanded, but 
the item is not able to perform the function adequately. 
2. Spurious activation. This failure occurs when the safety function is activated 
without being demanded or requested. 
The first failure category is treated in the previous chapters of this book. The aim 
of this chapter is to shed some light on the second category: spurious activation. The 
topic is introduced briefly by two examples. 
fl 
EXAMPLE 12.1 A fail-safe gate valve 
Consider the fail-safe gate valve in Figure 1.7. The valve is designed according 
to the de-energize-to-trip principle. The valve is held open by hydraulic pressure 
from an accumulator. The hydraulic pressure compresses a steel spring and the 
valve is kept open as long as the pressure is maintained. When the pressure is 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
359 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

360 
SPURIOUS ACTIVATION 
bled off, the valve closes by spring force and carries out its safety function. The 
hydraulic outlet from the valve actuator is controlled by a pilot (solenoid) valve 
that is energized to keep the valve closed. 
When a hazardous situation occurs on the downstream side of the valve, the 
electricity to the pilot valve is cut off such that the pilot valve opens by spring 
force and relieves the hydraulic pressure in the valve actuator. The valve has two 
main failure modes of type 1 (fail to function): "fail to close on command" and 
"leakage in closed position." If there is a leakage in the hydraulic system or an 
electrical failure, the valve gets a type 2-failure. This failure mode is often called 
"spurious closure" or "premature closure." Due to the de-energize-to-trip design 
principle, type 2-failures are often more frequent than type 1-failures. Because 
the valve goes to a closed and safe position when the external control and/or 
energy is lost, the valve is said to be ^fail-safe-close valve. 
The gate valve could, alternatively, be designed according to the energize-to-
trip principle. By this principle, the valve actuator is not energized in normal 
operation and a spring is not used. To carry out the safety function and close the 
valve, energy must be supplied to the actuator to move the gate into a closed po-
sition. If the hydraulic pressure is lost, the valve fails to function. This principle 
is obviously less reliable with respect to safety, but the rate of spurious closures 
may be negligible. 
Θ 
fl 
EXAMPLE 12.2 Level transmitter in a vessel 
Consider a level transmitter that is installed to send a signal to a logic solver if 
the fluid in a vessel exceeds a specified high level. A type 1 failure occurs when 
the level transmitter fails to send a signal when the fluid passes the specified 
high level, and a type 2 failure occurs when a signal is sent at a fluid level below 
the specified high level. The causes of the type 2 failure may be 
(a) Internal failure. A technical failure in the transmitter. 
(b) Miscalibration. The test/repair team has miscalibrated the transmitter 
(c) Discrimination error. The transmitter is not able to discriminate between 
the real fluid level and, for example, foam that has formed on top of the 
liquid. 
φ 
The term spurious activation is barely mentioned in IEC 61508, except in the 
definition of a safe failure. 
Safe failure. Failure of an element and/or subsystem and/or system that plays a part 
in implementing the safety function that: 

INTRODUCTION 
361 
(a) results in the spurious operation1 of the safety function to put the EUC (or 
part thereof) into a safe state or maintain a safe state; or 
(b) increases the probability of the spurious operation of the safety function to 
put the EUC (or part thereof) into a safe state or maintain a safe state. 
(IEC 61508-4, para. 3.6.8)2 
A spurious operation is therefore a safe failure according to IEC 61508, but a 
safe failure is not necessarily a spurious operation. Spurious activation is known by 
several different names in the literature, such as spurious operation, spurious action, 
spurious trip, spurious stop, nuisance trip, false trip, and false alarm. 
As illustrated in Example 12.2, a spurious activation may be due to item failure 
or a false demand. A false demand is a demand that is erroneously treated as a real 
demand, for example, a stray ray of sunlight that is mistakenly read as a fire by a 
flame detector. 
This chapter focuses on SIFs that are operated in low-demand mode, and the 
presentation is related to applications in the process industry. For this reason, the 
EUC is referred to as the process; the term used in IEC 61511. 
A spurious activation of a SIF may lead to a partial or full shutdown of the process. 
In the process industry, shutdowns and start-ups are sometimes hazardous operations 
and a spurious shutdown of the process may involve high risk. It is therefore impor-
tant to reduce the number of spurious activations to avoid: 
1. Unnecessary production or service interruptions 
2. Hazards due to unplanned process shutdowns 
3. Hazards during unscheduled system restoration and restart 
Some spurious activations are inherently dangerous. Chapter 1 mentioned spuri-
ous activation of the airbag system in a car. If such a spurious activation occurs when 
driving with high speed, it may be as dangerous as a fail-to-function in a collision. 
There are many safety systems in the process industry where spurious activation may 
be dangerous. 
The main focus of IEC 61508 and IEC 61511 is to ensure that the SIF is suc-
cessfully performed on demand and limited focus is given to spurious activations. 
IEC 61508 has no requirements related to spurious activations, whereas IEC 61511 
requires a maximum spurious trip rate be specified, but the standard does not provide 
any definition of a spurious trip or guidance on how the rate should be determined. 
To determine the spurious trip rate, the oil and gas industry uses the formulas 
provided in ISA TR 84.00.02 (2002) and SINTEF (2013b). When comparing these 
formulas, it is evident that there is no unique interpretation of the concept of spurious 
trip. Whereas the PDS method defines a spurious trip as a spurious activation of 
a single channel or of a SIF, ANSI/ISA 84.01-1996 refers to a spurious trip as a 
1 Emphasized by the author. 
2IEC 61508-4 ed.2.0 "Copyright © 2010 IEC Geneva, Switzerland, www.iec.ch." 

362 
SPURIOUS ACTIVATION 
nonintended process shutdown. As a result, the concept of spurious trip is rather 
confusing and it is difficult to compare the spurious trip rate in different applications. 
12.2 Main Concepts 
The term spurious indicates that the cause of activation is improper, erroneous, false, 
fake, or non-genuine, whereas activation indicates that there is some type of action 
or transition from one state to another. 
12.2.1 Three Types of Spurious Activation 
There are three main types of spurious activation: (i) spurious activation of individual 
channels, (ii) spurious activation of a SIF, and (iii) spurious shutdown of the process. 
To use the same concept to describe all the three types may lead to misunderstanding 
and confusion. 
To distinguish the different types of spurious activation, the author suggests the 
following terms and definitions: 
Spurious operation. A spurious operation (SO) is an activation of the safety function 
of a channel without the presence of a specified process demand. A spurious 
operation of a channel is said to be an SO failure and the SO failure rate is denoted 
^■so-
Spurious trip. A spurious trip (ST) is an activation of a SIF without the presence of 
a specified process demand. 
Spurious shutdown. A spurious shutdown is a partial or full process shutdown with-
out the presence of a specified process demand. 
All three terms are used in the literature but, unfortunately, often without any clear 
definition. In ISO 14224 (2006), the term spurious operation is close to our defini-
tion, whereas the term spurious trip is sometimes used to describe an unexpected 
shutdown of a process or service (e.g., see CCPS, 2007). 
12.2.2 Spurious Trips and Spurious Shutdowns 
A spurious operation (i.e., an SO failure) of a channel may or may not lead to a 
spurious trip of the SIF, and a spurious trip of a SIF may or may not lead to a spurious 
shutdown of the process. A spurious trip of a SIF may in some cases only lead 
to a minor disturbance and in other cases to a process shutdown. The effect of a 
spurious activation depends on the architecture of the SIS and the process properties. 
A spurious activation of, for example, a single gas detector in a 2oo3 voted group of 
detectors does not lead to spurious activation of the SIF, and a spurious activation of 
a SIF leading to start-up of a fire pump does not necessarily disturb the process. 
From the end-user's point of view, spurious process shutdowns are obviously of 
prime interest. To quantify the frequency of spurious shutdowns requires detailed 

MAIN CONCEPTS 
3 6 3 
knowledge about the whole process system, and is therefore not possible without 
more detailed information about the process. If there are too many spurious shut-
downs, the end-user may be tempted to disconnect the SIF, with the associated neg-
ative effects this has on safety. 
12.2.3 Fault Tolerance for Spurious Trips 
The following sections are focused on koon voted groups and it is assumed that 
the voted groups are subsystems of a SIS such that a spurious activation of a voted 
group will be a spurious trip of the associated SIF. Generally, a voted group does not 
have the same hardware fault tolerance (HFT) for spurious operation failures as for 
dangerous failures, as illustrated in Example 12.3. 
β 
EXAMPLE 12.3 Redundant shutdown valves 
Consider two shutdown valves SDVi and SDV2 that are installed in physical se-
ries in a pipeline, as shown in Figure 12.1. The objective of the valves is to close 
the flow in the pipeline if a downstream hazardous situation should occur. The 
two valves are redundant in the sense that each of them can close the flow. To fail 
to perform the safety function, both valves must fail to close as illustrated by the 
fault tree in Figure 12.1(a). The two valves are therefore functioning as a loo2 
system and can be illustrated by the reliability block diagram in Figure 12.1(c), 
where the functional blocks |_lj and |_2j represent the closing function of SDVi 
and SDV2, respectively. The valve system is able to function when at most one 
of the valves is unable to perform its safety function (i.e., is in a failed state or 
is being repaired) and the system therefore has a hardware fault tolerance, HFT 
= 1 with respect to the safety (closing) function. 
With respect to spurious closure, the flow in the pipeline is spuriously stopped 
if any of the two valves has a spurious closure (i.e., operation). This is illustrated 
by the fault tree in Figure 12.1(b) and in the corresponding reliability block 
diagram in Figure 12.1(d). The hardware fault tolerance with respect to spurious 
closure is therefore HFT = 0. The functional blocks, [Jj and [_2j are shaded in 
Figure 12.1(d) to show that the functions the blocks represent are different from 
the corresponding functions in Figure 12.1(c). 
Θ 
Consider a loon voted group of independent and identical channels. Recall that 
loon is an abbreviation for loon:G and is always related to the safety function. A 
loon voted group is able to perform its safety function if at least one of the channels 
can perform its safety function. This means that the voted group is able to perform 
its safety function when up to n — 1 of the channels are in a failed state and not able 
to perform their safety functions. The hardware fault tolerance of the loon voted 
group is therefore HFT — n — \. 
As indicated in Example 12.3, a loon voted group is not at all reliable with re-
spect to spurious trips, because a spurious trip occurs when the first channel has a 

364 
SPURIOUS ACTIVATION 
^
^
^ 
SDV, 
SDV, 
(a) 
Γ 
Valves fail 
to perform 
safety function 
/ " N 
SDV fails 
to close 
on command 
(SDVJ) 
(c) 
I 
1 
(b) 
Spurious 
closure 
of pipeline 
Ί 
SDV fails 
to close 
on command 
(SDV) 
Γ Q 
Spurious 
closure 
of SDV, 
(SDV) 
I 
Spurious 
closure 
of SDV2 
(SDvJ) 
Figure 12.1 
A pipeline with two shutdown valves. 
Table 12.1 
loo2 
Dangerous failures 
1 
Spurious trips 
0 
Fault tolerance for koon 
loo3 
2 
0 
voted groups 
Voting 
2oo3 
1 
1 
loo4 
3 
0 
2oo4 
2 
1 
3oo4 
1 
2 
spurious operation failure. The hardware fault tolerance with respect to spurious 
trips is therefore HFT = 0. 
A koon voted group is functioning when at least k of its channels are functioning. 
This means the voted group has a hardware fault tolerance HFT — n—k with respect 
to the safety function, and a hardware fault tolerance HFT = k — 1 with respect to 
spurious trips. The fault tolerances for some simple voting configurations are listed 
in Table 12.1 for dangerous failures and spurious trips, respectively. 
In general, a koon:G voted group is functioning when at least k of its n channels 
are functioning and fails when at least n — k + 1 of the n channels fail, such that 
koon:G = (n — k + l)oo«:F 
A voted group that can perform its safety function when at least k of its n channels 
can perform their safety functions is therefore a koon:G voted group with respect to 
the safety function. The same voted group gets a spurious trip when at least k of its n 

CAUSES OF SPURIOUS ACTIVATION 
365 
channels get spurious operation failures and is a koon.F voted group with respect to 
spurious failures and a(n—k + l)oon:G with respect to the function "avoid spurious 
trip." 
12.3 Causes of Spurious Activation 
This section indicates briefly some potential causes for the three types of spurious 
activation. 
Spurious operation. A spurious operation of a channel may be a random hardware 
failure or due to systematic faults. Systematic faults may be even more important 
for spurious operation than for dangerous failures. More detailed causes are: 
1. Internal hardware failure of the channel (or it's supporting equipment) 
2. External stress failure 
3. Systematic fault 
- Error in the embedded software 
- Inability to discriminate between a real demand and false demand 
- Inadequate specification 
- Installation error 
- Calibration error 
Spurious trip. A spurious trip may be a multiple independent random hardware fail-
ure, a common-cause failure (CCF), or due to systematic faults. CCFs are usually 
much more important than multiple independent failures. Many CCFs are due to 
systematic faults. 
1. Loss of utilities, such as pneumatic, hydraulic, or power supply. Loss of util-
ities may directly lead to a spurious trip if the SIF is designed to be fail-safe 
(which is the typical situation on oil and gas installations) 
2. CCFs due to: 
- Inadequate specification 
- Lack of diversity 
- Calibration error (all detectors miscalibrated) 
- Reaction to false demands (e.g., flame detectors react to a welding flame 
or a stray of sunlight) 
3. DD failures: In some cases, the SIF is activated spuriously when the presence 
of DD failures prevents the SIF from functioning on demand. IEC 61508 and 
IEC 61511 require this performance if the channels have not been restored 
within a specified mean time to restoration. A 2oo3 configuration is still able 
to act if a single DD failure is present. When two DD failures are present, the 
SIF is unable to respond to a real process demand, and the time the SIS is in 

366 
SPURIOUS ACTIVATION 
such a condition should be minimized. This spurious trip may be activated 
automatically or manually. If the spurious trip causes a process shutdown, the 
shutdown is usually more controlled and smooth than a spurious trip of the 
previous type. 
Spurious shutdown. A spurious trip will sometimes, but not always, lead to a spuri-
ous shutdown of the process. If the SIF does not interact directly (or indirectly by 
activating other SIFs) with the process, the process may not be disturbed upon a 
spurious trip. 
A spurious shutdown may also be caused by a spurious closure/stop of non-SIS 
equipment that interacts with the process, such as control valves and pumps. A 
spurious closure of a control valve or a spurious stop of a pump may be due to 
channel internal failures, human errors, or automatic control system errors. 
See Lundteigen & Rausand (2008b) for a more thorough treatment of failure 
causes. 
12.3.1 
Identifying Failure Causes by Fault Tree Approach 
Through its deductive reasoning, a fault tree approach is often suitable for identifying 
the causes of a spurious trip of a SIF. Figure 12.2 shows the TOP structure of a fault 
tree for the event "Spurious trip of the SIF." Because a spurious trip of the SIF must 
be due to the sensor subsystem, the logic solver subsystem, or the final element 
subsystem of the safety loop that performs the SIF, the TOP structure has an OR-gate. 
A fault tree for each subsystem can now be established based on a careful consid-
eration of possible causes. The analysis can be guided by questions, such as: 
- How many channels must have a spurious operation for the subsystem to cause 
a spurious trip? 
- Are there any utilities (e.g., electricity, hydraulics, pneumatics) that may fail 
and cause a spurious trip? 
- Is it possible that an external event can cause a spurious trip? 
- Are the channels able to discriminate between a real demand and a false de-
mand? If no, which false demands are likely to occur? 
- Is it possible that the channels have been miscalibrated? 
- Are the channels of the same type, or diverse? 
The fault tree approach is suitable for identifying causes of spurious trips but not 
so suitable as a model for quantitative analysis. The reason is that the fault tree is 
a static model that cannot easily accommodate dynamic effects. A case study of a 
fault tree analysis for spurious trips is given by Torres-Echeverria et al. (2009a). 

CAUSES OF SPURIOUS ACTIVATION 
3 6 7 
1 
Spurious trip 
of sensor 
subsystem 
A 
Spurious trip 
ofSIF 
0 
1 
Spurious trip 
of logic solver 
subsystem 
/ L S \ 
1 
Spurious trip 
of final element 
subsystem 
/ F E \ 
Figure 12.2 
Fault tree for a spurious trip of a SIR Only the TOP structure is shown. 
12.3.2 
Failure Categories 
Spurious operations due to internal causes are often considered as safe failures as 
they do not prevent the activation of the SIF upon a demand. All safe failures do 
not lead to spurious operation, and it is therefore necessary to study the safe failure 
modes of each channel to determine which ones are relevant for spurious operation. 
An internal leakage in the valve actuator of a fail-safe-close safety valve may, for 
example, lead to a spurious operation, whereas a failure of a valve position indicator 
(limit switch) will not. 
IEC 61508 and IEC 61511 distinguish between two categories of safe failures, 
safe random hardware failures and safe systematic failures. The IEC standards con-
sider systematic failures as unpredictable failures and do not require the rates of 
these failures be quantified. The PDS method SINTEF (2013b), on the other hand, 
suggests a quantification method where the contribution from systematic failures is 
included. 
Common-Cause Failures. Spurious CCFs do not necessarily have the same root 
causes and coupling factors as dangerous CCFs. Two safety valves may, for example, 
fail to close on demand due to scaling, whereas scaling will never lead to spurious 
operation of the same valves. A leakage in a common hydraulic supply system for 
two (fail-safe-close) safety valves may lead to spurious operation, but cannot prevent 
the safety valves from closing. Because dangerous CCFs are different in nature from 
spurious CCFs, the procedure in IEC 61508-6 is not suitable for estimating the beta-
factor, /Jso> for spurious CCFs. The same types of defenses against coupling factors 
(e.g., reduce similarities in design, installation or procedures) apply to both spurious 
and dangerous CCFs. 
False Demands. 
False demands are important contributors to spurious operations 
and spurious trips. A false demand may have similar characteristics (e.g., visual 
appearance and composition) with a real process demand, and it may therefore be 
difficult for the input channel to distinguish the two. A stray ray of sunlight may, 
for example, look like a small flame from certain angles, and a flame detector may 
therefore erroneously read it as a flame. 

368 
SPURIOUS ACTIVATION 
It may not be possible to reduce the occurrence of false demands, but we may in-
fluence how the input channels respond to them. It may, for example, not be possible 
to remove sunlight or alter the density of foam, but we may select channels that are 
designed to better discriminate between false and real process demands, or we may 
relocate the channels to make them less vulnerable to false demands. 
Some false demands are man-made, such that we may be able to influence how 
often they occur by improving operation and maintenance procedures and work pro-
cesses. To avoid flame detectors from responding to welding, for example, inhibits 
may be set or the welding flame may be covered such that it is not seen by the flame 
detectors. 
12.4 
Reliability Data for Spurious Operations 
12.4.1 Systematic Faults 
Systematic faults can lead to spurious operation of single channels and also to multi-
ple channels failures and CCFs. IEC 61508 states that systematic faults should not be 
quantified. Omitting systematic faults inevitably lead to a nonconservative estimate 
of the rate of spurious trips (e.g., see ISA TR 84.00.02, 2002). 
12.4.2 Reliability Data 
Reliability data are typically provided for channels; see Chapter 6 for a survey of 
available data sources. Many data sources, such as OREDA (2009) and SINTEF 
(2013b), are based on maintenance reports. In some cases, a spurious operation of a 
channel only needs a reset to be functioning again. A formal workorder is therefore 
often not required and the spurious operation does, consequently, not appear in the 
maintenance reports. The failure rate estimates for spurious operation failures are, 
therefore, often nonconservative. 
IEC 61508-6 and IEC 62061 provide approaches for determining the beta-factor 
for dangerous CCFs, but a similar approach for determining the beta-factor for safe 
failures is not available. 
12.5 Quantitative Analysis 
IEC 61511 requires a spurious trip rate be specified but without defining what is 
meant by the concept. It is therefore necessary to explain what we mean by a spurious 
trip rate and develop formulas that can help us to quantify the the spurious trip rate. 
12.5.1 Spurious Trip Rate 
A spurious trip was defined in Section 7.5.5 as an unintended activation of a SIF 
without the presence of a specific demand. A spurious trip is therefore & false acti-
vation of the SIF. 

QUANTITATIVE ANALYSIS 
369 
Spurious trips of the SIF are usually assumed to occur according to a homoge-
neous Poisson process3 with a rate, which we denote STR. Let NST(Î) be the num-
ber of spurious trips during a time interval of length t. The probability of exactly n 
spurious trips in the interval is 
Pr(tfST(0 = n) = ( S T R ; ° " e~STR-< for n = 0,1,... 
The rate of the occurrence of spurious trips is 
S T R = ^ f M 
(12.1) 
STR is therefore the mean number of spurious trips of the SIF per time unit and it is 
therefore natural to define this rate as the spurious trip rate (STR). The time unit is 
usually chosen to be one hour such that STR is presented, for example, as 5.0 · 10~5 
per hour. The mean number of spurious trips in a time interval of length / is 
E [JVST(0] = STR · t 
When counting spurious trips, it is important to include only trips that are not 
associated with problems in the process that the SIF is designed to handle (e.g., see 
ISA TR 84.00.02, 2002). 
A SIF is performed by a safety loop of a SIS with a sensor (S) subsystem, a logic 
solver (LS) subsystem, and a final element (FE) subsystem. We use the term spurious 
trip rate, STR (with a subscript) when quantifying the rate of spurious activations of 
these subsystems. Because the subsystems are largely independent, the STR of a SIF 
is equal to the sum of the spurious trip rates of the three subsystems of the safety 
loop (see also Figure 12.2). 
STR = STRS + STRLS + STRFE 
(12.2) 
STR of a Voted Group. The main object of study of the remaining sections is a 
koon voted group (G) ant it is assumed that the voted group is also a subsystem of a 
safety loop. The symbol STRQ is used to denote the part of the STR that is caused 
by spurious activations of the group G. 
A subsystem may sometimes comprise one or more voted groups of channels. 
When a subsystem has two voted groups, such a 2oo3 voted group of pressure trans-
mitters and a loo3 voted group of level transmitters, the voted groups are often voted 
as a loo2 group, in which case the STR of the subsystem is the sum of the STRs for 
the two voted groups. If the two groups are voted 2oo2, such that both groups have 
to provide alarms to activate the logic solver subsystem, the STR of the subsystem is 
normally very low. 
3This model is adequate only when the repair times of spurious trip failures are negligible compared to 
the mean time between spurious trips. 

370 
SPURIOUS ACTIVATION 
fl 
EXAMPLE 12.4 Single channel 
Consider a lool voted group (G) with a single channel with SO failure rate 
A so and let MTTRso denote the mean downtime of the channel caused by an 
SO failure. In this case, an SO failure of the channel automatically leads to a 
spurious trip. 
The mean time to an SO failure is 
MTTFso = 
^~ 
Λ-SO 
and the mean time between spurious trips (ST) of the channel is 
MTBFST = MTTFso + MTTRSo 
The spurious trip rate of this simple subsystem is therefore 
1 
1 
STRG = 
MTBFST 
MTTFso + MTTRso 
A_so 
1 + AsoMTTRso 
In most cases, the downtime MTTRso » is short that the approximation STRQ = 
Aso is adequate. In some applications, however, the downtime may be so long 
that it cannot be disregarded, such as in subsea oil and gas production systems. 
The mean time between spurious trips of a voted group is calculated as the inverse 
of the spurious trip rate. 
MTBFST,G = ^
- 
(12.3) 
al KG 
In the following, we consider either the sensor subsystem or the final element sub-
system. The logic solver subsystem has a lot of software, is more complex, and needs 
a more thorough treatment, which is outside the scope of this book. The spurious trip 
rate of the logic solver subsystem is typically obtained from the manufacturer of the 
logic solvers. See also Torres-Echeverria et al. (2011). 
► Remark: The mean downtime, MTTRso > associated with an SO failure is often 
different from the mean repair times of DD and DU faults. The failure mechanisms 
are generally different and an SO failure is, in many cases, easier to repair. This can 
be illustrated by a hydraulically operated fail-safe valve. To repair a DU failure, it is 
usually necessary to dismantle the valve and change some internal parts, whereas an 
SO failure may be caused by a leakage in the hydraulic system that can be repaired 
without dismantling the valve. 
Θ 

QUANTITATIVE ANALYSIS 
371 
12.5.2 Three Categories of Spurious Trips 
As discussed above, there are three main causes of spurious trips of a SIF: 
Internal failures (IF). Internal failures may affect one or more of the subsystems. 
The number of channels that have to fail to give a spurious trip depends on the 
architecture of the subsystem. 
False demands (FD). There are two main types of false demands: (a) Demands that 
have nothing to do with a real demand, but are mistakenly read as a demand by 
the sensors (e.g., a stray ray of sunlight that is read as a flame by a flame detector), 
and (b) demands that are real, but should not be read as demands (e.g., welding 
that has not been covered and therefore is read as a flame). 
False demands of type (a) can only be prevented by a redesign of the system. False 
demands of type (b) are mainly caused by human errors or inadequate procedures 
and can therefore be more easily prevented. 
Dangerous detected failures (DD). Whether or not one or more DD failures result in 
a spurious trip of the SIF depends on the subsystem architecture and the operating 
philosophy. 
If we assume that spurious trips due to two or more different causes do not occur 
simultaneously, the spurious trip rate of a SIF may be written as 
STR G = STRIF + STR F D + S T R D D 
(12.4) 
Equation (12.4) can also be applied to each subsystem in the safety loop performing 
the SIE 
We now discuss each category of spurious trips separately. 
12.5.3 Spurious Trips Due to Internal Failures 
According to (12.2), the STR of a SIF can be determined as the sum of the STRs 
for the subsystems of the safety loop that is performing the SIF. The subsystems can 
therefore be studied separately. We start by considering the STRQ of a voted group 
due to internal failures (IF). 
fl 
EXAMPLE 12.5 Spurious trip rate for a loon voted group 
Consider a loon voted group of independent and identical channels. Any SO 
failure of a channel gives a spurious trip of the voted group. The spurious trip 
rate due to internal failures of a loon configuration of channels is therefore 
STRQJF = ηλ$ο 
The spurious trip rate of a loon voted group due to internet failures is there-
fore proportional to the number n of channels. 
Θ 

372 
SPURIOUS ACTIVATION 
Spurious Trips ofkoon Voted Groups. 
Consider a koon voted group of identical 
channels, where n > k > 2. With the beta-factor model, the SO failures of each 
channel can be split into independent failures (i.e., with multiplicity 1) and CCFs 
(i.e., with multiplicity n). If an SO-CCF occurs, with rate ßso^so* a spurious trip 
occurs. 
Independent SO failures can also lead to a spurious trip. Each channel has inde-
pendent failure rate (1 — ßso)^so· Because there are n independent channels that 
can fail, the first independent SO failure occurs with rate n(l — ßso)^-so- This failure 
is assumed to be revealed immediately (e.g., by a local alarm), and a repair action is 
initiated to bring the channel back to a functioning state. Let the mean restoration 
time for a channel be MTTRgo · To get a spurious trip, at least k — 1 of the other n — 1 
channels must have a SO failure before the restoration action of the first channel is 
finished. This can be described as a binomial situation: 
(a) There are n — 1 independent channels. 
(b) Each channel will either survive the interval of length MTTRso without an SO 
failure or not survive the interval. 
(c) The probability, p, that a channel gets an SO failure in the restoration interval 
is 
p = 1 - e-O-feoUsoMTTRso % (l - £so)AsoMTTRso 
The number, M, of SO failures that occur in the restoration interval is therefore 
binomially distributed (n — 1, p) and a spurious trip occurs if M > k — 1. 
The spurious trip rate of a koon voted group due to internal failures is 
STR£°°n) = n(\ - β50)λ50 
-Pi(M>k-l)+ 
ßsoXso 
n-1 
n(\ -/öso)Aso 
+ ßso^so 
Σ 
.m=k-\ 
\n-\ 
\pm{\-pf 
(12.5) 
where p — (1 — /3so)^soMTTRso- As p usually is a very small number, 1 — p % 1, 
and the STR can be approximated by 
STR (toon) 
G,IF 
«(1 -ßso)Aso 
"(1 - ß s o H s o 
+ ßso^so 
Σ 
-m=k—1 
η—\ 
Σ 
-m=k—\ 
f n - f 
^ m > 
(η-ΐ 
ι m 
+ ßso^so 
l((l-£so)AsoMTTRsor 

QUANTITATIVE ANALYSIS 
373 
Because p is a very small number, pm+1 <£, pm for all m > 1. This means that the 
sum can be approximated by the first addend, such that 
S T R ^ n ) « n(l - ßso)Xso( 
\ ~_ [ V 
- ^so)AsoMTTRso]A:-1 
+ ßso-^so 
[(1 - |ÖSO)Aso]fc MTTR^1 + j W s o 
(12.6) 
Equation (12.6) is close to correct for k = 2, but slightly underestimates the 
spurious trip rate for A: > 3. To have a spurious trip due to independent SO fail-
ures, additional and independent SO failures have to occur while the initial failure 
is repaired. These failures may have restoration times that extend beyond the initial 
restoration time, but these restoration times are generally shorter than MTTRso be-
cause the repair personnel is already on site. The error caused by the approximation 
is considered to be negligible, because of the remote probability of having more than 
one additional, independent SO failure in the short interval of length MTTRso· 
The beta factor ßso and the restoration time MTTRso may obviously be different 
for different types of channels. When several types of channels are used in the same 
configuration, we need to use a more comprehensive approach (e.g., see Rausand & 
H0yland, 2004). 
The probability of multiple independent channel failures in a short interval is 
usually so small that it can be neglected, and we may therefore use the approximation 
STRg°^,) « 
fcoAso 
(12.7) 
for all values of 2 < k < n. 
fl 
EXAMPLE 12.6 STR for a 2oo3 voted group 
Consider a 2oo3 voted group of identical channels and assume that the following 
data has been provided: 
Symbol 
λβο 
ßso 
MTTRso 
= 
= 
= 
Value 
1.0· 10~5 per hour 
0.07 
6 hours 
Description 
SO failure rate for a channel 
Beta-factor for SO-CCFs 
Downtime of a channel after an SO failure 
With these input parameters, the probability p is 
o = l - e-(i-£soUsoMTTRso % 5 5 8 . 1 0 -5 
with this number of decimals, the approximation gives exactly the same proba-
bility. The independent part of (12.5) due to independent failures becomes 
STR G,IF,I « 3.11 · 10"9 per hour 
= n 

3 7 4 
SPURIOUS ACTIVATION 
The independent part of the approximation formula (12.6) gives the same result. 
The CCF part of the STR G,IF is 
STR G,IF,C = jßsoAso « 7.00 · 10~7 per hour 
The total STR for the 2oo3 voted group caused by internal failures (IF) is 
STR G,IF « STRG,IF,I + STRG,IF,C » 7.03 · 10~7 per hour 
* 
J-—'-* J-—'-* 
0.44% 
99.56% 
In this case, the CCF part of the STR gives an adequate approximation. 
φ 
12.5.4 Spurious Trips Due to False Demands 
Let ApDa be the rate of false demands of type (a), and let ApDb be the rate of false 
demands of type (b). When a false demand occurs, the SIF treats this demand as a 
real demand and carries out its safety function. There may be situations where the 
false demand is only detected by at most k — 1 channels in a koon voted group of 
input channels and therefore will not initiate the SIF. To be on the conservative side, 
the spurious trip rate of a SIF caused by false demands is determined by 
STRFD = (AFDa + AFDb)(l - PFDavg) 
(12.8) 
PDFavg is usually so small that 1 — PFDavg % 1 and can be omitted from (12.8), 
such that 
STRFD » (AFDa + AFDb) 
(12.9) 
12.5.5 Spurious Trips Due to DD failures 
Let ADD denote the rate of DD failures of a channel. When a DD failure is revealed, 
a repair action is initiated to restore the function. The mean restoration time is de-
noted MTTR. This restoration time may, or may not, be equal to the restoration time 
MTTRso f°r SO failures. The two downtimes may be different due to different 
priorities of the two types of failures. 
DD failures may be independent failures or common-cause failures. Also in this 
case, a beta-factor model is used for the CCFs with parameter βς,. If a CCF of DD 
failures occurs, with rate /JD^-DD, the system is not able to perform its safety func-
tion, and the system will be stopped. Several references, IS A TR 84.00.02 (2002); 
Smith (2005), treat this stop as a spurious trip. 
► Remark: Two different operational policies may be used: (i) The process may be 
immediately brought to a safe state when a dangerous SIF failure is detected, and (ii) 
the process is brought to a safe state when a dangerous SIF failure is detected if this 
failure is not restored within a specified time interval of length ÎQ. The first policy is 

QUANTITATIVE ANALYSIS 
375 
adopted in the following. The results derived may, however, be applied to the sec-
ond policy by multiplying the STR with the probability q of unsuccessful restoration 
within time to- 
Θ 
Consider a koon voted group of identical channels and assume an operating pol-
icy where the SIF is (spuriously) tripped as soon as at least n — k + 1 dangerous 
failures are detected. Dangerous detected CCFs (with multiplicity n) always lead 
to shutdown. First, consider the independent DD failures (i.e., with multiplicity 1). 
The first independent DD failure occurs with rate n(\ — βϋ)^ΌΌ- The failure is 
revealed almost immediately and a repair action is initiated. To have a spurious 
shutdown, at least n — k of the remaining n — 1 channels must get a DD failure 
before the restoration of the first DD failure is finished. In the same way as for SO 
failures, this can be treated as a binomial situation where the number M* of DD 
failures in the interval of length MTTR is binomially distributed (n — 1, p*), where 
p* = l - e-Ü-foUDDMTTR % ( 1 _ jßD)ADDMTTR. 
The spurious trip rate of a koon configuration of channels due to DD failures is 
S T R ^ ' = n{\ - βΌ)λΌΌ 
-PT(M*>n-k)+ 
βΌλΌΌ 
n-l 
n{\ - 
βΌ)λΏΏ 
Σ 
(n
m
1)(p*r(i-P*r-1-m 
-m=n—k 
+ βΌλΏΌ 
(12.10) 
where p* = (1 - 
βΏ)λΌΌΜΤΐ^. 
By the same arguments as used to derive (12.6), the STRG.DD is approximately 
S T R ^ « « Γ " M [(1 - / J D A D D ^ M T T R * - 1 + £ D A D D 
(12.11) 
Most often, the contribution from the independent DD failures is so small that the 
approximation (12.11) can be used. 
STR^°on) % £DADD 
(12.12) 
β 
EXAMPLE 12.7 STR caused by DD failures for a 2oo3 voted group 
Consider a 2oo3 voted group (G) of identical channels and assume that the 
dataset in Table 7.2 is applicable. The probability of a channel DD failure in 
an interval of length MTTR is p* = 4.56 · 1(Γ5. The STR G,DD is 
STRg°^ = STRG,DD,I + STRG>DD,C = 3.02 · 10"7 per hour 
* 
v 
' 
* 
v 
' 
0.52% 
99.48% 
The contribution from independent channel DD failures, STRQ,DD,I is seen to 
be negligible compared to the contribution STRG,DD,C from DD-CCFs. The 

376 
SPURIOUS ACTIVATION 
STRQ,DD for the 2oo3 voted group can therefore be adequately approximated 
by 
S T RG, 0DD % ^DADD = 3.00 · 10"7 per hour 
► Remark: The derivation above is based on the assumption that a spurious trip 
occurs whenever a SIF DD failure occurs. As mentioned in the first remark in this 
section, another operational policy would be to trip the process only if the SIF can-
not be restored within a specific time interval of length to- The probability q of a 
not successful restoration depends on the maintainability of the channels and on the 
available maintenance resources. With this policy the spurious trip rate due to DD 
failures would be 
; > 1 KDD,T — à l K D D 
<Z 
where STRQQ 0" is the "total" spurious trip rate due to DD failures and STRJ-,ρ 
is the spurious trip rate in (12.11). 
Θ 
12.5.6 Spurious Trip Rate by the PDS Method 
The PDS method (SINTEF, 2013b) does not distinguish between different types of 
spurious activations and determines STR by a formula similar to (12.7) as 
STR^oon) % C^k+l)oon 
βλ3υ 
for2< k<n 
(12.13) 
where Asu is the rate of safe undetected failures and C(n-fc+i)oon is the correction 
factor used by the PDS method in the multiple beta-factor model for CCFs. The 
same correction factor as for CCFs involving dangerous failures is used. Numerical 
values for the correction factor for various values of k and n are given in Table 8.8. 
It should be noted that the PDS method applies the rate of safe undetected failures 
instead of the rate of SO failures. In most applications, however, these two rates are 
similar. 
As discussed in Section 12.2.3, a koon voted group with respect to the safety 
function is a (n — k + l)oo/z voted group with respect to the function "avoid spurious 
trip." This is the reason why the correction factor related to spurious trips for a koon 
voted group is C(„-k+i)oon. 
fl 
EXAMPLE 12.8 STR for different configurations by the PDS method 
Consider a 2oo3 voted group of identical channels. The spurious trip rate of the 
2oo3 voted group is by the PDS method 
STR(2oo3) = C 2 o o 3 j ß A s u = 2.0 · βλ3υ 
If an extra channel is added to get a 2oo4 voting, the spurious trip rate becomes 
STRG 
= C3oo4jßAsu = 2.8 · βλςυ 

QUANTITATIVE ANALYSIS 
377 
If the voting is changed to 3oo4, the spurious trip rate becomes 
STRG(3oo4) = C2oo4JÖAsu = 1.1 ■ £ASU 
Θ 
The same symbol ß is used in the PDS method for both dangerous CCFs and for 
safe CCFs, but the PDS method warns that these do not necessarily have the same 
value. 
12.5.7 Markov Approach 
The Markov approach can also be used to determine the spurious trip rate. The 
approach is a simple extension of the approach that was described in Chapters 5, 8, 
and 9 and is here illustrated only by a small example. 
fl 
EXAMPLE 12.9 Two Independent and Identical Channels Voted loo2 
Consider a loo2 voted group of independent and identical channels that can 
have DU failures, but no DD failures are possible. The channels are also ex-
posed to SO failures with failure rate Aso- Assume that a channel cannot get an 
SO failure when it already has a DU failure. The voted group is a subsystem of 
a SIS. Because the architecture is loo2, the system goes to a safe state as soon 
as one of the two channels gets an SO failure. In the safe state, the subsystem 
is de-energized and additional failures cannot occur. When an SO failure has 
occurred, a repair action is initiated to bring the subsystem back to a fully oper-
ational state (i.e., state 0). The mean associated downtime is MTTRso and the 
corresponding repair rate is μςο- A Markov model for the subsystem is shown 
in Figure 12.3. The relevant symbols and data in Table 7.2 is used. In addition, 
assume that Aso = 1.0 · 10~5 per hour and that MTTRso = 6 hours. 
The possible states of the subsystem are: 
State 
State description 
0 
Both channels are able to function (i.e., do not have a DU fault) 
1 
One channel is able to function and the other has a DU fault 
2 
Both channels have DU faults 
3 
The subsystem is in a safe state 
In state 0, both channels can get an SO failure and the transition rate from 
state 0 to state 3 is therefore 2Aso- In state 1, one channel has a DU failure and 
cannot get an SO failure such that the transition rate from state 1 to state 3 is 
Aso- The repair rate of a single DU failure is μι,ου = 1/ (t/2 + MRT) and 
the repair rate of a double DU failure is /^2,DU = 1/ (j/3 + MRT). 

378 
SPURIOUS ACTIVATION 
Figure 12.3 
Markov model for a subsystem with two independent and identical channels 
voted loo2, where only DU and SO failures are considered. 
The transition rate matrix for the loo2 voted group is 
—(2ADU + 2Aso) 
2ADU 
0 
2Aso 
MI,DU 
— (μι,ϋυ + λου 
+ Aso) 
λου 
Aso 
M2,DU 
0 —ß2,DU 
0 
\ 
Mso 
0 
0 
- M S O / 
The steady-state equations can be solved in the same way as in Chapters 5 
and 8 to obtain the steady-state probabilities: 
P0 = 9.916 -ΚΓ 1 
Pi = 8.30 · 1<Γ3 
P2 = 2.43 · 10-5 
P3 = 1.19· 10-4 
The PFDavg of the loo2 voted group is 
PFD^°o2) = P2 = 2.43 · 10"5 
No states are absorbing and the (steady-state) spurious trip rate of the loo2 
voted group is therefore the frequency of transitions into state 3. 
STR = P0 ·2Α3ο + Λ -Aso = 1-99-10~5 per hour 
(12.14) 
A spurious trip caused by the loo2 voted group will, on the average, occur every 
1/STR = 50215 hours, that is, every 5.73 years. 
Θ 

ADDITIONAL READING 
379 
Table 12.2 
Spurious trip levels. 
STL 
Description 
6 
Spurious trip costs higher than 20M EUR 
5 
Spurious trip costs between 10M and 20M EUR 
4 
Spurious trip costs between 5M and 10M EUR 
3 
Spurious trip costs between IM and 5M EUR 
2 
Spurious trip costs between 500k and IM EUR 
1 
Spurious trip costs between 100k and 500k EUR 
12.5.8 Spurious Trip Levels 
When selecting architecture for a SIS, it is necessary to weigh the safety integrity 
with the loss of production due to spurious trips. A loon architecture has a high 
safety integrity, but many spurious trips and a significant loss of production. 
The safety integrity is classified into one out of four safety integrity levels (SILs), 
but a similar classification has not been available for loss of production. This led the 
company RISKNOWLOGY® to propose a set of spurious trip levels (STLs).4 The 
levels are defined based on the mean cost of the spurious trips. It is up to the user 
to choose the number of levels to use and also the cost boundaries. An example is 
presented in Table 12.2. 
The STL approach has been criticized because the levels are defined solely based 
on economic loss and because safety and other noneconomic impacts are not consid-
ered. 
12.6 Additional Reading 
The following titles are recommended for further study related to Chapter 12: 
- Spurious activation of safety instrumented systems in the oil and gas industry: 
Basic concepts and formulas (Lundteigen & Rausand, 2008b). The presentation 
in this chapter is mainly based on this paper. 
- Reliability prediction methods for safety instrumented systems, PDS method 
handbook (SINTEF, 2013b) provides simple formulas for the determination of 
the spurious trip rate. 
- Estimation of average hazardous-event-frequency for allocation of safety-inte-
grity levels (Misumi & Sato, 1999). This paper presents many interesting views 
and is a good supplement to this chapter. 
http ://www.risknowlogy.com 

3 8 0 
SPURIOUS ACTIVATION 
- Petroleum, Petrochemical and Natural Gas Industries - Reliability Modeling 
and Calculation of Safety Systems (ISO/DTR 12489, 2012). Draft to technical 
report. This technical report describes several ways to determine the spurious 
trip rate based on the Markov approach. 

CHAPTER 13 
UNCERTAINTY ASSESSMENT 
13.1 
Introduction 
Uncertainty assessment of reliability estimates has not been thoroughly addressed in 
industrial applications. The exception is the nuclear power industry, where guide-
lines for probabilistic risk/safety assessment (PRA/PSA) prescribe uncertainty as-
sessment. The uncertainty of the reliability estimates of a SIF is discussed briefly in 
Annex B6 of IEC 61508-6, but the discussion concerns only parameter uncertainty. 
Many authors discuss uncertainty in risk and reliability analysis, but very few 
discuss aspects related directly to the reliability of a SIF, and these are mainly limited 
to the uncertainty of input parameters (Wang et al., 2004). Other types of uncertainty 
are not addressed, even though they may have significant influence on the reliability 
estimates. Jin et al. (2012) are among the few who investigate the uncertainty related 
to reliability estimates of a SIF from an overall perspective. 
Many suppliers claim compliance to IEC 61508 when they come up with a calcu-
lated reliability that is within the ranges of the requirements in the standard, without 
being concerned with the uncertainty of the PFDavg or PFH estimates. 
The author's point of departure is that he does not believe that it is possible to 
quantify the uncertainty of a PFDavg or a PFH estimate in any objective way. The 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
381 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

3 8 2 
UNCERTAINTY ASSESSMENT 
person most capable of making judgments about the uncertainty is the analyst, and 
she should communicate to the decision-makers her "degree of belief" about the 
uncertainty together with the results from the reliability analysis of the SIF. Her 
"degree of belief" must be communicated qualitatively and supplemented by some 
quantitative arguments. 
To simplify the presentation, only SIFs operated in low-demand mode are dis-
cussed in this chapter, and most discussions are related to process industry applica-
tions. 
13.2 What Is Uncertainty? 
13.2.1 What Do We Mean by Uncertainty? 
Uncertainty is a common word in our daily parlance, but is used with different mean-
ing in different contexts. Related to reliability and risk assessments, the interpreta-
tion of uncertainty is still debated (e.g., see Aven, 2011 ; Rausand, 2011). According 
to the author's view, probabilities in risk and reliability assessment must be inter-
preted as subjective probabilities, where a prior probability distribution is used to 
describe our uncertainty. This also applies for the PFDavg and the PFH. We use the 
knowledge available to select appropriate models and input parameters, calculate a 
value for PFDavg (PFH), and call this value our PFDavg (PFH) estimate. In this 
process, we are aware that we make a lot of simplifications and approximations that 
influence the PFDavg (PFH) estimate and make our estimate uncertain. As part of the 
reliability analysis of a SIF, we should assess this uncertainty and communicate our 
assessment to the decision-maker. The objective of this chapter is to give guidance 
to analysts on issues to be aware of when assessing the uncertainty of the PFDavg 
(PFH) estimate. 
Uncertainty is the imperfect knowledge about the individual aspects of a system as 
well as the overall inaccuracy of the output determined by the system. 
Uncertainty may stem from two main causes, natural variation and the lack of 
knowledge about the system or process. These categories of uncertainty are referred 
to as aleatory and epistemic uncertainty, respectively (e.g., see NUREG 1855, 2009). 
- Aleatory uncertainty: Uncertainty arising from or associated with the inherent, 
irreducible, and natural randomness of a system or process. 
- Epistemic uncertainty: Uncertainty due to lack of knowledge about the perfor-
mance of a system or process. 
The epistemic uncertainty will be reduced when new knowledge becomes available, 
while the aleatory uncertainty cannot, in principle, be reduced. However, several 
types of uncertainty that in the past were classified as aleatory are now considered 
to be epistemic, indicating that the uncertainty classification is not fixed but may 
vary as fundamental understanding of natural phenomena increases. Some authors 

COMPLETENESS UNCERTAINTY 
3 8 3 
therefore take the stand that all uncertainty is epistemic (Aven, 2008; Kieureghian 
& Ditlevsen, 2009). Despite its limitations, the classification gives a conceptual 
allocation of uncertainties into controllable and not so easily controllable categories. 
Statistical models are used as a basis for estimating risk and reliability parameters, 
for example, to determine the frequency of accident scenarios in a quantitative risk 
assessment and the reliability of SIFs. By using statistical models for this purpose, 
we acknowledge the existence of uncertainty. Epistemic uncertainty as a source of 
uncertainty is related to how well our models and data are able to capture the behavior 
of the system. An additional process can address this uncertainty. In this chapter, we 
use uncertainty assessment to account for the epistemic uncertainty of the reliability 
performance of the SIF, that is, of the PFDavg (PFH) estimate. 
13.2.2 How Can Uncertainty Be Presented? 
If it is relevant to quantify the uncertainty of a PFDavg estimate, several measures 
can be used: 
(a) A standard deviation for PFDavg 
(b) A confidence interval, for example, given by 
Pr (PFDavgiL < PFDavg < PFDavg>u) > 0.90 
(c) A probability of meeting a target PFDavg>target, for example, 
Pr (PFDavg < PFDavg,target) > 0.95 
(d) As a (subjective) distribution of PFDavg 
13.2.3 Uncertainty Classification 
The nuclear industry (NUREG 1855, 2009) distinguishes between three sources of 
epistemic uncertainty: completeness uncertainty, model uncertainty, and parameter 
uncertainty. The same categories are adopted in this chapter and each of them is 
discussed in the following. 
13.3 Completeness Uncertainty 
Completeness uncertainty is about factors that are not properly included in the anal-
ysis. Failing to include all relevant factors in the analysis gives an incorrect estimate 
of the reliability, even if the data and model selections are "perfect." We may distin-
guish between: 
1. Known completeness uncertainty, which is due to factors that are known, but 
deliberately not included. Reasons for exclusion may be lack of understanding 

384 
UNCERTAINTY ASSESSMENT 
Table 13.1 
The degree of newness of technology (DNV-RP-A203, 2011 ). 
Experience with the 
operating condition 
Previous experience 
No experience by company/user 
No industry experience 
Proven 
1 
2 
3 
Level of technology maturity 
Limited field history or not 
used by company/user 
2 
3 
4 
New or 
unprover 
3 
4 
4 
the limitations of the system in it's operating context, time or cost constraints, 
lack of models, lack of data to support the models, or lack of competence in 
using the models. 
The known completeness uncertainty reflects assumptions and simplifications 
that have been made in a trade-off of costs, available resources, competence of 
analysts, and the state of knowledge about the system and its operating environ-
ment. The number (or extent) of exclusions may indicate the level of uncertainty 
in a PFDavg estimate. 
2. Unknown completeness uncertainty, which is due to factors that are not known 
or not identified. This category is sometimes called ignorance. The factors are 
truly unknown, and are therefore difficult to account for or make judgments 
about. 
The unknown completeness is problematic, as its contribution is invisible. In-
direct factors that impact the extent to which "we don't know," may give an 
indication of the contribution. The use of new technology, or the use of existing 
technology in new application areas may suggest that the contribution from un-
known completeness uncertainty is high compared to when proven technology 
is used. The classification in Table 13.1 of the newness of technology may be 
useful here. In Table 13.1, the newness is classified in four categories, rang-
ing from 1 to 4, where 4 represents the newest and least familiar technology. 
The E/E/PE technology is developing fast and newer and more advanced logic 
solvers and smart transmitters are launched and implemented at a fast pace. 
Additional factors/issues that may influence the (known and unknown) complete-
ness uncertainty include: 
- Interactions with external systems: A SIF is integrated in an EUC and a big-
ger system (e.g., the process) and may interact with the system it is protecting 
and with other safety and control systems. These interactions may influence 
(enhance or reduce) the reliability of the SIF. 
- Failure mechanisms that are not known and/or not taken into account: Such 
failure mechanisms may be related to stresses during operation and mainte-

COMPLETENESS UNCERTAINTY 
385 
nance, and to environmental conditions. Failure mechanisms may also be for-
gotten due to inadequate failure analysis (e.g., FMECA). 
- Side effects of diagnostic testing: Diagnostic testing is a means to timely dis-
covery of dangerous faults, thereby improving the reliability of the SIE Whether 
or not such testing leads to side effects is seldom evaluated. 
- Placement of input elements (e.g., sensors, transmitters): Installing input ele-
ments at places other than where they should be may expose the components to 
different environmental stresses, and hence the components may demonstrate a 
different reliability behavior. 
- Testing and maintenance strategies: Complex testing and maintenance strate-
gies are difficult to model in SIF reliability analyses and are therefore often 
simplified in the calculations. This may increase the completeness uncertainty. 
- Human and organizational factors: Several studies have indicated that human 
and organizational factors influence the reliability of the SIF (Rahimi et al., 
2011; Schönbeck et al., 2010), but such factors are usually not included in the 
analyses. 
13.3.1 Systematic Failures 
IEC 61508 does not require systematic failures to be taken into consideration in 
the quantitative reliability analyses of a SIF. This inevitably means that the "real" 
PFDavg will be lower than the PFDavg estimate. The PDS method (SINTEF, 2013b) 
proposes an approach to include, at least some, systematic failures, but this approach 
is still not fully adequate. 
13.3.2 
Complexity 
Johansen & Rausand (2011) define complexity as "a state of difficulty in determining 
the output of a system based on knowledge about individual inputs and given our 
current knowledge base." In a model "world," the output can be a risk or reliability 
estimate, such as the PFDavg. Since the E/E/PE technology develops so fast, the 
knowledge base may not always follow the pace. Due to complexity, the analyst 
may not fully understand the behavior of the system (both technical and operational) 
and may therefore not be able to establish adequate models. 
Johansen & Rausand (2011) list a set of complexity attributes related to (i) the 
physical system, and (ii) the operation of the system. These attributes can be used to 
indicate the complexity and may be a guide to the awareness we should have related 
to complexity. 

386 
UNCERTAINTY ASSESSMENT 
13.4 Model Uncertainty 
Model uncertainty arises from the fact that any model, conceptual or mathematical, 
is always a simplification of the reality it is designed to represent (e.g., see Mosleh 
et al., 1995). Several authors discuss the choice of models for SIF reliability analysis 
and give recommendations. One of the first articles with this purpose is Rouvroye & 
Brombacher (1999). The models required may be split into two categories; compo-
nent models and system models. 
13.4.1 Component Models 
Almost all SIF reliability analyses assume that the channels have constant failure 
rates. This assumption implies that the channels do not show any deterioration and 
that they are as-good-as-new as long as they are functioning. This assumption may 
be adequate for electronic and some electrical items, and for mechanical items that 
are regularly maintained and upgraded/replaced if deterioration is revealed. 
In some applications, such as in subsea oil and gas production systems, the items 
are left alone for a long time (e.g., five years) without any type of preventive main-
tenance. Several of these are mechanical items that are exposed to seawater, high 
pressure, and a corrosive environment. This indicates that the items deteriorate with 
time and that the constant failure rate assumption is not adequate. If, for example, 
the life distribution of a component is Weibull distributed with a shape parameter 
that is greater than one, and we use a constant failure rate model, we overestimate 
the probability of failure (i.e., be conservative) in the first part of the item's life and 
underestimate the probability of failure in the last part of the item's life. A deterio-
rating item is not as-good-as-new after a successful proof test. This problem is very 
seldom accounted for in SIF reliability analyses. 
Component models must also take into consideration issues related to testing and 
maintenance, such as the coverage of diagnostic tests, the coverage of proof tests, 
the possible use of partial stroke testing of shutdown valves (Lundteigen & Rausand, 
2008a), and several more. 
13.4.2 System Models and Methods 
Several models/methods are listed in Section 7.7. The sequence in which the meth-
ods are listed indicates their applicability to an increasing system complexity. The 
first five methods consider the SIS to be a static system without any dynamic prop-
erties, while the Markov approach and the Petri net approach can incorporate some 
dynamic effects due to testing and maintenance. 
The most adequate model/method is determined by the assumptions and simpli-
fications that are made. As long as the analyst is competent and familiar with the 
different methods, it does not matter very much whether she chooses the simplest 
method that fits the assumptions or a more advanced method. The approximation 
formulas give the most conservative estimates and the conservativeness decreases 
with increasing method complexity—which should be expected since the complex 

PARAMETER UNCERTAINTY 
3 8 7 
methods are based on more detailed modeling. Too few and simple systems are, 
however, analyzed to give any firm and general conclusions. The different methods 
are also studied and compared by Rouvroye & Brombacher (1999) and Rouvroye & 
van den Bliek (2002). 
Several authors have studied model uncertainty. Among these are Zio & Aposto-
lakis (1996) and Droguett & Mosleh (2008). The causes (assumptions and simplifi-
cations) of model uncertainty are the same as for known completeness uncertainty. 
It is therefore not obvious that uncertainties can uniquely be classified as model un-
certainty or known completeness uncertainty. Some sources, such as PDS method 
(SINTEF, 2013b), do not differentiate between completeness uncertainty and model 
uncertainty, but use the term model uncertainty to represent both. 
CCF Models. 
It is often claimed (Lundteigen & Rausand, 2007) that CCFs are 
more crucial for the reliability of a SIF than independent channel failures. Therefore, 
how CCFs are incorporated into the methods is important. In some methods, we 
may choose among the available CCF models (Hokstad & Rausand, 2008), other 
methods come with a dedicated CCF model (the PDS method), while some methods 
are mainly based on explicit modeling of CCFs. To choose the "best" CCF model is 
a difficult task and the analysts therefore too often select the simplest possible model, 
which is the beta-factor model. The beta-factor model is a suggested CCF model in 
IEC 61508-6, and the choice of this model is therefore compatible with the standard 
and usually leads to conservative PFDavg estimates. 
In most cases, other dependencies than CCFs are not covered in the reliability 
analyses of a SIF. Among such dependencies are cascading failures and negative 
dependencies. A special challenge is related to modeling dependencies that are partly 
within and partly between channels and modules of the SIS. 
Modeling Dynamic Effects. 
Dynamic effects may be related to phased mission, 
the effect of DD-failures and safe failures, and testing procedures. An example of 
such a testing procedure is to carry out a full proof test of similar channels each 
time a DD-failure is restored - either in addition to the planned proof test or by 
postponing the planned proof test. Another example is to supplement the proof test 
by regular inspections (e.g., once a week) where some failure possibilities may be 
partly examined. The inspection may, for example, involve moving a valve slightly 
to check that it is not stuck. 
13.5 Parameter Uncertainty 
Parameter uncertainty is related to uncertainty of the parameter values used in the 
quantification. In the current context, these parameters are component failure rates, 
mean repair times, common-cause beta-factors, test coverage factors, and so on. Fail-
ure rates are available in data sources, such as OREDA (2009) and SINTEF (2013a).1 
1A survey of reliability data sources is given at h t t p : //www.ntnu . e d u / r o s s / i n f o/data. 

3 8 8 
UNCERTAINTY ASSESSMENT 
Estimates of some of the other parameters may be found in SINTEF (2013a), partly 
based on expert judgment. 
Reliability data sources and challenges related to data sources are discussed in 
Chapter 6. Several uncertainties are related to the provision of input parameters, and 
some few of these are discussed in this section. 
13.5.1 
Failure Rates 
Most SISs are designed to be highly reliable. Few failures are therefore expected to 
occur even during a long operating period, such that failure rate estimates based on 
experience data become rather uncertain. Another problem is that the failure rates 
we find, for example, in OREDA (2009), are based on data from components that 
were installed several years (often 10-15 years) before the data collection exercise 
was terminated. Due to the rapid technological development of, for example, smart 
sensors, the failure rate estimates may not at all represent the technology that will be 
used in the new SIS. 
The operational and environmental conditions of the elements used in a new SIS 
are sometimes very different from the conditions under which the data were col-
lected. For electronic components, this is handled by the approach outlined in MIL-
HDBK-217F (1995). For complicated equipment, the approach outlined in MIL-
HDBK-217F is too simple and we have to use more advanced approaches, such as 
the one described by Brissaud et al. (2010). 
OREDA (2009) and SINTEF (2013a) are both based on the maintenance actions 
found in the maintenance reporting system. Only failures for which a work-order 
(or job-order) has been issued are normally recorded in the maintenance reporting 
system. Because some safe failures can be rectified without any formal maintenance 
action, they may not be included in the data source. 
70% Confidence Level. IEC 61508 and IEC 61511 give few requirements that ad-
dress uncertainty of the PFDavg or the PFH, but the standards add conservatism to the 
reliability estimation, by requiring that the failure rates used should have a 70% con-
fidence level.2 To meet this requirement, it is necessary to consider the failure rate 
as a random variable Λ with a prior probability distribution f(X) that describes our 
knowledge/belief about the failure rate. The value λ„ that is used in the calculations 
must fulfill Pr(A < Xu) > 0.70, to have a 70% confidence level. The approach is 
illustrated in Figure 13.1. SINTEF (2013a) presents some "best estimate" (or mean) 
ADUS together with estimated 70% upper confidence values to illustrate the differ-
ence between the two values. The difference depends on the actual data available 
and the model that is chosen to describe our uncertainty. 
IEC 61508 also requires that a confidence level of at least 90% shall be demon-
strated on the reliability estimates in the selection of hardware architectures. 
2See paragraph 7.4.9.5 in IEC 61508-2, and paragraph 11.9.2 in IEC 61511-1. 

PARAMETER UNCERTAINTY 
389 
Figure 13.1 
Illustration of failure rate with 70% confidence limit. The figure is not drawn 
to scale. 
Alternative Approach. 
Some suppliers use "best estimates" but add a similar con-
servatism by making the SIL requirement more strict, such that compliance with, for 
example, SIL 3, is only claimed when PFDavg < 0.7 · 10-3. 
13.5.2 Common-Cause Failure Rates 
Very few data sources are available for CCF rates. The only exception is the nu-
clear power industry, which has established the International Common-Cause Data 
Exchange database (NEA, 2004). The author is not aware of any similar initiative 
for any non-nuclear sector. CCF rates are highly dependent on the physical condi-
tions and the operational and environmental conditions and it is therefore difficult to 
claim that a CCF rate in one installation will be similar to the CCF rate in another 
installation. 
IEC 61508 has therefore chosen another approach where the factor ß of the beta-
factor model is determined based on a checklist of 37 questions and a calculation 
procedure described in Annex D of IEC 61508-6. The checklist is generic and is 
intended for all types of SIS. An immediate observation is that the number of ques-
tions related to human and organizational factors is low compared to the importance 
of these factors. This is further discussed by Rahimi et al. (2011). 
A simpler checklist and calculation procedure is provided in Annex F of IEC 62061 
(2005) for machinery systems. 
Several other approaches may be used to determine the factor ß. One of these is 
the unified partial method (UPM) that has been extended by using influence diagrams 
(see Chapter 10). 
13.5.3 Test Coverage 
Very little guidance is available on how to estimate the test coverage factor, both 
for diagnostic and proof testing. An exception is the PDS data handbook (SINTEF, 
2013a) that presents coverage factors determined by expert judgment. 
The coverage factors used in many reliability analyses of a SIF are therefore at 
best guesstimates. 

3 9 0 
UNCERTAINTY ASSESSMENT 
13.5.4 
Uncertainty Propagation 
Parameter uncertainty is the most studied type of uncertainty (Abrahamsson, 2002; 
Rausand, 2011; Thunnissen, 2005) and is usually carried out by Monte Carlo sim-
ulation. An uncertainty distribution—sometimes expressed by an error factor— is 
given for the main parameters, a value from each distribution is chosen at random, 
and an output value is generated. This is repeated a high number of times and we say 
that the uncertainty is propagated through the model to give an uncertainty distribu-
tion of the output measure of interest (in our case, the PFDavg) (Stamatelatos et al., 
2002). Such a simulation module is a separate module of many computer programs 
for reliability analysis, such as fault tree analysis. 
In addition to uncertainty propagation, sensitivity analysis can be used to rank the 
impact of parameters and facilitate the uncertainty propagation. Sensitivity analysis 
is treated in many books (e.g., see Rausand, 2011), so we do not discuss this any 
further here. 
13.6 Concluding Remarks 
This chapter has treated some main issues related to the uncertainty of the PFDavg 
estimate for a SIF operating in low-demand mode. The sources of the uncertainty 
are classified into completeness, model, and parameter uncertainty and each category 
has been briefly discussed. It is argued that uncertainty assessment is an important 
part of a SIF reliability analysis and that the uncertainty should be communicated to 
the relevant decision-makers together with the PFDavg estimate. 
The person who is most capable of assessing the uncertainty is the analyst who 
knows how the various attributes of the SIS are implemented in the models and in the 
analyses. We do not believe that it is possible to present any objective estimate of the 
uncertainty, so the analyst has to judge the different contributors to the uncertainty 
and present her "degree of belief." 
Of the three categories, the author judges completeness uncertainty to be the most 
important. This category is split into two subcategories; known and unknown com-
pleteness uncertainty. For the known completeness uncertainty, the analyst is aware 
of the relevant issues and has deliberately excluded them from the analysis. Using 
conservative approximations can in some cases, compensate for this type of simplifi-
cations. For the unknown completeness uncertainty, the analyst does not know what 
she does not know and does not include. This uncertainty is most prevalent for new 
technology and partly known technology in new areas of application. The analyst 
may be warned about the possible uncertainty by using the classification of newness 
proposed in DNV (2008). 
Model uncertainty is linked to completeness uncertainty. The choice of model 
and method is strongly dependent on the assumptions and simplifications made. If 
the analyst is competent and familiar with the limitations of the various methods, it 
is not very important which method she chooses, as long as the method fits to the 
assumptions made. 

ADDITIONAL READING 
391 
Parameter uncertainty is considered to be more important than model uncertainty. 
The technological development of the E/E/PE technology is moving fast, and the 
failure rate estimates in data sources may therefore be outdated. Another issue is 
that the failure rate estimates may not fit to the current operational context and we 
may need to extrapolate the estimates from the known to the new application. An 
approach for this purpose is outlined by Brissaud et al. (2010) and is described briefly 
in Chapter 6. 
CCFs often have a dominating influence on the reliability of a SIF and it is there-
fore essential to have adequate estimates of CCF rates. Most reliability analyses of 
a SIF use the beta-factor model or the multiple beta-factor model. We have argued 
that the beta-factor depends on the physical system and the operational context and 
that collection of CCF data therefore may not an adequate beta-factor for a specific 
system. Today, the estimation of the beta-factor is mainly based on the checklist and 
the calculation procedure in IEC 61508-6. This checklist has certain deficiencies and 
may not give a fully realistic estimate of the beta-factor. 
This chapter has only addressed uncertainty for low-demand systems. High-
demand systems may be treated in a similar way but also have specific features that 
are not covered here. 
13.7 Additional Reading 
The following titles are recommended for further study related to Chapter 13: 
- Guidance on the Treatment of Uncertainties Associated with PRAs in Risk-
Informed Decision Making (NUREG 1855, 2009) is concerned mainly with risk 
analyses of nuclear power plants but is also a valuable source of information for 
other application areas. 
- Uncertainty in Industrial Practice: A Guide to Quantitative Uncertainty Man-
agement (De Rocquigny et al., 2008) gives a thorough treatment of many as-
pects of uncertainty and sensitivity analysis. 
- Risk Assessment; Theory, Methods, and Applications (Rausand, 2011, Chapter 
16) gives an introduction to uncertainty assessment in risk analysis. 
- Uncertainties in Quantitative Risk Analysis: Characterisation and Methods of 
Treatment (Abrahamsson, 2002). This Ph.D. thesis gives a good introduction to 
many different areas of uncertainty analysis. 
- Uncertainties in risk analysis: Six levels of treatment (Paté-Cornell, 1996) dis-
cusses several ways of treating uncertainty in risk analysis. 


CHAPTER 14 
CLOSURE 
14.1 
Introduction 
You have now come to the end of this book and I hope you have got some benefits 
from your efforts. I also hope that you realize that you have just touched upon a 
sizable and challenging area. 
The book has presented the main approaches to reliability assessment of a SIF, 
presented the main formulas, tried to give the rationale for each method, and de-
scribed some main pros and cons related to each method. The book is no replace-
ment for IEC 61508 and its sector-specific standards inasmuch as functional safety 
and safety integrity embrace much more than what is presented in the book. I hope, 
however, that this book can be considered a useful supplement to the standards, and 
also that the book can give insight into aspects of reliability assessment that are not 
covered in the standards. I have tried to keep the descriptions of the various ap-
proaches as simple as possible and to illustrate the approaches with many worked 
examples. The book only covers a small fraction of reliability assessment of a SIF, 
because all software issues have been disregarded. The same applies for human and 
organizational issues and the large area of systematic faults. 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
393 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

394 
CLOSURE 
14.2 Which Approach Should Be Used? 
I fully realize that many companies are operating in a very competitive environment 
and that they have to cut costs wherever they can. Against this background, one may 
ask why they should spend valuable time reading this book. Is it not sufficient to 
calculate the PFDavg and the PFH by the simplest possible formulas, as long as the 
client accepts this? 
The Simplest Approach. 
The quickest and simplest approach for determining the 
PFDavg or PFH for a SIF is the following: 
1. Provide a set of input data (such as Table 7.2). Relevant data sources are listed 
in Chapter 6. 
2. Enter the IEC formulas into a spreadsheet program (or another suitable pro-
gram) with one entry for each voting. 
3. Select the architecture (i.e., the voting) 
4. Edit the input data to fit your application 
5. Press "execute" 
6. Voila, you have the result! 
By this approach, you have used the formulas provided in IEC 61508 and you may 
claim compliance with the standard. The approach is fast and effective and you do 
not need to have any understanding of the concepts and the underlying methods (i.e., 
you have wasted your time reading this book). The final step in this undertaking 
is to compare the result with the SIL tables and conclude whether or not the SIL 
requirement is met. 
The simple examples that have been used to illustrate the various formulas in this 
book have given surprisingly similar results. This may indicate that the choice of 
method is not important, at least for simple systems. So, why not use the simplest 
approach? 
Complexity and Adaptability. 
A SIS can range from a simple unit with a sin-
gle channel in each subsystem up to a very complex system with a high number of 
channels and a network of logic solvers. An example of a complex SIS is the total 
ESD system on an offshore oil and gas production platform. The operational strate-
gies for SISs are very different with respect to which faults are temporarily tolerable 
and which require immediate shutdown. The shutdown strategies during testing and 
repair may also be very different. The spectrum of options requires the analytical 
approach be flexible and adaptable to the specific application. 
The IEC formulas are presented as single formulas in IEC 61508-6 without any 
explanation or justification. Some assumptions are presented, but they are not easily 
noticed when reading the standard. Many users tend to believe that the IEC formulas 
are generally applicable and always correct, which is not the case. The basis for the 

REMAINING ISSUES 
395 
IEC formulas is intuitive and adequate, but it is not easy to adapt the formulas to 
changes in the assumptions. It is, for example, not easy to see how the IEC formulas 
can be modified to be applied to voted groups of diverse channels. It is, further, not 
obvious how to split the PFDavg into unknown and known unavailability, as is done 
by the PDS method. If you have a thorough knowledge in reliability theory, you 
might be able to make the required adaptations and, in that case, the IEC formulas 
are not a bad choice. 
The simplified formulas are, from the author's point of view, easier to understand 
and adapt to special applications. It is, for example, rather easy to apply the simpli-
fied to voted groups of diverse channels (as done in Chapter 8). The main drawback 
with the simplified formulas is the difficulty of incorporating the effects of DD fail-
ures in a proper way. In most cases, however, the DD failures have a negligible 
effect on the PFDavg and the PFH, such that an accurate modeling of DD failures 
may not be required. The IEC formulas and the simplified formulas provide slightly 
conservative results and can, in principle, be used to analyze SISs on any level of 
complexity. 
To establish a fault tree is an intuitive process that many reliability engineers are 
familiar with. The fault tree approach is therefore the first priority of many users. A 
problem with the fault tree approach is that the standard algorithms for quantitative 
fault tree analysis give nonconservative approximations for PFDavg and PFH. 
The Markov and Petri net approaches require more analytical skills and are mainly 
suitable for small and complex parts of the SIS. These approaches are especially 
suitable for the analysis of logic solver subsystems and voted groups with dynamic 
features. 
In this book, you have seen that the IEC formulas and the simplified formulas and 
the approaches based on reliability block diagrams and fault trees are static meth-
ods that cannot easily take into account any dynamic effects of your SIS design, 
such as dynamics rooted in phased mission operation, testing, and maintenance. The 
Markov and Petri net approaches are tailor-made for this purpose and can elegantly 
accommodate dynamic features of your SIS. 
14.3 Remaining Issues 
The book covers a lot of issues but there are still many that should have been better 
explained and that should be better understood. Among these issues are: 
(a) Software is increasingly used in modern SISs but IEC 61508 does not require 
software reliability be quantified, and software issues are consequently outside 
the scope of this book. At the same time, the software reliability strongly influ-
ences the diagnostic coverage and other parameters that are used to determine 
the reliability performance parameters. A complete book on SIF reliability as-
sessment should include a treatment of software reliability, but this author does 
not have the required competence to cover this aspect. 

396 
CLOSURE 
(b) The logic solver subsystem has been mentioned several times in the book but 
not analyzed specifically. The Markov and Petri net approaches are suitable for 
the analysis of logic solver subsystems but specific examples are not presented. 
The main reason for this omission is the same as for item (a). 
(c) IEC 61508-6 presents formulas for PFDavg and PFH for loo2:D voted groups, 
but this voting is not studied in this book. 
(d) Systematic faults (including software errors) are causes of many SIF failures on 
demand, but quantification of the effects of systematic faults is not required in 
IEC 61508. In this respect, I support the view of the PDS method that the effect 
of systematic faults should be quantified. I realize that this will be difficult, but 
an approximate value is usually better than ignorance. 
(e) CCFs have dominating effects on all the SIF reliability measures, but this book 
has concentrated on the simplest possible model for CCFs: the beta-factor 
model. The multiple beta-factor model has been presented, but not used in 
many cases. With regards to the importance of CCFs, too little emphasis has 
been put on developing realistic and usable CCF models and to provide input 
data to CCF models. 
(f) As a follow-up of item (e), the beta-factor is the most important input parameter 
to a SIF reliability assessment. This is also the parameter with the least reliable 
estimates. Checklist procedures to obtain plant-specific estimates of the beta-
factor are provided in IEC 61508-6 and IEC 62061. The author has never seen 
any scientific justification for the questions in the checklists and finds some 
questions to have low relevance, whereas other relevant questions are missing. 
The checklist in IEC 615086 is used extensively in many SIF reliability assess-
ment, for both DU and DD failures, and should have been better substantiated. 
(g) Continuously demanded SIFs are mentioned in Chapter 9, but not treated ade-
quately. The integration of the SIF into the EUC control system (e.g., the BPCS) 
should have been studied far more thoroughly. 
(h) For SIFs operating in high-demand mode, demands occur rather frequently. In 
most cases, the SIF is successful in handling the demands and each successfully 
handled demand is therefore a partial test that might be taken into account in 
the determination of the PFH. This issue is briefly mentioned in the book, but 
has not been subject to any adequate investigation. 
(i) Imperfect and partial proof tests are generally disregarded in most SIF reliability 
assessments. Modeling of imperfect proof testing is introduced in Chapter 11, 
but not given any rigorous treatment. The most promising modeling approach, 
based on phased Markov chains, is introduced briefly but is not further investi-
gated. 
(j) When a DD failure occurs in a multi-channel group, it sometimes creates an 
opportunity to partially test other channels for DU faults. When this is possi-

A FINAL WORD 
397 
ble, it will have a positive effect on the SIF reliability, but the author has not 
investigated this issue any further. 
(k) Many analysts have encountered problems when the demand rate is close to 
once per year. As this is the boundary between low- and high-demand mode in 
IEC 61508, the analysis may be based on either PFDavg or PFH. When using 
the IEC formulas, one option may meet the SIL requirement, whereas the other 
option may not. Whether this difference is due to (i) inconsistency in the IEC 
formulas, (ii) lack of calibration of the SIL tables for high- and low-demand 
mode, or (iii) differences in the assumptions made for the IEC formulas has not 
been investigated in detail by the author. 
(1) Almost all SIF reliability assessments are based on the assumption that failure 
rates are constant. This may be adequate for electronic items, but is a doubtful 
assumption for mechanical items such as valves. It is especially problematic for 
shutdown valves that are kept in a static position for long periods of time. The 
problem is mentioned in Chapter 9, but not investigated in this book. 
I hope that some of you may find these topics of interest for further research. 
14.4 A Final Word 
FMECA (or FMEA) was introduced in 1949 and is still the most popular reliability 
tool in the design of complex products. The success of FMECA is not grounded in 
the nice printouts from the analysis, but the fact that a carefully performed FMECA 
improves the quality of the products. By using FMECA, the designers understand 
better how and why the products can fail and thereby proactively avoid bad design 
solutions. If you know the problem, you can often avoid it by minor design changes. 
In the worked examples in the book, you have seen that you get approximately 
the same numerical result whichever method you use to determine PFDavg and PFH. 
So, why should you spend time learning and using the advanced approaches? The 
answer is the same as for the FMECA example. By using the suggested methods, 
you will better understand your SIS design, how it functions, and why and how it 
can fail. The lesson you learn is more complete the more advanced approach you 
choose. 
The ultimate objective of reliability engineering is to provide equipment and sys-
tems with high reliability (seen in relation to cost). I strongly recommend that you 
implement some of the knowledge you have learned by reading this book in your de-
sign processes. I also recommend that you acquire an adequate computerized tool for 
SIL analysis and use this to check the effects of proposed design changes, variation 
in the values of input parameters, and so on. 
Your job as a SIS reliability analyst or engineer is very important and you can 
significantly contribute to saving lives and to protecting our common environment! 


APPENDIX A 
ELEMENTS OF PROBABILITY THEORY 
A.1 
Introduction 
This appendix gives an introduction to some of the main results from probability 
theory. It is not intended to be comprehensive but rather a brief repetition that may be 
useful when reading the rest of the book. Readers who are familiar with probability 
theory may skip this appendix. More extensive introductions to probability theory 
may, for example, be found in Ross (2004, 2007). 
A.1.1 Outcomes and Events 
Random Experiment. A random experiment is an experiment that may be repeated 
over and over again under "essentially the same conditions." Examples of random 
experiments include simple experiments such as flipping a coin, counting failures 
over time, and observing whether or not an E/E/PE safety-related system is able to 
function as expected during demand situations. In many cases it may be impossible 
to repeat the same experiment under exactly the same conditions. We will, however, 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
399 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

4 0 0 
ELEMENTS OF PROBABILITY THEORY 
refer to the trial as a random experiment whenever we can imagine that it can be 
repeated under approximately the same conditions. 
Single Outcome. 
The result of a specific random experiment is called a single 
outcome, or just an outcome. The letter e is often used to denote a single outcome. 
Sample Space. 
The set of all possible single outcomes of a random experiment is 
called the sample space and is represented by the symbol S. The sample space may 
have a finite, countable, or noncountable number of single outcomes. A sample space 
with a finite number n of single outcomes may be written as S = {e\, e2, ■ ■., e„}. 
Event. An event E is a specified set of possible single outcomes in the sample 
space S. We write E C S where C denotes subset. Most events are compound 
events comprising several single outcomes, but a single outcome e is also an event 
and is sometimes called a simple event. 
fl 
EXAMPLE A.l 
Football match 
Consider a football match between two teams, A and B. We are interested in 
observing the result of the match measured as a number of goals to each team. 
In this case, the sample space is 
S = {(0,0), (0,1), (1,0), (1, 1), (0, 2), (2,0), (1,2), (2,1),...} 
In this case, an outcome may, for example, be (2,0), which means that team A 
wins the match with two goals against nil. Team A can win the match in many 
different ways. If the outcome eis part of the set E\ = {(1,0), (2,0), (2,1),...}, 
then team A has won. This set, E\, is an event in S. The events we consider in 
this book can usually be stated in verbal terms, such as 
"Team A wins" <$■ e e E\ 
Another event, E2, is "no more than 2 goals in the match," and is represented by 
the set E2 = {(0,0), (0,1), (1,0), (1,1), (2,0), (0,2)}. 
Θ 
Complementary Event. The complementary events E* of an event E with respect 
to S is the subset of all outcomes of S that are not in E. The complementary event 
of the sample space S must, per definition, be empty, and is therefore called the null 
set or empty set and is denoted by 0. 
Venn Diagram. To illustrate the sample space and the various events, it is common 
to use a Venn diagram, illustrated in Figure A.l. In this diagram the outer rectangle 
represents the sample space S. All the possible outcomes of the experiment are rep-
resented as single points in the rectangle. The ellipse E comprises all the outcomes 
that are part of the event E. The complementary event E* contains all outcomes in 
S that are not in E. 

PROBABILITY 
401 
Figure A.2 
Venn diagram of two events, E\ and E2. In (a) the two events are overlapping 
and in (b) their intersection is empty. 
Intersection of Events. 
The intersection of two events, E\ and E2, is denoted by 
the symbol E\ Π E2, and is the event containing all outcomes that are common to 
E\ and £2· 
fl 
EXAMPLE A.2 Football match (cont.) 
In Example A.l, the intersection is 
El DE2 = {(1,0), (2,0)} 
Team A wins the match and there are no more than two goals. 
φ 
Union of Events. 
The union of two events, E\ and E2, is denoted by the symbol 
E\ U E2 and is the event containing all outcomes that belong to E\ or E2 or both, 
see Figure A.2 (a). 
Mutually Exclusive Events. 
Two events, E\ and E2, are mutually exclusive, or 
disjoint, if £Ί Π E2 = 0, that is, if E\ and E2 have no outcomes in common. This 
is illustrated in Figure A.2 (b) and means that events E\ and E2 cannot occur at the 
same time. 
A.2 
Probability 
In the following, we define the main terminology for probability calculation and 
present the main rules and formulas. 

402 
ELEMENTS OF PROBABILITY THEORY 
A.2.1 
Definition of Probability 
Let S be the sample space of a random experiment. For each event E of the sample 
space, we assume that a number PT(E) is defined and satisfies the following three 
conditions: 
1. 0 < Pr(£) < 1 
2. Pr(S) = 1 
3. For any sequence of events E\, E2, ■ ■ ■ that are mutually exclusive, such that 
Ei ΓΊ Ej: = 0 for all i φ j , then 
(
00 
\ 
00 
\JE,: =£>r(£,) 
(A.l) 
i=\ 
! 
i=\ 
The quantity Pr(£) is called the probability of the event E. 
The conditions imply that the probability of an event E must always be between 
zero and 1. A probability of 1 means that the event is certain to occur. An event with 
probability zero means the event is impossible or certain not to occur. If the events 
have no single outcomes in common, the probability of either of them happening is 
the sum of their probabilities. 
A.2.2 Basic Rules for Probability Calculations 
Probability of Complementary Events. 
Let E* be the complementary event of 
the event E. The probability of E* is given by 
Pr(£*) = 1 - Pr(£) 
(A.2) 
The probability of an event E happening is therefore equal to 1 minus the probability 
of the event E not happening. 
Addition Rule of Probability. If E\ and E2 are any two events, then 
ΡΓ(£Ί U E2) = Pr(£i) + Pr(£2) - Pr(£i ΓΊ E2) 
(A3) 
Equation (A.3) is called the addition rule of probability. The addition rule is illus-
trated by the Venn diagram in Figure A.2. Note that when E\ and E2 are mutually 
exclusive (disjoint) as in Figure A.2 (b), then 
Pr(£i U E2) = Pr(£0 + Pr(£2) 
The addition rule can be easily extended such that for a sequence 
E\,E2,...,En 
of events that are mutually exclusive, then 
ΡΓ(£Ί U E2 U · · · U En) = Pr(£i) + Pr(£2) + · · · + Pr(£„) 
n 
= £Pr(£,-) 
( = 1 

PROBABILITY 
403 
Pr(E2 | E,) 
I 
Pr(E2\E:') 
Figure A.3 
Probability tree for Example A.3. 
Conditional Probability. Assume that we are interested in finding the probability 
of event Ei when we know that event E\ has already occurred. If event E\ has 
occurred, then we are interested in the relative proportion of E2, which is a subset of 
E\, namely E\ Π Ei. For this reason, the conditional probability of Ei given E\ is 
defined by 
Prtft|£.) = ï ^ ^ 
(A.4) 
Pr(£i) 
ifPr(£i) > 0. 
β 
EXAMPLE A.3 Process demands 
Let event E2 be a process demand of a certain type in a process plant. We 
want to study how the probability of the process demand, Ρτ(Ε2), depends on 
whether or not a critical level controller is functioning. Let Ei be the event that 
the level controller is failed. This situation can be illustrated by the tree structure 
in Figure A.3, and we note that the probabilities of the end nodes of the tree are: 
- Pr(£2 I Ei) (i.e., the probability of a process demand when the level controller 
is failed) 
- Pr(£2 I E*) (i.e., the probability of a process demand when the level controller 
is functioning) 
The tree in Figure A.3 is called a probability tree or an event tree. 
φ 
Product Rule of Probability. From (A.4), we obtain the product rule of probability 
Pr(£i ΓΊ E2) = Pr(£ 2 | EX) ■ Ρτ(Εχ) = Pr(£j | E2) ■ Pr(£2) 
(A.5) 
The intersection of E\ and E2 is illustrated by the Venn diagram in Figure A.4. This 
can be generalized, by induction, to the following result 
Pr(£i nE2n---r\Ek) 
(A.6) 
= Pr(£,)-Pr(£ 2 I £i)-Pr(£ 3 I £ι Π E2)---Pv(Ek 
\ Ex Π · ·· ΓΊ £*_,) 

4 0 4 
ELEMENTS OF PROBABILITY THEORY 
Figure A.4 
Event E\ and event E2 occur. 
Figure A.5 
Partition of the sample space. 
Independent Events. 
Two events E\ and £2 are said to be independent if the 
occurrence of one of the events gives us no information about whether or not the 
other event will occur; that is, the events have no influence on each other. This 
means that the two events E\ and £2 are independent if and only if 
Pr(£ 2 \Ei) = Pr(£2) or P r ^ | E2) = Pr(£0 
in which case we may write 
Ρτ(Ει n £2) = Pr(£i) · Pr(£2) 
(A.7) 
If a sequence E\, Ε2,..., En of events are independent, then 
n 
Pr(£i Π £ 2 Π · · · n En) = Pr(£0 · Pr(£ 2) ■ · · Pr(£„) = f [ Pr(Ef) 
(A.8) 
i' = l 
► Remark: When ΡΓ(£Ί Π Ε2) φ Pr(£i) · Pr(£2) the events E\ and E2 are said 
to be dependent. The "link" between E\ and E2 is logical, not necessarily causal, φ 
Partition of the Sample Space. 
A collection of events E\, E2,..., 
En in a sample 
space S is said to be a partition of S \îE\,E2,...,En 
are mutually exclusive and 
S = Ei U E2 U · · · U En 
A partition of the sample space S is illustrated in Figure A.5. 

PROBABILITY 
4 0 5 
Total Probability. Let F be an event in the sample space 5, and let E\, F 2,..., E„ 
be a partition of S. The probability of F can then be written 
n 
n 
Pr(F) = ΣPr(F 
n £,) = £ P r ( F | F,·) · Pr(F,·) 
(A.9) 
i' = l 
1 = 1 
This result is called the law of total probability. 
Bayes Formula. If the events Fi, F 2 , . . . , F„ constitute a partition of the sample 
space S, where Pr(F,) > 0 for all / = 1,2,..., n, then for any event F in S such 
that Pr(F) > 0, we have Bayes formula 
Pr(*> I F) = J ^ ( ^ n ^ ) 
= 
JriF\Ej).TnEj) 
Σ?=ι Pr(^ Π Ei) 
YS=lVT(F\Et)'VT{Ei) 
for all; = 1 , 2 , . . . , « . 
A.2.3 Uniform Probability Models 
The simplest probability model we can make is the model known as the uniform 
probability model. Let S be a finite, nonempty, sample space with n outcomes. The 
uniform probability model on S is defined by 
Pr(e) = - 
for all e € S 
n 
Consider an event F in 5 comprising HE distinct outcomes. When having a uniform 
probability model, the probability of the event E is 
Pr(F) = Ü* 
n 
fl EXAMPLE A.4 Tossing dice 
Consider the random experiment of tossing a (fair) six-sided dice. The sample 
space is S = {1,2,3,4,5,6}, and the number of outcomes is « = 6 . The 
probability of getting the outcome e = 2 is 
Pr(2) = I 
The event E — "even number" consists of the outcomes {2,4,6}. The probabil-
ity of F is therefore 
Pr(F) = ^ = - = 0.50 
n 
6 

4 0 6 
ELEMENTS OF PROBABILITY THEORY 
A.2.4 
Random Variables 
A random variable is a function that associates a real number with each outcome in 
the sample space. In this book a capital letter, say X, is used to denote a random 
variable, and a corresponding lowercase letter, x in this case, for one of its values. 
A.3 Discrete Distributions 
A random variable is said to be discrete if the variable can take on at most a countable 
number of values. 
fl 
EXAMPLE A.5 Brothers and sisters 
Consider a group of persons, and select a person at random from this group. The 
group is then the sample space S and the person is an outcome e from the sample 
space. We want to measure a property of the outcome, say the total number of 
brothers and sisters this person has. Let X denote this number. We note that X 
is a function of the outcome, and that X is a discrete variable. When we have 
selected the person and observed the variable, we know the value. This value is 
denoted x, and we may, for example, get the result x = 2. The randomness in 
this experiment is connected to the outcome. We do not know the value of X 
because we do not know which person will be selected. Once we have selected 
the outcome, we can, in many cases, "measure" the number of brothers/sisters 
without any uncertainty. 
Θ 
Probability Mass Function. Let X be a discrete random variable that can take 
values in the set {xi,X2,...}, and let Ρτ(Χ = χ,) denote the probability that the 
variable X takes the value x, for some i. This probability is called the probability 
mass of xi and can be interpreted as the probability of obtaining an outcome that is 
measured (by X) to be x,. We then have that: 
1. PrC* = xi) > 0 for all i = 1,2,... 
00 
2. £ Pr(* = *,-) = 1 
( = 1 
Probability Distribution Function. The probability distribution function of the 
discrete random variable X is 
F(x) = Ρτ(Χ <x)=J2 
PT(X = Xi) 
(AM) 
Xj<X 
The distribution function is sometimes called the cumulative distribution function. 
The probability that the random variable X has a value in the interval (χ,, xj ] is 
Pr(x,- < X < Xj) = Px(X <XJ)- 
PT(X <*,-) = F(xj) - F(x,) 
forx, < xi. 

DISCRETE DISTRIBUTIONS 
4 0 7 
Mean Value, Variance, and Standard Deviation. The mean value of a discrete 
random variable X with value set {x\, X2,.. .} is 
00 
E(X) = μ = Υ^Χί-Ρτ(Χ 
= Xi) 
(A.12) 
1 = 1 
Let g(X) be a function of X. The mean value of the random variable g(X) is 
00 
E[g(x)} = 2 > t a ) ' P r ^ = *«·>
 
( Α ·
1 3 ) 
i = l 
The variance of Z is 
00 
Var(X) = E [(X - μ)2] = £(*,- - μ) 2 Pr(Z = je,·) 
(A. 14) 
1 = 1 
The standard deviation of X is 
SD(AT) = v/Var(X) 
(A.15) 
The Binomial Distribution. The binomial distribution is used in the following sit-
uation: 
1. We have n independent trials. 
2. Each trial has two possible outcomes E and E*. 
3. The probability Pr(£) = p is the same in all trials. 
This situation is called a binomial situation, and the trials are sometimes referred to 
as Bernoulli trials. The "bi" in binomial indicates two possible outcomes. 
Let X denote the number of the n trials that have outcome E. Then X is a discrete 
random variable with probability mass function 
Pr(X =x) = I " \ρχ(1-χ)"-χ 
ΐοτχ=0,1,...,η 
(Α.16) 
where (") is the binomial coefficient 
Ö
n\ 
x\(n — x)\ 
The distribution (A. 16) is called the binomial distribution (n, p), and we sometimes 
write X ~ bin(n, p). Here, n, the number of trials, is usually a known constant, 
while p is a parameter of the distribution. The parameter p is usually an unknown 
constant and is not directly observable. It is not possible to "measure" a parameter. 

4 0 8 
ELEMENTS OF PROBABILITY THEORY 
The parameter can only be estimated as a relative frequency based on observations 
of a high number of trials. 
The mean value and the variance of X are 
E(X) =np 
(A.17) 
Var(X) =np{\ - p) 
(A.18) 
β 
EXAMPLE A.6 Fire pump 
A fire pump is tested regularly. During the tests we attempt to start the pump and 
let it run for a short while. We observe a "fail to start" (event E) if the fire pump 
cannot be started within a specified interval. Assume that we have performed 
n — 150 tests and that we can consider these tests to be independent. In total, 
X = 2 events E have been recorded. From (A.17) we know that p = E(X)/n 
and it is therefore natural to estimate the fail-to-start probability by 
p = - = 
«a 0.0133 = 1.33% 
y 
n 
150 
β 
EXAMPLE A.7 2-out-of-3 system 
A 2-out-of-3 system is a system that is functioning when at least 2 of its 3 com-
ponents are functioning. We assume that the components are independent, and 
let X be the number of components that are functioning. Let p be the probability 
that a specific component is functioning. This situation may be considered as a 
binomial situation with n = 3 trials, and X therefore has a binomial distribution. 
The probability ps that the 2-out-of-3 system is functioning is 
ps = Ρτ(Χ > 2) = Ρτ(Χ = 2) + Pr(Z = 3) 
^0-,)~ +g)*.-3)» 
= 3p2(l-p) + p3 = 3p2-2p3 
The Geometric Distribution. Assume again that we have a binomial situation, and 
let Z be the number of trials until the first trial with outcome E. If Z = z, this 
means that the first (z — 1) trials will result in E*, and that the first E will occur in 
trial z. The probability mass function for Z is then 
Pr(Z = z) = (1 - p)2-1 p forz = l,2,... 
(A.19) 

DISCRETE DISTRIBUTIONS 
4 0 9 
The distribution (A. 19) is called the geometric distribution. We have that 
Pr(Z > z) = (1 - pf 
The mean value and the variance of Z are 
E(Z) = - 
(A.20) 
P 
Var(Z) = 
X-^- 
(A.21) 
P2 
β 
EXAMPLE A.8 DU failure revealed in a proof test 
Consider an E/E/PE safety-related system that is proof tested with test interval r, 
and let p denote the probability that a system DU failure is present and detected 
in the proof test. Further, let Z denote the number of tests until a system DU 
failure is detected in the proof test. If Z = z, this means that the first (z — 1) 
tests did not reveal any system DU failure, but that a system DU failure was 
found in test z. This is a geometric situation and the probability of Z = z is 
Pr(Z = z) = (1 — p)z~l p. The mean time until a system DU failure occurs is 
then MTTFDU = E(Z) · τ + g(r) = τ/ρ + g(r), where g(z) is a part of the 
last test interval. 
Θ 
The Poisson Distribution and the Poisson Process. 
It is often assumed that 
demands for an E/E/PE safety-related system occur according to a homogeneous 
Poisson process (HPP). An HPP is a stochastic process where we count the number 
of occurrences of an event E during a specified time period. 
To be an HPP, the following conditions must be fulfilled: 
1. The number of occurrences of E in one time interval is independent of the 
number that occur in any other disjoint time interval. This implies that the HPP 
has no memory. 
2. The probability that event E will occur during a very short time interval is pro-
portional to the length of the time interval and does not depend on the number 
of events occurring outside this time interval. 
3. The probability that more than one event will occur in a very short time interval 
is negligible. 
Without loss of generality we let t = 0 be the starting point of the process. 
Let NE(Î) be the number of times event E occur during the time interval (0, t]. 
The discrete random variable Nß(t) is called a Poisson random variable, and its 
probability distribution is called a Poisson distribution. The probability mass func-
tion of NE (t) is 
Pr(NE(t) 
=n) = 
Et' 
e~kEt 
for« = 0 , 1 , . . . 
(A.22) 

4 1 0 
ELEMENTS OF PROBABILITY THEORY 
where XE > 0 is a parameter and e = 2.71828 
The mean number of occurrences of E in the time interval (0, i) is 
E(NE(t)) 
= Σ η -Pr(Afe(0 = ») = λ £ί 
(Α.23) 
n=0 
and 
E(N(t)) 
The parameter Xg is therefore the mean number of occurrences of E per time unit, 
and is called the rate of the Poisson process or the rate of occurrence of events E. 
The variance of Ng (0 is 
Var(NE(t)) = XEt 
(A.24) 
0 
EXAMPLE A.9 Random demands 
Demands for an E/E/PE safety-related system are often assumed to occur as an 
HPP with demand rate A<je· The mean number of demands in an interval (0, t) is 
then E(N(t)) = Xdet. The probability that no demand will occur in an interval 
(0,/) is 
Pr(iV(r) = 0) = e"Adei 
A.4 Life Distributions 
Let T be a random variable with a value set that is not countable. We say that T is a 
continuous random variable if there exists a non-negative function f(t), defined for 
all values oit, having the property that for any set A of real numbers 
Pr(TeA) 
= f 
f(t)dt 
The function f(t) is called the probability density function of the random variable 
T. 
In this chapter, T denotes a continuous random variable that takes only positive, 
real values. Other types of continuous random variables may take values on the 
whole axis, from —oo to +oo, or be limited to some interval(s) on this axis. 
The probability density function f(t) fulfills: 
1. / ( 0 > 0 for alii 
2. fQ°°f(t)dt 
= l 

LIFE DISTRIBUTIONS 
411 
Failure 
Figure A.6 
The state variable and the time to failure of an item. 
3. PT(a<T<b) 
= 
tff(t)dt 
In the next sections, we assume that the random variable T is the time to failure 
of an item, and introduce the main probabilistic measures for such a variable. 
Time to Failure. By the time to failure of an item we mean the time elapsing from 
when the item is put into operation until it fails for the first time. We set t = 0 as 
the starting point. The time to failure is, at least to some extent, subject to chance 
variations. It is therefore natural to interpret the time to failure as a random variable, 
T. 
The state of the item at time t may be described by the state variable X(t), which 
is also a random variable. 
X«) 
j 1 
if the item is functioning at time t 
I0 
if the item is in a failed state at time t 
The connection between the discrete state variable X(t) and the time to failure T is 
illustrated in Figure A.6. 
Note that the time to failure T is not always measured in calendar time. It may also 
be measured by more indirect time concepts, such as the number of times a switch is 
operated. The time to failure that is a discrete variable can, however, be approximated 
by a continuous variable. Here, unless stated otherwise, we will always assume that 
the time to failure T is a continuous variable. 
Probability Distribution Function. The probability distribution function of T is 
defined by 
F(t) = Pr(T < f) 
for / > 0 
(A.25) 
We note that F(t) is the probability that the item will fail in the time interval (0, t] 
and F(t) is therefore sometimes called the failure probability function for the item. 
Probability Density Function. 
fit) = 
TtF(t) 
The probability density function f(t) is 
F(t + At) - F(t) 
■■ lim 
Δί->·0 
: lim 
Δί->-0 
Δί 
Pr(i < T < t + At) 
Δί 
(Α.26) 

4 1 2 
ELEMENTS OF PROBABILITY THEORY 
Timet 
Figure A.7 
Distribution function F(t) and probability density function f(t). 
Timet 
Figure A.8 
The survivor function R(t). 
This implies that when At is small, 
Pr(r < T < t + At) as f{t) ■ At 
(A.27) 
The distribution function F(t) and the probability density function f(t) are illus-
trated is Figure A.7. From (A.27) we note that when f(t) has a low value, the 
probability of failing in (t, t + At) is low; and when f(t) has a high value, the prob-
ability of failing in (t, t + At) is high. This is why f(t) is called a density function. 
Survivor Function. The survivor function of an item is denned by 
R(t) = 1 - F(t) = Ρ Γ ( Γ > t) for / > 0 
(A.28) 
Hence, R(t) is the probability that the item does not fail in the time interval (0, t], 
or, in other words, the probability that the item survives the time interval (0, t] and is 
still functioning at time t. The survivor function is illustrated in Figure A.8. 
Failure Rate Function. The probability that an item will fail in the time interval 
(t,t + At] when we know that the item is functioning at time t is given by the 
conditional probability 
Pr(i < Γ < ί - | - Δ ί | 7 / > 0 = 
Pr(f < T < t + At) 
F(t + At) - F(t) 
Pr(T > t) 
R(t) 

LIFE DISTRIBUTIONS 
413 
By dividing this probability by the length of the time interval, At, and letting Δί -» 
0, we get the failure rate function z(t) of the item 
Pr(f < T < t + At | T > t) 
z(t) = lim 
Δί->0 
Δί 
=A^O — Δ Γ — m= 
m
 
(Α·
29) 
This implies that when Δ/ is small, 
Pr(i <T<t 
+ At\T>t)& 
z(t) ■ At 
(A.30) 
Note the difference between the probability density function f(t) and the failure 
rate function z(t). Assume that we start out with a new item at time t = 0 and 
at time t = 0 ask "What is the probability that this item will fail in the interval 
(t, t + Δ?]?" According to (A.27), this probability is approximately equal to the 
probability density function f(t) at time t multiplied by the length of the interval, 
Δί. Next consider an item that has survived until time t, and that we at time t ask 
"What is the probability that this item will fail in the next interval (t, t + Δ?]?" This 
(conditional) probability is according to (A.30) approximately equal to the failure 
rate function z(t) at time / multiplied by the length of the interval, Δί. 
Mean Time to Failure. Let T be the time to failure of an item. The mean time to 
failure (MTTF) or expected value of T is 
/•OO 
MTTF = E(T) = / 
tf(t)dt 
(A.31) 
Jo 
Since f(t) = 
-R'(t), 
/•OO 
MTTF = - / 
tR'(t)dt 
Jo 
By partial integration 
/•OO 
MTTF = - [fÄ(i)]~ + / 
R(f) dt 
Jo 
If MTTF < oo, it can be shown that [iÄ(i)]g° = 0. In that case 
/•OO 
MTTF= / 
R(t)dt 
(A.32) 
./o 
It is often easier to determine MTTF by (A.32) than by (A.31). 
Median Life. The median life tm is defined by 
R(tm) = 0.50 
(A.33) 
The median divides the distribution in two halves. The item will fail before time i, 
with 50% probability, and will fail after im with 50% probability. 

414 
ELEMENTS OF PROBABILITY THEORY 
Marginal and Conditional Distributions. 
Let T\ and T2 be two continuous ran-
dom variables with probability density functions f\(t) and f2(t), respectively. The 
joint probability density function of 7\ and Ύ2 is written as f(t\, £2)· The marginal 
probability density function of T\ is 
l-OO 
Mh)= 
\ 
f(h,t2)dt2 
Jo 
The conditional probability density function of T2 when we have observed that 
T\ = t\ is 
/ ( ί 2 | ί ι ) = ^ 7 ^ 
for/ 1(i 1)>0 
This result may, for example, be used to find 
Pr(a <T2<b\Tl=ti)= 
[ f(t2 | h) dt2 
Ja 
and the conditional mean value as 
ΛΟΟ 
E(T2\tO= 
/ 
t2f(t2\ti)dt2 
Jo 
Independent Variables. The two continuous random variables T\ and T2 are inde-
pendent if /(Î2 | ii) = f2(t2) 
and f(t\ \ t2) = /i(ii), which is the same as saying 
that Ti and T2 are independent if 
/(ii, h) = /1 (ίι) · f2(f2) 
for all h and i2 
The definition is easily extended to more than two variables. 
A.4.1 The Exponential Distribution 
Let us assume that the time to failure T of an item is exponentially distributed with 
parameter λ. The probability density function of T is then given by 
ί λ β - 
f o r i X U X ) 
^ 
/ 0 
otherwise 
The probability distribution function is 
F(t) = Pr(7 < 0 = 1 - e~Xt 
for t > 0 
(A.35) 
The probability density function and the distribution function of the exponential dis-
tribution are illustrated in Figure A.9. 
The reliability (survivor) function becomes 
/
oo 
f(u) du = e~Xt 
for t > 0 
(A.36) 

LIFE DISTRIBUTIONS 
4 1 5 
Figure A.9 
Exponential distribution (X = 0.4). 
The mean time to failure is 
MTTF = 
The variance of T is 
and the failure rate function is 
/ 
R(t)dt 
= 
Jo 
Jo 
Var(D = 
γ2 
-Xt dt = 
z(t) 
fit) 
_ λβ 
R(t) 
-λί 
ο-λί 
(A.37) 
(A.38) 
Accordingly, the failure rate function of an item with exponential life distribution is 
constant (i.e., independent of time). 
The results (A.37) and (A.38) compare well with the use of the concepts in every-
day language. If an item has on the average A = 4 failures/year, the mean time to 
failure MTTF of the item is 1/4 year. 
fl 
EXAMPLE A.10 Fire detector 
Consider a fire detector with constant failure rate A DU = 2.5· 10~6 per hour with 
respect to DU failures. Assume that the fire detector is in continuous operation 
and is proof tested every 6 months (i.e., every 4380 hours). The probability 
that the detector survives a test interval without any DU failure is, according to 
(A.36): 
Pr(r D U > 4380 hours) = e 
_—ADU"4380 hours 
0.989 = 98.9% 
The mean time until a DU failure occurs is according to (A.37): 
1 
MTTFDU 
^DU 
400000 hours % 45.7 years 

4 1 6 
ELEMENTS OF PROBABILITY THEORY 
The Memoryless Property. 
Suppose that an item has exponential time to failure 
T. For such an item 
Pr(T >t+x\T>t)= 
1 * 
+*' 
= —^r- 
= e~Xx = ΡΓ(Γ > x) 
Pr(T > t) 
e~At 
This implies that the probability that an item will be functioning at time t + x, given 
that it is functioning at time /, is equal to the probability that a new item has a time 
to failure longer than x. Hence, the remaining lifetime of an item that is functioning 
at time t, is independent of t. This means that the exponential distribution has no 
"memory" and we say that the exponential distribution has the memoryless property. 
An assumption of exponentially distributed lifetime therefore implies that: 
- A used item is stochastically as good as new. Thus, there is no reason to replace 
a functioning item. 
- For the estimation of the survivor function, the mean time to failure, and so 
on, it is sufficient to collect data on the number of hours of observed time in 
operation and the number of failures. The age of the items is of no interest in 
this connection. 
The exponential distribution is the most commonly used life distribution in applied 
risk and reliability analyses. The reason for this is its mathematical simplicity and 
that it leads to realistic lifetime models for certain types of items. 
The Exponential Distribution and the Poisson Process. 
Assume that failures 
occur according to a Poisson process with rate A, and let N(t) be the number of 
failures in the time interval (0, t]. The probability mass function of N(t) is 
(It)" 
Pi(N(t) = n) = — - e~Xt 
for« = 0 , 1 , . . . 
n\ 
Let T\ be the time from t = 0 until the first failure, Ti be the time between the first 
and second failures, and so on. The properties of the HPP imply that the variables 
Τχ,Τ-ι,... are independent and identically distributed. The survivor function of T\ 
is 
Ri(t) = ΡΓ(7Ί > 0 = Pr(tf(0 = 0) = ^-^-e^' 
= e~Xt 
If the time to the first failure is greater than /, there cannot be any failure in the 
interval (0, i) (i.e., N(t) = 0), and T\ is therefore exponentially distributed with rate 
X. 
A.4.2 The Weibull Distribution 
The Weibull distribution is one of the most widely used life distributions in reliability 
analysis. The distribution is named after the Swedish professor Waloddi Weibull 
(1887-1979), who developed the distribution for modeling the strength of materials. 

LIFE DISTRIBUTIONS 
417 
0,0 
\α=0.5 
I 
1 
0,0 
0,5 
1,0 
1,5 
2,0 
2,5 
3,0 
Timet 
Figure A.10 
The probability density function of the Weibull distribution for selected values 
of the shape parameter a (X = 1). 
The Weibull distribution is very flexible and can, through an appropriate choice of 
parameters, model many types of failure rate behaviors. 
The time to failure T of an item is said to be Weibull distributed with parameters 
a and λ if the distribution function is given by 
F(t) 
I 1 - e-(Xt)" 
for / > 0, λ > 0, a > 0 
10 
otherwise 
The corresponding probability density function is 
d 
, v 
\akat"-le-{-Xty" 
f o r i > 0 
f{t) = -j-F{t) = { 
at 
10 
otherwise 
(A.39) 
(A.40) 
where λ is a scale parameter and a is referred to as a shape parameter. Note that 
when a = 1, the Weibull distribution is equal to the exponential distribution. The 
probability density function f(t) is illustrated in Figure A.10 for selected values of 
a. The survivor function is 
R(t) = Pr(T > t) = e~(Xtr 
for t > 0 
and the failure rate function is 
z(t) 
fit) 
R(t) = aXat 
ata—\ 
for t > 0 
(A.41) 
(A.42) 
The failure rate function z {t ) of the Weibull distribution is illustrated in Figure A. 11 
for selected values of a. Because of its flexibility, the Weibull distribution may be 
used to model life distributions where the failure rate function is decreasing, constant, 
and increasing. 
The mean time to failure, MTTF, of the Weibull distribution is 
MTTF = j 
R(t)dt 
= j r ( ^ - l \ 
(A.43) 

4 1 8 
ELEMENTS OF PROBABILITY THEORY 
Figure A.ll 
The failure rate function of the Weibull distribution for selected values of the 
shape parameter a (X = 1). 
The Gamma Function 
The gamma function Γ(χ) is defined by 
Γ(Λ 
tx~xe~' dt for* >0 
Jo 
By partial integration, we get 
Γ(χ + 1) = χΓ(χ) for all x > 0 
In particular, we have 
Γ(« + 1) = «! for« =0,1,2,... 
The gamma function is a standard function in computer programs such as MATLAB, 
Scilab, and GNU Octave. 
where Γ(·) is the gamma function (see box). 
The variance of T is 
Var<r) = ^[
r(; + 1)-
r2G + 1I 
A.5 
Repairable Items 
Consider an item that has been installed to perform a specified function. When the 
item is able to perform this function, we say that the item is functioning. If not, we 
say that the item is not able to function or that it is failed. In this section, we disregard 
failure modes that are not related to the specified function. We assume that the item 
is repaired when it fails and is put into operation again. The most simple situation 

REPAIRABLE ITEMS 
4 1 9 
• Failure 
J_J 
1 
1 
1—1 
► 
T, 
D, 
T2 
D2 
T3 
D3 
T4 
t 
Figure A.12 
The state of a repairable item as a function of time. 
is when we assume that the item is "as good as new" when it is put into operation 
again after a repair. We also assume that all uptimes and downtimes are independent 
variables. The state of the item as a function of time t can then be illustrated as 
in Figure A.12, where T\, T2, ■ ■ ■ denote the independent uptimes and D\, 
Di,... 
denote the independent downtimes. 
A.5.1 Availability and Unavailability 
The reliability of the repairable item is often measured by the availability of the item 
A(t) = Pr(X(t) = 1) = Pr(The item is able to function at time t) 
(A.44) 
In this book, we are mainly concerned with the unavailability of the item defined as 
A{t) = 1 — A(t) = Pr(The item is not able to function at time t) 
(A.45) 
In some cases, it is not necessary to know the unavailability at a specified point t 
of time and we may suffice with the average unavailability over an interval (t\, Î2) 
defined as 
Äavg(h, t2) = — — ί 2 Ä(t) dt 
(A.46) 
«2 ~ «1 Jt\ 
When we consider the average value over a very long interval, we talk about 
the average unavailability Aavg without specifying the time interval. The average 
unavailability of an item will obviously depend on how often the item fails (i.e., its 
reliability), how long time it takes to repair the item (i.e., its maintainability), and also 
whether maintenance staff, equipment, and tools are available (i.e., the maintenance 
support). 
The average unavailability, Aavg, can be expressed as 
- The average probability that the item will not be able to perform the specified 
function at some future point of time t 
- The average proportion of time (in the future) where the item is not able to 
perform the specified function 
*w 

4 2 0 
ELEMENTS OF PROBABILITY THEORY 
fl 
EXAMPLE A.ll 
Gas detector 
Consider a gas detector that has been installed to detect and alarm gas releases 
of a specific type. The specific function mentioned above is then "detect and 
alarm gas release." No other function or failure modes (e.g., false alarm) are 
considered in this example. Assume that the average unavailability of the gas 
detector has been determined to be Aavg = 5 · 10-3 = 0.005. This means that if 
a gas release occurs at a random time in the future, the average probability that 
the gas detector will fail to raise an alarm is 5 · 10~3, which means that it, on 
the average, will fail once per 2 000 gas releases. It also means that the average 
proportion of time the gas detector is not able to function is 5 · 10~3. If the gas 
detector is in continuous operation (i.e., 8760 hours per year), this proportion 
corresponds to 
T- 
r. <Λ_ι 
8 760 hours 
.„ „ . 
AavR = 5-10 
· 
«a 43.8 hours per year 
1 year 
When all uptimes and downtimes are independent and the item is "as good as 
new" after a repair action, the uptimes T\, T2,... will be independent and identi-
cally distributed random variables with a common mean time to failure, MTTF. In 
the same way, the downtimes D\, D2,... will also be independent and identically 
distributed random variables with a common mean downtime, MDT. 
Now, consider a large number n of cycles (consisting of one uptime 7) and the 
associated downtime £>,·). In the timespan of the n cycles, the average proportion of 
time the item is not able to function is 
Σ?=,Α· 
£Σ7-ιΑ 
Σ7=ι τι + Σ7=ι Di ι Σ»=1 r, + ± EU Di 
which is the average downtime divided by the average cycle time. When n increase 
the average uptime will be approximately identical to the MTTF and the average 
downtime will approximate the MDT1 The average availability in this case, will 
therefore be 
MDT 
Λανκ = 
(A.47) 
g 
MTTF + MDT 
Note that formula (A.47) is applicable only when the assumptions above are fulfilled. 
The formula will be wrong if the repair action does not return the item to an "as good 
as new" condition, if the up- and downtimes are not independent and identically 
distributed, or if the item is subject to periodic proof testing and associated repair. 
1 In some textbooks and guidelines the mean downtime is called mean time to repair, MTTR. The term 
MDT is, however, considered to be more appropriate as the downtime may comprise much more than the 
actual repair action. 

REPAIRABLE ITEMS 
421 
If the item has constant failure rate λ, such that MTTF = l/λ and MTTF » 
MDT, (A.47) may be approximated by 
MDT 
Ι/λ+MDT 
Aavg = , ,, , „ _ 
« λ · MDT 
(A.48) 
β 
EXAMPLE A.12 Gas detector (cont.) 
Reconsider the gas detector in Example A. 11 and assume that the mean time 
to failure (MTTF) of this gas detector has been estimated to be 2000 hours. 
Assume that the gas detector has a built-in test and a beeper that will make a 
sound when the detector fails. The average downtime MDT from the detector 
fails until it is repaired and put into function again has been estimated to 10 
hours. From (A.47) the average unavailability of the gas is 
MDT 
10 hours 
10 
A 
= 
_ 
— 
ss 5 · 10' 
avg 
MTTF + MDT 
2000 hours + 10 hours 
2010 
-3 
Here, we assume that the gas detector is not subject to any proof testing. 
φ 


ACRONYMS 
AC 
ALARP 
ANSI 
ATS 
BDD 
BPCS 
CCF 
CCPS 
CF 
CPN 
CPU 
CSU 
D 
DC 
DD 
DD-CCF 
Architectural constraint 
As low as reasonably practicable 
American National Standards Institute 
Automatic train stop 
Binary decision diagram 
Basic Process Control System 
Common-cause failure 
Center for Chemical Process Safety 
Correction factor 
Colored Petri net 
Central processing unit 
Critical safety unavailability 
Dangerous 
Diagnostic coverage 
Dangerous detected (failure) 
CCF comprising only DD failures 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
423 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

424 
ACRONYMS 
DGF 
DIN 
DPS 
DTU 
DU 
DU-CCF 
E/E/PE 
EN 
EHSR 
ESD 
EU 
EUC 
FAT 
FE 
FMECA 
FMEDA 
FRACAS 
FSA 
FTC 
FTA 
FTF 
GSPN 
HAZID 
HAZOP 
HE 
HEF 
HER 
HFT 
HIPPS 
HFT 
HPP 
HSE 
HSE 
IEC 
IEEE 
Dangerous group failure 
Deutches Institut für Normung 
Dynamic positioning system 
Downtime unavailability 
Dangerous undetected (failure) 
CCF comprising only DU failures 
Electrical, electronic, programmable electronic 
European norm 
Essential health and safety requirements 
Emergency shutdown 
European Union 
Equipment under control 
Factory acceptance test 
Final element 
Failure modes, effects, and criticality analysis 
Failure modes, effects, and diagnostic analysis 
Failure reporting analysis and corrective action system 
Functional safety assessment 
Failure to close 
Fault tree analysis 
Fail to function 
Generalized stochastic Petri net 
Hazard identification 
Hazard and operability analysis 
Hazardous event 
Hazardous event frequency 
Hazardous event rate 
Hardware fault tolerance 
High-integrity pressure protection system 
Hardware fault tolerance 
Homogeneous Poisson process 
Health, safety, and environment 
Health and Safety Executive (UK) 
International Electrotechnical Commission 
Institute of Electrical and Electronic Engineers 

IEV 
ISA 
ISO 
LCP 
LOPA 
LS 
MBF 
MDD 
MDT 
;MFSC 
MRT 
MTBDe 
MTBF 
MTT 
MTTF 
MTTR 
NTNU 
PES 
PFD 
PFH 
PL 
PLC 
PN 
PSA 
PST 
PT 
PTC 
RBD 
RG 
RIAC 
ROCOF 
RRF 
SAN 
SAR 
SAT 
International Electrotechnical Vocabulary, see 
h t t p : / / w w w . e l e c t r o p e d i a . o r g 
International Society for Automation 
International Organization for Standardization 
Leakage in closed position 
Layer of protection analysis 
Logic solver 
Multiple beta-factor 
Mean duration of a demand 
Mean downtime 
Multiple failures with shared cause 
Mean repair-time 
Mean time between demands 
Mean time between failures 
Mean test time 
Mean time to failure 
Mean time to restore 
Norwegian University of Science and Technology 
Programmable electronic system 
Probability of failure on demand 
Frequency of dangerous failures per hour 
Performance level 
Programmable electronic controller 
Petri net 
Petroleum Safety Authority 
Partial stroke testing 
Pressure transmitter 
Proof test coverage 
Reliability block diagram 
Reachability graph 
Reliability Information Analysis Center 
Rate of occurrence of failures 
Risk-reducing factor 
Stochastic activity network 
Safety analysis report 
Site acceptance test 

4 2 6 
ACRONYMS 
SD 
SD 
SDV 
SFF 
SIF 
SIL 
SIS 
SO 
SO-CCF 
SRCF 
SRECS 
SRS 
ST 
STL 
STR 
SU 
SWIFT 
TOP 
UPM 
Safe detected (failure) 
Standard deviation 
Shutdown valve 
Safe failure fraction 
Safety instrumented function 
Safety integrity level 
Safety-instrumented system 
Spurious operation 
CCF comprising only SO failures 
Safety-related control function 
Safety-related electrical control system 
Safety requirements specification 
Spurious trip 
Spurious trip level 
Spurious trip rate 
Safe undetected (failure) 
Structured what-if technique 
Top event (in a fault tree) 
Unified partial method 

SYMBOLS 
A 
Transition rate matrix as used in the Markov approach 
A; 
Transition rate matrix for transient states in a Markov process 
A a 
Transition rate matrix for absorbing states in a Markov process 
α,-y 
Transition rate from state ( to state j in a Markov model 
A(t) 
The time-dependent availability of an item at time r, i.e., A(t) = Vx(X(t) — 1) 
A(t) 
The time-dependent unavailability of an item at time t, i.e., A(t) = 1 — A(t) 
ß 
The fraction of all failures of an item that are common-cause failures. When both 
DD and DU failures are considered, ß is related to the DU failures 
ßu 
The fraction of all DD failures of an item that are common-cause failures 
ßso 
The fraction of SO failures of an item that are common-cause failures 
Ckoon 
Configuration factor used by PDS-method in CCF modeling 
D\ 
Unknown downtime in a proof test interval (i.e., due to DU failure) 
V 
Down-states in a Markov model, i.e., the subset of the state space X for which the 
system is not functioning (down) 
427 

428 
SYMBOLS 
e 
Single outcome in a random experiment 
E 
Event in a random experiment 
E(Y) 
Expected (i.e., mean) value of a random variable Y 
fit) 
Probability density function, fit) = 
-jjF(t) 
F(t) 
Probability distribution function, i.e., Fit) = ΡΓ(Γ < t) 
f*is) 
Laplace transfer of the function 
fit) 
Π 
Identity matrix (size appropriate to the problem) 
/ (/ | / ) 
Birnbaum's measure of importance of item / at time t 
/ F V(i | i) 
Fussell-Vesely's measure of importance of item ('at time / 
Π 
Intersection of sets/events, used as E\ Π Ei 
M 
OR-relation, used as x\ II X2- The symbol is read as 'ΐρ" 
kootl 
Â:oon:G 
Â:oon:F 
λ 
λ(0 
AD 
ADD 
Ade 
ADU 
λ<'"> 
As 
ASD 
Aso 
Asu 
AV(0 
C[fit)\ 
MRT 
MTTR 
A A:-out-of-n voting. The system functions when at least k out of the n items func-
tion. Special cases are 1οο2, 1οο3, 2oo3, and so on 
Same as koon 
A /c-out-of-« voting for which the system fails when at least k out of the n items fail 
Constant failure rate of an item 
Failure rate of dependent (i.e., common-cause failures) 
Failure rate of an item with respect to all dangerous (D) failures (i.e., both DD and 
DU failures, such that AD = ADD + ADU 
Failure rate of an item (channel or subsystem) with respect to dangerous detected 
(DD) failures 
The demand rate, i.e., same as frequency of demands 
Failure rate of an item (channel or subsystem) with respect to dangerous undetected 
(DU) failures 
Failure rate of independent failures 
Failure rate of all safe failures of an item, such that As = ASD + Asu 
Failure rate of all safe detected (SD) failures of an item 
Failure rate of spurious operation (SO) of an item 
Failure rate of all safe undetected (SU) failures of an item 
The Vesely failure rate 
The Laplace transform of the function fit). Also written as 
f*is) 
Mean repair time. Used for the repair of faults revealed in a proof test 
Mean time to restoration. Used for faults detected by diagnostics. Comprises the 
time from when the failure occurs until it is detected and the repair time of the fault 

SYMBOLS 
429 
μ 
Repair rate for an item 
Pi (j) 
The probability that a process or system is in state ; at time t. Called state probability 
in Markov models 
P(t) 
The time derivative of the probability P(t),i.e., P(t) = $-,P{t) 
PFD(?) 
Probability of failure on demand as a function of time t 
PFDavg 
Average probability of failure on demand. The average of PFD(i) is taken over a 
rather long time interval 
PFDavg 
PFDavg for the final element subsystem 
PFDaVg 
PFDavg for the logic solver subsystem 
fS) 
PFDavg 
PFDavg for the sensor subsystem 
ΡΓ(Λ) 
The probability that event A will occur 
Π 
Product of numbers, used as Π;=ι xi ~ 
χ\χ2χ^ 
φ(Χ) 
Structure function for a system 
qi (?) 
Probability of basic event i in a fault tree 
Qo(t) 
Probability of the TOP event is occurring in a fault tree at time t 
Qj (t ) 
Probability that minimal cut set j is in a fault state at time t 
R(t) 
Survivor function for an item, i.e., R(t) = Ρτ(Τ > t) 
S 
Sample space 
S 
The state space for a Markov process, i.e., the set of all possible states for the process 
T 
Time to failure 
'CE 
Channel-equivalent mean downtime for a koon voted subsystem 
'GE 
Voted group-equivalent mean downtime. This is the combined mean downtime for a 
voted group of channels 
r 
The time interval between two consecutive proof tests, called the proof test interval. 
In IEC 61508, this interval is called TI 
ro 
The time interval between two consecutive diagnostic tests, called the diagnostic test 
interval 
r^e 
The mean time between two consecutive demands 
U 
Up-states in a Markov model, i.e., the subset of the state space X in which the system 
is functioning (up) 
U 
Union of sets/events, used as E\ U £2 
w(t) 
The unconditional failure rate function of a process. Also called rate of occurrence 
of failures (ROCOF) 

4 3 0 
SYMBOLS 
X(t) 
State variable for an item or a Markov process 
X 
The state space of a Markov model, i.e., the collection of the possible states of the 
system 
z(t) 
The failure rate function of an item 
1(0, τ) 
Average failure rate in the interval (0, τ), calculated as 1(0, τ) = ^ JQ z(t) dt 

Bibliography 
Abrahamsson, M. (2002). Uncertainty in Quantitative Risk Analysis - Characterisation and 
Methods of Treatment. PhD thesis, Department of Fire Safety Engineering, Lund Univer-
sity, Lund, Sweden. 
Ali, R. (2004). Problems, concerns and possible solutions for testing (and diagnostics cover-
age) of final control element of SIF loops. Technical Papers of ISA, 454, 995-1002. 
Ali, R. & Goble, W. M. (2004). Smart positioners to predict health of ESD valves. In Proceed-
ings of the Annual Symposium on Instrumentation for the Process Industries, (pp. 29-37)., 
Sellersville, PA. 
ANSI/ISA 84.01-1996 (1996). Application of Safety Instrumented Systems for the Process In-
dustries: Parts 1-5. Technical report, ISA - The Instrumentation, Systems, and Automation 
Society, Research Triangle Park, NC. 
Aven, T. (2008). Risk Analysis: Assessing Uncertainties Beyond Expected Values and Proba-
bilities. Chichester, UK: Wiley. 
Aven, T. (2011). Quantitative Risk Assessment. The Scientific Platform. Cambridge, UK.: 
Cambridge University Press. 
Ayyub, B. M. (2001). Elicitation of Expert Opinions for Uncertainty and Risk. Boca Raton, 
FL: CRC Press. 
431 

432 
BIBLIOGRAPHY 
Baradits, G. (2010). Safety instrumented system management. PhD thesis, University of 
Pannonia, Veszprém, Hungary. 
Bause, F. & Kritzinger, P. (2002). Stochastic Petri Nets: An Introduction to the Theory (2nd 
ed.). Braunschweig, Germany: Vieweg. 
Baybutt, P. (2007). An improved risk graph approach for determination of safety integrity 
levels (sils). Process Safety Progress, 26(1), 66-76. 
Beckman, L. (1995). Match redundant system architectures with safety requirements. Chem-
ical Engineering Progress, 54—61. 
Birnbaum, Z. W. (1969). On the importance of different components in a multicomponentsys-
tem. In P. R. Krishnaiah (Ed.), Multivariate Analysis (pp. 581-592). San Diego: Academic 
Press. 
Boulanger, J.-L. (2010). Safety of Computer Architectures. Hoboken, NJ: Wiley. 
Brissaud, F., Charpentier, D., Fouladirad, M., Barros, A., & Berenguer, C. (2010). Failure rate 
evaluation with influencing factors. Journal of Loss Prevention in the Process Industries, 
23, 187-193. 
Cacheux, P.-J., Collas, S., Dutuit, Y., Folleau, C, Signoret, J.-R, & Thomas, P. (2013). As-
sessment of the expected number and frequency of failures of periodically tested systems. 
Reliability Engineering and System Safety, 118, 61-70. 
Cartwright, J., Stadterman, T., Jackson, M., & Huang, Z. (1999). Draft assessment of reliabil-
ity prediction methodologies. Technical report, CALCE Electronic Products and Systems 
Consortium. 
CCPS (2001). Layer of Protection Analysis: Simplified Process Risk Assessment. New York, 
NY: Center for Chemical Process Safety of AIChE. 
CCPS (2007). Guidelines for Safe and Reliable Instrumented Protective Systems. Hoboken, 
New Jersey: Wiley (Center for Chemical Process Safety of AIChE). 
Childs, J. A. & Mosleh, A. ( 1999). A modified FMEA tool for use in identifying and assessing 
common cause failure risk in industry. In Proceedings Annual Reliability and Maintain-
ability Symposium. 
Cocozza-Thivent, C. (1997). Processus Stochastiques et Fiabilité des Systèmes (in French). 
Paris: Springer. 
Cooper, S. E., Lofgren, E. V., Samanta, P. K., & Wong, S.-M. (1993). Dependent failure 
analysis of NPP data bases. Nuclear Engineering and Design, 142, 137-153. 
David, R. & Alla, H. (2004). Discrete, Continuous, and Hybrid Petri Nets. Springer. 
De Rocquigny, E., Devictor, N., & Tarantola, S. (2008). Uncertainty in Industrial Practice: A 
Guide to Quantitative Uncertainty Management. Chichester, UK: Wiley. 
Department of Labour (1987). Robot safety. Technical report, Department of Labour, Indus-
trial Welfare Division, Wellington, New Zealand. 

BIBLIOGRAPHY 
433 
DNV (2008). Guideline for qualification of upstream process technology. Technical report, 
Det Norske Veritas, H0vik, Norway. 
DNV-RP-A203 (2011). Qualification procedures for new technology. Recommended practice, 
Det Norske Veritas, H0vik, Norway. 
DOE-STD-1195 (2011). Design of Safety Significant Safety Instrumented Systems Used at 
DOE Nonreactor Nuclear Facilities. Washington, DC: U.S. Department of Energy. 
Droguett, E. & Mosleh, A. (2008). Bayesian methodology for model uncertainty using model 
performance data. Risk Analysis, 28, 1457-1476. 
Dutuit, Y, Chatelet, E., Signoret, J. P., & Thomas, P. (1997). Dependability modelling and 
evaluation using stochastic petri nets: application to two test cases. Reliability Engineering 
and System Safety, 55(2), 117-124. 
Dutuit, Y, Innal, E, Rauzy, A., & Signoret, J.-P. (2008). Probabilistic assessments in relation-
ship with safety integrity levels by using fault trees. Reliability Engineering and System 
Safety, 93(12), 1867-1876. 
EASIS (2006). EASIS - Electronic architecture and safety engineering for integrated safety 
systems. Technical report, European Commission, Brussels. 
ECSS-Q-HB-30-08A (2011). Space product assurance, components reliability data sources 
and their use. Handbook, ESA-ESTEC, Noordwijk, The Netherlands. 
EN 954-1 (1997). Safety of machinery. Safety related parts of control systems. General prin-
ciples for design. European norm, CENELEC. 
EPSMA (2005). Guidelines to understanding reliability prediction. EPSMA guideline, Euro-
pean Power Supply Manufacturer Association, Wellingborough, UK. 
EU (2012). Directive 2012/18/EU of 4 July 2012 on the Control of Major-Accident Hazards 
Involving Dangerous substances, (Seveso III Directive). Official Journal of the European 
Communities, L 197/1. 
EU-2006/42/EC (2006). Council Directive 2006/42/EC of 17 May 2006 on machinery. Brus-
sels: Official Journal of the European Union, L 157/24 (2006). 
Evans, M. G. K., Parry, G. W., & Wreathall, J. (1984). On the treatment of common-cause 
failures in system analysis. Reliability Engineering, 9, 107-115. 
exida (2007). Safety Equipment Reliability Handbook (3rd ed.). Sellersville, PA: exida.com. 
exida (2010). Position paper on IEC 61508 (2010). Definitions regarding minimum hardware 
fault tolerance / architectural constraints. Position paper, exida.com, Sellersville, PA. 
Felgner, F. & Frey, G. (2011). Multi-phase Markov models for functional safety prediction. 
IEEE Explore, 133-140. 
Fleming, K. N. (1975). A reliability model for common mode failures in redundant safety 
systems. Technical Report GA-A13284, General Atomic Company, San Diego, CA. 

4 3 4 
BIBLIOGRAPHY 
FMD-2013 (2013). Failure Mode/Mechanism Distribution. New York: Reliability Informa-
tion Analysis Center (RIAC). 
Gentile, M. & Summers, A. (2006). Random, systematic, and common cause failure: How do 
you manage them? Process Safety Progress, 25(4), 331-338. 
Goble, W. M. (2005). Implementing the new ANSI/ISA 84.01-2004 standard. Hydrocarbon 
Processing, 84(10), 118. 
Goble, W. M. & Brombacher, A. C. (1999). Using a failure modes, effects and diagnostic 
analysis (FMEDA) to measure diagnostic coverage in programmable electronic systems. 
Reliability Engineering and System Safety, 66(2), 145-148. 
Goble, W. M. & Cheddie, H. L. (2005). Safety Instrumented Systems Verification. Research 
Triangle Park, NC: The Instrumentation, Systems, and Automation Society (ISA). 
Grebe, J. C. & Goble, W. M. (2007). FMEDA - Accurate product failure metrics. Technical 
report, exida.com, Sellersville, PA. 
Gruhn, P. & Cheddie, H. (2006). Safety Instrumented Systems: Design, Analysis, and Justifica-
tion (2nd ed.). Research Triangle Park, NC: The Instrumentation, Systems, and Automation 
Society (ISA). 
Hauge, S., Onshus, T, 0ien, K., Gr0tan, T. O., Holmstr0m, S., & Lundteigen, M. A. (2006). 
Independence of safety systems on offshore oil and gas installations - status and challenges 
(in Norwegian). STF50 A06011, SINTEF, Trondheim, Norway. 
Hoekstra, B. (2005). Safety integrity - not only a matter of reliable hardware. Business 
Briefing: Exploration and Production: The Oil and Gas Review, 114-117. 
Hokstad, P. & Corneliussen, K. (2004). Loss of safety assessment and the IEC 61508 standard. 
Reliability Engineering and System Safety, 83, 111-120. 
Hokstad, P. & Rausand, M. (2008). Common cause failure modeling: Status and trends. In 
K. B. Misra (Ed.), Handbook of Performability Engineering chapter 39, (pp. 621-640). 
London: Springer. 
HSE (2002). Principles for proof testing of safety instrumented systems in the chemical in-
dustry. CRR 428/2002, Health and Safety Executive, Norwich, UK. 
HSE (2003). Out of control - Why control systems go wrong and how to prevent failure. 
Colegate, Norwich: Health and Safety Executive (HSE), 2nd edition. 
HSE (2007). Managing competence for safety-related systems. Part 1 Key guidance; Part 2 
Supplementary material. Technical note, Health and Safety Executive, Norwich, UK. 
Humphreys, R. A. (1987). Assigning a numerical value to the beta factor common cause 
evaluation. In Proceedings: Reliability'87, volume 2C. 
IAEA (2009). Protecting against common cause failures in digital I&C systems of nuclear 
power plants. Technical report NP-T-1.5, International Atomic Energy Agency, Vienna. 
IEC 60812 (2006). Analysis Techniques for System Reliability - Procedures for Failure Mode 
and Effect Analysis (FMEA) (2nd ed.). Geneva: International Electrotechnical Commission. 

BIBLIOGRAPHY 
4 3 5 
IEC 61025 (2006). Fault Tree Analysis (FTA) (2nd ed.). Geneva: International Electrotechni-
cal Commission. 
IEC61078 (2006). Analysis Techniques for Dependability - Reliability Block Diagram and 
Boolean Methods (2nd ed.). Geneva: International Electrotechnical Commission. 
IEC 61165 (2006). Application of Markov Techniques (2nd ed.). Geneva: International Elec-
trotechnical Commission. 
IEC 61508 (2010). 
Functional Safety of Electrical/Electronic/Programmable Electronic 
Safety-Related Systems. Part 1-7. Geneva: International Electrotechnical Commission. 
IEC 61511 (2003). Functional Safety - Safety Instrumented Systems for the Process Industry. 
Geneva: International Electrotechnical Commission. 
IEC 61513 (2004). Nuclear power plants - Instrumentation and control for systems impor-
tant to safety - General requirements for systems. Geneva: International Electrotechnical 
Commission. 
IEC 61709 (2011). Electric Components - Reliability - Reference Conditions for Failure 
Rates and Stress Models for Conversion (2nd ed.). Geneva: International Electrotechnical 
Commission. 
IEC 62061 (2005). Safety of Machinery - Functional Safety of Safety-related Electrical, Elec-
tronic and Programmable Electronic Control Systems. Geneva: International Electrotech-
nical Commission. 
IEC 62278 (2002). Railway applications - Specification and demonstration of reliability, 
availability, maintainability and safety (RAMS). Geneva: International Electrotechnical 
Commission. 
IEC 62279 (2002). Railway applications - Communications, signalling, and processing sys-
tems - Software for railway control and protection systems. Geneva: International Elec-
trotechnical Commission. 
IEC 62425 (2007). Railway applications - Communication signalling and processing systems 
- safety rleated electronic systems for signalling. Geneva: International Electrotechnical 
Commission. 
IEC 62551 (2012). Analysis Techniques for Dependability: Petri Net Techniques. International 
Electrotechnical Commission. 
IECTR 62061-1 (2010). Guidance on the application of ISO 13849-1 and IEC62061 in the 
design of safety-related control systems for machinery. Geneva: International Electrotech-
nical Commission. 
IEC TR 62380 (2004). Reliability data handbook - Universal model for reliability prediction 
of electronic components, PCBs and equipment. Geneva: International Electrotechnical 
Commission. 
Innal, F. (2008). Contribution to modelling safety instrumented systems and to assessing their 
performance. Critical analysis of IEC 61508 standard. PhD thesis, University of Bordeaux, 
Bordeaux, France. 

4 3 6 
BIBLIOGRAPHY 
ISA-TR84.00.02-4 (2002). Safety instrumented functions (SIF) - safety integrity level (SIL) 
evaluation techniques. Part 4: Determining the SIL of a SIF via Markov analysis. Technical 
report, The Instrumentation, Systems, and Automation Society, Research Triangle Park, 
NC. 
ISA TR 84.00.02 (2002). Safety Instrumented Functions (SIF) - Safety Integrity Level (SIL) 
Evaluation Techniques. Parts 1-5. The instrumentation, systems, and automation society. 
ISA TR 84.00.03 (2002). Guidance for Testing of Process Sector Safety Instrumented Func-
tions (SIF) Implemented as or within Safety Instrumented Systems (SIS). Research Triangle 
Park, NC: Instrumentation, Systems, and Automation Society. 
ISO 10418 (2003). Petroleum and natural gas industries - Offshore production installations, 
basic surface process safety systems. Geneva: International Organization for Standardiza-
tion. 
ISO 12100 (2010). Safety of machinery - General principles for design - Risk assessment and 
risk reduction. Geneva: International Organization for Standardization. 
ISO 13849-1 (2006). Safety of Machinery - Safety-Related Parts of Control Systems - Part 1: 
General Principles for Design. Geneva: International Organization for Standardization. 
ISO 14224 (2006). Petroleum, petrochemical and natural gas industries - Collection and ex-
change of reliability and maintenance data for equipment. Geneva: International Standards 
Organization. 
ISO 26262 (2011). Road Vehicles - Functional Safety. Geneva: International Standardization 
Organization. 
ISO/DTR 12489 (2012). Petroleum, petrochemical and natural gas industries - reliability 
modeling and calculation of safety systems. Draft technical report, International Organiza-
tion for Standardization, Geneva. 
ISO/IEC 15909-1 (2004). Software and System Engineering - High-level Petri Nets, Part 1: 
Concepts, Definitions and Graphical Notation. International Organization for Standardiza-
tion/International Electrotechnical Commission. 
Jensen, K. & Kristensen, L. M. (2009). Coloured Petri Nets: Modelling and Validation of 
Concurrent Systems. Heidelberg, Germany: Springer. 
Jin, H., Lundteigen, M. A., & Rausand, M. (2011). Reliability performance of safety instru-
mented systems: A common approach for both low- and high-demand mode of operation. 
Reliability Engineering and System Safety, 96(3), 365 - 373. 
Jin, H., Lundteigen, M. A., & Rausand, M. (2012). Uncertainty assessment of reliability 
estimates for safety-instrumented systems. Proceedings of the Institution of Mechanical 
Engineers, Part O: Journal of Risk and Reliability, 226(6), 646-655. 
Jin, H., Lundteigen, M. A., & Rausand, M. (2013). New PFH-formulas for k-out-of-n:F-
systems. Reliability Engineering and System Safety, 111(0), 112-118. 
Jin, H. & Rausand, M. (2014). Reliability of safety-instrumented systems subject to partial 
testing and common-cause failures. Reliability Engineering and System Safety, 121(0), 
146-151. 

BIBLIOGRAPHY 
437 
Johansen, I. L. & Rausand, M. (2011). Complexity in risk assessment of sociotechnical sys-
tems. In 11th International Probabilistic Safety Assessment and Management Conference 
and the Annual Europen Safety and Reliability Conference (pp. 2274—2283). Helsinki, Fin-
land: Curran Associates. 
Johnston, B. D. (1987). A structured procedure for dependent failure analysis (DFA). Relia-
bility Engineering, 19, 125-136. 
Kieureghian, A. & Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter? Structural 
Safety, 31, 102-112. 
Lagnoux, A. (2006). Rare event simulation. Probability in the Engineering and Informational 
Sciences, 20(1), 45-66. 
Littlewood, B. (1996). The impact of diversity upon common mode failures. Reliability 
Engineering and System Safety, 51, 101-113. 
Liu, Y. (2013). Follow-up testing strategies of redundant safety instrumented systems with 
dangerous detected failures. In 2013 Prognostics and System Health Management Confer-
ence, Milan, Italy. 
Liu, Y. & Rausand, M. (2011). Reliability assessment of safety instrumented systems subject 
to different demand modes. Journal of Loss Prevention in the Process Industries, 24(1), 
49-56. 
Lundteigen, M. A. & Rausand, M. (2007). Common cause failures in safety instrumented sys-
tems on oil and gas installations: Implementing defense measures through function testing. 
Journal of Loss Prevention in the Process Industries, 20(3), 218-229. 
Lundteigen, M. A. & Rausand, M. (2008a). Partial stroke testing of process shutdown valves: 
how to determine the test coverage. Journal of Loss Prevention in the Process Industries, 
21, 579-588. 
Lundteigen, M. A. & Rausand, M. (2008b). Spurious activation of safety instrumented sys-
tems in the oild and gas industry: Basic concepts and formulas. Reliability Engineering 
and System Safety, 93, 1208-1217. 
Lundteigen, M. A. & Rausand, M. (2009). Architectural constraints in IEC 61508: Do they 
have the intended effect? Reliability Engineering and System Safety, 94(2), 520-525. 
Lundteigen, M. A., Rausand, M., & Utne, I. B. (2009). Integrating RAMS engineering and 
management with the life cycle of IEC 61508. Reliability Engineering and System Safety, 
94(12), 1894-1903. 
Macdonald, D. (2004a). Practical Industrial Safety, Risk Assessment and Shutdown Systems. 
Burlington, MA: Elsevier. 
Macdonald, D. (2004b). Practical Machinery Safety. Burlington, MA: Elsevier. 
Malhotra, M. & Trivedi, K. (1995). Dependability modeling using Petri-nets. IEEE Transac-
tions on Reliability, 44(3), 428^40. 
Marseguerra, M. & Zio, E. (1996). Monte Carlo approach to PSA for dynamic process sys-
tems. Reliability Engineering & System Safety, 52(3), 227-241. 

4 3 8 
BIBLIOGRAPHY 
MIL-HDBK-217F (1995). Reliability Prediction of Electronic Equipment (F2 ed.). Washing-
ton, DC: U.S. Department of Defense. 
Miller, A. G., Kaufer, B., & Carlson, L. (2000). Activities on component reliability under the 
OECD Nuclear Energy Agency. Nuclear Engineering and Design, 198, 325-334. 
Misumi, Y. & Sato, Y. (1999). Estimation of average hazardous-event-frequency for allocation 
of safety-integrity levels. Reliability Engineering and System Safety, 66(2), 135-144. 
Modarres, M. (2006). Risk Analysis in Engineering: Techniques, Tools, and Trends. Boca 
Raton, FL.: Taylor & Francis. 
Mosleh, A., Siu, N., Smidts, C, & Lui, C. (1995). Model Uncertainty: Its Characterization 
and Quantification. Advanced Topics in Reliability and Risk Analysis. College Park, MD: 
University of Maryland. 
Murata, T. (1989). Petri nets: properties, analysis and applications. Proceedings of the IEEE, 
77(4), 541-580. 
Murthy, D. N. P., Rausand, M., & 0sterâs, T. (2008). Product Reliability; Specification and 
Performance. London: Springer. 
NEA (2004). 
International common-cause failure data exchange. 
Number 
NEA/CSNI/R(2004). Nuclear Energy Agency. 
NOG-070 (2004). Application of IEC 61508 and IEC61511 in the Norwegian Petroleum 
Industry. Stavanger, Norway: The Norwegian Oil and Gas Association. 
NORSOKS-001 (2008). Technical safety (4th ed.). Lysaker, Norway: Standards Norway. 
NSWC-11 (2011). Handbook of Reliability Prediction Procedures for Mechanical Equipment. 
West Bethesda, MD: Naval Surface Warfare Center - Carderock Division. 
NUREG-75/014 (1975). Reactor Safety: An Assessment of Accident Risk in U.S. Commercial 
Nuclear Power Plants. Washington, DC: U.S. Nuclear Regulatory Commission. 
NUREG 1855 (2009). Guidance on the Treatment of Uncertainties Associated with PRAs in 
Risk-Informed Decision Making. Washington, DC: U.S. Nuclear Regulatory Commission. 
NUREG/CR-4780 (1989). Procedures for Treating Common-Cause Failures in Safety and 
Reliability Studies, volume 2, Analytical Background and Techniques. Washington, DC: 
U.S. Nuclear Regulatory Commission. 
NUREG/CR-5485 (1998). Guidelines on modeling common-cause failures in probabilistic 
risk assessment. Technical Report NUREG/CR-5485 (INEEL/EXT-97-01327), U.S. Nu-
clear Regulatory Commission, Washington, DC. 
NUREG/CR-6268 (2007). Common-Cause Failure Database and Analysis System: Event 
Data Collection, Classification, and Coding. Washington, DC: U.S. Nuclear Regulatory 
Commission. 
NUREG/CR-6303 (1998). Method for Performing Diversity and Defense-in-Depth Analyses 
of Reactor Protection Systems. Washington, DC: U.S. Nuclear Regulatory Commission. 

BIBLIOGRAPHY 
439 
O'Connor, P. (2001). Test Engineering: A Concise Guide to Cost-Effective Design, Develop-
ment and Manufacture. West Sussex, UK: Wiley. 
OREDA (2009). OREDA Reliability Data (5th ed.). Available from: Det Norske Veritas, NO 
1322 H0vik, Norway: OREDA Participants. 
Parry, G. W. (1991). Common cause failure analysis: A critique and some suggestions. Reli-
ability Engineering and System Safety, 34, 309-326. 
Paté-Cornell, M. E. (1996). Uncertainties in risk analysis: Six levels of treatment. Reliability 
Engineering and System Safety, 54, 95-111. 
Paula, H. M, Campbell, D. J., & Rasmuson, D. M. (1991). Qualitative cause-defense ma-
trices: Engineering tools to support the analysis and prevention of common cause failures. 
Reliability Engineering & System Safety, 34(3), 389—415. 
Rahimi, M., Rausand, M, & Lundteigen, M. A. (2011). Management factors that influence 
common-cause failures of safety-instrumented systems in the operational phase. In Ad-
vances in Safety, Reliability, and Risk Management - Proceedings of the European safety 
and Reliability Conference, ESREL 2011 (pp. 2036-2044). CRC Press. 
Rasmuson, D. M. (1991). Some practical considerations in treating dependencies in pras. 
Reliability Engineering and System Safety, 34, 327-343. 
Rausand, M. (2011). Risk Assessment; Theory, Methods, and Applications. Hoboken, NJ: 
Wiley. 
Rausand, M. & H0yland, A. (2004). System Reliability Theory: Models, Statistical Methods, 
and Applications (2nd ed.). Hoboken, NJ: Wiley. 
Rausand, M. & Vatn, J. (1998). Reliability modeling of surface controlled subsurface safety 
valves. Reliability Engineering and System Safety, 61, 159-166. 
Robinson, R. M. & Anderson, K. J. (2003). SIL rating fire protection equipment. In P. Lind-
say & T. Cant (Eds.), 8th Australian Workshop on Safety Critical Systems and Software, 
volume 33. Canberra: Australian Computer Society. 
Ross, S. M. (1996). Stochastic Processes (2nd ed.). New York: Wiley. 
Ross, S. M. (2004). Introduction to Probability and Statistics for Engineers and Scientists. 
Amsterdam: Elsevier. 
Ross, S. M. (2007). Introduction to Probability Models. Amsterdam: Elsevier. 
Rouvroye, J. L. & Brombacher, A. C. (1999). New quantitative safety standards: different 
techniques, different results? Reliability Engineering and System Safety, 66(2), 121-125. 
Rouvroye, J. L. & van den Bliek, E. G. (2002). Comparing safety analysis techniques. Relia-
bility Engineering and System Safety, 75(3), 289-294. 
SAE ARP 5580 (2001). Recommended Failure Modes and Effects Analysis (FMEA) Practices 
for Non-automobile Applications. Warrendale, PA: The Engineering Society for Advancing 
Mobility in Land, Sea, Air, and Space. 

4 4 0 
BIBLIOGRAPHY 
Sanders, W. H. & Meyer, J. F. (2001). Stochastic activity networks: formal definitions and 
concepts. In Lecture Notes in Computer Science (pp. 315-343). Springer. 
Schneeweiss, W. G. (1999). Petri Nets for Reliability Modeling. LiLoLe-Verlag. 
Schönbeck, M., Rausand, M., & Rouvroye, J. L. (2010). Human and organizational factors 
in the operational phase of safety instrumented systems: A new approach. Safety Science, 
48(3), 310-318. 
SEMATECH (1992). Failure Mode and Effect Analysis (FMEA): A Guide for Continuous 
Improvement for the Semiconductor Equipment Industry. Austin, TX: SEMATECH. 
Signoret, J.-R, Dutuit, Y., Cacheux, P.-J., Folleau, C, Collas, S., & Thomas, P. (2013). Make 
your Petri nets understandable: Reliability block diagrams driven Petri nets. Reliability 
Engineering and System Safety, 113(5), 61 - 75. 
SINTEF (2013a). Reliability data for safety instrumented systems, PDS data handbook. Hand-
book STF A24443, SINTEF Safety Research, Trondheim. 
SINTEF (2013b). 
Reliability prediction methods for safety instrumented systems, PDS 
method handbook. Handbook STF A24442, SINTEF Safety Research, Trondheim, Nor-
way. 
Sklet, S. (2006). Safety barriers: Definition, classification, and performance. Journal of Loss 
Prevention in the Process Industries, 19(5), 494—506. 
Smith, D. J. (2005). Reliability, Maintainability and Risk: Practical Methods for Engineers 
(7th ed ed.). Amsterdam: Elsevier Butterworth-Heinemann. 
Smith, D. J. & Simpson, K. G. L. (2011). Safety Critical Systems Handbook: A Straightfor-
ward Guide to Functional Safety: IEC 61508 and Related Standards (3rd ed.). Oxford: 
Elsevier: Butterworth Heinemann. 
Stamatelatos, M., Apostolakis, G., Dezfuli, H., Everline, C, Guarro, S., Moieni, P., Mosleh, 
A., Paulos, T., & Yongblood, R. (2002). Probabilistic risk assessment procedures guide 
for NASA managers and practitioners. Guideline, U.S. National Aeronautics and Space 
Administration, Washington, DC. 
Stamatelatos, M., Vesely, W., Dugan, J., Fragola, J., Minarick, J., & Railsback, J. (2002). 
Fault tree handbook with aerospace applications. Handbook, U.S. National Aeronautics 
and Space Administration, Washington, DC. 
Summers, A. E. (1998). Techniques for assigning a target safety integrity level. ISA Transac-
tions, 37(2), 95-104. 
Summers, A. E. & Zachary, B. (2000). Partial-stroke testing of safety block valves. Control 
Engineering, 47(12), 87-89. 
Taylor, H. M. & Karlin, S. (1998). An Introduction to Stochastic Modeling (3rd ed.). San 
Diego, CA: Academic Press. 
Thunnissen, D. P. (2005). Propagating and Mitigating Uncertainty in th Design of Complex 
Multidisciplinary Systems. PhD thesis, California Institute of Technology, Pasadena, CA. 

BIBLIOGRAPHY 
441 
Torres-Echeverria, A. C, Martorell, S., & Thompson, H. A. (2009a). Design optimization of 
a safety-instrumented system based on RAMS+C addressing IEC 61508 requirements and 
diverse redundancy. Reliability Engineering and System Safety, 94, 162-179. 
Torres-Echeverria, A. C, Martorell, S., & Thompson, H. A. (2009b). Modelling and opti-
mization of proof testing policies for safety instrumented systems. Reliability Engineering 
and System Safety, 94, 838-854. 
Torres-Echeverria, A. C, Martorell, S., & Thompson, H. A. (2011). Modeling safety instru-
mented systems with MooN voting architectures addressing system reconfiguration for test-
ing. Reliability Engineering and System Safety, 96, 545-563. 
US DOE (1992). Root cause analysis guidance document. Technical Report DOE-NE-STD-
1004-92, U.S. Department of Energy, Office of Nuclear Energy, Washington, DC. 
Vesely, W. E. (1970). A time-dependent methodology for fault tree evaluation. Nuclear 
Engineering and Design, 13(2), 337-360. 
Vesely, W. E. (1977). Estimating common cause failure probabilities in reliability and risk 
analyses: Marshall-Olkin specializations. In J. B. Fussell & G. R. Burdick (Eds.), Nuclear 
Systems Reliability Engineering and Risk Assessment (pp. 314-341). Philadelphia: SIAM. 
Wang, Y., West, H. H., & Mannan, M. S. (2004). The impact of data uncertainty in determining 
safety intergrity level. Process Safety and Environmental Protection, 82, 393-397. 
Yoshimura, I. & Sato, Y (2008). Safety achieved by the safe failure fraction (SFF) in 
IEC 61508. IEEE Transactions on Reliability, 57(4), 662-669. 
Zio, E. & Apostolakis, G. (1996). Two methods for the structured assessment of model uncer-
tainty by experts in performance assessments of radioactive waste repositories. Reliability 
Engineering and System Safety, 54, 225-241. 
Zitrou, A. & Bedford, T. (2003). Foundation of the UPM common cause model. In Proceed-
ings ESREL 2003, (pp. 1769-1775)., Lisse, The Netherlands. Balkema. 
Zitrou, A., Bedford, T., & Walls, L. (2004). Developing soft factors inputs to common cause 
failure models. In C. Spitzer, U. Schmocker, & V. N. Dang (Eds.), Probabilistic Safety 
Assessment and Management (PSAM 7 - ESREL'04) (pp. 825-830). Berlin: Springer. 


Index 
Airbag system, 2, 3, 6, 58 
ALARP, 10, 45 
ANSI/ISA 84.00.01, 22 
Architectural constraint, 33, 34 
Availability, 137, 419 
Average downtime, 198 
Beta-factor 
Humpreys' method, 326 
IEC61508 method, 327 
IEC 62061 method, 329 
plant-specific, 325 
Unified partial method, 330 
Beta-factor model, 119, 235, 294, 317-
322, 335, 374 
multiple, 235 
nonidentical channels, 238, 239, 321 
Binary decision diagram, 117 
Binomial distribution, 101 
Binomial failure rate model, 331 
Birnbaum's measure of importance, 118, 
303 
C-factor model, 322 
CCF, 70, 119, 209, 211, 217, 235, 255, 
310 
beta-factor model, 294, 374 
binomial failure rate model, 331 
C-factor model, 322 
coupling factor, 313 
DD-CCF, 281 
defenses, 314 
DU-CCF, 294 
explicit modeling, 303, 315 
implicit modeling, 315 
Markov approach, 323-325 
multiple beta-factor model, 336, 376 
PDS method, 340 
root cause, 313 
SO-CCF, 372 
spurious, 367 
CCF event, 312 
Channel, 26 
safety function, 26 
Channel-equivalent mean downtime, 226 
Common-cause component group, 316 
Common-cause failure, see CCF 
Complementary event, 400 
Complexity, 385 
Concomitant variable, 170 
Configuration factor, 236 
Continuous mode, 29, 38, 180 
Reliability of Safety-Critical Systems: Theory and Applications, First Edition. 
443 
By Marvin Rausand. Copyright © 2014 John Wiley & Sons, Inc. 

444 
INDEX 
Correction factor, 205 
Covariate, 170 
Critical safety unavailability, 234 
Dangerous detected, see DD 
Dangerous group failure, see DGF 
Dangerous undetected, see DU 
Data dossier, 172 
DD failure, 61 
DD faiure, 32 
DD fault, 61 
DD-CCF, 189, 281, 294, 312 
De-energize-to-trip, 17, 20, 359 
Defence-in-depth, 46 
Degraded mode, 180 
Degree of belief, 382 
Demand, 31 
demand rate, 31 
duration, 32 
false, 361, 371, 374 
prolonged duration, 32, 258, 263 
rate, 30, 259 
Demanded mode, 36, 176 
DGF, 215, 223, 284-286, 289 
DHSV, 81, 213 
slam-shut, 81 
Diagnostic test, 32, 82 
coverage, 83 
discrepancy alarm, 84 
interval, 84 
self-diagnostics, 84 
DIN V 19250, 21 
Discrimination error, 360 
Distribution 
binomial, 101, 407 
conditional, 414 
exponential, 102, 123, 414 
geometric, 151, 408 
marginal, 414 
Poisson, 409 
Weibull, 417 
Downhole safety valve, see DHSV 
Downtime unavailability, 239 
DU failure, 32, 62 
DU fault, 61 
DU-CCF, 189, 294, 312 
Dynamic positioning system, 30 
Element, 26, 27 
Emergency shutdown system, see ESD 
End-user, 11 
Energize-to-trip, 17, 360 
Equipment under control, see EUC 
Error, 54 
ESD, 31 
Essential health and safety requirement, 
15 
EUC, 2, 25, 29 
Event 
complementary, 400 
Event tree analysis, 44 
Events, 400 
independent, 404 
intersection, 401 
mutually exclusive, 401 
union, 401 
exida, 60, 62, 75, 166, 168 
Exponential distribution, 102 
exSILentia, 75 
Fail-safe, 17, 20, 213, 360 
actuator, 213 
Fail-to-function, 359 
Failure 
aging, 63 
annunciation, 60, 185 
catastrophic, 59 
cause, 58 
critical, 70 
dangerous, 60, 280 
definition, 54 
degraded, 70 
dependent, 310 
effect, 58 
evident, 74 
excessive stress, 63 
failure mode, 55 
gradual, 59 
hidden, 74 
incipient, 54, 70 
mechanism, 58 
mishandling, 64 
no effect, 60 
no part, 60, 185 
random hardware, 54, 64, 68 
safe, 60, 360 
secondary, 64 
spurious, 60 
sudden, 59 
systematic, 35, 65 
test-independent, 233, 354 
Failure mode, 55, 167 
severity, 74 
Failure rate, 57, 210 
average, 274, 275 
channel-specific, 210 
common-cause, 120, 210 
constant, 279 

INDEX 
445 
function, 274, 413 
independent, 120, 210 
nonconstant, 257, 354 
unconditional, 277 
Vesely, 279 
False alarm, 192, 361 
False trip, 361 
FAR, 44 
Fatal accident rate, see FAR 
Fault 
complete, 59 
Dangerous detected, 61 
dangerous undetected, 61 
definition, 54 
detected, 60 
extended, 59 
fault mode, 55 
functional, 63 
installation, 63 
intermittent, 59 
maintenance, 63 
manufacturing, 63 
multiplicity, 333 
non-physical, 66 
partial, 59 
software, 63, 64 
systematic, 54, 64, 65, 67, 368 
undetected, 61 
Fault tree analysis, 44, 105-119, 241-248, 
303, 366 
FIDES, 169 
Final element, 20 
Fire and gas detector, 18 
Flixborough accident, 21 
FMECA, 44, 71-74, 397 
FMEDA, 75, 81, 84, 166 
Frequency of dangerous failures per hour 
average, see PFH 
instantaneous, 36 
Fukushima Daiichi accident, 12 
Function test, 78 
Functional block, 92 
Functional safety, 10 
assessment, 49 
certificate, 48 
Fussell-Vesely's measure of importance, 
119 
Gamma distribution, 258 
Gamma function, 418 
Gas detector, 354 
GRIF Workshop, 117, 162, 261, 265 
Group-equivalent mean downtime, 227 
for koon voted group, 227 
Hardware fault tolerance, see HFT 
Harmonized standard, 15 
Hazard, 42 
mechanical, 43 
Hazard analysis, 42 
Hazard and risk analysis, 42 
Hazardous event, 36, 183, 258 
Hazardous event feequency, see HF264 
HAZID, 44 
HAZOP, 44 
HEF, 37, 183, 264 
HFT, 29, 34, 185, 363 
High-demand mode, 29, 30, 38, 179 
HIPPS, 98, 101, 106 
Humphreys' method, 326 
IEC formulas 
for PFDavg, 223-233 
for PFH, 295-302 
imperfect testing, 347-348 
IEC 15909, 162 
IEC 60812, 76 
IEC61025, 106 
IEC 61078, 92 
IEC61165, 120 
IEC61508, 13, 176 
IEC61511, 13, 36, 176, 191 
IEC61513, 16 
IEC 61709, 167 
IEC 62061, 16, 177, 368 
IEC 62278, 16 
IEC 62279, 16 
IEC 62380, 167 
IEC 62425, 17 
IEC 62551, 162 
Individual risk per annum, 44 
ISO/TR 12489, 272 
ISO/TR 12489, 308, 380, 393 
ISO 10418, 46 
ISO 12100, 16, 42 
ISO 13849, 168 
ISO 14224, 70, 166, 168, 362 
ISO 26262, 16 
Job safety analysis, 89 
koon system, 94 
koon voting, 94 
Laplace transform, 129, 130, 142, 262 
Layer of protection, 7, 46 
Layer of protection analysis, 45 
Level transmitter, 360 
Logic solver, 19 
Low-demand mode, 29, 30, 37, 178 

446 
INDEX 
Machinery directive, 15, 21, 43 
Macondo accident, 12 
Maintainability, 89 
Maintenance 
corrective, 87 
preventive, 87, 89 
MAPLE, 143, 222, 257, 262 
Markov approach, 120-145, 248-264, 323-
325, 355, 377 
Markov model, 121 
Markov process, 121, 122 
absorbing state, 140 
Chapman-Kolmogorov equation, 127 
continuous, 121 
irreducible, 133 
Kolmogorov equation, 127 
mean duration of failure, 139 
memoryless property, 122, 123 
MTTF, 140 
multi-phase, 356 
sojourn time, 123 
state transition diagram, 121 
stationary, 122 
steady-state solution, 132, 249 
survivor function, 140 
time-dependent solution, 140, 260 
transient state, 140 
transition, 121 
transition probability, 122 
transition rate, 125 
transition rate matrix, 126, 249 
visit frequency, 138 
Markov property, 122, 123 
MATLAB, 257 
Mean downtime, 198, 207 
conditional, 209 
Mean repair time, 81 
Mean test-time, 192 
Mean time to first failure, 262 
Mean uptime, 199 
Mean value, 99, 407 
MechRel, 169 
Median life, 413 
Memoryless property, 416 
MFSC, 312 
MIL-HDBK 217F, 168, 170 
Minimal cut set, 96, 110 
identification, 113 
order, 111 
Minimal path set, 95 
MODSafe, 17 
Monte Carlo simulation, 118, 129, 162 
MTTF, 413 
Multiple beta-factor model, 235, 336 
Multiple failure with shared cause, 312 
NOG-070, 15, 45, 48, 79, 89 
NPRD-2011, 169 
NUREG 1855, 382, 383 
NUREG-75/014, 309 
NUREG/CR-4780, 342 
NUREG/CR-5485, 314 
NUREG/CR-6268, 342 
OREDA, 68, 70, 168, 368 
Overhaul, 181, 347 
Parallel structure, 93 
Partial stroke test, 57, 82, 352 
PDS method, 177, 186, 233, 354, 379 
CCF model, 235 
Configuration factor, 236 
configuration factor, 236 
critical safety unavailability, 234 
data handbook, 168, 233 
downtime unavailability, 234, 235, 
239 
failure classification, 67 
loss-of-safety, 233 
MBF model, 235 
method handbook, 233 
spurious operation, 376 
test-independent failure, 233 
Petri net approach, 146-163 
arc, 146 
bounded, 149 
colored, 162 
enabled transition, 147 
firing, 147 
generalized stochastic, 152 
high-level, 162 
inhibitor arc, 148 
marking, 146 
place, 146 
proof test model, 265 
RBD-driven, 156 
reachable, 147 
sequential proof-testing, 267 
simultaneous proof-testing, 267 
staggered proof-testing, 269 
stochastic, 152 
token, 146 
transition, 146 
Petri net approch, 265-272 
Petroleum Safety Authority, 22 
PFD, 182 
PFDavg, 191, 194, 196 
fault tree approach, 241-248 
IEC formula, 227 

INDEX 
447 
koon voted group, 229 
IEC formulas, 223-233 
Markov approach, 248-264 
PDS method, 233-241 
Petri net approach, 265-272 
PFH, 36, 183, 280 
average, 283 
fault tree approach, 302-304 
IEC formulas, 295-302 
Markov approach, 304-307 
PDS method, 302 
Petri net approach, 307-308 
simplified formulas, 285-295 
Pilot valve, 21 
Piper Alpha accident, 21 
Poisson process, 220, 258, 278, 369, 409 
Power supply, 194 
Preliminary hazard analysis, 44 
Pressure transmitter, 62, 67, 80 
Probability, 402 
addition rule, 402 
Bayes formula, 405 
conditional, 403 
density function, 410 
distribution function, 406 
mass function, 406 
product rule, 403 
total, 405 
uniform model, 405 
Probability density function, 274 
Probability of failure on demand, see PFDavg 
average, see PFDavg 
instantaneous, 35 
Process hazard analysis, 44 
Process safety time, 31, 179 
Programmable electronic system, 21 
Programmable logic controller, 19 
Proof test, 32, 78 
coverage, 80, 81, 344 
full, 344 
imperfect, 80, 344 
interval, 80 
non-perfect, 80 
offline, 85, 180 
online, 85, 180 
partial, 78, 82, 344 
sequential, 86 
simultaneous, 86 
staggered, 86, 221, 269 
Random experiment, 399 
Random variable, 406 
independent, 414 
Rate of occurrence of failures, see ROCOF 
RBD, 92, 110, 195 
Redundancy, 27 
active, 27 
dynamic, 27 
hardware, 27 
software, 27 
standby, 27 
cold, 27 
partly loaded, 27 
Reliability 
definition, 47 
Reliability block diagram, see RBD 
Repair 
online, 216, 217 
Repair matrix, 356 
Repair time 
non-negligible, 212 
RIAC, 168, 169 
Risk, 3 
Risk analysis, 4 
Risk graph, 45 
calibrated, 45 
Risk metric, 44 
Risk reduction, 8 
Risk-reduction factor, 38, 184 
RISKNOWLOGY, 379 
ROCOF, 183, 276, 277, 279 
average, 278 
Safe failure fraction, see SFF 
Safe state, 30, 31, 37, 60, 179, 281 
Safety analysis report, 48 
Safety barrier, 4 
active, 5 
intermediate, 36, 187 
passive, 5 
proactive, 5 
reactive, 5 
ultimate, 37, 263 
Safety control function, 3 
Safety functional requirement, 47 
Safety integrity, 33 
hardware, 33 
requirement, 47 
SIL, 33 
software, 33 
systematic, 33, 35, 67 
Safety integrity level, see SIL 
Safety life cycle, 39, 40 
Safety loop, 29, 88 
Safety protective function, 3 
Safety requirement specification, see SRS 
Safety-critical system, 1 
Safety-instrumented function, see SIF 

448 
INDEX 
Safety-instrumented system, see SIS 
Safety-related control function, 178 
Safety-related electrical control system, 16 
Safety-related protective function, 177 
Sample space, 400 
Sensitivity analysis, 117 
Series structure, 93 
Seveso accident, 21 
Seveso directive, 21 
SFF, 34, 185 
SHARPE, 161 
Shutdown valve, 55, 62, 171, 219, 347, 
363 
failure modes, 55 
spurious closure, 360 
Siemens SN 29500, 169 
SIF, 31, 45 
SIL, 33 
allocation, 45 
budget, 46 
high-demand mode, 39 
low-demand, 38 
requirement, 176 
Simplified formulas 
for PFDavg, 196-223 
for PFH, 285-295 
PDS method, 237 
SINTEF, 233, 236, 314 
SIS, 9, 25, 29 
final element subsystem, 9, 26 
item, 27 
logic solver subsystem, 9, 26 
sensor subsystem, 9, 26 
subsystem, 26 
Smart sensor, 18 
SO-CCF, 372 
SPNP, 162 
Spurious activation, 359, 360 
cause, 365 
Spurious operation, 60, 361, 362 
Spurious shutdown, 362 
Spurious trip, 56, 184, 362, 368 
Spurious trip level, 379 
Spurious trip rate, see STR 
SRS, 47 
Standard deviation, 407 
State variable, 92 
State vector, 93 
Steady-state solution, 132, 249 
Stochastic activity network, 163 
STR, 184, 369-377 
koon voted group, 372 
Markov approach, 377 
PDS method, 376 
simplified formulas, 371-375 
Structure function, 92 
Subsystem, 26 
Survivor function, 99, 140, 275, 412 
SWIFT, 44 
Systematic failure, 35 
Taylor series, 200-202 
Telcordia SR332, 169 
Time to failure, 411 
Tolerable risk, 45 
TOP event probability, 116 
Type A element, 34 
Type B element, 34 
Unavailability, 182, 419 
average, 419 
downtime, 239 
known, 194, 212 
unknown, 193, 212 
Uncertainty, 382 
aleatory, 382 
completeness, 383 
epistemic, 382 
model, 386 
parameter, 387 
propagation, 390 
Undesired event, 43 
Unified partial method, 330 
Upper bound approximation, 117 
Variance, 407 
Venn diagram, 400 
Voted group, 27, 195 
Voting, 28 
Weibull distribution, 275, 417 
X-mas tree, 20 


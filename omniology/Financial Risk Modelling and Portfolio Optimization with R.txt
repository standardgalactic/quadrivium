
Financial Risk Modelling and
Portfolio Optimization with R

Statistics in Practice
Series Advisory Editors
Marian Scott
University of Glasgow, UK
Stephen Senn
CRP-Sant´e, Luxembourg
Wolfgang Jank
University of Maryland, USA
Founding Editor
Vic Barnett
Nottingham Trent University, UK
Statistics in Practice is an important international series of texts which provide
detailed coverage of statistical concepts, methods and worked case studies in speciﬁc
ﬁelds of investigation and study.
With sound motivation and many worked practical examples, the books show
in down-to-earth terms how to select and use an appropriate range of statistical
techniques in a particular practical ﬁeld within each title’s special topic area.
The books provide statistical support for professionals and research workers
across a range of employment ﬁelds and research environments. Subject areas cov-
ered include medicine and pharmaceutics; industry, ﬁnance and commerce; public
services; the earth and environmental sciences, and so on.
The books also provide support to students studying statistical courses applied to
the above areas. The demand for graduates to be equipped for the work environment
has led to such courses becoming increasingly prevalent at universities and colleges.
It is our aim to present judiciously chosen and well-written workbooks to meet
everyday practical needs. Feedback of views from readers will be most valuable to
monitor the success of this aim.
A complete list of titles in this series appears at the end of the volume.

Financial Risk Modelling and
Portfolio Optimization with R
Bernhard Pfaff
Invesco Global Strategies, Germany
A John Wiley & Sons, Ltd., Publication

This edition ﬁrst published 2013
C⃝2013 John Wiley & Sons, Ltd
Registered Ofﬁce
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ,
United Kingdom
For details of our global editorial ofﬁces, for customer services and for information about how to apply
for permission to reuse the copyright material in this book please see our website at www.wiley.com.
The right of the author to be identiﬁed as the author of this work has been asserted in accordance with the
Copyright, Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or
transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or otherwise,
except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of
the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may
not be available in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand
names and product names used in this book are trade names, service marks, trademarks or registered
trademarks of their respective owners. The publisher is not associated with any product or vendor
mentioned in this book. This publication is designed to provide accurate and authoritative information in
regard to the subject matter covered. It is sold on the understanding that the publisher is not engaged in
rendering professional services. If professional advice or other expert assistance is required, the services
of a competent professional should be sought.
Library of Congress Cataloging-in-Publication Data
Pfaff, Bernhard.
Financial risk modelling and portfolio optimization with R / Bernhard Pfaff.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-470-97870-2 (cloth)
1. Financial risk–Mathematical models.
2. Portfolio management.
3. R (Computer program language)
I. Title.
HG106.P484 2013
332.0285′5133–dc23
2012030904
A catalogue record for this book is available from the British Library.
ISBN: 978-0-470-97870-2
Set in 10/12 pt Times Roman by Aptara Inc., New Delhi, India

Contents
Preface
xi
List of abbreviations
xiii
Part I
MOTIVATION
1
1
Introduction
3
Reference
5
2
A brief course in R
6
2.1
Origin and development
6
2.2
Getting help
7
2.3
Working with R
10
2.4
Classes, methods and functions
12
2.5
The accompanying package FRAPO
20
References
25
3
Financial market data
26
3.1
Stylized facts on ﬁnancial market returns
26
3.1.1
Stylized facts for univariate series
26
3.1.2
Stylized facts for multivariate series
29
3.2
Implications for risk models
32
References
33
4
Measuring risks
34
4.1
Introduction
34
4.2
Synopsis of risk measures
34
4.3
Portfolio risk concepts
39
References
41
5
Modern portfolio theory
43
5.1
Introduction
43

vi
CONTENTS
5.2
Markowitz portfolios
43
5.3
Empirical mean–variance portfolios
47
References
49
Part II
RISK MODELLING
51
6
Suitable distributions for returns
53
6.1
Preliminaries
53
6.2
The generalized hyperbolic distribution
53
6.3
The generalized lambda distribution
56
6.4
Synopsis of R packages for the GHD
62
6.4.1
The package fBasics
62
6.4.2
The package GeneralizedHyperbolic
63
6.4.3
The package ghyp
64
6.4.4
The package QRM
65
6.4.5
The package SkewHyperbolic
66
6.4.6
The package VarianceGamma
67
6.5
Synopsis of R packages for GLD
67
6.5.1
The package Davies
67
6.5.2
The package fBasics
67
6.5.3
The package gld
68
6.5.4
The package lmomco
69
6.6
Applications of the GHD to risk modelling
69
6.6.1
Fitting stock returns to the GHD
69
6.6.2
Risk assessment with the GHD
73
6.6.3
Stylized facts revisited
75
6.7
Applications of the GLD to risk modelling and
data analysis
78
6.7.1
VaR for a single stock
78
6.7.2
Shape triangle for FTSE 100 constituents
79
References
82
7
Extreme value theory
84
7.1
Preliminaries
84
7.2
Extreme value methods and models
85
7.2.1
The block maxima approach
85
7.2.2
rth largest order models
86
7.2.3
The peaks-over-threshold approach
87
7.3
Synopsis of R packages
89
7.3.1
The package evd
89
7.3.2
The package evdbayes
90
7.3.3
The package evir
91

CONTENTS
vii
7.3.4
The package fExtremes
93
7.3.5
The packages ismev and extRemes
95
7.3.6
The package POT
96
7.3.7
The package QRM
97
7.3.8
The package Renext
97
7.4
Empirical applications of EVT
98
7.4.1
Section outline
98
7.4.2
Block maxima model for Siemens
99
7.4.3
r block maxima model for BMW
101
7.4.4
POT method for Boeing
105
References
110
8
Modelling volatility
112
8.1
Preliminaries
112
8.2
The class of ARCH models
112
8.3
Synopsis of R packages
116
8.3.1
The package bayesGARCH
116
8.3.2
The package ccgarch
117
8.3.3
The package fGarch
118
8.3.4
The package gogarch
118
8.3.5
The packages rugarch and rmgarch
120
8.3.6
The package tseries
122
8.4
Empirical application of volatility models
123
References
125
9
Modelling dependence
127
9.1
Overview
127
9.2
Correlation, dependence and distributions
127
9.3
Copulae
130
9.3.1
Motivation
130
9.3.2
Correlations and dependence revisited
131
9.3.3
Classiﬁcation of copulae
133
9.4
Synopsis of R packages
136
9.4.1
The package BLCOP
136
9.4.2
The packages copula and nacopula
138
9.4.3
The package fCopulae
140
9.4.4
The package gumbel
141
9.4.5
The package QRM
142
9.5
Empirical applications of copulae
142
9.5.1
GARCH–copula model
142
9.5.2
Mixed copula approaches
149
References
151

viii
CONTENTS
Part III
PORTFOLIO OPTIMIZATION APPROACHES
153
10
Robust portfolio optimization
155
10.1
Overview
155
10.2
Robust statistics
156
10.2.1
Motivation
156
10.2.2
Selected robust estimators
157
10.3
Robust optimization
160
10.3.1
Motivation
160
10.3.2
Uncertainty sets and problem formulation
160
10.4
Synopsis of R packages
166
10.4.1
The package covRobust
166
10.4.2
The package fPortfolio
166
10.4.3
The package MASS
167
10.4.4
The package robustbase
168
10.4.5
The package robust
168
10.4.6
The package rrcov
169
10.4.7
The package Rsocp
170
10.5
Empirical applications
171
10.5.1
Portfolio simulation: Robust versus classical statistics
171
10.5.2
Portfolio back-test: Robust versus classical statistics
177
10.5.3
Portfolio back-test: Robust optimization
182
References
187
11
Diversiﬁcation reconsidered
189
11.1
Introduction
189
11.2
Most diversiﬁed portfolio
190
11.3
Risk contribution constrained portfolios
192
11.4
Optimal tail-dependent portfolios
195
11.5
Synopsis of R packages
197
11.5.1
The packages DEoptim and RcppDE
197
11.5.2
The package FRAPO
199
11.5.3
The package PortfolioAnalytics
201
11.6
Empirical applications
201
11.6.1
Comparison of approaches
201
11.6.2
Optimal tail-dependent portfolio against benchmark
206
11.6.3
Limiting contributions to expected shortfall
211
References
215
12
Risk-optimal portfolios
217
12.1
Overview
217
12.2
Mean–VaR portfolios
218
12.3
Optimal CVaR portfolios
223
12.4
Optimal draw-down portfolios
227

CONTENTS
ix
12.5
Synopsis of R packages
229
12.5.1
The package fPortfolio
229
12.5.2
The package FRAPO
230
12.5.3
Packages for linear programming
232
12.5.4
The package PerformanceAnalytics
236
12.6
Empirical applications
238
12.6.1
Minimum-CVaR versus minimum-variance portfolios
238
12.6.2
Draw-down constrained portfolios
242
12.6.3
Back-test comparison for stock portfolio
247
References
253
13
Tactical asset allocation
255
13.1
Overview
255
13.2
Survey of selected time series models
256
13.2.1
Univariate time series models
256
13.2.2
Multivariate time series models
262
13.3
Black–Litterman approach
270
13.4
Copula opinion and entropy pooling
273
13.4.1
Introduction
273
13.4.2
The COP model
273
13.4.3
The EP model
274
13.5
Synopsis of R packages
276
13.5.1
The package BLCOP
276
13.5.2
The package dse
278
13.5.3
The package fArma
281
13.5.4
The package forecast
281
13.5.5
The package MSBVAR
283
13.5.6
The package PairTrading
284
13.5.7
The packages urca and vars
285
13.6
Empirical applications
288
13.6.1
Black–Litterman portfolio optimization
288
13.6.2
Copula opinion pooling
295
13.6.3
Protection strategies
299
References
310
Appendix A
Package overview
314
A.1
Packages in alphabetical order
314
A.2
Packages ordered by topic
317
References
320
Appendix B
Time series data
324
B.1
Date-time classes
324
B.2
The ts class in the base package stats
327
B.3
Irregular-spaced time series
328

x
CONTENTS
B.4
The package timeSeries
330
B.5
The package zoo
332
B.6
The packages tframe and xts
334
References
337
Appendix C
Back-testing and reporting of portfolio strategies
338
C.1
R packages for back-testing
338
C.2
R facilities for reporting
339
C.3
Interfacing databases
339
References
340
Appendix D
Technicalities
342
Index
343

Preface
The project for this book began in mid-2010. At that time, ﬁnancial markets were in
distress and far from operating smoothly. The impact of the US real estate crisis could
still be felt and the sovereign debt crisis in some European countries was beginning
to emerge. Major central banks implemented measures to avoid a collapse of the
inter-bank market by providing liquidity. Given the massive ﬁnancial book and real
losses sustained by investors, it was also a time when quantitatively managed funds
were in jeopardy and investors questioned the suitability of quantitative methods for
protecting their wealth from the severe losses they had made in the past.
Two years later not much has changed, though the debate on whether quantitative
techniques per se are limited has ceased. Hence, the modelling of ﬁnancial risks
and the adequate allocation of wealth is still as important as it always has been, and
these topics have gained in importance driven by experiences since the ﬁnancial crisis
started in the latter part of the previous decade.
The content of the book is aimed at these two topics by acquainting and famil-
iarizing the reader with market risk models and portfolio optimization techniques
that have been proposed in the literature. These more recently proposed methods are
elucidated by code examples written in the R language, a freely available software
environment for statistical computing.
This book certainly could not have been written without the public provision of
such a superb piece of software as R and the numerous package authors who have
greatly enriched this software environment. I therefore wish to express my sincere
appreciation and thanks to the R Core Team members and all the contributors and
maintainers of the packages cited and utilized in this book. By the same token, I
would like to apologize to those authors whose packages I have not mentioned. This
can only be ascribed to my ignorance of their existence. Second, I would like to
thank John Wiley & Sons, Ltd. for the opportunity to write on this topic, in particular
Ilaria Meliconi who initiated this book project in the ﬁrst place and Heather Kay and
Richard Davies for their careful editorial work. A special thank belongs to Richard
Leigh for his meticulous and mindful copy-editing. Needless to say, any errors and
omissions are entirely my responsibility. Finally, I owe a debt of profound gratitude

xii
PREFACE
to my beloved wife, Antonia, who while bearing the burden of many hours of solitude
during the writing of this book remained a constant source of support.
This book includes an accompanying website. Please visit www.wiley.com/
go/financial_risk
Bernhard Pfaff
Kronberg im Taunus

List of abbreviations
2OLS
Two-stage ordinary least-squares
3OLS
Three-stage ordinary least-squares
ACF
Autocorrelation function
ADF
Augmented Dickey–Fuller
AMPL
A modelling language for mathematical programming
ANSI
American National Standards Institute
AP
Active premium
APARCH
Asymmetric power ARCH
API
Application programming interface
ARCH
Autoregressive conditional heteroscedasticity
AvDD
Average draw-down
BL
Black–Litterman
BP
Break point
CDaR
Conditional draw-down at risk
CLI
Command line interface
CLT
Central limit theorem
COM
Component object model
COP
Copula opinion pooling
CPPI
Constant proportion portfolio insurance
CRAN
Comprehensive R Archive Network
CVaR
Conditional value at risk
DBMS
Data Base Management System
DD
Draw-down
DE
Differential evolution
DR
Diversiﬁcation ratio
EDA
Exploratory data analysis
EGARCH
Exponential GARCH
EM
Expectation maximization
EMA
Exponentially weighted mean
EP
Entropy pooling
ERS
Elliott–Rothenberg–Stock
ES
Expected shortfall
EVT
Extreme value theory
FIML
Full-information maximum likelihood

xiv
LIST OF ABBREVIATIONS
GARCH
Generalized autoregressive conditional heteroscedasticity
GEV
Generalized extreme value
GHD
Generalized hyperbolic distribution
GIG
Generalized inverse Gaussian
GLD
Generalized lambda distribution
GLPK
GNU Linear Programming Kit
GMPL
GNU MathProg modelling language
GMV
Global minimum variance
GoF
Goodness of ﬁt
GOGARCH
Generalized orthogonal GARCH
GPD
Generalized Pareto distribution
GUI
Graphical user interface
HYP
Hyperbolic
IDE
Integrated development environment
i.i.d.
independent and identically distributed
IR
Information ratio
JDBC
Java-based Data Access Technology
LP
Linear program
MaxDD
Maximum draw-down
MCD
Minimum covariance determinant
MCMC
Markov chain Monte Carlo
MDA
Maximum domain of attraction
mES
Modiﬁed expected shortfall
MILP
Mixed Integer Linear Program
ML
Maximum likelihood
MLE
Maximum likelihood estimation
MPS
Mathematical Programming System
MRL
Mean residual life
MSEM
Multiple-structural equation model
mVaR
Modiﬁed value at risk
MVE
Minimum volume ellipsoid
NIG
Normal inverse Gaussian
NN
Nearest neighbour
OBPI
Option-based portfolio insurance
ODBC
Open Data Base Connectivity
OGK
Orthogonalized Gnanadesikan–Kettenring
PACF
Partial autocorrelation function
POT
Peaks over threshold
PWM
Probability weighted moments
QMLE
Quasi-maximum likelihood estimation
RDBMS
Relational Data Base Management System
RE
Relative efﬁciency
RNG
Random number generator
RPC
Remote procedure call

LIST OF ABBREVIATIONS
xv
RS
Ramberg–Schmeiser
SDE
Stahel–Donoho estimator
SMA
Simple moving average
SMEM
Structural multiple equation model
SPI
Swiss Performance Index
SVAR
Structural vector-autoregressive model
SVEC
Structural vector-error-correction model
TAA
Tactical asset allocation
TDC
Tail dependence coefﬁcient
TE
Tracking error
VAR
Vector autoregressive model
VaR
Value at risk
VECM
Vector error correction model
WMA
Weighted moving average
XML
Extended Mark-up Language
Unless otherwise stated, the following notation, symbols and variables are used.
Notation:
Boldlowercase:y, α
Vectors
Upper case: Y, 
Matrices
Greekletters: α, β, γ
Scalars
Greekletterswithˆor˜or¯
Sample values (estimates or estimators)
Symbols and Variables:
| · |
Absolute value of an expression
∼
Distributed according to
⊗
Kronecker product of two matrices
arg max
Maximum value of an argument
arg min
Minimum value of an argument
⊥
Complement of a matrix
C, c
Copula
Cor
Correlation(s) of an expression
Cov
Variance–covariance of an expression
D
Draw-down
det
Determinant of a matrix
E
Expectation operator
I
Information set
I(d)
Integrated of order d
L
Lag operator

xvi
LIST OF ABBREVIATIONS
L
(Log)-likelihood function
μ
Expected value
N
Normal distribution
ω
Weight vector
P
Portfolio problem speciﬁcation
P
Probability expression
R
Set of real numbers

Variance–covariance matrix
σ
Standard deviation
σ 2
Variance
U
Uncertainty set
Var
Variance of an expression

Part I
MOTIVATION

1
Introduction
The period since the late 1990s has been marked by ﬁnancial crises – the Asian crisis
of 1997, the Russian debt crisis of 1998, the bursting of the dot-com bubble in 2000,
the crises following the attack on the World Trade Center in 2001 and the invasion
of Iraq in 2003, the sub-prime mortgage crisis of 2007 and European sovereign debt
crisis since 2009 being the most prominent. All of these crises had a tremendous
impact on the ﬁnancial markets, in particular an upsurge in observed volatility and a
massive destruction of ﬁnancial wealth. During most of these episodes the stability
of the ﬁnancial system was in jeopardy and the major central banks were more or less
obliged to take counter-measures, as were the governments of the relevant countries.
Of course, this is not to say that the time prior to the late 1990s was tranquil – in this
context we may mention the European Currency Unit crisis in 1992–1993 and the
crash on Wall Street in 1987, known as Black Monday. However, it is fair to say that
the frequency of occurrence of crises has increased during the last 15 years.
Given this rise in the frequency of crises, the modelling and measurement of
ﬁnancial market risk have gained tremendously in importance and the focus of port-
folio allocation has shifted from the μ side of the (μ, σ) medal to its σ side. Hence,
it has become necessary to devise and employ methods and techniques that are better
able to cope with the empirically observed extreme ﬂuctuations in the ﬁnancial mar-
kets. The hitherto fundamental assumption of independent and identically normally
distributed ﬁnancial market returns is no longer sacrosanct, having been challenged
by statistical models and concepts that take the occurrence of extreme events more
adequately into account than the Gaussian model assumption does. As will be shown
in the following chapters, the more recently proposed methods of and approaches to
wealth allocation are not of a revolutionary kind, but can be seen as an evolutionary
development: a recombination and application of already existing statistical concepts
to solve ﬁnance-related problems. Sixty years after Markowitz’s seminal paper ‘Mod-
ern Portfolio Theory’, the key (μ, σ) paradigm must still be considered as the anchor
for portfolio optimization. What has been changed by the more recently advocated
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

4
MOTIVATION
approaches, however, is how the riskiness of an asset is assessed and how portfolio
diversiﬁcation, that is, the dependencies between ﬁnancial instruments, is measured,
and the deﬁnition of the portfolio’s objective per se.
The purpose of this book is to acquaint the reader with some of these recently
proposed approaches. Given the length of the book this synopsis must be selective,
but the topics chosen are intended to cover a broad spectrum. In order to foster the
reader’s understanding of these advances, all the concepts introduced are elucidated by
practical examples. This is accomplished by means of the R language, a free statistical
computing environment (see R Development Core Team 2012). Therefore, almost
regardless of the reader’s computer facilities in terms of hardware and operating
system, all the code examples can be replicated at the reader’s desk and s/he is
encouraged not only to do so, but also to adapt the code examples to her/his own
needs. This book is aimed at the quantitatively inclined reader with a background in
ﬁnance, statistics and mathematics at upper undergraduate/graduate level. The text
can also be used as an accompanying source in a computer lab class, where the
modelling of ﬁnancial risks and/or portfolio optimization are of interest.
The book is divided into three parts. The chapters of this ﬁrst part are primarily
intended to provide an overview of the topics covered in later chapters and serve
as a motivation for applying techniques beyond those commonly encountered in
assessing ﬁnancial market risks and/or portfolio optimization. Chapter 2 provides a
brief course in the R language and presents the FRAPO package accompanying the
book. For the reader completely unacquainted with R, this chapter cannot replace
a more dedicated course of study of the language itself, but it is rather intended
to provide a broad overview of R and how to obtain help. Because in the book’s
examples quite a few R packages will be presented and utilized, a section on the
existing classes and methods is included that will ease the reader’s comprehension
of these two frameworks. In Chapter 3 stylized facts of univariate and multivariate
ﬁnancial market data are presented. The exposition of these empirical characteristics
serves as motivation for the methods and models presented in Part II. Deﬁnitions used
in the measurement of ﬁnancial market risks at the single-asset and portfolio level are
the topic of the Chapter 4. In the ﬁnal chapter of Part I (Chapter 5), the Markowitz
portfolio framework is described and empirical artefacts of the accordingly optimized
portfolios are presented. The latter serve as a motivation for the alternative portfolio
optimization techniques presented in the Part III.
In Part II, alternatives to the normal distribution assumption for modelling and
measuring ﬁnancial market risks are presented. This part commences with an exposi-
tion of the generalized hyperbolic and generalized lambda distributions for modelling
returns of ﬁnancial instruments. In Chapter 7, the extreme value theory is introduced
as a means of modelling and capturing severe ﬁnancial losses. Here, the block-maxima
and peaks-over-threshold approaches are described and applied to stock losses. Both
Chapters 6 and 7 have the unconditional modelling of ﬁnancial losses in common.
The conditional modelling and measurement of ﬁnancial market risks is presented in
the form of GARCH models – deﬁned in the broader sense – in Chapter 8. Part II
concludes with a chapter on copulae as a means of modelling the dependencies
between assets.

INTRODUCTION
5
Part III commences by introducing robust portfolio optimization techniques as
a remedy to the outlier sensitivity encountered by plain Markowitz optimization. In
Chapter 10 it is shown how robust estimators for the ﬁrst and second moments can
be used as well as portfolio optimization methods that directly facilitate the inclusion
of parameter uncertainty. In Chapter 11 the concept of portfolio diversiﬁcation is
reconsidered. In this chapter the portfolio concepts of the most diversiﬁed, equal risk
contributed and minimum tail-dependent portfolios are described. In Chapter 12 the
focus shifts to downside-related risk measures, such as the conditional value at risk
and the draw-down of a portfolio. Chapter 13 is devoted to tactical asset allocation
(TAA). Aside from the original Black–Litterman approach, the concept of copula
opinion pooling and the construction of a wealth protection strategy are described.
The latter is a synthesis between the topics presented in Part II and TAA-related
portfolio optimization.
In the Appendix all the R packages cited and used are listed by name and topic.
Due to alternative means of handling longitudinal data in R, a separate chapter in the
Appendix is dedicated to the presentation of the available classes and methods. In
Appendix C it is shown how R can be invoked and employed on a regular basis for
producing back-tests, utilized for generating or updating reports and/or embedded in
an existing IT infrastructure for risk assessment/portfolio rebalancing. Because all of
these topics are highly custom-speciﬁc, only pointers to the R facilities are provided.
A section on the technicalities concludes the book.
The chapters in Parts Two and Three adhere to a common structure. First the
methods and/or models are presented from a theoretical viewpoint only. The following
section is reserved for the presentation of R packages and the last section in each
chapter contains applications of the concepts and methods previously presented. The
R code examples provided are written at an intermediate language level and are
intended to be digestible and easy to follow. Each code example could certainly be
improved in terms of proﬁling and the accomplishment of certain computations, but
at the risk of too cryptic a code design. It is left to the reader as an exercise to adapt
and/or improve the examples to her/his own needs and preferences.
All in all, the aim of this book is to enable the reader to go beyond the ordinarily
encountered standard tools and techniques and provide some guidance on when to
choose among them. Each quantitative model certainly has its strengths and draw-
backs and it is still a subjective matter whether the former outweigh the latter when it
comes to employing the model in managing ﬁnancial market risks and/or allocating
wealth at hand. That said, it is better to have a larger set of tools available than to be
forced to rely on a more restricted set of methods.
Reference
R Development Core Team 2012 R: A Language and Environment for Statistical Computing
R Foundation for Statistical Computing Vienna, Austria. ISBN 3-900051-07-0.

2
A brief course in R
2.1
Origin and development
R is mainly a programming environment for conducting statistical computations and
producing high-level graphics (see R Development Core Team 2012). These two
areas of application should be interpreted widely, and indeed many tasks that one
would not normally directly subsume under these topics can be accomplished with
the R language. The website of the R project is http://www.r-project.org. The
source code of the software is published as free software under the terms of the GNU
General Public License (GPL; see http://www.gnu.org/licenses/gpl.html).
The R language is a dialect of the S language, which was developed by John
Chambers and colleagues at Bell Labs in the mid-1970s.1 At that time the software
was implemented as FORTRAN libraries. A major advancement of the S language
took place in 1988, following which the system was rewritten in C and functions
for conducting statistical analysis were added. This was version 3 of the S language,
referred to as S3 (see Becker et al. 1988; Chambers and Hastie 1992). At that stage
in the development of S, the R story commences (see Gentleman and Ihaka 1997). In
August 1993 Ross Ihaka and Robert Gentleman, both afﬁliated with the University
of Auckland, New Zealand, released a binary copy of R on Statlib, announcing it on
the s-news mailing list. This ﬁrst R binary was based on a Scheme interpreter with
an S-like syntax (see Ihaka and Gentleman 1996). The name of R traces back to the
initials of the ﬁrst names of Ihaka and Gentleman and is by coincidence a one-letter
abbreviation to the language in the same way as S is. The announcement by Ihaka and
Gentleman did not go unnoticed and credit is due to Martin M¨achler of ETH Z¨urich,
who persistently advocated the release of R under GNU’s GPL. This then happened
in June 1995. Interest in the language grew by word of mouth, and as a ﬁrst means of
1 A detailed account of the history of the S language is accessible at http://www.stat.bell-
labs.com/S/history.html.
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

A BRIEF COURSE IN R
7
communication and coordination a mailing list was established in March 1996 which
was then replaced a year later by the electronic mail facilities that still exist today.
The growing interest in the project led to the need for a powerful distribution channel
for the software. This was accomplished by Kurt Hornik, at that time afﬁliated to
the Technische Universit¨at in Vienna. The master repository for the software (known
as the ‘Comprehensive R Archive Network’ or CRAN) is still located in Vienna,
albeit now at the Wirtschaftsuniversit¨at, and mirror server are spread all over the
globe. In order to keep pace with requested changes by users and the ﬁxing of
bugs in a timely manner, a core group of R developers was set up in mid-1997. This
established framework and infrastructure is probably the reason why R has since made
such tremendous further progress. Users can contribute packages to solve speciﬁc
problems or tasks and hence advances in statistical methods and/or computations can
be swiftly disseminated. A detailed analysis and synopsis of the social organization
and development of R is provided by Fox (2009). The next milestone in the history
of language was in 1998, when John Chambers introduced a more formal class and
method framework for the S language (version 4), which was then adopted in R (see
Chambers 1998, 2008). This evolution explains the coexistence of S3- and S4-like
structures in the R language, and the user will meet them both in Section 2.4. More
recent advancements are the inclusion of support for high-performance computations
and a byte code compiler for R. From these humble beginnings, R has become the
lingua franca for statistical computing.
2.2
Getting help
It is beyond the scope of this book to provide the reader with an introduction to the
R language itself. Those who are completely new to R are referred to the manual An
Introduction to R, available on the project’s website under ‘Manuals’. The purpose
of this section is rather to provide the reader with some pointers on obtaining help
and retrieving the relevant information for solving a problem at hand.
As already indicated in the previous paragraph, the ﬁrst resort for obtaining help
is by reading the R manuals. These manuals cover different aspects of R and the one
mentioned above provides a useful introduction to R. The following R manuals are
available, and their titles are self-explanatory:
r An Introduction to R
r The R Language Deﬁnition
r Writing R Extensions
r R Data Import/Export
r R Installation and Administration
r R Internals
r The R Reference Index

8
MOTIVATION
These manuals can either be accessed from the project’s website or invoked from an
R session by typing
> help.start()
This function will load an HTML index ﬁle into the user’s web browser and local
links to these manuals appear at the top. Note that a link to the ‘Frequently Asked
Questions’ is included, as well as a ‘Windows FAQ’ if R has been installed under
Microsoft Windows.
Incidentally, in addition to these R manuals, many complementary tutorials
and related material can be accessed from http://www.r-project.org/other-
docs.html and an annotated listing of more than 100 books on R is available
at http://www.r-project.org/doc/bib/R-books.html. The reader is also
pointed to the The R Journal (formerly R News), which is a biannual publication
covering the latest developments in R and consists of articles contributed by users.
Let us return to the subject of invoking help within R itself. As shown above, the
function help.start() as invoked from the R prompt is one of the in-built help
facilities that R offers. Other means of accessing help are:
> ## Invoking the manual page of help() itself
> help()
> ## Help on how to search in the help system
> help("help.search")
> ## Help on search by partial matching
> help("apropos")
> ## Displaying available demo files
> demo()
> demo(scoping)
> ## Displaying available package vignettes
> ?vignette
> vignette()
> vignette("parallel")
The ﬁrst command will invoke the help page for help() itself; its usage is described
therein and pointers given to other help facilities. Among these other facilities are
help.search(), apropos(), and demo(). If the latter is executed without argu-
ments, the available demonstration ﬁles are displayed and demo(scoping) then runs
the R code for familiarizing the user with the concept of lexical scoping in R, for
instance. More advanced help is provided in vignettes associated with packages. The
purpose of these documents is to show the user how the functions and facilities of a
package can be employed. These documents can be opened in either a PDF reader or
a web browser. In the last code line, the vignette contained in the parallel package is
opened and the user is given a detailed description of how parallel computations can
be carried out with R.
A limitation of these help facilities is, that with these functions only local
searches are conducted, so that the results returned depend on the R installation itself
and the contributed packages installed. To conduct an on-line search the function

A BRIEF COURSE IN R
9
RSiteSearch() is available which includes searches in the R mailing lists (mailing
lists will be covered as another means of getting help in due course).
> ## Online search facilities
> ?RSiteSearch
> RSiteSearch("Portfolio")
> ## The CRAN package sos
> ## 1. Installation
> install.package("sos")
> ## 2. Loading
> library(sos)
> ## 3. Getting an overview of the content
> help(package = sos)
> ## 4. Opening the package’s vignette
> vignette("sos")
> ## 5. Getting help on findFn
> ?findFn
> ## 6. Searching online for ’Portfolio’
> findFn("Portfolio")
A very powerful tool for conducting on-line searches is the sos package (see
Graves et al. 2011). If the reader has not installed this contributed package by now,
s/he is recommended to do so. The cornerstone function is findFn() by which on-
line searches are conducted. In the example above, all relevant entries with respect
to the keyword ‘Portfolio’ are returned into a browser window and the rightmost
column contains a description of the entries with a direct web link.
As shown above, findFn() can be used for answering questions of the form
‘Can this be achieved with R?’ or ‘Has this already been implemented in R?’. In
this respect, given that at the time of this writing almost 3700 packages are available
on CRAN (not to speak of R-Forge), the ‘task view’ concept is beneﬁcial. CRAN
packages that ﬁt into a certain category, say ‘Finance’, are grouped together and each
is brieﬂy described by the maintainer of the task view in question. Hence, the burden
of searching the archive for a certain package with which a problem or task can be
solved has been greatly reduced. Not only do the task views provide a good overview
of what is available, but with the CRAN package ctv (see Zeileis 2005) the user can
choose to install either the complete set of packages in a task view along with their
dependencies or just those considered to be core packages. A listing of the task views
can be found at http://cran.r-project.org/web/views/.
> install.packages("ctv")
> library(ctv)
> install.views("Finance")
As mentioned above, mailing lists are available, where users can post their
problem/question to a wide audience. An overview of those available is provided at
http://www.r-project.org/mail.html. Probably of most interest are R-help
and R-SIG-Finance. The former is a high-trafﬁc list dedicated to general questions

10
MOTIVATION
about R and the latter is focused on ﬁnance-related problems. In either case, before
submitting to these lists the user should adhere to the posting guidelines., which can
be found at http://www.r-project.org/posting-guide.html.
This section concludes with an overview of R conferences that have taken place
in the past and will most likely come around again in the future.
r useR!: This is an international R user conference and consists of keynote lec-
tures and user-contributed presentations which are grouped together by topic.
Finance-related sessions are ordinarily a part of these topics. The conference
started in 2004 on a biannual schedule in Vienna, but now takes place every
year at a different location. For more information, see the announcement at
http://www.r-project.org.
r R/Rmetrics: This annual conference started in 2007 and is solely dedicated
to ﬁnance-related subjects. The conference has recently been organized as
a workshop with tutorial sessions in the morning and user presentations in
the afternoon. The venue is Meielisalp, Lake Thune, Switzerland and the
conference usually takes place in the third week of June. More information is
provided at http://www.rmetrics.org.
r R in Finance: Akin to the R/Rmetrics workshop, this conference is also solely
dedicated to ﬁnance-related topics. It is a two-day event held annually during
spring in Chicago at the University of Illinois. Optional pre-conference tutorials
are given and the main conference consists of keynote speeches and user-
contributed presentations.
r DSC: DSC stands for ‘Directions in Statistical Computing’ and, as its name
indicates, is targeted at developers of statistical software. As such, the confer-
ence is not conﬁned to R itself, though the lion’s share of topics do relate to
advances in this language.
2.3
Working with R
By default, R is provided with a command line interface (CLI). At ﬁrst sight, this might
be perceived as a limitation and as an antiquated software design. This perception
might be intensiﬁed for novice users of R. However, the CLI is a very powerful tool
that gives the user direct control over calculations. The dilemma is that most probably
only experienced users of R with a good command on the language might share this
view on working with R, but do you become a proﬁcient R user in the ﬁrst place? In
order to solve this puzzle and ease the new user’s way on this learning path, several
graphical user interfaces (GUIs) and/or integrated development environments (IDEs)
are available. Incidentally, it is possible to make this rather rich set of eye-catching
GUIs and IDEs available because R is provided with a CLI in the ﬁrst place, and all
of them are factored around it.
In this section some of the platform-independent GUIs and IDEs are presented,
acknowledging the fact that R is shipped with a GUI on the Microsoft Windows

A BRIEF COURSE IN R
11
operating system only. The listing below is in alphabetical order and does not advocate
the usage of one GUI/IDE framework over another, for good reasons. Deciding which
system to use is a matter of personal preference and taste. Therefore, the reader is
invited to inspect each of the GUIs and IDEs presented and then choose whichever
is to his liking.
1. Eclipse: Eclipse is a Java-based IDE and was ﬁrst designed as an IDE for
this language. More information about Eclipse are available on the project’s
website at http://www.eclipse.org/. Since then many modules/plug-ins
for other languages have been made available. The plug-in for R is called
StatET and is distributed via http://www.walware.de/goto/statet.
Instructions for installing and conﬁguring this module into Eclipse
can be found on this website. Further on-line guidance is available
elsewhere.
2. Emacs/ESS: GNU Emacs is an extensible and customisable text editor, which
at its core is an interpreter for the Emacs Lisp language. The project’s website
is http://www.gnu.org/software/emacs/. Derived from this editor are
the distributions XEmacs (http://www.xemacs.org), where the ‘X’ in-
dicates the X window system of Unix/Linux platforms, and Aquamacs
(http://aquamacs.org/) for Mac OS X only. Similar to Eclipse, the
connection between this editor and R is established by means of a mod-
ule, ESS, which stands for Emacs Speaks Statistics. The project’s website is
http://ess.r-project.org/ where this Lisp module can be downloaded
and installation instructions are available. A strength of ESS is that other
statistical packages such as S-PLUS, SAS, Stata, OpenBUGS and JAGS are
also supported. A dedicated mailing list for ESS is available in the ‘Getting
help’ section of the website cited above. Users working in Microsoft Windows
might be interested in the prepackaged Emacs/ESS version made available by
Vincent Goulet: http://vgoulet.act.ulaval.ca/en/emacs/.
3. JGR: In contrast to Eclipse and Emacs/ESS, JGR is a GUI rather than an
IDE for R. Like Eclipse, it is based on Java, and ‘JGR’ stands for Java Gui
for R. The project’s website is http://rforge.net/JGR, where installation
instructions can be found for Microsoft Windows, Mac OS X and Linux
platforms.
4. RStudio: The latest addition to platform-independent GUIs/IDEs is RStudio.
The software is hosted at http://www.rstudio.org/ and is distributed
under the AGPLv3 license. A feature of this IDE is that it can be either
installed as a desktop application or run on a server, where users can access
RStudio via a web browser.
5. Vim: Last, but not least, there is an R plug-in for the Vim editor available. The
Vim editor itself has been available for more than twenty years. The software
is hosted at http://www.vim.org/ and is based on the Unix vi editor. The
R plug-in is contained in the section ‘Scripts’.

12
MOTIVATION
Further information about R GUIs and IDEs can be found at http://www.
sciviews.org/_rgui/, where a synopsis of available GUIs and IDEs is provided,
some of which are platform-dependent. This is quite an extensive listing, and software
solutions that might not appear as a GUI, such as a web service, are included too.
Furthermore, the reader can subscribe to a special interest group mailing list, R-
SIG-GUI, by following the instructions at https://stat. ethz.ch/mailman/
listinfo/r-sig-gui.
2.4
Classes, methods and functions
In this section a concise introduction to the two ﬂavours of class and method deﬁ-
nitions in R is provided. The ﬁrst class and method mechanism is referred to as S3
and the second as S4. Because the S4 classes and methods were included in R at a
later stage in its development cycle (since version 1.4.0), S3 classes and methods are
sometimes also called old-style classes and methods. Detailed accounts of these class
and method schemes are provided in Becker et al. (1988) and Chambers and Hastie
(1992) for S3 and in Chambers (1998, 2008) for S4. In addition to these sources,
the manual pages for classes and methods can be inspected by typing ?Classes and
?Methods, respectively. The need to familiarize oneself with these class and method
concepts is motivated by the fact that nowadays contributed R packages utilize either
one or the other concept and in some packages a link between the class and method
deﬁnitions of either kind is established. It should be noted that there are also R pack-
ages in which neither concept of object-oriented programming has been employed
at all in the sense of new class and/or method deﬁnitions, and such packages can be
viewed as collections of functions only.
Before each of the two concepts is discussed, the reader should recall that every-
thing in R is an object, that a class is the deﬁnition of an object, and that a method
is a function by which a predeﬁned calculation or manipulation is carried out on
the object. Furthermore, there are generic functions which have the sole purpose of
determining the class of the object and associating the appropriate method to it. If
no speciﬁc method can be associated to an object, a default method will be called as
a fall-back. Generic functions can therefore be viewed as an umbrella under which
all available class-speciﬁc methods are collected. The difference between S3 and
S4 classes/methods lies in how a certain class is associated to an object and how a
method is dispatched to it.
Because R is a dialect of the S language, S3 objects, classes and methods have
been available since the very beginning of R, almost 20 years ago. The assignment
of a class name and the method dispatch in this scheme are rather informal and hence
very simple to implement. No formal requirements on the structure of an S3 class
objects are necessary, it is just a matter of adding a class attribute to an object. How
swiftly such an attribute can be added to an object and/or changed is shown in the
following in-line example:
> x <−1:5
> x

A BRIEF COURSE IN R
13
[1] 1 2 3 4 5
> class(x)
[1] "integer"
> xchar <−x
> class(xchar) <−"character"
> class(xchar)
[1] "character"
> xchar
[1] "1" "2" "3" "4" "5"
Noteworthy in this example are the different shapes when x and xchar are
printed. In the former case the object is printed as a numeric vector, and in the latter
as a character vector indicated by quotation marks. This observation directly leads
on to how method dispatching is accomplished within the S3 framework. Here, a
simple naming convention is followed: foo() methods for objects of class bar are
called foo.bar(). Whence such a function is not deﬁned, then S3 method dispatch
mechanism searches for a function foo.default(). The available methods for
computing the mean of an object shall be taken as an example:
> mean
function (x, ...)
UseMethod("mean")
<bytecode: 0x9158070>
<environment: namespace:base>
> methods("mean")
[1] mean.data.frame
mean.Date
[3] mean.default
mean.difftime
[5] mean.POSIXct
mean.POSIXlt
[7] mean.simple_sparse_array*
mean.simple_triplet_matrix*
Non-visible functions are asterisked
Here, mean() is a generic function and the deﬁned methods for mean.bar()
are returned by the methods() function. As one can see from the output, apart from
the default, methods for computing the mean of quite a few other classes of objects
have been deﬁned. By now, it should be apparent that S3 classes and methods can
best be described as a naming convention, but fall short of what one assumes under
the rubric of a mature object-oriented programming approach. A major pitfall of
S3 classes and methods is that no validation process exists for assessing whether
an object claimed to be of a certain class really belongs to it or not. Sticking to
our previous example, this is exhibited by trying to compute the means for x and
xchar:
> mean(x)
[1] 3
> mean(xchar)
[1] NA

14
MOTIVATION
Given that x is of type integer, the default method for computing the mean is
invoked. Because, there is no method mean.character(), the default method is
also called for xchar. However, within this default method it is tested whether the
argument is either numeric or logical and because these test fail for xchar, an NA
value is returned and the associated warning is pretty indicative of why such a value
has been returned. Just for exemplary purposes, one could deﬁne a mean() method
for objects of class character in the sense that the average count of characters in
the strings is returned as shown next:
> mean.character <−function(x, ...){
+
ans <−mean(nchar(x, ...))
+
ans
+ }
> mean(xchar)
[1] 1
However, its simplicity and quite powerful applicability are points in favour of the
S3 system.
The S4 classes offer a rigorous deﬁnition of an object, by demanding that any valid
object must be compliant with the speciﬁed class structure. Recall that for S3 classes
no formal testing of the correct contents of an object belonging to a certain class is
required. The introduction of a more formal class mechanism is, however, associated
with a cost: complexity. Now it is no longer sufﬁcient to assign a certain object
with a class attribute and deﬁne methods by adhering to the foo.bar() naming
convention, but rather the handling of S4 classes and methods is accomplished by
a set of functions contained in methods package, which is included in the base R
installation. The most commonly encountered ones are:
r setClass() for deﬁning a new S4 class,
r new() for creating an object of a certain class,
r setGeneric() for deﬁning a function as generic,
r setMethods() for deﬁning a method for a certain class,
r as() and setAs() for coercing an object of one class into another class,
r setValidity() and validObject() for validating the appropriateness of
an object belonging to a certain class,
r showClass(), getClass(), showMethods(), findMethods() and get-
Methods() for displaying the deﬁnition/availability of S4 classes and
methods,
r slot(), getSlots(), @ for extracting elements from an object.
In the following in-line examples, it is shown (i) how a class for portfolio weights
can be deﬁned, (ii) how these objects can be validated and (iii) how methods can be
created for objects of this kind. A more elaborate deﬁnition can certainly be designed,

A BRIEF COURSE IN R
15
but the purpose of these code examples is to give the reader an impression of how S4
classes and methods are handled.
First a class PortWgt is created:
> setClass("PortWgt",
+
representation(Weights = "numeric",
+
Name = "character",
+
Date = "character",
+
Leveraged = "logical",
+
LongOnly = "logical"))
> showClass("PortWgt")
Class "PortWgt" [in ".GlobalEnv"]
Slots:
Name:
Weights
Name
Date Leveraged
LongOnly
Class:
numeric character character
logical
logical
The portfolio weight class is deﬁned in terms of a numeric vector Weights that
will contain the portfolio weights, a character string Name for naming the portfolio
associated with this weight vector, as well as a date reference, Date. In addition to
these slots, the kind of portfolio is characterized: whether it is of the long-only kind
and/or whether leverage is allowed or not. This is accomplished by including the two
logical slots LongOnly and Leveraged, respectively.
At this stage, objects of class PortWgt could in principle already be created by
utilizing new():
> P1 <−new("PortWgt", Weights = rep(0.2, 5),
+
Name = "Equal Weighted",
+
Date = "2001−03−31",
+
LongOnly = TRUE,
+
Leveraged = FALSE)
However, ordinarily a constructor function is provided for creating these objects.
Within the function body some measures for safeguarding the appropriateness of
the user-provided input can already be taken into account, but this can also be
implemented by means of a speciﬁc validation function.
> PortWgt <−function(Weights, Name, Date = NULL,
+
LongOnly = TRUE, Leveraged = FALSE){
+
Weights <−as.numeric(Weights)
+
Name <−as.character(Name)
+
if(is.null(Date)) Date <−as.character(Sys.Date())
+
ans <−new("PortWgt", Weights = Weights, Name = Name,
+
Date = Date, LongOnly = LongOnly,
+
Leveraged = Leveraged)

16
MOTIVATION
+
ans
+}
> P2 <−PortWgt(Weights = rep(0.2, 5),
+
Name = "Equal Weighted")
One of the strengths of S4 is its validation mechanism. In the above example,
for instance, an object of class PortWgt could have been created for a long-only
portfolio whereby some of the weights could have been negative, or if the portfolio
should not be leveraged, but the absolute weight sum could have been greater than
unity. In order to check whether the arguments supplied in the constructor function
are not violating the class speciﬁcation from a content point of view, the following
validation function is speciﬁed, mostly for elucidating this concept:
> validPortWgt <−function(object) {
+
if(object@LongOnly) {
+
if(any(object@Weights < 0)) {
+
return("\nNegative weights for long-only.")
+
}
+ }
+
if(!object@Leveraged) {
+
if(sum(abs(object@Weights)) > 1) {
+
return("\nAbsolute sum of weights greater than one.")
+
}
+ }
+
TRUE
+ }
> setValidity("PortWgt", validPortWgt)
Class "PortWgt" [in ".GlobalEnv"]
Slots:
Name:
Weights
Name
Date Leveraged
LongOnly
Class:
numeric character character
logical
logical
This function returns TRUE if the supplied information is valid and in concordance
with the class speciﬁcation, or an informative message otherwise:
> PortWgt(Weights = rep(−0.2, 5),
+
Name = "Equal Weighted", LongOnly = TRUE)
Error in validObject(.Object) : invalid class PortWgt
object: Negative weights for long-only.
In the above in-line statement an erroneous creation of a PortWgt object was tried
for a long-only portfolio, but with negative weights. An error message is returned
and the user is alerted that at least one weight is negative and hence in conﬂict with
the long-only characteristic.

A BRIEF COURSE IN R
17
Similarly, in the following example the sum of weights is greater than 1 for a
non-leveraged portfolio and hence the object is not created, but an informative error
message as deﬁned in validPortWgt() is returned:
> PortWgt(Weights = rep(0.3, 5),
+
Name = "Equal Weighted", Leveraged = FALSE)
Error in validObject(.Object) : invalid class PortWgt
object: Absolute sum of weights greater than one.
So far, an S4 class for portfolio weights, PortWgt, has been deﬁned and a
constructor function PortWgt() has been created as well as a function for validating
the user-supplied arguments. The rest of this section shows how S4 methods can be
deﬁned. First, a show() method for nicely displaying the portfolio weights is created
by means of setMethod():
> setMethod("show", signature = "PortWgt",
+
function(object) {
+
if(is.null(names(object@Weights))) {
+
N <−length(object@Weights)
+
names(object@Weights) <−paste("Asset", 1:N)
+
}
+
cat(paste("Portfolio:", object@Name))
+
cat("\n")
+
cat(paste("Long-Only:", object@LongOnly))
+
cat("\n")
+
cat(paste("Leveraged:", object@Leveraged))
+
cat("\n")
+
cat("Weights:\n")
+
print(object@Weights)
+
cat("\n")
+ })
[1] "show"
If the supplied weight vector has been passed to the creation of the object without
names, a generic character vector is created ﬁrst. In the rest of the body of the show()
method are calls to the function cat() by which the content of the PortWgt object
will be displayed. The result will then look like this:
> P2
Portfolio: Equal Weighted
Long-Only: TRUE
Leveraged: FALSE
Weights:
Asset 1 Asset 2 Asset 3 Asset 4 Asset 5
0.2
0.2
0.2
0.2
0.2

18
MOTIVATION
It might make sense to deﬁne a summary() method for producing descriptive
statistics of the weight vector, which is accomplished by:
> setMethod("summary", "PortWgt", function (object, ...) {
+
summary(object@Weights, ...)
+ })
[1] "summary"
> summary(P2)
Min. 1st Qu.
Median
Mean 3rd Qu.
Max.
0.2
0.2
0.2
0.2
0.2
0.2
In this method deﬁnition the already existing summary() method for objects of class
numeric is directly applied to the slot Weights. Similarly, a length() method can
be deﬁned, which returns the count of assets as the length of this vector:
> setMethod("length", "PortWgt", function(x)
+
length(x@Weights)
+
)
[1] "length"
> length(P2)
[1] 5
The reader might wonder why, in the ﬁrst instance, the function’s deﬁnition
is in terms of function(object, ...) and in the second function(x) only.
The reason lies in the differing speciﬁcations of the ‘generic’ function. This spec-
iﬁcation is displayed by invoking getMethod(’foo’) for method foo. Inciden-
tally, a skeleton of a method deﬁnition for a particular class bar is created by
method.skeleton(’foo’, ’bar’).
The next in-line example shows the creation of a generic function weights()
and an associated method for objects of class PortWgt. First, the generic function
is deﬁned without a default method and the method deﬁnition for PortWgt objects
follows next:
> setGeneric("weights", function(object)
+
standardGeneric("weights")
+
)
[1] "weights"
> setMethod("weights", "PortWgt", function(object)
+
object@Weights
+
)
[1] "weights"
> weights(P2)
[1] 0.2 0.2 0.2 0.2 0.2

A BRIEF COURSE IN R
19
It would be handy if this method could also be used for assigning new values to
the slot Weights. For this, one proceeds likewise by deﬁning:
> setGeneric("weights<−", function(x, ...,value)
+
standardGeneric("weights<−")
+
)
[1] "weights<−"
> setReplaceMethod("weights", "PortWgt",
+
function(x, ..., value) {
+
x@Weights <−value
+
x
+
})
[1] "weights<−"
> weights(P2) <−rep(0.25, 4)
> P2
Portfolio: Equal Weighted
Long-Only: TRUE
Leveraged: FALSE
Weights:
Asset 1 Asset 2 Asset 3 Asset 4
0.25
0.25
0.25
0.25
but now, because weights() is used as a replacement method, the function setRe-
placeMethod() has been invoked in its deﬁnition.
A ﬁnal example shows how a coerce() method can be created by utilizing the
setAs() function of the methods package:
> setAs(from = "PortWgt", to = "data.frame", func-
tion(from) {
+
anames <−names(from@Weights)
+
if(is.null(anames)) {
+
N <−length(from)
+
anames <−paste("Asset", 1:N)
+
}
+
ans <−data.frame(from@Date, t(weights(from)))
+
colnames(ans) <−c("Date", anames)
+
ans
+ })
> as(P2, "data.frame")
Date Asset 1 Asset 2 Asset 3 Asset 4
1 2012-05-22
0.25
0.25
0.25
0.25
In this call an object of class PortWgt is coerced into a data.frame object by
combining the date stamp and the portfolio weight vector. In this deﬁnition the
previously deﬁned length() and weights() methods have been used. Note how
this coercing method is invoked: the target class is included as a character string in

20
MOTIVATION
the call to as(). This scheme is different than the S3-style coercing methods, such as
as.data.frame(). Of course, this old-style method can also be deﬁned for objects
of class PortWgt, but this is left as an exercise for the reader.
2.5
The accompanying package FRAPO
A package accompanying to this book, FRAPO, is available on CRAN. It can also be
downloaded from the author’s website at www.pfaffikus.de. Within the package
S4 classes and methods are employed. The purpose of this package is to provide:
r the examples in this book;
r the data sets that are used in the R code listings;
r classes, methods and functions for portfolio optimization techniques and ap-
proaches that have not been covered in other contributed R packages;
r utility functions, such as the computation of returns, trend/momentum-based
indicators, and descriptive portfolio statistics.
Listing 2.1 shows ﬁrst how the package can swiftly be installed, whereby it is
assumed that the user has access to the Internet. Additional packages are automatically
installed as required. Next, the package is loaded into the work space. An overview of
the package in terms of the help topics covered is returned to the console by executing
line 6. The description part of the package is followed by an index of the help topics.
The ﬁrst entry is ‘BookEx: Utility functions for handling book examples’. The help
page for this entry is then opened by executing the command on line 8, where the
short-cut ? to the help() function is used. The functions implemented to handle
Listing 2.1 The package FRAPO.
## Installation of the book’s accompanying package
1
install.packages("FRAPO")
2
## Loading of the package
3
library(FRAPO)
4
## Overview of the help topics
5
help(package = FRAPO)
6
## Displaying help for the examples
7
?BookEx
8
## Showing the listing of R code examples
9
listEx()
10
## Returning the content of this example
11
showEx("Part1Chapter2Ex1")
12
## Executing this script
13
runEx("Part1Chapter2Ex1", echo = TRUE)
14

A BRIEF COURSE IN R
21
the R code examples covered in this book are listed in the usage section. That is,
listEx() will return a character vector of the names of the R code examples, the
function showEx() will display an R code example on the standard output (i.e., the
console), or it can be directly executed by calling runEx(), a copy of an example
can be created in the working directory by means of the function saveEx() and
can be edited by calling editEx(). The latter function comes in handy if the reader
wishes to ‘play around’ with the code or to modify it in some way (s/he is encouraged
and welcome to do so). To reiterate, a copy of the R code example shipped with
the package is ﬁrst created in the working directory, and editing this ﬁle and saving
the changes will not affect the original R code as listed in this book, unless R is
started in the package sub-directory ‘BookEx’, which is not recommended for the
above reason. In the remaining code lines, the handling of the R code examples is
elucidated.
Given the sparsity of publicly available ﬁnancial time series, some data sets are
included in the package, as listed below. Some of these data sets can be considered
as benchmark data.
r EESCBFX: ESCB FX Reference Rates
r EuroStoxx50: EURO STOXX 50
r FTSE100: FTSE 100
r INDTRACK1: Hang Seng Index and Constituents
r INDTRACK2: DAX 100 Index and Constituents
r INDTRACK3: FTSE 100 Index and Constituents
r INDTRACK4: Standard & Poor’s 100 Index and Constituents
r INDTRACK5: Nikkei 225 Index and Constituents
r INDTRACK6: Standard & Poor’s 500 Index and Constituents
r MIBTEL: Milano Indice Borsa Telematica
r MultiAsset: Multi Asset Index Data
r NASDAQ: NASDAQ
r SP500: Standard & Poor’s 500
r StockIndex: Stock Index Data
r StockIndexAdj and StockIndexAdjD: Stock Index Data, month-end and
daily
The data sets EuroStoxx50, FTSE100, MIBTEL, NASDAQ, and SP500 are used
in Cesarone et al. (2011) and can be retrieved from http://w3.uniroma1.
it/Tardella/datasets.html. These data sets comprise weekly observations of
the index constituents starting on 3 March 2003 and ending on 24 March 2008. The
authors adjusted the price data for dividends and removed stocks if two or more

22
MOTIVATION
consecutive missing values were found. In the remaining cases the NA entries were
replaced by interpolated values.
The series of data objects INDTRACK* are part of the OR library (see
http://people.brunel.ac.uk/ mastjjb/jeb/info.html) and are used in
Beasley et al. (2003) and Canakgoz and Beasley (2008). Similar to the data sets de-
scribed above, these objects hold weekly observations of the index and its constituents.
Stocks with missing values during the sample period have been discarded. The data
was downloaded from DATASTREAM and made anonymous. The ﬁrst column refers
to the index data itself. The data licence for these time series is included in the pack-
age as ﬁle BeasleyLicence and can also be found at http://people.brunel.
ac.uk/ mastjjb/jeb/orlib/legal.html.
The ESCBFX data set consists of daily spot exchange rates for major currencies
against the euro. The sample starts on 1 April 1999 and ends on 4 April 2012, giving
a total of 3427 observations. The currencies are AUD, CAD, CHF, GBP, HKD, JPY
and USD.
The source of the remaining data sets MultiAsset, StockIndex, StockIndex-
Adj,
and
StockIndexAdjD
is
Yahoo!
Finance
(see
http://finance.
yahoo.com)). The un/adjusted month’s-end/daily prices for the major stock and/or
bond markets as well as gold are provided. The sample of MultiAsset starts in
November 2004 and ends in November 2011. The sample period for data sets cov-
ering the major stock markets is larger, starting in July 1991 and ending in June
2011.
With respect to portfolio optimization, which is thoroughly covered in Chapter
5 and Part III of this book, the following approaches are available (in alphabetical
order):
r PAveDD(), portfolio optimization with average draw-down constraint;
r PCDaR(), portfolio optimization with conditional draw-down at risk constraint;
r PERC(), equal risk contributed portfolios;
r PGMV(), global minimum variance portfolio;
r PMD(), most diversiﬁed portfolio;
r PMTD(), minimum tail-dependent portfolio;
r PMaxDD(), portfolio optimization with maximum draw-down constraint;
r PMinCDaR(), portfolio optimization for minimum conditional draw-down at
risk.
These are constructor functions for objects of the S4 class PortSol deﬁned in
FRAPO. In order to foster the reader’s comprehension of S4 classes and methods as
introduced in Section 2.4, the handling of these objects is elucidated in the following
R code in-line statements. As an example the solution of a global minimum-variance
(GMV) portfolio is determined for the major stock indexes as contained in the
StockIndexAdj data set:

A BRIEF COURSE IN R
23
> data(StockIndexAdj)
> R <−returnseries(StockIndexAdj, method = "discrete",
+
trim = TRUE)
> P <−PGMV(R)
After the data set has been loaded into the work space, the discrete returns of the
price series are computed and assigned to the object R. The result of calling PGMV is
then stored in the object P. The structure of an unknown object can be investigated
with the function str(), but here we will query the class of P:
> class(P)
[1] "PortSol"
attr(,"package")
[1] "FRAPO"
The structure of this class can then be returned:
> showClass("PortSol")
Class "PortSol" [package "FRAPO"]
Slots:
Name:
weights
opt
type
call
Class:
numeric
list character
call
Known Subclasses: "PortCdd", "PortAdd", "PortMdd"
In the output it is indicated that this class is deﬁned in the package FRAPO and
contains the slots weights, opt, type, and call. In the second line of the slots
section, the classes of these entities are listed. Thus, the class of the portfolio weights
is numeric, the outcome of the optimization is a list object, the type of the
portfolio is described as character, and the call to the function by which the object
has been created is of class call. The last line of the output indicates which other
classes inherit from PortSol. The manual page of the PortSol class is displayed
by help(’PortSol-class’).
The methods deﬁned for PortSol objects are displayed by calling showMeth-
ods() whereby the class name is passed as argument classes.
> showMethods(classes = "PortSol", inherited = FALSE)
Function: show (package methods)
object="PortSol"
Function: Solution (package FRAPO)
object="PortSol"
Function: update (package stats)
object="PortSol"
Function: Weights (package FRAPO)
object="PortSol"

24
MOTIVATION
This says that a show() method is available, which is executed automatically when the
object is returned. The generic function for this method is available in the methods
package. The Solution() method for retrieving the outcome of the optimizer is
deﬁned in the package FRAPO itself, as is the method Weights() for extracting the
optimal weight vector.
> P
Optimal weights for porfolio of type:
Global Minimum Variance
SP500
N225 FTSE100
CAC40
GDAX
HSI
36.6719 13.9499 49.3782
0.0000
0.0000
0.0000
> Weights(P)
SP500
N225
FTSE100
CAC40
3.667185e+01 1.394990e+01 4.937825e+01 8.119150e-16
GDAX
HSI
8.758867e-15 0.000000e+00
An update() method is available, too. This comes in handy and saves typing
if one wishes to update an existing object by altering the values passed to the gen-
erating function’s arguments. For instance, portfolio back-testing whereby the fund
is rebalanced regularly can be carried out by utilizing the update() method and
incrementally increasing the underlying data sample.
In order to see the actual source code of a method deﬁnition, rather than entering
the name of the method one can use selectMethod():
> showMethods("Weights", inherited = FALSE)
Function: Weights (package FRAPO)
object="PortSol"
> selectMethod(f = "Weights", signature = "PortSol")
Method Definition:
function (object)
{
ans <−slot(object, "weights")
return(ans)
}
<environment: namespace:FRAPO>
Signatures:
object
target
"PortSol"
defined "PortSol"
As can be seen from the function’s body, alternative means for retrieving the slot
weights are:

A BRIEF COURSE IN R
25
> Weights(P)
SP500
N225
FTSE100
CAC40
3.667185e+01 1.394990e+01 4.937825e+01 8.119150e-16
GDAX
HSI
8.758867e-15 0.000000e+00
> slot(P, "weights")
SP500
N225
FTSE100
CAC40
3.667185e+01 1.394990e+01 4.937825e+01 8.119150e-16
GDAX
HSI
8.758867e-15 0.000000e+00
> P@weights
SP500
N225
FTSE100
CAC40
3.667185e+01 1.394990e+01 4.937825e+01 8.119150e-16
GDAX
HSI
8.758867e-15 0.000000e+00
With these last pointers and examples, the reader should hopefully have some
insight into working with the formal class and methods scheme in R.
References
Beasley J., Meade N. and Chang T. 2003 An evolutionary heuristic for the index tracking
problem. European Journal of Operational Research 148, 621–643.
Becker R., Chambers J. and Wilks A. 1988 The New S Language. Chapman & Hall, London.
Canakgoz N. and Beasley J. 2008 Mixed-integer programming approaches for index tracking
and enhanced indexation. European Journal of Operational Research 196, 384–399.
Cesarone F., Scozzari A. and Tardella F. 2011 Portfolio selection problems in practice: a
comparison between linear and quadratic optimization models. Quantitative Finance Papers
1105.3594, arXiv.org.
Chambers J. 1998 Programming with Data. Springer, New York.
Chambers J. 2008 Software for Data Analysis: Programming with R. Springer, New York.
Chambers J. and Hastie T. 1992 Statistical Models in S. Chapman & Hall, London.
Fox J. 2009 Aspects of the social organization and trajectory of the R project. R Journal 1(2),
5–13.
Gentleman R. and Ihaka R. 1997 The R language In Proceedings of the 28th Symposium on
the Interface (ed. Billard L. and Fisher N.) Interface Foundation of North America.
Graves S., Dorai-Raj S. and Francois R. 2011 sos: sos. R package version 1.3-1.
Ihaka R. and Gentleman R. 1996 R: A language for data analysis and graphics. Journal of
Computational and Graphical Statistics 5, 299–314.
R Development Core Team 2012 R: A Language and Environment for Statistical Computing
R Foundation for Statistical Computing Vienna, Austria. ISBN 3-900051-07-0.
Zeileis A. 2005 CRAN task views. R News 5(1), 39–40.

3
Financial market data
3.1
Stylized facts on ﬁnancial market returns
3.1.1
Stylized facts for univariate series
Before we turn to the topic of modelling ﬁnancial market risks, it is worthwhile
to consider and review typical characteristics of ﬁnancial market data. These are
summarized in the literature as ‘stylized facts’ (see Campbell et al. 1997; McNeil
et al. 2005). These observed properties have important implications for assessing
whether the risk model chosen is appropriate or not. Put differently, a risk model that
does not capture the time series characteristics of ﬁnancial market data adequately
will also not be useful for deriving risk measures. For observed ﬁnancial market data,
the following stylized facts can be stated:
r Time series data of returns, in particular daily return series, are in general not
independent and identically distributed (i.i.d.). This fact is not jeopardized by
low absolute values of the ﬁrst-order autocorrelation coefﬁcient.
r The volatility of return processes is not constant with respect to time.
r The absolute or squared returns are highly autocorrelated.
r The distribution of ﬁnancial market returns is leptokurtic. The occurrence of
extreme events is more likely compared to the normal distribution.
r Extreme returns are observed closely in time (volatility clustering).
r The empirical distribution of returns is skewed to the left; negative returns are
more likely to occur than positive ones.
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

FINANCIAL MARKET DATA
27
Listing 3.1 Stylized facts on the returns for Siemens.
library(fBasics)
1
library(evir)
2
data(siemens)
3
SieDates <−as.character(format(as.POSIXct(attr(siemens, "times")),
4
"%Y-%m-%d"))
5
SieRet <−timeSeries(siemens ∗100, charvec = SieDates)
6
colnames(SieRet) <−"SieRet"
7
## Stylised Facts I
8
par(mfrow = c(2, 2))
9
seriesPlot(SieRet, title = FALSE, main = "Daily Returns of Siemens",
10
col = "blue")
11
boxPlot(SieRet, title = FALSE, main = "Box plot of Returns",
12
col = "blue", cex = 0.5, pch = 19)
13
acf(SieRet, main = "ACF of Returns", lag.max = 20, ylab = " ",
14
xlab = " ", col = "blue", ci.col = "red")
15
pacf(SieRet, main = "PACF of Returns", lag.max = 20, ylab = " ",
16
xlab = " ", col = "blue", ci.col = "red")
17
## Stylised Facts II
18
SieRetAbs <−abs(SieRet)
19
SieRet100 <−tail(sort(abs(series(SieRet))), 100)[1]
20
idx <−which(series(SieRetAbs) > SieRet100, arr.ind = TRUE)
21
SieRetAbs100 <−timeSeries(rep(0, length(SieRet)),
22
charvec = time(SieRet))
23
SieRetAbs100[idx, 1] <−SieRetAbs[idx]
24
acf(SieRetAbs, main = "ACF of Absolute Returns", lag.max = 20,
25
ylab = " ", xlab = " ", col = "blue", ci.col = "red")
26
pacf(SieRetAbs, main = "PACF of Absolute Returns", lag.max = 20,
27
ylab = " ", xlab = " ", col = "blue", ci.col = "red")
28
qqnormPlot(SieRet, main = "QQ-Plot of Returns", title = FALSE,
29
col = "blue", cex = 0.5, pch = 19)
30
plot(SieRetAbs100, type = "h", main = "Volatility Clustering",
31
ylab = " ", xlab = " ", col = "blue")
32
As an example, we will now check whether these stylized facts are applicable
to the returns of the Siemens stock. The data set of daily returns is contained in the
package evir (see McNeil and Stephenson 2011). This series starts on 2 January 1973
and ends on 23 July 1996, and comprises 6146 observations.
In Listing 3.1 the necessary packages fBasics (see W¨urtz 2012) and evir are
loaded ﬁrst. Functions contained in the former package will be utilized to produce
some of the graphs. Next, the siemens data set is loaded and converted into an object
of class timeSeries with the function of the same name. The time series plot of

28
MOTIVATION
1973−01−02
1987−02−19
−10
0
5
Daily Returns of Siemens
−10
0
5
Box plot of Returns
0
5
10
15
20
0.0
0.4
0.8
ACF of Returns
5
10
15
20
−0.04
0.02
PACF of Returns
Figure 3.1
Stylized facts for Siemens, part I.
the percentage returns, a boxplot thereof as well as the autocorrelation and partial
autocorrelation are produced next and exhibited in Figure 3.1. As can been deduced
from the time series plot, volatility clustering does exist. This is more pronounced
in the second half of the sample period. Furthermore, by mere eyeball econometrics
the returns are skewed to the left and heavy-tails are evident, as can be seen from
the boxplot (upper right panel). The largest loss occurred on 16 October 1989, at
−12.01%. The highest return of 7.67% occurred on 17 January 1991. The skewness
is −0.52 and the excess kurtosis 7.74, clearly indicating heavy tails. The ACF and
PACF hint at a slight autocorrelation of ﬁrst order. Incidentally, the series shows
some systematic variation on the weekly frequency; though signiﬁcant, it is much
less pronounced than the daily autocorrelation.
Figure 3.2 further investigates whether the stylized facts about ﬁnancial market
returns hold in the case of Siemens. In the upper panels of this ﬁgure, the autocorre-
lations and partial autocorrelations of the absolute returns are plotted. Clearly, these
are signiﬁcantly different from zero and taper off only slowly. In the lower left panel
a quantile–quantile (QQ) plot compared to the normal distribution is produced. The
negative skew as well as the heavy tails are mirrored from their quantitative values in
this graph. Finally, in Listing 3.1 the 100 largest absolute returns have been retrieved
from the object SieRet. These values are shown in the lower right-hand panel. This
time series plot vindicates more clearly what could already be deduced from the
upper left-hand panel in Figure 3.1: ﬁrst, the existence of volatility clustering; and
second, that the returns become more volatile in the second half of the sample period.
Although these stylized facts have been exempliﬁed by the stock returns of
Siemens only, they not only hold for basically all stock returns, but also are applicable
to other asset classes, such as bonds, currencies and commodity futures.

FINANCIAL MARKET DATA
29
Figure 3.2
Stylized facts for Siemens, part II.
3.1.2
Stylized facts for multivariate series
The previous subsection presented the stylized facts for univariate ﬁnancial market
returns. From the portfolio point of view the characteristics of multivariate return
series are of interest. Here we will focus on these stylized facts:
r The absolute value of cross-correlations between return series is less pro-
nounced and the contemporaneous correlations are in general the strongest.
r In contrast, the absolute or squared returns do show high cross-correlations.
This empirical ﬁnding is similar to the univariate case.
r The contemporaneous correlations are not constant over time.
r Extreme observations in one return series are often accompanied by extremes
in the other return series.
These stylized facts are elucidated in Listing 3.2. Here, daily European stock
market data for France, Germany and the United Kingdom are employed. This data
set is ﬁrst loaded into R and converted from an object with class attribute mts into a
zoo object. The package zoo (see Zeileis and Grothendieck 2005) is an alternative to
the package timeSeries which was utilized in the previous listing. Next a time series
chart of the index data is produced.
The co-movement between these three European equity market is quite apparent
from Figure 3.3. The co-movement with respect to the volatility between the three
continuous return series (not shown here) is similar.

30
MOTIVATION
Listing 3.2 Stylized facts on the European equity market.
library(zoo)
1
data(EuStockMarkets)
2
## Time series plot of levels
3
EuStockLevel <−as.zoo(EuStockMarkets)[, c("DAX", "CAC", "FTSE")]
4
plot(EuStockLevel, xlab = " ", main = " ")
5
## Percentage returns
6
EuStockRet <−diff(log(EuStockLevel)) ∗100
7
plot(EuStockRet, xlab = " ", main = " ")
8
## Cross correlations
9
layout(matrix(1:6, nrow = 3, ncol = 2, byrow = TRUE))
10
ccf(EuStockRet[, 1], EuStockRet[, 2], ylab = " ", xlab = " ",
11
lag.max = 20, main = "Returns DAX vs CAC")
12
ccf(abs(EuStockRet)[, 1], abs(EuStockRet)[, 2], ylab = " ",
13
xlab = " ", lag.max = 20, main = "Absolute returns DAX vs CAC")
14
ccf(EuStockRet[, 1], EuStockRet[, 3], ylab = " ", xlab = " ",
15
lag.max = 20, main = "Returns DAX vs FTSE")
16
ccf(abs(EuStockRet)[, 1], abs(EuStockRet)[, 3], ylab = " ",
17
xlab = " ", lag.max = 20, main = "Absolute returns DAX vs FTSE")
18
ccf(EuStockRet[, 2], EuStockRet[, 3], ylab = " ", xlab = " ",
19
lag.max = 20, main = "Returns CAC vs FTSE")
20
ccf(abs(EuStockRet)[, 2], abs(EuStockRet)[, 3], ylab = " ",
21
xlab = " ", lag.max = 20, main = "Absolute returns CAC vs FTSE")
22
## Rolling correlations
23
rollc <−function(x){
24
dim <−ncol(x)
25
rcor <−cor(x)[lower.tri(diag(dim), diag = FALSE)]
26
return(rcor)
27
}
28
rcor <−rollapply(EuStockRet, width = 250, rollc,
29
align = "right", by.column = FALSE)
30
colnames(rcor) <−c("DAX & CAC", "DAX & FTSE", "CAC & FTSE")
31
plot(rcor, main = " ", xlab = " ")
32
This artefact is mirrored when one views the cross-correlations of the absolute
returns in Figure 3.4 (graphs shown to the right). As pointed out earlier, the returns
themselves are barely cross-correlated between markets and taper off fairly quickly
(graphs shown to the left). However, signiﬁcant cross-correlations are evident for
their absolute counterpart. This artefact is most pronounced for the German vis-`a-vis
the British stock market.
Finally, the correlations based upon a moving window of 250 observations are
exhibited in Figure 3.5. In order to calculate these rolling correlations, an auxiliary

FINANCIAL MARKET DATA
31
2000
4000
6000
DAX
1500 2500 3500
CAC
1992
1993
1994
1995
1996
1997
1998
3000
5000
FTSE
Figure 3.3
European stock market data.
−0.05
0.00
0.05
0.0
0.6
Returns DAX vs CAC
−0.05
0.00
0.05
0.0 0.4
Absolute returns DAX vs CAC
−0.05
0.00
0.05
0.0
0.5
Returns DAX vs FTSE
−0.05
0.00
0.05
0.0 0.3
Absolute returns DAX vs FTSE
−0.05
0.00
0.05
0.0
0.5
Returns CAC vs FTSE
−0.05
0.00
0.05
0.0 0.3
Absolute returns CAC vs FTSE
Figure 3.4
Cross correlations between European stock market returns.

32
MOTIVATION
0.4
0.6
0.8
DAX & CAC
0.4
0.6
0.8
DAX & FTSE
1993
1994
1995
1996
1997
1998
0.4
0.6
0.8
CAC & FTSE
Figure 3.5
Rolling correlations of European stock markets.
function rcor() is deﬁned ﬁrst. Within this function the correlation matrix is cal-
culated and, due to its symmetry, the upper triangular part thereof is extracted. The
computation of the rolling correlations is then performed with the function rol-
lapply() contained in zoo. The plots of these correlations are fairly similar for
all pairs portrayed, as are their ranges. For the DAX/CAC the correlation ranges
between 0.505 and 0.838, for the DAX/FTSE the values are in between 0.42 and
0.749 and lastly for the CAC/FTSE the correlation is in the interval from 0.451
to 0.76.
3.2
Implications for risk models
The stylized facts for univariate and multivariate ﬁnancial returns have been given in
the previous section. With respect to risk models and the risk measures derived from
them the following normative requirements can be deduced so far:
r Risk models which assume i.i.d. processes for the losses are not adequate
during all market episodes.
r Risk models that are based on the normal distribution will fall short in predicting
the frequency of extreme events (losses).
r Risk models should be able to encompass and address the different volatility
regimes. This means that the derived risk measures should be adaptive to
changing environments of low and high volatility.

FINANCIAL MARKET DATA
33
r In the portfolio context, the employed model should be ﬂexible enough to allow
for changing dependencies between the assets; in particular, the co-movement
of losses should be taken care of.
Part II of this book will focus on these issues. Statistical methods and techniques will
be presented which either in whole or in part address these stylized facts.
References
Campbell J., Lo A. and MacKinlay A. 1997 The Econometrics of Financial Markets. Princeton
University Press, Princeton, NJ.
McNeil A. and Stephenson A. 2011 evir: Extreme values in R. R package version 1.7-2.
McNeil A., Frey R. and Embrechts P. 2005 Quantitative Risk Management: Concepts, Tech-
niques and Tools. Princeton University Press, Princeton, NJ.
W¨urtz D. 2012 fBasics: Rmetrics – markets and basic statistics. R package version 2160.81.
Zeileis A. and Grothendieck G. 2005 zoo: S3 infrastructure for regular and irregular time
series. Journal of Statistical Software 14(6), 1–27.

4
Measuring risks
4.1
Introduction
Toward the end of the ﬁrst decade of this century, the focus of investors shifted, not
least because of the ﬁnancial crisis, to the need to adequately measure risks. But it
is also worth being able to capture potential losses correctly during market episodes
that are more tranquil. If an investor were too conservative with respect to risk during
such phases, he would jeopardize potential investment opportunities. Furthermore,
there are regulatory reasons and economic arguments which necessitate a proper
risk assessment. With respect to the former the stricter requirements imposed by the
Basel Accords are worthy of mention, and for the latter the normative presumption
of an efﬁcient resource allocation between risky assets. If the riskiness of a ﬁnancial
instrument is not captured correctly, its price would in general be misleading and
would therefore cause an inefﬁcient allocation of funds. Finally, a proper assessment
of market risks by its participants helps to ensure a smooth functioning of the ﬁnancial
system. For example, if market participants have to revise their risk estimates on a
larger scale, as was the case during the sub-prime mortgage crisis, and hence want
or have to rebalance their funds, a stampede-like reaction will quickly dry-up market
liquidity and aggravate the potential losses, due to the lack of counter-parties.
4.2
Synopsis of risk measures
The risk measures introduced in this section are based upon a probability model for
the potential losses an investor would face. The investor’s wealth Wt is viewed as
a random variable, and the realization thereof at the time point t is assumed to be
known. The value of wealth at this speciﬁc point in time is dependent on the vector
of risk factors zt, which exert an inﬂuence on the investor’s wealth position:
Wt = f (t, zt).
(4.1)
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

MEASURING RISKS
35
The prices of ﬁnancial instruments, exchange rates and/or interest rates can be
modelled as risk factors, for instance. It is assumed that these are known at the
time t.
In contrast, the future value of wealth after a time span  is unknown. This could
be the wealth position in 1 or 10 days’ time. The loss that results after a time  is
denoted by Lt,t+, and this is just the difference in wealth positions at t +  and t.
It is conventional that losses are expressed as positive numbers:
Lt,t+ := −(Wt+ −Wt).
(4.2)
Substituting equation (4.1) in equation (4.2), it is immediately evident that the
losses are determined by the changes in the risk factors:
Lt+ = −( f (t + , zt + xt+) −f (t, zt)),
(4.3)
where xt = zt −zt−.
Because wealth Wt is a random variable, the loss function as the difference
between two wealth positions in time, is also a random variable. As such it has
a probability distribution, which will henceforth be termed the loss distribution.
This distribution can be made dependent upon the information available at the time
point t. In this case, a conditional distribution would result. If, on the other hand,
the distribution is time-independent, the distribution is unconditional with respect
to time.
As a ﬁrst step, only the modelling of market risk for a single ﬁnancial instrument
is considered. Therefore, the price of the asset is the risk factor and the loss depends
on the time span and the price change within this period. We will further conﬁne the
analysis to the unconditional distribution of losses and the risk measures that can be
deduced from it.
In practice, the most commonly encountered risk measure is the value at risk
(VaR). This concept was introduced by JP Morgan in the ﬁrst version of their publi-
cation RiskMetrics and was then covered thoroughly in the academic literature (see
RiskMetrics Group 1994).1 Textbook expositions are included, for example, in Jorion
(2001) and McNeil et al. (2005). For a given conﬁdence level α ∈(0, 1), the VaR
deﬁned as the smallest number l such that the probability of a loss L is not higher
than 1 −α for losses greater than l. This value corresponds to a quantile of the loss
distribution and can be formally expressed as
VaRα = inf {l ∈R : P(L > l) ≤1 −α} = inf {l ∈R : FL(l) ≥α} ,
(4.4)
where FL is the distribution function of the losses. For the sake of completeness the
concept of the mean VaR, VaRmean
α
, is also introduced. This risk measure is deﬁned
as the difference between VaRα and the expected return μ. If the chosen time period
is 1 day, this measure is also referred to as the daily earnings at risk.
1 Incidentally, as stressed in Alexander and Baptista (2002), this sort of risk measure was advo-
cated as early as 1963 by Baumol and was referred to as the expected gain-conﬁdence limit criterion
(see Baumol 1963).

36
MOTIVATION
One criticism against the use of VaR as a measure of risk is that it is inconclusive
about the size of the loss if it is greater than that implied by the chosen conﬁdence
level. Put differently, if there are 95 sunny days and ﬁve rainy days, one is typically
not interested in the least amount of rain to expect on these ﬁve rainy days, but
rather would like to have a gauge for the average rainfall in these cases. The expected
shortfall (ES) risk measure, introduced by Artzner et al. (1997) and Artzner (1999),
directly addresses this issue. This measure provides hindsight about the size of the
expected loss if the VaR has been violated for a given level of conﬁdence. It is deﬁned
for a Type I error α as
ESα =
1
1 −α
 1
α
qu(FL)du,
(4.5)
where qu(FL) is the quantile function of the loss distribution FL. The ES can therefore
be expressed in terms of the VaR as
ESα =
1
1 −α
 1
α
VaRu(L)du,
(4.6)
and can be interpreted as the average VaR in the interval (1 −α, 1). For a better
comprehension of these concepts the VaR and ES risk measures as well as the
expected loss are shown in Figure 4.1.
−4
−2
0
2
4
0.00
0.05
0.10
0.15
0.20
0.25
Losses
Density
E(L)
VaR
ES
(α)
(1 −α)
Figure 4.1
Density of losses with VaR and ES.

MEASURING RISKS
37
Hitherto no explicit assumption has been made about the kind of loss distribution.
The VaR and ES can be computed based upon the empirical distribution for a given
sample. In this approach of a historic simulation one sorts the losses with respect to
their size. As an example, take a sample size of 1000 loss observations. Then the
VaR at the 99% conﬁdence level would correspond to the 10th largest loss and the
ES can be determined as the mean or median of the 10 largest losses. In contrast
to determining the risk measures from historic data alone, one could also assume
that the losses follow some distribution. If one assumes that the losses are normally
distributed, then these risk measures can be computed in closed form as
VaRα = σ−1(α) −μ,
(4.7)
ESα = σ φ(−1(α))
1 −α
−μ,
(4.8)
where φ denotes the density function of the standard normal distribution.
Although we have assumed so far that the losses are i.i.d., it is worth stressing that
by choosing an ill-founded assumption with respect to the chosen distribution from
which the risk measures are derived, a severe modelling error can result. Recall from
the stylized facts of the previous chapter that one cannot ordinarily assume normally
distributed returns. The empirical distribution of returns possesses more probability
mass in the tails compared to the Gaussian distribution. The implication is then that
risk measures that are derived from the normal assumption generally underestimate
the riskiness of a position in a single ﬁnancial instrument. The modelling error is
– cum grano salis – lessened by assuming, for instance, a Student’s t distribution
with fewer than 30 degrees of freedom, due to a greater probability mass in the
tails compared to the normal distribution. But a modelling error may still result. It
should further be stressed that these two distributions are symmetric. Hence, this
characteristic would imply a modelling error in cases of skewed losses, that is, the
losses are distributed asymmetrically. This modelling can in principle be rectiﬁed by
employing partial moment estimators for the dispersion of losses.
The stylized facts of skewed and fat-tailed return/loss distributions can also be
reﬂected directly in the calculation of the VaR. Zangari (1996) proposed the modiﬁed
VaR (mVaR), which explicitly takes the higher moments into account. Here the true
but unknown quantile function is approximated by a second-order Cornish–Fisher
expansion based upon the quantile function of the normal distribution (see Cornish
and Fisher 1937; Jaschke 2001). Hence, the mVaR is also referred to as Cornish–
Fisher VaR and is deﬁned as
mVaRα = VaRα + (q2
α −1)S
6
+ (q3
α −3qα)K
24
−(2q3
α −5qα)S2
36
,
(4.9)
where S denotes the skewness, K the excess kurtosis and qα the quantile of a
standard normal random variable with level α. In the case of a normally distributed
random variable mVaRα = VaRα, because the skewness and excess kurtosis are zero.
The Cornish–Fisher VaR produces a more conservative risk estimate if the return
distribution is skewed to the left (losses are skewed to the right) and/or the excess

38
MOTIVATION
0.90
0.92
0.94
0.96
0.98
1.00
2
3
4
5
Probability of Losses
VaR Levels
stVaR
nVaR
mVaR
Figure 4.2
Gaussian VaR, mVaR and VaR of a skewed Student’s t distribution.
kurtosis is greater than zero. It should be noted that the mVaR yields less conservative
risk estimates than the Gaussian VaR if these conditions are not met.
A comparison between the mVaR and the Gaussian VaR for conﬁdence levels
in the interval [90%, 100%) is shown in Figure 4.2. Here the losses have been
computed from a skewed Student’s t distribution with 10 degrees of freedom and a
skew parameter of ξ = 1.5. The skewness parameter ξ > 0 is deﬁned as the square
root of the ratio for probabilities above and below the mean. If ξ < 1 a left-skewed
distribution results, if ξ = 1 the distribution is symmetric, and if ξ > 1 a right-
skewed distribution occurs (see Lambert and Laurent 2001). It can be concluded
from this ﬁgure that for the chosen parameter constellation the actual VaR is pretty
well captured by the mVaR, and that if one were to use the Gaussian VaR instead the
risk would be markedly underestimated.
In Boudt et al. (2008) a modiﬁed ES (mES) was derived from the Cornish–Fisher
VaR, whereby a second-order Edgeworth expansion was utilized. Its computation is
conﬁned to the left tail of the distribution only. The mES thus derived can, however, be
smaller than the mVaR for α →0. This artefact is due to the nature of the Edgeworth
expansion. The authors suggest in these instances choosing the more conservative
risk estimate of the two as a pragmatic solution.
More direct approaches and promising alternatives in terms of modelling market
risks while adhering to the stylized facts of ﬁnancial market returns will be covered
in Part II.

MEASURING RISKS
39
4.3
Portfolio risk concepts
Needless to say, the risk measures introduced in the previous section can be computed
for portfolio return data too. But in a portfolio context an investor is often interested in
the risk contributions of single positions or a group thereof. Furthermore, s/he might
be interested how the portfolio risk is affected if the weight of a position is increased
by one unit, that is, the marginal contribution to risk. The concept of component VaR
or component ES addresses the former question. This notion of breaking down the
overall risk into single positions held in a portfolio can also applied to the mVaR and
mES (see Boudt et al. 2008).
We will now assume that a portfolio consists of N ﬁnancial instruments with
weight vector ω. The portfolio return equals rp = ω′μ and the portfolio variance is
computed as m2 = ω′ω, where μ denotes the return vector of the instruments and
 is the variance–covariance matrix of the portfolio’s assets. Under the assumption
of normality the risk measures VaR and ES can be computed as
VaRα = −ω′μ −√m2−1(α),
(4.10)
ESα = −ω′μ + √m2
1
α φ[−1(α)],
(4.11)
where φ(·) is the density and −1(·) the quantile function of the standard normal
distribution.
These two risk measures depend only on the ﬁrst and second central moments
and are linear homogeneous in ω. For risk measures of this kind (i.e., f (ω)), a
componentwise breakdown can be achieved by considering the partial derivatives
f (ω) =
N

i=1
ωiδi f (ω),
(4.12)
where δi f (ω) = δf (ω)/δωi is the partial derivative for the portfolio weight of the ith
asset, i = 1, . . . , N.
The risk contribution of the ith portfolio position is therefore given by Ci f (ω) =
ωiδi f (ω). This can also be expressed in relative terms (i.e., as a percentage), by using
%Ci f (ω) = Ci f (ω)/f (ω) × 100. The risk contributions for each of the portfolio’s
components can therefore be computed as
δiVaR(α) = −μi −δim2
2√m2
−1(α),
(4.13)
δiES(α) = −μi + δim2
2√m2
1
α φ[−1(α)],
(4.14)
where δim2 = 2(ω)i. It should be noted that, in the case of the normal distribution,
the partial derivatives with respect to the portfolio weights can be computed rather
easily and that this is not the case if one assumes alternative distributions and/or
more complex measures of risk. In these cases, the risk contributions can be obtained
heuristically by a Monte Carlo analysis, for instance.

40
MOTIVATION
As shown above, the third and fourth moments are required for computing the
modiﬁed risk measures mVaR and mES. In the case of multivariate random variables,
these moments are deﬁned by the (N × N 2) matrix
M3 = E[(r −μ)(r −μ)′ ⊗(r −μ)]
(4.15)
for the skewness, and by the N × N 3 matrix
M4 = E[(r −μ)(r −μ)′ ⊗(r −μ)′ ⊗(r −μ)′]
(4.16)
for the kurtosis. Here the operator ⊗denotes the Kronecker product between two
matrices. It follows that the third moment of a portfolio is given by m3 = ω′M3(ω ⊗
ω) and the fourth moment can be computed as m4 = ω′M4(ω ⊗ω ⊗ω), where
the theoretical moments are expressed by their respective estimates. The partial
derivatives for these moments are δim3 = 3(M3(ω ⊗ω))i and δim4 = 4(M4(ω ⊗
ω ⊗ω))i. Finally, the skewness of a portfolio can be determined as sp = m3/M3/2
2
and the excess kurtosis by evaluating the expression kp = m4/M2
2 −3. Boudt et al.
(2008) provide a detailed derivation of these results as well as the partial derivatives
for determining the risk contributions. These can be stated similarly as was done for
the non-modiﬁed risk measure (see equations (4.13) and (4.14) above).
Having introduced the most commonly encountered measures of market risk,
namely VaR and ES as well as the modiﬁcations thereof, we will next address the
issue of their appropriateness in the portfolio context. Artzner et al. (1996) and
Artzner et al. (1997, 1999) deﬁned four axioms and derived from these the deﬁnition
of a coherent measure of risk. The axioms are termed:
r monotonicity;
r translation invariance;
r positive homogeneity;
r sub-additivity.
These four axioms will now be brieﬂy explained. Let ρ denote a risk measure and
ρ(L) the risk value of a portfolio, where the loss L is a random variable.
The axiom of monotonicity requires, for two given losses L1 and L2 with L1 ≤L2,
that this relation is also reﬂected when the risk measures are calculated for each loss.
Therefore a risk measure must fulﬁl the condition ρ(L1) ≤ρ(L2) in order to be
considered as a monotone measure of risk.
The axiom of translation invariance ensures that the risk measure is deﬁned in the
same units as the losses and is formally written as ρ(L + l) = ρ(L) + l with l ∈R.
A risk measure satisﬁes the axiom of positive homogeneity if ρ(λL) = λρ(L)
with λ > 0. This requirement ensures the scaling of the risk measures with respect to
the size of a position. This axiom would be violated if the size of a portfolio position
directly inﬂuenced its riskiness. As an example, suppose that the price of a stock is
100 monetary units and the associated risk measure is 3% for a holding period of 1
day. Then the risk should be scalable with respect to the size of the position. If one
owned one stock the risk would be 3 monetary units, and if one owned 10 stocks

MEASURING RISKS
41
then the corresponding risk would be tenfold, namely 30 monetary units. This axiom
should not be mistaken for liquidity risk. When one needs to sell certain assets the
exercise price can of course be a function of the trade size.
Finally, a risk measure is said to be sub-additive if ρ(L1 + L2) ≤ρ(L1) + ρ(L2).
Intuitively, this axiom states that the portfolio risk shall be less than or equal to the
sum of the single risk measures of the assets contained in the portfolio. Put differently,
due to diversiﬁcation effects the holding of a portfolio is less risky.
A risk measure ρ is said to be coherent if it satisﬁes all four axioms. Because
the VaR is a quantile, the ﬁrst three axioms are fulﬁlled, but not the axiom of sub-
additivity. Hence, VaR is not a coherent measure of risk in the portfolio context.
This implies that the VaR of a portfolio can be greater than the sum of the individual
risks. Further, if the VaR is employed for portfolio optimization, ordinarily a more
concentrated portfolio results. For an exposition of this the reader is referred to Frey
and McNeil (2002) and McNeil et al. (2005, Chapter 6). It should be noted that
the standard deviation is also not a coherent measure of risk because the axiom
of monotonicity is violated. Incidentally, the semi-standard deviation is coherent,
because when this partial moment is calculated only the values above the average
value are used. Finally, the ES derived from a continuous density function qualiﬁes
as a coherent measure of risk (see Acerbi and Tasche 2002).
References
Acerbi C. and Tasche D. 2002 On the coherence of expected shortfall. Journal of Banking and
Finance 26(7), 1487–1503.
Alexander G. and Baptista A. 2002 Economic implications of using a mean-VaR model
for portfolio selection: A comparison with mean-variance analysis. Journal of Economic
Dynamics & Control 26, 1159–1193.
Artzner P. 1999 Coherent measures of risk. Mathematical Finance 9, 203–228.
Artzner P., Delbaen F., Eber J. and Heath D. 1996 A characterization of measures of risk.
Technical Report 1186, School of Operations Research and Industrial Engineering, College
of Engineering, Cornell University, Ithaca, NY.
Artzner P., Delbaen F., Eber J. and Heath D. 1997 Thinking coherently. Risk 10(11), 68–71.
Baumol W. 1963 An expected gain-conﬁdence limit criterion for portfolio selection. Manage-
ment Science 10, 174–182.
Boudt K., Peterson B. and Croux C. 2008 Estimation and decomposition of downside risk for
portfolios with non-normal returns. Journal of Risk 11(2), 79–103.
Cornish E. and Fisher R. 1937 Moments and cumulants in the speciﬁcation of distributions.
Revue de l’Institut International de Statistique 5(4), 307–320.
Frey R. and McNeil A. 2002 VaR and expected shortfall in portfolios of dependent credit risks:
Conceptual and practical insights. Journal of Banking and Finance 26, 1317–1334.
Jaschke S. 2001 The Cornish-Fisher-expansion in the context of delta-gamma-normal approx-
imations. Technical Report 54, Sonderforschungsbereich 373: Quantiﬁcation and Simula-
tion of Economic Processes, Humboldt-Universit¨at, Wirtschaftswissenschaftliche Fakult¨at,
Berlin.

42
MOTIVATION
Jorion P. 2001 Value at Risk: The New Benchmark for Measuring Financial Risk 2nd edn.
McGraw-Hill, New York.
Lambert P. and Laurent S. 2001 Modelling ﬁnancial time series using GARCH-type models
and a skewed Student density Universit´e de Li`ege.
McNeil A., Frey R. and Embrechts P. 2005 Quantitative Risk Management: Concepts, Tech-
niques and Tools. Princeton University Press, Princeton, NJ.
RiskMetrics Group 1994 Riskmetrics technical document. Technical report, J.P. Morgan, New
York.
Zangari P. 1996 A VaR methodology for portfolios that include options. RiskMetrics Monitor
Q1, 4–12. JP Morgan-Reuters.

5
Modern portfolio theory
5.1
Introduction
Sixty years have passed since Harry Markowitz’s path-breaking article ‘Portfolio
Selection’ was published (see Markowitz 1952). Because the more recently advocated
approaches to portfolio optimization are still based on this approach, Markowitz’s
work on modern portfolio theory will be reviewed in this chapter. In the next section
the approach itself will be discussed and subsequently the problems encountered in
practice are highlighted. Within this last section the path is laid for the topics covered
in Part III of the book, where portfolio optimization techniques designed to cope with
the problems of modern portfolio theory and/or are tailor-made for certain investors’
demands will be covered.
Given the turbulence in the ﬁnancial markets witnessed during the ﬁrst decade
of this century, the focus of academia and practitioners alike has again shifted to the
Markowitz approach for selecting assets in a portfolio, in particular minimum variance
portfolios. Thus, the concluding remark of Rubinstein’s article on the occasion of the
ﬁftieth anniversary of Markowitz’s seminal paper remains true:
Near the end of his reign in 14 AD, the Roman emperor Augustus could
boast that he had found Rome a city of brick and left it a city of marble.
Markowitz can boast that he found the ﬁeld of ﬁnance awash in the
imprecision of English and left it with the scientiﬁc precision and insight
made possible only by mathematics. (Rubinstein 2002)
5.2
Markowitz portfolios
In this section the portfolio selection approach proposed by Markowitz and the
notation used in the chapters of Part III will be introduced. Given the scope of this
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

44
MOTIVATION
book, a more detailed derivation of the results is omitted and the reader is referred to
such works as Ingersoll (1987), Huang and Litzenberger (1988), Markowitz (1991),
Elton et al. (2007) and Scherer (2010).
The path-breaking insight of Markowitz was that the risk/return proﬁles of single
assets should not be viewed separately but in their portfolio context. In this respect,
portfolios are considered to be efﬁcient if they are either risk minimal for a given
return level or have the maximum return for a given level of risk. Even though
both views of efﬁcient portfolios are equivalent, the kind of portfolio optimization
does differ for these two cases. The former is a quadratic optimization with linear
constraints, whereas in the latter the objective function is linear and the constraints
are quadratic.
In the following it is assumed that there are N assets and that these are inﬁnitely
divisible. The returns of these assets are jointly normally distributed. The portfolio
return ¯r is deﬁned by the scalar product of the (N × 1) weight and return vectors
ω and μ. The portfolio risk is measured by the portfolio variance σ 2
W = ω′ω,
where  denotes the positive semi-deﬁnite variance–covariance matrix of the assets’
returns. For the case of minimal variance portfolios for a given portfolio return, ¯r,
the optimization problem can be stated as:
P = arg min σ 2
W = ω′ω,
ω
ω′μ = ¯r,
(5.1)
ω′i = 1,
where i is the (N × 1) vector of ones.
In the same year as Markowitz published his seminal paper, the function for
determining efﬁcient portfolios was derived by Roy (1952), although the paper by
Merton (1972) is cited in the literature more frequently. According to this function,
the weight vector for a minimal variance portfolio and a given target return is given
by
ω∗= ¯rω∗
0 + ω∗
1,
(5.2)
with
ω∗
0 = 1
d (c−1μ −b−1i),
ω∗
1 = 1
d (b−1μ −a−1i).
The portfolio standard deviation is given by
σ =

1
d (c¯r2 −2b¯r + a),
(5.3)
with a = μ′−1μ, b = μ′−1i, c = i′−1i and d = ac −b2. Equation (5.2) results
from a Lagrange optimization with the constraints for a given target return and
weights summing to one. A detailed exposition is contained in Merton (1972). It can

MODERN PORTFOLIO THEORY
45
0
2
4
6
8
0.0
0.1
0.2
0.3
0.4
0.5
0.6
σ
μ
●
●
●
●
GMV
MSR
CML
Efficient frontier
Asymptotes
Utility
Figure 5.1
Global minimum variance and maximum Sharpe ratio portfolios.
be concluded from this equation that the portfolio weights are a linear function of
the expected returns. Furthermore, it can be shown that each efﬁcient portfolio can
be generated as a linear combination of two other efﬁcient portfolios. In particular,
the risk/return proﬁle of an efﬁcient portfolio can be expressed in terms of a linear
combination between the global minimal variance (GMV) portfolio and any other
efﬁcient portfolio. The covariance between these two portfolios equals the variance
of the minimum variance portfolio. Though it might not be evident at ﬁrst glance,
it should be stressed that the only constraint with respect to the portfolio weights
is that their sum equals one. Hence, neither the existence of negative weights (short
positions) nor weights greater than one (leveraged positions) can be ruled out, per se.
Equation (5.3) describes a hyperbola for efﬁcient mean–variance portfolios. The
hyperbola is enclosed by the asymptotes ¯r = b/c ± √d/cσ. The locus of the GMV
portfolio is the apex of the hyperbola with weights given by ω∗
GMV = −1i/i′−1i
(see Figure 5.1).
In contrast to the mean–variance portfolios, the weight vector of the global
minimum variance one does not depend on the expected returns of the assets. The
upper branch of the hyperbola is the geometric location of all efﬁcient mean–variance
portfolios. The marginal risk contributions of the assets contained in these kinds
of portfolios are all the same and the weights correspond to the percentage risk
contributions. Hence, these weights are Pareto-efﬁcient. Intuitively this makes sense,
because in the case of differing marginal contributions to risk, an overall reduction in
risk would be feasible and this would violate the minimum variance characteristic for
these kinds of portfolios. The risk/return points that are enclosed by the hyperbola are
referred to as the feasible portfolios, although these are sub-optimal. In other words,
portfolios exist that have either a higher return for a given level of risk or are less

46
MOTIVATION
risky for certain portfolio return. Both instances would yield a higher utility for the
investor.
So far, it has been assumed that the portfolio holdings of an investor are entirely
in risky assets. We will now depart from the Markowitz model in the strict sense
and allow the holding of a riskless asset with a return of r f . The question that now
arises is how high the optimal holding of this asset in a portfolio should be. This
depends on the risk aversion of the investor. A risk averse investor tries to maximize
his end-of-period wealth (his expected utility), whereby the decision as to the shape
of the portfolio has to be taken at the beginning of the period:1
max E[U(Wt+1)],
(5.4)
where E denotes the expectation operator. The utility function can be approximated
by a Taylor series expansion and it is further assumed that this function is twice
differentiable. After a neutral expansionU(Wt+1) = U(Wt+1 + E[Wt+1] −E[Wt+1]),
the utility function to be maximized can be written as
E[U(Wt+1)] = U(E[Wt+1]) + U ′(E[Wt+1])
1!
E[Wt+1 −E[Wt+1]]
+U ′′(E[Wt+1])
2!
E[Wt+1 −E[Wt+1]]2
+
∞

i=3
U (i)(E[Wt+1])
i!
E[Wt+1 −E[Wt+1]]i.
(5.5)
It is worth noting that so far no assumptions have been made about how wealth is
distributed. Therefore, equation (5.5) is deﬁned for a broad class of distribution func-
tions. Further, utility is a function of the higher moments of the wealth distribution.
If wealth is assumed to be normally distributed, then the above expression simpliﬁes
to
E[U(Wt+1)] = U(E[Wt+1]) + U ′(E[Wt+1])
1!
E[Wt+1 −E[Wt+1]]
+U ′′(E[Wt+1])
2!
E[Wt+1 −E[Wt+1]]2
(5.6)
Hence, the investor’s utility only depends on the ﬁrst two moments. It should be
stressed that even in the case of non-normality, the above optimization approach
can be used for quadratic utility functions of the form U(W) = W −λ
2 W 2. Here the
parameter λ is a measure of the risk aversion of an investor.
If one assumes quadratic utility, then the weight vector is given by ωU =
(1/λ)−1μ. The greater the risk aversion is, the smaller the sum of the weights.
The expected total return R from the risky assets and the riskless asset is given by
E[R] = (1 −γ )r f + γ ¯r
= r f + γ (¯r −r f ),
(5.7)
1 The following exposition draws on Huang and Litzenberger (1988).

MODERN PORTFOLIO THEORY
47
where γ = i′ω is the share of risky assets compared to total wealth. The standard
deviation for this portfolio is σ(R) = γ σW. From this the capital market line (CML)
can be derived – a linear relationship in the (μ, σ) plane – as
E[R] = r f + ¯r −r f
σW
σ(R).
(5.8)
The optimal portfolio is located at the tangency point of this line and the upper
branch of the efﬁcient frontier. This is given when the slope is greatest and hence the
Sharpe ratio is at its maximum. The portfolio that is characterized at this tangency
point is therefore also referred to as maximum Sharpe ratio (MSR) portfolio. In
the case of the MSR portfolio the investor holds only risky assets. The marginal
contributions of the selected assets to the Sharpe ratio are all the same. The investment
grade of an investor is determined by the tangency point of his utility function and
the CML. This point lies south-west of the MSR portfolio, and the higher the risk
aversion is, the closer it will be located to the ordinate.
5.3
Empirical mean–variance portfolios
The theoretical portfolio concepts outlined in the previous section are unfortunately
not directly applicable in practice. So far the population moments have been em-
ployed in the analysis, but these entities are unknown. In empirical applications these
unknown parameters must be replaced by estimates. The locus of the set for the
feasible portfolios is below the efﬁcient frontier. At ﬁrst glance, the sample mean and
the unbiased estimator of the variance–covariance matrix of the returns seem to be
appropriate candidates for replacing the population moments. In practice, however,
potential estimation errors exert a direct impact on the portfolio weights such that, for
instance, the desired properties of an efﬁcient and/or a minimum variance portfolio
are no longer valid, in general. Ultimately, estimation errors are mirrored by a higher
portfolio risk compared to the case of population moments. This is regardless of
whether the estimates have been generated from historic data or ex ante forecasts
for these parameters are employed. For portfolio optimizations that are based on
estimators for the expected returns and the variance–covariance matrix these should
have a greater effect compared to optimization approaches that only rest on estimates
for the return dispersion, ceteris paribus (see Chopra and Ziemba 1993; Merton
1980). Hence, mean–variance portfolio optimizations should suffer more severely
from estimation error than minimum variance ones. In empirical simulations and
studies it was found that the weights of the former kind of portfolio optimizations
are characterized by wide spans and erratic behaviour over time (see, for example,
DeMiguel et al. 2007; Frahm 2008; Jagannathan and Ma 2003; Kempf and Memmel
2006; Ledoit and Wolf 2003). From a normative point of view both characteristics are
undesired. The effect of ‘haphazardly’ behaving weights is ameliorated for minimum
variance portfolios, and hence this portfolio design is to be preferred with respect to
the potential impact of estimation errors. The sensitivity of the optimal solutions for
mean–variance portfolios with respect to the utilized expected returns is per se not a

48
MOTIVATION
ﬂaw of the approach proposed by Markowitz, but rather an artefact of the quadratic
optimization for deriving the portfolio weights.
The errors of the estimates for the expected returns and the variance–covariance
matrix could be quantiﬁed heuristically beforehand by means of Monte Carlo
simulations. This portfolio resampling was proposed by Michaud (1998) and a
detailed description with a critique is given in Scherer (2002) and Scherer (2010,
Chapter 4). In a ﬁrst step the estimates ˆμ0 and ˆ0 for the theoretical moments μ
and  for given sample size T are calculated and m points on the empirical efﬁcient
frontier are computed. Next, K random samples of dimension T × N are generated
and from these the sample moments are determined, giving in total K pairs ( ˆμi, ˆi),
i = 1, . . . , K. Each of these pairs is then used to compute m points on the respective
efﬁcient frontiers. The locus of these simulated efﬁcient frontiers is below the efﬁ-
cient frontier for ˆμ0 and ˆ0. To assess the impact of the estimation error with respect
to the mean, the above procedure is repeated, but now the random pairs ( ˆμi, ˆ0),
i = 1, . . . , K, are used. That is, the estimation error is conﬁned to the expected
returns. Likewise, the impact of the estimation error for the return dispersion can be
evaluated by generating random pairs ( ˆμ0, ˆi), i = 1, . . . , K. To conclude the expo-
sition of portfolio resampling, it should be noted that randomized efﬁcient portfolios
can be retrieved by averaging the weights over m and K. This approach should not
be viewed as a panacea for coping with estimation errors. The main critique against
this approach is the propagation of errors. The initial values ( ˆμ0, ˆ0) from which the
random samples are generated, are estimates themselves and are hence contaminated
by estimation errors. Therefore, the initial errors are replicated and mirrored by
applying the Monte Carlo analysis. Furthermore, for the case of constrained portfolio
optimizations the above procedure can yield unintuitive results (see Scherer 2010,
Chapter 4).
Further, recall from Chapter 3 that asset returns are in general not multivari-
ate normally distributed. This implies that in addition to estimation errors a model
error often exist. A non-stationary return process would be modelled according to
a distribution for stationary processes. Hence, there is a trade-off between using a
distribution assumption for stationary processes on the one hand, thereby committing
a model error, and using a longer sample span by which the stationarity assumption
is more likely to be violated but the estimation error diminishes.
The above-stated consequences of estimation errors could in principle be ame-
liorated beforehand by imposing restrictions on the weights. The following example
elucidates the effect of placing constraints on the portfolio weights. Consider two
independent investment opportunities A and B with an expected return of 3% and
a volatility of 10%. A return-maximizing agent would be indifferent to all linear
combinations between these two assets. However, an estimation error as high as one
basis point would result in a very different outcome. Suppose that the estimates for
the expected returns are ˆμA = 3.01 and ˆμB = 2.99, respectively. This would imply
an inﬁnitely high long position in asset A which is ﬁnanced by an equal-sized short
position in asset B. Matters are rather different if long-only and/or bound constraints
are included in the optimization. It was found that these kinds of restrictions yield a
favourable out-of-sample performance (see, for instance, Frost and Savarino 1988)

MODERN PORTFOLIO THEORY
49
or are associated with a reduced portfolio risk (see, for instance, Gupta and Eichhorn
1998; Jagannathan and Ma 2003). Both of these empirical ﬁndings can be traced
back to a smaller implied estimation error if restrictions are imposed on the weights.
It is worth mentioning that the locus of portfolios in the (μ, σ) plane are inferior
to efﬁcient portfolios to a greater degree as the restrictions become more binding.
However, in general an investor is eager to achieve a portfolio allocation that comes
as close as possible to the efﬁcient frontier. But the implementation of long-only con-
straints is undesirable for other reasons. For instance, the implementation of most of
the hedge-fund type strategies requires short positioning to be allowed. In summary,
the imposition of constraints is not a panacea for all kinds of portfolio strategies and
optimizations. Part III of this book will address these issues in more detail and also
offer examples of how these more recent advances in portfolio construction can be
explored with R.
References
Chopra V. and Ziemba W. 1993 The effect of errors in means, variances, and covariances on
optimal portfolio choice. Journal of Portfolio Management 19, 6–11.
DeMiguel V., Garlappi L. and Uppal R. 2007 Optimal versus naive diversiﬁcation: How
inefﬁcient is the 1/n portfolio strategy?. Review of Financial Studies 22(5), 1915–1953.
Elton E., Gruber M., Brown S. and Goetzmann W. 2007 Modern Portfolio Theory and Invest-
ment Analysis 7th edn. John Wiley & Sons, Inc., Hoboken, NJ.
Frahm G. 2008 Linear statistical inference for global and local minimum variance portfolios.
Statistical Papers 51(4), 789–812.
Frost P. and Savarino J. 1988 For better performance: Constrain portfolio weights. Journal of
Portfolio Management 15(1), 29–34.
Gupta F. and Eichhorn D. 1998 Mean-variance optimization for practioners of asset alloca-
tion In Handbook of Portfolio Management (ed. Fabozzi FJ.) Frank J. Fabozzi Associates
New Hope, PA pp. 57–74.
Huang C. and Litzenberger R. 1988 Foundations for Financial Economics. Elsevier Science,
Amsterdam.
Ingersoll J. 1987 Theory of Financial Decision Making. Rowman & Littleﬁeld, Totowa, NJ.
Jagannathan R. and Ma T. 2003 Risk reduction in large portfolios: Why imposing wrong
constraints helps. Journal of Finance 58, 1651–1683.
Kempf A. and Memmel C. 2006 Estimating the global minimum variance portfolio. Schmalen-
bach Business Review 58, 332–348.
Ledoit O. and Wolf M. 2003 Improved estimation of the covariance matrix of stock returns
with an application to portfolio selection. Journal of Empirical Finance 10, 603–621.
Markowitz H. 1952 Portfolio selection. Journal of Finance 7(1), 77–91.
Markowitz H. 1991 Portfolio Selection: Efﬁcient Diversiﬁcation of Investments 2nd edn. Basil
Blackwell, Cambridge, MA.
Merton R. 1972 An analytical derivation of the efﬁcient portfolio frontier. Journal of Financial
and Quantitative Analysis 7, 1851–1872.

50
MOTIVATION
Merton R. 1980 On estimating the expected return on the market: An exploratory investigation.
Journal of Financial Economics 8, 323–361.
Michaud R. 1998 Efﬁcient Asset Management: A Practical Guide to Stock Portfolio Optimiza-
tion and Asset Allocation. Oxford University Press, New York.
Roy A. 1952 Safety ﬁrst and the holding of assets. Econometrica 20, 431–449.
Rubinstein M. 2002 Markowitz’s ‘portfolio selection’: A ﬁfty-year retrospective. Journal of
Finance 57(3), 1041–1045.
Scherer B. 2002 Portfolio resampling: Review and critique. Financial Analysts Journal 58(6),
98–109.
Scherer B. 2010 Portfolio Construction and Risk Budgeting 4th edn. Risk Books, London.

Part II
RISK MODELLING

6
Suitable distributions
for returns
6.1
Preliminaries
It was shown in Chapter 4 that risk measures like VaR and ES are quantile values
located in the left tail of a distribution. Given the stylized facts of empirical return
series, it would therefore sufﬁce to capture the tail probabilities adequately. This
is the subject of extreme value theory, which will be covered in the next chapter.
However, the need often arises to model not just the tail behaviour of the losses, but
the entire return distribution. This need arises when, for example, returns have to be
sampled for Monte Carlo type applications. Therefore, the topic of this chapter is the
presentation of distribution classes that allow returns to be modelled in their entirety,
thereby acknowledging the stylized facts. A desired distribution should be capable
of mirroring not only heavy-tail behaviour but also asymmetries. In particular, the
classes of the generalized hyperbolic distribution (GHD) and its special cases, namely
the hyperbolic (HYP) and normal inverse Gaussian distributions (NIG), as well as
the generalized lambda distribution (GLD) will be introduced in Sections 6.2 and 6.3.
A synopsis of available R packages follows in Sections 6.4 and 6.5, and the chapter
ends with applications of the GHD, HYP, NIG and GLD to ﬁnancial market data.
6.2
The generalized hyperbolic distribution
The GHD was introduced into the literature by Barndorff-Nielsen (1977). The ap-
plication this distribution to the increments of ﬁnancial market price processes was
probably ﬁrst proposed by Eberlein and Keller (1995). Further contributions fol-
lowed in which this distribution class was applied to ﬁnancial market data. The work
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

54
RISK MODELLING
of Prause (1997), Barndorff-Nielsen (1997), Barndorff-Nielsen (1998), Eberlein and
Prause (1998) and Prause (1999) paved the way for this distribution class to become
more widely known in the ﬁnancial community. The generalized hyperbolic distribu-
tion owes it name to the fact that the logarithm of the density function is of hyperbolic
shape, whereas the logarithmic values of the normal distribution are parabolic.
The density of the GHD is given by
gh(x; λ, α, β, δ, μ) = a(λ, α, β, δ)(δ2 + (x −μ)2)(λ−1/2)/2
×Kλ−1/2(α

δ2 + (x −μ)2) exp(β(x −μ)),
(6.1)
where
a(λ, α, β, δ) =
(α2 −β2)λ/2
√
2παλ−1/2δλKλ(δ

α2 −β2)
,
(6.2)
and Kν denotes a modiﬁed third-order Bessel function with index value ν. The density
is deﬁned for x ∈R and encompasses ﬁve parameters λ, α, β, δ, μ. The allowable
parameter space is deﬁned as λ, μ ∈R, δ > 0 and 0 ≤|β| < α. The parameter λ can
be interpreted as a class-deﬁning parameter, whereas the μ and δ are location and
scale parameters.
Three reparameterizations of the GHD can also be found in the literature:
ζ = δ

α2 −β2,
ρ = β/α;
ξ = (1 + ζ)−1/2,
χ = ξ/ρ;
¯α = αδ,
¯β = βδ.
(6.3)
These reparameterizations have in common a lack of location and scale parameters.
Put differently, these parameterizations do not change for afﬁne transformations of the
random variable x. Figure 6.1 shows the densities for different parameter constella-
tions. As is evident from this graph, it is possible to capture not only semi-strong tails
(i.e., with a kurtosis greater than 3), but also skewed distributions. From an intuitive
point of view it should be reasonable to expect that the following continuous distri-
butions can be derived from the GHD: the hyperbolic, hyperboloid, normal inverse
Gaussian, normal reciprocal inverse Gaussian, normal, variance gamma, Student’s t,
Cauchy, generalized inverse Gaussian and skewed Laplace distributions.
The HYP is a special case of the GHD. If the parameter λ = 1, then the following
density results:
hyp(x; α, β, δ, μ) =

α2 −β2
2δαK1(δ

α2 −β2)
exp(−α

δ2 + (x −μ)2 + β(x −μ)),
(6.4)
where x, μ ∈R, 0 ≤δ and |β| < α.
As mentioned above, λ can be interpreted as a class-selecting parameter. Anal-
ogously to the reparameterization of the GHD, the parameters of the HYP can be

SUITABLE DISTRIBUTIONS FOR RETURNS
55
−4
−2
0
2
4
0.0
0.3
0.6
Density of the GHD with λ = 1, β = 0, μ = 0
Values of Random Variable
Density
α = 2, δ = 1
α = 4, δ = 2
α = 4, δ = 4
−4
−2
0
2
4
0.0
0.3
0.6
Density of the GHD with λ = 1, α = 2, δ = 1, μ = 0
Values of Random Variable
Density
β = −1
β = 0
β = 1
Figure 6.1
Density function of the GHD class.
expressed in these speciﬁcations. Here, the reparameterization in the form of (ξ, χ)
is of particular interest, since the deﬁned range is given by 0 ≤|χ| < ξ < 1. This
relation describes a triangle, the so-called shape triangle. Asymptotically, the param-
eters reﬂect the third and fourth moments of the distribution (skewness and kurtosis).
The HYP can itself be viewed as a general class of distributions which encompasses
the following distributions at the limit: for ξ →0 a normal distribution results; for
ξ →1 one obtains symmetric and asymmetric Laplace distributions; for χ →±ξ
the HYP converges to a generalized inverse Gaussian distribution; and for |χ| →1
an exponential distribution results. The shape triangle can therefore be used as a
graphical means of assessing whether a return process can be approximated by one
these distributions.
The NIG distribution can be derived from the GHD if the class-selecting parameter
is set to λ = −1/2. The density of the NIG is given by
nig(x; α, β, δ, μ) = αδ
π exp(δ

α2 −β2 + β(x −μ)) K1(α

δ2 + (x −μ)2)

δ2 + (x −μ)2
,
(6.5)
where the parameter space is deﬁned as x, μ ∈R, 0 ≤δ and 0 ≤|β| ≤α. For in-
stance, this distribution was employed by Barndorff-Nielsen (1998) for the modelling
of ﬁnancial market time series.
The unknown parameters can be estimated by the maximum likelihood (ML)
principle for a given sample. However, closed-form estimators cannot be derived and
hence the negative log-likelihood has to be minimized numerically.

56
RISK MODELLING
6.3
The generalized lambda distribution
The GLD is an extension to the family of lambda distributions proposed by Tukey
(see Tukey 1962). The latter family is deﬁned by the quantile function Q(u) with
u ∈[0, 1], that is, the inverse of the distribution function:
Q(u) =
⎧
⎪⎪⎨
⎪⎪⎩
uλ −(1 −u)λ
λ
,
λ ̸= 0,
log u
1 −u ,
λ = 0.
(6.6)
The parameter λ is referred to as a shape parameter and Q(u) is symmetric. It should
be noted that this quantile function does not have a simple closed form for any param-
eter values λ, except for λ = 0, and hence the values of the density and distribution
function have to be computed numerically. Incidentally, the density function can be
expressed parametrically for all values of λ in terms of the quantile function as in the
equation above and the reciprocal of the quantile density function, that is, the ﬁrst
derivative of equation (6.6). Tukey’s lambda distribution is termed a family of dis-
tribution, because many other statistical distributions can be approximated by it. For
instance, if λ = −1, then Q(u) behaves approximately as a Cauchy distribution, and
if λ = 1, a uniform [−1, 1] distribution results. Indeed, an approximate distribution
for a given data series can be discerned by plotting the probability plot correlation
coefﬁcient.
By splitting the parameter λ in equation (6.6) into distinct parameters, one obtains
the GLD. Here different parameterizations have been proposed in the literature. A
four-parameter extension of the quantile function is due to Ramberg and Schmeiser
(1974):
Q(u)RS = λ1 + uλ3 −(1 −u)λ4
λ2
(6.7)
Tukey’s lambda distribution is recovered from this speciﬁcation when λ1 = 0 and
λ2 = λ3 = λ4 = λ. The four parameters represent the location (λ1), the scale (λ2)
and the shape characteristics (λ3 and λ4) of a distribution. A symmetric distribution
is given for λ3 = λ4. The characteristics of this speciﬁcation have been extensively
investigated by Ramberg and Schmeiser (1974), Ramberg et al. (1979), King and
MacGillivray (1999) and Karian and Dudewicz (2000), among others. As it turns
out, not all parameter combinations yield a valid density/distribution function. The
probability density function of the GLD at the point x = Q(u) is given by
f (x) = f (Q(u)) =
λ2
λ3uλ3−1 + λ4(1 −u)λ4−1 .
(6.8)
Valid parameter combinations for λ must yield the following, such that equation (6.8)
qualiﬁes as a density function:
f (x) ≥0
(6.9)
and

f (x)dx = 1.
(6.10)

SUITABLE DISTRIBUTIONS FOR RETURNS
57
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
λ 3
λ4
Region 1:
λ3 < −1, λ4 > 1
Region 4:
λ3 ≤0, λ4 ≤0
Region 3:
λ3 ≥0, λ4 ≥0
Region 2:
λ3 > 1, λ4 < −1
5
6
Figure 6.2
GLD: valid parameter combinations of λ3 and λ4 in non-shaded areas.
Originally, only four regions of valid parameter constellations for λ3 and λ4 were
identiﬁed by Ramberg and Schmeiser (1974). In Karian et al. (1996) this scheme
was amended by additional regions which are labelled ‘5’ and ‘6’ in Figure 6.2. The
distributions pertinent to these regions share the same boundaries as for adjacent
regions. The parameter constellations for the four/six regions and the implied support
boundaries of the GLD are replicated in Table 6.1.
From Table 6.1 two observations can be made. First, the support of the GLD
distribution can change quite abruptly for slightly different parameter constellations.
Table 6.1
Range of valid GLD parameter combinations.
Region
λ1
λ2
λ3
λ4
Minimum
Maximum
1 and 5
all
< 0
< −1
> 1
−∞
λ1 + (1/λ2)
2 and 6
all
< 0
> 1
< −1
λ1 −(1/λ2)
∞
all
> 0
> 0
> 0
λ1 −(1/λ2)
λ1 + (1/λ2)
3
all
> 0
= 0
> 0
λ1
λ1 + (1/λ2)
all
> 0
> 0
= 0
λ1 −(1/λ2)
λ1
all
< 0
< 0
< 0
−∞
∞
4
all
< 0
= 0
< 0
λ1
∞
all
< 0
< 0
= 0
−∞
λ1

58
RISK MODELLING
Second, parameter constellations that fall in the third quadrant imply skewed and
heavy-tailed distributions. These characteristics are part of the stylized facts about
ﬁnancial return series, stated earlier. Recalling that the market risk measures VaR and
ES are quantile values, the GLD seems to be an ideal candidate for computing these
measures. This will be shown further below.
In order to avoid the problem that the GLD is conﬁned to certain param-
eter constellations for λ3 and λ4, Freimer et al. (1988) proposed a different
speciﬁcation:
Q(u)FMKL = λ1 +
uλ3−1
λ3
−(1−u)λ4
λ4
λ2
(6.11)
This speciﬁcation yields valid density functions over the entire (λ3, λ4) plane.
The distribution given by this speciﬁcation will have ﬁnite kth-order moment if
min(λ3, λ4) > −1/k.
Recently, Chalabi et al. (2010, 2011) proposed a respeciﬁcation of the GLD. This
proposed approach is a combination of using robust estimators for location, scale,
skewness and kurtosis, and expressing the tail exponents λ3 and λ4 by more intuitive
steepness and asymmetric parameters ξ and χ. The new parameterization takes the
following form:
ˆμ = u0.5,
(6.12)
ˆσ = u0.75 −u0.25,
(6.13)
χ =
λ3 −λ4

1 + (λ3 −λ4)2 ,
(6.14)
ξ = 1
2 −
λ3 + λ4
2

1 + (λ3 + λ4)2 .
(6.15)
Here, the bounds for χ and ξ are −1 < χ < 1 and 0 ≤ξ < 1, respectively. The
quantile function of the GLD can then be written as
Q(u| ˆμ, ˆσ, χ, ξ) = ˆμ + ˆσ
ˆS(u|χ, ξ) −ˆS(0.5|χ, ξ)
ˆS(0.75|χ, ξ) −ˆS(0.25|χ, ξ)
.
(6.16)
The function ˆS(u|χ, ξ) is deﬁned for the following cases:
ˆS(u|χ, ξ)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
log(u) −log(1 −u),
χ = 0, ξ = 0.5,
log(u) −(1 −u)2α −1
2α
,
χ = 2ξ −1,
u2α −1
2α
−log(1 −u),
χ = 1 −2ξ,
uα+β −1
α + β
−(1 −u)α−β −1
α −β
,
otherwise.
(6.17)

SUITABLE DISTRIBUTIONS FOR RETURNS
59
−1.0
−0.5
0.0
0.5
1.0
0.0
0.2
0.4
0.6
0.8
1.0
χ
ξ
Infinite support
Finite support
Lower infinite support
Upper infinite support
Figure 6.3
GLD shape plot.
where
α = 1
2
0.5 −ξ
√ξ(1 −ξ),
(6.18)
β = 1
2
χ

1 −χ2 .
(6.19)
In this parameterization the GLD has inﬁnite support if the condition (|χ| + 1)/2 ≤ξ
is met. Akin to the shape triangle of the HYP, one can now construct a triangle which
starts at χ = 0 and has as corners (ξ = 1, χ = −1) and (ξ = 1, χ = 1). All parameter
combinations of χ and ξ would thus give a distribution with inﬁnite support. The
GLD shape plot is shown in Figure 6.3.
As already hinted, the computation of VaR and ES can easily achieved when a
return series has been ﬁtted to the GLD. Here, the formulae are expressed in terms
of returns and not for the losses, which are expressed as positive numbers. The VaR
for a given probability of error is given by
VaRα = Q(u|λ) = λ1 + αλ3 −(1 −α)λ4
λ2
,
(6.20)
and the formula for computing the ES for a given probability of error can be expressed
as
ESα =
 VaR
−∞
x f (x|λ)dx =
 α
−∞
Q(u|λ)du
= λ1 +
1
λ2(λ3 + 1)αλ3+1 +
1
λ2(λ4 + 1)

(1 −α)λ4+1 −1
	
(6.21)

60
RISK MODELLING
Various estimation methods for ﬁnding optimal values for the parameter vector λ
have been proposed in the literature. Among these are the
r moment-matching approach,
r percentile-based approach,
r histogram-based approach,
r goodness-of-ﬁt approach,
r maximum likelihood and maximum product spacing.
The method of moment matching was suggested in the seminal papers of Ramberg
and Schmeiser (1974) and Ramberg et al. (1979). The ﬁrst four moments are matched
to the distribution parameters λ1, . . . , λ4. For λ1 = 0, the kth moment of the GLD is
deﬁned as
E(Xk) = λ−k
2
k

i=0
k
i

(−1)iβ(α, γ )
(6.22)
where β(α, γ ) denotes the beta function evaluated at α = λ3(k −i) + 1 and γ =
λ4i + 1. A non-linear system of four equations in four unknowns results and has to
be solved. Incidentally, this can be accomplished sequentially, by ﬁrst determining
estimates for λ3 and λ4 and then solving for the two remaining parameters (see
Ramberg and Schmeiser 1974, Section 3). This approach is only valid in the parameter
regions for which these moments exist (region 4), and the condition min(λ3, λ4) <
−1/4 must be met. As an aside, because the estimates for the skewness and the
kurtosis of a data set are very sensitive to outliers, the resulting parameter vector λ is
affected likewise. In order to achieve robust estimation results with respect to outlier
sensitivity, Chalabi et al. (2010) suggested replacing the moment estimators by their
robust counterparts. The robust moments for location, scale, skewness and kurtosis
are deﬁned as (see, for instance, Kim and White 2004):
μr = π1/2,
(6.23)
σr = π3/4 −π1/4,
(6.24)
sr = π3/4 + π1/4 −2π1/2
π3/4 −π1/4
,
(6.25)
kr = π7/8 −π5/8 + π3/8 −π1/8
π6/8 −π2/8
.
(6.26)
These statistics can be estimated by inserting the empirical quantiles pq. It is shown
in Chalabi et al. (2010) that the higher robust moments skewness and kurtosis only
depend on λ3 and λ4. Hence, a non-linear system of two equations in two unknowns
results, which has to be solved. The robust estimation of the GLD parameters has

SUITABLE DISTRIBUTIONS FOR RETURNS
61
the further advantage of deriving a standardized distribution characterized by a zero
median, a unit inter-quartile range and the two shape parameters λ1 and λ2:
Q(u|λ3, λ4) = Q(u|λ∗
1, λ∗
2, λ3, λ4),
λ∗
2 = Sλ3,λ4(3/4) −Sλ3,λ4(1/4),
λ∗
1 = −Sλ3,λ4(1/2)/λ∗
2,
(6.27)
where Sλ3,λ4(u) is equal to the numerator in equation (6.7).
Karian and Dudewicz (1999) propose an estimation approach based on the em-
pirical percentiles of the data. From the order statistics ˆπp of the data the following
four percentiles are deﬁned, where u ∈(0, 0.25):
ˆp1 = ˆπ0.5,
(6.28)
ˆp2 = ˆπ1−u −ˆπu,
(6.29)
ˆp3 =
ˆπ0.5 −ˆπu
ˆπ1−u −ˆπ0.5
,
(6.30)
ˆp4 = ˆπ0.75 −ˆπ0.25
ˆp2
.
(6.31)
For u = 0.1 these percentiles refer to the sample median ( ˆp1), the inter-decile range
( ˆp2), the left–right tail weight ratio ( ˆp3) and a measure of relative tail weights of the
left tail to the right tail ( ˆp4). These quantiles correspond to the following quantiles of
the GLD:
p1 = Q(0.5) = λ1 + 0.5λ3 −0.5λ4
λ2
,
(6.32)
p2 = Q(1 −u) −Q(u) = (1 −u)λ3 −uλ4 + (1 −u)λ4 −uλ3
λ2
,
(6.33)
p3 =
Q(0.5) −Q(u)
Q(1 −u) −Q(0.5) = (1 −u)λ4 −uλ3 + 0.5λ3 −0.5λ4
(1 −u)λ3 −uλ4 + 0.5λ4 −0.5λ3 ,
(6.34)
p4 = Q(0.75) −Q(0.25)
p2
= 0.75λ3 −0.25λ4 + 0.75λ4 −0.25λ3
(1 −u)λ3 −uλ4 + (1 −u)λ4 −uλ3 .
(6.35)
This non-linear system of four equations in four unknowns has to be solved. Similar
to the moment-matching method, a sequential approach by ﬁrst solving only the
subsystem consisting of ˆp3 = p3 and ˆp4 = p4 for λ3 and λ4 can be applied.
Deriving estimates for λ from histograms was proposed by Su (2005). Within
this approach the empirical probabilities are binned in a histogram and the resulting
mid-point probabilities are ﬁtted to the true GLD density. A drawback of this method
is that the resultant estimates are dependent on the chosen number of bins.
The fourth kind of estimation method is based on goodness-of-ﬁt statistics, such as
the Kolmogorov–Smirnov, Cram´er–von Mises or Anderson–Darling statistic. These
statistics measure the discrepancy between the hypothetical GLD and the empirical
distribution, which is derived from the order statistics of the data in question. Parame-
ter estimates can be employed when these statistics are minimized with respect to the
parameters of the GLD. The determination of the parameter values can be achieved

62
RISK MODELLING
with the starship method as proposed by Owen (1988) and adapted to the ﬁtting of
the GLD by King and MacGillivray (1999). It consists of the following four steps:
1. Compute the pseudo-uniform variables of the data set.
2. Specify a valid range of values for λ1, . . . , λ4 and generate a four-dimensional
grid of values that obey these bounds.
3. Calculate the goodness-of-ﬁt statistics compared to the uniform (0, 1) distri-
bution.
4. Choose the grid-point (λ1, λ2, λ3, λ4) that minimizes the goodness-of-ﬁt statis-
tic as the estimate for λ.
Finally, the GLD parameters could also be estimated by the ML principle and/or
the method of maximum product spacing. The latter method was proposed separately
by Cheng and Amin (1983) and Ranneby (1984). This method is also based on the
order statistics {x(1), x(2), . . . , x(N)} of the sample {x1, x2, . . . , xN} of size N. Next, the
spacings between adjacent points are deﬁned as D(x(i)|λ) = F(x(i)|λ) −F(x(i−1)|λ)
for i = 2, . . . , N. The objective is the maximization of the sum of the logarithmic
spacings. Compared to ML method, the maximum product spacing method has the
advantage of not breaking down when the support of the distribution is not warranted
for a given parameter combination.
6.4
Synopsis of R packages for the GHD
6.4.1
The package fBasics
The package fBasics is part of the Rmetrics suite of packages (see W¨urtz 2012).
The primary purpose of this package is to provide basic tools for the statistical
analysis of ﬁnancial market data. Within the package S4 classes and methods are
utilized. The package is considered a core package in the CRAN ‘Finance’ Task
View and also listed in the ‘Distributions’ Task View. The package has dependencies
on other packages contained in the Rmetrics suite. With respect to the modelling,
ﬁtting and inferences drawn from the GHD, quite a few functions have been directly
ported and/or included from the package GeneralizedHyperbolic to this package.
The latter package will be presented in the next subsection.
With respect to the topic of this chapter, the following distributions are addressed
in this package: the generalized hyperbolic, the generalized hyperbolic Student’s t,
the hyperbolic, the normal inverse Gaussian as well as the standardized version of
the GHD and NIG distributions. For each of these distributions, functions for calcu-
lating the value of the density, the probabilities, the quantiles and the generation of
random numbers are available. The R naming convention for the density (preﬁx d),
distribution (preﬁx p), quantile function (preﬁx q) and the generation of random
numbers (preﬁx r) is followed. In addition, routines for ﬁtting, and for calculation of
the mode and moments are included in the package. In particular, the mean, variance,
skewness and kurtosis are implemented for all distributions. It is further possible

SUITABLE DISTRIBUTIONS FOR RETURNS
63
to return their robust counterparts, namely, the median, the inter-quartile range and
skewness or kurtosis measures that are derived from these quartiles. The shape of the
density for the GHD, HYP and NIG can be plotted interactively for various parameter
combinations through a tcl/TK interface. These functions are termed ghSlider(),
hypSlider() and nigSlider(), respectively, and are intended primarily for illus-
trative purposes.
The functions that relate to the GHD have gh in their names, while those for the
generalized hyperbolic Student’s t have ght, those for the HYP have hyp and those
for the NIG have nig. Thus, the routines for ﬁtting these distributions to ﬁnancial
market return data are termed fooFit() where foo is replaced by one of these
abbreviations. Similarly, the mode is returned by the function fooMode() and the
routines for the moments are fooMean(), fooVar(), fooSkew() and fooKurt()
for the mean, the variance, the skewness and the kurtosis, respectively. Analogously,
the functions fooMED(), fooIQR(), fooSKEW() and fooKURT() relate to the robust
counterparts, namely the median, the inter-quartile range and the robust deﬁnitions
of the skewness and the kurtosis.
By default the unknown parameters of the distributions are estimated by applying
the ML principle. Here, the negative log-likelihood is minimized with the function
nlminb(). It should be noted that the ellipsis argument in the function fooFit() is
passed down to the plotting of the ﬁtted distribution, hence the user cannot pass argu-
ments directly to the optimization routine. With respect to ﬁtting the NIG, in addition
to the ML principle, the parameters can be estimated by the generalized methods of
moments, maximum product spacing or minimum variance product spacing. It is also
possible to produce a shape triangle for ﬁtted objects with the routine nigShape-
Triangle(). All fit() methods return an object of formal class fDISTFIT, for
which a show() method is deﬁned.
6.4.2
The package GeneralizedHyperbolic
This package offers functions not only for the GHD, but also for the derived distri-
butions HYP, GIG and skew Laplace (see Scott 2011). The package is written purely
in R. A NAMESPACE ﬁle is included in the package’s source that contains the export
directives for the functions and S3 methods pertinent to the above-mentioned distri-
butions. Some of the routines contained in this package have been ported to fBasics.
Although the name implies that this package deals primarily with the GHD, it is fair
to say that the focus is rather on the hyperbolic distribution. Only this distribution can
be ﬁtted to data. Three data sets from the ﬁelds of geology, engineering and ﬁnance
are provided.
First, the routines that cope with the hyperbolic will be discussed in more detail,
before the routines for the GHD, GIG and skew Laplace are presented. The chosen
acronym for the hyperbolic distribution in this package is hyperb. The dpqr naming
convention referred to in Section 6.4.1 is followed. Furthermore, the ﬁrst derivative of
the density function has been implemented as function ddhyperb(). To determine
the probabilities and quantiles of the HYP the function hyperbBreaks() is utilized.
With this function suitable intervals on the real line are determined. The moments

64
RISK MODELLING
and mode of the HYP distribution can be computed with the functions hyperb-
Mean(), hyperbVar(), hyperbSkew(), hyperbKurt() and hyperbMode() for
the mean, variance, skewness, kurtosis and mode, respectively. Two utility functions
are worth mentioning, namely hyperbCalcRange() for determining the ranges
of the HYP distribution with negligible probability mass and the function hyper-
bChangePars() for calculating the parameters of the alternative speciﬁcations. The
ﬁtting of the HYP distribution is achieved with the function hyperbFit(). Suitable
starting parameters can be determined with the function hyperbFitStart(). The
user has the option to numerically optimize the log-likelihood using either the Nelder–
Meade or the BFGS algorithm. Arguments to control the behaviour of the optimizer
can be speciﬁed in the call to hyperbFit(). This function returns an object with class
attributes hyperbFit, distFit. For objects of this kind print(), summary() and
plot() methods have been deﬁned. In addition, there are coef() and vcov() meth-
ods for retrieving the estimated parameters and their variance–covariance matrix. The
ﬁt can be assessed in the form of a quantile–quantile (QQ) plot (qqhyperb()) or
probability–probability (PP) plot (pphyperb()). Furthermore, a Cram´er-von–Mises
goodness-of-ﬁt test is implemented as function hyperbCvMTest(). This function
returns an object with class attribute hyperbCvMTest for which a print() method
has been deﬁned.
The functions that relate to the generalized hyperbolic have the acronym ghyp
in their names. Routines for the density, the distribution, the quantile function
as well as the generation of random numbers are implemented, and the dpqr
naming convention is followed. It is further possible to return the ranges on the
real line where the probability mass is small for a given parameter set (ghyp-
CalcRange()). Whether certain parameters are in the allowable set of values can
be checked with the function ghypCheckPars(). Furthermore, with the routine
ghypChangePars() the user can calculate the parameters for the alternative speciﬁ-
cations of the GHD. The moments and mode of the GHD distribution can be computed
with the functions ghyperbMean(), ghyperbVar(), ghyperbSkew(), ghyper-
bKurt() and ghyperbMode() for the mean, variance, skewness, kurtosis and mode,
respectively. The moments of the GHD can also be computed with the function
ghypMom().
Basically, the same set of functions that have been deﬁned for the GHD are made
available for the generalized inverse Gaussian (acronym gig). In addition, QQ and
PP plot routines are included in the package for this distribution.
In addition the functions that directly relate to the skew Laplace distribution
(acronym skewlap) – the density, the distribution, the quantile function and the
generation of random numbers – methods for producing QQand PPplots are available.
6.4.3
The package ghyp
In contrast to the previous discussed package, ghyp provides functions for ﬁtting
not only the univariate HYP, but also the GHD, NIG, VG, Student’s t and Gaussian
distributions for the univariate and multivariate case (see Luethi and Breymann 2011).
The package utilizes S4 classes and methods and is shipped with a NAMESPACE ﬁle.

SUITABLE DISTRIBUTIONS FOR RETURNS
65
It is contained in the CRAN ‘Distributions’ and ‘Finance’ Task Views. In addition to
the package’s help ﬁles, a vignette is available.
The functions that relate to the GHD or the GIG are the density, quantile, prob-
ability and random variates routines. The dpqr naming convention is followed and
the chosen acronyms are ghyp and gig for the GHD and the GIG, respectively.
A feature of this package is the inclusion of routines for calculating the expected
shortfall for these distributions. The functions are named ESghyp() and ESgig().
Furthermore, the package offers a function for portfolio optimization. The user can
choose between the risk measure employed, namely the standard deviation, the VaR
or the ES, and whether the portfolio should be a minimum risk, a tangency or a target
return portfolio. These portfolios are derived from a multivariate GHD.
To estimate the unknown coefﬁcients of the GHD and its special cases, the ML
principle is employed. The function names are made up of the preﬁx fit., followed
by the acronym of the desired distribution, followed by either uv or mv for ﬁtting
univariate or multivariate data. The objects returned are of formal class mle.ghyp.
For objects of this kind show() and summary() methods are deﬁned as well as
methods for extracting the Akaike information criterion (AIC) and the value of the
log-likelihood. Furthermore, a routine for model selection using the AIC is imple-
mented as function stepAIC.ghyp(). In addition, a function for discriminating
between models in the form of a likelihood ratio test is implemented (see routine
lik.ratio.test()). The estimated parameters can be extracted with the coef()
method. The moments of objects that inherit from the class ghyp can be computed
with the function mean() for the mean, with the function vcov() for the variance
in the univariate and the variance–covariance matrix in the multivariate case, and
with the functions ghyp.skewness() and ghyp.kurtosis() for the skewness
and kurtosis, respectively. In general, central and non-central moments can be com-
puted with the function ghyp.moment(). By default, the skewness and the kurtosis
are returned.
For plotting purposes, a QQ plot, a histogram view, a pairs plot for the graphical
display of multivariate QQ plots as well as plotting the density or superimposing the
density on existing plot devices are available. The functions are termed qqghyp(),
hist(), pairs(), plot() and lines(), in that order.
The package comes with two data sets. The ﬁrst, indices, contains monthly
returns for ﬁve asset classes between August 1999 and October 2008. The second
data set contains daily return data for the Swiss equity market and equity returns of
selected Swiss companies from 5 January 2000 to 10 January 2007.
6.4.4
The package QRM
Most of the examples contained in McNeil et al. (2005) can can be replicated with
the functions contained in the package QRM (see Pfaff 2012). These were originally
written in the S-PLUS language by A. McNeil and distributed as package QRMlib.
An initial R port was accomplished by S. Ulman and is still available from CRAN
archive (see McNeil and Ulman 2011). The package QRM is based on this initial
R port. It has dependencies on the CRAN packages gsl, mvtnorm, numDeriv and

66
RISK MODELLING
timeSeries. Within QRM partial use of S3 classes and methods is made. The more
burdensome computations are interfaced from C routines. In addition, 14 ﬁnancial
data sets are available.
With respect to the GHD, functions for ﬁtting data to its special cases, namely the
NIG and HYP, are included in this package. These are termed fit.NH() for univariate
and fit.mNH() for multivariate data. The case argument of these functions controls
whether the negative log-likelihood of an NIG or HYP is minimized. Both routines
return a list object without a class attribute. Hence, no further methods are available.
In addition, the moment and log moment of the GIG can be computed with the
functions EGIG() and ElogGIG(), respectively. Random variates of this distribution
can be generated with the function rGIG() that interfaces a routine written in C.
6.4.5
The package SkewHyperbolic
The package SkewHyperbolic is dedicated solely to the modelling and ﬁtting of
the skew hyperbolic Student’s t distribution (see Scott and Grimson 2010). The
package is written purely in R and S3 classes and methods are used. It is shipped
with a NAMESPACE ﬁle, and some underlying utility functions are imported from the
packages GeneralizedHyperbolic and DistributionUtils. Apart from the functions
that primarily deal with the skew hyperbolic distribution, three data sets are included
in the package.
With respect to the distribution itself, routines for its density, distribution and
quantile functions as well as for the generation of random variates are included. The
dpqr naming convention is followed, and the chosen acronym for this distribution
is skewhyp. In addition to these functions a routine for returning the ﬁrst deriva-
tive of the density function (ddskewhyp()) and for determining suitable breaks
(skewhypBreaks()) is available. Similar to the package GeneralizedHyperbolic,
a function for determining ranges for which the probability mass is small is available
(skewhypCalcRange()). The coherence of a parameter set can be checked with the
function skewhypCheckPars().
The included routines skewhypMean(), skewhypVar(), skewhypSkew() and
skewhypKurt() are used for calculating the mean, variance, skewness and kurtosis,
respectively. The mode of the distribution for a given parameter set can be computed
with skewhypMode(). Similarly to the package ghyp, the central and non-central
moments of any order can be calculated with the routine skewhypMom().
The ﬁtting of data to the skew hyperbolic Student’s t distribution is accomplished
by the function skewhypFit(). Suitable starting values can be determined with the
routine skewhypFitStart. The parameters are determined numerically by applying
the ML principle. The negative log-likelihood is minimized by employing either the
general purpose optimizer optim() or the function nlm(). For the former the user
can use either the BFGS or Nelder–Mead algorithm. The function skewhypFit()
returns an object of informal class skewhypFit. For objects of this kind print(),
plot() and summary() methods are available. Goodness of ﬁt can be inspected
graphically by means of a QQ and/or PP plot. The relevant functions are termed
qqskewhyp() and ppskewhyp(), respectively.

SUITABLE DISTRIBUTIONS FOR RETURNS
67
6.4.6
The package VarianceGamma
The package VarianceGamma can be considered as a twin package to the
SkewHyperbolic package discussed in the previous subsection, but its focus is on the
variance gamma distribution (see Scott and Dong 2010). The package is contained
in the CRAN ‘Distributions’ Task View. Within the package S3 classes and methods
are employed and the package is shipped with a NAMESPACE ﬁle in which import di-
rectives for the utility functions contained in the packages GeneralizedHyperbolic
and DistributionUtils are included. Basically, all functionalities contained in the
package SkewHyperbolic have been mirrored in this package, and the dpqr naming
convention is followed. The acronym vg is used for the variance gamma distribution.
Hence, the discussion of the functions, methods and classes in Section 6.4.5 carries
over in these instances, too.
6.5
Synopsis of R packages for GLD
6.5.1
The package Davies
Even though the focus of the package Davies is an implementation of the Davies
quantile function (see Hankin and Lee 2006), R routines that address the GLD
distribution are included, too. The package is listed in the CRAN ‘Distributions’
Task View. The package is neither shipped with a NAMESPACE ﬁle nor are S3 or S4
classes/methods employed. Hence, in addition two data sets, the package offers a
collection of functions for dealing with these two kinds of distributions – no more
and no less.
With respect to the GLD, the functions are based on the Ramberg–Schmeiser
(RS) speciﬁcation. The density, distribution and quantile functions of the GLD have
been implemented as well as a function for generating random variates, and the
dpqr naming convention is followed for naming these routines, (e.g., dgld()).
Furthermore, the routine dgld.p() is an implementation of the density function
expressed in terms of the quantile.
The expected value of the GLD for a given parameterization can be retrieved either
as an exact value with the routine expected.gld() or an approximation thereof
with expected.gld.approx(). Within both functions the values are determined
as the sum of a constant (λ1) and two Davies quantile functions.
6.5.2
The package fBasics
A general description of the package fBasics has already been provided in Section
6.4.1. Hence, in the following description the focus will be on the R routines for
handling the GLD.
The density, distribution and quantile functions and a routine for obtaining random
variates have been implemented as R routines, and the dpqr naming convention is
followed (e.g., dgld()). These functions are wrapper functions to the bodies of
the routines with the same name contained in the package gld by King (2011)

68
RISK MODELLING
(see Section 6.5.3 for a discussion of this package). However, these wrapper functions
are limited to the RS speciﬁcation, and parameter values for λ1, . . . , λ4 pertinent to
region 4 of the parameter space can be supplied as arguments to these functions,
otherwise an error is returned.
The ﬁtting of data to the GLD is provided by the function gldFit(). Similar
to the above functions, the GLD is expressed in the RS speciﬁcation and the op-
timization is carried out for the parameter space pertinent to region 4. Apart from
the data argument x and the initial parameter values lambda1[234], the function
has an argument method by which the estimation method can be set. The available
estimation procedures are: maximization of the log-likelihood (’mle’), the method
of maximum product spacing (’mps’), robust moment matching (’rob’), goodness
of ﬁt (’gof’) and histogram binning (’hist’). If one of the latter two methods is
chosen, the user can set the type of goodness-of-ﬁt statistic or the binning method via
the function’s argument type. For estimation methods based on goodness of ﬁt this
can be the Anderson–Darling- (’ad’), Cram´er–von Mises (’cvm’) or Kolmogorov–
Smirnov (’ks’) statistic. If the histogram approach is chosen, the count of bins can
be determined by the Freedman–Diaconis binning (’fd’), Scott’s histogram binning
(’scott’) or Sturges binning approach (’sturges’). The function returns an S4
object fDISTFIT. The estimates, the value of the objective and the convergence code
of the nlminb() optimizer are returned as a list in the slot fit of objects of this
kind. By default, a plot of the estimated density is produced, which can be suppressed
by setting doplot = FALSE.
The mode of the GLD can be computed for given parameter values of λ1, . . . , λ4
with the function gldMode(). Robust estimates for location, dispersion, skew-
ness and kurtosis can be computed for given parameter values with the functions
gldMED(), gldIQR(), gldSKEW() and gldKURT(), respectively.
6.5.3
The package gld
The package gld is, to the author’s knowledge, the only one that implements all
three GLD speciﬁcations: the RS, FMKL and FM5 (see King 2011). The latter is
an extension of the FMKL version in which a ﬁfth parameter is included in order
explicitly to capture the skewness of the data. The FM5 speciﬁcation is derived from
the modiﬁcation of the RS speciﬁcation by Gilchrist (2000).
The package is included in CRAN ‘Distributions’ Task View. S3 classes and meth-
ods have been utilized, but a NAMESPACE ﬁle is missing. The distribution functions
of the GLD speciﬁcations are interfaced from routines written in in the C language.
The density, quantile density, distribution and quantile distribution functions are
implemented as R routines dgl(), dqgl(), pgl(), qdgl(), respectively. Random
variates of the GLD can be generated with the function rgl(). With respect to
parameter estimation, the starship method has been implemented as function star-
ship(). Here, the initial values are determined according to an adaptive grid search
(starship.adaptivegrid()) and then used in the call to optim(). The objective
function itself is included in the package as function starship.obj(). The func-
tion starship() returns an object of informal class starship for which plot(),

SUITABLE DISTRIBUTIONS FOR RETURNS
69
print() and summary() methods are made available. The validity of the estimated
λ parameters can be checked with the function gl.check.lambda(). As a means
of assessing the goodness of ﬁt graphically, the function qqgl() produces a QQ plot.
Finally, the density of the GLD can be depicted with the function plotgl().
6.5.4
The package lmomco
Estimation methods based on L-moments for various distributions are implemented
in the package lmomco (see Asquith 2012). Here we will concentrate on those tools
that directly address the GLD. The package is considered to be a core package in
the CRAN ‘Distributions’ Task View. It is written purely in R and is shipped with a
NAMESPACE ﬁle with export directives for all relevant user functions. The package is
quite huge, judged by the size of its manual, which runs to more than 300 pages. It is
worth mentioning that, in addition to estimation methods based on L-moments and
their extensions, probability-weighted moment (PWM) estimators are available.
In order to estimate the parameters of the GLD, the L-moments for univariate sam-
ple data must be determined ﬁrst. This can be achieved with the function lmom.ub()
for unbiased L-moment estimates, with the function TLmoms() for trimmed L-
moments, or with the function pwm.ub() for unbiased sample PWMs. If the latter
route is chosen, these PWM estimates can be converted to L-moment estimates with
the function pwm2lmom. Having estimated the L-moments, the resulting object can
be used in the call to the functions pargld() and/or parTLgld() to estimate the
parameters of the GLD by L-moments or the trimmed L-moments, respectively. Ac-
cording to Asquith (2012), the latter two functions should be regarded as experimental
and should not be employed for production purposes. However, the package offers
routines for checking the validity of the estimated parameters and/or L-moments
(functions are.par.valid(), are.pargld.valid() and are.lmom.valid())
as well as means of converting between parameter estimates and the associated L-
moments for a given distribution (functions vec2par(), lmom2par(), par2lmom()
and lmomgld()).
The R functions that directly relate to the GLD are cdfgld() for the cumu-
lative distribution function, quagld() for the quantile function and pdfgld() for
the density function. Random variates for a given parameterization of the GLD can
be generated with the function rlmomco(). The correctness of an empirically deter-
mined probability or density function can be assessed with the functions check.fs()
and check.pdf(), respectively.
6.6
Applications of the GHD to risk modelling
6.6.1
Fitting stock returns to the GHD
In this subsection the daily returns of the Hewlett-Packard (HWP) stock are ﬁtted
to the GHD and its special cases, the HYP and NIG. The R code is shown in
Listing 6.1. The sample runs from 31 December 1990 to 2 January 2001 and consists
of 2529 observations. The following analysis has been conducted with the functions

70
RISK MODELLING
Listing 6.1 Fitting HWP returns to the GHD.
library(ghyp)
1
library(timeSeries)
2
library(fEcoﬁn)
3
## Return calculation
4
data(DowJones30)
5
y <−timeSeries(DowJones30[, ”HWP”], charvec =
6
as.character(DowJones30[, 1]))
7
yret <−na.omit(diff(log(y)) * 100)
8
## Fitting
9
ef <−density(yret)
10
ghdﬁt <−ﬁt.ghypuv(yret, symmetric = FALSE,
11
control = list(maxit = 1000))
12
hypﬁt <−ﬁt.hypuv(yret, symmetric = FALSE,
13
control = list(maxit = 1000))
14
nigﬁt <−ﬁt.NIGuv(yret, symmetric = FALSE,
15
control = list(maxit = 1000))
16
## Densities
17
ghddens <−dghyp(ef$x, ghdﬁt)
18
hypdens <−dghyp(ef$x, hypﬁt)
19
nigdens <−dghyp(ef$x, nigﬁt)
20
nordens <−dnorm(ef$x, mean = mean(yret), sd = sd(c(yret[, 1])))
21
col.def <−c("black", "red", "blue", "green", "orange")
22
plot(ef, xlab = " ", ylab = expression(f(x)), ylim = c(0, 0.25))
23
lines(ef$x, ghddens, col = "red")
24
lines(ef$x, hypdens, col = "blue")
25
lines(ef$x, nigdens, col = "green")
26
lines(ef$x, nordens, col = "orange")
27
legend("topleft",
28
legend = c("empirical", "GHD", "HYP", "NIG", "NORM"),
29
col = col.def, lty = 1)
30
## QQ plots
31
qqghyp(ghdﬁt, line = TRUE, ghyp.col = "red", plot.legend = FALSE,
32
gaussian = FALSE, main = " ", cex = 0.8)
33
qqghyp(hypﬁt, add = TRUE, ghyp.pch = 2, ghyp.col = "blue",
34
gaussian = FALSE, line = FALSE, cex = 0.8)
35
qqghyp(nigﬁt, add = TRUE, ghyp.pch = 3, ghyp.col = "green",
36
gaussian = FALSE, line = FALSE, cex = 0.8)
37
legend("topleft", legend = c("GHD", "HYP", "NIG"),
38
col = col.def[−c(1,5)], pch = 1:3)
39
## Diagnostics
40
AIC <−stepAIC.ghyp(yret, dist = c("ghyp", "hyp", "NIG"),
41
symmetric = FALSE,
42
control = list(maxit = 1000))
43
LRghdnig <−lik.ratio.test(ghdﬁt, nigﬁt)
44
LRghdhyp <−lik.ratio.test(ghdﬁt, hypﬁt)
45

SUITABLE DISTRIBUTIONS FOR RETURNS
71
−20
−10
0
10
0.00
0.05
0.10
0.15
0.20
0.25
f(x)
empirical
GHD
HYP
NIG
NORM
Figure 6.4
Fitted densities for HWP returns.
contained in the package ghyp. In the listing this package is loaded into the work space
ﬁrst. The package fEcoﬁn contains the data set DowJones30 which includes the HWP
stock price. This series is converted into a timeSeries object and the continuous
percentage returns are computed next. For comparison of the ﬁtted distributions,
the empirical distribution (EDF) is ﬁrst retrieved from the data with the function
ef(). Then the returns are ﬁtted to GHD, HYP and NIG distributions. In each case,
possible asymmetries in the data are allowed (i.e., non-zero skewness). In the next
chunk of code the shapes of the estimated densities are computed, along with a
Gaussian distribution which serves as the benchmark. A plot of the empirical and
ﬁtted densities is produced next (see Figure 6.4).
The rather poor description of the empirical return distribution for the Gaussian
case is immediately evident from this plot. The normal distribution falls short of
capturing the excess kurtosis of 4.811. Matters are different for the class of generalized
hyperbolic distributions. In these instances the empirical distribution function is
tracked rather well. The ﬁtted HYP and NIG models almost coincide, and from this
plot these two distributions cannot be discerned. The ﬁtted GHD seems to mirror
the returns slightly better. In particular, the values of the density are closer to their
empirical counterparts around the median of the EDF. Ceteris paribus, this implies
higher probability masses in the tails of the distribution compared to the λ-restricted
HYP and NIG distributions.
As a second means of graphically comparing the ﬁtted distributions, QQ
plots are produced in the ensuing code lines of Listing 6.1. These are shown in
Figure 6.5. For a better clarity the marks of the ﬁtted normal distribution have been
omitted from the plot. The reader is encouraged to adopt the plot accordingly. What
has been already concluded from the density becomes even more evident when the
QQ plot is examined. The daily returns can be tracked better with the GHD than with
the HYP and NIG distributions, especially in the tails. Furthermore – this conclusion

72
RISK MODELLING
−15
−10
−5
0
5
10
15
−20
−15
−10
−5
0
5
10
15
Theoretical Quantiles
Sample Quantiles
GHD
HYP
NIG
Figure 6.5
QQ plot of ﬁtted GHD for HWP returns.
was less clear from the density plot – the returns can be slightly better explained by
the NIG than by the HYP distribution.
In the last three lines of the listing diagnostic measures for the three models are
produced. First, the function stepAIC.ghyp() is utilized for determining with which
distribution the data can be explained best in terms of the AIC. This function returns
a list object with three elements: best.model, all.models and fit.table.
The latter is of most interest because it not only provides information about the
AICs and the values of the log-likelihood (LLH), but also returns the estimates of
the distribution parameters, whether a symmetric distribution has been ﬁtted or not,
as well as whether the optimizer achieved convergence and the number of iterations
required. An excerpt from these results is provided in Table 6.2.
The conclusions drawn from the graphical inspection of the results are mirrored
by their quantitative counterparts. Clearly, a GHD-based model is favoured over the
NIG and HYP distributions according to the AIC. However, the differences between
the AIC and/or the log-likelihood of the GHD and NIG are rather small. A cross-
comparison to the values of the HYP model would yield a preference for the NIG, if
one had to choose between the restricted distributions. The reason for this is primarily
that the unrestricted estimate of ˆλ is −2.27 closer to the parameter restriction for λ
of the NIG than that of the HYP. Whether the differences of the values for the
Table 6.2
Results of distributions ﬁtted to HWP returns.
Distribution
AIC
LLH
λ
¯α
μ
σ
γ
GHD
11684.07
−5837.04
−2.270
0.051
0.093
2.584
−0.013
NIG
11691.90
−5841.95
−0.500
1.110
0.078
2.552
0.003
HYP
11704.41
−5848.21
1.000
0.903
0.069
2.528
0.011

SUITABLE DISTRIBUTIONS FOR RETURNS
73
log-likelihoods are signiﬁcantly different from zero can be tested by means of a
likelihood ratio test. These tests are carried out in the last two lines of the R code
listing. First, it is checked whether the GHD can be replaced by the NIG. The value
of the test statistic is 0.007 and the p-value is 0.002. Hence, the null hypothesis
that the explanatory power of the two distributions is equal must be rejected at a
conﬁdence level of 95%. The corresponding value of the test statistic for comparing
the GHD with the HYP is 0 and the implied p-value is 0. Here, the null hypothesis
must be rejected even more clearly, which is plausible given the ordering of the three
log-likelihood values.
6.6.2
Risk assessment with the GHD
In this subsection the behaviour of the VaR and ES risk measures according to each
of the models is investigated. In particular, the two risks measures are derived from
the ﬁtted GHD, HYP and NIG distributions for the HWP returns from the previous
subsection. These measures are calculated over a span from the 95.0% to the 99.9%
levels. The resulting trajectories of the VaR and ES are then compared to their
empirical counterparts. For the ES the mean of the lower quintile values is used.
The relevant code is provided in Listing 6.2. First, the sequence of probabilities
is created for which the VaR and ES are to be computed. Because we are dealing
with returns instead of losses, these are deﬁned for the left tail of the distribution. In
the next lines the VaR for these levels is computed by utilizing the quantile function
for the GHD. By convention, losses are expressed as positive numbers, and hence the
absolute values of the quantiles returned by the function are used. The VaR based on
the normal distribution can be computed by providing the necessary estimates for the
location and scale. The VaR values thus determined are compared to their empirical
counterparts which are determined by the quantile() function.
The development of these risk measures is plotted in Figure 6.6. The quantiles
derived from the GHD and its special cases track the associated empirical loss levels
VaR
0.1%
0.7%
1.3%
1.9%
2.5%
3%
3.5%
4%
4.5%
5%
4
6
8
10
12
14
Empirical
GHD
HYP
NIG
Normal
Figure 6.6
VaR trajectory based upon GHD, HYP and NIG models.

74
RISK MODELLING
Listing 6.2 VaR and ES derived from the GHD, HYP and NIG.
## Probabilities
1
p <−seq(0.001, 0.05, 0.001)
2
## VaR
3
ghd.VaR <−abs(qghyp(p, ghdﬁt))
4
hyp.VaR <−abs(qghyp(p, hypﬁt))
5
nig.VaR <−abs(qghyp(p, nigﬁt))
6
nor.VaR <−abs(qnorm(p, mean = mean(yret), sd = sd(c(yret[, 1]))))
7
emp.VaR <−abs(quantile(x = yret, probs = p))
8
# Plot of VaR
9
plot(emp.VaR, type = "l", xlab = " ", ylab = "VaR", axes = FALSE,
10
ylim = range(c(hyp.VaR, nig.VaR, ghd.VaR, nor.VaR, emp.VaR)))
11
box()
12
axis(1, at = seq(along = p), labels = names(emp.VaR), tick = FALSE)
13
axis(2, at = pretty(range(emp.VaR, ghd.VaR, hyp.VaR,
14
nig.VaR, nor.VaR)))
15
lines(seq(along = p), ghd.VaR, col = "red")
16
lines(seq(along = p), hyp.VaR, col = "blue")
17
lines(seq(along = p), nig.VaR, col = "green")
18
lines(seq(along = p), nor.VaR, col = "orange")
19
legend("topright",
20
legend = c("Empirical", "GHD", "HYP", "NIG", "Normal"),
21
col = col.def, lty = 1)
22
## ES
23
ghd.ES <−abs(ESghyp(p, ghdﬁt))
24
hyp.ES <−abs(ESghyp(p, hypﬁt))
25
nig.ES <−abs(ESghyp(p, nigﬁt))
26
nor.ES <−abs(mean(yret) −sd(c(yret[, 1])) ∗
27
dnorm(qnorm(1 −p)) / p)
28
obs.p <−ceiling(p ∗length(yret))
29
emp.ES <−sapply(obs.p, function(x) abs(mean(sort(c(yret))[1:x])))
30
## Plot of ES
31
plot(emp.ES, type = "l", xlab = " ", ylab = "ES", axes = FALSE,
32
ylim = range(c(hyp.ES, nig.ES, ghd.ES, nor.ES, emp.ES)))
33
box()
34
axis(1, at = 1:length(p), labels = names(emp.VaR), tick = FALSE)
35
axis(2, at = pretty(range(emp.ES, ghd.ES, hyp.ES, nig.ES, nor.ES)))
36
lines(1:length(p), ghd.ES, col = "red")
37
lines(1:length(p), hyp.ES, col = "blue")
38
lines(1:length(p), nig.ES, col = "green")
39
lines(1:length(p), nor.ES, col = "orange")
40
legend("topright",
41
legend = c("Empirical", "GHD", "HYP", "NIG", "Normal"),
42
col = col.def, lty = 1)
43

SUITABLE DISTRIBUTIONS FOR RETURNS
75
ES
0.1%
0.7%
1.3%
1.9%
2.5%
3%
3.5%
4%
4.5%
5%
6
8
10
12
14
16
Empirical
GHD
HYP
NIG
Normal
Figure 6.7
ES trajectory based upon GHD, HYP and NIG models.
fairly closely. Only in the extreme conﬁdence region of 99.0% or greater is the risk
slightly underestimated by these models. The ordering of the goodness of ﬁt for
the three distributions can also be concluded from this graph: the ﬁtted GHD tracks
the data in that region very well, whereas the risk is underestimated by the NIG
and HYP models. Two conclusions can be drawn from the VaR based on the normal
distribution. First, as expected, the normal distribution falls short of capturing extreme
risk events (i.e., above the 97.5% level). Second, the riskiness of holding a position
in the HWP stock is overestimated for the conﬁdence region between 95.0% and
97.5%. Put differently, for these levels the VaR derived from the normal distribution
is consistently too conservative and hence an investor could be disadvantaged by not
being allowed to take a larger position in that stock.
Next in Listing 6.2, the ES is calculated for the ﬁtted model objects. As mentioned
in Section 6.4.3, this risk measure can be computed with the function ESghyp().
The expected shortfall for the normal distribution is determined by formula (4.8) in
Section 4.2. Similarly to the VaR, the trajectory of the ES for alternative conﬁdence
levels is compared to its empirical counterpart. Here, the mean of the values smaller
than the quantile is employed. The risk measures thus determined are shown in
Figure 6.7. In contrast to the VaR, now the risk measures derived from all models
consistently underestimate the expected loss in case of such an event. However, this
underestimation is less severe for the GHD-based models and for the less conser-
vative levels. The ES derived from the normal distribution fares worst. Overall, the
reason why the risk is consistently underestimated, regardless of the model chosen,
is primarily that all distributions underestimate the probability of extreme losses and
hence these errors accumulate when the ES is calculated.
6.6.3
Stylized facts revisited
In this subsection the stylized facts of ﬁnancial returns are reconsidered by em-
ploying the shape triangle of the HYP distribution. Ordinarily, the empirical

76
RISK MODELLING
distributionof ﬁnancial market returns is characterizedbyexcess kurtosis andnegative
skewness. These characteristics are most pronounced for higher-frequency returns
(i.e., intra-daily or daily). Therefore the need arises to utilize models/distributions
which acknowledge these stylized facts. One might also be interested in whether
these more complicated (compared to the normal distribution) models are indeed
necessary if the focus is shifted to lower-frequency return processes (i.e., weekly,
monthly or bimonthly). As discussed in Section 6.2, the exponential, Laplace, left-
or right-skewed hyperbolic and normal distributions are contained within the HYP
distribution as limit cases. Whether the HYP can be approximated by one of these
distributions can be graphically inspected by the shape triangle. This tool can also be
employed to assess whether the stylized facts are still applicable for lower-frequency
return processes. This kind of analysis is conducted in the R code in Listing 6.3.
The ﬁrst line of the listing deﬁnes the return days used. These correspond to
daily, weekly, biweekly, monthly and bimonthly returns. The next line computes the
returns for these frequencies. This can be achieved most easily with the lapply()
function. The resulting list object is then converted to a matrix and NA values are
omitted. The package ghd does not provide a function for parameterizing the HYP in
the (ξ, χ) space. Hence, in the subsequent lines the function xichi() is speciﬁed for
this purpose. Then the unknown parameters of the HYP distribution are estimated by
means of the function fit.hypuv() for each of the columns in yrets by employing
the function apply(). The returned list object hypfits which contains the ﬁtted
distributions is then submitted to lapply() in order to extract the (ˆξ, ˆχ) pairs. These
points are plotted in the ﬁnal lines of the listing and the resulting shape triangle is
provided in Figure 6.8.
A clear pattern can be detected from this shape triangle: the lower the frequency of
the return, the more it approaches the south of the triangle, hence the HYP distribution
could in principle be approximated by a normal distribution for these lower-frequency
−1.0
−0.5
0.0
0.5
1.0
−0.2
0.2
0.4
0.6
0.8
1.0
1.2
χ
ξ
1 day return
5 day return
10 day return
20 day return
40 day return
Laplace
Exponential
Exponential
Normal
Hyperbolic, left skewed
Hyperbolic, right skewed
Figure 6.8
Shape triangle for HWP returns.

SUITABLE DISTRIBUTIONS FOR RETURNS
77
Listing 6.3 Shape triangle for HYP distribution.
rd <−c(1, 5, 10, 20, 40)
1
yrets <−na.omit(matrix(unlist(lapply(rd,
2
function(x) diff(log(y), lag = x))), ncol = 5))
3
## Function for xi/chi coefﬁcients
4
xichi <−function(x){
5
param <−coef(x, type = "alpha.delta")
6
rho <−param[["beta"]] / param[["alpha"]]
7
zeta <−param[["delta"]] ∗sqrt(param[["alpha"]]∧2 −
8
param[["beta"]]∧2)
9
xi <−1 / sqrt(1 + zeta)
10
chi <−xi ∗rho
11
result <−c(chi, xi)
12
names(result) <−c("chi", "xi")
13
return(result)
14
}
15
## HYP ﬁtting
16
hypﬁts <−apply(yrets, 2, ﬁt.hypuv, symmetric = FALSE)
17
points <−matrix(unlist(lapply(hypﬁts, xichi)),
18
ncol = 2, byrow = TRUE)
19
## Shape triangle
20
col.def <−c("black", "blue", "red", "green", "orange")
21
leg.def <−paste(rd, rep("day return", 5))
22
plot(points, ylim = c(−0.2, 1.2), xlim = c(−1.2, 1.2),
23
col = col.def, pch = 16, ylab = expression(xi),
24
xlab = expression(chi))
25
lines(x = c(0, −1), y = c(0, 1))
26
lines(x = c(0, 1), y = c(0, 1))
27
lines(x = c(−1, 1), y = c(1, 1))
28
legend("bottomright", legend = leg.def, col = col.def, pch = 16)
29
text(x = 0.0, y = 1.05, label = "Laplace", srt = 0)
30
text(x = −1.0, y = 1.05, label = "Exponential", srt = 0)
31
text(x = 1.0, y = 1.05, label = "Exponential", srt = 0)
32
text(x = 0.0, y = −0.1, label = "Normal", srt = 0)
33
text(x = −0.6, y = 0.5, label = "Hyperbolic, left skewed",
34
srt = 302)
35
text(x = 0.6, y = 0.5, label = "Hyperbolic, right skewed",
36
srt = 57)
37

78
RISK MODELLING
returns. Overall, however, the point cloud remains fairly close to the centre of the
triangle, so this approximation may not work well. Furthermore, a comparison of the
(ˆξ, ˆχ) pairs indicates that the bimonthly returns show the greatest negative skewness
in absolute terms. Surprisingly, even the weekly returns are more heavily skewed to
the left than their daily counterparts.
6.7
Applications of the GLD to risk modelling and
data analysis
6.7.1
VaR for a single stock
In the ﬁrst application of the GLD a back-test is conducted for the 99% VaR of
the weekly losses of the QCOM stock contained in the S&P 500 Index. The data
is provided in the R package FRAPO. The sample covers the period from 2003 to
2008 and comprises 265 observations. The back-test is expressed in terms of the
unconditional VaR implied by the GLD and the normal distribution. The R code for
conducting the back-test is exhibited in Listing 6.4.
First, the necessary packages are loaded into memory. The ﬁtting of the loss series
to the GLD is accomplished with the functions of the package lmomco and, as stated
above, the data set is contained in the package FRAPO. The data.frame object
SP500 is loaded next and the weekly percentage losses of QCOM are stored in the
object L. Then, the shape of the back-test is deﬁned. A moving window approach
with a window size of 104 observations, equivalent to a time span of 2 years, has been
chosen. The objects ep and sp deﬁne the relevant end and start points, respectively.
The 99% conﬁdent level is assigned to the object level. At line 12 a matrix object is
initialized with row dimension equal to the number of back-testing periods and two
columns in which the VaR of the GLD and the normal distributions will be written.
One might argue that a for loop is not R-like, but to correct a common prejudice,
as long as the proper memory slot of the object VaR is allotted before the loop is
executed, one is not penalized by a worse execution time. Within the loop, the relevant
data window is extracted and ﬁtted to the GLD, and the VaR measures are calculated
for the two distributions and stored in the ith row of VaR. The last line in the loop
informs the user about the progress of the loop execution by printing the time index
and the values of the VaRs. The results are then aggregated into the object Res which
has the losses as ﬁrst column and the VaR measures lagged by 1 week in the next
two columns. The trajectory of the two unconditional VaR measures is then plotted
together with losses as points in the ensuing block of plot statements.
The plot is shown in Figure 6.9. The VaR trajectory according to the GLD model
is more volatile than for the normal distribution. Notice the sharp drop of the GLD
VaR towards the end of the back-test period, which goes hand in hand with a lower
loss volatility, but also reﬂects the sensitivity of moment estimates with respect to
outliers. During this episode, the VaR measures according to the normal distribution
are too conservative. Both models violate the actual losses only once and hence the
size of the risk measure at the 99% conﬁdence level is not violated – that is, given
160 back-test observations one would expect at most two violations.

SUITABLE DISTRIBUTIONS FOR RETURNS
79
Listing 6.4 VaR of QRM stock: comparison of GLD and normal distribution.
## Loading of packages
1
library(lmomco)
2
library(FRAPO)
3
## Data loading
4
data(SP500)
5
Idx <−SP500[, "QCOM"]
6
L <−−1 ∗returnseries(Idx, method = "discrete", trim = TRUE)
7
## Computing VaR (normal and GLD) 99%, moving window
8
ep <−104 : length(L)
9
sp <−1 : length(ep)
10
level <−0.99
11
VaR <−matrix(NA, ncol = 2, nrow = length(ep))
12
for(i in 1 : length(sp)){
13
x <−L[sp[i]:ep[i]]
14
lmom <−lmom.ub(x)
15
ﬁt <−pargld(lmom)
16
VaRGld <−quagld(level, ﬁt)
17
VaRNor <−qnorm(level, mean(x), sd(x))
18
VaR[i, ] <−c(VaRGld, VaRNor)
19
print(paste("Result for", ep[i], ":", VaRGld, "and", VaRNor))
20
}
21
## Summarising results
22
Res <−cbind(L[105 : length(L)], VaR[−nrow(VaR), ])
23
colnames(Res) <−c("Loss", "VaRGld", "VaRNor")
24
## Plot of back−test results
25
plot(Res[, "Loss"], type = "p", xlab = "Time Index",
26
ylab = "Losses in percent", pch = 19, cex = 0.5,
27
ylim = c(−15, max(Res)))
28
abline(h = 0, col = "grey")
29
lines(Res[, "VaRGld"], col = "blue", lwd = 2)
30
lines(Res[, "VaRNor"], col = "red", lwd = 2)
31
legend("bottomleft", legend = c("Losses", "VaR GLD", "VaR Normal"),
32
col = c("black", "blue", "red"),
33
lty = c(NA, 1, 1), pch = c(19, NA, NA), bty = "n")
34
6.7.2
Shape triangle for FTSE 100 constituents
In the second example, the characteristics of FTSE 100 stocks are analysed by
means of a shape triangle for the standardized GLD as in equations (6.23)–(6.27).
This kind of shape triangle was proposed in Chalabi et al. (2010) and applied to
the constituent stocks of the NASDAQ 100. The shape triangle is depicted in the
(δ = λ4 −λ3, β = λ3 + λ4) plane. The R code is shown in Listing 6.5.

80
RISK MODELLING
0
50
100
150
−15
−10
−5
0
5
10
Time Index
Losses in percent
Losses
VaR GLD
VaR Normal
Figure 6.9
Back-test: VaR (99%) and losses for QCOM stock.
The robust estimation of the GLD parameters is covered in the package fBasics
and the weekly price data of the FTSE 100 constituents is part of the package
FRAPO, hence these two packages are loaded into the work space ﬁrst. Next the
data object INDTRACK3 is loaded and its ﬁrst column – representing the FTSE 100
index – omitted from the further analysis. The percentage returns of the stocks are
assigned to the object R which is then used to ﬁt each of its columns to the GLD with
the function gldFit(). This task is swiftly accomplished by utilizing the apply()
function. The object Fit is a list with the returned objects of gldFit. In lines 10–13
a small function is deﬁned that returns the (δ, β) parameter pairs, which are then
plotted in the shape triangle. The output is shown in Figure 6.10.
−2
−1
0
1
2
−2.0
−1.5
−1.0
−0.5
0.0
δ = λ4 −λ3
β = λ3 + λ4
Figure 6.10
Shape triangle for FTSE100 stock returns.

SUITABLE DISTRIBUTIONS FOR RETURNS
81
Listing 6.5 FTSE100 stocks: shape triangle of standardized GLD.
library(FRAPO)
1
library(fBasics)
2
## Loading of data
3
data(INDTRACK3)
4
P <−INDTRACK3[, −1]
5
R <−returnseries(P, method = "discret", trim = TRUE)
6
## Fitting and calculating beta and lambda
7
Fit <−apply(R, 2, gldFit, method = "rob", doplot = FALSE,
8
trace = FALSE)
9
DeltaBetaParam <−matrix(unlist(lapply(Fit, function(x){
10
l <−x@ﬁt$estimate[c(3, 4)]
11
res <−c(l[2] −l[1], l[1] + l[2])
12
res})), ncol = 2, byrow = TRUE)
13
## Shape triangle
14
plot(DeltaBetaParam, xlim = c(−2, 2), ylim = c(−2, 0),
15
xlab = expression(delta == lambda[4] −lambda[3]),
16
ylab = expression(beta == lambda[3] + lambda[4]),
17
pch = 19, cex = 0.5)
18
segments(x0 = −2, y0 = −2, x1 = 0, y1 = 0,
19
col = "grey", lwd = 0.8, lty = 2)
20
segments(x0 = 2, y0 = −2, x1 = 0, y1 = 0,
21
col = "grey", lwd = 0.8, lty = 2)
22
segments(x0 = 0, y0 = −2, x1 = 0, y1 = 0, col = "blue",
23
lwd = 0.8, lty = 2)
24
segments(x0 = −0.5, y0 = −0.5, x1 = 0.5, y1 = −0.5,
25
col = "red", lwd = 0.8, lty = 2)
26
segments(x0 = −1.0, y0 = −1.0, x1 = 1.0, y1 = −1.0,
27
col = "red", lwd = 0.8, lty = 2)
28
The x-axis represents the difference between the right- and left-tail shape param-
eters, and the y-axis their sum. There are a total of six regions discernible in this
triangle. The light grey dashed line discriminates between stocks that are character-
ized by either a left-skewed or a right-skewed distribution. Points on that line refer
to a symmetric distribution. As can easily be seen, the majority of stock returns are
characterized by being skewed to the left, thus conﬁrming a stylized fact of ﬁnancial
returns. Points in the top part of triangle represent return distributions with ﬁnite
variance and kurtosis and points in the middle region, between the −0.5 and −1.0
dark grey dashed lines, refer to return distributions with inﬁnite kurtosis but ﬁnite
variance. The (δ, β) pairs below the −1 line represent return processes where these
moments are inﬁnite, which is the case for one of the FTSE 100 constituents stocks.

82
RISK MODELLING
References
Asquith W. 2012 lmomco – L-moments, trimmed L-moments, L-comoments, censored L-
moments, and many distributions. R package version 1.4.5.
Barndorff-Nielsen O. 1977 Exponential decreasing distributions for the logarithm of particle
size. Proceedings of the Royal Society London A 353, 401–419.
Barndorff-Nielsen O. 1997 Normal inverse Gaussian distributions and stochastic volatility
modelling. Scandinavian Journal of Statistics 24, 1–13.
Barndorff-Nielsen O. 1998 Processes of normal inverse Gaussian type. Finance & Stochastics
2, 41–68.
Chalabi Y., Scott D. and W¨urtz D. 2010 The generalized lambda distribution as an alternative
to model ﬁnancial returns. Working paper, Eidgen¨ossische Technische Hochschule and
University of Auckland, Zurich and Auckland.
Chalabi Y., Scott D. and W¨urtz D. 2011 A more intuitive parameterization of the generalized
lambda distribution R/Rmetrics Meielisalp Workshop, Lake Thune, Switzerland.
Cheng R. and Amin N. 1983 Estimating parameters in continuous univariate distributions with
a shifted origin. Journal of the Royal Statistical Society, Series B 45(3), 394–403.
Eberlein E. and Keller U. 1995 Hyperbolic distributions in ﬁnance. Bernoulli 1, 281–299.
Eberlein E. and Prause K. 1998 The generalized hyperbolic model: ﬁnancial derivatives and
risk measures FDM Preprint 56, University of Freiburg.
Freimer M., Mudholkar G., Kollia G. and Lin C. 1988 A study of the generalised Tukey lambda
family. Communications in Statistics – Theory and Methods 17, 3547–3567.
Gilchrist W. 2000 Statistical Modelling with Quantile Functions. Chapman & Hall/CRC, Boca
Raton, FL.
Hankin R. and Lee A. 2006 A new family of non-negative distributions. Australia and New
Zealand Journal of Statistics 48, 67–78.
Karian Z. and Dudewicz E. 1999 Fitting the generalized lambda distribution to data: a method
based on percentiles. Communications in Statistics – Simulation and Computation 28(3),
793–819.
Karian Z. and Dudewicz E. 2000 Fitting Statistical Distributions: The Generalized Lambda
Distribution and Generalised Bootstrap Methods. CRC Press, Boca Raton, FL.
Karian Z., Dudewicz E. and McDonald P. 1996 The extended generalized lambda distribution
system for ﬁtting distributions to data: History, completion of theory, tables, applications,
the ‘ﬁnal word’ on moment ﬁts. Communications in Statistics – Simulation and Computation
25, 611–642.
Kim T. and White H. 2004 On more robust estimation of skewness and kurtosis. Finance
Research Letters 1(1), 56–73.
King R. 2011 gld: Estimation and use of the generalised (Tukey) lambda distribution. R
package version 1.9.2.
King R. and MacGillivray H. 1999 A starship estimation method for the generalised lambda
distributions. Australia and New Zealand Journal of Statistics 41(3), 353–374.
Luethi D. and Breymann W. 2011 ghyp: A package on the generalized hyperbolic distribution
and its special cases. R package version 1.5.5.
McNeil A. and Ulman S. 2011 QRMlib: Provides R-language code to examine quantitative
risk management concepts. R package version 1.4.5.1.

SUITABLE DISTRIBUTIONS FOR RETURNS
83
McNeil A., Frey R. and Embrechts P. 2005 Quantitative Risk Management: Concepts,
Techniques and Tools. Princeton University Press, Princeton, NJ.
Owen D. 1988 The starship. Communications in Statistics – Simulation and Computation 17,
315–323.
Pfaff B. 2012 QRM: Provides R-language code to examine quantitative risk management
concepts. R package version 0.4-7.
Prause K. 1997 Modelling ﬁnacial data using generalized hyperbolic distributions
FDM Preprint 48, University of Freiburg.
Prause K. 1999 How to use NIG laws to measure market risk FDM Preprint 65, University of
Freiburg.
Ramberg J. and Schmeiser B. 1974 An approximate method for generating asymmetric random
variables. Communications of the Association for Computing Machinery 17, 78–82.
Ramberg J., Pandu R., Dudewicz E. and Mykytka E. 1979 A probability distribution and its
uses in ﬁtting data. Technometrics 21(2), 201–214.
Ranneby B. 1984 The maximum spacing method: An estimation method related to the maxi-
mum likelihood method. Scandinavian Journal of Statistics 11(2), 93–112.
Scott D. 2011 GeneralizedHyperbolic: The generalized hyperbolic distribution. R package
version 0.7-0.
Scott D. and Dong CY. 2010 VarianceGamma: The variance gamma distribution.R package
version 0.3-0.
Scott D. and Grimson F. 2010 SkewHyperbolic: The skew hyperbolic Student t-distribution.R
package version 0.2-0.
Su S. 2005 A discretized approach to ﬂexibly ﬁt generalized lambda distributions to data.
Journal of Modern Applied Statistical Methods 4(2), 408–424.
Tukey J. 1962 The future of data analysis. Annals of Mathematical Statistics 33(1), 1–67.
W¨urtz D. 2012 fBasics: Rmetrics – markets and basic statistics. R package version 2160.81.

7
Extreme value theory
7.1
Preliminaries
Extreme value theory (EVT) is a branch of statistics. Its subject is the modelling
of large deviations from the median of a distribution. EVT is by no means a young
discipline – its roots can be traced back to the 1920s. However, interest in EVT and
its application to modelling ﬁnancial market risks has picked up only during the
last decade among practitioners. The reason for this development might well be the
increasing occurrence of ﬁnancial market turmoil episodes.
This chapter begins with the theoretical underpinnings of EVT. Three approaches
to modelling extreme values are presented: the block maxima method, the peaks-over-
threshold method and Poisson processes. This exposition is by no means exhaustive,
and its purpose is only to help the reader gain a better idea of how these methods are
implemented in R. For a more thorough treatment of the subject the reader is referred
to the monographs of Coles (2001), Embrechts et al. (1997), Leadbetter et al. (1983)
and McNeil et al. (2005, Chapter 7).
Section 7.3 provides a synopsis of the currently available packages in R. Although
there is quite an overlap between the various EVT models and their implementations
in R, a comparison of the various packages reveals subtle differences, weaknesses
and strengths. Hopefully, the user will gain an insight to what is already imple-
mented and how. Having done so, s/he can utilize the package(s) as desired. All
packages presented are available on CRAN and can be swiftly installed by executing
install.packages(’foo’) for package foo.
The chapter concludes with some empirical applications of EVT, in which the
various methods are applied to ﬁnancial market data. The focus is on how EVT fares
in contrast to the assumption of normally distributed losses.
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

EXTREME VALUE THEORY
85
7.2
Extreme value methods and models
7.2.1
The block maxima approach
The focus of EVT is on the modelling and inference of maxima. Assume that a
sequence of i.i.d. random variables X1, . . . , Xn over a time span of n periods is
given. Ordinarily, the time span is a calendar period such as a month, quarter, half
year, or year, and the observed data within this period is of the daily frequency. With
respect to EVT, the question arises as to which distribution the maximum of these
random variables,
Mn = max {X1, . . . , Xn} ,
(7.1)
follows or, to put it more precisely, is asymptotically best approximated by.
In principle, if the distribution function for the Xi is assumed to be known, then
the distribution of Mn could be derived as:
P {Mn ≤z} = P {X1 ≤z, . . . , Xn ≤z}
= P {X1 ≤z} × . . . × P {Xn ≤z}
= {F(z)}n .
(7.2)
In practice, this approach is not feasible for various reasons. First, the distribution
function F is in general unknown. This could be rectiﬁed by either estimating a
kernel density or assuming that the Xi are governed by a particular distribution. If
this route is taken, the next obstacle is that estimation errors are raised to the power
of n, leading to quite divergent results. An alternative route would be to seek a family
of distributions Fn that can be used to approximate any kind of F. Therefore the
characteristics and properties of Fn for n →∞need to be investigated. However,
this asymptotic reasoning would imply that the values of the distribution function for z
less than z+ approach zero, whereby z+ denotes the upper right point. Put differently,
the mass of the distribution would collapse over the point z+. This artefact can be
circumvented by a linear transformation M∗
n of Mn:
M∗
n = Mn −bn
an
(7.3)
where an > 0 and bn are sequences of constants. The purpose of these constants is
to straighten out Mn, such that the probability mass would not collapse over a single
point. Under the assumption that the sequences an and bn exist, it can be shown that
the probability expression
P

M∗
n = Mn −bn
an
≤z

→G(z)
for n →∞,
(7.4)
converges to a non-degenerate distribution G(z), which belongs to one the following
distribution families: Gumbel, Fr´echet or Weibull. All three distributions have a
location and scale parameter. The Fr´echet and Weibull distributions also have a shape

86
RISK MODELLING
parameter. The three distributions can be subsumed into the generalized extreme
value (GEV) distribution,
G(z) = exp

−1

1 + ξ
z −μ
σ
−1/ξ	
.
(7.5)
The GEV is a three-parameter distribution where μ is the location, σ the scale
and ξ the shape parameter. For the limit ξ →0 the Gumbel distribution is obtained,
for ξ > 0 the Fr´echet, and for ξ < 0 the Weibull. The Weibull has a ﬁnite right point,
whereas z+ is inﬁnity for the other two distributions. The density is exponential in the
case of Gumbel and polynomial for the Fr´echet distribution. Hence, the characteristics
and properties of the GEV can be deduced from the value of the shape parameter.
7.2.2
rth largest order models
A problem that arises when the block maxima method is applied to ﬁnancial series is
the lack of a sufﬁciently long data history. In these instances, the unknown distribution
parameters will be estimated with greater uncertainty. To circumvent this problem
the rth largest order model has been suggested. Not only the maximal loss in a
given block is used as a data point for ﬁtting the GEV, but also the r largest loss
observations are employed. The data selection pertinent to the block maxima and the
r largest orders is shown in Figure 7.1.
In this ﬁgure 100 losses have been randomly generated and subdivided into 10
blocks of equal size. The maximum losses in each block are indicated by dark grey
squares. Theses data points would be used in the block maxima approach. Further, the
second largest losses in each of the 10 blocks are marked by solid light grey points.
The sets of these ﬁrst- and second-order losses would be utilized if one were to ﬁt
a second largest order model. The variance of the parameter estimates will certainly
0
20
40
60
80
100
0
1
2
3
4
Losses
Figure 7.1
Block maxima, r largest orders and peaks-over-threshold.

EXTREME VALUE THEORY
87
be reduced by increasing the sample size. However, one runs the risk of having a
biased sample in the sense that the second highest losses possibly would not qualify
as extreme events or be considered as such. Finally, a dashed horizontal line has been
drawn on the ﬁgure at the ordinate value of 2.5. This threshold value leads directly
into the topic of the next subsection.
7.2.3
The peaks-over-threshold approach
With regard to the application of the block maxima method and the rth largest order
model in the context of ﬁnancial market data, the following issues arise:
r For a given ﬁnancial instrument a sufﬁciently long data history is often not
available. This will result in wide conﬁdence bands for the unknown distribu-
tion parameters. As a consequence, the derived risk measures should be used
with great caution.
r Not all observations that should be considered as extremes are exploited in
estimating the distribution parameters – although this issue is ameliorated in
the case of the rth largest order statistics. Hence, the available information
about extreme events is not fully taken into account.
r Given the stylized facts for univariate return series, in particular volatility clus-
tering, data points during tranquil periods could be selected as block maxima
values, which they are not. Hence, an estimation bias would result in these
instances.
Due to these issues the peaks-over-threshold (POT) method is more widely en-
countered when ﬁnancial risks are of interest. As the name of this method suggests,
neither the block maxima data points nor the r largest values within a block are con-
sidered as extreme observations, but rather all observations above a certain threshold
value. This can be summarized for a given threshold u by the following probability
expression:
P {X > u + y|X > u} = 1 −F(u + y)
1 −F(u)
,
y > 0.
(7.6)
It is evident from this equation that when the distribution function F is known,
the expression can be solved for the exceedances y > u. In practice, however, the
distribution function F is generally unknown and hence, similarly to the derivation
of the GEV, one needs an approximative distribution for sufﬁciently large threshold
values. It can be shown that the exceedances (X −u) are distributed according to the
generalized Pareto distribution (GPD),
H(y) = 1 −

1 + ξy
˜σ
−1/ξ
,
(7.7)
for y : y > 0 and ˜σ = σ + ξ(u −μ). This means, that if the distribution of the block
maxima for an i.i.d. sample can be approximated by the GEV, then the exceedances
can be approximated by the GPD for a sufﬁciently large threshold value u. By

88
RISK MODELLING
comparison of the GEV and GPD, it can be concluded that the shape parameter ξ
is identical for both and hence independent of the chosen number of block maxima.
It can then be deduced further that the distribution for ξ < 0 possesses an upper
bound of u −˜σ/ξ and is unbounded to the right for parameter values satisfying
ξ > 0. At the limit of ξ →0 the GPD converges to an exponential distribution with
parameter 1/˜σ.
In practice, a difﬁculty arises when an adequate threshold has to be selected. If
this value is chosen too small, a biased sample results. In this case observations would
be selected which are too small such that the GPD approximation would be violated.
In contrast, if u is chosen too large, the sample size might become too small to yield
reliable estimates for the unknown distribution parameters. Thus, a trade-off between
a biased estimate and a large estimation error is required. An adequate threshold
value can be determined graphically by means of a mean residual life (MRL) plot.
This kind of plot is based on the expected value of the GPD: E(Y) = σ/(1 −ξ). For
a given range of thresholds u0 the conditional expected values
E(X −u0|X > u0) =
σu0
1 −ξ =
σu
1 −ξ = σu0 + ξu
1 −ξ
(7.8)
are plotted against u. This equation is linear with respect to the threshold u:

u, 1
nu
nu

i=1
(xi −u)

: u < xmax
	
.
(7.9)
Hence, a suitable value for u is given when this line starts to become linear. Due
to the central limit theorem (CLT), conﬁdence bands can be calculated according to
the normal distribution, owing to the fact that arithmetic means are depicted in an
MRL plot.
Most often the parameters of the GPD are estimated by applying the ML principle,
but other methods, such as probability-weighted moments, are also encountered in
empirical applications. The advantage of the former method is the possibility to plot
the proﬁle log-likelihood for the shape parameter ξ and compute conﬁdence bands
based on this proﬁle. In contrast to the bands derived from the normality assumption
for the estimator, these are in general asymmetric. Hence, inference with respect to
the sign of the shape parameter becomes more precise.
The VaR and ES risk measures can be inferred directly from the GPD as follows:
VaRα = qα(F) = u + ˜σ
ξ

1 −α
¯F(u)
−ξ
−1

,
(7.10)
ESα =
1
1 −α
 1
α
qx(F)dx = VaRα
1 −ξ + ˜σ −ξu
1 −ξ ,
(7.11)
where ¯F(u) denotes the number of non-exceedances relative to the sample size.
Analogously to the conﬁdence bands for the distribution parameters, intervals for
these risk measures can also be derived.

EXTREME VALUE THEORY
89
7.3
Synopsis of R packages
7.3.1
The package evd
Although the packages are listed in alphabetical order in this section, another rea-
son for beginning with the package evd (see Stephenson 2002) is that it has the
longest history on CRAN. It was ﬁrst published in February 2002. The package’s
name is an acronym for ‘Extreme Value Distributions’. It is contained in the CRAN
‘Distributions’, ‘Environmetrics’ and ‘Finance’ Task Views. Within the package S3
methods and classes are implemented. The computationally burdensome algorithms
are implemented as C routines. Incidentally, the package evd does not ship with a
NAMESPACE ﬁle per se.
The functions, methods and data sets provided in this package cover basically all
aspects of EVT. To cite from the package’s description:
Extends simulation, distribution, quantile and density functions to uni-
variate and multivariate parametric extreme value distributions, and pro-
vides ﬁtting functions which calculate maximum likelihood estimates for
univariate and bivariate maxima models, and for univariate and bivariate
threshold models.
In particular, the following univariate extreme value distributions are implemented:
r the extreme value distributions [dqpr]extreme()
r the Fr´echet distribution [dqpr]frechet()
r the Generalized extreme value distribution [dqpr]gev()
r the Generalized Pareto distribution [dqpr]gpd()
r the Gumbel distribution [dqpr]gumbel()
r the Distributions of order statistics [dqpr]order()
r the reversed Weibull distribution [dqpr]rweibull()
For all these distributions the density, quantile and distribution functions are imple-
mented. Random number generation according to these distributions is also avail-
able. The dpqr naming convention referred to in Section 6.4.1 is followed: a preﬁx
d is added for the values of the density function, q for the quantile function, p
for the distribution function and r for generating random numbers from one of the
above mentioned distributions. The ML method for estimating the unknown param-
eters of these distributions is used. The relevant functions are fgev(), fpot(),
forder() and fextreme(). Two models can be speciﬁed, if the POT method is
utilized, namely, the generalized Pareto distribution as well as a Poisson point pro-
cess. To aid the user’s choice of an appropriate threshold value, the mean residual life
plot (mrlplot()), the threshold choice plot (tcplot()) and dependence measure
plots (chiplot()) are implemented as preliminary diagnostic tools. Fitted model

90
RISK MODELLING
objects can be graphically inspected by the available plot() methods. In addition,
S3 methods for retrieving and plotting the proﬁle log-likelihood are available.
Conﬁdence intervals are calculated for either ﬁtted model objects or log-likelihood
proﬁles with the S3 method confint(). Finally, it can be checked whether nested
models differ signiﬁcantly from each other by means of a likelihood ratio test, which
is implemented as an anova() method.
Apart from these univariate distributions, the package offers bivariate (bvevd())
and multivariate (mvevd()) extreme value distributions as well as non-parametric
estimation methods (abvnonpar(), amvnonpar(), qcbvnonpar()). For bivariate
extreme value distributions parametric estimation is accomplished with the func-
tions fbevd() and fbvpot(). The parametric spectral density function of bivariate
extreme value models can be computed and plotted with the function hbvevd().
A distinctive feature of the package is the possibility of simulating extreme value
stochastic processes. These can be speciﬁed as either a Markov chain (evmc())
or a maximum autoregressive moving average (marma(), mar(), mma()) process.
Clusters of extreme values can be inspected with the function clusters(), and the
extreme value index is implemented as function exi().
The package is shipped with nine data sets that have been used extensively in
the academic literature. A user guide in the form of a PDF ﬁle within the ./doc
subdirectory of the installed package is available as well as a demo ﬁle that can be
called within R as demo(soe9). Within this demonstration, the results of Beirlant
et al. (2004, Chapter 9) are replicated.
7.3.2
The package evdbayes
The package evdbayes provides functions and methods for the Bayesian analysis
of extreme value models (see Stephenson and Ribatet 2010). Markov chain Monte
Carlo (MCMC) techniques are utilized. Four different kinds of EVT models are
implemented: the generalized extreme value distribution, the generalized Pareto dis-
tribution, the order statistics and the Poisson point process model. The likelihood
functions, prior distributions and Gibbs samplers (mixing and non-mixing) are inter-
faced from C routines. The latter routines are used for generating the Markov chains.
Unfortunately, a NAMESPACE ﬁle is not included. The package is contained in the
CRAN ‘Bayesian’, ‘Distributions’, ‘Environmetrics’ and ‘Finance’ Task Views.
The main functions of the package can be split into four categories: generat-
ing prior distributions, generating Markov chains, local maximization of the prior/
posterior distribution and diagnostic tools. First, the functions for specifying the
prior distributions are named prior.prob(), prior.quant(), prior.norm()
and prior.loglognorm(). The user can choose the normal, beta or gamma dis-
tribution as prior for the unknown model parameters. The functions return a list
object of informal S3 class evprior. Next, a Markov chain must be generated with
the function posterior(), with the previously generated evprior object as one of
its arguments. The package’s authors suggest the function mcmc contained in the pack-
age coda (see Plummer et al. 2006) for inspecting the validity and well-behavedness
of the Markov chain generated. The prior and posterior distributions are optimized

EXTREME VALUE THEORY
91
with the function mposterior(). This function is a wrapper for the general-purpose
optimizer optim() which is shipped within the base installation of R. Finally, two
functions for graphical inspection of an EVD model are included: rl.pst() and
rl.pred(). These functions depict the return levels of GEV quantiles and predictive
distributions, respectively.
The rainfall data set is contained in the package as well as user documentation.
The latter is stored in the installation tree of the package in its doc subdirectory
and named ‘evdbayesGuide.pdf’. Within this guide the theoretical underpinnings
of Bayesian analysis in the context of EVT are outlined and the reader is guided
through several empirical applications in which the usage of the functions provided
is exhibited and discussed.
7.3.3
The package evir
The package evir has been ported from the S-PLUS package EVIS written by McNeil
(1999) (see McNeil and Stephenson 2011). Its name is an acronym for ‘Extreme
Values in R’. Because it was orphaned in 2008, the author of this book became the
package’s maintainer. In contrast to the two previously introduced packages, evir does
not interface any C routines, but is rather a self-contained collection of R functions
that encompasses the most commonly encountered techniques in EVT. Within the
package informal S3 classes and methods are employed. The package is contained in
the CRAN ‘Distributions’, ‘Environmetrics’ and ‘Finance’ Task Views.
The functions included can be grouped loosely into the following categories:
r exploratory data analysis (EDA);
r block maxima method;
r peaks-over-thresholds (POT);
r bivariate POT;
r POT with Poisson point processes;
r generalized extreme value and generalized Pareto distributions.
Within the ﬁrst group of functions, the user can investigate the data set of interest
graphically by plotting the empirical distribution function (emplot()), the Hill es-
timate of the tail index of heavy-tailed data (hill()), or the mean excesses over
increasing thresholds (meplot()). Finally, QQ plots against the exponential distri-
bution or the GPD can be created with the function qplot(). In addition to these
graphical means of EDA the user can estimate an extremal index with the function
exindex() and display the records of a data set against the expected behaviour of
an i.i.d. data set with the function records().
The functions belonging to the second category, the block maxima method, are
gev() and gumbel() for the generalized extreme value and the Gumbel distribution,
respectively. The latter function is designed to help the user assess whether a Fr´echet

92
RISK MODELLING
or Weibull distribution is more appropriate. Both functions return an object with
class attribute gev. For these kinds of objects a plot and return level method are
implemented.
The cornerstone function for applying the POT techniques is gpd(). A GPD
can be ﬁtted by either the ML or the PWM method. The function returns an object
with class attribute gpd. A plot method exists for these ﬁtted models. The user
can choose from four different kind of plots: the excess distribution, the tail of the
underlying distribution, a scatterplot of the residuals or a QQ plot of the residuals.
The tail plot can produced directly with the function tailplot(), bypassing the
interactive plot menu. Furthermore, quantile estimates and conﬁdence intervals for
high quantiles above the threshold value can be superimposed on this plot with the
function gpd.q(). In addition, an estimate of the expected shortfall can be included
in the tail plot by utilizing the function gpd.sfall(). The responsiveness of the
estimate for the shape parameter of the GPD with respect to the threshold values can
be depicted with the function shape(). The relation of the estimate for a speciﬁed
high quantile and the threshold or the number of extremes can be analysed graphically
with the function quant(). Both of these graphical displays can be used as a means
of sensitivity analysis for the ﬁtted GPD and thus help assess whether an appropriate
threshold value or the number of exceedances has been set. Finally, the function
riskmeasures() returns a point estimate of a quantile for a given probability.
Hence, this function can be swiftly employed for extracting the value at risk of a
ﬁtted GPD object.
Similarly to the package evd, bivariate GPD can be ﬁtted with evir. The function
is termed gpdbiv() and the user can specify either the threshold values or the number
of extreme observations. In addition, the user can choose to conduct a global or a
two-stage optimization. With the latter optimization the marginal distributions are
ﬁtted ﬁrst and, based thereon, the dependence parameter is determined in a second
step. The function returns an object with class attribute gpdbiv. A plot() method
exists for this kind of object. Similarly to the implemented plot() method for objects
of class gpd, one can choose from ﬁve different plot types. Plots for the threshold
exceedances, a contour plot of the ﬁtted bivariate distribution function, a contour
plot of the ﬁtted joint survival function and plots of the marginal distribution are all
implemented. Finally, the interpret() method has been deﬁned for calculating
joint and conditional probabilities.
Turning to the ﬁfth category, the user can estimate a Poisson point process with
the function pot(). This function returns an object of informal class potd for which
a plot() method is deﬁned. Eight different kind of plots are available within this
method. First, the exceedances of the point process can be plotted. Whether the gaps
of this process are well-behaved can be inspected by either a scatterplot of the gaps or
a QQ plot, as well as the autocorrelation function thereof. In addition, the residuals
of the ﬁtted model object can be graphically analysed in the form of a scatterplot,
a QQ plot and their autocorrelation function. Finally, the user can employ the same
plot() methods that are available for ﬁtted GPD models.
Within the ﬁnal category, functions for the GEV distribution and the GPD are
subsumed. The usual dpqr naming convention for the various functions and for

EXTREME VALUE THEORY
93
random number generation is followed. Hence, the relevant functions for the GEV are
named dgev(), qgev(), pgev() and rgev(), and for the GPD the corresponding
functions are termed dgpd(), qgpd(), pgpd() and rgpd().
Seven data sets from the ﬁelds of hydrology (nidd.annual and nidd.thresh),
insurance (danish) and ﬁnance (bmw, siemens, sp.raw and spto87) are included
in the package. Furthermore, demo ﬁles are provided to help the user understand of
how to apply the functions contained in the package.
7.3.4
The package fExtremes
The package fExtremes is part of the Rmetrics suite of packages (see W¨urtz 2009b).
In contrast to the packages presented above, formal S4 classes and methods are
utilized and – like evir – fExtremes is written purely in R. The package is considered
a core package in the CRAN ‘Finance’ Task View and is listed in the ‘Distributions’
Task View. It has dependencies on other packages contained in the Rmetrics suite.
Quite a few functions have been directly ported and/or included from the packages
evd, evir, ismev and extRemes. The latter two packages are presented in the next
subsection.
The functions and methods contained in fExtremes can be grouped loosely
into ﬁve groups: data preprocessing, EDA, the GEV distribution, the GPD and the
calculation of risk measures. The bmw and danish data sets from evir are also
contained in fExtremes.
The functions blockMaxima(), findThreshold(), pointProcess() and
deCluster() are provided for data preprocessing. The ﬁrst function can be used
for extracting the block maximum values that could then be used for ﬁtting a GEV
distribution. The next two functions can be used when POT techniques are employed.
Within the function findThreshold() the number of exceedances must be speciﬁed
and the function returns the values that exceed the threshold in accordance with this
number. The function pointProcess() also returns exceedances, but here the user
speciﬁes an upper quantile. Finally, the function deCluster() can be used when a
Poisson point process is to be ﬁtted and the extreme observations occur closely with
respect to time (e.g., volatility clustering).
Like the package evir, functions and methods for exploratory data analysis
are also contained in this package. These can be split into functions with which
the data can be analysed for the occurrence of extremal events as well as tools
for visual inspection. The functions blockTheta(), clusterTheta(), run-
Theta() and ferrosegersTheta() belong to the former subgroup. The last
of these, for calculating the extremal index, was proposed by Ferro and Segers
(2003). All of these functions return an object of S4 class fTHETA for which a
show() method is deﬁned. The functions exindexesPlot() and exindexPlot()
for graphically displaying extremal indexes are also available. It is further possi-
ble to simulate a series for given value of the extremal index with the function
thetaSim().
Similar to the package evir, functions for plotting the empirical distribution
function (emdPlot()), producing a QQ plot against the GPD or exponential

94
RISK MODELLING
distribution (qqparetoPlot()), displaying the mean excesses (mePlot() and
mxfPlot()) as well as the development of records (recordsPlot() and
ssrecordsPlot()) are available in fExtremes. In addition, the user can graph
the mean residual lives (mrlPlot()), the ratio of maximum over sum (msratio-
Plot()) as well as the ﬁts of the mean excesses according to either the normal
(normMeanExcessFit()) or the generalized hyperbolic distribution (ghMeanEx-
cessFit()) and its derived distributions, namely the normal inverse Gaussian
(nigMeanExcessFit()) and the hyperbolic distribution (hypMeanExcessFit()).
The ACF of the exceedances as well as their distance can be plotted with the function
xacfPlot(). Finally, Kolmogorov’s strong law of large numbers and Khinchine’s
law of the iterated logarithm can be veriﬁed with the functions sllnPlot() and
lilPlot(), respectively.
The third group of functions encompasses the block maxima technique. The den-
sity, quantile and distribution function of the GEV and the random number generator
are termed the same as in the package evir, namely, dgev(), qgev(), pgev() and
rgev(), respectively. In addition, the true moments of the GEV distribution can
be calculated with the function gevMoments() and either the density or random
numbers can be displayed for combinations of the distribution parameters with
the function gevSlider(). As the function’s name implies, the user can specify
the values for the size and the distribution parameters by a Tcl/TK interface and the
resultant GEV or random numbers are displayed immediately. The parameters of
the GEV and the Gumbel distribution can be ﬁtted to block maxima data by the
functions gevFit() and gumbelFit(). Both functions return an object of S4 class
fGEVFIT. Here, the unknown parameters can be estimated by either the ML principle
or the PWM method. There are plot(), print(), and summary() methods for such
objects. A return level plot can be generated with the function gevrlevelPlot().
Furthermore, data sets that are distributed according to either the GEV or Gumbel
distribution can be generated with the functions gevSim() and gumbelSim(),
respectively. These functions are wrappers for rgev(), but the user can specify
the seed of the random number generator as a functional argument. Finally, the shape
parameter can determined according to maximum domain of attraction (MDA) esti-
mators. The relevant functions are hillPlot() and shaparmPlot() for the Hill
and shape parameter plot, respectively.
Basically, the same naming convention is adhered to when the POT technique is
applied to the GPD. The dpqr naming convention is followed, hence dgpd() denotes
the density, pgpd() the distribution and qgpd() the quantile function of the GPD.
Random numbers can be generated with the function rgpd. A wrapper for this func-
tion for simulating data according to the GPD is provided as gpdSim(). A function
for retrieving the true moments (gpdMoments()) and the dynamic plotting facility
(gpdSlider()) are also available. The parameters of the GPD can be estimated with
the function gpdFit(). This can be accomplished by using either the ML or the
PWM method. The resulting object is of formal class fGPDFIT for which print(),
plot() and summary() methods exist.
With respect to the calculation and visual inspection of tail risks, the func-
tions tailRisk(), gpdRiskMeasures(), tailplot() and gpdTailPlot() are

EXTREME VALUE THEORY
95
available for ﬁtted GPD models. In addition, the empirical VaR and ES can computed
with the functions VaR() and CVaR().
7.3.5
The packages ismev and extRemes
The package ismev is the second oldest contributed package on CRAN (see Coles
and Stephenson 2011). It is an R-port of functions originally written in S by S.
Coles (see Coles 2001). The package is contained in the CRAN ‘Environmetrics’
and ‘Finance’ Task Views. It includes routines for replicating the computations and
results in Coles (2001). It is shipped with 12 data sets originating in the ﬁelds of
ﬁnance and environmetrics. In addition, demonstration ﬁles are available to help the
user understand how to use the package.
The functions can be grouped loosely into ﬁve categories: GEV models, GPD
models, Poisson point processes, order statistics models and utility functions to guide
the user in choosing threshold values for the POT method.
The functions for ﬁtting block maxima models (i.e., the GEV and Gumbel dis-
tributions) are gev.fit() and gum.fit(), respectively. The unknown distribution
parameters are estimated by the applying the ML principle. Diagnostic testing of
the appropriateness of the model can be checked graphically with the functions
gev.diag() for the GEV model and gum.diag() for the Gumbel distribution.
The proﬁle log-likelihood function for the GEV distribution can be plotted with the
functions gev.prof() and gevprofxi(). The former function produces a return
level plot for m years per block. The latter function returns the proﬁle with respect to
the shape parameter only. Conﬁdence bands can be superimposed on both kinds of
graph.
Two kinds of model for the POT techniques are available: the GPD distribution
and a point process model. The unknown model parameters are estimated with the
functions pdf.fit() and pp.fit() by applying the ML principle. Similarly to the
block maxima method, the functions for graphically inspecting the model’s adequacy
are named gpd.diag() for the GPD and pp.diag() for the point process model.
The proﬁle log-likelihood can be plotted analogously to the GEV with the functions
gpd.prof() and gpd.profxi().
Three functions are intended for guiding the user by choosing an appropriate
threshold value when the POT technique is applied. First, an MRL plot can be gener-
ated with the function mrl.plot(). The two other functions ﬁt the GPD or the point
process model over a user-speciﬁed range of threshold values and depict the path of
the shape parameter for the GPD and the location, scale and shape parameters for the
point process model.
Finally, an order statistics model can be ﬁtted with the function rlarg.fit()
and the outcome can be graphically investigated with the function rlarg.diag().
Within the package extRemes a Tcl/TK graphical user interface (GUI) for the
package ismev has been implemented (see Gilleland and Katz 2011). This package is
mainly intended for illustrating the application of EVT pedagogical purposes. Apart
from the facilities offered in ismev, the package extRemes also includes functions for
de-clustering, bootstrapping, simulation of extremal data as well as the calculation

96
RISK MODELLING
and analysis of extremal indexes. More information can be found on the package’s
website http://www.assessment.ucar.edu/toolkit/.Inparticular,adetailed
tutorial on this package is available.
7.3.6
The package POT
As the name of the package implies, POT is solely dedicated to the POT technique
(see Ribatet 2011). Functions for the estimation of point processes, univariate and
bivariate ﬁtting of the GPD to the exceedances are included, as well as tools for EDA
and diagnostic testing. The more burdensome computations are interfaced from C
routines. The package is shipped with a vignette not only to help the user understand
EVT but also to elaborate on the usage of the functions contained in the package.
The latter is mainly achieved through a concrete statistical analysis of the POT
technique. The package is contained in the CRAN ‘Distributions’, ‘Environmetrics’
and ‘Finance’ Task Views.
The facilities provided can be grouped into three main categories: GPD; threshold
selection; and estimation, inference and diagnostic testing. Incidentally, functions for
the de-clustering of extreme values with respect to time and the conversion of return
periods to non-exceedance probabilities and vice versa are also included.
As with the other packages that deal with the GPD, the dpqr naming convention is
followed in POT for the GPD and the generation of random numbers. So, for example
the routine rgpd() is available for random number generation. The functions pgpd()
and qgpd() allow the user to specify whether the returned values should be calculated
for the lower or upper tail of the GPD. With respect to the density of the GPD, the
log value can be returned instead.
For determining an appropriate threshold value the functions tcplot(),
mrlplot(), lmomplot() and diplot() are at the user’s disposal. Similar to the
package ismev, the ﬁrst two functions return either the path of the shape and mod-
iﬁed scale parameter for speciﬁed lower and upper bounds of the threshold values,
while the latter produces an MRL plot. The lower partial moments (i.e., the empirical
moments of the exceedances) can be plotted with the function lmomplot(). For a
speciﬁed range of threshold values the dispersion index proposed by Cunnane (1979)
can be plotted with the function diplot(). The user can check that the exceedances
follow a Poisson process.
Univariate or bivariate data can be ﬁtted to the GPD with the functions fit-
gpd() and fitbvgpd(), respectively. The unknown parameters of the univariate
GPD can be estimated by the maximum likelihood, method of moments, biased and
unbiased probability weighted moments, mean power density divergence, median,
Pickand’s maximum penalized likelihood and maximum goodness-of-ﬁt estimators.
The parameters for the bivariate GPD can be estimated by the ML principle. The
dependence between the two series of exceedances can be captured according to the
logistic, asymmetric logistic, negative logistic, asymmetric negative logistic, mixed
or asymmetric mixed model. The possibility of estimating the unknown distribution
parameters as a Markov chain of exceedances is also available. Point processes can
be estimated with the function fitpp(). Fisher conﬁdence intervals for the shape

EXTREME VALUE THEORY
97
and scale parameter as well as the return level can be determined for ﬁtted GPD
models with the functions gpd.fishape(), gpd.fiscale and gpd.firl(), re-
spectively. The user can investigate the appropriateness of a model graphically with
the plot() method. By default, this function will produce four graphs: a PP plot, a
QQ plot, a return level plot and a histogram with the ﬁtted density superimposed on
it. Furthermore, it is possible to generate these graphics separately.
7.3.7
The package QRM
The package QRM was introduced in Section 6.4.4. Its EVT facilities include func-
tions for block maxima, POT, point processes as well as the calculation of risk
measures. In addition, methods for graphically depicting the ﬁtted model objects or
aspects thereof are included, too.
The GEV and Gumbel distributions are implemented, and the naming convention
for the density, distribution, quantile function and the generation of random numbers
is followed, giving, for example, dGPD() and dGumbel() for the respective densities.
The ﬁtting of a GEV model is accomplished with the function fit.GEV(). Here,
the ML principle is applied and the optimization is conducted with the general
purpose optimizer optim(). The function is speciﬁed with an ellipsis argument
within optim(). This gives the user a choice of optimizer and provides further
control of the numeric optimization.
With respect to the POT techniques, the values of the exceedances for a given
number thereof can be retrieved with the function findthreshold(). It is further
possible to plot the sample mean excesses for varying thresholds with the function
MEplot() and/or to produce a Hill plot (hillPlot()). The latter function is the
same as the one in the package evir. To estimate a GPD the routine fit.GPD() can
be used. Here, the user can choose between the functions optim() and nlminb()
for numerically determining the unknown coefﬁcients. The ﬁtted model can be
investigated with the functions plotTail() and plotFittedGPDvsEmpiri-
calExcesses(). The trajectory of the shape parameter can be displayed for a range
of threshold values with the function xiplot(). The risk measures VaR and ES for
models ﬁtted to the GPD can be calculated with the function RiskMeasures() or
showRM(). The latter function returns a tail plot with the value of the estimated risk
measure and a conﬁdence band.
The extremal point process data can be retrieved via the function extremalPP()
and visually inspected as either a marked point process (plot.MPP()) or a point
process (plot.PP()). Data objects of this kind can be ﬁtted to a GPD with the func-
tion fit.POT(). Furthermore, it is possible to estimate self-exciting point processes
with the routines fit.sePP() and fit.seMPP().
7.3.8
The package Renext
The package Renext is the latest addition to the R packages on CRAN dealing
explicitly with EVT (see Deville 2012). It was ﬁrst added in 2010. Within the pack-
age functions for the ‘m´ethode de renouvellement’ – a technique similar to the

98
RISK MODELLING
POT – are implemented. The major difference of this technique compared to the POT
is the assumption that exceedance times follow a homogeneous Poisson process and
are independent of the sizes thereof. Given the validity of these assumptions, the
exceedances can be modelled not only with the GPD but also with other distribu-
tions, such the exponential, Weibull, gamma, log-normal, mixtures of the exponential,
shifted left truncated Weibull and the shifted Pareto. As stated in the package’s vi-
gnette, this method is quite popular among French hydrologists. The package is
written purely in R and a NAMESPACE is implemented.
The functions contained in the package can be grouped mainly into the three
categories: EDA and estimation; inference and simulation of this kind of model; and
utility functions.
Within the ﬁrst category, plot functions for investigating whether the exceedances
can be approximated by the exponential or Weibull distribution are named exp-
plot() and weibplot(), respectively. The number of exceedances within speciﬁed
time plots can be depicted as a barplot with the function barplotRebnouv().
The function for ﬁtting renewal models is termed fRenouv() and that for gener-
ating random numbers from this kind of model is named rRenouv(). The unknown
parameters of the speciﬁed distribution are estimated according to the ML principle
using the function optim(). By default, the data will be ﬁtted to the exponential
distribution, though the package also contains the relevant functions for ﬁtting the
above-listed distributions. A return level plot is generated in addition to the estimation
results. This plot can also be produced separately with the function RLplot().
The functions for reading Rendata objects in a speciﬁc XML format (readXML()),
the plotting thereof (plot.Rendata()), the computation of parameter values from
theoretical moments (mom2par()) and goodness-of-ﬁt tests for the repartition of
dates and the exponential distribution (functions gof.date() and gofExp.test,
respectively) can be grouped into the ﬁnal category.
7.4
Empirical applications of EVT
7.4.1
Section outline
The extreme value theory will now be applied to ﬁnancial data. It will be shown
how the methods and techniques presented above can be applied to the modelling of
market risks. In particular, extreme value models using the block maxima, POT and
point process techniques are ﬁtted to daily stock returns.
The empirical applications begin with an exploratory data analysis of the stock
returns of Siemens. This series will be used subsequently for applying the block
maxima method to this data set. In the following examples the r block maxima
approach is applied to the losses of the BMW stock and the POT to the losses of
the Boeing stock. In the last example the sensitivity of the de-clustering technique
is investigated for the losses of the NYSE index. It will be shown in these examples
how the different implementations in the packages evir, ismev and fExtremes can
be utilized.

EXTREME VALUE THEORY
99
7.4.2
Block maxima model for Siemens
In this subsection the block maxima method is applied to the daily losses of the
Siemens stock. This data set was introduced in Section 3.1. The ﬁtting of the
GEV to this data set, as well as inference and diagnostic tests, are shown in
Listing 7.1.
First, the daily returns are converted to positive loss ﬁgures expressed as per-
centages. Next, this series is ﬁtted against the GEV distribution by employing the
function gev() of the package evir. The function’s argument block has been set to
‘semester’. This will extract the biannual maxima of the series if the series contains
a time attribute with time stamps of class POSIXct or of a class that can be coerced
to it. The data points extracted are shown in Figure 7.2. The ﬁnding of an increased
volatility during the second half of the sample period is mirrored in this graph. Hence,
the assumption of identically distributed block maxima might be violated. However,
Listing 7.1 Block maxima for the losses of Siemens.
library(evir)
1
data(siemens)
2
## Losses
3
SieLoss <−−100.0 ∗siemens
4
## package evir:
5
SieGEV <−gev(SieLoss, block = "semester")
6
SieGEV
7
plot(SieGEV$data, type = "h", col = "blue", xlab = " ",
8
ylab = "Block Maxima",
9
main = "Maximum Biannual Losses of Siemens")
10
## package ismev:
11
library(ismev)
12
SieGEV2 <−gev.ﬁt(SieGEV$data)
13
SieGEV2
14
gev.diag(SieGEV2)
15
par(mfrow = c(2, 1))
16
gev.prof(SieGEV2, m = 20, xlow = 5, xup = 16, conf = 0.95)
17
gev.profxi(SieGEV2, xlow = 0.0, xup = 0.7, conf = 0.95)
18
mLoss <−max(SieGEV$data)
19
mYears <−1 / (1 −pgev(mLoss, mu = SieGEV2$mle[1],
20
sigma = SieGEV2$mle[2],
21
xi = SieGEV2$mle[3])) / 2
22
## package fExtremes:
23
library(fExtremes)
24
SieGEV3 <−gevFit(SieGEV$data, type = "pwm")
25
SieGEV3
26

100
RISK MODELLING
0
10
20
30
40
2
4
6
8
10
12
Maximum Biannual Losses of Siemens
Block Maxima
Figure 7.2
Block maxima for Siemens losses.
for the time being we will neglect this potential violation and address it again later,
when a trend as a covariate is included in the model.
The ML estimation results are provided in Table 7.1. All coefﬁcients are sig-
niﬁcantly different from zero. The estimate of the shape parameter, ˆξ, implies the
existence of heavy tails and non-ﬁnite losses – that is, the GEV is of the Fr´echet type.
Alternatively, the block maxima data could be ﬁtted by applying the ML principle
to the GEV with the function gev.fit() of the package ismev. The parameter
estimates are equal to those returned by gev(). Diagnostic plots can be swiftly
produced with the function gev.diag(), as shown in Figure 7.3. As indicated by
the probability and quantile plots, data points in the far right tail are not captured
well by this model speciﬁcation. This can be attributed to the relatively small losses
witnessed during the beginning of the sample period. This artefact shows up in
the return level plot. For data points in the far right tail the estimated return levels
systematically fall short compared to the empirical levels, though the latter stayed
within the 95% conﬁdence bands.
In addition these diagnostic plots, further inference from the model can be made
using the proﬁle log-likelihoods. Figure 7.4 shows those for a 10-year return level
(upper panel) and for the shape parameter (lower panel). A daily loss as high as 7.6%
Table 7.1
Fitted GEV to block maxima of Siemens.
GEV
ξ
σ
μ
Estimate
0.287
1.047
2.700
Standard Error
0.117
0.141
0.170

EXTREME VALUE THEORY
101
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
Probability Plot
Empirical
Model
2
4
6
8
10
2
6
10
Quantile Plot
Model
Empirical
1e−01
1e+01
1e+03
5
15
25
Return Period
Return Level
Return Level Plot
Density Plot
z
f(z)
0
2
4
6
8
10
14
0.0
0.2
Figure 7.3
Diagnostic plots for ﬁtted GEV model.
would be observed once every 10 years. This point estimate falls within in a 95%
conﬁdence level ranging from 6% to 11.75%. Hence, the maximum observed loss
of 12.01% would not have been covered as a ‘once in every 10 years’ event, but
rather this loss would occur only once every 42 years or so. The relevant computation
for this time span is included in Listing 7.1 (see lines 20–22). In the lower panel of
Figure 7.4 the proﬁle log-likelihood for the shape parameter is shown with a 95%
conﬁdence band (the horizontal blue lines) superimposed. As can be clearly seen, the
conﬁdence band is asymmetric and to the right for the point estimate of ˆξ = 0.287.
A value of almost 0.6 would be covered by this conﬁdence band.
In the next lines of the R code listing, the parameters of the GEV are determined
by the PWM methods. The estimates are fairly close to the ML estimates. The shape,
scale and location parameter take the values ˆξ = 0.319, ˆσ = 1.001 and ˆμ = 2.675,
respectively.
7.4.3
r block maxima model for BMW
We will now ﬁt an r block maxima model to the daily losses of BMW. Here, the two
largest annual losses are extracted from the data set. The return series is contained
in the package evir and the functions for ﬁtting and diagnosing models of this type
are provided in the package ismev, namely rlarg.fit() and rlarg.diag(),
respectively. The time span and structure of the data object bmw is the same as for
siemens.
In Listing 7.2 the above-mentioned packages are loaded ﬁrst. Next, the percentage
losses for BMW stock are computed as positive ﬁgures (i.e., object BmwLoss). The

102
RISK MODELLING
6
8
10
12
14
16
−92
−86
Return Level
 Profile Log−likelihood
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
−90
−86
Shape Parameter
Profile Log−likelihood
Figure 7.4
Proﬁle log-Likelihood plots for ﬁtted GEV model.
Listing 7.2 r block maxima for BMW losses.
## Loading of packages
1
library(evir)
2
library(ismev)
3
## Order statistics
4
data(bmw)
5
BmwLoss <−−1.0 ∗bmw ∗100
6
Years <−format(attr(BmwLoss, "time"), "%Y")
7
attr(BmwLoss, "years") <−Years
8
Yearu <−unique(Years)
9
idx <−1:length(Yearu)
10
r <−2
11
BmwOrder <−t(sapply(idx, function(x)
12
head(sort(BmwLoss[attr(BmwLoss, "years") ==
13
Yearu[x]], decreasing = TRUE), r)))
14
rownames(BmwOrder) <−Yearu
15
colnames(BmwOrder) <−paste("r", 1:r, sep = " ")
16
## Plot of order data
17
plot(Yearu, BmwOrder[, 1], col = "black", ylim = range(BmwOrder),
18
ylab = "Losses BMW (percentages)", xlab = " ",
19
pch = 21, bg = "black")
20
points(Yearu, BmwOrder[, 2], col = "grey", pch = 23, bg = "grey")
21
## Fit and diagnostics
22
BmwOrderFit <−rlarg.ﬁt(BmwOrder)
23
rlarg.diag(BmwOrderFit)
24

EXTREME VALUE THEORY
103
1975
1980
1985
1990
1995
2
4
6
8
10
12
14
Losses BMW (percentages)
Figure 7.5
Two largest annual losses of BMW.
object bmw has a time attribute that is of class POSIXct. In order to derive the two
largest annual losses of the series and prepare a matrix object suitable for ﬁtting,
a series of unique years is recovered from the time attribute by employing the
format() method for objects of this kind. The resulting object is added as attribute
years to the object BmwLoss. The unique years are stored as object Yearu and an
index idx is created equal to the length of this item. Next, the order r is set to 2,
and this is then used to retrieve the two largest losses per year. Here, the losses per
year are sorted in decreasing order and the ﬁrst r values are extracted. The resulting
object BmwOrder is of class matrix and has a column dimension of 2 for the two
largest observed losses and a row dimension of 24 which coincides with the annual
time span from 1973 to 1996. The losses retrieved are plotted in Figure 7.5.
As is evident from the plot, the second largest observed loss is not in all years
of similar magnitude to the largest annual loss. In particular, the differences are
quite marked in the years 1989 and 1991, at 11.357 and 6.249 percentage points,
respectively.
Having prepared the data in a format suitable for ﬁtting, the GEV is ﬁtted to the
r block maxima data. The estimation results are provided in Table 7.2. All estimates
are signiﬁcantly different from zero, but for ξ only at the 10% level. The sign of
Table 7.2
Fitted r block maxima of BMW.
GEV
μ
σ
ξ
Estimate
4.480
1.752
0.303
Standard error
0.341
0.289
0.159

104
RISK MODELLING
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
Probability Plot
Empirical
Model
4
6
8
10
12
14
2
6
10
14
Quantile Plot
Model
Empirical
1e−01
1e+01
1e+03
10
30
Return Period
Return Level
Return Level Plot
Density Plot
z
f(z)
2
4
6
8
10
14
0.00
0.10
0.20
Figure 7.6
Diagnostic plots for r block maxima: largest losses.
the parameter value indicates a Fr´echet-type distribution, which implies the potential
occurrence of unbounded losses.
The goodness of ﬁt can be inspected visually with the function rlarg.diag().
This function returns two graphics. First, plots for the largest losses are produced
similar to the function gev.diag(), and then PP and QQ plots for r = 1, 2 are
returned. These diagnostic tools are shown in Figures 7.6 and 7.7, respectively.
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
k=1
4
6
8
10
12
14
2
6
10
14
k=1
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
k=2
3
4
5
6
7
2
4
6
8
k=2
Figure 7.7
Diagnostic plots for r block maxima: PP and QQ plots.

EXTREME VALUE THEORY
105
Overall the PP plot indicates a decent ﬁt. However, the QQ plot reveals that losses
in the range from roughly 8% to 10% are not covered so well. This is also reﬂected in
the return level plot. Here, the values for a 1-year return period are at the boundary of
the upper conﬁdence band. The upward sloping return level curve reiterates that the
GEV is of Fr´echet type. The relatively large estimated standard error for the shape
parameter is mirrored by quite sharply widening conﬁdence bands for higher return
periods, alerting the researcher to the vagaries of extrapolating too far into the tails
of the distribution.
Figure 7.7 shows the PP and QQ plots for each of the two orders. As should be
expected from the descriptive analysis of this data set above, the results for k = 2
look less favourable in terms of the QQ plot (lower right-hand panel).
7.4.4
POT method for Boeing
In this subsection the GPD distribution is ﬁtted to the daily losses of the Boeing
stock by utilizing the POT method. The closing prices for this stock are contained
in the object DowJones30 which is part of the package fEcoﬁn (see W¨urtz 2009a).
The sample runs from 31 December 1990 to 2 January 2001 and comprises 2529
observations.
In Listing 7.3 the necessary packages are loaded ﬁrst. The package fExtremes
will be used to conduct the analysis. Next, the data set is loaded into the work space
and is converted to a timeSeries object. The daily losses of Boeing are computed
and expressed as percentage ﬁgures. The resulting object has been termed BALoss.
Listing 7.3 POT GPD for Boeing losses.
## Loading of packages
1
library(fEcoﬁn)
2
library(fExtremes)
3
## Data handling
4
data(DowJones30)
5
DJ <−timeSeries(DowJones30[, −1],
6
charvec = as.character(DowJones30[, 1]))
7
BALoss <−−1.0 ∗returns(DJ[, "BA"], percentage = TRUE,
8
trim = TRUE)
9
## MRL plot
10
mrlPlot(BALoss, umin = −10, umax = 10)
11
## GPD
12
BAFit <−gpdFit(BALoss, u = 3)
13
## Diagnostic plots
14
plot(BAFit)
15
## Risk measures
16
gpdRiskMeasures(BAFit, prob = c(0.95, 0.99, 0.995))
17

106
RISK MODELLING
−10
−5
0
5
10
0
2
4
6
8
10
Threshold u
Mean Excesses
Figure 7.8
MRL plot for Boeing losses.
To apply the POT method a suitable threshold value must be determined. This
choice can be guided by the MRL plot which is shown in Figure 7.8. In this graph 95%
conﬁdence bands around the mean excesses have been superimposed and are coloured
light grey. Unfortunately, and this is often encountered empirically, a deﬁnite choice
for the threshold value can hardly be deduced from this kind of plot. However, it
seems reasonable to assume a daily loss as high as 3% as threshold value, given that a
linear relationship exists between the plotted thresholds and the mean excesses above
this value. In principle, a threshold slightly higher or lower than 3% could be chosen,
but then there is a trade-off between greater uncertainty for the estimates and bias.
For the given threshold a total of 102 exceedances result. This data set corresponds
roughly to the upper 96% quantiles of the empirical distribution function.
Having ﬁxed the threshold value, the GPD can be ﬁtted to the exceedances. This
is accomplished with the function gpdFit(). The estimates for the scale and shape
parameters as well as their estimated standard deviations are shown in Table 7.3. The
shape parameter is greater than zero and signiﬁcantly different from one, indicating
heavy tails.
In addition to the estimate, the appropriateness of the ﬁtted model can be inves-
tigated graphically by means of the plot() method for objects of class fGPDFIT.
Table 7.3
Fitted GPD of Boeing.
GPD
ξ
β
Estimate
0.331
1.047
Standard error
0.128
0.166

EXTREME VALUE THEORY
107
5
10
20
0.0
0.4
0.8
Excess Distribution
Fu(x−u)
x [log Scale]
5
10
20
5e−05
2e−03
Tail of Underlying Distribution
x [log scale]
1−F(x) [log scale]
0
20
40
60
80
100
0
2
4
Scatter Plot of Residuals
Ordering
Residuals
0
1
2
3
4
5
0
2
4
QQ−Plot of Residuals
Ordered Data
Exponential Quantiles
Figure 7.9
Diagnostic plots of ﬁtted GPD model.
Figure 7.9 shows the diagnostic plots obtained. The upper panels show the ﬁtted
excess distribution and a tail plot. Both indicate a good ﬁt of the GPD to the ex-
ceedances. The lower panels display the residuals with a ﬁtted ordinary least-squares
(OLS) line on the left and a QQ plot on the right. Neither plot gives cause for concern
as to a size effect, that is to say, the OLS line stays fairly ﬂat and in the QQ plot the
points plotted do not deviate much from the diagonal.
Finally, point estimates for the VaR and ES risk measures can be swiftly computed
with the function GPDRiskMeasures(). In Listing 7.3 these are computed for the
95%, 99% and 99.5% levels. The results are shown in Table 7.4. These measures
would qualify as unconditional risk assessments for the next business day.
In the example above it was implicitly assumed that the exceedances are i.i.d.
data points. However, this assumption is barely tenable for ﬁnancial market returns.
In particular, given the validity of the stylized facts, returns large in absolute value are
clustered with respect to time and this empirical characteristic also holds cum grano
Table 7.4
Risk measures for Boeing.
Conﬁdence level
VaR
ES
95.0%
2.783
4.240
99.0%
4.855
7.336
99.5%
6.149
9.268

108
RISK MODELLING
Listing 7.4 De-clustering of NYSE exceedances.
library(fExtremes)
1
library(fEcoﬁn)
2
data(nyse)
3
NYSELevel <−timeSeries(nyse[, 2],
4
charvec = as.character(nyse[, 1]))
5
NYSELoss <−na.omit(−1.0 ∗diff(log(NYSELevel)) ∗100)
6
colnames(NYSELoss) <−"NYSELoss"
7
## Point process data
8
NYSEPP <−pointProcess(x = NYSELoss, u = quantile(NYSELoss, 0.95))
9
## Declustering
10
DC05 <−deCluster(x = NYSEPP, run = 5, doplot = FALSE)
11
DC10 <−deCluster(x = NYSEPP, run = 10, doplot = FALSE)
12
DC20 <−deCluster(x = NYSEPP, run = 20, doplot = FALSE)
13
DC40 <−deCluster(x = NYSEPP, run = 40, doplot = FALSE)
14
DC60 <−deCluster(x = NYSEPP, run = 60, doplot = FALSE)
15
DC120 <−deCluster(x = NYSEPP, run = 120, doplot = FALSE)
16
## Fit of declustered data
17
DC05Fit <−gpdFit(DC05, u = min(DC05))
18
DC10Fit <−gpdFit(DC10, u = min(DC10))
19
DC20Fit <−gpdFit(DC20, u = min(DC20))
20
DC40Fit <−gpdFit(DC40, u = min(DC40))
21
DC60Fit <−gpdFit(DC60, u = min(DC60))
22
DC120Fit <−gpdFit(DC120, u = min(DC40))
23
salis for losses that exceed a high value chosen as threshold. The focus is now shifted
to the time domain of the exceedances, called marks, rather than the actual values.
The issue raised here is exempliﬁed in Listing 7.4 by employing the daily returns of
the New York Stock Exchange Index.
First, the necessary packages fEcoﬁn and fExtremes are loaded into the work
space. Next, the daily continuous losses of the stock index are computed and expressed
as percentages. The data for exceedances above the 95% percentile are recovered
from the losses with the function pointProcess(). This function returns an object
with the time stamps and marks, that is, the losses above the threshold. For an i.i.d.
point process the time gaps between consecutive exceedances should be Poisson
distributed. This can easily be checked by means of an appropriate QQ plot. Another
graphical means to detect departure from the i.i.d. assumption is to portray the ACF
and/or PACF of the gaps. If the exceedances occurred randomly during the sample
period, the gaps should not be autocorrelated. Such graphs are shown in Figure 7.10.
All four graphs indicate that the exceedances cannot be assumed to be i.i.d. The
upper left panel shows the point process. It is fairly evident from this graph that
the exceedances are clustered. This is mirrored by the QQ plot in the upper right

EXTREME VALUE THEORY
109
Losses
1966−05−05
1988−04−30
5
15
0
5
10
15
20
0
2
4
6
0
5
10
15
20
25
0.0
0.4
0.8
ACF
0
5
10
15
20
25
−0.10
0.10
0.25
Partial ACF
Figure 7.10
Plots for clustering of NYSE exceedances.
panel. Here the time gaps between consecutive marks are plotted against a Poisson
distribution. The scatter points deviate considerably from the ﬁtted line. The non-
randomness of the occurrences is also reﬂected by the ACF and PACF plot in the
lower panel. The ACF tapers off only slowly and the PACF indicates a signiﬁcant
second-order autocorrelation.
As a means of data preprocessing, the exceedances can be de-clustered. In other
words, only the maximum is recovered within a cluster of exceedances as the repre-
sentative extreme loss. Hence, the de-clustered exceedances are at least the assumed
width of the cluster apart from each other with respect to time. The aim of this data
preprocessing technique is to ensure the validity of the GPD assumptions. Note that
the results are sensitive to the choice of number of runs. To highlight this issue, de-
clustered series have been retrieved for various periodicities in Listing 7.4. That is, the
point process data have been de-clustered for weekly, biweekly, monthly, bimonthly
and quarterly as well as half-yearly runs. Next, the GPD is ﬁtted to these series and
the results are provided in Table 7.5.
In this table the estimates for ξ and β are reported. The estimated standard errors
are reported in square brackets. In the two columns to the right the numbers of de-
clustered exceedances and the values of the negative log-likelihood are provided.
The estimates for the shape parameter increase with run frequency (as one moves
down the table), given that each run frequency is a valid choice. However, these point
estimates become less reliable due to the decreased number of observations available
for minimizing the negative log-likelihood, though the values for the latter do pick up
for longer run periods. With respect to the derivation of VaR and/or ES these results
imply quite different assessments of the riskiness inherent in the NYSE index. In
particular, the point processes for the monthly, bimonthly, quarterly and half-yearly
de-clustered series do not indicate that the their time gaps depart to a large extent
from the exponential distribution, but the inference drawn from the ﬁtted models does
differ markedly.

110
RISK MODELLING
Table 7.5
Results for de-clustered GPD models.
De-cluster
ˆξ
ˆβ
Exceedances
NLLH
Weekly
0.252
0.536
303
189.9
[0.066]
[0.046]
Biweekly
0.324
0.518
222
147.5
[0.084]
[0.055]
Monthly
0.33
0.59
149
118.76
[0.102]
[0.076]
Bimonthly
0.376
0.752
86
92.67
[0.146]
[0.133]
Quarterly
0.421
0.824
61
73.59
[0.181]
[0.178]
Half-yearly
0.581
0.928
25
37.66
[0.342]
[0.352]
References
Beirlant J., Goetgebeur Y., Segers J. and Teugels J. 2004 Statistics of Extremes: Theory and
Applications. John Wiley & Sons, Ltd, Chichester.
Coles S. 2001 An Introduction to Statistical Modeling of Extrem Values. Springer-Verlag,
London.
Coles S. and Stephenson A. 2011 ismev: An introduction to statistical modeling of extreme
values. R package version 1.37.
Cunnane C. 1979 A note on the Poisson assumption in partial duration series model. Water
Resource Research 15(2), 489–494.
Deville Y. 2012 Renext: Renewal method for extreme values extrapolation. R package version
2.0-0.
Embrechts P., Kl¨uppelberg C. and Miksoch T. 1997 Modelling Extremal Events for Insurance
and Finance vol. 33 of Stochastic Modelling and Applied Probability. Springer-Verlag,
Berlin.
Ferro C. and Segers J. 2003 Inference for clusters of extreme values. Journal of the Royal
Statistical Society, Series B 65(2), 545–556.
Gilleland E. and Katz R. 2011 New software to analyze how extremes change over time. Eos
92(2), 13–14.
Leadbetter M., Lindgren G. and Rootz´en, H. 1983 Extremes and Related Properties of Random
Sequences and Series. Springer-Verlag, New York.

EXTREME VALUE THEORY
111
McNeil A. 1999 Extreme value theory for risk managers Internal Modelling and CAD II RISK
Books pp. 93–113.
McNeil A. and Stephenson A. 2011 evir: Extreme values in R. R package version 1.7-2.
McNeil A., Frey R. and Embrechts P. 2005 Quantitative Risk Management: Concepts, Tech-
niques and Tools. Princeton University Press, Princeton, NJ.
Plummer M., Best N., Cowles K. and Vines K. 2006 CODA: Convergence diagnosis and output
analysis for MCMC. R News 6(1), 7–11.
Ribatet M. 2011 POT: Generalized Pareto Distribution and Peaks Over Threshold. R package
version 1.1-1.
Stephenson A. 2002 evd: Extreme value distributions. R News 2(2), 31–32.
Stephenson A. and Ribatet M. 2010 evdbayes: Bayesian analysis in extreme value theory.
R package version 1.0-8.
W¨urtz D. 2009a fEcoﬁn: Economic and ﬁnancial data sets. R package version 290.76.
W¨urtz D. 2009b fExtremes: Rmetrics – extreme ﬁnancial market data. R package version
2100.77.

8
Modelling volatility
8.1
Preliminaries
The previous two chapters introduced quantitative methods for risk modelling in the
case of non-normally distributed returns, that is, the extreme value theory and the
generalized hyperbolic and generalized lambda distribution classes. The ﬁrst method
addresses the tail modelling of a return process, whereas the second focuses on
adequately capturing the entire distribution. With respect to the value-at-risk and
expected shortfall risk measures it was assumed that the ﬁnancial market returns are
i.i.d. Hence, these risk measures are unconditional in the sense that these measures do
not depend on prior information. As already shown in Chapter 3, volatility clustering
is one of the stylized facts of ﬁnancial market returns. Given this stylized fact, the
assumption of i.i.d. returns is clearly violated. Therefore, this chapter introduces a
model class that takes volatility clustering explicitly into account. As will be shown,
conditional risk measures can be deduced from these models. Here the phenomenon
of volatility clustering directly feeds into the derived risk measures for future periods
in time.
8.2
The class of ARCH models
The class of autocorrelated conditional heteroscedastic (ARCH) models was intro-
duced in the seminal paper by Engle (1982). This type of model has since been
modiﬁed and extended in several ways. The articles by Engle and Bollerslev (1986),
Bollerslev et al. (1992) and Bera and Higgins (1993) provide an overview of the
model extensions during the decade or so after the original paper. Today ARCH mod-
els are not only well established in the academic literature but also widely applied in
the domain of risk modelling. In this section the term ‘ARCH’ will be used both for
the speciﬁc ARCH model and for its extensions and modiﬁcations.
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

MODELLING VOLATILITY
113
The starting point of ARCH models is an expectations equation which only
deviates from the classical linear regression with respect to the assumption of inde-
pendent and identically normally distributed errors:
yt = x′
tβ + εt,
(8.1)
εt|t−1 ∼N(0, ht).
(8.2)
The error process εt is assumed to be normally distributed, but the variance is not
constant, being allowed to change over time. This variability of the variance is
expressed as dependence with respect to the available information at time t −1,
denoted by t−1. An alternative notation is commonly encountered in the literature:
εt = ηt

ht,
(8.3)
ηt ∼Dν(0, 1),
(8.4)
where ηt denotes a random variable with distribution D with expected value zero
and unit variance. Additional parameters of this distribution are subsumed in the
parameter vector ν. This notation is more general in the sense that now a normally
distributed error process is no longer required, but distributions with non-zero excess
kurtosis and/or skewness can be interspersed. In the literature one often encounters
the Student’s t distribution, the skewed Student’s t distribution or the generalized
error distribution.
The second building block of ARCH models is the variance equation. In an ARCH
model of order q the conditional variance is explained by the history of the squared
errors up to time lag q:
ht = α0 + α1ε2
t−1 + . . . + αqε2
t−q,
(8.5)
where α0 > 0 and αi ≥0, i = 1, . . . , q. These parameter restrictions guarantee a
positive conditional variance. The inclusion of the available information up to time
t −1 is evident from
εt−i = yt−i −x′
t−iβ,
i = 1, . . . , q.
(8.6)
It can already be deduced why this model class can capture the stylized fact of
volatility clustering: the conditional variance is explained by the errors in previous
periods. If these errors are large in absolute value, a large value for the conditional
variance results, and vice versa. By way of illustration, Figure 8.1 shows the plots
of a white noise and simulated ARCH(1) and ARCH(4) processes. A clear pattern
of volatility clustering is evident for the ARCH(1) and ARCH(4) processes which
is more pronounced for the latter. One might be tempted to conclude that the two
ARCH processes are more volatile than the white noise. However, all processes have
been simulated to have unit unconditional variances. For an ARCH(1) process the
unconditional variance is given by
σ 2
ε = E(ε2
t ) =
α0
1 −α(1),
(8.7)

114
RISK MODELLING
0
1000
2000
3000
4000
5000
−10
0
5
White Noise
0
1000
2000
3000
4000
5000
−10
0
5
ARCH(1)
0
1000
2000
3000
4000
5000
−10
0
5
ARCH(4)
Figure 8.1
Plots of simulated white noise, ARCH(1) and ARCH(4) processes.
where α(1) represents the sum of the coefﬁcients for the lagged squared errors. The
processes in Figure 8.1 have been generated according to ht = 0.1 + 0.9ε2
t−1 and ht =
0.1 + 0.36ε2
t−1 + 0.27ε2
t−2 + 0.18ε2
t−3 + 0.09ε2
t−4 for the ARCH(1) and ARCH(4)
models, respectively. Hence, for both models the unconditional variance equals unity.
Note that the fourth moment (i.e., the kurtosis) for ARCH(1) models,
E(ε4
t )
σ 4ε
= 3
 1 −α2
1
1 −3α2
1

,
(8.8)
is greater than 3 and thus has more probability mass in the tails than the normal
distribution. This is further evidence that with ARCH models the stylized facts of
ﬁnancial market returns can be captured well.
Having discussed the baseline ARCH model, the focus is now shifted to its
modiﬁcation and extensions. Bollerslev (1986) introduced the GARCH(p, q) model
into the literature. This differs from the ARCH model in the inclusion of lagged
endogenous variables in the variance equation – that is, now the conditional variance
depends not only on past squared errors but also on lagged conditional variances:
ht = α0 + α1ε2
t−1 + . . . + αqε2
t−q + β1ht−1 + . . . + βpht−p,
(8.9)
with the restrictions α0 > 0, αi ≥0 for i = 1, . . . , q and βi ≥0 for j = 1, . . . , p
such that the conditional variance process is strictly positive. The advantage of
this approach is that a GARCH model is the same as an ARCH model with an
inﬁnite number of lags, if the roots of the lag polynomial 1 −β(z) lie outside the
unit circle, hence a more parsimonious speciﬁcation is possible when GARCH-type

MODELLING VOLATILITY
115
speciﬁcations are used. The unconditional variance for a GARCH(1, 1) model is
given by
σ 2
ε = E(ε2
t ) =
α0
1 −α1 −β1
.
From this, the condition for a stable variance process can be directly deduced, namely,
α1 + β1 < 1.
So far it has been assumed that the sign of the shock does not have an impact on the
conditional variance. This is because the past errors enter as squares into the variance
equation. Nelson (1991) extended the class of ARCH to make it possible to take
this effect into account. He proposed the class of exponential GARCH (EGARCH)
models to capture such asymmetries. The modelling of asymmetric effects can be
justiﬁed from an economic point of view with leverage effects, in particular when
equity returns are investigated. For these, a negative relationship between volatility
and past returns is postulated (see Black 1976). The variance equation now takes the
form
log(ht) = α0 +
q

i=1
αig(ηt−i) +
p

j=1
β j log(ht−j),
(8.10)
where the previously introduced alternative speciﬁcation of ARCH models is em-
ployed and the function g(ηt) is deﬁned as:
g(ηt) = θηt + γ [|ηt| −E(|ηt|)].
(8.11)
This approach has some peculiarities worth mentioning. First, a multiplicative re-
lationship for explaining the conditional variances is assumed. This becomes evident
from the logarithmic form for the variance equation. Hence, there is no need for non-
negativity constraints on the parameter space, αi, i = 1, . . . , q, and β j, j = 1, . . . , p,
because ht = exp (·) will always be positive. Second, the impact of the error vari-
ables is piecewise linear and takes for a positive shock a value of αi(θ + γ ) and
for a negative one αi(θ −γ ). Here, the sign-dependent impact of past errors with
respect to the contemporaneous conditional variance shows up. The ﬁrst term, g(ηt),
in the equation depicts the correlation between the error process ηt and the future
conditional volatility. The ARCH effects themselves are captured by the coefﬁcients
γ . The greater the deviation of the variable ηt from its expected value, the higher the
value of the function g(ηt) and hence of the log-value for the conditional variance.
The last of the many extensions of the ARCH model to be discussed is the
asymmetric power ARCH (APARCH) proposed by Ding et al. (1993). The reason
for this choice is that the APARCH model encompasses other ARCH speciﬁcations,
as will be shown below. Its variance equation is deﬁned as
εt = ηtht,
(8.12)
ηt ∼Dν(0, 1),
(8.13)
hδ
t = α0 +
q

i=1
αi(|εt−i| −γiεt−i)δ +
p

j=1
β jhδ
t−j,
(8.14)

116
RISK MODELLING
with parameter restrictions α0 > 0, δ ≥0, αi ≥0, i = 1, . . . , q, −1 < γi < 1, i =
1, . . . , q, and βi ≥0, i = 1, . . . , p. One peculiarity of this approach for modelling
the conditional variance is the exponent δ. For δ = 2 the conditional variance results,
similarly to the models discussed above. But the parameter is deﬁned for non-negative
values in general and hence the long memory characteristic often encountered for
absolute and/or squared daily return series can be taken explicitly into account. The
long memory characteristic can be described simply by the fact that for absolute and/or
squared returns the autocorrelations taper off only slowly and hence dependencies
exist between observations that are further apart. Potential asymmetries with respect
to positive/negative shocks are reﬂected in the coefﬁcients γi, i = 1, . . . , q. The
APARCH model includes the following special cases:
r ARCH model, if δ = 2, γi = 0 and β j = 0;
r GARCH model, if δ = 2 and γi = 0;
r TS-GARCH (see Schwert 1990; Taylor 1986), if δ = 1 and γi = 0;
r GJR-GARCH (see Glosten et al. 1993), if δ = 2;
r T-ARCH (see Zakoian 1994), if δ = 1;
r N-ARCH (see Higgins and Bera 1992), if γi = 0 and β j = 0;
r log-ARCH (see Geweke 1986; Pentula 1986), if δ →0.
The reader is referred to the citations above for further details on these special cases.
Estimates for the unknown model parameters are often obtained by applying
the maximum likelihood or quasi-maximum likelihood principle. Here, numerical
optimization techniques are applied. Less commonly encountered are Bayesian esti-
mation techniques for the unknown parameter in applied research. Whence a model ﬁt
is obtained, forecasts for the conditional variances can be computed recursively and
the desired risk measures can be deduced together with the quantiles of the assumed
distribution for the error process from these.
8.3
Synopsis of R packages
8.3.1
The package bayesGARCH
The package bayesGARCH implements the Bayesian estimation of GARCH(1, 1)
models with Student’s t innovations (see Ardia 2011). The package is contained in
the CRAN ‘Bayesian’, ‘Finance’ and ‘TimeSeries’ Task Views. It has dependen-
cies on the packages mvtnorm and coda. The latter is used to generate MCMC
objects which can be utilized to form the joint posterior sampler. Furthermore,
bayesGARCH interfaces routines written in C for recursively computing condi-
tional variances. The package is shipped with a NAMESPACE ﬁle in which the two
cornerstone functions bayesGARCH() and formSmpl() are exported. It includes a

MODELLING VOLATILITY
117
demo ﬁle in which the proper usage of the functions is exempliﬁed as well as a data
set of the Deutschmark/Sterling (DEM/GBP) continuous returns.
Within the call to bayesGARCH(), the priors for the coefﬁcients and the associated
covariance matrices can be speciﬁed. In addition, the hyperparameters for either the
translated exponential distribution or the degrees of freedom parameter ν of the t
distribution can be provided. If the values for these are set to reasonably high values
a model with normal innovations is obtained at the limit. Finally, the estimation can
be inﬂuenced by providing a list element for the control argument of the function.
Here, the number of Markov chains, the length of the chains, starting values, prior
conditions as well as list elements that determine the shape of the estimation report
(frequency thereof and number of digits to be printed) can be supplied. The prior
conditions have to be provided in the form of a function that returns the logical values
TRUE/FALSE.
8.3.2
The package ccgarch
This package is one of three in which multivariate GARCH models can be dealt
with. In particular, the conditional correlation approach to multivariate GARCH is
implemented (see Nakatani 2010). The package is contained in the CRAN ‘Finance’
Task View. It is shipped with a NAMESPACE ﬁle. The package itself is a collection
of functions and does not employ either S3 or S4 methods or classes. Ordinarily
the functions return list objects and their elements can then be used as functional
arguments in other calls. The computationally burdensome estimation routines are
interfaced from C code.
The unknown parameters of a CC-GARCH model are estimated by utilizing
the function dcc.estimation(). Within this function the two-step estimation
procedure is carried out. Each of these estimators can also be accessed and
employed on a stand-alone basis through the functions dcc.estimation1()
and dcc.estimation2(). These functions return list objects and the ele-
ments thereof can then be used in subsequent analysis. For instance, robust
standard errors for the model’s parameters can be returned with the function
dcc.results(), by providing the necessary arguments explicitly. Similarly, the
values of the log-likelihoods can be computed with the functions loglik.dcc(),
loglik.dcc1() and loglik.dcc2(), for the CC-GARCH model, the ﬁrst-stage
and the second-stage optimization, respectively. The stationarity condition of an
empirical CC-GARCH model can be checked by utilizing the function sta-
tionarity(). For diagnostic testing, the causality tests proposed by Hafner and
Herwatz (hh.test()) and by Nakatani and Ter¨asvirta (nt.test()) are imple-
mented. The validity of the normality assumption can be assessed with the function
jb.test(). Routines are included in ccgarch for simulating data according to pre-
speciﬁed CC-GARCH parameterizations, namely dcc.sim() and eccc.sim().
The innovations can follow either the normal or Student’s t distribution. The
package lacks a routine for obtaining forecasts from CC-GARCH type models,
however.

118
RISK MODELLING
8.3.3
The package fGarch
The package fGarch is part of the Rmetrics suite of packages (see W¨urtz and Chalabi
2009). It is contained in the CRAN ‘Finance’ and ‘TimeSeries’ Task Views and is
considered a core package in the former. This package is the broadest implementation
of univariate ARCH models and the extensions thereof. It interfaces FORTRAN
routines for the more computationally burdensome calculations. Within the package
S4 methods and classes are utilized. As a technicality, a unit testing framework based
upon the package RUnit (Burger et al. 2010) is implemented.
The cornerstone function for ﬁtting ARCH-type models is garchFit(). Within
this function the mean and variance models to be used are speciﬁed in the form of
a formula. Currently, pure GARCH/APARCH models can be supplied in which
the mean equation is a simple intercept or more general speciﬁcations such as
ARMA/GARCH or ARMA/APARCH models can be estimated. The innovations
can be distributed according to the normal, Student’s t, the skewed Student’s t,
the GEV or the skewed GEV. The maximization of the log-likelihood is conducted
by means of numerical optimizers. Here, the user can choose between nlminb,
lbfgsb, nlminb+nm and lbfgsb+nm, whereby in the latter two cases the Nelder–
Meade optimizer is employed in a ﬁrst stage. Null restrictions can be superimposed
for the more complex ARCH-type models by employing include directives as
logical ﬂags for these coefﬁcients. The function returns an object of formal class
fGARCH. For objects of this kind coef(), fitted(), formula(), initialize(),
plot(), predict(), residuals(), show(), summary() and update() meth-
ods have been deﬁned. Data for univariate ARCH-type models can be generated
with the function garchSim(). Here, a particular model can be speciﬁed with the
function garchSpec() and the object returned can then be used in calls to this
function.
8.3.4
The package gogarch
The package gogarch (Pfaff 2012) implements the generalized orthogonal GARCH
(GOGARCH) model, a multiple GARCH model proposed by van der Weide (2002)
and Boswijk and van der Weide (2006, 2009). The package is contained in the CRAN
‘Finance’ and ‘TimeSeries’ Task Views. It utilizes formal S4 classes and methods
and is written purely in R.
The cornerstone function in the package is gogarch(). This function initial-
izes the model and its speciﬁcation and carries out the estimation according to the
methods chosen. The unknown parameters of a GOGARCH model can be estimated
by maximum likelihood, non-linear least squares, the method of moments or by ap-
plying an independent component analysis via the fastICA() function contained
in the package of the same name. When the unknown model parameters have been
estimated the user can then proceed by employing the methods shown in Table 8.1.
The availability of the methods is partly dependent on the estimation technique cho-
sen. However, regardless of this, the user can not only retrieve the ﬁtted values,
residuals, coefﬁcients and covariances and/or correlations but also obtain a summary

Table 8.1
Overview of package gogarch.
Classes
Methods
Goinit
GoGARCH
Goestica
Goestmm
Goestml
Goestnls
Gosum
Gopredict
Orthom
angles
×
ccor
×
×
×
×
×
×
ccov
×
×
×
×
×
×
converged
×
×
×
×
×
coef
×
×
×
×
×
cvar
×
×
×
×
×
×
formula
×
×
×
×
×
logLik
×
M
×
plot
×
×
×
×
×
predict
×
×
×
×
×
print
×
resid
×
×
×
×
×
residuals
×
×
×
×
×
show
×
×
×
×
×
×
×
×
×
summary
×
×
×
×
×
t
×
update
×
×
×
×
×

120
RISK MODELLING
of the ﬁtted model. Furthermore, forecasts can be swiftly produced by the pre-
dict() method. Although there are quite a few methods available in the package,
the coding effort is reduced to a minimum by deﬁning the methods for the parent
class GOGARCH such that the child classes deﬁned inherit the methods by making use
of callNextMethod().
8.3.5
The packages rugarch and rmgarch
The package rugarch is a recent contribution to CRAN (see Ghalanos 2012b) and is
contained in the‘Finance’ Task View. It provides functions and methods for handling
many speciﬁcations of univariate GARCH-type models. S4 classes and methods are
employed and the package is shipped with a NAMESPACE ﬁle. The computationally
burdensome routines are interfaced from C++ functions. This is accomplished with the
packages Rcpp (Eddelbuettel and Franc¸ois 2011) and RcppArmadillo (see Franc¸ois
et al. 2012), on which this package depends. Because of the numerical complexity
and the demanding task of ﬁtting the covered models to data, wrapper functions that
support multi-core capabilities are available if models are ﬁtted to more than one
endogenous variable. The package is shipped with a vignette in which the major
capabilities and applications are elucidated by examples. Three data sets are included
in rugarch: a return series of the Dow Jones Index (dji30ret), a return series of the
S&P 500 index (sp500ret), and a spot exchange rate series for DEM/GBP (dmbp),
all daily.
A typical work ﬂow would start by specifying the kind of GARCH model with
the function ugarchspec(). This is similar in design and purpose to the function
garchSpec() in fGarch introduced earlier. It takes ﬁve arguments. The kind of
variance model is determined by a list object for the variance.model argument,
the mean equation is set with the mean.model argument and the distribution of
the error process with the distribution.model argument. Starting values for the
parameters according to these ﬁrst three arguments, as well as whether any of them
should be kept ﬁxed, can be controlled with the start.pars and fixed.pars
arguments. The function returns an object of formal class uGARCHspec.
Objects of this class can then be used for ﬁtting data to the chosen speciﬁ-
cation, which is achieved by calling ugarchfit(). Apart from an uGARCHspec
which is passed as argument spec to the function, the data set is passed to the
body of the function as argument data. This can be either a numeric vector,
a matrix or data.frame object or one of the speciﬁc time series class objects:
zoo, xts, timeSeries or irts. The numerical solution of the model can be
determined by one of the optimizers nlminb(), solnp() or gosolnp(), which
is set by the argument solver. Control arguments pertinent to these solvers can
be passed down by providing a list object for the argument solver.control.
In addition, the conformance of stationarity constraints, the calculation of stan-
dard errors in the case of ﬁxed parameters and/or whether the data is to be scaled
prior to optimization can be set with the argument fit.control. A feature of the
function is the argument out.sample. Here the user can curtail the size of the

MODELLING VOLATILITY
121
sample used for ﬁtting, leaving the remaining n data points available for pseudo
ex ante forecasts. The function ugarchfit() returns an object of S4 class
uGARCHfit. This class has the slots fit for the ﬁtted model and model for its
speciﬁcation. A total of 21 methods are deﬁned for these objects, including data
extractions, displaying and analysing the ﬁtted model, diagnostic testing and plotting
facilities.
Forecasts can be generated with the function ugarchforecast(). Here, either
a ﬁtted or a speciﬁed model has to be provided. If the latter is used, a valid set of
ﬁxed parameters must have been set and a data set must be passed to the function
as argument data. The number of forecast periods and/or whether the forecasts
shall be derived from a rolling window are determined by the arguments n.ahead
and n.roll, respectively. If exogenous regressors are speciﬁed in either the mean
or the variance equation, values for these must be supplied as a list object for
external.forecats. The forecast function returns an object of S4 class uGARCH-
forecast for which data extraction, plotting and performance measure methods
are available. Conﬁdence bands for the forecasts can be generated by means of
bootstrapping which is implemented as function ugarchboot.
In addition to using a ﬁtted model for forecasting purposes, the function ugarch-
sim() enables the user to simulate data from it. Data extraction, show and plot
methods are implemented for the returned object of class uGARCHsim.
The function ugarchroll() can be used to back-test the VaR of a return/loss
series. The user can swiftly produce VaR forecasts of a GARCH model and analyse
the results of the object returned, which is of class uGARCHroll, with the methods
provided for data extraction of the VaR numbers, plotting, reporting, testing of the
forecast performance and/or summarizing the back-test results. The back-test can be
conducted either for a recursively extending or a moving sample window.
The user can ﬁlter data for a given model speciﬁcation by utilizing the function
ugarchfilter(). The function returns an S4 object uGARCHfilter for which the
same set of methods is available as in the case of uGARCHfit objects. Hence, ﬁltering
could in principle be utilized to assess the robustness of results, given differing
parameter values, and to check whether an alternative speciﬁcation yields reasonable
results.
Lastly, it should be pointed out that the package contains a routine for bench-
mark comparison of results (ugarchbench()) and supports parallel computation
through the wrapper functions multispec(), multifit(), multiforcast() and
multifilter(), as mentioned earlier in this subsection.
A companion package to rugarch by the same author is rmgrach (Ghalanos
2012a). At the time of writing, this package was only available on R-Forge. In
this package multivariate GARCH model classes and concepts are implemented,
namely, the dynamic conditional correlation GARCH models as proposed by Engle
(2002), the GOGARCH models and copula-GARCH models. The latter concept will
be presented in Chapter 9 when the copula concept is discussed. For each of these
multivariate GARCH models a set of S4 classes and methods are deﬁned. The naming
convention is DCCfoo, goGARCHfoo and cGARCHfoo for the three models, where

122
RISK MODELLING
foo serves as a placeholder for the object’s purpose – spec for the speciﬁcation, fit
for the ﬁtted model, filter for the ﬁlter solution of a speciﬁed model, roll for a
rolling estimation of the model, sim for simulating trajectories from a given model
speciﬁcation and forecast for containing the forecasts of a model. Hence, a total
of 16 classes have been deﬁned. In addition to these class deﬁnitions, virtual classes
from which the model-speciﬁc ones inherit are deﬁned as mGARCHfoo. Objects for
each of the classes can be created by calling the constructor functions of the same
name as the S4 class, but spelt in lower-case letters. For instance, a DCC model is
speciﬁed by calling the function dccsim(). The resulting object can then be used to
estimate the unknown coefﬁcients by invoking dccfit().
8.3.6
The package tseries
The package tseries was the ﬁrst contributed package on CRAN in which primarily
time series models and related statistical tests are implemented (see Trapletti and
Hornik 2012). Its history dates back to the late 1990s. It is contained in the ‘Econo-
metrics, ‘Environmetrics’, ‘Finance’ and ‘TimeSeries’ Task Views, and it is a core
package in all of these except ‘Environmetrics’. Within the package S3 classes and
methods are employed, and it is shipped with a NAMESPACE ﬁle.
With respect to ARCH models, the function garch() is provided. This function
allows the user to ﬁt GARCH models to data. Errors are assumed to be normally
distributed. The order of the GARCH model is set by the argument order. This
two-element vector determines the number of lags to be included in the model, the
ﬁrst element determining the consecutive lags for the lagged conditional variances
and the second element determining the number of lags for the squared residuals. It
is not possible to omit the intermediate lags for either the GARCH or the ARCH part.
ML estimates for the unknown model parameters are determined by employment of
a quasi-Newton optimizer. This optimizer is interfaced from C/FORTRAN routines.
Optimizer-speciﬁc arguments can be provided via a list object garch.control.
The ellipsis argument of garch() is passed down to the function qr() which is
employed in the computation of the asymptotic covariance matrix. The function
returns a list object with class attribute garch.
For objects of this kind print(), coef(), vcov(), residuals(), fitted(),
logLik(), plot() and predict() methods are deﬁned. The estimation result
is returned with the ﬁrst method. The values of the estimated coefﬁcients can be
extracted by coef() and the associated variance–covariance matrix by vcov().
The value of the log-likelihood can be retrieved with the method logLik(). This
method is useful for setting up likelihood ratio tests for statistically discriminating
between nested models. The ± conditional standard deviations are returned by the
method fitted(). The appropriateness of a given GARCH order can be graphically
investigated with the plot() method. Here, time series plots of the series itself
and the residuals can be returned as well as a histogram, a QQ plot and an ACF
representation thereof. Finally, predictions for the ± conditional standard deviations
can be generated with the predict() method.

MODELLING VOLATILITY
123
8.4
Empirical application of volatility models
In this section a back-test of the expected shortfall at a 99% conﬁdence level for New
York Stock Exchange (NYSE) daily returns is conducted. The R code is shown in
Listing 8.1. This data set is a supplement to the monograph of Stock and Watson
(2007) and is contained in the AER package (see Kleiber and Zeileis 2008). The
sample runs from 2 January 1990 to 11 November 2005 and contains 4003 observa-
tions. The necessary packages and the data set are loaded into the work space with
the ﬁrst three commands in the R code listing. Next, the daily compound losses are
computed as percentages and expressed as positive numbers. The stylized facts of
this series are pretty evident from Figure 8.2.
In order to conduct the back-test, the function ESgarch() is deﬁned in lines
6–14. Within this function a GARCH(1, 1) model with a Student’s t-distributed
innovation process is estimated ﬁrst. The one-step-ahead forecast of the conditional
standard deviation is computed next and the ﬁtted value of the degrees-of-freedom
Listing 8.1 Expected shortfall derived from GARCH(1, 1) models.
library(AER)
1
library(fGarch)
2
data(NYSESW)
3
NYSELOSS <−timeSeries(−1.0 ∗diff(log(NYSESW)) ∗100,
4
char.vec = time(NYSESW))
5
## Function for ES of t−GARCH
6
ESgarch <−function(y, p = 0.99){
7
gﬁt <−garchFit(formula = ∼garch(1, 1), data = y,
8
cond.dist = "std", trace = FALSE)
9
sigma <−predict(gﬁt, n.ahead = 1)[3]
10
df <−coef(gﬁt)["shape"]
11
ES <−sigma ∗(dt(qt(p, df), df)/(1 −p)) ∗
12
((df + (qt(p, df))∧2)/(df −1))
13
return(ES)
14
}
15
## Date vectors for back−test
16
from <−time(NYSELOSS)[−c((nrow(NYSELOSS) −999) : nrow(NYSELOSS))]
17
to <−time(NYSELOSS)[−c(1:1000)]
18
NYSEES <−fapply(NYSELOSS, from = from, to = to, FUN = ESgarch)
19
NYSEESL1 <−lag(NYSEES, k = 1)
20
res <−na.omit(cbind(NYSELOSS, NYSEESL1))
21
colnames(res) <−c("NYSELOSS", "ES99")
22
plot(res[, 2], col = "red", ylim = range(res),
23
main = "NYSE: t−GARCH(1,1) ES 99%",
24
ylab = "percentages", xlab = " ")
25
points(res[, 1], type = "p", cex = 0.2, pch = 19, col = "blue")
26
legend("topleft", legend = c("Loss", "ES"),
27
col = c("blue", "red"), lty = c(NA, 1), pch = c(19, NA))
28

124
RISK MODELLING
percentages
1990−01−03
1996−05−07
2002−09−09
−4
−2
0
2
4
6
Figure 8.2
Time series plot for daily losses of NYSE.
parameter is assigned to the object df. The expected shortfall is then computed for
the default conﬁdence level of p = 0.99. Incidentally, the mean equation of this
GARCH model consists of a constant only and is omitted in the calculation of the ES.
Given that its estimate represents the mean of the series which is empirically close to
zero, it can safely discarded from the computation of the ES.
The back-test itself is then conducted by utilizing a sliding window with 1000
observations. The function fapply() comes in handy for conducting back-tests of
this kind. Two date vectors are created in which the start and end dates of this moving
NYSE: t−GARCH(1,1) ES 99%
percentages
0
500
1000
1500
2000
2500
3000
−5
0
5
10
Loss
ES
Figure 8.3
Comparison of daily losses of NYSE and ES.

MODELLING VOLATILITY
125
window through time are stored. In the next line the function fapply() will call
ESgarch() with the subsamples of the losses according to the date values contained
in the objects from and to. It should be noted that the ES numbers are now associated
with the date values of from. But these conditional risk measures pertain to the next
trading day and hence the series must be lagged by one period (object NYSEESL1)
for comparison with the actual losses. The size of the back-test consists therefore of
3001 risk measure–loss pairs. A graphical comparison of the actual losses and the
conditional ES for a 99% conﬁdence level is produced in the ﬁnal lines. The outcome
is shown in Figure 8.3.
Not only are the spikes of the daily losses captured pretty well by this condi-
tional risk measure, but also the level of the risk measure decreases rapidly during
the more tranquil periods. During this back-test simulation only ﬁve violations oc-
curred. This is a little too conservative, given that roughly 15 exceedances can be
expected.
References
Ardia D. 2011 bayesGARCH: Bayesian Estimation of the GARCH(1,1) Model with Student-t
Innovations in R. R package version 1-00.10.
Bera A. and Higgins H. 1993 ARCH models: Properties, estimation and testing. Journal of
Economic Surveys 7(4), 305–362.
Black F. 1976 Studies of stock price volatility changes Proceedings of the 1976 Meeting of the
Business and Economics Statistics Section, pp. 177–181 American Statistical Association.
Bollerslev T. 1986 Generalised autoregressive conditional heteroscedasticity. Journal of Econo-
metrics 31, 307–327.
Bollerslev T., Chou R. and Kramer K. 1992 ARCH modeling in ﬁnance. Journal of Econo-
metrics 52, 5–59.
Boswijk HP. and van der Weide R. 2006 Wake me up before you GO-GARCH. Discussion
Paper TI 2006-079/4, University of Amsterdam and Tinbergen Institute, Amsterdam.
Boswijk HP. and van der Weide R. 2009 Method of moments estimation of GO-GARCH
models. Working paper, University of Amsterdam and Tinbergen Institute and World Bank,
Amsterdam.
Burger M., J¨unemann K. and K¨onig T. 2010 RUnit: R Unit test framework. R package version
0.4.26.
Ding Z., Granger C. and Engle R. 1993 A long memory property of stock market returns and
a new model. Journal of Empirical Finance 1, 83–106.
Eddelbuettel D. and Franc¸ois R. 2011 Rcpp: Seamless R and C++ integration. Journal of
Statistical Software 40(8), 1–18.
Engle R. 1982 Autoregressive conditional heteroscedasticity with estimates of the variance of
United Kingdom inﬂation. Econometrica 50(4), 987–1007.
Engle R. 2002 Dynamic conditional correlation. Journal of Business & Economic Statistics
20(3), 339–350.
Engle R. and Bollerslev T. 1986 Modelling the persistence of conditional variances. Econo-
metric Reviews 5, 1–50.

126
RISK MODELLING
Franc¸ois R., Eddelbuettel D. and Bates D. 2012 RcppArmadillo: Rcpp integration for Armadillo
templated linear algebra library. R package version 0.2.36.
Geweke J. 1986 Modelling the persistence of conditional variances: A comment. Econometric
Reviews 5, 57–61.
Ghalanos A. 2012a rmgarch: Multivariate GARCH models. R package version 0.94.
Ghalanos A. 2012b rugarch: Univariate GARCH models. R package version 1.0-8.
Glosten L., Jagannathan R. and Runkle D. 1993 On the relation between expected value and
the volatility of the nominal excess return on stocks. Journal of Finance 48, 1779–1801.
Higgins M. and Bera A. 1992 A class of nonlinear ARCH models. International Economic
Review 33, 137–158.
Kleiber C. and Zeileis A. 2008 Applied Econometrics with R. Springer-Verlag, New York.
Nakatani T. 2010 ccgarch: An R Package for Modelling Multivariate GARCH Models with
Conditional Correlations. R package version 0.2.0.
Nelson D. 1991 Conditional heteroscedasticity in asset returns: A new approach. Econometrica
59(2), 347–370.
Pentula S. 1986 Modelling the persistence of conditional variances: A comment. Econometric
Reviews 5, 71–74.
Pfaff B. 2012 gogarch: Generalized Orthogonal GARCH (GO-GARCH) models. R package
version 0.7-1.
Schwert G. 1990 Stock volatility and the crash of ’87. Review of Financial Studies 3(1),
77–102.
Stock J. and Watson M. 2007 Introduction to Econometrics. Addison Wesley, Boston.
Taylor S. 1986 Modeling Financial Time Series. John Wiley & Sons, Inc., New York.
Trapletti A. and Hornik K. 2012 tseries: Time Series Analysis and Computational Finance. R
package version 0.10-28.
van der Weide R. 2002 GO-GARCH: A multivariate generalized orthogonal GARCH model.
Journal of Applied Econometrics 17(5), 549–564.
W¨urtz D. and Chalabi Y. 2009 fGarch: Rmetrics – Autoregressive Conditional Heteroskedastic
Modelling. R package version 2110.80.
Zakoian J. 1994 Threshold heteroscedasticity models. Journal of Economic Dynamics and
Control 15, 931–955.

9
Modelling dependence
9.1
Overview
In the previous chapters alternatives to the Gaussian model have been put forward
for assessing market price risks of single ﬁnancial instruments. The potential losses
arising from holding a position in an asset were interpreted as a random variable and
unconditional as well as conditional risk measures were derived.
In this chapter the topic of ﬁnancial risk modelling in the context of multiple
ﬁnancial instruments is addressed. Section 9.2 introduces the correlation coefﬁcient
between two assets and investigates its appropriateness as a measure of dependence
between two assets. Section 9.3 discusses alternative measures of dependence, namely
the use of rank correlations and the concept of the copula. Section 9.4 provides
a synopsis of the R packages that speciﬁcally include copula modelling. Finally,
Section 9.5 shows how copula models can be fruitfully combined with the techniques
outlined in Chapters 6–8. In particular, a GARCH–copula model is proposed for
measuring the market risk of a portfolio.
9.2
Correlation, dependence and distributions
The computation and usage of Pearson’s correlation coefﬁcient is quite ubiquitous
in the quantitative analysis of ﬁnancial markets. However, applied quantitative re-
searchers are often unaware of the pitfalls involved in careless application and
usage of correlations as a measure of risk. It is therefore appropriate to investi-
gate this dependence concept in some detail and point out the shortcomings of this
measure.
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

128
RISK MODELLING
Pearson’s correlation coefﬁcient between two random variables X1 and X2 with
ﬁnite variances is deﬁned as
ρ(X1, X2) =
Cov(X1, X2)
√Var(X1)√Var(X2),
(9.1)
where
Cov(X1, X2) = E ((X1 −E(X1))(X2 −E(X2)))
denotes
the
covariance
between the random variables and Var(X1) and Var(X2) are their variances, re-
spectively. As is evident from equation (9.1), the correlation coefﬁcient is a scalar
measure. In the case of a perfect linear dependence the value of this measure is
ρ(X1, X2) = |1|, where the perfect linear dependence is given by X2 = β + αX1,
with α ∈R 0, β ∈R, otherwise the correlation coefﬁcient can take values in the
interval −1 < ρ(X1, X2) < 1. Furthermore, the linear correlation coefﬁcient is in-
variant with respect to linear afﬁne transformations of the random variables X1 and
X2: ρ(α1X1 + β1, α2X2 + β2) = sign(α1α2)ρ{X1, X2} for α1, α2 ∈R 0, β1, β2 ∈R.
Hence, Pearson’s correlation coefﬁcient depicts the strength of a linear relationship
between two random variables. The logical reasoning that ρ(X1, X2) = 0 implies
independence between X1 and X2 is only valid in the case of multivariate elliptically
distributed random variables. Put differently, lack of correlation is only a sufﬁcient
condition for independence and only in the case of elliptical distributions can the
two concepts be used interchangeably. Elliptic distributions are characterized by
the fact that the points of the density function which yield the same values repre-
sent an ellipse, that is, horizontal cuts through the probability mass are elliptically
shaped.
The implication for multivariate risk modelling is that only in the case of jointly
elliptically distributed risk factors can the dependence between these be captured
adequately by the linear correlation coefﬁcient. Given the stylized facts of ﬁnancial
market returns this assumption is barely met. It should be pointed out at this point that
with respect to risk modelling one is usually more concerned with the dependence
structure in the tail of a multivariate loss distribution than with an assessment of the
overall dependence, but a correlation measure just depicts the latter, in principle.
Furthermore, the linear correlation coefﬁcient is not invariant with respect to
non-linear (e.g., log-transformed) random variables or if non-linear (e.g., quadratic
or cubic) dependence between variables exists. This fact will be illustrated with the
following two examples.
First, a standardized normal random variable X1 ∼N(0, 1) with quadratic depen-
dence X2 = X2
1 is investigated. Between these two variables a direct relationship is
evident; for a given realization x1 of X1 the value for x2 can be concluded. However,
if one were to calculate the linear correlation coefﬁcient between these two vari-
ables, the result would be Cov(X1, X2) = E

X1 · (X2
1 −1)

= E(X3
1) −E(X1) = 0,
because the skewness of normally distributed random variables is zero.
The purpose of the second example is to highlight the fact that the correlation
coefﬁcient depends on the marginal distributions of the random variables in question
and the possibility that the correlation coefﬁcient cannot take all values in the interval
−1 ≤ρ ≤1 if one views the multivariate distribution of these random variables.
Given two log-normal random variables X1 ∼log N(0, 1) and X2 ∼log N(0, σ 2)

MODELLING DEPENDENCE
129
with σ > 0, the feasible range of values for the correlation coefﬁcient has a lower
bound of ρmin = ρ(eZ, e−σ Z) and an upper bound of ρmax = ρ(eZ, eσ Z), where Z ∼
N(0, 1). The sign of the exponent depends on whether the random variables move
in the same direction or opposite directions. In the ﬁrst case a positive dependence
(co-monotonicity) results and in the latter a negative one (counter-monotonicity). The
lower bound of the correlation coefﬁcient is therefore given by
ρmin =
e−σ −1

(e −1)(eσ 2 −1)
,
(9.2)
and the upper bound is determined by
ρmax =
eσ −1

(e −1)(eσ 2 −1)
.
(9.3)
As is evident from equations (9.2) and (9.3), these bounds depend only on the variance
of the marginal distribution for X2. Here, the lower bound is greater than −1 and the
upper bound coincides with the case of perfect dependence only when σ = 1 and
is less than 1 in all other instances. Incidentally, with ever increasing variance the
correlation coefﬁcient approaches zero.
With respect to risk modelling the following conclusions can be drawn. For
instance, a correlation coefﬁcient as high as 0.2 would not necessarily indicate a
weak relationship between two risk factors. Indeed, the opposite can be true and
therefore a tentative diversiﬁcation gain in the context of a portfolio is void when
capital is allocated to the respective assets. By converse reasoning, the information
of a correlation coefﬁcient as high as 0.7 between two log-normally distributed risk
factors with expected values of zero and variances of 1 and 4, respectively, is void. If
these two random variables are jointly distributed, such a high value for the correlation
coefﬁcient cannot be achieved.
In summary, the dependence between ﬁnancial instruments can only be depicted
correctly with the linear correlation coefﬁcient if these are jointly elliptically dis-
tributed. It was also shown that the value of the correlation coefﬁcient depends on
the marginal distributions and that not all values in the range of [−1, 1] are attain-
able. In particular, a perfect positive dependence between two random variables is
not interchangeable with a correlation coefﬁcient of one, but the value can be much
lower and at the limit a value of zero is obtained. Conversely, a correlation coefﬁ-
cient of zero does not lead to a conclusion of independence. This reasoning is only
valid in the case of jointly elliptical distributions. Last, but not least, it should be
pointed out that the linear correlation coefﬁcient is only deﬁned for pairs of random
variables with ﬁnite variance. This assumption may be violated for risk factors with
great probability masses located in the tails. As a consequence, the application of
alternative concepts for measuring the dependence between risk factors becomes
necessary. In addition to the rank correlation coefﬁcients of Kendall and Spearman,
the copula concept is a promising route to take. The methods will be introduced in the
next section.

130
RISK MODELLING
9.3
Copulae
9.3.1
Motivation
The copula approach was introduced by Sklar (1959). Detailed textbook exhibitions
can be found in Schweizer and Sklar (1983), Nelsen (2006), Joe (1997) and McNeil
et al. (2005). But only since the mid-1990s have copulae been used as a tool for
modelling dependencies between assets in empirical ﬁnance. The word ‘copula’
derives from the Latin verb copulare and means to ‘bond’ or to ‘tie’. What exactly
is bonded by a copula will be the subject of the rest of this subsection.
The marginal distributions of jointly distributed random variables as well as their
dependence are contained in their joint distribution function,
F(x1, . . . , xd) = P [X1 ≤x1, . . . , Xd ≤xd] .
(9.4)
It was shown in the previous section that in the case of normal random variables the
value of the correlation coefﬁcient is dependent upon the marginal distributions. This
measure may take values less than 1 even if there is perfect dependence between the
two random variables. Hence, it is necessary to separate the marginal distributions
from the dependence structure between the random variables. This separation can be
achieved by means of a copula.
For this purpose an elementwise transformation of the random vector X =
(X1, . . . , Xd) is required, such that the resulting variables follow a uniform dis-
tribution U(0, 1). Given the assumption that all distribution functions F1, . . . , Fn of
the random vector X are continuous, this projection is given by the probability inte-
gral: Rd →Rd, (x1, . . . , xn)t →(F1(x1), . . . , Fd(xd))t. The joint density function C
of the transformed random variables (F1(x1), . . . , Fd(xd))t is the copula of the vector
X = (X1, . . . , Xd):
F(x1, . . . , xd) = P [F1(X1) ≤F1(x1), . . . , Fd(Xd) ≤Fd(xd)]
= C(Fd(xd), . . . , Fd(xd)).
(9.5)
Hence, a copula is the distribution function in Rd space of a d-element random vector
with standard uniform marginal distributions U(0, 1). Alternatively, a copula can be
interpreted as a function that maps from the d-dimensional space [0, 1]d space into
the unit interval: C : [0, 1]d →[0, 1]. In this interpretation further conditions must
be met by the function C in order to qualify as a copula. The dependence structure
of the random vector X is embodied in the copula. If the marginal distributions are
continuous, then the copula is uniquely deﬁned. It is therefore possible to employ
different distributions for the random variables (i.e., risk factors) as marginals and
capture the dependences between those with a copula. In principle, all of the previ-
ously introduced distributions would classify as potential candidates for modelling
the risk factors in a portfolio’s loss function.

MODELLING DEPENDENCE
131
9.3.2
Correlations and dependence revisited
It was shown in Section 9.2 that the linear correlation coefﬁcient does not capture
dependencies well in the case of non-elliptically distributed random variables. Fur-
thermore, while this measure depicts the overall dependence, in the assessment of
the riskiness of a portfolio what matters most is the dependence in the tail of the
joint distribution. Hence, in this subsection two further concepts for capturing the
dependence between risk factors are introduced, namely, concordance and tail de-
pendence. The ﬁrst concept entails the rank correlation coefﬁcients of Kendall and
Spearman.
The concept of concordance will be illustrated by the following example.
Suppose one has two realizations (xi, yi) and (x j, y j) of a two-dimensional random
vector (X, Y). If one plots these two points in a Cartesian coordinate system and joins
them by a line, one says that a positive dependence exists if the line slopes upward
(concordance) and a negative one prevails if the line slopes downward (discordance).
Equivalently, if one calculates the product of the pairwise differences (xi −x j)
(yi −y j) then concordance obtains if the result is positive and discordance if the
result is negative. It is now assumed that the continuous and independent random
vectors (X1, Y1) and (X2, Y2) each have a joint density function H1 and H2 with
copulae C1 and C2. Let Q denote the difference in probability for the cases of
concordance and discordance of (X1, Y1) and (X2, Y2):
Q = P((X1 −X2)(Y1 −Y2) > 0) −P((X1 −X2)(Y1 −Y2) < 0).
(9.6)
The probability expression of equation (9.6) can be cast in terms of copulae
Q = Q(C1, C2) = 4
 1
0
 1
0
C2(u, v) dC1(u, v) −1;
(9.7)
a detailed derivation can be found in Genest and Favre (2007). If the two random
vectors share the same dependence, then equation (9.7) can be simpliﬁed to
Q = Q(C1, C2) = 4
 1
0
 1
0
C(u, v) dC(u, v) −1.
(9.8)
Kendall’s rank correlation coefﬁcient, referred to as ‘tau’ and denoted by ρτ, is
deﬁned as
ρτ(X1, X2) := E[sign((X1 −X2)(Y1 −Y2))],
(9.9)
where E denotes the expectation operator. The deﬁnition of Kendall’s tau in equation
(9.9) is identical to the probability expression of equation (9.6). Therefore, Kendall’s
tau can be interpreted as a probability integral of the copulae (see equations (9.7) and
(9.8) and depends only on these. Like the linear correlation coefﬁcient, Kendall’s tau
is invariant with respect to monotone transformations of the random variables and
measures an overall dependence between them. In contrast to the linear correlation

132
RISK MODELLING
coefﬁcient, rank correlations are less sensitive to outliers or extreme observations.
Furthermore, it can be shown for elliptical copulae that
ρτ(X1, X2) = 2
π arcsin ρ,
where ρ denotes Pearson’s correlation coefﬁcient.
We now turn to Spearman’s rank correlation coefﬁcient, ρS. This coefﬁcient is
deﬁned for the bivariate case as
ρS = Cor(F1(X1), F2(X2)).
The dependence on a copula can be expressed analogously to that for Kendall’s tau
shown above:
ρS(X1, X2) = 12
 1
0
 1
0
C(u, v) du dv −3.
(9.10)
The following relation holds between Spearman’s and Pearson’s correlations:
ρS(X1, X2) = 6
π arcsin ρ
2 .
The difference between the two concepts might appear to be negligible, but Spear-
man’s rank correlation coefﬁcient possesses the characteristic that E(ρS) = ±1 if
there is a functional relationship between the two random variables (in this case the
copula would coincide with one of the two Fr´echet–Hoeffding bounds). In contrast,
E(ρ) = ±1 holds only for a linear relationship between the two random variables.
In addition, ρS is a distribution parameter and as such always deﬁned. Pearson’s ρ,
however, cannot be theoretically derived from all distributions.
An alternative to the concordance concept and rank correlations for modelling
dependencies is the concept of tail dependencies, which is of great importance in the
risk assessment of a portfolio. The focus of the dependence measure is solely on the
tails of the joint distribution. It is therefore possible to determine the likelihood that
for a given loss in one ﬁnancial instrument an equivalent or even greater loss would
occur in a second holding. Formally, the upper and lower tail dependencies for two
random variables (X, Y) with marginal distributions FX and FY are deﬁned as
λu = lim
q↗1 P(Y > F−1
Y (q)|X > F−1
X (q)),
(9.11a)
λl = lim
q↘0 P(Y ≤F−1
Y (q)|X ≤F−1
X (q)).
(9.11b)
In other words, tail dependence is a conditional probability expression, namely, the
likelihood of observing a great (small) value for Y, given a great (small) value for
X. If λu > 0 then there is upper tail dependence between the two random variables,
and if λl > 0 then there is lower tail dependence. For the cases of λu = 0 or λl = 0,
it is said that (X, Y) are asymptotically independent at the upper or lower end of

MODELLING DEPENDENCE
133
the distribution. According to Bayes’ formula the tail dependencies for continuous
distribution functions are given as:
λl = lim
q↘0
P(Y ≤F−1
Y (q)|X ≤F−1
X (q))
P(X ≤F−1
X (q))
= lim
q↘0
C(q, q)
q
,
(9.12)
for the lower tail dependence coefﬁcient, and analogously
λu = 2 + lim
q↘0
C(1 −q, 1 −q) −1
q
.
(9.13)
for the upper tail dependence coefﬁcient.
9.3.3
Classiﬁcation of copulae
The previous two subsections gave a brief outline of the concept of the copula and
how it relates to rank correlations. In this subsection copulae are introduced more
formally and their classiﬁcation into sub-categories described. It will be shown how
the tail dependence as a measure of risk can be deduced from them. It is beyond
the scope of this book to provide a detailed exposition of copulae; the aim is rather
to better grasp the underlying concept and apply copulae as a tool for modelling
dependencies. Thorough textbook expositions can be found in Joe (1997) and Nelsen
(2006).
In general, a copula is expressed in the form of a multivariate density function,
C(u) = P(U1 ≤u1,U2 ≤u2, . . . ,Ud ≤ud) =
 u1
0
· · ·
 ud
0
c(u) du,
(9.14)
where u is a d-dimensional random vector. The implicit form of a copula for a
continuous distribution function F with continuous marginal distributions F1, . . . , Fd
can therefore be written as
C(u) =
f (F−1
1 (u1), . . . , F−1
d (ud))
f1(F−1
1 (u1)) · · · fd(F−1
d (ud))
.
(9.15)
Equivalently, equation (9.15) can be written as
c(F1(x1), . . . , Fd(xd)) =
f (x1, . . . , xd)
f1(x1) · · · fd(xd).
(9.16)
As can be seen from this equation, a d-dimensional density function can be repre-
sented as a multiplicative concatenation of the copula with the marginal distributions.
It was shown above that certain conditions must be met by a copula with re-
spect to its domain. These Fr´echet–Hoeffding bounds were also introduced in the
context of the linear correlation coefﬁcient for jointly normally distributed random
variables. These bounds will now be derived for arbitrary copulae. Two uniformly
distributed random variables U1 and U2 are assumed. Between these two variables
three extreme cases of dependence can be distinguished: concordance, independence

134
RISK MODELLING
and discordance. First, the case of perfect positive dependence will be investigated
(e.g., U1 = U2). The copula for this case could then be written as
C(u1, u2) = P(U1 ≤u1,U1 ≤u2) = min(u1, u2).
(9.17)
The copula would also be valid if a monotone transformation is applied to the random
variables. As an intermediate step between this case and that of discordant random
variables, independence will be investigated next. The copula is then the product of
the two random variables C(u1, u2) = u1 · u2. Symbolically one can imagine this as
ﬂipping a coin twice. The joint density function of independent random variables is
the product of the respective marginal distributions, which equals the independence
copula:
C(u) = d
i=1 ui = P(U1 ≤u1) · · · P(Ud ≤ud)
= P(U1 ≤u1, . . . ,Ud ≤ud) = f (u).
(9.18)
The case of discordance occurs if U2 = 1 −U1 is fulﬁlled. The copula is then given
by
C(u1, u2) = P(U1 ≤u1, 1 −U1 ≤u2)
= P(U1 ≤u1, 1 −u2 ≤U1) = u1 + u2 −1,
(9.19)
and it is zero for all other instances. The domain of a copula for the d-dimensional
case C(u = C(u1, . . . , ud)) is then bounded by
max
 d

i=1
ui + 1 −d, 0
	
≤C(u) ≤min{u1, . . . , ud}
(9.20)
and the lower bound is only attainable for the bivariate case.
Hitherto the discussion of the copula concept has been quite abstract, in terms
of its main characteristics and bounds. We will now classify copulae into two broad
categories, namely the Archimedean copulae on the one hand and the distribution-
based copulae on the other. Within the latter group of copulae the dependence between
the random variables will be captured implicitly by a distribution parameter. For
instance, the bivariate case of a Gauss copula is deﬁned as
CGA
ρ (u1, u2) = 	
(	−1(u1), 	−1(u2))
=
 	−1(u1)
−∞
 	−1(u1)
−∞
1
2π

1 −ρ2 exp

−x2
1 −2ρx1x2 + x2
2
2(1 −ρ2)

dx1 dx2.
(9.21)
The distribution function in equation (9.21) depends on the variance–covariance ma-
trix 
. Because monotone transformations do not alter the dependence structure, the
variance–covariance matrix can be replaced by the correlation matrix. Hence, the
Gauss copula includes the extreme cases of co-monotonicity (ρ = 1), the indepen-
dence copula (ρ = 0) and counter-monotonicity (ρ = −1). A characteristic of the
Gauss copula is that the tail dependence is zero. Therefore it has limited application
to risk modelling.

MODELLING DEPENDENCE
135
As a second example, we will now focus on the Student’s t copula, which is
deﬁned as
Ct
ρ,ν(u1, u2) = t
,ν(t−1
ν (u1), t−1
ν (u2))
=
 t−1
ν (u1)
−∞
 t−1
ν (u1)
−∞
1
2π

1−ρ2

1+x2
1−2ρx1x2+x2
2
ν(1−ρ2)
−(ν+2)/2
dx1 dx2,
(9.22)
where ν denotes the degrees of freedom as an additional distribution parameter. In
contrast to the Gauss copula, the coefﬁcient of tail dependence for the Student’s t
copula can be computed as
λu = λl = 2tν+1(−
√
ν + 1

(1 −ρ)/(1 + ρ)) .
(9.23)
The advantages of distribution-based copulae are twofold: ﬁrst, and most impor-
tantly, their simplicity; and second, that simulations based upon these copula models
can be carried out easily. Among the disadvantages is the fact that many parameters
have to be estimated. Furthermore, in the case of elliptical distributions their sym-
metry is a problem, given the stylized fact that the empirical distributions of loss
function are ordinarily skewed. Finally, a closed form for distribution-based copulae
cannot be derived.
Archimedean copulae do not share the latter disadvantage. An Archimedean
copula is deﬁned as
C(u1, u2) = ψ−1(ψ(u1) + ψ(u2)),
(9.24)
where ψ is the copula-generating function. Two prominent examples of this category
are the Clayton and Gumbel copulae. The copula-generating function for the Clayton
copula is given by
φ(t) = (t−δ −1)/δ,
with δ ∈(0, ∞).
For δ →∞a perfect dependence results and for δ →0 independence. If the gener-
ating function is inserted into equation (9.24), the Clayton copula is then given by
CCL
δ
= ψ−1(ψ(u1) + ψ(u2)) = (u−δ
1 + u−δ
2 −1)1/δ.
(9.25)
The Clayton copula possesses lower tail dependence. The relevant coefﬁcient is
calculated according to λl = 2−1/δ.
The copula-generating function for the Gumbel copula is given as ψ(t) =
(−ln t)θ, with the parameter restriction θ ≥1. Inserting this generating function
into equation (9.24) leads to the Gumbel copula,
CGU
θ
= ψ−1(ψ(u1) + ψ(u2)) = exp(−[(−ln u1)θ + (−ln u2)θ]1/θ).
(9.26)
The Gumbel copula possesses upper tail dependence. Perfect dependence exists for
θ →∞and independence for θ →1. The coefﬁcient of tail dependence can be
determined according to λu = 2 −21/θ.

136
RISK MODELLING
The main advantages of Archimedean copulae are twofold: these copulae do have
a closed form and manifold dependence structures can be modelled with them. A
drawback is their quite complex structure for higher dimensions. Ordinarily this is
circumvented through nested/hierarchical modelling designs.
In principle, the unknown parameters of a copula can be estimated in two ways.
The ﬁrst procedure is fully parametric. Because for multivariate distribution models
derived from a copula of higher dimensions this procedure can be quite burdensome,
Joe and Xu (1996) and Shih and Louis (1995) proposed a two-step estimation.
Here, the unknown parameters for the assumed models of the marginal distributions
are estimated ﬁrst. Based upon these ﬁtted models, the pseudo-uniform variables
are retrieved from the inverse distribution functions. These can then be used for
maximizing the likelihood (copula). This approach is, for obvious reasons, often
termed inference functions for margins. The second procedure is based upon a semi-
parametric approach. In contrast to the ﬁrst procedure, no models are assumed for the
marginals, but rather the empirical distribution functions are employed to retrieve the
pseudo-uniform variables. These are then used for maximizing the pseudo-likelihood.
The parameters of the copula are determined by means of numerical optimization
techniques. For the case of a Student’s t copula a simpliﬁcation results if Kendall’s
tau is calculated ﬁrst and then the pseudo-likelihood has to be maximized only with
respect to the degrees of freedom parameter ν. For the bivariate Archimedean copulae
of Clayton and Gumbel type, the following simpliﬁcations are given, where ˆρτ denotes
the empirical Kendall rank correlation (see, for instance, Genest and Favre 2007):
ˆδ =
2 ˆρτ
1 −ˆρτ
,
ˆθ =
1
1 −ˆρτ
.
(9.27)
It was argued in Section 9.2 that the linear correlation coefﬁcient is not well suited
to capturing dependencies between loss factors, namely that quite distinct shapes of
dependence can yield the same value for this coefﬁcient. We conclude this section
by highlighting this fact. Figure 9.1 shows four scatter diagrams. The sample size
for each of the data pairs has been set to 2000 observations and the samples have
been generated such that the correlation coefﬁcient for each of the four sets equals
0.7. From these scatter diagrams, it is pretty evident that the dependence structures
vary and that the information content of the correlation coefﬁcient with respect to the
tail dependence is quite limited. In the case of the Gumbel copula a concentration of
positive pairs and in the case of the Clayton copula a clustering of negative pairs is
evident. The Student’s t copula is characterized by a wider dispersion of pairs with
opposite signs.
9.4
Synopsis of R packages
9.4.1
The package BLCOP
The package BLCOP (see Gochez 2010) implements the Black–Litterman approach
(see Black and Litterman 1990) to portfolio optimization and the framework of cop-
ula opinion pooling (see Meucci 2006a,b, 2010). It is presented in this chapter

MODELLING DEPENDENCE
137
−4
−2
0
2
4
−4
−2
0
2
4
Gauss copula
X1
X2
−4
−2
0
2
4
−4
−2
0
2
4
Gumbel copula
X1
X2
−4
−2
0
2
4
−4
−2
0
2
4
Clayton copula
X1
X2
−4
−2
0
2
4
−4
−2
0
2
4
t copula
X1
X2
Figure 9.1
Different types of copulae.
because of the latter feature, but an application is deferred to Part III of the
book (Chapter 13). The package is contained in the CRAN ‘Distributions’ and
‘Finance’ Task Views. It is written purely in R, and S4 classes and methods are
employed. The package depends on the packages methods, MASS and quadprog –
the latter is a non-standard package and must be installed separately. BLCOP is
shipped with a NAMESPACE ﬁle and there is a vignette to guide the user. In addition, a
unit testing framework is implemented such that the proper working of the package’s
functions can be controlled. For this, the package RUnit (see Burger et al. 2010)
needs to be installed. BLCOP is shipped with three data sets: monthlyReturns,
containing the monthly returns of six US stocks; and sp500Returns and US13wTB,
containing the monthly returns of the S&P 500 Index and the 13-week Treasury bill
interest rates.
With regard to the copula opinion pooling framework the class COPViews is
deﬁned. The various opinions about certain assets are stored in objects of this class.
This kind of object can be generated with a function of the same name. A delete-
Views() and a show() method are available. The former facilitates the removal of
views from such an object, whereas the latter method produces a user-friendly print
of the views. Views (i.e., opinions about certain assets) can be added to an existing

138
RISK MODELLING
COPViews object with the function addCOPViews(). As an aside, a graphical user
interface for accessing and manipulating views is made available by the function
createCOPViews().
The posterior distribution is generated by means of Monte Carlo simulations
through the function COPPosterior(). Here, a multivariate distribution object for
the overall market distribution has to be provided as well as an object of class
COPViews. The function returns an object of class COPResult for which methods
are deﬁned that display the prior and marginal prior distributions of each asset (den-
sityPlot()), print information about the posterior results (show()) and generate
optimal prior and posterior portfolios (optimalPortfolios.fPort()). The last
function utilizes routines that are contained in the package fPortfolio (see W¨urtz
et al. 2010).
In addition to these cornerstone classes and methods that deal with the copula
opinion pooling framework, the package also offers utility functions for retrieving
and manipulating slots from objects of the two classes presented.
9.4.2
The packages copula and nacopula
In the package copula the copula concept is implemented in a broad and self-
contained manner (see Kojadinovic and Yan 2010; Yan 2007). The package is con-
sidered to be a core package of the CRAN ‘Distributions’ Task View and is also
listed in the ‘Finance’ and ‘Multivariate’ Task Views. S4 classes and methods not
only for elliptical and Archimedean but also for extreme value copulae are deﬁned
which enable the user to estimate copulae and conduct statistical tests with respect
to the appropriateness of a chosen copula. Indeed, an application of a goodness-of-ﬁt
test and the estimation of a Clayton copula are available in the form of R DEMO ﬁles.
The computationally burdensome routines are interfaced from code written in the
C language. Finally, the package contains three data sets originating in the ﬁelds of
insurance, ﬁnance and chemistry. Incidentally, the ﬁnance data set is used in McNeil
et al. (2005, Chapter 5).
For dealing with copulae the virtual class copula is deﬁned. This class is extended
by classes for elliptical, Archimedean and extreme value copulae – ellipCopula,
archmCopula and evCopula, respectively. Particular classes for copulae belonging
to and inheriting from one of these three kinds are deﬁned, too. On the one hand the
peculiarities of each speciﬁc copula can be taken into account, and on the other hand
methods need only be deﬁned for the parent class in most cases. Copula objects can
be created swiftly by employing creator functions of the same name as the kind of
copula. The speciﬁc copula is determined by the common argument family within
these functions. The proper creation of objects belonging to these classes is controlled
by validity functions. Speciﬁc copula objects can also be generated by calling the
function fooCopula(), where foo is a place holder for the name of the copula;
for example, a normal copula can be created by a call to normalCopula() and a
Gumbel copula by utilizing gumbelCopula().
Methods for returning values of the density and distribution functions for objects
of class copula are available as well as the generation of random numbers. The

MODELLING DEPENDENCE
139
R naming convention ﬁrst introduced in Section 6.4 is used whereby the name of
the copula is preﬁxed with [d] for the density, [p] for the distribution and [r]
for random number generation. Contour and perspective plots of a copula object
can be drawn with the deﬁned methods contour() and persp(), respectively.
Furthermore, methods calculating the measures of association due to Kendall
(kendallsTau()), Spearman (spearmansRho()) and the tail index (tailIn-
dex()) are available. It is possible to calibrate the parameters of a copula for a
given rank correlation with the functions calibKendallsTau() and calibSpear-
mansRho() for the Kendall and Spearman coefﬁcient, respectively. The generating
functions and their inverses for Archimedean copulae can be accessed through the
functions genFun() and genInv(). In addition, the ﬁrst and second derivatives
of the generating function for this group of copulae are termed genFunDer1()
and genFunDer2(). Similarly, the generating function for extreme value copulae is
Afun() and a data frame of the ﬁrst and second derivatives thereof can be returned
by the function AfunDer().
Estimation of the unknown copula parameters is implemented in the function
fitCopula(). The user can choose between maximum likelihood, maximum
pseudo-likelihood, and the inversion of Kendall’s tau or Spearman’s ρ as estima-
tion methods. The function returns an object of class fitCopula for which show()
and summary() methods are deﬁned. The maximization of the likelihood is carried
out with the function optim(). The user can supply the optimizer to be used as well
as a list of control parameters in accordance with this function. It was brieﬂy men-
tioned in Section 9.3.3 that it can be quite burdensome to estimate all of the unknown
parameters for higher dimensions and hence that a two-stage estimation should be
conducted (i.e., inference functions for margins). Yan (2007, Section 4) provides an
example showing how this can be achieved with the function fitCopula().
The second cornerstone class deﬁned in copula is mvdc which contains the rele-
vant items for multivariate distributions constructed from copulae. Objects belonging
to this class can be swiftly generated with the function mvdc(). The arguments of
the function refer to the copula and marginal distributions to be used and a list
object containing the parameters for each of the marginal distributions. Similarly to
the class copula, methods for the density and distribution function as well as the
generation of random numbers are available. The ﬁtting of a multivariate distribution
model deﬁned by a copula is implemented in the function fitMvdc() which returns
an object of class fitMvdc. The unknown parameters are estimated by applying the
ML principle via optim() and the log-likelihood is deﬁned through the function
loglikMvdc(). Similarly to objects of class fitCopula, show() and summary()
methods are deﬁned for the results of fitMvdc().
Finally, goodness-of-ﬁt tests and tests for independence and extreme value de-
pendence are implemented. The appropriateness of a certain copula can be assessed
with the functions gofCopula() and gofEVCopula(). Whether the assumption of
independence for a given multivariate sample holds can be tested with the functions
indepTest() and/or multIndepTest(). The outcome of the former function can
be inspected graphically using a dependogram. This kind of ﬁgure is created by
a function of the same name. Similarly, the functions serialIndepTest() and

140
RISK MODELLING
multSerialIndepTest() can be utilized to test serial independence of the data
set. These tests are described in Kojadinovic and Yan (2010). Three different tests of
extreme value dependence are also implemented. A bivariate test based on Pickand’s
dependence function is available through the function evTestA() and one based
on a Kendall process is available as evTestK(). A large-sample test of multivari-
ate extreme value dependence can be carried out by calling evTestC(). Further
information about these statistical tests is provided in the help pages of the functions.
In the package nacopula the approach of nesting Archimedean copulae is im-
plemented (see Hofert and Maechler 2011). S4 classes and methods are employed.
The Archimedean copulae are interfaced from routines written in the C language.
The usage of the package is outlined in a vignette. Furthermore, a standard testing
framework for proper working of the methods is available. The package is listed in
the CRAN ‘Distributions’ Task View.
The two cornerstone classes of the package are acopula and nacopula for
single arbitrary and nested Archimedean copulae, respectively. In addition, the class
outer_nacopula inherits from the latter class, but the two differ only with respect
to their validation function. The copula families available are Ali–Mikhail–Haq,
Clayton, Frank, Gumbel and Joe. Objects of these copulae can be nested with the
function onacopula(). Random variates of a nested copula object can be generated
with the function rnacopula(), and for its children the function rnchild() is
used. Nested Archimedean copula objects can be evaluated, that is, the probability
is computed for a given d-dimensional vector of observations, with the function
pnacopula(). Probabilities that refer to intervals are returned with the function
prob(). Both functions refer to points/limits of the hypercube.
As of version 0.8-0 the package has been merged into the package copula and
hence the classes, methods and functions of this package are all available in copula.
The package nacopula still exists as a separate entity on CRAN in order to ease
this transition, but a warning is issued when it is loaded so as to alert the user to the
existence of the copula package.
9.4.3
The package fCopulae
Similar to the package copula, the package fCopulae offers a uniﬁed treatment of
copulae (see W¨urtz 2009). The package is part of the Rmetrics suite of packages
and is considered a core package in the CRAN ‘Distributions’ and ‘Finance’ Task
Views. It employs S4 as well as S3 classes and methods. A utility function for
multidimensional numerical integration is interfaced from a FORTRAN routine.
The package is shipped with a NAMESPACE ﬁle, and a unit testing framework is
implemented through the package RUnit (see Burger et al. 2010). Furthermore,
fCopulae offers graphical user interfaces written in tcl/TK for displaying the impact
of alternative copula parameter/settings. A drawback of the package is that only two-
dimensional copulae are implemented, but the functions that address the skewed
normal ([dpr]mvsnorm()) and skewed Student’s t ([dpr]mvst()) multivariate
distributions are not conﬁned to this case. Incidentally, data can be ﬁtted to these
multivariate distributions by the function mvfit().

MODELLING DEPENDENCE
141
Similarly to the package copula, the copulae in the package fCopulae are
classiﬁed into three categories: Archimedean, elliptical and extreme value. With re-
gard to the Archimedean copulae a total of 22 variants are available. The density and
distribution functions thereof and the generation of random numbers can be accessed
with the functions [dpr]archmCopula(). As stated in the previous paragraph,
interactive displays of the density, the distribution function and random variates can
be generated with the functions [dpr]archmCopula(), respectively. Because of its
importance, the density, the distribution function and the generation of random vari-
ates of the Gumbel copula can be accessed by calling [dpr]gumbelCopula(). As
measures of association, Kendall’s tau, Spearman’s ρ and the tail coefﬁcient can be
computed with the functions archmTau(), archmRho() and archmTailCoeff(),
respectively. The latter can be plotted with the function archmTailPlot(). The
ﬁtting and simulation of Archimedean copulae can be conducted with the functions
archmCopulaFit() and archmCopulaSim(). In contrast to the package copula,
the optimization routine employed for estimating the unknown copula parameters
is nlminb(). The ellipsis argument of archmCopulaFit() is passed down to this
optimizer.
In contrast to the package copula, the Cauchy copula is available in addition
to the normal and Student’s t copulae, although all of these distribution-based
copulae are conﬁned to the two-dimensional case. The nomenclature for the
density and distribution function and the generation of random variates follows
the R naming convention in the same way as for the Archimedean copulae,
namely, [dpr]ellipticalCopula(). Interactive displays thereof can be created
through
a
tcl/TK
graphical
user
interface
which
is
started
by
calling
[dpr]ellipticalSlider(). The type of elliptical copula is selected by the type
argument pertinent to these functions. Similarly, Kendall’s tau, Spearman’s ρ and
the tail coefﬁcient are computed by the functions ellipticalTau(), ellipti-
calRho() and ellipticalTailCoeff(). For the latter measure of association a
plot can be displayed via ellipticalTailPlot(). Needless to say, the tail coefﬁ-
cient is zero in the case of a normal copula. Bivariate elliptical copulae can be ﬁtted
and/or simulated with the functions ellipticalCopulaFit() and elliptical-
CopulaSim(), respectively. Within the former function nlminb() is utilized as the
optimizer and the ellipsis argument of the ﬁtting function is passed down to it.
For extreme value copulae the same naming convention and set of functions are
used as in the cases of Archimedean and elliptical copulae. The acronym used for
this class of copulae is ev; for example, the functions [dpr]evCopula() refer to
the density and distribution function and the generation of random variates. The
package fCopulae implements not only the copulae available in copula but also the
Galambos, Husler–Reis, Tawn and BB5 copulae.
9.4.4
The package gumbel
The package gumbel is dedicated solely to the Gumbel copula (see Caillat et al.
2008). It is contained in the CRAN ‘Distribution’ Task View. The package is writ-
ten purely in R and is accompanied by a vignette (in French only). Neither S3

142
RISK MODELLING
nor S4 classes and methods are employed, but the functions are exported via the
NAMESPACE mechanism. In addition to the density and distribution function and the
generation of random variates, the generating function of the Gumbel copula and its
inverse are available. For the former group of functions the R naming convention
is followed (i.e., [dpr]gumbel()). The generating function and inverse generating
function are named phigumbel() and invphigumbel(), respectively. A strength
of the package is the inclusion of functions for estimating the unknown parameters
of the Gumbel copula and the margins through the methods of maximum likeli-
hood (gumbel.EML()), canonical maximum likelihood (gumbel.CML()), inference
for margins (gumbel.IFM()) and a moment-based method (gumbel.MBE()). The
functions optim() and optimize() are utilized for numerically maximizing the
objective functions. Unfortunately, only the estimates are returned as a vector object,
hence statistical inference is not possible. An additional drawback of the implemen-
tation is that the marginal distributions must be either exponential or gamma for all
estimation methods except canonical maximum likelihood.
9.4.5
The package QRM
The package QRM was introduced in Section 6.4.4 and encountered again in Sec-
tion 7.3.7. For our present purposes, it contains functions for the Frank, Gumbel,
Clayton, normal and Student’s t copulae. Here, the densities and the generation of
random variates are implemented. The ﬁtting of Archimedean copulae is conducted
with the function fit.AC(). Numerical maximization of the likelihood is accom-
plished by nlminb(). The function returns a list object with the maximized value
of the log-likelihood, the estimated coefﬁcients, the standard errors thereof and a
logical ﬂag indicating whether or not the algorithm converged. The parameters of
the elliptical normal and Student’s t copulae are estimated by means of the func-
tions fit.gausscopula() and fit.tcopula(), respectively. It is advantageous
to employ the function fit.tcopula() for higher-dimensional data by setting the
argument method to either Kendall or Spearman when a t copula is used. In these
cases a two-step estimation is carried out whereby ﬁrst the rank correlations are deter-
mined, and then the optimization is done only with respect to the degrees-of-freedom
parameter ν.
9.5
Empirical applications of copulae
9.5.1
GARCH–copula model
Motivation
The stylized facts of daily returns were set out in Part I of this book. In particular, it
was stated that the assumptions of an i.i.d. and normal return process are generally not
met in practice. The stylized facts regarding ‘fat tails’ and ‘volatility clustering’ can
be viewed as the ﬂip-side of these violated assumptions. As should be evident from
this chapter the appropriate determination of the dependence between the ﬁnancial

MODELLING DEPENDENCE
143
instruments in a portfolio is of pivotal importance. It has been shown that the variance–
covariance or correlation matrix is only appropriate for elliptically distributed random
variables. This assumption is similarly detrimental to the observed stylized facts for
multivariate ﬁnancial return series as is the i.i.d. assumption for the univariate case.
Hence, the utilization of the copula concept has been advocated in this chapter.
That said, the question of the kind of model to employ in simultaneously address-
ing the univariate and multivariate stylized facts for market returns in the portfolio
context arises. We are now in a position to put together the pieces from the previous
chapters of this part of the book, that is, to combine GARCH and copula models.
Incidentally, the modelling of the marginals could in principle also be achieved by
applying extreme value theory or a generalized hyperbolic or generalized lambda
distribution, but the advantages of employing volatility models instead are striking if
one contrasts the GARCH–copula model with a ‘plain vanilla’ variance–covariance
approach and the assumption of normally distributed returns:
1. GARCH models possess a higher kurtosis than the normal distribution, hence
the higher probability of witnessing extreme returns can be captured with these
volatility models.
2. Risk measures based on the variance–covariance approach are unconditional in
nature and therefore imply an i.i.d. process. It is therefore impossible to model
volatility clusters. The GARCH models speciﬁcally capture this empirical
artefact and allow the computation of conditional risk measures.
3. Quite often a risk assessment for the next 10 days is desired. This is a require-
ment of the Basel II Accord (see Basel Committee on Banking Supervision
2006, p. 195). In practice, the standard deviation derived from daily returns
is used and scaled up by a factor of
√
10. The so-called ‘square-root-of-time’
scaling is explicitly mentioned in the document just cited , but in a more
recent draft it has only explanatory status (see Basel Committee on Banking
Supervision 2008). However, this kind of scaling can result in a severely
underestimated volatility if it is not applied to a homoscedastic process (see,
for instance, Danielsson and Zigrand 2004). In contrast, outright 10-day
forecasts of the volatility can be derived from GARCH models.
4. If the variance–covariance approach with the assumption of normally dis-
tributed portfolio returns/losses is employed, then one implicitly also assumes
that all marginal distributions are of the same type. One cannot exclude this
possibility per se, but it is unlikely to be met in empirical applications. The
GARCH–copula approach explicitly enables the use of different distribu-
tions/models for the margins, thus providing great ﬂexibility.
5. To reiterate, only in the case of elliptical distributions do the correlations
correctly depict the dependence between the random variables. But in prac-
tice the assumption of jointly elliptically distributed random variables seldom
holds, hence the dependencies are depicted wrongly. The copula approach

144
RISK MODELLING
circumvents this problems by distinguishing between the dependence
structure on the one hand and the modelling of the marginal distributions
on the other hand.
Description
The GARCH–copula approach was probably ﬁrst introduced by Rockinger and
Jondeau (2001). This combination of models has since been used not only for the
modelling of portfolio risk, but also in the domain of ﬁnancial derivatives. For in-
stance, Patton (2006), Jondeau and Rockinger (2006), Micocci and Masala (2005),
Embrechts and Dias (2003), Hsu et al. (2008) and Chiou and Tsay (2008) have uti-
lized this approach in their empirical research. The GARCH–copula model rests on
the concept of ‘inference for margins’. Here the dependence between ﬁnancial risk
factors is deduced from the marginal distributions which are the speciﬁed GARCH
models.
GARCH models were introduced in Chapter 8. We now focus on the expecta-
tion and variance equations for ARMA-GARCH models for processes of the form
(Xk,t)k∈N,t∈Z:
Xk,t = μk,t + εk,t,
(9.28)
μk,t = μk +
p1,k

i=1
φi(Xk,t−i −μk) +
q1,k

j=1
θ jεk,t−j,
(9.29)
εk,t = σk,t Zk,t,
(9.30)
Zk,t ∼Dk,ν(0, 1),
(9.31)
σ 2
k,t = α0 +
p2,k

i=1
αiε2
k,t−i +
q2,k

j=1
β jσ 2
k,t−j,
(9.32)
where (Zk,t)k∈N,t∈Z is a Dk,ν-distributed random variable with expected value 0 and
variance 1, and Xk denotes the return/loss of the kth ﬁnancial instrument contained in
the portfolio, k = 1, . . . , K. Assumptions about the distributions thereof are needed.
Potential candidates are the standard normal, the Student’s t and its skewed ver-
sion. Estimates for the unknown model parameters, ζ k = (μk, φk, θk, αk, βk, νk)′,
k = 1, . . . , K, could then be computed by applying the ML principle. It should again
be stressed that the speciﬁcations of the GARCH models chosen for each ﬁnan-
cial return/loss series can differ. This is not conﬁned to the order of the GARCH
model, but applies equally to the distribution assumed for the innovations Zk,t. Es-
timates for the standardized residuals of these GARCH models can be computed as
ˆzk,t = (xk,t −ˆμk,t)/ ˆ
σk,t.
The joint distribution of the K processes can be expressed with respect to a copula
C as follows:
F(x1, . . . , xK; ξ) = C(F1(x1; ζ 1), . . . , FK(xK; ζ K)).
(9.33)
Here ξ denotes the parameter vector of the copula C and F1(·), . . . , FK(·) are the
marginal distributions. If the k-dimensional density function of the copula, c, exists,

MODELLING DEPENDENCE
145
then the joint density can be expressed as the product of the marginal distributions
f1(·), . . . , fK(·) with density
f (x1, . . . , xK; ξ) = c(F1(x1; ζ 1), . . . , FK(xK; ζ K); ξ)
K

i=1
fi(xi; ζ i),
(9.34)
c(u1, . . . , uK) = δKc(u1, . . . , uK)
δu1, . . . , δuK
.
(9.35)
In principle it would be possible to estimate the unknown parameters by setting
up the pseudo-log-likelihood and maximizing it. However, this approach can be
computationally burdensome and time-consuming. Instead, a two-step estimation is
carried out in practice. First, the parameters of the GARCH models are estimated and
the standardized residuals, ˆz1, . . . , ˆzK, are determined. These are then transformed
into pseudo-uniform variables by employing either the assumed distribution functions
or their empirical distributions. The former option is called parametric IFM and the
latter semi-parametric IFM, for obvious reasons. The log-likelihood to be maximized
is then given by
L(ξ; ˆz1, . . . , ˆzk) =
T

i=m
log c(F1(ˆz1,i, . . . , F1(ˆzK,i); ξ)),
(9.36)
where m = max(pi,k, qi,k) for i = 1, 2 and k = 1, . . . , K. The estimator for maxi-
mizing this likelihood has the properties of consistency and asymptotic normality, if
the data can be assumed to be independent and identically normally distributed (see
Genest et al. 1995). This assumption can be accepted if the standardized residuals
are employed.
Therefore, the GARCH–copula approach for determining portfolio risks consists
of the following steps:
1. Specify and estimate the GARCH models for each loss factor.
2. Determine the standardized residuals.
3. Calculate the pseudo-uniform variables from the standardized residuals:
r either parametrically from the assumed distributions for the GARCH error
processes,
r or semi-parametrically from the empirical distribution functions.
4. Estimate the copula model.
5. Use the dependence structure determined by the estimated copula for gen-
erating N data sets of random variates for the pseudo-uniformly distributed
variables.
6. Compute the quantiles for these Monte Carlo draws.
7. Use these quantiles in conjunction with the weight vector to calculate the N
portfolio return scenarios.

146
RISK MODELLING
8. Finally, use this series in the calculation of the desired risk measure for a given
conﬁdence level.
Application
The GARCH–copula framework is now applied to a ﬁctional passive portfolio con-
sisting of European stocks and the 95% daily expected shortfall is computed. The
EuStockMarkets data set is used, which ﬁrst appears in Chapter 3. It is assumed
that the portfolio consists of 40% DAX, with the remaining 60% distributed equally
between the FTSE, SMI and CAC.
The R code is shown in Listing 9.1. First, the packages QRM and fGarch are
loaded into the work space. Then the data is retrieved and the losses are expressed
as continuous percentages. In lines 7–9, the GARCH models for the marginals are
Listing 9.1 GARCH–copula model: expected shortfall.
library(QRM)
1
library(fGarch)
2
## Losses
3
data(EuStockMarkets)
4
loss <−as.data.frame(na.omit(−1.0 ∗diff(log(EuStockMarkets)) ∗
5
100.0))
6
## GARCH
7
gﬁt <−lapply(loss, garchFit, formula = garch(1,1),
8
cond.dist = "std", trace = FALSE)
9
gprog <−unlist(lapply(gﬁt, function(x)
10
predict(x, n.ahead = 1)[3]))
11
gshape <−unlist(lapply(gﬁt, function(x) x@ﬁt$coef[5]))
12
gresid <−as.matrix(data.frame(lapply(gﬁt,
13
function(x) x@residuals / sqrt(x@h.t))))
14
## Copula
15
U <−sapply(1 : 4, function(y) pt(gresid[, y], df = gshape[y]))
16
cop <−ﬁt.tcopula(Udata = U, method = "Kendall")
17
rcop <−rcopula.t(100000, df = cop$nu, Sigma = cop$P)
18
qcop <−sapply(1 : 4, function(x) qstd(rcop[, x], nu = gshape[x]))
19
ht.mat <−matrix(gprog, nrow = 100000, ncol = ncol(loss),
20
byrow = TRUE)
21
pf <−qcop ∗ht.mat
22
## ES 95 percent
23
weights <−c(0.4, 0.2, 0.2, 0.2)
24
pfall <−(qcop ∗ht.mat) %∗% weights
25
pfall.es95 <−median(tail(sort(pfall), 5000))
26
pfall.es95
27

MODELLING DEPENDENCE
147
Table 9.1
Fitted GARCH(1, 1) models.
GARCH
Estimate
Std. error
t value
P(> |t|)
DAX
μ
−0.076
0.019
−4.046
0.000
ω
0.022
0.009
2.509
0.012
α1
0.079
0.016
4.886
0.000
β1
0.904
0.020
44.950
0.000
ν
6.038
0.814
7.418
0.000
SMI
μ
−0.114
0.018
−6.439
0.000
ω
0.058
0.019
3.062
0.002
α1
0.114
0.024
4.742
0.000
β1
0.822
0.039
21.079
0.000
ν
5.697
0.727
7.839
0.000
CAC
μ
−0.052
0.023
−2.234
0.025
ω
0.042
0.025
1.672
0.094
α1
0.044
0.016
2.822
0.005
β1
0.922
0.033
27.894
0.000
ν
7.986
1.364
5.856
0.000
FTSE
μ
−0.051
0.016
−3.128
0.002
ω
0.006
0.003
1.754
0.079
α1
0.036
0.009
3.790
0.000
β1
0.956
0.013
75.026
0.000
ν
9.526
1.787
5.331
0.000
estimated and the standardized residuals are computed. For the sake of simplicity
GARCH(1, 1) models with t-distributed innovation processes are speciﬁed for the
four markets. The task of estimating can be swiftly carried out with the lapply()
function. The object gfit is a list with elements of class fGARCH. The estimation
results are provided in Table 9.1.
As can be seen from this table, for each model all coefﬁcients are signiﬁcantly
different from zero and the stability requirement for GARCH(1, 1) models is not
violated, that is, α1 + β1 < 1. Furthermore, the magnitude of the estimates for the
degrees of freedom parameter ν indicate a pronounced deviation from normality, that
is, the heavy tail characteristic is reﬂected by these estimates.
Figure 9.2 shows the QQ plots of the standardized residuals. Apart from a few
outliers, it can be concluded that the assumption of a t-distributed error process does
hold in general.

148
RISK MODELLING
−4
−2
0
2
4
0
5
10
DAX
Theoretical Quantiles
Sample Quantiles
−6
−4
−2
0
2
4
6
−5
0
5
10
SMI
Theoretical Quantiles
Sample Quantiles
−4
−2
0
2
4
−4
0
4
8
CAC
Theoretical Quantiles
Sample Quantiles
−4
−2
0
2
4
−6
−2
2
FTSE
Theoretical Quantiles
Sample Quantiles
Figure 9.2
QQ plots for GARCH(1, 1) models.
Whether the chosen GARCH(1, 1) speciﬁcation is appropriate for each of the
loss factors can be graphically inspected by the ACF of the squared residuals. These
are portrayed in Figure 9.3. In neither of the ACF plots does a signiﬁcant spike at
the 95% conﬁdence level occur, hence it can be concluded that the GARCH models
assumed for the marginal distributions appropriately capture the characteristics of the
loss factors.
0
5
10
15
20
0.0
0.4
0.8
Lag
ACF
DAX
0
5
10
15
20
0.0
0.4
0.8
Lag
ACF
SMI
0
5
10
15
20
0.0
0.4
0.8
Lag
ACF
CAC
0
5
10
15
20
0.0
0.4
0.8
Lag
ACF
FTSE
Figure 9.3
Autocorrelations of squared standardized residuals.

MODELLING DEPENDENCE
149
In lines 10–14 of the R code the conditional volatilities are computed, and the
estimates for the degrees-of-freedom parameters and the standardized residuals are
extracted. All of these tasks can be swiftly accomplished by utilizing lapply(). The
one-step-ahead forecasts of the conditional variance will be used in the computation
of the portfolio loss variates, and both of these are needed for the calculation of the
pseudo-uniform variables. These are obtained with the expression in line 13. Next,
a Student’s t copula is estimated based on Kendall’s rank correlations. The result is
assigned to the object cop, which is then used in the generation of 100 000 random
sets. The spectrum of simulated losses for each ﬁnancial instrument is determined in
line 20. The volatility forecasts are repeated for each simulation of random sets and
the Hadamard product of these two matrices is computed. The simulated portfolio
losses are then determined as the outcome of the matrix–weight vector product (i.e.,
object pfall). This item is then sorted by size and the expected shortfall at the 95%
conﬁdence level is expressed as the median of the 5000 largest losses, which takes
a value of 2.573. Incidentally, it has been implicitly assumed that the 1-day mean
returns/losses are zero.
9.5.2
Mixed copula approaches
The Clayton and Gumbel copulae were introduced in Section 9.3.3. With these
Archimedean copulae either lower or upper tail dependence can be modelled, but
not both. In principle, this can be rectiﬁed by using the Joe–Clayton copula (see Joe
1997),
C JC
α
= 1 −{1 −[(1 −¯uk
1)−γ + (1 −¯uk
2)−γ −1]−1/γ }1/k,
(9.37)
with ¯ui = 1 −ui, for i = 1, 2, and α = (k, γ )′. This copula has lower tail dependence
equal to λl = 2−1/γ and upper tail dependence equal to λu = 2 −21/k. For k = 1, the
Joe–Clayton copula reduces to the Clayton copula, and for γ →0, it approaches the
Gumbel copula. The Joe–Clayton copula is characterized by an increasing upper tail
dependence for increasing parameter values k. However, this copula does not account
for extreme events. Because a linear combination of two Archimedean copulae is an
Archimedean copula (see Nelsen 2006), one can create a mixture of the Clayton and
Gumbel copula in the form
CCG
ξ
= πCC
δ (u1, u2) + (1 −π)CG
θ (u1, u2)
(9.38)
where π ∈(0, 1) is the weighting coefﬁcient between these two copulae. The tail
dependencies for this copula are given by λl = π2−1/δ for the lower and (1 −π)(2 −
21θ) for upper coefﬁcient, respectively.
In Listing 9.2 the relevant code lines for estimating a mixed copula model are
exhibited. First, the necessary packages are loaded into the work space. Here, the
functions and methods contained in the package copula will be used for creating
the log-likelihood consisting of the weighted densities of the Clayton and Gumbel
copulae. The function returnseries() contained in the package FRAPO will be
used for calculating the discrete returns expressed as percentages, and the data set
will be taken from the package QRM, as will the function edf() for determining

150
RISK MODELLING
Listing 9.2 Mixing of copulae: Clayton and Gumbel.
library(copula)
1
library(FRAPO)
2
library(QRM)
3
## Retrieving data and edf
4
data(DJ.df)
5
Data <−DJ.df[, c("GM", "UTX")]
6
R <−returnseries(Data, method = "discrete", trim = TRUE)
7
U <−apply(R, 2,edf)
8
## Initialising copula objects
9
copC <−claytonCopula(2)
10
copG <−gumbelCopula(2)
11
## Objective function
12
LLCG <−function(params, x, copC, copG){
13
slot(copC, "parameters") <−params[1]
14
slot(copG, "parameters") <−params[2]
15
pi <−params[3]
16
opt <−sum(log(pi ∗dcopula(copC, x) + (1 −pi) ∗
17
dcopula(copG, x)))
18
opt
19
}
20
## Parameter bounds and initialisation
21
lower <−c(copC@param.lowbnd, copG@param.lowbnd, 0)
22
upper <−c(copC@param.upbnd, copG@param.upbnd, 1)
23
par1 <−copula ::: ﬁtCopula.itau(copC, U)@estimate
24
par2 <−copula ::: ﬁtCopula.itau(copG, U)@estimate
25
par3 <−0.5
26
## Optimization
27
opt <−optim(c(par1, par2, par3), LLCG, x = U, copC = copC,
28
copG = copG, lower = lower, upper = upper,
29
method = "L−BFGS−B",
30
control = list(fnscale = −1, trace = 2),
31
hessian = TRUE)
32
## Variance−covariance
33
varcov <−round(solve(−opt$hessian), 4)
34
the pseudo-uniform variables of the stock returns for GM and UTX. In the next block
of statements, two copula objects are initialized. The chosen parameters for δ and θ
are arbitrarily set to 2 and will be updated during the numerical minimization of the
negative log-likelihood. Next comes the deﬁnition of the objective function LLCG().
Its arguments are params for the three-element parameter vector, x for the matrix of
pseudo-uniform variables, and copC and copG for the copula objects deﬁned earlier

MODELLING DEPENDENCE
151
Table 9.2
Fitted mix of Clayton and Gumbel.
Clayton–Gumbel
ˆδ
ˆθ
ˆπ
Estimated values
0.5586
1.1316
0.4503
Standard errors
0.2227
0.0469
0.1411
for the Clayton and Gumbel copulae, respectively. In the subsequent chunk of code
the lower and upper bounds of the parameter space are set and initial values for δ
and θ are determined according to the rank correlations. The minimization of the
negative log-likelihood is carried out by means of optim(). Because the value of
the log-likelihood is returned by LLCG(), the control argument fnscale must be
set to −1. In addition, for better tracing of the numerical optimization, the control
argument trace has been set to 2.
The parameter estimates and standard errors are shown in Table 9.2. The estimated
value of π implies an almost equal weighting between the Clayton and Gumbel
copulae and is signiﬁcantly different from zero. From the estimates for δ and θ, the
tail dependence coefﬁcients are 0.13 for the lower and 0.802 for the upper.
References
Basel Committee on Banking Supervision 2006 International Convergence of Capital Mea-
surement and Capital Standards: A Revised Framework Bank for International Settlements
Basel.
Basel Committee on Banking Supervision 2008 Proposed Revisions to the Basel II Market
Risk Framework Bank for International Settlements Basel.
Black F. and Litterman R. 1990 Asset allocation: combining investor views with market
equilibrium. Technical report, Goldman Sachs Fixed Income Research.
Chiou S. and Tsay R. 2008 A copula-based approach to option pricing and risk assesment.
Journal of Data Science 6, 273–301.
Danielsson J. and Zigrand J. 2005 On time-scaling of risk and the square-root-of-time rule efa
2004 maastricht meetings paper no. 5399 edn London School of Economics London, UK.
Embrechts P. and Dias A. 2003 Dynamic copula models for multivariate high-frequency data
in ﬁnance Discussion Paper, Department of Mathematics, ETH Zurich.
Genest C. and Favre AC. 2007 Everything you always wanted to know about copula modeling
but were afraid to ask. Journal of Hydrologic Engineering pp. 347–368.
Genest C., Ghoudi K. and Rivest LP. 1995 A semiparametric estimation procedure of depen-
dence parameters in multivariate families of distributions. Biometrika 82(3), 543–552.
Gochez F. 2010 BLCOP: Black-Litterman and copula-opinion pooling frameworks. R package
version 0.2.6.
Hofert M. and Maechler M. 2011 Nested Archimedean copulas meet R: The nacopula package.
Journal of Statistical Software 39(9), 1–20.

152
RISK MODELLING
Hsu C., Wang Y. and Tseng C. 2008 Dynamic hedging with futures: A copula-based GARCH
model. Journal of Futures Markets 28, 1095–1116.
Joe H. 1997 Multivariate Models and Dependence Concepts number 73 in Monographs on
Statistics and Applied Probability. Chapmann & Hall, London.
Joe H. and Xu J. 1996 The estimation method of inference functions for margins for multivariate
models. Technical Report 166, University of British Columbia, Department of Statistics.
Jondeau E. and Rockinger M. 2006 The copula-GARCH model of conditional dependencies:
An international stock marktet application. Journal of International Money and Finance 25,
827–853.
Kojadinovic I. and Yan J. 2010 Modeling multivariate distributions with continuous margins
using the copula R package. Journal of Statistical Software 34(9), 1–20.
McNeil A., Frey R. and Embrechts P. 2005 Quantitative Risk Management: Concepts, Tech-
niques and Tools. Princeton University Press, Princeton, NJ.
Meucci A. 2006a Beyond Black-Litterman in practice: A ﬁve-step recipe to input views on
non-normal markets. Risk 19(9), 114–119.
Meucci A. 2006b Beyond Black-Litterman: Views on non-normal markets. Risk 19(2), 96–102.
Meucci A. 2010 The Black-Litterman approach: Original model and extensions In The En-
cyclopedia of Quantitative Finance (ed. Cont R. and Tankov P.) John Wiley & Sons, Inc.
Hoboken, NJ.
Micocci M. and Masala G. 2005 Backtesting value-at-risk estimation with non Gaussian
marginals Working Paper, 8th International Congress on Insurance: Mathematics & Eco-
nomics, Rome.
Nelsen R. 2006 An Introduction to Copulas number 139 in Lecture Notes in Statistics 2nd edn.
Springer-Verlag, New York.
Patton A. 2006 Modelling asymmetric exchange rate dependence. International Economic
Review 47(2), 527–556.
Rockinger M. and Jondeau E. 2001 Conditional dependency of ﬁnancial series: An application
of copulas. Notes d’´Etudes et de Recherche NER 82, Banque de France, Paris.
Schweizer B. and Sklar A. 1983 Probabilistic Metric Spaces. North-Holland/Elsevier,
New York.
Shih J. and Louis T. 1995 Inferences on the association parameter in copula models for bivariate
survival data. Biometrics 51, 1384–1399.
Sklar A. 1959 Fonctions de r´epartition `a n dimensions et leurs marges. Publications de l’Institut
de l’Universit´e de Paris 8, 229–231.
W¨urtz D. 2009 fCopulae: Rmetrics – dependence structures with copulas. R package version
2110.78.
W¨urtz D., Chalabi Y., Chen W. and Ellis A. 2010 Portfolio Optimization with R/Rmetrics. Rmet-
rics Association & Finance Online, www.rmetrics.org. R package version 2130.80.
Yan J. 2007 Enjoy the joy of copulas: With a package copula. Journal of Statistical Software
21(4), 1–21.

Part III
PORTFOLIO
OPTIMIZATION
APPROACHES

10
Robust portfolio optimization
10.1
Overview
Markowitz portfolios and their application in R were introduced in Chapter 5. The
problems that one might encounter by directly optimizing portfolios of this kind
were addressed. In particular, it was stated that the use of sample estimators for
the expected returns and the covariance matrix can result in sub-optimal portfolio
results due to estimation error. Furthermore, extreme portfolio weights and/or erratic
swings in the asset mix are commonly observed in ex post simulations. In general,
this empirical fact is undesirable because of transaction costs. From a statistical point
of view, these artefacts can mainly be attributed to the sensitivity of the ordinary
sample estimators with respect to outliers. These outlying data points inﬂuence the
dispersion estimates to a lesser extent than the means, ceteris paribus. Hence, but
not only for this reason, minimum-variance portfolios are advocated compared to
mean–variance portfolios (see the references in Chapter 5). It would therefore be
desirable to have estimators available which lessen the impact of outliers and thus
produce estimates that are representative of the bulk of sample data, and/or opti-
mization techniques that incorporate estimations errors directly. The former can be
achieved by utilizing robust statistics and the latter by employing robust optimization
techniques.
In the next two sections these two means are presented from a theoretical point
of view. Then the sources in R are discussed. The chapter concludes with empirical
applications in the form of a Monte Carlo simulation and back-test comparisons,
where these robust portfolio optimizations are compared to portfolio solutions based
on ordinary sample estimators.
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

156
PORTFOLIO OPTIMIZATION APPROACHES
10.2
Robust statistics
10.2.1
Motivation
It has already been pointed out in Chapter 3 that the normality assumption quite often
does not hold for ﬁnancial market return data (see also Pfaff 2010, Chapter 1). The
violation of this assumption is justiﬁed on empirical grounds by the stylized facts for
single and multivariate returns. But it was also shown in Chapter 6 that the normality
assumption is violated to a lesser extent the lower the data frequency is (see also Pfaff
2010, Chapter 2).
It is fair to say that in empirical work the arithmetic mean and the sample
covariance are widely employed in estimating the corresponding theoretical location
and dispersion moments of the population. These estimators can be derived from the
ML principle and are asymptotically efﬁcient and consistent. Furthermore, they are
asymptotically normally distributed. However, when the distributional assumption is
not met, the estimators lose their desirable properties. In fact, the arithmetic mean –
as an estimator for the location of a population – is sensitive to extreme observations,
such that the estimate does not reﬂect the bulk of the data well. On a similar note,
the dependence between two random variables can be highly distorted by a single
outlying data pair. In light of this, it would be desirable to have recourse to methods
and techniques that are relative immune to such outliers and/or to violations of
the underlying model assumptions, but could still deliver reasonable results for the
majority of observations in a sample. The ﬁeld of robust statistics deals with problems
of this kind and offers solutions in the form of robust estimators and inference based
upon these. Textbook expositions can be found, for instance, Huber (1981), Hampel
et al. (1986), Rousseeuw and Leroy (1987), Staudte and Sheather (1990) and Maronna
et al. (2006).
Formerly, the outlier problem sketched above would be resolved by means of
trimming (removing of outliers) or winsorizing (equalizing extreme observations
to a ﬁxed quantile value). Indeed, both methods can be considered as means of
robustiﬁcation. Unfortunately, the outcome of these procedures is much dependent on
the subjective choice of the researcher. The median could perhaps be employed as an
estimator for the location and the mean absolute deviation for the scale. Both produce
more robust estimates than the ordinary sample estimators of the arithmetic mean
and standard deviation. The advantage of these robust estimators is that they avoid
specifying in advance which data points are viewed as outliers. Hence, a subjective
element in the data analysis is removed. Before the most commonly encountered
robust statistics are introduced in the next subsection, it is worth mentioning that so
far the term ‘outlier’ or ‘extreme observation’ has not been precisely deﬁned. The
reason for this is simple: there is no clear-cut way to assess whether a data point is
an outlier or not. The question is always a relative one and crucially depends on the
underlying model/distribution assumption. For example, given the standard normal
distribution and a sample observation of 5, one could surely classify this data point
as an outlier. But would this classiﬁcation still hold when a Student’s t distribution
with 4 degrees of freedom or a Cauchy distribution is assumed?

ROBUST PORTFOLIO OPTIMIZATION
157
10.2.2
Selected robust estimators
The following presentation of selected robust estimators and their properties is based
on Maronna et al. (2006) and Todorov and Filzmoser (2009). The most commonly
utilized measure for assessing the robustness of an estimator is the breakdown point
(BP). This measure is deﬁned as the relative share of outliers in a sample such
that the estimator does not take an arbitrary large value. By deﬁnition, the BP can
take values between 0 and 0.5. The arithmetic mean has a BP of 0, because if
a single observation is replaced by one value, the location estimate can be made
arbitrarily high. In contrast, the median has a BP of 0.5. The upper bound of the BP
is explained by the fact, that if more than half of the observations are outliers, the
sample is falsiﬁed to a degree such that no inference with respect to the population
can be drawn from it. A further criterion for assessing the appropriateness of a robust
estimator is the relative efﬁciency (RE). Here the asymptotic variance of a robust
estimator is expressed relative to the variance of an optimal estimator which has been
derived under strict adherence to the model/distribution assumption. As such, it can
be interpreted as a percentage ﬁgure, indicating by how much the sample size has to
be increased such that the variances of the two estimators are equalized.
The class of M- and MM estimators
As early as 1964 the class of M-estimators was been introduced by Huber. The
class name should indicate the resemblance of this estimator to the ML principle
(see Huber 1964, 1981). It is convenient to restrict the following presentation of
the estimator to univariate samples. Recall that, according to the ML principle, the
unknown parameters, θ, are determined such that they have most likely produced a
given i.i.d. sample, {x1, . . . , xn}, drawn from the distribution F(x, θ) with density
function f (·). Due to the i.i.d. assumption, the joint distribution is equal to the product
of the marginals, which is maximized:
ˆθ = arg max
 n
i=1
f (xi, θ)

.
(10.1)
Because the logarithm is a strictly monotone transformation, the optimization is
generally carried out by minimizing the negative log-likelihood function:
ˆθ = arg min

−
n

i=1
log( f (xi, θ))

(10.2)
Analogously, the M-estimators are deﬁned as the minimum of a sum of functions
ρ(x, θ):
ˆθ = arg min
 n

i=1
ρ(xi, θ)

(10.3)
This broad class of estimators includes the ML principle (ρ(·) = −log f (x, θ))
as well as the method of least squares (LS). In the latter case the function ρ(·)

158
PORTFOLIO OPTIMIZATION APPROACHES
is the quadratic error. The function ρ(·) must meet the requirements of symmetry,
positive deﬁniteness and a global minimum at zero. Of course, the function should
provide decent estimates when the model/distribution assumptions are met and not be
negatively affected by instances of violation. The difference between the robust forms
of the M-estimators and those of the ML and LS principles is in the speciﬁcation of
ρ(·). For the former estimators extreme data points obtain a smaller weight and are
thus less inﬂuential with respect to the parameter estimates. Two kinds of speciﬁcation
for ρ(·) are usually employed in empirical work. First, there is the class of Huber
functions, deﬁned as
ρk(x) =
 x2
if |x| ≤k
2k|x| −k2
if |x| > k.
This function is quadratic in a central region around k and linear outside it. For k →∞
and k →0 the M-estimators are identical to the arithmetic mean and the median,
respectively. Second, the bi-square function proposed by Tukey is also encountered
in empirical work. It is given by
ρk(x) =
1 −[1 −(x/k)2]3
if |x| ≤k
1
if |x| > k.
This function is bounded for large absolute values of x. The bi-square function is
preferred to the Huber functions for symmetric distributions characterized by excess
kurtosis, because the inﬂuence of outliers can be completely suppressed.
Instead of directly optimizing equation (10.3), it is often easier to ﬁnd the solution
by equating the gradient ψ(x) = δρ(x, θ)/δθ to zero.
The class of MM estimators was originally introduced by Yohai (1987) and
Yohai et al. (1991) in the context of robust regression analysis. Lopuha¨a (1991, 1992)
adapted these estimators for multivariate data analysis. In a ﬁrst step, the dispersion of
the data is estimated by an M-estimator. Based upon this, a second M-type estimation
for the location parameters is carried out, which are in close accord with the variance–
covariance matrix of the ﬁrst step.
Estimators based on robust scaling
A general class of robust estimators is now presented that can be derived from robust
scaling, namely the minimum volume ellipsoid (MVE) estimator, the minimum co-
variance determinant (MCD) estimator and the S-estimator. A p-dimensional random
variable x = (x1, . . . , x p)′ ∈Rp is considered which is jointly normally distributed,
x ∼N(μ, ). The location vector is given by μ = E(x) = (E(x1), . . . , E(x p))′ and
the covariance matrix by  = Var(x) = E((x −μ)(x −μ)′). The distance measure is
the squared Mahalanobis distance between the sample and the location and dispersion
parameters: d(x, μ, ) = (x −μ)′−1(x −μ). The trivial solution of zero distance
can be achieved if the smallest eigenvalue of an estimated covariance matrix ˆ tends
to zero. In order to exclude this trivial solution, | ˆ| = 1 is required, where | · | denotes

ROBUST PORTFOLIO OPTIMIZATION
159
the determinant of a matrix. The three above-mentioned robust estimators for ˆμ and
ˆ are based on the optimization
arg min ˆσ

d(X, ˆμ, ˆ)

,
(10.4)
where X is the full sample and d is the vector of distances d(xi, ˆμ, ˆ) for i =
p + 1, . . . , N and ˆσ is a robust metric.
The MVE and MCD estimators were introduced by Rousseeuw (1985) and
Rousseeuw and Leroy (1987). In the case of the MVE estimator the smallest data
cloud is determined such that at least half of the sample observations is included. This
is achieved by utilizing the median of ˆσ. The convergence rate for this estimator is
only N −1/3. In the case of the MCD estimator the robust subsample with size h > N/2
is chosen such that the determinant of the variance–covariance matrix is minimal. The
location vector is determined as the p arithmetic means of the selected data points
and the estimator for the variance–covariance matrix is adjusted by correction factors,
such that the resultant estimate ˆ follows the normal distribution. The BP is maximal
for h = int[(N + p + 1)/2], but any value in the interval [(N + p + 1)/2, N] can be
chosen. Davies (1987) introduced the class of S-estimators. The letter ‘S’ stands for
scale. With respect to the above optimization, an M-estimator for the scale according
to 1/N 	N
i=1 ρ(di) = δ is inserted, where ρ(·) must be a bounded function (e.g., the
bi-square function) and δ ∈(0, 1).
The Stahel–Donoho estimator
The Stahel–Donoho estimator (SDE) is due to Stahel (1981) and Donoho (1982).
According to this procedure, the degree of outlyingness is determined in terms of a
projection of the data matrix X with a vector a ∈Rp. This vector is deﬁned for given
observations x, robust location ˆμ and scale estimator ˆ with respect to the complete
data set X as
t(x, a) = x′a −ˆμ(Xa)
ˆσ(Xa)
.
(10.5)
The degree of outlyingness is given as the maximum of equation (10.5) for all vectors
a where the norms of these vectors equal 1. This solution is then used as weighting
scheme for X such that a non-increasing function t(xi) is applied.
The OGK estimator
One problem of robust estimators which possess the property of afﬁne invariance
is the resulting non-convex optimization. The estimator proposed by Maronna and
Zamar (2002) avoids this problem by estimating the covariances between two random
variables as
s jk = 1
4

σ

 Y j
σ(Y j) +
Yk
σ(Yk)
2
−σ

 Y j
σ(Y j) −
Yk
σ(Yk)
2
.
(10.6)

160
PORTFOLIO OPTIMIZATION APPROACHES
This form of dispersion estimation was introduced by Gnanadesikan and Kettenring
(1972). If the covariances are determined according to equation (10.6) the property of
afﬁne invariance is traded off against a simpler computation. However, if one applied
this dispersion estimator directly to the data pairs of X, the resulting variance–
covariance matrix would cease to be positive deﬁnite. For this reason, Maronna
and Zamar (2002) proposed an orthogonalization of X and hence the estimator is
termed orthogonalized Gnanadesikan–Kettenring (OGK). If a robust estimator for σ
is used for the pairwise covariances s jk, with j = 1, . . . , p and k = 1, . . . , p, then
the resulting variance–covariance matrix is also robust.
10.3
Robust optimization
10.3.1
Motivation
In this section robust approaches and techniques for optimizing portfolios will be pre-
sented. The previous section dealt with the utilization of robust rather than classical
estimators for the unknown location and dispersion parameters. In both cases the re-
sulting estimates – whether obtained by maximum likelihood or by robust estimation
techniques – have been taken as ﬁxed and would be used directly as mathematical
program inputs. However, in both instances these parameter values are subject to
uncertainty. In contrast, we will now present and investigate optimization approaches
whereby the parameter uncertainty is directly taken into account in the formulation
of the mathematical program. The term ‘robust’ is now deﬁned differently than in
the previous section. There it was used for point estimators that are less prone to
outliers when compared to the behaviour of the classical ML estimators. Now it is
deﬁned as an optimization technique that will produce a solution which is not nega-
tively impacted by an alternative parameter speciﬁcation – for example, if the return
expectations are turning out to be less favourable. Incidentally, robust optimization
techniques differ from stochastic optimization in the sense that the latter are based on
a speciﬁc distribution assumption for the parameters. This is not generally required
for robust optimization. In a nutshell, the aim of robust optimization is the deriva-
tion of an optimal solution for sets of possible parameter constellations. Extended
presentations of the approaches to tackling such problems are given in Ben-Tal and
Nemirovski (1998), Cornuejols and T¨ut¨unc¨u (2007), Fabozzi et al. (2007), Meucci
(2005, Chapter 9), Scherer (2010, Chapter 5) and T¨unt¨uc¨u and K¨onig (2004).
10.3.2
Uncertainty sets and problem formulation
The concept of robust optimization will now be elucidated for mean–variance port-
folios, although the approach is also applicable to other kinds of optimization. The
classical portfolio optimization is given by1
Pλ = arg min
ω∈
(1 −λ)
√
ω′ω −λω′μ,
(10.7)
1 Most of this subsection is based upon Sch¨ottle (2007, Chapter 4).

ROBUST PORTFOLIO OPTIMIZATION
161
where ω denotes the N × 1 portfolio weight vector and  ⊂{ω ∈RN|ω′1 = 1} is
the set of all allowable solutions. The (expected) returns of the N assets are contained
in the vector μ, with variance–covariance matrix  ∈RN×N which is assumed to
be positive deﬁnite. The parameter λ is allowed to take all values in the interval
[0, 1] and determines the weighting between the portfolio return and its risk. Of
course, the above problem formulation can be amended by further constraints, such
as a budget constraint (ω′1 = 1), a non-negativity constraint (ω ≥0) and/or bounds
on the weights for single assets or groups of assets (Aω ≤b). The mathematical
program in the above formulation includes the special cases of a minimum-variance
and a maximum-return portfolio if the values for λ are chosen λ = 0 or λ = 1,
respectively. All values of λ between these bounds yield portfolio solutions that lie
on the feasible efﬁcient frontier.
So far, point estimates for the unknown parameters (μ, ) have been utilized.
These can be derived from classical or robust estimators, as shown in the previous
section. Incidentally, it will be shown by means of simulation in Section 10.5 that
the latter class of estimators turn out to be beneﬁcial compared to the ML estimators.
However, regardless of whether the point estimates are obtained from classical or ro-
bust estimators, these point estimates are subject to uncertainty and in either case small
deviations from the true but unknown parameter values can result in quite distinct port-
folio solutions. Viewed from this angle, it would be desirable to include the parameter
uncertainties directly in formulation of the portfolio optimization. Because param-
eter uncertainty exerts a greater impact on the portfolio composition in the case of
expected returns compared to their dispersion, we will limit the following exposition
to the inclusion of uncertain return parameters only and treat the parameters pertinent
to portfolio risk as ﬁxed. So far, the term ‘uncertainty’ has been used rather vaguely.
For the purpose of applying robust optimization techniques this gap needs to be ﬁlled
with a concrete deﬁnition. This is achieved by deﬁning an uncertainty set U ˆμ for all
allowable parameter values. In principle, three different kind of sets are possible:
U ˆμ = {μ ∈RN| ˆμi, i = 1, . . . , M},
(10.8a)
U ˆμ = {μ ∈RN| | ˆμi −μi| ≤δi, i = 1, . . . , N},
(10.8b)
U ˆμ =

μ ∈RN| (μ −ˆμ)′ ˆ−1(μ −ˆμ) ≤δ2
T

.
(10.8c)
For the uncertainty set in equation (10.8a), M scenarios for the expected re-
turn vector μ must be speciﬁed. These can comprise subjective expectations and/or
location estimates derived from alternative estimators. In equation (10.8b) the un-
certainty set is deﬁned as intervals around the true, but unknown, return vector.
These bounds can be subjectively set, but can also be derived from a distributional
assumption. For instance, if it is assumed that each of the asset returns is normally
distributed, the central ﬂuctuation interval with 1 −α conﬁdence level is given as
δi = 
−1(α/2) · ˆσi/
√
T ), where 
−1 is the quantile function, ˆσi the standard devi-
ation of the returns for the ith asset and T denotes the sample size. The stylized
fact of an excess kurtosis of empirical return processes can be taken into account,
by utilizing a Student’s t distribution, for example. In this case, an ML estimation is

162
PORTFOLIO OPTIMIZATION APPROACHES
conducted ﬁrst, to obtain an estimate of the degrees-of-freedom parameter ν. Then
the quantile at the desired conﬁdence level is determined by using this empirical
estimate. It is worth mentioning that in the case of an uncertainty set as in equation
(10.8b) no dependencies between the uncertainty margins for the different assets are
taken into account, but rather these are treated as being independent of each other. For
the uncertainty set in equation (10.8c) an elliptical shaped uncertainty set is deﬁned.
In contrast to the uncertainty set in equation (10.8b), it is now assumed that the uncer-
tainties originate from a multivariate elliptical distribution; the covariances between
assets’ returns are now explicitly included in the uncertainty set. Last, but not least,
it should be pointed out that the ﬁrst and second uncertainty sets can be combined.
The scenarios (e.g., ML and robust based estimators for the unknown returns) would
then be given by M := { ˆμML, ˆμMC D, ˆμM, ˆμM M, ˆμMV E, ˆμS, ˆμSD, ˆμOGK} and an
elliptical shaped uncertainty set can be generated as
Uest = {μ ∈RN| (μ −¯μ)′ ¯−1(μ −¯μ) ≤¯δ2},
(10.9)
with
¯μ =
1
|M|

m∈M
m,
(10.10a)
¯ = diag(¯σ11, . . . , ¯σN N)
where ¯σii =
1
|M| −1

m∈M
(mi −¯μi)2, (10.10b)
¯δ = arg max
m∈M
(m −¯μ)′ ¯−1(m −¯μ).
(10.10c)
Having introduced alternative speciﬁcations for the uncertainty sets, it will next
be shown how these uncertainties can be expressed in a mathematical program. In
general and independent of the chosen form of the uncertainty set, a worst-case
approach is employed for solving optimization tasks robustly. This approach is also
known as a min-max approach. This tackles the question of what is the optimal
weight vector given the least favourable parameter constellation, that is, the smallest
portfolio return for a given risk level. Loosely speaking: expect the worst and you
will at least not be disappointed.
In the case of an uncertainty set as in equation (10.8a), in a ﬁrst step the weight
vectors for the M scenarios would be determined and then the one that yields the
lowest portfolio return would be selected. It should be pointed out that for this kind of
uncertainty set the determination of the optimal solution can become computationally
expensive and depends on the number of scenarios M. It should also be stressed that
all scenarios are treated as equally likely, but that the solution is determined solely by
the worst scenario. It can be deduced from this that the approach is very conservative
and that the optimal outcome can be highly inﬂuenced by single outliers for the
expected returns of the assets. Hence, a sound speciﬁcation of the M scenarios is of
the utmost importance.
Next, it is described how the problem for an uncertainty set of a symmetrical
interval around the location parameters μ as shown in equation (10.8b) can be

ROBUST PORTFOLIO OPTIMIZATION
163
implemented. The true but unknown subsequent return of the ith asset is contained in
the interval μi ∈[ ˆμi −δi, ˆμi + δi] for a given conﬁdence level. The least favourable
return for the ith asset is therefore given as μi = ˆμi −δi for a long position and as
μi = ˆμi + δi for a short position. These N conﬁdence intervals form a polyhedron
and can be expressed as a system of linear inequality constraints. However, it is
unknown beforehand whether an asset enters into the portfolio with a positive or
negative weight. To solve this problem, two slack variables, ω+ and ω−, are included
in the objective function:
PRλ = arg max
ω,ω+,ω−
ω′ ˆμ −δ′(ω+ −ω−) −(1 −λ)
√
ω′ω,
ω = ω+ −ω−,
ω+ ≥0,
ω−≥0.
(10.11)
Of course, the above problem formulation can be amended by a budget constraint
ω′1 = 1 and/or other constraints. During optimization for positive weights ω−> 0,
the returns are equal to ˆμi −δi and for negative weights the returns are set as ˆμi + δi.
In the ﬁrst case, the ith element of ω+ is positive and that of ω−is set equal to zero
according to the inequality constraint and vice versa. Assets with a higher return
uncertainty will obtain a lower portfolio weight than those for which the expected
return is more certain. The uncertainty of the expected returns can also be limited
to a subset U ⊂N of the assets. In this case, δi /∈U are set to zero. If long-only
constraints are included as side constraints, then the formulation of the mathematical
program reduces to that exhibited in equation (10.7), where the return vector is set as
ˆμ −δ with the additional non-negativity constraint ω ≥0.
Finally, the treatment of an elliptical uncertainty set for the returns as in equation
(10.8c) is elucidated. Analogously to the previous approaches, this problem is also
solved in the form of a worst-case approach:
PRλ = arg min
ω∈
arg max
μ∈U
(1 −λ)
√
ω′ω −λ(ω′μ)
(10.12)
If the returns are multivariate normally distributed, then (μ −ˆμ)′−1(μ −ˆμ) is
distributed as χ2 with N degrees of freedom. The scalar δ2 is then the corresponding
quantile value for a given conﬁdence level (1 −α). The stochastic return vector
μ therefore lies in the ellipse deﬁned by the level of conﬁdence. The maximal
distance between this uncertainty ellipsoid and the empirical location vector is now
determined, such that the returns correspond to the least favourable outcome. It is
evident from equation (10.12) that the uncertainty is conﬁned to the returns only
and that the variance–covariance matrix is taken as given. Therefore, in a ﬁrst step
the maximal distance can be determined by utilizing a Lagrange approach, where
ˆP = 1
T ˆ is the variance–covariance matrix of the empirical returns and γ denotes
the Lagrangian multiplier:
L(μ, γ ) = ω′ ˆμ −ω′μ −
γ
2 ( ˆμ −μ)′ ˆP−1( ˆμ −μ) −δ2
.
(10.13)

164
PORTFOLIO OPTIMIZATION APPROACHES
The optimal solution is then found by taking the partial derivatives of equation
(10.13) with respect to μ and γ and setting these to zero. This yields a system of two
equations, which is solved for μ:
μ = ˆμ −

δ

ω′ ˆPω
Pω

.
(10.14)
After left-multiplication by ω′ one obtains for the portfolio returns,
ω′μ = ω′ ˆμ −δ

ω′ ˆPω
ω′ ˆμ −δ∥ˆP1/2ω∥.
(10.15)
It is evident from this equation that the portfolio return is in the case of a robust
optimization with elliptical uncertainty smaller than the classical solution by the
term δ
√
ω′Pω. The square root of the quantile value can be interpreted as a risk
aversion parameter with respect to the uncertainty of the estimates. Substituting the
inner solution from equation (10.15) into the robust optimization speciﬁcation as in
equation (10.12), one obtains
PRλ = arg min
ω∈
arg max
μ∈U
(1 −λ)
√
ω′ω −λ(ω′μ)
= arg min
ω∈
(1 −λ)
√
ω′ω −λ(ω′μ) + λ δ
√
T

ω′ ˆω
= arg min
ω∈

1 −λ + λ δ
√
T

−λω′ ˆμ.
(10.16)
Equation (10.16) has the following implications:
r The efﬁcient frontier of a portfolio when optimized robustly under elliptical
uncertainty is, except for a shortening factor, the same as the efﬁcient frontier
of a classical mean–variance portfolio.
r The optimal weight vector for a minimum-variance portfolio is the same for
both kinds of optimization. This must be true by deﬁnition, because the uncer-
tainty has been limited to returns only.
With respect to the ﬁrst point, the risk–return trade-off parameter in equation
(10.16) is now expressed as θ. The equivalent trade-off parameter λ in the problem
formulation of equation (10.7) is then given by
λ :=
θ
1 + θ
δ
√
T
.
(10.17)
The deﬁned interval for θ ∈[0, 1] is now upper-bounded for the equivalent classical
mean–variance portfolios: λ ∈[0, 1/(1 + δ/
√
T )].
But the solution is also conservative in the case of an elliptical uncertainty set, in
the sense that the optimal portfolio weights correspond to a situation where for all
assets the least favourable returns are realized. The mathematical problem formulated
as in equation (10.16) can be expressed in the form of a second-order cone program

ROBUST PORTFOLIO OPTIMIZATION
165
(SOCP). SOCPs are solved by means of interior-point methods. More precisely, a
mathematical program is expressed as an SOCP if it can be written as
arg min
x
f′x
subject to ∥A jx + b j∥≤c′
jx + d j
for j = 1, . . . , J,
(10.18)
where J denotes the number of cone constraints. The SOCP formulation includes
linear programs (the A j would be null matrices and the b j would be null vectors),
quadratic programs and problems with hyperbolic constraints as well as problems
that can contain sums and maxima of vector norms (see Lobo et al. 1998). As already
implied by the name of this formulation, a quadratic cone optimization requires a
corresponding cone – as known as a Lorenz or ice-cream cone – with the property that
the ﬁrst element is at least as great as the Euclidean norm of its remaining elements.
Analytically, the cone is deﬁned as:
C = {x = (x1, . . . , xN) ∈RN : x1 ≥∥x2, . . . , xN∥}
(10.19)
The SOCP in equation (10.18) can also be expressed in its dual form:
arg max
z,w
−
J

j=1

b′
jz j + d jw j

subject to
J

j=1

A′
jz j + c jw j

= f,
∥z j∥≤w j for j = 1, . . . , J.
(10.20)
Here, z ∈Rn j−1 and w ∈RJ denote the optimization variables. Like its primal form,
this a convex program, since the objective function to be maximized is concave and
the constraints are convex. Further details about these kinds of optimization can be
found, for instance, in Nesterov and Nemirovsky (1994) or Boyd and Vandenberghe
(2004).
Now, with regard to the task of bringing the problem formulation as in equation
(10.12) into the SOCP form, one deﬁnes as slack variable for the unknown worst-
case portfolio return t = μ′ω. The vector x in equations (10.18) and (10.19) is then
x = (t, ω)′ and the problem can be written as
arg min
t,ω
t
subject to t ≤ˆμ′ω −δα∥P1/2ω∥,
σmax ≥∥1/2ω∥.
(10.21)
The ﬁrst inequality constraint is the cone constraint and the second is a quadratic
constraint with respect to the portfolio risk. Of course, the problem speciﬁcation
in equation (10.21) can be amended by other linear constraints, such as budget,
non-negativity and/or bound and group constraints.

166
PORTFOLIO OPTIMIZATION APPROACHES
10.4
Synopsis of R packages
In this section, only packages that are partly or wholly dedicated to robust estimation
methods in the context of multivariate data analysis are presented. Thus, R packages
that cover robust regression, robust time series analysis and other robust topics are
not included in this synopsis. The reader is referred to the CRAN ‘Robust’ Task View
for an overview and brief description of these packages.
10.4.1
The package covRobust
The package covRobust was written by Wang et al. (2003). It is contained in the
CRAN ‘Robust’ and ‘Multivariate’ Task Views. The package consists of the function
cov.nnve() only, which is an implementation of the covariance and location esti-
mator introduced by Wang and Raftery (2002). Outliers or groups thereof are detected
by a nearest neighbour (NN) procedure and hence the function’s sufﬁx is an acronym
for ‘nearest neighbour variance estimator’. The function is speciﬁed with the two
main arguments datamat for the matrix of sample observations and k for choosing
the number of neighbours to be considered, with a default value of 12. The remaining
arguments can be used to control the optimization procedure. The function returns a
list object with elements for the robust covariance estimate (cov), the robust mean
vector (mu) and the posterior probabilities (postprob), that is, the odds that data
points will have to be considered as outliers. The list element classification
contains the (0, 1) discretized posterior probabilities. Finally, the initial results of the
NN cleaning are contained in the list element innc, which itself is a list with the four
above-named elements.
10.4.2
The package fPortfolio
The package fPortfolio is a powerful R package for conducting many different kinds
of portfolio optimization task. It is part of the Rmetrics bundle of packages. It is
considered as a core package in the CRAN ’Finance’ Task View. The capabilities
of the package are concisely described in W¨urtz et al. (2010). In this subsection,
we focus on how robust estimators can be employed for portfolio optimizations; the
package is discussed further in Chapter 12.
The pivotal slot for an object of class fPFOLIOSPEC is model. This slot is itself a
list object and contains the named element estimator. The value of this list element
denotes the function for estimating mean and covariance of the assets’ returns and its
default value is covEstimator, which refers to the classical product-moment esti-
mator. The kind of estimator for a given portfolio speciﬁcation can be queried with
the function getEstimator() and altered by the function setEstimator(). The
latter function expects the name of a function which returns estimates for the locations
and dispersion. It returns a list object with elements mu and sigma for the location
and scatter, respectively. In fPortfolio the functions for robustifying portfolios are
kendallEstimator(), spearmanEstimator(), mcdEstimator(), mveEsti-
mator(), covMcdEstimator() and covOGKEstimator(). The ﬁrst two functions

ROBUST PORTFOLIO OPTIMIZATION
167
employ Kendall’s and Spearman’s rank correlations as measures of association
between the assets’ returns. The difference between the functions mcdEstimator()
and covMcdEstimator() is that the former employs the function contained in the
package MASS and the latter uses the function contained in the package robustbase.
Similarly, the MVE estimator, mveEstimator(), is imported from the package
MASS and the OGK estimator, covOGKEstimator(), from the package robust-
base. However, it should be stressed again that the user can provide an estimator on
its own and in W¨urtz et al. (2010, Chapter 20) examples are provided in which this
approach is elucidated.2
10.4.3
The package MASS
MASS is the package accompanying the book Modern Applied Statistics with S
(Venables and Ripley 2002). Because of its importance and wide-ranging implemen-
tation of methods encountered in statistics, it is part of the base R distribution.
The package MASS is contained in the CRAN ‘Distributions’, ‘Econometrics’,
‘Environmetrics’, ‘Multivariate’, ‘Pharmaconetics’, ‘Psychometrics’, ‘Robust’, and
‘SocialSciences’ Task Views. Within the package S3-type classes and methods are
employed and the package is shipped with a NAMESPACE ﬁle. In the following, only
the functions pertinent to robust statistics are presented.
The Huber M-estimators are implemented as functions huber() and hubers().
The former function estimates the location of a data vector. The median absolute
deviation (MAD) is used as a scale estimate and the default value for winsorizing
the sample is set to k = 1.5. The latter function can be employed when either the
location or the scale is speciﬁed or if neither is supplied by the user. Both functions
return a list object with elements for the location and scale estimates.
For multivariate data, robust estimates for the location vector and scatter can
be computed with the function cov.rob(). Two methods are available, minimum
volume ellipsoid and minimum covariance determinant, the former being the default.
A classical covariance estimate according to the product moments is calculated if
method = ’classical’ has been set as argument. The functions cov.mve() and
cob.mcd() are wrappers for the former two methods. The function returns a list
object containing elements for the location (center) and dispersion estimates (cov).
If correlations are also to be computed (the default is cor = FALSE), these will be
returned as list element cor. The remaining list items contain information about the
outcome of the numerical optimization.
Finally, the dispersion matrix of a multivariate Student’s t distribution can be
estimated with the function cov.trob(). The t distribution provides some robustness
with respect to outliers due to a higher probability mass in the tails compared to the
normal distribution. The function allows the inclusion of a weighting vector (wt)
2 For completeness’ sake, it should be mentioned that shrinkage and bagged estimators for the dis-
persion matrix are also included in the package fPortfolio, namely shrinkEstimator() and
baggedEstimator(), respectively. Furthermore, the reader is directed to the package tawny (see
Rowe 2012) for facilitating portfolio optimizations based on shrinkage estimators.

168
PORTFOLIO OPTIMIZATION APPROACHES
for the sample observations. The default is to weight the observations equally. The
degrees of freedom have to be speciﬁed by the argument nu; the default value is
5. The argument cor is a logical switch (the default is FALSE) which serves the
same purpose as in cov.rob(). The optimization process can be controlled by
the arguments maxit and tol which limit the maximum number of iterations and
the convergence criteria, respectively; the default values are 25 and 0.01. The function
returns a list object and the arguments cov, center and cor denote the covariance,
the location and correlation estimates, respectively. Akin to the function cov.rob(),
the remaining list elements contain information about the numerical optimization.
10.4.4
The package robustbase
The package robustbase includes the base and essential functions for robust estima-
tion (see Rousseeuw et al. 2012). The package serves as a building block for other
packages in the domain of robust statistics. This CRAN package is contained in the
‘Multivariate’, ‘Robust’ and ‘SocialSciences’ Task Views. It employs S4 methods
and classes, but also imports S3 methods. The burdensome computations for some
robust methods, in particular the linear model, are interfaced from C routines.
With respect to robust statistics the functions covMcd() and covOGK() can be
used to estimate the dispersion. The former is an implementation of the minimum co-
variance determinant, and the latter of the orthogonalized Gnanadesikan–Kettenring
estimator. Both functions return a list object from which the location (center) and
dispersion cov estimates can be extracted.
The generalized class of Huber M-estimators is implemented as function
huberM(). This returns a list element with robust location estimate (mu) and scale
estimate (s), where the MAD is used as default, and the number of iterations is
returned as list element it.
10.4.5
The package robust
The package robust is an R port of the Insightful package of the same name (see
Wang et al. 2010). It is aimed at casual users of robust methods and statistics. It is
part of the CRAN ‘Robust’ and ‘SocialSciences’ Task Views and considered a core
package – similar to robustbase – of the former. The package has dependencies on
the packages MASS, lattice, robustbase, rrcov and stats. The estimation of linear
and generalized linear models is interfaced from routines written in C.
For robust multivariate statistics the function covRob() is available. The data
for which the robust statistics are required can be speciﬁed either as a matrix
or data.frame object. The user can choose whether covariances or correlations
are returned by setting the logical argument corr accordingly. The default is to
compute a variance–covariance matrix. Further, the argument distance is used as
a switch to determine whether or not squared Mahalanobis distances are computed
(the default value is TRUE). Missing data values can be handled by specifying a
function for the argument na.action; the default action is na.fail, but na.omit
can be utilized instead. The robust estimation method to be employed is determined

ROBUST PORTFOLIO OPTIMIZATION
169
by the argument estim. A character string can be provided to choose between the
following multivariate estimators: ’mcd’ for the fast MCD algorithm; ’weighted’
for the reweighted MCD estimator; ’donostah’ for the Stahel–Donoho estimator;
’M’ for the class of M-estimators; ’pairwiseQC’ for the orthogonalized quadrant
correlation estimator; and ’pairwiseGK’ for the orthogonalized Gnanadesikan–
Kettenring estimator. The default value of the argument is ’auto’, in which case the
function automatically selects one of either the Stahel–Donoho, MCD or pairwise
quadrant correlation estimators by trading off computation time against reasonable
estimates for location and scatter. The rule is to use the Stahel–Donoho estimator for
data sets that contain less than 1000 observations and less than 10 variables or less
than 5000 observations and less than ﬁve variables. If the count of variables is greater
than 10 and less than 20 with a maximum of 5000 data vectors, the MCD estimator is
used. In all higher-dimensional cases the orthogonalized pairwise correlation method
is used. The MCD estimator are imported from the package robustbase and the rest
from rrcov (see Section 10.4.6 below). The function covRob() accepts an object
for controlling the robust estimation. This functional argument can be created with
the function covRob.control(), which returns a named list of control parameters.
These can also be provided via the ellipsis argument when estim is not set to its
default value ’auto’.
The function returns a list object of informal S3 class covRob. The list element
call shows how the user invoked the function, that is, the kind of arguments used.
Hence, the generic update() method can be used for this object. The dispersion
estimate (covariances or correlations) is returned as element cov and the location
vector as center. If Mahanalobis distances have been computed, they will appear
in the list element dist. If the algorithm has been initialized with robust estimates,
these values are returned as raw.cov, raw.center and raw.dist, for dispersions,
locations and distances, respectively. The name of the chosen estimator is returned as
list element estim and the control parameters as list object control. For objects
of this informal class print(), plot() and summary() methods are provided,
and the latter also has its own print() method. For the plot() method, the user
can interactively choose between a display of the eigenvalues, the square root of the
Mahalanobis distances or an ellipses plot. The eigenvalues of the dispersion estimates
are contained in objects of informal S3 class summary.covRob.
Finally, the function ccov() is an implementation of the classical estimators and
returns a list object of informal S3 class cov. The main intention of the package’s
developer is to provide a function that returns an object similar in structure as objects
of S3 class covRob and hence facilitate an easy comparison of estimates. For these
objects plot(), print() and summary() methods are deﬁned.
10.4.6
The package rrcov
The package rrcov is dedicated solely to robust multivariate statistics. As such
it depends on the package robustbase, but offers a self-contained S4 classes and
methods approach for implementing robust estimators (see Todorov and Filzmoser
2009). Further dependencies of the package are methods, pcaPP (see Filzmoser

170
PORTFOLIO OPTIMIZATION APPROACHES
et al. 2011) and mvtnorm (see Genz and Bretz 2009). It is contained in the CRAN
‘Multivariate’ and ‘Robust’ Task Views and is considered to be a core package in the
latter. Its structure with regard to the classes deﬁned, methods provided and robust
statistics covered is detailed in a vignette. The package is shipped with a NAMESPACE
ﬁle in which all relevant functions, methods and classes of the package are exported
and S3 methods are imported as generic functions. The computationally intensive
calculations are interfaced from C routines.
In addition to robust multivariate statistics, in the package robust methods for
principal component and linear discriminant analysis are also implemented, but the
focus here will be on the robust estimators only. A hierarchical class structure is
employed, with a virtual class CovRobust for which the usual show(), summary()
and plot() methods are deﬁned. An S4 class for each of the multivariate robust
estimators is deﬁned; these are derived classes of this virtual class. Similarly, a
virtual class CovControl is deﬁned from which the classes that control a speciﬁc
robust method are derived. Basically all robust methods for multivariate statistics are
implemented: M-, MM and S-estimators as well as MVE, MCD and OGK methods
and the Stahel–Donoho estimator. The naming convention for the classes is that
a corresponding constructor function of the same name exists. Thus, for instance,
the class pertinent to the minimum-covariance determinant method is CovMcd, the
object’s constructor function is CovMcd(), and the construction of these objects (i.e.,
parameter settings for estimation) is controlled by CovControlMcd(), which returns
an object of class CovControlMcd. Therefore, the classes of the above-mentioned
robust methods are CovMest, CovMMest, CovSest, CovMve, CovMcd, CovOgk and
CovSde respectively, and the associated constructor functions have the same names.
In addition, a wrapper function CovRobust() for these robust methods is available.
In addition to an argument x for the data object, it is deﬁned with a control argument
that requires an object created by one of the control routines for a particular estimation
method, for example, the output of CovControlMve(). The slots of the formal S4
class objects can be extracted by calling one of the functions getfoo(), where foo
is the name of the slot.
Similar to the package robust, the classic estimators for location and scatter
are implemented as function CovClassic() which returns an S4 object CovClas-
sic. The user can choose whether sample or population moments are estimated by
employing the logical argument unbiased; the default is TRUE.
The user can choose between ﬁve different kinds of plot for either classical or
robust estimates of location and scatter: an index plot of the robust and Mahalanobis
distances, a distance–distance plot (for robust methods only), a χ2 QQ plot of the
robust and Mahalanobis distances, an ellipses plot and a scree plot of the eigenvalues.
The plot() method takes an argument which that the user can employ to control the
kind of plot produced; the default is which = ’all’, whereby all plots are returned
successively if executed in interactive mode.
10.4.7
The package Rsocp
The package Rsocp is part of the Rmetrics suite of packages (see Chalabi and W¨urtz
2010). At the time of writing, the package was hosted on R-Forge only. Within

ROBUST PORTFOLIO OPTIMIZATION
171
the package wrapper functions for the software socp of Lobo et al. (1997) are
implemented. The algorithm for solving SOCPs has been developed in C. These
C routines are interfaced from R. In addition to these high-level language func-
tions, the authors provide MatLab scripts. The original software is still available at
http://stanford.edu/˜boyd/old_software/socp/ .
The package contains two functions: socp() and socpControl(). The former
function serves as the wrapper around the underlying C code, and to control the be-
haviour of the algorithm a list object created by the latter function can be speciﬁed.
The SOCP has to be formulated at least in its primal form (see equation 10.18) and
provides arguments for specifying the parameters of its dual form. If the latter argu-
ments are not provided, then an internal solution is computed before the parameters
of the SOCP are passed down to the C routine. This is accomplished by the internal
functions .socp.phase1() and .socp.phase2(). The ﬁrst of these functions re-
turns an internal solution for x and the second determines corresponding values for
z. Incidentally, the package contains a further hidden function .SqrtMatrix() for
computing the square root of a matrix. The user’s guide (Lobo et al. 1997) and a draft
version of Lobo et al. (1998) are contained in the package’s ./doc directory.
10.5
Empirical applications
10.5.1
Portfolio simulation: Robust versus classical statistics
In this ﬁrst empirical application a simulation comparing classical and robust estima-
tors will be conducted. Returns for ﬁve ﬁctional assets will be randomly generated
according to one of the following data-generating processes (DGPs):
r Gauss copula with normally distributed margins;
r Gauss copula with t-distributed margins;
r Student’s t copula with t-distributed margins.
The ﬁrst DGP corresponds to the case where the classical estimators are the best
linear unbiased estimators for the Gauss copula with normal margins; the other two
DGPs reﬂect the stylized facts of ﬁnancial market returns, namely excess kurtosis and
tail dependence. For the t distribution ﬁve degrees of freedom have been chosen, and
the sample sizes for all DGPs are 60, 120 and 240 observations. Assuming a monthly
frequency, this corresponds to a 5-, 10- and 20-year time span for the data in each
of the portfolio optimizations. For each DGP a set of 1000 samples has been gen-
erated. An equally correlated dependence structure between the ﬁve assets has been
assumed with a value ρ = 0.5. The generation of these random samples is shown in
Listing 10.1.
First, the required packages are loaded into the work space. The random samples
are generated by functions contained in the package copula (see Section 9.4.2). Next,
copula objects for the Gauss and Student’s t copula are created and named ncop and
tcop, respectively. The DGPs are created by utilizing the function mvdc(). This
requires an object of class copula and a list object that contains the parameter

172
PORTFOLIO OPTIMIZATION APPROACHES
Listing 10.1 Portfolio simulation: data generation.
## Loading of packages
1
library(copula)
2
library(quadprog)
3
library(rrcov)
4
## Creating copula objects
5
ncop <−normalCopula(param = 0.5, dim = 5)
6
tcop <−tCopula(param = 0.5, dim = 5, df = 5, df.ﬁxed = TRUE)
7
## Creating DGPs
8
NcopMargN <−mvdc(ncop, margins = "norm",
9
paramMargins = list(list(mean = 0, sd = 1)),
10
marginsIdentical = TRUE)
11
NcopMargT <−mvdc(ncop, margins = "t",
12
paramMargins = list(df = 5),
13
marginsIdentical = TRUE)
14
TcopMargT <−mvdc(tcop, margins = "t",
15
paramMargins = list(df = 5),
16
marginsIdentical = TRUE)
17
## Initialising list objects for DGP
18
Lobj <−list()
19
length(Lobj) <−1000
20
## Setting a seed
21
set.seed(12345)
22
## Generating random samples
23
rNcopMargN <−lapply(Lobj, function(x) rmvdc(NcopMargN, 240))
24
rNcopMargT <−lapply(Lobj, function(x) rmvdc(NcopMargT, 240))
25
rTcopMargT <−lapply(Lobj, function(x) rmvdc(TcopMargT, 240))
26
information about the marginal distributions assumed. The objects are labelled Ncop-
MargN, NcopMargT and TcopMargT for the three multivariate distribution models.
These objects are then employed to draw the random samples. In order to do so,
ﬁrst a list object Lobj is created and the size of the simulation is assigned as its
length. Second, a seed is set for replication purposes. In principle, the 1000 random
samples for each of the DGPs could be assigned as list elements with a for loop,
but it is more in the nature of R to use lapply() instead. In the last three lines the
list object Lobj is used for this purpose and the results are stored in the objects
rNcopMargN, rNcopMargT and rTcopMargT. These list objects consists of 1000
samples for each of the DGPs with 240 rows and ﬁve columns for the ﬁctional asset
returns. Sample data for the shorter sample spans can then swiftly extracted from the
list elements.
As a next step, a function is created that returns the dispersion estimates for the
classical and robust methods. This function can then be applied for the simulated data

ROBUST PORTFOLIO OPTIMIZATION
173
Listing 10.2 Portfolio simulation: function for estimating moments.
## Function for moment estimation
1
Moments <−function(x, method = c("CovClassic", "CovMcd",
2
"CovMest", "CovMMest", "CovMve", "CovOgk",
3
"CovSde", "CovSest"), ...){
4
method <−match.arg(method)
5
ans <−do.call(method, list(x = x, ...))
6
return(getCov(ans))
7
}
8
sets for each of the DGPs and then used for optimizing the minimum-variance portfo-
lios. The comparative simulation will encompass the classical estimators and the M-,
MM, S-, MCD, MVE, SD and OGK robust estimators. The function speciﬁcation is
shown in Listing 10.2, where the estimators of the rrcov package are used.
The function is speciﬁed with three arguments. The ﬁrst, x, is used for the random
data set, the second for determining what kind of estimator is employed, and the third
is the ellipsis argument that is passed down to do.call() so that the user has
command of the arguments of the estimating function. In the ﬁrst line of the function
body partial matching for the name of the estimating function is included. In the
second line the function determined by the argument method is applied to the data
set x. Finally, the estimate is extracted from the object ans by means of the access
function getCov() and its result is returned.
This function can now be used to estimate the second moment for each of the list
elements and for each of the DGPs and sample sizes. Listing 10.3 shows how this task
can be swiftly accomplished. First, the dimension of the simulation study is deﬁned:
there are three models for the multivariate distributions, DGP. These list objects were
created in Listing 10.1. Each list object contains 1000 elements in the form of 240 × 5
matrices. Next, the function names of the estimators to be used are collected in the
character vector EST. Finally, the sample sizes are set in the numeric vector SAMPLE.
In the ensuing code section the samples of different sizes are created. In the ﬁrst
double for loop, the object names are created and displayed by the cat() function
so that the user can better track the progress of the loop. In addition, the names of
these list objects are saved in the vector datnames for use in the second for loop
construction. In the last line, lapply() is used to extract for each list element and
for each DGP the number of rows according to SAMPLE. Thus, after the execution of
the loop has ﬁnished, nine new list objects have been created. With these data sets
at hand, one can now proceed and estimate the moments for each single list element.
This is done in the second double for loop. First, the names of the list objects that
will store the estimates for the mean and covariance are created, and these names are
displayed to enable progress monitoring when the for loop is executed. Similar to the
ﬁrst loop, the names of these objects are saved in the character vector objnames.
In the ﬁnal line, lapply() is used to apply the function Moments() to each list

174
PORTFOLIO OPTIMIZATION APPROACHES
Listing 10.3 Portfolio simulation: estimates for data processes.
## Dimensions of simulation
1
DGP <−c("rNcopMargN", "rNcopMargT", "rTcopMargT")
2
EST <−c("CovClassic", "CovMcd", "CovMest", "CovMMest",
3
"CovMve", "CovOgk", "CovSde", "CovSest")
4
SAMPLE <−c(60, 120, 240)
5
## Creating list objects for combinations of
6
## DGP and sample sizes
7
## initialising vector for data objects
8
datnames <−NULL
9
for(i in DGP){
10
for(j in SAMPLE){
11
objname <−paste(i, j, sep = " ")
12
datnames <−c(datnames, objname)
13
cat(paste("Creating list object", objname, "/n"))
14
assign(objname, lapply(eval(as.name(i)), function(x) x[1:j, ]))
15
}
16
}
17
## Creating list objects with estimates of
18
## location and dispersion for combinations of
19
## DGP, sample sizes and estimators
20
## initialising vector for list objects
21
objnames <−NULL
22
for(i in datnames){
23
for(j in EST){
24
objname <−paste(j, i, sep = " ")
25
objnames <−c(objnames, objname)
26
cat(paste("Creating list object", objname, "/n"))
27
assign(objname, lapply(eval(as.name(i)), Moments, method = j))
28
}
29
}
30
element for each of the DGPs and sample sizes. After the successful completion of
this loop, a total of 3 × 3 × 8 = 72 list objects have been created and each contains
a dispersion estimate. The portfolio optimizations are then conducted with respect to
these 72 objects, which implies a total of 72 000 optimizations to be done.
Having created all the necessary data objects, one can proceed with the portfolio
optimizations. In Listing 10.4 the function PortMinVar() has been deﬁned. The
optimization is carried out by employing solve.QP() from the package quadprog.
The minimum-variance optimization takes place under the constraints of being fully
invested (objects a1 and b1) and with only long positions allowed (objects a2 and
b2). The function returns the weight vector. In the ensuing for loop the optimizations

ROBUST PORTFOLIO OPTIMIZATION
175
Listing 10.4 Portfolio simulation: minimum-variance optimizations.
## Function for minimum-variance portfolio
1
## Constraints: fully invested, long-only
2
PortMinVar <−function(x){
3
Dmat <−x
4
k <−ncol(Dmat)
5
dvec <−rep.int(0, k)
6
a1 <−rep.int(1, k)
7
b1 <−1
8
a2 <−diag(k)
9
b2 <−rep.int(0, k)
10
Amat <−t(rbind(a1, a2))
11
bvec <−c(b1, b2)
12
opt <−solve.QP(Dmat = Dmat, dvec = dvec, Amat = Amat,
13
bvec = bvec, meq = 1)
14
return(opt$solution)
15
}
16
## Conduct optimization
17
portnames <−NULL
18
idx <−1:1000
19
for(i in objnames){
20
objname <−paste("Port", i, sep = " ")
21
portnames <−c(portnames, objname)
22
obj <−eval(as.name(i))
23
weights <−lapply(obj, PortMinVar)
24
assign(objname, sapply(idx, function(x)
25
sqrt(t(weights[[x]]) %∗% obj[[x]] %∗%
26
weights[[x]])))
27
}
28
## Caluculate median and IQR of portfolio risks
29
mednames <−NULL
30
iqrnames <−NULL
31
for(i in portnames){
32
objname1 <−paste("Med", i, sep = " ")
33
objname2 <−paste("IQR", i, sep = " ")
34
mednames <−c(mednames, objname1)
35
iqrnames <−c(iqrnames, objname2)
36
assign(objname1, median(eval(as.name(i))))
37
assign(objname2, IQR(eval(as.name(i))))
38
}
39

176
PORTFOLIO OPTIMIZATION APPROACHES
Table 10.1
Portfolio simulation: summary of portfolio risks.
Normal / Normal
Normal / Student
Student / Student
Estimator
Median
IQR
Median
IQR
Median
IQR
T = 60
Classic
0.75
0.097
0.92
0.137
0.92
0.174
MCD
0.77
0.168
0.80
0.167
0.82
0.165
M
0.72
0.135
0.76
0.150
0.78
0.154
MM
0.74
0.103
0.83
0.117
0.82
0.134
MVE
0.75
0.155
0.78
0.162
0.80
0.167
OGK
0.66
0.124
0.68
0.127
0.67
0.130
SDE
0.70
0.148
0.73
0.145
0.75
0.163
S
0.74
0.122
0.80
0.133
0.82
0.141
T = 120
Classic
0.76
0.069
0.95
0.109
0.96
0.137
MCD
0.77
0.094
0.83
0.114
0.83
0.116
M
0.75
0.087
0.80
0.106
0.81
0.108
MM
0.76
0.067
0.85
0.088
0.84
0.096
MVE
0.77
0.090
0.82
0.121
0.83
0.114
OGK
0.69
0.079
0.71
0.095
0.69
0.096
SDE
0.74
0.096
0.77
0.106
0.79
0.115
S
0.75
0.077
0.83
0.096
0.84
0.099
T = 240
Classic
0.77
0.047
0.97
0.078
0.98
0.103
MCD
0.77
0.058
0.83
0.075
0.85
0.080
M
0.77
0.058
0.81
0.069
0.82
0.080
MM
0.77
0.049
0.87
0.059
0.86
0.068
MVE
0.77
0.067
0.84
0.074
0.86
0.082
OGK
0.70
0.058
0.72
0.067
0.71
0.068
SDE
0.76
0.066
0.80
0.072
0.81
0.081
S
0.77
0.056
0.84
0.064
0.86
0.072
are carried out and the 1000 portfolio risk ﬁgures are stored for each of the DGP,
estimator and sample size combinations. Finally, the median and inter-quartile range
(IQR) are computed for the portfolio risks.
The simulation results are summarized in Table 10.1. The medians and inter-
quartile ranges are reported in the columns for each of the three DGPs. The results
are grouped by sample size. A couple of conclusions can be drawn from this table.
Even in the case of the Gauss copula with normally distributed margins the classical
covariance estimator does not yield a portfolio structure of lowest risk on average and
the dispersion of the portfolio risks is also not the smallest. This result holds for all

ROBUST PORTFOLIO OPTIMIZATION
177
sample sizes, but the differences are becoming negligible for T = 240 with respect
to the median risks. Lower portfolio risks can be achieved with the robust OGK and
Stahel–Donoho estimators for this DGP. For T = 60 the M-, MM and S-estimators
excel likewise.
For the second DGP, a multivariate distribution with excess kurtosis but no tail
dependence, two observations can be made. First, the risk levels increase for all
types of estimator and this increase is most pronounced for the classical estimator. In
contrast, the increase in the median risks for the robust estimators is rather negligible,
and all of these estimators outperform the classic estimator, i.e., producing portfolio
allocations of lower risk. The OGK estimator fares best.
This picture remains pretty much unchanged for the third DGP, where now tail
dependencies exist between the included assets. The differences between the second
and third DGPs in the median levels are rather small for all estimators and sample
sizes. One can conclude that tail dependence does not have a material impact on the
overall portfolio risk for the given the simulation design.
The two main takehome messages are, ﬁrst, that even in the case of the multivariate
normal distribution, it can be advantageous to employ a robust estimator, in particular
OGK, instead of the classical estimator; and second, all robust estimators are quite
immune with respect to different DGPs, but violations of the model assumptions
exert a great impact on portfolio riskiness when a classical estimator is utilized.
When the inter-quartile ranges of the portfolio risks for each of the estimators and
sample sizes are compared, two features emerge. First, in the case of the Gaussian
DGP, the portfolio risks are more concentrated compared to the robust estimators for
all sample sizes. This result conﬁrms the best linear unbiased estimator property of
the classical estimator. Second, by slightly violating the assumption of normality the
implied portfolio risks of the classical estimator are dispersed more widely compared
to most of the robust estimators. For the latter, the inter-quartile range remains pretty
much the same, regardless of DGP.
10.5.2
Portfolio back-test: Robust versus classical statistics
In this subsection a back-test of a minimum-variance portfolio optimization with
long-only and full investment for major stock market indexes is conducted. The
classical and the robust estimators are utilized for moment estimation. The six stock
indexes covered are the S&P 500, Nikkei 225, FTSE 100, CAC 40, DAX and Hang
Seng. The sample of month’s-end index levels start on 31 July 1991 and ends on
30 June 2011, thus comprising 240 observations.
In Listing 10.5 the necessary packages are ﬁrst loaded into memory. The data
set is contained in the package FRAPO accompanying this book. In principle, the
portfolio back-test could be conducted with the facilities offered in fPortfolio, but
for ease of comprehension a customized back-test will be described in which the
functions and methods of the packages loaded in lines 3–6 will be employed.
With the packages in memory, the data set is loaded and converted to a zoo object.
Percentage discrete returns are calculated, and by way of preliminary data analysis
descriptive statistics are calculated and boxplots drawn for each index.

178
PORTFOLIO OPTIMIZATION APPROACHES
Listing 10.5 Portfolio back-test: descriptive statistics of returns.
## Loading of packages
1
library(FRAPO)
2
library(PerformanceAnalytics)
3
library(quadprog)
4
library(rrcov)
5
library(zoo)
6
## Loading data, calculate returns
7
data(StockIndex)
8
pzoo <−zoo(StockIndex, order.by = rownames(StockIndex))
9
rzoo <−(pzoo / lag(pzoo, k = −1) −1) ∗100
10
## Boxplot and descriptive statistics
11
boxplot(coredata(rzoo))
12
rstats <−rbind(apply(rzoo, 2, summary),
13
skewness(rzoo),
14
kurtosis(rzoo)
15
)
16
The boxplot is shown in Figure 10.1, from which the stylized facts of ﬁnancial
market returns are clearly evident. The returns are skewed to the left and the span
of the returns indicates excess kurtosis. The most volatile returns series is the Hang
Seng index. All returns series have outliers in the left tail, but only the German DAX
and the Hang Seng have them in the right tail. The key descriptive statistics are
summarized in Table 10.2.
The code for conducting the back-test is shown in Listing 10.6. First, a character
vector, EST, with the names of the estimators to be used, is created. Next, a function,
PfBack, is deﬁned for determining the weights of the minimum-variance portfolio for
a given estimator. The previously deﬁned functions Moments() and PortMinVar()
areused.Thefunction PfBack isthenusedby rollapply() tocalculatetheportfolio
weights by using a rolling window of 120 observations. The resulting zoo object
is named PfWeights. The function rollapply() is called within a lapply()
enclosure and the character vector EST has been used as list object. Hence, the
object PfWeights is itself a list of zoo objects with the weights for each of the
estimation method as in EST. Next, its index is stored as the object periods. For
each of the underlying estimators, the portfolio returns can be computed swiftly,
by employing the lag() function, such that the weights – lagged by one period –
are multiplied by the respective returns and the row sums of these products are the
portfolio returns. This calculation again involves the lapply() function. In the next
two lines this list object is transformed to a zoo object and the excess returns of the
portfolios based on robust estimators compared to the classical covariance estimator
are computed. In the last two blocks of code the excess returns are displayed and
summary statistics computed.

ROBUST PORTFOLIO OPTIMIZATION
179
SP500
N225
FTSE100
GDAX
HSI
−30
−20
−10
0
10
20
30
Figure 10.1
Boxplots of stock index returns.
The excess returns of the back-test based upon the robust statistics compared to the
classical estimators are displayed in Figure 10.2 and the relevant summary statistics
in Table 10.3. For the given measures, the outcome of the robust optimizations is more
favourable than that of the classical estimator. The average excess return is positive
for all robust portfolios, and they are skewed to the right. Certainly the skewness
is affected by the large positive excess return in 2009, but even if one disregards
this data point a positive skew results. This can be concluded by the difference
between the third quartile and the absolute value of the ﬁrst quartile, which is greater
than zero for all robust estimators. Compared to the other robust estimators, the
Table 10.2
Portfolio back-test: descriptive statistics of returns.
Statistics
SP500
N225
FTSE100
CAC40
GDAX
HSI
Minimum
−16.940
−23.830
−13.020
−17.490
−25.420
−29.410
First Quartile
−1.854
−4.319
−1.762
−3.351
−2.533
−3.122
Median
1.067
−0.082
0.690
1.266
1.320
0.959
Mean
0.604
−0.190
0.436
0.499
0.829
1.009
Third Quartile
3.323
3.886
3.115
4.017
4.431
4.679
Maximum
10.640
16.150
10.400
13.410
21.380
30.160
Skewness
−0.554
−0.247
−0.478
−0.294
−0.454
0.240
Excess Kurtosis
0.952
0.455
0.391
0.188
1.723
2.266

180
PORTFOLIO OPTIMIZATION APPROACHES
Listing 10.6 Portfolio back-test: rolling window optimization.
## Conduct back-test
1
EST<−c("CovClassic","CovMcd","CovMest","CovMMest","CovMve",
2
"CovOgk", "CovSde", "CovSest")
3
## Function for back−test
4
PfBack <−function(x, method = c("CovClassic","CovMcd","CovMest",
5
"CovMMest", "CovMve", "CovOgk", "CovSde",
6
"CovSest"), . . .){
7
cov <−Moments(x, method = method)
8
return(PortMinVar(cov))
9
}
10
## Conducting back-test
11
PfWeights <−lapply(EST, function(x)
12
rollapply(rzoo, width = 120, FUN = PfBack,
13
method = x, by.column = FALSE,
14
align = "right"))
15
16
periods <−as.Date(index(PfWeights[[1]]))
17
## Calculate portfolio returns/relative performance
18
PfReturns <−lapply(PfWeights, function(x)
19
rowSums(lag(x, k = −1) ∗rzoo))
20
PfReturns <−zoo(matrix(unlist(PfReturns),
21
ncol = length(PfReturns)), periods)
22
colnames(PfReturns) <−EST
23
PortOut <−(PfReturns[, −1] −PfReturns[, 1])
24
## Plot relative peformance
25
plot(PortOut, type = "h",
26
xlab = " ",
27
ylab = EST[−1],
28
main = "Relative Performance",
29
ylim = range(PortOut))
30
## Statistics on relative performance
31
PortRelStats <−rbind(apply(PortOut, 2, summary),
32
skewness(PortOut)
33
)
34
performance of the MM estimator is the least favourable, giving the lowest mean
and maximum excess return. Among the robust estimators with a positive median
excess return, the MCD and MM estimators produced portfolio allocations that
yielded excess returns of highest skewness, followed by the OGK and Stahel–Donoho
estimators.

ROBUST PORTFOLIO OPTIMIZATION
181
−2
0
1
2
3
4
CovMcd
−2
0
1
2
3 4
CovMest
−2
0
1
2
3 4
CovMMest
2002
2004
2006
2008
2010
−2
0
1
2
3
4
CovMve
−2
0
1
2
3
4
CovOgk
−2
0
1
2
3 4
CovSde
2002
2004
2006
2008
2010
−2
0
1
2
3 4
CovSest
Figure 10.2
Relative performance of robust portfolios.
Table 10.3
Portfolio back-test: descriptive statistics of excess returns.
1st
3rd
Estimator
Min
Quartile
Median
Mean
Quartile
Max
Skew
MCD
−1.572
−0.339
0.020
0.027
0.403
3.768
1.095
M
−1.312
−0.319
−0.029
0.040
0.407
3.506
1.148
MM
−0.685
−0.166
0.004
0.024
0.195
1.696
1.101
MVE
−1.795
−0.314
−0.009
0.038
0.371
3.636
1.080
OGK
−1.468
−0.337
0.017
0.037
0.484
2.670
0.359
SDE
−2.068
−0.411
0.047
0.041
0.534
2.534
0.123
S
−1.360
−0.294
0.000
0.044
0.423
2.749
0.800

182
PORTFOLIO OPTIMIZATION APPROACHES
10.5.3
Portfolio back-test: Robust optimization
In this subsection robust portfolio optimizations with elliptical uncertainty for the
expected returns are applied to the data set of the previous section. The elliptical
uncertainty will be based on the ML point estimates, but also an elliptical uncertainty
set for alternative location estimators will be formed. The efﬁcient frontiers for a
classical mean–variance optimization and these two robust optimizations will then
be compared. The SOCP will be solved by utilizing the function Socp() contained in
the package FRAPO. Before doing so, a function for carrying out this task is deﬁned
which returns not only the portfolio weights but also the rest of the output from the
optimizer. The function PR1() is shown in Listing 10.7.
Here the SOCP is expressed in its primal form as in equation (10.18). The
arguments of the function are SRoot for the square root of the variance–covariance
matrix, PRoot for the square root of the variance–covariance matrix of the estimated
location parameter, mu for the point estimates of the returns, SigMax for the portfolio
risk, lambda for the weighting parameter as in equation (10.12), delta for the square
root of the χ2 quantile, and the ellipsis argument passed down to the call of Socp() in
line 31. First, the number of assets is set and the dimension of the variable vector x of
the SOCP is determined. The weighting of the portfolio risk is assigned to the object
ra. In line 6 the parameters of the objective are set. Here the order is x = [ω, t]′.
Because the target is minimized in the function Socp() a −1 is used for the cone
variable t. Next the vector for the cone constraint is speciﬁed. The object C2 is a null
vector, and specifying it as such has the effect of formulating the portfolio risk as a
quadratic constraint. The parameters for the non-negativity and budget constraints are
stored in the objects C3 and C4, respectively. These right-hand-side constraints are
vertically stacked as matrix object C. In line 19 a zero matrix is created. This matrix
has Nassets + 1 rows, and column dimension equal to the number of variables.
Its purpose is to ﬁll the matrix A, and the number of rows is equal to the number of
non-negativity and budget constraints. The objects A1 and A2 are used to store the
variances and covariances appropriately. Note that SRoot and PRoot are back-ﬁlled
with zeros for the variable t. Commencing in line 26, the A matrix of the SOCP is
created. Because of its afﬁne nature, the vector b is set to zero. The constants d j on
the right-hand side of the inequality constraints are set appropriately. Having created
all of the required inputs, the function Socp() can be called and the resulting list
object is returned.
For comparison of the robust optimization results with those obtained from a
mean–variance portfolio, a function is written which expresses the latter optimization
in the form of an SOCP, though a cone constraint is not needed and the maximum
allowable portfolio risk is speciﬁed as a quadratic constraint. The function PMV()
is provided in Listing 10.8. The objective function is now the minimization of the
negative expected portfolio return, which is the same as the maximization of the
positive return. In this function the quadratic portfolio risk constraint is written as
a cone constraint, but c1 is set to the zero vector. The corresponding matrix A1 is
the square root of the variance–covariance matrix. The non-negativity and budget
constraints are speciﬁed similarly to PR1(), except that the cone variable t is absent

ROBUST PORTFOLIO OPTIMIZATION
183
Listing 10.7 Robust portfolio optimization with elliptical uncertainty.
PR1 <−function(SRoot, PRoot, mu, SigMax, lambda, delta, ...){
1
Nassets <−nrow(SRoot)
2
Nvar <−Nassets + 1
3
ra <−1 −lambda
4
## Objective function
5
f <−c(rep(0, Nassets), −1)
6
## Cone constraint for returns
7
C1 <−matrix(c(mu, −1) / delta / ra, nrow = 1)
8
## Quadratic constraint for portfolio risk
9
C2 <−matrix(rep(0, Nvar), nrow = 1)
10
## Non−negativity constraint
11
C3 <−cbind(diag(Nassets), rep(0, Nassets))
12
## Budget constraint (non−overinvestment)
13
C4 <−matrix(c(rep(−1, Nassets), 0), nrow = 1)
14
C <−rbind(C1,
15
C2,
16
C3,
17
C4)
18
ZeroM <−matrix(0, nrow = Nassets + 1, ncol = Nvar)
19
A1 <−cbind(PRoot, rep(0, Nassets))
20
A1 <−rbind(A1,
21
c(rep(0, Nassets), 0))
22
A2 <−cbind(SRoot, rep(0, Nassets))
23
A2 <−rbind(A2,
24
c(rep(0, Nvar)))
25
A <−rbind(A1,
26
A2,
27
ZeroM)
28
b <−rep(0, nrow(A))
29
d <−c(0, SigMax ∗ra, rep(0, Nassets), 1)
30
ans <−Socp(f = f, A = A, b = b, C = C, d = d,
31
N = c(nrow(A1), nrow(A2), rep(1, nrow(ZeroM))), . . .)
32
return(ans)
33
}
34
and hence the object ZeroM in the function body has column dimension reduced
by 1.
With the wrapper functions in Listings 10.7 and 10.8 one can now trace the
efﬁcient frontiers of the mean–variance and the robustly optimized portfolios, whereas
for the latter the uncertainty has been conﬁned to the expected returns. The relevant
code is shown in Listing 10.9.

184
PORTFOLIO OPTIMIZATION APPROACHES
Listing 10.8 Mean–variance portfolio optimization in SOCP form.
PMV <−function(SRoot, mu, SigMax, lambda, . . .){
1
Nassets <−length(mu)
2
ra <−1 −lambda
3
f <−−mu
4
C1 <−matrix(rep(0, Nassets), nrow = 1)
5
## No cone, but quadratic constraint
6
C2 <−diag(Nassets)
7
## Non−negativity constraint
8
C3 <−matrix(rep(−1, Nassets), nrow = 1)
9
## Budget constraint
10
C <−rbind(C1,
11
C2,
12
C3)
13
ZeroM <−matrix(0, nrow = Nassets + 1, ncol = Nassets)
14
A <−rbind(SRoot,
15
ZeroM)
16
b <−rep(0, nrow(A))
17
d <−c(SigMax ∗ra, rep(0, Nassets), 1)
18
ans <−Socp(f = f, A = A, b = b, C = C, d = d,
19
N = c(nrow(SRoot), rep(1, nrow(ZeroM))), . . .)
20
return(ans)
21
}
22
First, the mathematical program parameters are determined. The number of assets
(i.e., the number of columns of the object rzoo) is stored as object Nassets and
the sample size (i.e., the number of rows of the object rzoo) as object Nobs. Next
the variance–covariance matrix and its square root are computed and likewise the
dispersion matrix and its square root for the expected but uncertain returns. The
maximum allowable and feasible portfolio risk is equated to the maximum standard
deviation of the assets’ returns, which is stored as object SigMax. Finally, the square
root of quantile value for a conﬁdence level of 90% is determined, as is a sequence
of trade-off parameters between the portfolio risk and return. In a next step, the
points along the efﬁcient frontier of a mean–variance portfolio are computed in a
for loop. The portfolio risk and return as well as the weight vector are stored in
rows in the previously speciﬁed matrix object PmvAns. For the sake of completeness,
the point of a maximum-return portfolio will be added to these points. For this
example, the maximum-return portfolio would coincide with a 100% allotment to
Hong Kong equities. Similarly to the calculation of the mean–variance portfolio
points, the same exercise is carried out with respect to the robust optimization with
elliptical uncertainty for μ. The results are stored in the matrix object RobAns. As
shown in equation (10.17), a mean–variance equivalent trade-off parameter λ for a

ROBUST PORTFOLIO OPTIMIZATION
185
Listing 10.9 Robust optimization with elliptical uncertainty of μ.
## Setting of parameters
1
Nassets <−ncol(rzoo)
2
Nobs <−nrow(rzoo)
3
S <−cov(rzoo)
4
SR <−sqrm(S)
5
P <−S / Nobs
6
PR <−sqrm(P)
7
SigMax <−max(sqrt(diag(S)))
8
mu <−colMeans(rzoo)
9
delta <−sqrt(qchisq(0.9, Nassets))
10
lambda <−seq(0.3, 0.9, by = 0.1)
11
## Determining efﬁcient frontier for MV portfolio
12
PmvAns <−matrix(NA, nrow = length(lambda), ncol = Nassets + 2)
13
for(i in 1 : length(lambda)){
14
obj <−PMV(SRoot = SR, mu = mu, SigMax = SigMax,
15
lambda = lambda[i])
16
w <−obj$x
17
PmvAns[i, ] <−c(sqrt(t(w) %∗% S %∗% w), t(mu) %∗% w, w)
18
}
19
## Determining maximum return portfolio
20
PmrAns <−c(sd(rzoo[, "HSI"]), mean(rzoo[, "HSI"]))
21
## Determining robust portfolio
22
RobAns <−matrix(NA, nrow = length(lambda), ncol = Nassets + 2)
23
for(i in 1 : length(lambda)){
24
obj <−PR1(SRoot = SR, PRoot = PR, mu = mu, SigMax = SigMax,
25
lambda = lambda[i], delta = delta)
26
w <−obj$x[1 : ncol(SR)]
27
RobAns[i, ] <−c(sqrt(t(w) %*% S %*% w), t(mu) %∗% w, w)
28
}
29
## Equivalent parameter for theta = 0.3
30
theta2lambda <−function(theta, Nassets, Nobs, level){
31
delta <−sqrt(qchisq(level, Nassets))
32
lambda <−theta / (1 + theta ∗(delta / sqrt(Nobs)))
33
return(lambda)
34
}
35
lstar <−theta2lambda(0.3, Nassets = Nassets, Nobs = Nobs, level = 0.9)
36
wstar <−PMV(SRoot = SR, mu = mu, SigMax = SigMax, lambda = lstar)$x
37
mveq <−c(sqrt(t(wstar) %∗% S %∗% wstar), t(mu) %∗% wstar)
38

186
PORTFOLIO OPTIMIZATION APPROACHES
0
2
4
6
8
0.0
0.5
1.0
1.5
Portfolio Risk
Portfolio Return
●
●
●
●
●
●
●
●
Efficient Frontier
Robust Portfolio Solutions
Equivalent MV−Portfolio
Figure 10.3
Efﬁcient frontier of mean–variance and robust portfolios.
given value of θ for the robust optimization can be determined. This corresponding
value is computed in lines 30–35. The mean–variance solution pertinent to this
equivalent trade-off parameter is stored in the object mveq.
Figure 10.3 shows the resulting efﬁcient frontier with superimposed solutions for
robustly optimized portfolios and the equivalent mean–variance portfolio solution for
θ = 0.3. The corresponding R code is provided in Listing 10.10.
Listing 10.10 Plot of efﬁcient frontier.
## Efﬁcient frontier
1
EF <- rbind(PmrAns,
2
PmvAns[, c(1, 2)])
3
plot(EF, type = "l", xlim = c(0, 8), ylim = c(0, 1.5),
4
ylab = "Portfolio Return", xlab = "Portfolio Risk", lwd = 2)
5
points(RobAns[, c(1,2)], col = "blue", pch = 19, cex = 1.2)
6
points(mveq[1], mveq[2], col = "red", pch = 19, cex = 1.2)
7
legend("topleft", legend = c("Efﬁcient Frontier",
8
"Robust Portfolio Solutions", "Equivalent MV-Portfolio"),
9
lty = c(1, NA, NA), pch = c(NA, 19, 19),
10
col = c("black", "blue", "red"), lwd = 2)
11

ROBUST PORTFOLIO OPTIMIZATION
187
It can be seen that the robust optimization solutions selected are part of the
mean–variance efﬁcient frontier, though the solutions of the former fall short of
the latter. This becomes apparent when one compares the equivalent solutions of
the mean–variance and robust optimizations. The solution indicated as a point
to the far north-east is the corresponding mean–variance solution to the adjacent
robust counterpart, which lies to the south-west of it.
References
Ben-Tal A. and Nemirovski A. 1998 Robust convex optimization. Mathematics of Operations
Research 23(4), 769–805.
Boyd S. and Vandenberghe L. 2004 Convex Optimization. Cambridge University Press, Cam-
bridge.
Chalabi Y. and W¨urtz D. 2010 Rsocp: An R extension library to use SOCP from R. R package
version 271.1/r4910.
Cornuejols G. and T¨ut¨unc¨u R. 2007 Optimization Methods in Finance. Cambridge University
Press, Cambridge.
Davies P. 1987 Asymptotic behavior of S-estimators of multivariate location parameters and
dispersion matrices. Annals of Statistics 15, 1269–1292.
Donoho D. 1982 Breakdown properties of multivariate location estimators. Technical report,
Harvard University, Cambridge, MA.
(ed. Fabozzi F., Focardi S., Kolm P. and Pachamanova D.) 2007 Robust Portfolio Optimization
and Management. John Wiley & Sons, Inc., Hoboken, NJ.
Filzmoser P., Fritz H. and Kalcher K. 2011 pcaPP: Robust PCA by Projection Pursuit. R
package version 1.9-44.
Genz A. and Bretz F. 2009 Computation of Multivariate Normal and t Probabilities number
195 in Lecture Notes in Statistics. Springer-Verlag, Heidelberg.
Gnanadesikan R. and Kettenring J. 1972 Robust estimates, residuals and outlier detection with
multiresponse data. Biometrics 28, 81–124.
Hampel FR., Rochetti EM., Rousseeuw PJ. and Stahel WA. 1986 Robust Statistics: The
Approach Based on Inﬂuence Functions. John Wiley & Sons, Inc., New York.
Huber P. 1964 Robust estimation of a location parameter. Annals of Mathematical Statistics
35, 73–101.
Huber PJ. 1981 Robust Statistics. John Wiley & Sons, Inc., New York.
Lobo M., Vandenberghe L. and Boyd S. 1997 SOCP: Software for Second-Order Cone Pro-
gramming, User’s Guide Stanford University Stanford. Beta version.
Lobo M., Vandenberghe L., Boyd S. and Lebret H. 1998 Applications of second-order cone
programming. Linear Algebra and its Applications 284, 193–228.
Lopuha¨a H. 1991 Multivariate τ-estimators for location and scatter. Canadian Journal of
Statistics 19, 307–321.
Lopuha¨a H. 1992 Highly efﬁcient estimators of multivariate location with high breakdown
point. Annals of Statistics 20, 398–413.
Maronna R. and Zamar R. 2002 Robust estimates of location and dispersion of high-
dimensional datasets. Technometrics 44(4), 307–317.

188
PORTFOLIO OPTIMIZATION APPROACHES
Maronna R, Martin D. and Yohai V. 2006 Robust Statistics: Theory and Methods. John Wiley
& Sons, Inc., Hoboken, NJ.
Meucci A. 2005 Risk and Asset Allocation. Springer, New York.
Nesterov Y. and Nemirovsky A. 1994 Interior-Point Polynomial Methods in Convex Program-
ming vol. 13 of Studies in Applied Mathematics. SIAM, Philadelphia.
Pfaff B. 2010 Modelling Financial Risks: Fat Tails, Volatility Clustering and Copulae. Frankfurt
Allgemeine Buch, Frankfurt am Main.
Rousseeuw P. 1985 Multivariate estimation with high breakdown point In Mathematical Statis-
tics and Applications (ed. Grossmann W., Pﬂug G., Vincze I. and Wertz W.) vol. B Reidel
Publishing Dordrecht pp. 283–297.
Rousseeuw P. and Leroy A. 1987 Robust Regression and Outlier Detection. John Wiley &
Sons, Inc., New York.
Rousseeuw P., Croux C., Todorov V., Ruckstuhl A., Salibian-Barrera M., Verbeke T., Koller
M. and Maechler M. 2012 robustbase: Basic Robust Statistics. R package version 0.8-1-1.
Rowe B. 2012 tawny: Provides various portfolio optimization strategies including random
matrix theory and shrinkage estimators. R package version 2.0.2.
Scherer B. 2010 Portfolio Construction and Risk Budgeting 4th edn. Risk Books, London.
Sch¨ottle K. 2007 Robust Optimization with Application in Asset Management Dissertation
Technische Universit¨at M¨unchen.
Stahel W. 1981 Robuste Sch¨atzungen: Inﬁnitesimale Optimalit¨at und Sch¨atzungen von Kovar-
ianzmatrizen PhD thesis ETH Zurich.
Staudte RG. and Sheather SJ. 1990 Robust Estimation and Testing. John Wiley & Sons, Inc.,
New York.
Todorov V. and Filzmoser P. 2009 An object oriented framework for robust multivatiate
analysis. Journal of Statistical Software 32(3), 1–47.
T¨unt¨uc¨u R. and K¨onig M. 2004 Robust asset allocation. Annals of Operation Reserach 132,
132–157.
Venables W. and Ripley BD. 2002 Modern Applied Statistics with S 4th edn. Springer, New
York.
Wang J., Zamar R., Marazzi A., Yohai V., Salibian-Barrera M., Maronna R., Zivot E., Rocke
D., Martin D., Maechler M. and Konis K. 2010 robust: Insightful Robust Library. R package
version 0.3-11.
Wang N. and Raftery A. 2002 Nearest neighbor variance estimation (NNVE): Robust covari-
ance estimation via nearest neighbor cleaning (with discussion). Journal of the American
Statistical Association 97, 994–1019.
Wang N., Raftery A. and Fraley C. 2003 covRobust: Robust Covariance Estimation via Nearest
Neighbor Cleaning. R package version 1.0.
W¨urtz D., Chalabi Y., Chen W. and Ellis A. 2010 Portfolio Optimization with R/Rmetrics.
Rmetrics Association & Finance Online, www.rmetrics.org. R package version 2130.80.
Yohai V. 1987 High breakdown-point and high efﬁciency estimates for regression. Annals of
Statistics 15, 642–656.
Yohai V., Stahel W. and Zamar R. 1991 A procedure for robust estimation and inference in
linear regression In Directions in Robust Statistics and Diagnostics (Part II) (ed. Stahel W.
and Weisberg S.) vol. 34 of IMA Volumes in Mathematics and its Applications Springer
New York pp. 365–374.

11
Diversiﬁcation reconsidered
11.1
Introduction
It almost goes without saying that one purpose of wealth allocation is the diver-
siﬁcation of risks. That being so, the utility of a risk-averse investor is increased
when the wealth is allocated to an asset mix instead of holding a single risky asset.
Concepts and methods for the assessment of market risks have been presented in
Part II of this book. The risk measures derived from the statistical models presented
have been applied on an ex post basis – that is, for a given portfolio allocation and
its implied loss function, the riskiness of the investment can be assessed for a given
conﬁdence level. In this chapter and the next, approaches to portfolio optimization
are presented that directly address the issue of asset allocation such that the purpose
of risk diversiﬁcation is directly taken into account.
So far, the term ‘diversiﬁcation’ has been used rather loosely and the approaches
presented have not questioned the view that the variance–covariance matrix of the
returns is an appropriate statistic for measuring diversiﬁcation, except in the chap-
ter on modelling dependencies (i.e., copulae). Diversiﬁcation entails at least two
dimensions. The ﬁrst dimension addresses the question of the underlying common
characteristic with respect to which the assets are diverse. The second addresses the
question how to measure the degree of diversiﬁcation with respect to this character-
istic. For instance, a risk-averse long-only investor would not be better off if he held
a portfolio with strong co-movements in sign of the returns compared to an asset mix
where the returns of the constituent assets do not share this characteristic, ceteris
paribus. The variance–covariance matrix of the returns is then often employed as the
measure to assess the riskiness of the assets and the dependencies between them. But
here diversiﬁcation is deﬁned simply with respect to the overall dependencies in the
returns, measured correctly with the variance–covariance matrix. In general, this is
no safeguard against risk per se.
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

190
PORTFOLIO OPTIMIZATION APPROACHES
In Section 11.2 the focus is ﬁrst on how to measure the degree of diversiﬁcation
and how to determine a portfolio allocation such that it is ‘most diversiﬁed’. Then, in
Section 11.3, ‘diversiﬁcation’ is characterized by the contribution of the portfolio’s
constituent assets to the overall portfolio risk. Here diversiﬁcation is favourable if the
risk contributions are not concentrated on only a few assets, and hence a portfolio al-
location is regarded as well diversiﬁed if each of its constituents contributes an equal
amount to the overall portfolio risk. In Section 11.4, in which diversiﬁcation is recon-
sidered, optimal tail-(in)dependent portfolios are constructed. Now ‘diversiﬁcation’
is used in the sense of extreme market events, such that the focus is shifted towards
the portfolio’s behaviour in overall market downturns. This can then be measured by
the (lower) tail dependence coefﬁcient (TDC; see Chapter 9). In Section 11.5, the R
packages in which the portfolio optimization approaches either are directly imple-
mented or can be employed for optimization are presented, and the chapter concludes
with examples in which the solutions of the most diversiﬁed, the equal-risk con-
tributed and the minimum tail-dependent portfolio approaches are contrasted with
the allocation of the global-minimum variance portfolio. All optimization approaches
have a common goal, namely to minimize investor’s risk through diversiﬁcation, but
they differ in the two dimensions of ‘diversity with respect to which characteristic?’
and the ‘how to measure it?’.
11.2
Most diversiﬁed portfolio
In a series of papers Choueifaty and Coignard (2008) and Choueifaty et al. (2011)
addressed the theoretical and empirical properties of portfolios when diversiﬁcation
is used as a criterion. In order to do so, they ﬁrst established a measure by which
the degree of diversiﬁcation for a long-only portfolio can be assessed. Let  denote
the variance–covariance matrix of the returns for N assets and σ the vector of asset
volatilities measured by their respective standard deviations. The diversiﬁcation ratio
(DR) is then deﬁned for a given (N × 1) weight vector ω in the allowed set of portfolio
solutions  as
DRω∈ =
ω′σ
√
ω′ω
.
(11.1)
The numerator is the weighted average volatility of the single assets, and is given
by the scalar product of the weight vector and the standard deviations of the assets’
returns. The denominator is the portfolio standard deviation. By this deﬁnition, the
higher the DR, the more the portfolio is diversiﬁed. This ratio has a lower bound
of one, which will only be achieved in the case of a single-asset portfolio. Portfolio
solutions that are characterized by either a highly concentrated allocation or highly
correlated asset returns would qualify as being poorly diversiﬁed. The authors conﬁrm
this statement by considering the following decomposition of the diversiﬁcation ratio
(proof provided in Appendix A of Choueifaty et al. (2011)):
DRω∈ =
1
√ρ + CR) −ρCR
(11.2)

DIVERSIFICATION RECONSIDERED
191
where ρ and CR denote the volatility-weighted average correlation and the volatility-
weighted concentration ratio, respectively. The former is deﬁned as
ρω∈ =
N
i̸= j(ωiσiω jσ j)ρi j
N
i̸= j(ωiσiω jσ j)
,
(11.3)
and the latter as the normalized Herﬁndahl–Hirschmann index (see Hirschman 1964),
CRω∈ =
N
i=1(ωiσi)2
(N
i=1 ωiσi)2 ,
(11.4)
which lies in the interval [1/N, 1].
The partial derivatives of equation (11.2) are
∂DR
∂ρ
= −1
2
1 −CR
√ρ + CR −ρCR
ρ + CR −ρCR ,
(11.5a)
∂DR
∂CR = −1
2
1 −ρ
√ρ + CR −ρCR
ρ + CR −ρCR
(11.5b)
from which the correctness of the relationship stated above can directly inferred.
Because of the relationship between the number of assets and the CR with respect
to the lower bound of the CR, a comparison of different portfolio solutions is only
valid if these have been derived from the same set of investment opportunities.
Furthermore, the DR depends only on the volatility-weighted average correlations in
the case of a naive (i.e., equally weighted) allotment between assets. If one assumes
that the assets’ expected excess returns are proportional to their respective volatilities,
then the diversiﬁcation ratio is also proportional to the Sharpe ratio of a portfolio.
Hence, under this assumption the solutions of a maximum DR portfolio and that
of the tangency portfolio will coincide. Of course, excess returns do not enter into
equation (11.1) explicitly, but by cross-comparing the allocation of a portfolio which
maximizes the DR with the solution of the tangency portfolio, one can infer for which
assets a higher/lower return expectation has implicitly been factored in.
Choueifaty et al. (2011) then state the condition for a most diversiﬁed portfolio
(MDP):
PMDP = arg max
ω∈
DR.
(11.6)
By introducing a set of synthetic assets that share the same volatility, the diversiﬁca-
tion ratio is maximized by minimizing ω′Cω, where C denotes the correlation matrix
of the initial assets’ returns. Hence, the objective function coincides with that for a
GMV portfolio, but instead of using the variance–covariance matrix, the correlation
matrix is employed. The ﬁnal weights are then retrieved by rescaling the intermediate
weight vector (based on the correlation matrix) with the standard deviations of the
assets’ returns. Put differently, the optimal weight vector is determined in two stages.

192
PORTFOLIO OPTIMIZATION APPROACHES
First, an allocation is determined that yields a solution for a least correlated asset
mix. This solution is then inversely adjusted by the asset volatilities. This proce-
dure yields a different outcome than the GMV, because in the latter approach the
asset volatilities enter directly into the quadratic form of the objective function to be
minimized. Hence, the impact of the assets’ volatilities is smaller for an MDP than
for a GMV portfolio. This can be directly deduced by intuitive reasoning from the
decomposition of the DR as in equations (11.2)–(11.4). The asset volatilities only
enter the computation of the CR as a relative share. Finally, the MDP possesses the
following two core properties (see Choueifaty et al. 2011, pp. 7ff.):
1. ‘Any stock not held by the MDP is more correlated to the MDP than any of the
stocks that belong to it. Furthermore, all stocks belonging to the MDP have
the same correlation to it.’
2. ‘The long-only MDP is the long-only portfolio such that the correlation be-
tween any other long-only portfolio and itself is greater than or equal to the
ratio of their DRs.’
The second core property can be stated equivalently by noting that the more a long-
only portfolio is diversiﬁed, the higher will be the correlation of the portfolio’s returns
with that of the MDP solution.
11.3
Risk contribution constrained portfolios
In this section the term ‘diversiﬁcation’ is applied to the portfolio risk itself. The
portfolio risk can in principle be captured by either a volatility-based measure (i.e.,
the portfolio’s standard deviation) or a downside-based measure (i.e., the conditional
value at risk (CVaR) or expected shortfall). Hence, two approaches will be presented
here. Heuristically these approaches are motivated by the empirical observation that
the risk contributions are a good predictor for actual portfolio losses, and hence, by
diversifying directly on these contributions, portfolio losses can potentially be limited
compared to an allocation which witnesses a high risk concentration on one or a few
portfolio constituents (see Qian 2005).
In the ﬁrst approach, an asset allocation is sought such that the contributions
of the portfolio’s constituents contribute the same share to the overall portfolio
volatility. In this case, the equal-weight allocation is now applied to the assets’ risk
contributions, leading to the term ‘equal-risk contribution’ (ERC). Diversiﬁcation
is therefore deﬁned and achieved by a weight vector that is characterized by a
least concentrated portfolio allocation with respect to the risk contributions of its
constituents. It was introduced in the literature by Qian (2005, 2006, 2011) and the
properties of this portfolio optimization approach were analysed in Maillard et al.
(2009, 2010). Zhu et al. (2010) show how this approach can be adapted when the risk
contributions are constrained or budgeted.
In the second approach, the risk contributions of the portfolio constituents are
measured by the portfolio’s downside risk, such as the CVaR. The term ‘diversiﬁca-
tion’ is now related to the notion that the downside risk contributions of the assets

DIVERSIFICATION RECONSIDERED
193
contained in a portfolio are either bounded by an upper threshold or evenly spread
out between the constituents. This approach was introduced by Ardia et al. (2010)
and Boudt et al. (2010, 2011a), based on results in Boudt et al. (2007, 2008) and
Peterson and Boudt (2008). An Internet site dedicated to this approach can be found
at http://www.econ.kuleuven.be/public/N06054/riskbudgets.htm.
In the following paragraphs, ERC portfolios are more formally presented and then
the focus is shifted towards the second kind of portfolio optimization which limits
the contributions to a downside risk measure. Recall from Section 4.3 the deﬁnition
of the risk contribution of an asset, which is restated in more general terms below:
Ci Mω∈ = ωi
∂Mω∈
∂ωi
,
(11.7)
where Mω∈ denotes a linear homogeneous risk measure and ωi is the weight of
the ith asset. As such, the risk measure Mω∈ can be either the portfolio’s standard
deviation, the value at risk or the expected shortfall. All of these measures have in
common the above characteristic and hence, by Euler’s homogeneity theorem, the
total portfolio risk is equal to the sum of the risk contributions as deﬁned in equation
(11.7). These contributions can also be expressed as percentage ﬁgures by dividing
the risk contributions by the value of Mω∈:
%Ci Mω∈ = Ci Mω∈
Mω∈
× 100.
(11.8)
In the empirical application, the marginal contributions will be provided in this
notation.
If one inserts the formula for the portfolio standard deviation σ(ω) =
√
ω′ω for
Mω∈, where ω is the (N × 1) weight vector and  denotes the variance–covariance
matrix of asset returns with off-diagonal elements σi j and with σ 2
i the ith element
on its main diagonal (i.e., the variance of the returns for the ith asset), then partial
derivatives in the above equation are given by
∂σ(ω)
∂ωi
=
ωiσ 2
i + N
i̸= j ω jσi j
σ(ω)
.
(11.9)
These i = 1, . . . , N partial derivatives are proportional to the ith row of (ω)i and
hence the problem for an ERC portfolio with a long-only and a budget constraint can
be stated as
PERC : ωi(ω)i = ω j(ω) j,
∀i, j,
0 ≤ωi ≤1,
for i = 1, . . . , N,
ω′i = 1,
(11.10)
where i is an (N × 1) vector of 1s. A solution to this problem can be found
numerically by minimizing the standard deviation of the risk contributions. The
optimal ERC solution is valid if the value of the objective function is equal to zero,
which is only the case when all risk contributions are equal. A closed-form solution
can only be derived under the assumption that all asset pairs share the same correla-
tion coefﬁcient. Under this assumption optimal weights are determined by the ratio

194
PORTFOLIO OPTIMIZATION APPROACHES
of the inverse volatility of the ith asset and the average of the inverse asset volatilities.
Assets with a more volatile return stream are penalized by a lower weight in an ERC
portfolio (see Maillard et al. 2010). It was further shown by these authors that with
respect to the portfolio’s standard deviation, the ERC solution takes an intermediate
position between the solution of a GMV and an equal-weighted portfolio. They also
showed that under the assumption of constant correlations and if the Sharpe ratios of
the assets are all identical, then the ERC solution coincides with that of the tangency
portfolio.
If one inserts the portfolio’s CVaR instead of its standard deviation as a measure
of risk in equation (11.7), one obtains for the marginal CVaR contribution of the ith
asset to the portfolio’s CVaR,
CiCVaRω∈,α = ωi
∂CVaRω∈,α
∂ωi
.
(11.11)
Obviously, by employing the CVaR or any other quantile-based risk measure, all
portfolio optimizations will be dependent on the prespeciﬁed nuisance parameter α,
which is the conﬁdence level pertinent to the downside risk. The derivation of the
partial derivatives in equation (11.11) depends on the distribution model assumed, but
can be provided fairly easily in the case of elliptical distributions. As such the CVaR
either based on the normal assumption or modiﬁed by the Cornish–Fisher extension
is assumed in practice (see Section 4.3). If one assumes that the returns follow a
multivariate normal distribution, then the CVaR of a portfolio allocation is given by
CVaRω∈,α = −ω′μ +
√
ω′ω′ φ(zα)
α
(11.12)
where μ denotes the (N × 1) vector of expected returns, φ the standard normal
density function and zα the α-quantile of the standard normal distribution. The
marginal contribution of the ith asset is then given by
CiCVaRω∈,α = ωi

μi +
(ω)i
√
ω′ω′
φ(zα)
α

(11.13)
It was shown in Scaillet (2002) that the CVaR contributions are equal to the loss
contributions for portfolio losses that exceed the portfolio’s VaR at the conﬁdence
level α. Incidentally, if one further assumes that the assets’ returns behave like a
random walk and hence the best forecast for the expected returns is μ = 0, then
the portfolio allocation obtained by equating all marginal CVaR contributions will
be identical to an ERC allocation, regardless of the speciﬁed conﬁdence level. This
is because the ﬁrst summand drops out of the objective function and φ(zα)/α is a
constant which does not affect the portfolio composition. However, equation (11.13)
could be fruitfully employed in portfolio optimizations when either placing an upper
bound on the marginal CVaR contributions or demanding a well-diversiﬁed allocation
with respect to the constituents’ downside risk contributions. The latter objective can
be achieved by minimizing the maximum marginal CVaR contribution:
Cω∈,α = max CiCVaRω∈,α,
(11.14)

DIVERSIFICATION RECONSIDERED
195
as proposed in Boudt et al. (2011a). The authors term this approach ‘minimum
CVaR concentration’ (MCC), as it turns out that by using this objective in practice
one ordinarily obtains a more balanced solution—not equal—with respect to the
marginal contributions and the CVaR level itself is reasonably low, compared to a
solution where the risk contributions are budgeted (BCC). Instead of minimizing the
maximum marginal contribution to CVaR, one could directly utilize a measure of
divergence instead.
11.4
Optimal tail-dependent portfolios
The concept presented in this section can be viewed as a synthesis of the approaches
presented in the previous two sections. In an MDP portfolio an asset allocation is
determined which yields the greatest diversiﬁcation, by deﬁnition. However, this ap-
proach is based on the symmetric correlations between the assets. As already pointed
out in Chapter 9, the Pearson correlation coefﬁcient only measures the dependence
between two random variables correctly if these are jointly normally (elliptically)
distributed. Furthermore, covariance/correlation takes deviations below and above
the mean into account, but a risk-averse (long-only) investor seeks positive returns
(the upside or right tail of the distribution) and only considers negative returns (the
downside) as risk. This point was made by Markowitz (1959) when he noted the
appropriateness of the lower partial second moment as a dispersion measure. In
that sense, the lower tail dependence coefﬁcient measures the strength of the rela-
tionship between asset returns when both are extremely negative at the same time,
and is therefore a downside risk measure as VaR or CVaR is. Dependent on the
underlying copula assumption (e.g., the EVT copula and/or the empirical copula),
the value of the TDC also depends on the count of ranked observations used in its
calculation, which is akin to the conﬁdence level of VaR/CVaR. For Archimedean
copulae the TDC only depends on the copula parameter. In this section the TDC for
a distributional copula, such as the Student’s t copula, is discarded because it is a
dependence measure for the lower and the upper tail and, hence, does not ﬁt well
in the task of risk management (see Section 9.3). In the following a brief account
of the non-parametric estimation of the TDC is given, followed by a description
of how optimal portfolio solutions can be derived from it. A general account and
synopsis of tail dependence is provided, for instance, in Coles et al. (1999) and
Heffernan (2000).
A synopsis of the non-parametric TDC estimators is provided in Dobri´c and
Schmid (2005), Frahm et al. (2005) and Schmidt and Stadtm¨uller (2006). Let (X, Y)
denote the percentage losses of two investments. The lower tail dependence coef-
ﬁcient, λL, between these two random variables is invariant with respect to strictly
increasing transformations applied to (X, Y) and does not depend on the marginal
distributions of the assets’ returns. It is solely a function of the assumed copula. The
joint distribution function of X and Y is given by
FX,Y(x, y) = P(X ≤x, Y ≤y),
for (x, y) ∈R2.
(11.15)

196
PORTFOLIO OPTIMIZATION APPROACHES
This bivariate distribution can be stated equivalently in terms of the copula, C, as
FX,Y(x, y) = C(FX(x), FY(y)),
(11.16)
where FX(x) and FY(y) are the marginal distributions of X and Y, respectively. The
copula is the joint distribution of the marginal distribution functions: C = P(U ≤
u, V ≤v), with U = FX(x) and V = FY(y), and therefore maps from [0, 1]2 into
[0, 1]. If the limit exists, then the lower tail dependence coefﬁcient is deﬁned as
λL = lim
u→0
C(u, u)
u
(11.17)
This limit can be interpreted as a conditional probability and as such the lower tail
dependence coefﬁcient is bounded in the interval [0, 1]. The bounds are realized for
an independence copula (λL = 0) and a co-monotonic copula (λL = 1), respectively.
The non-parametric estimators for λL are derived from the empirical copula. For
a given sample of N observation pairs, (X1, Y1), . . . , (X N, YN), with corresponding
order statistics X(1) ≤X(2) ≤. . . ≤X(N) and Y(1) ≤Y(2) ≤. . . ≤Y(N), the empirical
copula is deﬁned as
CN
 i
N , j
N

= 1
N
N

l=1
I(Xl ≤X(i) ∧Yl ≤Y( j))
(11.18)
with i, j = 1, . . . , N and I is the indicator function, which takes a value of 1 if the
condition stated in the parentheses is true. By deﬁnition CN takes a value of zero for
i = j = 0.
In the literature cited above, three consistent and asymptotically unbiased estima-
tors for λL are provided, which all depend on a threshold parameter k (i.e., the number
of order statistics considered). The ﬁrst estimator, λ(1)
L (N, k), is an approximation of
the derivative of λL with respect to u by the slope of a secant in the neighbourhood
of it (note that u is written as k/N):
λ(1)
L (N, k) =
 k
N
−1
· CN
 k
N , k
N

.
(11.19)
The second estimator is based on the slope coefﬁcient of a simple afﬁne linear
regression between the values of the copula as regressand and the tail probabilities
i/N, i = 1, . . . , k, as the regressor. It is deﬁned as
λ(2)
L (N, k) =
 k

i=1
 i
N
2	−1
·
k

i=1
 i
N · CN
 i
N , i
N

.
(11.20)
A third means of estimating λL non-parametrically is derived from a mixture of the
co-monotonic and independence copulae. Here the lower tail dependence coefﬁcient

DIVERSIFICATION RECONSIDERED
197
is the weighting parameter between these two copulae. The estimator is then deﬁned
as
λ(3)
L (N, k) =
k
i=1

CN
 i
N , i
N

−
 i
N
2 
 i
N

−
 i
N
2
k
i=1

i
N −
 i
N
22
.
(11.21)
Of crucial importance for estimating the lower tail dependence coefﬁcient is
an appropriate selection of k. This parameter is akin to the threshold value for
the peaks-over-threshold method in the ﬁeld of EVT and hence the trade-off be-
tween bias and variance is applicable here, too. Choosing k too small will result
in an imprecise estimate, and too high in a biased estimate. The conditions stated
above of consistency and unbiasedness are met if k ∼
√
N, as shown in Dobri´c and
Schmid (2005).
The information on the size of lower tail dependence between assets can be
utilized in several ways. For instance, a straightforward application would be to
replace Pearson’s correlation matrix—employed in determining the allocation for a
most diversiﬁed portfolio—with the lower tail dependence coefﬁcients, whereby the
main diagonal elements are set equal to one. After rescaling the weight vectors by the
assets’ volatilities, one would obtain a ‘most diversiﬁed/minimum tail-dependent’
portfolio. Alternatively, if one ranked the TDCs between the constituent assets of a
benchmark and the benchmark itself, one could employ this information to select
the ﬁnancial instruments that are the least dependent with extreme losses in the
benchmark, but independent with respect to the upside. It should be clear that this
asset selection will yield a different outcome than following a low-β strategy. Even
though the present account has focused on the lower tail dependence, the approach
sketched above for selecting constituent assets could also be based on the difference
between the upper and lower tail dependence. These two TDCs could be retrieved
from the Gumbel and Clayton copula, respectively. This procedure is akin to selecting
assets according to their risk–reward ratio. However, it should be stressed that using
tail dependence as a criterion in portfolio optimization should always be accompanied
by an additional measure of risk for the assets. A low value of tail dependence is
non-informative with respect to the asset risk in terms of either its volatility or its
downside risk.
11.5
Synopsis of R packages
11.5.1
The packages DEoptim and RcppDE
It should be evident from the portfolio optimizations presented above, in particular
the equal-risk contributions to the ES of an asset allocation, that the task of ﬁnding a
best global solution can become quite complicated from a numerical point of view. In
light of this and also due to the dependence of the package PortfolioAnalytics (see
Section 11.5.3) on DEoptim, a separate presentation of R packages that implement
differential evolution is in order. Of course, the solvers implemented are not solely

198
PORTFOLIO OPTIMIZATION APPROACHES
dedicated to ﬁnding a portfolio allocation which gives equal marginal contributions
to the ES for a given conﬁdence level, but can also be used in other optimizations.
The Differential Evolution (DE) algorithm was introduced by Storn and Ulrich
(1997) and its application is elucidated in Price et al. (2006). It is classiﬁed as a
genetic algorithm and is hence a derivative-free global optimizer. In this respect, DE
is an evolutionary algorithm where an initial population of candidate solutions is
subjected to alteration and selection operations to yield a global minimum of the ob-
jective function. Because the DE algorithm is derivative-free, the only requirements
are that the objective function and the parameters are real-valued. Neither differen-
tiability nor continuity of the objective function is necessary. However, because the
initial population of candidate solutions is random, one should set a seed for the
random number generator in order to replicate results. A thorough account of genetic
algorithms can be found in Mitchell (1998).
Historically, the ﬁrst package on CRAN to implement the DE algorithm was
DEoptim by Mullen et al. (2011). The package is contained in the ‘Optimization’
Task View. In earlier releases, the DE algorithm was implemented solely in R (written
by David Ardia), but in order to improve the computation speed the DE routines were
ported to the C language, similar to the MS Visual C++ implementation in Price
et al. (2006). The package utilizes S3 classes and methods and is shipped with two
vignettes. Incidentally, in one of the vignettes Ardia et al. (2010) give an example
showing a large-scale portfolio optimization using the function DEoptim() which
fails if gradient-based optimizations are used.
The function DEoptim() is speciﬁed with four arguments in its closure and the
ellipsis argument. The objective function to be minimized is provided as argument
fn. The function should return a real scalar value. Lower and upper bounds for the
parameters can be set by means of the arguments lower and upper, respectively.
Incidentally, constraints other than box constraints must be speciﬁed as penalty
terms in the objective function. Constraints can be directly included in the objective
function as additive terms. Each constraint is multiplied by a positive constant factor.
A violation of a constraint would, however, increase the value of the objective function
to be minimized. The algorithm can be tailored by the control argument, which
expects a list object returned by the function DEoptim.control().
The function DEoptim() returns a list object of S3 class DEoptim for which
a summary() and a plot() method are available. The list object consists of
two named elements, optim and member, which are themselves list objects. The
former contains the optimal solution (bestmem), the value of the objective function
evaluated at the optimimum (bestval), the count of function evaluations (nfeval)
and the number of procedure iterations (iter). The latter list element contains
the speciﬁed lower and upper bounds (lower and upper), the best value of the
objective function at each iteration (bestvalit) and its associated parameter values
(bestmemit) as well as the population generated at the last iteration (pop) and a list
of the intermediate populations (storepop). As indicated above, the user can ﬁne-
tune the algorithm by means of the function DEoptim.control(), which returns
a list with class attribute of the same name. Here, the optimization process can
be terminated by specifying a value of the objective function (argument VTR). Its

DIVERSIFICATION RECONSIDERED
199
default value is set to -Inf; if changed to a higher value, the process is terminated
when either the maximum number of iterations is reached (which can be changed
by the argument itermax with a default value of 200) or the real value of the
objective function is less than VTR. Further, the user can control the termination of
the optimization process by specifying a value for the relative convergence tolerance
(argument reltol). Depending on the platform on which R is run, this amounts
roughly to 1-e8. Next, the user can choose between six strategies for propagating
the populations from one iteration to the next by means of the argument strategy.
These strategies are detailed in the package manual and vignette and are based on
the speciﬁcation as in Price et al. (2006). The number of population members is
determined from the argument NP. If its default value of NA remains unchanged, then
10 sets of parameter vectors are created randomly. On the other hand, the user can
provide a matrix object for the argument initialpop in which the initial population
of parameter vectors is speciﬁed. The row dimension of this matrix refers to the
parameters of the objective function and the column dimension to the population.
The parameters’ progress through the iterations can be controlled by the arguments
storepopfrom and storepopfreq. The former sets the generation from which the
following intermediate population will be stored with a default setting such that no
intermediate population is stored. The latter argument determines the frequency with
which the populations are stored. Its default value is 1, whereby every intermediate
population is stored. The step size from one iteration to the next is determined by the
value assigned to F. The remaining arguments in the function’s closure are pertinent
to the kind of parameter heritage and are partly dependent on the strategy chosen. The
reader is referred to the package manual for more details on these controls. Finally,
the progress of the optimization can be monitored by means of the argument trace.
If set to TRUE (the default) then the intermediate results of every iteration are printed,
and if speciﬁed as an integer value i then the outcome of only every ith iteration is
output.
The package RcppDE is closely related to the package DEoptim (see Eddelbuet-
tel 2012). It is also contained in the CRAN ‘Optimization’ Task View. The major
difference between the two packages is the interface to the DE algorithm. As the
package’s name implies, the routines are interfaced from code written in C++. As
shown in the package’s vignette by cross-comparison of this implementation with
the C-based one in DEoptim, the former results in a faster execution of various
optimization tasks. In contrast to the implementation in the package DEoptim, the
function DEoptim() now has an additional argument env for deﬁning the environ-
ment in which the objective function is evaluated. If not speciﬁed, then the function
assigned to fn will be evaluated in a new environment. The methods deﬁned for
objects with class attribute DEoptim are also available, as well as control of the
algorithm’s behaviour using DEoptim.control().
11.5.2
The package FRAPO
This subsection presents the functions and methods of the package FRAPO that
relate to the topics introduced in this chapter.

200
PORTFOLIO OPTIMIZATION APPROACHES
The functions dr(), cr() and rhow() can be used to measure the degree of
diversiﬁcation for a given weight vector and variance–covariance matrix. These
function return the diversiﬁcation ratio, concentration ratio and volatility-weighted
average correlation as introduced in Section 11.2. All three functions have the same
closure, consisting of arguments for the weight vector (weight) and the variance–
covariance matrix (Sigma). The solution of the most diversiﬁed portfolio can be
computed with the function PMD(). The array of returns is provided as argument
Returns, and the logical argument percentage determines whether the weights
are returned as percentages (the default) or decimal values. The ellipsis argument is
passed down to the call of cov(), allowing the user to control how the variance–
covariance matrix is computed. The resulting object is then converted to a correlation
matrix by means of the function cov2cor(). The function PMD() returns an ob-
ject of formal S4 class PortSol for which show(), Weights(), Solution() and
update() methods are available.
The marginal contributions to risk for a given asset allocation and dispersion
matrix are returned by the function mrc(). In addition to the arguments weights
and Sigma for the weight vector and the variance–covariance matrix, the user can
specify whether the contributions are returned as percentages that sum to 100% (the
default) or as decimal numbers. The solution of an equal risk contribution portfolio
can be determined with the function PERC(). The function’s closure takes three
arguments: Sigma for the variance–covariance matrix, par for the initial weight
vector to be used, and the ellipsis argument which is passed down to the nlminb()
function in order to carry out the optimization. By default par = NULL and an equal-
weights vector is used in the call to nlminb(). In addition to the objective and initial
values, the arguments lower = 0 and upper = 1 are set in the optimization call.
Finally, for optimal tail-dependent portfolio construction the functions tdc() and
PMTD() are available. The former function returns the bivariate TDCs; the user can
choose between a non-parametric estimation according to the empirical tail copula
(the default) or the stable tail function by means of the method argument. The array
of returns has to be provided for the argument x with the only requirement that it can
be coerced to a matrix object. Whether lower or upper tail dependence coefﬁcients
are returned is determined by the logical argument lower, with default value TRUE.
The threshold value k is speciﬁed as NULL, and if not otherwise stated the square
root of the count of observations is used. The ellipsis argument is passed down
to the rank() function, hence the user can determine how the order statistics are
computed with respect to ties in the data. The function returns a matrix object with
the bivariate TDCs and 1s on the diagonal as by deﬁnition a series has a dependence
of unity with itself. For a long-only investor, the minimum tail-dependent portfolio
solution is returned by calling PMTD(). The function body is akin to that of the
function PGMV() by which the solution of a global minimum-variance portfolio is
returned. But instead of employing the variance–covariance matrix as a measure of
dispersion, the TDC matrix returned from the call to tdc() is utilized. In addition to
the arguments pertinent to this function, the array of returns has to be assigned to the
argument Returns. The function returns an object of formal S4 class PortSol for
which the above-mentioned methods are available.

DIVERSIFICATION RECONSIDERED
201
11.5.3
The package PortfolioAnalytics
The package PortfolioAnalytics (see Boudt et al. 2011b) takes a conceptually differ-
ent approach to portfolio optimization compared to the packages discussed above. It
allows the user to obtain a numerical portfolio solution for complex constraints and/or
objective functions. With regard to the risk measure related approaches to portfolio
optimization presented in this chapter, one could directly use a function that returns
the risk measure in question as the objective function, for instance. The numerical op-
timization is then carried out either with the differential evolution optimizer contained
in the package DEoptim (see Mullen et al. 2011) or by means of randomly generated
portfolios obeying the speciﬁed constraints. The former algorithm was introduced
by Storn and Ulrich (1997). The general approach is that the objective function is
amended by the constraint functions, which are multiplied by penalty factors. The
package is hosted on R-Forge and is shipped with a vignette showing how it can be
used to optimize a portfolio with CVaR budgets.
The three cornerstone functions are constraints() for deﬁning the de-
sired constraints on the portfolio weights, add.objective() for adding the
objective function to the list of constraints returned by constraints(), and op-
timize.portfolio() for carrying out the numerical optimization of the portfolio
problem thus stated. The constraints() function returns a list object containing
the constraints of informal S3 class constraint. For objects of this kind an is()
and an update() method are available. The function optimize.portfolio()
returns an object of informal S3 class optimize.portfolio.DEoptim, opti-
mize.portfolio.random or optimize.portfolio. For such objects plot()
and extractStats() methods are provided. The latter method returns statistics
for the optimal portfolio solution. The values of the objective function for a given
set of weights can be queried by the function constrained_objective(). Two
utility functions are worth highlighting: optimize.portfolio.rebalancing()
for
carrying
out
portfolio
rebalancing
at
a
speciﬁed
frequency
and
optimize.portfolio.parallel() for carrying out multiple calls to opti-
mize.portfolio() on a multi-processor computer. The default value is set to
use four nodes. The remaining functions of the package are primarily intended for
plotting and querying/extracting information from optimal portfolio solutions.
11.6
Empirical applications
11.6.1
Comparison of approaches
The ﬁrst R code example (Listing 11.1) shows how the portfolio solutions for the
sector indexes of the Swiss Performance Index (SPI) can be determined according to
a global minimum variance, equal-risk contribution, most diversiﬁed and minimum
tail dependence allocation. All optimizations have in common that no return forecasts
are required, but that the shares to be invested are solely dependent on different risk
measures. Hereby, the GMV allocation will serve as the benchmark in the portfolio
comparisons. In addition to the allocations themselves, the marginal risk contributions

202
PORTFOLIO OPTIMIZATION APPROACHES
Listing 11.1 Comparison of portfolio solution for Swiss equity sectors.
library(FRAPO)
1
library(fPortfolio)
2
library(lattice)
3
## Loading data and calculating returns
4
data(SPISECTOR)
5
Idx <−interpNA(SPISECTOR[, −1], method = "before")
6
R <−returnseries(Idx, method = "discrete", trim = TRUE)
7
V <−cov(R)
8
## Portfolio optimizations
9
GMVw <−Weights(PGMV(R))
10
MDPw <−Weights(PMD(R))
11
MTDw <−Weights(PMTD(R))
12
ERCw <−Weights(PERC(V))
13
## Combining solutions
14
W <−cbind(GMVw, MDPw, MTDw, ERCw)
15
MRC <−apply(W, 2, mrc, Sigma = V)
16
rownames(MRC) <−colnames(Idx)
17
colnames(MRC) <−c("GMV", "MDP", "MTD", "ERC")
18
## Plot of allocations
19
oldpar <−par(no.readonly = TRUE)
20
par(mfrow = c(2, 2))
21
dotchart(GMVw, xlim = c(0, 40), main = "GMV Allocation", pch = 19)
22
dotchart(MDPw −GMVw, xlim = c(−20, 20), main = "MDP vs. GMV",
23
pch = 19)
24
abline(v = 0, col = "grey")
25
dotchart(MTDw −GMVw, xlim = c(−20, 20), main = "MTD vs. GMV",
26
pch = 19)
27
abline(v = 0, col = "grey")
28
dotchart(ERCw −GMVw, xlim = c(−20, 20), main = "ERC vs. GMV",
29
pch = 19)
30
abline(v = 0, col = "grey")
31
par(oldpar)
32
## Lattice plots of MRC
33
Sector <−factor(rep(rownames(MRC), 4),
34
levels = sort(rownames(MRC)))
35
Port <−factor(rep(colnames(MRC), each = 9),
36
levels = colnames(MRC))
37
MRCdf <−data.frame(MRC = c(MRC), Port, Sector)
38
dotplot(Sector MRC | Port, groups = Port, data = MRCdf,
39
xlab = "Percentages",
40
main = "MR Contributions by Sector per Portfolio",
41
col = "black", pch = 19)
42
dotplot(Port ˜ MRC | Sector, groups = Sector, data = MRCdf,
43
xlab = "Percentages",
44
main = "MR Contributions by Portfolio per Sector",
45
col = "black", pch = 19)
46

DIVERSIFICATION RECONSIDERED
203
and the overall portfolio characteristics will be evaluated. The sectors covered are:
basic materials (BASI), industrial (INDU), consumer goods (CONG), health care
(HLTH), consumer services (CONS), telecommunications (TELE), utilities (UTIL),
ﬁnancial (FINA) and technology (TECH).
In lines 1–3 of Listing 11.1 the necessary packages are loaded. All optimizations
will be carried out with the functions provided in FRAPO. The data set is taken from
the package fPortfolio and the base package lattice is used in the graphical analysis
of the results. In the next lines the level data of the sector indexes are extracted from
SPISECTOR, NA values are interpolated and discrete daily returns and their variance–
covariance matrix are computed. The sample starts on 30 December 1999 and ends
on 17 October 2008, giving a total of 2216 observations per sector. The allocations
according to the four portfolio optimizations are then extracted by means of the
Weights() method and assigned to the objects GMVw, MDPw, MTDw and ERCw for
the global-minimum variance, most diversiﬁed, minimum tail-dependent and equal-
risk contributed solutions, respectively. The weights are then gathered in the matrix
object W. This object is used in the call to the apply() function for determining the
marginal risk contributions for each of the four optimal weight vectors. The results
are graphically exhibited in code lines 20 onwards.
First, the weights are displayed in the form of dot charts. Asset allocations are
usually portrayed as pie charts. However, this kind of visualization is problematic
because the relation of surfaces and their sizes cannot be well digested by eyeball
inspection. In the top left panel of Figure 11.1 the sector allocations according to the
BASI
INDU
CONG
HLTH
CONS
TELE
UTIL
FINA
TECH
0
10
20
30
40
GMV Allocation
BASI
INDU
CONG
HLTH
CONS
TELE
UTIL
FINA
TECH
−20
−10
0
10
20
MDP vs. GMV
BASI
INDU
CONG
HLTH
CONS
TELE
UTIL
FINA
TECH
−20
−10
0
10
20
MTD vs. GMV
BASI
INDU
CONG
HLTH
CONS
TELE
UTIL
FINA
TECH
−20
−10
0
10
20
ERC vs. GMV
Figure 11.1
Comparison of sector allocations.

204
PORTFOLIO OPTIMIZATION APPROACHES
Percentages
BASI
CONG
CONS
FINA
HLTH
INDU
TECH
TELE
UTIL
0
10
20
30
GMV
MDP
BASI
CONG
CONS
FINA
HLTH
INDU
TECH
TELE
UTIL
MTD
0
10
20
30
ERC
Figure 11.2
Marginal risk contributions by sector per portfolio.
GMV portfolio are shown, and relative to this set of weights the MDP, MTD and
ERC portfolio solutions are depicted in the remaining three panels.
In Figures 11.2 and 11.3 the marginal risk contributions for each of the portfolios
are shown as dot plots, ordered ﬁrst by sector and per portfolio optimization and then
by portfolio and per sector. For this purpose the sectors and the kind of portfolio have
to be coerced to factors; this kind of lattice plot can then be produced by the formula
arguments in the calls to the dotplot() function which is part of the package lattice
(see Sarkar 2008).
As is evident from Figure 11.1, the GMV solution is characterized by a dominant
allocation in the utilities sector (roughly 40%), followed by telecommunications,
health care and consumer goods. The technology, ﬁnancial and industrial sectors are
basically excluded from the portfolio. More than 80% of the wealth is allocated to
only four out of nine sectors. This portfolio concentration is quite typical of GMV
approaches and is governed by the low volatility of the dominating sectors.
The relative allocation of the most diversiﬁed portfolio is shown in the top right
panel. The biggest difference is a distinct under-weighting of the health care sector.
The remaining sector weights do not differ materially from the GMV solution, hence
the highest weight is still attributed to the utilities sector.
Quite a different picture emerges for the MTD and ERC allocations. In both cases,
the utilities sector has an under-weighting and the technology, consumer services and
industrial sectors an over-weighting. In addition, the ﬁnancial sector has an over-
weighting in the ERC portfolio. All in all, these two portfolio approaches result in a
more balanced wealth allocation.

DIVERSIFICATION RECONSIDERED
205
Percentages
GMV
MDP
MTD
ERC
0
10
20
30
BASI
CONG
0
10
20
30
CONS
GMV
MDP
MTD
ERC
FINA
HLTH
INDU
GMV
MDP
MTD
ERC
TECH
0
10
20
30
TELE
UTIL
Figure 11.3
Marginal risk contributions by portfolio per sector.
These characteristics are also mirrored by the marginal contributions to risk in
the Figures 11.2 and 11.3. Of course, for the ERC portfolio these are all equal. In
absolute terms the greatest share of risk contribution is taken by the utilities sector in
the GMV and MDP cases, whereas the risk contributions of the MTP portfolio are
more evenly dispersed.
Summary statistics for the four portfolio solutions are computed in Listing 11.2.
The package PerformanceAnalytics is used to compute the standard deviations and
Listing 11.2 Key measures of portfolio solutions for Swiss equity sectors.
library(PerformanceAnalytics)
1
## Portfolio risk measures and characteristics
2
Rdec <−R / 100
3
Pret <−apply(W, 2, function(x) Rdec %∗% x / 100)
4
SD <−apply(Pret, 2, sd) ∗100
5
ES95 <−apply(Pret, 2, function(x)
6
abs(ES(R = x, method = "modiﬁed") ∗100))
7
DR <−apply(W, 2, dr, Sigma = V)
8
CR <−apply(W, 2, cr, Sigma = V)
9
## Summarising results
10
Res <−rbind(SD, ES95, DR, CR)
11

206
PORTFOLIO OPTIMIZATION APPROACHES
Table 11.1
Key measures of portfolio solutions for Swiss equity sectors.
Measures
GMV
MDP
MTD
ERC
Standard deviation
0.813
0.841
0.903
0.949
ES (modiﬁed, 95%)
2.239
2.189
2.313
2.411
Diversiﬁcation ratio
1.573
1.593
1.549
1.491
Concentration ratio
0.218
0.194
0.146
0.117
expected shortfalls at the 95% level. The diversiﬁcation and concentration ratios
can be calculated with the functions dr() and cr() discussed in Section 11.5.2,
respectively. All computations are carried out using the apply() function with the
matrix object W, in which the weight vectors are contained in its columns. The results
are provided in Table 11.1.
Judged by these selected key measures, the GMV solution does not deliver an op-
timal weight vector in that it yields the lowest portfolio standard deviation, is riskier
in terms of its expected shortfall and has the highest concentration ratio. At the other
extreme, the ERC allocation implies the most risky allocation in terms of portfolio
standard deviation and expected shortfall. Furthermore, it exhibits the smallest port-
folio concentration, but its diversiﬁcation ratio fares the worst. Intermediate positions
are taken by the MDP and MTD solutions, whereby the former is more akin to the
GMV allocation and the latter resembles more the wealth allotment according to the
ERC approach.
11.6.2
Optimal tail-dependent portfolio against benchmark
It is shown in this subsection how the lower tail dependence coefﬁcient between
a broad market aggregate and its constituents can be utilized such that the stocks
selected for the long-only portfolio are least concordant only with negative market
returns. This approach contrasts with a low-β strategy in the sense that now the aim
is to shelter investors from concurrently occurring index losses and the stocks held
in the portfolio. The intention of a low-β strategy is basically the same, by selecting
stocks that co-move less proportionally than the market in absolute terms. However,
this kind of selection might neglect stocks that are characterized by a high value
of the upper tail dependence coefﬁcient with the market and hence such a portfolio
allocation would miss out on the upside. Incidentally, it should be stressed that neither
strategy takes the riskiness of the portfolio components explicitly into account.
In Listings 11.3 and 11.4 the two strategies are applied to the constituents of the
S&P 500 index. First, the packages FRAPO and nacopula necessary for conducting
the comparison are loaded into the work space. In contrast to the application in the
previous subsection, the lower tail dependence will be derived from a Clayton copula;
this task can be swiftly accomplished using nacopula, as will be shown in due course.
Next the data set INDTRACK6 is loaded from FRAPO. This object is a data frame
with 291 weekly observations of 457 constituents of the S&P 500 index. The sample

DIVERSIFICATION RECONSIDERED
207
Listing 11.3 S&P 500: Tail dependence versus low-β portfolio.
library(FRAPO)
1
library(nacopula)
2
## S&P 500
3
data(INDTRACK6)
4
## Market and asset returns
5
RM <−returnseries(INDTRACK6[1:260, 1], method = "discrete",
6
trim = TRUE)
7
RA <−returnseries(INDTRACK6[1:260, −1], method = "discrete",
8
trim = TRUE)
9
Beta <−apply(RA, 2, function(x) cov(x, RM) / var(RM))
10
Tau <−apply(RA, 2, function(x) cor(x, RM, method = "kendall"))
11
## Clayton copula: lower tail dependence
12
ThetaC <−copClayton@tauInv(Tau)
13
LambdaL <−copClayton@lambdaL(ThetaC)
14
## Selecting stocks below median; inverse log−weighted and scaled
15
IdxBeta <−Beta < median(Beta)
16
WBeta <−−1 ∗log(abs(Beta[IdxBeta]))
17
WBeta <−WBeta / sum(WBeta) ∗100
18
## TD
19
IdxTD <−LambdaL < median(LambdaL)
20
WTD <−−1 ∗log(LambdaL[IdxTD])
21
WTD <−WTD / sum(WTD) ∗100
22
Intersection <−sum(names(WTD) %in% names(WBeta)) /
23
length(WBeta) ∗100
24
## Out−of−sample performance
25
RMo <−returnseries(INDTRACK6[260:290, 1], method = "discrete",
26
percentage = FALSE) + 1
27
RAo <−returnseries(INDTRACK6[260:290, −1], method = "discrete",
28
percentage = FALSE) + 1
29
## Benchmark
30
RMo[1] <−100
31
RMEquity <−cumprod(RMo)
32
## Low beta
33
LBEquity <−RAo[, IdxBeta]
34
LBEquity[1, ] <−WBeta
35
LBEquity <−rowSums(apply(LBEquity, 2, cumprod))
36
## TD
37
TDEquity <−RAo[, IdxTD]
38
TDEquity[1, ] <−WTD
39
TDEquity <−rowSums(apply(TDEquity, 2, cumprod))
40
## Collecting results
41
y <−cbind(RMEquity, LBEquity, TDEquity)
42

208
PORTFOLIO OPTIMIZATION APPROACHES
Listing 11.4 Plotting of wealth trajectory and relative performance.
## Time series plots of equity curves
1
plot(RMEquity, type = "l", ylim = range(y), ylab = "Equity Index",
2
xlab = "Out−of−Sample Periods")
3
lines(LBEquity, lty = 2)
4
lines(TDEquity, lty = 3)
5
legend("topleft",
6
legend = c("S&P 500", "Low Beta", "Lower Tail Dep."),
7
lty = 1:3)
8
## Bar plot of relative performance
9
RelOut <−rbind((LBEquity / RMEquity −1) ∗100,
10
(TDEquity / RMEquity −1) ∗100)
11
RelOut <−RelOut[, −1]
12
barplot(RelOut, beside = TRUE, ylim = c(−5, 17),
13
names.arg = 1:ncol(RelOut),
14
legend.text = c("Low Beta", "Lower Tail Dep."),
15
args.legend = list(x = "topleft"))
16
abline(h = 0)
17
box()
18
starts in March 1991 and ends in September 1997. The data set was ﬁrst used in
Canakgoz and Beasley (2008) (see also Beasley 1990). Stocks with missing values
during the sample period were discarded. Originally, the data set was downloaded
from DATASTREAM and made anonymous. The ﬁrst column refers to the index
data itself.
The index and stock returns are computed in code lines 6–9. Only the ﬁrst 260 data
points are used, the rest being saved for a pseudo ex ante evaluation of the portfolios’
wealth trajectories. The β values of the stocks are assigned to the object Beta in
line 10 by using the apply() function on the column dimension of RA and the slope
coefﬁcient estimator. The lower tail dependence coefﬁcients derived from the Clayton
copula are calculated in lines 11–14. These measures are derived from Kendall’s tau
rank correlations by ﬁrst returning estimates of the copula parameter θ from which the
lower tail dependence coefﬁcients can be deduced. In the next block of R statements,
the stocks with | ˆβ| and tail dependence coefﬁcients below their respective median
value are determined and the weights are calculated by an inverse logarithmic scale.
By using this scheme, roughly 80% of the stocks are common to both selections. The
allocation is employed only for illustration purposes, but any other kind of weight
assignment can be chosen for the two sets of stocks, for example, an allocation in
accordance with a global minimum variance approach. The trajectory of the out-of-
sample benchmark equity and for the two strategies are computed in lines 25–42. In
Listing 11.4 these trajectories are depicted in the form of a time series plot (Figure
11.4) and the performances relative to benchmark as a barplot (Figure 11.5).

DIVERSIFICATION RECONSIDERED
209
0
5
10
15
20
25
30
100
105
110
115
Out−of−Sample Periods
Equity Index
S&P 500
Low Beta
Lower Tail Dep.
Figure 11.4
Trajectory of out-of-sample portfolio wealth.
Clearly, both strategies outperformed the benchmark, albeit that the low-β strategy
did worse for roughly the ﬁrst half of the out-of-sample period. The lower tail
dependence strategy did better than the benchmark consistently throughout the ex
ante period except for eight data points and maintained a slight edge at sample’s
end compared to the low-β strategy. The low-β portfolio performed worse than the
benchmark in 11 out of 30 instances; only for the ﬁrst out-of-sample observation did
it deliver a portfolio value greater than the tail-dependent strategy, and even then only
marginally so.
1
3
5
7
9
11
14
17
20
23
26
29
Low Beta
Lower Tail Dep.
−5
0
5
10
15
Figure 11.5
Relative performance of strategies versus S&P 500.

210
PORTFOLIO OPTIMIZATION APPROACHES
Listing 11.5 Key measures of portfolio solutions for S&P 500.
library(PerformanceAnalytics)
1
## Key measures (ex post)
2
RAdec <−RA / 100
3
RALB <−RAdec[, names(WBeta)]
4
RATD <−RAdec[, names(WTD)]
5
LbStd <−StdDev(rowSums(RALB ∗WBeta / 100)) ∗100
6
TdStd <−StdDev(rowSums(RATD ∗WTD / 100)) ∗100
7
LbES95 <−abs(ES(R = rowSums(RALB ∗WBeta / 100),
8
method = "gaussian")) ∗100
9
TdES95 <−abs(ES(R = rowSums(RATD ∗WTD / 100),
10
method = "gaussian")) ∗100
11
LbDr <−dr(WBeta, Sigma = cov(RALB))
12
TdDr <−dr(WTD, Sigma = cov(RATD))
13
LbCr <−cr(WBeta, Sigma = cov(RALB))
14
TdCr <−cr(WTD, Sigma = cov(RATD))
15
## Key measure (ex ante)
16
LbRetO <−returnseries(LBEquity, percent = FALSE, trim = TRUE)
17
LbRetO <−timeSeries(LbRetO, charvec = 1:30)
18
TdRetO <−returnseries(TDEquity, percent = FALSE, trim = TRUE)
19
TdRetO <−timeSeries(TdRetO, charvec = 1:30)
20
BmRetO <−timeSeries(RMo[−1] −1, charvec = 1:30)
21
km <−function(pf, bm, scale){
22
ra <−Return.annualized(pf, scale = scale) ∗100
23
ir <−InformationRatio(pf, bm, scale = scale)
24
upr <−UpDownRatios(pf, bm, method = "Capture", side = "Up")
25
dnr <−UpDownRatios(pf, bm, method = "Capture", side = "Down")
26
res <−c(ra, ir, upr, dnr)
27
names(res) <−c("Return", "IR", "UpRatio", "DownRatio")
28
return(res)
29
}
30
LbKM <−km(LbRetO, BmRetO, scale = 52)
31
TdKM <−km(TdRetO, BmRetO, scale = 52)
32
Akin to the key measures in the previous example for quantitatively assessment
of portfolio allocations, a similar set of ﬁgures is computed in Listing 11.5 for the
low-β and minimum tail dependence portfolios. All key measures are computed
by the package PerformanceAnalytics. Because Listing 11.3 includes an out-of-
sample part, the key ﬁgures are split into a block of ‘in-sample’ and ‘out-of-sample’
statistics. Within the former category the same statistics are computed as in Listing
11.2 and need no further elaboration. The latter are related to the benchmark returns,
except for the annualized returns of the two portfolio approaches. The out-of-sample

DIVERSIFICATION RECONSIDERED
211
Table 11.2
Key measures of portfolio solutions for S&P 500.
Measures
Low-β
Lower Tail Dependence
In-sample
Standard deviation
1.948
2.050
ES (95%)
3.813
3.954
Diversiﬁcation ratio
2.350
2.576
Concentration ratio
0.008
0.007
Out-of-sample
Return (annualized)
23.712
24.086
Information ratio
2.293
2.742
Upside capture ratio
0.661
0.771
Downside capture ratio
0.115
0.218
performance is evaluated by means of the information ratio and the upside and
downside capture ratios (Bacon 2004, page 47). A high value is desirable for the
upside capture ratio and a low value for the downside capture ratio. The computation
of these statistics is handled by the function km() and the outcome is assigned
to the objects LbKM and TdKM for the low-β and lower tail-dependent portfolios,
respectively. The results are provided in Table 11.2.
With respect to the in-sample statistics the low-β strategy is slightly less risky
than the lower tail-dependent allocation. However, the latter possesses a greater
diversiﬁcation ratio and is less concentrated. The ex ante information ratio is greater
for the tail dependence strategy and also fares better in terms of the upside capture
ratio, whereas the low-β strategy has an edge over the former allocation with respect
to the downside capture ratio.
11.6.3
Limiting contributions to expected shortfall
The ﬁnal example of this chapter shows how a portfolio allocation can be determined
with respect to limits on the downside risk contributions. These limitations can be
deﬁned in two ways: ﬁrst, as a budget constraint such that the ES contribution is below
an upper bound; and second, such that the maximum contribution to the portfolio’s
downside risk of an asset is minimized. These two approaches are contrasted with the
solutions of a global minimum-variance and an equal-risk contributed allocation. The
latter two portfolio optimizations are directed to the portfolio’s volatility, whereas the
former two explicitly relate to the portfolio’s downside risk. In all cases, it is assumed
that only long positions in an asset are allowed and that the wealth is fully invested
in the risky assets.
The R code by which this comparison is carried out is shown in Listing 11.6.
The required packages FRAPO and PortfolioAnalytics are brought into memory
ﬁrst. Next the data set MultiAsset is loaded. This object is a data frame with 85
month’s-end observations of stock and bond indexes and gold (represented by the

212
PORTFOLIO OPTIMIZATION APPROACHES
Listing 11.6 Comparison of restricted ES portfolios with GMV and ERC.
library(FRAPO)
1
library(PortfolioAnalytics)
2
## Loading data and computing returns
3
data(MultiAsset)
4
R <−returnseries(MultiAsset, percentage = FALSE, trim = TRUE)
5
N <−ncol(R)
6
## Deﬁning constraints and objective for CVaR budget
7
C1 <−constraint(assets = colnames(R), min = rep(0, N),
8
max = rep(1, N), min_sum = 1, max_sum = 1)
9
ObjCVaR <−add.objective(constraints = C1, type = "risk",
10
name = "ES", arguments = list(p = 0.95),
11
enabled = TRUE)
12
ObjCVaRBudget <−add.objective(constraints = ObjCVaR,
13
type = "risk_budget",
14
name = "ES", max_prisk = 0.2,
15
arguments = list(p = 0.95),
16
enabled = TRUE)
17
SolCVaRBudget <−optimize.portfolio(R = R,
18
constraints = ObjCVaRBudget,
19
optimize_method = "DEoptim",
20
itermax = 50,
21
search_size = 20000,
22
trace = TRUE)
23
WCVaRBudget <−SolCVaRBudget$weights
24
CVaRBudget <−ES(R, weights = WCVaRBudget, p = 0.95,
25
portfolio_method = "component")
26
## Minimum CVaR concentration portfolio
27
ObjCVaRMinCon <−add.objective(constraints = ObjCVaR,
28
type = "risk_budget",
29
name = "ES",
30
min_concentration= TRUE,
31
arguments = list(p = 0.95),
32
enabled = TRUE)
33
SolCVaRMinCon <−optimize.portfolio(R = R,
34
constraints = ObjCVaRMinCon,
35
optimize_method = "DEoptim",
36
itermax = 50,
37
search_size = 20000,
38
trace = TRUE)
39
WCVaRMinCon <−SolCVaRMinCon$weights
40
CVaRMinCon <−ES(R, weights = WCVaRMinCon, p = 0.95,
41
portfolio_method = "component")
42

DIVERSIFICATION RECONSIDERED
213
SPDR Gold Shares exchange traded fund). In total, 10 asset classes are included,
represented by their indexes. The sample starts on 30 November 2004 and ends on
30 November 2011. The data set was obtained from Yahoo! Finance, and unadjusted
closing prices have been retrieved. If a month’s-end value was not reported, the value
of the previous day has been used. More information about the data set can be found
in the package manual. In lines 5 and 6 the discrete decimal returns for the assets are
computed as object R and the number of assets is assigned to the object N.
In the next block of statements a portfolio structure is deﬁned by which the
marginal contribution to the ES is bounded by an upper limit of 20%. The portfolio
structure consists of three elements. First, the object C1 contains the non-negativity
and budget restrictions. Next, the ﬁrst objective is added to this constraint, namely
to ﬁnd an asset allocation which is characterized by a minimal CVaR at the 95%
conﬁdence level. These two elements are combined into the object ObjCVaR and
could be used as such to ﬁnd a weight vector that yields a minimum downside risk
at the speciﬁed conﬁdence level. The third element deﬁnes the budget constraint
with respect to the assets’ downside risk contribution. Here, it is set to be at most
20%. Incidentally, by utilizing an upper bound equal to 1/N, one would obtain the
same solution as in the case of an ERC portfolio. The reason for this is that there is
a linear relationship between the marginal contributions to CVaR and the marginal
contributions with respect to the variance–covariance matrix of returns. This portfolio
setting is then used in the call to optimize.portfolio(). The optimizer has been
selected as DEoptim, and as optimizing parameters itermax has been set to 50
and search_size to 20 000. It is recommended that the ratio between the search
size and the maximum number of iterations should exceed the number of parameters
by a factor of 10. The weight vector is extracted from the returned list object
SolCVaRBudget and assigned to WCVaRBudget. In lines 25–26 the weight obtained
is inspected with respect to the portfolio’s ES and to see whether the solution found
complies with the downside risk budget. This information can be returned by the
function ES() contained in the package PerformanceAnalytics, which is stated as
a dependency of PortfolioAnalytics.
In the following block of R statements, a minimum concentrated portfolio with
respect to the downside risk contributions of its constituents is deﬁned. The object
ObjCVaR is reutilized and the minimization of the maximum downside contribution
is achieved by setting the argument min_concentration to TRUE for the port-
folio type risk_budget. The object ObjCVaRMinCon holds the speciﬁcation for
an MCC portfolio. The solution to this portfolio setting can—similar to the above
optimization—be determined by calling optimize.portfolio(). The allocation
is assigned to WCVaRMinCon and the downside risk, as well as the assets’ contribu-
tions to it, can be queried by employing the ES() function. In the ﬁnal section of
Listing 11.5 the solutions pertinent to an equal-risk contributed strategy and a global
minimum-variance strategy are determined.
The four asset allocations and associated marginal CVaR contributions are sum-
marized in Table 11.3. The expected concentration of the GMV solution with respect
to the least volatile asset, namely German bonds, is striking. Almost 90% of wealth
is allocated to this asset, with the rest shared in roughly equal chunks between US

214
PORTFOLIO OPTIMIZATION APPROACHES
Table 11.3
Weight and downside risk contributions of multi-asset portfolios.
Weights
Risk contributions
Assets
GMV
ERC
BCC
MCC
GMV
ERC
BCC
MCC
S&P 500
4.55
3.72
5.84
2.02
9.83
16.63
12.73
6.53
Russell 3000
0.00
3.59
2.42
1.01
0.00
16.80
5.57
3.30
DAX
4.69
3.47
10.74
1.01
12.19
14.34
18.98
3.20
FTSE 100
0.00
4.12
15.85
4.04
0.00
11.20
19.94
10.10
Nikkei 225
1.35
3.38
2.90
1.01
3.16
22.36
8.99
4.12
MSCI EM
0.00
2.14
5.72
1.01
0.00
14.22
18.65
5.36
US Treasury
0.00
16.42
15.45
18.18
0.00
5.40
2.31
18.88
German REX
88.72
42.44
18.11
66.67
74.75
−17.60
−3.42
39.61
UK Gilts
0.40
15.93
13.95
1.01
0.50
5.00
0.49
1.18
Gold
0.29
4.78
9.03
4.04
−0.43
11.63
15.78
7.70
and German stocks. The remaining assets either do not feature in the GMV solution
or do so only negligibly. The ERC solution is more balanced in terms of its weight
distribution, but still almost two thirds would have been invested in German or UK
sovereign bonds. The MCC portfolio takes a middle stance between the GMV and
ERC solutions, and the BCC is the most akin to an equal-weighted solution. This
picture is qualitatively mirrored by the percentage contributions to downside risk of
each of the portfolios. The negative contribution of German bonds in the ERC and
BCC allocations is worth highlighting. This is primarily due to the comparatively
high allotment to stocks and hence the bond allocation serves as a risk diversiﬁer
for tail losses. In the case of the BCC optimization this was forced by limiting the
maximum contribution to 20% which resulted in a smaller weight for German bonds.
This can be seen by comparing the BCC with the MCC allocations. The minimum
CVaR allocation delivers an allocation where the risk contributions are the least con-
centrated, but the German REX still contributes almost 40% to risk, which is twice
as high as allowed for the BCC setting.
Summary statistics on the portfolio levels for the four approaches are provided
in Table 11.4. With respect to the portfolios’ return volatility, the GMV allocation
Table 11.4
Key measures of portfolio solutions for multi-asset portfolios.
Measures
GMV
ERC
BCC
MCC
Standard Deviation
0.815
1.151
2.232
0.920
ES (95 %)
1.437
2.923
5.996
1.411
Diversiﬁcation Ratio
1.853
2.035
1.582
1.902
Concentration Ratio
0.417
0.110
0.130
0.216

DIVERSIFICATION RECONSIDERED
215
yields the lowest outcome and the BCC solution implies the highest return dispersion,
measured by the portfolio standard deviation. The latter is due to the binding CVaR
budget constraint which forces a lower weight on the least volatile asset and hence
the remaining wealth has to be allocated to riskier constituents. This fact is mirrored
by the highest value of the portfolios’ downside risk. Here, the MCC allocation yields
the least risky wealth distribution, followed by the GMV solution. The ERC outcome
is situated between the MCC and BCC portfolios with regard to the volatility and
downside risk characteristics. The diversiﬁcation ratio is highest for the ERC portfolio
and lowest for the BCC. As regards the concentration ratio, however, the BCC solution
fares almost as well as the ERC allocation and the allotment according to the MCC
approach is slightly worse.
References
Ardia D., Boudt K., Carl P., Mullen K. and Peterson B. 2010 Differential Evolution (DEoptim)
for non-convex portfolio optimization.
Bacon C. 2004 Practical Portfolio Performance Measurement and Attribution. John Wiley &
Sons, Ltd, Chichester.
Beasley J. 1990 OR-library: Distributing test problems by electronic mail. Journal of the
Operational Research Society 41(11), 1069–1072.
Boudt K., Carl P. and Peterson B. 2010 Portfolio optimization with CVaR budgets. Presentation
at R/Finance Conference, Chicago.
Boudt K., Carl P. and Peterson B. 2011a Asset allocation with conditional value-at-risk budgets.
Technical report, http://ssrn.com/abstract=1885293.
Boudt K., Carl P. and Peterson B. 2011b PortfolioAnalytics: Portfolio Analysis, including
Numeric Methods for Optimization of Portfolios. R package version 0.6.1/r1849.
Boudt K., Peterson B. and Croux C. 2007 Estimation and decomposition of downside risk
for portfolios with non-normal returns. Working Paper KBI 0730, Katholieke Universteit
Leuven, Faculty of Economics and Applied Economics, Department of Decision Sciences
and Information Management (KBI), Leuven.
Boudt K., Peterson B. and Croux C. 2008 Estimation and decomposition of downside risk for
portfolios with non-normal returns. Journal of Risk 11(2), 79–103.
Canakgoz N. and Beasley J. 2008 Mixed-integer programming approaches for index tracking
and enhanced indexation. European Journal of Operational Research 196, 384–399.
Choueifaty Y. and Coignard Y. 2008 Toward maximum diversiﬁcation. Journal of Portfolio
Management 34(4), 40–51.
Choueifaty Y., Froidure T. and Reynier J. 2011 Properties of the most diversiﬁed portfolio.
Working paper, TOBAM.
Coles S., Heffernan J. and Tawn J. 1999 Dependence measures for extreme value analysis.
Extremes 2(4), 339–365.
Dobri´c J. and Schmid F. 2005 Nonparametric estimation of the lower tail dependence λl in
bivariate copulas. Journal of Applied Statistics 32(4), 387–407.
Eddelbuettel D. 2012 RcppDE: Global optimization by differential evolution in C++. R pack-
age version 0.1.1.

216
PORTFOLIO OPTIMIZATION APPROACHES
Frahm G., Junker M. and Schmidt R. 2005 Estimating the tail dependence coefﬁcient:
Properties and pitfalls. Insurance: Mathematics and Economics 37(1), 80–100.
Heffernan J. 2000 A directory of coefﬁcients of tail dependence. Extremes 3(3), 279–290.
Hirschman A. 1964 The paternity of an index. American Economic Review 54(5), 761.
Maillard S., Roncalli T. and Teiletche J. 2009 On the properties of equally-weighted risk
contributions portfolios. Working paper, SGAM Alternative Investments, Lombard Odier
and University of Paris Dauphine.
Maillard S., Roncalli T. and Teiletche J. 2010 The properties of equally weighted risk contri-
bution portfolios. Journal of Portfolio Management 36(4), 60–70.
Markowitz H. 1959 Portfolio Selection Efﬁcient Diversiﬁcation of Investments. John Wiley &
Sons, New York.
Mitchell M. 1998 An Introduction to Genetic Algorithms. MIT Press, Cambridge, MA.
Mullen K., Ardia D., Gil D., Windover D. and Cline J. 2011 DEoptim: An R package for global
optimization by differential evolution. Journal of Statistical Software 40(6), 1–26.
Peterson B. and Boudt K. 2008 Component VAR for a non-normal world. Risk.
Price K., Storn R. and Lampinen J. 2006 Differential Evolution – A Practical Approach to
Global Optimization. Springer-Verlag, Berlin.
Qian E. 2005 Risk parity portfolios: Efﬁcient portfolios through true diversiﬁcation. White
paper, PanAgora, Boston, MA.
Qian E. 2006 On the ﬁnancial interpretation of risk contribution: Risk budgets do add up.
Journal of Investment Management 4(4), 1–11.
Qian E. 2011 Risk parity and diversiﬁcation. Journal of Investing 20(1), 119–127.
Sarkar D. 2008 Lattice: Multivariate Data Visualization with R. Springer, New York.
Scaillet O. 2002 Nonparametric estimation and sensitivity analysis of expected shortfall.
Mathematical Finance 14(1), 74–86.
Schmidt R. and Stadtm¨uller U. 2006 Nonparametric estimation of tail dependence. Scandina-
vian Journal of Statistics 33, 307–335.
Storn R. and Ulrich J. 1997 Differential evolution – a simple and efﬁcient heuristic for global
optimization over continuous spaces. Journal of Global Optimization 11(4), 341–359.
Zhu S., Li D. and Sun X. 2010 Portfolio selection with marginal risk control. Journal of
Computational Finance 14(1), 3–28.

12
Risk-optimal portfolios
12.1
Overview
In this chapter portfolio optimization methods are presented in which some sort of
risk measure and its level are directly factored into the weightings of the ﬁnancial
instruments. These approaches can be distinguished from Markowitz portfolios in
the sense that now a certain VaR or ES level is not the result of an efﬁcient portfolio
allocation, but an objective. Hence the user can ask questions such as: ‘How should the
portfolioweightsbechosensuchthattheresulting99%VaRis3.5%?’.Athoroughand
concise comparison between mean–VaR and mean–variance portfolios is provided
in Alexander and Baptista (1999, 2002, 2004, 2008) as well as in De Giorgi (2002).
This approach has recently been investigated by Durand et al. (2011). In the light
of the Basel Accord, these kinds of portfolio optimization attracted interest from
practitioners, because the capital adequacy requirements can be derived from these
kinds of risk measures (see Alexander and Baptista 2002, for instance). Portfolio
optimizations involving the expected shortfall have been analysed by Uryasev and
Rockafellar (1999) and Rockafellar and Uryasev (2000, 2001).
In a similar vein, portfolios can be optimized with respect to their draw-down.
This type of optimization is applicable when a portfolio is benchmarked against a
broader market index.
In the next two sections, the mean–VaR and mean-ES portfolio approaches are
presented and contrasted with the classical mean–variance portfolios. In Section 12.4
portfolios for which some sort of draw-down measure is minimized or targeted are
presented. The Section 12.5 presents the relevant R packages, and the applications in
Section 12.6 conclude the chapter.
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

218
PORTFOLIO OPTIMIZATION APPROACHES
12.2
Mean–VaR portfolios
In this section the mean–VaR portfolio approach is presented and contrasted to
Markowitz portfolios as presented in Section 5.2. This comparison draws on the
work of Alexander and Baptista (2002) and De Giorgi (2002). The VaR risk measure
was deﬁned in equation 4.4, repeated here for the reader’s convenience:
VaRα = inf {l ∈R : P(L > l) ≤1 −α} = inf {l ∈R : FL(l) ≥α} ,
(12.1)
where FL is the distribution function of the losses, which is now assumed to be
elliptical, that is, it is now assumed that the distribution of the returns is either
multivariate normal or multivariate Student’s t. In the synopsis of the risk measures
the mean–VaR risk measure has been deﬁned as the difference between the expected
return and VaR. This double nomenclature is perhaps unfortunate, because we are
using ‘mean–VaR’ in two ways: ﬁrst for the risk measure, and second in naming the
axis in the efﬁcient frontier plane. However, it should be evident from the context
when mean–VaR refers to the risk measure and when it is used in the context of
optimal mean–VaR portfolios along the mean–VaR efﬁcient frontier.
Akin to the classical Markowitz portfolio, mean–VaR portfolio optimization can
approached from two perspectives: either determining the weights for a speciﬁed VaR
at a given conﬁdence level which maximizes the portfolio return, or minimizing the
mean VaR for a given conﬁdence level for a ﬁxed portfolio return. In accordance with
the exposition in Section 5.2, the latter route is chosen in the following. In a ﬁrst step,
it assumed that there are N risky assets and the wealth is completely allocated among
them. Let zα = −1(α) denote the quantile of the loss distribution at the conﬁdence
level α ∈(1/2, 1). Then the mean VaR is deﬁned as
VaRm
α = zασW −¯r,
(12.2)
where ¯r is the portfolio return ω′μ. A portfolio ¯ω ∈ then belongs to the mean–VaR
boundary at conﬁdence level α if it solves the following problem:
PVaRm = arg min
ω
VaRm
α = zασW −¯r,
ω′μ = ¯r,
ω′i = 1
(12.3)
where i is an (N × 1) vector of 1s. Hence, a quadratic objective problem with linear
constraints results, due to σW = ω′ω. This optimization problem is quite similar
to mean–variance portfolios as described in Section 5.2, and indeed it is shown in
Alexander and Baptista (2002) that mean–VaR portfolios are a subset of the mean–
variance boundary. Hence, the condition for a mean–variance efﬁcient portfolio (see
Merton 1972),
σ 2
w
1/C −(¯r −A/C)2
D/C2
= 1
(12.4)

RISK-OPTIMAL PORTFOLIOS
219
where the scalars A, B, C and D are deﬁned as
A = i′−1μ,
(12.5)
B = μ′−1μ,
(12.6)
C = i′−1i,
(12.7)
D = BC −A2,
(12.8)
can be written in terms of the mean–VaR frontier as
[(VaRm + ¯r)/zα]2
1/C
−(¯r −A/C)2
D/C2
= 1
(12.9)
in which equation (12.2) has been solved for σW.
The shapes of the efﬁcient frontiers are analysed in Alexander and Baptista (2002)
at the limits of the conﬁdence level α, that is, α →1/2 and α →1. In the former
case, the efﬁcient frontier is a straight downward sloping line through the origin
with slope coefﬁcient equal to −1 in the (expected return, mean–VaR) space. This is
due to the fact that zα approaches zero and hence, according to equation (12.2), the
mean VaR equals minus the expected portfolio return. In the latter case, the efﬁcient
frontier converges to the traditional mean-standard deviation hyperbola. In this case,
zα →∞and hence VaRm/zα = σW, because ¯r/zα →0. The general formula for the
mean–VaR boundary in the (expected return, mean–VaR) plane is given by
VaRm = −¯r + zα

1
D

c¯r2 −2a¯r + b

.
(12.10)
The band of efﬁcient frontiers between these two extreme mean–VaR levels, which
is a straight line for α = 0.5, becomes increasingly hyperbolic as the conﬁdence
level increases. This is illustrated in Figure 12.1. The mean–VaR boundaries in the
−5
0
5
10
−1
0
1
2
3
4
5
VaRα
m
Expected Portfolio Return
α = 0.5
α = 0.7 α = 0.8
α = 0.9 α = 0.95
α = 0.99
Figure 12.1
Boundaries of mean–VaR portfolios in the mean–VaR plane.

220
PORTFOLIO OPTIMIZATION APPROACHES
0
1
2
3
4
5
1
2
3
4
5
VaRα
m zα
Expected Portfolio Return
α = 0.9 α = 0.95 α = 0.99
Figure 12.2
Boundaries of mean–VaR portfolios in the mean-standard deviation
plane.
(expected portfolio return, standard deviation) space are shown in Figure 12.2 for the
conﬁdence levels for 90%, 95% and 99%, respectively.
As can be discerned from Figures 12.1 and 12.2, not for all conﬁdence levels of
mean VaR does a proper efﬁcient frontier for mean–VaR portfolios result, but only for
values of α that are sufﬁciently large. The reason for this can be gleaned from equation
(12.2). The right-hand side consists of a standard deviation term zασW and the mean
effect term ¯r. If the former does not outweigh the latter, then the minimization of the
mean VaR has no solution. If the condition α > (√D/C) is met, than a valid mean–
VaR optimization problem results and the weights of the minimum-VaR portfolio at
the α conﬁdence level can be determined in closed form as
ωVaRmα = g + h

A
C +

D
C

z2α
C(zα)2 −D −1
C
	
,
(12.11)
where the (N × 1) vectors g and h are deﬁned as
g = 1
D

B−1i −A−1μ

,
(12.12)
h = 1
D

C−1μ −A−1i

.
(12.13)
The value of VaRm
α corresponding to the weight vector of the minimum-VaR portfolio
for a given conﬁdence level is then
VaRm
α = zα

z2α
C(zα)2 −D −

A
C +

D
C

z2α
C(zα)2 −D −1
C
	
. (12.14)
Because a minimum VaR portfolio is on the efﬁcient frontier of mean–variance
portfolios, if it exists, by reverse conclusion all portfolios on the mean–variance

RISK-OPTIMAL PORTFOLIOS
221
efﬁcient frontier except for the minimum variance portfolio are also minimum VaR
portfolios for a certain conﬁdence level. The values of these conﬁdence levels αMV
are equal to
αMV = 
⎛
⎝

D
C +
D2/C3
(¯r −A/C)2
⎞
⎠.
(12.15)
Furthermore, each mean–variance efﬁcient portfolio except for the minimum-
variance portfolio is mean–VaR inefﬁcient for conﬁdence levels lower than ex-
pressed in equation (12.15) and mean–VaR efﬁcient for greater conﬁdence levels.
In addition, the minimum-variance portfolio is mean–VaR inefﬁcient. Finally, it also
worth highlighting the fact that the minimum-VaR portfolio solution approaches the
minimum-variance portfolio as α →1.
Next, the relation between the mean–variance and mean–VaR efﬁcient frontiers
will be presented when a risk-free security, r f > 0, is included as an investment
opportunity. Therefore, the scope of the assets is extended and the set of admissible
weights is now given as  = {ω ∈RN+1 : N+1
i=1 ωi = 1}, where ωN+1 denotes the
share invested in the risk-free asset. The problem formulation as in equation (12.3)
remains unchanged and hence a portfolio that belongs to the mean–VaR boundary is
also on the efﬁcient frontier for mean–variance portfolios for which an investment
opportunity in a safe asset exists. In Merton (1972) the condition to be met for this
kind of portfolio is stated as
σ 2
W = (¯r −r f )2/H,
(12.16)
where the scalar H is deﬁned as H = Cr2
f −2Ar f + B and the constants A, B and
C are given by equations (12.5)–(12.7). Similar to the reformulation of the condition
for mean–VaR portfolios without a risk-free asset, equation (12.14) can be expressed
in terms of VaRm
α :
[(VaRm
α + ¯r)/zα]2 = (¯r −r f )2/H.
(12.17)
Similar to the enclosing asymptotes of the mean–variance hyperbola as shown in
Section 5.2, the asymptotes for the mean–VaR boundary are given by
VaRm
α =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
¯r(zα −
√
H) −zαr f
√
H
if ¯r ≥r f
¯r(−zα −
√
H) + zαr f
√
H
if ¯r < r f ,
(12.18)
after solving equation (12.17) for VaRm
α . The qualitative difference between these
asymptotes and those for the mean–variance efﬁcient frontier is that the slopes of
former differ in absolute terms.
For portfolios with an investment opportunity in a risk-free asset, efﬁcient mean–
VaR portfolios exist only for sufﬁciently large conﬁdence levels α. The condition now
becomes α > (
√
H) for a mean–VaR efﬁcient portfolio that is an element of the
mean–variance efﬁcient frontier, too. In the case of a risk-free asset the two frontiers
coincide, whereas when a riskless security is excluded from the set of investment

222
PORTFOLIO OPTIMIZATION APPROACHES
opportunities, the mean–VaR boundary is only a subset of the mean–variance efﬁcient
frontier.
Recall from Section 5.2 the derivation of the capital market line for mean–variance
portfolios, in particular equations (5.7) and (5.8). For mean–VaR portfolios with a
riskless asset, the CML can be deduced from the deﬁnition of its VaR:
P{E[R] = rp ≤−VaRm
α } = 1 −α,
P{r f + γ (¯r −r f ) ≤−VaRmp
α } = 1 −α,
(12.19)
where VaRmp
α
now refers to the VaR of the total portfolio and not only the VaR of the
risky assets. The probability statement in the curly braces can be transformed to
P{¯r ≤r f −(VaRmp
α
+ r f )/γ } = 1 −α.
(12.20)
The relation between the portfolio’s VaR and that of the risky assets only is then
given by
VaRmp
α
= −r f + γ (VaRm
α + r f ).
(12.21)
The share to be invested in the risky assets is thus given as a function of the ratio
between the two adjusted VaR measures: γ = (VaRmp
α
+ r f )/(VaRm
α + r f ). Finally,
the formula for the CML can be expressed similarly to equation (5.8):
E[R] = rp = r f +
¯r −r f
VaRmα + r f
(VaRmp
α
+ r f ),
(12.22)
but now the relevant mean–VaR measures adjusted by the risk-free rate are used, rather
than the portfolio dispersion. This mean–VaR CML is applicable for VaRmp
α
≥−r f ,
and hence the minimal mean–VaR portfolio is the trivial solution of being invested
100% in the risk-free asset. The mean–VaR efﬁcient frontier with a risk-free asset
and the lower and upper asymptotes, that is, the CML, are depicted in Figure 12.3.
−1
0
1
2
3
4
5
−1
0
1
2
3
4
5
VaRα
m
Expected Portfolio Return
●
PT
●
rf
●
PMV
Figure 12.3
Tangency mean–VaR portfolio.

RISK-OPTIMAL PORTFOLIOS
223
The point PT represents the tangency portfolio which is deﬁned as the locus where
the slope of the CML is equal to the slope of efﬁcient frontier. Akin to the maximum
Sharpe-ratio portfolio in the mean-standard deviation space, at this point the reward-
to-VaR ratio is maximized. The origin of the two asymptotes is deﬁned by (−r f ,r f ),
that is, the VaR of a 100% investment is equal to the negative of its rate of return.
Finally, the point PMV corresponding to a minimum-variance portfolio is marked
on the efﬁcient frontier, highlighting the fact that minimum-variance portfolios are
in principle not mean–VaR efﬁcient, that is, there are portfolio solutions that are
characterized by a higher return and less risk.
12.3
Optimal CVaR portfolios
In a series of papers by Uryasev and Rockafellar portfolio optimization with respect
to the conditional VaR (CVaR) is advocated (see Rockafellar and Uryasev 2000,
2001; Uryasev and Rockafellar 1999). The market risk measure CVaR is deﬁned
as the expected loss exceeding the VaR for a given conﬁdence level. Synonyms for
CVaR are ‘mean excess loss’, ‘mean shortfall’ and ‘tail VaR’. For continuous loss
distributions, this risk measure is identical to the expected shortfall as introduced in
Section 4.2.
As elucidated in Chapter 4, the VaR is not a coherent risk measure, but both
ES and CVaR are coherent. From an optimization point of view, Rockafellar and
Uryasev (2001) show that CVaR is a convex function, but VaR may be non-convex,
which can yield portfolio solutions pertinent to local and not global optima. However,
an outright optimization with respect to CVaR/ES is complicated from a numerical
point of view due to the dependence of the ES on the VaR (see equation (4.6)). To
cope with this issue, the authors introduced more granular risk measures, and then
deﬁned CVaR as a weighted average. This step enables the derivation of a convex
function with respect to the portfolio weights. In addition, the objective function can
be optimized by utilizing linear programming techniques and is also applicable for
non-normal loss distributions.
The following paragraphs elucidate the approach taken by the authors in opti-
mizing portfolios with respect to CVaR and how portfolios of this kind relate to
mean–variance and mean–VaR portfolios as shown in the previous section. The latter
topic has been investigated by Alexander and Baptista (2004).
The following risk measures are needed in order to deﬁne CVaR (see Uryasev
2005):
r VaRα, the α-quantile of a loss distribution;
r CVaR+, the expected losses strictly exceeding VaR (i.e., mean excess loss or
expected shortfall);
r CVaR−, expected losses that are weakly exceeding VaR (i.e., losses that are
either equal to or exceed VaR) – this risk measure is also known as ‘tail VaR’.
The CVaR is then deﬁned as a weighted average between VaR and CVaR+:
CVaR = λVaR + (1 −λ)CVaR+
(12.23)

224
PORTFOLIO OPTIMIZATION APPROACHES
1
6
1
6
1
6
1
6
1
6
1
6
f1
f2
f3
f4
f5
f6
CVaR
CVaR+
CVaR−
VaR
α = 4
6 →λ =0
Loss
Probability
α =
7
12 →λ = 1
5
Loss
Probability
1
6
1
6
1
6
1
6
1
6
1
6
f1
f2
f3
f4
f5
f6
CVaR
CVaR−
CVaR+
VaR
Figure 12.4
Discrete loss distribution: VaR, CVaR, CVaR+ and CVaR−.
where the weight 0 ≤λ ≤1 is given by λ = (
(VaR) −α)/(1 −α) and 
(VaR)
denotes the probability that losses do not exceed or are equal to VaR for a given
conﬁdence level.
The relations between the risk measures for a discrete loss distribution are shown
in Figure 12.4, adapted from Uryasev (2005). In this ﬁgure six ﬁctional and equal
likely losses, f1, . . . , f6, are portrayed on the real line. The ﬁrst case considered in
the ﬁgure refers to a conﬁdence level α = 4/6 that coincides with the VaR quantile,
hence the weight λ takes a value of zero. The risk measures CVaR and CVaR+ are
then identical and equal to the average of the losses f5 and f6. The VaR is given
by the loss f4 and the risk measure CVaR−is the probability-weighted average of
f4, f5, f6. In this instance the relation between the risk measures is VaR < CVaR−<
CVaR = CVaR+. Now, consider a conﬁdence level α = 7/12, which is depicted in
the lower part of Figure 12.4. The value of CVaR+ is the same as implied in the
panel above. However, the value of λ now becomes λ = 1/5, and hence CVaR =
1
5VaR + 4
5CVaR+, which translates into loss terms as CVaR = 1
5 f4 + 2
5 f5 + 2
5 f6. In
the second case, the relation between the risk measures read as: VaR < CVaR−<
CVaR < CVaR+.
Having deﬁned the targeted risk measure CVaR, one can then derive the objective
function. Though the existence of a (continuous) multivariate distribution function

RISK-OPTIMAL PORTFOLIOS
225
for the portfolio returns, p(r|ω), and hence the losses is not critical, its existence is
assumed for the sake of notational convenience. Generally, the loss function in the
context of portfolios can be expressed as f (ω, r), where the dimension of ω is N and
r is M. The difference in dimensions is due to the possibility of non-random returns,
that is, those that are deemed to be ﬁxed and hence riskless. The loss function then
maps RN × RM →R onto the real line. The probability function for losses less than
a threshold value z can then be expressed as:

(ω, z) =

f (ω,r)≤z
p(r|ω) dr
(12.24)
If one substitutes for z the VaR for a given conﬁdence level α, and introduce a
performance function for CVaR as
1
1−α(ω),
(ω) =

f (ω,r)≥VaR(ω,α)
f (ω, r)p(r|ω) dr
(12.25)
then the minimization of the excess loss function
1
1−α(ω) can be reduced to the
minimization of:
F(ω, z) = (1 −α)z +

f (ω,r)≥z
( f (ω, r) −z)p(r|ω) dr
(12.26)
which resembles the deﬁnition of CVaR introduced above. It is proved in Rockafellar
and Uryasev (2001) that the minimization of equation (12.26) with respect to (ω, z)
yields the same outcome as the minimization of equation (12.25) with respect to ω.
The optimal value obtained for z is equal to the VaR. Hence, if one optimizes equation
(12.26) one directly obtains a portfolio weights solution for VaR and minimal CVaR.
In order to pave the way to showing how portfolio optimization with respect to CVaR
can be expressed in terms of a linear program, the objective function can be expressed
equivalently as
F(ω, z) = (1 −α)z +

r∈Rm( f (ω, r) −z)+ p(r|ω) dr
(12.27)
where f (ω, r) −z)+ = b+ is deﬁned as b+ = max{0, b}. Therefore, expressed in
discrete terms, the portfolio problem that yields a minimal CVaR can be written as
PCVaR = arg min
ω∈,ζ∈R
ζ + ν
J

j=1
y j,
y j ≥f (ω, r j) −ζ,
y j ≥0,
ω′μ = ¯r,
ω′i = 1,
(12.28)
where ν =
1
(1−α)J is a constant factor used in the calculation of the mean excess
losses. The portfolio problem can be amended by further constraints, such box or
group constraints and/or the exclusion of short positions. In the context of a portfolio,

226
PORTFOLIO OPTIMIZATION APPROACHES
the loss function f (·) and hence the inequality constraints are linear with respect to
the weights and thus can be solved by linear programming techniques. The optimal
solution contains the value for VaR, ζ opt, and the weights that produce a portfolio
with a minimum CVaR, ωopt. When an optimal solution for the portfolio weights has
been found, these values can be inserted into the performance function
1
1−α F(ω =
ωopt, z = ζ opt) to compute the CVaR. In practice, the question arises as to which
data should be employed for r j. Two approaches are ordinarily encountered in the
literature: either one utilizes historic returns or one conducts a Monte Carlo simulation
and draws random samples from a ﬁtted multivariate distributions. A CVaR constraint,
that is, a constraint such that the CVaR of a portfolio does not exceed a certain value
C, can easily be included in the linear program as shown in equation (12.28) by
including an additional set of linear constraints: ζ + ν J
j=1 y j ≤C.
We conclude this section by highlighting the relation between mean–variance,
mean–VaR and mean–CVaR portfolios in the model framework of the previous sec-
tion, drawing on the work of Alexander and Baptista (2004). It is assumed throughout
that the returns of the ﬁnancial instruments are elliptically distributed (e.g., multivari-
ate normal). Recall equations (4.7) and (4.8) in which the VaR and ES were deﬁned
for normally distributed losses. The terms −1(α) = zα and φ(−1(α))/(1 −α) = kα
in those equations are the slope coefﬁcients in the (standard deviation, risk measure)
space, and the slope coefﬁcient for ES is greater than that for VaR. Hence, ceteris
paribus, the ES yields the more conservative risk measure, as should also be evident
from Figures 4.1 and 12.4. Furthermore, it was stated in Chapter 4 that VaR and ES
are both quantile values pertinent to the loss distribution. Therefore, a conﬁdence
level α∗exists with α∗< α, such that ESα∗= VaRα (e.g., ES0.975 = VaR0.99). In Sec-
tion 12.2 it was shown that mean–VaR portfolios are on the mean–variance efﬁcient
frontier if the former exist. Because the value of the expected shortfall can be made
coincident with VaR, but for a smaller conﬁdence level, portfolios that are optimized
with respect to ES/CVaR will be elements on the mean–variance efﬁcient frontier, too.
Furthermore, the following relation of the expected portfolio returns exists between
minimum-variance, minimum-VaR and minimum-CVaR portfolios:
E[μVaR] > E[μCVaR] > E[μMV].
(12.29)
This is because, ﬁrst, kα > zα, and second, one can simultaneously decrease the
expected return and the standard deviation by moving in a south-west direction
along the mean–variance efﬁcient frontier starting at the point of the minimum-
VaR portfolio. The reason why the expected portfolio return in the case of a CVaR
portfolio is greater than that for a minimum-variance portfolio lies in the deﬁnition of
the former. The latter does not depend on μ and hence the derivative with respect to
the standard deviation is zero. Matters are different for a minimum-CVaR portfolio;
here if one moves in a north-east direction along the efﬁcient frontier, the mean
increases by a greater amount relative to the standard deviation, which implies a
lower CVaR. Therefore, minimum-variance portfolios are also CVaR inefﬁcient for
any conﬁdence level α < 1. It was stated in the previous section that a mean–VaR
efﬁcient portfolio exists at the α conﬁdence level if zα > √D/C, where the scalars
D and C are as deﬁned in equations (12.7) and (12.8), respectively. Furthermore,

RISK-OPTIMAL PORTFOLIOS
227
the mean–VaR boundary is a proper subset of the mean–variance boundary, that is,
E[μMV] ≥E[μVaR]. A similar statement for CVaR is proved in Alexander and Baptista
(2004, Appendix), if the quantile zα is replaced by kα and the portfolio return of the
mean–VaR portfolio with μCVaR. In addition, if a risk-free asset is included, then the
proposition that a portfolio belongs to the mean–VaR efﬁcient frontier if zα >
√
H
for a given conﬁdence level α and that it is an element on the mean–variance efﬁcient
frontier is also true for mean–CVaR portfolios when zα is replaced with kα.
12.4
Optimal draw-down portfolios
Closely related to CVaR-optimal portfolios are portfolio optimization problems that
try to achieve weight solutions with respect to the portfolio’s draw-down. This kind
of optimization was proposed by Chekhlov et al. (2000, 2004, 2005). A comparison
between draw-down optimal portfolios and other risk-measure related optimization
techniques is made in Krokhmal et al. (2002). The task of ﬁnding optimal portfolio
allocations with respect to draw-down is of considerable interest to asset managers.
In an attempt to limit the chances of business termination, asset managers are eager to
avoid either large portfolio draw-downs and/or draw-downs over an extended period
of time. A client might be tempted to withdraw his mandate when this occurs, resulting
in the loss of management fees. This point has been exempliﬁed by Chekhlov et al.
(2005) for a commodity trading adviser.
The draw-down of a portfolio at time t is deﬁned as the difference between the
maximum uncompounded portfolio value prior to t and its value at t. More formally,
denote by W(ω, t) = y′
tω the uncompounded portfolio value at time t, with ω the
portfolio weights for the N assets included in it and yt the cumulated returns. Then
the draw-down, D(ω, t), is deﬁned as
D(ω, t) = max
0≤τ≤t{W(ω, τ)} −W(ω, t)
(12.30)
The authors cited above deduced three functional risk measures from this deﬁni-
tion: maximum draw-down (MaxDD), average draw-down (AvDD) and conditional
draw-down at risk (CDaR). Similar to CVaR, CDaR is dependent on the chosen
conﬁdence level α, so that this concept constitutes a family of risk functions. The
deﬁnition of the CDaR is akin to CVaR (see equation (12.27) above) for a data set in
the time interval [0, T ]:
CDaR(ω)α = min
ζ

ζ +
1
(1 −α)T
 T
0
[D(ω, t) −ζ]+ dt

,
(12.31)
where ζ is a threshold value for the draw-downs, such that only (1 −α)T observations
exceed this value. If the number of observations is not an integer, then ζ is the
weighted average of draw-downs that strictly exceed this threshold value and its next
lower bounded value, hence a computation likewise to CVaR is utilized, but now
CDaR is a risk functional and not a risk measure as in the case of CVaR, and it is
expressed in nominal rather than in return space. The limiting cases of this family
of risk functions are the MaxDD and the AvDD. For α →1, CDaR approaches

228
PORTFOLIO OPTIMIZATION APPROACHES
the maximum draw-down: CDaR(ω)α→1 = MaxDD(ω) = max0≤t≤T {D(ω, t)}. The
AvDD results for α = 0, that is, CDaR(ω)α=0 = AvDD(ω) = (1/T )
 T
0 D(ω, t) dt.
With respect to the optimization of a portfolio’s draw-down, these risk functionals
can be implemented as inequality constraints for a ﬁxed share of wealth at risk. For
instance, one could require that the MaxDD is at most 100ν1 per cent of the capital:
MaxDD(ω) ≤ν1C, where 0 ≤ν1 ≤1 and C is the available capital/wealth. Similarly,
this draw-down constraint can be speciﬁed for AvDD, AvDD(ω) ≤ν2C, and CDaR,
CDaR(ω) ≤ν3C, as can a linear combination of these three risk functionals, with
0 ≤ν1, ν2, ν3 ≤1. The portfolio optimization is then expressed in discrete terms and
the objective is deﬁned as maximizing the annualized average portfolio return:
R(ω) =
1
dC y′
T ω
(12.32)
where d is the number of years in the time interval [0, T ]. More precisely, the
following three linear program formulations are stated in Chekhlov et al. (2005) for
optimizing a portfolio such that its MaxDD, AvDD or CDaR is bounded by a certain
wealth fraction. The task of maximizing the average annualized portfolio return with
respect to limiting the maximum draw-down is given by
PMaxDD = arg max
ω,u
R(ω) =
1
dC y′
T ω,
uk −y′
kω ≤ν1C,
uk ≥y′
kω,
uk ≥uk−1,
u0 = 0,
(12.33)
where u denotes a (T + 1 × 1) vector of slack variables in the program formulation,
that is, the maximum portfolio values up to time period k with 1 ≤k ≤T .
When the portfolio is optimized with respect to limiting the average draw-down,
only the ﬁrst set of inequality constraints needs to be replaced with the discrete
analogue of the mean draw-down expressed in continuous time as stated in the text
above, leading to
PAvDD = arg max
ω,u
R(ω) =
1
dC y′
T ω,
1
T
T
k=1

uk −y′
kω

≤ν2C,
uk ≥y′
kω,
uk ≥uk−1,
u0 = 0,
(12.34)
The set-up for the CDaR linear program is slightly more cumbersome compared
to the previous two. In addition to the slack variables, u, that represent the maximum
portfolio wealth levels, two additional auxiliary variables need to be introduced: ﬁrst,
the threshold draw-down value ζ dependent on the conﬁdence level α; and second,

RISK-OPTIMAL PORTFOLIOS
229
the (T × 1) vector z representing the weak threshold exceedances. Hence, the linear
programming problem is given by
PCDaR = arg max
ω,u,z,ζ
R(ω) =
1
dC y′
T ω,
ζ +
1
(1−α)T
T
k=1 zk ≤ν3C,
zk ≥uk −y′
kω −ζ,
zk ≥0,
uk ≥y′
kω,
uk ≥uk−1,
u0 = 0.
(12.35)
These linear programs can be amended by further restrictions on ω such as budget,
non-negativity and/or box constraints.
A word of caution concerning these concepts is in order. The MaxDD as well
as the AvDD might capture or reﬂect portfolio losses inadequately. Recall that his-
toric return trajectories are employed in the linear program. Therefore, the portfolio
allocation that obeys a maximum draw-down constraint can be severely impacted
by a single worst draw-down. On the other hand, the average draw-down concept
is inconclusive about the size of the maximum portfolio’s draw-down, but this can
become quite an important issue in terms of risk management and control. The CDaR
measure circumvents these pitfalls to some extent, but all measures assume that the
historic path dependencies will somehow prevail in the subsequent periods.
12.5
Synopsis of R packages
12.5.1
The package fPortfolio
The package fPortfolio has already been discussed in Section 10.4.2. Here, the
package’s capabilities with respect to CVaR-portfolios as presented in Section 12.3 are
highlighted. A detailed description with worked examples is provided in W¨urtz et al.
(2010). In order to get started, an object of class fPFOLIOSPEC must be generated
with the portfolioSpec() function. If the default settings of this function are
used, then the CVaR characteristics must be explicitly speciﬁed. That is, the type of
portfolio is set to ’CVaR’ with the function setType(). Next, the user must decide
the conﬁdence level at which the optimization is carried out. This is accomplished with
the function setAlpha(). Incidentally, the conﬁdence level is pertinent to returns
and not losses (i.e., the left tail of the distribution). The minimum required settings for
a CVaR portfolio optimization problem are complete once the solver to be used has
been provided. At the time of writing, only the GLPK solver has been implemented
for solving the linear program and hence the right-hand side of the assignment reads
’solveRglpk’ in the call to setSolver(). Incidentally, constraints on the weights

230
PORTFOLIO OPTIMIZATION APPROACHES
are deﬁned later, when a given portfolio speciﬁcation is utilized in the call to the
optimizing function.
An efﬁcient CVaR portfolio solution for a given target return is returned by
the function efficientPortfolio(). The desired target return can be assigned
to an object of class fPFOLIOSPEC with the function setTargetReturn(). The
arguments of efficientPortfolio() are the return data (argument data), the fP-
FOLIOSPEC object (argument spec) and the constraints (argument constraints).
The solution of a global minimum-CVaR portfolio is returned by calling min-
riskPortfolio(), and the portfolio solution that maximizes the return to CVaR
ratio by the function maxratioPortfolio(). The risk-free rate is assigned to the
portfolio speciﬁcation object with the function setRiskFreeRate(). The portfolio
frontier of a CVaR portfolio can be computed with the function portfolioFron-
tier(), which returns an object of formal class fPORTFOLIO. The efﬁcient frontier
can then be plotted with the functions frontierPlot() and/or tailoredFron-
tierPlot(). The efﬁcient frontier can also be accomplished displayed by calling
the plot() method; this brings up a menu by which the plotting of the frontier can be
controlled and additional particular portfolio solutions can be marked on the frontier
as points.
12.5.2
The package FRAPO
The package FRAPO accompanying this book implements functions for computing
portfolio solutions with draw-down constraints. S4 classes and methods are employed.
All functions, classes and methods are exported via directives in its NAMESPACE. The
S4 class structure is hierarchically ordered such that the classes deﬁned for draw-
down optimal portfolios inherit from the general portfolio class PortSol. The virtual
class PortDD is the union of the particular draw-down classes in which the common
characteristics of this kind of portfolio are brought together. The methods available
for further analysing the solution of draw-down optimal portfolios are deﬁned either
for the S4 class PortSol or for the virtual class PortDD.
The linear programs as shown in equations (12.33)–(12.35) for the constrained
maximum draw-down, average draw-down and conditional draw-down at risk port-
folios are solved with GLPK by utilizing the function Rglpk_solve_LP() of the
package Rglpk.
A portfolio solution for a maximum draw-down constrained portfolio is returned
by the function PMaxDD(). This function is speciﬁed with a total of four arguments.
The ﬁrst element of the function’s closure is PriceData for holding the historic
asset prices. The only requirement for PriceData is that the object is rectangular
and, hence, has dimension 2. All time series related classes are supported as well
as matrix and data.frame objects. The second argument is MaxDD by which the
upper bound of the draw-down is speciﬁed. The user has the option by the logical
argument softBudget to decide whether the budget constraint is implemented as a
strict equality constraint (fully invested) or whether a partial allocation (i.e., the sum of
weights is less than 1), is allowed. The ﬁnal item in the function’s closure is the ellipsis
argument; its content, if any, is passed down to the function Rglpk_solve_LP().

RISK-OPTIMAL PORTFOLIOS
231
The function returns an object of formal class PortMdd with the slots weights, opt,
type, call, MaxDD and DrawDown. The ﬁrst slot carries the optimal weight vector. It
is displayed with some further information by the show() method or can be queried
by the Weights() method, both deﬁned for objects of class PortSol. The slot opt
is a list object and carries the output from the solver. The solution returned from
the optimizer can be queried by the Solution() method deﬁned for objects of class
PortSol. The next slot type is a character string containing information about the
kind of portfolio optimization, in this case ‘maximum draw-down’. The slot call is
of class call and contains the call of the object’s creating function. By providing
this information, one can provide an update() method for swiftly changing the
values of the arguments. So far, all slots are pertinent to the general class of portfolio
solutions, PortSol. The last two slots contain information peculiar to maximum
draw-down portfolios. The ﬁrst is MaxDD in which the maximum draw-down of the
portfolio solution is returned. This value need not coincide with the speciﬁed upper
bound, that is, the speciﬁed upper bound in the call of PMaxDD() is a not a binding
constraint. The historic portfolio draw-downs are contained in the slot DrawDown. In
addition to the methods described above, a DrawDowns() and a plot() method are
deﬁned for objects of virtual class PortDD. The former returns the historic portfolio
draw-downs as a timeSeries object and the latter plots the portfolio’s draw-downs.
The maximum draw-down is superimposed on this time series chart.
Portfolio solutions constrained by the average draw-down can be computed by
means of the function PaveDD(). The function’s argument speciﬁcation differs only
by one argument from PMaxDD(). The upper bound of the draw-down constraint
is now AveDD instead of MaxDD. The function returns an object of formal class
PortAdd and differs only with respect to the slot that carries the constraint infor-
mation: AveDD for the average historical portfolio draw-down. The methods de-
scribed above are also available through inheritance. With respect to the plot()
method, the average draw-down is now superimposed on the time series chart of the
draw-downs.
Portfolio optimizations that refer to the conditional draw-down concept are avail-
able in two ﬂavours: ﬁrst, a solution for a portfolio that is constrained by the CDaR
can be computed with the function PCDaR(); and second, a minimum CDAR port-
folio is returned by calling PMinCDaR(). Both functions return an object of class
PortCdd, which inherits from the class PortSol and is a member of the class
union PortDD. The object’s deﬁnition differ from the previous ones by having slots
CDaR and thresh instead of MaxDD for the maximum draw-down and AveDD for
the average draw-down portfolios. The slot CDaR carries the value of the conditional
draw-down at risk for the speciﬁed conﬁdence level alpha in the call to either PC-
DaR() or PMinCDaR(), and the slot thresh is the threshold value for the conditional
draw-downs. In the linear program speciﬁed in equation (12.35) above, ζ denotes
this value. If a constrained CDaR portfolio solution is computed by calling PCDaR(),
the permitted upper bound of the CDaR is speciﬁed via the argument bound (default
value 0.05). The conﬁdence level in both functions is set by the argument alpha
with a default value of 0.95. The plot of the draw-downs is now superimposed by the
value of CDaR.

232
PORTFOLIO OPTIMIZATION APPROACHES
12.5.3
Packages for linear programming
In this subsection R packages for solving linear programs are presented. The focus
is on package implementations based on freely available (i.e., open source) solvers,
which are ordinarily covered by a GPL or similar licence. However, it should be noted
that R packages that interface commercial optimizers are also hosted on CRAN and/or
R-Forge. The package Rcplex provides an interface to the CPLEX solver package
(see Corrada Bravo et al. 2011) and the package Rmosek provides an interface to the
commercial MOSEK optimizer (see Friberg 2012). For more information, consult the
vendors’ Internet sites: http://www-01.ibm.com/software/integration/
optimization/cplex-optimizer/ and http://www.mosek.com/, respec-
tively. At the time of writing, both vendors offered free trial versions and/or free
academic licence agreements. Finally, it should be mentioned that the R package
rneos (see Pfaff 2011) provides user access to the Network-Enabled Optimization
System (NEOS – see Czyzyk et al. 1998; Dolan 2001; Gropp and Mor´e 1997) by
means of the XML-RPC API. Incidentally, the MOSEK solver is available in NEOS. For
more information about NEOS, see the project’s Internet site at http://www.neos-
server.org.
The packages glpkAPI and Rglpk
The packages glpkAPI (see Gelius-Dietrich 2012) and Rglpk (see Theussl and
Hornik 2012) both provide access to the GNU Linear Programming Kit (GPLK).
This software package is organized in the form of a callable library written in ANSI
C. With the GLPK, large-scale linear programs as well as mixed integer programs
can be solved. As optimization techniques primal/dual simplex, primal/dual interior-
point and branch-and-cut methods have been implemented. The GLPK software is
shipped with an API and a stand-alone LP/MILP solver. Problems can be formulated
in the GNU MathProg modelling language (GMPL) which is a subset of the AMPL
language. More information about GLPK can be found on the project’s Internet site,
http://www.gnu.org/s/glpk/.
The two R packages differ in terms of how access between R and GLPK is
established and the layer of abstraction. An implementation of the C API is made
available in the package glpkAPI. This package is hosted on CRAN. It requires a
GLPK installation, including libraries and header ﬁles. Directives for installing the
package are provided in an INSTALL ﬁle, located in the root directory of the source
package. The package is shipped with a vignette in which the usage of the cornerstone
functions for setting up and solving a linear program are shown.
A different route is followed for accessing GLPK from R within the package
Rglpk. Instead of offering the user a low-level entry point to the C API, as is the case
with the previous package, the package Rglpk offers high-level access. The package
is hosted on CRAN and R-Forge. It is considered to be a core package in the CRAN
‘Optimization’ Task View. The package is shipped with a NAMESPACE ﬁle in
which the two main functions Rglpk_read_file() and Rglpk_solve_LP() are
exported as well as a print() method for ﬁles that have been imported by the GLPK

RISK-OPTIMAL PORTFOLIOS
233
ﬁle reader. The package is dependent on the package slam for handling sparse arrays
and matrices. Linear programs in either the ﬁxed or free MPS format or expressed in
the CPLEX LP macro language can be imported and brought into memory with the
function Rglpk_read_file(). This is accomplished with the GLPK ﬁle reader.
The function is endowed with four arguments. The argument file expects a char-
acter string which speciﬁes the absolute or relative path to the model ﬁle. With the
argument type the format of the model ﬁle is selected. The remaining two arguments
are of type logical and the user can indicate whether the ﬁrst row should be ignored
by setting ignore_first_row to FALSE (the default) and whether additional solver
output should be printed while the model ﬁle is parsed with the verbose argument
(the default is set to FALSE). The function returns a list object of informal class
MP_data_from_file for which a print() method is available. The relevant
elements of this kind of object could then be used in the call of Rglpk_solve_LP(),
the cornerstone function of the package. The arguments of this function are:
r obj for the coefﬁcients of the objective function;
r mat for the constraint coefﬁcients (either a vector or a matrix object);
r dir for indicating the kind of inequality/equality constraint speciﬁed as a
character vector (possible directions are "<", "<=", ">", ">=", or "==");
r rhs for the right-hand-side values of the constraints;
r types for determining whether the variables of the objective function can take
continuous ("C"), integer ("I") or binary ("B") values (the default setting is
that all variables are deﬁned as continuous variables);
r max for indicating whether the objective is to maximized or minimized (the
default value is FALSE, which means that the objective will be minimized);
r bounds for deﬁning allowable ranges of the variables;
r verbose which serves as a logical switch for controlling the printing of addi-
tional output from the solver.
The function returns a list object with the elements solution, objval and sta-
tus. The ﬁrst list element contains the solution found for the variables, the second
element is the value of the objective function, and the last element is a status ﬂag
indicating whether an optimal solution was determined (0) or not.
The package linprog
For solving linear programs, in the package linprog (see Henningsen 2010) the
function lp() contained in the package lpSolve can be employed; otherwise the
linear program is solved by the simplex algorithm which is directly implemented
as R code. The package lpSolve is discussed below. The package linprog is hosted
on CRAN and contained in the ‘Optimization’ Task View. It is shipped with a
NAMESPACE ﬁle and S3 classes and methods are employed.

234
PORTFOLIO OPTIMIZATION APPROACHES
Linear programs in the MPS format can be brought into memory with the function
readMps(). In contrast to the package Rglpk in which the GLPK ﬁle reader is
employed, the parsing of the MPS ﬁle is accomplished entirely in R. In addition
to the argument file which points to the MPS ﬁle to be read, the user can specify
whether the linear program should immediately be solved by setting the logical switch
solve to TRUE (the default value is FALSE), and whether the objective function
should be maximized or minimized by the argument maximum (the default value is
FALSE, which means that the objective function will be minimized). The function
readMps() will return a list of named objects with elements containing the relevant
information such that these can be employed in a call to solveLP(). These are name
for carrying the name of the problem and the elements cvec, bvec and Amat which
refer to coefﬁcients of the objective function, the right-hand-side variables and the
left-hand-side constraint matrix, respectively. In the case of direct optimization, where
solve=TRUE has been used, the function will return an additional list element, res,
which is an object of informal class solveLP.
The user can specify the direction of the inequality constraints by means of the
argument const.dir. The default value is to treat all constraints as less-than-or-
equal-to constraints. At the time of writing, equality constraints are not implemented.
The next group of arguments are intended for controlling the algorithm. The maximum
number of iterations can be set with the argument maxiter, a threshold value for
ﬂoating point numbers to be considered as equal is determined by the argument
zero and the convergence tolerances for the primal and dual speciﬁcation of the
linear program can be speciﬁed with the arguments tol and dualtol, respectively.
Whether the the problem is solved by means of the simplex algorithm or by calling
lp() is controlled by the logical switch lpsolve. The default is lpsolve=FALSE.
With the last two arguments the user can also specify whether the linear program in
its dual form is solved (the default is FALSE) and/or whether any intermediate results
are returned to the console (and, if so, in how much detail) by assigning an integer
value between 0 and 4 to the argument verbose. The function returns a named
list object of S3 class solveLP for which print() and summary() methods are
available.
Finally, the function writeMps() enables the user to save a linear program in
MPS format. In addition to the arguments cvec, bvec and Amat for specifying the
coefﬁcient, a name argument is used to assign a name to the linear program (the
default is set to ’LP’). The main purpose of this function is to cross-compare results
between different solvers, given that these can digest MPS ﬁles.
The packages lpSolve and lpSolveAPI
Akin to GLPK, the software lp_solve is in the public domain and its purpose is to
provide routines for solving linear (integer) programs. This suite supports not only LP
and MILP, but also binary, semi-continuous and special ordered set problems. These
routines are collected into a library which can be accessed through an API written in
ANSI C. The project is hosted on the Sourceforge platform, and more information is
available on the project’s Internet site at http://lpsolve.sourceforge.net/.

RISK-OPTIMAL PORTFOLIOS
235
Two R packages are available that give users access to lp_solve: lpSolve (see
Berkelaar 2011) and lpSolveAPI (see Konis 2011). They differ from each other
with respect to the level of interfacing with lp_solve. In the former package a rather
high-level interface is implemented in the form of a driver routine, whereas the latter
package provides low-level access by means of the C API functions.
The package lpSolve is hosted on CRAN and is considered to be a core package
in the ‘Optimization’ Task View. S3 classes and methods have been made employed,
and the package ships with a NAMESPACE ﬁle with export directives for the functions
and methods deﬁned. The cornerstone function for solving LP or MILP is lp(). The
function returns a named list object of informal class lp, for which a print()
method has been deﬁned. The elements consist of the elements that have been passed
to the solver when lp() has been called – the direction of the optimization, the co-
efﬁcients of the objective function, the right-hand-side matrix and the left-hand-side
constraints. In addition, objects of class lp contain as list elements the value of the
objective function and the associated solution vector. Furthermore, the number of
(integer) variables, their positions in the vector of variables and the number of con-
straints are returned. Finally, the list element status indicates successful completion
of the algorithm if it takes the value 0; if no feasible solution is found, a 2 is returned
instead. Incidentally, the package limSolve (see Soetaert et al. 2009) provides the
wrapper function linp(), in which the constraints are separated according to their
direction. The remaining functions lp.assign() and lp.transport() contained
in the package lpSolve are intended for speciﬁc types of linear program, namely
assignment and transportation problems, respectively.
Hosted on CRAN and R-Forge as project lpsolve, the package lpSolveAPI
is contained in the CRAN ‘Optimization’ Task View. Development versions of
the package can be obtained from the source (http://lpsolve.r-forge.r-
project.org). A vignette is available in which the deﬁnition, handling, solving
and solution retrieval of a linear program model object (LPMO) are elucidated by
a worked-through example linear program. Therefore, a typical work ﬂow would
consist of ﬁrst deﬁning a linear program with respect to its dimension by the function
make.lp() and then ﬁlling the LPMO object (an external pointer to a C structure)
with data and specifying the type of variables and the directions of the constraints.
When this is accomplished, the model can be solved by a call to the function solve().
The results can then be retrieved by means of access methods. Methods for LPMO
printing and plotting (two-dimensional only) are available, as well as R functions
for reading a linear program speciﬁcation (read.lp()) that has been expressed in
either LP (a macro-language pertinent to lp_solve) or ﬁxed/free MPS format. Finally,
a linear program speciﬁcation can be written to a ﬁle in the same formats with the
function write.lp().
The package Rsymphony
The package Rsymphony provides a high-level interface to the MILP solver Sym-
phony (see Harter et al. 2012). The package is hosted on CRAN and R-Forge. It
is contained in the ‘Optimization’ Task View and has the status of a core package.

236
PORTFOLIO OPTIMIZATION APPROACHES
The solver SYMPHONY is part of the COIN-OR project (see http://www.coin-
or.org/SYMPHONY/ for more information). The solver implements the branch, cut,
and price approach for solving MILP problems. As such the approach taken is more
powerful than using the branch and bound method for solving linear programs alone.
The solver itself is written in the C language.
The package exports the function Rysmphony_solve_LP() for solving MILP
problems. The target vector of the objective function has to be provided as argument
obj to this function. The coefﬁcient matrix, the relation operator between this and
the right-hand-side variables, and the right-hand-side variables themselves have to
be speciﬁed as arguments mat, dir and rhs, respectively. The following relation
operators can be used as characters for dir: ’<’, ’<’, ’<=’, ’>’, ’>=’, ’==’,
and ’!=’. Bounds for the target variables can be speciﬁed as a list object with
elements lower and upper for the respective lower and upper bounds. The type
of target variables is determined by the argument type. Here, the user can specify
whether a target variable is treated as continuous (’C’), integer (’I’) or binary (’B’).
If this argument is left NULL then all targets are taken as continuous variables. Finally,
the logical argument max controls whether the objective function is maximized or
minimized (the default). The function Rysmphony_solve_LP() returns a list
object with elements solution, objval, and status for the solved target variables,
the value of the objective function, and an integer which indicates the status of the
solver’s execution, respectively.
The package is further endowed with functions that convert the coefﬁcient matrix
into the sparse column major order used by SYMPHONY as well as for preparing
and checking the speciﬁed bounds. These functions are not exported via directives in
the NAMESPACE ﬁle of the package.
12.5.4
The package PerformanceAnalytics
The focus of the package PerformanceAnalytics is on the provision of functions
and methods for assessing the performance of a given portfolio allocation and the
associated risks (see Carl et al. 2012). The package is contained in the CRAN
‘Finance’ Task View and is considered to be a core package. The latest development
versions can be obtained from both CRAN and R-Forge. The user is guided by means
of three vignettes through the utilization and application of the cornerstone functions.
A feature of the package is the support of the most commonly encountered time series
classes – time series objects of classes xts, timeSeries and zoo are supported.
Due to the large number of functions contained in the package, the following
presentation concentrates on the facilities directly related to the topics discussed in
this chapter as well as in Chapter 4.
The risk measures VaR and CVaR/ES are implemented as functions VaR() and
ES(), respectively. Both functions can be applied to either a univariate stream of
returns or an object that contains the returns of the portfolio constituents (function
argument R). If the latter is chosen, a corresponding weight vector has to be provided
for the argument weight. The conﬁdence level is determined by the argument p, with
default value 0.95. With respect to the estimation method of these risk measures, the

RISK-OPTIMAL PORTFOLIOS
237
user can choose between a Cornish–Fisher modiﬁed type (method = ’modified’),
a calculation based on the assumption of normally distributed returns (method
= ’gaussian’), the use of historic returns (method = ’historical’), or a
kernel density estimate (method = ’kernel’). A feature of these functions is the
option to ‘clean’ the returns by means of the argument clean. Two ways of robus-
tifying the results are implemented: a return adjustment according to the ﬁrst-order
autocorrelation coefﬁcient (clean = ’geltner’), or a scheme proposed in Boudt
et al. (2008) by which the returns are trimmed while preserving the tail characteristics
(clean = ’boudt’). If a multivariate return series object is passed to the function,
the user can determine whether the VaR or ES is computed as a single ﬁgure for
the implied portfolio returns for a given weight vector or whether in addition the
component and/or marginal contributions of the assets to the overall risk ﬁgures are
also computed by means of the portfolio_method argument. Finally, it should
be noted that instead of passing an object with the assets’ returns to the function
ES() or VaR(), the user can directly provide moment estimates by means of the
arguments mu, sigma, m3 and m4 for the mean(s), the variance–covariance matrix,
the (co-)skewness and the (co-)kurtosis, respectively.
The
functions
that
relate
to
the
draw-down
concept
are
CDD(),
chart.Drawdown(),
Drawdowns(),
findDrawdowns(),
max.Drawdown(),
sort.Drawdowns() and table.Drawdowns(), in alphabetical order. The CDD()
function is an implementation of the conditional draw-down at risk concept. In
addition to the returns object, R, the user can specify the conﬁdence level by the
argument p and provide a weight vector, in the case of a multivariate return object,
by means of the argument weights. In addition, one has to specify whether the
returns are discrete or continuous by the logical argument geometric. The draw-
down measure can be inverted to a positive scale by employing the logical argument
invert. Incidentally, this last argument is also part of the functions’ closures in
VaR() and ES(). The implied draw-downs of a return stream can be visualized with
the function chart.Drawdown(). For a multivariate return object, the draw-down
curves are plotted on the same panel and the user can specify where a legend is placed
with the argument legend.loc and which colours are used by means of colorset.
The functions Drawdowns() and findDrawdowns() can be utilized for inspecting
the draw-downs in more detail. Both functions return a list object with elements
the size of draw-down, the starting, trough and ending periods, the length of these
episodes in total and the length from peak to trough and from trough until the next
peak. The function Drawdowns() returns the draw-downs expressed as a percentage
from the equity levels and is employed in the function chart.Drawdown(). The user
must specify whether the return data is pertinent to discrete or continuous returns.
The maximum draw-down of a return stream is returned by the function maxDraw-
down(). Again, the user has to specify by means of the argument geometric the
kind of return data. If the function is employed for returning the maximum draw-
down of a portfolio, the weights have to be supplied by weights. The draw-downs
can be sorted by their sizes with the function sortDrawdowns(). The function has
one argument runs, which is the list object returned by the function findDraw-
downs(). Finally, the historic draw-downs can be returned in a tabular format by

238
PORTFOLIO OPTIMIZATION APPROACHES
the function table.Drawdowns(). In addition to the returns (provided through the
argument R), the user can specify how many draw-downs to include in the tabular
structure by the argument top and the number of decimal places for the returns via
the argument digits (the default is to print the draw-downs to four decimal places).
12.6
Empirical applications
12.6.1
Minimum-CVaR versus minimum-variance portfolios
In this ﬁrst application, a comparison between a global minimum-CVaR and a global
minimum-variance portfolio is conducted in the form of a back-test for an index-based
equity portfolio. The monthly returns of the following stock indexes are considered:
the S&P 500, the Nikkei 225, the FTSE 100, the French CAC 40, the German DAX
and the Hang Seng. The sample period of the index data runs from 31 July 1991 until
30 June 2011 and comprises a total of 240 observations. The R code for conducting
the back-test is shown in Listings 12.1 and 12.2.
First, the packages are brought into memory. The data set StockIndex is
contained in the package FRAPO and the routines of the package fPortfolio for
optimizing the minimum-CVaR and minimum-variance portfolios are utilized. In
line 4 of Listing 12.1, the data set is loaded into the work space and in line 5 the
discrete percentage returns are computed.
The commands in which the portfolio speciﬁcations are deﬁned are given on
lines 8–14. First, a default speciﬁcation is assigned to the object pspec which can
directly be used for the global minimum-variance portfolios (object gmv). In order
to deﬁne a CVaR portfolio (object cvar) some amendments to the default portfolio
deﬁnition have to be made. The type of the portfolio has to be set to ’CVaR’ with
the setType() function. A conﬁdence level has to be assigned. Here, a 10% level
is assigned to the portfolio speciﬁcation by the setAlpha() function. Incidentally,
and as stated previously, the conﬁdence level must be speciﬁed in return and not
in loss space, hence a value of 0.1 coincides with the 90% conﬁdence level of the
loss distribution. Finally, the solver to be used is GLPK, hence the character string
’solveRglpk’ is assigned to cvar by the setSolver() function. That is all it
takes to deﬁne the two approaches for optimizing the portfolios in the following.
The back-test itself is carried out in lines 15–28. The portfolios will be optimized
with respect to subsamples of size 60, that is, a prior data history of 5 years is
employed. The start and end points for the sliding data windows are assigned to the
objects start and end. Next, two matrices are deﬁned which are designated to store
the portfolio weight vectors as rows. Hence, sufﬁcient memory is allotted which will
not hamper the speed of execution in the ensuing for loop. Executing the back-test
in a for loop might be considered as not R-like and one could have put the back-test
in terms of the fapply() function which is contained in the package timeSeries
for instance, or one could have created an object for the returns with class attribute
zoo in the ﬁrst place and utilized the rollapply() function of the package zoo
instead. However, the aim of putting the back-test in a for loop is threefold. First,

RISK-OPTIMAL PORTFOLIOS
239
Listing 12.1 Minimum-CVaR versus minimum-variance portfolio: back-test.
library(FRAPO)
1
library(fPortfolio)
2
## Retrieving data and calculating returns
3
data(StockIndex)
4
StockReturn <−na.omit(timeSeries(returnseries(StockIndex,
5
method = "discrete"),
6
charvec = rownames(StockIndex)))
7
## Specifying portfolio
8
pspec <−portfolioSpec()
9
gmv <−pspec
10
cvar <−pspec
11
setType(cvar) <−"CVaR"
12
setAlpha(cvar) <−0.1
13
setSolver(cvar) <−"solveRglpk"
14
## Conducting back−test
15
end <−time(StockReturn)[60:239]
16
from <−time(StockReturn)[1:length(end)]
17
wGMV <−matrix(NA, ncol = ncol(StockReturn), nrow = length(end))
18
wCVAR <−wGMV
19
for(i in 1:length(end)){
20
series <−window(StockReturn, start = from[i], end = end[i])
21
gmvpf <−minvariancePortfolio(data = series, spec = gmv,
22
constraints = "LongOnly")
23
wGMV[i, ] <−c(getWeights(gmvpf))
24
cvarpf <−minriskPortfolio(data = series, spec = cvar,
25
constraints = "LongOnly")
26
wCVAR[i, ] <−c(getWeights(cvarpf))
27
}
28
it is shown that the prejudice against for loops is unjustiﬁed (if they are properly
set up). Second, by looking at the body of the for loop, the user sees exactly
what is happening. Finally, the deﬁnition of a function that must be called from
either fapply() or rollapply() is avoided. Within the for loop, the relevant
data window from StockReturn is assigned to the object series. This object,
together with the speciﬁcation gmv and the long-only constraint, is then used next
in the determination of the solution for global minimum-variance portfolios, which
is accomplished by the minvariancePortfolio() function. The corresponding
weight vector is retrieved by the access function function getWeights() and stored
in the ith row of the matrix object wGMV. The same is done in the code lines 25–27 for
the global minimum-CVaR portfolios. Here, the function minriskPortfolio() is
called and as arguments the series object and the long-only constraint together with

240
PORTFOLIO OPTIMIZATION APPROACHES
Listing 12.2 Plotting wealth trajectory.
## Compute portfolio values (subsequent returns)
1
wGMVL1 <−lag(timeSeries(wGMV, charvec = end), k = 1)
2
colnames(wGMVL1) <−colnames(StockReturn)
3
wCVARL1 <−lag(timeSeries(wCVAR, charvec = end), k = 1)
4
colnames(wCVARL1) <−colnames(StockReturn)
5
## Return factors and portfolio values
6
GMVRetFac <−1 + rowSums(wGMVL1 ∗
7
StockReturn[time(wGMVL1), ]) / 100
8
GMVRetFac[1] <−100
9
GMVPort <−timeSeries(cumprod(GMVRetFac),
10
charvec = names(GMVRetFac))
11
CVARRetFac <−1 + rowSums(wCVARL1 ∗
12
StockReturn[time(wCVARL1), ]) / 100
13
CVARRetFac[1] <−100
14
CVARPort <−timeSeries(cumprod(CVARRetFac),
15
charvec = names(CVARRetFac))
16
## Plotting of portfolio values
17
ylims <−range(cbind(GMVPort, CVARPort))
18
plot(GMVPort, ylim = ylims, xlab = " ",
19
ylab = "Portfolio Value (Index)")
20
lines(CVARPort, col = "blue")
21
legend("topleft",
22
legend = c("Minimum-Variance", "Minimum-CVaR"),
23
col = c("black", "blue"), lty = 1)
24
## Relative performance
25
RelOutPerf <−(CVARPort −GMVPort) / GMVPort ∗100
26
plot(RelOutPerf, type = "h", col = "blue", xlab = " ",
27
ylab = "Percent",
28
main = "Relative Performance Minimum−CVaR vs. Minimum−Variance")
29
abline(h = 0, col = "grey")
30
the portfolio speciﬁcation object cvar are employed. The optimal portfolio weight
vector is then assigned into the ith row of the object wCVAR.
In order to compute the portfolios’ equity lines, the inner products between the
lagged portfolio weights and the subsequent returns have to be computed for each of
the two portfolio approaches. This is accomplished in the ﬁrst part of Listing 12.2.
From these portfolio return streams, the return factors can swiftly computed by means
of the cumprod() function, given that the returns have previously been calculated
as discrete percentage changes and the initial wealth position has been assumed to be
equal to 100. The portfolio values for the back-test period are assigned to the objects
GMVPort and CVARPort for the global minimum-variance portfolios and the global
minimum-CVaR portfolios, respectively.

RISK-OPTIMAL PORTFOLIOS
241
Portfolio Value (Index)
1996−07−31
2002−07−18
2008−07−05
100
150
200
250
Minimum−Variance
Minimum−CVaR
Figure 12.5
Trajectory of mininum-CVaR and minimum-variance portfolio values.
These two equity curves are then displayed by means of lines 17–24 (see Figure
12.5) and the relative performance with respect to the equity line of the GMV portfolio
as a barplot in the ﬁnal lines of the listing (see Figure 12.6).
For this back-test design, the CVaR portfolio approach outperforms the GMV
portfolio solutions. However, the sharp portfolio draw-downs witnessed during the
ﬁnancial market crisis during 2007 and 2008 could not be averted by either of the
two approaches.
Relative Performance Minimum−CVaR vs. Minimum−Variance
Percentages
1998−01−01
2002−01−01
2006−01−01
2010−01−01
0
5
10
15
20
25
Figure 12.6
Relative performance of minimum-CVaR and minimum-variance
portfolios.

242
PORTFOLIO OPTIMIZATION APPROACHES
12.6.2
Draw-down constrained portfolios
In this example, the solution of a global minimum-variance portfolio is again utilized
as a benchmark allocation for long-only investments. The characteristics of this
solution are compared with the allocations of portfolios that are restricted by their
maximum, average and conditional draw-downs and with a minimum conditional
draw-down at risk portfolio. The spectrum of assets covers the major equity and bond
markets as well as investment in gold. The R code is presented in Listing 12.3.
First the relevant R packages are brought into memory. Then the MultiAsset
data set is loaded, which is part of the FRAPO package. The data set covers the
month’s-end prices of a number of equity indexes (S&P 500, Russell 3000, DAX
(XETRA), FTSE 100, Nikkei 225 and iShares MSCI Emerging Markets) and ﬁxed
income indexes (Dow Jones CBOT Treasury, German REX Performance and United
Kingdom gilts (all maturities)) as well as the price of the SPDR Gold Shares exchange
traded fund from 30 November 2004 to 30 November 2011. In lines 5–8 the discrete
returns are computed and converted to a timeSeries object. Commencing in line 11
the solution of the global minimum-variance portfolio is determined and the historic
draw-downs are computed with the function Drawdowns() contained in the package
PerformanceAnalytics. Plots of these are displayed through the commands in lines
17–21. The time series plot is created from scratch in order to make its appearance the
same as the draw-down plots for the ensuing portfolio plots. The result is provided
in Figure 12.7. Incidentally, one could have used the function chart.Drawdown()
instead, as mentioned in Section 12.5.4.
In the following lines the various draw-down portfolio solutions are computed,
with the maximum draw-down of the GMV portfolio serving as anchor value. The
conditional draw-down at risk portfolios are computed for a conﬁdence level of 95%.
Draw−Downs of Global Minimum Variance
Draw−Downs (percentage)
2005−01−01
2007−01−01
2009−01−01
2011−01−01
−6
−5
−4
−3
−2
−1
0
Figure 12.7
Draw-downs of GMV portfolio.

RISK-OPTIMAL PORTFOLIOS
243
Listing 12.3 Comparison of draw-down and GMV portfolios.
library(fPortfolio)
1
library(FRAPO)
2
library(PerformanceAnalytics)
3
data(MultiAsset)
4
## Return calculation
5
Rets <−returnseries(MultiAsset, method = "discrete",
6
percentage = FALSE, trim = TRUE)
7
Rets <−timeSeries(Rets, charvec = rownames(Rets))
8
## Benchmark portfolio: GMV
9
gmvspec <−portfolioSpec()
10
GMV <−minvariancePortfolio(data = Rets, spec = gmvspec,
11
constraints = "LongOnly")
12
GMVret <−timeSeries(Rets %∗% getWeights(GMV),
13
charvec = time(Rets))
14
GMVDD <−Drawdowns(GMVret)
15
## Plot of draw−downs for GMV
16
ylims <−c(−6, 0)
17
plot(GMVDD ∗100, xlab = " ", ylab = "Draw−Downs (percentage)",
18
main = "Draw−Downs of Global Minimum Variance", ylim = ylims)
19
abline(h = 0, col = "grey")
20
grid()
21
## Max DD of GMV
22
GMVMaxDD <−max(−1.0 ∗GMVDD)
23
## Draw−down portfolios
24
MaxDD <−PMaxDD(MultiAsset, MaxDD = GMVMaxDD)
25
AveDD <−PAveDD(MultiAsset, AveDD = GMVMaxDD)
26
CDaR95 <−PCDaR(MultiAsset, alpha = 0.95, bound = GMVMaxDD)
27
CDaRMin95 <−PCDaR(MultiAsset, alpha = 0.95)
28
## Plot of draw−downs
29
oldpar <−par(no.readonly = TRUE)
30
par(mfrow = c(2, 2))
31
plot(AveDD, main = "(a) AveDD")
32
plot(MaxDD, ylim = ylims, main = "(b) MaxDD")
33
plot(CDaR95, ylim = ylims, main = "(c) CDaR")
34
plot(CDaRMin95, ylim = ylims, main = "(d) Minimum CDaR")
35
par(oldpar)
36

244
PORTFOLIO OPTIMIZATION APPROACHES
Draw−Downs (percentages)
(a) AveDD
2005−01−01
2009−01−01
−20
−10
0
Draw−Downs (percentages)
(b) MaxDD
2005−01−01
2009−01−01
−6
−4
−2
0
Draw−Downs (percentages)
(c) CDaR
2005−01−01
2009−01−01
−6
−4
−2
0
Draw−Downs (percentages)
(d) Minimum CDaR
2005−01−01
2009−01−01
−6
−4
−2
0
Figure 12.8
Comparison of draw-downs.
Next, the trajectory of the four historic draw-down series is depicted in the form of a
(2 × 2) plot where for a better cross-comparison the ordinates share the same scale
except for the average draw-down constrained portfolio (see Figure 12.8).
As already hinted in Section 12.4, the solution of the average draw-down con-
strained portfolio can imply the occurrence of large draw-downs. In this case the
maximum draw-down of the average draw-down constrained solution is greater by
roughly a factor of 4 than the maximum historic draw-down of the minimum condi-
tional draw-down at risk portfolio. By cross-comparing the draw-downs with those
implied by the GMV solution, the latter seems to fare reasonable well, at ﬁrst glance.
The characteristics of the ﬁve portfolio solutions are next analysed with respect
to the different allocations and their associated risk contributions and diversiﬁcation
ratios. The calculation of these statistics is provided in Listing 12.4.
In line 2 of this listing, the names for the ﬁve portfolio types are deﬁned; these will
be used as column names. Next the portfolio weights are extracted from the objects
created earlier and assigned to the matrix object WeightMatrix. This object can then
be swiftly utilized to compute the portfolios’ ES at the 95% level and the percentage
contributions of the underlying assets to it with the function ES(). The intermediate
result in the form of a list object is assigned to tmp. The relevant items can then
be easily extracted by employing lapply() and using the unlist() function to
assemble the results as a matrix. The marginal contributions to the portfolio risk
are computed in line 20 onwards with the function mrc() which is included in a
call to apply(). In a similar manner the diversiﬁcation ratios for the ﬁve portfolios
are computed using the function dr() in line 25. The results are summarized in
Table 12.1.

RISK-OPTIMAL PORTFOLIOS
245
Listing 12.4 Analysis of portfolio solutions.
## Portfolio names
1
Pnames <−c("GMV", "MaxDD", "AveDD", "CDaR95",
"CDaRMin95")
2
## Portfolio allocations
3
WeightMatrix <−cbind(getWeights(GMV),
4
Weights(MaxDD),
5
Weights(AveDD),
6
Weights(CDaR95),
7
Weights(CDaRMin95))
8
colnames(WeightMatrix) <−Pnames
9
## Expected shortfall and components
10
tmp <−apply(WeightMatrix, 2, function(x) ES(Rets, weights = x,
11
method = "gaussian", portfolio_method = "component"))
12
## ES 95
13
PES <−unlist(lapply(tmp, function(x) x[[1]])) ∗100
14
## Marginal contributions to ES
15
PMES <−matrix(unlist(lapply(tmp, function(x) x[[3]])),
16
nrow = ncol(Rets)) ∗100
17
rownames(PMES) <−colnames(Rets)
18
colnames(PMES) <−Pnames
19
## Marginal contributions to StdDev
20
V <−cov(Rets)
21
PMRC <−apply(WeightMatrix, 2, mrc, Sigma = V)
22
rownames(PMRC) <−colnames(Rets)
23
## Diversiﬁcation ratio
24
PDR <−apply(WeightMatrix, 2, dr, Sigma = V)
25
The GMV portfolio is characterized by a high share allotted to German govern-
ment bonds, due to the low volatility of this asset. Exposure to equities is roughly
10% and investments in UK gilts and gold amount to less than 1% in each case.
According to the GMV solution, the capital is split between six of the ten possible
assets. Overall, the empirical stylized fact of highly concentrated solutions for GMV
portfolios is vindicated. This becomes even more apparent when the marginal contri-
butions to the 95% ES and the overall portfolio risk is taken into account. However,
when one compares the diversiﬁcation ratio of this portfolio with the values taken
by the draw-down portfolios, the conclusion would be that the GMV yields the most
favourable asset allocation in terms of the degree of diversiﬁcation. This artefact can
again be primarily attributed to the low volatility of the REX, which is mirrored by a
low ﬁgure for the ES.
The number of assets invested in is the same for the constrained maximum
draw-down portfolio. However, this solution differs in two major points with respect

246
PORTFOLIO OPTIMIZATION APPROACHES
Table 12.1
Comparison of portfolio allocations and characteristics.
Analysis
GMV
MaxDD
AveDD
CDaR95
CDaRMin95
S&P 500
Weight
4.89
0.00
0.00
0.00
0.00
MES
5.58
0.00
0.00
0.00
0.00
MPR
4.89
0.00
0.00
0.00
0.00
Russell 3000
Weight
0.00
3.00
0.00
6.40
0.00
MES
0.00
−0.93
0.00
−3.68
0.00
MPR
0.00
−0.44
0.00
−2.10
0.00
DAX (XETRA)
Weight
4.34
3.62
0.00
0.00
0.00
MES
3.46
−3.51
0.00
0.00
0.00
MPR
4.34
−1.83
0.00
0.00
0.00
FTSE 100
Weight
0.00
0.00
0.00
0.00
0.00
MES
0.00
0.00
0.00
0.00
0.00
MPR
0.00
0.00
0.00
0.00
0.00
Nikkei 225
Weight
1.73
0.91
0.00
0.00
0.00
MES
2.37
−0.50
0.00
0.00
0.00
MPR
1.73
−0.45
0.00
0.00
0.00
MSCI EM
Weight
0.00
0.00
0.00
0.00
0.00
MES
0.00
0.00
0.00
0.00
0.00
MPR
0.00
0.00
0.00
0.00
0.00
CBOT Treasury
Weight
0.00
48.03
0.00
16.62
0.00
MES
0.00
75.01
0.00
25.73
0.00
MPR
0.00
69.75
0.00
23.39
0.00
German REX
Weight
87.87
42.83
55.07
72.49
85.73
MES
87.47
28.09
3.73
67.97
54.50
MPR
87.87
30.21
6.93
66.94
54.97
UK Gilts
Weight
0.96
0.00
0.00
0.00
0.00
MES
1.12
0.00
0.00
0.00
0.00
MPR
0.96
0.00
0.00
0.00
0.00
Gold
Weight
0.21
1.62
44.93
4.49
14.27
MES
−0.01
1.83
96.27
9.97
45.50
MPR
0.21
2.76
93.07
11.76
45.03
Overall
ES 95 %
1.31
1.82
4.36
1.53
1.98
DivRatio
1.86
1.63
1.17
1.67
1.34

RISK-OPTIMAL PORTFOLIOS
247
to the asset allocation: ﬁrst, the allotment to government bonds is now roughly
equally split between US Treasuries and German Bunds; and second, the equity
exposure is characterized by negative marginal contributions to the ES and the overall
portfolio risk. Hence, from a risk perspective this outcome is more akin to a general
understanding of risk diversiﬁcation, even though the diversiﬁcation ratio is less than
that of the GMV and the ES is half a percentage point greater.
The solutions for the average draw-down and minimum conditional draw-down at
risk constrained portfolios imply an asset allocation in the REX and gold. However,
these two solutions are quite distinct from each other. Whereas for the average
draw-down portfolio the capital is shared almost equally between the two assets,
resulting in a high marginal contribution of gold to the portfolio’s risk, for the
minimum conditional draw-down portfolio the pattern is reversed. Here, the lion’s
share of capital would be invested in German Bunds and the implied marginal risk
contributions are almost equal.
The asset allocation for the constrained conditional draw-down at risk portfolio
yields an intermediate solution between the maximum draw-down constrained and
the other draw-down portfolios discussed above. Its equity exposure is limited to
the Russell 3000 only, which contributes negatively to the ES and the portfolio’s
risk, on the margin. It is further characterized by bond exposure to US Treasuries
and German Bunds, where the latter constitute almost three quarters of the capital
invested. Compared to the GMV allocation, the risk contributions are more evenly
spread between only four investments. The ES and the diversiﬁcation ratio is pretty
close to the corresponding ﬁgures for the GMV portfolio.
12.6.3
Back-test comparison for stock portfolio
In this ﬁnal example the results of a conditional draw-down at risk strategy are
compared to the outcome of a minimum-variance allocation. The portfolio solutions
consist of the constituents of the Euro Stoxx 50 index. The purpose of this example
is to show that a global minimum-variance allocation is not a shelter from losses per
se. The ﬁrst section of the code is provided in Listing 12.5.
First, the necessary packages are brought into the work space. The package
FRAPO will be employed for the data set used (EuroStoxx50) and to conduct
the CDaR optimizations. The data set is also used in Cesarone et al. (2011). It is
included in the package as a data frame with 265 weekly observations of 48 members
of the index. The sample starts on 3 March 2003 and ends on 24 March 2008. The
solutions of the global minimum-variance portfolios are determined by means of the
functions contained in the package fPortfolio, and the analysis of the simulations
is carried out using the package PerformanceAnalytics. After the packages have
been loaded, the data set is brought into memory and converted to a timeSeries
object, and the the discrete returns of the stock prices are assigned to the object RDP.
The back-test is then carried out in the form of a recursive window with an initial
length of 208 observations which coincides with a span of 4 years. Lines 10–22
deﬁne the parameters and portfolio speciﬁcations employed. The CDaR portfolio
will be optimized for a conditional draw-down as high as 10% for a conﬁdence level

248
PORTFOLIO OPTIMIZATION APPROACHES
Listing 12.5 Back-test: GMV versus CDaR portfolio optimization.
library(FRAPO)
1
library(fPortfolio)
2
Library(PerformanceAnalytics)
3
## Loading of data set
4
data(EuroStoxx50)
5
## Creating timeSeries of prices and returns
6
pr <−timeSeries(EuroStoxx50, charvec = rownames(EuroStoxx50))
7
NAssets <−ncol(pr)
8
RDP <−na.omit((pr / lag(pr, k = 1) −1) ∗100)
9
## Back−test of GMV vs. CDaR
10
## Start and end dates
11
to <−time(RDP)[208:nrow(RDP)]
12
from <−rep(start(RDP), length(to))
13
## Portfolio speciﬁcations
14
## CDaR portfolio
15
DDbound <−0.10
16
DDalpha <−0.95
17
## GMV portfolio
18
mvspec <−portfolioSpec()
19
BoxC <−c("minsumW[1:NAssets] = 0.0", "maxsumW[1:NAssets] = 1.0")
20
## Initialising weight matrices
21
wMV <−wCD <−matrix(NA, ncol = ncol(RDP), nrow = length(to))
22
## Conducting back−test
23
for(i in 1:length(to)){
24
series <−window(RDP, start = from[i], end = to[i])
25
prices <−window(pr, start = from[i], end = to[i])
26
mv <−minvariancePortfolio(data = series,
27
spec = mvspec,
28
constraints = BoxC)
29
cd <−PCDaR(prices, alpha = DDalpha, bound = DDbound,
30
softBudget = TRUE)
31
wMV[i, ] <−c(getWeights(mv))
32
wCD[i, ] <−Weights(cd)
33
}
34
## Lagging optimal weights and sub−sample of returns
35
wMV <−rbind(rep(NA, ncol(RDP)), wMV[−nrow(wMV), ])
36
wMVL1 <−timeSeries(wMV, charvec = to)
37
colnames(wMVL1) <−colnames(RDP)
38
wCD <−rbind(rep(NA, ncol(RDP)), wCD[−nrow(wCD), ])
39
wCDL1 <−timeSeries(wCD, charvec = to)
40
colnames(wCDL1) <−colnames(RDP)
41
RDPback <−RDP[to,]
42
colnames(RDPback) <−colnames(RDP)
43

RISK-OPTIMAL PORTFOLIOS
249
of 95%. The GMV portfolio is deﬁned with a long-only and soft-budget constraint,
such that the sum of weights is in the interval [0, 1]. This restriction is created
as a two-element character vector which deﬁnes the lower and upper bounds. In
the last line of this chunk of code the matrix objects wMV and wCD are created
so that the necessary memory is reserved in advance. The simulation itself is then
carried out with the ensuing for loop. Within this closure, the subsamples of the
returns and prices are extracted and each data set is then passed into the calls of
minvariancePortfolio() for the GMV and PCDaR() for the CDaR solutions,
respectively. The optimal portfolio weights are then assigned to the ith row of the
matrix objects wMV and wCD whereby the relevant methods for extracting the weight
vectors have been utilized. Next the weights are lagged by one period, such that the
portfolio equities can be computed by direct multiplication with the returns for the
back-test period.
In the ﬁrst lines of Listing 12.6, the return factors for each strategy are assigned
to the objects MVRetFac and CDRetFac, where an initial wealth of 100 monetary
units has been assumed. The trajectory of the portfolios’ equity is then given as the
cumulative product of the return factors. These equity values are stored as objects
MVPort for the GMV strategy and CDPort for the CDaR solution. The R code for
the evaluation of the two simulations is shown in the bottom part of Listing 12.6 and
in Listing 12.7. First, the wealth trajectories are depicted in Figure 12.9.
The hump-shaped pattern during the ﬁnancial crises is eye-catching and much
more pronounced for the GMV than for the CDaR strategy. The ﬁnal wealth of the
GMV portfolio is slightly greater than that of the CDaR allocation. It is noteworthy
that during the back-test period the GMV strategy yielded a fully invested solution,
whereas according to the CDaR strategy not all capital was allocated to stocks at each
rebalancing date. In particular, during the heyday of the ﬁnancial crisis in 2007 and
2008 only between roughly 55% and 85% would have been allotted to stocks. This
also partly explains the rather shallow wealth trajectory at the end of the back-test
period. Accrued interest from the wealth not invested in stocks was not considered.
Index values
2007−02−26
2007−08−01
2008−01−05
100
110
120
130
CDaR
GMV
Figure 12.9
Comparison of wealth trajectories.

250
PORTFOLIO OPTIMIZATION APPROACHES
Listing 12.6 Back-test: evaluation of results, part I.
## Portfolio equities of strategies
1
MVRetFac <−1 + rowSums(wMVL1 ∗RDPback) / 100
2
MVRetFac[1] <−100
3
MVPort <−timeSeries(cumprod(MVRetFac), charvec = names(MVRetFac))
4
CDRetFac <−1 + rowSums(wCDL1 ∗RDPback) / 100
5
CDRetFac[1] <−100
6
CDPort <−timeSeries(cumprod(CDRetFac), charvec = names(CDRetFac))
7
## Progression of wealth
8
ylims <−range(cbind(MVPort, CDPort))
9
plot(CDPort, main = " ", ylim = ylims, ylab = "Index values",
10
xlab = " ")
11
lines(MVPort, col = "darkgrey")
12
legend("topleft", legend = c("CDaR", "GMV"),
13
col = c("black", "darkgrey"),
14
lty = 1)
15
## Portfolio returns
16
MVRet <−returns(MVPort, method = "discrete", percentage = FALSE,
17
trim = TRUE)
18
CDRet <−returns(CDPort, method = "discrete", percentage = FALSE,
19
trim = TRUE)
20
## Draw−down table
21
table.Drawdowns(MVRet)
22
table.Drawdowns(CDRet)
23
## Plot of draw−down curves
24
MVD <−100 ∗Drawdowns(MVRet)
25
CDD <−100 ∗Drawdowns(CDRet)
26
plot(CDD, main = " ", ylab = "Percentages", xlab = " ",
27
ylim = c(min(c(MVD, CDD)), 0))
28
lines(MVD, col = "darkgrey")
29
abline(h = 0, col = "lightgrey")
30
abline(h = −10, col = "lightgrey", lty = 2)
31
legend("bottomleft", legend = c("CDaR", "GMV"),
32
col = c("black", "darkgrey"), lty = 1)
33
In code lines 16–20 of Listing 12.6 the portfolio returns for each strategy are
assigned to the objects MVRet and CDRet, respectively. These objects are utilized
in the computations of characteristic portfolio statistics. The ﬁrst of these is an
overview of the ﬁve greatest draw-downs within each strategy (see the results in
Table 12.2). As is evident from this table, the portfolio draw-downs are all larger
for the GMV strategy than for the CDaR allocation. The conditional draw-down
level for the CDaR strategy has been violated in one instance, albeit marginally. The

RISK-OPTIMAL PORTFOLIOS
251
Listing 12.7 Back-test: evaluation of results, part II.
## Portfolio statistics
1
## VaR
2
MVVAR <−−100 ∗VaR(MVRet, p = 0.95, method = "gaussian")
3
CDVAR <−−100 ∗VaR(CDRet, p = 0.95, method = "gaussian")
4
## ES
5
MVES <−−100 ∗ES(MVRet, p = 0.95, method = "gaussian")
6
CDES <−−100 ∗ES(CDRet, p = 0.95, method = "gaussian")
7
## Sharpe
8
MVSR <−SharpeRatio(MVRet)
9
CDSR <−SharpeRatio(CDRet)
10
## Annualised returns
11
MVRA <−Return.annualized(MVRet, scale = 52)
12
CDRA <−Return.annualized(CDRet, scale = 52)
13
## Draw−downs
14
MVDD <−−100 ∗ﬁndDrawdowns(MVRet)$return
15
MVDD <−MVDD[MVDD!=0.0]
16
length(MVDD)
17
summary(MVDD)
18
CDDD <−−100 ∗ﬁndDrawdowns(CDRet)$return
19
CDDD <−CDDD[CDDD!=0.0]
20
length(CDDD)
21
summary(CDDD)
22
Table 12.2
Overview of draw-downs (positive, percentages).
Portfolio
From
Trough
To
Depth
→
↘
↗
GMV
1
2007-12-10
2008-03-17
NA
20.11
17
15
2
2007-06-04
2007-08-13
2007-10-08
9.75
19
11
8
3
2007-10-15
2007-11-05
2007-11-26
3.34
7
4
3
4
2007-03-12
2007-03-12
2007-03-19
2.30
2
1
1
5
2007-04-23
2007-04-23
2007-04-30
0.76
2
1
1
CDaR
1
2007-11-12
2008-01-21
NA
11.53
21
11
2
2007-06-04
2007-09-03
2007-10-08
5.58
19
14
5
3
2007-05-07
2007-05-07
2007-05-14
0.51
2
1
1
4
2007-03-12
2007-03-12
2007-03-19
0.49
2
1
1
5
2007-10-22
2007-10-29
2007-11-05
0.30
3
2
1

252
PORTFOLIO OPTIMIZATION APPROACHES
Percentages
2007−03−05
2007−08−06
2008−01−07
−20
−15
−10
−5
0
CDaR
GMV
Figure 12.10
Comparison of draw-down trajectories.
draw-downs are displayed in lines 24–33, now expressed as negative percentages (see
Figure 12.10).
In Listing 12.7 some key portfolio performance statistics are computed. A sum-
mary of these is provided in Table 12.3. In retrospect, the Gaussian tail risks VaR
and ES for the GMV portfolio are both greater than the respective numbers for the
CDaR strategy at the 95% level. Because the higher riskiness of the former strategy
is roughly balanced against the more favourable annualized return for the GMV, the
portfolios’ Sharpe ratios are almost equal. The summary statistics for the portfolio
Table 12.3
Performance statistics.
Statistics
GMV
CDaR
Risk/return
VaR 95%
5.16
3.05
VaR 95%
6.54
3.86
Sharpe ratio
0.07
0.07
Return annualized %
10.20
6.62
Draw-down
Count
6
7
Minimum
0.02
0.00
1st Quartile
1.14
0.30
Median
2.82
0.49
Mean
6.05
2.67
3rd Quartile
8.15
3.05
Maximum
20.11
11.53

RISK-OPTIMAL PORTFOLIOS
253
draw-downs all indicate a less risky allocation of the CDaR strategy, except for the
number of draw-downs. As stated at the beginning of this section, the purpose of this
example is to remind the reader that a GMV optimization is not a panacea against
large losses per se, and the often encountered corner solutions of such a strategy can
turn into the opposite of what was initially intended.
References
Alexander G. and Baptista A. 1999 Value at risk and mean-variance analysis. Working Paper
9804, University of Minnesota, Minneapolis.
Alexander G. and Baptista A. 2002 Economic implications of using a mean-VaR model
for portfolio selection: A comparison with mean-variance analysis. Journal of Economic
Dynamics & Control 26, 1159–1193.
Alexander G. and Baptista A. 2004 A comparison of VaR and CVaR constraints on portfolio
selection with the mean-variance model. Management Science 50(9), 1261–1273.
Alexander G. and Baptista A. 2008 Active portfolio management with benchmarking: Adding
a value-at-risk constraint. Journal of Economic Dynamics & Control 32, 779–820.
Berkelaar M. 2011 lpSolve: Interface to Lp_solve v. 5.5 to solve linear/integer programs.
R package version 5.6.6.
Boudt K., Peterson B. and Croux C. 2008 Estimation and decomposition of downside risk for
portfolios with non-normal returns. Journal of Risk 11(2), 79–103.
Carl P., Peterson B., Boudt K. and Zivot E. 2012 PerformanceAnalytics: Econometric tools for
performance and risk analysis. R package version 1.0.4.4.
Cesarone F., Scozzari A. and Tardella F. 2011 Portfolio selection problems in practice: a
comparison between linear and quadratic optimization models. Quantitative Finance Papers
1105.3594, arXiv.org.
Chekhlov A., Uryasev S. and Zabarankin M. 2000 Portfolio optimization with drawdown
constraints. Research report 2000-5, Department of Industrial and Systems Engineering,
University of Florida, Gainesville.
Chekhlov A., Uryasev S. and Zabarankin M. 2004 Portfolio optimization with drawdown con-
straints In Supply Chain and Finance (ed. Pardalos P., Migdalas A. and Baourakis G.)
vol. 2 of Series on Computers and Operations Research World Scientiﬁc Singapore
pp. 209–228.
Chekhlov A., Uryasev S. and Zabarankin M. 2005 Drawdown measure in portfolio optimiza-
tion. International Journal of Theoretical and Applied Finance 8(1), 13–58.
Corrada Bravo H., Theussl S. and Hornik K. 2011 Rcplex: R interface to CPLEX. R package
version 0.3-0.
Czyzyk J., Mesnier M. and Mor´e J. 1998 The NEOS server. IEEE Journal on Computational
Science and Engineering 5, 68–75.
De Giorgi E. 2002 A note on portfolio selection under various risk measures. Working Paper
9, National Centre of Competence in Research, Financial Valuation and Risk Management,
Zurich.
Dolan E. 2001 The neos server 4.0 administrative guide. Technical memorandum anl/mcs-tm-
250, Mathematics and Computer Science Division, Argonne National Laboratory.

254
PORTFOLIO OPTIMIZATION APPROACHES
Durand R., Gould J. and Miller R. 2011 On the performance of the minimum var portfolio.
European Journal of Finance 17(7), 553–576.
Friberg H. 2012 Rmosek: The R-to-MOSEK Optimization Interface. R package version 1.2.2.
Gelius-Dietrich G. 2012 glpkAPI: R Interface to C API of GLPK. R package version 1.2.1.
Gropp W. and Mor´e J. 1997 Optimization environments and the NEOS server In Approximation
Theory and Optimization (ed. Buhmann MD. and Iserles A.) Cambridge University Press
Cambridge pp. 167–182.
Harter R., Hornik K. and Theussl S. 2012 Rsymphony: Symphony in R. R package version
0.1-14.
Henningsen A. 2010 linprog: Linear Programming/Optimization. R package version 0.9-0.
Konis K. 2011 lpSolveAPI: R Interface for lp_solve version 5.5.2.0. R package version 5.5.2.0-
5.
Krokhmal P., Uryasev S. and Zrazhevsky G. 2002 Risk management for hedge fund portfolios.
Journal of Alternative Investments 5(1), 10–29.
Merton R. 1972 An analytical derivation of the efﬁcient portfolio frontier. Journal of Financial
and Quantitative Analysis 7, 1851–1872.
Pfaff B. 2011 rneos: XML-RPC Interface to NEOS. R package version 0.2-6.
Rockafellar R. and Uryasev S. 2000 Optimization of conditional value-at-risk. Journal of Risk
2(3), 21–41.
Rockafellar R. and Uryasev S. 2001 Conditional value-at-risk for general loss distributions.
Research Report 2001-5, Department of Industrial and Systems Engineering, University of
Florida, Gainesville, FL.
Soetaert K., Van den Meersche K. and van Oevelen D. 2009 limSolve: Solving Linear Inverse
Models. R package 1.5.1.
Theussl S. and Hornik K. 2012 Rglpk: R/GNU Linear Programming Kit Interface. R package
version 0.3-8.
Uryasev S. 2005 Conditional value-at-risk (CVaR): Algorithms and applications. Technical
report, Universita degli Studi di Milano, Milan. http://bit.ly/OrIgZn.
Uryasev S. and Rockafellar T. 1999 Optimization of conditional value-at-risk. Research Report
99-4, Department of Industrial and Systems Engineering, University of Florida, Gainesville,
FL.
W¨urtz D., Chalabi Y., Chen W. and Ellis A. 2010 Portfolio Optimization with R/Rmetrics.
Rmetrics Association & Finance Online, www.rmetrics.org. R package version 2130.80.

13
Tactical asset allocation
13.1
Overview
The previous chapters were concerned with the modelling of portfolio risk and how
this can be directly incorporated in the process of portfolio optimization. The focus
now shifts to the other side of the return/risk coin. The following sections will ﬁrst
outline how directional and relative forecasts can be obtained and then how portfolio
allocations can be derived from a set of quantitatively derived trades/signals. Of
course, the techniques portrayed are not conﬁned to tactical asset allocation (TAA),
but can also be applied to wealth allocations other than tactically shifting assets
in and out of a portfolio. The domain of TAA is rather wide. Trading strategies
can be derived from purely technical indicators, by subjective reasoning and from
statistical/econometric models. The ﬁrst and second category will not be covered
in this chapter, rather the focus will be on the description of selected time series
methods for deriving forecasts of asset prices. Incidentally, even though technical
analysis will not be covered, the reader is referred to the CRAN packages fTrading
(W¨urtz 2009a) and TTR (Ulrich 2012) in which numerous technical indicators are
implemented. There are also quite a few different ways of deriving TAA allocations,
Black–Litterman model probably being most widely known and almost synonymous
with TAA-driven portfolio allocations. However, TAA can also be combined with
risk-overlay and/or high-watermark strategies.
Given the quite huge variety of applications and combinations of TAA, it is quite
surprising that the literature directly related to TAA is rather sparse. To the author’s
knowledge, the monograph of Lee (2000) is the only book devoted entirely to the
theory and practice of TAA. Also the number of articles explicitly covering TAA is
rather small. Although this chapter can by no means completely ﬁll this gap, the aim
is to provide as thorough an account of the topic as space permits.
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

256
PORTFOLIO OPTIMIZATION APPROACHES
13.2
Survey of selected time series models
13.2.1
Univariate time series models
AR( p) time series process
We start by considering a simple ﬁrst-order autoregressive process. The value of y
at the current time t is explained by its value at time t −1, a constant c, and a white
noise error process {εt}:
yt = c + φyt−1 + εt.
(13.1)
Basically, equation (13.1) is a ﬁrst-order inhomogeneous difference equation. The
path of this process depends on the value of φ. If |φ| ≥1, then shocks accumulate
over time and the process is non-stationary. Incidentally, if |φ| > 1 the process grows
without bounds, and if |φ| = 1 it has a unit root. The latter will be discussed in more
detail in the following subsection on multivariate time series modelling. For now,
only the covariance-stationary case, |φ| < 1, is considered. With the lag operator L,
equation (13.1) can be rewritten as
(1 −φL)yt = c + εt.
(13.2)
The stable solution to this process is given by an inﬁnite sum of past errors with
decaying weights:
yt = (c + εt) + φ(c + εt−1) + φ2(c + εt−2) + φ3(c + εt−3) + . . .
(13.3a)
=

c
1 −φ

+ εt + φεt−1 + φ2εt−2 + φ3εt−3 + . . . .
(13.3b)
The expected value and the second-order moments of the AR(1) process in equation
(13.1) are given by
μ = E[yt] =
c
1 −φ ,
(13.4a)
γ0 = E[(yt −μ)2] =
σ 2
1 −φ2 ,
(13.4b)
γ j = E[(yt −μ)(yt−j −μ)] =

φ j
1 −φ2

σ 2.
(13.4c)
From equation (13.4c), the geometrically decaying pattern of the autocovariances is
evident.
The AR(1) process can be generalized to an AR(p) process:
yt = c + φ1yt−1 + φ2yt−2 + . . . + φpyt−p + εt.
(13.5)
As with equation (13.1), equation (13.5) can be rewritten as
(1 −φ1L −φ2L2 −. . . −φpLp)yt = c + εt.
(13.6)

TACTICAL ASSET ALLOCATION
257
It can be shown that such an AR(p) process is stationary if all roots z0 of the
polynomial
φp(z) = 1 −φ1z −φ2z2 −. . . −φpz p
(13.7)
have a modulus greater than one. The modulus of a complex number z = z1 + iz2 is
deﬁned as |z| =

z2
1 + z2
2. Viewing the stationarity condition from that angle, it turns
out that in the case of an AR(1) process, as in equation (13.1), |φ| < 1 is required
because the only solution to 1 −φz = 0 is given for z = 1/φ and |z| = |1/φ| > 1
when |φ| < 1.
If the error process {εt} is normally distributed, equation (13.5) can be consis-
tently estimated by the ordinary least-squares (OLS) method. Furthermore, the OLS
estimator for the unknown coefﬁcient vector β = (c, φ)′ is asymptotically normally
distributed. Alternatively, the model parameters can be estimated by the principle of
maximum likelihood. However, one problem arises in the context of AR(p) models
and this holds true for the more general class of ARMA(p, q) models discussed later.
For i.i.d. random variables with probability density function f (yt; θ) for t = 1, . . . , T
and parameter vector θ, the joint density function is the product of the marginal
densities:
f (y; θ) = f (y1, . . . , yT ; θ) =
T
t=1
f (yt; θ).
(13.8)
This joint density function can, in line with the ML principle, be interpreted as a
function of the parameters θ given the data vector y; that is, the likelihood function
is given by
L(θ|y) = L(θ|y1, . . . , yT ) =
T
t=1
f (yt; θ).
(13.9)
The log-likelihood function then has the simple form
ln L(θ|y) =
T

t=1
ln f (yt; θ).
(13.10)
Because our model assumes that the time series {yt} has been generated from
a covariance-stationary process, the i.i.d. assumption is violated and hence the log-
likelihood cannot be derived as swiftly as in equations (13.8)–(13.10). That is, yt
is modelled as a function of its own history, and therefore yt is not independent of
yt−1, . . . , yt−p given that {εt} is normally distributed with expectation μ = 0 and
variance σ 2. In order to apply the ML principle, one therefore has two options: either
estimate the full-information likelihood function or derive the likelihood function
from a conditional marginal factorization. The derivation of the log-likelihood for
both options is provided, for instance, in Hamilton (1994). Here, we will focus on
the second option. The idea is that the joint density function can be factored as the

258
PORTFOLIO OPTIMIZATION APPROACHES
product of the conditional density function given all past information and the joint
density function of the initial values:
f (yT , . . . , y1; θ) =
⎛
⎝
T
t=p+1
f (yt|It−1, θ)
⎞
⎠· f (yp, . . . , y1; θ)
(13.11)
where It−1 denotes the information available at time t. This joint density function
can then be interpreted as the likelihood function with respect to the parameter vector
θ given the sample y, and therefore the log-likelihood is given by
ln L(θ|y) =
T

t=p+1
ln f (yt|It−1, θ) + ln f (yp, . . . , y1; θ),
(13.12)
The log-likelihood consists of two terms. The ﬁrst term is the conditional log-
likelihood and the second term the marginal log-likelihood for the initial values.
Whether one maximizes the exact log-likelihood as in equation (13.12) or only
the conditional log-likelihood, that is, the ﬁrst term of the exact log-likelihood, is
asymptotically equivalent. Both are consistent estimators and have the same limiting
normal distribution. Bear in mind that in small samples the two estimators might
differ by a non-negligible amount, in particular if the roots are close to unity. Because
a closed-form solution does not exist, numerical optimization methods are used to
derive optimal parameter values.
MA(q) time series process
It was shown above that a ﬁnite stable AR(p) process can be inverted to a moving
average of current and past shocks. It is now considered how a process can be
modelled as a ﬁnite moving average of its shocks. Such a process is called MA(q),
where the parameter q refers to the highest lag of shocks included in such a process.
An MA(1) process is given by
yt = μ + εt + θεt−1,
(13.13)
where {εt} is a white-noise process and μ, θ can be any constants. This process has
moments
μ = E[yt] = E[μ + εt + θεt−1],
(13.14a)
γ0 = E[(yt −μ)2] = (1 + θ2)σ 2,
(13.14b)
γ1 = E[(yt −μ)(yt−1 −μ)] = θσ 2.
(13.14c)
The higher autocovariances γ j, j > 1, are zero. Neither the mean nor the autoco-
variance are functions of time, hence an MA(1) process is covariance-stationary for
all values of θ. Incidentally, this process also has the characteristic of ergodicity.
The MA(1) process can be extended to the general class of MA(q) processes:
yt = μ + εt + θ1εt−1 + . . . + θqεt−q.
(13.15)

TACTICAL ASSET ALLOCATION
259
With the lag operator L, this process can be rewritten as
yt −μ = εt + θ1εt−1 + . . . θqεt−q
(13.16a)
= (1 + θ1L + . . . + θqLq)εt = θq(L)εt.
(13.16b)
Much as a stable AR(p) process can be rewritten as an inﬁnite MA process, an
MA(q) process can be transformed into an inﬁnite AR process as long as the roots of
the characteristic polynomial, the z-transform, have modulus greater than 1, that is,
are outside the unit circle:
θqz = 1 + θ1z + . . . + θqzq.
(13.17)
The expected value of an MA(q) process is μ and hence invariant with respect to
its order. The second-order moments are
γ0 = E[(yt −μ)2] = (1 + θ2
1 + . . . + θ2
q )σ 2,
(13.18a)
γ j = E[(εt + θ1εt−1 + . . . + θqεt−q)
× (εt−q + θ1εt−j−1 + . . . + θqεt−j−q)].
(13.18b)
Because the {εt} are uncorrelated with each other by assumption, equation (13.18b)
can be simpliﬁed to
γ j =
(1 + θ j+1θ1 + θ j+2θ2 + . . . + θqθq−j)σ 2
for j = 1, 2, . . . , q
0
for j > q.
(13.19)
That is, empirically an MA(q) process can be detected by its ﬁrst q signiﬁcant auto-
correlations and a slowly decaying or alternating pattern of its partial autocorrelations.
For large sample sizes T , a 95% signiﬁcance band can be calculated as

ϱ j −
2
√
T
, ϱ j +
2
√
T

,
(13.20)
where ϱ j refers to the jth-order autocorrelation.
It has been stated above that a ﬁnite AR process can be inverted to an inﬁnite
MA process. Before we proceed further, let us ﬁrst examine the stability condition of
such an MA(∞) process,
yt = μ +
∞

j=0
ψ jεt−j.
(13.21)
The coefﬁcients for an inﬁnite process are denoted by ψ instead of θ. It can be shown
that such an inﬁnite process is covariance-stationary if the coefﬁcient sequence {ψ j}
is either square summable,
∞

j=0
ψ2
j < ∞,
(13.22)
or absolutely summable,
∞

J=0
|ψ j| < ∞,
(13.23)

260
PORTFOLIO OPTIMIZATION APPROACHES
where absolute summability is sufﬁcient for square summability – that is, the former
implies the latter, but not vice versa.
ARMA( p, q) time series process
It has been shown in the foregoing how a time series can be explained either by
its history or by current and past shocks. Furthermore, the moments of these data-
generating processes have been derived and the mutual convertibility of these model
classes has been stated for parameter sets that fulﬁl the stability condition. These two
time series processes are now put together and a more general class of ARMA(p, q)
processes is investigated.
In practice, it is often cumbersome to detect a pure AR(p) or MA(q) process by
the behaviour of its empirical ACF and PACF because neither one tapers off with
increasing lag order. In these instances, the time series might have been generated by
a mixed autoregressive moving average process.
For a stationary time series {yt}, such a mixed process is deﬁned as
yt = c + φ1yt−1 + . . . + φpyt−p + εt + θ1εt−1 + . . . + θqεt−q.
(13.24)
By assumption, {yt} is stationary, that is, the roots of the characteristic polynomial
lie outside the unit circle. Hence, with the lag operator, equation (13.24) can be
transformed to:
yt =
c
1 −φ1L −. . . −φpL p + 1 + θ1L + . . . + θqLq
1 −φ1L −. . . −φpLp εt
(13.25a)
= μ + ψ(L)εt.
(13.25b)
The condition of absolute summability for the lag coefﬁcients {ψ j} must hold. Put
differently, the stationarity condition depends only on the AR parameters and not on
the MA ones.
We now brieﬂy touch on the Box–Jenkins approach to time series modelling
(see Box and Jenkins 1976). This approach consists of three stages: identiﬁcation,
estimation, and diagnostic checking. As a ﬁrst step, the series is visually inspected
for stationarity. If an investigator has doubts that this condition is met, he/she has
to suitably transform the series before proceeding. Such transformations could in-
volve the removal of a deterministic trend or taking ﬁrst differences with respect to
time. Furthermore, variance instability such as higher ﬂuctuations as time proceeds
can be coped with by using the logarithmic values of the series instead. By inspect-
ing the empirical ACF and PACF, a tentative ARMA(p, q) model is speciﬁed. The
next stage is the estimation of a preliminary model. The ML principle allows one
to discriminate between different model speciﬁcations by calculating information
criteria and/or applying likelihood-ratio tests. Thus, one has a second set of tools
to determine an appropriate lag order for ARMA(p, q) models compared with the
order decision that is derived from ACF and PACF. Speciﬁcally, the Akaike, Schwarz
and/or Hannan–Quinn information criteria can be utilized in the determination of an
appropriate model structure (see Akaike 1981; Hannan and Quinn 1979; Quinn 1980;

TACTICAL ASSET ALLOCATION
261
Schwarz 1978):
AIC(p, q) = ln(ˆσ 2) + 2(p + q)
T
,
(13.26a)
BIC(p, q) = ln(ˆσ 2) + ln T (p + q)
T
,
(13.26b)
HQ(p, q) = ln(ˆσ 2) + ln(ln(T ))(p + q)
T
,
(13.26c)
where ˆσ 2 denotes the estimated variance of an ARMA(p, q) process. The lag order
(p, q) that minimizes the information criteria is then selected. As an alternative, a
likelihood-ratio test can be computed for an unrestricted and a restricted model. The
test statistic is deﬁned as:
2[L(ˆθ) −L(˜θ)] ∼χ2(m)
(13.27)
where L(ˆθ) denotes the estimate of the unrestricted log-likelihood and L(˜θ) that of
the restricted log-likelihood. This test statistic is distributed as χ2 with m degrees of
freedom, which corresponds to the number of restrictions. Next, one should check
the model’s stability as well as the signiﬁcance of its parameters. If one of these tests
fails, the econometrician has to start again by specifying a more parsimonious model
with respect to the ARMA order. In the last step, diagnostic checking, one should
then examine the residuals for lack of correlation and for normality and conduct
tests for correctness of the model order, (i.e., over- and underﬁtting). Incidentally,
by calculating pseudo ex ante forecasts, the model’s suitability for prediction can be
examined.
Once a stable (in the sense of covariance-stationary) ARMA(p, q) model has
been estimated, it can be used to predict future values of yt. These forecasts can be
computed recursively from the linear predictor:
yT (h) = φ1 ¯yT +h−1 + . . . + φp ¯yT +h−p
+εt + θ1εt−T −1 + . . . + θqεt−T −q + . . . ,
(13.28)
where ¯yt = yt for t ≤T and ¯yT + j = yT ( j) for j = 1, . . . , h −1. Using the Wold rep-
resentation of a covariance-stationary ARMA(p, q) process (see equations (13.25a)
and (13.25b), this predictor is equivalent to
yT (h) = μ + ψhεt + ψh+1εt−1 + ψh+2εt−2 + . . . .
(13.29)
It can be shown that this predictor is minimal with respect to the mean squared
error criterion based on the information set It – see, for instance, Judge et al. (1985,
Chapter 7) and Hamilton (1994, Chapter 4). Incidentally, if the forecast horizon h is
greater than the moving average order q, the forecasts are determined solely by the
autoregressive terms in equation (13.28).
If {εt} is assumed to be standard normally distributed, then it follows that the
h-steps-ahead forecast is distributed as
yt+h|It ∼N

yt+h|t, σ 2 
1 + ψ2
1 + . . . + ψ2
h−1

(13.30)

262
PORTFOLIO OPTIMIZATION APPROACHES
where the ψi, i = 1, . . . , h −1, denote the coefﬁcients from the Wold representation
of a covariance-stationary ARMA(p, q) process. The 95% forecast conﬁdence band
can then be computed as
yt+h|It ± 1.96 ·

σ 2 
1 + ψ2
1 + . . . + ψ2
h−1

.
(13.31)
13.2.2
Multivariate time series models
Structural multiple equation models
It almost goes without saying that today’s capital markets are highly interdependent.
This interdependence occurs across countries and/or across assets. Structural multiple
equation models allow the modelling of explicit interdependencies as observed in
ﬁnancial markets. In addition, exogenous variables (e.g., macroeconomic data) can
be included in this model type. Textbook accounts of this model class are included,
for example, in Judge et al. (1985, 1988) and Greene (2008). Structural multiple
equation models (SMEMs) can be utilized for forecasting, scenario analysis and risk
assessment as well as for multiplier analysis, and they are therefore ideally suited
for tactical asset allocation. The origins of the SMEM can be traced back to the
1940s and 1950s, when this model type was proposed by the Cowles Foundation. At
that time the SMEM was primarily applied to large-scale macroeconomic modelling.
Prominent examples of this application were the Klein model for the US economy
and the models run by most central banks and other organizations and international
institutions, such as the Deutsche Bundesbank. However, this type of model can also
be applied to ﬁnancial markets and forecasts for a set of assets can be obtained for a
TAA as shown in Pfaff (2007).
The structural form of a multiple equation model is
Ayt + Bzt = ut,
for t = 1, . . . , T,
(13.32)
where A is the (N × N) coefﬁcient matrix of the endogenous variables, yt is the
(N × 1) vector of the endogenous variables, B is the (N × K) coefﬁcient matrix of
the predetermined variables, zt is the (K × 1) vector of the predetermined variables
(i.e., exogenous and lagged endogenous variables) and ut is the (N × 1) vector of
white noise disturbances. If applicable, one can simplify the model’s structure by
concentrating out the irrelevant endogenous variables. This yields the revised form
of the model. The reduced form of the model can be obtained from the revised form
if it is expressed explicitly in terms of the endogenous variables:
yt = −A−1Bzt + A−1ut,
for t = 1, . . . , T.
(13.33)
If the reduced form is solved in terms of starting values for the endogenous and
exogenous variables, one obtains the ﬁnal form. This representation of the model can
be utilized for stability and multiplicand analysis. An SMEM can be characterized
as either a recursive or an interdependent model and as either dynamic or static. If
the matrix A can be rearranged so that it is either upper or lower diagonal, then
the multiple equation model is recursive, otherwise interdependencies between the
endogenous variables exist. This is equivalent to det(A) = n
i=1 ai,i. If the matrix Z

TACTICAL ASSET ALLOCATION
263
t + 1
t
t −1
y1
y2
y3
x
Figure 13.1
Tinbergen’s arrow diagram.
does not contain lagged endogenous variables, the model is said to be static, otherwise
it is dynamic. Both of the above can be visualized in a Tinbergen arrow diagram. An
example is shown in Figure 13.1.
This example shows a ﬁctitious SMEM structure for three endogenous variables
(y1, y2, y3) and a set of model exogenous variables, x. An interdependent structure
is obtained if the curved arrows indicate a loop on a per-period basis, and a dynamic
quantity
price
Demand
Supply
Supply '
Demand '
Figure 13.2
Identiﬁcation: partial market model.

264
PORTFOLIO OPTIMIZATION APPROACHES
structure is indicated when arrows are drawn from one period to the next for at least
one endogenous variable. As can be seen from this ﬁgure, although there is no direct
interdependency between y1 and y3 a closed loop exists via the indirect links through
y2. It can further be concluded that y2 is dynamically affected by the trajectories of y1
and y3 through the inclusion of lagged endogenous variables in the reaction equations
for these two variables. Thus, the structure of the SMEM in Figure 13.1 is dynamic
and interdependent.
In addition to these model characteristics, the degree of identiﬁcation with re-
spect to the structural form parameters is important. Because ordinarily only the
reduced-form parameters are estimated, it might not be feasible to infer to structural
parameters. This problem is illustrated by a simple partial market model.
Consider a partial market model for a good, with an upward-sloping supply curve
and downward-sloping demand curve (see Figure 13.2). The intersection of the two
lines is a point of equilibrium. We observe only the pairs (pi, qi). The simple partial
market model (grey lines, not identiﬁed) can be written as:
qd = α1 + α2 p,
(13.34a)
qs = β1 + β2 p,
(13.34b)
qs = qd.
(13.34c)
A model in which the demand and supply curve are identiﬁed is obtained by intro-
ducing shift variables – for example, a cost variable for supply and a wealth variable
for demand (black lines) – such that the structural parameters can be determined,
hence a need arises for the inclusion of exogenous variables, in contrast to the vector
autoregressive and vector error correction models discussed later in this subsection,
which are ordinarily expressed solely in terms of past values for the endogenous
variables:
qd = α1 + α2 p + α3w
(13.35a)
qs = β1 + β2 p + β3c
(13.35b)
qs = qd
(13.35c)
An interdependent multiple equation model is identiﬁed if a unique solution for
its structural coefﬁcients exists. A structural equation is said to be identiﬁed if it is not
possible to obtain another structural equation with the same statistical characteristics
by a linear combination of the remaining equations. As a general rule, a structural
equation is identiﬁed if a sufﬁcient number of predetermined variables are excluded
from this equation. Two criteria for determining the degree of identiﬁcation are
the number (necessary) and rank (necessary and sufﬁcient) criteria. If K −denotes
the number of excluded predetermined variables in a particular equation and H +
denotes the number of included endogenous variables in a particular equation, then
according to the number criterion, an equation is not identiﬁed if K −< H + −1, is
just identiﬁed if K −= H + −1 and is overidentiﬁed if K −> H + −1. Consider the

TACTICAL ASSET ALLOCATION
265
matrix P = −A−1B and let PK −, H + be the sub-matrix for K −and H + of a particular
equation. According to the rank criteria, an equation is said to be identiﬁed if
rank(PK −, H +) = H + −1.
(13.36)
The unknown coefﬁcients can be estimated by either a two- or three-stage least
squares method (2SLS or 3SLS) or by full-information maximum likelihood (FIML).
The former two methods are implemented in the package systemﬁt (see Henningsen
and Hamann 2007) and the latter method can be embedded in a call to a numeric
optimization routine, such as optim(). However, in practice, the reaction equations
of a reduced form are estimated by OLS and the model solution for the endogenous
variables can be determined by iterative methods. Iterative methods can be expressed
as y(k) = By(k−1) + c, with k denoting the iteration number. An iterative method
is said to be stationary if neither B nor c depends on k. An iterative method is
deemed to have converged if some measure is smaller than a predeﬁned threshold,
for instance ||y(k) −y(k−1)|| < ϵ, with ϵ being chosen suitably small (e.g., ϵ < 0.001).
An example of such a method is the Gauss–Seidel algorithm which can be applied on
a per-period basis. The Gauss–Seidel algorithm is a stationary iterative method for
linear models of the form Ayt = bt, where A is an (n × n) matrix, yt is an (n × 1)
vector of endogenous variables and bt is an (n × 1) vector of the right-hand-side
expressions. The algorithm is deﬁned as
y(k)
t,i =
⎛
⎝bt,i −
n

j<i
ai, j y(k)
t, j −
n

j>i
y(k−1)
t, j
⎞
⎠/ai,i,
(13.37)
or in matrix notation,
y(k)
t
= (D −L)−1 
Uy(k−1)
t
+ b

,
(13.38)
where D, L,U represent the diagonal, lower- and upper-triangular parts of A. The
latter approach is implemented, for instance, in the freely available Fair–Parke pro-
gram – see http://fairmodel.econ.yale.edu/ and Fair (1984, 1998, 2004)
for more information. The appropriateness of an SMEM should then be evaluated
in terms of its dynamic ex post forecasts. This is the strictest test for evaluating the
overall stability of the model. The ﬁtted values in each period are used as values for
the lagged endogenous variables in the subsequent periods. Hence, forecast errors can
accumulate over time and the forecasts can diverge. Thus, a small root-mean-square
error between the actual and ﬁtted values of the dynamic ex post forecasts is sought.
Vector autoregressive models
Since the critique of Sims (1980), multivariate data analysis in the context of vector
autoregressive models (VAR) has evolved as a standard instrument in econometrics.
Because statistical tests are frequently used in determining interdependencies and
dynamic relationships between variables, this methodology was soon enriched by in-
corporating non-statistical a priori information. VAR models explain the endogenous
variables solely in terms of their own history, apart from deterministic regressors. In

266
PORTFOLIO OPTIMIZATION APPROACHES
contrast, structural vector autoregressive (SVAR) models allow the explicit modelling
of contemporaneous interdependence between the left-hand-side variables. Hence,
these types of models try to bypass the shortcomings of VAR models. At the same
time as Sims challenged the paradigm of multiple structural equation models laid out
by the Cowles Foundation in the 1940s and 1950s, Granger (1981) and Engle and
Granger (1987) gave econometricians a powerful tool for modelling and testing eco-
nomic relationships, namely, the concept of cointegration. Nowadays these branches
of research are uniﬁed in the form of vector error correction models (VECMs) and
structural vector error correction (SVEC) models. A thorough theoretical exposition
of all these models is provided in the monographs of Banerjee et al. (1993), Hamilton
(1994), Hendry (1995), Johansen (1995) and L¨utkepohl (2006).
In its basic form, a VAR consists of a set of K endogenous variables yt =
(y1t, . . . , ykt, . . . , yKt) for k = 1, . . . , K. The VAR(p) process is then deﬁned as1
yt = A1yt−1 + . . . + Apyt−p + ut,
(13.39)
where the Ai are (K × K) coefﬁcient matrices for i = 1, . . . , p and ut is a K-
dimensional process with E(ut) = 0 and time invariant positive deﬁnite covariance
matrix E(utu′
t) = u (white noise).
One important characteristic of a VAR(p) process is its stability. This means
that it generates stationary time series with time-invariant means, variances and
covariance structure, given sufﬁcient starting values. One can check this by evaluating
the characteristic polynomial
det(IK −A1z −. . . −Apz p) ̸= 0,
for |z| ≤1.
(13.40)
If the solution of the above equation has a root for z = 1, then either some or all
variables in the VAR(p) process are integrated of order 1, I(1). It might be the case,
that cointegration between the variables exists. This is better analysed in the context
of a VECM.
In practice, the stability of an empirical VAR(p) process can be analysed by
considering the companion form and calculating the eigenvalues of the coefﬁcient
matrix. A VAR(p) process can be written as a VAR(1) process,
ξ t = Aξ t−1 + vt,
(13.41)
with
ξ t =
⎡
⎢⎣
yt
...
yt−p+1
⎤
⎥⎦,
A =
⎡
⎢⎢⎢⎢⎢⎣
A1
A2
· · ·
Ap−1
Ap
I
0
· · ·
0
0
0
I
· · ·
0
0
...
...
...
...
...
0
0
· · ·
I
0
⎤
⎥⎥⎥⎥⎥⎦
,
vt =
⎡
⎢⎢⎢⎣
ut
0
...
0
⎤
⎥⎥⎥⎦, (13.42)
1 Without loss of generality, deterministic regressors are suppressed in the following notation. Further-
more, vectors are assigned by bold lower-case letters and matrices by capital letters. Scalars are written as
lower-case letters, possibly subscripted.

TACTICAL ASSET ALLOCATION
267
where the stacked vectors ξ t and vt have dimension (K p × 1) and the matrix A has
dimension (K p × K p). If the moduli of the eigenvalues of A are less than 1, then the
VAR(p) process is stable.
For a given sample of the endogenous variables y1, . . . , yT and sufﬁcient pre-
sample values y−p+1, . . . , y0, the coefﬁcients of a VAR(p) process can be estimated
efﬁciently by least squares applied separately to each of the equations.
Once a VAR(p) model has been estimated, further analysis can be carried out.
A researcher might—indeed, should—be interested in diagnostic tests, such as test-
ing for the absence of autocorrelation, heteroscedasticity or non-normality in the
error process. He might be interested further in causal inference, forecasting and/or
diagnosing the empirical model’s dynamic behaviour – impulse response functions
(IRFs) and forecast error variance decomposition. The latter two are based upon
the Wold moving average decomposition for stable VAR(p) processes which is
deﬁned as
yt = 0ut + 1ut−1 + 2ut−2 + . . . ,
(13.43)
where 0 = IK and s can be computed recursively from
s =
s

j=1
s−j A j,
for s = 1, 2, . . . ,
(13.44)
with A j = 0 for j > p.
Finally, forecasts for horizons h ≥1 of an empirical VAR(p) process can be
generated recursively from
yT +h|T = A1yT +h−1|T + . . . + ApyT +h−p|T ,
(13.45)
where yT + j|T = yT + j for j ≤0. The forecast error covariance matrix is given as:
Cov
⎛
⎜⎝
⎡
⎢⎣
yT +1 −yT +1|T
...
yT +h −yT +h|T
⎤
⎥⎦
⎞
⎟⎠
=
⎡
⎢⎢⎢⎣
I
0
· · ·
0
1
I
0
...
...
0
h−1
h−2
. . .
I
⎤
⎥⎥⎥⎦(u ⊗Ih)
⎡
⎢⎢⎢⎣
I
0
· · ·
0
1
I
0
...
...
0
h−1
h−2
. . .
I
⎤
⎥⎥⎥⎦
′
, (13.46)
and the matrices i are the empirical coefﬁcient matrices of the Wold moving average
representation of a stable VAR(p) process as shown above. The operator ⊗is the
Kronecker product.

268
PORTFOLIO OPTIMIZATION APPROACHES
Structural vector autoregressive models
Recall the deﬁnition of a VAR(p) process, in particular equation (13.39). A VAR(p)
can be interpreted as a reduced-form model. An SVAR model is its structural form
and is deﬁned as
Ayt = A∗
1yt−1 + . . . + A∗
pyt−p + Bεt
(13.47)
It is assumed that the structural errors, εt, are white noise and the coefﬁcient matrices
A∗
i , i = 1, . . . , p, are structural coefﬁcients that differ in general from their reduced-
form counterparts. To see this, consider the equation that results from left-multiplying
equation (13.47) by the inverse of A:
yt = A−1 A∗
1yt−1 + . . . + A−1A∗
pyt−p + A−1Bεt
= A1yt−1 + . . . + Apyt−p + ut.
(13.48)
An SVAR model can be used to identify shocks and trace these out by employing
impulse response analysis and/or forecast error variance decomposition to impose
restrictions on the matrices A and/or B. Incidentally, although an SVAR model is a
structural model, it departs from a reduced-form VAR(p) model and only restrictions
for A and B can be added. It should be noted that the reduced-form residuals can be
retrieved from an SVAR model by ut = A−1Bεt and its variance–covariance matrix
by u = A−1BB′(A−1)′. Depending on the restrictions imposed, three types of SVAR
models can be distinguished:
r In the A model, B is set to IK (minimum number of restrictions for identiﬁcation
is K(K −1)/2 ).
r In the B model, A is set to IK (minimum number of restrictions for identiﬁcation
is the same as for the A model).
r In the AB model, restrictions can be placed on both matrices (minimum number
of restrictions for identiﬁcation is K 2 + K(K −1)/2).
The parameters are estimated by minimizing the negative of the concentrated log-
likelihood function:
ln Lc(A, B) = −K T
2
ln(2π) + T
2 ln |A|2 −T
2 ln |B|2
−T
2 tr(A′(B−1)′B−1A ˜u),
(13.49)
where ˜u denotes an estimate of the reduced-form variance–covariance matrix for
the error process.
Vector error correction models
Consider again the VAR in equation (13.39). There are two vector error correction
speciﬁcations. The ﬁrst is given by
yt = αβ′yt−p + 1yt−1 + . . . + p−1yt−p+1 + ut,
(13.50)

TACTICAL ASSET ALLOCATION
269
with
i = −(I −A1 −. . . −Ai),
i = 1, . . . , p −1,
(13.51)
and
 = αβ′ = −(I −A1 −. . . −Ap).
(13.52)
The i matrices contain the cumulative long-run impacts, hence this VECM speciﬁ-
cation is referred to as the long-run form. The other speciﬁcation is given as follows
and is in common use:
yt = αβ′yt−1 + 1yt−1 + . . . + p−1yt−p+1 + ut,
(13.53)
with
i = −(Ai+1 + . . . + Ap),
i = 1, . . . , p −1,
(13.54)
with the same  matrix as in equation (13.52). Since the i matrices in equation
(13.54) now measure transitory effects, this speciﬁcation is referred to as the transitory
form. In the case of cointegration the matrix  = αβ′ is of reduced rank. The
dimensions of α and β are K × r, and r is the cointegration rank, denoting how
many long-run relationships exist between the elements of yt. The matrix α is the
loading matrix, and the coefﬁcients of the long-run relationships are contained in β.
Structural vector error correction models
Consider again the VECM in equation (13.53). It is possible to apply the same
SVAR model reasoning to SVEC models, in particular when the equivalent level-
VAR representation of the VECM is used. However, the information contained in the
cointegration properties of the variables is not then used for identifying restrictions
on the structural shocks. Hence, typically a B model is assumed when a SVEC model
is speciﬁed and estimated:
yt = αβ′yt−1 + 1yt−1 + . . . + p−1yt−p+1 + Bεt,
(13.55)
where ut = Bεt and εt ∼N(0, IK). In order to exploit this information, one considers
the Beveridge–Nelson moving average representation of the variables yt if they follow
the VECM process as in equation (13.53):
yt = 
t
i=1
ui +
∞

j=0
∗
jut−j + y∗
0.
(13.56)
The variables contained in yt can be decomposed into a part that is integrated
of order 1 and a part that is integrated of order 0. The ﬁrst term on the right-hand
side of equation (13.56) is referred to as the ‘common trends’ of the system, and this
term drives the system yt. The middle term is integrated of order 0 and it is assumed
that the inﬁnite sum is bounded, that is, the ∗
j converge to zero as j →∞. The
initial values are captured by y∗
0. For the modelling of SVEC the interest centres
on the common trends in which the long-run effects of shocks are captured. The

270
PORTFOLIO OPTIMIZATION APPROACHES
matrix  is of reduced rank K −r, where r is the number of stationary cointegration
relationships. The matrix is deﬁned as
 = β⊥

α′
⊥

IK −
p−1

i=1
i

β⊥
−1
α′
⊥
(13.57)
Because of its reduced rank, only K −r common trends drive the system. Therefore,
knowing the rank of , one can conclude that at most r of the structural errors
can have a transitory effect. This implies that at most r columns of  can be set to
zero. One can combine the Beveridge–Nelson decomposition with the relationship
between the VECM error terms and the structural innovations. The common trends
term is then B ∞
t=1 εt, and the long-run effects of the structural innovations are
captured by the matrix B. The contemporaneous effects of the structural errors are
contained in the matrix B. As in the case of SVAR models of type B one needs
for locally just-identiﬁed SVEC models 1
2 K(K −1) restrictions. The cointegration
structure of the model provides r(K −r) restrictions on the long-run matrix. The
remaining restrictions can be placed on either matrix, whereby at least r(r −1)/2 of
them must be imposed directly on the current matrix B.
13.3
Black–Litterman approach
In Chapter 10 it was stated that the weight solution is sensitive to the input parameters
for Markowitz solutions. In that chapter the focus was on robust estimation techniques
for the dispersion matrix to rectify this problem. The sensitivity of the weight solution
to the assumed return vector was neglected, but references to the literature were
provided.
In this and the next section approaches are presented that deal indirectly with this
issue. The ﬁrst of these is the Black–Litterman (BL) model (see Black and Litterman
1990, 1991, 1992; He and Litterman 2002). This consists of ﬁve building blocks, with
a Bayesian estimation of equilibrium and expected returns at its core. The latter lets a
portfolio manager directly include his return expectations into the portfolio solution
and it is this characteristic that has led to the ubiquitous application of this method
for tactical asset allocation. It will be assumed that the returns are joint normally
distributed. The ﬁve building blocks are
r the capital asset pricing model (CAPM; Sharpe 1964),
r reverse Optimization (Sharpe 1974),
r mixed (Bayesian) estimation,
r the concept of a universal hedge ratio (Black 1989),
r mean–variance optimization (Markowitz 1952),
and these will be discussed in turn.

TACTICAL ASSET ALLOCATION
271
The CAPM is used as an equilibrium model, where the supply and demand of
assets are equated. Recall Figure 5.1 in which this point is depicted as the locus where
the capital market line is tangent to the curve of efﬁcient portfolios. At equilibrium the
efﬁcient market portfolio consists, therefore, of an efﬁcient allocation of risky assets
and the risk-free asset. This point is further characterized by the market capitalization
of each asset. The relative market capitalizations represent the assets’ weights at
equilibrium and are denoted by ωMKT. If one assumes that the unconstrained portfolio
optimization problem can be stated as
arg max
ω∈
ω′μ −λ
2ω′ω
(13.58)
where the expected excess returns, μ, are balanced against the portfolio’s riskiness,
ω′ω, for a risk aversion parameter, λ > 0, then the optimal (equilibrium) weight
vector is given by
ωOPT = [λ]−1 μ.
(13.59)
For arbitrary expectations for the assets’ excess returns, the allocation ωOPT will
generally differ from ωMKT, only coinciding if μ is equal to the implied excess return
expectations implied by the relative market capitalizations. By reverse optimization,
that is, left-multiplying of equation (13.59) by λ, one obtains the implied excess
returns, π, for a given market capitalization structure:
π = λωMKT
(13.60)
The implied equilibrium returns, the vector elements of π, are employed as the market
neutral starting point in the BL model, that is, the returns are a priori distributed with
an expected mean of π. In the absence of any views on the future values of the assets,
an investor is therefore best advised to allocate his wealth according to ωMKT, thus
implicitly sharing the market expectation for the excess returns as stated in equation
(13.60).
The question of how the investor’s expectations about the excess returns can be
included in this model arises next. In the BL model the inclusion of these ‘views’
is accomplished by deriving the Bayesian posterior distribution for the returns. Let
A denote the (multivariate) return expectation(s) and B the implied excess returns at
equilibrium. The joint likelihood function, P(A, B) can then be expressed by Bayes’
theorem as
P(A, B) = P(A|B)P(B) = P(B|A)P(A),
(13.61)
and because one is interested in the conditional return distribution in terms of the
equilibrium returns, one obtains, from the right-hand equality in equation 13.61,
P(A|B) = P(B|A)P(A)
P(B)
.
(13.62)
If the expected excess returns are stated as E(r), then equation (13.62) is written as
P(E(r)|π) = P(π|E(r))P(E(r))
P(π)
.
(13.63)

272
PORTFOLIO OPTIMIZATION APPROACHES
A feature of the BL model is that expectations do not have to be supplied for all
market assets and/or that these return expectations can be either expressed as absolute
or relative return targets. Therefore, the views can be modelled as:
PE(r) ∼N(q, )
(13.64)
where P denotes a (K × N) pick-matrix, q is the (K × 1) vector of absolute/relative
return expectations and  a (K × K) diagonal matrix expressing the uncertainties
about each view. Given the normality assumption, the latter assumption implies
independence between the expressed views. The implied independence of views is
awkward. It does not make much sense to assume that return forecasts for various
assets are formed independently of each other, in particular when these are derived
from a multivariate statistical model, so this assumption will be relaxed in the next
section on copula opinion pooling, but retained for the time being. Finally, the
conditional equilibrium returns for the expected returns are distributed as
π|E(r) ∼N(E(r), τ)
(13.65)
where a homogeneity assumption for the market participants has been exploited such
that E(π) = E(r) and τ is a scalar for scaling the uncertainty around the implicit
market equilibrium excess returns.
The posterior probability density function of the conditional random variable
r = E(r)|π is multivariate normally distributed with parameters
E(r) =
 
(τ)−1 + P′−1P
!−1  
(τ)−1 π + P′−1q
!
,
(13.66)
Var(r) =
 
(τ)−1 + P′−1P
!−1 .
(13.67)
In essence, equation (13.66) is a risk-scaled weighted average of the market equi-
librium returns and the view returns. The latter two are scaled by the inverse of the
variance–covariance matrix, (τ)−1, and by the conﬁdence of the views, P′−1,
respectively. The relative importance of the market equilibrium return and the view
return expressed in the kth row of P is determined by the ratio kk/τ = p′
kp. The
term on the right-hand side of this equation is the variance of the view portfolio. This
equation can also be used to determine implicit conﬁdence levels kk for a given
value of τ. If this route is followed, then the point estimates for E(r) are unaffected
by the chosen value for τ (see He and Litterman 2002). In the limiting case of no
expressed views (P = 0), the point estimates for E(r) are the equilibrium returns. At
the other extreme, when the views are expressed without prediction error, the point
estimates for E(r) are identical to the view expectations if speciﬁed and otherwise
are equal to the market returns. If these point estimates are used in an unconstrained
Markowitz-type portfolio optimization, then only the weights will differ from the
relative market capitalization ones, for which a return expectation has been formu-
lated. To summarize, the BL method – in its original form – consists of a Bayesian
derived vector of expected returns, which is then utilized in a Markowitz-type port-
folio optimization. Of course, the Bayesian estimates can also be used in any other
kind of portfolio problem formulation, for example, with respect to optimizing a
downside-risk measure.

TACTICAL ASSET ALLOCATION
273
13.4
Copula opinion and entropy pooling
13.4.1
Introduction
Copula opinion pooling (COP) and entropy pooling (EP) can both be viewed as
extensions to the original BL model. These methods are due to Meucci (2006a,b,
2010a,b). The BL model and the two extensions have in common that a synthesis
between an a priori distribution and a prior view distribution is accomplished in order
to yield a posterior distribution. The major difference between these two methods
and the BL model is in how these two ingredients are supplied or speciﬁed and
that ordinarily the posterior distributions according to the COP and EP models are
retrieved by means of a Monte Carlo simulation. In the rest of this section the COP
and EP models are presented.
13.4.2
The COP model
In the BL model the a priori distribution for the returns is derived from an equilib-
rium model, namely the CAPM, and the views are expressed as either absolute or
relative return forecasts. Furthermore, the BL model assumes that the returns follow a
multivariate normal distribution. These model constituents are too restrictive, and the
stylized facts of ﬁnancial returns conﬂict with the normal assumption. In contrast, the
COP method is much more ﬂexible and less demanding with respect to its underlying
assumptions. First, the a priori distribution need not be deﬁned in terms of the returns
on a set of securities, but can also be given in terms of risk factors or a set of any other
random variables, and it is not assumed that these random variables follow a multi-
variate normal distribution function. Instead the prior distribution can be represented
by a copula and the marginal distributions can be of any kind. Second, the views
are expressed as cumulative distribution functions and are as such not conﬁned to
absolute or relative return forecasts. However, conﬁdence levels, ck, k = 1, . . . , K,
for the views have to be provided by the user, similar to the diagonal elements of the
uncertainty matrix  in the BL model. In addition, a ‘pick’ matrix P is a required
input. The cost of this ﬂexible approach is that in general no closed-form solutions
can be provided, but the synthesis between the two ingredients is accomplished by
means of a Monte Carlo simulation.
Deriving the posterior distribution by the COP method involves the following ﬁve
steps (see Meucci 2006a):
1. A rotation of the prior distribution into the views’ coordinates.
2. Computation of the views’ cumulative distribution function and the market-
implied prior copula.
3. Computation of the marginal posterior cumulative distribution functions of
each view.
4. Computation of the joint posterior distribution of the views.
5. Computation of the joint posterior realizations of the market distributions.

274
PORTFOLIO OPTIMIZATION APPROACHES
Each of these ﬁve steps are now described in more detail. It is assumed that a
Monte Carlo simulation, M, for the N securities of size J is given. These simulated
values can be drawn, for instance, from a Student’s t copula and any of the proposed
distributions in Chapter 6 could have been utilized as marginal models. But the Monte
Carlo values contained in M could also have been generated from a multiple time
series model as outlined in the previous sections of this chapter.
In the ﬁrst step, the simulated values contained in the (J × N) matrix M are
mapped into the coordinate system pertinent to the views, V. This is accomplished by
means of a simple matrix multiplication, V = M ¯P′, where ¯P denotes the (invertible)
pick matrix. There is a qualitative difference between this pick matrix and the one
used in the BL model. In the latter approach, the matrix P consists of K rows in
which the securities for which views are expressed are selected, hence its dimension
is K × N. In the COP model, however, the dimension is N × N. The ﬁrst K rows
of ( ¯P) are the views P, and the remaining N −K rows are ﬁlled by its matrix
complement, P⊥with dimension (N −K) × N, hence ¯P = (P|P⊥)′.
In the second step, the ﬁrst K columns of V are sorted in ascending order,
which results in the (J × K) matrix W, the prior cumulative distribution functions.
The copula of the views, C, are the order statistics of W, that is, C j,k = Fk(W j,k =
j/(J + 1).
Next, the marginal posterior cumulative distribution functions, F, for each view
are computed as a weighted average,
F j,k = ck ˆFk(W j,k) + (1 −ck)
j
J + 1.
(13.68)
In the fourth step, the joint posterior distribution of the views, ˜V (the quantile
function) is retrieved by means of interpolation between the grid points (F·,k, W·,k).
In the ﬁnal step the joint posterior simulated realizations of the market distribution
are recovered by inversion of the equation in the ﬁrst step. Because the dimension
of ˜V is only J × K, the matrix is amended from the right by the rightmost N −K
columns of V, and hence
˜
M = ˜V( ¯P′)−1. This set of J joint posterior realizations for
the N assets can then be utilized in the portfolio optimization problem at hand.
13.4.3
The EP model
The EP model is a further generalization of the BL and COP models. It has in common
with the COP model an arbitrary model for the market prior distribution and that the
COP and EP models are based on a Monte Carlo simulation of the market’s prior
distribution. A qualitative difference between the COP and EP models is the kind
of views expressed. In the former model class views on the dispersion, volatility or
correlations of and between securities and/or the expression of non-linear views were
not feasible. This limitation is rectiﬁed for EP models. A concise description of how to
formulate these views is provided in Meucci (2010b, Section 2 and Appendix A.2).
A further material difference between the COP and EP models is the form of the
posterior distribution. In the latter model, the same simulated values used in the

TACTICAL ASSET ALLOCATION
275
modelling of the market’s prior distribution are utilized, but with altered probabilities
assigned to these random variables.
In the following, the steps for deriving the posterior distribution are presented.
As stated in the previous subsection, it is assumed that the market for N assets can be
modelled by a set of random variables X which follow an a priori joint distribution,
fM. In its simplest form, X represents a sample of the securities’ returns, but is not
limited to these market factors. When the data has been ﬁtted to the assumed distribu-
tion model, simulated values for this market distribution can be obtained by means of
Monte Carlo, and hence one obtains a matrix M of dimension J × N as in the COP
model. The columns in M are the marginal prior distributions and the rows are simu-
lated outcomes for the market factors. Associated with each of these outcomes M j,· is
a probability p j and most easily these J probability are set equal to 1/J, that is, each
Monte Carlo draw is treated as being equally likely. Hence, the (J × 1) probability
vector p has as elements the reciprocal of the size of the Monte Carlo simulation.
The second input is the formulation of the views. This is now a (K × 1) vector of
function values V = g = (g1(X), . . . , gk(X), . . . , gK(X))′ with joint distribution fv,
a priori. The functions gk, k = 1, . . . , K, can be non-linear in nature. The implied
distribution of these views is then empirically approximated by the market simulations
M according to:
V j,k = gk(M j,1, . . . , M j,N)
(13.69)
for k = 1, . . . , K and j = 1, . . . , J, such that a J × K matrix results containing the
empirical distribution of the views implied by the Monte Carlo simulation for the
market.
Two panels M and V have now been created, and the question is how these can
be combined to retrieve the posterior distribution of the market that obeys the views.
This problem is resolved in three steps. First, the views are expressed in terms of a set
of linear inequality constraints, aL ≤A¯p ≤aU, where now the probability vector ¯p
is treated as the objective variable in the ensuing optimization. The lower and upper
bounds (aL, aU) and the matrix A are deduced from V.
In the second step, the relative entropy – loosely speaking, a distance measure
between distributions – is minimized.2 The (discrete) objective function is deﬁned as
RE(¯p, p) =
J

j=1
¯p j
 
log( ¯p j) −log(p j)
!
,
(13.70)
and hence the probabilities of the posterior distribution under the assumption of
perfect foresight are obtained by evaluating
¯p = arg min
aL≤A¯x≤aU
RE(x, p).
(13.71)
That is, the probabilities ¯p under the assumption of perfect foresight are determined
such that the resulting posterior distribution is least distorted by the distribution
of the views, or put differently, the view distribution is made most akin to the
2 A detailed exposition of the entropy concept is given in Golan et al. (1996)

276
PORTFOLIO OPTIMIZATION APPROACHES
reference model. At ﬁrst sight, the optimization stated in equation (13.71) seems to
be a numerically demanding exercise, given the dimension of the target variable.
However, in Meucci (2010b, Appendix A.3) the dual form of this mathematical
program is derived from the Lagrangian function, which results in a convex program
with linear constraints and a size of the objective variable equal to the number of
views K.
In the third step the empirical conﬁdence-weighted posterior distribution, (M, pc)
is determined similarly as in the COP model according to
pc = (1 −c)p + c¯p
(13.72)
The empirical posterior distribution thus constructed can then be utilized in the
portfolio optimization problem at hand.
13.5
Synopsis of R packages
13.5.1
The package BLCOP
The package BLCOP was brieﬂy discussed in Section 9.4.1 with regard to its capa-
bilities in the modelling of copulae. We will now present its functions and procedures
for the BL method.
To handle the views in the BL model an S4 class BLViews is available. The class
deﬁnition consists of four slots: p for the pick matrix, qv for the expressed returns
(absolute and relative), confidences for the expressed subjective conﬁdences of the
views and assets for the names of the assets contained in the BL model. Objects of
this class can be created by the generating function BLViews(). They have methods
for displaying the views (method show()), for deleting views (method delete-
Views()) and for adding views (method addViews()). A convenient function for
setting up the pick matrix is newPMatrix(). This function will return by default a
zero-ﬁlled matrix with number of rows equal to the value of the argument numViews
and number of columns equal to the length of assetNames, this argument being a
character vector of the assets included in the universe. The values contained in the
pick matrix or in the vector of returns and conﬁdences can be altered with the assign-
ment functions PMatrix(), qv() and confidences(), respectively. Furthermore,
functions for inspecting certain aspects from objects of class BLViews() are: as-
setSet() for the asset names contained in the universe; viewMatrix() for the
pick matrix amended on the right by the absolute/relative return expectations of each
view; PMatrix() for the pick matrix only; and confidences() for the associated
conﬁdence levels of each view.
The cornerstone function for estimating the posterior distribution in the BL model
is posteriorEst(). This function has as arguments views for the BLViews
object, mu for the equilibrium returns, tau for the scaling parameter of the
equilibrium distribution (default value 0.5), sigma for the variance–covariance ma-
trix of returns, and kappa with default value zero (if set greater than zero, then the
conﬁdence levels are replaced by κ PP′). The function returns an object of S4 class
BLResult. Alternatively, this kind of object can be created by calling the wrapper

TACTICAL ASSET ALLOCATION
277
function BLPosterior(). The difference between the two functions is that the latter
computes the prior distribution parameters internally. The user has to provide the
matrix of asset returns, the returns for the market index, and the value of the risk-
free rate. A function other than cov() for estimating the dispersion matrix can be
set by the argument covEstimator(). Of course, values for the other arguments
pertinent to the BL model must also be provided – views, tau and kappa. Objects
of class BLResult consist of seven slots: views, which contains the BLViews object
provided; tau for indeterminacy of the equilibrium distribution; four slots for the
prior and posterior distribution parameters, priorMean, priorCovar, posteri-
orMean and posteriorCovar; and a logical indicator for whether kappa has been
set to a value greater than zero. The parameters of the posterior distribution can be
extracted as a list from objects of this class with the function posteriorMeanCov().
In addition to this extractor function, the package provides methods for display-
ing the prior and posterior densities (method densityPlots()) and for using the
estimated BL posterior distribution parameters directly in portfolio optimization
(method optimalPortfolios.fPort()).
In BLCOP two functions/methods for determining an optimal portfolio
allocation for given prior/posterior distributions are provided. The function op-
timalPortfolios() takes as argument result an object of class BLResult,
and as argument optimizer the name of the optimizing function. By default, this
argument is set equal to the internal function optimalWeights.simpleMV() which
performs a Markowitz optimization by calling the function solve.QP() contained
in the package quadprog. The body of the internal function can be displayed by
utilizing the ::: operator. In addition to the estimates for the expected returns
and their variance–covariance matrix, it is speciﬁed with an additional argument
constraints which is designed to hold a named list object with elements per-
tinent to the arguments of solve.QP(). If this argument is left unspeciﬁed, then a
portfolio with a budget and non-negativity constraints is assumed. If a different
function than the default is to be employed, then the ﬁrst two arguments must be
the mean and the variance–covariance matrix (in that order). Additional arguments
can be speciﬁed and are passed down to the call of optimizer by means of the
ellipsis argument in optimalPortfolios(). Whether and how barplots of the
prior and posterior allocations are returned can be controlled by the logical argu-
ments doPlot and beside. As stated in the previous paragraph, optimal portfolio
allocations can also be determined by the method optimalPortfolio.fPort().
This method is a wrapper around the portfolio optimization facilities contained in
the package fPortfolio. In addition to the BLResult object, this function has argu-
ments for the portfolio speciﬁcation (class fPFOLIOSPEC with generating function
portfolioSpec(), both deﬁned and contained in the package fPortfolio), a valid
character string for the constraints on the portfolio weights (e.g., ’LongOnly’) and
the kind of portfolio optimization to be carried out. The latter argument is set by
default to ’minriskPortfolio’, but any other optimizing function in fPortfo-
lio can be provided. Both optimization functions will return a named list with two
elements for the results according to the prior (priorPFolioWeights) and posterior
(postPFolioWeights) distributions. If the S4 method has been employed the

278
PORTFOLIO OPTIMIZATION APPROACHES
list elements will be objects of formal class fPORTFOLIO, and otherwise vectors
containing the optimal allocations.
13.5.2
The package dse
The focus of the package dse is on model implementations for univariate and multi-
variate time series (see Gilbert 2012a).3 The model classes implemented are primarily
ARMA models and dynamic systems of equations expressed in state-space form. The
package is hosted on CRAN and contained in the ‘Econometrics’, ‘Environmetrics’,
‘Finance’ and‘TimeSeries’ TaskViews. Development versions arehostedonR-Forge.
Within it S3 classes and methods are employed. The more demanding computations
with respect to ARMA modelling and Kalman ﬁltering, the estimation and simulation
of these models are interfaced from FORTRAN routines. A vignette is shipped with
it to illustrate the utilization of these model classes and methods. Furthermore, demo
ﬁles are available to elucidate data handling, deﬁnition of model structure as well as
estimation, simulation and forecasting.
At the centre of the package are the three S3 classes TSdata, TSmodel and
TSestModel. The ﬁrst is for handling the data set, the second for deﬁning the
model structure and the last for containing information on an estimated model. The
generating function for objects of class TSdata is TSdata(). This function can
also be used to extract the data part from objects of class TSestModel. The data
class is based on the time series class tframe contained in the package of the
same name (see Gilbert 2012b). Objects of this class contain either the time series
as such or the user can provide data sets to be used as input (ordinarily the data
to be used on the right-hand side of a time series model) and output (ordinarily
the data to be used on the left-hand side of a time series model). The time series
used as either input or output series can also be queried from a TSdata object or
assigned to it by the methods inputData() or oututData(). Methods deﬁned for
these objects are print(), plot(), tfplot(), summary(), as.TSdata() and
is.TSdata(), with obvious meanings. It is also possible to assign different names
to the series than the column names or query the series names by means of the
methods seriesNames(), seriesNamesInput() and seriesNamesOutput().
The column location of a series used in either the input or output data set can
be retrieved by either of the nseriesInput() or nseriesOutput() methods.
Information on when the sample starts and ends, the number of observations and the
frequency of the time series can be queried by the methods:
r start(), startInput(), startOutput(),
r end(), endInput(), endOutput(),
r Tobs(), TobsInput(), TobsOutput(),
r frequency(), frequencyInput(), frequencyOutput().
3 The package EvalEst complements dse by providing methods for analysing the properties of different
estimators (see Gilbert 2011).

TACTICAL ASSET ALLOCATION
279
The equality of two objects with class attribute TSdata can be assessed by
the method testEqual() and whether the dimensions agree by the method
checkConsistentDimensions(). To manipulate the sample span or bind two
objects together, the methods deﬁned in the package tframe – tframed(), tfwin-
dow(), tbind(), and trimNA() – are available, as well as a combine() method
for joining two TSdata objects. Further, the window() method deﬁned in the
base package stats can also be applied to TSdata objects. Incidentally, the acf()
method for returning the autocorrelations deﬁned in package stats can also be ap-
plied to TSdata objects, and the scaling of the TSdata object can be accomplished
by a method deﬁnition for the generic function scale() deﬁned in the base R
distribution. Percentage changes of time series data can be swiftly computed by
the percentChange() method. This is a rather detailed listing of methods de-
ﬁned for the time series class employed in the package dse. However, it should be
noted that these methods are also available for objects of class TSestModel, where
applicable.
But before giving a more detailed description of this class, the second cornerstone
class, TSmodel, and its associated methods will be discussed. First, objects of this
class can be created by calling the generating function TSmodel() which takes as
argument the deﬁnition of either an ARMA or a state-space model. The two supported
model classes, which themselves return an objects inheriting from TSmodel, are
ARMA() (class ARMA) and SS() (class SS), respectively. The coefﬁcients can be
recovered and/or assigned by the coef() method, whereby the generic deﬁnition
is imported from the base package stats. The function TSmodel() can also be
used to extract the ﬁtted model from an object of class TSestModel. For such
objects print() and summary() methods are available. Whether an object has
one of the class attributes TSmodel, ARMA or SS can be checked by the is.foo()
methods, where foo is a placeholder for the class name. The equality of two time
series models can be tested by means of testEqual(). To assess the validity of a
model, the methods stability() and roots() can be used. A plot() method
for objects returned by the latter function is also available, and with the function
addPlotRoots() these can be superimposed on an existing graphics device. The
representation of a certain model structure can be transformed from an ARMA to
a state-space speciﬁcation, and vice versa, by means of the methods toSS() and
toARMA(), respectively. A model structure can be evaluated for a given time series
object TSdata by the method l(). A state-space model can also be evaluated by
applying Kalman ﬁltering to a given state-space structure and time series object
by calling the smoother() method. The states of a state-space model can also be
returned by calling state() and setting the argument smoother to TRUE, otherwise
this function will return the state information for a ﬁtted state-space model. Finally,
simulated values for a given ARMA or state-space model structure can be generated
by calling the simulate() method.
So far, classes and methods for handling time series models with a speciﬁed
structure have been presented. This paragraph discusses the ﬁnal class, TSestModel,
and how estimates for the unknown coefﬁcients can be determined. A given data set
is ﬁtted to a time series model by calling the function estfoo(), where foo is

280
PORTFOLIO OPTIMIZATION APPROACHES
a placeholder for the model class and/or the estimation method. These functions
return an object with class attribute TSestModel, for which numerous evaluation,
analytical and extraction methods are provided. The currently implemented models
and estimators are:
r estVARXar(), which estimates the coefﬁcients of a VAR with optional ex-
ogenous variables by means of the Yule–Walker equations;
r estVARXls(), which estimates the coefﬁcients of a VAR with optional ex-
ogenous variables by means of OLS;
r estSSMittnik(), which estimates the coefﬁcients of a nested state-space
model with Markov switching;
r estMaxLik(), for ML-based estimation of TSmodel objects;
r estSSfromVARX(), which converts an estimated VAR model with optional
exogenous variables to its state-space representation.
In addition to these functions, ﬁrst, a generic function estimateModels() is
available where the kind of model/estimation method is provided by the argu-
ment estimation.methods(), and second, functions estBlackBoxn() with
n = 1,..., 4 are available as wrapper functions for estVARXls() where certain
arguments and restrictions on the parameter space are preset. The function est-
BlackBox() will call each consecutively and selects the model speciﬁcation that is
most appropriate. Similar to this function is bestTSestModel(), which selects the
best model from a list of ﬁtted models. Objects of class TSestModel can then be
further analysed by methods residuals() for retrieving the residuals, coef() for
displaying the estimates, stability() and roots() for assessing the model’s em-
pirical stability, and simulate() for simulating trajectories based on the estimates.
The ﬁtted values can be recovered by applying the l() method and the state informa-
tion can be recovered by the smoother() method. The residuals can be checked for
their white noise characteristic by calling checkResiduals() and the information
criteria are returned by the functions informationTests() and information-
TestsCalculations(). Further functions and methods, albeit of less importance,
are available and the reader is referred to the package’s manual for a description of
these.
Finally, forecasts can be created by calling forecast(), horizonFore-
casts(), or featherForecasts(). The latter two functions can be used to
generate one- and ﬁve-step-ahead forecasts. Each of these functions returns a named
list object with class attribute set equal to the function’s name. For visual in-
spection of the forecast accuracy plot() methods are available. A feature of these
methods is that the in-sample forecasts are aligned to the actual data, such that
the actual values and the n-ahead forecasts for that time period can be directly
compared.

TACTICAL ASSET ALLOCATION
281
13.5.3
The package fArma
The package fArma is part of the Rmetrics bundle of R packages and is hosted
on CRAN (see W¨urtz 2012). Within the package S4 classes and methods are
employed. The computationally intensive calculations are interfaced from rou-
tines written in FORTRAN. In particular, estimation and inference of the degree
of fractional integration are conducted by calling the underlying functions of the
fracdiff package (see Fraley et al. 2012). The package is shipped with a NAMES-
PACE containing the relevant export and import directives, but some functions are
internal.
The strength of the package is the provision of a uniﬁed interface for specifying
the different types of ARMA models – AR, MA, ARMA, ARIMA and ARFIMA.
This accomplished by providing the order of a time series model as a formula in the
cornerstone function armaFit(). The other important arguments to this function are
data for the univariate time series, method for determining whether the coefﬁcients
are estimated by ML or OLS, include.mean as a logical argument for indicating
whether a constant is to be included, and fixed where the user can provide a vector of
preset values for the coefﬁcients. The remaining arguments title, description,
and ... can be used for descriptive purposes of the model object returned, which
is of formal class fARMA. For objects of this class, show(), plot(), summary()
and predict() methods are available. Furthermore, the estimated coefﬁcients can
be extracted by means of a deﬁned coef() method, as can the ﬁtted values with the
method fitted() and the residuals by residuals().
Model trajectories can be simulated by means of the function armaSim(). This
function takes as input a parameter speciﬁcation in the form of a list object as
argument model and the length of the simulated series as argument n. The user
can control the generation of the random variables by providing a time series of
innovations to be used (argument innov) where by default these will be drawn from
a normal population. In addition, the number of pre-sample innovations is controlled
by either of the arguments n.start or start.innov. A seed for random number
generation can be provided as argument rseed, and the kind of distribution from
which the innovations are drawn is speciﬁed by the argument rand.gen which should
be the name of the function which returns samples from the distribution in question
(the default is to use rnorm as indicated above). The function returns an object of
class timeSeries.
The roots of an ARMA model and its associated ACF can be inspected by the
functions armaRoots() and armaTrueacf(), respectively. The former function
takes a vector of coefﬁcient values as input and the latter a speciﬁcation of the
ARMA model as a list object as in the function armaSim().
13.5.4
The package forecast
The purpose of the package forecast is to provide implementations of univariate
time series models and ﬁlter/decomposition methods from which forecasts can be

282
PORTFOLIO OPTIMIZATION APPROACHES
deduced (see Hyndman 2012). The package is hosted on CRAN and listed in the
‘Econometrics’, ‘Environmetrics’, ‘Finance’ and ‘TimeSeries’ Task Views; it is con-
sidered a core package in the latter. Within the package S3 classes and methods
are employed, with corresponding export directives contained in its NAMESPACE.
Certain computationally burdensome models are interfaced from C or C++ routines.
In particular, the estimation of an exponential smoothing state-space model with a
Box–Cox transformation, ARMA errors and with trend/seasonal components (BATS)
is interfaced from C++ routines (single and parallel processing). Five data sets are
included in the package to demonstrate the use of the various functions and methods
available.
The univariate statistical time series models implemented are ARFIMA, ARIMA,
BATS and exponential smoothing state-space models. The methods implemented
that can be considered as ﬁlters, transformations or time series decompositions are
Box–Cox transformation, Holt–Winters with extensions, moving averages and cubic
splines. The focus here is on the functions and methods pertinent to the time series
models outlined in Section 13.2.1.
The classical Box–Jenkins ARIMA model is implemented as function Arima(),
which is a wrapper for the function arima() in the package stats. A tentative
order for the ARIMA model can be deduced from the autocorrelation and partial
autocorrelation functions of the series. These are implemented as functions Acf()
and Pacf(), which are wrapper functions for acf() and pacf contained in the base
package stats, respectively. Furthermore, for inspecting the characteristics of a time
series, the function tsdisplay() is available. This function plots the trajectory of
the series with its ACF and, additionally, either the PACF or a scatterplot of the series
itself against its one-period lag or its spectrum. If exogenous regressors have been
included in the ARIMA model, except for the drift term, the residuals of this model
can be retrieved with the function arima.errors(). The ARIMA order can also be
automatically determined by calling auto.arima(). Here, the AR and MA order
and how often the series must be differenced to achieve stationarity are determined
using either the Akaike or Schwarz’s Bayesian information criterion. The user can
restrict the search by providing upper limits for orders of the ARIMA model.
ARFIMA models can be ﬁtted with the function arfima(). The degree of frac-
tional integration is ﬁrst estimated from an autoregressive model of order 2, where
by default the fractional parameter is restricted to yield a stationary series. The
ﬁltered series according to its degree of fractional integration is then used in the
Box–Jenkins approach. Unless otherwise speciﬁed by the user, the automatic proce-
dure auto.arima() for selecting the AR and MA order is executed in order to ﬁt
the ARMA model and determine optimal orders. After the ARMA order has been
determined the ARFIMA model is re-estimated by calling the function fracdiff()
from the package of the same name.
For these models, but also for the others implemented, a forecast() method
is available. The user can specify the number of forecast steps and the conﬁdence
level. The object returned is of informal class forecast, and print(), plot() and
summary() methods are available. If pseudo ex ante forecasts have been produced,
that is, there are actual values of the series to compare the forecasts against, the

TACTICAL ASSET ALLOCATION
283
function accuracy() can be used to obtain a set of accuracy measures (mean error,
root mean square error, mean absolute error, mean percentage error, mean absolute
percentage error, mean absolute scaled error, Theil’s inequality coefﬁcient and the
ACF). To cross-compare forecasts the test proposed by Diebold and Mariano (1995)
is implemented as function dm.test(). Finally, in this rubric of forecast evaluation,
the functions rwf() and naive() should be mentioned. Both can be used to generate
benchmark forecasts from a random walk model (with drift) or simply to produce
naive forecasts.
13.5.5
The package MSBVAR
The package MSBVAR enables the user to estimate classical and Bayesian (struc-
tural) VAR models and carry out inferential analysis on them (see Brandt 2011). The
package is available on CRAN and is contained in the ‘Bayesian’, ‘Econometrics’,
‘Finance’, ‘Multivariate’ and ‘TimeSeries’ Task Views. The package employs S3
classes and methods, and the computationally burdensome calculations for Monte
Carlo simulations and Markow chain Monte Carlo based estimations are interfaced
from routines written in C++, in particular the Gibbs sampler implemented for the
latter model class. The NAMESPACE contains the relevant export directives for the
deﬁned classes, methods and functions. Two data sets are shipped with the package,
which are both used in the example sections of the manual. Given the large number
of model classes, methods and functions implemented, the focus here is on multiple
time series models, from which out-of-sample forecasts can be deduced.
A VAR model as described in equation (13.39) can be estimated with the func-
tion reduced.form.var() by means of the seemingly unrelated regression (SUR)
principle, which yields the same estimates as an OLS estimator applied equation
by equation. This function takes three arguments: Y for the matrix of endogenous
variables, p for the lag order of the VAR(p) model and z for the matrix of ex-
ogenous (deterministic) variables. A suitable lag order can be determined from
χ2 tests and the Akaike, Schwarz and/or Hannan–Quinn information criteria with
the function var.lag.specification() for a maximum lag order set by the
argument lag.max. The former function returns a named list object with class
attribute VAR. Its elements are: intercept for the estimated intercepts of the VAR
equations, ar.coefs for an array of the coefﬁcients for the lagged endogenous vari-
ables, Bhat for a combination of the former two sets of estimates, exog.coefs for
the estimated coefﬁcients of the exogenous variables (if z was non-null), vcv and
mean.S for the (posterior) variance–covariance matrix of the residuals, hstar for
the cross-product of the right-hand-side variables, and the objects X, Y and y for
the left- and right-hand-side variables. A summary() method for objects of this S3
class is available. The package next provides functions for producing out-of-sample
forecasts (forecast()) and for the density of unconditional forecasts by means
of MCMC with the function uc.forecast()), computing the impulse response
(irf() or, based on Monte Carlo methods, mc.irf()), and the forecast error de-
composition (dfev()). Time series plots of the forecasts can be created with the
function plot.forecast() and the impulse responses can be visually inspected by

284
PORTFOLIO OPTIMIZATION APPROACHES
means of the functions plot.irf() and plot.mc.irf(). The forecast accuracy
can be assessed by the mean absolute error (mae()) or by the root mean squared
errors (rmse()). Both functions expect two matrix objects, one for the forecasts and
the other for actual or reference values.
For Bayesian estimation of the coefﬁcients for the reduced-form VAR model as
proposed by Sims and Zha (1998) the function szbvar() is available. It returns
a named list object with class attribute BVAR. Bayesian estimation of an SVAR
model as described in Waggoner and Zha (2003) can be carried out with the func-
tion szbsvar(). The (1, 0) matrix used for identiﬁcation has to be provided as
argument ident. This matrix places zero restrictions on the coefﬁcient matrix for
the current dependencies between the variables. The function returns a named list
object with class attribute BSVAR. The posterior distribution of the structural param-
eters of this Bayesian SVAR model can then be determined by applying a Gibbs
sampler to objects of this class by means of the function gibbs.A0(). The densities
for each of the structural parameters can then be visually inspected by the function
plot.gibbs.A0(). The same set of methods and functions that are applicable to
objects of class VAR can also be applied to these objects.
13.5.6
The package PairTrading
Facilities for the implementation and evaluation of pair trading strategies are imple-
mented in the package PairTrading (see Takayanagi and Ishikawa 2012). Pair trading
is sometimes referred to as ‘statistical arbitrage’, and it is based on the cointegration
model proposed by Engle and Granger (1987).
The package is hosted on CRAN and contained in the ‘Finance’ Task View. It is
written purely in R and neither S3 nor S4 classes and methods are employed. It is
shipped with a NAMESPACE which contains export directives for the most important
functions. Furthermore, a presentation describing the concept of a pair trading strategy
is contained in the package’s ./doc subdirectory. The package depends on the
packages xts and tseries (see Ryan and Ulrich 2012; Trapletti and Hornik 2012). A
data set of three stock prices complements the package, though the series provided
are not explicitly named.
The cornerstone function for determining whether a stable long-run relationship
exists between two assets is EstimateParameters(). The bivariate data set is ﬁtted
to an intrinsically linear relationship, that is, the logarithms of the two price series are
used in a bivariate regression model, hence the slope coefﬁcient can be interpreted as
an elasticity. The function returns a list object with the following elements: spread,
the residual (error) series of the Engle–Granger long-run relationship; hedge.ratio,
the slope coefﬁcient; and premium, the intercept of the regression. The function
IsStationary() can be used to evaluate whether the two series are cointegrated.
This function employs the unit root tests proposed by Phillips and Perron (1988),
Dickey and Fuller (1981) and Said and Dickey (1984). The unit root tests used
are PP.test() for the Phillips–Perron test, which is part of the package tseries,
and adf.test() for the augmented Dickey–Fuller test, which is contained in the
base package stats. Of course, any other cointegration test can be used to determine

TACTICAL ASSET ALLOCATION
285
whether a stable long-run relationship exists, such as the relevant functions provided in
the package urca (see Pfaff 2008a). The function IsStationary() returns a named
two-element logical vector which indicates whether the residuals are stationary for a
given threshold value compared with the p-value of these tests. A series of trading
signals can be determined from the error term with the function Simple(), which
assesses whether the error term is greater than a user-provided spread.entry level.
An amended version of this function is SimpleWithTakeProfit(), in which an
absolute upper bound on the error term can be speciﬁed; when this is exceeded the
signal is terminated. This function can be accessed by using the triple-colon operator,
as it is not exported. The performance of a pair trading strategy can be evaluated
with the functions EstimateParametersHistorically() and Return(). The
former function estimates the long-run relationship by a moving sample window of
size period and returns a list object with the last residual of each window (spread)
and the the estimates for the intercept (premium) and slope (hedge.ratio). A signal
series of the ﬁrst list element can be computed as stated above and then utilized in
the call of the function Return(). In addition to the signal series, the price series of
the two securities considered have to be supplied as arguments as well as the slope
coefﬁcients, both lagged by one period to allow an implementation lag. The slope
coefﬁcient hedge.ratio is converted in this function to a weighting between the two
assets by means of the internal function HedgeRatio2Weight(). This function can
also be made accessible by utilizing the triple-colon operator. The function Return
has as output a stream of returns implied by the chosen pair trading strategy.
13.5.7
The packages urca and vars
In this last subsection the packages urca and vars are presented. The focus of
the former is the analysis of integrated and cointegrated time series, whereas the
emphasis of the latter is on the implementation of multivariate time series models and
inferences associated with these as described in Section 13.2.2. Thus, the packages are
mutually complementary. A detailed description is provided in Pfaff (2008a, c). Both
packages are hosted on CRAN and are contained in the ‘Econometrics’, ‘Finance’
and ‘TimeSeries’ Task Views. The package urca is considered a core package in the
ﬁrst two task views.
In the package urca S4 classes and methods are employed. To assess the degree of
integration of a time series, the following unit root/stationary tests are implemented
(in alphabetical order of the authors’ names):4
r (augmented) Dickey–Fuller (see Dickey and Fuller 1979, 1981), ur.df();
r Elliot–Rothenberg–Stock (see Elliott et al. 1996), ur.ers();
r Kwiatkowski–Phillips–Schmidt–Shin
(see
Kwiatkowski
et
al.
1992):
ur.kpss();
4 Facilities for testing the degree of integration are also implemented in the packages tseries (Trapletti
and Hornik 2012) and fUnitRoots (W¨urtz 2009b).

286
PORTFOLIO OPTIMIZATION APPROACHES
r Phillips–Perron (see Phillips and Perron 1988), ur.pp();
r Schmidt–Phillips (see Schmidt and Phillips 1992), ur.sp();
r Zivot–Andrews (see Zivot and Andrews 1992), ur.za().
Each of these functions returns an S4 object with class name the same as the func-
tion name. For objects of these classes show(), summary() and plot() methods
are available. Functions to compute the distribution of unit root/cointegration tests
by means of critical surface responses as proposed in MacKinnon (1991, 1996) are
available as punitroot(), qunitroot() and unitrootTable(). These func-
tions interface the original FORTRAN routines, which were kindly provided by
MacKinnon.
To investigate the long-run relationship between integrated time series, the pro-
cedures proposed by Johansen (1988, 1991, 1995) and Johansen and Juselius (1990,
1992) (as function ca.jo()) and Phillips and Ouliaris (1990) (as function ca.po())
can be used. Both functions return S4 objects of the same name as the generat-
ing functions and show(), summary() and plot() methods are deﬁned for these
kinds of object. Numerous functions are also available to handle the ML approach
to estimation and inference of/from VECM models. Unrestricted and restricted esti-
mates for the right-hand-side coefﬁcients are returned by the functions cajools()
and cajorls(), respectively. The VECM residuals can be inspected graphically by
the function plotres(). Inference on the kind of long-run relationship(s) and the
validity of restrictions imposed can be conducted with the functions alrtest(),
blrtest(), ablrtest(), bh5lrtest() and bh6lrtest(). All of these return
an S4 object cajo.test for which show() and summary() methods are available.
A test of the cointegration rank by allowing a level shift at an unknown point in
time, as proposed by L¨utkepohl et al. (2004), can be conducted with the function
cajolst(). Finally, the package is shipped with 15 data sets for replicating results
in the references cited.
In contrast to urca, S3 classes and methods have been employed in the package
vars. An overview of the informal classes, methods and functions implemented is
provided in Table 13.1.
The cornerstone functions for estimating VAR, SVAR or SVEC models are
VAR(), SVAR(), and SVEC(), respectively. The link between urca and vars is estab-
lished by converting a VECM model (object of S4 class ca.jo) into its level form
by means of the function vec2var(). For all of these returned model objects at least
summary(), logLik() and plot() methods are available. Furthermore, impulse
response functions and the forecast error variance decomposition are returned by the
methods irf() and fevd(), respectively. For objects with class attributes varest
or vec2var statistical tests for assessing whether the error term is spherical and/or
normally distributed or not can be accomplished by the functions serial.test(),
arch.test(), and normality.test(), with obvious meanings. These functions
return an object with class attribute varcheck for which a plot() method is avail-
able. In addition to the statistical testing procedures for these two classes, extrac-
tor methods for the ﬁtted values (fitted()) and residuals (residuals()) are

TACTICAL ASSET ALLOCATION
287
Table 13.1
Structure of package vars.
Function/method
Class
Methods for class
Functions for class
VAR()
varest
coef(), fevd(),
fitted(), irf(),
logLik(), Phi(),
plot(),
predict(),
print(), Psi(),
resid(),
summary()
Acoef(),
arch.test(),
Bcoef(), BQ(),
causality(),
normal-
ity.test(),
restrict(),
roots(),
serial.test(),
stability()
SVAR()
svarest
fevd(), irf(),
logLik(), Phi(),
print(),
summary()
SVEC()
svecest
fevd(), irf(),
logLik(), Phi(),
print(),
summary()
vec2var()
vec2var
fevd(), fitted(),
irf(), logLik(),
Phi(), predict(),
print(), Psi(),
resid()
arch.test(),
normal-
ity.test(),
serial.test()
fevd()
varfevd
plot(), print()
irf()
varirf
plot(), print()
predict()
varprd
plot(), print()
fanchart()
summary()
varsum,
svarsum,
svecsum
print()
arch.test()
varcheck
plot(), print()
normality.test()
varcheck
plot(), print()
serial.-test()
varcheck
plot(), print()
stability()
varstabil
plot(), print()

288
PORTFOLIO OPTIMIZATION APPROACHES
available. To obtain forecasts from a VAR or VECM-in-level model, predict()
methods can be used. These produce point forecasts for n.ahead steps with as-
sociated conﬁdence bands at the level determined by the argument ci (the default
is 95%). Finally, the data set Canada for replicating the results of L¨utkepohl and
Kr¨atzig (2004) as shown in the package’s vignette is included in vars.
13.6
Empirical applications
13.6.1
Black–Litterman portfolio optimization
This ﬁrst example shows how the BL approach can be applied to the one-step-ahead
forecasts for European equity markets derived from a VECM model. The estimated
parameters of the posterior distributions are then used to obtain a weight vector that
maximizes the portfolio’s Sharpe ratio – that is, the solution to the empirical tangency
portfolio is sought. The example involves a back-test where, in addition to the BL
approach, portfolio weights based upon the prior distribution and a naive allocation
are computed and the resulting portfolios’ equities are contrasted with each other.
Listing 13.1 Integration and cointegration analysis of equity indexes.
library(urca)
1
library(vars)
2
## Loading data set and converting to zoo
3
data(EuStockMarkets)
4
Assets <−as.zoo(EuStockMarkets)
5
## Aggregating as month’s−end series
6
AssetsM <−aggregate(Assets, as.yearmon, tail, 1)
7
head(AssetsM)
8
## Applying unit root tests for subsample
9
AssetsMsub <−window(AssetsM, start = start(AssetsM),
10
end = "Jun 1996")
11
## Levels
12
ADF <−lapply(AssetsMsub, ur.df, type = "drift",
13
selectlags = "AIC")
14
ERS <−lapply(AssetsMsub, ur.ers)
15
## Differences
16
DADF <−lapply(diff(AssetsMsub), ur.df, selectlags = "AIC")
17
DERS <−lapply(diff(AssetsMsub), ur.ers)
18
## VECM
19
VEC <−ca.jo(AssetsMsub, ecdet = "none", spec = "transitory")
20
summary(VEC)
21

TACTICAL ASSET ALLOCATION
289
Table 13.2
Test statistics for unit root tests.
ADF
ERS
Unit roots
Level
Diff
Level
Diff
DAX
−0.33
−5.75
−0.05
−2.82
SMI
0.33
−4.74
0.36
−2.76
CAC
−2.56
−6.62
−1.64
−3.09
FTSE
−0.66
−6.10
0.26
−2.49
The ﬁrst step of this analysis is shown in Listing 13.1. The packages urca and
vars are brought into memory and the EuStockMarkets data set (see Section 9.5.1)
is loaded. To summarize, this data set consists of the daily levels for the DAX, FTSE,
CAC and SMI indexes and has class attribute mts. In line 5 the data set is converted
to a zoo object and in line 7 the month’s-end values are extracted. The sample starts
in June 1991 and ends in August 1998, comprising a total of 87 observations. The
ﬁrst 61 entries will be used as a subsample for unit root and cointegration testing, as
is achieved by the window() function in line 10. The remaining data is kept back
for executing the back-test. In lines 12–18 the degree of integration is determined by
applying the ADF and ERS test to the level and ﬁrst differences of the series. The
outcome of these tests is shown in Table 13.2. Given critical values for the tests at the
5% level of −2.89 (with constant) and −1.95 (without constant), it can be concluded
that all series are difference stationary.
Next, the subsample is ﬁtted to a VECM model by means of the expression in line
20. The default lag order of 2 (expressed in level form) and the transitory speciﬁcation
of the VECM have been used. The results of the maximum eigenvalue test are shown
in Table 13.3. The null hypothesis of no long-run relationship is rejected, hence the
four equity markets are cointegrated with rank 1. This model speciﬁcation will be
maintained throughout the back-test.
Having established the structure of the working model, the one-step-ahead fore-
casts and the return expectations deduced from them are computed as shown in
Table 13.3
Results of maximum eigenvalue test for VECM.
Max. eigenvalue
Critical values
Cointegration
Statistic
10 %
5 %
1 %
r ≤3
0.10
6.50
8.18
11.65
r ≤2
5.68
12.91
14.90
19.19
r ≤1
16.62
18.90
21.07
25.75
r = 0
33.33
24.78
27.14
32.14

290
PORTFOLIO OPTIMIZATION APPROACHES
Listing 13.2 Generating views derived from VAR model of assets.
## Index of time stamps in back−test (extending window)
1
idx <−index(AssetsM)[−c(1:60)]
2
ANames <−colnames(AssetsM)
3
NAssets <−ncol(AssetsM)
4
## Function for return expectations
5
f1 <−function(x, ci, percent = TRUE){
6
data <−window(AssetsM, start = start(AssetsM), end = x)
7
Lobs <−t(tail(data, 1))
8
vec <−ca.jo(data, ecdet = "none", spec = "transitory")
9
m <−vec2var(vec, r = 1)
10
fcst <−predict(m, n.ahead = 1, ci = ci)
11
LU <−matrix(unlist(fcst$fcst),
12
ncol = 4, byrow = TRUE)[, c(2, 3)]
13
RE <−rep(0, NAssets)
14
PView <−LU[, 1] > Lobs
15
NView <−LU[, 2] < Lobs
16
RE[PView] <−(LU[PView, 1] / Lobs[PView, 1] −1)
17
RE[NView] <−(LU[NView, 1] / Lobs[NView, 1] −1)
18
names(RE) <−ANames
19
if(percent) RE <−RE ∗100
20
return(RE)
21
}
22
ReturnEst <−lapply(idx, f1, ci = 0.5)
23
qv <−zoo(matrix(unlist(ReturnEst),
24
ncol = NAssets, byrow = TRUE), idx)
25
colnames(qv) <−ANames
26
tail(qv)
27
Listing 13.2. First the date stamps for the extending back-test windows are assigned
to the object idx and the names and sizes of the equity indexes are stored in the
objects ANames and NAssets, respectively. In the body of function f1() a simple
forecasting rule is implemented – only those forecasts deemed sufﬁciently large in
absolute terms will enter into a view for a certain equity market. This function is
used in the call to lapply() in line 23, so that the views for the entire back-test
window are computed by one line of code. Within the function ﬁrst the subsample
for recursive estimation of the VECM is extracted from the object AssetsM and the
last row of this observation is assigned to the object Lobs. This object will be used
later to determine whether a directional view warrants a certain return for a given
conﬁdence level. In lines 9–10 the VECM is estimated and converted into its level
representation by utilizing the function vec2var(), where a cointegration rank of
1 is assumed. This step is necessary because the one-step-ahead forecasts together

TACTICAL ASSET ALLOCATION
291
with the lower and upper bounds for a given conﬁdence level ci can then be swiftly
computed by means of the available predict() method for this kind of object.
Because the predict() method returns a list object, the lower and upper bounds
for the one-step-ahead forecasts are extracted and put in the matrix LU. The vector
of return expectations is initialized for all four markets to zero. A positive (negative)
view on a market is given when the lower (upper) bound is above (below) the latest
observed price. Whether this is the case is determined by the logical vectors PView
and NView, respectively. The return expectations are then replaced at the relevant
entries in RE accordingly. Hence, the absolute return should be at least as high as in
RE with 1 - ci/2 conﬁdence. The list object of the views, where each element
contains the view for the subsequent period, is assigned to the object ReturnEst.
In lines 24–27 this list object is converted to a zoo object (qv) where the rows,
with their date information, carry the views formed at that period for the subsequent
period.
Having determined the views for each period, the estimates for the parameters
of the posterior distributions have to be computed for each period in the back-test
sample, and these can then be employed in portfolio optimizations. These tasks are
accomplished in Listing 13.3.
First the necessary packages BLCOP and fPortfolio for conducting these com-
putations are loaded into the work space. In line 4 the discrete one-period percentage
returns are calculated and assigned to the object R. In the following lines the pick
matrix P is created; because only directional views for each asset will be imple-
mented, this pick matrix is diagonal. In lines 9–21 an auxiliary function BL() is
created which returns an object of class BLResult. This S4 object contains the
parameters of the prior and posterior distributions. Similar to the previous function
for generating views, BL() includes an argument x that tells it when the extending
sample ends. Within the function the view of a certain period is recovered from qv
and the subsample of the returns is assigned to the object Rw. Next, the parameters
of the prior distribution are estimated. The conﬁdences in the views can take any
value, because in the estimation of the posterior distribution they will be replaced
by the sample variances of the returns, enforced by calling BL() with the argument
kappa=1. In the ﬁnal lines, an object view of class BLViews is created, which is
then employed in the estimation of the posterior parameters. This function is then
called in line 22 by applying lapply() to the time index idx. The result will be a
list object of length 27 and each element contains the objects of class BLResult.
In the following block of statements two functions EstPrior() and EstBL() are
deﬁned which will be used to extract the estimated parameters of the prior and pos-
terior distributions, respectively. These functions are then assigned as estimators in
the portfolio speciﬁcations employed for determining the solutions of the maximum
Sharpe ratio portfolios derived from the estimates for the parameters of the prior and
posterior distributions, respectively. A box constraint where the weights are allowed
to vary in the interval [−0.8, 0.8] completes the speciﬁcation.
In Listing 13.4 an auxiliary function BT() is created with the purpose of returning
a matrix containing the optimal weights for the prior- and posterior-based maximum
Sharpe ratio portfolios. This function is then employed in a for loop where these

292
PORTFOLIO OPTIMIZATION APPROACHES
Listing 13.3 Maximum Sharpe ratio portfolio speciﬁcations.
library(BLCOP)
1
library(fPortfolio)
2
## Computing returns
3
R <−(AssetsM / lag(AssetsM, k = −1) −1.0) ∗100
4
## Pick matrix for directional views
5
P <−diag(NAssets)
6
colnames(P) <−ANames
7
## Function for BL posterior distribution
8
BL <−function(x, tau, kappa){
9
q <−qv[time(qv) == x, ]
10
q <−c(coredata(q))
11
Rw <−window(R, start = start(R), end = x)
12
mu <−colMeans(Rw)
13
cov <−cov(Rw)
14
clevel <−rep(1, NAssets)
15
views <−BLViews(P = P, q = q, conﬁdences = clevel,
16
assetNames = ANames)
17
post <−posteriorEst(views, mu = mu, tau = tau,
18
sigma = cov, kappa = kappa)
19
return(post)
20
}
21
PostDist <−lapply(idx, BL, tau = 1, kappa = 1)
22
## Deﬁning portfolio speciﬁcations
23
EstPrior <−function(x, spec = NULL, ...){
24
list(mu = BLR@priorMean, Sigma = BLR@priorCovar)
25
}
26
EstBL <−function(x, spec = NULL, ...){
27
list(mu = BLR@posteriorMean, Sigma = BLR@posteriorCovar)
28
}
29
## Prior speciﬁcication
30
MSPrior <−portfolioSpec()
31
setEstimator(MSPrior) <−"EstPrior"
32
## BL speciﬁcation
33
MSBl <−portfolioSpec()
34
setEstimator(MSBl) <−"EstBL"
35
## Constraints
36
BoxC <−c("minW[1:NAssets] = −0.8", "maxW[1:NAssets] = 0.8")
37

TACTICAL ASSET ALLOCATION
293
Listing 13.4 Maximum Sharpe ratio portfolio back-test.
## Back−test function
1
BT <−function(DataSub, BLR){
2
DataSub <−as.timeSeries(DataSub)
3
PPrior <−tangencyPortfolio(data = DataSub, spec = MSPrior,
4
constraints = BoxC)
5
PBl <−tangencyPortfolio(data = DataSub, spec = MSBl,
6
constraints = BoxC)
7
Weights <−rbind(getWeights(PPrior), getWeights(PBl))
8
colnames(Weights) <−ANames
9
rownames(Weights) <−c("Prior", "BL")
10
return(Weights)
11
}
12
## Conducting back−test
13
Backtest <−list()
14
for(i in 1:length(idx)){
15
DataSub <−window(R, start = start(AssetsM), end = idx[i])
16
BLR <−PostDist[[i]]
17
Backtest[[i]] <−BT(DataSub, BLR)
18
}
19
matrices are stored as elements in the list object Backtest. Note that in this loop the
object BLR is created which will be used by means of lexical scoping in the call to
tangencyPortfolio(). To that end, the optimal allocations in each period of the
back-test sample are determined and can then be used for calculating the portfolio
wealth trajectories.
As a ﬁnal step, the weights have to be recovered from the list object and
the portfolio equities calculated as shown in Listing 13.5. In the ﬁrst two blocks of
statements the ﬁrst and second rows are extracted from each list element and arranged
as zoo objects for the prior and posterior weights. The weights are lagged by one
period such that the allocations can then be directly multiplied with the corresponding
returns. These calculations are accomplished in lines 12–18. First the subsample of
returns pertinent to the back-test period is extracted from the object R which holds
the returns for the entire sample period. These returns are then multiplied with the
optimal allocations, the row sums are computed and 1 is added, to arrive at the return
factors. The seed wealth is set to 100 monetary units and the cumulated product of
initial wealth and the return factors equals the portfolio wealth trajectories for the
maximum Sharpe ratio allocations according to the prior and posterior distributions.
For better comparison, thewealthtrajectoryof anaive(i.e., equal-weighted) allocation
is computed which can be used as a benchmark.
The paths of the portfolio equities and the relative wealth shares between the
BL-based portfolio allocations vis-`a-vis those determined from the prior distribution

294
PORTFOLIO OPTIMIZATION APPROACHES
Listing 13.5 Comparison of portfolio strategies.
## Extracting weights based on prior
1
WPrior <−matrix(unlist(lapply(Backtest, function(x) x[1, ])),
2
ncol = NAssets, byrow = TRUE)
3
WPrior <−zoo(WPrior, order.by = idx)
4
WPriorL1 <−lag(WPrior, k = −1, na.pad = TRUE)
5
## Extracting weights based on BL
6
WBl <−matrix(unlist(lapply(Backtest, function(x) x[2, ])),
7
ncol = NAssets, byrow = TRUE)
8
WBl <−zoo(WBl, order.by = idx)
9
WBlL1 <−lag(WBl, k = −1, na.pad = TRUE)
10
## Compute portfolio equities
11
Rsub <−R[time(WBlL1), ] / 100
12
RetFacPrior <−rowSums(Rsub ∗WPriorL1) + 1
13
RetFacPrior[1] <−100
14
RetFacBl <−rowSums(Rsub ∗WBlL1) + 1
15
RetFacBl[1] <−100
16
EquityPrior <−zoo(cumprod(RetFacPrior), index(Rsub))
17
EquityBL <−zoo(cumprod(RetFacBl), index(Rsub))
18
## Equal−weight strategy
19
EW <−matrix(1 / NAssets, ncol = NAssets, nrow = nrow(WBlL1))
20
RetFacEw <−rowSums(Rsub * EW) + 1
21
RetFacEw[1] <−100
22
EquityEw <−zoo(cumprod(RetFacEw), indexs(Rsub))
23
and the equal-weighted allocations are displayed in Figure 13.3 and the associated R
code is shown in Listing 13.6.
The MSR portfolio allocation well outperforms the equal-weighted approach and
the BL-approach adds value compared to the allocation based on the prior distribu-
tion. Finally, the distributions of the allocations derived from the prior and posterior
parameter estimates are shown as boxplots in Figure 13.4. The weights applied to
the equity indexes according to the prior distribution are much more concentrated
compared to the portfolio solution derived from the BL posterior distribution. Qual-
itatively, only short positions would have been taken in the former approach with
respect to the French stock market and only long positions for the remaining three
markets. In contrast to this, the allocations according to the BL posterior span a wider
interval and long positions for the French stock market as well as short positions
for the UK stock market would have been entered during the back-test, reﬂect-
ing the impact of the diverging return expectations between the prior and posterior
estimates.

TACTICAL ASSET ALLOCATION
295
1997
1998
100
140
180
220
Value
Wealth Trajectory
Black−Litterman
Prior
Equal−Weight
Jun 1996
Dec 1996
Jun 1997
Dec 1997
Jun 1998
Against Prior
Against EW
Relative Performance
Percentages
0
5
10
15
20
Figure 13.3
Equity for BL, prior and equal-weighted portfolios.
13.6.2
Copula opinion pooling
To continue the previous example, we will now employ the latest return forecasts in
the copula opinion pooling framework. The relevant commands are shown in Listing
13.7. In contrast to Listing 13.3, a multivariate skewed Student’s t distribution is
now assumed for the market returns. The unknown parameters are estimated with
the function mvFit() which is contained in the package fCopula. This package was
loaded into the work space when fPortfolio was brought into memory. The estimated
distribution parameters are then extracted from the ﬁtted model object and passed to
the function mvdistribution() of the package BLCOP. The object CopPrior
now contains the relevant information on the assumed market distribution.
In the next block of R statements, the latest return forecast is extracted from
the previously created list object ReturnEst and assigned to RetEstCop. For
that time period return views different from zero at the 50% conﬁdence level were
recovered for the DAX, SMI and FTSE indexes. Hence, the pick matrix will consist
of three rows and four columns and is ﬁlled with 1s at the relevant row entries for
these indexes. After the pick matrix PCop has been created, the return forecasts
have to be expressed in terms of a distributional argument. The return variances are
used as a measure of dispersion and the point forecasts are assumed to be normally

296
PORTFOLIO OPTIMIZATION APPROACHES
Listing 13.6 Display of portfolio strategies.
## Graphical display of back−test result
1
par(mfrow = c(2, 1))
2
## Plotting of equity curves
3
plot(EquityBL, main = "Wealth Trajectory",
4
ylab = "Value", xlab = " ")
5
lines(EquityPrior, col = "darkgrey", lty = 2)
6
lines(EquityEw, col = "grey", lty = 3)
7
legend("topleft",
8
legend = c("Black−Litterman", "Prior", "Equal−Weight"),
9
col = c("black", "darkgrey", "grey"), lty = 1:3)
10
## Relative performance
11
RelOut <−cbind((EquityBL / EquityPrior −1) ∗100,
12
(EquityBL / EquityEw −1) ∗100)
13
barplot(RelOut, xlab = " ", ylab = "Percentages",
14
main = "Relative Performance",
15
ylim = c(0, 20), beside = TRUE,
16
legend.text = c("Against Prior", "Against EW"),
17
args.legend = list(x = "topleft"))
18
box()
19
## Boxplots of weights
20
par(mfrow = c(2, 1))
21
boxPR <−coredata(WPriorL1)
22
colnames(boxPR) <−ANames
23
boxplot(boxPR, ylim = c(−0.8, 0.8),
24
main = "Based on Prior Distribution")
25
abline(h = 0, col = "grey")
26
boxBL <−coredata(WBlL1)
27
colnames(boxBL) <−ANames
28
boxplot(boxBL, ylim = c(−0.8, 0.8),
29
main = "Based on Posterior Distribution")
30
abline(h = 0, col = "grey")
31
distributed. These three distribution speciﬁcations are included in the list object
RetViews which is utilized in the subsequent assignment of copula views. So far,
objects pertinent to the prior and the views distributions have been deﬁned. The
posterior distribution is then obtained by pooling of the random variates thereof. The
simulation size is set to 10 000 random draws and the simulated random values of
the posterior distributions are computed by invoking the function COPPosterior().
These values are contained in the slot posteriorSims and the prior distribution and
its parameters are contained in the slot marketDist.

TACTICAL ASSET ALLOCATION
297
DAX
SMI
CAC
FTSE
−0.5
0.0
0.5
Based on Prior Distribution
DAX
SMI
CAC
FTSE
−0.5
0.0
0.5
Based on Posterior Distribution
Figure 13.4
Boxplots of weights based on prior and BL distributions.
As shown in Listing 13.8, these simulated random variates can then be employed
to derive location and dispersion estimates. The listing concludes with an overview of
the density plots for the prior and posterior distributions for each of the four markets
considered. The outcome is shown in Figure 13.5.
Clearly, the prior and posterior densities for the CAC index pretty much overlap,
given that no view has been expressed for this market aggregate. The differences in
shape of the prior and posterior densities for the other aggregates primarily reﬂect
the differences between the sample means and the stated return forecasts.
Listing 13.9 shows how the allocations according to the MSR objective for the
Gaussian and skewed Student’s t prior distributions as well as according to the
BL and COP models are determined. The aim here is to convince the reader that
quite differing portfolio allocations can result, given the same set of inputs. The
portfolio optimizations are again conducted by means of the function tangency-
Portfolio(). Similar to the speciﬁcations for the Gaussian and BL MSR portfolio
solutions as shown earlier, two functions for recovering the moment estimates are
deﬁned ﬁrst. The function SSTPrior() returns the estimated location and disper-
sion values as given by the object MSTfit. The corresponding moment estimates
for the simulated random variates of the COP model are returned by the function

298
PORTFOLIO OPTIMIZATION APPROACHES
Listing 13.7 Copula opinion pooling.
## Prior distribution
1
## Fitting of skewed Student’s t distribution
2
MSTﬁt <−mvFit(R, method = "st")
3
mu <−c(MSTﬁt@ﬁt[["beta"]])
4
S <−MSTﬁt@ﬁt[["Omega"]]
5
skew <−c(MSTﬁt@ﬁt[["alpha"]])
6
df <−MSTﬁt@ﬁt[["df"]]
7
CopPrior <−mvdistribution("mvst", dim = NAssets, mu = mu,
8
Omega = S, alpha = skew, df = df)
9
## Pick matrix and view distributions for last forecast
10
RetEstCop <−ReturnEst[[27]]
11
RetEstCop
12
PCop <−matrix(0, ncol = NAssets, nrow = 3)
13
colnames(PCop) <−ANames
14
PCop[1, ANames[1]] <−1
15
PCop[2, ANames[2]] <−1
16
PCop[3, ANames[4]] <−1
17
Sds <−apply(R, 2, sd)
18
RetViews <−list(distribution("norm", mean = RetEstCop[1],
19
sd = Sds[1]),
20
distribution("norm", mean = RetEstCop[2],
21
sd = Sds[2]),
22
distribution("norm", mean = RetEstCop[4],
23
sd = Sds[4])
24
)
25
CopViews <−COPViews(pick = PCop, viewDist = RetViews,
26
conﬁdences = rep(0.5, 3),
27
assetNames = ANames)
28
## Simulation of posterior
29
NumSim <−10000
30
CopPost <−COPPosterior(CopPrior, CopViews,
31
numSimulations = NumSim)
32
slotNames(CopPost)
33
BlCopPost(). These two functions are then deﬁned as estimators in the portfolio
speciﬁcations for the MSR portfolios based on the skewed Student’s t distribution,
MSPriorSST, and on the opinion pooling framework, MSBlCop, respectively. Having
created these two remaining portfolio speciﬁcations, the solutions to the tangency
portfolios can be swiftly determined by including them in the list object PSpecs
and employing lapply(). The object POpt is again a list object with elements
of formal class fPORTFOLIO. The allocations can then be extracted by the using

TACTICAL ASSET ALLOCATION
299
Listing 13.8 Copula opinion pooling: Densities.
## Plotting of densities
1
CopPriorSim <−sampleFrom(CopPost@marketDist, NumSim)
2
CopPostSim <−CopPost@posteriorSims
3
oldpar <−par(no.readonly = TRUE)
4
par(mfrow = c(2, 2))
5
for(i in 1:NAssets){
6
plot(density(CopPostSim[, i]), main = ANames[i],
7
ylab = "density", ylim = c(0, 0.12), xlab = " ")
8
lines(density(CopPriorSim[, i]), col = "grey", lty = 2)
9
abline(v = mean(CopPostSim[, i]))
10
abline(v = mean(CopPriorSim[, i]), col = "grey", lty = 2)
11
legend("topleft", legend = c("Posterior", "Prior"),
12
lty = c(1, 2), col = c("black", "grey"), bty = "n")
13
}
14
par(oldpar)
15
lapply() and invoking the getWeights() method. Finally, the allocations are
made into a matrix object and expressed as percentages. The outcome is shown in
Table 13.4.
From a qualitative point of view, all allocations coincide. That is, long positions
would have been entered for the German, Swiss and British equity markets and a
short position is indicated for the French stock aggregate. However, the position sizes
do differ materially, between each of the two prior/posterior pairs and also across the
BL and COP models. Given that the BL and COP results were derived from the same
set of return forecasts, the latter observation shows the importance of specifying the
marginal distributions and dependence structure appropriately, as has been stressed
in Chapters 3 and 9.
13.6.3
Protection strategies
Overview
Wealth protection strategies have been known for a long time. Their origins can
probably be traced back to the work of Leland and Rubinstein (1976) where the
authors introduced the idea of option-based portfolio insurance (OBPI). In the 1980s
the concept of a constant proportion portfolio insurance (CPPI) was advocated by
Perold (1986) and Perold and Sharpe (1988). Extensions and modiﬁcations of the
latter approach were put forward by Black and Perold (1992) and Black and Jones
(1987). A comparison of these strategies was conducted by Black and Rouhani (1989)
and Bookstaber and Langsam (2000), for instance. It is fair to say that both of these
approaches were proposed during ﬁnancial crisis episodes in the aftermath of the oil

300
PORTFOLIO OPTIMIZATION APPROACHES
−10
0
10
20
0.00
0.04
0.08
0.12
DAX
density
Posterior
Prior
−20
−10
0
10
20
0.00
0.04
0.08
0.12
SMI
density
Posterior
Prior
−20
−10
0
10
20
0.00
0.04
0.08
0.12
CAC
density
Posterior
Prior
−15
−5
0
5
10
20
0.00
0.04
0.08
0.12
FTSE
density
Posterior
Prior
Figure 13.5
Prior and posterior densities.
price shocks and the ensuing deterioration of ﬁnancial wealth positions, primarily in
stocks.
This ﬁnal subsection, based on Pfaff (2008b), shows how a TAA-related protection
strategy can be implemented as an alternative to the OBPI/CPPI approaches. Similarly
to the insurance strategies, a portfolio ﬂoor value is deﬁned that may not be violated.
The difference between the current wealth and this ﬂoor is then used as a risk buffer,
where the risk budget is allocated between the assets according to their downside risk.
This constraint can be expressed in terms of a simple linear program with a return
maximization as its target. In addition to the risk budget, further constraints such as
budget, box and/or group constraints can be included in the linear program. The latter
can be derived from a forecasting model, as will be shown in due course. Hence,

TACTICAL ASSET ALLOCATION
301
Listing 13.9 Comparison of portfolio allocations.
## Deﬁning portfolio speciﬁcations
1
SSTPrior <−function(x, spec = NULL, ...){
2
list(mu = c(MSTﬁt@ﬁt$beta), Sigma = MSTﬁt@ﬁt$Omega)
3
}
4
BlCopPost <−function(x, spec = NULL, ...){
5
Sim <−CopPost@posteriorSims
6
list(mu = colMeans(Sim), Sigma = cov(Sim))
7
}
8
## Skewed Student’s t
9
MSPriorSST <−portfolioSpec()
10
setEstimator(MSPriorSST) <−"SSTPrior"
11
## BLCOP speciﬁcation
12
MSBlCop <−portfolioSpec()
13
setEstimator(MSBlCop) <−"BlCopPost"
14
## Tangency portfolios
15
R <−as.timeSeries(R)
16
BLR <−PostDist[[27]]
17
PSpecs <−list(MSPrior, MSBl, MSPriorSST, MSBlCop)
18
POpt <−lapply(PSpecs, function(x)
19
tangencyPortfolio(data = R, spec = x,
20
constraints = BoxC)
21
)
22
PWeights <−unlist(lapply(POpt, getWeights))
23
Weights <−matrix(PWeights, ncol = NAssets, nrow = 4,
24
byrow = TRUE) ∗100
25
colnames(Weights) <−ANames
26
rownames(Weights) <−c("Gauss", "Skewed Student’s t",
27
"BL", "BLCop")
28
Weights
29
Table 13.4
Comparison of portfolio allocations.
Model
DAX
SMI
CAC
FTSE
Gauss
40.00
78.18
−34.01
15.84
Skewed Student’s t
51.30
30.18
−42.87
61.39
BL
60.47
59.70
−22.39
2.22
BLCOP
44.22
29.31
−7.96
34.43

302
PORTFOLIO OPTIMIZATION APPROACHES
the three pieces that are put together are a forecasting model (univariate models or
multivariate), a risk model and a linear program.
Data preparation
In Listing 13.10 the data used in the simulation is prepared. It is assumed that the TAA
part of the portfolio can be allocated to the stock markets of the US, Great Britain,
France, Germany, Japan and Hong Kong (China) represented by the S&P 500, FTSE
100, CAC 40, DAX, Nikkei 225 and Hang Seng (HSI) indexes, respectively. The
focus is on a non-hedged, euro-based, long-only investor.
First the packages required for conducting the simulation are brought into mem-
ory. The package FRAPO contains the data sets and loads the packages timeSeries
and Rglpk. The package evir will be used to assess the downside risk, and the
package forecast to derive the one-step-ahead return expectations. Next the data sets
StockIndexAdjD and ESCBFX are brought into the work space. The former contains
the adjusted daily closing prices of the indexes and the latter the ESCB reference
rates, which will be employed to denominate the market values in euros. Both data
sets are converted timeSeries objects. The FX data set is curtailed to the sample
end of the stock data and the daily time stamps thereof are assigned to the object
DDates. These dates are then used to extract the dates that fall on Wednesdays. It
is also necessary to create a time date series of Wednesdays for the entire sample
period, given that some public holidays coincide with this day of the week. If this
happens, the last observed daily observation is carried forward, such that ﬁnally the
objects PWeekly and FXWeekly consist of consecutive regular time series data sets
for the equity and currency markets. In lines 25–31 the levels of the foreign stock
indexes are converted to euro-based values (object PEWeekly), the discrete decimal
returns thereof are computed and assigned to the object REDWeekly and the log levels
are stored in PELWeekly. In the last block of statements the time dates of the mov-
ing window utilized in the forthcoming simulation are created and the size thereof
and the names of the stock indexes are assigned to the objects size and NAssets,
all used in the ensuing examples. The moving window has been set to a length of
roughly 5 years. Finally, the trajectory of the euro-denominated index values is created
(see Figure 13.6).
Forecasting model
The ﬁrst piece of the protection strategy is a forecasting model. In this example
the return expectations are taken as the one-step-ahead predictions from a simple
ARIMA(1, 1, 1) model. To produce these forecasts for the back-test period, two
functions are deﬁned in Listing 13.11. The ﬁrst, Forecast(), is designed to take
a log-level series as argument x and an order speciﬁcation as argument order. In
the function body, the coefﬁcients of the ARIMA model are estimated and the one-
step-ahead point forecast is produced. The return expectation is then formed as the
difference between the latest log value and the point forecast. The functions and
methods of package forecast are employed. RetExp() is a wrapper function for

TACTICAL ASSET ALLOCATION
303
Listing 13.10 Data preparation.
library(FRAPO)
1
library(evir)
2
library(forecast)
3
## Data preparation
4
data(StockIndexAdjD)
5
data(ESCBFX)
6
PDaily <−timeSeries(StockIndexAdjD,
7
charvec = rownames(StockIndexAdjD))
8
FXDaily <−timeSeries(ESCBFX, charvec = rownames(ESCBFX))
9
FXDaily <−window(FXDaily, start = start(FXDaily),
10
end = end(PDaily))
11
DDates <−time(FXDaily)
12
WedDays <−isWeekday(DDates, wday = 3)
13
FirstWed <−head(which(WedDays, arr.ind = TRUE), 1)
14
LastWed <−tail(which(WedDays, arr.ind = TRUE), 1)
15
AllWedDays <−timeSequence(from = DDates[FirstWed],
16
to = DDates[LastWed],
17
by = "week")
18
DumWed <−timeSeries(rep(1, length(AllWedDays)),
19
charvec = AllWedDays)
20
PWeekly <−interpNA(cbind(DumWed, PDaily),
21
method = "before")[AllWedDays, −1]
22
FXWeekly <−interpNA(cbind(DumWed, FXDaily),
23
method = "before")[AllWedDays, −1]
24
PEWeekly <−PWeekly
25
PEWeekly[, "SP500"] <−PWeekly[, "SP500"] / FXWeekly[, "USD"]
26
PEWeekly[, "N225"] <−PWeekly[, "N225"] / FXWeekly[, "JPY"]
27
PEWeekly[, "FTSE100"] <−PWeekly[, "FTSE100"] / FXWeekly[, "GBP"]
28
PEWeekly[, "HSI"] <−PWeekly[, "HSI"] / FXWeekly[, "HKD"]
29
REDWeekly <−(PEWeekly / lag(PEWeekly, k = 1) −1)
30
PELWeekly <−log(PEWeekly)
31
## Deﬁning moving window and indexes
32
epoints <−time(PELWeekly)[−c(1:259)]
33
size <−length(epoints)
34
spoints <−time(PELWeekly)[1:size]
35
idx <−1:size
36
NAssets <−ncol(PEWeekly)
37
## Time series chart of euro−denominated indexes
38
plot(PEWeekly, main = " ", xlab = " ", col = "black")
39

304
PORTFOLIO OPTIMIZATION APPROACHES
600
1000
1400
SP500
100
150
200
N225
4000
7000
10000
1999−01−06
2004−01−02
2008−12−29
FTSE100
3000
5000
CAC40
2000
4000
6000 8000
GDAX
1000
2000
1999−01−06
2004−01−02
2008−12−29
HSI
Figure 13.6
Trajectory of stock index values in euros.
Listing 13.11 Forecasting model.
## Producing one−step−ahead forecasts
1
Forecast <−function(x, order = c(1, 1, 1), ...){
2
mod <−arima(x, order = order, ...)
3
f1 <−forecast(mod, h = 1)$mean
4
re <−(f1 −tail(x, 1)) ∗100
5
re
6
}
7
RetExp <−function(x, order = c(1, 1, 1), ...){
8
ans <−apply(x, 2, Forecast, order = order, ...)
9
ans
10
}
11
## Computing return forecasts
12
RE <−fapply(PELWeekly, from = spoints, to = epoints, FUN = RetExp)
13
head(RE)
14
tail(RE)
15

TACTICAL ASSET ALLOCATION
305
Forecast(), in the sense that the latter is applied per column if a multivariate object
is used as input. The return forecasts are then swiftly returned by utilizing fapply()
on the weekly log prices.
Risk model
The second piece consists of a market price risk model. In Listing 13.12 the ES for
a conﬁdence level of 95% is used. This downside risk measure is derived from the
generalized Pareto distribution. Because the riskiness is determined for each loss
series alone, the weighted sum thereof will form an upper bound for the overall
portfolio downside risk. That is, a perfect concordance of the stock losses is assumed.
Given that losses are expressed as positive numbers, in line 3 the sign factors are
determined from the return expectations which will then be used in altering the sign
of the actual returns such that these comply with the pertinent losses from long or
short positions. In line 4 a matrix object ES is deﬁned to hold the relevant downside
Listing 13.12 Risk model.
## Computing market risk measures
1
## Long/short risk
2
RiskFac <−−1.0 ∗sign(RE)
3
ES <−matrix(0, ncol = NAssets, nrow = size)
4
GpdEs <−function(x, nextremes = 30, method = "pwm", level = 0.95,
5
RiskFac){
6
x <−RiskFac ∗x
7
mod <−gpd(data = x, nextremes = nextremes, method = method)
8
GpdEs <−riskmeasures(mod, level)[3]
9
GpdEs
10
}
11
for(i in idx){
12
DatSub <−window(REDWeekly, start = spoints[i],
13
end = epoints[i])
14
FacSub <−window(RiskFac, start = epoints[i],
15
end = epoints[i])
16
for(j in 1:6){
17
x <−na.omit(DatSub[, j])
18
ES[i, j] <−GpdEs(x, RiskFac = c(FacSub[, j]))
19
}
20
}
21
ES <−timeSeries(ES, charvec = epoints)
22
colnames(ES) <−colnames(REDWeekly)
23
head(ES)
24
tail(ES)
25

306
PORTFOLIO OPTIMIZATION APPROACHES
Listing 13.13 Formulation of the linear program.
## Creating LP
1
Lp <−function(RE, ES, Buffer, ub = 0.4){
2
obj <−as.vector(RE)
3
## Initialise LHS matrix and RHS vector
4
nvar <−length(obj)
5
## Wealth constraint
6
a1 <−rep(1, nvar)
7
b1 <−1
8
d1 <−"<="
9
## Risk constraint
10
a2 <−as.vector(ES)
11
b2 <−Buffer
12
d2 <−"<="
13
## Upper bound
14
a3 <−diag(nvar)
15
b3 <−rep(ub, nvar)
16
d3 <−rep("<=", nvar)
17
## Combining
18
A <−rbind(a1, a2, a3)
19
b <−c(b1, b2, b3)
20
d <−c(d1, d2, d3)
21
ans <−Rglpk_solve_LP(obj, mat = A, dir = d, rhs = b, max = TRUE)
22
ans
23
}
24
risk estimates. Next the function GpdEs() is created. In its body, the 30 largest
losses are considered in the estimation of the GPD parameters. The EVT-based risk
measures are then recovered from the output of the function riskmeasures(). The
elements of the matrix ES are next ﬁlled by means of a double for loop, where in the
outer loop the subsample of the discrete returns is extracted from REDWeekly and
the trade/loss direction from the object RiskFac. In the inner loop, the risk measures
are determined for each stock market. In the last block of statements the results are
converted to an object of class timeSeries.
Formulation of linear program
The third and ﬁnal piece is the formulation of the linear program. This is contained
in the function Lp() shown in Listing 13.13. The function is speciﬁed with four
arguments. The ﬁrst, RE, takes the return expectations which are the parameters of
the objective function to be maximized. The second, ES, holds the downside risk
ES estimates as derived from the GPD. The object Buffer is for the risk budget,

TACTICAL ASSET ALLOCATION
307
Listing 13.14 Portfolio simulation.
## Recursive back−test
1
## Initialising variables
2
LO <−timeSeries(rep(NA, size), charvec = epoints)
3
LO[1] <−100
4
PLevel <−0.9
5
FLO <−LO[1, ] ∗PLevel
6
Returns <−REDWeekly[epoints, ]
7
MoneyRate <−1.01∧(1/52) −1
8
## Simulation
9
for(i in 2:size){
10
BLO <−c(PLevel ∗LO[i −1, ])
11
if(BLO < FLO){
12
LO[i, ] <−LO[i −1, ] ∗(1 + MoneyRate)
13
} else {
14
re <−c(RE[i −1, ])
15
if(all(re <= 0)){
16
LO[i, ] <−LO[i −1, ] ∗(1 + MoneyRate)
17
} else {
18
es <−c(ES[i −1, ])
19
r <−c(Returns[i, ])
20
B <−c(LO[i −1, ]) / c(FLO) −1
21
ans <−Lp(RE = re, ES = es, Buffer = B, ub = 0.4)
22
w <−ans$solution
23
LO[i, ] <−LO[i −1, ] ∗(1 + t(w) %*% c(r))
24
}
25
}
26
}
27
such that the portfolio’s downside risk as a weighted sum of its components shall
not exceed this limit. The last argument, ub will be used as an upper bound on the
portfolio weights in order to circumvent concentrated portfolio solutions.
In the ﬁrst line of the function’s body, the return expectations are coerced into
a vector and assigned to the object obj, which will be employed later in the call to
Rglpk_solve_LP(). The number of portfolio constituents is determined next. The
following three blocks of statements contain: (i) the budget constraint, such that the
sum of weights shall not exceed unity, (ii) the risk constraint, such that the portfolio
risk shall not exceed the buffer, and (iii) the upper bound constraints. In each of these
three blocks the row-wise entries of the constraint matrix, the inequalities and the
right-hand-side values are speciﬁed. These items are then collected into the matrix
object A, the inequality vector d and the binding constraint values b. These objects
are then used in the call to the linear program function of the package Rglpk. The

308
PORTFOLIO OPTIMIZATION APPROACHES
Listing 13.15 Comparison of portfolio allocations.
## Equal−weighted long−only strategy
1
EwRetfac <−1 + rowMeans(Returns)
2
EwRetfac[1] <−100
3
EW <−timeSeries(cumprod(EwRetfac), epoints)
4
## Plot of portfolio wealth
5
ylims <−range(cbind(LO, EW))
6
plot(LO, ylim = ylims, xlab = " ", ylab = "Index")
7
lines(EW, col = "grey", lty = 2)
8
legend("topleft",
9
legend = c("TAA long−only", "EW long−only"),
10
lty = 1:2, col = c("black", "grey"))
11
## Portfolio analytics
12
library(PerformanceAnalytics)
13
## Portfolio returns
14
LORet <−returns(LO, method = "discrete", percentage = FALSE,
15
trim = TRUE)
16
EWRet <−returns(EW, method = "discrete", percentage = FALSE,
17
trim = TRUE)
18
## VaR
19
LOVAR <−−100 ∗VaR(LORet, p = 0.95, method = "gaussian")
20
EWVAR <−−100 ∗VaR(EWRet, p = 0.95, method = "gaussian")
21
## ES
22
LOES <−−100 ∗ES(LORet, p = 0.95, method = "gaussian")
23
EWES <−−100 ∗ES(EWRet, p = 0.95, method = "gaussian")
24
## Sharpe
25
LOSR <−SharpeRatio(LORet)
26
EWSR <−SharpeRatio(EWRet)
27
## Annualised returns
28
LORA <−Return.annualized(LORet, scale = 52)
29
EWRA <−Return.annualized(EWRet, scale = 52)
30
function returns a list object with the solution and further output as described in
Section 12.5.3.
Portfolio simulation
The previously created objects are now used in a portfolio simulation. Given the
path dependency of the risk budget, the back-test is cast in a for loop as shown in
Listing 13.14. But ﬁrst, an object LO of class timeSeries is created in which the
portfolio equity is stored. It is initialized to 100 monetary units. Next, the portfolio’s
ﬂoor level is deﬁned and the returns pertinent to the back-testing period are extracted.

TACTICAL ASSET ALLOCATION
309
Index
2003−12−24
2006−12−25
2009−12−27
80
100
120
140
160
180
200
TAA long−only
EW long−only
Figure 13.7
Portfolio wealth trajectory.
It is further assumed that the funds can be invested at a money market rate of 1%
per annum. Within the for loop the 90% current wealth level is determined and
compared to the portfolio ﬂoor to be protected. Only if the former is greater than the
latter is the optimization carried out. Otherwise, the remaining funds earn interest at
the money market rate until the initial ﬂoor level is exceeded again. Given that the
forecasts for all markets can be negative at the same time, and hence a long-only
investor would lose from exposure to any of these markets, the wealth would also
earn interest only. This situation is checked in the inner if/else clause. In the ﬁnal
lines of the listing, the solution of the linear program is recovered and the proceeds
from the current allocation are expressed in terms of the portfolio wealth.
Analysis of results
The simulated wealth trajectory is now contained in the object LO. The outcome of
this strategy is compared with the solution of an equal-weighted strategy. In Listing
13.15 the portfolio equity according to the equal-weighted strategy is computed ﬁrst
and the two equity curves are plotted in Figure 13.7.
Table 13.5
Key portfolio performance statistics.
Statistics
TAA long-only
Equal-weighted long-only
VaR 95%
3.37
3.64
VaR 95%
4.27
4.59
Sharpe ratio
0.09
0.04
Return annualized %
8.76
3.50

310
PORTFOLIO OPTIMIZATION APPROACHES
Clearly, the trajectory according to the TAA strategy leads to a higher wealth level
at the end of the simulation period than the the equal-weighted strategy. Furthermore,
the draw-down witnessed during the ﬁnancial market crisis is less pronounced in the
case of the former strategy. These two characteristics are also mirrored by the perfor-
mance and risk-related characteristics as computed in the ﬁnal part of Listing 13.15
and shown in Table 13.5.
References
Akaike H. 1981 Likelihood of a model and information criteria. Journal of Econometrics 16,
3–14.
Banerjee A., Dolado J., Galbraith J. and Hendry D. 1993 Co-integration, Error-Correction,
and the Econometric Analysis of Non-stationary Data. Oxford University Press, New York.
Black F. 1989 Universal hedging: Optimizing currency risk and reward in international equity
portfolios. Financial Analysts Journal pp. 16–22.
Black F. and Jones R. 1987 Simplifying portfolio insurance. Journal of Portfolio Management
pp. 48–51.
Black F. and Litterman R. 1990 Asset allocation: combining investor views with market
equilibrium. Technical report, Goldman Sachs Fixed Income Research.
Black F. and Litterman R. 1991 Global asset allocation with equities, bonds, and currencies.
Technical report, Goldman Sachs Fixed Income Research.
Black F. and Litterman R. 1992 Global portfolio optimization. Financial Analysts Journal
48(5), 28–43.
Black F. and Perold A. 1992 Theory of constant proportion portfolio insurnace. Journal of
Economics, Dynamics & Control 16(3/4), 403–426.
Black F. and Rouhani R. 1989 Constant proportion portfolio insurance and the synthetic put
option: a comparison In Institutional Investor Focus on Investment Management (ed. Fabozzi
FJ.) Ballinger Cambridge, MA pp. 695–708.
Bookstaber R. and Langsam J. 2000 Portfolio insurance trading rules. Journal of Futures
Markets 8, 15–31.
Box G. and Jenkins G. 1976 Time Series Analysis: Forecasting and Control revised edn.
Holden-Day, San Francisco.
Brandt P. 2011 MSBVAR: Markov-Switching, Bayesian, Vector Autoregression Models. R
package version 0.6-0.
Dickey DA. and Fuller WA. 1979 Distributions of the estimators for autoregressive time series
with a unit root. Journal of the American Statistical Association 74, 427–431.
Dickey DA. and Fuller WA. 1981 Likelihood ratio statistics for autoregressive time series with
a unit root. Econometrica 49, 1057–1072.
Diebold F. and Mariano R. 1995 Comparing predictive accuracy. Journal of Business &
Economic Statistics 13, 253–263.
Elliott G., Rothenberg T. and Stock J. 1996 Efﬁcient tests for an autoregressive unit root.
Econometrica 64(4), 813–836.
Engle R. and Granger C. 1987 Co-integration and error correction: Representation, estimation
and testing. Econometrica 55, 251–276.

TACTICAL ASSET ALLOCATION
311
Fair R. 1984 Speciﬁcation, Estimation, and Analysis of Macroeconomic Models. Harvard
University Press, Cambridge, MA.
Fair R. 1998 Testing Macroeconometric Models. Harvard University Press, Cambridge, MA.
Fair R. 2004 Estimating How the Macroeconomy Works. Harvard University Press, Cambridge,
MA.
Fraley C., Leisch F., Maechler M., Reisen V. and Lemonte A. 2012 fracdiff: Fractionally
differenced ARIMA aka ARFIMA(p,d,q) models. R package version 1.4-1.
Gilbert P. 2011 EvalEst: Dynamic Systems Estimation – extensions. R package version
2011.11-1.
Gilbert P. 2012a Brief User’s Guide: Dynamic Systems Estimation.
Gilbert P. 2012b tframe: Time Frame coding kernel. R package version 2012.3-1.
Golan A., Judge G. and Miller D. 1996 Maximum Entropy Econometrics Series in Financial
Economics and Quantitative Analysis. John Wiley & Sons, Ltd., Chichester.
Granger CWJ. 1981 Some properties of time series data and their use in econometric model
speciﬁcation. Journal of Econometrics 16, 121–130.
Greene W. 2008 Econometric Analysis 7th edn. Prentice Hall, Upper Saddle River, NJ.
Hamilton JD. 1994 Time Series Analysis. Princeton University Press, Princeton, NJ.
Hannan E. and Quinn B. 1979 The determination of the order of an autoregression. Journal of
the Royal Statistical Society, Series B 41, 190–195.
He G. and Litterman R. 2002 The intuition behind Black-Litterman model portfolios. Work-
ing paper, Goldman Sachs Group, Inc. Quantitative Strategy Group, http://ssrn.com/
abstract=334304.
Hendry DF. 1995 Dynamic Econometrics. Oxford University Press, Oxford.
Henningsen A. and Hamann J. 2007 systemﬁt: A package for estimating systems of simulta-
neous equations in R. Journal of Statistical Software 23(4), 1–40.
Hyndman R. 2012 forecast: Forecasting functions for time series. R package version 3.20.
Johansen S. 1988 Statistical analysis of cointegration vectors. Journal of Economic Dynamics
and Control 12, 231–254.
Johansen S. 1991 Estimation and hypothesis testing of cointegration vectors in Gaussian vector
autoregressive models. Econometrica 59(6), 1551–1580.
Johansen S. 1995 Likelihood-Based Inference in Cointegrated Vector Autoregressive Models
Advanced Texts in Econometrics. Oxford University Press, Oxford.
Johansen S. and Juselius K. 1990 Maximum likelihood estimation and inference on cointe-
gration – with applications to the demand for money. Oxford Bulletin of Economics and
Statistics 52(2), 169–210.
Johansen S. and Juselius K. 1992 Testing structural hypothesis in a multivariate cointegration
analysis of the PPP and the UIP for UK. Journal of Econometrics 53, 211–244.
Judge G., Hill R., Grifﬁths W., L¨utkepohl H. and Lee T. 1985 The Theory and Practice of
Econometrics 2nd edn. John Wiley & Sons, Inc., New York.
Judge G., Hill R., Grifﬁths W., L¨utkepohl H. and Lee T. 1988 Introduction to the Theory and
Practice of Econometrics 2nd edn. John Wiley & Sons, Inc., New York.
Kwiatkowski D., Phillips P., Schmidt P. and Y. Shin 1992 Testing the null hypothesis of
stationarity against the alternative of a unit root: How sure are we that economic time series
have a unit root?. Journal of Econometrics 54, 159–178.

312
PORTFOLIO OPTIMIZATION APPROACHES
Lee W. 2000 Theory and Methodology of Tactical Asset Allocation. Frank J. Fabozzi Associates,
New Hope, PA.
Leland H. and Rubinstein M. 1976 The evolution of portfolio insurance In Portfolio Insurance:
A Guide to Dynamic Hedging (ed. Luskin DL.) John Wiley & Sons, Inc. New York.
L¨utkepohl H. 2006 New Introduction to Multiple Time Series Analysis. Springer-Verlag,
New York.
L¨utkepohl H. and Kr¨atzig M. 2004 Applied Time Series Econometrics. Cambridge University
Press, Cambridge.
L¨utkepohl H., Saikkonen P. and Trenkler C. 2004 Testing for the cointegrating rank of a VAR
with level shift at unknown time. Econometrica 72(2), 647–662.
MacKinnon J. 1991 Critical values for cointegration tests In Long-Run Economic Relation-
ships: Readings in Cointegration (ed. Engle RF. and Granger CWJ.) Advanced Texts in
Econometrics Oxford University Press Oxford, UK chapter 13.
MacKinnon J. 1996 Numerical distribution functions for unit root and cointegration tests.
Journal of Applied Econometrics 11, 601–618.
Markowitz H. 1952 Portfolio selection. Journal of Finance 7(1), 77–91.
Meucci A. 2006a Beyond Black-Litterman in practice: A ﬁve-step recipe to input views on
non-normal markets. Risk 19(9), 114–119.
Meucci A. 2006b Beyond Black-Litterman: Views on non-normal markets. Risk 19(2), 96–102.
Meucci A. 2010a The Black-Litterman approach: Original model and extensions In The En-
cyclopedia of Quantitative Finance (ed. Cont R. and Tankov P.) John Wiley & Sons, Inc.
Hoboken, NJ.
Meucci A. 2010b Fully ﬂexible views: Theory and practice. Working paper, Symmys,
http://ssrn.com/abstract=1213325.
Perold A. 1986 Constant portfolio insurance Harvard Business School.
Perold A. and Sharpe W. 1988 Dynamic strategies for asset allocation. Financial Analyst
Journal pp. 16–27.
Pfaff B. 2007 UseR in the ﬁnancial sector International Workshop on Computational and
Financial Econometrics, Geneva, Switzerland.
Pfaff B. 2008a Analysis of Integrated and Cointegrated Time Series with R 2nd edn. Springer,
New York.
Pfaff B. 2008b Tactical asset allocation: Putting the pieces together 2nd International
R/Rmetrics User and Developer Workshop, Meielisalp, Lake Thune, Switzerland.
Pfaff B. 2008c VAR, SVAR and SVEC models: Implementation within R package vars. Journal
of Statistical Software.
Phillips P. and Ouliaris S. 1990 Asymptotic properties of residual based tests for cointegration.
Econometrica 58, 165–193.
Phillips P. and Perron P. 1988 Testing for a unit root in time series regression. Biometrika
75(2), 335–346.
Quinn B. 1980 Order determination for multivariate autoregression. Journal of the Royal
Statistical Society, Series B 42, 182–185.
Ryan J. and Ulrich J. 2012 xts: eXtensible Time Series. R package version 0.8-6.
Said S. and Dickey D. 1984 Testing for unit roots in autoregressive-moving average models of
unknown order. Biometrika 71, 599–607.

TACTICAL ASSET ALLOCATION
313
Schmidt P. and Phillips P. 1992 LM tests for a unit root in the presence of deterministic trends.
Oxford Bulletin of Economics and Statistics 54(3), 257–287.
Schwarz H. 1978 Estimating the dimension of a model. Annals of Statistics 6, 461–464.
Sharpe W. 1964 Capital asset prices: A theory of market equilibrium. Journal of Finance
pp. 425–442.
Sharpe W. 1974 Imputing expected security returns from portfolio composition. Journal of
Financial and Quantitative Analysis pp. 463–472.
Sims C. and Zha T. 1998 Bayesian methods for dynamic multivariate models. International
Economic Review 39(4), 949–968.
Sims CA. 1980 Macroeconomics and reality. Econometrica 48, 1–48.
Takayanagi S. and Ishikawa K. 2012 PairTrading: Classical pair trading based on cointegra-
tion in ﬁnance. R package version 1.1.
Trapletti A. and Hornik K. 2012 tseries: Time Series Analysis and Computational Finance. R
package version 0.10-28.
Ulrich J. 2012 TTR: Technical Trading Rules. R package version 0.21-1.
Waggoner D. and Zha T. 2003 A Gibbs sampler for structural vector autoregressions. Journal
of Economic Dynamics & Control 28, 349–366.
W¨urtz D. 2009a fTrading: Technical Trading Analysis. R package version 2100.76.
W¨urtz D. 2009b fUnitRoots: Trends and Unit Roots. R package version 2100.76.
W¨urtz D. 2012 fArma: ARMA Time Series Modelling. R package version 2160.77.
Zivot E. and Andrews D. 1992 Further evidence on the Great Crash, the Oil-Price Shock, and
the Unit-Root Hypothesis. Journal of Business & Economic Statistics 10(3), 251–270.

Appendix A
Package overview
The R packages cited and/or used are listed in this appendix. In Table A.1 the
packages are ordered alphabetically and their descriptions are provided. In Table A.2
the packages are sorted by topic, and the package version numbers, dates and sources
are provided. All of these package have been updated prior to publishing and the
example code contained in this book has been processed with these packages.
A.1
Packages in alphabetical order
Table A.1
Packages in alphabetical order.
Name
Title
AER
Applied Econometrics with R
bayesGARCH
Bayesian Estimation of the GARCH(1, 1) Model with
Student-t Innovations
BLCOP
Black–Litterman and copula-opinion pooling
frameworks
ccgarch
Conditional Correlation GARCH models
chron
Chronological objects which can handle dates and times
coda
Output analysis and diagnostics for MCMC
copula
Multivariate Dependence with Copulas
covRobust
Robust Covariance Estimation via Nearest Neighbor
Cleaning
ctv
CRAN Task Views
date
Functions for handling dates
Davies
The Davies quantile function
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

APPENDIX A: PACKAGE OVERVIEW
315
Table A.1
(continued)
Name
Title
DEoptim
Global optimization by Differential Evolution
dse
Dynamic Systems Estimation (time series package)
EvalEst
Dynamic Systems Estimation – extensions
evd
Functions for extreme value distributions
evdbayes
Bayesian Analysis in Extreme Value Theory
evir
Extreme Values in R
extRemes
Extreme value toolkit
fArma
ARMA Time Series Modelling
fBasics
Rmetrics – Markets and Basic Statistics
fCopulae
Rmetrics – Dependence Structures with Copulas
fEcoﬁn
Economic and Financial Data Sets
fExtremes
Rmetrics – Extreme Financial Market Data
fGarch
Rmetrics – Autoregressive Conditional Heteroskedastic
Modelling
forecast
Forecasting functions for time series
fPortfolio
Rmetrics – Portfolio Selection and Optimization – ebook
available at www.rmetrics.org
fPortfolioBacktest
Rmetrics – Portfolio Backtesting
FRAPO
Financial Risk Modelling and Portfolio Optimization
with R
fTrading
Technical Trading Analysis
fts
R interface to tslib (a time series library in C++)
fUnitRoots
Trends and Unit Roots
GeneralizedHyperbolic
The generalized hyperbolic distribution
ghyp
A package on the generalized hyperbolic distribution
and its special cases
gld
Estimation and use of the generalised (Tukey) lambda
distribution
glpkAPI
R Interface to C API of GLPK
gogarch
Generalized Orthogonal GARCH (GO-GARCH) models
gumbel
Gumbel copula
Hmisc
Harrell Miscellaneous
ismev
An Introduction to Statistical Modeling of Extreme
Values
its
Irregular Time Series
limSolve
Solving Linear Inverse Models
linprog
Linear Programming/Optimization
lmomco
L-moments, Censored L-moments, Trimmed
L-moments, L-comoments, and Many Distributions
(continued)

316
APPENDIX A: PACKAGE OVERVIEW
Table A.1
(continued)
Name
Title
lpSolve
Interface to Lp_solve v. 5.5 to solve linear/integer
programs
lpSolveAPI
R Interface for lp_solve version 5.5.2.0
MASS
Support Functions and Datasets for Venables and
Ripley’s MASS
MSBVAR
Markov-Switching, Bayesian, Vector Autoregression
Models
nacopula
Nested Archimedean Copulas
PairTrading
Classical pair trading based on cointegration in ﬁnance
PerformanceAnalytics
Econometric tools for performance and risk analysis.
PortfolioAnalytics
Portfolio Analysis, including Numeric Methods for
Optimization of Portfolios
POT
Generalized Pareto Distribution and Peaks Over
Threshold
QRM
Provides R-language code to examine Quantitative Risk
Management concepts
quadprog
Functions to solve Quadratic Programming Problems
quantmod
Quantitative Financial Modelling Framework
Rcpp
Seamless R and C++ Integration
RcppArmadillo
Rcpp integration for Armadillo templated linear algebra
library
RcppDE
Global optimization by differential evolution in C++
Renext
Renewal method for extreme values extrapolation
Rglpk
R/GNU Linear Programming Kit Interface
rmgarch
Multivariate GARCH models
rneos
XML-RPC Interface to NEOS
robust
Insightful Robust Library
robustbase
Basic Robust Statistics
rportfolios
Random portfolio generation
rrcov
Scalable Robust Estimators with High Breakdown Point
Rsocp
An R extension library to use SOCP from R
Rsymphony
Symphony in R
rugarch
Univariate GARCH models
RUnit
R Unit test framework
SkewHyperbolic
The Skew Hyperbolic Student t-distribution
sos
sos
systemﬁt
Estimating Systems of Simultaneous Equations
tawny
Provides various portfolio optimization strategies
including random matrix theory and shrinkage
estimators

APPENDIX A: PACKAGE OVERVIEW
317
Table A.1
(continued)
Name
Title
tframe
Time Frame coding kernel
tframePlus
Time Frame coding kernel extensions
timeDate
Rmetrics – Chronological and Calendarical Objects
timeSeries
Rmetrics – Financial Time Series Objects
tis
Time Indexes and Time Indexed Series
tseries
Time series analysis and computational ﬁnance
TTR
Technical Trading Rules
urca
Unit root and cointegration tests for time series data
VarianceGamma
The Variance Gamma Distribution
vars
VAR Modelling
xts
eXtensible Time Series
zoo
S3 Infrastructure for Regular and Irregular Time Series
(Z’s ordered observations)
A.2
Packages ordered by topic
Table A.2
Packages ordered by category.
Name
Version
Date
Source
Copula
BLCOP
0.2.6
2011-05-22
Gochez (2010)
copula
0.99-1
2012-03-27
Kojadinovic and Yan (2010)
fCopulae
2110.78
2009-10-27
W¨urtz (2009a)
gumbel
1.03
2011-10-01
Caillat et al. (2008)
nacopula
0.7-9-1
2012-03-30
Hofert and Maechler (2011)
QRM
0.4-7
2012-04-27
Pfaff (2012b)
EVT
evd
2.2-4
2008-07-12
Stephenson (2002)
evdbayes
1.0-8
2010-01-14
Stephenson and Ribatet (2010)
evir
1.7-1
2010-08-09
McNeil and Stephenson (2011)
extRemes
1.63
2004-03-15
Gilleland and Katz (2011)
fExtremes
2100.77
2009-09-28
W¨urtz (2009c)
ismev
1.36
2009-14-07
Coles and Stephenson (2011)
POT
1.1-1
2011-04-01
Ribatet (2011)
QRM
0.4-7
2012-04-27
Pfaff (2012b)
Renext
1.0-0
2010-06-10
Deville (2012)
(continued)

318
APPENDIX A: PACKAGE OVERVIEW
Table A.2
(continued)
Name
Version
Date
Source
GARCH
bayesGARCH
1-00.10
2011-04-14
Ardia (2011)
ccgarch
0.2.0
2010-03-20
Nakatani (2010)
fGarch
2110.80
2009-11-09
W¨urtz and Chalabi (2009)
gogarch
0.7-0
2011-03-21
Pfaff (2012a)
rmgarch
0.94
2012-03-16
Ghalanos (2012a)
rugarch
1.0-5
2011-10-15
Ghalanos (2012b)
tseries
0.10-27
2011-10-23
Trapletti and Hornik (2012)
GHD
fBasics
2110.79
2010-02-08
W¨urtz (2012b)
GeneralizedHyperbolic
0.2-0
2010-02-24
Scott (2011)
ghyp
1.5.5
2011-03-30
Luethi and Breymann (2011)
QRM
0.4-7
2012-04-27
Pfaff (2012b)
SkewHyperbolic
0.2-0
2010-02-10
Scott and Grimson (2010)
VarianceGamma
0.3-0
2010-02-10
Scott and Dong (2010)
GLD
Davies
1.1-7
2011-08-11
Hankin and Lee (2006)
fBasics
2110.79
2010-02-08
W¨urtz (2012b)
gld
1.9.2
2011-10-31
King (2011)
lmomco
1.4.3
2011-08-30
Asquith (2012)
Optimization
DEoptim
2.2-0
2012-04-06
Mullen et al. (2011)
glpkAPI
1.2.1
2012-03-15
Gelius-Dietrich (2012)
limSolve
1.5.2
2010-04-01
Soetaert et al. (2009)
linprog
0.9-0
2010-04-23
Henningsen (2010)
lpSolve
5.6.6
2011-04-19
Berkelaar (2011)
lpSolveAPI
5.5.2.0-5
2011-07-28
Konis (2011)
quadprog
1.5-4
2011-05-11
Turlach and Weingessel (2011)
RcppDE
0.1.0
2012-04-08
Eddelbuettel (2012)
Rglpk
0.3-5
2010-02-15
Theussl and Hornik (2012)
rneos
0.2-6
2011-04-11
Pfaff (2011)
Rsocp
271.1
2008-31-01
Chalabi and W¨urtz (2010)
Rsymphony
0.1-14
2011-12-27
Harter et al. (2012)
Portfolio
fPortfolio
2130.80
2011-02-10
W¨urtz et al. (2010a)
fPortfolioBacktest
2110.4
2009-05-11
W¨urtz et al. (2010b)
PerformanceAnalytics
1.0.3.2
2010-09-14
Carl et al. (2012)
PortfolioAnalytics
0.6.1
2011-08-13
Boudt et al. (2011)
rportfolios
1.0
2010-11-23
Novomestky (2012)
tawny
2.0.2
2012-02-07
Rowe (2012)

APPENDIX A: PACKAGE OVERVIEW
319
Table A.2
(continued)
Name
Version
Date
Source
Robust
covRobust
1.0
2003-12-04
Wang et al. (2003)
fPortfolio
2130.80
2011-02-10
W¨urtz et al. (2010a)
MASS
7.3-18
2012-04-26
Venables and Ripley (2002)
robust
0.3-11
2010-04-24
Wang et al. (2010)
robustbase
0.7-8
2011-10-25
Rousseeuw et al. (2012)
rrcov
1.3-01
2011-05-05
Todorov and Filzmoser (2009)
Series
AER
1.1-8
2011-06-18
Kleiber and Zeileis (2008)
chron
2.3-42
2011-08-21
James and Hornik (2011)
date
1.2-32
2011-12-23
Therneau et al. (2011)
fBasics
2110.79
2010-02-08
W¨urtz (2012b)
fEcoﬁn
290.76
2009-04-15
W¨urtz (2009b)
fts
0.7.7
2012-01-12
Armstrong (2012)
its
1.1.8
2009-09-06
Armstrong (2009)
tframe
2011.11-1
2011-11-03
Gilbert (2012b)
tframePlus
2011.11-2
2011-11-28
Gilbert (2011b)
timeDate
2131.00
2011-10-24
W¨urtz et al. (2012)
timeSeries
2130.92
2011-03-09
W¨urtz and Chalabi (2012)
tis
1.19
2012-04-06
Hallman (2012)
tseries
0.10-27
2011-10-23
Trapletti and Hornik (2012)
xts
0.8-2
2011-08-08
Ryan and Ulrich (2012)
zoo
1.7-6
2011-11-02
Zeileis and Grothendieck (2005)
TAA
BLCOP
0.2.6
2011-05-22
Gochez (2010)
dse
2011.11-1
2011-11-03
Gilbert (2012a)
EvalEst
2011.11-1
2011-11-03
Gilbert (2011a)
fArma
2100.76
2009-09-28
W¨urtz (2012a)
forecast
3.13
2011-11-19
Hyndman (2012)
FRAPO
0.3-5
2012-05-03
This book
fTrading
2100.76
2009-09-28
W¨urtz (2009d)
fUnitRoots
2100.76
2009-09-28
W¨urtz (2009e)
MSBVAR
0.6-0
2011-01-31
Brandt (2011)
PairTrading
1.1
2012-03-24
Takayanagi and Ishikawa (2012)
quantmod
0.3-17
2011-06-13
Ryan (2011)
systemﬁt
1.1-10
2011-11-11
Henningsen and Hamann (2007)
TTR
0.21-0
2011-08-30
Ulrich (2012)
urca
1.2-5
2011-02-25
Pfaff (2008a)
vars
1.4-8
2010-08-22
Pfaff (2008b)
(continued)

320
APPENDIX A: PACKAGE OVERVIEW
Table A.2
(continued)
Name
Version
Date
Source
Misc
coda
0.14-6
2011-11-04
Plummer et al. (2006)
ctv
0.7-4
2011-12-22
Zeileis (2005)
Hmisc
3.9-0
2010-04-26
Harrell (2012)
Rcpp
0.9.7
2012-02-16
Eddelbuettel and Franc¸ois (2011)
RcppArmadillo
0.2.30
2011-03-05
Franc¸ois et al. (2012)
RUnit
0.4.26
2010-09-14
Burger et al. (2010)
sos
1.3-1
2011-07-06
Graves et al. (2011)
References
Ardia D. 2011 bayesGARCH: Bayesian Estimation of the GARCH(1,1) Model with Student-t
Innovations in R. R package version 1-00.10.
Armstrong W. 2009 its: Irregular Time Series. R package version 1.1.8, Portfolio & Risk
Advisory Group and Commerzbank Securities.
Armstrong W. 2012 fts: R interface to tslib (a time series library in C++). R package version
0.7.7.
Asquith W. 2012 lmomco – L-moments, trimmed L-moments, L-comoments, censored
L-moments, and many distributions. R package version 1.4.5.
Berkelaar M. 2011 lpSolve: Interface to Lp_solve v. 5.5 to solve linear/integer programs.
R package version 5.6.6.
Boudt K., Carl P. and Peterson B. 2011 PortfolioAnalytics: Portfolio Analysis, including
Numeric Methods for Optimization of Portfolios. R package version 0.6.1/r1849.
Brandt P. 2011 MSBVAR: Markov-Switching, Bayesian, Vector Autoregression Models. R
package version 0.6-0.
Burger M., J¨unemann K. and K¨onig T. 2010 RUnit: R Unit test framework. R package version
0.4.26.
Caillat AL., Dutang C., Larrieu V. and NGuyen T. 2008 gumbel: package for Gumbel copula.
R package version 1.01.
Carl P., Peterson B., Boudt K. and Zivot E. 2012 PerformanceAnalytics: Econometric tools for
performance and risk analysis. R package version 1.0.4.4.
Chalabi Y. and W¨urtz D. 2010 Rsocp: An R. extension library to use SOCP from R. R package
version 271.1/r4910.
Coles S. and Stephenson A. 2011 ismev: An introduction to statistical modeling of extreme
values. R package version 1.37.
Deville Y. 2012 Renext: Renewal method for extreme values extrapolation. R package version
2.0-0.
Eddelbuettel D. 2012 RcppDE: Global optimization by differential evolution in C++. R pack-
age version 0.1.1.
Eddelbuettel D. and Franc¸ois R. 2011 Rcpp: Seamless R and C++ integration. Journal of
Statistical Software 40(8), 1–18.

APPENDIX A: PACKAGE OVERVIEW
321
Franc¸ois R., Eddelbuettel D. and Bates D. 2012 RcppArmadillo: Rcpp integration for Armadillo
templated linear algebra library. R package version 0.2.36.
Gelius-Dietrich G. 2012 glpkAPI: R Interface to C API of GLPK. R package version 1.2.1.
Ghalanos A. 2012a rmgarch: Multivariate GARCH models. R package version 0.94.
Ghalanos A. 2012b rugarch: Univariate GARCH models. R package version 1.0-8.
Gilbert P. 2011a EvalEst: Dynamic Systems Estimation – extensions. R package version
2011.11-1.
Gilbert P. 2011b tframePlus: Time Frame coding kernel extensions. R package version
2011.11-2.
Gilbert P. 2012a Brief User’s Guide: Dynamic Systems Estimation.
Gilbert P. 2012b tframe: Time Frame coding kernel. R package version 2012.3-1.
Gilleland E. and Katz R. 2011 New software to analyze how extremes change over time. Eos
92(2), 13–14.
Gochez F. 2010 BLCOP: Black-Litterman and copula-opinion pooling frameworks. R package
version 0.2.6.
Graves S., Dorai-Raj S. and Francois R. 2011 sos: sos. R package version 1.3-1.
Hallman J. 2012 tis: Time Indexes and Time Indexed Series. R package version 1.19.
Hankin R. and Lee A. 2006 A new family of non-negative distributions. Australia and New
Zealand Journal of Statistics 48, 67–78.
Harrell F. 2012 Hmisc: Harrell Miscellaneous. R package version 3.9-3.
Harter R., Hornik K. and Theussl S. 2012 Rsymphony: Symphony in R. R package version
0.1-14.
Henningsen A. 2010 linprog: Linear Programming/Optimization. R package version 0.9-0.
Henningsen A. and Hamann J. 2007 systemﬁt: A package for estimating systems of simulta-
neous equations in R. Journal of Statistical Software 23(4), 1–40.
Hofert M. and Maechler M. 2011 Nested Archimedean copulas meet R: The nacopula package.
Journal of Statistical Software 39(9), 1–20.
Hyndman R. 2012 forecast: Forecasting functions for time series. R package version 3.20.
James D. and Hornik K. 2011 chron: Chronological Objects which Can Handle Dates and
Times. R package version 2.3-42. S original by David James, R port by Kurt Hornik.
King R. 2011 gld: Estimation and use of the generalised (Tukey) lambda distribution. R
package version 1.9.2.
Kleiber C. and Zeileis A. 2008 Applied Econometrics with R. Springer-Verlag, New York.
Kojadinovic I. and Yan J. 2010 Modeling multivariate distributions with continuous margins
using the copula R package. Journal of Statistical Software 34(9), 1–20.
Konis K. 2011 lpSolveAPI: R Interface for lp_solve version 5.5.2.0. R package version
5.5.2.0-5.
Luethi D. and Breymann W. 2011 ghyp: A package on the generalized hyperbolic distribution
and its special cases. R package version 1.5.5.
McNeil A. and Stephenson A. 2011 evir: Extreme values in R. R package version 1.7-2.
Mullen K., Ardia D., Gil D., Windover D. and Cline J. 2011 DEoptim: An R package for global
optimization by differential evolution. Journal of Statistical Software 40(6), 1–26.
Nakatani T. 2010 ccgarch: An R Package for Modelling Multivariate GARCH Models with
Conditional Correlations. R package version 0.2.0.

322
APPENDIX A: PACKAGE OVERVIEW
Novomestky F. 2012 rportfolios: Random portfolio generation. R package version 1.0.
Pfaff B. 2008a Analysis of Integrated and Cointegrated Time Series with R 2nd edn. Springer,
New York.
Pfaff B. 2008b VAR, SVAR and SVEC models: Implementation within R package vars. Journal
of Statistical Software.
Pfaff B. 2011 rneos: XML-RPC Interface to NEOS. R package version 0.2-6.
Pfaff B. 2012a gogarch: Generalized Orthogonal GARCH (GO-GARCH) models. R package
version 0.7-1.
Pfaff B. 2012b QRM: Provides R-language code to examine quantitative risk management
concepts. R package version 0.4-7.
Plummer M., Best N., Cowles K. and Vines K. 2006 CODA: Convergence diagnosis and output
analysis for MCMC. R News 6(1), 7–11.
Ribatet M. 2011 POT: Generalized Pareto Distribution and Peaks Over Threshold. R package
version 1.1-1.
Rousseeuw P., Croux C., Todorov V., Ruckstuhl A., Salibian-Barrera M., Verbeke T.,
Koller M. and Maechler M. 2012 robustbase: Basic Robust Statistics. R package version
0.8-1-1.
Rowe B. 2012 tawny: Provides various portfolio optimization strategies including random
matrix theory and shrinkage estimators. R package version 2.0.2.
Ryan J. 2011 quantmod: Quantitative Financial Modelling Framework. R package version
0.3-17.
Ryan J. and Ulrich J. 2012 xts: eXtensible Time Series. R package version 0.8-6.
Scott D. 2011 GeneralizedHyperbolic: The generalized hyperbolic distribution. R package
version 0.7-0.
Scott D. and Dong CY. 2010 VarianceGamma: The variance gamma distribution. R package
version 0.3-0.
Scott D. and Grimson F. 2010 SkewHyperbolic: The skew hyperbolic Student t-distribution.
R package version 0.2-0.
Soetaert K., Van den Meersche K. and van Oevelen D. 2009 limSolve: Solving Linear Inverse
Models. R package 1.5.1.
Stephenson A. 2002 evd: Extreme value distributions. R News 2(2), 31–32.
Stephenson A. and Ribatet M. 2010 evdbayes: Bayesian analysis in extreme value theory.
R package version 1.0-8.
Takayanagi S. and Ishikawa K. 2012 PairTrading: Classical pair trading based on cointegra-
tion in ﬁnance. R package version 1.1.
Therneau T., Lumley T., Halvorsen K. and Hornik K. 2011 date: Functions for handling dates.
R package version 1.2-32. S original by Terry Therneau, R port by Thomas Lumley, Kjetil
Halvorsen, and Kurt Hornik.
Theussl S. and Hornik K. 2012 Rglpk: R/GNU Linear Programming Kit Interface. R package
version 0.3-8.
Todorov V. and Filzmoser P. 2009 An object oriented framework for robust multivatiate
analysis. Journal of Statistical Software 32(3), 1–47.
Trapletti A. and Hornik K. 2012 tseries: Time Series Analysis and Computational Finance.
R package version 0.10-28.

APPENDIX A: PACKAGE OVERVIEW
323
Turlach BA. and Weingessel A. 2011 quadprog: Functions to solve Quadratic Programming
Problems. R package version 1.5-4.
Ulrich J. 2012 TTR: Technical Trading Rules. R package version 0.21-1.
Venables W. and Ripley BD. 2002 Modern Applied Statistics with S 4th edn. Springer, New
York.
Wang J., Zamar R., Marazzi A., Yohai V., Salibian-Barrera M., Maronna R., Zivot E., Rocke
D., Martin D., Maechler M. and Konis K. 2010 robust: Insightful Robust Library. R package
version 0.3-11.
Wang N., Raftery A. and Fraley C. 2003 covRobust: Robust Covariance Estimation via Nearest
Neighbor Cleaning. R package version 1.0.
W¨urtz D. 2009a fCopulae: Rmetrics – dependence structures with copulas. R package version
2110.78.
W¨urtz D. 2009b fEcoﬁn: Economic and ﬁnancial data sets. R package version 290.76.
W¨urtz D. 2009c fExtremes: Rmetrics – extreme ﬁnancial market data. R package version
2100.77.
W¨urtz D. 2009d fTrading: Technical Trading Analysis. R package version 2100.76.
W¨urtz D. 2009e fUnitRoots: Trends and Unit Roots. R package version 2100.76.
W¨urtz D. 2012a fArma: ARMA Time Series Modelling. R package version 2160.77.
W¨urtz D. 2012b fBasics: Rmetrics – markets and basic statistics. R package version 2160.81.
W¨urtz D. and Chalabi Y. 2009 fGarch: Rmetrics – Autoregressive Conditional Heteroskedastic
Modelling. R package version 2110.80.
W¨urtz D. and Chalabi Y. 2012 timeSeries: Rmetrics – Financial Time Series Objects. R package
version 2160.94.
W¨urtz D., Chalabi Y., Chen W. and Ellis A. 2010a Portfolio Optimization with R/Rmetrics.
Rmetrics Association & Finance Online, www.rmetrics.org. R package version 2130.80.
W¨urtz D., Chalabi Y., Chen W. and Ellis A. 2010b Portfolio Optimization with R/Rmetrics.
Rmetrics Association & Finance Online, www.rmetrics.org. R package version 2110.4.
W¨urtz D., Chalabi Y., Maechler M. and Byers J. 2012 timeDate: Rmetrics – Chronological
and Calendarical Objects. R package version 2160.95.
Zeileis A. 2005 CRAN task views. R News 5(1), 39–40.
Zeileis A. and Grothendieck G. 2005 zoo: S3 infrastructure for regular and irregular time
series. Journal of Statistical Software 14(6), 1–27.

Appendix B
Time series data
There are several possibilities for dealing with time series data in R. In this appendix
a concise account is provided of the date–time classes and the available implementa-
tions for creating and handling time series data objects. This is based on Ripley and
Hornik (2001), Grothendieck and Petzoldt (2004), Zeileis and Grothendieck (2005),
W¨urtz et al. (2009) and Chalabi et al. (2011). In the next section the classes and meth-
ods for handling dates and times are presented. In the rest of this appendix the various
possibilities of creating a time series object and its manipulations are discussed.
B.1
Date-time classes
Classes and methods for deﬁning and manipulating dates and times are provided in
the base R distribution as well as in contributed packages. In the following the classes
and methods are presented with increasing degree of complexity.
The class Date was introduced to base R in version R-1.9.0. This class supports
only dates without a time stamp. Internally, dates are represented as the day count
since 1 January 1970. The printing of these objects can be adjusted according to the
percentage abbreviations listed in ?strptime with the format() method deﬁned
for these objects. A coercion method (as.Date()) and utility functions for extract-
ing the day of week (weekdays()) and the month (months()) are available, too.
Furthermore, methods for axis labelling in plots and histograms are available as well
as facilities for computing and manipulating objects of this class.
A second class for handling dates only was implemented in the CRAN package
date (see Therneau et al. 2011). In this package an S3 class date is deﬁned in which
dates are stored as the count of days since 1 January 1960. This origin is identical
to the chosen reference date in SAS. The package offers methods for converting
Julian calendar dates to Gregorian calendar dates, where the latter can be expressed
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

APPENDIX B: TIME SERIES DATA
325
in various formats, and vice versa. Furthermore, a coercion method (as.date()) and
a logical test to check whether an object is of class date (is.date()) are available.
Simple addition and subtraction operations can be carried out on objects of this class.
The following classes enable the user to handle date–time objects. The least
complex of these classes is chron, provided in the CRAN package of the same
name (see James and Hornik 2011). Objects of class chron do not support time
zones nor daylight saving time rules. Objects are created by calling the generating
function chron() whereby the dates are provided as vector argument dates. and
the time stamps as argument times. (note the full points). The input and output
format can be controlled by the arguments format and out.format, respectively.
Abbreviations as detailed in ?strptime can be employed. A different origin than the
default of 1 January 1970 can be provided by the argument origin. Alternatively, the
chron origin can be set via an options() expression. If only a dates. argument
is provided, the function returns an object of S3 class dates and if only a time stamp
is passed to chron() an object with class attribute times is returned, which has the
characteristic of a super-class, and hence chron and dates objects inherit methods
deﬁned for times objects. Objects that only refer to either dates or times can also be
created by the generating function dates() and times(), respectively. Internally,
times are represented as fractions of a day, so that, for example, 0.5 means noon
of the day in question. Numerous methods have been speciﬁed for these kinds of
object. Arithmetic operations are deﬁned in the form of differences between chron
objects and the addition of a constant. All logical comparisons are supported, too. The
conversion between Julian and Gregorian calendar dates is implemented as functions
julian(), month.day.year() and day.of.week(). Whether or not a year is a
leap year can be assessed by calling leap.year() with the year as argument. A
sequence of dates is swiftly returned by the function seq.dates(), and a cut()
method has been deﬁned for subsetting a chron or dates object. Certain periods from
either a chron or dates object are returned by the functions days(), weekdays(),
months(), quarters() and years(), with obvious meanings. Similarly, with
respect to times objects, the functions hours(), minutes() and second() can
be used to extract the elements of a time stamp. To test whether a certain date falls
on a weekend or a holiday, the functions is.weekend() and is.holiday() can
be used, respectively. By default, only US holidays during the year 1992 are supplied
as object .Holidays, but this object can be amended/modiﬁed by holiday dates
supplied by the user. Finally, coercion methods from objects of classes yearmon(),
yearqtr() and ts are provided. The ﬁrst two classes are deﬁned in the package zoo,
discussed in Section B.5, and the time series class ts is discussed in Section B.2.
In the R base distribution POSIX classes are deﬁned for the handling of dates
associated with a time stamp. The classes are POSIXct and POSIXlt with a super/
parent class POSIXt. The former class is intended for handling calendar times (hence
the sufﬁx ‘ct’ in its name) and the latter is intended for handling local times (hence
the sufﬁx ‘lt’ in its name). A concise description of these classes is given in Ripley
and Hornik (2001). Objects of class POSIXct are represented by the count of seconds
since 1 January 1970 (GMT). In contrast, date times in the form of POSIXlt objects
are represented as a list of nine elements: sec for the seconds, min for the minutes,

326
APPENDIX B: TIME SERIES DATA
hour for the hour, mday for the day in a month, mon for the month in a year, year
for the year, wday for the day of the week (0 for Sunday, . . . , 6 for Saturday), yday
for the day count in a year and isdst as a logical ﬂag whether a daylight saving
scheme was in existence at the particular date time. POSIXlt objects can further
contain an optional time zone attribute (tzone), and objects of class POSIXct can
also be amended by such an attribute (the default is GMT). The POSIX class in the
wider sense therefore provides a means of handling different time zone date times as
well as periods of daylight saving time. For these S3 classes, methods for conversion
to and from character presentation and date–time manipulations are speciﬁed. The
former group consists of the methods format(), as.character(), strftime(),
and strptime(). Furthermore, objects of one child class can be coerced into the
other child class by utilizing the as.POSIXct() and as.POSIXlt() methods, re-
spectively. Arithmetic operations are deﬁned for adding/subtracting a constant (in
seconds) from these objects. Any logical operator can be applied to two sets of
POSIX objects for comparison. The cutting and sequencing of POSIX objects can
be achieved by the methods cut() and seq(), so the rounding and truncation of
POSIX objects can be done by the methods round() and trunc(). These four
methods are deﬁned objects of the super-class POSIXt. Similarly, methods for ex-
tracting elements pertinent to a date are weekdays(), months(), quarters(),
and julian() for objects inheriting from this super-class, with obvious meanings.
Finally, time intervals created by either the subtraction of two POSIX objects or by
the function difftime() will produce an object of class difftime. The printing
of the time difference can be tailored by a format() method and/or by choosing the
units of the time interval. By default the unit will be equal to the lowest frequency
implied by the time interval. A coercion method for character and numeric values is
deﬁned (as.difftime()). Finally, mathematical operations for rounding, extract-
ing the signiﬁcant digits, the ﬂoor and ceiling values, truncation and the absolute
value of a time interval are deﬁned as methods.
The most sophisticated representation of date–time objects has been accomplished
in the class timeDate. This class is deﬁned in the contributed package timeDate (see
W¨urtz et al. 2012) and a concise description of the class and its associated methods
and functions is given in Chalabi et al. (2011). Objects of S4 class timeDate consist
of three slots: the Data slot of class POSIXct, and the format and FinCenter
slots, both characters. The format slot contains the information on how Data is to
be displayed and FinCenter is the time domain of the ﬁnancial centre. The latter
slot is of importance when data from different time zones and/or ﬁnancial centres are
aggregated. By default GMT is assumed for both: the time zone of the data and the
time zone of the ﬁnancial centre. In addition to the time offset with respect to GMT of
a ﬁnancial centre, further information can be queried from the concept of a ﬁnancial
centre, such as the day-count convention and the opening hours of a stock exchange.
This information can be queried and modiﬁed with the setRmetricsOptions()
function as the package timeDate is part of the Rmetrics bundle. Objects of formal
S4 class timeDate can be generated by calling timeDate(), timeSequence() or
timeCalendar(). The ﬁrst function takes a character vector of date times as input,
whereby the format of this character vector can be provided by a format argument,

APPENDIX B: TIME SERIES DATA
327
the time zone by the argument zone and the location of the ﬁnancial centre by the
argument FinCenter. If the latter two arguments are not speciﬁed, GMT will be
assumed. A set of equally spaced date times can be created with second function. It
takes as arguments the starting date time (from) and either the ﬁnal date time (to)
or the length of the timeDate object required. The frequency is speciﬁed via its
by argument. This character string deﬁnes the increments as sec, min, hour, day,
week, month, year, with obvious meanings. Finally, if the calendar date information
is available as elements, an object of class timeDate is returned by the last function,
timeCalendar(), where by default the current year is used as the year stamp and
the user can supply vectors for the months, days, hours, minutes and seconds, as
well as information on the time zone and the ﬁnancial centre. If the latter two are
omitted from the call, then GMT will be assumed for both. A strength of the timeDate
package is the testing for weekdays, business days and holidays and the creation of
so-called special dates. The functions isWeekday(), isWeekend(), isBizday(),
isHoliday() and dayOfWeek() belong to the former group. The holiday calendar
is speciﬁc to a ﬁnancial centre, but user-deﬁned holiday calendars can easily be set
up by either amending an existent holiday calendar or by specifying a new calendar
consisting of a timeDate object referring to the dates of the holidays. Special date
sequences, such as the last day in a month, quarter or year or the nth weekday in a
month or quarter are created by choosing from the functions listed below:
r timeFirstDayInMonth(),
r timeLastDayInMonth(),
r timeFirstDayInQuarter(),
r timeLastDayInQuarter(),
r timeNthNdayInMonth(),
r timeLastNdayInMonth(),
r timeNdayOnOrAfter(),
r timeNdayOnOrBefore().
Finally, the start and end date of a timeDate object can be queried with the functions
start() and end(), respectively. Incidentally, min(), max() and range() meth-
ods are also available. A window of date times can be extracted from a timeDate
object by means of the window() function, thus subsetting is possible using the []
operator.
B.2
The ts class in the base package stats
The S3 class ts in the base package stats is well suited for handling regularly spaced
time series data. The generating function ts() returns a time series object with class
attribute ts if a univariate series (i.e. a vector) has been supplied, or an object with
class attributes c(’mts’, ’ts’) if a multivariate series (i.e. a matrix or a data

328
APPENDIX B: TIME SERIES DATA
frame) has been provided. Objects can be coerced to ts objects by the as() method
and tested if they belong to this class with the function is.ts(). The start and end
dates are set by the arguments start and end respectively, and the frequency of the
regular spaced observations by the argument frequency. Instead of the frequency,
its reciprocal can be provided for the argument deltat. The dates for the ﬁrst and
last observations can be speciﬁed either as a single number or as two-element integer
vectors that represent the natural time unit. Objects of this class possess a time series
property that can be shown by the function tsp(). The time series property is a ts
object attribute – a three-element vector consisting of the start and end periods and
the frequency. To check whether an arbitrary object is endowed with this attribute the
function hasTsp() can be used. Furthermore, a time series property can be modiﬁed
or amended to an object x by tsp(x) <−value. The time component of an object
of class ts can be retrieved with the function time(). Subsetting a time series object
to a narrower sample range is accomplished by using the window() function. The
dates of the ﬁrst and last observations are returned by the functions start() and
end(), respectively. Finally, plot() and print() methods are deﬁned for objects
of this class.
B.3
Irregular-spaced time series
The class ts is conﬁned to the representation of regular-spaced series. The creation
and manipulation of irregular-spaced time series is covered by means of the two
CRAN packages its (Armstrong 2009) and tseries (Trapletti and Hornik 2012). The
former is dedicated solely to irregular-spaced time series, and a class deﬁnition for
these special time series is contained in the latter package with associated functions
and methods.
An S3 class irts is deﬁned in the package tseries. Objects with this class attribute
are deﬁned as a list object with elements time and value for holding the date time
stamps and the values of the time series, respectively. Akin to the class design in
the package its, the element time is of class POSIXct and the object of the second
list element can be either a vector or a matrix object. Methods for coercion and
logical testing for this kind of object are implemented as as.irts() and is.irts(),
respectively. For objects with class attribute irts, methods for plotting, printing and
the extraction of date time stamps and/or the values of the series are deﬁned. The
S3 methods for plotting are: lines() for superimposing on an existing plotting
device the trajectory of the irregular time series; plot() for plotting the time series
itself as a time series chart; and points() for superimposing points on an existing
graphic, where the points of the irregular time series are joined by line segments. The
print() method has a format argument, by which the representation of the date
time stamps can be controlled. The date time stamps can be recovered from an irts
object with the function time() and the values of the series itself with the function
value(). Subsetting of an irts object can be accomplished by the [] operator. In
addition to these methods, functions are provided for inspection and analysis of the
date time stamps and the handling of irts objects. In the former group, the count
of seconds after midnight is found by calling daysecond() and the day of week

APPENDIX B: TIME SERIES DATA
329
by calling the function weekday(). The weekdays are coded as integers from 0
for Sunday to 6 for Saturday. It is further possible to assess whether an entry falls
on a business day or on a weekend by utilizing the functions is.businessday()
or is.weekend(). Intermittent values of an irregular-spaced time series can be
approximated by interpolation for a prespeciﬁed sequence of date time stamps with
the function approx.irts(). Finally, reading/writing from/to a ﬁle connection can
be accomplished with the functions read.irts and write.irts. Both functions
have a format argument for specifying the representation of the date time stamps.
In the CRAN package its an S4 class of the same name is provided for han-
dling irregular time series objects. Objects of this class are endowed with the two
slots dates and .Data for holding the date time information as a POSIXt object
and the series data as a matrix object, respectively. Objects of this class are cre-
ated by the generating function its(). Methods for coercing a matrix to an its
object and testing for such an object are provided by as.its() and is.its(),
respectively. Because the .Data slot is of class matrix, all arithmetic and logical
operations deﬁned for these objects are available for its objects, too. The former
method assumes that the ﬁrst column contains the count of seconds since 1 Jan-
uary 1970 as date time stamps. Further, methods for coercion of its objects to lists
and data frames are available via the methods as.list() and as.data.frame(),
respectively. The date times and the data can be extracted from its objects with
the function dates() and core(). Both functions can also be used for assigning
date times – in the form of POSIXt objects – and time series data – in the form of
matrix objects – to an existing its object. With respect to the date time stamps
of an irregular series the functions daysecondIts() and weekdayIts() for de-
termining the count of seconds since midnight and/or whether the date times fall
on weekdays are available. A last observation carry forward function, locf(), is
available for replacing NA values with the previous non-NA value. Two objects of
formal class its can be joined by alignment, by appending or by collapsing with the
functions alignedIts(), appendIts() and collapseIts, respectively. There
are two means of lagging a time series object: lagIts() and lagdistIts(). The
former function lags (leads) the time series by k periods if k is a positive (negative)
integer. The latter function applies the former function to a sequence of lags deﬁned
by the arguments kmin and kmax. The object returned contains the sequence lagged
as additional columns of the original series. The subsetting of an its object can be
achieved in two ways. First, the function rangeIts() works similarly to the previ-
ously described window() function. Second, the function extractIts() returns a
semi-regular its object, where the user can provide the periodicity (week, month
and year) and specify whether all data points, or only the ﬁrst or last observation
for a period, will be returned (all, last, first). The function’s arguments are
named period and find for the two sets of possibilities. The accrued annual inter-
est rate as a per-period return is output by the function accrueIts() in the form of
a decimal return. The day count convention to be used in this computation is con-
trolled by the argument daysperyear. Finally, for downloading data from Internet
services the function priceIts() is available, and the reading/writing from/to CSV
ﬁles can be accomplished with the functions readcsvIts() and writecsvIts(),
respectively.

330
APPENDIX B: TIME SERIES DATA
B.4
The package timeSeries
An S4 timeSeries class deﬁnition is provided in the package of the same name
(see W¨urtz and Chalabi 2012). This class has a total of eight slots, including those
for the date-time representation in the class timeDate (see Section B.1). Objects of
class timeSeries can be created either by the generating function timeSeries(),
by reading from a ﬁle connection (readSeries()) or by coercion. The two most
important arguments for the generating function are data and charvec. As input for
data a matrix object or an object that can be coerced to a matrix must be provided,
and charvec can be basically any kind of character vector containing the date time
stamps of data. The only requirement for charvec is that it can be coerced to an
object of class timeDate. The format of the vector supplied for charvec can be
speciﬁed as argument format. In addition to these arguments, information on the
time zone (argument zone), on the ﬁnancial centre (argument FinCenter), new col-
umn names (units), a description of the time series (argument description), a title
for the returned object (argument title) and record identiﬁers (argument recor-
dIDs) can also be provided, but are left unset by default. For the purposes of coercion
just mentioned, coercion methods for objects of classes numeric, data.frame,
matrix, ts, character, and zoo are provided. Whether an object x is of formal
class timeSeries can be tested by calling is.timeSeries(x). Whether the slot
positions, which carries the date time information, is empty can be assessed with
the function is.signalSeries(), hence the time series object would only con-
sist of the time series data itself. The characteristics of a timeSeries object with
respect to its dimension and regularity/periodicity can be tested by the functions
isMultivariate() and isUnivariate() for the former and by isRegular(),
isDaily(), isMonthly(), isQuarterly(), and frequency() for the latter,
with obvious meanings. The series and date time elements of a timeSeries ob-
ject can be queried and assigned by the functions seriesData() and time(),
respectively. The start and end date times of a series are returned by the functions
start() and end(). Because it is not required that the date time information in the
charvec vector is ordered per se, an object of class timeSeries can be ordered
with respect to its positions with the sort() method (by default increasing) or can
be reversed with respect to time by the rev() method. Sampling of a time series can
be accomplished with the sample() method. To display the contents of a time-
Series object, print(), head() and tail() methods are deﬁned. Incidentally,
the show() method for timeSeries objects only returns the number of entries as
deﬁned in getRmetricsOption(’max.print’), which is limited to 100 rows by
default. Hence, the complete time series is output by calling print() or by increasing
max.print accordingly, if the time series consists of more entries. The subsetting of
a time series can be conducted using either the [] or window() method. The latter
requires start and end as arguments for the data window to be exerted. Similar to
window() is the cut() method, where the returned data ranges are deﬁned by the ar-
guments from and to. Methods for displaying a time series graphically are plot(),
lines() and points(). The ﬁrst method has argument plot.type for determin-
ing whether the time series are plotted separately (single) or together (multiple,

APPENDIX B: TIME SERIES DATA
331
the default). The latter two methods can be used to superimpose a time series on an
existing device.
So far, basic methods and functions for creating and handling timeSeries
objects have been presented. In this paragraph methods and functions for manipulat-
ing/computing these objects are discussed. Key descriptive statistics can be calculated
per column with the function colStats(), where the function to be applied to each
column is provided as argument FUN. Wrapper functions for the most frequently
encountered descriptive statistical measures are deﬁned as colFoo(), where Foo is
a placeholder for Sds, Vars, Skewness, Kurtosis, Maxs, Mins, Prods, Quan-
tiles, Means, Stdevs or Avgs. The functions colFoo() are all speciﬁed with an
ellipsis argument, passed down the function in question. To apply a function only
over certain date time ranges the functions fapply() and applySeries() are
available. In addition to the argument for the function (FUN), both take as further
arguments vectors which determine the start and end points of the data windows
that are each supplied to FUN. The difference between these two functions is that
applySeries() has a by argument that refers to a coarser periodicity and will
be used if from and to are left NULL. A lag() method for lagging or leading a
time series by k periods is available, as is a diff() method is available; a positive
integer value indicates the number of periods by which a series should be lagged.
With respect to ﬁnancial time series data a method for computing returns of a series
is available as returns(). The user can determine whether discrete or continu-
ous returns are computed, whether these are expressed as decimals or percentages,
and whether NA values are removed prior to calculating the returns or whether the
series is trimmed for NA values (ﬁrst row) after the computation. Moving aver-
ages (weighted) can be computed by applying a linear filter() to a time series.
Also related to ﬁnancial time series applications are the functions drawdowns()
for determining the draw-downs of a return series, spreads() for determining the
bid–ask spread and midquotes() for the implied mid-quote between a bid and an
ask series. The length of runs, as an indication of the randomness of a series, is
returned by the function runlengths(). With respect to the creation of pseudo-
uniform variables of a series, the order statistics are returned by the function order-
Statistics() and the associated ranks by the function rank(). Cumulated column
and row sums are returned by the functions colCumsums() and rowCumsums(),
respectively. The latter is handy for calculating portfolio performance-based mea-
sures on a per-period basis, if the time series contains its constituents. With respect
to the column dimension, functions for the cumulated maxima, minima, products
and returns are also available as functions colCummaxs(), colCummins, col-
Cumprods() and colCumreturns(), respectively. In the last function, the user
can specify whether the cumulated returns are computed for discrete or continuous
returns. Finally, two timeSeries objects can be merged together by the cbind()
and rbind() methods provided and a single object can be aligned to a ﬁner grid
of date times with the functions align() and alignDailySeries(). With re-
spect to the latter, the function dummyDailySeries() is also useful for creating
a dummy series that can then employed in a merge with another timeSeries
object.

332
APPENDIX B: TIME SERIES DATA
B.5
The package zoo
An S3-based infrastructure for the handling of regular- and irregular-spaced time
series is implemented in the CRAN package zoo (see Zeileis and Grothendieck 2005).
The package’s name is an acronym for Zeileis’s ordered observations. A feature of
this implementation is the class independence of the date time object. The class
deﬁnitions in the packages its, tseries and timeSeries require explicitly or implicitly
through coercion that the date time stamps belong to a certain class – either POSIXt
or POSIXct or timeDate. This is not the case for the S3 class zoo where the index
can be of almost any class. Furthermore, the package bridges the gap between the
simplest time series representation in the form of objects with class attribute ts and
the more advanced features and manipulations of time series data, by deﬁning the
zooreg class which inherits from the zoo class. The former is particularly speciﬁed
for regular-spaced time series. The use of the package is detailed in vignettes. Space
does not permit a full presentation of the package here, and the reader is referred to
the above sources for further details.
The generating function for creating zoo objects is zoo(). It has three arguments:
x for the values of the time series; order.by for the index vector; and the optional
argument frequency. The important requirements for the index vector are that it
can be ordered and that its entries can be matched against another index vector, where
applicable. If the latter argument is left NULL (the default), the function will return
an object with class attribute zoo; and if speciﬁed in accordance with the index
provided, an object with class attributes c(’zooreg’, ’zoo’) will be returned.
The child class is similar in nature to the ts class provided in the stats package of the
base R distribution. This kind of object can also be created by calling the function
zooreg() with basically the same set of arguments as in ts(). Objects of class
zoo consist of the object holding the time series values (a vector, matrix or factor),
and have a further attribute, the index. Objects of class zooreg have, in addition, a
frequency attribute. Whether an object is a regular time series can be determined with
the method is.regular(). This function has an argument, strict, in addition to
the object x, to test whether or not the series is regular but has NA values – that is, the
time stamps of the non-NA values are non-consecutive (irregular). In the following
we will only refer to zoo objects as all of the methods and functions presented can
also be applied to zooreg objects.
Apart from these generating functions, zoo objects can be created by coercion
from ts, irts, its, fts, mcmc, tis or xts objects.1 Furthermore, coercion methods
from zoo objects to data.frame, list, matrix and vector objects are available.
When a
zoo object has been created its content can be further in-
spected by
print(),
head(),
tail(),
str() and
summary() methods.
Graphical displays of a zoo object can be created either by calling the plot()
1 The class fts is deﬁned in the package fts (see Armstrong 2012); the class mcmc is provided by
the package coda (see Plummer et al. 2006) and the class tis is shipped with tis (see Hallman 2012).
The class xts will be described in Section B.6.

APPENDIX B: TIME SERIES DATA
333
method or by superimposing a time series on an existing graphics device with the
methods lines() and points(). Lattice plots of zoo objects can be produced
by the xyplot() method. Similar to the plot() method, lines and points can be
superimposed on an existing lattice plot with the methods llines() and lpoints,
respectively. In addition, functions for tailoring the panel shape of lattice plots are
available.
Numerous S3 methods and functions are available for objects with these class
attributes. The series data can be recovered by the coredata() method, which
removes any zoo-related attributes from the object. The time index can be queried
by employing either the index() or the time() method. These three functions can
also be used in the assignment of values to a zoo object. For subsetting zoo objects,
a window() method is available that can also be used in assigning new values to an
object. This can also be achieved with the [] method. The start and end time points
are returned by calling start() and end(), respectively.
In the package zoo the handling of NA values can be accomplished by (in alpha-
betical order):
r na.aggregate, where NA values are replaced by an aggregated value derived
from a lower-frequency grid of the series;
r na.approx(), where NA values are replaced by means of linear interpolation
– leading NA values are discarded;
r na.contiguous(), where the longest sequence of non-NA values is recovered
from the zoo object;
r na.fill(), where NA values are replaced by user-speciﬁed values/rules;
r na.locf(), where the last non-NA value is carried forward as in the package
its – leading NA values are discarded;
r na.omit(), where all NA values are removed from the object;
r na.spline(), where NA values are replaced by values derived from a cubic
spline function;
r na.StrucTS, where NA values are replaced by values derived from a structural
time series model by means of Kalman ﬁltering;
r na.trim(), where leading and trailing NA values are removed from the object.
To aggregate a time series from a higher frequency (say daily) to a lower frequency
(say monthly) an aggregate() method can be used; the user has to provide the
index for the coarser time grid of equal length to the index itself and a function that
is applied as the aggregation rule. Two zoo objects can be combined/joined by the
rbind(), cbind() and/or merge() methods provided. For lagging and leading a
time series, a lag() method has been available. Note that here the argument k for
the lag order must be speciﬁed as negative for lagging and positive for leading the

334
APPENDIX B: TIME SERIES DATA
series entries by k periods. In addition, differenced series are returned by applying the
diff() method. This method has as a logical argument the switch arithmetic for
controlling whether arithmetic (the default) or geometric differences are computed.
The method rollapply() can be used to apply a function on a rolling window.
The length of the moving window is set by the argument width, and whether the
values resulting from the function applied pertain to the left-hand end, centre or right-
hand end is controlled by the argument align. Resulting NA values can be either
ﬁlled (argument fill) or padded (argument na.pad). Further, the logical switch
by.column determines whether the function FUN is applied per column (the default)
or on the whole set of series contained in a multivariate time series. Hence, by setting
this argument to FALSE a moving grand mean can be computed, for instance. A
partial application of the function taking NA entries into account can be achieved with
the partial argument. Because quite often it is desired to right-align the values
returned by the function, a rollapplyr() method has been deﬁned which is a
wrapper for rollapply(), where the align argument is preset to ’right’. In a
similar vein, the methods rollmean(), rollmeanr(), rollmax(), rollmaxr(),
rollmedian(), and rollmedianr() are available, with obvious meanings.
Incidentally, user-deﬁned index classes are supported and in the package zoo
predeﬁned classes for supporting regular monthly and quarterly time indexes are
already implemented as classes yearmon and yearqtr, respectively. Finally, similar
to previous implementations, reading/writing directly from/to a ﬁle connection can be
accomplished with the functions read.zoo() and write.zoo() which are based
on the functions read.table() and write.table(), respectively.
B.6
The packages tframe and xts
So far, many possibilities for dealing with time series data in R have been presented
and it is up to the user to choose the class that will best suit the task in hand. However,
matters are quite different for package developers. They are faced with the problem
that their functions and methods have to stand up to these numerous ways of creating
time series and classes for the date time stamps. Providing a solution to this problem
is the raison d’ˆetre for the packages xts (see Ryan and Ulrich 2012) and tframe (see
Gilbert 2012), that is, providing a unifying class and method infrastructure such that
a developer only has to cope with these time series classes. Of course, the use of xts
and tframe is by no means restricted to this group of R users.
The package tframe is hosted on CRAN and R-Forge. In addition to functions
for the handling of time series irrespective of the representation of dates and times,
facilities for plotting and data manipulation are provided. The package tframePlus
is an extension of tframe, in particular with respect to plotting and data manipulation
(see Gilbert 2011). A time frame attribute of class tframe is set or extracted to an
object by the function tframe(). Alternatively, a time frame can be superimposed
on an object by either of the functions tfSet() or tframed() or by coercion
as.tframe(). The existence of a tframe object is assessed by calling either
tframe() or tframed(). Whether two objects are equal can be determined by the

APPENDIX B: TIME SERIES DATA
335
function testEqual(), and whether the time frames of two objects are the same
with the function testEqualframes(). The tframe class attribute of an object bar
can also be transferred to another object foo by using frame() as extractor function
on the former object and the same function in the assignment of the latter, that is,
tframe(foo) <−tframe(bar). The start and end periods are returned by calling
start() or tfstart() for the start period and by end() or tfend() for the end
period of a time frame. For multivariate time series objects, the functions latest-
Start() and latestStartIndex() can be used. These functions return the latest
start date of the series and the column index pertinent to the series, respectively. Sim-
ilarly, the functions earliestEnd(), earliestEndIndex() and latestEnd(),
latestEndIndex() are available, with obvious meanings. The sample size is
recovered by the function tfSpan(), and subsetting of a time series is accomplished
by tfwindow(), where the relevant new start and end periods are speciﬁed. A
tframe object can be expanded horizontally by means of the function tfExpand()
and vertically (i.e., by right-appending time series to an existing tframe object)
by the tbind() method. The analogue to this function with respect to the time
dimension is splice(). NA entries can be dropped by calling the function
trimNA(). The names of the series can be either extracted or assigned to a frame
object by the function seriesNames(). A certain series of a multiple time series
object is extracted by the function selectSeries(). Utility functions are tfL()
for lagging a time series, diff() and difflog() for computing (logarithmic)
differences, tfplot() for producing time series plots and tfprint() for producing
a formatted print of a tframe object. Reading and writing a tframe object from/to
a ﬁle connection is accomplished by the functions tsScan() and tsWrite(),
respectively. In the complementary package tframePlus facilities for the aggregation
of time series (as.quarterly(), as.annually() and as.weekly()) are
available. The aggregation method is provided as argument FUN, where the default
is set to compute sums of the data belonging to the lower-frequency periods. The
date time representation can be replaced by a new date time class with the function
changeTSrepresentation(). To apply a function to consecutive periods of a
coarser time grid, the function rollAggregate() can be used. Cumulated sums
of a time series are returned by calling tfI(). In addition producing time series
plots with the function tfplot(), a routine for generating perspective plots of a
multivariate time series is provided by tfpersp(). Finally, tframe objects can be
written to a spreadsheet ﬁle (in either Excel (.xls) or CSV format) by the function
TSwriteXLS() or TSwriteCSV(), respectively.
Objects of S3 class xts are an amended and modiﬁed version of zoo objects
and can be created either by calling its generating function xts(), or by coercion
(methods as.xts and try.xts()). The modiﬁcations implemented are that: (i) a
time-based index is required for xts objects, whereas for zoo objects any ordered
index can be used; (ii) xts are amended by internal attributes which basically store
the class of the original series data (.CLASS) and a row names attribute (.ROWNAMES),
which takes care of the date time information of the series; and (iii) it is possible to
add user-speciﬁed attributes. Because the xts class is derived from zoo basically all
the methods and functions presented in the previous subsection can also be applied

336
APPENDIX B: TIME SERIES DATA
to xts objects. Hence, in what follows only the facilities special to xts objects will
be presented.
The ﬁrst group of functions to discussed refer to the date time handling of xts
objects. The function endpoints() returns the index entries of a time series that
fall on the last ‘date time’ of a period. The periodicities supported are: microseconds,
milliseconds, seconds, minutes, hours, days, weeks, months, quarters, andyears. Next,
to recover the ﬁrst and last n observations of a time series the methods first() and
last() are available. A feature of these methods is that, in addition to a numeric
value for the number of periods to return, this can also be expressed as the count
of a periodicity; for example, first(x, ’1 month’) would return the time series
data pertinent to the ﬁrst calendar month. The index values only would then be
recovered by either the index() or time() methods. Date time stamps as POSIXct
objects for the ﬁrst or last index entry for a speciﬁed period are returned by the
functions firstof() and lastof(). The index class of an xts object can be
queried with the function indexClass() and can also be employed in its assignment.
Furthermore, the index can be converted to a different class through coercion by
means of the function convertIndex(). The index classes supported are Date,
POSIXct, chron, yearmon, yearqtr or timeDate. To test whether or not the
index class is supported, either the function is.timeBased() or timeBased() can
be used. The time zone of the index can be queried or assigned with the function
indexTZ(). Similarly, the format of an index representation can be retrieved or
modiﬁed with the function indexFormat(). Whether a time series index is ordered
can be queried with the function isOrdered (increasing/decreasing and with/without
duplicate index entries). The index entries of a time series can be made unique with
either of the functions make.index.unique() or make.time.unique(). This is
achieved either by dropping duplicate entries or by adding fractions of a second to
a time stamp. The default increment is set to a hundredth of a second, which would
appropriate for for high-frequency data. To produce a sequence of date time vectors
the functions timeBasedSeq() or timeBasedRange() can be used (the latter
will return a two-element vector containing the number of seconds elapsed since 01
January 1970 for the start and end date times). The kind of speciﬁcation for the start,
end and periodicity for these two functions is quite user-friendly. This information is
supplied to the function in the form of a character string with format equal to either
from/to/by or from::to::by. Incidentally, the latter can be omitted and then
the date time increments depend on the granularity of the date time speciﬁcations
for from and/or to. The class of the returned object can be controlled by means
of the retclass argument and the length of the object returned can controlled by
the argument length.out, which takes precedence if in conﬂict with the stated
range/periodicity. An estimate of the periodicity of an xts object is returned by the
function periodicity(), which is accomplished by computing the median time
between observations. The length (in given units) of an xts object is returned by the
function nfoo(), where foo is a placeholder for seconds, minutes, hours, days,
weeks, months, quarters or years.
The second group of functions are for manipulating, aggregating and subsetting
xts objects. Similarly to the aggregate() method in the package zoo, two sets of

APPENDIX B: TIME SERIES DATA
337
facilities for applying a function to date/frequency ranges are implemented. The ﬁrst
set are the apply.foo() functions, where foo is a placeholder for daily, weekly,
monthly, quarterly or yearly. The function FUN is applied to the time series
data contained in the non-overlapping periods as deﬁned by foo. The second set of
functions consist of period.apply() and wrapper functions period.foo() where
foo is the short-cut for the functions max, min, prod or sum. These routines return
the function value pertinent to the time intervals implied by the argument index,
and the subsequences are deﬁned as INDEX[k] to INDEX[k + 1] for k = 1 :
(length(INDEX)-1). Finally, the method split.xts() enables the user to split
an xts object according to a certain periodicity. This method returns a list object
of xts objects in which the elements are the time series data of the non-overlapping
periods requested.
References
Armstrong W. 2009 its: Irregular Time Series. R package version 1.1.8, Portfolio & Risk
Advisory Group and Commerzbank Securities.
Armstrong W. 2012 fts: R interface to tslib (a time series library in C++). R package version
0.7.7.
Chalabi Y., M¨achler M., and W¨urtz D. 2011 Rmetrics – timeDate Package. R Journal 3(1),
19–24.
Gilbert P. 2011 tframePlus: Time Frame coding kernel extensions. R package version
2011.11-2.
Hallman J. 2012 tis: Time Indexes and Time Indexed Series. R package version 1.19.
James D. and Hornik K. 2011 chron: Chronological Objects which Can Handle Dates and
Times. R package version 2.3-42. S original by David James, R port by Kurt Hornik.
Plummer M., Best N., Cowles K. and Vines K. 2006 CODA: Convergence diagnosis and output
analysis for MCMC. R News 6(1), 7–11.
Ripley B. and Hornik K. 2001 Date-time classes. R News 1(2), 8–11.
Ryan J. and Ulrich J. 2012 xts: eXtensible Time Series. R package version 0.8-6.
Therneau T., Lumley T., Halvorsen K. and Hornik K. 2011 date: Functions for handling dates.
R package version 1.2-32. S original by Terry Therneau, R port by Thomas Lumley, Kjetil
Halvorsen, and Kurt Hornik.
Trapletti A. and Hornik K. 2012 tseries: Time Series Analysis and Computational Finance. R
package version 0.10-28.
W¨urtz D. and Chalabi Y. 2012 timeSeries: Rmetrics – Financial Time Series Objects. R package
version 2160.94.
W¨urtz D., Chalabi Y. and Ellis A. 2009 A Discussion of Time Series Objects for R in Finance
Rmetrics Ebook Series 9 july 2009 edn. Finance Online GmbH, Zurich.
W¨urtz D., Chalabi Y., Maechler M. and Byers J. 2012 timeDate: Rmetrics Chronological and
Calendarical Objects. R package version 2160.95.
Zeileis A. and Grothendieck G. 2005 zoo: S3 infrastructure for regular and irregular time
series. Journal of Statistical Software 14(6), 1–27.

Appendix C
Back-testing and reporting of
portfolio strategies
C.1
R packages for back-testing
The following packages provide either functions or class/method deﬁnitions for
conducting portfolio back-tests and analysing the results thereof. It should be noted
that the results of these back-tests might fall short of the user’s requirements and
so might be amended by further statistics and ﬁgures, for instance, by employing
the facilities available in the package PerformanceAnalytics (see Carl et al. 2012).
Possibilities for generating tailor-made reports will be introduced in the next section.
The package stockPortfolio (see Diez and Christou 2012) is dedicated solely
to stock portfolio analysis and provides routines for data retrieval from Yahoo!,
portfolio optimization and carrying out a back-test for a given strategy. The latter is
implemented as function testPort(). The package is hosted on CRAN.
In the package backtest (see Enos et al. 2010) an S4 class backtest is deﬁned
with associated methods for exploring portfolio-based conjectures about the assets
included. This class deﬁnition is not conﬁned to assets belonging to a certain class, but
the constituent ﬁnancial instruments can be stocks, bonds, swaps, options, currencies,
etc. Objects of this class can be generated with the function backtest(). The
package is hosted on CRAN.
Finally, the package fPortfolioBacktest (see W¨urtz et al. 2010) is hosted on
R-Forge only. Similar to backtest, an S4 class fPFOLIOBACKTEST is deﬁned which
contains all of the necessary back-test information. Objects of this class are created
with the function portfolioBacktest() where the user has control over the re-
balancing periodicity, the kind of portfolio optimization and whether a smoothing
algorithm for the portfolio weights is employed.
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

APPENDIXC:BACK-TESTINGANDREPORTINGOF PORTFOLIOSTRATEGIES
339
C.2
R facilities for reporting
Quite often the need arises to prepare summary reports on a portfolio strategy/
optimization and/or to evaluate the performance of a certain indicator or market risk
model. Here a list of R packages and functions is provided that enable the user to
produce either output in the form of a certain type of ﬁle or R objects in a form that
can be digested by other applications for further processing. The reader is referred to
the Omegahat project (http://www.omegahat.org) for additional packages that
might be useful in this respect.
r Platform- or application-unspeciﬁc:
– PDF: Sweave() in utils (see Leisch 2002), Hmisc (see Harrell 2012), r2lh
(see Genolini et al. 2011), xtable (see Dahl 2012)
– HTML: batade (see Daisuke 2011), HTMLUtils (see Loecher 2010),
hwriter (see Pau 2010), R2HTML (see Lecoutre 2003), r2lh (see Genolini
et al. 2011), SortableHTMLTables (see White, 2010), xtable (see Dahl
2012)
– XML:
Runiversal
(see
Satman
2010),
XML
(see
Lang
2012a),
XMLSchema (see Lang 2012b)
r Platform- or application-speciﬁc:
– MS Windows: rcom (see Baier 2012), RDCOMClient (see Lang 2007),
RDCOMServer (see Lang 2005)
– MS Ofﬁce: R2PPT (see Jones 2011), excel.link (see Demin 2011),
RExcelInstaller (see Neuwirth 2012), xlsReadWrite (see Suter 2011),
XLConnect (see GmbH 2012), xlsx (see Dragulescu 2012)
– OpenOfﬁce: ODB (see Mareschal 2011), odfWeave (see Kuhn et al. 2011)
C.3
Interfacing databases
There are quite a few R packages available that allow the user to import and export
data from an (R)DBMS. A typical work ﬂow would then involve importing the data
sample from a database, executing the risk and/or portfolio optimization computations
and exporting the results back into the database for further processing. Here a list of R
packages hosted on CRAN is given with which this procedure can be accomplished.
In addition to the documentation for each package, the reader is referred to the R
manual R Data Import/Export for further information. There is also a SIG email list,
R-sig-DB, dedicated to interfacing databases from R. Subscriptions to this list can
be established via https://stat.ethz.ch/mailman/listinfo/r-sig-db.
r DBMS-speciﬁc (in alphabetical order):
– Berkely: RBerkeley (see Ryan 2011)
– H2: RH2 (see Grothendieck and Mueller 2011)

340
APPENDIXC:BACK-TESTINGANDREPORTINGOF PORTFOLIOSTRATEGIES
– Mongo: RMongo (see Chheng 2011), rmongodb (see Lindsly 2012)
– Oracle: ROracle (see Mukhin et al. 2012), ROracleUI (see Magnusson
2011)
– PostgreSQL: RPostgresSQL (see Conway et al. 2012), RpgSQL (see
Grothendieck 2011)
– SQL: RMySQL (see James and DebRoy 2012), RSQLite (see James 2011),
dbConnect (see Kurkiewicz et al. 2011)
r DBMS-unspeciﬁc (in alphabetical order):
– Generic database interfaces: DBI (see James 2009)
– Java-API: RJDBC (see Urbanek 2011)
– ODBC: RODBC (see Ripley and Lapsley 2012)
References
Baier T. 2012 rcom: R COM Client Interface and internal COM Server. R package version
2.2-3.1.1.
Carl P., Peterson B., Boudt K. and Zivot E. 2012 PerformanceAnalytics: Econometric tools for
performance and risk analysis. R package version 1.0.4.4.
Chheng T. 2011 RMongo: MongoDB Client for R. R package version 0.0.21.
Conway J., Eddelbuettel D., Nishiyama T., Prayaga S. and Tifﬁn N. 2012 RPostgreSQL: R
interface to the PostgreSQL database system. R package version 0.3-2.
Dahl D. 2012 xtable: Export tables to LaTeX or HTML. R package version 1.7-0.
Daisuke I. 2011 batade: HTML reports and so on. R package version 0.1.
Demin G. 2011 excel.link: Convenient way to work with data in Microsoft Excel. R package
version 0.5.
Diez D. and Christou N. 2012 stockPortfolio: Build stock models and analyze stock portfolios.
R package version 1.2.
Dragulescu A. 2012 xlsx: Read, write, format Excel 2007 and Excel 97/2000/XP/2003 ﬁles. R
package version 0.4.2.
Enos J., Kane D., Campbell K., Gerlanc D., Schwartz A., Suo D., Colin A., and Zhao L. 2010
backtest: Exploring portfolio-based conjectures about ﬁnancial instruments. R package
version 0.3-1.
Genolini C., Desgraupes B. and Franca L. 2011 r2lh: R to LaTeX and HTML. R package
version 0.7.
GmbH MS. 2012 XLConnect: Excel Connector for R. R package version 0.1-9.
Grothendieck G. 2011 RpgSQL: DBI/RJDBC interface to PostgreSQL Database. R package
version 0.1-5.
Grothendieck G. and Mueller T. 2011 RH2: DBI/RJDBC interface to h2 Database. R package
version 0.1-2.8.
Harrell F. 2012 Hmisc: Harrell Miscellaneous. R package version 3.9-3.

APPENDIXC:BACK-TESTINGANDREPORTINGOF PORTFOLIOSTRATEGIES
341
James D. 2009 DBI: R Database Interface. R package version 0.2-5.
James D. 2011 RSQLite: SQLite interface for R. R package version 0.11.1.
James D. and DebRoy S 2012 RMySQL: R interface to the MySQL database. R package version
0.9-3.
Jones W. 2011 R2PPT: Simple R Interface to Microsoft PowerPoint using rcom or RDCOM-
Client. R package version 2.0.
Kuhn M., Weston S., Coulter N., Lenon P. and Otles Z. 2011 odfWeave: Sweave processing of
Open Document Format (ODF) ﬁles. R package version 0.7.17.
Kurkiewicz D., Hofmann H. and Genschel U. 2011 dbConnect: Provides a graphical user
interface to connect with databases that use MySQL. R package version 1.0.
Lang D. 2005 R-DCOM object server. R package version 0.6-1.
Lang D. 2007 RDCOMClient: R-DCOM client. R package version 0.92-0.1.
Lang D. 2012a XML: Tools for parsing and generating XML within R and S-Plus. R package
version 3.9-4.1.
Lang D. 2012b XMLSchema: R facilities to read XML schema. R package version 0.7-0.
Lecoutre E. 2003 The R2HTML package. R News 3(3), 33–36.
Leisch F. 2002 Dynamic generation of statistical reports using literate data analysis. In Comp-
stat 2002 – Proceedings in Computational Statistics (ed. H¨ardle W. and R¨onz B.), pp.
575–580. Physika Verlag, Heidelberg.
Lindsly G. 2012 rmongodb: R-MongoDB driver. R package version 1.0.3.
Loecher M. 2010 HTMLUtils: Facilitates automated HTML report creation. R package version
0.1.4.
Magnusson A. 2011 ROracleUI: Convenient Tools for Working with Oracle Databases. R
package version 1.3-2.
Mareschal S. 2011 ODB: Open Document Databases (.odb) management. R package version
1.0.0.
Mukhin D., James D. and Luciani J. 2012 ROracle: OCI based Oracle database interface for
R. R package version 1.1-2.
Neuwirth E. 2012 RExcelInstaller: Integration of R and Excel (use R in Excel, read/write XLS
ﬁles). R package version 3.2.3-1.
Pau G. 2010 hwriter: HTML Writer – Outputs R objects in HTML format. R package version
1.3.
Ripley B. and Lapsley M. 2012 RODBC: ODBC Database Access. R package version 1.3-5.
Ryan J. 2011 RBerkeley: R API to Oracle Berkeley DB. R package version 0.7-4.
Satman M. 2010 Runiversal: Runiversal – Package for converting R objects to Java variables
and XML. R package version 1.0.1.
Suter HP. 2011 xlsReadWrite: Read and write Excel ﬁles (.xls). R package version 1.5.4.
Urbanek S. 2011 RJDBC: Provides access to databases through the JDBC interface. R package
version 0.2-0.
White J. 2010 SortableHTMLTables: Turns a data frame into an HTML ﬁle containing a
sortable table. R package version 0.1-2.
W¨urtz D., Chalabi Y., Chen W. and Ellis A. 2010 Portfolio Optimization with R/Rmetrics.
Rmetrics Association & Finance Online, www.rmetrics.org. R package version 2110.4.

Appendix D
Technicalities
This book was typeset in LaTEX. In addition to the publisher’s style ﬁle, the following
LaTEX packages were used (in alphabetical order): amsfonts, amsmath, amssymb,
booktabs, ﬂoat, listings, longtable, natbib, rotﬂoat, tikz and url. The bibliography
was generated with BiBTeX. The program aspell was used for spell-checking.
The Emacs text editor was used with the LISP modules ESS and AUCTEX. The
processing of all ﬁles (i.e., the creation of the book) was accomplished with the make
program, and Subversion (SVN) was used as source control management system.
All the R code examples were processed as Sweave ﬁles. Therefore, the proper
working of the R commands is guaranteed. In the .Rprofile ﬁle the seed for
generating random numbers was set to set.seed = 123456 and as random number
generator R’s default setting was employed, that is, random numbers were generated
using the Mersenne Twister algorithm. Where possible, the results are exhibited as
tables by making use of the function latex() contained in the contributed package
Hmisc. The examples were processed in R version 2.15.0 on an i686 PC with Linux
as operating system and kernel 2.6.38-15-generic. Linux is a registered trademark of
Linus Torvalds (Helsinki, Finland), the original author of the Linux kernel.
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

Index
Note: Figures are indicated by italic page numbers, listings and tables by emboldened
numbers, and footnotes by sufﬁx ‘n’.
ACF plots
GARCH(1, 1) models for European
data, 148
NYSE exceedance for Boeing losses,
109
Siemens stock returns, 28
ADF, see augmented Dickey–Fuller test
AER package, 314, 319
data set in, 123
Archimedean copulae, 134, 135, 136,
141
advantages, 136
ARCH models, 112–116
expectations equation, 113
variance equation, 113
ARCH(1) process, 113, 114
ARCH(4) process, 113, 114
ARFIMA models, R packages for, 281,
282
ARIMA models
in protection strategy example, 302
R packages for, 281, 282
ARMA-GARCH models, 118, 144
ARMA models, R packages for,
278–280, 281
ARMA(p,q) time series process,
260–262
AR(p) time series process, 256–258
asymmetric power ARCH (APARCH)
models, 115, 116
special cases, 116
augmented Dickey–Fuller (ADF) unit
root test, 284, 285, 289
autocorrelation function, see ACF plots
autoregressive conditional
heteroscedastic models, see
ARCH models
autoregressive moving average, see
ARMA
autoregressive process, see AR(p) time
series process
average drawdown (AvDD), 227, 229
average drawdown (AvDD) portfolio
compared with other portfolio asset
allocations, 246, 247
drawdown plots, 244
linear program formulation, 228
solution, 231
backtesting
GMV vs CDaR portfolio
optimization, 247–253
minimum-CVaR vs minimum-
variance portfolios, 238–241
Financial Risk Modelling and Portfolio Optimization with R, First Edition. Bernhard Pfaff.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

344
INDEX
backtesting (Continued )
minimum-variance portfolio,
robust vs classical estimators,
177–181
MSR portfolio, 291, 293
portfolio simulation for protection
strategy, 308, 309
R packages for, 338
backtest package, 338
Basel Accords requirements, 34, 143,
217
bayesGARCH package, 116, 117, 314,
318
Bayesian analysis/estimation
expected returns in BL model, 271,
272
extreme value models, 90
GARCH(1, 1) models, 116
SVAR model, 284
VAR models, 283, 284
BCC portfolio solution, 195
for multi-asset portfolios, 211–215
bi-square function, 158
bivariate extreme value distributions,
90, 92
Black–Litterman (BL) model, 255,
270–272
COP extension, 273, 274
EP extension, 274–276
example application, 288–295
R package to handle, 276–278
BLCOP package, 136–138, 276–278,
314, 317, 319
applications, 291, 295
block maxima method
applications, Siemens stock losses,
99–101
extreme value distributions, 85, 86,
91, 92, 94, 95
BMW losses, r block maxima model
for, 101–105
Boeing stock losses
ﬁtted GPD model, 106
diagnostic plots, 106, 107
MRL plot, 106
POT method for, 105–110
risk measures for, 107
Box–Jenkins approach [to time series
modelling], 260, 282
breakdown point [of estimator],
157
CAC Index, 31
boxplot, 179
correlation with other European data,
31, 32
descriptive statistics, 179
GARCH(1, 1) models, 147
ACF plots, 148
QQ plots, 147, 148
prior and posterior density plots,
297, 300
stock index value trajectory, 304
unit root test statistics, 289
weights based on prior and BL
distributions, 297
capital asset pricing model (CAPM),
271
capital market line (CML)
mean–variance portfolio, 47
mean–VaR portfolio, 222
Cauchy copula, 141
ccgarch package, 117, 314, 318
chron package, 315, 319, 325
Clayton copula, 135, 136, 137
mix with Gumbel copula, 149–151
coda package, 314, 320
applications, 90, 116, 332n1
coherent risk measure, 41
co-integration model, 266, 284
co-monotonicity, 129, 134
concentration ratio (CR), 191, 200
various portfolio solutions
for multi-asset portfolios, 214
for S&P500 Index constituents,
211
for Swiss equity sectors, 206
concordance, 131
conditional draw-down at risk (CDaR),
227, 228, 229

INDEX
345
conditional draw-down at risk (CDaR)
portfolio
compared with global minimum-
variance allocation, 247–253
compared with other portfolio asset
allocations, 246, 247
draw-down plots, 244
linear program for, 228, 229, 237
solution, 231
conditional value-at-risk (CVaR)
deﬁnition in terms of other risk
measures, 223, 224
as risk measure, 194
see also expected shortfall
conditional value-at-risk (CVaR)
portfolios, optimization of,
223–227, 229, 230
constructor functions, 15–16, 22
copulae, 130–136
classiﬁcation of, 133–136
Archimedean copulae, 134–136
Clayton copula, 135, 136, 137
Gauss copula, 134, 137
Gumbel copula, 135, 136, 137
scatter diagrams for, 136, 137
Student’s t copula, 135, 136, 137
empirical applications, 142–151
GARCH–copula models,
142–149
mixed copula approaches,
149–151
relationship to rank correlations,
131–133
R packages, 136–142
BLCOP package, 136–138, 314,
317
copula package, 138–140, 314,
317
fCopulae package, 140, 141, 315,
317
gumbel package, 141, 142, 315,
317
nacopula package, 140, 316, 317
QRM package, 142, 316, 317
copula–GARCH models, 121, 142–149
copula opinion pooling (COP), 136,
137, 273, 274
example application, 295–299
copula package, 138–140, 314, 317
applications, 149, 171, 172
Cornish–Fisher VaR, 37, 38
correlation coefﬁcients, 127–129
counter-monotonicity, 129, 134
covRobust package, 166, 314, 319
CPLEX solver package, interface to,
232
CRAN (Comprehensive R Archive
Network), 7
packages, 9
ctv package, 9, 314, 320
cVaR, see conditional value-at-risk
CVaR-optimal portfolios, 223–227
daily-earnings-at-risk measure, 35
database interfacing, R packages for,
339, 340
date package, 315, 319, 324, 325
date-time classes, 324–327
Davies package, 67, 314, 318
DAX Index, 21, 31, 177
boxplots, 179, 297
comparison of draw-down portfolios,
246
correlation with other European data,
31, 32
descriptive statistics, 179
GARCH(1, 1) models, 147
ACF plots, 148
QQ plots, 147, 148
prior and posterior density plots,
297, 300
stock index value trajectory, 304
unit root test statistics, 289
weights and risk contributions for
various asset allocations, 214
weights based on prior and BL
distributions, 297
DEoptim package, 197–199, 315,
318
dependence modelling, 127–152

346
INDEX
Dickey–Fuller test, see augmented
Dickey–Fuller test
Differential Evolution (DE) algorithm,
198
see also DEoptim package
discrete loss distribution, relations
between risk measures for, 224
distribution classes, 53–62
diversiﬁcation
empirical applications, 201–215
comparison of approaches,
201–206
limiting contributions to expected
shortfall, 211–215
optimal tail-dependent portfolio
against benchmark, 206–211
meaning of term, 189, 192
see also most-diversiﬁed portfolio;
optimal tail-dependent
portfolios; risk contribution
constrained portfolios
diversiﬁcation ratio (DR), 190
GMV versus draw-down portfolios,
245, 246, 247
various portfolio solutions
for multi-asset portfolios, 214,
245, 246, 247
for S&P500 Index constituents,
211
for Swiss equity sectors, 206
Dow Jones 30 data set, 71, 105
drawdown
AvDD portfolio, 244
CDaR portfolio, 244, 251, 252
GMV portfolio, 242, 251, 252
MaxDD portfolio, 244
meaning of term, 227
drawdown constrained portfolios,
227–229
applications, 242–247
see also average draw-down
portfolio; conditional
draw-down at risk portfolio;
maximum drawdown portfolio;
minimum-CDaR portfolio
dse package, 278–280, 315, 319
efﬁcient frontiers
mean–variance portfolios, 45, 47, 48,
49
compared with robustly optimized
portfolios, 164, 182, 186, 187
mean–VaR portfolios, 219, 220
Elliott–Rothenberg–Stock (ERS) unit
root test, 285, 289
elliptical uncertainty sets, 162, 163
empirical mean–variance portfolios,
47–49
Engle–Granger long-run relationship,
284
entropy pooling (EP) model, 273,
274–276
‘equal-risk contribution’ (ERC)
portfolio, 192, 193
multi-asset portfolios, 211–215
solution, 200
Swiss equity sectors, 201–206
ERS, see Elliott–Rothenberg–Stock test
ESCBFX data set, 21, 22, 302
European stocks
data sets, 21, 29–32, 146, 247, 288,
289
ﬁtted GARCH(1, 1) models, 147
ACF plots of squared standardized
residuals, 148
QQ plots of standardized
residuals, 147, 148
stylized facts on, 29–32
EuroStoxx50 data set, 21, 247
EuStockMarkets data set, 29, 146, 289
EvalEst package, 278n3, 315, 319
evdbayes package, 90, 91, 315, 317
evd package, 89, 90, 315, 317
evir package, 91–93, 315, 317
applications, 27, 91–93, 99–101,
302
data sets in, 27, 93, 101
expected shortfall (ES) risk measure, 36
behaviour with GHD, HYP and NIG
models, 75
computation for given probability of
error, 59
dependence on VaR, 36, 223

INDEX
347
inferred from GPD, 88
Boeing stock losses, 107
modiﬁed, 38
in protection strategy example,
305
various portfolio solutions
for multi-asset portfolios, 214
for S&P500 Index constituents,
211
for Swiss equity sectors, 206
and volatility of NYSE daily losses,
123–125
exploratory data analysis (EDA), in
extreme value theory, 91, 93
exponential GARCH (EGARCH)
models, 115
extRemes package, 95, 96, 315,
317
extreme value copulae, 141
extreme value distributions, 85, 86, 87,
88, 89
extreme value theory (EVT), 84–111
empirical applications, 98–110
methods and models, 85–88
block maxima approach, 85, 86
peaks-over-threshold (POT)
approach, 87, 88
rth largest order models, 86, 87
R packages, 89–98
evdbayes package, 90, 91, 315,
317
evd package, 89, 90, 315, 317
evir package, 91–93, 315, 317
extRemes package, 95–96, 315,
317
fExtremes package, 93–95, 315,
317
ismev package, 95, 315, 317
POT package, 96, 97, 316, 317
QRM package, 97, 316, 317
Renext package, 97, 98, 316, 317
fArma package, 281, 315, 319
‘fat/heavy tails’, 28, 142
fBasics package, 27, 62, 63, 67, 68, 80,
315, 318
fCopulae package, 140, 141, 295, 315,
317
fEcoﬁn package, 315, 319
data sets in, 71, 105
fExtremes package, 93–95, 315, 317
applications, 105–110
fGarch package, 118, 315, 318
applications, 123–125, 146
ﬁnancial crises, 3
GMV compared with CDaR
strategies, 249
GMV compared with CVaR
strategies, 241
wealth-protection strategies, 299,
300
ﬁnancial market returns, stylized facts,
26–32
ﬁnancial market risks, modelling of,
34–42
forecast package, 281–283, 302, 315,
319
fPortfolioBacktest package, 315, 318,
338
fPortfolio package, 166, 167, 315, 318,
319
applications, 138, 167n2, 177, 203,
229, 230, 247, 277, 291, 295
fracdiff package, 281
FRAPO package, 20–25, 315, 319
applications, 78, 80, 149, 203, 302
data sets in, 21, 22, 78, 80, 177, 206,
211, 212, 238, 242, 247, 302
installation and loading, 20
portfolio optimization approaches,
22, 199, 200, 230, 231
Fr´echet distribution, 85, 86, 89, 100,
104, 105
Fr´echet–Hoeffding bounds, 133
fTrading package, 255, 315, 319
FTSE 100 Index, 21, 31, 80, 177
boxplots, 179, 297
comparison of draw-down portfolios,
246
correlation with other European data,
31, 32
descriptive statistics, 179

348
INDEX
FTSE 100 Index (Continued )
GARCH(1, 1) models, 147
ACF plots, 148
QQ plots, 147, 148
prior and posterior density plots,
297, 300
shape triangle for, 80, 81
stock index value trajectory, 304
unit root test statistics, 289
weights and risk contributions for
various asset allocations, 214
weights based on prior and BL
distributions, 297
fts package, 315, 319, 332n1
fUnitRoots package, 285n4, 315, 319
GARCH–copula models, 121, 142–149
application(s), 146–149
contrasted with variance–covariance
approach, 143, 144
steps in determining portfolio risks,
145, 146
GARCH models, 114, 115
R packages, 116–122
bayesGARCH package, 116, 117,
314, 318
ccgarch package, 117, 314, 318
fGarch package, 118, 315, 318
gogarch package, 118–120, 315,
318
rmgarch package, 121, 122, 316,
318
rugarch package, 120, 121, 316,
318
tseries package, 122, 317, 318
GARCH(1, 1) models
Bayesian estimation of, 116
expected shortfall derived from,
123–125
ﬁtted for European stock market
data, 147
ACF plots of squared standardized
residuals, 148
QQ plots of standardized
residuals, 147, 148
unconditional variance for, 115
GARCH(p, q) models, 114
Gauss copula, 134, 137
with normally distributed margins,
portfolio simulation comparing
robust and classical estimators,
171, 176, 176, 177
with t-distributed margins, portfolio
simulation comparing robust
and classical estimators, 171,
176, 177
Gauss–Seidel algorithm, 265
generalized extreme value (GEV)
distribution, 86, 89, 92, 93
GeneralizedHyperbolic package, 63,
64, 315, 318
generalized hyperbolic distribution
(GHD), 53–55
applications to risk modelling, 69–78
density function, 54
ﬁtting stock returns to, 69–73
reparameterizations, 54
risk assessment with, 73–75
R packages, 62–67
fBasics, 27, 62, 63, 315, 318
GeneralizedHyperbolic, 63, 64,
315, 318
ghyp, 64, 65, 71, 315, 318
QRM, 65, 66, 316, 318
SkewHyperbolic, 66, 316, 318
VarianceGamma, 67, 317, 318
see also hyperbolic (HYP)
distribution; normal inverse
Gaussian (NIG) distribution
generalized lambda distribution (GLD),
56–62
applications to data analysis, 79, 80
applications to risk modelling, 78, 79
estimation methods for optimal
values of λ, 60–62
goodness-of-ﬁt approach, 61, 62
histogram-based approach, 61
maximum-likelihood/maximum-
product-spacing methods, 62
moment-matching approach, 60,
61
percentile-based approach, 61

INDEX
349
probability density function, 56
R packages, 67–69
Davies, 67, 314, 318
fBasics, 67, 68, 80, 315, 318
gld, 68, 69, 315, 318
lmonco, 69, 315, 318
reparameterizations, 58
shape plot, 59
valid parameter combinations, 57, 58
generalized orthogonal GARCH
(GOGARCH) models, 118, 121
generalized Pareto distribution (GPD),
87, 88, 89, 92, 93
generic functions, 12, 13
German REX bond index, 214, 242,
246
ghyp package, 64, 65, 71, 76, 315, 318
gld package, 68, 69, 315, 318
global minimal variance (GMV)
portfolio, 45
compared with draw-down
portfolios, 245, 246, 247–253
compared with global
minimum-CVaR portfolio,
238–241
draw-down plot, 242
multi-asset portfolios, 211–215
Swiss equity sectors, 201–206
glpkAPI package, 232, 315, 318
GNU Linear Programming Kit
(GLPK), 232, 238
access to, 232
gogarch package, 118–120, 315, 318
gold index, 214, 242, 246
Gumbel copula, 135, 136, 137
mix with Clayton copula, 149–151
Gumbel distribution, 86, 89
gumbel package, 141, 142, 315, 317
Hang Seng Index (HSI), 21, 178, 179,
179, 304
Hewlett-Packard (HWP) stock returns
ﬁtted-density plots, 71
ﬁtting to GHD, 69–73
QQ plots, 71, 72, 72
shape triangle for, 76
Hmisc package, 315, 320, 339, 342
Huber functions, 158
Huber M-estimators, 157, 158
implementation of, 167, 168, 169,
170
hyperbolic (HYP) distribution, 54, 55
shape triangle, 55
‘inference for margins’ approach, 136,
144
ismev package, 95, 315, 317
applications, 101–105
functions included, 95, 100, 101
its package, 315, 319, 328, 329, 332
Joe–Clayton copula, 149
Kendall’s rank correlation coefﬁcient
(tau), 131, 132, 136
lambda distributions, 56
see also generalized lambda
distribution
lattice package, 203, 204
least-squares (LS) method, 157, 158
compared with M-estimators, 158
limsolve package, 235, 315, 318
linear programming
optimal CVaR portfolios, 225, 226
optimal draw-down portfolios, 228,
229
R packages
glpkAPI package, 232, 315, 318
linprog package, 233, 234, 315,
318
lpsolve package, 233, 235
lpSolveAPI package, 235
Rcplex package, 232
Rglpk package, 230, 232, 233,
316, 318
Rmosek package, 232
Rsymphony package, 235, 236,
316, 318
wealth-protection strategies, 300,
306–308
linprog package, 233, 234, 315, 318

350
INDEX
L-moments, estimation methods based
on, 69, 315
lmonco package, 69, 78, 315, 318
Lorenz cone, 165
loss distribution, 35
lpSolveAPI package, 235, 316, 318
lpSolve package, 233, 235, 316, 318
macroeconomic modelling, large-scale
examples, 262
MA(q) time series process, 258–260
marginal risk contributions, 39, 193
Swiss equity sectors example, 204,
205
Markov chain Monte Carlo (MCMC)
techniques, R packages dealing
with, 90, 116, 283, 332n1
Markowitz portfolios, 43–47
optimization of, 44
in BL model, 272
MASS package, 167, 168, 316, 319
applications, 137
maximum drawdown (MaxDD), 227,
229
maximum drawdown (MaxDD)
portfolio
compared with other portfolio asset
allocations, 245–247
draw-down plot, 244
linear program formulation, 228,
237
solution, 230, 231
maximum-likelihood (ML) principle,
157, 158
ARMA-GARCH model parameters
estimated using, 144, 145
AR(p) process estimated using, 257
compared with M-estimators, 158
extreme value distribution
parameters estimated using, 89,
95, 98, 100
GHD/HYP/NIG parameters
estimated using, 63
GLD parameters estimated using, 62
GPD parameters estimated using, 88
maximum Sharpe ratio (MSR)
portfolio, 45, 47
backtest, 291, 293
compared with BL and equal-
weighted approaches, 294
speciﬁcations, 292, 298
MCC portfolio solution, 195
for multi-asset portfolios, 211–215
mean–CVaR portfolios, 223–227
relation with mean–VaR and mean–
variance portfolios, 226, 227
mean–variance portfolios, 43–45
asymptotes of hyperbola, 45
efﬁcient frontier, 45, 47, 48, 49,
186
empirical applications, 47–49
relation with mean–CVaR and mean–
VaR portfolios, 226, 227
robust optimization of, 160, 161
mean–VaR portfolios, 218–223
asymptotes of hyperbola, 221
efﬁcient frontiers, 219, 220
relation with mean–CVaR and mean–
variance portfolios, 226, 227
M-estimators, 157, 158
implementation of, 167, 168, 169,
170
performance, 176, 177
methods package, 14, 19, 24, 137
minimum-CDaR portfolio
compared with other portfolio asset
allocations, 246, 247
drawdown plots, 244
minimum covariance determinant
(MCD) estimator, 158, 159
implementation of, 167, 169, 170
performance, 176, 180, 181
minimum-CVaR portfolio, 226
compared with minimum-variance
portfolio, 238–241
minimum tail dependence (MTD)
portfolio
solution, 200
Swiss equity sectors, 201–206
minimum-VaR portfolio, 220, 221

INDEX
351
minimum volume ellipsoid (MVE)
estimator, 158, 159
implementation of, 167, 170
performance, 176, 181
mixed copula model, 149–151
ML principle, see maximum-likelihood
principle
MM estimators, 158
implementation of, 170
performance, 176, 177, 180, 181
modern portfolio theory, 43–49
modiﬁed expected-shortfall (mES) risk
measure, 38
modiﬁed value-at-risk (mVaR)
measure, 37, 38
monotonicity risk measure, 40
MOSEK optimizer, interface to, 232
most-diversiﬁed portfolio (MDP),
190–192
core properties, 192
solution, 200
Swiss equity sectors, 201–206
moving average, see MA
MSBVAR package, 283, 284, 316, 319
MSCI Emerging Market index, 214,
242, 246
MultiAsset data set, 21, 22, 211, 213,
242
multivariate ﬁnancial market returns,
stylized facts for, 28–32
multivariate GARCH classes and
concepts, implementation of,
117, 121, 122
multivariate risk modelling,
dependence in, 128
multivariate time series models,
262–270
R packages for, 278–280
structural multiple equation models
(SMEMs), 262–265
structural vector autoregressive
(SVAR) models, 266, 268
structural vector error correction
models (SVEC) models, 266,
269, 270
vector autoregressive (VAR) models,
265–267
vector error correction models
(VECMs), 266, 268, 269
nacopula package, 140, 316, 317
applications, 206
nested Archimedean copulae, 140
Network-Enabled Optimization System
(NEOS), access to, 232
New York Stock Exchange (NYSE)
Index daily returns, 108, 123
exceedances
clustering of, 108, 109
de-clustering of, 108, 109, 110
losses compared with expected
shortfall, 123–125
Nikkei 225 (N225) Index, 21, 177, 179,
179, 214, 238, 242, 246, 304
normal inverse Gaussian (NIG)
distribution, 55
normality assumption, violation in
ﬁnancial market return data, 156
OGK estimator, see orthogonalized
Gnanadesikan–Kettenring
estimator
optimal CVaR portfolios, 223–227
optimal drawdown portfolios, 227–229
optimal tail-dependent portfolio,
195–197
construction of, 200
performance against benchmark,
206–211
optimal weight vector
determination in most-diversiﬁed
portfolio, 191, 192
method for extracting, 24
minimum-variance portfolio, 164
option-based portfolio insurance
(OBPI), 299
order statistics distributions, 89, 95
ordinary least-squares (OLS) method,
AR(p) process estimated using,
257

352
INDEX
orthogonalized Gnanadesikan–
Kettenring (OGK) estimator,
159, 160
implementation of, 169, 170
performance, 176, 177, 180, 181
outlier(s)
in ﬁnancial market returns data, 178,
179
meaning of term, 156
outlier sensitivity, 60, 132, 155, 156
PACF plots
NYSE exceedance for Boeing losses,
109
Siemens stock returns, 28
pair trading, 284
PairTrading package, 284, 285, 316,
319
partial autocorrelation function, see
PACF plots
partial market model, 263, 264
peaks-over-threshold (POT) method
applications, Boeing stock losses,
105–110
extreme value distributions, 87, 88,
92, 94, 95
Pearson’s correlation coefﬁcient, 127,
128
PerformanceAnalytics package,
236–238, 316, 318
applications, 205, 210, 213, 242,
247, 338
Phillips–Perron unit root test, 284, 286
Poisson point process, estimation in
extreme value theory, 92, 95
PortfolioAnalytics package, 201, 316,
318
applications, 211
portfolio backtest
classical and robust estimators
compared, 177–181
robust optimization with elliptical
uncertainty, 182–187
portfolio optimization
classical case, 160, 161
risk diversiﬁcation and, 189–215
risk-optimal portfolios, 217–254
robust approaches and techniques,
160–165
empirical applications, 171–187
problem formulation, 162–165
R packages, 166–171
R packages, 197–201
DEoptim package, 197–199, 315,
318
PortfolioAnalytics package, 201,
316, 318
RcppDE package, 199, 315,
318
see also robust portfolio optimization
portfolio simulation
classical and robust estimators
compared, 171–177
minimum-variance optimizations,
174, 175
portfolio standard deviation, 44, 190,
192
various portfolio solutions
for multi-asset portfolios, 214
for S&P500 Index constituents,
211
for Swiss equity sectors, 206
portfolio weight(s)
class deﬁned, 15
constructor function(s), 15, 16
effect of constraints on, 48, 49
methods created/deﬁned for, 17–20
validation of, 15–17
positive homogeneity risk measure, 40,
41
POT package, 96, 97, 315, 317
probability–probability (PP) plots, 64,
66, 97
BMW losses, 104, 105
proﬁle log-likelihood plots for GEV
distribution, 95
for Siemens losses, 100, 101, 102
protection strategies, 299, 300
constant-proportion portfolio
insurance, 299
example application, 300–310
analysis of results, 309, 310

INDEX
353
data preparation for simulation,
302, 303
forecasting model, 302–305
linear program, 306–308
portfolio simulation, 308, 309
risk model, 305, 306
option-based portfolio insurance,
299
TAA-related approach, 300, 309, 310
QCOM stock, risk measures, 78, 79
QRM package, 65, 66, 97, 142, 316,
317, 318
data sets in, 146, 149
quadprog package, 316, 318
applications, 174, 277
quantile–quantile (QQ) plots, 64, 66, 97
BMW losses, 104, 105
GARCH(1, 1) models for European
data, 147, 148
Hewlett-Packard returns, 71, 72, 72
Siemens returns, 28, 29
quantmod package, 316, 319
R packages 199, 315, 318port
Ramberg–Schmeiser (RS)
speciﬁcation, 67, 68
Rcplex package, 232
RcppArmadillo package, 120, 316,
320
RcppDE package, 199, 315, 318
Rcpp package, 120, 316, 320
R Development Core Team, 7
relative efﬁciency [of estimator], 157
Renext package, 97, 98, 316, 317
reporting, R facilities fofr, 339
reversed Weibull distribution, 89
reverse optimization, 271
Rglpk package, 230, 232, 233, 316,
318
applications, 302, 307
risk contribution constrained portfolios,
192–195
risk measures, 34–38, 53
conditional value-at-risk deﬁned
using, 223, 224
in portfolio context, 39–41
in protection strategy example, 305,
306
risk-optimal portfolios
empirical applications, 238–253
backtest comparison for stock
portfolio, 247–253
draw-down constrained portfolios,
242–247
minimum-CVaR vs minimum-
variance portfolios, 238–241
mean–VaR portfolios, 218–223
optimal CVaR portfolios, 223–227
optimal draw-down portfolios,
227–229
R packages, 229–238
for linear programming, 232–236
fPortfolio package, 229, 230
PerformanceAnalytics package,
236–238
R language, 4, 6
classes and methods, 12–20
S3 classes and methods, 12–14
S4 classes and methods, 14–20
command line interface, 10
conferences, 10
graphical user interfaces, 10–12
help facilities, 7–10
integrated development
environments, 10–12
mailing lists, 9, 10
manuals, 7, 8
origin and development, 6, 7
Rmetrics packages, 62, 63, 67, 68,
93–95, 118, 140, 141, 166, 167,
170, 171, 281, 315, 317, 318,
319, 326, 330, 331
see also fArma; fBasics; fCopulae;
fExtremes; fGarch;
fPortfolio; fPortfolioBacktest;
Rsocp; timeDate; timeSeries
rmgarch package, 121, 122, 316, 318
Rmosek package, 232
rneos package, 232, 316, 318
robustbase package, 168, 169, 317,
319

354
INDEX
robust estimators, 157–160
measure to assess robustness, 157
robust optimization, 160–165
robust package, 168, 169, 317, 319
robust portfolio optimization, 155–188
empirical applications, 171–187
backtest comparing classical and
robust estimators, 177–181
robust optimization with elliptical
uncertainty, 177–181
simulation comparing classical
and robust estimators, 171–177
R packages, 166–171
covRobust package, 166, 314, 319
fPortfolio package, 166, 167, 315,
319
MASS package, 167, 168, 316,
319
robustbase package, 168, 317,
319
robust package, 168, 169, 317,
319
rrcov package, 169, 170, 317, 319
Rsocp package, 170, 171, 318,
319
robust scaling, estimators based on,
158, 159
robust statistics, 156–160
R packages 199, 315, 318port
rportfolios package, 316, 318
rrcov package, 169, 170, 317, 319
applications, 173
Rsocp package, 170, 171, 318, 319
Rsymphony package, 235, 236, 316,
318
rugarch package, 120, 121, 316, 318
RUnit package, 118, 137, 140, 316, 320
Russell 3000 index, 214, 242, 246
second-order cone program (SOCP),
164, 165
solving, 165, 171, 182
seemingly unrelated regression (SUR)
principle, 283
series, see time series
S-estimators, 159
implementation of, 170
performance, 176, 177, 181
shape triangle
for GLD, 79
FTSE100 returns, 80
for hyperbolic distribution, 55, 75, 76
Hewlett-Packard returns, 76
Siemens stock returns
block maxima model applied to,
99–101
stylized facts on, 27–29
skewed hyperbolic Student’s t
distribution, ﬁtting of data to, 66
skewed Student’s t distribution
in copula opinion pooling example,
295, 298, 301
value-at-risk measures, 38
SkewHyperbolic package, 66, 316,
318
slam package, 233
S language, 6
version 3 (S3), 6
version 4 (S4), 7
SMI
GARCH(1, 1) models, 147
ACF plots, 148
QQ plots, 147, 148
prior and posterior density plots,
297, 300
unit root test statistics, 289
weights based on prior and BL
distributions, 297
sos package, 9, 316, 320
Spearman’s correlation coefﬁcient, 132
SPI, see Swiss Performance Index
Stahel–Donoho estimator (SDE), 159
implementation of, 169, 170
performance, 176, 177, 180, 181
Standard & Poor’s 500 (S&P500)
Index, 21, 78, 177, 238, 242
box plot, 179
comparison of draw-down portfolios,
246
descriptive statistics, 179

INDEX
355
stock index value trajectory, 304
weights and risk contributions for
various asset allocations, 214
Standard & Poor’s 500 (S&P500) Index
and Constituents, 21, 206
portfolio solutions for, 206–211
statistical arbitrage, see pair trading
stats package, 279, 282, 284, 327, 332
StockIndexAdj data set, 21, 22, 23
StockIndexAdjD data set, 21, 22, 302
StockIndex data set, 21, 22, 177, 238
stockPortfolio package, 338
structural multiple equation models
(SMEMs), 262–265
structural vector autoregressive (SVAR)
models, 266, 268
R package for, 286, 287
structural vector error correction
(SVEC) models, 266, 269, 270
R package for, 286, 287
Student’s t copula, 135, 136, 137
with t-distributed margins, portfolio
simulation comparing robust
and classical estimators, 171,
176, 177
stylized facts
on ﬁnancial market returns, 26–32
for multivariate series, 28–32
for univariate series, 26–28
implications for risk models, 32,
33
sub-additivity risk measure, 41
Swiss Market Index, see SMI
Swiss Performance Index (SPI), sector
indexes, comparison of
portfolio solutions, 201–206
Symphony package, interface to, 235,
236
systemﬁt package, 265, 316, 319
tactical asset allocation (TAA),
255–313
empirical applications, 288–310
protection strategy based on, 300,
309, 310
R packages, 276–288
BLCOP package, 276–278, 314,
319
dse package, 278–280, 315, 319
EvalEst package, 278n3, 315, 319
fArma package, 281, 315, 319
forecast package, 281–283, 315,
319
fTrading package, 255, 315, 319
fUnitRoots package, 285n4, 315,
319
MSBVAR package, 283, 284,
316, 319
PairTrading package, 284, 285,
316, 319
quantmod package, 316, 319
systemﬁt package, 265, 316, 319
TTR package, 255, 317, 319
urca package, 285, 286, 317, 319
vars package, 285, 286–288, 317,
319
tail dependence coefﬁcients (TDCs),
195
tail dependencies, 132, 133
tangency mean–VaR portfolio, 222, 223
tawny package, 167n2, 316, 318
tframe package, 279, 317, 319, 334,
335
tframePlus package, 317, 319, 334,
335
timeDate package, 317, 319, 326, 327
time series data, 324–337
date–time classes, 324–327
irregular-spaced time series, 328, 329
R packages
chron package, 315, 319, 325
date package, 315, 319, 324, 325
fts package, 315, 319, 332n1
its package, 315, 319, 328, 329
tframe package, 317, 319, 334,
335
tframePlus package, 317, 319,
334, 335
timeDate package, 317, 319, 326,
327

356
INDEX
time series data (Continued )
timeSeries package, 317, 319,
330, 331
tis package, 317, 319, 332n1
tseries package, 317, 319, 328,
329
xts package, 317, 319, 335–337
zoo package, 317, 319, 332, 334
time series models, 256–270
multivariate time series models,
262–270
univariate time series models,
256–262
timeSeries package, 317, 319, 330, 331
applications, 29, 238, 302, 332
Tinbergen arrow diagram, 263
tis package, 317, 319, 332n1
translation invariance risk measure, 40
tseries package, 317, 318, 319, 328,
329
applications, 122, 284, 285n4, 332
TTR package, 255, 317, 319
Tukey’s bi-square function, 158
Tukey’s lambda distributions, 56
uncertainty sets, 161–163
UK gilts index, 214, 242, 246
US Treasury bond index, 214, 242, 246
unit root tests, 284, 286
univariate ﬁnancial market returns,
stylized facts for, 26–28
univariate time series models, 256–262
ARMA(p,q) process, 260–262
AR(p) process, 256–258
MA(q) process, 258–260
R packages for, 278–280, 281–283
urca package, 285, 286, 289, 317, 319
value-at-risk (VaR) measure, 35, 218
applications
with GHD, HYP and NIG models,
73–75
with GLD, 78, 79
computation for given probability of
error, 59
criticism on use, 36
ES risk measure and, 36, 223
inferred from GPD, 88
Boeing stock losses, 107
modiﬁed, 37, 38
see also conditional value-at-risk;
mean–VaR portfolios
variance–covariance matrix of returns,
189, 200
VarianceGamma package, 67, 317,
318
vars package, 285, 286–288, 289, 317,
319
vector autoregressive (VAR) models,
265–267
R packages for, 283, 284, 286, 287
VAR(p) process, 266, 267
vector error correction models
(VECMs), 266, 268, 269
BL approach applied to forecasts
derived from, 289–291
volatility clustering, 26, 28, 112, 113
volatility modelling, 112–126
see also ARCH models; GARCH
models
volatility-weighted average correlation,
191, 200
volatility-weighted concentration ratio,
191, 200
wealth protection strategies, 299, 300
example application, 300–310
wealth trajectories
BL, prior, and equal-weighted
portfolios, 295
CDaR and GMV strategies, 249
low-β and lower tail dependence
strategies compared with
S&P500 benchmark, 209
TAA long-only and equal-weighted
long-only strategies, 309
Weibull distribution, 85, 86
reversed, 89
xts package, 284, 317, 319, 335–337
zoo package, 317, 319, 332–334
applications, 29, 32, 238, 325

Statistics in Practice
Human and Biological Sciences
Berger – Selection Bias and Covariate Imbalances in Randomized Clinical Trials
Berger and Wong – An Introduction to Optimal Designs for Social and Biomedical
Research
Brown and Prescott – Applied Mixed Models in Medicine, Second Edition
Carstensen – Comparing Clinical Measurement Methods
Chevret (Ed) – Statistical Methods for Dose-Finding Experiments
Ellenberg, Fleming and DeMets – Data Monitoring Committees in Clinical Trials: A
Practical Perspective
Hauschke, Steinijans & Pigeot – Bioequivalence Studies in Drug Development: Meth-
ods and Applications
K¨all´en – Understanding Biostatistics
Lawson, Browne and Vidal Rodeiro – Disease Mapping with WinBUGS and MLwiN
Lesaffre, Feine, Leroux & Declerck – Statistical and Methodological Aspects of Oral
Health Research
Lui – Statistical Estimation of Epidemiological Risk
Marubini and Valsecchi – Analysing Survival Data from Clinical Trials and Obser-
vation Studies
Millar – Maximum Likelihood Estimation and Inference: With Examples in R, SAS
and ADMB
Molenberghs and Kenward – Missing Data in Clinical Studies
O’Hagan, Buck, Daneshkhah, Eiser, Garthwaite, Jenkinson, Oakley & Rakow –
Uncertain Judgements: Eliciting Expert’s Probabilities
Parmigiani – Modeling in Medical Decision Making: A Bayesian Approach
Pintilie – Competing Risks: A Practical Perspective
Senn – Cross-over Trials in Clinical Research, Second Edition
Senn – Statistical Issues in Drug Development, Second Edition
Spiegelhalter, Abrams and Myles – Bayesian Approaches to Clinical Trials and
Health-Care Evaluation
Walters – Quality of Life Outcomes in Clinical Trials and Health-Care Evaluation
Welton, Sutton, Cooper and Ades – Evidence Synthesis for Decision Making in
Healthcare
Whitehead – Design and Analysis of Sequential Clinical Trials, Revised Second
Edition
Whitehead – Meta-Analysis of Controlled Clinical Trials
Willan and Briggs – Statistical Analysis of Cost Effectiveness Data
Winkel and Zhang – Statistical Development of Quality in Medicine
Earth and Environmental Sciences
Buck, Cavanagh and Litton – Bayesian Approach to Interpreting Archaeological Data
Chandler and Scott – Statistical Methods for Trend Detection and Analysis in the
Environmental Statistics

Glasbey and Horgan – Image Analysis in the Biological Sciences
Haas – Improving Natural Resource Management: Ecological and Political Models
Helsel – Nondetects and Data Analysis: Statistics for Censored Environmental Data
Illian, Penttinen, Stoyan, H and Stoyan D – Statistical Analysis and Modelling of
Spatial Point Patterns
McBride – Using Statistical Methods for Water Quality Management
Webster and Oliver – Geostatistics for Environmental Scientists, Second Edition
Wymer (Ed) – Statistical Framework for Recreational Water Quality Criteria and
Monitoring
Industry, Commerce and Finance
Aitken – Statistics and the Evaluation of Evidence for Forensic Scientists, Second
Edition
Balding – Weight-of-evidence for Forensic DNA Proﬁles
Brandimarte – Numerical Methods in Finance and Economics: A MATLAB-Based
Introduction, Second Edition
Brandimarte and Zotteri – Introduction to Distribution Logistics
Chan – Simulation Techniques in Financial Risk Management
Coleman, Greenﬁeld, Stewardson and Montgomery (Eds) – Statistical Practice in
Business and Industry
Frisen (Ed) – Financial Surveillance
Fung and Hu – Statistical DNA Forensics
Gusti Ngurah Agung – Time Series Data Analysis Using EViews
Kenett (Eds) – Operational Risk Management: A Practical Approach to Intelligent
Data Analysis
Kenett (Eds) – Modern Analysis of Customer Surveys: With Applications using R
Kruger and Xie – Statistical Monitoring of Complex Multivariate Processes:
With Applications in Industrial Process Control
Jank and Shmueli (Ed.) – Statistical Methods in e-Commerce Research
Lehtonen and Pahkinen – Practical Methods for Design and Analysis of Complex
Surveys, Second Edition
Ohser and M¨ucklich – Statistical Analysis of Microstructures in Materials Science
Pfaff – Financial Risk Modelling and Portfolio Optimization with R
Pourret, Naim & Marcot (Eds) – Bayesian Networks: A Practical Guide to Applica-
tions
Taroni, Aitken, Garbolino and Biedermann – Bayesian Networks and Probabilistic
Inference in Forensic Science
Taroni, Bozza, Biedermann, Garbolino and Aitken – Data Analysis in Forensic
Science


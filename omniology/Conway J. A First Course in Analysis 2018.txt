

A First Course in Analysis
This rigorous textbook is intended for a year-long analysis or advanced calculus course for
advanced undergraduate or beginning graduate students. Starting with detailed, slow-paced
proofs that allow students to acquire facility in reading and writing proofs, it clearly and
concisely explains the basics of diﬀerentiation and integration of functions of one and sev-
eral variables, and covers the theorems of Green, Gauss, and Stokes. Minimal prerequisites
are assumed, and relevant linear algebra topics are reviewed right before they are needed,
making the material accessible to students from diverse backgrounds. Abstract topics are
preceded by concrete examples to facilitate understanding – e.g. before introducing diﬀer-
ential forms, the text examines low-dimensional examples. The meaning and importance of
results are thoroughly discussed, and numerous exercises of varying diﬃculty give students
ample opportunity to test and improve their knowledge of this diﬃcult yet vital subject.
John B. Conway is Professor Emeritus of Mathematics at George Washington University.
He is the author of eleven books, including Mathematical Connections: A Capstone Course,
A Course in Functional Analysis, and the two-volume Functions of One Complex Variable.

CAMBRIDGE MATHEMATICAL TEXTBOOKS
Cambridge Mathematical Textbooks is a program of undergraduate and beginning grad-
uate level textbooks for core courses, new courses, and interdisciplinary courses in pure and
applied mathematics. These texts provide motivation with plenty of exercises of varying dif-
ﬁculty, interesting examples, modern applications, and unique approaches to the material.
Advisory Board
John B. Conway, George Washington University
Gregory F. Lawler, University of Chicago
John M. Lee, University of Washington
John Meier, Lafayette College
Lawrence C. Washington, University of Maryland, College Park
A complete list of books in the series can be found at www.cambridge.org/mathematics
Recent titles include the following:
Chance, Strategy, and Choice: An Introduction to the Mathematics of Games and
Elections, S. B. Smith
Set Theory: A First Course, D. W. Cunningham
Chaotic Dynamics: Fractals, Tilings, and Substitutions, G. R. Goodson
Introduction to Experimental Mathematics, S. Eilers & R. Johansen
A Second Course in Linear Algebra, S. R. Garcia & R. A. Horn
Exploring Mathematics: An Engaging Introduction to Proof, J. Meier & D. Smith
A First Course in Analysis, J. B. Conway

“This is an excellent text for a ﬁrst course in analysis in one and several variables for
students who know some linear algebra. The book starts with the real numbers, does
diﬀerentiation and integration ﬁrst in one variable, then in several, and ﬁnally covers
diﬀerential forms and Stokes’ theorem. The style is friendly and conversational, and
hews to the principal of going from the speciﬁc to the general, making it a pleasure
to read.”
– John McCarthy, Washington University in St. Louis
“Conway’s previous texts are all considered classics. “A First Course in Analysis”
is destined to be another. It is written in the same friendly, yet rigorous, style that
his readers know and love. Instructors seeking the breadth and depth of Rudin, but
in a less austere and more accessible form, have found their book.”
– Stephan Ramon Garcia, Pomona College
“This is a beautiful yet practical introduction to rigorous analysis at the senior under-
graduate level, written by a master expositor. Conway understands how students
learn, from the particular to the general, and this informs every aspect of his text.
Highly recommended.”
– Douglas Lind, University of Washington
“A First Course in Analysis charts a lively path through a perennially tough subject.
Conway writes as if he’s coaching his reader, leavening the technicalities with advice
on how to think about them, and with anecdotes about the subject’s heroes. His
enjoyment of the material shines through on page after page.”
– Bruce Solomon, Indiana University, Bloomington
“This year-long undergraduate book carefully covers real analysis “from sets to
Stokes” and is done in a friendly style by an experienced teacher and masterful
expositor. There are plenty of examples, exercises, and historical vignettes that both
give the student the opportunity to gain technical mastery of the material and to whet
their appetites for further study.”
– William T. Ross, University of Richmond
“A First Course in Analysis is a beautifully written and very accessible treatment
of a subject that every math major is required to learn. It will join Conway’s other
textbooks as a classic in Advanced Calculus. Those who teach and learn analysis
through Conway’s book will appreciate his cheerful and easy-to-understand style.”
– Wing Suet Li, Georgia Institute of Technology


A First Course in Analysis
John B. Conway
The George Washington University, Washington, DC, USA

University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
4843/24, 2nd Floor, Ansari Road, Daryaganj, Delhi - 110002, India
79 Anson Road, #06-04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of
education, learning and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781107173149
DOI: 10.1017/9781316779811
© John B. Conway 2018
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2018
Printed in the United States of America by Sheridan Books, Inc.
A catalog record for this publication is available from the British Library.
ISBN 978-1-107-17314-9 Hardback
Cambridge University Press has no responsibility for the persistence or accuracy
of URLs for external or third-party internet websites referred to in this publication,
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.

For Ann
As always


Contents
Preface
page xi
1
The Real Numbers
1
1.1
Sets and Functions
1
1.2
The Real Numbers
6
1.3
Convergence
16
1.4
Series
24
1.5
Countable and Uncountable Sets
29
1.6
Open Sets and Closed Sets
33
1.7
Continuous Functions
37
1.8
Trigonometric Functions
45
2
Differentiation
47
2.1
Limits
47
2.2
The Derivative
52
2.3
The Sign of the Derivative
57
2.4
Critical Points
62
2.5
Some Applications
65
3
Integration
71
3.1
The Riemann Integral
71
3.2
The Fundamental Theorem of Calculus
78
3.3
The Logarithm and Exponential Functions
81
3.4
Improper Integrals
85
3.5
Sets of Measure Zero and Integrability
89
3.6
The Riemann–Stieltjes Integral
93
4
Sequences of Functions
103
4.1
Uniform Convergence
103
4.2
Power Series
107
5
Metric and Euclidean Spaces
115
5.1
Definitions and Examples
115
5.2
Sequences and Completeness
119
5.3
Open and Closed Sets
121
5.4
Continuity
128
5.5
Compactness
133

x
Contents
5.6
Connectedness
140
5.7
The Space of Continuous Functions
146
6
Differentiation in Higher Dimensions
155
6.1
Vector-valued Functions
155
6.2
Differentiable Functions, Part 1
158
6.3
Orthogonality
168
6.4
Linear Transformations
173
6.5
Differentiable Functions, Part 2
188
6.6
Critical Points
192
6.7
Tangent Planes
197
6.8
Inverse Function Theorem
201
6.9
Implicit Function Theorem
207
6.10 Lagrange Multipliers
212
7
Integration in Higher Dimensions
219
7.1
Integration of Vector-valued Functions
219
7.2
The Riemann Integral
223
7.3
Iterated Integration
233
7.4
Change of Variables
239
7.5
Differentiation under the Integral Sign
250
8
Curves and Surfaces
257
8.1
Curves
257
8.2
Green’s Theorem
263
8.3
Surfaces
269
8.4
Integration on Surfaces
275
8.5
The Theorems of Gauss and Stokes
284
9
Differential Forms
293
9.1
Introduction
293
9.2
Change of Variables for Forms
304
9.3
Simplexes and Chains
309
9.4
Oriented Boundaries
317
9.5
Stokes’s Theorem
320
9.6
Closed and Exact Forms
323
9.7
Denouement
327
Bibliography
331
Index of Terms
333
Index of Symbols
339

Preface
This book is intended for the year-long course on analysis taught to undergraduates
near the end of their education and graduate students at the beginning of theirs.
It is recommended for students in the physical sciences, economics, and statistics.
Sometimes this course is titled Advanced Calculus and sometimes just Analysis. The
subject matter comprises the basics of diﬀerentiation and integration of functions
of one and several variables, topped oﬀwith some form of the theorems of Green,
Gauss, and Stokes.
How is the material presented in this book? A guiding principle I have followed
for a long time when I teach or when I write a textbook, as opposed to a mono-
graph, is to go from the particular to the general. This practice came about from
a combination of what I observed in the classroom and the historical way mathe-
matics developed. The present book contains many instances of this approach, but
a dramatic illustration is what happens in the last two chapters on surfaces and the
Green, Gauss, and Stokes Theorems. I begin (Chapter 8) with an exposition of what
happens in R2 and R3 including proofs of these just mentioned theorems in this
setting. Unlike the rest of this book, however, there are places here where I relax
the rigor. This is especially so when I discuss the orientation of surfaces in R3. In
the following chapter (Chapter 9) I introduce diﬀerential forms on Rp, constantly
illustrating everything with reference to what was seen in the lower dimensional
spaces. Here rigor is enforced. After we establish the Generalized Stokes Theorem,
we go back and prove the particular cases of the three big theorems in R2 and R3 as
a consequence. I think this is a better approach than going directly to a treatment
of diﬀerential forms and surfaces in Rp. My experience is that most students at this
level are not ready for such a direct route without some preparation.
Another philosophy of mine in writing is not to introduce a concept until it’s
needed; I want the reader to quickly see the concept in action. For example I wanted
to give a rather detailed diagnosis of the nature of a critical point for a function from
Rp into R. For me this entails introducing symmetric matrices and the Spectral
Theorem. This could have been done in a special chapter on linear algebra. Instead
I waited until we were ready to discuss critical points. In the ﬁrst course in Linear
Algebra the usual practice, due to time constraints, is never to talk about the Spectral
Theorem. I therefore prove it in Rp.
Speaking of linear algebra, that’s a major diﬃculty for anyone teaching a version
of this course. It was certainly a problem when I was writing this book. Linear alge-
bra is a stated prerequisite, but I know most of the students whom I’ve taught over

xii
Preface
the years have forgotten some of the linear algebra they learned. Nevertheless when
we teach a course on multivariable analysis we cannot reteach linear algebra. The
path I take is to recall some of the high points, usually without proofs. On the other
hand I need and use more on determinants than I think students know or have even
seen. As a consequence I deﬁne determinants (6.4.18) and derive their properties,
though the proofs tend more toward the sketchy than elsewhere in this book.
Another of my beliefs about teaching and writing a text is that just because I know
some topic that extends the basic material doesn’t mean I should teach it in a class
or include it in the text. Besides, I want to keep the book lean. I also want to have
conﬁdence that anyone using this book could make it to the multivariable versions
of the Fundamental Theorem of Calculus. I suspect many instructors will regret the
absence of some topic they like. If this is your case, I’m sorry; but I think you’ll
ﬁnd other interesting things here.
Universities with graduate programs frequently have graduate students enrolled
in this course. When I teach a mixed class I focus on the undergraduates; this is what
I did while writing this book. This has several consequences. I assume a minimum
background: three semesters of calculus and a semester of linear algebra. I have to
also assume that the students have some level of comfort reading and writing proofs.
I certainly am aware that someone with my stated minimum background may not
be comfortable with proofs. As a partial bridge, the proofs that are in the ﬁrst half
of the book are more detailed.
Finally, three additional statements about this book. When I write a text I always
imagine myself presenting the material in front of a classroom. Some have com-
mented on my “chatty” style and this is the source. Sections marked with an asterisk
(*) are optional and are not needed for the rest of the book. You might also observe
that to some extent computational exercises are sparse (but not absent). It seems to
me that students have done a lot of such things in the three semesters of calculus
that is assumed of the readers of this text. If they survived that, I didn’t see a reason
to put more of the same here.
Synopsis of the Chapters
The book begins with a chapter on the real numbers and some other basic material.
It may be that some students could skip part of this, but I suspect it will be rare that
there is a student in a class using this book who could skip the entire chapter. The
starting point is a development of the real numbers assuming that you understand
the rationals. I give two approaches to this: an abbreviated one and a more thorough
one. (This thorough one still has some gaps as this is not the focus of this book.) The
chapter also includes material on sequences, open and closed sets in R, continuity,
as well as countable and uncountable sets. The point of this chapter is to give all
readers a chance to ﬁll in gaps and expose them to proofs. (The proofs here are
straightforward and presented in more detail than elsewhere in the book.)
The next three chapters contain the core of the basic material in one dimension.
Chapter 2 is on diﬀerentiation in one variable and concludes with l’Hôpital’s Rule

Preface
xiii
and Taylor’s Theorem. Chapter 3 gives the theory of the Riemann integral over
subsets of R. It also contains starred sections on Lebesgue’s Theorem giving a nec-
essary and suﬃcient condition for integrability and the Riemann–Stieltjes integral.
Chapter 4 is entitled “Sequences of Functions.” It covers the standard material on
uniform convergence as well as power series.
Chapter 5 is entitled “Metric and Euclidean spaces.” I am aware that many books
at this level avoid metric spaces, but I decided not to. Less you think this violates my
principle of going from the particular to the general, don’t forget that almost every-
thing here has ﬁrst appeared as a result for R in Chapter 1. When this is combined
with the fact that the exposition is replete with examples in Rp that illustrate the
results, I feel comfortable that my principle is intact. It also seems to me that proofs
in the abstract spaces are easier. In fact if you state a result in Euclidean space and
try to fashion a proof of it, you are strongly tempted to get involved with coordinates
and such while the proof in metric spaces is clean and shows what is really going on.
The chapter ends with a section on spaces of continuous functions. Included here
are the Stone–Weierstrass Theorem and the Arzela–Ascoli Theorem, two results I
consider basic but are frequently omitted at this level. (In fact the Stone–Weierstrass
Theorem is used to prove Fubini’s Theorem in Chapter 7.)
Chapter 6, “Diﬀerentiation in Higher Dimensions,” covers the standard topics in
this subject. An introduction and recollection of most of the linear algebra needed
for the rest of the book is presented and used here. The treatment of critical points
may diﬀer from most books. Here the observation that the second derivative of a
C(2)-function from Rp into R is a hermitian linear transformation is central. This
permits the application of the Spectral Theorem and allows us to carefully analyze
the behavior of the function at a critical point, even when it’s a saddle point.
Chapter 7 is titled “Integration in Higher Dimensions” and covers the Riemann
integral in an abbreviated way. It seems to me that some books spend too much time
on this integral, time that I think can be better spent on other things. The treatment
of Riemann integration given in Chapter 7 achieves simplicity and rigor by only
integrating continuous functions. Students who continue their study of mathematics
will see the Lebesgue integral, which is not only more general but a lot easier to
understand than an in-depth treatment of the Riemann integral in Rp. For example,
the issue of integrability is largely dormant in the Lebesgue case but complicated in
the Riemann case. Students who don’t continue in mathematics are not hurt, since
when they encounter integration in higher dimensions in their future life it is almost
always an iterated integral and what we do here is ample.
Chapter 8, “Curves and Surfaces,” focuses on these objects in R2 and R3. Chap-
ter 9, “Diﬀerential Forms,” extends this to Rp. I’ve already discussed these chapters
and the approach taken.
References and Sources
As I indicate at various places in the body of this book, I have used [7], [11], and
[15] while I was writing this book. I have certainly used the last two as sources

xiv
Preface
of exercises. Also I’ve made use of the Internet more heavily than with any of my
previous books. Sometimes I used articles from the Web that, as far as I know, are
not available in print. When I did this I usually corresponded with the author if (s)he
was identiﬁable. In the Bibliography I list this reference with the site as I found it
during one of my ﬁnal readings of this book. I also found a lot of my exercises online.
Sometimes these were found on the Web where no author was designated; in such
a case I did not reference the site.
Biographical notes
As in my last two books I have added a short biographical note for any mathemati-
cian whose work is quoted. There is no scholarship on my part in this, as all the
material is from secondary sources, principally what I could ﬁnd on the Web. In
particular, I made heavy use of the St Andrews University site www-history.mcs
.st-andrews.ac.uk/history/BiogIndex.html and some use of Wikipedia.
I emphasize the personal aspects of the mathematicians we encounter along the
way, rather than recite their achievements. This is especially so when I discover
something unusual or endearing in their lives. I ﬁgure many students will see their
achievements if they stick with the subject and most, at this stage of their education,
won’t know enough mathematics to fully appreciate the accomplishments. In addi-
tion I think the students will enjoy learning that these famous people were human
beings.
For Students
From a simplistic point of view, this course repeats calculus. That’s grossly mis-
leading. True, we’ll talk about diﬀerentiation and integration in one and several
variables and I am assuming that all students have completed the standard three-
semester sequence in calculus. On the other hand everything done here will be pre-
sented with complete mathematical rigor. The emphasis is not on computing but on
understanding the concepts; part of that understanding is proving the results as well
as working out examples and exercises. In addition, we’ll see new material in the
second semester when we discuss integration of functions deﬁned on surfaces.
I’ve long thought this course is the most diﬃcult in the undergraduate curriculum.
It’s also one of the most important for your future in mathematics, whether you go
on to graduate school or begin a career just after graduation. So work hard at this.
My advice to all students is to read the book with paper and a writing implement
of your choice. Draw pictures, if this is needed. For goodness sake, read the material
between the proofs; these paragraphs contain a lot that will help your understand-
ing. I leave a lot of detail checking to the reader and frequently insert such things
as (Why?) or (Verify!) in the text. I want you to delve into the details and answer
these questions. It will check your understanding and give some perspective on the
proof.

Preface
xv
I am also convinced there are some details that should not appear in books and
that professors should not present in the classroom. Good examples of such material
are detailed calculations and long proofs that contain a jungle of subscripts. You
just can’t absorb such things by watching and listening. One place where I leave a
lot of details to the reader is the deﬁnition and properties of the determinant of a
square matrix. This starts at (6.4.18). You aren’t going to understand this course by
watching someone else do the work. You need to go to your study desk and push on
ahead. As I have often said, learning mathematics is not a spectator sport.
I also strongly advise you to at least read all the exercises. With your schedule
and taking other courses, you might not have the time to try to solve them all, but
at least read them. They contain additional information.
Thanks
First I thank all the students who have sat in my classes over the years. I think you
learned something from me, but I also learned something from you. I’ve had conver-
sations during my career with several professional mathematicians on subjects that
appear here. These are too numerous to remember let alone list, but I thank you all. I
speciﬁcally want to thank Waclaw Szymanski who showed me a nice improvement
of the proof of the Stone–Weierstrass Theorem. Undoubtedly my greatest thanks
go to Professor Mark Hunacek who read many of the chapters, gave me several
exercises, and provided valuable feedback.
Thanks for your attention.
Live long and prosper


1
The Real Numbers
In this chapter we’ll develop the properties of the real numbers, the foundation of
this book. But ﬁrst we start with an introduction to set theory.
1.1. Sets and Functions
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
The concept of a set is the basis for all of mathematics. The reader is likely to have
come across many of the notions associated with set theory, and if this experience
has been extensive, please skip to the next section.
A set X is a collection of things called elements or points. We’ll treat the idea of a
set and an element of a set as a fundamental notion and not try to give a more formal
deﬁnition. The notation x ∈X is used to denote the fact that x is an element that is
contained in the set X. You might also sometimes see x, y ∈X; this means that x
and y are both elements of X. (Note that the statement x, y ∈X does not imply that
these two points are distinct.) When x fails to be an element of the set X, we write
x /∈X.
A basic concept of set theory is that of a subset. We say that A is a subset of X
if A is a set and every element of A is also an element of X. Using symbols we can
write this relation by saying that if x ∈A, then x ∈X. We introduce the notation
A ⊆X to denote this; this is read as “A is contained in X,” or “A is a subset of X.”
We could also write X ⊇A, read as “X contains A.” Another notation for A ⊆X
that the reader will encounter in the literature is A ⊂X, but we won’t use this in this
book. Note that if A = X it is also the case that A ⊆X. We say that A is a proper
subset of X provided A ⊆X but A ̸= X.
There are many ways to deﬁne sets and subsets, depending on the situation. For
example, consider the set of natural numbers; that is, the set of positive integers
denoted by N. To deﬁne this set we could also write N = {1, 2, 3, . . .}. If we want
to denote the subset of N consisting of even integers, we can write {2n : n ∈N}. The
odd integers can be expressed by {n −1 : n ∈N and n is even}, or we could write
{2n −1 : n ∈N}. We also have the set Z of all integers, positive and negative as
well as zero. So
Z = {n −m : n, m ∈N}
Hence N ⊆Z. (We are assuming that the reader is familar with the natural numbers,
N, the integers, Z, the rational numbers, Q, and with their properties. In the next
section we’ll introduce the real numbers, R.)

2
The Real Numbers
If we are given a set X and A and B are both subsets of X, then in a similar way
as above we say that A is a subset of B if every element of A is also an element of B;
that is, if x ∈A, then x ∈B. In symbols we write this as A ⊆B or B ⊇A. We can
also say that A is contained in B or B contains A. The two sets A and B are equal if
we have both A ⊆B and B ⊆A.
There are two special subsets of any set X: the set X itself and the empty set ∅
consisting of no elements. The empty set may take a moment to digest, but it plays
an important role in mathematics. A distinction of ∅is that it is a subset of every
subset of X, the only such subset of X. If A ⊆X and x ∈X, there is also the notation
x /∈A to indicate that the point x does not belong to A. So if x ∈X, then x /∈∅. There
is another special subset – actually a collection of them. If x ∈X, then {x} denotes
the set consisting of the single element x. This is called a singleton set. So x ∈{x}
and {x} ⊆X.
We want to deﬁne some operations between subsets. So we consider X as the
universe and examine its various subsets A, B,C, . . .
1.1.1. Definition. If A, B ⊆X, the intersection of A and B is the set A ∩B deﬁned
by
A ∩B = {x ∈X : x ∈A and x ∈B}
1.1.2. Example. (a) If A ⊆B, then A ∩B = A.
(b) Consider the set Q of all rational numbers, positive, negative, and 0. That is
Q =
 n
m : n, m ∈Z and m ̸= 0

If A = {x ∈Q : −7 ≤x ≤4}, then A ∩N = {1, 2, 3, 4}.
(c) If A = {x ∈Q : x ≤0} and B = N, then A ∩B = ∅.
(d) Note that if A ⊆B, then A ∩B = A.
If A ∩B = ∅, we say that the sets A and B are disjoint.
1.1.3. Definition. If A, B ⊆X, then the union of A and B is the set A ∪B deﬁned
by
A ∪B = {x ∈X : x ∈A or x ∈B or both}
It’s worth emphasizing that the use of the word “or” in the preceding deﬁnition
is not the exclusive “or”. In other words, when we say in the deﬁnition that x ∈A or
x ∈B we do not exclude the possibility that x belongs to both A and B. That is, we
do not insist that A ∩B = ∅.
1.1.4. Example. (a) If A ⊆B, then A ∪B = B.
(b) A ∩B ⊆A ∪B.
(c) For any subset A of X, A ∪∅= A.

1.1 Sets and Functions
3
(d) If A = {x ∈Q : −7 < x ≤1} and B = {x ∈Q : −1 < x < 5}, then A ∪B =
{x ∈Q : −7 < x < 5}.
(e) If A = {x ∈Q : −7 < x ≤1} and B = {x ∈Q : 2 ≤x ≤5}, then A ∪B =
{x ∈Q : either −7 < x ≤1 or 2 ≤x ≤5}.
1.1.5. Proposition. The distributive laws hold for union and intersection. That is:
(a) if A, B,C ⊆X, then A ∩(B ∪C) = (A ∩B) ∪(A ∩C); and
(b) if A, B,C ⊆X, then A ∪(B ∩C) = (A ∪B) ∩(A ∪C).
Proof. This proof is the prototype for establishing that two sets are equal: we take
an element of the left-hand side and show it’s an element of the right-hand side;
then take an element of the right-hand side and show it’s an element of the left-hand
side.
(a) If x ∈A ∩(B ∪C), then x ∈A and x ∈B ∪C. Thus x ∈B and x ∈C. But this
says that either “x ∈A and x ∈B” or “x ∈A and x ∈C.” Therefore x ∈(A ∩B) ∪
(A ∩C). Conversely, assume that x ∈(A ∩B) ∪(A ∩C). So either x ∈(A ∩B) or
x ∈(A ∩C). In the ﬁrst case, x ∈A and x ∈B; in the second case, x ∈A and x ∈
C. Thus in either case, x ∈A; also, depending on the case, either x ∈B or x ∈C.
Therefore x ∈A ∩(B ∪C).
(b) The proof of this part has a slightly diﬀerent ﬂavor than the proof of the
ﬁrst part. If x ∈(A ∪B) ∩(A ∪C), then x ∈A ∪B and x ∈A ∪C. So either x ∈A
or x ∈B. If x ∈A then we have that x ∈A ∪(B ∩C). If x /∈A, then the fact that
x ∈A ∪B and x ∈A ∪C implies that x ∈B and x ∈C; hence x ∈B ∩C. Therefore
x ∈A ∪(B ∩C). The proof of the other half of (b) is Exercise 2.
■
We also deﬁne the diﬀerence of the two sets as
A\B = {x ∈X : x ∈A but x /∈B}
Some mathematicians use the notation A −B instead of A\B. I prefer the notation
A\B because in some situations A −B is ambiguous. For example, if A and B are
subsets of Q, we will use the deﬁnition A −B = {a −b : a ∈A, b ∈B}. The same
applies when A and B are subsets of a vector space. So throughout this book the
diﬀerence of two sets will be denoted using the backslash.
1.1.6. Example. (a) If X is any set and A ⊆X, then A\∅= A and A\X = ∅.
(b) If A = {x ∈Q : 0 < x < 1} and B = {x ∈Q : 1
2 < x ≤3}, then A\B = {x ∈
Q : 0 < x ≤1
2}.
(c) If A = {x ∈Q : 0 < x < 1} and B = {x ∈Q : 3 ≤x < ∞}, then A\B = A.
For any subset A of X, the diﬀerence X\A is called the complement of A. Else-
where the reader might encounter the notation Ac or A to denote the complement of
A. Note that X\(X\A) = A (Exercise 4).

4
The Real Numbers
1.1.7. Proposition (De Morgan’s1 Laws). If X is any set and A and B are subsets
of X, then:
(a) X\(A ∪B) = (X\A) ∩(X\B);
(b) X\(A ∩B) = (X\A) ∪(X\B).
Proof. We prove (a) and leave the proof of (b) as Exercise 5. Again we use
the standard approach to proving that two sets are equal. If x ∈X\(A ∪B), then
x /∈A ∪B. The only way this can happen is that both of the following two state-
ments are true: x /∈A and x /∈B. That is, x ∈X\A and x ∈X\B; equivalently,
x ∈(X\A) ∩(X\B).
Now assume that x ∈(X\A) ∩(X\B). This says that x ∈X\A and x ∈X\B; that
is, x /∈A and x /∈B. But combining these two statements means x /∈A ∪B, or that
x ∈X\(A ∪B).
■
We now extend the concepts of intersection and union to multiple sets. Indeed,
we’ll extend these to inﬁnite collections of sets. Namely, assume that A1, A2, . . . are
subsets of X and deﬁne
∞

n=1
An = {x ∈X : x ∈An for all n ≥1}
∞

n=1
An = {x ∈X : x ∈An for some n ≥1}
There is a version of De Morgan’s Laws for this as well.
1.1.8. Theorem (De Morgan’s Laws). If X is a set and {An : n ≥1} is a collection
of subsets, then:
(a) X\[∞
n=1 An] = ∞
n=1(X\An);
(b) X\[∞
n=1 An] = ∞
n=1(X\An).
The proof of this last theorem is Exercise 6 and proceeds like the proof of Propo-
sition 1.1.7.
We conclude this section with a discussion of functions. If X and Y are two sets,
then a function from X intoY is a rule, denoted by f : X →Y, that assigns to each x
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
1 Augustus De Morgan was born in 1806 at Madura, India (now Madurai). His father was an oﬃcer in the British
Army stationed there. The young Augustus lost his sight in one eye shortly after birth and the family returned
to England when he was seven months old. He entered Trinity College, Cambridge in 1823. He received his BA
but refused to take a theology exam, which was required for an advanced degree. He returned to London in 1826
to study for the Bar, but instead he became the ﬁrst professor of mathematics at the newly established University
College London in spite of the fact that he had never published in the subject, a fact he soon remedied. On a
matter of principle he resigned in 1831. He was once again appointed in 1836, but resigned again in 1866. In
1838 he introduced the term “mathematical induction.” The process had been in use before, but without clarity;
De Morgan managed to give it a rigorous basis. Through his life he was a proliﬁc author with hundreds of articles
and many books. He introduced the laws of the present proposition and was a great reformer of mathematical
logic. In addition he founded the London Mathematical Society and became its ﬁrst president. He was quite
dogmatic, as his two resignations might indicate. He never voted and never visited the House of Commons, the
Tower of London, or Westminster Abbey. He died in 1871 in London.

1.1 Sets and Functions
5
in X a unique point y in Y. Synonyms for function are the terms map and mapping.
The set X is called the domain of f and the set Y is called the range of f . The set
f (X ) = { f (x) ∈Y : x ∈X} is called the image of f . Note the distinction between
range and image. Now that you have noted the distinction you should be aware that
some mathematicians deﬁne the range of a function to be what we have called the
image and vice versa. When they do this they sometimes use the term codomain for
what we call the range. Confused? Don’t worry too much about it except when you
consult other sources; we will consistently use the terms as we deﬁned them above.
Frankly, the distinction will aﬀect very little that is said.
1.1.9. Example. (a) f : Q →Q deﬁned by f (x) = x2 is a function. Its domain
is Q and its image is the set of rational numbers in [0, ∞). What is its range? You
could say it’s the same as its image or you could say it’s Q. Perhaps this vagueness is
unappealing, but there is no “correct” answer. What you call its range might depend
on your purpose or the context of the discussion.
(b) If for each x in Q we let f (x) = +1 when x ≥0 and f (x) = −1 when x ≤0,
then this is not a function since the value of f (0) is not uniquely deﬁned. If we were
to redeﬁne f by stating that f (x) = −1 when x < 0, then it is a function.
(c) If X and Y are sets, y0 ∈Y, and f (x) = y0 for every x in X, then f : X →Y
is a function – called a constant function.
(d) If X is any set and A ⊆X, deﬁne χA : X →R by
χA(x) =
	
1
if x ∈A
0
if x /∈A
This function is called the characteristic function of A. Some call this the indicator
function. Observe that for all x in X, χ∅(x) = 0 and χX (x) = 1.
If f : X →Y and g : Y →Z, then the composition of f and g is the function
g ◦f : X →Z deﬁned by
g ◦f (x) = g( f (x))
for all x in X. So, for example, if f (x) = x2 and g(x) = x3, then g ◦f (x) = (x2)3 =
x6. Similarly f ◦g(x) = x6, so that in this case g ◦f = f ◦g. If again f (x) = x2 and
g(x) = sin x, then g ◦f (x) = sin(x2) while f ◦g(x) = (sin x)2. So it is not true that
composition is commutative.
1.1.10. Definition. A function f : X →Y is called surjective if for each y in Y
there is at least one x in X such that f (x) = y. f is injective if for x1, x2 in X, the
relation f (x1) = f (x2) implies that x1 = x2. f is bijective if it is both injective and
surjective.
In the literature the reader will often see the term “onto” instead of surjective and
one-to-one instead of injective. I have no problem with the term one-to-one; I often
use it and the reader might see it in this book. I have a language problem, however,

6
The Real Numbers
with using onto as an adjective when it is a preposition. While I might use it that
way in a casual conversation, when I am being a bit more formal I won’t.
We’ll encounter the terms surjective, injective, and bijective frequently as we
progress.
1.1.11. Example. (a) The function f : Q →Q deﬁned by f (x) = x + 1 is bijec-
tive.
(b) The function f : N →N deﬁned by f (x) = x2 is injective but not bijective.
(c) The function f : Z\{0} →{n2 : n ∈N} deﬁned by f (x) = x2 is surjective but
not injective.
Exercises
In these exercises X is a given set and A, B,C, . . . are subsets of X.
(1)
Let A = {2n : n ∈N}, B = {2n + 1 : n ∈N}, C = {3n : n ∈N}, D = {3n + 1 :
n ∈N}, E = {3n + 2 : n ∈N}. Determine each of the following sets: (a) A ∩B;
(b) A ∩C; (c) A ∪D; (d) D ∪E; (e) (A ∩E) ∪(B ∩E).
(2)
Complete the proof of Proposition 1.1.5(b).
(3)
Prove that the associative laws apply to unions and intersections: A ∩(B ∩C) =
(A ∩B) ∩C and A ∪(B ∪C) = (A ∪B) ∪C.
(4)
Give a detailed proof that X\(X\A) = A.
(5)
Prove part (b) of Proposition 1.1.7.
(6)
Prove Theorem 1.1.8.
(7)
If f : X →Y is a function, prove that the following statements are equivalent. (a) f
is injective. (b) f (A\B) = f (A)\ f (B) for all subsets A and B of X. (c) f (a ∩B) =
f (A) ∩f (B) for all subsets A and B of X.
1.2. The Real Numbers
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
I am aware that the reader has been working with the real numbers R for the entirety
of his/her mathematical life. However I suspect that many students studying the
present material for the ﬁrst time may be unaware of some of the properties of
R that are crucial for making calculus work. Indeed one of these properties, The
Completeness Property, is not shared by Q and makes it impossible for calculus to
survive as a theory only involving rational numbers.
For the teacher there are two ways to handle this. One is to just state the needed
properties of R and proceed to develop calculus. Another is to start with the prop-
erties of Q and carefully develop the deﬁnition of the real numbers and derive the
needed properties. This second approach gives a solid understanding of the mate-
rial and gives a grounding in writing proofs in analysis. The disadvantage is that
going through this material takes time that an instructor may not have. The ﬁrst
approach has the advantage of quickly getting to the more advanced topics. If I were

1.2 The Real Numbers
7
the instructor, I can easily imagine diﬀerent circumstances under which I would be
led to use either of these approaches.
So which approach will be adopted in this book? In some sense both. We will
begin with a quick survey of the ﬁrst approach and then go through a more thor-
ough grounding. Why? I assure you that doing it this way is not chosen because of
intellectual indecision or cowardice. As I said before, there are reasons that sup-
port taking either route. I’m going to leave it to individual instructors and readers
to ﬁgure out if they have the time to do the second or are comfortable only taking
the ﬁrst.
Quick Approach
In this approach we assume the reader is familiar with the arithmetic properties of
the set of real numbers, R, as well as its usual order relations, < and ≤. We also
assume the reader knows the distinction between rational numbers and irrational
numbers. The ﬁrst important fact about R that the reader may not be fully conscious
of is the following.
1.2.1. Axiom (Density Property). If a and b are rational numbers and a < b, then
there is an irrational number x with a < x < b. Similarly, if a, b are irrational num-
bers and a < b, then there is a rational number x with a < x < b.
The Density Property will be used frequently as we develop the theory of diﬀeren-
tiation and integration. The other important property we need involves the ordering
on R.
If E ⊆R we say that E is bounded above if there is a number a such that x ≤a for
all x in E. Such a number a is called an upper bound of E. Similarly E is bounded
below if there is a number b with b ≤x for all x in E; b is called a lower bound of
E. It is easy to see that E is bounded above with an upper bound a if and only if the
set −E = {−x : x ∈E} is bounded below with a lower bound −a. For this reason
any statement about the upper bound of a set has its analogue for the lower bound
and we will frequently only do the upper version. A set E is bounded if it is both
bounded above and bounded below.
1.2.2. Definition. If E ⊆R and E is bounded above, then a least upper bound or
supremum of E is a number α that satisﬁes: (i) α is an upper bound for E; (ii) α ≤a
for any other upper bound a for E. Similarly, if E is bounded below, then the greatest
lower bound or inﬁmum of E is a number β that satisﬁes: (i) β is a lower bound for
E; (ii) b ≤β for any other lower bound b for E. In symbols we write α = sup E and
β = inf E. (The reader may have seen the notation α = lub E and β = glb E, but
we will use the sup and inf notation.)
1.2.3. Axiom (Completeness Property). If a non-empty subset E of R has an upper
bound, it has a supremum. If a non-empty subset E of R has a lower bound, it has
an inﬁmum.

8
The Real Numbers
We also have uniqueness for the supremum.
1.2.4. Proposition. If the subset E of R is bounded above, its supremum is unique.
That is, if α and α′ are both the supremum of E, then α = α′. If E is bounded below,
its inﬁmum is unique.
Proof. Let α and α′ be as in the statement of the proposition. Since α′ is an upper
bound for E and α is a supremum, the deﬁnition of a least upper bound implies that
α ≤α′. Similarly, since α′ is also a supremum, we have that α′ ≤α. Thus α = α′.
That the inﬁmum is unique when it exists can be proved in a manner analogous
to the preceding proof or you can use Exercise 2.
■
Also see Exercise 3.
The density and completeness properties may seem obvious to you, but that is
probably because you have always thought of R as having them. Nevertheless, unless
you are in possession of an exact deﬁnition of the real numbers, as will be carried out
shortly, you cannot give a rigorous proof of their existence. Let’s also remark that the
set Q does not have the Completeness Property. For example {a ∈Q : a2 < 2} does
not have a supremum within the set Q. (See Proposition 1.2.9 below.) Of course it
has a supremum in R, namely
√
2, but this is not a rational number as is established
in the proof of Proposition 1.2.9.
More Thorough Approach
Here we will deﬁne the real numbers. From the student’s point of view this may
seem unnecessary. After all, you have been working with real numbers since high
school, solving equations and doing calculus. Most of the time calculus is presented
in what might be called a “naive” way: the presentation glosses over some intrinsic
properties of the real numbers that make calculus work. Here we want to present
the theory in a precise mathematical way. To do this we need a precise deﬁnition of
the real numbers.
Caveat. I have entitled this subsection with the words “More Thorough” rather than
just “Thorough.” I am not going to present every detail of a thorough approach.
To prove every detail and totally explore the deﬁnition of the real numbers would
make it impossible to complete our study of functions of a single variable in one
semester. I will present more than enough of the material to establish the density and
completeness properties. But some topics will not be encountered. In addition the
proofs of many facts will be left as exercises. If the reader is interested in seeing a
complete development of the real numbers, the books [6] and [13] will provide them.
How do we deﬁne the set of real numbers? There has to be a starting point. In
some treatments the beginning is a development of the properties of the natural
numbers. In others it is the properties of the rational numbers. We are going to start
somewhere in between these two. We will deﬁnitely assume you know the natural
numbers. There are, however, some properties of N that I think you will readily

1.2 The Real Numbers
9
accept but may not have seen explicitly stated. Here is one that is a ﬁrst cousin of
the fact that there is no largest integer. No proof is given.
1.2.5. Lemma. If m, n ∈N, then there is a natural number N such that Nn > m
The deﬁnition of Q has already been given in (1.1.2). It’s an algebraically deﬁned
entity and we are certainly going to assume the reader is knowledgeable of all its
algebraic properties. There are some non-algebraic properties of Q that we need
and that some readers may not have been exposed to. These will involve the order
structure of Q. Here is one that is the version of the preceding lemma for Q.
1.2.6. Proposition. If x and ϵ are positive rational numbers, then there is an n in
N with nϵ > x.
Proof. Put x = a/b and ϵ = c/d with a, b, c, d in N. If n ∈N, then
ϵ −x
n = c
d −a
nb = nbc −ad
nbd
By the preceding lemma we can choose n such that nbc > ad. For that value of n,
nϵ > x.
■
In the Quick Approach above we deﬁned the concept of a set of real numbers
that is bounded above or below. The same concept applies to subsets of Q, which
are, of course, subsets of R. But here we want to underline that we must choose the
upper bound to be a rational number. This is not so important until we also discuss
the concept of the supremum and inﬁmum of subsets of Q. Here for a bounded
subset of Q to have a supremum it must be rational. There are subsets of Q that
are bounded above but do not have a supremum in Q. To do this we present two
lemmas.
1.2.7. Lemma. There is no rational number x with x2 = 2.
Proof. In fact if there were such a rational number, we could write it as x = n/m,
where n, m ∈Z and n and m have no common divisor other than ±1. If 2 = (n/m)2,
then we get n2 = 2m2; so n2 is even. This implies that n is even; in fact if it weren’t,
we would have that it’s odd. That is, we would have that n = 2k + 1. But then
n2 = (2k + 1)2 = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1, which is odd. This is in direct
contradiction to the fact that n2 is even. But if n is even we can write n = 2p, so
that 4p2 = n2 = 2m2. Dividing by 2 we get that m2 is even; as before we get that
m is even. That is, we have shown that 2 is a common divisor of both n and m,
contradicting the fact that they were chosen with ±1 as the only common divisor. ■
1.2.8. Lemma. If a, b are positive rational numbers with a2 < b2, then a < b.
Proof. By the hypothesis we have that 0 < b2 −a2 = (b −a)(b + a). But since a
and b are positive, a + b > 0. Thus b −a > 0.
■
1.2.9. Proposition. The set A = {a ∈Q : a2 < 2} is bounded above but has no
supremum in Q.

10
The Real Numbers
Proof. It is immediate that 2 is an upper bound for A so it remains to show that A has
no supremum in Q. The proof is by contradiction; so assume there is an x in Q with
x = sup A. We will show that x2 = 2, thus contradicting Lemma 1.2.7 and ﬁnishing
the proof. We show that x2 = 2 by showing that x2 can be neither larger nor smaller
than 2. First assume that x2 > 2. We will show that there must be a number w in
Q with 0 < w < x and 2 < w2. In fact suppose such a w exists. It then follows that
if a ∈A, then a2 < 2 < w2. But if a < 0, then a < w; if a > 0, then Lemma 1.2.8
shows a < w. This establishes that w is an upper bound of the set A. Since w < x,
we have our desired contradiction.
In n ∈N, then

x −1
n
2
= x2 −2x
n + 1
n2 > 2 −
2x
n −1
n2

Using (1.2.6) we can choose n in N such that 2x > 1
n. Thus 2x/n −1/n2 > 0 and
the above inequality shows that w = x −1
n works.
Now assume that x2 < 2. Again let n ∈N and examine x + 1
n. We have that
2 −

x + 1
n
2
= [2 −x2] −
2x
n + 1
n2

As we did before, choose n suﬃciently large that w = x + 1
n satisﬁes 2 −w2 > 0.
This says that w ∈A. But w > x, contradicting the fact that x is an upper bound of
A. So again we arrive at a contradiction.
■
The last proposition inspires the deﬁnition of the real numbers.
1.2.10. Definition. A Dedekind 2 cut, or simply a cut, is a non-empty subset A of
Q satisfying the following three conditions: (a) A is a proper subset of Q; (b) if
a ∈A and b < a, then b ∈A; (c) if a ∈A, there is a b in A with a < b.
An example of a cut can be obtained from Proposition 1.2.9 except we have to
adjoin to the set A in that proposition all the negative numbers. That is, an example
of a Dedekind cut is
1.2.11
A = {a ∈Q : a2 < 2 or a < 0}
This can be shown to be a cut by using the techniques of the preceding proof (Exer-
cise 10). Note that if q ∈Q, then the set B = {x ∈Q : x ≤q} is not a cut (Why?)
while C = {x ∈Q : x < q} is. (See (1.2.13).)
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
2 Richard Dedekind was born in 1831 in Braunschweig, Germany also referred to as Brunswick. His father was a
professor at the Collegium Carolinum in Brunswick, an institution between a high school and a university. He
received his doctorate in 1852 from the University of Göttingen and was Gauss’s last student. In 1854 he began
to teach at Göttingen and was joined by Riemann and then Dirichlet, who inﬂuenced him greatly. In 1858 he
accepted a position in Zurich and it was during his ﬁrst year there while teaching calculus that the idea of a cut
came to him. In 1862 he accepted a position at the Brunswick Polytechnikum in his hometown. He made many
contributions to number theory and algebra, where he introduced the idea of an ideal in a ring. He never married
and he had no PhD students. He died in 1916 in the same town where he was born.

1.2 The Real Numbers
11
A useful observation about cuts is the following. If A is a cut and x ∈Q\A, then
a < x for every a in A. In fact otherwise there is an a in A with x ≤a. Since x /∈A,
it must be that x < a. But then (b) in the deﬁnition says x ∈A.
In the literature there is another deﬁnition of a Dedekind cut as a pair of non-
empty subsets (A, B) of Q having certain properties. See [8].
1.2.12. Definition. The set of real numbers is the collection of Dedekind cuts of
Q. The real numbers are denoted by R.
Does this deﬁnition strike you as strange?
I thought it strange the ﬁrst time I saw this. But think of the cut given in (1.2.11).
Isn’t it natural to call this
√
2? Nevertheless, you have a right to pause when we
deﬁne a number to be a set.
We have a rather involved task before us. We want to show that R as deﬁned above
has all the properties we are used to thinking the set of real numbers has: the ability
to add and multiply as well as a notion of order. We’ll also see that it has the Density
and Completeness properties, (1.2.1) and (1.2.3). A caveat is that many details will
be left to the reader to prove.
We start by showing that Q ⊆R in a natural way. But ﬁrst let’s make a temporary
agreement about notation. For elements of R we’ll use small Greek letters, while
rational numbers will be denoted by small roman letters.
1.2.13. Proposition. If a ∈Q, then α = {b ∈Q : b < a} is a real number; that is,
α is a cut.
Proof. Clearly α is a proper subset of Q, and if b ∈α and c < b, then c ∈α. Finally
if b ∈α, then c = (b + a)/2 ∈Q and b < c < a. Thus α is a cut.
■
What is happening in the last proposition is that we have embedded Q into R by
using the map a →α. We identify each rational number a with its image α in R.
Unless there is a good reason to make a distinction between a rational number a and
its image α under this map, no distinction will be made. Thus we write
a = {b ∈Q : b < a}
In particular, 0 = {b ∈Q : b < 0}. So we use the symbol 0 to denote both the ratio-
nal number 0 and the corresponding cut.
We introduce the concept of an order on R.
1.2.14. Definition. If α, β ∈R, say that α < β if α ⊆β and α ̸= β. Say that
α ≤β if α ⊆β with the possibility they are equal. We deﬁne α > β and α ≥β
similarly.
As usual, if α > 0, we say α is positive; if 0 > α, we say α is negative. The proof
of the next result is Exercise 11.
1.2.15. Proposition. (a) If a ∈Q and β ∈R such that a < β, then a ∈β.
(b) If α, β ∈R, then either α ≤β or β ≤α.
(c) (Trichotomy Law) If γ ∈R, then either γ < 0, 0 < γ , or γ = 0.

12
The Real Numbers
1.2.16. Theorem. If E is a non-empty subset of R that is bounded above, then
sup E exists. That is, R has the Completeness Property.
Proof. So E is a set of cuts; that is, a set of subsets of Q. Take a moment to absorb
this and the notation and reasoning that follow will seem more natural. Let ζ be an
upper bound for E and let α = {ϵ : ϵ ∈E}. The crux of the proof is to ﬁrst show
that α is a cut, and so α ∈R; then we’ll show that α = sup E. To see that α is a
cut, observe that it is non-empty since E ̸= ∅. Since ϵ ⊆ζ for every ϵ in E, α ⊆ζ
and so α ̸= Q. To show that α satisﬁes the other two parts of the deﬁnition of a cut,
ﬁx b in α and let ϵ ∈E such that b ∈ϵ. If y ∈Q with y < b; it follows that y ∈ϵ
because ϵ is a cut; hence y ∈α. Finally, since ϵ is a cut there is a rational number x
in ϵ with b < x; since x ∈ϵ, x ∈α. Therefore α is a cut.
Now that we have that α ∈R, we show that α = sup E. Since ϵ ⊆α for every
ϵ in E, α is an upper bound for E. On the other hand, if ζ is any upper bound for
E, then ϵ ≤ζ for all ϵ in E. That is, ϵ ⊆ζ for all ϵ in E. Therefore α ≤ζ.
■
Now that we have the Completeness Property, we can prove half of the Density
Property. In fact it is a direct consequence of the deﬁnition of R that if α, β ∈R
and α < β, then there is a rational number γ with α < γ < β. Indeed, since α ̸= β,
there is a b in β such that b /∈α. If we let γ be the cut deﬁned by b, then α < γ < β.
The proof of the other half of the Density Property must wait until we have estab-
lished the arithmetic properties of the real numbers. We start with the deﬁnition of
addition in R. To justify this we need a proposition.
1.2.17. Proposition. If α, β ∈R, then
γ = {a + b : a ∈α, b ∈β}
is a cut.
Proof. (a) To show that γ is a proper set we ﬁrst note that it is clear that it is not
empty. Now take positive rational numbers p, q such that a < p for all a in α and
b < q for all b in β. (Why do p and q exist?) It follows that p + q > a + b for all a
in α and b in β. Thus p + q /∈γ , and so γ ̸= Q.
(b) Let x ∈γ and suppose y < x. Put x = a + b, where a ∈α and b ∈β. So y −
a < b; hence y −a ∈β. But then y = a + (y −a) ∈γ .
(c) Take a + b in γ and let a′ ∈α such that a < a′. So a′ + b ∈γ and is strictly
bigger that a + b.
■
1.2.18. Definition. If α, β ∈R, then α + β is the cut γ deﬁned in the preceding
proposition.
1.2.19. Proposition. If α, β, γ ∈R, the following hold.
(a) α + β = β + α.
(b) α + (β + γ ) = (α + β) + γ .
(c) α + 0 = α.
The proof is an exercise.

1.2 The Real Numbers
13
Now we start the path to the deﬁnition of the negative of a real number α. It
is convenient to introduce the notation Q+ = {a ∈Q : a > 0}. The method for
deﬁning −α may seem a bit opaque at ﬁrst, so let’s take a minute to motivate it.
Examine the cut A = α introduced in Proposition 1.2.9 to show that
√
2 is not
rational; suppose we want to ﬁnd the cut associated with −
√
2. This would be
β = {b ∈Q : b < −
√
2}. (We have departed somewhat from the notation and spirit
we have been following up to this point, but this is an intuitive discussion and so
we take some liberties.) Note that b ∈β if and only if
√
2 < −b. Now for any b in
β there is an r in Q+ such that b + r ∈β. Thus b ∈β if and only if there is an r
in Q+ such that −b −r /∈α. Note that this necessary and suﬃcient condition for
membership in −α is consistent with the rigorous approach we are following for
deﬁning R; remember this intuitive discussion when we deﬁne the negative of a real
number in the proposition below.
1.2.20. Proposition. For every α in R there is a β in R such that α + β = 0.
Proof. Fix α in R and let
β = {b ∈Q : there is an r in Q+ such that −b −r /∈α}
The ﬁrst task is to show that β is a cut. To see that β ̸= ∅, assume x /∈α and put
b = −x −1. Since −b −1 = x, b ∈β. On the other hand, if a ∈α and r ∈Q+,
a −r < a and so −(−a) −r ∈α for every r > 0. That is, −a /∈β and β ̸= Q. The
remainder of the proof that β is a cut is left to the reader.
Now observe that if b ∈β, then −b /∈α. In fact if −b ∈α, then for every r in
Q+, −b −r ∈α since it is smaller than −b; this violates the deﬁnition of member-
ship in β. Hence for any a in α, a < −b and so a + b < 0. Therefore α + β ⊆0. To
show that 0 ⊆α + β, let z ∈0 and put x = −z/2; so x > 0. By Lemma 1.2.6 there is
an n in Z such that nx ∈α but (n + 1)x /∈α. (Details?) So b = −(n + 2)x ∈β and
−b −x /∈α. Therefore z = nx + b ∈α + β and we have shown that 0 ⊆α + β. ■
1.2.21. Definition. For α in R, we deﬁne −α to be the cut β in Proposition 1.2.20.
The proof of the next proposition is Exercise 12.
1.2.22. Proposition. Let α, β, γ ∈R.
(a) The element −α is unique. That is, if α + β = α + γ = 0, then β = γ .
(b) −(−α) = α.
(c) If α ≥0, then −α ≤0.
Deﬁning multiplication is more complicated than deﬁning addition due to the
fact that the product of two negative rational numbers is positive. Consequently we
begin with the deﬁnition of the product of two elements of R+ = {α ∈R : α > 0}
and later we will see how to extend this to the deﬁnition of the product of arbitrary
real numbers. The proofs of the various properties of the deﬁnition are very similar
and will be left as exercises. The proof of the next proposition is Exercise 13.

14
The Real Numbers
1.2.23. Proposition. If α, β ∈R+, then
{ab : a ∈α, b ∈β and a > 0}
is a cut.
1.2.24. Definition. If α, β ∈R+, then αβ is the cut deﬁned in the preceding propo-
sition.
The proof of the next proposition is Exercise 14.
1.2.25. Proposition. If α, β, γ ∈R+, then the following hold.
(a) αβ = βα.
(b) (αβ)γ = α(βγ ).
(c) If 1 is the cut 1 = {x ∈Q : x < 1}, then α = 1 · α.
(d) α(β + γ ) = αβ + αγ .
(e) α−1 = {c : c < 1
a when a /∈α} is a cut and α(α−1) = 1.
Now we extend the deﬁnition of the product to all real numbers.
1.2.26. Definition. Let α, β ∈R. (a) Deﬁne α0 = 0α = 0. (b) When neither α nor
β is 0, deﬁne
αβ =
⎧
⎪⎨
⎪⎩
(−α)(−β)
if α < 0, β < 0
−[(−α)β]
if α < 0, β > 0
−[α(−β)]
if α > 0, β < 0
I am going to leave it to the interested reader to formulate and prove a version of
Proposition 1.2.25 for products of arbitrary real numbers. There are also properties
relating the order structure and the arithmetic structure of R, such as αβ < 0 when
α > 0 and β < 0, that will be left to the reader.
Deﬁne the absolute value of α, |α|, by
|α| =
	
α
if α ≥0
−α
if α ≤0
1.2.27. Proposition. If α, β ∈R, the following hold.
(a) |α| = | −α|.
(b) |αβ| = |α||β|.
(c) (Triangle Inequality) |α + β| ≤|α| + β|.
(d) (Reverse Triangle Inequality) | |α| −|β| | ≤|α −β|.
Proof. The proof of the ﬁrst three parts is left to the reader in Exercise 14. The
proof of (d) is easy if we apply the triangle inequality and use a little trick that will be
used frequently in the future, namely adding and subtracting a quantity. Observe that
|α| = |(α −β) + β| ≤|α −β| + |β|. Subtracting we get that |α| −|β| ≤|α −β|.
Now reverse the roles of α and β in this inequality to get |β| −|α| ≤|α −β|. Using
the deﬁnition of absolute value we get (d).
■

1.2 The Real Numbers
15
The triangle inequality will be used frequently as we progress in this book and
we will not cite it by name; it’s that fundamental. Also be aware that adding and
subtracting a term and then applying the triangle inequality is a method of proof
you will also see used often in the future.
The reader knows that the deﬁnition of an irrational number is an element
in R\Q. In the language of cuts, it is a cut that is not of the form given in
Proposition 1.2.13.
1.2.28. Theorem. (a) If α, β ∈Q, then there is an irrational number γ with α <
γ < β.
(b) If α, β ∈R\Q and α < β, then there is a rational number x with α < x < β.
Proof. The proof of (b) was given just after Theorem 1.2.16, so it remains to
prove (a). To start we show there is an irrational number t with 0 < t < 1. Indeed,
t =
√
2/2 works, where
√
2 denotes the irrational number corresponding to the
cut {a ∈Q : a2 < 2 or a < 0}. If α, β ∈Q and α < β, then it is easy to see that
α < γ = (1 −t)α + tβ < β. Also t = (β −α)−1(γ −α), so if it were the case
that γ is rational, we would have that t is rational.
■
Before we conclude the section, we introduce the concept of inﬁnity as related
to R as well as the set of extended real numbers R∞. We ﬁrst deﬁne R∞as the set
R ∪{∞, −∞}, where ±∞are two abstract points, and we deﬁne an order on this
set by declaring that for any α in R we have that −∞< α < ∞. We assume that
the reader has some familiarity with the idea of inﬁnity and understands that ±∞
are not real numbers.
This concludes the present section. The reader interested in going deeper into
these matters can consult [6] and [13]. We now abandon the formalism of this sec-
tion and starting in the next section we return to using R as we have all known it.
Exercises
(1)
For real numbers a and b write the quantities a + b + |a −b| and a + b −|a −b|
without using absolute values.
(2)
If α = sup E, show that −α = inf{−E}.
(3)
Let E
be a subset of R that is bounded above and set A = {a ∈R :
a is an upper bound for E}. Show that sup E = inf A.
(4)
Give a proof that
√
3 is not a rational number.
(5)
Fill in the missing details in the proof of Proposition 1.2.9.
(6)
For each of the following sets X ﬁnd inf X and sup X if they exist and say if they
belong to X. (a) X = {x : |3x −9| < 5}. (b) X = {x : |3x −1| > 1
3}. (c) X = {x ∈
Q : x2 > 7}.
(7)
Find the inﬁmum and supremum of the set X = {1
2, 2
3, 3
4, 4
5, . . .}.
(8)
Find the inﬁmum and supremum of the set X = {3 −

1
2+n : n ∈N}.

16
The Real Numbers
(9)
If E and F are bounded subsets of real numbers, show that sup{x −y : x ∈E, y ∈
F} = sup E −inf F.
(10)
Show that the set A = {a ∈Q : a2 < 2 or a < 0} is a cut.
(11)
Prove Proposition 1.2.15.
(12)
Prove Proposition 1.2.22.
(13)
Prove Proposition 1.2.23.
(14)
Prove Proposition 1.2.27.
(15)
If A and B are two non-empty subsets of R that are bounded, give a necessary and
suﬃcient condition that sup A = inf B.
(16)
If E is a non-empty bounded subset of R and a, b ∈R, ﬁnd formulas for sup{ax +
b : x ∈E} and inf{ax + b : x ∈E} in terms of sup E and inf E.
1.3. Convergence
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
We divorce ourselves from the notation convention of the preceding section where
we denoted real numbers as lower case Greek letters. We will often use the idea
of an interval in R, of which there are several types. An open interval I is a subset
of the form I = {x ∈R : c < x < d}, where −∞≤c < d ≤∞; it is denoted by
(c, d). A closed interval is a set of the form [c, d] = {x ∈R : c ≤x ≤d}, where
−∞< c ≤d < ∞. A half-open interval is a set having one of two forms: [c, d) =
{x ∈R : c ≤x < d} or (c, d] = {x ∈R : c < x ≤d}, where there are appropriate
restrictions on c and d. (What restrictions?) An interval in R is a set I of any one of
these types.
In this section we will examine the convergence of sequences of real numbers.
Recall that a sequence in R is an enumeration of some real numbers: a1, a2, . . . It
is usually denoted by {an}.
1.3.1. Definition. A sequence {an} in R converges to a if for any open interval
containing a, I contains all but a ﬁnite number of terms of the sequence. This is
denoted by writing an →a or a = limn an = lim an.
Observe that if an →a, what happens with the ﬁrst ﬁnite number of elements of a
sequence is irrelevant to whether it converges. That is, for any integer n0, an →a if
and only if the sequence {an : n ≥n0} →a. The ﬁrst result is that when a sequence
converges, the limit is unique.
1.3.2. Proposition. A convergent sequence can only converge to one point. That is,
if an →a and an →b, then a = b.
Proof. Adopting the notation in the statement of the proposition, assume that a ̸= b.
Thus we may assume that a < b. It follows that
a ∈

a −b −a
2
, a + b −a
2

while b ∈

b −b −a
2
, b + b −a
2


1.3 Convergence
17
Since these two intervals are disjoint, it cannot be that they each contain all but
ﬁnitely many members of the sequence {an}. This contradicts the hypothesis that
the sequence converges to both a and b.
■
Before presenting some examples, let’s establish the next result; this proposition
quantiﬁes the deﬁnition of convergence.
1.3.3. Proposition. Let {an} be a sequence in R and let a ∈R.
(a) an →a if and only if for every ϵ > 0 there is an N such that |an −a| < ϵ for
all n ≥N.
(b) an →a if and only if (an −a) →0.
Proof. (a) Suppose an →a and ϵ > 0. Since I = (a −ϵ, a + ϵ) is an open interval
that contains A, I contains all but a ﬁnite number of the terms of the sequence.
This means there is an integer N such that an ∈I when n ≥N; that is, |a −an| <
ϵ when n ≥N. Conversely, assume the condition holds and I = (c, d) is an open
interval containing a. Since c < a < d, there is an ϵ > 0 such that c < a −ϵ < a <
a + ϵ < d. Thus there is an integer N such that |an −a| < ϵ when n ≥N. That is
{aN, aN+1, . . .} ⊆I.
(b) Exercise 1.
■
1.3.4. Example. For the sequence {an} deﬁned by an = n−2, we have that an →0.
Note that if ϵ = 1/100 and N = 11, then |an| < ϵ when n ≥N. We could also take
N = 20 and get that |an| < ϵ when n ≥N. In other words, in the preceding propo-
sition that value of N obtained for a given ϵ is not unique. If ϵ = 1/10, 000, then
we could take N = 101. In general, for any ϵ we could take for N any integer larger
than 1/√ϵ.
It is worth observing that in part (a) of the preceding proposition the condition
n ≥N can be replaced by n > N and the conclusion |an −a| < ϵ can be replaced
by |an −a| ≤ϵ. (Verify!) The value of part (a) above is that it quantiﬁes the idea
of convergence as the preceding example shows. See Exercise 2.
1.3.5. Example. (a) If an = a for all n, then an →a.
(b) 1
n →0. In fact if ϵ > 0, then by Proposition 1.2.6 there is an N in N with
Nϵ > 1. It follows that for n ≥N, 1
n < ϵ.
(c) If an →0 and {bn} is a bounded sequence, then anbn →0. In fact let |bn| ≤B
for all n ≥1 and let ϵ > 0. Choose N such that |an| < ϵ/B when n ≥N. If n ≥N,
then |anbn| ≤B|an| < ϵ.
(d) If an →a, then {an} is bounded. In fact choose an integer N such that |a −
an| < 1 for n ≥N. Since {a1, . . . , aN} is ﬁnite, there is a number A with |an| ≤A
for 1 ≤n ≤N. If n ≥N, then |an| ≤|an −a| + |a| ≤1 + |a|. Therefore for every
n ≥1, |an| ≤M = max{A, 1 + |a|}.

18
The Real Numbers
1.3.6. Proposition. If an →a and bn →b, the following hold.
(a) an + bn →a + b.
(b) anbn →ab.
(c) If b ̸= 0, then bn ̸= 0 except for possibly a ﬁnite number of integers n. If bn ̸= 0
for all n, then an/bn →a/b.
Proof. The proof of (a) is left to the reader. To prove (b) note that
|anbn −ab| ≤|anbn −anb + anb −ab|
≤|anbn −anb| + |anb −ab|
≤|an||bn −b| + |b||an −a|
By Example 1.3.5(d) there is a constant C such that |an| ≤C for all n. Choose M >
max{C, |b|}. If ϵ > 0, let N be such that |an −a| < ϵ/2M and |bn −b| < ϵ/2M
whenever n ≥N. From the above inequality we have that when n ≥N, |anbn −
ab| < ϵ and so (b) holds.
To start the proof of (c), choose n0 such that |bn −b| < |b|/2 when n ≥n0. Thus
for n ≥n0, |b| = |b −bn + bn| ≤|b −bn| + |bn| < |b|/2 + |bn|, and it follows that
|bn| > |b|/2. In particular we have the ﬁrst part of the statement of (c). Now observe
that we actually proved that we have a constant c > 0 such that |bn| ≥c for all but a
ﬁnite number of integers n. As we observed after Proposition 1.3.3, we can assume
that |bn| ≥c > 0 for all n. Note that if we prove that b−1
n
→b−1, an application of
part (b) shows that (c) is valid. But

1
bn
−1
b
 =

b −bn
bbn
 ≤1
c2 |b −bn|
From here the reader can easily supply the details to demonstrate that b−1
n
→
b−1.
■
A sequence {an} is said to be increasing if an ≤an+1 for all n; it is strictly increas-
ing if an < an+1 for all n. It should be noted that some call a sequence satisfying
an ≤an+1 for all n a non-decreasing sequence and reserve the term “increasing” for
what we have called strictly increasing. Similarly we deﬁne {an} to be decreasing if
an+1 ≤an for all n, and we can deﬁne strictly decreasing if the ≤sign is replaced
by <. Observe that if {an} is a decreasing sequence, then {−an} is increasing. So
every time we prove a result for increasing sequences we have an analogous result
for decreasing sequences.
1.3.7. Proposition. If {an} is an increasing sequence that is bounded above and a =
supn an, then an →a. Similarly, if {an} is a decreasing sequence that is bounded
below and a = infn an, then an →a.
Proof. As we observed before the statement of the proposition, we need only
prove the statement concerning increasing sequences. If ϵ > 0, then by deﬁnition
of the supremum, a −ϵ is not an upper bound of {an}. Thus there is an integer N

1.3 Convergence
19
such that a −ϵ < aN. But since {an} is increasing we have that a −ϵ < an ≤a
whenever n ≥N. Thus an →a.
■
1.3.8. Definition. If {an} is a sequence and {nk} is a sequence of positive integers
with n1 < n2 < · · · , then {ank} is called a subsequence of {an}.
Note that a subsequence is a new sequence, so it makes sense to discuss the con-
vergence of a subsequence.
1.3.9. Example. (a) {a2n} is a subsequence of {an}.
(b) {a1, a5, a3, a9, a7, . . .} is not a subsequence of {an}. (Why?)
(c) {a1, a1, a1, . . .} is not a subsequence of {an}. (Why?)
The proof of the next proposition is Exercise 8. Also see Exercise 9.
1.3.10. Proposition. If an →a and {ank} is a subsequence of {an}, then {ank} →a.
The next result is extremely important, our crucial result on convergence. As
might be expected, its proof is rather complex.
1.3.11. Theorem (Bolzano3–Weierstrass4 Theorem). A bounded sequence has a
convergent subsequence.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
3 Bernard Bolzano was born in Prague in 1848. His father was from northern Italy and migrated to Prague where
he was an art dealer. Bernard was the fourth of twelve children, but only he and a brother made it to adulthood.
Bernard had delicate health his entire life. In 1796 he entered the Charles University in Prague, where he studied
philosophy, physics, and mathematics. In 1800 he began a course in theological studies while simultaneously
writing a doctoral thesis in geometry; he earned his doctorate in 1804. Two days after receiving that degree he was
ordained as a Catholic priest. He soon realised he wanted to teach rather than minister to people. Nevertheless
he was given a chair in philosophy and religion at the university, though his views diﬀered from those of the
Habsburgh rulers. Between 1821 and 1825 he was tried for heresy but refused to recant. He resigned his chair
and spent most of his time working on mathematics. It was earlier, in 1817, that Bolzano proved the present
theorem, which he used to prove the Intermediate Value Theorem. (See Theorem 1.7.7 below.) It was proved
50 years later by Weierstrass and it was recognized as a fundamental result. Bolzano died in 1848 in Prague. It
seems a bit mysterious why Weierstrass shares the credit for this result, given the amount of time that elapsed
before he found it. I suspect it has something to do with the great prestige Weierstrass has in mathematics.
4 Karl Weierstrass was born in 1815 in Ostenfelde, Germany. His early schooling was rocky, though he exhibited
greater than usual mathematical ability. The diﬃculty was that his father wanted him to pursue a career in
ﬁnance, but his passion was for mathematics. At ﬁrst he reacted to this with a rebellious approach, neglecting all
his studies and focusing on fencing and drinking. Nevertheless he studied mathematics on his own with extensive
readings, even though he was supposed to follow a course in ﬁnance. After having left the university of Bonn
without taking any examinations, he seems to have reached an understanding with his father and attended the
academy at Münster with the intention of becoming a high school teacher. Here he came under the inﬂuence of
Christoph Gudermann, a mathematician of note, and impressed his mentor with a paper on elliptic functions.
In 1841 Weierstrass passed the exam to become a teacher, a career he followed for some years. In 1854 he
published a paper on Abelian functions and attracted considerable attention from the research world – suﬃcient
for the University of Königsberg to give him an honorary doctorate, enabling him to launch his university career
at Braunsberg. He obtained a chair at the University of Berlin in 1856, where he remained for the rest of his
life. He had a profound inﬂuence on mathematics, setting new standards of rigor and fostering the careers of
numerous mathematicians including many whose contributions were profound. He became known as the father
of modern analysis. He was plagued by health problems that periodically surfaced and then ebbed. Starting in
the early 1860s he lectured while seated and while a student assistant wrote on the board. During his last three
years he was conﬁned to a wheelchair and died of pneumonia in Berlin in 1897. He never married.

20
The Real Numbers
Proof. As we mentioned the proof is involved so be careful. Start by letting {an} be
the bounded sequence and assume that x and y are real numbers such that x ≤an ≤y
for all n. The proof is trivial if the sequence has only a ﬁnite number of distinct
elements since in that case there is at least one point that is repeated inﬁnitely often;
hence we can take a subsequence all of whose entries are equal to that point. So
assume {an} has an inﬁnite number of distinct points. Divide the interval I = [x, y]
into two equal parts and look at which points in the sequence {an} belong to each
half. Since {an : n ∈N} is an inﬁnite set, one of the intervals has an inﬁnite number
of these points; call it J1 and let N1 = {n ∈N : an ∈J1}. Observe that the length
of J1, |J1|, equals 2−1(y −x). Put b1 = inf{an : n ∈N1}, and let n1 = min N1. We
have the ﬁrst element in our desired subsequence, an1, and it satisﬁes |b1 −an1| ≤
|J1| = 2−1(y −x).
To ﬁnd the second element in the desired subsequence, we proceed in a similar
way. Divide the interval J1 into two equal parts, and let J2 be a half interval that
contains inﬁnitely many of the points {an : n ∈N1}. Put N2 = {n ∈N1 : an ∈J2},
and let b2 = inf{an : n ∈N2}. We are tempted to put n2 = min N2. It might happen,
however, that n1 ∈N2, in which case n1 = min N2 and we would have that n2 = n1.
Since we want to deﬁne the second element an2 in a subsequence, we need that
n2 > n1. Thus put n2 = min{n ∈N2\{n1}}. From the construction we have that
|J2| = 2−1|J1| = 2−2(y −x) and |b2 −an2| ≤2−2(y −x)
Also because N2 ⊆N1, we have that
b1 ≤b2
Continue this process. We obtain decreasing intervals J1, J2, . . . ; inﬁnite sets of
integers N1, N2, . . . with N1 ⊆N2 ⊆· · · ; and integers n1 < n2 < · · · . We deﬁne
bk = inf{ank : nk ∈Nk}. We have that these satisfy:
|Jk| = 1
2k (y −x)
bk ≤bk+1
|bk −ank| ≤1
2k (y −x)
Now these conditions imply that {bk} is an increasing sequence and it is bounded
above by y. By Proposition 1.3.7 there is a number a such that bk →a. Therefore
|ank −a| = |ank −bk + bk −a|
≤|ank −bk| + |bk −a|
≤1
2k (y −x) + |bk −a|
→0
■
The reader would do well to study the preceding proof. To begin, be conscious of
the fact that we have used the Completeness Property of the real numbers. (Where?)

1.3 Convergence
21
There are many bounded sequences in Q that do not have a subsequence that con-
verges to another rational number. For example, take a sequence in Q that converges
to
√
2. Also not only does the proof establish an important theorem, but it uses tech-
niques that can and will be employed in the future.
1.3.12. Definition. A sequence {an} is called a Cauchy5 sequence if for every ϵ > 0
there is an integer N such that |an −am| < ϵ when m, n ≥N.
1.3.13. Theorem. A sequence converges if and only if it is a Cauchy sequence.
Proof. Half of the proof is straightforward. Assume that an →a and ϵ > 0. There
is an integer N such that when n ≥N, |an −a| < ϵ/2. Hence when m, n ≥N, |am −
an| ≤|am −a| + |a −an| < ϵ.
Now assume that {an} is a Cauchy sequence.
Claim. If {ank} is a subsequence of {an} that converges to a, then an →a.
In fact let ϵ > 0 and choose an integer N1 such that |ank −a| < ϵ/2 when nk ≥
N1. Choose N2 such that |an −am| < ϵ/2 when n, m ≥N2. Put N = max{N1, N2},
and suppose n ≥N. Pick any integer nk ≥N. Since we also have that nk ≥N1, it fol-
lows that |ank −a| < ϵ/2. Since n, nk ≥N2, we have that |an −ank| < ϵ/2. Hence
|an −a| ≤|an −ank| + |ank −a| < ϵ and we have established the claim.
From here the proof is an easy consequence of Theorem 1.3.11. In fact there
is an integer N such that |an −am| < 1 when m, n ≥N. Thus when n ≥N,
|an| ≤|an −aN| + |aN| ≤1 + |aN|, and so {an : n ≥N} is bounded. Since what
remains of the sequence is ﬁnite, {an} is a bounded sequence. By the Bolzano–
Weierstrass Theorem {an} has a convergent subsequence. Hence the claim proves the
theorem.
■
In light of this theorem, we see the importance of the concept of a Cauchy
sequence – if we are given a Cauchy sequence, we don’t have to produce a limit
in order to know that the sequence converges. We will see the importance of this as
we proceed.
We conclude this section with two important concepts for sequences. First, how-
ever, we need to introduce the idea of a sequence converging to ±∞. Say that a
sequence {xn} converges to ∞if for each real number R there is an integer N such
that xn > R for all n ≥N. Similarly, xn →−∞if for each real number R there is
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
5 Augustin Louis Cauchy was born in Paris in August 1789, a month after the storming of the Bastille. He was
educated in engineering and his ﬁrst job was in 1810 working on the port facilities at Cherbourg in preparation
for Napoleon’s contemplated invasion of England. In 1812 he returned to Paris and his energies shifted toward
mathematics. His contributions were monumental, with a plethora of results bearing his name. His collected
works ﬁll 27 published volumes. As a human being he left much to be desired. He was highly religious with
a totally dogmatic personality, often treating others with dismissive rudeness. Two famous examples were his
treatment of Abel and Galois, where he refused to consider their monumental works, which they had submitted to
him. Both suﬀered an early death. Perhaps better treatment by Cauchy would have given them some recognition
that would have resulted in a longer life and a productive career to the betterment of mathematics; we’ll never
know. He had two doctoral students, one of which was Bunyakowsky. Cauchy died in 1857 in Sceaux near Paris.

22
The Real Numbers
an integer N such that xn < R for all n ≥N. It is easy to see that xn →−∞if and
only if −xn →∞. Also see Exercise 13.
If {an} is a given sequence, let xn = sup{an, an+1, . . .} and consider the sequence
{xn}. Note that as n increases, we are taking the supremum over a smaller collection
of terms. Thus xn ≥xn+1 for all n; that is, {xn} is a decreasing sequence. As such it
has a limit, though that limit might be ±∞. (Note that it could be that xn = ∞for
all n if {an} is not bounded above; in this case xn →+∞. Though it is impossible
that any xn = −∞, it might still happen that xn →−∞.) Similarly if we set yn =
inf{an, an+1, . . .}, then {yn} is an increasing sequence and so it converges though it
may be that the limit is ±∞. We formalize this discussion in the following.
1.3.14. Definition. If {an} is a sequence of real numbers, the limit superior or upper
limit of {an} is deﬁned as
lim sup
n
an = lim sup an = lim
n [sup{an, an+1, . . .}]
The limit inferior or lower limit of {an} is deﬁned as
lim inf
n
an = lim inf an = lim
n [inf{an, an+1, . . .}]
We will frequently just say the limsup or liminf of the sequence {an}.
Notice that because we allow ±∞as the limits, the limsup and liminf of a
sequence always exists as an element of R∞= R ∪{−∞, ∞}.
1.3.15. Example. (a) If an = (−1)n, then lim sup an = 1 and lim inf an = −1.
(b) If an = 1 when n is odd and an = 1
n when n is even, then lim sup an = 1 and
lim inf an = 0.
(c) If {an} is the sequence
{1/2, 1/4, 2/4, 3/4, 1/8, 2/8, . . . , 7/8, 1/16, . . . , 15/16, . . .}
then lim sup an = 1 and lim inf an = 0.
(d) If an = −n when n is odd and an = n when n is even, then lim sup an = +∞
and lim inf an = −∞.
The proof of the next proposition is Exercise 14.
1.3.16. Proposition. If {an} is a sequence of real numbers, then the following hold.
(a) lim inf an ≤lim sup an.
(b) {an} is bounded above if and only if lim sup an < ∞and it is bounded below if
and only if lim inf an > −∞.
(c) an →a if and only if lim sup an = a = lim inf an.
(d)
If {bn} is another sequence and an ≤bn for all n ≥1, then lim inf an ≤
lim inf bn and lim sup an ≤lim sup bn.
1.3.17. Proposition. If {an} is any sequence in R, then
lim inf an = −[lim sup(−an)]

1.3 Convergence
23
This last result allows us to reduce the proof of an assertion about lim inf to
proving an analogous assertion about lim sup. The reader is asked to prove this as
Exercise 15. In fact anyone who is not thoroughly familiar with lim sup and lim inf
should complete this exercise to solidify the concepts in their brain.
1.3.18. Proposition. If {an} is a sequence in R, α = lim sup an, and β = lim inf an,
then there is a subsequence of {an} that converges to α and another that converges
to β.
Proof. We only prove the statement about α = lim sup an as the other half of
the proposition follows by the preceding proposition. In addition we will assume
α ∈R; when α = ±∞, we leave the proof to the reader. Let sn = sup{an, an+1, . . .};
so sn ↘α. By the deﬁnition of the limit superior, for each k ≥1 there is an
mk with α ≤smk < α + k−1. By the deﬁnition of supremum there is an nk ≥mk
such that α −k−1 < ank ≤smk. Thus |α −ank| < k−1 and so the subsequence
{ank} converges to α.
■
1.3.19. Corollary. Let {an} be a sequence in R.
(a) If a ∈R such that lim sup an > a, then there are inﬁnitely many values of n such
that an > a.
(b) If a ∈R such that lim sup an < a, then there is an integer N such that an < a
for all n ≥N.
Proof. (a) By the preceding proposition there is a subsequence {ank} such that
ank →lim sup an > a. Thus there is an N such that ank > a for all nk ≥N.
(b) If lim sup an < a, then the deﬁnition of the lim sup implies there is an N with
sup{aN, aN+1, . . .} < a. Hence part (b).
■
Exercises
(1)
Prove Proposition 1.3.3(b).
(2)
For each of the following sequences {an} ﬁnd the value of the limit and for each
stipulated value of ϵ, ﬁnd a value of N such that |a −an| < ϵ when n ≥N. (a) an =
n−1, ϵ = .0001. (b) an = 2−n, ϵ = .0001. (Are the values for N you found the small-
est possible? This has no bearing on the convergence, but it’s a bit more challenging
to ﬁnd the smallest possible N.)
(3)
If an →a and {ank} is a renumbering of the original sequence, does ank converge to
a?
(4)
Prove that the sequence {n2} does not converge.
(5)
(a) Show that {2
1n
n } converges and ﬁnd its limit. (b) What is limn n+5
2n+6? ( c) Show
that limn
5n2+2
8n+10n2 = 1
2.
(6)
Show the following. (a)
√
n + 1−√n →0. (b)
√
n2 + 2n −n →1. (c)
√
n+1
n
→0.

24
The Real Numbers
(7)
Show that the following sequences converge. (a) {xn} = { n
2n }. (b) {xn} =
{1·3···(2n−1)
2·4···(2n) }.
(8)
Prove Proposition 1.3.10.
(9)
If {xn} is a sequence in [a, b] with the property that every convergent subsequence
of {xn} converges to the same point x, show that xn →x.
(10)
Prove the converse of Proposition 1.3.7. That is, show that if {an} is an increasing
sequence and an →a, then {an} is bounded above and a = supn an.
(11)
(a) If p > 0, show that limn n−p = 0. (b) If |x| < 1, show that limn xn = 0.
(12)
If {xn} →0, show that
x1 + · · · + xn
n
→0.
(13)
Let {xn} be a sequence in R. (a) If xn →∞and {xnk} is a subsequence, show that
xnk →∞. (b) Find an example of a sequence {xn} then converges to ∞and a
sequence {yn} such that yn →y, but {xnyn} does not converge to ∞. (c) Give a con-
dition on a sequence {yn} such that if xn →∞and yn →y, then xnyn →∞. (d) If
{xn} is a sequence and xn →∞, show that x−1
n
→0. (e) If xn →0, is it true that
x−1
n
→∞?
(14)
Prove Proposition 1.3.16.
(15)
Prove Proposition 1.3.17.
(16)
If {an} and {bn} are sequences of positive numbers such that bn →b > 0, show that
lim sup anbn = blim sup an. Are the requirements that an, bn, b > 0 needed?
(17)
Show that there are sequences of real numbers {an} and {bn} such that lim supn(an +
bn) ̸= lim supn an + lim supn bn.
1.4. Series
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
If {an} is a sequence in R, and m ≥1 we deﬁne the m-th partial sum of {an} as
sm = m
n=1 an = a1 + · · · + am. The notation ∞
n=1 an is called the inﬁnite series
or just the series and stands for the sequence {sm} of partial sums. Sometimes
we may want to start the summing process at a diﬀerent number than n = 1. In
particular we will often examine inﬁnite series of the form ∞
n=0 an as we do in
Proposition 1.4.4 below.
1.4.1. Definition. If {an} is a given sequence of numbers in R and s ∈R, we say
that the inﬁnite series ∞
n=1 an converges to s if the sequence of partial sums {sm},
sm = m
n=1 an, converges to s. When this happens we call the series a convergent
series and we write ∞
n=1 an = s. If the series ∞
n=1 an fails to converge to any real
number s, then we say that the series diverges or that it is a divergent series.
1.4.2. Proposition. If the inﬁnite series ∞
n=1 an converges, then an →0.
Proof. Suppose ∞
n=1 an = s and let sm be the m-th partial sum. So sm →s. Thus
an = sn −sn−1 →s −s = 0.
■

1.4 Series
25
Most of you learned the last result in calculus as well as the fact that the condition
that an →0 does not imply that the series converges. Here is the standard example.
1.4.3. Example. Consider the series ∞
n=1 n−1. This is called the harmonic series.
The origin of this name has to do with the concept of overtones in music, a topic
we won’t go into. To see that this diverges we rewrite the series grouping together
the terms as follows:
∞

n=1
1
n = 1 + 1
2 +

1
3 + 1
4

+

1
5 + · · · 1
8

+

1
9 + · · · + 1
16

+ · · ·
> 1 + 1
2 +

1
4 + 1
4

+

1
8 + · · · 1
8

+

 1
16 + · · · + 1
16

+ · · ·
In other words we group the terms together in such a way that each sum within
parentheses is larger that a half. So the next parenthetical expression would contain
16 terms with the last term being
1
32. We therefore see that the partial sums grow
inﬁnitely large. (This example was important to present at this moment in the text,
but what we just did was to use Corollary 1.4.8 below. Nevertheless in this speciﬁc
instance the application is evidently valid.)
The next result is an example, but its importance elevates it to a proposition.
1.4.4. Proposition.
When |x| < 1, the series ∞
n=0 xn converges to (1 −x)−1.
When |x| ≥1 the series ∞
n=0 xn diverges.
Proof. We begin with the following.
Claim. If x ̸= 1 and n ≥1, then
1 + x + · · · + xn = 1 −xn+1
1 −x
In fact this is demonstrated by verifying that (1 −x)(1 + x + · · · + xn) = 1 −
xn+1.
Now that the claim is established we invoke Exercise 1.3.11(b) to get that the
series converges to (1 −x)−1 when |x| < 1. Notice that when |x| ≥1, limn xn ̸= 0
so the series diverges by Proposition 1.4.2.
■
The series in the preceding proposition is called the geometric series.
We can use Theorem 1.3.13 to obtain a necessary and suﬃcient condition for an
inﬁnite series to converge.
1.4.5. Proposition. An inﬁnite series ∞
n=1 an converges if and only if for every
ϵ > 0 there is an integer N such that when N ≤n < m, | m
k=n ak| < ϵ.
Proof. If sn denotes the n-th partial sum of the series and n < m, then m
k=n ak =
sm −sn−1. Hence the condition stated in this proposition is equivalent to the con-
dition that {sn} is a Cauchy sequence. By Theorem 1.3.13 this is equivalent to the
condition that {sn} converges.
■

26
The Real Numbers
If all the terms of a series are positive, it becomes a bit easier to discuss con-
vergence. In fact note that if an ≥0 for all n, then sn = a1 + · · · + an deﬁnes an
increasing sequence. Hence the next result is immediate from Proposition 1.3.7.
1.4.6. Proposition. If an ≥0 for all n, then ∞
n=1 an converges if and only if the
sequence of partial sums is bounded.
Also see Exercise 3.
1.4.7. Theorem (Comparison Test). If ∞
n=1 an and ∞
n=1 bn are two inﬁnite series
such that there is an integer N with |an| ≤bn when n ≥N and if ∞
n=1 bn converges,
then ∞
n=1 an converges.
Proof. First note that the hypothesis implies that bm ≥0 for all n. Let sn denote the
n-th partial sum of the series ∞
n=1 an. If ϵ > 0, the fact that ∞
n=1 bn converges
implies that we can choose M ≥N such that when m > n ≥M, m
k=n+1 bk < ϵ.
Thus when m>n ≥M, |sm −sn| = |m
k=n+1 an|≤m
k=n+1|an|≤m
k=n+1bn < ϵ.
That is the sequence {sn} is a Cauchy sequence and hence must converge.
■
1.4.8. Corollary. If ∞
n=1 an and ∞
n=1 bn are two inﬁnite series such that there is
an integer N with 0 ≤an ≤bn when n ≥N and if ∞
n=1 an diverges, then ∞
n=1 bn
diverges.
Proof. If it were the case that ∞
n=1 bn converges, then the theorem would imply
that ∞
n=1 an converges.
■
It is worth underlining that unlike in the theorem, in the corollary we are assuming
that each term of the series 
n an is non-negative.
1.4.9. Definition. An inﬁnite series ∞
n=1 an converges absolutely if the series of
positive terms ∞
n=1 |an| converges.
1.4.10. Proposition. If a series converges absolutely, then it converges.
Proof. Suppose ∞
n=1 an is an absolutely convergent series. Let ϵ > 0 and choose
an integer N such that when n ≥N, ∞
k=n |an| < ϵ (Exercise 3). If sn is the n-th
partial sum of the series ∞
n=1 an, then for n > m > N we have that |sn −sm| =
| n
k=m+1 ak| ≤n
k=m+1 |ak| < ϵ. That is, {sn} is a Cauchy sequence and hence it
converges.
■
1.4.11. Example.
Consider the alternating harmonic series ∞
n=1(−1)n 1
n. By
Example 1.4.3 we know this series is not absolutely convergent. On the other hand
for n > m,

n

k=m+1
(−1)k 1
k
 =

1
m + 1 −
1
m + 2 + · · · + (−1)n−m−1 1
n

Now observe that in this sum, if we group successive pairs of terms after (m + 1)−1,
each of those pairs is positive and they are all subtracted from (m + 1)−1. Hence

1.4 Series
27
we have that

n

k=m+1
(−1)k 1
k
 <
1
m + 1
By Proposition 1.4.5, the alternating harmonic series converges. So the converse
of Proposition 1.4.10 is false: there are convergent series that are not absolutely
convergent.
1.4.12. Theorem (Root Test). Let {an} be a sequence in R and put
r = lim sup |an|
1
n
(a) If r < 1, then the series ∞
n=1 an converges absolutely.
(b) If r > 1, then {an} does not converge to 0 and the series ∞
n=1 an diverges.
Proof. (a) Let r < x < 1. By Corollary 1.3.19(b) there is an integer N such that
|an|
1
n < x for all n ≥N. Now the geometric series ∞
n=1 xn converges since 0 ≤
x < 1. Thus the comparison test implies that ∞
n=1 |an| converges.
(b) Since r > 1, Corollary 1.3.19(a) implies that |an|
1
n > 1 for inﬁnitely many
values of n. Hence it cannot be that an →0 and so ∞
n=1 an diverges.
■
1.4.13. Example.
When lim sup |an|
1
n = 1 there is no conclusion. Consider the
harmonic series ∞
n=1 n−1, which diverges. We claim that ( 1
n)
1
n →1. To see this
we need two results from later in this book: L’Hôpital’s Rule (2.5.1) and prop-
erties of the natural logarithm (§3.3). Since the reader has encountered both of
these in calculus and we are only concerned with an example here, we will use
these results from the future to explore the present example. By L’Hôpital’s Rule,
(log x)/x →0 as x →∞. (See Exercise 3.3.14.) Hence log[( 1
n)
1
n ] = −1
n log n →0,
and so ( 1
n)
1
n = exp[log( 1
n)
1
n ] →1. On the other hand the alternating harmonic series
converges and satisﬁes lim sup |an|
1
n = 1.
We should also underscore an additional disparity between the two parts in the
Root Test. Part (a) concludes that the series converges absolutely, while (b) says that
the series diverges, not merely that it fails to converge absolutely. The same disparity
pertains to the next result.
1.4.14. Theorem (Ratio Test). Let {an} be a sequence in R and assume that r =
limn→∞|an+1/an| exists.
(a) If r < 1, then the series ∞
n=1 an converges absolutely.
(b) If r > 1, then the series ∞
n=1 an diverges.

28
The Real Numbers
Proof. The proof is similar to that of the Root Test. If r < x < 1, then there is an N
such that |an+1/an| ≤x for all n ≥N. Thus |an+1| ≤x|an| whenever n ≥N. There-
fore for k ≥1, |aN+k| ≤x|aN+k−1| ≤x2|aN+k−2| ≤· · · ≤xk|aN|. By the Compari-
son Test with the geometric series ∞
n=1 xn, ∞
n=1 |an| converges. The proof of part
(b) is similar to the proof of (1.4.12(b)) and is left to the reader.
■
1.4.15. Example. The series
∞

n=0
xn
n!
converges absolutely for every real number x. (Recall that 0! = 1.) In fact we can
apply the Ratio Test to ﬁnd that
xn+1/(n + 1)!
xn/n!
=

xn+1
(n + 1)!
 n!
xn

=
x
n + 1 →0
The preceding series is very important and we’ll see it again later. In fact the
reader may remember from calculus that this series converges to ex. We’ll establish
this and more when we discuss convergence of functions in Chapter 4.
Absolutely convergent series have many additional properties. For example it can
be shown that if ∞
n=1 an converges absolutely and σ : N →N is a bijection, then
∞
n=1 aσ (n) converges. On the other hand, if the series ∞
n=1 an is conditionally
convergent, that is, it converges but not absolutely, then for any real number x there is
a bijection σ on N such that ∞
n=1 aσ (n) converges to x. This amazing result is called
the Riemann6 series theorem. A proof can be found at http://en.wikipedia.org/wiki/
Riemann_series_theorem
Exercises
(1)
If the series ∞
n=1 an converges, then prove that for any integer m ≥1 the series
∞
n=m an converges.
(2)
(a) Show that if ∞
n=1 an converges to A and ∞
n=1 bn converges to B, then
∞
n=1(an + bn) converges to A + B. (b) If ∞
n=1 an converges to A and x ∈R, show
that x ∞
n=1 an converges to xA.
(3)
Show that a series ∞
n=1 an, where each an ≥0 , converges if and only if for every
ϵ > 0 there is an integer N such that | ∞
k=N ak| < ϵ.
(4)
Prove Theorem 1.4.14(b).
(5)
Prove the following. If {an} and {bn} are two sequences of strictly positive numbers
such that an+1/an ≤bn+1/bn for all n ≥1, then the following hold: (a) If 
n bn
converges, then 
n an converges; (b) if 
n bn diverges, then 
n an diverges.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
6 See the deﬁnition of Riemann integrable in §3.1 below for a biographical note.

1.5 Countable and Uncountable Sets
29
(6)
Does
∞

n=1
(−1)n n22n
n!
converge absolutely?
(7)
Suppose that the series ∞
k=0 ak converges absolutely. (a) If {bk} is a bounded
sequence of numbers, show that ∞
k=0 akbk converges absolutely. (b) By giving an
example, show that if it is only assumed that the series ∞
k=0 ak converges (not
absolutely), then ∞
k=0 akbk may diverge.
(8)
If 
n an converges absolutely, does 
n a2
n converge absolutely?
(9)
If 
n an converges and bn →0, does 
n anbn converge?
1.5. Countable and Uncountable Sets
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Here we will explore the notions of countable and uncountable sets. The fact that
not all inﬁnite sets are equivalent is one that comes as a surprise to many people.
Indeed historically this was a shock to the world of mathematics when Cantor7 ﬁrst
revealed it.
1.5.1. Definition. A set X is countable if there exists a subset A of the natural
numbers N and a bijective function f : A →X. It is called uncountable if it fails to
be countable.
1.5.2. Example. (a) Any ﬁnite set is countable. For inﬁnite sets that are count-
able, we will say they are countably inﬁnite. Some say that such an inﬁnite set is
denumerable. Below we show the existence of sets that are uncountable.
(b) The set of all integers, Z, is countable. In fact, 0, 1, −1, 2, −2, 3, . . . describes
a bijective function from N onto Z. For convenience we will often show a set is
countable by describing how to exhaust the set by writing it as a sequence as we
just did. Such an undertaking tells us how to deﬁne a bijective function even though
ﬁnding a formula for that function may be unclear. In the present case it is not
diﬃcult to write a formula for this function. Indeed if we deﬁne f : N →Z by
f (n) =
	
n
2
if n is even
−n−1
2
if n is odd
then this is the function that gives the correspondence described above. In other situ-
ations writing a formula for the function may range from challenging to impossible.
Understand, however, that proving the existence of such a function does not mean
we have to write its formula. If we describe a process or algorithm for determining
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
7 See the biographical note in Theorem 1.6.8.

30
The Real Numbers
which element of the set corresponds to each integer and if this process exhausts
the set, then we have described the required function.
(c) Any subset of a countable set is countable. In fact this is immediate from the
deﬁnition of a countable set.
The next two propositions are useful in showing that a given set is countable. See,
for example, Corollary 1.5.5 below.
1.5.3. Proposition. (a) If X is any set such that there is a subset A of N and a
surjective function f : A →X, then X is countable.
(b) If X is a countable set, Y is another set, and there is a surjection f : X →Y,
then Y is countable.
Proof. To prove (a), let f and A be as in the statement. For each x in X let nx be the
ﬁrst integer n in A with f (n) = x; that is, nx = min f −1(x). So B = {nx : x ∈X} is
another subset of N and g : B →X deﬁned by g(nx) = x is a bijection. Part (b) is
immediate from (a) and the deﬁnition of a countable set.
■
We take a moment to interrupt the discussion of diﬀerent types of inﬁnite sets to
return to the discussion of set theory. We speciﬁcally want to deﬁne the cartesian
product of two sets, X × Y, something many readers may have already seen. This
is the set
X × Y = {(x, y) : x ∈X, y ∈Y}
Similarly if X1, . . . , Xn are a ﬁnite number of sets we could deﬁne the cartesian
product
X1 × · · · × Xn = {(x1, . . . , xn) : xk ∈Xk for 1 ≤k ≤n}
1.5.4. Proposition. If X and Y are countable sets, then so is X × Y ≡{(x, y) : x ∈
X, y ∈Y}.
Proof. We only consider the case where the two sets are inﬁnite. To prove the propo-
sition it is equivalent to show that N × N is countable. (Why?) Here we want to
deﬁne a bijection f : N →N × N. Again we need only show how to arrange the
elements (m, n) in N × N in a sequence. So imagine N × N as an inﬁnite square
array of pairs of positive integers. On the ﬁrst row are all the pairs {(1, n) : n ∈N};
on the second {(2, n) : n ∈N}; etc. We write down the following sequence of
entries.
(1, 1), (2, 1), (1, 2), (3, 1), (2, 2), (1, 3), (4, 1), (3, 2), (2, 3), (1, 4), . . .
If you write the array on paper and draw northeast diagonal lines connecting these
pairs, you should be able to discern the pattern. (Many other patterns are possible.)
This describes the bijection.
■
1.5.5. Corollary. The set of rational numbers is countable.

1.5 Countable and Uncountable Sets
31
Proof. Writing each rational number as a fraction in reduced terms we see that
there is a bijection between Q and a subset of Z × Z, which is countable by the
proposition.
■
Using induction and the preceding proposition we can obtain the following
corollary.
1.5.6. Corollary. If X1, . . . , Xn are countable sets, then so is X1 × · · · × Xn.
1.5.7. Proposition. If X = ∞
n=1 Xn and each of the sets Xn is countable, then Xis
countable.
Proof. We write Xn = {x1
n, x2
n, . . .}. If Xn is inﬁnite, we can do this with xk
n ̸= x j
n for
all k ̸= j; if Xn is ﬁnite, repeat one of the points an inﬁnite number of times. Thus f :
N × N →X deﬁned by f (n, k) = xk
n is surjective. It follows by Proposition 1.5.3(b)
that X is countable.
■
1.5.8. Corollary. The set of all ﬁnite subsets of N is countable.
Proof. If F denotes the set of all ﬁnite subsets of N, then note that F = ∞
n=1 Sn,
where Sn is the set of all subsets of {1, 2, . . . , n}. But Sn is a ﬁnite set. (In fact from
combinatorics we know that Sn has 2n elements.) By the preceding proposition, F
is countable.
■
Now we turn to some results showing the existence of uncountable sets.
1.5.9. Proposition. The set of all sequences of zeros and ones is not countable.
Proof. Let X be the set of all sequences of zeros and ones, and suppose it is
countable; so we can write X = {x1, x2, . . .}. We manufacture an element a in X
such that a ̸= xn for any n ≥1. This will furnish a contradiction to the assump-
tion that we have an exhaustive list and thus prove the proposition. Suppose that
for each n ≥1, xn = x1
nx2
n · · · is a sequence of zeros and ones; in other words,
xn = {xk
n : k = 1, 2, . . .}. If n ≥1 and xn
n = 0, let an = 1; if xn
n = 1, let an = 0. This
deﬁnes an a = a1a2 · · · in X. Since an ̸= xn
n, a ̸= xn for any n ≥1. This gives our
desired contradiction.
■
1.5.10. Corollary. The collection of all subsets of N, 2N, is not countable.
Proof. In fact by looking at the characteristic functions of subsets of N, we see that
the set of all sequences of zeros and ones is in bijective correspondence with 2N. ■
To prove the next proposition we have to consider dyadic expansions of numbers
in the unit interval. For 0 ≤x ≤1 we can write
x =
∞

n=1
xn
2n

32
The Real Numbers
where each xn = 0 or 1. (This series always converges since it is dominated by
∞
n=1 2−n = 1.) The proof that each x in the unit interval can be so expanded is not
too complicated and proceeds as follows. Consider x and divide the interval into its
equal halves: [0, 1
2] and [ 1
2, 1]. If x belongs to the ﬁrst half, let x1 = 0; if x ∈[ 1
2, 1],
let x1 = 1. (We note an ambiguity here if x = 1
2 and we will address this shortly.)
Note that in either case we have that |x −x1/2| < 1
2. Now consider whichever half
interval contains x and divide it into two equal halves; let x2 = 0 if x belongs to the
ﬁrst half and x2 = 1 if it belongs to the second half. Now we have that
x −
x1
2 + x2
22
 < 1
22
Continue this process and we see that the series so deﬁned will converge to x. (The
reader who wants to write out the details can formulate an induction statement based
on what we just did and prove it. See Exercise 2.)
What about the ambiguity? If x = a/2n for some n ≥1 and 0 < a < 2n, then the
choice of xn can be either 0 or 1. In fact this is the only way such an ambiguity
arises. In fact using the summation for a geometric series,
∞

k=n
1
2k = 1
2n
∞

k=0
1
2k = 1
2n
1
1 −1
2
=
1
2n−1
It follows that if {xn}, {yn} are two sequences of zeros and ones, then the only way
that we can have that ∞
n=1 xn/2n = ∞
n=1 yn/2n is that either there exists an integer
N such that xn = yn for all n ≥N, or one sequence ends in all zeros and the other
ends in all ones. See Exercise 3.
1.5.11. Proposition. The interval (0, 1) is not countable.
Proof. In a sense, this proposition is a corollary of Proposition 1.5.9, but its proof
is a bit more involved than you usually associate with a corollary. Let X be the set
of all sequences of zeros and ones that are not constantly one from some point
on. Let Y be the set of all sequences of zeros and ones that are constantly one
from some point on. Note that X ∩Y = ∅and X ∪Y is the set of all sequences
of zeros and ones. Now by considering the characteristic functions of subsets of N,
there is a bijective mapping between X ∪Y and 2N. Hence X ∪Y is uncountable by
Corollary 1.5.10.
LetY1 be the singleton consisting of the identically 1 sequence and for each n > 1
let Yn be the set of all sequences {xk} in Y with xk = 1 whenever k ≥n. We note that
there is a bijection between Yn and the set of all subsets of {1, . . . , n −1}. Hence
Yn is ﬁnite. By Proposition 1.5.7, Y = ∞
n=1 Yn is countable. Again using Propo-
sition 1.5.7 we have that the only way for X ∪Y to be uncountable is for X to be
uncountable. Therefore (0, 1) is uncountable.
■

1.6 Open Sets and Closed Sets
33
Exercises
(1)
Show that if A is an inﬁnite subset of N, X is a countably inﬁnite set, and f : A →X
is a bijection, then there is a bijection g : N →X. (Hint: First show that if A is an
inﬁnite subset of N, then there is a bijection h : N →A.)
(2)
Write out a detailed proof that each x in the unit interval has a dyadic expansion.
(3)
If {xn}, {yn} are two sequences of zeros and ones, show that
∞

n=1
xn/2n =
∞

n=1
yn/2n
if and only if there is an integer n such that xk = 0 and yk = 1 for all k ≥n.
1.6. Open Sets and Closed Sets
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Here we examine certain special subsets of R that together with their extensions to
higher dimensional space underpin analysis.
1.6.1. Definition. A subset F of R is said to be closed if whenever {xn} is a sequence
of points in F and xn →x we have that x ∈F. A subset G is said to be open if R\G
is a closed set.
1.6.2. Example. (a) R and ∅are simultaneously closed and open.
(b) Finite subsets of R are closed.
(c) A closed interval is a closed set. (Why?) Similarly intervals of the form
(−∞, a] and [b, ∞) are closed. A closed interval is not, however, an open set.
(d) The union of two closed intervals is a closed set. In fact if F = [a, b] ∪
[c, d] and {xn} is a sequence in F such that xn →x, then there is a subsequence
{xnk} contained in either [a, b] or [c, d]. Since xnk →x, we have that x ∈F and F
must be closed.
(e) An open interval (a, b) is an open set since its complement is the union of
two closed intervals. An open interval is not, however, a closed set unless it is the
interval (−∞, ∞).
1.6.3. Proposition. (a) If F1, . . . , Fn are closed sets, then n
k=1 Fk is closed.
(b) If F1, F2, . . . are closed sets, then ∞
k=1 Fk is closed.
(c) If G1, . . . , Gn are open sets, then n
k=1 Gk is open.
(d) If G1, G2, . . . are open sets, then ∞
k=1 Gk is open.
Proof. (a) Let {xn} be a sequence in n
k=1 Fk such that xn →x. It follows that at
least one of the sets F1, . . . , Fn contains inﬁnitely many of the points in {xn}. That is,
there is a set Fj, 1 ≤j ≤n and a subsequence {xnk} contained in Fj. Since xnk →x,
x ∈Fj ⊆n
k=1 Fk, so this union is closed.

34
The Real Numbers
(c) By De Morgan’s Law, R\ n
k=1 Gk = n
k=1(R\Gk), which is closed by (a).
(b) If {xn} is a sequence in ∞
k=1 Fk that converges to x, then for 1 ≤k < ∞{xn} is
a sequence in Fk. Thus x ∈Fk since Fk is closed. Thus x ∈∞
k=1 Fk, which is there-
fore closed by deﬁnition.
(d) As in the proof of (c), R\ ∞
k=1 Gk = ∞
k=1(R\Fk) so that ∞
k=1 Gk must be
open by (b).
■
Also see Exercises 4 and 3. Now we give an equivalent formulation of open sets
that will be used more often than the deﬁnition.
1.6.4. Proposition. A subset G of R is open if and only if for each x in G there is
an ϵ > 0 such that {y ∈R : |y −x| < ϵ} = (x −ϵ, x + ϵ) ⊆G.
Proof. Suppose G is open and x ∈G. If the stated condition in the proposition
is false, then for every n ≥1 there is a point xn in R\G with |xn −x| < 1
n. Thus
xn →x. Since R\G is closed it follows that x ∈R\G, a contradiction. Now assume
that G satisﬁes the stated condition; we want to show that R\G is closed. Let {xn} be
a sequence in R\G that converges to a point x. If it were the case that x /∈R\G, then
x ∈G. By assumption there is an ϵ > 0 with (x −ϵ, x + ϵ) ⊆G. But since xn →x
there is an n such that |xn −x| < ϵ and so xn ∈G, a contradiction.
■
1.6.5. Example. Let a, b ∈R with a < b. If E = {x : a < x < b and x ∈Q}, then
E is not open. In fact if x ∈E, we can choose ϵ > 0 such that (x −ϵ, x + ϵ) ⊆
(a, b), but the Density Property of R implies there is an irrational number y in
(x −ϵ, x + ϵ). Hence (x −ϵ, x + ϵ) is not a subset of E. Similarly, {x : a ≤x ≤
b and x ∈Q} is not a closed set.
We need a lemma that characterizes intervals in R – whether they are bounded
or not, closed, open, or neither. To be clear an interval is any set of the form
[a, b], (a, b), [a, b), or (a, b], where a and b are any real numbers and there is the
possibility that a = −∞, or b = ∞, or both.
1.6.6. Lemma. If I ⊆R, then I is an interval if and only if whenever a, b ∈I with
a < b it follows that [a, b] ⊆I.
Proof. It is clear that every interval has the stated property, so we need only assume
that I has the property and show that it is an interval. To do this let α = inf I and β =
sup I; it may be that α = −∞or β = ∞or both. If α < x < β, then the deﬁnition
of supremum and inﬁmum implies there are points a and b in I with α ≤a < x <
b ≤β. By hypothesis, [a, b] ⊆I; in particular, x ∈I. Hence (α, β) ⊆I. Therefore
it must be that I is one of the four possible intervals with endpoints α and β.
■
1.6.7. Proposition. A subset G of R is open if and only if it is the union of a count-
able number of pairwise disjoint open intervals.
Proof. Clearly if G is the union of a sequence of open intervals, then it is open
by Proposition 1.6.3(d). Now assume that G is an open set. By the preceding
proposition whenever x ∈G, there is an ϵ > 0 such that (x −ϵ, x + ϵ) ⊆G. Let

1.6 Open Sets and Closed Sets
35
Ix denote the collection of all open intervals that contain the point x and are con-
tained in G. By what we just said, Ix ̸= ∅; put Jx = {(a, b) : (a, b) ∈Ix}. We’ll
use the preceding lemma to show that Jx is an interval. Let a, b ∈Jx and assume
that a < b. By deﬁnition there are intervals Ia, Ib in Ix such that a ∈Ia and b ∈Ib.
Since Ia is an open interval containing a as well as x, Ia = (c, d) and c < a, x < d.
Similarly Ib = (c′, d′) and c′ < b, x < d′. Let’s assume that a < b ≤x. It follows
that c < a < b ≤x < d, so [a, b] ⊆Ia ⊆Jx. Now let’s assume that a ≤x ≤b; so
[a, x] ⊆Ia ⊆Jx and [x, b] ⊆Ib ⊆Jx and thus [a, b] ⊆Jx. The remaining case that
x ≤a < b is similar to the ﬁrst. By the lemma, Jx is an interval and it contains x by
its deﬁnition.
We claim that if x and y are distinct points in G it follows that either Jx = Jy or
Jx ∩Jy = ∅. In fact if Jx ∩Jy ̸= ∅, then Exercise 5 implies that Jx ∪Jy is an open
interval. As such it must belong to both Ix and Iy. By deﬁnition, Jx = Jy.
Therefore {Jx : x ∈G} is a collection of pairwise disjoint open intervals. On the
other hand the Density Property of R implies each Jx contains a rational number.
Since Q is countable, {Jx : x ∈G} must be a countable collection of open intervals
whose union is G.
■
For any set E contained in R deﬁne its diameter as
diam E = sup{|x −y| : x, y ∈E}
Note that the set E is bounded if and only if it has ﬁnite diameter.
1.6.8. Theorem (Cantor’s8 Theorem). Let {Fn} be a sequence of non-empty subsets
of R satisfying: (i) each Fn is closed, and (ii) F1 ⊇F2 ⊇· · · .
(a) If one of the sets Fn is bounded, then ∞
n=1 Fn ̸= ∅.
(b) If the sequence of sets {Fn} also satisﬁes (iii) diam Fn →0, then ∞
n=1 Fn is a
single point.
Proof. Let {Fn} be as in the statement of the theorem and put ∞
n=1 Fn = F. By (i)
and Proposition 1.6.3, F is closed. (a) If Fn is bounded, then condition (ii) implies
that Fk is bounded for all k ≥n. Without loss of generality we may assume that F1
is bounded. For each n ≥1 pick a point xn in Fn; {xn} is a bounded sequence. By the
Bolzano–Weierstrass Theorem there is a subsequence {xnk} and a point x such that
xnk →x. Fix any positive integer m; by (ii), xnk ∈Fm for all nk ≥m. By (i), x ∈Fm.
Since m was arbitrary, x ∈F and so F = ∞
n=1 Fn ̸= ∅.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
8 Georg Cantor was the child of an international family. His father was born in Denmark and his mother was
Russian; he himself was born in 1845 in St. Petersburg where his father was a successful merchant and stock
broker. He is recognized as the father of set theory, having invented cardinal and ordinal numbers and proved
that the irrational numbers are uncountable. He received his doctorate from the University of Berlin in 1867
and spent most of his career at the University of Halle. His work was a watershed event in mathematics, but
it was condemned by many prominent contemporary mathematicians. The work was simply too radical, with
counterintuitive results such as R and Rp having the same number of points. He began to suﬀer from depression
around 1884. This progressed and plagued him the rest of his life. He died in a sanatorium in Halle in 1918.

36
The Real Numbers
(b) Note that by condition (iii), from some point on the sets Fn are all bounded.
So by part (a), F ̸= ∅. On the other hand, F ⊆Fn for all n, so diam F ≤diam Fn for
all n. Therefore (iii) implies diam F = 0 and so it can have only one point.
■
In most of the literature you will ﬁnd that Cantor’s Theorem is part (b) alone. Part
(a) is separated because we will need it later in this book. At this point Exercise 6
is compulsory.
1.6.9. Theorem. If X is a closed and bounded subset of R and {Gn} is a sequence
of open sets such that X ⊆∞
n=1 Gn, then there is an integer N such that X ⊆
N
n=1 Gn.
Proof. Let Fn = X\[n
k=1 Gk]. So Fn is closed and bounded and F1 ⊇F2 ⊇· · · . If
Fn ̸= ∅for all n, then Cantor’s Theorem implies ∅̸= ∞
n=1 Fn = X\[n
n=1 Gn]; but
this contradicts the assumption that X ⊆∞
n=1 Gn. Therefore it must be that there
is an N such that FN = ∅, proving the theorem.
■
For any non-empty subset E of R deﬁne the distance from a point x to E by
dist (x, E) = inf{|x −y| : y ∈E}
1.6.10. Definition. For a non-empty subset E of R deﬁne the closure of E to be the
set cl E = {x ∈R : dist (x, E) = 0}.
1.6.11. Proposition. Let E be any non-empty subset of R.
(a) The closure of E is a closed set.
(b) If F is any subset of R such that E ⊆F, then cl E ⊆cl F.
(c) If E is a closed set, then E = cl E.
Proof. Suppose E is not empty and {xn} is a sequence in cl E that converges to x;
we want to show that x ∈cl E. Since xn ∈cl E, there is a yn in E with |yn −xn| < 1
n.
Thus |x −yn| ≤|x −xn| + |xn −yn| →0. Hence x ∈cl E (Why?), proving (a). If
F is a set that contains E and x ∈cl E, then 0 = dist (x, X ) = inf{|x −y| : y ∈E} ≥
inf{|x −y| : y ∈F} = dist (x, F). Hence x ∈cl F. The proof of (c) is Exercise 7. ■
The next result is a corollary of the proof rather than the statement of the preced-
ing proposition.
1.6.12. Corollary. If E ⊆R, then
cl E = {x ∈R : there is a sequence {xn} in E such that xn →x}
The preceding proposition says that the closure of E is the smallest closed set that
contains E.
Exercises
(1)
Prove that a closed interval is a closed set.
(2)
Prove part (d) of Example 1.6.2.

1.7 Continuous Functions
37
(3)
Show that parts (a) and (c) of Proposition 1.6.3 are false if the ﬁnite collections of
sets are replaced by an inﬁnite sequence of sets.
(4)
If G is an open set and F is a closed set, prove that G\F is open and F\G is closed.
(5)
Show that if I and J are two interval in R and I ∩J ̸= ∅, then I ∪J is an interval.
Note that I and J are not assumed to be open or closed intervals. (Hint: Lemma 1.6.6
may be useful.)
(6)
Consider the three restrictions (i), (ii), and (iii) placed on the sets {Fn} in Cantor’s
Theorem. (a) Find a sequence of sets {Fn} that satisﬁes (i) and (ii), but ∞
n=1 Fn = ∅.
(b) Find a sequence of sets {Fn} that satisﬁes (i) and (iii), but ∞
n=1 Fn = ∅. (c) Find
a sequence of sets {Fn} that satisﬁes (ii) and (iii), but ∞
n=1 Fn = ∅.
(7)
Prove part (c) of Proposition 1.6.11.
(8)
Prove Corollary 1.6.12.
(9)
If {an} is a sequence in R and
E = {a ∈R : there is a subsequence {ank} that converges to x}
show that E is a closed set.
(10)
Find the closure of the set E in Example 1.6.5.
(11)
If E ⊆R, say that x is a limit point of E if for every ϵ > 0, there is a point e in
E with 0 < |x −e| < ϵ. (Note that we insist that e ̸= x though there is nothing to
preclude x being in the set E.) A point x is called an isolated point of E if x ∈E but
x is not a limit point. (a) Show that E is a closed set if and only if it contains all its
limit points. (b) Show that x is a limit point of E if and only if there is a sequence
of distinct points in E that converges to x.
1.7. Continuous Functions
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In this section we begin to focus on functions deﬁned on subsets of R. This focus
and the related one of functions deﬁned on subsets of higher dimensional Euclidean
space will remain for the rest of the book. We begin by studying properties that many
functions have. Here is an important and desirable elementary property.
1.7.1. Definition. If X ⊆R and a ∈X, a function f : X →R is continuous at a if
whenever x ∈X and {xn} is a sequence in X that converges to x, f (xn) →f (x). It is
said to be continuous on X if it is continuous at each point of X.
Easy examples of continuous functions are the constant function, f (x) = a for
all x, and the identity function, deﬁned by f (x) = x for all x in X. Usually we will
be looking at functions deﬁned on intervals and we may want to restrict the range to
something less than the entirety of R. This, however, does not aﬀect the deﬁnition.
Suppose X is any set, not just a subset of R, and f , g : X →R. (We are taking X
to not necessarily be a subset of R because we will need this in the future.) Deﬁne
the following functions from X into R. f ± g : X →R is deﬁned by ( f ± g)(x) =
f (x) ± g(x) for all x in X. fg : X →R is deﬁned by ( fg)(x) = f (x)g(x) for all x in
X. If g(x) ̸= 0 for all x in X, then ( f /g)(x) = f (x)/g(x).

38
The Real Numbers
The proof of the next proposition follows immediately from Proposition 1.2.6.
1.7.2. Proposition. If X ⊆R and f , g : X →R are functions continuous at a point
a in X, then the following hold.
(a) f + g : X →R is continuous at a.
(b) fg : X →R is continuous at a.
(c) If g(x) ̸= 0 for all x in X, then f /g : X →R is continuous at a.
1.7.3. Corollary. (a)
Every polynomial is a continuous function on all of R.
(b) A rational function is continuous at every point where its denominator does
not vanish.
Proof. The identity function and the constant functions are continuous from R into
itself. Now repeatedly apply the preceding proposition.
■
We will see many more examples of continuous functions as we progress. In par-
ticular, the trig functions introduced in the next section will be seen to be continuous.
Recall the deﬁnition of the composition of two functions.
1.7.4. Proposition.
Let X,Y ⊆R, f : X →R such that f (X ) ⊆Y, and let g :
Y →R. If f is continuous at a and gis continuous at α = f (a), then g ◦f : X →R
is continuous at a.
Proof. If {an} ⊆X and an →a, then f (an) →f (a) by the continuity of f . Thus
g ◦f (an) = g( f (an)) →g( f (a)) by the continuity of g.
■
Here is an equivalent formulation of continuity.
1.7.5. Theorem. If X ⊆R, a ∈X, and f : X →R, then f is continuous at a if
and only if for every ϵ > 0 there is a δ > 0 such that | f (x) −f (a)| < ϵ whenever
x ∈X and |x −a| < δ.
Before beginning the proof of the theorem, let’s take a moment to understand this
equivalent formulation of continuity at the point a. (Let me add that many books
take the statement of this theorem as the deﬁnition of continuity.) If you want to
phrase the condition that is equivalent to continuity in words (with the equivalent
symbolic statement inserted in brackets) you would say: Tell me how close you want
f (x) to come to f (a) [for every ϵ > 0] and no matter how close you want to get I
can always tell you how close you must take x to a [there is a δ > 0] so that it always
works.
Proof. Assume f is continuous at a and let ϵ > 0. Suppose no δ > 0 can be found
such that the condition is satisﬁed; that is, for any δ > 0 there is at least one x with
|x −a| < δ and | f (x) −f (a)| ≥ϵ. It follows, by taking δ = 1
n, that for every n in
N there is a point an in X with |an −a| < 1
n but | f (an) −f (a)| ≥ϵ. But then we
have that an →a and { f (an)} does not converge to f (a), violating the deﬁnition of
continuity.

1.7 Continuous Functions
39
Now assume that f satisﬁes the stated condition and let {an} be a sequence in
X that converges to a. If ϵ > 0, then we know we can ﬁnd δ as in the condition.
But since an →a, there is an N such that |an −a| < δ when n ≥N. Thus | f (a) −
f (an)| < ϵ when n ≥N. By deﬁnition, f (an) →f (a) so we have shown that f is
continuous at a.
■
At this point we will stop discussing functions that are continuous at a point and
focus on functions continuous on a subset of R, usually an interval.
1.7.6. Theorem (Extreme Value Theorem). If [a, b] is a bounded closed interval
in R and f : [a, b] →R is a continuous function, then f is a bounded function and
there are points x0 and y0 in [a, b] such that f (x0) ≤f (x) ≤f (y0) for every x in
[a, b].
Proof. Let’s prove that the point x0 exists. To do this put α = inf{ f (x) : x ∈[a, b]},
with the full realization at this stage in the argument that it could be that α = −∞.
That is, it could be that f is not bounded below on the interval. The fact that this
cannot happen will follow when we show the existence of the point x0 such that
f (x0) = α. By the deﬁnition of an inﬁmum there is a sequence {xn} in [a, b] such
that f (xn) →α. But [a, b] is a bounded interval and so {xn} is a bounded sequence.
By the Bolzano–Weierstrass Theorem there is a subsequence {xnk} and a point x0
such that xnk →x0; of necessity, x0 ∈[a, b] since [a, b] is a closed interval. But
then f (xnk ) →f (x0), so f (x0) = α.
The proof of the existence of y0 follows by using the ﬁrst half of the proof applied
to the function −f .
■
The theorem states that the function attains its maximum and minimum values,
hence its designation as the Extreme Value Theorem. We will refer to this theorem
as the EVT. If the interval is not closed and bounded the theorem is no longer valid;
just consider the the identity function on any interval that is either not closed or not
bounded.
1.7.7. Theorem (Intermediate Value Theorem). If I is an interval in R, f : I →R
is a continuous function, a, b ∈I, and τ ∈R such that f (a) < τ < f (b), then there
is a t in I that lies between a and b such that f (t) = τ.
Proof. First note that since I is an interval, [a, b] ⊆I. Let A = {x ∈[a, b] : f (x) <
τ}; so A ̸= ∅since a ∈A and x < b for all x in A. Let t = sup A. By the deﬁnition
of supremum there is a sequence {an} in A that converges to t. Since f is continu-
ous, f (an) →f (t); thus f (t) ≤τ. We’ll show that f (t) = τ. In fact if f (t) < τ,
there is an ϵ > 0 such that f (t) + ϵ < τ. By Theorem 1.7.5 there is a δ > 0
such that | f (x) −f (t)| < ϵ when |x −t| < δ. In particular when t < x < t + δ,
f (x) < f (t) + ϵ < τ and so x ∈A. Since t = sup A, this contradiction shows that
it must be that f (t) = τ.
■
We will refer to the Intermediate Value Theorem as the IVT.

40
The Real Numbers
The preceding proof shows the importance of Theorem 1.7.5. In the future we will
use this equivalent formulation of continuity without citing the reference (1.7.5).
Now we state a consequence of the Intermediate Value Theorem together with the
Extreme Value Theorem.
1.7.8. Corollary. The image of a closed and bounded interval under a continuous
function is a closed and bounded interval.
Proof. Given a bounded interval [a, b] and a continuous function f : [a, b] →
R, put α = inf f ([a, b]), β = sup f ([a, b]). Thus f ([a, b]) ⊆[α, β]. The Extreme
Value Theorem implies α, β ∈f ([a, b]). The preceding theorem implies that if
α < τ < β, then τ ∈f ([a, b]). Thus f ([a, b]) = [α, β].
■
1.7.9. Definition. If f , g are two functions deﬁned on a set X and taking values in
R, deﬁne the two functions f ∨g : X →R and f ∧g : X →R as follows:
f ∨g(x) = max{ f (x), g(x)}
f ∧g(x) = min{ f (x), g(x)}
For obvious reasons the function f ∨g is called the maximum of f and g and f ∧g
is called the minimum of f and g.
If | f | is the function deﬁned as | f |(x) = | f (x)|, let’s point out that | f | = f ∨0
and −| f | = f ∧0. The proof of the next lemma is Exercise 7.
1.7.10. Lemma. If X ⊆R and f : X →R is continuous, then | f | is a continuous
function on X.
1.7.11. Proposition. If X ⊆R and f , g : X →R are continuous functions, then
f ∨g : X →R and f ∧g : X →R are both continuous.
Proof. In light of Exercise 6, we need only show that f ∨g is continuous. Begin
by showing that for any two real numbers a and b, max{a, b} = 1
2[a + b + |a −b|].
Hence for the functions f and g,
f ∨g = 1
2[ f + g + | f −g|]
The proposition now follows from the preceding lemma and other previous
results.
■
The preceding proposition allows us to construct new continuous functions from
old ones. For your cultural ediﬁcation, what’s going on here is that we have shown
that if we deﬁne C(X ) to be the collection of all continuous functions from X into
R, then, with the operations ∨and ∧deﬁned above, C(X ) becomes what is called
a lattice. The interested reader can look up the deﬁnition on the Web. (Actually it
must be checked that the axioms deﬁning a lattice are all satisﬁed.)

1.7 Continuous Functions
41
We will close this section by introducing and exploring the following concept.
1.7.12. Definition. If X ⊆R, then a function f : X →R is uniformly continuous
on X if for every ϵ > 0 there is a δ > 0 such that whenever x, y ∈X and |x −y| < δ
we have | f (x) −f (y)| < ϵ.
Note that by Theorem 1.7.5 every uniformly continuous function is continu-
ous. The diﬀerence is in that theorem when we are given the ϵ, the choice of δ is
allowed to depend on a point. For a uniformly continuous function we can obtain
the δ independent of the point. Keep this in mind as you consider the following
example.
1.7.13. Example. The function x →x2 from R into itself is not uniformly contin-
uous even though it is continuous. In fact |x2 −y2| = |x −y||x + y|. So if we are
given an ϵ > 0, no matter how small we make δ if we take x > y > ϵ/δ, then when
|x −y| = δ/2 we have |x2 −y2| = (δ/2)(x + y) > (δ/2)(2ϵ/δ) = ϵ.
If E ⊆R, f : E →R is called a Lipschitz9 function if there is a positive constant
M such that | f (x) −f (y)| ≤M|x −y| for all x and y in E. As we progress in this
book, we’ll see many examples of Lipschitz functions, including one below. First,
however, we establish one of their fundamental properties, a property that is easy
to prove.
1.7.14. Proposition. A Lipschitz function is uniformly continuous.
Proof. If ϵ > 0, let 0 < δ < ϵ/M and the deﬁnition of uniform continuity is
satisﬁed.
■
Recall the deﬁnition of the distance from a point to a set E, dist (x, E), given in
§1.6.
1.7.15. Proposition. If E ⊆R and x, y ∈R, then
|dist (x, E) −dist (y, E)| ≤|x −y|
Consequently the function f : R →R deﬁned by f (x) = dist (x, E) is a Lipschitz
function.
Proof. If e ∈E, then |x −e| ≤|x −y| + |y −e|; so taking the inﬁmum over all
e in E we get dist (x, E) ≤inf{|x −y| + |e −y| : e ∈E} = |x −y| + dist (y, E).
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
9 Rudolf Lipschitz was born in Königsberg, Germany (now Kaliningrad, Russia) in 1832 to well-to-do parents.
(As you read these biographical notes, you might note that most of the mathematicians discussed here came
from professional families. Long ago higher education was not readily available to the children of those who
did physical labor.) He began his studies at the University of Königsberg but moved to Berlin. With a year oﬀ
for health reasons he received his doctorate from Berlin in 1853. After four years teaching in a gymnasium he
became a privatdozent at the University of Berlin. In 1862 he became an extraordinary professor at Breslau.
During this time he married a woman who had lived near him in Königsberg. Then in 1864 he left Breslau
for Bonn where he spent the rest of his distinguished career, making substantial contributions in a variety of
ﬁelds including number theory, Fourier series, diﬀerential equations, mechanics, and potential theory. He died
in Bonn in 1903.

42
The Real Numbers
Reversing the roles of x and y we have dist (y, E) ≤|x −y| + dist (x, E), whence
we get the desired inequality.
■
Here is the main result on uniformly continuous functions.
1.7.16. Theorem. If X is a bounded closed subset of R and f : X →R is contin-
uous, then f is uniformly continuous.
Proof. Assume f is not uniformly continuous. So there is an ϵ > 0 such that for
any δ > 0 there are points x, y in [a, b] with |x −y| < δ but | f (x) −f (y)| ≥ϵ.
Letting δ = 1
n we obtain sequences {xn} and {yn} in X such that |xn −yn| < 1
n and
| f (xn) −f (yn)| ≥ϵ. Since X is bounded there is a subsequence {xnk} and a point
x in X such that xnk →x. Because |xnk −ynk| < n−1
k , ynk →x. Since f is continu-
ous, f (xnk ) →f (x) and f (ynk ) →f (x). This gives a contradiction to the fact that
| f (xnk ) −f (ynk )| ≥ϵ for all integers nk.
■
Before presenting the ﬁnal result of this important section, we start with a dis-
cussion. Suppose we are given a closed and bounded interval [a, b] and a closed
subset X of [a, b]. What is the nature of the set [a, b]\X? Such a set in mathematics
is called a relatively open subset of [a, b]. In other words, it wants to be open in
relation to [a, b]. In fact Exercise 1.6.4 implies that (a, b)\X is open. So the only
way that [a, b]\X fails to be an open subset of [a, b] is if a /∈X or b /∈X or both.
Remember this when we prove the next result.
Now suppose a < c < d < b and we have a continuous function f : [c, d] →R.
We want to get an extension of f to a continuous function ˜f : [a, b] →R. That
is we want ˜f to be a continuous function on [a, b] such that ˜f (x) = f (x) when
x ∈[c, d]. There is a simple way to do this. Let α and β be any real numbers and
let ˜f be the function deﬁned on [a, b] whose graph looks as follows: (i) between a
and c the graph of ˜f is the straight line connecting the points (a, α) and (c, f (c));
(ii) between c and d it has the same graph as f ; and (iii) between d and b the graph is
the straight line between (d, f (d)) and (b, β). (The interested reader can write down
the equations that deﬁne ˜f . In fact, in the proof of the theorem below we’ll need
this.) The choice of α and β was arbitrary, but if we want ˜f to have an additional
property enjoyed by f , a restriction on the choice of α and β must be made. For
example, if m ≤f (x) ≤M when x ∈[c, d] and we choose n ≤α, β ≤M, then we
will have m ≤˜f (x) ≤M for all x in [a, b]. In particular, if f is a positive function,
we can make ˜f positive by stipulating that α, β ≥0. The next result generalizes this
discussion and the proof uses the method just described.
1.7.17. Theorem. If X is a closed subset of [a, b] and f : X →R is a continuous
function with m ≤f (x) ≤M for all x in X, then there is a continuous function
˜f : [a, b] →R that is an extension of f such that m ≤˜f (x) ≤M for all x in [a, b].
Proof. Consider G = [a, b]\X. For the moment assume that a, b ∈X so that
G = (a, b)\X is open (Exercise 1.6.4). By Proposition 1.6.7, G = ∞
n=1(an, bn),
where the intervals {(an, bn)} are pairwise disjoint. Deﬁne ˜f (x) to be f (x) when
x ∈X; and, when x ∈(an, bn), deﬁne it so that the portion of its graph lying above

1.7 Continuous Functions
43
this interval is the straight line connecting the points (an, f (an)) and (bn, f (bn))
as discussed prior to the statement of this theorem. It is clear that ˜f is an extension
of f and m ≤˜f (x) ≤M for all x in [a, b]. It remains to show that it is continu-
ous. Clearly ˜f is continuous at the points in G. Assume x ∈X and that {xk} is a
sequence in [a, b] that converges to x; we want to show that ˜f (xk) →˜f (x) = f (x).
The sequence {xk} can be partitioned into two subsequences: one that consists of
the points that belong to X and the other the points that belong to G. In the ﬁrst case
we have that ˜f of that subsequence converges to f (x) since ˜f is an extension of f .
Therefore the proof will be ﬁnished if we assume {xk} is a sequence entirely lying
in G. Thus for every k ≥1 there is a integer nk such that (ank, bnk ) contains xk.
Claim. [ f (bn) −f (an)] →0 and f (bnk ) →f (x).
By Theorem 1.7.16, f is uniformly continuous. So if ϵ > 0 there is a δ > 0 such
that | f (z) −f (y)| < ϵ when z, y ∈X and |z −y| < δ. Since G is a subset of
the bounded interval [a, b], G is bounded and so ∞
n=1(bn −an) < ∞; hence
(bn −an) →0 and there is an integer N with bn −an < δ/2 for n ≥N. (The
reason for using δ/2 rather than δ will become clear a little later.) It follows that
| f (bn) −f (an)| < ϵ when n ≥N, establishing the ﬁrst part of the claim. Now
when nk ≥N, since ank < xk < bnk it follows that bnk −xk < δ/2. Since xk →x
and nk →∞, there is an integer K such that |xk −x| < δ/2 and nk ≥N when
k ≥K. Therefore when k ≥K, |bnk −x| ≤|bnk −xk| + |xk −x| < δ. Therefore
| f (x) −f (bnk )| < ϵ when k ≥K, establishing the second part of the claim.
From the deﬁnition of ˜f (xk) we have that
˜f (xk) = f (ank ) + f (bnk ) −f (ank )
bnk −ank
(xk −ank )
Keeping in mind that xk −ank < bnk −ank, we have that
˜f (xk) −f (x) = f (ank ) + [ f (bnk ) −f (ank )] xk −ank
bnk −ank
−f (x)
< f (ank ) + [ f (bnk ) −f (ank )] −f (x)
= f (bnk ) −f (x) →0
by the claim. Therefore ˜f is continuous at x.
What happens when one or both of a and b do not belong to X? For example,
assume a /∈X and put c = inf X. Since X is closed, c ∈X; by deﬁnition [a, c) ⊆G.
If in addition b ∈X, then G = [a, c) ∪∞
n=1(an, bn), where the intervals {(an, bn)}
are pairwise disjoint. If neither a nor b belong to X, then there is a number d
such that G = [a, c) ∪(d, b] ∪∞
n=1(an, bn). If a ∈X but b /∈X, then there is a
number d such that G = (d, b] ∪∞
n=1(an, bn). In any of these three cases the
proof proceeds as in the case that both endpoints are in X. The details are left to the
reader.
■
The preceding theorem can be generalized and is called the Tietze Extension
Theorem, something the reader will see if (s)he continues the study of mathematics.

44
The Real Numbers
A cursory examination of the proof above shows that it uses properties particular to
R and any generalization necessitates a completely diﬀerent proof.
Exercises
(1)
Using Proposition 1.7.2, perform an induction argument to show that every
polynomial is a continuous function on R.
(2)
If f is a continuous function deﬁned on [a, b] and | f (x)| = 1 for all x in [a, b], show
that f is a constant function.
(3)
Let f : R →R be deﬁned by f (x) = 0 when x is irrational and f (a/b) = 1/b when
a ∈Z, b ∈N, and a and b have no common divisor except 1. Where is f continuous?
(4)
Use the IVT to show that if f : [0, 1] →[0, 1] is a continuous function, then it has
a ﬁxed point. That is, there is a point x with f (x) = x. Equivalently there is a point
x with x −f (x) = 0.
(5)
If f : [0, 1] →R is a continuous function such that f (0) < 0 and f (1) > 1, show
there is a number c in [0, 1] such that f (c) = c3.
(6)
If f , gare two functions deﬁned on a set X and taking values in R, show that (−f ) ∨
(−g) = −[ f ∧g]. Similarly (−f ) ∧(−g) = −[ f ∨g].
(7)
Prove Lemma 1.7.10.
(8)
Complete the proof of Proposition 1.7.11.
(9)
Let f : [a, b] →R be an increasing function and assume that f has the intermediate
value property. That is, assume that if f (a) ≤c ≤f (b), then there is a number x0
in [a, b] such that f (x0) = c. Prove that f is continuous.
(10)
If f : R →R is continuous and f (x) ∈Q for every x in R, show that f is constant.
(11)
Give an example of a bounded function that is continuous on (0, 1) but not uniformly
continuous.
(12)
Let a > 0 and show that f (x) = (x + 1)−1 is uniformly continuous on [a, ∞).
(13)
Let X1, . . . , Xn be pairwise disjoint closed and bounded subsets of R. If f : X →
R and f is uniformly continuous on Xk for 1 ≤k ≤n, show that f is uniformly
continuous on X.
(14)
For a function f : (a, b) →R and for each c in (a, b) and δ > 0, deﬁne ω(c, δ) =
sup{| f (x) −f (c)| : |x −c| < δ}. (a) Show that f is continuous at c if and only if
limδ→0 ω(c, δ) = 0. (b) Show that f is uniformly continuous if and only if
lim
δ→0[sup{ω(c, δ) : c ∈I}] = 0.
(15)
Is the composition of two uniformly continuous functions a uniformly continuous
function?
(16)
If f and g are uniformly continuous functions on X, are f ∨g and f ∧g uniformly
continuous?
(17)
If E ⊆R and f : E →R is uniformly continuous, show that when {an} is a Cauchy
sequence in E it follows that { f (an)} is a Cauchy sequence in R. Show that the

1.8 Trigonometric Functions
45
A
θ
B
Figure 1.8.1
function x →x2 on R maps Cauchy sequences into Cauchy sequences even though
it is not uniformly continuous (1.7.13).
(18)
In reference to the preceding exercise, show that if f is only assumed to be continu-
ous it does not follow that { f (an)} is a Cauchy sequence in R when {an} is a Cauchy
sequence in E.
(19)
If f : X →R, show that f is uniformly continuous if and only if whenever {xn} and
{yn} are sequences in X such that xn →x ∈X an yn →y ∈X we have that | f (xn) −
f (yn)| →0.
(20)
If E is a closed subset of R, f : E →R is a continuous function, and {an} is a
sequence in E, is f (lim sup an) = lim sup f (an)? (If the answer is yes, prove it; if
the answer is no, give a counterexample.) (b) If the answer in (a) was no, can you
add a condition on f that makes the statement true?
1.8. Trigonometric Functions
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
We are going to assume that the reader knows the deﬁnition of all the trig functions
as well as the various trig identities. What we want to concentrate on here is showing
that they are continuous. We start with the sine function and we show it is continuous
at θ = 0. (Note: all angles in this book are measured in radians. They are the natural
measurement of angles and are the only measurement we can use for calculus.)
Consider the circle of radius 1 centered at the origin; this is referred to as the
unit circle. Let θ > 0 and draw a line starting at the origin O and making an angle
having θ radians with the positive x-axis. The point B where this line meets the unit
circle has coordinates B = (cos θ, sin θ). Let A = (1, 0) ∈R2. (See Figure 1.8.1.)
Note that the length of the base of the triangle AOB is |OA| = 1, while its height is
sin θ. We get that Area AOB = 1
2 sin θ. On the other hand we know that the area of

46
The Real Numbers
the sector of the unit circle subtended by the arc of length θ is 1
2θ. (This is where we
need to have θ in radians.) Since AOB is contained in this sector, we get that 0 <
sin θ < θ when θ > 0. On the other hand we also know that sin(−θ) = −sin θ. So
when θ > 0, | sin(−θ)| = sin θ < θ. Thus we arrive at the fact that | sin θ| < |θ|
when θ ̸= 0. So if θn →0, we have that sin θn →0. This proves half the following.
1.8.1. Lemma. The functions sin and cos are continuous at 0.
Proof. We have the statement involving the sine function. Now for θ close to 0 we
know that cos θ =

1 −sin2 θ. So if θn →0, cos θn →1 = cos 0.
■
1.8.2. Proposition. The functions sin and cos are continuous everywhere.
Proof. If a ∈R, then we know that sin(θ + a) = sin θ cos a + cos θ sin a. From
the lemma we see that as θn →0, sin(θn + a) →sin a, so that the sine function is
continuous at a. The proof for the cosine function is similar.
■
Now using Proposition 1.7.2 we can discover where the other trig functions are
continuous. For example, tan θ = sin θ/ cos θ is continuous everywhere cos θ ̸= 0.
That is, tan θ is continuous everywhere except the points {π
2 ± nπ : n ∈N ∪{0}}.
The next example is not only revealing but will be important as we progress.
1.8.3. Example. If a ∈R, let f : R →R be deﬁned by
f (x) =
	
sin 1
x
if x ̸= 0
a
if x = 0
It follows that no matter how a is chosen, f is not continuous at 0 but it is con-
tinuous at each point x ̸= 0. In fact the continuity away from x = 0 follows by
Proposition 1.7.4. On the other hand if an = (2πn)−1, then an →0, but f (an) = 0
for all n ≥1; if bn = (π/2 + 2πn)−1, then bn →0, but f (bn) = 1 for all n ≥1.
Therefore no matter how we deﬁne f (0) it cannot be that f is continuous at 0.
Exercises
(1)
Prove that {sin n} has a convergent subsequence.
(2)
Where is the function x →tan x continuous?
(3)
Where is the function x →tan x −sec x continuous?

2
Differentiation
2.1. Limits
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In this section we want to introduce and explore the concept of a limit of a function.
We require the reader to consult Exercise 1.6.11 for the concepts of an isolated point
and a limit point of a set. These will be used in the next deﬁnition.
2.1.1. Definition. If X ⊆R, a is a limit point of X, and f : X →R, say that the
limit of f on X at a is the number L provided that for every ϵ > 0 there is a δ >
0 such that | f (x) −L| < ϵ whenever x ∈X and 0 < |a −x| < δ. In symbols we
denote this by
lim
x→a f (x) = L
or
f (x) →L as x →a
If a is an isolated point of X, then we deﬁne limx→a f (x) = f (a).
There are a few additional things worth emphasizing. First, it is not necessary
that a belong to the set X if it is a limit point; the restriction that a is a limit point,
however, insures that there are points in X that are arbitrarily close to a. In particular
we do not require that the function f be deﬁned at a. Second, look at the part of the
deﬁnition “x ∈X.” It may well be that there are other points, not in X, where f is
deﬁned; these do not inﬂuence whether f (x) →L as x →a with x in X. Third, look
at the part of the deﬁnition that states that | f (x) −L| < ϵ holds when “0 < |x −
a| < δ.” Even if a ∈X so that f (a) is deﬁned, we do not insist that | f (a) −L| < ϵ.
Now consider the case that a is an isolated point of X. (For example it might be
that a = 1 and X = [−1, 0] ∪{1}.) In this case there are suﬃciently small values
of δ such that there are no points x in X satisfying 0 < |x −a| < δ. Technically or
logically, if we used the ﬁrst part of this deﬁnition, we would have the sorry state
of aﬀairs that no matter what we choose for L the conclusion | f (x) −L| < ϵ holds
whenever x ∈X and 0 < |a −x| < δ. In other words the value of limx→a f (x) could
be anything. Of course this is intolerable, so we make a separate deﬁnition of the
limit of f (x) for isolated points. Finally note that the case when a is a limit point
or an isolated point can be covered with the statement that a ∈cl X, though the
deﬁnition of the limit still diﬀers from one case to the other.

48
Differentiation
As we progress we won’t make an issue of the comments in the last paragraph,
though the reader should keep them in mind. In fact we see that these issues do not
appear in the notation used to denote the limit. There are certain situations, however,
where we want to emphasize how the variable x approaches a. When X is an interval
and a is one of the endpoints of the interval, we introduce special notation for the
limit. (Note that in this case a is not an isolated point of X.) For example if a is the
left-hand endpoint of the interval X we will use the notation
lim
x→a+ f (x) = L = f (a+)
This is called the right-hand limit of f as x approaches a. If b < a < c and f :
(b, c) →R we might also discuss limx→a+ f (x), where we apply the original def-
inition to the restriction of f to the set X = (a, c). Similarly if a is the right-hand
endpoint of an interval X we use
lim
x→a−f (x) = L = f (a−)
This is called the left-hand limit of f as x approaches a. Again when b < a < c and
f : (b, c) →R we might also discuss limx→a−f (x), where we apply the original
deﬁnition to the restriction of f to the set X = (b, a).
Finally note that in the deﬁnition above both a and L are numbers. Later in this
section we will explore the notion of a limit being equal to ±∞as well as taking a
limit as x →±∞. The reader will note a similarity between the proof of this next
proposition and that of Theorem 1.7.5.
2.1.2. Proposition.
If X ⊆R, a is a limit point of X, and f : X →R, then
limx→a f (x) = L if and only if for every sequence {an} in X\{a} that converges to
a, f (an) →L.
Proof. Suppose limx→a f (x) = L and let {an} be a sequence in X\{a} that converges
to a. If ϵ > 0, let δ > 0 such that | f (x) −L| < ϵ when 0 < |x −a| < δ. By the
deﬁnition of convergence there is an N such that 0 < |an −a| < δ when n ≥N.
It follows that | f (an) −L| < ϵ when n ≥N, and so f (an) →L. Now assume that
f (an) →L whenever {an} is a sequence in X\{a} that converges to a, and let ϵ > 0.
Suppose no δ > 0 can be found to satisfy the deﬁnition. Thus for every n ≥1 there
is a point an in X such that 0 < |an −a| < 1
n but | f (an) −L| ≥ϵ. It follows that
{an} is a sequence in X\{a} that converges to a, but { f (an)} does not converge to L,
thus furnishing a contradiction.
■
2.1.3. Corollary. A function f : X →R is continuous at a point a in X if and only
if limx→a f (x) = f (a).
The next result follows from the fact that limx→c f (x) = L if and only if
limx→c+ f (x) = L = limx→c−f (x). (Verify!)
2.1.4. Proposition. If a < c < b and f : (a, b) →R, then f is continuous at c if
and only if f (c−) and f (c+) exist and are equal to f (c).

2.1 Limits
49
The next result is a consequence of Proposition 1.3.6 applied to Proposition 2.1.2.
2.1.5. Proposition.
Suppose X ⊆R, a ∈cl X, f : X →R, and g : X →R. If
limx→a f (x) = L and limx→a g(x) = K, then the following hold.
(a) limx→a[ f (x) + g(x)] = L + K.
(b) limx→a[ f (x)g(x)] = LK.
(c) If g(x) ̸= 0 for all x in X and K ̸= 0, then limx→a[ f (x)/g(x)] = L/K.
We return to Proposition 2.1.4 above to make a closer examination of how a func-
tion can be discontinuous at a point.
2.1.6. Definition. If a < c < b and f : (a, b) →R, then f has a simple dis-
continuity or a jump discontinuity at c provided limx→c+ f (x) = f (c+) and
limx→c−f (x) = f (c−) exist but f (c+) ̸= f (c−).
2.1.7. Example.
(a) If f (x) = −2 when x ≤0 and f (x) = 1 when x > 0, then
f : R →R has a simple discontinuity at 0.
(b) If f : [−1, 1] →R is deﬁned by f (x) = sin(x−1) when x ̸= 0 and f (0) = 0,
then the discontinuity of f at 0 is not simple. See Example 1.8.3.
If X ⊆R, a function f : X →R is increasing if f (x) ≤f (y) whenever x, y ∈X
and x ≤y. The reader should look once again at the deﬁnition of an increasing
sequence in §1.3 where there is a discussion of diﬀerent terminology. The same dis-
cussion applies here. When discussing functions we will also use terms like strictly
increasing, decreasing, and strictly decreasing; the deﬁnitions should be apparent
from their use with sequences. The function is monotonic if it is either increasing or
decreasing and strictly monotonic if the monotonicity is strict. Note that f : X →R
is increasing if and only if −f is decreasing. So any result established for an increas-
ing function has a companion result that holds for decreasing functions.
2.1.8. Proposition. If f : (a, b) →R is a bounded increasing function, then f (x−)
and f (x+) exist for every x in (a, b). Moreover
sup
y<x
f (y) = f (x−) ≤f (x) ≤f (x+) = inf
y>x f (y)
and every discontinuity of f is simple. In addition f (a+) and f (b−) exist with
f (a+) = inf{ f (y) : a < y} and f (b−) = sup{ f (y) : y < b}
Proof. Fix x in (a, b) and put L = supy<x f (y). If ϵ > 0 let y0 < x such that L −
ϵ < f (y0) ≤L. Because f is increasing we have that L −ϵ < f (y0) ≤f (y) ≤L
whenever y0 ≤y ≤x. Letting δ = x −y0 shows that when x −δ < y < x we have
y0 < y < x and so L −ϵ < f (y) ≤L. Thus f (x−) exists and equals L. Also note
that since f is increasing, f (x) is an upper bound for { f (y) : y < x} so that f (x−) ≤
f (x). The proof of the statement for f (x+) is similar and left to the reader as are
the statements for f (a+) and f (b−).

50
Differentiation
Finally, the only way that f can fail to be continuous at x is for the limit
limy→x f (y) not to exist. By what we have just shown this means that f (x−) <
f (x+) and so there is a jump discontinuity at x.
■
Of course there is an analogue of the preceding proposition for decreasing func-
tions. See Exercise 3. Note that in light of this proposition when f is a bounded
monotonic function on (a, b) we can extend f to a function f : [a, b] →R by let-
ting f (a) = f (a+), f (b) = f (b−). The extended function remains monotonic, and,
moreover, is continuous at the endpoints of [a, b].
2.1.9. Proposition. If f : (a, b) →R is a monotonic function, then the number of
discontinuities of f is countable.
Proof. Assume f is increasing and for the moment assume f is bounded. As
remarked just before the statement of this proposition, we can assume that f :
[a, b] →R is increasing and continuous at the endpoints. Let Dn = {x ∈(a, b) :
f (x+) −f (x−) ≥1
n}. Assume that Dn has an inﬁnite number of points; thus
there is a sequence {xk} of distinct points in Dn. If we think geometrically, each
f (xk+) −f (xk−) measures the vertical “gap” in the graph of f at the discontinuity
xk. The sum of all the gaps at the points xk must be less than the gap f (b) −f (a).
That is, ∞
k=1[ f (xk+) −f (xk−)] ≤f (b) −f (a) < ∞. But by the deﬁnition of
Dn, ∞
k=1[ f (xk+) −f (xk−)] = ∞, a contradiction. So Dn is ﬁnite. By Proposi-
tion 1.5.7, D = ∞
n=1 Dn is countable; by the preceding proposition D is the set
of discontinuities of f . When f is not bounded, consider the restriction of f to
[a + 1
n, b −1
n]; this restriction is bounded. (See Exercise 6.) If En denotes the set
of discontinuities of f inside this closed interval, then En is countable by the ﬁrst
part of the proof. Since ∞
n=1 En is the set of discontinuities of f on (a, b), again
Proposition 1.5.7 implies that the set of discontinuities of f is countable.
■
2.1.10. Definition. If X ⊆R, a is a limit point of X, and f : X →R, say that the
limit of f on X at a is ∞provided that for every positive number P there is a δ > 0
such that f (x) > P whenever x ∈X and 0 < |a −x| < δ. In symbols we denote this
by
lim
x→a f (x) = ∞
An alternate phrasing of the same thing is to say f (x) →∞as x →a. Say that
lim
x→a f (x) = −∞
if for every negative number N there is a δ > 0 such that f (x) < N whenever x ∈X
and 0 < |a −x| < δ.
We leave it to the reader to formulate the deﬁnition of limx→a+ f (x) = ∞.
Also we are only going to state results for the case that f (x) →∞and leave the
statements for the case that f (x) →−∞to the reader. The analogue for Proposi-
tion 2.1.2 seems suﬃciently clear that we leave it as Exercise 5. The only result we

2.1 Limits
51
pay close attention to here is the analogue of Proposition 2.1.5 as it contains a few
dangerous curves.
2.1.11. Proposition.
Suppose X ⊆R, a ∈cl X, f : X →R, and g : X →R. If
limx→a f (x) = ∞and limx→a g(x) = K where 0 < K ≤∞, then the following
hold.
(a) limx→a[ f (x) + g(x)] = ∞.
(b) limx→a[ f (x)g(x)] = ∞.
(c) If g(x) ̸= 0 for all x in X and K ̸= ∞, then limx→a[ f (x)/g(x)] = ∞.
The proof is Exercise 7. If in the preceding proposition we were to allow K to
take on negative values then all hell can break loose. See Exercise 8.
Recall that a subset X of R is not bounded above if for every positive number P
there is an x in X with x > P.
2.1.12. Definition. If X ⊆R such that X is not bounded above, f : X →R, and
L ∈R, say that the limit of f on X as x approaches ∞is L provided that for every
ϵ > 0 there is a positive number P such that | f (x) −L| < ϵ when x ∈X and x > P.
In symbols we denote this by
lim
x→∞f (x) = L
An alternate phrasing of the same thing is to say f (x) →L as x →∞.
Needless to say there is a deﬁnition of the statement limx→−∞f (x) = L as well as
the statements limx→∞f (x) = ∞, limx→∞f (x) = −∞, etc. It doesn’t seem worth-
while to explicitly state all these.
Exercises
(1)
If b < a < c and f : (b, c) →R, show that limx→a f (x) = L if and only if
limx→a−f (x) = L = limx→a+ f (x).
(2)
Let b < a < c and f : (b, c) →R. (a) Show that limx→a+ f (x) = L if and only if for
every decreasing sequence {an} in (a, c) that converges to a, f (an) →L. (b) Show
that limx→a−f (x) = L if and only if for every increasing sequence {an} in (b, c) that
converges to a, f (an) →L.
(3)
State and prove the version of Proposition 2.1.8 for decreasing functions.
(4)
The following result is called the squeezing principle. Suppose f , g, h are functions
on X and a ∈cl X. Show that if g(x) ≤f (x) ≤h(x) for all x in X and limx→a g(x) =
limx→a h(x) = L, then limx→a f (x) = L.
(5)
State and prove a version of Proposition 2.1.2 for f (x) →∞as x →a.
(6)
Why in the last half of the proof of Proposition 2.1.9 can we assume that a + 1
n ≤
b −1
n?
(7)
Prove Proposition 2.1.11.

52
Differentiation
(8)
Find examples of functions f and g deﬁned on all of R such that the following
happen. (a) limx→0 f (x) = ∞and limx→0 g(x) = −∞, but
lim
x→0 f (x)g(x) = 0
(b) limx→0 f (x) = ∞and limx→0 g(x) = −∞, but
lim
x→0 f (x)g(x) = −∞
(c) limx→0 f (x) = ∞, limx→0 g(x) = −∞, but
lim
x→0 f (x)g(x)
does not exist.
(d) Is it possible that limx→0 f (x) = ∞and limx→0 g(x) = −∞, but
lim
x→0 f (x)g(x) = ∞
2.2. The Derivative
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
2.2.1. Definition. Suppose (a, b) is a given open interval, x ∈(a, b), and t0 > 0
such that (x −t0, x + t0) ⊆(a, b). Say that a function f : (a, b) →R is diﬀeren-
tiable at x if the function deﬁned on (x −t0, x + t0) by
t →f (x + t) −f (x)
t
has a ﬁnite limit as t →0. When this happens we call the value of this limit the
derivative of f at x and denote it by
2.2.2
f ′(x) = lim
t→0
f (x + t) −f (x)
t
Of course another notation for the derivative of f is
df
dx
There are equivalent ways to deﬁne these notions. Using the same set-up as in
the preceding deﬁnition but not making speciﬁc the restrictions to make everything
precise, we can say that f is diﬀerentiable at x if and only if
f ′(x) = lim
y→x
f (y) −f (x)
y −x
exists. As we just did, we will henceforth drop specifying the eligible values of y
where this quotient is deﬁned.
The reader can consult other books to see discussions of the right and left deriva-
tives of f at x. These are deﬁned just as above except that in (2.2.2) we take the

2.2 The Derivative
53
left-hand and right-hand limits. These have a use, especially when f is deﬁned on
a closed interval and we want to deﬁne the derivative at an endpoint as we do now.
2.2.3. Definition. If f : [a, b] →R we say that f is diﬀerentiable at a if
f ′(a) = lim
t→0+
f (x + t) −f (x)
t
exists. Similarly we can deﬁne f to be diﬀerentiable at b. Say that f is diﬀerentiable
on [a, b] or (a, b) if it is diﬀerentiable at every point of the interval.
In what follows we often state results about functions diﬀerentiable at a point of
[a, b], but we will only prove them for the behavior of the function at a point x in
the open interval (a, b). The proofs in the case where x = a or x = b are left to the
reader.
2.2.4. Proposition. If f : [a, b] →R, x ∈[a, b], and f is diﬀerentiable at x, then
f is continuous at x.
Proof. Assume that x ∈(a, b). (The case that x is an endpoint is similar.) Note that
lim
y→x | f (y) −f (x)| = lim
y→x

f (y) −f (x)
y −x
 |y −x|
= lim
y→x

f (y) −f (x)
y −x
 · lim
y→x |y −x|
= | f ′(x)| · 0 = 0
■
Now for the algebraic permanence of derivatives.
2.2.5. Proposition. If f and g are diﬀerentiable at a point x in [a, b], then the
following hold.
(a) f + g is diﬀerentiable at x and ( f + g)′(x) = f ′(x) + g′(x).
(b) fg is diﬀerentiable at x and ( fg)′(x) = f ′(x)g(x) + f (x)g′(x).
(c) When g(y) ̸= 0 for all y in [a, b], f /g is diﬀerentiable at x and

 f
g
′
(x) = f ′(x)g(x) −f (x)g′(x)
g(x)2
Proof. We assume x ∈(a, b). The proof of (a) is easy in light of previous results
on limits. To establish (b), put h = fg and note that h(x + t) −h(x) = [ f (x + t) −
f (x)]g(x + t) + f (x)[g(x + t) −g(x)]. Dividing by t gives
h(x + t) −h(x)
t
= f (x + t) −f (x)
t
g(x + t) + f (x)g(x + t) −g(x)
t
Now let t →0 and use the fact that g is continuous at x (2.2.4) to obtain part (b).

54
Differentiation
The proof of (c) proceeds in a similar way, but here we let h = f /g. After some
algebra we get
h(x + t) −h(x)
t
=
1
g(x + t)g(x)
·
 f (x + t) −f (x)
t
g(x) −f (x)g(x + t) −g(x)
t

Letting t →0 produces the result.
■
We might point out that (c) needs the proof given because we have to show that
h = f /g is diﬀerentiable. If we knew that h were diﬀerentiable, however, we could
easily ﬁnd the formula for h′ by setting f = gh, using (b), and solving for h′.
Recall from calculus the geometric signiﬁcance of f ′(c). If we plot the graph of f
near the point (c, f (c)), we see that for any x diﬀerent from a the quotient ( f (x) −
f (c))/(x −c) is the slope of the straight line Lx going through the points (c, f (c))
and (x, f (x)). If f has a derivative at c, then as x →c the line Lx approaches the
line tangent to the graph of f . Thus the quotient ( f (x) −f (c))/(x −c) approaches
the slope of the tangent line. Hence the equation of the line tangent to the graph at
the point (c, f (c)) is
y = f (c) + f ′(c)(x −c)
2.2.6. Example.
Here we sketch the details of showing that polynomials are
diﬀerentiable. Note that from this and part (c) of the preceding proposition we
obtain that the rational functions are diﬀerentiable everywhere the denominator
does not vanish. We begin with the easy calculation from the deﬁnition that for
any constant c, dc/dx = 0. Also if we are given the identity function, x →x, we
quickly get from the deﬁnition that dx/dx = 1. Now if n ≥2 and a ∈R, we have
that xn −an = (x −a)(xn−1 + axn−2 + · · · + an−1). Hence if f (x) = xn, forming
[ f (x) −f (a)]/(x −a) = (xn −an)/(x −a) and letting x →a we see that
f ′(a) = nan−1
Now let’s derive the formulas for diﬀerentiating the trig functions. To start we
need a lemma.
2.2.7. Lemma. (a) limθ→0 sin θ
θ
= 1.
(b) limθ→0 cos θ−1
θ
= 0.
Proof. (a) Since (sin(−θ))/(−θ) = (sin θ)/θ, we need only consider the limit as
θ →0+. To do this return to the proof of Lemma 1.8.1 and the triangle AOB
1
2 sin θ. Now extend the line
OB until it intersects the line perpendicular to the x-axis at the point A and denote
the point of intersection by C. Consider the right triangle AOC and note that
since the length of the segment OA is |OA| = 1, we have that |AC| = tan θ. Hence
Area AOC = 1
2 tan θ. Comparing the areas of AOB, the sector of the unit circle
in Figure 1.8.1. We showed there that Area AOB =

2.2 The Derivative
55
subtended by the arc AB, and the area on AOC we get that 1
2 sin θ < 1
2θ < 1
2 tan θ
or
sin θ < θ < sin θ
cos θ
Therefore
1 <
θ
sin θ <
1
cos θ
Now we know that cos θ is continuous so if we let θ →0+ we get the desired
conclusion. (See Exercise 2.1.4.)
(b) Note that
cos θ −1
θ
= [cos θ −1][cos θ + 1]
θ[cos θ + 1]
=
−sin2 θ
θ[cos θ + 1]
= θ

−sin2 θ
θ2

[cos θ + 1]
By part (a) this must converge to 0 as θ →0.
■
2.2.8. Proposition. (sin x)′ = cos x and (cos x)′ = −sin x.
Proof. Using the formula for the sine of the sum of two numbers we get
sin(x + θ) −sin x
θ
= sin x cos θ + sin θ cos x −sin x
θ
= sin xcos θ −1
θ
+ cos xsin θ
θ
By the preceding lemma this converges to cos x as θ →0. The proof that (cos x)′ =
−sin x is similar and left as Exercise 5.
■
Deriving the formulas for the derivatives of the remaining trig functions can now
proceed by using the preceding proposition with Proposition 2.2.5. For example,
since tan x = sin x/ cos x, we can use (2.2.5(c)) to determine that the tangent func-
tion is diﬀerentiable whenever cos x ̸= 0 and obtain that for such x we have that
(tan x)′ = sec2 x.
The next result is an equivalent formulation of diﬀerentiability that is often useful
in executing proofs. Its proof is easy.
2.2.9. Proposition. A function f : X →R is diﬀerentiable at a point x in X if and
only if there there is a function F : X →R and a number D such that limy→x F(y) =
0 and f (y) −f (x) = (y −x)[D + F(y)] for all y in X. When f is diﬀerentiable at
x, we have D = f ′(x).

56
Differentiation
Proof. We only prove half of this result. The proof of the other half is left as Exer-
cise 7. Assume f is diﬀerentiable at X, put D = f ′(x), and let F(y) = [ f (y) −
f (x]/(y −x) −D when y ̸= x and F(x) = 0. The condition is easily seen to be
satisﬁed.
■
We’ll see the usefulness of the preceding result in this next proof.
2.2.10. Theorem (Chain Rule). Let f : (a, b) →R be diﬀerentiable at a point x in
(a, b), f ((a, b)) ⊆(c, d), and let g : (c, d) →R be a function that is diﬀerentiable
at f (x). If h : (a, b) →R is deﬁned by h = g ◦f , then h is diﬀerentiable at x and
h′(x) = g′( f (x)) f ′(x)
Proof. Use the preceding proposition to write f (y) −f (x) = (y −x)[ f ′(x) +
F(y)] and g(ζ ) −g( f (x)) = (ζ −f (x))[g′( f (x)) + G(ζ )], where ζ ∈(c, d), F
and G are functions deﬁned on the appropriate sets, limy→x F(y) = 0, and
limζ→f (x) G(ζ ) = 0. Therefore
h(y) −h(x) = g( f (y)) −g( f (x))
= [ f (y) −f (x)][g′( f (x)) + G( f (y))]
= (y −x)[ f ′(x) + F(y)][g′( f (x)) + G( f (y))]
= (y −x)[g′( f (x)) f ′(x) + T (y)]
where
T (y) = f ′(x)G( f (y)) + F(y)g′( f (x)) + F(y)G( f (y))
We must show that T (y) →0 as y →x. Let’s consider what happens to each of
the summands in the deﬁnition of T (y) as y →x. We know that since f is diﬀer-
entiable at x, it is also continuous there. Hence f (y) →f (x) and so G( f (y)) →0.
Thus the ﬁrst summand converges to 0. The second summand converges to 0 since
F(y) →0. The third summand converges to 0 by combining the two things we have
just established: G( f (y)) →0 and F(y) →0. By Proposition 2.2.9 this proves the
Chain Rule.
■
2.2.11. Example. Before studying this example, the reader should look at Exam-
ple 1.8.3. Deﬁne f : R →R by
f (x) =
	
x sin 1
x
if x ̸= 0
0
if x = 0
We will show that f is a continuous function on R that is diﬀerentiable when
x ̸= 0, but it is not diﬀerentiable at x = 0. The fact that f is continuous when x ̸= 0
is clear, and continuity when x = 0 follows by observing that | f (x)| ≤|x|. The
diﬀerentiability of f when x ̸= 0 follows from the Chain Rule and a judicious use
of Proposition 2.2.5(b). The fact that f is not diﬀerentiable at x = 0 is seen by

2.3 The Sign of the Derivative
57
observing that for t ̸= 0,
f (t) −f (0)
t
= sin 1
t
and using Example 1.8.3.
Exercises
(1)
State and prove a version of Proposition 2.2.4 when f : [a, b] →R is diﬀerentiable
at a.
(2)
Show that if f : (a, b) →R and there are constants M and α > 1 such that | f (x) −
f (y)| ≤M|x −y|α for all x, y in [a, b], then f is a constant.
(3)
Let a < c < b and suppose that f : (a, c] →R and g : [c, b) →R are continuous
functions with f (c) = g(c) and such that f is diﬀerentiable on (a, c), g is diﬀeren-
tiable on (c, b). If we deﬁne h : (a, b) →R by
h(x) =
	
f (x)
if a < x ≤c
g(x)
if c ≤x < b
give a necessary and suﬃcient condition that h is diﬀerentiable at x = c.
(4)
For each positive integer n, ﬁnd the points x in R where f (x) = xn−1|x| is diﬀeren-
tiable and compute its derivative.
(5)
Show that (cos x)′ = −sin x.
(6)
For which values of x is tan x diﬀerentiable and derive the formula for its derivative.
(7)
Prove the other half of Proposition 2.2.9.
(8)
Use Proposition 2.2.5(b) and induction to show that (xn)′ = nxn−1 (Example 2.2.6).
(9)
Find f ′(x) when f (x) = x3 sin(x + 1)2.
(10)
Are there any values of n in N such that f (x) = |x|xn is diﬀerentiable at x = 0?
(11)
For each of the following functions determine where it is diﬀerentiable and ﬁnd a
formula for its derivative. (a) cos3 x. (b) tan2(x + π). (c) (sin2 x)/(2x + 3).
(12)
Let f : R →R. The function f is an even function if f (−x) = f (x) for all x; f is an
odd function if f (−x) = −f (x) for all x. (a) Give two examples of an even function
and two examples of an odd function. (b) If f is a diﬀerentiable even function, show
that f ′ is an odd function. (c) If f is a diﬀerentiable odd function, what can you say
about f ′?
2.3. The Sign of the Derivative
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
2.3.1. Definition. If f : (a, b) →R and x ∈(a, b), we say that f has a local max-
imum at x if there is a δ > 0 with (x −δ, x + δ) ⊆(a, b) with f (x) ≥f (y) when
|x −y| < δ. Similarly we deﬁne what it means for f to have a local minimum at x.

58
Differentiation
We note that f has a local maximum at x if and only if −f has a local minimum
at x. Also a constant function has both a local maximum and minimum everywhere,
while f (x) = x has neither a local maximum nor a local minimum anywhere. Now
for a result that should be beloved and familiar to all calculus students.
2.3.2. Theorem. If f is a diﬀerentiable function on (a, b) and f has a local max-
imum or minimum at a point x, then f ′(x) = 0.
Proof. Assume that f has a local maximum at x. (If f has a local minimum at
x, we apply the argument that follows to −f .) Thus there is a δ > 0 such that (x −
δ, x + δ) ⊆(a, b) and f (y) ≤f (x) when |x −y| < δ. In particular when 0 < t < δ,
f (x + t) −f (x) ≤0. This implies that [ f (x + t) −f (x)]/t ≤0. Taking the limit
as t →0+ shows that f ′(x) ≤0. Now assume that −δ < t < 0. Again f (x + t) −
f (x) ≤0, but now [ f (x + t) −f (x)]/t ≥0. Taking the limit as t →0−shows that
f ′(x) ≥0. Therefore we have proved the theorem.
■
Be aware that an examination of the proof shows that in the statement of the
theorem it is required that the interval (a, b) is open. That is, if f is diﬀerentiable
on [a, b] and has a maximum at b it does not follow that f ′(b) = 0. A consideration
of f (t) = t on [0, 1] makes the point.
2.3.3. Example. The function f : R →R deﬁned by f (x) = x3 satisﬁes f ′(0) = 0,
but 0 is neither a local maximum nor a local minimum for f . Hence the converse
of the preceding theorem is not true.
2.3.4. Theorem (Mean Value Theorem). If f : [a, b] →R is continuous and f is
diﬀerentiable at each point of (a, b), then there is a point x in (a, b) with
f ′(x) = f (b) −f (a)
b −a
Proof. Deﬁne h : [a, b] →R by h(t) = [ f (b) −f (a)]t −(b −a) f (t). A little
algebra reveals that to prove the theorem it suﬃces to show there is a point x in
(a, b) with h′(x) = 0. Some more algebra reveals that h(a) = h(b). If h is constant,
h′(t) = 0 for every t in (a, b) and we have the result. Suppose there is some point
t in [a, b] with h(t) > h(a). Since h is continuous on [a, b], the EVT implies there
is a point x where h attains its maximum. Of necessity, x ∈(a, b). It must be that
this maximum for h is a local maximum; so h′(x) = 0 by Proposition 2.3.2. On the
other hand if it is the case that there is a point t in (a, b) with h(t) < h(a), then by
similar reasoning h attains its minimum value at a point x in (a, b). For this point x
we have that h′(x) = 0.
■
We’ll refer to the preceding theorem as the MVT.
2.3.5. Corollary. If f and g are continuous functions on [a, b] and they are diﬀer-
entiable on (a, b), then there is a point x in (a, b) with
[ f (b) −f (a)]g′(x) = [g(b) −g(a)] f ′(x)

2.3 The Sign of the Derivative
59
Proof. Apply the MVT to the function F(t) = [ f (b) −f (a)]g(t) −[g(b) −
g(a)] f (t).
■
This last corollary is sometimes called the Generalized Mean Value Theorem.
That it is more general than (2.3.6) can be see by letting g(t) = t in the corollary.
2.3.6. Corollary. If f : (a, b) →R is a diﬀerentiable function and f ′(x) = 0 for
all x in (a, b), then f is a constant function.
Proof. If a < x < y < b and f (x) ̸= f (y), the MVT implies there is a point c
between x and y with f ′(c) = [ f (y) −f (x)]/(y −x) ̸= 0, a contradiction.
■
The student is, undoubtedly, very familiar with the fact that the derivative mea-
sures the rate of change of the function. Indeed the very deﬁnition of the derivative
shows this. The next result formalizes such a discussion.
2.3.7. Proposition. If f is a diﬀerentiable function on (a, b), the following hold.
(a) f is an increasing function if and only if f ′(x) ≥0 for all x.
(b) If f ′(x) > 0 for all x, then f is strictly increasing.
(c) f is a decreasing function if and only if f ′(x) ≤0 for all x.
(d) If f ′(x) < 0 for all x, then f is strictly decreasing.
Proof. The proofs of (c) and (d) follow from their counterparts in (a) and (b) by
considering −f .
(a) Assume f is increasing. If x ∈(a, b), let δ > 0 be such that [x −δ, x + δ] ⊆
(a, b). If 0 < t < δ, then f (x + t) −f (t) ≥0, and so [ f (x + t) −f (x)]/t ≥0. Tak-
ing the limit shows that f ′(x) ≥0. Conversely assume that f ′(x) ≥0 for all x in
(a, b). If a < c < d < b, then the MVT implies there is an x in (c, d) such that
f (d) −f (c) = f ′(x)(d −c) ≥0, and we have that f is increasing.
(b) Assume f ′(x) > 0 for all x, and let a < c < d < b. Just as we did in the proof
of (a), there is a point x in (c, d) such that f (d) −f (c) = f ′(x)(d −c) > 0, and we
have that f is strictly increasing.
■
Note that the function f (x) = x3 shows that the converse of part (b) is not true.
See Exercise 2.
The next result is a cousin of the preceding one but is unrelated to derivatives. It
is presented here for the reader’s cultural ediﬁcation and its use in the further study
of diﬀerentiable functions.
2.3.8. Proposition.
If I is an interval and f is a continuous injective function
deﬁned on I, then f (I) is an interval and f is strictly monotonic.
Proof. The fact that f (I) is an interval is part of the IVT (1.7.7).
Observe that since f is injective, it suﬃces to show that it is monotonic. Assume
that f is not monotonic; so (Exercise 4) there are points a, x, b ∈I with a < x < b
such that either f (a) < f (x) and f (x) > f (b) or f (a) > f (x) and f (x) < f (b). As
usual we need only consider the ﬁrst of these since if the second occurs we can
consider −f .

60
Differentiation
Let’s compare f (a) and f (b): either f (a) < f (b) or f (a) > f (b). If it is the case
that f (a) < f (b), then we have that f (a) < f (b) < f (x); that is f (b) is in the inter-
val ( f (a), f (x)). By the IVT there is a point ζ in (a, x) with f (ζ ) = f (b), contra-
dicting the fact that f is injective. If it is the case that f (a) > f (b), then we have that
f (b) < f (a) < f (x); that is, f (a) is in the interval ( f (b), f (x)). Again an applica-
tion of the IVT yields a contradiction. These contradictions lead to the conclusion
that f must be monotonic.
■
To set the stage for the next topic, we return to the abstract setting of functions
deﬁned between sets that are not necessarily contained in R. If X and Y are sets
and f : X →Y is a bijective function, we can deﬁne its inverse f −1 : Y →X as
follows. If y ∈Y, then the fact that f is surjective implies there is an x in X such
that f (x) = y. Because f is also injective, the point x is the only such point in X.
Therefore we can deﬁne f −1(y) = x and we have a function f −1 : Y →X. We note
that f ( f −1(y)) = y for all y in Y, and f −1( f (x)) = x for all x in X.
Now to return to subsets of R. If I is an interval in R and f : I →R is a continu-
ous function that is also injective, then Proposition 2.3.8 above implies that J = f (I)
is also an interval and f is monotonic. Thus f −1 : J →I is also monotonic.
2.3.9. Proposition. If I is an interval, f : I →R is an injective continuous func-
tion, and J = f (I), then f −1 : J →R is a continuous function.
Proof. f is strictly monotonic by the preceding proposition. We only consider the
case where it is increasing. Suppose α ∈J and let a = f −1(α). First let’s assume
that a is not an endpoint of I; the case where it is an endpoint will be treated later.
There are points b and c in I such that b < a < c; put β = f (b), γ = f (c). Since f
is strictly increasing, β < α < γ . To show that f −1 is continuous at α we need to
show that if {αn} is a sequence in (β, γ ) that converges to α, then f −1(αn) →a. But
if an = f −1(αn), then {an} ⊆[b, c]. By the Bolzano–Weierstrass Theorem there is a
subsequence {ank} and a point a′ in [b, c] such that ank →a′. Since f is continuous,
αnk = f (ank ) →f (a′). But since {αnk} is a subsequence of the convergent sequence
{αn}, it must be that f (a′) = α; thus a′ = a. We have shown that every convergent
subsequence of {an} converges to a. By Exercise 1.3.9, an →a.
Now for the case that a is an endpoint of I; assume it is the left-hand endpoint.
Since f is increasing, α must be the left-hand endpoint of J. Choose a point c in I
with a < c. It follows that γ = f (c) ∈J and α < γ . The proof of this part follows
the lines of the argument used to prove the case where a was not an endpoint and is
left to the reader as Exercise 7.
■
2.3.10. Proposition. If f : (a, b) →(α, β) is a diﬀerentiable function that is bijec-
tive, then the function f −1 : (α, β) →(a, b) is diﬀerentiable and
( f −1)′(ζ ) =
1
f ′( f −1(ζ ))
for every ζ in (α, β).

2.3 The Sign of the Derivative
61
Proof. Fix ζ in (α, β). By Proposition 2.2.9 we want to ﬁnd a function G(ω) deﬁned
on a small interval about ζ such that G(ω) →0 as ω →ζ and such that
f −1(ω) −f −1(ζ ) = (ω −ζ ){[ f ′( f −1(ζ ))]−1 + G(ω)}
Put x = f −1(ζ ). By the diﬀerentiability of f at x we have the existence of a function
F(y) deﬁned in a small interval about x such that F(y) →0 as y →x and
f (y) −f (x) = (y −x)[ f ′(x) + F(y)]
If ω is given and y = f −1(ω), then
f −1(ω) −f −1(ζ ) = (y −x)
= f (y) −f (x)
f ′(x) + F(y
= [ f (y) −f (x)]

1
f ′(x) +

1
f ′(x) + F(y) −
1
f ′(x)

= (ω −ζ )

1
f ′( f −1(ζ )) + G(ω)

where
G(ω) =
1
f ′(x) + F( f −1(ω)) −
1
f ′(x)
Now by the preceding proposition we know that f −1 is a continuous function. Thus
as ω →ζ, f −1(ω) →f −1(ζ ) = x; hence F( f −1(ω)) →0. It follows that G(ω) →
0 as required.
■
We know several functions that are diﬀerentiable bijections and so we can apply
the preceding result. For example, sin : (−π
2 , π
2 ) →(−1, 1) is a diﬀerentiable bijec-
tion, and we can use (2.3.10) to ﬁnd the derivative of its inverse, arcsin : (−1, 1) →
(−π
2 , π
2 ). See Exercise 9.
Exercises
(1)
Give the details of the proof of Corollary 2.3.5.
(2)
Try to prove the converse of Proposition 2.3.7(b) by assuming f is strictly increasing
and starting the argument used to prove the ﬁrst part of (a) of that proposition. Where
does it break down?
(3)
Let f : [0, ∞) →R be a continuously diﬀerentiable function such that f (0) > 0
and there is a constantC with | f ′(x)| ≤C < 1 for all x. (a) Use the Mean Value The-
orem to show that f (x) ≤f (0) + Cx for all x ≥0. (b) Prove that limx→∞[ f (x) −
x] = −∞. (c) Show that there is a x0 > 0 such that f (x0) = x0. (d) Prove that there
is only one point x0 > 0 satisfying f (x0) = x0.
(4)
If I is an interval and f : I →R is injective, show that f is increasing if and only if
when a, b, x ∈I with a < x < b we have that f (a) < f (x) < f (b).

62
Differentiation
(5)
Let n be an integer with n > 2. If f (x) = xn + ax + b, show that there are at most
three numbers x with f (x) = 0.
(6)
Show that if I and J are intervals and f : I →J is a continuous bijection, then
f −1 : J →I is strictly monotonic. (See Proposition 2.3.8.)
(7)
Fill in the details of the proof of Proposition 2.3.9.
(8)
Why can’t you prove Proposition 2.3.10 by applying the Chain Rule to the fact that
f ( f −1(ζ )) = ζ for all ζ in (α, β)?
(9)
Find an appropriate domain on which the trig functions sin, cos, and tan are bijective
and use Proposition 2.3.10 to calculate the derivatives of their inverses.
2.4. Critical Points
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
2.4.1. Definition. If f is a diﬀerentiable function on (a, b), a critical point is a
point x in (a, b) with f ′(x) = 0.
Why is it called a critical point?
Because interesting things happen there. Of course if the derivative is zero on an
entire interval, then it is constant there; from one point of view there is not much
critical happening there, from another there’s a lot that’s going on. Suppose it is
zero at an isolated point; then three possible things might happen. The derivative
could change from being positive to negative; change from being negative to being
positive; or it could have the same sign on both sides of the place where it is zero. An
example of this last phenomenon is the function x3. In the ﬁrst two cases something
truly interesting happens. In fact recall Theorem 2.3.2 where it is shown that when
a function has a local maximum or minimum it has a critical point. We saw that
x3 shows the converse of this is false, but below we present the partial converse of
(2.3.2), something the reader has encountered in calculus.
2.4.2. Theorem. Suppose f : (a, b) →R is a diﬀerentiable function and x0 is a
critical point of f .
(a) If there is a δ > 0 such that a < x0 −δ < x0 + δ < b, f ′(x) > 0 when x0 −δ <
x < x0, and f ′(x) < 0 when x0 < x < x0 + δ, then f has a local maximum at x0.
(g) If there is a δ > 0 such that a < x0 −δ < x0 + δ < b, f ′(x) < 0 when x0 −δ <
x < x0, and f ′(x) > 0 when x0 < x < x0 + δ, then f has a local minimum at x0.
Proof. The proof is intuitively clear, but we must translate that intuition into precise
mathematics. We only prove (a). If x0 −δ < x < x0, then the MVT implies there
is a y with x < y < x0 and f (x0) −f (x) = f ′(y)(x0 −x). But the assumption on
f ′(y) when x0 −δ < y < x0 implies that f (x0) −f (x) > 0. If x0 < x < x0 + δ, the
MVT implies there is a z with x0 < z < x and f (x) −f (x0) = f ′(z)(x −x0). Since
f ′(z) < 0 by hypothesis and (x −x0) > 0 we have that f (x) −f (x0) < 0. Thus f
has a local maximum at x0.
■

2.4 Critical Points
63
There is a small drawback to the preceding theorem. Namely we assume that the
δ > 0 exists. There is, however, a simple extra condition on f that helps in this
process. To set the stage, realize that if f is a diﬀerentiable function on the open
interval (a, b), then the derivative deﬁnes another function, f ′ : (a, b) →R.
2.4.3. Definition. Say that f is a continuously diﬀerentiable function on (a, b) if it
is a diﬀerentiable function and f ′ : (a, b) →R is a continuous function. The collec-
tion of all continuously diﬀerentiable functions on (a, b) is denoted by C(1)(a, b) =
C′(a, b). Functions in C′(a, b) are also called smooth functions. Now f ′ can also be
diﬀerentiable, in which case we say that f is a twice diﬀerentiable function. The
second derivative is denoted by f ′′. If f ′′ : (a, b) →R is continuous, we say that
f is a twice continuously diﬀerentiable function; denote the collection of all twice
continuously diﬀerentiable functions on (a, b) by C(2)(a, b) = C′′(a, b). This con-
tinues with the deﬁnition of higher derivatives denoted by f (3), . . . , f (n), . . . If f (n)
deﬁnes a continuous function, we say that f is n-times continuously diﬀerentiable
and we denote the space of all such functions on (a, b) by C(n)(a, b). If we can form
all the derivatives f (n) for n ≥1, we say that f is inﬁnitely diﬀerentiable and denote
the space of all such functions as C(∞)(a, b). In some places in the literature the
term “smooth function” is reserved for functions in C(∞)(a, b), where we have used
the term for functions in C′(a, b).
See Exercise 1.
2.4.4. Example. (a) Recall Example 2.2.11 and deﬁne f2 : R →R by
f2(x) =
	
x2 sin 1
x
if x ̸= 0
0
if x = 0
Clearly f2 is diﬀerentiable at x ̸= 0. In fact, f ′
2(x) = 2x sin(x−1) −cos(x−1) when
x ̸= 0. Also note that
f2(t) −f2(0)
t
= t sin 1
t →0
as t →0, so that f2 is diﬀerentiable at x = 0 and f ′(0) = 0. However, using Exam-
ple 2.2.11 we have that f2 is not continuously diﬀerentiable on all of R though it
is on any open interval not containing 0. Since f ′
2 is not continuous at 0, f2 is not
twice diﬀerentiable.
(b) Deﬁne f3 : R →R by
f3(x) =
	
x3 sin 1
x
if x ̸= 0
0
if x = 0
It follows that f3 ∈C′(R) but it is not twice diﬀerentiable at x = 0. The reader is
asked to show this in Exercise 4 as well as explore additional functions.

64
Differentiation
We now present the second derivative test for a critical point.
2.4.5. Theorem. Suppose f ∈C′′(a, b) and f has a critical point at x0.
(a) If f ′′(x0) < 0, then f has a local maximum at x0.
(b) If f ′′(x0) > 0, then f has a local minimum at x0.
(c) If f ′′(x0) = 0, then the nature of the critical point at x0 cannot be determined.
Proof. (a) Since f ∈C′′(a, b), there is a δ > 0 such that (x0 −δ, x0 + δ) ⊆(a, b)
and f ′′(x) < 0 for |x −x0| < δ. Since f ′(x0) = 0, this means that we can apply
Theorem 2.4.2(a) and conclude that f has a local maximum at x0. The proof of
(b) is similar.
(c) Note that when f (x) = x3, f ′′(0) = 0, and f has nether a local maximum nor
a local minimum at 0. If f (x) = x4, f ′′(0) = 0, and f has a local minimum at x = 0;
if f (x) = −x4, f ′′(0) = 0, and f has a local maximum at x = 0.
■
The next curious result says that if the function is diﬀerentiable, then it has the
intermediate value property even if we do not assume that the derivative is a con-
tinuous function.
2.4.6.
Theorem
(Darboux’s1 Theorem). If
f : (a, b) →R
is
diﬀerentiable,
[c, d] ⊆(a, b), and y is a point between f ′(c) and f ′(d), then there is an x in
[c, d] with f ′(x) = y.
Proof. Without loss of generality we can assume that y ̸= f ′(c), f ′(d); for con-
venience assume that f ′(c) < y < f ′(d). If we deﬁne g : (a, b) →R by g(t) =
f (t) −yt, then g′(t) = f ′(t) −y and g′(d) > 0 > g′(c). Since g is continuous
there is a point x in [c, d] with g(x) = min{g(t) : t ∈[c, d]}. Now 0 > g′(c) =
limt→c+[g(t) −g(c)]/[t −c], and so there is a point t1 > c with g(c) > g(t) for
c < t < t1. Hence x ≥t1. Similarly there is a t2 < d with x ≤t2. This shows that
the point x is in (c, d). By Theorem 2.3.2, 0 = g′(x) = f ′(x) −y.
■
We conclude this section with a discussion of using what we have done to ﬁnd
the global maximum and minimum of a diﬀerentiable function f : [a, b] →R. By
the EVT we know these exist. To ﬁnd them we can ﬁnd the critical points of f in the
interval, compute the value f (x0) for each critical point x0, then compute the values
f (a) and f (b). Comparing all these values will then reveal the extreme values as
well as their location. See the exercises.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
1 Jean Gaston Darboux was born in 1842 in Nimes in the Languedoc region of France. His early education was in
Nimes and the nearby city of Montpellier. After this, in 1861, he entered the École Polytechnique and then the
École Normale Supérieure. While still a student he published his ﬁrst paper on orthogonal surfaces, a subject
that became his main research focus. These results were part of his doctoral dissertation, which he was awarded
in 1866. He had several academic positions until he was appointed as suppléant to Liouville at the Sarbonne
in 1873. He continued in this position for ﬁve years. In 1878 he became suppléant to Chasles in the chair of
higher geometry at the Sorbonne. Two years later Chasles died and Darboux was appointed as his successor. He
held this chair for the rest of his life with a term as dean of the Faculty of Science from 1889 to 1903. He made
many contributions to geometry and there is also the Darboux Integral. He was also an excellent teacher and
administrator. Among his doctoral students are Émile Borel, Elie Cartan, Édouard Goursat, and Émile Picard.

2.5 Some Applications
65
Exercises
(1)
Show that C(n)(a, b) is an algebra; that is, it is a vector space over R that also has
the property that the product of any two elements is again in the vector space and
all the usual distributive laws hold.
(2)
If f and g are functions on (a, b) such that each has derivatives there of order k for
1 ≤k ≤n, show that fg has a derivative of order n and
( fg)(n) =
n

k=0
Cn,k f (k)g(n−k)
where Cn,k = n!/(n −k)!k!
(3)
For each of the following functions f , decide whether x = 0 is a local maximum
or local minimum of f or neither. (a) f (x) = (1 + x2)/(1 + x3). (b) f (x) = (1 +
x3)/(1 + x2). (c) f (x) = x2 sin3 x + x2 cos x.
(4)
Deﬁne fn : R →R by
fn(x) =
	
xn sin 1
x
if x ̸= 0
0
if x = 0
(a) Show that f3 ∈C′(R), but f3 is not twice diﬀerentiable at x = 0. (b) Show that
f5 ∈C′′(R), but is not three-times diﬀerentiable at x = 0. (c) Show that f2n+1 ∈
C(n)(R), but it is not (n + 1)-times diﬀerentiable at x = 0. (Warning: This is a com-
plicated computation.)
(5)
Let f be aC′′ function on (a, b) and suppose there is a point x0 in (a, b) with f (x0) =
f ′(x0) = f ′′(x0) = 0. Show that there is a continuous function g on (a, b) with
f (x) = (x −x0)2g(x) for all x in (a, b).
(6)
Let f (x) = x3 −3x2 + x + 1. Find the critical points of f and diagnose their
nature.
(7)
Let f (x) = 3x5 −5x3 + 3. Find the critical points of f and diagnose their
nature.
(8)
Find the global maximum and minimum values of the function f (x) = x3 −3x on
the interval [−2, 2] and ﬁnd the point x where they occur.
(9)
Find the global maximum and minimum values of the function f (x) = 6x4/3 −
3x1/3 on the interval [−1, 1] and ﬁnd the point x where they occur.
2.5. Some Applications
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In this section we will use what we have learned about derivatives to establish two
results, L’Hôpital’s Rule and Taylor’s Theorem.

66
Differentiation
2.5.1. Theorem (L’Hôpital’s2 Rule). Assume a < c < b and f and g are diﬀer-
entiable functions on (a, b)\{c} such that g′(x) ̸= 0 for all x. If limx→c f (x) = 0 =
limx→c g(x) and there is an L in R such that
lim
x→c
f ′(x)
g′(x) = L
then
lim
x→c
f (x)
g(x) = L
Proof. We begin by showing that limx→c+[ f (x)/g(x)] = L. Let ϵ > 0 and choose
δ > 0 such that |[ f ′(z)/g′(z)] −L| < ϵ when c < z < c + δ. Corollary 2.3.5
implies that when c < y < x < c + δ, there is a z with y < z < x and f ′(z)/g′(z) =
[ f (x) −f (y)]/[g(x) −g(y)]. Hence when c < y < x < c + δ, we have that

f (x) −f (y)
g(x) −g(y) −L
 =

f ′(z)
g′(z) −L
 < ϵ
Now f (y) →0 and g(y) →0 as y →c+. So holding x ﬁxed in (c, c + δ) and letting
y →c+ in the above inequality, we get that |[ f (x)/g(x)] −L| ≤ϵ when c < x <
c + δ. Since ϵ was arbitrary, this shows that limx→c+[ f (x)/g(x)] = L. The proof
that limx→c−[ f (x)/g(x)] = L is similar and left to the reader.
■
There are variations on L’Hôpital’s Rule, which are also called by the same name.
Usually these involve having ±∞involved. Here is one.
2.5.2. Theorem (L’Hôpital’s Rule). Assume a < c < b and f and g are diﬀeren-
tiable functions on (a, b)\{c} such that g′(x) ̸= 0 for all x. If limx→c g(x) = ∞and
there is an L in R such that
lim
x→c
f ′(x)
g′(x) = L
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
2 Guillaume François Antoine Marquis de L’Hôpital was born in Paris in 1661. An earlier version of his family
name is l’Hospital and you will sometimes see this used today when his result is presented. His family was
prominent in France for centuries, and his father was a general in the King’s army. L’Hôpital’s mathematical
story is complicated; it is cloaked in obscurity, and, for present day eyes, strangeness. This is partly due to the
fact that he lived so long ago, a fact that means that many documents are forever lost and also that social behavior
was so diﬀerent from what it is today. From an early age L’Hôpital displayed mathematical ability, but he would
have probably never been known to calculus students except for his meeting Johann Bernoulli, who at the age
of 24 in 1691 had just arrived in Paris. Bernoulli was an expert in the diﬀerential calculus of Leibniz. (Realize
that Newton was just 18 years older than L’Hôpital and had published his Principia Mathematica in 1683; also
at the same time Leibniz was developing calculus in Paris.) L’Hôpital and Bernoulli became friends but later
had a falling out due to a priority dispute. In 1694 L’Hôpital wrote a letter to Bernoulli promising him money in
return for his working on problems of L’Hôpital’s choice. Needless to say he added the proviso that Bernoulli not
publish this work independently. There is no copy of Bernoulli’s reply, but apparently he accepted the proposal
and later letters record the exchange of ideas. (The practices of mathematicians as well as the rest of society
have changed since then. Others have written on this agreement with more authority than I can muster for this
footnote.) In 1696 L’Hôpital published the ﬁrst textbook on diﬀerential calculus; it contains L’Hôpital’s Rule.
This book was extremely inﬂuential with new editions published until 1781. L’Hôpital was a married man and
had four children. He died in Paris in 1704.

2.5 Some Applications
67
then
lim
x→c
f (x)
g(x) = L
Proof. The proof begins as in the proof of the preceding theorem. Let ϵ >
0 and choose δ > 0 such that |[ f ′(z)/g′(z)] −L| < ϵ/2 when c < z < c + δ. Corol-
lary 2.3.5 implies that when c < y < x < c + δ, there is a z with y < z < x
and f ′(z)/g′(z) = [ f (y) −f (x)]/[g(y) −g(x)]. Hence when c < y < x < c + δ, we
have that
2.5.3
L −ϵ
2 < f (y) −f (x)
g(y) −g(x) < L + ϵ
2
Fix x with c < x < c + δ. Since g(y) →∞as y →c, there is a δ1 > 0 with c +
δ1 < x such that when c < y < c + δ1 we have both g(y) > 0 and g(y) > g(x); note
that for such a y, [g(y) −g(x)]/g(y) > 0. So if we multiply the inequalities (2.5.3)
by this quotient we obtain the following two inequalities, separated for convenience
and valid for all y with c < y < c + δ1
2.5.4
f (y) −f (x)
g(y) −g(x)
g(y) −g(x)
g(y)

<

L + ϵ
2
 g(y) −g(x)
g(y)

and
2.5.5

L −ϵ
2
 g(y) −g(x)
g(y)

< f (y) −f (x)
g(y) −g(x)
g(y) −g(x)
g(y)

Doing some algebra (2.5.4) becomes
f (y) −f (x)
g(y)
<

L + ϵ
2
 g(y) −g(x)
g(y)

or
f (y)
g(y) <

L + ϵ
2

−

L + ϵ
2
 g(x)
g(y) + f (x)
g(y)
Remember that x is ﬁxed. Since g(y) →∞as y →c+, we can ﬁnd a δ2 with 0 <
δ2 < δ1 so that when c < y < c + δ2 we have that
−

L + ϵ
2
 g(x)
g(y) + f (x)
g(y)
can be made as small as we want. Thus we can ﬁnd such a δ2 > 0 so that for c <
y < c + δ2
f (y)
g(y) < L + ϵ
Now for (2.5.5). Doing similar algebra we arrive at the inequality

L −ϵ
2

−

L −ϵ
2
 g(x)
g(y) + f (x)
g(y) < f (y)
g(y)

68
Differentiation
We again use the hypothesis to ﬁnd a δ3 with 0 < δ3 < δ2 such that when c < y <
c + δ3 we have that
L −ϵ < f (y)
g(y)
Combining our inequalities we get that when c < y < c + δ3, |L −f (y)/g(y)| < ϵ.
This establishes that limy→c+ f (y)/g(y) = L. In similar fashion we get that
limy→c−f (y)/g(y) = L.
■
You should also see [11] where there are several variations on the theme of
L’Hôpital’s Rule as well as a number of exercises. As we progress we’ll use these
results, referred to by the name but possibly with small variations.
2.5.6. Theorem (Taylor’s3 Theorem). If n ≥1, f ∈Cn−1(a, b), f is n-times dif-
ferentiable at the point c in (a, b), and P(x) is the polynomial
P(x) =
n−1

k=0
f (k)(c)
k!
(x −c)k
then there is a function h : (a, b) →R such that h(x) →0 as x →c and
f (x) = P(x) + h(x)(x −c)n
Moreover for every x in (a, b) there is a point d between x and c such that
h(x) = f (n)(d)
n!
Proof. LetP(x) be the polynomial deﬁned in the statement of the theorem. If we set
h(x) =
	 f (x)−P(x)
(x−c)n
when x ̸= c
0
when x = c
then f (x) = P(x) + h(x)(x −c)n. Let’s show that limx→c h(x) = 0. This is accom-
plished by repeated use of L’Hôpital’s Rule. Note that applying the rule once
won’t suﬃce when n ≥2. In fact this becomes clear once we calculate that
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
3 Brook Taylor was born in 1685 in Edmonton, England to a ﬁnancially comfortable family. He was schooled at
home until he entered Cambridge. His ﬁrst paper of note was published in 1714 and was on mechanics using
Newton’s calculus. In 1721 he married, but his wife died in 1723 during childbirth; the baby also died. He
married again in 1725, but his second wife also died in childbirth, though the baby survived. Besides the present
theorem Taylor discovered integration by parts and invented the ﬁeld of ﬁnite diﬀerences. He made contributions
to a variety of sciences as well as the mathematical study of perspective in art. Unfortunately he became distracted
as he was virulently involved in the debate over whether the credit for inventing calculus belonged to Newton or
Leibniz. This involved nationalism to a degree that is surprising to a modern mathematician. He died in 1731
in London.

2.5 Some Applications
69
f (k)(a) = P(k)(a) for 0 ≤k < n. So L’Hôpital’s rule repeated yields
lim
x→c h(x) = lim
x→c
f (x) −P(x)
(x −c)n
= 1
n lim
x→c
[ f (x) −P(x)]′
(x −c)n−1
=
1
n(n −1) lim
x→c
[ f (x) −P(x)]′′
(x −c)n−2
...
= 1
n! lim
x→c
[ f (n−1)(x) −f (n−1)(c)]
(x −c)
= 0
Now ﬁx x in (a, b); we want to show there is a point d between x and c such that
h(x) = 1
n! f (n)(d). To do this introduce the function g(y) = f (y) −P(y) −M(y −
c)n, where
M = f (x) −P(x)
(x −c)n
= h(x)
From the deﬁnition of the polynomial P(y) we can compute that g(n)(y) = f (n)(y) −
n!M = f (n)(y) −n!h(x); so we want to produce a point d between x and c with
g(n)(d) = 0. We will do this by applying the MVT to the successive derivatives of
g(y).
To begin we use the fact that for 0 ≤k ≤n −1, P(k)(c) = f (k)(c) and we con-
clude that g(c) = g′(c) = · · · = g(n−1)(c) = 0. Now from the deﬁnition of M we get
that g(x) = 0. Thus the MVT applied to g shows there is a point y1 between x and
c with g′(y1) = 0. But g′(c) = 0; so applying the MVT to g′ shows there is a point
y2 between y1 and c with g′′(y2) = 0. Now apply the MVT to g′′ and so on. Eventu-
ally we show the existence of a point yn between yn−1 and c such that g(n)(yn) = 0.
Therefore if we let d = yn, then it lies between x and c and h(x) = f (n)(d)
n!
.
■
Again I can recommend [11] for more on Taylor’s Theorem.
Exercises
(1)
In Theorem 2.5.1 assume L = ∞and prove that the theorem remains valid. Can
you do the same for Theorem 2.5.2?
(2)
Can you state and prove a version of L’Hôpital’s Rule covering limits as x →∞?
(3)
Assume f , g : (a, b) →R are diﬀerentiable functions such that g′(x) ̸= 0 for all x
in (a, b). If limx→a+ g(x) = ∞= limx→a+ f (x) and there is an L in R ∪{∞} such
that limx→a+ f ′(x)/g′(x) = L, then limx→a+ f (x)/g(x) = L.

70
Differentiation
(4)
For each of the following speciﬁed functions ﬁnd the stated limit if it exists,
where existing includes the possibility that the limit is ±∞. (a) limx→0 sin x
x .
(b) limx→0
sin x−x+x3/6
x5
.
(5)
Let f be an inﬁnitely diﬀerentiable function on an open interval (a, b) and suppose
a < c < b. If n ∈N and f (k)(c) = 0 for 0 ≤k ≤n −1 and f (n)(c) ̸= 0, show that
there is an inﬁnitely diﬀerentiable function g : (a, b) →R such that f (x) = (x −
c)ng(x) and g(c) ̸= 0.

3
Integration
This chapter gives a mathematical foundation of the theory of integration the reader
began in calculus. Unlike when the student took calculus, there will not be as much
emphasis on computing integrals. Before the chapter concludes we’ll see some top-
ics never covered in calculus as well as establish a foundation for future embel-
lishments and extensions. Integration is a vast, powerful, and useful subject. The
development given in calculus, as well as the material on this subject contained in
this book, only introduces the subject.
3.1. The Riemann Integral
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Throughout this section we work with a closed and bounded interval, [a, b]. A par-
tition of [a, b] is a ﬁnite, ordered subset of the form P = {a = x0 < x1 < · · · <
xn = b}. (This is the usual notation for a partition where both the points and the
ordering are listed.) Say that a partition Q is a reﬁnement of a partition P if P ⊆Q.
Hence Q adds additional points to P, and each subinterval of [a, b] determined by
two consecutive elements of P is the union of one or more of the intervals deter-
mined by the elements of Q. For example P = {0 < .2 < .4 < .7 < 1} is a partition
of [0, 1]. Both {0 < .1 < .2 < .4 < .6 < .7 < 1} and {0 < .2 < .4 < .6 < .7 < 1}
are reﬁnements of P as is P itself. An observation that will be used often as we pro-
ceed is that if P and Q are two partitions of [a, b], the partition P ∪Q is a reﬁnement
of both P and Q.
If f : [a, b] →R is any bounded function and P = {a = x0 < x1 < · · · < xn =
b}, then for 1 ≤j ≤n deﬁne
M j = MP
j = sup{ f (x) : xj−1 ≤x ≤xj}
m j = mP
j = inf{ f (x) : xj−1 ≤x ≤xj}
and let
U( f , P) =
n

j=1
M j(xj −xj−1) and L( f , P) =
n

j=1
m j(xj −xj−1)
The term U( f , P) is called the upper sum of f for the partition P; L( f , P) is called
the lower sum. If you remember the way the integral was introduced in (many)

72
Integration
calculus courses, for a positive function f , L( f , P) is the sum of the areas of rectan-
gles placed below the graph of the function f while U( f , P) is the total of the areas
of the rectangles placed above the graph of f .
3.1.1. Example. (a) Assume f : [a, b] →R is a constant function: f (x) = c for
all x in [a, b]. For any partition P and with the notation as above, mj = c = M j for
each j. Thus for any partition P, L( f , P) = c(b −a) = U( f , P).
(b) Now deﬁne f : [a, b] →R by
f (x) =
	
1
if x is a rational number
0
if x is an irrational number
By the Density Property (1.2.1), for any partition P = {a = x0 < x1 < · · · <
xn = b} and 1 ≤j ≤n, mj = 0 and M j = 1. This implies that L( f , P) = 0 and
U( f , P) = b −a.
Of course we’ll see many other examples, but not until we develop a little more
of the theory. Before we start we might call attention to Exercise 1, which can often
be used to derive a result for the lower sum from a result for the upper sum.
3.1.2. Proposition. If f is a bounded function on [a, b] with m ≤f (x) ≤M for all
x in [a, b] and P and Q are partitions of [a, b], then the following hold.
(a) m(b −a) ≤L( f , P) ≤U( f , Q) ≤M(b −a).
(b) If Q is a reﬁnement of P, then
L( f , P) ≤L( f , Q) ≤U( f , Q) ≤U( f , P).
(c) If Q is a reﬁnement of P, then
0 ≤U( f , Q) −L( f , Q) ≤U( f , P) −L( f , P).
Proof. (b) Let P = {a = x0 < x1 < · · · < xn = b} and, as usual, deﬁne mj =
inf{ f (x) : xj−1 ≤x ≤xj} for 1 ≤j ≤n. Let’s consider the case that Q is obtained
by adding one point to P; in fact assume that Q = {a = x0 < x∗< x1 < · · · < xn =
b}. (The case where x∗is between two other points is similar and only involves
more complicated notation.) Let m′
1 = inf{ f (x) : x0 ≤x ≤x∗}, m′′
1 = inf{ f (x) :
x∗≤x ≤x1}. Note that m1 = min{m′
1, m′′
1}. Thus m1(x1 −x0) ≤m′
1(x∗−x0) +
m′′
1(x1 −x∗) and so L( f , P) ≤L( f , Q). The fact that this inequality holds when
Q is obtained by adding any number of points is an argument using induction; the
details are left to the reader. The proof that U( f , Q) ≤U( f , P) is analogous and
left to the reader as Exercise 2.
(c) This is immediate from (b) and is stated only for convenience.
(a) For the moment consider just one partition P. Using the usual deﬁnitions of
mj and M j for 1 ≤j ≤n, we have that m ≤mj ≤M j ≤M. From here we easily
get that m(b −a) ≤L( f , P) ≤U( f , P) ≤M(b −a). Now assume we have two
partitions, P and Q. Note that P ∪Q is a reﬁnement of both P and Q. Applying

3.1 The Riemann Integral
73
(b)
twice
we
get
that
m(b −a) ≤L( f , P) ≤L( f , P ∪Q) ≤U( f , P ∪Q) ≤
U( f , Q) ≤M(b −a).
■
3.1.3. Definition. Using the preceding proposition we have that
sup{L( f , P) : P is a partition of [a, b]} ≤
inf{U( f , Q) : Q is a partition of [a, b]}
When these two expressions are equal, we have a desirable event and we set
 b
a
f =
 b
a
f (x)dx
= sup{L( f , P) : P is a partition of [a, b]
= inf{U( f , Q) : Q is a partition of [a, b]}
In this situation we say that f is Riemann1integrable or simply integrable. The set
of all Riemann integrable functions on [a, b] is denoted by R[a, b].
A brief word about notation. Ordinarily we will use the notation
 b
a f rather than
 b
a f (x) dx as seen in calculus. The use of the notation involving dx will be limited
to those occasions where there might be some confusion as to the variable of inte-
gration. This will be our practice partly because the x is redundant, but mainly to
emphasize that we are integrating a function. The notation for a function is f , while
f (x) is the value of the function at the point x in its domain.
Note that the function f appearing in Example 3.1.1(b) is not integrable, while
the constant functions are. The next result gives a necessary and suﬃcient condition
for integrability that is more convenient than the deﬁnition and that will usually be
employed when we prove results about integrability.
3.1.4. Proposition. If f is a bounded function on [a, b], then f is Riemann inte-
grable if and only if for every ϵ > 0 there is a partition P of [a, b] such that
U( f , P) −L( f , P) < ϵ. Moreover
 b
a f is the unique number such that for every
reﬁnement Q of P we have
L( f , Q) ≤
 b
a
f ≤U( f , Q)
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
1 Georg Friedrich Bernhard Riemann was born in 1826 in Breselenz, Germany. His early schooling was closely
supervised by his father. When he entered the university at Göttingen in 1846, at his father’s urging he began
to study theology. Later, with his father’s blessing, he switched to the Faculty of Philosophy so he could study
mathematics. In 1847 he transferred to Berlin where he came under the inﬂuence of Dirichlet, which inﬂuence
was permanent. He returned to Göttingen in 1849 where he completed his doctorate in 1851, working under
the direction of Gauss. He took up a lecturer position there and in 1862 he married a friend of his sister. In
the autumn of that same year he contracted tuberculosis. This began a period of ill health and he went between
Göttingen and Italy, where he sought to recapture his health. He died in 1866 in Selasca, Italy on the shores of
beautiful Lake Maggiore. Riemann is one of the giants of analysis and geometry. He made a series of signiﬁcant
discoveries and initiated theories. Besides this integral there are many other things named after him, including
the Riemann zeta function, Riemann surfaces, the Riemann Mapping Theorem.

74
Integration
Proof. Deﬁne
L = sup{L( f , P) : P is a partition of [a, b]}
U = inf{U( f , Q) : Q is a partition of [a, b]}
We always have that L ≤U. Suppose that for every ϵ > 0 we can ﬁnd a partition
P with U( f , P) −L( f , P) < ϵ. But U −L ≤U( f , P) −L( f , P) < ϵ. Since ϵ was
arbitrary we have that f ∈R[a, b]. Conversely, assume f is Riemann integrable
and let ϵ > 0. Choose partitions P1, P2 such that 0 ≤L −L( f , P1) < ϵ/2 and 0 ≤
U( f , P2) −U < ϵ/2. If we put P = P1 ∪P2, then Proposition 3.1.2 implies that
U( f , P) −L( f , P) < ϵ. If Q is a reﬁnement of P, then Proposition 3.1.2(c) implies
that U( f , Q) −L( f , Q) ≤U( f , P) −L( f , P) < ϵ. Since ϵ was arbitrary we have
that there can be only one number between L( f , Q) and U( f , Q) for every such
reﬁnement. By deﬁnition, this unique number must be
 b
a f .
■
The uniqueness part of the last proposition is there mainly for emphasis, since
when a function is integrable there can be no other number between all the lower
and upper sums. There is, however, some small beneﬁt in using only partitions Q
that are reﬁnements of P as seen in the proof of Proposition 3.1.6 below.
3.1.5. Corollary. If f is a bounded function on [a, b], then f is Riemann integrable
if and only if there is a sequence of partitions {Pk} of [a, b] such that each Pk+1 is a
reﬁnement of Pk and U( f , Pk) −L( f , Pk) →0 as k →∞. When this happens we
have that
 b
a
f = lim
k→∞U( f , Pk) = lim
k→∞L( f , Pk)
Proof. If such a sequence of partitions exists, then it is immediate from the propo-
sition that f ∈R[a, b]. Now assume that f is integrable. By the proposition for
each k ≥1 there is a partition Qk with U( f , Qk) −L( f , Qk) < 1/k. If Pk = Q1 ∪
· · · ∪Qk, then Pk is a reﬁnement of Qk and so Proposition 3.1.2(c) implies that
U( f , Pk) −L( f , Pk) < 1/k. The evaluation of the integral as the limit of the upper
and lower sums follows from the deﬁnition of the integral.
■
3.1.6. Proposition. If f : [a, b] →R is a bounded function and {a = x0 < x1 <
· · · < xn = b} is a partition such that f is Riemann integrable on [xj−1, xj] for 1 ≤
j ≤n, then f is Riemann integrable on [a, b] and
 b
a
f =
n

j=1
 x j
x j−1
f
Proof. Let ϵ > 0 and let Pj be a partition of [xj−1, xj] such that U( f , Pj) −
L( f , Pj) < ϵ/n. If P = n
j=1 Pj, then P is a partition of [a, b] and
U( f , P) −L( f , P) =
n

j=1
[U( f , Pj) −L( f , Pj)] < ϵ
Thus f ∈R[a, b].

3.1 The Riemann Integral
75
To ﬁnish the proof let Q be any reﬁnement of P and set Q ∩[xj−1, xj] = Qj.
It follows that Q = n
j=1 Q j and each Qj is a reﬁnement of Pj. We have that
L( f , Qj) ≤
 x j
x j−1 f ≤U( f , Qj) for 1 ≤j ≤n. Thus
L( f , Q) =
n

j=1
L( f , Qj) ≤
n

j=1
 x j
x j−1
f
≤
n

j=1
U( f , Qj) = U( f , Q)
By the uniqueness part of Proposition 3.1.4 it follows that
 b
a
f =
n

j=1
 x j
x j−1
f
■
Now we see a useful suﬃcient condition for integrability.
3.1.7. Theorem. If f : [a, b] →R is a bounded function that is continuous at all
but a ﬁnite number of points, then f is Riemann integrable.
Proof. Suppose −M ≤f (x) ≤M for all x in [a, b]. We begin with a special case.
3.1.8. Claim. If f is continuous at every point, then f is Riemann integrable.
Let ϵ > 0. Recall that Theorem 1.7.16 says that f : [a, b] →R is uniformly
continuous. Hence there is a δ > 0 such that |x −y| < δ implies that | f (x) −
f (y)| < ϵ/(b −a). Let P = {a = x0 < x1 < · · · < xn = b} such that xj −xj−1 < δ
for 1 ≤j ≤n. Since |x −y| < δ when xj−1 ≤x, y ≤xj, it follows that M j −mj <
ϵ/(b −a). (Why?) Therefore
U( f , P) −L( f , P) =
n

j=1
(M j −mj)(xj −xj−1) <
ϵ
b −a
n

j=1
(xj −xj−1) = ϵ
and so f ∈R[a, b] by the Proposition 3.1.4.
3.1.9. Claim. If f has a single point of discontinuity in [a, b], then f ∈R[a, b].
We will assume the point of discontinuity is at the left-hand endpoint, a. (The
proof of the case where the discontinuity is at b is practically identical. The proof
of the case when the discontinuity occurs at an interior point is Exercise 3.) Let
ϵ > 0 and choose the point x1 with x1 −a < ϵ/4M. Since f is continuous on [x1, b]
there is a partition of this interval, P1 = {x1 < x2 < · · · < xn = b}, withU( f , P1) −
L( f , P1) < ϵ/2. If P = {a = x0 < x1 < x2 < · · · < xn = b}, then
U( f , P) −L( f , P) = (M1 −m1)(x1 −a) + U( f , P1) −L( f , P1)
< 2M ϵ
4M + ϵ
2
= ϵ
By (3.1.4) f ∈R[a, b].

76
Integration
Now assume that f is continuous on [a, b] except for the points c1 < c2 <
· · · < cm. Pick points a = x0 < x1 < · · · < xm = b in [a, b] such that [a, b] =
m
k=1[xk−1, xk] and each subinterval [xk−1, xk] contains ck but cj /∈[xk−1, xk] when
j ̸= k. Hence Claim 3.1.9 implies f ∈R[xk−1, xk] for 1 ≤k ≤m. By Proposition
3.1.6, f is Riemann integrable on [a, b].
■
With the last theorem we have a large supply of integrable functions. In §3.5
below a necessary and suﬃcient condition for Riemann integrability is presented,
but the preceding theorem covers the cases we will see in the rest of this book. Now
it is time to develop some of the properties of an integrable function.
3.1.10. Proposition. If f , g ∈R[a, b], then the following hold.
(a) If f (x) ≤g(x) for all x in [a, b], then
 b
a
f ≤
 b
a
g.
(b) If | f (x)| ≤M for all x in [a, b], then

 b
a
f
 ≤M(b −a).
Proof. (a) From the deﬁnition of the upper sum we have that for any partition P,
U( f , P) ≤U(g, P). The result follows from Corollary 3.1.5.
(b) Since −M ≤f (x) ≤M for all x in [a, b], part (b) follows from part (a).
■
3.1.11. Proposition. R[a, b] is a vector space over R. Moreover if f , g ∈R[a, b]
and α, β ∈R, then
 b
a
(α f + βg) = α
 b
a
f + β
 b
a
g
Proof. If
E ⊆[a, b],
then
note
that
sup{ f (x) + g(x) : x ∈E} ≤sup{ f (x) :
x ∈E} + sup{g(x) : x ∈E} and inf{ f (x) + g(x) : x ∈E} ≥inf{ f (x) : x ∈E} +
inf{g(x) : x ∈E}. Thus for any partition P of [a, b] we have that
U( f + g, P) −L( f + g, P) ≤[U( f , P) + U(g, P)] −[L( f , P) + L(g, P)]
= [U( f , P) −L( f , P) + [U(g, P) −L(g, P)]
It follows from Proposition 3.1.4 that if f , g ∈R[a, b], then f + g ∈R[a, b]. Now
use Corollary 3.1.5 to ﬁnd a sequence of partitions {Pk} such that each Pk+1 is a
reﬁnement of Pk and simultaneouslyU( f + g, Pk) −L( f + g, Pk) →0,U( f , Pk) −
L( f , Pk) →0,
and
U(g, Pk) −L(g, Pk) →0.
(Supply
the
details
needed.)

3.1 The Riemann Integral
77
From Corollary 3.1.5 (see Exercise 8(a)) we have that
 b
a
( f + g) = lim
k U(( f + g), Pk)
= lim
k [U( f , Pk) + U(g, Pk)]
=
 b
a
f +
 b
a
g
The rest of the proof is Exercise 8(b).
■
Finally we introduce what might be considered as a deﬁnition or a convention. If
a < b and f ∈R[a, b], then
3.1.12
 a
b
f = −
 b
a
f
Let’s also add the agreement that
 a
a
f = 0
In some ways this is unnecessary and follows from the deﬁnition of the integral. On
the other hand we have, at least implicitly, deﬁned everything to do with the integral
under the assumption that a < b.
Exercises
(1)
If f is a bounded function on [a, b] and P is any partition of [a, b], show that
L( f , P) = −U(−f , P).
(2)
Prove that U( f , Q) ≤U( f , P) when Q is a reﬁnement of P. (Proposition 3.1.2(b).)
(3)
Prove (3.1.9) when the discontinuity occurs at an interior point of [a, b]. (Hint: Use
Proposition 3.1.6.)
(4)
If f : [a, b] →R is a monotonic function, show that f is integrable.
(5)
If f ∈R[a, b] and g : [a, b] →R is such that {x ∈[a, b] : f (x) ̸= g(x)} is ﬁnite,
show that g ∈R[a, b] and
 b
a g =
 b
a g.
(6)
Let {a0, a1, . . . } be an increasing sequence in [a, b] such that a0 = a and an →b.
If f is a bounded function on [a, b] that is integrable on [an, an+1] for all n ≥0, is
f integrable on [a, b]?
(7)
Show that if f : [a, b] →R is a continuous function, f (x) ≥0 for all x, and
 b
a f =
0, then f (x) = 0 for all x in [a, b].
(8)
(a) Supply the missing details in the proof of Proposition 3.1.11 needed to show
that
 b
a ( f + g) =
 b
a f +
 b
a g. (b) Show that when f ∈R[a, b] and α ∈R, α f ∈
R[a, b] and
 b
a (α f ) = α
 b
a f .

78
Integration
(9)
(a) Show that if f ∈R[a, b], then f 2 ∈R[a, b]. (b) Use the identity ( f + g)2 −
( f −g)2 = 4 fg and part (a) to show that if f , g ∈R[a, b], then fg ∈R[a, b].
(10)
Using (3.1.12) show that if a, b, c ∈R and f is integrable on some interval including
these three numbers, then
 b
a f =
 c
a f +
 b
c f .
3.2. The Fundamental Theorem of Calculus
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
There are several versions of what people call the Fundamental Theorem of Calcu-
lus. Here is the most general.
3.2.1. Theorem (Fundamental Theorem of Calculus). If f is a bounded Riemann
integrable function on [a, b] and F : [a, b] →R is deﬁned by
F(x) =
 x
a
f (t)dt
then F is a continuous function. If f is continuous at a point c in [a, b], then F is
diﬀerentiable at c and F′(c) = f (c).
Proof. If | f (x)| ≤M for all x in [a, b], then for a ≤x ≤y ≤b,
|F(y) −F(x)| =

 y
a
f −
 x
a
f

=

 y
x
f
 (3.1.6)
≤M|y −x| (3.1.10)(b)
So not only is f continuous, it is a Lipschitz function.
Now assume f is continuous at c, which we assume is an interior point of the
interval. If ϵ > 0 there is a δ > 0 such that | f (x) −f (c)| < ϵ when |x −c| < δ.
Thus when 0 < |t| < δ,

F(c + t) −F(c)
t
−f (c)
 =


1
t
 c+t
c
f

−f (c)

=

1
t
 c+t
c
[ f (x) −f (c)]dx

≤1
|t|ϵ|t| (3.1.10)(b)
≤ϵ
By deﬁnition F′(c) exists and equals f (c). When c is one of the endpoints of the
interval the proof is similar and left to the reader (Exercise 1).
■
Because of its frequent use in the rest of the book, we refer to this result as well as
its corollary below as the FTC. The function F : [a, b] →R is called the indeﬁnite

3.2 The Fundamental Theorem of Calculus
79
integral of f . It is also sometimes referred to as the anti-derivative of f , though
that is a moniker that comes from the following consequence of the preceding
theorem.
3.2.2. Corollary. If f : [a, b] →R is a continuous function and F is its indeﬁ-
nite integral, then F is a continuously diﬀerentiable function on [a, b] and F′ = f .
Equivalently, if F : [a, b] →R is a continuously diﬀerentiable function and F′ =
f , then
 b
a f = F(b) −F(a).
Proof. The ﬁrst statement in the corollary is immediate from the theorem. For the
second we note that if G is the indeﬁnite integral of f = F′, then G′ = F′, so G −F
is the constant function. Thus
 b
a f = G(b) −G(a) = F(b) −F(a).
■
We can use the FTC to calculate several integrals.
3.2.3. Example.
(a) If n ∈N, then
 b
a xn = (n + 1)−1(bn+1 −an+1). In fact,
(xn+1)′ = (n + 1)xn (2.2.6), so this is immediate from the FTC.
(b)
 b
a sin x = −(cos b −cos a) and
 b
a cos x = sin b −sin a.
(c) To ﬁnd F(x) =
 x3
a sin y dy, note that F(x) = G(φ(x)), where G(y) =
 y
a sin x
and φ(x) = x3. So the Chain Rule and the FTC imply that F′(x) = G′(φ(x))φ′(x) =
(sin x3)(3x2) = 3x2 sin x3.
We now apply the FTC to obtain other important results.
3.2.4. Theorem (Mean Value Theorem). If f : [a, b] →R is a continuous func-
tion, then there is a point c in [a, b] such that
 b
a
f = f (c)(b −a)
Proof. Deﬁne F : [a, b] →R as the indeﬁnite integral of f . By the MVT for deriva-
tives (Theorem 2.3.4) there is a point c in [a, b] with (b −a)F′(c) = F(b) −F(a) =
 b
a f . By the Fundamental Theorem this ﬁnishes the proof.
■
3.2.5. Theorem (Change of Variable Theorem). Suppose φ : [a, b] →R is a con-
tinuously diﬀerentiable function and I is an interval such that φ([a, b]) ⊆I. If
f : I →R is a continuous function, then
 φ(b)
φ(a)
f (x) dx =
 b
a
f (φ(t))φ′(t) dt
Proof. Deﬁne F : I →R as the indeﬁnite integral of f :
F(u) =
 u
φ(a)
f (x) dx

80
Integration
By the FTC, F′(u) = f (u). Hence the Chain Rule implies (F ◦φ)′(t) =
F′(φ(t))φ′(t) = f (φ(t))φ′(t). Again applying the FTC we have that
 b
a
f (φ(t))φ′(t) dt =
 b
a
(F ◦φ)′(t) dt
= (F ◦φ)(b) −(F ◦φ)(a)
= F(φ(b)) −F(φ(a))
=
 φ(b)
φ(a)
f (x) dx
■
The preceding result will be referred to as the COV Theorem, which, of course,
is used in integration by substitution.
3.2.6. Example.
Consider the integral
 1
0 x2 cos(2 −x3) dx. If φ(t) = 2 −t3
for 0 ≤t ≤1, then φ maps [0, 1] onto [1, 2] (Why?). So the COV The-
orem implies
 1
0 x2 cos(2 −x3) dx = −1
3
 1
0 cos(φ(t))φ′(t) dt −1
3
 1
2 cos x dx =
−1
3[sin(1) −sin(2)] = 1
3[sin(2) −sin(1)]. This example emphasizes the impor-
tance of correctly placing the limits of integration φ(0) and φ(1) and using (3.1.12).
3.2.7. Theorem (Integration by Parts). If f and g are two continuously diﬀeren-
tiable functions on [a, b], then
 b
a
fg′ = f (b)g(b) −f (a)g(a) −
 b
a
f ′g
Proof. This is one of the easiest proofs of a result labeled as a theorem. Just apply
the FTC to obtain that f (b)g(b) −f (a)g(a) =
 b
a ( fg)′ =
 b
a f ′g +
 b
a fg′.
■
In calculus this is presented as one of the many techniques of integration. It is
much more than that and is a basic tool of analysis.
Exercises
(1)
Prove the Fundamental Theorem of Calculus when the point c is an endpoint of
[a, b].
(2)
Find F′(x) where F(x) =
 cos x
0
sin y dy.
(3)
Evaluate the following. (a)
 1
0 x2(6x3 + 5)
1
4 . (b)
 π
0 cos3 x sin x.
(4)
If f is a continuously diﬀerentiable function on R such that f (0) = 0 and 1 ≤
f ′(x) ≤2 for all x in R, show that x ≤f (x) ≤2x on R.
(5)
Compute the following integrals. (a)
 π/2
0
x sin x. (b)
 2
1
√x−1
√x . (c)
 π
0 x cos x2.
(6)
For any continuous function f : [−1, 1] →R, show that
 1
−1
x f (x2) = 0.

3.3 The Logarithm and Exponential Functions
81
(7)
If f is inﬁnitely diﬀerentiable on [a, b] and n ≥1, show that
f (b) =
n

k=0
f (k)(a)
k!
(b −a)k + 1
n!
 b
a
f (n+1)(x)(b −x)n dx.
(Hint: Use integration by parts.)
3.3. The Logarithm and Exponential Functions
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In this section we introduce the logarithm of a positive number, study the properties
of the associated function, and explore its inverse, the exponential function. The
logarithm we study is often called the natural logarithm so as to distinguish it from
the logarithm to base 10, which will be introduced later.
3.3.1. Definition. If x > 0, deﬁne
log x =
 x
1
1
t dt
This is called the logarithm of x and the function log : (0, ∞) →R is also called
the logarithm function or sometimes the log function.
We note that many people use the notation ln x instead of log x, which they reserve
for the logarithm to base 10. This seems more understandable when these notions
are used in an algebra course. The only use I know for the logarithm to base 10 is in
performing calculations, a virtue that seems outdated since the advent of computers.
The natural logarithm is truly natural, as its deﬁnition shows, while the others are
somewhat artiﬁcial. Hence we use log x as the natural logarithm.
3.3.2. Theorem. The logarithm function has the following properties.
(a) The logarithm function is diﬀerentiable and (log x)′ = 1/x.
(b) log 1 = 0, log x > 0 when x > 1, and log x < 0 when 0 < x < 1.
(c) If a, b > 0, then log ab = log a + log b and log(a/b) = log a −log b.
(d)
The logarithm function is a strictly increasing function that is a bijection
between the interval (0, ∞) and the entire real line R.
Proof. (a) This follows immediately from the FTC.
(b) It’s clear that log 1 = 0 and when x > 1 the integral deﬁning log x integrates
a strictly positive function. When 0 < x < 1, log x =
 x
1 t−1 dt = −
 1
x t−1 dt < 0
(3.1.12).
(c) Fix a in (0, ∞) and consider the function f (x) = log(ax) −log a −log x.
An easy computation (using the Chain Rule) shows that f ′(x) = 0 for 0 < x < ∞.
Hence f is a constant function; so for all x, f (x) = f (1) = 0. Therefore 0 = f (b),
yielding the ﬁrst formula. Using this we have that 0 = log 1 = log bb−1 = log b +
log b−1; so we have that log b−1 = −log b. Now evaluate f (b−1) to obtain the sec-
ond formula in (c).

82
Integration
(d) By (a), (log x)′ > 0 so it is strictly increasing and hence injective (2.3.8). Using
(c) we have log 2n = n log 2; since log 2 > log 1 = 0, we have that log 2n →∞as
n →∞. Therefore if y > 0, choose n suﬃciently large that log 2n > y. Now con-
sider the log function on the interval [1, 2n]. We have that log 1 < y < log 2n. There-
fore the IVT implies that there is an x in this interval with log x = y. Similarly if
y < 0, there is a point x in the interval (0, 1) with log x = y. Thus the logarithm is
bijective.
■
Since the log function is a bijection from (0, ∞) onto all of R, it has an inverse
function from R onto (0, ∞).
3.3.3. Definition. The inverse of the logarithm function is called the exponential
function and is denoted by exp : R →(0, ∞).
3.3.4. Theorem. The exponential function has the following properties.
(a) The exponential function is continuously diﬀerentiable and (exp x)′ = exp x.
(b) exp 0 = 1, exp x > 1 when x > 0, and 0 < exp x < 1 when x < 0.
(c) If a, b ∈R, exp(a + b) = (exp a)(exp b) and exp(−a) = (exp a)−1.
(d) The exponential function is a strictly increasing function that maps R bijectively
onto (0, ∞).
(e) We have that
lim
x→∞exp x = ∞
and
lim
x→−∞exp x = 0.
Proof. As a glance at Theorem 3.3.2 reveals, the parts (a) through (d) in this the-
orem parallel the corresponding ones of that result and are direct consequences
of the fact that the exponential function is the inverse of the log function. The
details are left to the reader in Exercise 1. Establishing (e) proceeds as follows.
Since the exponential is strictly increasing and maps onto (0, ∞), we must have
that limx→∞exp x = ∞. Similarly we obtain the other half of (e).
■
3.3.5. Definition. If x > 0 and a ∈R deﬁne
xa = exp(a log x)
First note that this deﬁnition makes sense since x > 0.
From here we can get the usual laws of exponents by using the previous two
theorems. The details are left to the reader in Exercise 4.
3.3.6. Proposition. If x, y > 0 and a, b ∈R, the following hold.
(a) xaxb = x(a+b).
(b) xaya = (xy)a.
(c) (xa)b = x(ab).
(d) x(a−b) = xa/xb.
The number x where log x = 1 is denoted by e. We want to emphasize the
following:
ea = exp a

3.3 The Logarithm and Exponential Functions
83
So we frequently will write the exponential function as ex as does the rest of the
mathematical world. The next deﬁnition is possible because both the logarithm and
exponential functions are bijective.
3.3.7. Definition. If b, x > 0, we deﬁne y = logb(x) to mean that x = by.
The proof of the next result is left to the reader (Exercise 5).
3.3.8. Proposition. If b, x, y > 0, the following hold.
(a) logb(xy) = logb x + logb y.
(b) logb(x/y) = logb x −logb y.
(c) For any a in R, logb(xa) = a logb x.
We return to an examination of the properties of the exponential function.
3.3.9. Proposition. For any real number a,
ea = lim
n→∞

1 + a
n
n
Proof. Observe that proving this result is equivalent to showing that
lim
n→∞log(1 + a/n)n = a
(Why?) Now if f (x) = log(1 + x), then f ′(x) = (1 + x)−1. Thus
lim
n→∞log

1 + a
n
n
= lim
n→∞n log

1 + a
n

= a lim
n→∞

log

1 + a
n

−log 1
a
n

= a lim
t→0
log(1 + t) −log 1
t

= a f ′(0) = a
■
The next result says that exponential decay is more rapid that polynomial decay.
3.3.10. Proposition. For any n ≥1,
lim
x→∞xne−x = 0 and lim
x→0+ x−ne−1
x = 0
Proof. Note that the second of these equalities follows from the ﬁrst by substituting
x−1 for x. The proof of the ﬁrst equality consists of applying L’Hôpital’s Rule n
times.
lim
x→∞xne−x = lim
x→∞
xn
ex = lim
x→∞
nxn−1
ex
= · · · = lim
x→∞
n!
ex = 0
by Theorem 3.3.4(e).
■
In the statement of the preceding proposition we made expicit in the second equal-
ity a variation on the theme of the ﬁrst. There are many other variations that will be

84
Integration
used and we will just refer to this proposition. In particular we’ll see an additional
variation in the following example.
3.3.11. Example. If
f (x) =
	
e−1
x2
when x ̸= 0
0
when x = 0
then f is inﬁnitely diﬀerentiable and f (n)(0) = 0 for all n ≥1. To see this put
ξ(x) = −x−2 so that when x ̸= 0, f (x) = exp(ξ(x)). By the Chain Rule we have that
f ′(x) = ξ′(x) f (x) = 2x−3 f (x). Also [ f (t) −f (0)]/t = t−1 f (t) →0. So (3.3.10)
implies that f is diﬀerentiable at 0 and f ′(0) = 0.
Claim. For every n ≥1 there is a polynomial pn such that when x ̸= 0, f (n)(x) =
pn(x−1) f (x).
This is proved by induction. Just before the statement of the claim we saw that
the claim holds when n = 1. Assume it holds for n and let’s prove it for n + 1. In
fact when x ̸= 0
f (n+1)(x) = [pn(x−1) f (x)]′
= [pn(x−1)]′ f (x) + pn(x−1) f ′(x)
= −x−2p′
n(x−1) f (x) + pn(x−1) f ′(x)
= [−x−2p′
n(x−1) + pn(x−1)(2x−3)] f (x)
= pn+1(x−1) f (x)
where pn+1(y) = −y2p′
n(y) + 2y3pn(y), a polynomial. This establishes the above
claim.
Claim. For each n ≥1, f (n)(0) = 0.
Again we have already shown this for n = 1. Now assume that we have that
f (n)(0) = 0. The ﬁrst claim shows that [ f (n)(t) −f (n)(0)]/t = t−1pn(t−1) f (t), and
by Proposition 3.3.10 this converges to 0 as t →0.
We have seen that (ex)′ = ex. It turns out that, except for multiplying it by a con-
stant, the exponential function is the only function that has the property that it equals
its derivative. The proof is easy.
3.3.12. Proposition.
If f : (a, b) →R is a diﬀerentiable function such that
f ′(x) = f (x) for all x in (a, b), then there is a constant c such that f (x) = cex on
(a, b).
Proof. In fact for any x in (a, b)
 f (x)
ex
′
= f ′(x)ex −f (x)ex
e2x
= 0
Hence f /ex must be the constant function.
■

3.4 Improper Integrals
85
Exercises
(1)
Give the details of the proof of parts (a) through (d) in Theorem 3.3.4. (Hint: use
Proposition 2.3.10 to establish (a).)
(2)
Let
f : (0, 1) →R
be
deﬁned
by
f (x) = (x −1)/ log x.
(a)
Show
that
limx→0+ f (x) = 1. (b) Show that f is uniformly continuous.
(3)
Let f : (0, ∞) →R be a continuous function that satisﬁes f (x) −f (y) = f (x/y)
for all x, y in (0, ∞). Show that if f is diﬀerentiable at x = 1 with f ′(1) = 1, then
f is diﬀerentiable everywhere and f (x) = log x for all x.
(4)
Prove Proposition 3.3.6.
(5)
Prove Proposition 3.3.8.
(6)
What is (logb x)′?
(7)
Which is bigger, ex or xe? (Of course you can answer this with a calculator or com-
puter, but try to answer it by an examination of the function f (x) = log ex −log xe.)
(8)
For each of the following functions f determine whether x = 0 is a local maximum,
local minimum, or neither. (a) f (x) = x2ex3. (b) f (x) = x3ex2. (c) f (x) = ex2 sin x.
(d) f (x) = ex2 cos x. (e) f (x) = ex sin x2.
(9)
Evaluate the following. (a)
 e
1 x log x. (b)
 π
0 x cos x.
(10)
Find the derivative of f (x) = xa when x ∈(0, ∞) and a ∈R.
(11)
Let V be the vector space of all functions from R into itself and show that any ﬁnite
collection of functions in {x →eax : a ∈R} is linearly independent.
(12)
If f : R →R is deﬁned by
f (x) =
	
e−1
x2 sin 1
x
when x ̸= 0
0
when x = 0
show that f is inﬁnitely diﬀerentiable. Compare this with Example 2.4.4 and ponder
this comparison while regarding Proposition 3.3.10.
(13)
If {xn} is a sequence of positive real numbers such that xn →0, show that xn
n →0.
(14)
Show that limx→0+ x log x = 0. (Hint: Apply L’Hôpital’s Rule as expressed in Exer-
cise 2.5.3 to x log x = (log x)/(x−1).)
3.4. Improper Integrals
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
When we deﬁned the Riemann integral at the start of this chapter, we did so only for
bounded functions on a closed and bounded interval. We are frequently confronted
with situations where we want to integrate unbounded functions over intervals that
are not bounded, or not closed, or neither. Here we will deal with such situations,
relying on what we know from the previous sections. The material in this section is
inﬂuenced by the treatment of this subject in [15].

86
Integration
3.4.1. Definition. Suppose −∞≤a < b ≤∞and f : (a, b) →R. We say f is
locally integrable on (a, b) if whenever a < c < d < b we have that f is integrable
on [c, d]. Say that f is integrable or improperly integrable on (a, b) if it is locally
integrable on (a, b) and
lim
c→a+

lim
d→b−
 d
c
f

exists and is ﬁnite. When f is integrable on [a, b] we denote this iterated limit as
 b
a f =
 b
a f (x) dx.
Let’s emphasize that the interval (a, b) could be the whole line and that the func-
tion f need not be bounded. Because of the fact that f is assumed to be locally
integrable, however, it is required that f is bounded on every closed and bounded
interval contained in (a, b). We’ll see many examples below, but the ﬁrst thing we
should do is establish that the order of the two limits in the deﬁnition above does
not matter.
3.4.2. Proposition. If f is integrable over (a, b), then
lim
c→a+

lim
d→b−
 d
c
f

= lim
d→b−

lim
c→a+
 d
c
f

Proof. Pick some arbitrary point x in (a, b) and observe that
lim
c→a+

lim
d→b−
 d
c
f

= lim
c→a+
 x
c
f + lim
d→b−
 d
x
f

= lim
c→a+
 x
c
f + lim
d→b−
 d
x
f
= lim
d→b−

lim
c→a+
 x
c
f +
 d
x
f

= lim
d→b−

lim
c→a+
 d
c
f

■
Often we will encounter the situation where the diﬃculty in deﬁning the integral
of f on (a, b) only occurs at one of the endpoints. Consider the following.
3.4.3. Proposition. If −∞< a < b ≤∞and f : [a, b) →R is a function that is
integrable on (a, b), then
 b
a
f = lim
d→b−
 d
a
f
The proof is Exercise 1.

3.4 Improper Integrals
87
We record here several facts about integrable functions whose statements and
proofs are similar to the corresponding facts about integrable functions on closed
and bounded intervals. The proofs are left to the reader’s discretion.
3.4.4. Proposition. If −∞≤a < b ≤∞and f : (a, b) →R, the following hold.
(a) If a < c < b and f is integrable both on (a, c] and [c, b), then f is integrable
on (a, b) and
 b
a f =
 c
a f +
 b
c f .
(b) If f and g are integrable functions on (a, b) and α, β ∈R, then α f + βg is
integrable and
 b
a (α f + βg) = α
 b
a f + β
 b
a g.
The next result could easily be called the Comparison Test for improper integrals.
3.4.5. Proposition. If f and g are functions deﬁned on (a, b) such that 0 ≤f (x) ≤
g(x) for all x in (a, b) and g is integrable on (a, b), then f is integrable on (a, b) and
 b
a
f ≤
 b
a
g
Proof. Let a < c < d < b, temporarily ﬁx c, and deﬁne F(d) =
 d
c f , G(d) =
 d
c g; it follows that F(d) ≤G(d) whenever c < d < b. Moreover because the func-
tions f and g are positive, F and G are increasing. Hence their limits exist as
d →b−, and we have that
 b
c f = limd→b−F(d) ≤limd→b−G(d) =
 b
c g < ∞.
To complete the proof a similar argument can be used when we let c →a+.
■
In analogy with inﬁnite series we say that f : (a, b) →R is absolutely integrable
when | f | is integrable.
3.4.6. Proposition. If f : (a, b) →R is absolutely integrable, then f is integrable
and

 b
a
f
 ≤
 b
a
| f |
Proof. Note that −| f (x)| ≤f (x) ≤| f (x)| for all x in (a, b) and use the preceding
proposition.
■
3.4.7. Example.
(a) The integral
 ∞
1
1
xp converges if and only if p > 1. First
note that when p = 1 we have that
 d
1
1
x = log d →∞as d →∞, so the
integral diverges. We assume that p ̸= 1. Let d > 1 and consider
 d
1
1
xp = (−p +
1)−1[
1
d p−1 −1]. Let d →∞. When p > 1, this converges to
1
p−1; when p < 1, this
diverges to ∞. Note that since x−p is positive on [1, ∞), it is absolutely integrable
if and only if p > 1.
(b)
 ∞
−∞xe−x2 = limc→−∞[limd→∞
 d
c xe−x2 dx]. Since we intend to use the sub-
stitution u = x2 over an interval involving both positive and negative numbers, it is
expeditious to split the integral as
 d
c xe−x2 =
 0
c xe−x2 +
 d
0 xe−x2. Now making

88
Integration
the substitution in the second of these integrals gives
lim
d→∞
 d
0
xe−x2 = lim
d→∞
1
2
 d2
0
e−u du = lim
d→∞

−1
2[e−d2 −1]

= 1
Treating the ﬁrst integral similarly we get
lim
c→−∞
 0
c
xe−x2 =
lim
c→−∞
1
2
 0
c2 e−u du
= −lim
c→−∞
1
2
 c2
0
e−u du
= −lim
c→−∞

−1
2[e−c2 −1]

= −1
Therefore xe−x2 is integrable on R and
 ∞
−∞xe−x2 = 0.
(c)
 ∞
0 sin x = limd→∞
 d
0 sin x. Now
 d
0 sin x = (1 −cos d), which has no limit
as d →∞. Hence the sine function is not integrable on (0, ∞).
(d) Note that
 c
−c cos x = 0 for every c since cos(−x) = −cos x. Hence
limc→∞
 c
−c cos x = 0. However cos x is not integrable on R as an argument similar
to that used in (c) shows. This provides a cautionary note that as we did in (b) we
must use the deﬁnition of an improper integral and cannot take shortcuts without
justiﬁcation.
(e)
 ∞
1
cos x
x2
converges. In fact | cos x
x2 | ≤x−2 on [1, ∞), so that the statement fol-
lows by Proposition 3.4.6.
(f)
 ∞
1
sin x
x
is integrable. In fact let 1 < d < ∞. Since the function is continuous
here it is integrable over any bounded interval in [1, ∞). Using integration by parts
we get
 d
1
sin x
x
= −
cos d
d
−cos(1)

−
 d
1
cos x
x2
By the preceding example, x−1 sin x is integrable. It is not, however, absolutely inte-
grable. See Exercise 6.
3.4.8. Theorem (Integral Test). If f : [1, ∞) →R is a non-negative function that
is decreasing, then the series ∞
n=1 f (n) converges if and only if the function f is
integrable on [1, ∞).
Proof. Because f is non-negative and decreasing, f (n + 1) ≤f (x) ≤f (n) when
n ≤x ≤n + 1 and so f (n + 1) ≤
 n+1
n
f (x) ≤f (n). Thus if the series converges,
then 0 ≤
 ∞
1
f ≤∞
n=1 f (n) < ∞, and so f is integrable. If the function is inte-
grable, then 0 ≤∞
n=1 f (n + 1) ≤∞
n=1
 n+1
n
f =
 ∞
1
f .
■

3.5 Sets of Measure Zero and Integrability
89
3.4.9. Example. The series ∞
n=1
1
np converges if and only if p > 1. In fact this
follows by using the Integral Test and Example 3.4.7(a). Recall that when p = 1
this series is called the harmonic series and diverges (1.4.3). For other values of p
it is called the harmonic p-series.
Exercises
(1)
Prove Proposition 3.4.3.
(2)
For which values of p do the following integrals converge?
(a)
 1
0
1
xp.
(b)
 ∞
e
1
x logp x.
(3)
Show that
 1
0 | sin x/x
1
3 | converges. (Hint: First show that | sin x| ≤|x| on [0, 1].)
(4)
Justify the statements made in Example 3.4.7(d).
(5)
Use integration by parts to show that
 ∞
0 xne−xdx = n! for every integer n ≥
0. (Note that this necessitates combining integration by parts with improper
integrals.)
(6)
Show that | sin x|/x is not integrable over [1, ∞). (Hint: Note that
 nπ
1
| sin x|/x dx ≥1
kπ
n

k=2
 kπ
(k−1)π
| sin x| dx
and evaluate.)
(7)
Show that
 ∞
0
√xe−√x dx exists and determine its value.
(8)
Is f (x) = x−2(1 −cos x) integrable over (0, ∞)?
(9)
Show that the product of two improperly integrable functions is not always improp-
erly integrable.
(10)
Let
f , g : [a, b) →R be locally integrable positive functions and assume
L = limx→b−f (x)/g(x) exists and satisﬁes 0 ≤L < ∞. Show that if the
improper integral of g exists on [a, b), then the improper integral of f exists
on [a, b).
(11)
(a) If f is integrable over [0, ∞) and limx→∞f (x) exists, show that this limit must
be 0. (b) If f is integrable over [0, ∞), show by example that limx→∞f (x) may not
exist.
3.5. Sets of Measure Zero and Integrability∗
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In this section we will give a necessary and suﬃcient condition for a bounded func-
tion on a closed and bounded interval to be Riemann integrable. This condition is
phrased using a concept that has widespread use in a more general setting.

90
Integration
3.5.1. Definition. A subset E of R has measure zero if for every ϵ > 0 there is a
sequence of intervals {(an, bn)} such that E ⊆∞
n=1(an, bn) and
∞

n=1
(bn −an) < ϵ
The quantity bn −an is the length of the interval (an, bn), so the deﬁnition of a
set E of measure zero is that E can be covered by a sequence of intervals the sum of
whose lengths is as small as desired. Note that the inequality < ϵ can be replaced
by ≤ϵ. It’s clear that ∅has measure zero, and here are some more examples.
3.5.2. Example. (a) Any ﬁnite set has measure zero. In fact if E = {x1, . . . , xn},
then the intervals (x1 −ϵ/3n, x1 + ϵ/3n), . . . , (xn −ϵ/3n, xn + ϵ/3n) work.
(b) Any sequence has measure zero. If {xn} is given, then the sequence of intervals
{(xn −ϵ/2n+1, xn + ϵ/2n+1)} will work.
(c) Sets of measure zero can be very large in a certain sense. For example Q, the
set of rational numbers, has measure zero. In fact by Corollary 1.5.5, Q is count-
able so it is possible to arrange Q as a sequence and it follows from (b) that Q has
measure zero.
From these examples it becomes clear we will use some of the material from §1.5,
so the reader has to be familiar with countable sets.
3.5.3. Proposition. If {En} is a sequence of sets of measure zero, then E = ∞
n=1 En
has measure zero.
Proof. If ϵ > 0 and n ≥1, let {(ak
n, bk
n)}k be a sequence of open intervals covering
En and satisfying ∞
k=1(bk
n −ak
n) < ϵ/2n. It follows that {(ak
n, bk
n) : k ≥1, n ≥1}
is countable and can be written as a sequence. Moreover ∞
n=1
∞
k=1(bk
n −ak
n) <
∞
n=1 ϵ/2n = ϵ.
■
To prove the main result of this section we need to introduce an additional con-
cept. If f : [a, b] →R and I ⊆[a, b], deﬁne ω f (I) = sup{| f (y) −f (z)| : y, z ∈I}.
It will be useful as we progress to observe that if J ⊆I, then ω f (J) ≤ω f (I). If
x ∈[a, b], deﬁne the oscillation of f at x to be
ω f (x) = inf{ω f ((x −δ, x + δ) ∩[a, b]) : δ > 0}
(The interested reader can compare this with Exercise 1.7.14.) Note that when x is
not an endpoint we can write
ω f (x) = inf{ω f (x −δ, x + δ) : δ > 0}
So ω f (x) measures how much the values of f vary as points get closer to x (and
hence its name). It is not diﬃcult to see that if f : [0, 1] →R is deﬁned by letting
f (x) = sin x−1 when x ̸= 0 and f (0) = 0, then ω f (0) = 2.
Keep in mind that we have two types of oscillation for a function: one for intervals
and one for points. The pertinence of the oscillation for integrability can be seen in
the following.

3.5 Sets of Measure Zero and Integrability
91
3.5.4. Proposition. If P = {a = x0 < x1 < · · · < xn = b} is a partition of [a, b]
and f : [a, b] →R, then
U( f , P) −L( f , P) =
n

j=1
ω f ([xj−1, xj])(xj −xj−1)
Proof. If a ≤c < d ≤b, then it is easy to verify that ω f ([c, d]) = M −m, where
M = sup{ f (x) ∈[c, d]} and m = inf{ f (x) : x ∈[c, d]}. From here the proposition
follows.
■
The next result gives some basic properties of the oscillation. Particularly notice
the last part.
3.5.5. Proposition. If f : [a, b] →R, the following hold.
(a) For any t > 0, {x ∈[a, b] : ω f (x) < t} is open unless it contains one of the
endpoints of [a, b]. If it contains an endpoint it contains a half-open interval that
includes that endpoint.
(b) For any t > 0, {x ∈[a, b] : ω f (t) ≥t} is closed.
(c) The function f is continuous at x if and only if ω f (x) = 0.
Proof. The proof of (a) when the set contains an endpoint is similar to that of the
statement when it contains no endpoints and is left to the reader (Exercise 1). So
assume that a, b /∈G = {x : ω f (x) < t} and ﬁx x in G. By deﬁnition there is a δ >
0 such that ω f (x −δ, x + δ) = sup{| f (w) −f (z)| : w, z ∈(x −δ, x + δ)} < t.
Now for any y in (x −δ, x + δ) let r > 0 such that (y −r, y + r) ⊆(x −δ, x + δ);
we see that ω f (y −r, y + r) < t. Therefore ω f (y) < t; that is (y −r, y + r) ⊆G,
and so G is open. Part (b) follows since {x : ω f (t) ≥t} = [a, b]\{x : ω f (x) < t}.
To prove (c) assume f is continuous at x and ϵ > 0. Thus there is a δ > 0 such that
| f (x) −f (y)|<ϵ/2 when |x −y|<δ. This says that ω f (x) ≤ω f (x −δ, x + δ)< ϵ.
Since ϵ was arbitrary, ω f (x) = 0. Conversely, assume ω f (x) = 0 and let ϵ > 0.
By deﬁnition this implies there is a δ > 0 such that ω f (x −δ, x + δ) < ϵ. That is
| f (x) −f (y)| < ϵ when |x −y| < δ.
■
Given the basic criterion for integrability (3.1.4) as well as the relationship
between oscillation and the continuity of f at a point we just established, the state-
ment of the next theorem might be less surprising.
3.5.6. Theorem (Lebesgue’s2 Theorem).
A bounded function f : [a, b] →R is
integrable if and only if its set of discontinuities has measure zero.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
2 Henri Léon Lebesgue was born in 1875 in Beauvais, which is in the Picardie region of France just north of Paris.
He began his education in Beauvais and entered the École Normale Supérieure in Paris in 1894, where he was
awarded the teaching diploma in mathematics in 1897. He remained there for two years, reading mathematics
in the library. In 1899 he went to Nancy as a professor at a lycée where he remained for another two years.
In 1901 he formulated the theory of measure in a ground breaking paper that deﬁned what we now know as
the Lebesgue integral. This formed the basis of his doctoral dissertation and earns him the title of the father
of measure theory. The present theorem is important for us, but does not top the list of his most important

92
Integration
Proof. Suppose f : [a, b] →R is a function with | f (x)| ≤M for all x in [a, b] and
let D denote its set of discontinuities. By Proposition 3.5.5(c), D = {x : ω f (x) > 0};
for each t > 0 let Dt = {x : ω f (x) ≥t}. Assume f is Riemann integrable; we must
show that D has measure zero. The reader can check that D = ∞
n=1 D1/n, so by
Proposition 3.5.3 we will be successful if we show that each set Dt has measure
zero. Fix t > 0 and let ϵ > 0. From Proposition 3.5.4 we have the existence of a
partition P = {a = x0 < x1 < · · · < xn = b} such that
n

j=1
ω f ([xj−1, xj])(xj −xj−1) < tϵ/2
If J = {j : (x j−1, xj) ∩Dt ̸= ∅}, then for each j in J we have that ω f ([xj−1, xj]) ≥t.
(Verify!) Hence

j∈J
(xj −xj−1) ≤t−1 
j∈J
ω f ([xj−1, xj])(xj −xj−1) < ϵ
2
But 
j∈J(xj−1, xj) contains Dt except possibly for some subset of the endpoints of
these intervals, which has at most n + 1 points. Since each of these points can be
included in an interval of length less than ϵ/2(n + 1) and ϵ was arbitrary, we have
that Dt has measure zero.
Now assume that D has measure zero. Let t > 0; so Dt has measure zero.
(As we see what we need, we will take t to be a multiple of an arbitrary ϵ.)
Note that Dt is closed and bounded by Proposition 3.5.5(b). Because of this and
the fact that Dt has measure zero we can ﬁnd a ﬁnite number of open intervals
{(u1, v1), . . . , (up, vp)} such that Dt ⊆p
j=1(u j, v j) and p
j=1(v j −u j) < t. By
replacing any two of the closed intervals U = {[u1, v1], . . . , [up, vp]} that inter-
sect with their union, we can assume that these closed intervals are pairwise dis-
joint. Put X = [a, b]\ p
i=1(ui, vi). Note that X is closed and bounded. More-
over, since X is the complement in [a, b] of the union of a ﬁnite number of
open intervals, X itself is the union of a ﬁnite number of pairwise disjoint closed
intervals.
Observe that ω f (x) < t for each x in X. Therefore for each x in X there is a closed
interval I = [c, d] with x in (c, d) and such that ω f [c, d] < t. Since X is compact
there are a ﬁnite number of closed intervals whose union contains X and such that
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
results. In 1902 he joined the faculty at the University of Rennes in Brittany. He married in 1903 and he and
his wife had two children. Unfortunately, the union ended in divorce in 1916. He published two fundamental
monographs, Leçons sur l’intégration et la recherche des fonctions primitives in 1904 and Leçons sur les séries
trigonométriques in 1906. These books were unjustly criticized at the time by the classicists. Nevertheless he
joined the faculty at the University of Poitiers and ﬁnally overcame his critics before joining the faculty at the
Sorbonne in 1910. In 1921 he became Professor of Mathematics at the Collège de France and held this position
until he died in 1941. Lebesgue received many honors throughout his life and made serious contributions in
many areas, but he will forever be associated with measure theory and its impact on analysis.

3.6 The Riemann–Stieltjes Integral
93
each satisﬁes this inequality. By intersecting these intervals with X and using the
fact that X is itself the union of a ﬁnite number of closed intervals, we obtain closed
intervals I = {I1, . . . , Iq} having the following properties:
(i) X =
q
i=1
Ii,
(ii) ω f (Ii) < t for 1 ≤i ≤q,
(iii) the intervals Ii do not overlap
(When we say the intervals “do not overlap” in (iii), we mean they can only intersect
at their endpoints. How do we get (iii)?) Put Ii = [ci, di] for 1 ≤i ≤q.
Because of our construction we have that the closed intervals U ∪I are a
collection of non-overlapping intervals whose union is all of [a, b]. Therefore
their endpoints form a partition P = {a = x0 < x1 < · · · < xn = b}; thus U ∪I =
{[x0, x1], . . . , [xn−1, xn]}. In additon we have that
U( f , P) −L( f , P) =
n

k=1
ω f ([xk−1, xk])(xk −xk−1)
=
p

j=1
ω f ([u j, v j])(v j −u j) +
q

i=1
ω f ([cj, d j])(d j −cj)
≤
p

j=1
2M(v j −u j) +
q

i=1
t(d j −cj)
< 2Mt + t(b −a)
If we are given ϵ > 0, we can choose t > 0 such that 2Mt + t(b −a) < ϵ and so
we have that f is Riemann integrable by Proposition 3.1.4.
■
Exercise
(1)
Prove Proposition 3.5.5(a) under the assumption that the set contains one of the
endpoints.
3.6. The Riemann–Stieltjes Integral∗
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
For a ﬁxed closed, bounded interval J = [a, b] we want to deﬁne an extension of
the Riemann integral. This extended integral will also assign a number to each con-
tinuous function on the interval.

94
Integration
3.6.1. Definition. A function α : J →R is of bounded variation if there is a con-
stant M such that for every partition P = {a = x0 < · · · < xn = b} of J,
n

j=1
|α(xj) −α(xj−1)| ≤M
The quantity
Var(α) = Var(α, J) = sup
⎧
⎨
⎩
n

j=1
|α(xj) −α(xj−1)| : P is a partition of J
⎫
⎬
⎭
is called the total variation of α over J.
We’ll see many examples of functions of bounded variation. The ﬁrst below is
speciﬁc and the other two contain collections of such functions.
3.6.2. Example.
(a) The “mother of all” functions of bounded variation is
α(x) = x.
(b) Suppose α : [a, b] →R is a smooth function and M is a constant with
|α′(t)| ≤M for all t in [a, b]. If a = x0 < x1 < · · · < xn = b, then for each j
the Mean Value Theorem for derivatives says there is a point tj in [xj−1, xj]
such that α(xj) −α(xj−1) = α′(t j)(xj −xj−1). Hence 
j |α(xj) −α(xj−1)| =

j |α′(t j)|(x j −xj−1) ≤M(b −a), so that α is of bounded variation.
(c) Any increasing or decreasing function is of bounded variation.
Also see the Exercises.
For any interval [a, b] let BV[a, b] denote the set of all functions of bounded
variation deﬁned on [a, b]. The proof of the next proposition is left to the reader.
3.6.3. Proposition. BV[a, b] is a vector space over R, where the algebraic opera-
tions on these functions are deﬁned pointwise.
In light of the preceding proposition any linear combination of increasing func-
tions is a function of bounded variation. The surprising thing is that the converse
holds.
3.6.4. Proposition. If α : [a, b] →R is a function of bounded variation, then we
can write α = α+ −α−, where α± are increasing functions.
Proof. Let α+(t) = 1
2[Var(α, [a,t]) + α(t)] and α−(t) = 1
2[Var(α, [a,t]) −α(t)].
It is clear that α = α+ −α−, so what we have to do is show that these functions are
increasing. Lett > s, ϵ > 0, and let a = x0 < · · · < xn = s such that n
j=1 |α(xj) −
α(x j−1)| > Var(α, [a, s]) −ϵ. Now it is easy to verify that
|α(t) −α(s)| ± [α(t) −α(s)] ± α(s) ≥±α(s)
(Note that we are not allowed to randomly make a choice of sign each time ±
appears; it must be consistent.) Since a = x0 < · · · < xn < t is a partition of [a,t],

3.6 The Riemann–Stieltjes Integral
95
we get that
Var(α, [a,t]) ± α(t) ≥
n

j=1
|α(xj) −α(xj−1)| + |α(t) −α(s)|
±

[α(t) −α(s)] + α(s)

=
n

j=1
|α(xj) −α(xj−1)|
+ |α(t) −α(s)| ± [α(t) −α(s)] ± α(s)
≥Var(α, [a, s]) −ϵ ± α(s)
Since ϵ is arbitrary, we have that Var(α, [a,t]) ± α(t) ≥Var(α, [a, s]) ± α(s) and
so the functions α± are increasing.
■
Now that we have discussed functions of bounded variation, gotten many exam-
ples, and discovered a structure of such functions (3.6.4), we might pose a question.
Why the interest?
The important thing for us is that we can deﬁne integrals or averaging processes
for continuous functions by using a function of bounded variation. These integrals
have geometric interpretations as well as applications to the study of various prob-
lems in analysis. Let’s deﬁne the integrals, where the reader will notice a close
similarity with the deﬁnition of the Riemann integral. Indeed if α is the increas-
ing function α(t) = t, then what we do below will result in the Riemann integral
over J even though we will bypass the upper and lower sums for a partition. (This
is because here we will only integrate continuous functions. )
If α is a function in BV (J), f : J →R is some function, and P is a partition,
deﬁne
Sα( f , P) =
n

j=1
f (t j)[α(xj) −α(xj−1)]
where the points t j are chosen in the subinterval [xj−1, xj]. Yes, the notation does
not reﬂect the dependency of this sum on the choice of the points tj, but I am afraid
we’ll just have to live with that; indicating such a dependency is more awkward than
any gained beneﬁt. When the function α is the special one, α(t) = t, let Sα( f , P) =
S( f , P). That is,
S( f , P) =
n

j=1
f (t j)[xj −xj−1]
Deﬁne the mesh of the partition P to be the number ∥P∥= max{|xj −xj−1| : 1 ≤
j ≤n}, and for any positive number δ let Pδ denote the collection of all partitions P
with ∥P∥< δ. From the proof of Theorem 3.1.7 in the continuous case, it is seen that
when f : J →R is a continuous function, then there is a unique number I =
 b
a f
such that for every ϵ > 0 there is a δ > 0 with |I −S( f , P)| < ϵ whenever P ∈Pδ.

96
Integration
In fact, a look at that proof shows this follows from the fact that f is uniformly
continuous. (Verify!) We now start the process of showing that a similar existence
result holds if we replace the Riemann sum S( f , P) by the sum Sα( f , P) for an
arbitrary function of bounded variation α.
Here is another bit of notation that will simplify matters. For X ⊆R and a
function f : X →R, the modulus of continuity of f for any δ > 0 is the num-
ber ω( f , δ) = sup{| f (x) −f (y)| : |x −y| < δ}. (Compare this with the deﬁnition
of the oscillation of a function deﬁned in the preceding section and Exercise 1.7.14.)
This will be inﬁnite for some functions, but the main place we will use it is when
X is a closed and bounded interval and f is continuous. In that case f is uniformly
continuous so that we have that for any ϵ > 0 there is a δ such that ω( f , δ) < ϵ.
Just as in the deﬁnition of the Riemann integral, we want to deﬁne the integral
of a function with respect to a function of bounded variation α. Here is the crucial
lemma to get us to that goal.
3.6.5. Lemma.
If α is a function of bounded variation on J and f : J →R is
a continuous function, then for any ϵ > 0 there is a δ > 0 such that |Sα( f , P) −
Sα( f , Q)| ≤ϵ whenever P, Q ∈Pδ.
Proof. We start by observing that when P, Q are two partitions and Q is a reﬁne-
ment of P, then Sα( f , P) ≤Sα( f , Q). (Imitate the proof of the appropriate part of
Proposition 3.1.2.) In light of the preceding observation and using the same rea-
soning as in the proof of (3.1.2) it suﬃces to prove that there is a δ such that when
P, Q ∈Pδ and P ⊆Q, then |Sα( f , P) −Sα( f , Q)| ≤ϵ/2.
Use the uniform continuity of f to ﬁnd a δ such that ω( f , δ) < ϵ
2Var(α). Assume
that P ⊆Q and that they belong to Pδ. To simplify matters we will assume that Q
adds only one point to P and that this point lies between x0 and x1. That is, we assume
P = {a = x0 < x1 < · · · < xn = b} and Q = {a = x0 < x∗
0 < x1 < · · · < xn = b}.
Now for 2 ≤j≤n, let xj−1 ≤t j, s j ≤xj, x0 ≤t1 ≤x1, x0 ≤s∗
0 ≤x∗
0, x∗
0 ≤s∗
1 ≤x1.
Note that
 f (t1)[α(x1) −α(x0)] −
"
f (s∗
0)[α(x∗
0) −α(x0)] + f (s∗
1)[α(x1) −α(x∗
0)]
#
=
 f (t1)[α(x∗
0) −α(x0) + α(x1) −α(x∗
0)]
−
"
f (s∗
0)[α(x∗
0) −α(x0)] + f (s∗
1)[α(x1) −α(x∗
0)]
#
≤| f (t1) −f (s∗
0)| |α(x∗
0) −α(x0)|
+ | f (t1) −f (s∗
1)| |α(x1) −α(x∗
0)|
≤ω( f , δ)
$
|α(x∗
0) −α(x0)| + |α(x1) −α(x∗
0)|
%
We therefore obtain
|Sα( f , P) −Sα( f , Q)| ≤ω( f , δ)
$
|α(x∗
0) −α(x0)| + |α(x1) −α(x∗
0)|
%
+
n

j=2
| f (t j) −f (s j)| |α(xj) −α(xj−1)|
≤ω( f , δ)Var(α)
< ϵ/2

3.6 The Riemann–Stieltjes Integral
97
An inspection of the preceding argument shows that if Q had added more than
a single point to P, then the same reasoning would prevail and yielded the same
result.
■
It is important to emphasize that the value of the inequality obtained in the pre-
ceding lemma is independent of the choices of transitory points tj in [xj−1, xj] that
are used to deﬁne S(α, P). This amply justiﬁes not incorporating them in the nota-
tion used to denote such a sum.
3.6.6. Theorem. If α is a function of bounded variation on J and f : J →R is
a continuous function, then there is a unique number I with the property that for
every ϵ > 0 there is a δ > 0 such that when P ∈Pδ,
|Sα( f , P) −I| < ϵ
The number I is called the Riemann3– Stieltjes4 integral of f with respect to α, is
denoted by
I =
 b
a
f dα =

f dα
and satisﬁes


f dα
 ≤Var(α) max{| f (t)| : t ∈J}
Proof. According to the preceding lemma, for every integer n ≥1 there is a δn such
that if P, Q ∈Pδn, |Sα( f , P) −Sα( f , Q)| < 1
n. We can choose the δn so that they
are decreasing. Let Kn be the closure of the set of numbers {Sα( f , P) : P ∈Pδn}.
If | f (x)| ≤M for all x in J, then for any partition P, |Sα( f , P)| ≤M Var(α). So
each set Kn is bounded and hence compact. Since the numbers δn are decreas-
ing, for all n ≥1 Pδn ⊇Pδn+1 and so Kn ⊇Kn+1. Finally by the choice of the δn,
diam Kn ≤n−1 →0. Therefore by Cantor’s Theorem (Theorem 5.5.3 in the next
section) ∞
n=1 Kn = {I} for a single number I. It is now routine to verify that I has
the stated properties; its uniqueness is guaranteed by its construction.
■
A standard example comes, of course, when α(t) = t for all t in J and this is the
Riemann integral. The proofs of the next two results are left to the reader as a way
of ﬁxing the ideas in his/her head. These results should not come as a surprise.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
3 See Deﬁnition 3.1.3 for a biographical note.
4 Thomas Jan Stieltjes was born in 1856 in Zwolle, The Netherlands. He attended the university at Delft, spending
most of his time in the library reading mathematics rather than attending lectures. This had the eﬀect of causing
him to fail his exams three years in a row and he left the university without a degree. (This phenomenon of tal-
ented people having trouble passing exams is not unique and examples exist in the author’s personal experience.)
The absence of a degree plagued the progress of his career, in spite of the recognition of his mathematical talent
by some of the prominent mathematicians of the day. In 1885 he was awarded membership in the Royal Academy
of Sciences in Amsterdam. He received his doctorate of science in 1886 for a thesis on asymptotic series. In the
same year he secured a position at the University of Toulouse in France. He did fundamental work on continued
fractions and is often called the father of that subject. He extended Riemann’s integral to the present setting. He
died in 1894 in Toulouse, where he is buried.

98
Integration
3.6.7. Proposition. Let α and β be functions of bounded variation on J, f , g : J →
R continuous functions, and s,t ∈R.
(a)
 b
a (sf + tg) dα = s
 b
a f d α + t
 b
a gdβ.
(b) If f (x) ≥0 for all x in J and α is increasing, then
 b
a f dα ≥0.
(c)
 b
a f d(sα + tβ) = s
 b
a f dα + t
 b
a f dβ.
3.6.8. Proposition. If α is a function of bounded variation on J, f is a continuous
function on J, and a < c < b, then
 b
a f dα =
 c
a f dα +
 b
c f dα.
Here is an important result that enables us to compute some Riemann–Stieltjes
integrals from what we know about the Riemann integral.
3.6.9. Theorem. If α is a function on J that has a continuous derivative at every
point of J, then
 b
a
f dα =
 b
a
f (x)α′(x)dx
for any continuous function f .
Proof. We already know from Example 3.6.2 that such a function is of bounded
variation, so everything makes sense. Fix a continuous function f on J, let ϵ > 0,
and choose δ such that simultaneously


f dα −Sα( f , P)
 < ϵ/2
and


f α′dx −S( f α′, P)
 < ϵ/2
whenever P ∈Pδ. Momentarily ﬁx P = {a = x0 < · · · < xn = b} in Pδ. For each
subinterval [xj−1, xj] deﬁned by P, the MVT for derivatives implies there is a tj in
this subinterval with α(xj) −α(xj−1) = α′(t j)(xj −xj−1). Therefore

 b
a
f dα −
 b
a
f (x)α′(x)dx
 ≤

 b
a
f dα −
n

j=1
f (t j)[α(xj) −α(xj−1)]

+

n

j=1
f (t j)α′(t j)(xj −xj−1) −
 b
a
f (x)α′(x)dx

< ϵ
Since ϵ was arbitrary, we have the desired equality.
■
We now introduce some special increasing functions whose role in the general
theory has signiﬁcance in spite of their simplicity.
3.6.10. Example. Fix s in J. When a ≤s < b, deﬁne
αs(t) =
	
0
for t ≤s
1
for t > s

3.6 The Riemann–Stieltjes Integral
99
and
αb(t) =
	
0
for t < b
1
for t = b
Each of these functions is increasing. Let ϵ > 0 and choose δ > 0 such that
|

f dαs −Sαs( f , P)| < ϵ/2 whenever P ∈Pδ and also such that | f (u) −f (t)| <
ϵ/2 when |u −t| < δ. Assume s < b. (A separate argument is required when s = b
and this is left to the reader. See Exercise 6.) Choose a P in Pδ that contains s and
let s0 be the point in P that immediately follows s. Using s0 as the point in [s, s0] at
which to evaluate f , a moment’s reﬂection reveals that Sαs( f , P) = f (s0)[α(s0) −
α(s)] = f (s0). Thus | f (s) −

f dαs|≤| f (s) −f (s0)| + |Sαs( f , P) −

f dαs| < ϵ.
Since ϵ was arbitrary we have that

f dαs = f (s)
for every continuous function f on J. Similarly,

f dαb = f (b) for all such f .
Using the preceding example we can calculate the integrals with respect to many
functions that only diﬀer from a continuously diﬀerentiable one by having jump
discontinuities. Consider the following.
3.6.11. Example. Deﬁne α : [0, 1] →R by α(t) = t2 for t ≤1
2 and α(t) = t2 + 1
for t > 1
2. What is
 1
0 f (t) dα(t) for f in C[0, 1]? Note that α(t) = t2 + α 1
2 , where
α 1
2 is deﬁned in the preceding example. So using Proposition 3.6.7(c) and Theorem
3.6.9 we get
 1
0
f (t) dα(t) =
 1
0
f (t)d(t2) +
 1
0
f (t) dα 1
2 (t)
= 2
 1
0
t f (t)dt + f

1
2

We want to spend a little time studying functions of bounded variation. We begin
with the following.
3.6.12. Proposition. If α : J →R is a function of bounded variation, then α has
at most a countable number of discontinuities.
Proof. By Proposition 3.6.4 it suﬃces to show that the conclusion holds for increas-
ing functions. Hence the result now follows from Proposition 2.1.9.
■
A function α is called left-continuous at c if the left limit α(c−) exists and is
equal to α(c). The deﬁnition of right-continuous is analogous. If J = [a, b] theh
the left limit of α at a doesn’t really make sense, but we will deﬁne α(a−) = α(a).
Similarly, α(b+) = α(b). So every function α on J is left-continuous at a and right-
continuous at b by default. Note that if a ≤s < b, the function αs (3.6.10) is left-
continuous at s; however, αb is not left-continuous at b.

100
Integration
3.6.13. Corollary. If α : J →R is increasing and we deﬁne β : J →R by β(t) =
α(t−), then β is an increasing function that is left-continuous everywhere, has the
same discontinuities as α on [a, b), and agrees with α except possibly at its discon-
tinuities.
Proof. It is clear that β is increasing and left-continuous at each point of J. If c is a
point of discontinuity of α, then β(c) = α(c−) < α(c+). If c < b, then since there
are points of continuity for α that approach c from the right, we have that β(c+) =
α(c+) > β(c) and β is discontinuous at c. If c = b, then β(b) = α(b−) = β(b+),
so that β is continuous at b irrespective of whether α is continuous at b. (See the
function αb deﬁned in Example 3.6.10.)
■
Note that the integral of a continuous function with respect to a constant function
is 0; hence

f d(α + c) =

f dα for all continuous functions f . There are other
ways that we can produce two functions of bounded variation that yield the same
integral. (Compare the function deﬁned in Exercise 3 with the functions in Exam-
ple 3.6.10.) We want to examine when

f dα =

f dβ for two ﬁxed functions of
bounded variation and for every continuous function f on J.
For a function of bounded variation α on J and a ≤t ≤b, deﬁne
3.6.14
α(t) = α(t−) −α(a) + [α(b) −α(b−)]αb
where αb is deﬁned in Example 3.6.10. Call α the normalization of α. The ﬁrst thing
to observe is that α(t) −α(t) = α(a) except at the points where α is discontinuous.
Also if α is increasing, so is its normalization. Furthermore, if α = α+ −α−, then
α = α+ −α−. Therefore α is also a function of bounded variation. Finally note that
if α is continuous at b, then α(t) = α(t−) −α(a).
3.6.15. Proposition. If α is a function of bounded variation on J and α is its nor-
malization, then

f dα =

f dα for every continuous function f on J.
Proof. We split this into two cases.
Case 1. α is continuous at b.
Let D be the set of points in J where α is discontinuous – a countable set. Here
α(t) = α(t−) −α(a); ﬁx ϵ > 0 and f in C([a, b]). Let δ > 0 such that |Sα( f , P) −

f dα| < ϵ/2 and |Sα( f , P) −

f dα| < ϵ/2 whenever P ∈Pδ. Since D is a
countable set in [a, b], we can choose P = {a = x0 < · · · < xn = b} in Pδ such that
x j /∈D for 0 < j ≤n. We have by deﬁnition
Sα( f , P) = f (a)[α(x1) −α(a)] +
n

j=2
f (xj)[α(xj) −α(xj−1)]
and
Sα( f , P) = f (a)[α(x1) −α(a)] +
n

j=2
f (xj)[α(xj) −α(xj−1)]

3.6 The Riemann–Stieltjes Integral
101
For 0 < j ≤n, xj /∈D and so α(xj) = α(xj) + α(a). Since α(a) = 0, we also have
α(a) = α(a) + α(a); thus α(xj) −α(xj−1) = α(xj) −α(x j−1) for 1 ≤j ≤n. That
is, Sα( f , P) = Sα( f , P). Therefore |

f dα −

f dα| < ϵ. Since ϵ was arbitrary,
this proves Case 1.
Case 2. α is discontinuous at b.
Here α(t) = α(t−) + [α(b) −α(b−)]αb. Consider the function β = α −
[α(b) −α(b−)]αb. It follows that β is continuous at b since β(b−) = α(b−) =
β(b). By Case 1,

f dβ =

f dβ. Moreover since αb(t−) = 0 for all t in J,
including t = b, it follows that β(t) = α(t−) = α(t) −[α(b) −α(b−)]αb(t). Now

f dα −[α(b) −α(b−)] f (b) =

f dβ =

f dβ =

f dα −[α(b) −α(b−)] f (b).
After canceling we get that

f dα =

f dα.
■
The converse of the above result holds. To be precise, if α and β are two functions
of bounded variation on the interval J, then

f dα =

f dβ for every continuous
function f on J if and only if α = β. The proof of this requires more advanced
techniques and can be found in [3], Proposition 4.5.3.
Exercises
We continue to assume that J = [a, b] unless the interval is otherwise speciﬁed.
(1)
Show that a function of bounded variation is a bounded function.
(2)
Show that the function x2 sin(x−1) is not of bounded variation.
(3)
Deﬁne α on the unit interval [0, 1] by α( 1
2) = 1 and α(t) = 0 when t ̸= 1
2.
Observe that α is neither left nor right-continuous at 1
2. Show that α is of bounded
variation and ﬁnd increasing functions α± such that α = α+ −α−. Compute

f dα for an arbitrary continuous function f on the unit interval. Are you
surprised?
(4)
If α is an increasing function on J such that

f dα = 0 for every continuous func-
tion f on J, show that α is constant. Contrast this with Exercise 3. (Hint: First show
that you can assume that α(a) = 0, then show that α must be continuous at each
point of J. Use Example 3.6.10. Now show that α is identically 0.)
(5)
Give the details of the proof of Proposition 3.6.7
(6)
If αb is deﬁned as in Example 3.6.10, show that

f dαb = f (b) for every continuous
function f on J.
(7)
Suppose a left-continuous increasing function α : J →R has a discontinuity at t0
and a0 = α(t0+) −α(t0−). Let αt0 be the increasing function deﬁned as in Exam-
ple 3.6.10: αt0(t) = 0 for t ≤t0 and αt0(t) = 1 for t > t0. (Once again if t0 is the
right-hand endpoint of J, a separate argument is required.) (a) Show that α −a0αt0
is an increasing function that is continuous at t0. (b) Show that any left-continuous
increasing function α on J can be written as α = β + γ , where both β and γ are
increasing, β is continuous, and γ has the property that if γ is continuous on the
open subinterval (c, d), then γ is constant there.

102
Integration
(8)
If γ is an increasing function with the property of γ in Exercise 7, calculate

f dγ
for any continuous function f on J.
(9)
If A is any countable subset of J, show that there is an increasing function α on J
such that A is precisely the set of discontinuities of α. (So, in particular, there is an
increasing function on J with discontinuities at all the rational numbers in J.)
(10)
Let {rn} denote the set of all rational numbers in the interval J and deﬁne α : J →R
by α(t) =  1
2n where the sum is taken over all n such that rn < t. (a) Show that
α is a strictly increasing function that is left-continuous and satisﬁes α(a) = 0 and
α(b) = 1. (b) Show that α is continuous at each irrational number and discontinuous
at all the rational numbers in J.
(11)
If α is an increasing function, discuss the possibility of deﬁning

f dα when f has
a discontinuity.
(12)
Suppose α : [0, ∞) →R is an increasing function. Is it possible to deﬁne
 ∞
0
f dα
for a continuous function f : [0, ∞) →R?

4
Sequences of Functions
In this chapter we present some basic results in analysis that are used throughout
the theory and its applications. We also start a process of increased sophistication
that eventually migrates to thinking of functions as points in a larger space. As the
reader progresses, (s)he will notice a similarity between various aspects of our
discussion of sequences of functions and sequences of real numbers. This similarity
is no coincidence as the next chapter in this book will reveal.
4.1. Uniform Convergence
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In this section we discuss a notion of convergence for sequences and series of
functions.
4.1.1. Definition. If { fn} is a sequence of functions from a subset X of R into R,
say that { fn} converges uniformly to a function f if for every ϵ > 0 there is an N
such that | fn(x) −f (x)| < ϵ for all x in X and all n ≥N. In symbols this is written
as fn →u f on X.
In other words, fn →u f if for every ϵ > 0 it is possible to ﬁnd an N such that
| fn(x) −f (x)| < ϵ when n ≥N, where the same N works for all x at the same time.
We are interested in examining the properties of functions that are preserved under
taking uniform limits. That is, if fn →u f on X and each fn has some property, does
f have this same property? We’ll see several examples of this shortly. There is, of
course, another notion of convergence of a sequence of functions on a subset X of
R. Namely, we could investigate what happens when fn(x) →f (x) for every x in
X. This is called pointwise convergence. This has some value but not as much as
uniform convergence, as we’ll see below in Example 4.1.7 as well as in some of the
exercises. Observe that when fn →u f , pointwise convergence follows.
4.1.2. Example. (a) Suppose 0 < a < 1 and fn : [0, a] →R is deﬁned by fn(x) =
xn. Since | fn(x)| ≤an for all x in [0, a] and an →0, we have that { fn} converges
uniformly on [0, a] to the constantly zero function. See Exercise 1.
(b) If X = [0, 1] and fn : X →R is deﬁned by fn(x) = 1
nxn, then fn(x) →u 0 on
X. In fact if {gn} is any sequence of functions on any subset X in R such that there is
a constant M with |gn(x)| ≤M for all x in X, and {an} is a sequence of real numbers
converging to 0, then angn →u 0 on X.

104
Sequences of Functions
(c) If X = [0, 1] and fn(x) = xn, then for each x in [0, 1] fn(x) →f (x), where f
is the function deﬁned by f (x) = 0 when 0 ≤x < 1 and f (1) = 1. In this case { fn}
does not converge uniformly to f . This can be shown directly (Exercise 3), but an
easier way is to use what is contained in Example 4.1.7 below.
The next result is often useful in showing that a sequence of functions converges
uniformly.
4.1.3. Proposition. If { fn}, {gn}, and {hn} are three sequences of functions on X
such that gn(x) ≤fn(x) ≤hn(x) for all x in X and there is a function f : X →R
such that gn →u f and hn →u f on X, then fn →u f on X.
Proof. Let ϵ > 0 and choose N such that |gn(x)−f (x)|< ϵ and |hn(x) −f (x)| < ϵ
for all x in X and all n ≥N. It follows that for each x in X and all n ≥N, f (x) −ϵ
< gn(x) ≤fn(x) ≤hn(x) < f (x) + ϵ. Hence | fn(x) −f (x)| < ϵ for all x in X and
n ≥N. By deﬁnition, fn →u f .
■
The preceding proposition is often called the Squeeze Principle or Sandwich
Principle. It is actually what is happening in Example 4.1.2, parts (a) and (b). Here
is another use of the Sandwich Principle.
4.1.4. Example. Deﬁne fn : [−1, 1] →R by fn(x) =
√
x2 + n−2. Note that |x|2 ≤
fn(x)2 ≤|x|2 + 1
n
2 on [−1, 1]. So if we let gn(x) = |x| and hn(x) = |x| + 1
n, we have
that for all n ≥1, gn(x) ≤fn(x) ≤hn(x) on [−1, 1]. By the preceding proposition,
fn(x) →|x| uniformly on [−1, 1].
4.1.5. Proposition. Let X ⊆R. If { fn} is a sequence of bounded functions on X
that converges uniformly to a function f : X →R, then f is a bounded function
and the sequence of functions { fn} is uniformly bounded; that is, there is a constant
M such that | fn(x)| ≤M for all x in X and all n ≥1.
Proof. Let N be such that | fn(x) −f (x)| < 1 for all x in X and all n ≥N. If
LN is a number such that | fN(x)| ≤LN for all x, then for each x in X, | f (x)| ≤
| f (x) −fN(x)| + | fN(x)| ≤1 + LN. Hence f is a bounded function. Note that if
n ≥N and x ∈X, then | fn(x)| ≤| fn(x) −f (x)| + | f (x)| ≤2 + LN. If 1 ≤n ≤N
and | fn(x)| ≤Ln for all x in X, put M = max{L1, . . . , LN, 2 + LN}. It follows that
| fn(x)| ≤M for all x in X and all n ≥1.
■
Also see Exercise 6.
4.1.6. Theorem. Let X ⊆R. If { fn} is a sequence of bounded continuous functions
on X and fn →u f on X, then f : X →R is a continuous function.
Proof. Fix an arbitrary point a in X; we want to show that f is continuous at
a. If ϵ > 0, then by uniform convergence we can choose an integer n such that
| f (x) −fn(x)| < ϵ/3 for all x in X. Since fn is continuous there is a δ > 0 such
that | fn(x) −fn(a)| < ϵ/3 when x ∈X and |x −a| < δ. Thus for any x in X with
|x −a| < δ, | f (x) −f (a)| ≤| f (x) −fn(x)| + | fn(x) −fn(a)| + | fn(a) −f (a)| <
3(ϵ/3) = ϵ. By deﬁnition, f is continuous at a.
■

4.1 Uniform Convergence
105
4.1.7. Example. If the functions fn are as in Example 4.1.2(c), the fact that the
limit function f is not continuous shows, in light of the preceding result, that { fn}
does not converge uniformly on [0, 1] to f .
4.1.8. Theorem. Let X ⊆R. If { fn} is a sequence of bounded uniformly continu-
ous functions on X and fn →u f on X, then f : X →R is a uniformly continuous
function.
Proof. This proof is similar to the proof of the preceding result but with a critical
diﬀerence. If ϵ > 0, then by the uniform convergence we can choose an integer
n such that | f (x) −fn(x)| < ϵ/3 for all x in X. Since fn is uniformly continuous,
there is a δ > 0 such that | fn(x) −fn(y)| < ϵ/3 whenever |x −y| < δ. Thus for |x −
y| < δ we have that | f (x) −f (y)| ≤| f (x) −fn(x)| + | fn(x) −fn(y)| + | f (n(y) −
f (y)| < 3(ϵ/3) = ϵ. By deﬁnition, f is uniformly continuous on X.
■
4.1.9. Theorem. If { fn} is a sequence of bounded integrable functions on the inter-
val [a, b] and fn →u f , then f is integrable and
 b
a
fn →
 b
a
f
Proof. By Proposition 4.1.5 there is a constant M such that | fn(x)| ≤M for all x
in [a, b] and all n ≥1. If ϵ > 0, then by the uniform convergence we can ﬁnd an
integer N such that | f (x) −fn(x)|<ϵ/[9(b −a)] for all x in [a, b] and all n ≥N.
Temporarily ﬁx an n ≥N. Since fn is integrable there is a partition P = {a =
x0 < x1 < · · · < xn = b} withU( fn, P) −L( fn, P) < ϵ/9. Let M j( f ) = sup{ f (x) :
x j−1 ≤x ≤xj} and M j( fn) = sup{ fn(x) : xj−1 ≤x ≤xj}. Since | f (x) −fn(x)| <
ϵ/[9(b −a)] for all x, it follows that |Mj( f ) −M j( fn)| ≤ϵ/9(b −a) for 1 ≤j ≤n.
(Verify!) Thus |U( f , P) −U( fn, P)| ≤ϵ/9. Similarly |L( f , P) −L( fn, P)| ≤ϵ/9.
Therefore
U( f , P) −L( f , P) ≤|U( f , P) −U( fn, P)| + U( fn, P) −L( fn, P)
+ |L( f , P) −L( fn, P)|
≤ϵ/3
and so f is integrable. By Proposition 3.1.4, |
 b
a f −U( f , P)| ≤ϵ/3. Similarly
|
 b
a fn −U( fn, P)| ≤ϵ/3. Hence

 b
a
f −
 b
a
fn
 ≤

 b
a
f −U( f , P)
 + |U( f , P) −U( fn, P)|
+
U( fn, P) −
 b
a
fn

≤ϵ
Since n was an arbitrarily ﬁxed integer greater than N, this implies
 b
a fn →
 b
a f .
■

106
Sequences of Functions
It is a bit unfortunate that uniform convergence has no respect for diﬀerentiability
as it did for integrability in the preceding theorem. See Example 4.1.11 below. Here
is the best we can do.
4.1.10. Proposition. If { fn} is a sequence of continuously diﬀerentiable functions
on [a, b], fn →u f on [a, b], and there is a function g : [a, b] →R such that f ′
n →u
g on [a, b], then f is continuously diﬀerentiable and f ′ = g.
Proof. This is a consequence of the FTC. Note that fn(x) =
 x
a f ′
n + fn(a); also note
that Theorem 4.1.6 implies that gis a continuous function on [a, b]. By the preceding
theorem,
 x
a f ′
n →
 x
a g for every x in [a, b]. By hypothesis, this implies that f (x) =
limn fn(x) =
 x
a g + f (a). Again using the FTC, we have that f is diﬀerentiable and
f ′ = g.
■
4.1.11. Example. (a) For each n ≥1 deﬁne fn : [0, 2π] →R by fn(x) = 1
n sin(nx).
It is easy to see that fn →u 0 on [0, 2π]. However f ′
n(x) = cos(nx), so that { f ′
n} does
not converge uniformly even though the limit function of the original sequence is
continuously diﬀerentiable.
(b) Examine the sequence of functions { fn} on [−1, 1] in Example 4.1.4. This
shows that when a sequence { fn} of continuously diﬀerentiable functions converges
uniformly, it does not follow that the limit function is diﬀerentiable.
Exercises
(1)
Let fn : X →R and suppose there is a real number an with | fn(x)| ≤an for all x in
X and each n ≥1. Show that if an →0, then fn →u 0.
(2)
Find a sequence of bounded functions { fn} on [0, 1] and a function f : [0, 1] →R
such that fn(x) →f (x) for all x in [0, 1], but where f is not a bounded function.
(3)
Using only the deﬁnition of uniform convergence, show that the sequence { fn} in
Example 4.1.2(c) does not converge uniformly to f .
(4)
Let g : [0, 1] →R be a continuous function with g(1) = 0. Show that xng(x) →0
uniformly on [0, 1].
(5)
Say that { fn} is a uniformly Cauchy sequence on a subset X of R if each fn is
bounded and for every ϵ > 0 there is an integer N such that | fn(x) −fm(x)| < ϵ for
all x in X and all m, n ≥N. (a) Show that if { fn} is a uniformly Cauchy sequence,
then there is a constant M such that | fn(x)| ≤M for all x in X and all n ≥1. (b) Show
that if { fn} is a uniformly Cauchy sequence on X, then there is a function f : X →R
such that f →u f on X.
(6)
If fn : X →R, fn →u f on X, and f is a bounded function, show that there is a
constant M and an integer N such that | fn(x)| ≤M for all x in X and all n ≥N.
(7)
For each n ≥1 let fn : [−1, 1] →R be a continuous function with fn(x) ≥0 and
assume that limn
 1
−1 fn = 1; also assume that g : [−1, 1] →R is a continuous func-
tion. If for each c > 0, fn →u 0 on [−1, −c] ∪[c, 1], show that limn
 1
−1 gfn =
g(0).

4.2 Power Series
107
4.2. Power Series
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
We start with a deﬁnition that is a bit more general than the topic of this section.
4.2.1. Definition. If X ⊆R and for each n ≥1 there is a function fn : X →R, then
say that the series of functions ∞
n=1 fn converges uniformly on X if the sequence
of partial sums {n
k=1 fk} converges uniformly on X.
The reader can go through the last section and state and prove a collection of
results about series of functions. For example if each function fn is continuous on
X and f (x) = ∞
n=1 fn(x) converges uniformly on X, then f is a continuous func-
tion on X. We will use such results as we proceed without reference. Instead we
will present one result that implies uniform convergence and then proceed with the
development of power series.
4.2.2. Theorem (Weierstrass1 M-Test). Let X ⊆R and for each n ≥1 suppose
fn : X →R and there is a constant Mn with | fn(x)| ≤Mn for all x in X. If ∞
n=1 Mn
converges, then ∞
n=1 fn converges uniformly on X.
Proof. Put M = ∞
n=1 Mn and let ϵ > 0. By Proposition 1.4.5 we can choose N
such that n
k=m−1 Mk < ϵ whenever n > m ≥N. If Fn(x) = n
k=1 fk(x) and n >
m > N, then for every x in X, |Fn(x) −Fm(x)| ≤n
k=m−1 |Fk(x)| ≤n
k=m−1 Mk <
ϵ for n > m > N. This says that the sequence {Fn(x)} is a Cauchy sequence in R.
Thus F(x) = limn Fn(x) exists and deﬁnes a function F : X →R. Now |F(x) −
Fm(x)| ≤|F(x) −Fn(x)| + |Fn(x) −Fm(x)|. So if x is any point in X and n > m >
N, we have that |F(x) −Fm(x)| ≤|F(x) −Fn(x)| + ϵ. If we hold m ﬁxed but larger
than N and let n →∞, this shows that |F(x) −Fm(x)| ≤ϵ for all x in X and all
m > N. That is, Fm →u F on X.
■
Now we come to the focus of this section.
4.2.3. Definition. If c ∈R, a power series about the point c is an inﬁnite series of
the form
∞

n=0
an(x −c)n
where {an} is some sequence of real numbers.
We note two things about a power series. First, the inﬁnite sum begins with n = 0.
This is no big deal and, indeed, some power series will begin the sum with n = 1.
Just be conscious of this. Second is the “center” of the power series, c. If we translate
the center to another number, we don’t change any of the convergence properties of
the power series except for those that involve the center. Thus the basic properties
of a power series depend more on the coeﬃcients {an} than the center. In fact many
proofs about an arbitrary power series will be proved under the assumption that
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
1 See the footnote to Theorem 1.3.11 for a biographical note.

108
Sequences of Functions
c = 0, and many examples will have c = 0. An example of a power series about
0 is the geometric series (1.4.4), in which case each an = 1. Another is the series
∞
n=0 xn/n! (1.4.15), where an = 1/n!. The ﬁrst result we prove about power series
is the fundamental fact that will form the basis for all that follows on this topic.
4.2.4. Theorem. For a given power series ∞
n=0 an(x −c)n deﬁne the extended
real number R, 0 ≤R ≤∞, by
1
R = lim sup |an|
1
n
(a) If |x −c| < R, the series converges absolutely.
(b) If |x −c| > R, the series diverges.
(c) If 0 < r < R, the series converges uniformly on {x : |x −c| ≤r}.
(d) R is the unique number having properties (a) and (b).
Proof. Without loss of generality we may assume that c = 0.
(a) and (b). Note that lim sup |anxn|
1
n = |x| lim sup |an|
1
n = |x|R−1. Thus both (a)
and (b) follow by the Root Test (1.4.12).
(c) If |x| ≤r, then |anxn| ≤|anrn| = Mn. By (b), ∞
n=0 Mn converges, so the
Weierstrass M-Test implies ∞
n=0 anxn converges uniformly for x in {x : |x| ≤r}.
(d) This is routine and left to the reader. In fact it is more of an observation than
a result.
■
The number R obtained in the last theorem is called the radius of convergence of
the power series.
4.2.5. Example. (a) The radius of convergence of the geometric series is R = 1
since each coeﬃcient an = 1.
(b) The power series ∞
n=0 xn/n! has radius of convergence R = ∞. To see this
it is better not to use the formula for R. Instead we invoke Example 1.4.15 where it
is shown as a consequence of the Ratio Test that the series converges for all x in R.
By part (d) of the theorem, R = ∞.
(c) Here is an example that says you have to be a little careful when using the
Ratio Test to determine the radius of convergence. Consider the series
∞

n=1
(−1)n
n2n x2n
Because there are “gaps” in the powers of x we cannot apply the Ratio Test directly.
If however we let y = x2, we get the series ∞
n=1
(−1)n
n2n yn. Now we can use the Ratio
Test and we get that

(−1)n+1
(n+1)2n+1 yn+1
(−1)n
n2n yn

=

n2n
(n + 1)2n+1
 |y| →1
2|y|

4.2 Power Series
109
So the original power series converges when 1
2|x|2 < 1 and so the radius of conver-
gence is
√
2. See Exercise 1.
4.2.6. Theorem. Assume the power series ∞
n=0 an(x −c)n has positive radius of
convergence R > 0. If f : (c −R, c + R) →R is deﬁned by f (x) = ∞
n=0 an(x −
c)n, then the following hold.
(a) The function f is inﬁnitely diﬀerentiable, and for |x −c| < R and k ≥0
4.2.7
f (k)(x) =
∞

n=k
n(n −1) · · · (n −k + 1)an(x −c)n−k
(b) The radius of convergence of the power series (4.2.7) is also R.
(c) For every n ≥0
an = 1
n! f (n)(c)
Proof. Begin by observing that part (c) is an immediate consequence of (a). To
prove (a) and (b) we again assume that c = 0 and start by establishing the following.
Claim. lim sup |an|1/(n−1) = R−1.
Put R1 equal to the the reciprocal of the lim sup in the claim so that R1
is the radius of convergence of the power series ∞
n=1 anxn−1 = ∞
n=0 an+1xn.
Notice that x ∞
n=0 an+1xn + a0 = ∞
n=0 anxn. Hence when |x| < R1 we have that
∞
n=0 |anxn| = |a0| + |x| ∞
n=0 |an+1||x|n < ∞. By the uniqueness of the radius
of convergence this shows that R1 ≤R. On the other hand, if 0 < |x| < R,
then ∞
n=1 |an||x|n−1 = |x|−1 ∞
n=1 |an||x|n < ∞. Hence R ≤R1, establishing the
claim.
If (a) and (b) are established for k = 1, then, by applying them to the power series
∞
n=0 nanxn, we will obtain (a) and (b) for k = 2. By repeating this we arrive at
the validity of (a) and (b) for an arbitrary k ≥0. So to prove (a) and (b) we can
restrict our attention to the case where k = 1. We start this by showing that R−1 =
lim sup |nan|1/(n−1). Now L’Hôpital’s rule shows that limn→∞(log n)/(n −1) = 0.
Hence n1/(n−1) = exp[(log n)/(n −1)] →1. By Exercise 1.3.16 and the claim,
lim sup |nan|1/(n−1) = lim sup |an|1/(n−1) = R−1, giving (b).
To get (a) when k = 1 deﬁne the functions g, sn, and Rn on the interval (−R, R)
by
g(x) =
∞

n=1
nanxn−1,
sn(x) =
n

k=0
anxn,
Rn(x) =
∞

k=n+1
anxn
(The deﬁnition of g is valid by part (b).) Fix a number y with |y| < R; we want to
show that f ′(y) exists and f ′(y) = g(y). Choose r with |y| < r < R and let δ > 0 be
arbitrary except that (y −δ, y + δ) ⊆(−r, r). (Later in this proof we will impose
an additional restriction on δ.) Let |x −y| < δ. For any integer n ≥1 (this integer

110
Sequences of Functions
will be speciﬁed later) we can write

f (x) −f (y)
x −y
−g(y)
 ≤

sn(x) −sn(y)
x −y
−s′
n(y)
 + |s′
n(y) −g(y)|
+

Rn(x) −Rn(y)
x −y

Let ϵ > 0. We want to show that if we take δ suﬃciently small and n suﬃciently
large, we can make each of the summands on the right side of the preceding inequal-
ity less than ϵ
3. This will prove that f is diﬀerentiable and f ′(y) = g(y). The most
cumbersome of these three terms to handle is the last, so let’s tackle it ﬁrst. For any
choice of n
Rn(x) −Rn(y)
x −y
=
1
x −y
∞

k=n+1
ak(xk −yk)
=
∞

k=n+1
ak

xk −yk
x −y

But

xk −yk
x −y
 = |xk−1 + xk−2y + · · · + yk−1| ≤krk−1
so that

Rn(x) −Rn(y)
x −y
 ≤
∞

k=n+1
|ak|krk−1
Now since r < R, (b) implies that ∞
k=1 |ak|krk−1 converges. Thus there is an
integer N1 such that for all n ≥N1 and for any x in the interval (−r, r),

Rn(x) −Rn(y)
x −y
 ≤ϵ
3
Now {s′
n(y)} is just the sequence of partial sums of the series g(y). So there is an
integer N2 such that when n ≥N2, |s′
n(y) −g(y)| < ϵ
3.
Let n = max{N1, N2}. (So we have now ﬁxed n.) Because sn is a polynomial and
diﬀerentiable, there is a δ > 0 such that

sn(x) −sn(y)
x −y
−s′
n(y)
 < ϵ
3
when |x −y| < δ. (Be aware that the choice of δ depends on the ﬁxed value of n.)
With these choices we have that

f (x) −f (y)
x −y
−g(y)
 < ϵ
when |x −y| < δ. This completes the proof of (a) and of the theorem.
■

4.2 Power Series
111
4.2.8. Example.
(a) Theorem 4.2.6 says that when a function is deﬁned by a
power series it is inﬁnitely diﬀerentiable. The converse of this is not true. In fact
in Example 3.3.11 we saw that the function deﬁned by f (x) = e−1
x2 when x ̸= 0
and f (0) = 0 is inﬁnitely diﬀerentiable on R with f (n)(0) = 0 for all n ≥0. By
(4.2.6(c)) we have that this function cannot be written as a power series centered at
c = 0.
(b) The function ex can be written as a power series. In fact
ex =
∞

n=0
xn
n!
This can be seen as follows. In Example 4.2.5(b) we saw that this power series has
radius of convergence R = ∞. If f (x) denotes this power series, then Theorem 4.2.6
implies
f ′(x) =
∞

n=0
nxn−1
n!
=
∞

n=1
xn−1
(n −1)! =
∞

n=0
xn
n! = f (x)
By Proposition 3.3.12 there is a constant c such that f (x) = cex. Since f (0) = 1 =
e0, this constant c must be 1.
When we know that a function can be represented as a power series, this gives us
additional power. In fact Theorem 4.2.6 can be phrased as saying that the derivative
of a power series is obtained by diﬀerentiating the individual terms of the power
series. (This is often referred to as diﬀerentiating term-by-term.) Also since the
convergence of the power series is uniform on any closed interval contained in
(c −R, c + R), Theorem 4.1.9 says we can ﬁnd the integral of this function over
this interval by integrating the terms of the series. Since simply being inﬁnitely
diﬀerentiable does not guarantee the representation as a power series, we seek a
criterion to show that such a function can be so represented. The primary tool for
this is Taylor’s Theorem (2.5.6).
4.2.9. Theorem. Let a < b and suppose f ∈C(∞)(a, b). If there is a constant M
such that | f (n)(x)| ≤Mn for all x in (a, b) and all n ≥1, then for any c in (a, b)
f (x) =
∞

n=0
f (n)(c)
n!
(x −c)n
and this power series has radius of convergence R with R ≥min{c −a, b −c}.
Proof. Let r = min{c −a, b −c}, and for each n ≥1 let
Pn(x) =
n−1

k=0
f (k)(c)
k!
(x −c)k

112
Sequences of Functions
By Taylor’s Theorem (2.5.6) for each x in (c −r, c + r) there is a point d between
c and x such that
f (x) −Pn(x) = f (n)(d)
n!
(x −c)n
By the hypothesis this implies
| f (x) −Pn(x)| ≤rn Mn
n!
Now an application of the Ratio Test reveals that ∞
n=1 rn Mn
n! converges (Verify!)
and so the n-th term rn Mn
n! →0. Therefore Pn(x) →f (x) whenever c −r < x <
c + r, and so f has the power series expansion with radius of convergence at
least r.
■
The preceding theorem won’t cover all cases, but it will cover many that are of
interest. Let’s look at a few.
4.2.10. Example. (a) The function sin x has the power series expansion
sin x =
∞

n=0
(−1)n
(2n + 1)!x2n+1
The even derivatives of the sine function are ± sin x, so that f (2n)(0) = 0 for all
n ≥0. The odd derivatives are ± cos x, so that f (2n+1)(0) = ±1 for all n ≥0.
Thus we can apply the preceding theorem with M = 1. The exact coeﬃcients that
appear in the above power series are determined by ﬁnding the correct formula for
f (2n+1)(0). The reader can ﬁll in the details.
(b) The function cos x has the power series expansion
cos x =
∞

n=0
(−1)n
(2n)! x2n
See Exercise 6.
(c) The natural logarithm has the power series expansion
log x =
∞

n=1
(−1)n+1
n
(x −1)n
and the radius of convergence is 1. Note that here we are ﬁnding the power series
expansion centered at c = 1. A small induction argument shows that when f (x) =
log x and n ≥1
f (n)(x) = (−1)n+1(n −1)!x−n

4.2 Power Series
113
So | f (n)(x)| cannot be bounded as required in Theorem 4.2.9. We have to do some-
thing diﬀerent. Here we have that for |x −1| < 1
log x =
 x
1
1
t dt
=
 x
1
1
1 −(1 −t) dt
=
 x
1
∞

n=0
(1 −t)n dt
Now the geometric series under the integral sign converges uniformly on
{t : |1 −t| ≤|1 −x|}, so Theorem 4.1.9 implies
log x =
∞

n=0
 x
1
(1 −t)n dt =
∞

n=1
(−1)n+1
n
(x −1)n
Since this convergence holds when |x −1| < 1, we have that R ≥1. Since log x is
not deﬁned at x = 0, it must be that R = 1.
Exercises
(1)
Show that if we have a power series ∞
n=0 anxn with radius of convergence R and if
limn |an+1/an| exists, then the value of this limit is R.
(2)
Find the radius of convergence of the following: (a) ∞
n=0
xn
2n ; (b) ∞
n=1
xn
n3 ; and
(c) ∞
n=0
2n
n2+1xn.
(3)
Find
the
radius
of
convergence
of
the
following:
(a)
∞
n=1(log n)xn;
(b) ∞
n=1 2nnaxn, where a is some positive constant; and (c) ∞
n=1
nn
n!xn.
(4)
Show that the radius of convergence of the power series
∞

n=1
(−1)nxn(n+1)
is 1 and discuss convergence of the series when x = ±1.
(5)
Find the radius of convergence of the power series
∞

n=0
sin(nπ/6)
2n
xn.

114
Sequences of Functions
(6)
(a) For Example 4.2.10(a), ﬁnd the precise formula for f (2n+1)(0). (b) Fill in the
details needed to verify Example 4.2.10(b).
(7)
Fill in the details to obtain Example 4.2.10(c).
(8)
If f ∈C(∞)(a, b) and c ∈(a, b), show that f can be represented as a power series
in an interval about c if and only if there is an r > 0 with (c −r, c + r) ⊆(a, b)
such that f (n)(c)
n!
(x −c)n →0 when |x −c| < r.

5
Metric and Euclidean Spaces
In this chapter we begin the study of p-dimensional Euclidean space, Rp, but in this
beginning we will carry it a step further. We want to discuss diﬀerentiation and inte-
gration on Rp, but ﬁrst we need to extend the notions of sequential convergence, the
properties of sets, and the concept of continuity to the higher dimensional spaces.
The eﬀort to explore these concepts in Rp, however, is not greater than what is
required to explore these notions in what are called metric spaces. In many respects
the abstract spaces are easier to cope with than Rp. Moreover some of what we have
already done is properly couched in metric spaces. Indeed, the material of §4.1
can be set there with little additional eﬀort. Nevertheless during this venture into
abstraction the main set of examples will be Euclidean space.
We start with the concept of distance between points. This must be general
enough to encompass a variety of circumstances, but it should conform to the intu-
itive notion we all have of what is meant by distance. Since this is done at the start of
the ﬁrst section, it would be proﬁtable before proceeding for the reader to reﬂect on
what properties (s)he thinks should be included in an abstract concept of distance;
then you can compare your thoughts with the deﬁnition that starts the following
section.
The treatment of metric spaces here is based on Chapter 1 of [4].
5.1. Definitions and Examples
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
5.1.1. Definition. A metric space is a pair of objects, (X, d), where X is a set and
d is a function d : X × X →[0, ∞) called a metric, that satisﬁes the following for
all x, y, z in X:
(a) d(x, y) = d(y, x);
(b) d(x, y) = 0 if and only if x = y; and
(c) (Triangle Inequality) d(x, y) ≤d(x, z) + d(z, y).
Condition (a) is sometimes called the symmetric property and says that the dis-
tance from x to y is the same as the distance from y to x. The second property says
the obvious: the distance from a point to itself is 0 and the only point at a distance
zero from x is x itself. The third, the triangle property, says that the shortest dis-
tance between two points is the direct one – not a distance involving a third point.
In Rp this is phrased by saying that the shortest distance between two points is a

116
Metric and Euclidean Spaces
straight line. In the abstract setting we have no concept of straight lines. Though you
might have thought of other properties for an idea of distance, these three are usually
part of what most people intuitively associate with the concept. In fact, I think the
properties above are the minimal ones. There are several particular situations where
additional axioms for a distance are assumed; those are more specialized theories,
and what we are now going to explore is the basic one. Here are some examples of
metric spaces.
5.1.2. Example.
(a) Let X = R, the set of real numbers, and deﬁne d(x, y) =
|x −y|. See Exercise 1.
(b) Let X = R2, the plane, and deﬁne d((x1, y1), (x2, y2)) = [(x1 −x2)2 + (y1 −
y2)2]
1
2 . The reader knows from the Pythagorean Theorem that this is the straight-
line distance and (s)he can use geometry to verify that this standard notion of the
distance between two points satisﬁes the axioms in the preceding deﬁnition.
(c) We deﬁne p-dimensional Euclidean space, Rp, to be
Rp = {(x1, . . . , xp) : xn ∈R for 1 ≤n ≤p}
For x = (x1, . . . , xp) and y = (y1, . . . , yp) in Rq, deﬁne
d(x, y) =
 p

n=1
(xn −yn)2
 1
2
This is a metric on Rp; however to prove this satisﬁes the triangle inequality requires
some eﬀort that we’ll do below (5.1.6).
(d) Let X = Rp and for x, y in Rp deﬁne
d(x, y) =
p

n=1
|xn −yn|
This is also a metric on Rp but it is easier to verify this than the previous example
(Exercise 2).
(e) Again let X = Rp and now deﬁne
d(x, y) = max{|xn −yn| : 1 ≤n ≤p}
Once again (Rp, d) is a metric space (Exercise 3). It is worth observing that in each
of the last three examples, when p = 1, all these metrics are the standard absolute
value on R.
(f) Let X be any set and deﬁne
d(x, y) =
	
0
if x = y
1
if x ̸= y
It is a simple exercise to verify that (X, d) is a metric space. This is called the
discrete metric on X. You won’t encounter this much except in the study of metric
spaces, as it is a nice example to test the concepts.

5.1 Definitions and Examples
117
(g) An important class of examples arise as follows. Suppose (X, d) is a given
metric space. If Y is a non-empty subset of X, then (Y, d) is a metric space and
is referred to as a subspace. As a speciﬁc instance of this we can take X = R and
Y = [a, b]. We saw this often in R and will see this often in Rp when we consider
subsets of Euclidean space and examine them as metric spaces.
The next result will be quite useful in our discussion of metric spaces. It is a
direct consequence of the triangle inequality and is often called the Reverse Triangle
Inequality.
5.1.3. Proposition. If (X, d) is a metric space and x, y, z ∈X, then
|d(x, y) −d(y, z)| ≤d(x, z)
Proof. The triangle inequality implies that d(x, y) −d(y, z) ≤d(x, z). Now reverse
the roles of x and z in this inequality and we get that d(z, y) −d(y, x) ≤d(z, x),
from which we also have that d(y, z) −d(x, y) ≤d(x, z). That is, ±[d(x, y) −
d(y, z)] ≤d(x, z) from which the proposition follows.
■
Now let’s show that the function d given in Example 5.1.2(c) is a metric. To
do this we need a famous inequality. To facilitate the proof, introduce the help-
ful notation that for vectors x = (x1, . . . , xp) and y = (y1, . . . , yp) in Rp, ⟨x, y⟩=
p
n=1 xnyn. Actually this is more than just “helpful” notation as it denotes the inner
or “dot” product in the vector space Rp. This connection will not be explored here,
where we will only regard this as notation. Of course some of you may have explored
the inner product in linear algebra. It is useful to observe the following properties
for all vectors x, y, z in Rp and all real numbers t.
5.1.4
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
⟨x, x⟩≥0
⟨x, y⟩= ⟨y, x⟩
⟨tx + z, y⟩= t⟨x, y⟩+ ⟨z, y⟩
⟨x, y + tz⟩= ⟨x, y⟩+ t⟨x, z⟩
5.1.5. Theorem (Cauchy1–Schwarz2 Inequality).
For x = (x1, . . . , xp) and y =
(y1, . . . , yp) in Rp we have
 p

n=1
xnyn
2
≤
 p

n=1
x2
n
  p

n=1
y2
n

• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
1 See the footnote in Deﬁnition 1.3.12 for a biographical note.
2 Hermann Amandus Schwarz was a German mathematician born in 1843 in Hermsdorf, Silesia, which at present
is part of Poland. He began his studies at Berlin in Chemistry, but switched to mathematics and received his
doctorate in 1864 under the direction of Weierstrass. He held positions at Halle, Zurich, Göttingen, and Berlin.
His work centered on various geometry problems that were deeply connected to analysis. This included work
on surfaces and conformal mappings in analytic function theory, any student of which will see his name in
prominence. He died in Berlin in 1921.

118
Metric and Euclidean Spaces
Proof. First note that using the inner product notation introduced above, the sought
after inequality becomes
⟨x, y⟩2 ≤⟨x, x⟩⟨y, y⟩
Using (5.1.4) we have that
0 ≤⟨x −ty, x −ty⟩
= ⟨x, x⟩−t⟨y, x⟩−t⟨x, y⟩+ t2⟨y, y⟩
= ⟨x, x⟩−2t⟨x, y⟩+ t2⟨y, y⟩
= γ −2βt + αt2 ≡q(t)
where γ = ⟨x, x⟩, β = ⟨x, y⟩, α = ⟨y, y⟩. Thus q(t) is a quadratic polynomial in the
variable t. Since q(t) ≥0 for all t, the graph of q(t) stays above the x-axis except
that it might be tangent at a single point; that is, q(t) = 0 has at most one real root.
From the quadratic formula we get that 0 ≥4β2 −4αγ = 4(β2 −αγ ). Therefore
0 ≥β2 −αγ = ⟨x, y⟩2 −⟨x, x⟩⟨y, y⟩
proving the inequality.
■
5.1.6. Corollary. If d : Rp × Rp →[0, ∞) is deﬁned as in Example 5.1.2(b), then
d is a metric.
Proof. We begin by noting that d(x, y) = √⟨x −y, x −y⟩. Using the Cauchy–
Schwarz Inequality, (5.1.4), and the vector space properties of Rp we get that
d(x, y)2 = ⟨x −y, x −y⟩
= ⟨(x −z) + (z −y), (x −z) + (z −y)⟩
= ⟨x −z, x −z⟩+ 2⟨x −z, z −y⟩+ ⟨z −y, z −y⟩
≤d(x, z)2 + 2

⟨x −z, x −z⟩

⟨z −y, z −y⟩+ d(z, y)2
= d(x, z)2 + 2d(x, z)d(z, y) + d(z, y)2
= [d(x, z) + d(z, y)]2
Taking square roots shows that the triangle inequality holds. The remainder of the
proof that d deﬁnes a metric is straightforward. (Verify!)
■
Notice. Whenever we are discussing Rp we assume that the metric under con-
sideration is that deﬁned in Example 5.1.2(c).
We close with the following, whose proof is Exercise 5.
5.1.7. Proposition. If (X, d) and (Z, ρ) are two metric spaces and we deﬁne the
function δ : (X × Z) × (X × Z) →R by
δ((x1, z1), (x2, z2)) = d(x1, x2) + ρ(z1, z2)
then δ is a metric on X × Z.

5.2 Sequences and Completeness
119
Exercises
(1)
Verify the statement in Example 5.1.2(a).
(2)
Verify the statement in Example 5.1.2(d).
(3)
Verify the statement in Example 5.1.2(e).
(4)
In the Cauchy–Schwarz Inequality, show that equality holds if and only if the vectors
x and y are linearly dependent.
(5)
Prove Proposition 5.1.7.
5.2. Sequences and Completeness
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
For the remainder of this chapter (X, d) is a given metric space.
Now that we have a generalization of the concept of distance, we can introduce
convergent sequences. As in Chapter 1, a sequence is just a way of enumerating some
points in the space X: x1, x2, . . .; this is denoted by {xn}. Precisely, this is a function
from the natural numbers N into X: n →xn. As when we discussed sequences of
real numbers, we’ll sometimes change the domain of this function to N ∪{0} so that
we get a sequence {x0, x1, . . .}; or maybe it might be changed to get {x2, x3, . . .}.
There is no real diﬀerence; we have a speciﬁc beginning and a countably inﬁnite
following. This means that the set of all integers, Z, is not permitted as an indexing
set for sequences.
Unlike when we considered sequences in R we have no concept of an interval
so we cannot use the deﬁnition of convergence we used in R. Instead we use the
generalization of the equivalent formulation from Proposition 1.3.3.
5.2.1. Definition. A sequence {xn} in (X, d) converges to x if for every ϵ > 0 there
is an integer N such that d(x, xn) < ϵ when n ≥N. The notation for this is xn →x
or x = limn xn.
5.2.2. Example. (a) A sequence in R converges in the sense of §1.3 if and only if
it converges in R considered as a metric space. (This is the content of (1.3.3).)
(b) If xn = (x1
n, . . . , xp
n) and (x1, . . . , xp) are in Rp, then xn →x if and only if
x j
n →x j in R for 1 ≤j ≤p. In fact if xn →x, then we need only note that for 1 ≤
j ≤p, |x j
n −x j| ≤d(xn, x) to conclude that x j
n →x j. Now assume that x j
n →x j
for 1 ≤j ≤p. Let ϵ > 0 and for 1 ≤j ≤p choose Nj such that |x j
n −x j| < ϵ/√p
when n ≥Nj. If we put N = max{N1, . . . , Np} and n ≥N, then
p

j=1

x j
n −x j2 <
p

j=1
ϵ2
p = ϵ2
Hence d(xn, x) < ϵ for n ≥N and we have that xn →x in Rp.

120
Metric and Euclidean Spaces
(c) If (X, d) is the discrete metric space, then a sequence {xn} in X converges to
x if and only if there is an integer N such that xn = x whenever n ≥N. In words, a
sequence in X converges if and only if it is eventually constant.
As in R, if {xn} is a sequence in (X, d) and {nk} is a sequence of positive integers
with n1 < n2 < · · · , then {xnk} is called a subsequence of {xn}. As we saw in Propo-
sition 1.3.10, if a sequence in (X, d) converges to x, every subsequence converges
to x. Unlike R it makes no sense to talk of monotonic sequences in an abstract met-
ric space. So the results proved in §1.3 about monotonic sequences have no analogy
here. A concept from our study of R that had little signiﬁcance there, however, will
have great signiﬁcance now, though its true prominence will not arise until §5.3.
5.2.3. Definition. A sequence {xn} in (X, d) is called a Cauchy sequence if for every
ϵ > 0 there is an integer N such that d(xm, xn) < ϵ when m, n ≥N. The discrete
metric space (X, d) is said to be complete if every Cauchy sequence converges.
5.2.4. Example.
(a) As in R, every convergent sequence in (X, d) is a Cauchy
sequence in (X, d) (Exercise 2).
(b) R is a complete metric space (1.3.13) as is Rp (Exercise 3).
(c) If we furnish Q with the metric it has as a subspace of R, then it is not a
complete metric space. In fact just take a sequence {xn} in Q that converges to
√
2
in R. It is a Cauchy sequence in (Q, d) but does not converge to a point in Q.
(d) The discrete metric space is complete. Indeed a sequence in the discrete metric
space is a Cauchy sequence if and only if it is eventually constant.
5.2.5. Proposition. If {xn} is a Cauchy sequence and some subsequence of {xn}
converges to x, then xn →x.
Proof. Suppose xnk →x and let ϵ > 0. Choose an integer N1 such that d(xnk, x) <
ϵ/2 for nk ≥N1, and choose an integer N2 such that d(xn, xm) < ϵ/2 when m, n ≥
N2. Put N = max{N1, N2} and let n ≥N. Fix any nk ≥N. Since we have that
nk ≥N1 and both n and nk are larger than N2, we get that d(x, xn) ≤d(x, xnk ) +
d(xnk, xn) < ϵ/2 + ϵ/2 = ϵ.
■
Exercises
(1)
Suppose {xn} is a sequence in X that converges to x and z1, . . . , zm is a ﬁnite collec-
tion of points in X. Deﬁne a new sequence {yn} in X by letting yk = zk for 1 ≤k ≤m
and yk = xk−m when k ≥m + 1. Show that yn →x.
(2)
Verify the statement in Example 5.2.4(a).
(3)
Show that Rp is complete. (Hint: Use the method in Example 5.2.2(b) to show that
a sequence in Rp is a Cauchy sequence if and only if each of the sequences of its
coordinates is a Cauchy sequence in R.)
(4)
Show that a sequence {(xn
1, xn
2)} in Rp × Rq (see Proposition 5.1.7) converges to
(x1, x2) if and only if the same thing happens when we consider the sequence as
belonging to Rp+q.

5.3 Open and Closed Sets
121
(5)
Let (X, d) be the cartesian product of the two metric spaces (X1, d1) and (X2, d2) as
in Proposition 5.1.7. (a) Show that a sequence {(x1
n, x2
n)} in X is a Cauchy sequence
in X if and only if {x1
n} is a Cauchy sequence in X1 and {x2
n} is a Cauchy sequence
in X2. (b) Show that X is complete if and only if both X1 and X2 are complete.
5.3. Open and Closed Sets
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
As we did for R in §1.6, we deﬁne open and closed sets in a metric space. In an
abstract metric space these sets will play an even bigger role than they did in R.
5.3.1. Definition. A subset F of (X, d) is said to be closed if whenever {xn} is a
sequence of points in F and xn →x we have that x ∈F. A subset G is said to be
open if X\G is a closed set.
Again we note that as in the case that X ⊆R, the entire space X and the empty
set are simultaneously open and closed; also any ﬁnite subset of (X, d) is closed.
Also see Exercise 1. The ﬁrst two parts of the next proposition are restatements in
the case of (X, d) of the ﬁrst two parts of Proposition 1.6.3. The exact same proof
used to demonstrate (1.6.3) can be used word for word to prove these. The other two
parts of the next proposition are analogous to the last two parts of Proposition 1.6.3
with one big diﬀerence. Unlike when we discuss open and closed subsets of R we
need to talk about arbitrary unions and intersections of sets, not just their countable
versions. Here are the deﬁnitions.
Suppose I is an arbitrary non-empty set, sometimes called in this circumstance
an index set, and assume that for each i in I there is a subset Ai of X. We deﬁne the
union and intersection of the collection {Ai : i ∈I} by

i∈I
Ai = {x ∈X : x ∈Ai for some i in I}

i∈I
Ai = {x ∈X : x ∈Ai for every i in I}
These operations satisfy the customary rules enjoyed by their ﬁnite versions. In
particular, there is a version of De Morgan’s Laws that is valid (Exercise 2).
5.3.2. Proposition. Let (X, d) be a metric space.
(a) If F1, . . . , Fn are closed sets in (X, d), then n
k=1 Fk is closed.
(b) If G1, . . . , Gn are open sets in (X, d), then n
k=1 Gk is open.
(c) If {Fi : i ∈I} is a collection of closed sets, then 
i∈I Fi is closed.
(d) If {Gi : i ∈I} is a collection of open sets, then 
i∈I Gi is open.
Proof. As we said the proofs of (a) and (b) are verbatim copies of the proofs of
the similar statements for R. The proofs of (c) and (d) are similar to the proofs for
the analogous statements for sequences of sets in R given in Proposition 1.6.3. The
details are left to the reader.
■

122
Metric and Euclidean Spaces
When x ∈X and r > 0, introduce the notation
B(x; r) = {y ∈X : d(x, y) < r}
B(x; r) = {y ∈X : d(x, y) ≤r}
The set B(x; r) is called the open ball of radius r about x, or centered at x; B(x; r) is
called the closed ball of radius r about x. If X = R, then B(x; r) is the open inter-
val (x −r, x + r) and B(x; r) is the closed interval [x −r, x + r]. If X = R2, then
B(x; r) is the so-called “open” ball or disk centered at x of radius r that does not
include the bounding circle; and B(x; r) is the corresponding “closed” disk that does
include the bounding circle. The use of the words open and closed here will be jus-
tiﬁed momentarily. Meanwhile notice the trivial but useful observation that when
s < r, B(x; s) ⊆B(x; r).
The next result is a restatement of Proposition 1.6.4 for an arbitrary metric space.
Small modiﬁcations of the proof of (1.6.4) will furnish a proof of the present result.
5.3.3. Proposition. A subset G of (X, d) is open if and only if for each x in G there
is an r > 0 such that B(x; r) ⊆G.
The preceding proposition quantiﬁes what it means for a set to be open and as
such is most useful as we will see.
5.3.4. Example. (a) For any r > 0, B(x; r) is closed. In fact let {an} be a sequence
in B(x; r) that converges to a. Thus d(a, x) ≤d(a, an) + d(an, x) ≤d(a, an) + r.
Since d(a, an) →0, we have that a ∈B(x; r).
(b) For any r >0, B(x; r) is open. In fact let F =X\B(x; r)={y ∈X :d(y, x) ≥r}.
If {an} is a sequence in F that converges to a, then the reverse triangle inequal-
ity implies |d(x, a) −d(an, a)| ≥d(an, x) ≥r. Since d(an, a) →0 we have that
a ∈F.
It is important when discussing open and closed sets
to be conscious of the universe.
When (X, d) is a metric space and Y ⊆X, we have that (Y, d) is also a metric
space (Example 5.1.2(f)). To say that we have an open set A in (Y, d) does not mean
that A is open in X. Note that in such a circumstance when y ∈Y and r > 0, then
the open ball about y of radius r is BY (y; r) = {z ∈Y : d(z, y) < r} = B(y; r) ∩Y.
This may not be an open set in X. For example if X = R and Y = [0, 1], then [0, 1
2)
is open as a subset of Y but not as a subset of X; another example: BY ( 1
4; 1
3) =
[0, 7/12). When we want to emphasize the open and closed sets in the subspace
metric (Y, d), we’ll use the terms open relative to Y or relatively open in Y and
closed relative to Y or relatively closed in Y. The proof of the next proposition is
Exercise 3.
5.3.5. Proposition. Let (X, d) be a metric space and let Y be a subset of X.
(a) A subset G of Y is relatively open in Y if and only if there is an open subset U
in X with G = U ∩Y.

5.3 Open and Closed Sets
123
(b) A subset F of Y is relatively closed in Y if and only if there is a closed subset D
in X such that F = D ∩Y.
Now to introduce certain concepts that had little signiﬁcance when we studied
R but will have importance for Rp as well as any other metric space.
5.3.6. Definition. Let A be a subset of X. The interior of A, denoted by int A, is the
set deﬁned by
int A =

{G : G is open and G ⊆A}
The closure of A, denoted by cl A, is the set deﬁned by
cl A =

{F : F is a closed and A ⊆F}
The boundary of A, denoted by ∂A, is the set deﬁned by
∂A = cl A ∩cl (X\A)
Let’s note that there is always an open set contained in any set A – namely the
empty set, ∅. It may be, however, that ∅is the only open set contained in A, in which
case int A = ∅. Similarly X is a closed set containing any set A; but it may be the
only such set, in which case cl A = X. (We’ll have more to say about this latter case
below.) It follows from what we have established that int A is open (though possibly
empty) and cl A is closed (though possible equal to X). We also have that int ∅=
∅= cl ∅and int X = X = cl X. Before looking at more interesting examples, it is
proﬁtable to ﬁrst prove some properties of the closure and interior of a set. We start
with a characterization of these sets using open balls.
5.3.7. Proposition. Let A ⊆X.
(a) x ∈int A if and only if there is an r > 0 such that B(x; r) ⊆A.
(b) x ∈cl A if and only if for every r > 0, B(x; r) ∩A ̸= ∅.
(c) x ∈∂A if and only if for every r > 0 we have that B(x; r) ∩A ̸= 0 and B(x; r) ∩
(X\A) ̸= ∅.
Proof. (a) If B(x; r) ⊆A, then since B(x; r) is open we have that B(x; r) ⊆int A;
hence x ∈int A. Now assume that x ∈int A. So there is an open set G such that
x ∈G ⊆A. But since G is open, there is a radius r > 0 with B(x; r) ⊆G and we
have established the converse.
(b) Suppose x ∈cl A. If r > 0, then B(x; r) is open and X\B(x; r) is closed. It
cannot be that A ⊆X\B(x; r) since, by deﬁnition, this implies cl A ⊆X\B(x; r),
contradicting the fact that x ∈cl A. Thus B(x; r) ∩A ̸= ∅. Now assume that x /∈cl A;
that is, x ∈X\cl A, an open set. By Proposition 5.3.3 there is a radius r > 0 such
that B(x; r) ⊆X\cl A. So for this radius, B(x; r) ∩A = ∅.
(c) This is immediate from (b).
■
The preceding proposition is very useful as it provides a concrete, one-point-at-
a-time method to determine the closure and the interior of a set. We’ll see this in
the following example.

124
Metric and Euclidean Spaces
5.3.8. Example. (a) Sometimes things can become weird with interiors and clo-
sures. Consider the metric space R and the subset Q of all rational numbers. If
x ∈R, then B(x; r) = (x −r, x + r) and this interval must contain a rational num-
ber. By the preceding proposition, x ∈cl Q. We also have that int Q = ∅. To see
this again use the preceding proposition and the fact that between any two real
numbers there is an irrational number. This means that when x ∈Q, no open ball
B(x; r) can be contained in Q so that int Q = ∅. Using the same reasoning we see
that cl [R\Q] = R and int [R\Q] = ∅. (Verify!) It follows that ∂Q = R.
(b) Here is a cautionary tale. Since B(x; r) is closed, we have that cl B(x; r) ⊆
B(x; r). It may be, however, that cl B(x; r) ̸= B(x; r). In fact suppose that X =
{(0, 0)} ∪{(a, b) : a2 + b2 = 1} ⊆R2. So X consists of the origin in the plane
together with the unit circle centered at the origin. Give X the metric it inherits
as a subset of R2. In this case {(0, 0)} = cl B((0, 0); 1) ̸= B((0, 0); 1) = X. It is
also true that when X is the discrete metric space, then {x} = B(x; 1) = cl B(x; 1)
whereas B(x; 1) = X.
(c) We haven’t said much about the boundary of a set, but note that for any x in
Rp and r > 0 we have that ∂B(x; r) = ∂B(x; r) = {y ∈X : d(x, y) = r}. Is this true
in every metric space? (Exercise 5).
The next proposition contains some useful information about closures and inte-
riors of sets. Its proof is left as Exercise 6.
5.3.9. Proposition. Let A be a subset of X.
(a) A is closed if and only if A = cl A.
(b) A is open if and only if A = int A.
(c) cl A = X\[int (X\A)], int A = X\cl (X\A), and ∂A = cl A\int A.
(d) If A1, . . . , An are subsets of X, then cl [n
k=1 Ak] = n
k=1 cl Ak.
Part (d) of the preceding proposition does not hold for the interior. For example
if X = R, a < b < c, A = (a, b], and B = [b, c), then int (A ∪B) = (a, c) while
int A ∪int B = (a, b) ∪(b, c). Also see Exercises 7 and 8.
5.3.10. Definition. A subset E of a metric space (X, d) is dense if cl E = X. A
metric space (X, d) is separable if it has a countable dense subset.
We made a reference to this concept just after deﬁning the closure of a set. So a
set E is dense if and only if X is the only closed subset of X that contains E.
5.3.11. Example. (a) Every metric space is dense in itself.
(b) The rational numbers form a dense subset of R as do the irrational numbers.
This is a rephrasing of Example 5.3.8(a). We note that this implies that R is sepa-
rable since Q is countable (1.5.5).
(c) The set of all points in Rp with rational coordinates is dense in Rp. This
follows from the preceding example and it also says that Rp is separable by
Corollary 1.5.6.

5.3 Open and Closed Sets
125
(d) If X is any set and d is the discrete metric on X (5.1.2(e)), then the only dense
subset of X is X itself. In fact if E is a dense subset of (X, d) and x ∈X, then it must
be that B(x; 1/2) ∩E ̸= ∅; but from the deﬁnition of the discrete metric it follows
that B(x; 1/2) = {x}. Hence the discrete metric space is separable if and only if it
is countable.
In part (d) of the preceding example we used the next result, and we record it here
for future reference. Also it is a consequence of Proposition 5.3.7(b).
5.3.12. Proposition. A set E is dense in (X, d) if and only if for every x in X and
every r > 0, B(x; r) ∩E ̸= ∅.
5.3.13. Definition. If A ⊆X, a point x in X is called a limit point of A if for every
ϵ > 0 there is a point a in B(x; ϵ) ∩A with a ̸= x.
See Exercise 1.6.11, where this concept was deﬁned in R.
The emphasis here is that no matter how small we take ϵ we can ﬁnd such a point
a diﬀerent from x that belongs to B(x; ϵ) ∩A. It is not required that x belongs to A
for it to be a limit point (more on this later). If x is not a limit point of A and, in
addition, belongs to A, then it is called an isolated point of A.
5.3.14. Example. (a) Let X = R and A = (0, 1) ∪{2}. Every point in [0, 1] is a
limit point of A but 2 is not. In fact, 2 is an example of an isolated point of A, the
only isolated point of A.
(b) If X = R and A = Q, then every point of X is a limit point of A and A has no
isolated points.
(c) If X = R and A = {n−1 : n ∈N}, then 0 is a limit point of A while the points
n−1 are all isolated points.
5.3.15. Proposition. Let A be a subset of the metric space X.
(a) A point x is a limit point of A if and only if there is a sequence of distinct points
in A that converges to x.
(b) A is a closed set if and only if it contains all its limit points.
(c) cl A = A ∪{x : x is a limit point of A}.
Proof. In Exercise 1.6.11 the reader was asked to prove parts (a) and (b) when
X = R.
(a) Suppose {an} is a sequence of distinct points in A such that an →x. If ϵ > 0,
then there is an N such that an ∈B(x; ϵ) for n ≥N. Since the points in {an} are dis-
tinct, there is at least one diﬀerent from x. Thus x is a limit point. Now assume that x
is a limit point. Let a1 ∈A ∩B(x; 1) such that a1 ̸= x. Let ϵ2 = min{2−1, d(x, a1)};
so there is a point a2 ∈A ∩B(x; ϵ2) with a2 ̸= x. Note that a2 ̸= a1.
Claim. There is a sequence of positive numbers {ϵn} and a sequence of dis-
tinct points {an} in A such that: (i) ϵn ≤n−1; (ii) an ̸= x for all n ≥1; and (iii)
d(x, an) < ϵn.
We already have a1. Assume that a1, . . . , an−1 have been chosen. Since the num-
bers d(x, a1), . . . , d(x, an−1) are all positive we can choose 0 < ϵn < n−1 and also

126
Metric and Euclidean Spaces
smaller than any of these numbers. Let an ∈A ∩B(x, ϵn) with an ̸= x. This proves
the claim and ﬁnishes the proof of (a).
(b) and (c). Clearly (b) will follow once we prove (c). Let B denote the set on the
right-hand side of the equation in (c). By the deﬁnition of a closed set and part (a)
we have that B ⊆cl A. On the other hand, if x ∈cl A, (5.3.7) implies that for every
n ≥1 there is a point an in A with d(an, x) < n−1. Either {an} has an inﬁnite number
of distinct terms or a ﬁnite number. In the ﬁrst case there is a subsequence {ank} of
distinct terms; by (a), x is a limit point. Thus x ∈B. In the case that {an} has only a
ﬁnite number of points, there is a subsequence {ank} that is constant; thus ank = x
for all k ≥1 and so x ∈B.
■
5.3.16. Definition. If A ⊆X and x ∈X, the distance from x to A is
dist (x, A) = inf{d(x, a) : a ∈A}
Clearly when x ∈A, dist (x, A) = 0. But it is possible for the distance from a
point to a set to be 0 when the point is not in the set as we now see.
5.3.17. Proposition. If A ⊆X, then cl A = {x ∈X : dist (x, A) = 0}.
Proof. If x ∈cl A, then there is a sequence {an} in A such that an →x; so
dist (x, an) →0 and it follows that dist (x, A) = 0. Conversely if dist (x, A) = 0,
then there is a sequence {an} in A such that d(x, an) →0. Thus an →x and so
x ∈cl A.
■
Also see Exercise 14. Now we extend Cantor’s Theorem (1.6.8) to the setting of
a complete metric space. First we need to extend the deﬁnition of the diameter of
set from that given in §1.6 to the setting of a metric space; a little thought will show
this is the natural extension. If E ⊆X, deﬁne the diameter of E to be
diam E = sup{d(x, y) : x, y ∈E}
5.3.18. Theorem (Cantor’s Theorem). A metric space (X, d) is complete if and
only if whenever {Fn} is a sequence of non-empty subsets satisfying: (i) each Fn is
closed; (ii) F1 ⊇F2 ⊇· · · ; and (iii) diam Fn →0, then ∞
n=1 Fn is a single point.
Proof. Assume (X, d) is complete and {Fn} is as in the statement of the theorem.
For each n let xn ∈Fn. If ϵ > 0, let N be such that diam Fn < ϵ for n ≥N. So if
m, n ≥N, (ii) implies xn, xm ∈FN and so d(xn, xm) ≤diam FN < ϵ. Thus {xn} is a
Cauchy sequence. Since (X, d) is complete, there is an x in X such that xn →x.
But each Fn is closed, so x ∈∞
n=1 Fn. If there is another point y in ∞
n=1 Fn, then
d(x, y) ≤diam Fn for each n ≥1. By (iii), y = x.
Now assume that (X, d) satisﬁes the stated conditions and {xn} is a Cauchy
sequence. Put Fn = cl {xn, xn+1, . . .}. Clearly (i) and (ii) are satisﬁed. If ϵ > 0, let N
be such that d(xn, xm) < ϵ for m, n ≥N. But for k ≥N, diam Fk = sup{d(xn, xm) :
m, n ≥k} ≤ϵ. Thus {Fn} satisﬁes the three conditions so that ∞
n=1 Fn = {x} for
some point x. But for any n ≥1, d(x, xn) ≤diam Fn →0. Therefore xn →x and
(X, d) is complete.
■

5.3 Open and Closed Sets
127
Notice that we did not state this extension as we did Theorem 1.6.8. What happens
in the preceding version of Cantor’s Theorem if we do not assume diam Fn →0?
See Exercise 16.
The proof of the next proposition is Exercise 17.
5.3.19. Proposition. If (X, d) is a complete metric space and Y ⊆X, then (Y, d)
is complete if and only if Y is closed in X.
We close this section by dwelling a bit on the concept of the diameter of a set.
5.3.20. Definition. Say that a subset A of (X, d) is bounded if diam A < ∞.
5.3.21. Proposition.
(a) A subset A of (X, d) is bounded if and only if for any x in X there is an r > 0
such that A ⊆B(x; r).
(b) The union of a ﬁnite number of bounded sets is bounded.
(c) A Cauchy sequence in (X, d) is a bounded set.
Proof. (a) If A ⊆B(x; r), then diam A ≤2r, so that A is bounded. Conversely
assume that A is bounded with δ as its ﬁnite diameter. Fix a point x in X and some
point a0 in A. For any point a in A, d(x, a) ≤d(x, a0) + d(a0, a) ≤d(x, a0) + δ. If
we let r = 2[d(a0, x) + δ], then A ⊆B(x; r).
(b) If Ak is bounded for 1 ≤k ≤n and x ∈X, let rk > 0 such that Ak ⊆B(x; rk).
If we set r = max{r1, . . . , rn}, then A1 ∪· · · ∪An ⊆B(x; r).
(c) If {xn} is a Cauchy sequence, there is an N ≥1 such that d(xn, xm) < 1 for
m, n ≥N. If B = {xn : n ≥N}, this says that diam B ≤1 so that B is bounded. On
the other hand, A = {x1, . . . , xN} is bounded since ﬁnite sets are bounded. By part
(b), {xn} = A ∪B is bounded.
■
The concept of boundedness in an arbitrary metric space is not as useful as it
was in R and does not have the same consequences. We will see, however, that the
beneﬁts of boundedness we had in R carry over to Rp.
Exercises
(1)
Show that the following sets F are closed. (a) F = {(x, y) ∈R2 : −1 ≤x ≤1
and −1 ≤y ≤1}. (b) F = {(x, y) ∈R2 : y ≥x−1}. (c) F = {(x, y, z) ∈R3 : y ≤
x ≤z}.
(2)
Prove the following version of De Morgan’s Laws for any collection of subsets {Ai :
i ∈I}, where each Ai ⊆X:
X\

i∈I
Ai =

i∈I
(X\Ai)
X\

i∈I
Ai =

i∈I
(X\Ai).

128
Metric and Euclidean Spaces
(3)
Prove Proposition 5.3.5. (Hint: If G is a relatively open subset of Y, for each y in G
let ry > 0 such that BY (y; ry) ⊆G. Now consider {B(y; ry) : y ∈G}.)
(4)
If Y is a subset of X, consider the metric space (Y, d) and suppose Z ⊆Y. (a) Show
that H is a relatively open subset of Z if and only if there is a relatively open subset
H1 of Y such that H = H1 ∩Z. (b) Show that D is a relatively closed subset of Z if
and only if there is a relatively closed subset D1 of Y such that D = D1 ∩Z. (Hint:
Use Proposition 5.3.5.)
(5)
Is it true that ∂B(x; r) = ∂B(x; r) = {y ∈X : d(x, y) = r} in an arbitrary metric
space?
(6)
Prove Proposition 5.3.9.
(7)
Show that if A1, . . . , An are subsets of X, then int [n
k=1 Ak] = n
k=1 int Ak.
(8)
Show that (5.3.9(d)) does not hold for inﬁnite unions and the preceding exercise
does not hold for inﬁnite intersections.
(9)
If E ⊆X and x ∈∂E, show that x is either an isolated point of E or a limit point.
(10)
If A is a subset of X that is simultaneously open and closed, show that ∂A = ∅.
(11)
See Corollary 1.5.6 for the deﬁnition of X1 × X2. Deﬁne ρ : X1 × X2 →[0, ∞) by
ρ((x1, x2), (y1, y2)) = max{d1(x1, y1), d2(x2, y2)}.
(a) Show that ρ is a metric on X1 × X2. (b) Show that a set G is open in (X1 × X2, ρ)
if and only if for every (x1, x2) in G there are open sets G1 in X1 and G2 in X2 such
that x1 ∈G1, x2 ∈G2, and G1 × G2 ⊆G.
(12)
Let ℓ∞denote the set of all bounded sequences of real numbers; that is, ℓ∞con-
sists of all sequences {xn} such that xn ∈R for all n ≥1 and supn |xn| < ∞. If
x = {xn}, y = {yn} ∈ℓ∞, deﬁne d(x, y) = supn |xn −yn|. (a) Show that d deﬁnes a
metric on ℓ∞. (b) If en denotes the sequence with a 1 in the n-th place and zeros else-
where, show that B(en; 1
2) ∩B(em; 1
2) = ∅when n ̸= m. (c) Is the set {en : n ≥1}
closed?
(13)
Show that the metric space ℓ∞deﬁned in Exercise 12 is complete.
(14)
If A ⊆X, show that int A = {x : dist (x, X\A) > 0}. Can you give an analogous
characterization of ∂A?
(15)
(a) If A ⊆X, show that x ∈cl A if and only if x is either a limit point of A or an
isolated point of A. (b) Show that if a set has no limit points, it is closed. (c) Give
an example of an inﬁnite subset of R that has no limit points.
(16)
If (X, d) is a complete metric space, show that if {Fn} is a sequence of bounded
closed subsets of X satisfying (i) and (ii) in Cantor’s Theorem, then ∞
n=1 Fn ̸= ∅.
(17)
Prove Proposition 5.3.19.
5.4. Continuity
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Here we will extend the concept of a continuous function seen in Chapter 1
to a mapping between two metric spaces and investigate the properties of such
functions.

5.4 Continuity
129
5.4.1. Definition. If (X, d) and (Z, ρ) are two metric spaces, a function f : X →
Z is continuous at a point a in X if whenever {xn} is a sequence in X that converges
to a, f (xn) →f (x) in Z. f is said to be a continuous function if it is continuous at
each point of X
Note that if X = Z = R, then this is the same as the deﬁnition of continuity found
in §1.7. The most common situation we will encounter in the remainder of this book
is where X is a subset of Rp and Z = Rq. To establish results in such situations,
however, requires no more eﬀort than to prove them in the setting described in the
deﬁnition. The next few results should have a familiar ring from §1.7.
5.4.2. Proposition. If (X, d) and (Z, ρ) are metric spaces and f : X →Z, then f
is continuous at a if and only if for every ϵ > 0 there is a δ > 0 such that when
d(a, x) < δ it follows that ρ( f (a), f (x)) < ϵ.
Proof. Suppose f is continuous at a and, to the contrary, there is an ϵ > 0 such
that for every δ > 0 there is at least one x with d(x, a) < δ and ρ( f (x), f (a)) ≥ϵ.
In particular taking δ = n−1 we have that for every n ≥1 there is an xn with
d(xn, a) < n−1 and ρ( f (xn), f (a)) ≥ϵ. But this says that xn →a and { f (xn)} does
not converge to f (a), contradicting the assumption of continuity.
Now assume that for every ϵ > 0 there is a δ > 0 as stated in the proposition.
If xn →a in X, and ϵ > 0, let δ > 0 such that when d(a, x) < δ it follows that
ρ( f (a), f (x)) < ϵ. Let N be an integer such that d(xn, a) < δ when n ≥N. So
ρ( f (xn), f (a)) < ϵ when n ≥N. Since ϵ was arbitrary, this says that f (xn) →f (a)
and f is continuous at a.
■
Let’s mention that the previous result is often taken as the deﬁnition of a continu-
ous function. It is this equivalent formulation of continuity that generalizes to more
abstract structures than a metric space.
We won’t spend any time investigating functions continuous at a single point, but
we will have much to say about functions continuous on the entire metric space,
especially when that metric space is some subset of Rp.
5.4.3. Theorem. If (X, d) and (Z, ρ) are metric spaces and f : X →Z, then the
following statements are equivalent.
(a) f is a continuous function on X.
(b) If U is an open subset of Z, then f −1(U) is an open subset of X.
(c) If D is a closed subset of Z, then f −1(D) is a closed subset of X.
Proof. (b) is equivalent to (c). Note that
f −1(Z\U) = X\ f −1(U) and f −1(Z\D) = X\ f −1(D)
From these equalities the equivalence of the two statements is straightforward.
(a) implies (b). Let a ∈f −1(U) so that α = f (a) ∈U. Since U is open there is
an ϵ > 0 such that B(α; ϵ) ⊆U. Since f is continuous there is a δ > 0 such that

130
Metric and Euclidean Spaces
d(a, x) < δ implies ρ( f (a), f (x)) < ϵ. In other words, B(a; δ) ⊆f −1(B(α; ϵ)) ⊆
f −1(U). Since a was an arbitrary point in f −1(U), this says that f −1(U) is open.
(b) implies (a). If a ∈X and ϵ > 0, then B( f (a); ϵ) is open; so by (b) we have
that f −1(B( f (a); ϵ)) is an open set in X that contains a. Thus there is a δ > 0 such
that B(a; δ) ⊆f −1(B( f (a); ϵ). That is, d(a, x) < δ implies ρ( f (a), f (x)) < ϵ and
so f is continuous at a.
■
See Exercise 5. The next result extends Proposition 1.7.4 to the present setting.
The same proof given there applies here as the reader will see when (s)he does a
step-by-step process of translation.
5.4.4. Proposition. The composition of two continuous functions is also continu-
ous.
The proof of the next proposition is elementary.
5.4.5. Proposition. If (X, d) is a metric space, f , g : X →Rp are continuous, and
α ∈R, then:
(a) ( f + g) : X →Rp deﬁned by ( f + g)(x) = f (x) + g(x) is continuous;
(b) (α f ) : X →Rp deﬁned as (α f )(x) = α f (x) is continuous; and
(c) the function from X into R deﬁned by x →⟨f (x), g(x)⟩is continuous.
Now to generate some examples of continuous functions. Of course §1.7 fur-
nishes several examples of continuous functions from the real line into itself. We
also have the trigonometric functions, the exponential, and logarithm.
5.4.6. Example. (a) The function from Rp × Rp →Rp deﬁned by (x, y) →x + y
(vector addition) is continuous.
(b) The function from R × Rp →Rp deﬁned by (t, x) →tx (scalar multiplica-
tion) is continuous.
(c) If fk : X →R is a continuous function for 1 ≤k ≤p, then f : X →Rp
deﬁned by f (x) = ( f1(x), . . . , fp(x)) is a continuous function. (Exercise 7(b)).
(d) If fk : X →Rp is a continuous function for 1 ≤k ≤m, then f : X →Rp
deﬁned by f (x) = m
k=1 fk(x) is a continuous function. (Exercise 7(c)).
(e) If (X, d) is any metric space, x0 ∈X, and we deﬁne f : X →R by f (x) =
d(x, x0), then f is continuous. In fact the reverse triangle inequality (5.1.3) says
that | f (x) −f (y)| ≤d(x, y), from which continuity follows by either the deﬁnition
or Proposition 5.4.2.
(f) If (X, d) is the discrete metric space, then the only continuous functions from
[0, 1] into (X, d) are the constant functions. On the other hand, for any metric space
(Z, ρ) every function f : (X, d) →(Z, ρ) is continuous.
(g) Consider Rp, 1 ≤q < p, and integers 1 ≤n1 < n2 < · · · nq ≤p. The func-
tion π : Rp →Rq deﬁned by π(x1, . . . , xp) = (xn1, . . . , xnq) is continuous. (Exer-
cise 7(e)).
Recall the deﬁnition of the distance from a point to a set A, dist (x, A) (5.3.16).

5.4 Continuity
131
5.4.7. Proposition. If (X, d) is a metric space and A ⊆X, then
|dist (x, A) −dist (y, A)| ≤d(x, y)
for all x, y in X.
Proof. If a ∈A, d(x, a) ≤d(x, y) + d(y, a); so taking the inﬁmum over all a in A
we get dist (x, A) ≤inf{d(x, y) + d(y, a) : a ∈A} = d(x, y) + dist (y, A). Revers-
ing the roles of x and y we have dist (y, A) ≤d(x, y) + dist (x, A), whence we get
the inequality.
■
5.4.8. Corollary. If A is a non-empty subset of X, then f : X :→R deﬁned by
f (x) = dist (x, A) is a continuous function.
In the next proof we’ll use the preceding results to construct continuous functions
with speciﬁed behavior.
5.4.9. Theorem (Urysohn’s3 Lemma). If A and B are two disjoint closed subsets of
X, then there is a continuous function f : X →R having the following properties:
(a) 0 ≤f (x) ≤1 for all x in X;
(b) f (x) = 0 for all x in A; and
(c) f (x) = 1 for all x in B.
Proof. Deﬁne f : X →R by
f (x) =
dist (x, A)
dist (x, A) + dist (x, B)
which is well-deﬁned since the denominator never vanishes (Why?). It is easy to
check that f has the desired properties.
■
Don’t be fooled by the simple proof; the result is powerful. We did a lot of work
before this theorem to be able to concoct such a simple proof.
5.4.10. Corollary. If F is a closed subset of X and G is an open set containing F,
then there is a continuous function f : X →R such that 0 ≤f (x) ≤1 for all x in
X, f (x) = 1 when x ∈F, and f (x) = 0 when x /∈G.
Proof. In Urysohn’s Lemma, take A to be the complement of G and B = F.
■
Now we extend the concept of a uniformly continuous function to the metric
space setting. The deﬁnition is just a transliteration of the deﬁnition in the case
of R.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
3 Pavel Samuilovich Urysohn was born in 1898 in Odessa, Ukraine. He was awarded his habilitation in June 1921
from the University of Moscow, where he remained as an instructor. He began his work in analysis but switched
to topology where he made several important contributions, especially in developing a theory of dimension. His
work attracted the attention of the mathematicians of the day, and in 1924 he set out on a tour of the major
universities in Germany, Holland, and France, meeting with Hausdorﬀ, Hilbert, and others. That same year,
while swimming oﬀthe coast of Brittany, France, he drowned. He is buried in Batz-sur-Mer in Brittany. In just
three years he left his mark on mathematics.

132
Metric and Euclidean Spaces
5.4.11. Definition. A function f : (X, d) →(Z, ρ) between two metric spaces is
uniformly continuous if for every ϵ > 0 there is a δ such that ρ( f (x), f (y)) < ϵ
when d(x, y) < δ.
5.4.12. Example. (a) We also extend the concept of a Lipschitz function. A func-
tion f : (X, d) →(Z, ρ) is a Lipschitz function if there is a constant M > 0 such that
ρ( f (x), f (y)) ≤Md(x, y) for all x, y in X. A ready collection of examples occurs
by letting I be an interval in R and letting f : I →R be a continuously diﬀeren-
tiable function with | f ′(x))| ≤M for all x in I. Thus | f (x) −f (y)| = |
 x
y f ′(t)dt| ≤
 x
y | f ′(t)|dt ≤M|x −y|. As we saw in Proposition 1.7.14, every Lipschitz function
is uniformly continuous and the same is true for functions on a metric space.
(b) We note that when A ⊆X, the function x →dist (x, A) is a Lipschitz function
by Proposition 5.4.7. Thus the distance function gives rise to a plentiful source of
uniformly continuous functions on any metric space.
Exercises
(1)
Let (X, d) and (Z, ρ) be metric spaces, A ⊆X, f : A →(X, d), and let a be a limit
point of A. Say that limx→a f (x) = z if for every ϵ > 0 there is a δ > 0 such that
ρ( f (x), z) < ϵ when 0 < d(x, a) < δ. Fix the set A and its limit point a. (a) If a ∈A,
show that f is continuous at a if limx→a f (x) = f (a). (b) Show that limx→a f (x) = z
if and only if for every sequence {xn} in A that converges to a we have that f (xn) →z.
(2)
If (X, d) is a metric space, f : B(a; r) →R is continuous at a with f (a) = 0, and
g : B(a; r) →R is a function satisfying |g(x)| ≤M for some constant M and all x
in X (but not necessarily continuous), then fg is continuous at a.
(3)
If f : (X, d) →(Z, ρ) is continuous, A is a dense subset of X, and z ∈Z such that
f (a) = z for every a in A, show that f (x) = z for every x in X.
(4)
If f : (X, d) →(Z, ρ) is both continuous and surjective and A is a dense subset of
X, show that f (A) is a dense subset of Z.
(5)
Prove the equivalence of (b) and (c) in Theorem 5.4.3 by using only the sequential
deﬁnition of continuity.
(6)
In Theorem 5.4.3, give an independent proof that shows that conditions (a) and
(c) are equivalent. (Here, “independent” means that the proof should not use the
equivalence of (a) and (b) or of (b) and (c).)
(7)
(a) Prove the statements made in parts (a) and (b) of Example 5.4.6. (b) Prove
(5.4.6(c)). (Hint: Write the function as the composition of continuous functions.)
(c) Prove (5.4.6(d)). (d) Prove the statement made in Example 5.4.6(f). (e) Prove
(5.4.6(g)).
(8)
If f : R2 →R2 is deﬁned by
f (x, y) =
	 x2−y2
x−y
if x ̸= y
x −y
if x = y
where is f continuous?

5.5 Compactness
133
(9)
If f and g are continuous functions from (X, d) into R prove that f ∨g(x) =
max{ f (x), g(x)} and f ∧g(x) = min{ f (x), g(x)} are continuous. (See Proposition
1.7.11.)
(10)
Is the composition of two uniformly continuous functions a uniformly continuous
function?
(11)
Note that Proposition 5.4.5 says that if (X, d) is a metric space and we deﬁne the
collection of all continuous functions f : X →R by C(X ), then C(X ) is a vector
space over R. Show that C(X ) is a ﬁnite dimensional vector space if and only if X
is a ﬁnite set. (Hint: use Urysohn’s Lemma.)
(12)
Let (X, d) be a metric space and let U denote the set of all uniformly continuous
functions from X into R. (a) If f , g ∈U, show that f + g ∈U. In words, U is a
vector space over R. (b) If f , g ∈U, show by an example that it does not necessarily
follow that fg ∈U. If, however, the functions are also bounded, then fg ∈U. (A
function f : X →(Z, ρ) is bounded if f (X ) is a bounded subset of Z.) (c) Can you
give some conditions under which the quotient of two functions in U is uniformly
continuous?
(13)
Is the function π deﬁned in Example 5.4.6(g) uniformly continuous?
(14)
If X,Y, Z are metric spaces and f : X × Y →Z, say that f is separately continuous
at (a, b) if the following hold: (i) the function from Y into Z deﬁned by y →f (a, y)
is continuous at b; (ii) the function from X into Z deﬁned by x →f (x, b) is contin-
uous at a. (a) Show that if f is continuous at (a, b), then it is separately continuous
at (a, b). (b) Show that the function f : R2 →R deﬁned by
f (x, y) =
	
xy
x2+y2
if (x, y) ̸= (0, 0)
0
if (x, y) = (0, 0)
is separately continuous at (0, 0) but not continuous there.
5.5. Compactness
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
To a large extent this section is concerned with extending the Bolzano–Weierstrass
Theorem to the setting of metric spaces. But the concept of compactness involves
much more. In fact as we progress we will develop other ideas that are useful in the
study of Rp and beyond.
If G is a collection of subsets of X and E ⊆X, then G is a cover of E if E ⊆
{G : G ∈G}. A subcover of E is a subset G1 of G that is also a cover of E. Finally
we say that G is an open cover of E if G is a cover and every set in the collection
G is open.
5.5.1. Definition. A subset K of the metric space (X, d) is said to be compact if
every open cover of K has a ﬁnite subcover.
We mention that the term “open cover” in this deﬁnition can be replaced by “cover
by subsets of K that are relatively open.” See Exercise 2.

134
Metric and Euclidean Spaces
It is easy to ﬁnd examples of sets that are not compact. Speciﬁcally, the open
interval (0, 1) is not compact. In fact if we put Gn = (n−1, 1), then G = {Gn : n ∈
N} is an open cover of the interval that has no ﬁnite subcover. Similarly R is not
compact since {(−n, n) : n ∈N} is an open cover of R that has no ﬁnite subcover.
We can easily see that every ﬁnite subset of X is compact, but ﬁnding non-trivial
examples of compact sets requires us to ﬁrst prove some results.
5.5.2. Proposition. Let (X, d) be a metric space.
(a) If K is a compact subset of X, then K is closed and bounded.
(b) If K is compact and F is a closed set contained in K, then F is compact.
(c) The continuous image of a compact subset is a compact subset. That is, if f :
(X, d) →(Z, ρ) is continuous and K is a compact subset of X, then f (K) is a
compact subset of Z.
Proof. (a) If x /∈K, then for each z in K let rz, sz > 0 such that B(z; rz) ∩B(x; sz) =
∅. Now {B(z; rz) : z ∈K} is an open cover of K. Since K is compact, there are points
z1, . . . , zn in K such that K ⊆n
k=1 B(zk; rzk ). Let s = min{szk : 1 ≤k ≤n}. Note
that B(x; s) ∩K = ∅; in fact, if there is a y in B(x; s) ∩K, then there is a k such that
y ∈B(x; s) ∩B(zk; rzk ) ⊆B(x; szk ) ∩B(zk; rzk ), which contradicts the choice of the
numbers szk and rzk. Therefore B(x; s) ⊆X\K. Since x was arbitrary, this says that
X\K is open and so K is closed. Also for any point x0 in X, {B(x0; n) : n ∈N} is
an open cover of K; hence there is a ﬁnite subcover. But the sets in this cover are
increasing, so there is an integer n such that K ⊆B(x0; n) and so K is bounded.
(b) Let G be an open cover of F and observe that since F is closed, {X\F} ∪G is
an open cover of K. The existence of a ﬁnite subcover of K implies there is a ﬁnite
sub-collection of G that covers F.
(c) Let f : X →(Z, ρ) be a continuous function and assume K is a compact
subset of X; we want to show that f (K) is a compact subset of Z. Let U be an open
cover of f (K) in (Z, ρ). Since f is continuous it follows that G = { f −1(U) : U ∈U}
is an open cover of K (5.4.3(b)). Therefore there are sets U1, . . . ,Un in U such that
K ⊆n
k=1 f −1(Uk). It follows that f (K) ⊆n
k=1 Uk.
■
We can use the preceding proposition to prove the EVT for continuous functions
on a compact metric space.
5.5.3. Corollary. If (X, d) is a compact metric space and f : X →R is a contin-
uous function, then there are points a and b in X such that f (a) ≤f (x) ≤f (b) for
all x in X.
Proof. We have from Proposition 5.5.2 that f (X ) is a closed and bounded subset of
R. Put α = inf{ f (x) : x ∈X}, β = sup{ f (x) : x ∈X}. Since f (X ) is closed, α, β ∈
f (X ); this proves the corollary.
■
Before extending the Bolzano–Weierstrass Theorem, we need two more
deﬁnitions.

5.5 Compactness
135
5.5.4. Definition. Say that a subset K of the metric space (X, d) is totally bounded
if for any radius r > 0 there are points x1, . . . , xn in K such that K ⊆n
k=1 B(xk; r).
A collection F of subsets of K has the ﬁnite intersection property or FIP if whenever
F1, . . . , Fn ∈F, n
k=1 Fk ̸= ∅.
The following is the main result on compactness in metric spaces.
5.5.5. Theorem. The following statements are equivalent for a closed subset K of
a metric space (X, d).
(a) K is compact.
(b) If F is a collection of closed subsets of K having the FIP, then

F∈F
F ̸= ∅.
(c) Every sequence in K has a convergent subsequence.
(d) Every inﬁnite subset of K has a limit point.
(e) (K, d) is a complete metric space that is totally bounded.
Proof. (a) implies (b). Let F be a collection of closed subsets of K having the FIP.
Suppose {F : F ∈F} = ∅. If G = {X\F : F ∈F}, then it follows that G is an
open cover of X and therefore of K. By (a), there are F1, . . . , Fn in F such that
K ⊆n
j=1(X\Fj) = X\[n
j=1 Fj]. But since each Fj is a subset of K this implies
n
j=1 Fj = ∅, contradicting the fact that F has the FIP.
(b) implies (a). Let G be an open cover of K and put F = {K\G : G ∈G}. Since
G covers K, {K\G : G ∈G} = ∅. Thus F cannot have the FIP and there must be
a ﬁnite number of sets G1, . . . , Gn in G with ∅= n
j=1(K\Gj). But this implies
that {G1, . . . , Gn} is a ﬁnite cover of K. Hence K is compact.
(d) implies (c). Assume {xn} is a sequence of distinct points in K. By (d), {xn}
has a limit point; since K is closed, that limit point must be in K. We are tempted
here to invoke (5.3.15(a)), but we have to manufacture an actual subsequence of the
original sequence. That is, we must ﬁnd {xnk} such that xnk →x and n1 < n2 < · · ·
This takes a little bit of care and eﬀort, which we leave to the interested reader.
(c) implies (d). If S is an inﬁnite subset, then S has a sequence of distinct points
{xn}; by (c) there is a subsequence {xnk} that converges to some point x. It follows
that x is a limit point of S. (Details?)
(a) implies (d). Assume that (d) is false. So there is an inﬁnite subset S of K
with no limit point; it follows that there is an inﬁnite sequence {xn} in S with no
limit point. Thus for each n ≥1, Fn = {xk : k ≥n} contains all its limit points and
is therefore closed. Also ∞
n=1 Fn = ∅. But each ﬁnite subcollection of {F1, F2, . . . }
has non-empty intersection, contradicting (b), which is equivalent to (a).
(a) implies (e). First let {xn} be a Cauchy sequence in K. Since (a) implies (d),
which is equivalent to (c), there is an x in K and a subsequence {xnk} such that
xnk →x. But this implies xn →x by Proposition 5.2.5. Hence (K, d) is complete.
To show that K is totally bounded, just note that {B(x; r) : x ∈K} is an open cover
of K for any r > 0.

136
Metric and Euclidean Spaces
(e) implies (c). Fix an inﬁnite sequence {xn} in K and let {ϵn} be a decreasing
sequence of positive numbers such that ϵn →0. By (e) there is a covering of K by
a ﬁnite number of balls of radius ϵ1. Thus there is a ball B(y1; ϵ1) that contains an
inﬁnite number of points from {xn}; let N1 = {n ∈N : d(xn, y1) < ϵ1}. Now con-
sider the sequence {xn : n ∈N1} and balls of radius ϵ2. As we just did, there is
a point y2 in K such that N2 = {n ∈N1 : d(y2, xn) < ϵ2} is an inﬁnite set. Using
induction we can show that for each k ≥1 we get a point yk in K and an inﬁnite
set of positive integers Nk such that Nk+1 ⊆Nk and {xn : n ∈Nk} ⊆B(yk; ϵk). If
Fk = cl {xn : n ∈Nk}, then Fk+1 ⊆Fk and diam Fk ≤2ϵk. Since K is complete, Can-
tor’s Theorem implies that ∞
k=1 Fk = {x} for some point x in X. Now, using a small
induction argument, pick integers nk in Nk such that nk < nk+1. It follows that {xnk}
is a subsequence of the original sequence and xnk →x.
(e) implies (a). We ﬁrst prove the following.
5.5.6. Claim. If K satisﬁes (c) and G be an open cover of K, then there is an r > 0
such that for each x in K there is a G in G such that B(x; r) ⊆G.
Let G be an open cover of K and suppose the claim is false; so for every n ≥1
there is an xn in K such that B(xn; n−1) is not contained in any set G in G. By (c) there
is an x in K and a subsequence {xnk} such that xnk →x. Since G is a cover, there is
a G in G such that x ∈G; choose a positive ϵ such that B(x; ϵ) ⊆G. Let nk > 2ϵ−1
such that xnk ∈B(x; ϵ/2). If y ∈B(xnk; n−1
k ), then d(x, y) ≤d(x, xnk ) + d(xnk, y) <
ϵ/2 + n−1
k
< ϵ, so that y ∈B(x; ϵ) ⊆G. Thus B(xnk; n−1
k ) ⊆G, contradicting the
restriction imposed on xnk. This establishes the claim.
From here it is easy to complete the proof. We know that (e) implies (c), so for
an open cover G of K let r > 0 be the number guaranteed by Claim 5.5.6. Now let
x1, . . . , xn ∈K such that K ⊆n
k=1 B(xk; r) and for 1 ≤k ≤n let Gk ∈G such that
B(xk; r) ⊆Gk. {G1, . . . , Gn} is the sought after ﬁnite subcover.
(c) implies (e). If {xn} is a Cauchy sequence in K, then (c) implies it has a con-
vergent subsequence; by Proposition 5.2.5 the original sequence converges. Thus
(K, d) is complete. Now ﬁx an r > 0. Let x1 ∈K; if K ⊆B(x1; r), we are done. If
not, then there is a point x2 in K\B(x1; r). Once again, if K ⊆B(x1; r) ∪B(x2; r),
we are done; otherwise pick an x3 in K\[B(x1; r) ∪B(x2; r)]. Continue. If this pro-
cess does not stop after a ﬁnite number of steps, we produce an inﬁnite sequence
{xn} in K with d(xn, xm) ≥r whenever n ̸= m. But this implies that this sequence
can have no convergent subsequence, contradicting (c).
■
As an application we tweak Cantor’s Theorem (5.3.18) and address a question
that arose in connection with it.
5.5.7. Corollary. If (X, d) is compact and {Fn} is a sequence of non-empty closed
subsets of X such that F1 ⊇F2 ⊇· · · , then ∞
n=1 Fn ̸= ∅.
The proof of this corollary is clear since a decreasing sequence of closed non-
empty sets has the FIP.

5.5 Compactness
137
Compactness is one of the most important properties in mathematics. Many good
things follow from it as we shall see in this book and the reader will continue to see
as (s)he continues his/her career. The point is that compact sets are “almost” ﬁnite
in a very precise sense, and this approximation of ﬁniteness often suﬃces to allow
us to carry out an argument for a compact set that we can easily make for a ﬁnite
set. We already saw this in Corollary 5.5.3.
We might observe that the Bolzano–Weierstrass Theorem together with Theo-
rem 5.5.5 show that every closed and bounded interval in R is a compact set. By
Proposition 5.5.2 the converse is also true. The next result extends this to Rp.
5.5.8. Theorem (Heine4–Borel5 Theorem). A subset of Rp is compact if and only
if it is closed and bounded.
Proof. If K is compact, then K is closed and bounded by Proposition 5.5.2. Now
assume that K is closed and bounded. It follows that there are bounded intervals
[a1, b1], . . . , [ap, bp] in R such that K ⊆[a1, b1] × · · · × [ap, bp]. Suppose {xn} is
a sequence in K with xn = (x1
n, . . . , xp
n). Thus {x1
n} is a sequence in [a1, b1], so
the Bolzano–Weierstrass Theorem implies it has a convergent subsequence. The
notation in this proof could become grotesque if we do the standard things, so
we depart from the standard. Denote the convergent subsequence by {x1
n : n ∈N1},
where N1 is an inﬁnite subset of N with its natural ordering. We have that the
limit exists, so put x1 = limn∈N1 x1
n. Now consider the sequence {x2
n : n ∈N1} in
[a2, b2]. It has a convergent subsequence {x2
n : n ∈N2} with x2 = limn∈N2 x2
n. Con-
tinue and we get Np ⊆· · · ⊆N1 ⊆N and xk = limn∈Nk xk
n for 1 ≤k ≤p. It follows
that {xn : n ∈Np} is a subsequence of the original sequence and it converges to
x = (x1, . . . , xp); it must be that x ∈K since K is closed. By Theorem 5.5.5, K is
compact.
■
5.5.9. Example. For the metric space Q, if a, b ∈Q, a < b, the set F = {x ∈Q :
a ≤x ≤b} = Q ∩[a, b] is closed and bounded but not compact.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
4 Heinrich Eduard Heine was born in 1821 in Berlin, the eighth of nine children. He received his doctorate in 1842
from Berlin; in 1844 he received the habilitation at Bonn where he was appointed a privatdozent. In 1856 he was
made professor at the University of Halle, where he remained for the rest of his career. In 1850 he married Sophie
Wolﬀfrom Berlin, and, over the years, they had ﬁve children. He worked on partial diﬀerential equations and
then on special functions – Legendre polynomials, Lam functions, and Bessel functions. He made signiﬁcant
contributions to spherical harmonics and introduced the concept of uniform continuity. It was in 1872 that he
gave a proof of the present theorem. It requires some scholarship to discover the diﬀerences in the contribution
to this result between him and Borel, who published it in 1895. He died in 1881 in Halle.
5 Emile Borel was born in Saint Aﬀrique in the south of France in 1871. He published his ﬁrst two papers in
1890, two years before receiving his doctorate in Paris and joining the faculty at Lille. He returned to Paris in
1897. In 1909 a special chair in the Theory of Functions was created for him at the Sorbonne. During World
War I he was very supportive of his country and was put in charge of a central department of research. He also
spent time at the front and in 1918 he was awarded the Croix de Guerre. In 1928 he set up the Institute Henri
Poincaré. He was one of the founders of the modern theory of functions along with Baire and Lebesgue and
he also worked on divergent series, complex variables, probability, and game theory. He continued to be very
active in the French government, serving in the French Chamber of Deputies (1924–36) and as Minister of the
Navy (1925–40). He died in 1956 in Paris.

138
Metric and Euclidean Spaces
The next theorem extends Theorem 1.7.16. In fact the proof of that result
will also extend to the present setting. The details are left to the reader in
Exercise 9.
5.5.10. Theorem. If (X, d) is a compact metric space and f : X →(Z, ρ) is a
continuous function, then f is uniformly continuous.
5.5.11. Proposition. A compact metric space is separable and complete.
Proof. The completeness of (X, d) is explicit in Theorem 5.5.5(e). To prove that
(X, d) is separable again Theorem 5.5.5(e) implies that for each natural number n
we can ﬁnd a ﬁnite set Fn such that X = {B(x; n−1) : x ∈Fn}. Put F = ∞
n=1 Fn;
we will show that this countable set F is dense in X. In fact if x0 is an arbitrary point
in X and ϵ > 0, choose n such that n−1 < ϵ. Thus there is a point x in Fn ⊆F with
d(x0, x) < n−1 < ϵ, proving that x0 ∈cl F.
■
In light of the preceding proposition any closed and bounded subset of Rp is
separable.
5.5.12. Theorem.
Let (X, d) and (Z, ρ) be compact metric spaces and assume
D is a dense subset of X. If f : D →Z is uniformly continuous, then there is a
uniformly continuous function F : X →Z that extends f ; that is, the function F
satisﬁes F(x) = f (x) for all x in D.
Proof. If x ∈X, let {xn} be a sequence in D that converges to x. We claim
that { f (xn)} is a Cauchy sequence in Z. In fact, if ϵ > 0, let δ > 0 such that
ρ( f (y), f (w)) < ϵ when y, w ∈D and d(y, w) < δ. Now there is an integer N
with d(xn, xm) < δ for m, n ≥N. Hence ρ( f (xn), f (xm)) < ϵ when m, n ≥N. Thus
{ f (xn)} is a Cauchy sequence in Z. By Proposition 5.5.11, (Z, ρ) is complete, so
there is a ζ in Z such that f (xn) →ζ. If {yn} is another sequence in D that converges
to x, then the same argument shows there is a ζ ′ in Z such that f (yn) →ζ ′. But we
have that d(xn, yn) →0, so the uniform continuity of f implies ρ( f (xn), f (yn)) →
0 (Why?). Thus ζ = ζ ′. This means we can deﬁne a function F : X →Z by letting
F(x) = limn f (xn) for any sequence {xn} in D that converges to x. Clearly F is an
extension of f .
Now to show that this function F is uniformly continuous. Let ϵ > 0. Choose
δ > 0 such that ρ( f (x), f (y)) < ϵ when x, y ∈D and d(x, y) < δ. Fix points x and
y in X with d(x, y) < δ/3 and let {xn} and {yn} be sequences in D that converge
to x and y. Choose N such that d(xn, x) < δ/3 and d(yn, y) < δ/3 when n ≥N.
So for all n ≥N we have that d(xn, yn) ≤d(xn, x) + d(x, y) + d(y, yn) < δ. Hence
ρ( f (xn), f (yn)) < ϵ when n ≥N. it follows that ρ(F(x), F(y)) ≤ϵ. Therefore F
is uniformly continuous.
■
The above proposition fails if f is not uniformly continuous. See Example 5.6.13
in the next section.

5.5 Compactness
139
5.5.13. Theorem (Dini’s6 Theorem). If (X, d) is compact, { fn} is an increasing
sequence of continuous functions on X, and f is a continuous function on X such
that fn(x) →f (x) for all x in X, then fn →u f on X.
Proof. Let ϵ > 0 and for each n ≥1 let Fn = {x ∈X : f (x) ≥fn(x) + ϵ}. We note
that because both f and fn are continuous, Fn is a closed subset of X. Also because
the sequence of functions is increasing, F1 ⊇F2 ⊇· · · . We want to show that there
is an integer N such that FN = ∅. In fact if this is done, then for all n ≥N we have
that fn(x) ≤f (x) < fn(x) + ϵ for all x in [a, b]. That is, | f (x) −fn(x)| < ϵ for all x
in X and all n ≥N, establishing that fn →u f on X. But if there is an x in ∞
n=1 Fn,
then x ∈Fn for every n ≥1. This says that f (x) ≥fn(x) + ϵ for all n, contradicting
the fact that fn(x) →f (x).
■
Exercises
(1)
Show that the union of a ﬁnite number of compact sets is compact.
(2)
If K is a subset of (X, d), show that K is compact if and only if every cover of K by
relatively open subsets of K has a ﬁnite subcover.
(3)
Show that the closure of a totally bounded set is totally bounded.
(4)
Show that a totally bounded set is bounded. Is the converse true?
(5)
If {En} is a sequence of totally bounded sets such that diam En →0, show that
∞
n=1 En is totally bounded.
(6)
If (X, d) is a complete metric space and E ⊆X, show that E is totally bounded if
and only if cl E is compact.
(7)
(a) If G is an open set and K is a compact set with K ⊆G, show that there is a δ > 0
such that {x : dist (x, K) < δ} ⊆G. (b) Find an example of an open set G in a metric
space X and a closed, non-compact subset F of G such that there is no δ > 0 with
{x : dist (x, F) < δ} ⊆G.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
6 Ulisse Dini was born in 1845 in Pisa. He attended Scuola Normale Superiore in Pisa, a teaching preparatory
college. In 1865 he won a scholarship for study abroad, which he used to go to Paris for a year. During this time
he was very active in research, eventually publishing seven papers based on the work he had done. He returned
to Pisa and an academic position at the university. Dini’s life span was a period of myriad political developments
in Italy as the country worked its way toward uniﬁcation. This is not a period for the casual historian. In 1859
there was a war with Austria and in 1861 the Kingdom of Italy was formed, though it did not include Venice and
Rome. (Can you imagine Italy without Venice and Rome?) It was not until 1866 that Venice became part of the
kingdom and Rome had to wait until 1870. The turmoil aﬀected Dini as he progressed in both his academic as
well as a political career. In 1871 he took over Betti’s chair of analysis, and that same year he was elected to the
Pisa City Council. In 1877 he was appointed to a second chair in mathematics, and in 1880 he was elected as
a representative of Pisa to the national assembly. In 1883 he was appointed rector of the university, holding the
position for two years. In 1892 he was elected a senator in the Italian Parliament; and in 1908 he became director
of the Scuola Normale Superiore, a position he held for the rest of his life. This was a period of development
in mathematical analysis when the turmoil seemed to be trying to parody the events in Italy; mathematicians
sought rigorous proofs of results that had only casually been established, and they sought the boundaries of
validity for these results. Dini seemed to ﬂourish in this undertaking. In addition to the present result, there is
one in Fourier series that bears his name. He also wrote several inﬂuential texts. He died in 1918 in Pisa.

140
Metric and Euclidean Spaces
(8)
If K is a compact set in (X, d) and G is an open set such that K ⊆G, show that there
is an open set G1 such that cl G1 is compact and K ⊆G1 ⊆cl G1 ⊆G.
(9)
Give the details of the proof of Theorem 5.5.10.
(10)
For two subsets A and B of X, deﬁne the distance from A to B by
dist (A, B) = inf{d(a, b) : a ∈A, b ∈B}. (a) Show that dist (A, B) = dist (B, A) =
dist (cl A, cl B). (b) If A and B are two disjoint closed subsets of X such that B is
compact, then dist (A, B) > 0. (c) Give an example of two disjoint closed subsets
A and B of the plane R2 such that dist (A, B) = 0. (d) Is this exercise related to
Exercise 7?
(11)
Consider the metric space ℓ∞(see Exercise 5.3.12) and show that
&
x = {xn} ∈ℓ∞: sup
n
|xn| ≤1
'
is not totally bounded and, therefore, not compact.
(12)
Say that a metric space is σ-compact if it can be written as the union of a countable
number of compact sets. (a) Give three examples of σ-compact metric spaces that
are not compact. (b) Show that a σ-compact metric space is separable.
5.6. Connectedness
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Consider the following two examples of subsets of R. The ﬁrst is the set X =
[0, 1] ∪(2, 3) and the second is Y = [0, 1] ∪(1, 2). In X there are two distinct
“parts,” [0, 1] and (2, 3). In the second we have written Y as the union of two dis-
joint sets, but these two sets are not really separate “parts.” (The term “parts” will
be made technically precise soon, though we won’t use that term.) In a sense writing
Y as the union of those two sets is just accidental. We could just as well have writ-
ten Y = [0, 1) ∪[1, 2) or Y = [0, 1
2] ∪( 1
2, 2) or even Y = [0, 2). What’s the true
diﬀerence between the two sets X and Y?
Note that in the metric space (X, d) of the last paragraph, the set [0, 1] is simul-
taneously both an open and a closed subset of X. For example, BX (1; 1
2) = {x ∈X :
|x −1| < 1
2} = ( 1
2, 1] ⊆[0, 1]. The set X is an example of what we now deﬁne as
a set that is not connected, or, more succinctly, a disconnected set.
5.6.1. Definition. A metric space (X, d) is connected if there are no subsets of X
that are simultaneously open and closed other than X and ∅. If E ⊆X, we say that
E is connected if (E, d) is connected. If E is not connected we will say that it is
disconnected or a non-connected set.
An equivalent formulation of connectedness is to say that (X, d) is connected
provided that when X = A ∪B where A ∩B = ∅and both A and B are open (or
closed), then either A = ∅or B = ∅. This is the sense of our use of the term “parts”
in the introduction of this section; for the set X there, A = [0, 1] and B = (2, 3) are
two disjoint, non-trivial sets that are both open and closed in X. Let’s collect some

5.6 Connectedness
141
of these equivalent formulations of connectedness. The proof is left to the reader
(Exercise 1); we will often use this result without referencing it.
5.6.2. Proposition. If (X, d) is a metric space, the following statements are equiv-
alent.
(a) X is connected.
(b) If X = A ∪B where A ∩B = ∅and both A and B or open (or closed), then
either A = ∅or B = ∅.
(c) If A ⊆X, A ̸= ∅, and A is both open and closed, then A = X.
The next result can be considered an example, but it is much more than that.
5.6.3. Proposition. A subset of R is connected if and only if it is an interval.
Proof. Assume that X = [a, b] and let’s show that X is connected. (The proof that
other types of intervals are connected is Exercise 2.) Assume that [a, b] = A ∪B,
where both A and B are open and A ∩B = ∅. One of these sets contains the point
a; suppose a ∈A. Note that A is also closed. We want to show that A = X. Since A
is open there is an ϵ > 0 such that [a, a + ϵ) ⊆A. Put r = sup{ϵ : [a, a + ϵ) ⊆A}.
We claim that [a, a + r) ⊆A. In fact if a ≤x < a + r then the deﬁnition of the
supremum implies there is an ϵ > 0 such that ϵ < r, [a, a + ϵ) ⊆A, and a ≤x <
a + ϵ; thus x ∈A. Now A is also closed and [a, a + r) ⊆A, so it must also be that
a + r ∈A. If a + r ̸= b, then the fact that A is open implies that there is a δ > 0 such
that (a + r −δ, a + r + δ) ⊆A. But this means that [a, a + r + δ) = [a, a + r) ∪
(a + r −δ, a + r + δ) ⊆A, contradicting the deﬁnition of r. Therefore a + r = b
and we have that A = [a, b] = X. (So B = ∅.)
Recall that by Lemma 1.6.6 a subset I of R is an interval if and only if when-
ever a, b ∈I with a < b it follows that [a, b] ⊆I. Assume that X is a non-empty
connected subset of R and a, b ∈X with a < b; suppose a < c < b. If c /∈X, let
A = X ∩(−∞, c), B = X ∩(c, ∞). Clearly A and B are open subsets relative to
X; since c /∈X we also have that A = X ∩(−∞, c], B = X ∩[c, ∞), and they
are also closed relative to X. Since a ∈A and b ∈B, neither is empty, contradict-
ing the assumption that X is connected. Hence c ∈X. That is, [a, b] ⊆X and by
Lemma 1.6.6 X is an interval.
■
5.6.4. Theorem. The continuous image of a connected set is connected.
Proof. Let f : (X, d) →(Z, ρ) be a continuous function and E a connected subset
of X; we want to show that f (E) is a connected subset of Z. By replacing X with
E, we may assume X = E is connected; by replacing Z with f (E), we may assume
that f is surjective. We must now show that Z is connected. If D is a subset of Z that
is both open and closed, then the continuity of f implies f −1(D) is both open and
closed in X. Since X is connected, f −1(D) is either ∅or X. But since f is surjective,
this implies D is either ∅or Z. Thus Z is connected.
■

142
Metric and Euclidean Spaces
The preceding theorem together with Proposition 5.6.3 allows us to deduce the
IVT (1.7.7) in this setting.
5.6.5. Corollary (Intermediate Value Theorem). If f : (X, d) →R is continuous,
X is connected, a, b ∈f (X ) with a < b, then for any number c in the interval [a, b]
there is a point x in X with f (x) = c.
Proof. We know that f (X ) is a connected subset of R so that it must be an interval
(5.6.3). Since a, b ∈f (X ), it must be that [a, b] ⊆f (X ).
■
5.6.6. Example. (a) If x, y ∈Rp, then the straight line segment [x, y] ≡{ty + (1 −
t)x : 0 ≤t ≤1} is connected. In fact t →ty + (1 −t)x is a continuous function
from the unit interval into Rp. Since the unit interval is connected, so is its image
under this continuous mapping.
(b) In Rp the balls B(x; r) are connected. In fact let A be a non-empty subset of
B(x; r) that is both relatively open and closed and ﬁx a point y in A. If z ∈B(x; r),
then the line segment [y, z] ⊆B(x; r) and [y, z] is connected by part (a). But it fol-
lows that A ∩[y, z] is a non-empty subset of this line segment that is both relatively
open and relatively closed. Thus [y, z] ⊆A; in particular the arbitrary point z from
B(x; r) belongs to A so that A = B(x; r).
(c) Any circle in R2 is connected. In fact if X = {(x, y) ∈R2 : (x −a)2 + (y −
b)2 = r}, then γ : [0, 2π] →R2 deﬁned by γ (θ) = (a + r cos θ, b + r sin θ) is a
continuous function and γ ([0, 2π]) = X. By Theorem 5.6.4, X is connected.
(d) If (X, d) is the discrete metric space, then X is not connected if X has more
than one point. In fact each singleton set {x} is a non-empty set that is both open
and closed.
5.6.7. Proposition. Let (X, d) be a metric space.
(a) If {Ei : i ∈I} is a collection of connected subsets of X such that Ei ∩E j ̸= ∅for
all i, j in I, then E = 
i∈I Ei is connected.
(b) If {En : n ≥1} is a sequence of connected subsets of X such that En ∩En+1 ̸= ∅
for each n, then E = ∞
n=1 En is connected.
Proof. (a) Let A be a non-empty subset of E that is relatively open and closed. If
i ∈I, then A ∩Ei is a relatively closed and open subset of Ei; if A ∩Ei ̸= ∅, then
the fact that Ei is connected implies Ei ⊆A. Now since A is non-empty, there is at
least one i such that Ei ⊆A. But then for every j in I, the hypothesis implies there
is a point in E j that belongs to A; thus E j ⊆A. Therefore A = E and E must be
connected.
(b) Let A be a non-empty relatively open and closed subset of E. Since A ̸= ∅,
there is some integer N with A ∩EN ̸= ∅. But A ∩EN is both relatively open and
closed in EN, so EN ⊆A by the connectedness of EN. By hypothesis, EN−1 ∩
EN ̸= ∅, so EN−1 ∩A ̸= ∅and it follows that EN−1 ⊆A. Continuing we get that
En ⊆A for 1 ≤n ≤N. Since EN ∩EN+1 ̸= ∅, similar arguments show that EN+1 ⊆
A. Continuing we get that En ⊆A for all n ≥1. That is, E = A and so E is
connected.
■

5.6 Connectedness
143
5.6.8. Corollary. The union of two intersecting connected subsets of a metric space
is connected.
See Exercise 8.
5.6.9. Definition. If (X, d) is a metric space, a component of X is a maximal con-
nected subset of X.
The word “maximal” in the deﬁnition means that there is no connected set that
properly contains it. So if C is a component of X and D is a connected subset of X
with C ⊆D, then D = C.
A component is the correct interpretation of the word “part” used in the intro-
duction of this section. The set X in that introduction has two components. Notice
that a connected metric space has only one component. In the discrete metric space
each singleton set is a component.
5.6.10. Proposition. For any metric space, every connected set is contained in a
component, distinct components are disjoint, and the union of all the components
is the entire space.
Proof. Fix a connected subset D of X and let CD denote the collection of all con-
nected subsets of X that contain D. According to Proposition 5.6.7(a), C = {A :
A ∈CD} is connected. Clearly C is a component and contains D. By taking D = {x}
in what was just established, we have that every point of X is contained in a com-
ponent so that the union of all the components is X. Finally note that if C and D
are two components and C ∩D ̸= ∅, then C ∪D is connected by Corollary 5.6.8;
so it has to be that C = C ∪D = D by the maximality of C and D. That is, distinct
components are disjoint sets.
■
A consequence of the preceding proposition is that the components form a parti-
tion of X – they divide the space X into a collection of pairwise disjoint connected
sets. The next result says that the components are all closed, the proof of which
emphasizes once again that, when discussing relatively open and closed sets, you
must be aware of what the universe is.
5.6.11. Proposition. If C is a connected subset of the metric space X and C ⊆Y ⊆
clC, then Y is connected.
Proof. Let A be a non-empty subset of Y that is both relatively open and closed, and
ﬁx a point x0 in A. By Proposition 5.3.5 there is an open subset G of X such that A =
Y ∩G. Since x0 ∈clC and x0 ∈G, there must be a point x in G ∩C = A ∩C; that
is, A ∩C is a non-empty relatively open subset of C. Since A is relatively closed in
Y, Proposition 5.3.5 and an analogous argument implies that A ∩C is also relatively
closed in C. Since C is connected, C = C ∩A ⊆A. That is, C ⊆A ⊆Y ⊆clC so
that A is both closed in Y and dense in Y; hence A = Y, and it must be that Y is
connected.
■

144
Metric and Euclidean Spaces
5.6.12. Corollary. The closure of a connected set is connected and each component
is closed.
In light of the preceding proposition and Example 5.6.6(b), if x ∈Rp and
B(x; r) ⊆E ⊆B(x; r), then E is connected. Here is an example that will illustrate
additional properties as we proceed. In fact this example is used so often it has a
name, the topologist’s sine curve.
5.6.13. Example. X = {(x, sin x−1) ∈R2 : 0 < x ≤1} ∪{(0, 0)} is connected. In
fact, f : (0, 1] →X deﬁned by f (x) = (x, sin x−1) is a continuous function, soC =
f ((0, 1]) is connected. Since C ⊆X ⊆clC, X is connected by Proposition 5.6.11.
The space X consists of the graph of the function sin x−1 for 0 < x ≤1 together
with the origin. Note that instead of the origin we could have added to the graph
any subset of {(0, y) : −1 ≤y ≤1} and the resulting set would still be connected.
We now focus on Rp. Recall Proposition 1.6.7 where it is proved that every open
subset of R is the union of a countable number of pairwise disjoint open intervals.
We can interpret this as saying that the components of an open subset G of R are
open intervals, which are not closed subsets of R. They are, however, relatively
closed in G. We want to extend Proposition 1.6.7 to Rp; of course the extension is
not literal. We start with an idea in Rp that is useful.
5.6.14. Definition. If E ⊆Rp, x, y ∈E, and ϵ > 0, say that there is an ϵ-chain
from x to y in E when there are a ﬁnite number of points x1, . . . , xn in E such that:
(i) for 1 ≤k ≤n, B(xk; ϵ) ⊆E; (ii) for 2 ≤k ≤n, xk−1 ∈B(xk; ϵ); (iii) x1 = x and
xn = y.
The proof of the next lemma is Exercise 11.
5.6.15. Lemma. If r > 0 and z ∈Rp, then for any pair of points x and y in B(z; r)
and all suﬃciently small ϵ there is an ϵ-chain in B(z; r) from x to y.
5.6.16. Proposition. Consider the metric space Rp.
(a) If G is an open subset of Rp, then every component of G is open and there are
countably many components.
(b) An open subset G of Rp is connected if and only if for any x, y in G there is an
ϵ > 0 such that there is an ϵ-chain in G from x to y.
Proof. (a) If H is a component of G and x ∈H, choose r > 0 such that B(x; r) ⊆G.
Since B(x; r) is also connected (5.6.6), Corollary 5.6.8 implies H ∪B(x; r) is con-
nected. Since this is also a subset of G it follows that H = H ∪B(x; r), so B(x; r) ⊆
H and H is open. Because Rp is separable, there is a countable dense subset D.
Now since each component is open, each component contains an element of D and
diﬀerent components contain diﬀerent points. If there are uncountably many com-
ponents this would show there is an uncountable subset of D (Why?), which is
nonsense.

5.6 Connectedness
145
(b) Assume that the open set G satisﬁes the stated condition and let’s prove that G
is connected. Fix x and let H be the component of G that contains x; we want to show
that H = G. We know that H is open by part (a). If y ∈G there is an ϵ > 0 such that
there is an ϵ-chain x1, . . . , xn in G from x to y. Since xk−1 ∈B(xk−1; ϵ) ∩B(xk; ϵ)
for 2 ≤k ≤n, Proposition 5.6.7(b) says B = n
k=1 B(xk; ϵ) is connected. Condition
(i) of the deﬁnition of an ϵ-chain implies B ⊆G, and so B ⊆H. In particular, y ∈H.
Since y was arbitrary, H = G and G is connected.
Now assume that G is connected. Fix a point x in G and let
D = {y ∈G : there is an ϵ > 0 and an ϵ-chain in G from x to y}
The strategy of the proof will be to show that D is both relatively open and closed
in G; since it is not empty (x ∈D), it will then follow that D = G and so G will
have been shown to satisfy the condition. If y ∈D, let ϵ > 0 and let x1, . . . , xn be
an ϵ-chain from x to y. It follows from the deﬁnition of an ϵ-chain that B(y; ϵ) ⊆D.
Thus D is open. Now suppose z ∈G ∩cl D – this is the relative closure of D in G.
(Why?) Choose r > 0 such that B(z; r) ⊆G; so B(z; r) ∪D ⊆G. Since z ∈cl D,
there is a point y in B(z; r) ∩D. Let x0 = x, x1, . . . , xn be an ϵ-chain from x to y.
By Exercise 10 there is an ϵ′-chain from x to y in G whenever 0 < ϵ′ < ϵ. Applying
this with 0 < ϵ′ < min{ϵ, r}, we may assume ϵ < r. Using Lemma 5.6.15 we see
that this implies there is an ϵ-chain in G from x to z. Thus z ∈D and so D is relatively
closed in G.
■
We note that (i) of the deﬁnition of an ϵ-chain was used to establish that the
condition in part (b) was suﬃcient for connectedness. Without this, the result is
false as we see in the following example.
5.6.17. Example. Let X = {(x, y) ∈R2 : y > x−1} ∪{(x, y) ∈R2 : y < 0}. Clearly
X is not connected; in fact it has two components. It is also easy to see that if
¯x = (x, y) with y < 0 and ¯y = (w, z) with z > w−1, then for all suﬃciently small ϵ
there are points ¯x1, . . . , ¯xn in X such that for 2 ≤k ≤n, ¯xk−1 ∈B(¯xk; ϵ), and ¯x1 = ¯x
and ¯xn = ¯y.
Unlike open subsets of Rp, components in an arbitrary metric space are not
necessarily open. (Exercise 13.)
Exercises
(1)
Prove Proposition 5.6.2.
(2)
Prove that open and half-open intervals are connected, completing the proof of half
of Proposition 5.6.3.
(3)
If (X, d) is connected and f : X →R is a continuous function such that | f (x)| = 1
for all x in X, show that f must be constant.
(4)
Show that X is connected if and only if whenever a, b are two points in X, there is
a connected subset E of X such that a, b ∈E.

146
Metric and Euclidean Spaces
(5)
If A is a subset of X, deﬁne the characteristic function of A as the function χA :
X →R such that χA(x) = 1 when x ∈A and χA(x) = 0 when x /∈A. Show that A
is simultaneously open and closed if and only if χA is continuous.
(6)
Look at the preceding exercise for the deﬁnition of the characteristic function on a
set X. (a) If A and B are subsets of X, which function is χAχB? (b) Which function
is χA + χB? (c) What is the characteristic function of the empty set? (d) Is 1 −χA
the characteristic function of some set?
(7)
Can you think of any way to generalize Proposition 5.6.7(a) and obtain a theorem
whose conclusion is that E = {Ei : i ∈I} is connected?
(8)
Give an example of two connected sets whose intersection is not connected.
(9)
Let E ⊆R2 and let
E1 = {x ∈R : there is a y ∈R such that (x, y) ∈E}
(a) Show that if E is compact, then E1 is compact. (b) Give an example of a set
E such that E1 is compact, but E is not. (c) Show that if E is connected, then E1
is connected. (d) Give an example of a set E such that E1 is connected, but E is
not.
(10)
If E is a subset of Rp, x, y ∈E, and ϵ > 0 such that there is an ϵ-chain from x to y,
show that for any ϵ′ with 0 < ϵ′ < ϵ there is an ϵ′-chain from x to y.
(11)
Prove Lemma 5.6.15.
(12)
A polygon [x, x1, . . . , xn−1, y] is the union of straight line segments in Rp of the
form
[x, x1], [x1, x2], . . . , [xn−1, y]
(a) Show that a polygon is a connected subset of Rp. (b) Show that an open subset G
of Rp is connected if and only if for any two points x and y in G there is a polygon
[x, x1, . . . , xn−1, y] contained in G.
(13)
Give an example of a metric space (X, d) such that the components are not all open.
5.7. The Space of Continuous Functions
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In this section we extend the deﬁnition of uniform convergence encountered in §4.1
to sequences of continuous functions deﬁned on a metric space. After extending
the deﬁnition and proving some basic propositions, we will shift the point of view
by considering the set of all bounded continuous functions as a metric space. We
continue to assume that (X, d) is a ﬁxed metric space.
5.7.1. Definition. If { fn} is a sequence of functions from (X, d) into R, say that
{ fn} converges uniformly to a function f if for every ϵ > 0 there is an N such that
| fn(x) −f (x)| < ϵ for all x in X and all n ≥N. In symbols this is written as fn →uf
on X.
I suspect that many readers could have guessed this was the deﬁnition of uniform
convergence. We could have extended the deﬁnition further by having the sequence

5.7 The Space of Continuous Functions
147
{ fn} consist of functions from (X, d) into a second metric space (Z, ρ). We will not
explore this here, however. The next result extends Proposition 4.1.3 to the present
context. The same proof will work here.
5.7.2. Proposition. If { fn}, {gn}, and {hn} are three sequences of functions from
X into R such that gn(x) ≤fn(x) ≤hn(x) for all x in X and there is a function
f : X →R such that gn →u f and hn →u f on X, then fn →u f on X.
The next proposition extends Proposition 4.1.5 and the same proof will work.
5.7.3. Proposition. If { fn} is a sequence of bounded functions from X into R that
converges uniformly to a function f : X →R, then f is a bounded function and the
sequence of functions { fn} is uniformly bounded; that is, there is a constant M such
that | fn(x)| ≤M for all x in X and all n ≥1.
The next result extends Theorem 4.1.6. The same proof works provided we make
a small modiﬁcation, which is carried out below.
5.7.4. Theorem. If { fn} is a sequence of bounded continuous functions from X into
R and fn →u f on X, then f : X →R is a continuous function.
Proof. Fix a point a in X; we want to show that f is continuous at a. If ϵ > 0, then by
the uniform convergence we can choose an integer n such that | f (x) −fn(x)| < ϵ/3
for all x in X. Since fn is continuous there is a δ > 0 such that | fn(x) −fn(a)| < ϵ/3
when d(x, a) < δ. Thus for any x in X with d(x, a) < δ, | f (x) −f (a)| ≤| f (x) −
fn(x)| + | fn(x) −fn(a)| + | fn(a) −f (a)| < 3(ϵ/3) = ϵ. By deﬁnition, f is contin-
uous at a.
■
The next result extends Theorem 4.1.8 to uniformly continuous functions deﬁned
on a metric space. A slight modiﬁcation of that proof will work here. The details
are left to the reader.
5.7.5. Theorem. Let X ⊆R. If { fn} is a sequence of bounded uniformly continu-
ous functions on X and fn →u f on X, then f : X →R is a uniformly continuous
function.
Now we change the context of the discussion.
5.7.6. Definition. We denote the set of all continuous functions from X into R by
C(X ). Cb(X ) denotes the subset of all those functions f in C(X ) that are bounded.
That is, Cb(X ) consists of all those continuous functions f from X into R such that
∥f ∥≡sup{| f (x)| : x ∈X} < ∞
Here we call upon the student to have a somewhat diﬀerent point of view and to
begin to think of continuous functions as points in the space C(X ). We’ll see this
repeatedly as we progress.

148
Metric and Euclidean Spaces
An algebra is a vector space A over R in which there is a multiplication and all
the usual distributive and associative laws hold: for all a, b, c in A and t in R
a(b + c) = ab + ac
(b + c)a = ba + ca
a(bc) = (ab)c
t(ab) = (ta)b = a(tb)
In other words an algebra is both a vector space and a ring such that the distributive
laws hold. If there is a multiplicative identity, it is denoted by 1. Note that when the
algebra A has an identity, it contains a replica of the constants: t = t1 for every t in
R.
5.7.7. Proposition. For any metric space (X, d) the following hold.
(a) C(X ) and Cb(X ) are algebras.
(b) If we set d( f , g) = ∥f −g∥for f and g in Cb(X ), then d deﬁnes a metric.
Whenever we discuss Cb(X ) as a metric space we refer to this metric.
(c) A sequence { fn} in Cb(X ) converges to f in the metric space Cb(X ) if and only
if fn →u f .
(d) Cb(X ) is a complete metric space.
Proof. Part (a) was already proved in Proposition 5.4.5. (Also see Exercise 5.4.11.)
Part (b) is left as Exercise 1.
(c) If fn →u f and ϵ > 0, let n be an integer such that | fn(x) −f (x)| < ϵ for all
n ≥N. It follows that d( fn, f ) = ∥fn −f ∥≤ϵ for n ≥N; so fn →f inCb(X ). For
the converse assume that when n ≥N, ∥fn −f ∥= sup{| fn(x) −f (x)| : x ∈X} <
ϵ. It follows that for n ≥N we have that | fn(x) −f (x)| < ϵ for every x in X; so
fn →u f .
(d) If { fn} is a Cauchy sequence in Cb(X ), let ϵ > 0 and choose N such that ∥fn −
fm∥< ϵ when m, n ≥N. Thus for each x in X, { fn(x)} is a Cauchy sequence in R;
hence it has a limit. Put f (x) = limn fn(x) so that f : X →R deﬁnes a function.
Let ϵ > 0, and choose N such that ∥fn −fm∥< ϵ/6 when m, n ≥N. For m, n ≥N
and x an arbitrary point in X we have that
5.7.8
| f (x) −fn(x)| ≤| f (x) −fm(x)| + | fm(x) −fn(x)| < 2ϵ
6
Now hold n ≥N ﬁxed. Since the above inequality holds for any x in X it also holds
for any y in X and so
| f (x) −f (y)| ≤| f (x) −fn(x)| + | fn(x) −fn(y)| + | fn(y) −f (y)|
< 2ϵ
3 + | fn(x) −fn(y)|
But fn is continuous so there is a δ > 0 such that | fn(x) −fn(y)| < ϵ/3 when
d(x, y) < δ. Putting all this together gives that | f (x) −f (y)| ≤ϵ when d(x, y) < δ;

5.7 The Space of Continuous Functions
149
that is,
f
is a continuous function. By (5.7.8)
fn →u f , so
f ∈Cb(X )
(Proposition 5.7.3).
■
What we have done so far in our study of C(X ) and Cb(X ) can be thought of as
fundamental orientation and housekeeping. Now we begin the process of establish-
ing one of the important results about C(X ) when X is compact. We begin with a
lemma about [0, 1].
5.7.9. Lemma. There is a sequence of polynomials {pn} such that pn →√x in
C[0, 1].
Proof. We deﬁne the sequence of polynomials inductively by letting p1(x) = 0 and
pn+1(x) = pn(x) + 1
2[x −pn(x)2]. It is an easy job to show that all these polynomi-
als are positive on [0, 1] (Exercise 4).
Claim. For all n ≥1 and all x in [0, 1], pn(x) ≤√x.
In fact this is clear for n = 1, so assume it is true for n. Since pn(x) ≤√x ≤1
when x ∈[0, 1], we have that 1
2[√x + pn(x)] ≤1. Hence
pn+1(x) = pn(x) + 1
2[x −pn(x)2]
= pn(x) + 1
2[√x −pn(x)][√x + pn(x)]
≤pn(x) + [√x −pn(x)]
= √x
establishing the claim.
In light of this claim x −pn(x)2 ≥0 so that pn(x) ≤pn+1(x). Thus for each x in
[0, 1] we have that {pn(x)} is an increasing sequence in R that is bounded above by
√x; hence there is a t in R such that pn(x) →t ≤√x. Therefore
t = lim
n→∞pn+1(x) = lim
n→∞pn(x) + lim
n→∞
1
2[x −pn(x)2] = t + 1
2[x −t2]
implying that t = √x. Thus the sequence {pn} is increasing and converges pointwise
to the continuous function f (x) = √x on [0, 1]. By Dini’s Theorem (5.5.13), the
convergence is uniform.
■
There is a part of me that doesn’t like the preceding proof.
It works, so what’s not to like and why am I raising the issue? The rub is that it’s
not clear how the sequence {pn} is thought up and maybe this disturbs some readers
as well. The source of the polynomials is connected to Taylor’s Theorem (2.5.6) and
Newton’s method from calculus. An explication of this would be a distraction. So
let’s just say we obtain one approximation pn(x), add on half the amount by which
pn(x)2 fails to equal x, then we keep our ﬁngers crossed. Lo and behold, it works.

150
Metric and Euclidean Spaces
Recall the deﬁnition of the maximum and minimum of two functions (1.7.9),
f ∨gand f ∧g. (That deﬁnition was given for functions deﬁned on subsets of R but
the same deﬁnition extends.)
5.7.10. Lemma. If A is a closed subalgebra of Cb(X ) that contains the identity
and f , g ∈A, then f ∨g, f ∧g ∈A.
Proof. In light of Exercise 3, it suﬃces to show that | f | ∈A whenever f ∈A. To
do this ﬁx f in A; we may assume that f is not the function 0. Put a = ∥f ∥and
observe that a−2 f 2 ∈A and takes its values in [0, 1]. Let {pn} be the sequence of
polynomials from the preceding lemma such that pn →u
√x on [0, 1]. Since A is an
algebra that contains the constant functions, pn ◦(a−2 f 2) = pn((a−2 f 2)) ∈A for
each n ≥1. It follows that in Cb(X ), pn((a−2 f 2)) →

a−2 f 2 = a−1| f |. (Why?)
Because A is a closed subalgebra of Cb(X ) we have that | f | ∈A.
■
A collection of functions S inC(X ) is said to separate the points of X if whenever
x, y ∈X and x ̸= y, there is a function f in S with f (x) ̸= f (y). Both Cb(X ) and
C(X ) separate the points of X. (You can show this by using Urysohn’s Lemma, but
it can be done directly.) If X is any subset of R, the collection of all polynomials
separates the points of X.
5.7.11. Theorem (Stone7–Weierstrass8 Theorem).
If X is compact and A is a
closed subalgebra of C(X ) that separates the points of X and contains the constant
functions, then A = C(X ).
Proof. Fix an arbitrary f in C(X ) and let’s show that f ∈A. We start with a simple
claim.
Claim. If x and y are distinct points in X, there is a function hx,y in A such that
hx,y(x) = f (x) and hx,y(y) = f (y).
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
7 Marshall H Stone was born in 1902 in New York. His father was Harlan Stone who, after time as the dean of
the Columbia Law School, became a member of the US Supreme Court, including a term as its chief justice.
Marshall Stone entered Harvard in 1919 intending to study law. He soon diverted to mathematics and received
his doctorate in 1926 under the direction of David Birkhoﬀ. Though he had brief appointments at Columbia
and Yale, most of his early career was spent at Harvard. His initial work continued the direction it took under
Birkhoﬀ, but in 1929 he started working on hermitian operators. His American Mathematical Society book, Lin-
ear Transformations in Hilbert space and Their Applications to Analysis, became a classic. Indeed, a reading
of that book today shows how the arguments and clarity might easily lead to the conclusion that it is a contem-
porary monograph. During World War II he worked for the Navy and the War Department and in 1946 he left
Harvard to become the chairman of the mathematics department at the University of Chicago. He himself said
that this decision was arrived at because of “my conviction that the time was also ripe for a fundamental revision
of graduate and undergraduate mathematical education.” Indeed he transformed the department at Chicago. The
number of theorems that bear his name is truly impressive. Besides the present theorem there is the Stone– ˇCech
compactiﬁcation, the Stone–von Neumann Theorem, the Stone Representation Theorem in Boolean algebra,
and Stone’s Theorem on one-parameter semigroups. He stepped down as chair at Chicago in 1952 and retired
in 1968, but then went to the University of Massachusetts where he taught in various capacities until 1980. He
loved to travel and on a trip to India in 1989 he died in Madras. He had 14 doctoral students.
8 See Theorem 1.3.11 for a biographical note.

5.7 The Space of Continuous Functions
151
The proof is easy. Since A separates points there is a function g in A with g(x) ̸=
g(y). Set
hx,y = f (x) + [ f (y) −f (x)] g −g(x)
g(y) −g(x)
The reader can verify that this function has the desired properties.
There are many functions in A that take on the values f (x) and f (y) at x and
y. (Name one more.) For every such pair x, y, however, we ﬁx one such function
hx,y. Now ﬁx x in X and let ϵ > 0. For each y in X put G(y) = {z ∈X : hx,y(z) <
f (z) + ϵ}. Note that the continuity of f and hx,y implies that G(y) is open. Using
the properties of hx,y we have that x, y ∈G(y). Hence {G(y) : y ∈X} is an open
cover of X. By compactness there are points y1, . . . , yn such that X = n
j=1 G(yj).
Put
hx = hx,y1 ∧· · · ∧hx,yn
Lemma 5.7.10 implies that hx ∈A. Since hx,y(x) = f (x) for every y in X, we
have that hx(x) = f (x); because the sets {G(yj) : 1 ≤j ≤n} cover X, we have that
hx(z) < f (z) + ϵ for every z in X.
For each x in X let H(x) = {z ∈X : hx(z) > f (z) −ϵ}. Once again we note that
x ∈H(x) and so {H(x) : x ∈X} is an open cover of X; let {H(xi) : 1 ≤i ≤m} be a
ﬁnite subcover, and put
h = hx1 ∨· · · ∨hxm
Once again Lemma 5.7.10 implies h ∈A. It follows (Verify!) that for every z in X,
f (z) −ϵ < h(z) < f (z) + ϵ. That is, ∥f −h∥< ϵ. since ϵ was arbitrary and A is
closed, f ∈A.
■
The proof of the next two corollaries is required in Exercise 5.
5.7.12. Corollary (Weierstrass Theorem). For any closed and bounded interval
[a, b] ⊆R, the polynomials are dense in C[a, b].
The preceding corollary is the reason that Weierstrass’s name is attached to
(5.7.11).
5.7.13. Corollary. If X is a closed and bounded subset of Rp, the set of polynomials
in the p-variables x1, . . . , xp is dense in C(X ).
The reader should examine Exercises 6 and 7.
Now we obtain another important result aboutC(X ) when X is compact: the char-
acterization of the compact subsets of C(X ). We start with a deﬁnition.
5.7.14. Definition. If F is a subset of Cb(X ), then F is said to be equicontinuous
if for every ϵ > 0 and every point x0, there is a neighborhood U of x0 such that
| f (x) −f (x0)| < ϵ for all x in U and every f in F.

152
Metric and Euclidean Spaces
Note that if we were considering a single f , the fact that for each ϵ and x0 there
is such a neighborhood U is just the fact that f is continuous at x0. So every ﬁnite
set in Cb(X ) is equicontinuous. The salient point in the deﬁnition is that one open
neighborhood U of x0 works for every function in the family F. That is, the family
of functions F is suﬃciently constrained that there is a uniformity in the way that
they are continuous — hence the preﬁx “equi”.
5.7.15. Theorem (Arzelà9–Ascoli10 Theorem). If X is compact, then a subset F of
C(X ) is totally bounded if and only if F is bounded and equicontinuous.
Proof. Assume that F is totally bounded. Automatically F is bounded (Exer-
cise 5.5.4). To establish equicontinuity let ϵ > 0. So there are f1, . . . , fn in F such
that F ⊆n
k=1{ f ∈C(X ) : ∥f −fk∥< ϵ/3}. If x0 ∈X, let U be a neighborhood
of x0 such that for 1 ≤k ≤n, | fk(x) −fk(x0)| < ϵ/3 when x ∈U. Thus if f ∈F
and we choose fk with ∥f −fk∥< ϵ/3, then | f (x) −f (x0)| ≤| f (x) −fk(x)| +
| fk(x) −fk(x0)| + | fk(x0) −f (x0)| < ϵ. Hence F is equicontinuous.
Now assume that F is equicontinuous and bounded. Without loss of generality
we can assume that ∥f ∥≤1 for every f in F. Fix ϵ > 0; by equicontinuity we have
that for each x in X there is an open neighborhood Ux of x such that | f (y) −f (x)| <
ϵ/3 whenever y ∈Ux and f ∈F. It follows that {Ux : x ∈X} is an open cover of X;
by the compactness of X we can extract a ﬁnite subcover {Ux1, . . . ,Uxn}. Now choose
α1, . . . , αm in [−1, 1] such that [−1, 1] ⊆m
j=1{α : |α −αj| < ϵ/6}. Note that the
collection of all ordered n-tuples of elements from the set {α1, . . . , αm} is ﬁnite. We
don’t want to consider all such n-tuples, however, but only the set B of those ordered
n-tuples b = (β1, . . . , βn) with β1, . . . , βn ∈{α1, . . . , αm} such that there is a fb in
F with | fb(xj) −β j| < ϵ/6. Since for each f in F we have that f (X ) ⊆[−1, 1] we
see that B ̸= ∅. For each b in B ﬁx one such function fb in F; so { fb : b ∈B} is a
ﬁnite subset of F. The fact that F is totally bounded holds once we establish the
following.
Claim. F ⊆
b∈B{ f ∈C(X ) : ∥f −fb∥< ϵ}.
If f ∈F, { f (x1), . . . , f (xn)} ⊆[−1, 1] and so there is a b = (β1, . . . , βn) in B
with | f (xk) −βk| < ϵ/6 for 1 ≤k ≤n. Thus | f (xk) −fb(xk)| < ϵ/3 for 1 ≤k ≤
n. For each x in X choose xk with x inUxk. Hence | f (x) −fb(x)| ≤| f (x) −f (xk)| +
| f (xk) −fb(xk)| + | fb(xk) −fb(x)| < ϵ. Since x was arbitrary, the claim is estab-
lished.
■
5.7.16. Corollary. If X is compact and F ⊆C(X ), then F is compact if and only
if F is closed, bounded, and equicontinuous.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
9 Cesare Arzelà was born in 1847 in La Spezia, Italy. He received his doctorate from the university in Pisa
under the direction of Enrico Betti. He held positions at Florence and Palermo before he became a professor
in Bologna in 1880. His most famous work is this result, where he established the condition as suﬃcient for
compactness. He died in 1912 in La Spezia.
10 Giulio Ascoli was born in Trieste in 1843. He received his degree from Pisa and became a professor at Milan
in 1872. He had a distinguished career and this theorem is his most notable result. He died in 1896 in Milan.

5.7 The Space of Continuous Functions
153
Exercises
(1)
Prove Proposition 5.7.7(b).
(2)
(a) Extend the deﬁnition of a uniformly Cauchy sequence given in Exercise 4.1.5 to
the context of continuous functions from X into R. (b) Show that a sequence { fn} in
(Cb(X ), d) is a Cauchy sequence if and only if it is a uniformly Cauchy sequence.
(3)
Show that if f , g : X →R, then f ∨g = 1
2( f + g + | f −g|) and f ∧g = 1
2( f +
g −| f −g|). (Hint: ﬁrst prove it for numbers.)
(4)
Show that each of the polynomials pn in the proof of Lemma 5.7.9 satisﬁes pn(x) ≥
0 for all x in [0, 1]. (Hint: Use induction.)
(5)
Prove Corollaries 5.7.12 and 5.7.13.
(6)
Let X be compact and assume A is a closed subalgebra ofC(X ). Assume A contains
the constants and there are two points x1 and x2 such that if x, y ∈X\{x1, x2} then
there is a function f in A with f (x) ̸= f (y).
(a) Show that { f ∈C(X ) : f (x1) = f (x2)} ⊆A, with equality if and only if
f (x1) = f (x2) for every f in A.
(b) Show that A = { f ∈C[0, 2π] : f (0) = f (2π)} is the closure of the set of all
polynomials in sin x and cos x. (Such polynomials are called trigonometric polyno-
mials.)
(7)
In the Stone–Weierstrass Theorem consider each possible pair of the conditions: (a)
A is an algebra; (b) A separates points; and (c) A contains the constant functions.
For each possible pair of these three conditions, ﬁnd an example of a compact space
X and a closed algebra A ⊆C(X ) such that A satisﬁes those two conditions but
A ̸= C(X ).
(8)
If f ∈C[0, 1] such that
 1
0 f (x)xn dx = 0 for all n ≥0, show that f = 0.
(9)
If (X, d) is a compact metric space and F ⊆C(X ), show that F is equicontinuous
if and only if for every ϵ > 0 there is a δ > 0 such that when d(x, y) < δ, | f (x) −
f (y)| < ϵ for every f in F
(10)
If F is a family of functions in Cb(X ) that is equicontinuous at each point of X,
does it follow that F is a bounded set? What happens if you also assume that X is
compact?


6
Differentiation in Higher Dimensions
Here we want to extend the theory of diﬀerentiation to situations involving several
variables. First we’ll assume only the range of the function has more than one vari-
able, then only the domain, and ﬁnally where both domain and range have more
than a single dimension. After doing the necessary work to deﬁne the concept of a
diﬀerentiable function in each of these three cases, we’ll see some applications.
6.1. Vector-valued Functions
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
This section starts the chapter by discussing functions γ deﬁned on a subset of
R and taking values in Rp with p ≥1; in other words, vector-valued functions of a
single variable. Deﬁning diﬀerentiability in this situation presents few diﬃculties
and the resulting theory does not diﬀer very much from the case where p = 1 that
was presented in Chapter 2. The ﬁrst task is to deﬁne limt→x γ (t) when γ : X →Rp
for some subset X of R. This is not a major problem since Rp is a metric space. To
keep the discussion similar to that in Chapter 2, however, we start by ﬁnding a sub-
stitute for the absolute value of a real number. For any x in Rp let
∥x∥=
⎡
⎣
p

j=1
x2
j
⎤
⎦
1
2
The quantity ∥x∥is called the norm of x. Recall that ∥x∥2 = ⟨x, x⟩, where this latter
symbol is the inner product in Rp. See (5.1.4). So when x, y ∈Rp, ∥x −y∥is pre-
cisely the distance from x to y as deﬁned in (5.1.2(c)) and makes Rp into a metric
space. We note from Corollary 5.1.6 that the norm satisﬁes the triangle inequality:
∥x + y∥≤∥x∥+ ∥y∥.
Now when X ⊆R, a function γ : X →Rp is a function from one metric space
into another, so the deﬁnition of limits and continuity is inherited from that more
general setting. (Also see Exercise 2.) Speciﬁcally limt→x γ (t) = z means that for
every ϵ > 0 there is a δ > 0 such that
∥γ (t) −z∥< ϵ when 0 < |t −x| < δ
and t ∈X. See Exercise 5.4.1.

156
Differentiation in Higher Dimensions
(The author asks the reader’s indulgence for a certain ambiguity in the notation
here. We will use letters like x, y, etc. both for numbers in R and vectors in Rp.
This is to maintain some consistency with the past and future and hopefully will
not cause confusion. Some people employ the convention of using boldface letters
for the vectors, but that strikes me as cumbersome and not justiﬁed. It is also the
case that in more advanced courses no such use of boldface is made. My policy,
however, will call for a bit of extra attention and awareness of the context when
reading this material. Maybe that’s an extra beneﬁt of such ambiguity.)
With the concept of limit there is almost no problem deﬁning the derivative of
such a function.
6.1.1. Definition. A function γ : (a, b) →Rp is diﬀerentiable at x if
lim
t→0
γ (x + t) −γ (t)
t
exists. The value of this limit is denoted by γ ′(x) and is called the derivative of γ
at x. Note that when the derivative exists, γ ′(x) ∈Rp. γ is said to be diﬀerentiable
on (a, b) if it is diﬀerentiable at each point. Observe that if γ : [a, b] →Rp we can
deﬁne γ ′(a) and γ ′(b) as we did in (2.2.2).
Also see Exercise 3.
Notice that when γ : [a, b] →Rp is diﬀerentiable, then γ ′ : [a, b] →Rp, so we
can speak of γ being continuously diﬀerentiable or twice diﬀerentiable or having
any number of derivatives. We refer to a function that is continuously diﬀerentiable
as a smooth function.
6.1.2. Definition. A curve in Rp is a continuous function γ : [a, b] →Rp. Say
that the curve γ is smooth if γ is continuously diﬀerentiable. The trace of the curve
is its image in Rp and is denoted by {γ } = {γ (t) : a ≤t ≤b}.
It would not surprise me if some readers are a bit uncomfortable when we deﬁne
a curve as a function. Most think of a curve as a set of points, what we call the trace
of the curve. For example, we might say the top arc of the circle in R2 centered
at the origin and having radius r is a curve. This language is not, however, suf-
ﬁciently precise to do mathematics. Deﬁning it as a function allows us to better
introduce and use the analysis we are developing to study curves. For example, we
eventually want to examine the direction of the curve. This is easy to do when
we deﬁne the curve as the function γ : [a, b] →Rp as γ has the natural direc-
tion it inherits from the interval [a, b]; we simply look at the direction of γ (t)
as t goes from a to b. As another example, in (7.1.2) we deﬁne the length of a
smooth curve, something that is awkward to formulate if a curve is deﬁned as a set of
points.
When the curve γ : [a, b] →Rp is smooth, its derivative is a tangent vector to
the curve. We’ll examine this in a more general context in §6.7 below.

6.1 Vector-valued Functions
157
We state a version of Proposition 2.2.9 for the present situation.
6.1.3. Proposition. A function γ : (a, b) →Rp is diﬀerentiable at a point x in
(a, b) if and only if there there is a function  : (a, b) →Rp and a vector G in
Rp such that limy→x (y) = 0 and
γ (y) −γ (x) = (y −x)[G + (y)]
for all y in (a, b). When γ is diﬀerentiable at x, we have G = γ ′(x).
As in the case of a scalar-valued function, the proof is straightforward; see the
proof of Proposition 2.2.9.
Most of the results for the derivatives of scalar-valued functions obtained in §2.2
carry over to this situation. In particular the derivative of the sum of two vector-
valued functions is the sum of the derivatives. The straight product of two functions
with values in Rp makes no sense, though we can employ the inner product of two
such functions. Recall the inner product notation on Rp introduced when we proved
the Cauchy–Schwarz Inequality:
⟨x, y⟩=
p

j=1
xjyj
The essential properties of the inner product were listed in (5.1.4).
6.1.4. Proposition. If γ and τ are two functions from (a, b) into Rp that are dif-
ferentiable at x, then the function f : (a, b) →R deﬁned by f (t) = ⟨γ (t), τ(t)⟩is
diﬀerentiable at x and
f ′(x) = ⟨γ ′(x), τ(x)⟩+ ⟨γ (x), τ ′(x)⟩
Proof. The proof proceeds much like the proof of Proposition 2.2.5(b) except that
we use the properties of the inner product to get
f (x + t) −f (x)
t
= ⟨γ (x + t), τ(x + t)⟩−⟨γ (x), τ(x)⟩
t
= ⟨γ (x + t) −γ (x), τ(x + t)⟩
t
+ ⟨γ (x), τ(x + t) −τ(x)⟩
t
Now we let t →0 and appeal to Exercise 1 to get the result.
■
In Exercises 2 and 3 we show that discussing the diﬀerentiability of a function
γ : (a, b) →Rp can be reduced to discussing the functions t →⟨γ (t), ej⟩, where
e1, . . . , ep is the standard basis. In fact what we have done is demonstrate that there
is little distinction between the theory of diﬀerentiation of functions with values in
R and those with values in Rp.
There is, however, a result for derivatives of functions from [a, b] into R
that is dramatically lacking when we consider vector-valued functions:
the Mean Value Theorem (2.3.4).
See Exercise 4.

158
Differentiation in Higher Dimensions
Exercises
(1)
If
φ, ψ : (a, b) →Rp,
limy→x φ(y) = ,
and
limy→x ψ(y) = ,
show
limy→x⟨φ(y), ψ(y)⟩= ⟨, ⟩.
(2)
For γ : (a, b) →Rp and 1 ≤j ≤p, let γj : (a, b) →R be deﬁned by γj(t) =
⟨γ (t), ej⟩, where e1, . . . , ep are the standard basis vectors. (a) Show that if x ∈
(a, b), then limt→x γ (t) = G if and only if limt→x γj(t) = ⟨G, ej⟩for 1 ≤j ≤p.
(b) Show that γ is continuous on (a, b) if and only if each γj is continuous on
(a, b).
(3)
Use the notation established in the preceding exercise. (a) Show that γ is diﬀeren-
tiable on (a, b) if and only if each γj is diﬀerentiable on (a, b). (b) If γ is diﬀeren-
tiable on (a, b), show that γ ′(t) = (γ ′
1(t), . . . , γ ′
p(t)).
(4)
Show that the function γ : [0, 1] →R2 deﬁned by γ (t) = (t2,t3) does not satisfy
γ (1) −γ (0) = γ ′(c) for any point c in [0, 1].
6.2. Differentiable Functions, Part 1
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Now we confront the diﬀerentiability of functions from an open subset of Rp
into R. In this situation the diﬃculties in deﬁning diﬀerentiability are signiﬁcant.
We start by examining the partial derivatives of such functions, something everyone
has brushed up against in calculus. Here we renew that acquaintance and perhaps
broaden and deepen the encounter.
For a variety of reasons, including ease of notation, we will continue to denote the
elements of Rp as vectors x rather than a p-tuple of real numbers x = (x1, . . . , xp)
unless there is a reason for such speciﬁcity. As usual the standard basis of the vector
space Rp will be denoted by e1, . . . , ep. In other words e1 = (1, 0, . . . , 0), e2 =
(0, 1, 0, . . . , 0), . . . , ep = (0, . . . , 0, 1).
In this section, G will always denote an open subset of Rp.
6.2.1. Definition. If f : G →R, x ∈G, and 1 ≤j ≤p, say that the j-th partial
derivative of f exists at x provided
lim
t→0
f (x + tej) −f (x)
t
exists. When this limit exists, it is denoted by
∂f
∂xj
(x) = ∂j f (x)
There is a possible confusion in the term “ j-th partial derivative.” For example,
the term “second partial derivative” could mean ∂2 f as deﬁned above or the same

6.2 Differentiable Functions, Part 1
159
phrase might mean ∂2 f /∂x2 or ∂2 f /∂x∂y. The context should indicate which we
mean and the notation is certainly diﬀerent.
What is happening here is that for x ﬁxed in G we are considering the scalar-valued
function t →f (x + tej) deﬁned on a small interval about 0 and we are diﬀerenti-
ating this with respect to t. This is why we insist that f be deﬁned on an open set G
so that for 1 ≤j ≤p there is an ϵ > 0 such that t →f (x + tej) is deﬁned for t in
(−ϵ, ϵ) and we can discuss its derivative. We are assuming the reader remembers
from calculus how to carry out the partial diﬀerentiation: just treat all the variables
{xk : k ̸= j} as constants and diﬀerentiate the remainder as though it were a function
of the variable xj alone.
If the j-th partial derivative of f : G →R exists throughout G, then this gives us
another function ∂j f : G →R. We can discuss the continuity of ∂j f as well as the
existence of its partial derivatives. When ∂j f has a partial derivative with respect to
xi for some i we write that derivative as
∂ji f = ∂i(∂j f )
Note the order of the indices i and j in ∂ji f as this is important. The question imme-
diately arises whether ∂ji f = ∂ij f . This is not always the case.
6.2.2. Example. Deﬁne f : R2 →R by
f (x, y) =
	xy(x2−y2)
x2+y2
when (x, y) ̸= (0, 0)
0
when (x, y) = (0, 0)
(In the calculations that follow, the reader is asked in Exercise 2 to supply the
details.) It follows that ∂2 f (x, 0) = x. Hence
∂21 f (0, 0) = lim
x→0
∂2 f (x, 0) −∂2 f (0, 0)
x
= 1
On the other hand ∂1 f (0, y) = −y and so ∂12 f (0, 0) = −1.
The key is whether those second partial derivatives are continuous. The next proof
uses the MVT for derivatives of functions of one variable (2.3.4).
6.2.3. Proposition. If f : G →R, 1 ≤j, i ≤p, and both ∂i∂j f and ∂j∂i f exist and
are continuous at x in G, then ∂i∂j f (x) = ∂j∂i f (x).
Proof. Fix i and j and ﬁx xk when k ̸= i, j. By considering the function (xi, xj) →
f (x1, . . . , xp), we see that if we prove the proposition when p = 2, we prove the
proposition for any value of p. So without loss of generality assume p = 2.
Fix a point (a, b) in G; we want to show that ∂12 f (a, b) = ∂21 f (a, b). Fix r > 0
such that B = B((a, b); r) ⊆G, and let δ be any positive number with 0 < δ < r.
(δ will be further speciﬁed later.) Put Bδ = B((a, b); δ). Consider a point (x, y)
in the disk Bδ with x ̸= a and y ̸= b. Apply the MVT for derivatives to the

160
Differentiation in Higher Dimensions
function of one variable t →[ f (t, y) −f (t, b)] to obtain a point ξ1 between x and
a such that
[ f (x, y) −f (x, b)] −[ f (a, y) −f (a, b)] = [∂1 f (ξ1, y) −∂1 f (ξ1, b)](x −a)
Now apply the MVT to the function t →∂1 f (ξ1,t) (Is the hypothesis of the MVT
satisﬁed?) to obtain a point η1 between y and b such that
∂1 f (ξ1, y) −∂1 f (ξ1, b) = ∂12 f (ξ1, η1)(y −b)
Combining these equations we see that for any (x, y) in the disk Bδ there is a point
(ξ1, η1) in Bδ with
6.2.4
[ f (x, y) −f (x, b)] −[ f (a, y) −f (a, b)] = ∂12 f (ξ1, η1)(x −a)(y −b)
Interchanging the roles of the ﬁrst and second variable in the preceding argument
we see that when (x, y) ∈Bδ with x ̸= a and y ̸= b, there is a point (ξ2, η2) in Bδ
with ξ2 between x and a and η2 between y and b such that
6.2.5
[ f (x, y) −f (a, y)] −[ f (x, b) −f (a, b)] = ∂21 f (ξ2, η2)(x −a)(y −b)
But a quick inspection of the left-hand sides of (6.2.4) and (6.2.5) shows they are
equal. Hence ∂12 f (ξ1, η1)(x −a)(y −b) = ∂21 f (ξ2, η2)(x −a)(y −b). But since
(x −a)(y −b) ̸= 0 we have that
∂12 f (ξ1, η1) = ∂21 f (ξ2, η2)
Now we use the hypothesis that both ∂12 f and ∂21 f are continuous. If
ϵ > 0 there is a δ > 0 such that |∂12 f (x, y) −∂12 f (a, b)| < ϵ/2 and |∂21 f (x, y) −
∂21 f (a, b)| < ϵ/2 when (x, y) ∈Bδ. Combining this with the last equation we see
that |∂12 f (a, b) −∂21 f (a, b)| < ϵ for every positive ϵ, so the conclusion of the
proposition follows.
■
There is something disagreeable about all this.
If we want to use partial derivatives to study the behavior of a function, we can-
not do it one variable at a time. Exercise 5.4.14 shows that there are functions
deﬁned on Rp that are continuous in each variable separately, but not continu-
ous. In fact that example illustrates an additional anomaly. In Exercise 3 below the
reader is asked to show that that function has partial derivatives at the origin with
∂1 f (0, 0)) = ∂2 f (0, 0) = 0 even though it is not continuous at the origin. We must
have a concept of the diﬀerentiability of a function f : G →R that simultaneously
incorporates the inﬂuence of all the variables on f . We will now see such a concept
of diﬀerentiability for a real-valued function deﬁned on an open subset of Rp that
implies the existence of all the partial derivatives and also has the psychologically
satisfying property that it implies continuity.
To do this recall Proposition 2.2.9 where it is shown that f : (a, b) →R is
diﬀerentiable at x if and only if there there is a a number D and a function
F : (a, b) →R such that f (y) −f (x) = D(y −x) + F(y)(y −x) for all y in X and
limy→x F(y) = 0. (When this is the case, D = f ′(x).) We also saw a similar result

6.2 Differentiable Functions, Part 1
161
for functions γ : (a, b) →Rp in Proposition 6.1.3. We modify these results for
functions f deﬁned on an open subset of Rp. The ﬁrst thing to observe is that should
we write such an equation when f is deﬁned on a subset of Rp, the left-hand side
of this equation, f (y) −f (x), is a number while on the right-hand side the vector
y −x in Rp appears. Therefore we have to a ﬁnd suitable way to interpret D and the
function F.
We start with F(y)(y −x); ﬁnding a substitute for D will come shortly. If we
return to the case of a scalar-valued function we see that we could change the F-
term to F(y)|y −x| without changing the deﬁnition of diﬀerentiability. So in our
present case we can use F(y)∥x −y∥instead of F(y)(y −x), where F remains a
scalar-valued function F : G →R. But now we need an appropriate deﬁnition of
the limiting process as y →x in Rp. We need look no further than what we did
in the preceding chapter since such a function F maps one metric space to another.
Namely, if x ∈G and F : G →R, say that limy→x F(y) = A if for every ϵ > 0 there
is a δ > 0 such that |F(y) −A| < ϵ whenever 0 < ∥y −x∥< δ.
It is more involved to ﬁnd a replacement for the number D that appears in the
equation f (y) −f (x) = D(y −x) + F(y)(y −x) in Proposition 2.2.9. Unlike with
the term F(y)(y −x), we cannot replace y −x with |y −x| in the one-variable situa-
tion without changing the deﬁnition of diﬀerentiability. (Why?) So the replacement
for D cannot be a number. What to do? Don’t forget that in the case of a function f
deﬁned on an open interval it was the case that D = f ′(x). This argues, perhaps, that
the correct replacement for D should be some form of a vector though it must be able
to interact with y −x and produce a number. It is possible to once again resort to
the inner product. But for a variety of reasons the solution is that D will be replaced
by a linear functional L : Rp →R from linear algebra. We review here some of the
pertinent facts about linear functionals and present some additional things needed
for our discussion of diﬀerentiability. We start by recalling the deﬁnition.
6.2.6. Definition. A linear functional on Rp is a function L : Rp →R that satis-
ﬁes L(αx + βy) = αL(x) + βL(y) for all α, β in R and all x, y in Rp. Denote the
collection of all linear functionals on Rp by Rp∗.
We recall that Rp∗is also a vector space where for two linear functionals L1 and
L2, (L1 + L2)(x) = L1(x) + L2(x). Since the unit vectors e1, . . . , ep form a basis
for Rp, the numbers L(e1), . . . , L(ep) determine any linear functional L. That is, if
ϵ j = L(ej) and x = (x1, . . . , xp), then L(x) = p
j=1 ϵ jxj. (Also see Exercise 4.) We
rephrase this a bit diﬀerently.
6.2.7. Proposition. If 1 ≤j ≤p and we deﬁne Ej : Rp →R by
E j(x1, . . . , xp) = xj
then Ej ∈Rp∗. Moreover, {E1, . . . , Ep} is a basis for Rp∗.
Proof. We leave it to the reader to show that each Ej is a linear functional. To
show that {E1, . . . , Ep} is linearly independent, ﬁrst observe that for 1 ≤j, k ≤p,

162
Differentiation in Higher Dimensions
E j(ek) = 0 when j ̸= k and E j(ej) = 1. Thus if 0 = p
j=1 α jE j in Rp∗, we have
that for each k, 0 = p
j=1 α jE j(ek) = αk. So the linear functionals {E1, . . . , Ep}
are linearly independent. If L ∈Rp∗and αj = L(ej), then for 1 ≤j ≤q
⎛
⎝L −
p

j=1
α jE j
⎞
⎠(ek) = L(ek) −αkEk(ek) = 0
Since {e1, . . . , ep} is a basis for Rp, this shows that L = p
j=1 α jE j.
■
Now we relate linear functionals to the inner product.
6.2.8. Proposition. If L ∈Rp∗, then there is a unique vector a in Rp such that
L(x) = ⟨x, a⟩. In fact a = (L(e1), . . . , L(ep)).
Proof. If L is given, set aj = L(ej) and put a = (a1, . . . , ap) in Rp. It follows that
L(x) = L
⎛
⎝
p

j=1
xjej
⎞
⎠=
p

j=1
xjL(ej) =
p

j=1
xja j = ⟨x, a⟩
The proof of uniqueness is Exercise 5.
■
We need the next result for making estimates. The key here is to set things up so
we can apply the Cauchy–Schwarz Inequality. Recall that using the inner product
the Cauchy–Schwartz Inequality becomes
⟨x, y⟩2 ≤⟨x, x⟩⟨y, y⟩
for x, y in Rp.
6.2.9. Proposition. If we deﬁne
∥L∥=
⎡
⎣
p

j=1
L(ej)2
⎤
⎦
1
2
then for every x in Rp we have that |L(x)| ≤∥L∥∥x∥. Moreover there is a vector x
in Rp with ∥x∥= 1 and L(x) = ∥L∥. Therefore
∥L∥= sup{|L(x)| : ∥x∥≤1}
Proof. From the preceding proposition L(x) = ⟨x, a⟩, where a is the vector whose
j-th coordinate is aj = L(ej). Thus the quantity ∥L∥= ∥a∥, and so |L(x)|2 =
⟨x, a⟩2 ≤⟨x, x⟩⟨a, a⟩= ∥L∥2∥x∥2. Also setting x = ∥a∥−1a gives that ∥x∥= 1 and
L(x) = ∥L∥. The fact that ∥L∥= sup{|L(x)| : ∥x∥≤1} is now straightforward.
■
The quantity ∥L∥is called the norm of the linear functional L. We are now in a
position to deﬁne a diﬀerentiable function following the discussion that precedes
the deﬁnition of a linear functional.

6.2 Differentiable Functions, Part 1
163
6.2.10. Definition. If G is an open subset of Rp, f : G →R, and x ∈G say that
f is diﬀerentiable at x if there is a linear functional L : Rp →R and a function
F : G →R such that
f (y) −f (x) = L(y −x) + F(y)∥y −x∥
for all y in G and limy→x F(y) = 0. We deﬁne the derivative of f at x to be the linear
functional
D f (x) = f ′(x) = L
If f is diﬀerentiable at every point of G then f is said to be diﬀerentiable on G.
Again you might want to show that this coincides with the deﬁnition of a function
f : (a, b) →R being diﬀerentiable at a point x by examining Proposition 2.2.9. For
such a function what is the linear functional L : R →R?
The introduction of the notation D f = f ′ may seem capricious, but I’ll ask the
reader to be patient. There are times that D f is more convenient than f ′. This is
especially true when we later consider functions from Rp into Rq. This notation
D f is in wide use and rather standard. In fact it seems to me more convenient. The
reader might want to look at Exercise 6.
In the atmosphere of the preceding deﬁnition, since f ′(x) = D f (x) is a linear
functional it makes sense to write f ′(x)(y) = D f (x)(y) for any y in Rp. Indeed f ′ :
G →Rp∗.
6.2.11. Proposition. If f : G →Rp and g : G →Rp are diﬀerentiable at x then
for any scalars α and β, the function α f + βg is diﬀerentiable at x and D(α f +
βg)(x) = αD f (x) + βDg(x).
Proof. Exercise 7
■
6.2.12. Theorem. If G is an open subset of Rp, x ∈G, and f : G →R is diﬀeren-
tiable at x, then the following hold.
(a) f is continuous at x.
(b) For 1 ≤j ≤p, the j-th partial derivative of f at x exists.
(c) If we deﬁne the vector
∇f (x) =

 ∂f
∂x1
(x), . . . , ∂f
∂xp
(x)

then for every y in Rp we have
D f (x)(y) = ⟨y, ∇f (x)⟩
(d) The linear functional L in the deﬁnition of diﬀerentiability is unique.

164
Differentiation in Higher Dimensions
As in calculus we call ∇f (x) the gradient of f at x.
Proof. The theorem contains a lot of information, but part-by-part the proof is not
diﬃcult. We use the notation of Deﬁnition 6.2.10.
(a) We have that | f (y) −f (x)| ≤∥L∥∥y −x∥+ |F(y)|∥y −x∥, so the continuity
is immediate.
(b) From Deﬁnition 6.2.10 we have that for t ̸= 0 and 1 ≤j ≤p
lim
t→0
f (x + tej) −f (x)
t
= L(ej) + lim
t→0
F(x + tej)∥tej∥
t
= L(ej) + lim
t→0 F(x + tej)|t|
t
= L(ej)
since |t|/t remains bounded by 1. Thus ∂f
∂x j (x) exists and equals L(ej).
(c) This is immediate from Proposition 6.2.8.
(d) This follows from part (c) since the partial derivatives are unique.
■
Here is a convenient suﬃcient condition for diﬀerentiability. If you are presented
with a function, you can often use it to establish this.
6.2.13. Theorem. If f : G →R is such that each partial derivative of f exists and
is continuous, then f is diﬀerentiable.
Proof. Fix a point x in G. We have to ﬁnd a linear functional L such that
lim
y→x ∥y −x∥−1[ f (y) −f (x) −L(y −x)] = 0
That is we must ﬁnd L with the property that for every ϵ > 0 there is a δ > 0 such
that when ∥h∥< δ,
| f (x + h) −f (x) −L(y −x)| < ϵ∥h∥
Finding the linear functional L is easy since we know from the preceding theorem
that if the derivative exists, it must be given by the gradient: L(y) = ⟨y, ∇f (x)⟩,
which the hypothesis implies exists. Since each ∂j f is continuous and there are
only a ﬁnite number of these derivatives, for every ϵ > 0 there is a δ > 0 such that
|∂j f (y) −∂j f (x)| <
ϵ
√p
when ∥y −x∥< δ and 1 ≤j ≤p.
Assume h ∈Rp with ∥h∥< δ and write h = (h1, . . . , hp) = p
j=1 h jej. Deﬁne
vectors y0, y1, . . . , yp in Rp by y0 = 0 and yk = h1e1 + · · · + hkek for 1 ≤k ≤p.
Note that f (x + h) −f (x) = p
j=1[ f (x + yj) −f (x + yj−1)]. (Verify!) Now the
hypothesis says we can apply the MVT to the function deﬁned on [0, 1] by t →
f (x + y j−1 + thjej). This yields a t j in [0, 1] such that
f (x + yj) −f (x + yj−1) = d
dt [ f (x + yj−1 + th jej)](t j)
= h j[∂j f (x + yj−1 + t jh jej)]

6.2 Differentiable Functions, Part 1
165
Because ∥(x + yj−1 +t jh jej) −x∥= ∥yj−1 +t jh jej∥= ∥j−1
i=1 hiei + t jh jej∥≤
∥h∥< δ, we have that |(∂j f )(x + yj−1 + t jh jej) −∂j f (x)| < ϵ/√p. Therefore
using the Cauchy–Schwarz Inequality we have
| f (x + h) −f (x) −⟨∇f (x), h⟩|
=

p

j=1
[ f (x + yj) −f (x + yj−1)] −
p

j=1
∂j f (x)hj

=

p

j=1
h j[(∂j f )(x + yj−1 + t jh jej) −∂j f (x)]

≤
p

j=1
|h j||(∂j f )(x + yj−1 + t jh jej) −∂j f (x)|
≤∥h∥
⎡
⎣
p

j=1
|(∂j f )(x + yj−1 + t jh jej) −∂j f (x)|2
⎤
⎦
1
2
≤ϵ∥h∥
■
Because we have so many diﬀerent situations where functions are deﬁned with
values in Rp or domain in Rp we will have many Chain Rules. Here is the ﬁrst.
6.2.14. Theorem (Chain Rule). Let γ : (a, b) →Rp with range contained in G
and suppose f : G →R. If t0 ∈(a, b), γ is diﬀerentiable at t0, x0 = γ (t0), and f
is diﬀerentiable at x0, then f ◦γ : (a, b) →R is diﬀerentiable at t0 and
( f ◦γ )′(t0) = ⟨∇f (γ (t0)), γ ′(t0)⟩
Proof. We use the notation of Deﬁnition 6.2.10. Also let γ (t) −γ (t0) =
(t −t0)[γ ′(t0) + χ(t)], where χ : (a, b) →Rp such that χ(t) →0 as t →t0.
If t ∈(a, b) with t −t0 ̸= 0 and we put x0 = γ (t0), then
f ◦γ (t) −f ◦γ (t0)
t −t0
=
0
∇f (x0), γ (t) −x0
t −t0
1
+ F(γ (t))∥γ (t) −x0∥
t −t0
Now
∥γ (t) −x0∥
t −t0
=
2222
γ (t) −x0
t −t0
2222
|t −t0|
t −t0
Thus as t →t0, this remains bounded while F(γ (t)) →0. Hence
lim
t→t0
f ◦γ (t) −f ◦γ (t0)
t −t0
= lim
t→t0
0
∇f (x0), γ (t) −x0
t −t0
1
= ⟨∇f (γ (t0)), γ ′(t0)⟩
■

166
Differentiation in Higher Dimensions
6.2.15. Theorem (Mean Value Theorem). If B(a; r) ⊆Rp, f : B(a; r) →R is dif-
ferentiable, and b ∈B(a; r), then there is a point x on the line segment [a, b] such
that
D f (x)(a −b) = ⟨∇f (x), b −a⟩= f (b) −f (a)
Proof. Deﬁne the function g : [0, 1] →R by g(t) = f (tb + (1 −t)a). Using the
just established Chain Rule with γ (t) = tb + (1 −t)a we have that g′(t) =
⟨∇f (tb + (1 −t)a), γ ′(t)⟩= ⟨∇f (tb + (1 −t)a), b −a⟩. Now apply the MVT
to g.
■
6.2.16. Definition. If G is an open subset of Rp, f : G →R, x ∈G, and d ∈Rp,
then f is said to have a directional derivative at x in the direction d provided the
function t →f (x + td) is diﬀerentiable at t = 0. When the directional derivative
exists we denote it by
∇d f (x) = lim
t→0
f (x + td) −f (x)
t
If you consult the literature you may sometimes ﬁnd a small variation on the def-
inition of the directional derivative where the vector d is constrained to be a unit
vector. This diﬀerence is conceptually minor though certain formulas will be diﬀer-
ent. Note that if the vector d = ej, then the function f has a directional derivative
at x in the direction ej precisely when the corresponding partial derivative of f at x
exists, and in this case ∇e j f (x) = ∂j f (x). Also observe that if in the deﬁnition of the
directional derivative we choose r > 0 such that x + td ∈G for |t| < r and deﬁne
γ : (−r, r) →G by γ (t) = x + td, then we can apply the version of the Chain Rule
proved above. With this observation the proof of the next proposition is an imme-
diate consequence of Theorem 6.2.14
6.2.17. Proposition. If x ∈G and f : G →R is diﬀerentiable at x, then ∇d f (x)
exists for any direction d and
∇d f (x) = ⟨∇f (x), d⟩
6.2.18. Proposition. If G is a connected open subset of Rp, f : G →R is diﬀeren-
tiable at every point of G, and f ′(x) = 0 for all x in G, then f is a constant function.
Proof. If a ∈G and we consider f −f (a) instead of f , we may assume there is a
point a in G with f (a) = 0. Set X = {x ∈G : f (x) = 0}; so we want to show that
X = G. We do this by showing that X is non-empty and both open and relatively
closed in G, so that the equality will follow from the connectedness of G.
We know that X ̸= ∅since it contains a; since f is continuous on G, it fol-
lows that X is relatively closed in G. We want to show that X is also open. Fix
c in X and pick r > 0 such that B(c; r) ⊆G. If b ∈B(c; r), let d be the vector
d = b −c and deﬁne γ (t) = c + td. By Theorem 6.2.14 for all t where γ (t) ∈G,
( f ◦γ )′(t) = ⟨∇f (γ (t)), γ ′(t)⟩= 0. By Theorem 2.3.2, f ◦γ is constant. Hence
f (b) = f ◦γ (1) = f ◦γ (0) = f (c) = 0. Since b was an arbitrary point in B(c; r)
we have that B(c; r) ⊆X and so X is open.
■

6.2 Differentiable Functions, Part 1
167
We extend some deﬁnitions given in §2.4.
6.2.19. Definition. If a ∈G and f : G →R is diﬀerentiable at a, then f has a
critical point at a provided ∇f (a) = 0; equivalently, if f ′(a) = 0. We say that f has
a local maximum at a if there is a δ > 0 such that f (a)≥f (x) when ∥a −x∥<δ.
Similarly we say that f has a local minimum at a if there is a δ > 0 such that f (a) ≤
f (x) when ∥a −x∥< δ. If f has either a local maximum or local minimum at a,
then we say f has a local extremum there. If there is a vector x in Rp such that the
function t →f (a + tx) deﬁned on [−1, 1] has a local maximum at t = 0 and there
is a vector y in Rp such that the function t →f (a + ty) deﬁned on [−1, 1] has a
local minimum at t = 0, then f is said to have a saddle point at a.
6.2.20. Theorem. If a ∈G and f : G →R is diﬀerentiable on G and has a local
extremum at a, then ∇f (a) = 0.
Proof. Let d ∈Rp and consider the function t →f (a + td). Since f has a local
extremum at a, this function has a local extremum at t = 0. By Theorem 2.3.2, the
derivative of this function at t = 0 must vanish. According to Proposition 6.2.17
this means that ⟨∇f (a), d⟩= 0. Since d was an arbitrary vector in Rp, this proves
the theorem.
■
We can extend Theorem 2.4.2 to the present case.
6.2.21. Theorem. Let G be an open subset of Rp, a ∈G, and let f : G →R be a
diﬀerentiable function on all of G.
(a)
If for each vector d in Rp there is δ > 0 such that ∇d f (a −td) > 0 and
∇d f (a + td) < 0 for 0 < t < δ, then f has a local maximum at a.
(b)
If for each vector d in Rp there is δ > 0 such that ∇d f (a −td) < 0 and
∇d f (a + td) > 0 for 0 < t < δ, then f has a local minimum at a.
Proof. The idea is to look at the behavior of f near a in each direction d and apply
Theorem 2.4.2. That is, for each vector d in Rp consider the function t →f (a + td)
and apply Theorem 2.4.2 to this function. Exercise 9 asks the reader to supply the
details.
■
We’ll say more about critical points in §6.6.
6.2.22. Proposition. Let G be an open subset of Rp, let a ∈G, and let f : G →R
be a diﬀerentiable function on all of G. If d is any unit vector in Rp, then |∇d f (a)| ≤
∥∇f (a)∥. Equality is achieved when d = ∥∇f (a)∥−1∇f (a).
Proof. According to Proposition 6.2.17 ∇d f (a) = ⟨∇f (a), d⟩. Therefore by the
Cauchy–Schwarz Inequality, |∇d f (a)| ≤∥∇f (a)∥. The statement on equality is
immediate.
■
In light of the preceding proposition we conclude that for any a the vector ∇f (a)
points in the direction in which the scalar-valued function f is changing the fastest.

168
Differentiation in Higher Dimensions
Exercises
(1)
Deﬁne f : R2 →R by
f (x, y) =
	 (x2y+xy2) sin(x−y)
x2+y2
when (x, y) ̸= (0, 0)
0
when (x, y) = (0, 0)
Find ∂x f (x, y) and ∂y f (x, y) for every point (x, y) in R2. (Be careful when (x, y) =
(0, 0).)
(2)
Supply the missing details in Example 6.2.2.
(3)
Show that the function f deﬁned in Exercise 5.4.14 has partial derivatives at the
origin with ∂1 f (0, 0) = ∂2 f (0, 0) = 0 even though it fails to be continuous at (0, 0).
(4)
Show that if L, K ∈Rp∗and L(ej) = K(ej) for 1 ≤j ≤p, then L(x) = K(x) for all
x in Rp.
(5)
(a) Show that if x ∈Rp and ⟨x, y⟩= 0 for every y in Rp, then x = 0. (b) Prove the
uniqueness statement in Proposition 6.2.8.
(6)
Let f : Rp →R and let x ∈Rp. Suppose we say that the function f has Property
D at x if
lim
y→x
f (y) −f (x)
∥y −x∥
exists. (a) Show that if f is diﬀerentiable at x, it has property D at x. (b) Does
Property D at x imply that f is diﬀerentiable at x? (Prove or give a conterexample.)
(7)
Prove Proposition 6.2.11.
(8)
(a) Let f (x, y, z) = exp(x2 + z + cos y) and ﬁnd ∇d f (x, y, z) when d = (1, π, 0).
(b) Let f : R3 →R be deﬁned by f (x, y, z) = xyz −x2 + 3x3 and d = (−1, 2, 1)
and calculate ∇d f .
(9)
Supply the required details for the proof of Theorem 6.2.21.
6.3. Orthogonality
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
This section and the next are a linear algebra interlude, one that is required to go
further in our study of diﬀerentiability in Rp. Speciﬁcally, when we study the diﬀer-
entiability of functions from open subsets of Rp into Rq, we need to explore linear
transformations and the inner product more extensively than we have.
6.3.1. Definition. If x, y ∈Rp we say that x and y are orthogonal if ⟨x, y⟩= 0; in
symbols we write x ⊥y. Say that the vectors in a non-empty subset S of Rp are
pairwise orthogonal if x ⊥y whenever x and y are distinct points in S. Say that the
set S is orthonormal if S is pairwise orthogonal and also ∥x∥= 1 for all x in S. An
orthonormal basis for Rp is a basis for Rp that is also an orthonormal set.
If S ⊆Rp, write x ⊥S when x ⊥y for all y in S. Let S⊥denote the set of all
vectors that are orthogonal to the set S. Two non-empty subsets S and T of Rp are

6.3 Orthogonality
169
said to be orthogonal if every vector in S is orthogonal to every vector in T ; in
symbols, S ⊥T .
First note that x ⊥x if and only if x = 0. Now observe that if S is a pairwise
orthogonal set of non-zero vectors, then the vectors in S are linearly independent.
In fact, if x1, . . . , xm are distinct vectors in S and if α1, . . . , αm are scalars such that
m
j=1 α jxj = 0, then for a ﬁxed k, 1 ≤k ≤m,
0 =
3 m

j=1
α jxj, xk
4
=
m

j=1
α j⟨x j, xk⟩= αk∥xk∥2
So each αk is zero. Since Rp has dimension p, S cannot have more than p vectors.
Examples of pairwise orthogonal sets abound. To start, any non-empty subset of
the standard basis for Rp is an orthonormal set, and the standard basis is an exam-
ple of an orthonormal basis. There are other orthonormal bases as we will see in
Corollary 6.3.4 below.
The reader may recall some of this from linear algebra. Assume the vectors x and y
are linearly independent and consider the two-dimensional subspace of Rp spanned
by them. These vectors are orthogonal if the straight lines they deﬁne form a right
angle. Note that for any x, y we have
∥x + y∥2 = ⟨x + y, x + y⟩= ∥x∥2 + ⟨x, y⟩+ ∥y∥2
This is called the polar identity. From here we get the following.
6.3.2. Proposition (Pythagorean1 Theorem). If x1, . . . , xm are pairwise orthogonal
vectors in Rp, then
∥x1 + · · · + xm∥2 = ∥x1∥2 + · · · + ∥xm∥2
Proof. If m = 2, then this easily follows from the polar identity. The proof can now
be completed using induction (Exercise 1).
■
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
1 Pythagoras of Samos was born around 569 BC in Samos, a city in Ionia. Little is known about him, especially
since none of the work he wrote still exists. He was the leader of a society dedicated to science and religion, and
some of the ancient books claim that he had divine powers. When he was a young man Thales introduced him to
mathematics, and it is likely that Anaximander, a student of Thales, gave a series of mathematical lectures that
Pythagoras attended. All dates here are dubious, but around 535 BC he traveled to Egypt. From here he adopted
many practices of Egyptian priests, which he later imposed on the members of his society. These included the
refusal to wear anything made from an animal skin and an abstinence from eating beans. In 525 BC the Persians
invaded Egypt and brought Pythagoras to Babylon. In 520 BC he left there and returned to Samos; there is
no explanation how he obtained his freedom. After founding his society, in 518 BC he went to southern Italy.
Apparently while he was in Samos he used the “symbolic method of teaching” and the Samians did not approve.
In what is present day Crotone, half-way up the heel of Italy, he founded a mathematical and philosophical
society. One of their beliefs was that at its deepest level, reality is mathematical in nature. This present theorem
(in two-dimensions) was known to the Babylonians 1000 years before Pythagoras, though it is likely he was
the ﬁrst to prove it. He introduced abstraction into mathematics and made many other discoveries in geometry,
including that the sum of the interior angles of a triangle equals two right angles. He was also intensely interested
in numbers and discovered the irrationals. He died about 475 BC. The Pythagorean Society continued after his
death but they made powerful enemies. Eventually they were attacked and some 50 or 60 members were killed.
Those who survived took refuge at Thebes and other places.

170
Differentiation in Higher Dimensions
We observed earlier that a set of pairwise orthogonal vectors is linearly indepen-
dent. A similar argument shows that if x ∈Rp and x ⊥S, then x ⊥5 S, the linear
span of S (Exercise 3).
6.3.3. Theorem (Gram2–Schmidt3 Process). If x1, . . . , xm are linearly indepen-
dent, then there are orthonormal vectors y1, . . . , ym such that for 1 ≤j ≤m, yj is
in the linear span of {x1, . . . , xj}. Consequently,
6
{x1, . . . , xj} =
6
{y1, . . . , yj}
for 1 ≤j ≤m.
Proof. Observe that the last statement of the theorem follows from the ﬁrst. In fact
since the ﬁrst part of the proposition implies 5{y1, . . . , yj} ⊆5{x1, . . . , xj}, to
show equality we need only show that these two subspaces have the same dimension.
But since {y1, . . . , yj} are orthonormal and {x1, . . . , xj} are linearly independent,
both subspaces have dimension j.
The proof of the ﬁrst part is by induction on m. When m = 1 just take y1 =
∥x1∥−1x1. Now assume the proposition is true for some k < m and that x1, . . . ,
xk+1 are linearly independent. By the induction hypothesis there are orthonormal
vectors y1, . . . , yk such that for 1 ≤j ≤k, yj is in the linear span of {x1, . . . , xj}.
Consider y = xk+1 −k
i=1⟨xk+1, yi⟩yi. If 1 ≤j ≤k, then ⟨y, yj⟩= ⟨xk+1, yj⟩−
k
i=1⟨xk+1, yi⟩⟨yi, yj⟩= ⟨xk+1, yj⟩−⟨xk+1, yj⟩= 0.
Also
note
that
y ̸= 0
since xk+1 /∈5{x1, . . . , xk} = 5{y1, . . . , yk}. If we set yk+1 = ∥y∥−1y, then
{y1, . . . , yk+1} is an orthonormal set. It is left to the reader to check that yk+1 ∈
5{x1, . . . , xk+1}.
■
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
2 Jorgen Pedersen Gram was the son of a farmer, born in 1850 in Nustrup, Denmark. In 1868 he began his uni-
versity education, receiving a masters degree in 1873. When he received this degree he had already published
his ﬁrst paper in algebra. In 1875 he began work with an insurance company, but this work led him into a study
of probability and numerical analysis. He soon published a paper on these topics. As a result of this paper he
was awarded a doctorate in 1879. His work here, which he also applied to forestry, led him to study abstract
problems in number theory. He continued working at the insurance company but was invited to lecture in the
Danish Mathematical Society. For his work the Videnskabernes Society awarded him their Gold Medal in 1884.
He married twice. The ﬁrst time in 1879 and the second in 1896, just over a year after his ﬁrst wife’s death. He
died in 1916 in Copenhagen; he was on his way to a meeting of the Videnskabernes Society and was struck by
a bicycle.
3 Erhard Schmidt was born in 1876 in what is today Tartu, Estonia. His early education was typical of someone
who was born to a professional family. (His father was a medical biologist.) He began his university studies in
Berlin and went to Göttingen where he obtained his doctorate under the supervision of Hilbert; his thesis was in
integral equations. He went to Bonn and this was followed by several academic posts before he was awarded a
professorship at Berlin in 1917. He was quickly drawn into administrative matters involved with ﬁlling recently
vacated faculty positions. He is credited with establishing applied mathematics at Berlin. Needless to say his
role at the university was diﬃcult during the Nazi era. Many of his Jewish colleagues were forced out of their
positions. Some have criticized his role in this and others have defended it. In the ﬁnal analysis his reputation
survived. The present result was obtained by Schmidt independently in 1907. There are also the Hilbert–Schmidt
operators named after him. He deserves a place as one of several mathematicians who founded the abstract theory
of functional analysis. He died in 1959 in Berlin.

6.3 Orthogonality
171
We will eventually want to manufacture an orthonormal basis for Rp different
from the standard basis. The next corollary allows us to do this.
6.3.4. Corollary.
If E0 is a set of orthonormal vectors in Rp, then there is an
orthonormal basis E for Rp that contains E0.
Proof. From linear algebra we know there is a basis B for Rp that contains E0 since
E0 is a linearly independent set. By The Gram–Schmidt Process we can replace
B by orthonormal vectors {y1, . . . , yp} with the same span. Hence {y1, . . . , yp}
is an orthonormal basis. But if the Gram–Schmidt process is examined, we see
that the orthonormal vectors E0 will not be altered by the process. Hence E0 ⊆
{y1, . . . , yp}.
■
The proof of the next result is Exercise 4.
6.3.5. Proposition. If y1, . . . , yp is an orthonormal basis for Rp and x ∈Rp, then
x =
p

j=1
⟨x, y j⟩y j
The next result is crucial in our study of orthogonality.
6.3.6. Theorem.
If M is a vector subspace of Rp and x ∈Rp, then there is a
unique vector y0 in M such that
∥x −y0∥= dist (x, M) = inf{∥x −y∥: y ∈M}
In addition x −y0 ⊥M.
Proof. Let {y1, . . . , ym} be an orthonormal basis for M and set
y0 =
m

j=1
⟨x, y j⟩y j
We want to show that y0 has the desired properties. It is easily checked that ⟨x −
y0, yj⟩= 0 for 1 ≤j ≤m. Since {y1, . . . , ym} is a basis for M, this shows that x −
y0 ⊥M. Now let y be an arbitrary vector in M. Since x −y0 ⊥M and y −y0 ∈
M, the Pythagorean Theorem implies that
∥x −y∥2 = ∥(x −y0) −(y −y0)∥2
= ∥x −y0∥2 + ∥y −y0∥2
≥∥x −y0∥2
This shows that [dist (x, M)]2 ≥∥x −y0∥2. Since y0 ∈M, we must have equality.
Before showing the uniqueness of y0 we ﬁrst prove the following, which is another
way in which y0 is unique.
Claim. y0 is the only vector in M such that x −y0 ⊥M.

172
Differentiation in Higher Dimensions
Suppose z0 is another vector in M such that x −z0 ⊥M. Since ⟨x −y0, yi⟩=
0 = ⟨x −z0, yi⟩, it follows that ⟨z0, yi⟩= ⟨y0, yi⟩for 1 ≤i ≤m. Thus z0 −y0 is
both in M and orthogonal to M. In particular it is orthogonal to itself and thus
z0 −y0 = 0.
Now assume w0 ∈M such that ∥x −w0∥= dist (x, M). If y ∈M, then w0 +
y ∈M so using the polar identity we get that
∥x −w0∥2 ≤∥x −(w0 + y)∥2
= ∥(x −w0) −y∥2
= ∥x −w0∥2 −2⟨x −w0, y⟩+ ∥y∥2
Thus 2⟨x −w0, y⟩≤∥y∥2 for all y in M. If y ∈M, then by substituting −y for y
if necessary we may assume that ⟨x −w0, y⟩≥0. Letting t > 0 and replacing y by
ty, the inequality becomes 2t⟨x −w0, y⟩≤t2∥y∥2, valid for all t > 0. Divide both
sides by t and letting t →0 shows that ⟨x −w0, y⟩= 0. That is, x −w0 ⊥M. By
the claim, this implies that w0 = y0.
■
If x, M, and y0 are as in the preceding theorem, then y0 is called the orthogonal
projection of x onto M. We will return to this concept after we begin the study of
linear transformations in the next section. See Deﬁnition 6.4.13.
6.3.7. Definition. When M and N are two linear subspaces of Rp and M ⊥N,
write
M ⊕N = {x + y : x ∈M, y ∈N}
This is actually the linear space M + N and the symbol ⊕is used for emphasis
of the fact that every vector in M is orthogonal to every vector in N. In fact we will
sometimes want to write x ⊕y when x ⊥y.
Exercises
(1)
Perform the induction argument needed to complete the proof of the Pythagorean
Theorem.
(2)
Prove the Parallelogram Law: ∥x + y∥2 + ∥x −y∥2 = 2(∥x∥2 + ∥y∥2) for all x and
y in Rp. Why is this called the parallelogram law?
(3)
Prove that if S ⊆Rp and x is a vector that is orthogonal to S, then x ⊥5 S.
(4)
Prove Proposition 6.3.5. (Hint: Show that [x −p
j=1⟨x, y j⟩y j] ⊥yk for 1 ≤k ≤p.)
(5)
In R3 let x1 = (1, 0, 2), x2 = (0, 1, 2), and x3 = (1, 2, 0). Show that {x1, x2, x3} is
a set of linearly independent vectors and carry out the Gram–Schmidt process to
manufacture the corresponding orthonormal basis.
(6)
In R4 let x1 = (1, 0, 2, 0), x2 = (0, 1, 2, 0), x3 = (1, 2, 0, 0), x4 = (0, 0, 0, 1) and
describe the vector space 5{x1, x3}⊥.

6.4 Linear Transformations
173
6.4. Linear Transformations
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
We want to deﬁne the derivative of a function f : G →Rq when G is an open subset
of Rp. When q = 1 we saw that the natural object to deﬁne as f ′(x) was a linear
functional. When q > 1, we’ll see that the derivative is a linear transformation A :
Rp →Rq. In this section we will discuss some of the essential features of linear
transformations. We continue to assume that every reader knows the basics of vector
spaces and has been exposed to linear transformations. For some what is presented
here may be familiar. If that’s the case just be patient or you can skip to the next
section, though I suspect most readers will encounter some material here for the
ﬁrst time. In particular I think Theorem 6.4.29 below will be a ﬁrst-time experience
for most readers of this book.
Only linear transformations between Rp and Rq are discussed here, rather than
the usual linear transformations between arbitrary vector spaces. Recall that a lin-
ear transformation is a function A : Rp →Rq such that whenever α, β ∈R and
x, y ∈Rp we have that A(αx + βy) = αA(x) + βA(y). (A notational point should
be made here: we will often use the notation Ax rather than A(x). This practice is
widespread and not limited to the author.) The set of all linear transformations from
Rp into Rq is denoted by L(Rp, Rq). This itself is a vector space where for A, B in
L(Rp, Rq) and α, β in R we deﬁne (αA + βB)(x) = αA(x) + βB(x). We note that
L(Rp, R) = Rp∗. An important special case occurs when q = p. In this case we let
L(Rp) = L(Rp, Rp)
Also recall that the kernel and range of A are deﬁned as
ker A = {x ∈Rp : Ax = 0}
ran A = {y ∈Rq : there is an x in Rp with Ax = y}
= A(Rp)
So A is injective if and only if ker A = (0); and A is surjective if and only if
ran A = Rq.
The ﬁrst step is to represent the linear transformation as a q × p matrix. To facil-
itate this and keep things straight, we’ll continue to denote the standard basis in
Rp by e1, . . . , ep; however, we will denote the standard basis in Rq by d1, . . . , dq.
So when A ∈L(Rp, Rq) and 1 ≤j ≤p,
A(ej) =
q

i=1
aijdi =
q

i=1
⟨A(ej), di⟩di
This leads to the matrix representation of A
6.4.1
⎡
⎢⎢⎢⎣
a11
a12
· · ·
a1p
a21
a22
· · ·
a2p
...
...
...
...
aq1
aq2
· · ·
aqp
⎤
⎥⎥⎥⎦

174
Differentiation in Higher Dimensions
We assume the reader remembers matrix multiplication and that if x =
(x1, . . . , xp) ∈Rp, then
A(x) =
⎡
⎢⎢⎢⎣
a11
a12
· · ·
a1p
a21
a22
· · ·
a2p
...
...
...
...
aq1
aq2
· · ·
aqp
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
x1
x2
...
xp
⎤
⎥⎥⎥⎦
which is a q × 1 column vector; equivalently, a vector in Rq.
When A ∈L(Rp, Rq), deﬁne (see Exercise 2) the norm of A to be the quantity
∥A∥= sup{∥Ax∥: x ∈Rp and ∥x∥≤1}
We might point out that when q = 1 and the linear transformation A becomes the
linear functional L, we deﬁned the norm of L in Proposition 6.2.9. It is left to the
reader to prove the following in Exercise 3.
6.4.2. Proposition. If A, B ∈L(Rp, Rq) and α ∈R, then the following hold.
(a) ∥αA + B∥≤|α|∥A∥+ ∥B∥.
(b) d(A, B) = ∥A −B∥deﬁnes a metric on L(Rp, Rq).
(c) For every x in Rp we have that ∥Ax∥≤∥A∥∥x∥.
(d) If q = p, then AB ∈L(Rp, Rq) and ∥AB∥≤∥A∥∥B∥.
6.4.3. Corollary. With the metric d on L(Rp, Rq) deﬁned in the proposition, addi-
tion and scalar multiplication are continuous functions.
Proof. See Exercise 4.
■
6.4.4. Proposition. (a) With the deﬁnition of distance given in the preceding propo-
sition, L(Rp, Rq) is a complete metric space.
(b) If {An} is a sequence in L(Rp, Rq) and ∞
n=1 ∥An∥< ∞, then the inﬁnite series
∞
n=1 An converges in L(Rp, Rq).
Proof. (a) If {An} is a Cauchy sequence in L(Rp, Rq) and x ∈Rp, then ∥Anx −
Amx∥= ∥(An −Am)x∥≤∥x∥∥An −Am∥; it follows that {Anx} is a Cauchy sequence
in Rq. Hence there is a vector y in Rq such that Anx →y; deﬁne A : Rp →Rq by
Ax = limn Anx. It is left to the reader to show that A is a linear transformation. It
remains to show that ∥An −A∥→0. If x ∈Rp with ∥x∥≤1, then ∥Anx −Ax∥≤
∥Anx −Amx∥+ ∥Amx −Ax∥≤∥An −Am∥+ ∥Amx −Ax∥. If ϵ > 0 then we can
choose N such that ∥An −Am∥< ϵ/2 when m, n ≥N. Thus ∥Anx −Ax∥< ϵ/2 +
∥Amx −Ax∥when n, m ≥N. Now choose M > N such that ∥Amx −Ax∥< ϵ/2
when m ≥M. (Note that M depends on x.) This implies that ∥Anx −Ax∥< ϵ when
n ≥N and the value of N is independent of x. Since x was arbitrary with ∥x∥≤1,
we have that ∥An −A∥< ϵ when n ≥N. That is, An →A in L(Rp, Rq).
(b) The proof of this part is like that of the Weierstrass M-Test (4.2.2). Put
Mn = ∥An∥and note that ∥Anx∥≤Mn∥x∥for each x in Rp. Put Bn = n
k=1 Ak in

6.4 Linear Transformations
175
L(Rp, Rq). We have that for n > m,
∥Bn −Bm∥=
22222
n

k=m+1
Ak
22222 ≤
n

k=m+1
∥Ak∥
Thus {Bn} is a Cauchy sequence in L(Rp, Rq). By (a) there is a linear transformation
A in L(Rp, Rq) such that Bn →A.
■
When p = q we have an additional property.
6.4.5. Proposition. If {An} is a sequence of invertible linear transformations in
L(Rp), A is an invertible linear transformation in L(Rp), and An →A, then A−1
n
→
A−1.
Proof. We begin with the following.
Claim. If B ∈L(Rp) and ∥1 −B∥< 1, then B is invertible and
B−1 =
∞

n=0
(1 −B)n
To prove the claim ﬁrst observe that since there is an r with ∥1 −B∥< r < 1, we
have that ∥(1 −B)n∥< rn and so the series ∞
n=0 ∥(1 −B)n∥converges. By part
(b) of the preceding proposition this implies that C = ∞
n=0(1 −B)n converges in
L(Rp). Put Cn = n
k=0(1 −B)k. Now
CnB = Cn −Cn(1 −B)
=
n

k=0
(1 −B)k −
n+1

k=1
(1 −B)k
= 1 −(1 −B)n+1
But ∥(1 −B)n+1∥→0, so we have that CB = 1. Similarly BC = 1 and so the claim
is established. (Using linear algebra we could have said that 1 = CB means B is left
invertible, but a linear transformation on Rp that is left invertible is invertible.)
Claim. If {Bn} is a sequence in L(Rp) and Bn →1, then there is an integer N such
that Bn is invertible for all n ≥N and B−1
n
→1.
We choose N1 such that ∥Bn −1∥< 1 when n ≥N1. By the preceding claim Bn
is invertible for all such n. If δ > 0 (we’ll further specify δ in a moment), choose
N > N1 such that ∥1 −Bn∥< δ when n ≥N. Again the ﬁrst claim implies that
B−1
n
= [1 −(1 −Bn)]−1 = ∞
k=0(1 −Bn)k = 1 + ∞
k=1(1 −Bn)k. Hence
22B−1
n
−1
22 =
22222
∞

k=1
(1 −Bn)k
22222 ≤
∞

k=1
∥1 −Bn∥k <
δ
1 −δ
Now if ϵ > 0 we can choose δ > 0 with δ/(1 −δ) < ϵ and we have that ∥B−1
n
−
1∥< ϵ when n ≥N, establishing the claim.

176
Differentiation in Higher Dimensions
Now to ﬁnish the proof. Using the notation in the statement of the proposition,
we have that A−1An →1. By the last claim
A−1
n A =

A−1An
−1 →1
Therefore A−1
n
= (A−1
n A)A−1 →A−1 by Exercise 4.
■
The preceding proposition can be rephrased as follows: If Gp is the set of all
invertible linear transformations from Rp into itself, then the map A →A−1 is a
continuous function from Gp into itself.
Writing x = p
j=1⟨x, e j⟩e j and using the notation established in (6.4.1) we have
A(x) =
p

j=1
A(⟨x, ej⟩e j) =
p

j=1
⟨x, e j⟩A(ej)
=
p

j=1
⟨x, e j⟩
q

i=1
aijdi =

i, j
⟨x, e j⟩aijdi
Hence by the Cauchy–Schwarz Inequality
∥A(x)∥2 ≤
⎡
⎣
i, j
|⟨x, ej⟩||aij|∥di∥
⎤
⎦
2
≤
⎡
⎣
j
|⟨x, ej⟩|2
⎤
⎦
⎡
⎣
i, j
|aij|2
⎤
⎦
= ∥x∥2
⎡
⎣
i, j
|aij|2
⎤
⎦
This implies the following.
6.4.6. Proposition. If A ∈L(Rp, Rq) and A has the matrix (aij), then
∥A∥≤
⎡
⎣
i, j
|aij|2
⎤
⎦
1
2
See Exercise 6.
Let A ∈L(Rp, Rq) and ﬁx a vector z in Rq. Observe that the map x →⟨A(x), z⟩
is a linear functional on Rp. By Proposition 6.2.8 there is a unique vector yz in
Rp such that ⟨A(x), z⟩= ⟨x, yz⟩for every x in Rp. Since this vector yz is unique,
we have deﬁned a function from Rq into Rp: z →yz. Note that the deﬁnition of
this map depends on the linear transformation A and so we denote it by A∗; that is
yz = A∗(z) ∈Rp for every z in Rp. From what we have just done we have that for

6.4 Linear Transformations
177
all x in Rp and all z in Rq,
6.4.7
⟨x, A∗(z)⟩= ⟨A(x), z⟩
The function A∗is called the adjoint of A. The reader is asked to recall from his/her
encounter with linear algebra the deﬁnition of the transpose of a matrix. The trans-
pose of a q × p matrix (aij) is denoted by (aij)t and is the p × q matrix that has as
its ji-entry the number aji. This is needed in the next proposition.
6.4.8. Proposition. If A ∈L(Rp, Rq), then the function A∗: Rq →Rp is a linear
transformation. The matrix of the adjoint A∗is the transpose of the matrix for A.
Proof. The proof that A∗∈L(Rq, Rp) is Exercise 7. Observe that the matrix of A∗
is size p × q. If its matrix is given by (bji) with 1 ≤j ≤p, 1 ≤i ≤q, then bji =
⟨A∗(di), ej⟩. But using (6.4.7) we have that bji = ⟨ej, A∗(di)⟩= ⟨A(ej), di⟩= aij.
Therefore the matrix (bji) is precisely (aij)t.
■
We also have the following. The proof is left to the reader in Exercise 8(a); doing
this exercise will help further cement the properties of the adjoint in your mind.
6.4.9. Proposition. If A, B ∈L(Rp, Rq) and λ ∈R, then (A + λB)∗= A∗+ λB∗.
We now focus on linear transformations A : Rp →Rp. So when A ∈L(Rp), its
matrix is a square one of size p × p. Recall from linear algebra that in this case the
three statements A is invertible, A is injective, and A is surjective are equivalent.
When A ∈L(Rp) we have that A∗∈L(Rp). When A, B ∈L(Rp) we can form
the product AB. In this case we have the following (Exercise 8(b)).
6.4.10. Proposition. If A, B ∈L(Rp), then (AB)∗= B∗A∗.
6.4.11. Definition. A linear transformation in L(Rp) is self-adjoint or hermitian4
if A = A∗.
A linear transformation A is hermitian if and only if its matrix is equal to its
transpose. That is, A = A∗if and only if (aij) = (aij)t. (Why?) Thus A = (aij) is
hermitian if and only if aij = a ji for all i and j. In other words, A is hermitian if and
only if its matrix is symmetric. In the literature many will use the term “symmetric”
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
4
Charles Hermite was born in 1822 in Dieuze, France, which is east of Paris near the German border. In 1840
he went to school at Collège Louis-le-Grand in Paris, 15 years after Galois studied there. His tendency was
to read original papers of mathematicians rather than work to pass the exams. Nevertheless, with a somewhat
average performance on the entrance exam, he was admitted to the École Polytechnique. Unfortunately he had
a birth defect that resulted in a malformed foot and because of this he was told he had to leave. (From today’s
perspective, this is truly amazing; but such things happened.) An appeal led to a reversal of the decision, but
strict conditions were imposed on him and he decided to leave. On his own he pursued his studies, all the while
doing research. In 1847 he passed the exams to receive the baccalauréat. A year later he was appointed to the
faculty at École Polytechnique, the same school that had made life diﬃcult for him. He worked on number theory
and algebra, orthogonal polynomials, and elliptic functions, with several important contributions. The Hermite
polynomials are named after him, and he was the ﬁrst to prove that the number e is transcendental – that is, it is
not the zero of any polynomial with rational coeﬃcients. He had nine doctoral students, including Poincaré and
Stieltjes. He died in 1901 in Paris.

178
Differentiation in Higher Dimensions
for these linear transformations rather than “hermitian.” This discrepancy with our
usage of hermitian arises because you can also carry out this study of linear trans-
formations and adjoints on vector spaces over the complex numbers and some prefer
to reserve the term hermitian for this case. The point is when you use the complex
numbers the matrix of a self-adjoint linear transformation is not quite symmetric. If
you are curious and/or interested, please investigate the literature.
6.4.12. Example.
Denote by diag (λ1, . . . , λp) the matrix that has the entries
λ1, . . . , λp along its main diagonal and zeros everywhere else. Call such a matrix a
diagonal matrix. A linear transformation whose matrix is diagonal is hermitian.
Warning. Soon we will prove an important result about hermitian linear transfor-
mations (6.4.29). To do this we need to extend the deﬁnition of the adjoint, and
therefore of hermitian linear transformations, to linear transformations deﬁned on
a subspace of Rp rather than the entirety of Euclidean space. In reality this is a
technical matter and not a substantial problem since, as some of you may recall
from your course on linear algebra, all ﬁnite dimensional vector spaces over R are
isomorphic to some Euclidean space. The details of all this are not presented here
as this extension is only used in one place in the proof of Theorem 6.4.29. The inter-
ested reader can carry out the details as a project; this would involve taking each
result and deﬁnition involving the adjoint and making the appropriate modiﬁcations
so that it holds for linear transformations deﬁned on a linear subspce M of Rp.
We now use the main result obtained in the last section (Theorem 6.3.6) to deﬁne
an important hermitian linear transformation.
6.4.13. Definition. If M is a linear subspace of Rp and x ∈Rp, then the unique
vector Px = P(x) in M such that x −Px ⊥M is called the orthogonal projection
of x onto M.
Note that if x ∈M, then Px = x; if x ⊥M, then Px = 0. The converses of these
two statements are also true.
6.4.14. Proposition. If M is a linear subspace of Rp and, for each x in Rp, Px is
the orthogonal projection of x onto M, then the following hold.
(a) P : Rp →Rp is a hermitian linear transformation.
(b) ∥Px∥≤∥x∥for all x in X.
(c) P is an idempotent; that is, P2 ≡PP = P.
(d) ker P = M⊥and ran P = M = {x ∈Rp : Px = x}.
(e) If y1, . . . , ym is any orthonormal basis for M, then
Px =
m

j=1
⟨x, y j⟩y j.
Proof. Note that (e) has already been proven when we proved Theorem 6.3.6 and
is only included for emphasis. But (e) easily implies that P is linear. Also for any

6.4 Linear Transformations
179
x, w in Rp
⟨Px, w⟩=
3 m

j=1
⟨x, y j⟩y j, w
4
=
m

j=1
⟨x, y j⟩⟨yj, w⟩
=
m

j=1
9
x, ⟨w, yj⟩y j
:
=
3
x,
m

j=1
⟨w, yj⟩y j
4
= ⟨x, Pw⟩
Hence P = P∗, completing the proof of (a).
Since Px ⊥x −Px, we have by the Pythagorean Theorem that ∥x∥2 = ∥x −Px +
Px∥2 = ∥x −Px∥2 + ∥Px∥2 ≥∥Px∥2, proving (b).
The meaning of (c) is that P(P(x)) = P(x) for all x in Rp. If x is any vector in Rp,
then Px ∈M. But as we observed, Py = y for all y in M, so P(Px) = Px.
Using (e) it is clear that ran P ⊆M and M⊥⊆ker P. Since Px = x whenever
x ∈M, it must be that ran P = M. Also since ∥Px∥2 = m
j=1 |⟨x, yj⟩|2, the only
way that x can be in ker P is for x to be orthogonal to each yj, 1 ≤j ≤m. Since the
yj form a basis for M, it follows that x ⊥M whenever x ∈ker P.
■
6.4.15. Definition. An orthogonal projection is an idempotent P : Rp →Rp such
that x −Px ⊥ran P for every x in Rp.
In a certain sense the preceding deﬁnition is redundant. There is a sense, however,
in which it is required. Deﬁnition 6.4.13 depends on ﬁrst being given a subspace
M of Rp; Deﬁnition 6.4.15 deﬁnes what it means for a linear transformation to be an
orthogonal projection without ﬁrst being given such a subspace. On the other hand,
if P is as in Deﬁnition 6.4.15 and M = ran P, then P is the orthogonal projection
of Rp onto M as in Deﬁnition 6.4.13.
We need to recall and establish the deﬁnition and properties of the determinant
for a square matrix. The reader is assumed to be somewhat familiar with this, but
my experience is that this material is not fully known to most students who take
this course; consequently there follows a presentation of determinants, including
the deﬁnition. This must be preceded, however, by a discussion of permutations.
Most of the results on permutations will be stated without proof. The reader can
consult any source on permutations for the missing details, but I have used [2].
For p ≥2 we want to consider an ordered set having p elements, where the word
“ordered” is key. An example, of course, is the set of the ﬁrst p integers, {1, . . . , p}.
A permutation is a reordering of the set. An example when p = 5 is given by

180
Differentiation in Higher Dimensions
(2, 3, 1, 5, 4); the meaning of this notation is that this permutation maps 2 →3 →
1 →5 →4 →2. Another permutation when p = 5 is (2, 3, 1)(5, 4); this permuta-
tion maps 2 →3 →1 →2 and 5 →4 →5. Another is (2, 3, 1) where the absence
of 4 and 5 means they are left ﬁxed; if you prefer, (2, 3, 1) = (2, 3, 1)(4)(5). In par-
ticular (1) denotes the identity permutation that leaves every number ﬁxed. If you
prefer, a permutation is a bijection of the set, but where the resulting order is impor-
tant. We let Sp denote the set of all permutations of the integers 1, . . . , p. If you
know the concept of a group, Sp is a group under composition: if σ, τ ∈Sp, then
στ is the element of Sp deﬁned by (στ )(k) = σ (τ(k)) for 1 ≤k ≤p. In fact it is
one of the ﬁrst examples of a group when you study this subject; it is called the
symmetric group on p elements. See Exercise 9.
A permutation that switches two of the elements and leaves all the others ﬁxed
is called a transposition. For example the permutation (4, 5) is a transposition that
interchanges 4 and 5 and leaves the other integers ﬁxed. A fact that seems intuitively
clear after thinking about it but requires proof (see [2], p. 94) is that every σ in Sp
is the product of a ﬁnite number of transpositions. As one example (1, 2, . . . , p) =
(1, 2)(1, 3) · · · (1, p). The way to write σ as a product of transpositions, however,
is not unique.
6.4.16. Proposition. Any permutation σ in Sp can be written as a product of trans-
positions. If there are two products of transpositions that are both equal to σ, one
containing m factors and the other containing n factors, then m and n are either
simultaneously even or simultaneously odd.
A proof of the existence of the factorization could be concocted by using the
example factorization of (1, 2, . . . , p) above. For a complete proof of the result see
[2], p. 94.
This enables us to deﬁne the sign of a permutation or its parity. If σ ∈Sp, deﬁne
sign(σ ) to be +1 if σ is the product of an even number of transpositions and
sign(σ ) = −1 otherwise. In light of the preceding proposition, this is well-deﬁned.
6.4.17. Proposition. If σ, τ ∈Sp, then sign(στ ) = sign(σ )sign(τ ).
Proof. Let τ be arbitrary in Sp and let σ be a transposition. Thus the factorization
of στ as a product of transpositions has one more transposition than a factorization
of τ and so sign(στ ) = −sign(τ ) = sign(σ )sign(τ ). If we repeatedly apply this to
an arbitrary σ, we get the result.
■
We now use this material on permutations to deﬁne the determinant of a square
p × p matrix. Let A ∈L(Rp) with p × p matrix (aij); for convenience let a(i, j) =
aij.
6.4.18. Definition. If A = (aij) is a p × p matrix, deﬁne the determinant of A as
det A =

σ∈Sp
sign(σ ) a(1, σ (1)) · · · a(p, σ (p))

6.4 Linear Transformations
181
It is helpful to have another expression for the sign of a permutation. To state it
we need to introduce the sign of a real number a as: si(a) = 1 if a > 0; si(a) = −1
if a < 0; si(0) = 0. (A word about notation. The usual notation for the sign of a real
number a is sign(a). I’ve chosen the diﬀerent notation si(a) to avoid any confusion
with the sign of a permutation.)
6.4.19. Lemma. For any σ in Sp,
sign(σ ) =
;
k<m
si(σ (m) −σ (k))
Hence for any p × p matrix A = (ai, j),
det A =

s( j1, . . . , jp)a(1, ji) · · · a(p, jp)
where s( j1, . . . , jp) = <
k<m si( jm −jk) and the sum is taken over all the distinct
p-tuples ( j1, . . . , jp) with 1 ≤jk ≤p.
Proof. The proof of the formula for sign(σ ) can be fashioned from the material in
[2], page 98 in the section labeled “Second Proof”. The proof of the additional for-
mula for det A is immediate from the formula once we realize that s( j1, . . . , jp) =
s(σ (1), . . . , σ (p)), where σ is the permutation deﬁned as σ (m) = jm.
■
There are other ways to deﬁne the determinant, and if you have deﬁned it dif-
ferently you should show that the two deﬁnitions are the same. You might also try
Exercise 10. It will be helpful to regard the determinant as a function of the columns
of A. So if x1, . . . , xp ∈Rp, we deﬁne det(x1, . . . , xp) = det A, where A is the matrix
with column vectors x1, . . . , xp.
6.4.20. Theorem. If A ∈L(Rp) with columns x1, . . . , xp, the following hold.
(a) det I = 1.
(b)
If σ ∈Sp and B is the matrix with columns xσ (1), . . . , xσ (p), then det B =
sign(σ ) det A.
(c) If two columns in A are equal, then det A = 0.
(d) For 1 ≤j ≤p, if the columns {xk : k ̸= j} are held ﬁxed, then the function
x j →det(x1, . . . , xp) is a linear functional of Rp into R.
(e) For any B in L(Rp), det(BA) = (det B)(det A).
(f) A is invertible if and only if det A ̸= 0, in which case det(A−1) = (det A)−1.
Proof. (a) If A = I, we have that a( j, j) = 1 and a(i, j) = 0 when i ̸= j. It follows
that the only permutation σ such that a(1, σ (1)) · · · a(p, σ (p)) ̸= 0 is σ = (1), in
which case this product is 1. Since the sign of (1) is 1, this proves the result.
(b) Here we will use Lemma 6.4.19 that gives the second formula for det A.
We also start by assuming that σ is a transposition. Notice from the deﬁnition
of s( j1, . . . , jp) that if two of these integers are interchanged, then s( j1, . . . , jp)
changes sign. For example s( j1, . . . , jp) = −s( j2, j1, j3, . . . , jp). Since interchang-
ing two columns has precisely the eﬀect of interchanging two of these integers, this

182
Differentiation in Higher Dimensions
proves (b) when σ is a transposition. The general form of (b) follows by induction
since every permutation is the product of transpositions
(c) If two columns in A are equal, interchanging those columns does not eﬀect A.
So (c) is a corollary of (b).
(d) Again use the formula for det A in (6.4.19). Consider each of the summands
s( j1, . . . , jp)a(1, ji) · · · a(p, jp) in Deﬁnition 6.4.2. Since the sum in this formula
is over all the p! distinct p-tuples and all but the r-th column are held ﬁxed, what is
left is a linear function of that column.
(e) The proof of this part is more involved than the preceding parts. Fix B and
deﬁne  : L(Rp) →R by (A) = det(BA). If x1, . . . , xp are the columns of A, then
the columns of BA are Bx1, . . . , Bxp. Thus
(A) = (x1, . . . , xp) = det(Bx1, . . . , Bxp)
Note that  also enjoys properties (b), (c), and (d). (Verify!) Hence considering only
the ﬁrst column of A, property (d) implies that
(A) = 
= p

i=1
a(i, 1)e1, x2, . . . , xp
>
=
p

i=1
a(i, 1)(e1, x2, . . . , xp)
Repeating this argument for all the succeeding columns we get that
6.4.21
(A) =

a(i1, 1) · · · a(ip, p)(ei1, . . . , eip)
where the sum is over all p-tuples of integers (i1, . . . , ip) between 1 and p.
Using properties (b) and (c) for , we get that for any p-tuple (i1, . . . , ip),
(ei1, . . . , eip) = t(i1, . . . , ip)(e1, . . . , ep), where the number t(i1, . . . , ip) equals
0 or ±1. (For the moment do not worry about the relation between the numbers
t(i1, . . . , ip) and s(i1, . . . , ip), where this last number was used in the deﬁnition of
the determinant. As we will see, it all comes out in the end.) Using the fact that
(I) = det B, using the preceding equalities, and substituting them into (6.4.21)
shows that
det(BA) = (A) =
?
a(i1, 1) · · · a(ip, p)t(i1, . . . , ip)
@
det B
Observe that when B = I, the preceding equation becomes det A = (A) =
 a(i1, 1) · · · a(ip, p)t(i1, . . . , ip), and we see that the preceding displayed equa-
tion becomes what we want to prove.
(f) If A is invertible, then by (e) we have that (det A)(det(A−1)) = det(AA−1) =
det I = 1, so det A ̸= 0 and det(A−1) = (det A)−1. Conversely assume that A is not
invertible. This implies there is at least one column of A that is a linear combination
of the others. (Why?) For notational convenience assume the dependent column is
the ﬁrst. Hence there are real numbers c2, . . . , cp such that x1 = p
j=2 cjxj. Observe
that
det(x1 −c2x2, x2, . . . , xp) = det A −c2 det(x2, x2, . . . , xp)
(by (d))
= det A
(by (c))

6.4 Linear Transformations
183
Repeating this argument for successive columns we get that
det A = det
⎛
⎝x1 −
p

j=2
cjxj, x2, . . . , xp
⎞
⎠= det(0, x2, . . . , xp) = 0
■
The following corollary follows by invoking Exercise 12.
6.4.22. Corollary. If A ∈L(Rp), the following hold.
(a) If B is the matrix obtained by applying the permutation σ in Sp to the rows of
A, then det B = sign(σ ) det A.
(b) If two rows in A are equal, then det A = 0.
(c) For 1 ≤j ≤p, if the rows {yk : k ̸= j} are held ﬁxed and Ay j is the matrix with
rows y1, . . . , yp, then the function yj →det(Ay j) is a linear functional from Rp
into R.
Also see Exercises 10, 12, and 13.
We will use the above material on permutations and determinants now as well as
in §9.2 and elsewhere in Chapter 9.
(Note that here, as well as in many other places in the literature, for a scalar λ,
A −λ is used for the linear transformation A −λI. In other words, when we write
λ we are talking about both the scalar λ and the linear transformation λI.)
6.4.23. Proposition. If A ∈L(Rp), then the mapping of R into itself deﬁned by
λ →det(A −λ) is a polynomial in λ of degree p.
Proof. Let x1, . . . , xp be the columns of A. Repeatedly using part (d) of the preced-
ing theorem as well as part (a), we have
det(A −λ) = det(x1 −λe1, . . . , xp −λep)
= det(x1, x2 −λe2, . . . , xp −λep) −λ det(e1, x2 −λe2, . . . , xp)
= . . .
= det A + λc1λ + · · · + cp−1λp−1 + (−1)pλp
for some choice of constants c1, . . . , cp. This completes the proof since the coeﬃ-
cient of λp is not zero.
■
The polynomial det(A −λ) = det(A −λI) is called the characteristic poly-
nomial of A. Recall that if A ∈L(Rp), an eigenvalue of A is a scalar λ such that
there is a non-zero vector x called an eigenvector with Ax = λx. The eigenvector x
is said to correspond to λ. So the eigenvectors are precisely the vectors that belong to
ker(A −λ). The subspace ker(A −λ) is called the eigenspace of A corresponding
to λ. Thus λ is an eigenvalue for A if and only if ker(A −λ) ̸= (0); equivalently,
if and only if A −λ is not invertible; equivalently, if and only if det(A −λ) = 0.
So the eigenvalues of A are precisely the zeros of the characteristic polynomial.
The multiplicity of the eigenvalue λ is the dimension of its eigenspace. (Be aware

184
Differentiation in Higher Dimensions
that some books give a diﬀerent deﬁnition of multiplicity.) The reader will often
encounter in this section and beyond a phrase like the following: let λ1, . . . , λn be
the eigenvalues of A repeated as often as their multiplicity. This means that if λ is
an eigenvalue of A of multiplicity m, then λ appears in the sequence λ1, . . . , λn
precisely m times. Also realize that since det(A −λ) is a polynomial of degree p
and eigenvalues happen only when this polynomial has a zero, A can have at most
p eigenvalues counting multiplicity. It is possible that a linear transformation has
signiﬁcantly fewer eigenvalues as the next example shows.
6.4.24. Example. (a) Let p = 2 and let
A =
0
−1
1
0

So det(A −λ) = λ2 + 1, which has no zeros. Thus A has no eigenvalues.
(b) If A = diag (λ1, . . . , λp) then each λj is an eigenvalue. The multiplicity of λj
is the number of times it occurs in the ﬁnite sequence λ1, . . . , λp.
6.4.25. Definition. The set of all eigenvalues of A, not counting multiplicity, is
called the spectrum of A and is denoted by σ (A).
From Example 6.4.24(a) we see that it is possible for σ (A) to be empty. If A is
λ times the identity, σ (A) is the singleton {λ}. When A is hermitian, σ (A) ̸= ∅as
we will see shortly.
6.4.26. Proposition. If A is a hermitian linear transformation, then
∥A∥= sup{|⟨Ax, x⟩| : ∥x∥= 1}
Proof. Let M denote the supremum in the statement. Since |⟨Ax, x⟩| ≤∥Ax∥∥x|| ≤
∥A∥∥x∥2, we have M ≤∥A∥.
If ∥x∥= ∥y∥= 1, using the fact that A∗= A we have the following two equations.
(Here when ± appears more than once in an equation it is always a + in that equation
or always a −.)
⟨A(x ± y), x ± y⟩= ⟨Ax, x⟩± ⟨Ax, y⟩± ⟨Ay, x⟩+ ⟨Ay, y⟩
= ⟨Ax, x⟩± ⟨Ax, y⟩± ⟨y, A∗x⟩+ ⟨Ay, y⟩
= ⟨Ax, x⟩± ⟨Ax, y⟩± ⟨A∗x, y⟩+ ⟨Ay, y⟩
Since A = A∗, when we subtract one of these equations from the other and do
some simplifying, we get ⟨A(x + y), x + y⟩−⟨A(x −y), x −y⟩= 4⟨Ax, y⟩. Now
|⟨Az, z⟩| ≤M∥z∥2 for all z in Rp. When ∥x∥= ∥y∥= 1, the parallelogram law
(Exercise 6.3.2) shows that this last equation yields
4⟨Ax, y⟩≤M(∥x + y∥2 + ∥x −y∥2)
= 2M(∥x∥2 + ∥y∥2)
= 4M

6.4 Linear Transformations
185
Since ⟨Ax, y⟩= ±|⟨Ax, y⟩|, substituting −x for x in the above inequality if necessary
gives |⟨Ax, y⟩| ≤M whenever ∥x∥= ∥y∥= 1. If we take the supremum over all y
with ∥y∥= 1, we get ∥Ax∥≤M; now take the supremum over all x with ∥x∥= 1
and the proof is complete.
■
6.4.27. Corollary. If A is hermitian and ⟨Ax, x⟩= 0 for all x, then A = 0.
The preceding proposition and corollary are decidedly false if the operator A is
not hermitian. For example consider the linear transformation deﬁned in Exam-
ple 6.4.24(a). It is easy to check that in that example ⟨Ax, x⟩= 0 for every x.
6.4.28. Proposition. If A is hermitian, then either ∥A∥or −∥A∥is an eigenvalue
for A.
Proof. According to Proposition 6.4.26, ∥A∥= sup{|⟨Ax, x⟩| : ∥x∥= 1}. But x →
|⟨Ax, x⟩| is a continuous function from Rp →R (Exercise 15) and {x ∈Rp : ∥x∥=
1} is a compact set by the Heine–Borel Theorem. Therefore the supremum is
attained and there is a vector x0 with ∥x0∥= 1 such that |⟨Ax0, x0⟩| = ∥A∥. Let λ =
⟨Ax0, x0⟩. We will show that λ is an eigenvalue with eigenvector x0, completing the
proof. Indeed, ∥(A −λ)x0∥2 = ⟨Ax0 −λx0, Ax0 −λx0⟩= ∥Ax0∥2 −2λ⟨Ax0, x0⟩+
λ2∥x0∥2 = λ2 −2λλ + λ2 = 0. Therefore (A −λ)x0 = 0.
■
Now for a very important result in mathematics. Before tackling the proof, the
reader should solve Exercise 18.
6.4.29. Theorem (The Spectral Theorem). Assume A is a hermitian linear trans-
formation on Rp. If λ1, . . . , λm are the distinct eigenvalues of A and, for 1 ≤j ≤m,
Pj is the orthogonal projection of Rp onto ker(A −λj), then ker(A −λj) ⊥ker(A −
λi) for j ̸= i and
6.4.30
A =
m

j=1
λ jPj.
Proof. We can assume that A ̸= 0. According to Proposition 6.4.28, A has an eigen-
value λ1. Put M1 = ker(A −λ1) and let P1 be the orthogonal projection of Rp onto
M1. By Exercise 18 we can consider the restriction A1 = A|M⊥
1 , a linear transfor-
mation of M⊥
1 into itself. It is left to the reader to verify that A1 is hermitian. (Here
is where we encounter the small wrinkle mentioned in the Warning given earlier
in this section. We have only discussed hermitian linear transformations on Rp, and
M⊥
1 is a subspace of Rp.) Therefore A1 has an eigenvalue λ2 by (6.4.28); clearly λ2
is also an eigenvalue for A. It is transparent that every eigenvector for A1 is also an
eigenvector for A. Thus we note that λ2 must be diﬀerent from λ1 since all the eigen-
vectors for A corresponding to λ1 were disposed of in M1. Put M2 = ker(A −λ2)
and let P2 be the orthogonal projection of Rp onto M2. Note that λ1 ̸= 0. (Why?)
So if x1 ∈M1 and x2 ∈M2,
⟨x1, x2⟩= λ−1
1 ⟨Ax1, x2⟩= λ−1
1 ⟨x1, Ax2⟩= λ2λ−1
1 ⟨x1, x2⟩

186
Differentiation in Higher Dimensions
That is, (1 −λ2λ−1
1 )⟨x1, x2⟩= 0. Since (1 −λ2λ−1
1 ) ̸= 0 and x1 and x2 were arbi-
trarily chosen, it must be that M1 ⊥M2.
Let A3 = A|(M1 ⊕M2)⊥and continue the above process. Since Rp is ﬁnite
dimensional, this process must stop after a ﬁnite number of steps and we obtain
distinct eigenvalues λ1, . . . , λm of A with Pj the orthogonal projection of Rp onto
ker(A −λ j), 1 ≤j ≤m. Just as we showed that M1 ⊥M2, we can show any pair
of distinct eigenspaces are orthogonal. (Do it!) Now ker(A −λ1) ⊕· · · ⊕ker(A −
λm) = Rp or we could continue the process still further. Therefore if x ∈Rp,
x = m
j=1 Pjx and so
Ax = A
⎛
⎝
m

j=1
Pjx
⎞
⎠=
m

j=1
APjx =
m

j=1
λ jPjx =
⎛
⎝
m

j=1
λ jPj
⎞
⎠x
■
When A is hermitian the expression in (6.4.30) is called the spectral decomposi-
tion of A.
6.4.31. Example. Let α1, . . . , αp ∈R and let A be the linear transformation on Rp
deﬁned by the diagonal matrix with these entries α1, . . . , αp on the main diagonal.
So if e1, . . . , ep is the standard basis for Rp, Aej = αjej for 1 ≤j ≤p. To ﬁnd
the spectral decomposition of A, let λ1, . . . , λm be the distinct eigenvalues of A. So
each λk is at least one of the numbers αj, but it may appear several times in the
list α1, . . . , αp. If for 1 ≤j ≤m, Kj = {k : αk = λ j}, let M j be the linear span of
{ek : k ∈Kj}. If Pj is the orthogonal projection of Rp onto M j, then the spectral
decomposition of A is A = m
j=1 λ jPj.
There are other ways in which The Spectral Theorem is sometimes stated. Here
is one, which partially furnishes a converse of the preceding example.
6.4.32. Corollary. If A is a hermitian linear transformation on Rp, then there is
an orthonormal basis for Rp consisting of eigenvectors for A.
Proof. Using the notation from The Spectral Theorem, let M j = PjRp = ker(A −
λ j). For 1 ≤j ≤m pick an orthonormal basis B j for M j. The union of these m
bases, m
j=1 B j, is an orthonormal basis for Rp and each vector in this basis is an
eigenvector for A.
■
Exercises
(1)
Let A, B ∈L(Rp, Rq) and let y1, . . . , yp be some basis for Rp. Show that if
Ay j = Byj for 1 ≤j ≤p, then A = B. In other words, a linear transformation in
L(Rp, Rq) is determined by its values on a basis.

6.4 Linear Transformations
187
(2)
In deﬁning the norm of a linear transformation why does the supremum exist?
(3)
Prove Proposition 6.4.2.
(4)
(a) Prove Corollary 6.4.3. That is, show that the following functions are continuous:
(i) the map from L(Rp, Rq) × L(Rp, Rq) →L(Rp, Rq) deﬁned by (A, B) →A +
B; and (ii) the map from R × L(Rp, Rq) →L(Rp, Rq) deﬁned by (α, A) →αA.
(b) Show that when p = q, the map from L(Rp) × L(Rp) →L(Rp) deﬁned by
(A, B) →AB is continuous.
(5)
If A ∈L(Rp, Rq), show that A is a continuous function.
(6)
Find an example of a linear transformation in L(Rp, Rq) such that the inequality in
Proposition 6.4.6 is strict.
(7)
Prove that the function A∗in Proposition 6.4.8 is a linear transformation.
(8)
(a) Prove Proposition 6.4.9. (b) Prove Proposition 6.4.10.
(9)
If Sp is the set of all permutations of {1, . . . , p} and σ, τ ∈Sp, show that with the
deﬁnition of multiplication στ as composition the following hold. (a) στ ∈Sp. (b)
This multiplication is associative. (c) There is a σ −1 in Sp such that σσ −1 = σ −1σ
is the identity permutation. (This shows that Sp is a group under composition.)
(10)
(a) Use Deﬁnition 6.4.18 to show that when A is either a 2 × 2 or a 3 × 3 matrix,
the deﬁnition gives the expected answer for det A. (b) Use Deﬁnition 6.4.18 to show
that det A equals its expansion by minors using the ﬁrst column of A. (c) If your
deﬁnition of a determinant is not that given in (6.4.18), prove that formula from
your deﬁnition.
(11)
In Theorem 6.4.20(d) ﬁnd the unique vector u in Rp such that the linear functional
xj →det(x1, . . . , xp) equals ⟨xj, u⟩,
(12)
Let A be a p × p matrix. (a) Show that det A∗= det A. (b) If two columns of A are
linearly dependent, show that det A = 0. (c) If two rows of A are linearly dependent,
show that det A = 0. (d) Show that any statement about the columns of A relative to
det A can also be made about the rows of A relative to det A since the rows of A are
the columns of A∗as stated in Corollary 6.4.22.
(13)
When we compute the matrix (aij) for a linear transformation A in L(Rp), recall
that we are using the usual basis e1, . . . , ep. Recall from linear algebra that for any
basis B of Rp we can form another matrix of A, which we denote by [A]B. Show
that the determinant of this new matrix is the same as det A as deﬁned in (6.4.18).
(14)
If f (t) is a polynomial and A is a linear transformation on Rp such that f (A) = 0,
show that f (λ) = 0 for every λ in σ (A). What does this say about the relationship
between f (t) and the characteristic polynomial of A?
(15)
Prove that for any A in L(Rp), x →|⟨Ax, x⟩| is a continuous function from Rp →R.
(16)
Show that if E is an idempotent on Rp, then σ (E) = {0, 1}.
(17)
Let M be a linear subspace of Rp, give M its subspace topology, and show that the
orthogonal projection P : Rp →M is an open mapping; that is, P(G) is an open
subset of M whenever G is open in Rp.
(18)
Let A be a hermitian linear transformation on Rp and let λ be an eigenvalue of A.
(a) Show that A(ker(A −λ)) ⊆ker(A −λ) and A([ker(A −λ)]⊥) ⊆[ker(A −λ)]⊥.
(b) If P is the orthogonal projection of Rp onto ker(A −λ), show that AP = PA.

188
Differentiation in Higher Dimensions
6.5. Differentiable Functions, Part 2
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
6.5.1. Definition. If G is an open subset of Rp, x ∈G, and f : G →Rq, then f is
diﬀerentiable at x when there is a linear transformation A : Rp →Rq and a function
F : G →Rq such that limy→x F(y) = 0 and for all y in G
f (y) −f (x) = A(y −x) + ∥y −x∥F(y)
We deﬁne the derivative of f at x to be the linear transformation D f (x) = f ′(x) =
A. If f is diﬀerentiable at every point of G then f is said to be diﬀerentiable on G.
Once again in this section G is always an open subset of Rp.
6.5.2. Proposition.
Suppose f : G →Rq and x ∈G. If there are A1, A2 in
L(Rp, Rq) and functions F1, F2 from G into Rq such that limy→x F1(y) = 0 =
limy→x F2(y) and
f (y) −f (x) = A1(y −x) + ∥y −x∥F1(y) = A2(y −x) + ∥y −
x∥F2(y) whenever y ∈G, then A1 = A2. In other words the derivative of f at x,
if it exists, is unique.
Proof. Just following Theorem 6.2.12 we pointed out that that theorem implies the
uniqueness of the derivative of a function from G into R. The idea here is to reduce
the proof of this proposition to the scalar case.
For any vector d in Rq let ⟨f , d⟩: G →R be deﬁned as
⟨f , d⟩(y) = ⟨f (y), d⟩
for all y in G. (The notation is a bit awkward, but we’ll just have to live with it. The
alternatives all seem awkward.) We note that for k = 1, 2,
⟨f , d⟩(y) −⟨f , d⟩(x) = ⟨Ak(y −x), d⟩+ ∥y −x∥⟨Fk(y), d⟩
By Deﬁnition 6.2.10 ⟨f , d⟩has a derivative at x and that derivative is the linear func-
tional on Rp deﬁned by w →⟨Ak(w), d⟩. Since the derivative of ⟨f , d⟩is unique,
⟨A1(w), d⟩= ⟨A2(w), d⟩for all w in Rp. Since d was arbitrary in Rq, we have that
A1 = A2.
■
6.5.3. Example. (a) If A ∈L(Rp, Rq) and f : Rp →Rq is deﬁned by f (x) = A(x)
for all x in Rp, then f is diﬀerentiable everywhere and D f (x) = A for all x in Rp.
In fact this f satisﬁes the deﬁnition with F(y) = 0 for all y in Rp.
(b) If f : G →R is diﬀerentiable, then D f : G →Rp∗= Rp. It therefore makes
sense to ask whether D f is diﬀerentiable. As we progress we’ll revisit this idea of
the second derivative of a real-valued diﬀerentiable function of several variables.
The proof of the next result is left to the reader as Exercise 1.
6.5.4. Proposition. A function f : G →Rq is diﬀerentiable at x if and only if there
is a linear transformation A in L(Rp, Rq) such that
lim
z→0
∥f (x + z) −f (x) −A(z)∥
∥z∥
= 0

6.5 Differentiable Functions, Part 2
189
We set some notation that will be frequently used. Suppose f : G →Rq and
d1, . . . , dq is the standard basis for Rq. Recalling the notation introduced in the
proof of Proposition 6.5.2, for 1 ≤i ≤q deﬁne fi : G →R by
6.5.5
fi(x) = ⟨f , di⟩(x) = ⟨f (x), di⟩
We note that if fi(x) = 0 for 1 ≤i ≤q, then f (x) = 0 in Rq. Hence the functions
f1, . . . , fq completely determine f . If f is diﬀerentiable at x, then each fi : G →R
is diﬀerentiable at x, with its derivative a linear functional from Rp into R such that
for each z in Rp,
6.5.6
⟨f ′
i (x), z⟩= ⟨f ′(x)z, di⟩
(Verify!)
6.5.7. Theorem. If G is an open subset of Rp, x ∈Rp, and f : G →Rq is diﬀer-
entiable at x, then the following hold.
(a) f is continuous at x.
(b) For 1 ≤i ≤q and 1 ≤j ≤p the j-th partial derivative of fi exists at x.
(c) The matrix representing the linear transformation D f (x) is
6.5.8
 ∂fi
∂xj
(x)

=
⎡
⎢⎢⎢⎢⎢⎣
∂f1
∂x1 (x)
∂f1
∂x2 (x)
· · ·
∂f1
∂xp (x)
∂f2
∂x1 (x)
∂f2
∂x2 (x)
· · ·
∂f2
∂xp (x)
...
...
· · ·
...
∂fq
∂x1 (x)
∂fq
∂x2 (x)
· · ·
∂fq
∂xp (x)
⎤
⎥⎥⎥⎥⎥⎦
Proof. The proof of part (a) follows as the analogous part of Theorem 6.2.12 did.
The reader is required to show the details in Exercise 2. For (b) we leave it to the
reader to show that for 1 ≤i ≤q, fi : G →R is diﬀerentiable at x by using Deﬁni-
tion 6.2.10 and the fact that f is diﬀerentiable. Thus the partial derivative of fi with
respect to xj exists by (6.2.12(b)). For (c) we observe that the ij-entry of the matrix
representation of D f (x) is, by Theorem 6.2.12,
⟨D f (x)ej, di⟩= ( fi)′(x)(ej) = ∂fi
∂xj
(x) = ∂j fi(x)
■
The next result is the extension of Theorem 6.2.13 to the present situation. It can
be proved by using that theorem. The details are left to the reader in Exercise 6.
6.5.9. Theorem. If f : G →Rq such that for 1 ≤i ≤q and 1 ≤j ≤p the j-th
partial derivative of fi exists and is continuous, then f is diﬀerentiable.
6.5.10. Proposition. If G is a connected set and f : G →Rq is diﬀerentiable with
D f (x) = 0 for all x in G, then f is constant.

190
Differentiation in Higher Dimensions
Proof. For 1 ≤i ≤q, examine the function fi : G →R deﬁned in (6.5.5). As in
(6.5.6) we have that the linear functional f ′
i (x) on Rp is given by z →⟨f ′
i (x), z⟩=
⟨f ′(x)z, di⟩= 0 by hypothesis. By Proposition 6.2.18, each fi is constant. Thus
f (x) = ( f1(x), . . . , fq(x)) is constant in Rq.
■
6.5.11. Definition. If f : G →Rq is diﬀerentiable, then f is continuously diﬀer-
entiable provided D f : G →L(Rp, Rq) is continuous.
The metric on L(Rp, Rq) is the one deﬁned in Proposition 6.4.2, so in the above
deﬁnition the function D f is a mapping between two metric spaces. It is in this
sense that the continuity of D f is deﬁned. Also from Theorem 6.5.7(c), if f is con-
tinuously diﬀerentiable, then all the partial derivatives ∂fi/∂xj are also continuous.
(Why?)
The next result is undoubtedly expected. The proof is left to the reader in
Exercise 7.
6.5.12. Proposition. If f : G →Rq and g : G →Rq are diﬀerentiable, then f +
g : G →Rq is diﬀerentiable and D( f + g) = D f + Dg. If α ∈R, then α f is dif-
ferentiable and D(α f ) = αD f .
6.5.13. Theorem (Chain Rule). If f : G →Rq is diﬀerentiable, H is an open sub-
set of Rq that contains f (G), and g : H →Rd is diﬀerentiable, then g ◦f : G →
Rd is diﬀerentiable and
D( f ◦g)(x) = Dg( f (x)) ◦D f (x) = Dg( f (x))[D f (x)]
or, equivalently,
(g ◦f )′(x) = g′( f (x)) ◦f ′(x) = g′( f (x))[ f ′(x)]
for all x in G.
Proof. The proof parallels the proof of Theorem 2.2.10. Fix x in G, let ξ = f (x) in
H, and set A = f ′(x) and B = g′(ξ) = g′( f (x)). From the deﬁnition of diﬀerentia-
bility we know that
f (y) −f (x) = A(y −x) + ∥y −x∥F(y)
g(η) −g(ξ) = B(η −ξ) + ∥η −ξ∥K(η)
where F : G →Rq, K : H →Rd, and limy→x F(y) = 0 = limη→ξ K(η). We want
to show that
(g ◦f )(y) −(g ◦f )(x) = (B ◦A)(y −x) + ∥y −x∥(y)
= B[A(y −x)] + ∥y −x∥(y)
where limy→x (y) = 0.

6.5 Differentiable Functions, Part 2
191
Now
(g ◦f )(y) −(g ◦f )(x) = B( f (y) −f (x)) + ∥f (y) −f (x)∥K( f (y))
= B[A(y −x)
+ ∥y −x∥F(y)] + ∥f (y) −f (x)∥K( f (y))
= B[A(y −x)] + 1(y)
where 1(y) is the vector in Rd deﬁned by
1(y) = ∥y −x∥B[F(y)] + ∥f (y) −f (x)∥K( f (y))
= ∥y −x∥B[F(y)] +
22A(y −x) + ∥y −x∥F(y)
22K( f (y))
Note that
∥1(y)∥≤∥y −x∥∥B[F(y)]∥+ [∥A∥∥y −x∥+ ∥y −x∥∥F(y)∥]∥K(y)∥
= ∥y −x∥[∥B[F(y)]∥+ ∥A∥∥K(y)∥+ ∥F(y)∥∥K(y)∥]
So if we set (y) = ∥y −x∥−11(y) we have that limy→x (y) = 0 and
(g ◦f )(y) −(g ◦f )(x) = B[A(y −x)] + ∥y −x∥(y)
proving the theorem.
■
As we said, when f : G →Rq is diﬀerentiable, D f : G →L(Rp, Rq). We have
discussed the continuity of the derivative, but can we take a second derivative?
Therein lies the road to complication.
We can make sense of this second derivative, but the eﬀort doesn’t seem worth-
while. For example we could identify L(Rp, Rq) with Rpq by identifying each
linear transformation in L(Rp, Rq) with its q × p matrix and then considering
D f : G →Rpq. In this setup D2F(x) = f ′′(x) ∈L(Rp, Rpq). This is beyond the
scope of this course.
Exercises
(1)
Prove Proposition 6.5.4.
(2)
(a) Prove Theorem 6.5.7(a). (b) Fill in the missing details of the proof of
Theorem 6.5.7(b).
(3)
If f : R3 →R3 is given by f (x, y, z) = (x2 + sin z, ey, xy + cos z), compute D f .
(4)
If f : R3 →R2 is given by f (x, y, z) = (xyexz, x sin y cos z), compute D f .
(5)
If f : R2 →R3 is given by f (x, y) = (xey, y cos xy, x2 sin y), compute D f .
(6)
Prove Theorem 6.5.9.
(7)
Prove Proposition 6.5.12.

192
Differentiation in Higher Dimensions
(8)
Let f : R2 →R2 and g : R2 →R3 be deﬁned by f (x, y) = (x sin y, xy2) and
g(x, y) = (x2, y, xy2) and compute D(g ◦f ) in two diﬀerent ways: ﬁrst by using the
Chain Rule and then by computing g ◦f (x, y) and performing the calculation of the
derivative.
6.6. Critical Points
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Here is a theorem found in many Calculus books.
6.6.1. Theorem. Suppose G ⊆R2 and f : G →R is twice continuously diﬀer-
entiable. If a ∈G, ∇f (a) = 0, ∂11 f (a) > 0, and ∂11 f (a)∂22 f (a) −∂12 f (a)2 > 0,
then f has a relative minimum at a.
When I ﬁrst saw this as a student, I was befuddled
First there was the sense of disappointment. The test for a relative minimum of a
function of one variable was so plain and simple, while this test was even diﬃcult
to remember. Second, the generalization of this to functions deﬁned on G when G
is an open subset of Rp and p > 2 seems formidable if not impossible. If we are to
succeed we need a diﬀerent approach. In this section we’ll see this and, I think, see
what is really going on in the above theorem.
We continue to assume that G is always an open subset of Rp. In §6.2 we deﬁned a
diﬀerentiable function f : G →R to have a critical point at x0 if f ′(x0) = 0. (Recall
that the derivative of such a function, f ′(x), is a linear functional on Rp while ∇f (x)
is the vector in Rp that implements this linear functional via the inner product:
f ′(x)(w) = ⟨∇f (x), w⟩for all w in Rp.) In this section we want to use what we
have developed about linear transformations to analyze the behavior of f at such a
point. What we do here is analogous to what we did for a diﬀerentiable function of
one variable in Theorem 2.4.5.
Let’s underline that we are only discussing functions that are real-valued. We
could just as easily deﬁne a diﬀerentiable function f : G →Rq to have a critical
point at x0 when D f (x0) = 0. However the diagnosis of the behavior of f near a
critical point for such a function is not as complete and satisfying as what we can
do for a real-valued function.
Making no distinction between f ′(x) and the vector ∇f (x) in Rp for the moment,
we have that D f : G →Rp. Thus D2 f (x) ∈L(Rp). What we will show is that when
f ′(x0) = ∇f (x0) = 0, then f has a local minimum at x0 when D2 f (x0) > 0. What
does this condition D2 f (x0) > 0 mean? This is discussed below, but we start by
examining Theorem 6.5.7(c) and applying this to D f (x).
6.6.2. Proposition.
If G is an open subset of Rp and f : G →R is a twice
diﬀerentiable function, then the matrix representing the linear transformation

6.6 Critical Points
193
D2 f (x) is
 ∂2 f
∂xi∂xj
(x)

=
⎡
⎢⎢⎢⎢⎢⎢⎣
∂2 f
∂2x1 (x)
∂2 f
∂x2∂x1 (x) · · ·
∂2 f
∂xp∂x1 (x)
∂2 f
∂x1∂x2 (x)
∂2 f
∂2x2 (x) · · ·
∂2 f
∂xp∂x2 (x)
...
...
· · ·
...
∂2 f
∂x1∂xp (x)
∂2 f
∂x2∂xp (x) · · ·
∂2 f
∂2xp (x)
⎤
⎥⎥⎥⎥⎥⎥⎦
Observe that by Proposition 6.2.3, if we assume the second partial derivatives of
f : G →R are continuous, the matrix above that represents D2 f (x) is hermitian
because all its mixed partial derivatives are equal. This leads us to introduce the
following.
6.6.3. Definition. A hermitian linear transformation is positive (or non-negative)
if all its eigenvalues are positive numbers; it is negative or (non-positive) if the
eigenvalues are all negative numbers. (We will always use the terms “positive” and
“negative”; other terms are sometimes seen in the literature.) The hermitian linear
transformation is positive deﬁnite if it is positive and invertible. A negative deﬁnite
linear transformation is deﬁned similarly.
Note that in light of the Spectral Theorem (6.4.29) a hermitian linear transforma-
tion is positive deﬁnite if all its eigenvalues are strictly positive.
6.6.4. Theorem. Let G be an open subset of Rp and let f : G →R be a twice
continuously diﬀerentiable function. If a ∈G such that ∇f (a) = 0 and A is the
hermitian matrix
A =
 ∂2 f
∂xi∂xj
(a)

= [∂ij f (a)]
the following hold.
(a) If A is positive deﬁnite, then a is a local minimum for f .
(b) If A is negative deﬁnite, then a is a local maximum for f .
(c) If A is invertible and has some eigenvalues that are positive as well as some
that are negative, then a is a saddle point.
(d) If A is not invertible, the nature of this critical point is undetermined.
What happens when p = 1? In this case f ′(a) and A = f ′′(a) are real numbers. So
the statement that A is positive deﬁnite is just the condition that f ′′(a) > 0. There-
fore when p = 1, (a) and (b) are just the conditions we saw in Theorem 2.4.5. Also
in this case (c) is meaningless. When p = 1 the statement in (d) that A is not invert-
ible is the statement that f ′′(a) = 0; so again this is what we have seen before for
critical points of functions of a single variable.
The proof of the general theorem requires a few lemmas.
6.6.5. Lemma. If A is a hermitian linear transformation on Rp that is positive
deﬁnite, then there is a constant c > 0 such that ⟨Ax, x⟩≥c∥x∥2 for all x in Rp.

194
Differentiation in Higher Dimensions
Proof. Adopt the notation of The Spectral Theorem 6.4.29, and let A = m
j=1 λ jPj
be the spectral decomposition of A. Because A is positive deﬁnite, λj > 0 for
1 ≤j ≤m. Let c = min{λj : 1 ≤j ≤m}. For any x in Rp, x = m
j=1 xj with xj
in Pj(Rp). Hence xj ⊥xi for i ̸= j and so
⟨Ax, x⟩=
3
A
⎛
⎝
m

j=1
xj
⎞
⎠,
m

i=1
xi
4
=
3 m

j=1
λ jxj,
m

i=1
xi
4
=
m

j=1
λ j∥xj∥2
≥c
m

j=1
∥xj∥2
= c∥x∥2
■
6.6.6. Lemma. With the hypothesis of Theorem 6.6.4, there is a real number r > 0
and a function η : {h ∈Rp : ∥h∥< r} →[0, ∞) such that:
(a) limh→0 η(h) = 0; and
(b) for ∥h∥< r,
f (a + h) = f (a) + 1
2⟨Ah, h⟩+ η(h)∥h∥2
Proof. Choose r > 0 such that B(a; r) ⊆G. Deﬁne η : B(0; r) →[0, ∞) by
η(0) = 0 and for 0 < ∥h∥< r,
η(h) = f (a + h) −f (a) −1
2⟨Ah, h⟩
∥h∥2
Clearly (b) holds and we must show that (a) holds as well. Fix a δ with 0 < δ < r;
in a moment we will specify δ further. Let ∥h∥< δ and deﬁne φ : (−1, 1) →R
by φ(t) = f (a + th); we apply Taylor’s Theorem (2.5.6) to the function φ. By the
Chain Rule (6.2.14) we have that φ′(t) = ⟨∇f (a + th), h⟩; so φ′(0) = 0. To calcu-
late φ′′(t), apply the Chain Rule (6.5.13) to the composition of ∇f and the function
t →a + th. This shows that
φ′′(t) = d
dt ⟨∇f (a + th), h⟩= (∇f )′(a + th)[h]
But (∇f )′ = D2 f . Putting these values of the derivative into Taylor’s Theorem and
using the fact that ∇f (a) = 0 we get that there is a value of t in [−1, 1] such that
f (a + h) = f (a) + 1
2⟨D2 f (a + th)h, h⟩

6.6 Critical Points
195
Letting y = a + th gives that
 f (a + h) −f (a) −1
2⟨Ah, h⟩
 =

1
2
9
A −D2 f (y)

h, h
:
≤∥A −D2 f (y)∥∥h∥2
So η(h) ≤∥A −D2 f (y)∥. Because the second derivatives of f are continuous, if
ϵ > 0 we can specify that 0 < δ < r is such that ∥a −D2 f (x)∥< ϵ whenever ∥a −
x∥< δ. But y is on the line segment [a −h, a + h] so that it is indeed the case that
∥a −y∥< δ. Thus |η(h)| < ϵ when ∥h∥< δ. This proves part (a).
■
Proof of Theorem 6.6.4. (a) Using the preceding two lemmas we have that there
is an r > 0 such that for ∥h∥< r
f (a + h) −f (a) = 1
2⟨Ah, h⟩+ η(h)∥h∥2
≥
c
2 + η(h)

∥h∥2
Since η(h) →0 as h →0 and c>0, we can ﬁnd a δ <r such that c/2 + η(h) > 0
for ∥h∥< δ. Thus f (x) > f (a) for 0 < ∥x −a∥< δ, showing that a is a local min-
imum for f .
The proof of (b) follows from (a) by consideration of −f . To prove (c) let λ, μ
be two eigenvalues of A with λ > 0 and μ < 0. Let x and y be eigenvectors for λ
and μ, respectively, and assume that ∥x∥= ∥y∥= 1. For 0 < t < δ,
f (a + tx) −f (a) = t2
2 ⟨Ax, x⟩+ η(tx)t2
= t2

1
2λ + η(tx)

Again η(tx) →0 as t →0, so for all suﬃciently small t, f (a + tx) > f (a).
Similarly, for all suﬃciently small s, f (a + sy) < f (a). Hence f has a saddle point
at a.
To establish (d) one need only consider various examples. See Exercise 1.
■
The reader should note that the proof of (c) yields additional information. Using
the notation in that proof we have that along the direction determined by the vector
x – that is, along the line a + tx – the function has a local minimum at a or when
t = 0. On the other hand if we go along the direction determined by the vector y
the function has a local maximum at a. It is precisely by concentrating on these two
directions that we see the saddle nature of the behavior of the function at this critical
point.

196
Differentiation in Higher Dimensions
Now let’s reconcile Theorem 6.6.1 with what we have done.
6.6.7. Lemma. If α, β, γ ∈R and A is the hermitian matrix
A =
α
β
β
γ

then A is positive deﬁnite if and only if α > 0 and det A > 0.
Proof. Assume A is positive deﬁnite. Then 0 < ⟨Ae1, e1⟩= α. Also since det A is
the product of the eigenvalues of A, it must be positive. Now assume that α and det A
are positive. If z = (x, y) ∈R2, then
⟨Az, z⟩= αx2 + 2βxy + γ y2
= α

x2 + 2β
α xy

+ γ y2
= α

x + β
α y
2
+

γ −β2
α

y2
= α

x + β
α y
2
+ 1
α (det A)y2
> 0
■
We can now see that when p = 2, Theorem 6.6.1 is a direct consequence of The-
orem 6.6.4 once the criterion for positive deﬁniteness from the preceding lemma is
applied to the matrix
 f11(a)
f12(a)
f21(a)
f22(a)

Actually Lemma 6.6.7 can be generalized. For any square real matrix
A =
⎡
⎢⎢⎢⎣
α11
α12
· · ·
α1n
α21
α22
· · ·
α2n
...
...
...
...
αn1
αn2
· · ·
αnn
⎤
⎥⎥⎥⎦
deﬁne the principal minors of A to be the square matrices
Ak =
⎡
⎢⎢⎢⎣
α11
α12
· · ·
α1k
α21
α22
· · ·
α2k
...
...
...
...
αk1
αk2
· · ·
αkk
⎤
⎥⎥⎥⎦
where 1 ≤k ≤n.
6.6.8. Theorem.
A hermitian matrix is positive deﬁnite if and only if det Ak is
positive for each of its principal minors.
For a proof see [5], page 328.

6.7 Tangent Planes
197
Exercises
(1)
Let f (x, y) = 1
2x2 and show that f has a local minimum at (0, 0) but D2 f (0, 0) is
not invertible. Give an example of a function g on R2 that has a local maximum at
(0, 0) but D2g(0, 0) is not invertible.
(2)
Find all local extrema of the function f (x, y) = x2 −xy + y3 −y and decide
whether each is a local minimum, maximum, or saddle point. If the function has
a saddle point at (x0, y0), ﬁnd the eigenvectors corresponding to the two eigenval-
ues of D2 f (x0, y0).
(3)
Find all local extrema of the function f (x, y) = sin x + cos y and decide whether
each is a local minimum, maximum, or saddle point.
(4)
Let  = {(x, y, z) ∈R3 : x2 + y2 + z2 < 1} and deﬁne f on  by f (x, y, z) =
sin(πx)2 + cos(πy)2 −z2. Find all the critical points of f in  and give the nature
of each whenever you can.
(5)
Find the critical points of f (x, y, z, w) = x2 + (y + w)2 + exp[y2 −cos(z)] on R4
and, whenever you can, decide whether each is a local minimum, maximum, or
saddle point.
(6)
Find the critical points of f (x, y) = x2 −xy + y3 −y and, whenever you can,
decide whether each is a local minimum, maximum, or saddle point.
(7)
Using the notation of Theorem 6.6.4, assume that A is positive deﬁnite so that f has
a local minimum at a. In which direction is the function f increasing the fastest?
Prove your assertion.
6.7. Tangent Planes
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
As we have mentioned and the student no doubt recalls, when f : (a, b) →R is
diﬀerentiable at c in (a, b), f ′(c) is the slope of the straight line tangent to the graph
of f at the point (c, f (c)). Thus the equation of the line tangent to the graph is
y = f (c) + f ′(c)(x −c). When G is an open subset of Rp, c ∈G, and f : G →R
is diﬀerentiable, what is the geometric signiﬁcance of f ′(c) or ∇f (c)? First note
that the graph of f in this case is the surface S = {(x, f (x)) : x ∈G} ⊆Rp+1.
(We are taking the idea of a surface as an intuitive concept and are not precisely
deﬁning it. This will be properly deﬁned in Chapter 9 below.) It makes no sense
to talk about the tangent line to the surface since there are many such lines.
Therefore we must increase the level, or dimension, of the discussion. For inspir-
ation we return to the case where p = 1 and show that f ′(c) has an additional
property.
6.7.1. Proposition. If f : (a, b) →R is diﬀerentiable and c ∈(a, b), then the vec-
tor ( f ′(c), −1) in R2 is orthogonal to the line tangent to the graph of f at the point
(c, f (c)).

198
Differentiation in Higher Dimensions
Proof. We know that the typical point in the tangent line to the graph at the point
(c, f (c)) is (x, f (c) + f ′(c)(x −c)). Thus in R2
⟨( f ′(c), −1), (x −c, f ′(c)(x −c)⟩= 0
■
The preceding result rephrases something else the student may recall from calcu-
lus. Namely that when f ′(c) ̸= 0, −[ f ′(c)]−1 is the slope of the line perpendicular
to the graph of f at the point (c, f (c)). The statement in the preceding proposition
has an advantage over this last fact since it does not need to exclude the possibility
that f ′(c) = 0.
So we might hope that for f deﬁned on an open subset of Rp the vector
(∇f (c), −1) plays a similar role. In fact we’ll show that it is perpendicular to all
lines tangent to the surface at (c, f (c)). But rather than discuss all the tangent lines
we will incorporate this into another concept.
6.7.2. Definition. A subset H of Rp+1 is called an aﬃne hyperplane if there is a
vector v in H such that H −v = {x −v : x ∈H} is a linear subspace of Rp+1 having
dimension p. If H itself is a linear subspace of Rp+1 having dimension p, then H is
said to be a hyperplane.
For emphasis, a hyperplane has dimension one less than the dimension of the
containing space. Let’s quickly state the following, whose proof is Exercise 1.
6.7.3. Proposition. If H is an aﬃne hyperplane, then for all vectors v and w in H,
H −v = H −w. Consequently, H −v is a linear subspace of Rp+1 having dimen-
sion p for every vector v in H.
6.7.4. Example. (a) Any straight line in R2 is an aﬃne hyperplane.
(b) The aﬃne hyperplanes in R3 are the translates of planes.
(c) If a1, . . . , ap ∈R and not all of them are 0, then H = {x ∈Rp+1 :
p
j=1 xja j = 1} is an aﬃne hyperplane. In fact if v ∈H, H −v = {y ∈Rp+1 :
p
j=1 a jyj + yp+1 = yp+1} = {y ∈Rp+1 : p
j=1 a jyj = 0}. (See Exercise 2.) So if
a = (a1, . . . , ap, 0) ∈Rp+1, then H −v = {y ∈Rp+1 : y ⊥a} and thus has dimen-
sion p.
(d) If H is any aﬃne hyperplane and x ∈Rp+1, then H + x is an aﬃne hyper-
plane. In particular this works when H is a linear subspace of Rp+1 of dimension p.
(e) If x0 is any non-zero vector in Rp+1 and u ∈Rp+1, then {x + u : x ⊥x0} is an
aﬃne hyperplane.
Here is an easy way to manufacture an aﬃne hyperplane. Let L : Rp+1 →R be a
non-zero linear functional. For any real number c, Hc = {x ∈Rp+1 : L(x) = c} is an
aﬃne hyperplane. In fact if v ∈Hc, then Hc −v = H0. It is easy to check that H0 is
a vector subspace of Rp+1. Since L ̸= 0 there is a vector u in Rp+1 with L(u) = 1.
So if x ∈Rp+1 and L(x) = b, x −bu ∈H0; that is, H0 + Ru = Rp+1 so that the
dimension of H0 must be p. Note that this is exactly how we got Example 6.7.4(c).

6.7 Tangent Planes
199
In fact if L(x) = p
j=1 a jxj, H1 is the aﬃne hyperplane in that example. This leads
to the following.
6.7.5. Proposition. If H ⊆Rp+1, the following statements are equivalent.
(a) H is an aﬃne hyperplane in Rp+1.
(b) There is a non-zero linear functional L on Rp+1 and a real number c such that
H = {x ∈Rp+1 : L(x) = c}.
(c) There is a non-zero vector a in Rp+1 and a real number c such that H = {x ∈
Rp+1 : ⟨x, a⟩= c}.
Proof. The equivalence of (b) and (c) is a consequence of the fact that every
linear functional L on Rp+1 has the form L(x) = ⟨x, a⟩for some a in Rp+1 (6.2.8).
The fact that (b) implies (a) is in the discussion that preceded the proposition. It
remains to prove that (a) implies (b). If H is an aﬃne hyperplane, ﬁx a vector v in
H and consider M = H −v. By deﬁnition M is a linear subspace of dimension
p. Therefore if w ∈Rp+1\M, we have that Rp+1 = M + Rw. It follows that
for any x in Rp+1 there is a unique vector m in M and a unique scalar α such
that x = m + αw (Exercise 3). By the uniqueness of these objects it follows that
L(x) = α deﬁnes a linear functional on Rp+1 and M = {x ∈Rp+1 : L(x) = 0}.
(Verify!) If we put c = L(v), then H = {x ∈Rp+1 : L(x) = c}.
■
What about the uniqueness of the vector a in part (c) of the preceding proposition?
Here is the answer.
6.7.6. Proposition. If H is an aﬃne hyperplane, a1, a2 ∈Rp+1, and c1, c2 ∈R
such that {x ∈Rp+1 : ⟨x, a1⟩= c1} = H = {x ∈Rp+1 : ⟨x, a2⟩= c2}, then there is
a t in R such that a2 = ta1.
Proof. Let v ∈H and consider M = H −v. So both a1 and a2 are orthogonal to
M. Since dim M = p, it must be that there is a scalar t with a2 = ta1.
■
So the vector that is orthogonal to an aﬃne hyperplane is not unique, but it’s
close to unique. The frequent practice is to take a unit vector η that’s orthogonal
to H.
Let’s establish some notation. As before assume that G is an open subset of Rp,
f : G →R is a diﬀerentiable function, and c ∈G. Put S = {(x, f (x)) : x ∈G} ⊆
Rp+1; we say that S is the surface in Rp+1 associated with f . Note that when
γ : (−1, 1) →G is a diﬀerentiable curve, γ (t) = (γ (t), f (γ (t)) deﬁnes a curve
γ : (−1, 1) →Rp+1 and this curve lies in the surface S. We leave it to the reader
(Exercise 4) to prove that γ is diﬀerentiable and
6.7.7
γ ′(t) =

γ ′(t),
9
∇f (γ (t)), γ ′(t)
:
∈Rp+1
As usual the vector γ ′(t) is tangent to the curve deﬁned by γ .

200
Differentiation in Higher Dimensions
6.7.8. Proposition. Using the notation above, put γ (0) = c. We have that
(∇f (c), −1) ⊥˜γ ′(0)
Proof. Using (6.7.7) and the fact that γ (0) = c, we have that
γ ′(0) = (γ ′(0), ⟨∇f (c), γ ′(0)⟩)
Hence
⟨γ ′(0), (∇f (c), −1)⟩= ⟨(γ ′(0), ⟨∇f (c), γ ′(0)⟩), (∇f (c), −1)⟩
= ⟨γ ′(0), ∇f (c)⟩−⟨∇f (c), γ ′(0)⟩
= 0
■
6.7.9. Lemma.
With G, f , S as above, if (c, d) ∈S and γ : (−1, 1) →S is a
smooth curve with γ (0) = (c, f (c)), then there is a smooth curve γ : (−1, 1) →G
with γ (0) = c and γ (t) = (γ (t), f (γ (t)) for all t in (−1, 1).
Proof. Consider the projection map of Rp+1 onto Rp and let π : S →G be its
restriction to S; so we know π to be continuous and given by π(x, f (x)) = x. Note
that x →(x, f (x)) is a map of G onto S and π is its inverse. In fact, if S has its rela-
tive topology, this map is a homeomorphism. (Verify!) Deﬁne γ : (−1, 1) →G by
γ (t) = π(γ (t)). Save for verifying that γ is smooth, it is immediate that γ has all
the desired properties. To show smoothness it suﬃces to show that γ ′(t) = π(γ ′(t))
as in (6.7.7). This is left to the reader (Exercise 5).
■
Reﬂect on Proposition 6.7.8. Note that the vector (∇f (c), −1) depends only on
the function f and the point c in G. In light of the preceding lemma, this proposition
says that this vector is orthogonal to every curve lying in S that passes through the
point (c, f (c)). Thus we make the following deﬁnition and say that (∇f (c), −1) is
orthogonal to the surface deﬁned by f at the point (c, f (c)).
6.7.10. Definition. If G be an open subset of Rp, f : G →R is a diﬀerentiable
function, and c ∈G, call
(c, f (c)) + {x ∈Rp+1 : x ⊥(∇f (c), −1)}
the tangent aﬃne hyperplane to the graph of f at the point (c, f (c)).
Exercises
(1)
Prove Proposition 6.7.3.
(2)
Verify the statements made in Example 6.7.4(c). Give a basis for H −v when
v ∈H.

6.8 Inverse Function Theorem
201
(3)
Prove that if M is a subspace of Rp+1 of dimension p and w ∈Rp+1\M, then for
any x in Rp+1 there is a unique vector m in M and a unique scalar α such that
x = m + αw.
(4)
Prove (6.7.7).
(5)
In the proof of Lemma 6.7.9 show that γ is smooth.
(6)
Let f : R2 →R be deﬁned by f (x, y) = 3x2 −xy and ﬁnd the tangent aﬃne hyper-
plane to the graph of f when (x, y) = (1, 2).
(7)
Let f (x, y) = log(3x + y) and ﬁnd the tangent aﬃne hyperplane to the graph of f
when (x, y) = (1, −1).
(8)
Let f (x, y, z) = cos(xy) + sin(yz) and ﬁnd the tangent aﬃne hyperplane to the
graph of f at the point (x, y, z) = (π, −1, π/2).
(9)
Let
G = {(x, y) ∈R2 : x2 + y2 < 1}
and
deﬁne
f : G →R
by
f (x, y) =

1 −x2 −y2. For any point c in G ﬁnd the tangent aﬃne hyperplane to the
graph of f at the point (c, f (c)).
6.8. Inverse Function Theorem
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
We begin this section with a result from metric spaces. While the only use of this
result in this book is the proof of the title result of this section, it has many uses
elsewhere in mathematics. There are proofs of the Inverse Function Theorem that
do not use this ﬁxed point theorem, but its use lends a simplicity to the argument.
6.8.1. Theorem (Banach5 Fixed Point Theorem). Let (X, d) be a complete metric
space. If φ : X →X is a function such that there is a constant c with 0 < c < 1 and
d(φ(x), φ(y)) ≤cd(x, y)
for all x, y in X, then there is a unique point x∗in X with φ(x∗) = x∗.
The point x∗in the theorem is called a ﬁxed point of φ.
Proof. Take any point x0 and put xn+1 = φ(xn) for all n ≥0. Note that for n ≥1,
d(xn+1, xn) = d(φ(xn), φ(xn−1)) ≤cd(xn, xn−1). Using mathematical induction we
obtain that d(xn+1, xn) ≤cnd(x1, x0).
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
5 Stefan Banach was born in 1892 in Krakow, presently in Poland but then part of Austria-Hungary (Polish history
is complicated). In 1922 the university in Lvov (a city that during its history has belonged to at least three
countries and is presently in Ukraine) awarded Banach his habilitation for a thesis on measure theory. His
earlier thesis on Integral Equations is sometimes said to mark the birth of functional analysis. He was one of
the stars in a particularly brilliant constellation of Polish mathematicians during this period. Banach and his
colleague Hugo Steinhaus in Lvov as well as other mathematicians in Warsaw began publishing a series of
mathematical monographs. The ﬁrst to appear in 1931 was Banach’s Théorie des Opérations Linéaires, which
had an enormous impact on analysis and continues to hold its place as a classic still worth reading. Banach was
one of the founders of functional analysis, which had been brewing in academic circles for some time. You will
see Banach’s name appear often if you ever study the subject. He died in Lvov in 1945 just after the end of World
War II and has many results besides this one that bear his name.

202
Differentiation in Higher Dimensions
Claim. {xn} is a Cauchy sequence.
In fact when m > n,
d(xm, xn) ≤d(xm, xm−1) + · · · + d(xn+1, xn)
≤cm−1d(x1, x0) + · · · + cnd(x1, x0)
= cnd(x1, x0)
m−n−1

j=0
c j
≤cnd(x1, x0)
∞

j=0
c j
=
cn
1 −cd(x1, x0)
Provided we take N suﬃciently large this can be made arbitrarily small for m >
n ≥N.
Since (X, d) is complete there is a point x∗such that xn →x∗. Note that since
xn = φ(xn−1), we have that x∗= limn xn = limn φ(xn−1) = φ(x∗); thus x∗is a ﬁxed
point of φ. If y∗is another ﬁxed point of φ, then d(x∗, y∗) = d(φ(x∗), φ(y∗)) ≤
cd(x∗, y∗). Since c < 1 this is impossible unless d(x∗, y∗) = 0. Thus x∗is
unique.
■
There are other proofs of this theorem than the one presented. Finding other
proofs is good, but the proof above is Banach’s original one and it has always struck
me as the “natural” proof. It certainly seems intuitive. Banach’s original proof also
has the advantage in that it tells you how to ﬁnd the ﬁxed point.
It is rather easy to manufacture examples illustrating this result. For example, if
φ : [0, 1] →[0, 1] is deﬁned as the identity function, φ(x) = x for all x, then φ has
every point as a ﬁxed point even though it satisﬁes the inequality but with c = 1. If
instead we let φ(x) = x2, there are two ﬁxed points. Also see Exercise 1.
Recall Proposition 2.3.10 where we saw that when f : (a, b) →(α, β) is a dif-
ferentiable function that is bijective, then the function f −1 : (α, β) →(a, b) is dif-
ferentiable and
( f −1)′(ζ ) =
1
f ′( f −1(ζ ))
for every ζ in (α, β). Here we wish to extend this result to diﬀerentiable functions
f from an open subset of Rp into Rp. Let’s underline the fact that this concerns a
function from a subset of Euclidean space into the space of the same dimension.
Hence the derivative belongs to L(Rp) and we can speak of its inverse.
6.8.2. Theorem (Inverse Function Theorem). Let G be an open subset of Rp and
assume f : G →Rp is a continuously diﬀerentiable function. If a ∈G with b =
f (a) and D f (a) is an invertible linear transformation, then there is an open neigh-
borhood U of a such that  = f (U) is open in Rp and f is a bijection of U onto

6.8 Inverse Function Theorem
203
. If g :  →U is the inverse of f : U →, then g is continuously diﬀerentiable
and for every ζ in 
[Dg](ζ ) = [D f (g(ζ ))]−1
Before starting the proof, let’s be sure you understand that we already have this
result when p = 1. In fact the hypothesis in the case that p = 1 is that f ′(a) ̸= 0.
Thus there is a small open interval (a −δ, a + δ) where f is strictly monotonic.
Hence by the result referred to before the statement of the theorem, f is bijective
and its inverse function is diﬀerentiable.
Proof. Let A denote the invertible linear transformation A = D f (a) and let 0 <
c < 1/[2∥A−1∥]. Since D f : G →L(Rp) is continuous there is an ϵ > 0 such that
∥D f (x) −A∥< c when ∥x −a∥< ϵ. Put U = B(x; ϵ) and  = f (U). We want to
show that f is injective on U and that  is open; then we’ll tackle showing that the
inverse of f on  is continuously diﬀerentiable. Doing these things is not particu-
larly deep, but it is involved.
For any ζ in  deﬁne φζ : U →Rp by
φζ (x) = x + A−1(ζ −f (x))
Note that a point x is a ﬁxed point of φζ if and only if ζ = f (x). So showing that
f is injective on U amounts to showing that φζ has a unique ﬁxed point on U. (By
the way, we will not use Banach’s Fixed Point Theorem here. We will use it later
when we show that  is open.) Since ζ ∈ = f (U), we know there is at least
one ﬁxed point x in U. Now φ′
ζ (x) = 1 −A−1D f (x) = A−1[A −D f (x)]. Hence
when ∥x −a∥< ϵ, ∥φ′
ζ (x)∥≤∥A−1∥∥A −f (x)∥≤c∥A−1∥< 1
2. Using Exercise 2
we have that
6.8.3
∥φζ (x1) −φζ (x2)∥≤1
2∥x1 −x2∥whenever ∥x1 −x2∥< ϵ
So if both x1 and x2 are ﬁxed points of φy in U, it must be that x1 = x2. Thus f :
U → is injective and therefore bijective.
Now to show that  is open. This is done as follows. Fix ζ0 in , and let x0 be
the unique point in U with f (x0) = ζ0. Choose r > 0 such that B(x0; r) ⊆U.
Claim. B(ζ0; cr) ⊆ and so  is open.
Fix ζ in B(ζ0; cr) and deﬁne the function φζ as above. Thus ∥φζ (x0) −x0∥=
∥A−1(ζ −ζ0)∥< ∥A−1∥cr < 1
2r. If x ∈B(x0; r), then (6.8.3) implies
∥φζ (x) −x0∥≤∥φζ (x) −φζ (x0)∥+ ∥φζ (x0) −x0∥
≤1
2∥x −x0∥+ r
2
≤r

204
Differentiation in Higher Dimensions
In other words, φζ maps B(x0; r) into itself. Because B(x0; r) is compact and thus
complete, we can apply Banach’s Fixed Point Theorem. Therefore φζ has a unique
ﬁxed point x and this means f (x) = ζ, establishing the claim.
Let g :  →U be the inverse of f on U and let’s show that it is continuously dif-
ferentiable. Note that the argument above shows that f : U → is an open map.
That is, if V is an open subset of U, the same reasoning will show that f (V ) is open.
Equivalently, f : U → is a homeomorphism so that g = f −1 :  →U is contin-
uous. This implies we need only show that g is diﬀerentiable. Indeed once this is
done we can apply the Chain Rule (6.5.13) to g( f (x)) = x for x in U and obtain that
[Dg]( f (x)) = [D f (x)]−1 on U. Thus on  we have that Dg(ζ ) = [D f ( f −1(ζ ))]−1
and, by Proposition 6.4.5, g is continuously diﬀerentiable.
To show that g is diﬀerentiable we’ll use the deﬁnition. Fix ζ in  and let
k ∈Rp with ∥k∥suﬃciently small that ζ + k ∈. Let x = f −1(ζ ) = g(ζ ) and
xk = f −1(ζ + k) = g(ζ + k). Put S = D f (x) and T = S−1. If Dg(ζ ) exists, the
Chain Rule tells us that it should be that Dg(ζ ) = T . By deﬁnition to show that
g is diﬀerentiable we want to show the following:
6.8.4
lim
k→0
∥g(ζ + k) −g(ζ ) −T (k)∥
∥k∥
= 0
In what follows it is helpful to note that k = (ζ + k) −ζ = f (xk) −f (x).
With
this
g(ζ + k) −g(ζ ) −T (k) = xk −x −T (k) = −T [k −S(xk −x)] =
−T [ f (xk) −f (x) −S(xk −x)]. Hence
∥g(ζ + k) −g(ζ ) −T (k)∥
∥k∥
= ∥−T [ f (xk) −f (x) −S(xk −x)]∥
∥xk −x∥
≤∥T ∥∥[ f (xk) −f (x) −S(xk −x)]∥
∥xk −x∥
Since S = D f (x), the right-hand side of this inequality converges to 0 as ∥x −
xk∥→0, but this happens exactly when ∥k∥→0. (Why?) This establishes (6.8.4),
showing that g is diﬀerentiable.
■
6.8.5. Corollary. If G is an open subset of Rp and f : G →Rp is continuously
diﬀerentiable such that D f (x) is invertible for every x in G, then whenever U is an
open subset of G, f (U) is an open subset of Rp.
The proof of this corollary is left to the reader in Exercise 3. We note that this
corollary says that the function f is an open mapping. For this reason it is sometimes
called the Open Mapping Theorem. We aren’t going to use this name as it is more
frequently used for another result.
When f : Rp →Rp is diﬀerentiable, the p × p matrix that represents the lin-
ear transformation D f (x) in L(Rp) is given in (6.5.8). To determine if this linear
transformation is invertible, we could take its determinant. This is denoted by
 f (x) = det D f (x)

6.8 Inverse Function Theorem
205
and is called the Jacobian6 of f . We will sometimes refer to this as the Jacobian
determinant. We do this to make a distinction with the p × p matrix (6.5.8) which
is denoted as
J f (x) = ∂( f1, . . . , fp)
∂(x1, . . . , xp) =
 ∂fi
∂xj
(x)

and is also sometimes referred to as the Jacobian or Jacobian matrix. Another deﬁ-
nition of the Jacobian matrix is applied to diﬀerentiable maps f : Rp →Rq where
it is the q × p rectangular matrix (∂f j/∂xi). Here the notation is
J f (x) = ∂( f1, . . . , fq)
∂(x1, . . . , xp)
At this stage the reader might be concerned about the possible confusion, but in
practice the context will make it clear what is being discussed.
6.8.6. Example. (a) Deﬁne f : R2 →R2 by f (x, y) = (ex cos y, ex sin y). Comput-
ing the Jacobian yields
 f (x, y) = det
ex cos y
−ex sin y
ex sin y
ex cos y

= e2x
Thus D f (a, b) is invertible for any (a, b) in the plane so that f is an open mapping
on R2. (See Exercise 4.) Since f (0, 0) = f (0, 2π), f does not have an inverse on
the entire plane.
(b) Many examples are available in R to show the existence of local inverses but
where the function does not have a global inverse. See Exercise 5.
(c) Here is another example in one variable that illustrates that we must have the
continuity of the derivative. Deﬁne f : R →R by
f (x) =
	
x + 2x2 sin x−1
when x ̸= 0
0
when x = 0
The function f is diﬀerentiable everywhere on the real line with f ′(0) = 1. However
f fails to be injective in any neighborhood of 0. The reader is asked to supply the
details in Exercise 6.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
6 Carl Jacobi was born in Potsdam, Germany in 1804. He was tutored at home by his uncle until he was 12 and
entered the gymnasium in Potsdam. After a year he was judged ready for the university, but remained at the
gymnasium for another three years because at the time the University of Berlin would not accept students under
the age of 18. Four years later he had completed his doctoral dissertation and obtained a post at a prestigious
gymnasium in Berlin. He soon became a Christian, permitting him to obtain a position at the university in Berlin
(you read that correctly) and then he transferred to Königsberg. His early work on number theory attracted the
attention of Gauss. He also obtained results on elliptic functions that attracted Legendre. In 1831 he married and
in 1832 was promoted to a full professorship. He conducted fundamental work on partial diﬀerential equations,
but in 1843 he was diagnosed as a diabetic and this caused him severe problems. He went to Italy to recover. The
weather agreed with him and he resumed his publications. It was around this time that he returned to a position
in Berlin starting in 1844. There followed a particularly turbulent period in Prussian history and Jacobi fell out
of favor because of his political beliefs. In January 1851 he caught the ﬂu, which was a more serious event then
than now. In his weakened condition he also contracted smallpox and soon died.

206
Differentiation in Higher Dimensions
Exercises
(1)
(a) Is there a continuous function φ : [0, 1] →[0, 1] with no ﬁxed points? (b) Find
an example showing that the condition on φ in Theorem 6.8.1 is not necessary for
there to be a unique ﬁxed point. (c) Find an example of an incomplete metric space
(X, d) and a continuous function φ of (X, d) into itself such that d(φ(x), φ(y)) ≤
cd(x, y) for some constant c with 0 < c < 1 and all x, y in X but where φ does not
have a ﬁxed point.
(2)
If ϵ > 0, c ≥0, B(x; ϵ) ⊆Rp, and φ : B(x; ϵ) →Rq is a continuously diﬀeren-
tiable function with ∥Dφ(x)∥≤c for all x in B(x; ϵ), show that ∥φ(x1) −φ(x2)∥≤
c∥x1 −x2∥whenever ∥x1 −x2∥< ϵ. (Hint: Use Proposition 3.1.10(b).)
(3)
Give the details of the proof of Corollary 6.8.5.
(4)
In Example 6.8.6(a) note that f (0, 0) = (1, 0) and ﬁnd the inverse of f deﬁned in
a neighborhood of (1, 0).
(5)
Give an example of a continuously diﬀerentiable function f on R such that f ′(x) ̸=
0 for every x but such that f does not have a global inverse.
(6)
Give the details in Example 6.8.6(c).
(7)
Deﬁne f : R2 →R2 by f (x, y) = (x2 + 2xy + y2, 2x + 2y). Determine all the
points (a, b) where there is a function g deﬁned in a neighborhood of f (a, b) with
f (g(u, v)) = (u, v) for all (u, v) in this neighborhood. Justify your answer.
(8)
Deﬁne f : R3 →R3 by f (x, y, z) = (x + xyz, y + xz, z + 2x + 3z2). (a) Compute
the Jacobian matrix (not determinant) of f . (b) Does the Inverse Function Theo-
rem imply there is a function g deﬁned in a neighborhood of f (0, 0, 0) such that
f ((g(u, v, w)) = (u, v, w) in this neighborhood? (c) Does the Inverse Function
Theorem imply there a function g deﬁned in a neighborhood of f (1, 1, 1)) such
that f ((g(u, v, w)) = (u, v, w) in this neighborhood?
(9)
Deﬁne f : R2 →R2 by
f (x, y) =

x2 −y2
x2 + y2 ,
xy
x2 + y2

(a) Compute the Jacobian matrix (not determinant) of f . (b) Does the Inverse Func-
tion Theorem imply there a function g deﬁned in a neighborhood of f (0, 1) such
that f ((g(u, v)) = (u, v) in this neighborhood?
(10)
Recall the Chain Rule (6.5.13) when p = q = d: if G and H are open subsets of Rp,
f : G →Rp is diﬀerentiable, f (G) ⊆H, and g : H →Rp is diﬀerentiable, then
g ◦f : G →Rp is diﬀerentiable and
D( f ◦g)(x) = Dg( f (x))[D f (x)]
Express the Jacobian determinant  f ◦g in terms of  f and g.
(11)
Deﬁne f : R2 →R2 by f (x, y) = (xey, xy) and show that there is a ball B of pos-
itive radius and centered at (1, 0) and a continuous function g : B →R2 such that
g(1, 0) = (1, 0) and f (g(x, y)) = (x, y) for all (x, y) in B. Justify your answer.

6.9 Implicit Function Theorem
207
(12)
Deﬁne f : R2 →R2 by f (x, y) = (x2 + y, xy). (a) Does the Inverse Function The-
orem apply to f at the point (1, 1)? If it does not, state why. If it does, state what it
implies. (b) Does the Inverse Function Theorem apply to f at the point (1, 2)? If it
does not, state why. If it does, state what it implies.
6.9. Implicit Function Theorem
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
I can remember when I was a student and ﬁrst encountered the Implicit Function
Theorem. It perplexed me. There is a certain aspect of the theorem that is mathe-
matically pleasing, but I think at least part of the reason for my confusion was that
the proof is complicated and I couldn’t understand why we were doing all this work.
What was the use of this diﬃcult theorem? Let’s see if you have less consternation
and irritation than I did on my ﬁrst encounter. I hope so.
As far as the purpose of the result is concerned, you can Google “implicit function
theorem economics” and discover a great interest in this theorem by economists.
There are also the uses of this theorem in analysis and especially geometry, some
of which you will see later in this book; but I am afraid that to see most of this you
must wait until a future course. That’s true of much of what you study at this point in
your development just as it was for me at the corresponding stage. We are exploring
basic properties of functions and these underly most of what mathematics is about.
This fundamental understanding has permanent value, but a lot of that value only
becomes clear later.
The diﬃculty in understanding the result cannot be removed by a magic wand,
but there are some helpful guide posts. You can regard the Implicit Function Theo-
rem as an extension of the following. If we are given the equation x2 + y2 = 1, this
expresses an interdependence of the real variables x and y. Can I solve for one in
terms of the other? Equivalently, can I express one, say y, as a function of x? The
answer is clearly no as the graph of this relation in the plane is a circle and that’s
not the graph of a function of x. On the other hand we can write y =
√
1 −x2 and
this works for some values of x.
Let’s recast the above problem by writing f (x, y) = x2 + y2 −1. If we set
f (x, y) = 0 we say that this equation “deﬁnes” y implicitly in terms of x (and vice
versa, x in terms of y). The word “deﬁnes” here is set in quotation marks because
for the values of x the corresponding value of y is not unique. So it is impossible
to solve for y in terms of x in a global way, if by solving we mean expressing y
as a function of x. But as we mentioned y =
√
1 −x2 works sometimes; that is, it
works locally near some values of x. The key here is to consider the partial deriva-
tive ∂f (x, y)/∂y = 2y. When this partial derivative is zero, we cannot get a local
solution. Indeed, the partial derivative being zero corresponds to the points (±1, 0)
on the graph of f (x, y) = 0 and an examination of the graph near these points shows
that it cannot be the graph of a function y = h(x). As long as we avoid places where
this partial derivative is 0 we can indeed ﬁnd a function h such that where h is

208
Differentiation in Higher Dimensions
deﬁned we have that f (x, h(x)) = 0. This condition involving the partial derivative
is the key to the general case. Indeed we have the following result that is a special
case of the general Theorem 6.9.2 below.
6.9.1. Proposition. Let G be an open subset of R2, let (a, b) ∈G, and let f : G →
R be a continuously diﬀerentiable function such that f (a, b) = 0. If ∂y f (a, b) ̸= 0,
then there exists an open interval I that contains a and a continuously diﬀerentiable
function h : I →R satisfying the following.
(a) h(a) = b.
(b) f (x, h(x)) = 0 for all x in I.
We aren’t going to prove this now; indeed as was mentioned it is a corollary of
Theorem 6.9.2. You will be asked later to prove this proposition as a consequence
of the theorem.
Some of the diﬃculty in stating and understanding the general result stems from
the complexity of the situation, so let’s introduce some notation that might be
helpful. We consider the Euclidean space Rp+q and express it as the direct sum
Rp+q = Rp ⊕Rq. See Deﬁnition 6.3.7. Thus Rp+q has the elements x ⊕y where
x ∈Rp and y ∈Rq. This is the true orthogonal direct sum: when x ∈Rp and y ∈Rq,
then the norm of x ⊕y as an element of Rp+q is precisely

∥x∥2 + ∥y∥2. Note that
the order is important. Though mathematically Rp+q and Rq+p are the same, we are
making a distinction for the purposes of presenting the Implicit Function Theorem.
This theorem gives a condition on a function f : Rp+q →Rq at a point a in Rp such
that there is a function h deﬁned in a neighborhood of a and taking values in Rq for
which we have f (x ⊕h(x)) = 0. In other words, in the equation f (x ⊕y) = 0 the
function h locally solves for y in terms of x.
We introduce similar notation to that above but now for linear transformations. If
T ∈L(Rp+q, Rq) we write T = Tp ⊕Tq, where Tp : Rp →Rq, Tq : Rq →Rq, and
they are deﬁned as
Tp(x) = T (x ⊕0)
and
Tq(y) = T (0 ⊕y)
This means that T (x ⊕y) = Tp(x) + Tq(y). (At the risk of seeming pedantic but to
make sure all have the right idea about what’s going on, we emphasize that the +
sign on the right side of this equation is not a mistake; it is not ⊕.)
The idea here is that we have an open subset G of Rp+q, a continuously dif-
ferentiable function f : G →Rq, and we are interested in the set {x ⊕y ∈Rp+q :
f (x ⊕y) = 0}. On this set y is implicitly deﬁned as a function of x. We would like
to know conditions on f under which we can make this deﬁnition explicit. In other
words, where can we deﬁne y locally as a function of x? The following theorem
gives a suﬃcient condition for this to happen.
6.9.2. Theorem (Implicit Function Theorem). Let G be an open subset of Rp+q, let
a ⊕b ∈G, and let f : G →Rq be a continuously diﬀerentiable function such that
f (a ⊕b) = 0. If T = D f (a ⊕b) and Tq : Rq →Rq is invertible, then there exists

6.9 Implicit Function Theorem
209
an open subset  of Rp that contains the point a and a continuously diﬀerentiable
function h :  →Rq satisfying the following.
(a) h(a) = b.
(b) f (x ⊕h(x)) = 0 for all x in .
(c) Dh(b) = −T −1
q
Tp.
Proof. Begin by deﬁning F : G →Rp+q by F(x ⊕y) = x ⊕f (x ⊕y). It follows
that F is continuously diﬀerentiable. (Why?) Also note that F(a ⊕b) = a ⊕0.
Claim. DF(a ⊕b) is an invertible linear transformation in L(Rp+q).
Since T = D f (a ⊕b), we use Proposition 6.5.4 and the fact that f (a ⊕b) = 0
to get that for s ⊕t in Rp+q we have f [(a + s) ⊕(b + t)] = [T (s ⊕t) + R(s ⊕t)],
where ∥R(s ⊕t)∥/∥(s ⊕t)∥→0 as ∥s ⊕t∥→0. Now
F[(a + s) ⊕(b + t)] −F(a ⊕b) = (a + s) ⊕f [(a + s) ⊕(b + t)] −a ⊕0
= s ⊕[T (s ⊕t) + R(s ⊕t)]
= [s ⊕T (s ⊕t)] + [0 ⊕R(s ⊕t)]
Since ∥0 ⊕R(s ⊕t)∥/∥s ⊕t∥→0 as ∥s ⊕t∥→0, we have that DF(a ⊕b) is the
linear transformation in L(Rp+q) deﬁned by
DF(a ⊕b) : s ⊕t →s ⊕T (s ⊕t)
So if DF(a ⊕b)(s ⊕t) = 0, then s = 0 and 0 = T (s ⊕t) = Tq(t). Since the
hypothesis is that Tq is invertible, we also get that t = 0. Hence DF(a ⊕b) is
injective and therefore invertible. (Recall from linear algebra that an injective linear
transformation on a ﬁnite dimensional space is invertible.) This establishes the
claim.
The claim allows us to apply the Inverse Function Theorem to F : Rp+q →Rp+q
and get an open neighborhood U of a ⊕b on which F is a bijection, V = F(U)
is open, and the inverse map H : V →U ⊆Rp+q is continuously diﬀerentiable.
Put  = {x ∈Rp : x ⊕0 ∈V}; since V is open, it follows that  is open (Exer-
cise 2). Since F(a ⊕b) = a ⊕f (a ⊕b) = a ⊕0 ∈V, a ∈. For x in  we
have that x ⊕0 ∈V and so H(x ⊕0) = x ⊕y ∈U ⊆G for some y. Hence
x ⊕0 = F(x ⊕y) = x ⊕f (x ⊕y). Because F is injective on U, this x ⊕y is
unique. Thus y must be unique. What we have shown is that for every x in  there
is a unique y in Rq with H(x ⊕0) = x ⊕y. The uniqueness of this y for each x in
 means we have deﬁned a function h :  →Rp so that for each x in 
H(x ⊕0) = x ⊕h(x)
Note that h is continuously diﬀerentiable since H is (Verify!). Since F(a ⊕b) =
a ⊕0, we have that (a) is satisﬁed. By deﬁnition x ⊕0 = F(x ⊕h(x)) =
x ⊕f (x ⊕h(x)), so that (b) is satisﬁed.
It remains to show that (c) holds. For convenience let g :  →Rp+q be
the function deﬁned by g(x) = x ⊕h(x). So for each x in  we have that

210
Differentiation in Higher Dimensions
Dg(x) ∈L(Rp, Rp+q). In fact
Dg(x) = 1 ⊕Dh(x)
where this 1 stands for the identity linear transformation on Rp and Dh(x) ∈
L(Rp, Rq). We know from (b) that 0 = f (g(x)) so the Chain Rule implies that
0 = [D f ](g(x))Dg(x). Now let x = a. From (a) we have that h(a) = b so we get
0 = D f (a ⊕b)Dg(a) = T Dg(a). Thus
0 = T [1 ⊕Dh(a)] = Tp + TqDh(a)
Since Tq is invertible, Dh(a) = −T −1
q
Tp.
■
At this point you should complete Exercise 1, which asks you to prove Proposition
6.9.1 as a consequence of the theorem.
We will refer to the Implicit Function Theorem as the IPFT. Observe that the IPFT
gives a suﬃcient condition for the existence of the function h. This condition is not
necessary. This is like the situation with f (x) = x3 on R, where the function has an
inverse even though f ′(0) = 0. See Exercise 4. Let’s also note that in the statement
of the IPFT rather than the condition f (a ⊕b) = 0 we could have assumed f (a ⊕
b) = c for some constant c and then obtained the function h with f (x ⊕h(x)) = c
for all x in . In fact we can do this by just substituting the function f −c for f .
The condition in the theorem that Tq is invertible can be interpreted using Jaco-
bians. Since f : G →Rq, we can write f using its coordinate functions: f (x ⊕y) =
( f1(x ⊕y), . . . , fq(x ⊕y)). When we write y = (y1, . . . , yq) in Rq, we get that the
linear transformation Tq is given by the Jacobian matrix
∂( f1, . . . , fq)
∂(y1, . . . , yq)(a ⊕b)
The invertibility of this matrix is guaranteed by having its determinant non-zero.
6.9.3. Corollary. Let G be an open subset of R2, let a ⊕b ∈G, and let f : G →R
be a continuously diﬀerentiable function such that f (a ⊕b) = 0. If (∂f /∂y)(a ⊕
b) ̸= 0, then there is an interval (α, β) containing the point a and a continuously
diﬀerentiable function h : (α, β) →R satisfying: (a) h(a) = b; (b) f (x ⊕h(x)) = 0
for all x in (α, β); and (c)
h′(a) = −
∂f
∂x (a ⊕b)
∂f
∂y (a ⊕b)
See Exercise 5.
6.9.4. Example. (a) Can the equation (x2 + y2 + 2z2)
1
2 = cos z be solved for y in
terms of x and z near the point (0, 1, 0)? To do this write f (x, z, y) = (x2 + y2 +
2z2)
1
2 −cos z, where we reversed the alphabetical order of y and z to more easily

6.9 Implicit Function Theorem
211
interpret the IPFT. So f : R3 →R. A calculation shows that
∂f
∂y =
y

x2 + y2 + 2z2
Since this is not 0 at (0, 1, 0) we can apply the IPFT.
(b) Can we solve the above equation for z in terms of x and y near the point
(0, 1, 0)? Again a calculation shows that
∂f
∂z =
2z

x2 + y2 + 2z2 + sin z
and we have that (∂f /∂z)(0, 1, 0) = 0. Thus we cannot apply the IPFT.
Exercises
(1)
Prove Proposition 6.9.1 as a consequence of the Implicit Function Theorem.
(2)
Show that the set  in the proof of Theorem 6.9.2 is open. (See Exercise 6.4.17.)
(3)
Prove the following version of the Implicit Function Theorem for linear transfor-
mations. If T ∈L(Rp+q, Rq) and Tq is invertible, then the linear transformation
−T −1
q
Tp : Rq →Rp satisﬁes
T

x ⊕

−T −1
q
Tp(x)

= 0
for every x in Rp. (It’s actually easier to just give a direct proof of this rather than
showing how it follows from the IPFT, but seeing how it is a corollary of that result
might be instructive.)
(4)
Find a smooth function f : R2 →R with (∂f /∂y)(0 ⊕0) = 0 but where there is an
open interval (α, β) in R that contains 0 and a continuous function h : (α, β) →R
such that h(0) = 0 and f (x ⊕h(x)) = 0 for all x in (α, β). Can you get such an
example with h diﬀerentiable?
(5)
Let G be an open subset of Rp+1, let a ⊕b ∈G, and let f : G →R be a continu-
ously diﬀerentiable function such that f (a ⊕b) = 0. (a) Give a suﬃcient condition
on f that there is an open subset  of Rp that contains a and a continuously diﬀer-
entiable function h :  →R such that h(a) = b and f (x ⊕h(x)) = 0 for all x in .
(b) Can you express this condition in terms of Jacobians? (c) Write out a formula
for h′(a).
(6)
Consider the two equations x2 −y2 −u3 + v2 + 4 = 0 and 2xy + y2 −2u2 +
3v4 + 8 = 0. Show that near the point (x, y, u, v) = (2, −1, 2, 1) we can solve for
u and v in terms of x and y.
(7)
Let f (x, y) = exy + sin y + y2 −1 and show that there is a continuously diﬀeren-
tiable function h deﬁned in a neighborhood U of 2 such that f (x, h(x)) = 0 for all
x in U. Compute h′(x) for x in U.

212
Differentiation in Higher Dimensions
(8)
Show that the Inverse Function Theorem can be derived as a consequence of the
IPFT so that a diﬀerent proof of the IPFT would yield the Inverse Function Theorem
as a corollary. (Such a proof exists.)
6.10. Lagrange Multipliers*
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
We want to investigate optimization problems for real-valued functions that involve
constraints on the possible set where the extremum occurs. For example suppose we
are given an open set G in Rp+1 and two continuously diﬀerentiable functions f , g :
G →R; and the objective is to maximize the value of f (x) subject to the constraint
that g(x) = 0. (We use Rp+1 here rather than Rp in order to be consistent with what
comes later.) Such a problem often arises in geometry, physics, and economics, as
well as other areas of science. You can consider the set {x ∈Rp+1 : g(x) = 0} as
deﬁning a surface in Rp+1, and we are seeking to ﬁnd the maximum value of f
on this surface. (We haven’t formally deﬁned the concept of a surface in Rp+1, but
when p = 2 this set is a traditional surface in R3; when p = 1 it is a curve in the
plane. The general deﬁnition will come later and, if you wish, you can just consider
it as a subset of Rp+1.)
The approach for solving such a problem is to consider the function F : G →R
deﬁned by F(x) = f (x) −λg(x) for some scalar λ. Why do this? Suppose we can
ﬁnd a λ such that ∇F(c) = 0 and c satisﬁes the constraint g(c) = 0. Since c is a
critical point for F there is the possibility that F has a relative extremum at c. But
F(c) = f (c) since g(c) = 0 and there is the additional possibility that f has a rela-
tive extremum at c. Clearly this won’t always work. Nevertheless in the result below
(see Corollary 6.10.6 and the more general Theorem 6.10.3) we show that under rea-
sonable conditions this is a necessary condition for f to have a relative extremum at
c with g(c) = 0. That is, we will see that when f does have an extremum at such a c
then we can ﬁnd a scalar λ such that ∇( f −λg)(c) = 0. This discussion may seem
abstruse, but after seeing some examples it will become apparent that the method
works in many situations.
6.10.1. Definition. Assume p, q ≥1 and G is an open subset of Rp+q, f : G →R
is a continuously diﬀerentiable function, c ∈G, and g : G →Rq is a continuously
diﬀerentiable function with g(c) = 0. The function f is said to have a local max-
imum at c subject to the constraint g(c) = 0 if there is an r > 0 such that when
x ∈B(c; r) and g(x) = 0, we have that f (c) ≥f (x). Similarly we can deﬁne f to
have a local minimum at c subject to the constraint g(c) = 0. f is said to have a
local extremum at c subject to the constraint g(c) = 0 if f has either a local maxi-
mum or minimum at c subject to this constraint.
It is probably needless to point out that the problem of ﬁnding the local minimum
of f subject to the constraint g(x) = 0 is the same as ﬁnding the local maximum of
−f subject to the same constraint. Also the use of 0 in the constraint is arbitrary;
any constant would do. Indeed we could have considered constraints of the form

6.10 Lagrange Multipliers
213
g(x) = h(x), where h is another continuously diﬀerentiable function on G. This
reduces to what was described in the preceding deﬁnition by simply replacing this
constraint by the one g(x) −h(x) = 0.
6.10.2. Example.
(a) Suppose in the deﬁnition above p = 2, q = 1, and g :
R3 →R is deﬁned by g(x, y, z) = x2 + y2 + z2 −1. We note that the set {(x, y, z) :
g(x, y, z) = 0} is the surface of the unit ball centered at the origin, the so-called unit
sphere. So if we are given a function f : R3 →R, the object would be to maximize
or minimize f over this sphere.
(b) Find the maximum and minimum values of f (x, y, z) = x2 −y2 on the ellip-
soid x2 + 2y2 + 3z2 = 1. (We’ll solve this problem below in Example 6.10.9 after
we prove Corollary 6.10.6.) Note that f does have a maximum and a minimum since
it is continuous and the ellipsoid is compact.
Adopt the notation in Deﬁnition 6.10.1. As in §6.9 we write Rp+q = Rp ⊕Rq.
That is, points in Rp+q are written as x ⊕y with x in Rp and y in Rq. In particular
the distinguished point c in G is written as c = a ⊕b. For the set G let
Gq = {y ∈Rq : a ⊕y ∈G}
Next deﬁne the function ˆg : Gq →Rq by
ˆg(y) = g(a ⊕y)
Hence ˆg(b) = 0. The notation gq seems more appropriate than ˆg, but for 1 ≤i ≤q
we write the coordinate functions gi : G →R of g : G →Rp as
gi(x ⊕y) = ⟨g(x ⊕y), di⟩
where d1, . . . , dq is the standard basis for Rq. So the notation gq is already taken.
With this notation established we can state the theorem, which gives a necessary
condition for the constrained extremum to exist.
6.10.3. Theorem (Lagrange7 Multipliers). Assuming the notation established in
the preceding paragraph, if f has a local extremum at c subject to the constraint
g(c) = g(a ⊕b) = 0 and
6.10.4
Dˆg(b) : Rq →Rq is invertible
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
7 Joseph-Louis Lagrange is claimed by many Italian mathematicians as their countryman, and with justiﬁcation
in spite of his French name. Both his parents were Italian, he was born in Turin, Italy in 1736, and he was
baptized Giuseppe Lodovico Lagrangia. He did have French ancestors and as a youth called himself Lagrange.
He studied at the college in Turin and came reluctantly to mathematics. In fact he once wrote, “If I had been
rich, I probably would not have devoted myself to mathematics,” which certainly seems like a non-sequitur to
a modern reader. His early work on the calculus of variations attracted the attention of Euler and at the age of
19 he was appointed professor of mathematics at the Royal Artillery School in Turin. He continued to work on
analysis and astronomy. Eventually he succeeded Euler as Director of Mathematics at the Berlin Academy in
1766. A year later he married his cousin, Vittoria Conti. She died in 1783 and they had no children. In 1787 he
left Berlin to become a member of the Académie des Sciences in Paris, where he remained for the rest of his
career. His work covers a wide range of topics from number theory to astronomy, with numerous publications.
Napoleon named him to the Legion of Honour and Count of the Empire in 1808. In 1813 he died in Paris.

214
Differentiation in Higher Dimensions
then there is a vector λ in Rq such that
6.10.5
[Dg(a ⊕b)]tλ = ∇f (c)
We will only prove this result when q = 1. The proof of the general theorem
is notationally complicated involving matrices and it isn’t clear that it’s worth the
eﬀort. The interested reader can see [15] or [11] for a proof. After stating and proving
the corollary we give a more matrix oriented way of viewing the hypothesis and
conclusion of the theorem.
6.10.6. Corollary. Let G be an open subset of Rp+1, c = a ⊕b ∈G, f : G →R a
continuously diﬀerentiable function, and g : G →R a continuously diﬀerentiable
function with g(c) = 0. If f has a local extremum at c subject to the constraint
g(c) = 0 and G1 = {y ∈R : a ⊕y ∈G}, ˆg : G1 →R is deﬁned as ˆg(y) = g(a ⊕y),
and
[ˆg]′(b) = ∂p+1g(c) ̸= 0
then there is a λ in R such that
∇[ f −λg](c) = 0
Thus f −λg has a critical point at c.
Proof. Writing out the components of the equation ∇[ f −λg](c) = 0, we see that
we want to ﬁnd a scalar λ such that
6.10.7
∂j f (c) −λ∂jg(c) = 0
for
1 ≤j ≤p + 1
Using the hypothesis that ∂p+1g(c) ̸= 0 we deﬁne
λ = ∂p+1 f (c)
∂p+1g(c)
We recognize that this deﬁnition of λ is precisely (6.10.7) when j = p + 1; it
remains to show that we have this equation for the other values of j. To do this
we will use the fact that f has a local extremum at c subject to the constraint.
We apply the IPFT. Since ∂p+1g(c) ̸= 0 there is an open neighborhood  of a in
Rp and a continuously diﬀerentiable function h :  →R such that h(a) = b and
g(x ⊕h(x)) = 0 for all x in . We will use the observation that because f has a
local extremum at c subject to the constraint g(x) = 0 this implies that the map
x →f (x ⊕h(x)) of  into R has an unconstrained local extremum at a. (Verify!)
Therefore by Theorem 6.2.20 all the partial derivatives of the function ω :  →R
deﬁned by ω(x) = f (x ⊕h(x)) must vanish at a. We need to compute. Since ω is
the composition of f and x →x ⊕h(x), the Chain Rule implies that for 1 ≤j ≤p
∂jω(x) = [∂j f ](x ⊕h(x)) + [∂fp+1](x ⊕h(x))[∂jh(x)]
Hence for 1 ≤j ≤p
6.10.8
0 = ∂jω(a) = ∂j f (c) + ∂p+1 f (c) ∂jh(a)

6.10 Lagrange Multipliers
215
Now we need to apply the Chain Rule again. Since g(x ⊕h(x)) = 0 on , we have
that the partial derivatives of x →g(x ⊕h(x)) are all 0. Therefore for 1 ≤j ≤p
0 = ∂j[g(x ⊕h(x))]
= [∂jg](x ⊕h(x)) + [∂gp+1](x ⊕h(x))[∂jh(x)]
Since a ⊕h(a) = c, this last equation implies that
0 = ∂jg(c) + ∂gp+1(c) ∂jh(a)
for
1 ≤j ≤p
Again using the hypothesis, this implies that
∂jh(a) = −∂jg(c)
∂p+1g(c)
for
1 ≤j ≤p
Substituting this into (6.10.8) we get that
0 = ∂j f (c) + ∂p+1 f (c) −
 ∂jg(c)
∂p+1g(c)

= ∂j f (c) −λ∂jg(c)
which is exactly the sought after (6.10.7).
■
Let’s reﬂect on the preceding proof.
The challenge was to ﬁnd one number λ that simultaneously solved the p + 1
equations (6.10.7). Perhaps it was surprising that we obtained the solution for all
the equations by solving one of them, the (p + 1)-st. To bring this about we used the
existence of the constrained extremum and the Implicit Function Theorem. That will
set the pattern for the solution of the general theorem if you consult the references.
6.10.9. Example. Find the maximum and minimum values of f (x, y, z) = x2 −y2
on the ellipsoid x2 + 2y2 + 3z2 = 1. This is the situation in the above corollary
where p = 2, q = 1, and g(x, y, z) = x2 + 2y2 + 3z2 −1. Let λ ∈R and note that
∇( f −λg) = ∇[(x2 −λx2) + (−y2 −2λy2) −3λz2 −λ]
=

2x(1 −λ), 2y(−1 −2λ), −6λz

Setting these three coordinates equal to 0 we see we must ﬁnd all possible values of
λ such that the following four equations have a solution for x, y, and z:
6.10.10
0 = x(1 −λ); 0 = y(−1 −2λ); 0 = λz; 0 = x2 + 2y2 + 3z2 −1
If we have λ = 0, we see that x = y = 0 while 0 = 3z2 −1. Thus we have two crit-
ical points

0, 0, ± 1
√
3

when λ = 0
Now assume λ ̸= 0. The ﬁrst observation is that this forces z = 0. We also see that
we must consider the two cases when x = 0 and x ̸= 0. In the ﬁrst case we get there

216
Differentiation in Higher Dimensions
are two more critical points

0, ± 1
√
2
, 0

when λ ̸= 0 and x = 0
When x ̸= 0 it has to be that λ = 1; consequently y = 0. Thus from the last of the
equations in (6.10.10) we have that x = ±1. This produces two more critical points
(±1, 0, 0)
when λ ̸= 0 and x ̸= 0
Now it’s a matter of calculating the value of f at each of the critical points:
f

0, 0, ± 1
√
3

= 0
f

0, ± 1
√
2
, 0

= −1
2
f (±1, 0, 0) = 1
Therefore the function f has constrained maximums at (±1, 0, 0) and constrained
minimums at (0, ±1/
√
2, 0).
Let’s examine what (6.10.4) and (6.10.5) mean in terms of matrices. When you
apply the theorem, you are quite likely to use systems of equations. Let’s start by
using the Jacobian determinant to interpret the ﬁrst of these, which is equivalent to
6.10.11
ˆg(a) = ∂(g1, . . . , gq)
∂(y1, . . . , yq) (c) ̸= 0
Now Dg(c) ∈L(Rp+q, Rq), so that [Dg(c)]t ∈L(Rq, Rp+q). Hence when λ ∈Rq
we have that [Dg(c)]tλ ∈Rp+q as is ∇f (c). Hence the conclusion (6.10.5) can be
phrased as saying we want a λ = (λ1, . . . , λq) in Rq that satisﬁes the two sets of
equations
6.10.12
q

i=1
λi
∂gi
∂yk
(c) = ∂f
∂yk
(c)
for 1 ≤k ≤q
and
6.10.13
q

i=1
λi
∂gi
∂xℓ
(c) = −∂f
∂xℓ
(c)
for 1 ≤ℓ≤p
We can ﬁnd a λ in Rq such that (6.10.12) holds since this set of equations trans-
lates to the requirement that
Dˆg(b)λ =

 ∂f
∂y1
(c), . . . , ∂f
∂yq
(c)

and in light of the assumption (6.10.4) we can ﬁnd such a λ. The objective of the
proof of the main theorem is to show that for this same λ the equations (6.10.13)
are also satisﬁed. As we did when we proved Corollary 6.10.6 this is achieved by an

6.10 Lagrange Multipliers
217
application of the Implicit Function Theorem and the fact that when a scalar-valued
function has a relative extremum its gradient vanishes.
6.10.14. Example.
Minimize f (x, y, z, w) = x2 + y2 + z2 + w2 subject to x +
y + z + w = 10 and x −y + z + 3w = 6. Let’s point out that relative to Theorem
6.10.3 p = 2, q = 2, and g(x, y, z, w) = (x + y + z + w −10, x −y+z+3w−6).
Also the two sets in R4 where x + y + z + w = 10 and x −y + z + 3w = 6 are
aﬃne hyperplanes (6.7.3). So the constraining set where g = 0 is the intersection
of these two aﬃne hyperplanes. Therefore this example asks to ﬁnd the (square of
the) distance from the origin in R4 to the intersection of the two aﬃne hyperplanes.
This example is from [12] where the reader can ﬁnd the solution on page 14.
Exercises
(1)
Find the extrema of f (x, y) = x2 −4xy + 4y2 subject to x2 + y2 = 1.
(2)
A rectangle has perimeter p. Find its largest possible area.
(3)
A rectangle has area A. Find its smallest possible perimeter.
(4)
(a) Show that f (x, y, z)=z2 has only one critical point on the surface x2+y2−z= 0.
(b) Show that at that critical point f has a constrained minimum. (c) Why isn’t there
a constrained maximum?
(5)
Find the extreme values of f (x, y, z)=x+y2+2z subject to 4x2+9y2−36z2 = 36.


7
Integration in Higher Dimensions
Here we will extend the development of the Riemann integral of functions deﬁned
on the real line, as seen in Chapter 3, to the integral of functions deﬁned on subsets
of Rp. The objective of this chapter is not to give the most exhaustive treatment
of this integral in Rp, far from it. Those who wish to see such a treatment can
consult the books [11] and [15]. In the next level of a course in analysis, a far more
general theory of integration is developed. (It’s called Lebesgue integration after its
discoverer.) What we want to do here is to prepare the reader to tackle the uses of
the integral in Rp that we’ll encounter later in this book. In many ways what we
seek to do here is to explore and understand situations in Rp where integration can
be reduced to what we did in Chapter 3. In that sense the key is §7.3 below where
integrals in Rp are reduced to a succession of integrals in R.
We’ll start with a much less complicated situation where we continue to integrate
over an interval in R but the functions we integrate will be vector valued.
7.1. Integration of Vector-valued Functions
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In §6.1 we discussed continuity and diﬀerentiability of vector-valued functions γ :
[a, b] →Rp and we saw there were few diﬃculties in this. Here we discuss the
integration of such functions and again we’ll see that the theory is almost the same
as what we did when we deﬁned the integral of functions f : [a, b] →R.
We could discuss Riemann sums of functions γ : [a, b] →Rp in parallel with
what we did in Chapter 3, but rather than risk possible boredom we’ll just use what
we developed there and go forward. Given a function γ : [a, b] →Rp, for 1 ≤j ≤
p deﬁne a function γj : [a, b] →R by
γj(t) = ⟨γ (t), ej⟩
where, as usual, {e1, . . . , ep} is the standard basis for Rp. Now deﬁne
7.1.1
 b
a
γ =
p

j=1
 b
a
γj

e j ∈Rp
So
 b
a γ is a vector in Rp. The reader can easily scan the results from Chapter 3 to
see that they almost all carry over to this setting. In particular the FTC is valid: If
γ : [a, b] →Rp is continuously diﬀerentiable, then
 b
a γ ′ = γ (b) −γ (a). One of

220
Integration in Higher Dimensions
the few exceptions to this idea that Chapter 3 results carry over to the integration of
vector-valued functions is the Mean Value Theorem for integrals (Theorem 3.2.4).
See Exercise 1. The interested reader can look as much as (s)he wants at the details
of this new integral, but we’ll spend the rest of this section looking at the integral
from Chapter 3 as applied to a new concept.
Recall (6.1.2) that a curve in Rp is a continuous function γ : [a, b] →Rp and
{γ } = {γ (t) : a ≤t ≤b} is called its trace.
7.1.2. Definition.
If γ : [a, b] →Rp is a smooth curve, then the length or arc
length of γ is deﬁned as
ℓ(γ ) =
 b
a
∥γ ′(t)∥dt
If γ is continuous on [a, b] and smooth on the intervals [ti−1,ti] for 1 ≤i ≤n,
where a = t0 < · · · < tn = b is a partition on [a, b], then we deﬁne the length of
γ as
ℓ(γ ) =
n

i=1
 ti
ti−1
∥γ ′(t)∥dt =
n

i=1
ℓ(γ ([ti−1,ti]))
We should point out that since γ ′ is continuous as is the function x →∥x∥from
Rp into R, the functiont →∥γ ′(t)∥is the composition of two continuous functions;
thus it is integrable and the preceding deﬁnition is legitimate.
7.1.3. Example. (a) If x, y ∈Rp, deﬁne γ : [0, 1] →Rp by γ (t) = ty + (1 −t)x.
So γ traces out the straight line segment in Rp from x to y. Here ℓ(γ ) =
 1
0 ∥y −
x∥dt = ∥y −x∥, as expected.
(b) Recall the deﬁnition of a polygon [x0, x1, . . . , xn] in Rp (Exercise 5.6.12). If
γ is this polygon, then γ is piecewise smooth and ℓ(γ ) = n
i=1 ∥xi −xi−1∥.
(c) If f : [a, b] →R is a continuously diﬀerentiable function and γ : [a, b] →
R2 is deﬁned by γ (t) = (t, f (t)) = te1 + f (t)e2, then
ℓ(g) =
 b
a

1 + | f ′(t)|2 dt
In fact γ ′(t) = e1 + f ′(t)e2, so that ∥γ ′(t)∥=

1 + | f ′(t)|2.
(d) More generally, if f : [a, b] →Rp−1 is a continuously diﬀerentiable function
and γ : [a, b] →Rp is deﬁned by γ (t) = t ⊕f (t) ∈R ⊕Rp−1, then
ℓ(g) =
 b
a

1 + ∥f ′(t)∥2 dt
7.1.4. Proposition. If γ : [a, b] →Rp is a smooth curve and ϵ > 0, then there is
a partition a = t0 < ti < · · · < tn = b such that

 b
a
∥γ ′(t)∥dt −
n

i=1
∥γ (ti) −γ (ti−1)∥
 < ϵ

7.1 Integration of Vector-valued Functions
221
Proof. The lack of a MVT for vector-valued functions (Exercise 6.1.4) creates a bit
of a complication for the proof and we must introduce an auxiliary function F. Let
γ (t) = (γ1(t), . . . , γp(t)) for all t in [a, b], let [a, b]p be the cartesian product of the
interval [a, b] with itself p times, and deﬁne F : [a, b]p →R by F(t1, . . . ,tp) =
p
j=1 |γ ′
j(t j)|2. Observe that F is a continuous function on a compact set, and
thus is uniformly continuous. Hence if ϵ > 0 there is a δ > 0 such that for τ =
(t1, . . . ,tp), σ = (s1, . . . , sp) in [a, b]p with ∥τ −σ∥< δ we have that |F(σ ) −
F(τ )| < ϵ/2(b −a). In particular since F(t, . . . ,t) = ∥γ ′(t)∥, we have that if {a =
t0 < t1 < · · · < tn = b} satisﬁes ti −ti−1 < δ/√p for 1 ≤i ≤n, then
7.1.5

 b
a
∥γ ′(t)∥dt −
n

i=1
∥γ ′(ti)∥(ti −ti−1)
 < ϵ
2
Now we apply the one-dimensional MVT to each γj to conclude that for 1 ≤j ≤
p, 1 ≤i ≤n we can ﬁnd a point cji in [ti−1,ti] with
γj(ti) −γj(ti−1) = γ ′
j(cji)(ti −ti−1)
Therefore for each i = 1, . . . , n we have that
F(c1i, . . . , cpi)(ti −ti−1) =
⎛
⎝
p

j=1
|γ ′
j(cji)|2
⎞
⎠
1
2
(ti −ti−1) = ∥γ (ti) −γ (ti−1)∥
Note that for 0 ≤i ≤n we have that ∥(c1i, . . . , cpi) −(ti, . . . ,ti)∥< δ, so that
|F(c1i, . . . , cpi) −F(ti, . . . ,ti)| < ϵ/2(b −a). Hence
n

i=1
∥γ (ti) −γ (ti−1)∥=
n

i=1
F(c1i, . . . , cpi)(ti −ti−1)
=
n

i=1
[F(c1i, . . . , cpi) −F(ti, . . . ,ti)](ti −ti−1)
+
n

i=1
F(ti, . . . ,ti)(ti −ti−1)
<
ϵ
2(b −a)
n

i=1
(ti −ti−1) +
n

i=1
∥γ ′(ti)∥(ti −ti−1)
= ϵ
2 +
n

i=1
∥γ ′(ti)∥(ti −ti−1)
Similarly
n

i=1
∥γ (ti) −γ (ti−1)∥> −ϵ
2 +
n

i=1
∥γ ′(ti)∥(ti −ti−1)

222
Integration in Higher Dimensions
Combining these inequalities with (7.1.5) we get that

 b
a
∥γ ′(t)∥dt −
n

i=1
∥γ (ti) −γ (ti−1)∥
 < ϵ
proving the proposition.
■
So this proposition says that ℓ(γ ) can be approximated as close as desired by the
length of an inscribed polygon [γ (t0), . . . , γ (tn)] (Example 7.1.3(b)). This justiﬁes
deﬁning the length of the curve γ as we did. Also see Exercise 2.
A problem arises from deﬁning a curve as a function. Is there a diﬀerence
between the curve γ : [0, π] →R2 deﬁned by γ (t) = (r cost, r sint) and the curve
ρ : [−r, r] →R2 deﬁned by ρ(t) = (−t,
√
r2 −t2)? These curves describe the
same set of points and they should have the same length. Here is how we address
this.
7.1.6. Definition. If γ : [a, b] →Rp and ρ : [c, d] →Rp are two smooth curves,
say that ρ is equivalent to γ if there is a continuously diﬀerentiable, strictly increas-
ing surjection τ : [a, b] →[c, d] such that γ (t) = ρ(τ(t)) for all t in [a, b]. In sym-
bols we write γ ∼ρ. We also say that ρ is a reparametrization of γ .
Note that since the inverse of a continuously diﬀerentiable, strictly increasing
surjection is a continuously diﬀerentiable, strictly increasing surjection, it easily
follows that ∼is an equivalence relation on the set of all smooth curves in Rp. (Exer-
cise 7). Clearly equivalent curves trace out the same subset of Rp. Now we show
that the deﬁnition of the length of a curve is independent of which parametrization
of the curve we take.
7.1.7. Proposition. If γ and ρ are equivalent curves in Rp, then ℓ(γ ) = ℓ(ρ).
Proof. Adopt the notation of the preceding deﬁnition. By the COV Theorem (3.2.5)
ℓ(ρ) =
 d
c
∥ρ′(s)∥ds =
 b
a
∥ρ′(τ(t))∥τ ′(t)dt =
 b
a
∥γ ′(t)∥dt = ℓ(γ )
■
Strictly speaking we should have deﬁned a curve as an equivalence class of func-
tions using the equivalence relation ∼deﬁned above. Instead we opted for a less
stringent approach.
It might have been observed by the reader that the last proposition remains valid
if we allow the reparametrization function τ to be strictly decreasing. We insist that
τ ′ > 0 for a reason that will appear in Chapter 8.
Exercises
(1)
If γ : [0, 1] →R2 is deﬁned by γ (t) = (t,t2), then there is no point c in [0, 1] such
that
 1
0 γ = γ (c).

7.2 The Riemann Integral
223
(2)
Show that if γ : [a, b] →Rp is a smooth curve, then
ℓ(γ ) = sup
	 n

i=1
∥γ (ti) −γ (ti−1)∥: {a = t0 < · · · < tn = b}
A
(3)
Are the curves γ : [0, π] →R2 and ρ : [−r, r] →R2 deﬁned by γ (t) =
(r cost, r sint) and ρ(t) = (−t,
√
r2 −t2) equivalent?
(4)
Let γ : [0, 4] →R3 be the helix deﬁned by γ (t) = (cost, sint,t3/2) and compute
its length.
(5)
Deﬁne γ : [0, 2π] →R3 by γ (t) = (et sint, et cost, et) and compute ℓ(γ ).
(6)
Compute the length of the graph of the function f (x) = (x2)
1
3 with −1 ≤x ≤1.
(7)
Show that γ ∼ρ deﬁnes an equivalence relation on the set of all smooth curves in
Rp.
7.2. The Riemann Integral
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In this section X always denotes a bounded subset of Rp and f : X →R is a
bounded function.
A rectangle in Rp is a set of the form
R = [a1, b1] × · · · × [ap, bp] = [a, b] ⊆Rp
where a = (a1, . . . , ap), b = (b1, . . . , bp) are points in Rp, and a j <b j for 1≤j≤p.
(The notation R = [a, b] is a convenient shorthand and the context will usually pre-
vent confusion with a closed interval in R.) We will also refer to the volume of R as
the number
V (R) = (b1 −a1) · · · (bp −ap)
Two rectangles in Rp are non-overlapping if their intersection has empty interior.
Equivalently, if R = [a, b], S = [c, d] ⊆Rp, then R and S are non-overlapping if
(a, b) ∩(c, d) = ∅, where I hope the meaning of (a, b) and (c, d) as the open rect-
angles in Rp is clear. Equivalently they are non-overlapping if R ∩S = ∂R ∩∂S,
where this intersection could be empty. When X ⊆Rp, say that a ﬁnite collection
of rectangles R1, . . . , Rm is a R-cover of X if these rectangles are non-overlapping,
X ⊆m
k=1 Rk, and Rk ∩X ̸= ∅for 1 ≤k ≤m. (This last restriction is included
to prevent trivialities.) Note that the fact that we are assuming that X is always a
bounded set means we can always ﬁnd a R-cover of X. More on this in the follow-
ing example.) In Rp a R-cover will play the role that a partition of an interval did in
Chapter 3.
7.2.1. Example. (a) An easy example of a R-cover of a bounded set X is R = {R},
where R is a rectangle that contains X.
(b) When p = 1, what we call a rectangle is just a closed interval and its vol-
ume is the interval’s length. A R-cover of [a, b] in this setting is just a partition of

224
Integration in Higher Dimensions
the interval, though the way we deﬁned a R-cover R of [a, b] allows some of the
intervals in R to extend outside of [a, b].
(c) In R2 a rectangle is what we usually call a closed rectangle and two rect-
angles in R2 are non-overlapping if their intersection is contained in the intersec-
tion of their sides. If X is a bounded subset of R2, then we can form a R-cover
of X as follows. Draw a grid consisting of vertical and horizontal lines such that
these lines do not accumulate anywhere. (You could achieve this by requiring all
the vertical and horizontal lines to be separated by at least some minimum distance,
though this is not the only way to do this.) This grid deﬁnes an inﬁnite collection of
non-overlapping closed rectangles. Now discard all the rectangles that are disjoint
from X. Because X is bounded, only a ﬁnite number of rectangles remain. For future
reference note that by making the distance between the vertical and horizontal lines
smaller we can make the R-cover ﬁner, a term made precise below.
(d) In R3 a rectangle is a parallelepiped whose faces are two-dimensional rectan-
gles; that is, it is a box. Two such rectangles are non-overlapping if their intersection
is contained in the intersection of their faces.
(e) Extending what was done in (c), if X is a bounded subset of Rp, then we
can form a R-cover of X as follows. Fix each coordinate j, 1 ≤j ≤p, choose
a doubly inﬁnite sequence {an
j : −∞< n < ∞}, where an
j →±∞as n →±∞.
Now consider the countable collection of aﬃne hyperplanes Hn
j = {x ∈Rp : x j =
an
j}. The collection of these hyperplanes {Hn
j : 1 ≤j ≤p, −∞< n < ∞} forms a
p-dimensional grid in Rp and the resulting rectangles that meet X forms a R-cover
of X. You might follow this when p = 2 to see this is what we did in part (c) and
then follow it through when p = 3.
The conceptual diﬃculties in making the transition in integration
from the line to higher dimensional Euclidean spaces already manifest
themselves when we consider R2.
Maybe that is somewhat contradicted for the reader when (s)he passed from part
(c) in the last example to part (e); but that diﬃculty is more technical than concep-
tual. As the reader progresses, (s)he can achieve a good deal of understanding by
thinking in terms of the plane and seeing what the results say there.
If Q and R are two R-covers of X say that Q is a reﬁnement of R if each rectangle
in Q is contained in a rectangle from R. Let’s note that we have that 
Q∈Q Q ⊆

R∈R R, but, unlike when we consider partitions of a compact interval in R, we
do not insist that these two unions are the same. Indeed to require that as well as
having every rectangle in the ﬁner R-cover Q meet X is often impossible. Let’s note
that if R = {R1, . . . , Rm} and S = {S1, . . . , Ss} are any two R-covers of X, then
{R j ∩Si : 1 ≤j ≤m, 1 ≤i ≤s and R j ∩Si ̸= ∅} is a reﬁnement of both R and S.
When we are given a R-cover R of X and R j ∈R, let
MR
j = M j = sup{ f (x) : x ∈R j ∩X}
mR
j = mj = inf{ f (x) : x ∈R j ∩X}

7.2 The Riemann Integral
225
and let
U( f , R) =
m

j=1
MR
j V (R j) and L( f , R) =
m

j=1
mR
j V (R j)
(We will use the notation MR
j and mR
j when we are discussing more than one R-
cover at the same time.) The term U( f , R) is called the upper sum of f for the
R-cover R and L( f , R) is called the lower sum.
The proof of the next proposition is similar to the proof of Proposition 3.1.2 and
is left to the reader as Exercise 2.
7.2.2. Proposition. Assume R is a rectangle in Rp that contains X. If f : X →R
with m ≤f (x) ≤M for all x in X and R and Q are R-covers of X that both reﬁne
the R-cover consisting of R alone, then the following hold.
(a) mV (R) ≤L( f , R) ≤U( f , Q) ≤MV (R).
(b) If Q is a reﬁnement of R, then
L( f , R) ≤L( f , Q) ≤U( f , Q) ≤U( f , R)
(c) If Q is a reﬁnement of R, then
0 ≤U( f , Q) −L( f , Q) ≤U( f , R) −L( f , R)
7.2.3. Definition. Using the preceding proposition we have that for any bounded
subset X of Rp and any bounded function f : X →R
sup{L( f , R) : R is a R-cover of X} ≤inf{U( f , Q) : Q is a R-cover of X}
When these two quantities are equal, we set

X
f =

X
f (x)dx
= sup{L( f , R) : R is a R-cover of X}
= inf{U( f , Q) : Q is a R-cover of X}
In this situation we say that f is Riemann integrable or just integrable. The set of
all Riemann integrable functions on X is denoted by R(X ).
Note that the above deﬁnition only applies to bounded subsets X and bounded
functions f . These assumptions may be omitted when we state results but they are
always assumed.
In the preceding deﬁnitions we see a hint of a diﬃculty in extending the Riemann
integral from the line to higher dimensional Euclidean space. In R we restrict our
attention to integrals over intervals and this allows us to partition them. In Rp we
want to integrate over sets that are not rectangles. (For example, integration over
balls and similar sets.) You might say that we should consider covers by sets more
general than rectangles, but then how do we deﬁne their volume? This lack of sim-
plicity will cause us diﬃculty as we proceed.

226
Integration in Higher Dimensions
As in §3.1 several results follow. Their statements and proofs follow their ana-
logues in §3.1. The ﬁrst such result is similar to that of Proposition 3.1.4.
7.2.4. Proposition. If f is a bounded function, then f is Riemann integrable if and
only if for every ϵ > 0 there is a R-cover R of X such that U( f , R) −L( f , R) < ϵ.
Moreover

X f is the unique number such that for every reﬁnement Q of R we have
L( f , Q) ≤

X
f ≤U( f , Q)
The next is the analogue of Corollary 3.1.5.
7.2.5. Corollary. If f is a bounded function, then f is Riemann integrable if and
only if there is a sequence of R-covers {Rn} of X such that each Rn+1 is a reﬁnement
of Rn and U( f , Rn) −L( f , Rn) →0 as n →∞. When this happens we have that

X
f = lim
n→∞U( f , Rn) = lim
n→∞L( f , Rn)
The next two results give some elementary properties of the integral that extend
their counterparts in §3.1. No proofs will be given.
7.2.6. Proposition. Assume that X is bounded, R is a rectangle containing X, and
f , g ∈R(X ).
(a) If f (x) ≤g(x) for all x in X, then

X
f ≤

X
g
(b) If | f (x)| ≤M for all x in X, then


X
f
 ≤

X
| f | ≤MV (R)
7.2.7. Proposition. R(X ) is a vector space over R. Moreover if f , g ∈R(X ) and
α, β ∈R, then α f + βg ∈R(X ) and

X
(α f + βg) = α

X
f + β

X
g
There are some very complicated subsets of R2, to say nothing of higher dimen-
sional spaces. The interested reader might play with Exercise 3 for a strange exam-
ple. The diﬃculty in developing the Riemann integral for higher dimensions occurs
at the boundary of the set X. Here we introduce a collection of sets which are simul-
taneously abundant and on which there are many integrable functions.
7.2.8. Definition. If E ⊆Rp, say that E has volume zero if for every ϵ > 0 there
is a R-cover {R1, . . . , Rm} of E such that
m

j=1
V (R j) < ϵ

7.2 The Riemann Integral
227
A set X in Rp is called a Jordan1 set if it is bounded, cl X = cl [int X], and its topo-
logical boundary, ∂X, has volume zero.
We’ll see that Jordan sets are the proper places to do Riemann integration. As
we progress we’ll establish several results implying that a set is a Jordan set. An
example of a set that is not a Jordan set follows. To ﬁnd a closed set that is not a
Jordan set is more diﬃcult. But, as we said at the start of this chapter, our aim is not
to explore the boundaries of the theory of integration in Rp but rather to establish
places where the theory works and can be applied to the business at hand.
7.2.9. Example. (a) It is easy to see that any ﬁnite set in Rp has volume zero. So
in R the union of a ﬁnite number of intervals is a Jordan set.
(b) The set X = {(x, y) ∈[0, 1] × [0, 1] : x, y ∈Q} is not a Jordan set since
cl X ̸= cl [int X] = ∅. Also, adapting the argument used in Example 5.3.8(a) we see
that ∂X = [0, 1] × [0, 1] and this certainly does not have volume zero.
(c) If X1, . . . , Xn are sets of volume zero, then X = n
j=1 Xj has volume zero.
Indeed if ϵ > 0 let R j be a R-cover of Xj such that the sum of the volumes of the
rectangles is less than ϵ/n. We would like to put R equal to the union of all the
rectangles in each of the R j, 1 ≤j ≤n. However some of the rectangles in this
union may not be overlapping. This, however, can be ﬁxed by considering all the
possible intersections of rectangles from the Rj. See Exercise 5. Once this is done
we have a R-cover and the sum of the volumes of its rectangles is smaller than ϵ,
proving that X has volume zero.
7.2.10. Proposition. A closed rectangle in Rp is a Jordan set.
Proof. Let R = [a, b] = [a1, b1] × · · · × [ap, bp]. If ϵ > 0 there is a δ > 0 such
that if we set S = [a1 + δ, b1 −δ] × · · · × [ap + δ, bp −δ], then S ⊆R andV (R) −
V (S) < ϵ. The remainder of the proof consists in showing that there is a R-cover
R of ∂R that is contained in R\S, thus implying that the sum of the volumes of
the rectangles in R is smaller than ϵ. (You might want to draw pictures of the
next argument when p = 2. The general argument is a bit cumbersome.) For each
j = 1, . . . , p consider the partition {aj, a j + δ, b j −δ, b j} of [a j, b j] and form the
grid of hyperplanes (as in Example 7.2(e)) determined by these values in each
dimension. Let R = {R1, . . . , Rm} be the rectangles determined by this grid that
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
1 Camille Jordan was born in 1838 in Lyon, France. He was educated as an engineer at École Polytechnique and
pursued that profession while continuing to explore mathematics. He defended his doctoral thesis in 1861. He
continued working as an engineer, though taking time in 1862 to marry. He and his wife had eight children. He
made his way back to Paris and became a professor, ﬁrst at École Polytechnique in 1876 and then at Collège
de France in 1883. His mathematical contributions spanned the list of ﬁelds under study at the time, from
algebra to analysis to the emerging ﬁeld of topology. He is the author of the Jordan Curve Theorem and Jordan
Normal forms of matrices. (To be clear he is not the Jordan in Gauss–Jordan elimination or Jordan algebras.)
He also introduced the notion of homotopic paths and worked extensively on ﬁnite groups. He retired in 1912.
During World War I three of his sons were killed. Another son was a government minister, another a professor
of history, and the last son was an engineer. He received many honors during his life, which ended in 1922
in Paris.

228
Integration in Higher Dimensions
intersect ∂R. Thus R is a R-cover of ∂R, and, moreover, each Rk ⊆R\S. It follows
that m
j=1 V (R j) < ϵ.
■
To prove that a set is a Jordan set we need results that tell us when a set has volume
zero. The next one will be useful.
7.2.11. Proposition. If Y is a compact subset of Rp−1 and g : Y →R is a continu-
ous function, thenW = {(y, g(y)) : y ∈Y} is a compact subset of Rp having volume
zero.
Proof. The fact that W is compact follows from the observation that y →(y, g(y))
is a continuous function from Y onto W. Let ϵ > 0 and let R be a ﬁxed closed
rectangle in Rp−1 such that Y ⊆R; put V = V (R). Because Y is compact and g is
continuous it follows that g is uniformly continuous. Hence there is a δ > 0 such
that |g(z) −g(y)| < ϵ/4V when y, z ∈Y with ∥z −y∥< δ. Let T = {T1, . . . , Tm}
be a R-cover of Y such that each Tj ⊆R and diam Tj < δ. For 1 ≤j ≤m choose a
point yj in Tj and put
R j = Tj × [g(yj) −ϵ/4V, g(yj) + ϵ/4V]
Observe thatW ⊆m
j=1 R j. In fact, if (y, g(y)) ∈W, then y ∈Y and there is a j such
that y ∈Tj. Thus ∥y −yj∥< δ and so |g(y) −g(yj)| < ϵ/4V. Hence (y, g(y)) ∈R j.
Also V (R j) = V (Tj)[ϵ/2V]. Thus m
j=1 V (R j) = V[ϵ/2V] < ϵ. Since ϵ was arbi-
trary this proves that W has volume zero.
■
Let’s point out that if X ⊆Rp and we can cover X by a ﬁnite number of sets
having the form of W in the preceding proposition, then X has volume zero. (See
Example 7.2.9(c) and Exercise 4.)
7.2.12. Corollary. If a ∈Rp and δ > 0, then X = cl B(a; r) is a Jordan set.
Proof. Let a = (a1, . . . , ap) and Y = {y ∈Rp−1 : ∥y∥≤r}. Deﬁne g : Y →R
by g(y) =

r2 −∥y∥2. By the proposition W = {(y, g(y)) : y ∈Y} = {x ∈Rp :
∥x∥2 ≤r2 and xp ≥0} has volume zero. It follows that a + W and a −W have vol-
ume zero (Why?). Since ∂X = (a + W ) ∪(a −W ), ∂X has volume zero.
■
We might mention that the continuous image of a set of volume zero need not
have volume zero. Indeed look at the preceding proof and examine the continuous
function (y, g(y)) →y from W onto Y. However when the continuous function has
its image in the same Euclidean space the story is sometimes diﬀerent. See Propo-
sition 7.4.2 below.
The next result is crucial for our progress. It also illustrates how assuming X is a
Jordan set can be used to overcome some natural diﬃculties.
7.2.13. Theorem. If X is a Jordan set in Rp and f : X →R is uniformly contin-
uous, then f is integrable.
Proof. As usual X is bounded and R is a rectangle such that X ⊆R; put V = V (R).
Let M > 0 such that | f (x)| ≤M for all x in X. If ϵ > 0, the uniform continuity

7.2 The Riemann Integral
229
of f implies there is a δ > 0 such that | f (x) −f (y)| < ϵ/8V for all x, y in X with
∥x −y∥< δ.
Let S be a R-cover of X such that each rectangle that belongs to R has diameter
smaller than δ. (Do this ﬁrst when p = 2 and then examine the technique in Exam-
ple 7.2(e).) Consider {S ∈S : S ∩∂X ̸= ∅}. This forms a R-cover of ∂X. Since X is
a Jordan set we can ﬁnd a reﬁnement B = {B1, . . . , Bk} of this R-cover of ∂X with
k
i=1 V (Bi) < ϵ/4M. Let T = {R1, . . . , Rm} be a collection of non-overlapping
rectangles such that R = B ∪T is a R-cover of X that reﬁnes S (Exercise 12). Note
that each rectangle in R has diameter smaller than δ.
For 1 ≤i ≤k and 1 ≤j ≤m let MB
i , mB
i , MT
j , mT
j
be deﬁned by MB
i =
sup{ f (x) ∈Bi ∩X}, etc. Because of the choice of δ and the fact that each rectangle
in T has diameter smaller than δ, we get that MT
j −mT
j < ϵ/8V for 1 ≤j ≤m.
Hence
U( f , R) −L( f , R) = [U( f , B) −L( f , B)] + [U( f , T ) −L( f , T )]
=
k

i=1
$
MB
i −mB
i
%
V (Bi) +
m

j=1
?
MT
j −mT
j
@
V (R j)
< 2M
k

i=1
V (Bi) + ϵ
8V
m

j=1
V (R j)
< 2M ϵ
4M + ϵ
8V V
< 5ϵ
8 < ϵ
By Proposition 7.2.4, f is integrable.
■
Also see Exercise 11.
7.2.14. Corollary. If X is a compact Jordan set and f : X →R is a continuous
function, then f is integrable.
The next result and its corollaries will be used later in this chapter.
7.2.15. Proposition. If U and X are Jordan sets with U open, X compact, and
U ⊆X, then X\U is a Jordan set. If f : X →R is a continuous function, then

X
f =

X\U
f +

U
f
Proof. Note that ∂[X\U] = ∂X ∪∂U because U ⊆int X. So ∂[X\U] is the union
of two sets having volume zero and thus has volume zero (Exercise 4). Hence X\U
is a Jordan set.
Now let f : X →R be a continuous function. Since every continuous func-
tion can be written as the diﬀerence of two continuous non-negative functions,
without loss of generality we may assume that f ≥0. Let f (y) ≤M for all x in

230
Integration in Higher Dimensions
X, V = V (X ), and let ϵ > 0. Since X is compact, f is uniformly continuous and
there is a δ > 0 such that | f (x) −f (y)| < ϵ/V when ∥x −y∥< δ. Begin by mak-
ing a R-cover Z1 of X such that the diameter of every rectangle in in Z1 is less
than min{δ, dist (∂X, ∂U)}. Because of this restriction no rectangle in Z1 can meet
both ∂X and ∂U. Let B = {B1, . . . , Bb} be a R-cover of ∂X that reﬁnes {Z ∈Z1 :
Z ∩∂X ̸= ∅} and such that b
i=1 V (Bi) < ϵ/2M. Let C = {C1, . . . ,Cc} be a R-
cover of ∂U that reﬁnes {Z ∈Z1 : Z ∩∂U ̸= ∅} and such that c
i=1 V (Ci) < ϵ/2M.
Now let Z be a R-cover of X that reﬁnes Z1, B, and C. That is, every rectangle in
Z1 ∪B ∪C is the union of rectangles from Z. Note that Z is a R-cover of X and
we still have that each rectangle in Z has diameter less than min{δ, dist (∂X, ∂U)}.
Let E = {E1, . . . Ee} be the rectangles in Z that meet ∂X and let R = {R1, . . . , Rr}
those rectangles in Z that are contained in int X. It follows that Z = E ∪R.
Now let ZU = {Z ∈Z : Z ∩U ̸= ∅}, which is a R-cover of U. Since the closed
set {Z ∈ZU} contains U, it contains clU. Let D = {D1, . . . , Dd} be those rect-
angles in this R-cover that meet ∂U and let S = {S1, . . . , Ss} be those contained in
U. Hence ZU = S ∪D.
Let ZX\U = D ∪E ∪{Z ∈Z : Z ⊆int (X\U)}. Since D = {D1, . . . , Dd} are the
rectangles in Z that meet ∂U and E = {E1, . . . , Ee} those that meet ∂X, this makes
ZX\U a R-cover of X\U. Put T = {Z ∈Z : Z ⊆int (X\U)} = {T1, . . . , Tt}. Hence
ZX\U = D ∪E ∪T .
Now Z = E ∪R = E ∪D ∪S ∪T and E, D, S, and T are pairwise disjoint
collections. Similarly ZU = D ∪S and ZX\U = D ∪E ∪T . Therefore
U( f , Z) = U( f , ZU ) + U( f , ZX\U ) −
d

i=1
MD
i V (Di)
≥

U
f +

X\U
f −
d

i=1
MD
i V (Di)
≥L( f , ZU ) + L( f , ZX\U ) −
d

i=1
[MD
i + mD
j ]V (Di)
= L( f , Z) −
d

i=1
[MD
i + 2mD
j ]V (Di)
Now
U( f , Z) −L( f , Z) ≤2M
e

i=1
V (Ei) +
r

j=1
[MR
j −mR
j ]V (R j)
< 2M ϵ
2M + ϵ
V
r

j=1
V (R j)
< 2ϵ

7.2 The Riemann Integral
231
Also
d

i=1
[MD
i + mD
j ]V (Di) < 3M(ϵ/2M) = 3ϵ
2
So

X f belongs to the interval [L( f , Z),U( f , Z)] and

U f +

X\U f belongs to
the interval [L( f , Z) −3ϵ/2,U( f , Z)]. So the distance between

X f and

U f +

X\U f is at most 2ϵ + 3ϵ/2. Since ϵ was arbitrary, this completes the proof.
■
Also see Exercise 13.
7.2.16. Corollary. If X is a compact Jordan set and f : X →R is a continuous
function, then

X
f =

int X
f
Proof. Let U = int X in the preceding proposition and use the fact that because
X\U = ∂X has volume zero,

X\U f = 0.
■
7.2.17. Corollary.
If U1, . . . ,Un are pairwise disjoint open Jordan sets, U =
n
j=1 Uj, and f : U →R is uniformly continuous, then U is a Jordan set and

U
f =
n

j=1

Uj
f
Proof. It follows that ∂U ⊆n
j=1 ∂Uj and so U is a Jordan set. If n = 2, put X =
clU. Then Proposition 7.2.15 and the preceding corollary imply

U
f =

X
f =

U1
f +

X\U1
f =

U1
f +

U2
f
Now use induction.
■
7.2.18. Corollary. If X1, . . . , Xn are compact Jordan sets such that for 1 ≤i < j ≤
n, Xi ∩Xj = ∂Xi ∩∂Xj, X = n
j=1 Xj, and if f : X →R is a continuous function,
then X is a compact Jordan set and

X
f =
n

j=1

Xj
f
Proof. Since X is the union of a ﬁnite number of compact sets, it is compact. Also
the hypothesis implies that ∂X = n
j=1 ∂Xj; hence X is a Jordan set. Note that the
open sets {int Xj : 1 ≤j ≤n} are pairwise disjoint. Using the preceding results we
have that

X
f =

int X
f =
n

j=1

int Xj
f =
n

j=1

Xj
f
■

232
Integration in Higher Dimensions
We conclude this section with a deﬁnition.
7.2.19. Definition. If X ⊆Rp and the constant function 1 is integrable on X, then
deﬁne the volume of X as
V (X ) =

X
1
We note that this means that in light of Theorem 7.2.13 we can deﬁne the volume
of every Jordan set. Also since each rectangle is a Jordan set (7.2.10) we have just
given a second deﬁnition of the volume of a rectangle. However in the next sec-
tion on iterated integrals we’ll show that the two deﬁnitions are the same (Theorem
7.3.9). The proof of the next result is Exercise 17.
7.2.20. Proposition. (a) If U1, . . . ,Un are pairwise disjoint open Jordan sets and
U = n
j=1 Uj, then V (U) = n
j=1 V (Uj).
(b) If X1, . . . , Xn are compact Jordan sets such that for 1 ≤i < j ≤n, Xi ∩Xj =
∂Xi ∩∂Xj and X = n
j=1 Xj, then V (X ) = n
j=1 V (Xj).
Exercises
(1)
If R and S are two rectangles and S ⊆R, show that V (S) ≤V (R).
(2)
Prove Proposition 7.2.2 using Proposition 3.1.2 as a guide.
(3)
Show that there are open disks { j} in R2 of radius r j having the following prop-
erties: (i) cl  j ⊆D and cl  j ∩cl i = ∅for i ̸= j; (ii) 
j r j < ∞; (iii) K =
cl D\ 
j  j has no interior. The set K is called a Swiss cheese2. Does K have zero
area?
(4)
Prove that the union of a ﬁnite number of sets having volume zero also has volume
zero.
(5)
If X1, . . . , Xn are bounded sets and for 1 ≤j ≤n we are given a R-cover Rj =
{R1
j, . . . , Rmj
j } of Xj, show that there is a R-cover R of X = n
j=1 Xj such that if
R = {R1, . . . , Rq}, then q
k=1 Rk = n
j=1
mj
i=1 Ri
j. Apply this to verify Example
7.2.9(c).
(6)
Can you use Proposition 7.2.11 to give an induction argument that shows that a
rectangle is a Jordan set?
(7)
If X is a bounded subset of Rp−1 and we consider X as a subset of Rp, show that X
has volume zero as a subset of Rp.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
2 The Swiss cheese was ﬁrst discussed by the Swiss mathematician, Alice Roth. She was born in 1905 in Bern,
where she spent her entire life. The history of mathematics has few women as contributors. Indeed it was almost
impossible for a woman to enter higher education until well into the twentieth century. Today this situation has
changed signiﬁcantly and outstanding women mathematicians are numerous. Alice Roth made contributions to
rational approximation of analytic functions of a complex variable. She died in Bern in 1977.

7.3 Iterated Integration
233
(8)
Let Y ⊆X ⊆Rp, g : Y →R, f : X →R. If g is integrable on Y, f (y) = g(y) for
all y inY, and X\Y has volume zero, show that f is integrable on X and

Y g =

X f .
(9)
(a) If X and Y are Jordan sets in Rp and Rq, respectively, then X × Y is a Jordan set
in Rp+q. (b) Use part (a) to give another proof of Proposition 7.2.10.
(10)
Can you use Exercise 3 to manufacture a compact set that has no interior and does
not have volume zero?
(11)
If X is a Jordan set and f : X →R is a uniformly continuous function, show
that for every ϵ > 0 there is a R-cover R = {B1, . . . , Bk, R1, . . . , Rm} of X such
that: (a) B1, . . . , Bk are the rectangles in R that meet ∂X and k
i=1 V (Bi) < ϵ; (b)
R1, . . . , Rm are the rectangles contained in int X and if xj ∈R j for 1 ≤j ≤m, then


X f −m
j=1 f (xj)V (R j)
 < ϵ. (Hint: See the proof of Theorem 7.2.13.)
(12)
Show that the collection of non-overlapping rectangles T in the proof of Theorem
7.2.13 exists.
(13)
Let X1 and X2 be disjoint Jordan sets and assume that f : X1 ∪X2 →R is uniformly
continuous. Is it true that X1 ∪X2 is a Jordan set and

X1∪X2 f =

X1 f +

X2 f ?
(14)
IfV is an open set in Rp, f : V →R is a bounded continuous function, and

X f = 0
for every compact Jordan set X contained in V, show that f (x) = 0 for every x
in V.
(15)
Assume X is a Jordan set and for each n ≥1, fn is an integrable function on X. If
{ fn} converges uniformly on X to a function f , show that f is integrable on X and

X fn →

X f .
(16)
Let X be a Jordan set and assume f is an integrable function on X. If there is a
compact subset K of R with f (X ) ⊆K and g : K →R is continuous, show that
g ◦f is integrable on X.
(17)
Prove Proposition 7.2.20.
7.3. Iterated Integration
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Here we want to examine certain compact Jordan subsets of Rp and show that we
can calculate the integral of a continuous function as we did in calculus by iterating
the integrals; that is by putting together integrals over subsets contained in subspaces
of smaller dimension. To get started, however, we establish a result that doesn’t
involve integration.
7.3.1. Proposition. If X andY are compact subsets of Rp and Rq, respectively, Z is
a compact subset of X × Y, and A is the linear span inC(Z) of the set of restrictions
{g(x)h(y)|Z : g ∈C(X ), h ∈C(Y )}
then A is dense in C(Z).
Proof. This proof is an easy consequence of the Stone–Weierstrass Theorem
(5.7.11). Observe that A is a subalgebra of C(Z) that contains the constant

234
Integration in Higher Dimensions
functions and separates points in Z (Verify!). Hence the same holds for its closure,
so the Stone–Weierstrass Theorem implies cl A = C(Z).
■
7.3.2. Corollary. If X and Y are compact subsets of Rp and Rq, respectively, and
A is the linear span in C(X × Y ) of
{g(x)h(y) : g ∈C(X ), h ∈C(Y )}
then A is dense in C(X × Y ).
7.3.3. Corollary. Let X be a compact subset of Rp contained in the rectangle [a, b].
If A is the linear span of
{ f1(x1) · · · fp(xp) : f j ∈C[a j, b j] for 1 ≤j ≤p}
then A is dense in C(X ).
Proof. This follows from Corollary 7.3.2 by using induction (See Exercise 2) or
you can give a direct argument using the Stone–Weierstrass Theorem.
■
We might also mention that in the deﬁnition of A in Proposition 7.3.1 we could
replace C(X ) and C(Y ) by dense subspaces of these algebras. Consequently in the
statement of the last corollary we could assume that each f j is a polynomial. Now
to return to integration.
7.3.4. Proposition. If X and Y are compact subsets of Rp and Rq, respectively,
Y is a Jordan set, and f : X × Y →R is a continuous function, then the function
F : X →R deﬁned by
F(x) =

Y
f (x, y) dy
is continuous.
Proof. Because Y is a Jordan set it follows from Theorem 7.2.13 that the inte-
gral deﬁning the function F : X →R makes sense. Let R be a rectangle such that
X × Y ⊆R and put V = V (R). Since f must be uniformly continuous, for any
ϵ > 0 there is a δ > 0 such that | f (x1, y1) −f (x2, y2)| < ϵ/V when ∥(x1, y1) −
(x2, y2)∥< δ. Thus when ∥x1 −x2∥< δ
|F(x1) −F(x2)| ≤

Y
| f (x1, y) −f (x2, y)|dy ≤ϵ
V V = ϵ
and the proposition follows.
■
The main reason for proving this last proposition is that, as a consequence, F
is a bounded uniformly continuous function on X and hence integrable if we also
assume that X is a Jordan set. This is needed for the conclusion in the next theorem
to make sense. To do this important theorem the reader should complete Exercise
7.2.9.

7.3 Iterated Integration
235
7.3.5. Theorem (Fubini’s3 Theorem).
If X and Y are compact Jordan sets of
Rp and Rq, respectively, and f : X × Y →R is a continuous function, then

X×Y
f =

X

Y
f (x, y) dy

dx
Proof. Let R and S be rectangles in Rp and Rq such that X ⊆R and Y ⊆S. We
ﬁrst consider the case that f (x, y) = g(x)h(y) where g ∈C(R) and h ∈C(S). Here
we want to show that
7.3.6

X×Y
f =

X
g
 
Y
h

Observe that a R-cover of X × Y can be written as R × S = {R × S : R ∈R, S ∈
S}, where R and S are R-covers of X and Y. (Verify!) Since f is integrable,
Proposition 7.2.4 implies there is a R-cover R × S of X × Y such that U( f , R ×
S) −L( f , R × S) < ϵ. Moreover

X×Y f is the unique number such that whenever
Q and T are reﬁnements of R and S, respectively, then L( f , Q × T ) ≤

X×Y f ≤
U( f , Q × T ). Thus to prove (7.3.6) we need only show that for all such Q and T
7.3.7
L( f , Q × T ) ≤

X
g
 
Y
h

≤U( f , Q × T )
Let’s begin. (This argument is notationally cumbersome but conceptually
straightforward, so be patient.) Let Q = {Q1, . . . , Qm} and T = {T1, . . . , Tt} and
deﬁne the numbers MQ
j (g), mQ
j (g), MT
j (h), mT
j (h) in the usual way. Note that
sup{ f (x, y) : (x, y) ∈Qj × Ti ∩X ×Y} = sup{g(x)h(y) : x ∈Qj ∩X, y ∈Ti ∩Y} =
MQ
j (g)MT
i (h). Similarly inf{ f (x, y) : (x, y) ∈Qj × Ti ∩X × Y} = mQ
j (g), mT
i (h).
Therefore since both g and h are integrable we have that
L( f , Q × T ) = L(g, Q)L(h, T )
≤

X
g
 
Y
h

≤U(g, Q)U(h, T )
= U( f , Q × T )
proving (7.3.7) and hence (7.3.6).
Now that we have the theorem when f (x, y) = g(x)h(y) as in (7.3.6), it is a trivial
matter to extend the theorem to the linear span A of all the functions of the form
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
3 Guido Fubini was born in Venice in 1879. He received his doctorate in 1900 from the University of Pisa, writing
a thesis in diﬀerential geometry. He took up a position at the University of Catania in Sicily. Shortly after
that he was oﬀered a professorship at the University of Genoa and then in 1908 he went to Turin. He began
to change the course of his research, gravitating to analysis. During the 1930s he saw the political situation
in Italy deteriorating with the rise of Mussolini and anti-semitism. He decided the future was looking bleak
for his family, and in 1939, in spite of his deteriorating health, the Fubini family emigrated to New York. His
contributions were in several diﬀerent areas including integral equations, group theory, geometry, continuous
groups, and applied mathematics. He was inﬂuential in the development of mathematics in Italy. He died in
1943 in New York.

236
Integration in Higher Dimensions
g(x)h(y) where g ∈C(R), h ∈C(S). If f is any function in C(X × Y ), then Proposi-
tion 7.3.1 implies there is a sequence { fn} in A that converges uniformly on X × Y
to f . For each n ≥1, set Fn(x) =

Y fn(x, y) dy. So each Fn is in C(X ) (7.3.4).
Claim. Fn(x) →F(x) =

Y f (x, y) dy uniformly on X.
In fact, if ϵ > 0 there is an N such that | fn(x, y) −f (x, y)| < ϵ/V (S) for all
n ≥N and all (x, y) in X × Y. Hence for all n ≥N and all x in X we have that
|Fn(x) −F(x)| ≤

Y | fn(x, y) −f (x, y)| dy < ϵ. This establishes the claim.
By Exercise 7.2.15,

X×Y fn →

X×Y f and

X Fn →

X F. Therefore

X×Y
f = lim
n

X×Y
fn
= lim
n

X

Y
fn(x, y) dy

dx
=

X

Y
f (x, y) dy

dx
completing the proof.
■
Reversing the roles of X and Y in Fubini’s Theorem we get the next corollary,
which is also referred to as Fubini’s Theorem.
7.3.8. Corollary. If X and Y are compact Jordan sets of Rp and Rq and f : X ×
Y →R is a continuous function, then

X×Y
f =

Y

X
f (x, y) dx

dy
It has to be mentioned that Fubini’s Theorem holds in much greater generality
than is stated here. The more general result involves the dramatic extension of inte-
gration from what is presented in this book. See [3].
By using induction, Fubini’s Theorem can yield other results such as the following
one.
7.3.9. Theorem. If R is the rectangle {x ∈Rp : a j ≤xj ≤b j, 1 ≤j ≤p} and f ∈
C(X ), then for any permutation {i1, . . . , ip} of {1, . . . , p}

R
f =
 bi1
ai1
 bi2
ai2
· · ·
 bip
aip
f (x1, . . . , xp)dxip

· · · dxi2

dxi1
The preceding results say something about the concept of the volume of a Jordan
set (7.2.19). First Fubini’s Theorem says that if X is a Jordan set in Rp and Y is a
Jordan set in Rq, then V (X × Y ) = V (X )V (Y ). In particular the last theorem says
the two deﬁnitions of the volume of a rectangle are the same. That is, using the
notation of Theorem 7.3.9,

R 1 = (b1 −a1) · · · (bp −ap).
Now we consider a diﬀerent type of iterated integration, but ﬁrst we need a pre-
liminary result.

7.3 Iterated Integration
237
7.3.10. Proposition. If X is a compact Jordan set in Rp−1 and φ, ψ : X →R are
continuous functions such that φ(x) ≤ψ(x) for all x in X, then
Z = {(x,t) ∈Rp−1 × R : x ∈X,t ∈R and φ(x) ≤t ≤ψ(x)}
is a compact Jordan set in Rp.
Proof. We begin by deﬁning the sets
A = {(x,t) : x ∈∂X and φ(x) ≤t ≤ψ(x)}
B = {(x, φ(x)) : x ∈X}
C = {(x, ψ(x)) : x ∈X}
Claim. ∂Z = A ∪B ∪C
(This argument is a bit cumbersome so you might want to follow along by
assuming p = 2 and drawing a picture.) We start by showing the containment
A ∪B ∪C ⊆∂Z. If (x,t) ∈A and r > 0, consider the ball B((x,t); r) ⊆Rp. Since
x ∈∂X there is a point c in Rp−1 such that c ∈[Rp−1\X] ∩B(x; r). It follows
that (c,t) ∈[Rp\Z] ∩B((x,t); r). Thus (x,t) ∈∂Z and we have that A ⊆∂Z.
Now assume that (x, φ(x)) ∈B and r > 0. If φ(x) −r < t < φ(x), then (x,t) ∈
(Rp\Z) ∩B((x, φ(x)); r); hence B ⊆∂Z. Similarly C ⊆∂Z.
Now assume (x,t) ∈∂Z and let’s show that (x,t) ∈A ∪B ∪C. Since ∂Z is a
part of Z, we have that x ∈X and φ(x) ≤t ≤ψ(x). Assume that x ∈int X and that
φ(x) < t < ψ(t). Let r > 0 and δ > 0 such that B(x; r) ⊆int X and φ(x) < t −δ <
t + δ < ψ(x). It follows that U = B(x; r) × (t −δ,t + δ) is an open set contained
in Z, contradicting the fact that (x,t) ∈∂Z. Thus either x ∈∂X or t = φ(x) or t =
ψ(x). In the ﬁrst case, (x,t) ∈A; in the second, (x,t) ∈B; in the third, (x,t) ∈C,
thus establishing the claim.
According to Proposition 7.2.11, B and C have volume zero. If we can show that
A has volume zero, it follows from Exercise 7.2.4 that Z is a Jordan set. To show
that A has volume zero ﬁrst observe that since φ and ψ are continuous functions
on a compact set there are real numbers a and b such that a ≤φ(x) ≤ψ(x) ≤b
for all x in X. Thus A ⊆∂X × [a, b]; we show that ∂X × [a, b] has volume zero.
If ϵ > 0 there is a R-cover R = {R1, . . . , Rm} of ∂X such that m
j=1 R jV (R j) <
ϵ/(b −a). It follows that {R1 × [a, b], . . . , Rm × [a, b]} is a R-cover of ∂X × [a, b]
and the sum of the volumes of these rectangles is less than ϵ.
■
7.3.11. Theorem. If X is a compact Jordan set in Rp−1 and φ, ψ : X →R are
continuous functions such that φ(x) ≤ψ(x) for all x in X, and
Z = {(x,t) ∈Rp−1 × R : x ∈X,t ∈R and φ(x) ≤t ≤ψ(x)}
then for any continuous function f : Z →R

Z
f =

X
 ψ(x)
φ(x)
f (x,t) dt

dx

238
Integration in Higher Dimensions
Proof. First we show that if we deﬁne
F : X →R
as F(x) =
 ψ(x)
φ(x) f (x,t) dt, then F is continuous and so the integral in the statement
of the theorem is legitimate. Let xn →x in X and observe that
F(xn) −F(x) =
 ψ(xn)
φ(xn)
f (x,t) dt −
 ψ(x)
φ(x)
f (x,t) dt
=
 ψ(x)
φ(xn)
f (x,t) dt +
 ψ(xn)
ψ(x)
f (x,t) dt −
 ψ(x)
φ(x)
f (x,t) dt
=
 φ(x)
φ(xn)
f (x,t) dt +
 ψ(x)
φ(x)
f (x,t) dt
+
 ψ(xn)
ψ(x)
f (x,t) dt −
 ψ(x)
φ(x)
f (x,t) dt
=
 φ(x)
φ(xn)
f (x,t) dt +
 ψ(xn)
ψ(x)
f (x,t) dt
Let | f (x,t)| ≤M for all (x,t) in Z and let ϵ > 0. We can choose N such that when
n ≥N, |φ(xn) −φ(x)| < ϵ/2M and |ψ(xn) −ψ(x)| < ϵ/2M. From the preceding
equation we get that |F(xn) −F(x)| < ϵ when n ≥N.
Let [a, b] be an interval such that a ≤φ(x) ≤ψ(x) ≤b for all x in X, let S be
a rectangle in Rp−1 that contains X, and let R = S × [a, b]. Deﬁne f : R →R
by setting f (x,t) = f (x,t) when (x,t) ∈Z and f (x,t) = 0 otherwise.
Claim. f is integrable on R and

Z f =

R f =

S
? b
a f (x,t) dt
@
dx
To prove the ﬁrst part of the claim we let ϵ > 0 and invoke Exercise 7.2.11 to
ﬁnd a R-cover R = {B1, . . . , Bk, R1, . . . , Rm} of Z such that the following two con-
ditions hold: (a) B1, . . . , Bk are the rectangles in R that meet ∂Z and k
i=1 V (Bi) <
ϵ/2M; (b) R1, . . . , Rm are the rectangles contained in int X and if (xj,t j) ∈R j for
1 ≤j ≤m, then


Z f −m
j=1 f (xj,t j)V (R j)
 < ϵ/2. Let T = R ∪{T1, . . . , Tn},
where T1, . . . , Tn are rectangles contained in R chosen such that T is a R-cover
of R. It follows that f vanishes on each Tk since it is disjoint from Z. Thus
U(f , T ) −L(f , T ) = U( f , R) −L( f , R) < ϵ. It follows that f is integrable and

Z f =

R f . Though the function f is not continuous on R we can adapt the proof
of Fubini’s Theorem to complete the proof of the claim. The proof is left to the
reader in Exercise 7.
Using the claim we get

Z
f =

R
f =

S
 b
a
f (x,t) dt

dx =

X
 ψ(x)
φ(x)
f (x,t) dt

dx
■

7.4 Change of Variables
239
7.3.12. Corollary. Assume [c, d] ⊆R and φ, ψ : [c, d] →R are continuous func-
tions such that φ(x) ≤ψ(x) for all x in [c, d]. If
X = {(x,t) ∈R2 : c ≤x ≤d, φ(y) ≤t ≤ψ(y)}
and f : X →R is a continuous function, then

X
f =
 d
c
 ψ(x)
φ(x)
f (x,t) dt

dx
Exercises
(1)
Show that Proposition 7.3.1 can be strengthened a little by replacing C(X ) and
C(Y ) by dense subalgebras of these two spaces.
(2)
Give the details needed to prove Corollary 7.3.3.
(3)
If R = [0, 2] × [−1, 1] and f (x, y) = xy2, ﬁnd

R f .
(4)
If R = [a1, b1] × [a2, b2] × [a3, b3], evaluate

R f for the following examples of the
function f (x, y, z): (a) f (x, y, z)=x + y + z; (b) f (x, y, z)=xyz; (c) f (x, y, z) = x.
(5)
Find

X f in each of the following cases: (a) f (x, y) = x + y, X is the trian-
gle with vertices (0, 0), (0, 1), (1, 0); (b) f (x, y) = sin x2, X = {(x, y) : 0 ≤y ≤1,
y ≤x ≤1}.
(6)
Find

X f in each of the following cases: (a) f (x, y, z) =
√
x3 + z, X = {(x, y, z) :
x3 ≤z ≤1, √y ≤x ≤1, 0 ≤y ≤1}; (b) f (x, y, z) = x, X = {(x, y, z) : 0 ≤z ≤
1 −x2, 0 ≤y ≤x2 + z2, x ≥0}.
(7)
Establish the claim in the proof of Theorem 7.3.11.
7.4. Change of Variables
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Recall the Change of Variables Theorem for integration on intervals [a, b] in R
(Theorem 3.2.5). There we are given a continuously diﬀerentiable function φ :
[a, b] →R and an interval I such that φ([a, b]) ⊆I. If f : I →R is a continuous
function, then
 φ(b)
φ(a)
f (x) dx =
 b
a
f (φ(t))φ′(t) dt
In this section we’ll see an extension of this result to the integral in Rp. The proof
of this theorem is very involved and necessitates some preliminary work and sev-
eral lemmas. We begin by showing that under suitable restrictions the image of a
Jordan set is again a Jordan set, but establishing this also requires a few preliminary
results.
Let’s set some notation. We are given an open subset G of Rp and a continu-
ously diﬀerentiable function  : G →Rp. For each of the standard basis elements

240
Integration in Higher Dimensions
ei in Rp let i : G →R be deﬁned by i(x) = ⟨(x), ei⟩. The statement of our ﬁrst
lemma is highly technical, but many lemmas are that way.
7.4.1. Lemma. Let R be a rectangle in Rp whose shortest side has length s, whose
longest side has length ℓ, and assume these lengths satisfy ℓ≤2s. If G is an open
set containing R,  : G →Rp is a continuously diﬀerentiable function, and there
is a constant M such that |∂x ji(x)| ≤M for all 1 ≤i, j ≤p and all x in G, then
there is a rectangle Q in Rp such that
(R) ⊆Q and V (Q) ≤(2pM)pV (R)
Proof. Let x0 be the center point of R. If x ∈R, consider the straight line from x0 to
x: γ (t) = tx + (1 −t)x0. By the Mean Value Theorem 6.2.15, for 1 ≤i ≤p there
is a point ti in [0, 1] such that
i(x) −i(x0) = ⟨∇i(γ (ti)), x −x0⟩
By hypothesis for each i, ∥∇i(γ (ti))∥≤p
j=1 |∂x ji(γ (ti))| ≤pM. In addition
∥x −x0∥≤ℓ/2. Thus we have that |i(x) −i(x0| ≤|⟨∇i(γ (ti)), x −x0⟩| ≤
∥∇i(γ (ti))∥∥x −x0∥≤pMℓ/2; so the image of the rectangle R under the scalar-
valued function i is contained in the interval with center i(x0) and length pMℓ.
That is,
i(R) ⊆[i(x0) −pMℓ/2, i(x0) + pMℓ/2]
Therefore if we let Q be the rectangle in Rp with center (x0) and with each side of
length pMℓ, we have (R) ⊆Q and V (Q) = (pMℓ)p. Now since s ≥ℓ/2, V (R) ≥
sp ≥(ℓ/2)p. Hence
V (Q) = (2pM)p(ℓ/2)p ≤(2pM)pV (R)
as we wanted to show.
■
Let’s point out that in the preceding lemma the existence of the constant M is not
a formidable restriction. In fact R is a compact subset of G and so we can always
ﬁnd an open set G1 such that R ⊆G1 ⊆cl G1 ⊆G and cl G1 is compact (Exercise
5.5.8). Since  is continuously diﬀerentiable, the constant M exists for the open set
G1.
7.4.2. Proposition. If X is a compact subset of Rp having volume zero, G is an
open set containing X, and  : G →Rp is a continuously diﬀerentiable function,
then (X ) has volume zero.
Proof. From the comments preceding the statement of this proposition, we may
assume that there is a constant M such that |∂x ji(x)| ≤M for all 1 ≤i, j ≤p and
all x in G as in the hypothesis of Lemma 7.4.1. Let ϵ > 0. Since X has volume zero
there is a R-cover R = {R1, . . . , Rm} of X such that m
k=1 V (Rk) < ϵ/(2pM)p. By
Exercise 1 we can write each Rk as the union of non-overlapping rectangles having
the property that the length of the longest side of each is at most twice the length
of its shortest side. But the volume of each Rk is the sum of the volumes of these

7.4 Change of Variables
241
smaller rectangles. Hence, without loss of generality, we can assume that each Rk
in the R-cover R has the property that the length of its longest side is at most twice
the length of its shortest side. By the preceding lemma, for 1 ≤k ≤m there is a
rectangle Qk in Rp such that (Rk) ⊆Qk and V (Qk) ≤(2pM)pV (Rk). Hence
m

k=1
V (Qk) < (2pM)p
m

k=1
V (Rk) < ϵ
Since (X ) ⊆m
k=1 (Rk) ⊆m
k=1 Qk, we have that (X ) has volume zero.
■
We want a result that gives conditions on a function  : G →Rp such that when
X is a Jordan set contained in G, (X ) is a Jordan set. If we knew that ∂(X ) ⊆
(∂X ), the preceding proposition yields the result. Corollary 6.8.5, a consequence
of the Inverse Function Theorem, gives us what we need.
7.4.3. Proposition. If X is a compact Jordan set in Rp, G is an open set containing
X, and  : G →Rp is a continuously diﬀerentiable function such that D(x) is
invertible for every x in int X, then (X ) is a Jordan set.
Proof. First observe that (X ) is compact. If x ∈int X, let U be an open set such
that x ∈U ⊆X. By Corollary 6.8.5, (U) is open. Since (U) ⊆(X ), it must be
that (U) ⊆int (X ). Hence (int X ) ⊆int (X ). If ζ ∈∂(X ) ⊆(X ), let x ∈
X such that (x) = ζ. By what we just proved, it cannot be that x ∈int X; thus x ∈
∂X and we have that ∂(X ) ⊆(∂X ). But (X ) has volume zero by Proposition
7.4.2 and the fact that X is a Jordan set. Therefore (X ) is a Jordan set.
■
7.4.4. Theorem (Change of Variables Theorem). Let X be a compact Jordan set in
Rp. If G is an open subset of Rp such that X ⊆G and  : G →Rp is a continuously
diﬀerentiable function such that  is injective on int X with det[D(x)] ̸= 0 for all
x in int X, then for any continuous function f : (X ) →R
7.4.5

(X )
f =

X
f ◦ |det[D]|
Note that according to Proposition 7.4.3, (X ) is a Jordan set so that the ﬁrst inte-
gral in (7.4.5) makes sense. We might compare this result to the previously quoted
result for p = 1 and note the diﬀerences. The ﬁrst thing that stands out is the abso-
lute value around the det[D] as opposed to not having one around φ′ in the case
when p = 1. This is due to the fact that in one-dimensional integration there is a nat-
ural direction involved in the integral and this does not happen when p > 1. Since
φ′(x) ̸= 0 on the interval [a, b] we have either φ′(x) > 0 for all x or φ′(x) < 0 for
all x. In the latter case the eﬀect is to use |φ′(x)| in the equation but switch the lim-
its of integration. So we could write the COV Theorem when p = 1 as saying that
when φ′(x) ̸= 0 on [a, b] then

[φ(a),φ(b)]
f =

[a,b]
f ◦φ |φ′|

242
Integration in Higher Dimensions
and so the theorem above is indeed a generalization of the COV Theorem we proved
in Chapter 3.
We still need some more lemmas. The next one can be phrased as saying that if
the Change of Variables Theorem is true for every rectangle, it is true for an arbitrary
compact Jordan set. The proof is rather technical so be careful.
7.4.6. Lemma. Assume X, G, and  are as in the statement of Theorem 7.4.4.
If for every closed rectangle R contained in int X and every continuous function
f : (X ) →R we have that

(R)
f =

R
f ◦ |det[D]|
then (7.4.5) is valid.
Proof. We begin the proof by making some simplifying assumptions that do noth-
ing to interfere with the generality of the situation. First, since any continuous func-
tion can be written as the diﬀerence of two positive continuous functions and (7.4.5)
remains valid for the diﬀerence of two functions, without loss of generality we can
assume that f ≥0 on (X ). Deﬁne a new function g(x) = f ((x))|det[D(x)]|
for all x in X, which we observe is also positive on X. We can also assume that G is
a bounded Jordan set with  deﬁned and continuously diﬀerentiable in an open set
that contains cl G. To see this ﬁrst realize that since X is compact and contained in
an open set, there is a r > 0 such that Xr = {x ∈Rp : dist (x, X ) ≤r} ⊆G (Exer-
cise 5.5.7). Let Z be a R-cover of Xr, where each each rectangle in Z is contained
in G. Replace G by int [{Z : Z ∈Z}], a bounded open Jordan set. (We needed the
bigger set Xr to be sure that X is contained in the replacement open set.) With this
replacement we have that  is deﬁned in a neighborhood of cl G = {Z : Z ∈Z}.
Since G is a Jordan set we can deﬁne V = V (G).
Because (X ) and X are both compact, we have that both f and g are bounded
on these domains. Also since  and its derivatives are continuous on cl G we have
that there is a constant M such that
f (ζ ) ≤M for ζ ∈(X )
g(x) ≤M for x ∈X
|∂x ji(x)| ≤M for x ∈cl G, 1 ≤i, j ≤p
Let ϵ > 0.
Claim. There is a R-cover
R = {B1, . . . , Bb, R1, . . . , Rr}
of X, where {B1, . . . , Bb} meet ∂X, {R1, . . . , Rr} are contained in int X, and the
following hold: (a) the longest side of each rectangle in R is less than twice the
length of its shortest side; (b)
b

i=1
V (Bi) < ϵ min
&
1
M(2pM)p, 1
M
'

7.4 Change of Variables
243
To verify this claim we use methods we have used before and so the details are
left to the interested reader. Suﬃce it to say that (a) follows from Exercise 1 and (b)
is a consequence of the fact that ∂X has volume zero.
Let
 =
r
j=1
(R j)
and
 = (X )\
Note that (R j) is a Jordan set by Proposition 7.4.3. Also the Inverse Function The-
orem and its corollary imply that ∂(Rj) = (∂R j). By the corollaries of Theorem
7.2.13, in particular Corollary 7.2.18, we have that

(X )
f =


f +


f
=
r

j=1

(R j)
f +


f
If we set K = r
j=1 R j ⊆int X, the hypothesis implies
r

j=1

(R j)
f =
r

j=1

R j
f ◦|det[D]| =

K
f ◦|det[D]|
Lemma 7.4.1 implies that for each rectangle Bi there is a rectangle Qi such that
(Bi) ⊆Qi and V (Qi) ≤(2pM)pV (B j). Now
 = (X )\ ⊆
b

i=1
(Bi) ⊆
b

i=1
Qi
so


f ≤
b

i=1

Qi
f
≤
b

i=1
MV (Qi)
≤M
b

i=1
(2pM)pV (B j)
< M(2pM)p
ϵ
M(2pM)p = ϵ
Therefore


(X )
f −

K
f ◦|det[D]|
 < ϵ

244
Integration in Higher Dimensions
On the other hand

X g =

K g +

X\K g. But X\K ⊆b
i=1 Bi and so

X\K
g ≤
b

i=1

Bi
g ≤M
b

i=1
V (Bi) < M ϵ
M = ϵ
Thus


X
f ◦|det[D]| −

K
f ◦|det[D]|
 < ϵ
Therefore


X
f ◦|det[D]| −

(X )
f
 < 2ϵ
Since ϵ was arbitrary this completes the proof.
■
Now we parlay the preceding lemma to show that if we prove the COV Theorem
locally, then we have a proof of the general theorem.
7.4.7. Lemma. Assume X, G, and  are as in the statement of Theorem 7.4.4. If
for every point x in int X there is an open ball B(x; r) such that B(x; r) ⊆int X with
the property that for any Jordan region Y contained in B(x; r) and any continuous
function f : (Y ) →R we have that

(Y )
f =

Y
f ◦ |det D|
then (7.4.5) is valid.
Proof. We only sketch the proof as it follows lines similar to previous proofs. The
reader is asked in Exercise 3 to supply the details. Let f : (X ) →R be a contin-
uous function. To show that (7.4.5) is valid, the previous lemma says we need only
show that

(R) f =

R f ◦ |det D| for every rectangle R ⊆int X. Let B(x; r) be
an open ball as in the hypothesis, and let Rx be a rectangle that contains x in its inte-
rior and such that Rx ⊆B(x; r). Since the rectangle R is compact we can ﬁnd a ﬁnite
number of points x1, . . . , xm such that R is covered by int Rx1, . . . , int Rxm. Now ﬁnd
non-overlapping rectangles R1, . . . , Rn such that R = n
j=1 R j and each R j is con-
tained in one of the rectangles Rxi. Note that for 1 ≤j, k ≤n, 0 = V (R j ∩Rk) =
V ((R j) ∩(Rk)). Hence

(R)
f =
n

j=1

(R j)
f
=
n

j=1

R j
f ◦ |det D|
=

R
f ◦ |det D|
By the preceding lemma this completes the proof.
■

7.4 Change of Variables
245
The next phase of the proof consists of showing that the theorem is true when
 is a special type of function. After this we’ll show that these special maps can be
combined to get any eligible  and then we will complete the proof.
7.4.8. Definition. If G is an open subset of Rp and H : G →Rp, then H is called
a simple mapping if there is an integer k with 1 ≤k ≤p and a function h : G →R
such that for all x in G
H(x) = h(x)ek +

j̸=k
xjej = x + [h(x) −xk]ek
Thus a simple function is one that disturbs only one coordinate. Notice that
the continuity or diﬀerentiability of H is determined by that of h. If H is dif-
ferentiable at a point a in G then the matrix of H′(a) = (aij) has the follow-
ing properties. When i ̸= k, the i-th row of (aij) has a one in the i-th spot
and zeros elsewhere. That is aij = 0 when j ̸= i and aii = 1. The k-th row has
entries ∂x1h(a), . . . , ∂xkh(a), . . . , ∂xph(a). So the Jacobian determinant takes on the
form
det[H′(a)] = ∂xkh(a)
Hence the simple function H has H′(a) invertible if and only if ∂xkh(a) ̸= 0.
7.4.9. Lemma. The Change of Variables Theorem is true if we assume that  is a
simple function.
Proof. By Lemma 7.4.6 it suﬃces to prove this when X is a rectangle [a, b].
To simplify the notation we assume that the simple function  has the form
(x) = φ(x)e1 + p
j=2 xjej; that is, in the above deﬁnition we take k = 1. Let
R = [a2, b2] × · · · × [ap, bp], the rectangle in Rp−1 such that X = [a1, b1] × R. As
we pointed out, det[′(x)] = ∂x1φ(x) and the hypothesis guarantees ∂x1φ(x) ̸= 0
for all x in X. X being a rectangle and hence connected, it must be that ∂x1φ(x) is
either always positive or always negative; we will assume that ∂x1φ(x) > 0 for all x
in X. (The reader can carry out the similar proof in the other case.) We have that
(X ) = (φ([a1, b1])) × R. Hence by Fubini’s Theorem
7.4.10

(X )
f =

(φ([a1,b1]))×R
f =

R

φ([a1,b1])
f (x1) dx1

dx2 · · · dxp
and
7.4.11
X
f ◦|det[′]| =

R
 b1
a1
f (φ(x), x2, . . . , xp)∂x1φ(x) dx1

dx2 · · · dxp
For the moment ﬁx (x2, . . . , xp) in R and deﬁne ψ : [a1, b1] →R by ψ(t) =
φ(t, x2, . . . , xp). It follows that ψ′(t) = ∂x1φ(t, x2, . . . , xp). So by the COV

246
Integration in Higher Dimensions
Theorem for one variable (3.2.5) we get that
 b1
a1
f (φ(x), x2, . . . , xp)∂x1φ(x) dx1 =
 b1
a1
f (ψ(t))ψ′(t) dt
=
 ψ(b1)
ψ(a1)
f
=

φ([a1,b1])
f (x1) dx1
If we substitute this into (7.4.10) and (7.4.11) we see that the proof is complete. ■
7.4.12. Definition. A linear transformation in L(Rp) is called a ﬂip if it inter-
changes two elements of the standard basis and leaves the others ﬁxed.
So a T in L(Rp) is a ﬂip if there are distinct i and j, 1 ≤i, j ≤p, such that Tei =
e j, Tej = ei, and Tek = ek when k ̸= i, j. Notice that if T is a ﬂip, then T 2 = 1. We
observe that with (x) = T x, ′(x) = T and so |det[′(x)]| = |det T | = 1.
7.4.13. Lemma. The Change of Variables Theorem is true if we assume that  is
a ﬂip T .
Proof. Again we need only prove this for a rectangle. As in the proof of Lemma
7.4.9 let X = [a, b] and, to simplify the notation, we assume that T ﬂips the ﬁrst
two basis vectors. So Te1 = e2, Te2 = e1, and Tek = ek for 3 ≤k ≤p. Let R =
[a3, b3] × · · · × [ap, bp], the rectangle in Rp−2 such that X = [a1, b1] × [a2, b2] ×
R. Thus T (X ) = [a2, b2] × [a1, b1] × R, and

T (X )
f =

R
 b1
a1
 b2
a2
f (s,t, x3, . . . , xp) ds

dt

dx3 · · · dxp
and

X
f ◦T |det[T ]| =

R
 b1
a1
 b2
a2
f (t, s, xs, . . . , xp) dt

ds

dx3 · · · dxp
Fubini’s Theorem shows these two integrals are the same.
■
7.4.14. Lemma. If G is an open subset of Rp that contains the origin,  : G →Rp
is continuously diﬀerentiable with (0) = 0 and D(0) invertible, then there is a
neighborhood W of 0 such that for all x in W
7.4.15
(x) = T1 · · · Tp−1 ◦Hp ◦· · · H1(x)
where: (i) each Tj is either a ﬂip or the identity linear transformation; (ii) each Hj
is a simple mapping satisfying Hj(0) = 0 and H′
j(0) is invertible.
Proof. We start by introducing the linear projections P0, . . . , Pp on Rp, where for
all x in Rp: P0(x) = 0 and Pk(x) = x1e1 + · · · + xkek. So Pk projects Rp onto the
subspace deﬁned by the ﬁrst k members of the standard basis.

7.4 Change of Variables
247
Claim. For 1 ≤m ≤p there is a neighborhood of 0, Wm, and a continuously diﬀer-
entiable function m : Wm →Rp such that m(0) = 0, Dm(0) is invertible, and
for all x in Wm
Pm−1m(x) = Pm−1(x)
We show this by induction. Take 1 to be the given function  with W1 = G.
Since P0 = 0, this works. Now assume 1 ≤m < p and that we have the function
m and neighborhood Wm as in the statement of the claim. From the equation
Pm−1m(x) = Pm−1(x) it follows that for each x in Wm we have
m(x) = Pm−1(x) +
p

j=m
h j(x)ej
for some scalar-valued functions hm, . . . , hp deﬁned on Wm. Since m is continu-
ously diﬀerentiable, so are each of the functions hj. Thus
Dm(0)em =
p

j=m
[∂xmh j](0)ej
Because Dm(0) is invertible, there must be a ﬁrst integer q, m ≤q ≤p, with
[∂xmhq](0) ̸= 0. Fix such a q and let Tm be the ﬂip that interchanges this eq and
em. (If q = m, then Tm = 1.) Deﬁne Hm : Wm →Rp by
Hm(x) = x + [hq(x) −xm]em
It follows that Hm is a simple function on Wm that is continuously diﬀerentiable and
DHm(0) is invertible. (Verify!) Since m(0) = 0 it follows that hq(0) = 0 so that
Hq(0) = 0. We apply the Inverse Function Theorem to Hm and conclude the follow-
ing: (a) there is an open set Um with 0 ∈Um ⊆Wm; (b) Hm is injective on Um and
Hm(Um) = Wm+1, an open set containing 0; (c) H−1
m
: Wm+1 →Um is continuously
diﬀerentiable.
Deﬁne m+1 : Wm+1 →Rp for all y in Wm+1 by
7.4.16
m+1(y) = Tmm ◦H−1
m (y)
Clearly m+1 is continuously diﬀerentiable with m+1(0) = 0. By the Chain Rule
we have that Dm+1(0) = Tm[Dm(H−1
m (0))]D[H−1
m ](0) = Tm[Dm(0)]D[H−1
m ](0),
so that Dm+1(0) is invertible. Finally, if x ∈Um,
Pmm+1(Hm(x)) = PmTmm(x)
= Pm[Pm−1x + hq(x)em + · · · ]
= Pm−1x + hq(x)em
= PmHm(x)
Hence Pmm+1(y) = Pmy for all y in Wm+1, establishing the claim.
Finishing the proof of the proposition now goes quickly. Revisit (7.4.16) with
y = Hm(x) for some x in Um and use the fact that T 2
m = 1 to conclude that

248
Integration in Higher Dimensions
Tmm+1(Hm(x)) = T 2
mm(x) = m(x). Applying this for successive values of m =
1, . . . , p shows that in some neighborhood of 0 we have that
 = 1 = T12 ◦H1 =, T1T23 ◦H2 ◦H1 = · · · = T1 · · · Tp−1p ◦Hp−1 ◦· · · ◦H1
But according to the claim we have that Pp−1p(x) = Pp−1(x). Now Pp−1 projects
Rp onto the subspace deﬁned by the ﬁrst p −1 members of the standard basis. This
means that there is a function g deﬁned in a neighborhood of 0 such that p(x) =
g(x)ep + 
j<p xjej so that p is a simple function.
■
Proof of the Change of Variables Theorem. By Lemma 7.4.7 we need only show
that for any point x0 in int X there is a neighborhood of x0 on which (7.4.5) is valid.
If we replace G by W = G −x0 and  by 1(x) = (x + x0) −(x0), we note
that 0 ∈W, 1(0) = 0, and D1(0) is invertible. Hence it suﬃces to assume that
x0 = 0 ∈G, (0) = 0, and D(0) is invertible.
Combine the preceding lemma with the fact that the theorem is valid if  is a
simple function (Lemma 7.4.9) as well as if  is a ﬂip (Lemma 7.4.13), and we see
that we need only show that when the theorem is true for functions  and , then
it is true for  =  ◦. In this case we have that

(X )
f =

((X ))
f
=

(X )
f ◦|det[D]|
=

X
[( f ◦) ◦] |det[D] ◦||det[D]|
=

X
f ◦|det[D]|
where the last equality is from the Chain Rule and the multiplicativity of the
determinant.
■
7.4.17. Proposition. If T ∈L(Rp) and T is invertible, then for any rectangle R in
Rp,
V (T (R)) = |det T |V (R)
Proof. This is immediate from the Change of Variables Theorem and the fact that
if  : Rp →Rp is deﬁned by (x) = T (x), then D(x) = T for all x in Rp.
■
7.4.18. Example. [Polar Coordinates] (a) Deﬁne  : R2 →R2 by (r, θ) =
(r cos θ, r sin θ). A simple computation shows that det D(r, θ) = r. Thus if we
have any compact Jordan set X in the plane such that int X does not contain the
origin and  is injective on int X and if f : X →R is continuous, then

(X )
f (x, y) d(x, y) =

X
f (r cos θ, r sin θ)r d(r, θ)

7.4 Change of Variables
249
Note that for  to be injective on int X it must be that if (r, θ1), (r, θ2) ∈int X, then
0 < |θ1 −θ2| < 2π.
(b) Let Z = {(x, y) ∈R2 : 1 ≤x2 + y2 ≤4} and evaluate

Z(x2 + y)dxdy. If we
let X = {(r, θ) : 1 ≤r ≤2, 0 ≤θ ≤2π} and deﬁne  as in (a), then Z = (X )
and, on int X,  is injective and det[D] ̸= 0. Using Fubini’s Theorem we get that

Z
(x2 + y) d(x, y) =

X
(r2 cos2 θ + r sin θ)r d(r, θ)
=
 2
1
r2
 2π
0
(r cos2 θ + sin θ)d θ

dr
= 15π
4
See Exercise 4.
7.4.19. Example. [Spherical Coordinates] (a) Denote the origin in R3 by O =
(0, 0, 0). For any point P in R3 let Q be the projection of P onto the xy-plane.
We associate the spherical coordinates (r, θ, φ) as follows: r is the distance of P
to the origin; θ is the angle from the positive x-axis to the line segment OQ; φ is
the angle from the positive z-axis to the line OP. Using these quantities we get that
(x, y, z) = (r sin φ cos θ, r sin φ sin θ, r cos φ). We deﬁne
(r, θ, φ) = (r sin φ cos θ, r sin φ sin θ, r cos φ)
A computation shows that
D(r, θ, φ) = ∂(x, y, z)
∂(r, θ, φ) =
⎡
⎣
sin φ cos θ −r sin φ sin θ r cos φ cos θ
sin φ sin θ r sin φ cos θ r cos φ sin θ
cos φ
0
−r sin φ
⎤
⎦
and so det D(r, θ, φ) = −r2 sin φ. Hence the Change of Variables Theorem
applies to any Jordan set X such that r2 sin φ ̸= 0 on int X.
(b) Find the volume of a sphere in R3. The volume of a sphere of radius a is
1
8V (Z), where Z = {(x, y, z) : x ≥0, y ≥0, z ≥0, and x2 + y2 + z2 ≤a2}. If we
deﬁne X = {(r, θ, φ) : 0 ≤r ≤a, 0 ≤θ ≤π/2, 0 ≤φ ≤π/2}, then the function
 in (a) maps X onto Z. Using the Change of Variables Theorem and Fubini’s
Theorem we have that
V (Z) =

Z
1 =

X
r2 sin φ d(r, θ, φ)
=
 a
0
r2
 π/2
0
 π/2
0
sin φ d φ

dθ

dr
= πa3
6
Therefore the volume of the sphere is 4πa3/3.

250
Integration in Higher Dimensions
Exercises
(1)
If R is any rectangle, show that R can be written as the union of a ﬁnite number of
non-overlapping rectangles {R1, . . . , Rm} such that each Rk has the property that its
longest side is at most twice the length of its shortest side
(2)
In the proof of Lemma 7.4.6 show that


X\Y f
 < ϵ.
(3)
Supply the details of the proof of Lemma 7.4.7.
(4)
Give the details of the calculation in Example 7.4.18(b).
(5)
Let X be the set in R2 bounded by the x-axis and the curve r = 1 −cos θ where
0 ≤θ ≤π; ﬁnd

X y.
(6)
Find the area of the following subsets X of R2. (a) X = {(x, y) : y ≤x ≤4y, 1 ≤
x + 2y ≤3}. (b) {(x, y) : 2 ≤xy ≤4, 2x ≤y ≤5x}.
(7)
Let X be the top half of the unit ball in R3 and ﬁnd

X 16z.
(8)
Let X be the sphere in R3 centered at the origin with radius 3. When f (x, y, z) =
10 −x2 −y2 −z2, show that

X f = 828π/5.
7.5. Differentiation under the Integral Sign
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Here we want to explore the idea of the diﬀerentiation of functions that are deﬁned
by an integral. The proof of the main result (7.5.2) is technically complicated
because there are so many things to keep track of. Instead of starting with that we
will begin with a special case which is a corollary of the main result. We do this
because the proof of this special case is straightforward and it is this result that
seems to be used more frequently.
7.5.1. Theorem.
If [a, b], [c, d] ⊆R, G is an open subset of R2 that contains
[a, b] × [c, d], and f : G →R is bounded and continuously diﬀerentiable, then
d
dx
 d
c
f (x, y)dy

=
 d
c
∂1 f (x, y)dy
Proof. We only give the proof for the case that x ∈(a, b). Set F(x) =
 d
c f (x, y)dy
so that F : [a, b] →R. Fix x in (a, b) and let δ1 > 0 such that [x −δ1, x + δ1] ⊆
(a, b). Put K equal to the rectangle [a, b] × [c, d]. Note that since ∂1 f is continuous,
it is uniformly continuous on K. Thus we can ﬁnd δ2 > 0 such that
∂1 f (w, y) −
∂1 f (w′, y′)
 < ϵ/2(d −c) when ∥(w, y) −(w′, y′)∥< δ2. Also the MVT applied
to ∂1 f says that for any y and t with 0 < |t| < δ1 there is a x1 between x and x + t
with

f (x + t, y) −f (x, y)
t
−∂1 f (x1, y)
 <
ϵ
2(d −c)

7.5 Differentiation under the Integral Sign
251
We note that x1 depends on x, y, and t. Let 0 < |t| < δ = min{δ1, δ2}. Note
that for any y in [c, d], ∥(x, y) −(x1, y)∥= |x −x1| < δ ≤δ2 so that |∂1 f (x, y) −
∂1 f (y, x1)| < ϵ/2(d −c). Hence for 0 < |t| < δ and any y in [c, d] we have
that

f (x + t, y) −f (x, y)
t
−∂1 f (x, y)

≤

f (x + t, y) −f (x, y)
t
−∂1 f (x1, y)
 + |∂1 f (x1, y) −∂1 f (x, y)|
<
ϵ
2(d −c) +
ϵ
2(d −c)
=
ϵ
d −c
Therefore when 0 < |t| < δ, we have that

F(x + t) −F(x)
t
−
 d
c
∂1 f (x, y)dy

≤
 d
c

f (x + t, y) −f (x, y)
t
−∂1 f (x, y)
 dy
< ϵ
Since ϵ was arbitrary, this completes the proof.
■
Now we present a more general result.
7.5.2. Theorem. Consider [a, b] in R and suppose φ, ψ : [a, b] →R are contin-
uously diﬀerentiable with φ(x) ≤ψ(x) for all x in [a, b]. If G is an open subset of
R2 that contains {(x, y) : a ≤x ≤b and φ(x) ≤y ≤ψ(x)} and f : G →R is con-
tinuously diﬀerentiable, then x →
 ψ(x)
φ(x) f (x, y)dy is diﬀerentiable and
d
dx
 ψ(x)
φ(x)
f (x, y)dy

=
f (x, ψ(x))ψ′(x) −f (x, φ(x))φ′(x) +
 ψ(x)
φ(x)
∂1 f (x, y)dy
Proof. Deﬁne F : [a, b] →R by
F(x) =
 ψ(x)
φ(x)
f (x, y)dy
Fix x in (a, b) and let d > 0 such that [x −d, x + d] ⊆(a, b). (We leave the proof
of the theorem when x is an endpoint of [a, b] to the reader as Exercise 1.) When

252
Integration in Higher Dimensions
0 < |t| < d, we have that
F(x + t) −F(x) =
 ψ(x+t)
φ(x+t)
f (x + t, y)dy −
 ψ(x)
φ(x)
f (x, y)dy
=
 φ(x)
φ(x+t)
f (x + t, y)dy +
 ψ(x)
φ(x)
f (x + t, y)dy
+
 ψ(x+t)
ψ(x)
f (x + t, y)dy −
 ψ(x)
φ(x)
f (x, y)dy
= −
 φ(x+t)
φ(x)
f (x + t, y)dy +
 ψ(x+t)
ψ(x)
f (x + t, y)dy
+
 ψ(x)
φ(x)
[ f (x + t, y) −f (x, y)]dy
We now apply the MVT to the ﬁrst and second of these three integrals. So for any
t with 0 < |t| < d we obtain a point y1 between φ(x) and φ(x + t) and y2 between
ψ(x) and ψ(x + t) such that
F(x + t) −F(x) = −[φ(x + t) −φ(x)] f (x + t, y1)
+ [ψ(x + t) −ψ(x)] f (x + t, y2)
+
 ψ(x)
φ(x)
[ f (x + t, y) −f (x, y)]dy
(The point x is ﬁxed but be conscious of the fact that the points y1 and y2 depend
on t.) Dividing both sides by t gives that
F(x + t) −F(x)
t
= −φ(x + t) −φ(x)
t
f (x + t, y1)
+ ψ(x + t) −ψ(x)
t
f (x + t, y2)
+
 ψ(x)
φ(x)
 f (x + t, y) −f (x, y)
t

dy
Fix ϵ > 0 and let’s make a few observations. First, since both φ′ and ψ′ are
continuous, there is a constant C such that |φ′(w)| ≤C and |ψ′(w)| ≤C for all
w in [a, b]. Second, the set K = {(w, y) : w ∈[a, b], φ(w) ≤y ≤ψ(w)} is com-
pact (Why?). This tells us two things: since f is continuous it is bounded on K and
it is uniformly continuous there. So there is a constant M with | f (w, y)| ≤M when-
ever (w, y) ∈K and there is a δ′ > 0 such that | f (w, y) −f (w′, y′)| < ϵ/6C when
∥(w, y) −(w′, y′)∥< δ′ and these points belong to K.
Consider the ﬁrst summand in the last equation above; so we are focusing on the
function φ. There is a δ11 with 0 < δ11 < min{δ′, d} such that when 0 < |t| < δ11,
φ′(x) −φ(x + t) −φ(x)
t
 <
ϵ
6M

7.5 Differentiation under the Integral Sign
253
Because φ is uniformly continuous on [a, b] there is a δ12 > 0 such that |φ(w) −
φ(w′)| < 1
2δ11 when |w −w′| < δ12. Put δ1 = 1
2 min{δ11, δ12}. If 0 < |t| < δ1, then
with the choice of y1 made as above we have that

φ(x + t) −φ(x)
t
f (x + t, y1) −φ′(x) f (x, φ(x))

≤

φ(x + t) −φ(x)
t
−φ′(x)
 | f (x + t, y1)|
+ |φ′(x)| | f (x + t, y1) −f (x, φ(x))|
≤ϵ
6 + C| f (x + t, y1) −f (x, φ(x))|
Now
∥(x + t, y1) −(x, φ(x))∥2 = t2 + |y1 −φ(x)|2
< δ2
11
4 + [φ(x + t) −φ(x)]2
< δ2
11
4 + δ2
11
4 ≤δ2
11
2 < δ′2
Thus | f (x + t, y1) −f (x, φ(x))| < ϵ/6C and so the above inequality becomes that
for 0 < |t| < δ1 we have that

φ(x + t) −φ(x)
t
f (x + t, y1) −φ′(x) f (x, φ(x))
 < ϵ
3
Similarly if we focus on the function ψ there is a δ2 > 0 such that when
0 < |t| < δ2

ψ(x + t) −ψ(x)
t
f (x + t, y2) −ψ′(x) f (x, ψ(x))
 < ϵ
3
Combining what we have done above we get that when 0 < |t| < min{δ1, δ2}

F(x + t) −F(x)
t
−
 ψ(x)
φ(x)
 f (x + t, y) −f (x, y)
t

dy
 < 2ϵ
3
We’re almost done. Note that since ∂1 f is continuous, it is uniformly continuous
on K. Thus we can ﬁnd δ3 > 0 such that
∂1 f (w, y) −∂1 f (w′, y′)
 < ϵ/6C when
∥(w, y) −(w′, y′)∥< δ3 and these points belong to K. Also the MVT applied to
∂1 f says that for any y and t there is an x1 between x and x + t with

f (x + t, y) −f (x, y)
t
−∂1 f (x1, y)
 < ϵ
6C

254
Integration in Higher Dimensions
Let 0 < δ < min{δ1, δ2, δ3}. Fix any y and let 0 < |t| < δ. Since ∥(x, y) −
(x1, y)∥= |x −x1| ≤|t| < δ3, we have for the appropriate x1 that

f (x + t, y) −f (x, y)
t
−∂1 f (x, y)
 ≤

f (x + t, y) −f (x, y)
t
−∂1 f (x1, y)

+
∂1 f (x1, y) −∂1 f (x, y)

< ϵ
6C + ϵ
6C = ϵ
3C
Putting this together with our previous estimates we have that for 0 < |t| < δ

F(x + t) −F(x)
t
−
 ψ(x)
φ(x)
∂1 f (x, y)dy

≤

F(x + t) −F(x)
t
−
 ψ(x)
φ(x)
 f (x + t, y) −f (x, y)
t

dy

+
 ψ(x)
φ(x)

f (x + t, y) −f (x, y)
t
−∂1 f (x, y)
 dy
< 2ϵ
3 + ϵ
3CC
= ϵ
Since ϵ was arbitrary, this completes the proof.
■
One of the important uses of diﬀerentiating under the integral sign is that it
enables us to compute certain integrals over R that are very diﬃcult to compute
otherwise. Here is an example.
7.5.3. Example. Of importance in probability and Fourier analysis is the improper
integral
 ∞
−∞
e−x2/2dx =
√
2π
which we want to verify. It isn’t clear how to use the above theorems to obtain this,
but be patient. Let’s ﬁrst observe that by the symmetry of the integrand we need
only prove
 ∞
0
e−x2/2dx =
Bπ
2
If you play with this integral using the standard tools from calculus, you’ll see they
all fail you. We take a diﬀerent approach. Deﬁne the function α : [0, ∞) →R by
α(r) =
 r
0
e−x2/2dx
2

7.5 Differentiation under the Integral Sign
255
so we want to ﬁnd √limr→∞α(r). Note that
α′(r) = 2e−r2/2
 r
0
e−x2/2dx

Now we make a change of variables x = ry, where 0 ≤y ≤1, and get that
α′(r) = 2e−r2/2
 1
0
re−r2y2/2dy =
 1
0
2re−1
2 (1+y2)r2dy
After staring at the integrand and thinking, we see that
2re−1
2 (1+y2)r2 = −∂
∂r

2 exp

−1
2(1 + y2)r2
1 + y2

Using Theorem 7.5.1 we therefore have that
α′(r) = −2 d
dr
 1
0
exp

−1
2(1 + y2)r2
1 + y2
dy
Putting β(r) equal to the integral in this last equation, this means that α′(r) =
−2β′(r) so that α(r) = −2β(r) + C, for some constant C. We need to evaluate this
constant. In fact
0 = lim
r→0 α(r)
= C −2 lim
r→0
 1
0
exp

−1
2(1 + y2)r2
1 + y2
dy
= C −2
 1
0
1
1 + y2 dy
(Why?)
= C −π
2
since this last integral can be evaluated because (1 + y2)−1 has arctan y as its prim-
itive. Putting it all together we have that
 r
0
e−x2/2dx
2
= π
2 −2
 1
0
exp

−1
2(1 + y2)r2
1 + y2
dy
Letting r →∞shows that [
 ∞
0 e−x2/2dx]2 = π/2, whence the result.
This example was accessed at www.math.uconn.edu/~kconrad/blurbs/analysis/
diﬀunderint.pdf on 21 Jan 2015. This site contains other examples involving diﬀer-
entiating under the integral sign.

256
Integration in Higher Dimensions
Exercises
(1)
Prove Theorem 7.5.2 when x is an endpoint of [a, b].
(2)
Can you formulate and prove a version of Corollary 7.5.1 for [a, b] × [0, ∞)?
(3)
Show that
 1
0 [(x5 −1)/ log x] dx = log 6 by using the function
f (t) =
 1
0
xt −1
log x dx
and diﬀerentiating under the integral sign.
(4)
Show that
 π
0
log(1 + t cos x) = π log

1 +
√
1 −t2
2

.

8
Curves and Surfaces
The next two chapters are on the same topic, one where geometry and analysis over-
lap. This present chapter will focus on R2 and R3, covering curves and surfaces in
those spaces including the theorems of Green, Gauss, and Stokes. Initially, however,
we’ll encounter the concepts in Rp since in the basic material there is little diﬀer-
ence between what happens in the lower dimensions and the general space. When,
however, we discuss in this chapter the concepts leading to the three big theorems,
we will focus on p = 2 and p = 3. Be aware that we will often use heuristic argu-
ments in the present chapter, something that is facilitated by these small dimensions.
This is not true in the following chapter where we adopt a diﬀerent approach that
leads to greater rigor. In fact, you could begin reading Chapter 9 after the ﬁrst sec-
tion of this chapter and refer back to the present chapter for the heuristics. I don’t
advise this as I believe seeing the material in this chapter ﬁrst will better prepare
you to understand Chapter 9. Moreover Chapter 9 is not written with this approach
in mind so it might be awkward to do this. At the end of the next chapter we will
revisit these lower dimensions to supply whatever rigor is absent here.
The historical origins of much of what we do in this chapter and the next started
with an eﬀort to use mathematics to study physics. That same statement can be
made about most of the mathematics that appears in textbooks. As we progress in
this chapter the connection with these historical roots will be evident in some of the
language used. For example in the ﬁrst section we will talk about a particle moving
along a curve. You need not know any physics to understand the next two chapters
since we deal in abstractions of the physical concepts. Indeed we see the virtue
of this abstraction as the mathematics has application to the social and biological
sciences as well as the physical.
8.1. Curves
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In §6.1 we deﬁned a smooth curve in Rp as a continuously diﬀerentiable function
γ : [a, b] →Rp. In §7.1 we introduced the length of a smooth curve, deﬁning it as
ℓ(γ ) =
 b
a ∥γ ′(t)∥dt. We also said that γ and a second smooth curve ρ : [c, d] →
Rp are equivalent, γ ∼ρ, if there is a continuously diﬀerentiable, increasing surjec-
tion τ : [a, b] →[c, d] such that γ (t) = ρ(τ(t)) for all t in [a, b]. Proposition 7.1.7
showed that equivalent curves have the same length. In this section we continue the

258
Curves and Surfaces
study of smooth curves and go more deeply into their analysis. In particular we will
explore the orientation or direction of the curve and introduce another integral asso-
ciated with these curves. Some of the examples and exercises here are from [15].
We begin with some terminology that could have appeared earlier.
8.1.1. Definition. Let γ : [a, b] →Rp be a curve. We say that γ (a) and γ (b) are
the starting and ﬁnal points, respectively. The curve γ is said to be a simple curve if
γ is injective on the open interval (a, b). Curve γ is a closed curve if γ (a) = γ (b).
A simple closed curve is also referred to as a Jordan curve. As usual the trace of
γ is its range and is denoted by {γ } = {γ (t) : t ∈[a, b]}.
8.1.2.
Example.
(a) Fix r > 0 and deﬁne γ : [0, 2π] →R2 by γ (θ) =
(r cos θ, r sin θ). So γ is a Jordan curve and it traces out the circle centered at the
origin of radius r that moves in the counter clockwise direction.
(b) Let a, b > 0 and deﬁne γ : [0, 2π] →R2 by γ (t) = (a cost, bsint). The
trace of γ is an ellipse centered at the origin. What are its two axes? See Exercise 2.
The length of this curve is what is called an elliptic integral and it can seldom be
evaluated explicitly; usually it can only be approximated numerically.
(c) If f : [a, b] →Rp−1 is continuously diﬀerentiable, deﬁne γ : [a, b] →Rp by
γ (t) = t ⊕f (t). So γ is the curve in Rp that traces out the graph of the function f .
We note that γ ′(t) = 1 ⊕f ′(t).
8.1.3. Proposition. Let γ : [a, b] →Rp be a smooth curve with ℓ= ℓ(γ ). If we
deﬁne τ : [a, b] →R by
τ(t) =
 t
0
∥γ ′(u)∥du
then τ is an increasing, continuously diﬀerentiable function that maps [a, b] onto
[0, ℓ] and with τ ′(t) = ∥γ ′(t)∥.
Proof. We begin by noting that the function u →∥γ ′(u)∥is a continuous function.
Hence the FTC can be applied to conclude that τ is continuously diﬀerentiable and
τ ′(u) = ∥γ ′(u)∥. Since τ has a positive derivative, it is increasing. Clearly it is a
surjection onto [0, ℓ].
■
The preceding proposition, in spite of the brevity of its proof, tells us a lot. At
any point t in [a, b], τ(t) measures the length of the curve γ from the beginning of
the curve to the time t. Thus at any t in [a, b], τ ′(t) = ∥γ ′(t)∥is the rate of change
of the arc length of the curve with respect to time; that is, ∥γ ′(t)∥is the speed of a
particle moving along the curve γ while the vector γ ′(t) is tangent to the curve and
points in the direction the particle is moving.
8.1.4. Definition. If γ : [a, b] →Rp is a smooth curve and f : {γ } →R is a con-
tinuous function, the integral of f along γ is deﬁned as

γ
f =

γ
f ds =
 b
a
f (γ (t))∥γ ′(t)∥dt

8.1 Curves
259
We point out that if in this deﬁnition we take f to be the constant function 1,
then we get the length of the curve γ . This is symbolically captured by using the
ds in the integral. The notation is traditional and is related to Proposition 8.1.3 as
follows. If τ is as in that proposition and if τ is injective, we could take the inverse
s →τ −1(s) to give a reparametrization of the curve. This is often called the natural
parametrization of the curve when it exists.
8.1.5. Definition. Say that a curve γ : [a, b] →Rp is regular if γ is smooth and
γ ′(t) ̸= 0 for all t in [a, b]. The curve γ is piecewise regular if there is a partition
{a = t0 < t1 < · · · < tn = b} such that γ is regular on [ti−1,ti] for 1 ≤i ≤n. We
denote this by saying that γ = γ1 + · · · + γn, where for 1 ≤i ≤n, γi : [ti−1,ti] →
Rp is deﬁned by γi(t) = γ (t).
Observe that the curve in Example 8.1.2(c) is regular. If γ is regular and τ is as in
Proposition 8.1.3, then according to that result τ ′(t) > 0 for allt. Hence τ is injective
and we can form its inverse, thus producing the equivalent curve (Deﬁnition 7.1.6)
s →γ (τ −1(s)) deﬁned on [0, ℓ]. This proves the following.
8.1.6. Proposition. A regular curve can be given its natural parametrization.
8.1.7. Example. (a) The curve γ : [0, 2π] →R2 deﬁned by γ (θ) = (cos θ, sin θ)
with ∥γ ′(θ)∥= ∥(−sin θ, cos θ)∥= 1. Thus ℓ(γ ) = 2π. If τ is as in (8.1.3), we
have that τ(θ) =
 θ
0 1du = θ, so this form of the equation of the unit circle is
already in its natural parametrization.
(b) If f : [a, b] →Rp−1 is continuously diﬀerentiable and γ (t) = t ⊕f (t) as
in (8.1.2(c)), then ∥γ ′(t)∥=

1 + ∥f ′(t)∥2 > 0. So γ is regular and has a natural
parametrization. If g : {γ } →R is a continuous function, then

γ
g =
 b
a
g(γ (t))

1 + ∥f ′(t)∥2 dt
8.1.8. Definition. If γ : [a, b] →Rp is a regular curve, then the unit tangent vector
for γ at the point γ (t) is the vector
T (t) =
γ ′(t)
∥γ ′(t)∥
If F : {γ } →Rp is a continuous function, the line integral of F along γ is deﬁned
as

γ
F · T ds =
 b
a
⟨F(γ (t)), γ ′(t)⟩dt
If γ is piecewise regular with γ = γ1 + · · · + γn, then

γ
F · T ds =
n

i=1

γi
F · Ti ds
where for 1 ≤i ≤n, Ti(t) = γ ′
i (t)/∥γ ′
i (t)∥when ti−1 ≤t ≤ti.

260
Curves and Surfaces
Strictly speaking we should have used the notation Tγ (t) for the unit tangent vec-
tor, but the notation T (t) is traditional and assumes we have speciﬁed the curve
γ . Similarly the notation

γ F · T ds is also traditional and is sometimes called the
path integral. The term line integral is frequently also used for the integral in Def-
inition 8.1.4, where we integrate a scalar-valued function. We chose not to use that
term to avoid the possibility of ambiguity, but in actuality there is little possibility
of this since in one case we integrate a scalar-valued function and in the other a
vector-valued function. In fact the earlier deﬁnition is used in the preceding one.
Indeed

γ
F · T ds =
 b
a
⟨F(γ (t)), γ ′(t)⟩dt
=
 b
a
⟨F(γ (t)), T (t)⟩∥γ ′(t)∥dt
=

γ
⟨F ◦γ , T ⟩ds
There is a symbolic representation of

γ F · T ds that we will use and the reader
will see in the literature. Namely if γ (t) = p
j=1 γj(t)ej and F(x) = p
j=1 Fj(x)ej,
then
8.1.9

γ
F · T ds =
p

j=1

γ
Fj dxj
To see where this comes from observe that it means

γ
F · T ds =
 b
a
⟨F(γ (t)), γ ′(t)⟩dt =
p

j=1
 b
a
Fj(γ (t))γ ′
j(t) dt
Symbolically, xj = γj(t) and so dxj = γ ′
j(t)dt. (We’ll say more about this in the
next chapter when we study diﬀerential forms.)
Consider what happens if we have equivalent curves (Deﬁnition 7.1.6). That is
suppose γ : [a, b] →Rp and ρ : [c, d] →Rp are two smooth curves and there is
a continuously diﬀerentiable, increasing surjection τ : [a, b] →[c, d] such that
γ (t) = ρ(τ(t)) for all t in [a, b]. So γ ′(t) = ρ(τ(t))τ ′(t). Note that if ρ is regular,
then unless τ is strictly increasing so that τ ′(t) > 0 for all t in [a, b], it does not
follow that γ is regular. See Exercise 1. On the other hand if γ is regular and
ρ ∼γ , then it must be that ρ is regular and τ is strictly increasing. It also follows
that
Tγ (t) =
γ ′(t)
∥γ ′(t)∥=
ρ′(τ(t))τ ′(t)
∥ρ′(τ(t))τ ′(t)∥= Tρ(τ(t))

8.1 Curves
261
With the same notation if both γ and ρ are regular, then

γ
f ds =
 b
a
f (γ (t))∥γ ′(t)∥dt
=
 b
a
f (ρ(τ(t)))∥ρ′(τ(t))∥τ ′(t) dt
=
 d
c
f (ρ(u))∥ρ′(u)∥du
Similarly

γ
F · T ds =

ρ
F · T ds
8.1.10. Example. (a) Let γ : [0, 2π] →R2 be deﬁned by γ (θ) = (cos θ, sin θ), so
that γ represents the unit circle in the plane traversed once in the counterclockwise
direction. If F : R2 →R2 is deﬁned by F(x, y) = (y, x), then

γ
F · T ds =
 2π
0
⟨(sin θ, cos θ), (−sin θ, cos θ)⟩dθ
=
 2π
0
[−sin2 θ + cos2 θ] dθ
=
 2π
0
cos(2θ) dθ = 0
(b) Let γ be as in part (a) and put G(x, y) = (−y, x). So

γ
G · T ds =
 2π
0
⟨(−sin θ, cos θ), (−sin θ, cos θ)⟩dθ =
 2π
0
θ dθ = 2π
(c) Deﬁne γ : [0, 1] →R3 by γ (t) = (t, 2t, −t) and let F : R3 →R3 be deﬁned
by F(x, y, z) = (x + y, x2, −yz). Here

γ
F · T ds =
 1
0
⟨(3t,t2, 2t2), (1, 2, −1)⟩dt
=
 1
0
3t dt = 3
2
(d) Let γ be the boundary of the square [0, 1] × [0, 1] in R2 in the coun-
terclockwise direction and F(x, y) = (xy, x2 + y2). Here γ = γ1 + γ2 + γ3 + γ4,
where each of these pieces is a straight line deﬁned on [0, 1] as follows: γ1(t) =
(t, 0), γ2(t) = (1,t), γ3(t) = (1 −t, 1), γ4(t) = (0, 1 −t). So the derivatives of
these curves are γ ′
1(t) = (1, 0), γ ′
2(t) = (0, 1), γ ′
3(t) = (−1, 0), γ ′
4(t) = (0, −1).

262
Curves and Surfaces
Hence

γ
F · T ds =
 1
0
[⟨(0,t2), (1, 0)⟩+ ⟨(t, 1 + t2), (0, 1)⟩
+ ⟨(1 −t, (1 −t)2+1), (−1, 0)⟩+⟨(1 −t, (1 −t)2), (0, −1)⟩] dt
=
 1
0
[0 + (1 + t2) −(1 −t) −(1 −t)2] dt
=
 1
0
[1 + t2 −1 + t −(1 −2t + t2)] dt
= 1
2
We conclude this section with a brief discussion of curves in R2. If γ : [a, b] →
R2 is a regular curve and γ (t) = (x(t), y(t)), then we have functions t →x(t) and
t →y(t) from [a, b] into R. It is easy to check that each of these functions is smooth
and γ ′(t) = (x′(t), y′(t)). If F : {γ } →R2 is a continuous function with F(x, y) =
(P(x, y), Q(x, y)), then
8.1.11

γ
F · T ds =
 b
a
[P(x(t), y(t))x′(t) + Q(x(t), y(t))y′(t)] dt =

γ
P dx + Q dy
We will frequently encounter expressions such as the right-hand side of the last
equation when we study diﬀerential forms in the next chapter.
Exercises
(1)
Find an example of a regular curve ρ : [c, d] →R2 and an increasing function τ :
[a, b] →[c, d] such that the curve τ(t) = ρ(τ(t)) is not regular.
(2)
Verify the statements in Example 8.1.2(b).
(3)
Let F(x, y) = (x, xy) and ﬁnd

γ F · T ds for the following choices of γ : (a) γ (t) =
(t,t) with 0 ≤t ≤1; (b) γ (t) = (t,t2) for 0 ≤t ≤1; (c) γ is the square [0, 1] ×
[0, 1] in the counterclockwise direction.
(4)
Let F(x, y, z) = (z, x2, y) and ﬁnd

γ F · T ds for each of the following
curves γ : (a) γ (t) = (t,t,t) for t in [0, 1]; (b) γ (t) = (cost, sint,t) for
0 ≤t ≤2π.
(c)
γ is
the
closed
polygon
whose
successive
vertices
are
(0, 0, 0), (2, 0, 0), (2, 3, 0), (0, 0, 1), (0, 0, 0).
(5)
Find

γ F · T ds for each of the following choices of F and γ . (a) F(x, y) =
(xy, y −x) and γ is the parabola y = x2 from (1, 1) to (3, 0). (b) F(x, y, z) =
(

x3 + y3 + 5, z, x2) and γ is the intersection of the elliptical cylinder y2 + 2z2 =
1 with the plane x = −1 that is oriented in the clockwise direction when viewed
from far out along the positive x-axis. (This last will test your viewing skills.)

8.2 Green’s Theorem
263
(6)
If γ : [a, b] →Rp is a regular curve, let −γ be the curve deﬁned on [−b, −a]
by (−γ )(t) = γ (−t). Show that if F : {γ } →Rp is a continuous function, then

−γ F · T−γ ds = −

γ F · Tγ ds. What is the relation between {−γ } and {γ }?
(7)
If f and g are continuous functions of a single variable and F is deﬁned on R2 by
F(x, y) = ( f (x), g(y)), show that

γ F · T ds = 0 whenever γ is a regular closed
curve in R2.
8.2. Green’s Theorem
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In this section we will focus on curves in R2. They diﬀer quite a bit from curves
in Rp when p > 2, and we begin with a result that is unique to the plane. It is a
famous result from planar topology that is highly intuitive but diﬃcult to prove. A
moment’s thought reveals that this theorem is decidedly false in higher dimensional
spaces. Recall the deﬁnition of Jordan curve as a simple closed curve, which was
given at the start of the last section.
8.2.1. Theorem (Jordan1 Curve Theorem).
If γ is a Jordan curve in R2, then
R2\{γ } has two components, only one of which is bounded. Moreover {γ } is the
boundary of each of these components.
We won’t prove this theorem. For a proof see [14]. If G is the bounded compo-
nent of the complement of γ , then G is called the inside of the curve γ and γ is
the boundary of G. The unbounded component is called the outside of γ and its
boundary is also γ .
If we assume γ is a regular curve, then it has a natural direction given by its unit
tangent vector. Thus γ : [a, b] →R2 has a direction – as the variable t goes from a
to b, γ (t) goes from γ (a) to γ (b). What we want, however, is to discuss the direction
of a curve relative to an open subset of R2 when the curve forms part of the boundary
of the set as, for example, is the case with the Jordan Curve Theorem and G is the
inside of the curve. Intuitively we want to say that γ is positively oriented relative
to G if when we walk along γ in the direction of T ′(t), equivalently in the direction
of γ ′(t), then G lies to our left. We saw this for the curve γ in Example 8.1.10(a).
We want to make this mathematical. In doing so we generalize the concept to all
regular curves, not just those that are Jordan curves.
If γ is regular and γ (t) = (x(t), y(t)), we have that for each t in [a, b] at least one
of x′(t) and y′(t) is not zero. It follows that
N(t) = (−y′(t), x′(t))/∥γ ′(t)∥
is a unit vector orthogonal to T (t). In fact, N(t) results from rotating T (t) 90◦in
the counterclockwise direction. For each t the vector N(t) is called the unit normal
vector to the curve.
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
1 A biographical note for Camille Jordan can be found in Deﬁnition 7.2.8.

264
Curves and Surfaces
8.2.2. Definition. If a piecewise regular curve γ : [a, b] →R2 forms part of the
boundary ∂G of an open subset G of the plane, say that γ is positively oriented
relative to G, if for every t in [a, b] where γ ′(t) exists there is an ϵ0 > 0 such that
when 0 < ϵ < ϵ0 we have that γ (t) + ϵN(t) ∈G.
Intuitively, γ is positively oriented if N(t) is pointing inside G. Just for emphasis,
in the preceding deﬁnition we are not assuming that γ is a Jordan curve or that it is
the entirety of the boundary. Let’s look at an example.
8.2.3. Example.
Let 0 < r < R and put G = {(x, y) : r <

x2 + y2 < R}, an
annulus. If γ1(θ) = (R cos θ, R sin θ) and γ2(θ) = (r cos(2π −θ), r sin(2π −θ))
for 0 ≤θ ≤2π, then both γ1 and γ2 are positively oriented relative to G. See Exer-
cises 1 and 2.
Now that we have established the idea of orientation of a curve relative to a set,
we want to prove one of the fundamental theorems on integrating around a curve
in the plane. In fact this is the Fundamental Theorem of Calculus for R2. First we
need to set the stage with a few additional concepts.
8.2.4. Definition. A subset X of R2 is said to be of Type I if there is an inter-
val [a, b] and piecewise regular real-valued functions f1, f2 deﬁned on [a, b] such
that f1(x) ≤f2(x) on (a, b) and X = {(x, y) ∈R2 : a ≤x ≤b, f1(x) ≤y ≤f2(x)}.
X is said to be of Type II if there is an interval [c, d] and piecewise regular real-
valued functions g1, g2 deﬁned on [c, d] such that g1(y) ≤g2(y) on (c, d) and
X = {(x, y) ∈R2 : c ≤y ≤d, g1(y) ≤x ≤g2(y)}.
The term Type I is traditional though it usually requires only that f1 and f2 be
continuous functions, as we did when we discussed Fubini’s Theorem (7.3.5). We
say they are piecewise regular in the deﬁnition of Type I because this is the only
context in which we will use the term. Similar comments apply to Type II. There
are Type I sets that are not also Type II (Why?); but there are sets, for example a disk
or a square, that are simultaneously Type I and Type II. Such sets will hold special
interest for us.
8.2.5. Proposition. If X is either Type I or Type II, then X is a compact Jordan
set and there is a piecewise regular curve γ whose trace is ∂X and γ is positively
oriented relative to int X.
Proof. Here is the proof for a Type I set; the proof for a Type II set is similar. Use
the notation established in the deﬁnition above. Proposition 7.3.10 establishes that
X is a compact Jordan set. Deﬁne the following four curves on [0, 1], where γ1 and
γ3 trace the graphs of f1 and f2, respectively, but in opposite directions; while γ2

8.2 Green’s Theorem
265
and γ4 trace vertical straight line segments, but again in opposite directions.
γ1(t) = (tb + (1 −t)a, f1(tb + (1 −t)a))
γ2(t) = (b,t f2(b) + (1 −t) f1(b))
γ3(t) = (ta + (1 −t)b, f2(ta + (1 −t)b))
γ4(t) = (a,t f1(a) + (1 −t) f2(a))
If we set γ = γ1 + γ2 + γ3 + γ4, then γ is a piecewise regular curve (Verify!) that
traces ∂X. It easily follows that γ is positively oriented relative to int X.
■
If, in the case of Type I, f1(x) < f2(x) for a < x < b, then the curve γ in the
proof of the preceding proposition is also a Jordan curve and int X is its inside. A
similar statement applies to a Type II set. See Exercise 3.
8.2.6. Definition. Let X be a compact Jordan set in R2 whose boundary is a ﬁnite
collection of piecewise regular curves. A collection {Yj : 1 ≤j ≤m} of closed sub-
sets of X is called a Type I cover of X if eachYj is a set that is Type I and the following
are satisifed: (a) X = m
j=1 Yj; (b) when 1 ≤i < j ≤m, Yi ∩Yj = ∂Yi ∩∂Yj; (c) if
Yi,Yj, and Yk are three distinct sets, then ∂Yi ∩∂Yj ∩∂Yk is either empty or a ﬁnite
set. The collection {Yj : 1 ≤j ≤m} of closed subsets of Y is called a Type II cover
if each set Yj is a Type II set and conditions (a), (b), and (c) above are also satisﬁed.
The set X is a G-set if it has a Type I cover as well as a Type II cover.
The terminology is not standard. It is given to facilitate the proof of the next
theorem. Let’s point out that from what we did in Chapter 7 it is automatic that X
is a Jordan set if we assume that ∂X is a system of curves as described. We added
the assumption that X is a Jordan set for emphasis. To be clear, condition (b) in the
deﬁnition includes the possibility that Yi ∩Yj = ∅. Note that (b) also implies that
intYi ∩intYj = ∅when i ̸= j. Condition (c) prevents the boundary of one of the
sets Yi from doubling back on itself.
Examples of G-sets abound. Of course any set like an ellipse that is already both
Type I and II is an example of a G-set. Here is a description of a general class of
G-sets.
Suppose X is a compact subset of R2 such that X = cl [int X] and ∂X consists of a
ﬁnite number of non-intersecting regular Jordan curves γ1, . . . , γm. Suppose there is
a R-cover R of X that has the following properties. If R ∈R and R ∩∂X ̸= ∅, then
R meets exactly one of the boundary curves γk. (See Figure 8.2.1.) Further assume
that if R meets γk, then ∂[R ∩X] is a Jordan curve consisting of three regular curves:
a vertical and a horizontal line segment that meet at some point (a, b) in int X and
a part of γk that joins the endpoints of these two segments that are diﬀerent from
(a, b). (In Figure 8.2.1, γk is γ3.) The vertical and horizontal line segments form
an L and, after suitable rotation, there are four diﬀerent positions for this L. If we
also have that int [R ∩X] has the property that for any two points in this set the line
segment joining them is also contained in the set, then R ∩X is simultaneously a

266
Curves and Surfaces
R
(a, b)
γ1
γ2
γ3
Figure 8.2.1
Type I and a Type II set. Since every rectangle is simultaneously a Type I and Type
II set, we have that {R ∩X : R ∈R} is both a Type I and a Type II cover of X. Hence
X is a G-set.
Frankly it seems to me that we can carry out the procedure described in the pre-
ceding paragraph for any set whose boundary consists of a ﬁnite number of pairwise
disjoint regular Jordan curves. Attempting to write a proof of this with all the details
of the argument required, however, doesn’t seem worth it.
The following idea will be useful in our discussions for showing a set is a
G-set. If X is a compact subset of R2 with int X ̸= ∅, then a cut of X is a simple
piecewise regular curve λ : [0, 1] →X such that λ(0), λ(1) ∈∂X and λ(t) ∈int X
for 0 < t < 1. So a cut literally cuts the set X. If int X is connected and λ is a cut,
then (int X )\{λ} has two components. In practice we will show that a set X is a G-set
by making a judicious choice of cuts. See the next example.
8.2.7. Example. Let 0 < a < b < ∞and put X = {(x, y) : a2 ≤x2 + y2 ≤b2}, an
annulus. Let λ1, λ2, λ3, λ4 be the cuts deﬁned on [0, 1] as follows; λ1(t) = (tb +
(1 −t)a, 0),
λ2(t) = (0,tb + (1 −t)a),
λ3(t) = (−tb −(1 −t)a, 0),
λ4(t) =
(0, −tb −(1 −t)a). If X1, X2, X3, X4 are the sets whose interiors are the com-
ponents of int X\ 4
j=1{γj}, then these sets show that X is a G-set. Also see
Exercise 5.
If X is a G-set and ∂X consists of the traces of the disjoint piecewise regu-
lar curves γ1, . . . , γn, we adopt the notation that for any continuous function f :
∂X →R
8.2.8

∂X
f ds =
n

k=1

γk
f ds
Before stating and proving the main theorem, here is a lemma, which is little more
than an observation.

8.2 Green’s Theorem
267
8.2.9. Lemma. Let X be a G-set with {Xj} either a Type I or Type II cover.
(a) If F : X →R2 is a smooth function, then

∂X F · T ds = m
j=1

∂Xj F · T ds.
(b) If g : X →R is a continuous function, then

X g = m
j=1

Xj g.
Proof. We only do this for a Type I cover; the proof for a Type II cover is similar.
The proof of (b) is immediate since if {Xj : 1 ≤j ≤m} is a Type I cover of X the
area of m
j=1 ∂Xj is zero. To prove (a) observe what happens when 1 ≤i < j ≤
m and ∂Xi ∩∂Xj ̸= ∅. If this intersection contains a piecewise regular arc γ , then
from Proposition 8.2.5 we know that when γ appears as part of ∂Xi its direction is
opposite to that of its direction when it appears as part of ∂Xj. So the contributions
of γ to the two integrals

∂Xi gds and

∂Xj gds are the negative of one another. If
∂Xi ∩∂Xj has an isolated point, this does not contribute to the corresponding line
integrals. This establishes (a).
■
8.2.10. Theorem (Green’s2 Theorem). If X is a G-set and F : X →R2 is a smooth
function with F(x, y) = (P(x, y), Q(x, y)), then

∂X
F · T ds =

X

∂Q
∂x −∂P
∂y

Proof. Using (8.1.11) we want to show that

X

∂Q
∂x −∂P
∂y

=

∂X
P dx + Q dy
Separating the sides of the equation we’ll show that
8.2.11

X
∂P
∂y = −

∂X
P dx
and

X
∂Q
∂x =

∂X
Q dy
Let’s show the ﬁrst of these. Since X is a G-set it has a Type I cover. The preced-
ing lemma says it suﬃces to prove this under the additional assumption that X is a
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
2 George Green was born in 1793 in Sneinton, England. His father was a baker and George’s formal schooling was
rather limited. In fact he had only about a year’s formal education starting when he was eight. He had to withdraw
from school when he was needed back in the bakery. With time his father prospered and purchased several
other bakeries and even established his own mill where George worked. Somehow through all this, George
continued to study mathematics. How he became familiar with the forefront of mathematics is not known.
He never married Jane Smith, the daughter of the manager of his father’s mill, but they lived together and
eventually had seven children. In 1828 Green published one of his landmark papers on electricity and magnetism.
It was oﬀered “on subscription,” meaning that to obtain a copy you had to pay a fee. One of the subscribers
was Sir Edward Bromhead, an established mathematician, who immediately contacted Green and oﬀered to
communicate his future papers to the Royal Society of London. After a delay by Green, he and Bromhead began
regular meetings. Green soon produced three new papers, two on electricity and one on hydrodynamics, all
of which were published in leading journals. Later Bromhead suggested Green study at Cambridge, where he
enrolled in 1833; he became an undergraduate at the age of 40. He graduated four years later and in 1839 he
became a Perse fellow. (The fellowship required that the holders be unmarried, which Green was in spite of his
having six children at the time. His seventh was born shortly after receiving the fellowship.) Unfortunately his
health deteriorated and he died in 1841 in Sneinton.

268
Curves and Surfaces
set of Type I; let X = {(x, y) : a ≤x ≤b and f1(x) ≤y ≤f2(x)}. Let the positively
oriented piecewise regular curve ∂X = γ1 + γ2 −γ3 + γ4, where these four curves
are deﬁned as in Proposition 8.2.5. Note that since γ2 and γ4 are vertical line seg-
ments, the value of x is constant (either a or b). Hence

γ2 P dx =

γ4 P dx = 0. As
we pointed out in the proof of Proposition 8.2.5, γ1 traces the graph of f1 from left
to right while γ3 traces the graph of f3 but in the opposite direction. Remember this
in the next step of this proof.
We now apply Fubini’s Theorem to get

X
∂P
∂y =
 b
a
 f2(x)
f1(x)
∂P
∂y (x, y) dy

dx
=
 b
a
[P(x, f2(x)) −P(x, f1(x))]dx
= −

γ3
P dx −

γ1
P dx
= −

γ3
P dx −

γ1
P dx −

γ2
P dx −

γ4
P dx
= −

∂X
P dx
The next step is to show that
8.2.12

γ
Q dy =

X
∂Q
∂x
This is left to the reader as Exercise 6.
■
8.2.13. Example. (a) Find

γ F · T ds when γ is the perimeter of the rectangle R =
[0, 3] × [1, 4] with the counterclockwise direction and F(x, y) = (x3, 3x + y4). We
could parametrize γ and carry out the process, but it’s a bit easier to use Green’s
Theorem. Here ∂P/∂y = 0, ∂Q/∂x = 3, so

γ F · T ds =

R(3) = 27.
(b) Let F(x, y) = (0, x) and let X be any G-set. Green’s Theorem says that

∂X
x dy = Area (X )
Exercises
(1)
Verify Example 8.2.3.
(2)
Evaluate

∂X F · T ds when X is the annulus described in Example 8.2.3 and
F(x, y) = (−y3, x2).
(3)
If X = {(x, y) ∈R2 : a ≤x ≤b, f1(x) ≤y ≤f2(x)} as in the deﬁnition of a Type I
set, show that ∂X is a Jordan curve if f1(x) < f2(x) when a < x < b.

8.3 Surfaces
269
(4)
(a) Show that a disk is both a Type I set and a Type II set. (b) Give an example of a
Type I set that is not Type II. (c) Give an example of a Type II set that is not Type I.
(5)
(a) Show that X = {(x, y) : 0 ≤x ≤2π, |y| ≤| sin x|} is a G-set. (b) Let X be
the set in R2 bounded by the three circles {(x, y) : x2 + y2 = 1} and {(x, y) :

(x ± 1
8)2 + y2 = 1
8} and show that it is a G-set.
(6)
Prove (8.2.12) as part of the proof of Green’s Theorem.
(7)
In Green’s Theorem what happens if you take P(x, y) = −y and Q(x, y) = 0?
Suppose P(x, y) = 0 and Q(x, y) = x?
(8)
Find

γ F · T ds when F(x, y) = (xy, x2y3) and γ is the positively oriented perime-
ter of the triangle with vertices (0, 0), (1, 0), (1, 2).
8.3. Surfaces
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
We are predisposed to think of a surface as a two-dimensional object inside three-
dimensional space and that’s what we’ll do in this section. Later in §9.1 we’ll discuss
q-dimensional surfaces that are contained in p-dimensional spaces.
8.3.1. Definition. A surface domain in R2 is a compact Jordan subset R of R2 such
that int R is connected, R = cl (int R), and ∂R is a ﬁnite collection of pairwise dis-
joint piecewise regular curves. A 2-surface in R3, or just a surface, is a pair (, R)
where R is a surface domain in R2 and  : R →R3 is a continuous function that is
smooth on a neighborhood of R. The trace of (, R) is the set {} = (R) ⊆R3.
Now that we have stated that a surface is a function, let’s mention that we will
sometime violate this and refer to a surface as a set S. When we do this, what we
mean is that there is a surface (, R) whose trace is the set S. In such a situation
(, R) is called a parametrization of S. (See Exercise 1.) As you might expect from
the treatment of curves, we’ll see that such a parametrization is not unique.
To be completely precise, always a virtue, we should have deﬁned an object (with
some name) as we deﬁned a surface above, then deﬁne an equivalence relation
between these objects, and deﬁne a surface as an equivalence class of such objects.
On the other hand there is another virtue, simplicity of exposition. It seems to me in
this case that this latter virtue is more important than the ﬁrst. We will soon deﬁne,
however, an equivalence relation between surfaces and show that equivalent surfaces
have the same properties we will be interested in exploring.
8.3.2. Example. Let 0 < r < R and call the set S = {x ∈R3 : r ≤∥x∥≤R} the
corona. Note that ∂S = {x ∈R3 : ∥x∥= r} ∪{x ∈R3 : ∥x∥= R}. Each component
of this boundary is a surface (Exercise 1), but since ∂S is not connected it is not a
surface. Each component, however, is.
We’ll see more examples of surfaces shortly; we gave the preceding one to under-
line a facet of the deﬁnition. As we noted the deﬁnition of a surface (, R) requires
that int R, and hence R, be connected and this implies that {} is connected as well

270
Curves and Surfaces
as compact. Hence the boundary of a corona is not, strictly speaking, a surface.
This is not signiﬁcant. We want the interior of a surface domain to be connected
to apply previous results, but we could have dropped the connectedness assumption
and treat one component at a time. Later, in the following section, we’ll broaden the
concept of a surface where we drop this insistence on connectivity. We’ll stick for
the moment with the approach taken here, however, as it is more eﬃcacious.
8.3.3. Example. (a) If R is a surface domain in R2 and f : R →R, then deﬁne  :
R →R3 by (x, y) = (x, y, f (x, y)). If the function f is smooth in a neighborhood
of R, then (, R) is a 2-surface.
(b) If R = {u ∈R2 : ∥u∥≤1}, let S = {(u,

1 −∥u∥2) : u ∈R}. This set is
a hemisphere centered at the origin. If we deﬁned  : R →R3 by (u) =
(u,

1 −∥u∥2), then {} = S but  fails to be smooth in a neighborhood of R.
Exercise 2 gives a parametrization (, T ) that is smooth in a neighborhood of the
surface domain T and has this hemisphere as its trace.
(c) A truncated cylinder is the trace of a 2-surface. For example, let S =
{(u, v,t) ∈R3 : u2 + v2 = 1, 0 ≤t ≤2}, R = [0, 2π] × [0, 2], and deﬁne the sur-
face  : R →R3 by (θ,t) = (cos θ, sin θ,t).
(d) The torus in R3 is a 2-surface. In fact let a > b > 0, let R = [−π, π] ×
[−π, π] ⊆R2, and deﬁne  : R →R3 by (u, v) = ((a + bcos v) cos u, (a +
bcos v) sin u, bsin v). The reader can verify (Exercise 4) that the image of {0} ×
[−π, π] is a circle in the xz-plane centered at (a, 0, 0) and having radius b and that
(R) is a torus.
In analogy with what we did for curves in the last section, we seek a way to orient
surfaces. A moment’s thought shows this is not as straightforward a task as it was for
curves where we oriented by establishing the unit tangent vector T (t) for a regular
curve. As an introduction, let’s consider a special kind of surface.
Let R be a surface domain in R2 and suppose we have a function f : R →R
smooth in a neighborhood of R. We deﬁne a 2-surface  on R by (u, v) =
(u, v, f (u, v)) as in Example 8.3.3(a). In §6.7 we examined a smooth curve
γ : (−1, 1) →R and the induced curve γ : (−1, 1) →R3 deﬁned by γ (t) =
(γ (t), f (γ (t)) = γ (t) ⊕f (γ (t)) in {} ⊆R2 ⊕R. We computed
γ ′(0) = (γ ′(0), ∇f (γ (0)) · γ ′(0)) = (γ ′(0), ⟨∇f (γ (0)), γ ′(0)⟩)
a vector in R3 that is tangent to the curve γ at t = 0. In Proposition 6.7.8 we proved
that if γ (0) = (u0, v0), then the vector (∇f (u0, v0), −1) is orthogonal to γ ′(0).
Note that (∇f (u0, v0), −1) does not depend on the curve γ and so it is perpendic-
ular to every curve in {} passing through (u0, v0, f (u0, v0)). Thus we have (Deﬁ-
nition 6.7.10) that
(u0, v0, f (u0, v0)) + {x ∈R3 : x ⊥(∇f (u0, v0), −1)}
is the tangent aﬃne hyperplane to the surface  and
(∇f (u0, v0), −1)/

∥∇f (u0, v0)∥2 + 1

8.3 Surfaces
271
is a unit normal vector to the surface at the point (u0, v0). So we see that when a
2-surface in R3 is the graph of a smooth function, it has a unit normal vector. We
seek to extend this to arbitrary 2-surfaces in R3.
If (, R) is a 2-surface in R3, then write (u, v) = (φ1(u, v), φ2(u, v), φ3(u, v))
for (u, v) in R. We want to consider the Jacobians
φi,φ j(u0, v0) = ∂(φi, φj)
∂(u, v) (u0, v0)
The next result hints at the reason for our consideration of these Jacobians. The
idea is that under a mild restriction we can use what we did above to show that the
surface has a unit normal vector.
8.3.4. Proposition.
If (, R) is a smooth 2-surface with  = (φ1, φ2, φ3) and
(u0, v0) ∈int R such that φ1,φ2(u0, v0) ̸= 0, then there is an open set  in R2 and
a smooth function f :  →R such that
(u0, v0) ∈{(x, y, f (x, y)) : (x, y) ∈} ⊆(R)
Proof. Deﬁne ψ : R →R2 by ψ(u, v) = (φ1(u, v), φ2(u, v)). Observe that the
hypothesis implies that the derivative of ψ at (u0, v0) is an invertible linear trans-
formation from R2 into itself. By the Inverse Function Theorem (6.8.2) there is an
open set G with G ⊆R ⊆R2 that contains (u0, v0) such that  = ψ(G) is open, ψ
is injective on G, and h = (ψ|G)−1 :  →R2 is continuously diﬀerentiable. Deﬁne
f :  →R by
f (x, y) = φ3(h(x, y))
and note that f is a smooth function on . If (u, v) ∈G ⊆R and ψ(u, v) = (x, y),
then h(x, y) = (u, v) and
(u, v) = (φ1(u, v), φ2(u, v), φ3(u, v)) ∈R3
= ψ(u, v) ⊕φ3(h(x, y)) ∈R2 ⊕R
= ψ(u, v) ⊕f (x, y) ∈R2 ⊕R
= (x, y, f (x, y)) ∈R3
completing the proof.
■
Observe that the hypothesis in the preceding result that φ1,φ2(u0, v0) ̸= 0 can
be replaced by φi,φ j(u0, v0) ̸= 0 for any i ̸= j and the conclusion will remain that
the point (u0, v0) is contained in a small relatively open subset of {} that is a
graph. For example if φ2,φ3(u0, v0) ̸= 0 we will have an open subset  of R2 and
a smooth function f :  →R such that
(u0, v0) ∈{( f (x, y), x, y) : (x, y) ∈} ⊆(R)
A similar conclusion holds if the hypothesis is that φ1,φ3(u0, v0) ̸= 0.
Before we can state a condition for there to be a unit normal vector, we need to
brieﬂy review the cross product of vectors in R3.

272
Curves and Surfaces
Recall that if x = (x1, x2, x3), y = (y1, y2, y3) ∈R3, then
x × y = (x2y3 −x3y2, x3y1 −x1y3, x1y2 −x2y1)
There is a certain cyclic pattern in this deﬁnition, but the easiest way to remember
it was given in Calculus as the determinant
x × y = det
⎡
⎣
i
j
k
x1
x2
x3
y1
y2
y3
⎤
⎦= det
⎡
⎣
e1
e2
e3
x1
x2
x3
y1
y2
y3
⎤
⎦
where the unit vectors i, j, k are now being labeled e1, e2, e3. You can see that these
equations are the same as the deﬁnition above by expanding the determinant by
minors using the ﬁrst row. Here are the basic properties of the cross product.
8.3.5. Proposition. If x, y, z ∈R3 and α ∈R, the following hold.
(a) x × x = 0 and x × y = −y × x.
(b) (αx) × y = x × (αy) = α(x × y).
(c) x × (y + z) = (x × y) + (x × z).
(d) ⟨x × y, z⟩= ⟨x, y × z⟩= det
⎡
⎣
x1
x2
x3
y1
y2
y3
z1
z2
z3
⎤
⎦.
(e) x × (y × z) = ⟨x, z⟩y −⟨x, y⟩z.
(f) ∥x × y∥2 = ∥x∥2∥y∥2 −⟨x, y⟩2.
(g) If x × y ̸= 0, then x × y is orthogonal to both x and y.
Proof. The proof of much of this proposition is routine and in Exercise 6 the reader
is asked to supply the details. (a) Regard the determinant deﬁnition of the cross
product and realize that part (a) is a consequence of two facts about determinants.
x × x = 0 is a consequence of the fact that a determinant is 0 when two rows are
identical. x × y = −y × x is a consequence of the fact that when two rows in a
determinant are interchanged, the new determinant is the negative of the other.
(b) and (c). Just use the deﬁnition of the cross product,
(d), (e), and (f). By the deﬁnition of the cross product, x × y = (x2y3 −
x3y2, x3y1 −x1y3, x1y2 −x2y1). Take the inner product with z. Now write out the
cross product of y × z and take its inner product with x. Compare the two calcula-
tions to see that they are equal. Expand the determinant in (d) by minors using the
ﬁrst row to see that this equals the formula for ⟨x, y × z⟩. (e) Compute both sides
and compare. (Ugh!) Follow the same instructions to show (f).
(g) By (d) and (a), ⟨x × y, y⟩= ⟨x, y × y⟩= 0. Similarly, ⟨y × x, x⟩= ⟨y, x ×
x⟩= 0.
■
Now to ﬁx some notation that will help us explore what we mean by an orientation
of a 2-surface.

8.3 Surfaces
273
8.3.6. Definition. If (, R) is a 2-surface in R3 with  = (φ1, φ2, φ3), then
u = ∂
∂u =

∂φ1
∂u , ∂φ2
∂u , ∂φ3
∂u

∈R3
v = ∂
∂v =

∂φ1
∂v , ∂φ2
∂v , ∂φ3
∂v

∈R3
Note that u and v are precisely the ﬁrst and second columns in the matrix of
D(u, v) in L(R2, R3). So u, v are continuous functions from R into R3. Thus
we obtain another function u × v : R →R3; by a calculation
u × v = det
⎡
⎣
i
j
k
(φ1)u
(φ2)u
(φ3)u
(φ1)v
(φ2)v
(φ3)v
⎤
⎦
=

φ2,φ3, φ3,φ1, φ1,φ2

For any point (u, v) in R, deﬁne
8.3.7
N(u, v) = (u × v)(u, v) =

∂(φ2, φ3)
∂(u, v) , ∂(φ3, φ1)
∂(u, v) , ∂(φ1, φ2)
∂(u, v)

8.3.8. Example. Let  be an open subset of R2, let R be a surface domain contained
in , and assume f :  →R is a smooth function. If  is given by (u, v) =
(u, v, f (x, y)), then u = (1, 0, fu) and v = (0, 1, fv). It follows that N(u, v) =
(−∇f (x, y), 1). See Exercise 7.
8.3.9. Proposition. If (, R) is a 2-surface in R3 and there is a point (u0, v0) in
int R such that N(u0, v0) ̸= 0, then N(u0, v0) is orthogonal to every smooth curve
that lies in {} and passes through the point (u0, v0).
Proof. Let w0 = (u0, v0) ∈int R and let ζ0 = (u0, v0) ∈{}. First note that if
N(w0) ̸= 0, then at least one of its coordinates is not 0; without loss of generality
we may assume that φ1,φ2(w0) ̸= 0. By Proposition 8.3.4 there is an open set  in
R2 and a smooth function f :  →R such that
ζ0 ∈S = {(u, v, f (u, v)) : (u, v) ∈} ⊆(R)
Thus to prove the proposition it suﬃces to assume that  is given by ((u, v) =
(u, v, f (u, v)) for a smooth function f :  →R as in Example 8.3.8. Using that
example and Proposition 6.7.8, this ﬁnishes the proof.
■
In analogy with Deﬁnition 6.7.10 we present the following.
8.3.10. Definition. If (, R) is a 2-surface in R3 and (u0, v0) ∈int R such that
N(u0, v0) ̸= 0, then the tangent plane to {} at the point (u0, v0) is deﬁned as
(u0, v0) + {ζ ∈R3 : ζ ⊥N(u0, v0)}

274
Curves and Surfaces
Here is another deﬁnition.
8.3.11. Definition. A 2-surface (, R) in R3 is said to be regular if N(u, v) ̸= 0
for all (u, v) in int R.
So for a regular surface we have a well-deﬁned normal vector at every point of
the interior of its domain. Now we want to examine what happens when we change
variables.
8.3.12. Proposition. If (, R) and (, Q) are 2-surfaces in R3 and τ : Q →R is
a mapping that is smooth in some neighborhood of Q such that  =  ◦τ, then for
all (u, v) in int Q we have that
N(u, v) = τ (u, v)N(τ(u, v)) = ∂(τ1, τ2)
∂(u, v) N(τ(u, v))
Proof. This is basically an application of the Chain Rule. As usual we set  =
(φ1, φ2, φ3) and  = (ψ1, ψ2, ψ3). For 1 ≤i < j ≤3 and the fact that (ψi, ψj) =
(φi, φj) ◦τ, a computation shows that (ψi,ψj)(u, v) = τ (u, v) (φi,φ j)(u, v) for
all (u, v) in Q. The result now follows directly by computation.
■
8.3.13. Definition. Two regular 2-surfaces (, R) and (, Q) in R3 are equivalent
if there is a neighborhood  of Q and a bijective regular mapping τ from  onto a
neighborhood of R such that τ(Q) = R and  =  ◦τ.
It follows that this is indeed an equivalence relation between 2-surfaces (Exer-
cise 8). Technically we should deﬁne a 2-surface as an equivalence class of what
we have deﬁned as a 2-surface. Indeed in many books this is the approach taken.
Our approach of deﬁning a surface as a function was taken for simplicity and ease
of exposition. Nevertheless we will still have to show that various properties of sur-
faces remain the same for equivalent surfaces.
Note that in practice when we want to deﬁne a map τ to demonstrate that two
surfaces are equivalent, we won’t specify the neighborhood  on which τ is deﬁned
but only deﬁne τ on the set Q. The deﬁnition of τ on a neighborhood will be clear.
Exercises
(1)
Let R > 0 and put S = {x ∈R3 : ∥x∥= R}. Find a surface (, R) such that
{} = S.
(2)
Let T = [0, 2π] × [0, π] and deﬁne  : T →R3 by
(u, v) = (cos u cos v, sin u cos v, sin v)
Show that (, T ) is a surface whose trace is the hemisphere as in Example 8.3.3.
(3)
Verify the statement in Example 8.3.3(c).
(4)
In Example 8.3.3(d), what is the image of the segment [−π, π] × {v}? What are
the images of the segments [−π, π] × {±π}? Verify that ([−π, π] × [−π, π])
is a torus.

8.4 Integration on Surfaces
275
(5)
If R = [0, 2π] × [0, b] and  : R →R3 is deﬁned by (u, v) = (v cos u,
v sin u, v), show that  is a 2-surface whose trace is the truncated cone {(x, y, z) :
z =

x2 + y2, 0 ≤z ≤b}.
(6)
Supply the missing details in the proof of Proposition 8.3.5.
(7)
In Exercise 2 above calculate N.
(8)
Show that the concept of equivalence deﬁned in Deﬁnition 8.3.13 is an equivalence
relation on the collection of all 2-surfaces in R3.
8.4. Integration on Surfaces
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In this rather long section we’ll deﬁne an integral on a regular 2-surface using the
normal vector and explore its properties. We’ll also introduce orientable surfaces
and the concept of the positive orientation of a surface relative to a solid it surrounds.
8.4.1. Definition. If (, R) is a regular 2-surface in R3 and f : {} →R is a con-
tinuous function, deﬁne the surface integral of f over  as


f dσ =

R
f ◦ ∥N∥=

R
f ((u, v)) ∥N(u, v)∥dudv
and deﬁne the surface area of {} as
σ () =

R
∥N((u, v))∥dudv
Why deﬁne the integral this way?
It is possible to look at this formula in a way that makes it seem natural once
we justify the the formula for σ (); let’s undertake this. Imagine approximating
the area of {} by the sum of small rectangles contained in the tangent planes to
the surface. The area of each of these small rectangles is close to the length of
the normal vector ∥N∥times the area of the underlying rectangle in the surface
domain R. Once you accept this we see that this leads to the integral deﬁning σ ()
in the deﬁnition. We’ll avoid the details but a web search may shed some light on
such an undertaking. Once this is established we see that the deﬁnition of

 f dσ
is obtained by approximating by the sum of the values of the function f times a
small amount of area of the underlying surface.
The reasons for looking at this integral are rooted in physics, but we won’t go
into this; the interested reader can do a web search for “vector ﬁelds” and “surface
integral.”
We’ll look at some examples below that show that the above formula gives the
value of the surface area we expect. First let’s point out that the surface area and
surface integrals for equivalent regular surfaces are equal (Exercise 6).
8.4.2. Example.
(a) Let’s start with a rectangle in the xy-plane considered
as a subset of R3. Let R = [a, b] × [c, d] ⊆R2 and deﬁne (u, v) = (u, v, 0).

276
Curves and Surfaces
Here u(u, v) = (1, 0, 0) and v(u, v) = (0, 1, 0) for all (u, v). It follows that
N(u, v) = (0, 0, 1). Thus
σ () =

R
∥(0, 0, 1)∥dudv = (b −a)(d −c)
as expected. We might give a preview here of something we will soon do, namely
discuss the surface area of a box in R3. This is an example of a non-smooth surface,
but we can call it a piecewise smooth surface. We can approach the calculation of
this surface area by partitioning the surface of the box into six smooth surfaces,
calculating each of their areas and adding them together.
(b) How about a circular disk in the xy-plane? The approach is similar to what
we did in part (a) and this is left for the reader as Exercise 1.
(c) Calculate the surface area of a truncated cylinder of radius 1. Recall from
Example 8.3.3(c) that we let R = [0, 2π] × [0, 2] and deﬁne the surface  :
R →R3 by (θ,t) = (cos θ, sin θ,t); and {} = {(x, y,t) ∈R3 : x2 + y2 = 1,
0 ≤t ≤2}. So θ(θ,t) = (−sin θ, cos θ, 0) and t(θ,t) = (0, 0, 1). Hence
N(θ,t) = (cos θ, sin θ, 0) for all (θ,t) in R and ∥Nφ∥= 1. Thus σ () =
 2π
0
 2
0 dtdθ = 4π. If you slit the cylinder vertically and lay it ﬂat, you get a rectan-
gle with sides of length 2π and 2. So again the formula from Deﬁnition 8.4.1 yields
the correct answer.
(d) The area of a graph of f : R →R. Here (u, v) = (u, v, f (u, v)), so that
N(u, v) = (−fu(u, v), −fv(u, v), 1). Thus
σ () =

R

1 + fu(u, v)2 + fv(u, v)2 dudv
We need to introduce additional concepts.
I ask the reader to be patient when (s)he reads the next deﬁnition as it may seem
strange. We’ll have a discussion after the statement that I think will help. Remember
that an open disk in R2 is a set of the form B((a, b); r).
8.4.3. Definition. If (, R) is a smooth surface in R3 and (x, y, z) is a point in
{}, say that (x, y, z) is an interior point of  if there is a homeomorphism of an
open disk in R2 onto a relatively open subset of {} that takes the center of the disk
onto (x, y, z). Let int  denote the set of interior points of . A point of {} is a
boundary point if it is not an interior point. We denote the set of boundary points
by ∂. The surface is said to be closed if it has no boundary points.
The ﬁrst comment is that this terminology is standard even though it introduces
an ambiguity with the topological terms interior, boundary, and closed as well as the
notation for the interior and boundary. On the other hand the deﬁnition of an interior
point and boundary point of a surface is consistent with the topological notions if
we think of the relative topology. The deﬁnition of a closed surface is completely at
odds, however, with its topological cousin. This all seems unfortunate but in practice
it should not lead to confusion. We’ll see some examples shortly.

8.4 Integration on Surfaces
277
Realize that the trace of a two-surface, {}, is the continuous image of the com-
pact set R so topologically it is always closed in R3. Also it is a two-dimensional
object sitting inside R3 and so it cannot have any interior points in the topological
sense. Thus in the topological sense, every point is a boundary point. With such
observations we see that to a large extent the topological versions of the words are
irrelevant for a discussion of a surface and won’t arise very often. Nevertheless to
aid in making this distinction we wiil use the symbols int  and ∂ for the interior
and boundary points of the surface as deﬁned above and always use {} to discuss
topological properties of the trace of the surface.
It is useful to be aware of an intuitive or visual way of deﬁning the interior points
of a surface. Imagine (and do not take this literally) you are standing at some point
of the surface and you decide to move a small amount. If it is the case that you
can move in any direction while remaining on the surface, then you are standing at
an interior point. If there is some direction such that no matter how small a step
you take in that direction you must fall oﬀthe surface, then you are standing at a
boundary point.
Now let’s consider a few examples. Consider the surface  whose trace is the
rectangle sitting inside the xy-plane in R3 as introduced in Example 8.4.2(a) above.
Here ∂ is the set of four edges while the other points are in int . The sphere in
R3 is a closed surface since every point is an interior point. The same is true of the
torus. The hemisphere, however, is not a closed surface.
The proof of the next result is an application of the Inverse Function Theorem
and is left to the reader as Exercise 7.
8.4.4. Proposition. If (, R) is a regular 2-surface, then the image of the topolog-
ical interior of R is contained in int .
8.4.5. Definition. Say that a regular surface (, R) is oriented if  is aC′′ function
and for (u1, v1), (u2, v2) in R with (u1, v1) = (u2, v2) in int , it follows that
N(u1, v1)
∥N(u1, v1)∥=
N(u2, v2)
∥N(u2, v2)∥
If (, R) is oriented and (x0, y0, z0) = (u0, v0) ∈int , we deﬁne the unit normal
to be the vector-valued function n : int  →R3 by
n(x0, y0, z0) =
N(u0, v0)
∥N(u0, v0)∥
When (, R) is oriented with unit normal n and F : {} →R3 is a continuous
function, then the oriented surface integral of F on  is deﬁned as


F · n dσ =

R
⟨(F ◦)(u, v), N(u, v)⟩dudv
See Exercise 8.

278
Curves and Surfaces
The condition that n = N/∥N∥is the same unit vector at points where  takes
on the same value means that the vector N points in the same direction at such
points. Hence the deﬁnition of n is unambiguous.
The reader may be perplexed when

 F · n dσ is deﬁned since N is not nor-
malized in the integral on the right-hand side of the formula. But this happens
because we deﬁned dσ = ∥N(u, v)∥dudv in (8.4.1) so that this occurrence of
∥N(u, v)∥cancels the one in the denominator of the deﬁnition of n. Finally we
have used the boldfaced notation n for unit normal after having said previously that
we won’t use boldface to denote vectors. Alas, sometimes clarity must trump doc-
trine. The letter n is used too often as a subscript or as the number of elements in
a set to let it be used in its naked state as an important vector-valued function. In
addition this is the traditional notation.
8.4.6. Example. When (, R) is a regular surface in R3, suppose the continuous
function F : {} →R3 is given by
F(u, v) = (P(u, v), Q(u, v), R(u, v))
Using (8.3.7) we have that

 F · n dσ equals


F · n dσ =

R
9
(P ◦, Q ◦, R ◦),

φ2,φ3, φ3,φ1, φ1,φ2
:
dudv
=

R
$
(P ◦)φ2,φ3 + (Q ◦)φ3,φ1 + (R ◦)φ1,φ2
%
dudv
Symbolically we write this as
8.4.7


F · n dσ =


P dydz +


Q dzdx +


R dxdy
We’ll have more to say about this when we encounter diﬀerential forms in the next
chapter.
8.4.8. Example. The Möbius3 Strip is the most famous of all non-oriented sur-
faces. I suspect that every reader has come across this surface in some form,
and many of you may have constructed one by giving a strip of paper a single
twist. If you do this you see that you obtain a surface with “one side.” This is
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
3 August Ferdinand Möbius was born in 1790 in Schulpforta, Saxony; his father was a dancing teacher and died
when Möbius was three years old. He was educated at home until he was 13 and then was enrolled in school. In
1809 he entered the University of Leipzig. In spite of his interest in mathematics, he began by studying law at the
urging of his family. He soon decided to follow his own inclinations and switched to mathematics, astronomy,
and physics. In 1813 he went to Göttingen to study astronomy with Gauss. He then went to Halle to study
mathematics with Pfaﬀ, Gauss’s teacher. He soon completed a doctoral thesis in mathematics and astronomy
and in 1816 he was appointed to the faculty at the University of Leipzig. In spite of not being able to secure
a post at Leipzig that was more prestigious than the one he had and in spite of oﬀers from other universities,
he remained at Leipzig. (Apparently he was not a good teacher.) Finally in 1844 he received an oﬀer from the
University of Jena; to counter this, Leipzig oﬀered him a full professorship in astronomy. He made contributions
to projective geometry and other areas of mathematics and astronomy. His discovery of the strip described here
was part of a larger work on one sided surfaces. It must be mentioned that the strip was independently discovered
at about the same time by Johann Listing. Möbius died in 1868 in Leipzig.

8.4 Integration on Surfaces
279
the trait of not being oriented, but let’s examine this strip from a mathematical
point of view and match it with the deﬁnition of an oriented surface given
above. Let R = [−π, π] × [−1, 1] and deﬁne  : R →R3 by (u, v) = ((2 +
v sin(u/2)) cos u, (2 + v sin(u/2)) sin u, v cos(u/2)). Sketching any surface in
R3 is a challenge, but doing this one is particularly diﬃcult and will tax your artistic
abilities; but it does give the Möbius Strip. Note that if u is a constant, then as v
varies from −1 to 1 a straight line segment Lu is transcribed. As u progresses from
−π to π this segment Lu rotates. It is this rotation that creates a lack of orientation
and leads us to say it is a surface with only one side. In Exercise 9 the reader is
asked to show the (, R) is not an oriented surface.
8.4.9. Definition. If two oriented surfaces (, R) and (, Q) are equivalent with a
smooth, bijective mapping τ : Q →R such that  =  ◦τ as in Deﬁnition 8.3.13,
then they are orientation equivalent if the Jacobian of τ satisﬁes τ (u, v) > 0 for
all (u, v) in Q.
As we pointed out when we deﬁned equivalent surfaces earlier, since the sur-
faces are regular it follows that τ (u, v) ̸= 0 for all (u, v) in Q. But because Q is
connected it follows that the Jacobian must be always positive or always negative.
8.4.10. Proposition. Let (, R) and (, Q) be equivalent oriented surfaces with
a smooth, bijective mapping τ : Q →R such that  =  ◦τ and let F be a contin-
uous function from {} = {} into R3.
(a)
If the surfaces are orientation equivalent, then

 F · n dσ =

 F ◦τ ·
n dσ.
(b) If the surfaces are not orientation equivalent, then

 F · n dσ = −

 F ◦
τ · n dσ.
Proof. (a) Assume τ (u, v) is always positive. By Proposition 8.3.12, N(s,t) =
τ (s,t)N(τ(s,t)). Hence


F · n dσ =

Q
⟨(F ◦)(s,t), N(s,t)⟩dsdt
=

Q
⟨(F ◦ ◦τ )(s,t), N(τ(s,t))⟩τ (s,t) dsdt
=

Q
H ◦τ(s,t)|τ (s,t)| dsdt
where H : R →R is deﬁned by H(u, v) = ⟨F((u, v)), N(u, v)⟩. Now using the
COV Theorem (7.4.4) we get that


F · n dσ =

R
H(u, v) dudv =


F · n dσ
The proof of (b) is Exercise 10.
■

280
Curves and Surfaces
8.4.11. Example. (a) Let R = [a, b] × [c, d] and Q = [0, 1] × [0, 1]. Deﬁne τ :
Q →R by
τ(s,t) = ((1 −s)a + sb, (1 −t)c + td)
A calculation shows that τ (s,t) = (b −a)(d −c) > 0 everywhere on the unit
square. Thus every regular surface with surface domain a rectangle is orientation
equivalent to a surface with domain the unit square.
(b) Let (, R) be a regular surface and put Q = {(v, u) : (u, v) ∈R}. Deﬁne
τ : Q →R by τ(v, u) = (u, v). A computation shows that Dτ =
$0
1
1
0
%
so that
τ (v, u) = −1 everywhere on Q. If (, Q) is the regular surface deﬁned by  =
 ◦τ, then this surface is equivalent to (, R) but not orientation equivalent. In
fact n(v, u) = −n(u, v). In other words, if we are given any regular surface
(, R) there is an equivalent surface (, Q) such that at every point of {} = {},
n = −n.
In (8.2.2) we deﬁned what it means for a piecewise smooth curve γ in R2 that
forms part of the boundary of an open set G in R2 to be positively oriented relative to
G. We’d like to extend this to curves that form part of the boundary of an oriented
surface (, R). In fact for an oriented surface (, R) it is often the case that ∂
consists of a ﬁnite number of piecewise smooth curves. (For example, if  is a
hemisphere or a rectangle.) We would like to induce an orientation on the curves
in ∂ relative to the orientation of (, R). This can be done in a precise manner,
but that requires more concepts and will be postponed until the next chapter when
we discuss diﬀerential forms in higher dimensional Euclidean space. Here we give
a heuristic description of this induced orientation.
Suppose we are given a point (x, y, z) = (u, v) on the surface. Near this point
the surface has two sides, including one where the normal vector n is pointing. That
side where n is pointing is called the positive side of  near this point. Assume γ is
one of the curves that make up ∂, and imagine you are walking along γ with your
head pointing in the direction of n; that is, your head is in the positive side of .
We say you are going in the positive direction of γ relative to  provided {} is
on your left. This is also called the right-hand orientation of ∂. Notice that this
is consistent with our deﬁnition of the positive direction of a curve in R2 given in
(8.2.2). In fact let (, R) be a surface with {} a subset of the xy-plane in R3 and
∂ is a piecewise regular curve γ . Assume the unit normal vector n for  points
upward into the half-space {(x, y, z) : z ≥0}. Then the positive direction of γ as
discussed above is the counterclockwise direction as discussed in §8.2.
It must be emphasized that the positive direction of γ depends on the orientation
of the surface whose boundary includes γ . We’ll see that in the following example,
especially part (b).
8.4.12. Example. (a) Consider the hemisphere centered at the origin of radius r
that lies above the xy-plane; so R = {(u, v) ∈R2 : u2 + v2 ≤r2} and (u, v) =
(u, v,
√
r2 −u2 −v2). (This  is not regular in a neighborhood of R as required;

8.4 Integration on Surfaces
281
a correct parametrization can be found in Exercise 8.3.2.) Here ∂{} is the circle
{(u, v, 0) : u2 + v2 = r2}. It follows that
N =

u
√
r2 −u2 −v2 ,
v
√
r2 −u2 −v2 , 1

(See Exercise 2.) So the positive direction for this surface is upwards and the positive
direction for ∂{} is to go along this circle in the counterclockwise direction.
(b)
Suppose
0 < a < b < ∞,
R = {(u, v) ∈R2 : a2 ≤u2 + v2 ≤b2},
and
(u, v) = (u, v, 0). So {} = R × {0} and ∂{} consists of two circles γ1 and γ2
with {γ1} = {(u, v, 0) : u2 + v2 = b2}, {γ2} = {(u, v, 0) : u2 + v2 = a2}. The unit
normal vector for {} is e3, the positive direction for γ1 is counterclockwise, and
the positive direction for γ2 is clockwise.
Now we extend the ideas of regular surfaces and oriented surfaces to piecewise
versions to accommodate examples such as the boundary of a cube in R3.
8.4.13. Definition. If (i, Ri) is a surface for 1 ≤i ≤n and  = n
i=1{i}, then
 is called a piecewise regular surface if the following conditions are satisﬁed:
(i) each (i, Ri) is a regular surface; (ii) for i ̸= j either {i} ∩{j} = ∅or {i} ∩
{j} ⊆∂i ∩∂j; (iii) for three distinct indices in {1, . . . , n}, {i} ∩{j} ∩{k}
is either empty or ﬁnite. The boundary of , ∂, is deﬁned as the closure of the set
of points x in R3 such that for some index i, x ∈∂i but x /∈∂j for all j ̸= i.
The surface area of  is deﬁned by
σ () =
n

i=1
σ (i)
If f :  →R is a continuous function, then


f dσ =
n

i=1

i
f dσ
There is a certain inconsistency in the above deﬁnition in that the surface  is
described as a set of points rather than a function. There are some things we could
do to make it a function, but they could easily be considered contrived and it doesn’t
seem worth the eﬀort. It’s better to live with this inconsistency. Besides the surface
of a cube, there are other examples of sets  that we want to consider that are not
surfaces as in Deﬁnition 8.3.1 since they are not connected. The ﬁrst such example
is ∂S where S is the corona of Example 8.3.2. Since ∂S is not connected it is not a
surface but it does satisfy the deﬁnition of a piecewise regular surface. Let’s point
out a similarity to the deﬁnition above and that of a G-set (8.2.6).
The preceding deﬁnition seems plain enough, but if  is described geometrically
as a set of points in R3, it may be diﬃcult to ﬁnd the regular surfaces (i, Ri) needed
to make the piecewise regular surface ﬁt the deﬁnition. Let’s look at an example
taken from [15].

282
Curves and Surfaces
8.4.14. Example. If  is the surface of the tetrahedron in R3 resulting from tak-
ing the topological boundary of the solid bounded by the four planes x = 0, y = 0,
z = 0, and x + y + z = 1, compute

 f dσ when
f (x, y, z) = x + y2 + z3.
 has four faces {(i, R) : 1 ≤i ≤4}, where each face has the same surface
domain R = {(u, v) : 0 ≤u ≤1, 0 ≤v ≤1, u + v ≤1} (the triangle in R2 with
vertices (0, 0), (0, 1), (1, 0)); and for each (u, v) in R we deﬁne 1(u, v) =
(u, v, 0), 2(u, v) = (0, u, v), 3(u, v) = (u, 0, v), 4(u, v) = (u, v, 1 −u −v).
A computation shows that ∥Ni∥= 1 for i = 1, 2, 3 and ∥N4∥=
√
3. Setting up
the integrals and performing the computations (Exercise 15) shows that


f dσ = 3
10(2 +
√
3)
Deﬁning what is meant by a piecewise regular surface to be orientable is tricky.
For 1 ≤i ≤n let (i, Ri) be a surface such that  = n
i=1{i} is a piecewise regu-
lar surface. If we want to have  orientable, we clearly must have each i orientable;
this is not suﬃcient as the reader can see by using the Möbius Strip (Exercise 19).
We need to be sure the direction of the various normals Ni are consistent. This
is easy with something like the surface of a box where for each of the six surfaces
that correspond to the surfaces of the box we can choose the outward (or inward)
pointing normal vector.
The next deﬁnition is again heuristic, but it suﬃces for the examples we will see.
In the next chapter we will use a diﬀerent approach and make it all precise.
8.4.15. Definition. For 1 ≤i ≤n let (i, Ri) be a surface such that  = n
i=1{i}
is a piecewise regular surface and each ∂i is a piecewise regular curve that is pos-
itively oriented relative to i.  is a a piecewise orientable surface if the following
hold: (a) each (i, Ri) is oriented; (b) if 1 ≤i < j ≤n and ∂i ∩∂j ̸= ∅, then
the positive direction of any curve in this intersection relative to i is the oppo-
site of the positive direction of this curve relative to j; (c) the surfaces (i, Ri)
are so oriented that the directions of the normal vectors Ni are consistent on each
component of .
When  = n
i=1{i} is an oriented piecewise regular surface and F :  →R3
is a continuous function, deﬁne the surface integral by


F · n dσ =
n

i=1

i
F · ni dσ
The meaning of condition (c) is imprecise, but for most examples it becomes
clear. For example with a box it must be that the normal vectors for each face
must point outward or they must all point inwards. Or consider the boundary of the
corona, , a piecewise regular surface. Here we could make the normal vector on the
outer boundary point away from the origin and the normal vector on the inner bound-
ary point toward the origin. We can similarly orient the boundary of a cylinder.

8.4 Integration on Surfaces
283
Exercises
(1)
Give the details to show that the deﬁnition of surface area in (8.4.1), when applied
to {} = {(x, y, 0) : x2 + y2 ≤r2}, yields σ ({}) = πr2.
(2)
Let (, R) be the hemisphere of radius r as in Example 8.4.12(a). (a) Verify the
formula for N given there. (b) Find σ (). (c) Find

 f dσ when {} is the
hemisphere of radius r centered at the origin and lying above the xy-plane and
f (x, y, z) = √z.
(3)
Find the surface area of the torus (Example 8.3.3(d)).
(4)
Find

 z dσ, where {} is the upper half of the sphere centered at the origin in
R3 and having radius 2.
(5)
Find



x2 + y2 dσ if {} is the surface of the circular cone sitting on the xy-
plane with base {(x, y) : x2 + y2 ≤2} and height 3.
(6)
If (, R) and (, Q) are equivalent regular surfaces, show that σ ({}) = σ ({});
and if f is a continuous real-valued function on {} = {}, then show that

 f dσ =

 f dσ.
(7)
Prove Proposition 8.4.4.
(8)
If (, R) is given as the graph of a smooth function f : R →R (Example 8.4.2(d)),
is it oriented?
(9)
In the Möbius Strip (Example 8.4.8) ﬁnd distinct points (u1, v1) and (u2, v2) in R
such that (u1, v1) = (u2, v2) but
N(u1, v1)/∥N(u1, v1)∦= N(u2, v2)/∥N(u2, v2)∥
(10)
Prove part (b) of Proposition 8.4.10.
(11)
Show that there is an oriented surface (, R) such that {} = {(x, y, z) : z = x2 +
y2 and 0 ≤z ≤4} and determine the positive direction of ∂{}.
(12)
Consider the surface (, R) where R = [0, 2π] × [0, 2] and (u, v) = (cos u,
sin u, v). Determine the positive direction of each curve that makes up ∂{}.
(13)
Let R = [0, 2] × [0, 2] and deﬁne  : R →R3 by (u, v) = (u, v, 4u −v). If
F(x, y, z) = (3x2, −2xy, 8), ﬁnd

 F · n dσ.
(14)
(a) Find a surface (, R) such that {} = {(x, y, z) : x2 + z2 ≤1, y = x2 + z2},
show that it is oriented, and determine the positive direction of the curves com-
prising ∂{}. (b) Find

 F · n dσ when F(x, y, z) = (0, y, −z).
(15)
Carry out the computations in Example 8.4.14.
(16)
Find

 f dσ when f (x, y, z) = x2 + yz and  is the topological boundary of the
box [0, 1] × [0, 1] × [0, 1].
(17)
Find

(y + z)dσ where  is the surface in R3 bounded by the cylinder x2 + y2 =
3, the plane y + z = 4, and the disk x2 + y2 ≤3.
(18)
If
 = {(x, y, z) : z ≥0, x2 + y2 + z2 = 1} ∪{xyz : z = 0, x2 + y2 ≤1}
and
F(x, y, z) = (2x, 2y, 2z), ﬁnd

 F · n dσ.

284
Curves and Surfaces
(19)
Consider the Möbius Strip X (8.4.8) and for k = 1, 2 let Rk = [π(k −2), π(k −
1)] × [−1, 1]. Show that for the map  we can consider X as the piecewise regular
surface (, R1) ∪(, R2) and each (, Rk) is orientable.
8.5. The Theorems of Gauss and Stokes
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
This last section of the chapter contains two important theorems, but ﬁrst we need
two pieces of notation that the reader may recall from Calculus.
8.5.1. Definition. Let X be an open subset of R3 and suppose F : X →R3 is a
smooth function with F = (P, Q, R). The curl and divergence of F are the functions
curl F : X →R3 and div F : X →R deﬁned by
curl F =

∂R
∂y −∂Q
∂z , ∂P
∂z −∂R
∂x , ∂Q
∂x −∂P
∂y

and
div F = ∂P
∂x + ∂Q
∂y + ∂R
∂z
The readers who have not seen these before can be forgiven their bewilderment,
but realize that to remember the formulas we can do the following. Recall the def-
inition of the gradient of a diﬀerentiable function f : G →R, where G is an open
subset of R3, and deﬁne the symbol
∇=

 ∂
∂x, ∂
∂y, ∂
∂z

Treating ∇as a vector in R3 we have that symboically
curl F = ∇× F and div F = ∇· F
The concepts of the curl and divergence of a function originated with the study of
ﬂuid ﬂow.
Just as we did when we proved Green’s Theorem, we want to discuss particular
types of Jordan sets in R3.
8.5.2. Definition. Here we say that a Jordan subset X of R3 is Type I if there is a
compact rectangle S in R2 and regular functions f1, f2 : S →R such that f1(x, y) ≤
f2(x, y) for all (x, y) in S and X = {(x, y, z) : (x, y) ∈S, f1(x, y) ≤z ≤f2(x, y)};
X is Type II if there is a compact rectangle S in R2 and regular functions g1, g2 :
S →R such that g1(x, z) ≤g2(x, z) for all (x, z) in S and X = {(x, y, z) : (x, z) ∈
S, g1(x, z) ≤y ≤g2(x, z)}; X is Type III if there is a compact rectangle S in R2 and
regular functions h1, h2 : S →R such that h1(y, z) ≤h2(y, z) for all (y, z) in S and
X = {(x, y, z) : (y, z) ∈S, h1(y, z) ≤x ≤h2(y, z)}.
Once again, as we said in the preface to this chapter, at a few points in the remain-
der of this section we will rely more on the reader’s intuition and, perhaps, call on

8.5 The Theorems of Gauss and Stokes
285
him/her to ﬁll in more of the details. Most of this will occur when we deal with ori-
enting piecewise regular surfaces. In the next chapter these things will be rectiﬁed.
The next proof is a dramatic example of this approach.
8.5.3. Proposition. If X is either a Type I, Type II, or Type III set as deﬁned above,
then ∂X is a piecewise orientable surface.
Proof. We only prove this for Type I sets, so let X be such a set with the notation
as in the deﬁnition and assume the rectangle S = [a, b] × [c, d]. Put
∂Xa = {(a, y, z) : c ≤y ≤d, f1(a, y) ≤z ≤f2(a, y)}
∂Xb = {(b, y, z) : c ≤y ≤d, f1(b, y) ≤z ≤f2(b, y)}
∂Xc = {(x, c, z) : a ≤x ≤b, f1(x, c) ≤z ≤f2(x, c)}
∂Xb = {(x, d, z) : c ≤x ≤b, f1(x, d) ≤z ≤f2(x, d))}
∂X1 = {(x, y, f1(x, y)) : (x, y) ∈S}
∂X2 = {(x, y, f2(x, y)) : (x, y) ∈S}
It follows that ∂X is the union of these six surfaces, now called faces, and is a
piecewise regular surface. Moreover we leave it to the reader to show that each face
is orientable and if we take the normal vector to each face pointing outward from X
(that is away from int X), ∂X is positively oriented.
■
In analogy with Deﬁnition 8.2.6 we make the following deﬁnition.
8.5.4. Definition.
Let X be a compact Jordan subset of R3 whose topological
boundary is a piecewise regular surface that is positively oriented. Let {Xj : 1 ≤j ≤
m} be a collection of closed subsets of X. We say that {Xj : 1 ≤j ≤m} is a Type I
cover of X if each set Xj is Type I and the following are satisﬁed: (a) X = m
j=1 Xj;
(b) when 1 ≤i < j ≤m, Xi ∩Xj = ∂Xi ∩∂Xj; (c) if Xi, Xj, and Xk are three distinct
sets, then ∂Xi ∩∂Xj ∩∂Xk is either empty or a ﬁnite set; (d) each ∂Xj is oriented
and if 1 ≤i < j ≤m and Xi ∩Xj is not ﬁnite, then ni(x, y, z) = −n j(x, y, z) for
each (x, y, z) in Xi ∩Xj. The collection {Xj : 1 ≤j ≤m} is called a Type II cover
(respectively, Type III cover) if each set Xj is a Type II (respectively, Type III) set
and the analogues of conditions (a), (b), (c), and (d) above are also satisﬁed.
A G-set X in R3 is a compact Jordan subset of R3 whose topological boundary
is a piecewise regular surface with positive orientation and such that it has a Type I
cover as well as a Type II cover and a Type III cover.
Note that as a consequence of condition (d) in the above deﬁnition it follows that
for any smooth function F : X →R3 we have that
8.5.5

∂X
F · n dσ =
m

j=1

∂Xj
F · n dσ

286
Curves and Surfaces
8.5.6. Theorem (Gauss’s4 Theorem). If X is a G-set in R3 and F : X →R3 is a
smooth function, then

∂X
F · n dσ =

X
div F
(Gauss’s Theorem is also called the Divergence Theorem.)
Proof. If F = (P, Q, R), then we have that

X
div F =

X
∂P
∂x +

X
∂Q
∂y +

X
∂R
∂z
and from (8.4.7)

∂X
F · n dσ =

∂X
P dydz +

∂X
Q dzdx +

∂X
R dxdy
This reduces the proof to establishing three separate equalities that only involve the
scalar-valued functions P, Q, or R one at a time:

X
∂P
∂x =

∂X
P dydz

X
∂Q
∂y =

∂X
Q dzdx

X
∂R
∂z =

∂X
R dxdy
Let’s show that

∂X
R dxdy =

X
∂R
∂z
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
4 Johann Carl Friedrich Gauss was born in Brunswick, Germany in 1777. At the age of seven he began his school-
ing and from the start exhibited extraordinary talent in mathematics, making small discoveries of known results.
As a teenager he discovered the law of quadratic reciprocity and the prime number theorem. In 1795 he enrolled
in Göttingen University, but left in 1798 without a diploma in spite of great success that included the construc-
tion of a regular 17-gon with ruler and compass, the ﬁrst progress in such questions since the time of the ancient
Greeks. He returned to the university and received the degree the next year. In 1801 he published his book Dis-
quisitiones Arithmeticae, which was devoted to number theory except for the last section that contained his work
on the 17-gon. He also began extensive work in astronomy and made many signiﬁcant contributions, leading to
his appointment as director of the Göttingen observatory in 1807. Two years earlier he married. Sadly his wife
died four years later after giving birth to their second son, who also died shortly after his mother. A year later
he married his ﬁrst wife’s best friend and they had three children. Gauss’s professional life was long and ﬁlled
with discoveries. His motto was “Few but Ripe.” This meant he never published anything unless he had fully
developed the subject. He kept a notebook in which he wrote his ideas, many of which were eventually redis-
covered and were signiﬁcant for the progress of mathematics. He is thought by some to have been the greatest
mathematician ever. While there is no doubt that he was one of the giants of the profession, I’ve always thought
the comparison of achievement of people from diﬀerent epochs to be unwise. Suﬃce it to say that he did work
in diﬀerent areas of mathematics and whatever he turned his attention to resulted in extraordinary advancement.
He died in Göttingen in 1855.

8.5 The Theorems of Gauss and Stokes
287
Since X is a G-set there is a Type I cover {Xj : 1 ≤j ≤m}. Since the volume of
each ∂Xj is zero, we have that

X
∂R
∂z =
m

j=1

Xj
∂R
∂z
Therefore by (8.5.5) we need only prove that for 1 ≤j ≤m

Xj
∂R
∂z =

∂Xj
R dxdy
To simplify the notation we can assume that X is Type I and use the notation of
Deﬁnition 8.5.2. From Fubini’s Theorem and the Fundamental Theorem of Calculus
we have that

X
∂R
∂z =

S
 f2(x,y)
f1(x,y)
∂R
∂z (x, y, z) dz

dxdy
=

S
[R(x, y, f2(x, y)) −R(x, y, f1(x, y))]dxdy
Now let’s examine

∂X R dxdy. As in Proposition 8.5.3, ∂X is composed of six
faces: Xa, Xb, Xc, Xd, X1, X2. Note that on the ﬁrst four of these faces either the vari-
able x or variable y is held constant. Thus the integral of R over these faces is 0.
Therefore we have (Exercise 1) that
8.5.7

∂X
R dxdy =

S
[R(x, y, f2(x, y)) −R(x, y, f1(x, y))]dxdy
(Why the minus sign?) We note this is the same as the value of

X
∂R
∂z .
Now X also has a Type II and a Type III cover and we can use these to give proofs
that

∂X P dydz =

X
∂P
∂x and

∂X Q dzdx =

X
∂Q
∂y .
■
8.5.8. Example. This example shows how Gauss’s Theorem can be used to simplify
the evaluation of an integral.
(a) Deﬁne F : R2 →R2 by F(x, y, z) = (5x, 3y, 0) and let X be the unit sphere
in R3 centered at the origin and having radius 2. So X is a G-set. (Why?) Find

∂X F · n dσ. Note that div F = 5 + 3 + 0 = 8. So

∂X F · n dσ =

X 8 =
8VolX. From Example 7.4.19(b) we know the formula for the volume of this
sphere is 4π(2)3/3 = 32π/3, so

∂X F · n dσ = 256π/3. This was easier than
parametrizing the boundary of the sphere.
(b) Let F(x, y, z) = (2x −z, x2y, −xz2) and X = [0, 1] × [0, 1] × [0, 1] and
evaluate

∂X F · n dσ. Here div F = 2 + x2 −2xz so

∂X
F · n dσ =
 1
0
 1
0
 1
0
(2 + x2 −2xz) dxdydz = 11
6
This is easier than integrating F · n over the six diﬀerent sides of ∂X.

288
Curves and Surfaces
8.5.9. Example. Find

X z2 when X is the unit sphere in R3 centered at the origin.
Here we want to use Gauss’s Theorem to express this as an integral over ∂X. To
start we need a function F on R3 such that div F = z2. We’re free to choose any so
let’s pick the easiest: F(x, y, z) = (0, 0, 1
3z3). Now we need to parametrize ∂X. Let
S = [0, 2π] × [−π/2, π/2] and deﬁne  : S →∂X ⊆R3 by
(θ, v) = (cos θ cos v, sin θ cos v, sin v)
It follows that
θ × v = det

i
j
k
−sin θ cos v
cos θ cos v
0
−cos θ sin v
−sin θ sin v
cos v

= (cos θ cos2 v, sin θ cos2 v, sin v cos v)
Hence

X
z2 =

∂X

0, 0, 1
3z3

· n dσ
=

S

0, 0, 1
3(sin v)3

· (cos θ cos2 v, sin θ cos2 v, sin v cos v) dθdv
= 1
3
 2π
0
 π/2
−π/2
(sin v)3 sin v cos v dvdt
= 2π
3
 π/2
−π/2
(sin v)4 cos v dv
= 2π
3
2
5 = 4π
15
The next celebrated theorem examines the case of a surface like a hemisphere in
R3 whose boundary is a curve. We only prove this theorem for a special case where
the surface is the graph of a smooth function. Later in Theorem 9.5.1 we’ll prove a
generalization of this theorem and so it doesn’t seem justiﬁed to do the extra work
to prove the result here in R3.
8.5.10. Theorem (Stokes’s5 Theorem). If R is a G-set in R2 and (, R) is a regular
surface in R3 such that ∂ is a piecewise regular curve with positive orientation
• • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •
5 George Gabriel Stokes was born in County Sligo, Ireland in 1819. His father was a church minister and he
remained deeply religious his entire life. His early education was near home, but at the age of 13 he went to
school in Dublin where he stayed with his uncle. At 16, in 1835, he enrolled in Bristol College in England where
his mathematical talent was recognized. Two years later he entered Cambridge University where he remained as
a student and then a faculty member for the rest of his life. In 1842 he published papers on hydrodynamics, and in
particular incompressible ﬂuids, a subject he focused on for much of his life and from which the present theorem
emerged. He also made signiﬁcant contributions to the study of light. In 1857 he began a courtship with Mary
Susanna Robinson, the daughter of an astronomer at Armagh Observatory in Ireland. The courtship followed
a path diﬀerent from romances today in that it was accompanied by an extensive exchange of letters. They
married that year and this meant he had to resign his fellowship at Cambridge. (Five years later the university
did away with the rule that prohibited fellows from being married.) He and his wife had ﬁve children. As his
career progressed he turned his attention to administration and from 1887 to 1892 he was one of three members
of Parliament from the university. He died in Cambridge in 1903.

8.5 The Theorems of Gauss and Stokes
289
relative to , then for any smooth function F : {} →R3,

∂
F · T ds =


curl F · n dσ
Proof of a Special Case. Assume (x, y) = (x, y, f (x, y)), where f is a twice
continuously diﬀerentiable function f : S →R deﬁned on the G-set S. Thus for
 the unit normal vector is n = N/∥N∥where N = (−fx, −fy, 1). Putting F =
(P, Q, R) and using (8.1.9) we have that

∂
F · T ds =

∂
P dx + Q dy + R dz
To simplify matters we’ll assume that the topological boundary of the G-set S is
parametrized by a single piecewise regular curve γ deﬁned on [a, b] with γ (t) =
(x(t), y(t)). (If ∂S consists of a ﬁnite number of such curves, the argument that
follows is the same but notationally more cumbersome.) Thus ∂ is parametrized
by the function on [a, b] deﬁned by t →(x(t), y(t), f (x(t), y(t)).
Now in the integral above we have
dz = ∂f
∂x dx + ∂f
∂y dy
so that symbolically the integral above becomes

∂{}
F · T ds =

γ

P + R∂f
∂x

dx +

Q + R∂f
∂y

dy
To evaluate this integral we will use Green’s Theorem. To do this we need to calcu-
late the partial derivatives of the functions in parentheses, and to do this we use the
Chain Rule to get
∂
∂x

Q + R∂f
∂y

= ∂Q
∂x + ∂Q
∂z
∂f
∂x + ∂R
∂x
∂f
∂y + ∂R
∂z
∂f
∂x
∂f
∂y + R ∂2 f
∂x∂y
and
∂
∂y

P + R∂f
∂x

= ∂P
∂y + ∂P
∂z
∂f
∂y + ∂R
∂y
∂f
∂x + ∂R
∂z
∂f
∂y
∂f
∂x + R ∂2 f
∂y∂x
Now because f has continuous second partial derivatives, fxy = fyx. Therefore
∂
∂x

Q + R∂f
∂y

−∂
∂y

P + R∂f
∂x

=

∂R
∂y −∂Q
∂z
 
−∂f
∂x

+

∂P
∂z −∂R
∂x
 
−∂f
∂y

+

∂Q
∂x −∂P
∂y

= curl F · N
Putting all this together with Green’s Theorem, we have that

∂
F · T ds =

S
curl F · N =


curl F · n dσ
■

290
Curves and Surfaces
8.5.11. Example.
Green’s Theorem is a special case of Stokes’s Theorem, at
least when the G-set X has its boundary as a single piecewise regular curve. In
that case using the notation of both Green’s Theorem, let S = X and F(x, y) =
(P(x, y), Q(x, y), 0). Deﬁne  : S →R3 by (x, y) = (x, y, 0). In this case n = e3
and (curl F) × e3 = ∂Q
∂x −∂P
∂y , the term that appears in Green’s Theorem. The for-
mula in Stokes’s Theorem now yields Green’s Theorem.
8.5.12. Example.
Let  be the upper hemisphere x2 + y2 + z2 = 1 with upper
pointing normal vector and evaluate

(x3ey, −3x2ey, 0) · n dσ. If we want to do
this using Stokes’s Theorem we must ﬁnd a surface (, S) such that  = {} and
a function F such that curl F = (x3ey, −3x2ey, 0). The surface is the easiest: let
S = {(x, y) ∈R2 : x2 + y2 ≤1} and deﬁne (x, y) = (x, y,

1 −x2 + y2). To ﬁnd
F = (P, Q, R) we want the partial derivatives to satisfy
x3ey = Ry −Qz,
−3x2ey = Pz −Rx,
0 = Qx −Py
The process here is trial and error, so using the last of these three equations let’s start
by guessing that P = Q = 0. If we do this the ﬁrst two equations become Ry = x3ey,
Rx = 3x2ey. Aha! R = x3ey works. The appropriate curve for ∂{} = ∂ is the unit
circle in the xy-plane in the counterclockwise direction; that is, γ : [0, 2π] →R3
given by γ (θ) = (cos θ, sin θ, 0). By Stokes’s Theorem we have


(x3ey, −3x2ey, 0) · n dσ =

γ
F · T ds
=

γ
R dz
= 0
since z is constant along γ .
Exercises
Some of the exercises below were taken from websites where I could ﬁnd no authors
or other attribution.
(1)
Verify (8.5.7).
(2)
Let S be the sphere x2 + y2 + z2 = 9 in R3 and let F : R3 →R3 be deﬁned by
F(x, y, z) = (3x, 2y, 0). (a) Find a G-set X in R3 with ∂X = S. (b) Use Gauss’s
Theorem to evaluate

S F · n dσ.
(3)
Let S be the boundary of the unit ball in R3 with positive orientation. If F(x, y, z) =
(x3, y3, z3), evaluate

S F · n dσ.
(4)
Let X = {(x, y, z) ∈R3 : −1 ≤x ≤1, −1 ≤y ≤1, 0 ≤z ≤2} and deﬁne F :
R3 →R3 by F(x, y, z) = (y2z, y3, xz). Find

∂X F · n dσ.
(5)
If X is the unit sphere in R3 of radius 1, use Gauss’s Theorem to evaluate

X z2.

8.5 The Theorems of Gauss and Stokes
291
(6)
Let S be the boundary of the unit ball in R3 with positive orientation. If F(x, y, z) =
(x3, y3, z3), evaluate

S F · n dσ.
(7)
Compute

γ ⟨F, T⟩ds where γ is the circle in R3 {(x, y, z) : x2 + z2 = 1, y = 0}
with the counterclockwise direction and F(x, y, z) = (x2z +
√
x3 + x2 + 2, xy,
xy +
√
z3 + z2 + 2).
(8)
Deﬁne γ : [0, 2π] →R3 by γ (θ) = (0, 2 + 2 cos θ, 2 + 2 sin θ) and deﬁne F :
R3 →R3 by F(x, y, z) = (x2e5z, x cos y, 3y). Evaluate

γ F · T ds by using Stokes’s
Theorem.
(9)
Let S be the hemisphere z =

1 −x2 −y2. If n is the outward pointing normal, and
F(x, y, z) = (x, x, x2y3 log(z + 1)), ﬁnd

∂S curlF · n dσ.
(10)
Let S = {(x, y, z) : x2 + y2 + z2 = 1 and z ≥0}. (a) Give a parametrization (, R)
for S. (b) Use Stokes’s Theorem to evaluate

(x3ey, −3x2ey, 0) · n dσ.


9
Differential Forms
Diﬀerential forms constitutes an approach to multivariable calculus that simpliﬁes
the study of integration over surfaces of any dimension in Rp. This topic introduces
algebraic techniques into the study of higher dimensional geometry and allows us
to recapture with rigor the results obtained in the preceding chapter.
There are many approaches to deﬁning and exploring diﬀerential forms, all lead-
ing to the same objective. One is to just deﬁne a form as a symbol with certain prop-
erties. That was the approach taken when I ﬁrst encountered the subject. Though
this appeals to many and ultimately leads to the same objective, it strikes me as
inconsistent with the approach we have taken. So in this chapter I have decided to
follow the approach in [7] where a form is deﬁned in terms that are more in line
with what I think is the background of readers of this book. The reader might also
want to look at [1], [9], and [10] where there are diﬀerent approaches.
9.1. Introduction
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Here we will deﬁne diﬀerential forms and explore their algebraic properties and the
process of diﬀerentiating them. The ﬁrst deﬁnition extends that of a surface as given
in the preceding chapter.
9.1.1. Definition. Let q ≥1. A q-surface domain or just a surface domain is a
compact Jordan subset R of Rq such that int R is connected and R = cl (int R). A
q-surface in Rp is a pair (, R) where R is a q-surface domain and  is a function
from R into Rp that is smooth on some neighborhood of R. The trace of (, R) is
the set {} = (R) ⊆Rp. If G is an open subset of Rp and {} ⊆G, we say (, R)
is a q-surface in G; let Sq(G) be the collection of all q-surfaces contained in G. We
deﬁne a 0-surface in G as a constant function with value in G.
A comparison of the above deﬁnition with the deﬁnition of a 2-surface in
R3 (8.3.1) reveals a diﬀerence. In (8.3.1) we required the boundary of the surface
domain to consist of curves; here we do not. Indeed we cannot. If R ⊆R4, for exam-
ple, this would prohibit all but a few possible choices of R. As in the last chapter we
will sometimes describe a set S in Rp and say it is a q-surface. What we mean by
this is that there is a q-surface (, R) as deﬁned above with {} = S; again (, R)
will be called a parametrization of S.

294
Differential Forms
9.1.2. Example. Of course all the examples from the previous chapter are available,
but here are two more. (a) Let R = {u ∈Rp−1 : ∥u∥≤1} and let S be the unit hemi-
sphere in Rp: S = {(u, ζ ) ∈Rp−1 ⊕R = Rp : u ∈R and ζ =

1 −∥u∥2}. This is
a p −1 surface in Rp, though as we mentioned in Example 8.3.3(b) the function
 : R →Rp deﬁned by (u) = (u,

1 −∥u∥2) is not a parametrization since it
fails to be smooth in a neighborhood of R. In Exercise 8.3.2 a parametrization when
p = 3 was given, here is one when p = 4. Let T = [0, 2π] × [0, 2π] × [0, π] ⊆R3
and deﬁne  : T →R4 by
(u, v, w) = (cos u cos v cos w, sin u cos v cos w, sin v cos w, sin w)
The reader can check that (, T ) is a parametrization of S when p = 4. Also see
Exercise 1.
(b) Let 1 ≤q < p, let R be any surface domain in Rq, ﬁx indices i1, . . . , iq in
{1, . . . , p}, and let x0 be a ﬁxed vector in Rp. If  : R →Rp is deﬁned by (u) =
x0 + q
k=1 ukeik, where e1, . . . , ep is the standard basis for Rp, then (, R) is a
q-surface in Rp. (Exercise 2)
9.1.3. Definition. Let G be an open subset of Rp. A 0-form on G is a continuous
function f : G →R. When q ≥1, a diﬀerential form of order q or simply a q-form
on G is a function ω : Sq(G) →R deﬁned as follows. Let I be a ﬁnite collection of
subsets I = {i1, . . . , iq} of {1, . . . , p}, each of which has q elements, and suppose
that for each I in I we are given a continuous function fI = fi1,...,iq : G →R. For
any  : R →G in Sq(G) we deﬁne
9.1.4
ω() =


ω =

R

I∈I
fi1,...,iq((u))∂(xi1, . . . , xiq)
∂(u1, . . . , uq) du
This is symbolically written as
ω =

I∈I
fi1,...,iq dxi1 ∧· · · ∧dxiq
The q-form ω is smooth if the coeﬃcient functions fi1,...,iq are continuously diﬀeren-
tiable. If each coeﬃcient function is continuously diﬀerentiable up to and including
the derivatives of order n ≥1, then ω is said to be a q-form of order n.
Whew! That’s a complicated deﬁnition.
Let’s take it apart and examine it and then we’ll look at some examples. First
note that a q-form is a function deﬁned on the set of functions Sq(G). This is not
the only example you’ll ever see of a function whose domain is another collection of
functions. In fact you can interpret an integral as being just such a function. As we
said S0(G) = G, so a 0-form f on G acts on a point in G by evaluating the function
at the point. Next, for any q ≥1 the term dxi1 ∧· · · ∧dxiq in the deﬁnition can be
treated at this stage solely as notation. (The symbol ∧is read “wedge.”) Later we
will give it more content and develop ∧as a form of multiplication for diﬀerential

9.1 Introduction
295
forms, but now it’s just notation. The ﬁrst equality in (9.1.4), ω() =

 ω, is just
an alternate way of writing the value of the function ω at an element of its domain.
It is, however, a useful way of interpreting the value of ω at a point in its domain
and this interpretation will be ampliﬁed later. As stated, the indices i1, . . . , iq are
contained in {1, . . . , p}, and there is no guarantee that they are distinct; again, we’ll
see more on this in the next paragraph. There are, however, only a ﬁnite number of
the continuous functions fi1,...,iq. Yes, the sum inside the integral in (9.1.4) could
have been put outside the integral sign; that’s just the way we chose to do it.
The Jacobian that appears in (9.1.4) arises as follows. If  : R →G ⊆Rp
has coordinate functions (u) = (φ1(u), . . . , φp(u)), then the Jacobian is the
determinant of the mapping from R into Rq deﬁned by u = (u1, . . . , uq) →
(φi1(u), . . . , φiq(u)) ∈Rq. That is,
9.1.5
∂(xi1, . . . , xiq)
∂(u1, . . . , uq) = ∂(φi1, . . . , φiq)
∂(u1, . . . , uq)
Remember that  is assumed smooth in a neighborhood of R so the derivatives
present no problem and the Jacobian is a continuous function. Also note that if the
indices i1, . . . , iq are not distinct, the Jacobian is the determinant of a matrix with
identical columns and so it must be 0. Finally, if the formula in (9.1.4) reminds
you of the Change of Variable Theorem (7.4.4), it should. Suppose q = p and  ∈
Sp(G). So here we have a surface domain R contained in Rp and a function  :
R →G ⊆Rp that almost satisﬁes the hypothesis of Change of Variable Theorem
(7.4.4). If  does indeed satisfy this hypothesis, then for f ∈C(G),

(R)
f =

R
f ◦ | det[D]|
Except for the absolute value sign around the Jacobian of , this is precisely the
formula for ω() when ω is the p-form f dx1 ∧· · · ∧dxp. For a q-form ω with
q < p, as the notation ω() =

 ω suggests, we want to think of integrating ω over
the surface  where we deﬁne such an integral by transferring to the integral over
the surface domain in the way stipulated in formula (9.1.4).
Now for some examples. Study them carefully. You will notice that we have seen
diﬀerential forms many times before without ever calling them by this name.
9.1.6. Example. (a) Suppose q = p = 1, R = [a, b] ⊆Rq = R, and G is an open
subset of Rp = R. Here the only 1-form possible is ω = f dx. A 1-surface in G
is a continuously diﬀerentiable function  : [a, b] →G and, using the Change of
Variables Theorem,
ω() =

R
f ((u))′(u)du =

([a,b])
f (x)dx
We therefore see again the idea mentioned in the paragraph preceding this exam-
ple that diﬀerential forms are a way of deﬁning an integration of functions over a
surface. This will be reinforced as we develop the theory.

296
Differential Forms
(b) Let γ : [a, b] →Rp be a regular curve. As we pointed out, γ is a 1-surface if
we assume γ is smooth in a neighborhood of [a, b]. Assume the trace of γ is con-
tained in the open set G, and let F : G →Rp be a continuous function. If e1, . . . , ep
is the usual basis for Rp, then we set γj(t) = ⟨γ (t), ej⟩and Fj(x) = ⟨F(x), ej⟩. So
if we deﬁne the one form ω = p
j=1 Fj(x)dxj on G, then by deﬁnition
ω(γ ) =

γ
ω =

[a,b]
p

j=1
Fj(γ (t))γ ′
j(t)dt
=
 b
a
⟨F(γ (t)), γ ′(t)⟩dt
=

γ
F · T ds
So we see that the line integral deﬁned in (8.1.8) is an example of a 1-form. In fact the
general 1-form on Rp is ω = p
j=1 f j(x)dxj for continuous functions f1, . . . , fp.
(c) Let (, R) be a 2-surface in R3 and recall the deﬁnition of N = u × v
(8.3.7). Consider the 2-form ω = ∥N(u, v)∥du ∧dv. So
ω() =

R
f ((u, v)) ∥N(u, v)∥dudv =


f dσ
the surface integral deﬁned in (8.4.1).
(d) Again let (, R) be a regular 2-surface in R3 with {} ⊆G, an open subset
of R3. If F : G →R3 is a continuous function denoted by F = (P, Q, R), let ω be
the 2-form on G deﬁned by
ω = P dy ∧dz + Q dz ∧dx + R dx ∧dy
It follows that ω() =

 F · n dσ, the oriented surface integral as in (8.4.7).
(e) Let γ : [a, b] →R3 be a 1-surface in R3 and let ω be the 1-form ω = xdy +
ydx. It follows that

γ
ω =
 b
a
[γ1(t)γ ′
2(t) + γ2(t)γ ′
1(t)]dt = γ1(b)γ2(b) −γ1(a)γ2(a)
(f) A general (p −1)-form on Rp is
ω =
p

j=1
f j(x) dx1 ∧· · · ∧C
dxj ∧· · · ∧dxp
where the notation C
dx j means that the term dxj is missing.
We now proceed to develop some basic operations on q-forms, ﬁrst an arithmetic
and then a diﬀerential calculus for forms. To set the scene ﬁx an open subset G
in Rp and deﬁne Fq(G) to be the set of q-forms on G. If ω, ω1, ω2 ∈Fq(G), then,

9.1 Introduction
297
since they are all functions on Sq(G), for c ∈R we deﬁne
(c ω)() = c ω() and (ω1 + ω2)() = ω1() + ω2()
for every  in Sq(G). With these operations we see that Fq(G) is a vector space.
We want to deﬁne a multiplication of forms, which is more complex. To do this
we ﬁrst examine the representation of diﬀerential forms.
9.1.7. Proposition.
Let ω = f (x)dxi1 ∧· · · ∧dxiq ∈Fq(G) and let k and m be
distinct integers in {1, . . . , q}.
(a) If ik = im, then ω = 0.
(b) If k and m are distinct integers in {1, . . . , q} and
{i1, . . . , iq} = {j1, . . . , jq}
with ik = jm, im = jk, and iℓ= jℓwhen ℓ̸= k, m then ω = −f (x)dxj1 ∧· · · ∧
dx jq.
In words, part (a) of this proposition says that if any two indices in the deﬁni-
tion of ω are equal, then ω = 0; part (b) says that interchanging exactly two indices
produces the q-form −ω. In particular, when q = 2, dxi ∧dxj = −dxj ∧dxi.
Proof. (b) This follows from Corollary 6.4.22: if A and B are two q × q matrices
such that B is the same as A except that two rows are interchanged, then det B =
−det A. Therefore if η = f (x)dxj1 ∧· · · ∧dxjq and if  ∈Sq(G), then
η() =

R
f ((x))∂(xj1, . . . , xjq)
∂(u1, . . . , uq) du
= −

R
f ((x))∂(xi1, . . . , xiq)
∂(u1, . . . , uq) du
= −ω()
(a) We again use Corollary 6.4.22 to get that when a q × q matrix has two iden-
tical rows, its determinant is 0. If  ∈Sq(G), then in the deﬁnition of ω() the
Jacobian in that deﬁnition has two identical rows; so that ω() = 0.
■
9.1.8. Corollary. If ω = f (x)dxi1 ∧· · · ∧dxiq ∈Fq(G) and ω ̸= 0, then
ω = ± f (x)dxj1 ∧· · · ∧dxjq ∈Fq(G)
where j1 < · · · < jq and {j1, . . . , jq} = {i1, . . . , iq}.
The preceding proposition and its corollary allow us to give a canonical rep-
resentation of q-forms. First observe that by (9.1.7(a)), when q > p we have that
Fq(G) = (0) for any open subset G of Rp. Next if 1 ≤i1 < · · · < iq ≤p, we put
I = {i1, . . . , iq} and set
dxI = dxi1 ∧· · · ∧dxiq

298
Differential Forms
This is called a basic q-form. A small counting argument shows there are p!/q!
(p −q)! diﬀerent basic q-forms on Sq(G).
9.1.9. Corollary. If ω is a q-form on G, then
ω =

I
fIdxI
where I ranges over all strictly ordered q-tuples of the integers 1, . . . , p and each
fI ∈C(G).
Proof. If we are given any ω in Fq(G), by Corollary 9.1.8 we can replace the coef-
ﬁcient functions in the representation of ω as it appears in the deﬁnition as follows.
(i) Put fI = 0 if no permutation of the index set I appears. (ii) If some permuta-
tion of the strictly increasing q-tuple I does appear, then let fI be sum of all the
functions ± fi1,...,iq where {i1, . . . , iq} is a permutation of I and the ± sign is chosen
according to how {i1, . . . , iq} is permuted to obtain I. If this is done, we obtain the
representation as it appears in the corollary.
■
Such a representation of a q-form is called its standard representation.
9.1.10. Proposition. If ω = 
I fI(x)dxI is a q-form in its standard representation,
then ω = 0 in Fq(G) if and only if fI = 0 for every I.
Proof. One way is clear: if each fI = 0, then ω is the zero q-form. Conversely
assume that ω() = 0 for every  in Sq(G) and assume there is a strictly ordered
q-tuple K = {k1 < · · · < kq} and a point x in G with fK(x) > ϵ > 0. By the continu-
ity of fK there is a positive number a > 0 such that fK(w) > ϵ when |xj −w j| ≤a
for 1 ≤j ≤p. Let R = {u ∈Rq : |ui| ≤a when 1 ≤i ≤q}, note that the rectangle
R is a surface domain, and deﬁne  : R →Rp by (u) = x + q
i=1 uieki. As in
Example 9.1.2(g),  ∈Sq(G).
Now a computation shows that
∂(xk1, . . . , xkq)
∂(u1, . . . , uq) = 1
while for I ̸= K,
∂(xi1, . . . , xiq)
∂(u1, . . . , uq) = 0
since it is the determinant of a matrix having at least one row of zeros. Thus
ω() =

R
fK((u))du ≥ϵV (R) > 0
a contradiction.
■
If you know the language of modules, the preceding proposition says that the basic
q-forms constitute a basis for Fq(G) as a module over the ring C(G). If you don’t
know this language, don’t worry about it. The comment was made for the ediﬁcation

9.1 Introduction
299
of those who know the language and nothing later in this book depends on what
was just said. What is important about the preceding proposition is that when a
diﬀerential form is expressed in its standard representation that representation is
unique.
Now we want to multiply forms. Since each ω in Fq(G) is a function taking val-
ues in R, we might be tempted to deﬁne this as we would the product of any such
functions: (ω1ω2)() = ω1()ω2(). That’s legitimate as a deﬁnition, but there is
a big problem with this. Namely this product is not always a diﬀerential form as a lit-
tle experimentation will show. We need a deﬁnition where the product of two forms
is again a form. What we will deﬁne is the product of forms of diﬀerent dimensions.
We start with the case of a 0-form times a q-form.
9.1.11. Definition. If ω = 
I fI(x)dxI ∈Fq(G) and f ∈C(G), deﬁne
f ω =

J
f (x)gJ(x)dxJ
That was natural enough. To deﬁne the product of other forms we need to start
with basic forms of diﬀerent dimensions and, when we multiply them, produce a
form in the sum of the dimensions.
9.1.12. Definition. Let 1 ≤q, r ≤p and suppose I = {i1, . . . , iq} and J =
{j1, . . . , jr}, where i1 < · · · < iq and j1 < · · · < jr. The product or wedge prod-
uct of the q-form dxI and the r-form dxJ is the (q+r)-form denoted by dxI ∧dxJ and
deﬁned by
dxI ∧dxJ = dxi1 ∧· · · ∧dxiq ∧dxj1 ∧· · · ∧dxjr
In analogy with this deﬁnition and notation we could denote the product of a 0-
form f and a q-form ω by f ∧ω. Also note that we have that 0 ∧ω = 0. Maintain-
ing the notation of this deﬁnition, we observe that if I ∩J ̸= ∅, then dxI ∧dxJ = 0
since at least one index is repeated (9.1.7a). In particular if q + r > p, dxI ∧dxJ =
0. If I ∩J = ∅, let [I, J] be the reordering of I ∪J into an increasing sequence.
It follows that
dx[I,J] = ±dxI ∧dxJ
(See Exercise 3.)
9.1.13. Proposition. If I, J, K are, respectively, an ordered q-index, an ordered
r-index, and an ordered s-index; then
(dxI ∧dxJ) ∧dxK = dxI ∧(dxJ ∧dxK)
Proof. The proof of this proposition is not diﬃcult, but it is somewhat tedious.
The ﬁrst step is, however, easy: if any pair of the sets I, J, K have an element in
common, then both sides of the above equation are 0. Assuming that I, J, K are
pairwise disjoint, the idea is to let dx[I,J] = (−1)αdxI ∧dxJ as in Exercise 3, and
analogously dx[J,K] = (−1)βdxJ ∧dxK and dx[I,K] = (−1)γ dxI ∧dxK. If [I, J, K]

300
Differential Forms
is the ordering of I ∪J ∪K, we must show that
(dxI ∧dxJ) ∧dxK = (−1)α(−1)β+γ dx[I,J,K]
dxI ∧(dxJ ∧dxK) = (−1)β(−1)α+γ dx[I,J,K]
Thus completing the proof. Carrying out the details is left to the reader
(Exercise 4).
■
9.1.14. Definition. If ω = 
I fI(x)dxI ∈Fq(G) and η = 
J gJ(x)dxJ ∈Fr(G),
each in its standard representation, then their product, denoted by ω ∧η, is deﬁned
as the (q+r)-form on G
ω ∧η =

I,J
fI(x)gJ(x)dxI ∧dxJ
Let’s underline that in the preceding deﬁnition we took two forms in their standard
representation and multiplied them, but the form that is the product is not in its
standard representation.
9.1.15. Proposition.
If ω, ω1, ω2 ∈Fq(G), η, η1, η2 ∈Fr(G), and σ ∈Fs(G),
then:
(a) (ω1 + ω2) ∧η = ω1 ∧η + ω2 ∧η;
(b) ω ∧(η1 + η2) = ω ∧η1 + ω ∧η2; and
(c) ω ∧(η ∧σ ) = (ω ∧η) ∧σ.
The proof of this proposition establishing the distributive and associative laws for
the multiplication of diﬀerential forms is left to the reader (Exercise 5). As we have
already seen, the multiplication is not commutative.
Now we discuss diﬀerentiating smooth q-forms.
9.1.16. Definition. If f is a smooth 0-form on G, then its derivative is the 1-form
df =
p

i=1
(Di f )(x)dxi =
p

i=1
∂f
∂xi
(x)dxi
If ω = 
I fI(x)dxi is the standard representation of a smooth q-form on G, then its
derivative is the (q+1)-form
dω =

I
(dfI) ∧dxI
9.1.17. Example.
(a) Return to Example 9.1.6(a) where p = q = 1. If ω is the
1-form ω = f dx where f is a smooth function on [a, b], then dω = df ∧dx = 0
since it is a 2-form on the one-dimensional space. If we consider the 0-form f ,
then df = f ′(x)dx. Thus we can interpret the Fundamental Theorem of Calculus as
saying that the integral of the 1-form df over the 1-surface equals the integral of the
0-form f over its boundary, though we would have to give ∂[a, b] an orientation to
obtain the minus sign in f (b) −f (a).

9.1 Introduction
301
(b) Let p = q = 3, suppose X ⊆R3, and assume F : X →R3 is a smooth func-
tion given by F(x, y, z) = (P(x, y, z), Q(x, y, z), R(x, y, z)). Put η = Pdy ∧dz +
Qdz ∧dx + Rdx ∧dy as in Example 9.1.6(d). So using the fact that dy ∧dz ∧dx =
−dy ∧dx ∧dz = dx ∧dy ∧dz and dz ∧dx ∧dy = −dx ∧dz ∧dy = dx ∧dy ∧
dz
dη = dP ∧(dy ∧dz) + dQ ∧(dz ∧dx) + dR ∧(dx ∧dy)
= ∂P
∂x dx ∧dy ∧dz + ∂Q
∂y dy ∧dz ∧dx + ∂R
∂z dz ∧dx ∧dy
=

∂P
∂x + ∂Q
∂y + ∂R
∂z

dx ∧dy ∧dz
= (div F) dx ∧dy ∧dz
Recall Gauss’s Theorem where for an appropriate solid X with ∂X a 2-surface we
have

∂X F · n dσ =

X div F. Rephrasing this using the form η and its deriva-
tive we have that Gauss’s Theorem says that

∂X
η =

X
dη
That is, the integral of the derivative dη over the inside of X equals the integral of
the η over the boundary of X. With this interpretation we can rightfully call Gauss’s
Theorem an extension of the FTC to R3.
(c) As in part (b) let p = q = 3, suppose S ⊆R3, and assume F : S →R3 is a
smooth function given by F(x, y, z) = (P(x, y, z), Q(x, y, z), R(x, y, z)). This time
put ω = Pdx + Qdy + Rdz, and compute dω. Using the rules for diﬀerentiating and
multiplying forms we obtain
dω = dP ∧dx + dQ ∧dy + dR ∧dz
=

∂P
∂y dy + ∂P
∂z dz

∧dx
+

∂Q
∂x dx + ∂Q
∂z dz

∧dy +

∂R
∂x dx + ∂R
∂y dy

∧dz
=

∂R
∂y −∂Q
∂z

dy ∧dz +

∂P
∂z −∂R
∂x

dz ∧dx +

∂Q
∂x −∂P
∂y

dx ∧dy
You may recognize the coeﬃcients of the basic forms dy ∧dz, dz ∧dx, dx ∧dy,
which have been arranged here in a certain rotational pattern, as forming the com-
ponents of curl F. We will return to this later in this chapter when we prove a gen-
eralized version of Stokes’s Theorem.
(d) Suppose ω is a basic q-form: ω = dxI = dxi1 ∧· · · ∧dxiq. It follows that
dω = 0. In fact using the deﬁnition of diﬀerentiation of q-forms shows dω =
d(1 · dxI) = d(1) ∧dxI = 0.

302
Differential Forms
(e) Consider ω = f dx1, the 1-form on G ⊆Rp where p ≥2 and f is smooth. So
dω = df ∧dx1 =
= p

i=1
∂f
∂xi
(x)dxi
>
∧dx1
=
p

i=2
∂f
∂xi
(x) dxi ∧dx1
We now examine how diﬀerentiation of forms interacts with the algebraic oper-
ations as well as what happens when we take multiple derivatives. Regarding this
latter statement, if ω = 
I fIdxI ∈Fq(G), say that ω is a form of class C′′ if each
fI has two continuous derivatives. That is, for each I, D fI : G →Rp exists and also
D2 fI = D(D fI) : G →L(Rp) exists and is continuous. Equivalently, each fI has
continuous second partial derivatives.
9.1.18. Theorem. If G is an open subset of Rp, ω a smooth element of Fq(G), and
η a smooth element of Fr(G), then the following hold.
(a) When r = q, d(ω + η) = dω + dη.
(b) d(ω ∧η) = (dω) ∧η + (−1)qω ∧(dη).
(c) If ω is of class C′′, then
d(dω) = 0
Proof. The proof of (a) is routine. To prove (b) it suﬃces to show this under the
assumption that ω = f dxI and η = gdxJ. (Why?) First if q = r = 0 so that ω = f
and η = g, then the product rule for diﬀerentiating functions shows that (b) holds
here. When q, r ≥1, we have ω ∧η = fgdxI ∧dxJ. If I and J have a term in com-
mon, then both sides of the equation in (b) are 0. So assume I ∩J = ∅. Thus
d(ω ∧η) = d( fgdxI ∧dxJ)
= ±d( fgdx[I,J])
= ±(gdf + f dg) ∧dx[I,J]
= (gdf + f dg) ∧dxI ∧dxj
where at each stage of these equalities the choice of a plus or minus sign is the same.
Now using the fact that dg is a 1-form and applying Proposition 9.1.7(b) q times we
have that dg ∧dxI = (−1)qdxI ∧dg. Therefore the above equalities become
d(ω ∧η) = (gdf ) ∧dxI ∧dxJ + ( f dg) ∧dxI ∧dxJ
= (df ∧dxI) ∧(gdxJ) + (−1)q( f dxI) ∧(dg ∧dxJ)
= (dω) ∧η + (−1)qω ∧(dη)
proving (b).

9.1 Introduction
303
(c) First we prove this when ω = f , a 0-form. In this case, remembering that
Dij f = Dji f since f has continuous second partial derivatives and that dxi ∧dxj =
−dx j ∧dxi,
d(df ) = d
= p

i=1
(Di f )(x)dxi
>
=
p

i=1
d(Di f ) ∧dxi
=
p

i=1
p

j=1
(Dij f )(x)dxj ∧dxi
= −
p

j=1
p

i=1
(D ji f )(x)dxi ∧dx j
= −d(df )
so that it must be that d(df ) = 0.
Now let q ≥1 and assume that ω = f dxI so that, by deﬁnition, dω = (df ) ∧
dxI. Using part (b) this implies that d(d(ω)) = (d(df )) ∧dxI −(df ) ∧d(dxI) =
−(df ) ∧d(dxI). But from Example 9.1.17(d) we know that d(dxI) = d(1) ∧dxI =
0. So that d(d(ω)) = 0 when ω = f dxI. The case of a general q-form now follows
from part (a).
■
9.1.19. Example. (a) Let G be an open subset of Rp and suppose f is a contin-
uously diﬀerentiable function on G. If γ : [a, b] →G is any regular curve and
ω = df , then Example 9.1.6(b) and the Chain Rule applied to f ◦γ shows that
ω(γ ) =

γ
df =
 b
a
⟨(D f )(γ (t)), γ ′(t)⟩dt
=
 b
a
( f ◦γ )′(t) dt
= f (γ (b)) −f (γ (a))
So the value of df (γ ) does not depend on the precise curve γ but only on its starting
and ﬁnal points, γ (a) and γ (b). Note that we can interpret this as an analogue of
the Fundamental Theorem of Calculus.
(b) Consider the 1-form ω = xdy in R2. If γ : [0, 2π] →R2 is deﬁned by γ (θ) =
(cos θ, sin θ), then γ traces the unit circle in the counterclockwise direction. We
have that
ω(γ ) =
 2π
0
cos θ d(sin θ) =
 2π
0
cos2 θ dθ = π
From part (a) of this example we see that there is no continuously diﬀerentiable
function f on R2 such that xdy = df . More on such a phenomenon in §9.6.

304
Differential Forms
Exercises
(1)
(a) Give the details for Example 9.1.2(a). (b) In Example 9.1.2(a) give a parametriza-
tion of the hemisphere S when p = 5. Can you give a parametrization for
arbitrary p?
(2)
Verify Example 9.1.2(b).
(3)
In the deﬁnition of two basic forms dxI ∧dxJ show that dx[I,J] = (−1)αdxI ∧dxJ,
where α is the number of diﬀerences {jℓ−ik : 1 ≤k ≤q, 1 ≤ℓ≤r} that are
negative.
(4)
Supply the details in the proof of Proposition 9.1.13.
(5)
Prove Proposition 9.1.15. (I know this is tedious, but this is one of those instances
where if you just read the proof it does no good and does not have any meaning. If,
on the other hand, you work it out on your own you will gain extra familiarity with
the concepts.)
The next two exercises are from [1].
(6)
Let ω be the 2-form on R4 deﬁned by ω = x2 dx1 ∧dx3 −x4 dx3 ∧dx4; let (, R)
be the 2-surface in R4 where R = {(x, y) : x2 + y2 ≤1} and (x, y) = (x, x −
y, 3 −x + xy, −3y). Compute

 ω.
(7)
Let ω be the 3-form in R4 deﬁned by ω = x3 dx1 ∧dx2 ∧dx4; let (, R) be the 3-
surface where R = {(x, y, z) : x2 + y2 + z2 ≤1} and (x, y, z) = (yz, x2, 1 −3y +
z, xy). Compute

 ω.
(8)
For each of the following choices of forms ω and η, compute ω ∧η and
write the product in its standard representation. (a) ω = y dx + x dy and η =
x dx ∧dz + y dy ∧dz. (b) ω = (x + y) dx + z dy + z dz and η = (y −z) dx + (x −
z) dy + (y −z) dz.
(9)
For each of the following choices of the form ω compute dω and write it in its
standard representation. (a) ω = f dx ∧dy + gdx ∧dz + h dy ∧dz. (b) ω = (x +
y2) dx ∧dy + x3 dy ∧dz.
9.2. Change of Variables for Forms
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Suppose G is an open subset of Rp and H is an open subset of Rm. If T : G →H
is a continuously diﬀerentiable mapping and  : R →G is in Sq(G), then T ◦ :
R →H is in Sq(H). If ω ∈Fq(H), we want to show that this change of variables
leads naturally to a q-form ωT in Fq(G) and to explore the relation between these
forms. This is a fundamental step in exploring forms and also in the orientation of
q-surfaces.
Note that we automatically have a function  →ω(T ◦) with domain Sq(G).
This does not mean, however, that this is a q-form on G. To be a q-form it must
satisfy additional properties as stated in the deﬁnition. The process of ﬁnding what
we will deﬁne as the change of variables for ω is rather straightforward, nevertheless
you must be careful because the notation gets a bit complicated.

9.2 Change of Variables for Forms
305
Let ω = 
I gI(y)dyI ∈Fq(H), written in its standard representation. To get the
sought for q-form ωT on G, two things seem clear: the index sets should remain the
same and the coeﬃcient functions should be fI(x) = gI(T (x)) for each x in G and
each index set I. The more complicated step will be to get the corresponding basic
q-forms on G. To do this examine the components of T : G →H,
y = (y1, . . . , ym) = T (x) = (t1(x), . . . ,tm(x))
where it follows that for 1 ≤i ≤m, ti : G →R is continuously diﬀerentiable. Thus
we can obtain the 1-forms
dti =
p

j=1
(D jti)(x) dxj
for 1 ≤i ≤m. This leads us to the following.
9.2.1. Definition.
If G and H are open sets in Rp and Rm, respectively, and
ω = 
I gI(y)dyI ∈Fq(H) expressed in its standard representation, then for any
continuously diﬀerentiable mapping T = (t1, . . . ,tm) : G →H, deﬁne the q-form
ωT on G as
ωT =

I
gI(T (x))dti1 ∧· · · ∧dtiq
where I = {i1 < · · · < iq}.
Observe that since each dti j is a 1-form and the indices ij are distinct, dti1 ∧· · · ∧
dtiq is a q-form so that ωT is indeed a q-form on G. Also each part of the deﬁnition
of ωT involves the mapping T as well as the given q-form ω.
To simplify the statements and notation below, maintain the notation we have
used so far: G always denotes an open subset of Rp, H an open subset of Rm, and
T : G →H is a continuously diﬀerentiable function.
9.2.2. Proposition. If ω ∈Fq(H) and η ∈Fr(H), the following hold.
(a) When r = q, (ω + η)T = ωT + ηT .
(b) (ω ∧η)T = ωT ∧ηT .
(c) If ω is continuously diﬀerentiable and T is a twice continuously diﬀerentiable
function, then d(ωT ) = (dω)T .
Proof. (a) This is a matter of examining the deﬁnition and is left to the reader
(Exercise 1(a)).
(b) This proof is also easy, though it is cumbersome. Let’s ﬁrst examine what
happens to basic q and r-forms. If I = {i1 < · · · < iq}, J = {j1 < · · · < jr}, ω =
dxI, and η = dxJ, then ωT = dti1 ∧· · · ∧dtiq, ηT = dt j1 ∧· · · ∧dt jr. So
ω ∧η = dxi1 ∧· · · ∧dxiq ∧dxj1 ∧· · · ∧dtxr
(ω ∧η)T = dti1 ∧· · · ∧dtiq ∧dt j1 ∧· · · ∧dtyr

306
Differential Forms
So clearly (b) is satisﬁed for basic forms. To show that (b) holds for all q-forms is
now routine (Exercise 1(b)).
(c) First assume ω is a 0-form: ω = g for a continuously diﬀerentiable function g.
So ωT = g ◦T and dω = m
i=1(Dig)(y)dyi ∈F1(H). Hence. using the Chain Rule,
we get
d(ωT ) =
p

j=1
[D j(g ◦T )](x)dxj
=
p

j=1
 m

i=1
(Dig)(T (x))(Djti)(x)

dxj
=
m

i=1
⎡
⎣
p

j=1
(D jti)(x)dxj
⎤
⎦(Dig)(T (x))
=
m

i=1
(Dig)(T (x))dti
= (dω)T
Now assume ω is a basic q-form on H: ω = dyI = dyi1 ∧· · · ∧dyiq so that
ωT = dti1 ∧· · · ∧dtiq. As we saw in Example 9.1.17(d), dω = 0 and also
d(ωT ) = 0.
Finally assume ω = gdyI so that ωT = (g ◦T )dti1 ∧· · · ∧dtiq. Therefore
d(ωT ) = d(g ◦T ) ∧dti1 ∧· · · ∧dtiq
= (dg)T ∧dti1 ∧· · · ∧dtiq
= [(dg) ∧dyI]T
= (dω)T
The general case now follows by using part (a).
■
See Exercise 2.
9.2.3. Proposition. Let G, H, and W be open sets with G ⊆Rp, H ⊆Rm, and
W ⊆Rd; let T : G →H and S : H →W be continuously diﬀerentiable mappings.
If ω ∈Fq(W ) and ST : G →W is the composition of S and T , then (ωS)T = ωST .
Proof. Just to be clear, ωS ∈Fq(H) and both (ωS)T and ωST ∈Fq(G). To set the
notation let T (x) = (t1(x), . . . ,tm(x)) for each x in G, S(y) = (s1(y), . . . , sd(y)) for
each y in H, and ST (x) = (r1(x), . . . , rd(x)) for each x in G. Let’s note that for
1 ≤k ≤d,
rk(x) = sk(T (x)) = sk(t1(x), . . . ,tm(x))

9.2 Change of Variables for Forms
307
so that by the Chain Rule
(Dirk)(x) =
p

j=1
(D jsk)(T (x))(Dit j)(x)
For ω a 0-form, the result is the usual chain rule for functions. Now let’s prove the
proposition when ω is the 1-form ω = dzk on W, where 1 ≤k ≤d. So ωST = drk
and ωS = dsk = m
j=1(D jsk)(y)dyj. Using the Chain Rule we have that
(ωS)T = (dsk)(T (x))
=
m

j=1
(D jsk)(T (x))dtj
=
m

j=1
(D jsk)(T (x))
 p

i=1
(Dit j)(x)dxi

=
p

i=1
⎡
⎣
m

j=1
(D jsk)(T (x))(Dit j)(x)
⎤
⎦dxi
=
p

i=1
(Dirk)(x)dxi
= drk = ωST
Now if ω and η are forms on W, Proposition 9.2.2 shows that ((ω ∧η)S)T =
(ωS ∧ηS)T = (ωS)T ∧(ηS)T . Since the result holds when ω is a 0-form, this
observation combined with what we did above completes the proof of the
proposition.
■
Assume ω ∈Fq(G) and  : R →G is in Sq(G). So we can consider  as a
change of variables to obtain a q-form on R, the surface domain for  that is con-
tained in Rq. That is, using the notation in Deﬁnition 9.2.1 we have ω ∈Fq(R). So
for any q-surface  in Sq(R) we can form ω() =

 ω. After thinking about
the hypothesis of the next result, you might predict the conclusion though the proof
takes some eﬀort.
9.2.4. Proposition. If ω ∈Fq(G),  : R →G is in Sq(G) with R ⊆Rq, and  :
R →Rq is deﬁned by (u) = u for all u in R, then


ω =


ω
Proof. It suﬃces to prove the proposition under the assumption that
ω = f (x)dxi1 ∧· · · ∧dxiq

308
Differential Forms
(Why?) Assume (u) = (φ1(u), . . . , φp(u)) with φi : R →R for 1 ≤i ≤p. Recall
from the deﬁnition of a q-form and (9.1.5) that
9.2.5


ω =

R
f ((u))∂(φi1, . . . , φiq)
∂(u1, . . . , uq) du
Let J(u) be the Jacobian in the preceding equation; recall that this is the determinant
of the matrix A(u) = [amn(u)], where for 1 ≤m, n ≤q
amn(u) = (Dnφim)(u)
Thus dφim = q
n=1 amn(u)dun. By a herculean manipulation of symbols we get that
dφi1 ∧· · · ∧dφiq =

[a1,n1(u) · · · aq,nq(u)] dun1 ∧· · · ∧dunq
where in this sum n1, . . . , nq ranges independently over 1, . . . , q. Now σ (m) = nm
for 1 ≤m ≤q deﬁnes a permutation in Sq. From Theorem 9.1.18 we know that
dum ∧duk = −duk ∧dum. Thus when σ (m) = nm for 1 ≤m ≤q,
dun1 ∧· · · ∧dunq = sign(σ ) du1 ∧· · · ∧duq
Remembering that J(u) = det A(u) and using Deﬁnition 6.4.18, we get
dφi1 ∧· · · ∧dφiq =

σ∈Sq
[a1,σ (1)(u) · · · aq,σ (q)(u)] duσ (1) ∧· · · ∧duσ (q)
=

σ∈Sq
sign(σ ) [a1,σ (1)(u) · · · aq,σ (q)(u)] du1 ∧· · · ∧duq
= J(u) du1 ∧· · · ∧duq
Applying this equation to (9.2.5) we get


ω =

R
f ((u))J(u)du
=


f ((u))J(u) du1 ∧· · · ∧duq
=


ω
■
Combining Proposition 9.2.3 and Proposition 9.2.4 will prove the following,
which is the Change of Variable Formula for forms.
9.2.6. Theorem.
Let G be an open subset of Rp and assume T : G →Rd is a
continuously diﬀerentiable mapping with H an open subset of Rd that contains
T (G). If  ∈Sq(G) and ω ∈Fq(H), then

T ◦
ω =


ωT
Proof. Suppose  : R →G with R ⊆Rq, and let  : R →Rq be deﬁned by
(u) = u as in Proposition 9.2.4. By that proposition

T ◦ ω =

 ωT ◦. But

9.3 Simplexes and Chains
309
Proposition 9.2.3 says this last integral becomes

 ωT ◦ =

(ωT ). Apply-
ing (9.2.4) once again we have that the last integral becomes

(ωT ) =

 ωT ,
whence the theorem.
■
See Exercise 3.
Exercises
(1)
(a) Give the details of the proof of Proposition 9.2.2(a). (b) Supply the missing
details in the proof of Proposition 9.2.2(b).
(2)
Where in the proof of Proposition 9.2.2 was use made of the assumption that T is
twice continuously diﬀerentiable?
(3)
Show that both Proposition 9.2.3 and Proposition 9.2.4 are special cases of
Theorem 7.4.19.
9.3. Simplexes and Chains
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Before getting into the heart of this section, we introduce an elementary concept
that will play a role.
9.3.1. Definition. A subset X of Rp is convex provided that when x, y ∈X, the
line segment [x, y] = {(1 −t)x + ty : 0 ≤t ≤1} ⊆X.
9.3.2. Proposition.
(a) Every ball in Rp is a convex set.
(b) A convex set is connected.
(c) The intersection of any collection of convex sets is convex.
The proof of the preceding proposition is left to the reader in Exercise 1, where a
few additional facts about convex sets also appear. Part (c) of the proposition leads
us to an additional concept. For any non-empty subset E of Rp we deﬁne the convex
hull of E as the intersection of all convex subsets containing E. This is denoted by
con(E). Note that the collection of all convex subsets containing E is non-empty
since Rp is one such set and con(E) is convex by (c).
9.3.3. Lemma. A subset X of Rp is convex if and only if when x1, . . . , xn ∈X and
t1, . . . ,tn ≥0 with n
i=1 ti = 1 we have that n
i=1 tixi ∈X.
Proof. If X satisﬁes the stated condition of the lemma when n = 2, then this is
the deﬁnition of convexity. Conversely, assume X is convex and x1, . . . , xn ∈X
and t1, . . . ,tn ≥0 with n
i=1 ti = 1. The proof that n
i=1 tixi ∈X is by induction.

310
Differential Forms
The statement n = 1 is trivial and n = 2 is the deﬁnition. Assume n ≥3 and the
statement for n −1 is true. Note that
n

i=1
tixi = (1 −tn)
n−1

i=1
ti
1 −tn
xi + tnxn
But by the induction hypothesis, n−1
i=1
ti
1−tn xi ∈X. The case n now follows.
■
9.3.4. Proposition. If E ⊆Rp, then
con(E) =
	 n

i=1
tixi : x1, . . . , xn ∈E, each ti ≥0 and
n

i=1
ti = 1
A
Proof. Denote the set on the right of the above displayed equation as S. It is left as
Exercise 2 to show that S is convex. Since E ⊆S it follows that con(E) ⊆S. Now
assume that X is a convex set that contains E. By the preceding lemma, S ⊆X and
the proposition follows.
■
9.3.5. Corollary. If E is a ﬁnite subset of Rp, then con(E) is compact.
Proof. If E = {x1, . . . , xn}, let
T =
	
(t1, . . . ,tn) ∈Rn : ti ≥0 and
n

i=1
ti = 1
A
If we deﬁne f : T →Rp by f (t1, . . . ,tn) = n
i=1 tixi, then f is continuous and
f (T ) = con(E) by the proposition. On the other hand it is immediate that T is a
compact subset of Rn since it is closed and bounded, so f (T ) is compact.
■
Now we deﬁne a convex subset of Rq that will be very important for our discus-
sion, in fact a key building block in this development.
9.3.6. Definition. If {e1, . . . , eq} is the standard basis in Rq, the unit simplex in
Rq is the set
Cq =
⎧
⎨
⎩
q

j=1
a jej : 0 ≤a j ≤1 for 1 ≤j ≤q and
q

j=1
a j ≤1
⎫
⎬
⎭
= {(a1, . . . , aq) ∈Rq : a j ≥0 for 1 ≤j ≤q and a1 + · · · + aq ≤1}
So C1 = [0, 1] and C2 = {(x, y) : x ≥0, y ≥0, and x + y ≤1}, the triangle in
the plane with vertices (0, 0), (0, 1), (1, 0).C3 is the tetrahedron in R3 with the four
vertices {(0, 0, 0), (1, 0, 0), (0, 1, 0), (0, 0, 1)}. We easily see that Cq is a convex
set. In fact, Cq is the convex hull of {0, e1, . . . , eq}.
Now for the next building block. Say that a mapping σ : Rq →Rp is aﬃne if there
is a linear transformation A : Rq →Rp and a vector v0 in Rp such that σ (u) = v0 +
A(u) for all u in Rq. Equivalently σ : Rq →Rp is an aﬃne mapping if A : Rq →Rp,
deﬁned by A(u) = σ (u) −σ (0), belongs to L(Rq, Rp). (Exercise 3.) We note in

9.3 Simplexes and Chains
311
passing the connection between aﬃne maps and the notion of an aﬃne hyperplane
deﬁned in (6.7.2): if p = 1 and σ : Rq →R is an aﬃne mapping, then σ −1(0) is an
aﬃne hyperplane. The proof of the next proposition is Exercise 4.
9.3.7. Proposition. If σ : Rq →Rp is an aﬃne map and X is a convex subset of
Rq, then σ (X ) is a convex subset of Rp.
9.3.8. Definition. An aﬃne q-simplex is a q-surface (σ,Cq) such that σ is an aﬃne
map.
Of course an aﬃne map is deﬁned on all of Rq, but in the preceding deﬁnition
we are restricting σ to the unit simplex. Also there is a double usage of the term
“simplex” here: once when we discuss the unit simplex (a set) and then when we
deﬁne an aﬃne q-simplex (a function). This shouldn’t cause any confusion as long
as we consider the context in which the terms are used.
So the trace of an aﬃne q-surface is a convex set, but there are convex subsets that
are not the trace of an aﬃne q-simplex; for example the closed disk in R2 (Why?).
Note that every line segment in Rp is the trace of an aﬃne 1-simplex. For the ease of
notation in the remainder of this chapter, e1, . . . , eq will always denote the standard
basis elements in Rq and we set e0 = 0.
9.3.9. Proposition. Let σ be an aﬃne q-simplex and for 0 ≤j ≤q put v j = σ (ej).
(a) σ is uniquely determined by the q + 1 points v0, . . . , vq. That is, if τ is also an
aﬃne q-simplex and τ(ej) = v j for 0 ≤j ≤q, then τ = σ.
(b) If (a1, . . . , aq) ∈Cq, then
σ (a1, . . . , aq) = v0 +
q

j=1
a j(v j −v0).
Proof. Clearly part (a) follows from part (b). To prove (b) let A(u) = σ (u) −
σ (0) = σ (u) −v0. Since σ is aﬃne, A ∈L(Rq, Rp). If 1 ≤j ≤q, then A(ej) =
σ (e j) −v0, from which (b) follows.
■
In light of the last proposition we can also deﬁne σ as the ordered (q+1)-tuple of
points in Rp:
σ = [v0, v1, . . . , vq]
Our use of the word “ordered” in the last sentence is a precursor of our next under-
taking: giving an aﬃne q-simplex σ an orientation, determined by the order of the
points v0, . . . , vq when we write σ = [v0, v1, . . . , vq].
Suppose κ is a permutation of 0, 1, . . . , q and ij = κ( j) for 0 ≤j ≤q. Let
σκ = [vi0, . . . , viq] = [vκ(0), . . . , vκ(q)]. So σκ is a new aﬃne q-simplex with the
same trace as σ but a diﬀerent orientation. If σ is an oriented aﬃne 1-simplex in
Rp and κ is the permutation κ(0) = 1, κ(1) = 0, then σκ is the same line seg-
ment traced in the opposite direction. As another example consider C2 ⊆R2. As
mentioned previously this is the triangle with vertices v0 = (0, 0), v1 = (1, 0), and

312
Differential Forms
v2 = (0, 1). So if we take σ = [v0, v1, v2] this orients ∂C2 in the counterclock-
wise or positive direction. Considering σκ = [v0, v2, v1] orients the boundary in the
clockwise direction. More complicated interpretations apply to higher dimensional
aﬃne q-simplexes.
Recall (6.4.16) that for any permutation κ of 0, . . . , q we have sign(κ) = ±1. If
sign(κ) = +1, we say that σ and σκ have the same orientation and they are equiv-
alent aﬃne q-simplexes. If sign(κ) = −1, the two q-simplexes are said to have the
opposite orientation.
The next result establishes the eﬀect that changing the orientation of an aﬃne
q-simplex has on the value of a diﬀerential form when it is evaluated at the simplex.
To start, let’s examine the interaction between a q-form and an aﬃne q-simplex. As
above let σ (u) = v0 + A(u) for some A in L(Rq, Rp) and all u in Rq, and let G be
an open subset of Rp that contains the trace of σ, {σ} = σ (Cq). Recall that q ≤p
and we let I be the subsets I = {i1 < · · · < iq} of {1, . . . , p}. The typical q-form
on G in its standard representation is given by ω = 
I∈I fIdxI ∈Fq(G). We seek
to give speciﬁcity to the formula (9.1.4) for

σ ω when σ is an aﬃne q-simplex. In
this the important thing is to calculate the Jacobians involved.
The matrix of the linear transformation A has size p × q. Using the explication
and notation employed to obtain (9.1.5) we have that for every u in Rq, σ (u) =
(φ1(u), . . . , φp(u)) ∈Rp. Thus for 1 ≤i ≤p, φi corresponds to the i-th row of the
matrix of A. Since σ is an aﬃne map, each φi : Rq →R is an aﬃne map. If for
I = {i1 < · · · < iq} in I we let AI be the q × q matrix with the same columns as A
and the i1, . . . , iq rows we get, using the notation of (9.1.4) and (9.1.5),
∂(xi1, . . . , xiq)
∂(u1, . . . , uq) = det AI
Thus (9.1.4) becomes
9.3.10

σ
ω =

Cq

I∈I
fI(σ (u)) det AI du
9.3.11. Proposition. If σ is an aﬃne q-simplex, κ is a permutation of 0, 1, . . . , q,
and σκ is as above, then for any q-form on an open set G containing σ (Cq) we have
that

σκ ω = sign(κ)

σ ω.
Proof. Let σ = [v0, . . . , vq] so that for all u in Rq, σ (u) = v0 + A(u) for some
A in L(Rp, Rq). First we assume that κ is the following transposition; ﬁx j with
1 ≤j ≤q and let κ(0) = j, κ( j) = 0, and κ(i) = i when i ̸= 0, j. So κ is a trans-
position and sign(κ) = −1. Hence σκ(x) = v j + B(x) where B ∈L(Rq, Rp) and is
deﬁned by B(ej) = v0 −v j and B(ei) = vi −v j when i ̸= j. (Pay close attention
as the following argument requires it.) Note that in arriving at the formula (9.3.10)
above and the deﬁnition of the q × q matrix AI, the columns of A were not disturbed
and the choice of I in I determined the q rows. We therefore concentrate on relating
the columns of B to those of A so we can establish that det BI = −det AI for each I
in I.

9.3 Simplexes and Chains
313
The columns of A are the vectors xi = A(ei) = vi −v0 for 1 ≤i ≤q. The
columns of B are v1 −v j, . . . , v j−1 −v j, −v j, v j+1 −v j, . . . , vq −v j = x1 −
x j, . . . , xj−1 −xj, −xj, xj+1 −xj, . . . , xq −xj. Now form the matrix B by subtract-
ing the j-th column of B from each of the others. Thus the columns of B are
x1, . . . , xj−1, −xj, xj+1, . . . , xq. Note that for each I in I, det BI = det BI. But the
columns of B are identical to those of A except for the j-th, which is the negative
of that column in A. Therefore we have that det BI = −det AI and we therefore have
that

σσ ω = −

σ ω.
The case where σ is any transposition is similar, though the notation is more
cumbersome. Since every permutation σ of {1, . . . , p} is the composition of trans-
positions, the proposition follows by combining this result for transpositions with
Proposition 9.2.3.
■
The plot thickens.
In fact I’m afraid the plot gets quite thick with an exceptionally high concentration
of deﬁnitions before we get to the main result of this section.
9.3.12. Definition. Let G be an open subset of Rp. An aﬃne q-chain  in G is
a ﬁnite collection  = {σ1, . . . , σr} of oriented aﬃne q-simplexes in G. An aﬃne
q-simplex may appear more that once in  and the number of times it is repeated is
called its multiplicity. If ω ∈Sq(G), then we write


ω =
r

i=1

σi
ω
The trace of  is deﬁned as
{} =
r
i=1
{σi}
We deﬁne
 = σ1 + · · · + σr =
r

i=1
σi
This is the deﬁnition of addition on the set of aﬃne q-simplexes in an open subset
G of Rp. In fact just as we deﬁned q-forms as a certain type of function on the set of
q-surfaces, each aﬃne q-simplex σ in G can be thought of as a function deﬁned on
the set Fq(G), where σ (ω) =

σ ω for all ω in Fq(G). If σ and τ are two such aﬃne
q-simplexes, then  = {σ, τ} is an aﬃne q-chain in G and for each ω in Fq(G) we
deﬁne
9.3.13
(σ + τ )(ω) =

{σ,τ}
ω =


ω =

σ
ω +

τ
ω = σ (ω) + τ(ω)

314
Differential Forms
Notice that Proposition 9.3.11 says that when σ is an oriented aﬃne q-simplex, we
have a negative for this addition. Indeed if κ is an odd permutation of 0, 1, . . . , q,
then in this deﬁnition of addition σκ = −σ. That is σ + σκ = 0.
I think the following examples hint at the usefulness of introducing aﬃne
q-chains.
9.3.14. Example. (a) The unit square I2 = [0, 1] × [0, 1] in R2 is the trace of a
2-chain. In fact the four vertices of I2 are 0, e1, e1 + e2, and e2. Deﬁne σ, τ onC2 by
σ (u) = u and τ(u) = e1 + e2 −u for all u in C2. So these are the aﬃne simplexes
σ = [0, e1, e2] and τ = [e1 + e2, e2, e1]. We have that I2 = {σ} ∪{τ}. We might
point out that the directions given the line segment joining the vertices e1 and e2 by
σ and τ are the opposite of one another.
(b) Generalizing part (a), the unit cube in Rq, Iq, is the trace of a q-chain in Rq.
This follows as in part (a). Note that if X1 = Cq and X2 = {(a1, . . . , aq) ∈Iq : a1 +
· · · + aq ≥q −1}, then X1 ∪X2 = Iq and X1 ∩X2 = {a ∈Iq : a1 + · · · + aq = 1}
(Exercise 6). Deﬁne σ and τ on Cq by σ (u) = u and τ(u) = e1 + · · · + eq −u. So
σ = [0, e1, . . . , eq] and a calculation reveals that τ(ej) = e1 + · · · + ej−1 + ej+1 +
· · · + eq. Hence
τ = [(e1 + · · · + eq), (e2 + · · · + eq), . . . , (e1 + · · · + eq−1)]
Thus Iq = {σ} ∪{τ}, the trace of a q-chain.
9.3.15. Definition. If σ = [v0, . . . , vq] is an oriented aﬃne q-simplex, its bound-
ary is deﬁned to be the aﬃne (q −1)-chain
∂σ =
q

j=0
(−1) j[v0, . . . , v j−1, v j+1, . . . , vq]
So once again we are using the addition of aﬃne simplexes with emphasis on
their orientation. Let’s call attention to the ambiguity in the notation: the use of the
symbol ∂in ∂σ. The trace of σ, {σ}, has no interior if q < p. Hence the topological
boundary of {σ} is the trace itself. However the trace of ∂σ consists of the image
under σ of the topological boundary of Cq.
9.3.16. Example. (a) If σ is the 1-simplex σ = [v0, v1] in Rp, then ∂σ = v1 −v0,
where this is the diﬀerence of two 0-simplexes.
(b) If σ = [v0, v1, v2], an aﬃne 2-simplex, then ∂σ = [v1, v2] −[v0, v2] +
[v0, v1]. Note that {σ} is a triangle in Rp and this formulation of its boundary gives
its customary counterclockwise (positive) orientation.
(c) Using the notation in the preceding deﬁnition, let σ0 = [v1, . . . , vq]. So σ0
is an aﬃne (q −1)-simplex deﬁned on the unit simplex Cq−1 ⊆Rq−1 by σ0(u) =
v1 + A0(u), where A0 is the linear transformation from Rq−1 into Rp deﬁned
by A0(ei) = vi+1 −v1 for 1 ≤i ≤q −1. Similarly for 1 ≤j ≤q if σj =
[v0, . . . , v j−1, v j+1, . . . , vq] as in Deﬁnition 9.3.15, then σj is an aﬃne (q −1)-
simplex deﬁned on the unit simplex Cq−1 ⊆Rq−1 by σj(u) = v0 + A j(u), where

9.3 Simplexes and Chains
315
A j : Rq−1 →Rp is the linear transformation deﬁned by A j(ei) = vi −v0 when
1 ≤i ≤j −1 and A j(ei) = vi+1 −v0 when j + 1 ≤i ≤q −1.
The exploration of the idea of a q-simplex and aﬃne q-chains was preliminary to
the next concept and that in Deﬁnition 9.3.18 that follows. The idea is to ﬁrst take
a q-simplex in Rp and then apply a C′′ map to it.
9.3.17. Definition. If H is an open subset of Rd, an oriented q-simplex of class C′′
in H is a q-surface (,Cq) in H that arises as follows. There is an open subset G in
Rp, a C′′ mapping T : G →H, and there is an oriented aﬃne q-simplex σ : Cq →
G such that
 = T ◦σ = T σ
The boundary of  is deﬁned by
∂ = T ◦∂σ = T (∂σ )
Let’s note a few things. First, T σ is just a shorthand notation for T ◦σ; similarly
for T (∂σ ). Second, the surface domain of  is the unit q-simplex Cq. Next  is
usually not an aﬃne q-simplex, hence the word “aﬃne” does not appear in its name.
What we have done is to introduce a collection of q-surfaces in H that we can orient
by using the orientation of an aﬃne q-simplex.
9.3.18. Definition. If H is an open subset of Rd, an oriented q-chain of class C′′
in H is a ﬁnite set  = {1, . . . , r} of oriented q-simplexes of class C′′ on H. If
ω ∈Fq(H), then


ω =
r

i=1

i
ω
and we use the notation  = r
i=1 i. The boundary of  is deﬁned as
∂ =
r

i=1
∂i
Note that in the notation  = r
i=1 i we are introducing an addition for ori-
ented q-simplexes of class C′′. In fact we have begun to consider a duality between
q-simplexes of classC′′ and diﬀerential forms. We deﬁned a q-form as a certain type
of function from surfaces into R. We also want to think of surfaces and oriented
q-chains of class C′′ as functions from q-forms into R. So if  = {1, . . . , r} as
in the preceding deﬁnition, we can consider the function  : Fq(H) →R deﬁned
by
(ω) =


ω
So when we say that
 = 0

316
Differential Forms
we mean the value of every q-form at  is 0. Let’s also point out that in some ways
an oriented q-chain is a generalization of a piecewise regular surface (8.4.13) to
surfaces in Rp. (In which ways is it a generalization and in which ways is it not?) If
we look at Example 9.3.14 above we see a hint of this.
9.3.19. Example. Again let G be an open subset of Rp, H an open subset of Rd, and
T : G →H a C′′ mapping. If  = r
i=1 σi is an aﬃne q-chain in G, and i = T σi
for 1 ≤i ≤r, we will write
 =
r

i=1
i = T
= r

i=1
σi
>
=
r

i=1
T (σi)
and
∂ =
r

i=1
T (∂σi)
9.3.20. Example. Here we want to show how some of the examples seen in the
preceding chapter ﬁt into the current landscape. Since many of those examples had
surface domains that were rectangles (see Exercise 8.3.2), Example 9.3.14 will be
useful. For example let’s see how a 2-surface whose surface domain is the square I2
can be handled. We start by assuming the surface is (T, I2), T isC′′, and this surface
lies in an open subset H of Rd. So there is an open subset G of R2 that contains I2
and on which T is C′′. Adopt the notation in Example 9.3.14(a) and let X1 = σ (C2)
and X2 = τ(C2) so that X1 ∪X2 = I2. Write G = G1 ∪G2, where Gk is open and
contains Xk for k = 1, 2. If 1 = T σ : C2 →H and 2 = T τ : C2 →H, then 1
and 2 are oriented 2-simplexes and  = {1, 2} is an oriented q-chain of class
C′′. Observe that ∂1 = T (∂σ ) and ∂2 = T (∂τ ).
Now suppose we are given a rectangle R = [a, b] × [c, d] and a surface (S, R)
of class C′′ that lies in the open subset H ⊆Rd. If we deﬁne A : R2 →R2 by
A(x, y) = ((1 −x)a + xb, (1 −y)c + yd), then A is a linear transformation and
A(I2) = R. Thus T = S ◦A : I2 →H is of class C′′ and the discussion in the pre-
ceding paragraph applies. For use in the next section when we discuss orientation,
note that det A = (b −a)(d −c) > 0.
Exercises
(1)
(a) Prove Proposition 9.3.2. (b) Give three examples in R3 of compact convex sets
that are not balls. (c) Show that the union of two convex sets may not be convex.
(2)
Show that the set S deﬁned in the proof of Proposition 9.3.4 is convex.
(3)
Show that a function σ : Rq →Rp is an aﬃne mapping if and only if A : Rq →Rp,
deﬁned by A(u) = σ (u) −σ (0) is linear.
(4)
Prove Proposition 9.3.7.

9.4 Oriented Boundaries
317
(5)
Let σ and τ be two aﬃne q-simplexes. If we deﬁne (σ ˙+τ )(u) = σ (u) + τ(u) for
all u in Cq, is (σ ˙+τ )(u) the same as σ + τ as deﬁned in 9.3.12?
(6)
Supply the details needed for Example 9.3.14(b).
(7)
Write out the details of Example 9.3.20 for the hemisphere in R3 as parametrized
in Exercise 8.3.2. What can be said about the common parts of ∂1 and ∂2?
(8)
Can you represent the 3-cube in R3 as the trace of a 3-chain?
9.4. Oriented Boundaries
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In the preceding section we introduced the idea of an oriented q-chain of class C′′,
which was a ﬁnite set of functions. Now we want to use this and the natural orien-
tation of an aﬃne simplex to orient the boundary of certain smooth surfaces. This
is similar to what we did in Deﬁnition 8.2.2 when we oriented a closed curve in
R2 relative to the open set it surrounded and Deﬁnition 8.4.15 when we introduced
a piecewise orientable surface. The ﬁrst step is to orient the boundary of the unit
simplex Cp for p ≥2.
9.4.1. Definition. If p ≥2 and σ is the identity map on Cp, then σ is an oriented
p-simplex. Indeed, using the notation of the preceding section, σ = [0, e1, . . . , ep].
Thus the (p −1)-chain ∂σ orients the boundary of Cp. We refer to this as the pos-
itive orientation of ∂Cp. Whenever we use the notation ∂Cp we assume it has this
positive orientation.
9.4.2. Example. (a) Recall Example 9.3.16(b) where we saw that the positive ori-
entation orientation of the boundary of the 2-simplex C2 is ∂σ = ∂[0, e1, e2] =
[0, e1] + [e1, e2] −[0, e2] = [0, e1] + [e1, e2] + [e2, 0]. This gives the boundary of
the simplex C2 the direction of going from 0 to e1 to e2 and back to 0, the counter-
clockwise direction.
(b) The positively oriented boundary of C3 = [0, e1, e2, e3] is [e1, e2, e3] −
[0, e2, e3] + [0, e1, e3] −[0, e1, e2].
Now suppose that G is an open subset of Rp that contains Cp and that  : G →
Rp is a C′′ mapping that is injective on Cp and whose Jacobian is positive on intCp.
It follows from the Inverse Function Theorem that X = (Cp) = cl (intCp) and
that (intCp) is an open set.
9.4.3. Definition. Let G be an open subset of Rp that containsCp and let  : G →
Rp be aC′′ mapping that is injective on intCp and whose Jacobian is strictly positive
there. If X = (Cp), then the positively oriented boundary of X is the (p −1)-chain
∂X = ∂ = (∂σ )
where σ is the identity map on Cp.

318
Differential Forms
If i : G →Rp is such a map for 1 ≤i ≤r, Xi = i(Cp), and int Xi ∩int Xj = ∅
for i ̸= j, put X = r
i=1 Xi. Then
∂X = ∂X1 + · · · + ∂Xr = ∂1 + · · · + ∂r
is called the positively oriented boundary of X.
Before going further we must point out that when p = 2, we now have two deﬁni-
tions of the positive orientation of ∂X when X is as above. Indeed if p = 2, (∂C2)
is a curve. Since  is smooth on G with a strictly positive Jacobian, this Jacobian is
strictly positive in a neighborhood of C2. Thus this curve is piecewise regular and
so Deﬁnition 8.2.2 gives a concept of ∂X being positively oriented. Are these two
concepts the same? Yes! Using linear algebra and some eﬀort we can prove this. We
choose, however, not to do this proof. Why? The use of these concepts of orienta-
tion when p = 2 is, in both cases, for the statement and proof of Green’s Theorem.
This was done in (8.2.10) when p = 2 and will be extended below in Theorem 9.7.1
as a consequence of a generalization of Stokes’s Theorem. In each case the value
of the integrals over the boundary of the set is the same, whereas if the orienta-
tions diﬀered they would be the negative of one another. (This is established in this
more general setting for p ≥2 in Proposition 9.3.11; in two-space it is immediate
from the deﬁnition of the integral over a curve (8.1.8).) Thus the two concepts of
positively oriented curves must be the same.
Why have we introduced oriented q-chains and
the concepts in the preceding deﬁnition?
Where is all this leading?
As we said in the preceding paragraph we are headed towards obtaining a Gener-
alized Stokes’s Theorem (9.5.1), for which we now have all the terminology needed
to read the statement but not to begin the proof. What we will see is that this result
also yields Green’s Theorem (8.2.10) and Gauss’s Theorem (8.5.6) as consequences.
How? What is the relationship? You may recall that when we gave the versions
of these results in R2 and R3 we needed to express surfaces as the union of non-
overlapping surfaces of a special type. (Especially recall the introduction of a G-set
in connection with Green’s Theorem.) In deﬁning sets of the type X in the last def-
inition we have extended and made precise the concept of a G-set in higher dimen-
sions. Realize that C2 is both a Type I and a Type II set, while C3 is simultaneously
Type I, II, and III. We saw that each of the expressions in Green’s, Gauss’s, and
Stokes’s Theorems are examples of diﬀerential forms. Now we must explore the
interaction between these extended notions and arbitrary diﬀerential forms.
9.4.4. Proposition. Let G be an open subset of Rp that containsCp and for i = 1, 2
let i : G →Rp be a C′′ mapping that is injective on a neighborhood of Cp and
whose Jacobian is strictly positive on intCp. If 1(Cp) = X = 2(Cp) and if ω is
a (p −1)-form deﬁned on some open set containing X, then

∂1 ω =

∂2 ω.

9.4 Oriented Boundaries
319
The complicated proof of this proposition, which won’t be used in the rest of this
chapter, is left to the interested reader. Here is an outline of the proof. Let H be an
open subset of Rp that contains 1(Cp) = 2(Cp) and on which −1
2
is deﬁned.
Let T : H →G be deﬁned by T = 1 ◦−1
2 . Note that T (∂2) = ∂1. By
Theorem 9.2.6 we have

∂1
ω =

T ◦∂2
ω =

∂2
ωT
Now we need to use the properties of T and the deﬁnition of ωT to show that

∂2 ωT =

∂2 ω, and here is where the complications arise.
9.4.5. Example. (a) Recall Example 9.3.14 on the unit square I2 = [0, 1] × [0, 1]
in R2, where we deﬁned σ1 = [0, e1, e2] and σ2 = [e1 + e2, e2, e1] and observed
that I2 = {σ1} ∪{σ2}. Therefore
∂σ1 = [e1, e2] −[0, e2] + [0, e1]
∂σ2 = [e2, e1] −[e1 + e2, e1] −[e1 + e2, e2]
When we add these boundaries together we get [e1, e2] + [e2, e1] = 0 so that
∂I2 = [0, e1] + [e1, e1 + e2] + [e1 + e2, e2] + [e2, 0]
and this orients the boundary of I2 in the counterclockwise direction.
(b) This is actually a continuation of the preceding part. Assume (, I2) is a
2-surface with surface domain I2. As a function on 2-forms we have  =  ◦σ1 +
 ◦σ2. Thus ∂ = ∂( ◦σ1) + ∂( ◦σ2) = (∂σ1) + (∂σ2) = (∂I2).
(c) We continue with the preceding parts. Recall from §8.2 where we deﬁned a
subset X to be Type I. Let f1, f2 : [a, b] →R be C′′ functions such that f1(x) <
f2(x) on (a, b) and set X = {(x, y) ∈R2 : a ≤x ≤b, f1(x) ≤y ≤f2(x)}. Assume
that  : I2 →X is deﬁned by
(u, v) = ((1 −u)a + ub, (1 −v) f1((1 −u)a + ub) + v f2((1 −u)a + ub))
= (φ(u, v), ψ(u, v))
It is easy to check that  is injective on int I2.
We have that
φu(u, v) = b −a
φv(u, v) = 0
ψu(u, v) = (b −a)[(1 −v) f ′
1((1 −u)a + ub) + v f ′
2((1 −u)a + ub)]
ψv(u, v) = f2((1 −u)a + ub) −f1((1 −u)a + ub)
A further computation shows that
∂(φ, ψ)
∂(u, v) = (b −a)[ f2((1 −u)a + ub) −f1((1 −u)a + ub)] > 0

320
Differential Forms
on int I2. So if we let Ti : C2 →R2 be deﬁned by Ti =  ◦σi, we have the situation
of Deﬁnition 9.4.3.
9.4.6. Example. Let X = [0, π] × [0, 2π] and deﬁne  : X →R3 by
(u, v) = (sin u cos v, sin u sin v, cos u)
The reader might note that {} is the unit sphere in R3. Here ∂ = (∂X ) =
γ1 + γ2 + γ3 + γ4, where these are the four curves in R3 deﬁned on [0, π], [0, 2π],
[0, π], [0, 2π], respectively, by the formulas
γ1(u) = (u, 0) = (sin u, 0, cos u)
γ2(v) = (π, v) = (0, 0, −1)
γ3(u) = (π −u, 2π) = (sin u, 0, −cos u)
γ4(v) = (0, 2π −v) = (0, 0, 1)
Let ω be any 1-form deﬁned in a neighborhood of {}. Since γ2 and γ4 are constant
we have that

γ2 ω =

γ4 ω = 0. Observing that γ3(u) = γ1(π −u), it follows by an
application of Example 9.1.6(a) that

γ3 ω = −

γ1 ω. Therefore we have that

∂
ω = 0
for any 1-form. Equivalently, ∂ = 0 (as a function on forms).
Exercises
(1)
Carry out an analysis similar to that in Examples 9.4.5(a) and (b) for C3.
(2)
Repeat Example 9.4.6 but this time for a parametrization of the unit hemisphere
above the xy-plane.
9.5. Stokes’s Theorem
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Here is the main result of this chapter, whose proof will occupy us for the rest of
the section.
9.5.1. Theorem (Generalized Stokes’s Theorem). If H is an open subset of Rp and
 is a q-chain of class C′′ in H, then for every smooth (q −1)-form ω in H,


dω =

∂
ω
In §9.7 we’ll see how the theorems of Green, Gauss, and Stokes from Chap-
ter 8 follow from this theorem, but now let’s point out that the above result
extends the Fundamental Theorem of Calculus. (You might want to look at Exam-
ple 9.1.6(a).) In Theorem 9.5.1 take q = 1 and let ω be the 0-form deﬁned by the

9.5 Stokes’s Theorem
321
smooth function f : [a, b] →R. For H take any open set that contains [a, b] and for
the 1-chain let  : [a, b] →H ⊆R be the identity map, (x) = x. Consequently

 dω =
 b
a f ′(x) dx and

∂ ω = f (b) −f (a); Theorem 9.5.1 says they are equal,
thus establishing the FTC. (Of course the number of times the FTC was used to get
to this point is probably large, so this is not an independent proof. In fact the FTC
is used directly in the proof of Theorem 9.5.1 below.)
Proof of The Generalized Stokes’s Theorem. To begin we show that what has to
be proved can be reduced to a simpler environment. To begin note that it suﬃces
to show that

 dω =

∂ ω when  is a single d-simplex of class C′′. By deﬁni-
tion  = T ◦σ where σ = [0, v1, . . . , vq] deﬁned on Cq and T : G →H ⊆Rp is
a mapping of class C′′ on an open subset G of Rq that contains σ (Cq). By the rules
for diﬀerentiating forms (9.2.2(c)) and the Change of Variables Theorem for forms
(9.2.6) we have that


dω =

T σ
dω =

σ
(dω)T =

σ
d(ωT )
Since ∂ = ∂(T σ ) = T (∂σ ), we also have that

∂
ω =

T (∂σ )
ω =

∂σ
ωT
This means that to prove the theorem we need only prove
9.5.2

σ
dζ =

∂σ
ζ
for every smooth (q −1)-form ζ.
As we saw prior to the proof, if q = 1 this amounts to the Fundamental Theorem
of Calculus. So we can assume that q ≥2. Suppose 1 ≤r ≤q and f is a smooth
function on G. It suﬃces to establish (9.5.2) when
ζ = f dx1 ∧· · · ∧dxr−1 ∧dxr+1 ∧· · · ∧dxq
since the arbitrary (q −1)-form is the sum of such forms as this. Now
∂σ = [v1, . . . , vq] +
q

i=1
(−1)iτi
where τi = [0, v1, . . . , vi−1, vi+1, . . . , vq] for 1 ≤i ≤q.
We now need to do some tedious calculations. Put
τ0 = [vr, v1, . . . , vr−1, vr+1, . . . , vq]
and observe that τ0 can be obtained from [v1, . . . , vq] by r −1 successive inter-
changes with its left-hand neighbor. Hence
∂σ = (−1)r−1τ0 +
q

i=1
(−1)iτi

322
Differential Forms
Recall that for 0 ≤i ≤q each τi is deﬁned on Cq−1. Consider i = 0 and ﬁx u =
(u1, . . . , uq−1) in Cq−1. We have from (9.3.9) that
τ0(u) = vr +
r−1

i=1
ui(vi −vr) + ur(vr+1 −vr) + · · · + uq−1(vq−1 −vr)
Thus if u ∈Cq−1 and we put x = τ0(u) ∈Rq, then
9.5.3
x j =
⎧
⎪⎨
⎪⎩
u j
(1 ≤j < r)
1 −(u1 + · · · + uq−1)
( j = r)
uj−1
(r < j ≤q)
Similarly if 1 ≤i ≤q and x = τi(u) for some u in Cq−1, then
9.5.4
xj =
⎧
⎪⎨
⎪⎩
u j
(1 ≤j < i)
0
( j = i)
uj−1
(i < j ≤q)
For 0 ≤i ≤q let Ji be the Jacobian of the map
(u1, . . . , uq−1) →(x1, . . . , xr−1, xr+1, . . . , xq)
as deﬁned above using the simplexes τi. Observe that when i = 0 or i = r, this is
the identity map. (Verify!) Hence J0 = 1 = Jr. When i ̸= 0, r, then the fact that the
above formula for x has xi = 0 implies that when we compute the determinant Ji,
its associated square matrix has a row of zeros. Thus Ji = 0 when i ̸= 0, r. From
the deﬁnition of the action of ζ this implies that

τi ζ = 0 when i ̸= 0, r. Therefore

∂σ
ζ = (−1)r−1

τ0
ζ +
q

i=1
(−1)i

τi
ζ
= (−1)r−1

τ0
ζ + (−1)r

τr
ζ
= (−1)r−1

Cq−1[ f (τ0(u)) −f (τr(u))] du
But we also have that
dζ = (Dr f )(x) dxr ∧dx1 ∧· · · ∧dxr−1 ∧dxr+1 ∧· · · ∧dxq
= (−1)r−1(Dr f )(x) dx1 ∧· · · ∧dxq
so that

σ
dζ = (−1)r−1

Cq(Dr f )(x)dx
To evaluate this last integral we use Fubini’s Theorem and ﬁrst integrate with respect
to xr over the interval [0, 1 −(x1 + · · · + xr−1 + xr+1 + · · · + xq)] = [0, 1 −yr].

9.6 Closed and Exact Forms
323
Doing this we get
 1−yr
0
(Dr f )(x1, . . . , xq) dxr = f (x1, . . . , xr−1, 1 −yr, xr+1, . . . , xq)
−f (x1, . . . , xr−1, 0, xr+1, . . . , xq)
Using (9.5.3) we see that
yr = x1 + · · · + xr−1 + xr+1 + · · · + xq = u1 + · · · + uq−1
Hence
f (x1, . . . , xr−1, 1 −yr, xr+1, . . . , xq) = f (τ0(u))
Now using (9.5.4) we have that
f (x1, . . . , xr−1, 0, xr+1, . . . , xq) = f (τr(u))
Therefore 
σ
dζ = (−1)r−1 =

Cq−1[ f (τ0(u)) −f (τr(u))] du =

∂σ
ζ
completing the proof.
■
9.6. Closed and Exact Forms∗
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In R every continuous function has a primitive by the FTC. Namely, if f is a con-
tinuous function on some interval (a, b) that contains the point c, then F(x) =
 x
c f
has the property that F′(x) = f (x) for all x in (a, b). In this section we will be con-
cerned with an analogous problem of deciding if a form in Rp has a primitive. This
is not always the case, as we will soon see. Here is the basic language.
9.6.1. Definition.
Let G be an open subset of Rp and let ω be a q-form on G.
The form ω is closed if it is smooth and dω = 0; ω is exact if there is a smooth
(q −1)-form η on G such that ω = dη.
We know from Theorem 9.1.18 that if ω is smooth and exact, then it is closed.
When p > 1 there are closed forms that are not exact, and the analogous problem
referred to in the introduction to this section is which closed forms are exact. Before
giving an example let’s establish a few facts.
9.6.2. Proposition. Let G be an open subset of Rp and let ω be a C′′ q-form on G.
(a) If q = 1 and ω = p
j=1 f jdxj, then ω is closed if and only if Di f j = Dj fi for
all i, j.
(b) If ω is closed and  is a (q + 1)-chain of class C′′, then

∂ ω = 0.
(c) If ω is exact and  and  are two q-chains in G with ∂ = ∂, then

 ω =

 ω. Consequently, if ∂ = 0, then

 ω = 0 for every exact form ω.

324
Differential Forms
Proof. (a) This is straightforward. Note that
dω =
p

j=1
(df j) ∧dxj
=
p

j=1
= p

i=1
(Di f j) ∧dxi
>
∧dxj
=

i< j
(Di f j −Dj fi)dxi ∧dx j
(b) This is a direct application of Stokes’s Theorem (9.5.1). (How is Exam-
ple 9.1.6(e) related to this part?)
(c) Let ω = dη. Again we apply Stokes’s Theorem to get that

 ω =

 dη =

∂ η =

∂ η =

 ω.
■
Note that showing that a form ω is exact means solving several partial diﬀerential
equations simultaneously. For example if we have a 1-form ω = p
j=1 f jdxj on
G ⊆Rp, then to show that ω is exact we must ﬁnd a smooth function g : G →R
such that ω = dg; this means that Djg = f j for 1 ≤j ≤p. When ω is a q-form
and q ≥2, the diﬀerential equations that are to be solved are more complex. See
Exercise 1.
9.6.3. Example. If G is an open subset of R2, note that when γ is a smooth curve
in G we have that γ is a 1-chain in G. If γ is a closed curve, then ∂γ = 0. Thus
if ω is an exact 1-form on G it must be that

γ ω = 0 by part (a) of the preceding
proposition. Let G be the punctured plane R2\{(0, 0)}. Deﬁne ω on G by
ω = x dy −y dx
x2 + y2
It is left to the reader to verify that ω is a closed form. Now deﬁne γ : [0, 2π] →G
by γ (θ) = (cos θ, sin θ). So γ is the unit circle in the positive direction. The reader
can verify that

γ ω = 2π (Exercise 2). By what we said at the start of this example,
ω cannot be exact. The impediment to exactness of closed forms is that puncture at
the origin.
Proposition 9.6.2(c) gives a necessary condition for a form to be exact. We want
to discover suﬃcient conditions on an open subset G of Rp such that every closed
form on G is exact.
At the beginning of §9.3 we discussed convex subsets of Rp. We revisit this topic
here. The proof of the next proposition is left to the reader (Exercise 4).
9.6.4. Proposition. (a) Any ball in Rp is convex as is any aﬃne hyperplane or
half-space.
(b) If X is a convex subset of Rp and T ∈L(Rp, Rq), then T (X ) is a convex subset
of Rq.

9.6 Closed and Exact Forms
325
(c) IfY is a convex subset of Rq and T ∈L(Rp, Rq), then T −1(Y ) is a convex subset
of Rp.
Note that the punctured plane is not convex.
It holds that whenever G is an open convex subset of Rp, every closed form on G is
exact. To establish this would involve more background than we deem appropriate,
because the argument is very involved and after all the eﬀort convexity is not the
most general hypothesis we can impose on G to have the same conclusion. Instead
we will focus on R2, where the proof is straightforward but not trivial. A proof of
the convex case in Rp can be found in [7], page 278. A statement and proof of the
more general result can be found in [9], page 457.
If α and β are two points in R2, let [α, β] denote the straight line segment from
α to β. We begin the proof for R2 with a lemma.
9.6.5. Lemma. Let G be an open disk in R2, let (a, b) ∈G, and let η = P dx +
Q dy be a smooth closed form on G. If (x, y) ∈G, then

[(a,b),(x,y)]
P dx + Q dy =
 x
a
P(t, b) dt +
 y
b
Q(x,t) dt
Proof. First note that
 x
a
P(t, b) dt =

[(a,b),(x,b)]
(P dx + Q dy)
and
 y
b Q(x,t) dt =

[(x,b),(x,y)](P dx + Q dy). Now note that the line segments
[(a, b), (x, b)], [(x, b), (x, y)], and [(x, y), (a, b)] form the sides of a triangle; denote
by X this triangle together with its inside. By Green’s Theorem we have that

∂X
P dx + Q dy =

X
(D1Q −D2P)
But Proposition 9.6.2(a) says the condition that ω is closed is equivalent to the con-
dition that D2P = D1Q. So we have that

∂X P dx + Q dy = 0 and the lemma fol-
lows.
■
9.6.6. Theorem. If G is an open convex subset of R2, then every smooth closed
form on G is exact.
Proof. Assume that ω is a smooth form on G that is closed. Thus ω = P dx +
Q dy + f dx ∧dy. So
0 = dω
= dP ∧dx + dQ ∧dy + df ∧dx ∧dy
= (D1Q −D2P)dx ∧dy
Thus we see that the 1-form η = P dx + Q dy is also closed.
9.6.7. Claim. The form η is exact.

326
Differential Forms
First assume that G is an open disk with center (a, b). Deﬁne F :→R by
F(x, y) =

[(a,b),(x,y)]
P dx + Q dy
We claim that dF = η. To show this we must show that D1F = P and D2F = Q.
By the lemma and the FTC we have that D2F(x, y) = D2
 y
b Q(x,t) dt = Q(x, y).
On the other hand ﬁrst employing Theorem 7.5.2 about diﬀerentiating under the
integral sign and then the assumption that ω is closed, we get
D1F(x, y) = D1
 x
a
P(t, b) dt + D1
 y
b
Q(x,t) dt
= P(x, b) +
 y
b
∂Q(x,t)
∂x
dt
= P(x, b) +
 y
b
∂P(x,t)
∂t
dt
= P(x, b) + P(x, y) −P(x, b)
= P(x, y)
Now assume G is any open convex set and ﬁx a point (c, d) in G. By convexity
[(c, d), (x, y)] ⊆G for every (x, y) in G. Deﬁne F(x, y) =

[(c,d),(x,y)] P dx + Q dy.
Note that the line segment [(c, d), (x, y)] can be covered by a ﬁnite number of
open disks, each contained in G. By what we did before we have that D1F = P
and D2F = Q in each of these disks (Why?). It follows that dF = η in all of G,
establishing the claim.
It remains to show that f dx ∧dy is exact. Let U = {x ∈R : (x, y) ∈G
for some y}. Since U is the projection of G onto the ﬁrst coordinate, it is open and
connected. Hence it is an open interval. Fix a point c in U and deﬁne H : G →R
by
H(x, y) =
 x
c
f (t, y) dt
The FTC implies D1H = f . Hence d(H dy) = dH ∧dy = (D1H dx + D2H dy) ∧
dy = f dx ∧dy.
Letting λ = H + η, we get that dλ = ω and so ω is exact.
■
9.6.8. Proposition. Let G be an open subset of R2 such that every closed form on
G is exact. IfU is an open subset of R2 such that there is aC′′ bijection T : G →U,
then every closed form on U is exact.
Proof. Let ω be a closed form on U. Thus ωT is a form on G and, by Proposition
9.2.2(c), d(ωT ) = (dω)T = 0. By hypothesis there is a form η on G such that ωT =
dη. If S = T −1 : U →G, let ζ = ηS, which is a form onU. Using Proposition 9.2.3
we have that dζ = d(ηS) = (dη)S = (ωT )S = (ω)T S = ω, so that ω is exact.
■
See Exercise 5.

9.7 Denouement
327
Exercises
(1)
In G ⊆R3 if ω is the 2-form ω = f dxdy + gdxdz + hdxdy, phrase the problem of
showing that ω is exact in terms of solving diﬀerential equations.
(2)
Show that

γ ω = 2π in Example 9.6.3.
(3)
Let G = R3\{0}, punctured three-space, and put
ω = x dy ∧dz + y dz ∧dx + z dx ∧dy
(x2 + y2 + z2)3/2
(a) Show that ω is closed. (b) Let  be as in Example 9.4.6 and show that

 ω = 4π. (c) Conclude that ω is not exact.
(4)
Verify the statements in Example 9.6.4.
(5)
Let U be the open half-annulus {(x, y) ∈R2 : −2 < x < −1 or 1 < t < 2,
0 < y < 2} and show that every closed form on U is exact.
9.7. Denouement
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
In this section we want to derive the theorems of Green, Gauss, and Stokes in R2 and
R3 as a consequence of the generalized Stokes’s Theorem. We begin with Green’s
Theorem 8.2.10. The setup for Green’s Theorem is as follows. Let G be an open
subset of R2 that contains C2 and for 1 ≤j ≤r let j : G →R2 be a C′′ mapping
that is injective on intC2 and whose Jacobian is positive there. Put Xj = j(C2),
and assume int Xi ∩int Xj = ∅for i ̸= j; put X = r
j=1 Xi. Give ∂X the positive
orientation as in Deﬁnition 9.4.3. We note that each ∂Xj is a smooth closed curve in
R2. The reader can mull over the connection between this setup and the assumption
that X is a G-set given in (8.2.10). Establishing this connection is left to the inclined
reader. (Exercise 1.) Be sure to ﬁrst go over Example 9.4.5(c).
9.7.1. Theorem (Green’s Theorem). Let G be an open subset of R2 that contains
C2, let X be as described above, and let  be an open subset of R2 containing X.
If F :  →R2 is a smooth function with F(x, y) = (P(x, y), Q(x, y)), then

∂X
F · T ds =

X

∂Q
∂x −∂P
∂y

Proof. Put ω = Pdx + Qdy. By the Generalized Stokes’s Theorem (9.5.1)

∂X
ω =

X
dω
Now
dω = (∂yP) ∧dy + (∂xQ)) ∧dx =

∂Q
∂x −∂P
∂y

dx ∧dy

328
Differential Forms
so

X
dω =

X

∂Q
∂x −∂P
∂y

From Example 9.1.6(b) we know that ω(γ ) =

γ F · T ds for any smooth curve γ .
Given that ∂X consists of smooth curves, this completes the proof.
■
Again we adopt the setup from Deﬁnition 9.4.3, this time in R3. Let G be an open
subset of R3 that contains C3 and for 1 ≤j ≤r let j : G →R3 be a C′′ mapping
that is injective on intC3 and whose Jacobian is positive there. Also assume that with
Xj = j(C3) we have that int Xi ∩int Xj = ∅for i ̸= j; put X = r
j=1 Xj. Give ∂X
its positive orientation as in (9.4.3). Again there is a connection between this setup
and the assumption in Theorem 8.5.6 that X is a G-set; it is left to the reader to
explore this connection (Exercise 2).
9.7.2. Theorem (Gauss’s Theorem). Let G be an open set in R3 containing C3,
let X be as described above, and let  be an open subset of R3 containing X. If
F :  →R3 is a smooth function, then

∂X
F · n dσ =

X
div F
Proof. Let
F(x, y, z) = (P(x, y, z), Q(x, y, z), R(x, y, z))
In Example 9.1.17(b) we saw that for the 2-form ω = Pdy ∧dz + Qdz ∧dx +
Rdx ∧dy,
dω = (div F) dx ∧dy ∧dz
So using (8.4.7) and the generalized Stokes’s Theorem we have that

∂X
F · n dσ =

∂X
P dydz +

∂X
Q dzdx +

∂X
R dxdy
=

∂X
ω
=

X
dω
=

X
div F
■
Coordinating the statement in the theorem below with Theorem 8.5.10 is up to
the reader.
9.7.3. Theorem (Stokes’s Theorem). Let  be an open subset of R3 and let  be
a regular 2-surface contained in . If F :  →R3 is a smooth function, then

∂
F · T ds =


curl F · n dσ

9.7 Denouement
329
Proof. Put F(x, y, z) = (P(x, y, z), Q(x, y, z), R(x, y, z)). For eﬃciency in expres-
sion we write curl F = C1(F)e1 + C2(F)e2 + C3(F)e3. Let ω = Pdx + Qdy +
Rdz and η = C1(F)dy ∧dz + C2(F)dz ∧dx + C3(F)dx ∧dy. As in Exam-
ple 9.1.17(c), we have that η = dω. From Example 9.1.6(b) applied to a chain of
curves rather than one, we know that

∂
ω =

∂
F · T ds
From (8.4.7) we know that


dω =


η =


curl F · n dσ
Therefore Stokes’s Theorem (8.5.10) follows from the generalized version.
■
Exercises
(1)
This is a rather involved exercise. Can you show that a G-set (8.2.6) in R2 is an
oriented q-chain for an appropriate value of q?
(2)
This is also a rather involved exercise. Can you show that a G-set (8.5.4) in R3 is an
oriented q-chain for an appropriate value of q?


Bibliography
[1] K. Bryan, “Diﬀerential Forms,” www.rose-hulman.edu/~bryan/lottamath/diﬀorm.pdf
[2] P. Cameron, “Ten Chapters of the Algebraic Art,” www.maths.qmul.ac.uk/~pjc/notes/intalg.pdf
[3] J. B. Conway, A Course in Abstract Analysis, AMS (2012).
[4] J. B. Conway, A Course in Point Set Topology, Springer–Verlag (2013).
[5] K. Hoﬀman and R. Kunze, Linear Algebra, Prentice Hall Publishing (1971).
[6] E. Landau, Foundations of Analysis, AMS Chelsea Publishing (2001).
[7] W. Rudin, Principles of Mathematical Analysis, McGraw-Hill Education; 3rd edition (1976).
[8] R. Schwartz, “Dedekind cuts,” www.math.brown.edu/~res/INF/handout3.pdf
[9] J. Shurman, Multivariable Calculus, http://people.reed.edu/~jerry/211/vcalc.pdf
[10] R. Sjamaar, “Manifolds and diﬀerential forms,” www.math.cornell.edu/~sjamaar/manifolds/
manifold.pdf
[11] William F. Trench, Introduction to Real Analysis (2013). Books and Monographs. Book 7.
http://digitalcommons.trinity.edu/mono/7
[12] William F. Trench, “The Method of Lagrange Multipliers” 2013 Available at: http://works
.bepress.com/william_trench/130
[13] H. A. Thurston, The Number System, Dover (2012).
[14] H. Tverberg, “A proof of the Jordan Curve Theorem,” Bull. London Math. Soc. 12 1980, 34–38.
[15] W. R. Wade, An Introduction to Analysis, Pearson (2009).


Index of Terms
ϵ-chain, 144
σ-compact, 140
0-form, 294
absolute value, 14
absolutely integrable, 87
adjoint, 177
aﬃne hyperplane, 198
aﬃne mapping, 310
aﬃne q-chain, 313
aﬃne q-simplex, 311
algebra, 65, 148
alternating harmonic series, 26
anti-derivative, 79
arc length, 220
Arzela, Cesare, 152
Arzela–Ascoli Theorem, 152
Ascoli, Giulio, 152
Banach Fixed Point Theorem, 201
Banach, Stefan, 201
basic q-form, 298
bijective, 5
Bolzano, Bernard, 19
Bolzano–Weierstrass Theorem, 19, 134
Borel, Emile, 137
boundary, 123
boundary of , 281
boundary of an aﬃne q-simplex, 314
boundary of an oriented q-chain, 315
boundary of an oriented q-simplex, 315
boundary point of a surface, 276
bounded, 7
bounded above, 7
bounded below, 7
bounded function, 133
bounded set, 127
bounded variation, 94
Cantor, Georg, 35
Cantor’s Theorem, 35, 126
cartesian product, 30
Cauchy, Augustin Louis, 21
Cauchy sequence, 21, 120
Cauchy–Schwarz Inequality, 117
Chain Rule, 56, 165, 190
Change of Variables Theorem, 79
characteristic function, 5, 146
characteristic polynomial, 183
closed ball, 122
closed curve, 258
closed form, 323
closed interval, 16
closed relative to, 122
closed set, 33, 121
closed surface, 276
closure, 123
compact, 133
Comparison Test, 26, 87
complement, 3
complete, 120
Completeness Property, 7
component, 143
composition, 5
conditionally convergent, 28
connected, 140
constant function, 5, 37
constraint, 212
contained in, 2
contains, 2
continuous at a point, 129
continuous function, 37, 129
continuously diﬀerentiable, 63, 190
convergent series, 24
converges, 119
converges absolutely, 26
converges uniformly, 146
convex, 309
convex hull, 309
corona, 269
countable set, 29
countably inﬁnite, 29

334
Index of Terms
COV, 80
cover, 133
critical point, 62, 167, 192
cross product, 272
curl, 284
curve, 156
cut, 10, 266
Darboux, Jean Gaston, 64
Darboux’s Theorem, 64
De Morgan, Augustus, 4
De Morgan’s Laws, 4, 121, 127
decreasing function, 49
decreasing sequence, 18
Dedekind cut, 10
Dedekind, Richard, 10
dense, 124
Density Property, 7
denumerable, 29
derivative, 52, 156, 163, 188
determinant, 180
diagonal matrix, 178
diameter, 35, 126
diﬀerence, 3
diﬀerentiable, 52, 53, 156, 163, 188
diﬀerential form of order q, 294
Dini, Ulisse, 139
Dini’s Theorem, 139
directional derivative, 166
disconnected, 140
discrete metric, 116
disjoint, 2
distance, 126
distance from A to B, 140
distance from a point to a set, 36
divergence, 284
Divergence Theorem, 286
divergent series, 24
domain, 5
dot product, 117
dyadic expansion, 31
eigenspace, 183
eigenvalue, 183
eigenvector, 183
element, 1
empty set, 2
equal sets, 2
equicontinuous, 151
equivalent aﬃne q-simplexes, 312
equivalent curves, 222
Euclidean space, 116
even function, 57
EVT, 39
exact form, 323
exponential function, 82
extended real numbers, 15
extension, 42
Extreme Value Theorem, 39, 134
ﬁnal point, 258
ﬁnite intersection property, 135
FIP, 135
ﬁxed point, 201
ﬂip, 246
FTC, 78
Fubini, Guido, 235
Fubini’s Theorem, 234
function, 4
Fundamental Theorem of Calculus, 78
G-set, 265, 285
Gauss, Johann Carl Friedrich, 286
Gauss’s Theorem, 286
Generalized Mean Value Theorem, 59
Generalized Stokes’s Theorem, 320
geometric series, 25
gradient, 164
Gram, Jorgen, 170
Gram–Schmidt Process, 170
greatest lower bound, 7
Green, George, 267
Green’s Theorem, 267
half-open interval, 16
harmonic p-series, 89
harmonic series, 25
Heine, Heinrich Eduard, 137
Heine–Borel Theorem, 137
Hermite, Charles, 177
hermitian, 177
hyperplane, 198
idempotent, 178
identity function, 37
image, 5
Implicit Function Theorem, 208
improperly integrable, 86
increasing function, 49
increasing sequence, 18
indeﬁnite integral, 79
index set, 121

Index of Terms
335
indicator function, 5
inﬁmum, 7
inﬁnite series, 24
inﬁnitely diﬀerentiable, 63
injective, 5, 173
inner product, 117
inside of a curve, 263
integrable, 73, 86, 225
Integral test, 88
Integration by Parts, 80
interior, 123
interior point of a surface, 276
Intermediate Value Theorem, 39, 142
intersection, 2, 121
interval, 16
inverse function, 60
Inverse Function Theorem, 202
IPFT, 210
isolated point, 37, 125
IVT, 39
Jacobi, Carl, 205
Jacobian, 205
Jacobian determinant, 205
Jacobian matrix, 205
Jordan, Camille, 226
Jordan curve, 258
Jordan set, 226
jump discontinuity, 49
kernel, 173
L’Hôpital, Guillaume François Antoine, 65
L’Hôpital’s Rule, 65
Lagrange, Joeseph-Louis, 213
Lagrange multiplier, 213
lattice, 40
least upper bound, 7
Lebesgue, Henri Léon, 91
Lebesgue’s Theorem, 91
left-continuous, 99
left-hand limit, 48
length of a curve, 220
liminf, 22
limit, 47, 50, 51
limit inferior, 22
limit point, 37, 125
limit superior, 22
limsup, 22
line integral, 260
linear functional, 161
linear transformation, 173
Lipschitz function, 41, 132
Lipschitz, Rudolph, 41
local extremum, 167, 212
local maximum, 57, 167, 212
local maximum subject to a constraint, 212
local minimum, 57, 167, 212
local minimum subject to a constraint, 212
locally integrable, 86
logarithm, 81
lower bound, 7
lower limit, 22
lower sum, 71, 225
Möbius, August Ferdinand, 279
Möbius Strip, 279
map, 5
mapping, 5
maximum of functions, 40
Mean Value Theorem, 58, 79, 166
measure zero, 90
mesh, 95
metric, 115
metric space, 115
minimum of two functions, 40
modulus of continuity, 96
monotonic function, 49
multiplicity, 183, 313
MVT, 58
natural logarithm, 81
natural numbers, 1
natural parametrization, 259
negative deﬁnite, 193
negative matrix, 193
non-connected, 140
non-decreasing sequence, 18
non-overlapping, 223
norm, 155, 162, 174
normalization, 100
odd function, 57
open ball, 122
open cover, 133
open interval, 16
Open Mapping Theorem, 204
open relative to, 122
open set, 33, 121
opposite orientation, 312
orientable piecewise regular surface, 282
orientation, 311

336
Index of Terms
oriented aﬃne q-simplex, 311
oriented q-chain of class C′′, 315
oriented q-simplex of class C′′, 315
oriented surface, 277
orthogonal, 168
orthogonal projection, 172, 178, 179
orthonormal, 168
orthonormal basis, 168
oscilation, 90
outside of a curve, 263
pairwise orthogonal, 168
Parallelogram Law, 172
parametrization, 293
parametrization of a surface, 269
parity, 180
partial derivative, 158
partial sum, 24
partition, 71
path integral, 260
permutation, 179
piecewise orientable surface, 282
piecewise regular, 259
piecewise regular surface, 281
point, 1
pointwise convergence, 103
polar coordinates, 248
polar identity, 169
polygon, 146
positive deﬁnite, 193
positive direction relative to , 280
positive matrix, 193
positive side of a surface, 280
positively oriented, 264
power series, 107
principal minors, 196
product of forms, 299
proper subset, 1
Pythagoras, 169
Pythagorean Theorem, 169
q-chain, 315
q-form, 294
q-form of order n, 294
q-surface, 293
q-surface domain, 293
R-cover, 223
radius of convergence, 108
range, 5, 173
Ratio Test, 27
real numbers, 11
rectangle, 223
reﬁnement, 71
regular, 259, 274
relatively closed in, 122
relatively open, 42, 122
reparametrization, 222
Reverse Triangle Inequality, 14, 117
Riemann, Georg Friedrich Bernhard, 73
Riemann integrable, 73, 225
Riemann–Stieltjes integral, 97
right-continuous, 99
right-hand limit, 48
right-hand orientation, 280
Root Test, 27
saddle point, 167
same orientation, 312
Sandwich Principle, 104
Schmidt, Erhard, 170
Schwarz, Hermann Amandus, 117
second derivative, 63
self-adjoint, 177
separable, 124
separately continuous, 133
separates points, 150
sequence, 16
sequence converges, 16
series, 24
series converges, 24
series diverges, 24
sign of a permutation, 180
sign of a real number, 181
simple curve, 258
simple discontinuity, 49
simple mapping, 245
singleton, 2
smooth curve, 156
smooth function, 63, 156
smooth q-form, 294
spectral decomposition, 186
Spectral Theorem, 185
spectrum, 184
spherical coordinates, 249
Squeeze Principle, 104
squeezing principle, 51
standard basis, 158
standard representation, 298
starting point, 258
Stieltjes, Thomas Jan, 97
Stokes, George Gabriel, 288

Index of Terms
337
Stokes’s Theorem, 288
Stone, Marshall, 150
Stone–Weierstrass Theorem, 150
strictly decreasing function, 49
strictly decreasing sequence, 18
strictly increasing function, 49
strictly increasing sequence, 18
strictly monotonic function, 49
subcover, 133
subsequence, 19, 120
subset, 1, 2
subspace, 117
supremum, 7
surface, 199, 269
surface area, 275
surface domain, 269, 293
surface integral, 275
surjective, 5, 173
Swiss cheese, 232
symmetric, 177
symmetric group, 180
tangent aﬃne hyperplane, 200
tangent plane to a surface, 273
Taylor, Brook, 68
Taylor’s Theorem, 68
topologist’s sine curve, 144
total variation, 94
totally bounded, 135
trace of a curve, 156
trace of a q-chain, 313
trace of a surface, 269, 293
transpose, 177
transposition, 180
Triangle Inequality, 115
Trichotomy Law, 11
trigonometric polynomial, 153
twice continuously diﬀerentiable, 63
twice diﬀerentiable, 63
Type I, 264, 284
Type I cover, 265, 285
Type II, 264, 284
Type II cover, 265, 285
Type III, 284
Type III cover, 285
uncountable set, 29
uniform convergence, 103, 107
uniformly Cauchy sequence, 106
uniformly continuous, 41, 132
union, 2, 121
unit circle, 45
unit normal, 277
unit normal vector, 263
unit simplex, 310
unit tangent vector, 260
upper bound, 7
upper limit, 22
upper sum, 71, 225
Urysohn, Pavel Samuilovich, 131
Urysohn’s Lemma, 131
volume, 223, 232
volume zero, 226
wedge, 294
wedge product, 299
Weierstrass, Karl, 19
Weierstrass M-Test, 107
Weierstrass Theorem, 151
zero-form, 294


Index of Symbols
(X, d), 115
(, R), 269, 293
(c, d), 16
(c, d], 16
A −B, 3
A\B, 3
A ∩B, 2
A ∪B, 2
A ⊆X, 1
A∗, 177
At, 177
B(x; r), 122
BV[a, b], 94
C′′(a, b), 63
C′(a, b), 63
Cq, 310, 311
C(1)(a, b), 63
C(2)(a, b), 63
C(n)(a, b), 63
D2 f (x), 192
D f (x) = f ′(x), 163, 188
Iq, 314
L( f , P), 71
L( f , R), 225
MR
j , 225
N(t), 263
N(u, v), 273
S( f , P), 95
Sα( f , P), 95
T (t), 260
U( f , P), 71
U( f , R), 225
V (R), 223
V (X ), 232
X ⊇A, 1
X × Y, 30
X1 × · · · × Xn, 30
[c, d), 16
[x, y], 142
∅, 2
∥A∥, 174
∥L∥, 162
∥P∥, 95
∥x∥, 155
αt, 99

i∈I Ai, 121
∞
n=1 An, 4

i∈I Ai, 121
∞
n=1 An, 4
5, 170
[c, d], 16
χA, 5, 146
cl A, 123
cl E, 36
cos x, 46
curl F, 284
det A, 180
div F, 284
diag (λ1, . . . , λp), 178
diam E, 35, 126
dist (A, B), 140
dist (x, E), 36
dist (x, A), 126
ℓ∞, 128, 140
exp, 82
∂( f1,..., fp)
∂(x1,...,xp) , 205
∂f
∂x , 158
df
dx , 52
γ = γ1 + · · · + γn, 259, 260
γ ′(x), 156
 f , 205
φi,φ j (u0, v0), 271
γ ∼ρ, 222
σ (A), 184
σ = [v0, v1, . . . , vq], 311
σκ, 312
ω f (x), 90

 f dσ, 275
inf E, 7

f dα, 97

X f , 225

340
Index of Symbols

X f (x)dx, 225

 ω, 313

γ F · T ds, 260

γ f , 259
 b
a f , 73
 b
a f (x)dx, 73

γ f ds, 259
int A, 123
int , 276
ker A, 173
λ(γ ), 220
⟨f , d⟩(y), 188
⟨x.y⟩, 117
limn an = a, 16
limx→a+, 48
limx→a−, 48
limx→a f (x), 47
limx→a f (x) = ±∞, 49
lim inf an, 22
lim sup an, 22
log x, 81
logb x, 83
∇f (x), 163
∇d f (x), 166
N, 1
Q, 2
R, 6, 11
R+, 13
R∞, 15, 22
Z, 1
ω( f , δ), 96
B(x; r), 122
co(E), 309
∂A, 123
∂Cp, 317
∂σ, 314
∂, 315
∂, 315
∂, 281
∂j f , 158
∂ji f , 159
∂, 276
ran A, 173
R(X), 225
R[a, b], 73
S⊥, 168
Sq(G), 293, 296
σ (), 275
sin x, 46
∞
n=1 an, 24
m
n=1 an, 24
sup E, 7
| f |, 40
C
dx j, 296
α, 100
{}, 313
{}, 293
{γ }, 156, 258
{an}, 16
{x}, 2
an →a, 16
dxI ∧dxJ, 299
dxI, 298
dx[I,J], 299
dxi1 ∧· · · ∧dxiq, 294
e, 82
e1, . . . , ep, 158
f ′(x), 52
f (X ), 5
f (a+), 48
f (a−), 48
f (x) →L, 47
f /g, 37
f : X →Y, 4
f ◦g, 5
f ± g, 37
f ∨g, 40
f ∧g, 40
f −1, 60
fn →u f , 146
fn →u f , 103
fg, 37
mR
j , 225
s( j1, . . . , jp), 181
x ∈X, 1
x /∈X, 1
x ⊕y, 172, 208
x ⊥y, 168
x × y, 272
xa, 82
xn →−∞, 22
xn →∞, 22
n = n(x, y, z), 277
Rp, 116
Pδ, 95
Var(α), 94
glb E, 7
lub E, 7
sign(σ ), 180
si(a), 181


arXiv:math-ph/0005032   31 May 2000
An Elementary Introduction to Groups and
Representations
Brian C. Hall
Author address:
University of Notre Dame, Department of Mathematics, Notre
Dame IN 46556 USA
E-mail address: bhall@nd.edu


Contents
1.
Preface
ii
Chapter 1.
Groups
1
1.
Deï¬nition of a Group, and Basic Properties
1
2.
Some Examples of Groups
3
3.
Subgroups, the Center, and Direct Products
4
4.
Homomorphisms and Isomorphisms
5
5.
Exercises
6
Chapter 2.
Matrix Lie Groups
9
1.
Deï¬nition of a Matrix Lie Group
9
2.
Examples of Matrix Lie Groups
10
3.
Compactness
15
4.
Connectedness
16
5.
Simple-connectedness
18
6.
Homomorphisms and Isomorphisms
19
7.
Lie Groups
20
8.
Exercises
22
Chapter 3.
Lie Algebras and the Exponential Mapping
27
1.
The Matrix Exponential
27
2.
Computing the Exponential of a Matrix
29
3.
The Matrix Logarithm
31
4.
Further Properties of the Matrix Exponential
34
5.
The Lie Algebra of a Matrix Lie Group
36
6.
Properties of the Lie Algebra
40
7.
The Exponential Mapping
44
8.
Lie Algebras
46
9.
The Complexiï¬cation of a Real Lie Algebra
48
10.
Exercises
50
Chapter 4.
The Baker-Campbell-Hausdorï¬€Formula
53
1.
The Baker-Campbell-Hausdorï¬€Formula for the Heisenberg Group
53
2.
The General Baker-Campbell-Hausdorï¬€Formula
56
3.
The Series Form of the Baker-Campbell-Hausdorï¬€Formula
63
4.
Subgroups and Subalgebras
64
5.
Exercises
65
Chapter 5.
Basic Representation Theory
67
1.
Representations
67
2.
Why Study Representations?
69
iii

iv
CONTENTS
3.
Examples of Representations
70
4.
The Irreducible Representations of su(2)
75
5.
Direct Sums of Representations and Complete Reducibility
79
6.
Tensor Products of Representations
82
7.
Schurâ€™s Lemma
86
8.
Group Versus Lie Algebra Representations
88
9.
Covering Groups
94
10.
Exercises
96
Chapter 6.
The Representations of SU(3), and Beyond
101
1.
Preliminaries
101
2.
Weights and Roots
103
3.
Highest Weights and the Classiï¬cation Theorem
105
4.
Proof of the Classiï¬cation Theorem
107
5.
An Example: Highest Weight (1, 1)
111
6.
The Weyl Group
112
7.
Complex Semisimple Lie Algebras
115
8.
Exercises
117
Chapter 7.
Cumulative exercises
119
Chapter 8.
Bibliography
121

1. PREFACE
v
1. Preface
These notes are the outgrowth of a graduate course on Lie groups I taught
at the University of Virginia in 1994.
In trying to ï¬nd a text for the course I
discovered that books on Lie groups either presuppose a knowledge of diï¬€erentiable
manifolds or provide a mini-course on them at the beginning. Since my students
did not have the necessary background on manifolds, I faced a dilemma: either use
manifold techniques that my students were not familiar with, or else spend much
of the course teaching those techniques instead of teaching Lie theory. To resolve
this dilemma I chose to write my own notes using the notion of a matrix Lie group.
A matrix Lie group is simply a closed subgroup of GL(n; C). Although these are
often called simply â€œmatrix groups,â€ my terminology emphasizes that every matrix
group is a Lie group.
This approach to the subject allows me to get started quickly on Lie group the-
ory proper, with a minimum of prerequisites. Since most of the interesting examples
of Lie groups are matrix Lie groups, there is not too much loss of generality. Fur-
thermore, the proofs of the main results are ultimately similar to standard proofs
in the general setting, but with less preparation.
Of course, there is a price to be paid and certain constructions (e.g. covering
groups) that are easy in the Lie group setting are problematic in the matrix group
setting. (Indeed the universal cover of a matrix Lie group need not be a matrix
Lie group.) On the other hand, the matrix approach suï¬ƒces for a ï¬rst course.
Anyone planning to do research in Lie group theory certainly needs to learn the
manifold approach, but even for such a person it might be helpful to start with a
more concrete approach. And for those in other ï¬elds who simply want to learn
the basics of Lie group theory, this approach allows them to do so quickly.
These notes also use an atypical approach to the theory of semisimple Lie
algebras, namely one that starts with a detailed calculation of the representations
of sl(3; C). My own experience was that the theory of Cartan subalgebras, roots,
Weyl group, etc., was pretty diï¬ƒcult to absorb all at once. I have tried, then, to
motivate these constructions by showing how they are used in the representation
theory of the simplest representative Lie algebra.
(I also work out the case of
sl(2; C), but this case does not adequately illustrate the general theory.)
In the interests of making the notes accessible to as wide an audience as possible,
I have included a very brief introduction to abstract groups, given in Chapter 1.
In fact, not much of abstract group theory is needed, so the quick treatment I give
should be suï¬ƒcient for those who have not seen this material before.
I am grateful to many who have made corrections, large and small, to the notes,
including especially Tom Goebeler, Ruth Gornet, and Erdinch Tatar.

vi
CONTENTS

CHAPTER 1
Groups
1. Deï¬nition of a Group, and Basic Properties
Definition 1.1. A group is a set G, together with a map of G Ã— G into G
(denoted g1 âˆ—g2) with the following properties:
First, associativity: for all g1, g2 âˆˆG,
g1 âˆ—(g2 âˆ—g3) = (g1 âˆ—g2) âˆ—g3.
(1.1)
Second, there exists an element e in G such that for all g âˆˆG,
g âˆ—e = e âˆ—g = g.
(1.2)
and such that for all g âˆˆG, there exists h âˆˆG with
g âˆ—h = h âˆ—g = e.
(1.3)
If g âˆ—h = h âˆ—g for all g, h âˆˆG, then the group is said to be commutative (or
abelian).
The element e is (as we shall see momentarily) unique, and is called the iden-
tity element of the group, or simply the identity. Part of the deï¬nition of a
group is that multiplying a group element g by the identity on either the right or
the left must give back g.
The map of GÃ—G into G is called the product operation for the group. Part
of the deï¬nition of a group G is that the product operation map G Ã—G into G, i.e.,
that the product of two elements of G be again an element of G. This property is
referred to as closure.
Given a group element g, a group element h such that g âˆ—h = hâˆ—g = e is called
an inverse of g. We shall see momentarily that each group element has a unique
inverse.
Given a set and an operation, there are four things that must be checked to show
that this is a group: closure, associativity, existence of an identity, and existence of
inverses.
Proposition 1.2 (Uniqueness of the Identity). Let G be a group, and let e, f âˆˆ
G be such that for all g âˆˆG
e âˆ—g = g âˆ—e = g
f âˆ—g = g âˆ—f = g.
Then e = f.
Proof. Since e is an identity, we have
e âˆ—f = f.
1

2
1. GROUPS
On the other hand, since f is an identity, we have
e âˆ—f = e.
Thus e = e âˆ—f = f.
Proposition 1.3 (Uniqueness of Inverses). Let G be a group, e the (unique)
identity of G, and g, h, k arbitrary elements of G. Suppose that
g âˆ—h = h âˆ—g = e
g âˆ—k = k âˆ—g = e.
Then h = k.
Proof. We know that g âˆ—h = g âˆ—k (= e). Multiplying on the left by h gives
h âˆ—(g âˆ—h) = h âˆ—(g âˆ—k).
By associativity, this gives
(h âˆ—g) âˆ—h = (h âˆ—g) âˆ—k,
and so
e âˆ—h = e âˆ—k
h = k.
This is what we wanted to prove.
Proposition 1.4. Let G be a group, e the identity element of G, and g an
arbitrary element of G. Suppose h âˆˆG satisï¬es either h âˆ—g = e or g âˆ—h = e. Then
h is the (unique) inverse of g.
Proof. To show that h is the inverse of g, we must show both that h âˆ—g = e
and g âˆ—h = e. Suppose we know, say, that h âˆ—g = e. Then our goal is to show that
this implies that g âˆ—h = e.
Since h âˆ—g = e,
g âˆ—(h âˆ—g) = g âˆ—e = g.
By associativity, we have
(g âˆ—h) âˆ—g = g.
Now, by the deï¬nition of a group, g has an inverse. Let k be that inverse. (Of
course, in the end, we will conclude that k = h, but we cannot assume that now.)
Multiplying on the right by k and using associativity again gives
((g âˆ—h) âˆ—g) âˆ—k = g âˆ—k = e
(g âˆ—h) âˆ—(g âˆ—k) = e
(g âˆ—h) âˆ—e = e
g âˆ—h = e.
A similar argument shows that if g âˆ—h = e, then h âˆ—g = e.
Note that in order to show that h âˆ—g = e implies g âˆ—h = e, we used the fact
that g has an inverse, since it is an element of a group. In more general contexts
(that is, in some system which is not a group), one may have h âˆ—g = e but not
g âˆ—h = e. (See Exercise 11.)

2. SOME EXAMPLES OF GROUPS
3
Notation 1.5. For any group element g, its unique inverse will be denoted
gâˆ’1.
Proposition 1.6 (Properties of Inverses). Let G be a group, e its identity, and
g, h arbitrary elements of G. Then
 gâˆ’1âˆ’1 = g
(gh)âˆ’1 = hâˆ’1gâˆ’1
eâˆ’1 = e.
Proof. Exercise.
2. Some Examples of Groups
From now on, we will denote the product of two group elements g1 and g2
simply by g1g2, instead of the more cumbersome g1 âˆ—g2. Moreover, since we have
associativity, we will write simply g1g2g3 in place of (g1g2)g3 or g1(g2g3).
2.1. The trivial group. The set with one element, e, is a group, with the
group operation being deï¬ned as ee = e. This group is commutative.
Associativity is automatic, since both sides of (1.1) must be equal to e. Of
course, e itself is the identity, and is its own inverse. Commutativity is also auto-
matic.
2.2. The integers. The set Z of integers forms a group with the product
operation being addition. This group is commutative.
First, we check closure, namely, that addition maps Z Ã— Z into Z, i.e., that the
sum of two integers is an integer. Since this is obvious, it remains only to check
associativity, identity, and inverses. Addition is associative; zero is the additive
identity (i.e., 0 + n = n + 0 = n, for all n âˆˆZ); each integer n has an additive
inverse, namely, âˆ’n. Since addition is commutative, Z is a commutative group.
2.3. The reals and Rn. The set R of real numbers also forms a group under
the operation of addition. This group is commutative. Similarly, the n-dimensional
Euclidean space Rn forms a group under the operation of vector addition. This
group is also commutative.
The veriï¬cation is the same as for the integers.
2.4. Non-zero real numbers under multiplication. The set of non-zero
real numbers forms a group with respect to the operation of multiplication. This
group is commutative.
Again we check closure: the product of two non-zero real numbers is a non-zero
real number. Multiplication is associative; one is the multiplicative identity; each
non-zero real number x has a multiplicative inverse, namely, 1
x. Since multiplication
of real numbers is commutative, this is a commutative group.
This group is denoted Râˆ—.
2.5. Non-zero complex numbers under multiplication. The set of non-
zero complex numbers forms a group with respect to the operation of complex
multiplication. This group is commutative.
This group in denoted Câˆ—.

4
1. GROUPS
2.6. Complex numbers of absolute value one under multiplication.
The set of complex numbers with absolute value one (i.e., of the form eiÎ¸) forms a
group under complex multiplication. This group is commutative.
This group is the unit circle, denoted S1.
2.7. Invertible matrices. For each positive integer n, the set of all n Ã— n
invertible matrices with real entries forms a group with respect to the operation of
matrix multiplication. This group in non-commutative, for n â‰¥2.
We check closure: the product of two invertible matrices is invertible, since
(AB)âˆ’1 = Bâˆ’1Aâˆ’1. Matrix multiplication is associative; the identity matrix (with
ones down the diagonal, and zeros elsewhere) is the identity element; by deï¬nition,
an invertible matrix has an inverse. Simple examples show that the group is non-
commutative, except in the trivial case n = 1. (See Exercise 8.)
This group is called the general linear group (over the reals), and is denoted
GL(n; R).
2.8. Symmetric group (permutation group). The set of one-to-one, onto
maps of the set {1, 2, Â· Â· Â·n} to itself forms a group under the operation of compo-
sition. This group is non-commutative for n â‰¥3.
We check closure: the composition of two one-to-one, onto maps is again one-
to-one and onto. Composition of functions is associative; the identity map (which
sends 1 to 1, 2 to 2, etc.) is the identity element; a one-to-one, onto map has an
inverse. Simple examples show that the group is non-commutative, as long as n is
at least 3. (See Exercise 10.)
This group is called the symmetric group, and is denoted Sn. A one-to-one,
onto map of {1, 2, Â· Â· Â·n} is a permutation, and so Sn is also called the permutation
group. The group Sn has n! elements.
2.9. Integers mod n. The set {0, 1, Â· Â· Â·n âˆ’1} forms a group under the oper-
ation of addition mod n. This group is commutative.
Explicitly, the group operation is the following. Consider a, b âˆˆ{0, 1 Â· Â· Â·n âˆ’1}.
If a + b < n, then a + b mod n = a + b, if a + b â‰¥n, then a + b mod n = a + b âˆ’n.
(Since a and b are less than n, a+bâˆ’n is less than n; thus we have closure.) To show
associativity, note that both (a+b mod n)+c mod n and a+(b+c mod n) mod n
are equal to a + b + c, minus some multiple of n, and hence diï¬€er by a multiple of
n. But since both are in the set {0, 1, Â· Â· Â·n âˆ’1}, the only possible multiple on n
is zero. Zero is still the identity for addition mod n. The inverse of an element
a âˆˆ{0, 1, Â· Â· Â·n âˆ’1} is n âˆ’a. (Exercise: check that n âˆ’a is in {0, 1, Â· Â· Â·n âˆ’1}, and
that a+(nâˆ’a) mod n = 0.) The group is commutative because ordinary addition
is commutative.
This group is referred to as â€œZ mod n,â€ and is denoted Zn.
3. Subgroups, the Center, and Direct Products
Definition 1.7. A subgroup of a group G is a subset H of G with the follow-
ing properties:
1. The identity is an element of H.
2. If h âˆˆH, then hâˆ’1 âˆˆH.
3. If h1, h2 âˆˆH, then h1h2 âˆˆH .

4. HOMOMORPHISMS AND ISOMORPHISMS
5
The conditions on H guarantee that H is a group, with the same product
operation as G (but restricted to H). Closure is assured by (3), associativity follows
from associativity in G, and the existence of an identity and of inverses is assured
by (1) and (2).
3.1. Examples. Every group G has at least two subgroups: G itself, and the
one-element subgroup {e}. (If G itself is the trivial group, then these two subgroups
coincide.) These are called the trivial subgroups of G.
The set of even integers is a subgroup of Z: zero is even, the negative of an
even integer is even, and the sum of two even integers is even.
The set H of nÃ—n real matrices with determinant one is a subgroup of GL(n; R).
The set H is a subset of GL(n; R) because any matrix with determinant one is invert-
ible. The identity matrix has determinant one, so 1 is satisï¬ed. The determinant of
the inverse is the reciprocal of the determinant, so 2 is satisï¬ed; and the determi-
nant of a product is the product of the determinants, so 3 is satisï¬ed. This group
is called the special linear group (over the reals), and is denoted SL(n; R).
Additional examples, as well as some non-examples, are given in Exercise 2.
Definition 1.8. The center of a group G is the set of all g âˆˆG such that
gh = hg for all h âˆˆG.
It is not hard to see that the center of any group G is a subgroup G.
Definition 1.9. Let G and H be groups, and consider the Cartesian product
of G and H, i.e., the set of ordered pairs (g, h) with g âˆˆG, h âˆˆH. Deï¬ne a product
operation on this set as follows:
(g1, h1)(g2, h2) = (g1g2, h1h2).
This operation makes the Cartesian product of G and H into a group, called the
direct product of G and H and denoted G Ã— H.
It is a simple matter to check that this operation truly makes G Ã— H into a
group. For example, the identity element of G Ã— H is the pair (e1, e2), where e1 is
the identity for G, and e2 is the identity for H.
4. Homomorphisms and Isomorphisms
Definition 1.10. Let G and H be groups.
A map Ï† : G â†’H is called a
homomorphism if Ï†(g1g2) = Ï†(g1)Ï†(g2) for all g1, g2 âˆˆG. If in addition, Ï† is
one-to-one and onto, then Ï† is called an isomorphism. An isomorphism of a
group with itself is called an automorphism.
Proposition 1.11. Let G and H be groups, e1 the identity element of G, and
e2 the identity element of H. If Ï† : G â†’H is a homomorphism, then Ï†(e1) = e2,
and Ï†(gâˆ’1) = Ï†(g)âˆ’1 for all g âˆˆG.
Proof. Let g be any element of G. Then Ï†(g) = Ï†(ge1) = Ï†(g)Ï†(e1). Mul-
tiplying on the left by Ï†(g)âˆ’1 gives e2 = Ï†(e1).
Now consider Ï†(gâˆ’1).
Since
Ï†(e1) = e2, we have e2 = Ï†(e1) = Ï†(ggâˆ’1) = Ï†(g)Ï†(gâˆ’1). In light of Prop. 1.4, we
conclude that Ï†(gâˆ’1) is the inverse of Ï†(g).
Definition 1.12. Let G and H be groups, Ï† : G â†’H a homomorphism, and
e2 the identity element of H. The kernel of Ï† is the set of all g âˆˆG for which
Ï†(g) = e2.

6
1. GROUPS
Proposition 1.13. Let G and H be groups, and Ï† : G â†’H a homomorphism.
Then the kernel of Ï† is a subgroup of G.
Proof. Easy.
4.1. Examples. Given any two groups G and H, we have the trivial homo-
morphism from G to H: Ï†(g) = e for all g âˆˆG. The kernel of this homomorphism
is all of G.
In any group G, the identity map (id(g) = g) is an automorphism of G, whose
kernel is just {e}.
Let G = H = Z, and deï¬ne Ï†(n) = 2n. This is a homomorphism of Z to itself,
but not an automorphism. The kernel of this homomorphism is just {0}.
The determinant is a homomorphism of GL(n, R) to Râˆ—. The kernel of this map
is SL (n, R).
Additional examples are given in Exercises 12 and 7.
If there exists an isomorphism from G to H, then G and H are said to be
isomorphic, and this relationship is denoted G âˆ¼= H. (See Exercise 4.) Two groups
which are isomorphic should be thought of as being (for all practical purposes) the
same group.
5. Exercises
Recall the deï¬nitions of the groups GL(n; R), Sn, Râˆ—, and Zn from Sect. 2, and
the deï¬nition of the group SL(n; R) from Sect. 3.
1. Show that the center of any group G is a subgroup G.
2. In (a)-(f), you are given a group G and a subset H of G. In each case,
determine whether H is a subgroup of G.
(a) G = Z, H = {odd integers}
(b) G = Z, H = {multiples of 3}
(c) G = GL(n; R), H = {A âˆˆGL(n; R) |det A is an integer}
(d) G = SL(n; R), H = {A âˆˆSL(n; R) |all the entries of A are integers}
Hint: recall Kramerâ€™s rule for ï¬nding the inverse of a matrix.
(e) G = GL(n; R), H = {A âˆˆGL(n; R) |all of the entries of A are rational}
(f) G = Z9, H = {0, 2, 4, 6, 8}
3. Verify the properties of inverses in Prop. 1.6.
4. Let G and H be groups. Suppose there exists an isomorphism Ï† from G to
H. Show that there exists an isomorphism from H to G.
5. Show that the set of positive real numbers is a subgroup of Râˆ—. Show that
this group is isomorphic to the group R.
6. Show that the set of automorphisms of any group G is itself a group, under
the operation of composition. This group is the automorphism group of
G, Aut(G).
7. Given any group G, and any element g in G, deï¬ne Ï†g : G â†’G by Ï†g(h) =
ghgâˆ’1. Show that Ï†g is an automorphism of G. Show that the map g â†’Ï†g
is a homomorphism of G into Aut(G), and that the kernel of this map is the
center of G.
Note: An automorphism which can be expressed as Ï†g for some g âˆˆG
is called an inner automorphism; any automorphism of G which is not
equal to any Ï†g is called an outer automorphism.

5. EXERCISES
7
8. Give an example of two 2Ã—2 invertible real matrices which do not commute.
(This shows that GL(2, R) is not commutative.)
9. Show that in any group G, the center of G is a subgroup.
10. An element Ïƒ of the permutation group Sn can be written in two-row form,
Ïƒ =

1
2
Â· Â· Â·
n
Ïƒ1
Ïƒ2
Â· Â· Â·
Ïƒn

where Ïƒi denotes Ïƒ(i). Thus
Ïƒ =
 1
2
3
2
3
1

is the element of S3 which sends 1 to 2, 2 to 3, and 3 to 1. When multiplying
(i.e., composing) two permutations, one performs the one on the right ï¬rst,
and then the one on the left. (This is the usual convention for composing
functions.)
Compute
 1
2
3
2
1
3
  1
2
3
1
3
2

and
 1
2
3
1
3
2
  1
2
3
2
1
3

Conclude that S3 is not commutative.
11. Consider the set N= {0, 1, 2, Â· Â· Â·} of natural numbers, and the set F of all
functions of N to itself. Composition of functions deï¬nes a map of F Ã— F
into F, which is associative. The identity (id(n) = n) has the property that
id â—¦f = f â—¦id = f, for all f in F. However, since we do not restrict to
functions which are one-to-one and onto, not every element of F has an
inverse. Thus F is not a group.
Give an example of two functions f, g in F such that f â—¦g = id, but
g â—¦f Ì¸= id. (Compare with Prop. 1.4.)
12. Consider the groups Z and Zn. For each a in Z, deï¬ne a mod n to be the
unique element b of {0, 1, Â· Â· Â·n âˆ’1} such that a can be written as a = kn+b,
with k an integer. Show that the map a â†’a mod n is a homomorphism of
Z into Zn.
13. Let G be a group, and H a subgroup of G. H is called a normal subgroup
of G if given any g âˆˆG, and h âˆˆH, ghgâˆ’1 is in H.
Show that any subgroup of a commutative group is normal. Show that
in any group G, the trivial subgroups G and {e} are normal. Show that the
center of any group is a normal subgroup. Show that if Ï† is a homomorphism
from G to H, then the kernel of Ï† is a normal subgroup of G.
Show that SL(n; R) is a normal subgroup of GL(n; R).
Note: a group G with no normal subgroups other than G and {e} is
called simple.

8
1. GROUPS

CHAPTER 2
Matrix Lie Groups
1. Deï¬nition of a Matrix Lie Group
Recall that the general linear group over the reals, denoted GL(n; R), is the
group of all n Ã— n invertible matrices with real entries. We may similarly deï¬ne
GL(n; C) to be the group of all n Ã— n invertible matrices with complex entries. Of
course, GL(n; R) is contained in GL(n; C).
Definition 2.1. Let An be a sequence of complex matrices. We say that An
converges to a matrix A if each entry of An converges to the corresponding entry
of A, i.e., if (An)ij converges to Aij for all 1 â‰¤i, j â‰¤n.
Definition 2.2. A matrix Lie group is any subgroup H of GL(n; C) with the
following property: if An is any sequence of matrices in H, and An converges to
some matrix A, then either A âˆˆH, or A is not invertible.
The condition on H amounts to saying that H is a closed subset of GL(n; C).
(This is not the same as saying that H is closed in the space of all matrices.) Thus
Deï¬nition 2.2 is equivalent to saying that a matrix Lie group is a closed subgroup
of GL(n; C).
The condition that H be a closed subgroup, as opposed to merely a subgroup,
should be regarded as a technicality, in that most of the interesting subgroups of
GL(n; C) have this property. (Almost all of the matrix Lie groups H we will consider
have the stronger property that if An is any sequence of matrices in H, and An
converges to some matrix A, then A âˆˆH.)
There is a topological structure on the set of n Ã— n complex matrices which
goes with the above notion of convergence. This topological structure is deï¬ned by
identifying the space of n Ã— n matrices with Cn2 in the obvious way and using the
usual topological structure on Cn2.
1.1. Counterexamples. An example of a subgroup of GL(n; C) which is not
closed (and hence is not a matrix Lie group) is the set of all n Ã— n invertible
matrices all of whose entries are real and rational. This is in fact a subgroup of
GL(n; C), but not a closed subgroup. That is, one can (easily) have a sequence
of invertible matrices with rational entries converging to an invertible matrix with
some irrational entries. (In fact, every real invertible matrix is the limit of some
sequence of invertible matrices with rational entries.)
Another example of a group of matrices which is not a matrix Lie group is the
following subgroup of GL(2, C). Let a be an irrational real number, and let
H =

eit
0
0
eita

|t âˆˆR

9

10
2. MATRIX LIE GROUPS
Clearly, H is a subgroup of GL(2, C). Because a is irrational, the matrix âˆ’I is not
in H, since to make eit equal to âˆ’1, we must take t to be an odd integer multiple
of Ï€, in which case ta cannot be an odd integer multiple of Ï€. On the other hand,
by taking t = (2n + 1)Ï€ for a suitably chosen integer n, we can make ta arbitrarily
close to an odd integer multiple of Ï€. (It is left to the reader to verify this.) Hence
we can ï¬nd a sequence of matrices in H which converges to âˆ’I, and so H is not a
matrix Lie group. See Exercise 1.
2. Examples of Matrix Lie Groups
Mastering the subject of Lie groups involves not only learning the general the-
ory, but also familiarizing oneself with examples. In this section, we introduce some
of the most important examples of (matrix) Lie groups.
2.1. The general linear groups GL(n; R) and GL(n; C). The general linear
groups (over R or C) are themselves matrix Lie groups. Of course, GL(n; C) is a
subgroup of itself. Furthermore, if An is a sequence of matrices in GL(n; C) and An
converges to A, then by the deï¬nition of GL(n; C), either A is in GL(n; C), or A is
not invertible.
Moreover, GL(n; R) is a subgroup of GL(n; C), and if An âˆˆGL(n; R), and An
converges to A, then the entries of A are real. Thus either A is not invertible, or
A âˆˆGL(n; R).
2.2. The special linear groups SL(n; R) and SL(n; C). The special linear
group (over R or C) is the group of nÃ—n invertible matrices (with real or complex
entries) having determinant one. Both of these are subgroups of GL(n; C), as noted
in Chapter 1. Furthermore, if An is a sequence of matrices with determinant one,
and An converges to A, then A also has determinant one, because the determinant
is a continuous function. Thus SL(n; R) and SL(n; C) are matrix Lie groups.
2.3. The orthogonal and special orthogonal groups, O(n) and SO(n).
An n Ã— n real matrix A is said to be orthogonal if the column vectors that make
up A are orthonormal, that is, if
n
X
i=1
AijAik = Î´jk
Equivalently, A is orthogonal if it preserves the inner product, namely, if âŸ¨x, yâŸ©=
âŸ¨Ax, AyâŸ©for all vectors x, y in Rn. ( Angled brackets denote the usual inner product
on Rn, âŸ¨x, yâŸ©= P
i xiyi.) Still another equivalent deï¬nition is that A is orthogonal
if AtrA = I, i.e., if Atr = Aâˆ’1. (Atr is the transpose of A, (Atr)ij = Aji.) See
Exercise 2.
Since det Atr = det A, we see that if A is orthogonal, then det(AtrA) =
(det A)2 = det I = 1. Hence det A = Â±1, for all orthogonal matrices A.
This formula tells us, in particular, that every orthogonal matrix must be in-
vertible. But if A is an orthogonal matrix, then

Aâˆ’1x, Aâˆ’1y = 
A  Aâˆ’1x, A  Aâˆ’1x = âŸ¨x, yâŸ©
Thus the inverse of an orthogonal matrix is orthogonal. Furthermore, the product
of two orthogonal matrices is orthogonal, since if A and B both preserve inner
products, then so does AB. Thus the set of orthogonal matrices forms a group.

2. EXAMPLES OF MATRIX LIE GROUPS
11
The set of all n Ã— n real orthogonal matrices is the orthogonal group O(n),
and is a subgroup of GL(n; C). The limit of a sequence of orthogonal matrices is
orthogonal, because the relation AtrA = I is preserved under limits. Thus O(n) is
a matrix Lie group.
The set of n Ã— n orthogonal matrices with determinant one is the special or-
thogonal group SO(n). Clearly this is a subgroup of O(n), and hence of GL(n; C).
Moreover, both orthogonality and the property of having determinant one are pre-
served under limits, and so SO(n) is a matrix Lie group. Since elements of O(n)
already have determinant Â±1, SO(n) is â€œhalfâ€ of O(n).
Geometrically, elements of O(n) are either rotations, or combinations of rota-
tions and reï¬‚ections. The elements of SO(n) are just the rotations.
See also Exercise 6.
2.4. The unitary and special unitary groups, U(n) and SU(n). An nÃ—n
complex matrix A is said to be unitary if the column vectors of A are orthonormal,
that is, if
n
X
i=1
AijAik = Î´jk
Equivalently, A is unitary if it preserves the inner product, namely, if âŸ¨x, yâŸ©=
âŸ¨Ax, AyâŸ©for all vectors x, y in Cn. (Angled brackets here denote the inner product
on Cn, âŸ¨x, yâŸ©= P
i xiyi. We will adopt the convention of putting the complex
conjugate on the left.) Still another equivalent deï¬nition is that A is unitary if
Aâˆ—A = I, i.e., if Aâˆ—= Aâˆ’1. (Aâˆ—is the adjoint of A, (Aâˆ—)ij = Aji.) See Exercise 3.
Since det Aâˆ—= det A, we see that if A is unitary, then det (Aâˆ—A) = |det A|2 =
det I = 1. Hence |det A| = 1, for all unitary matrices A.
This in particular shows that every unitary matrix is invertible.
The same
argument as for the orthogonal group shows that the set of unitary matrices forms
a group.
The set of all n Ã— n unitary matrices is the unitary group U(n), and is a
subgroup of GL(n; C). The limit of unitary matrices is unitary, so U(n) is a matrix
Lie group. The set of unitary matrices with determinant one is the special unitary
group SU(n). It is easy to check that SU(n) is a matrix Lie group. Note that a
unitary matrix can have determinant eiÎ¸ for any Î¸, and so SU(n) is a smaller subset
of U(n) than SO(n) is of O(n). (Speciï¬cally, SO(n) has the same dimension as
O(n), whereas SU(n) has dimension one less than that of U(n).)
See also Exercise 8.
2.5. The complex orthogonal groups, O(n; C) and SO(n; C). Consider
the bilinear form ( ) on Cn deï¬ned by (x, y) = Pxiyi. This form is not an inner
product, because of the lack of a complex conjugate in the deï¬nition. The set of all
nÃ—n complex matrices A which preserve this form, (i.e., such that (Ax, Ay) = (x, y)
for all x, y âˆˆCn) is the complex orthogonal group O(n; C), and is a subgroup
of GL(n; C). (The proof is the same as for O(n).) An n Ã— n complex matrix A is
in O(n; C) if and only if AtrA = I. It is easy to show that O(n; C) is a matrix Lie
group, and that det A = Â±1, for all A in O(n; C). Note that O(n; C) is not the
same as the unitary group U(n). The group SO(n; C) is deï¬ned to be the set of all
A in O(n; C) with det A = 1. Then SO(n; C) is also a matrix Lie group.

12
2. MATRIX LIE GROUPS
2.6. The generalized orthogonal and Lorentz groups. Let n and k be
positive integers, and consider Rn+k. Deï¬ne a symmetric bilinear form [ ]n+k on
Rn+k by the formula
[x, y]n,k = x1y1 + Â· Â· Â· + xnyn âˆ’xn+1yn+1 Â· Â· Â· âˆ’yn+kxn+k
(2.1)
The set of (n+k)Ã—(n+k) real matrices A which preserve this form (i.e., such that
[Ax, Ay]n,k = [x, y]n,k for all x, y âˆˆRn+k) is the generalized orthogonal group
O(n; k), and it is a subgroup of GL(n+k; R) (Ex. 4). Since O(n; k) and O(k; n) are
essentially the same group, we restrict our attention to the case n â‰¥k. It is not
hard to check that O(n; k) is a matrix Lie group.
If A is an (n + k) Ã— (n + k) real matrix, let A(i) denote the ith column vector
of A, that is
A(i) =
ï£«
ï£¬
ï£­
A1,i
...
An+k,i
ï£¶
ï£·
ï£¸
Then A is in O(n; k) if and only if the following conditions are satisï¬ed:
A(i), A(j)
n,k
=
0
i Ì¸= j
A(i), A(i)
n,k
=
1
1 â‰¤i â‰¤n

A(i), A(i)
n,k
=
âˆ’1
n + 1 â‰¤i â‰¤n + k
(2.2)
Let g denote the (n + k) Ã— (n + k) diagonal matrix with ones in the ï¬rst n
diagonal entries, and minus ones in the last k diagonal entries. Then A is in O(n; k)
if and only if AtrgA = g (Ex. 4). Taking the determinant of this equation gives
(det A)2 det g = det g, or (det A)2 = 1. Thus for any A in O(n; k), det A = Â±1.
The group SO(n; k) is deï¬ned to be the set of matrices in O(n; k) with det A = 1.
This is a subgroup of GL(n + k; R), and is a matrix Lie group.
Of particular interest in physics is the Lorentz group O(3; 1). (Sometimes
the phrase Lorentz group is used more generally to refer to the group O(n; 1) for
any n â‰¥1.) See also Exercise 7.
2.7. The symplectic groups Sp(n; R), Sp(n; C), and Sp(n). The special
and general linear groups, the orthogonal and unitary groups, and the symplectic
groups (which will be deï¬ned momentarily) make up the classical groups. Of the
classical groups, the symplectic groups have the most confusing deï¬nition, partly
because there are three sets of them (Sp(n; R), Sp(n; C), and Sp(n)), and partly
because they involve skew-symmetric bilinear forms rather than the more familiar
symmetric bilinear forms. To further confuse matters, the notation for referring to
these groups is not consistent from author to author.
Consider the skew-symmetric bilinear form B on R2n deï¬ned as follows:
B [x, y] =
n
X
i=1
xiyn+i âˆ’xn+iyi
(2.3)
The set of all 2n Ã— 2n matrices A which preserve B (i.e., such that B [Ax, Ay] =
B [x, y] for all x, y âˆˆR2n) is the real symplectic group Sp(n; R), and it is a
subgroup of GL(2n; R). It is not diï¬ƒcult to check that this is a matrix Lie group

2. EXAMPLES OF MATRIX LIE GROUPS
13
(Exercise 5). This group arises naturally in the study of classical mechanics. If J
is the 2n Ã— 2n matrix
J =

0
I
âˆ’I
0

then B [x, y] = âŸ¨x, JyâŸ©, and it is possible to check that a 2nÃ—2n real matrix A is in
Sp(n; R) if and only if AtrJA = J. (See Exercise 5.) Taking the determinant of this
identity gives (det A)2 det J = det J, or (det A)2 = 1. This shows that det A = Â±1,
for all A âˆˆSp(n; R). In fact, det A = 1 for all A âˆˆSp(n; R), although this is not
obvious.
One can deï¬ne a bilinear form on Cn by the same formula (2.3). (This form is
bilinear, not Hermitian, and involves no complex conjugates.) The set of 2n Ã— 2n
complex matrices which preserve this form is the complex symplectic group
Sp(n; C). A 2n Ã— 2n complex matrix A is in Sp(n; C) if and only if AtrJA = J.
(Note: this condition involves Atr, not Aâˆ—.) This relation shows that det A = Â±1,
for all A âˆˆSp(n; C). In fact det A = 1, for all A âˆˆSp(n; C).
Finally, we have the compact symplectic group Sp(n) deï¬ned as
Sp(n) = Sp (n; C) âˆ©U(2n).
See also Exercise 9. For more information and a proof of the fact that det A = 1,
for all A âˆˆSp(n; C), see Miller, Sect. 9.4. What we call Sp (n; C) Miller calls Sp(n),
and what we call Sp(n), Miller calls USp(n).
2.8. The Heisenberg group H. The set of all 3 Ã— 3 real matrices A of the
form
A =
ï£«
ï£­
1
a
b
0
1
c
0
0
1
ï£¶
ï£¸
(2.4)
where a, b, and c are arbitrary real numbers, is the Heisenberg group. It is easy
to check that the product of two matrices of the form (2.4) is again of that form, and
clearly the identity matrix is of the form (2.4). Furthermore, direct computation
shows that if A is as in (2.4), then
Aâˆ’1 =
ï£«
ï£­
1
âˆ’a
ac âˆ’b
0
1
âˆ’c
0
0
1
ï£¶
ï£¸
Thus H is a subgroup of GL(3; R). Clearly the limit of matrices of the form (2.4)
is again of that form, and so H is a matrix Lie group.
It is not evident at the moment why this group should be called the Heisenberg
group.
We shall see later that the Lie algebra of H gives a realization of the
Heisenberg commutation relations of quantum mechanics. (See especially Chapter
5, Exercise 10.)
See also Exercise 10.
2.9. The groups Râˆ—, Câˆ—, S1, R, and Rn. Several important groups which
are not naturally groups of matrices can (and will in these notes) be thought of as
such.
The group Râˆ—of non-zero real numbers under multiplication is isomorphic to
GL(1, R). Thus we will regard Râˆ—as a matrix Lie group. Similarly, the group Câˆ—

14
2. MATRIX LIE GROUPS
of non-zero complex numbers under multiplication is isomorphic to GL(1; C), and
the group S1 of complex numbers with absolute value one is isomorphic to U(1).
The group R under addition is isomorphic to GL(1; R)+ (1Ã—1 real matrices with
positive determinant) via the map x â†’[ex]. The group Rn (with vector addition)
is isomorphic to the group of diagonal real matrices with positive diagonal entries,
via the map
(x1, Â· Â· Â· , xn) â†’
ï£«
ï£¬
ï£­
ex1
0
...
0
exn
ï£¶
ï£·
ï£¸.
2.10. The Euclidean and PoincarÂ´e groups. The Euclidean group E(n)
is by deï¬nition the group of all one-to-one, onto, distance-preserving maps of Rn
to itself, that is, maps f : Rn â†’Rn such that d (f (x), f (y)) = d (x, y) for all
x, y âˆˆRn. Here d is the usual distance on Rn, d (x, y) = |x âˆ’y| . Note that we
donâ€™t assume anything about the structure of f besides the above properties. In
particular, f need not be linear. The orthogonal group O(n) is a subgroup of E(n),
and is the group of all linear distance-preserving maps of Rn to itself. The set of
translations of Rn (i.e., the set of maps of the form Tx(y) = x+y) is also a subgroup
of E(n).
Proposition 2.3. Every element T of E(n) can be written uniquely as an or-
thogonal linear transformation followed by a translation, that is, in the form
T = TxR
with x âˆˆRn, and R âˆˆO(n).
We will not prove this here. The key step is to prove that every one-to-one,
onto, distance-preserving map of Rn to itself which ï¬xes the origin must be linear.
Following Miller, we will write an element T = TxR of E(n) as a pair {x, R}.
Note that for y âˆˆRn,
{x, R}y = Ry + x
and that
{x1, R1}{x2, R2}y = R1(R2y + x2) + x1 = R1R2y + (x1 + R1x2)
Thus the product operation for E(n) is the following:
{x1, R1}{x2, R2} = {x1 + R1x2, R1R2}
(2.5)
The inverse of an element of E(n) is given by
{x, R}âˆ’1 = {âˆ’Râˆ’1x, Râˆ’1}
Now, as already noted, E(n) is not a subgroup of GL(n; R), since translations
are not linear maps. However, E(n) is isomorphic to a subgroup of GL(n + 1; R),
via the map which associates to {x, R} âˆˆE(n) the following matrix
ï£«
ï£¬
ï£¬
ï£¬
ï£­
x1
R
...
xn
0
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸
(2.6)
This map is clearly one-to-one, and it is a simple computation to show that it is a
homomorphism. Thus E(n) is isomorphic to the group of all matrices of the form

3. COMPACTNESS
15
(2.6) (with R âˆˆO(n)). The limit of things of the form (2.6) is again of that form,
and so we have expressed the Euclidean group E(n) as a matrix Lie group.
We similarly deï¬ne the PoincarÂ´e group P(n; 1) to be the group of all transfor-
mations of Rn+1 of the form
T = TxA
with x âˆˆRn+1, A âˆˆO(n; 1). This is the group of aï¬ƒne transformations of Rn+1
which preserve the Lorentz â€œdistanceâ€ dL(x, y) = (x1 âˆ’y1)2 + Â· Â· Â· + (xn âˆ’yn)2 âˆ’
(xn+1 âˆ’yn+1)2. (An aï¬ƒne transformation is one of the form x â†’Ax + b, where
A is a linear transformation and b is constant.) The group product is the obvious
analog of the product (2.5) for the Euclidean group.
The PoincarÂ´e group P(n; 1) is isomorphic to the group of (n + 2) Ã— (n + 2)
matrices of the form
ï£«
ï£¬
ï£¬
ï£¬
ï£­
x1
A
...
xn+1
0
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸
(2.7)
with A âˆˆO(n; 1). The set of matrices of the form (2.7) is a matrix Lie group.
3. Compactness
Definition 2.4. A matrix Lie group G is said to be compact if the following
two conditions are satisï¬ed:
1. If An is any sequence of matrices in G, and An converges to a matrix A,
then A is in G.
2. There exists a constant C such that for all A âˆˆG, |Aij| â‰¤C for all 1 â‰¤
i, j â‰¤n.
This is not the usual topological deï¬nition of compactness. However, the set
of all n Ã— n complex matrices can be thought of as Cn2. The above deï¬nition says
that G is compact if it is a closed, bounded subset of Cn2. It is a standard theorem
from elementary analysis that a subset of Cm is compact (in the usual sense that
every open cover has a ï¬nite subcover) if and only if it is closed and bounded.
All of our examples of matrix Lie groups except GL(n; R) and GL(n; C) have
property (1). Thus it is the boundedness condition (2) that is most important.
The property of compactness has very important implications.
For exam-
ple, if G is compact, then every irreducible unitary representation of G is ï¬nite-
dimensional.
3.1. Examples of compact groups. The groups O(n) and SO(n) are com-
pact. Property (1) is satisï¬ed because the limit of orthogonal matrices is orthogonal
and the limit of matrices with determinant one has determinant one. Property (2)
is satisï¬ed because if A is orthogonal, then the column vectors of A have norm one,
and hence |Aij| â‰¤1, for all 1 â‰¤i, j â‰¤n. A similar argument shows that U(n),
SU(n), and Sp(n) are compact. (This includes the unit circle, S1 âˆ¼= U(1).)

16
2. MATRIX LIE GROUPS
3.2. Examples of non-compact groups. All of the other examples given
of matrix Lie groups are non-compact. GL(n; R) and GL(n; C) violate property (1),
since a limit of invertible matrices may be non-invertible. SL (n; R) and SL (n; C)
violate (2), except in the trivial case n = 1, since
An =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
n
1
n
1
...
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
has determinant one, no matter how big n is.
The following groups also violate (2), and hence are non-compact: O(n; C) and
SO(n; C); O(n; k) and SO(n; k) (n â‰¥1, k â‰¥1); the Heisenberg group H; Sp (n; R)
and Sp (n; C); E(n) and P(n; 1); R and Rn; Râˆ—and Câˆ—. It is left to the reader to
provide examples to show that this is the case.
4. Connectedness
Definition 2.5. A matrix Lie group G is said to be connected if given any
two matrices A and B in G, there exists a continuous path A(t), a â‰¤t â‰¤b, lying
in G with A(a) = A, and A(b) = B.
This property is what is called path-connected in topology, which is not (in
general) the same as connected. However, it is a fact (not particularly obvious at
the moment) that a matrix Lie group is connected if and only if it is path-connected.
So in a slight abuse of terminology we shall continue to refer to the above property
as connectedness. (See Section 7.)
A matrix Lie group G which is not connected can be decomposed (uniquely)
as a union of several pieces, called components, such that two elements of the
same component can be joined by a continuous path, but two elements of diï¬€erent
components cannot.
Proposition 2.6. If G is a matrix Lie group, then the component of G con-
taining the identity is a subgroup of G.
Proof. Saying that A and B are both in the component containing the identity
means that there exist continuous paths A(t) and B(t) with A(0) = B(0) = I,
A(1) = A, and B(1) = B. But then A(t)B(t) is a continuous path starting at I and
ending at AB. Thus the product of two elements of the identity component is again
in the identity component. Furthermore, A(t)âˆ’1 is a continuous path starting at I
and ending at Aâˆ’1, and so the inverse of any element of the identity component is
again in the identity component. Thus the identity component is a subgroup.
Proposition 2.7. The group GL(n; C) is connected for all n â‰¥1.
Proof. Consider ï¬rst the case n = 1. A 1 Ã— 1 invertible complex matrix A is
of the form A = [Î»] with Î» âˆˆCâˆ—, the set of non-zero complex numbers. But given
any two non-zero complex numbers, we can easily ï¬nd a continuous path which
connects them and does not pass through zero.
For the case n â‰¥1, we use the Jordan canonical form. Every n Ã— n complex
matrix A can be written as
A = CBCâˆ’1

4. CONNECTEDNESS
17
where B is the Jordan canonical form. The only property of B we will need is that
B is upper-triangular:
B =
ï£«
ï£¬
ï£­
Î»1
âˆ—
...
0
Î»n
ï£¶
ï£·
ï£¸
If A is invertible, then all the Î»iâ€™s must be non-zero, since det A = det B = Î»1 Â· Â· Â·Î»n.
Let B(t) be obtained by multiplying the part of B above the diagonal by (1âˆ’t),
for 0 â‰¤t â‰¤1, and let A(t) = CB(t)Câˆ’1. Then A(t) is a continuous path which
starts at A and ends at CDCâˆ’1, where D is the diagonal matrix
D =
ï£«
ï£¬
ï£­
Î»1
0
...
0
Î»n
ï£¶
ï£·
ï£¸
This path lies in GL(n; C) since det A(t) = Î»1 Â· Â· Â·Î»n for all t.
But now, as in the case n = 1, we can deï¬ne Î»i(t) which connects each Î»i to 1
in Câˆ—, as t goes from 1 to 2. Then we can deï¬ne
A(t) = C
ï£«
ï£¬
ï£­
Î»1(t)
0
...
0
Î»n(t)
ï£¶
ï£·
ï£¸Câˆ’1
This is a continuous path which starts at CDCâˆ’1 when t = 1, and ends at I
(= CICâˆ’1) when t = 2. Since the Î»i(t)â€™s are always non-zero, A(t) lies in GL(n; C).
We see, then, that every matrix A in GL(n; C) can be connected to the identity
by a continuous path lying in GL(n; C).
Thus if A and B are two matrices in
GL(n; C), they can be connected by connecting each of them to the identity.
Proposition 2.8. The group SL (n; C) is connected for all n â‰¥1.
Proof. The proof is almost the same as for GL(n; C), except that we must
be careful to preserve the condition det A = 1. Let A be an arbitrary element of
SL (n; C). The case n = 1 is trivial, so we assume n â‰¥2. We can deï¬ne A(t) as above
for 0 â‰¤t â‰¤1, with A(0) = A, and A(1) = CDCâˆ’1, since det A(t) = det A = 1. Now
deï¬ne Î»i(t) as before for 1 â‰¤i â‰¤n âˆ’1, and deï¬ne Î»n(t) to be [Î»1(t) Â· Â· Â· Î»nâˆ’1(t)]âˆ’1.
(Note that since Î»1 Â· Â· Â·Î»n = 1, Î»n(0) = Î»n.) This allows us to connect A to the
identity while staying within SL (n; C).
Proposition 2.9. The groups U(n) and SU(n) are connected, for all n â‰¥1.
Proof. By a standard result of linear algebra, every unitary matrix has an
orthonormal basis of eigenvectors, with eigenvalues of the form eiÎ¸. It follows that
every unitary matrix U can be written as
U = U1
ï£«
ï£¬
ï£­
eiÎ¸1
0
...
0
eiÎ¸n
ï£¶
ï£·
ï£¸U âˆ’1
1
(2.8)

18
2. MATRIX LIE GROUPS
with U1 unitary and Î¸i âˆˆR. Conversely, as is easily checked, every matrix of the
form (2.8) is unitary. Now deï¬ne
U(t) = U1
ï£«
ï£¬
ï£­
ei(1âˆ’t)Î¸1
0
...
0
ei(1âˆ’t)Î¸n
ï£¶
ï£·
ï£¸U âˆ’1
1
As t ranges from 0 to 1, this deï¬nes a continuous path in U(n) joining U to I. This
shows that U(n) is connected.
A slight modiï¬cation of this argument, as in the proof of Proposition 2.8, shows
that SU(n) is connected.
Proposition 2.10. The group GL(n; R) is not connected, but has two compo-
nents. These are GL(n; R)+, the set of nÃ—n real matrices with positive determinant,
and GL(n; R)âˆ’, the set of n Ã— n real matrices with negative determinant.
Proof. GL(n; R) cannot be connected, for if det A > 0 and det B < 0, then any
continuous path connecting A to B would have to include a matrix with determinant
zero, and hence pass outside of GL(n; R).
The proof that GL(n; R)+ is connected is given in Exercise 14. Once GL(n; R)+
is known to be connected, it is not diï¬ƒcult to see that GL(n; R)âˆ’is also connected.
For let C be any matrix with negative determinant, and take A, B in GL(n; R)âˆ’.
Then Câˆ’1A and Câˆ’1B are in GL(n; R)+, and can be joined by a continuous path
D(t) in GL(n; R)+.
But then CD(t) is a continuous path joining A and B in
GL(n; R)âˆ’.
The following table lists some matrix Lie groups, indicates whether or not the
group is connected, and gives the number of components.
Group
Connected?
Components
GL(n; C)
yes
1
SL (n; C)
yes
1
GL(n; R)
no
2
SL (n; R)
yes
1
O(n)
no
2
SO(n)
yes
1
U(n)
yes
1
SU(n)
yes
1
O(n; 1)
no
4
SO(n; 1)
no
2
Heisenberg
yes
1
E (n)
no
2
P(n; 1)
no
4
Proofs of some of these results are given in Exercises 7, 11, 13, and 14.
(The
connectedness of the Heisenberg group is immediate.)
5. Simple-connectedness
Definition 2.11. A connected matrix Lie group G is said to be simply con-
nected if every loop in G can be shrunk continuously to a point in G.
More precisely, G is simply connected if given any continuous path A(t), 0 â‰¤
t â‰¤1, lying in G with A(0) = A(1), there exists a continuous function A(s, t),

6. HOMOMORPHISMS AND ISOMORPHISMS
19
0 â‰¤s, t â‰¤1, taking values in G with the following properties: 1) A(s, 0) = A(s, 1)
for all s, 2) A(0, t) = A(t), and 3) A(1, t) = A(1, 0) for all t.
You should think of A(t) as a loop, and A(s, t) as a parameterized family of
loops which shrinks A(t) to a point. Condition 1) says that for each value of the
parameter s, we have a loop; condition 2) says that when s = 0 the loop is the
speciï¬ed loop A(t); and condition 3) says that when s = 1 our loop is a point.
It is customary to speak of simple-connectedness only for connected matrix Lie
groups, even though the deï¬nition makes sense for disconnected groups.
Proposition 2.12. The group SU(2) is simply connected.
Proof. Exercise 8 shows that SU(2) may be thought of (topologically) as the
three-dimensional sphere S3 sitting inside R4. It is well-known that S3 is simply
connected.
The condition of simple-connectedness is extremely important. One of our most
important theorems will be that if G is simply connected, then there is a natural
one-to-one correspondence between the representations of G and the representations
of its Lie algebra.
Without proof, we give the following table.
Group
Simply connected?
GL(n; C)
no
SL (n; C)
yes
GL(n; R)
no
SL (n; R)
no
SO(n)
no
U(n)
no
SU(n)
yes
SO(1; 1)
yes
SO(n; 1) (n â‰¥2)
no
Heisenberg
yes
6. Homomorphisms and Isomorphisms
Definition 2.13. Let G and H be matrix Lie groups. A map Ï† from G to H
is called a Lie group homomorphism if 1) Ï† is a group homomorphism and 2)
Ï† is continuous. If in addition, Ï† is one-to-one and onto, and the inverse map Ï†âˆ’1
is continuous, then Ï† is called a Lie group isomorphism.
The condition that Ï† be continuous should be regarded as a technicality, in
that it is very diï¬ƒcult to give an example of a group homomorphism between two
matrix Lie groups which is not continuous. In fact, if G = R and H = Câˆ—, then
any group homomorphism from G to H which is even measurable (a very weak
condition) must be continuous. (See W. Rudin, Real and Complex Analysis, Chap.
9, Ex. 17.)
If G and H are matrix Lie groups, and there exists a Lie group isomorphism
from G to H, then G and H are said to be isomorphic, and we write G âˆ¼= H. Two
matrix Lie groups which are isomorphic should be thought of as being essentially
the same group. (Note that by deï¬nition, the inverse of Lie group isomorphism is
continuous, and so also a Lie group isomorphism.)

20
2. MATRIX LIE GROUPS
6.1. Example: SU(2) and SO(3). A very important topic for us will be the
relationship between the groups SU(2) and SO(3).
This example is designed to
show that SU(2) and SO(3) are almost (but not quite!) isomorphic. Speciï¬cally,
there exists a Lie group homomorphism Ï† which maps SU(2) onto SO(3), and which
is two-to-one. (See Miller 7.1 and BrÂ¨ocker, Chap. I, 6.18.)
Consider the space V of all 2 Ã— 2 complex matrices which are self-adjoint and
have trace zero. This is a three-dimensional real vector space with the following
basis
A1 =
 0
1
1
0

;
A2 =

0
i
âˆ’i
0

;
A3 =
 1
0
0
âˆ’1

We may deï¬ne an inner product on V by the formula
âŸ¨A, BâŸ©= 1
2trace(AB)
(Exercise: check that this is an inner product.)
Direct computation shows that {A1, A2, A3} is an orthonormal basis for V .
Having chosen an orthonormal basis for V , we can identify V with R3.
Now, if U is an element of SU(2), and A is an element of V , then it is easy to
see that UAU âˆ’1 is in V . Thus for each U âˆˆSU(2), we can deï¬ne a linear map Ï†U
of V to itself by the formula
Ï†U(A) = UAU âˆ’1
(This deï¬nition would work for U âˆˆU(2), but we choose to restrict our attention
to SU(2).) Moreover, given U âˆˆSU(2), and A, B âˆˆV , note that
âŸ¨Ï†U(A), Ï†U(B)âŸ©= 1
2trace(UAU âˆ’1UBU âˆ’1) = 1
2trace(AB) = âŸ¨A, BâŸ©
Thus Ï†U is an orthogonal transformation of V âˆ¼= R3, which we can think of as an
element of O(3).
We see, then, that the map U â†’Ï†U is a map of SU(2) into O(3). It is very
easy to check that this map is a homomorphism (i.e., Ï†U1U2 = Ï†U1Ï†U2), and that
it is continuous. Thus U â†’Ï†U is a Lie group homomorphism of SU(2) into O(3).
Recall that every element of O(3) has determinant Â±1. Since SU(2) is connected
(Exercise 8), and the map U â†’Ï†U is continuous, Ï†U must actually map into SO(3).
Thus U â†’Ï†U is a Lie group homomorphism of SU(2) into SO(3).
The map U â†’Ï†U is not one-to-one, since for any U âˆˆSU(2), Ï†U = Ï†âˆ’U.
(Observe that if U is in SU(2), then so is âˆ’U.) It is possible to show that Ï†U is a
two-to-one map of SU(2) onto SO(3). (See Miller.)
7. Lie Groups
A Lie group is something which is simultaneously a group and a diï¬€erentiable
manifold (see Deï¬nition 2.14). As the terminology suggests, every matrix Lie group
is a Lie group, although this requires proof (Theorem 2.15).
I have decided to
restrict attention to matrix Lie groups, except in emergencies, for three reasons.
First, this makes the course accessible to students who are not familiar with the
theory of diï¬€erentiable manifolds.
Second, this makes the deï¬nition of the Lie
algebra and of the exponential mapping far more comprehensible. Third, all of the
important examples of Lie groups are (or can easily be represented as) matrix Lie
groups.

7. LIE GROUPS
21
Alas, there is a price to pay for this simpliï¬cation. Certain important topics
(notably, the universal cover) are considerably complicated by restricting to the
matrix case. Nevertheless, I feel that the advantages outweigh the disadvantages in
an introductory course such as this.
Definition 2.14. A Lie group is a diï¬€erentiable manifold G which is also a
group, and such that the group product
G Ã— G â†’G
and the inverse map g â†’gâˆ’1 are diï¬€erentiable.
For the reader who is not familiar with the notion of a diï¬€erentiable manifold,
here is a brief recap. (I will consider only manifolds embedded in some Rn, which is a
harmless assumption.) A subset M of Rn is called a k-dimensional diï¬€erentiable
manifold if given any m0 âˆˆM, there exists a smooth (non-linear) coordinate
system (x1, Â· Â· Â·xn) deï¬ned in a neighborhood U of m0 such that
M âˆ©U =

m âˆˆU
xk+1(m) = c1, Â· Â· Â· , xn(m) = cnâˆ’k
	
This says that locally, after a suitable change of variables, M looks like the k-
dimensional hyperplane in Rn obtained by setting all but the ï¬rst k coordinates
equal to constants.
For example, S1 âŠ‚R2 is a one-dimensional diï¬€erentiable manifold because in
the usual polar coordinates (Î¸, r), S1 is the set r = 1. Of course, polar coordinates
are not globally deï¬ned, because Î¸ is undeï¬ned at the origin, and because Î¸ is not
â€œsingle-valued.â€ But given any point m0 in S1, we can deï¬ne polar coordinates in
a neighborhood U of m0, and then S1 âˆ©U will be the set r = 1.
Note that while we assume that our diï¬€erentiable manifolds are embedded in
some Rn (a harmless assumption), we are not saying that a Lie group has to be
embedded in Rn2, or that the group operation has to have anything to do with
matrix multiplication. A Lie group is simply a subset G of some Rn which is a
diï¬€erentiable manifold, together with any map from G Ã— G into G which makes
G into a group (and such that the group operations are smooth). It is remarkable
that almost (but not quite!) every Lie group is isomorphic to a matrix Lie group.
Note also that it is far from obvious that a matrix Lie group must be a Lie
group, since our deï¬nition of a matrix Lie group G does not say anything about G
being a manifold. It is not too diï¬ƒcult to verify that all of our examples of matrix
Lie groups are Lie groups, but in fact we have the following result which makes
such veriï¬cations unnecessary:
Theorem 2.15. Every matrix Lie group is a Lie group.
Although I will not prove this result, I want to discuss what would be involved.
Let us consider ï¬rst the group GL(n; R). The space of all n Ã— n real matrices can
be thought of as Rn2. Since GL(n; R) is the set of all matrices A with det A Ì¸= 0,
GL(n; R) is an open subset of Rn2. (That is, given an invertible matrix A, there
is a neighborhood U of A such that every matrix B âˆˆU is also invertible.) Thus
GL(n; R) is an n2-dimensional smooth manifold. Furthermore, the matrix product
AB is clearly a smooth (even polynomial) function of the entries of A and B, and
(in light of Kramerâ€™s rule) Aâˆ’1 is a smooth function of the entries of A. Thus
GL(n; R) is a Lie group.

22
2. MATRIX LIE GROUPS
Similarly, if we think of the space of n Ã— n complex matrices as Cn2 âˆ¼= R2n2,
then the same argument shows that GL(n; C) is a Lie group.
Thus, to prove that every matrix Lie group is a Lie group, it suï¬ƒces to show
that a closed subgroup of a Lie group is a Lie group. This is proved in BrÂ¨ocker and
tom Dieck, Chapter I, Theorem 3.11. The proof is not too diï¬ƒcult, but it requires
the exponential mapping, which we have not yet introduced. (See Chapter 3.)
It is customary to call a map Ï† between two Lie groups a Lie group homomor-
phism if Ï† is a group homomorphism and Ï† is smooth, whereas we have (in Deï¬nition
2.13) required only that Ï† be continuous. However, the following Proposition shows
that our deï¬nition is equivalent to the more standard one.
Proposition 2.16. Let G and H be Lie groups, and Ï† a group homomorphism
from G to H. Then if Ï† is continuous it is also smooth.
Thus group homomorphisms from G to H come in only two varieties: the very
bad ones (discontinuous), and the very good ones (smooth). There simply arenâ€™t
any intermediate ones. (See, for example, Exercise 16.) For proof, see BrÂ¨ocker and
tom Dieck, Chapter I, Proposition 3.12.
In light of Theorem 2.15, every matrix Lie group is a (smooth) manifold. As
such, a matrix Lie group is automatically locally path connected. It follows that
a matrix Lie group is path connected if and only if it is connected. (See Remarks
following Deï¬nition 2.5.)
8. Exercises
1. Let a be an irrational real number. Show that the set of numbers of the
form e2Ï€ina, n âˆˆZ, is dense in S1. Now let G be the following subgroup of
GL(2; C):
G =
 eit
0
0
eiat

|t âˆˆR

Show that
G =

eit
0
0
eis

|t, s âˆˆR

,
where G denotes the closure of the set G inside the space of 2 Ã— 2 matrices.
Note: The group G can be thought of as the torus S1 Ã— S1, which in
turn can be thought of as [0, 2Ï€] Ã— [0, 2Ï€], with the ends of the intervals
identiï¬ed. The set G âŠ‚[0, 2Ï€] Ã— [0, 2Ï€] is called an irrational line. Draw
a picture of this set and you should see why G is dense in [0, 2Ï€] Ã— [0, 2Ï€].
2. Orthogonal groups. Let âŸ¨âŸ©denote the standard inner product on Rn, âŸ¨x, yâŸ©=
P
i xiyi. Show that a matrix A preserves inner products if and only if the
column vectors of A are orthonormal.
Show that for any n Ã— n real matrix B,
âŸ¨Bx, yâŸ©=

x, Btry

where (Btr)ij = Bji. Using this fact, show that a matrix A preserves inner
products if and only if AtrA = I.
Note: a similar analysis applies to the complex orthogonal groups O(n; C)
and SO(n; C).

8. EXERCISES
23
3. Unitary groups. Let âŸ¨âŸ©denote the standard inner product on Cn, âŸ¨x, yâŸ©=
P
i xiyi. Following Exercise 2, show that Aâˆ—A = I if and only if âŸ¨Ax, AyâŸ©=
âŸ¨x, yâŸ©for all x, y âˆˆCn. ((Aâˆ—)ij = Aji.)
4. Generalized orthogonal groups. Let [x, y]n,k be the symmetric bilinear form
on Rn+k deï¬ned in (2.1). Let g be the (n + k) Ã— (n + k) diagonal matrix
with ï¬rst n diagonal entries equal to one, and last k diagonal entries equal
to minus one:
g =
 In
0
0
âˆ’Ik

Show that for all x, y âˆˆRn+k,
[x, y]n,k = âŸ¨x, gyâŸ©
Show that a (n + k) Ã— (n + k) real matrix A is in O(n; k) if and only if
AtrgA = g. Show that O(n; k) and SO(n; k) are subgroups of GL(n + k; R),
and are matrix Lie groups.
5. Symplectic groups. Let B [x, y] be the skew-symmetric bilinear form on R2n
given by B [x, y] = Pn
i=1 xiyn+i âˆ’xn+iyi. Let J be the 2n Ã— 2n matrix
J =

0
I
âˆ’I
0

Show that for all x, y âˆˆR2n
B [x, y] = âŸ¨x, JyâŸ©
Show that a 2nÃ—2n matrix A is in Sp (n; R) if and only if AtrJA = J. Show
that Sp (n; R) is a subgroup of GL(2n; R), and a matrix Lie group.
Note: a similar analysis applies to Sp (n; C).
6. The groups O(2) and SO(2). Show that the matrix
A =
 cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

is in SO(2), and that
 cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
  cos Ï†
âˆ’sin Ï†
sin Ï†
cos Ï†

=
 cos(Î¸ + Ï†)
âˆ’sin(Î¸ + Ï†)
sin(Î¸ + Ï†)
cos(Î¸ + Ï†)

Show that every element A of O(2) is of one of the two forms
A =
 cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

A =
 cos Î¸
sin Î¸
sin Î¸
âˆ’cos Î¸

(If A is of the ï¬rst form, then det A = 1; if A is of the second form, then
det A = âˆ’1.)
Hint: Recall that for A =
 a
b
c
d

to be in O(2), the column vectors

a
c

and

b
d

must be unit vectors, and must be orthogonal.

24
2. MATRIX LIE GROUPS
7. The groups O(1; 1) and SO(1; 1). Show that
A =
 cosh t
sinh t
sinh t
cosh t

is in SO(1; 1), and that

cosh t
sinh t
sinh t
cosh t
 
cosh s
sinh s
sinh s
cosh s

=

cosh(t + s)
sinh(t + s)
sinh(t + s)
cosh(t + s)

Show that every element of O(1; 1) can be written in one of the four forms
 cosh t
sinh t
sinh t
cosh t

 âˆ’cosh t
sinh t
sinh t
âˆ’cosh t

 cosh t
âˆ’sinh t
sinh t
âˆ’cosh t


âˆ’cosh t
âˆ’sinh t
sinh t
cosh t

(Since cosh t is always positive, there is no overlap among the four cases.
Matrices of the ï¬rst two forms have determinant one; matrices of the last
two forms have determinant minus one.)
Hint: For
 a
b
c
d

to be in O(1; 1), we must have a2âˆ’c2 = 1, b2âˆ’d2 =
âˆ’1, and ab âˆ’cd = 0. The set of points (a, c) in the plane with a2 âˆ’c2 = 1
(i.e., a = Â±
âˆš
1 + c2 ) is a hyperbola.
8. The group SU(2). Show that if Î±, Î² are arbitrary complex numbers satisfying
|Î±|2 + |Î²|2 = 1, then the matrix
A =

Î±
âˆ’Î²
Î²
Î±

(2.9)
is in SU(2). Show that every A âˆˆSU(2) can be expressed in the form (2.9)
for a unique pair (Î±, Î²) satisfying |Î±|2 + |Î²|2 = 1.
(Thus SU(2) can be
thought of as the three-dimensional sphere S3 sitting inside C2 = R4. In
particular, this shows that SU(2) is connected and simply connected.)
9. The groups Sp (1; R), Sp (1; C), and Sp (1). Show that Sp (1; R) = SL (2; R),
Sp (1; C) = SL (2; C), and Sp(1) = SU(2).
10. The Heisenberg group. Determine the center Z(H) of the Heisenberg group
H. Show that the quotient group H/Z(H) is abelian.
11. Connectedness of SO(n). Show that SO(n) is connected, following the out-
line below.
For the case n = 1, there is not much to show, since a 1Ã—1 matrix with
determinant one must be [1]. Assume, then, that n â‰¥2. Let e1 denote the

8. EXERCISES
25
vector
e1 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£¸
in Rn. Given any unit vector v âˆˆRn, show that there exists a continuous
path R(t) in SO(n) with R(0) = I and R(1)v = e1. (Thus any unit vector
can be â€œcontinuously rotatedâ€ to e1.)
Now show that any element R of SO(n) can be connected to an element
of SO(n âˆ’1), and proceed by induction.
12. The polar decomposition of SL (n; R). Show that every element A of SL (n; R)
can be written uniquely in the form A = RH, where R is in SO(n), and H
is a symmetric, positive-deï¬nite matrix with determinant one.
(That is,
Htr = H, and âŸ¨x, HxâŸ©â‰¥0 for all x âˆˆRn).
Hint: If A could be written in this form, then we would have
AtrA = HtrRtrRH = HRâˆ’1RH = H2
Thus H would have to be the unique positive-deï¬nite symmetric square root
of AtrA.
Note:
A similar argument gives polar decompositions for GL(n; R),
SL (n; C), and GL(n; C).
For example, every element A of SL (n; C) can
be written uniquely as A = UH, with U in SU(n), and H a self-adjoint
positive-deï¬nite matrix with determinant one.
13. The connectedness of SL (n; R). Using the polar decomposition of SL (n; R)
(Ex. 12) and the connectedness of SO(n) (Ex. 11), show that SL (n; R) is
connected.
Hint: Recall that if H is a real, symmetric matrix, then there exists a
real orthogonal matrix R1 such that H = R1DRâˆ’1
1 , where D is diagonal.
14. The connectedness of GL(n; R)+. Show that GL(n; R)+ is connected.
15. Show that the set of translations is a normal subgroup of the Euclidean
group, and also of the PoincarÂ´e group. Show that (E(n)/translations) âˆ¼=
O(n).
16. Harder. Show that every Lie group homomorphism Ï† from R to S1 is of the
form Ï†(x) = eiax for some a âˆˆR. In particular, every such homomorphism
is smooth.

26
2. MATRIX LIE GROUPS

CHAPTER 3
Lie Algebras and the Exponential Mapping
1. The Matrix Exponential
The exponential of a matrix plays a crucial role in the theory of Lie groups.
The exponential enters into the deï¬nition of the Lie algebra of a matrix Lie group
(Section 5 below), and is the mechanism for passing information from the Lie alge-
bra to the Lie group. Since many computations are done much more easily at the
level of the Lie algebra, the exponential is indispensable.
Let X be an n Ã— n real or complex matrix. We wish to deï¬ne the exponential
of X, eX or exp X, by the usual power series
eX =
âˆ
X
m=0
Xm
m! .
(3.1)
We will follow the convention of using letters such as X and Y for the variable in
the matrix exponential.
Proposition 3.1. For any n Ã— n real or complex matrix X, the series (3.1)
converges. The matrix exponential eX is a continuous function of X.
Before proving this, let us review some elementary analysis. Recall that the
norm of a vector x in Cn is deï¬ned to be
âˆ¥xâˆ¥=
p
âŸ¨x, xâŸ©=
qX
|xi|2.
This norm satisï¬es the triangle inequality
âˆ¥x + yâˆ¥â‰¤âˆ¥xâˆ¥+ âˆ¥yâˆ¥.
The norm of a matrix A is deï¬ned to be
âˆ¥Aâˆ¥= sup
xÌ¸=0
âˆ¥Axâˆ¥
âˆ¥xâˆ¥.
Equivalently, âˆ¥Aâˆ¥is the smallest number Î» such that âˆ¥Axâˆ¥â‰¤Î» âˆ¥xâˆ¥for all x âˆˆCn.
It is not hard to see that for any n Ã— n matrix A, âˆ¥Aâˆ¥is ï¬nite. Furthermore,
it is easy to see that for any matrices A, B
âˆ¥ABâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥Bâˆ¥
(3.2)
âˆ¥A + Bâˆ¥â‰¤âˆ¥Aâˆ¥+ âˆ¥Bâˆ¥.
(3.3)
It is also easy to see that a sequence of matrices Am converges to a matrix A if and
only if âˆ¥Am âˆ’Aâˆ¥â†’0. (Compare this with Deï¬nition 2.1 of Chapter 2.)
A sequence of matrices Am is said to be a Cauchy sequence if âˆ¥Am âˆ’Alâˆ¥â†’0
as m, l â†’âˆ. Thinking of the space of matrices as Rn2 or Cn2, and using a standard
result from analysis, we have the following:
27

28
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
Proposition 3.2. If Am is a sequence of n Ã— n real or complex matrices, and
Am is a Cauchy sequence, then there exists a unique matrix A such that Am con-
verges to A.
That is, every Cauchy sequence converges.
Now, consider an inï¬nite series whose terms are matrices:
A0 + A1 + A2 + Â· Â· Â· .
(3.4)
If
âˆ
X
m=0
âˆ¥Amâˆ¥< âˆ
then the series (3.4) is said to converge absolutely. If a series converges abso-
lutely, then it is not hard to show that the partial sums of the series form a Cauchy
sequence, and hence by Proposition 3.2, the series converges. That is, any series
which converges absolutely also converges. (The converse is not true; a series of
matrices can converge without converging absolutely.)
Proof. In light of (3.2), we see that
âˆ¥Xmâˆ¥â‰¤âˆ¥Xâˆ¥m ,
and hence
âˆ
X
m=0

Xm
m!
 â‰¤
âˆ
X
m=0
âˆ¥Xâˆ¥m
m!
= eâˆ¥Xâˆ¥< âˆ.
Thus the series (3.1) converges absolutely, and so it converges.
To show continuity, note that since Xm is a continuous function of X, the
partial sums of (3.1) are continuous.
But it is easy to see that (3.1) converges
uniformly on each set of the form {âˆ¥Xâˆ¥â‰¤R}, and so the sum is again continuous.
Proposition 3.3. Let X, Y be arbitrary n Ã— n matrices. Then
1. e0 = I.
2. eX is invertible, and  eXâˆ’1 = eâˆ’X.
3. e(Î±+Î²)X = eÎ±XeÎ²X for all real or complex numbers Î±, Î².
4. If XY = Y X, then eX+Y = eXeY = eY eX.
5. If C is invertible, then eCXCâˆ’1 = CeXCâˆ’1.
6.
eX â‰¤eâˆ¥Xâˆ¥.
It is not true in general that eX+Y = eXeY , although by 4) it is true if X and
Y commute. This is a crucial point, which we will consider in detail later. (See
the Lie product formula in Section 4 and the Baker-Campbell-Hausdorï¬€formula in
Chapter 4.)
Proof. Point 1) is obvious. Points 2) and 3) are special cases of point 4). To
verify point 4), we simply multiply power series term by term. (It is left to the
reader to verify that this is legal.) Thus
eXeY =

I + X + X2
2! + Â· Â· Â·
 
I + Y + Y 2
2! + Â· Â· Â·

.

2. COMPUTING THE EXPONENTIAL OF A MATRIX
29
Multiplying this out and collecting terms where the power of X plus the power of
Y equals m, we get
eXeY =
âˆ
X
m=0
m
X
k=0
Xk
k!
Y mâˆ’k
(m âˆ’k)! =
âˆ
X
m=0
1
m!
m
X
k=0
m!
k!(m âˆ’k)!XkY mâˆ’k.
(3.5)
Now because (and only because) X and Y commute,
(X + Y )n =
m
X
k=0
m!
k!(m âˆ’k)!XkY mâˆ’k,
and so (3.5) becomes
eXeY =
âˆ
X
m=0
1
m!(X + Y )m = eX+Y .
To prove 5), simply note that
 CXCâˆ’1m = CXmCâˆ’1
and so the two sides of 5) are the same term by term.
Point 6) is evident from the proof of Proposition 3.1.
Proposition 3.4. Let X be a n Ã— n complex matrix, and view the space of all
n Ã— n complex matrices as Cn2. Then etX is a smooth curve in Cn2, and
d
dtetX = XetX = etXX.
In particular,
d
dt

t=0
etX = X.
Proof. Diï¬€erentiate the power series for etX term-by-term. (You might worry
whether this is valid, but you shouldnâ€™t. For each i, j,
 etX
ij is given by a con-
vergent power series in t, and it is a standard theorem that you can diï¬€erentiate
power series term-by-term.)
2. Computing the Exponential of a Matrix
2.1. Case 1: X is diagonalizable. Suppose that X is a nÃ—n real or complex
matrix, and that X is diagonalizable over C, that is, that there exists an invertible
complex matrix C such that X = CDCâˆ’1, with
D =
ï£«
ï£¬
ï£­
Î»1
0
...
0
Î»n
ï£¶
ï£·
ï£¸.
Observe that eD is the diagonal matrix with eigenvalues eÎ»1, Â· Â· Â· , eÎ»n, and so in
light of Proposition 3.3, we have
eX = C
ï£«
ï£¬
ï£­
eÎ»1
0
...
0
eÎ»n
ï£¶
ï£·
ï£¸Câˆ’1.

30
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
Thus if you can explicitly diagonalize X, you can explicitly compute eX. Note that
if X is real, then although C may be complex and the Î»iâ€™s may be complex, eX
must come out to be real, since each term in the series (3.1) is real.
For example, take
X =
 0
âˆ’a
a
0

.
Then the eigenvectors of X are
 1
i

and
 i
1

, with eigenvalues âˆ’ia and ia,
respectively. Thus the invertible matrix
C =

1
i
i
1

maps the basis vectors
 1
0

and
 0
1

to the eigenvectors of X, and so (check)
Câˆ’1XC is a diagonal matrix D. Thus X = CDCâˆ’1:
eX =
 1
i
i
1
  eâˆ’ia
0
0
eia
 
1/2
âˆ’i/2
âˆ’i/2
1/2

=
 cos a
âˆ’sin a
sin a
cos a

.
Note that explicitly if X (and hence a) is real, then eX is real.
2.2. Case 2: X is nilpotent. An n Ã— n matrix X is said to be nilpotent
if Xm = 0 for some positive integer m. Of course, if Xm = 0, then Xl = 0 for all
l > m. In this case the series (3.1) which deï¬nes eX terminates after the ï¬rst m
terms, and so can be computed explicitly.
For example, compute etX, where
X =
ï£«
ï£­
0
a
b
0
0
c
0
0
0
ï£¶
ï£¸.
Note that
X2 =
ï£«
ï£­
0
0
ac
0
0
0
0
0
0
ï£¶
ï£¸
and that X3 = 0. Thus
etX =
ï£«
ï£¬
ï£­
1
ta
tb + 1
2t2ac
0
1
tc
0
0
1
ï£¶
ï£·
ï£¸.
2.3. Case 3: X arbitrary. A general matrix X may be neither nilpotent nor
diagonalizable. However, it follows from the Jordan canonical form that X can be
written (Exercise 2) in the form X = S + N with S diagonalizable, N nilpotent,
and SN = NS. (See Exercise 2.) Then, since N and S commute,
eX = eS+N = eSeN
and eS and eN can be computed as above.

3. THE MATRIX LOGARITHM
31
For example, take
X =
 a
b
0
a

.
Then
X =
 a
0
0
a

+
 0
b
0
0

.
The two terms clearly commute (since the ï¬rst one is a multiple of the identity),
and so
eX =

ea
0
0
ea
 
1
b
0
1

=

ea
eab
0
ea

.
3. The Matrix Logarithm
We wish to deï¬ne a matrix logarithm, which should be an inverse function to
the matrix exponential. Deï¬ning a logarithm for matrices should be at least as
diï¬ƒcult as deï¬ning a logarithm for complex numbers, and so we cannot hope to
deï¬ne the matrix logarithm for all matrices, or even for all invertible matrices. We
will content ourselves with deï¬ning the logarithm in a neighborhood of the identity
matrix.
The simplest way to deï¬ne the matrix logarithm is by a power series. We recall
the situation for complex numbers:
Lemma 3.5. The function
log z =
âˆ
X
m=1
(âˆ’1)m+1 (z âˆ’1)m
m
is deï¬ned and analytic in a circle of radius one about z = 1.
For all z with |z âˆ’1| < 1,
elog z = z.
For all u with |u| < log2, |eu âˆ’1| < 1 and
log eu = u.
Proof. The usual logarithm for real, positive numbers satisï¬es
d
dx log(1 âˆ’x) =
âˆ’1
1 âˆ’x = âˆ’ 1 + x + x2 + Â· Â· Â·
for |x| < 1. Integrating term-by-term and noting that log1 = 0 gives
log(1 âˆ’x) = âˆ’

x + x2
2 + x3
3 + Â· Â· Â·

.
Taking z = 1 âˆ’x (so that x = 1 âˆ’z), we have
logz = âˆ’

(1 âˆ’z) + (1âˆ’z)2
2
+ (1âˆ’z)3
3
+ Â· Â· Â·

=
âˆ
X
m=1
(âˆ’1)m+1 (z âˆ’1)m
m
.
This series has radius of convergence one, and deï¬nes a complex analytic func-
tion on the set {|z âˆ’1| < 1}, which coincides with the usual logarithm for real z
in the interval (0, 2). Now, exp(log z) = z for z âˆˆ(0, 2), and by analyticity this
identity continues to hold on the whole set {|z âˆ’1| < 1}.

32
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
On the other hand, if |u| < log 2, then
|eu âˆ’1| =
u + u2
2! + Â· Â· Â·
 â‰¤|u| + |u|2
2! + Â· Â· Â·
so that
|eu âˆ’1| â‰¤e|u| âˆ’1 < 1.
Thus log(exp u) makes sense for all such u. Since log(exp u) = u for real u with
|u| < log2, it follows by analyticity that log(exp u) = u for all complex numbers
with |u| < log2.
Theorem 3.6. The function
logA =
âˆ
X
m=1
(âˆ’1)m+1 (A âˆ’I)m
m
(3.6)
is deï¬ned and continuous on the set of all nÃ—n complex matrices A with âˆ¥A âˆ’Iâˆ¥<
1, and logA is real if A is real.
For all A with âˆ¥A âˆ’Iâˆ¥< 1,
elog A = A.
For all X with âˆ¥Xâˆ¥< log2,
eX âˆ’1
 < 1 and
log eX = X.
Proof. It is easy to see that the series (3.6) converges absolutely whenever
âˆ¥A âˆ’Iâˆ¥< 1. The proof of continuity is essentially the same as for the exponential.
If A is real, then every term in the series (3.6) is real, and so log A is real.
We will now show that exp(log A) = A for all A with âˆ¥A âˆ’Iâˆ¥< 1. We do this
by considering two cases.
Case 1: A is diagonalizable.
Suppose that A = CDCâˆ’1, with D diagonal. Then A âˆ’I = CDCâˆ’1 âˆ’I =
C(D âˆ’I)Câˆ’1. It follows that (A âˆ’I)m is of the form
(A âˆ’I)m = C
ï£«
ï£¬
ï£­
(z1 âˆ’1)m
0
...
0
(zn âˆ’1)m
ï£¶
ï£·
ï£¸Câˆ’1,
where z1, Â· Â· Â· , zn are the eigenvalues of A.
Now, if âˆ¥A âˆ’Iâˆ¥< 1, then certainly |zi âˆ’1| < 1 for i = 1, Â· Â· Â· , n. (Think about
it.) Thus
âˆ
X
m=1
(âˆ’1)m+1 (A âˆ’I)m
m
= C
ï£«
ï£¬
ï£­
log z1
0
...
0
logzn
ï£¶
ï£·
ï£¸Câˆ’1
and so by the Lemma
elog A = C
ï£«
ï£¬
ï£­
elog z1
0
...
0
elog zn
ï£¶
ï£·
ï£¸Câˆ’1 = A.
Case 2: A is not diagonalizable.

3. THE MATRIX LOGARITHM
33
If A is not diagonalizable, then, using the Jordan canonical form, it is not
diï¬ƒcult to construct a sequence Am of diagonalizable matrices with Am â†’A. (See
Exercise 4.) If âˆ¥A âˆ’Iâˆ¥< 1, then âˆ¥Am âˆ’Iâˆ¥< 1 for all suï¬ƒciently large m. By Case
1, exp(log Am) = Am, and so by the continuity of exp and log, exp(logA) = A.
Thus we have shown that exp(log A) = A for all A with âˆ¥A âˆ’Iâˆ¥< 1. Now, the
same argument as in the complex case shows that if âˆ¥Xâˆ¥< log2, then
eX âˆ’I
 <
1. But then the same two-case argument as above shows that log(exp X) = X for
all such X.
Proposition 3.7. There exists a constant c such that for all nÃ—n matrices B
with âˆ¥Bâˆ¥< 1
2
âˆ¥log(I + B) âˆ’Bâˆ¥â‰¤c âˆ¥Bâˆ¥2 .
Proof. Note that
log(I + B) âˆ’B =
âˆ
X
m=2
(âˆ’1)m Bm
m = B2
âˆ
X
m=2
(âˆ’1)m Bmâˆ’2
m
so that
âˆ¥log(I + B) âˆ’Bâˆ¥â‰¤âˆ¥Bâˆ¥2
âˆ
X
m=2
  1
2
m
m
.
This is what we want.
Proposition 3.8. Let X be any nÃ—n complex matrix, and let Cm be a sequence
of matrices such that âˆ¥Cmâˆ¥â‰¤const.
m2 . Then
lim
mâ†’âˆ

I + X
m + Cm
m
= eX.
Proof. The expression inside the brackets is clearly tending to I as m â†’âˆ,
and so is in the domain of the logarithm for all suï¬ƒciently large m. Now
log

I + X
m + Cm

= X
m + Cm + Em
where Em is an error term which, by Proposition 3.7 satisï¬es âˆ¥Emâˆ¥â‰¤c
 X
m + Cm
2 â‰¤
const.
m2 . But then
I + X
m + Cm = exp
X
m + Cm + Em

,
and so

I + X
m + Cm
m
= exp (X + mCm + mEm) .
Since both Cm and Em are of order
1
m2 , we obtain the desired result by letting
m â†’âˆand using the continuity of the exponential.

34
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
4. Further Properties of the Matrix Exponential
In this section we give three additional results involving the exponential of a
matrix, which will be important in our study of Lie algebras.
Theorem 3.9 (Lie Product Formula). Let X and Y be nÃ—n complex matrices.
Then
eX+Y = lim
mâ†’âˆ

e
X
m e
Y
m
m
.
This theorem has a big brother, called the Trotter product formula, which gives
the same result in the case where X and Y are suitable unbounded operators on an
inï¬nite-dimensional Hilbert space. The Trotter formula is described, for example,
in M. Reed and B. Simon, Methods of Modern Mathematical Physics, Vol. I, VIII.8.
Proof. Using the power series for the exponential and multiplying, we get
e
X
m e
Y
m = I + X
m + Y
m + Cm,
where (check!) âˆ¥Cmâˆ¥â‰¤
const.
m2 . Since e
X
m e
Y
m â†’I as m â†’âˆ, e
X
m e
Y
m is in the
domain of the logarithm for all suï¬ƒciently large m. But
log

e
X
m e
Y
m

= log

I + X
m + Y
m + Cm

= X
m + Y
m + Cm + Em
where by Proposition 3.7 âˆ¥Cmâˆ¥â‰¤const.
 X
m + Y
m + Cm
2 â‰¤const.
m2 . Exponentiating
the logarithm gives
e
X
m e
Y
m = exp
X
m + Y
m + Cm + Em

and

e
X
m e
Y
m
m
= exp (X + Y + mCm + mEm) .
Since both Cm and Em are of order
1
m2 , we have (using the continuity of the
exponential)
lim
mâ†’âˆ

e
X
m e
Y
m
m
= exp (X + Y )
which is the Lie product formula.
Theorem 3.10. Let X be an n Ã— n real or complex matrix. Then
det
 eX
= etrace(X).
Proof. There are three cases, as in Section 2.
Case 1: A is diagonalizable. Suppose there is a complex invertible matrix C
such that
X = C
ï£«
ï£¬
ï£­
Î»1
0
...
0
Î»n
ï£¶
ï£·
ï£¸Câˆ’1.

4. FURTHER PROPERTIES OF THE MATRIX EXPONENTIAL
35
Then
eX = C
ï£«
ï£¬
ï£­
eÎ»1
0
...
0
eÎ»n
ï£¶
ï£·
ï£¸Câˆ’1.
Thus trace(X) = PÎ»i, and det(eX) = Q eÎ»i = e
P Î»i. (Recall that trace(CDCâˆ’1) =
trace(D).)
Case 2: X is nilpotent. If X is nilpotent, then it cannot have any non-zero
eigenvalues (check!), and so all the roots of the characteristic polynomial must be
zero. Thus the Jordan canonical form of X will be strictly upper triangular. That
is, X can be written as
X = C
ï£«
ï£¬
ï£­
0
âˆ—
...
0
0
ï£¶
ï£·
ï£¸Câˆ’1.
In that case (it is easy to see) eX will be upper triangular, with ones on the diagonal:
eX = C
ï£«
ï£¬
ï£­
1
âˆ—
...
0
1
ï£¶
ï£·
ï£¸Câˆ’1.
Thus if X is nilpotent, trace(X) = 0, and det(eX) = 1.
Case 3: X arbitrary. As pointed out in Section 2, every matrix X can be
written as the sum of two commuting matrices S and N, with S diagonalizable
(over C) and N nilpotent. Since S and N commute, eX = eSeN. So by the two
previous cases
det
 eX
= det
 eS
det
 eN
= etrace(S)etrace(N) = etrace(X),
which is what we want.
Definition 3.11. A function A : R â†’GL(n; C) is called a one-parameter
group if
1. A is continuous,
2. A(0) = I,
3. A(t + s) = A(t)A(s) for all t, s âˆˆR.
Theorem 3.12 (One-parameter Subgroups). If A is a one-parameter group in
GL(n; C), then there exists a unique n Ã— n complex matrix X such that
A(t) = etX.
By taking n = 1, and noting that GL(1; C) âˆ¼= Câˆ—, this Theorem provides an
alternative method of solving Exercise 16 in Chapter 2.
Proof. The uniqueness is immediate, since if there is such an X, then X =
d
dt

t=0 A(t). So we need only worry about existence.
The ï¬rst step is to show that A(t) must be smooth. This follows from Proposi-
tion 2.16 in Chapter 2 (which we did not prove), but we give a self-contained proof.

36
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
Let f(s) be a smooth real-valued function supported in a small neighborhood of
zero, with f(s) â‰¥0 and R f(s)ds = 1. Now look at
B(t) =
Z
A(t + s)f(s) ds.
(3.7)
Making the change-of-variable u = t + s gives
B(t) =
Z
A(u)f(u âˆ’t) du.
It follows that B(t) is diï¬€erentiable, since derivatives in the t variable go onto f,
which is smooth.
On the other hand, if we use the identity A(t + s) = A(t)A(s) in (3.7), we have
B(t) = A(t)
Z
A(s)f(s) ds.
Now, the conditions on the function f, together with the continuity of A, guarantee
that R A(s)f(s) ds is close to A(0) = I, and hence is invertible. Thus we may write
A(t) = B(t)
Z
A(s)f(s)ds
âˆ’1
.
(3.8)
Since B (t) is smooth and R A(s)f(s)ds is just a constant matrix, this shows that
A (t) is smooth.
Now that A(t) is known to be diï¬€erentiable, we may deï¬ne
X =
d
dt

t=0 A(t).
Our goal is to show that A(t) = etX. Since A(t) is smooth, a standard calculus
result (extended trivially to handle matrix-valued functions) says
âˆ¥A(t) âˆ’(I + tX)âˆ¥â‰¤const.t2.
It follows that for each ï¬xed t,
A   t
m
 = I + t
mX + O   1
m2
 .
Then, since A is a one-parameter group
A(t) = A   t
m
m = I + t
mX + O   1
m2
m .
Letting m â†’âˆand using Proposition 3.8 from Section 3 shows that A(t) =
etX.
5. The Lie Algebra of a Matrix Lie Group
The Lie algebra is an indispensable tool in studying matrix Lie groups. On the
one hand, Lie algebras are simpler than matrix Lie groups, because (as we will see)
the Lie algebra is a linear space. Thus we can understand much about Lie algebras
just by doing linear algebra. On the other hand, the Lie algebra of a matrix Lie
group contains much information about that group. (See for example, Proposition
3.23 in Section 7, and the Baker-Campbell-Hausdorï¬€Formula (Chapter 4).) Thus
many questions about matrix Lie groups can be answered by considering a similar
but easier problem for the Lie algebra.
Definition 3.13. Let G be a matrix Lie group. Then the Lie algebra of G,
denoted g, is the set of all matrices X such that etX is in G for all real numbers t.

5. THE LIE ALGEBRA OF A MATRIX LIE GROUP
37
Note that even if G is a subgroup of GL(n; C) we do not require that etX be in
G for all complex t, but only for all real t. Also, it is deï¬nitely not enough to have
just eX in G. That is, it is easy to give an example of an X and a G such that
eX âˆˆG but etX /âˆˆG for some values of t. Such an X is not in the Lie algebra of G.
It is customary to use lower case Gothic (Fraktur) characters such as g and h
to refer to Lie algebras.
5.1. Physicistsâ€™ Convention. Physicists are accustomed to considering the
map X â†’eiX instead of X â†’eX.
Thus a physicist would think of the Lie
algebra of G as the set of all matrices X such that eitX âˆˆG for all real t. In
the physics literature, the Lie algebra is frequently referred to as the space of
â€œinï¬nitesimal group elements.â€ See BrÂ¨ocker and tom Dieck, Chapter I, 2.21. The
physics literature does not always distinguish clearly between a matrix Lie group
and its Lie algebra.
Before examining general properties of the Lie algebra, let us compute the Lie
algebras of the matrix Lie groups introduced in the previous chapter.
5.2. The general linear groups. If X is any nÃ—n complex matrix, then by
Proposition 3.3, etX is invertible. Thus the Lie algebra of GL(n; C) is the space of
all n Ã— n complex matrices. This Lie algebra is denoted gl(n; C).
If X is any nÃ—n real matrix, then etX will be invertible and real. On the other
hand, if etX is real for all real t, then X =
d
dt

t=0 etX will also be real. Thus the
Lie algebra of GL(n; R) is the space of all n Ã— n real matrices, denoted gl(n; R).
Note that the preceding argument shows that if G is a subgroup of GL(n; R),
then the Lie algebra of G must consist entirely of real matrices. We will use this
fact when appropriate in what follows.
5.3. The special linear groups. Recall Theorem 3.10: det
 eX
= etraceX.
Thus if traceX = 0, then det
 etX
= 1 for all real t. On the other hand, if X is
any n Ã— n matrix such that det  etX = 1 for all t, then e(t)(traceX) = 1 for all t.
This means that (t)(traceX) is an integer multiple of 2Ï€i for all t, which is only
possible if traceX = 0. Thus the Lie algebra of SL (n; C) is the space of all n Ã— n
complex matrices with trace zero, denoted sl(n; C).
Similarly, the Lie algebra of SL (n; R) is the space of all nÃ—n real matrices with
trace zero, denoted sl (n; R).
5.4. The unitary groups. Recall that a matrix U is unitary if and only if
U âˆ—= U âˆ’1. Thus etX is unitary if and only if
 etXâˆ—=  etXâˆ’1 = eâˆ’tX.
(3.9)
But by taking adjoints term-by-term, we see that
 etXâˆ—= etXâˆ—, and so (3.9)
becomes
etXâˆ—= eâˆ’tX.
(3.10)
Clearly, a suï¬ƒcient condition for (3.10) to hold is that Xâˆ—= âˆ’X. On the other
hand, if (3.10) holds for all t, then by diï¬€erentiating at t = 0, we see that Xâˆ—= âˆ’X
is necessary.
Thus the Lie algebra of U(n) is the space of all n Ã— n complex matrices X such
that Xâˆ—= âˆ’X, denoted u(n).

38
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
By combining the two previous computations, we see that the Lie algebra of
SU(n) is the space of all n Ã— n complex matrices X such that Xâˆ—= âˆ’X and
traceX = 0, denoted su(n).
5.5. The orthogonal groups. The identity component of O(n) is just SO(n).
Since (Proposition 3.14) the exponential of a matrix in the Lie algebra is automat-
ically in the identity component, the Lie algebra of O(n) is the same as the Lie
algebra of SO(n).
Now, an n Ã— n real matrix R is orthogonal if and only if Rtr = Râˆ’1. So, given
an n Ã— n real matrix X, etX is orthogonal if and only if (etX)tr = (etX)âˆ’1, or
etXtr = eâˆ’tX.
(3.11)
Clearly, a suï¬ƒcient condition for this to hold is that Xtr = âˆ’X. If (3.11) holds for
all t, then by diï¬€erentiating at t = 0, we must have Xtr = âˆ’X.
Thus the Lie algebra of O(n), as well as the Lie algebra of SO(n), is the space of
all n Ã— n real matrices X with Xtr = âˆ’X, denoted so(n). Note that the condition
Xtr = âˆ’X forces the diagonal entries of X to be zero, and so explicitly the trace
of X is zero.
The same argument shows that the Lie algebra of SO(n; C) is the space of nÃ—n
complex matrices satisfying Xtr = âˆ’X, denoted so(n; C). This is not the same as
su(n).
5.6. The generalized orthogonal groups. A matrix A is in O(n; k) if and
only if AtrgA = g, where g is the (n + k) Ã— (n + k) diagonal matrix with the ï¬rst
n diagonal entries equal to one, and the last k diagonal entries equal to minus one.
This condition is equivalent to the condition gâˆ’1Atrg = Aâˆ’1, or, since explicitly
gâˆ’1 = g, gAtrg = Aâˆ’1. Now, if X is an (n + k) Ã— (n + k) real matrix, then etX is
in O(n; k) if and only if
getXtrg = etgXtrg = eâˆ’tX.
This condition holds for all real t if and only if gXtrg = âˆ’X.
Thus the Lie
algebra of O(n; k), which is the same as the Lie algebra of SO(n; k), consists of all
(n + k) Ã— (n + k) real matrices X with gXtrg = âˆ’X. This Lie algebra is denoted
so(n; k).
(In general, the group SO(n; k) will not be connected, in contrast to the group
SO(n). The identity component of SO(n; k), which is also the identity component
of O(n; k), is denoted SO(n; k)I. The Lie algebra of SO(n; k)I is the same as the
Lie algebra of SO(n; k).)
5.7. The symplectic groups. These are denoted sp (n; R), sp(n; C) , and
sp (n) . The calculation of these Lie algebras is similar to that of the generalized
orthogonal groups, and I will just record the result here. Let J be the matrix in
the deï¬nition of the symplectic groups. Then sp (n; R) is the space of 2n Ã— 2n real
matrices X such that JXtrJ = X, sp(n; C) is the space of 2nÃ—2n complex matrices
satisfying the same condition, and sp (n) =sp(n; C) âˆ©u (2n).

5. THE LIE ALGEBRA OF A MATRIX LIE GROUP
39
5.8. The Heisenberg group. Recall the Heisenberg group H is the group of
all 3 Ã— 3 real matrices A of the form
A =
ï£«
ï£­
1
a
b
0
1
c
0
0
1
ï£¶
ï£¸
(3.12)
Recall also that in Section 2, Case 2, we computed the exponential of a matrix of
the form
X =
ï£«
ï£­
0
Î±
Î²
0
0
Î³
0
0
0
ï£¶
ï£¸
(3.13)
and saw that eX was in H. On the other hand, if X is any matrix such that etX is
of the form (3.12), then all of the entries of X =
d
dt

t=0 etX which are on or below
the diagonal must be zero, so that X is of form (3.13).
Thus the Lie algebra of the Heisenberg group is the space of all 3 Ã— 3 real
matrices which are strictly upper triangular.
5.9. The Euclidean and PoincarÂ´e groups. Recall that the Euclidean group
E(n) is (or can be thought of as) the group of (n + 1) Ã— (n + 1) real matrices of the
form
ï£«
ï£¬
ï£¬
ï£¬
ï£­
x1
R
...
xn
0
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸
with R âˆˆO(n). Now if X is an (n + 1) Ã— (n + 1) real matrix such that etX is in
E(n) for all t, then X =
d
dt

t=0 etX must be zero along the bottom row:
X =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
y1
Y
...
yn
0
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£·
ï£¸
(3.14)
Our goal, then, is to determine which matrices of the form (3.14) are actually
in the Lie algebra of the Euclidean group. A simple computation shows that for
n â‰¥1
ï£«
ï£¬
ï£¬
ï£¬
ï£­
y1
Y
...
yn
0
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£·
ï£¸
n
=
ï£«
ï£¬
ï£¬
ï£­
Y n
Y nâˆ’1y
0
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£¸,
where y is the column vector with entries y1, Â· Â· Â· , yn. It follows that if X is as in
(3.14), then etX is of the form
etX =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
etY
...
âˆ—
0
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸.

40
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
Now, we have already established that etY is in O(n) for all t if and only if
Y tr = âˆ’Y . Thus we see that the Lie algebra of E(n) is the space of all (n+1)Ã—(n+1)
real matrices of the form (3.14) with Y satisfying Y tr = âˆ’Y .
A similar argument shows that the Lie algebra of P(n; 1) is the space of all
(n + 2) Ã— (n + 2) real matrices of the form
ï£«
ï£¬
ï£¬
ï£¬
ï£­
y1
Y
...
yn+1
0
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£·
ï£¸
with Y âˆˆso(n; 1).
6. Properties of the Lie Algebra
We will now establish various basic properties of the Lie algebra of a matrix
Lie group. The reader is invited to verify by direct calculation that these general
properties hold for the examples computed in the previous section.
Proposition 3.14. Let G be a matrix Lie group, and X an element of its Lie
algebra. Then eX is an element of the identity component of G.
Proof. By deï¬nition of the Lie algebra, etX lies in G for all real t. But as t
varies from 0 to 1, etX is a continuous path connecting the identity to eX.
Proposition 3.15. Let G be a matrix Lie group, with Lie algebra g. Let X be
an element of g, and A an element of G. Then AXAâˆ’1 is in g.
Proof. This is immediate, since by Proposition 3.3,
et(AXAâˆ’1) = AetXAâˆ’1,
and AetXAâˆ’1 âˆˆG.
Theorem 3.16. Let G be a matrix Lie group, g its Lie algebra, and X, Y ele-
ments of g. Then
1. sX âˆˆg for all real numbers s,
2. X + Y âˆˆg,
3. XY âˆ’Y X âˆˆg.
If you are following the physics convention for the deï¬nition of the Lie algebra,
then condition 3 should be replaced with the condition âˆ’i (XY âˆ’Y X) âˆˆg.
Proof. Point 1 is immediate, since et(sX) = e(ts)X, which must be in G if X is
in g. Point 2 is easy to verify if X and Y commute, since then et(X+Y ) = etXetY . If
X and Y do not commute, this argument does not work. However, the Lie product
formula says that
et(X+Y ) = lim
mâ†’âˆ

etX/metY /mm
.
Because X and Y are in the Lie algebra, etX/m and etY /m are in G, as is
 etX/metY /mm,
since G is a group. But now because G is a matrix Lie group, the limit of things
in G must be again in G, provided that the limit is invertible. Since et(X+Y ) is
automatically invertible, we conclude that it must be in G. This shows that X + Y
is in g.

6. PROPERTIES OF THE LIE ALGEBRA
41
Now for point 3. Recall (Proposition 3.4) that
d
dt

t=0 etX = X. It follows that
d
dt

t=0 etXY = XY , and hence by the product rule (Exercise 1)
d
dt

t=0
 etXY eâˆ’tX = (XY )e0 + (e0Y )(âˆ’X)
= XY âˆ’Y X.
But now, by Proposition 3.15, etXY eâˆ’tX is in g for all t. Since we have (by points
1 and 2) established that g is a real vector space, it follows that the derivative of
any smooth curve lying in g must be again in g. Thus XY âˆ’Y X is in g.
Definition 3.17. Given two n Ã— n matrices A and B, the bracket (or com-
mutator) of A and B is deï¬ned to be simply
[A, B] = AB âˆ’BA.
According to Theorem 3.16, the Lie algebra of any matrix Lie group is closed
under brackets.
The following very important theorem tells us that a Lie group homomorphism
between two Lie groups gives rise in a natural way to a map between the corre-
sponding Lie algebras. In particular, this will tell us that two isomorphic Lie groups
have â€œthe sameâ€ Lie algebras. (That is, the Lie algebras are isomorphic in the sense
of Section 8.) See Exercise 6.
Theorem 3.18. Let G and H be matrix Lie groups, with Lie algebras g and h,
respectively. Suppose that Ï† : G â†’H be a Lie group homomorphism. Then there
exists a unique real linear map eÏ† : g â†’h such that
Ï†(eX) = e
eÏ†(X)
for all X âˆˆg. The map eÏ† has following additional properties
1. eÏ†
 AXAâˆ’1
= Ï†(A)eÏ†(X)Ï†(A)âˆ’1, for all X âˆˆg, A âˆˆG.
2. eÏ†([X, Y ]) =
h
eÏ†(X), eÏ†(Y )
i
, for all X, Y âˆˆg.
3. eÏ†(X) =
d
dt

t=0 Ï†(etX), for all X âˆˆg.
If G, H, and K are matrix Lie groups and Ï† : H â†’K and Ïˆ : G â†’H are Lie
group homomorphisms, then
]
Ï† â—¦Ïˆ = eÏ† â—¦eÏˆ.
In practice, given a Lie group homomorphism Ï†, the way one goes about com-
puting eÏ† is by using Property 3. Of course, since eÏ† is (real) linear, it suï¬ƒces to
compute eÏ† on a basis for g. In the language of diï¬€erentiable manifolds, Property
3 says that eÏ† is the derivative (or diï¬€erential) of Ï† at the identity, which is the
standard deï¬nition of eÏ†. (See also Exercise 19.)
A linear map with property (2) is called a Lie algebra homomorphism. (See
Section 8.) This theorem says that every Lie group homomorphism gives rise to a
Lie algebra homomorphism. We will see eventually that the converse is true under
certain circumstances.
Speciï¬cally, suppose that G and H are Lie groups, and
eÏ† : g â†’h is a Lie algebra homomorphism. If G is connected and simply connected,
then there exists a unique Lie group homomorphism Ï† : G â†’H such that Ï† and eÏ†
are related as in Theorem 3.18.

42
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
Proof. The proof is similar to the proof of Theorem 3.16. Since Ï† is a con-
tinuous group homomorphism, Ï†(etX) will be a one-parameter subgroup of H, for
each X âˆˆg. Thus by Theorem 3.12, there is a unique Z such that
Ï†  etX = etZ
(3.15)
for all t âˆˆR. This Z must lie in h since etZ = Ï†
 etX
âˆˆH.
We now deï¬ne eÏ†(X) = Z, and check in several steps that eÏ† has the required
properties.
Step 1: Ï†(eX) = eeÏ†(X).
This follows from (3.15) and our deï¬nition of eÏ†, by putting t = 1.
Step 2: eÏ†(sX) = seÏ†(X) for all s âˆˆR.
This is immediate, since if Ï†(etX) = etZ, then Ï†(etsX) = etsZ.
Step 3: eÏ†(X + Y ) = eÏ†(X) + eÏ†(Y ).
By Steps 1 and 2,
eteÏ†(X+Y ) = e
eÏ†[t(X+Y )] = Ï†

et(X+Y )
.
By the Lie product formula, and the fact that Ï† is a continuous homomorphism:
= Ï†

lim
mâ†’âˆ

etX/metY /mm
= lim
mâ†’âˆ

Ï†

etX/m
Ï†(etY /m)
m
.
But then we have
eteÏ†(X+Y ) = lim
mâ†’âˆ

eteÏ†(X)/meteÏ†(Y )/mm
= et(eÏ†(X)+eÏ†(Y )).
Diï¬€erentiating this result at t = 0 gives the desired result.
Step 4: eÏ†
 AXAâˆ’1
= Ï†(A)eÏ†(X)Ï†(A)âˆ’1.
By Steps 1 and 2,
exp teÏ†(AXAâˆ’1) = exp eÏ†(tAXAâˆ’1) = Ï†
 exp tAXAâˆ’1
.
Using a property of the exponential and Step 1, this becomes
exp teÏ†(AXAâˆ’1) = Ï†  AetXAâˆ’1 = Ï†(A)Ï†(etX)Ï†(A)âˆ’1
= Ï†(A)eteÏ†(X)Ï†(A)âˆ’1.
Diï¬€erentiating this at t = 0 gives the desired result.
Step 5: eÏ†([X, Y ]) =
h
eÏ†(X), eÏ†(Y )
i
.
Recall from the proof of Theorem 3.16 that
[X, Y ] =
d
dt

t=0 etXY eâˆ’tX.
Hence
eÏ†([X, Y ]) = eÏ†
  d
dt

t=0 etXY eâˆ’tX
=
d
dt

t=0 eÏ†
 etXY eâˆ’tX
where we have used the fact that a derivative commutes with a linear transforma-
tion.

6. PROPERTIES OF THE LIE ALGEBRA
43
But then by Step 4,
eÏ†([X, Y ]) =
d
dt

t=0 Ï†(etX)eÏ†(Y )Ï†(eâˆ’tX)
=
d
dt

t=0 eteÏ†(X) eÏ†(Y )eâˆ’teÏ†(X)
=
h
eÏ†(X), eÏ†(Y )
i
.
Step 6: eÏ†(X) =
d
dt

t=0 Ï†(etX).
This follows from (3.15) and our deï¬nition of eÏ†.
Step 7: eÏ† is the unique real-linear map such that Ï†(eX) = eeÏ†(X).
Suppose that Ïˆ is another such map. Then
etÏˆ(X) = eÏˆ(tX) = Ï†(etX)
so that
Ïˆ(X) =
d
dt

t=0 Ï†(etX).
Thus by Step 6, Ïˆ coincides with eÏ†.
Step 8: ]
Ï† â—¦Ïˆ = eÏ† â—¦eÏˆ.
For any X âˆˆg,
Ï† â—¦Ïˆ
 etX
= Ï†
 Ïˆ
 etX
= Ï†

et e
Ïˆ(X)
= eteÏ†( e
Ïˆ(X)).
Thus ]
Ï† â—¦Ïˆ(X) = eÏ† â—¦eÏˆ(X).
Definition 3.19 (The Adjoint Mapping). Let G be a matrix Lie group, with
Lie algebra g. Then for each A âˆˆG, deï¬ne a linear map AdA : g â†’g by the
formula
AdA(X) = AXAâˆ’1.
We will let Ad denote the map A â†’AdA.
Proposition 3.20. Let G be a matrix Lie group, with Lie algebra g. Then for
each A âˆˆG, AdA is an invertible linear transformation of g with inverse AdAâˆ’1,
and Ad : G â†’GL(g) is a group homomorphism.
Proof. Easy. Note that Proposition 3.15 guarantees that AdA(X) is actually
in g for all X âˆˆg.
Since g is a real vector space with some dimension k, GL(g) is essentially the
same as GL(k; R). Thus we will regard GL(g) as a matrix Lie group. It is easy to
show that Ad : G â†’GL(g) is continuous, and so is a Lie group homomorphism. By
Theorem 3.18, there is an associated real linear map f
Ad from the Lie algebra of G
to the Lie algebra of GL(g), i.e., from g to gl(g), with the property that
e
f
AdX = Ad
 eX
.

44
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
Proposition 3.21. Let G be a matrix Lie group, let g its Lie algebra, and let
Ad : G â†’GL(g) be the Lie group homomorphism deï¬ned above. Let f
Ad : g â†’gl(g)
be the associated Lie algebra map. Then for all X, Y âˆˆg
f
AdX(Y ) = [X, Y ].
Proof. Recall that by Theorem 3.18, f
Ad can be computed as follows:
f
AdX =
d
dt

t=0 Ad(etX).
Thus
f
AdX(Y ) =
d
dt

t=0 Ad(etX)(Y ) =
d
dt

t=0 etXY eâˆ’tX
= [X, Y ]
which is what we wanted to prove. See also Exercise 13.
7. The Exponential Mapping
Definition 3.22. If G is a matrix Lie group with Lie algebra g, then the ex-
ponential mapping for G is the map
exp : g â†’G.
In general the exponential mapping is neither one-to-one nor onto. Neverthe-
less, it provides an crucial mechanism for passing information between the group
and the Lie algebra. The following result says that the exponential mapping is
locally one-to-one and onto, a result that will be essential later.
Theorem 3.23. Let G be a matrix Lie group with Lie algebra g. Then there
exist a neighborhood U of zero in g and a neighborhood V of I in G such that the
exponential mapping takes U homeomorphically onto V .
Proof. We follow the proof of Theorem I.3.11 in BrÂ¨ocker and tom Dieck. In
view of what we have proved about the matrix logarithm, we know this result for
the case of GL(n; C). To prove the general case, we consider a matrix Lie group
G < GL(n; C), with Lie algebra g.
Lemma 3.24. Suppose gn are elements of G, and that gn â†’I. Let Yn = loggn,
which is deï¬ned for all suï¬ƒciently large n. Suppose Yn/ âˆ¥Ynâˆ¥â†’Y âˆˆgl (n; C).
Then Y âˆˆg.
Proof. To show that Y âˆˆg, we must show that exp tY âˆˆG for all t âˆˆR. As
n â†’âˆ, (t/ âˆ¥Ynâˆ¥) Yn â†’tY . Note that since gn â†’I, Yn â†’0, and so âˆ¥Ynâˆ¥â†’0.
Thus we can ï¬nd integers mn such that (mn âˆ¥Ynâˆ¥) â†’t.
Then exp (mnYn) =
exp [(mn âˆ¥Ynâˆ¥) (Yn/ âˆ¥Ynâˆ¥)] â†’exp (tY ). But exp (mnYn) = exp (Yn)mn = (gn)mn âˆˆ
G, and G is closed, so exp (tY ) âˆˆG.
We think of gl(n; C) as Cn2 âˆ¼= R2n2. Then g is a subspace of R2n2. Let D
denote the orthogonal complement of g with respect to the usual inner product on
R2n2. Consider the map Î¦ : g âŠ•D â†’GL(n; C) given by
Î¦ (X, Y ) = eXeY .
Of course, we can identify g âŠ•D with R2n2. Moreover, GL(n; C) is an open subset
of gl (n; C) âˆ¼= R2n2. Thus we can regard Î¦ as a map from R2n2 to itself.

7. THE EXPONENTIAL MAPPING
45
Now, using the properties of the matrix exponential, we see that
d
dt

t=0
Î¦ (tX, 0) = X
d
dt

t=0
Î¦ (0, tY ) = Y .
This shows that the derivative of Î¦ at the point 0 âˆˆR2n2 is the identity. (Recall
that the derivative at a point of a function from R2n2 to itself is a linear map of
R2n2 to itself, in this case the identity map.) In particular, the derivative of Î¦ at 0
is invertible. Thus the inverse function theorem says that Î¦ has a continuous local
inverse, deï¬ned in a neighborhood of I.
Now let U be any neighborhood of zero in g. I want to show that exp (U)
contains a neighborhood of I in G. Suppose not. Then we can ï¬nd a sequence
gn âˆˆG with gn â†’I such that no gn is in exp (U). Since Î¦ is locally invertible,
we can write gn (for large n) uniquely as gn = exp (Xn) exp (Yn), with Xn âˆˆg and
Yn âˆˆD. Since gn â†’I and Î¦âˆ’1 is continuous, Xn and Yn tend to zero. Thus (for
large n), Xn âˆˆU. So we must have (for large n) Yn Ì¸= 0, otherwise gn would be in
exp (U).
Let egn = exp (Yn) = exp (âˆ’Xn) gn. Note that egn âˆˆG and egn â†’I. Since the
unit ball in D is compact, we can choose a subsequence of {Yn} (still called {Yn})
so that Yn/ âˆ¥Ynâˆ¥converges to some Y âˆˆD, with âˆ¥Y âˆ¥= 1. But then by the Lemma,
Y âˆˆg! This is a contradiction, because D is the orthogonal complement of g.
So for every neighborhood U of zero in g, exp (U) contains a neighborhood of
the identity in G. If we make U small enough, then the exponential will be one-to-
one on U. (The existence of the matrix logarithm implies that the exponential is
one-to-one near zero.) Let log denote the inverse map, deï¬ned on exp
 U

. Since U
is compact, and exp is one-to-one and continuous on U, log will be continuous. (This
is a standard topological result.) So take V to be a neighborhood of I contained in
exp  U, and let U â€² = expâˆ’1 (V ) âˆ©U. Then U â€² is open and the exponential takes
U â€² homeomorphically onto V .
Definition 3.25. If U and V are as in Proposition 3.23, then the inverse map
expâˆ’1 : V â†’g is called the logarithm for G.
Corollary 3.26. If G is a connected matrix Lie group, then every element A
of G can be written in the form
A = eX1eX2 Â· Â· Â· eXn
(3.16)
for some X1, X2, Â· Â· Â·Xn in g.
Proof. Recall that for us, saying G is connected means that G is path-
connected. This certainly means that G is connected in the usual topological sense,
namely, the only non-empty subset of G that is both open and closed is G itself.
So let E denote the set of all A âˆˆG that can be written in the form (3.16). In light
of the Proposition, E contains a neighborhood V of the identity. In particular, E
is non-empty.
We ï¬rst claim that E is open. To see this, consider A âˆˆE. Then look at the
set of matrices of the form AB, with B âˆˆV . This will be a neighborhood of A. But

46
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
every such B can be written as B = eX and A can be written as A = eX1eX2 Â· Â· Â· eXn,
so AB = eX1eX2 Â· Â· Â· eXneX.
Now we claim that E is closed (in G). Suppose A âˆˆG, and there is a sequence
An âˆˆE with An â†’A.
Then AAâˆ’1
n
â†’I.
Thus we can choose some n0 such
that AAâˆ’1
n0 âˆˆV . Then AAâˆ’1
n0 = eX and A = An0eX. But by assumption, An0 =
eX1eX2 Â· Â· Â· eXn, so A = eX1eX2 Â· Â· Â·eXneX. Thus A âˆˆE, and E is closed.
Thus E is both open and closed, so E = G.
8. Lie Algebras
Definition 3.27. A ï¬nite-dimensional real or complex Lie algebra is a
ï¬nite-dimensional real or complex vector space g, together with a map [ ] from gÃ—g
into g, with the following properties:
1. [ ] is bilinear.
2. [X, Y ] = âˆ’[Y, X] for all X, Y âˆˆg.
3. [X, [Y, Z]] + [Y, [Z, X]] + [Z, [X, Y ]] = 0 for all X, Y, Z âˆˆg.
Condition 3 is called the Jacobi identity. Note also that Condition 2 implies
that [X, X] = 0 for all X âˆˆg. The same three conditions deï¬ne a Lie algebra over
an arbitrary ï¬eld F, except that if F has characteristic two, then one should add the
condition [X, X] = 0, which doesnâ€™t follow from skew-symmetry in characteristic
two. We will deal only with ï¬nite-dimensional Lie algebras, and will from now on
interpret â€œLie algebraâ€ as â€œï¬nite-dimensional Lie algebra.â€
A Lie algebra is in fact an algebra in the usual sense, but the product operation
[ ] for this algebra is neither commutative nor associative.
The Jacobi identity
should be thought of as a substitute for associativity.
Proposition 3.28. The space gl(n; R) of all n Ã— n real matrices is a real Lie
algebra with respect to the bracket operation [A, B] = AB âˆ’BA. The space gl(n; C)
of all nÃ—n complex matrices is a complex Lie algebra with respect to the analogous
bracket operation.
Let V is a ï¬nite-dimensional real or complex vector space, and let gl(V ) denote
the space of linear maps of V into itself. Then gl(V ) becomes a real or complex Lie
algebra with the bracket operation [A, B] = AB âˆ’BA.
Proof. The only non-trivial point is the Jacobi identity. The only way to
prove this is to write everything out and see, and this is best left to the reader.
Note that each triple bracket generates four terms, for a total of twelve. Each of
the six orderings of {X, Y, Z} occurs twice, once with a plus sign and once with a
minus sign.
Definition 3.29. A subalgebra of a real or complex Lie algebra g is a sub-
space h of g such that [H1, H2] âˆˆh for all H1, H2 âˆˆh. If g is a complex Lie algebra,
and h is a real subspace of g which is closed under brackets, then h is said to be a
real subalgebra of g.
If g and h are Lie algebras, then a linear map Ï† : g â†’h is called a Lie algebra
homomorphism if Ï† ([X, Y ]) = [Ï†(X), Ï†(Y )] for all X, Y âˆˆg. If in addition Ï† is
one-to-one and onto, then Ï† is called a Lie algebra isomorphism. A Lie algebra
isomorphism of a Lie algebra with itself is called a Lie algebra automorphism.

8. LIE ALGEBRAS
47
A subalgebra of a Lie algebra is again a Lie algebra. A real subalgebra of a
complex Lie algebra is a real Lie algebra. The inverse of a Lie algebra isomorphism
is again a Lie algebra isomorphism.
Proposition 3.30. The Lie algebra g of a matrix Lie group G is a real Lie
algebra.
Proof. By Theorem 3.16, g is a real subalgebra of gl(n; C) complex matrices,
and is thus a real Lie algebra.
Theorem 3.31 (Ado). Every ï¬nite-dimensional real Lie algebra is isomorphic
to a subalgebra of gl(n; R). Every ï¬nite-dimensional complex Lie algebra is isomor-
phic to a (complex) subalgebra of gl(n; C).
This remarkable theorem is proved in Varadarajan. The proof is well beyond
the scope of this course (which is after all a course on Lie groups), and requires
a deep understanding of the structure of complex Lie algebras. The theorem tells
us that every Lie algebra is (isomorphic to) a Lie algebra of matrices. (This is
in contrast to the situation for Lie groups, where most but not all Lie groups are
matrix Lie groups.)
Definition 3.32. Let g be a Lie algebra.
For X âˆˆg, deï¬ne a linear map
adX : g â†’g by
adX(Y ) = [X, Y ].
Thus â€œadâ€ (i.e., the map X â†’adX) can be viewed as a linear map from g into
gl(g), where gl(g) denotes the space of linear operators from g to g.
Since adX(Y ) is just [X, Y ], it might seem foolish to introduce the additional
â€œadâ€ notation. However, thinking of [X, Y ] as a linear map in Y for each ï¬xed X,
gives a somewhat diï¬€erent perspective. In any case, the â€œadâ€ notation is extremely
useful in some situations. For example, instead of writing
[X, [X, [X, [X, Y]]]]
we can now write
(adX)4 (Y ).
This kind of notation will be essential in Section 1.
Proposition 3.33. If g is a Lie algebra, then
ad[X, Y ] = adXadY âˆ’adY adX = [adX, adY ].
That is, ad: g â†’gl(g) is a Lie algebra homomorphism.
Proof. Observe that
ad[X, Y ](Z) = [[X, Y ], Z]
whereas
[adX, adY ](Z) = [X, [Y, Z]] âˆ’[Y, [X, Z]].
So we require that
[[X, Y ], Z] = [X, [Y, Z]] âˆ’[Y, [X, Z]]

48
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
or equivalently
0 = [X, [Y, Z]] + [Y, [Z, X]] + [Z, [X, Y ]]
which is exactly the Jacobi identity.
Recall that for any X âˆˆg, and any A âˆˆG, we deï¬ne
AdA(X) = AXAâˆ’1
and that Ad: G â†’GL(g) is a Lie group homomorphism. We showed (Proposition
3.21) that the associated Lie algebra homomorphism f
Ad : g â†’gl(g) is given by
f
AdX(Y ) = [X, Y ].
In our new notation, we may say
f
Ad = ad
By the deï¬ning property of f
Ad, we have the following identity: For all X âˆˆg,
Ad(eX) = eadX.
(3.17)
Note that both sides of (3.17) are linear operators on the Lie algebra g. This is
an important relation, which can also be veriï¬ed directly, by expanding out both
sides. (See Exercise 13.)
8.1. Structure Constants. Let g be a ï¬nite-dimensional real or complex Lie
algebra, and let X1, Â· Â· Â· , Xn be a basis for g (as a vector space). Then for each i, j,
[Xi, Xj] can be written uniquely in the form
[Xi, Xj] =
n
X
k=1
cijkXk.
The constants cijk are called the structure constants of g (with respect to the
chosen basis). Clearly, the structure constants determine the bracket operation on
g. In some of the literature, the structure constants play an important role, although
we will not have occasion to use them in this course. (In the physics literature, the
structure constants are deï¬ned as [Xi, Xj] = âˆšâˆ’1 P
k cijkXk, reï¬‚ecting the factor
of âˆšâˆ’1 diï¬€erence between the physics deï¬nition of the Lie algebra and our own.)
The structure constants satisfy the following two conditions,
cijk + cjik = 0
X
m
(cijmcmkl + cjkmcmil + ckimcmjl) = 0
for all i, j, k, l. The ï¬rst of these conditions comes from the skew-symmetry of the
bracket, and the second comes from the Jacobi identity. (The reader is invited to
verify these conditions for himself.)
9. The Complexiï¬cation of a Real Lie Algebra
Definition 3.34. If V is a ï¬nite-dimensional real vector space, then the com-
plexiï¬cation of V , denoted VC, is the space of formal linear combinations
v1 + iv2

9. THE COMPLEXIFICATION OF A REAL LIE ALGEBRA
49
with v1, v2 âˆˆV . This becomes a real vector space in the obvious way, and becomes
a complex vector space if we deï¬ne
i(v1 + iv2) = âˆ’v2 + iv1.
We could more pedantically deï¬ne VC to be the space of ordered pairs (v1, v2),
but this is notationally cumbersome. It is straightforward to verify that the above
deï¬nition really makes VC into a complex vector space. We will regard V as a real
subspace of VC in the obvious way.
Proposition 3.35. Let g be a ï¬nite-dimensional real Lie algebra, and gC its
complexiï¬cation (as a real vector space). Then the bracket operation on g has a
unique extension to gC which makes gC into a complex Lie algebra. The complex
Lie algebra gC is called the complexiï¬cation of the real Lie algebra g.
Proof. The uniqueness of the extension is obvious, since if the bracket oper-
ation on gC is to be bilinear, then it must be given by
[X1 + iX2, Y1 + iY2] = ([X1, Y1] âˆ’[X2, Y2]) + i ([X1, Y2] + [X2, Y1]) .
(3.18)
To show existence, we must now check that (3.18) is really bilinear and skew-
symmetric, and that it satisï¬es the Jacobi identity. It is clear that (3.18) is real
bilinear, and skew-symmetric. The skew-symmetry means that if (3.18) is complex
linear in the ï¬rst factor, it is also complex linear in the second factor. Thus we
need only show that
[i(X1 + iX2), Y1 + iY2] = i [X1 + iX2, Y1 + iY2] .
(3.19)
Well, the left side of (3.19) is
[âˆ’X2 + iX1, Y1 + iY2] = (âˆ’[X2, Y1] âˆ’[X1, Y2]) + i ([X1, Y1] âˆ’[X2, Y2])
whereas the right side of (3.19) is
i {([X1, Y1] âˆ’[X2, Y2]) + i ([X2, Y1] + [X1, Y2])}
= (âˆ’[X2, Y1] âˆ’[X1, Y2]) + i ([X1, Y1] âˆ’[X2, Y2]) ,
and indeed these are equal.
It remains to check the Jacobi identity. Of course, the Jacobi identity holds if
X, Y, and Z are in g. But now observe that the expression on the left side of the
Jacobi identity is (complex!) linear in X for ï¬xed Y and Z. It follows that the
Jacobi identity holds if X is in gC, and Y, Z in g. The same argument then shows
that we can extend to Y in gC, and then to Z in gC. Thus the Jacobi identity holds
in gC.
Proposition 3.36. The Lie algebras gl(n; C), sl(n; C), so(n; C), and sp(n; C)
are complex Lie algebras, as is the Lie algebra of the complex Heisenberg group. In
addition, we have the following isomorphisms of complex Lie algebras
gl (n; R)C
âˆ¼=
gl(n; C)
u(n)C
âˆ¼=
gl(n; C)
sl (n; R)C
âˆ¼=
sl(n; C)
so(n)C
âˆ¼=
so(n; C)
sp(n; R)C
âˆ¼=
sp(n; C)
sp(n)C
âˆ¼=
sp(n; C).

50
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
Proof. From the computations in the previous section we see easily that the
speciï¬ed Lie algebras are in fact complex subalgebras of gl(n; C), and hence are
complex Lie algebras.
Now, gl(n; C) is the space of all nÃ—n complex matrices, whereas gl(n; R) is the
space of all n Ã— n real matrices. Clearly, then, every X âˆˆgl (n; C) can be written
uniquely in the form X1 + iX2, with X1, X2 âˆˆgl (n; R). This gives us a complex
vector space isomorphism of gl (n; R)C with gl(n; C), and it is a triviality to check
that this is a Lie algebra isomorphism.
On the other hand, u(n) is the space of all n Ã— n complex skew-self-adjoint
matrices. But if X is any n Ã— n complex matrix, then
X = X âˆ’Xâˆ—
2
+ X + Xâˆ—
2
= X âˆ’Xâˆ—
2
+ i(âˆ’iX) âˆ’(âˆ’iX)âˆ—
2
.
Thus X can be written as a skew matrix plus i times a skew matrix, and it is easy
to see that this decomposition is unique. Thus every X in gl(n; C) can be written
uniquely as X1 + iX2, with X1 and X2 in u(n). It follows that u(n)C âˆ¼= gl(n; C).
The veriï¬cation of the remaining isomorphisms is similar, and is left as an
exercise to the reader.
Note that u(n)C âˆ¼= gl(n; R)C âˆ¼= gl(n; C). However, u(n) is not isomorphic to
gl(n; R), except when n = 1. The real Lie algebras u(n) and gl(n; R) are called
real forms of the complex Lie algebra gl(n; C). A given complex Lie algebra may
have several non-isomorphic real forms. See Exercise 11.
Physicists do not always clearly distinguish between a matrix Lie group and
its (real) Lie algebra, or between a real Lie algebra and its complexiï¬cation. Thus,
for example, some references in the physics literature to SU(2) actually refer to the
complexiï¬ed Lie algebra, sl(2; C).
10. Exercises
1. The product rule. Recall that a matrix-valued function A(t) is smooth if
each Aij(t) is smooth. The derivative of such a function is deï¬ned as
dA
dt

ij
= dAij
dt
or equivalently,
d
dtA(t) = lim
hâ†’0
A(t + h) âˆ’A(t)
h
.
Let A(t) and B(t) be two such functions. Prove that A(t)B(t) is again
smooth, and that
d
dt [A(t)B(t)] = dA
dt B(t) + A(t)dB
dt .
2. Using the Jordan canonical form, show that every n Ã— n matrix A can be
written as A = S + N, with S diagonalizable (over C), N nilpotent, and
SN = NS. Recall that the Jordan canonical form is block diagonal, with

10. EXERCISES
51
each block of the form
ï£«
ï£¬
ï£­
Î»
âˆ—
...
0
Î»
ï£¶
ï£·
ï£¸.
3. Let X and Y be n Ã— n matrices. Show that there exists a constant C such
that
e(X+Y )/m âˆ’eX/meY /m â‰¤C
m2
for all integers m â‰¥1.
4. Using the Jordan canonical form, show that every n Ã— n complex matrix A
is the limit of a sequence of diagonalizable matrices.
Hint: If the characteristic polynomial of A has n distinct roots, then A
is diagonalizable.
5. Give an example of a matrix Lie group G and a matrix X such that eX âˆˆG,
but X /âˆˆg.
6. Show that two isomorphic matrix Lie groups have isomorphic Lie algebras.
7. The Lie algebra so(3; 1). Write out explicitly the general form of a 4Ã—4 real
matrix in so(3; 1).
8. Verify directly that Proposition 3.15 and Theorem 3.16 hold for the Lie
algebra of SU(n).
9. The Lie algebra su(2). Show that the following matrices form a basis for the
real Lie algebra su(2):
E1 = 1
2
 i
0
0
âˆ’i

E2 = 1
2

0
1
âˆ’1
0

E3 = 1
2
 0
i
i
0

.
Compute [E1, E2], [E2, E3], and [E3, E1]. Show that there is an invert-
ible linear map Ï† : su(2) â†’R3 such that Ï†([X, Y ]) = Ï†(X) Ã— Ï†(Y ) for all
X, Y âˆˆsu(2), where Ã— denotes the cross-product on R3.
10. The Lie algebras su(2) and so(3). Show that the real Lie algebras su(2) and
so(3) are isomorphic.
Note: Nevertheless, the corresponding groups SU(2) and SO(3) are not
isomorphic. (Although SO(3) is isomorphic to SU(2)/ {I, âˆ’I}.)
11. The Lie algebras su(2) and sl(2; R). Show that su(2) and sl(2; R) are not
isomorphic Lie algebras, even though su(2)C âˆ¼= sl(2; R)C.
Hint: Using Exercise 9, show that su(2) has no two-dimensional subal-
gebras.
12. Let G be a matrix Lie group, and g its Lie algebra. For each A âˆˆG, show
that AdA is a Lie algebra automorphism of g.
13. Ad and ad. Let X and Y be matrices. Show by induction that
(adX)n (Y ) =
n
X
k=0
n
k

XkY (âˆ’X)nâˆ’k.
Now show by direct computation that
eadX(Y ) = Ad(eX)Y = eXY eâˆ’X.
You may assume that it is legal to multiply power series term-by-term. (This
result was obtained indirectly in Equation 3.17.)

52
3. LIE ALGEBRAS AND THE EXPONENTIAL MAPPING
Hint: Recall that Pascalâ€™s Triangle gives a relationship between things
of the form  n+1
k
 and things of the form  n
k
.
14. The complexiï¬cation of a real Lie algebra. Let g be a real Lie algebra, gC its
complexiï¬cation, and h an arbitrary complex Lie algebra. Show that every
real Lie algebra homomorphism of g into h extends uniquely to a complex
Lie algebra homomorphism of gC into h. (This is the universal property
of the complexiï¬cation of a real Lie algebra. This property can be used as
an alternative deï¬nition of the complexiï¬cation.)
15. The exponential mapping for SL (2; R). Show that the image of the exponen-
tial mapping for SL (2; R) consists of precisely those matrices A âˆˆSL (2; R)
such that trace (A) > âˆ’2, together with the matrix âˆ’I (which has trace âˆ’2).
You will need to consider the possibilities for the eigenvalues of a matrix in
the Lie algebra sl (2; R) and in the group SL (2; R). In the Lie algebra, show
that the eigenvalues are of the form (Î», âˆ’Î») or (iÎ», âˆ’iÎ») with Î» real. In the
group, show that the eigenvalues are of the form (Î±, 1/a) or (âˆ’a, âˆ’1/a) with
a real and positive, or else of the form
 eiÎ¸, eâˆ’iÎ¸
, with Î¸ real. The case of
a repeated eigenvalue ((0, 0) in the Lie algebra and (1, 1) or (âˆ’1, âˆ’1) in the
group) will have to be treated separately.
Show that the image of the exponential mapping is not dense in SL (2; R).
16. Using Exercise 4, show that the exponential mapping for GL(n; C) maps onto
a dense subset of GL(n; C).
17. The exponential mapping for the Heisenberg group. Show that the exponen-
tial mapping from the Lie algebra of the Heisenberg group to the Heisenberg
group is one-to-one and onto.
18. The exponential mapping for U(n).
Show that the exponential mapping
from u(n) to U(n) is onto, but not one-to-one. (Note that this shows that
U(n) is connected.)
Hint: Every unitary matrix has an orthonormal basis of eigenvectors.
19. Let G be a matrix Lie group, and g its Lie algebra. Let A(t) be a smooth
curve lying in G, with A(0) = I. Let X =
d
dt

t=0 A(t). Show that X âˆˆg.
Hint: Use Proposition 3.8.
Note: This shows that the Lie algebra g coincides with what would be
called the tangent space at the identity in the language of diï¬€erentiable
manifolds.
20. Consider the space gl(n; C) of all nÃ—n complex matrices. As usual, for X âˆˆ
gl(n; C), deï¬ne adX : gl(n; C) â†’gl(n; C) by adX(Y ) = [X, Y ]. Suppose
that X is a diagonalizable matrix. Show, then, that adX is diagonalizable
as an operator on gl(n; C).
Hint: Consider ï¬rst the case where X is actually diagonal.
Note: The problem of diagonalizing adX is an important one that we
will encounter again in Chapter 6, when we consider semisimple Lie algebras.

CHAPTER 4
The Baker-Campbell-Hausdorï¬€Formula
1. The Baker-Campbell-Hausdorï¬€Formula for the Heisenberg Group
A crucial result of Chapter 5 will be the following: Let G and H be matrix
Lie groups, with Lie algebras g and h, and suppose that G is connected and simply
connected. Then if eÏ† : g â†’h is a Lie algebra homomorphism, there exists a unique
Lie group homomorphism Ï† : G â†’H such that Ï† and eÏ† are related as in Theorem
3.18. This result is extremely important because it implies that if G is connected
and simply connected, then there is a natural one-to-one correspondence between
the representations of G and the representations of its Lie algebra g (as explained
in Chapter 5). In practice, it is much easier to determine the representations of
the Lie algebra than to determine directly the representations of the corresponding
group.
This result (relating Lie algebra homomorphisms and Lie group homomor-
phisms) is deep. The â€œmodernâ€ proof (e.g., Varadarajan, Theorem 2.7.5) makes
use of the Frobenius theorem, which is both hard to understand and hard to
prove (Varadarajan, Section 1.3). Our proof will instead use the Baker-Campbell-
Hausdorï¬€formula, which is more easily stated and more easily motivated than the
Frobenius theorem, but still deep.
The idea is the following. The desired group homomorphism Ï† : G â†’H must
satisfy
Ï†  eX = e
eÏ†(X).
(4.1)
We would like, then, to deï¬ne Ï† by this relation. This approach has two serious
diï¬ƒculties. First, a given element of G may not be expressible as eX, and even if
it is, the X may not be unique. Second, it is very far from clear why the Ï† in (4.1)
(even to the extent it is well-deï¬ned) should be a group homomorphism.
It is the second issue which the Baker-Campbell-Hausdorï¬€formula addresses.
(The ï¬rst issue will be addressed in the next chapter; it is there that the simple con-
nectedness of G comes into play.) Speciï¬cally, (one form of) the Baker-Campbell-
Hausdorï¬€formula says that if X and Y are suï¬ƒciently small, then
log(eXeY ) = X + Y + 1
2[X, Y ] + 1
12[X, [X, Y ]] âˆ’1
12[Y, [X, Y ]] + Â· Â· Â·.
(4.2)
It is not supposed to be evident at the moment what â€œÂ· Â· Â·â€ refers to. The only
important point is that all of the terms in (4.2) are given in terms of X and Y ,
brackets of X and Y , brackets of brackets involving X and Y , etc. Then because
53

54
4. THE BAKER-CAMPBELL-HAUSDORFF FORMULA
eÏ† is a Lie algebra homomorphism,
eÏ†  log eXeY  = eÏ†(X) + eÏ†(Y ) + 1
2[eÏ†(X), eÏ†(Y )]
+ 1
12[eÏ†(X), [eÏ†(X), eÏ†(Y )]] âˆ’1
12[eÏ†(Y ), [eÏ†(X), eÏ†(Y )]] + Â· Â· Â·
= log

e
eÏ†(X)e
eÏ†(Y )
(4.3)
The relation (4.3) is extremely signiï¬cant. For of course
eXeY = elog(eXeY )
and so by (4.1),
Ï†  eXeY  = e
eÏ†(log(eXeY )).
Thus (4.3) tells us that
Ï†  eXeY  = elog

e
e
Ï†(X)e
e
Ï†(Y )
= e
eÏ†(X)e
eÏ†(Y ) = Ï†(eX)Ï†(eY ).
Thus, the Baker-Campbell-Hausdorï¬€formula shows that on elements of the form
eX, with X small, Ï† is a group homomorphism. (See Corollary 4.4 below.)
The Baker-Campbell-Hausdorï¬€formula shows that all the information about
the group product, at least near the identity, is â€œencodedâ€ in the Lie algebra. Thus
if eÏ† is a Lie algebra homomorphism (which by deï¬nition preserves the Lie algebra
structure), and if we deï¬ne Ï† near the identity by (4.1), then we can expect Ï† to
preserve the group structure, i.e., to be a group homomorphism.
In this section we will look at how all of this works out in the very special case
of the Heisenberg group. In the next section we will consider the general situation.
Theorem 4.1. Suppose X and Y are n Ã— n complex matrices, and that X and
Y commute with their commutator. That is, suppose that
[X, [X, Y ]] = [Y, [X, Y ]] = 0.
Then
eXeY = eX+Y + 1
2 [X,Y ].
This is the special case of (4.2) in which the series terminates after the [X, Y ]
term.
Proof. Let X and Y be as in the statement of the theorem. We will prove
that in fact
etXetY = exp

tX + tY + t2
2 [X, Y ]

,
which reduces to the desired result in the case t = 1. Since by assumption [X, Y ]
commutes with everything in sight, the above relation is equivalent to
etXetY eâˆ’t2
2 [X,Y ] = et(X+Y ).
(4.4)
Let us call the left side of (4.4) A(t) and the right side B (t). Our strategy
will be to show that A (t) and B (t) satisfy the same diï¬€erential equation, with the
same initial conditions. We can see right away that
dB
dt = B (t) (X + Y ) .

1. THE BAKER-CAMPBELL-HAUSDORFF FORMULA FOR THE HEISENBERG GROUP 55
On the other hand, diï¬€erentiating A (t) by means of the product rule gives
dA
dt = etXXetY eâˆ’t2
2 [X,Y ] + etXetY Y eâˆ’t2
2 [X,Y ] + etXetY eâˆ’t2
2 [X,Y ] (âˆ’t [X, Y ]) .
(4.5)
(You can verify that the last term on the right is correct by diï¬€erentiating term-
by-term.)
Now, since X and Y commute with [X, Y ], they also commute with eâˆ’t2
2 [X,Y ].
Thus the second term on the right in (4.5) can be rewritten as
etXetY eâˆ’t2
2 [X,Y ]Y .
The ï¬rst term on the right in (4.5) is more complicated, since X does not necessarily
commute with etY . However,
XetY = etY eâˆ’tY XetY
= etY Ad
 eâˆ’tY 
(X)
= etY eâˆ’tadY (X) .
But since [Y, [Y, X]] = âˆ’[Y, [X, Y ]] = 0,
eâˆ’tadY (X) = X âˆ’t [Y, X] = X + t [X, Y ]
with all higher terms being zero. Using the fact that everything commutes with
eâˆ’t2
2 [X,Y ] gives
etXXetY eâˆ’t2
2 [X,Y ] = etXetY eâˆ’t2
2 [X,Y ] (X + t [X, Y ])
Making these substitutions into (4.5) gives
dA
dt = etXetY eâˆ’t2
2 [X,Y ] (X + t [X, Y ]) + etXetY eâˆ’t2
2 [X,Y ]Y + etXetY eâˆ’t2
2 [X,Y ] (âˆ’t [X, Y ])
= etXetY eâˆ’t2
2 [X,Y ] (X + Y )
= A (t) (X + Y ) .
Thus A (t) and B (t) satisfy the same diï¬€erential equation. Moreover, A (0) =
B (0) = I. Thus by standard uniqueness results for ordinary diï¬€erential equations,
A (t) = B (t) for all t.
Theorem 4.2. Let H denote the Heisenberg group, and h its Lie algebra. Let
G be a matrix Lie group with Lie algebra g, and let eÏ† : h â†’g be a Lie algebra
homomorphism. Then there exists a unique Lie group homomorphism Ï† : H â†’G
such that
Ï†
 eX
= e
eÏ†(X)
for all X âˆˆh.
Proof. Recall that the Heisenberg group has the very special property that
its exponential mapping is one-to-one and onto. Let â€œlogâ€ denote the inverse of
this map. Deï¬ne Ï† : H â†’G by the formula
Ï† (A) = e
eÏ†(log A).
We will show that Ï† is a Lie group homomorphism.

56
4. THE BAKER-CAMPBELL-HAUSDORFF FORMULA
If X and Y are in the Lie algebra of the Heisenberg group (3 Ã— 3 strictly
upper-triangular matrices), then [X, Y ] is of the form
ï£«
ï£­
0
0
a
0
0
0
0
0
0
ï£¶
ï£¸;
such a matrix commutes with both X and Y . That is, X and Y commute with
their commutator. Since eÏ† is a Lie algebra homomorphism, eÏ† (X) and eÏ† (Y ) will
also commute with their commutator:
h
eÏ†(X) ,
h
eÏ† (X) , eÏ†(Y )
ii
= eÏ† ([X, [X, Y ]]) = 0
h
eÏ† (Y ) ,
h
eÏ†(X) , eÏ†(Y )
ii
= eÏ† ([Y, [X, Y ]]) = 0.
We want to show that Ï† is a homomorphism, i.e., that Ï† (AB) = Ï† (A) Ï† (B).
Well, A can be written as eX for a unique X âˆˆh and B can be written as eY for a
unique Y âˆˆh. Thus by Theorem 4.1
Ï† (AB) = Ï†
 eXeY 
= Ï†

eX+Y + 1
2 [X,Y ]
.
Using the deï¬nition of Ï† and the fact that eÏ† is a Lie algebra homomorphism:
Ï† (AB) = exp

eÏ† (X) + eÏ†(Y ) + 1
2
h
eÏ†(X) , eÏ†(Y )
i
.
Finally, using Theorem 4.1 again we have
Ï† (AB) = e
eÏ†(X)e
eÏ†(Y ) = Ï† (A) Ï† (B) .
Thus Ï† is a group homomorphism. It is easy to check that Ï† is continuous (by
checking that log, exp, and eÏ† are all continuous), and so Ï† is a Lie group homo-
morphism. Moreover, Ï† by deï¬nition has the right relationship to eÏ†. Furthermore,
since the exponential mapping is one-to-one and onto, there can be at most one Ï†
with Ï†
 eX
= eeÏ†(X). So we have uniqueness.
2. The General Baker-Campbell-Hausdorï¬€Formula
The importance of the Baker-Campbell-Hausdorï¬€formula lies not in the details
of the formula, but in the fact that there is one, and the fact that it gives log(eXeY )
in terms of brackets of X and Y , brackets of brackets, etc. This tells us something
very important, namely that (at least for elements of the form eX, X small) the
group product for a matrix Lie group G is completely expressible in terms of the Lie
algebra. (This is because log
 eXeY 
, and hence also eXeY itself, can be computed
in Lie-algebraic terms by (4.2).)
We will actually state and prove an integral form of the Baker-Campbell-
Hausdorï¬€formula, rather than the series form (4.2). However, the integral form is
suï¬ƒcient to obtain the desired result (4.3). (See Corollary 4.4.) The series form of
the Baker-Campbell-Hausdorï¬€formula is stated precisely and proved in Varadara-
jan, Sec. 2.15.
Consider the function
g(z) = log z
1 âˆ’1
z
.

2. THE GENERAL BAKER-CAMPBELL-HAUSDORFF FORMULA
57
This function is deï¬ned and analytic in the disk {|z âˆ’1| < 1}, and thus for z in
this set, g(z) can be expressed as
g(z) =
âˆ
X
m=0
am(z âˆ’1)m.
This series has radius of convergence one.
Now suppose V is a ï¬nite-dimensional complex vector space. Choose an arbi-
trary basis for V , so that V can be identiï¬ed with Cn and thus the norm of a linear
operator on V can be deï¬ned. Then for any operator A on V with âˆ¥A âˆ’Iâˆ¥< 1,
we can deï¬ne
g(A) =
âˆ
X
m=0
am(A âˆ’1)m.
We are now ready to state the integral form of the Baker-Campbell-Hausdorï¬€for-
mula.
Theorem 4.3 (Baker-Campbell-Hausdorï¬€). For all nÃ—n complex matrices X
and Y with âˆ¥Xâˆ¥and âˆ¥Y âˆ¥suï¬ƒciently small,
log
 eXeY 
= X +
Z 1
0
g(eadXetadY )(Y ) dt.
(4.6)
Corollary 4.4. Let G be a matrix Lie group and g its Lie algebra. Suppose
that eÏ† : g â†’gl(n; C) is a Lie algebra homomorphism. Then for all suï¬ƒciently
small X, Y in g, log
 eXeY 
is in g, and
eÏ†

log
 eXeY 
= log

e
eÏ†(X)e
eÏ†(Y )
.
(4.7)
Note that eadXetadY , and hence also g(eadXetadY ), is a linear operator on the
space gl(n; C) of all nÃ—n complex matrices. In (4.6), this operator is being applied to
the matrix Y . The fact that X and Y are assumed small guarantees that eadXetadY
is close to the identity operator on gl(n; C) for all 0 â‰¤t â‰¤1. This ensures that
g(eadXetadY ) is well deï¬ned.
If X and Y commute, then we expect to have log  eXeY  = log(eX+Y ) = X+Y .
Exercise 3 asks you to verify that the Baker-Campbell-Hausdorï¬€formula indeed
gives X + Y in that case.
Formula (4.6) is admittedly horrible-looking. However, we are interested not in
the details of the formula, but in the fact that it expresses log  eXeY  (and hence
eXeY ) in terms of the Lie-algebraic quantities adX and adY .
The goal of the Baker-Campbell-Hausdorï¬€theorem is to compute log
 eXeY 
.
You may well ask, â€œWhy donâ€™t we simply expand both exponentials and the loga-
rithm in power series and multiply everything out?â€ Well, you can do this, and if
you do it for the ï¬rst several terms you will get the same answer as B-C-H. However,
there is a serious problem with this approach, namely: How do you know that the
terms in such an expansion are expressible in terms of commutators? Consider for
example the quadratic term. It is clear that this will be a linear combination of X2,
Y 2, XY , and Y X. But to be expressible in terms of commutators it must actually
be a constant times (XY âˆ’Y X). Of course, for the quadratic term you can just
multiply it out and see, and indeed you get 1
2 (XY âˆ’Y X) = 1
2 [X, Y ]. But it is far
from clear how to prove that a similar result occurs for all the higher terms. See
Exercise 4.

58
4. THE BAKER-CAMPBELL-HAUSDORFF FORMULA
Proof. We begin by proving that the corollary follows from the integral form
of the Baker-Campbell-Hausdorï¬€formula. The proof is conceptually similar to the
reasoning in Equation (4.3). Note that if X and Y lie in some Lie algebra g then
adX and adY will preserve g, and so also will g(eadXetadY )(Y ). Thus whenever
formula (4.6) holds, log
 eXeY 
will lie in g. It remains only to verify (4.7). The
idea is that if eÏ† is Lie algebra homomorphism, then it will take a big horrible looking
expression involving â€˜adâ€™ and X and Y , and turn it into the same expression with
X and Y replaced by eÏ†(X) and eÏ†(Y ).
More precisely, since eÏ† is a Lie algebra homomorphism,
eÏ†[Y, X] = [eÏ†(Y ), eÏ†(X)]
or
eÏ†(adY (X)) = adeÏ†(Y )

eÏ†(X)

.
More generally,
eÏ† ((adY )n (X)) =

adeÏ†(Y )
n 
eÏ† (X)

.
This being the case,
eÏ†
 eadY (X)

=
âˆ
X
m=0
tm
m!
eÏ† ((adY )n (X))
=
âˆ
X
m=0
tm
m!

adeÏ†(Y )
n 
eÏ† (X)

= etadeÏ†(Y ) 
eÏ†(X)

.
Similarly,
eÏ†
 eadXetadY (X)

= eadeÏ†(X)etadeÏ†(Y ) 
eÏ†(X)

.
Assume now that X and Y are small enough that B-C-H applies to X and Y , and
to eÏ†(X) and eÏ†(Y ). Then, using the linearity of the integral and reasoning similar
to the above, we have:
eÏ† log  eXeY  = eÏ†(X) +
Z 1
0
âˆ
X
m=0
am eÏ†
h eadXetadY âˆ’In (X)
i
dt
= eÏ†(X) +
Z 1
0
âˆ
X
m=0
am

eadeÏ†(X)etadeÏ†(Y ) âˆ’I
n
(eÏ†(X)) dt
= log

e
eÏ†(X)e
eÏ†(Y )
.
This is what we wanted to show.
Before coming to the proof Baker-Campbell-Hausdorï¬€formula itself, we will
obtain a result concerning derivatives of the exponential mapping. This result is
valuable in its own right, and will play a central role in our proof of the Baker-
Campbell-Hausdorï¬€formula.
Observe that if X and Y commute, then
eX+tY = eXetY

2. THE GENERAL BAKER-CAMPBELL-HAUSDORFF FORMULA
59
and so
d
dt

t=0 eX+tY = eX
d
dt

t=0 etY = eXY .
In general, X and Y do not commute, and
d
dt

t=0 eX+tY Ì¸= eXY .
This, as it turns out, is an important point. In particular, note that in the language
of multivariate calculus
d
dt

t=0 eX+tY =
 directional derivative of exp at X,
in the direction of Y
.
(4.8)
Thus computing the left side of (4.8) is the same as computing all of the directional
derivatives of the (matrix-valued) function exp. We expect the directional derivative
to be a linear function of Y , for each ï¬xed X.
Now, the function
1 âˆ’eâˆ’z
z
= 1 âˆ’(1 âˆ’z + z2
2! âˆ’Â· Â· Â· )
z
is an entire analytic function of z, even at z = 0, and is given by the power series
1 âˆ’eâˆ’z
z
=
âˆ
X
n=1
(âˆ’1)nâˆ’1 znâˆ’1
n!
= 1 âˆ’z
2! + z2
3! âˆ’Â· Â· Â·.
This series (which has inï¬nite radius of convergence), make sense when z is replaced
by a linear operator A on some ï¬nite-dimensional vector space.
Theorem 4.5 (Derivative of Exponential). Let X and Y be nÃ—n complex ma-
trices. Then
d
dt

t=0
eX+tY = eX
I âˆ’eâˆ’adX
adX
(Y )

= eX

Y âˆ’[X, Y ]
2!
+ [X, [X, Y ]]
3!
âˆ’Â· Â· Â·

.
(4.9)
More generally, if X (t) is a smooth matrix-valued function, then
d
dt

t=0
eX(t) = eX(0)
I âˆ’eâˆ’adX(0)
adX(0)
  dX
dt

t=0

.
(4.10)
Note that the directional derivative in (4.9) is indeed linear in Y for each ï¬xed
X. Note also that (4.9) is just a special case of (4.10), by taking X(t) = X + tY ,
and evaluating at t = 0.
Furthermore, observe that if X and Y commute, then only the ï¬rst term in the
series (4.9) survives. In that case, we obtain
d
dt

t=0 eX+tY = eXY as expected.
Proof. It is possible to prove this Theorem by expanding everything in a
power series and diï¬€erentiating term-by-term; we will not take that approach. We
will prove only form (4.9) of the derivative formula, but the form (4.10) follows by
the chain rule.
Let us use the Lie product formula, and let us assume for the moment that it
is legal to interchange limit and derivative. (We will consider this issue at the end.)
Then we have
eâˆ’X d
dt

t=0
eX+tY = eâˆ’X lim
nâ†’âˆ
d
dt

t=0

eX/netY /nn
.

60
4. THE BAKER-CAMPBELL-HAUSDORFF FORMULA
We now apply the product rule (generalized to n factors) to obtain
eâˆ’X d
dt

t=0
eX+tY = eâˆ’X lim
nâ†’âˆ
nâˆ’1
X
k=0

eX/netY /nnâˆ’kâˆ’1 
eX/netY /nY/n

eX/netY /nk
t=0
= eâˆ’X lim
nâ†’âˆ
nâˆ’1
X
k=0

eX/nnâˆ’kâˆ’1 
eX/nY/n

eX/nk
= lim
nâ†’âˆ
1
n
nâˆ’1
X
k=0

eX/nâˆ’k
Y

eX/nk
.
But

eX/nâˆ’k
Y

eX/nk
=
h
Ad

eâˆ’X/nik
(Y )
=

eâˆ’adX/nk
(Y )
(where we have used the relationship between Ad and ad). So we have
eâˆ’X d
dt

t=0
eX+tY = lim
nâ†’âˆ
1
n
nâˆ’1
X
k=0

eâˆ’adX/nk
(Y ).
(4.11)
Observe now that Pnâˆ’1
k=0
 eâˆ’adX/nk is a geometric series. Let us now reason
for a moment at the purely formal level. Using the usual formula for geometric
series, we get
eâˆ’X d
dt

t=0
eX+tY = lim
nâ†’âˆ
1
n
I âˆ’
 eâˆ’adX/nn
I âˆ’eâˆ’adX/n
(Y )
= lim
nâ†’âˆ
I âˆ’eâˆ’adX
n
h
I âˆ’

I âˆ’adX
n
+ (adX)2
n22!
âˆ’Â· Â· Â·
i(Y )
= lim
nâ†’âˆ
I âˆ’eâˆ’adX
adX âˆ’(adX)2
n2!
+ Â· Â· Â·
(Y )
= I âˆ’eâˆ’adX
adX
(Y ).
This is what we wanted to show!
Does this argument make sense at any rigorous level?
In fact it does.
As
usual, let us consider ï¬rst the diagonalizable case. That is, assume that adX is
diagonalizable as an operator on gl(n; C), and assume that Y is an eigenvector for
adX. This means that adX(Y ) = [X, Y ] = Î»Y , for some Î» âˆˆC. Now, there are
two cases, Î» = 0 and Î» Ì¸= 0. The Î» = 0 case corresponds to the case in which X
and Y commute, and we have already observed that the Theorem holds trivially in
that case.
The interesting case, then, is the case Î» Ì¸= 0. Note that (adX)n (Y ) = Î»nY ,
and so

eâˆ’adX/nk
(Y ) =

eâˆ’Î»/nk
(Y ).
Thus the geometric series in (4.11) becomes an ordinary complex-valued series, with
ratio eâˆ’Î»/n. Since Î» Ì¸= 0, this ratio will be diï¬€erent from one for all suï¬ƒciently

2. THE GENERAL BAKER-CAMPBELL-HAUSDORFF FORMULA
61
large n. Thus we get
eâˆ’X d
dt

t=0
eX+tY =
 
lim
nâ†’âˆ
1
n
I âˆ’
 eâˆ’Î»/nn
I âˆ’eâˆ’Î»/n
!
Y .
There is now no trouble in taking the limit as we did formally above to get
eâˆ’X d
dt

t=0
eX+tY = 1 âˆ’eâˆ’Î»
Î»
Y
= I âˆ’eâˆ’adX
adX
(Y ).
We see then that the Theorem holds in the case that adX is diagonalizable and
Y is an eigenvector of adX. If adX is diagonalizable but Y is not an eigenvector,
then Y is a linear combination of eigenvectors and applying the above computation
to each of those eigenvectors gives the desired result.
We need, then, to consider the case where adX is not diagonalizable. But
(Exercise 20), if X is a diagonalizable matrix, then adX will be diagonalizable
as an operator on gl(n; C). Since, as we have already observed, every matrix is
the limit of diagonalizable matrices, we are essentially done. For it is easy to see
by diï¬€erentiating the power series term-by-term that eâˆ’X
d
dt

t=0 eX+tY exists and
varies continuously with X. Thus once we have the Theorem for all diagonalizable
X we have it for all X by passing to the limit.
The only unresolved issue, then, is the interchange of limit and derivative which
we performed at the very beginning of the argument. I do not want to spell this
out in detail, but let us see what would be involved in justifying this. A standard
theorem in elementary analysis says that if fn(t) â†’f(t) pointwise, and in addition
dfn/dt converges uniformly to some function g(t), then f(t) is diï¬€erentiable and
df/dt = g(t). (E.g., Theorem 7.17 in W. Rudinâ€™s Principles of Mathematical Anal-
ysis.) The key requirement is that the derivatives converge uniformly. Uniform
convergence of the fnâ€™s themselves is deï¬nitely not suï¬ƒcient.
In our case, fn(t) = eâˆ’X  eX/netY /nn. The Lie product formula says that this
converges pointwise to eâˆ’XeX+tY . We need, then, to show that
d
dteâˆ’X 
eX/netY /nn
converges uniformly to some g(t), say on the interval âˆ’1 â‰¤t â‰¤1. This computation
is similar to what we did above, with relatively minor modiï¬cations to account for
the fact that we do not take t = 0 and to make sure the convergence is uniform.
This part of the proof is left as an exercise to the reader.
2.1. Proof of the Baker-Campbell-Hausdorï¬€Formula. We now turn to
the proof of the Baker-Campbell-Hausdorï¬€formula itself. Our argument follows
Miller, Sec. 5.1, with minor diï¬€erences of convention. (Warning: Millerâ€™s â€œAdâ€ is
what we call â€œad.â€) Deï¬ne
Z(t) = log
 eXetY 
If X and Y are suï¬ƒciently small, then Z (t) is deï¬ned for 0 â‰¤t â‰¤1. It is left as an
exercise to verify that Z(t) is smooth. Our goal is to compute Z(1).
By deï¬nition
eZ(t) = eXetY

62
4. THE BAKER-CAMPBELL-HAUSDORFF FORMULA
so that
eâˆ’Z(t) d
dteZ(t) =  eXetY âˆ’1 eXetY Y = Y .
On the other hand, by Theorem 4.5,
eâˆ’Z(t) d
dteZ(t) =
I âˆ’eâˆ’adZ(t)
adZ(t)
 dZ
dt

.
Hence
I âˆ’eâˆ’adZ(t)
adZ(t)
 dZ
dt

= Y .
If X and Y are small enough, then Z(t) will also be small, so that
 I âˆ’eâˆ’adZ(t)
/adZ(t)
will be close to the identity and thus invertible. So
dZ
dt =
I âˆ’eâˆ’adZ(t)
adZ(t)
âˆ’1
(Y ).
(4.12)
Recall that eZ(t) = eXetY . Applying the homomorphism â€˜Adâ€™ gives
Ad

eZ(t)
= Ad  eX Ad  etY  .
By the relationship (3.17) between â€˜Adâ€™ and â€˜ad,â€™ this becomes
eadZ(t) = eadXetadY
or
adZ(t) = log
 eadXetadY 
.
Plugging this into (4.12) gives
dZ
dt =
(
I âˆ’
 eadXetadY âˆ’1
log(eadXetadY )
)âˆ’1
(Y ).
(4.13)
But now observe that
g(z) =
1 âˆ’zâˆ’1
logz
âˆ’1
so, formally, (4.13) is the same as
dZ
dt = g  eadXetadY  (Y ).
(4.14)
Reasoning as in the proof of Theorem 4.5 shows easily that this formal argument
is actually correct.
Now we are essentially done, for if we note that Z(0) = X and integrate (4.14),
we get
Z(1) = X +
Z 1
0
g(eadXetadY )(Y ) dt
which is the Baker-Campbell-Hausdorï¬€formula.

3. THE SERIES FORM OF THE BAKER-CAMPBELL-HAUSDORFF FORMULA
63
3. The Series Form of the Baker-Campbell-Hausdorï¬€Formula
Let us see how to get the ï¬rst few terms of the series form of B-C-H from the
integral form. Recall the function
g (z) = z logz
z âˆ’1
=
[1 + (z âˆ’1)]
h
(z âˆ’1) âˆ’(zâˆ’1)2
2
+ (zâˆ’1)3
3
Â· Â· Â·
i
(z âˆ’1)
= [1 + (z âˆ’1)]
"
1 âˆ’z âˆ’1
2
+ (z âˆ’1)2
3
#
.
Multiplying this out and combining terms gives
g (z) = 1 + 1
2 (z âˆ’1) âˆ’1
6 (z âˆ’1)2 + Â· Â· Â· .
The closed-form expression for g is
g (z) = 1 +
âˆ
X
n=1
(âˆ’1)n+1
n (n + 1) (z âˆ’1)n .
Meanwhile
eadXetadY âˆ’I =
 
I + adX + (adX)2
2
+ Â· Â· Â·
! 
I + tadY + t2 (adY )2
2
+ Â· Â· Â·
!
âˆ’I
= adX + tadY + tadXadY + (adX)2
2
+ t2 (adY )2
2
+ Â· Â· Â·.
The crucial observation here is that eadXetadY âˆ’I has no zero-order term, just
ï¬rst-order and higher in adX/adY . Thus
 eadXetadY âˆ’I
n will contribute only
terms of degree n or higher in adX/adY .
We have, then, up to degree two in adX/adY
g
 eadXetadY 
= I + 1
2
"
adX + tadY + tadXadY + (adX)2
2
+ t2 (adY )2
2
+ Â· Â· Â·
#
âˆ’1
6 [adX + tadY + Â· Â· Â·]2
= I + 1
2adX + t
2adY + t
2adXadY + (adX)2
4
+ t2 (adY )2
4
âˆ’1
6
h
(adX)2 + t2 (adY )2 + tadXadY + tadY adX
i
+ higher-order terms.
We now to apply g  eadXetadY  to Y and integrate. So (neglecting higher-order
terms) by B-C-H, and noting that any term with adY acting ï¬rst is zero:
log
 eXeY 
= X +
Z 1
0

Y + 1
2 [X, Y ] + 1
4 [X, [X, Y ]] âˆ’1
6 [X, [X, Y ]] âˆ’t
6 [Y, [X, Y ]]

dt
= X + Y + 1
2 [X, Y ] +
1
4 âˆ’1
6

[X, [X, Y ]] âˆ’1
6
Z 1
0
t dt [Y, [X, Y ]] .

64
4. THE BAKER-CAMPBELL-HAUSDORFF FORMULA
Thus if we do the algebra we end up with
log  eXeY  = X + Y + 1
2 [X, Y ] + 1
12 [X, [X, Y ]] âˆ’1
12 [Y, [X, Y ]]
+ higher order terms.
This is the expression in (4.2).
4. Subgroups and Subalgebras
Suppose that G is a matrix Lie group, H another matrix Lie group, and suppose
that H âŠ‚G. Then certainly the Lie algebra h of H will be a subalgebra of the Lie
algebra g of G. Does this go the other way around? That is given a Lie group G
with Lie algebra g, and a subalgebra h of g, is there a matrix Lie group H whose
Lie algebra is h?
In the case of the Heisenberg group, the answer is yes. This is easily seen using
the fact that the exponential mapping is one-to-one and onto, together with the
special form of the Baker-Campbell-Hausdorï¬€formula. (See Exercise 6.)
Unfortunately, the answer in general is no. For example, let G = GL (2; C) and
let
h =
 it
0
0
ita
 t âˆˆR

,
where a is irrational. If there is going to be a matrix Lie group H with Lie algebra
h, then H would contain the set
H0 =
 eit
0
0
eita
 t âˆˆR

.
To be a matrix Lie group, H would have to be closed in GL (2; C), and so it would
contain the closure of H0, which (see ) is the set
H1 =
 eit
0
0
eis
s, t âˆˆR

.
But then the Lie algebra of H would have to contain the Lie algebra of H1, which
is two-dimensional!
Fortunately, all is not lost. We can still get a subgroup H for each subalgebra
h, if we weaken the condition that H be a matrix Lie group. In the above example,
the subgroup we want is H0, despite the fact that H0 is not a matrix Lie group.
Definition 4.6. If H is any subgroup of GL (n; C), deï¬ne the Lie algebra h of
H to be the set of all matrices X such that
etX âˆˆH
for all real t.
Definition 4.7. If G is a matrix Lie group with Lie algebra g, then H is a
connected Lie subgroup of G if
i) H is a subgroup of G
ii) H is connected
iii) the Lie algebra h of H is a subspace of g
iv) Every element of H can be written in the form eX1eX2 Â· Â· Â· eXn, with X1, Â· Â· Â· , Xn âˆˆ
h.

5. EXERCISES
65
Theorem 4.8. If G is a matrix Lie group with Lie algebra g, and H is a
connected Lie subgroup of G, then the Lie algebra h of H is a subalgebra of g.
Proof. Since by deï¬nition h is a subspace of g, it remains only to show that
h is closed under brackets. So assume X, Y âˆˆh. Then etX and esY are in H, and
so (since H is a subgroup) is the element
etXesY eâˆ’tX = exp s  etXY eâˆ’tX .
This shows that etXY eâˆ’tX is in h for all t. But h is a subspace of g, which is
necessarily a closed subset of g. Thus
[X, Y ] = d
dt

t=0
etXY eâˆ’tX = lim
hâ†’0
 ehXY eâˆ’hX âˆ’Y

h
is in h. (This argument is precisely the one we used to show that the Lie algebra
of a matrix Lie group is a closed under brackets, once we had established that it is
a subspace.)
We are now ready to state the main theorem of this section, which is our second
major application of the Baker-Campbell-Hausdorï¬€formula.
Theorem 4.9. Let G be a matrix Lie group with Lie algebra g. Let h be a Lie
subalgebra of g. Then there exists a unique connected Lie subgroup H of G such
that the Lie algebra of H is h.
Given a matrix Lie group G and a subalgebra h of g, the associated connected
Lie subgroup H might be a matrix Lie group.
This will happen precisely if H
is a closed subset of G. There are various conditions under which you can prove
that H is closed. For example, if G = GL (n; C), and h is semisimple, then H is
automatically closed, and hence a matrix Lie group. (See Helgason, Chapter II,
Exercises and Further Results, D.)
If only the Baker-Campbell-Hausdorï¬€formula worked globally instead of only
locally the proof of this theorem would be easy. If the B-C-H formula converged
for all X, Y we could just deï¬ne H to be the image of h under the exponential
mapping. In that case B-C-H would show that this image is a subgroup, since then
we would have eH1eH2 = eZ, with Z = H1 + H2 + 1
2 [H1, H2] + Â· Â· Â· âˆˆh provided
that H1, H2 âˆˆh. Unfortunately, the B-C-H formula is not convergent in general,
and in general the image of H under the exponential mapping is not a subgroup.
Proof. Not written at this time.
5. Exercises
1. The center of a Lie algebra g is deï¬ned to be the set of all X âˆˆg such that
[X, Y ] = 0 for all Y âˆˆg. Now consider the Heisenberg group
H =
ï£±
ï£²
ï£³
ï£«
ï£­
1
a
b
0
1
c
0
0
1
ï£¶
ï£¸|a, b, c âˆˆR
ï£¼
ï£½
ï£¾
with Lie algebra
h =
ï£±
ï£²
ï£³
ï£«
ï£­
0
Î±
Î²
0
0
Î³
0
0
0
ï£¶
ï£¸|Î±, Î², Î³ âˆˆR
ï£¼
ï£½
ï£¾.

66
4. THE BAKER-CAMPBELL-HAUSDORFF FORMULA
Determine the center Z(h) of h.
For any X, Y âˆˆh, show that [X, Y ] âˆˆ
Z(h). This implies, in particular that both X and Y commute with their
commutator [X, Y ].
Show by direct computation that for any X, Y âˆˆh,
eXeY = eX+Y + 1
2 [X,Y ].
(4.15)
2. Let X be a n Ã— n complex matrix. Show that
I âˆ’eâˆ’X
X
is invertible if and only if X has no eigenvalue of the form Î» = 2Ï€in, with n
an non-zero integer.
Hint: When is (1 âˆ’eâˆ’z) /z equal to zero?
Remark: This exercise, combined with the formula in Theorem 4.5,
gives the following result (in the language of diï¬€erentiable manifolds): The
exponential mapping exp : g â†’G is a local diï¬€eomorphism near X âˆˆg if
and only adX has no eigenvalue of the form Î» = 2Ï€in, with n a non-zero
integer.
3. Verify that the right side of the Baker-Campbell-Hausdorï¬€formula (4.6)
reduces to X + Y in the case that X and Y commute.
Hint: Compute ï¬rst eadXetadY (Y ) and
 eadXetadY âˆ’I

(Y ).
4. Compute log
 eXeY 
through third order in X/Y by using the power series
for the exponential and the logarithm. Show that you get the same answer
as the Baker-Campbell-Hausdorï¬€formula.
5. Using the techniques in Section 3, compute the series form of the Baker-
Campbell-Hausdorï¬€formula up through fourth-order brackets. (We have
already computed up through third-order brackets.)
6. Let a be a subalgebra of the Lie algebra of the Heisenberg group. Show that
exp (a) is a connected Lie subgroup of the Heisenberg group. Show that in
fact exp (a) is a matrix Lie group.
7. Show that every connected Lie subgroup of SU (2) is closed. Show that this
is not the case for SU (3).

CHAPTER 5
Basic Representation Theory
1. Representations
Definition 5.1. Let G be a matrix Lie group. Then a ï¬nite-dimensional
complex representation of G is a Lie group homomorphism
Î  : G â†’GL(n; C)
(n â‰¥1) or more generally a Lie group homomorphism
Î  : G â†’GL(V )
where V is a ï¬nite-dimensional complex vector space (with dim(V ) â‰¥1). A ï¬nite-
dimensional real representation of G is a Lie group homomorphism Î  of G
into GL(n; R) or into GL(V ), where V is a ï¬nite-dimensional real vector space.
If g is a real or complex Lie algebra, then a ï¬nite-dimensional complex
representation of g is a Lie algebra homomorphism Ï€ of g into gl(n; C) or into
gl(V ), where V is a ï¬nite-dimensional complex vector space.
If g is a real Lie
algebra, then a ï¬nite-dimensional real representation of g is a Lie algebra
homomorphism Ï€ of g into gl(n; R) or into gl(V ).
If Î  or Ï€ is a one-to-one homomorphism, then the representation is called
faithful.
You should think of a representation as a (linear) action of a group or Lie
algebra on a vector space.
(Since, say, to every g âˆˆG there is associated an
operator Î (g), which acts on the vector space V .) In fact, we will use terminology
such as, â€œLet Î  be a representation of G acting on the space V .â€ Even if g is a real
Lie algebra, we will consider mainly complex representations of g. After making a
few more deï¬nitions, we will discuss the question of why one should be interested
in studying representations.
Definition 5.2. Let Î  be a ï¬nite-dimensional real or complex representation
of a matrix Lie group G, acting on a space V .
A subspace W of V is called
invariant if Î (A)w âˆˆW for all w âˆˆW and all A âˆˆG. An invariant subspace W
is called non-trivial if W Ì¸= {0} and W Ì¸= V . A representation with no non-trivial
invariant subspaces is called irreducible.
The terms invariant, non-trivial, and irreducible are deï¬ned analogously
for representations of Lie algebras.
Definition 5.3. Let G be a matrix Lie group, let Î  be a representation of
G acting on the space V , and let Î£ be a representation of G acting on the space
W. A linear map Ï† : V â†’W is called a morphism (or intertwining map) of
representations if
Ï†(Î (A)v) = Î£(A)Ï†(v)
67

68
5. BASIC REPRESENTATION THEORY
for all A âˆˆG and all v âˆˆV . The analogous property deï¬nes morphisms of repre-
sentations of a Lie algebra.
If Ï† is a morphism of representations, and in addition Ï† is invertible, then
Ï† is said to be an isomorphism of representations. If there exists an isomor-
phism between V and W, then the representations are said to be isomorphic (or
equivalent).
Two isomorphic representations should be regarded as being â€œthe sameâ€ rep-
resentation.
A typical problem in representation theory is to determine, up to
isomorphism, all the irreducible representations of a particular group or Lie alge-
bra. In Section 5.4 we will determine all the ï¬nite-dimensional complex irreducible
representations of the Lie algebra su(2).
Proposition 5.4. Let G be a matrix Lie group with Lie algebra g, and let Î 
be a (ï¬nite-dimensional real or complex) representation of G, acting on the space
V . Then there is a unique representation Ï€ of g acting on the same space such that
Î (eX) = eÏ€(X)
for all X âˆˆg. The representation Ï€ can be computed as
Ï€(X) = d
dt

t=0
Î 
 etX
and satisï¬es
Ï€
 AXAâˆ’1
= Î (A)Ï€(X)Î (A)âˆ’1
for all X âˆˆg and all A âˆˆG.
Proof. Theorem 3.18 in Chapter 3 states that for each Lie group homomor-
phism Ï† : G â†’H there is an associated Lie algebra homomorphism eÏ† : g â†’h.
Take H = GL(V ) and Ï† = Î . Since the Lie algebra of GL(V ) is gl(V ) (since the ex-
ponential of any operator is invertible), the associated Lie algebra homomorphism
eÏ† = Ï€ maps from g to gl(V ), and so constitutes a representation of g.
The properties of Ï€ follow from the properties of eÏ† given in Theorem 6.
Proposition 5.5. Let g be a real Lie algebra, and gC its complexiï¬cation.
Then every ï¬nite-dimensional complex representation Ï€ of g has a unique extension
to a (complex-linear) representation of gC, also denoted Ï€. The representation of
gC satisï¬es
Ï€(X + iY ) = Ï€(X) + iÏ€(Y )
for all X âˆˆg.
Proof. This follows from Exercise 14 of Chapter 3.
Definition 5.6. Let G be a matrix Lie group, let H be a Hilbert space, and
let U(H) denote the group of unitary operators on H.
Then a homomorphism
Î  : G â†’U(H) is called a unitary representation of G if Î  satisï¬es the following
continuity condition: If An, A âˆˆG and An â†’A, then
Î (An)v â†’Î (A)v
for all v âˆˆH. A unitary representation with no non-trivial closed invariant sub-
spaces is called irreducible.

2. WHY STUDY REPRESENTATIONS?
69
This continuity condition is called strong continuity. One could require the
even stronger condition that âˆ¥Î (An) âˆ’Î (A)âˆ¥â†’0, but this turns out to be too
stringent a requirement. (That is, most of the interesting representations of G will
not have this stronger continuity condition.) In practice, any homomorphism of G
into U(H) you can write down explicitly will be strongly continuous.
One could try to deï¬ne some analog of unitary representations for Lie alge-
bras, but there are serious technical diï¬ƒculties associated with getting the â€œrightâ€
deï¬nition.
2. Why Study Representations?
If a representation Î  is a faithful representation of a matrix Lie group G, then
{Î (A) |A âˆˆG} is a group of matrices which is isomorphic to the original group G.
Thus Î  allows us to represent G as a group of matrices. This is the motivation for
the term representation. (Of course, we still call Î  a representation even if it is not
faithful.)
Despite the origin of the term, the point of representation theory is not (at
least in this course) to represent a group as a group of matrices. After all, all of
our groups are already matrix groups! While it might seem redundant to study
representations of a group which is already represented as a group of matrices, this
is precisely what we are going to do.
The reason for this is that a representation can be thought of (as we have
already noted) as an action of our group on some vector space. Such actions (rep-
resentations) arise naturally in many branches of both mathematics and physics,
and it is important to understand them.
A typical example would be a diï¬€erential equation in three-dimensional space
which has rotational symmetry. If the equation has rotational symmetry, then the
space of solutions will be invariant under rotations. Thus the space of solutions will
constitute a representation of the rotation group SO(3). If you know what all of the
representations of SO(3) are, this can help immensely in narrowing down what the
space of solutions can be. (As we will see, SO(3) has lots of other representations
besides the obvious one in which SO(3) acts on R3.)
In fact, one of the chief applications of representation theory is to exploit sym-
metry. If a system has symmetry, then the set of symmetries will form a group, and
understanding the representations of the symmetry group allows you to use that
symmetry to simplify the problem.
In addition, studying the representations of a group G (or of a Lie algebra g)
can give information about the group (or Lie algebra) itself. For example, if G
is a ï¬nite group, then associated to G is something called the group algebra.
The structure of this group algebra can be described very nicely in terms of the
irreducible representations of G.
In this course, we will be interested primarily in computing the ï¬nite-dimensional
irreducible complex representations of matrix Lie groups.
As we shall see, this
problem can be reduced almost completely to the problem of computing the ï¬nite-
dimensional irreducible complex representations of the associated Lie algebra. In
this chapter, we will discuss the theory at an elementary level, and will consider in
detail the example of SO(3) and SU(2). In Chapter 6, we will study the represen-
tations of SU(3), which is substantially more involved than that of SU(2), and give

70
5. BASIC REPRESENTATION THEORY
an overview of the representation theory of a very important class of Lie groups,
namely, the semisimple ones.
3. Examples of Representations
3.1. The Standard Representation. A matrix Lie group G is by deï¬nition
a subset of some GL(n; R) or GL(n; C). The inclusion map of G into GL(n) (i.e.,
Î (A) = A) is a representation of G, called the standard representation of G.
Thus for example the standard representation of SO(3) is the one in which SO(3)
acts in the usual way on R3. If G is a subgroup of GL(n; R) or GL(n; C), then its
Lie algebra g will be a subalgebra of gl(n; R) or gl(n; C). The inclusion of g into
gl(n; R) or gl(n; C) is a representation of g, called the standard representation.
3.2. The Trivial Representation. Consider the one-dimensional complex
vector space C. Given any matrix Lie group G, we can deï¬ne the trivial repre-
sentation of G, Î  : G â†’GL(1; C), by the formula
Î (A) = I
for all A âˆˆG. Of course, this is an irreducible representation, since C has no non-
trivial subspaces, let alone non-trivial invariant subspaces. If g is a Lie algebra, we
can also deï¬ne the trivial representation of g, Ï€ : g â†’gl(1; C), by
Ï€(X) = 0
for all X âˆˆg. This is an irreducible representation.
3.3. The Adjoint Representation. Let G be a matrix Lie group with Lie
algebra g. We have already deï¬ned the adjoint mapping
Ad : G â†’GL(g)
by the formula
AdA(X) = AXAâˆ’1.
Recall that Ad is a Lie group homomorphism. Since Ad is a Lie group homomor-
phism into a group of invertible operators, we see that in fact Ad is a representation
of G, acting on the space g. Thus we can now give Ad its proper name, the adjoint
representation of G. The adjoint representation is a real representation of G.
Similarly, if g is a Lie algebra, we have
ad : g â†’gl(g)
deï¬ned by the formula
adX(Y ) = [X, Y ].
We know that ad is a Lie algebra homomorphism (Chapter 3, Proposition 3.33), and
is therefore a representation of g, called the adjoint representation. In the case
that g is the Lie algebra of some matrix Lie group G, we have already established
(Chapter 3, Proposition 3.21 and Exercise 13) that Ad and ad are related as in
Proposition 5.4.
Note that in the case of SO(3) the standard representation and the adjoint
representation are both three dimensional real representations. In fact these two
representations are equivalent (Exercise 4).

3. EXAMPLES OF REPRESENTATIONS
71
3.4. Some Representations of SU(2). Consider the space Vm of homoge-
neous polynomials in two complex variables with total degree m (m â‰¥0). That is,
Vm is the space of functions of the form
f(z1, z2) = a0zm
1 + a1zmâˆ’1
1
z2 + a2zmâˆ’2
1
z2
2 Â· Â· Â· + amzm
2
(5.1)
with z1, z2 âˆˆC and the aiâ€™s arbitrary complex constants.
The space Vm is an
(m + 1)-dimensional complex vector space.
Now by deï¬nition an element U of SU(2) is a linear transformation of C2. Let
z denote the pair z = (z1, z2) in C2. Then we may deï¬ne a linear transformation
Î m(U) on the space Vm by the formula
[Î m(U)f] (z) = f(U âˆ’1z).
(5.2)
Explicitly, if f is as in (5.1), then
[Î m(U)f] (z1, z2) =
m
X
k=0
ak
 U âˆ’1
11 z1 + U âˆ’1
12 z2
mâˆ’k  U âˆ’1
21 z1 + U âˆ’1
22 z2
k .
By expanding out the right side of this formula we see that Î m(U)f is again a
homogeneous polynomial of degree m. Thus Î m(U) actually maps Vm into Vm.
Now, compute
Î m (U1) [Î m (U2) f] (z) = [Î m (U2) f] (U âˆ’1
1
z) = f
 U âˆ’1
2
U âˆ’1
1 z

= Î m (U1U2) f(z).
Thus Î m is a (ï¬nite-dimensional complex) representation of SU(2).
(It is very
easy to do the above computation incorrectly.)
The inverse in deï¬nition (5.2)
is necessary in order to make Î m a representation.
It turns out that each of
the representations Î m of SU(2) is irreducible, and that every ï¬nite-dimensional
irreducible representation of SU(2) is equivalent to one (and only one) of the Î mâ€™s.
(Of course, no two of the Î mâ€™s are equivalent, since they donâ€™t even have the same
dimension.)
Let us now compute the corresponding Lie algebra representation Ï€m. Accord-
ing to Proposition 5.4, Ï€m can be computed as
Ï€m(X) = d
dt

t=0
Î m
 etX
.
So
(Ï€m(X)f) (z) = d
dt

t=0
f
 eâˆ’tXz

.
Now let z(t) be the curve in C2 deï¬ned as z(t) = eâˆ’tXz, so that z(0) = z. Of
course, z(t) can be written as z(t) = (z1(t), z2(t)), with zi(t) âˆˆC. By the chain
rule,
Ï€m(X)f = âˆ‚f
âˆ‚z1
dz1
dt

t=0
+ âˆ‚f
âˆ‚z2
dz2
dt

t=0
.
But dz/dt|t=0 = âˆ’Xz, so we obtain the following formula for Ï€m(X)
Ï€m(X)f = âˆ’âˆ‚f
âˆ‚z1
(X11z1 + X12z2) âˆ’âˆ‚f
âˆ‚z2
(X21z1 + X22z2) .
(5.3)
Now, according to Proposition 5.5, every ï¬nite-dimensional complex represen-
tation of the Lie algebra su(2) extends uniquely to a complex-linear representation

72
5. BASIC REPRESENTATION THEORY
of the complexiï¬cation of su(2). But the complexiï¬cation of su(2) is (isomorphic
to) sl(2; C) (Chapter 3, Proposition 3.36). To see that this is so, note that sl(2; C)
is the space of all 2 Ã— 2 complex matrices with trace zero. But if X is in sl(2; C),
then
X = X âˆ’Xâˆ—
2
+ X + Xâˆ—
2
= X âˆ’Xâˆ—
2
+ iX + Xâˆ—
2i
where both (Xâˆ’Xâˆ—)/2 and (X+Xâˆ—)/2i are in su(2). (Check!) It is easy to see that
this decomposition is unique, so that every X âˆˆsl(2; C) can be written uniquely as
X = X1 + iY1 with X1, Y1 âˆˆsu(2). Thus sl(2; C) is isomorphic as a vector space to
su(2)C. But this is in fact an isomorphism of Lie algebras, since in both cases
[X1 + iY1, X2 + iY2] = [X1, X2] âˆ’[Y1, Y2] + i ([X1, Y2] + [X2, Y1]) .
(See Exercise 5.)
So, the representation Ï€m of su(2) given by (5.3) extends to a representation
of sl(2; C), which we will also call Ï€m. I assert that in fact formula (5.3), still holds
for X âˆˆsl(2; C). Why is this? Well, (5.3) is undoubtedly (complex) linear, and it
agrees with the original Ï€m for X âˆˆsu(2). But there is only one complex linear
extension of Ï€m from su(2) to sl(2; C), so this must be it!
So, for example, consider the element
H =
 1
0
0
âˆ’1

in the Lie algebra sl(2; C). Applying formula (5.3) gives
(Ï€m(H)f) (z) = âˆ’âˆ‚f
âˆ‚z1
z1 + âˆ‚f
âˆ‚z2
z2.
Thus we see that
Ï€m(H) = âˆ’z1
âˆ‚
âˆ‚z1
+ z2
âˆ‚
âˆ‚z2
.
(5.4)
Applying Ï€m(H) to a basis element zk
1zmâˆ’k
2
we get
Ï€m(H)zk
1zmâˆ’k
2
= âˆ’kzk
1zmâˆ’k
2
+ (m âˆ’k)zk
1 zmâˆ’k
2
= (m âˆ’2k)zk
1zmâˆ’k
2
.
Thus zk
1 zmâˆ’k
2
is an eigenvector for Ï€m(H) with eigenvalue (m âˆ’2k). In particular,
Ï€m(H) is diagonalizable.
Let X and Y be the elements
X =
 0
1
0
0

;
Y =
 0
0
1
0

in sl(2; C). Then (5.3) tells us that
Ï€m(X) = âˆ’z2
âˆ‚
âˆ‚z1 ;
Ï€m(Y ) = âˆ’z1
âˆ‚
âˆ‚z2
so that
Ï€m(X)zk
1 zmâˆ’k
2
= âˆ’kzkâˆ’1
1
zmâˆ’k+1
2
Ï€m(Y )zk
1 zmâˆ’k
2
= (k âˆ’m)zk+1
1
zmâˆ’kâˆ’1
2
.
(5.5)
Proposition 5.7. The representation Ï€m is an irreducible representation of
sl(2; C).

3. EXAMPLES OF REPRESENTATIONS
73
Proof. It suï¬ƒces to show that every non-zero invariant subspace of Vm is in
fact equal to Vm. So let W be such a space. Since W is assumed non-zero, there is
at least one non-zero element w in W. Then w can be written uniquely in the form
w = a0zm
1 + a1zmâˆ’1
1
z2 + a2zmâˆ’2
1
z2
2 Â· Â· Â· + amzm
2
with at least one of the akâ€™s non-zero. Let k0 be the largest value of k for which
ak Ì¸= 0, and consider
Ï€m(X)k0w.
Since (by (5.5)) each application of Ï€m(X) lowers the power of z1 by 1, Ï€m(X)k0
will kill all the terms in w whose power of z1 is less than k0, that is, all except the
ak0zk0
1 zmâˆ’k0
2
term. On the other hand, we compute easily that
Ï€m(X)k0 
ak0zk0
1 zmâˆ’k0
2

= k0!(âˆ’1)k0ak0zm
2 .
We see, then, that Ï€m(X)k0w is a non-zero multiple of zm
2 . Since W is assumed
invariant, W must contain this multiple of zm
2 , and so also zm
2 itself.
But now it follows from (5.5) that Ï€m(Y )kzm
2 is a non-zero multiple of zk
1 zmâˆ’k
2
.
Therefore W must also contain zk
1 zmâˆ’k
2
for all 0 â‰¤k â‰¤m. Since these elements
form a basis for Vm, we see that in fact W = Vm, as desired.
3.5. Two Unitary Representations of SO(3). Let H = L2(R3, dx). For
each R âˆˆSO(3), deï¬ne an operator Î 1(R) on H by the formula
[Î 1(R)f] (x) = f  Râˆ’1x .
Since Lebesgue measure dx is rotationally invariant, Î 1(R) is a unitary operator
for each R âˆˆSO(3). The calculation of the previous subsection shows that the
map R â†’Î 1(R) is a homomorphism of SO(3) into U(H). This map is strongly
continuous, and hence constitutes a unitary representation of SO(3).
Similarly, we may consider the unit sphere S2 âŠ‚R3, with the usual surface
measure â„¦. Of course, any R âˆˆSO(3) maps S2 into S2. For each R we can deï¬ne
Î 2(R) acting on L2(S2, dâ„¦) by
[Î 2(R)f] (x) = f
 Râˆ’1x

.
Then Î 2 is a unitary representation of SO(3).
Neither of the unitary representations Î 1 and Î 2 is irreducible. In the case of
Î 2, L2(S2, dâ„¦) has a very nice decomposition as the orthogonal direct sum of ï¬nite-
dimensional invariant subspaces.
This decomposition is the theory of â€œspherical
harmonics,â€ which are well known in the physics (and mathematics) literature.
3.6. A Unitary Representation of the Reals. Let H = L2(R, dx). For
each a âˆˆR, deï¬ne Ta : H â†’H by
(Taf) (x) = f(x âˆ’a).
Clearly Ta is a unitary operator for each a âˆˆR, and clearly TaTb = Ta+b. The
map a â†’Ta is strongly continuous, so T is a unitary representation of R. This
representation is not irreducible. The theory of the Fourier transform allows you
to determine all the closed, invariant subspaces of H (W. Rudin, Real and Complex
Analysis, Theorem 9.17).

74
5. BASIC REPRESENTATION THEORY
3.7. The Unitary Representations of the Real Heisenberg Group.
Consider the Heisenberg group
H =
ï£±
ï£²
ï£³
ï£«
ï£­
1
a
b
0
1
c
0
0
1
ï£¶
ï£¸|a, b, c âˆˆR
ï£¼
ï£½
ï£¾.
Now consider a real, non-zero constant, which for reasons of historical convention
we will call â„(â€œaitch-barâ€). Now for each â„âˆˆR\{0}, deï¬ne a unitary operator Î â„
on L2(R, dx) by
Î â„
ï£«
ï£­
1
a
b
0
1
c
0
0
1
ï£¶
ï£¸f = eâˆ’iâ„beiâ„cxf(x âˆ’a).
(5.6)
It is clear that the right side of (5.6) has the same norm as f, so Î â„is indeed
unitary.
Now compute
Î â„
ï£«
ï£­
1
ea
eb
0
1
ec
0
0
1
ï£¶
ï£¸Î â„
ï£«
ï£­
1
a
b
0
1
c
0
0
1
ï£¶
ï£¸f
= eâˆ’iâ„ebeiâ„ecxeâˆ’iâ„beiâ„c(xâˆ’ea)f(x âˆ’ea âˆ’a)
= eâˆ’iâ„(eb+b+cea)eiâ„(ec+c)xf (x âˆ’(ea + a)) .
This shows that the map A â†’Î â„(A) is a homomorphism of the Heisenberg group
into U  L2(R). This map is strongly continuous, and so Î â„is a unitary represen-
tation of H.
Note that a typical unitary operator Î â„(A) consists of ï¬rst translating f, then
multiplying f by the function eiâ„cx, and then multiplying f by the constant eâˆ’iâ„b.
Multiplying f by the function eiâ„cx has the eï¬€ect of translating the Fourier trans-
form of f, or in physical language, â€œtranslating f in momentum space.â€
Now,
if U1 is an ordinary translation and U2 is a translation of the Fourier transform
(i.e., U2 = multiplication by some eiâ„cx), then U1 and U2 will not commute, but
U1U2U âˆ’1
1 U âˆ’1
2
will be simply multiplication by a constant of absolute value one.
Thus {Î â„(A) |A âˆˆH } is the group of operators on L2(R) generated by ordinary
translations and translations in Fourier space. It is this representation of the Heisen-
berg group which motivates its name. (See also Exercise 10.)
It follows fairly easily from standard Fourier transform theory (e.g., W. Rudin,
Real and Complex Analysis, Theorem 9.17) that for each â„âˆˆR\{0} the repre-
sentation Î â„is irreducible. Furthermore, these are (up to equivalence) almost all
of the irreducible unitary representations of H. The only remaining ones are the
one-dimensional representations Î Î±,Î²
Î Î±,Î²
ï£«
ï£­
1
a
b
0
1
c
0
0
1
ï£¶
ï£¸= ei(Î±a+Î²c)I
with Î±, Î² âˆˆR. (The Î Î±,Î²â€™s are the irreducible unitary representations in which
the center of H acts trivially.) The fact that Î â„â€™s and the Î Î±,Î²â€™s are all of the
(strongly continuous) irreducible unitary representations of H is closely related
to the celebrated Stone-Von Neumann theorem in mathematical physics. See, for

4. THE IRREDUCIBLE REPRESENTATIONS OF su(2)
75
example, M. Reed and B. Simon, Methods of Modern Mathematical Physics, Vol.
3, Theorem XI.84. See also Exercise 11.
4. The Irreducible Representations of su(2)
In this section we will compute (up to equivalence) all the ï¬nite-dimensional
irreducible complex representations of the Lie algebra su(2). This computation is
important for several reasons.
In the ï¬rst place, su(2) âˆ¼= so(3), and the repre-
sentations of so(3) are of physical signiï¬cance. (The computation we will do here
is found in every standard textbook on quantum mechanics, under the heading
â€œangular momentum.â€) In the second place, the representation theory of su(2) is
an illuminating example of how one uses commutation relations to determine the
representations of a Lie algebra. In the third place, in determining the represen-
tations of general semisimple Lie algebras (Chapter 6), we will explicitly use the
representation theory of su(2).
Now, every ï¬nite-dimensional complex representation Ï€ of su(2) extends by
Prop. 5.5 to a complex-linear representation (also called Ï€) of the complexiï¬cation
of su(2), namely sl(2; C).
Proposition 5.8. Let Ï€ be a complex representation of su(2), extended to a
complex-linear representation of sl(2; C). Then Ï€ is irreducible as a representation
of su(2) if and only if it is irreducible as a representation of sl(2; C).
Proof. Let us make sure we are clear about what this means. Suppose that
Ï€ is a complex representation of the (real) Lie algebra su(2), acting on the com-
plex space V . Then saying that Ï€ is irreducible means that there is no non-trivial
invariant complex subspace W âŠ‚V . That is, even though su(2) is a real Lie alge-
bra, when considering complex representations we are interested only in complex
invariant subspaces.
Now, suppose that Ï€ is irreducible as a representation of su(2).
If W is a
(complex) subspace of V which is invariant under sl(2; C), then certainly W is
invariant under su(2) âŠ‚sl(2; C).
Therefore W = {0} or W = V .
Thus Ï€ is
irreducible as a representation of sl(2; C).
On the other hand, suppose that Ï€ is irreducible as a representation of sl(2; C),
and suppose that W is a (complex) subspace of V which is invariant under su(2).
Then W will also be invariant under Ï€(X+iY ) = Ï€(X)+iÏ€(Y ), for all X, Y âˆˆsu(2).
Since every element of sl(2; C) can be written as X + iY , we conclude that in fact
W is invariant under sl(2; C). Thus W = {0} or W = V , so Ï€ is irreducible as a
representation of su(2).
We see, then that studying the irreducible representations of su(2) is equivalent
to studying the irreducible representations of sl(2; C). Passing to the complexiï¬ed
Lie algebra makes our computations easier.
We will use the following basis for sl(2; C):
H =
 1
0
0
âˆ’1

;
X =
 0
1
0
0

;
Y =
 0
0
1
0

which have the commutation relations
[H, X]
=
2X
[H, Y ]
=
âˆ’2Y
[X, Y ]
=
H
.

76
5. BASIC REPRESENTATION THEORY
If V is a (ï¬nite-dimensional complex) vector space, and A, B, and C are operators
on V satisfying
[A, B]
=
2B
[A, C]
=
âˆ’2C
[B, C]
=
A
then because of the skew-symmetry and bilinearity of brackets, the linear map
Ï€ : sl(2; C) â†’gl(V ) satisfying
Ï€(H) = A;
Ï€(X) = B;
Ï€(Y ) = C
will be a representation of sl(2; C).
Theorem 5.9. For each integer m â‰¥0, there is an irreducible representation
of sl(2; C) with dimension m+1. Any two irreducible representations of sl(2; C) with
the same dimension are equivalent. If Ï€ is an irreducible representation of sl(2; C)
with dimension m + 1, then Ï€ is equivalent to the representation Ï€m described in
Section 3.
Proof. Let Ï€ be an irreducible representation of sl(2; C) acting on a (ï¬nite-
dimensional complex) space V . Our strategy is to diagonalize the operator Ï€(H).
Of course, a priori, we donâ€™t know that Ï€(H) is diagonalizable. However, because
we are working over the (algebraically closed) ï¬eld of complex numbers, Ï€(H) must
have at least one eigenvector.
Proof. The following lemma is the key to the entire proof.
Lemma 5.10. Let u be an eigenvector of Ï€(H) with eigenvalue Î± âˆˆC. Then
Ï€(H)Ï€(X)u = (Î± + 2)Ï€(X)u.
Thus either Ï€(X)u = 0, or else Ï€(X)u is an eigenvector for Ï€(H) with eigenvalue
Î± + 2. Similarly,
Ï€(H)Ï€(Y )u = (Î± âˆ’2)Ï€(Y )u
so that either Ï€(Y )u = 0, or else Ï€(Y )u is an eigenvector for Ï€(H) with eigenvalue
Î± âˆ’2.
Proof. We call Ï€(X) the â€œraising operator,â€ because it has the eï¬€ect of raising
the eigenvalue of Ï€(H) by 2, and we call Ï€(Y ) the â€œlowering operator.â€ We know
that [Ï€(H), Ï€(X)] = Ï€ ([H, X]) = 2Ï€(X). Thus
Ï€(H)Ï€(X) âˆ’Ï€(X)Ï€(H) = 2Ï€(X)
or
Ï€(H)Ï€(X) = Ï€(X)Ï€(H) + 2Ï€(X).
Thus
Ï€(H)Ï€(X)u = Ï€(X)Ï€(H)u + 2Ï€(X)u
= Ï€(X) (Î±u) + 2Ï€(X)u
= (Î± + 2)Ï€(X)u.
Similarly, [Ï€(H), Ï€(Y )] = âˆ’2Ï€(Y ), and so
Ï€(H)Ï€(Y ) = Ï€(Y )Ï€(H) âˆ’2Ï€(Y )

4. THE IRREDUCIBLE REPRESENTATIONS OF su(2)
77
so that
Ï€(H)Ï€(Y )u = Ï€(Y )Ï€(H)u âˆ’2Ï€(Y )u
= Ï€(Y ) (Î±u) âˆ’2Ï€(Y )u
= (Î± âˆ’2)Ï€(Y )u.
This is what we wanted to show.
As we have observed, Ï€(H) must have at least one eigenvector u (u Ì¸= 0), with
some eigenvalue Î± âˆˆC. By the lemma,
Ï€(H)Ï€(X)u = (Î± + 2)Ï€(X)u
and more generally
Ï€(H)Ï€(X)nu = (Î± + 2n)Ï€(X)nu.
This means that either Ï€(X)nu = 0, or else Ï€(X)nu is an eigenvector for Ï€(H) with
eigenvalue (Î± + 2n).
Now, an operator on a ï¬nite-dimensional space can have only ï¬nitely many
distinct eigenvalues. Thus the Ï€(X)nuâ€™s cannot all be diï¬€erent from zero. Thus
there is some N â‰¥0 such that
Ï€(X)Nu Ì¸= 0
but
Ï€(X)N+1u = 0.
Deï¬ne u0 = Ï€(X)Nu and Î» = Î± + 2N. Then
Ï€(H)u0 = Î»u0
(5.7)
Ï€(X)u0 = 0
(5.8)
Then deï¬ne
uk = Ï€(Y )ku0
for k â‰¥0. By the second part of the lemma, we have
Ï€(H)uk = (Î» âˆ’2k)uk.
(5.9)
Since, again, Ï€(H) can have only ï¬nitely many eigenvalues, the ukâ€™s cannot all be
non-zero.
Lemma 5.11. With the above notation,
Ï€(X)uk = [kÎ» âˆ’k(k âˆ’1)] ukâˆ’1
(k > 0)
Ï€ (X) u0 = 0.
Proof. We proceed by induction on k. In the case k = 1 we note that u1 =
Ï€(Y )u0. Using the commutation relation [Ï€(X), Ï€(Y )] = Ï€(H) we have
Ï€(X)u1 = Ï€(X)Ï€(Y )u0 = (Ï€(Y )Ï€(X) + Ï€(H)) u0.
But Ï€(X)u0 = 0, so we get
Ï€(X)u1 = Î»u0
which is the lemma in the case k = 1.

78
5. BASIC REPRESENTATION THEORY
Now, by deï¬nition uk+1 = Ï€(Y )uk. Using (5.9) and induction we have
Ï€(X)uk+1 = Ï€(X)Ï€(Y )uk
= (Ï€(Y )Ï€(X) + Ï€(H)) uk
= Ï€(Y ) [kÎ» âˆ’k(k âˆ’1)] ukâˆ’1 + (Î» âˆ’2k)uk
= [kÎ» âˆ’k(k âˆ’1) + (Î» âˆ’2k)] uk.
Simplifying the last expression give the Lemma.
Since Ï€(H) can have only ï¬nitely many eigenvalues, the ukâ€™s cannot all be
non-zero. There must therefore be an integer m â‰¥0 such that
uk = Ï€(Y )ku0 Ì¸= 0
for all k â‰¤m, but
um+1 = Ï€(Y )m+1u0 = 0.
Now if um+1 = 0, then certainly Ï€(X)um+1 = 0. Then by Lemma 5.11,
0 = Ï€(X)um+1 = [(m + 1)Î» âˆ’m(m + 1)] um = (m + 1)(Î» âˆ’m)um.
But um Ì¸= 0, and m+1 Ì¸= 0 (since m â‰¥0). Thus in order to have (m+1)(Î»âˆ’m)um
equal to zero, we must have Î» = m.
We have made considerable progress.
Given a ï¬nite-dimensional irreducible
representation Ï€ of sl(2; C), acting on a space V , there exists an integer m â‰¥0 and
non-zero vectors u0, Â· Â· Â·um such that (putting Î» equal to m)
Ï€(H)uk = (m âˆ’2k)uk
Ï€(Y )uk = uk+1
(k < m)
Ï€(Y )um = 0
Ï€(X)uk = [km âˆ’k(k âˆ’1)] ukâˆ’1
(k > 0)
Ï€(X)u0 = 0
(5.10)
The vectors u0, Â· Â· Â·um must be linearly independent, since they are eigenvectors
of Ï€(H) with distinct eigenvalues.
Moreover, the (m + 1)-dimensional span of
u0, Â· Â· Â·um is explicitly invariant under Ï€(H), Ï€(X), and Ï€(Y ), and hence under
Ï€(Z) for all Z âˆˆsl(2; C). Since Ï€ is irreducible, this space must be all of V .
We have now shown that every irreducible representation of sl(2; C) is of the
form (5.10). It remains to show that everything of the form (5.10) is a represen-
tation, and that it is irreducible.
That is, if we deï¬ne Ï€(H), Ï€(X), and Ï€(Y )
by (5.10) (where the ukâ€™s are basis elements for some (m + 1)-dimensional vector
space), then we want to show that they have the right commutation relations to
form a representation of sl(2; C), and that this representation is irreducible.
The computation of the commutation relations of Ï€(H), Ï€(X), and Ï€(Y ) is
straightforward, and is left as an exercise. Note that when dealing with Ï€(Y ), you
should treat separately the vectors uk, k < m, and um. Irreducibility is also easy
to check, by imitating the proof of Proposition 5.7. (See Exercise 6.)
We have now shown that there is an irreducible representation of sl(2; C) in
each dimension m + 1, by explicitly writing down how H, X, and Y should act
(Equation 5.10) in a basis.
But we have shown more than this.
We also have
shown that any (m + 1)-dimensional irreducible representation of sl(2; C) must be
of the form (5.10). It follows that any two irreducible representations of sl(2; C)

5. DIRECT SUMS OF REPRESENTATIONS AND COMPLETE REDUCIBILITY
79
of dimension (m + 1) must be equivalent. For if Ï€1 and Ï€2 are two irreducible
representations of dimension (m + 1), acting on spaces V1 and V2, then V1 has a
basis u0, Â· Â· Â·um as in (5.10) and V2 has a similar basis eu0, Â· Â· Â· eum. But then the
map Ï† : V1 â†’V2 which sends uk to euk will be an isomorphism of representations.
(Think about it.)
In particular, the (m+1)-dimensional representation Ï€m described in Section 3
must be equivalent to (5.10).This can be seen explicitly by introducing the following
basis for Vm:
uk = [Ï€m(Y )]k (zm
2 ) = (âˆ’1)k
m!
(m âˆ’k)!zk
1 zmâˆ’k
2
(k â‰¤m).
Then by deï¬nition Ï€m(Y )uk = uk+1 (k < m), and it is clear that Ï€m(Y )um = 0.
It is easy to see that Ï€m(H)uk = (m âˆ’2k)uk. The only thing left to check is the
behavior of Ï€m(X). But direct computation shows that
Ï€m(X)uk = k(m âˆ’k + 1)ukâˆ’1 = [km âˆ’k(k âˆ’1)] ukâˆ’1.
as required.
This completes the proof of Theorem 5.9.
5. Direct Sums of Representations and Complete Reducibility
One way of generating representations is to take some representations you know
and combine them in some fashion. We will consider two methods of generating
new representations from old ones, namely direct sums and tensor products of
representations. In this section we consider direct sums; in the next section we look
at tensor products. (There is one other standard construction of this sort, namely
the dual of a representation. See Exercise 14.)
Definition 5.12. Let G be a matrix Lie group, and let Î 1, Î 2, Â· Â· Â·Î n be rep-
resentations of G acting on vector spaces V1, V2, Â· Â· Â·Vn. Then the direct sum of
Î 1, Î 2, Â· Â· Â·Î n is a representation Î 1âŠ•Â· Â· Â·âŠ•Î n of G acting on the space V1âŠ•Â· Â· Â·âŠ•Vn,
deï¬ned by
[Î 1 âŠ•Â· Â· Â· âŠ•Î n(A)] (v1, Â· Â· Â·vn) = (Î 1(A)v1, Â· Â· Â· , Î n(A)vn)
for all A âˆˆG.
Similarly, if g is a Lie algebra, and Ï€1, Ï€2, Â· Â· Â·Ï€n are representations of g acting
on V1, V2, Â· Â· Â·Vn, then we deï¬ne the direct sum of Ï€1, Ï€2, Â· Â· Â·Ï€n, acting on V1 âŠ•
Â· Â· Â· âŠ•Vn by
[Ï€1 âŠ•Â· Â· Â· âŠ•Ï€n(X)] (v1, Â· Â· Â·vn) = (Ï€1(X)v1, Â· Â· Â· , Ï€n(X)vn)
for all X âˆˆg.
It is trivial to check that, say, Î 1 âŠ•Â· Â· Â· âŠ•Î n is really a representation of G.
Definition 5.13. A ï¬nite-dimensional representation of a group or Lie alge-
bra, acting on a space V , is said to be completely reducible if the following
property is satisï¬ed: Given an invariant subspace W âŠ‚V , and a second invariant
subspace U âŠ‚W âŠ‚V , there exists a third invariant subspace eU âŠ‚W such that
U âˆ©eU = {0} and U + eU = W.
The following Proposition shows that complete reducibility is a nice property
for a representation to have.

80
5. BASIC REPRESENTATION THEORY
Proposition 5.14. A ï¬nite-dimensional completely reducible representation of
a group or Lie algebra is equivalent to a direct sum of (one or more) irreducible
representations.
Proof. The proof is by induction on the dimension of the space V . If dimV =
1, then automatically the representation is irreducible, since then V is has no non-
trivial subspaces, let alone non-trivial invariant subspaces. Thus V is a direct sum
of irreducible representations, with just one summand, namely V itself.
Suppose, then, that the Proposition holds for all representations with dimension
strictly less than n, and that dimV = n. If V is irreducible, then again we have
a direct sum with only one summand, and we are done. If V is not irreducible,
then there exists a non-trivial invariant subspace U âŠ‚V . Taking W = V in the
deï¬nition of complete reducibility, we see that there is another invariant subspace
eU with U âˆ©eU = {0} and U + eU = V . That is, V âˆ¼= U âŠ•eU as a vector space.
But since U and eU are invariant, they can be viewed as representations in
their own right. (That is, the action of our group or Lie algebra on U or eU is
a representation.) It is easy to see that in fact V is isomorphic to U âŠ•eU as a
representation. Furthermore, it is easy to see that both U and eU are completely
reducible representations, since every invariant subspace W of, say, U is also an
invariant subspace of V . But since U is non-trivial (i.e., U Ì¸= {0} and U Ì¸= V ),
we have dim U < dim V and dim eU < dim V . Thus by induction U âˆ¼= U1 âŠ•Â· Â· Â·Un
(as representations), with the Uiâ€™s irreducible, and eU âˆ¼= eU1 âŠ•Â· Â· Â· eUm, with the eUiâ€™s
irreducible, so that V âˆ¼= U1 âŠ•Â· Â· Â· Un âŠ•eU1 âŠ•Â· Â· Â· eUm.
Certain groups and Lie algebras have the property that every (ï¬nite-dimensional)
representation is completely reducible. This is a very nice property, because it im-
plies (by the above Proposition) that every representation is equivalent to a direct
sum of irreducible representations.
(And, as it turns out, this decomposition is
essentially unique.) Thus for such groups and Lie algebras, if you know (up to
equivalence) what all the irreducible representations are, then you know (up to
equivalence) what all the representations are.
Unfortunately, not every representation is irreducible. For example, the stan-
dard representation of the Heisenberg group is not completely reducible. (See Ex-
ercise 8.)
Proposition 5.15. Let G be a matrix Lie group. Let Î  be a ï¬nite-dimensional
unitary representation of G, acting on a ï¬nite-dimensional real or complex Hilbert
space V . Then Î  is completely reducible.
Proof. So, we are assuming that our space V is equipped with an inner prod-
uct, and that Î (A) is unitary for each A âˆˆG. Suppose that W âŠ‚V is invariant,
and that U âŠ‚W âŠ‚V is also invariant. Deï¬ne
eU = U âŠ¥âˆ©W.
Then of course eU âˆ©U = {0}, and standard Hilbert space theory implies that
eU + U = W.
It remains only to show that eU is invariant. So suppose that v âˆˆU âŠ¥âˆ©W.
Since W is assumed invariant, Î (A)v will be in W for any A âˆˆG. We need to
show that Î (A)v is perpendicular to U. Well, since Î (Aâˆ’1) is unitary, then for any

5. DIRECT SUMS OF REPRESENTATIONS AND COMPLETE REDUCIBILITY
81
u âˆˆU
âŸ¨u, Î (A)vâŸ©=

Î (Aâˆ’1)u, Î (Aâˆ’1)Î (A)v

=

Î (Aâˆ’1)u, v

.
But U is assumed invariant, and so Î (Aâˆ’1)u âˆˆU.
But then since v âˆˆU âŠ¥,

Î (Aâˆ’1)u, v = 0. This means that
âŸ¨u, Î (A)vâŸ©= 0
for all u âˆˆU, i.e., Î (A)v âˆˆU âŠ¥.
Thus eU is invariant, and we are done.
Proposition 5.16. If G is a ï¬nite group, then every ï¬nite-dimensional real or
complex representation of G is completely reducible.
Proof. Suppose that Î  is a representation of G, acting on a space V . Choose
an arbitrary inner product âŸ¨âŸ©on V . Then deï¬ne a new inner product âŸ¨âŸ©G on V
by
âŸ¨v1, v2âŸ©G =
X
gâˆˆG
âŸ¨Î (g)v1, Î (g)v2âŸ©.
It is very easy to check that indeed âŸ¨âŸ©G is an inner product. Furthermore, if h âˆˆG,
then
âŸ¨Î (h)v1, Î (h)v2âŸ©G =
X
gâˆˆG
âŸ¨Î (g)Î (h)v1, Î (g)Î (h)v2âŸ©
=
X
gâˆˆG
âŸ¨Î (gh)v1, Î (gh)v2âŸ©.
But as g ranges over G, so does gh. Thus in fact
âŸ¨Î (h)v1, Î (h)v2âŸ©G = âŸ¨v1, v2âŸ©G .
That is, Î  is a unitary representation with respect to the inner product âŸ¨âŸ©G.
Thus Î  is completely reducible by Proposition 5.15.
There is a variant of the above argument which can be used to prove the
following result:
Proposition 5.17. If G is a compact matrix Lie group, then every ï¬nite-
dimensional real or complex representation of G is completely reducible.
Proof. This proof requires the notion of Haar measure. A left Haar mea-
sure on a matrix Lie group G is a non-zero measure Âµ on the Borel Ïƒ-algebra in
G with the following two properties: 1) it is locally ï¬nite, that is, every point in
G has a neighborhood with ï¬nite measure, and 2) it is left-translation invariant.
Left-translation invariance means that Âµ (gE) = Âµ (E) for all g âˆˆG and for all
Borel sets E âŠ‚G, where
gE = {ge |e âˆˆE } .
It is a fact which we cannot prove here that every matrix Lie group has a left Haar
measure, and that this measure is unique up to multiplication by a constant. (One
can analogously deï¬ne right Haar measure, and a similar theorem holds for it. Left
Haar measure and right Haar measure may or may not coincide; a group for which
they do is called unimodular.)

82
5. BASIC REPRESENTATION THEORY
Now, the key fact for our purpose is that left Haar measure is ï¬nite if and
only if the group G is compact. So if Î  is a ï¬nite-dimensional representation of a
compact group G acting on a space V , then let âŸ¨âŸ©be an arbitrary inner product
on V , and deï¬ne a new inner product âŸ¨âŸ©G on V by
âŸ¨v1, v2âŸ©G =
Z
G
âŸ¨Î (g)v1, Î (g)v2âŸ©dÂµ (g) ,
where Âµ is left Haar measure.
Again, it is easy to check that âŸ¨âŸ©G is an inner
product. Furthermore, if h âˆˆG, then by the left-invariance of Âµ
âŸ¨Î (h)v1, Î (h)v2âŸ©G =
Z
G
âŸ¨Î (g)Î (h)v1, Î (g)Î (h)v2âŸ©dÂµ (g)
=
Z
G
âŸ¨Î (gh)v1, Î (gh)v2âŸ©dÂµ (g)
= âŸ¨v1, v2âŸ©G .
So Î  is a unitary representation with respect to âŸ¨âŸ©G, and thus completely reducible.
Note that âŸ¨âŸ©G is well-deï¬ned only because Âµ is ï¬nite.
6. Tensor Products of Representations
Let U and V be ï¬nite-dimensional real or complex vector spaces. We wish to
deï¬ne the tensor product of U and V , which is will be a new vector space U âŠ—V
â€œbuiltâ€ out of U and V . We will discuss the idea of this ï¬rst, and then give the
precise deï¬nition.
We wish to consider a formal â€œproductâ€ of an element u of U with an element
v of V , denoted u âŠ—v. The space U âŠ—V is then the space of linear combinations of
such products, i.e., the space of elements of the form
a1u1 âŠ—v1 + a2u2 âŠ—v2 + Â· Â· Â· + anun âŠ—vn.
(5.11)
Of course, if â€œâŠ—â€ is to be interpreted as a product, then it should be bilinear. That
is, we should have
(u1 + au2) âŠ—v = u1 âŠ—v + au2 âŠ—v
u âŠ—(v1 + av2) = u âŠ—v1 + au âŠ—v2.
We do not assume that the product is commutative. (In fact, the product in the
other order, v âŠ—u, is in a diï¬€erent space, namely, V âŠ—U.)
Now, if e1, e2, Â· Â· Â· , en is a basis for U and f1, f2, Â· Â· Â· , fm is a basis for V , then
using bilinearity it is easy to see that any element of the form (5.11) can be written
as a linear combination of the elements ei âŠ—fj . In fact, it seems reasonable to
expect that {ei âŠ—fj |0 â‰¤i â‰¤n, 0 â‰¤j â‰¤m} should be a basis for the space U âŠ—V .
This in fact turns out to be the case.
Definition 5.18. If U and V are ï¬nite-dimensional real or complex vector
spaces, then a tensor product of U with V is a vector space W, together with a
bilinear map Ï† : U Ã— V â†’W with the following property: If Ïˆ is any bilinear map
of U Ã— V into a vector space X, then there exists a unique linear map eÏˆ of W into
X such that the following diagram commutes:
U Ã— V
Ï†â†’
W
Ïˆ â†˜
â†™eÏˆ
X
.

6. TENSOR PRODUCTS OF REPRESENTATIONS
83
Note that the bilinear map Ïˆ from U Ã— V into X turns into the linear map eÏˆ
of W into X. This is one of the points of tensor products: bilinear maps on U Ã— V
turn into linear maps on W.
Theorem 5.19. If U and V are any ï¬nite-dimensional real or complex vector
spaces, then a tensor product (W, Ï†) exists. Furthermore, (W, Ï†) is unique up to
canonical isomorphism. That is, if (W1, Ï†1) and (W2, Ï†2) are two tensor products,
then there exists a unique vector space isomorphism Î¦ : W1 â†’W2 such that the
following diagram commutes
U Ã— V
Ï†1
â†’
W1
Ï†2 â†˜
â†™Î¦
W2
.
Suppose that (W, Ï†) is a tensor product, and that e1, e2, Â· Â· Â· , en is a basis for
U and f1, f2, Â· Â· Â· , fm is a basis for V . Then {Ï†(ei, fj) |0 â‰¤i â‰¤n, 0 â‰¤j â‰¤m} is a
basis for W.
Proof. Exercise 12.
Notation 5.20. Since the tensor product of U and V is essentially unique, we
will let U âŠ—V denote an arbitrary tensor product space, and we will write uâŠ—v in-
stead of Ï†(u, v). In this notation, the Theorem says that {ei âŠ—fj |0 â‰¤i â‰¤n, 0 â‰¤j â‰¤m}
is a basis for U âŠ—V , as expected. Note in particular that
dim (U âŠ—V ) = (dimU) (dimV )
(not dim U + dim V ).
The deï¬ning property of U âŠ—V is called the universal property of tensor
products.
While it may seem that we are taking a simple idea and making it
confusing, in fact there is a point to this universal property. Suppose we want to
deï¬ne a linear map T from U âŠ—V into some other space. The most sensible way to
deï¬ne this is to deï¬ne T on elements of the form u âŠ—v. (You might try deï¬ning it
on a basis, but this forces you to worry about whether things depend on the choice
of basis.) Now, every element of U âŠ—V is a linear combination of things of the form
u âŠ—v. However, this representation is far from unique. (Since, say, if u = u1 + u2,
then you can rewrite u âŠ—v as u1 âŠ—v + u2 âŠ—v.)
Thus if you try to deï¬ne T by what it does to elements of the form u âŠ—v,
you have to worry about whether T is well-deï¬ned. This is where the universal
property comes in. Suppose that Ïˆ(u, v) is some bilinear expression in u, v. Then
the universal property says precisely that there is a unique linear map T (= eÏˆ)
such that
T(u âŠ—v) = Ïˆ(u, v).
(Think about it and make sure that you see that this is really what the universal
property says.)
The conclusion is this: You can deï¬ne a linear map T on U âŠ—V by deï¬ning it
on elements of the form u âŠ—v, and this will be well-deï¬ned, provided that T(u âŠ—v)
is bilinear in (u, v). The following Proposition shows how to make use of this idea.

84
5. BASIC REPRESENTATION THEORY
Proposition 5.21. Let U and V be ï¬nite-dimensional real or complex vector
spaces. Let A : U â†’U and B : V â†’V be linear operators. Then there exists a
unique linear operator from U âŠ—V to U âŠ—V , denoted A âŠ—B, such that
A âŠ—B(u âŠ—v) = (Au) âŠ—(Bv)
for all u âˆˆU, v âˆˆV .
If A1, A2 are linear operators on U and B1, B2 are linear operators on V , then
(A1 âŠ—B1) (A2 âŠ—B2) = (A1A2) âŠ—(B1B2) .
Proof. Deï¬ne a map Ïˆ from U Ã— V into U âŠ—V by
Ïˆ(u, v) = (Au) âŠ—(Bv) .
Since A and B are linear, and since âŠ—is bilinear, Ïˆ will be a bilinear map of U Ã—V
into U âŠ—V . But then the universal property says that there is an associated linear
map eÏˆ : U âŠ—V â†’U âŠ—V such that
eÏˆ(u âŠ—v) = Ïˆ(u, v) = (Au) âŠ—(Bv) .
Then eÏˆ is the desired map A âŠ—B.
Now, if A1, A2 are operators on U and B1, B2 are operators on V , then compute
that
(A1 âŠ—B1) (A2 âŠ—B2) (u âŠ—v) = (A1 âŠ—B1) (A2u âŠ—B2v)
= A1A2u âŠ—B1B2v.
This shows that (A1 âŠ—B1) (A2 âŠ—B2) = (A1A2) âŠ—(B1B2) are equal on elements of
the form uâŠ—v. Since every element of UâŠ—V can be written as a linear combination of
things of the form uâŠ—v (in fact of eiâŠ—fj), (A1 âŠ—B1) (A2 âŠ—B2) and (A1A2)âŠ—(B1B2)
must be equal on the whole space.
We are now ready to deï¬ne tensor products of representations. There are two
diï¬€erent approaches to this, both of which are important. The ï¬rst approach starts
with a representation of a group G acting on a space V and a representation of
another group H acting on a space U, and produces a representation of the product
group G Ã— H acting on the space U âŠ—V . The second approach starts with two
diï¬€erent representations of the same group G, acting on spaces U and V , and
produces a representation of G acting on U âŠ—V . Both of these approaches can be
adapted to apply to Lie algebras.
Definition 5.22. Let G and H be matrix Lie groups. Let Î 1 be a represen-
tation of G acting on a space U and let Î 2 be a representation of H acting on a
space V . The the tensor product of Î 1 and Î 2 is a representation Î 1 âŠ—Î 2 of
G Ã— H acting on U âŠ—V deï¬ned by
Î 1 âŠ—Î 2(A, B) = Î 1(A) âŠ—Î 2(B)
for all A âˆˆG and B âˆˆH.
Using the above Proposition, it is very easy to check that indeed Î 1 âŠ—Î 2 is a
representation of G Ã— H.
Now, if G and H are matrix Lie groups, that is, G is a closed subgroup of
GL(n; C) and H is a closed subgroup of GL(m; C), then G Ã— H can be regarded in
an obvious way as a closed subgroup of GL(n + m; C). Thus the direct product of
matrix Lie groups can be regarded as a matrix Lie group. It is easy to check that

6. TENSOR PRODUCTS OF REPRESENTATIONS
85
the Lie algebra of G Ã— H is isomorphic to the direct sum of the Lie algebra of G
and the Lie algebra of H. See Exercise 13.
In light of Proposition 5.4, the representation Î 1 âŠ—Î 2 of G Ã— H gives rise to a
representation of the Lie algebra of GÃ—H, namely gâŠ•h. The following Proposition
shows that this representation of g âŠ•h is not what you might expect at ï¬rst.
Proposition 5.23. Let G and H be matrix Lie groups, let Î 1, Î 2 be represen-
tations of G, H respectively, and consider the representation Î 1 âŠ—Î 2 of GÃ—H. Let
Ï€1 âŠ—Ï€2 denote the associated representation of the Lie algebra of G Ã— H, namely
g âŠ•h. Then for all X âˆˆg and Y âˆˆh
Ï€1 âŠ—Ï€2(X, Y ) = Ï€1(X) âŠ—I + I âŠ—Ï€2(Y ).
Proof. Suppose that u(t) is a smooth curve in U and v(t) is a smooth curve
in V . Then we verify the product rule in the usual way:
lim
hâ†’0
u(t + h) âŠ—v(t + h) âˆ’u(t) âŠ—v(t)
h
= lim
hâ†’0
u(t + h) âŠ—v(t + h) âˆ’u(t + h) âŠ—v(t)
h
+ u(t + h) âŠ—v(t) âˆ’u(t) âŠ—v(t)
h
= lim
hâ†’0

u(t + h) âŠ—(v(t + h) âˆ’v (t))
h

+ lim
hâ†’0
(u(t + h) âˆ’u (t))
h
âŠ—v(t)

.
Thus
d
dt (u(t) âŠ—v(t)) = du
dt âŠ—v(t) + u(t) âŠ—dv
dt .
This being the case, we can compute Ï€1 âŠ—Ï€2(X, Y ):
Ï€1 âŠ—Ï€2(X, Y )(u âŠ—v) = d
dt

t=0
Î 1 âŠ—Î 2(etX, etY )(u âŠ—v)
= d
dt

t=0
Î 1(etX)u âŠ—Î 2(etY )v
=
 d
dt

t=0
Î 1(etX)u

âŠ—v + u âŠ—
 d
dt

t=0
Î 2(etY )v

.
This shows that Ï€1 âŠ—Ï€2(X, Y ) = Ï€1(X) âŠ—I + I âŠ—Ï€2(Y ) on elements of the form
u âŠ—v, and therefore on the whole space U âŠ—V .
Definition 5.24. Let g and h be Lie algebras, and let Ï€1 and Ï€2 be represen-
tations of g and h, acting on spaces U and V . Then the tensor product of Ï€1 and
Ï€2, denoted Ï€1 âŠ—Ï€2, is a representation of g âŠ•h acting on U âŠ—V , given by
Ï€1 âŠ—Ï€2(X, Y ) = Ï€1(X) âŠ—I + I âŠ—Ï€2(Y )
for all X âˆˆg and Y âˆˆh.
It is easy to check that this indeed deï¬nes a representation of g âŠ•h. Note that
if we deï¬ned Ï€1 âŠ—Ï€2(X, Y ) = Ï€1(X)âŠ—Ï€2(Y ), this would not be a representation of
gâŠ•h, for this is not even a linear map. (E.g., we would then have Ï€1âŠ—Ï€2(2X, 2Y ) =
4Ï€1 âŠ—Ï€2(X, Y )!) Note also that the above deï¬nition applies even if Ï€1 and Ï€2 do
not come from a representation of any matrix Lie group.

86
5. BASIC REPRESENTATION THEORY
Definition 5.25. Let G be a matrix Lie group, and let Î 1 and Î 2 be repre-
sentations of G, acting on spaces V1 and V2. Then the tensor product of Î 1 and
Î 2 is a representation of G acting on V1 âŠ—V2 deï¬ned by
Î 1 âŠ—Î 2(A) = Î 1(A) âŠ—Î 2(A)
for all A âˆˆG.
Proposition 5.26. With the above notation, the associated representation of
the Lie algebra g satisï¬es
Ï€1 âŠ—Ï€2(X) = Ï€1(X) âŠ—I + I âŠ—Ï€2(X)
for all X âˆˆg.
Proof. Using the product rule,
Ï€1 âŠ—Ï€2(X) (u âŠ—v) = d
dt

t=0
Î 1
 etX
u âŠ—Î 2
 etX
v
= Ï€1 (X) u âŠ—v + v âŠ—Ï€2 (X) u.
This is what we wanted to show.
Definition 5.27. If g is a Lie algebra, and Ï€1 and Ï€2 are representations of g
acting on spaces V1 and V2, then the tensor product of Ï€1 and Ï€2 is a represen-
tation of g acting on the space V1 âŠ—V2 deï¬ned by
Ï€1 âŠ—Ï€2(X) = Ï€1(X) âŠ—I + I âŠ—Ï€2(X)
for all X âˆˆg.
It is easy to check that Î 1 âŠ—Î 2 and Ï€1 âŠ—Ï€2 are actually representations of G
and g, respectively. There is some ambiguity in the notation, say, Î 1 âŠ—Î 2. For
even if Î 1 and Î 2 are both representations of the same group G, we could still
regard Î 1 âŠ—Î 2 as a representation of G Ã— G, by taking H = G in deï¬nition 5.22.
We will rely on context to make clear whether we are thinking of Î 1 âŠ—Î 2 as a
representation of G Ã— G or as representation of G.
Suppose Î 1 and Î 2 are irreducible representations of a group G.
If we re-
gard Î 1 âŠ—Î 2 as a representation of G, it may no longer be irreducible. If it is
not irreducible, one can attempt to decompose it as a direct sum of irreducible
representations. This process is called Clebsch-Gordan theory. In the case of
SU(2), this theory is relatively simple. (In the physics literature, the problem of
analyzing tensor products of representations of SU(2) is called â€œaddition of angular
momentum.â€) See Exercise 15.
7. Schurâ€™s Lemma
Let Î  and Î£ be representations of a matrix Lie group G, acting on spaces V
and W. Recall that a morphism of representations is a linear map Ï† : V â†’W
with the property that
Ï† (Î (A)v) = Î£(A) (Ï†(v))
for all v âˆˆV and all A âˆˆG. Schurâ€™s Lemma is an extremely important result which
tells us about morphisms of irreducible representations. Part of Schurâ€™s Lemma
applies to both real and complex representations, but part of it applies only to
complex representations.

7. SCHURâ€™S LEMMA
87
It is desirable to be able to state Schurâ€™s lemma simultaneously for groups and
Lie algebras. In order to do so, we need to indulge in a common abuse of notation.
If, say, Î  is a representation of G acting on a space V , we will refer to V as the
representation, without explicit reference to Î .
Theorem 5.28 (Schurâ€™s Lemma).
1. Let V and W be irreducible real or
complex representations of a group or Lie algebra, and let Ï† : V â†’W be a
morphism. Then either Ï† = 0 or Ï† is an isomorphism.
2. Let V be an irreducible complex representation of a group or Lie algebra,
and let Ï† : V â†’V be a morphism of V with itself. Then Ï† = Î»I, for some
Î» âˆˆC.
3. Let V and W be irreducible complex representations of a group or Lie algebra,
and let Ï†1, Ï†2 : V â†’W be non-zero morphisms. Then Ï†1 = Î»Ï†2, for some
Î» âˆˆC.
Corollary 5.29. Let Î  be an irreducible complex representation of a matrix
Lie group G. If A is in the center of G, then Î (A) = Î»I. Similarly, if Ï€ is an
irreducible complex representation of a Lie algebra g, and if X is in the center of g
(i.e., [X, Y ] = 0 for all Y âˆˆg), then Ï€(X) = Î»I.
Proof. We prove the group case; the proof of the Lie algebra case is the same.
If A is in the center of G, then for all B âˆˆG,
Î (A)Î (B) = Î (AB) = Î (BA) = Î (B)Î (A).
But this says exactly that Î (A) is a morphism of Î  with itself. So by Point 2 of
Schurâ€™s lemma, Î (A) is a multiple of the identity.
Corollary 5.30. An irreducible complex representation of a commutative group
or Lie algebra is one-dimensional.
Proof. Again, we prove only the group case. If G is commutative, then the
center of G is all of G, so by the previous corollary Î (A) is a multiple of the identity
for each A âˆˆG. But this means that every subspace of V is invariant! Thus the
only way that V can fail to have a non-trivial invariant subspace is for it not to have
any non-trivial subspaces. This means that V must be one-dimensional. (Recall
that we do not allow V to be zero-dimensional.)
Proof. As usual, we will prove just the group case; the proof of the Lie algebra
case requires only the obvious notational changes.
Proof of 1. Saying that Ï† is a morphism means Ï†(Î (A)v) = Î£(A) (Ï†(v)) for all
v âˆˆV and all A âˆˆG. Now suppose that v âˆˆker(Ï†). Then
Ï†(Î (A)v) = Î£(A)Ï†(v) = 0.
This shows that ker Ï† is an invariant subspace of V . Since V is irreducible, we must
have ker Ï† = 0 or ker Ï† = V . Thus Ï† is either one-to-one or zero.
Suppose Ï† is one-to-one. Then the image of Ï† is a non-zero subspace of W. On
the other hand, the image of Ï† is invariant, for if w âˆˆW is of the form Ï†(v) for
some v âˆˆV , then
Î£(A)w = Î£(A)Ï†(v) = Ï†(Î (A)v).
Since W is irreducible and image(V ) is non-zero and invariant, we must have
image(V ) = W. Thus Ï† is either zero or one-to-one and onto.

88
5. BASIC REPRESENTATION THEORY
Proof of 2. Suppose now that V is an irreducible complex representation, and
that Ï† : V â†’V is a morphism of V to itself. This means that Ï†Î (A) = Î (A)Ï†
for all A âˆˆG, i.e., that Ï† commutes with all of the Î (A)â€™s. Now, since we are over
an algebraically complete ï¬eld, Ï† must have at least one eigenvalue Î» âˆˆC. Let U
denote the eigenspace for Ï† associated to the eigenvalue Î», and let u âˆˆU. Then for
each A âˆˆG
Ï† (Î (A)u) = Î (A)Ï†(v) = Î»Î (A)u.
Thus applying Î (A) to an eigenvector of Ï† with eigenvalue Î» yields another eigen-
vector of Ï† with eigenvalue Î». That is, U is invariant.
Since Î» is an eigenvalue, U Ì¸= 0, and so we must have U = V . But this means
that Ï†(v) = Î»v for all v âˆˆV , i.e., that Ï† = Î»I.
Proof of 3. If Ï†2 Ì¸= 0, then by (1) Ï†2 is an isomorphism. Now look at Ï†1 â—¦Ï†âˆ’1
2 .
As is easily checked, the composition of two morphisms is a morphism, so Ï†1 â—¦Ï†âˆ’1
2
is a morphism of W with itself. Thus by (2), Ï†1 â—¦Ï†âˆ’1
2
= Î»I, whence Ï†1 = Î»Ï†2.
8. Group Versus Lie Algebra Representations
We know from Chapter 3 (Theorem 3.18) that every Lie group homomorphism
gives rise to a Lie algebra homomorphism. In particular, this shows (Proposition
5.4) that every representation of a matrix Lie group gives rise to a representation of
the associated Lie algebra. The goal of this section is to investigate the reverse pro-
cess. That is, given a representation of the Lie algebra, under what circumstances
is there an associated representation of the Lie group?
The climax of this section is Theorem 5.33, which states that if G is a con-
nected and simply connected matrix Lie group with Lie algebra g, and if Ï€ is a
representation of g, then there is a unique representation Î  of G such that Î  and
Ï€ are related as in Proposition 5.4. Our proof of this theorem will make use of the
Baker-Campbell-Hausdorï¬€formula from Chapter 4. Before turning to this general
theorem, we will examine two special cases, namely SO(3) and SU(2), for which we
can work things out by hand. See BrÂ¨ocker and tom Dieck, Chapter II, Section 5.
We have shown (Theorem 5.9) that every irreducible complex representation of
su(2) is equivalent to one of the representations Ï€m described in Section 3. (Recall
that the irreducible complex representations of su(2) are in one-to-one correspon-
dence with the irreducible representations of sl(2; C).) Each of the representations
Ï€m of su(2) was constructed from the corresponding representation Î m of the group
SU(2). Thus we see, by brute force computation, that every irreducible complex
representation of su(2) actually comes from a representation of the group SU(2)!
This is consistent with the fact that SU(2) is simply connected (Chapter 2, Prop.
2.12).
Let us now consider the situation for SO(3). (Which is not simply connected.)
We know from Exercise 10 of Chapter 3 that the Lie algebras su(2) and so(3) are
isomorphic. In particular, if we take the basis
E1 = 1
2
 i
0
0
âˆ’i

E2 = 1
2

0
1
âˆ’1
0

E3 = 1
2
 0
i
i
0

for su(2) and the basis
F1 =
ï£«
ï£­
0
0
0
0
0
âˆ’1
0
1
0
ï£¶
ï£¸
F2 =
ï£«
ï£­
0
0
1
0
0
0
âˆ’1
0
0
ï£¶
ï£¸
F3 =
ï£«
ï£­
0
âˆ’1
0
1
0
0
0
0
0
ï£¶
ï£¸

8. GROUP VERSUS LIE ALGEBRA REPRESENTATIONS
89
then direct computation shows that [E1, E2] = E3, [E2, E3] = E1, [E3, E1] = E2,
and similarly with the Eâ€™s replaced by the Fâ€™s. Thus the map Ï† : so(3) â†’su(2)
which takes Fi to Ei will be a Lie algebra isomorphism.
Since su(2) and so(3) are isomorphic Lie algebras, they must have â€œthe sameâ€
representations. Speciï¬cally, if Ï€ is a representation of su(2), then Ï€ â—¦Ï† will be
a representation of so(3), and every representation of so(3) is of this form.
In
particular, the irreducible representations of so(3) are precisely of the form Ïƒm =
Ï€m â—¦Ï†. We wish to determine, for a particular m, whether there is a representation
Î£m of the group SO(3) such that Ïƒm and Î£m are related as in Proposition 5.4.
Proposition 5.31. Let Ïƒm = Ï€m â—¦Ï† be the irreducible complex representations
of the Lie algebra so(3) (m â‰¥0). If m is even, then there is a representation Î£m
of the group SO(3) such that Ïƒm and Î£m are related as in Proposition 5.4. If m is
odd, then there is no such representation of SO(3).
Note that the condition that m be even is equivalent to the condition that
dim Vm = m + 1 be odd. Thus it is the odd-dimensional representations of the Lie
algebra so(3) which come from group representations.
In the physics literature, the representations of su(2)/so(3) are labeled by the
parameter l = m/2. In terms of this notation, a representation of so(3) comes from
a representation of SO(3) if and only if l is an integer. The representations with l
an integer are called â€œinteger spinâ€; the others are called â€œhalf-integer spin.â€
8.0.1. Proof.
Proof. Case 1: m odd. In this case, we want to prove that there is no rep-
resentation Î£m such that Ïƒm and Î£m are related as in Proposition 5.4. (We have
already considered the case m = 1 in Exercise 7.) Suppose, to the contrary, that
there is such a Î£m. Then Proposition 5.4 says that
Î£m(eX) = eÏƒm(X)
for all X âˆˆso(3). In particular, take X = 2Ï€F1. Then, computing as in Chapter
3, Section 2 we see that
e2Ï€F1 =
ï£«
ï£­
1
0
0
0
cos 2Ï€
âˆ’sin 2Ï€
0
sin 2Ï€
cos 2Ï€
ï£¶
ï£¸= I.
Thus on the one hand Î£m
 e2Ï€F1
= Î£m(I) = I, while on the other hand Î£m
 e2Ï€F1
=
e2Ï€Ïƒm(F1).
Let us compute e2Ï€Ïƒm(F1). By deï¬nition, Ïƒm(F1) = Ï€m(Ï†(F1)) = Ï€m(E1). But,
E1 = i
2H, where as usual
H =

1
0
0
âˆ’1

.
We know that there is a basis u0, u1, Â· Â· Â· , um for Vm such that uk is an eigenvector
for Ï€m(H) with eigenvalue m âˆ’2k. This means that uk is also an eigenvector for
Ïƒm(F1) = i
2Ï€m(H), with eigenvalue i
2(m âˆ’2k). Thus in the basis {uk} we have
Ïƒm(F1) =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
i
2m
i
2(m âˆ’2)
...
i
2(âˆ’m)
ï£¶
ï£·
ï£·
ï£·
ï£¸.

90
5. BASIC REPRESENTATION THEORY
But we are assuming the m is odd! This means that m âˆ’2k is an odd integer.
Thus e2Ï€ i
2 (mâˆ’2k) = âˆ’1, and in the basis {uk}
e2Ï€Ïƒm(F1) =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
e2Ï€ i
2 m
e2Ï€ i
2 (mâˆ’2)
...
e2Ï€ i
2 (âˆ’m)
ï£¶
ï£·
ï£·
ï£·
ï£¸= âˆ’I.
Thus on the one hand, Î£m
 e2Ï€F1 = Î£m(I) = I, while on the other hand Î£m
 e2Ï€F1 =
e2Ï€Ïƒm(F1) = âˆ’I. This is a contradiction, so there can be no such group representa-
tion Î£m.
Case 2: m is even. We will use the following:
Lemma 5.32. There exists a Lie group homomorphism Î¦ : SU(2) â†’SO(3)
such that
1) Î¦ maps SU(2) onto SO(3),
2) ker Î¦ = {I, âˆ’I}, and
3) the associated Lie algebra homomorphism eÎ¦ : su(2) â†’so(3) is an isomor-
phism which takes Ei to Fi. That is, eÎ¦ = Ï†âˆ’1.
Proof. Exercise 17.
Now consider the representations Î m of SU(2). I claim that if m is even, then
Î m(âˆ’I) = I. To see this, note that
e2Ï€E1 = exp
 Ï€i
0
0
âˆ’Ï€i

= âˆ’I.
Thus Î m(âˆ’I) = Î m(e2Ï€E1) = eÏ€m(2Ï€E1). But as in Case 1,
eÏ€m(2Ï€E1) =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
e2Ï€ i
2 m
e2Ï€ i
2 (mâˆ’2)
...
e2Ï€ i
2 (âˆ’m)
ï£¶
ï£·
ï£·
ï£·
ï£¸.
Only, this time, m is even, and so
i
2(m âˆ’2k) is an integer, so that Î m(âˆ’I) =
eÏ€m(2Ï€E1) = I.
Since Î m(âˆ’I) = I, Î m(âˆ’U) = Î m(U) for all U âˆˆSU(2).
According to
Lemma 5.32, for each R âˆˆSO(3), there is a unique pair of elements {U, âˆ’U} such
that Î¦(U) = Î¦(âˆ’U) = R. Since Î m(U) = Î m(âˆ’U), it makes sense to deï¬ne
Î£m(R) = Î m(U).
It is easy to see that Î£m is a Lie group homomorphism (hence, a representation).
By construction, we have
Î m = Î£m â—¦Î¦.
(5.12)
Now, if eÎ£m denotes the Lie algebra representation associated to Î£m, then it
follows from (5.12) that
Ï€m = eÎ£m â—¦eÎ¦.
But the Lie algebra homomorphism eÎ¦ takes Ei to Fi, that is, eÎ¦ = Ï†âˆ’1. So Ï€m =
eÎ£m â—¦Ï†âˆ’1, or eÎ£m = Ï€m â—¦Ï†. Thus eÎ£m = Ïƒm, which is what we want to show.

8. GROUP VERSUS LIE ALGEBRA REPRESENTATIONS
91
It is now time to state the main theorem.
Theorem 5.33.
1. Let G, H be a matrix Lie groups, let Ï†1, Ï†2 : G â†’H
be Lie group homomorphisms, and let eÏ†1, eÏ†2 : g â†’h be the associated Lie
algebra homomorphisms. If G is connected and eÏ†1 = eÏ†2, then Ï†1 = Ï†2.
2. Let G, H be a matrix Lie groups with Lie algebras g and h. Let eÏ† : g â†’h be
a Lie algebra homomorphism. If G is connected and simply connected, then
there exists a unique Lie group homomorphism Ï† : G â†’H such that Ï† and
eÏ† are related as in Theorem 3.18 of Chapter 3.
This has the following corollaries.
Corollary 5.34. Suppose G and H are connected, simply connected matrix
Lie groups with Lie algebras g and h. If g âˆ¼= h then G âˆ¼= H.
Proof. Let eÏ† : g â†’h be a Lie algebra isomorphism. By Theorem 5.33, there
exists an associated Lie group homomorphism Ï† : G â†’H. Since eÏ†âˆ’1 : h â†’g is also
a Lie algebra homomorphism, there is a corresponding Lie group homomorphism
Ïˆ : H â†’G. We want to show that Ï† and Ïˆ are inverses of each other.
Well, ]
Ï† â—¦Ïˆ = eÏ† â—¦eÏˆ = Ih, so by the Point 1 of the Theorem, Ï† â—¦Ïˆ = IH.
Similarly, Ïˆ â—¦Ï† = IG.
Corollary 5.35.
1. Let G be a connected matrix Lie group, let Î 1 and
Î 2 be representations of G, and let Ï€1 and Ï€2 be the associated Lie algebra
representations. If Ï€1 and Ï€2 are equivalent, then Î 1 and Î 2 are equivalent.
2. Let G be connected and simply connected. If Ï€ is a representation of g, then
there exists a representation Î  of G, acting on the same space, such that Î 
and Ï€ are related as in Proposition 5.4.
Proof. For (1), let Î 1 act on V and Î 2 on W. We assume that the associated
Lie algebra representations are equivalent, i.e., that there exists an invertible linear
map Ï† : V â†’W such that
Ï† (Ï€1(X)v) = Ï€2(X)Ï†(v)
for all X âˆˆg and all v âˆˆV . This is the same as saying that Ï†Ï€1(X) = Ï€2(X)Ï†, or
equivalently that Ï†Ï€1(X)Ï†âˆ’1 = Ï€2(X) (for all X âˆˆg).
Now deï¬ne a map Î£2 : G â†’GL(W) by the formula
Î£2(A) = Ï†Î 1(A)Ï†âˆ’1.
It is trivial to check that Î£2 is a homomorphism. Furthermore, diï¬€erentiation shows
that the associated Lie algebra homomorphism is
Ïƒ2(X) = Ï†Ï€1(X)Ï†âˆ’1 = Ï€2(X)
for all X. Then by (1) in the Theorem, we must also have Î£2 = Î 2, i.e.,
Ï†Î 1(A)Ï†âˆ’1 = Î 2(A)
for all A âˆˆG. But this shows that Î 1 and Î 2 are equivalent.
Point (2) of the Corollary follows immediately from Point (2) of the Theorem,
by taking H = GL(V ). â–¡
We now proceed with the proof of Theorem 5.33.

92
5. BASIC REPRESENTATION THEORY
Proof. Step 1: Verify Point (1) of the Theorem.
Since G is connected, Corollary 3.26 of Chapter 3 tells us that every element
A of G is a ï¬nite product of the form A = exp X1 exp X2 Â· Â· Â· exp Xn, with Xi âˆˆg.
But then if eÏ†1 = eÏ†2, we have
Ï†1
 eX1 Â· Â· Â·eXn
= e
eÏ†1(X1) Â· Â· Â·e
eÏ†1(Xn) = e
eÏ†2(X1) Â· Â· Â· e
eÏ†2(Xn) = Ï†2
 eX1 Â· Â· Â·eXn
.
So we now need only prove Point (2).
Step 2: Deï¬ne Ï† in a neighborhood of the identity.
Proposition 3.23 of Chapter 3 says that the exponential mapping for G has a
local inverse which maps a neighborhood V of the identity into the Lie algebra g.
On this neighborhood V we can deï¬ne Ï† : V â†’H by
Ï†(A) = exp
n
eÏ†(log A)
o
.
That is
Ï† = exp â—¦eÏ† â—¦log.
(Note that if there is to be a homomorphism Ï† as in Theorem 3.18 of Chapter 3,
then on V , Ï† must be exp â—¦eÏ† â—¦log.)
It follows from Corollary 4.4 to the Baker-Campbell-Hausdorï¬€formula that this
Ï† is a â€œlocal homomorphism.â€ That is, if A and B are in V , and if AB happens
to be in V as well, then Ï†(AB) = Ï†(A)Ï†(B). (See the discussion at the beginning
of Chapter 4.)
Step 3: Deï¬ne Ï† along a path.
Recall that when we say G is connected, we really mean that G is path-
connected.
Thus for any A âˆˆG, there exists a path A(t) âˆˆG with A(0) = I
and A(1) = A. A compactness argument shows that there exists numbers 0 = t0 <
t1 < t2 Â· Â· Â· < tn = 1 such that
A(s)A(ti)âˆ’1 âˆˆV
(5.13)
for all s between ti and ti+1.
In particular, for i = 0, we have A(s) âˆˆV for 0 â‰¤s â‰¤t1.
Thus we can
deï¬ne Ï† (A(s)) by Step 2 for s âˆˆ[0, t1]. Now, for s âˆˆ[t1, t2] we have by (5.13)
A(s)A(t1)âˆ’1 âˆˆV . Moving the A(t1) to the other side, this means that for s âˆˆ[t1, t2]
we can write
A(s) = A(s)A(t1)âˆ’1 A(t1).
with A(s)A(t1)âˆ’1 âˆˆV . If Ï† is to be a homomorphism, we must have
Ï† (A(s)) = Ï†
 
A(s)A(t1)âˆ’1
A(t1)

= Ï†
 A(s)A(t1)âˆ’1
Ï† (A(t1)) .
(5.14)
But Ï† (A(t1)) has already been deï¬ned, and we can deï¬ne Ï†
 A(s)A(t1)âˆ’1
by Step
2. In this way we can use (5.14) to deï¬ne Ï† (A(s)) for s âˆˆ[t1, t2].
Proceeding on in the same way, we can deï¬ne Ï† (A(s)) successively on each
interval [ti, ti+1] until eventually we have deï¬ned Ï† (A(s)) on the whole time interval
[0, 1]. This in particular serves to deï¬ne Ï† (A(1)) = Ï†(A).
Step 4: Prove independence of path.

8. GROUP VERSUS LIE ALGEBRA REPRESENTATIONS
93
In Step 3, we â€œdeï¬nedâ€ Ï†(A) by deï¬ning Ï† along a path joining the identity to
A. For this to make sense as a deï¬nition of Ï†(A) we have to prove that the answer
is independent of the choice of path, and also, for a particular path, independent
of the choice of partition (t0, t1, Â· Â· Â·tn).
To establish independence of partition, we ï¬rst show that passing from a par-
ticular partition to a reï¬nement of that partition doesnâ€™t change the answer. (A
reï¬nement of a partition is one which contains all the points of the original partition,
plus some other ones.) This is proved by means of the Baker-Campbell-Hausdorï¬€
formula. For example, suppose we insert an extra partition point s between t0 and
t1. Under the old partition we have
Ï† (A(t1)) = exp â—¦eÏ† â—¦log(A(t1)) .
(5.15)
Under the new partition we write
A(t1) = A(t1)A(s)âˆ’1 A(s)
so that
Ï† (A(t1)) = exp â—¦eÏ† â—¦log
 A(t1)A(s)âˆ’1
exp â—¦eÏ† â—¦log (A(s)) .
(5.16)
But (as noted in Step 2), Corollary 4.4 of the Baker-Campbell-Hausdorï¬€for-
mula (Chapter 4, Section 2) implies that for A and B suï¬ƒciently near the identity
exp â—¦eÏ† â—¦log(AB) =
h
exp â—¦eÏ† â—¦log(A)
i h
exp â—¦eÏ† â—¦log(B)
i
.
Thus the right sides of (5.15) and (5.16) are equal. Once we know that passing to a
reï¬nement doesnâ€™t change the answer, we have independence of partition. For any
two partitions of [0, 1] have a common reï¬nement, namely, the union of the two.
Once we know independence of partition, we need to prove independence of
path.
It is at this point that we use the fact that G is simply connected.
In
particular, because of simple connectedness, any two paths A1(t) and A2(t) joining
the identity to A will be homotopic with endpoints ï¬xed.
(This is a standard
topological fact.) Using this, we want to prove that Step 3 gives the same answer
for A1 and A2.
Our strategy is to deform A1 into A2 in a series of steps, where during each
step we only change the path in a small time interval (t, t + Ïµ), keeping everything
ï¬xed on [0, t] and on [t+Ïµ, 1]. Since we have independence of partition, we can take
t and t + Ïµ to be partition points. Since the time interval is small, we can assume
there are no partition points between t and t + Ïµ. Then we have
Ï† (A(t + Ïµ)) = Ï†
 A(t + Ïµ)A(t)âˆ’1
Ï† (A(t))
where Ï†
 A(t + Ïµ)A(t)âˆ’1
is deï¬ned as in Step 2.
But notice that our value for Ï† (A(t + Ïµ)) depends only on A (t) and A (t + Ïµ),
not on how we get from A (t) to A (t + Ïµ)!
Thus the value Ï† (A(t + Ïµ)) doesnâ€™t
change as we deform the path. But if Ï† (A(t + Ïµ)) doesnâ€™t change as we deform the
path, neither does Ï† (A(1)), since the path isnâ€™t changing on [t + Ïµ, 1].
Since A1 and A2 are homotopic with endpoints ï¬xed, it is possible (by a stan-
dard topological argument) to deform A1 into A2 in a series of small steps as above.
Step 5: Prove that Ï† is a homomorphism, and is properly related to eÏ†.
Now that we have independence of path (and partition), we can give a simpler
description of how to compute Ï†. Given any group element A, A can be written in

94
5. BASIC REPRESENTATION THEORY
the form
A = CnCnâˆ’1 Â· Â· Â· C1
with each Ci in V . (This follows from the (path-)connectedness of G.) We can then
choose a path A(t) which starts at the identity, then goes to C1, then to C2C1, and so
on to CnCnâˆ’1 Â· Â· Â·C1 = A. We can choose a partition so that A(ti) = CiCiâˆ’1 Â· Â· Â·C1.
By the way we have deï¬ned things
Ï†(A) = Ï†
 A(1)A(tnâˆ’1)âˆ’1
Ï†
 A(tnâˆ’1)A(tnâˆ’2)âˆ’1
Â· Â· Â·Ï† (A(t1)A(0)) .
But
A(ti)A(tiâˆ’1)âˆ’1 = (CiCiâˆ’1 Â· Â· Â· C1) (Ciâˆ’1 Â· Â· Â·C1)âˆ’1 = Ci
so
Ï†(A) = Ï†(Cn)Ï†(Cnâˆ’1) Â· Â· Â·Ï†(C1).
Now suppose that A and B are two elements of G and we wish to compute
Ï†(AB). Well, write
A = CnCnâˆ’1 Â· Â· Â· C1
B = DnDnâˆ’1 Â· Â· Â· D1.
Then
Ï† (AB) = Ï† (CnCnâˆ’1 Â· Â· Â·C1DnDnâˆ’1 Â· Â· Â·D1)
= [Ï†(Cn) Â· Â· Â·Ï†(C1)] [Ï†(Dn) Â· Â· Â· Ï†(D1)]
= Ï†(A)Ï†(B).
We see then that Ï† is a homomorphism.
It remains only to verify that Ï†
has the proper relationship to eÏ†. But since Ï† is deï¬ned near the identity to be
Ï† = exp â—¦eÏ† â—¦log, we see that
d
dt

t=0
Ï†
 etX
= d
dt

t=0
eteÏ†(X) = eÏ†(X).
Thus eÏ† is the Lie algebra homomorphism associated to the Lie group homomor-
phism Ï†.
This completes the proof of Theorem 5.33.
9. Covering Groups
It is at this point that we pay the price for our decision to consider only matrix
Lie groups. For the universal covering group of a matrix Lie group (deï¬ned below)
is always a Lie group, but not always a matrix Lie group. For example, the universal
covering group of SL (n; R) (n â‰¥2) is a Lie group, but not a matrix Lie group. (See
Exercise 20.)
The notion of a universal cover allows us to determine, in the case of a non-
simply connected group, which representations of the Lie algebra correspond to
representations of the group. See Theorem 5.41 below.
Definition 5.36. Let G be a connected matrix Lie group. A universal cov-
ering group of G (or just universal cover) is a connected, simply connected
Lie group eG, together with a Lie group homomorphism Ï† : eG â†’G (called the
projection map) with the following properties:

9. COVERING GROUPS
95
1. Ï† maps eG onto G.
2. There is a neighborhood U of I in eG which maps homeomorphically under
Ï† onto a neighborhood V of I in G.
Proposition 5.37. If G is any connected matrix Lie group, then a universal
covering group eG of G exists and is unique up to canonical isomorphism.
We will not prove this theorem, but the idea of proof is as follows. We assume
that G is a matrix Lie group, hence a Lie group (that is, a manifold). As a mani-
fold, G has a topological universal cover eG which is a connected, simply connected
manifold. The universal cover comes with a â€œprojection mapâ€ Ï† : eG â†’G which is
a local homeomorphism. Now, since G is not only a manifold but also a group, eG
also becomes a group, and the projection map Ï† becomes a homomorphism.
Proposition 5.38. Let G be a connected matrix Lie group, eG its universal
cover, and Ï† the projection map from eG to G. Suppose that eG is a matrix Lie
group with Lie algebra eg. Then the associated Lie algebra map
eÏ† : eg â†’g
is an isomorphism.
In light of this Proposition, we often say that G and eG have the same Lie
algebra.
The above Proposition is true even if eG is not a matrix Lie group. But to make
sense out of the Proposition in that case, we need the deï¬nition of the Lie algebra
of a general Lie group, which we have not deï¬ned.
Proof. Exercise 18.
9.1. Examples. The universal cover of S1 is R, and the projection map is the
map x â†’eix. The universal cover of SO(3) is SU(2), and the projection map is the
homomorphism described in Lemma 5.32.
More generally, we can consider SO(n) for n â‰¥3. As it turns out, for n â‰¥3 the
universal cover of SO(n) is a double cover. (That is, the projection map Ï† is two-to-
one.) The universal cover of SO(n) is called Spin(n), and may be constructed as a
certain group of invertible elements in the Cliï¬€ord algebra over Rn. See BrÂ¨ocker
and tom Dieck, Chapter I, Section 6, especially Propositions I.6.17 and I.6.19. In
particular, Spin(n) is a matrix Lie group.
The case n = 4 is quite special. It turns out that the universal cover of SO(4)
(i.e., Spin(4)) is isomorphic to SU(2) Ã— SU(2). This is best seen by regarding R4 as
the quaternion algebra.
Theorem 5.39. Let G be a matrix Lie group, and suppose that eG is also a
matrix Lie group. Identify the Lie algebra of eG with the Lie algebra g of G as in
Proposition 5.38. Suppose that H is a matrix Lie group with Lie algebra h, and that
eÏ† : g â†’h is a homomorphism. Then there exists a unique Lie group homomorphism
Ï† : eG â†’H such that Ï† and eÏ† are related as in Theorem 3.18 of Chapter 3.
Proof. eG is simply connected.

96
5. BASIC REPRESENTATION THEORY
Corollary 5.40. Let G and eG be as in Theorem 5.39, and let Ï€ be a repre-
sentation of g. Then there exists a unique representation eÎ  of eG such that
Ï€(X) = d
dt

t=0
eÎ   etX
for all X âˆˆg.
Theorem 5.41. Let G and eG be as in Theorem 5.39, and let Ï† : eG â†’G. Now
let Ï€ be a representation of g, and eÎ  the associated representation of eG, as in the
Corollary. Then there exists a representation Î  of G corresponding to Ï€ if and only
if
ker eÎ  âŠƒker Ï†.
Proof. Exercise 19.
10. Exercises
1. Let G be a matrix Lie group, and g its Lie algebra. Let Î 1 and Î 2 be
representations of G, and let Ï€1 and Ï€2 be the associated representations of
g (Proposition 5.4). Show that if Î 1 and Î 2 are equivalent representations
of G, then Ï€1 and Ï€2 are equivalent representations of g. Show that if G is
connected, and if Ï€1 and Ï€2 are equivalent representations of g, then Î 1 and
Î 2 are equivalent representations of G.
Hint: Use Corollary 3.26 of Chapter 3.
2. Let G be a connected matrix Lie group with Lie algebra g. Let Î  be a
representation of G acting on a space V , and let Ï€ be the associated Lie
algebra representation. Show that a subspace W âŠ‚V is invariant for Î  if
and only if it is invariant for Ï€. Show that Î  is irreducible if and only if Ï€
is irreducible.
3. Suppose that Î  is a ï¬nite-dimensional unitary representation of a matrix
Lie group G. (That is, V is a ï¬nite-dimensional Hilbert space, and Î  is
a continuous homomorphism of G into U(V ).)
Let Ï€ be the associated
representation of the Lie algebra g. Show that for each X âˆˆg, Ï€(X)âˆ—=
âˆ’Ï€(X).
4. Show explicitly that the adjoint representation and the standard represen-
tation are equivalent representations of the Lie algebra so(3). Show that the
adjoint and standard representations of the group SO(3) are equivalent.
5. Consider the elements E1, E2, and E3 in su(2) deï¬ned in Exercise 9 of
Chapter 3.
These elements form a basis for the real vector space su(2).
Show directly that E1, E2, and E3 form a basis for the complex vector space
sl(2; C).
6. Deï¬ne a vector space with basis u0, u1 Â· Â· Â·um. Now deï¬ne operators Ï€(H),
Ï€(X), and Ï€(Y ) by formula (5.10). Verify by direct computation that the op-
erators deï¬ned by (5.10) satisfy the commutation relations [Ï€(H), Ï€(X)] =
2Ï€(X), [Ï€(H), Ï€(Y )] = âˆ’2Ï€(Y ), and [Ï€(X), Ï€(Y )] = Ï€(H). (Thus Ï€(H),
Ï€(X), and Ï€(Y ) deï¬ne a representation of sl(2; C).) Show that this repre-
sentation is irreducible.
Hint: It suï¬ƒces to show, for example, that [Ï€(H), Ï€(X)] = 2Ï€(X) on
each basis element. When dealing with Ï€(Y ), donâ€™t forget to treat separately
the case of uk, k < m, and the case of um.

10. EXERCISES
97
7. We can deï¬ne a two-dimensional representation of so(3) as follows:
Ï€
ï£«
ï£­
0
0
0
0
0
1
0
âˆ’1
0
ï£¶
ï£¸= 1
2
 i
0
0
âˆ’i

;
Ï€
ï£«
ï£­
0
0
1
0
0
0
âˆ’1
0
0
ï£¶
ï£¸= 1
2

0
1
âˆ’1
0

;
Ï€
ï£«
ï£­
0
1
0
âˆ’1
0
0
0
0
0
ï£¶
ï£¸= 1
2
 0
i
i
0

.
(You may assume that this actually gives a representation.) Show that there
is no group representation Î  of SO(3) such that Î  and Ï€ are related as in
Proposition 5.4.
Hint: If X âˆˆso(3) is such that eX = I, and Î  is any representation of
SO(3), then Î (eX) = Î (I) = I.
Remark: In the physics literature, this non-representation of SO(3) is
called â€œspin 1
2.â€
8. Consider the standard representation of the Heisenberg group, acting on C3.
Determine all subspaces of C3 which are invariant under the action of the
Heisenberg group. Is this representation completely reducible?
9. Give an example of a representation of the commutative group R which is
not completely reducible.
10. Consider the unitary representations Î â„of the real Heisenberg group. As-
sume that there is some sort of associated representation Ï€â„of the Lie alge-
bra, which should be given by
Ï€â„(X)f = d
dt

t=0
Î â„
 etX
f
(We have not proved any theorem of this sort for inï¬nite-dimensional unitary
representations.)
Computing in a purely formal manner (that is, ignoring all technical
issues) compute
Ï€â„
ï£«
ï£­
0
1
0
0
0
0
0
0
0
ï£¶
ï£¸;
Ï€â„
ï£«
ï£­
0
0
0
0
0
1
0
0
0
ï£¶
ï£¸;
Ï€â„
ï£«
ï£­
0
0
1
0
0
0
0
0
0
ï£¶
ï£¸.
Verify (still formally) that these operators have the right commutation rela-
tions to generate a representation of the Lie algebra of the real Heisenberg
group. (That is, verify that on this basis, Ï€â„[X, Y ] = [Ï€â„(X), Ï€â„(Y )].)
Why is this computation not rigorous?
11. Consider the Heisenberg group over the ï¬eld Zp of integers mod p, with p
prime, namely
Hp =
ï£±
ï£²
ï£³
ï£«
ï£­
1
a
b
0
1
c
0
0
1
ï£¶
ï£¸|a, b, c âˆˆZp
ï£¼
ï£½
ï£¾.
This is a subgroup of the group GL (3; Zp), and has p3 elements.

98
5. BASIC REPRESENTATION THEORY
Let Vp denote the space of complex-valued functions on Zp, which is
a p-dimensional complex vector space. For each non-zero n âˆˆZp, deï¬ne a
representation of Hp by the formula
(Î nf) (x) = eâˆ’i2Ï€nb/pei2Ï€ncx/pf (x âˆ’a) x âˆˆZp.
(These representations are analogous to the unitary representations of the
real Heisenberg group, with the quantity 2Ï€n/p playing the role of â„.)
a) Show that for each n, Î n is actually a representation of Hp, and that
it is irreducible.
b) Determine (up to equivalence) all the one-dimensional representations
of Hp.
c) Show that every irreducible representation of Hp is either one-dimensional
or equivalent to one of the Î nâ€™s.
12. Prove Theorem 5.19.
Hints: For existence, choose bases {ei} and {fj} for U and V . Then
deï¬ne a space W which has as a basis {wij |0 â‰¤i â‰¤n, 0 â‰¤j â‰¤m}. Deï¬ne
Ï†(ei, fj) = wij and extend by bilinearity. For uniqueness, use the universal
property.
13. Let g and h be Lie algebras, and consider the vector space g âŠ•h. Show that
the following operation makes g âŠ•h into a Lie algebra
[(X1, Y1), (X2, Y2)] = ([X1, X2], [Y1, Y2]).
Now let G and H be matrix Lie groups, with Lie algebras g and h. Show
that G Ã— H can be regarded as a matrix Lie group in an obvious way, and
that the Lie algebra of G Ã— H is isomorphic to g âŠ•h.
14. Suppose that Ï€ is a representation of a Lie algebra g acting on a ï¬nite-
dimensional vector space V . Let V âˆ—denote as usual the dual space of V ,
that is, the space of linear functionals on V . If A is a linear operator on V ,
let Atr denote the dual or transpose operator on V âˆ—,
 AtrÏ†

(v) = Ï† (Av)
for Ï† âˆˆV âˆ—, v âˆˆV . Deï¬ne a representation Ï€âˆ—of g on V âˆ—by the formula
Ï€âˆ—(X) = âˆ’Ï€
 Xtr
.
a) Show that Ï€âˆ—is really a representation of g.
b) Show that (Ï€âˆ—)âˆ—is isomorphic to Ï€.
c) Show that Ï€âˆ—is irreducible if and only if Ï€ is.
d) What is the analogous construction of the dual representation for
representations of groups?
15. Recall the spaces Vm introduced in Section 3, viewed as representations of
the Lie algebra sl(2; C).
In particular, consider the space V1 (which has
dimension 2).
a) Regard V1 âŠ—V1 as a representation of sl(2; C), as in Deï¬nition 5.27.
Show that this representation is not irreducible.
b) Now view V1 âŠ—V1 as a representation of sl(2; C) âŠ•sl(2; C), as in
Deï¬nition 5.24. Show that this representation is irreducible.
c) More generally, show that Vm âŠ—Vn is irreducible as a representation
of sl(2; C) âŠ•sl(2; C), but reducible (except if one of n or m is zero) as a
representation of sl(2; C).

10. EXERCISES
99
16. Show explicitly that exp : so(3) â†’SO(3) is onto.
Hint: Using the fact that SO(3) âŠ‚SU(3), show that the eigenvalues of
R âˆˆSO(3) must be of one of the three following forms: (1, 1, 1), (1, âˆ’1, âˆ’1),
or (1, eiÎ¸, eâˆ’iÎ¸). In particular, R must have an eigenvalue equal to one. Now
show that in a suitable orthonormal basis, R is of the form
R =
ï£«
ï£­
1
0
0
0
cos Î¸
sin Î¸
0
âˆ’sin Î¸
cos Î¸
ï£¶
ï£¸.
17. Proof of Lemma 5.32.
Let {E1, E2, E3} be the usual basis for su(2), and {F1, F2, F3} be the
basis for so(3) introduced in Section 8. Identify su(2) with R3 by identifying
the basis {E1, E2, E3} with the standard basis for R3. Consider adE1, adE2,
and adE3 as operators on su(2), hence on R3. Show that adEi = Fi, for
i = 1, 2, 3. In particular, ad is a Lie algebra isomorphism of su(2) onto so(3).
Now consider Ad : SU(2) â†’GL (SU(2)) = GL (3; R). Show that the
image of Ad is precisely SO(3). Show that the kernel of Ad is {I, âˆ’I}.
Show that Ad : SU(2) â†’SO(3) is the homomorphism Î¦ required by
Lemma 5.32.
18. Proof of Proposition 5.38.
Suppose that G and eG are matrix Lie groups. Suppose that Ï† : eG â†’G
is a Lie group homomorphism such that Ï† maps some neighborhood U of I
in eG homeomorphically onto a neighborhood V of I in G. Prove that the
associated Lie algebra map eÏ† : eg â†’g is an isomorphism.
Hints: Suppose that eÏ† were not one-to-one.
Show, then, that there
exists a sequence of points An in eG with An Ì¸= I, An â†’I and Ï†(An) = I,
giving a contradiction.
To show that eÏ† is onto, use Step 1 of the proof of Theorem 5.33 to show
that on a suï¬ƒciently small neighborhood of zero in eg,
eÏ† = log â—¦Ï† â—¦exp .
Use this to show that the image of eÏ† contains a neighborhood of zero in g.
Now use linearity to show that the image of eÏ† is all of g.
19. Proof of Theorem 5.41.
First suppose that ker eÎ  âŠƒker Ï†. Then construct Î  as in the proof of
Proposition 5.31.
Now suppose that there is a representation Î  of G for which the as-
sociated Lie algebra representation is Ï€.
We want to show, then, that
ker eÎ  âŠƒker Ï†. Well, deï¬ne a new representation Î£ of eG by
Î£ = Î  â—¦Ï†.
Show that the associated Lie algebra homomorphism Ïƒ is equal to Ï€, so that,
by Point (1) of Theorem 5.33, eÎ  = Î£. What can you say about the kernel
of Î£?
20. Fix an integer n â‰¥2.
a) Show that every (ï¬nite-dimensional complex) representation of the
Lie algebra sl (n; R) gives rise to a representation of the group SL (n; R),
even though SL (n; R) is not simply connected. (You may use the fact that
SL (n; C) is simply connected.)

100
5. BASIC REPRESENTATION THEORY
b) Show that the universal cover of SL (n; R) is not isomorphic to any
matrix Lie group. (You may use the fact that SL (n; R) is not simply con-
nected.)
21. Let G be a matrix Lie group with Lie algebra g, let h be a subalgebra of g,
and let H be the unique connected Lie subgroup of G with Lie algebra h.
Suppose that there exists a compact simply connected matrix Lie group K
such that the Lie algebra of K is isomorphic to h. Show that H is closed.
Is H necessarily isomorphic to K?

CHAPTER 6
The Representations of SU(3), and Beyond
1. Preliminaries
There is a theory of the representations of semisimple groups/Lie algebras which
includes as a special case the representation theory of SU(3). However, I feel that
it is worthwhile to examine the case of SU(3) separately. I feel this way partly
because SU(3) is an important group in physics, but chieï¬‚y because the general
semisimple theory is diï¬ƒcult to digest. Considering a non-trivial example makes it
much clearer what is going on. In fact, all of the elements of the general theory are
present already in the case of SU(3), so we do not lose too much by considering at
ï¬rst just this case.
The main result of this chapter is Theorem 1, which states that an irreducible
ï¬nite-dimensional representation of SU(3) can be classiï¬ed in terms of its â€œhigh-
est weight.â€ This is analogous to labeling the irreducible representations Vm of
SU(2)/sl(2; C) by the highest eigenvalue of Ï€m(H).
(The highest eigenvalue of
Ï€m(H) in Vm is precisely m.)
We will then discuss, without proofs, what the
corresponding results are for general semisimple Lie algebras.
The group SU(3) is connected and simply connected (BrÂ¨ocker and tom Dieck),
so by Corollary 1 of Chapter 5, the ï¬nite-dimensional representations of SU(3)
are in one-to-one correspondence with the ï¬nite-dimensional representations of the
Lie algebra su(3). Meanwhile, the complex representations of su(3) are in one-to-
one correspondence with the complex-linear representations of the complexiï¬ed Lie
algebra su(3)C. But su(3)C âˆ¼= sl (3; C), as is easily veriï¬ed. Moreover, since SU(3)
is connected, it follows that a subspace W âŠ‚V is invariant under the action of
SU(3) if and only if it is invariant under the action of sl (3; C). Thus we have the
following:
Proposition 6.1. There is a one-to-one correspondence between the ï¬nite-
dimensional complex representations Î  of SU(3) and the ï¬nite-dimensional complex-
linear representations Ï€ of sl (3; C). This correspondence is determined by the prop-
erty that
Î 
 eX
= eÏ€(X)
for all X âˆˆsu(3) âŠ‚sl(3; C).
The representation Î  is irreducible if and only the representation Ï€ is irre-
ducible. Moreover, a subspace W âŠ‚V is invariant for Î  if and only if it is invariant
for Ï€.
Since SU(3) is compact, Proposition 5.17 of Chapter 5 tells us that all the
ï¬nite-dimensional representations of SU(3) are completely reducible.
The above
proposition then implies that all the ï¬nite-dimensional representations of sl (3; C)
are completely reducible.
101

102
6. THE REPRESENTATIONS OF SU(3), AND BEYOND
Moreover, we can apply the same reasoning to the group SU(2), its Lie algebra
su(2), and its complexiï¬ed Lie algebra sl(2; C). Since SU(2) is simply connected,
there is a one-to-one correspondence between the complex representations of SU(2)
and the representations of the complexiï¬ed Lie algebra sl(2; C). Since SU(2) is com-
pact, all of the representations of SU(2)â€“and therefore also of sl(2; C)â€“are completely
reducible. Thus we have established the following.
Proposition 6.2. Every ï¬nite-dimensional (complex-linear) representation of
sl(2; C) or sl (3; C) is completely reducible. In particular, every ï¬nite-dimensional
representation of sl(2; C) or sl (3; C) decomposes as a direct sum of irreducible in-
variant subspaces.
We will use the following basis for sl (3; C):
H1 =
ï£«
ï£­
1
0
0
0
âˆ’1
0
0
0
0
ï£¶
ï£¸
H2 =
ï£«
ï£­
0
0
0
0
1
0
0
0
âˆ’1
ï£¶
ï£¸
X1 =
ï£«
ï£­
0
1
0
0
0
0
0
0
0
ï£¶
ï£¸
X2 =
ï£«
ï£­
0
0
0
0
0
1
0
0
0
ï£¶
ï£¸
X3 =
ï£«
ï£­
0
0
1
0
0
0
0
0
0
ï£¶
ï£¸
Y1 =
ï£«
ï£­
0
0
0
1
0
0
0
0
0
ï£¶
ï£¸
Y2 =
ï£«
ï£­
0
0
0
0
0
0
0
1
0
ï£¶
ï£¸
Y3 =
ï£«
ï£­
0
0
0
0
0
0
1
0
0
ï£¶
ï£¸.
Note that the span of {H1, X1, Y1} is a subalgebra of sl (3; C) which is isomor-
phic to sl(2; C), by ignoring the third row and the third column. Similarly, the span
of {H2, X2, Y2} is a subalgebra isomorphic to sl(2; C), by ignoring the ï¬rst row and
ï¬rst column. Thus we have the following commutation relations
[H1, X1]
=
2X1
[H2, X2]
=
2X2
[H1, Y1]
=
âˆ’2Y1
[H2, Y2]
=
âˆ’2Y2
[X1, Y1]
=
H1
[X2, Y2]
=
H2.
We now list all of the commutation relations among the basis elements which
involve at least one of H1 and H2. (This includes some repetitions of the commu-
tation relations above.)
[H1, H2]
=
0
[H1, X1]
=
2X1
[H1, Y1]
=
âˆ’2Y1
[H2, X1]
=
âˆ’X1
[H2, Y1]
=
Y1
[H1, X2]
=
âˆ’X2
[H1, Y2]
=
Y2
[H2, X2]
=
2X2
[H2, Y2]
=
âˆ’2Y2
[H1, X3]
=
X3
[H1, Y3]
=
âˆ’Y3
[H2, X3]
=
X3
[H2, Y3]
=
âˆ’Y3
(6.1)

2. WEIGHTS AND ROOTS
103
We now list all of the remaining commutation relations.
[X1, Y1]
=
H1
[X2, Y2]
=
H2
[X3, Y3]
=
H1 + H2
[X1, X2]
=
X3
[Y1, Y2]
=
âˆ’Y3
[X1, Y2]
=
0
[X2, Y1]
=
0
[X1, X3]
=
0
[Y1, Y3]
=
0
[X2, X3]
=
0
[Y2, Y3]
=
0
[X2, Y3]
=
Y1
[X3, Y2]
=
X1
[X1, Y3]
=
âˆ’Y2
[X3, Y1]
=
âˆ’X2
Note that there is a kind of symmetry between the Xiâ€™s and the Yiâ€™s. If a
relation in the ï¬rst column involves an Xi and/or a Yj, the corresponding relation
in the second column will involve a Yi and/or an Xj. (E.g., we have the relation
[H1, X2] = âˆ’X2 in the ï¬rst column, and the relation [H2, Y2] = Y2 in the second
column.) See Exercise 1.
All of the analysis we will do for the representations of sl (3; C) will be in terms
of the above basis. From now on, all representations of sl (3; C) will be assumed to
be ï¬nite-dimensional and complex-linear.
2. Weights and Roots
Our basic strategy in classifying the representations of sl (3; C) is to simultane-
ously diagonalize Ï€(H1) and Ï€(H2). Since H1 and H2 commute, Ï€(H1) and Ï€(H2)
will also commute, and so there is at least a chance that Ï€(H1) and Ï€(H2) can be
simultaneously diagonalized.
Definition 6.3. If (Ï€, V ) is a representation of sl(3; C), then an ordered pair
Âµ = (Âµ1, Âµ2) âˆˆC2 is called a weight for Ï€ if there exists v Ì¸= 0 in V such that
Ï€(H1)v = Âµ1v
Ï€(H2)v = Âµ2v.
(6.2)
The vector v is called a weight vector corresponding to the weight Âµ.
If Âµ =
(Âµ1, Âµ2) is a weight, then the space of all vectors v satisfying (6.2) is the weight
space corresponding to the weight Âµ.
Thus a weight is simply a pair of simultaneous eigenvalues for Ï€(H1) and Ï€(H2).
Proposition 6.4. Every representation of sl(3; C) has at least one weight.
Proof. Since we are working over the complex numbers, Ï€(H1) has at least
one eigenvalue Âµ1. Let W âŠ‚V be the eigenspace for Ï€(H1) with eigenvalue Âµ1. I
assert that W is invariant under Ï€(H2). To see this consider w âˆˆW, and compute
Ï€(H1) (Ï€(H2)w) = Ï€(H2)Ï€(H1)w
= Ï€(H2) (Âµ1w) = Âµ1Ï€(H2)w.
This shows that Ï€(H2)w is either zero or an eigenvector for Ï€(H1) with eigenvalue
Âµ1; thus W is invariant.

104
6. THE REPRESENTATIONS OF SU(3), AND BEYOND
Thus Ï€(H2) can be viewed as an operator on W. Again, since we are over C,
the restriction of Ï€(H2) to W must have at least one eigenvector w with eigenvalue
Âµ2. But then w is a simultaneous eigenvector for Ï€(H1) and Ï€(H2) with eigenvalues
Âµ1 and Âµ2.
Now, every representation Ï€ of sl (3; C) can be viewed, by restriction, as a
representation of the subalgebra {H1, X1, Y1} âˆ¼= sl(2; C). Note that, even if Ï€ is
irreducible as a representation of sl(3; C), there is no reason to expect that it will
still be irreducible as a representation of the subalgebra {H1, X1, Y1}. Neverthe-
less, Ï€ restricted to {H1, X1, Y1} must be some ï¬nite-dimensional representation
of sl(2; C). The same reasoning applies to the restriction of Ï€ to the subalgebra
{H2, X2, Y2}, which is also isomorphic to sl(2; C).
Proposition 6.5. Let (Ï€, V ) be any ï¬nite-dimensional complex-linear repre-
sentation of sl(2; C) = {H, X, Y }. Then all the eigenvalues of Ï€(H) are integers.
Proof. By Proposition 6.2, V decomposes as a direct sum of irreducible in-
variant subspaces Vi. Each Vi must be one of the irreducible representations of
sl(2; C), which we have classiï¬ed. In particular, in each Vi, Ï€(H) can be diagonal-
ized, and the eigenvalues of Ï€(H) are integers. Thus Ï€(H) can be diagonalized on
the whole space V , and all of the eigenvalues are integers.
Corollary 6.6. If Ï€ is a representation of sl (3; C), then all of the weights of
Ï€ are of the form
Âµ = (m1, m2)
with m1 and m2 integers.
Proof. Apply Proposition 6.5 to the restriction of Ï€ to {H1, X1, Y1}, and to
the restriction of Ï€ to {H2, X2, Y2}.
Our strategy now is to begin with one simultaneous eigenvector for Ï€(H1) and
Ï€(H2), and then to apply Ï€(Xi) or Ï€(Yi), and see what the eï¬€ect is. The following
deï¬nition is relevant in this context. (See Lemma 6.8 below.)
Definition 6.7. An ordered pair Î± = (Î±1, Î±2) âˆˆC2 is called a root if
1. Î±1 and Î±2 are not both zero, and
2. there exists Z âˆˆsl(3; C) such that
[H1, Z] = Î±1Z
[H2, Z] = Î±2Z.
The element Z is called a root vector corresponding to the root Î±.
That is, a root is a non-zero weight for the adjoint representation. The com-
mutation relations (6.1) tell us what the roots for sl (3; C) are. There are six roots.
Î±
Z
(2, âˆ’1)
X1
(âˆ’1, 2)
X2
(1, 1)
X3
(âˆ’2, 1)
Y1
(1, âˆ’2)
Y2
(âˆ’1, âˆ’1)
Y3
(6.3)

3. HIGHEST WEIGHTS AND THE CLASSIFICATION THEOREM
105
It is convenient to single out the two roots corresponding to X1 and X2 and
give them special names:
Î±(1) = (2, âˆ’1)
Î±(2) = (âˆ’1, 2).
(6.4)
The roots Î±(1) and Î±(2) are called the simple roots. They have the property that
all of the roots can be expressed as linear combinations of Î±(1) and Î±(2) with integer
coeï¬ƒcients, and these coeï¬ƒcients are either all greater than or equal to zero or all
less than or equal to zero. This is veriï¬ed by direct computation:
(2, âˆ’1)
=
Î±(1)
(âˆ’1, 2)
=
Î±(2)
(1, 1)
=
Î±(1) + Î±(2)
(âˆ’2, 1)
=
âˆ’Î±(1)
(1, âˆ’2)
=
âˆ’Î±(2)
(âˆ’1, âˆ’1)
=
âˆ’Î±(1) âˆ’Î±(2).
The signiï¬cance of the roots for the representation theory of sl (3; C) is con-
tained in the following Lemma. Although its proof is very easy, this Lemma plays
a crucial role in the classiï¬cation of the representations of sl (3; C).
Note that
this Lemma is the analog of Lemma 5.10 of Chapter 5, which was the key to the
classiï¬cation of the representations of sl(2; C).
Lemma 6.8. Let Î± = (Î±1, Î±2) be a root, and ZÎ± Ì¸= 0 a corresponding root vector
in sl (3; C). Let Ï€ be a representation of sl (3; C), Âµ = (m1, m2) a weight for Ï€, and
v Ì¸= 0 a corresponding weight vector. Then
Ï€(H1)Ï€(ZÎ±)v = (m1 + Î±1)Ï€(ZÎ±)v
Ï€(H2)Ï€(ZÎ±)v = (m2 + Î±2)Ï€(ZÎ±)v.
Thus either Ï€(ZÎ±)v = 0 or else Ï€(ZÎ±)v is a new weight vector with weight
Âµ + Î± = (m1 + Î±1, m2 + Î±2).
Proof. The deï¬nition of a root tells us that we have the commutation relation
[H1ZÎ±] = Î±1ZÎ±. Thus
Ï€(H1)Ï€(ZÎ±)v = (Ï€(ZÎ±)Ï€(H1) + Î±1Ï€(Za)) v
= Ï€(ZÎ±)(m1v) + Î±1Ï€(ZÎ±)v
= (m1 + Î±1)Ï€(ZÎ±)v.
A similar argument allows us to compute Ï€(H2)Ï€(ZÎ±)v.
3. Highest Weights and the Classiï¬cation Theorem
We see then that if we have a representation with a weight Âµ = (m1, m2), then
by applying the root vectors X1, X2, X3, Y1, Y2, Y3 we can get some new weights of
the form Âµ + Î±, where Î± is the root. Of course, some of the weight vectors may
simply give zero. In fact, since our representation is ï¬nite-dimensional, there can
be only ï¬nitely many weights, so we must get zero quite often.
By analogy to
the classiï¬cation of the representations of sl(2; C), we would like to single out in
each representation a â€œhighestâ€ weight, and then work from there. The following
deï¬nition gives the â€œrightâ€ notion of highest.

106
6. THE REPRESENTATIONS OF SU(3), AND BEYOND
Definition 6.9. Let Î±(1) = (2, âˆ’1) and Î±(2) = (âˆ’1, 2) be the roots introduced
in (6.4). Let Âµ1 and Âµ2 be two weights. Then Âµ1 is higher than Âµ2 (or equivalently,
Âµ2 is lower than Âµ1) if Âµ1 âˆ’Âµ2 can be written in the form
Âµ1 âˆ’Âµ2 = aÎ±(1) + bÎ±(2)
with a â‰¥0 and b â‰¥0. This relationship is written as Âµ1 âª°Âµ2 or Âµ2 âª¯Âµ1.
If Ï€ is a representation of sl (3; C), then a weight Âµ0 for Ï€ is said to be a highest
weight if for all weights Âµ of Ï€, Âµ âª¯Âµ0.
Note that the relation of â€œhigherâ€ is only a partial ordering. That is, one can
easily have Âµ1 and Âµ2 such that Âµ1 is neither higher nor lower than Âµ2. For example,
Î±(1) âˆ’Î±(2) is neither higher nor lower than 0. This in particular means that a ï¬nite
set of weights need not have a highest element. (E.g., the set 0, Î±(1) âˆ’Î±(2)	 has
no highest element.)
We are now ready to state the main theorem regarding the irreducible repre-
sentations of sl (3; C).
Theorem 6.10.
1. Every irreducible representation Ï€ of sl (3; C) is the di-
rect sum of its weight spaces. That is, Ï€(H1) and Ï€(H2) are simultaneously
diagonalizable.
2. Every irreducible representation of sl (3; C) has a unique highest weight Âµ0,
and two equivalent irreducible representations have the same highest weight.
3. Two irreducible representations of sl (3; C) with the same highest weight are
equivalent.
4. If Ï€ is an irreducible representation of sl (3; C), then the highest weight Âµ0
of Ï€ is of the form
Âµ0 = (m1, m2)
with m1 and m2 non-negative integers.
5. Conversely, if m1 and m2 are non-negative integers, then there exists a
unique irreducible representation Ï€ of sl (3; C) with highest weight Âµ0 =
(m1, m2).
Note the parallels between this result and the classiï¬cation of the irreducible
representations of sl(2; C): In each irreducible representation of sl(2; C), Ï€(H) is
diagonalizable, and there is a largest eigenvalue of Ï€(H). Two irreducible repre-
sentations of sl(2; C) with the same largest eigenvalue are equivalent. The highest
eigenvalue is always a non-negative integer, and conversely, for every non-negative
integer m, there is an irreducible representation with highest eigenvalue m.
However, note that in the classiï¬cation of the representations of sl (3; C) the
notion of â€œhighestâ€ does not mean what we might have thought it should mean.
For example, the weight (1, 1) is higher than the weights (âˆ’1, 2) and (2, âˆ’1). (In
fact, (1, 1) is the highest weight for the adjoint representation, which is irreducible.)
It is possible to obtain much more information about the irreducible represen-
tations besides the highest weight. For example, we have the following formula for
the dimension of the representation with highest weight (m1, m2).
Theorem 6.11. The dimension of the irreducible representation with highest
weight (m1, m2) is
1
2(m1 + 1)(m2 + 1)(m1 + m2 + 2).

4. PROOF OF THE CLASSIFICATION THEOREM
107
We will not prove this formula. It is a consequence of the â€œWeyl character
formula.â€ See Humphreys, Section 24.3. Humphreys refers to sl (3; C) as A2.
4. Proof of the Classiï¬cation Theorem
It will take us some time to prove Theorem 1. The proof will consist of a series
of Propositions.
Proposition 6.12. In every irreducible representation (Ï€, V ) of sl (3; C), Ï€(H1)
and Ï€(H2) can be simultaneously diagonalized. That is, V is the direct sum of its
weight spaces.
Proof. Let W be the direct sum of the weight spaces in V . Equivalently, W is
the space of all vectors w âˆˆV such that w can be written as a linear combination of
simultaneous eigenvectors for Ï€(H1) and Ï€(H2). Since (Proposition 6.4) Ï€ always
has at least one weight, W Ì¸= {0}.
On the other hand, Lemma 6.8 tells us that if ZÎ± is a root vector corresponding
to the root Î±, then Ï€(ZÎ±) maps the weight space corresponding to Âµ into the weight
space corresponding to Âµ+Î±. Thus W is invariant under the action of all of the root
vectors, namely, under the action X1, X2, X3, Y1, Y2, and Y3. Since W is certainly
invariant under the action of H1 and H2, W is invariant. Thus by irreducibility,
W = V .
Definition 6.13. A representation (Ï€, V ) of sl (3; C) is said to be a highest
weight cyclic representation with weight Âµ0 = (m1, m2) if there exists v Ì¸= 0
in V such that
1. v is a weight vector with weight Âµ0.
2. Ï€(X1)v = Ï€(X2)v = 0.
3. The smallest invariant subspace of V containing v is all of V .
The vector v is called a cyclic vector for Ï€.
Proposition 6.14. Let (Ï€, V ) be a highest weight cyclic representation of sl (3; C)
with weight Âµ0. Then
1. Ï€ has highest weight Âµ0.
2. The weight space corresponding to the highest weight Âµ0 is one-dimensional.
4.0.1. Proof.
Proof. Let v be as in the deï¬nition. Consider the subspace W of V spanned
by elements of the form
w = Ï€(Yi1)Ï€(Yi2) Â· Â· Â·Ï€(Yin)v
(6.5)
with each il = 1, 2, and n â‰¥0. (If n = 0, it is understood that w in (6.5) is equal to
v.) I assert that W is invariant. To see this, it suï¬ƒces to check that W is invariant
under each of the basis elements.
By deï¬nition, W is invariant under Ï€(Y1) and Ï€(Y2). It is thus also invariant
under Ï€(Y3) = âˆ’[Ï€(Y1), Ï€(Y2)].
Now, Lemma 6.8 tells us that applying a root vector ZÎ± âˆˆsl (3; C) to a weight
vector v with weight Âµ gives either zero, or else a new weight vector with weight Âµ+
Î±. Now, by assumption, v is a weight vector with weight Âµ0. Furthermore, Y1 and
Y2 are root vectors with roots âˆ’Î±(1) = (âˆ’2, 1) and âˆ’Î±(2) = (1, âˆ’2), respectively.

108
6. THE REPRESENTATIONS OF SU(3), AND BEYOND
(See Equation (6.3).) Thus each application of Ï€(Y1) or Ï€(Y2) subtracts Î±(1) or
Î±(2) from the weight. In particular, each non-zero element of the form (6.5) is a
simultaneous eigenvector for Ï€(H1) and Ï€(H2). Thus W is invariant under Ï€(H1)
and Ï€(H2).
To show that W is invariant under Ï€(X1) and Ï€(X2), we argue by induction on
n. For n = 0, we have Ï€(X1)v = Ï€(X2)v = 0 âˆˆW. Now consider applying Ï€(X1)
or Ï€(X2) to a vector of the form (6.5). Recall the commutation relations involving
an X1 or X2 and a Y1 or Y2:
[X1, Y1]
=
H1
[X1, Y2]
=
0
[X2, Y1]
=
0
[X2, Y2]
=
H2.
Thus (for i and j equal to 1 or 2) Ï€(Xi)Ï€(Yj) = Ï€(Yj)Ï€(Xi) + Ï€(Hij), where Hij is
either H1 or H2 or zero. Hence (for i equal to 1 or 2)
Ï€(Xi)Ï€(Yi1)Ï€(Yi2) Â· Â· Â·Ï€(Yin)v
= Ï€(Yi1)Ï€(Xi)Ï€(Yi2) Â· Â· Â·Ï€(Yin)v + Ï€(Hij)Ï€(Yi2) Â· Â· Â· Ï€(Yin)v.
But Ï€(Xi)Ï€(Yi2) Â· Â· Â·Ï€(Yin)v is in W by induction, and Ï€(Hij)Ï€(Yi2) Â· Â· Â· Ï€(Yin)v is
in W since W is invariant under Ï€(H1) and Ï€(H2).
Finally, W is invariant under Ï€(X3) since Ï€(X3) = [Ï€(X1), Ï€(X2)]. Thus W is
invariant. Since by deï¬nition W contains v, we must have W = V .
Since Y1 is a root vector with root âˆ’Î±(1) and Y2 is a root vector with root
âˆ’Î±(2), Lemma 6.8 tells us that each element of the form (6.5) is either zero or a
weight vector with weight Âµ0 âˆ’Î±(i1) âˆ’Â· Â· Â· âˆ’Î±(in). Thus V = W is spanned by v
together with weight vectors with weights lower than Âµ0. Thus Âµ0 is the highest
weight for V .
Furthermore,every element of W can be written as a multiple of v plus a linear
combination of weight vectors with weights lower than Âµ0. Thus the weight space
corresponding to Âµ0 is spanned by v; that is, the weight space corresponding to Âµ0
is one-dimensional.
Proposition 6.15. Every irreducible representation of sl (3; C) is a highest
weight cyclic representation, with a unique highest weight Âµ0.
Proof. Uniqueness is immediate, since by the previous Proposition, Âµ0 is the
highest weight, and two distinct weights cannot both be highest.
We have already shown that every irreducible representation is the direct sum
of its weight spaces. Since the representation is ï¬nite-dimensional, there can be
only ï¬nitely many weights. It follows that there must exist a weight Âµ0 such that
there is no weight Âµ Ì¸= Âµ0 with Âµ âª°Âµ0. This says that there is no weight higher
than Âµ0 (which is not the same as saying the Âµ0 is highest). But if there is no
weight higher than Âµ0, then for any non-zero weight vector v with weight Âµ0, we
must have
Ï€(X1)v = Ï€(X2)v = 0.
(For otherwise, say, Ï€(X1)v will be a weight vector with weight Âµ0 + Î±(1) â‰»Âµ0.)
Since Ï€ is assumed irreducible, the smallest invariant subspace containing v
must be the whole space; therefore the representation is highest weight cyclic.
â–¡

4. PROOF OF THE CLASSIFICATION THEOREM
109
Proposition 6.16. Every highest weight cyclic representation of sl (3; C) is ir-
reducible.
Proof. Let (Ï€, V ) be a highest weight cyclic representation with highest weight
Âµ0 and cyclic vector v. By complete reducibility (Proposition 6.2), V decomposes
as a direct sum of irreducible representations
V âˆ¼=
M
i
Vi.
(6.6)
By Proposition 6.12, each of the Viâ€™s is the direct sum of its weight spaces.
Thus since the weight Âµ0 occurs in V , it must occur in some Vi. On the other hand,
Proposition 6.14 says that the weight space corresponding to Âµ0 is one-dimensional,
that is, v is (up to a constant) the only vector in V with weight Âµ0. Thus Vi must
contain v. But then that Vi is an invariant subspace containing v, so Vi = V . Thus
there is only one term in the sum (6.6), and V is irreducible.
Proposition 6.17. Two irreducible representations of sl(3; C) with the same
highest weight are equivalent.
Proof. We now know that a representation is irreducible if and only if it is
highest weight cyclic. Suppose that (Ï€, V ) and (Ïƒ, W) are two such representations
with the same highest weight Âµ0. Let v and w be the cyclic vectors for V and
W, respectively. Now consider the representation V âŠ•W, and let U be smallest
invariant subspace of V âŠ•W which contains the vector (v, w).
By deï¬nition, U is a highest weight cyclic representation, therefore irreducible
by Proposition.
6.16. Consider the two â€œprojectionâ€ maps P1 : V âŠ•W â†’V ,
P1(v, w) = v and P2 : V âŠ•W â†’W, P1(v, w) = w. It is easy to check that P1 and
P2 are morphisms of representations. Therefore the restrictions of P1 and P2 to
U âŠ‚V âŠ•W will also be morphisms.
Clearly neither P1|U nor P2|U is the zero map (since both are non-zero on
(v, w)). Moreover, U, V , and W are all irreducible. Therefore, by Schurâ€™s Lemma,
P1|U is an isomorphism of U with V , and P2|U is an isomorphism of U with W.
Thus V âˆ¼= U âˆ¼= W.
Proposition 6.18. If Ï€ is an irreducible representation of sl (3; C), then the
highest weight of Ï€ is of the form
Âµ = (m1, m2)
with m1 and m2 non-negative integers.
Proof. We already know that all of the weights of Ï€ are of the form (m1, m2),
with m1 and m2 integers. We must show that if Âµ0 = (m1, m2) is the highest weight,
then m1 and m2 are both non-negative. For this, we again use what we know about
the representations of sl(2; C). The following result can be obtained from the proof
of the classiï¬cation of the irreducible representations of sl(2; C).
Let (Ï€, V ) be any ï¬nite-dimensional representation of sl(2; C).
Let v be an
eigenvector for Ï€(H) with eigenvalue Î». If Ï€(X)v = 0, then Î» is a non-negative
integer.
Now, if Ï€ is an irreducible representation of sl (3; C) with highest weight Âµ0 =
(m1, m2), and if v Ì¸= 0 is a weight vector with weight Âµ0, then we must have
Ï€(X1)v = Ï€(X2)v = 0. (Otherwise, Âµ0 wouldnâ€™t be highest.) Thus applying the

110
6. THE REPRESENTATIONS OF SU(3), AND BEYOND
above result to the restrictions of Ï€ to {H1, X1, Y1} and to {H2, X2, Y2} shows that
m1 and m2 must be non-negative.
Proposition 6.19. If m1 and m2 are non-negative integers, then there exists
an irreducible representation of sl (3; C) with highest weight Âµ = (m1, m2).
Proof. Note that the trivial representation is an irreducible representation
with highest weight (0, 0). So we need only construct representations with at least
one of m1 and m2 positive.
First, we construct two irreducible representations with highest weights (1, 0)
and (0, 1). (These are the so-called fundamental representations.) The stan-
dard representation of sl (3; C) is an irreducible representation with highest weight
(1, 0), as is easily checked. To construct an irreducible representation with weight
(0, 1) we modify the standard representation. Speciï¬cally, we deï¬ne
Ï€(Z) = âˆ’Ztr
(6.7)
for all Z âˆˆsl (3; C). Using the fact that (AB)tr = BtrAtr, it is easy to check that
âˆ’[Z1, Z2]tr = âˆ’Ztr
1 , âˆ’Ztr
2

so that Ï€ is really a representation. (This is isomorphic to the dual of the standard
representation, as deï¬ned in Exercise 14 of Chapter 5.) It is easy to see that Ï€ is
an irreducible representation with highest weight (0, 1).
Let (Ï€1, V1) denote C3 acted on by the standard representation, and let v1 de-
note a weight vector corresponding to the highest weight (1, 0). (So, v1 = (1, 0, 0).)
Let (Ï€2, V2) denote C3 acted on by the representation (6.7), and let v2 denote a
weight vector for the highest weight (0, 1). (So, v2 = (0, 0, 1).) Now consider the
representation
V1 âŠ—V1 Â· Â· Â· âŠ—V1 âŠ—V2 âŠ—V2 Â· Â· Â·V2
where V1 occurs m1 times, and V2 occurs m2 times. Note that the action of sl (3; C)
on this space is
Z â†’(Ï€1(Z) âŠ—I Â· Â· Â· âŠ—I)
+ (I âŠ—Ï€1(Z) âŠ—I Â· Â· Â· âŠ—I) + Â· Â· Â· + (I âŠ—Â· Â· Â·I âŠ—Ï€2(Z)) .
(6.8)
Let Ï€m1,m2 denote this representation.
Consider the vector
vm1,m2 = v1 âŠ—v1 Â· Â· Â· âŠ—v1 âŠ—v2 âŠ—v2 Â· Â· Â· âŠ—v2.
Then applying (6.8) shows that
Ï€m1,m2(H1)vm1,m2 = m1vm1,m2
Ï€m1,m2(H2)vm1,m2 = m2vm1,m2
Ï€m1,m2(X1)vm1,m2 = 0
Ï€m1,m2(X2)vm1,m2 = 0.
(6.9)
Now, the representation Ï€m1,m2 is not irreducible (unless (m1, m2) = (1, 0) or
(0, 1)). However, if we let W denote the smallest invariant subspace containing the
vector vm1,m2, then in light of (6.9), W will be highest weight cyclic with highest
weight (m1, m2).
Therefore by Proposition 6.16, W is irreducible with highest
weight (m1, m2).
Thus W is the representation we want.

5. AN EXAMPLE: HIGHEST WEIGHT (1, 1)
111
We have now completed the proof of Theorem 1.
5. An Example: Highest Weight (1, 1)
To obtain the irreducible representation with highest weight (1, 1) we are sup-
posed to take the tensor product of the irreducible representations with highest
weights (1, 0) and (0, 1), and then extract a certain invariant subspace.
Let us
establish some notation for the representations (1, 0) and (0, 1). In the standard
representation, the weight vectors for
H1 =
ï£«
ï£­
1
0
0
0
âˆ’1
0
0
0
0
ï£¶
ï£¸;
H2 =
ï£«
ï£­
0
0
0
0
1
0
0
0
âˆ’1
ï£¶
ï£¸;
are the standard basis elements for C3, namely, e1, e2, and e3. The corresponding
weights are (1, 0), (âˆ’1, 1), and (0, âˆ’1). The highest weight is (1, 0).
Recall that
Y1 =
ï£«
ï£­
0
0
0
1
0
0
0
0
0
ï£¶
ï£¸;
Y2 =
ï£«
ï£­
0
0
0
0
0
0
0
1
0
ï£¶
ï£¸.
Thus
Y1(e1)
=
e2
Y2(e1)
=
0
Y1(e2)
=
0
Y2(e2)
=
e3
Y1(e3)
=
0
Y2(e3)
=
0.
(6.10)
Now, the representation with highest weight (0, 1) is the representation Ï€(Z) =
âˆ’Ztr, for Z âˆˆsl (3; C). Let us deï¬ne
Z = âˆ’Ztr
for all Z âˆˆsl (3; C). Thus Ï€(Z) = Z. Note that
H1 =
ï£«
ï£­
âˆ’1
0
0
0
1
0
0
0
0
ï£¶
ï£¸;
H2 =
ï£«
ï£­
0
0
0
0
âˆ’1
0
0
0
1
ï£¶
ï£¸.
The weight vectors are again e1, e2, and e3, with weights (âˆ’1, 0), (1, âˆ’1), and (0, 1).
The highest weight is (0, 1).
Deï¬ne new basis elements
f1
=
e3
f2
=
âˆ’e2
f3
=
e1.
Then since
Y1 =
ï£«
ï£­
0
âˆ’1
0
0
0
0
0
0
0
ï£¶
ï£¸;
Y2 =
ï£«
ï£­
0
0
0
0
0
âˆ’1
0
0
0
ï£¶
ï£¸;
we have
Y1(f1)
=
0
Y2(f1)
=
f2
Y1(f2)
=
f3
Y2(f2)
=
0
Y1(f3)
=
0
Y2(f3)
=
0.
(6.11)
Note that the highest weight vector is f1 = e3.

112
6. THE REPRESENTATIONS OF SU(3), AND BEYOND
So, to obtain an irreducible representation with highest weight (1, 1) we are
supposed to take the tensor product of the representations with highest weights
(1, 0) and (0, 1), and then take the smallest invariant subspace containing the vector
e1 âŠ—f1. In light of the proof of Proposition 6.14, this smallest invariant subspace
is obtained by starting with e1 âŠ—f1 and applying all possible combinations of Y1
and Y2.
Recall that if Ï€1 and Ï€2 are two representations of the Lie algebra sl (3; C), then
(Ï€1 âŠ—Ï€2) (Y1) = Ï€1(Y1) âŠ—I + I âŠ—Ï€2(Y1)
(Ï€1 âŠ—Ï€2) (Y2) = Ï€1(Y2) âŠ—I + I âŠ—Ï€2(Y2).
In our case we want Ï€1(Yi) = Yi and Ï€2(Yi) = Yi. Thus
(Ï€1 âŠ—Ï€2) (Y1) = Y1 âŠ—I + I âŠ—Y1
(Ï€1 âŠ—Ï€2) (Y2) = Y2 âŠ—I + I âŠ—Y2.
The actions of Yi and Yi are described in (6.10) and (6.11).
Note that Ï€1 âŠ—Ï€2 is not an irreducible representation.
The representation
Ï€1 âŠ—Ï€2 has dimension 9, whereas the smallest invariant subspace containing e1 âŠ—f1
has, as it turns out, dimension 8.
So, it remains only to begin with e1 âŠ—f1, apply Y1 and Y2 repeatedly until we
get zero, and then ï¬gure out what dependence relations exist among the vectors we
get. These computations are done on a supplementary page. Note that the weight
(0, 0) has multiplicity two. This is because, starting with e1 âŠ—f1, applying Y1 and
then Y2 gives something diï¬€erent than applying Y2 and then Y1.
6. The Weyl Group
The set of weights of an arbitrary irreducible representation of sl (3; C) has a
certain symmetry associated to it. This symmetry is in terms of something called
the â€œWeyl group.â€
(My treatment of the Weyl group follows BrÂ¨ocker and tom
Dieck, Chap. IV, 1.3.) We consider the following subgroup of SU(3):
W =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
w0 =
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸;
w1 =
ï£«
ï£­
0
0
1
1
0
0
0
1
0
ï£¶
ï£¸;
w2 =
ï£«
ï£­
0
1
0
0
0
1
1
0
0
ï£¶
ï£¸
w3 = âˆ’
ï£«
ï£­
0
1
0
1
0
0
0
0
1
ï£¶
ï£¸;
w4 = âˆ’
ï£«
ï£­
0
0
1
0
1
0
1
0
0
ï£¶
ï£¸;
w5 = âˆ’
ï£«
ï£­
1
0
0
0
0
1
0
1
0
ï£¶
ï£¸
ï£¼
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£¾
.
These are simply the matrices which permute the standard basis elements of C3,
with an adjustment of overall sign when necessary to make the determinant equal
one.
Now, for any A âˆˆSU(3), we have the associated map AdA : su(3) â†’su(3),
where
AdA(X) = AXAâˆ’1.
Now, since each element of sl(3; C) is of the form Z = X + iY with X, Y âˆˆsu(3),
it follows that sl (3; C) is invariant under the map Z â†’AZAâˆ’1. That is, we can
think of AdA as a map of sl (3; C) to itself.
The reason for selecting the above group is the following: If w âˆˆW, then
Adw(H1) and Adw(H2) are linear combinations of H1 and H2.
That is, each

6. THE WEYL GROUP
113
Adw preserves the space spanned by H1 and H2. (There are other elements of
SU(3) with this property, notably, the diagonal elements. However, these actually
commute with H1 and H2. Thus the adjoint action of these elements on the span
of H1 and H2 is trivial and therefore uninteresting. See Exercise 3.)
Now, for each w âˆˆW and each irreducible representation Ï€ of sl (3; C), letâ€™s
deï¬ne a new representation Ï€w by the formula
Ï€w(X) = Ï€
 Adwâˆ’1(X)

= Ï€(wâˆ’1Xw).
Since Adwâˆ’1 is a Lie algebra automorphism, Ï€w will in fact be a representation of
sl (3; C).
Recall that since SU(3) is simply connected, then for each representation Ï€ of
sl (3; C) there is an associated representation Î  of SU(3) (acting on the same space)
such that
Î 
 eX
= eÏ€(X)
for all X âˆˆsu(3) âŠ‚sl (3; C). The representation Î  has the property that
Ï€(AXAâˆ’1) = Î (A)Ï€(X)Î (A)âˆ’1
(6.12)
for all X âˆˆsu(3). Again since every element of sl (3; C) is of the form X + iY with
X, Y âˆˆsu(3), it follows that (6.12) holds also for X âˆˆsl (3; C).
In particular, taking A = wâˆ’1 âˆˆW we have
Ï€w(X) = Ï€(wâˆ’1Xw) = Î (w)âˆ’1Ï€(X)Î (w)
(6.13)
for all X âˆˆsl (3; C).
Proposition 6.20. For each representation Ï€ of sl(3; C) and for each w âˆˆW,
the representation Ï€w is equivalent to the representation Ï€.
Proof. We need a map Ï† : V â†’V with the property that
Ï† (Ï€w(X)v) = Ï€(X)Ï†(v)
for all v âˆˆV . This is the same as saying that Ï†Ï€w(X) = Ï€(X)Ï†, or equivalently
that Ï€w(X) = Ï†âˆ’1Ï€(X)Ï†. But in light of (6.13), we can take Ï† = Î (w).
Although Ï€ and Ï€w are equivalent, they are not equal. That is, in general
Ï€(X) Ì¸= Ï€w(X). You should think of Ï€ and Ï€w as diï¬€ering by a change of basis on
V , where the change-of-basis matrix is Î (w). Two representations that diï¬€er just
by a change of basis are automatically equivalent.
Corollary 6.21. Let Ï€ be a representation of sl (3; C) and w âˆˆW. Then
a pair Âµ = (m1, m2) is a weight for Ï€ if and only if it is a weight for Ï€w. The
multiplicity of Âµ as a weight of Ï€ is the same as the multiplicity of Âµ as a weight
for Ï€w.
Proof. Equivalent representations must have the same weights and the same
multiplicities.

114
6. THE REPRESENTATIONS OF SU(3), AND BEYOND
Let us now compute explicitly the action of Adwâˆ’1 on the span of H1 and H2,
for each w âˆˆW. This is a straightforward computation.
wâˆ’1
0 H1w0
=
H1
wâˆ’1
3 H1w3
=
âˆ’H1
wâˆ’1
0 H2w0
=
H2
wâˆ’1
3 H2w3
=
H1 + H2
wâˆ’1
1 H1w1
=
âˆ’H1 âˆ’H2
wâˆ’1
4 H1w4
=
âˆ’H2
wâˆ’1
1 H2w1
=
H1
wâˆ’1
4 H2w4
=
âˆ’H1
wâˆ’1
2 H1w2
=
H2
wâˆ’1
5 H1w5
=
H1 + H2
wâˆ’1
2 H2w2
=
âˆ’H1 âˆ’H2
wâˆ’1
5 H2w5
=
âˆ’H2.
(6.14)
We can now see the signiï¬cance of the Weyl group. Let Ï€ be a representation
of sl (3; C), Âµ = (m1, m2) a weight, and v Ì¸= 0 a weight vector with weight Âµ. Then,
for example,
Ï€w1(H1)v = Ï€(wâˆ’1
1 H1w1)v = Ï€(âˆ’H1 âˆ’H2)v = (âˆ’m1 âˆ’m2)v
Ï€w1(H2)v = Ï€(wâˆ’1
1 H2w1)v = Ï€(H1)v = m1v.
Thus v is a weight vector for Ï€w with weight (âˆ’m1 âˆ’m2, m1). But by Corollary
6.21, the weights of Ï€ and of Ï€w are the same!
Conclusion: If Âµ = (m1, m2) is a weight for Ï€, so is (âˆ’m1 âˆ’m2, m1).
The multiplicities of (m1, m2) and (âˆ’m1 âˆ’m2, m1) are the same.
Of course, a similar argument applies to each of the other elements of the
Weyl group. Speciï¬cally, if Âµ is a weight for some representation Ï€, and w is an
element of W, then there will be some new weight which must also be a weight
of Ï€. We will denote this new weight w Â· Âµ. For example, if Âµ = (m1, m2), then
w1 Â· Âµ = (âˆ’m1 âˆ’m2, m1). (We deï¬ne w Â· Âµ so that if v is a weight vector for Ï€ with
weight Âµ, then v will be a weight for Ï€w with weight w Â· Âµ.) From (6.14) we can
read oï¬€what w Â· Âµ is for each w.
w0 Â· (m1, m2)
=
(m1, m2)
w3 Â· (m1, m2)
=
(âˆ’m1, m1 + m2)
w1 Â· (m1, m2)
=
(âˆ’m1 âˆ’m2, m1)
w4 Â· (m1, m2)
=
(âˆ’m2, âˆ’m1)
w2 Â· (m1, m2)
=
(m2, âˆ’m1 âˆ’m2)
w5 Â· (m1, m2)
=
(m1 + m2, âˆ’m2)
(6.15)
It is straightforward to check that
wi Â· (wj Â· Âµ) = (wiwj) Â· Âµ.
(6.16)
We have now proved the following.
Theorem 6.22. If Âµ = (m1, m2) is a weight and w is an element of the Weyl
group, let w Â· Âµ be deï¬ned by (6.15). If Ï€ is a ï¬nite-dimensional representation
of sl (3; C), then Âµ is a weight for Ï€ if and only if w Â· Âµ is a weight for Ï€. The
multiplicity of Âµ is the same as the multiplicity of w Â· Âµ.
If we think of the weights Âµ = (m1, m2) as sitting inside R2, then we can
think of (6.15) as a ï¬nite group of linear transformations of R2. (The fact that
this is a group of transformations follows form (6.16).) Since this is a ï¬nite group
of transformations, it is possible to choose an inner product on R2 such that the
action of W is orthogonal. (As in the proof of Proposition 5.16 in Chapter 5.)
In fact, there is (up to a constant) exactly one such inner product. In this inner
product, the action (6.15) of the Weyl group is generated by a 120â—¦rotation and a

7. COMPLEX SEMISIMPLE LIE ALGEBRAS
115
reï¬‚ection about the y-axis. Equivalently, the Weyl group is the symmetry group of
an equilateral triangle centered at the origin with one vertex on the y-axis.
7. Complex Semisimple Lie Algebras
This section gives a brief synopsis of the structure theory and representation
theory of complex semisimple Lie algebras. The moral of the story is that all such
Lie algebras look and feel a lot like sl(3; C).
This section will not contain any
(non-trivial) proofs.
If g is a Lie algebra, a subspace I âŠ‚g is said to be an ideal if [X, Y ] âˆˆI for
all X âˆˆg and all Y âˆˆI. A Lie algebra g is a said to be simple if dimg â‰¥2 and g
has no ideals other than {0} and g. A Lie algebra g is said to be semisimple if g
can be written as the direct sum of simple Lie algebras.
In this section we consider semisimple Lie algebras over the complex numbers.
Examples of complex semisimple Lie algebras include sl (n; C), so(n; C) (n â‰¥3), and
sp(n; C). All of these are actually simple, except for so(4; C) which is isomorphic
to sl(2; C) âŠ•sl(2; C).
Definition 6.23. Let g be a complex semisimple Lie algebra. A subspace h of
g is said to be a Cartan subalgebra if
1. h is abelian. That is, [H1, H2] = 0 for all H1, H2 âˆˆh.
2. h is maximal abelian. That is, if X âˆˆg satisï¬es [H, X] = 0 for all H âˆˆh,
then X âˆˆh.
3. For all H âˆˆh, adH : g â†’g is diagonalizable.
Since all the Hâ€™s commute, so do the adHâ€™s. (I.e., [adH1, adH2] = ad [H1, H2] =
0.) By assumption, each adH is diagonalizable, and they commute, therefore the
adHâ€™s are simultaneously diagonalizable. (Using a standard linear algebra fact.)
Let hâˆ—denote the dual of h, namely, the space of linear functionals on h.
Definition 6.24. If g is a complex semisimple Lie algebra and h a Cartan
subalgebra, then an element Î± of hâˆ—is said to be a root (for g with respect to h) if
Î± is non-zero and there exists Z Ì¸= 0 in g such that
[H, Z] = Î±(H)Z
(6.17)
for all H âˆˆh. (Thus a root is a non-zero set of simultaneous eigenvalues for the
adHâ€™s.)
The vector Z is called a root vector corresponding to the root Î±, and the space
of all Z âˆˆg satisfying (6.17) is the root space corresponding to Î±. This space is
denoted gÎ±.
The set of all roots will be denoted âˆ†.
Note that if g = sl (3; C), then one Cartan subalgebra is the space spanned
by H1 and H2.
The roots (with respect to this Cartan subalgebra) have been
calculated in (6.3).
Theorem 6.25. If g is a complex semisimple Lie algebra, then a Cartan subal-
gebra h exists. If h1 and h2 are two Cartan subalgebras, then there is an automor-
phism of g which takes h1 to h2. In particular, any two Cartan subalgebras have
the same dimension.
From now on, g will denote a complex semisimple Lie algebra, and h a ï¬xed
Cartan subalgebra in g.

116
6. THE REPRESENTATIONS OF SU(3), AND BEYOND
Definition 6.26. The rank of a complex semisimple Lie algebra is the dimen-
sion of a Cartan subalgebra.
For example, the rank of sl (n; C) is n âˆ’1. One Cartan subalgebra in sl (n; C)
is the space of diagonal matrices with trace zero. (Note that in the case n = 3
the space of diagonal matrices with trace zero is precisely the span of H1 and H2.)
Both so(2n; C) and so(2n + 1; C) have rank n.
Definition 6.27. Let (Ï€, V ) be a ï¬nite-dimensional, complex-linear represen-
tation of g. Then Âµ âˆˆhâˆ—is called a weight for Ï€ if there exists v Ì¸= 0 in V such
that
Ï€(H)v = Âµ(H)v
for all H âˆˆh. The vector v is called a weight vector for the weight Âµ.
Note that the roots are precisely the non-zero weights for the adjoint represen-
tation.
Lemma 6.28. Let Î± be a root and Z a corresponding root vector. Let Âµ be a
weight for a representation Ï€ and v a corresponding weight vector. Then either
Ï€(Z)v = 0 or else Ï€(Z)v is a weight vector with weight Âµ + Î±.
Proof. Same as for sl(3; C).
Definition 6.29. A set of roots {Î±1, Â· Â· Â·Î±l} is called a simple system (or
basis) if
1. {Î±1, Â· Â· Â·Î±l} is a vector space basis for hâˆ—.
2. Every root Î± âˆˆâˆ†can be written in the form
Î± = n1Î±1 + n2Î±2 + Â· Â· Â· + nlÎ±l
with each ni an integer, and such that the niâ€™s are either all non-negative or
all non-positive.
A root Î± is said to be positive (with respect to the given simple system) if the
niâ€™s are non-negative; otherwise Î± is negative.
If g = sl(3; C) and h = {H1, H2}, then one simple system of roots is

Î±(1), Î±(2)	
=
{(2, âˆ’1), (âˆ’1, 2)} (with the corresponding root vectors being X1 and X2). The posi-
tive roots are {(2, âˆ’1), (âˆ’1, 2), (1, 1)}. The negative roots are {(âˆ’2, 1) , (1, âˆ’2) , (âˆ’1, âˆ’1)}.
Definition 6.30. Let {Î±1, Â· Â· Â·Î±l} be a simple system of roots and let Âµ1 and
Âµ2 be two weights. Then Âµ1 is higher than Âµ2 (or Âµ2 is lower than Âµ1) if Âµ1 âˆ’Âµ2
can be written as
Âµ1 âˆ’Âµ2 = a1Î±1 + a2Î±2 + Â· Â· Â· + alÎ±l
with ai â‰¥0. This relation is denoted Âµ1 âª°Âµ2 or Âµ2 âª¯Âµ1.
A weight Âµ0 for a representation Ï€ is highest if all the weights Âµ of Ï€ satisfy
Âµ âª¯Âµ0.
The following deep theorem captures much of the structure theory of semisimple
Lie algebras.
Theorem 6.31. Let g be a complex semisimple Lie algebra, h a Cartan subal-
gebra, and âˆ†the set of roots. Then
1. For each root Î± âˆˆâˆ†, the corresponding root space gÎ± is one-dimensional.

8. EXERCISES
117
2. If Î± is a root, then so is âˆ’Î±.
3. A simple system of roots {Î±1, Â· Â· Â·Î±l} exists.
We now need to identify the correct set of weights to be highest weights of
irreducible representations.
Theorem 6.32. Let {Î±1, Â· Â· Â·Î±l} denote a simple system of roots, Xi an element
of the root space gÎ±i and Yi an element of the root space gâˆ’Î±i. Deï¬ne
Hi = [Xi, Yi] .
Then it is possible to choose Xi and Yi such that
1. Each Hi is non-zero and contained in h.
2. The span of {Hi, Xi, Yi} is a subalgebra of g isomorphic (in the obvious way)
to sl(2; C).
3. The set {H1, Â· Â· Â·Hl} is a basis for h.
Note that (in most cases) the set of all Hiâ€™s, Xiâ€™s, and Yiâ€™s (i = 1, 2, Â· Â· Â·l) do
not span g. In the case g = sl (3; C), l = 2, and the span of H1, X1, Y1, H2, X2, Y2
represents only six of the eight dimensions of sl (3; C). Nevertheless the subalgebras
{Hi, Xi, Yi} play an important role.
We are now ready to state the main theorem.
Theorem 6.33. Let g be a complex semisimple Lie algebra, h a Cartan subal-
gebra, and {Î±1, Â· Â· Â·Î±l} a simple system of roots. Let {H1, Â· Â· Â·Hl} be as in Theorem
6.32. Then
1. In each irreducible representation Ï€ of g, the Ï€(H)â€™s are simultaneously di-
agonalizable.
2. Each irreducible representation of g has a unique highest weight.
3. Two irreducible representations of g with the same highest weight are equiv-
alent.
4. If Âµ0 is the highest weight of an irreducible representation of g, then for
i = 1, 2, Â· Â· Â·l, Âµ0(Hi) is a non-negative integer.
5. Conversely, if Âµ0 âˆˆhâˆ—is such that Âµ0(Hi) is a non-negative integer for all
i = 1, 2, Â· Â· Â·l, then there is an irreducible representation of g with highest
weight Âµ0.
The weights Âµ0 as in 4) and 5) are called dominant integral weights.
8. Exercises
1. Show that for any pair of n Ã— n matrices X and Y ,

Xtr, Y tr
= âˆ’[X, Y ]tr .
Using this fact and the fact that Xtr
i
= Yi for i = 1, 2, 3, explain the symme-
try between Xâ€™s and Y â€™s in the commutation relations for sl (3; C). For exam-
ple, show that the relation [Y1, Y2] = âˆ’Y3 can be obtained from the relation
[X1, X2] = X3 by taking transposes. Show that the relation [H1, Y2] = Y2
follows from the relation [H1, X2] = âˆ’X2.
2. Recall the deï¬nition of the dual Ï€âˆ—of a representation Ï€ from Exercise 14
of Chapter 5. Consider this for the case of representations of sl(3; C).
a) Show that the weights of Ï€âˆ—are the negatives of the weights of Ï€.

118
6. THE REPRESENTATIONS OF SU(3), AND BEYOND
b) Show that if Ï€ is the irreducible representation of sl (3; C) with highest
weight (m1, m2) then Ï€âˆ—is the irreducible representation with highest weight
(m2, m1).
Hint: If you identify V and V âˆ—by choosing a basis for V , then Atr is
just the usual matrix transpose.
3. Let h denote the subspace of sl(3; C) spanned by H1 and H2. Let G denote
the group of all matrices A âˆˆSU(3) such that AdA preserves h. Now let G0
denote the group of all matrices A âˆˆSU(3) such that AdA is the identity
on h, i.e., such that AdA(H1) = H1 and AdA(H2) = H2. Show that G0 is a
normal subgroup of G. Compute G and G0. Show that G/G0 is isomorphic
to the Weyl group W.
4. a) Verify Theorems 6.31 and 6.32 explicitly for the case g = sl (n; C).
b) Consider the task of trying to prove Theorem 6.33 for the case of
sl(n; C). Now that you have done (a), what part of the proof goes through
the same way as for sl(3; C)? At what points in the proof of the correspond-
ing theorem for sl (3; C) did we use special properties of sl (3; C)?
Hint: Most of it is the same, but there is one critical point which we do
something which does not generalize to sl(n; C).

CHAPTER 7
Cumulative exercises
1. Let G be a connected matrix Lie group, and let Ad : G â†’GL(g) be the
adjoint representation of G. Show that
ker(Ad) = Z(G)
where Z(G) denotes the center of G. If G = O(2), compute ker(Ad) and
Z(G) and show that they are not equal.
Hint: You should use the fact that if G is connected, then every A âˆˆG
can be written in the form A = eX1eX2 Â· Â· Â·eXn, with Xi âˆˆg.
2. Let G be a ï¬nite, commutative group. Show that the number of equivalence
classes of irreducible complex representations of G is equal to the number
of elements in G.
Hint: Use the fact that every ï¬nite, commutative group is a product of
cyclic groups.
3. a) Show that if R âˆˆO(2), and det R = âˆ’1, then R has two real, orthogonal
eigenvectors with eigenvalues 1 and âˆ’1.
b) Let R be in O(n). Show that there exists a subspace W of Rn which is
invariant under both R and Râˆ’1, and such that dimW = 1 or 2. Show that
W âŠ¥(the orthogonal complement of W) is also invariant under R and Râˆ’1.
Show that the restrictions of R and Râˆ’1 to W and to W âŠ¥are orthogonal.
(That is, show that these restrictions preserve inner products.)
c) Let R be in O(n). Show that Rn can be written as the orthogonal
direct sum of subspaces Wi such that
(a) 1) Each Wi is invariant under R and Râˆ’1,
(b) 2) Each Wi has dimension 1 or 2, and
(c) 3) If dimWi = 2, then the restriction of R to Wi has determinant one.
d) Show that the exponential mapping for SO(n) is onto. Make sure
you use the fact that the elements of SO(n) have determinant one.
Note: This provides an alternative proof that the group SO(n) is con-
nected.
4. Determine, up to equivalence, all of the ï¬nite-dimensional, irreducible (complex-
linear) representations of the Lie algebra sl(2; C)âŠ•sl(2; C). Can your answer
be expressed in terms of a sort of â€œhighest weightâ€?
Hint: Imitate the proof of the classiï¬cation of the irreducible represen-
tations of sl(2; C).
5. Consider the irreducible representation (Ï€, V ) of sl (3; C) with highest weight
(0, 2). Following the procedure in Chapter 6, Section 5, determine
1) The dimension of V .
2) All of the weights of Ï€.
119

120
7. CUMULATIVE EXERCISES
3) The multiplicity of each of the weights. (That is, the dimension of
the corresponding weight spaces.)

CHAPTER 8
Bibliography
1. Theodor BrÂ¨ocker and Tammo tom Dieck, Representations of Compact
Lie Groups. Springer-Verlag, 1985.
A good reference for basic facts on compact groups and their repre-
sentations, including characters and orthogonality relations. Analyzes rep-
resentations from a more analytic and less algebraic viewpoint than other
authors.
2. William Fulton and Joe Harris, Representation theory. A First Course.
Graduate Texts in Mathematics, 129. Readings in Mathematics, Springer-
Verlag, 1991.
Has lots of examples.
Written from an algebraic point of
view.
3. Sigurdur Helgason, Diï¬€erential Geometry, Lie Groups, and Symmetric
Spaces. Academic Press, 1978.
A good reference for a lot of things. Includes structure theory of semisim-
ple groups.
4. James E. Humphreys, Introduction to Lie Algebras and Representation
Theory. Springer-Verlag, 1972.
A standard reference for the Lie algebra side of things (no Lie groups).
5. N. Jacobson, Lie Algebras. Interscience Tracts No. 10, John Wiley and
Sons, 1962.
Another good reference for Lie algebras.
6. Anthony W. Knapp, Lie groups: beyond an introduction. Birkhauser,
1996. Good complement to Helgason on such matters as structure theory of
Lie groups. As title suggests, not the place to start, but a good reference.
7. W. Miller, Symmetry Groups and Their Applications. Academic Press.
Oriented toward applications to physics. Includes theory of ï¬nite groups.
8. Jean-Pierre Serre, Complex Semisimple Lie Algebras. Springer-Verlag,
1987.
A very concise summary of structure theory and representation theory
of semisimple Lie algebras.
9. Jean-Pierre Serre, Linear Representations of Finite Groups.
Springer-
Verlag.
An introduction to both complex and modular representations of ï¬nite
groups.
10. Barry Simon, Representations of ï¬nite and compact Lie groups, American
Mathematical Society, 1996. Covers much of the same material as BrÂ¨ocker
and tom Dieck, but from a more analytical perspective.
11. Frank W. Warner, Foundations of Diï¬€erentiable Manifolds and Lie Groups.
Springer-Verlag, 1983.
121

122
8. BIBLIOGRAPHY
Key word in the title is foundations. Gives a modern treatment of dif-
ferentiable manifolds, and then proves some important, non-trivial theorems
about Lie groups, including the relationship between subgroups and subal-
gebras, and the relationship between representations of the Lie algebra and
of the Lie group.
12. V.S. Varadarajan, Lie Groups, Lie Algebras, and Their Representations.
Springer-Verlag, 1974.
A comprehensive treatment of both Lie groups and Lie algebras.


Artiﬁ cial 
Intelligence 
Tools for Cyber 
Attribution
SPRINGER BRIEFS IN COMPUTER SCIENCE
Eric Nunes
Paulo Shakarian 
Gerardo I. Simari 
Andrew Ruef

SpringerBriefs in Computer Science
Series editors
Stan Zdonik, Brown University, Providence, Rhode Island, USA
Shashi Shekhar, University of Minnesota, Minneapolis, Minnesota, USA
Xindong Wu, University of Vermont, Burlington, Vermont, USA
Lakhmi C. Jain, University of South Australia, Adelaide, South Australia, Australia
David Padua, University of Illinois Urbana-Champaign, Urbana, Illinois, USA
Xuemin Sherman Shen, University of Waterloo, Waterloo, Ontario, Canada
Borko Furht, Florida Atlantic University, Boca Raton, Florida, USA
V. S. Subrahmanian, University of Maryland, College Park, Maryland, USA
Martial Hebert, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA
Katsushi Ikeuchi, University of Tokyo, Tokyo, Japan
Bruno Siciliano, Università di Napoli Federico II, Napoli, Italy
Sushil Jajodia, George Mason University, Fairfax, Virginia, USA
Newton Lee, Newton Lee Laboratories, LLC, Tujunga, California, USA

More information about this series at http://www.springer.com/series/10028

Eric Nunes • Paulo Shakarian • Gerardo I. Simari
Andrew Ruef
Artiﬁcial Intelligence Tools
for Cyber Attribution
123

Eric Nunes
Arizona State University
Tempe, AZ, USA
Gerardo I. Simari
Department of Computer Science
and Engineering
Universidad Nacional del Sur (UNS) &
Institute for Computer Science and
Engineering (UNS-CONICET)
Bahia Blanca, Argentina
Paulo Shakarian
Arizona State University
Tempe, AZ, USA
Andrew Ruef
University of Maryland
College Park, MD, USA
ISSN 2191-5768
ISSN 2191-5776
(electronic)
SpringerBriefs in Computer Science
ISBN 978-3-319-73787-4
ISBN 978-3-319-73788-1
(eBook)
https://doi.org/10.1007/978-3-319-73788-1
Library of Congress Control Number: 2017963778
© The Author(s) 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Acknowledgements
The authors would like to acknowledge the generous support from the U.S.
Department of the Navy, Ofﬁce of Naval Research Grant: N00014-15-1-2742 (in
particular, Sukarno Mertoguno), Arizona State University Global Security Initiative
(in particular, Nadya Bliss and Jamie Winterton), funds provided by Universidad
Nacional del Sur and CONICET, Argentina, the National Science Foundation, the
Army Research Ofﬁce under the Science of Security Lablet grant (SoSL), and the
DARPA URAP project (in particular, Dan Ragsdale). We would also like to thank
the following collaborators who have contributed in some of the research discussed
in the book (in no particular order): Nimish Kulkarni, Jay Little, Geoffrey Moores,
Damon Paulo, Simon Parsons, Marcelo Falappa, and Ashkan Aleali.
v

Contents
1
Introduction ...................................................................
1
References ......................................................................
3
2
Baseline Cyber Attribution Models .........................................
5
2.1
Introduction ..............................................................
5
2.2
Dataset ....................................................................
5
2.2.1
DEFCON CTF ...................................................
6
2.2.2
DEFCON CTF Data .............................................
7
2.2.3
Analysis of CTF Data ...........................................
7
2.3
Baseline Approaches.....................................................
10
2.4
Experimental Results ....................................................
12
2.4.1
Misclassiﬁed Samples ...........................................
13
2.4.2
Pruning ...........................................................
13
2.5
Conclusions ..............................................................
15
References ......................................................................
16
3
Argumentation-Based Cyber Attribution: The DeLP3E Model ........
17
3.1
Introduction ..............................................................
17
3.1.1
Application to the Cyber Attribution Problem .................
19
3.1.2
Structure of the Chapter .........................................
20
3.2
Technical Preliminaries ..................................................
21
3.2.1
Basic Language ..................................................
21
3.2.2
Environmental Model............................................
22
3.2.3
Analytical Model ................................................
24
3.3
The DeLP3E Framework ...............................................
29
3.3.1
Warranting Scenarios ............................................
31
3.3.2
Entailment in DeLP3E ..........................................
32
3.4
Consistency and Inconsistency in DeLP3E Programs .................
34
vii

viii
Contents
3.5
Case Study: An Application in Cybersecurity ..........................
36
3.5.1
Model for the Attribution Problem..............................
37
3.5.2
Applying Entailment to the Cyber Attribution Problem .......
41
3.6
Conclusions ..............................................................
43
References ......................................................................
44
4
Belief Revision in DeLP3E ...................................................
47
4.1
Introduction ..............................................................
47
4.2
Basic Belief Revision ....................................................
47
4.2.1
EM-Based Belief Revision ......................................
48
4.2.2
AM-Based Belief Revision......................................
50
4.2.3
Annotation Function-Based Belief Revision ...................
53
4.3
Quantitative Belief Revision Operators .................................
57
4.3.1
Towards Quantitative Revision..................................
62
4.3.2
Two Building Blocks ............................................
64
4.3.3
The Class QAFO ................................................
67
4.3.4
Computational Complexity......................................
68
4.3.5
Warranting Formulas ............................................
71
4.3.6
Outlook: Towards Tractable Computations.....................
73
4.4
Conclusions and Future Work ...........................................
73
References ......................................................................
74
5
Applying Argumentation Models for Cyber Attribution .................
75
5.1
Introduction ..............................................................
75
5.2
Baseline Argumentation Model (BM) ..................................
77
5.3
Extended Baseline Model I (EB1) ......................................
81
5.4
Extended Baseline Model II (EB2)......................................
82
5.5
Conclusions ..............................................................
84
References ......................................................................
84
6
Enhanced Data Collection for Cyber Attribution .........................
85
6.1
Introduction ..............................................................
85
6.2
Goals and Design ........................................................
86
6.2.1
Changing Contestant Behavior..................................
86
6.2.2
Game Rules ......................................................
87
6.2.3
Infrastructure Design ............................................
88
6.2.4
Motivating Attribution and Deception ..........................
89
6.2.5
Validity of Data ..................................................
90
6.3
Conclusion ...............................................................
90
References ......................................................................
90
7
Conclusion .....................................................................
91

Chapter 1
Introduction
Cyber attribution is the process by which the identity of an actor or aggressor
in a cyberactivity is determined. Conducting this process presents several unique
problems; chief among them are that the technical artifacts produced by cyberattacks
are difﬁcult to understand, and it is easy (and quite useful) for an actor to perform
deception.
The process, ﬂaws, outcomes, and methodology of cyber attribution have become
a subject of increasingly broad interest over the past few years. Part of this is due
to the increase in cyberactivity and the intersection of that cyberactivity with the
public sphere. For example, it used to be that a major company being hacked would
be of concern only to that company and its customers; however, the compromise of
Sony Pictures allegedly by North Korea in late 2014 elevated public interest in the
accurate attribution of cyberaggression to the national level.
The analysis that underpins cyber attribution involves many diverse sources of
data: network forensics, host forensics, malware analysis, and code similarity, to
name a few. Like intelligence analysis, independent and diverse sources of reporting
strengthen an analytic argument. In cybersecurity, an adversary might ﬁnd that they
have complete control over which hosts they use across the campaigns they conduct
but very little control over which malware they use. An attribution analysis that
considers only network level data would be inadequate against such an adversary.
Why do we perform cyber attribution, and who is the customer of cyber
attribution decisions? Law enforcement and the courts care about cyber attribution
decisions when making investigative or legal decisions. In other spheres, attribution
decisions can be used to help determine the direction and proportion of an
organizational response. For example, if a commercial company can determine if
an attacker is part of an unsophisticated hacktivist gang rather than a sophisticated
criminal enterprise, they could simply re-install the compromised computers as a
defensive response rather than engaging with law enforcement. Likewise, according
to Wheeler et al., “many offensive techniques, such as computer network attack,
legal action (e.g., arrests and lawsuits), and kinetic energy attacks, can only be
deployed if the source of the attack can be attributed with high conﬁdence” [7].
© The Author(s) 2018
E. Nunes et al., Artiﬁcial Intelligence Tools for Cyber Attribution, SpringerBriefs in
Computer Science, https://doi.org/10.1007/978-3-319-73788-1_1
1

2
1
Introduction
In the spring of 2017, the internet was gripped by a widespread ransomware
attack, dubbed WannaCry; the ransomware spread as a worm affecting 300,000
machines in 150 countries, holding ﬁles hostage with encryption and promising
decryption if a payment was made to a Bitcoin address. Hackers took advantage of
the fact that many systems were not updated with the patch released by Microsoft,
leaving them vulnerable. The attack was in fact discussed on darkweb forums in
several languages including English and Russian as identiﬁed by cybersecurity
company CYR3CON [6]; they also reported that hackers choose medical institutions
as prime targets based on the history of paid ransom from similar institutions. Both
the distributed nature of the attack and the use of Bitcoin as payment obscure the true
source and authorship of the worm. These features also limited the data available to
perform attribution, because no central or hosted command and control systems
were needed.
In this setting, the only artifacts available for attribution were the linguistic
properties of the ransom message, and the code that made up the worm itself.
An initial analysis and comparison of the ransomware code identiﬁed similarity
in the code between WannaCry and malware that had previously been attributed
to the “Lazarus Group.” However, other explanations exist for this one point of
similarity: perhaps the WannaCry authors borrowed from the Lazarus code after
it was published; perhaps both authors copied from a third-party open source
repository; perhaps this line was copied without the intent to deceive, because it
is well known that developers will copy and paste code whenever possible.
Our research has focused on establishing a more rigorous and scientiﬁc basis
for making attribution decisions. When the stakes are high, it is important to either
make the correct decision, or understand the possibilities for deception and gather
the additional information needed to make the correct decision. Having a single
data point, as in the WannaCry example, could bias the analysis by giving too much
weight to a single source.
In addition to researching and applying artiﬁcial intelligence tools to cyber
attribution, we also ask: how do researchers train and evaluate these tools? Using
data gathered from the real world is problematic for a few reasons. First, it is difﬁcult
to get real-world data due to the sensitive nature of the data. Additionally, even if
the data were available, it is difﬁcult to trust ground truth about that data. Could
attackers’ deceptions go unnoticed in this data? Who can say? To enable researchers
to develop and evaluate their tools, we used data from capture-the-ﬂag (CTF)
contests, where access to the ground truth is available. We also present frameworks
for executing and gathering your own CTF based data, while encouraging and
monitoring attempts at deception.
In this book, we present the results of a research program that focuses on using
artiﬁcial intelligence tools to enhance the cyber attribution process. In Chap.2, we
introduce a dataset collected from the capture-the-ﬂag (CTF) event at DEFCON that
has the identity of the attacker team (ground truth); the lack of such ground truth has
limited previous studies to evaluate proposed models. As a ﬁrst step, we use standard
classiﬁcation techniques to identify the attacker and summarize the results discussed
in [1].

References
3
In Chap.3, we propose a probabilistic structured argumentation framework that
arises from the extension of Presumptive Defeasible Logic Programming (PreDeLP)
with probabilistic models, and argue that this formalism is especially suitable
for handling such contradictory and uncertain data. The framework has been
demonstrated—via a case study—to handle cyber attribution [4].
In Chap. 4, we continue developing the DeLP3E model introduced in the
previous chapter. We ﬁrst propose a non-prioritized class of revision operators
called AFO (Annotation Function-based Operators), and then go on to argue that
in some cases it may be desirable to deﬁne revision operators that take quantitative
aspects into account. As a result, we propose the QAFO (Quantitative Annotation
Function-based Operators) class of operators, a subclass of AFO, and study the
complexity of several problems related to their speciﬁcation and application in
revising knowledge bases. we present an algorithm for computing the probability
that a literal is warranted in a DeLP3E knowledge base, and discuss how it could be
applied towards implementing QAFO-style operators that compute approximations
rather than exact operations [5].
In Chap. 5, we build argumentation models based on a formal reasoning frame-
work called DeLP (Defeasible Logic Programming). The models are evaluated on
the CTF data discussed in Chap. 2, comparing the performance of standard machine
learning techniques with the proposed framework [2].
Finally, we discuss a capture-the-ﬂag based framework to produce data with
deception that can be used to evaluate proposed cyber attribution models—this
framework is available as open source software [3].
References
1. E. Nunes, N. Kulkarni, P. Shakarian, A. Ruef, and J. Little. Cyber-deception and attribution
in capture-the-ﬂag exercises. In Proceedings of the IEEE/ACM International Conference on
Advances in Social Networks Analysis and Mining (ASONAM), pages 962–965. ACM, 2015.
2. E. Nunes, P. Shakarian, G. I. Simari, and A. Ruef. Argumentation models for cyber attribution.
In Proceedings of the IEEE/ACM International Conference on Advances in Social Networks
Analysis and Mining (ASONAM), pages 837–844. IEEE, 2016.
3. A. Ruef, E. Nunes, P. Shakarian, and G. I. Simari. Measuring cyber attribution in games. In
Proceedings of the APWG Symposium on Electronic Crime Research (eCrime), pages 28–32,
2017.
4. P. Shakarian, G. I. Simari, G. Moores, D. Paulo, S. Parsons, M. A. Falappa, and A. Aleali.
Belief revision in structured probabilistic argumentation. Annals of Mathematics and Artiﬁcial
Intelligence, 78(3-4):259–301, 2016.
5. G. I. Simari, P. Shakarian, and M. A. Falappa. A quantitative approach to belief revision in
structured probabilistic argumentation. Annals of Mathematics and Artiﬁcial Intelligence, 76(3-
4):375–408, 2016.
6. J. Swarner.
Before WannaCry was unleashed, hackers plotted about it on the Dark Web.
2017.
Available at: http://www.slate.com/blogs/future_tense/2017/05/23/before_wannacry_
was_unleashed_hackers_plotted_about_it_on_the_dark_web.html.
7. D. A. Wheeler and G. N. Larsen. Techniques for cyber attack attribution. Technical report,
Institute for Defense Analyses, 2003.

Chapter 2
Baseline Cyber Attribution Models
2.1
Introduction
Attributing the culprit of a cyberattack is widely considered one of the major
technical and policy challenges of cybersecurity. Since the lack of ground truth
for an individual responsible for a given attack has limited previous studies, in
this chapter we take an important ﬁrst step in developing computational techniques
toward attributing the actual culprit (here, a hacking group) responsible for a given
cyberattack. We leverage DEFCON capture-the-ﬂag (CTF) exercise data that we
have processed to be amenable to various machine learning approaches. Here, we
use various classiﬁcation techniques to identify the culprit in a cyberattack and
ﬁnd that deceptive activities account for the majority of misclassiﬁed samples. We
also explore several heuristics to alleviate some of the misclassiﬁcation caused by
deception. In this chapter, we:
•
Assemble a dataset of cyberattacks with ground truth derived from the trafﬁc of
the CTF held at DEFCON 21 in 2013.
•
Analyze this dataset to identify cyberattacks where deception occurred.
•
Frame cyber attribution as a multi-label classiﬁcation problem and leverage
several machine learning approaches, ﬁnding that deceptive incidents account
for the vast majority of misclassiﬁed samples.
•
Introduce several pruning techniques and show that they can reduce the effect of
deception, as well as provide insight into the conditions in which deception was
employed by the participants of the CTF.
2.2
Dataset
In this section, we ﬁrst describe the dataset and then go into the details of how we
processed it in order to make it amenable to analysis.
© The Author(s) 2018
E. Nunes et al., Artiﬁcial Intelligence Tools for Cyber Attribution, SpringerBriefs in
Computer Science, https://doi.org/10.1007/978-3-319-73788-1_2
5

6
2
Baseline Cyber Attribution Models
2.2.1
DEFCON CTF
The DEFCON security conference sponsors and hosts a capture-the-ﬂag (CTF)
competition every year, held on site with the conference in Las Vegas, Nevada.
DEFCON CTF is one of the oldest and best-known competitions—it has the highest
average weight of all other CTF competitions on https://ctftime.org, which provides
a ranking for CTF teams and CTF competitions.
CTF competitions can be categorized according to what role the competitors
play in the competition: either red team, blue team, or a combination. In a blue
team-focused CTF, the competitors harden their systems against a red team played
by the organizers. In a combined red/blue team CTF, every team plays both blue
and red team simultaneously. The NCCDC and CDX competitions are examples of
a blue team CTF, while DEFCON CTF is a combined red/blue team. Each team
is simultaneously responsible for hardening and defending their systems as well as
identifying vulnerabilities and exploiting them in other teams’ systems.
The game environment is created primarily by the DEFCON CTF organizers;
the game focuses around programs (known in the game as services) written by
them, which are engineered to contain speciﬁc vulnerabilities. The binary image
of the service is made available to each team at the start of the game, but no
other information about the service is released. Part of the challenge of the game
is identifying the purpose of each service, as well as the vulnerabilities present in it.
Identiﬁcation of vulnerabilities serves both a defensive and offensive goal—once
a vulnerability has been identiﬁed, a team may patch it in the binary program;
additionally, the teams may create exploits for that vulnerability and use them to
attack other teams and capture digital ﬂags from their systems.
Each team is also provided with a server running the services, which contains
the digital ﬂags to be defended. To deter defensive actions such as powering off the
server or stopping the services, the white team conducts periodic availability tests
of the services running on each teams server. A team’s score is the sum of the value
of the ﬂags they have captured, minus the sum of the ﬂags that have been captured
from that team, multiplied by an availability score determined by how often the
white team was able to test that team’s services. This scoring model incentivizes
teams to keep their server online, identify the vulnerabilities in services and patch
them quickly, and exploit other teams’ services to capture their ﬂags. On the other
hand, it disincentivizes host-level blocking and shutting down services, as this would
massively impact the ﬁnal score.
This game environment can be viewed as a microcosm of the global Internet, and
the careful game of “cat and mouse” between hacking groups and companies. Teams
are free to use different technical means to discover vulnerabilities, such as fuzzing
and reverse engineering on their own programs; alternatively, they may monitor the
network data sent to their services and dynamically study the effects that network
data has on unpatched services. If a team discovers a vulnerability and uses it against
another team, the ﬁrst team may discover that their exploit is re-purposed and used
against them within minutes.

2.2
Dataset
7
2.2.2
DEFCON CTF Data
The organizers of DEFCON CTF capture all of the network trafﬁc sent and received
by each team, and publish this data at the end of the competition [6]. This includes
IP addresses for source and destination, as well as the full data sent and received
(with timestamps). This data is not available to contestants in real time; depending
on the organizers’ choice from year to year, the contestants either have a real time
feed but with the IP address obscured, or a full feed delivered on a time delay of
minutes to hours. In addition to the trafﬁc captures, copies of the vulnerable services
are also distributed; though organizers usually do not disclose the vulnerabilities
they engineered into each service, competitors frequently disclose this information
publicly after the game is ﬁnished as technical write-ups.
The full interaction of all teams in the game environment is captured by this data.
We cannot build a total picture of the game at any point in time, since there is state
information from the servers that is not captured, but any exploit attempt would have
to travel over the network and that would be observed in the data set.
2.2.3
Analysis of CTF Data
The CTF data set is very large, about 170 GB compressed; we used multiple
systems with distributed and coordinated processing in our analysis of its contents.
Fortunately, analyzing individual streams is an embarrassingly parallel task; we
identiﬁed the TCP ports associated with each vulnerable service and, from this
information, we used the open source tool tcpﬂow1 to process the network captures
into a set of ﬁles representing data sent or received on a particular connection.
This produced a corpus of data that could be searched and processed with
standard UNIX tools, like grep. Further analysis of the game environment provided
an indicator of when a data ﬁle contained an exploit. The game stored keys for
services in a standard, hard-coded location on each competitors server. By searching
for the text of this location in the data, we identiﬁed data ﬁles that contained exploits
for services.
Once these data ﬁles were generated, we analyzed some of them by hand using
the Interactive Disassembler (IDA) to determine if the data contained shell-code,
which was indeed the case. We then used an automated tool to produce a summary
of each data ﬁle as a JSON encoded element; included in this summary was a
hash of the contents of the ﬁle and a histogram of the processor instructions
contained therein. These JSON ﬁles were the ﬁnal output of the low level analysis,
transforming hundreds of gigabytes of network trafﬁc into a manageable set of facts
about exploit trafﬁc in the data. Each JSON ﬁle is a list of tuples (time-stamp, hash,
byte-histogram, instruction-histogram).These individual ﬁelds of the tuple are listed
in Table 2.1.
1https://github.com/simsong/tcpﬂow.

8
2
Baseline Cyber Attribution Models
Table 2.1 Fields in a single instance of a network attack
Field
Intuition
byte_hist
Histogram of byte sequences in the payload
inst_hist
Histogram of instructions used in the payload
from_team
The team where the payload originates (attacking team)
to_team
The team being attacked by the exploit
svc
The service that the payload is running
payload_hash
Indicates the payload used in the attack (md5)
time
Indicates the date and time of the attack
Table 2.2 Example event
from the dataset
Field
Value
byte_hist
043:245, 069:8, 03a:9, 05d:1, . . . ..
inst_hist
cmp:12, svcmi:2, subs:8, movtmi:60 . . . . . .
from_team
Men in black hats
to_team
Robot Maﬁa
svc
02345
payload_hash
2cc03b4e0053cde24400bbd80890446c
time
2013-08-03T23:45:17
This pre-processing of the network data (packets) yielded around 10 million
network attacks. There are 20 teams in the CTF competition; in order to attribute an
attack to a particular team, apart from analyzing the payloads used by the team, we
also need to analyze the behavior of the attacking team towards their adversary. For
this purpose we separated the network attacks according to the team being targeted;
thus, we have 20 such subsets and we list them by team name in Table 2.3. An
example of an event in the dataset is shown in Table 2.2.
We now discuss two important observations from the dataset, that makes the task
of attributing a observed network attack to a team difﬁcult.
Deception In the context of this dataset, we deﬁne an attack to be deceptive
whenever multiple adversaries get mapped to a single attack pattern; therefore, in
the current setting it refers to the scenario where the same exploit is used by multiple
teams to target the same team. Figure 2.1 shows the distribution of unique deception
attacks with respect to the total unique attacks in the dataset based on the target team.
These unique deceptive attacks amount to just under 35% of the total unique attacks.
Duplicate Attacks A duplicate attack occurs when the same team uses the same
payload to attack a team at different time instances. Duplicate attacks can be
attributed to two reasons: ﬁrst, when a team is trying to compromise another system,
it does not just launch a single attack but rather a wave of attacks with very little time
difference between consecutive attacks; second, once a successful payload is created
that can penetrate the defense of other systems, it is used more by the original
attacker as well as the deceptive one as compared to other payloads. We group

2.2
Dataset
9
0
100000
200000
300000
400000
T-1
T-2
T-3
T-4
T-5
T-6
T-7
T-8
T-9
T-10
T-11
T-12
T-13
T-14
T-15
T-16
T-17
T-18
T-19
T-20
Unique Attacks
Teams
Unique Attacks
Deceptive Attacks
Fig. 2.1 Unique deceptive attacks directed towards each team
0
200000
400000
600000
800000
T-1
T-2
T-3
T-4
T-5
T-6
T-7
T-8
T-9
T-10
T-11
T-12
T-13
T-14
T-15
T-16
T-17
T-18
T-19
T-20
Total Attacks
Teams
Non-Deceptive
Deceptive
Total Attacks
Fig. 2.2 Total attacks and duplicate attacks (both deceptive and non-deceptive) directed towards
each team
duplicates as being either non-deceptive or deceptive. Non-deceptive duplicate are
the duplicates of the team that ﬁrst initiated the use of a particular payload; on the
other hand, deceptive duplicates are all the attacks from the teams that are being
deceptive. The latter form a large portion of the dataset, as seen in Fig. 2.2.
Analyzing the number of teams that use a particular payload, gives us insights
into the deceptive behavior of teams. We plot the usage of unique payloads with
respect to the number of teams using them in their attacks. We use four different
categories namely payloads used by a single team, payloads used by two teams,
payloads used by three teams and payloads used by more than three teams.

10
2
Baseline Cyber Attribution Models
1
100
10000
T-1
T-2
T-3
T-4
T-5
T-6
T-7
T-8
T-9
T-10
T-11
T-12
T-13
T-14
T-15
T-16
T-17
T-18
T-19
T-20
Unique Attacks(Log scale)
One Team
Two Teams
Three Teams
More than three teams
Fig. 2.3 Attacks on each target team carried out by one team, two teams, three teams, and more
than three teams
Figure 2.3 shows the plot for each target team. A large fraction of unique payloads
fall in the ﬁrst two categories (one team and two teams).
2.3
Baseline Approaches
Since the data set contains all network information, in particular we have the ground
truth (i.e., a team from Table 2.3) available for all the samples. Hence, we can use
supervised machine learning approaches to predict the attacking team.
Decision Tree (DT) For baseline comparisons we ﬁrst implemented a decision
tree classiﬁer—this hierarchical recursive partitioning algorithm is widely used for
classiﬁcation problems [3]. We built the decision tree by ﬁnding the attribute that
maximizes information gain at each split. This attribute is termed as the best split
attribute, and is used to split the node. The higher the information gain, the more
pure the nodes that are split will be. During the testing phase, we check the test
sample for the presence or absence of the best split attribute at each node until we
reach the leaf node. The team that has the largest number of samples at the leaf node
is predicted as the attack team for the test sample. In order to avoid over-ﬁtting, we
terminate the tree when the number of samples in the node is less than 0.1% of the
training data.
Random Forest (RF) Random forest is an ensemble classiﬁcation method pro-
posed by Breiman [2]; it is based on the idea of generating multiple predictors
that are then used in combination to classify unseen samples. The strength of the
model lies in injecting randomness when building each classiﬁer, and using random
low dimensional subspaces to split the data at each node. We use a random forest

2.3
Baseline Approaches
11
Table 2.3 Teams in the CTF
competition
Notation
Team
T-1
9447
T-2
APT8
T-3
Alternatives
T-4
PPP
T-5
Robot Maﬁa
T-6
Samurai
T-7
The European Nopsled Team
T-8
WOWHacker-BIOS
T-9
[Technopandas]
T-10
Blue lotus
T-11
clgt
T-12
Men in black hats
T-13
More smoked leet chicken
T-14
pwnies
T-15
pwningyeti
T-16
Routards
T-17
raon_ASRT (whois)
T-18
Shell corp
T-19
shellphish
T-20
sutegoma2
that combines bagging [1] for each tree with random feature selection at each
node to split the data, thus generating multiple decision tree classiﬁers. To split
the data at each node we use information gain with random subspace projection,
which indicates the amount of purity in the node with respect to class labels (more
pure nodes result in higher information gain). Hence, we try to ﬁnd the splits that
maximize the information gain. The advantage of using a random forest over a single
decision tree is low variance, and the notion that weak learners when combined
together have a strong predictive power. During the test phase, each test sample
gets a prediction from each individual decision tree (weak learner) giving its own
opinion on test sample. The ﬁnal decision is made by a majority vote among
those trees.
Support Vector Machine (SVM) Support vector machines are a popular super-
vised classiﬁcation technique proposed by Vapnik [5]; they work by ﬁnding a
separating margin (a hyperplane) that maximizes the geometric distance between
classes. We used the popular LibSVM implementation [4], which is publicly
available. SVM is inherently a binary classiﬁer, and it deals with multi-class
classiﬁcation problems by implementing several 1-vs-1 or 1-vs-all binary classiﬁers,
which adds to the complexity as the number of classes increases.
Logistic Regression (LOG-REG) Logistic regression classiﬁes samples by com-
puting the odds ratio, which gives the strength of association between the features

12
2
Baseline Cyber Attribution Models
and the class. As opposed to linear regression, the output of logistic regression
is the class probability of the sample belonging to that class. We implement the
multinomial logistic regression, which handles multi-class classiﬁcation.
2.4
Experimental Results
For our baseline experiments, we separate the attacks based on the team being
targeted; thus, we have 20 attack datasets. We then sort the attack according to
time, and reserve the ﬁrst 90% of the attacks for training and the remaining 10%
for testing. Attacker prediction accuracy is used as the performance measure for
the experiment, which is deﬁned as the fraction of correctly classiﬁed test samples.
Figure 2.4 shows the accuracy for predicting the attacker for each target team; as
we can see, machine learning techniques signiﬁcantly outperform random guessing,
which would have an average accuracy of choosing 1 out of 19 teams attacking
(which on average would be correct only 5.3% of the time). For this experiment,
the random forest classiﬁer performs better than logistic regression, support vector
machine, and decision tree for all the target teams. Table 2.4 summarizes the average
performance for each method.
0
0.1
0.2
0.3
0.4
0.5
0.6
T-1
T-2
T-3
T-4
T-5
T-6
T-7
T-8
T-9
T-10
T-11
T-12
T-13
T-14
T-15
T-16
T-17
T-18
T-19
T-20
Accuracy
Teams
LOG-REG
RF
SVM
DT
Fig. 2.4 Team prediction accuracy for LOG-REG, RF, SVM, and DT
Table 2.4 Summary of
prediction results averaged
across all teams
Method
Average performance
Decision tree (DT)
0.26
Logistic regression (LOG-REG)
0.31
Support vector machine (SVM)
0.30
Random forest (RF)
0.37
The bold value indicate the best performance of the model

2.4
Experimental Results
13
0.0
0.2
0.4
0.6
0.8
1.0
T-1
T-2
T-3
T-4
T-5
T-6
T-7
T-8
T-9
T-10
T-11
T-12
T-13
T-14
T-15
T-16
T-17
T-18
T-19
T-20
Fraction of Misclassified Samples
Teams
Non-Deceptive Duplicates
Deceptive Duplicates
Unseen payloads
Fig. 2.5 Sources of error in the misclassiﬁed samples
2.4.1
Misclassiﬁed Samples
Misclassiﬁcation can be attributed to the following sources,
•
Non-deceptive duplicate attacks attributed to one of the deceptive teams.
•
Deceptive duplicates attributed to some other deceptive team.
•
Payloads that were not encountered during the training phase.
The ﬁrst two sources of error make up the majority of misclassiﬁcations, since a
given attack can be attributed to any of the 19 teams.
Figure 2.5 shows the distribution of the above-mentioned sources of misclas-
siﬁcation for each team. Deceptive duplicates comprise the majority of misclassi-
ﬁcations; this is not surprising, given the fact that deceptive duplicates make up
almost 90% of the total attacks (see Fig. 2.2).
2.4.2
Pruning
We now explore different pruning techniques in order to address misclassiﬁcation
issues with respect to deceptive and non-deceptive duplicates. Pruning is only
applied to the training data, while the test data is maintained at 10% as mentioned in
Sect. 2.4; given that it is the best performer, we use the random forest classiﬁer for
all the pruning techniques. Our proposed pruning techniques are brieﬂy described
as follows:
•
All-but-majority: Only consider the duplicates of the most attacking team given
a payload, and prune other duplicates.

14
2
Baseline Cyber Attribution Models
Table 2.5 Summary of
prediction results averaged
across all teams
Method
Average performance
Baseline approach (RF)
0.37
All-but-majority pruning (RF)
0.40
All-but-K-majority pruning (RF)
0.42
All-but-earliest pruning (RF)
0.34
All-but-most-recent pruning (RF)
0.36
The bold value indicate the best performance of the model
Table 2.6 Pruning technique
performance comparison
Teams
RF
P-1(RF)
P-2(RF)
P-3(RF)
P-4(RF)
T-1
0.45
0.16
0.46
0.15
0.15
T-2
0.22
0.28
0.30
0.15
0.14
T-3
0.30
0.53
0.29
0.57
0.57
T-4
0.26
0.33
0.27
0.31
0.32
T-5
0.26
0.38
0.45
0.40
0.42
T-6
0.50
0.27
0.24
0.31
0.26
T-7
0.45
0.59
0.58
0.19
0.49
T-8
0.42
0.52
0.52
0.51
0.55
T-9
0.41
0.65
0.68
0.52
0.53
T-10
0.30
0.54
0.34
0.55
0.57
T-11
0.37
0.27
0.35
0.27
0.29
T-12
0.24
0.37
0.37
0.25
0.22
T-13
0.35
0.27
0.37
0.29
0.27
T-14
0.42
0.27
0.40
0.30
0.30
T-15
0.30
0.20
0.27
0.21
0.20
T-16
0.42
0.28
0.22
0.32
0.31
T-17
0.43
0.45
0.35
0.43
0.40
T-18
0.48
0.39
0.43
0.41
0.40
T-19
0.41
0.65
0.58
0.54
0.60
T-20
0.48
0.16
0.16
0.16
0.17
The bold values indicate the best performance of the model
•
All-but-K-majority: Only consider the duplicates of the top k most frequent
attacks given a payload, and prune the rest of the duplicates.
•
All-but-earliest-majority: Retain the duplicates of the team that initiates the
attack given a payload, while the rest of the duplicates are pruned.
•
All-but-most-recent-majority: Retain the duplicates of the team that last used the
payload in the training data, while the rest of the duplicates are pruned.
Table 2.5 gives the summary of the prediction results for all the pruning techniques
in comparison with the random forest baseline approach; All-but-K-majority works
best, with an average accuracy of 0.42.
All-But-Majority (P-1) In this pruning technique, for each payload we only retain
duplicates of the most frequent attacking team and prune the duplicates of all
other teams. This pruned set is then used to train the random forest classiﬁer.
Table 2.6 shows the classiﬁer performance in comparison with the baseline method.

2.5
Conclusions
15
All-but-majority pruning has better performance on the test set than the baseline
approach for 11 of the 20 teams. Using this pruning technique beneﬁts the majority
of the teams, as the prediction accuracy improves for them, but for some teams
the performance drops. The reason for this drop in performance is due to the
fact that the training set gets dominated by a single team that does not have a
majority in the testing set. Since the majority team gets represented in most of the
leaves of the random forest classiﬁer, it gets predicted more often, leading to many
misclassiﬁcations.
All-But-K-Majority (P-2) In order to address the issue of one team dominating in
the training set, we use all-but-K-majority, where we consider the k most frequent
teams for a payload under consideration. After trying out different values of k we
select k D 3, which gives the best performance. For higher values of k, the pruning
behaves like the baseline approach and for lower values it behaves like All-but-
majority. On average, each team gains about 40; 000 samples in the training set as
compared to all-but-majority pruning. Table 2.6 shows the classiﬁer performance;
in this case, pruning again performs better than baseline in 11 out of 20 teams, but
as compared to all-but-majority the performance for most teams is better.
All-But-Earliest (P-3) In this kind of pruning we only retain the duplicates of
the team that initiated the attack using a particular payload; this technique thus
retains all the non-deceptive duplicates while getting rid of the deceptive ones.
Table 2.6 shows the classiﬁer performance; as we can see, it performs better than
the baseline approach for 8 out of the 20 teams. Comparing this result to all-
but-majority (including all-but-K-majority), it turns out that deceptive duplicates
are informative in attributing an attack to a team, and should thus not be ignored
completely.
All-But-Most-Recent (P-4) Here we repeat a similar procedure to all-but-earliest,
but instead of retaining the duplicates of the team that initiated an attack, we retain
the duplicates of the team that used the payload last in the training set. Since the
data is sorted according to time, the last attacker becomes the most recent attacker
for the test set. Table 2.6 shows the classiﬁer performance.
2.5
Conclusions
In this chapter, we studied cyber attribution by examining DEFCON CTF data—this
provided us with ground truth regarding the culprit responsible for each attack. We
framed cyber attribution as a classiﬁcation problem, and examined it using several
machine learning approaches [7]. Our main ﬁnding was that deceptive incidents
account for the vast majority of misclassiﬁed samples. We also went on to introduce
heuristic pruning techniques that somewhat alleviate this problem. As we will see
later on, more sophisticated techniques can signiﬁcantly help push accuracy up.

16
2
Baseline Cyber Attribution Models
References
1. L. Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.
2. L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.
3. L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classiﬁcation and regression trees.
CRC press, 1984.
4. C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector machines. ACM Transactions
on Intelligent Systems and Technology (TIST), 2(3):27, 2011.
5. C. Cortes and V. Vapnik. Support vector networks. Machine learning, 20(3):273–297, 1995.
6. DEFCON. Capture the Flag. 2013. https://media.defcon.org/.
7. E. Nunes, N. Kulkarni, P. Shakarian, A. Ruef, and J. Little. Cyber-deception and attribution
in capture-the-ﬂag exercises. In Proceedings of the IEEE/ACM International Conference on
Advances in Social Networks Analysis and Mining (ASONAM), pages 962–965. ACM, 2015.

Chapter 3
Argumentation-Based Cyber Attribution:
The DeLP3E Model
3.1
Introduction
In cyber-attribution, knowledge bases consisting of all the information that is
available about a speciﬁc domain, along with all the available information about the
current state of affairs, will typically contain contradictory data—that is because the
knowledge base will have been constructed using data from different sources that
disagree. We noted several examples of this for cyber-attribution in Chap. 2. This
data will also, typically, contain some measure of uncertainty; thus, key problems
that need to be addressed by formalisms for knowledge representation are the ability
to handle contradictory information and to perform inference in the presence of
uncertainty. These general problem aspects are very well suited for cyber attribution.
We begin by providing a quick, motivating, sketch: the basic information in this
scenario comes from a variety of different sources that only have a partial view of the
domain, and since these sources disagree, having contradictory information in the
knowledge base is unavoidable. In a cyberattack, it is not uncommon for the attacker
to leave some false pieces of evidence with the goal of misleading the investigation,
adding further contradictory information. Virtually none of the evidence that is
gathered after an attack is conclusive, so there is uncertainty in the information that
must be handled. Finally, since in responding to an attack new information is added
to information that was gathered after previous attacks, it is necessary to update
the knowledge base. In particular, if new information contradicts old information, it
may be necessary to perform belief revision to recover consistency of some parts of
the knowledge-base. However, we postpone the discussion of belief revision until
Chap. 4, which is entirely devoted to this issue.
From this discussion we distill the requirements on any knowledge representation
formalism that will be used in real-world cyber attribution applications. Such a
formalism must be able to:
1. represent contradictory and uncertain information;
© The Author(s) 2018
E. Nunes et al., Artiﬁcial Intelligence Tools for Cyber Attribution, SpringerBriefs in
Computer Science, https://doi.org/10.1007/978-3-319-73788-1_3
17

18
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
Table 3.1 Examples of the kind of information that could be represented in the environmental and
analytical models in a cybersecurity application domain
Environmental model (EM)
Analytical model (AM)
Malware X was compiled on a system
Malware X was compiled on a system in English-
using the English language
speaking country Y
Country Y and country Z are currently
Country Y has a motive to launch a cyberattack
at war
against country Z
Malware W and malware X were created
Malware W and malware X are related
in a similar coding style
Country Y has a signiﬁcant investment
Country Y has the capability to conduct a
in math-science-engineering (MSE)
cyberattack
education
2. answer queries on a knowledge base; and
3. handle revisions to the knowledge base.
This chapter presents a formalism called DeLP3E that meets all these requirements.
A DeLP3E model consists of two parts, an environmental model (EM) and an
analytical model (AM), which represent different aspects of a scenario. The idea
is that the analytical model contains all the background information that is available
for the analysis of the scenario. We envisage that this information is a combination
of ontological information about the world; for instance (to take the old example),
“Tweety is a penguin”, “penguins are birds” and “penguins do not ﬂy”, and
commonsense information that is relevant, for example “birds generally ﬂy”. As can
be seen from this small example, the AM can be inconsistent, and so we will choose
a formal model for the AM that can cope with inconsistency. On the other hand, the
environmental model is intended to contain evidence that has been collected about
a speciﬁc situation (an instance of the more general model in the AM) about which
queries will be answered. In the classic example, “Tweety is a penguin” would be
an element of the EM, but the EM can also be more subtle than this, allowing for
the representation of uncertain information. If we did not know for sure that Tweety
was a penguin, but just had some suggestive evidence that this is so, we could for
example include in the EM the fact that “Tweety is a penguin” has a probability
of 0.8 of being true. The EM is not limited to facts—we could also choose to model
our evidence about Tweety with “Tweety is a bird” and “Tweety is black and white”
and the rule that “Black and white birds have a probability of 0.8 of being penguins”.
A more complex pair of EM and AM, which relates to our motivating cybersecurity
example, is given in Table 3.1.
The languages used in the AM and the EM are then related together though an
annotation function (AF), which pairs formulas in the EM and the AM. Reasoning
then consists of answering a query in the AM—when the AM is inconsistent this will
involve establishing the relevant consistent subset to answer the query, computing
the probability of the elements of the EM and, through the annotation function,
establishing the probabilities that correspond to the answer to the initial query. Thus,
in the Tweety example, to answer a query about whether Tweety can ﬂy, the AM

3.1
Introduction
19
would reason about this truth or falsity of the proposition “Tweety ﬂies”, the AF
would identify which elements of the EM relate to this query, and the EM would
provide a probability for these elements. The probability of the answer to the query,
in this case either “Tweety ﬂies” or “Tweety does not ﬂy”, could then be computed.
The inference of this probability is what we call entailment.
In our vision, DeLP3E is less a speciﬁc formalism and more a family of
formalisms where different formal models for handling uncertainty can be used
for the EM, and different logical reasoning models can be used for the AM. In
this chapter, to make the discussion concrete, we make some speciﬁc choices. In
particular, the EM is based on Nilsson’s Probabilistic Logic [14], and the AM is
based on the PreDeLP argumentation model from [13]. At the heart of PreDeLP is
the notion of presumptions, elements of the knowledge base that can be presumed
(assumed) to be true. This makes for a very natural connection to the EM—
presumptions are elements of the AM that connect (through the annotation function)
to elements of the EM, in the same way the other elements of the AM do. Thus, the
presumptions will have a probability associated with them, and this is then used to
establish the probability of the answer to the initial query.
This discussion has covered the requirement for DeLP3E to deal with inconsis-
tency and uncertainty, and identiﬁed the need for inference. The ﬁnal requirement
is for the ability to revise the knowledge base, in particular the ability to perform
belief revision in the sense of [1, 7, 8]. Given that belief revision is concerned with
maintaining the consistency of a set of beliefs and that DeLP3E is built around
an argumentation system that can handle inconsistency, at ﬁrst glance it might
not be obvious why belief revision is required. However, after some reﬂection it
becomes clear that all three parts of a DeLP3E model—the environmental model,
the analytical model, and the annotation function—may require revision, at least
in the instantiation of DeLP3E that we consider here. The EM is underpinned by
probability theory, and this places the constraint that the set of propositions used in
the EM be consistent (a constraint that would not necessarily exist if we were to use
a different uncertainty handling mechanism). The AM is built using PreDeLP, and
though there can be inconsistency in some elements of a PreDeLP model, the strict
rules and facts used to answer a speciﬁc query must be consistent, and so belief
revision is required (if we built the AM using an argumentation system that only
included defeasible knowledge, as in [16], belief revision would not be required).
Finally, though there is never a strict requirement for belief revision of the AF, as
we will discuss in Chap. 4, providing the ability to revise the annotation function
can help us to avoid revising other aspects of the model.
3.1.1
Application to the Cyber Attribution Problem
AS we have already discussed in previous chapters, cyber attribution—the problem
of determining who was responsible for a given cyberoperation, be it an incident
of attack, reconnaissance, or information theft [19]—is an important issue. The
difﬁculty of this problem stems not only from the amount of effort required to ﬁnd

20
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
forensic clues, but also the ease with which an attacker can plant false clues to
mislead security personnel. Further, while techniques such as forensics and reverse-
engineering [2], source tracking [25], honeypots [23], and sinkholing [17] are
commonly employed to ﬁnd evidence that can lead to attribution, it is unclear how
this evidence is to be combined and reasoned about. In some cases, such evidence
is augmented with normal intelligence collection, such as human intelligence
(HUMINT), signals intelligence (SIGINT) and other means—this adds additional
complications to the task of attributing a given operation.
In essence, cyber attribution is a highly-technical intelligence analysis problem
where an analyst must consider a variety of sources, each with its associated
level of conﬁdence, to provide a decision maker (e.g., a system administrator or
Chief Information Ofﬁcer) with insight into who conducted a given operation.
Indeed, while previous cyber attribution approaches only consider a single source
of information, our approach takes into account multiple sources of information due
to its ability to deal with inconsistency. As it is well-known that people’s ability to
conduct intelligence analysis is limited [9], and due to the highly technical nature of
many cyber evidence-gathering techniques, an automated reasoning system would
be best suited for the task. Such a system must be able to accomplish several goals:
•
Reason about evidence in a formal, principled manner, i.e., relying on strong
computational and mathematical foundations.
•
Consider evidence for cyber attribution associated with some level of uncertainty
(expressed via probabilities).
•
Consider logical rules that allow for the system to draw conclusions based on
certain pieces of evidence and iteratively apply such rules.
•
Consider pieces of information that may not be compatible with each other,
decide which information is most relevant, and express why.
•
Attribute a given cyberoperation based on the above-described features and
provide the analyst with the ability to understand how the system arrived at that
conclusion.
The ﬁt between these requirements and the abilities of DeLP3E led us to develop
a use case based around cyber attribution1 as a way of showcasing the functionality
of DeLP3E. This use case is described in detail in Sect. 3.5.
3.1.2
Structure of the Chapter
The structure of the chapter broadly follows the ﬁrst two of the requirements
identiﬁed above. First, in Sect. 3.2 we introduce the environmental and analytical
model, where the environmental model makes use of Nilsson’s probabilistic
1The causality is a little more complicated than this sentence suggests. The cyber attribution
problem was indeed the original motivation for the development of DeLP3E, and elements of
the example evolved along with the formalism.

3.2
Technical Preliminaries
21
logic [14] and the analytical model builds upon PreDeLP [13]. The resulting
framework is the general-purpose probabilistic argumentation language DeLP3E,
which stands for Defeasible Logic Programming with
Presumptions and
Probabilistic Environment. This is formally laid out in Sect. 3.3, which also studies
the entailment problem for DeLP3E. Finally, Sect. 3.5 then presents a use case of
DeLP3E in the context of cyber attribution.
3.2
Technical Preliminaries
This section presents the two main building blocks of the DeLP3E framework: the
environmental model and the analytical model.
3.2.1
Basic Language
We assume sets of variables and constants, denoted with V and C, respectively. In
the rest of this chapter, we will follow the convention from the logic programming
literature and use capital letters to represent variables (e.g., X; Y; Z) and lowercase
letters to represent constants.
The next component of the language is a set of predicate symbols. Each predicate
symbol has an arity bounded by a constant value; the EM and AM use separate sets
of predicate symbols, denoted with PEM; PAM, respectively—the two models can,
however, share variables and constants.
As usual, a term is composed of either a variable or a constant. Given terms
t1; : : : ; tn and n-ary predicate symbol p, p.t1; : : : ; tn/ is called an atom; if t1; : : : ; tn
are constants, then the atom is said to be ground. The sets of all ground atoms for
the EM and AM are denoted with GEM and GAM, respectively.
Given a set of ground atoms, a world is any subset of atoms—those that belong
to the set are said to be true in the world, while those that do not are false. Therefore,
there are 2jGEMj possible worlds in the EM and 2jGAMj worlds in the AM; these sets
are denoted with WEM and WAM, respectively. In order to avoid worlds that do not
model possible situations given a particular domain, we include integrity constraints
of the form oneOf.A 0/, where A 0 is a subset of ground atoms. Intuitively, such a
constraint states that any world where more than one of the atoms from set A 0
appears is invalid. We use ICEM and ICAM to denote the sets of integrity constraints
for the EM and AM, respectively, and the sets of worlds that conform to these
constraints is denoted with WEM.ICEM/ and WAM.ICAM/, respectively.
Finally, logical formulas arise from the combination of atoms using the tradi-
tional connectives (^, _, and :). As usual, we say that a world w satisﬁes formula
f, written w ˆ f, iff: (i) If f is an atom, then w ˆ f iff f 2 w; (ii) if f D :f 0 then
w ˆ f iff w 6ˆ f 0; (iii) if f D f 0 ^ f 00 then w ˆ f iff w ˆ f 0 and w ˆ f 00; and (iv) if
f D f 0 _ f 00 then w ˆ f iff w ˆ f 0 or w ˆ f 00. We use the notation formEM; formAM
to denote the set of all possible (ground) formulas in the EM and AM, respectively.

22
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
Example 3.1 Thus, the following are terms
a b c d e f p.X/
g h i j
k
p.a/
and the following are formulas using those terms:
a d ^ e
k
b f ^ g ^ h
c i _ :j

3.2.2
Environmental Model
The EM is used to describe the probabilistic knowledge that we have about the
domain. In general, the EM contains knowledge such as evidence, uncertain facts,
or knowledge about agents and systems. Here we base the EM on the probabilistic
logic of [14], which we now brieﬂy review.
Deﬁnition 3.1 Let f be a formula over PEM, V, and C, p 2 Œ0; 1, and  2
Œ0; min.p; 1  p/. A probabilistic formula is of the form f W p ˙ . A set KEM
of probabilistic formulas is called a probabilistic knowledge base.
In the above deﬁnition, the number  is referred to as an error tolerance. Intuitively,
the probabilistic formula f W p˙ is interpreted as “formula f is true with probability
between p and pC”. Note that there are no further constraints over this interval
apart from those imposed by other probabilistic formulas in the knowledge base.
The uncertainty regarding the probability values stems from the fact that certain
assumptions (such as probabilistic independence between all formulas) may not
hold in the environment being modeled.
Example 3.2 Consider the following set KEM:
f1 D a W 0:7 ˙ 0:2
f4 D d ^ e
W 0:8 ˙ 0:1
f7 D k
W 1 ˙ 0
f2 D b W 0:3 ˙ 0:1
f5 D f ^ g ^ h W 0:5 ˙ 0:1
f8 D a ^ b W 0:3 ˙ 0:1
f3 D c W 0:6 ˙ 0:2
f6 D i _ :j
W 0:8 ˙ 0:2
Throughout the chapter, we also use K 0
EM D f f1; f2; f3g

A set of probabilistic formulas describes a set of possible probability distributions
Pr over the set WEM.ICEM/. We say that probability distribution Pr satisﬁes
probabilistic formula f W p ˙  iff:
p   
X
w2WEM.ICEM/;wˆf
Pr.w/  p C :
A probability distribution over WEM.ICEM/ satisﬁes KEM iff it satisﬁes all proba-
bilistic formulas in KEM.
Given a probabilistic knowledge base and a (non-probabilistic) formula q, the
maximum entailment problem seeks to identify real numbers p;  such that all valid

3.2
Technical Preliminaries
23
probability distributions Pr that satisfy KEM also satisfy q W p ˙ , and there does
not exist p0; 0 s.t. Œ p; pC  Œ p00; p0C0, where all probability distributions
Pr that satisfy KEM also satisfy q W p0 ˙ 0. In order to solve this problem we must
solve the linear program deﬁned below.
Deﬁnition 3.2 Given a knowledge base KEM and a formula q, we have a variable
xi for each wi 2 WEM.ICEM/. Each variable xi corresponds with the probability of wi
occurring.
•
For each fj W pj ˙ j 2 KEM, there is a constraint of the form:
pj  j 
X
wi2WEM.ICEM/s:t: wiˆfj
xi  pj C j:
•
We also have the constraint:
X
wi2WEM.ICEM/
xi D 1:
•
The objective is to minimize the function:
X
wi2WEM.ICEM/s:t: wiˆq
xi:
We use the notation EP-LP-MIN.KEM; q/ to refer to the value of the objective
function in the solution to the EM-LP-MIN constraints.
The next step is to solve the linear program a second time, but this time
maximizing the objective function (we shall refer to this as EM-LP-MAX)—let `
and u be the results of these operations, respectively. In [14], it is shown that:
 D u  `
2
and p D ` C 
is the solution to the maximum entailment problem. We note that although the above
linear program has an exponential number of variables in the worst case (i.e., no
integrity constraints), the presence of constraints has the potential to greatly reduce
this space. Further, there are also good heuristics (cf. [10, 22]) that have been shown
to provide highly accurate approximations with a reduced-size linear program.
Example 3.3 Consider KB K 0
EM from Example 3.2 and a set of ground atoms
restricted to those that appear in that program; we have the following worlds:
w1 D fa; b; cg w2 D fa; bg w3 D fa; cg w4 D fb; cg
w5 D fbg
w6 D fag
w7 D fcg
w8 D ;
and suppose we wish to compute the probability for formula q D a _ c.

24
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
For each formula in KEM we have a constraint, and for each world above we have
a variable. An objective function is created based on the worlds that satisfy the query
formula (in this case, worlds w1; w2; w3; w4; w6; w7). Solving EP-LP-MAX.K 0
EM; q/
and EP-LP-MIN.K 0
EM; q/, we obtain the solution 0:9 ˙ 0:1. Thus, the former can
be written as follows:
max
x1 C x2 C x3 C x4 C x6 C x7
w:r:t: W
0:6 
x1 C x2 C x3 C x6
 0:8
0:2 
x1 C x2 C x4 C x5
 0:4
0:8 
x1 C x3 C x4 C x7
 1
x1 C x2 C x3 C x4 C x5 C x6 C x7 C x8 D 1
From this, we can solve EP-LP-MAX.K 0
EM; q/ and, after an easy modiﬁcation,
EP-LP-MIN.K 0
EM; q/, and obtain the solution 0:9 ˙ 0:1.

3.2.3
Analytical Model
The analytical model contains information that a user may conclude based on the
information in the environmental model. While the EM contains information that
can have probabilities associated with it, statements in the AM can be either true
or false depending on a certain combination (or several possible combinations) of
statements from the EM.
For the AM, we choose to represent information using a structured argumentation
framework [15] since this kind of formalism meets the representational
requirements discussed in the introduction. Unlike the EM, which describes
probabilistic information about the state of the real world, the AM must allow for
competing ideas. Therefore, it must be able to represent contradictory information.
The algorithmic approach we shall later describe allows for the creation of
arguments based on the AM that may “compete” with each other to answer a
given query. In this competition—known as a dialectical process—one argument
may defeat another based on a comparison criterion that determines the prevailing
argument. Resulting from this process, certain arguments are warranted (those that
are not defeated by other arguments), thereby providing a suitable explanation for
the answer to a given query.
The transparency provided by the system can allow knowledge engineers and
users of the system to identify potentially incorrect input information and ﬁne-tune
the models or, alternatively, collect more information. In short, argumentation-
based reasoning has been studied as a natural way to manage a set of inconsistent
information—it is the way humans settle disputes. As we will see, another desirable
characteristic of (structured) argumentation frameworks is that, once a conclusion
is reached, we are left with an explanation of how we arrived at it and information
about why a given argument is warranted; this is very important information for
users to have.

3.2
Technical Preliminaries
25
The formal model that we use for the AM is Defeasible Logic Programming
with Presumptions (PreDeLP) [13], a formalism combining logic programming with
defeasible argumentation. Here, we brieﬂy recall the basics of PreDeLP—we refer
the reader to [6, 13] for the complete presentation. Formally, we use the notation
˘AM D .; ˝; ˚; /
to denote a PreDeLP program, where ˝ is a set of strict rules,  is a set of facts,
 is a set of defeasible rules, and ˚ is a set of presumptions. We now deﬁne these
constructs formally.
Facts () are ground literals representing atomic information or its negation, using
strong negation “:”. Note that all of the literals in our framework must be formed
with a predicate from the set PAM. Note that information in the form of facts cannot
be contradicted. We will use the notation Œ to denote the set of all possible facts.
Strict Rules (˝) represent non-defeasible cause-and-effect information that resem-
bles an implication (though the semantics is different since the contrapositive does
not hold) and are of the form L0  L1; : : : ; Ln, where L0 is a ground literal and
fLigi>0 is a set of ground literals. We will use the notation Œ˝ to denote the set of
all possible strict rules.
Presumptions (˚) are ground literals of the same form as facts, except that they
are not taken as being true but rather are defeasible, which means that they can be
contradicted. Presumptions are denoted in the same manner as facts, except that
the symbol – is added. We note that, epistemologically, presumptions cannot be
treated as special cases of defeasible rules; intuitively, this is because unless further
criteria are applied, an argument that uses a set of presumptions should be defeated
by arguments that use a subset of them, and this would not necessarily be the case
if presumptions were expressed as defeasible rules. The treatment of presumptions
in this manner also necessitates an extension to generalized speciﬁcity; we refer the
interested reader to [13] for further details.
Defeasible Rules () represent tentative knowledge that can be used if nothing can
be posed against it. Just as presumptions are the defeasible counterpart of facts,
defeasible rules are the defeasible counterpart of strict rules. They are of the form
L0 –L1; : : : ; Ln, where L0 is a ground literal and fLigi>0 is a set of ground literals. In
both strict and defeasible rules, strong negation is allowed in the head of rules, and
hence may be used to represent contradictory knowledge.
Even though the above constructs are ground, we allow for schematic versions
with variables that are used to represent sets of ground rules. In Fig. 3.1, we provide
an example ˘AM of a ground knowledge base. (Figure 3.7 gives an example of a
non-ground knowledge base.)
Arguments Given a query in the form of a ground atom, the goal is to derive
arguments for and against its validity—derivation follows the mechanism of logic
programming [12]. Since rule heads can contain strong negation, it is possible
to defeasibly derive contradictory literals from a program. For the treatment
of contradictory knowledge, PreDeLP incorporates a defeasible argumentation

26
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
Fig. 3.1 A ground argumentation framework
formalism that allows the identiﬁcation of the pieces of knowledge that are in
conﬂict and, through the previously mentioned dialectical process, decides which
information prevails as warranted. This dialectical process involves the construction
and evaluation of arguments, building a dialectical tree in the process. Arguments
are formally deﬁned next.
Deﬁnition 3.3 An argument hA ; Li for a literal L is a pair of the literal and a
(possibly empty) set of the AM (A  ˘AM) that provides a minimal proof for
L meeting the following requirements: (i) L is defeasibly derived from A ; (ii)
˝ [  [ A is not contradictory; and (iii) A is a minimal subset of  [ ˚.
Literal L is called the conclusion supported by the argument, and A is the
support of the argument. An argument hB; Li is a subargument of hA ; L0i iff
B  A . An argument hA ; Li is presumptive iff A \ ˚ is not empty. We will also
use ˝.A / D A \ ˝, .A / D A \ , .A / D A \ , and ˚.A / D A \ ˚.
For convenience, we may sometimes call an argument by its support. (e.g. argument
A instead of argument hA ; Li.
Our deﬁnition differs slightly from that of [21], where DeLP is introduced, as we
include strict rules and facts as part of arguments—this is due to the fact that
in our framework, the components of an argument can only be used in certain
environmental conditions. Hence, a fact may be true in one EM world and not
another. This causes us to include facts and strict rules as part of the argument.
We discuss this further in Sect. 3.3 (page 30).
Deﬁnition 3.4 A literal is derived from an argument if it appears as a fact or a
presumption in the argument or appears in the head of a strict rule or a defeasible
rule where all the literals in the body of that strict rule or defeasible rule are derived
from that argument.
Example 3.4 Figure 3.2 shows example arguments based on the knowledge base
from Fig. 3.1. Note that hA5; ui is a sub-argument of hA2; si and hA3; si.

Given an argument hA1; L1i, counter-arguments are arguments that contradict
it. Argument hA1; L1i is said to counterargue or attack hA2; L2i at a literal L0 iff
there exists a subargument hA ; L0i of hA2; L2i such that the set ˝.A1/ [ ˝.A2/ [
.A1/ [ .A2/ [ fL1; L0g is inconsistent.

3.2
Technical Preliminaries
27
Fig. 3.2 Example ground arguments from the framework of Fig. 3.1
Example 3.5 Consider the arguments from Example 3.4. The following are some of
the attack relationships between them: A1, A2, A3, and A4 all attack A6; A5 attacks
A7; and A7 attacks A2.

A proper defeater of an argument hA; Li is a counter-argument that—by some
criterion—is considered to be better than hA; Li; if the two are incomparable
according to this criterion, the counterargument is said to be a blocking defeater.
An important characteristic of PreDeLP is that the argument comparison criterion
is modular, and thus the most appropriate criterion for the domain that is being
represented can be selected; the default criterion used in classical defeasible logic
programming (from which PreDeLP is derived) is generalized speciﬁcity [24],
though an extension of this criterion is required for arguments using presump-
tions [13]. We brieﬂy recall this criterion next—the ﬁrst deﬁnition is for generalized
speciﬁcity, which is subsequently used in the deﬁnition of presumption-enabled
speciﬁcity.
Deﬁnition 3.5 (Generalized Speciﬁcity) Let ˘AM D .; ˝; ˚; / be a PreDeLP
program and let F be the set of all literals that have a defeasible derivation from
˘AM. An argument hA1; L1i is preferred to hA2; L2i, denoted with A1 PS A2 iff
the two following conditions hold:
(1) For all H  F, ˝ [H is non-contradictory: if there is a derivation for L1 from
˝ [ H [ DR.A1/, and there is no derivation for L1 from ˝ [ H, then there is a
derivation for L2 from ˝ [ H [ DR.A2/.
(2) There is at least one set H0  F, ˝ [ H0 is non-contradictory, such that there
is a derivation for L2 from ˝ [ H0 [ DR.A2/, there is no derivation for L2 from
˝ [ H0, and there is no derivation for L1 from ˝ [ H [ DR.A1/.
Intuitively, the principle of speciﬁcity says that, in the presence of two conﬂicting
lines of argument about a proposition, the one that uses more of the available
information is more convincing. Returning to the Tweety example: there are
arguments stating both that Tweety ﬂies (because it is a bird) and that Tweety doesn’t
ﬂy (because it is a penguin). The latter uses more information about Tweety—it
is more speciﬁc because it is information that Tweety is not just a bird, but is a
penguin-bird, the subset of birds that are penguins—and is thus the stronger of the
two.
Deﬁnition 3.6 (Presumption-Enabled Speciﬁcity [13]) Given PreDeLP program
˘AM D .; ˝; ˚; /, an argument hA1; L1i is preferred to hA2; L2i, denoted with
A1  A2 iff any of the following conditions hold:

28
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
(1) hA1; L1i and hA2; L2i are both factual, which is an argument using none of the
presumptions or defeasible rules and hA1; L1i PS hA2; L2i.
(2) hA1; L1i is a factual argument and hA2; L2i is a presumptive argument, which
is an argument using at least one of the presumptions or defeasible rules.
(3) hA1; L1i and hA2; L2i are presumptive arguments, and
(a) ˚.A1/ ¨ ˚.A2/ or,
(b) ˚.A1/ D ˚.A2/ and hA1; L1i PS hA2; L2i.
Generally, if A and B are arguments with rules X and Y, respectively and X  Y,
then A is stronger than B. This also holds when A and B use presumptions P1
and P2, resp., and P1  P2.
Example 3.6 The following are some relationships between arguments from Exam-
ple 3.4, based on Deﬁnitions 3.5 and 3.6:
•
A1 and A6 are incomparable (blocking defeaters);
•
A6  A2, and thus A6 defeats A2;
•
A5 and A7 are incomparable (blocking defeaters).

A sequence of arguments called an argumentation line thus arises from this
attack relation, where each argument defeats its predecessor. To avoid undesirable
sequences, which may represent circular argumentation lines, in DELP an argu-
mentation line is acceptable if it satisﬁes certain constraints (see below). A literal L
is warranted if there exists a non-defeated argument A supporting L.
Deﬁnition 3.7 (Adapted from [6]) Let ˘AM
D .; ˝; ˚; / be a PreDeLP
program. Two arguments hA1; L1i and hA2; L2i are concordant iff the set A1 [ A2
is non-contradictory.
Deﬁnition 3.8 ([6]) Let  be an argumentation line.  is an acceptable argumen-
tation line iff:
(1)  is a ﬁnite sequence.
(2) The set S, of supporting arguments is concordant, and the set I of interfering
arguments is concordant.
(3) No argument hAk; Lki in  is a subargument of an argument hAi; Lii appearing
earlier in  .i < k/
(4) For all i, such that the argument hAi; Kii is a blocking defeater for hAi1; Ki1i,
if hAiC1; KiC1i exists, then hAiC1; KiC1i is a proper defeater for hAi; Kii.
Clearly, there can be more than one defeater for a particular argument hA ; Li.
Therefore, many acceptable argumentation lines could arise from hA ; Li, leading
to a tree structure. The tree is built from the set of all argumentation lines rooted
in the initial argument. In a dialectical tree, every node (except the root) represents
a defeater of its parent, and leaves correspond to undefeated arguments. Each path
from the root to a leaf corresponds to a different acceptable argumentation line.
A dialectical tree provides a structure for considering all the possible acceptable
argumentation lines that can be generated for deciding whether an argument
is defeated. This tree is called dialectical because it represents an exhaustive

3.3
The DeLP3E Framework
29
dialectical2 analysis for the argument in its root. For a given argument hA ; Li, we
denote the corresponding dialectical tree as T .hA ; Li/.
Given a literal L and an argument hA ; Li, in order to decide whether or not a
literal L is warranted, every node in the dialectical tree T .hA ; Li/ is recursively
marked as “D” (defeated) or “U” (undefeated), obtaining a marked dialectical tree
T .hA ; Li/ as follows:
1. All leaves in T .hA ; Li/ are marked as “U” s, and
2. Let hB; qi be an inner node of T .hA ; Li/. Then hB; qi will be marked as “U”
iff every child of hB; qi is marked as “D”. The node hB; qi will be marked as
“D” iff it has at least a child marked as “U”.
Given an argument hA ; Li obtained from ˘AM, if the root of T .hA ; Li/
is marked as “U”, then we will say that T .hA ; hi/ warrants L and that L is
warranted from ˘AM. (Warranted arguments correspond to those in the grounded
extension of a Dung argumentation system [4].) There is a further requirement when
the arguments in the dialectical tree contain presumptions—the conjunction of all
presumptions used in even levels of the tree must be consistent. This can give rise
to multiple trees for a given literal, as there can potentially be different arguments
that make contradictory assumptions.
We can then extend the idea of a dialectical tree to a dialectical forest. For a
given literal L, a dialectical forest F.L/ consists of the set of dialectical trees for all
arguments for L. We shall denote a marked dialectical forest, the set of all marked
dialectical trees for arguments for L, as F .L/. Hence, for a literal L, we say it is
warranted if there is at least one argument for that literal in the dialectical forest
F .L/ that is labeled as “U”, not warranted if there is at least one argument for
the literal :L in the dialectical forest F .:L/ that is labeled as “U”, and undecided
otherwise.
With this, we have a complete description of the analytical model, and can go on
to describe the DeLP3E framework.
3.3
The DeLP3E Framework
DeLP3E arises from the combination of the environmental model ˘EM, and the
analytical model ˘AM; the two models are held together by the annotation function.
This allows elements from the AM to be annotated with elements from the EM.
These annotations specify the conditions under which the various statements in the
AM can potentially be true.
Intuitively, given ˘AM, every element of ˝ [  [  [ ˚ might only hold in
certain worlds in the set WEM—that is, they are subject to probabilistic events.
Therefore, we associate elements of ˝ [  [  [ ˚ with a formula from formEM.
2In the sense of providing reasons for and against a position.

30
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
Fig. 3.3 Example annotation function
In doing so, we can in turn compute the probabilities of subsets of ˝ [  [  [ ˚
using the information contained in ˘EM, as we describe shortly. The notion of an
annotation function associates elements of ˝[[ [ ˚ with elements of formEM.
Deﬁnition 3.9 An annotation function is any function af W ˝ [  [  [ ˚ !
formEM. We use Œaf  to denote the set of all annotation functions.
Figure 3.3 shows an example of an annotation function.
We will sometimes denote annotation functions as sets of pairs . f; af. f// in order
to simplify the presentation. Function af may come from an expert’s knowledge or
the data itself. Choosing the correct function and learning the function from data is
the topic of ongoing work.
We also note that, by using the annotation function, we may have certain
statements that appear as both facts and presumptions (likewise for strict and
defeasible rules). However, these constructs would have different annotations, and
thus be applicable in different worlds. We note that the annotation function can allow
AM facts and strict rules to be true in some EM worlds and false in others—this is
why we include facts and strict rules as part of an argument in our framework.
Example 3.7 Suppose we added the following presumptions to the program in our
running example:
3 D l –
4 D m –
and suppose we extend af as follows:
af .3/ D a ^ b
af .4/ D a ^ b ^ c
So, for instance, unlike 1, 3 can potentially be true in any world of the form:
fa; bg

We now have all the components to formally deﬁne DeLP3E programs.

3.3
The DeLP3E Framework
31
Deﬁnition 3.10 Given environmental model ˘EM, analytical model ˘AM, and
annotation function af , a DeLP3E program is of the form I D .˘EM; ˘AM; af /.
We use notation ŒI  to denote the set of all possible programs.
The next step in the deﬁnition of DeLP3E is to explore entailment operations.
In an entailment query, we are given an AM literal L, probability interval p ˙ ,
and DeLP3E program I , and we wish to determine if L is entailed by I with a
probability p ˙ . However, before we can formally deﬁne this entailment problem,
we deﬁne a warranting scenario to determine the proper environment in question
and the entailment bounds (Sect. 3.3.1). This is followed by our formal deﬁnition
and method for computing entailment in Sect. 3.3.2.
3.3.1
Warranting Scenarios
In DeLP3E, we can consider a world-based approach; that is, the defeat relationship
among arguments depends on the current state of the (EM) world.
Deﬁnition 3.11 Let I
D .˘EM; ˘AM; af / be a DeLP3E program, argument
hA ; Li is valid w.r.t. world w 2 WEM iff 8c 2 A ; w ˆ af.c/.
We extend the notion of validity to argumentation lines, dialectical trees, and
dialectical forests in the expected way (for instance, an argumentation line is valid
w.r.t. w iff all arguments that comprise that line are valid w.r.t. w).
Example 3.8 Consider worlds w1; : : : ; w8 from Example 3.3 along with the argu-
ment hA5; ui from Example 3.4. This argument is valid in worlds w1, w2, w3, w4,
w6, and w7.

We also extend the idea of a dialectical tree w.r.t. worlds; so, for a given world
w 2 WEM, the dialectical (resp., marked dialectical) tree induced by w is denoted
with TwhA ; Li (resp., T 
w hA ; Li). We require that all arguments and defeaters in
these trees be valid with respect to w. Likewise, we extend the notion of dialectical
forests in the same manner (denoted with Fw.L/ and F 
w .L/, resp.). Based on these
concepts, we introduce the notion of warranting scenario.
Deﬁnition 3.12 Let I D .˘EM; ˘AM; af / be a DeLP3E program and L be a literal
formed with a ground atom from GAM; a world w 2 WEM is said to be a warranting
scenario for L (denoted w `war L) iff there is a dialectical forest F 
w .L/ in which L
is warranted and F 
w .L/ is valid w.r.t. w.
Note that a world w not being a warranting scenario for L, is not the same as it being
a warranting scenario for :L. For that to be the case, we would need a dialectical
tree F 
w .:L/ in which :L is warranted and F 
w .:L/ is valid w.r.t. w.
Example 3.9 Considering the arguments from Example 3.8, worlds w3, w6, and w7
are warranting scenarios for argument hA5; ui.


32
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
3.3.2
Entailment in DeLP3E
In this section, we use the idea of a warranting scenario to formally deﬁne our
entailment problem. We ﬁrst notice that the set of worlds in the EM where a literal
L in the AM must be true is exactly the set of warranting scenarios—these are the
“necessary” worlds:
nec.L/ D fw 2 WEM j .w `war L/g:
Now, the set of worlds in the EM where AM literal L can be true is the following—
these are the “possible” worlds:
poss.L/ D fw 2 WEM j w 6`war :Lg:
Example 3.10 Following from Example 3.8, we have that:
nec.u/ D fw3; w6; w7g and poss.u/ D fw1; w2; w3; w4; w6; w7g:

Deﬁnition 3.13 We deﬁne for.w/ D V
a2w a^V
a…w :a, which denotes the formula
that has w as its only model. Also, we extend this notation to sets of words: for.W/ D
W
w2W for.w/.
Deﬁnition 3.14 (Entailment) Given DeLP3E program, I
D .˘EM; ˘AM; af /,
AM literal L and probability interval p ˙, we say that I entails L with probability
p ˙  iff all probability distributions Pr that satisfy ˘EM satisfy for.nec.L// W p ˙ 
and for.poss.L// W p ˙ .
We will also refer to the tightest bound Œ p  ; p C  such that I entails L
with a probability p ˙  as the “tightest entailment bounds.” The intuition behind
the above deﬁnition of entailment is as follows. Let ` be the maximum value for
p   and u be the minimum value for p C  before we can no longer say that I
entails L with probability p ˙ . In this case, we can deﬁne probability distributions
Pr 
poss; Pr C
poss; Pr 
nec; Pr C
nec as follows:
•
Pr 
poss satisﬁes ˘EM and assigns the smallest possible probability to worlds that
satisfy for.poss.L//.
•
Pr C
poss satisﬁes ˘EM and assigns the largest possible probability to worlds that
satisfy for.poss.L//.
•
Pr 
nec satisﬁes ˘EM and assigns the smallest possible probability to worlds that
satisfy for.nec.L//.
•
Pr C
nec satisﬁes ˘EM and assigns the largest possible probability to worlds that
satisfy for.nec.L//.
We only need to compare Pr 
poss.poss.L// and Pr 
nec.nec.L// for ﬁnding the lower
bound since Pr C
poss.poss.L//  Pr 
poss.poss.L// and Pr C
nec.nec.L//  Pr 
nec.nec.L//.

3.3
The DeLP3E Framework
33
Similar reasoning holds for the case of ﬁnding the upper bound. Thus, we get the
following relationships:
` D min

Pr 
poss. poss.L//; Pr 
nec.nec.L//

(3.1)
u D max

Pr C
poss. poss.L//; Pr C
nec.nec.L//

(3.2)
However, we note that as nec.L/  poss.L/ we have the following:
` D Pr 
nec

nec.L/

(3.3)
u D Pr C
poss

poss.L/

(3.4)
Note that the values deﬁned in Eqs. (3.3) and (3.4) are equivalent to the belief and
plausibility values deﬁned in Dempster-Shafer theory [18].
Hence, the tightest possible entailment bounds that can be assigned to a literal
can be no less than the lower bound of the probability assigned to the necessary
warranting scenarios and no more than the probability assigned to the possible
warranting scenarios. Hence, we can compute the tightest probability bound such
that L is entailed (denoted PL;Pr;I ) as follows:
`L;Pr;I D
X
w2nec.L/
Pr 
nec.w/;
uL;Pr;I D
X
w2poss.L/
Pr C
poss.w/
`L;Pr;I  PL;Pr;I  uL;Pr;I
Thus, in interval form we have:
PL;Pr;I D

`L;Pr;I C uL;Pr;I  `L;Pr;I
2

˙ uL;Pr;I  `L;Pr;I
2
:
Now let us consider the computation of tightest probability bounds for entailment
on a literal when we are given a knowledge base KEM in the environmental model,
which is speciﬁed in I , instead of a probability distribution over all worlds. For a
given world w 2 WEM, let for.w/ D  V
a2w a^ V
a…w :a—that is, a formula that
is satisﬁed only by world w. Now we can determine the upper and lower bounds on
the probability of a literal w.r.t. KEM (denoted PL;I ) as follows:
`L;I D EP-LP-MIN
0
@KEM;
_
w2nec.L/
for.w/
1
A

34
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
uL;I D EP-LP-MAX
0
@KEM;
_
w2poss.L/
for.w/
1
A
`L;I  PL;I  uL;I
Hence, PL;I D

`L;KEM C uL;I `L;I
2

˙ uL;I `L;I
2
.
Example 3.11 Consider argument hA5; ui from Example 3.8. We can compute Pu;I
(where I D .˘ 0
EM; ˘AM; af //.
Note that for the upper bound, the linear program we need to set up is the one
shown in Example 3.3. For the lower bound, the objective function changes to:
min x3 C x6 C x7. From these linear constraints, we obtain Pu D 0:7 ˙ 0:2.

In the following, we study the problem of consistency in our framework, which
is the basis of the belief revision operators studied in the next chapter.
3.4
Consistency and Inconsistency in DeLP3E Programs
Even though our framework relies heavily on argumentation and reasoning under
uncertainty, inconsistency in our knowledge base can still arise. For instance,
the knowledge encoded in the environmental model could become contradictory,
which would preclude any probability distribution from satisfying that part of the
knowledge base. Even on the argumentation side, despite that fact that argumenta-
tion formalisms in general are inconsistency tolerant, there may be problems with
inconsistency. For example, it would be problematic for DeLP3E if the set of strict
facts and strict rules were contradictory, and the set of contradictory elements all
arise under the same environmental conditions.
In this section, we explore what forms of inconsistency can arise in DeLP3E
programs; this will form the basis for the material on belief revision, which is the
topic of Chap.4. We use the following notion of (classical) consistency of PreDeLP
programs: ˘ is said to be consistent if there does not exist a ground literal a s.t. ˘ `
a and ˘ ` :a. For DeLP3E programs, there are two main kinds of inconsistency
that can be present; the ﬁrst is what we refer to as EM, or Type I, (in)consistency.
Deﬁnition 3.15 An environmental model ˘EM is Type I consistent iff there exists a
probability distribution Pr over the set of worlds WEM that satisﬁes ˘EM.
We illustrate this type of consistency in the following example.
Example 3.12 It is possible to create probabilistic knowledge bases for which there
is no satisfying probability distribution. The following formula is a simple example
of such a case:

3.4
Consistency and Inconsistency in DeLP3E Programs
35
rain _ hail W 0:4 ˙ 0I
rain ^ hail W 0:4 ˙ 0:2:
The above is an example of Type I inconsistency in DeLP3E, as it arises from the
fact that there is no satisfying interpretation for the EM knowledge base.

However, even if the EM is consistent, the interaction between the annotation
function and facts and strict rules can still cause another type of inconsistency to
arise. We will refer to this as combined, or Type II, (in)consistency.
Deﬁnition 3.16 A DeLP3E program I
D
.˘EM; ˘AM; af /, with ˘AM D
h; ˝; ˚; i, is Type II consistent iff: given any probability distribution Pr that
satisﬁes ˘EM, if there exists a world w 2 WEM such that S
x2[˝ j wˆaf.x/fxg is
inconsistent, then we have Pr.w/ D 0.
Thus, any EM world in which the set of associated facts and strict rules are
inconsistent (we refer to this as “classical consistency”) must always be assigned
a zero probability. The intuition is as follows: any subset of facts and strict rules are
thought to be true under certain circumstances—these circumstances are determined
through the annotation function and can be expressed as sets of EM worlds. Suppose
there is a world where two contradictory facts can both be considered to be true
(based on the annotation function). If this occurs, then there must not exist a
probability distribution that satisﬁes the program ˘EM that assigns such a world
a non-zero probability, as this world leads to an inconsistency. We provide a more
concrete example of Type II inconsistency next.
Example 3.13 Consider the environmental model from Example 3.2 (Page 22), the
analytical model shown in Fig. 3.1 (Page 26), and the annotation function shown in
Fig. 3.3 (Page 30). Suppose the following fact is added to the argumentation model:
3 D :p;
and that the annotation function is expanded as follows:
af .3/ D k ^ :f
Clearly, fact 3 is in direct conﬂict with fact 1a. However, this does not necessarily
mean that there is an inconsistency. For instance, by the annotation function, 1a
holds in the world fk; fg while 3 does not. However, let’s consider following
world w D fkg. Note that w ˆ af .3/ and w ˆ af .2/. Hence, in this world
both contradictory facts can occur. However, can this world be assigned a non-zero
probability? A simple examination of the environmental model indicates that it can.
Hence, in this case, we have Type II inconsistency.

We say that a DeLP3E program is consistent iff it is both Type I and Type II
consistent. However, in this chapter, we focus on Type II consistency and assume
that the program is Type I consistent.

36
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
Fig. 3.4 A straightforward BFS-based algorithm for consistency checking
Figure 3.4 gives a straightforward approach to identifying Type II inconsistent
DeLP3E programs by running breath-ﬁrst search on a set of  [ ˝. The algorithm
works by examining all subsets of a set of facts and strict rules to ﬁnd inconsistent
subsets whose corresponding formula in the environmental model can be assigned
a non-zero probability. The following result states its correctness.
Proposition 3.1 For Type I consistent DeLP3E program I D .˘EM; ˘AM; af /
where  and ˝ are the sets of facts and strict rules in ˘AM, then
CON-CHK-BFS(˘EM; ˘AM; af ; d; f [ ˝g) (where d
D
j [ ˝j) returns
INCONSISTENT iff the DeLP3E is Type II inconsistent.
However, we note that even with an oracle for checking the classical consistency
of a subset (line 2) and for determining the upper bound on the probability of the
annotations (line 2a), this algorithm is still intractable as it explores all subsets of
 [ ˝. One possible way to attack this intractability is to restrict the depth of the
search by setting d to be less than the size of [˝. In this case, we get the following
result:
Proposition 3.2 Given Type I consistent DeLP3E program I D .˘EM; ˘AM; af /,
where  and ˝ are the sets of facts and strict rules in ˘AM and d < j [ ˝j,
then if CON-CHK-BFS(˘EM; ˘AM; af ; d; f [ ˝g) returns INCONSISTENT, the
program I is Type II inconsistent.
Therefore, by restricting depth, we can view this algorithm as an “anytime”
approach, essentially searching for a world leading to an inconsistent program and
not halting until it does.
In the next chapter, we will explore three main methods for resolving Type II
inconsistencies by applying belief revision operators.
3.5
Case Study: An Application in Cybersecurity
In this section we develop a complete example of how the DeLP3E framework can
be used to deal with a cyber attribution problem. In this scenario, a cyberattack has
been detected and we want to determine who is responsible for it.

3.5
Case Study: An Application in Cybersecurity
37
3.5.1
Model for the Attribution Problem
To specify the model we need to specify the environmental model, the analytical
model, and the annotation function. First we identify two special subsets of the set
of constants (C) for this application: Cact and Cops, which specify the actors that
could conduct cyberoperations and the operations themselves, respectively:
Cact D fbaja; krasnovia; mojaveg
Cops D fworm123g
That is, the possible actors are the states of baja, krasnovia and mojave, and the only
operation that we consider they can conduct is a worm123 attack.
Next, we need to specify the sets of predicates, PEM, the predicates for the
environmental model, and PAM, the predicates for the analytical model. These are
given in Fig. 3.5, which presents all the predicates with variables. The following are
examples of ground atoms over those predicates; again, we distinguish between the
subset of ground atoms from the environmental model GEM and the ground atoms
from the analytical model GAM:
Fig. 3.5 Predicate deﬁnitions for the environment and analytical models in the cyber attribution
example

38
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
GEM W origIP.mw123sam1; krasnovia/; mwHint.mw123sam1; krasnovia/;
inLgConf.krasnovia; baja/; mseTT.krasnovia; 2/
GAM W evidOf.mojave; worm123/; motiv.baja; krasnovia/; expCw.baja/;
tgt.krasnovia; worm123/
PAM and the set of constants provides all the information we need for the
analytical model. However, there is more to the environmental model than just PEM
and the constants. We need to specify the probabilities of formulas. This information
is given by the following set of probabilistic formulas KEM:
f1 D govCybLab.baja/ W 0:7 ˙ 0:2
f2 D cybCapAge.baja; 5/ W 0:3 ˙ 0:1
f3 D mseTT.baja; 2/ W 0:8 ˙ 0:2
f4 D mwHint.mw123sam1; mojave/ ^ compilLang.worm123; english/ W 0:8 ˙ 0:1
f5 D malwInOp.mw123sam1; worm123/
^ malwareRel.mw123sam1; mw123sam2/
^ mwHint.mw123sam2; mojave/ W 0:5 ˙ 0:2
f6 D inLgConf.baja; krasnovia/ _ :cooper.baja; krasnovia/ W 0:9 ˙ 0:1
f7 D origIP.mw123sam1; baja/ W 1 ˙ 0
Given this probabilistic information, we can demonstrate the linear programming
approach to the maximum entailment problem deﬁned in Deﬁnition 3.2. Consider
knowledge base K 0
EM and a set of ground atoms restricted to those that appear in
that program. Hence, we have the following worlds:
w1 D fgovCybLab.baja/; cybCapAge.baja; 5/; mseTT.baja; 2/g
w2 D fgovCybLab.baja/; cybCapAge.baja; 5/g
w3 D fgovCybLab.baja/; mseTT.baja; 2/g
w4 D fcybCapAge.baja; 5/; mseTT.baja; 2/g
w5 D fcybCapAge.baja; 5/g
w6 D fgovCybLab.baja/g
w7 D fmseTT.baja; 2/g
w8 D ;

3.5
Case Study: An Application in Cybersecurity
39
and suppose we wish to compute the probability for formula:
q D govCybLab.baja/ _ mseTT.baja; 2/
For each formula in KEM we have a constraint, and for each world above we have a
variable. An objective function is created based on the worlds that satisfy the query
formula (in this case, worlds w1; w2; w3; w4; w6; w7). Hence, EP-LP-MIN.K 0
EM; q/
can be written as follows:
max
x1 C x2 C x3 C x4 C x6 C x7
w:r:t: W
0:6 
x1 C x2 C x3 C x6
 0:8
0:2 
x1 C x2 C x4 C x5
 0:4
0:8 
x1 C x3 C x4 C x7
 1
x1 C x2 C x3 C x4 C x5 C x6 C x7 C x8 D 1
From this, we can solve EP-LP-MAX.K 0
EM; q/ and, after an easy modiﬁcation,
EP-LP-MIN.K 0
EM; q/, and obtain the solution 0:9 ˙ 0:1.
Now, given PAM and C, we can assemble the ground argumentation framework
of Fig. 3.6 as a sample ˘AM. From this argumentation framework, we can build the
following arguments:
hA1; condOp.baja; worm123/i
A1 D f1a; ı1ag
hA2; condOp.baja; worm123/i
A2 D f1; 2; ı4; !2a; 1a; 2g
hA3; condOp.baja; worm123/i
A3 D f1; ı2; ı4g
hA4; condOp.baja; worm123/i
A4 D f2; ı3; 2g
hA5; isCap.baja; worm123/i
A5 D f1; ı4g
hA6; :condOp.baja; worm123/i A6 D fı1b; 1b; !1ag
hA7; :isCap.baja; worm123/i
A7 D f3; ı5ag
Note that:
hA5; isCap.baja; worm123/i
is a sub-argument of both
hA2; condOp.baja; worm123/i
and
hA3; condOp.baja; worm123/i

40
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
Fig. 3.6 A ground argumentation framework
The following are some of the attack relationships between these arguments: A1,
A2, A3, and A4 all attack A6; A5 attacks A7; and A7 attacks A2.
In Fig. 3.7 we show an another example of a knowledge base for the attribution
problem, this time with a non-ground argumentation system.
With the environmental and analytical models speciﬁed, the remaining com-
ponent of the model is the annotation function; one suitable annotation func-
tion is given in Fig. 3.8. Consider worlds w1; : : : ; w8 along with the argument
hA5; isCap.baja; worm123/i. This argument is valid in worlds w1, w2, w3, w4, w6,
and w7. Similarly, worlds w3, w6, and w7 are warranting scenarios for argument
hA5; isCap.baja; worm123/i and
nec.isCap.baja; worm123// D fw3; w6; w7g
while
poss.isCap.baja; worm123// D fw1; w2; w3; w4; w6; w7g

3.5
Case Study: An Application in Cybersecurity
41
Fig. 3.7 A non-ground argumentation framework
Fig. 3.8 Example annotation function
3.5.2
Applying Entailment to the Cyber Attribution Problem
We now discuss how ﬁnding tight bounds on the entailment probability can be
applied to the cyber attribution problem. Following the domain-speciﬁc notation
introduced in the beginning of this case study (where the set of constants C
includes two subsets: Cact and Cops, that specify the actors that could conduct
cyberoperations and the operations themselves, respectively), we deﬁne a special
case of the entailment problem as follows.
Deﬁnition 3.17 Let I D .˘EM; ˘AM; af / be a DeLP3E program, S  Cact (the
set of “suspects”), O 2 Cops (the “operation”), e  GEM (the “evidence”), and
D  GEM (the “probabilistic fact”).

42
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
Fig. 3.9 A straightforward algorithm for ﬁnding a solution to an attribution query
An actor A 2 S is said to be a most probable suspect iff there does not exist
A0 2 S such that the midpoint of PcondOp.A0;O/;I 0 is greater than the midpoint of
PcondOp.A;O/;I 0, where I 0 D .˘EM[˘e[˘D; ˘AM; af 0/ with ˘e D S
c2efc W 1˙0g
and ˘D D S
c2Dfc W p ˙ g.
Note that we use midpoints of PcondOp.A0;O/;I 0 and PcondOp.A;O/;I 0 to compare
intervals; alternative formulations are possible based on the upper or lower bounds
of these intervals.
Given the above deﬁnition, we refer to Q D .I ; S ; O; e/ as an attribution query,
and A as an answer to Q. We note that in the above deﬁnition, the items of evidence
are added to the environmental model with a probability of 1. While in general this
may be the case, there are often instances in analysis of a cyberoperation where
the evidence may be true with some degree of uncertainty; for this reason, we are
allowing probabilistic facts in the deﬁnition.
To understand how uncertain evidence can be present in a cybersecurity setting,
consider the following scenario:
In Symantec’s initial analysis of the Stuxnet worm, analysts found the routine designed
to attack the S7-417 logic controller was incomplete, and hence would not function [5].
However, industrial control system expert Ralph Langner claimed that the incomplete code
would run provided a missing data block is generated, which he thought was possible [11].
In this case, though the code was incomplete, uncertainty was clearly present regarding its
usability.3
This situation provides a real-world example of the need to compare arguments—in
this case, in the worlds where both arguments are valid, Langner’s argument would
likely defeat Symantec’s by generalized speciﬁcity (the outcome, of course, will
depend on the exact formalization of the two).
In Fig. 3.9 we give a simple, straightforward algorithm for attribution queries.
The correctness of this algorithm clearly follows from the deﬁnitions above. We note
3Langner was later vindicated by the discovery of an older sample, Stuxnet 0.5, which generated
the data block [3].

3.6
Conclusions
43
that a key source of computational complexity lies in step 2, where all arguments
supporting the hypothesis that each actor conducted the operation are computed for
each world in the EM; this leads to a factor of 2jGEMj (exponential in the number
of ground atoms in the environmental model). However, we also note that this is
equal to the time complexity required to write out a linear program for answering
the entailment query.
Note that the exact approaches presented thus far for answering attribution
queries experience exponential running times in the worst case. Hence, for the
creation of a real-world system, we consider several practical approaches that can
be taken to answer attribution queries Q D .i; S ; O; e/. We are currently exploring
several of these ideas as we work to build a system for cyber attribution based on
DeLP3E:
1. Approximating the warranting formula: Instead of inspecting all possible classi-
cal dialectical trees as in Approach 1, either a subset of trees can be computed
according to a given heuristic or an anytime approach can be adopted to select
such a subset F 0. The computations with respect to F0 will then yield sound
approximations relative to the full forest F, which means that all probability
intervals will be supersets of the exact intervals.
2. Approximating the probability: Another alternative to Approach 1 is to apply
approximation algorithms to the formula; for instance:
a. Approximate satisﬁability: if the formula is unsatisﬁable, then the warranting
probability is zero;
b. A lower bound on the warranting probability can be obtained from a subset of
possible worlds (k most probable worlds, random sample of worlds, etc.).
3. “What-if” Reasoning: Given a set Wint of worlds of interest and a warranting
formula  (computed using any of the above approaches), each world can be
checked to see which literals condOp.Ai; O/, with Ai 2 S , are warranted.
That is, instead of computing probability of attribution, the attribution literal is
analyzed in each world of interest.
3.6
Conclusions
In this chapter we introduced the DeLP3E framework, consisting of an environmen-
tal model, an analytical model, and an annotation function that relates the two [20].
DeLP3E is an extension of the PreDeLP language in which sentences can be anno-
tated with probabilistic events. Such events are connected to a probabilistic model,
allowing a clear separation of interests between certain and uncertain knowledge
while allowing uncertainty to be captured and incorporated into reasoning. After
presenting the language, we discussed the types of inconsistencies that can arise
in DeLP3E programs, setting up the topic of belief revision, which is discussed
in detail in the next chapter. Finally, we presented an extended case study of the
application of DeLP3E to the attribution problem.

44
3
Argumentation-Based Cyber Attribution: The DeLP3E Model
This model is of interest to both the argumentation literature, in showing
how argumentation can be applied to a complex real-world problem, and to the
cybersecurity literature, suggesting tools that can be used to address this problem.
As part of the case study we considered a special kind of query, called an attribution
query, that is useful in tackling the problem of attributing responsibility to entities
given a cyberevent of interest.
References
1. C. E. Alchourrón, P. Gärdenfors, and D. Makinson. On the logic of theory change: Partial meet
contraction and revision functions. J. Sym. Log., 50(2):510–530, 1985.
2. C. Altheide. Digital Forensics with Open Source Tools. Syngress, 2011.
3. S. Corp. Stuxnet 0.5: Disrupting Uranium Processing at Natanz. Symantec Connect, Feb. 2013.
4. P. M. Dung. On the acceptability of arguments and its fundamental role in nonmonotonic
reasoning, logic programming and n-person games. Artiﬁcial Intelligence, 77:pp. 321–357,
1995.
5. N. Falliere, L. O. Murchu, and E. Chien.
W32.Stuxnet Dossier Version 1.4.
Symantec
Corporation, Feb. 2011.
6. A. J. García and G. R. Simari. Defeasible logic programming: An argumentative approach.
Theory and Practice of Logic Programming, 4(1-2):95–138, 2004.
7. P. Gardenfors. Knowledge in ﬂux: modeling the dynamics of epistemic states. MIT Press,
Cambridge, Mass., 1988.
8. P. Gärdenfors. Belief revision, volume 29. Cambridge University Press, 2003.
9. R. J. Heuer. Psychology of Intelligence Analysis. Center for the Study of Intelligence, 1999.
10. S. Khuller, M. V. Martinez, D. S. Nau, A. Sliva, G. I. Simari, and V. S. Subrahmanian.
Computing most probable worlds of action probabilistic logic programs: scalable estimation
for 1030,000 worlds.
Annals of Mathematics and Artiﬁcial Intelligence, 51(2-4):295–331,
2007.
11. R. Langner.
Matching Langner Stuxnet analysis and Symantic dossier update.
Langner
Communications GmbH, Feb. 2011.
12. J. W. Lloyd. Foundations of Logic Programming, 2nd Edition. Springer, 1987.
13. M. V. Martinez, A. J. García, and G. R. Simari. On the use of presumptions in structured
defeasible reasoning.
In Proceedings of the International Conference on Computational
Models of Argument (COMMA), pages 185–196, 2012.
14. N. J. Nilsson. Probabilistic logic. Artiﬁcial Intelligence, 28(1):71–87, 1986.
15. I. Rahwan and G. R. Simari. Argumentation in Artiﬁcial Intelligence. Springer, 2009.
16. L. Riley, K. Atkinson, T. R. Payne, and E. Black. An implemented dialogue system for inquiry
and persuasion. In Proceedings of the International Workshop on Theory and Applications of
Formal Argumentation (TAFA), pages 67–84. Springer, 2011.
17. Shadows in the Cloud: Investigating Cyber Espionage 2.0.
Technical report, Information
Warfare Monitor and Shadowserver Foundation, April 2010.
18. G. Shafer et al. A mathematical theory of evidence, volume 1. Princeton university press
Princeton, 1976.
19. P. Shakarian, J. Shakarian, and A. Ruef. Introduction to Cyber-Warfare: A Multidisciplinary
Approach. Syngress, 2013.
20. P. Shakarian, G. I. Simari, G. Moores, D. Paulo, S. Parsons, M. A. Falappa, and A. Aleali.
Belief revision in structured probabilistic argumentation. Annals of Mathematics and Artiﬁcial
Intelligence, 78(3-4):259–301, 2016.

References
45
21. G. R. Simari and R. P. Loui.
A mathematical treatment of defeasible reasoning and its
implementation. Artiﬁcial Intelligence, 53(2-3):125–157, 1992.
22. G. I. Simari, M. V. Martinez, A. Sliva, and V. S. Subrahmanian.
Focused most probable
world computations in probabilistic logic programs.
Annals of Mathematics and Artiﬁcial
Intelligence, 64(2-3):113–143, 2012.
23. L. Spitzner. Honeypots: Catching the Insider Threat. In Proceedings of the Computer Security
Applications Conference, pages 170–179. IEEE Computer Society, 2003.
24. F. Stolzenburg, A. García, C. I. Chesñevar, and G. R. Simari.
Computing Generalized
Speciﬁcity. Journal of Non-Classical Logics, 13(1):87–113, 2003.
25. O. Thonnard, W. Mees, and M. Dacier.
On a multicriteria clustering approach for attack
attribution. SIGKDD Explorations, 12(1):11–20, 2010.

Chapter 4
Belief Revision in DeLP3E
4.1
Introduction
Many real-world knowledge-based systems must deal with information coming
from different sources that invariably leads to uncertain content, be it from gaps
in knowledge (incompleteness), over speciﬁcation (inconsistency), or because the
knowledge is inherently uncertain (such as weather forecasts or measurements
that are necessarily imprecise). Far from considering such uncertain knowledge
useless, knowledge engineers face the challenge of putting it to its best possible
use when solving a wide range of problems. In particular, one basic problem
that needs to be investigated in depth is that of revising such knowledge bases
in a principled manner. In this chapter, we tackle the problem of carrying out
belief revision operations in the DeLP3E model introduced in Chap.3. We begin
with the proposal of two sets of rationality postulates characterizing how such
operations should behave: one for the analytical model and one for the annotation
function (as we show, revising the environmental model is not sufﬁcient to restore
consistency). These postulates are based on the classical approach proposed in [7]
for non-prioritized belief revision in classical knowledge bases. We then study two
classes of operators and their theoretical relationships with the proposed postulates,
concluding with representation theorems for each class. Then, we propose a subclass
of the annotation function-based operators called QAFO that takes quantitative
aspects into account when performing revisions, such as how the probabilities of
certain literals or formulas of interest change after the revision takes place.
4.2
Basic Belief Revision
We now explore three methods for resolving inconsistencies of Type II (cf.
Sect. 3.4); they can be brieﬂy summarized as follows:
© The Author(s) 2018
E. Nunes et al., Artiﬁcial Intelligence Tools for Cyber Attribution, SpringerBriefs in
Computer Science, https://doi.org/10.1007/978-3-319-73788-1_4
47

48
4
Belief Revision in DeLP3E
Revise the EM.
The probabilistic model can be changed in order to force the
worlds that induce contradicting strict knowledge to have probability zero. In
general, this type of revision by itself is not ideal as it will not work in all cases.
We discuss this method in Sect. 4.2.1.
Revise the AM.
The argumentation model can be changed in such a way that
the set of strict rules and facts is consistent. If this is the case, then Type II
consistency follows. We discuss this method in Sect. 4.2.2.
Revise the annotation function.
The annotations involved in the inconsistency can
be changed so that the conﬂicting information in the AM does not become
induced under any possible world. This can be viewed as a generalization of
AM revision. We discuss this method in Sect. 4.2.3.
4.2.1
EM-Based Belief Revision
We now study belief revision through updating the environmental model only
(˘EM). Suppose that ˘EM is consistent, but that the overall program is Type II
inconsistent. Then, there must exist a set of worlds in the EM such that there exists a
probability distribution that assigns each of them a non-zero probability. This gives
rise to the following result.
Proposition 4.1 If there exists a probability distribution Pr that satisﬁes ˘EM
s.t. there exists a world w 2 WEM where Pr.w/ > 0 and S
x2[˝ j wˆaf.x/fxg is
inconsistent (Type II inconsistency), then any change made in order to resolve this
inconsistency by modifying only ˘EM yields a new EM ˘ 0
EM such that
 V
a2w a ^
V
a…w :a

W 0 ˙ 0 is entailed by ˘ 0
EM.
Proposition 4.1 seems to imply an easy strategy to resolve Type II inconsisten-
cies: add formulas to ˘EM forcing the necessary worlds to have a zero probability.
However, this may lead to Type I inconsistencies in the resulting model ˘ 0
EM. If we
are applying an EM-only strategy to resolve inconsistencies, this would then lead to
further adjustments to ˘ 0
EM in order to restore Type I consistency. We illustrate this
situation in the following example.
Example 4.1 Consider two contradictory facts in an AM: a and :a such that
af .a/ D p and af .:a/ D q. Suppose that p and q are the only atoms in the EM, and
that we have:
p W 0:4 ˙ 0
q W 0:8 ˙ 0:1
:p ^ :q W 0:2 ˙ 0:1
which is consistent since the following distribution satisﬁes all constraints:
Pr.f pg/ D 0:2;
Pr.f p; qg/ D 0:2;
Pr.fqg/ D 0:5;
Pr.fg/ D 0:1.

4.2
Basic Belief Revision
49
Now, to restore Type II consistency of our simple DeLP3E program, we can add
formula p ^ q W 0 ˙ 0 to the EM so that world f p; qg is forced to have probability
zero. However, this leads to another inconsistency, this time of Type I, since putting
together all the constraints we have:
Pr.f p; qg/ D 0;
Pr.f pg/ C Pr.f p; qg/ D 0:4;
Pr.fqg/ C Pr.f p; qg/ D 0:8 ˙ 0:1;
Pr.fg/ D 0:2 ˙ 0:1;
Pr.f pg/ C Pr.f p; qg/ C Pr.fqg/ C Pr.fg/ D 1;
which is clearly inconsistent. Repairing this inconsistency involves changing the EM
further, for instance by relaxing the bounds in the ﬁrst two formulas to accommodate
the probability mass that world f p; qg had before and can no longer hold.

In the previous example, we saw how changes made to repair Type II inconsis-
tencies could lead to Type I inconsistencies. It is also possible that changing ˘ 0
EM
(for instance, by removing elements, relaxing probability bounds of the sentences,
etc.) causes Type II inconsistency in the overall DeLP3E program—this would lead
to the need to set more EM worlds to a probability of zero. Unfortunately, this
process is not guaranteed to arrive at a fully consistent program before being unable
to continue; consider the following example, where the process cannot even begin.
Example 4.2 Consider an AM composed of several contradictory facts, an EM with
just two atoms (as in the previous example), and the following annotation function:
af .a/ D p
af .b/ D :p
af .c/ D :p
af .d/ D q
af .:a/ D q
af .:b/ D :q
af .:c/ D p
af .:d/ D :q
Modifying the EM so that no two contradictory literals ever hold at once in a world
that has a non-zero probability leads to the constraints:
Pr.f p; qg/ D 0;
Pr.f pg/ D 0;
Pr.fqg/ D 0;
Pr.fg/ D 0;
Pr.f pg/ C Pr.f p; qg/ C Pr.fqg/ C Pr.fg/ D 1;
As in the previous example, the probability mass cannot be accommodated within
these constraints. It would thus be impossible to restore consistency by only
modifying ˘EM.

We thus arrive at the following observation from Example 4.2:
Observation 1 Given a Type II inconsistent DeLP3E program, consistency cannot
always be restored via modiﬁcations to ˘EM alone.
Therefore, due to this line of reasoning, in this chapter we focus our efforts
on modiﬁcations to the other two components of a DeLP3E framework: the AM
and the annotation function, as described in the next two sections. Approaches
combining two or more of these methods are the topic of future work.

50
4
Belief Revision in DeLP3E
4.2.2
AM-Based Belief Revision
The result of the previous section indicates that EM-based belief revision of a
DeLP3E framework (at least by itself) is not a tenable solution. Hence, in this
section, we resort to an alternate approach in which we only modify the AM (˘AM).
In this section (and the next), given a DeLP3E program I D .˘EM; ˘AM; af /, with
˘AM D ˝ [  [  [ ˚, we are interested in solving the problem of incorporating
an epistemic input . f; af 0/ into I , where f is either an atom or a rule and af 0 is
equivalent to af , except for its expansion to include f. For ease of presentation, we
assume that f is to be incorporated as a fact or strict rule, as incorporating defeasible
knowledge can never lead to inconsistency since any contradicting presumption can
be defeated by each other, and hence presumptions can rule out each other. As we
are only conducting ˘AM revisions, for I D .˘EM; ˘AM; af / and input . f; af 0/ we
denote the revision as follows: I 	 . f; af 0/ D .˘EM; ˘ 0
AM; af 0/ where ˘ 0
AM is the
revised argumentation model.
We also slightly abuse notation for the sake of presentation, as well as introduce
notation to convert sets of worlds to/from formulas:
•
I [ . f; af 0/ to denote I 0 D .˘EM; ˘AM [ f fg; af 0/.
•
. f; af 0/ 2 I D .˘AM; ˘EM; af / to denote f 2 ˘AM and af D af 0.
•
W 0
EM.I / D fw 2 WEM j ˘ I
AM.w/ is inconsistentg
•
W I
EM.I / D fw 2 W 0
EM j 9Pr s.t. Pr ˆ ˘EM ^ Pr.w/ > 0g
Intuitively, the set W 0
EM.I / contains all the EM worlds for a given program
where the corresponding knowledge base in the AM is classically inconsistent and
W I
EM.I / is a subset of these that can be assigned a non-zero probability—the latter
are the worlds where inconsistency in the AM can arise.
4.2.2.1
Postulates for AM-Based Belief Revision
We now analyze the rationality postulates for non-prioritized revision of belief bases
ﬁrst introduced in [7] and generalized in [5], in the context of AM-based belief
revision of DeLP3E programs.
AM Inclusion For I 	 . f; af 0/ D .˘EM; ˘ 0
AM; af 0/, ˘ 0
AM  ˘AM [ f fg.
This postulate states that the revised AM knowledge base is a subset of the union of
the original AM knowledge base and the input.
AM Vacuity If I [ . f; af 0/ is consistent, then I 	 . f; af 0/  I [ . f; af 0/
If simply adding the input does not cause inconsistency, then the revision operator
does precisely that.
AM Consistency Preservation If I is consistent, then I 	 . f; af 0/ must also be
consistent.
The operator maintains a consistent program.

4.2
Basic Belief Revision
51
AM Weak Success If I [ . f; af 0/ is consistent, then . f; af 0/ 2 I 	 . f; af 0/.
Whenever the simple addition of the input does not cause inconsistencies to arise,
the result will contain the input.
If a portion of the AM knowledge base is removed by the operator, then there exists
a subset of the remaining knowledge base that is not consistent with the removed
element and f.
AM Pertinence For I 	 . f; af 0/ D .˘EM; ˘ 0
AM; af 0/, where ˘ 0
AM D 0 [ ˝0 [
˚0 [ 0, for each g 2  [ ˝ n ˘ 0
AM there exists Yg 
 0 [ ˝0 [ f fg s.t. Yg is
consistent and Yg [ fgg is inconsistent.
If a portion of the AM knowledge base is removed by the operator, then there exists
a superset of the remaining knowledge base that is not consistent with the removed
element and f.
AM Uniformity 1 Let . f; af 0
1/; .g; af 0
2/ be two inputs where W I
EM.I [. f; af 0
1// D
W I
EM.I [ .g; af 0
2//; for all X   [ ˝; if X [ f fg is inconsistent iff X [ fgg is
inconsistent, then:
0
1 [ ˝0
1 n f fg D 0
2 [ ˝0
2 n fgg
where I 	 . f; af 0
1/ D .˘EM; ˘AM0
1; af 0
1/ and I 	 .g; af 0
2/ D .˘EM; ˘AM0
2; af 0
2/ and
˘AM0
i D 0
i [ ˝0
i [ ˚0
i [ 0
i.
If two inputs result in the same set of EM worlds leading to inconsistencies in an
AM knowledge base, and the consistency between analogous subsets (when joined
with the respective input) are the same, then the remaining elements in the AM
knowledge base are the same.
AM Uniformity 2 Let . f; af 0
1/; .g; af 0
2/ be two inputs where W I
EM.I [. f; af 0
1// D
W I
EM.I [ .g; af 0
2//; for all X   [ ˝; if X [ f fg is inconsistent iff X [ fgg is
inconsistent, then:
. [ ˝/ n .0
1 [ ˝0
1/ D . [ ˝/ n .0
2 [ ˝0
2/
where I 	 . f; af 0
1/ D .˘EM; ˘AM0
1; af 0
1/ and I 	 .g; af 0
2/ D .˘EM; ˘AM0
2; af 0
2/ and
˘AM0
i D 0
i [ ˝0
i [ ˚0
i [ 0
i.
If two inputs result in the same set of EM worlds leading to inconsistencies in an AM
knowledge base, and the consistency between analogous subsets (when joined with
the respective input) are the same, then the removed elements in the AM knowledge
base are the same.
We can show an equivalence between the Uniformity postulates under certain
conditions.
Proposition 4.2 For operator 	 where for program I 	. f; af 0/ D .˘EM; ˘ 0
AM; af 0/
and ˘ 0
AM  ˘AM [ f fg, we have that 	 satisﬁes AM Uniformity 1 iff it also satisﬁes
AM Uniformity 2.

52
4
Belief Revision in DeLP3E
4.2.2.2
AM-Based Revision Operators
In this section, we deﬁne a class of operators that satisﬁes all of the AM rationality
postulates of the previous section. We also show that there are no operators outside
this class that satisfy all of the postulates.
First, we introduce notation CandPgmAM.I /, which denotes a set of maximal
consistent subsets of ˘AM. So, if I is consistent, then CandPgmAM.I / D f˘AMg.
CandPgmAM.I / D f˘ 0
AM j ˘ 0
AM   [ ˝ s.t. ˘ 0
AM is consistent and
À˘ 00
AM   [ ˝ s.t. ˘ 00
AM  ˘ 0
AM s.t. ˘ 00
AM is consistentg
For our ﬁrst result, we show that an operator returning any subset of an element
of CandPgmAM.I / is a necessary and sufﬁcient condition for satisfying both the
Inclusion and Consistency Preservation postulates.
Lemma 4.1 Given program I and input . f; af 0/, operator 	 satisﬁes Inclusion
and Consistency Preservation iff for I 	 . f; af 0/ D .˘EM; ˘ 0
AM; af 0/, there exists
an element X 2 CandPgmAM.I [ . f; af 0// s.t. . [ ˝ [ f fg/ \ ˘ 0
AM  X.
Our next result extends Lemma 4.1 by showing that elements of ˘AM [ f fg that
are retained are also elements of CandPgmAM.I [ . f; af 0// if and only if the oper-
ator satisﬁes Inclusion, Consistency Preservation, and Pertinence (simultaneously).
Lemma 4.2 Given program I and input . f; af 0/, operator 	 satisﬁes Inclusion,
Consistency Preservation, and Pertinence iff for I 	 . f; af 0/ D .˘EM; ˘ 0
AM; af 0/,
we have . [ ˝ [ f fg/ \ ˘ 0
AM 2 CandPgmAM.I [ . f; af 0//.
To support the satisfaction of the ﬁrst Uniformity postulate, we provide the
following lemma that shows for a consistent program where two inputs cause
inconsistencies to arise in the same way, that the set of candidate replacement
programs (minus the added AM formula) is the same.
Lemma 4.3 Let I D .˘EM; ˘AM; af / be a consistent program, . f1; af 0
1/, . f2; af 0
2/
be two inputs, and Ii D .˘EM; ˘AM [ f fig; af 0
i/. If W I
EM.I1/ D W I
EM.I2/, then for
all X   [ ˝ we have that:
1. If X [ f f1g is inconsistent , X [ f f2g is inconsistent, then:
fX n f f1g j X 2 CandPgmAM.I1/g D fX n f f2g j X 2 CandPgmAM.I2/g.
2. If fX n f f1g j X 2 CandPgmAM.I1/g D fX n f f2g j X 2 CandPgmAM.I2/g then
X [ f f1g is inconsistent , X [ f f2g is inconsistent.
We now deﬁne the class of AM-based Operators, denoted AMO. Essentially, this
operator selects one of the candidate programs in a deterministic fashion.
Deﬁnition 4.1 (AM-Based Operators) A belief revision operator 	 is an “AM-
based” operator (	 2 AMO) iff given program I D .˘EM; ˘AM; af / and input
. f; af 0/, the revision is deﬁned as I 	 . f; af 0/ D .˘EM; ˘ 0
AM; af 0/, where ˘ 0
AM 2
CandPgmAM.I [ . f; af 0//.

4.2
Basic Belief Revision
53
Finally, we are able to prove our representation theorem for AM-based belief
revision. This theorem follows directly from the results presented in this section.
Theorem 4.1 (AM Representation Theorem) An operator 	 belongs to class
AMO iff it satisﬁes Inclusion, Vacuity, Consistency Preservation, Weak Success,
Pertinence, and Uniformity 1.
Example 4.3 Recall the AM knowledgebase from Fig. 3.1 and KEM deﬁned in
Example 3.2, and suppose we would like to add 3a D l and 3b D :l to the AM.
Let af .3a/
D
a and af .3a/
D
b; the input is then of the form
. f; af 0/ D .f3a; 3bg; af 0/, where af 0 is the new annotation function. The program
I [ . f; af 0/ D .˘EM; ˘AM [ f fg; af 0/ will be inconsistent because of f8. The
AM-based belief revision I 	 . f; af 0/ has the option of removing 3a or 3b to
recover consistency.

4.2.3
Annotation Function-Based Belief Revision
In this section we attack the belief revision problem from a different angle: adjusting
the annotation function. The advantage to changing the annotation function is that
we might not need to discard an entire fact or strict rule from the argumentation
model. Consider the following example.
Example 4.4 Let us consider two contradictory facts in an AM: a and :a such that
af .a/ D q ^r and af .:a/ D r ^s. If we assume that q; r; s are the only atoms in the
EM, then we know that a occurs under the environmental worlds fq; rg and fq; r; sg,
and that :a occurs under the environmental worlds fr; sg fq; r; sg.
Clearly, they cannot both be true in world fq; r; sg. Hence, a new annotation
formula af 0 where af 0.a/ D q^r and af 0.:a/ D r ^s^:for.fq; r; sg/ easily solves
the conﬂict (note that for.w/ speciﬁes a formula satisﬁed by exactly world w). Note
that we did not have to remove :a from the knowledge base, which means that
this information is not completely lost. In other word, the main difference between
the AM-based belief revision and adjusting the Annotation function is that the later
model allows more delicate changes to be made in order to preserve the information
gathered in AM.

We also note that modiﬁcations of the annotation function can be viewed as a
generalization of AM modiﬁcation. Consider the following:
Example 4.5 Consider again the present facts a and :a in the AM. Assuming that
this causes an inconsistency (that is, there is at least one world in which they both
hold), one way to resolve it would be to remove one of these two literals. Suppose
:a is removed; this would be equivalent to setting af.:a/ D ? (where ? represents
a contradiction in the language of the EM).

In this section, we introduce a set of postulates for reasoning about annotation
function-based belief revision. As in the previous section, we then go on to provide

54
4
Belief Revision in DeLP3E
a class of operators that satisfy all the postulates and show that this class includes
all operators satisfying the postulates.
As in this section we are only conducting annotation function revisions, for
I
D .˘EM; ˘AM; af / and input . f; af 0/ we denote the revision as follows:
I . f; af 0/ D .˘EM; ˘ 0
AM; af 00/ where ˘ 0
AM D ˘AM [ f fg and af 00 is the revised
annotation function. Further, in this section, we often refer to “removing elements
of ˘AM” to refer to changes to the annotation function that cause certain elements
of the ˘AM to not have their annotations satisﬁed in certain EM worlds. Further, as
we are looking to change the annotation function for a speciﬁc subset of facts and
strict rules, we specify these subsets with the following notation.
•
wld. f/ D fw j w ˆ fg—the set of worlds that satisfy formula f; and
•
for.w/ D V
a2w a ^ V
a…w :a—the formula that has w as its only model.
•
˘ I
AM.w/ D f f 2  [ ˝ j w ˆ af. f/g
Intuitively, ˘ I
AM.w/ is the subset of facts and strict rules in ˘AM whose
annotations are true in EM world w.
4.2.3.1
Postulates for Revising the Annotation Function
Just as we did for AM-based belief revision, here we introduce rationality postulates
for annotation function based belief revision. We note that except for vacuity,
consistency preservation, and weak success, the postulates are deﬁned in a different
manner from the AM postulates. The key difference between the AM-based and
the AF-based postulates is that AF postulates consider subsets of the AM that
occur in certain the environmental conditions—as opposed to considering the entire
analytical model as a whole. In this way, the AF-based postulates will give rise to a
more ﬁne-grained revision of the overall knowledgebase than the more coarse-grain
AM-based approach.
AF Inclusion For I . f; af 0/ D .˘EM; ˘AM [ f fg; af 00/, 8g 2 ˘AM, we have that
wld

af 00.g/

 wld.af 0.g//.
This postulate states that, for any element in the AM, the worlds that satisfy its
annotation after the revision are a subset of the original set of worlds satisfying the
annotation for that element.
AF Vacuity If I [ . f; af 0/ is consistent, then I . f; af 0/  I [ . f; af 0/.
This is the same as for the AM version of the postulate: no change is made if the
program is consistent with the added input.
AF Consistency Preservation If I is consistent, then I . f; af 0/ must also be
consistent.
Again, as with the AM version, the operator maintains a consistent program.
AF Weak Success If I [ . f; af 0/ is consistent, then . f; af 0/ 2 I . f; af 0/.

4.2
Basic Belief Revision
55
Whenever the input does not cause inconsistencies, it must be contained in the
revised program.
For a given EM world, if a portion of the associated AM knowledge base is removed
by the operator, then there exists a subset of the remaining knowledge base that is
not consistent with the removed element and f.
AF Pertinence For I . f; af 0/ D .˘EM; ˘AM[f fg; af 00/, for each w 2 W I
EM.I [
. f; af 0//, we have Xw D fh 2  [ ˝ j w ˆ af 00.h/g; for each g 2 ˘AM.w/ n Xw
there exists Yw 
 Xw [ f fg s.t. Yw is consistent and Yw [ fgg is inconsistent.
For a given EM world, if a portion of the associated AM knowledge base is removed
by the operator, then there exists a superset of the remaining knowledge base that is
not consistent with the removed element and f.
AF Uniformity 1 Let . f; af 0
1/; .g; af 0
2/ be two inputs where W I
EM.I [ . f; af 0
1// D
W I
EM.I [ .g; af 0
2//; for all w 2 W I
EM.I [ . f; af 0// and for all X  ˘AM.w/; if
fx j x 2 X [ f fg; w ˆ af 0
1.x/g is inconsistent iff fx j x 2 X [ fgg; w ˆ af 0
2.x/g is
inconsistent, then for each h 2 ˘AM, we have that:
fw 2 W I
EM.I [ . f; af 0
1// j w ˆ af 0
1.h/ ^ :af 00
1.h/g D
fw 2 W I
EM.I [ .g; af 0
2// j w ˆ af 0
2.h/ ^ :af 00
2.h/g:
If two inputs result in the same set of EM worlds leading to inconsistencies in an AM
knowledge base, and the consistency between analogous subsets (when joined with
the respective input) are the same, then the models removed from the annotation of
a given strict rule or fact are the same for both inputs.
AF Uniformity 2 Let . f; af 0
1/; .g; af 0
2/ be two inputs where W I
EM.I [ . f; af 0
1// D
W I
EM.I [ .g; af 0
2//; for all w 2 W I
EM.I [ . f; af 0// and for all X  ˘AM.w/; if
fx j x 2 X [ f fg; w ˆ af 0
1.x/g is inconsistent iff fx j x 2 X [ fgg; w ˆ af 0
2.x/g is
inconsistent, then
fw 2 W I
EM.I [ . f; af 0
1// j w ˆ af 0
1.h/ ^ af 00
1.h/g D
fw 2 W I
EM.I [ .g; af 0
2// j w ˆ af 0
2.h/ ^ af 00
2.h/g:
If two inputs result in the same set of EM worlds leading to inconsistencies in an
AM knowledge base, and the consistency between analogous subsets (when joined
with the respective input) are the same, then the models retained in the annotation
of a given strict rule or fact are the same for both inputs.
4.2.3.2
AF-Based Revision Operators
In this section, we introduce a class of operators for revising a DeLP3E program.
Unlike the AM revision, this ﬁne-grained approach requires an adjustment of the

56
4
Belief Revision in DeLP3E
conditions in which elements of ˘AM can hold true. Hence, any subset of ˘AM
associated with a world in W I
EM.I [ . f; af 0// must be modiﬁed by the operator
in order to remain consistent. So, for such a world w, we introduce the annotation
function version of the set of candidate replacement programs for ˘AM.w/ in order
to maintain consistency and satisfy the Inclusion postulate.
CandPgmaf.w; I / D f˘ 0
AM j ˘ 0
AM  ˘AM.w/ s.t. ˘ 0
AM is consistent and
À˘ 00
AM  ˘AM.w/ s.t. ˘ 00
AM  ˘ 0
AM s.t. ˘ 00
AM
is consistentg
Intuitively, for each world w, this is the set of is a maximal consistent subsets of
˘ I
AM.w/. However, unlike with AM based belief revision, the candidate replacement
program are speciﬁed for speciﬁc worlds—this in turn enables a more “surgical”
adjustment to the overall knowledgebase than AM belief revision. This is due to
the fact that in AM revision, components of the analytical model are deemed to no
longer hold in any world as opposed to a speciﬁc subset of worlds.
Before introducing our operator, we deﬁne some preliminary notation. Let ˚ W
WEM ! 2Œ[Œ˝. Recall that the sets of all facts and strict rules are denoted with 
and ˝, respectively. For each formula h in ˘AM [ f fg, where f is part of the input,
we deﬁne:
newFor.h; ˚; I ; . f; af 0// D af 0.h/ ^
^
w2W I
EM.I [. f;af 0// j h…˚.w/
:for.w/
Intuitively, newFor eliminates the inconsistency (arising from the addition of
input f to the existing program I ) by adding the negation of the formulas whose
only models are the inconsistent words—essentially, such models are then removed
from the old formula. These inconsistent worlds are.
Now we deﬁne the class of operators called AFO. We show that membership in
AFO is a necessary and sufﬁcient condition for satisfying all postulates introduced
in this chapter.
Deﬁnition 4.2 (AF-Based Operators) A belief revision operator  is an “anno-
tation function-based” (or af-based) operator ( 2 AFO) iff given program I D
.˘EM; ˘AM; af / and input . f; af 0/, the revision is then deﬁned as I . f; af 0/ D
.˘EM; ˘AM [ f fg; af 00/, where:
8h; af 00.h/ D newFor.h; ˚; I ; . f; af 0//
where 8w 2 WEM, ˚.w/ 2 CandPgmaf.w; I [ . f; af 0//.
Theorem 4.2 (Annotation Function Representation Theorem) An operator 
belongs to class AFO iff it satisﬁes Inclusion, Vacuity, Consistency Preservation,
Weak Success, Pertinence, and Uniformity 1.

4.3
Quantitative Belief Revision Operators
57
4.3
Quantitative Belief Revision Operators
We now propose exploring a novel class of operators that focus on quantitative
aspects of belief revision; in the DeLP3E setting, this means measuring aspects
such as the impact that revisions have on the probabilities with which literals are
warranted. We will adopt the following as a running example.
Example 4.6 We consider the capture-the-ﬂag scenario in which teams launch
cyber attacks against each other in an attack-defense style competition. We use
examples discussed in Chap. 2 to illustrate key concepts in this section.
Figure 4.1 shows the predicates that we will use throughout the chapter in the
running example.
As shown in the ﬁgure (and discussed in more detail below), some of these
predicates comprise the analytical model (AM), while others are part of the
environmental model (EM). For instance, in our example, predicates stating the
use of exploits as well as the teams that used the exploit in an attack such as
robot maﬁa and apt8, are part of the analytical model. On the other hand, the
environmental model contains predicates that are associated with uncertain events,
such as false negatives coming up when attributing to see whether the team blue
lotus was attacked by robot maﬁa.

Fig. 4.1 Explanation of the meaning of the predicates used in the running example

58
4
Belief Revision in DeLP3E
Fig. 4.2 Bayesian network used in the EM of the running example. The names of the random
variables are simply the abbreviations of their corresponding atoms: AR 7! robot_risk, DR 7!
apt8_risk, FNAT 7! FN  robot_test, and FNTS 7! FN  pwnies
Fig. 4.3 Probability distribution for the worlds in the running example
In the following, we use basicAM to denote all possible conjunctions or disjunc-
tions of literals from LAM, which we refer to as basic formulas. The following is an
example of a Bayesian Network over the running example.
Example 4.7 Consider the set PEM from Fig. 4.1. The Bayesian network depicted
in Fig. 4.2 describes the probability distribution Pr over all possible worlds WEM
shown in Fig. 4.3.
So, for instance, the probability that false negatives do not arise in any of the two
cases, and that the team blue lotus can be attacked by both robot maﬁa and pwnies
(world 	4) is 0:29808.

The following is an example of a PreDeLP program over the running example.
Example 4.8 Consider again the capture-the-ﬂag scenario from our running exam-
ple; the DeLP3E program in Fig. 4.4 encodes some basic knowledge that a security

4.3
Quantitative Belief Revision Operators
59
Fig. 4.4 A ground argumentation framework
Fig. 4.5 Example arguments based on the running example scenario
Fig. 4.6 An example of an
annotation function over the
running example
analysts might use for cyber attribution. For instance, strict rule !1 states that
based on a negative result of team pwnies being the attacker we can conclude that
exploit_X was not used in the attack. On the other hand, defeasible rule ı1 states
that if exploit_X was used to target apt8, then the attacker can be samurai.

Example 4.9 Figure 4.5 shows example arguments based on the PreDeLP program
from Fig. 4.4. Argument A3 uses an additional component not present in the original
program, and states that if we can assume a negative result for pwnies being the
attacker, we can conclude that exploit_X was not used in the attack.

Figure 4.6 shows an example of an annotation function for our running example;
for instance, the annotation for rule ı2 means that this rule only holds whenever the
probabilistic event robot_risk is true. If annotations are “True”, this means that they
hold in all possible worlds.

60
4
Belief Revision in DeLP3E
Fig. 4.7 A depiction of how the DeLP3E program in the running example can be decomposed
into one classical PreDeLP program for each possible EM world (cf. Fig. 4.3 for the deﬁnition of
worlds 	1–	16 in terms of the random variables in the EM)
In the following, given DeLP3E program I D .˘EM; ˘AM; af / and 	 2 WEM,
we use notation ˘AM.	/ D f f 2 ˘AM
s.t. 	 ˆ af . f/g. This gives rise to a
decomposed view of DeLP3E programs, as illustrated next.
Example 4.10 Consider the different examples presented so far: the EM from
Example 4.7 (with the worlds from Fig. 4.3), ˘AM from Fig. 4.4, the arguments
in Fig. 4.5, and the annotation function from Fig. 4.6—these components give
rise to a DeLP3E program I
D .˘EM; ˘AM; af /. Figure 4.7 shows how I
can be decomposed into one classical PreDeLP program ˘AM.	/ for each world
	 2 WEM.
For instance, ˘AM.	7/ contains 1, 2, 3, !1, !2, !3, !4, and ı1 because the
annotation function associates condition True to all of these components; it contains
ı2 and ı4 because condition robot_risk is true in 	7, and it does not contain ı3
because condition apt8_risk is false in 	7.

The most direct way of considering consequences of DeLP3E programs is thus
to consider what happens in each world in WEM; that is, the defeat relationship
among arguments depends on the current state of the (EM) world.
Example 4.11 Consider the different examples presented so far: the worlds in
Fig. 4.3, ˘AM from Fig. 4.4, the arguments in Fig. 4.5, and the annotation function
from Fig. 4.6.
Since argument A1 uses defeasible rule ı3, and af .ı3/ D apt8_risk (while the
other two components have annotation “True”), we can conclude that this argument
exists in worlds in which apt8_risk is true, i.e., 	1–	4 and 	9–	12.

Example 4.12 Let us return to the running example; consider ˘AM from Fig. 4.4,
˘EM from Fig. 4.2, and the annotation function from Fig. 4.6, with the addition of
fact 4 D pos_apt8 with af .4/ D True and fact 5 D neg_robot_maﬁa with
af .5/ D :FN-robot_test. It is now clear that the program is inconsistent, since
there exists world 	3 (among several others) such that S
x2[˝ j 	3ˆaf.x/fxg warrants

4.3
Quantitative Belief Revision Operators
61
both robot_maﬁa (via argument with 4 and !3) and :robot_maﬁa (via argument
with 5 and !2).

We have ﬁnally arrived at the main problem we address in this chapter—revising
knowledge bases. This problem can be generically stated as: given DeLP3E
program I D .˘EM; ˘AM; af /, with ˘AM D ˝ [  [  [ ˚ and a pair . f; af 0/
where f is either an atom or a rule and af 0 is equivalent to af , except for its expansion
to include f,1 obtain a new program I 0 called the revised knowledge base that
addresses the incorporation of the epistemic input . f; af 0/ into the original program;
we denote this operation with the symbol “	”—i.e., I 0 D I 	 . f; af 0/.
Now, the problem statement as presented above is quite vague, since we did not
give any details as to how the operator “addresses the incorporation”of the epistemic
input. There are many approaches in the literature that address this problem quite
differently; one of the main properties that characterize revision operators is whether
or not they satisfy the Success property, which states that the epistemic input must
be a consequence of the revised knowledge base. Here, we will adopt a cautious
stance and assume that this property does not hold in general; therefore, we focus
on so-called non-prioritized revision operators.
The basic issue that revision operators must deal with is inconsistency (we
will discuss this in more depth shortly); as we saw in Sect. 3.4, inconsistency in
DeLP3E programs involves worlds that have non-zero probability and an associated
PreDeLP program that is inconsistent. In the previous section we identiﬁed three
basic approaches that can be taken towards solving this problem: Modifying the
EM, Modifying the AM, and Modifying the annotation function. In the following,
we will assume that epistemic inputs involve only strict components ( facts or rules),
since defeasible components can always be added without inconsistencies arising.
Regarding these three possible approaches, we now focus on the third one since
it is a generalization of the second—as we saw earlier, if we only allow removing
elements from the AM, such an operation will have the same effect as not removing
the element but modifying the annotation function so that it associates the formula
“?” to it. Furthermore, operations of the ﬁrst kind alone do not sufﬁce to perform
revisions, as can be seen in the following simple example.
Example 4.13 Consider the following DeLP3E program, where the EM consists of
two worlds fag and f:ag, each with probability 0.5:
!1 W
p  q
af .!1/ D a
1 W
:p
af .1/ D a
!2 W
:p  q
af .!2/ D :a
2 W
p
af .2/ D :a
Now, suppose we wish to revise by formula 3 W q with af .3/ D True. Since
both EM worlds are inconsistent with the formula, it is impossible to change the
allocation of the probability mass in order to avoid inconsistencies; therefore, the
only option is to reject the input.

1That is, af 0.x/ D af .x/ for all x 2 dom.af /, and dom.af 0/ D dom.af / [ f fg.

62
4
Belief Revision in DeLP3E
Fig. 4.8 The DeLP3E program from the running example, after adding facts 4 and 5. The
annotation function is provided in a separate column for convenience
4.3.1
Towards Quantitative Revision
Traditionally, belief revision has been addressed from a qualitative point of view
rather than a quantitative one. A simple example of this is the fact that, faced with
the option of removing either both atoms a and b or only atom c, classical revision
operators typically declare both options to be just as good, since neither is a subset
of the other; it could be argued, then, that taking quantitative aspects into account
(such as the number of elements removed) may lead to a better solution—of course,
this may depend on other factors. As we will see, there are different ways in which
such quantitative aspects can be incorporated into revision operations. For instance,
in our setting, DeLP3E programs can be regarded in a world-by-world manner, and
changes made in one world can be compared to those made in another. The AFO
operators described in Sect. 4.2 make decisions for each world independently; we
now wish to address the issue of taking into account different kinds of quantitative
aspects when revising DeLP3E programs. The following example motivates our
approach in our capture-the-ﬂag scenario.
Example 4.14 Consider again the running example, and suppose the a security
analyst has decided to test who conducted the cyber attack between pwnies and
robot maﬁa, and that both tests yielded negative results. Note that the validity of
these tests is subject to probabilistic events (in this case, false negatives). The new
program is reproduced in Fig. 4.8.
Figure 4.9 shows the world-by-world decomposition of the new program, and the
atoms that are warranted in each case. From the information in this ﬁgure, we can

4.3
Quantitative Belief Revision Operators
63
Fig. 4.9 Atoms that are warranted in each possible EM world, given the AM and annotation
function in Fig. 4.8
compute the following probabilities for the hypotheses that the security analyst is
contemplating the teams (apt8, robot maﬁa, samurai):
Literal
Probability
apt8
W 0:06672
samurai
W 0:05088
:samurai
W 0:93324
robot_maﬁa
W 0:06992
:robot_maﬁa
W 0:92888
Since they all have low probabilities after performing the tests, the analyst decides
to test for team apt8 and in this case receives a positive result (atom pos_apt8).
For the sake of this example, we will assume that the validity of the outcome of
this test (unlike the other two) is not subject to probabilistic events—thus, we have
af .pos_apt8/ D True.
Now, while for the ﬁrst two tests we were able to simply add the corresponding
atoms and extend the annotation function accordingly,simply adding 6 D pos_apt8
with af .6/ D True causes inconsistencies to arise in eight of the possible worlds
(	3, 	4, 	7, 	8, 	11, 	12, 	15, and 	16). Essentially, the problems arise because the
negative robot maﬁa test allows us to conclude that team robot maﬁa was not the
attacker, while the positive apt8 test would allow us to conclude that indeed team
apt8 was the attacker. Since both derivations only involve strict components, this
leads to an inconsistent AM.


64
4
Belief Revision in DeLP3E
Example 4.14 shows an interesting case of belief revision in DeLP3E programs;
when presented with new information that is in conﬂict with existing one, we must
ﬁnd a way to address its incorporation into the existing knowledge—non-prioritized
operators are very ﬂexible, since they always have the option of ignoring the new
information. However, this ﬂexibility also means that—in the case of DeLP3E
programs—there is no guidance with respect to how revisions should be carried out
globally, since each world is treated as a separate revision problem. Next, we discuss
two kinds of functions that will prove to be useful in addressing this situation.
4.3.2
Two Building Blocks
We now introduce warrant probability functions and revision objective functions,
which are later used in the deﬁnition of our new class of non-prioritized belief
revision operators.
Warrant Probability Functions As one of the building blocks to our quantitative
approach, given a DeLP3E program I we deﬁne warrant probability functions
(WPFs, for short).
Before introducing these formulas, we need to present a simple extension to the
concept of “warrant status”, which is up to now deﬁned for literals. The following
deﬁnition is a simple extension to conjunctions or disjunctions of literals.
Deﬁnition 4.3 (Warranting a Conjunction/Disjunction of Literals) Let ˘AM be
a ground PreDeLP program and Q be either a conjunction or disjunction of ground
literals L1; : : : ; Ln. The warrant status of Q with respect to ˘AM is deﬁned as
follows:
1. If Q is a single literal L, then the warrant status of Q is the warrant status of L in
˘AM.
2. If Q D Q1 ^ Q2 then the warrant status of Q is:
•
Yes iff the warrant status of both Q1 and Q2 is Yes;
•
No if the warrant status of either Q1 or Q2 is No; and
•
Undecided whenever neither of the above cases hold.
3. If Q D Q1 _ Q2 then the warrant status of Q is:
•
Yes iff the warrant status of either Q1 or Q2 is Yes;
•
No if the warrant status of both Q1 and Q2 is No; and
•
Undecided whenever neither of the above cases hold.
Using Deﬁnition 4.3, we can easily extend the nec and poss notations (cf. Page 32)
to conjunctions and disjunctions of literals.
The following result is a consequence of the fact that conﬂicting literals cannot
be warranted in ( Pre)DeLP [6].

4.3
Quantitative Belief Revision Operators
65
Fig. 4.10 Histogram depiction of the entailment probability functions for the programs discussed
in Example 4.14
Proposition 4.3 Let ˘AM be a ground PreDeLP program and Q D L1 ^ : : : ^ Ln
be a conjunction of ground literals. Then, only one of the following cases holds: (i)
P `war Q, (ii) P `war :Q, or (iii) the warrant status of Q is undecided.
Warrant Probability Functions are then simply deﬁned as partial mappings with
signature:

I W basicAM ! Œ0; 1
such that for f 2 basicAM, 
I . f/ D p if and only if P
	2nec. f/ Pr.	/ D p.2 When
the program is clear from context, we drop the subscript and write simply 
 . In the
following, we use notation dom.
 / to denote the set of formulas for which 
 is
deﬁned. The table shown in Example 4.14 is a simple example of a WPF, whose
domain is a handful of literals. The following is another example along the same
vein.
Example 4.15 Figure 4.10 shows three examples of WPFs in which the domains
are ﬁxed to the set of literals that can be warranted in the input program. These
functions are related to the revision described in Example 4.14: the black bars
show the original probabilities, the striped bars give the probabilities yielded by
the program obtained by favoring the inclusion of the positive apt8 test, while the
light gray bars depict the probabilities obtained by favoring the negative robot maﬁa
test. Figure 4.11 shows the three revised programs.

2Note that this deﬁnition can easily be extended to deal with probability intervals as well (i.e.,
using both nec and poss); here, for simplicity of presentation, we adopt this deﬁnition in order to
work with point probabilities.

66
4
Belief Revision in DeLP3E
Fig. 4.11 The DeLP3E program from the running example, after performing three revisions: (i)
The addition of the 4 and 5, as discussed in Example 4.14; (ii) The revision by pos_apt8 by
prioritizing this input; and (iii) The same revision but prioritizing neg_robot_maﬁa
Revision Objective Functions The other building block allows us to effectively
quantify how good a revision is considered to be. Towards this end, we deﬁne
revision objective functions (ROFs, for short) as functions that take two DeLP3E
programs I1 and I2, along with an epistemic input . f; af /, and returns a positive
real number. We keep the deﬁnition of ROFs very general in order to allow different
kinds of objectives to be speciﬁed. The following is a simple example of a ROF over
our running example, which makes use of warranting probability functions.
Example 4.16 Let us return once again to the capture-the-ﬂag example. Suppose
that we take the three revised programs we presented (Fig. 4.11)—call them I1,
I2, and I3—and that we wish to compare them with respect to the effect of the
last revision over the warranted atoms, taking the probabilities yielded by I1 as the
baseline. So, we deﬁne the following revision objective function:
.I ; I 0; . f; af 0// D e P
L2LAM;L¤f j
I .L/
I 0.L/j
where 
I is the WPF for program I .
Intuitively, this function sums up all the differences between the probabilities of
literals entailed by the programs, but ignores the input (if it is a literal). In this way,
a distance between the original program and the two possible revisions is obtained
based on the effects that each revision had on the probabilities with which literals
are derived. So, for our revisions, we get:

4.3
Quantitative Belief Revision Operators
67
.I1; I2; .pos_apt8; af 2//  0:0547
.I1; I3; .pos_apt8; af 3//  0:8611
Therefore, we can conclude that the revision yielding I3 is preferred over the one
yielding I2 when this ROF is adopted.

Note that the function presented in Example 4.16 is just one possibility; the
framework is very ﬂexible and allows the user to express many different functions,
depending on the speciﬁc way in which they wish to express distances between the
original program and a given revised program.
4.3.3
The Class QAFO
Given the basic constructs introduced above, we can now deﬁne the class of
quantitative annotation function-based revision operators.
Deﬁnition 4.4 (The Class QAFO) Let I D .˘EM; ˘AM; af /, with ˘AM D ˝ [
[[˚ be a DeLP3E program, ? 2 AFO be an annotation function-based belief
revision operator, and  be a revision objective function. Operator ? is said to be a
quantitative af-based operator (denoted ? 2 QAFO) if:
Given an epistemic input . f; af 0/, we have that if I 0
D I ? . f; af 0/ then there
does not exist DeLP3E program I 00 D I  . f; af 0/ such that .I ; I 00; . f; af 0// >
.I ; I 0; . f; af 0//,
where 	 2 AFO is an arbitrary operator.
So, this subclass of AFO simply takes a revision objective function and uses it
to obtain the best possible revised program. We present a second example, based
on our previous work on applications of DeLP3E to problems in the cybersecurity
domain [9], shows how QAFO operators can be applied to belief revision problems.
Example 4.17 Suppose we are modeling a cybersecurity scenario in which a com-
puter worm has been deployed and has infected millions of computers worldwide—
by the time the worm is discovered, it is very difﬁcult to reason about the origin and
even the intended target of the attack.
Towards this end, we can model all knowledge available by means of a DeLP3E
program as discussed in Sect. 3.5: I D .˘EM; ˘AM; af /, in which there is one
distinguished predicate condOp.A; O/ in the AM that is intuitively read as “actor
A conducted operation O”. Furthermore, if we assume that only one actor is ever
responsible for an operation (an assumption that can easily be removed), we have
an integrity constraint of the form oneOf.C/, where C is the set of all ground atoms
built with the condOp predicate.
Given this setup, we can deﬁne a WPF with a domain consisting of some
formulas of interest that reﬂect conditions that the analysts would like to remain
relatively unaffected when incorporating new information. For instance, suppose
we deﬁne:

68
4
Belief Revision in DeLP3E
dom.
 / D
n
:condOp.countryA; worm/ ^ :condOp.countryB; worm/
condOp.countryD; worm/
o
;
denoting the fact that neither country A nor country B are responsible for deploying
the worm, and that country D is. If we pair this WPF with the ROF from
Example 4.16, the corresponding QAFO operator will prefer revisions that do not
affect the conclusions already reached regarding the probabilities assigned to these
statements.
In other words, this deﬁnition of dom.
 /, with the ROF in question, causes
distances to be gauged relative to their effect on the probabilities assigned to the
suspicions that (i) neither country A nor country B carried out the attack, and (ii)
country D is behind the attack. Thus, such a setup causes the operator to prefer
revisions that keep the probabilities assigned to such suspicions as close as possible
to the ones yielded by the original program.

In the next section, we study the computational complexity associated with this
approach to belief revision in the DeLP3E setting.
4.3.4
Computational Complexity
In this section, we will focus on some of the computational aspects of quantitative
af-based belief revision operations.
As a ﬁrst observation, we have that the problem of deciding the warranting status
in a (classical) PreDeLP program has not yet been pinpointed. In [2], the authors
present a proof for the PSPACE-completeness of the problem of marking a given
dialectical tree; PSPACE membership for deciding the warrant status is therefore
a direct consequence of this result, since a dialectical tree can be built within this
budget. As a step towards ﬁnding a lower bound for the complexity of the problem
in general, we have the following.
Proposition 4.4 Let ˘AM be a ground PreDeLP program and L be a ground literal.
Deciding ˘AM `war L is NP-hard.
As a corollary to Proposition 4.4, we have that deciding our extended notion of
warrant status remains within the same complexity bounds.
Corollary 4.1 Let ˘AM be a ground PreDeLP program and Q be either a conjunc-
tion or disjunction of ground literals. Deciding ˘AM `war Q is NP-hard and in
PSPACE.
Assumption Since, as stated above, the precise complexity of deciding the warrant
status of a literal in a PreDeLP program is not yet known, and with the objective
of separating the complexity of this problem from the complexity of the problems

4.3
Quantitative Belief Revision Operators
69
inherent to quantitative belief revision in DeLP3E programs, in the following
we will make the assumption that classical warranting in PreDeLP is decidable
in polynomial time. This is not an unreasonable assumption if we consider the
possibility of pre-compiling inferences [1] or having tractable approximation
algorithms to address the problem. We call this the polynomial-time warranting
( PTW) assumption. Note that, even though this assumption does not hold in general,
it is a useful tool in the analysis of the complexity of the problems studied here; it is
also with this spirit that we make use of the PTW assumption.
Unfortunately, our ﬁrst result regarding the probabilistic extension of PreDeLP
tells us that computing WPFs runs into a computational tractability hurdle.
Theorem 4.3 Under the PTW assumption, computing the warrant probability
function for a DeLP3E program is #P-hard.
The complexity class #P contains problems related to counting solutions (or, in
Turing machine terms, accepting paths) to problems in NP. The decision version of
this class is called PP, and contains problems decidable by a probabilistic Turing
machine in polynomial time, with error probability less than a certain proportion
(say, 1/2). Unfortunately, Toda’s theorem [12] tells us that a polynomial-time Turing
machine with either a PP or #P oracle can solve all problems in the polynomial
hierarchy.
Though it might be surmised that the #P-hardness is caused solely by the
computation of probabilities (as is the case in many probabilistic formalisms), the
construction used in proof of Theorem 4.3 [11] allows us to arrive at the following
conclusion.
Observation 2 Computing the warrant probability function for a DeLP3E pro-
gram is #P-hard even in the special case in which probabilities associated with EM
worlds can be computed in PTIME.
Though this intractability holds in general, restricting the EM can soften the
impact on complexity. For instance, if we assume that Nilsson’s probabilistic
logic [8] is used then the complexity is lowered, as we show next; ﬁrst, we introduce
a key lemma:
Lemma 4.4 ([3, 4]) If a system of m linear equalities and/or inequalities has a
nonnegative solution, then it has a nonnegative solution with at most m positive
variables.
This result was ﬁrst introduced in [3], and later used in [4] to show that deciding the
validity of a formula in their logic is NP-complete. We can now state our result.
Proposition 4.5 Under the PTW assumption, and assuming that Nilsson’s proba-
bilistic logic is used in the EM, computing the warrant probability function for a
DeLP3E program is NP-complete.
The previous result gives us a hint towards reaching the next one: if we combine
the simplifying assumption that probabilities can be computed tractably with the

70
4
Belief Revision in DeLP3E
further assumption that the number of EM worlds that have non-zero probability
is bounded by a polynomial (Condition 1 below), then we are guaranteed that
computing WPFs is also tractable.
Corollary 4.2 Let I D .˘EM; ˘AM; af /, with ˘AM D ˝ [  [  [ ˚, be a
DeLP3E program. If we make the following assumptions:
1. jf	 j 	 2 WEM and Pr.	/ > 0gj 2 O.poly.n//, where n represents the size of the
input;
2. Pr.	/ can be computed in PTIME for any 	 2 WEM; and
3. the PTW assumption holds,
then warrant probability functions for I can also be computed in PTIME.
Unfortunately, the following result states that even in this scenario we still face
an intractable hurdle when computing optimal revisions.
Theorem 4.4 Let I D .˘EM; ˘AM; af /, with ˘AM D ˝ [  [  [ ˚, be a
DeLP3E program, ? 2 QAFO, and  be a revision objective function that can be
computed in polynomial time. If we have that:
1. jf	 j 	 2 WEM and Pr.	/ > 0gj 2 O.poly.n//, where n represents the size of the
input;
2. Pr.	/ can be computed in PTIME for any 	 2 WEM; and
3. the PTW assumption holds,
then deciding if .I ; I ? . f; af 0/; . f; af 0//  k for some k 2 R, is NP-complete.
The construction used in the proof of Theorem 4.4 (cf. [11]) uses a very powerful
objective function that essentially encodes the NP-hard problem and, furthermore,
this objective function is not based on WPFs. We now provide an alternative
result that proves NP-completeness under the same conditions, but assumes that
the objective function is simply the sum of the probabilities assigned by the WPF to
the set of ground atoms in the language associated with the AM.
Theorem 4.5 Let I D .˘EM; ˘AM; af /, with ˘AM D ˝ [  [  [ ˚, be a
DeLP3E program, ? 2 QAFO, and  be a revision objective function that can be
computed in polynomial time. If we have that:
1. jf	 j 	 2 WEM and Pr.	/ > 0gj 2 O.poly.n//, where n represents the size of the
input;
2. Pr.	/ can be computed in PTIME for any 	 2 WEM; and
3. the PTW assumption holds,
then deciding if .I ; I ? . f; af 0/; . f; af 0//  k for some k 2 R, is NP-complete
even when  is deﬁned as P
a2GAM 
 .a/.
So, the Theorem 4.5 illustrates that the quantiﬁed revision problem is NP-hard
when the EM and the number of EM worlds, and (hence) the computation of the
WPF is not a source of complexity—even when the ROF used is a simple aggregate
over WPFs of atoms. Further, as we can embed the Simple Max Cut problem, the

4.3
Quantitative Belief Revision Operators
71
ROF—even a simple sum over WPFs—will not necessarily be monotonic, even
when using a revision operator that satisﬁes the Inclusion postulate (where the set
of worlds satisfying af . y/  af 0. y/). This also shows NP-completeness when the
belief revision operator performs modiﬁcations to ˘AM (by removing elements, as
discussed in [10]) as setting af . y/ D :x can be viewed as an operation that is
equivalent to removing it from ˘AM.
We also note that the related problem of consolidation or contraction by
falsum, where we start with an inconsistent program and then must adjust the
annotation function to make it consistent, can also be shown to be NP-complete:
we ﬁx the epistemic input to True, and change the rules of the form set1.vi/  
query; :set1.vi/  query to facts of the form set1.vi/; :set1.vi/.
4.3.5
Warranting Formulas
We now focus on an algorithmic approach that can be used to compute approximate
solutions and therefore address the computational intractability that we have seen in
the results above.
In the following, given a dialectical forest F.L/ and a node V corresponding
to argument A, we will use the notation label.V/ D V
c2A af .c/. For a given
probabilistic argumentation framework, literal, and dialectical tree, Algorithm war-
rantFormula in Fig. 4.12 computes the formula describing the set of possible worlds
that are warranting scenarios for the literal. Intuitively, this algorithm creates a
formula for every dialectical tree in the forest associated with an argument—the
algorithm iteratively builds a formula associated with the environmental conditions
under which the argument in the root of the tree is undefeated. It then returns the
disjunction of all such formulas in that forest. We refer to this disjunction as the
warranting formula for the literal.
Fig. 4.12 An algorithm that takes a classical dialectical forest and computes a logical formula
specifying the possible worlds under which a given literal is warranted

72
4
Belief Revision in DeLP3E
Fig. 4.13 Dialectical forest for literal L D exploit_X composed of trees T1 (left) and T2 (right)
The following result states the correctness of the warrantFormula algorithm.
Proposition 4.6 Given forest F .L/,
nec.L/ D
n
	 2 WEM j 	 ˆ warrantFormula

F .L/
o
poss.L/ D
n
	 2 WEM j 	 6ˆ warrantFormula

F .:L/
o
:
Even though warranting formulas are another way of solving the problem of
computing probabilities exactly, our main motivation for developing it was to
explore options for pursuing tractable algorithms, as discussed next.
The following is an example of the warranting formula approach in the setting of
our running example.
Example 4.18 Consider the DeLP3E in our running example as shown in Fig. 4.11
with annotation function af 1. If we run Algorithm warrantFormula for literal
exploit_X, we start with the dialectical forest shown in Fig. 4.13.
Suppose that the algorithm begins with tree T1 (on the left); the only leaf of this
tree corresponds to vertex v2 for argument A2, and its label remains the conjunction
of all annotations of elements in the argument—label.v2/ D :FN_pwnies. The
algorithm then moves to the next node up, which is already the root, and updates the
label by adding the conjunction with the negation of its child, which yields:
label.v1/ D apt8_risk ^ :

:FN_pwnies

D apt8_risk ^ FN_pwnies:
Processing tree T2 similarly yields:
label.v3/ D True ^ :

:FN_pwnies

D FN_pwnies:
Finally, the algorithm outputs the disjunction of these two formulas, which is simply
FN_pwnies.


4.4
Conclusions and Future Work
73
4.3.6
Outlook: Towards Tractable Computations
By applying the warrantFormula algorithm to the dialectical forest for a given
literal L, we can obtain the sets nec.L/ and poss.L/ with a running time proportional
to the size of the forest and the annotation formulas—though the worst-time
complexity has not been determined exactly, it is safe to conjecture that the worst
case is intractable. However, the warranting formula approach opens the door to
several possibilities for heuristics and approximate computations that either avoid
exhaustively enumerating worlds in WEM or working with full forests (or both).
When combined with existing heuristics for classical argumentation (the AM) and
probabilistic models (the EM), this provides us with a much more efﬁcient way
to compute warranting probability functions. Experimental evaluations for such
hypotheses are currently underway.
The use of the warranting formula approach can have several impacts in the
implementation of speciﬁc QAFO operators. First, warrant probability functions 
in this setting can now be redeﬁned to map elements in their domain to warranting
formulas instead of probabilities as in their original formulation. Revision objective
functions now have at their disposal formulas instead of raw numbers. This opens
up the possibility for speciﬁc implementations to leverage optimizations such as
applying SAT algorithms to decide whether Pr I1.L/  Pr I2.L/ (which can be
decided via the SAT check 
I1.L/ ) 
I2.L/). Such an approach is clearly
compatible with heuristic optimizations that may, for instance, sacriﬁce precision
for greater tractability.
An alternative class of operators can thus be deﬁned based on the same ideas
as QAFO except that approximations are allowed instead of exact computations.
There is much work to be done in this direction, which is outside the scope of the
current book.
4.4
Conclusions and Future Work
In this chapter, we focused on characterizing belief revision operations over
DeLP3E knowledge bases. In the ﬁrst part, we presented two sets of postulates,
both inspired by the postulates that were developed for non-prioritized revision of
classical belief bases. The ﬁrst set of postulates provides a coarse approach that
assumes that revision operations only allow changes to the analytical model, while
the second is a ﬁner-grained approach based on modiﬁcations to the annotation
function, yielding a class that called AFO. We then proceeded to study constructions
of operators based on these postulates, and prove that they are equivalent to their
characterizations by the respective postulates.
In the second part, we aimed to further explore the AFO class by considering
operators that have the further requirement of “quantitative optimality”—this gave
rise to the QAFO class of operators. Though this optimality criterion was kept as

74
4
Belief Revision in DeLP3E
general as possible so that knowledge engineers can specify their preferences, we
explored the computational complexity of the approach in general, arriving at a
host of results that range from intractability for the general case to polynomial-
time special cases. Finally, we presented an algorithm designed to compute the
probability with which a literal is warranted via so-called warranting formulas, and
provide some initial discussion regarding how this approach could be applied in the
implementation of QAFO operators or approximationsof them that trade theoretical
guarantees for tractability in practice.
References
1. M. Capobianco, C. I. Chesñevar, and G. R. Simari.
Argumentation and the dynamics of
warranted beliefs in changing environments. Journal on Autonomous Agents and Multiagent
Systems, 11:127–151, Sept. 2005.
2. L. A. Cecchi and G. R. Simari. El marcado de un árbol dialéctico en DeLP es PSPACE-
completo. In Proceedings of Congreso Argentino de Ciencias de la Computación (CACIC),
2011.
3. V. Chvátal. Linear Programming. W.H.Freeman, New York, 1983.
4. R. Fagin, J. Y. Halpern, and N. Megiddo. A logic for reasoning about probabilities. Information
and Computation, 87(1/2):78–128, 1990.
5. M. A. Falappa, G. Kern-Isberner, M. Reis, and G. R. Simari. Prioritized and non-prioritized
multiple change on belief bases. Journal of Philosophical Logic, 41(1):77–113, 2012.
6. A. J. García and G. R. Simari. Defeasible logic programming: An argumentative approach.
Theory and Practice of Logic Programming, 4(1–2):95–138, 2004.
7. S. Hansson. Semi-revision. Journal of Applied Non-Classical Logics, 7(1–2):151–175, 1997.
8. N. J. Nilsson. Probabilistic logic. Artiﬁcial Intelligence, 28(1):71–87, 1986.
9. P. Shakarian, G. I. Simari, G. Moores, S. Parsons, and M. A. Falappa. An argumentation-based
framework to address the attribution problem in cyber-warfare. In Proceedings of the ASE
International Conference on Cyber Security, 2014.
10. P. Shakarian, G. I. Simari, G. Moores, D. Paulo, S. Parsons, M. A. Falappa, and A. Aleali.
Belief revision in structured probabilistic argumentation: Model and application to cyber
security. Annals of Mathematics and Artiﬁcial Intelligence, 78(3–4):259–301, 2016.
11. G. I. Simari, P. Shakarian, and M. A. Falappa.
A quantitative approach to belief revision
in structured probabilistic argumentation. Annals of Mathematics and Artiﬁcial Intelligence,
76(3–4):375–408, 2016.
12. S. Toda. On the computational power of PP and ˚P. In Proc. of FOCS, pages 514–519, 1989.

Chapter 5
Applying Argumentation Models for
Cyber Attribution
5.1
Introduction
As we have shown and argued in previous chapters, cyber attribution is one of
the central technical and policy challenges in cybersecurity—the main reasons for
this is that oftentimes the evidence collected from multiple sources provides a
contradictory viewpoint, and this gets worse in cases of deception where either an
attacker plants false evidence or the evidence points to multiple actors. In Chap. 3
we proposed DeLP3E to address this issue using formal logic-based tools; in this
chapter, we discuss:
•
how a model for cyber attribution can be designed and implemented in the
DeLP3E framework;
•
experiments demonstrating that using argumentation-based tools can signiﬁ-
cantly reduce the number of potential culprits that need to be considered in the
analysis of a cyberattack; and
•
experiments showing that the reduced set of culprits, used in conjunction with
classiﬁcation, leads to improved cyber attribution decisions.
The DeLP3E models used in this chapter are a subset of DeLP3E since they
don’t involve probabilistic reasoning; such models are thus equivalent to classical
(Pre)DeLP models.
Preliminaries Before describing our models in detail, we recall some notation
introduced in previous chapters. Variables and constant symbols represent items
such as the exploits/payloads used for the attack, and the actors conducting the
cyberattack (in this case, the teams in the CTF competition). We denote the set
of all variable symbols with V and the set of all constants with C. For our model
we require two subsets of C: Cact, denoting the actors capable of conducting the
© The Author(s) 2018
E. Nunes et al., Artiﬁcial Intelligence Tools for Cyber Attribution, SpringerBriefs in
Computer Science, https://doi.org/10.1007/978-3-319-73788-1_5
75

76
5
Applying Argumentation Models for Cyber Attribution
Table 5.1 Example predicates and explanation
Predicate
Explanation
attack.exploit1, bluelotus)
exploit1 was targeted towards the team Blue Lotus
replay_attack.E ; Y/
Exploit E was replayed by team Y
deception.exploit1; apt8/
Team apt8 used exploit1 for deception
time_diff.I; Y/
Team Y was deceptive within the given time interval I
culprit.exploit1; apt8/
Team apt8 is the likely culprit for the attack (using exploit1
on the target team)
cyberoperation, and Cops, denoting the set of unique exploits used. We use symbols
in all capital letters to denote variables. In the running example, we use a subset of
our DEFCON CTF dataset.
Example 5.1 The following are examples of actors and cyberoperations appearing
in the CTF data:
Cact D fbluelotus, robotmaﬁa, apt8g;
Cops D fexploit1; exploit2; : : : ; exploitng:

The language also contains a set of predicate symbols that have constants or
variables as arguments, and denote events that can be either true or false. We denote
the set of predicates with PAM; examples of predicates are shown in Table 5.1. For
instance, culprit.exploit1; apt8/ will either be true or false, and denotes the event
where apt8 used exploit1 to conduct a cyberoperation.
A ground atom is composed by a predicate symbol and a tuple of constants, one
for each argument. The set of all ground atoms is denoted as GAM. A ground literal L
is a ground atom or a negated ground atom; hence, ground literals have no variables.
An example of a ground atom for our running example is attack.exploit1; bluelotus/.
We denote a subset of GAM with G0
AM.
Finally, recall that DeLP3E programs are comprised of an analytical model
(AM), an environmental model (EM), and an annotation function relating elements
from the former with elements from the latter; however, only the AM is of interest
in the models developed in this chapter. We thus adopt the usual notation for
(Pre)DeLP programs, denoting the knowledge base with ˘ D .; ˝; ˚; /, where
 is the set of facts, ˝ is the set of strict rules, ˚ is the set of presumptions, and
 is the set of defeasible rules. Examples of these constructs are provided with
respect to the CTF dataset in Fig. 5.1. For instance, 1 indicates the fact that exploit1
was used to target the team Blue Lotus, and 5 indicates that team pwnies is the
most frequent user of exploit1. For the strict rules, !1 says that for a given exploit1
the attacker is pwnies if it was the most frequent attacker and the attack exploit1
was replayed. Defeasible rules can be read similarly; ı2 indicates that exploit1 was
used in a deceptive attack by APT8 if it was replayed and the ﬁrst attacker was not

5.2
Baseline Argumentation Model (BM)
77
Fig. 5.1 A ground argumentation framework
Fig. 5.2 Example ground arguments from Fig. 5.1
APT8. By replacing the constants with variables in the predicates we can derive a
non-ground argumentation framework.
Figure 5.2 shows example arguments based on the KB from Fig. 5.1; here, ˝A1,
replay_attack.exploit1/˛ is a subargument of ˝A2, deception.exploit1; apt8/˛ and
˝A3, culprit.exploit1; apt8/˛. Furthermore, we can see that A4 attacks A3.
5.2
Baseline Argumentation Model (BM)
In Chap. 2 we discussed how machine learning techniques can be leveraged to
identify attackers; let us recall brieﬂy the setup, since the experiments reported
on here follow the same structure. The dataset was divided according to the target
team, yielding 20 subsets, and all the attacks were then sorted according to time.
The ﬁrst 90% of the attacks were reserved for training, and the remaining 10% for
testing. The byte and instruction histograms were used as features to train and test

78
5
Applying Argumentation Models for Cyber Attribution
Fig. 5.3 Defeasible and strict rule for non-deceptive attack
the models. The approaches based on random forest classiﬁers [1, 2] performed
the best, with an average accuracy of 0.37; most of the misclassiﬁed samples
corresponded to deceptive attacks and their duplicates.
When using machine learning approaches it is difﬁcult to map the reasons
why a particular attacker was predicted, especially in cases of deception where
multiple attackers were associated with the same attack. Knowing the arguments
that supported a particular decision would greatly aid the analyst in making better
decisions dealing with uncertainty. To address this issue we now describe how we
can form arguments/rules based on the latent variables computed from the training
data, given an attack for attribution.
We use the following notation: let E be the test attack under consideration aimed
at target team X, Y represent all the possible attacking teams, and D be the set of all
deceptive teams (those using the same payload to target the same team) if the given
attack is deceptive in the training set. For non-deceptive attacks, D will be empty.
We note that facts cannot have variables, only constants (however, to compress the
program for presentation purposes, we use meta-variables in facts). To begin, we
deﬁne the facts:
1 D attack (E ; X);
2 D ﬁrst_attack (E ; Y);
3 D last_attack (E ; Y)I
1 states that attack E was used to target team X, 2 states that team Y was the ﬁrst
team to use the attack E in the training data, and similarly 3 states that team Y was
the last team to use the attack E in the training data. The ﬁrst and last attacking team
may or may not be the same. We study the following three cases:
Case 1: Non-Deceptive Attacks In non-deceptive attacks, only one team uses the
payload to target other teams in the training data. It is easy to predict the attacker
for these cases, since the search space only has one team. To model this situation,
we deﬁne a set of defeasible and strict rules.
In Fig. 5.3, defeasible rule ı1 checks whether the attack was replayed in the
training data. Since it is a non-deceptive attack, it can only be replayed by the same
team. The strict rule !1 then puts forth an argument for the attacker (culprit) if the
defeasible rule holds and there is no contradiction for it.
Case 2: Deceptive Attacks These attacks form the majority of the misclassiﬁed
samples in Chap.2. The set D is not empty for this case; let Di denote the deceptive
teams in D. We also compute the most frequent attacker from the training data
given a deceptive attack. Let the most frequent deceptive attacker be denoted as F.
The DeLP components that model this case are shown in Fig. 5.4; fact 1 indicates

5.2
Baseline Argumentation Model (BM)
79
Fig. 5.4 Facts and rules for deceptive attacks
if the attack E was deceptive towards the team X and 2 indicates the most frequent
attacker team F from the training set. The strict rule !1 indicates that in case of
deception the ﬁrst team to attack (Y) is not the attacker, !2 states that the attacker
should be F if the attack is deceptive and F was the most frequent deceptive attacker.
For the defeasible rules, ı1 deals with the case in which the attack E was replayed,
ı2 deals with the case of deceptive teams from the set D, ı3 indicates that all the
deceptive teams are likely to be the attackers in the absence of any contradictory
information. and ı4 states that the attacker should be F if the attack is deceptive and
F was the most frequent attacker.
Case 3: Previously Unseen Attacks The most difﬁcult attacks to attribute in the
dataset are the unseen ones, i.e. attacks ﬁrst encountered in the test set and thus
did not occur in the training set. To build constructs for this kind of attack we
ﬁrst compute the k nearest neighbors from the training set according to a simple
Euclidean distance between the byte and instruction histograms of the two attacks.
In this case we choose k D 3. For each of the matching attacks from the training data
we check if the attack is deceptive or non-deceptive. If non-deceptive, we follow
the procedure for Case 1, otherwise we follow the procedure for Case 2. Since we
replace one unseen attack with three seen attacks, the search space for the attacker
increases for unseen attacks.
Attacker Time Analysis The CTF data provides us with timestamps for the attacks
in the competition. We can use this information to come up with rules for/against
an argument for a team being the attacker. We compute the average time for a
team to replay its own attack given that it was the ﬁrst one to deploy the attack
(see Fig. 5.5). It can be observed that teams like More Smoked Leet Chicken (T-13)
and Wowhacker-bios (T-8) are very quick to replay their own attacks as compared
to other teams. Figure 5.5 also shows the average time for a team to perform a
deceptive attack. Teams like The European (T-7) and Blue Lotus (T-10) are quick to
commit deception, while others take more time.
We use this time information to narrow down our search space for possible
attackers. In particular, for a deceptive test sample, we compute the time difference
between the test sample and the training sample that last used the same payload.
We denote this time difference as 4t, and include it as a fact 1. We then divide the

80
5
Applying Argumentation Models for Cyber Attribution
Fig. 5.5 Average time for team to perform a deceptive attack and replay its own attack (Log-scale)
Fig. 5.6 Time facts and rules. Interval indicates a small portion of the entire deceptive time (for
instance less than 2000 s, greater than 8000 s and so on)
deceptive times from Fig. 5.5 into appropriate intervals; each team is assigned to one
of those time intervals. We then check which time interval 4t belongs to and deﬁne
a defeasible rule ı1 that makes a case for all teams not belonging to the interval to
not be the culprits, as shown in Fig. 5.6.
We now provide a summary of the experimental results—the constructs for all
test samples based on the cases discussed in the previous section are computed,
and these arguments are used as input to the DeLP engine. For each test sample,
the DeLP system is queried to ﬁnd all possible attackers (culprits) based on the
arguments provided. If there is no way to decide between contradicting arguments,
these are blocked and thus return no answers. Initially, the search space for each test
sample is 19 teams (all except the one being attacked).
After running the queries to return the set of possible culprits, the average search
space across all target teams is 5.85 teams. This is a signiﬁcant reduction in search
space across all target teams; to gauge how much the reduced search space can
aid an analyst in predicting the actual culprit, a metric is computed that checks
if the reduced search space contains the ground truth (actual culprit). For all the
target teams, the ground truth is present on average in almost 66% of the samples
with reduced search space. For some teams like More Smoked Leet Chicken (T-13)

5.3
Extended Baseline Model I (EB1)
81
and Raon_ASRT (whois) (T-17) the average reduced search space is as low as 1.82
and 2.9 teams, with high ground truth fractions of 0.69 and 0.63, respectively.
Predictive analysis is then performed on the reduced search space. The exper-
imental setup is similar to the one described earlier; the only difference this time
is instead of having a 19 team search space as in Chap. 2, the machine learning
approach is allowed to make a prediction from the reduced search space only; a
random forest is used for learning, since it was the one with the best performance
for this CTF data in our previous experiments.
We report the following average accuracies across 20 target teams; the accuracy
achieved after running random forest without applying the argumentation-based
techniques, as reported in Chap. 2, is 0.37. This was the best performing approach
using standard machine learning techniques. The baseline model achieves an
average accuracy of 0.5, which is already signiﬁcantly better. We will now explore
several ways in which this baseline argumentation model can be improved.
5.3
Extended Baseline Model I (EB1)
Previously unseen attacks make up almost 20% of the test samples for each target
team. On analyzing the misclassiﬁcation from the baseline argumentation model, we
observe that the majority of the previously unseen attacks get misclassiﬁed (more
than 80%). The misclassiﬁcations can be attributed to two reasons: (i) the reduced
search space is not able to capture the ground truth for unseen attacks, leading the
learning model to a wrong prediction; and (ii) we represent each unseen attack by
the three most similar attacks in the training data; this leads to an increase in the
search space, which translates to more choices for the learning model.
We address these issues by proposing two sets of defeasible rules. First, for each
target team we compute from the training set the top three teams that come up
with the most unique exploits, as these teams are more likely to launch an unseen
attack in the test set. The intuition behind this rule is the fact that not all teams
write their own exploits, most teams just capture a successful exploit launched by
other teams and repackage it and use it as their own (deception). The second set
of rules is proposed to avoid addition of less similar teams to the reduced search
space. In the baseline model we use 3-nearest neighbors to represent an unseen
attack. In this extended version we consider only the nearest neighbors that are less
than a particular threshold value T, which is decided for each target team separately.
So, each attack will be represented by k  3 teams depending upon the threshold
requirement. In addition to the baseline model rules, we propose the following rules
for deceptive attacks. Let U denote the set of teams with the three highest numbers
of unique attacks in the training data. Also, let N denote the set of three most
similar culprits for the given unseen attack.
The extended model is shown in Fig. 5.7; the fact 1 indicates the teams present
in N and whose similarity is less than a particular threshold T, and 2 indicates
if the team ui was one of most unique attackers from set U . For the defeasible

82
5
Applying Argumentation Models for Cyber Attribution
Fig. 5.7 Rules for previously unseen attacks
rules, ı1 makes use of the fact 1 stating that the teams in N that satisfy the
threshold condition are likely to be the culprits, and ı2 indicates that if ui is a unique
attacker then it can be the culprit unless contradictory information is available. U
is independent of the test samples and will be the same for all previously unseen
attacks given a target team.
On the contrary, for each of the similar payloads (three or fewer) computed
from the training data we check if the attack is deceptive or non-deceptive. If non-
deceptive, we follow the procedure for Case 1, otherwise we follow the procedure
for Case 2 stated in the baseline argumentation model.
Experimental Results We evaluate EB1 using an experimental setup similar to
the one for the baseline argumentation model. We report the average reduced
search space and prediction accuracy for both EB1 and baseline model to provide
a comparison. EB1 performs better than the baseline with an average accuracy
of 0.53 vs. 0.50, and signiﬁcantly better than the machine learning model without
argumentation (with an average accuracy of 0.37). The improvement in performance
is due to the larger fraction of reduced search spaces with ground truth present
in them. Also, the search space reduced from on average 6.07 teams to 5.025
(less teams to consider). The results are reported in Table 5.2, along with a
comparison with the second extended baseline argumentation model (EB2), which
is described next.
5.4
Extended Baseline Model II (EB2)
Another source of misclassiﬁcation in the baseline argumentation model is the
presence of previously unseen deceptive teams and their duplicates. These refer to
teams that did not use the exploit in the training set but started using it in the test
set. It is difﬁcult for a machine learning approach to predict such a team as being the
culprit if it has not encountered it using the exploit in the training set. In our dataset
these attacks comprise 15% of the total, and up to 20% for some teams.

5.4
Extended Baseline Model II (EB2)
83
Table 5.2 Summary of
results
Team
ML [3]
BM
EB1
EB2
T-1
0.45
0.51
0.52
0.60
T-2
0.22
0.45
0.38
0.43
T-3
0.30
0.40
0.47
0.66
T-4
0.26
0.44
0.42
0.44
T-5
0.26
0.45
0.45
0.56
T-6
0.5
0.49
0.55
0.7
T-7
0.45
0.53
0.56
0.66
T-8
0.42
0.61
0.58
0.74
T-9
0.41
0.50
0.53
0.76
T-10
0.30
0.42
0.41
0.41
T-11
0.37
0.44
0.5
0.73
T-12
0.24
0.43
0.36
0.52
T-13
0.35
0.63
0.64
0.75
T-14
0.42
0.52
0.53
0.67
T-15
0.30
0.38
0.55
0.64
T-16
0.43
0.48
0.55
0.65
T-17
0.42
0.58
0.58
0.68
T-18
0.48
0.50
0.52
0.65
T-19
0.41
0.51
0.56
0.68
T-20
0.48
0.51
0.64
0.71
The bold values in the table indicate the best performing
model for the team
Fig. 5.8 Time facts and rules. Interval indicates a small portion of the entire deceptive time (for
instance, less than 2000 s, greater than 8000 s, and so on)
In order to address this issue we propose an extension of EB1, where we group
together teams that have similar deceptive behavior based on the time information
available to us from the training set; for instance teams that are deceptive within a
certain interval of time (e.g., less than 2000 s) after the ﬁrst attack has been played
are grouped together. For a given test attack we compute the time difference between
the test attack and the last time the attack was used in the training set. We then assign
this time difference to a speciﬁc group based on which interval the time difference
falls in. In order to ﬁne tune the time intervals, instead of using the average deceptive
times averaged across all target teams (as used in the baseline model), we compute
and use deceptive times for each target team separately. We model the time rules as
stated in Fig. 5.8; fact 1 states the time difference between the test sample and the
last training sample to use that attack, defeasible rule ı1 on the other hand states that

84
5
Applying Argumentation Models for Cyber Attribution
teams belonging to that interval (in which the time difference lies) are likely to be
the culprits unless a contradiction is present. It is clear that this rule will increase
the search space for the test sample, as additional teams are now being added as
likely culprits. We observe that for EB2 the search space is increased by an average
of almost 2.5 teams per test sample from EB1; at the same time the presence of
ground truth in the reduced search space increased to 0.78, which is a signiﬁcant
improvement over 0.68.
Experimental Results We evaluate EB2 using an experimental setup similar to
the one discussed in the baseline argumentation model. We report the prediction
accuracies for each of the proposed baseline argumentation models for each of
the target teams and compare it with the previous accuracy reported in Chap. 2,
denoted as ML. In Table 5.2, the second extended baseline model (EB2) performs
the best with an average prediction accuracy of 62%, as compared to other proposed
methods. The addition of teams based on time rules not only beneﬁts detection
of unseen deceptive teams but it also helps in predicting attackers for unseen
attacks. The major reason for the jump in performance is that for most unseen
deceptive team samples, the time rules proposed in the baseline model block
all deceptive teams from being the culprit, leading to an empty set of culprits.
The new set of rules proposed in EB2 adds similar-behaving teams to this set
based on time information; the learning algorithm can then predict the right one
from this set.
5.5
Conclusions
In this chapter we demonstrated how our argumentation-based framework
(DeLP3E) can be leveraged to improve cyber attribution decisions by building
models based on CTF data that afford a reduction of the set of potential culprits and
thus greater accuracy when using a classiﬁer for cyber attribution. These ﬁrst steps
were taken using DeLP3E models with an empty EM (that is, without probabilistic
information); current and future work involves extending these models to leverage
uncertainty.
In Chap. 6 we discuss designing our own CTF event in order to better mimic
real-world scenarios. In particular, it will encourage deceptive behavior among the
participants, and we are also enhancing our instrumentation of the CTF in order to
allow for additional data collection (host data is of particular interest). This richer
data will also help build models that take uncertainty into account.
References
1. L. Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.
2. L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.
3. E. Nunes, N. Kulkarni, P. Shakarian, A. Ruef, and J. Little. Cyber-deception and attribution
in capture-the-ﬂag exercises. In Proceedings of the IEEE/ACM International Conference on
Advances in Social Networks Analysis and Mining (ASONAM), pages 962–965, 2015.

Chapter 6
Enhanced Data Collection for Cyber
Attribution
6.1
Introduction
In the aftermath of a cyberattack or breach, a natural question to ask after discovery
is “who did it?”. This question is important in a variety of contexts: to determine the
proportionate defender response, to guide law enforcement investigations, and to
perform damage assessment and risk exposure of any data lost. However, how does
an analyst assess an attribution decision? To guide the creation of our attribution
processes and systems, we require data to train on where we can have ground truth.
To help guide the creation of the attribution systems that we have described in
previous chapters by creating data usable for attribution research with believable
ground truth, we propose using computer security games similar to the Capture-
the-Flag (CTF) setting described in Chap. 2, but with more data available. Such a
framework must not only encourage contestants to obtain access to target systems,
but must also encourage them to employ stealth and deception while conducting
operations.
We created a game framework where contestants are motivated to deceive
other contestants in a computer security setting. This game framework challenges
contestants to attack containerized systems while leaving as little of a trace as
possible, and to analyze the traces left by other contestants attempting to attribute
those traces to speciﬁc contestants. We implemented this framework as a Linux
system using Docker containers and the Linux container framework to isolate users
from each other and create a game environment hosted on a single system. The
goal of this game framework is to produce data useful to cyber attribution research.
We make our platform available as open-source software. It can be downloaded
from [3].
© The Author(s) 2018
E. Nunes et al., Artiﬁcial Intelligence Tools for Cyber Attribution, SpringerBriefs in
Computer Science, https://doi.org/10.1007/978-3-319-73788-1_6
85

86
6
Enhanced Data Collection for Cyber Attribution
6.2
Goals and Design
As described in Chap. 2, the goal of a capture-the-ﬂag event is to present a game
environment that preserves the essence of the computer security ecosystem. Teams
are charged with simultaneously identifying vulnerabilities in their own systems
and patching them as well as identifying vulnerabilities in the other competitors
systems and exploiting them. Teams are unaware of the exact nature and location
of the ﬂaws they have to ﬁnd. The scoring algorithm strongly penalizes the lack of
availability of a teams services, even if those services are vulnerable. Teams must
walk a tightrope where they have to favor availability over security, monitor their
potentially vulnerable systems closely to identify attacks from other teams, and
exploit other teams without making those teams aware of their vulnerability.
DEFCON CTF [1] rewards contestants for keeping their systems available and
exploiting other teams. We want to add an additional dimension to the game: we
would like contestants to actively attempt to deceive the teams they are attacking
into believing that a different team was the source of the attack. In the existing
CTF game concept there is little reason to do this. By motivating the contestants to
employ deception, the data we gather will be more relevant to studying deception
in attribution while retaining ground truth. As the “game masters”, we can maintain
visibility of the true facts of the game, and we can contrast contestant performance
with the performance of algorithms developed for the purpose of countering
deception.
6.2.1
Changing Contestant Behavior
In a game, contestants are presented with a game environment and a winning
objective. The game environment gives the contestants a way to make incremental
progress towards the winning objective, usually by acquiring points for completing
some action. If the game designer wants the contestants to do speciﬁc actions,
they should reward those actions with points. The designer should also take care to
balance the scoring system to not create perverse incentives or to create incentives
that the designer explicitly does not want.
In traditional attack/defend CTFs like DEFCON CTF, the organizers want
contestants to reverse engineer programs, ﬁnd vulnerabilities, write exploits, and
use those exploits against other teams. They also want the other teams to present a
challenging target for the use of the exploits. To motivate these activities, the CTF
scoring system awards points to a team if that team is able to capture another teams
“ﬂag,” almost always represented as a token stored in a ﬁle or in memory.
We want to borrow this motivation to reward contestants for ﬁnding and
exploiting vulnerabilities, however we also want contestants to try and cover their
tracks or otherwise be deceptive about their origins.

6.2
Goals and Design
87
6.2.2
Game Rules
Our game is split into two distinct phases (see Fig. 6.1). In the ﬁrst phase, contestants
are given three programs and asked to identify a vulnerability in each program.
The contestants demonstrate that they have found the vulnerability by exploiting a
running instance of the program and presenting a key to the scoring infrastructure.
While they do this, a trace of the trafﬁc they send to the running programs is
collected, as well as host-level interactions that occur in the context of the vulnerable
running program. Because each instance of each running program is allocated
uniquely per team, the infrastructure is able to gather ground truth that deﬁnitively
maps the activities of an attacking team to a system under attack.
During the second phase, each contestant is asked to make three attribution
decisions. For each decision the infrastructure chooses three successful attacks. The
infrastructure knows when an attack was successful as the infrastructure knows
which key was submitted for which vulnerable program instance. To create an
attribution question for contestants, the infrastructure chooses three successful
attacks and presents the network trafﬁc associated with those attacks. For two of the
questions, the system reveals which team produced the network trafﬁc. The choice
of teams included in each question is not entirely random. The goal is to present
the contestant with two known matches and see if they can infer from the known
matches which team is matched with the unidentiﬁed trafﬁc.
Fig. 6.1 Flow of game play.
In Phase 1, players attack
network services to capture
ﬂags, while their trafﬁc is
recorded by the game world.
In Phase 2, the recordings are
used to construct questions
for contestants to answer
about which team produced
which packet capture while
attacking a program
Player 1
Program 1
Flag 1
Program 2
Flag 2
Program 3
Flag 3
Packet
capture
Captured
packets
Player 3
Captured
packets
Player 2
Captured
packets
?
Program n 
Flag n
Player i
Phase 1
Phase 2
Player 1

88
6
Enhanced Data Collection for Cyber Attribution
Each contestant has an overall score: a contestant gains ten points for each of
the three vulnerable programs they exploit during the ﬁrst phase; during the second
phase, a contestant gains ten points for each attribution question in which they are
not correctly identiﬁed, and loses ten points for each attribution question in which
they are correctly identiﬁed. Finally, they also gain ﬁve points for each correct
attribution decision they make, and lose no points for getting an attribution question
incorrect.
6.2.3
Infrastructure Design
The infrastructure is designed to allow two goals: the contestants should be able to
interact with “real” systems and hack them, while simultaneously the contestants
should not be able to break the rules of the hacking competition itself by adjusting
their score or gaining too much visibility into the activities in the game. Scalability
is also desirable: hosting the system on fewer resources makes it more economical
to run more experiments, and run them for longer periods of time.
The infrastructure uses a combination of Docker and Linux containers (lxc)
managed through the ﬁrejail utility (see Fig. 6.2).1 Each contestant is given a
unique username and password that they use to access the system; these credentials
authenticate them via PAM, however the contestant user’s shell is set to ﬁrejail.
The ﬁrejail utility, in turn, is conﬁgured to restrict the ﬁle system, process, and
network resources that a contestant user may access. This is done to keep contestants
separated from each other.
Each contestant user is statically matched to three Docker containers, each
running a different, vulnerable program. Our vulnerable programs were curated
from the Cyber Grand Challenge (CGC) Challenge Binary (CB) corpus [2]. There
are more than 100 vulnerable programs available in this corpus, so different
problems could be rotated in and out over time.
Each vulnerable binary is also assigned a unique ﬂag. The ﬂag is placed in a
ﬁle alongside the vulnerable program within the Docker container. The scoring
infrastructure recognizes and awards points for the compromise of a particular ﬂag.
This is used during the ﬁrst phase of the contest to reward points for attacking
services.
In addition to recording the trafﬁc between users ﬁrejail sessions and vulnerable
binaries, the infrastructure also uses the auditd infrastructure from Linux to record
system calls and parameters throughout the system. This is useful because it adds
an additional level of information to the contest, where that information is also
available in a real world setting. This allows attribution researchers to consider the
behavior of attackers once they have compromised a system, not just their behavior
while attempting to compromise the host by exploiting it over the network.
1https://ﬁrejail.wordpress.com/.

6.2
Goals and Design
89
Fig. 6.2 Contest
infrastructure design. Each
user is isolated both in the
network and process
visibility. Each user may only
connect to the challenges
assigned to them and, by
extension, only the ﬂags that
have been allocated to them
Player 1
Program 1
Flag 1
Program 2
Flag 2
Program 3
Flag 3
firejail
shell
Docker
containers
Player 2
Program 4
Flag 4
Program 5
Flag 5
Program 6
Flag 6
Player i
Program j
Flag j
Program k
Flag k
Program n
Flag n
6.2.4
Motivating Attribution and Deception
There are a few possible choices for scoring attribution decisions. During the
attribution phase, there are two contestants playing against each other, rather than
one contestant as in the ﬁrst phase. This is a trade-off in the design that allows for
asynchronous data collection while preserving the head to head nature of deception
and attribution. Consider the alternatives. In a traditional attack and defense CTF,
the contestants are all online at the same time and contesting the same digital
space. The interaction is much richer but the barrier to entry is much higher and
the organizational load is also higher. We strike a balance where contestants don’t
need to be online all at the same time, but there is still the possibility for deception.
This is also our effort at making attribution a ﬁrst class element of gameplay. In
other CTFs, attribution is a secondary concern to a team. When a team is scored
against, that team isn’t given a motivation to try and attribute that action to a
particular team. They might be motivated to understand the attack, if the attack
is one they have not seen before, because they would like to not be scored against
again. It would be nice to give contestants a more natural motivation for attribution,
however that could come at the cost of a more expensive contest.

90
6
Enhanced Data Collection for Cyber Attribution
6.2.5
Validity of Data
To make the resulting data more suitable for attribution research, the produced
data must match to actual intrusions. The type of data produced by this contest
matches with data that would be produced by network defenders tracing their own
networks. Network defenders can also trace their own hosts using the same auditd
infrastructure that the game infrastructure uses.
An important issue to address is: does the behavior of the contestants match
the behavior of attackers in the real world? This is partly a question of contestant
recruitment and partly a question of incentives. We have worked to try to make the
incentive structure match the real world: contestants only receive points when they
hack into a system, but they lose points if someone is able to identify them as a
result of their hacking. This matches the incentive structure in a criminal setting on
the Internet: a computer criminal only proﬁts if he attempts to hack into a system,
but if he/she can be identiﬁed and prosecuted, then he/she clearly does not proﬁt.
Contestants that compete in hacking competitions can be a fair proxy for
attackers on the Internet. Like the Internet, there is a broad spectrum. Some
contestants are students or learners of low skill, while others work professionally
and view competing in contests as part of their professional identity. As long as the
incentive structures are set up to get contestants to practice deception, the deception
behavior of contestants should have real world validity.
6.3
Conclusion
In this chapter, we presented the framework, the design and implementation of a
game framework to collect information that can be used in attribution research. The
framework is made available publicly [3].
References
1. C. Cowan, S. Arnold, S. Beattie, C. Wright, and J. Viega. Defcon capture the ﬂag: Defending
vulnerable code from intense attack.
In DARPA Information Survivability Conference and
Exposition, 2003. Proceedings, volume 1, pages 120–129. IEEE, 2003.
2. DARPA. Cyber grand challenge, 2016. http://archive.darpa.mil/cybergrandchallenge/.
3. A. Ruef, E. Nunes, P. Shakarian, and G. I. Simari. Cyber attribution game framework. 2017.
Available at https://github.com/trailofbits/attribution-vm.

Chapter 7
Conclusion
There are many challenges in the area of cyber attribution. It is easy and useful
for an actor to perform deception, hindering the decision making ability of standard
machine learning models to identify the actor as demonstrated in Chap. 2. Structured
argumentation-based frameworks like DeLP can help alleviate deception to some
extent by providing arguments for the selection of a particular actor/actors respon-
sible for the attack based on the evidence. In Chap. 5, we provided results showing
how such models afford signiﬁcant performance improvements over approaches
based solely on machine learning techniques.
An important challenge in cyber attribution is to train and evaluate the proposed
models. This book presents a framework to collect data from hacking competitions,
i.e. capture-the-ﬂag events. The framework encourages and monitors competitors as
they adopt deceptive behavior. In addition to just the network trafﬁc, the framework
is also capable of capturing host data. Other practical challenges include making
attribution decisions in real time, and scaling the proposed models to larger datasets.
Though challenging, we believe that these issues will be resolved as the research
progresses.
We may be entering a golden age of threat intelligence. Modern security
operations’ practices and frameworks allow companies to gather more and more
data from their own computers and networks. Storage costs are decreasing, allowing
for more and more security monitoring data to be retained. Furthermore, the
infosec community is becoming more aware of machine learning and other types
of statistical analyses as mechanisms to gather new insights from existing data.
However, more data doesn’t automatically enable better analysis. In this book, we
aimed to deﬁne a path by which artiﬁcial intelligence tools can enable security
practitioners to make better sense of security data in order to make more reliable
attribution decisions.
© The Author(s) 2018
E. Nunes et al., Artiﬁcial Intelligence Tools for Cyber Attribution, SpringerBriefs in
Computer Science, https://doi.org/10.1007/978-3-319-73788-1_7
91


Advances in Intelligent Systems and Computing 694
Anibal Ollero
Alberto Sanfeliu
Luis Montano
Nuno Lau
Carlos Cardeira    Editors 
ROBOT 2017: 
Third Iberian 
Robotics 
Conference
Volume 2

Advances in Intelligent Systems and Computing
Volume 694
Series editor
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl

About this Series
The series “Advances in Intelligent Systems and Computing” contains publications on theory,
applications, and design methods of Intelligent Systems and Intelligent Computing. Virtually
all disciplines such as engineering, natural sciences, computer and information science, ICT,
economics, business, e-commerce, environment, healthcare, life science are covered. The list
of topics spans all the areas of modern intelligent systems and computing.
The publications within “Advances in Intelligent Systems and Computing” are primarily
textbooks and proceedings of important conferences, symposia and congresses. They cover
signiﬁcant recent developments in the ﬁeld, both of a foundational and applicable character.
An important characteristic feature of the series is the short publication time and world-wide
distribution. This permits a rapid and broad dissemination of research results.
Advisory Board
Chairman
Nikhil R. Pal, Indian Statistical Institute, Kolkata, India
e-mail: nikhil@isical.ac.in
Members
Rafael Bello Perez, Universidad Central “Marta Abreu” de Las Villas, Santa Clara, Cuba
e-mail: rbellop@uclv.edu.cu
Emilio S. Corchado, University of Salamanca, Salamanca, Spain
e-mail: escorchado@usal.es
Hani Hagras, University of Essex, Colchester, UK
e-mail: hani@essex.ac.uk
László T. Kóczy, Széchenyi István University, Győr, Hungary
e-mail: koczy@sze.hu
Vladik Kreinovich, University of Texas at El Paso, El Paso, USA
e-mail: vladik@utep.edu
Chin-Teng Lin, National Chiao Tung University, Hsinchu, Taiwan
e-mail: ctlin@mail.nctu.edu.tw
Jie Lu, University of Technology, Sydney, Australia
e-mail: Jie.Lu@uts.edu.au
Patricia Melin, Tijuana Institute of Technology, Tijuana, Mexico
e-mail: epmelin@hafsamx.org
Nadia Nedjah, State University of Rio de Janeiro, Rio de Janeiro, Brazil
e-mail: nadia@eng.uerj.br
Ngoc Thanh Nguyen, Wroclaw University of Technology, Wroclaw, Poland
e-mail: Ngoc-Thanh.Nguyen@pwr.edu.pl
Jun Wang, The Chinese University of Hong Kong, Shatin, Hong Kong
e-mail: jwang@mae.cuhk.edu.hk
More information about this series at http://www.springer.com/series/11156

Anibal Ollero
• Alberto Sanfeliu
Luis Montano
• Nuno Lau
Carlos Cardeira
Editors
ROBOT 2017: Third Iberian
Robotics Conference
Volume 2
123

Editors
Anibal Ollero
Escuela Técnica Superior de Ingeniería
Universidad de Sevilla
Sevilla
Spain
Alberto Sanfeliu
Institut de Robòtica I Informàtica Industrial
(CSIC-UPC)
Universitat Politècnica de Catalunya
Barcelona
Spain
Luis Montano
Departamento de Informática e Ingeniería
de Sistemas, Escuela de Ingeniería
y Arquitectura
Instituto de Investigación en Ingeniería de
Aragón
Zaragoza
Spain
Nuno Lau
Institute of Electronics and Telematics
Engineering of Aveiro (IEETA)
Universidade de Aveiro
Aveiro
Portugal
Carlos Cardeira
IDMEC, Instituto Superior Técnico de
Lisboa
Universidade de Lisboa
Lisbon
Portugal
ISSN 2194-5357
ISSN 2194-5365
(electronic)
Advances in Intelligent Systems and Computing
ISBN 978-3-319-70835-5
ISBN 978-3-319-70836-2
(eBook)
https://doi.org/10.1007/978-3-319-70836-2
Library of Congress Control Number: 2017957985
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This book contains a selection of papers accepted for presentation and discussion at
“Robot 2017: Third Iberian Robotics Conference,” held in Seville, Spain,
November 22–24, 2017. Robot 2017 is part of a series of conferences that are a
joint organization of Sociedad Española para la Investigación y Desarrollo de la
Robótica/Spanish Society for Research and Development in Robotics (SEIDROB)
and Sociedade Portuguesa de Robótica/Portuguese Society for Robotics (SPR). The
conference organization had also the collaboration of several universities and
research institutes, including University of Seville, Polytechnic University of
Catalonia, University of Zaragoza, University of Aveiro, and University of Lisbon.
Robot 2017 builds upon several successful events, including three biennial
workshops (Zaragoza–2007, Barcelona–2009, and Sevilla–2011) and two Iberian
Robotics Conferences (Madrid–2013 and Lisbon–2015).
The conference is focused on the robotics scientiﬁc and technological activities
in the Iberian Peninsula, although open to research and delegates from other
countries.
Robot 2017 featured three plenary talks by:
– Oussama Khatib, Director of Stanford Robotics Lab, Stanford University, USA,
President of the International Foundation of Robotics Research (IFRR)
– Dario Floreano, Director of the Laboratory of Intelligent Systems, EPFL,
Switzerland, Director of the Swiss National Center of Competence in Robotics,
Switzerland
– Alin Albu-Schäffer, Head of the Institute of Robotics and Mechatronics at the
German Aerospace Center (DLR), Germany.
Robot 2017 featured 27 special sessions, four of them with two slots in the
Conference Program, plus 5 sessions coming from the General Track. The con-
ference had an industrial track with four sessions. The main purpose of this track is
to present industrial needs and recent achievements in robotic industrial applica-
tions looking to promote new collaborations between industry and academia. Six
papers of the industrial track have been included in this book.
v

The Special Sessions were about Aerial Robotics for Inspection, Agricultural
Robotics and Field Automation, Autonomous Driving and Driver Assistance
Systems, Challenges in Medical Robotics in the Frame of Industry 4.0, Cognitive
Architectures, Communication-Aware Robotics, Cooperative and Active Perception
for Robotics, Educational Robotics, Legged Locomotion Robots, Machine Learning
in Robotics, Marine Robotics, Ontologies and Knowledge Representation for
Robotics, Rehabilitation and Assistive Robotics, Robotics and Cyber-Physical
Systems for Industry 4.0, Robotic and Unmanned Vehicles for Security, Robot
Competitions, Robots Cooperating with Sensor Networks, Robots for Health care,
Sensor Technologies oriented to Computer Vision Applications, Simulation in
Robotics, Vision and Learning for Robotics, and Visual Perception for Robotics.
Additionally, the four industrial Special Sessions were about Application of
Robotics to Manufacturing Processes in the Aeronautic Industry, Application of
Robotics to Shipbuilding, Integration of Drones in Low Altitude Aerial Space, and
Robotics Solutions for Flexible Manufacturing.
Finally, the sessions in the General Track were about the following topics: Aerial
Robotics (double slot), Manipulation, Mobile Robotics, and Mobile Robotics
Applications.
The Robot 2017 Call for papers received 201 papers. After a careful review
process with at least three independent reviews for each paper, 141 of them have
been selected to be included in this book. There are over 500 authors from 21
countries
including
Australia,
Brazil,
Colombia,
Croatia,
Czech
Republic,
Denmark, Ecuador, Finland, France, Germany, Ireland, Italy, Luxembourg, Macao,
Mexico, Poland, Portugal, Spain, United Arab Emirates, UK, and USA.
We would like to thank all Special Sessions’ organizers for their hard work on
promoting their special session, inviting the Program Committee, organizing the
Special Session review process, and helping to promote the ROBOT 2017
Conference. This acknowledgment goes especially to the members of the Program
Committee, Organizers of the Special Sessions, and Reviewers for the hard work
required to prepare this volume as they were crucial for ensuring the high scientiﬁc
quality of the event and to all the authors and delegates whose research work and
participation made this event a success. The work of the Local Organizing
Committee was also crucial to produce the Robot 2017 Program and this book.
Last but not the least, we acknowledge and thank our editor, Springer, that was
in charge of these proceedings, and in particular to Dr. Thomas Ditzinger.
November 2017
Anibal Ollero
Alberto Sanfeliu
Luis Montano
Nuno Lau
Carlos Cardeira
vi
Preface

Organization
General/Program Chairs
Anibal Ollero
University of Sevilla, Spain
Alberto Sanfeliu
Polytechnical University of Catalonia, Spain
Luis Montano
University of Zaragoza, Spain
Nuno Lau
University of Aveiro, Portugal
Carlos Cardeira
University of Lisbon-IST, Portugal
Organizing Committee
The following members of the Organizing Committee of Robot 2017 have
contributed to the preparation of this book and its dissemination:
Alberto Sanfeliu
Polytechnical University of Catalonia, Spain
Ángel Rodríguez
University of Sevilla, Spain
Anibal Ollero
University of Sevilla, Spain
Arturo Torres
University of Sevilla, Spain
Carlos Cardeira
University of Lisbon-IST, Portugal
Carlos Rodríguez
University of Sevilla, Spain
Francisco Real
University of Sevilla, Spain
Iván Maza
University of Sevilla, Spain
Jesús Capitán
University of Sevilla, Spain
José Ángel Acosta
University of Sevilla, Spain
José Antonio Cobano
University of Sevilla, Spain
Luis Montano
University of Zaragoza, Spain
Nuno Lau
University of Aveiro, Portugal
Pablo Ramón
University of Sevilla, Spain
Pedro Sánchez
University of Sevilla, Spain
vii

Robot 2017 Program Committee
Agapito Ledezma
Espino
University Carlos III Madrid
Alexandre Bernardino
University of Lisbon, Portugal
Angel Pasqual del Pobil
University Jaume I
Alfonso Garcia Cerezo
University of Malaga, Spain
Alícia Casals
Polytechnical University of Catalonia, Spain
Anders Christensen
University Institute of Lisbon, Portugal
Andry Pinto
University of Porto, Portugal
Ángel García Olaya
University Carlos III Madrid, Spain
Aníbal Matos
University of Porto, Portugal
Antidio Viguria
FADA-CATEC, Spain
Antonio Barrientos
CAR CSIC-UPM, Spain
Antonio Jesús Bandera
University of Málaga, Spain
António Paulo Moreira
University of Porto, Portugal
António Pedro Aguiar
University of Porto, Portugal
Armando Jorge Sousa
University of Porto, Portugal
Artur Pereira
University of Aveiro, Portugal
Arturo Morgado
University of Cadiz, Spain
Bernardo Cunha
University of Aveiro, Portugal
Begoña C. Arrue
University of Sevilla, Spain
Brígida Mónica Faria
Polytechnic Institute of Porto, Portugal
Carlos Cerrada
UNED, Spain
Carlos Vazquez
Regueiro
University A Coruña, Spain
Carme Torras
Institut de Robotica i Informatica Industrial
(CSIC-UPC), Spain
Cristina Santos
University of Minho, Portugal
Danilo Tardioli
University of Zaragoza, Spain
Estela Bicho
University of Minho, Portugal
Francisco J. Rodríguez
University of Luxembourg, Luxembourg
Fernando Fernández
University Carlos III de Madrid, Spain
Fernando Lasagni
FADA-CATEC, Spain
Fernando Ribeiro
University of Minho, Portugal
Filipe Santos
University of Porto, Portugal
Filomena Soares
University of Minho, Portugal
Guillermo Heredia
University of Sevilla, Spain
Ismael García Varea
University of Castilla-La Mancha, Spain
Javier Civera
University of Zaragoza, Spain
Jesús Capitán
University of Sevilla, Spain
João Calado
Polytechnic Institute of Lisbon, Portugal
João Sequeira
University of Lisbon, Portugal
Jon Agirre Ibarbia
TECNALIA, Spain
viii
Organization

Jorge Dias
University of Coimbra, Portugal
Jorge Lobo
University of Coimbra, Portugal
Jorge Martins
University of Lisbon, Portugal
José Barata
University of Lisbon, Portugal
José A. Castellanos
University of Zaragoza, Spain
José L. Villarroel
University of Zaragoza, Spain
José Luis Azevedo
University of Aveiro, Portugal
José Luis Lima
Polytechnic Institute of Bragança, Portugal
José Luis Pons
Instituto Cajal-CSIC, Spain
José María Cañas
University Rey Juan Carlos, Spain
Josep Amat
Polytechnical University of Catalonia, Spain
Juan Andrade
Institut de Robotica i Informatica Industrial
(CSIC-UPC), Spain
Juana Mayo
University of Sevilla, Spain
Lino Marques
University of Coimbra, Portugal
Lluís Ribas Xirgo
Autonomous University of Barcelona, Spain
Luis Almeida
University of Porto, Portugal
Luis Correia
University of Lisbon, Portugal
Luis Merino
University Pablo Olavide, Spain
Luis Moreno
University Carlos III de Madrid, Spain
Luis Paulo Reis
University of Minho, Portugal
Manuel Armada
CAR CSIC-UPM, Spain
Manuel Ferre
CAR CSIC-UPM, Spain
Manuel Silva
Polytechnic Institute of Porto, Portugal
Micael Couceiro
University of Coimbra, Portugal
Miguel Oliveira
University of Porto, Portugal
Miguel A. Cazorla
University of Alicante, Spain
Nicolás García-Aracil
University of Miguel Hernández, Spain
Nuno Cruz
University of Porto, Portugal
Oscar Reinoso
University of Miguel Hernández, Spain
Pablo Bustos
University of Extremadura, Spain
Pascual Campoy
Polytechnical University of Madrid, Spain
Paulo Costa
University of Porto, Portugal
Paulo Gonçalves
Polytechnic Institute of Castelo Branco, Portugal
Paulo Jorge Oliveira
University of Lisbon, Portugal
Pedro Costa
University of Porto, Portugal
Pedro Fonseca
University of Aveiro, Portugal
Pedro Lima
University of Lisbon, Portugal
Ramiro Martinez
de Dios
University of Sevilla, Spain
Raul Cano
FADA-CATEC, Spain
Raul Morais
UTAD, Portugal
Raul Suarez
Polytechnical University of Catalonia, Spain
Rodrigo Ventura
University of Lisbon, Portugal
Rui Araújo
University of Coimbra, Portugal
Organization
ix

Rui Rocha
University of Coimbra, Portugal
Thierry Keller
TECNALIA, Spain
Vicente Feliú
University of Castilla-La Mancha, Spain
Vicente Matellán
University of León, Spain
Vitor Santos
University of Aveiro, Portugal
Special Sessions’ Organizers
Aerial Robotics for Inspection
Guillermo Heredia
University of Seville, Spain
Agricultural Robotics and Field Automation
Raul Morais Santos
UTAD, Portugal
Filipe Neves dos Santos
INESC-TEC, Portugal
Application of Robotics to Manufacturing Processes
in the Aeronautic Industry
Raúl Cano
FADA-CATEC, Spain
Application of Robotics to Shipbuilding
Arturo Morgado
Universidad de Cádiz, Spain
Autonomous Driving and Driver Assistance Systems
Vitor Santos
Universidade de Aveiro, Portugal
Angel Sappa
CVC Barcelona and ESPOL Guayaquil, Spain
Miguel Oliveira
INESC-TEC, Portugal
Arturo de la Escalera
Universidad Carlos III de Madrid, Spain
Challenges in Medical Robotics in the Frame of Industry 4.0
Josep Amat
UPC, Spain
Alicia Casals
UPC, Spain
x
Organization

Cognitive Architectures
Vicente Matellán
Universidad de León, Spain
Communication Aware Robotics
Danilo Tardioli
CUD Zaragoza, Spain
Alejandro R. Mosteo
CUD Zaragoza, Spain
Luis Riazuelo
Universidad de Zaragoza, Spain
Cooperative and Active Perception in Robotics
Eduardo Silva
ISEP/IPP, Portugal
Jesús Capitán
Universidad de Sevilla, Spain
Luis Merino
Universidad Pablo de Olavide, Spain
Educational Robotics
Armando Sousa
FEUP, Portugal
Fernando Ribeiro
Universidade do Minho, Portugal
Integration of Drones in Low Altitude Aerial Space
Antidio Viguria
FADA-CATEC, Spain
Legged Locomotion Robots
Manuel Silva
ISEP/IPP and INESC-TEC, Portugal
Cristina Santos
University of Minho and Centro ALGORITMI,
Portugal
Manuel Armada
CSIC-UPM, Spain
Machine Learning in Robotics
Guillem Aleny
Institut de Robotica Informatica Industrial
(CSIC-UPC), Spain
Brígida Mónica Faria
ESS-PP, Portugal
Luis Merino
UPO, Spain
Carme Torras
Institut de Robotica i Informatica Industrial
(CSIC-UPC), Spain
Organization
xi

Marine Robotics
A. Pedro Aguiar
FEUP, Portugal
António Pascoal
IST, Portugal
João B. Sousa
FEUP, Portugal
Fernando Lobo Pereira
FEUP, Portugal
Ontologies and Knowledge Representation for Robotics
Paulo Gonçalves
IPCB and IDMEC/Universidade de Lisboa, Portugal
Julita Bermejo-Alonso
UPM, Spain
Ricardo Sanz
Autonomous Systems Laboratory (ASLab), UPM,
Spain
Rehabilitation and Assistive Robotics
Luis Paulo Reis
University of Minho, Portugal
Brígida Mónica Faria
P.Porto, Portugal
Robotic and Unmanned Vehicles for Security
A. García-Cerezo
Universidad de Málaga, Spain
Robotics and Cyber-Physical Systems for Industry 4.0
José Lima
INESC-TEC, Portugal
Germano Veiga
INESC-TEC, Portugal
Paulo Leitão
IPB, Portugal
Robotics Solutions for Flexible Manufacturing
Jon Agirre Ibarbia
Tecnalia Research & Innovation, Spain
Robot Competitions
Bernardo Cunha
University of Aveiro, Portugal
José Luis Azevedo
University of Aveiro, Portugal
xii
Organization

Robots Cooperating with Sensor Networks
J. Ramiro Martinez
de Dios
University of Sevilla, Spain
Robots for Health Care
Raquel Fuentetaja
Pizán
Universidad Carlos III de Madrid, Spain
Angel García Olaya
Universidad Carlos III de Madrid, Spain
Sensor Technologies Oriented to Computer Vision Applications
Carlos Cerrada
UNED, Spain
Simulation in Robotics
Luis Paulo Reis
University of Minho, Portugal
Artur Pereira
University of Aveiro, Portugal
Vision and Learning for Robotics
Miguel Cazorla
Universidad de Alicante, Spain
Oscar Reinoso
Universidad Miguel Hernández, Spain
Visual Perception for Robotics
B.C. Arrue
University of Sevilla, Spain
Sessions Program Committee
Aamir Ahmad
Abdelghani Chibani
Abdulla Al-Kaff
Ahmed Hussein
Alberto Olivares-Alarcos
Alberto Sanfeliu
Alejandro Linares-Barranco
Alejandro R. Mosteo
Alejandro Suarez
Alexandre Bernardino
Alfonso García-Cerezo
Alicia Casals
Alicja Wasik
Ana Lopes
Organization
xiii

Anais Garrell
Anders Christensen
Andre Dias
Andrea Alessandretti
Andry Pinto
Angel Garcia Olaya
Ángel Manuel Guerrero-Higueras
Angela Ribeiro
Angelos Amanatiadis
Anthony Mandow
Antidio Viguria
Antonio Adan
Antonio Agudo
Antonio Bandera
Antonio Barrientos
Antonio Morales
António Neves
Antonio Paulo Coimbra
António Paulo Moreira
Antonio Valente
Armando Sousa
Artur Pereira
Arturo Bertomeu-Motos
Arturo Morgado
Arturo Torres-González
Augusto Tavares
Begoña C. Arrue
Belén Curto
Benoit Landry
Brígida Mónica Faria
Bruno J.N. Guerreiro
Camino Fernández
Cándido Otero
Carlos Cardeira
Carlos Hernández Corbato
Carlos Miguel Costa
Carlos Rizzo
Carlos Vázquez Regueiro
Carme Torras
Claudio Rossi
Clauirton Siebra
Craig Schlenoff
Daniel Gutierrez Galan
Daniel Silva
Danilo Tardioli
David Alejo
David Valiente
Dimitrios Paraforos
Domenec Puig
Denes Nagy
Duarte Valério
Edson Prestes
Eduardo Ferrera
Eduardo Mendes
Eduardo Molinos Vicente
Enrique Valero
Estela Bicho
Eugenio Aguirre Molina
Eugenio Marinetto
Eurico Pedrosa
Felipe Jimenez
Fernando Caballero
Fernando Fernández
Fernando Garcia
Fernando Gomez-Bravo
Fernando Ribeiro
Filipe Neves Dos Santos
Filomena Soares
Francisco Alarcón
Francisco Bonnín
Francisco Gomez-Donoso
Francisco J. Rodríguez Lera
Francisco Javier Badesa
Francisco Madrigal
Francisco Martín Rico
Francisco Ramos
Francisco Rodríguez-Sedano
Frederic Lerasle
Gary Philippe Cornelius
Gil Lopes
Guillermo Heredia
Iñaki Diaz
Ismael Garcia-Varea
Ivan Armuelles
Ivan Maza
J.M. Aguilar
Jaime Duque Domingo
Javier Perez Turiel
Jesús Balsa
Jesus Capitan
xiv
Organization

Jesus Fernandez-Lozano
Joao Calado
Joao Fabro
João Macedo
João Sequeira
Joel Carbonera
Jon Agirre Ibarbia
Jorge De León Rivas
Jorge Díez
Jorge Lobo
Jorge Martins
Jose A. Castellanos
José Antonio Cobano
José Barbosa
José Carlos Rangel
Jose Eugenio Naranjo
Jose García Rodríguez
Jose Javier Anaya
Jose Joaquin Acevedo
Jose Lima
José Luis Villarroel
José M. Cogollor
José Manuel Cañas
José María Armingol
José María Cañas Plaza
José María Catalán Orts
Josenalde Oliveira
Josep Amat
Joshue Pérez
Josu Larrañaga Leturia
Juan Andrade Cetto
Juan Barios
Juan Braga
Juan Jesús Roldán Gómez
Juan Pedro Bandera Rubio
Julián Estévez
Julita Bermejo-Alonso
Justyna Gerłowska
Leopoldo Rodriguez
Lluis Ribas Xirgo
Luis Almeida
Luis Correia
Luis Costa
Luis Evaristo Caraballo de La Cruz
Luis Garrote
Luis Gomes
Luis Merino
Luis Montano
Luis Moreno
Luis Oliveira
Luis Paulo Reis
Luis Riazuelo
Luis Rocha
Manuel Armada
Manuel Ferre
Manuel Garcia
Manuel Jesus Dominguez Morales
Manuel Pérez
Manuel Silva
Marcelo Borges Nogueira
Marcelo Petry
Marco Ceccarelli
Margarita Bachiller
Martin Rico Francisco
Micael Couceiro
Miguel Ángel Conde
Miguel Angelo Sotelo
Miguel Cazorla
Miguel Trujillo
Mohammad Rouhani
Natalia Ayuso
Nicolas García Aracil
Nicolas Jouandeau
Nieves Pulido
Noé Pérez-Higueras
Nuno Cruz
Nuno Lau
Oscar Lima
Oscar Reinoso
Oscar Sipele
P. Javier Herrera
Pablo Bustos
Pablo Jiménez
Pablo Ramon Soria
Paulo Costa
Paulo Dias
Paulo Gonçalves
Paulo Menezes
Paulo Oliveira
Paulo Peixoto
Organization
xv

Pedro Costa
Pedro Fonseca
Pedro Lima
Pedro Neto
Pedro Pedro Costa
R. Praveen Kumar Jain
Rafael Barea
Ramiro Martinez de Dios
Raquel Fuentetaja
Raul Cano
Raul Morais Santos
Raul Suarez
Ricardo Pascoal
Ricardo Sanz
Rita Cunha
Robert Fitch
Roberto Arroyo
Rui Araujo
Rui Gomes
Rui Rocha
Sedat Dogru
Stephen Balakirsky
Tiago Nascimento
Valter Costa
Veera Ragavan
Vicente Feliú
Vicente Matellán
Víctor Vaquero
Vítor Carvalho
Vitor Santos
Yaroslav Marchukov
xvi
Organization

Contents
Robotic Solutions for Flexible Manufacturing
Full Production Plant Automation in Industry Using Cable
Robotics with High Load Capacities and Position Accuracy . . . . . . . . .
3
David Culla, Jose Gorrotxategi, Mariola Rodríguez, Jean Baptiste Izard,
Pierre Ellie Hervé, and Jesús Cañada
Human-Robot Collaboration and Safety Management
for Logistics and Manipulation Tasks . . . . . . . . . . . . . . . . . . . . . . . . . .
15
Gi Hyun Lim, Eurico Pedrosa, Filipe Amaral, Ricardo Dias,
Artur Pereira, Nuno Lau, José Luís Azevedo, Bernardo Cunha,
and Luis Paulo Reis
Grasp Quality Measures for Transferring Objects . . . . . . . . . . . . . . . . .
28
Fernando Soler, Abiud Rojas-de-Silva, and Raúl Suárez
Application of Robotics in Shipbuilding
Development of a Customized Interface for a Robotic Welding
Application at Navantia Shipbuilding Company. . . . . . . . . . . . . . . . . . .
43
Pedro L. Galindo, Arturo Morgado-Estévez, José Luis Aparicio,
Guillermo Bárcena, José Andrés Soto-Núñez, Pedro Chavera,
and Francisco J. Abad Fraga
Towards Automated Welding in Big Shipbuilding Assisted
by Programed Robotic Arm Using a Measuring Arm . . . . . . . . . . . . . .
53
Arturo Morgado-Estevez, Pedro L. Galindo,
Jose-Luis Aparicio-Rodriguez, Ignacio Diaz-Cano, Carlos Rioja-del-Rio,
Jose A. Soto-Nuñez, Pedro Chavera, and Francisco J. Abad-Fraga
xvii

Cognitive Architectures
Cybersecurity in Autonomous Systems: Hardening ROS
Using Encrypted Communications and Semantic Rules . . . . . . . . . . . . .
67
Jesús Balsa-Comerón, Ángel Manuel Guerrero-Higueras,
Francisco Javier Rodríguez-Lera, Camino Fernández-Llamas,
and Vicente Matellán-Olivera
Triaxial Sensor Calibration: A Prototype for Accelerometer
and Gyroscope Calibration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
P. Bernal-Polo and H. Martínez-Barberá
Automatic Characterization of Phase Resetting Controllers
for Quick Balance Recovery During Biped Locomotion . . . . . . . . . . . . .
91
Julián Cristiano, Domènec Puig, and Miguel Angel García
Interface Design of Haptic Feedback on Teleoperated System . . . . . . . .
102
Francisco Rodríguez-Sedano, Pere Ponsa, Pablo Blanco-Medina,
and Luis Miguel Muñoz
Machine Learning in Robotics
Deep Networks for Human Visual Attention: A Hybrid Model
Using Foveal Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
Ana Filipa Almeida, Rui Figueiredo, Alexandre Bernardino,
and José Santos-Victor
Mixed-Policy Asynchronous Deep Q-Learning . . . . . . . . . . . . . . . . . . . .
129
David Simões, Nuno Lau, and Luís Paulo Reis
Reward-Weighted GMM and Its Application to Action-Selection
in Robotized Shoe Dressing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
Adrià Colomé, Sergi Foix, Guillem Alenyà, and Carme Torras
Pose Invariant Object Recognition Using a Bag
of Words Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
Carlos M. Costa, Armando Sousa, and Germano Veiga
Tactile Sensing and Machine Learning for Human and Object
Recognition in Disaster Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
Juan M. Gandarias, Jesús M. Gómez-de-Gabriel,
and Alfonso J. García-Cerezo
Robots Cooperating with Sensor Networks
Autonomous Localization of Missing Items with Aerial Robots
in an Aircraft Factory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
Julio L. Paneque, Arturo Torres-González, J. Ramiro Martínez-de Dios,
Juan Ramón Astorga Ramírez, and Anibal Ollero
xviii
Contents

Wireless Sensor Networks for Urban Information Systems:
Preliminary Results of Integration of an Electric Vehicle
as a Mobile Node . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
J.J. Fernández-Lozano, J.A. Gomez-Ruiz, Miguel Martín-Guzmán,
Juan Martín-Ávila, Socarras Bertiz Carlos, and A. García-Cerezo
Design of a Robot-Sensor Network Security Architecture
for Monitoring Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
200
Francisco J. Fernández-Jiménez and J. Ramiro Martínez-de Dios
A Robust Reach Set MPC Scheme for Control of AUVs . . . . . . . . . . . .
213
Rui Gomes and Fernando Lobo Pereira
Sensor Technologies Oriented to Computer Vision Applications
Obtaining and Monitoring Warehouse 3D Models with Laser
Scanner Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
Antonio Adán, David de la Rubia, and Andrés S. Vázquez
3D Monitoring of Woody Crops Using a Medium-Sized Field
Inspection Vehicle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239
José M. Bengochea-Guevara, Dionisio Andújar,
Francisco L. Sanchez-Sardana, Karla Cantuña, and Angela Ribeiro
A Vision-Based Strategy to Segment and Localize Ancient
Symbols Written in Stone. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
Jaime Duque-Domingo, P. Javier Herrera, Carlos Cerrada,
and José A. Cerrada
Robot Competitions
euRathlon and ERL Emergency: A Multi-domain Multi-robot
Grand Challenge for Search and Rescue Robots . . . . . . . . . . . . . . . . . .
263
Alan F.T. Winﬁeld, Marta Palau Franco, Bernd Brueggemann,
Ayoze Castro, Gabriele Ferri, Fausto Ferreira, Xingcun Liu,
Yvan Petillot, Juha Roning, Frank Schneider, Erik Stengler,
Dario Sosa, and Antidio Viguria
Autonomous Landing of a Multicopter on a Moving Platform
Based on Vision Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
272
José Joaquín Acevedo, Manuel García, Antidio Viguria, Pablo Ramón,
Begoña C. Arrue, and Anibal Ollero
3D Mapping for a Reliable Long-Term Navigation . . . . . . . . . . . . . . . .
283
Jonathan Ginés, Francisco Martín, Vicente Matellán, Francisco J. Lera,
and Jesús Balsa
Contents
xix

A Lightweight Navigation System for Mobile Robots . . . . . . . . . . . . . . .
295
M.T. Lázaro, G. Grisetti, L. Iocchi, J.P. Fentanes, and M. Hanheide
Visual Perception for Robotics
Bridge Mapping for Inspection Using an UAV Assisted
by a Total Station . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
Javier Prada Delgado, Pablo Ramon Soria, B.C. Arrue, and A. Ollero
Multi-view Probabilistic Segmentation of Pome Fruit
with a Low-Cost RGB-D Camera. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
320
Pablo Ramon Soria, Fouad Sukkar, Wolfram Martens, B.C. Arrue,
and Robert Fitch
Vision-Based Deﬂection Estimation in an Anthropomorphic,
Compliant and Lightweight Dual Arm . . . . . . . . . . . . . . . . . . . . . . . . . .
332
Alejandro Suarez, Guillermo Heredia, and Anibal Ollero
3D Navigation for a Mobile Robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
Jan Škoda and Roman Barták
Educational Robotics
Robobo: The Next Generation of Educational Robot . . . . . . . . . . . . . . .
359
Francisco Bellas, Martin Naya, Gervasio Varela, Luis Llamas,
Moises Bautista, Abraham Prieto, and Richard J. Duro
The ROSIN Education Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
370
Alexander Ferrein, Stefan Schiffer, and Stephan Kallweit
Mobile Robots as a Tool to Teach First Year
Engineering Electronics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
382
Pedro Fonseca, Paulo Pedreiras, and Filipe Silva
Methodology and Results on Teaching Maths Using Mobile Robots . . .
394
Paola Ferrarelli, Tamara Lapucci, and Luca Iocchi
Autonomous Driving and Driver Assistance Systems (I)
Application of Sideslip Estimation Architecture to a Formula
Student Prototype. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
409
André Antunes, Carlos Cardeira, and Paulo Oliveira
Torque Vectoring for a Formula Student Prototype . . . . . . . . . . . . . . .
422
João Antunes, Carlos Cardeira, and Paulo Oliveira
Path and Velocity Trajectory Selection in an Anticipative
Kinodynamic Motion Planner for Autonomous Driving . . . . . . . . . . . . .
434
Jordi Pérez Talamino and Alberto Sanfeliu
xx
Contents

Deadzone-Quadratic Penalty Function for Predictive Extended
Cruise Control with Experimental Validation . . . . . . . . . . . . . . . . . . . .
446
Seyed Amin Sajadi-Alamdari, Holger Voos, and Mohamed Darouach
Autonomous Driving and Driver Assistance Systems (II)
Comparative Study of Visual Odometry and SLAM Techniques . . . . . .
463
Ana Rita Gaspar, Alexandra Nunes, Andry Pinto, and Anibal Matos
Real-Time Deep ConvNet-Based Vehicle Detection
Using 3D-LIDAR Reﬂection Intensity Data . . . . . . . . . . . . . . . . . . . . . .
475
Alireza Asvadi, Luis Garrote, Cristiano Premebida, Paulo Peixoto,
and Urbano J. Nunes
Modeling Trafﬁc Scenes for Intelligent Vehicles
Using CNN-Based Detection and Orientation Estimation . . . . . . . . . . . .
487
Carlos Guindel, David Martín, and José María Armingol
Complete ROS-based Architecture for Intelligent Vehicles. . . . . . . . . . .
499
Pablo Marin-Plaza, Ahmed Hussein, David Martin,
and Arturo de la Escalera
Challenges in Medical Robotics in the Frame of Industry 4.0
Health 4.0 Oriented to Non-surgical Treatment . . . . . . . . . . . . . . . . . . .
513
Carles Soler
Collaborative Robots for Surgical Applications . . . . . . . . . . . . . . . . . . .
524
Álvaro Bertelsen, Davide Scorza, Camilo Cortés, Jon Oñativia,
Álvaro Escudero, Emilio Sánchez, and Jorge Presa
New Technologies in Surgery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
536
Alícia Casals, Narcís Sayols, and Josep Amat
Collaborative Robotic System for Hand-Assisted
Laparoscopic Surgery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
548
Carmen López-Casado, Enrique Bauzano, Irene Rivas-Blanco,
Víctor F. Muñoz, and Juan C. Fraile
Rehabilitation and Assistive Robotics
Mechanical Design of a Novel Hand Exoskeleton Driven
by Linear Actuators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
557
Jorge A. Díez, Andrea Blanco, José M. Catalán, Arturo Bertomeu-Motos,
Francisco J. Badesa, and Nicolás García-Aracil
Robotic Platform with Visual Paradigm to Induce Motor
Learning in Healthy Subjects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
569
Guillermo Asín-Prieto, José E. González, José L. Pons,
and Juan C. Moreno
Contents
xxi

A Protocol Generator Tool for Automatic In-Vitro HPV
Robotic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
580
Juan Pedro Dominguez-Morales, Angel Jimenez-Fernandez,
Saturnino Vicente-Diaz, Alejandro Linares-Barranco,
Asuncion Olmo-Sevilla, and Antonio Fernandez-Enriquez
Robotics and Cyber-Physical Systems for Industry 4.0 (I)
End-Effector Precise Hand-Guiding for Collaborative Robots . . . . . . . .
595
Mohammad Safeea, Richard Bearee, and Pedro Neto
Integrating 3D Reconstruction and Virtual Reality:
A New Approach for Immersive Teleoperation . . . . . . . . . . . . . . . . . . .
606
Francisco Navarro, Javier Fdez, Mario Garzón, Juan Jesús Roldán,
and Antonio Barrientos
Enhancement of Industrial Logistic Systems with Semantic
3D Representations for Mobile Manipulators . . . . . . . . . . . . . . . . . . . . .
617
César Toscano, Rafael Arrais, and Germano Veiga
Human Intention Recognition in Flexible Robotized Warehouses
Based on Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . .
629
Tomislav Petković, Ivan Marković, and Ivan Petrović
Robotics and Cyber-Physical Systems for Industry 4.0 (II)
Dynamic Collision Avoidance System for a Manipulator Based
on RGB-D Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
643
Thadeu Brito, Jose Lima, Pedro Costa, and Luis Piardi
Development of a Dynamic Path for a Toxic Substances Mapping
Mobile Robot in Industry Environment . . . . . . . . . . . . . . . . . . . . . . . . .
655
Luis Piardi, José Lima, Paulo Costa, and Thadeu Brito
Poses Optimisation Methodology for High Redundancy
Robotic Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
668
Pedro Tavares, Pedro Costa, Germano Veiga
and António Paulo Moreira
Ofﬂine Programming of Collision Free Trajectories
for Palletizing Robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
680
Ricardo Silva, Luís F. Rocha, Pedro Relvas, Pedro Costa,
and Manuel F. Silva
Manipulation
Estimating Objects’ Weight in Precision Grips
Using Skin-Like Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
695
Francisco Azevedo, Joana Carmona, Tiago Paulino, and Plinio Moreno
xxii
Contents

Kinematic Estimator for Flexible Links in Parallel Robots . . . . . . . . . .
704
Pablo Bengoa, Asier Zubizarreta, Itziar Cabanes, Aitziber Mancisidor,
and Charles Pinto
Tactile-Based In-Hand Object Pose Estimation . . . . . . . . . . . . . . . . . . .
716
David Álvarez, Máximo A. Roa, and Luis Moreno
Legged Locomotion Robots
Study of Gait Patterns for an Hexapod Robot in Search
and Rescue Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
731
Jorge De León, Mario Garzón, David Garzón-Ramos,
and Antonio Barrientos
A Hybrid ZMP-CPG Based Walk Engine for Biped Robots . . . . . . . . .
743
S. Mohammadreza Kasaei, David Simões, Nuno Lau, and Artur Pereira
Modelling, Trajectory Planning and Control of a Quadruped
Robot Using Matlab®/Simulink™. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
756
Italo Oliveira, Ramiro Barbosa, and Manuel Silva
Communication-Aware Robotics (I)
Cooperative Perimeter Surveillance Using Bluetooth Framework
Under Communication Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . .
771
J.M. Aguilar, Pablo Ramon Soria, B.C. Arrue, and A. Ollero
Development of an Adaptable Communication Layer with QoS
Capabilities for a Multi-Robot System . . . . . . . . . . . . . . . . . . . . . . . . . .
782
Hannes Harms, Julian Schmiemann, Jan Schattenberg
and Ludger Frerichs
Trajectory Planning Under Time-Constrained Communication . . . . . . .
794
Yaroslav Marchukov and Luis Montano
Balancing Packet Delivery to Improve End-to-End Multi-hop
Aerial Video Streaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
807
Luis Ramos Pinto, Luis Almeida, and Anthony Rowe
Communication-Aware Robotics (II)
Discrete Robot Localization in Tunnels . . . . . . . . . . . . . . . . . . . . . . . . .
823
Teresa Seco, Carlos Rizzo, Jesús Espelosín, and José Luis Villarroel
Low-Bandwidth Telerobotics in Fading Scenarios . . . . . . . . . . . . . . . . .
835
Samuel Barrios, Natalia Ayuso, Danilo Tardioli, Luis Riazuelo,
Alejandro R. Mosteo, Francisco Lera, and José Luis Villarroel
Contents
xxiii

Wireless Propagation Characterization of Underground Sewers
Towards Autonomous Inspections with Drones . . . . . . . . . . . . . . . . . . .
849
Carlos Rizzo, Pedro Cavestany, François Chataigner,
Marcel Soler, German Moreno, Daniel Serrano, Francisco Lera,
and Jose Luis Villarroel
Author Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
861
xxiv
Contents

Robotic Solutions for Flexible
Manufacturing

Full Production Plant Automation in Industry Using Cable
Robotics with High Load Capacities and Position Accuracy
David Culla1(✉), Jose Gorrotxategi1, Mariola Rodríguez1, Jean Baptiste Izard2,
Pierre Ellie Hervé2, and Jesús Cañada3
1 Tecnalia Research and Innovation, Parque Cientíﬁco y Tecnológico de Gipuzkoa,
Mikeletegi Pasealekua 7, 20009 Guipúzcoa, Spain
david.culla@tecnalia.com
2 Tecnalia Francia, CSU Building, Bât 6, 950 Rue Saint Priest, 34090 Montpellier, France
3 Cemvisa Vicinay, c/Zelaietabide, 1, 48210 Otxandio, Bizkaia, Spain
Abstract. The aim of this paper is to introduce an innovative machinery result
of combining cable suspended robot technology based on parallel kinematics with
a traditional gantry crane. This machinery has been named Cablecrane. Its
purpose is to keep the same load capabilities as in traditional gantry cranes while
enabling full 6 degrees of freedom (DOF) control of the payload.
This means that the payload is fully controlled in position and orientation
while it is being manipulated. Thus, precision load handling and movement
without oscillations are possible in any direction, in any orientation.
In addition, combined with appropriate calibration, sensors and integrated
CNC controller, most of manipulation tasks in plant can be programmed and
automatized.
The ﬁnal result is an increase in production, full plant automation and
enhanced plant safety.
Keywords: Cablecrane · Industrial cable robot · Cable robotics · Parallel
kinematics · Plant automation · Factory automation
1
Introduction to Large Scale Full Plant Automation
In any industry factory or plant, the handling and assembly of large and heavy compo‐
nents or machines are two of the most demanding tasks that have to be carried out. These
tasks do not only involve moving objects from one position, but also relatively precise
positioning of the objects in particular spots or locations. The correct placement of an
object involves both spatial position and orientation, that is, angular position. This means
that three diﬀerent geometrical coordinates and three diﬀerent angles have to be consid‐
ered. Due to the physical characteristics of the often very large and heavy objects to be
handled, substantial amounts of technical skills or dexterity are often required to the
operators for the correct adjustment of the six degrees of freedom (DOF).
In addition, in many occasions fully automated movement is also desirable.
Currently, small scale automation activities in plant are carried out by anthropomorphic
robots distributed in cells. Recently, gantry robots have been successfully in large
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_1

gantries [9]. However, due to limitations such as the reduced workspace of commercial
robots, or high cost of bigger size gantry robots, large scale automation of heavy loads
with full load control is out of reach. Only manual manipulation is still done with tradi‐
tional cranes while the operator takes care not to swivel the load.
Thus, at least two diﬀerent issues have to be dealt with in the context of handling
and assembly:
• The importance of suﬃcient hoist capacity and adequate positioning. This is clear in
the case of, for example, the transport and assembly of wind turbines, airplanes,
machine tools, heavy structures for buildings, heavy equipment, among others.
• Automation of operations throughout the production plant.
Cablecrane (Fig. 1) system makes possible large scale automation along the plant
combining parallel cable-driven robotics embedded in a traditional gantry crane. This
provides the best cost eﬀective solution, automated and manually controlled, over most
of the handling and production processes in plant with full load position and orientation
control all over the plant.
Automated plant
CABLECRANE System
Fig. 1. Cablecrane system installed in a factory plant
The impact on the working conditions is highly positive as it guarantees the safety
in handling, reduces setting time after movements, increases productivity by 50% and
reduces cycle time as well as work accidents.
2
System Description
Cablecrane system has three parts clearly identiﬁable (Fig. 2):
• Cable driven parallel robot (CDPR).
• Automated gantry crane.
• Cable driven mobile platform.
4
D. Culla et al.

Gantry Crane
CDPR – winches 1 to 4
CDPR – winches 5 to 8
Cable driven mobile platform
Fig. 2. Main components of Cablecrane
Figure 3 shows Cablecrane system assembled on ﬂoor before installation in produc‐
tion facility. Commissioning will be carried out by December 2017.
Fig. 3. Cablecrane main gantry crane and motors
2.1
CDPR, Cable Robot
This sub-system is formed by 8 servomotors installed in the gantry crane. Each servo‐
motor controls the length of a cable that pulls a cable driven platform on its end. The
length that each motor collects or releases is synchronized with the rest of the motors
according to parallel kinematics design. Therefore, by controlling the cable lengths and
solving the kinematics, the desired position in the plant of the cable-driven platform
workspace is reached at any time.
Full Production Plant Automation in Industry
5

Besides, there is a set of pulleys (Fig. 4) that direct each cable from the drum towards
an exit point from which it connects with the attachment point in the platform. The
coordinates of these last two points as well as the way they are connected have been
deﬁned and optimized in order to maximize load capacities, increase workspace size
and decrease interferences between cables, platforms, loads and crane following opti‐
mization procedures [1, 3].
A
B
C
D
Fig. 4. Pulleys used to direct cables. Cable routing is A-B-C-D.
Fig. 5. Winch view
It is also important to minimize the number of pulleys while avoiding singularities
between pulleys and cables. On one hand, it is the cable, while it changes length, that
pulls and orients the pulley. Therefore, low amount of contact between cable and pulley
will produce sudden movements and vibrations because the pulley does not follow the
cable orientation smoothly. On the other hand, high number of pulleys reduces the life
of the cable.
Industrial design has been implemented in cable, drum and pulley design, following
ISO 4301/FEM 1001 and ISO 16625/FEM 1001.
6
D. Culla et al.

2.2
Automated Gantry Crane
The gantry crane has two functions:
• Act as a mobile supporting frame of the cable robot, displacing it to any place and
thus, extending the workspace all over the plant.
• Withstand most of the payload as any traditional crane.
These two functions are achieved, respectively, thanks to:
• All motors present in the crane are servomotors. Additional sensors are installed in
order to control the absolute position of the crane in plant: lasers are used to control
the global position of the crane along the plant. Inductive sensors close to the crane
wheels are used to keep crane orientation parallel to the rails used for displacement.
• Installation of a secondary gantry embedded in main gantry (Fig. 6). This secondary
gantry moves an additional winch that drives its own cable. This 9th cable is used to
withstand most of the load as in traditional cranes. Its innovation is that this cable is
force controlled. Thus, the load in each cable is distributed in a way that almost all
of it is held by the vertical cable. The rest of the load is left in order to keep minimum
required tension in the cables that form the cable robot. This allows them to keep the
platform controlled while it is being held in the most eﬃcient way.
• The pull of the 9th cable must always be vertical with respect to the platform to keep
it balanced in combination with the rest of the cables. In order to do so, the secondary
gantry installed in the main gantry has its own rails, servomotors and sensors used
to follow the planar (XY) position of the platform (Fig. 7). These allow for relative
x and y axes movement with respect to the main gantry crane.
Additional vertical cable, 
used to lifth heavy loads
while keeping tension in 
the rest of the cables
Fig. 6. Vertical cable can be used when payloads are high.
Full Production Plant Automation in Industry
7

x
Secondary gantry
y
x displacement
y displacement
Fig. 7. Secondary gantry is used to follow platform’s XY position.
2.3
Cable Driven Platform
The cable driven mobile platform (Fig. 8) acts in a factory plant as a large scale fully
automated end-eﬀector tool. Its design, based on parallel kinematics, is such that allows
installation of diﬀerent kind of grippers/manipulators in its lower part. Cables are
attached using industrial elevation rings so that they can be freely oriented while lengths
change.
Fig. 8. View of the platform (CAD and pre-assembly).
The end eﬀector of Cablecrane is characterized by:
• Large workspaces covered in a fully automated manner.
• High load capacity.
• Accurate position of any kind of tooling along a wide workspace withstanding
external loads.
8
D. Culla et al.

• Full control. They can manage 6 degrees of freedom.
• Highly productive. Their response time is low and can move around at a high speed.
• Versatile and multi-tasking.
• Low maintenance consumer.
• Low space user. Winches are installed in the gantry crane and robot architecture does
not need ﬂoor space to be stored.
• No swinging of the load: parts ﬁrmly held by 8 cables coming from diﬀerent direc‐
tions.
3
CDPR Deﬁnition Methodology
3.1
Workspace Deﬁnition
One key issue for the deﬁnition of the CDPR in Cablecrane is to deﬁne the coordinates
where the cables must be attached to the platform and where the pulleys have to be
placed. This is not an easy to predict task, especially if workspace is to be maximized,
and so the achievable turns. These coordinates are required to verify and compute the
main limitations to be considered [1, 2] in the design:
• Cable tension values must always be positive and smaller than a maximum allowable
value.
• Cable interferences must be avoided. These interferences can be between cables and/
or objects such as the platform, pulleys, gantry crane, etc.…
• Cable tension must compensate external loads within a given range [4].
• Cable mass and elongation aﬀect to the feasible workspace by modifying platform
balance [10–12].
Traditional CAD systems, although necessary in the design, are not enough by its
own to produce an optimal design in an eﬀective manner, as they do not consider solving
force systems with the conditions present in cable robotics. This is required because
each of the positions that the platform will reach in the space needs to be veriﬁed in
terms of collisions but also in terms of force balances. Introducing mathematical calcu‐
lation interference and force balance solving systems in a script tool makes the analysis
of feasible workspace for a given set of coordinates much faster (Figs. 9 and 10).
Besides, the coordinate deﬁnition for each cable exit point in pulleys and connecting
point in platform is usually submitted to a mathematical optimization [3] which target
can be to minimize cable tensions, motor consumptions, or any other criteria. Therefore,
it is required to develop optimization software tools in order to improve the design.
Tecnalia has developed its own tools for the last years. In the case of Cablecrane design,
the tools have been used to maximize workspace size and turns in z axis while keeping
good turn amplitude in x and y axis.
Full Production Plant Automation in Industry
9

3.2
Mechanical Design Calculation
From a classic mechanical design point of view, special attention needs to be payed to
platform’s and gantry crane’s mechanical design. In fact, for every position and orien‐
tation in the workspace:
Valid workspace considers cable collision points and positive cable tension solution
Cable-cable collision
Cable-platform collision
Fig. 9. Valid workspace considers cable collision points and positive cable tension solution.
Fig. 10. Cable tensions are calculated at each point, negative values are discarded.
10
D. Culla et al.

• Loads are changing continuously in direction, which makes complicate to determine
the most critical design cases.
• Structures need to be stiﬀened to keep cable exit points with the smallest deformations
as possible in order to guarantee the best precision positioning of the platform.
The classic approach is to use ﬁnite element method software to solve and verify the
most critical load cases. However, it is ﬁrst required to deﬁne of the most critical load
cases. With this purpose, the stiﬀness matrix of the CDPR was ﬁrst used as input to a
secondary tool that automatically pre-calculated deformation and tensions for each point
and orientation of the workspace considering the cable force application direction and
values at each of these workspace points.
Once these critical points were sorted and identiﬁed, FEM was used again for ﬁnal
mechanical design.
4
Feasible Workspace
The result of the design features a cable collision free workspace of 8 × 3.2 × 2 m with
a standard payload of up to 3000 kg (Fig. 11). Turn capacities are 40, 20 and 90º around
x, y, z axis respectively. The total volume required by the crane is 9 × 7.5 × 4.5 m.
Workspace of the cable robot
8 x 3.2 x 3 [m] 
Worskpace extends 
all over plant
thanks to gantry crane
Fig. 11. Available workspace.
Since all the servomotors are installed in the gantry crane, these workspace capa‐
bilities are extendable all along the reach of the gantry crane in plant.
5
Control System Improvement
Getting a full automated and precise control system has required to implement the main
modiﬁcations described in the following.
Full Production Plant Automation in Industry
11

First, the total cable length from each winch to the platform attachment point needs
to be precisely measured. Calibration is required to determine pulley positions, orien‐
tations and ﬁnal cable routing. This determines the total length of cable for each platform
position. The total cable length is used then to compute cable elongation and correct the
amount of cable to be reeled in or reeled out in each winch. Obviously, cable stress
deformation tests (ISO12076) have been carried out previously to get the elongation
model. Best ﬁt between strain 𝜖 and tensile force F has been found to be an oﬀset loga‐
rithmic squared curve with the shape of 𝜖= a.(ln2 (F −F0
)) + b (Fig. 12).
0
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
0
5
10
15
%
ln(x+xo)^2; x = [kN]
Best fit
Data
Fig. 12. Stress-elongation analysis for the cables used in Cablecrane
It is also necessary to take into account the swiveling angle of the pulleys. This
means, the change in orientation but also the length of the arc when the cable passes
through the pulleys. Details of the model implementation have been developed in [13].
The improvement in the position error due to the implementation of the elasticity
and pulley models was measured by measuring several positions along z axis.
(Figure 13) shows the impact on the precision. From an original error of 25 cm, the error
drops to a few millimeters (max 6 mm, mean error 2.5 mm) over the full height.
-300
-250
-200
-150
-100
-50
0
0
500
1000
1500
2000
2500
3000
[mm error]
Desired platform height [mm]
No elasticity model, no pulley
model
Logarithmic law for elasticity
Logarithmic law for elasticity
+ pulley model
Fig. 13. Error measures in Z versus desired height, in mm.
Another important modiﬁcation with respect to a traditional crane is that all axes in
the crane are driven by servomotors, which allows controlling the position of the crane
12
D. Culla et al.

at each time. For that purpose, distance sensors have been installed in order to maximize
placement precision and correct guiding through plant. In addition, CNC interpreter has
also been included as part of the standard control features, which allows the whole
system to behave automatically as any other machine tool.
Finally, cable robotics requires speciﬁc control strategies [5] that consider dual space
(joint and cartesian space) feed forward loops. These loops consider additional inertia
forces in platform and joints in order to get the smoothest and best calculated cable
tension transition from one position to another.
6
Multi-task Orientated Platform
Cablecrane architecture opens the doors for automation in multiple sectors and activities
due to its high ﬂexibility solution for large spaces [6–8]. These tasks can have direct
application in naval, construction, aeronautics, nuclear, civil engineering, logistics and
material handling industries, i.e.:
• Production of large and heavy metallic parts and structures, notably involving
welding, sand-blasting, painting, stripping, inspection and deconstruction.
• Manipulation, assembling and maintenance of large parts and systems in a precise
way (±2 mm) and with complete control of part orientation (6 DOF).
• Automated logistic operations: quick pallet storage in automated mode at high
speeds, with unmanned or manual operations and collision control.
• Inspection, monitoring and maintenance: fast movement along large spaces with
platforms and/or camera, incorporating speciﬁc tooling and repairing materials. It
can perform operations in highly risky conﬁned spaces.
• Operations on big surfaces: painting and welding.
• 3D printing of large scale construction parts & small scale buildings
Acknowledgement. This project has received funding from the Spanish Ministry of Economy,
Industry and Competitiveness’s RETOS research and innovation program under grant agreement
No RTC-2015-4244-8 - CABLECRANE: “Design, manufacturing and validation of the servo-
controlled gantry crane that integrates a cable-driven robot for fully controlled handling and
assembling of heavy and high added value parts in large workspaces”.
References
1. Riehl, N.: Modélisation et conception de robots parallèles à câbles de grande dimension. Ph.D.
thesis. Université Montpellier II, Montpellier (2011)
2. Merlet, J.-P.: Kinematics of the wire-driven parallel robot MARIONET using linear actuators.
In: IEEE International Conference on Robotics and Automation, Passadena, CA, USA (2008)
3. Gouttefarde, M., Collard, J., Riehl, N., Baradat, C.: Geometry selection of a redundantly
actuated cable-suspended parallel robot. IEEE Trans. Rob. 31(2), 501–510 (2015)
4. Gouttefarde, M., Gosselin, M.: Analysis of the wrench-closure workspace of planar parallel
cable-driven mechanisms. IEEE Trans. Robot. 22(3), 434–445 (2006). s.l., Elsevier
Full Production Plant Automation in Industry
13

5. Lamaury, J., Gouttefarde, M., Chemori, A., Hervé, P.: Dual-space adaptive control of
redundantly actuated cable-driven parallel robots. In: 2013 IEEE/RSJ International
Conference on Intelligent Robots and Systems, Tokyo (2013)
6. CoGiRo Project. http://www2.lirmm.fr/cogiro/. Accessed Jun 2017
7. Tecnalia, Cable Driven Parallel Robotics for industrial applications. https://youtu.be/
px8vwNerkuo. Accessed Jun 2017
8. Tecnalia, COGIRO: Cable Robot for agile operation in large workspaces. https://youtu.be/
6RjBKvoc6N4. Accessed Jun 2017
9. Megarob, development of ﬂexible, sustainable and automated platform for high accuracy
manufacturing operations in medium and large complex components using spherical robot
and laser tracker on overhead crane. http://www.megarob.eu/. Accessed Jun 2017
10. Riehl, N., Gouttefarde, M., Krut, S., Baradat, C., Pierrot, F.: Eﬀects of non-negligible cable
mass on the static behavior of large workspace cable-driven parallel mechanisms. In: IEEE
International Conference on Robotics and Automation, pp. 2193–2198 (2009)
11. Riehl, N., Gouttefarde, M., Baradat, C., Pierrot, F.: On the determination of cable
characteristics for large dimension cable-driven parallel mechanisms. In: IEEE International
Conference on Robotics and Automation, pp. 4709–4714 (2010)
12. Kozak, K., Zhou, Q., Wang, J.: Static analysis of cable-driven manipulators with non-
negligible cable mass. IEEE Trans. Robot. 22(3), 425–433 (2006)
13. Nguyen, D.-Q., Gouttefarde, M., Company O., Pierrot, F.: On the analysis of large-dimension
reconﬁgurable suspended cable-driven parallel robots robotics and automation. In: IEEE
International Conference on Robotics and Automation, Hong Kong (2014)
14
D. Culla et al.

Human-Robot Collaboration and Safety
Management for Logistics and Manipulation
Tasks
Gi Hyun Lim(B), Eurico Pedrosa, Filipe Amaral, Ricardo Dias,
Artur Pereira(B), Nuno Lau, Jos´e Lu´ıs Azevedo, Bernardo Cunha,
and Luis Paulo Reis
IEETA, Universidade de Aveiro, Aveiro, Portugal
{lim,efp,f.amaral,ricardodias,artur,nunolau,jla}@ua.pt, mbc@det.ua.pt,
lpreis@dsi.uminho.pt
Abstract. To realize human-robot collaboration in manufacturing,
industrial robots need to share an environment with humans and to
work hand in hand. This introduces safety concerns but also provides
the opportunity to take advantage of human-robot interactions to con-
trol the robot. The main objective of this work is to provide HRI without
compromising safety issues in a realistic industrial context. In the paper,
a region-based ﬁltering and reasoning method for safety has been devel-
oped and integrated into a human-robot collaboration system. The pro-
posed method has been successfully demonstrated keeping safety during
the showcase evaluation of the European robotics challenges with a real
mobile manipulator.
1
Introduction
Human-robot collaboration will play a key role to develop eﬃcient solutions for
small and middle-sized enterprises (SMEs), whose environments are typically
less structured and demands higher ﬂexibility than those of large-scale or mass-
production industries [15]. This paradigm tackles the challenge of the safety of
human operators [12]. Ensuring safety, an industrial robot can increase autonomy
in a production line beyond a repetitive and low intelligence paradigm. Up to the
present, workspace sharing by human operators and robots demands a redesign
of working environments with physical safety solutions such as a safety zone.
This paper proposes a solution applicable not only in a packaging industry, but
also in other industries to increase human-robot collaboration keeping safety.
Current packaging machine vendors do not provide any automated mech-
anism for the task of feeding blank piles and the state of the art is to have
a human operator dedicated to feed blank piles to a packaging machine, as
shown in Fig. 1. This is a tedious and repetitive task and human operators may
occasionally refrain from collecting blank piles from more distant pallets. If we
consider only the task of removing blank piles from a pallet, the industrial state
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_2

16
G.H. Lim et al.
(a) human operator feeds a blank pile
(b) manipulation in a safety zone
Fig. 1. Packaging environment.
of the art already provides robotised solutions. However, these solutions follow
a completely diﬀerent paradigm, as a robot operates in a very structured safety
zone [2], where several laser scanners and physical fences forbid the entrance of
humans. These robots cannot cope with small errors in the positioning of the
objects [10]. Furthermore, the robot uses a very complex and expensive gripper
with several degrees of freedom.
The EuRoC project [14] has launched industry-relevant challenges to exploit
synergies among all the actors of the value chain in manufacturing and servicing.
Especially the Challenge 2 (C2), named Shop Floor Logistics and Manipulation,
addresses Logistics and Robotic Co-Workers scenarios of the SRA2009 [4] in
small batch manufacturing. The work described in the paper was used in the
Showcase task of the EuRoC project by team TIMAIRIS. To complete the three
challenges of the showcase task, the proposed solution addresses several technical
issues as described below. It uses a skill-based anytime agent architecture [1],
that has been evolving from the one used for the Stage I simulation tasks [13].
The separation of task-dependent representations and a generic agent processing
algorithm allows the robot to start at any task state. Using this feature, human
collaborators or robot operators may repeat a task after recovering from failure
or to test the skill from a given starting point.
To develop the diﬀerent skills, several diﬀerent motion planners have been
tested, and algorithms have been devised to increase the robustness of the motion
planner and get feasible and usable motions that respect the task constraints.
The planners have been compared using 4 metrics: path length; joint variations;
number of replans and number of points [18].
The use of multimodal interaction is an important research topic and there
is no oﬀ-the-shelf solution [8], to the best of our knowledge, for recognizing
gesture commands in environments such as the TIMAIRIS showcase setup. The
proposed solution combines RGB and Depth features, extracted through Deep
Learning and Hand Boundary Histograms, that are combined to obtain the best
recognition rate. By encoding a ﬁxed number of symbols into interaction graphs,
the system can represent any number of commands through simple edition of
the graph [9].
The detection of the position of the pile of blanks is performed using a ﬁltered
edge detection image, where the blank edges are preserved while the edges of

Human-Robot Collaboration and Safety Management
17
(a)
(b)
Fig. 2. Showcase environment; (a) simulated workspace for autonomous packaging,
(b) KUKA KMR mobile manipulator.
the printed patterns are removed. This image is then used for spiral template
matching against a model of the blank edge, scaled considering the distance
between the camera and the blank piles. This approach proved to be robust to
very diﬀerent kinds of blank printed patterns and also to very diﬀerent lighting
conditions. During some tests, the decision on the top printed pattern to be used
in each of the piles was performed by an external user and all blank piles were
detected without failure. Other perception tasks also use a combination of Depth
and RGB images, always promoting the robustness of the approach.
This paper addresses safety management to share workspace by human oper-
ators and a real mobile manipulator for logistics and manipulation tasks. There
are several technical issues for safety: human detection and tracking, safety rea-
soning, multi-sensor integration, resource management.
2
EuRoC Realistic Showcase
The EuRoC challenges are organized into successive stages with increasing time-
lines for productization. This paper presents the result about the Showcase in the
Stage II: REALISTIC LABS of the TIMAIRIS team who is one of 15 challenger
teams advancing to this stage out of 102 teams in the simulation contest. The
TIMAIRIS showcase task is composed by a simpliﬁed version of the complete
blank feeding problem to be developed for the ﬁnal use case for real packaging
environments, as shown in Fig. 2. The main idea is to have a simpliﬁed problem
as a realistic lab, although having most of the components of the ﬁnal solution.
Hence, in the showcase, a prototype of a blank magazine (a mechanism that
feeds the packaging machine) and two pallets ﬁlled in with the same type of
blanks are used to demonstrate the capacity of the KUKA platform to pick a
pile of blanks from the pallets, transport it and place it in the blank magazine.
A special gripper has been developed for this purpose.
In this task the following technologies are going to be covered: the platform
is able to recognize empty pallets, plan and replan its actions; an initial version

18
G.H. Lim et al.
Solver
Human
Tracker
Perception
Modules
Sensor
Interface
Effector
Interface
Manipulation
/ Navigation skills
Planner
Safety
Manager
HRI
Fig. 3. An overview of skill-based architecture.
of the multimodal interface is used, integrating the possibility of gesture com-
mands in the interaction with the platform (gestures are used mostly for safe
navigation); the robot is able to navigate in an environment that includes a few
humans and interact with them; the manipulation of the selected type of blanks
is demonstrated.
The setup requires an open area of approximately 20 m2, where the prototype
of the blank magazine and the two pallets with blanks are placed. Equipment
also includes the KUKA KMR platform and the new blank gripper (developed in
the showcase). Initially, calibration may be required for conﬁguring the detection
and grasping of the blanks, and for the multimodal interface.
3
System Architecture
3.1
Skill-Based Framework
In a higher level view of the skill-based framework, as shown in Fig. 3, the func-
tional components are represented by boxes. A Perception module collects sen-
sory data and processes them to extract information used by high-level modules.
A Skill is the capacity of doing a particular task, such as, picking and placing
an object or moving the end-eﬀector of the manipulator to a desired pose. Per-
ception modules and Skills receive and transmit sensory-motor data via Sensor
interface and Eﬀector interface respectively. The Planner provides solutions for
the current packaging state, while the Solver is responsible for taking decisions
on how to solve the current task based on the current sensory data and avail-
able Skills. The Human tracker keeps on tracking humans in the workspace by
using laser scanners. The Human-Robot Interaction (HRI) module communicates

Human-Robot Collaboration and Safety Management
19
with a human by recognizing gestures and by providing information via multi-
modal interfaces [9]. The Safety manager infers whether it is in a safe state or
not based on system states and human tracking information. When the Safety
manager decides not to go ahead, it requests Solver to gaze the nearest human
operator and triggers HRI to interact with him. Then, HRI takes an order from
the operator to resume or stop the paused task using gestures.
3.2
Resource Management Scheme
To continuously monitor humans, the Human tracker and Safety manager need
to be continuously active, while all other modules including skills and perceptions
modules just run on request. That should cause conﬂicts over resources. For
example, when the robot wants to recognize the pose of a blank magazine to
feed a blank pile, a perception module tries to rotate a camera system to the
blank magazine on the table. If, at the moment, a human operator approaches,
the Safety manager also tries to rotate the camera system to gaze the operator.
Algorithm 1. Algorithm for resource management.
1: procedure Node(D)
▷D is the name of daemon
2:
R ←buildResourceMap(D)
3:
while available(R) do
4:
run(Proc)
▷run normal procedures
5:
release(D)
▷release resources at the end
6:
end while
7: end procedure
Based on the skill-based agent architecture [1,13], the system architecture
is added resource management scheme, as shown in Algorithm 1. When a robot
starts, each module which needs to run continuously is launched and becomes a
daemon. The Solver builds a resource map which lists all necessary resources for
each daemon module. At every spin, that means a wake-up for all subscriptions,
services, timers and so on in ROS (Robot Operating System)1, the daemon checks
the availability of its resources. If available, it runs normal procedures and release
all resources at the end of the procedures.
4
Human Tracking
4.1
Multi Sensor Integration
Because of limitations on the ﬁeld of view in single-sensory systems and resource
conﬂicts over access to shared resources such as cameras, multi sensor integration
is a foremost approach to derive synthesized information from multiple sources [6,
1 http://wiki.ros.org/tf.

20
G.H. Lim et al.
)
b
(
)
a
(
Fig. 4. (a) Hokuyo laser range ﬁnder (LRF) URG-04LX-UG01, (b) Sensors on mobile
manipulator: two Hokuyo LRFs in red circles, two stereo camera systems in blue rec-
tangles, and a Kinect RGB-D camera in a yellow rectangle.
7,16]. In the challenge environment, each perception module uses camera systems
at speciﬁc poses and locations based on spatial knowledge [1,11]. In order to track
humans constantly, two low cost Hokuyo LRFs have been mounted on both sides
of the platform manipulator to cover all directions without shadow regions from
its location, as shown in Fig. 4. Firstly, multiple single-plane laser scans are
merged to generate virtual laser scans, disregarding the actual occlusions as it
would be seen from the merged scan viewpoint [3]. Then the resulting scans are
transformed to the robot frame, and are aligned with the point clouds of the
stereo camera systems.
4.2
Human Detection and Tracking Using Laser Scanners
A machine-learning-trained classiﬁer2 is used to detect human body from single-
plane laser scans. Positive and negative samples of laser scanner readings of
human body patterns have been collected and are trained into random forests
beforehand [5]. Given a training set, the training algorithm repeatedly selects
a random sample with replacement of the training set and ﬁts trees in random
forests to these samples. After training, prediction for new samples can be made
by the cumulative result from all the trees (fb) in the forest on x′:
ˆf = 1
B
B

b=1
fb(x′),
(1)
where B is the number of training or bagging.
In the tracking procedure, a laser ring from merged scan is segmented and
classiﬁed by the forest classiﬁer. The segmented scans become the set of can-
didate clusters. Each candidate is matched within a threshold with the list of
trackers which are listed in the previous tracking procedure. If not matched, a
2 https://github.com/wg-perception/people.

Human-Robot Collaboration and Safety Management
21
Fig. 5. Realistic environment.
During Navigation
During Manipulation
Fig. 6. Regions for safety handling.
new tracker is added into the match list. Then all trackers are updated based on
Kalman ﬁlter [17]:
Hk = Pk−1 + Q,
Kk = P(P + R)−1,
ˆxk = ˆxk−1 + Kk( ˆf −ˆxk−1),
Pk = (1 −Kk)Pk−1Hk,
(2)
where Q and R are the uncertainty parameters of the Kalman ﬁlter and 0.002
and 10 are used in the work, respectively.
Finally, Human tracker attempts to pair individual body parts together and
estimates the center of one person, since whole bodies might consist of one or
more body parts, such as torso, legs and arms from the single-plane scan view-
point. In the realistic environment, there are many recognition failures, especially
false positives (FPs) from objects such as blank piles on a workbench, bags on
a windowsill and the robot arm, as shown in Fig. 4(b), because the LRFs are
mounted at the level.
5
Safety Management
Figure 5 shows the showcase setup of TIMAIRIS as a realistic lab for autonomous
packaging. The scenario of TIMAIRIS showcase is a simpliﬁed version of the

22
G.H. Lim et al.
complete blank feeding problem, and it is composed of a prototype of a blank
magazine, the mechanism that feeds the blanks into the packaging machine, and
two pallets ﬁlled in with actual blank piles used in production. The task consists
in recognizing the blank piles on two diﬀerent pallets, devising a plan for picking
them, and transporting and placing them in the blank magazine in a smooth
way so that it does not get jammed. The workspace is assumed to be shared
by humans and the robot, as shown in Fig. 2. The Safety Manager module is
responsible for taking care of safety issues, using the information provided by
the Human Tracker module. Its processing unfolds in two parts: region-based
ﬁltering and safety handling.
5.1
Region-Based Filtering and Reasoning
Figure 6 shows regions for safety reasoning. One green region and several black
regions represent the workspace and objects including the robot and tables,
respectively. The observations in yellow regions and red regions are considered
as humans and are evaluated in order to decide the safety action to be taken. The
details are given in Sect. 5.2. All observations in the green region are taken into
account as candidates to track and monitor humans. As referred above, in the
realistic environment, the observations delivered by the Human Tracker module
contain many FPs. Most of them appear in places where humans can not be,
such as objects in the windowsill and humans on the boundary of the workspace.
They are out of the workspace. To deal with these FPs, a green region is deﬁned
in advance, and any detection that lies outside of this region is discarded. On the
other hand, there are also FPs caused by objects on the robot, the mast of the
pan and tilt camera, piles on the pallets and the blank magazine. Those are in
the regions that are occupied by the robot and objects such as tables. For them
black regions are established, and any observation in these regions is discarded,
as shown in Fig. 7. There is a diﬀerence between the robot inner region and the
others: the others are ﬁxed in the environment during the whole showcase round;
the former is ﬁxed to the robot frame. Since the virtual laser scans are deﬁned
in the robot frame, the detected human candidates are also deﬁned in the same
frame. Thus, to ﬁlter FPs based on the exclusion regions of the environment, a
transformation from robot frame to environment frame is required.
Fig. 7. Filtered regions.

Human-Robot Collaboration and Safety Management
23
Along the showcase round, the only situations in which the robot arm is out
of the robot inner region are during the grasping operation of a pile and during
the feeding operation of the blank magazine. In both cases, the arm movement
can cause FPs to be detected by the human tracking module. In the former
situation, the robot is close to the pallet containing the pile; in the latter, it is
close to the table where the blank magazine is mounted on. In both cases, the
gap between the robot and the table or the pallet is narrower than a human
body. This fact is used to eliminate the FPs caused by the arm movement, by
deﬁning narrow regions around the table, the pallets and the robot and ﬁltering
out human detections that appear in the intersection of these regions.
Figure 8 is a snapshot of the rviz tool, showing the showcase environment
and safety handling information. The small circular spots represent possible
human parts (in our case, torso or arm), the color representing a reliability
value, increasing from black to blue. Only one of these spots was considered as
a human requiring safety attention, the one represented by the green wider spot
(see the linked video3).
Fig. 8. Regions for safety handling, shown using the rviz tool.
5.2
Safety Handling
Safety handling is based on regions deﬁned around the robot platform and the
robot arm. Two rectangular regions are deﬁned around the robot platform during
navigation. Red one is close to the robot and the other yellow one next, as shown
in Fig. 6, left. During manipulation, a red circular region is deﬁned around the
robot arm, whose radius is determined by the arm range, as shown in Fig. 6,
right. Actually, only the part of this region that does not intersect the inner
region of the robot counts, as detections in this robot inner region are ﬁltered
out in the aforementioned ﬁltering phase.
3 https://youtu.be/bJaJznbzMOI.

24
G.H. Lim et al.
Based on these regions, safety handling proceeds as follows. If every human
just stays in the green regions, it is considered that the robot operation does not
put humans in danger. So no safety action is taken at all, and the robot keeps
doing its operation. If any human enters a yellow region which is the furthest
rectangular region and outside of arm manipulation region, only the naviga-
tion operation is suspended, while the arm keeps doing its current manipulation
operation, if any. If a human enters a red region which is the arm manipulation
region or the rectangular region closer to the robot platform, the robot suspends
its current operation, holding its state, and stops platform and arm movements.
In any of these two last situations, the pan and tilt camera points to the chest
of the closest human, in order to allow for an human operator gesture command.
The pan value is determined by the (x, y) coordinates of the human detection
point; the tilt value is deﬁned assuming the average height of a human.
The robot gets out of the stopping and holding situation in two diﬀerent
ways. If a command is given by the human operator, the robot just executes it:
the suspended operation can be resumed, interrupted or the whole task can be
interrupted. If no command is given and the human moves out of yellow regions
and red regions, the robot automatically resumes the suspended operation.
Fig. 9. Interaction graph for the packaging task.
6
Human-Robot Interaction
For small batch production, changing or adding new features, such as contin-
uously changing printed patterns of blanks, can be a burden on both human
operators and autonomous robots. Since industrial environments are noisy, where
machines produce a continuous whirring sound, verbal communication is diﬃcult
for humans and impractical for robots. Thus, gestures have been considered as a
practical alternative way of communication. So far gesture recognition systems
in HRI have focused on small number of implicit interactions such as pointing

Human-Robot Collaboration and Safety Management
25
and handing over. With respect to pointing, the target object must be in the
ﬁeld of view of both human and robot. To ask to fetch objects out of view or
in a cluttered environment where a human is impossible to specify an object by
pointing, HRI systems need to have a rich vocabulary to designate an object.
Figure 9 shows the interaction graph of the showcase challenge. It enables to
compose sentences with a ﬁxed number of symbols [9].
7
Challenge Evaluation
During the showcase evaluation of the Stage II with a real KUKA mobile manip-
ulator, TIMAIRIS showcase addresses all of these issues (and others) in the form
of three challenges, with quantiﬁable metrics (see the oﬃcial video of the evalu-
ation4). The objectives and metrics are organized so that the execution can be
evaluated in a comprehensive and robust way. For safety management, the ﬁrst
objective on perception includes a metric: identifying the presence of humans in
the vicinity of the robot (O1M4). The second objective on manipulation includes
a metric: stopping manipulation and navigation if a human is close to the robot
(O2M4). The third objective on the planning capabilities includes a metric:
adapting navigation paths (O3M2). The last objective evaluates human robot
interaction and includes a metric: tracking a human for interaction (O4M2). The
achieved metrics are presented in Table 1.
Table 1. Quantiﬁable evaluation
Target Events Percent (%)
Recognize safety situations (O1M4)
2
2
100
Manipulate safely (O2M4)
2
2
100
Adapt navigation paths (O3M2)
2
2
100
Track a human for interaction (O4M2) 2
2
100
As can be seen from Table 1, TIMAIRIS completed the showcase with every
objective and metric being accomplished. In what regards Perception, Manipula-
tion/Navigation and Planning, i.e. Objectives 1, 2 and 3 previously speciﬁed, all
metrics have been achieved during the execution of the ﬁrst part of the showcase
evaluation that consisted on three challenges. Tracking humans with the pan-tilt
camera (O4M2) was also achieved during this ﬁrst part of the evaluation.
8
Conclusion
The proposed safety management system provides opportunities for a ﬂexible
and safe human-robot collaboration in a realistic industrial setting. As the result
4 https://youtu.be/8sNQLCs1jwE.

26
G.H. Lim et al.
of the Stage II evaluations, TIMAIRIS has been decided to advance to the ﬁnal
stage of the EuRoC. The achievements obtained in the showcase phase provide an
excellent base for the development of the pilot experiment. The pilot experiment
environment will be a real industrial setting but, all the developments in the
paper are directly applicable in the environment. Some new features will have
to be addressed that also depend on the speedup of the ﬁnal prototype, such
as considering several packaging machines and diﬀerent types/shapes of blanks,
enhanced safety and more challenging navigation issues.
Acknowledgement. This work was supported by the EuRoC Project under Grant
no. 608849.
References
1. Amaral, F., Pedrosa, E., Lim, G.H., Shaﬁi, N., Pereira, A., Azevedo, J.L., Cunha,
B., Reis, L.P., Badini, S., Lau, N.: Skill-based anytime agent architecture for logis-
tics and manipulation tasks: EuRoC Challenge 2, Stage II-Realistic Labs: Bench-
marking. In: 2017 IEEE International Conference on Autonomous Robot Systems
and Competitions (ICARSC), pp. 198–203. IEEE (2017)
2. Augustsson, S., Christiernin, L.G., Bolmsj¨o, G.: Human and robot interaction
based on safety zones in a shared work environment. In: Proceedings of the 2014
ACM/IEEE International Conference on Human-Robot Interaction, pp. 118–119.
ACM (2014)
3. Ballardini, A.L., Fontana, S., Furlan, A., Sorrenti, D.G.: ira laser tools: a ROS
laserscan manipulation toolbox. arXiv preprint arXiv:1411.1086 (2014)
4. Bischoﬀ, R., Guhl, T.: The strategic research agenda for robotics in europe [indus-
trial activities]. IEEE Robot. Autom. Mag. 1(17), 15–16 (2010)
5. Breiman, L.: Random forests. Mach. Learn. 45(1), 5–32 (2001)
6. Dias, R., Lau, N., Silva, J., Lim, G.H.: Multi-object tracking with distributed sens-
ing. In: 2016 IEEE International Conference on Multisensor Fusion and Integration
for Intelligent Systems (MFI), pp. 564–569. IEEE (2016)
7. Lim, G.H.: Two-step learning about normal and exceptional human behaviors
incorporating patterns and knowledge. In: 2016 IEEE International Conference
on Multisensor Fusion and Integration for Intelligent Systems (MFI), pp. 162–167.
IEEE (2016)
8. Lim, G.H., Oliveira, M., Mokhtari, V., Kasaei, S.H., Chauhan, A., Lopes, L.S.,
Tom´e, A.M.: Interactive teaching and experience extraction for learning about
objects and robot activities. In: 2014 RO-MAN: The 23rd IEEE International
Symposium on Robot and Human Interactive Communication, pp. 153–160. IEEE
(2014)
9. Lim, G.H., Pedrosa, E., Amaral, F., Lau, N., Pereira, A., Dias, P., Azevedo, J.L.,
Cunha, B., Reis, L.P.: Rich and robust human-robot interaction on gesture recog-
nition for assembly tasks. In: 2017 IEEE International Conference on Autonomous
Robot Systems and Competitions (ICARSC), pp. 159–164. IEEE (2017)
10. Lim, G.H., Suh, I.H.: Improvisational goal-oriented action recommendation under
incomplete knowledge base. In: 2012 IEEE International Conference on Robotics
and Automation (ICRA), pp. 896–903. IEEE (2012)

Human-Robot Collaboration and Safety Management
27
11. Lim, G.H., Yi, C., Suh, I.H., Ko, D.W., Hong, S.W.: Ontology representation
and instantiation for semantic map building by a mobile robot. In: Intelligent
Autonomous Systems 12, pp. 387–395. Springer (2013)
12. Pedrocchi, N., Vicentini, F., Matteo, M., Tosatti, L.M.: Safe human-robot cooper-
ation in an industrial environment. Int. J. Adv. Robot. Syst. 10(1), 27 (2013)
13. Pedrosa, E., Lau, N., Pereira, A., Cunha, B.: A skill-based architecture for pick and
place manipulation tasks. In: Progress in Artiﬁcial Intelligence: 17th Portuguese
Conference on Artiﬁcial Intelligence, EPIA 2015, pp. 457–468. Springer (2015)
14. Siciliano, B., Caccavale, F., Zwicker, E., Achtelik, M., Mansard, N., Borst, C.,
Achtelik, M., Jepsen, N.O., Awad, R., Bischoﬀ, R.: EuRoC-the challenge initiative
for european robotics. In: Proceedings of ISR/Robotik 2014, 41st International
Symposium on Robotics, pp. 1–7. VDE (2014)
15. Stenmark, M., Malec, J., Nilsson, K., Robertsson, A.: On distributed knowledge
bases for robotized small-batch assembly. IEEE Trans. Autom. Sci. Eng. 12(2),
519–528 (2015)
16. Teunissen, P.: An integrity and quality control procedure for use in multi sen-
sor integration. In: ION GPS Redbook, Integrated Systems, vol. 7. Institute of
Navigation (ION) (2010)
17. Thrun, S., Burgard, W., Fox, D.: Probabilistic Robotics. MIT Press, Cambridge
(2005)
18. Tudico, A., Lau, N., Pedrosa, E., Amaral, F., Mazzotti, C., Carricato, M.: Improv-
ing and benchmarking motion planning for a mobile manipulator operating in
unstructured environments. In: Portuguese Conference on Artiﬁcial Intelligence,
pp. 498–509. Springer (2017)

Grasp Quality Measures for Transferring Objects
Fernando Soler(B), Abiud Rojas-de-Silva, and Ra´ul Su´arez
Insitute of Industrial and Control Engineering, Universitat Polit`ecnica de Catalunya,
Barcelona, Spain
{fernando.soler,francisco.abiud.rojas.de.silva,raul.suarez}@upc.edu
Abstract. There is a lack of quality indexes to evaluate grasps that are
more likely to allow a hand-to-hand transfer of an object during a manip-
ulation task. In order to overcome it, this paper presents a proposal of
grasp transfer quality measures to evaluate how easy or feasible is that
an object grasped by one hand could be grasped by another hand to per-
form a hand-to-hand transfer. Experiments were conducted to evaluate
the proposed grasp transfer quality measures using diﬀerent objects and
the model of a real robotic hand.
Keywords: Grasping · Grasp quality measures · Transfer manipulation
1
Introduction
The use of dual-arm robots has been increasing in the ﬁeld of object manipu-
lation due to their advantages over one-arm robots. However, using two arms
simultaneously can lead to other speciﬁc problems, such as the computation of
suitable grasps to hold the object with both hands or to transfer the object from
one hand to the other. This, in turn, generates the need of quality measures to
evaluate and choose the best grasps to perform the transfer of an object.
The evaluation of grasp quality in a general sense has been already studied,
a review of diﬀerent quality measures proposed for grasp planning in literature
is presented in [1]. In this review, the quality measures are classiﬁed into two
groups: the ﬁrst group is related to the contact points on the grasped object
and the second group to the hand conﬁguration. Besides, the combination of the
quality measures from the previous two groups is also discussed to obtain good
grasps for speciﬁc purposes. Nevertheless, despite the list of already proposed
quality measures, none of them addresses the problem of measuring how good
is a grasp for an object transferring, even when this is a very frequent action.
This paper focuses on the proposal of quality measures to evaluate grasps in
order to determine how good they are in order to transfer objects from one hand
to another using a dual-arm robot, as shown in Fig. 1.
Work partially supported by the Spanish Government through the projects DPI2013-
40882-P and DPI2016-80077-R. F. Soler and A. Rojas-de-Silva—Partially supported
by the Mexican CONACyT doctoral grants 410931 and 313768.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_3

Grasp Quality Measures for Transferring Objects
29
Fig. 1. Dual-arm robot (ADARS: Anthropomorphic Dual Arm Robotic System) about
to perform a transfer of the object from the right to the left hand.
The rest of the paper is structured as follows: Sect. 2 summarizes relevant
literature on grasp quality measures. Section 3 describes the grasp transfer qual-
ity measures. Section 4 describes how the grasp transfer quality measures are
computed. Section 5 presents application examples of the grasp transfer qual-
ity measures and results. Section 6 presents the validation process, and Sect. 7
presents the summary and future work.
2
On the Quality of a Grasp
One of the fundamental property for grasps is force-closure (FC) [2]. A grasp is
FC if the forces applied by the ﬁngers on an object counteract any disturbance
force or torque in any direction ensuring object immobility. The most common
quality index is based on the FC property and was presented by Ferrari and
Canny [3] and describes the largest perturbation that the grasp can resist under
arbitrary disturbances or wrenches. This quality measure is directly associated
with the Grasp Wrench Space (GWS), which is the set of all wrenches than can
be applied on an object through unitary forces at the grasping contact points.
The quality measure is deﬁned by the radius of the largest ball centered at
the origin of the wrench space and contained in the convex hull of the possible
wrenches produced by unitary forces of the ﬁngers on the object. If the origin
is inside the convex hull, then the grasp is FC. To quantify the quality of a
grasp, several approaches have been proposed, such as the largest sphere ﬁtting
inside the convex hull of GWS, the nearest distance from the origin to the border
of the convex hull and the volume of the GWS [4]. The Object Wrench Space
(OWS) [5] contains all the wrenches that can be produced by a set of unitary
external disturbances forces acting on the surface of the object. The OWS is
hardly used by grasp planners due to the high amount of time required for
its computation which is a problem when many grasps have to be evaluated.

30
F. Soler et al.
Recently, an algorithm to speed up the computation time of a grasp quality
metric based on OWS was presented in [6]. Another tool used to evaluate grasps
based on wrench spaces is the Task Wrench Space (TWS) [7]. The TWS uses
detailed information about the given task and incorporates all the perturbation
wrenches that can be produced during a manipulation task. When the task
information is not speciﬁed, such wrenches emerge from the interaction between
the object and the environment, such as the gravity and object’s acceleration
due to arm movements.
Earlier works on grasping consider that the object being grasped is alone in
the environment. An algorithm is proposed in which a grasp scoring function uses
information about the environment around the object and the robot’s kinematics
to ﬁnd feasible grasps in a cluttered environment [8]. Other quality function uses
the sum of several measures to obtain grasps for each hand using a dual-arm
robot [9]. The main task is to move an object from one position to another by
transferring an object from one hand to another, so that at some moment of the
task, a bimanual grasp is done, which is evaluated by another quality function
based on the quality of each hand separately and the conﬁguration of the robot
when grasping the object at its start and goal conﬁguration. The GWS can also
be used to determine the quality of a bimanual grasp as in the case of single
grasp. The computation and evaluation of grasps are done on-line as well as the
search of no-collision motions which means that the search of feasible grasps is
not limited to a set of pre-computed grasps since grasp poses are deﬁned during
the planning process [10]. The manipulability information is used as a quality
measure [11] to ﬁnd grasps that allow a high manipulability to manipulate large
objects, since they limit the movement of a dual-arm robot when it is working
as a closed kinematic chain.
3
Grasp Transfer Quality Measures
The goal of the grasp transfer quality measures (TQ) is to determine how good
is an initial grasp so that a second suitable grasp can be done on the same
object in order to perform a hand-to-hand transfer. If the grasp being analyzed
obtains a good quality for transferring, it means that: (a) it allows good grasps
for another hand to perform a transfer, or, (b) it allows an easy ﬁnding of the
grasp for the second hand. Both cases will be discussed below in this work.
It is desired that both the initial grasp before the transfer and the second
grasp after the transfer are FC grasps in order to ensure object immobility during
any manipulation. How good is a FC grasp in terms of the maximal perturbation
it can resist in any direction is evaluated using a FC grasp quality Q [3], which
is deﬁned as
Q = min
ω∈∂W∥ω∥
(1)
where ∂W is the boundary of the convex hull of the set of wrenches produced
by the applied grasping forces on the object boundary.
Some of the grasp transfer quality measures TQ introduced below in this
work are based on Q.

Grasp Quality Measures for Transferring Objects
31
3.1
Measure Based on the Individual Maximum FC Grasp Quality
Reachable
This proposal is based on the maximum FC grasp quality that can be achieved
on the object. The grasp transfer quality TQMQ of the initial grasp is a function
of the maximum FC grasp quality that can be achieved by a second grasp when
the object is already grasped by the ﬁrst hand, i.e., the ﬁrst hand acts an obstacle
for the second hand to grasp the object and therefore not all the object surface
is reachable for a second grasp. TQMQ is computed as
TQMQ = Qp
Qg
(2)
where
Qp is the maximum FC grasp quality that could be reachable by the second
hand while the ﬁrst hand is holding the object.
Qg is the maximum FC grasp quality reachable on the whole object without
any constraint.
TQMQ ∈[0, 1], being 1 the best case. Note that in practice the computation
of TQMQ depends on the hands used, because when the ﬁrst hand grasps the
object its structure will make some points of the object to be unreachable by
the second hand. These practical aspects and how Qp and Qg are computed are
described below in Sect. 4.2.
3.2
Measure Based on the Grasp Quality Improvement
This proposal is based on the ratio of Qp, previously deﬁned, and the FC grasp
quality of a initial grasp Qi and is expressed as
TQGQ = Qp
Qi
(3)
TQGQ ∈[0, ∞]. TQGQ has a simple interpretation: the greater the value of
TQGQ, the better the chance to ﬁnd grasps with the second hand that improves
the quality of the initial grasp.
3.3
Measure Based on the Reachable Area of the Object Surface
The area of the object surface is used to deﬁne this measure. The surface of the
object that is reachable by a second hand (when the ﬁrst hand is grasping the
object) is used to compute the grasp transfer quality TQSA for the initial grasp.
It is computed as
TQSA = Sp
Sg
(4)

32
F. Soler et al.
where
Sp is the reachable area of the grasped object surface when the ﬁrst hand is
grasping it.
Sg is the whole area of the object surface.
TQSA ∈[0, 1], being 1 the best case. The computation of Sp and Sg is described
in Sect. 4.3.
3.4
Measure Based on the Number of Reachable Points of the
Point Cloud
This proposal is based on the number of points of the point cloud representing
the object that are reachable by the second hand while the ﬁrst hand is grasping
the object. The grasp transfer quality TQNP is computed as
TQNP = m
n ,
n > m
(5)
where
m are the number of points reachable by the second hand when the ﬁrst hand
is grasping the object.
n are the number of points of the whole object model.
TQNP ∈[0, 1], being 1 the best case.
4
Implementation
This section describes how the grasp transfer quality measures are computed in
practice. The object model is represented by a point cloud Pg = {pi, i = 1, . . . , n}
(Fig. 2), with high enough density, i.e. n must be large enough. The point clouds
have been manipulated using The Point Cloud Library (PCL), which is a large
scale open project for 2D/3D image and point cloud processing [12].
Fig. 2. Left: 3D object model. Right: Point cloud of the complete object Pg.

Grasp Quality Measures for Transferring Objects
33
4.1
Obtaining the Reachable Part of the Object
Given the point cloud Pg representing the whole object boundary, and an initial
grasp for a given hand, the reachable part of the object is determined by the
points of Pg that are not in contact nor enveloped by the hand and therefore
could be considered as grasp points for the second hand. In practice, this set of
points, denoted as Pp, is obtained as follows:
1. The 3D hand model for a given grasp conﬁguration is represented by a point
cloud denoted as Ph (see Fig. 3a-Left).
2. Compute the Oriented Minimum Volume Bounding Box of Ph (for simplicity
denoted as MBB-Ph henceforth) as shown in Fig. 3a-Right.
3. The reachable part of the object boundary is considered to be the set of points
Pp = {p ∈Pp | p ̸∈MBB−Ph}.
Figure 3b shows an example of the resulting reachable part of an object.
(a)
(b)
Fig. 3. (a) Left: Ph representing the grasping conﬁguration of the Allegro Hand. Right:
MBB-Ph obtained for a point cloud of the Allegro Hand representing the grasping
conﬁguration. (b) Left: Ph and Pg. The red points represent the Allegro hand, the
green points represent Pp and the blue points are in contact or enveloped by the hand.
Right: MBB-Ph is shown in solid to show its dimension.
4.2
Obtaining Qg and Qp Using OWS
As stated above, OWS includes the wrenches that can be generated by normal-
ized forces acting anywhere on the surface of the object. The general form of
OWS is deﬁned as
ω =
n

i=1
αiωi
∧αi ≥0 ∧
n

i=1
αi = 1
(6)
where ωi denotes the wrench that can be applied on the object at contact point
pi ∈Pg.

34
F. Soler et al.
This is equivalent to the convex hull given by
OWS = ConvexHull(
n

i=1
{ωi, ..., ωn)
(7)
The metric used to evaluate the grasp FC grasp quality Q is given by Eq.
(1). Therefore, Q obtained from OWS represents the maximum FC grasp quality
that a grasp can achieve on the whole object, i.e., Qg. Similarly, Qp represents
the maximum FC grasp quality that a grasp could achieve on the reachable part
of the object while the ﬁrst hand is holding the object.
4.3
Deﬁning Sg and Sp
Since the representation of the objects is given by point clouds, and there is no
guarantee of a uniform distribution of the points, it is used the Greedy Projection
Algorithm (GPA) [13] to create triangular mesh models representing the object
surfaces from unorganized point clouds. The area of a triangular mesh model
(M) is computed by
S =
nt

j=1
Aj
(8)
where
Aj is the area of the j-th triangle of M.
nt is the total number of triangles.
Aj is obtained by the Heron’s formula considering the lengths of its three sides
Aj =

s(s −a)(s −b)(s −c)
(9)
Fig. 4. Triangular mesh models Mg and Mp of a detergent bottle computed from Pg
and Pp respectively

Grasp Quality Measures for Transferring Objects
35
where
a, b, c are the lengths of the triangle edges.
s = a+b+c
2
is the semiperimeter of the triangle.
From the point clouds Pg and Pp of the object, their respective triangular
mesh models Mg and Mp are obtained. Figure 4 shows an example of the trian-
gular mesh model computation of a detergent bottle considering the grasp shown
in Fig. 3b.
Once Mg and Mp have been created, their respective areas Sg and Sp are
computed using Eq. (8).
(a)
(b)
Fig. 5. (a) Allegro Hands. (b) Objects used for experimentation: a detergent bottle, a
cookie box, a feeding bottle, a coﬀee mug and a milk box.
5
Application Examples
The experimental veriﬁcation of the four proposed TQ was done using two
robotic Allegro Hands from Simlab (Fig. 5a) which have 16 degrees of freedom
(DOF), 4 ﬁngers and 4 independent joints per ﬁnger.
Five objects with diﬀerent sizes and shapes have been used for the evaluation
of the proposed TQ (Fig. 5b). The object models were obtained from GrabCAD
and Autodesk 123D repositories [14,15], which oﬀer a large variety of CAD
models of object with diﬀerent shapes and sizes. All the selected objects can be
grasped and handled by one hand.
The computation of the grasps for each object was done using the robotic sim-
ulation toolbox Simox [16] that allows the generation of random grasps around
free-ﬂying objects satisfying the FC property. Equation 1 was used for the FC
grasp quality computation for each individual grasp.
The values of each TQ for each object are obtained as follows:
1. Compute a set of 30 grasps using the right hand, Gr = {gr
1, . . . , gr
j}
j = 1, . . . , 30. Figure 6 shows an example of a random grasp for each object
used in this work.

36
F. Soler et al.
Fig. 6. Example of a random grasp for each object. Red points represent the grasping
hand pose Ph. Green points compose the point cloud that represents the reachable
part of the object Pp. Blue points are the points that are in contact or enveloped by
MBB-Ph.
Table 1. Results of TQ for 10 grasps computed on a detergent bottle and the indices
TQ∗used for the correlation analysis.
Detergent bottle TQ
TQ∗
Grasp
TQMQ TQGQ TQSA TQNP TQ∗
MQ TQ∗
GQ TQ∗
RS
1
0.991
3.901
0.767
0.761
0.91
3.28
0.76
2
0.986
3.772
0.705
0.743
0.9
3.48
0.73
3
0.984
3.168
0.69
0.719
0.93
2.99
0.71
4
0.983
3.57
0.662
0.495
0.9
3.52
0.608
5
0.976
3.549
0.644
0.601
0.9
3.49
0.73
6
0.975
3.294
0.639
0.675
0.85
3.19
0.51
7
0.975
2.944
0.633
0.617
0.78
2.792
0.708
8
0.973
3.6
0.631
0.566
0.68
3.546
0.756
9
0.973
2.438
0.629
0.394
0.83
3.25
0.704
10
0.972
3.122
0.628
0.61
0.73
2.21
0.706
2. Compute a set of point clouds, Pp = {P 1
p , . . . , P j
p} j = 1, . . . , 30, representing
the reachable part of the object for each grasp gr
j.
3. Compute OWS.
The four TQ described by Eqs. 2, 3, 4 and 5 were computed for each gr
j ∈Gr
using Pg, OWS and Pp.
Table 1 shows the results of the grasp transfer quality measures of 10 ran-
domly chosen grasps of one particular object.
6
Validation
An auxiliary index TQ∗for each proposed grasp transfer quality TQ will be
computed in order to perform a correlation analysis between the values of TQ
obtained for the right hand and the values of TQ∗obtained for the left hand
(shown in Table 1). The indices TQ∗were computed as follows:

Grasp Quality Measures for Transferring Objects
37
Table 2. Correlations between TQ and TQ∗
Object
Correlations %
TQMQ-TQ∗
MQ TQGQ-TQ∗
GQ TQSA-TQ∗
RS TQNP-TQ∗
RS
Detergent bottle 76.09
80
81.86
81.31
Coﬀee mug
85.81
72.52
66.98
64.50
Cookies box
55.39
95.69
77.52
78.34
Feeding bottle
97.94
91.16
60.47
57.57
Milk box
80.66
81.70
76.01
77.21
1. Compute a set of 500 grasps using the left hand, Gl = {gl
1, . . . , gl
i}
i = 1, . . . , 500, in the same way as the set Gr around the whole object.
2. Select the grasp gl
i with the best quality Ql
gmax.
3. For each grasp gr
j:
(a) Remove the grasps in Gl that are in collision with MBB-Ph in order to
obtaining another set of grasps Gl
j = {gl
i ∈Gl | gl
i compatible with gr
j }.
(b) Select the grasp gl
i ∈Gl
j with the best quality Ql
pmax.
(c) Compute the TQ∗as
• For TQMQ
TQ∗
MQ = Ql
pmax
Qlgmax
(10)
• For TQGQ
TQ∗
GQ = Ql
pmax
Qr
j
(11)
where Qr
j is the FC grasp quality of gr
j.
• For TQSA and TQNP
TQ∗
RS = c
m
(12)
where c is the number of grasps in Gl
j and m is the number of grasps
in Gl.
Now, the correlation between TQ and TQ∗was computed using the Pearson
correlation coeﬃcient which is a regression measure that quantiﬁes the variation
between two variables and is obtained as:
CORR(%) =
 xy

( x2)( y2)
(13)
where x represents the value of TQ and y represents the value of TQ∗.
Table 2 shows the correlation percentage between the theoretical values
obtained with TQ and the real values obtained with TQ∗for diﬀerent objects.
The results show a high correlation between TQMQ and TQ∗
MQ for most of
the objects used and only one object shows a moderate correlation (coﬀee mug).

38
F. Soler et al.
Fig. 7. Scatter diagram for each grasp transfer quality measure for 30 grasps using the
detergent bottle (Fig. 2).
The correlation between TQGQ and TQ∗
GQ shows the best results obtaining
correlations above 80% for most objects.
The correlation between TQSA and TQ∗
RS, which is similar to the correlation
between TQNP and TQ∗
RS, shows the lowest correlation (under 80% for most
objects), three objects (detergent bottle, cookie box and milk box) have high
correlation and two objects (coﬀee mug and feeding bottle) have moderate cor-
relation. Figure 7 shows the scatter diagram of the correlation analysis for each
TQ for 30 grasps using the detergent bottle.
7
Summary and Future Work
This work proposes diﬀerent grasp transfer quality measures to evaluate grasps
with the purpose of transferring objects from one hand to another using a dual-
arm robot or more than one single-arm robot. These grasp transfer quality mea-
sures can determine whether a initial grasp on an object is good enough so that
another hand can easily ﬁnd a good grasp on the same object. This may help
to choose grasps that are more likely to allow transfer of objects. The validation
results showed a high correlation between TQ and TQ∗for most objects.
The results validated in this paper were obtained for one particular hand.
Future work includes the validation of the grasp transfer quality measures using
more objects with diﬀerent shapes and sizes and other diﬀerent robotics hands.

Grasp Quality Measures for Transferring Objects
39
References
1. Roa, M., Su´arez, R.: Grasp quality measures: review and performance. Auton.
Robots 38, 65–88 (2015)
2. Nguyen, V.: Constructing force-closure grasps. Int. J. Robot. Res. 7, 3–16 (1988)
3. Ferrari, C., Canny, J.: Planing optimal grasps. In: IEEE International Conference
on Robotics and Automation, pp. 2290–2295 (1992)
4. Li, Z., Sastry, S.: Task-oriented optimal grasping by multiﬁngered robot hands.
IEEE J. Robot. Autom. 4, 32–44 (1988)
5. Pollard, N.: Parallel methods for synthesizing whole-hand grasps from generalized
prototypes, Technical report AI-TR 1464 (1994)
6. Liu, S., Carpin, S.: A fast algorithm for grasp quality evaluation using the object
wrench space. In: IEEE Conference on Automation Science and Engineering, pp.
558–563 (2015)
7. Borst, C., Fisher, M., Hirzinger, G.: Grasp planning: how to choose a suitable task
wrench space. In: IEEE International Conference on Robotics Automation (2004)
8. Berenson, D., Diankov, R., Nishiwaki, K., Kaganami, S., Kuﬀner, J.: Planning in
complex scenes. In: IEEE-RAS International Conference on Robotics and Automa-
tion (2007)
9. Saut, J., Gharbi, M., Corts, J., Sidobre, D., Simon, T.: Planning pick-and-place
tasks with two-hand regrasping. In: IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (2010)
10. Vahrenkamp, N., Asfour, T., Dillman, R.: Simultaneous grasp and motion planning.
In: IEEE Robotics Automation Magazine (2012)
11. Vahrenkamp, N., Przybylski, M., Asfour, T., Dillman, R.: Bimanual grasp planning.
In: IEEE-RAS International Conference on Humanoid Robots (2011)
12. Rusu, R., Cousins, S.: 3D is here: point cloud library (PCL). In: IEEE International
Conference on Robotics and Automation (ICRA) (2011)
13. Marton, Z.C., Rusu, R., Beetz, M.: On fast surface reconstruction methods for large
and noisy datasets. In: IEEE International Conference on Robotics and Automa-
tion (2009)
14. https://grabcad.com. Accessed February 2017
15. https://123dapp.com. Accessed February 2017
16. Vahrenkamp, N., Krhnert, M., Ulbrich, S., Asfour, T., Metta, G., Dillmann, R.:
Simox: a robotics toolbox for simulation, motion and grasp planning. In: Intelligent
Autonomous Systems, vol. 12, pp. 585–594 (2013)

Application of Robotics
in Shipbuilding

Development of a Customized Interface for a Robotic
Welding Application at Navantia Shipbuilding Company
Pedro L. Galindo1(✉), Arturo Morgado-Estévez1, José Luis Aparicio1,
Guillermo Bárcena1, José Andrés Soto-Núñez2, Pedro Chavera2,
and Francisco J. Abad Fraga2
1 Universidad de Cádiz, Avda. Universidad de Cádiz, nº 10, 11519 Puerto Real, Cádiz, Spain
pedro.galindo@uca.es
2 Navantia S.A., S.M.E., Astillero Bahía de Cádiz, Polígono Astillero s/n, 11519 Puerto Real,
Cádiz, Spain
Abstract. In this paper, a customized interface developed in the framework of
the ROBOT FASE II project is described. This project aimed at improving the
productivity of two FANUC ARCMate 100iC MIG welding robots with R-30iA
controllers mounted in an 8 meters-high mobile gantry crane at Navantia company
in Puerto Real, Spain. The solution designed for welding application by the
University of Cadiz consists of four parts (1) a library of piece templates including
relevant information for each piece to be welded, including obviously the typical
piece geometry shape and dimensions, but also all parameters needed for
welding(sequence, intensity of the arc, waving description,…) and optimized to
get a perfect result by a professional welder team (2) a coordinate measuring arm
used to capture 3D information from the real world, (3) a software to generate
automatically the optimized FANUC welding program using both the template
and the 3D information captured by the arm, adapting the template to real-world
coordinates and orientation, (4) and ﬁnally an FTP interface to transmit the opti‐
mized welding program to each robot for immediate welding operation. The use
of this solution for welding operation has reduced robot programming time from
hours to minutes for a typical structure allowing an important increase of produc‐
tivity at Navantia company.
Keywords: Industry 4.0 · Robotics · Shipbuilding · Welding · Shipyard 4.0
1
Introduction
Through robotics, the value of human labor is increased, resulting in greater economic
return for business and higher wages for workers. Automated technology and robotic
systems are increasingly important in modern shipbuilding manufacturing industries.
Robots for welding, painting, heavy lifting and other tasks, are helping to increase the
productivity at shipyards, while sparing human workers the most dangerous and thank‐
less tasks.
Navantia is a Spanish state-owned shipbuilding company [1], giving service to both
military and civil sectors. It is the ﬁrst shipbuilding company in Spain, the ﬁfth largest
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_4

shipbuilder in Europe, and the ninth largest in the world. By size and technological
capability, it occupies a leading role in European and Worldwide military shipbuilding,
with experience in building technologically advanced ships (frigates, amphibious ships,
patrol vessels, submarines,…) for diﬀerent navies.
Robotics in the shipbuilding industry is neither a new [2] nor an easy task. Tradi‐
tionally, the shipbuilding industry has relied on the labour of a large number of skilled
workers, who worked for many hours in dangerous and highly demanding tasks such as
welding, cutting and painting. However, with the intensive use of robotic applications,
Asian countries such as South Korea, Japan and China have greatly reduced manufac‐
turing time and labour costs, becoming leaders of the market [3]. The reason for this is
the specialization of Asian companies oﬀering a narrow set of products, but highly
competitive and manufactured in an impressively short period of time. To become
competitive, European companies should oﬀer the best product of the highest quality at
a competitive price. This goal is achieved in Navantia thanks to its constant commitment
to innovation and technology. Navantia has a powerful technical oﬃce and makes extra‐
ordinary eﬀorts in Research and Development with the aim of always oﬀering the latest
products and services. The innovative spirit that reigns within the company leads not
just to foreseeing the future needs of clients but also to getting a step ahead with original
projects. It is precisely in this spirit that Navantia decided to consider robotics tech‐
nology for welding more than 5 years ago.
Robot Fase I project, developed in 2012 allowed Navantia, as it is shown in Fig. 1,
the acquisition of two FANUC ARCMate 100iC MIG welding robots with R-30iA
controllers mounted in an 8 meters-high mobile gantry crane at Navantia company in
Puerto Real, Spain. However, the use for production purposes of this equipment was
quite diﬃcult as the programming process was very diﬃcult, time-consuming, and quite
expensive in terms of highly specialized man-hours.
Fig. 1. Welding robotic structure: two FANUC ARCMate 100iC MIG welding robots with
R-30iA controllers mounted in an 8 meters-high mobile gantry crane at Navantia company.
44
P.L. Galindo et al.

Robot Fase II project was developed in 2016 in collaboration with the University of
Cádiz. In this project, the main objective was the improvement of the productivity and
the programming speed using the available infrastructure. The complete solution,
described in the next sections, was originally conceived and undertaken by two research
groups at the University of Cadiz with more than 20 years of experience (in areas such
as Robotics, Artiﬁcial Intelligence, Image Processing, Data Analysis, Simulation,…) in
Industry 4.0 related projects: (i) Intelligent Systems and (ii) Applied robotics.
Intelligent Systems research group took care of all software related questions, and
also designed the software for calibration and automatic generation of robot code.
Applied Robotics research group was in charge of all hardware-related tasks, such
as robot manipulation, maintenance and training.
2
Robotic Welding Solution Description
The shipbuilding process is mainly divided into design, cutting, welding, assembling,
grinding, blinding, and painting processes [4]. Among these manufacturing processes,
welding is the most crucial, expensive, and time-consuming process. For that reason,
the main objective of the project was the use of a robotic arm for welding purposes in a
production environment. This could yield an increased productivity and an improvement
in the working conditions of company workers, greatly reducing their work-related
musculoskeletal disorders [5–7].
Today, welding robots are vital to shipbuilding for their economic feasibility,
production eﬀectiveness, speed, and level of detail. The problem with the use of robotics
in shipbuilding is robot programming [8–10]. Usually, programming an industrial robot
takes much time and may be suitable for jobs consisting in the repetition of a monotonous
motion, applying therefore to identical pieces (same size, shape and orientation). But,
in shipbuilding, workpieces are not identical. What is more, they are usually so large
that their dimensions are prone to large errors (in the order of few millimeters in the best
case) not only for manufacturing accumulated errors but also to unavoidable causes,
such as metal dilatation.
2.1
System Description
The solution to the original problem had some requirements to be fulﬁlled. The main
requirement was that the provided solution should signiﬁcantly reduce the required time
for programming the robot in a typical production situation. A second requirement was
that workpieces to be welded should be considered of variable size. While the shape
among diﬀerent pieces was considered to be nearly constant, diﬀerences in the dimen‐
sions across the three directions of the space could vary signiﬁcantly.
Three diﬀerent solutions where considered to meet these requirements: (i) ﬁnding a
commercial solution, (ii) developing a completely automatic solution and (iii) designing
a semiautomatic process capable of reducing the required programming time.
While some commercial solutions were available for diﬀerent purposes, none of
them had enough ﬂexibility to be adapted to the available robotic system. What is more,
Development of a Customized Interface for a Robotic Welding Application
45

the ﬂexibility needed due to diﬀerent shapes and sizes reduced the options to a few
companies.
Some of them provide solutions for fully automatic welding of open blocks (proﬁles,
T-bars and girders) or determine the welding jobs in its working area by means of an
integrated logic system. However, the system design is not open, and new block shapes
or structures always means a dependency on this company for new developments that
we tried to avoid.
On the other side, other companies oﬀer diﬀerent solutions for programming in place
work pieces (Shipbuilding solutions, panel structures) or manipulated work pieces (any
product). However, its price and the diﬃculty in programming welding sequences for
new workpieces to expand the system without the help of the company motivated us to
discard this option as well.
The completely automatic solution was quickly discarded, as the company was
interested in a solution working in the workshop, not only in the laboratory, compatible
with a huge number of workpiece shapes and sizes and easily adaptable to new structures.
The high complexity of the task and the absence of such solutions in the market,
convinced us to ﬁnd a solution just in the middle, considering an ad-hoc semiautomatic
welding process developed entirely by the University of Cadiz and Navantia.
Once the semiautomatic process was deﬁned, the system was conceived into three
parts. First a 3D acquisition system was required to capture the real dimensions of the
pieces to be welded. Secondly, and given that an enormous range of shapes and sizes
was possible for the workpieces, the deﬁnition of a library of templates was considered,
including those shapes more frequently welded in the typical panel lines in a shipyard.
Templates had to be designed for each diﬀerent workpiece, but its conﬁguration should
be independent of its dimensions to a given tolerance limit. Then, in a real operation,
the worker should select the appropriate template according to the desired shape, collect
from 3D space the real dimensions using the 3D acquisition system in real time and the
software should mix automatically information from both sides (template, 3D real world
coordinates) to generate a welding program in the FANUC language to be loaded into
the robot. And ﬁnally, the program should be transmitted to the robot in an automatic
way.
2.2
3D Acquisition System
There were many alternatives to acquire 3D information. Both mechanical and optical
systems were considered, including coordinate measuring arms, trackers, optical
systems, laser and/or structured light scanners and stereo cameras. The accuracy in the
ﬁnal conﬁguration would be the sum of the inherent error of the chosen technology,
alignment errors in transferring the coordinate system of the 3D capture to the robot
coordinate system and measuring errors due to human intervention.
After a series of testing experiments using diﬀerent equipment solutions in a real
world environment, we decided to use a coordinate measuring arm, as the accuracy of
this solution was in the order of microns, the price was extremely competitive compared
to other alternatives and the interface with the ad-hoc software was guaranteed by the
use of the corresponding software development kit (SDK). The SDK would allow us to
46
P.L. Galindo et al.

easily capture 3D coordinates when the worker pressed the trigger of the arm. These
coordinates were referred to its own internal coordinate system and should be rotated
and translated easily, but this was possible using a simple matrix multiplication using
homogeneous coordinates [11].
The analysis of the market for coordinate measuring arms was quite exhaustive, and
several brands and distributors were considered (ROMER, FARO, NIKON, ECHO
Arm, HEXAGON, Kreon, RPS, CIMCore, etc.). Most of the requirements for the coor‐
dinate measuring arm (accuracy, light weight, number of degrees of freedom, possibility
to add an scanner,…) were fulﬁlled by all suppliers, but the ROMER arm length had no
competence and given the application for which it was going to be used, a 4.5 meters
length ROMER coordinate measuring arm was chosen.
The ROMER absolute Arm features absolute encoders (there is no need for refer‐
encing, when the arm is turned on, it is ready to go), versatility, portability, stability,
light weight and the possibility to adapt a high-performance laser scanner. What is more,
it does not require warm-up time or initialization, thanks to a stable carbon ﬁber structure
and industry leading absolute encoders.
2.3
Adopted Solution
The adopted solution considers a light-weight portable coordinate measuring arm (see
Fig. 2) that the worker uses to capture 3D coordinates. This information is sent to the
software that translates it into a real robotic program using the chosen template, and
automatically transmitted to the robot.
Fig. 2. Schematic description of the proposed welding equipment conﬁguration. 3D coordinate
measuring arm is used to capture 3D information, while welding robots use this information in a
continuous welding operation.
This approach has several advantages. When the robot is welding, the worker may
start taking coordinates of the adjacent piece. This is extremely important, as the worker
can advance the 3D coordinate measuring process while keeping out of the robot
working range. As a consequence, the robot may stay welding 100% of the time, as
welding is much more time consuming than coordinate measuring. In fact, as welding
usually takes one order of magnitude more time than measuring (including arm
Development of a Customized Interface for a Robotic Welding Application
47

movement, anchorage and workpiece measuring), this makes possible that a single
worker manipulates several robots at the same time.
Therefore, using this approach and taking into account the use of a coordinate meas‐
uring arm, the ﬁnal conﬁguration adopted in the ROBOT FASE II Project was the one
shown in Fig. 3.
Fig. 3. Schematic description of the proposed welding solution. A coordinate measuring arm is
used to capture 3D information. The software adapts a previously chosen template using real world
coordinates and generates the robotic program.
3
Software Structure
The software has been developed entirely by the Intelligent Systems and Applied
Robotics research groups of the University of Cadiz, in collaboration with Navantia
company and continuously supported by its most skilled and experienced welding
workers. The software is structured into four parts:
1. A library of templates
2. An automatic generator of FANUC code
3. A friendly Graphical User Interface to have access to all functions and easily manip‐
ulate the whole system
4. Other software components
• An interaction layer to SDKs from ROMER and FANUC companies
• An FTP interface to send the generated program to the robot
• ROBOGUIDE-WeldPro simulation software
3.1
Library of Templates
The library of templates consists of a collection of packages of highly specialized infor‐
mation for each piece. The gathering of this information was made with the help of
highly skilled professional welders, as it is the core of the system, being responsible for
48
P.L. Galindo et al.

the quality of the whole system. The information stored in each template consist of three
parts: the geometry, the welding sequence, and the welding parameters for each section.
The geometry (shape and theoretical dimensions) is stored in a CAD ﬁle using OBJ
format. This ﬁle is used not only for the generation of the corresponding program, but
also for visualization purposes in the user interface, as shown below.
The welding sequence is quite important as it eﬀectively controls distortion. It
consists of a series of steps that deﬁne the order of making the welds. It strongly depends
on the piece shape and dimensions, and is supplied by the skilled welder.
Welding parameters are also very important, but in this case they aﬀect directly the
quality of the welding at the end of the process. It is a complex set of parameters deﬁning
the process for each section of the welding, including the torch orientation and speed,
arc intensity, waving amplitude, waving frequency, waving orientation, supplied mate‐
rial, material, etc.
3.2
Automatic Generator of FANUC Code
Once the 3D coordinates of the real world piece are available, the chosen template is
modiﬁed accordingly. This process uses all the template information and creates a new
program specially ﬁtted to the real world piece. The generation of the FANUC code is
not easy, and it is divided into several sections: initial positioning, approach, welding,
separation, go to home.
Calibration is made by touching four previously identiﬁed and referenced points in
the robot using the coordinate measuring arm. These points have been chosen to be in
a position similar to the vertices of a tetrahedron in order to maximize the accuracy of
the transformation [12].
A relevant feature included in the ﬁnal version of the software is the orientation. The
system uses the captured 3D points to estimate the orientation of the piece in 3D, making
possible to weld similar pieces in any orientation in the space, as the angle of the piece
with respect to the theoretical piece stored in the template library is calculated and
properly considered in the welding program.
3.3
Graphical User Interface
In this project, the design of the Graphical User Interface (GUI) was very important, as
it is intended to be user friendly [13]. It has been developed in Lazarus, a cross-platform
IDE for Rapid Application Development [14]. It has a variety of components ready to
use and a graphical form designer to easily create complex GUIs. In this project, the
Castle Game engine was used for 3D visualization of OBJ ﬁles, allowing the code for
visualization to be quite simple and reduced in length.
The ﬁrst advantage of the developed software (in this case it was the main reason to
choose the Lazarus environment) is that all the code, including libraries, graphical inter‐
faces, etc. is encapsulated in a single executable ﬁle, making it quite easily to be updated,
as copying the new executable ﬁle with the same name is all what is needed. The software
is quite intuitive, as it is shown in Fig. 4.
Development of a Customized Interface for a Robotic Welding Application
49

Fig. 4. Main window of the Graphical User Interface. On the left side, the template tree is shown.
In the middle of the screen we may observe the chosen template in 3D. On the right hand side we
may observe the message window. Three main buttons are observed: in the left side to load the
template, in the center to capture coordinates using the coordinate measuring arm, and in the right
side to send the resulting FANUC code to each robot.
The main window is easy to navigate, as it is designed to be used by the worker using
a single window, consisting of (i) a template section to choose the desired template, (ii)
a visualization area to show in 3D the chosen piece in 3D, being possible to shift, rotate,
zoom in and out,…quite easily using the mouse, a message window to send some feed‐
back to the user of the evolution of the process and (iii) three main buttons, one to load
the template, another one to capture the real 3D coordinates with the coordinate meas‐
uring arm and the other one to send the automatically generated code to the FANUC
robot using FTP.
The software is quite eﬃcient in the sense that it is able to work with complex pieces
in 3D in an ordinary computer, taking only a few seconds to generate the FANUC code
at the end of the capture process.
Finally, the main advantage is that the software has been entirely developed by the
University of Cadiz, and therefore any modiﬁcation or improvement needed in the future
may be implemented quite easily.
3.4
Other Software Components
The rest of the software components are intended to communicate with the ROMER
arm and the FANUC robot.
In the ﬁrst case, an SDK has been available to communicate the software with the
ROMER arm, capturing the coordinates in 3D and accessing them in real time. The
capture is determined by the user by manually activating the rugged and accurate touch-
trigger probe of the ROMER arm.
50
P.L. Galindo et al.

In the case of the FANUC robot, two packages of software have been used. The ﬁrst
one is a FANUC SDK to have access to all parameters of the robot (specially 3D coor‐
dinates and robot joint angles) in real time. The other one is a FTP interface, that was
used to send the generated code to each of the robots.
It is worth mentioning that the software is manipulated by the worker remotely, using
a remote application (TeamViewer) and a ruggerized tablet connected via WIFI.
Finally, in the development of templates, the ROBOGUIDE-WeldPRO program‐
ming tool was used (see Fig. 5), as this tool allowed us to simulate two FANUC ARC
Mate 100iC arc welding robots in 3D space. It proved to be extremely accurate and easy,
and very useful for developing and testing purposes.
Fig. 5. ROBOGUIDE design with two FANUC ARcMate 100iC robots, strongly resembling the
real world conﬁguration in Navantia company
4
Conclusions
In this paper, the development of a customized interface for a robotic welding application
at Navantia shipbuilding company in the framework of the ROBOT FASE II project has
been described.
This software allows the reduction of the programming time from hours to a few
seconds by a non-experienced worker. The performance improvement in the welding
process is quite remarkable. It has been designed using a user-friendly Graphical User
Interface, being quite easily updated, improved and expanded.
The FANUC ROBOGUIDE programming tool has been used to simulate the real
environment and has been used for developing and testing purposes.
Future work will be devoted to replace the coordinate measuring arm with an arti‐
ﬁcial vision system in order to automatically capture the 3D real world environment.
Development of a Customized Interface for a Robotic Welding Application
51

References
1. Navantia Homepage. http://www.navantia.es. Last accessed 10 July 2017
2. Reeve Jr., R.C., Rongo, R.: Shipbuilding Robotics and Economics NSRP Ship Production
Symposium on Commercial Competitiveness for Small and Large North American
Shipyards, pp. 25–27 (1995)
3. Mickeviciene, R.: Global shipbuilding competition: trends and challenges for Europe, The
Economic Geography of Globalization. In: Pachura, P. (ed.) (2011). http://www.
intechopen.com/books/the-economic-geography-of-globalization/global-shipbuilding-
competitiontrends-and-challenges-for-europe
4. Kim, C.-S., Hong, K.-S., Han, Y.-S.: Welding Robot Applications in Shipbuilding Industry:
Oﬀ-Line Programming, Virtual Reality Simulation, and Open Architecture. https://www.
intechopen.com/books/industrial_robotics_programming_simulation_and_applications/
welding_robot_applications_in_shipbuilding_industry__oﬀ-line_programming__virtual_
reality_simulatio. Last accessed 10 July 2017
5. Keefe, J.: Shipconstructor drives automatic welding robots. Mar. News Mag. 28(2), 24–27
(2017)
6. Ogbemhe, J., Mpofu, K.: Towards achieving a fully intelligent robotic arc welding: a review.
Ind. Robot Int. J. 42(5), 475–484 (2015)
7. Shi, L., Tian, X.C., Zhang, C.H.: Automatic programming for industrial robot to weld
intersecting pipes. Int. J. Adv. Manuf. Technol. 81(9), 2099–2107 (2015)
8. Ku, N., Ha, S., Roh, M.-I.: Design of controller for mobile robot in welding process of
shipbuilding engineering. J. Comput. Des. Eng. 1(4), 243–255 (2014)
9. Larkin, N., Short, A., Pan, Z., van Duin, S.: Automated programming for robotic welding.
Trans. Intell. Weld. Manuf. 1(1), 48–59 (2017)
10. Wu, G., Wang, D., Dong, H.: Oﬀ-line programmed error compensation of an industrial robot
in ship hull welding. Lecture Notes in Artiﬁcial Intelligence, vol. 10463, pp. 135–146 (2017)
11. Bez, H.E.: Homogeneous coordinates for computer graphics. Comput. Aided Des. 15(6), 361–
365 (1983)
12. Garner, L.E.: Outline of Projective Geometry. Elsevier Science Ltd., Amsterdam (1981)
13. TechRepublic 
page. 
http://www.techrepublic.com/blog/10-things/10-things-that-make-
software-user-friendly/. Last accessed 10 July 2017
14. Lazarus Homepage. http://www.lazarus-ide.org. Last accessed 10 July 2017
52
P.L. Galindo et al.

Towards Automated Welding in Big Shipbuilding Assisted
by Programed Robotic Arm Using a Measuring Arm
Arturo Morgado-Estevez1(✉), Pedro L. Galindo1, Jose-Luis Aparicio-Rodriguez1,
Ignacio Diaz-Cano1, Carlos Rioja-del-Rio1, Jose A. Soto-Nuñez2, Pedro Chavera2,
and Francisco J. Abad-Fraga2
1 Faculty of Engineering, University of Cadiz, Av. de la Universidad de Cádiz, nº 10, 11519
Puerto Real, Cádiz, Spain
arturo.morgado@uca.es
2 Navantia S.A., S.M.E, Bay of Cádiz Shipyard, Polígono Astillero, s/n,
11519 Puerto Real, Cádiz, Spain
Abstract. This paper presents an automated robotic welding system adapted for
shipbuilding in large shipyards. This solution has been devised in the shipyard of
Navantia located in the south of Spain, in the context of ROBOT FASE II R&D
project. It also presents the human teams that have developed this welding system.
The article explains the 3 parts that make up the welding system. The location of
the robotic welding arm of the Fanuc brand is detailed in the ﬁrst part. In addition,
the gantry and spatial coordinate axes are described and indicated where the
robotic arm is housed. The second part contains the system capture of the coor‐
dinates in the space for the reading of the singular points to be soldered. These
points are measured by a portable measuring arm. The last part is composed of
the system of communication between the diﬀerent parts throughout the
computer. The computer is responsible for synchronizing the measuring arm and
the robotic welding arm by translating the points to be soldered.
Keywords: Robotic welding · Shipbuilding · Automated welding · Fanuc robotic
arm · Portable measuring arm · Industry 4.0 · Shipyard 4.0 · Naval industry
1
Introduction
In our days, it is necessary to increase the automation and robotization of the naval sector
with the aim of increasing productivity and at the same time becoming more competitive
in the market [7, 13].
Currently, the welding process occupies a big portion in shipbuilding construction;
more than 65% of entire welding processes were automated in shipbuilding area both in
the horizontal position and in the vertical position [1].
There are several sources of variability in robotic arc welding. Firstly, the accuracy
of component manufacture and assembly involves variability. Thus, attention must be
given to parts manufacturing processes, assembly tooling and quality control to ensure
that dimensional tolerances on assemblies for robot welding can be maintained within
acceptable limits.
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_5

Secondly, the location of assemblies on the work manipulator leads to variability.
Additional holding ﬁxtures and a precise ﬁxture manipulator location system are
required in the manner of those commonly used in NC machining systems. An accept‐
able level of repeatability is often the major system speciﬁcation requirement for robot
arc welding facilities. As with manual welding, careful attention is required to be given
to weld sequencing [8].
The need for autonomous welding tasks has increased recently in shipyards to
improve productivity. Autonomous welding tasks using robots have been used in many
applications for more than 20 years. However, the robots work only at a ﬁxed location,
and a crane is usually used to move the robots from one working location to another [9].
A survey of the late 1980s revealed that the borrowed technology and know-how
embodied in the imported robots are playing a crucial role to improve their competi‐
tiveness by upgrading the quality and conﬁdence of the products, and by increasing the
ﬂexibility of the production lines. In short, the developing countries which have other‐
wise been stagnant in permanent backwardness of technology could easily step up to
produce the products of same quality as high as the advanced countries [12].
In the 90’s there were several robotized solutions. The robot of Weston in 1989 was
a very attractive robotic welding system because of its robustness and ﬂexibility.
Skjolstrup and Ostergaard in 1994 achieved the next step in the technological advance
of shipbuilding [2].
In addition, in an eﬀort to increase productivity of welding process and to reduce the
impact to workers from industrial disaster, the research in the ﬁeld of welding automa‐
tion is sharply increasing in shipbuilding industry. DSME (Daewoo Shipbuilding and
Marine Engineering) had developed and successfully applied a welding robot system
for grand assembly line in OKPO shipyard in 1997 and has recently developed a multi
welding robot system for the sub-assembly line [11].
In the 90’s it was possible to ﬁnd a ship welding robotic system (SWERS). A robotic
solution was easy to use, that is, where an unskilled worker can operate it out of “common
sense”. This robot was not autonomous, it could be taught directly by the workers. This
project was a collaboration between National University of Singapore, Gintic Institute
of Manufacturing Technology and the Singapore Productivity and Standards Board [3]
(Fig. 1).
At the beginning of the 21st century, the Industrial Automation Institute (IAI) in
Spain developed for “Astilleros Españoles S.A.”, a robotic welding system called
“ROWER 1” that could be used in a closed block. The robot moved like a spider and
had four legs capable of extending and contracting. It could move autonomously [4]
(Fig. 2).
54
A. Morgado-Estevez et al.

Fig. 2. Robot ROWER 1
In 2011, the RRXC [1] showed up, which was composed of a 6-axis modularized
controller, and an auxiliary transportation device. The entire cross section of the RRXC
is small enough to be placed inside the double-hulled structures via a conventional access
hole of 500 × 700 mm2, from the exterior of the structure of shipyard. This robot was
also developed in its second and third version (RRX2 and RRX3) with more improve‐
ments and features as well as more axes [10]. This research was supported in part by
Daewoo Shipbuilding and Marine Engineering (DSME) of Republic of Korea [5, 6].
More recently, we could ﬁnd other welding robotic solutions based in autonomous
robots, with six-axis manipulator, with multiple welding and several movement types.
Fig. 1. Robot SWER
Towards Automated Welding in Big Shipbuilding
55

2
Robotic Welding System Architecture
The robotic welding system architecture shown below has the elements and intercon‐
nections needed to accomplish the robotic welding process. This architecture contains
mainly three diﬀerent subsystems:
• The robotic group system
• The measuring arm system
• The computer system
The robotic group system is formed by a blue gantry and two robotic groups attached
to it. This system is in charge of controlling and monitoring the welding process carried
out by the robotic arms. These two robotic groups contain a controller that manages all
the signals and movements performed by the two robotic groups: a ‘group one’ or
‘yellow group’ formed by a FANUC robot capable of moving around a limited work‐
place and a ‘group two’ or ‘orange group’, which is able to move the ‘yellow group’
and extend the motion capabilities of it. This robotic group has also a welding group in
charge of the welding process (Fig. 3).
Fig. 3. Robotic welding system architecture
In this architecture a computer system in charge of controlling and monitoring the
robotic group system and the measuring arm group can be found. This measuring arm
group has the responsibility of taking the coordinates of points in order to calculate the
path to weld right after.
56
A. Morgado-Estevez et al.

2.1
Joint Research Group Navantia-University of Cadiz
This automated robotic welding system has been developed between the R&D group of
Navantia company and two research groups from the University of Cadiz.
The research group at Navantia is composed of an expert welder in automated
welding supervised by the workshop manager. They have been in charge of carrying out
all the tests of operation to put it into production. The test pieces and homologation of
the operation of the welding system have been carried out by the members of Navantia.
The most important contribution of Navantia research group has been the experience
and knowledge of the welding parameters that have been programmed in the robot.
The Applied Robotics research group has worked on the interconnection of the
communication system between the coordinates system of the robotic arm and the
computer. In addition, it has wirelessly interconnected via wiﬁ the measuring arm and
the computer. The group has collaborated in the performance tests to integrate the system
into the production line. The most important contribution of the Applied Robotics
research group has been the interpretation of all the low-level orders generated by the
robots and sent to the computer. In this way, total control over the robotic welding system
has been achieved.
The Intelligent Systems research group has developed all the software needed for
the synchronization between the robotic arm and the measuring arm. The group has
developed a new algorithm that improves the accuracy of the robotic arm and the ‘group
two’. In addition, a new concept of translation of the coordinate system has been devised
by this research group obtaining weld beads with an accuracy of less than 1 mm.
This joint research group between Navantia and the University of Cadiz has rein‐
forced knowledge in automated robotic welding systems applied to the naval industry
setting new challenges for future work within the company.
The members who have participated in the development of the automated robotic
welding system appear in the following photo.
2.2
Description of Robotic Arm Coordinate Systems
The following ﬁgure shows the structure of the entire welding system. The blue gantry
is situated in a big workshop in Navantia. It is composed by two systems of three axes:
the ‘group one’ or ‘orange group’, and in the end of every ‘group one’, there is a ‘group
two’ (or ‘yellow group’) where the robotic arm is situated.
The three coordinate axis system is described below.
Blue gantry: In the Fig. 4, we can see the big gantry. It allows movement on the X-
axis. The gantry is moved manually by an operator. The controller is located at the base
of the gantry. It allows powerful movements, but it is not very accurate. Before starting
the procedure for welding automatically, the gantry must be positioned close to the area
to be welded.
Towards Automated Welding in Big Shipbuilding
57

Fig. 4. Joint research group Navantia-University of Cadiz
‘Group two’ (orange): In the Fig. 5, the ‘orange group’ can be distinguished. It
permits movements in three spatial directions (X, Y, and Z) with full precision. ‘Group
Fig. 5. The entire welding system.
58
A. Morgado-Estevez et al.

two’ is automatically moved by the Fanuc robot control system. In the sequence of
automated welding, this group intervenes reaching greater distance at the time of
welding. The X-axis can be moved 2 m, the Y-axis, 10 m and the Z-axis, 6 m. The
accuracy of that system of coordinate axes is less than 1 mm. The ‘group 2’ is suﬃciently
accurate for arc welding (Fig. 6).
Fig. 6. ‘Group one’ and ‘Group two’
Fig. 7. ‘Group one’
Towards Automated Welding in Big Shipbuilding
59

‘Group one’ (yellow): The Fanuc ARC Mate 100iC/12 robot has 6 DoF and it permits
extremely precise movements in three spatial directions (x, y and z) and gives it any
orientation (w, p and r). The robot is positioned inverted at the end of the Z-axis of ‘group
two’. The 30iB robot controller is mounted above the gantry next to the Lincoln Electric
welding system. It has more resolution than the ‘group one’. The Fanuc robot has a
precision greater than 0.08 mm. ‘Group one’ is connected to the computer via a RJ45
connector. The computer transfers the program code to the Robot’s console (TPE). The
program will be executed from the TPE. In the following ﬁgure, welding ‘group one’ is
shown (Fig. 7).
The following table summarizes the technical characteristics of the automated
robotic welding system (Table 1).
Table 1. Features of the automated robotic welding system.
Parts
DoF num.
Axes
Range (m)
Precision (mm)
Blue gantry
1
X
30
>20
Group two (orange)
3
X, Y, Z
2, 10, 6
>1
Group one (yellow)
6
x, y, z, w, p, r
1.4
>0.08
2.3
System for Picking up Singular Points
The portable measuring arm is a key component in the robotic welding system. The main
purpose of this measuring system is to take 3D points following a welding template
loaded in the software control system.
This measuring arm counts with a number of features that make it suitable to be used
as part of the robotic welding system in the shipyard. To start with, the arm contains a
magnet basement what makes it easier to place in a metallic environment. The operator
can take points in a speciﬁc place and move the measuring arm to another while the
robotic system is executing the welding routine. Furthermore, the arm has a battery
included and a spare one, giving the system even more ﬂexibility, since it is not necessary
to have wires connected.
The measuring arm installed is a high-precision-7-DoF arm with 4.5 m of length,
which makes it able to reach a great variety of far and diﬃcult access points in the
workplace. Besides, this arm has three diﬀerent types of proves, this fact helps the oper‐
ator to take points in diﬀerent location and access (Fig. 8).
The communication system between the control software installed in the computer
and the measuring arm can be wire or wireless; in this case, we use a wireless commu‐
nication in order to give the system more ﬂexibility. The arm counts with a speciﬁc
communication protocol that can manage the messages sent and received from the
computer wirelessly and give the developer an easy way to control and monitor all the
signals and points taken.
60
A. Morgado-Estevez et al.

2.4
Computer System
The computer system has an important role in the whole system. It is the part in charge
of managing the robotic welding system and it has the software to control the welding
process installed.
This computer has connections with diﬀerent systems. It is connected to a local
network to control and monitor all the signals from the robotic arm (‘yellow group’) and
the ‘orange group’ in a remote way. This computer communicates via Wi-Fi or USB
with the portable measuring arm in order to get the points taken and monitor diﬀerent
signals from it. In addition, the computer has a tablet associated; this tablet is connected
via Wi-Fi and replies the whole computer system. This helps the operator to be in contact
with the computer and operate the control software in every moment, everywhere in the
workspace, as being in front of the computer during the complete welding process is not
needed.
On the other hand, the computer system has a speciﬁc software installed, developed
to carry out the robotized welding process in an automatic way. This speciﬁc software
has ﬁve diﬀerent modules:
• Loading module
• Template visualization module
• Creation and modiﬁcation views module
• Capture points module
• Communication module
This ﬁrst module is in charge of loading the templates programmed that will be used
in the next capture of points with the measuring arm. Afterwards, the software will ask
for capturing points following the welding path from the template loaded. Once all the
Fig. 8. Portable measuring arm
Towards Automated Welding in Big Shipbuilding
61

points are captured, the software generates a program. This program will be transmitted
via Wi-Fi from the computer system towards the controller of the robotic system
(‘yellow group’ and ‘orange group’), ﬁnally the operator will supervise the program and
run it (Fig. 9).
Fig. 9. Communication with the computer system
With regard to the communication module, it is important to say that this module
allow the user to establish a connection between the software integrated in the computer
system and the two-robotics systems via FTP.
The robotic group can send the computer system information such as a list of
programs loaded in the robot, the state of the signals, or the welding parameters loaded,
and the computer system can upload new programs generated to the robot.
3
Conclusions
It has been presented the solution of an automated robotic welding system developed
by the Navantia R&D team and the Applied Robotics and Artiﬁcial Intelligence research
group of the University of Cadiz.
The Applied Robotics research group from the University of Cadiz has been mainly
in charge of developing and managing all the communications between the computer
system installed and the physical components of the architecture forming the robotic
welding system and measuring system. The Intelligent Systems research group has
developed the control software to manage the whole welding process. In addition,
Navantia research group has supported and guided all research activities thanks to its
experience in naval welding processes.
It has been taken the most of the infrastructure of one of Navantia’s workshops,
formed by a big blue gantry and two orange motion groups attached to it, holding each
of them one robotic arm.
62
A. Morgado-Estevez et al.

References
1. Ko, S.H., Kim, J.G., Moon, H.S.: Automatic welding robot system for the horizontal position
in the shipyard. In: 2012 12th International Conference on Control, Automation and Systems,
JeJu Island, pp. 240–245 (2012)
2. Lee, D.: Robots in the shipbuilding industry. Robot. Comput. Integr. Manufact. 30(5), 442–
450 (2014)
3. Ang Jr., M.H., Lin, W., Lim, S.: A walk-through programmed robot for welding in shipyards.
Ind. Robot Int. J. 26(5), 377–388 (1999)
4. De Santos, P.G., Armada, M., García, E., Akinﬁev, T., No, J., Prieto, M., Nabulsi, S., Cobano,
J.A., Ponticelli, R., Sarriá, J., Reviejo, J., y A. Ramos, C.S.: Desarrollo de robots caminantes
y escaladores en el IAI-CSIC, Congreso español de informática CEDI (2007)
5. Lee, D., Ku, N., Kim, T.-W., Kim, J., Lee, K.-Y., Son, Y.-S.: Development and application
of an intelligent welding robot system for shipbuilding. Robot. Comput. Integr. Manufact.
27(2), 377–388 (2011)
6. Ku, N., Cha, J., Lee, K.-Y., Kim, J., Kim, T., Ha, S., Lee, D.: Development of a mobile welding
robot for double-hull structures in shipbuilding. J. Marine Sci. Technol. 15(4), 374–385
(2010). doi:10.1007/s00773-010-0099-5
7. Kim, J.-H.: Automation and robotization of production processes in shipbuilding. In: 2008
International Conference on Control, Automation and Systems, Seoul, South Korea, pp. 35–
41 (2008). doi:10.1109/ICCAS.2008.4694703
8. Middle, J.E., Sury, R.J.: Advancing the application of robotic welding. Prod. Eng. 63(7), 38
(1984)
9. Ku, N., Ha, S., Roh, M.-I.: Design of controller for mobile robot in welding process of
shipbuilding engineering. J. Comput. Des. Eng. 1(4), 243–255 (2014)
10. Kim, J., Lee, K.-Y., Kim, T., Lee, D., Lee, S., Lim, C., Kang, S.-W.: Rail running mobile
welding robot ‘RRX3’ for double hull ship structure. IFAC Proc. 41(2), 4292–4297 (2008)
11. Kang, S.W., Youn, H.J., Kim, D.H., Kim, K.U., Lee, S.B., Kim, S.Y., Kim, S.H.:
Development of multi welding robot system for sub assembly in shipbuilding. IFAC Proc.
41(2), 5273–5278 (2008)
12. Torii, Y.: Robotization in Korea: trend and implications for industrial development. Technol.
Forecast. Soc. Change 35(2–3), 179–190 (1989)
13. Boekholt, R.: Welding Mechanisation and Automation in Shipbuilding Worldwide:
Production Methods and Trends Based on Yard Capacity. Elsevier (1996). doi:
10.1533/9780857093196.1
Towards Automated Welding in Big Shipbuilding
63

Cognitive Architectures

Cybersecurity in Autonomous Systems:
Hardening ROS Using Encrypted
Communications and Semantic Rules
Jes´us Balsa-Comer´on1(B), ´Angel Manuel Guerrero-Higueras1,
Francisco Javier Rodr´ıguez-Lera2, Camino Fern´andez-Llamas1,
and Vicente Matell´an-Olivera1
1 Research Institute on Applied Sciences in Cybersecurity (RIASC),
Universidad de Le´on, Av. de los Jesuitas s/n, 24008 Le´on, Spain
jbalc@unileon.es
2 Computer Science and Communications Research Unit (CSC),
University of Luxembourg, Luxembourg City, Luxembourg
Abstract. Cybersecurity in autonomous systems is a growing concern.
Currently most research robotic systems are built using ROS frame-
work, along with other commercial software. The goal of this paper is to
improve ROS security features by using encrypted communications and
semantic rules to ensure a correct behavior. To encrypt communications,
Advanced Encryption Standard algorithm has been applied. Then, the
framework ROSRV has been used to deﬁne semantic rules for ROS mes-
sages. In order to test this proposal, two experiments have been carried
out: in the ﬁrst one, plain-text messages are not allowed and must be
blocked; in the second one, rules for detecting denial of service attacks are
tested against a real attack performed on a Real-Time Locating System,
used by a mobile robot to estimate its location.
Keywords: Autonomous systems · Cybersecurity · Robotics · ROS
1
Introduction
Several vectors of attack have been described for robotic systems [5]. This paper
is focused on the cybersecurity problems associated with mobile service robots. In
particular, we are concerned about the problems associated to the middlewares
that are generalizing as development frameworks.
Robot Operating System (ROS) [9] has become the most popular frame-
work for developing robotic applications. It started as a research project, but
currently most manufacturers of commercial platforms use ROS as the de facto
standard for building robotic software. Just in July 2016, ROS binary packages
were downloaded 8,441,279 times [1], counting only downloads from the main
repository in the oﬃcial site, not the numerous mirrors around the globe.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_6

68
J. Balsa-Comer´on et al.
ROS is basically a set of libraries for robotics similar to operating system ser-
vices, providing hardware abstraction for sensors and actuators, low-level device
control, and inter-process communication. ROS framework is a message-passing
distributed system. Computation takes place in processes named nodes which
can receive and send messages.
Nodes publish messages into information buﬀers called topics. For instance, a
node can be in charge of accessing a sensor, performing the information process-
ing, and publishing it as an information structure on a named topic. Another
node can subscribe to this topic, that is, read its information. Then this process
can make a decision about the movement of the robot. Next, this node will
publish the commands in another topic to send them to the motors.
ROS nodes can be running in the same computer or in diﬀerent ones. Usual
conﬁguration is composed by at least one ROS Master and some clients. ROS
Master is the key element in the ROS system. It runs as a nameservice and
manages registration information about all topics used by nodes. The Master is
updated in real time by the running nodes, which provide information about top-
ics they publish/subscribe and the type of message used by each topic. Figure 1
describes the conceptual model of ROS, showing the Master with two nodes,
Talker and Listener. Node Talker publishes messages of type foo in the topic
bar, while node Listener is subscribed to topic bar to receive its messages.
Fig. 1. Conceptual model of ROS topics.
Unfortunately, no security was considered in the design of this distributed
communication mechanism. Major security problems in the current version of
ROS are plain-text communications, unprotected TCP ports, and unencrypted
data storage. Thus, ROS poses privacy risks to research organizations, companies
and individuals. The plain transport of ROS may suﬀer eavesdropping, spooﬁng
or denial of service (DoS). Intruders may compromise the privacy of ﬁnal users
by accessing cameras or sensors readings, or by tracking a person with a com-
promised ROS-based robot. In particular, when developing assistance robots,
privacy of patients and caregivers is a major concern.
The proposal here presented to improve ROS security includes two
approaches. First, we apply encryption algorithms to provide a security layer
to ROS data. Second, we deﬁne semantic rules which provide some meta-control
for speciﬁc behaviors. For instance, we could deﬁne a rule to improve safety by
ignoring decisions about the movement of the robot when a person is too close.

Cybersecurity in Autonomous Systems: Hardening ROS
69
A performance evaluation of hardening ROS by encrypting its communica-
tions with the Triple Data Encryption Algorithm (3DES) [11] was studied in [8].
In the present work, we apply Advanced Encryption Standard (AES) to encrypt
messages that selected nodes publish. Subscribing nodes are required to decrypt
the messages in order to use the data.
The second measure presented in this work is focused on both security and
safety. Securing communications is just one dimension in the cybersecurity of
autonomous systems. If we want to see these machines working in environments
like homes, we need to secure navigation abilities and interaction mechanisms, to
avoid manipulated or malicious behaviors and make robots reliable assistants.
With this objective in mind, we include semantic rules that control messages
passed between ROS nodes. We use the ROSRV framework [7], which allows to
monitor safety properties and enforce security policies. We deﬁne two rules as
proof of concept. The ﬁrst one aims to reject any message that is not encrypted.
This rule provides conﬁdentiality, but may also provide integrity and authenti-
cation. The second rule is in charge of detecting cyberattacks against Real-Time
Locating Systems (RTLSs), which are commonly used by autonomous robots.
The rule deﬁned is able to discard the location values given by the RTLS system
when it is suﬀering a Denial of Service (DoS) attack.
Two experiments were carried out to test the proposal. The ﬁrst one consisted
on encrypting the messages that a speciﬁc node publishes in a topic. Another
node subscribes to the topic and deciphers messages in order to use the data. At
the same time, other node publishes plain-text messages to the same topic. The
monitor implemented with the rule for detecting encrypted messages analyses
the topic data and blocks plain-text messages. In the second experiment, a DoS
attack is performed against the beacons of a RTLS system. Location messages
published by the RTLS node are discarded by the second monitor if they are
aﬀected by the attack.
The following sections that this paper includes are organized as follows:
Sect. 2 describes systems, tools, and environment used for the experiments, and
speciﬁes the procedure followed; Sect. 3 shows the executed tests and discusses
the results that can be extracted from the data analysis; Sect. 4 summarizes
conclusions and future work envisioned.
2
Materials and Methods
The experiments implemented were conducted in an indoor mock-up apartment,
shown in Fig. 2, located at the Robotics Lab of the University of Le´on (Spain).
OrBiOne, a ROS-based robot, was used as the main platform for testing the
developed proposal.
For the experiment with encrypted messages, a notebook was used for pub-
lishing both plain-text and encrypted messages into the topic /chatter, which the
robot was subscribed to. Robot and notebook were running Ubuntu 14.04 LTS
with ROS Indigo installed on it, and communication between the two hardware
platforms was made via WiFi connection.

70
J. Balsa-Comer´on et al.
Fig. 2. Plane of the Robotics Lab. Red dots show beacons location. Robot image shows
OrBiOne location during the experiments.
2.1
OrBiOne and KIO RTLS
OrBiOne is an assistant robot developed by Robotnik1. It has several sensors,
such as a RGBD camera, a laser sensor, and an inertial unit. It can operate a
manipulator arm attached to his torso, and it has a wheeled base for moving
around the room. OrBiOne includes a wireless access point, which allows WiFi
communications with other robots or computers. The software to control the
robot hardware is based on ROS framework.
KIO RTLS is a commercial solution for indoor localization, manufactured by
Eliko2. KIO estimates the real-time position of a mobile transceiver, called tag,
in either two or three dimensional space. The position is calculated using radio
beacons, called anchors, that have to be placed around the room in ﬁxed points.
Red dots in Fig. 2 show the distribution of six anchors in the Robotics Lab.
KIO RTLS has four types of beacons: A, B, C, and D. Each of the six beacons
used in the experiments has its own identiﬁer: 408A, 411A, 408B, 412B, 501C
and 401D. Thus there are two A-beacons, two B-beacons, one C-beacon and
one D-beacon. The tag needs to receive signals from at least one A-beacon, one
B-beacon, one C-beacon and one D-beacon in order to calculate a 3D location.
If only the 2D location is needed, just the signal of three beacons, each one of a
diﬀerent type, is required to estimate the position.
Figure 3 shows a KIO tag placed on the robot OrBiOne. Also, one of the
KIO anchors is detailed. This anchor was placed in the ceiling of the mock-up
apartment. The size of the anchor has been enlarged in the picture (tags and
anchors have the same size).
1 http://www.robotnik.es/manipuladores-roboticos-moviles/rb-one/.
2 http://www.eliko.ee/kio-rtls/.

Cybersecurity in Autonomous Systems: Hardening ROS
71
Fig. 3. OrBiOne robot with KIO RTLS system.
2.2
Encrypting Communications
As mentioned in the Introduction, the performance of hardening ROS by cipher-
ing communications with the 3DES algorithm was evaluated with positive results
in a previous work.
In this work we have used another well known symmetric encryption algo-
rithm, AES, recommended by the National Institute of Standards and Tech-
nology (NIST) [2]. It [4] is a symmetric key block cipher that has variable key
length of 128, 192, or 256 bits. AES is able to encrypt datablocks of 128 bits in
10, 12 and 14 rounds, depending on the key size. Due to AES ﬂexibility [6], it is
suitable for both hardware and software implementation.
The library PyCrypto 2.6.13 was used to apply AES algorithm to the node
that encrypts ROS messages.
2.3
ROSRV
ROSRV is a runtime veriﬁcation framework integrated seamlessly with ROS. It
aims to monitor safety properties and enforce security policies.
Its core is a monitoring infrastructure that intercepts, observes and option-
ally modiﬁes messages passing through the ROS system. ROSRV checks system’s
runtime behavior against user-deﬁned properties, triggering desired actions when
verifying these properties. It also controls system state and execution of com-
mands by enforcing an access policy to address security concerns.
ROSRV allows to ensure safety properties based on event sequences. Event
handlers, called monitors, are deﬁned with rules using a C++ based syntax. For
3 https://pypi.python.org/pypi/pycrypto/2.6.

72
J. Balsa-Comer´on et al.
automatic monitor generation out of formal speciﬁcations, ROSRV depends on
ROSMOP4 framework which generates code for monitors from the deﬁned rules.
It is important to note that the last available version of ROSRV5 was imple-
mented for ROS Groovy, which is deprecated since July 2014. Some modiﬁcations
in the source code were needed in order to get ROSRV working in ROS Indigo.
In the experiments implemented, two monitors were deﬁned to be used by
ROSRV. The ﬁrst one aims to detect whether a message is encrypted, and block
it when it is not. The second monitor is able to detect a DoS attack by analyzing
the data gathered by the KIO tag.
Monitor 1. Algorithm 1 describes the behavior for Monitor 1. It uses the ent
tool6 to identify whether a message is encrypted. This tool performs a variety of
statistical tests on the stream of bytes passed as input, and reports the results.
It is useful for evaluating pseudorandom number generators for encryption and
statistical sampling applications, compression algorithms, and other applications
where the information density of a ﬁle is of interest.
Values calculated by ent are: Entropy (S), Chi-square Test (χ and χerr),
Arithmetic Mean (μ), Monte-Carlo value for Pi (π), and Serial Correlation Coef-
ﬁcient (r). The ﬁrst one, S, shows the information density of the contents of the
message, expressed as a number of bits per character. Values close to 8.00 denote
a high entropy in the data [10], as happens in ciphered data. S is the best indi-
cator to detect encrypted messages [3], however it does not work ﬁne with short
messages, which is the case with most ROS topics.
With messages shorter than 200 bytes, there is not enough data to get a
reliable value for entropy. Then, we have to check the value of μ, which is simply
the result of summing all the bytes in the message and dividing by the message
length. If the data is close to random, μ result will approximate to 127.5, accord-
ing to ent documentation. In our tests, μ values greater than 100 are obtained
only for encrypted messages, so they can be clearly identiﬁed. In the cases when
μ is lower than 100, we need to also consider the results of Chi-square Test,
the most commonly used test for the randomness of data. Values of χ greater
than 290 along with χerr greater than 10% identify encrypted messages properly
when μ value is lower than 100.
Monitor 2. Listing 1.1 shows some data gathered by the KIO tag. Each line
represents a location estimate. Values of columns 2, 4, 6, and 8, show the identi-
ﬁer of the anchors whose data has been received. Columns 3, 4, 7, and 9 show the
distance from the respective anchors to the tag. Columns 10 and 11 represent the
tag identiﬁer and the timestamp. Last three columns show the 3D coordinates
of the location estimate.
4 https://github.com/Formal-Systems-Laboratory/ROSMOP/.
5 https://github.com/Formal-Systems-Laboratory/ROSRV/.
6 http://www.fourmilab.ch/random/.

Cybersecurity in Autonomous Systems: Hardening ROS
73
Algorithm 1. Monitor 1
Input: msg
▷Encrypted message
Output: IsEncrypted
▷Boolean value
1: S, χ, χerr, μ, π ←ent msg
▷Analyse msg with ent
2: if μ > 100 then
3:
return TRUE
4: else
5:
if χ < 290 and χerr > 10% then
6:
return TRUE
7:
else
8:
return FALSE
9:
end if
10: end if
1 003c 408a 684 408b 351 501c 263 401d 659
4062
1214180
00000000
00000000
00000000
00000000
3.40
1.34
1.54
Listing 1.1. Data gathered by the KIO tag.
A DoS attack may be detected when no data is received from any of the
anchors. When this happens, the tag gathers “0000” as the anchor identiﬁer and
“0” as the distance to the tag. Listing 1.2 shows the data gathered by the KIO
tag when the anchor 408A is suﬀering a DoS attack.
1 003c 0000 0 408b 351 501c 265 401d 664
4062
6419526
00000000
00000000
00000000
00000000
3.47
1.41
1.57
Listing 1.2. Data gathered by the KIO tag when suﬀering a DoS attack.
Algorithm 2 shows the behavior of Monitor 2. It is able to detect a DoS
attack on the KIO RTLS system, by analysing the data obtained by the KIO
tag.
Algorithm 2. Monitor 2
Input: msg
▷KIO RTLS message
Output: DoSDetected
▷Boolean value
1: regex ←(.* 0000 0 .*)
2: if regex in msg then
3:
return TRUE
4: else
5:
return FALSE
6: end if
2.4
Method
Two experiments were carried out. First one was intended to verify that Monitor
1 was able to block any plain-text messages. In the second one, Monitor 2 was
checked against real DoS attacks performed on the KIO RTLS. The results of
these tests are presented in Sect. 3.

74
J. Balsa-Comer´on et al.
Detection of Not Encrypted Messages. Encrypted and plain-text messages
were generated using two diﬀerent ROS nodes. They publish ROS String mes-
sages in the same /chatter topic. Monitor 1 detects unencrypted messages and
discards them (Fig. 4).
Fig. 4. Unencrypted messages detection through Monitor 1.
DoS Attacks Detection. Data from KIO RTLS was recorded by OrBiOne
robot standing in the apartment, as shown in Fig. 2, under two diﬀerent situa-
tions: during normal operation and when a DoS attack is being conducted. We
created a diﬀerent rosbag ﬁle7 for each situation.
In order to get the data, we ﬁrst developed the ROS node kio-rtls-talker to
publish the KIO outputs in a topic that will be used by OrBiOne robot. The
node kio-rtls-talker gets the outputs gathered by the KIO tag and publishes them
as geometry msgs/Point messages into the topic /kio rtls talker/stdout. Second,
we implemented the node kio-rtls-listener that consumes the messages published
by the kio-rtls-talker. Figure 5 depicts the conceptual model for the exchange of
messages through the topic /kio rtls talker/stdout. Monitor 2 was developed to
analyse the messages published on that topic.
Fig. 5. DoS attacks detection through Monitor 2.
7 A rosbag ﬁle is equivalent to a recording of the state of the robot in a period of time.
It can be used as data set.

Cybersecurity in Autonomous Systems: Hardening ROS
75
3
Results and Discussion
Figure 6 presents at the top of the image the text messages before being
encrypted by the node talkerAES. Below, it is shown the output given by Monitor
1, and the encrypted messages published in the topic /chatter.
Fig. 6. Plain-text messages before being encrypted by the node talkerAES (top);
ROSRV Monitor 1 (middle); and encrypted messages published in /chatter (bottom).
Figure 7 shows the output produced by node talker as well as the processing
done by Monitor 1, and the published messages (none in this case) in the topic
/chatter.
The output of Monitor 1 shows the results of parsing with the ent tool the
messages generated by the node talker. As can be seen in the Fig. 7, plain-text
messages are discarded. So, no messages are published in the topic /chatter.
Monitor 1 provides a security feature by ensuring that only encrypted mes-
sages are used.
Figure 8 shows the playback of the rosbag ﬁle containing the recordings of
the topic kio-rtls-talker during normal operation of KIO RTLS. The image
includes the output of Monitor 2 and the messages published in the topic
/kio rtls talker/stdout.
Figure 9 presents the same information when KIO RTLS is suﬀering a DoS
attack. As can be seen, Monitor 2 (middle) is able to detect the attack and block
the aﬀected messages.

76
J. Balsa-Comer´on et al.
Fig. 7. Output produced by node talker (top); ROSRV Monitor 1 (middle); and pub-
lished messages, none, in /chatter (bottom).
Fig. 8. KIO RTLS during normal operation. Rosbag ﬁle with the recorded data
from kio-rtls-talker (top); ROSRV Monitor 2 (middle); and published messages in
/kio rtls talker/stdout (bottom).

Cybersecurity in Autonomous Systems: Hardening ROS
77
Fig. 9. KIO RTLS during a DoS attack. Rosbag ﬁle with the recorded data from
kio-rtls-talker (top); ROSRV Monitor 2 (middle); and published messages, none, in
/kio rtls talker/stdout (bottom).
Following a similar approach, new security issues could be managed. In addi-
tion, Monitor 1 and 2 could be combined together to improve both security and
safety, allowing to block more complex security issues.
4
Conclusion and Further Work
It has been shown that the ROS framework can be hardened by using symmetric
encryption algorithms and semantic rules to ensure speciﬁc properties in ROS
messages. In our experiments, the ciphering key is previously known by all the
nodes used. A future line of work is to deﬁne a safe key exchange method, for
example by using a Diﬃe-Hellman algorithm.
It has also been proved that the semantic rules can be used to improve
security by deﬁning conditions or properties. In this work they have been applied
to detect plain-text messages in a ROS topic, as well as DoS attacks against the
KIO RTLS. Other rules may be deﬁned in an analogous way, to detect any other
kind of attack or to ensure safety in the robot behavior.
Monitor 1 could be improved not only to discard unencrypted messages, but
checking if they have been encrypted with a speciﬁc key. This feature would
greatly improve the identiﬁcation of trusted nodes. However, it means that all
nodes must know the key if symmetric encryption algorithms are used. The use
of asymmetric encryption can avoid the key exchange problem. This feature has
been considered in ROS 28, which uses asymmetric encryption between publish-
ers and subscribers. However, the new version of ROS is in beta phases and still
far to be released.
Finally, the technical contributions of this work are pointed out. They
include:
8 https://github.com/ros2/ros2/wiki.

78
J. Balsa-Comer´on et al.
– A ROS package to easily use the KIO RTLS in mobile robots.
– ROS nodes to encrypt/decrypt ROS string messages using AES.
– Migration of ROSRV framework from ROS Groovy to ROS Indigo.
– Implementation of the rules to generate the proposed ROSRV monitors.
Source code of the ROS nodes and ROSRV monitors mentioned in this paper
are available at public Git repository9. Also, the ROS package to use the KIO
RTLS has been uploaded to the ROS Wiki10.
Acknowledgment. The authors would like to thank the Spanish National Institute
of CyberSecurity (INCIBE) under grant Adenda21 ULE-INCIBE.
References
1. ROS community metrics report, July 2016. http://download.ros.org/downloads/
metrics/metrics-report-2016-07.pdf. Accessed 22 Nov 2016
2. Barker, E., Barker, W.C.: Cryptographic Standards in the Federal Government.
NIST Spec. Publ. 800, 175A (2016)
3. Cliﬀord, P., Cosma, I.: A simple sketching algorithm for entropy estimation over
streaming data. In: AISTATS, pp. 196–206 (2013)
4. Daemen, J., Rijmen, V.: The Design of Rijndael: AES-the Advanced Encryption
Standard. Springer, Heidelberg (2013)
5. Denning, T., Matuszek, C., Koscher, K., Smith, J.R., Kohno, T.: A spotlight on
security and privacy risks with future household robots: attacks and lessons. In:
Proceedings of the 11th International Conference on Ubiquitous Computing, pp.
105–114. ACM (2009)
6. Gueron, S.: Intel R
⃝advanced encryption standard (AES) new instructions set. Intel
Corporation (2010)
7. Huang, J., Erdogan, C., Zhang, Y., Moore, B., Luo, Q., Sundaresan, A., Rosu, G.:
Runtime veriﬁcation for robots. In: International Conference on Runtime Veriﬁca-
tion, pp. 247–254. Springer (2014)
8. Lera, F.J.R., Balsa, J., Casado, F., Fern´andez, C., Rico, F.M., Matell´an, V.: Cyber-
security in autonomous systems: evaluating the performance of hardening ROS,
M´alaga, Spain, p. 47 (2016)
9. Quigley, M., Conley, K., Gerkey, B., et al.: ROS: an open-source robot operating
system. In: ICRA Workshop on Open Source Software, vol. 3, pp. 5–10, Kobe,
Japan (2009)
10. Roman, S.: Coding and Information Theory, vol. 134. Springer, New York (1992)
11. Smid, M.E., Branstad, D.K.: Data encryption standard: past and future. In: Proc.
IEEE 76(5), 550–559 (1988)
9 http://niebla.unileon.es/proyectos/publications/robot2017.git.
10 http://wiki.ros.org/KIO-RTLS.

Triaxial Sensor Calibration: A Prototype
for Accelerometer and Gyroscope Calibration
P. Bernal-Polo(B) and H. Mart´ınez-Barber´a
University of Murcia, 30003 Murcia, Spain
pablo.bernal.polo@gmail.com, humberto@um.es
Abstract. The calibration of an accelerometer, and a gyroscope is per-
formed. A linear sensor model have been used. The triaxial sensor cali-
bration algorithm is based on the minimization of a cost function, and is
performed oﬄine. Calibration hardware has been designed, and used to
get the calibration data. The algorithm has been tested with the acquired
data, and the calibration results are presented. Although the sensor
model is linear, some experiences about dealing with non-linearities are
exposed.
Keywords: Triaxial sensor · Calibration · Accelerometer · Gyroscope ·
IMU
1
Introduction
Accuracy in the estimation of the mechanical state of a navigation system is
a desirable property. With the introduction of sensors such as accelerometers
and gyroscopes, the concept of inertial navigation emerged. This dead reckoning
technique allows higher update rates than others like odometry, GPS, or Doppler
eﬀect. In addition, it has the advantage of being self-contained, so no external
equipment is needed. Nowadays this concept has gained attention due to the
development of low-cost devices capable of providing very precise measurements
given their price.
An Inertial Measurement Unit (IMU), that comprises three accelerometers
and three gyroscopes, is an example of a triaxial sensor: a sensor composed of
three measuring axes. With IMUs we can measure accelerations and, after a
double integration, obtain position estimations. However, there is an inherent
problem with this kind of devices. Technological limitations in sensor manufac-
turing imply the existence of diﬀerent sensor sensitivities, measurement biases,
non-orthogonality between their axes, and non-linearities among others. These
intrinsic errors cause measurements to be deviated from their actual value. That
deviation is catastrophic when it comes to dead reckoning [5], which creates
the need for accurate calibration.
For accelerometer calibration, it is common to exploit the fact that the mag-
nitude of gravity is independent of the orientation of the sensor [6,9,10,13].
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_7

80
P. Bernal-Polo and H. Mart´ınez-Barber´a
Other works accommodate this idea into other types of triaxial sensors [3,4,8,11].
The advantage of this approach is that we do not need to know the actual ori-
entation of the sensor to perform the calibration; only the magnitude of the
measured vector is required. This work also relies on this concept. Naturally,
there are works in which this idea is not used [2].
It is also common to use a linear model for the sensor [2–4,6,8–11,13]. But
there are works in which non-linear models have been proposed [1]. In [12] even
a neural network model is used. However, the use of such a model would imply
a computational overload introduced by the online computation of the neural
network output.
Both online and oﬄine calibration algorithms have been developed. Online
calibration algorithms have the advantage of not having to perform a calibration
before using the sensor, but they have the disadvantages of involving a greater
computational overload and complexity of the algorithm, what usually implies
the simpliﬁcation of the calibration model. Reference [7] is an example of an
online calibration algorithm in which only gyroscope biases are calibrated. On
the other hand, oﬄine calibration algorithms have to be performed before using
the sensor, but they allow the calibration for complex sensor models, and after
the calibration the corrected measurements are obtained very eﬃciently. Then,
this second approach is the preferred choice.
Oﬄine calibration algorithms are usually methods for ﬁnding the solution to
an optimization problem, which generally consist on the minimization of a cost
function. It is common to obtain a non-linear system of equations after applying
the minimum condition. The solution to these non-linear systems can be found
using iterative methods [4,9–11], although there are also works in which an
analytical solution is found [8,13]. Nevertheless, to arrive at an analytic solution
they have to perform some change of variables. This change of variables may lead
to a distortion of the cost function, which could result in irrelevant solutions. Of
course, the strategy of minimizing a cost function is not always used. In [2] they
use an oﬄine Kalman ﬁlter to ﬁnd the model parameters.
This paper reviews the linear sensor model, and introduces some new perspec-
tives that help to understand the proposed one. An oﬄine calibration algorithm
is developed. The calibration method, based on the invariant magnitude of the
measured vector, is redeﬁned for arbitrary but known magnitudes of the mea-
sured vectors. We also present a low-cost prototype to automate the calibration
procedure, and show some preliminary calibration results with real data.
The paper is organized as follows. In Sect. 2 we introduce the sensor model.
In Sect. 3 we present the calibration method. In Sect. 4 the calibration prototype
is described. We also show examples of calibration data, which are then used in
Sect. 5 to perform a real calibration. Finally, in Sect. 6, we expose the conclusions.
2
Triaxial Sensor Model
In this Section we discuss the sensor error sources, and we present the model
used to correct them.

Triaxial Sensor Calibration: A Prototype
81
Let us consider a measurable vector y expressed in an orthonormal base
B = {e1, e2, e3} anchored to the sensor. Let us also consider the vector x mea-
sured by the sensor. We are interested in the function relating them. Because of
the misalignment between axes, the diﬀerence in sensitivity of each independent
axis, their bias, and the noise in the measurements, the relation between x and
y is not the identity.
Let B′ = {e′
1, e′
2, e′
3} denote the vectors deﬁning each of the sensor mea-
surement axes, expressed in the base B. We will assume that the i-th sensor axis
measures the projection of the y vector into the unit vector e′
i that deﬁnes such
axis. These projections, expressed in the base B, would be {e′
ie′T
i y}i. However,
the sensor measures y in the base B′. Then, our transformation is given by
x
=
⎛
⎝
— e′
1 —
— e′
2 —
— e′
3 —
⎞
⎠y
.
(1)
Or it would be if the sensor would have a particular sensitivity equal for every
axis, and no bias. Assuming that the sensitivity of the axes does not change
with the magnitude of the measured quantity, we can model it by multiplying
the i-th projection by a scale factor, si. Denoting b to the vector representing
the measurement bias, and r to the vector of random variables with 0 mean1
representing the measurement noise, the complete model for our triaxial sensor
will be
x
=
⎛
⎝
— s1 e′
1 —
— s2 e′
2 —
— s3 e′
3 —
⎞
⎠y + b + r
=
S y + b + r
.
(2)
It is important to note that this model does not incorporate non-linear rela-
tions between x and y, time variations, or temperature dependence. Therefore,
we need to be sure that our sensor is free of these behaviors, or at least that
they are negligible.
3
Calibration
Once our sensor model is deﬁned, we are interested in ﬁnding the parameters
that characterize a particular sensor. This section presents the method used to
ﬁnd those parameters: the calibration algorithm.
The fundamental concept from which the present calibration algorithm is
developed arises from a ﬁrst attempt to calibrate an accelerometer. A natural
idea emerges in such scenario: the measured module of the gravity vector should
be the same independently of the orientation of the sensor. This conception lead
us to consider the module of the y vector in our sensor model:
∥y∥
=
∥S−1 ( x −b −r ) ∥
=
∥T ( x + c ) + r′ ∥
,
(3)
1 Note that if r has an expected value diﬀerent than 0 its contribution could be
delegated to b.

82
P. Bernal-Polo and H. Mart´ınez-Barber´a
where T = S−1, c = −b, and r′ = −S−1r. Whenever the sensor is static, the
parameters of our model should satisfy this equation for any orientation. By
collecting a set of sensor measurements for a wide range of static orientations
we will obtain enough constraints to narrow the solution. Then, our calibration
problem turns into a ﬁtting problem. We will deﬁne an error function whose
minimum we want to ﬁnd. The location of the minimum will be the solution to
our problem: the parameters of our model.
3.1
Error Function Deﬁnition
Let {xn}N
n=1 be a set of measurements taken with our triaxial sensor in diﬀerent
static conditions2. Let also {yn}N
n=1 be the set of known modules of the vector
y for each static orientation. We deﬁne our error function as
f(T, c)
=
N

n=1

∥T (xn + c) ∥2 −y2
n
	2
.
(4)
We choose to minimize the distance between squared modules rather than the
distance between modules because it greatly simpliﬁes the mathematical treat-
ment. We have also extended the initial idea of a ﬁxed, invariant under rotations
module to the more general idea of an arbitrary module, but also invariant under
rotations, based on the premise that the module of a vector is easily obtainable. We
notice as well that given a solution to our minimization problem, {T∗, c∗}, then
{RT∗, c∗}, with R such that RT R = I, would also be a solution to our minimiza-
tion problem. Note that R does not need to be a rotation matrix. In fact, a change
in the sign of any column of the matrix T∗would not produce a change in the value
of the error function f. The solution is not unique.
3.2
Model Redeﬁnition
In order to limit the number of solutions, we can add further constraints to the
calibration matrix T. These constraints take the form of relations between the
vectors of B and the ones of B′.
We will choose the vector e1 of the base B to point in the direction of the
vector e′
1 of the base B′ (then e1 = e′
1). The vector e2 will point in the direction
of e′
2 minus its projection onto e1 (this is e2 ∝e′
2 −e1eT
1 e′
2). Finally, the vector
e3 will point in the direction of e′
3 minus its projection onto the vectors e1 and
e2 (that is e3 ∝e′
3 −e1eT
1 e′
3 −e2eT
2 e′
3). Then, our vectors {e′
i}i will satisfy
e1
= e′
1
=⇒
e′
1 =
 1
0
0

,
(5)
e2
∝

I −e1eT
1

e′
2
=⇒
 0
1
0

∝
 0 0 0
0 1 0
0 0 1

e′
2
=⇒
e′
2 =
 ∗
⋆
0

,
(6)
e3
∝

I −e1eT
1 −e2eT
2

e′
3
=⇒
 0
0
1

∝
 0 0 0
0 0 0
0 0 1

e′
3
=⇒
e′
3 =
 ∗
∗
⋆

,
(7)
2 A static condition for an accelerometer is one in which we are not measuring accel-
eration other than gravity; a static condition for a magnetometer is one in which we
are not measuring magnetic ﬁeld other than the reference one.

Triaxial Sensor Calibration: A Prototype
83
where ∗stands for any real value, and ⋆means a real value diﬀerent than 0. As
a consequence, our calibration matrix will have the form
S =
⎛
⎝
— s1 e′
1 —
— s2 e′
2 —
— s3 e′
3 —
⎞
⎠=
⎛
⎝
s1 0 0
∗⋆0
∗∗⋆
⎞
⎠
=⇒
T =
⎛
⎝
⋆0 0
∗⋆0
∗∗⋆
⎞
⎠,
(8)
where we have used the property that the inverse of an invertible lower triangular
matrix is also lower triangular. The matrix S should be invertible because if it
were not, the set {e′
i}i would not be a base, and the sensor would not be able
to measure a vector, but its projection onto a subspace of R3.
Some works use an upper triangular matrix in their calibration model
[3,4,10]. If we had chosen e3 = e′
3 instead of (5), and then we had deﬁned
e2 and e1 using the same procedure followed before, we would have obtained an
upper triangular matrix. As stated before, the solution is not unique, but our
choice on the meaning of each vector of B will deﬁne the form of the matrix T.
Lastly, we want our reference frame to have right-handed orientation. In a
right-handed reference frame the result of (e′
1 × e′
2) · e′
3 must be positive. This
expression turns out to be the determinant of our calibration matrix, det(S).
Knowing that det(S) = [det(S−1)]−1 = [det(T)]−1, and that for a triangular
matrix det(T) = 
i Tii, we can assure that our orientation will be right-handed
by forcing positive terms on the diagonal of T.
3.3
Calibration Algorithm
Our ﬁnal calibration model needs 9 parameters to be characterized:
θ = (T11, T21, T22, T31, T32, T33, c1, c2, c3)
.
(9)
Being interested in ﬁnding the minimum of the cost function (4) we are looking
for those values that satisfy
F
:=
∂f
∂θ
=
0
.
(10)
Equations resulting from (10) form a system of non-linear equations. Being these
equations diﬀerentiable, we can use an iterative algorithm like the Newton’s
method to solve this system:
J(θk) (θk+1 −θk)
=
−F(θk)
,
(11)
where J(θk) = ∂F(θ)
∂θ

θ=θk
.

84
P. Bernal-Polo and H. Mart´ınez-Barber´a
4
Experimental Testbed
In order to test our calibration algorithm, it is convenient to have an experimen-
tal testbed in which the process is automated. In this section we will depict our
hardware design, and how data acquisition is performed.
4.1
Hardware Design
Although the calibration algorithm can be applied to any triaxial sensor, our
main goal is to use it for calibrating accelerometers and gyroscopes. In addition,
several works have reported a dependence of the calibration parameters with
temperature. Even though we do not present results of a temperature-dependent
calibration, our hardware will be designed to be able to do so in the future.
To calibrate any triaxial sensor we need to be able to place it in diﬀerent
orientations. For this purpose we will use two servos, one attached to the other.
We will secure the sensor to an end, and we will ﬁx the other end to a pole.
With this setup we will be able to calibrate our accelerometer. However, we
will need to provide an angular velocity to calibrate our gyroscope. To this
end we will mount the pole on top of a rotating motor. The actual angular
velocity will be measured using an optical tachometer. Since (4) is invariant
under rotations, it will not be necessary to orient the servos accurately; we just
need them to provide a suﬃciently diverse set of orientations. However, we will
need the accuracy provided by the optical tachometer in the angular velocity
measurements.
The assembly will be placed inside a box that will function as thermal insu-
lation. The box will have a window to see what happens inside. A Peltier device
will be used to control the temperature inside the box.
Being a part of the assembly on top of the rotating motor, we will need two
diﬀerent controllers: one on the rotating part (C1), and the other on the out-
side (C2). C1 will be responsible for placing the servos in diﬀerent orientations,
reading the sensor measurements, and sending these measurements to the exter-
nal controller via an RF module. C2 will receive the measurements from the
internal controller, will measure the actual modulus of the angular velocity, and
will resend the received measurements along with the measured angular velocity
modulus to the PC. It also will set the motor speed, and the intensity going
through the Peltier device. Figure 1 illustrate the architecture of the calibration
system. Figure 2 shows the current version of the assembled hardware.
4.2
Data Acquisition
Our calibration subject is an MPU-6050 by InvenSense. This device is an IMU
containing an accelerometer and a gyroscope. We aim to ﬁnd the calibration
parameters that characterize these sensors.

Triaxial Sensor Calibration: A Prototype
85
C1
IMU
SERVOS
RF
MODULE
C2
TACHOMETER
MOTOR
PELTIER
SERIAL
PC
Fig. 1. Calibration hardware block diagram
Fig. 2. Calibration hardware prototype
Accelerometer Data. The accelerometer range has been established to mea-
sure accelerations in the interval (−16, 16) g. Using the hardware described in
Sect. 4.1 we obtained a set of static3 accelerometer measurements. Figure 3 shows
the 3-dimensional points that represent each acceleration measurement.
Gyroscope
Data.
The gyroscope measurement range has been set as
(−2000, 2000 ) ◦s−1 ≈(−34.9, 34.9)rads−1. Using the hardware described in
Sect. 4.1 we got a set of static4 gyroscope measurements. Figure 4a shows the
data collected with the gyroscope. Figure 4b shows the data obtained with the
tachometer.
3 Here static means with no acceleration other than the gravity.
4 Here static means with no angular velocity other than the tachometer measured one.

86
P. Bernal-Polo and H. Mart´ınez-Barber´a
-2000
0
2000
-2000
0
2000
−2000
0
2000
Fig. 3. Raw static accelerometer measurements (16-bit signed integer)
5
Experimental Results
This section presents the calibration results. Accelerometer calibration oﬀers no
complication. However, gyroscope calibration needs more attention.
5.1
Accelerometer Calibration
Knowing that our sensor measures accelerations in the interval (−16, 16) g, and
that it outputs its measurements as 16-bit signed integers, we would expect
Ta ≈I 16g
215 ≈I4.8823·10−4 g. After introducing the static acceleration samples
to the calibration algorithm we obtain:
Ta
=
⎛
⎝
4.8863
0
0
0.0111
4.8567
0
0.0663
−0.0086
4.8073
⎞
⎠· 10−4 g
,
(12)
ca
=
 −28.47
−1.51
67.95 T
.
(13)
The obtained result is close to what was expected. Nevertheless the eﬀects of
bias, misalignment, and scale factors are appreciated. Figure 5 displays a repre-
sentation of the accelerometer axes.
5.2
Gyroscope Calibration
Knowing
that
our
sensor
measures
angular
velocities
in
the
interval
(−2000, 2000)◦s−1, and that it outputs its measurements as 16-bit signed inte-
gers, we would expect Tω ≈I
2000◦s−1· πrad
180◦
215
≈I1.0653 · 10−3 rads−1. Taking the

Triaxial Sensor Calibration: A Prototype
87
-30000
0
30000
-30000
0
30000
−30000
0
30000
(a)
0
10
20
30
40
50
0
1000
2000
3000
4000
5000
angular velocity (rad s−1)
measurement number
0
10
20
30
40
50
0
1000
2000
3000
4000
5000
(b)
Fig. 4. Angular velocity measurements. a Raw static gyroscope measurements
(16-bit signed integer). b Tachometer data. The area outside the measuring range of
the gyroscope is ﬁlled.
0
2000
0
2000
0
2000
Fig. 5. Representation of the accelerometer measurement axes (g−1)
whole set of static angular velocity samples, the calibration algorithm reaches
the next solution:
Tω
=
⎛
⎝
1.4074
0
0
0.0595
1.3300
0
−0.2246
−0.0846
1.4198
⎞
⎠· 10−3 rads−1
,
(14)
cω
=

2008.2
383.9
−1804.7
T
.
(15)
Fig. 6a is a representation of the obtained gyroscope axes.
We notice a great misalignment between the axes. This could be due to an
actually large misalignment, or to an error in the calibration. Figure 6b shows
the relation between the tachometer measured angular velocity, and the angular
velocity module computed with the gyroscope measurements after being trans-
formed using the obtained parameterization. We did not take into account the
sensor saturation eﬀects. This saturation lead to a non-linearity in the gyro-
scope output. Then, a new calibration must be performed, leaving out this time
the measurements in the non-linear region. In our case, the non-linear eﬀects

88
P. Bernal-Polo and H. Mart´ınez-Barber´a
0
500
0
500
0
500
(a)
0
15
30
45
0
15
30
45
calibrated gyroscope measurement (rad s−1)
tachometer measurement (rad s−1)
0
15
30
45
0
15
30
45
(b)
Fig. 6. Results of the ﬁrst attempt to calibrate the gyroscope. a Representation of the
gyroscope measurement axes (rad−1s). b Comparison between tachometer measured
angular velocity, and the calibrated gyroscope measurements. The area outside the
measuring range of the gyroscope is ﬁlled.
start to be noticed for angular velocities greater than 20 rads−1. Calibrating our
gyroscope for measurements below this limit, we retrieve our ﬁnal calibration
transformation:
T∗
ω
=
⎛
⎝
1.0720
0
0
0.0022
1.0679
0
0.0089
0.0060
1.0702
⎞
⎠· 10−3 rads−1
,
(16)
c∗
ω
=

29.0904
26.9676
87.4518
T
.
(17)
The new result is closer to the result we expected than the previous one. As in
the case of the accelerometer, we still notice the eﬀects of bias, misalignment, and
scale factors. Figure 7a shows a representation of the corresponding gyroscope
measurement axes for these calibration parameters.
The ﬁnal comparison between the tachometer measured angular velocity,
and the angular velocity module computed using the calibrated gyroscope mea-
surements is displayed in Fig. 7b. We observe that the calibration will be valid
for angular velocities below 20 rads−1, but above that value non-linearities are
present.

Triaxial Sensor Calibration: A Prototype
89
0
500
0
500
0
500
(a)
0
15
30
45
0
15
30
45
calibrated gyroscope measurement (rad s−1
tachometer measurement (rad s−1)
0
15
30
45
0
15
30
45
(b)
Fig. 7. Results of the ﬁnal gyroscope calibration. a Representation of the gyroscope
measurement axes (rad−1s). b Comparison between tachometer measured angular
velocity, and the calibrated gyroscope measurements. The area outside the measur-
ing range of the gyroscope is ﬁlled.
6
Conclusion and Future Work
We have reviewed the triaxial sensor model, and revised its underlying concepts.
We have proposed an error function, and designed the calibration algorithm
based on it. A low-cost prototype for triaxial sensor calibration has been devel-
oped. Although it has been designed to calibrate accelerometers and gyroscopes,
it can also be used to calibrate magnetometers considering that the Earth’s
magnetic ﬁeld is a constant vector in the vicinity of the calibration prototype.
Calibration data have been acquired using this prototype, and the proposed cal-
ibration algorithm has been tested with them. We have become aware of the
saturation eﬀects in the sensor, which involve the emergence of non-linearities,
and we have learned that we must take them into account in the calibration
process.
Although we have calibrated satisfactorily our sensors, we still need to study
the limitations of our calibration algorithm. We also need to think about how
to handle the temperature dependence of the sensors.
References
1. Batista, P., Silvestre, C., Oliveira, P., Cardeira, B.: Accelerometer calibration and
dynamic bias and gravity estimation: analysis, design, and experimental evaluation.
IEEE Trans. Control Syst. Technol. 19(5), 1128–1137 (2011)

90
P. Bernal-Polo and H. Mart´ınez-Barber´a
2. Beravs, T., Begus, S., Podobnik, J., Munih, M.: Magnetometer calibration using
kalman ﬁlter covariance matrix for online estimation of magnetic ﬁeld orientation.
IEEE Trans. Instrum. Meas. 63(8), 2013–2020 (2014)
3. Bonnet, S., Bassompierre, C., Godin, C., Lesecq, S., Barraud, A.: Calibration meth-
ods for inertial and magnetic sensors. Sens. Actuators, A 156(2), 302–311 (2009)
4. Dorveaux, E., Vissiere, D., Martin, A.P., Petit, N.: Iterative calibration method
for inertial and magnetic sensors. In: Proceedings of the 48th IEEE Conference
on Decision and Control, 2009 Held Jointly with the 2009 28th Chinese Control
Conference, CDC/CCC 2009, pp. 8296–8303. IEEE (2009)
5. Flenniken, W., Wall, J., Bevly, D.: Characterization of various imu error sources
and the eﬀect on navigation performance. In: ION GNSS, pp. 967–978 (2005)
6. Fong, W., Ong, S., Nee, A.: Methods for in-ﬁeld user calibration of an inertial
measurement unit without external equipment. Measur. Sci. Technol. 19(8), 085,
202 (2008)
7. Markley, F.L.: Attitude error representations for kalman ﬁltering. J. Guid. Control
Dyn. 26(2), 311–317 (2003)
8. Renaudin, V., Afzal, M.H., Lachapelle, G.: Complete triaxis magnetometer cali-
bration in the magnetic domain. J. Sens. 2010 (2010)
9. Sipos, M., Paces, P., Rohac, J., Novacek, P.: Analyses of triaxial accelerometer
calibration algorithms. IEEE Sens. J. 12(5), 1157–1165 (2012)
10. Skog, I., H¨andel, P.: Calibration of a mems inertial measurement unit. In: XVII
IMEKO World Congress, pp. 1–6 (2006)
11. Vasconcelos, J.F., Elkaim, G., Silvestre, C., Oliveira, P., Cardeira, B.: Geometric
approach to strapdown magnetometer calibration in sensor frame. IEEE Trans.
Aerosp. Electron. Syst. 47(2), 1293–1306 (2011)
12. Wang, J.H., Gao, Y.: A new magnetic compass calibration algorithm using neural
networks. Meas. Sci. Technol. 17(1), 153 (2005)
13. Zhang, H., Wu, Y., Wu, W., Wu, M., Hu, X.: Improved multi-position calibration
for inertial measurement units. Measur. Sci. Technol. 21(1), 015, 107 (2009)

Automatic Characterization of Phase Resetting
Controllers for Quick Balance Recovery During
Biped Locomotion
Juli´an Cristiano1(B), Dom`enec Puig1, and Miguel Angel Garc´ıa2
1 Intelligent Robotics and Computer Vision Group,
Department of Computer Science and Mathematics, Rovira i Virgili University,
43007 Tarragona, Spain
{julianefren.cristiano,domenec.puig}@urv.cat
2 Department of Electronic and Communications Technology,
Autonomous University of Madrid, 28049 Madrid, Spain
miguelangel.garcia@uam.es
http://deim.urv.cat/~rivi/
Abstract. This paper proposes a methodology for automatic character-
ization of phase resetting controllers for quick balance recovery after loss
of it during biped locomotion. The system allows to easily characterize
and design useful phase resetting controllers using a simulation environ-
ment. Several experiments have been performed using a NAO humanoid
robot in order to automatically characterize and test the phase reset-
ting mechanism. Notwithstanding, it can be implemented by using any
humanoid robot with a similar kinematic structure. Once the controllers
are characterized, the proposed system detects the robot’s current state
through the information provided by its inertial sensors and then applies
the correct phase resetting in a short period of time in order to quickly
recover the robot’s balance. The proposed control scheme reacts quickly
whenever unknown external perturbations are applied to the robot’s
body by using the proposed phase resetting mechanism.
Keywords: Biped locomotion · Central Pattern Generators · CPGs ·
Humanoid robots · Phase resetting
1
Introduction
Many studies show the presence of specialized networks of neurons able to gen-
erate the rhythmic patterns in animals and humans. These networks are called
central pattern generators (CPGs). CPGs are modelled as networks of neurons
capable of generating stable and periodic signals controlled through a set of con-
stant parameters. In the case of vertebrates, these networks are located in the
central nervous system within the spinal cord. The output signals from these
CPGs are sent to the muscles through the peripheral nervous system. High-level
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_8

92
Ju. Cristiano et al.
commands are sent to the diﬀerent CPGs by the brain through the spinal cord.
These commands do not generate the periodic signal by themselves, since the
oscillation is autonomously generated within the CPG in the spinal cord. The
complex nervous system of animal and humans is able to automatically learn
and generate motion patterns by controlling extensor and ﬂexor movements.
One important characteristic of the locomotor system of animals and humans is
the quick adaptation of the motion patterns generated by the system as response
to internal changes or external environmental conditions.
Recent studies have shown that the oscillatory activity found in some neurons
uses phase resetting mechanisms in order to quickly synchronize and coordinate
its functioning. Basically, the phase resetting mechanism synchronizes the neu-
rons current state in order to conﬁgure the phase of an oscillation to a reference
point for a given event or stimulus. Inspired in this fact, some previous studies
have proposed the phase resetting mechanism during biped locomotion in order
to deal with external perturbations ([1,2]).
2
Related Work
Biped locomotion is a complex problem since it involves the inherent instability
of humanoid robots. Therefore, it is important to develop an appropriate control
scheme capable of generating stable motions and also that guarantees a quick
reaction whenever unstable situations appear. Phase resetting controllers have
shown to be a suitable feedback strategy to quickly react to internal changes or
external perturbations. The phase resetting mechanism is also an eﬃcient option
for robots with reduced computational capabilities because it does not require a
complex processing of data to react, which is not possible in most of the small
size humanoid robots [3].
Most of the systems proposed for locomotion control of biped robots using a
phase resetting mechanism as feedback strategy are only focused in the charac-
terization of phase resetting controllers by performing some experiments with the
real robot ([1,2]). However, in a real workspace it is possible that the robot can
be pushed by external forces coming from any unknown direction and the charac-
terization in a real scenario can be infeasible. Furthermore, the characterization
from one robot to another can be diﬀerent. Therefore, in this paper a methodol-
ogy for the correct characterization of these controllers is proposed. The idea is
to deﬁne a testing scenario in simulation which allows to extensively explore and
characterize these controllers by performing several simulations under diﬀerent
situations. The ﬁnal goal is to develop an initial model for a simulated scenario
which can be later adjusted in a real robot by performing a reduced number
of experiments in the real robot compared to those needed actually in order to
characterize these controllers.
This paper is organized as follows. Section 2 describes the control system
used to generate the locomotion patterns for biped robots and also presents
the implemented phase resetting mechanism. The methodology proposed for the

Automatic Design of Phase Resetting Controllers
93
Fig. 1. CPG network of 4 neurons as proposed in [5].
appropriate characterization of phase resetting controllers and some experimen-
tal results are presented and discussed in Sect. 3. Finally, conclusions and future
work are given in Sect. 4.
3
Generation and Control of Biped Locomotion Patterns
In this section, our previous CPG-based locomotion control system is used to
generate and control the locomotion patterns for biped robots [4]. The signals
generated by the system are used to directly drive the robot’s joins, and the
phase resetting mechanism described in [3] is used to guarantee a quicker and
deterministic response. Finally, the proposed methodology for automatic char-
acterization of phase resetting controllers is presented.
3.1
CPG Network and Neuron’s Model
The CPG utilized to generate the locomotion patterns is based on a network of 4
interconnected neurons with mutual inhibition previously proposed by Matsuoka
[5]. The topology of that CPG is shown in Fig. 1. That network has been chosen
as it generates oscillatory output signals in phase, anti-phase and with phase
diﬀerences of π
2 and 3π
2 radians. These phase diﬀerences are suﬃcient to control
the robot’s movement directly in the joint space, as shown in [6]. The intercon-
nection weights between the neurons of that CPG are shown in Table 1. Figure 2
shows the output signal of each neuron of the CPG network.
The CPG’s neurons are deﬁned according to the well-known Matsuoka’s
neuron model:
τ ˙ui = −ui −
N

j=1
wijyj −βvi + ue + fi
(1)
τ ′ ˙vi = −vi + yi
(2)
yi = max(0, ui),
i = 1, ..., N
The external input ue aﬀects the amplitude of the neuron’s output signal.
The frequency of the output signal is determined by the time constants τ and

94
Ju. Cristiano et al.
0
1
2
3
4
5
6
7
0
0.1
0.2
y1
Time[s]
0
1
2
3
4
5
6
7
0
0.1
0.2
y2
Time[s]
0
1
2
3
4
5
6
7
0
0.1
0.2
y3
Time[s]
0
1
2
3
4
5
6
7
0
0.1
0.2
y4
Time[s]
Fig. 2. Output signals of the 4-neuron CPG network.
Table 1. CPG’s interconnection weights
w1,1 0.0 w1,2 0.0 w1,3 2
w1,4 0.5
w2,1 0.5 w2,2 0.0 w2,3 0.0 w2,4 2
w3,1 2
w3,2 0.5 w3,3 0.0 w3,4 0.0
w4,1 0.0 w4,2 2
w4,3 0.5 w4,4 0.0
τ ′. The set of parameters must satisfy some requirements in order to yield stable
oscillations ([5,7]). Term fi is a feedback variable that can be used to control
the output amplitude and to synchronize the output signals with a periodic
input signal. Parameter wij represents the bidirectional interconnection weight
between two neurons. Those inteconnection weights determine the phase dif-
ference among the output signals generated by the CPG. When a network of
neurons is set, they all oscillate together according to their internal parame-
ters and the network interconnections, converging to speciﬁc patterns and limit
cycles. Variable N represents the number of neurons that constitute the CPG
(N = 4 in this work).
Parameter Kf has been introduced as proposed in [8] in order to modulate
the frequency of the output signal. The time constants in (1) and (2) are thus
reformulated as:
τ = τoKf
τ ′ = τ ′
oKf,
where τo and τ ′
o are the original time constants.

Automatic Design of Phase Resetting Controllers
95
The internal parameters that determine the behaviour of each neuron are
summarized in Table 2. The CPG generates stable oscillations provided those
parameters satisfy some requirements ([5,7]).
Table 2. Internal parameters for each neuron
Parameter
Value
Parameter
Value
τo
0.2800 ue
0.4111
τ ′
o
0.4977 fi
0
β
2.5000
The proposed system has been tested on the NAO platform [9], which is a
small size humanoid robot with 21 degrees of freedom, 56 cm tall and weighting
4.8 Kg. Notwithstanding, the same control system can easily be adapted to other
humanoid robots with a similar kinematic structure.
The controllers proposed in [6] have been used to determine the angle in
radians of the following joints of the NAO robot:
RHipPitch = bias1 + a(−ξ(y1 −y3) + (y2 −y4))
LHipPitch = bias1 + a(ξ(y1 −y3) −(y2 −y4))
LKneePitch = bias2 + b(y2 −y4)
RKneePitch = bias2 + b(y4 −y2)
RAnklePitch = bias3 + c(ξ(y1 −y3) + (y2 −y4))
LAnklePitch = bias3 + c(−ξ(y1 −y3) −(y2 −y4))
RHipRoll = d(y2 −y4)
LHipRoll = d(y2 −y4)
LAnkleRoll = e(y4 −y2)
RAnkleRoll = e(y4 −y2)
RShouldPitch = bias4 + f(y1 −y3)
LShouldPitch = bias4 −f(y1 −y3)
(3)
Those controllers depend on 10 internal parameters: 4 biases (bias1, ..., bias4)
and 6 gains (a, b, c, d, e, f). Parameter ξ controls the stride length. Both the
latter and the locomotion frequency, which is controlled through the value of
Kf, determine the robot’s walking velocity. Detailed information can be found
in [10].
3.2
Phase Resetting Controller
A phase resetting controller is a simple but eﬀective feedback strategy. These
controllers must detect the external force applied to the robot’s body through

96
Ju. Cristiano et al.
the quick tracking and analysis of the measures provided by the robot’s sensors.
Once the external perturbation is detected by the system, it must react by
activating the phase resetting mechanism in order to quickly recover balance.
In the control scheme used in this work, the controller synchronizes the neu-
rons’ output signals by modifying the current phase of the locomotion pattern
generated by the system to a desired phase given an external event or stimulus,
such as an external force applied to the robot’s body. The aim of the phase
resetting mechanism is the generation of a force in the direction opposite to the
one of the force generated by the external perturbation by changing the phase
of the current locomotion pattern in order to guarantee the quick recovery of
balance.
In this work, the information provided by the 3-axis accelerometer is tracked
and used to detect the exact instant at which the external force is applied to the
robot’s body and also to estimate its magnitude and direction. According to the
current phase of the generated locomotion pattern and the corresponding phase
in which the external force applied to the robot is detected, the phase resetting
controller must react by changing the current phase of the locomotion pattern
to another phase that allows the robot to recover its balance. This phase change
is eﬀective after Δt seconds. In the experiments, the controller’s response time
(Δt) was set to 40 ms. However, this time could be modiﬁed according to the
desired system’s response. Detailed information about the implementation of the
phase resetting mechanism can be found in [3].
Figure 3 shows a example of the phase resetting controller implemented for
the output signals of the 4-neuron CPG network shown in Fig. 1, which is used
to generate the locomotion pattern together with the controllers described pre-
viously in (3). In the ﬁgure the external force is detected by the system in the
sample number 3064 and later the phase resetting is activated in order to deal
with the external perturbation.
4
Proposed Methodology for Automatic Characterization
of Phase Resetting Controllers
The behaviour of a phase resetting controller is determined by the shape of
the corresponding Phase Resetting Curve (PRC). After the external force is
detected by the sensors, the PRC function indicates the phase shifts that must
be applied to the locomotion pattern generated by the CPG-based control sys-
tem according to its current phase in order to avoid the robot from falling down.
In this paper we extend our previous work by proposing a methodology to auto-
matically characterize and model the PRCs. This procedure must be done in
advance by performing several experiments that allow the previous deﬁnition of
the appropriate set of curves. However, these PRCs can be redeﬁned at any time
according with new sensory information or by modifying its shape according to
the deterioration of the robot’s physical parts.
Two PRCs, used to react to external perturbations in the forward and back-
ward directions, have been automatically characterized and tested on a NAO

Automatic Design of Phase Resetting Controllers
97
0
1000
2000
3000
4000
5000
6000
7000
−0.2
−0.1
0
0.1
0.2
Sample number
y2−y4
y1−y3
0
1000
2000
3000
4000
5000
6000
7000
−0.2
−0.1
0
0.1
0.2
Sample number
Fig. 3. Example of the phase resetting mechanism applied to the output signals of the 4-
neuron CPG network shown in Fig. 1. The plots represent the system’s response without
(top) and with (bottom) the phase resetting controller. In the ﬁgure the external force
is detected by the system in the sample number 3064. After the phase resetting response
time, the phase of the locomotion pattern is modiﬁed to produce a force in the opposite
direction to the one of the external perturbation.
biped robot in simulation on the Webots simulator following the procedure
described below.
• The locomotion pattern used for the experiments was generated by means of
the CPG-joint-space control scheme described in Sect. 2, with the parameters
for the straight-line locomotion pattern.
• The locomotion pattern’s phase is varied between 0 and 360 degrees using a
ﬁxed step-size of 10 degrees.
• External forces with diﬀerent values of magnitude are applied to the robot’s
body in the backward and forward directions for each tested phase of the
locomotion pattern, the aim is to determine those force magnitudes that
produce loss of balance during bipedal locomotion causing the robot falls
down.
• The information provided by the accelerometer was analysed and used to
predict the cases where the robot falls down whenever the external force is
applied to its body. Later, this information is used to activate the phase
resetting mechanism.
• Several experiments were performed on the Webots simulator in order to
characterize the phase resetting controllers by means of the deﬁnition of the
two PRCs for those external forces that cause the loss of balance during biped
locomotion. It was determined in simulation the phase shifts that must be

98
Ju. Cristiano et al.
0
50
100
150
200
250
300
350
Current phase [degrees]
0
2
4
PRC1 [Rad]
Fig. 4. Phase resetting curve found for the experiments using perturbations in the
forward direction.
0
50
100
150
200
250
300
350
Current phase [degrees]
1
2
3
4
PRC2 [Rad]
Fig. 5. Phase resetting curve found for the experiments using perturbations in the
backward direction.
applied by the system in order to avoid the robot from falling down. These
values were used to build the corresponding PRC.
• The phase resetting controller is characterized after the deﬁnition of the
PRCs, namely, if the system detects a external force applied to the robot,
it uses the previously estimated PRC in order to calculate the amount of
phase shift required to quickly recover the balance.
4.1
Experimental Results
In the experiments performed, the external forces were considered to be applied
to the robot’s trunk along the forward and backward directions. The simulator
allows the deﬁnition of the exact point in the robot’s body in which the force is
applied, as well as its desired magnitude and direction. The external force was
also applied at a known phase of the locomotion pattern and at the same point on
the robot’s body in order to test the system under the same dynamic conditions.
The whole system was implemented in webots using several controllers that
interact among them in order to allow the complete study of the whole system
in simulation.
The forces that guarantee that the robot will fall down whenever the feedback
mechanism is not activated have been used to deﬁne the PRC and also to test
the system operating in both open and closed loop. In all the experiments, the
robot starts its motion from the same position.

Automatic Design of Phase Resetting Controllers
99
The phase resetting curves PRC1 and PRC2 shown in Figs. 4 and 5, respec-
tively, were obtained after performing several experiments in the simulator. Using
the methodology described in this paper, several PRC can be implemented and
activated using diﬀerent sensory information. Experimental results showing the
behaviour of the overall system in the simulated workspace can be found on the
companion website1 and in Figs. 6, 7, 8 and 9.
Fig. 6. System response with the phase resetting controller deactivated for a forward
perturbation.
Fig. 7. System response with the phase resetting controller deactivated for a forward
perturbation.
Fig. 8. System response with the phase resetting controller deactivated for a backward
perturbation.
1 Companion website: https://youtu.be/C8FmdlcX7Ts.

100
Ju. Cristiano et al.
Fig. 9. System response with the phase resetting controller activated for a backward
perturbation.
5
Conclusions
The phase resetting mechanism is a suitable feedback strategy for humanoid
robots with reduced computational capabilities. Furthermore, according to the
experiments, this feedback strategy has a quick and eﬀective response able to
deal with considerable external perturbations.
In this paper a methodology to automatically characterize phase resetting
controllers taking advantage of simulation experiments is presented. Some exper-
iments have been performed, however, an exhaustive characterization must be
made in order to deal with all possible unstable situations for the humanoid
robot. Thus, future work will include the rigorous study of feedback controllers
in order to cope with more complex external perturbations. Future research will
be carried out in order to propose a generalized modelling and characterization of
phase resetting controllers under diﬀerent conditions. The ﬁnal goal is to ﬁrstly
develop an initial model for the controllers in a simulated scenario which can be
later adjusted to the real robot by performing a reduced number of experiments
compared to those actually needed in order to characterize these controllers.
References
1. Nakanishi, M., Nomura, T., Sato, S.: Stumbling with optimal phase reset during
gait can prevent a humanoid from falling. Biol. Cybern. 95(5), 503–515 (2006).
http://dx.doi.org/10.1007/s00422-006-0102-8
2. Nomura, T., Kawa, K., Suzuki, Y., Nakanishi, M., Yamasaki, T.: Dynamic stability
and phase resetting during biped gait. Chaos Interdisc. J. Nonlinear Sci. 19(2),
026103 (2009). 1–12. http://dx.doi.org/10.1063/1.3138725
3. Cristiano, J., Garcia, M.A., Puig, D.: Deterministic phase resetting with predeﬁned
response time for cpg networks based on matsuokas oscillator. Robot. Auton. Syst.
74, 88–96, (2015). http://dx.doi.org/10.1016/j.robot.2015.07.004
4. Cristiano, J., Puig, D., Garcia, M.A.: Eﬃcient locomotion control of biped robots
on unknown sloped surfaces with central pattern generators. Electron. Lett. 51(3),
220–222 (2015). http://dx.doi.org/10.1049/el.2014.4255
5. Matsuoka, K.: Sustained oscillations generated by mutually inhibiting neurons
with adaptation. Biol. Cybern. 52(6), 367–376 (1985). https://link.springer.com/
article/10.1007/BF00449593

Automatic Design of Phase Resetting Controllers
101
6. Morimoto, J., Endo, G., Nakanishi, J., Cheng, G.: A biologically inspired biped
locomotion strategy for humanoid robots: modulation of sinusoidal patterns by a
coupled oscillator model. IEEE Trans. Robot. 24(1), 185–191 (2008). http://dx.
doi.org/10.1109/TRO.2008.915457
7. Matsuoka, K.: Analysis of a neural oscillator. Biol. Cybern. 104(4–5), 297–304
(2011). http://dx.doi.org/10.1007/s00422-011-0432-z
8. Zhang, D., Poignet, P., Widjaja, F., Ang, W.T.: Neural oscillator based control for
pathological tremor suppression via functional electrical stimulation. Control Eng.
Pract. 19(1), 74–88 (2011). https://doi.org/10.1016/j.conengprac.2010.08.009
9. Gouaillier, D., Hugel, V., Blazevic, P., Kilner, C., Monceaux, J., Lafourcade, P.,
Marnier, B., Serre, J., Maisonnier, B.: Mechatronic design of NAO humanoid.
In: Proceedings of the IEEE International Conference on Robotics and Automa-
tion (ICRA), pp. 769–774. IEEE (2009). http://dx.doi.org/10.1109/ROBOT.2009.
5152516
10. Cristiano, J., Puig, D., Garcia, M.: Generation and control of locomotion patterns
for biped robots by using central pattern generators. J. Phys. Agents 8(1), 40–47
(2017). http://dx.doi.org/10.14198/JoPha.2017.8.1.06

Interface Design of Haptic Feedback on Teleoperated
System
Francisco Rodríguez-Sedano1(✉)
, Pere Ponsa2
, Pablo Blanco-Medina1
,
and Luis Miguel Muñoz2
1 Robotic Group, University of León, MIC, Campus de Vegazana, s/n, 24071 León, Spain
francisco.sedano@unileon.es
2 Automatic Control Department, Universitat Politècnica de Catalunya Barcelona Tech,
Av. Víctor Balaguer, 1, 08800 Vilanova i la Geltrú, Barcelona, Spain
Abstract. The use of teleoperation systems can provide substantial advantages
in various ﬁelds such as remote explosive dismantling and rescue operations, as
well as improve the accuracy on certain tasks like surgery. These systems oﬀer
feedback mainly based on visual information of the task performed showing rele‐
vant data of the systems used as well as their surroundings. Our proposal involves
the use of haptic devices to remotely control a robot in order for the user to receive
haptic feedback alongside the visual information. In a ﬁrst design that several
experts evaluated, it was concluded that the interface used had to be substantially
improved. This communication describes the process developed to design and
evaluate an interface to improve user interaction with the teleoperation system.
Keywords: Teleoperation · Human-Robot interface · Baxter robot · Haptics
1
Introduction
Certain tasks such as surgery, explosive dismantling and rescue operations can be quite
complex and even dangerous to perform in a direct manner. In these kind of situations,
teleoperated robots or other systems can be very helpful by reducing the risk involved
and potentially increasing the overall accuracy of said tasks by reducing the human error
factor that can be caused by various motives, such as hand tremor. In fact, these type of
systems are rather proﬁcient in the introduced areas and tasks [1].
These systems must provide proper enhancements for the user experience. In order
to do this, they use diﬀerent types of interfaces (e.g. cameras, microphones or input
devices) so they can provide enough information to the user and complete his/her
perception of the controlled device and its surroundings. However, the video feedback
obtained from the cameras can be limited by certain technical constraints [2, 3], such as
a decreased view due to obstacles or even bad resolution from the camera itself.
To make up for these limitations, haptic devices can be used in certain applications
to help the user understand better the systems involved as well as the items in the remote
environment [4, 5].
On the already mentioned scenario about surgery, the tissue’s texture or the weight
of other items could be replicated with the help of haptic devices. This information can
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_9

be presented on a graphical user interface, but the mere representation by numerical
standards would not be as eﬀective as the actual feeling those numbers represent in
reality. We believe this type of information to be very valuable in the context of tele‐
operated systems.
The motivation behind this work is to design and evaluate an eﬃcient interface to
test whether the feel of the touch improves the performance and user experience in
teleoperation tasks. So we can verify it, we have used an experiment designed in the
robotics group of the University of León and that has a suﬃciently long exposure to the
haptic feedback [6].
The teleoperation system of that experiment consists of a Geomatic Touch haptic
interface and a Baxter Research Robot, acting as the master and the slave devices
respectively. Additionally, two external cameras will be used to complete the user’s
perception of the controlled environment. This environment will be described in more
detail in the next section.
The physical appearance of the Baxter robot was designed to create the impression
that Baxter is a member of the team: emotional expressions, human-robot task collab‐
oration [7]. However, in this work, the most important feature is Baxter can detect when
there are no more objects to pick and the aim is show the Baxter’s behavior inside the
graphical interface.
Regarding within the user experience in teleoperated systems, this work follows the
recommendations of the ISO 9241-920 Ergonomics of human-system interaction (inter‐
face design, user centered design) [8].
This document is organized as follows. Section 2 will cover in greater detailed the
environment in which the experiment will be performed and the methodology used to
design the interface. Section 3 will describe the results obtained in the evaluation of the
designed interface prototypes. Section 4 will give some recommendations from the point
of view of usability testing. Finally, the document ends with our conclusions and future
lines.
2
Materials and Methods
Our main goal with this project is to prove the utility of haptic feedback in remote control
situations and their ability to improve the user’s perception and overall performance in
diﬀerent scenarios while performing varied tasks. To do this, we propose an experiment
in which we will teleoperated a robot with a haptic device trying to complete a simple
task while providing haptic feedback.
In addition, in a second phase of the project, we will also verify our initial hypothesis
via a questionnaire that will be ﬁlled by users who are familiar with these types of
systems, which will also help us to improve our design.
The performed task will be similar to a previous design introduced in [6]. In a ﬁrst
phase, this task was performed without the use of a proper interface, and the conclusions
we achieved revealed that the interaction with the environment was not natural, so it
would have to be improved in future works. This is the goal of this paper. But ﬁrst let’s
brieﬂy describe the working environment.
Interface Design of Haptic Feedback on Teleoperated System
103

2.1
The Environment
A teleoperation system is composed by at the very least two devices, a master device
and a slave device that establishes communication between them by using a communi‐
cation channel that connects them physically. The master’s movements will be replicated
on the slave, being able to control the latter’s motion. In our case, the slave will commu‐
nicate the master speciﬁc information regarding the environment that will be replicated
in the latter so as to provide haptic feedback in certain scenarios. Control of the teleop‐
eration system with human interaction.
Fig. 1. The teleoperated system.
As we have already mentioned, the main goal of our project is to demonstrate that
haptic feedback improves the user’s experience and performance, so our master device
will be a haptic device. As for our slave device, we will use a Baxter Research Robot,
completing our master-slave system.
The task will be simple to perform, and the environment will suit Baxter’s large size.
Furthermore, in order to prove the utility of the interface’s feedback, the user will not
be able to directly see the slave device by regular means, ﬁnding an obstacle if he tries
to do so. Additionally, the user will be able to use a second monitor to perform training
previous to the aforementioned task so as to grasp easily the concepts of movement,
picking and releasing. This training process will not have haptic feedback available,
merely focusing the user on understanding said concepts. The entire environment can
be seen in the Fig. 2.
104
F. Rodríguez-Sedano et al.

Fig. 2. Detail of the (users, teleoperated) environment.
In order to decrease communication delays as much as possible, both of the devices
are connect to the same workstations via Ethernet cables.
For the experiment, our master device must be able to send its movements to the
slave robot, but it also has to be able to provide the user with kinesthetic information.
That is, to reﬂect the forces or vibrations sensed by Baxter. Because of these reasons,
we decided to use a Geomatic Touch haptic device, a 6DOF device that has 3 DOF
dedicated to standard movements (X, Y and Z Cartesian coordinates) while the
remaining 3 help replicate orientation (pitch, roll and yaw movements)
In order to make it work in our workstation, we have used the package “Phantom
Omni”, developed by Suarez-Ruiz [9] and adapting it to our ROS Indigo distribution
due to it being developed in ROS Hydro.
Our chosen slave device was Baxter Research Robot. Baxter consists of 2 × 7-degree-
of-freedom arms, with inbuilt feedback systems, although for our experiment we only
used one of the arms. It was developed to allow collaborative human-robot co-work and
to improve Human-Robot Interaction.
Its 7DOF system provides kinematic redundancy, which allows for object manipu‐
lation enhancement. It also has several mounted sensors, one camera located in each
gripper that will be used for display purposes and an infrared sensor range that has a
4-40 cm range. It is connected to the same workstation as the Master device via Ethernet
so as to reduce delay between them as much as possible.
In order to interact with the environment, we must be able to properly translate the
movements of the master device to Baxter. However, as pointed as pointed in our
previous work [6], both of the devices have a diﬀerent number of DOFs. Due to this,
adjustments must be carried out in order to reduce the complexity of the movement
Interface Design of Haptic Feedback on Teleoperated System
105

correlation. Our approach will be to reduce the number of joints used in both systems,
trying to replicate a standard 3DOF movement on the X, Y and Z axis.
Speciﬁc details about our implementation can be found in our previous work that
concerns similar issues regarding teleoperation of the same systems [6]. Some adjust‐
ments have been made to this particular implementation, including an increased weight
of the checkers piece grabbed as well as the ability to stop the teleoperation at any given
time. This is due to the fact that the user can run out of physical space to perform speciﬁc
movement because of the diﬀerent workspaces in both devices.
With this function, when the user stops the teleoperation he or she can reset the haptic
device to a diﬀerent position without the movement being replicated, eﬀectively
increasing their workspace and reducing frustration and other negative consequences of
the limited original space.
2.2
The Interface
Even though our primary objective is to ascertain the utility of haptic feedback in tele‐
operated environments, we must also provide the user with a proper interface that allows
him to grasp the concept and layout of the remote environment, as well as displaying
relevant information about the teleoperation itself.
A ﬁrst version of the interface was designed for this experiment can be seen in Fig. 3.
Fig. 3. First version of the interface.
Once this ﬁrst prototype interface was designed, we had to evaluate it. For this, a
group of 7 expert users was chosen. Five of these experts were familiar with the use of
haptic devices. The other two had developed the evaluation methodology to be used and
described below.
106
F. Rodríguez-Sedano et al.

To evaluate the interface by taking into account experts’ perceptions about improve‐
ments in the design, we decided to do it from a qualitative perspective. For this, we
collected experts’ impressions using the think aloud technique [10] during the devel‐
opment of a task deﬁned with the teleoperated system. After their experience with the
system, a semi-structured interview was conducted. Speciﬁc software was used to record
screen and user actions. Also was recorded the interview with a video camera.
After analyzing the results of this ﬁrst evaluation, a second prototype interface was
designed, taking into account the suggestions of the experts and later evaluated
according to the same methodology.
2.3
Methodology
For the evaluation of interface prototypes, we used the Ergonomic Guideline for Super‐
visory Control Interface Design (GEDIS). The GEDIS guideline provides a series of
recommendations to consider when designing an interface or evaluate interfaces already
created [11]. GEDIS is not a standard, it is a guideline that seeks to cover all the aspects
of the interface design. But in our case study it is appropriate because it provides a clear
view of the usability of the interface by the end users.
The guide is based on the analysis and evaluation of 10 indicators to ﬁnally get an
overall index of the interface quality and possible areas for improvement. For the correct
use of the GEDIS guideline, the collaboration between the teleoperated technical team
and the human factor technician is necessary. As follows two evaluations of the interface
are presented.
The indicators values are used to calculate the global evaluation by solving the
following formula:
Global_evaluation =
10∑
i=1
piindi
10∑
i=1
pi
(1)
Where, ind = indicator and p = weight. all indicators have the same weight
(p1 = p2… = p10 = 1). The guide recommends that the global evaluation index should
not be lower than 3 points. A positive evaluation should reach at least 4 points.
The results of the evaluation of both prototypes are discussed below.
3
Results
The Table 1 show two evaluations of ﬁrst and second versions of the interface, (where
A = appropriate, M = medium, N.A. = Not appropriate). In this study only seven indi‐
cators were measured in the ﬁrst evaluation and each indicator in the second evaluation.
Interface Design of Haptic Feedback on Teleoperated System
107

Table 1. Applying the GEDIS guideline.
Indicator and sub-indicator
name
First Evaluation: April’17
Numeric/qualitative value
Second Evaluation: July’17
Numeric/qualitative value
Architecture
2.6
2.6
Map existence
[YES, NO] [5, 0] 0
[YES, NO] [5, 0] 0
Number of levels le
[le < 4, le > 4] [5, 0] 5
[le < 4, le > 4] [5, 0] 5
Division: plant, area, subarea,
team
[a, m, na] [5, 3, 0] 3
[a, m, na] [5, 3, 0] 3
Distribution
2.0
3.7
Model comparison
[a, m, na] [5, 3, 0] 3
[a, m, na] [5, 3, 0] 3
Flow process
[clear, medium, no clear] [5, 3,
0] 0
[clear, medium, no clear] [5, 3,
0] 5
Density
[a, m, na] [5, 3, 0] 3
[a, m, na] [5, 3, 0] 3
Navigation
0
3
Relationship with architecture [a, m, na] [5, 3, 0] 0
[a, m, na] [5, 3, 0] 3
Navigation between screen
[a, m, na] [5, 3, 0] 0
[a, m, na] [5, 3, 0] 3
Color
5.0
5.0
Absence of non-appropriated
combinations
[YES, NO] [5, 0] 5
[YES, NO] [5, 0] 5
Color number c
[4 < c<7, c > 7] [5, 0] 5
[4 < c<7, c > 7] [5, 0] 5
Blink absence (no alarm
situation)
[YES, NO] [5, 0] 5
[YES, NO] [5, 0] 5
Contrast screen versus
graphics objects
[a, m, na] [5, 3, 0] 5
[a, m, na] [5, 3, 0] 5
Relationship with text
[a, m, na] [5, 3, 0] 5
[a, m, na] [5, 3, 0] 5
Text Font
4.5
5.0
Font number f
[f < 4, f > 4] 5
[f < 4, f > 4] 5
Absence of small font (smaller
8)
[YES, NO] [5, 0] 5
[YES, NO] [5, 0] 5
Absence of non-appropriated
combinations
[YES, NO] [5, 0] 5
[YES, NO] [5, 0] 5
Abbreviation use
[a, m, na] [5, 3, 0] 3
[a, m, na] [5, 3, 0] 5
Status of the devices
1.5
2.5
Uniforms icons and symbols
[a, m, na] [5, 3, 0] 3
[a, m, na] [5, 3, 0] 5
Status team representativeness [YES, NO] [5, 0] 0
[YES, NO] [5, 0] 0
Process values
3.0
3.0
Visibility
[a, m, na] [5, 3, 0] 3
[a, m, na] [5, 3, 0] 3
Location
[a, m, na] [5, 3, 0] 3
[a, m, na] [5, 3, 0] 3
Graphs and tables
Non assessed
Non assessed
Data-entry commands
Non assessed
3.0
Visibility
[a, m, na] [5, 3, 0] 3
Usability
[a, m, na] [5, 3, 0] 3
Feedback
[a, m, na] [5, 3, 0] 3
Alarms
Non assessed
Non assessed
108
F. Rodríguez-Sedano et al.

From architecture’s perspective, the Fig. 3. shows a single screen application, which
can move in the direction of a broad structure and not deep. It is recommended to add
a main home screen. In this main screen it is necessary to diﬀerentiate between the use
that will be made by the user and the use that the facilitator will make.
– The user (expert in teleoperated tasks) will access his screen related to the task to be
performed.
– The facilitator (expert in usability testing) will access the conﬁgurable parameters:
way of recording the data, frames per second that are monitored on the screen, list
of numerical values for tracking failure in the grip of the part, Number of collisions
detected.
– It is necessary to evaluate the migration of the data gathering to a ﬁle in Excel for a
later analysis.
– Recording the video sequence of the cameras will allow analyze the Baxter’s behavior
and the user behavior.
It is advisable for the user to identify the necessary information in the local area
(user’s environment) and to diﬀerentiate it from the necessary on-screen information in
the remote area. In conventional industrial control room, the graphic interface presents
a certain complexity. Instead, in teleoperation it is more important to enhance the sense
of presence of the user in the workplace where the task is performed, so the graphical
interface is less complex.
In the case of two screens, developers could name each screen at the top of the screen.
For the screen of Fig. 3, for example, the name can be, “Teleoperation” for the pick and
place task. The process ﬂow is not clear since although we can correctly see the task in
the image provided by the cameras, it is confused with the lower part in which “Save
Data” is indicated. Should the user save the data? In any case, the user must have a guide
that indicates a sequential action: monitor, use the Phantom interface to perform the task,
save data.
The main distribution recommendation is:
– Split the screen into two parts
– Put name above each image of the camera (Camera in Body Robot, Camera in Robot
Arm)
– Developers could add a gray rectangle and it is sent to the background, this will
highlight the box with the information of faults, falls, and collisions. These variables
are perceived by the user as grouped.
– It is convenient to sort the information of the cited variables, analyze the size of each
image: the two images could have the same size
– The Save Data image suggests that the user quit the application.
For Navigation indicator, it is recommended to add screens and navigation buttons
between screens so that the user can move from the main screen to the screen of Fig. 3,
and can return or quit the application.
For the Color indicator, the use of gray background is appropriate as it highlights
the image of the cameras, this improves the sense of presence of the user in the remote
place where the task is carried out.
Interface Design of Haptic Feedback on Teleoperated System
109

It is important to evaluate the color of the table in which the piece of the checkers
game is placed. That color is the one that is captured by the camera on the graphic screen,
hence the user must see through the image of the camera the contrast between the bottom
of the table and the piece to be taken.
Use of red color should be revisited. Usually the red color is used to indicate alarms,
faults, etc., so it is necessary to evaluate if it is convenient that the checker piece and
the target have a red color.
In any case the image of the silhouette of the user is not appropriate that is red, since
our focus is focused there, when it should focus on the image transmitted by the cameras.
For Text indicator, the recommendation is avoid font size below eight. Some text
words are in Spanish Language, however, appears Swap in English Language.
Check the nomenclature used for the cameras: webcam is the body Baxter camera?
For Status of the Devices indicator, Check the iconography used because it shows a
combination of text and some icons. The save icon suggests leaving the application.
When the user is using the Phantom interface and the robot is moving, this feedback
of the movement should be visible on the screen. Therefore, is missing a small box in
the upper center that indicates the state of the robot, if it is at rest, if it is in movement,
if the grippers is open or closed. Round objects, emulating LED lights within a text box,
can give us this information.
For process values, it should be checked which variables and which numeric value
should appear on the screen. Failure, collision information allows log and after show
historical trends. The question is whether this information should be consulted while the
task is being carried out or should be consulted at the end of the task.
Along the haptic feedback, it is necessary to assess if it can be speciﬁed if the force
that the user’s hand is exerting on the Phantom interface is low, medium or high.
Graphs and Tables, indicator is not assessed.
Data-entry commands is not assessed.
Alarms indicators is not assessed.
Finally, the global evaluation index of the ﬁrst interface is 2.6 (in a 0-5 numeric
scale).
3.1
Proposal for Interface Improvements
After the initial feedback received by applying the GEDIS methodology, we improved
our original interface to an upgraded version that would be used for the future task and
related questionnaires. This interface is displayed in the Fig. 4.
Among other things, it shows the user a display of two interchangeable cameras for
better perception, boxes indicating if teleoperation is currently enabled or not as well as
haptic feedback and the amount of fails, drops and collisions caused during the teleop‐
eration.
It also displays a progress bar that indicates the proximity of an item by using
Baxter’s infrared capabilities.
A “Fail” is caused by the user opening the gripper with the haptic to try to grab a
piece, only to fail at said task by a wrong measurement of the actual distance to the piece.
110
F. Rodríguez-Sedano et al.

Similarly, a “Drop” will be added when the user drops a properly grabbed piece to the
table.
Finally, collisions are also counted if the user hits surrounding items with enough
speed or force. However, Baxter has proven quite sturdy when registering forces high
enough to cause a proper collision, the minimum of said force being remarkably high.
With that said, Baxter is more sensible to fast movements done on himself, so collisions
will serve both as a register of high enough impacts and fast movements done in the
master device. An example of said movement would be a quick circular move, which
will cause Baxter to complain about the speed reached when trying to replicate it, but
never compromising the stability of the teleoperation.
In addition to these values, we will also record the overall time spent performing the
task as well as the time spent on each of the cameras in order to understand which of
them is more useful for the users.
After applying GEDIS to evaluate this new prototype, the score has been improved
in the following indicators: distribution, navigation, text font and status of the devices.
In addition, it has been possible to evaluate the indicator “Data-entry commands” that
improves the ﬁnal score. The global evaluation index of the second version is 3.5 (in a
0-5 numeric scale). This indicates that the evaluation of the ﬁrst prototype has been
improved, although it does not reach 4.0 points necessary for a positive evaluation.
Fig. 4. Second version of the interface.
Interface Design of Haptic Feedback on Teleoperated System
111

4
Discussion
In addition to evaluating the interface, the experts who participated in the experiment
have reported some improvement on the task to be performed in the experiment. In
previous work [6] this task had been designed following the Guideline for Ergonomic
Haptic Interaction Design (GEHID). The GEHID guideline for ergonomic haptic inter‐
action design provides a method that seeks to cover aspects of the haptic interface design
and the human-robot task in order to improve the performance of the overall teleoperated
system [12].
The task developed in the ﬁrst test (April 2017) is correctly carried out, but the
temporal interval in which forces and moments can appear due to the haptic feedback
on the hand of the human performing the task, is very brief. Thus, performing the task
with or without haptic feedback may not have too much diﬀerentiation.
It is recommended to think about a task. For example: insertion of one piece’s part
into another, in which the contact between the parts causes a force/momentum feedback
on the hand of the user that is more intense (and measurable) and is carried out in an
interval time.
Increasing the complexity of the task, the time to carry it out, the eﬀort of the human
hand, can lead to a posteriori evaluation of physical eﬀort, fatigue, comfort.
Following the user-centered approach, the usability testing in laboratory follow a set
of steps. The main recommendation is improving the procedure of these steps. For
instance:
– Deﬁne a consent form.
– Explain the aim of the experimental study, location, data….
– Technical description of the teleoperated task. The user could be expert or novice
and must pay attention of the right use of the teleoperated system.
– At the end of the experimental session, deﬁne the satisfaction questionnaires adapted
for this study.
Taking into account all these recommendations, the design of the interface must be
modiﬁed before using it in the experiment.
5
Conclusions and Future Work
This paper presents our interface design for a haptic teleoperation system and its eval‐
uation through the GEDIS methodology.
The use of haptic devices can increase the user’s performance in a wide margin of
diﬀerent tasks performed in various areas. However, it has to be complemented with the
use of a graphical interface to oﬀer both the haptic feedback and the visual feedback,
enhancing the user’s experience with the remote environment as much as possible. We
will further research and improve the topics presented in our previous work using our
new developed interface, improving the user’s perception of the remote environment.
Our future work will be focused on conducting an experiment designed to evaluate
the use of haptic feedback in diﬀerent tasks with a more qualitative approach, analyzing
112
F. Rodríguez-Sedano et al.

the user’s performance by recording data that is relevant to the teleoperated system. This
experiment will follow recommendations proposed by the GEHID guideline in order to
prove our initial proposal about the implementation of haptic feedback in teleoperated
systems.
References
1. Vertut, J., Coiﬀet, P.: Teleoperation and Robotics. Applications and Technology, vol. 3B.
Springer, Netherlands (1985)
2. Woods, D.D., Tittle, J., Feil, M., Roesler, A.: Envisioning human-robot coordination in future
operations. IEEE Trans. Syst. Man Cybern. C (Appl. Rev.) 34(2), 210–218 (2004)
3. Woods, D., Watts, J.: How not to have to navigate through too many displays. In: Helander,
M., Landauer, T., Prabhu, P. (eds.) Handbook of Human-Computer Interaction. Restricting
the Field of View: Perceptual and Performance Eﬀects, 2nd edn., pp. 1177–1201. Elsevier
Science, Amsterdam (1997)
4. Son, H.I., Franchi, A., Chuang, L.L., Kim, J., Bulthoﬀ, H.H., Giordano, P.R.: Human-centered
design and evaluation of haptic cueing for teleoperation of multiple mobile robots. IEEE
Trans. Syst. Man Cybern. B (Cybern.) 43(2), 597–609 (2013)
5. Sitti, M., Hashimoto, H.: Teleoperated touch feedback from the surfaces at the nanoscale:
modeling and experiments. IEEE ASME Trans. Mechatron. 8(2), 287–298 (2003)
6. Rodríguez Lera, F., Blanco Medina, P., Inyesto Alonso, L., Esteban, G., Rodríguez Sedano,
F.: Strategies for Haptic-Robotic Teleoperation in Board Games: Playing Checkers with
Baxter, WAF 2016, June 2016
7. Faneuﬀ, J.: Designing for Collaborative Robotics. In: Follett, J. (ed.) Designing for Emerging
Technologies. O’Reilly (2014)
8. ISO. ISO 9241-920. Ergonomics of Human-system Interaction - Part 920: Guidance on tactile
and haptic interactions, Geneva (2009)
9. Suárez-Ruiz, F.: ROS Packages for Sensable Phantom Omni Device, GitHub (2014), https://
github.com/fsuarez6/phantom_omni. Accessed 06 Apr 2016
10. Lewis, C.H.: Using the “Thinking Aloud” Method in Cognitive Interface Design. Technical
Report RC-9265. IBM (1982)
11. Ponsa, P., Díaz, M.: Creation of an ergonomic guideline for supervisory control interface
design. Eng. Psychol. Cognit. Ergon. (2007)
12. Muñoz, L.M., Ponsa, P., Casals, A.: Design and development of a guideline for ergonomic
haptic interaction. In: Hippe, Z., Kulikowski, J.L., Mroczek, T. (eds.) Human-Computer
Systems Interaction: Backgrounds and Applications 2: Part 2, pp. 15–19. Springer,
Heidelberg (2012)
Interface Design of Haptic Feedback on Teleoperated System
113

Machine Learning in Robotics

Deep Networks for Human Visual Attention:
A Hybrid Model Using Foveal Vision
Ana Filipa Almeida, Rui Figueiredo(B), Alexandre Bernardino,
and Jos´e Santos-Victor
Institute for Systems and Robotics, Instituto Superior T´ecnico,
Universidade de Lisboa, Lisbon, Portugal
ana.j.almeida@ist.utl.pt,
{ruifigueiredo,alex,jasv}@isr.tecnico.ulisboa.pt
Abstract. Visual attention plays a central role in natural and artiﬁcial
systems to control perceptual resources. The classic artiﬁcial visual atten-
tion systems uses salient features of the image obtained from the infor-
mation given by predeﬁned ﬁlters. Recently, deep neural networks have
been developed for recognizing thousands of objects and autonomously
generate visual characteristics optimized by training with large data
sets. Besides being used for object recognition, these features have been
very successful in other visual problems such as object segmentation,
tracking and recently, visual attention. In this work we propose a bio-
logically inspired object classiﬁcation and localization framework that
combines Deep Convolutional Neural Networks with foveal vision. First,
a feed-forward pass is performed to obtain the predicted class labels.
Next, we get the object location proposals by applying a segmentation
mask on the saliency map calculated through a top-down backward pass.
The main contribution of our work lies in the evaluation of the perfor-
mances obtained with diﬀerent non-uniform resolutions. We were able
to establish a relationship between performance and the diﬀerent lev-
els of information preserved by each of the sensing conﬁgurations. The
results demonstrate that we do not need to store and transmit all the
information present on high-resolution images since, beyond a certain
amount of preserved information, the performance in the classiﬁcation
and localization task saturates.
Keywords: Computer
vision ·
Deep
neural
networks ·
Object
classiﬁcation and localization · Space-variant vision · Visual attention
1
Introduction
The available human brain computational resources are limited, therefore it is
not possible to process all the sensory information provided by the visual per-
ceptual modality. Selective visual attention mechanisms are the fundamental
mechanisms in biological systems, responsible for prioritizing the elements of
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_10

118
A.F. Almeida et al.
the visual scene to be attended. Likewise, an important issue in many com-
puter vision applications requiring real-time visual processing, resides in the
involved computational eﬀort [2]. Therefore, in the past decades, many biologi-
cally inspired attention-based methods and approaches, were proposed with the
goal of building eﬃcient systems, capable of working in real-time. Hence, atten-
tion modeling is still a topic under active research, studying diﬀerent ways to
selectively process information in order to reduce the time and computational
complexity of the existing methods.
Nowadays, modeling attention is still challenging due to the laborious and
time-consuming task that is to create models by hand, trying to tune where
(regions) and what (objects) the observer should look at. For this purpose, bio-
logically inspired neural networks have been extensively used, since they can
implicitly learn those mechanisms, circumventing the need of creating models
by hand.
Our work is inspired by [4] which proposed to capture visual attention
through feedback Deep Convolutional Neural Networks. Similarly in spirit, we
propose a biologically inspired hybrid attention model, that is capable of eﬃ-
ciently recognize and locate objects in digital images, using human-like vision.
Our method comprises two steps: ﬁrst, we perform a bottom-up feed-forward
pass to obtain the predicted class labels (detection). Second, a top-down back-
ward pass is made to create a saliency map that is used to obtain object location
proposals after applying a segmentation mask (localization). The main contri-
butions of this paper are the following: ﬁrst, we evaluate the performance of
our methodology for various well-known Convolutional Neural Network architec-
tures that are part of the state-of-the-art in tasks of detection and localization
of objects when combined with multi-resolution, human-inspired, foveal vision.
Then, we establish a relationship between performance and the diﬀerent levels
of information preserved by foveal sensing conﬁgurations.
The remainder of this paper is organized as follows: Sect. 2 overviews the
related work and some fundamental concepts behind the proposed attentional
framework. In Sect. 3.1, we describe in detail the proposed methodologies, more
speciﬁcally, a theoretical explanation of an eﬃcient artiﬁcial foveation system
and a top-down, saliency-based mechanism for class-speciﬁc object localization.
In Sect. 4, we quantitatively evaluate the our contributions. Finally, in Sect. 5,
we wrap up with conclusions and ideas for future work.
2
Background
The proposed object classiﬁcation and localization framework uses several bio-
logically inspired attention mechanisms, which include space-variant vision and
Artiﬁcial Neural Networks (ANN). As such, in the remainder of this section we
describe the fundamental concepts from neuroscience and computer science on
which the proposed framework is based.

A Hybrid Attention Model Using Foveal Vision
119
2.1
Space-Variant Vision
In this work, we propose to use a non-uniform distribution of receptive ﬁelds
that mimics the human eye for simultaneous detection and localization tasks.
Unlike the conventional uniform distributions which are typical in artiﬁcial visual
systems (e.g. in standard imaging sensors), the receptive ﬁeld distribution in the
human retina is composed by a region of high acuity – the fovea – and the periph-
ery, where central and low-resolution peripheral vision occurs, respectively [14].
The central region of the retina of the human eye named fovea is a photore-
ceptor layer predominantly constituted by cones which provide localized high-
resolution color vision. The concentration of these photoreceptor cells reduce
drastically towards the periphery causing a loss of deﬁnition. This space-variant
resolution decay is a natural mechanism to decrease the amount of information
that is transmitted to the brain (see Fig. 2). Many artiﬁcial foveation meth-
ods have been proposed in the literature that attempt to mimic similar behav-
ior: geometric method [15], ﬁltering-based method [16] and multi-resolution
methods [5].
2.2
Deep Convolutional Neural Networks
Deep neural networks are a subclass of Artiﬁcial Neural Networks (ANN) and
are characterized by having several hidden layers between the input and out-
put layers. The deep breakthrough occurred in 2006 when researchers brought
together by the Canadian Institute for Advanced Research (CIFAR) were capa-
ble of training networks with much more layers for the handwriting recognition
task [9].
As far as visual attention is concerned, the most commonly used are the
Convolutional Neural Networks (CNN), that are feed-forward Deep ANN that
take into account the spatial structure of the inputs. These, have the ability to
learn discriminative features from raw data input and have been used in several
visual tasks like object recognition and classiﬁcation.
A CNN is constituted by multiple stacked layers that ﬁlter (convolve) the
input stimuli to extract useful and meaningful information depending on the
task at hand. These layers have parameters that are learned in a way that
allows ﬁlters to automatically adjust to extract useful information without fea-
ture selection so there is no need to manually select relevant features. In this
work we study the performance of state-of-the-art CNN architectures that were
within our attentional framework, namely, CaﬀeNet [8], GoogLeNet [13] and
VGGNet [12].
3
Methodologies
Our hybrid detection and localization methodology can be brieﬂy outlined as
follows: In a ﬁrst feed-forward pass, a set of object class proposals is computed
(Sect. 3.2) which are further analyzed via top-down backward propagation to
obtain proposals regarding the location of the object in the scene (Sect. 3.2).

120
A.F. Almeida et al.
More speciﬁcally, for a given input image I, we begin by computing a set
of object class proposals by performing a feed-forward pass. The probability
scores for each class label (1 000 in total) are collected by accessing the network’s
output softmax layer. Then, retaining our attention on the ﬁve highest predicted
class labels, we compute the saliency map for each one of those predicted classes
(see Fig. 3). Then, a top-down back-propagation pass is done to calculate the
score derivative of the speciﬁc class c. The computed gradient indicates which
pixels are more relevant for the class score [11]. In the remainder of this section,
we describe in detail the components of the proposed attentional framework.
3.1
Artiﬁcial Foveal Vision
Our foveation system is based on the method proposed in [3] for image compres-
sion (e.g. in encoding/decoding applications) which, unlike the methods based
on log-polar transformations, is extremely fast and easy to implement, with
demonstrated applicability in real-time image processing and pattern recogni-
tion tasks [1].
Fig. 1. A summary of the steps in the foveation system with four levels. The image g0
corresponds to the original image and h0 to the foveated image. The thick up arrows
represent sub-sampling and the thick down arrows represent up-sampling.
Our approach comprises four steps that go as follow. The ﬁrst step consists on
building a Gaussian pyramid with increasing levels of blur, but similar resolution.
The ﬁrst pyramid level (level 0) contains the original image g0 that is down-
sampled by a factor of two and low-pass ﬁltered, yielding the image g1 at level 1.

A Hybrid Attention Model Using Foveal Vision
121
(a) f0 = 30
(b) f0 = 60
(c) f0 = 90
Fig. 2. Example images obtained with our foveation system where f0 deﬁnes the size of
the region with highest acuity (the fovea), from a 224 × 224 image uniform resolution
image.
More speciﬁcally, the image gk+1 can be obtained from the gk via convolution
with 2D isotropic and separable Gaussian ﬁlter kernels of the form
G(u, v, σk) =
1
2πσk
e
−u2+v2
2σ2
k ,
0 < k < K
(1)
where u and v represent the image coordinates and σk = 2k−1σ1 the Gaussian
standard deviation at the k-th level. These images are up-sampled to impose
similar resolution at all levels. Next, we compute a Laplacian pyramid from the
diﬀerence between adjacent Gaussian levels. The Laplacian pyramid comprises a
set of error images where each level represents the diﬀerence between two levels
of the previous output (see Fig. 1). Finally, exponential weighting kernels are
multiplied by each level of the Laplacian pyramid to emulate a smooth fovea.
The exponential kernels are given by
k(u, v, fk) = e
−(u−u0)2+(v−v0)2
2f2
k
,
0 ≤k < K
(2)
where fk = 2kf0 denotes the exponential kernel standard deviation at the k-th
level. These kernels are centered at a given ﬁxation point (u0, v0) which deﬁnes
the focus of attention. Throughout the rest of this paper, we assume that u0 =
v0 = 0. Figure 1 exempliﬁes the proposed foveation model with four levels and
Fig. 2 depicts examples of resulting foveated images.
Information Reduction. The proposed foveal visual system is a result of a
combination of low-pass Gaussian ﬁltering and exponential spatial weighting. To
be possible to establish a relationship between signal information compression
and task performance, one must understand how the proposed foveation system
reduces the image information depending on the method’s parameters (i.e. fovea
and image size).

122
A.F. Almeida et al.
Low-pass Gaussian Filtering. Let us deﬁne the original high-resolution image as
i(u, v) to which corresponds the discrete time Fourier Transform I(ejwu, ejwv).
The ﬁltered image O(ejwu, ejwv) at each pyramid level is given by the convolution
theorem as follows
O(ejwu, ejwv) = I(ejwu, ejwv) ∗G(ejwu, ejwv).
(3)
Following the Parseval’s theorem that describes the unitarity of a Fourier Trans-
form, the signal information of the original image i is given by
Ei =
+∞

u=−∞
+∞

v=−∞
|i(u, v)|2dudv =
1
4π2
 π
−π
 π
−π
|I(ejwu, ejwv)|2dwudwv.
(4)
and the information in the ﬁltered image o is given by
Eo =
+∞

u=−∞
+∞

v=−∞
|o(u, v)|2dudv
=
1
4π2
 π
−π
 π
−π
|I(ejwu, ejwv).G(ejwu, ejwv)|2dwudwv.
(5)
Assuming that I(ejwu, ejwv) has energy/information equally distributed
across all frequencies, of magnitude M, the information in the ﬁltered image
Eo can be expressed as
Eo = M 2
4π2
 π
−π
G(wu)2dwu
 π
−π
G(wv)2dwv
= M 2
4π2
 π
−π
e−w2
uσ2dwu
 π
−π
e−w2
vσ2dwv
= M 2
4π2

πerf (πσ)2
σ2

.
(6)
Finally, the normalized information gain due to ﬁltering for each level k of the
pyramid is given by
P(σk) =
1
4π2

πerf (πσk)2
σ2
k

(7)
Gaussian Spatial Weighting. The information due to spatial weighting is given by
R(fk) = 1
N
 N/2
−N/2
 N/2
−N/2
e
−1
2
u2+v2
f2
k
dudv
(8)
Hence, to compute the total information compression of the pyramid for the
non-uniform foveal vision system, we need to take into account the combined

A Hybrid Attention Model Using Foveal Vision
123
information due to ﬁltering and spatial weighting at each level of the pyramid.
The total information of the pyramid is thus given by
T(k) =
K−1

k=0
R(fk)Lk
(9)
where
Lk = Pk −Pk+1
with
P0 = 1
(10)
3.2
Weakly Supervised Object Localization
In this subsection we describe in detail our top-down object localization via
feedback saliency extraction.
Image-Speciﬁc Class Saliency Extraction. As opposed to Itti’s [6] that
processes the image with diﬀerent ﬁlters to generate speciﬁc feature maps, Cao [4]
proposed a way to compute a saliency map, in a top-down manner, given an
image I and a class c. The class score of an object class c in an image I, Sc(I), is
the output of the neural network for class c. An approximation of the neural net-
work class score with the ﬁrst-order Taylor expansion [4,11] in the neighborhood
of I can be done as follows
Sc(I) ≈G⊤
c I + b
(11)
where b is the bias of the model and Gc is the gradient of Sc with respect to I :
Gc = ∂Sc
∂I .
(12)
Accordingly, the saliency map is computed for a class c by calculating the score
derivative of that speciﬁc class employing a back-propagation pass. In order
to get the saliency value for each pixel (u, v) and since the images used are multi-
channel (RGB - three color channels), we rearrange the elements of the vector
Gc by taking the maximum magnitude of it over all color channels. This method
for saliency map computation is extremely simple and fast since only a back
propagation pass is necessary. Simonyan et al. [11] shows that the magnitude
of the gradient Gc expresses which pixels contribute more to the class score.
Consequently, it is expected that these pixels can give us the localization of the
object pertaining to that class, in the image.
Bounding Box Object Localization. Considering Simonyan’s ﬁndings [11],
the class saliency maps hold the object localization of the correspondent class in
a given image. Surprisingly and despite being trained on image labels only, the
saliency maps can be used on localization tasks.
Our object localization method based on saliency maps goes as follow. Given
an image I and the corresponding class saliency map Mc, a segmentation mask is

124
A.F. Almeida et al.
computed by selecting the pixels with the saliency higher than a certain thresh-
old, th, and set the rest of the pixels to zero.
Considering the stain of points resulting from the segmentation mask, for a
given threshold, we are able to deﬁne a bounding box covering all the non-zero
saliency pixels, obtaining a guess of the localization of the object (see Fig. 3).
Fig. 3. Representation of the saliency map and the correspondent bounding box of
each of the top 5 predicted class labels of a bee eater image of ILSVRC 2012 data set.
The rectangles represent the bounding boxes that cover all non-zero saliency pixels
resultant from a segmentation mask with th = 0.75.
4
Results
In this section, we begin by numerically quantifying the proposed non-uniform
foveation mechanism information compression dependence on the fovea size.
Then, we quantitatively assess the classiﬁcation and localization performance
obtained for the proposed feed-forward and feed-backward passes for various
state-of-the-art CNN architectures (Sect. 4.2).
4.1
Information Compression
In order to quantitatively assess the performance of our methodology, it is impor-
tant to ﬁrst quantify the amount of information preserved by the proposed non-
uniform foveation mechanism to further understand the fovea size inﬂuence in
task performance. Through a formal mathematical analysis of the information
compression (see Sect. 3.1) we can represent the relationship between fovea size
(f0), image size (N) and information compression. In our experiments σ1 was
set to 1, the original image resolution was set to N × N = 224 × 224 (the size
of the considered CNNs input layers) and the size of the fovea was varied in the

A Hybrid Attention Model Using Foveal Vision
125
interval f0 = [0.1; 224]. As depicted in Fig. 4, the gain grows monotonically and
exhibits a logarithmic behaviour for f0 ∈[1; 100]. Beyond f0 ≈100, the compres-
sion becomes residual, saturating at around f0 ≈120. Hence, from this point
our foveation mechanism becomes unnecessary since resulting images contain
practically the same information as the original uniform-resolution ones.
10 -1
10 0
10 1
10 2
-50
-40
-30
-20
-10
0
Fig. 4. Information gain in function of f0 for the proposed non-uniform foveal vision
mechanism.
4.2
Attentional Framework Evaluation
In this paper, our main goal was to develop a single CNN capable of perform-
ing, recognition and localization tasks, taking into account both bottom-up and
top-down mechanisms of selective visual attention. In order to quantitatively
assess the performance of the proposed framework we used the ImageNet Large
Scale Visual Recognition Challenge (ILSVRC) 20121 data set, which comprises a
total of 50 K test images with objects conveniently located in the images center.
Furthermore, we tested the performance of our methods with diﬀerent pre-
trained Convolutional Network (ConvNet) models which are publicly and read-
ily available at Caﬀe [7] Model Zoo, namely, CaﬀeNet [8], GoogLeNet [13] and
VGGNet [12]. As mentioned on Sect. 3.2, a feed-forward pass is executed origi-
nating a vector with the probability distribution of the class label scores. These
class labels are used to compute the classiﬁcation error which compares the
ground truth class label provided in ILSVRC with the predicted class labels.
Usually, two error rates are commonly used: the top-1 and the top-5. The for-
mer serves to verify if the predicted class label with the highest score is equal to
the ground truth label. For the latter, we verify if the ground truth label is in
the set of the ﬁve highest predicted class labels.
1 Source: http://image-net.org/challenges/LSVRC/2012/ [as seen on June, 2017].

126
A.F. Almeida et al.
For a given image, the object location was considered correct if at least one
of the ﬁve predicted bounding boxes overlapped over 50% with the ground truth
bounding box. This evaluation criterion [10] consists on the intersection over
union (IoU) between the computed and the ground truth bounding box.
Classiﬁcation Performance. The classiﬁcation performance for the various
CNN architectures combined with the proposed foveal sensing mechanism are
depicted in Fig. 5a. The CaﬀeNet pre-trained model which presents the shallower
architecture had the worst classiﬁcation performance. The main reason is that
the GoogLeNet and VGG models use smaller convolutional ﬁlters and deeper
networks that enhance the distinction between similar and nearby objects.
Regarding the impact of non-uniform foveal vision, a common tendency can
be seen for all three pre-trained models. The classiﬁcation error saturates at
approximately f0 = 70. This result is corroborated by the evolution of the gain,
depicted in Fig. 4, since after −3 dB compressions goes slowly below 3 dB. This
means that on average and for this particular data set, half of the information
contained in uniform resolution images is irrelevant for correct classiﬁcation.
Small size foveas exhibit extremely high error rates, which corresponds to a
very small region characterized by having high acuity. This is due to the fact that
images that make up the ILSVRC data set contain objects that occupy most of
the image area, that is, although the image has a region with high-resolution, it
may be small and not suﬃce to give an idea of the object in the image, which
leads to poor classiﬁcation performance.
Localization Performance. As can be seen in Fig. 5b, for thresholds smaller
than 0.4, the localization error remains consistent and stable at around 60%.
From this point, the evolution of the error presents the form of a valley where
the best localization results were obtained for thresholds between 0.65 and 0.7.
0
10
20
30
40
50
60
70
80
90
100
0
20
40
60
80
100
Classification Error (%)
Feed-foward (CaffeNet)
Feed-foward (VGGNet)
Feed-foward (GoogLeNet)
(a) Classiﬁcation error
0
0.2
0.4
0.6
0.8
0
20
40
60
80
100
Localization Error (%)
Backward (CaffeNet)
Backward (VGGNet)
Backward (GoogLeNet)
(b) Localization error
Fig. 5. Classiﬁcation and localization performance for various network architectures
and sensing conﬁgurations. Left column: Dashed lines correspond to top-1 error and
the solid ones correspond to top-5 error. Righ column: Dashed lines correspond to
f0 = 80 and solid lines to f0 = 100.

A Hybrid Attention Model Using Foveal Vision
127
Overall, GoogLeNet presents the best localization performance. We hypoth-
esize that this is mostly due to CaﬀeNet and VGG models feature two fully-
connected layers of 4096 dimension that may jeopardize the spatial distinction
of image characteristics. Furthermore, GoogLeNet is deeper than the aforemen-
tioned models and hence can learn discriminant features at higher levels of
abstraction.
5
Conclusions and Future Work
In this paper we proposed a biologically inspired framework for object classiﬁca-
tion and localization that combines bottom-up ant top-down attentional mech-
anisms, incorporating recent Deep Convolutional Neural Networks architectures
with human-like foveal vision. The main experimental goal of this study was
to assess the performance of our framework with well-known state-of-the-art
CNN architectures, in recognition and localization tasks, when combined with
non-uniform foveal vision.
Through the analysis performed in our tests, we can conclude that the
deeper neural networks present better performance when it comes to classiﬁca-
tion. Deeper networks have the capacity to learn more features which results in
improved ability in distinguishing similar and close objects (i.e. generalization).
Furthermore, the results obtained for non-uniform foveal vision are promising.
We conclude that it is not necessary to store and transmit all the information
present in a high-resolution images since, from a given fovea size (f0), the per-
formance in the classiﬁcation task saturates.
However, the tests performed in this work assumed that the objects were
centered, which is reasonable for the used data set, but unreasonable in real
scenarios. In the future, we intend to test this type of vision in other data sets
trained for recognition and localization tasks where objects may not be centered,
thus having a greater localization variety. Dealing with multiple scales is another
relevant limitation of non-uniform foveal sensors, in particular for close objects
whose overall characteristics become unperceivable as the resolution decays very
rapidly towards the periphery. To overcome this limitation, we intend to develop
active vision mechanisms that will allow to autonomously redirect the attentional
focus while integrating task-related evidence over time. Finally, it would also be
interesting to train the system directly with blur (uniform and non-uniform
foveal). In this case, it would be expected that with this tuning of the network,
its performance would improve for both classiﬁcation and localization tasks.
Acknowledgment. This work has been partially supported by the Portuguese
Foundation for Science and Technology (FCT) project [UID/EEA/50009/2013]. Rui
Figueiredo is funded by FCT PhD grant PD/BD/105779/2014.

128
A.F. Almeida et al.
References
1. Bastos, M.J.: Modeling human gaze patterns to improve visual search in
autonomous systems. Master’s thesis, Instituto Superior T´ecnico (2016)
2. Borji, A., Itti, L.: State-of-the-art in visual attention modelling. IEEE Trans. Patt.
Anal. Mach. Intell. 35(1), 185–207 (2013)
3. Burt, P., Adelson, E.: The Laplacian pyramid as a compact image code. IEEE
Trans. Commun. 31(4), 532–540 (1983)
4. Cao, C., Liu, X., Yang, Y., Yu, Y., Wang, J., Wang, Z., Wang, L., Huang, C.,
Huang, T.S., Xu, W., Ramanan, D., Huang, Y.: Look and think twice: capturing
top-down visual attention with Feedback. In: International Conference on Com-
puter Vision (2015)
5. Geisler, W.S., Perry, J.S.: Real-time foveated multiresolution system for low-
bandwidth video communication. In: Photonics West 1998 Electronic Imaging,
International Society for Optics and Photonics, pp. 294–305 (1998)
6. Itti, L., Koch, C., Niebur, E.: A model of saliency-based visual attention for rapid
scene analysis. IEEE Trans. Patt. Anal. Mach. Intell. 20(11), 1254–1259 (1998)
7. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.,
Guadarrama, S., Darrell, T.: Caﬀe: convolutional architecture for fast feature
embedding. arXiv preprint arXiv:1408.5093 (2014)
8. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with deep con-
volutional neural networks. In: Advances In Neural Information Processing Sys-
tems, pp. 1–9 (2012)
9. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444
(2015)
10. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet large
scale visual recognition challenge. Int. J. Comput. Vis. (IJCV) 115(3), 211–252
(2015)
11. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks:
visualising image classiﬁcation models and saliency maps. In: Computer Vision
and Pattern Recognition (2014)
12. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. In: International Conference on Learning Representations, pp.
1–14 (2015)
13. Szegedy, C., Liu, W., Jia, Y., Sermanet, P.: Going deeper with convolutions. In:
Computer Vision Foundation (2014)
14. Traver, V.J., Bernardino, A.: A review of log-polar imaging for visual perception
in robotics. Robot. Autonom. Syst. 58(4), 378–398 (2010)
15. Wallace, R.S., Ong, P.W., Bederson, B.B., Schwartz, E.L.: Space variant image
processing. Int. J. Comput. Vis. 13(1), 71–90 (1994)
16. Wang, Z.: Rate scalable foveated image and video communications [Ph.D. thesis]
(2003)

Mixed-Policy Asynchronous Deep Q-Learning
David Sim˜oes1,2,3(B), Nuno Lau1,3, and Lu´ıs Paulo Reis1,2,4
1 IEETA - Institute of Electronics and Informatics Engineering of Aveiro,
University of Aveiro, Aveiro, Portugal
2 LIACC - Artiﬁcial Intelligence and Computer Science Lab, Porto, Portugal
3 DETI/UA - Electronics, Telecommunications and Informatics Department,
University of Aveiro, Aveiro, Portugal
david.simoes@ua.pt
4 DSI/EEUM - Information Systems Department, School of Engineering,
University of Minho, Guimar˜aes, Portugal
Abstract. There are many open issues and challenges in the reinforce-
ment learning ﬁeld, such as handling high-dimensional environments.
Function approximators, such as deep neural networks, have been suc-
cessfully used in both single- and multi-agent environments with high
dimensional state-spaces. The multi-agent learning paradigm faces even
more problems, due to the eﬀect of several agents learning simultaneously
in the environment. One of its main concerns is how to learn mixed poli-
cies that prevent opponents from exploring them in competitive environ-
ments, achieving a Nash equilibrium. We propose an extension of several
algorithms able to achieve Nash equilibriums in single-state games to
the deep-learning paradigm. We compare their deep-learning and table-
based implementations, and demonstrate how WPL is able to achieve an
equilibrium strategy in a complex environment, where agents must ﬁnd
each other in an inﬁnite-state game and play a modiﬁed version of the
Rock Paper Scissors game.
Keywords: Deep reinforcement learning · Equilibrium strategies ·
Mixed-policy algorithms
1
Introduction
The relevance of multi-agent systems in the ﬁeld of artiﬁcial intelligence has
increased in the last years, due to the increasing popularity of scenarios such
as robotic soccer, disaster mitigation and rescue, autonomous driving, among
others [7]. These multi-agent challenges require robust algorithms that achieve
coordination and adapt to other agents present in the environment. In single-
agent scenarios, reinforcement learning has been employed with enormous suc-
cess to achieve optimal or super-human performance in complex scenarios, such
as video-games [14,15], combinatorial optimization problems [10], or robotic con-
trol tasks [2]. These policies can be learned partly due to the fact that the envi-
ronment is stationary, and can therefore be learned by the algorithms employed.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_11

130
D. Sim˜oes et al.
In multi-agent scenarios, since policies must take into account the remaining
agents’ policies, which can also adapt their behavior, agents face a moving tar-
get problem. The ﬁeld of Game Theory, where agents maximize their payoﬀs
in the presence of other agents doing the same, is heavily tied to multi-agent
learning. It deﬁnes a Nash equilibrium as a joint strategy σi∗to be the best
response of agent i to a vector of opponent strategies. A Nash equilibrium is
the joint strategy {σ1∗, ..., σn∗} such that each individual strategy is the best
response to the others. This describes a balance where no agent can change its
strategy as long as the remaining agents maintain theirs. A Nash equilibrium is
usually not a greedy policy, but instead a mixed (or stochastic) one, where prob-
ability distributions over the agent’s actions prevent opponents from exploring
deterministic strategies [6].
We propose to adapt several state-of-the-art mixed-policy algorithms, all of
which require no knowledge about other players’ actions or rewards and have
been shown to reach equilibrium strategies in self-play, to the deep learning
paradigm. These algorithms stored their values in table representations, and
thus could not be used in high-dimensional environments. We show how the
deep-learning implementations based on Asynchronous Deep Q-Learning not
only match the performance of the original algorithms in single-state games, but
can also ﬁnd equilibrium strategies in complex environments.
The remainder of this paper is structured as follows. Section 2 provides some
context on deep learning on highly complex environments and on equilibrium
strategy algorithms. Section 3 formally describes the analyzed algorithms, while
Sect. 4 describes the current state-of-the-art Deep Q-Learning implementation.
Section 5 details our proposal and implementation, and Sect. 6 shows the results
we obtained. Finally, Sect. 7 draws conclusions and lists future work directions.
2
Background
One of the major open issues in machine learning environments is their complex-
ity. In recent years, there have been many successes using deep representations
in complex high-dimensional reinforcement learning tasks. One such example
is Deep Q-Networks (DQN) [15], a deep Q-learning algorithm, or Asynchro-
nous Deep Q-learning [14], a distributed version of DQN, both of which rein-
forcement learning algorithms that allowed a single agent to achieve human-
level performance across many Atari games. They have since been extended
to the multi-agent paradigm [17] and shown to allow multiple agents to coor-
dinate in cooperative complex scenarios, while still being general enough to
adapt to previously unseen situations. However, all of the above are greedy
algorithms, that could not converge into a strategy able to play a game as sim-
ple as rock-paper-scissors. While several algorithms have been proven to con-
verge to Nash equilibria [5,7], many have unrealistic assumptions, such as know-
ing the underlying game structure or the optimal Nash Equilibrium [4,18], or
the actions performed by other agents and their received rewards [9,12]. These
assumptions are unrealistic in most scenarios, either due to their complexity

Mixed-Policy Asynchronous Deep Q-Learning
131
(where the game model is unknown or too complex) or due to conﬂicting goals
(non-cooperative agents will not provide reward information to others, for exam-
ple). This makes it so that even simple 2-player games are challenging. How-
ever, algorithms have been proposed that achieve Nash equilibriums with only
information about the agents’ own actions and rewards. These include WoLF-
PHC [7], GIGA-WoLF [5], WPL [1], and EMA-QL [3]. WoLF-PHC introduced
the Win or Learn Fast principle, where diﬀerent learning rates are used when
the agent is winning or losing, a principle also used by GIGA-WoLF. However,
both algorithms have shown problems in more complex games, such as Shap-
ley’s Game [16]. WPL and EMA-QL have been shown to achieve convergence
in such games, but with some setbacks. WPL has no formal analysis and proof
of convergence, and EMA-QL features some diﬃculties learning simpler games
with many actions and asymmetric probabilities. Not only that, but since all
of these algorithms derive from table-based Q-learning, they also cannot handle
high-complexity environments.
3
Mixed Q-Learning
All the mixed-policy algorithms listed in the previous section keep track of both
Q-values and of a probability distribution in each state. This probability may
tend to a pure strategy, where the algorithms become the original greedy Q-
learning. We now describe four state-of-the-art mixed-policy algorithms using a
similar notation in all for simplicity, and provide a short description of each.
We use πt(s) as the policy at time-step t for state s, representing a vector of
probabilities of picking each action, and ˆπt(s) as the average policy at time-step t
for state s, representing a policy that changes slower than π. The policy learning
rate at time-step t is denoted by δt, and is sometimes dependent on the current
state s and action a. A projection function P(π) is used to project policies into
the valid probability space. When used as P(π, η), each probability must have
at least a probability of η (0 by default).
3.1
WoLF-PHC
Policy Hill-Climbing with the Win or Learn Fast principle (WoLF-PHC) [7]
introduces a variable learning rate to achieve convergence in games. The Win or
Learn Fast (WoLF) principle consists on having a higher learning speed when
the current policy is worse than the average policy (representing the equilib-
rium policy). WoLF-PHC achieves both optimal strategies against static players
(rationality) and has been proven to converge in self-play. We start by increment-
ing a state counter Ct(s), which counts how many times the state s has been
visited, and computing the average policy ˆπt+1(s). We then choose a learning
rate δt(s) based on whether our current policy πt(s) is better or worse than the
average policy, and project our new policy through an added increment Δt(s, a).
ˆπt+1(s) = ˆπt(s) + πt(s) −ˆπt(s)
Ct+1(s)

132
D. Sim˜oes et al.
δt(s) =

δw if 
a′∈A πt(s, a′)Qt(s, a′) > 
a′∈A ˆπt(s, a′)Qt(s, a′)
δl otherwise
∀a ∈A Δt(s, a) =

−δt(s)
|A|−1 if a ̸= argmaxa′∈AQt(s, a′)
δt(s) otherwise
πt+1(s) = P

πt(s) + Δt(s)

3.2
GIGA-WoLF
Generalized Inﬁnitesimal Gradient Ascent using the Win or Learn Fast principle
(GIGA-WoLF) [5] keeps track of two gradient updated strategies, one of which
is updated faster than the other. Regret measures how much worse an algorithm
performs compared to the best static strategy, and GIGA-WoLF has been shown
to achieve both no-regret and convergence properties. We start by estimating a
new policy γt+1(s) and the average policy ˆπt+1(s). We then compute the learning
rate δt+1(s), whose magnitude is larger when the slower strategy ˆπt received
higher reward than πt, and change policy πt+1(s) is in the direction of positive
gradient.
γt+1(s) = P

πt(s) + ηtQt(s)

, ˆπt+1(s) = P

ˆπt(s) + ηtQt(s)
3

δt+1(s) = min

1, ||ˆπt+1(s) −ˆπt(s)||
||ˆπt+1(s) −γt+1(s)||

πt+1(s) = (1 −δt+1)γt+1(s) + δt+1ˆπt+1(s)
3.3
WPL
Weighted Policy Learner (WPL) [1] has a variable learning rate, and allows the
agent to move towards the equilibrium strategy faster than moving away from it.
Despite not having a formal proof of convergence due to the non-linear nature
of WPL’s dynamics, the authors numerically solve WPL’s dynamics diﬀeren-
tial equations and show that it features continuous non-linear dynamics, while
experimentally demonstrating it converges in several more complex games. We
calculate an increment vector Δ(s) from the gradients of the value function Vt(s)
and use it to compute our new policy πt+1(s).
Vt(s) =

a∈A
πt(s, a)Qt(s, a)
∀a ∈A Δt(s, a) = η ∂Vt(s)
∂πt(s, a)

πt(a) if ∂Vt(π)
∂πt(a) < 0
1 −πt(a) otherwise
πt+1(s) = P

πt(s) + Δt(s), ϵ


Mixed-Policy Asynchronous Deep Q-Learning
133
3.4
EMA-QL
Exponential Moving Average Q-Learning (EMA-QL) [3] also features two learn-
ing speeds. The algorithm is experimentally demonstrated to converge faster
than WPL in several scenarios with a smaller number of episodes, but it also
has no formal proof of convergence. We simply calculate an increment vector
Δ(s) and use it to compute our new policy πt+1(s).
δt(s, a) =

δw if a = argmaxa′Qt(s, a′)
δl otherwise
Δ1(s, a) = (u0, u1, ..., u|A|) where ua =

1 if a = argmaxa′Qt(s, a′)
0 otherwise
Δ2(s, a) = (u0, u1, ..., u|A|) where ua =

0 if a = argmaxa′Qt(s, a′)
1
|A|−1 otherwise
Δ(s, a) =

Δ1(a) if a = argmaxa′Qt(s, a′)
Δ2(a) otherwise
πt+1(s) = (1 −δt)πt(s) + δtΔ(s)
4
Deep Reinforcement Learning
The Deep Q-Networks (DQN) algorithm [15] uses a deep neural network with
weights θ as a function approximator, where Q(s, a; θ) ≈Q∗(s, a). Agents com-
pute a processed state φt = Φ(ot) (dependent on the problem; Mnih et al. used
a stack of the last 4 frames to compensate for partial observability) and, under
the policy π(φ, a), the sequence of loss functions in iteration i, denoted by
Li(θi) = Eφ,a∼π(.)[(r + γ max
a′ Q(φ′, a′; θ−
i ) −Q(φ, a; θi))2],
(1)
is minimized, in order to train the Q-network. A target network is kept, whose
parameters θ−
i−1 for iteration i are ﬁxed, and only updated (copied from the on-
line network) every τ time-steps. The gradient of the loss function with respect
to the weights can be optimized by stochastic gradient descent. The agent’s
experience at each time-step et = (φt, at, rt, φt+1) is stored in a data-set D =
e1, . . . , eN, known as a replay memory. Updates are performed with mini-batches
(k samples of experience e ∼D) drawn uniformly from the replay memory.
Mnih et al. [14] have shown that asynchronous methods running on multi-core
CPUs not only require less specialized hardware than their GPU counterparts,
but they also achieve greater results in a shorter amount of time. Asynchronous
methods essentially keep a global network which all worker threads asynchro-
nously update and whose weights are periodically copied by each worker into its
own network. There is no replay memory from which to draw samples, and each
thread provides its own uncorrelated samples.

134
D. Sim˜oes et al.
Asynchronous 1-step Q-learning is described in Algorithm 1. Each thread
computes a gradient of the Q-learning loss. A slowly changing target network is
used to stabilize learning, and gradients are accumulated over multiple time-steps
before they are globally applied. Mnih et al. have also proposed Asynchronous
n-step Q-learning, which operates in forward view and accumulates the gradients
for up to n Q-learning updates for each state-action pair executed. The gradients
are all applied simultaneously.
Input: Global shared learning rate η, discount factor γ, target network update period τt, network
update period τ, network weights θ, target network weights θ−, and counter T . Local network
gradients dθ, and counter t.
1: for iteration = 1, Tmax do
2:
Sample state s1
3:
for step t = 1, tmax do
4:
Take random action at with probability ϵ, otherwise best action at = maxa Q(st, a; θ)
5:
Sample state st+1 and reward rt
6:
y =

r
r + γ maxa′ Q(s′, a′; θ−)
7:
Accumulate gradients dθ ←dθ + ∂(y−Q(s,a;θ))2
∂θ
8:
Update target network weights θ−←θ every τt time-steps
9:
Update network weights θ using dθ and η every τ time-steps
10:
end for
11: end for
Algorithm 1: Asynchronous 1-step Q-learning
5
Mixed Deep Q-Learning
We now describe how the above mentioned mixed strategy algorithms are
adapted into the context of Asynchronous 1-step Q-learning. The basis of the
algorithm is shown in Algorithm 2, where each iteration’s policy network target
yπ is given by the mixed-strategy algorithm. We do not attempt to adapt the
original DQN, since its replay memory, while necessary to break dependencies
between samples, essentially incurs a delay in adapting to opponent strategies
and leads to diverging policies.
In algorithms that require two policy networks, one that changes slower than
the other, we keep an additional network and target network with weights θπ′
and θ−
π′. In practice, we don’t use separate networks, and actually share all
weights, except the ones for the output layers, as shown in Fig. 1. This allows
for computational simplicity both in forward and backward passes through the
network.
We optimize the cross entropy H(yπ, π(s, a; θ)) between the target policy yπ
given by each mixed strategy algorithm and the current policy π. Because each
algorithm requires diﬀerent learning rates for value and policy functions, in order
to take advantage of the shared weights, we use a learning rate η∗
π = ηπ
η when
computing the target policy, and a global learning rate η to apply the gradients
in a single backwards pass. When the gradients are calculated with the global

Mixed-Policy Asynchronous Deep Q-Learning
135
...
Hidden
layer 1
Hidden
layer N
Input
layer
Value
Output
layer
Policy
Output
layer
Fig. 1. Example of how the same network is shared to output both value and policy.
All the weights apart from the output layers are shared and optimized together in the
same backwards pass.
Input: Global shared learning rate η, policy learning rate ηπ, discount factor γ, target network
update period τt, network update period τ, network weights θ, target network weights θ−,
policy network weights θπ, target policy network weights θ−
π , and counter T . Local network
gradients dθ, local policy network gradients dθπ, and counter t.
1: for iteration = 1, Tmax do
2:
Sample state s1
3:
for step t = 1, tmax do
4:
Take random action at with probability ϵ, otherwise best action at = maxa Q(st, a; θ)
5:
Sample state st+1 and reward rt
6:
y =

r
r + γ maxa′ Q(s′, a′; θ−)
7:
Compute yπ based on mixed-policy algorithm
8:
Accumulate gradients dθ ←dθ + ∂(y−Q(s,a;θ))2
∂θ
9:
Accumulate gradients dθπ ←dθπ + ∂(H(yπ,π(s,a;θπ)))
∂θπ
10:
Update target network weights θ−←θ and θ−
π ←θπ every τt time-steps
11:
Update network weights θ using dθ and η and θπ using dθπ and ηπ every τ time-steps
12:
end for
13: end for
Algorithm 2: Asynchronous 1-step Mixed Q-learning
learning rate, the policy gradients are scaled to their correct magnitude, since
η∗
π ∗η = ηπ.
The target policy is given by the mixed strategy algorithms described previ-
ously. We replace all Qt(s, a) by Qt(s, a; θ−), all πt(s, a) by πt(s, a; θ−), and all
ˆπt(s, a) by ˆπt(s, a; θ−), and obtain πt+1(s) as the target policy yπ. For WoLF-
PHC, we use a game counter instead of a state counter, since we want to avoid
table-based representations.
6
Results
We chose several games to demonstrate how those chosen algorithms con-
verge to a mixed-strategy equilibrium. These include Matching Pennies, Tricky
Game, Biased Game, Rock-Paper-Scissors, and Null-Rock-Paper-Scissors, some

136
D. Sim˜oes et al.
0/3 3/2
1/0 2/1
(a) Tricky Game
1/1.75 1.75/1
1.25/1 1/1.25
(b) Biased Game
-1/-1 -1/-1 -1/-1 -1/-1
-1/-1 2/2
3/1
1/3
-1/-1 1/3
2/2
3/1
-1/-1 3/1
1/3
2/2
(c) Null-Rock-Paper-Scissors
Fig. 2. The pay-oﬀmatrices of popular Game Theoretic 2-player games. Rows represent
the actions of the ﬁrst player, columns the actions of the second player, and each cell
the pay-oﬀp0 of the ﬁrst player and the pay-oﬀp1 of the second player in the format
p0/p1.
of which are shown in Fig. 2. The equilibrium strategies for Matching Pennies
and Tricky Game is to play each action with a probability of 0.5. For the Biased
Game, it is to play one action with probability 0.75 and the other with proba-
bility 0.25. For Rock-Paper-Scissors, the Nash equilibrium is to play each action
with a probability of 1
3 and for Null-Rock-Paper-Scissors, the equilibrium is to
not play the ﬁrst action and the remaining actions with probability 1
3. Matching
Pennies and Rock-Paper-Scissors have an average return of 0, while the remain-
ing games have a positive average return.
We started by demonstrating how the deep versions of each of our algorithms
fare against their original implementations. We used the same hyper-parameters
for all algorithms, such that the learning rate η = 0.01, the policy learning rate
ηπ =
η
100+i/2000, the winning policy learning rate ηw = ηπ, and the losing policy
learning rate ηl = 2ηw. The evolutions of the policies over iterations is shown in
Fig. 3.
We then learned these games using a simple neural network, with only a
hidden layer of 9 nodes, 15 concurrent games, a learning rate η = 10−4, a policy
learning rate ηπ =
η
200, the winning policy learning rate ηw = ηπ, and the
losing policy learning rate ηl = 2ηw. Weights were initialized with the Xavier
initializer [11], optimized with the Adam optimizer [13], and ELU activators [8]
were used in all layers, except the output ones. The evolution of the policies
over iterations for the Biased and the Tricky games is shown in Fig. 4. We used
mini-batches of size 25 for the Biased Game in order to speed up the learning
process (as the Biased Game takes much longer to learn than the Tricky Game).
Another diﬀerence between the deep and the table-based policy evolutions shown
is that we could set an initial policy of {0.9, 0.1} for the agents in the original
implementations, in order to check how the policies oscillated from extreme cases.
However, we could not trivially or eﬃciently do it with deep implementations
without hand-tuning weights (which could disrupt the convergence properties of
the network) or optimizing the network for the target initial policy (which would
not be time-eﬃcient).
Ideally, all algorithms would converge to the equilibrium strategies, meaning
we would see the Tricky Game (solid lines) converging to 0.5 probability on each
agent’s ﬁrst action, and the Biased Game (dotted lines) converging to 0.75 and
0.25 probabilities. We can see that all the table-based algorithms can achieve

Mixed-Policy Asynchronous Deep Q-Learning
137
Fig. 3. The evolution of the policies of 2 agents in self-play using the Wolf-PHC, GIGA-
WoLF, WPL, and EMA-QL algorithms, over 1000 epochs of 10000 trials. The games
shown are the Tricky Game (solid) and the Biased Game (dotted), both shown in Fig. 2.
Each plot represents the probability of playing the ﬁrst action by each player.
equilibrium strategies with small variance using the deﬁned hyper-parameters.
The neural-based algorithms achieve similar results in the Tricky Game and
oscillate around the equilibrium strategies in the Biased Game. This oscillation
is due to the use of mini-batches in this single-state game, which incur a delay
in the agent’s adaptation to the opponent’s strategy.
To demonstrate the strength of deep learning techniques, we then created a
complex scenario where a mixed strategy needed to be learned. Two agents are
placed in a grid map, where each empty cell either has value 1 (if an agent is on it)
or has a random value otherwise (which implies an inﬁnite amount of states, such
that any table-based algorithm could not compute a policy without some input
pre-processing). The agents must ﬁnd each other and play Null-Rock-Paper-
Scissors, which resets the game and puts the agents back in random positions.
We chose this game for multiple reasons: it has the same amount of actions as
the remainder of the game (the network represents the 4 actions to move around
with the same neurons as the actions in the game), and it has a positive average
pay-oﬀ. Because of this, the agents have the motivation of playing it as many
times as possible. We used a network with 2 hidden layers of 150 neurons and a
policy learning rate ηπ =
η
400 to achieve the results shown in Fig. 5, where only

138
D. Sim˜oes et al.
0
0.2
0.4
0.6
0.8
1
(a) WoLF-PHC
0
0.2
0.4
0.6
0.8
1
(b) GIGA-WoLF
0
0.2
0.4
0.6
0.8
1
(c) WPL
0
0.2
0.4
0.6
0.8
1
(d) EMA-QL
Fig. 4. The evolution of the policies of 2 agents in self-play using the deep learning
implementations of Wolf-PHC, GIGA-WoLF, WPL, and EMA-QL algorithms, over
400 epochs of 250 iterations. The games shown are the Tricky Game (solid) and the
Biased Game (dotted), both shown in Fig. 2. Each plot represents the probability of
playing the ﬁrst action by each player.
100
200
300
400
500
600
700
0
0.2
0.4
0.6
0.8
1
Fig. 5. The evolution of the policies of 2 agents in Null-Rock-Paper-Scissors self-play
(shown in Fig. 2), using the deep learning implementations of WPL, over 700 epochs of
250 trials. Each plot represents the probability of playing an action by the ﬁrst player
(solid) or the second player (dotted).

Mixed-Policy Asynchronous Deep Q-Learning
139
WPL managed to learn the equilibrium strategy for the game in this complex
environment. Both WoLF-PHC and GIGA-WoLF diverge in their strategies, and
start oscillating between extremes, cycling between dominant actions in a loop.
We believe this is due to the use of a second slower network to represent the
average strategy, which is being aﬀected by its faster counterpart. On the other
hand, EMA-QL never actually learns that the ﬁrst action should not be played
at all and just plays all actions with equal probability. We tested the table-based
version of EMA-QL and it also could not reach a Nash equilibrium in self-play.
7
Conclusion
Multi-agent systems require algorithms able to achieve mixed policies that will
not be exploited by competitive opponents. These policies achieve a Nash equi-
librium, and several algorithms have been proposed to compute such strategies.
However, most have unrealistic assumptions, and the great majority use table
representations of the game state, meaning they cannot be used in high dimen-
sional environments.
We adapt several mixed-strategy algorithms that achieve Nash equilib-
ria with realistic assumptions to a deep learning version of Asynchronous
Q-learning. We combine the strengths of both algorithms, achieving mixed-
policy capabilities and the capacity of handling high dimensional environ-
ments. We show how in single-state games, all deep implementations can match
the performance of the original algorithms, but in a complex environment,
only the deep adaptation of the WPL algorithm achieves positive results.
We published our code and experiments at https://github.com/bluemoon93/
Mixed-Policy-Asynchronous-Deep-Q-Learning.
In the future, we would like to further study our results and infer why only one
of the algorithms could reach the equilibrium strategy. We would also extend and
compare WPL with Asynchronous n-step Q-learning, and Asynchronous Advan-
tage Actor Critic. We also intend on testing our algorithm in more complex envi-
ronments and determining whether the algorithm can maintain its convergence
properties, while still achieving equilibrium mixed policies.
Acknowledgements. The ﬁrst author is supported by FCT (Portuguese Foundation
for Science and Technology) under grant PD/BD/113963/2015. This research was par-
tially supported by IEETA and LIACC. The work was also funded by project EuRoC,
reference 608849 from call FP7-2013-NMP-ICT-FOF.
References
1. Abdallah, S., Lesser, V.R.: A multiagent reinforcement learning algorithm with
non-linear dynamics. CoRR abs/1401.3454 (2014)
2. Abdolmaleki, A., Simoes, D., Lau, N., Reis, L.P., Neumann, G.: Learning a
humanoid kick with controlled distance. In: Behnke, S., Lee, D.D., Sariel, S., Sheh,
R. (eds.) RoboCup 2016: Robot World Cup XX. Lecture Notes in Artiﬁcial Intel-
ligence, Leipzig, Germany. Springer (2016)

140
D. Sim˜oes et al.
3. Awheda, M.D., Schwartz, H.M.: Exponential moving average q-learning algorithm.
In: 2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement
Learning (ADPRL), pp. 31–38, April 2013
4. Banerjee, B., Peng, J.: Generalized multiagent learning with performance bound.
Auton. Agents Multi Agent Syst. 15(3), 281–312 (2007)
5. Bowling, M.: Convergence and no-regret in multiagent learning. In: Proceedings
of the 17th International Conference on Neural Information Processing Systems,
NIPS 2004, pp. 209–216. MIT Press, Cambridge (2004)
6. Bowling, M., Veloso, M.: Rational and convergent learning in stochastic games. In:
International Joint Conference on Artiﬁcial Intelligence, vol. 17, pp. 1021–1026.
Lawrence Erlbaum Associates Ltd. (2001)
7. Bowling, M., Veloso, M.: Multiagent learning using a variable learning rate. Artif.
Intell. 136(2), 215–250 (2002)
8. Clevert, D.A., Unterthiner, T., Hochreiter, S.: Fast and accurate deep network
learning by exponential linear units (elus) (2015). arXiv preprint: arXiv:1511.07289
9. Conitzer, V., Sandholm, T.: Awesome: a general multiagent learning algorithm
that converges in self-play and learns a best response against stationary opponents.
Mach. Learn. 67(1–2), 23–43 (2007)
10. Dorigo, M., Gambardella, L.: Ant-q: a reinforcement learning approach to the
traveling salesman problem. In: Proceedings of ML 1995, Twelfth International
Conference on Machine Learning, pp. 252–260 (2016)
11. Glorot, X., Bengio, Y.: Understanding the diﬃculty of training deep feedforward
neural networks. In: Aistats, vol. 9, pp. 249–256 (2010)
12. Hu, J., Wellman, M.P.: Nash q-learning for general-sum stochastic games. J. Mach.
Learn. Res. 4, 1039–1069 (2003)
13. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR
abs/1412.6980 (2014)
14. Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver,
D., Kavukcuoglu, K.: Asynchronous methods for deep reinforcement learning. In:
International Conference on Machine Learning, pp. 1928–1937 (2016)
15. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level
control through deep reinforcement learning. Nature 518(7540), 529–533 (2015)
16. Shapley, L.S.: A value for n-person games. Contrib. Theor. Games 2(28), 307–317
(1953)
17. Simoes, D., Lau, N., Reis, L.P.: Multi-agent double deep q-networks. In: Portuguese
Conference on Artiﬁcial Intelligence. Springer (2017)
18. Zhang, C., Lesser, V.: Multi-agent learning with policy prediction. In: Proceedings
of the Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2010, pp.
927–934. AAAI Press (2010)

Reward-Weighted GMM and Its Application
to Action-Selection in Robotized Shoe Dressing
Adri`a Colom´e(B), Sergi Foix, Guillem Aleny`a, and Carme Torras
Institut de Rob`otica i Inform`atica Industrial CSIC-UPC, Llorens i Artigas 4-6,
08028 Barcelona, Spain
{acolome,sfoix,galenya,torras}@iri.upc.edu
Abstract. In the context of assistive robotics, robots need to make
multiple decisions. We explore the problem where a robot has multi-
ple choices to perform a task and must select the action that maximizes
success probability among a repertoire of pre-trained actions. We inves-
tigate the case in which sensory data is only available before making the
decision, but not while the action is being performed. In this paper we
propose to use a Gaussian Mixture Model (GMM) as decision-making
system. Our adaptation permits the initialization of the model using
only one sample per component. We also propose an algorithm to use
the result of each execution to update the model, thus adapting the robot
behavior to the user and evaluating the eﬀectiveness of each pre-trained
action. The proposed algorithm is applied to a robotic shoe-dressing task.
Simulated and real experiments show the validity of our approach.
1
Introduction
Assistive robots are becoming a reality. Apart from the safety requirements, one
of the main issues is related to the high variability of human environments. In
this context, perceptions, and by extension the measurement of the current state,
are partial and uncertain. Additionally, the eﬀects of the actions of the robot
are also uncertain, as we consider that actions may not only succeed or fail, but
also have intermediate results.
In the framework of the I-DRESS project, we are investigating how robots
could be useful assistants in dressing tasks. These tasks require interaction with
the user, adaptation to the preferences of the user, and guaranteeing safety.
In assistive robotics, the ability to take adequate decisions is crucial. During
the completion of a task, a robot has to decide, multiple times, among diﬀerent
actions. In this paper we put the focus on choosing among actions that solve the
same goal but use diﬀerent strategies.
Let us use an example to provide further intuition. The task of shoe dressing is
quite complex: it involves several actions (choose a shoe, grasp, approach, insert);
diﬀerent types of shoe require diﬀerent insertion strategies; and people have dif-
ferent mobility restrictions. It is very diﬃcult to program a robot to cope with all
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_12

142
A. Colom´e et al.
Fig. 1. Overall setup.
this variability. We envisage that this can be done by providing the robot with a
repertoire of strategies and the ability of choosing one among them.
Here we concentrate on the insertion action as it provides a perfect example
(see Fig. 1). Imagine that the robot has a repertoire of insert actions, learned in
the factory environment, which are suited to several foot orientations of refer-
ence. In a real situation, the robot has to decide which insert action to execute
based on the current orientation of the user foot. Naturally, the current orienta-
tion will not perfectly correspond to a reference orientation, so an estimation of
the most convenient one is required.
When the relevant sensory data is available during the execution of the tra-
jectory at a high rate, the usual strategy is to deﬁne an error function and
reshape the trajectory to minimize this error function. In a recent work, Pignat
and Calinon [7] propose a method that allows to demonstrate few example tra-
jectories that include sensory data. Their system is able not only to reproduce
the expected behavior, but also to learn the relevant relationships between sen-
sory data and robot motion. Note that visual information is most often used, but
in assistive robotics force plays an important role. In this paper we explore the
case when this information is not available. We assume that the only information
available are some snapshots at the beginning and the end of the actions, so an
error function cannot be deﬁned.
Action selection (also known as the problem of what to do next) is a broad
ﬁeld in artiﬁcial intelligence. The available selection methods can be divided
in two families: global and local. Global methods use the information about
the long-term goal, the set of actions that the robot can execute, and their
eﬀects and costs, to obtain a plan that optimizes the cost of the entire set
of actions. Kaelbling and Lozano-Perez [5] proposed a hierarchical planning

Reward-Weighted GMM and Its Application
143
algorithm to limit the length of the obtained plans. Mart´ınez et al. [6] proposed
to use short-horizon planning in tasks where the uncertainty of the actions makes
the prediction of the state too ambiguous after few actions.
Contrarily, local methods use only the information at a given point to choose
the action that maximizes the immediate reward. Local action selection has
been used traditionally in active vision to decide the placement of the camera
(see [2] for a review). Diﬀerent measurements can be used as decision crite-
rion. For example, Foix et al. [3] propose to use a variation of the information
gain to combine gains from two diﬀerent sources. All these methods are based
on estimating the eﬀects of executing given actions. However, here we deal with
a slightly diﬀerent problem, as the only two possible outcomes of the actions
are success/not success, and the problem is to select the appropriate action that
maximizes the success probability.
In this paper we propose a decision-making system based on an adaptation
of a Gaussian Mixture Model (GMM) that permits initialization by using only
one sample per component. We also show how the model can be updated after
each new execution by taking advantage of the result of the preceding trial.
2
Revisiting Gaussian Mixture Models
A Gaussian Mixture Model (GMM) distribution over a random variable x can be
written as a weighted superposition of K Gaussians with mean μk and covariance
Σk, weighted by their mixing proportions πk [1], for k = 1..K:
p(x) =
K

k=1
πkN(x|μk, Σk),
(1)
where it is common to consider πk as the probability of a K-dimensional random
variable z, with zk ∈{0, 1} and p(zk = 1) = p(zk = 1, z̸=k = 0) = πk [1]. Note
that 0 ≤πk ≤1 and K
k=1 πk = 1. Therefore, p(x|zk = 1) = N(x|μk, Σk) and,
marginalizing x wrt. z we obtain Eq. (1) again:
p(x) =

z
p(x|z)p(z) =
K

k=1
πkN(x|μk, Σk)
(2)
The GMM model in Eq. (1) can be obtained with an Expectation-
Maximization (EM) algorithm [1], for which we need to compute a term
γ(zik) ≜p(zk = 1|xi) by using the Bayes’ rule:
γ(zik) =
πkN(xi|μk, Σk)
K

k=1
πkN(xi|μk, Σk)
.
(3)
γ(zik) is called the responsibility of the component k associated with a sample
xi, and we will use it later for action selection.

144
A. Colom´e et al.
Using the aforementioned EM algorithm with a set of N data samples X,
the log-likelihood lnp(X|π, μ, Σ) can be maximized in an iterative 2-step opti-
mization:
– E-step: Evaluate the responsibilities using current parameters with Eq. (3)
– M-step: Re-calculate the parameters with (see [1]):
μnew
k
= 1
Nk
N

i=1
γ(zik)xi
(4)
Σnew
k
= 1
Nk
N

i=1
γ(zik)(xi −μnew
k
)(xi −μnew
k
)T
(5)
πnew
k
= Nk
N
(6)
where N = 
k Nk and
Nk =
N

i=1
γ(zik)
(7)
– Evaluate the log-likelihood:
lnp(X|π, μ, Σ) =
N

i=1
ln
 K

k=1
πkN(x|μk, Σk)

(8)
and check for convergence.
3
Using a GMM for Action Selection
In this section, we adapt a GMM in order to be used for action selection. For
this purpose, we match the number of Gaussian components K to the number
of actions available, and use the samples x ∈Rd as contextual features that will
take the role of variables deciding which action to execute, d being the dimension
of the feature space.
We modify the GMM deﬁned in Sect. 2, so that it can be initialized with
fewer samples (even as few as one sample per component). Such small number
of samples results in the need to adapt the values Nk, as the responsibilities or
number of samples to generate the distribution might not be available. Moreover,
we add weights to the GMM responsibilities in order to update the model after
each sample, according to the results of the experimentation. Such procedure
helps to improve the decision-making algorithm.

Reward-Weighted GMM and Its Application
145
3.1
Initializing a GMM with Few Samples
In order to initialize a GMM with few samples, we assume we have K possible
actions, with mk samples for each of them, and we know the action associated
with each sample in the training dataset. If mk ≥4d, it is then considered
that there is probably enough data to ﬁt the covariance matrix of mixing the
component k [4]. In that case, the EM algorithm presented in Sect. 2 generates a
properly initialized model. Otherwise, We can initialize the means of the GMM
by using the average of the samples associated to each action k = 1..K:
μk =
1
mk

i∈K
xi.
(9)
The covariances for each component are then initialized by assuming a 1.5
standard deviations intersection. That is, Σk = λ2
kI, where
λk = minj=1..K,j̸=k
∥μj −μk∥
1.5
.
(10)
Last, we initialize πk = 1/K, ∀k = 1..K.
The rationale behind Eq. (10) is that the resulting Gaussian components
are intersecting, but with an overlap small enough to be discriminative. We
build hyperspheres in a d-dimensional space, centered at the mean points μk
(Eq. (9)) and radii so that their contacts are at most tangent. Then, we use
these radii and set them to be 1.5 standard deviations on an isotropic covariance
matrix, as deﬁned in Eq. (10). We found that the 1.5 ratio oﬀered a good trade-oﬀ
between Gaussians not being too small and having a relevant probability in their
region of overlap. Figure 2 shows an example of the proposed initialization of the
GMM with only 3 samples per component in a GMM with K = 5 components,
comparing diﬀerent values of the number of standard deviations-equivalence used
in Eq. (10).
0
0.5
1
0
0.2
0.4
0.6
0.8
1
(a) 1 std. deviation
0
0.5
1
0
0.2
0.4
0.6
0.8
1
(b) 1.5 std. deviations
0
0.5
1
0
0.2
0.4
0.6
0.8
1
(c) 2 std. deviations
Fig. 2. Initialization of a GMM with few samples. The means of the components (blue
crosses) are obtained by averaging the samples (black dots). Then, the covariance
matrix is obtained with Eq. (10). As we can see, 1 standard deviation generates a
GMM with too much overlap between components, while 2 standard deviations have
a too reduced overlap. Therefore, we used 1.5 standard deviations equivalence.

146
A. Colom´e et al.
3.2
Reward-Weighted Responsibility GMM (RWR-GMM)
In order to apply GMMs to action selection, we now present a variant of the
GMM presented in Sect. 2, where the responsibilities γ(zsk) play a crucial role.
Given a sample xs, the action to execute will be selected depending on such
responsibility values, as γ(zsk) = p(zk = 1|xs). However, the GMM needs to be
adapted after each execution, depending on the success of the decision taken by
our selector. For this reason, if the success is denoted by the random variable
r ∈[0, 1], such that rsk = 1 if action k was successful when applied in situation
xs, then we deﬁne the reward-weighted responsibility of an action as
dskγ(zsk) = p(rsk = 1|zk = 1, xs)p(zk = 1|xs),
(11)
which is equivalent to the probability of success when applying action k in a
situation described by xs.
Given such weighted responsibilities, we can substitute dskγ(zsk) for γ(zsk) in
Eqs. (4)–(7) and obtain a Reward-Weighted Responsibility GMM (RWR-GMM):
μnew
k
= 1
Nk
N

i=1
dskγ(zsk)xi
(12)
Σnew
k
= 1
Nk
N

i=1
dskγ(zsk)(xi −μnew
k
)(xi −μnew
k
)T
(13)
πnew
k
= Nk
N
(14)
where N = 
k Nk and
Nk =
N

s=1
dskγ(zsk)
(15)
This permits assigning importance to samples, but it is specially used to
update the model in a one-step manner, as will be described in Sect. 3.3.
3.3
Updating an RWR-GMM
Another issue we can encounter is how to update a given RWR-GMM (π, μ, Σ)
when a new sample is added and we do not know how many samples were used to
build it, or if it was artiﬁcially generated. This results in unknown values for the
responsibilities γ(zsk) and Nk. However, given Eq. (7), we can assume that the
GMM was generated with a certain number of samples N and so, isolating Nk
from Eq. (7), Nk = Nπk. The choice of N will depend on the relative importance
we want to give to a newly-generated sample and acts as a forgetting factor. For
example, setting N = 100 would be equivalent to treat the GMM as if it was
generated with such amount of samples, and the eﬀect of an additional sample
would be relative to that amount.

Reward-Weighted GMM and Its Application
147
Algorithm 1. RWR-GMM using all actions
Input: GMM with {πk, Σk, μk} and a preset value of N, so that Nk = Nπk
Output: Updated GMM parameters {πk, Σk, μk}
1: for s = 1 : Nsamples do
2:
For the given sample xs, execute all the actions k = 1..K and check whether
the execution was successful.
3:
Update the RWR-GMM with dsk = 1, ∀k that were successful, and dsk⋆= 0
otherwise.
4: end for
Algorithm 2. RWR-GMM checking only one action
Input: GMM with {πk, Σk, μk} and a preset value of N, so that Nk = Nπk
Output: Updated GMM parameters {πk, Σk, μk}
1: for s = 1 : Nsamples do
2:
Compute the responsibilities γ(zsk), ∀k = 1..K, for the given sample xs with
Eq. (3). Note that 
k γ(zsk) = 1.
3:
Randomly select an action k⋆using the probabilities obtained with responsi-
bilities
4:
Execute the action k⋆and check if it was successful.
5:
If action k⋆was successful, we update the RWR-GMM with dsk = 0, ∀k ̸= k⋆,
dsk⋆= 1. Otherwise, dsk = 1, ∀k ̸= k⋆and dsk⋆= 0.
6: end for
If, given a model (πk, μk, Σk), we add a sample xn, Eqs. (12)–(15) can be
reformulated by adding a point and substituting the old values of πk, μk, Σk:
μnew
k
=
Nk
Nk + dskγ(zsk)

μk + dskγ(zsk)xn
Nk

(16)
Σnew
k
=
Nk
Nk + dskγ(zsk)

Σk + dskγ(zsk)(xn −μk)(xn −μk)T
Nk

(17)
N new
k
= Nk + dskγ(zsk)
(18)
In practical applications, we will have a number K of actions to select from
and, given an initialized GMM as in Sect. 3.1, a new sample xs consisting of state
feature variables that can be observed/measured. We propose two alternatives
for improving the method: in the ﬁrst one, we assume we can test all the possible
actions for each point xs, and we will set dsk = 1 for the successful actions and 0
otherwise (see Algorithm 1). Alternatively, in case the assumption is not satisﬁed,
we will evaluate the responsibilities of each action for each new sample xs, and
randomly choose an action k⋆with a probability equal to its responsibility. Then,
this action will be executed and tested for success. If so, its weight will be set
to dsk⋆= 1 and the other actions to 0. Otherwise, dsk⋆= 0 and, for the other
actions, it will be set to 1, as shown in Algorithm 2.

148
A. Colom´e et al.
Fig. 3. Initial pose for each of the shoe insertion trajectories. Notice that the tip of the
foot is already into the shoe.
After
setting
the
reward
weights,
we
can
update
the
model
with
Eqs. (16)–(18).
4
Experimentation
Figure 1 shows the overall setup used in our experiments. In order to ensure a
good initialization pose for both shoe and foot, experiments were carried out
using two arm manipulators. As it can be seen, we used a mannequin leg instead
of a real human leg. Thanks to that, we managed to avoid two complicated
issues: the use of an external vision system for a precise foot pose estimation
and the diﬃculty for a human to repeatedly maintain a constant foot pose while
the robot is putting the shoe. Figure 3 shows a detailed view of the initial pose
for each of the demonstration trajectories. It is important to notice that both
the tip of the foot and the shoe poses are always in the same initial position for
all the experiments no matter the orientation. This way, we ensure that a wrong
action cannot be due to failed shoe insertion.
Obviously, the action required to insert a shoe depends on the position of the
foot but also on the type of shoe. Shoes like ﬂip ﬂops and some sandals require
easy motions as do not have a back strap. Contrarily, some orthopedic shoes are
challenging because the topline on top of the counter can bend over. Observe
that the shoe used in the experiments (Fig. 1) belongs to the last group, and
that means that insert actions should tend to fail more easily.
Experiments consisted of the following steps: ﬁrst, we taught the robot,
through kinesthetic teaching, how to perform a good shoe insertion for each
of the reference demonstration foot poses; and second, we equally divided the
conﬁguration space by 0.1 rad. and tested exhaustively each intermediate foot

Reward-Weighted GMM and Its Application
149
pose, xs =
pitchs
yaws
	
, with each of the taught shoe insertion actions. Foot poses
were chosen as realistic as possible and by taking into account a range foot ori-
entation comfortable for humans when performing that task, ±0.5 radian range
for both pitch and yaw angles. Given the bi-dimensionality of the parametriza-
tion and the short ranges, we trained the system with 5 diﬀerent demonstration
poses. One central and the other 4 located at the limit of each range. Using the
robot that was holding the mannequin leg as a reference frame, these are the
selected centers for each GMM:
μ1 = [0; −1.5],
μ2 = [−0.5; −1.5],
μ3 = [0.5; −1.5],
μ4 = [0; −2.0],
μ5 = [0; −1.0].
(19)
After using the initialization method described in Sect. 3.1, we obtain the ini-
tialization map shown in Fig. 4. Observe the homogeneous distribution of the
Gaussians over the central pose point and the 4 limit pose points. Figure 5 shows
the success of actions after testing for each action at each intermediate point xs
(pitch = −0.5 : 0.1 : 0.5 and yaw = −2 : 0.1 : −1), in a total of 605 execu-
tions. Notice how actions taught at diﬀerent foot pitch angles overlap on some
intermediate poses, showing a good compatibility; the incompatibility between
actions taught at diﬀerent foot yaw angles can also be seen.
pitch
5
.
0
0
5
.
0
-
yaw
-2
-1.8
-1.6
-1.4
-1.2
-1
Fig. 4. Initialization of the GMM with only one sample per node.
Whether a shoe insertion action was successfully achieved or not was con-
trolled by visual check. Please see Figs. 6a, b for a good and a bad example,
respectively.

150
A. Colom´e et al.
pitch
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
yaw
-2
-1.8
-1.6
-1.4
-1.2
-1
Fig. 5. Successful executions for each of the 5 actions.
4.1
Using Algorithm 1
Among the 121 tested points, we only considered those 46 which were success-
ful. Among these 46, we used 21 points for training, and the remaining 25 for
validation. Using the initialization in Fig. 4, with an initial value of N = 20, and
training the model following Algorithm 1, we compared the successful actions
for the validation set with the responsibilities obtained from the RWR-GMM
updates, see Fig. 7. In order to evaluate the accuracy of the system, we ﬁtted the
model with 10 randomly chosen sample subsets and checked its responsibility
for an action. After evaluating all of them and averaging the result, we obtained
(a) Success.
(b) Failure.
Fig. 6. Visual check of action fulﬁllment. (a) shows a perfectly successful action. (b)
shows a non-satisfactory case.

Reward-Weighted GMM and Its Application
151
98.4% accuracy in assigning the highest responsibility to an action that was
successful. The remaining 1.6% were failures corresponding to points where the
successful action was the second with the highest responsibility, with a value of
more than 0.3, therefore corresponding to overlapped regions.
pitch
5
.
0
0
5
.
0
-
yaw
-2
-1.8
-1.6
-1.4
-1.2
-1
Fig. 7. Results with sampling as in Algorithm 1.
pitch
5
.
0
0
5
.
0
-
yaw
-2
-1.8
-1.6
-1.4
-1.2
-1
Fig. 8. Results with sampling as in Algorithm 2.

152
A. Colom´e et al.
4.2
Using Algorithm 2
Similarly to the previous subsection, we took 21 of the points with at least
one successful action and trained the system using the procedure described in
Algorithm 2, see Fig. 8. In this case, we obtained 98% success in assigning the
highest probability to a successful action, the unsuccessful ones also being in
positions where a successful action had a responsibility higher than 0.3.
5
Conclusions
In this article we have shown how a GMM can be adapted to be used as an
action decision-making system. Such adaptation permits shortening the model
initialization process, by reducing the number of required samples thanks to
some minor assumptions, and to improve the update of the model at runtime,
by adding rewards as weights into the GMM responsibilities. Experiments have
demonstrated the validity of the approach by achieving 98% accuracy in a very
challenging real scenario, shoe dressing. Two algorithms have been presented,
one that considers observability of all possible actions for testing at each step
and another that evaluates only one at a time. We have shown how the lat-
ter, although slightly less precise, is highly eﬃcient and requires 5 times less
executions.
As future work we would like to extend experimentation to other user-
adjustable robotic assistive tasks such as jacket or gown dressing, food feeding,
or prosthetic grasping. To carry out such complex tasks, the proposed system
could be enhanced by making robot skills feature-dependent, so as to better
adapt robot motion to the observed situation.
References
1. Bishop, C.M.: Pattern Recognition and Machine Learning. Springer, New York
(2007)
2. Chen, S., Li, Y., Wang, W., Zhang, J.: Active Sensor Planning for Multiview Vision
Tasks, vol. 1. Springer, Heidelberg (2008)
3. Foix, S., Aleny`a, G., Torras, C.: 3D sensor planning framework for leaf probing. In:
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 6501–
6506 (2015)
4. Hansen, N.: The CMA evolution strategy: a tutorial. Computer Research Repository
(2016)
5. Kaelbling, L.P., Lozano-P´erez, T.: Hierarchical task and motion planning in the now.
In: 2011 IEEE International Conference on Robotics and Automation (ICRA), pp.
1470–1477. IEEE (2011)
6. Mart´ınez, D., Aleny`a, G., Torras, C.: Planning robot manipulation to clean planar
surfaces. Eng. Appl. Artif. Intell. 39, 23–32 (2015)
7. Pignat, E., Calinon, S.: Learning adaptive dressing assistance from human demon-
stration. Robot. Auton. Syst. 93, 61–75 (2017)

Pose Invariant Object Recognition Using a Bag
of Words Approach
Carlos M. Costa(B), Armando Sousa, and Germano Veiga
INESC TEC and Faculty of Engineering, University of Porto, Porto, Portugal
{carlos.m.costa,germano.veiga}@inesctec.pt, asousa@fe.up.pt
Abstract. Pose invariant object detection and classiﬁcation plays a crit-
ical role in robust image recognition systems and can be applied in a
multitude of applications, ranging from simple monitoring to advanced
tracking. This paper analyzes the usage of the Bag of Words model for
recognizing objects in diﬀerent scales, orientations and perspective views
within cluttered environments. The recognition system relies on image
analysis techniques, such as feature detection, description and clustering
along with machine learning classiﬁers. For pinpointing the location of
the target object, it is proposed a multiscale sliding window approach fol-
lowed by a dynamic thresholding segmentation. The recognition system
was tested with several conﬁgurations of feature detectors, descriptors
and classiﬁers and achieved an accuracy of 87% when recognizing cars
from an annotated dataset with 177 training images and 177 testing
images.
Keywords: Object recognition · Image feature analysis · Clustering ·
Machine learning
1
Introduction
Pose invariant object detection is a critical component in automated systems that
require robust detection and classiﬁcation of classes of objects within cluttered
environments. It also plays a pivotal role on extracting information from images
by providing the classiﬁcation of the objects along with their position. Given its
generalization properties, this kind of systems can be adapted to a multitude of
tasks, and an eﬃcient implementation could be used in real-time applications.
Several approaches were suggested over the years, ranging from the more com-
putationally intensive solutions that compare patches of the image to a database
of objects in several poses, to the more eﬃcient techniques that uses classiﬁers to
try to detect several variations of the target object [1–3]. This paper focuses on
the later and provides an analysis of the application of the Bag of Words model
to object detection and classiﬁcation.
The system relies on an initial setup phase for training a classiﬁer that
later on can be used for recognizing the target objects. It starts by building
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_13

154
C.M. Costa et al.
a visual vocabulary using the feature descriptor clusters of the training images.
This vocabulary represents characteristic structures of the target object and will
be used as the n-dimensional descriptor space to describe an image. Using this
vocabulary, a database of samples is built for training a machine learning classi-
ﬁer. This classiﬁer creates a descriptor model that later on can be employed to
detect the target object in test images. After the setup of the recognition system,
the detection of the target objects along with their location in the image relies
on a sliding window technique [4]. This approach uses the trained classiﬁer to
scan the image with windows of diﬀerent size. In the end, a voting mask with the
probable locations of the targets is retrieved and in conjunction with a dynamic
thresholding method the locations are extracted. This approach achieved promis-
ing results and can be used to recognize objects in diﬀerent perspective views
even if they are within cluttered environments. To allow the ﬁne tuning of the
system conﬁguration, several feature detector and descriptors can be selected in
conjunction with a range of machine learning classiﬁers.
In the following section it will be presented a brief overview of other
approaches that can be used to perform pose invariant object recognition. Later
on, Sect. 3 will provide a detailed description of the implemented system. Then,
the results along with the respective analysis will be discussed in Sects. 4, 5 and
6. Finally, Sect. 7 will give a brief set of conclusions and possible future work.
2
Related Work
There are numerous approaches for detecting objects that can appear in several
perspective views. Ranging from the very simple template matching to highly
advanced systems relying on point cloud perception. The most basic method to
perform pose invariant object detection is template matching. In this method,
a database of images taken from several points of view is used to scan the test
image and try to detect the target object. The problem of this approach is that
it requires the image to be scanned with this database in several scales and
orientations, which causes it to be very ineﬃcient.
To solve the scale and orientation problem [5], feature detection and descrip-
tion algorithms can be used [6]. In this approach the database is only scanned
once. Moreover, since the feature detection describes the image as feature points,
the size of data to be compared is reduced drastically, and as a result, it is orders
of magnitude more eﬃcient than template matching. In this method, it is critical
that the matching of descriptors is ﬁltered in order to remove outliers, using for
example a ratio test [5] or a homography computed using Random Sample Con-
sensus. Other approaches suggest the construction of an Implicit Shape Model
[3], that takes into consideration the relative position of interesting structures
in the target object, in order to build a 3D representation that can then be used
to recognize the intended objects. Other methods use image strip features [7] to
speed up recognition by focusing in structural parts of the target object or even
Haar wavelets and edge orientation histograms [8].
For recognition of speciﬁc 3D objects, a more advanced approach using point
clouds matching can be used. In this technique, 3D point clouds are matched

Pose Invariant Object Recognition Using a Bag of Words Approach
155
using algorithms such as the Iterative Closest Point [9]. Besides recognizing the
object, this method also allows the identiﬁcation of the position of the camera
in relation to the target object. However, this approach may not be suitable for
general classiﬁcation of objects, because it was designed to search for a particular
3D geometry. Moreover, it takes considerable computation time to extract 3D
point clouds from images, unless the point clouds are retrieved directly from the
environment using 3D sensors (such as LIDARs).
After reviewing the existing approaches, it was clear that an eﬃcient and
general recognition system should rely on machine learning algorithms in order to
be able to successfully recognize the intended category of objects within cluttered
environments. One way to implement such system is by employing the bag of
words model in conjunction with classiﬁers. As shown in [10], this approach has
promising results and good eﬃciency.
3
Recognition System
The recognition system is comprised with a setup phase, in which a classiﬁer is
trained with samples built with a visual vocabulary, and a recognition phase, in
which the classiﬁer is used to identify new instances of the target objects. In the
next sections it will be provided a detailed description of the main steps required
to successful recognize categories of objects in cluttered environments.
3.1
Preprocessing
To improve the detection of good features and ensure that the system has robust
recognition even when the images have considerable noise, a preprocessing step
was applied. In a ﬁrst phase, most of the noise was removed using a bilateral ﬁlter.
This ﬁlter was chosen because it preserves the edges of the image blobs, which
are very valuable structures in the detection of feature points. After the noise was
reduced, a Contrast Limited Adaptive Histogram Equalization (CLAHE) ﬁlter
was applied to increase the contrast. This can improve the recognition of the
system when the images are taken in low light environments. This technique has
better results over the simple histogram equalization because it can be applied
to images that have areas with high and low contrast and also limits the spread
of the noise. Finally, the brightness was adjusted to correct images that were too
dark or too bright.
3.2
Visual Vocabulary
The Bag of Words model [1] had its inception in the document classiﬁcation
realm, but its concepts can be extended to image recognition by treating image
features as words. As such, a visual vocabulary must be built from the target
objects feature descriptors. In this stage, each image in the vocabulary image
list set is preprocessed, and for each ground truth mask of the target objects, it
is computed the feature points and their associated descriptors. These extracted

156
C.M. Costa et al.
descriptors are then grouped using the k-means clustering algorithm in order
to obtain the visual words of the vocabulary. There are several algorithms to
select features from images. The supported feature detectors are SIFT, SURF,
GFTT, FAST, ORB, BRISK, STAR and MSER. For describing these features
there is also several algorithms that aim to be scale and rotation invariant.
The supported feature descriptors are SIFT, SURF, FREAK, BRIEF, ORB
and BRISK. The matching of these descriptors can be performed using either a
brute force or a heuristic approach. In the brute force approach, each descriptor
in the image is compared with all descriptors in the reference image to ﬁnd
the best correspondence. In the heuristic approach (that relies on the FLANN
library), several optimizations are employed to speed up the computations. These
optimizations can be related to the appropriate selection of which descriptors to
match, and to the use of eﬃcient data structures to speed up the search (such
as k-d trees).
3.3
Training Samples
Before a classiﬁer can be used, it must be trained with several annotated samples
of the target objects. As such, a training database was built using the vocabulary
of the visual words computed earlier. In this stage, each manually annotated
image of the training set list is preprocessed, its feature points are computed
and separated into the corresponding classes (target or background) according
to the ground truth masks. These manually annotated masks specify if a region
belongs to the target objects or the background (shown in Fig. 1 right image as
red and black respectively). After having the segmented keypoints, it is computed
the associated descriptors and the visual vocabulary is built. The results are a
set of normalized histograms of the visual words present in each training image,
associated with the corresponding labels, that will inform the classiﬁer which
class the training samples belong to (target or background).
3.4
Classiﬁer Training
After having the training samples, a classiﬁer can be trained in order to build
a model of the distribution of the target object visual words descriptors. This
model can then be used to predict with acceptable accuracy if the target objects
are in an image or not. There are several machine learning classiﬁers to per-
form object recognition. The included classiﬁers are Support Vector Machines,
Artiﬁcial Neural Networks, Normal Bayes Classiﬁers, Decision Trees, Boosting,
Gradient Boosting Trees, Random Trees and Extremely Randomized Trees.
3.5
Object Recognition
For achieving scale invariant object recognition it was implemented a sliding
window technique with Regions of Interest (ROI) with several sizes. In this
method the image is scanned with ROIs from left to right and from top to

Pose Invariant Object Recognition Using a Bag of Words Approach
157
bottom in a column by column and row by row approach. In order to be scale
invariant, the image is scanned several times with a increasing ROI size and
the ROI movement increment was carefully chosen for ensuring overlapping of
successive ROIs. During the image scanning, the trained classiﬁer is used to
evaluate if the target object is present or not within each image ROI. If the
classiﬁer predicts that the object is within the ROI with high conﬁdence, then
the voting mask cells within the ROI are incremented. In the end of the image
scanning, the voting mask are used in conjunction with a dynamic thresholding
method to pinpoint where are the target objects in the image. After having the
image binarized into target and background classes, a blob detection algorithm
is used to retrieve the bounding boxes of the target regions.
3.6
Evaluation of Results
To evaluate the results of the object recognition system, an image test set was
used, in which the computed voting masks were compared with the target objects
ground truth masks (in the right side of Fig. 1 is an example of a ground truth
mask for the left side image).
In this stage, each pixel in the computed voting masks was compared to the
ground truth masks, in order to see if the result was a true positive (correctly
detected that there was a target object), true negative (correctly labeled back-
ground), false positive (incorrectly labeled background as belonging to a target
object) or false negative (missed regions that belonged to the target objects and
were labeled as background). With each of these measures acquired for each test
image, the accuracy, precision and recall was computed.
To allow fast testing of the system, it was implemented an automatic eval-
uation module that analyzes all the test images and collects both intermediate
and ﬁnal results.
Fig. 1. Example of dataset image (left) and associated ground truth masks (right).
The manually annotated red regions represent target objects while the black regions
represent background.
4
Methodology
The results were collected using a Clevo P370EM, with an i7-720QM CPU,
NVIDIA GeForce GTX 680M GPU and 16 GB of RAM DDR3 (1600 MHz).

158
C.M. Costa et al.
It was used the Graz-02 dataset of car images, from which it was retrieved
177 images to build the vocabulary and the training samples, and another 177
images for testing the recognition system.
The visual vocabularies were built with a 1000 word size, and all the inter-
mediate results (vocabulary, training samples, and classiﬁers) were saved to xml
ﬁles to speedup future uses of the system.
The OpenCV algorithms were used with the default parameters with the
exception of the SVM classiﬁer, in which the maximum number of iterations
was set to 100000. Also, the Artiﬁcial Neural Networks were conﬁgured to
have 20 neurons in the intermediate layer. Moreover, for binary descriptors,
the FLANN matcher was modiﬁed to use the multi probe LSH index search,
and the BFMatcher to use Hamming distances.
The sliding window technique used 482 regions of interest per image. These
patches start at 20% of the image size, and after each scan of the image, (in
which the patch moves at 25% increments of its own size), the patch grows 10%
(in relation to the image size).
5
Results
In Figs. 2, 3, 4, 5, 6, 7 and 8 and Tables 1 and 2 are presented some representative
results of the recognition of the target objects in several perspective views and in
diﬀerent types of environments. On the right side of the images it is presented the
voting masks. These masks start with count 0 (black) and every time a classiﬁer
predicts that the target object is present in that ROI, the pixels in the masks are
incremented (becoming increasingly brighter in the images). As such, brighter
regions indicate that a lot of ROIs were marked as containing the target object,
Fig. 2. Results obtained with STAR detector, SIFT extractor, FLANN matcher and
ANN classiﬁer (right image with the voting masks and left image with the overlaid
results)

Pose Invariant Object Recognition Using a Bag of Words Approach
159
Fig. 3. Results obtained with STAR detector, SURF extractor, FLANN matcher and
SVM classiﬁer (right image with the voting masks and left image with the overlaid
results)
Fig. 4. Results obtained with STAR detector, FREAK extractor, FLANN matcher
and SVM classiﬁer (right image with the voting masks and left image with the overlaid
results)
Fig. 5. Results obtained with STAR detector, SIFT extractor, FLANN matcher and
SVM classiﬁer (right image with the voting masks and left image with the overlaid
results)

160
C.M. Costa et al.
Fig. 6. Results obtained with SURF detector, SURF extractor, FLANN matcher and
ANN classiﬁer (right image with the voting masks and left image with the overlaid
results)
Fig. 7. Results obtained with FAST detector, SURF extractor, FLANN matcher and
ANN classiﬁer (right image with the voting masks and left image with the overlaid
results)
Fig. 8. Results obtained with ORB detector, ORB extractor, FLANN matcher and
ANN classiﬁer (right image with the voting masks and left image with the overlaid
results)

Pose Invariant Object Recognition Using a Bag of Words Approach
161
and with a dynamic threshold, it can be extracted the regions in the image in
which the target objects reside (presented as an overlaid yellow rectangle on the
left images, along with the detected keypoints as colored circles).
6
Analysis of Results
In Tables 1 and 2 it is shown the detailed results that were obtained in the testing
of the recognition system. It is presented both the recognition performance and
Table 1. Object recognition conﬁgurations and performance results
Test ID
Feature
detector
Feature
descriptor
Feature
matcher
Classiﬁer
Accuracy
Precision
Recall
1
STAR
SIFT
FLANN
Neural Network
0.874
0.234
0.162
2
STAR
SURF
FLANN
SVM
0.855
0.271
0.214
3
STAR
SURF
BFMatcher
SVM
0.854
0.299
0.234
4
STAR
SIFT
FLANN
SVM
0.847
0.306
0.362
5
STAR
BRIEF
FLANN
SVM
0.841
0.276
0.277
6
ORB
ORB
FLANN
Neural Network
0.839
0.206
0.195
7
STAR
FREAK
FLANN
SVM
0.815
0.274
0.279
8
SURF
SURF
FLANN
Neural Network
0.815
0.168
0.202
9
SIFT
SIFT
BFMatcher
Neural Network
0.794
0.217
0.296
10
SIFT
SIFT
BFMatcher
SVM
0.784
0.242
0.385
11
SIFT
SIFT
FLANN
SVM
0.776
0.251
0.411
12
ORB
ORB
FLANN
SVM
0.739
0.239
0.549
13
SIFT
SURF
FLANN
SVM
0.714
0.219
0.543
14
SIFT
SURF
BFMatcher
SVM
0.705
0.213
0.528
15
GFTT
FREAK
FLANN
SVM
0.699
0.201
0.478
16
MSER
SURF
FLANN
SVM
0.672
0.241
0.735
17
FAST
FREAK
FLANN
SVM
0.666
0.204
0.596
18
BRISK
BRISK
FLANN
SVM
0.661
0.213
0.682
19
SIFT
BRIEF
FLANN
SVM
0.616
0.187
0.661
20
SIFT
FREAK
BFMatcher
SVM
0.606
0.188
0.696
21
SIFT
FREAK
FLANN
SVM
0.605
0.191
0.717
22
SIFT
BRIEF
BFMatcher
SVM
0.601
0.191
0.732
23
BRISK
FREAK
FLANN
SVM
0.579
0.191
0.801
24
SURF
SURF
FLANN
Decision Tree
0.578
0.175
0.648
25
SURF
SURF
FLANN
Random Tree
0.503
0.172
0.847
26
SURF
SURF
FLANN
Boosting Tree
0.499
0.171
0.845
27
SURF
SURF
FLANN
Extremely Random
Tree
0.469
0.167
0.864
28
ORB
ORB
FLANN
Normal Bayes
Classiﬁer
0.446
0.165
0.886
29
SURF
SURF
FLANN
Gradient Boosting
Tree
0.423
0.161
0.897
30
SIFT
BRISK
FLANN
SVM
0.421
0.159
0.889

162
C.M. Costa et al.
Table 2. Object recognition temporal performance results (dataset with a group of
177 images for training and another one of 177 images for testing)
Test ID
Vocabulary build time
Training samples
build time
Classiﬁer training
time
Classiﬁer test
time
1
00 min 31.204 s
00 min 44.265 s
00 min 00.028 s
15 min 14.323 s
2
00 min 21.251 s
00 min 17.901 s
00 min 38.217 s
03 min 02.452 s
3
00 min 20.932 s
00 min 17.985 s
00 min 37.934 s
03 min 33.083 s
4
00 min 31.204 s
00 min 44.265 s
00 min 36.318 s
09 min 43.652 s
5
00 min 20.131 s
00 min 20.105 s
00 min 35.184 s
03 min 46.283 s
6
01 min 25.694 s
00 min 43.962 s
00 min 00.188 s
17 min 04.451 s
7
00 min 20.824 s
00 min 24.739 s
00 min 36.273 s
05 min 22.562 s
8
00 min 37.574 s
00 min 35.434 s
00 min 00.201 s
13 min 03.423 s
9
01 min 46.338 s
01 min 32.902 s
00 min 00.234 s
43 min 00.362 s
10
01 min 40.631 s
01 min 30.025 s
00 min 49.265 s
41 min 43.748 s
11
01 min 46.338 s
01 min 32.902 s
00 min 50.727 s
42 min 32.801 s
12
01 min 25.695 s
00 min 43.966 s
00 min 44.078 s
16 min 56.037 s
13
01 min 17.674 s
00 min 43.966 s
00 min 51.802 s
27 min 05.743 s
14
01 min 11.727 s
00 min 39.477 s
00 min 50.481 s
26 min 21.736 s
15
01 min 01.011 s
00 min 40.011 s
00 min 50.479 s
40 min 07.149 s
16
00 min 22.772 s
00 min 20.369 s
00 min 47.321 s
07 min 41.181 s
17
00 min 56.567 s
01 min 49.256 s
00 min 54.863 s
51 min 38.865 s
18
00 min 21.704 s
00 min 30.616 s
00 min 47.038 s
13 min 12.818 s
19
01 min 03.294 s
00 min 47.819 s
00 min 48.438 s
29 min 37.773 s
20
01 min 08.355 s
00 min 38.618 s
00 min 49.269 s
25 min 22.225 s
21
01 min 06.325 s
01 min 00.102 s
00 min 53.349 s
35 min 35.147 s
22
01 min 05.877 s
00 min 38.599 s
00 min 50.382 s
25 min 04.586 s
23
00 min 30.058 s
00 min 29.093 s
00 min 45.131 s
11 min 03.882 s
24
00 min 37.188 s
00 min 34.271 s
00 min 00.064 s
18 min 05.666 s
25
00 min 37.073 s
00 min 43.967 s
00 min 00.199 s
16 min 17.609 s
26
00 min 37.495 s
00 min 43.962 s
00 min 00.956 s
15 min 41.621 s
27
00 min 35.759 s
00 min 43.969 s
00 min 00.491 s
18 min 33.911 s
28
01 min 24.585 s
00 min 26.650 s
00 min 05.779 s
27 min 22.274 s
29
00 min 37.207 s
00 min 43.964 s
00 min 04.295 s
17 min 23.841 s
30
01 min 08.126 s
01 min 00.105 s
00 min 49.559 s
45 min 40.242 s
also the temporal performance in order to evaluate if the recognition was good
enough and also if it can be used for real time applications.
From the analysis of the results, it can be seen that the best accuracy
(87.4%) was achieved by combining the STAR feature detector, the SIFT
feature extractor, the FLANN matcher and the Artiﬁcial Neural Network
classiﬁer. This can be attributed to the superior feature description of the SIFT
algorithm due to its scale and orientation invariance, and to the fact that the

Pose Invariant Object Recognition Using a Bag of Words Approach
163
Neural Network classiﬁer can achieve better generalization of models. Neverthe-
less, the second best accuracy result (85.5%), which was achieved with the STAR
feature detector, the SURF feature extractor, the FLANN matcher, and the Sup-
port Vector Machine classiﬁer, was 5 times faster to analyze all the test images.
This is greatly due to the application of a faster feature extractor (SURF), and
the usage of the more eﬃcient classiﬁer (the SVM shifted the computation time
to the training stage, in which it was more than 1300 times slower than the
best result, but since this is computed only once, it is an acceptable cost for the
overall usage of the system).
From the output of the system it can also be seen that the preprocessing
stage helped in the selection of better feature points by reducing the noise and
correcting the contrast and brightness. This can be seen in the Fig. 9, in which
the mud in the car was reduced and the pavement was smoothed.
Fig. 9. Eﬀect of removing noise and improving contrast and brightness (original image
on the left, preprocessed on the right)
7
Conclusions
The presented Bag of Words approach to pose invariant object recognition has
shown promising results and good versatility to handle diﬀerent shapes of cars
in diﬀerent views. Its eﬃciency, accuracy and ﬂexibility make it a viable solution
for recognition of classes of objects with variable geometry.
The clustering of descriptors obtained with scale and rotation invariance
signiﬁcantly contributed to the accuracy and robustness of the recognition and
in conjunction with the versatility of the bag of words model, allowed the system
to recognize the target objects within cluttered environments.
These results can be further improved if a more advanced and precise location
detection algorithm is used (instead of the sliding window approach). This can
be achieved by either improving this method, or by considering its result as an
initial step in identifying the target objects. For example, the peak in the voting
mask could be used has the centroid of a more advanced segmentation technique,
in order to retrieved the real location and contour of the target objects.

164
C.M. Costa et al.
Acknowledgments. This work is ﬁnanced by the ERDF - European Regional
Development Fund through the Operational Programme for Competitiveness and
Internationalisation - COMPETE 2020 Programme within project POCI-01-0145-
FEDER-006961, and by National Funds through the Portuguese funding agency, FCT -
Fundao para a Ciˆencia e a Tecnologia as part of project UID/EEA/50014/2013.
References
1. Csurka, G., Dance, C.R., Fan, L., Willamowski, J., Bray, C.: Visual categorization
with bags of keypoints. In: Workshop on Statistical Learning in Computer Vision
(2004)
2. Jang, D.M., Turk, M.: Car-Rec: a real time car recognition system. In: IEEE Work-
shop on Applications of Computer Vision (2011)
3. Thomas, A., Ferrar, V., Leibe, B., Tuytelaars, T., Schiel, B., Van Gool, L.: Towards
multi-view object class detection. In: IEEE Conference on Computer Vision and
Pattern Recognition (2006)
4. Lampert, C.H., Blaschko, M.B., Hofmann, T.: Beyond sliding windows: object
localization by eﬃcient subwindow search. In: IEEE Conference on Computer
Vision and Pattern Recognition (2008)
5. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Int. J. Com-
put. Vis. (2004)
6. Ponce, J., Lazebnik, S., Rothganger, F., Schmid, C.: Toward true 3D object recog-
nition. In: Congres de Reconnaissance des Formes et Intelligence Artiﬁcielle (2004)
7. Zheng, W., Liang, L.: Fast car detection using image strip features. In: IEEE
Conference on Computer Vision and Pattern Recognition (2009)
8. Ger´onimo, D., Sappa, A.D., L´opez, A., Ponsa, D.: Adaptive image sampling and
windows classiﬁcation for on-board pedestrian detection. In: International Confer-
ence on Computer Vision Systems (2007)
9. Besl, P.J., McKay, N.D.: A method for registration of 3-D shapes. IEEE Trans.
Pattern Anal. Mach. Intell. 14(2), 239–256 (1992)
10. Sivic, J., Zisserman, A.: Video Google: a text retrieval approach to object matching
in videos. In: IEEE International Conference on Computer Vision (2003)

Tactile Sensing and Machine Learning
for Human and Object Recognition
in Disaster Scenarios
Juan M. Gandarias(B), Jes´us M. G´omez-de-Gabriel,
and Alfonso J. Garc´ıa-Cerezo
System Engineering and Automation Department, University of M´alaga,
29071 M´alaga, Spain
{jmgandarias,jesus.gomez,ajgarcia}@uma.es
https://www.uma.es/robotics-and-mechatronics/
Abstract. This paper presents the application of machine learning to
tactile sensing for rescue robotics. Disaster situations often exhibit low-
visibility scenarios where haptic feedback provides a valuable information
for the search of potential victims. To extract haptic information from
the environment, a tactile sensor attached to a lightweight robotic arm
is used. Then, methods based on the SURF descriptor, support vector
machines (SVM), Deep Convolutional Neural Networks (DCNN) and
transfer learning are implemented to classify the data. Besides, exper-
iments have been carried out, to compare those procedures, using dif-
ferent contact elements, such as human parts and objects that could be
found in catastrophe scenarios. The best achieved accuracy of 92.22%,
results from the application of the transfer learning procedure using a
pre-trained DCNN and ﬁne-tuning the classiﬁcation layer of the network.
Keywords: Tactile sensors · Rescue robotics · Machine learning · Deep
learning · Transfer learning · Object recognition
1
Introduction
Teleoperation is still a critical element in rescue robotics due to the complexity
of the operations in an unstructured environment [15]. Previous experiences in
real situations have evidenced the problematic of using systems provided with
visual perception only. In low-light scenarios, or in presence of dust or smoke,
systems with haptic feedback contribute with additional information that can
compensate the lack of visual information [24].
A key task in rescue robotics operations with large number of victims, is to
locate and evaluate the urgency degrees of the victims, in function of the priority
of the treatment (triage). This task raise technological problems such as Human-
Robot Interaction (HRI), which is considered one of the biggest challenges in
this ﬁeld [20]. A ﬁrst approach to achieve a solution would be the identiﬁcation
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_14

166
J.M. Gandarias et al.
of the victims and the diﬀerent parts of the body, prior to the measurement of
the vital signs.
Identifying potential victims in disaster scenarios is a priority for search and
rescue teams, but it still an research problem. In [26], typical problems of locating
and tracking humans with video segmentations are presented. Also, a proposal
to enhance the visual tracking by fusing both color and thermal-infrared spec-
tra is described. Another approach [6], is based on the abrupt changes suﬀered
by the UWB signals characteristics when passing through a human body, to
simultaneously detect human beings and locate the rescue robot using UWB
signals.
Haptic feedback can beneﬁt both robotic guidance and victims manipulation.
In robotic guidance, haptics systems enhance the control of the teleoperator,
who can drive the vehicle with force feedback based on potential ﬁelds. This
perception reduces the visual-information dependence, driving the operator to
the target [1], avoiding collisions, or combining both functions [2]. Other applica-
tion of haptic technology to rescue robots are based on the use of a guide vehicle
in low-visibility situations [24]. The University of M´alaga has contributed previ-
ously to the application of tactile sensors to rescue-robots [28], where a tactile
sensor was developed to provide a pressure map of the external applied forces,
and this sensor was attached to an end-hydraulic-eﬀector of a rescue robot [7].
Diﬀerent tactile sensing implementations can’t be easily compared, due to
the diﬀerences of the hardware [21]. One approach consist on using tactile data
to recognize objects by their shape [14]. On the other hand, interpreting tactile
data as time series was also investigated [13,19]. The majority of these works
are based on artiﬁcial intelligence algorithms to classify the data. One propo-
sition consists on employing computer vision algorithms and machine learning
techniques [16,17], whilst other employs neural networks [9,21] and deep learn-
ing. For instance, [25] presents the use of Deep Learning with Dropout to reduce
overﬁtting, and the beneﬁts of including both kinesthetic and tactile information
to object shape recognition. [18] also present the beneﬁts of using both kines-
thetic and tactile information to recognize objects. Other approach of using
Deep Learning techniques and artiﬁcial tactile sensing consists on determining
the contact material [3].
This work proposes the use of tactile sensors in emergency situations, where
searching and rescuing potential victims are a priority. A system composed by a
lightweight robotic manipulator and a high-resolution tactile sensor is employed
to recognize objects. Moreover, machine learning methods for pressure image
recognition are presented. These methods are based on two steps: extracting
features from the pressure-tactile images, and classifying those images into pre-
deﬁned labels. Due to the shortage of features of the tactile images, using
deep convolutional neural networks (DCNN) [11] has been considered. First,
a method based on the Speeded-Up Robust Features (SURF) [4] and a support
vector machine (SVM) [5] (SURF-SVM) is implemented as a checkpoint for the
posterior comparison with the DCNN-based methods. One proposal consists on
using the features-extraction layers of a pre-trained DCNN as a feature extractor

Tactile Sensing in Disaster Scenarios
167
and a SVM to get a classiﬁer (DCNN-SVM method). Besides, a transfer learn-
ing procedure with a pre-trained DCNN is also tested. Finally, an experiment
with 9 classes is carried out to evaluate and compare the methods in terms of
accuracy rate. These experiments include objects that can easily encountered in
catastrophes situations and parts of human arms.
The rest of the document is structured as follows. In the Sect. 2, methods
characteristics and schemes are described. Thereafter, Sect. 3 shows the experi-
mental procedure, the methodology implementation considerations, and the data
and results obtained. Finally, the conclusions and future work are detailed in
Sect. 5.
2
Methods
2.1
SURF-SVM
SURF-SVM method applies the SURF descriptor as a feature extractor from
pressure images, and a SVM as a classiﬁer. Figure 1 presents the procedure of
the SURF-SVM method implementation. SURF brings out the pressure-tactile
images descriptors, which are then clustering in a bag of words framework (BoW)
[22] by a k-means algorithm [12], forming a dictionary. Finally, a supervised SVM
is trained using the dictionary generated and the known images labels.
Fig. 1. SURF-SVM scheme.
2.2
DCNN-SVM
This method is similar to the previous one. Nevertheless, the DCNN-SVM
replaces the SURF descriptor by the feature extraction part of a DCNN in
order to reinforce the lack of information presented in pressure images. This
DCNN has been previously pre-trained to classify visual images taken with a
normal camera. Hence, we can divide the DCNN architecture in two parts: fea-
tures extraction, and classiﬁcation. Figure 2 shows the DCNN-based algorithm,
which uses the activations of the last feature extraction layer of the DCNN and
a subsequent SVM, trained in base on these activations and the known labels.
The activations used for training the SVM represent the descriptive information
of the images.

168
J.M. Gandarias et al.
Fig. 2. DCNN-SVM scheme.
2.3
Transfer Learning
Transfer learning takes uses a Neural Network trained in an speciﬁc domain, with
big amount of data, using this network for another purpose in which that volume
of data is not available [23]. In our case, we have employed a DCNN that has
been previously trained for image classiﬁcation. As mentioned before, the last
section of the DCNN incorporates the classiﬁcation procedure. The classiﬁcation
section has been trained to classify pressure images. The scheme of this method
is showed in Fig. 3.
Fig. 3. Transfer learning scheme.
3
Experimental Setup
To carry out the experiments, the tactile sensor, model 60077 from Tekscan, has
been attached to the lightweight robotic manipulator AUBO OUR-i5. Figure 4
shows the experimental setup. The high-resolution resistive tactile-array has a
total of 1400 pressure sensels. Each sensel size is 53.3 mm × 95.3 mm and the
sensor presents a density of 27.6 sensels/cm2 distributed in a matrix composed

Tactile Sensing in Disaster Scenarios
169
Fig. 4. Tactile sensor attached to the AUBO OURi5 robotic manipulator.
by 28 rows and 50 columns. Further, a 2 mm silicone rubber cover has been
added to receive external forces and protect the sensor ﬁlm.
In order to evaluate the methods, a collection of 450 pressure images from
diﬀerent elements has been obtained. The data set consists of the raw pressure
data given by the tactile sensor in the form of pressure images. These images are
picked up from human body parts and inert objects, forming a set of 9 classes:
Fingers, Forearm, Hand, Cable Pipe, Rocks, Rubble, Timber Wood, Branch
Wood and Piece of Wood.
Figure 5 shows an example of the body parts and their corresponding pressure
maps. On the other hand, examples of each inert-object class and their respective
pressure maps are shown in Fig. 6. Those parts of the body are chosen to keep
the comfort of a suppose victim, and the ease of access and the manipulability of
the robot. Likewise, the inert objects selected are elements that could be found
in a disaster scenario.
The learning procedure, also called training, of the proposed methods
employs a subset of 180 images of the collected data. After training the methods,
a test phase is performed to validate the results, which employs a subset of 270
images.
For the DCNN-SVM method, a large variety of neural networks can be used.
To show the diﬀerences between them, three DCNNs have been implemented for
the application of this method: AlexNet [10], VGG-16 and VGG-19 [27], named
as ANET-SVM, VGG16-SVM and VGG19-SVM, respectively. The Neural Net-
works have been obtained from the Caﬀe repository [8].

170
J.M. Gandarias et al.
(a)
(b)
(c)
0
50
25
100
20
10
Pressure (0-255)
150
Rows (1-28)
15
20
Fingers
Columns (1-50)
200
10
30
250
5
40
50
(d)
0
50
25
100
20
Pressure (0-255)
150
Rows (1-28)
15
10
200
Forearm
10
Columns (1-50)
20
250
30
5
40
50
(e)
0
25
50
10
20
100
Rows (1-28)
20
Pressure (0-255)
15
150
Columns (1-50)
Palm Hand
30
10
200
40
5
250
50
(f)
Fig. 5. Parts of the body used for the experiments, labeled as Hand (a), Forearm (b)
and Fingers (c) and their corresponding pressure map (d), (e), (f).
Additionally, for the transfer learning application, the DCNN AlexNet has
been implemented. That conﬁguration has been named as TL-ANET. In this
method, only the classiﬁcation layer has been re-trained using a single GTX
1050Ti GPU with 4GB of memory.
4
Results
Figure 7 shows the resulting confusion matrices of the methods, whilst Table 1
presents the summary of the results. With the application of the SURF-SVM
method, a recognition rate of 70.74% has been achieved. An improvement of the
accuracy, produced by the application of Deep Learning techniques, in extract-
ing features from pressure images has been detected in contrast with the SURF-
based method. In particular, DCNN-SVM methods (ANET-SVM, VGG16-SVM
and VGG19-SVM) shows an improvement between 13.7% and 17.04%. Further-
more, a ﬁne-tune of the classiﬁcation layer of the DCNN (TL-ANET) implies an
improvement of 21.48% against the use of an SVM (SURF-SVM) to classify the
data.

Tactile Sensing in Disaster Scenarios
171
(a)
(b)
(c)
10
20
0
Columns (1-50)
50
25
Cable Pipe
30
20
100
Rows (1-28)
15
Pressure (0-255)
40
150
10
200
5
50
250
(d)
0
25
50
10
20
100
Rows (1-28)
20
15
Pressure (0-255)
Columns (1-50)
150
Rock
30
10
200
40
5
250
50
(e)
0
10
50
25
20
20
100
Columns (1-50)
Rubble
Rows (1-28)
15
Pressure (0-255)
30
150
10
200
40
5
250
50
(f)
(g)
(h)
(i)
0
50
25
20
100
Rows (1-28)
Pressure (0-255)
15
150
10
10
200
Columns (1-50)
Wood Timber
20
30
5
250
40
50
(j)
0
50
25
100
20
Pressure (0-255)
150
15
Rows (1-28)
200
10
Wood Branch
10
250
Columns (1-50)
5
20
30
40
50
(k)
0
50
25
100
150
20
Pressure (0-255)
200
15
Rows (1-28)
250
10
Piece of Wood
5
Columns (1-50)
5
10
15
20
25
30
35
40
45
50
(l)
Fig. 6. Inert objects used for the experiments, labeled as Cable pipe (a), Rocks (b),
Rubble (c), Timber Wood (g), Branch Wood (h) and Piece of Wood (i), and their
corresponding pressure maps (d), (e), (f), (j), (k), (l).

172
J.M. Gandarias et al.
0.47
0.00
0.00
0.00
0.17
0.00
0.00
0.10
0.13
0.00
0.87
0.00
0.03
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.87
0.03
0.00
0.00
0.03
0.07
0.00
0.00
0.00
0.13
0.90
0.00
0.00
0.00
0.03
0.00
0.17
0.00
0.00
0.00
0.40
0.00
0.00
0.07
0.30
0.00
0.00
0.00
0.00
0.07
0.97
0.20
0.00
0.03
0.13
0.07
0.00
0.03
0.07
0.03
0.77
0.00
0.03
0.13
0.07
0.00
0.00
0.17
0.00
0.00
0.70
0.04
0.10
0.00
0.00
0.00
0.13
0.00
0.00
0.03
0.43
Arm
Fingers Hand
Pipe
Wood Timber Branch
Rock
Rubble
Arm
Fingers
Hand
Pipe
Wood
Timber
Branch
Rock
Rubble
(a)
0.95
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.05
0.00
1.00
0.05
0.00
0.00
0.00
0.30
0.00
0.00
0.00
0.00
0.95
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.75
0.05
0.00
0.05
0.50
0.00
0.00
0.00
0.00
0.00
0.80
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.15
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.10
0.00
0.00
0.65
0.00
0.00
0.00
0.00
0.00
0.15
0.00
0.00
0.00
0.50
Arm
Fingers Hand
Pipe
Wood Timber Branch
Rock
Rubble
Arm
Fingers
Hand
Pipe
Wood
Timber
Branch
Rock
Rubble
(b)
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.95
0.00
0.00
0.00
0.00
0.00
0.05
0.00
0.00
0.00
0.95
0.05
0.00
0.00
0.00
0.05
0.00
0.00
0.00
0.05
0.95
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.25
0.00
0.00
0.10
0.10
0.00
0.00
0.00
0.00
0.00
0.95
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.05
1.00
0.00
0.00
0.00
0.05
0.00
0.00
0.20
0.00
0.00
0.80
0.00
0.00
0.00
0.00
0.00
0.55
0.00
0.00
0.00
0.90
Arm
Fingers Hand
Pipe
Wood Timber Branch
Rock
Rubble
Arm
Fingers
Hand
Pipe
Wood
Timber
Branch
Rock
Rubble
(c)
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.95
0.00
0.05
0.00
0.00
0.00
0.15
0.00
0.00
0.00
1.00
0.05
0.00
0.00
0.00
0.05
0.00
0.00
0.00
0.00
0.90
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.35
0.00
0.00
0.05
0.05
0.00
0.00
0.00
0.00
0.10
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1.00
0.00
0.00
0.00
0.05
0.00
0.00
0.20
0.00
0.00
0.75
0.00
0.00
0.00
0.00
0.00
0.35
0.00
0.00
0.00
0.95
Arm
Fingers Hand
Pipe
Wood Timber Branch
Rock
Rubble
Arm
Fingers
Hand
Pipe
Wood
Timber
Branch
Rock
Rubble
(d)
0.95
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.05
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1.00
0.00
0.00
0.00
0.00
0.05
0.00
0.00
0.00
0.00
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.75
0.00
0.00
0.00
0.30
0.00
0.00
0.00
0.00
0.05
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.05
0.00
0.00
0.90
0.00
0.00
0.00
0.00
0.00
0.15
0.00
0.00
0.05
0.70
Arm
Fingers Hand
Pipe
Wood Timber Branch
Rock
Rubble
Arm
Fingers
Hand
Pipe
Wood
Timber
Branch
Rock
Rubble
(e)
Fig. 7. Confusion matrix resulting of applying the methods: SURF-SVM (a), ANET-
SVM (b), VGG16-SVM (c), VGG19-SVM (d), TL-ANET (e).
Table 1. Summary of results
Method
Accuracy [%]
Improvement [%]
SURF-SVM
70.74
-
ANET-SVM
84.44
13.7
VGG16-SVM
86.11
15.37
VGG19-SVM
87.78
17.04
TL-ANET
92.22
21.48

Tactile Sensing in Disaster Scenarios
173
5
Conclusions
An application to the detection of victims in disaster scenarios, based on tac-
tile sensing, has been presented. A high-resolution tactile sensor provides raw
pressure data distribution. Due to the lack of information brought by the pres-
sure images, deep learning techniques have been considered and compared with
respect to a non-deep-learning algorithm (SURF-SVM), which implements the
SURF descriptor as a feature extractor and a SVM to get a classiﬁer. Further,
three pre-trained DCNN have been employed as feature extractors, exchanging
the last classiﬁcation layer by a SVM (DCNN-SVM). Additionally, a transfer
learning procedure to re-train the last layer of the AlexNet DCNN (TL-ANET)
in a single GPU is also developed. Besides, the lightweight robotic manipulator
AUBO OUR-i5 provided with a tactile sensor has been used to capture the pres-
sure images for carrying out a 9-classes experiment. The 9-classes are distributed
into 3-classes for human body parts (forearm, hand and ﬁngers) and 6-classes for
inert objects that could be found in a catastrophe scenario (cable pipe, rubble,
rocks, timber, branch wood and a small piece of wood). Results of the exper-
iments reveal an accuracy improvement between 13.7% and 17.04% using the
DCNN-SVM method with respect to the 70.74% accuracy of the SURF-SVM,
and an improvement of 21.48% achieved with the (TL-ANET), showing that a
recognition rate of 92.22% for distinguishing human from objects can be achieved
using transfer learning. In future work, time-series data combined with active
touch and palpation strategies will be considered. Also, other information such
as the relation between the applied forces and the displacement of the sensor
with respect to the contact with an element will be used.
Acknowledgment. This work was partially supported by the Spanish project
DPI2015-65186-R and the European Commission under grant agreement BES-2016-
078237.
References
1. Abdelrahman, W., Wei, L., Mullins, J., Nahavandi, S.: Wireless haptic rendering for
mobile platforms. In: Conference Proceedings - IEEE International Conference on
Systems, Man and Cybernetics, pp. 2213–2217. IEEE (2012). doi:10.1109/ICSMC.
2012.6378069
2. Ang, Q.Z., Horan, B., Nahavandi, S.: Multipoint haptic mediator interface for
robotic teleoperation. IEEE Syst. J. 9(1), 86–97 (2015). doi:10.1109/JSYST.2013.
2283955
3. Baishya, S.S., Bauml, B.: Robust material classiﬁcation with a tactile skin using
deep learning. In: 2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pp. 8–15. IEEE (2016). doi:10.1109/IROS.2016.7758088
4. Bay, H., Ess, A., Tuytelaars, T., Van Gool, L.: Speeded-Up Robust Features
(SURF). Comput. Vis. Image Underst. 110(3), 346–359 (2008). doi:10.1016/j.cviu.
2007.09.014
5. Cortes, C., Vapnik, V.: Support-vector networks. Mach. Learn. 20(3), 273–297
(1995). doi:10.1007/BF00994018

174
J.M. Gandarias et al.
6. Espes, D., Pistea, A.M., Canaﬀ, C., Iordache, I., Parc, P.L., Radoi, E.: New method
for localization and human being detection using UWB technology: helpful solution
for rescue robots. arXiv preprint arXiv:1312.4162 (2013)
7. Garc´ıa-Cerezo, A., Mandow, A., Mart´ınez, J.L., G´omez-de Gabriel, J., Morales,
J., Cruz, A., Reina, A., Ser´on, J.: Development of ALACRANE: a mobile robotic
assistance for exploration and rescue missions. In: SSRR2007 - IEEE International
Workshop on Safety, Security and Rescue Robotics Proceedings, pp. 1–6. IEEE
(2007). doi:10.1109/SSRR.2007.4381269
8. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.,
Guadarrama, S., Darrell, T.: Caﬀe. In: Proceedings of the ACM International
Conference on Multimedia - MM 2014, vol. 1436, pp. 675–678 (2014). doi:10.1145/
2647868.2654889
9. Khasnobish, A., Jati, A., Singh, G., Bhattacharyya, S., Konar, A., Tibarewala, D.,
Kim, E., Nagar, A.K.: Object-shape recognition from tactile images using a feed-
forward neural network. In: The 2012 International Joint Conference on Neural
Networks (IJCNN), pp. 1–8. IEEE (2012). doi:10.1109/IJCNN.2012.6252593
10. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in neural information processing systems,
pp. 1097–1105 (2012)
11. LeCun, Y., Bengio, Y., et al.: Convolutional networks for images, speech, and time
series. Handb. Brain Theor. Neural Networks 3361(10), 1995 (1995)
12. Likas, A., Vlassis, N., Verbeek, J.J.: The global k-means clustering algorithm.
Pattern Recogn. 36(2), 451–461 (2003)
13. Liu, H., Guo, D., Sun, F.: Object recognition using tactile measurements: ker-
nel sparse coding methods. IEEE Trans. Instrum. Measur. 65(3), 656–665 (2016).
doi:10.1109/TIM.2016.2514779
14. Liu, H., Song, X., Nanayakkara, T., Seneviratne, L.D., Althoefer, K.: A compu-
tationally fast algorithm for local contact shape and pose classiﬁcation using a
tactile array sensor. In: Proceedings - IEEE International Conference on Robotics
and Automation, pp. 1410–1415. IEEE (2012). doi:10.1109/ICRA.2012.6224872
15. Liu, Y., Nejat, G.: Robotic urban search and rescue: a survey from the con-
trol perspective. J. Intell. Robot. Syst. 72(2), 147–165 (2013). doi:10.1007/
s10846-013-9822-x
16. Luo, S., Liu, X., Althoefer, K., Liu, H.: Tactile object recognition with semi-
supervised learning. In: Lecture Notes in Computer Science (including subseries
Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), vol.
9245, pp. 15–26. Springer, Cham (2015). doi:10.1007/978-3-319-22876-1 2
17. Luo, S., Mou, W., Althoefer, K., Liu, H.: Novel Tactile-SIFT descriptor for object
shape recognition. IEEE Sens. J. 15(9), 5001–5009 (2015). doi:10.1109/JSEN.2015.
2432127
18. Luo, S., Mou, W., Althoefer, K., Liu, H.: Iterative closest labeled point for tactile
object shape recognition. In: IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS). IEEE (2016). doi:10.1109/IROS.2016.7759485
19. Madry, M., Bo, L., Kragic, D., Fox, D.: ST-HMP: Unsupervised Spatio-Temporal
feature learning for tactile data. In: 2014 IEEE International Conference on Robot-
ics and Automation (ICRA), pp. 2262–2269. IEEE (2014). doi:10.1109/ICRA.2014.
6907172
20. Murphy, R.R., Tadokoro, S., Nardi, D., Jacoﬀ, A., Fiorini, P., Choset, H., Erkmen,
A.M.: Search and rescue robotics. In: Springer Handbook of Robotics, pp. 1151–
1173. Springer (2008). doi:10.1007/978-3-540-30301-5 51

Tactile Sensing in Disaster Scenarios
175
21. Navarro, S.E., Gorges, N., W¨orn, H., Schill, J., Asfour, T., Dillmann, R.: Haptic
object recognition for multi-ﬁngered robot hands. In: Haptics Symposium 2012,
HAPTICS 2012 - Proceedings, pp. 497–502. IEEE (2012). doi:10.1109/HAPTIC.
2012.6183837
22. Nowak, E., Jurie, F., Triggs, B.: Sampling strategies for bag-of-features. Eccv 3954,
490–503 (2006). doi:10.1007/11744085 38
23. Pan, S.J., Yang, Q.: A Survey on Transfer Learning (2010). doi:10.1109/TKDE.
2009.191
24. Ranasinghe,
A.,
Sornkarn,
N.,
Dasgupta,
P.,
Althoefer,
K.,
Penders,
J.,
Nanayakkara, T.: Salient feature of haptic-based guidance of people in low visi-
bility environments using hard reins. IEEE Trans. Cybern. 46(2), 568–579 (2016).
doi:10.1109/TCYB.2015.2409772
25. Schmitz, A., Bansho, Y., Noda, K., Iwata, H., Ogata, T., Sugano, S.: Tactile
object recognition using deep learning and dropout. In: 2014 IEEE-RAS Inter-
national Conference on Humanoid Robots, pp. 1044–1050 (2014). doi:10.1109/
HUMANOIDS.2014.7041493
26. Serrano-Cuerda, J., Lopez, M.T., Fern´andez-Caballero, A.: Robust human detec-
tion and tracking in intelligent environments by information fusion of color and
infrared video. In: Proceedings - 2011 7th International Conference on Intelligent
Environments, IE 2011, pp. 354–357. IEEE (2011). doi:10.1109/IE.2011.21
27. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)
28. Vidal-Verd´u, F., Barquero, M.J., Castellanos-Ramos, J., Navas-Gonz´alez, R.,
S´anchez, J.A., Ser´on, J., Garc´ıa-Cerezo, A.: A large area tactile sensor patch
based on commercial force sensors. Sensors 11(5), 5489–5507 (2011). doi:10.3390/
s110505489

Robots Cooperating with Sensor
Networks

Autonomous Localization of Missing Items
with Aerial Robots in an Aircraft Factory
Julio L. Paneque1(B), Arturo Torres-Gonz´alez1, J. Ramiro Mart´ınez-de Dios1,
Juan Ram´on Astorga Ram´ırez2, and Anibal Ollero1
1 Robotics, Vision and Control Group, University of Seville, Seville, Spain
{jlpaneque,arturotorres,jdedios,aollero}@us.es
2 Airbus Defence and Space, Centro Bah´ıa de C´adiz, C´adiz, Spain
juan.r.astorga@military.airbus.com
Abstract. Missing tools is a problem in aircraft factories. It may reduce
the productivity of the assembly line and missing items may cause FOD
(Foreign Object Damage) if they are lost inside the aerostructure. This
paper proposes a method which uses aerial robots to search and locate
missing tools. Each tool will be equipped with a radio tag with an ID that
can listen and respond to request messages from the aerial robot. Thus,
the robot can take range measurements to the missing tools from diﬀerent
locations while performing other tasks in the factory. The range measure-
ments are used to estimate the location of every missing tool using a Par-
ticle Filter (PF) which will eventually converge to an Extended Kalman
Filter (EKF). The proposed method was evaluated and validated in real
experiments performed in an emulated scenario very similar to the real
factory. Preliminary tests were also performed in the Airbus DS CBC
factory with good results.
Keywords: Aerial robots · Range-only localization · Sensor networks ·
Aeronautic industry
1
Introduction
Assembly of aerostructure parts requires many diﬀerent tools. For instance, more
than 100 diﬀerent tools are necessary for the assembly of every single fan cowl.
These tools are very speciﬁc. Missing tools is a problem for two reasons: (1) it
may reduce the productivity of the assembly line and (2) missing items may
cause FOD (Foreign Object Damage) if they are lost inside the aerostructure,
originating a severe problem.
While the methods presented in this paper can be used in any factory, they
are motivated by one application. The target industry is the Airbus DS CBC
(“Centro Bah´ıa de C´adiz”) manufacturing plant, in the south of Spain. This
factory is specialized in manufacturing fan cowls.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_15

180
J.L. Paneque et al.
Nowadays, modern factories are equipped with intelligent tool closets,
which detect when a speciﬁc tool is missing at the end of the working shift.
When a tool is considered missing, it is searched in the factory requiring signif-
icant human labor.
This problem could be solved autonomously with diﬀerent approaches. For
instance, a system of ﬁxed sensor nodes could be installed in the factory in order
to continuously localize all the missing tools; however, this would require an
ad hoc installation on each factory, and it would not be suitable for dynamic
factories, where the structure and conditions of the factory may change over
time. Other approach could consist on installing the sensor nodes on each of
the worker’s clothes (the helmet would perform properly in this task due to
the height where it is located); however, this would compromise the safety and
comfort of the workers, and so is not a feasible approach for this problem.
The proposal is to use aerial robots to search and locate missing tools. Each
tool will be equipped with a radio tag with an ID that can listen to and respond
to request messages. The aerial robot will be equipped with a radio emitter
that can send requests and receive responses from missing tools. These messages
can be range measurements, e.g. based on RSS (received signal strength) or
TOF (time of ﬂight). The range measurements are used to estimate the location
of every missing tool using a combination of Particle Filter (PF) and Extended
Kalman Filter (EKF) [3,9]. The idea is to estimate the location of each tool with
the EKF, but having only range measurements makes it unfeasible to initialize
an EKF. Thus, we will use a PF which will eventually converge to an EKF. The
objective is to estimate their location with enough accuracy so that it will be
easy for a worker to ﬁnd the tool.
The operation is as follows. When a tool is considered missing, its ID is
inserted into the list of the tools that must be searched. During the next regular
ﬂights, e.g. for performing logistic operations, the aerial robot transmits request
messages with the IDs of the list of missing tools. When the radio tag of a
missing tool receives a request with its ID, it responds. The aerial robot receives
the responses and estimates the location of the mission tool in the factory map.
The system can search simultaneously as many missing tools as necessary. It
is also possible to perform missing tool search ﬂights on demand, which are
interesting for instance to urgently locate them. If at the end of a working shift
a tool is considered missing, the IDs of the missing tools are inserted in the list
and a search ﬂight is performed on-demand.
Besides the functionalities for autonomous navigation, the aerial robot will
implement the communication protocol with missing tools and functionalities
for estimating their location. Of course, performance in everyday operational
conditions in the factory imposes strong requirements in our development. The
robot will be able to perform in safe and robust conditions in everyday operation
in the factory.
This paper is structured as follows. Section 2 summarizes some related work
on localization with radio range sensors. Section 3 describes the proposed algo-
rithms to ﬁnd and locate the missing tools. Results performed in real experiments
are in Sect. 4. Preliminary tests performed in the Airbus DS CBC factory are
presented in Sect. 5. The conclusions are in the last section.

Autonomous Localization of Missing Items with Aerial Robots
181
2
Related Work
Most localization methods with radio tags use the signal strength (RSSI -
Received Signal Strength Indicator) as a range measurement. However, reﬂec-
tions and other interactions with the environment, such as multi-path propaga-
tion, make RSSI measurements very dependent on the scenario. Other technolo-
gies can be used to estimate the distance between two radio tags, such as time
of ﬂight (TOF). TOF sensors provide more accuracy than RSSI.
Localization with range-only measurements has the problem of partial
observability. The problem is that only one measurement is insuﬃcient to con-
strain the target location. Thus, the robot has to move and integrate measure-
ments from diﬀerent positions in order to initialize the locations of the targets.
Diﬀerent approaches to solve this problem are detailed below.
Multilateration [10,11] is one of the simplest methods to determine the target
position. The robot gathers measurements of the target from diﬀerent locations,
and then tries to estimate its pose through a simple least squares optimization.
Least squares is needed in order to deal with the noise of the measurements. Mul-
tilateration is computationally eﬃcient, but lacks robustness. It is very sensible
to measurement noise and outliers, and it can cause bad initialization, which can
lead to signiﬁcant estimation errors.
WCL (Weighted Centroid Localization) exhibits high robustness against
noise in the RSSI measurements. [1] demonstrated that although other methods
like, for example Least Squares (LS), are optimal with noiseless RSSI measure-
ments, the performance of WCL is better with realistic (non-Gaussian) levels of
noise.
The Maximum Likelihood (ML) [4] localization technique is based on classi-
cal statistical inference theory. Given a vector of measurements that the robot
gathered from n locations, the ML algorithm computes the a priori probability of
receiving a certain range measurement for each potential position of the target.
The position that maximizes this probability is then selected as the estimated
target location. This algorithm could be complex and sensible to outliers.
Probability grids [2,7] provide more robustness than multilateration but are
not scalable to large scenarios and their accuracy is related to the size of the
cells in the grid. The idea behind probability grids is the discretization of the
physical world into a 2D (or 3D) grid, with each grid cell corresponding to a
rectangular (or cube) area. Each measurement “votes” for its possible solutions.
Ideally, solutions that are near each other should end up in the same cell, even
in the presence of noise. This can be solved by choosing a grid size that matches
the total uncertainty. Once that all votes have been added to the accumulator,
one can search for the cell with the greatest number of votes. In order to ﬁnd
a solution, a vote ratio must be deﬁned. Higher ratios increase conﬁdence but
they can delay making a decision.
Particle Filters (PFs) [6,8] can represent any probability distribution, see
Sect. 2.1. In this case the probability distribution has the shape of a ring (in
2D). As more range measurements are integrated the particles tends to converge
to the real distribution. PFs can be computationally hard, but they can provide

182
J.L. Paneque et al.
a pure probabilistic solution to the partial observability problem. The accuracy
is related to the number of particles, but of course having more particles also
increases the resource consumption.
Gaussian mixtures [5] can represent any distribution as a linear combination
of Gaussian distributions. They provide a multi-hypothesis scheme in which only
one of the Gaussians will be the chosen one and the other will be discarded.
2.1
Particle Filter in a Nutshell
The key idea of PFs is to represent the belief bel(xt) by a set of random state
samples. Instead of representing the distribution by a parametric form, PFs
represent a distribution by a set of samples drawn from this distribution.
In PFs the samples of a posterior distribution are called particles and each
one is a hypothesis as to what the true world state may be at a concrete time.
Xt := x[1]
t , x[2]
t , . . . , x[M]
t
(1)
Each particle x[m]
t
(with 1 ≤m ≤M) is a concrete instantiation of the state
at time t, that is, a hypothesis of the real state at time t. Here M denotes the
number of particles in the particle set Xt, often a large number. PFs approximate
the belief bel(xt) by the set of particles Xt. Ideally, the likelihood for a state
hypothesis xt to be included in the particle set Xt shall be proportional to its
Bayes Filter posterior bel(xt):
x[m]
t
∼p(xt|z1:t, u1:t)
(2)
As a consequence of (2), the denser a subregion of the state space is populated
by particles, the more likely is that the true state falls into this region. This
property holds only asymptotically for M ↑∞for the standard Particle Filter
algorithm. For ﬁnite M, particles are drawn from a slightly diﬀerent distribution.
In practice, this diﬀerence is negligible as long as the number of particles is not
too small.
The PF algorithm maintains the belief bel(xt) recursively from the belief
bel(xt−1) one time step earlier. Since beliefs are represented by sets of particles,
this means that PFs construct the particle set Xt recursively from the set Xt−1.
3
PF-EKF Range-Only Localization
The only information available is the current location of the aerial robot and the
range measurements between itself and the radio-tagged tools. The proposal is
to use Particle Filters (PFs) to estimate an initial location of the tool and then
reﬁne this estimation with an EKF. The PFs solve the initial multi-hypothesis
problem inherent to range-only measurements while the EKFs improve the ﬁnal
estimation.

Autonomous Localization of Missing Items with Aerial Robots
183
The proposed method holds a PF-EKF for every missing tool. Assuming that
a high number of tools (N) can be missing, it is more eﬃcient to have N simpler
EKFs than the same number of more complex PFs. Thus, the convergence of
the PFs should be as quick as possible. It has to be considered that the aerial
robot could be simultaneously performing other tasks, e.g. logistic operations.
When the aerial robot receives the ﬁrst range measurement zi from tool i,
it initializes a PF, PFi, in which the particles, which are hypotheses of the tool
location, are spread around the aerial robot at distances drawn from an annu-
lar distribution with mean zi and a width that depends on the measurements
variance.
From this moment, every time the aerial robot receives a new measurement
from tool i, the particles of PFi are updated making them to condensate towards
the tool’s real position. This step estimates the importance factor or weight of
each particle. This weight is the probability of a certain particle to represent the
real state. Thus, higher weights represent more probable samples.
The likelihood function used for estimating the importance factor is:
p[j](x) =
1
σ
√
2π e
(|x[j]−xr|−zi)2
2σ2
,
(3)
where p[j](x) is the probability of particle j of representing the true state of x.
x[j] is the position of particle j. xr is the aerial robot location. zi is the current
measurement of tool i. σ is the variance of the measurement noise.
From time to time, PFs also implement the so-called resampling, which draws
with replacement of M particles. The probability of drawing each particle is given
by its importance factor. This step is the key of PF as it helps to have a faster
convergence of the PF, removing particles with low probabilities and duplicating
those with higher probabilities. In our application we replaced M = 10% of the
particles.
We considered that a PF has converged in a Gaussian distribution when the
maximum eigenvalue of the covariance matrix is lower than a certain threshold.
The eigenvalues of the covariance matrix are directly proportional to the vari-
ance along the corresponding eigenvectors. Thus, the maximum eigenvalue is a
measure of the volume (or at least the largest axis) of the conﬁdence ellipsoid of
the distribution. Using the eigenvalues puts the convergence condition on each
axis.
When PFi converges, the tool estimated location xi is computed as the
weighted mean of all particles, see Eq. (4), and it is used to initialize an EKF,
EKFi. The ﬂexibility of PFs is at the expense of increasing computer burden.
PF iterations require signiﬁcantly more computer burden than EKF iterations.
When the particles converge to a Gaussian-like distribution, the estimation of
the missing tool i will be continuously updated with EKFi.
xi =
M

j=1
ω[j]
i x[j]
i
(4)

184
J.L. Paneque et al.
Missing tools are supposed to be static. Thus, the EKF motion model will
be f(x) = xt−1. On the other hand, the observation model will be the euclidean
distance between the aerial robot and the missing tool:
h(x) = |xi −xr|
(5)
EKF will reﬁne the estimation obtained with the PF and consuming less
computation resources.
4
Experiments
A set of experiments were performed in the ETHZ1 and CATEC2 testbeds in
the scope of the EuRoC3 project (European Robotics Challenge). The aerial
platform used in these experiments was the AscTec Neo, an hex-rotor helicopter
with 9” blades. Figure 1-left shows the ﬁrst conﬁguration used in ETHZ prelim-
inary experiments. Figure 1-right shows the ﬁnal conﬁguration used in CATEC
experiments. The platform is equipped with an AscTec VI-Sensor, a stereo cam-
era looking in frontal-downward direction, with additional cameras with wide
ﬁeld of view (one looking upwards and one downwards) and with an onboard
computer, the Intel NUC with an Intel Core i7-5557U processor. The ﬁnal con-
ﬁguration changes the AscTec VI-Sensor by an ASUS RGB-D camera.
The radio tags used for these experiments were the Nanotron nanoPAN 5375
range sensors. They can provide range measurements up to 100 m with 0.6 m of
standard deviation in indoor scenarios.
Fig. 1. AscTec Neo with a Nanotron range sensor in the ETHZ (see Footnote 1) testbed
(left) and with the ﬁnal conﬁguration in CATEC (see Footnote 2) testbed (right).
The results in both scenarios were pretty similar. Thus, we will only describe
the details and show the results at the CATEC testbed. The scenario emulates
the real factory environment, see Fig. 2.
1 Swiss Federal Institute of Technology in Zurich: https://www.ethz.ch/en.html.
2 Center for Advanced Aerospace Technologies: http://www.catec.aero/en.
3 http://www.euroc-project.eu/.

Autonomous Localization of Missing Items with Aerial Robots
185
Fig. 2. Experimental scenario. The mock-up emulates the fan cowl manufacturing line
of the Airbus DS CBC factory.
In these experiments the PFs were conﬁgured with 500 particles, σ = 0.6 and
convergence threshold 0.4.
4.1
Performance
Figure 3 shows in six snapshots the evolution of the localization of 5 missing
tools in one experiment. It can be noticed that the convergence of the PFs is
fast and accurate and the ﬁnal estimation of the EKFs is even more precise.
Table 1 summarizes the performance of the localization algorithm. In average,
missing tools were localized with an accuracy of 0.36 m and the convergence time
of the PFs were 21 s.
Table 1. Average results in the experiments.
Accuracy Time of convergence
0.31 m
21 s
These results, extrapolated to the larger scenario of the CBC factory, validate
the proposed method. The total time for ﬁnding missing tools would be reduced
by 71% with respect to current operation time.

186
J.L. Paneque et al.
Fig. 3. Evolution of missing tool localization. Blue circles are the ground-truth loca-
tions of the missing tools. Red arrows represent the aerial robot pose and orientation.
Green ellipses represent the conﬁdence of the EKFs estimations.
5
Preliminary Tests in the Real Factory
Despite the similarities between the emulated scenario and the real factory, there
will always be diﬀerences which could aﬀect the performance of the proposed
solution. We could not perform a complete experiment in the CBC factory yet,
but we did perform preliminary tests. Those tests showed that the radio sensors
are a little more sensitive to outliers in the factory than in the testbed. Despite
the testbed scenario is pretty similar to the real one, the factory is a bigger
area with more obstacles and metallic structures, which aﬀect the radio range
measurements.
The measurements need to pass an outliers ﬁlter before being integrated
in the proposed localization algorithm. The implemented ﬁlter relies on two
assumptions: the position of the robot is known at each instant with low local-
ization error (despicable against the range measurement error), and each range
measurement is localized in the interval (μ±2σ) with probability 95%. Thus, the
maximum feasible diﬀerence between 2 measurements (with 90% probability) is:
dmax = (1 + α)d(x1, x2) + 4σ,
(6)
with α being a relaxation factor. In this application we used α = 0.3. The
algorithm is implemented as follows:
Several experiments where conducted in order to both measure the behav-
ior of the sensor nodes and the outlier ﬁlter. As the ﬁnal antennae chosen for
this application relied on an isotropic conﬁguration, no angle inﬂuence needed

Autonomous Localization of Missing Items with Aerial Robots
187
Algorithm 1. Outlier Filtering
1: function Outlier Filter(xr, zi, xr, zi, α)
2:
if xr.height < 0.5 then
3:
return zi as outlier
▷Avoid ﬂoor measurements (high outlier density)
4:
else
5:
if length(xr) < 9 then
6:
Add xr, zi to xr and zi
7:
return zi as outlier
▷Wait until enough measurements are gathered
8:
else
9:
Find the median in zi and save the index as idx
10:
dmax = (1 + α)d(xr[idx], xr) + 4σ
11:
if |zi −zi[idx]| < dmax then
12:
return zi as not outlier
13:
else
14:
return zi as outlier
to be measured. The distance between emitter and receiver implied almost no
change in the variance and outlier density of the measurements, and thus we
considered not to separate the experiments using this parameter. That provided,
approximately one thousand measurements were taken for each sensor node, in
15 diﬀerent conﬁgurations, where the main radio tag was located always at a
height of 2 meters. Also, sensor nodes 2 and 4 where always partially occluded
(e.g., below a fan cowl).
Figure 4 shows how the ﬁlter works removing the outliers from the good
measurements. Figure 4-top shows all the measurements gathered by the sensor
while Fig. 4-bottom presents the measurements which passed the outliers ﬁlter. It
can be seen that almost all the outliers are removed so we expect that the missing
tools could be as accurately located as in the testbed experiments. Moreover,
the sensor nodes 2 and 4 (partially occluded) work similarly as those that are
better positioned, which will also help to reach accurate results as those in the
testbeds, where no metallic structures were present.
6
Conclusions
This paper proposes a solution for using an aerial robot to autonomously ﬁnd and
locate missing radio-tagged tools in a factory. Real experiments were performed
in an emulated scenario, very similar to the real target factory, the Airbus DS
CBC in C´adiz, Spain.
The early detection and identiﬁcation of objects that may cause FOD is a
major issue in aircraft assembly line. The results show that the proposal could
really help to drastically decrease the required time to ﬁnd missing objects.
The missing tools localization method was evaluated in testbed scenarios
very similar to the real factory. The experiments showed that it could really
help the workers to ﬁnd those missing tools. We estimated a reduction of time
with respect to current operation of 71%.

188
J.L. Paneque et al.
Fig. 4. Range measurements without (top) and with (bottom) outliers ﬁlter.
By the time of writing this paper, only preliminary tests have been performed
in the CBC factory. However, in the next months the complete system will be
tested there.
The sensors used in the experiments are obviously too big for a ﬁnal imple-
mentation in a real factory. Thus, for the ﬁnal tests in the CBC factory we will
use smaller sensors which can be attached to the tools.
Acknowledgements. This work has received funding from the European Union under
grant agreement No. 608849 (EUROC).

Autonomous Localization of Missing Items with Aerial Robots
189
References
1. Blumenthal, J., Grossmann, R., Golatowski, F., Timmermann, D.: Weighted cen-
troid localization in Zigbee-based sensor networks. In: 2007 IEEE International
Symposium on Intelligent Signal Processing, WISP 2007, pp. 1–6. IEEE (2007)
2. Kantor, G., Singh, S.: Preliminary results in range-only localization and mapping.
In: 2002 IEEE International Conference on Robotics and Automation, Proceedings,
ICRA 2002, vol. 2, pp. 1818–1823. IEEE (2002)
3. Menegatti, E., Zanella, A., Zilli, S., Zorzi, F., Pagello, E.: Range-only SLAM with
a mobile robot and a wireless sensor networks. In: IEEE International Conference
on Robotics and Automation, ICRA, pp. 8–14 (2009)
4. Patwari, N., O’Dea, R.J., Wang, Y.: Relative location in wireless networks. In:
2001 IEEE VTS 53rd Vehicular Technology Conference, VTC 2001 Spring, vol. 2,
pp. 1149–1153. IEEE (2001)
5. Pfaﬀ, P., Plagemann, C., Burgard, W.: Gaussian mixture models for probabilistic
localization. In: 2008 IEEE International Conference on Robotics and Automation,
ICRA 2008, pp. 467–472. IEEE (2008)
6. Pitt, M.K., Shephard, N.: Filtering via simulation: auxiliary particle ﬁlters. J. Am.
Stat. Assoc. 94(446), 590–599 (1999)
7. Stoleru, R., Stankovic, J.A.: Probability grid: a location estimation scheme for
wireless sensor networks. In: 2004 First Annual IEEE Communications Society
Conference on Sensor and Ad Hoc Communications and Networks, 2004, IEEE
SECON 2004, pp. 430–438. IEEE (2004)
8. Thrun, S., Fox, D., Burgard, W., Dellaert, F.: Robust monte carlo localization for
mobile robots. Artif. Intell. 128(1–2), 99–141 (2001)
9. Torres-Gonz´alez, A., Martinez-de Dios, J.R., Ollero, A.: An adaptive scheme for
robot localization and mapping with dynamically conﬁgurable inter-beacon range
measurements. Sensors 14(5), 7684–7710 (2014)
10. Wang, X., Bischoﬀ, O., Laur, R., Paul, S.: Localization in wireless Ad-hoc sensor
networks using multilateration with RSSI for logistic applications. Procedia Chem.
1(1), 461–464 (2009)
11. Wessels, A., Wang, X., Laur, R., Lang, W.: Dynamic indoor localization using mul-
tilateration with RSSI in wireless sensor networks for transport logistics. Procedia
Eng. 5, 220–223 (2010)

Wireless Sensor Networks for Urban Information Systems:
Preliminary Results of Integration of an Electric Vehicle as
a Mobile Node
J.J. Fernández-Lozano1(✉)
, J.A. Gomez-Ruiz1
, Miguel Martín-Guzmán1,
Juan Martín-Ávila1, Socarras Bertiz Carlos2, and A. García-Cerezo1
1 Universidad de Málaga, 29071 Málaga, Spain
jfl@uma.es
2 Universidad de La Guajira, Riohacha, Colombia
Abstract. This paper addresses preliminary results of integration of a mobile
node in a wireless sensor network for an urban information system. The proposed
mobile node has been designed with a modular and scalable architecture that
allows changing the set of sensors in short time. An implementation of the mobile
node, including sensors for gas concentrations and environmental parameters, has
been installed on an electric vehicle and tested in real scenarios in the city of
Malaga.
Keywords: Wireless sensor network · Mobile sensors · Traﬃc monitoring ·
Sustainable mobility
1
Introduction
Wireless sensor networks (WSNs) are a well-known technology [1], enabling the
acquisition of information for diﬀerent applications when proximity from the sensors to
the phenomenon and persistence over time is required. An area where WSNs have
attracted attention is urban environments. Urban traﬃc is generally considered one of
the main problems aﬀecting the quality life of residents in cities and metropolitan areas.
The increasing demand for mobility in urban areas has resulted in more traﬃc conges‐
tion, linked to time and energy loss, and also associated with several medical conditions
[2–4]. These problems drive the development of new global strategies for sustainable
mobility, which must be focused not only in mitigation measures, but also in the use of
innovative technologies and infrastructures. The concept of eﬃcient, eﬀective and safe
transport is dominant today in any transport policy.
Intelligent Transportation Systems (ITSs) have been proposed to improve transpor‐
tation eﬃciency and safety [5], but also to enhance energy saving and to reduce the
emissions of vehicles [6]. But the availability of limited information on urban traﬃc is
one of the main obstacles to the implementation of new transport management strategies.
WSN technologies have been included in a part of ITSs for traﬃc control as a mean to
improve the amount and quality of the data available for transport planning and manage‐
ment [7, 8]. Their easy deployment, and the wide range of sensors available, make WSNs
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_16

an interesting alternative to acquire urban data [9]. The requirement for several types of
traﬃc sensors is justiﬁed by the need to obtain diﬀerent traﬃc features. For example, if
it is necessary to know the vehicle ﬂow circulating on a certain road, it will be enough
to install vehicle counting sensors. However, if the origins and destinations are required,
some kind of vehicle-ID sensors will be necessary [10].
The eﬀectiveness of WSNs depends strongly on the network coverage and connec‐
tivity provided by the sensor deployment [11, 12]. Although the ease to place sensor
nodes is one of the advantages of WSNs, once they are positioned the only way to
enhance the information gathered by the network is to add new nodes, or to re-deploy
the existing ones. Thus, the strategy to deﬁne the locations of the nodes becomes a key
aspect for the performance of WSNs [13–15].
A complementary approach is to provide mobility to the sensor nodes. This can be
done by replacing the sensor nodes if they present a malfunction [16]. An alternative is
to directly install the sensor nodes on vehicles or mobile robots [17, 18]. This approach
is valuable not only for urban applications, but also for other use-cases, like disaster
robotics [19].
This article presents the preliminary results of integration of a mobile node in a
Wireless Sensor Network for Urban Information, and is organized as follows. After this
introduction, an overview of the Urban Information System is given in Sect. 2, including
the system architecture and its implementation. Section 3 explains the proposed mobile
node and its integration with the Urban Information System. Finally Sect. 4 describes
some of the experiments and Sect. 5 presents the conclusions.
2
Description of Urban Information System
The main goal of the proposed Urban Information System (UIS) is to provide the trans‐
portation managers of a city with a way to obtain relevant information from an urban
area of interest in a fast and easy to deploy way. The system is based on a WSN, with
smart sensor nodes mounting several types of sensors: vehicle counting, noise, temper‐
ature, dust, gas emissions, or Bluetooth. This last feature allows the identiﬁcation of a
signiﬁcant sample of the vehicle ﬂows, making it possible to estimate the origin-desti‐
nation matrix (O-D matrix), by using a speciﬁc algorithm.
All the information gathered by the sensors is uploaded to a database in an external
server, and presented to the transportation managers using a SCADA (Supervisory
Control and Data Acquisition) system.
2.1
System Architecture
A modular architecture has been adopted so that the topology of the UIS can be recon‐
ﬁgured according to the needs of the area of interest. The UIS can be deployed with a
diﬀerent number of nodes, and with diﬀerent number or types of sensors installed in the
nodes. These act as transmitter nodes. Additionally, a receiver node is responsible for
conﬁguring the UIS network. Once the network has been set up, the transmitter nodes
carry out the data acquisition and processing. These data are sent to the receiver node,
Wireless Sensor Networks for Urban Information Systems
191

where an internal database is updated and synchronized with the database in an external
server (see Fig. 1). A solar kit can be added to all the nodes so that they can be inde‐
pendent from the electric power grid, and autonomous from an energy point of view, to
gain ﬂexibility in the deployment.
Fig. 1. System architecture of the UIS.
The ﬂexibility in the deployment is particularly important to estimate the origin-
destination matrix (O-D matrix). This matrix represents the number of vehicles going
from origin i to destination j. To estimate this number, a relevant sample of the vehicles
traveling from i to j has to be identiﬁed. The proposed UIS does so by means of the
Bluetooth nodes, which collect the MAC address of Bluetooth devices within their range,
and transmit them to the receiver node. All this information is synchronized with the
database in the server, and then the O-D matrix is estimated, given that the location of
the Bluetooth nodes is known [10]. Thus, freedom to select the points where the trans‐
mitter nodes are deployed is key for this feature to be useful.
2.2
Implementation
The proposed system is developed on the basis of the hardware platform provided by
Libelium [20]. The transmitter nodes share a basic module, called Waspmote V. 1.2,
with the addition of an XBee Pro S2 communications module, from Digi, to provide the
capability of using protocols like ZigBee or DigiMesh. The receiver node is based on a
multiprotocol router named Meshlium, also from Libelium, conﬁgured to work with
ZigBee, WiFi, Bluetooth and 3G/GPRS protocols, and including a GPS. From among
the several wireless protocols that can be conﬁgured, the ZigBee protocol (2.4 GHz) has
been selected to link the transmitter nodes with the receiver node. It can transmit small
192
J.J. Fernández-Lozano et al.

information packages with minimum energy consumption, as well as a range of over
700 meters (depending on visibility conditions). Other advantages are the simplicity and
speed to create the UIS network, or the ease to add new nodes.
Several transmitter nodes have been developed to include diﬀerent sensors. This
way, a UIS deployment can include the following types of nodes:
• UIS Bluetooth node. It includes a BLUEGIGA WT12 Bluetooth module to the
Waspmote V1.2 platform along with the communications module XBee Pro S2,
programmed to work with ZigBee wireless and Bluetooth 2.1 + EDR protocols
simultaneously.
• UIS Ultrasound node. It adds a Maxbotix model XL-MaxSonar-WR1 ultrasonic
sensor to the initial conﬁguration. These sensors operate at a frequency of 42 kHz,
and reach the maximum range of 6 m with a sensitivity of 3.2 mV/cm to 3.3 V, or
7 m and a sensitivity of 4.9 mV/cm to 5.5 V.
• UIS Laser node. It is based on a Nano Pico ITX 1.2 GHz processor board including
4 GB RAM DDR3 memory and a solid state hard disk with a capacity of 60 GB. The
laser sensor is a Hokuyo model UTM-30LX-EW. It is intended to classify the types
of vehicles crossing a given section.
• UIS Environmental Pollution node. It includes a dust sensor (GP2Y1010AU0F, from
Sharp) a light intensity sensor (a GL5528 photo resistor) and a noise sensor (WM-61a,
from Panasonic).
• UIS Gas node. It is composed of several gas sensors: O2 (SK-25, from Fígaro), O3
(MICS-2610, from E2 V), CO2 (TGS 4161, from Fígaro), CO (TGS 2442, from
Fígaro), NH3 (TGS 2444, from Fígaro), VOC (TGS 2600, from Fígaro). Additional
sensors include humidity (J808H5V5, from JIN ZON ENTREPRISE CO.), atmos‐
pheric pressure (MPX4115A, from Motorola) and temperature (MCP9700/9701,
from Microchip).
All UIS transmitter nodes are encapsulated in IP 67 boxes, so that the system can be
deployed outdoors in any weather condition. And a solar kit can be installed to provide
autonomy from the electric power grid, as mentioned above. Figure 2 shows the diﬀerent
nodes of the UIS platform.
Fig. 2. The diﬀerent types of nodes developed for the Urban Information System.
Wireless Sensor Networks for Urban Information Systems
193

The information gathered from the area of interest is transmitted to the receiver node,
and stored as tables in its internal database. It is sent as well to a database in an external
server, synchronized with the UIS Receiver node via 3G communication. This database
is the input for a SCADA system developed for this application using LabVIEW (see
Fig. 3), making possible to present the information obtained by the diﬀerent sensors, as
well as the estimated O-D matrix. A more detailed review of the UIS can be found at [9].
Fig. 3. SCADA.
3
Description of the Mobile Node
3.1
Overview
The UIS provides useful information on an area of interest, and it can be deployed with
speed and ﬂexibility to adapt to each case requirements. But once the network is
deployed, the nodes are static, and obtaining additional information requires installing
supplementary nodes. And some use-case require a diﬀerent approach, such as the case
of gas emissions. A signiﬁcant amount of the emissions in urban areas is linked to motor
vehicles. Obtaining up-to-date information about the levels of some gases can be very
relevant for traﬃc managers. A way of providing so is by means of sensors directly
installed on vehicles. But some other applications require also the capability to measure
environmental data in an adaptable fast way, such as in disaster robotics, where the
collaboration with humans or dogs can be limited by the level of some gases. For that
purpose, a mobile version of the UIS network has been developed. It has been prelimi‐
nary tested in an urban scenario, with experiments in emergency applications planned
for the next future.
194
J.J. Fernández-Lozano et al.

3.2
Architecture and Implementation
The UIS Mobile node is designed to be installed onboard a vehicle, so the area under
study can be modiﬁed without the need to re-deploy the UIS nodes. Since the commu‐
nication between the transmitter nodes and the receiver nodes is provided by the ZigBee
protocol, and its range depends on the obstacles between emitter and receiver, a UIS
Receiver node had to be added. This node gathers the information from the transmitter
nodes, and synchronizes it with the database in the external server via 3G (see Fig. 4).
The inclusion of a UIS Receiver node turns it into a mobile sub-network rather than a
mobile node. However, the interaction with the UIS network makes it closer to a node.
First, all the transmitter nodes installed onboard acquire and process data from the envi‐
ronment. These data are sent to the embarked receiver node, where they are collected,
and once data from all the present transmitter nodes have been received, localization is
added, and the full set is sent to the database at the external server. The information is
then seen as provided by a single, multi-sensor node, so that it can be compatible with
the deployment of conventional, static, UIS nodes. Thus the denomination of UIS
Mobile node over UIS Mobile sub-network is preferred.
Fig. 4. Architecture of the UIS Mobile node.
The use-case in this article requires measuring the gas levels, but some other
scenarios may arise. Therefore, the UIS Mobile node has been designed to accept as
many types of UIS transmitter nodes as possible, without modiﬁcations in neither hard‐
ware nor in software. The UIS Mobile node provides an interface with the UIS network,
via an additional UIS Receiver node, and power supply by using a battery bank. So far,
only the Laser node is to be integrated in the mobile node.
Tests of the UIS Mobile node have been performed for the case of gas emissions in
urban areas. In this mission, the node has been conﬁgured around the UIS Gas and EP
nodes, including sensors for gas concentrations and environmental parameters (see
Fig. 5). Along with a UIS Receiver node, and the battery bank, they have been installed
in a modiﬁed car top carrier.
Wireless Sensor Networks for Urban Information Systems
195

Fig. 5. UIS nodes included in the UIS Mobile node.
Fig. 6. Vehicle used in experiments showing the top case with the UIS Mobile node installed.
The set was mounted on a Nissan Leaf (see Fig. 6). An electric passenger car was
selected to prevent perturbation of gas and noise measurements.
4
Experiments
A set of experiments were performed in the city of Malaga, in Spain, to validate the
concept and conﬁguration of the UIS Mobile node. A series of routes were deﬁned to
test the UIS Mobile node in diﬀerent zones of the city (see Fig. 7), with the vehicle being
driven by a human according to this plan. The node was conﬁgured around the UIS Gas
and EP nodes, and provided measurements for concentration of O2, CO2, and VOC, as
well as luminance, dust, noise, temperature and humidity. The embedded UIS Gas and
196
J.J. Fernández-Lozano et al.

EP nodes worked acquiring and processing the respective data, and then sent them to
the embarked UIS Receiver node. Once that all sensors had provided data, a full set was
completed adding localization via the UIS Receiver node’s GPS, and then it was sent to
the external database at the server. These data were available at the SCADA system.
Figure 8 shows one of the routes, while Table 1 presents the data obtained in experiments
and associated to the route.
Fig. 7. Diﬀerent areas of the city of Malaga covered by experiments.
Fig. 8. Experiments in the route around the city center.
Wireless Sensor Networks for Urban Information Systems
197

Table 1. Data obtained by the UIS Mobile node for the city center route.
Coordinates
Luminance
(lux)
Dust
(ppm)
Noise
(dB)
Temp
(°C)
Humidity
(%)
O2 (%)
CO2
(ppm)
VOC
(ppm)
36°43’25.9”N 4°25’34.1”W
25
0,074
83
24
43,7
19,585
334,3
6,05
36°43’04.2”N 4°25’34.8”W
97
0,075
93
25,81
47,6
17,794
334,49
5,25
36°43’17.2”N 4°25’30.4”W
97,5
0,082
94
23,55
53,2
17,843
334,951
8,69
36°43’25.4”N 4°25’21.9”W
98
0,074
91
27,1
42,1
18,569
334,615
8,46
36°43’16.6”N 4°24’47.9”W
98,5
0,074
91
25,32
51,1
18,133
334,793
7,64
36°43’00.7”N 4°25’40.6”W
75,2
0,072
93
27,85
44,3
18,952
334,713
7,94
36°43’11.0”N 4°26’09.5”W
98,631
0,074
93
26,29
50
18,423
334,694
8,13
5
Conclusions
In this paper, preliminary results of integration of a mobile node in a Wireless Sensor
Network for Urban Information have been presented. In the ﬁrst place, a general descrip‐
tion of the Urban Information System (UIS) is provided including the system architec‐
ture and its implementation. A mobile node has been designed to be integrated with the
UIS. The resulting UIS Mobile node has a modular and scalable architecture allowing
an easy reconﬁguration to change the set of sensors in short time. An implementation
of the UIS Mobile node has been installed on an electric vehicle and tested in real
scenarios in the city of Malaga.
The experiments served to validate the UIS Mobile node, and to prove that useful
data can be obtained. These data are stored in a database in an external server for further
processing or analysis, besides being presented in near real time to a human operator at
the SCADA (Supervisory Control and Data Acquisition). However, the amount of data
was relatively reduced. This was due to the response time of the gas sensors (particularly
CO2), in the order of magnitude of minutes. Since a set of data was sent to the external
database only when all the sensors had provided a measurement, the slow response time
of these gas sensors limited the amount of data provided by the UIS Mobile node. This
mode of work, however, allows the identiﬁcation of trends even for the case of gas
sensors. Once a trend is identiﬁed, or a threshold is surpassed, the UIS Mobile node can
be sent to that area, and kept there for more time so that more data are available for the
traﬃc management to decide if any action is needed.
Future lines of work comprise modifying the schedule of data gathering within the
UIS Mobile node, to increase the amount of data available in the ﬁrst place. Tests on
diﬀerent use-case scenarios, like in disaster robotics, are to be performed. These
scenarios oﬀer the possibility to automate a second pass on a given area, upon the
identiﬁcation of a trend.
Acknowledgements. This article was partially supported by DPI2015-65186-R.
References
1. Akyildiz, I.F., Su, W., Sankarasubramaniam, Y., Cayirci, E.: Wireless sensor networks: a
survey. Comput. Netw. 38(4), 393–422 (2002)
198
J.J. Fernández-Lozano et al.

2. Kim, J.J., Smorodinsky, S., Lipsett, M., Singer, B.C., Hodgson, A.T., Ostro, B.: Traﬃc-related
air pollution near busy roads: the East Bay Children’s Respiratory Health Study. Am. J.
Respir. Crit. Care Med. 170(5), 520–526 (2004)
3. Shah, A.S., Lee, K.K., McAllister, D.A., Hunter, A., Nair, H., Whiteley, W., Mills, N.L.:
Short term exposure to air pollution and stroke: systematic review and meta-analysis. BMJ
350, h1295 (2015)
4. Tzai, W., Joe, J., Chih, S., Jehn, J., Tzu, L.: Monitoring street-level spatial-temporal variations
of carbon monoxide in urban settings using a Wireless Sensor Network (WSN) framework.
Int. J. Environ. Res. Public Health 10, 6380–6396 (2013)
5. Dimitrakopoulos, G., Demestichas, P.: Intelligent transportation systems. IEEE Veh.
Technol. Mag. 5, 77–84 (2010)
6. Losilla, F., Garcia-Sanchez, A.J., Garcia-Sanchez, F., Garcia-Haro, J., Haas, Z.J.: A
comprehensive approach to WSN-based ITS applications: a survey. Sensors 11, 10220–10265
(2011)
7. Friesen, M.R., McLeod, R.D.: Bluetooth in intelligent transportation systems: a survey. Int.
J. Intell. Transp. Syst. Res. 13, 143–153 (2014)
8. Yoo, S.E.: A wireless sensor network-based portable vehicle detector evaluation system.
Sensors 13, 1160–1182 (2013)
9. Fernandez, J.J., Martin, M., Martin, J., Garcia, A.: A wireless sensor network for urban traﬃc
characterization and trend monitoring. Sensors 15, 26143–26169 (2015)
10. Martín-Guzmán, M., Martín-Ávila, J., Fernández-Lozano, J.J., García-Cerezo, A.: A rapid
deployment wireless sensor network for sustainable urban mobility. In: Proceedings of the
23th Mediterranean Conference on Control and Automation (MED 2015), Torremolinos,
Spain, June 16–19, pp. 967–972 (2015)
11. Mrutyunjay, R., Rajarshi, R.: Dynamic deployment of randomly deployed mobile sensor
nodes in the presence of obstacles. Ad Hoc Netw. 46, 12–22 (2016)
12. Heo, N., Varshney, P.K.: A distributed self spreading algorithm for mobile wireless sensor
networks. In: Wireless Communications and Networking WCNC 2003, 2003 IEEE, vol. 3,
pp. 1597–1602 (2003)
13. Vales-Alonso, J., Parrado-García, F.J., López-Matencio, P., Alcaraz, J.J., González-Castaño,
F.J.: On the optimal random deployment of wireless sensor networks in non-homogeneous
scenarios. Ad Hoc Netw. 11(3), 846–860 (2013)
14. Yu, X., Huang, W., Lan, J., Qian, X.: A novel virtual force approach for node deployment in
wireless sensor network. In: IEEE 8th International Conference on Distributed Computing in
Sensor Systems (DCOSS), pp. 359–363 (2012)
15. Al-Turjman, F.M., Hassanein, H.S., Ibnkahla, M.A.: Efficient deployment of wireless sensor
networks targeting environment monitoring applications. Comput. Commun. 36(2), 135–148
(2013)
16. Mei, Y., Xian, C., Das, S., Hu, Y.C., Lu, Y.H.: Sensor replacement using mobile robots.
Comput. Commun. 30(13), 2615–2626 (2007)
17. Ewa, N.-S., Andrzej, S., Michał, M.: A movement-assisted deployment of collaborating
autonomous sensors for indoor and outdoor environment monitoring. Sensors 16, 1497 (2016)
18. Zhu, C., Yang, L.T., Shu, L., Leung, V.C., Rodrigues, J.J., Wang, L.: Sleep scheduling for
geographic routing in duty-cycled mobile sensor networks. IEEE Trans. Ind. Electron. 61(11),
6346–6355 (2014)
19. Tuna, G., Gungor, V.C., Gulez, K.: An autonomous wireless sensor network deployment
system using mobile robots for human existence detection in case of disasters. Ad Hoc Netw.
13, 54–68 (2014)
20. Libelium Homepage. http://www.libelium.com Accessed 23 June 2017
Wireless Sensor Networks for Urban Information Systems
199

Design of a Robot-Sensor Network Security
Architecture for Monitoring Applications
Francisco J. Fern´andez-Jim´enez1(B) and J. Ramiro Mart´ınez-de Dios2
1 Depto. Ing. Telem´atica, University of Sevilla, Escuela Superior de Ingenieros,
c/Camino de los Descubrimientos s/n, 41092 Seville, Spain
fjfj@trajano.us.es
2 Robotics Vision and Control Group, University of Sevilla, Escuela Superior de
Ingenieros, c/Camino de los Descubrimientos s/n, 41092 Seville, Spain
jdedios@us.es
Abstract. This paper presents the design and initial experimentation
of a novel robot-sensor network security architecture that exploits the
synergies between robots and sensor networks to provide high secu-
rity level with moderate resource consumption. The robot implements
security functionalities that in traditional schemes are assumed by the
sensor nodes and the Base Station. In contrast to traditional sensor net-
work security schemes, it is not sensor nodes but the robot who discov-
ers other sensor nodes and establishes the network topology, involving
important security advantages. This paper presents the design of the
architecture, its main advantages and shows its validity in initial ﬁeld
experiments.
Keywords: Sensor network · Robot · Security
1
Introduction
This work is motivated by schemes of robots cooperating with sensor networks
for monitoring of large scenarios such as industries and infrastructures. Tra-
ditional schemes for automatic monitoring are based on sensor network nodes
that are deployed at key locations and operate in unattended way collecting
measurements during long periods of time. Robots, and particularly Unmanned
Aerial Systems (UAS), are emerging technologies in inspection and monitoring
applications. The advantages of exploiting the synergies between UAS and sen-
sor networks for monitoring and inspection applications have been pointed out
in many works [1]. However, existing works dealing with robot-sensor network
cooperation disregard security, which is a main issue both in sensor network and
robot technologies and also from the application point of view.
Security has always been considered critical in communication technologies.
Sensor networks for monitoring applications are very exposed to security attacks.
The need for authentication, conﬁdentiality and data integrity in sensor networks
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_17

Robot-Sensor Network Security Architecture
201
has motivated the development of very sophisticated security algorithms and
schemes that require high resource consumption –computational burden, energy,
delays, severely reducing their feasibility in practical problems. The objective of
this work is to exploit robot-sensor network cooperation to improve the sensor
network security level while keeping its resource consumption eﬃciency.
This paper presents the design and initial experimentation of a novel robot-
sensor network security architecture that exploits the synergies between the het-
erogeneous processing and communication capabilities of the robot and sensor
network nodes in order to provide computationally-eﬃcient solutions secure to
the most common attacks. The proposed architecture contains signiﬁcant novel-
ties over traditional sensor network security schemes. The robot is employed to
conﬁgure the secure operation of the sensor network and is responsible for per-
forming sensor node authentication and revocation. In other words, the robot
implements security functionalities that in traditional schemes are assumed by
the Base Station, enabling single-hop communication between the robot and
each sensor node, which signiﬁcantly improves security. This paper presents the
design of the architecture, its main advantages and validates its operation. The
advantages of the proposed architecture are validated in ﬁeld experiments.
This paper is structured as follows. Section 2 summarizes the related work.
The general architecture proposed in this paper is presented in Sect. 3. The
network set-up stage, a critical stage in the operation of our architecture, is
described in Sect. 4. The experiments are in Sect. 5. Section 6 summarizes the
main conclusions of the paper.
2
Related Work
Traditional wireless sensor networks are composed of sensor nodes that gather
measurements and forward them to the sinks. It is well known that sensor net-
works contain a high number and variety of vulnerabilities originated mainly by:
the wireless transmission channel –it is not necessary to physically access the
sensor node to eavesdrop or to inject packets; the sensor nodes, often low-cost,
energetically eﬃcient and with low computational resources, are exposed and
can be easily disconnected, destroyed or modiﬁed; the simplicity of the network
infrastructure, ad-hoc networks, where nodes trust each other; and a dynamic
network topology that involves frequent interchanging of routing information
that might be false. Work [2] presents a detailed survey of the various types
of security attacks in sensor networks and techniques that could be adopted as
countermeasures. They are categorized according to the objective of the attack,
the OSI level involved in the attack, the type of attacker (internal or external),
and the type of attack (active or passive). Many of the attacks can be solved
employing cryptography to authenticate the nodes and to cipher data. This has
motivated the development of a wide variety of key managing schemes [3].
Various protocols have been developed to provide secure routing in ad-
hoc networks, see e.g. [4,5]. Some of them use the concept of SDN (Software

202
F.J. Fern´andez-Jim´enez and J.R. Mart´ınez-de Dios
Deﬁned Networking) –which was deﬁned by Openﬂow [6]– to the sensor net-
work domain [7,8]. These techniques involve routing decisions that are taken in
a centralized way, not in a decentralized way as in typical ad-hoc networks.
Asymmetric cryptography is more secure –but also signiﬁcantly less com-
putational eﬃcient– than symmetric cryptography. Some works have discarded
for that its use in sensor networks [2]. However, in [9] tests with asymmetric
cryptography using elliptic curve cryptography (ECC) –which is more eﬃcient
than RSA– demonstrated that its implementation is tractable even in nodes
with very low computational capacity [10–12]. The approach in our architecture
is to adopt asymmetric cryptography and use the Diﬃe-Hellman key interchange
protocol [13] in order to establish the keys that will be later used to cipher sym-
metrically. Once that the keys have been interchanged, all the data are ciphered
using symmetric cryptography, which is signiﬁcantly eﬃcient. The operation is
similar to that in the main security protocols used in Internet such as IPSec (for
IP) [14], TLS (for TCP) [15] and DTLS (for UDP) [16]. Some authors have per-
formed eﬀorts to implement the above standard protocols in sensor networks.
Works [17] is an example of the use of IPSec in sensor networks and [18], an
example of TLS/DTLS in sensor networks. DTLS is also used in the CoAP
protocol [19], a specialized web transfer protocol for constrained nodes and net-
works.
The countermeasures against the diﬀerent attacks can be classiﬁed into:
proactive –they prevent the attack– or reactive –they detect the attack and
react to stop it. For instance, [20] presents a proactive countermeasure against
jamming attacks whereas [21] presents a reactive one. Some countermeasures,
particularly reactive ones, require the monitoring of the performance of the net-
work and of the nodes. Monitoring can be centralized– the nodes report potential
anomalies to the Base Station– or distributed, as in [22].
To conclude, security in classical networks is a fundamental problem that
has motivated the development of very high variety of techniques. The security
techniques of classical networks have been considered very complex for their use
in sensor networks. Most sensor network security approaches tend to simplify
security techniques of classical networks to fulﬁll the limited resources of sensor
networks. As a result, security techniques for sensor networks are focused towards
speciﬁc attacks and in general their security level is not very high. The greater
part of the security breach in sensor network take place during the conﬁguration
and authentication of the nodes. Sensor networks require frequent conﬁguration
updates in order to adapt to the changing conditions.
Our architecture exploits the cooperation between robots and sensor net-
works in order to improve the security level adopting a similar concept of SDN.
To the best of our knowledge it is the ﬁrst architecture that addresses sensor
network security adopting a robot-sensor network cooperation approach.
3
Robot-SN Security Architecture
Assume a number of static sensor nodes deployed in an industrial/infrastructure
setting. Each node gathers measurements of interest for monitoring, ﬁlters the

Robot-Sensor Network Security Architecture
203
measurements and transmits the measurements to a Base Station (BS) –also
assumed static, where the measurements are logged, processed and/or trans-
mitted to a remote center. This is a typical conﬁguration of many existing
schemes in monitoring applications. The system also includes one aerial robot
(also referred to as UAS in the paper), which performs inspection and failure
detection tasks. The nodes are assumed endowed with low computational and
transmission capabilities. The network and scenario are assumed realistic. The
network is not very dynamic: sensor nodes will not have high failure probability
and new nodes are not frequently added to the network. The communication
channel is assumed unreliable. Packet loss –originated by collisions, interference,
noise, among others– is not negligible and in general will be diﬀerent at diﬀerent
scenario locations. The link layer at each node checks –using CRC or MAC code–
that the received message is correct or discards it otherwise. The sensor nodes
are designed for long-term monitoring operation. They are accessible and hence,
exposed to malicious attacks including tampering, Sybil attack or cloning. We
assume that the BS is at a secure location and cannot be tampered or cloned.
A general scheme with the proposed architecture is shown in Fig. 1. The
ﬁgure shows the main security modules performed in the robot, sensor nodes
and BS. Besides them, each entity executes the rest of the functionalities that
are necessary for its operation, e.g. for the robot, navigation and for sensor node,
measurement collection. They are not shown for brevity and clarity. The archi-
tecture includes proactive security techniques, such as a public key schema, and
also reactive techniques, such as anomaly detection that is decentralized at each
entity. The Base Station contains the main modules of a public key infrastruc-
ture (PKI) including: (1) Certiﬁcate Authority (CA), which is responsible for
issuing, updating and signing the digital certiﬁcates; (2) Registration Authority
(RA), which veriﬁes the identity of the owner of a digital certiﬁcate requesting
the use of the CA; and (3) Certiﬁcate Repository (CR), which stores and indexes
digital certiﬁcates, including a certiﬁcate revocation list (CRL) that includes all
those certiﬁcates that for some reason have ceased to be valid.
In our architecture they three are located in the BS for simplicity. They could
have been placed in an external and remote element that could provide greater
physical and network security. All components will have a pre-installed certiﬁ-
cate signed by the CA. The Common Name (CN) of the certiﬁcate includes the
component’s name and type. Besides, the BS includes modules for: (1) synchro-
nization with robots and (2) network monitoring and anomaly collection.
Each entity (BS, sensor node or robot) authenticates the rest of the peers
before communicating with them. Authentication is performed using PKI, hence
each entity implements a module for authentication. In our architecture the robot
performs sensor node discovery and localization and establishes the network
topology. The robot is commanded to ﬂy a trajectory such that it can communi-
cate with each sensor node. In the ﬁrst ﬂight the robot discovers and authenti-
cates all sensor nodes and establishes the topology of the network. The location
of the sensor nodes could be known a priori or the robot could be endowed with
techniques for localizing the sensor node such as [23]. The robot announces itself

204
F.J. Fern´andez-Jim´enez and J.R. Mart´ınez-de Dios
Sensor Node
Robot
Base Station
Certiﬁcate
Authority
Registration
Authority
Certiﬁcate
Repository
Information
sharing with
robot
Network
monitoring
and anomaly
collection
Network
monitoring
and anomaly
report
Robot
detection
Node, robot
and BS au-
thentication
Forwarding
Network
monitoring
and anomaly
collection
Information
sharing with
BS
Topology es-
tablishment
and routing
Discovery
and
localization
BS and node
authentica-
tion
Fig. 1. Scheme with the proposed architecture and the main security functionalities
performed in the robot, the Base Station and in the sensor nodes.
with beacon frames. Each sensor node analyzes the beacon frames it receives
and, if it is correct, it tries to connect to the robot. During the connection, the
robot and the sensor node interchange their public keys, they authenticate one
another (using keys signed by the CA) and ﬁnally establish a secure communi-
cation channel using symmetric keys. Authentication with PKI uses asymmetric
keys: more secure but also more computational intensive. Once the nodes have
been authenticated, a symmetric key is established since it involves lower com-
putational burden: that key will be used from now on.
Once the sensor nodes have been discovered, the robot establishes the topol-
ogy (determines the neighbors of each sensor node) and computes the routes
between all sensor nodes. The robot ensures secure communication between
authenticated sensor nodes, which increases the security of the initial stages of
the network conﬁguration and route computation. After deployment, each sensor
node waits until they are activated by the robot. Sensor nodes do not establish
communication with any sensor node that has not been authenticated by the
robot. Securing the initial conﬁguration stages is an advantage of our architec-
ture: these initial stages are critical in the network performance and are object
of a wide variety of attacks. Besides, it prevents the transmission of packets for
sensor node discovery, signiﬁcantly reducing sensor node energy consumption.

Robot-Sensor Network Security Architecture
205
To improve robustness, since the robot has non-null failure probability, the
robot interchanges with the BS the information obtained in sensor node dis-
covery and topology establishment. This interchange is performed by module
Information sharing with robot. The BS will use that information to communi-
cate –forward packets and authenticate– with the rest of the sensor nodes.
Reactive security functionalities also have high relevance in our architecture.
Each entity (sensor node, robot or BS) implement modules for detecting anom-
alies and simple malicious attacks. The robot and the BS perform as collectors
of alarms whereas sensor nodes implement functions for alarm reporting.
4
Network Set-Up Stage
The network set-up stage can be divided in several main steps: sensor node
discovery and topology establishing. The main diﬀerence over traditional sensor
network schemes is that, in our architecture, sensor nodes do not discover other
sensor nodes. The ﬁrst transmission of each sensor node will be to communicate
with the robot. Once conﬁgured by the robot, each sensor node will communicate
only with the nodes that the robot determined as its neighbors.
The presented architecture is not speciﬁc for any particular protocol in the
OSI link layer, but should require some modiﬁcations to allow the new messages
that are described in the following sections. The link layer should allow estab-
lishing a secure channel between neighbors similarly to protocols [14–16]. It is
not required either any assumption on the OSI network layer. For the transport
layer, we adopt a protocol based on datagrams similar to UDP, which should
establish secure end-to-end channels, similarly to protocols [14–16]. If both ends
are neighbor nodes it is not necessary to have encryption in the transport layer
since the link layer is already secure. That is the case, for instance, of the com-
munication between the robot and sensor nodes.
4.1
Sensor Node Discovery
The robot moves in the scenario periodically transmitting Beacon frames con-
taining information of the robot (name, serial number of its certiﬁcate, supported
communication channel). If the sensor node that receives that message have still
not connected to the robot, it transmits a Hello frame that will trigger the hand-
shake process. The robot and the sensor node authenticate one another, estab-
lish ciphering keys and the message authentication code (MAC). To ﬁnish, the
robot asks the sensor node about its state (conﬁguration version, CRL, among
others) and logs all the information of the sensor node. Of course, this proce-
dure assumes that packets can be lost or contain transmission errors. Hence, a
packet a retransmission procedure is established in case expected messages are
not received within a predeﬁned time interval.
Figure 2-left shows a diagram with the messages interchanged assuming that
all messages arrive with no error. The messages of the OSI link layer are in blue
color and, those of the application layer are in black. The complete handshake
is not shown since it depends on the protocol used.

206
F.J. Fern´andez-Jim´enez and J.R. Mart´ınez-de Dios
Robot
Sensor Node
Beacon frame
Hello
Handshake
Handshake
Authentication
and symmetric
keys generation
Status request
Status response
Robot
Sensor Node
As in the discovery phase
Robot-sensor pairing
Robot-sensor pairing
Conﬁguration request
Conﬁguration response
Fig. 2. Message interchange during the sensor node discovery stage (left) and the
conﬁguration stage (right).
4.2
Topology Establishing
Once the map of the valid sensor nodes has been obtained, the optimal topology
of the network is computed. It is not the objective of this work to research in
the best procedure to determine the sensor node neighbors or to compute the
routing tables. We adopt a simple solution where a radio propagation model is
used to predict if two sensor nodes will communicate. Next, the robot computes
the routing tables for each sensor node using a algorithm based on Dijkstra’s
algorithm and assigns a network address to each sensor node. Optimal routes
–and for robustness also alternative routes– between any origin-destination pair
are computed. The network addresses are assigned such that all sensor nodes that
are at a given hop number from the BS have network addresses with a common
preﬁx. The forwarding tables are computed and then they are compacted using
the network address preﬁxes.
Next, the robot conﬁgures the sensor nodes. The robot transmits to each
node a Conﬁguration request message that contains: (1) conﬁguration version,
(2) network address of the sensor node, (3) network address of the BS, (4)
list of neighbor sensor nodes and communication parameters (encryption, time-
outs, . . . ), (5) routing table for that sensor node and (6) Certiﬁcate Revocation
List (CRL). The sensor node receives the Conﬁguration request message, per-

Robot-Sensor Network Security Architecture
207
forms the changes and conﬁrms successful conﬁguration sending back a Conﬁgu-
ration response message. The message interchange is shown in Fig. 2-right. The
conﬁguration stage can be repeated at any time if the network topology changes
–e.g. addition or deletion of sensor nodes. The conﬁguration version is used to
check if the sensor node conﬁguration is updated.
4.3
Key Managing
Each entity of the architecture should have: (1) its own certiﬁcate signed by
the CA and the associated private key, (2) certiﬁcate from the CA, and (3) the
CRL. When a certiﬁcate is compromised (e.g. a sensor node or the robot has
been captured) it will be revoked by the CA (implemented in the BS). The
revocation will generate a new CRL that will be transmitted to all the sensor
nodes using one of the following procedures:
– The BS transmits a message that contains the revoked certiﬁcates to each
sensor node with which the BS has established an end-to-end session.
– The BS ﬂoods the sensor network with a message that contains the revoked
certiﬁcates. That message will be signed by the CA to ensure authenticity.
– The robot can also update the CRL of the sensor nodes that have a session
established with the robot.
Each entity implements modules for detecting anomalies. When a sensor node
detects an anomaly, it transmits the alarm to the robot or to the BS such that
all the information is centralized at the BS. The BS will have all the information
to decide how to react against the alarm.
5
Experimentation and Evaluation
The described architecture has been implemented and its operation has been
tested in initial ﬁeld experiments. The sensor nodes were implemented by Rasp-
berryPi modules equipped with WIFI dongles and running Linux. A total of 12
sensor nodes were deployed in an scenario emulating a bridge monitoring appli-
cation, see Fig. 3-left. The locations of three sensor nodes on the bridge surface
are marked in the ﬁgure. These initial experiments were performed by a Pioneer
3AT robot, see Fig. 3-right, instead of an aerial robot.
In order to avoid modifying the kernel of the Linux running in the Raspber-
ryPi modules, applications that employ an UDP socket as link layer have been
developed. The rest of the layers were implemented on top of it such that the
link address is composed by the pair IP-UDP port. The security layer both at
the link (LL) and the transport layer (TL) were implemented using the DTLS
protocol [16]. In that protocol one end performs as server and the other as client:
the robot decides which role is taken by each sensor node in each connection. In
most cases the server takes more computational burden than the client, so the
robot and the BS will always act as DTLS servers. In case of two sensor nodes,
the sensor node that has to transmit lower number of packets to its neighbors

208
F.J. Fern´andez-Jim´enez and J.R. Mart´ınez-de Dios
Fig. 3. (Left) Sensor nodes in the bridge monitoring scenario. (Right) Pioneer 3AT
robot employed in the ﬁeld tests.
is selected as server. DTLS enables reusing an already established session dur-
ing a predeﬁned time. Hence, it is not necessary to repeat key interchange or
asymmetric key operations for all connections.
The operation and robustness of the proposed architecture were tested with
12 scenarios with 12 nodes deployed at diﬀerent locations. Figure 4 shows the
scheme of Scenario12. The rest of the scenarios are not shown for brevity: nodes
are deployed at diﬀerent locations but the network is not partitioned. The DTLS
cipher suite used is TLS ECDHE ECDSA WITH AES 128 GCM SHA256. In
this paper we are not interested in comparing the performance of diﬀerent suites
but on validating the proposed scheme, which is independent of the suite selected.
We selected this suite because it uses ECC and AES, which is implemented in
hardware in many sensor platforms. To assess the performance of the proposed
architecture, experiments have been conducted using encryption both in Trans-
port Layer (TL) and Link layer (LL), only in Link layer and without encryption.
Fig. 4. Scheme of Scenario12.
In these scenarios the correct operation of the architecture has been success-
fully tested. Also speciﬁc tests were performed to evaluate security in diﬀerent
critical conditions:

Robot-Sensor Network Security Architecture
209
Fig. 5. (Left) Average time required by each sensor node to establish communication
with all its assigned neighbors in the set-up stage. (Right) Average number of packets
and bytes transmitted by each sensor node in the set-up stage.
– Abrupt drop of sensor nodes. In order to detect that sensor nodes stop
working, for both link layer and transport layer connections, keepalive mes-
sages are sent if no other messages are transmitted during conﬁgurable time
intervals. If the timeout for sensor node i expires without having received
messages from node j, node i takes the connection as dropped, reports the
anomaly and tries to reconnect. In diﬀerent scenarios tested, nodes 2, 4, 6
and 9 were turned oﬀand sensor nodes 11 and 12 –far away from the BS–
kept the connection with the BS.
– Use of incorrect or revoked certiﬁcates. In our architecture no sensor
node can connect with a sensor node without a correct certiﬁcate. If the
network is already established and the certiﬁcate of sensor node i is revoked,
all neighbors of node i immediately shut down the connection with node i.
– Injection of packets. Due to the characteristics of the DTLS protocol, all
message injected by an external entity are discarded.
– Spooﬁng. A malicious sensor node could transmit messages with a fake
address but the content of the message will never be valid since it does not
know its ciphering keys. During network establishing this can originate delays
and several tries to perform the connection. That anomaly is very easy to be
detected and is hence reported to the robot or the BS.
Figure 5 shows some results that summarize the performance of the archi-
tecture. Figure 5-left compares the average time taken by each node to establish
communication with all its assigned neighbors in 3 diﬀerent scenarios with the
same number of sensor nodes (12). The time starts at the reception of the con-
ﬁguration message and ﬁnishes at the end of the last handshake (see Fig. 2-left).
This average time is evaluated in the three cases: encryption in TL and LL,
encryption only in LL and no encryption. Although encryption enlarges this
time, the diﬀerent is very low in all the scenarios. Besides, network set-up is
performed only once. Similar conclusions were obtained in all scenarios. Results
are not shown for brevity.

210
F.J. Fern´andez-Jim´enez and J.R. Mart´ınez-de Dios
Figure 5-right shows all transmitted and received bytes as well as the number
of messages interchanged in the topology set-up stage until the BS receives the
ﬁrst measurement from that sensor node. These values are normalized by the
average number of hops to the base station. With encryption, the handshake
requires interchanging more messages and it is necessary to include an addi-
tional header in the messages. However, the values are kept low even in case of
encryption of TL and LL. Again, this traﬃc increment aﬀects only the set-up
stage: once that the set-up stage is completed, the number of messages is the
same with encryption in TL and LL, encryption in LL or no encryption.
6
Conclusions
Sensor networks for monitoring applications are very exposed to security attacks.
The security techniques of classical networks are too complex for their use in sen-
sor networks. Most sensor network security approaches tend to simplify classical
security techniques in order to fulﬁll the constrained resources of sensor net-
works. As a result, security techniques for sensor networks are focused towards
speciﬁc attacks and in general their security level is not very high.
This paper presents the design and initial experimentation of a novel robot-
sensor network security architecture that exploits the synergies between robots
and sensor networks in order to provide high security level with moderate
resource consumption. In our architecture sensor nodes do not discover other
sensor nodes: sensor node discovery and network topology establishment is per-
formed by the robot making use of a concept similar to SDN. The ﬁrst transmis-
sion of each sensor node is to communicate with the robot. Once conﬁgured by
the robot, each sensor node will communicate only with the nodes that the robot
determined as its neighbors. The architecture includes proactive security tech-
niques such as PKI centralized at the Base Station, and also reactive techniques
such as anomaly detection, distributed in all the entities.
The architecture was tested in initial ﬁeld experiments showing the expected
security level. The inclusion of more sophisticated anomaly detection techniques
and the performance of experiments with UAS instead of ground robots is object
of current work. Future work includes to use the robot localization information
to further improving the security of the system –e.g. detecting changes over the
already identiﬁed nodes– and implementation of “red-team” experiments.
Acknowledgments. This
work
has
been
partially
supported
by
EU
Project
AEROARMS Ref. H2020-ICT-2014-1-644271 and AEROMAIN funded by (DPI2014-
59383-C2-1-R). J.R. Mart´ınez-de Dios acknowledges EU Project EUROC (Contract
608849).
References
1. Shih, C.Y., Capit´an, J., Marr´on, P.J., Viguria, A., Alarc´on, F., Schwarzbach, M.,
Laiacker, M., Kondak, K., Mart´ınez-de Dios, J.R., Ollero, A.: On the Coopera-
tion between Mobile Robots and Wireless Sensor Networks, pp. 67–86. Springer,
Heidelberg (2014)

Robot-Sensor Network Security Architecture
211
2. Di Pietro, R., Guarino, S., Verde, N.V., Domingo-Ferrer, J.: Security in wireless
ad-hoc networks - a survey. Comput. Commun. 51, 1–20 (2014)
3. Hegland, A.M., Winjum, E., Mjolsnes, S.F., Rong, C., Kure, O., Spilling, P.: A
survey of key management in ad hoc networks. IEEE Commun. Surv. Tutorials
8(3), 48–66 (2006)
4. Yih-Chun, H., Perrig, A.: A survey of secure wireless ad hoc routing. IEEE Secur.
Priv. 2(3), 28–39 (2004)
5. Sreedhar, C., Verma, S.M., Kasiviswanath, N.: A survey on security issues in wire-
less ad hoc network routing protocols. Int. J. Comput. Sci. Eng. (IJCSE) 2(2),
224–232 (2010)
6. McKeown, N., Anderson, T., Balakrishnan, H., Parulkar, G., Peterson, L., Rexford,
J., Shenker, S., Turner, J.: OpenFlow: enabling innovation in campus networks.
ACM SIGCOMM Comput. Commun. Rev. 38(2), 69 (2008)
7. Galluccio, L., Milardo, S., Morabito, G., Palazzo, S.: SDN-WISE: design, prototyp-
ing and experimentation of a stateful SDN solution for WIreless SEnsor networks.
In: IEEE Conference on Computer Communications (INFOCOM), pp. 513–521
(2015)
8. Olivier, F., Carlos, G., Florent, N.: SDN based architecture for clustered WSN. In:
2015 9th International Conference on Innovative Mobile and Internet Services in
Ubiquitous Computing, pp. 342–347. IEEE (2015)
9. Koblitz, N.: Elliptic curve cryptosystems. Math. Comput. 48(177), 203–209 (1987)
10. Malan, D., Welsh, M., Smith, M.: A public-key infrastructure for key distribution
in TinyOS based on elliptic curve cryptography. In: IEEE Conference on Sensor
and Ad Hoc Communications and Networks (SECON 2004), pp. 71–80 (2004)
11. Kim, D., An, S.: Eﬃcient and scalable public key infrastructure for wireless sensor
networks. In: The 2014 International Symposium on Networks, Computers and
Communications, pp. 1–5. IEEE (2014)
12. Kim,
D.,
An,
S.:
PKC-based
dos
attacks-resistant
scheme
in
wire-
less
sensor
networks.
IEEE
Sens.
J.
16(8),
2217–2218
(2016).
https://doi.org/10.1109/JSEN.2016.2519539
13. Diﬃe, W., Hellman, M.: New directions in cryptography. IEEE Trans. Inf. Theor.
22(6), 644–654 (1976)
14. Kent, S., Seo, K.: Security architecture for the internet protocol. RFC 4301, RFC
Editor (2005)
15. Dierks, T., Rescorla, E.: The transport layer security (tls) protocol version 1.2.
RFC 5246, RFC Editor (2008)
16. Rescorla, E., Modadugu, N.: Datagram transport layer security version 1.2. RFC
6347, RFC Editor (2012)
17. Raza, S., Duquennoy, S., Chung, T., Yazar, D., Voigt, T., Roedig, U.: Securing
communication in 6LoWPAN with compressed IPsec. In: 2011 International Con-
ference on Distributed Computing in Sensor Systems (DCOSS), pp. 1–8 (2011)
18. Perelman, V.: Security in ipv6-enabled wireless sensor networks: An implementa-
tion of tls/dtls for the contiki operating system. Ph.D. diss., MSc Thesis, Jacobs
University Bremen (2012)
19. Shelby, Z., Hartke, K., Bormann, C.: The constrained application protocol (coap).
RFC 7252, RFC Editor (2014)
20. Wood, A.D., Stankovic, J.A., Zhou, G.: DEEJAM: defeating energy-eﬃcient jam-
ming in IEEE 802.15.4-based wireless networks. In: 4th Annual Conference on
Sensor, Mesh and Ad Hoc Communications and Networks, pp. 60–69. IEEE (2007)

212
F.J. Fern´andez-Jim´enez and J.R. Mart´ınez-de Dios
21. Wood, A., Stankovic, J., Son, S.: JAM: a jammed-area mapping service for sen-
sor networks. In: Proceedings of the International Symposium on System-on-Chip
(IEEE Cat. No.03EX748), pp. 286–297. IEEE Computer Society (2003)
22. Parno, B., Perrig, A., Gligor, V.: Distributed detection of node replication attacks
in sensor networks. In: 2005 IEEE Symposium on Security and Privacy (S&P 2005),
pp. 49–63. IEEE (2005)
23. Torres-Gonz´alez, A., Mart´ınez-de Dios, J., Ollero, A.: Range-only SLAM for
robot-sensor network cooperation. Auton. Robots (2017). https://doi.org/10.1007/
s10514-017-9663-8

A Robust Reach Set MPC Scheme for Control
of AUVs
Rui Gomes1(B) and Fernando Lobo Pereira2
1 SYTEC, Faculty of Engineering, Porto University, 4200-465 Porto, Portugal
rgomes@fe.up.pt
2 Institute for Systems and Robotics, Porto, 4200-465 Porto, Portugal
flp@fe.up.pt
http://systec.fe.up.pt
Abstract. A Robust Model Predictive Control (MPC) scheme for the
control of formations of Autonomous Underwater Vehicles (AUVs) is pre-
sented and discussed. This application domain is extremely relevant and
exhibits very diﬃcult control challenges: (i) slow, low data-rate acoustic
communications, (ii) signiﬁcant perturbations inherent to the hydrody-
namic environment, (iii) unexpected emergence of obstacles, and (iv)
severe onboard computation constraints. While the later aspect is dis-
cussed by the Reach Set MPC scheme implementation which maximizes
the a priori oﬀ-line computation as enabled by taking into account, as
much as possible, invariant data, the other challenges are addressed by
increasing the robustness of the proposed basic MPC scheme by consid-
ering a number of intermediate steps which, in spite of the increase of
the on-line computational burden, this remains strongly lower than the
one associated with typical standard MPC schemes.
Keywords: AUV control · Model predictive control · Robustness ·
Reach set
1
Introduction
This article concerns the investigation of a computationally eﬃcient and, at the
same time robust, Model Predictive Control (MPC) scheme satisfying the strict
requirements arising in the coordinated control of multi-agent systems as those
arising in the control of formations of the various types autonomous robotic
crafts. In this article, we pay special attention to Autonomous Underwater Vehi-
cles (AUVs) since it exhibits stricter constraints on onboard resources.
The issues motivating the development of such systems are extremely perva-
sive as they permeate most of the great challenges in most of the human endeav-
ors. Climate change, biodiversity, environment, natural resources management,
territory management, mobility, security and surveillance, to name just a few,
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_18

214
R. Gomes and F.L. Pereira
are some of the real world challenges that, today, human kind is perceiving as
extremely urgent, and impose increasingly sophisticated requirements for the
required ﬁeld studies data gathering, [23]. The satisfaction of requirements such
as spatial and temporal distribution, persistence, combination of wide area with
local area data sampling, among others, are essential in order to gather the data
needed for appropriate situational awareness, generation of critical knowledge,
and to support decision-making in a sustainable manner. It is obvious that the
embedding of intelligence in the various devices is required and, thus, gener-
ally, these systems fall within the so-called distributed networked cyber-physical
systems. One can easily devise many instances of missions (i.e., set of activi-
ties with a pre-deﬁned purpose) involving, possibly heterogeneous, networked
AUVs among which multiple and diverse sensors need to be distributed that
should move in a concerted way deﬁned to fulﬁll the speciﬁed data sampling
requirements.
Thus, it is not surprising that extensive research on multi-agent cooperative
control with autonomous systems has been conducted for some time now. The
range of AUV applications include seabed imaging and mapping, gradient search
and lost cargo detection. Moreover, many systems encompass the articulation
of AUVs with surface vehicles (ASVs) and unmanned air vehicles (UAVs) have
been widely considered for, not only speciﬁc applications such as inspection and
monitoring of systems and infrastructures (oil and gas platforms, port facilities,
energy parks, among other assets), ground surveying and mapping (accident or
ﬁre detection, military reconnaissance), and cargo transportation, etc. At the
heart of systems design lies the optimization of the scarce onboard resources
during the execution of the operations to attain the intended goals. In other
words, the control actions to be deﬁned on-line should not only optimize power
consumption for motion, communications, and sensors usage, but also in such a
way that the underlying computational eﬀort require underlying computational
power.
In order to facilitate the comprehension of the proposed control system, in
this article, we focus only on motion control. More speciﬁcally, we propose a
robust feedback control scheme that ensures adaptivity and some insensitivity
to some classes of perturbations, while enabling a good level of sub-optimality.
This article is organized as follows: In the next section we provide a succinct
overview of the relevant state-of-the-art. Then, we present and discuss the overall
formation control problem formulation with emphasis in its requirements. Then,
in Sect. 4, we present ﬁrst the basic version of the computationally eﬃcient Reach
Set MPC scheme for formation control of multiple AUVs, and then its robust
version involving several intermediate steps. Preliminary results are shown in
Sect. 5 just before the last section that includes some conclusions and future
work.
2
State-of-the-Art
The general problem of vehicles motion control in ﬂuids addressed is this article
is diﬃcult. The system not only exhibits strong nonlinearities and is subject to

Robust Reach Set MPC
215
signiﬁcant uncertainties in its parameters and is subject to strong perturbations,
but also is, in fact, of distributed parameter nature [6].
Non-linear control theory provided multiple types of controllers which
became very popular in this domain. [13] presents a review of existing meth-
ods on tight spacecraft formation ﬂying that uses state feedback. In [20], an
approach for formation control using a virtual structure is proposed. A back-
stepping controller which is robust to input constraints and parameter uncer-
tainties for spacecraft formation is developed in [15].
MPC is a well-known time domain control strategy that computes con-
trol inputs by solving ﬁnite horizon open-loop optimal control problems in a
receding horizon fashion. Because of its optimization, MPC provides a rather
ﬂexible framework to digest complicated system dynamics, and to incorporate
intractable constraints. The importance of optimization in AUVs formation con-
trol is acknowledged in [2] where an MPC based formation controller with sensing
noise is developed. The problem of cooperative control of a team of distributed
agents with decoupled nonlinear discrete-time dynamics and exchanging delayed
information is addressed in [7]. A decentralized scheme for the coordinated con-
trol of formations of autonomous vehicles is presented in [12]. If feasibility of the
decentralized control is lost, collision avoidance is ensured by invoking emergency
maneuvers that are computed via invariant set theory. In [5] a leader-follower
formations of nonholonomic mobile robots is presented. A nonlinear model pre-
dictive control (NMPC) framework for collision-free formation ﬂight controller
design for unmanned aerial vehicles was proposed in [3] being the formation
ﬂight controller designed in a distributed way. More recently, we found appli-
cations of output-feedback MPC [19] where the problem of two UAVs tracking
an evasive moving ground vehicle is solved, and a comprehensive framework for
the cooperative guidance of ﬂeets of autonomous vehicles relying on MPC and
addressing subjects as collision and obstacle avoidance, formation ﬂying and area
exploration [1].
All these approaches suﬀer from several key drawbacks for AUVs: (i) compu-
tationally intensive nature of the MPC scheme that involves solving recursively
a sequence of optimal control problems with very limited onboard computation
capabilities and energy; and (ii) the MPC schemes are parameterized in order to
ensure convergence and stability and, in general, they are not related to onboard
sensing capabilities. Our approach addresses these issues in this article.
3
Problem Formulation
The AUV formation control problem that we consider here consists in control-
ling a set of AUVs to track a trajectory while maintaining a formation under
constraints on the state, control and communications. There are several formula-
tions of this problem. Here, we consider a variant of leader-follower structure in
which the while the leading AUV tracks the trajectory, the follower AUVs have
to maintain the correct relative position as motion unfolds. Notice that it is not
possible to decouple the overall formation control problem in several separated

216
R. Gomes and F.L. Pereira
control problems as the leading vehicle has to guarantee that it can be followed
under the formation constraints.
In order to facilitate the exposition, we consider the following AUV model in
the x −y plane, with coeﬃcients based on the results from [18].
˙η =
⎡
⎣
u cos(ψ) −v sin(ψ)
u sin(ψ) + v cos(ψ)
r
⎤
⎦, ˙ν =
⎡
⎢⎣
τu−(m−Y ˙v)vr−Xu|u|u|u|
m−X ˙u
(m−X ˙u)ur−Yv|v|v|v|
m−Y ˙v
τr−(Y ˙v−X ˙u)uv−Nr|r|r|r|
Izz−N ˙r
⎤
⎥⎦,
where: η = [x, y, ψ]T and ν = [u, v, r]T represent, respectively, the state and
associated velocities; τ = [τu, τr] the longitudinal and rotational thrust controls;
X ˙u, Y ˙v, N ˙r the hydrodynamic added mass coeﬃcients; Xu|u|, Yv|v|, Nr|r| the
hydrodynamic drag coeﬃcients; and m is the vehicle mass.
From the above, we are interested in control strategies which, for each AUV
i, i = 1, . . . , nv, minimize, over a given time interval, the cost functional with
two terms, one that penalizes the trajectory tracking error forcing vehicles to
follow the desired path, ηi
r, and another that penalizes the control eﬀort, thus
saving the limited energy on board of vehicles, i.e.,
 t+T
t
	
(ηi(s) −ηi
r(s))T Q(ηi(s) −ηi
r(s)) + τ iT (s)Rτ i(s)

ds,
where: ηi and τ i satisfy the kinematic and dynamic constraints above and the
endpoint state constraints ηi(t + T) ∈Ct+T ; τ i satisﬁes the control constraints
τ i(s) ∈Ui; the state constraints (ηi(s), νi(s)) ∈Si hold; the communication
constraints gc
i,j(ηi(s), ηj(s)) ∈Cc
i,j, ∀j ∈Gc(i) and the formation constraints
gf
i,j(ηi(s), ηj(s)) ∈Cf
i,j, ∀j ∈Gf(i) have to be veriﬁed. The two later types of
constraints deserve further explanation since their general expression allows for
a wide versatility. While the formation constraints concern the relative position
of the vehicles which, subject to some tolerance, have to be maintained during
the motion, the communication constraints concern restrictions on the maxi-
mum distances among a selected number of vehicles which carry communication
resources. For additional details, see [9]. In this paper, we consider that each
vehicle runs its own Robust Reach Set MPC based controller tailored to its role
in the formation: for example, tracking the reference trajectory, maintaining the
relative positioning to certain other AUVs, etc. Moreover, each AUV have to
take into account the perturbations or other event (like failures, emergence of
obstacles) that might be aﬀecting the motion of other vehicles during the motion.
While some of this information might be perceived via sensing (say range ﬁnder),
the communication through the communication tree ensuring the connectivity
among the AUVs is a very eﬀective, albeit power hungry, way of ensuring the
eﬀectiveness of the formation control scheme.
There is a vast body of literature on MPC, [16]). This is a control scheme in
which the control action for the current time subinterval – control horizon – is
obtained, at each sampling time, by solving on-line an optimal control problem

Robust Reach Set MPC
217
over a certain large time horizon – the prediction horizon – with the state variable
initialized at the current best estimate updated with the latest sampled value.
Once the optimization yields an optimal control sequence, this is applied to the
plant during the control horizon. Then, once this time interval elapses, the state
is sampled and the process is re-iterated. The MPC scheme involves the following
steps:
1. Initialization. Let t0 be the current time, and set up the initial parameters
such as x0, T, Δ, initial ﬁlter parameters.
2. Sample the state variable at time t0.
3. Compute the optimal control strategy, u∗, in the prediction horizon, i.e.,
[t0, t0 + T], by solving the optimal control problem (P).
4. Apply the optimal control during the current control horizon, [t0, t0 + Δ].
5. Slide time by Δ, i.e., t0 = t0 + Δ, and adapt parameters and models.
6. Go to step 2.
where x0 is the initial state, T is the prediction horizon for control optimization,
and Δ is the control horizon. A typical general formulation of the optimal control
problem (P) is as follows:
(P) Minimize g(x(t0 + T)) +
 t0+T
t0
f0(t, x(t), u(t))dt
subject to ˙x(t) = f(t, x(t), u(t))L −a.e.
u(t) ∈ΩL −a.e.
h(t, x(t)) ≤0
g(t, x(t), u(t)) ≤0
x(t0 + T) ∈Cf
where g is the endpoint cost functional, f0 is the running cost integrand, f, h,
and g represent, respectively, the vehicle dynamics, the state constraints, and the
mixed constraints, C is a target that may also be speciﬁed in order to ensure sta-
bility. If one wants to take into account the uncertainty with respect to the initial
state, then one may consider an initial state constraint, i.e., x(t0) ∈CI where
CI is an estimate of the initial set of uncertainty, being the minimization taken
over the worst case of the initial state. For a discussion on stability and robust-
ness see [16]. However, solving problem (P) is too computationally expensive for
the usual onboard computational resources, power budget, as well as real-time
constraints. However, this diﬃculty is overcome by replacing the optimal con-
trol problem (P) by a much smaller ﬁnite dimensional optimization that will be
described in the next section. The decentralized character of the overall MPC
controller is due to the fact that each vehicle runs its own MPC scheme (which
encompasses the models of its neighboring AUVs) and communicates only with
its neighbors or, alternatively, with a pre-selected number of vehicles. Moreover,
as mentioned above, communication delays and packet dropouts as well as noise
and disturbances can easily be incorporated subject to an pre-deﬁned trade-oﬀ
between performance and feasibility.

218
R. Gomes and F.L. Pereira
4
Reach Set MPC
The reachable set based MPC scheme we consider here is in the context of
approximating a long (possibly inﬁnite) time horizon optimization problem by
a sequence of sliding shorter time horizon ﬁnite dimensional optimization sub-
problems initialized with the current sampled state. Thus, the basic idea consists
in replacing the long horizon optimization problem by a sequence of much shorter
horizon ones and, in the process, close the feedback control loop by sampling the
state variable. This requires two items: (i) back propagation in time of the cost
functional in order to ensure consistency of the current time control decisions
with the global horizon optimization; and (ii) forward propagation of the reach
set starting on the current value of the sampled state variable. Notice that this
formulation of the optimization problem exhibits all the advantages inherent
to the geometric nature of the dynamic system captured by its reachable set,
namely in what concerns the incorporation of additional constraints as well as
uncertainties in the dynamics.
This also enables to close the control loop since the sampled state reﬂects
the eﬀect of perturbations in the evolution of the state trajectory. Moreover, this
scheme enables the incorporation of features of the environment - e.g., potential
static or dynamic obstacles arising within the sensors detection range - that are
not present in the conventional optimal control formulation, but that are quite
natural for many application scenarios arising in autonomous robotic vehicles
systems. The key general issue underlying this novel MPC scheme is to pre-
compute oﬀ-line all the ingredients required for the on-line feedback control
synthesis that remain invariant in the course of the “mission execution”. The next
important issue involved in this scheme consists in the mechanisms to adjust the
involved ingredients: the Reach Set at each (t, x) and the short cost functional
to be considered and that approximates the overall cost functional. These two
ingredients are considered next.
4.1
Short Term “equivalent” Cost Functional and Reach Set
First, we consider the optimization of a dynamic control system over a very long
time horizon [0, T], with, possibly T = ∞,
(PTf ) Minimize g(x(Tf)) +
 Tf
0
l(t, x(t), u(t))dt
subject to ˙x(t) = f(t, x(t), u(t)), L −a.e., x(Tf) ∈Cf, x(0) is given, u ∈U,
where U := {u : [0, Tf] →IRm : u(t) ∈Ω}, with Ω closed. For T = ∞, we
consider trajectories converging asymptotically to some equilibrium point. For
T < Tf, the Principle of Optimality implies that the solution to (PTf ) restricted
to the interval [t, t+T] is also a solution to (PT ) below.
(PT ) Min V (t + T, x(t + T)) +
 t+T
t
l(τ, x(τ), u(τ))dτ
s.t. ˙x(τ) = f(τ, x(τ), u(τ)), L −a.e., u ∈U, and x(t) is given,

Robust Reach Set MPC
219
where V (t, z) :=
min
u∈U,ξ∈Cf{g(ξ) +
 Tf
t
l(τ, x(τ), u(τ))dτ} with x(Tf) = ξ, x(t) =
z, ˙x(τ) = f(τ, x(τ), u(τ)), L-a.e..
The value function is propagated by solving the Hamilton-Jacobi-Bellman
equation (H-J-B PDE). In general, the value function is, at most, merely contin-
uous, and, thus, the partial derivatives have to be understood in a generalized
sense, and the solution concept has to be cast in a nonsmooth context, [4].
The later will have to be adapted to the nature of the solution: in a viscosity,
generalized, and proximal normal senses, for, respectively, continuous, Lipschitz
continuous, and lower semi-continuous solutions. There are a number of software
packages to solve the H-J-B PDE numerically and thus compute the value func-
tion, see, for example, [17,21] and the associated huge computational burden is
well known.
In our framework, one or more value functions associated with a reasonable
number of typiﬁed situations (which are strongly application dependent) are
computed oﬀ-line and stored in a look-up table to be stored on-board the AUVs
computers. During the execution of the “mission” in real-time, the relevant value
function is identiﬁed via sensed data or by estimation from navigation data and
invoked to determine the next optimal control at any point (t, x) that leads the
AUV to the optimum of the recruited value function within the control horizon
reach set.
Of course, situations may arise in which none of the anticipated typiﬁed sit-
uations occur. Then, two possibilities arise: either (i) the issue lies on “small”
variations of parameters used in the typiﬁcation process, or (ii) either the vari-
ations in (i) are large, or there occurs signiﬁcant unexpected events, like, for
example, emergence, of obstacles. In case (i), the value function is updated by an
approximation constructed with a computationally simple linear interpolation.
In case (ii), there are two possibilities: either the control architecture changes
the mode of operation, or more sophisticated backward propagation techniques
using the current value of the adjoint variable computed by using the current
conditions is used to propagate a ﬁrst order approximation of the value function
on the short term reach set. Details appear in [8]. Whenever there are changes in
the environment or in the system that aﬀect the formulation of the underlying
optimal control problem as it follows from the general requirements discussed
above.
Now, we introduce the Reach Set for the control horizon to be used in our
MPC scheme. Since the dynamics of robotic are time invariant, we can easily
compute the points in the state space that can be reached in case there are no
unexpected external interferences. Let us deﬁne the forward reach set at time t,
from the state x0 and time t0 ≤t, [10,14,22], by
Rf(t; t0, x0) := {x(t) : ˙x = f(t, x, u), u ∈U, x(t0) = x0}.

220
R. Gomes and F.L. Pereira
4.2
The Reach Set MPC Procedure
Without any loss of generality, we proceed with a standard change of variable
that eliminates the running cost to facilitate the presentation. Let ˜V (t, ˜x) =
V (t, x)+y where ˜x = (x, y), being ˙y = l(t, x, u) with y(0) = 0. Without relabeling
(i.e., x = ˜x, and V = ˜V ), the optimal control problem (PT ) can be expressed in
terms of Reach sets and the value function as follows:
(PT ) Minimize V (t + T, x(t + T))
subject to x(t + T) ∈Rf(t + T; t, x(t)).
Let T be the optimization horizon, Δ, the control horizon, and t the current
time. Then, the MPC scheme can be formulated as follows:
1. Initialization: t = t0, x(t0)
2. Solve (PT ) over [t, t + T] to obtain u∗
3. Apply u∗during [t, t + Δ]
4. Sample x at t + Δ to obtain ¯x = x(t + Δ)
5. Slide time, i.e., t = t + Δ, update the Reach Set from the new x(t) by
appropriate translation and rotation, update the value function at the new
t + T if necessary, and goto 2.
It is clear in this scheme that the computational burden in real-time is extremely
low, particularly, when comparing with the conventional MPC schemes. Under
mild assumptions on the data and, by using the fact that the value function is
continuous, it has been shown that this scheme is robust. Stability is proved by
showing that there exists an uniform neighborhood along the reference trajectory
for which there exists a control for which the value function satisﬁes a Lyapunov
inequality in a generalized sense. From the continuity of the value function,
we obtain sub-optimality estimates in both global and local senses as stated
in the asymptotic performance convergence result below. To state this result,
we require some notation. Let T, and Δ, be, respectively, the optimization and
control horizons, and denote by (x∗
T,Δ, u∗
T,Δ) the associated MPC optimal control
process. Let J(x, u) be the value of the cost functional associated with the (x, u)
over [0, ∞), by J(x, u)|[α,β] be its restriction to the interval [α, β], and by Jk(x, u)
its restriction to the interval [kΔ, (k + 1)Δ].
Proposition 1. Let Tf = ∞and assume that the optimal control horizon
has an optimal control process (x∗, u∗) such that lim
t→∞x∗(t) = ξ∗, being ξ∗an
equilibrium point in C∞. Then,
(i)
lim
Δ↓0,T ↑∞
∞

k=1
Jk(x∗
T,Δ, u∗
T,Δ) = J(x∗, u∗)
(ii)
lim
k→∞
Jk(x∗
T,Δ, u∗
T,Δ) −J(x∗, u∗)|[kΔ,(k+1)Δ]
 = 0.
The simplicity of the optimization problem is apparent due to the complexity of
the Reach Set computation. However, the invariance of the dynamics allows the

Robust Reach Set MPC
221
Fig. 1. (i) AUV forward reach set and (ii) the control value function
pre-computation oﬀ-line of an approximation of Rf(t0 +T; t0, x0) as depicted in
the Fig. 1 (i) and store it in the AUV onboard computer.
Since the points of the state space that can be reached by the AUV from
some reference position at a given time depend solely on the reference position
and on the orientation at that time, the computation of the Reachable Set along
the trajectory can be obtained by translations and rotations of Rf(t0+T; t0, x0).
As an example, we present in Fig. 1 (ii), the computed value function for an
AUV within an area where the target point to which the system must be steered
is (0, 10). The value function was pre-computed by solving several oﬀ-line optimal
control problems with diﬀerent initial conditions spread across a state space
partition. Each problem took approximately 3 s to compute and gave rise to a
set of trajectories starting from the partition and converging to the ﬁnal target.
The controls to be applied to the vehicle are found by searching for the minimum
value within the vehicle’s forward reach set. The required time for this search can
be neglected when compared with the corresponding on-line optimization time.
Moreover, since we are solving several oﬀ-line optimal control problems using
potentially super computers, we can include more complex nonlinear dynamics.
4.3
The Robust Reach Set MPC Procedure
The sampling period in the proposed Reach Set MPC scheme might be too long
when perturbations may become too signiﬁcant and induce instabilities in the
system. In order to prevent this undesirable situation, we use the same ideas as
before in order to introduce some computationally low cost intermediate steps.
For this, we need the notion of Backward Reach Set that has been deﬁned in
[10,14,22]. The basic idea is to retain the sub-optimality of the optimization
horizon while allowing ﬂexibility in the control to steer the state to the desired
target to counter small perturbations in the short control horizon.
Deﬁnition. We denote by Rb(τ; t, zf) the Backward Reach Set of the system
at time τ when x(t) = zf for some time t > τ as the set of points of the state
space which at time τ can be driven by feasible controls to the point zf at time
t. More formally, we have
Rb(τ; t, zf) := {z ∈IRn : zf ∈Rf(t; τ, z)}.

222
R. Gomes and F.L. Pereira
We consider the sampling period Δ, organized into NT subintervals of length
¯
Δ. In order to keep track of the trajectory in spite of perturbations, the following
robust version of the Reach Set MPC scheme is implemented.
(1) Initialization.
(2) Compute Rf(t + Δ; t, x(t)).
(3) Compute z∗= arg min{V (t + Δ, z) : z ∈Rf(t + Δ; t, x(t))}
For i = 1 to NT
Compute z∗
i ∈Rf(t + i ¯
Δ; t + (i −1) ¯
Δ, z∗
i−1) ∩Rb(t + i ¯
Δ; t + Δ, z∗)
Compute u∗
i driving the state from z∗
i−1 to z∗
i on [t + (i −1) ¯
Δ, t + ¯
Δ].
Here, Δ = NT ¯
Δ, z∗
NT = z∗, and z∗
0 = x(t).
(4) Let u∗= u∗
1 and apply u∗during [t, t + ¯
Δ]
(5) Sample x at t + ¯
Δ to obtain x(t + ¯
Δ) = xs
(6) Slide time, i.e., t = t + ¯
Δ. Goto (2)
Now, we present some simulation results and compare the approach proposed
in this article with a classical implementation of MPC control schemes. We
consider the example of equilateral triangle formation of AUVs with one leader
and two followers tracking a sinusoidal trajectory. The leader can communicate
with each one of the followers but the followers can not communicate between
them. At some point in time, the leader’s range ﬁnder sensor detects an obstacle
and changes its strategy to avoid the collision and by taking into account that
each one of the followers also can do it without distorting the formation. Figure 2
depicts the paths of each one of the vehicles.
Fig. 2. Tracking with reach set MPC formation control with obstacle avoidance
To simplify computations we considered the unicycle model to compute the
reachable set of a vehicle with the parameters: 1 m/s and −1 ≤u ≤1 rad/s longi-
tudinal linear and rotation velocities, respectively. The considered optimization
and the control horizons are respectively of 20 and 10 s. The range ﬁnder obsta-
cle distance detection is 5 m. The leader vehicle is endowed with localization
capabilities equivalent to that of a simple GPS but all vehicles could locate rela-
tively to one another via a short baseline scheme. The considered perturbations
in the water column were originated just by a 45◦Gaussian horizontal with and
average of 0.1 m/s and a variance of 0.5 m/s. The failures of communications
were modeled by a Poisson process with parameter λ = 1 sec−1 and simulation

Robust Reach Set MPC
223
of future behavior using the latest information available was used to mitigate
communication failures.
We made a comparison between this approach and the traditional implemen-
tation of the MPC using the eﬃcient ACADO nonlinear programming solver [11]
implemented in C++ and found that, although the performance of both con-
trol schemes were strikingly similar, the computational eﬀort required by both
controllers are remarkably diﬀerent. While our on-line computational time to
compute the control strategy is 0.05 s, the traditional scheme needed 3 s. This
represents an improvement of about 600%. Moreover, a dedicated CPU working
full time is required to compute a similar control strategy with a power bud-
get of around 20 W. The impact of this extra power consumption is extremely
signiﬁcant specially in small AUVs where battery space is very limited.
5
Conclusions
In this article, we introduced a novel MPC scheme that enters into account the
speciﬁc requirements that arise in the coordinated control of multiple AUVs. The
key driver of the approach concerns the mitigation of the real-time computational
burden for two strong reasons: onboard limited energy, computational power, and
communication capabilities. The mathematical details were strongly omitted due
to the lack of space. However, the simulation results obtained so far are extremely
encouraging, being the next step the migration for this control structure for the
AUV onboard control software for the required ﬁeld testing.
Acknowledgements. The authors acknowledge the partial support of: FCT R&D
Unit SYSTEC - POCI-01-0145-FEDER-006933/SYSTEC funded by ERDF — COM-
PETE2020 — FCT/MEC — PT2020, Project STRIDE - NORTE-01-0145-FEDER-
000033, funded by ERDF — NORTE 2020, and contract no 02.a03.21.0008 of the
Ministry of Education and Science of the Russian Federation.
References
1. Bertrand, S., Marzat, J., Piet-Lahanier, H., Kahn, A., Rochefort, Y.: MPC strate-
gies for cooperative guidance of autonomous vehicles. AerospaceLab J. (8) (2014)
2. Breger, L., How, J., Richards, A.: Model predictive control of spacecraft formations
with sensing noise. In: American Control Conference, vol. 4, pp. 2385–2390, June
2005
3. Chao, Z., Ming, L., Shaolei, Z., Wenguang, Z.: Collision-free UAV formation ﬂight
control based on nonlinear MPC. In: Conference on Electronics, Communications
and Control, pp. 1951–1956, September 2011
4. Clarke, F., Ledyaev, Y., Stern, R., Wolenski, P.: Control Theory and Nonsmooth
Analysis. Springer, New York (1998)
5. Consolini, L., Morbidi, F., Prattichizzo, D., Tosques, M.: Leader-follower formation
control of nonholonomic mobile robots with input constraints. Automatica 44(5),
1343–1349 (2008)
6. Fossen, T.I.: Guidance and Control of Ocean Vehicles. Wiley, Chichester (1994)

224
R. Gomes and F.L. Pereira
7. Franco, E., Parisini, T., Polycarpou, M.: Cooperative control of discrete-time
agents with delayed information exchange: a receding-horizon approach. Decis.
Control Conf. 4, 4274–4279 (2004)
8. Gomes, R.: AUV formation control: a model predictive control approach. Ph.D.
thesis, Faculty of Engineering, Porto University (2017)
9. Gomes, R., Calado, P., Lobo Pereira, F., Borges de Sousa, J.: Motion coordina-
tion of autonomous underwater vehicles under acoustic communications. In: IFAC,
Milan, Italy, August 2011
10. Graettinger, T., Krogh, B.: Hyperplane method for reachable state estimation for
linear time-invariant systems. J. Optim. Theory Appl. 69, 555–588 (1991)
11. Houska, B., Ferreau, H., Diehl, M.: ACADO toolkit - an open source framework for
automatic control and dynamic optimization. Optimal Control Appl. Meth. 32(3),
298–312 (2011)
12. Keviczky, T., Borrelli, F., Fregene, K., Godbole, D., Balas, G.: Decentralized reced-
ing horizon control and coordination of autonomous vehicle formations. IEEE
Trans. Control Syst. Technol. 16, 19–33 (2008)
13. Kristiansen, R., Nicklasson, P.: Spacecraft formation ﬂying: a review and new
results on state feedback control. Acta Astronautica 65(11–12), 1537–1552 (2009)
14. Kurzhanski, A., Varaiya, P.: Dynamic optimization for reachability problems. J.
Optim. Theory Appl. 108(2), 227–251 (2001)
15. Lv, Y., Hu, Q., Ma, G., Zhou, J.: 6 DOF synchronized control for spacecraft for-
mation ﬂying with input constraint and parameter uncertainties. ISA Trans. 50,
573–580 (2011)
16. Mayne, D., Rawlings, J., Rao, C., Scokaert, P.: Constrained model predictive con-
trol: stability and optimality. Automatica 36(6), 789–814 (2000)
17. Michel, I., Bayen, A., Tomlin, C.: Computing reachable sets for continuous dynam-
ics games using level sets methods. IEEE Trans. Autom. Control 50, 980–1001
(2005)
18. Prestero, T.: Veriﬁcation of a six-degree of freedom simulation model for the
REMUS AUV. Master’s thesis, MIT/WHOI (2001)
19. Quintero, S., Copp, D., Hespanha, J.: Robust UAV coordination for target tracking
using output-feedback model predictive control with moving horizon estimation.
In: American Control Conference, pp. 3758–3764, July 2015
20. Ren, W., Beard, R.: Virtual structure based spacecraft formation control with for-
mation feedback. In: AIAA Guidance, Navigation and Control Conference, Mon-
terey, CA, pp. 2002–4963 (2002)
21. Sethian, J.: Level Set Methods and Fast Marching Methods, 2nd edn. Cambridge
University Press, Cambridge (1999)
22. Varaiya, P.: Reach set computation using optimal control. In: Proceedings of KIT
Workshop (1998)
23. Zhang, F., Fratantoni, D., Paley, D., Lund, J., Leonard, N.: Control of coordinated
patterns for ocean sampling. Int. J. Control 80(7), 1186–1199 (2007)

Sensor Technologies Oriented to
Computer Vision Applications

Obtaining and Monitoring Warehouse 3D Models
with Laser Scanner Data
Antonio Adán
(✉), David de la Rubia, and Andrés S. Vázquez
3D Visual Computing and Robotics Lab, Universidad de Castilla La Mancha,
13071 Ciudad Real, Spain
{Antonio.Adan,Andress.vazquez}@uclm.es,
David.larubia1@alu.uclm.es
Abstract. This paper is focused on creating semantic 3D models of unstructured
warehouses from coloured point clouds. Several scans from diﬀerent locations of
a laser scanner are integrated into a unique 3D dataset that is afterwards processed.
The paper presents an eﬃcient 3D processing algorithm that is able to segment,
recognize and locate the existing materials of a storage place. The obtained 3D
model provides to logistic managers precise and valuable information, such as:
the current location of the stock, the free space for coming merchandises or the
occupied volume variations between two scanning sessions taken at diﬀerent
times. The method has been tested under noise conditions in simulated scenarios
and the extracted model has been compared with a ground truth model. The good
results demonstrate that this approach could be useful in the logistic ﬁeld.
Keywords: 3D data processing · Laser scanner · 3D models · Logistic · 3D
recognition and pose
1
Introduction
Nowadays, monitoring and controlling the state of stores and warehouses is a very
important topic in construction and logistic ﬁelds. Research shows that 80% of current
warehouses are manually operated with no supporting automation and just 5% of current
warehouses are totally automated. Among the diversity of the automation aspects, the
understanding and monitoring of the warehouse’s content is a key subject that is strongly
connected with intelligent automation [1].
In the robotics ﬁeld, in particular, managers are learning the advantages of supple‐
menting workers with collaborative mobile robots. In fact, the ﬁrst examples of self-
contained mobile picking robots are already working in industrial storages [2].
In this paper, we propose a methodology for extracting 3D semantic models of
warehouses as an input of further automatic applications. Our method obtains de-tailed
3D models of an unstructured storehouse (not yet organized in shelves). Figure 1 shows
some examples of structured and unstructured depositories.
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_19

Fig. 1. Examples of structured (left) and unstructured (right) storage
2
Related Work
Daily, the geometry of a warehouse may signiﬁcantly change because of many merchan‐
dises are uploaded and downloaded on delimited areas of the storehouses. Usually, the
daily knowledge and control of the material’s layout of an industrial store is manually
done, which entails some management problems. For example, the loss of material and
inaccurate/erroneous positions of certain objects of the scene are frequent problems in
human monitoring systems.
In order to improve the tracking of materials and merchandises, some technologies
have been accepted in the logistic world in the last two decades. Among those technol‐
ogies are: Radio Frequency Identiﬁcation (RFID), Global Positioning System (GPS) and
vision sensors [3]. Some authors have also evaluated the productivity impact of auto‐
mating the monitoring of industrial stores in the construction ﬁeld [4] using recent tech‐
nologies. The ﬁnal conclusion is that with the help of those technologies, the labor
productivity improves and the facility management becomes more eﬃcient and safe.
Nevertheless, the referred technologies provide limited information. Thus, RFID
essentially only gives information about the existence of a particular item in the store
but does not provide the location and size of the object. Additionally, the interferences
produced by certain materials, makes the network RFID systems ineﬃcient for 3D
monitoring. For its part, the GPS systems are neither really accurate as pose estimators,
and are exclusively used in outdoor storage places.
Computer vision systems (basically cameras and structured light systems) combined
with other technologies have improved the quality of the object tracking in industrial [5]
and construction environments, but the object positioning is still imprecise. In [6], a
couple of RFID readers equipped with cameras are installed at the disposal sites with
the objective of controlling the movement of the waste material. Current 3D technolo‐
gies, like photogrammetry and laser scanning, have been extensively used for digitiza‐
tion and creation of automatic BIM models. Many works refer to construction ﬁeld
obtaining the as-is and as-built 3D models. Other works perform a 3D representation of
the site under construction with the aim of detecting defects in constructive components
[7] and compare the as-planned and as-built models [8].
However, few work has been developed in the issue of monitoring and tracking
of warehouse materials to date. Some related, but not similar, works can be found in
228
A. Adán et al.

[9, 10]. Bosché et al. [9] propose a system that tracks MEP (Mechanical, Electrical
and Plumbing) components using laser scanners. By means of a Scan-vs-BIM and
Scan-to-BIM integration, the approach is able to identify present and absent material
between two consecutive scanning sessions. This method assumes a manually-built
initial 3D model of the scene. In [10], Zeibak et al. propose an approach that recog‐
nizes the as-built deformed objects of a scene. They consider surface primitives and
prior knowledge contained in the BIM model of the scene.
This paper presents a novel methodology that generates a 3D model of an industrial
warehouse with the objective of controlling and monitoring the content and position of
the materials. To date, no similar works addressing this problem has been published.
3
Overview of the Method
Our system uses a 3D laser scanner which takes data from diﬀerent positions of the scene
and generates an integrated point cloud. The point cloud is later processed and a semantic
3D model is ﬁnally obtained. Figure 2 shows a general ﬂowchart of the method. We
particularly detail the sequence of the processes that takes place when a new scanning
session is carried out at a certain time t.
Fig. 2. Warehouse BIM model creation
Obtaining and Monitoring Warehouse 3D Models
229

First of all, we state two assumptions in our method:
1. About the objects: the scene consists of a set of objects, typically polyhedral objects,
with diﬀerent colours and shapes.
2. About the layout: a random layout of the objects of the scene is assumed. In other
words, this is an unstructured warehouse in which the objects may be piled up
without an established order.
The coloured point cloud of the scene, S(t), is obtained following an automatic scan‐
ning process in which a robot, that carries a laser scanner, is sequentially placed on a
set of a priori calculated positions. The set of scans is registered in a common coordinate
system and a unique point cloud is eventually obtained.
Assuming that a number of scanning sessions have already been performed, the 3D
scene model at time (t-1), B(t-1), is available for a comparison with a new model, B(t).
From here on we shall refer these models as As-is 3D models. This term has extensively
been used in the creation of BIM models, that is, semantic models that represent the
architectural structure of buildings. However, this term is unusual for describing the
layout of the objects contained in a scene.
The system ﬁrstly separates into points of indoor structural components (essentially
walls, columns, ceiling and ﬂoor) and those that belong to the existing merchandises.
Since the building structure is constant, the ﬁrst set of points is easily extracted from the
earlier model B(t-1), which is a scan-vs-BIM process. But, the main problem is in
segmenting the rest of the data points and recognizing the existing objects of the ware‐
house, which is a scan-to-BIM process. Section 4 presents a solution for this.
After identifying and positioning the current merchandise deposited in the storage
place, we can generate the current as-is 3D model, B(t), and ﬁnd the absent and new
objects. The last part of the method consists in comparing models B(t-1) and B(t) and
extracting valuable information. For example, we can evaluate the occupancy variation,
made some corrections with regard the materials’ layout or carry out simulations in case
new objects arrive to the warehouse. Algorithms that calculate the next best location of
a coming object or suggestions for relocating existing objects can also be carried out.
This paper primary focuses on object segmentation, recognition and warehouse moni‐
toring.
4
The Proposed Method
4.1
3D Data Segmentation
The segmentation stage is divided into three steps.
Segmentation of structural elements
Since the 3D model of the building’s structure is known, the segments corresponding
to ground, ceiling and walls can easily be extracted from the point cloud S(t). Let us
assume that SSE and SO are the data corresponding to structural elements and objects
respectively. The M-estimator SAmple Consensus algorithm [11] (LMSAC) extracts
230
A. Adán et al.

the points ﬁtted to walls, ceiling and ﬂoor, which are usually ﬂat surfaces. MSAC is a
variant of the RANSAC (RANdom SAmple Consensus) algorithm.
Colour segmentation
SO is segmented in clusters with similar colours as follows. In order to have a RGB
projected image of the scene, SO is projected onto the XY plane and converted into a 2D
image, which is denoted as IZ. Note that, the laser scanner provides a non-totally-uniform
colour for each object, mainly owing to the noise injected in the data and the illumination
changes. Light variations of the object’s colour are therefore expected (see the detail in
Fig. 3(a)). A k-mean colour clustering algorithm, in which a set of RGB prototypes are
imposed, follows. The set of clusters with similar colour, {ICi}, allows us to separate
superimposed objects in IZ, making easier the rest of the segmentation process.
(a)                                                         
(b)
(c)
Fig. 3. (a) Image IZ and a detail that illustrates small colour variations in the objects. (b) Examples
of clusters with similar colours. (c) Examples of incomplete segments.
Obtaining and Monitoring Warehouse 3D Models
231

Object segmentation
Finally, each cluster is again segmented into their connected components, each appa‐
rently belonging to a diﬀerent object. A region growing algorithm is used in this step.
Note that occlusions can generate incomplete or even separated segments of the same
object. To solve this problem, a repairing algorithm calculates the edges and angles of
the incomplete segments and infers the enclosing shape of the object.
Figure 3(b) and (c) illustrate an example of the colour clusters and some incomplete
segments detected.
4.2
Object Classiﬁcation and Positioning
Object classiﬁcation has been performed using supervised learning techniques. Specif‐
ically, we have used distance based discriminant functions by means of a perceptron
technique.
The pattern is a vector ⃗x =
{
x1, x2, … x7
} composed of the following descriptors:
x1: size of the bounding box that contains the object
x2: ratio of the minor axis/major axis of the bounding box
x3: average of red colour
x4: average of green colour
x5: average of blue colour
x6: internal angle average of the polygon that encloses the object
x7: standard deviation of the polygon’s internal angles
After recognizing an object in IZ, the polyhedron that encloses it, along with its
position in the real scene, are calculated. The polyhedron’s base is obtained by scaling
the polygon that ﬁts the object in image IZ (this scale turns pixels into centimeters), and
the polyhedron’s height is that of the model of the recognized object.
An algorithm that places the polyhedron within the scene is then carried out by
positioning its upper base. The X and Y coordinates of the upper-base vertices are that
of the polyhedron’s base. As regard to the Z coordinates, we assume all them to be equal
to the maximum Z coordinate of the corresponding segment. To facilitate this calcula‐
tion, we previously obtain the Z-max-image, IZm, in which a pixel contains the highest
Z coordinate of the points projected onto it. Figure 4(a) shows a ﬂowchart that clariﬁes
this process. In this way, the 3D models of the recognized objects are eventually posi‐
tioned in the scene, creating a three-dimensional model of the material currently existing
in the warehouse (see Fig. 4(b)).
232
A. Adán et al.

(a)
(b)
Fig. 4. (a) Recognition and positioning of objects in the warehouse. (b) 3D model calculated.
The number aside the object model is the assigned class.
Obtaining and Monitoring Warehouse 3D Models
233

4.3
3D Discretized Semantic Model, Labelling and Volume Measure
In order to eﬃciently analyze and monitor the warehouse, a 3D voxel model has been
generated. A voxel that contains data (i.e. points of the point cloud) is initially labelled
as occupied. Otherwise the voxel is empty.
As the recognition/positioning stage progresses, we ﬁnd new labels. For occupied
voxels we distinguish between: (1) structure voxels, which belong to structural compo‐
nents of the building, and (2) object voxels, which are eventually labelled with their own
class label.
Regarding the empty voxels, we distinguish three sub-types. (1) free voxels (potential
occupied voxels): they are those that can be occupied by new objects at the next moni‐
toring session, (2) scanner voxels: they are reserved for the scanner to move, (3) unim‐
portant voxels: they are the rest of empty voxels.
The labelled voxel space allows us to measure the current occupied volume and the
available volume for further loadings. Besides, the system can easily asses the variations
in the warehouse in terms of items, volumes and positions. Thus, between two consec‐
utive scanning sessions, we know the objects that remain in the scene, the absent objects
and the new ones. Additionally, the system could provide valuable suggestions about
where to place one or several new objects in the warehouse.
5
Experimental Test
In order to test our approach on a controlled framework, we have designed synthetic
models and carried out simulated scanning processes. The synthetic model is created
with the Blender software, whereas the simulated scans are obtained from Blensor [12].
Blensor faithfully simulates our Riegl VZ-400 laser scanner, providing dense 3D
coloured data, with injected gaussian noise.
The advantage of the simulated data is that the ground truth is available (i.e. location,
shape and size of the objects) and the errors can precisely be calculated.
In this paper, we show the results in a representative case of study. The warehouse
is a rectangular room 10 m*16 m*4 m in size, that contains 9 diﬀerent type of objects.
This scene is scanned from 12 positions, providing a ﬁnal coloured point cloud of around
15 million data points (See Fig. 5(a)).
We conducted several scanning sessions of the same scene in which we previously
removed and added some objects for each new session. Figure 5(a) shows the point
clouds for two consecutive sessions. The ﬁrst scene consists of 51 objects, occupying a
total of 45,34 m3 and leaving a free volume of 122,66 m3, that is 27% of the potential
storing space is used. The system correctly recognizes 51 objects (100% of true positive
cases) and there is one false positive case. In the second scene, the occupied volume is
32,99 m3 (24% of the storing space), leaving a free volume of 135 m3. This scene consists
of 42 objects and our algorithm provides 41 true positives (97,6%) and 1 false positive.
As a consequence of these results, we infer that the total variation in volume between
both samples was –12,353 m. Figure 5(b) and (c) illustrate the images IZ and the voxel
space generated for each sample.
234
A. Adán et al.

Table 1 shows some other quantitative results: the occupied volume detected per
class, the corresponding volumetric errors and the error in the total occupied space. Note
here the low ﬁgures in the average errors (0,28% and 0,16%) and the total volume errors
(2,53% and 1,44%).
(a)
(b)
(c)
Fig. 5. The warehouse tested at two diﬀerent times. Processing stages (a) Points clouds (b) Images
IZ. (c) The respective 3D voxel models calculated.
Obtaining and Monitoring Warehouse 3D Models
235

Table 1. Objects recognized and occupancy volumetric errors per class. (OV: occupied volume,
TV: true occupied volume, E: inter-class volume error, G: global volume error).
Class
Scene 1
Scene 2
Units
OV(m3)
TV(m3)
EIC(%)
G(%)
Units
OV(m3)
TV(m3)
EIC(%)
G(%)
#1
4
0.53
0.5
5.67
0.06
7
0,79
0,87
9,2
0.24
#2
7
2.95
2.94
0.34
0,02
10
3,90
4,2
7,7
0,36
#3
4
13.79
13.52
1.9
0,59
4
13,76
13,52
1,5
0,72
#4
8
0.21
0.17
19
0,09
3
0,07
0,07
0
0
#5
6
2.32
2.25
3.02
0,15
3
1,14
1,11
2,6
0,09
#6
3
9.15
9
1.64
0,33
2
6,00
6,00
0
0
#7
8
2.60
2.62
0.77
0,04
6
1,87
1,92
2,6
0,15
#8
5
13.67
13.15
3.8
1.14
2
5,27
5,24
0,5
0.09
#9
7
0.12
0.17
41
0.11
5
0,19
0,13
31,6
0,12
Total
52
45,34
44,32
2,53
42
32,99
33,06
1,44
Aver.
8,57
0,28
6,1
0,16
Table 2 provides a comparison between both scenes, including the number of objects
per class and the occupancy volume variation per class. It is clear a high positive varia‐
tion of class #1 (26%) and high negative variation of class #8 (–8,4%). Globally, the free
space (i.e. space for new merchandise) in the second scene has increased 12,35 m3 with
respect to the ﬁrst one.
Table 2. Monitoring two scenes. (S1: scene 1, S2: scene 2, Var: variation, Vol: volume)
Class
Units S1
Units S2
Var. Units
Vol S1 (m3) Vol S2 (m3) Vol. Var.
(m3)
Vol. Var.
(%)
#1
4
6 (7)
+2
0.53
0,79
0,26
49,1
#2
7
10
+3
2,95
3,90
0,95
32,2
#3
4
4
0
13,79
13,76
–0,03
–0,2
#4
8 (7)
2 (3)
–6
0.21
0,07
–0,14
–66,7
#5
6
3
–3
2,32
1,14
–1,18
–50,9
#6
3
2
–1
9,15
6,00
–3,15
–34,4
#7
8
6
–2
2,60
1,87
–0,73
–28,1
#8
5
2
–3
13,67
5,27
–8,4
–61,4
#9
7
6 (5)
–1
0.12
0,19
0,07
58,3
Total
52
42
–
45,34
32,99
–12,35
6
Conclusions
To date, the daily control of the layout of the material in industrial stores is manually
done. In non-yet-structured environments, where the objects are randomly piled by the
operators, a manual monitoring of the warehouse is ineﬃcient and prone to errors.
Therefore, new solutions in the automatic warehouse monitoring ﬁeld are demanded.
This paper presents a methodology for extracting semantic 3D models of warehouses
as an input of further automatic applications in the logistic ﬁeld. On the whole, the
presented system could be helpful for automating some aspects in the logistic
236
A. Adán et al.

framework. Thus, the storehouse manager could use the generated 3D models to extract
information about the number and position of the merchandise as well as the occupied
volume per type of object and the total available volume. An assessment of the variation
of the state of the warehouse, in terms of units and volumes, could also be carried out.
Future improvements will focus on dealing with an extended objects database in
terms of colour and shape. Segmentation is a key point in the whole process that should
be extended to scenes with uncoloured point clouds. Earlier segmentation techniques
[13] for automatic segmentation of 3D complex scenes could be adapted to this kind of
scenes.
An important point to be considered concerns to the severe occlusion cases. When
an object is sensed under high occlusion circumstances, a set of unconnected small
segments could appear in image IZ, making much more diﬃcult the object recognition
stage. The algorithm that infers the true polygon (or free-shape contour) that encloses
this information and identiﬁes the object, needs therefore to be reformulated and tested
in such critical cases. Following the idea published in [14], some initial work has been
addressed in our lab by using primary spline curves which are later modeled with fuzzy
techniques.
Acknowledgments. 
This work has been supported by the Spanish Economy and
Competitiveness Ministry [DPI2016-76380-R, (AEI/FEDER, UE)] and by Castilla-La Mancha
Government [PEII-2014-017-P project].
References
1. Workshop on Automation for Warehouse Logistics. IEEE CASE-ISAM 2016. Fort Worth,
Texas, USA
2. Amazon Robotics Challenge. https://www.amazonrobotics.com/#/roboticschallenge
3. Valero, E., Adán, A.: Integration of RFID with other technologies in construction.
Measurement 94, 614–620 (2016)
4. Grau, D., Caldas, C.H., Haas, C.T., Goodrum, P.M., Gong, J.: Assessing the impact of
materials tracking technologies on construction craft productivity. Autom. Constr. 18, 903–
911 (2009)
5. Adán, A., Molina, F., Vazquez, A.S., Morena, L.: 3D feature tracking using a dynamic
structured light system. In: The 2nd Canadian Conference on Computer and Robot Vision,
pp. 168–175 (2005)
6. Huang, R.Y., Tsai, T.Y.: Development of an RFID system for tracking construction residual
soil in Taiwan. In: Proceedings of the 28th ISARC (2011)
7. Kiziltas, S., Akinci, B., Ergen, E., Tang, P.: Technological assessment and process
implications of ﬁeld data capture technologies for construction and facility/infrastructure
management. ITcon 13, 134–154 (2008). Special Issue: Sens. Constr. Infrastruct. Manage.
8. Rebolj, D., Babič, N.Č., Magdič, A., Podbreznik, P., Pšunder, M.: Automated construction
activity monitoring system. Adv. Eng. Inform. 22, 493–503 (2008)
9. Bosché, F., Guillemet, A., Turkan, Y., Haas, C.T., Hass, R.G.: Tracking the built status of
MEP works: assessing the value of a Scan-vs-BIM system. J. Comput. Civ. Eng. 28(4) (2014)
Obtaining and Monitoring Warehouse 3D Models
237

10. Zeibak-Shini, R., Sacks, R., Filin, S.: Toward generation of a building information model of
a deformed structure using laser scanning technology. In: 14th International Conference on
Computing in Civil and Building Engineering (ICCCBE) (2012)
11. Torr, P.H.S., Zisserman, A.: MLESAC: a new robust estimator with application to estimating
image Geometry. Comput. Vis. Image Underst. 78, 138–156 (2000)
12. Gschwandtner, M., Kwitt, R., Uhl, A., Pree, W.: BlenSor: blender sensor simulation toolbox.
In: Lecture Notes in Computer Science (Lecture Notes in Artiﬁcial Intelligence. Lecture Notes
Bioinformatics). LNCS, vol. 6939, pp. 199–208 (2011)
13. Merchán, P., Adán, A., Salamanca, S., Cerrada, C.: 3D complex scenes segmentation from a
single range image using virtual exploration. In: Advances in Artiﬁcial Intelligence. Lecture
Notes in Computer Science, vol. 2527, pp. 923–932 (2002)
14. Bueno, G., Martínez-Albalá, A., Adán, A.: Fuzzy-snake segmentation of anatomical
structures applied to CT images. In: Image Analysis and Recognition. Lecture Notes in
Computer Science, vol. 3212 (2004)
238
A. Adán et al.

3D Monitoring of Woody Crops Using a Medium-Sized
Field Inspection Vehicle
José M. Bengochea-Guevara1, Dionisio Andújar1,2, Francisco L. Sanchez-Sardana1,
Karla Cantuña1,3, and Angela Ribeiro1(✉)
1 Center for Automation and Robotics, CSIC-UPM, Arganda del Rey, 28500 Madrid, Spain
angela.ribeiro@csic.es
2 Institute of Agricultural Sciences, CSIC, 28006 Madrid, Spain
3 Cotopaxi Technical University, Latacunga 050101, Ecuador
Abstract. In this work, a crop inspection system is presented. A mobile platform,
based on a commercial electric vehicle, is equipped with diﬀerent on-board
sensors to inspection annual crops (maize, cereal, etc.) and multi-annual crops
(orchards, vineyards, etc.). The use of a low-cost RGB-D sensor, the Microsoft
Kinect v2 sensor, for the inspection of woody crops is tested. A method to generate
automatic 3D reconstructions of large areas, such as a complete crop row, from
the information directly supplied by the RGB-D sensor is shown as well as a
procedure to correct the drift that appears in the reconstruction of crop rows. All
these methods were tested and validated in real ﬁelds at diﬀerent times throughout
2016. The development presented in this paper is a promising technology to
achieve better crop management, which will increase crop yield.
Keywords: 3D woody crop reconstruction · Low cost RGB-D sensor · Crop
inspection platform
1
Introduction
Agriculture is the primarily land use in the European Union, covering 40% of the total
land area [1]. A common desire of all farmers is to know their crop yields, since accurate
yield predictions help they improve crop quality and reduce operational costs by making
better crop management decisions. Yield estimation is usually based on the knowledge
of the crop, historical data, meteorological conditions and manual samplings performed
by operators. Manual sampling is a time-consuming, labour-intensive, and inaccurate
process, since the number of samples is often too small to capture the magnitude of
variations in yield in each crop block. Thus, it is extremely important to ﬁnd an auto‐
mated and eﬃcient alternative that can accurately capture the spatial and temporal
variations of a crop. Vehicles equipped with on-board sensing equipment are a promising
choice among the various means of collecting well-structured information. The use of
medium-sized platforms for full crop inspecting is a suitable choice to minimise soil
compaction. This makes possible to perform more than one sampling throughout the
year because of the minimal impact on the crop.
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_20

On the other hand, 3D reconstruction of woody crops using non-destructive methods
is a valuable technique for decision-making processes. The use of sensors for the char‐
acterisation of crops leads to a better understanding of the processes involved in tree
development throughout the life cycle. With information obtained from a 3D recon‐
struction of the crop, important parameters such as growth status, height, shape, biomass,
need for nutrients, health status, etc. can be estimated. The use of the information
extracted from the 3D reconstructions can improve the decisions made related to crop
management and contribute to creating new protocols to improve the proﬁtability and
health of the plants.
Currently, the Microsoft Kinect v2 RGB-D sensor, based on ToF (time-of-ﬂight)
technology, is a very eﬀective device among 3D vision systems thanks to its low cost
and good performance. Kinect v2 sensors have been used for the characterisation of
plants in agriculture. In [2], the authors compared two low-cost 3D systems, including
the Kinect v2 sensor, with an expensive high-precision laser scanner and concluded that
low cost systems could replace the more expensive scanner in several plant phenotyping
scenarios. The use of the Kinect v2 sensor is proposed in [3] to determine the volume
of weeds in maize crops and to deﬁne their treatment period. The results suggest that
this sensor can be a high precision device in estimating the volume of weeds and deter‐
mining the state of the crop. In [4] a Kinect v2 sensor was used to estimate the volume
of onions, showing that the calculated volume was directly related to the estimation with
measurements from the sensor.
Although 3D reconstruction is a research line with numerous and important works
in computer vision, the emergence of a type of cameras with good performance and low
cost in last years, like above-mentioned Microsoft Kinect and Asus Xtion sensors, which
provide information of distance to the objects closest to the scene, have opened up new
possibilities in 3D reconstruction. Several of the recent researches have been focused
on acquiring 3D scene reconstruction using the depth images supplied by these new
sensors. Thus, diﬀerent techniques that employ distinct types of accelerated data struc‐
tures using graphic hardware to combine consecutive depth images with a certain degree
of overlap have been developed. Each technique has its own advantages and disadvan‐
tages in terms of speed, scale and quality of the reconstruction.
Some methods use a voxel structure (a voxel represents a value on a regular grid in
3D space) to store the 3D sensor information [5–7]. A well-known example is the method
described in [6], which generates high quality 3D reconstructions [8] and was adopted
by KinectFusion [9, 10], the 3D reconstruction method included by Microsoft in its
software development kit, Kinect for Windows Software Development Kit 2.0 [11].
However, the method presents an important constraint; it restricts the reconstruction to
volumes up to 8 m3. This justiﬁes the emergence of variants on the method that allow
reconstructions of larger volumes using voxel structures [12–14]. Other strategies
proposed the use of hierarchical data structures that subdivide space eﬀectively, but
these strategies do not parallelise eﬃciently given added computational complexity
[15, 16].
One of the problems with all 3D reconstruction methods is that they estimate the
position and orientation of the sensor with the information from its images, i.e. there
may be slight variations among the position and orientation calculated and the actual
240
J.M. Bengochea-Guevara et al.

values obtained by the sensor. This happens primarily with similar scenes, such as crops,
where the same structure is repeated over and over again, appearing as analogous infor‐
mation (canopies full of similar leaves to analogous distances from the sensor). These
slight variations in position and orientation calculated may give rise to a drift that causes
deformations in 3D reconstruction, with it being more pronounced with the greater size
of the reconstruction.
The overall objective of this work is shown a method to generate automatic 3D re-
constructions of large areas, such as a complete crop row, from the information directly
supplied by the Microsoft Kinect v2 sensor on-board an inspection vehicle. Additionally
the paper presents a procedure developed to correct the drift that appears in the 3D
reconstructions of long rows.
2
Materials and Methods
2.1
Field Platform
The ﬁeld platform (Fig. 1) is based on a Renault Twizy Urban 80 model, which has a
13 kW electric motor and is able to travel up to 80 kmh-1. The vehicle has an autonomy
of approximately 80 km. The ﬁeld platform is an ultra-compact vehicle, with a length
of 2.32 m, width of 1.19 m, height of 1.46 m and unladen weight of 450 kg. The electric
motor of the vehicle allows vibration free speeds below 3 kmh-1, which is convenient
for high-quality information acquisition.
Fig. 1. Electric inspection platform
An aluminium support structure has been integrated for easy placement of sensors.
Two devices were integrated, adapting their positions to the crop features of each
sampling. The ﬁrst one is a Microsoft Kinect v2 RGB-D sensor, which at 30 fps, supplies
RGB images with a resolution of 1920 × 1080 pixels together with depth information
with a resolution of 512 × 424 pixels. Depth data is obtained by using ToF technology.
The depth measurement range of the sensor is 0.5–4.5 m, but outdoors the maximum
range is smaller. Thus, studies carried out outdoors with sunny daytime illumination
3D Monitoring of Woody Crops Using a Medium-Sized Field Inspection Vehicle
241

show that the sensor supplies valid depth measurements up to 1.9 m. The distance
increases until reaching 2.8 m with the diﬀuse illumination of an overcast day [17].
The other on-board sensor is a Canon EOS 7D reﬂex camera, which, at 2 fps, supplies
high-quality RGB images with a resolution of 2592 × 1728 pixels. Both sensors were
connected to the on-board computer, which has an Intel Core i7-4771@3.5 GHz
processor, 16 GB of RAM, and an NVIDIA GeForce GTX 660 graphic card. Further‐
more, the inspection vehicle is equipped with an R220 Hemisphere RTK-GPS receiver,
which provides location data at 20 Hz sample rate with an error below 2 cm.
To get an idea of what a journey of 80 km (range of vehicle) involves in inspection,
we analysed how many hectares would be covered in the case of a woody crop inspected
with RGB-D sensors. If the woody crop is a vineyard, space between crop rows are
usually 2 m wide. If two RGB-D sensors are placed on both sides of the vehicle to take
images of each row while the vehicle advances along the lane, it would cover a total of
16 ha with its 80 km of range, if the movements in crop headers, to change lane, are not
taken into account. As the average size of a vineyard in Spain is 1.8 ha [18], in the best-
case scenario, this platform on a single charge could inspect around 9 vineyards. In the
case of carrying a single RGB-D sensor, the vehicle would be able to cover 8 ha, since
would have to cover the same lane twice; therefore, it could inspect more than 4 vine‐
yards of 1.8 ha under a single charge. The covered area could be somewhat reduced,
considering the consumption of equipment connected to the battery of the vehicle and
movements made in the crop headers.
The inspection plan to be followed by the platform is generated by a path planner
[19], which can be formulized as the well-known Capacitated Vehicle Routing Problem,
as stated in [20]. Basically, the problem consists of determining the best inspection route
that provides complete coverage of the ﬁeld considering features such as ﬁeld shape,
crop row direction, the type of crop and some characteristics of the platform, such as
the turning radii or the number of sensors on-board. Therefore, the planner determines
the order for performing the lane analysis in such a manner that an established optimi‐
sation criterion gets minimal.
While this mobile platform is prepared to inspection annual crops (maize, cereal,
etc.) and multi-annual crops (orchards, vineyards, etc.) with its on-board sensors, the
present work is focused on the inspection of woody crops with the information supplied
by the RGB-D sensor.
2.2
3D Reconstruction
After studying diﬀerent 3D reconstruction methods [12–14, 16], the algorithm described
in [14] was selected for the 3D reconstruction of woody crops. This method provides
good results in large area reconstruction (fundamental in this application) from the
information directly supplied by the Microsoft Kinect sensor.
The method extends the algorithm proposed by [6] to reconstruct large surfaces using
the fusion of diﬀerent overlapped depth images, storing information only on the voxels
closest to the observed object. All other voxels are not allocated in the memory. In this
way, the need to have a complete regular voxel grid stored in the memory is eliminated,
242
J.M. Bengochea-Guevara et al.

given the computational advantages that this involves. To exploit this aspect, the
implemented procedure uses a hash table to store the voxels.
Given a new input depth image and known camera position, the ray casting technique
[21] is used to project a ray from the camera focus for each pixel of the input depth
image in order to determine the voxels in the 3D world that cross each lightning. In this
way, the voxels related to the depth information are determined.
Once the surface of the scene has been extracted using the ray casting technique, this
information is used to estimate the position and orientation of the camera (6 degrees of
freedom) when a new input image arrives. For this, a variant of the ICP (Iterative Closest
Point) algorithm [22] is used.
To dump the information of the voxels to a triangular mesh that represents the 3D
reconstruction of the scene, the Marching Cubes algorithm [23] is used. The 3D recon‐
struction obtained represents a woody row, but it does not appear to be as straight as the
original due to the drift eﬀect (Fig. 2).
Fig. 2. 3D reconstruction of a woody row with presence of drift
A method is developed to correct this drift using minimal scene information, such
as the location of the starting and ending points of the row as well as the fact that trees
are planted in straight lines (sowing with precision seeder).
First, the algorithm divides the whole mesh into smaller meshes with the same
number of vertices. It then calculates the centroid for each of these meshes using the
position coordinates of its vertices (Fig. 3a). Note that although each mesh contains the
same number of vertices, it does not represent the same length of the woody row because
the vertices distribution in the whole mesh is not uniform. The number of pieces of
smaller meshes may be estimated from the row total length. Next, the line that joins all
the centroids (model line) is built and used to divide the initial mesh into same-length
sections separated by planes perpendicular to the line in each separating point (Fig. 3b).
(a)
(b)
Fig. 3. (a) Model line by using centroids. Note that centroids are not equidistant (b) Crop row
divided into sections. Note that all sections have the same length.
3D Monitoring of Woody Crops Using a Medium-Sized Field Inspection Vehicle
243

Since the model line goes over the crop line, the vector that indicates the direction
of the crop in each section can be calculated using the starting and ending segment points
of the model line where the section is contained. Ideally, the direction of this vector
should coincide with the direction of the actual crop row. However, this does not happen
due to the drift that appears in the 3D reconstruction. To solve this and correct the drift,
each section must be corrected so that it is rotated to align the vector that indicates the
model line in that section with the actual direction of the crop. The rotational formula
of Rodrigues [24] is used to calculate the rotation matrix applied to each section.
Once the drift of each section has been corrected, the sections are aligned, thus
correcting the drift produced during 3D reconstruction.
3
Results
Several tests were conducted in 2016 in the vineyards owned by Codorniu S.A. (Raimat,
Lleida, Spain). The inspection platform moving at 3 km/h was used to collect the infor‐
mation. The sensor Kinect v2 was mounted at approximately 1.4 m in height with a 10º
pitch angle, oriented to the crop rows, and approximately 1 m from the crop row (Fig. 4).
Fig. 4. Sampling in vineyards owned by Codorniu S.A. (Raimat, Lleida, Spain) in May 2016
From each row inspected, the starting and ending geographical positions of the row,
supplied by the RTK-GPS receiver from the vehicle, were stored.
Figure 5 shows examples of the information provided by Kinect v2 sensor in the
vineyard in the tests conducted in May 2016. Figure 5a shows the RGB information of
the scene. Figure 5b shows a false colour representation of the depth information
(distance of objects in the scene), where near objects appear in red and further ones in
blue, and the rest of the intermediate objects are shown in various shades of orange,
yellow and green, depending on their distance from the sensor. From the depth images,
it can be seen that the sensor range of operation meets the inspection requirements of
the vineyard rows because objects of no interest are ignored, such as those close and the
distant areas that usually contain other vineyard rows.
244
J.M. Bengochea-Guevara et al.

(a)
(b)
(c)
(d)
Fig. 5. (a) and (c) RGB images supplied by Kinect v2 sensor; (b) and (d) Depth images supplied
by the sensor at the same time as (a) and (c)
With the information supplied by the Kinect v2 sensor and stored in the vehicle’s
on-board computer, the 3D reconstruction of the sampled rows of vines was performed.
For that, a desktop computer with an Intel Core i7-6900K@3.2 GHz processor, 64 GB
of RAM, and an NVIDIA GeForce GTX Titan X graphics card was used. Figure 6 shows
an example of one of the 3D reconstructions generated, and Fig. 7, shows the 3D mesh
structure obtained in one of these reconstructions.
Fig. 6. 3D reconstruction of a row of vines
3D Monitoring of Woody Crops Using a Medium-Sized Field Inspection Vehicle
245

Fig. 7. Detail of a 3D reconstruction of a vine that shows the mesh obtained from a point cloud
As discussed before, when performing 3D reconstructions of long crop rows, a drift
may appear that causes deformation in the reconstruction. Figure 8 shows the deforma‐
tion that appears in the reconstruction of one of the sampled vineyard rows of 85 m
length.
Fig. 8. 3D reconstruction of a row of vines with signiﬁcant presence of drift
Using the method described above, it is possible to correct the drift obtained during
the reconstruction of the rows. The vineyard row of Fig. 8 is divided in sections of 5 m,
and, during the process, the rotation angle of each section, to use in the Rodrigues’
rotation formula to correct the drift, is calculated. Table 1 shows the calculated rotation
angles.
246
J.M. Bengochea-Guevara et al.

Table 1. Calculated rotation angles applied to each section to correct it.
Section
Angle (°)
1
0.96
2
6.34
3
4.95
4
11.50
5
5.59
6
6.67
7
5.31
8
5.75
9
5.66
10
4.31
11
5.60
12
4.15
13
5.15
14
6.65
15
8.06
16
6.88
17
3.11
After the drift correction process ﬁnishes, the corrected reconstruction of the row
displayed in Fig. 9 is obtained.
Fig. 9. 3D reconstruction of the row of vines of the Fig. 8 with the drift corrected
The average generation time of a 3D reconstruction with the corrected drift for a
50 m vineyard row using the computer described above is approximately 15 min. The
average number of vertices in a reconstruction of a 50 m row is around 8 million.
Figure 10 shows four vineyard rows reconstructions belonging to a plot of ecological
vineyard with a presence of highly developed vegetation cover sampled in July 2016,
in a false colour scale representation according to its height from the ground, and the
path followed by the vehicle to acquire data (red).
3D Monitoring of Woody Crops Using a Medium-Sized Field Inspection Vehicle
247

Fig. 10. Four vineyard rows reconstructions in a false colour scale representation according to
its height from the ground, and path followed by the vehicle to acquire data (red)
Furthermore, the 3D reconstruction method with the drift correction was tested in a
plot of pear trees. Due to the size of the plot, two passes had to be made: one to sample
the top of the trees and the other one for their bottom, joining the reconstructions of both
passes together later. A 3D reconstruction of a row of pear trees with the drift corrected
can be seen in Fig. 11. Figure 12 shows a part of this row at a greater magniﬁcation.
Fig. 11. 3D reconstruction of a row of pear trees
Fig. 12. 3D reconstruction of a part of a pear trees row
248
J.M. Bengochea-Guevara et al.

4
Conclusions
This paper describes a crop inspection system. The mobile platform, based on a commer‐
cial electric vehicle, is equipped with diﬀerent on-board sensors to scan annual crops
(maize, cereal, etc.) and multi-annual crops (orchards, vineyards, etc.).
The use of a low-cost RGB-D sensor has been tested and validated outdoors under
uncontrolled lighting conditions. This sensor has been used for the inspection of woody
crops, enabling 3D reconstruction of these scenarios from the RGB image and the depth
information supplied by the sensor.
Diﬀerent methods have been studied to perform automatic 3D reconstructions, since
it is fundamental to generate the reconstruction of large areas, such as a complete crop
row. Finally, the algorithm implemented provides good results in the reconstruction of
large areas from the information directly supplied by the RGB-D sensor.
A method to correct the drift that appears in the reconstruction of crop rows has been
developed and tested in actual ﬁelds. This method uses the Rodrigues’ rotation formula
and a minimal scene information, such as the location of the starting and ending points
of the row, and requires that crops are planted in straight lines.
The work presented in this paper shows a promising technology that can achieve
better crop management.
Acknowledgements. The Spanish Government has provided full and continuing support for this
research work through project AGL2014-52465-C4-3-R (3DWeed). The authors wish to thank
Codorniu S.A company for the use of the facilities on the estate of Raimat and extend their
gratitude to Jordi Recasens and his team (Weed Science and Plant Ecology Research Group of
the UdL) for their invaluable help in the ﬁeld trials. Karla Cantuña thanks the service commission
for the remuneration given by the Cotopaxi Technical University. The authors also wish to
acknowledge the ongoing technical support of Damián Rodríguez.
References
1. Eurostat. http://ec.europa.eu/eurostat/web/agri-environmental-indicators/overview. Last
accessed 6 July 2017
2. Paulus, S., Behmann, J., Mahlein, A.-K., Plümer, L., Kuhlmann, H.: Low-cost 3D systems:
suitable tools for plant phenotyping. Sensors 14(2), 3001–3018 (2014)
3. Andújar, D., Dorado, J., Fernández-Quintanilla, C., Ribeiro, A.: An approach to the use of
depth cameras for weed volume estimation. Sensors 16(7), 972 (2016)
4. Wang, W., Li, C.: Size estimation of sweet onions using consumer-grade RGB-depth sensor.
J. Food Eng. 142, 153–162 (2014)
5. Hilton, A., Stoddart, A., Illingworth, J., Windeatt, T.: Reliable surface reconstruction from
multiple range images. In: Proceedings of 4th European Conference on Computer Vision,
United Kingdom, pp. 117–126. Springer (1996)
6. Curless, B., Levoy, M.: A volumetric method for building complex models from range
images. In: Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive
Techniques, United States, pp. 303–312. ACM (1996)
3D Monitoring of Woody Crops Using a Medium-Sized Field Inspection Vehicle
249

7. Wheeler, M.D., Sato, Y., Ikeuchi, K.: Consensus surfaces for modeling 3D objects from
multiple range images. In: Proceedings of Sixth International Conference on Computer
Vision, India, pp. 917–924. IEEE (1998)
8. Zhou, Q.-Y., Koltun, V.: Dense scene reconstruction with points of interest. ACM Trans.
Graph. 32(4), 112 (2013)
9. Newcombe, R.A., Izadi, S., Hilliges, O., Molyneaux, D., Kim, D., Davison, A.J., Kohi, P.,
Shotton, J., Hodges, S., Fitzgibbon, A.: KinectFusion: real-time dense surface mapping and
tracking. In: Proceedings of 10th IEEE International Symposium on Mixed and Augmented
Reality, ISMAR, Switzerland, pp. 127–136. IEEE (2011)
10. Izadi, S., Newcombe, R.A., Kim, D., Hilliges, O., Molyneaux, D., Hodges, S., Kohli, P.,
Shotton, J., Davison, A.J., Fitzgibbon, A.: Kinectfusion: real-time dynamic 3d surface
reconstruction and interaction. In: Proceedings of the Annual Conference on Computer
Graphics and Interactive Techniques, SIGGRAPH, Canada, p. 23. ACM (2011)
11. Microsoft 
Kinect 
for 
Windows 
Software 
Development 
Kit 
2.0. 
https://
developer.microsoft.com/es-es/windows/kinect/develop. Last accessed 6 July 2017
12. Steinbrucker, F., Kerl, C., Cremers, D.: Large-scale multi-resolution surface reconstruction
from RGB-D sequences. In: Proceedings of the IEEE International Conference on Computer
Vision, Australia, pp. 3264–3271. IEEE (2013)
13. Whelan, T., Kaess, M., Fallon, M., Johannsson, H., Leonard, J., McDonald, J.: Kintinuous:
spatially extended kinectfusion. In: Proceedings of Workshop RGB-D, Advanced Reasoning
with Depth Cameras, Australia, pp. 573–580 (2012)
14. Nießner, M., Zollhöfer, M., Izadi, S., Stamminger, M.: Real-time 3D reconstruction at scale
using voxel hashing. ACM Trans. Graph. 32(6), 169 (2013)
15. Zeng, M., Zhao, F., Zheng, J., Liu, X.: Octree-based fusion for realtime 3D reconstruction.
Graph. Models 75(3), 126–136 (2013)
16. Chen, J., Bautembach, D., Izadi, S.: Scalable real-time volumetric surface reconstruction.
ACM Trans. Graph. 32(4), 113 (2013)
17. Fankhauser, P., Bloesch, M., Rodriguez, D., Kaestner, R., Hutter, M., Siegwart, R.: Kinect
v2 for mobile robot navigation: evaluation and modeling. In: Proceedings of 2015
International Conference on Advanced Robotics (ICAR), Turkey, pp. 388–394. IEEE (2015)
18. Ministerio de Agricultura, Pesca y Alimentación. http://www.mapama.gob.es/es/estadistica/
temas/estadisticas-agrarias/memoﬁnalvinedo_tcm7-443391.pdf. Last accessed 6 July 2017
19. Conesa-Munoz, J., Bengochea-Guevara, J.M., Andujar, D., Ribeiro, A.: Eﬃcient distribution
of a ﬂeet of heterogeneous vehicles in agriculture: a practical approach to multi-path planning.
In: Proceedings of the 2015 IEEE International Conference on Autonomous Robot Systems
and Competitions (ICARSC), Portugal, pp. 56–61. IEEE (2015)
20. Bochtis, D.D., Sørensen, C.G.: The vehicle routing problem in ﬁeld logistics part I. Biosyst.
Eng. 104(4), 447–457 (2009)
21. Roth, S.D.: Ray casting for modeling solids. Comput. Graph. Image Process. 18(2), 109–144
(1982)
22. Chen, Y., Medioni, G.: Object modelling by registration of multiple range images. Image Vis.
Comput. 10(3), 145–155 (1992)
23. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3D surface construction
algorithm. In: Proceedings of the 14th Annual Conference on Computer Graphics and
Interactive Techniques, SIGGRAPH, United States, pp. 163–169. ACM (1987)
24. Rodrigues, O.: Des lois géométriques qui régissent les déplacements d’un système solide dans
l’espace: et de la variation des cordonnées provenant de ces déplacements considérés
indépendamment des causes qui peuvent les produire. J. Math. Pures Appliquées 5, 380–440
(1840)
250
J.M. Bengochea-Guevara et al.

A Vision-Based Strategy to Segment and Localize Ancient
Symbols Written in Stone
Jaime Duque-Domingo
(✉), P. Javier Herrera, Carlos Cerrada
(✉), and José A. Cerrada
Dpto. Ingeniería de Software y Sistemas Informáticos, ETSI Informática,
UNED, C/Juan del Rosal, 16, 28040 Madrid, Spain
jaimeduque@amenofis.com,
{pjherrera,ccerrada,jcerrada}@issi.uned.es
Abstract. This work proposes an automatic method to detect ancient symbols
written in stone. The proposed method takes into account well-known techniques
used in computer vision to identify the contour of the symbols in the image. The
two-stage method consists of segmentation and localization processes. Segmen‐
tation process includes a pre-processing step, edge detection and thresholding.
Localization process is based on two conditions that take into account several
parameters, like the distance between points, and the orientation and the
continuity of the edges. This proposal has been applied to localize Egyptian
cartouches (borders enclosing the name of a king) and stonemason’s marks from
images obtained under varying lighting conditions (controlled and natural
lighting). The proposed method is compared favorably against other methods
based on chain coding, neural networks and statistical correlation. The promising
results give new possibilities to identify and recognize complex symbols and
ancient texts.
Keywords: Image segmentation · Edge detection · Object localization
1
Introduction
Several methods have been developed in order to detect edges and localize objects
[1–3]. Recent works have studied how to extract and decipher ancient glyphs of the
Maya civilization [4, 5]. These methods work properly when the edges are extracted
completely intact. However, the effects of time, exposure and even vandalism make
the process difficult. Other strategies have been revised in this work. It was consid‐
ered extracting the symbols using a conversion to grey scale and applying a
threshold and the Hu moments [6, 7]. Nevertheless the symbols have the same color
than other parts of the background and the extraction process becomes difficult. A
strategy of regions by frontier based on the gradient, the Laplacian and the Hu
moments was used [1]. For this purpose the edges were extracted using Sobel, Canny
and Susan methods [1, 8, 9]. The drawback is that sometimes there is noise or edges
that are not well defined.
The Generic Hough Transform (GHT) is a well-known method that can be used to
obtain a concrete object using its edge [10]. However, the main drawback is that it only
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_21

considers the points that exactly match with the model. If there is a big diﬀerence
between the points of the edge and the edge of the image, it returns non-valid results.
Other methods as Statistical Shape Models (SSM), Active Shape Models (ASM),
and Active Appearance Models (AAM) are used to ﬁnd objects with a similar shape
using key points or landmarks and the textures between these landmarks [11]. These
methods work better when there is a clear diﬀerence between the object and the rest of
the image. In this work, the symbols have the same color than the rest of the scene, and
the edges are very soft. Finally, the Active Contours Algorithm was also tested in order
to detect the edges of the symbols used in this work [12].
As it has been seen, the study of the state-of-the-art proposes several techniques of
computer vision. This work provides a new method to localize ancient symbols tested
on images of Egyptian cartouches (i.e. borders enclosing the name of a king) and stone‐
mason’s marks [13] as shown in Fig. 1. This work takes into account some requirements:
(1) Ancient symbols were written in low-relief or high-relief art. In the low-relief the
symbols were incised in the stone. In the high-relief the rest of the symbol was incised.
(2) The majority of symbols that have survived until the present day have suﬀered the
eﬀects of time, exposure and even vandalism.
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
(k)
(l)
(m)
(n)
(o)
(p)
Fig. 1. (a–h) Egyptian cartouches from the Abydos King List; (i–p) stonemason’s marks from
the castle of La Adrada (Ávila, Spain).
252
J. Duque-Domingo et al.

The proposed method works with symbols written in stone, but it would be able to
accept painted symbols. Besides, it works in low-relief and high-relief writing. Due to
bad state of preservation of some symbols, it can appears noise that is properly detected,
but if it is broken, or some part has been lost, there are more problems to process the
recognition.
This paper is organized as follows. Section 2 describes the segmentation process.
Section 3 details the localization process. Section 4 analyses and compares the perform‐
ance of the proposed method against three methods based on chain coding, neural
networks and statistical correlation. Section 5 presents the conclusions and future work.
2
Segmentation Process
The segmentation process consists of three steps where well-known image processing
techniques are used. The main challenge of this stage is to clarify the contour of each
symbol in the image. The steps of the segmentation process are the following: pre-
processing, edge detection and thresholding.
2.1
Pre-processing Step
The original image is converted to grey-scale as in (1). A transformation from RGB to
YIQ is realized, and the process is applied to Y channel. The YIQ (Y - luma; I - in-phase;
Q - quadrature) representation is usually employed in color image processing transfor‐
mations [1].
Y = 0.299 ⋅R + 0.587 ⋅G + 0.114 ⋅B
(1)
After this transformation, a median ﬁlter and a morphological erosion are applied.
These ﬁlters are used to remove the noise without reducing the edges signiﬁcantly. This
is a typical pre-processing step to improve the results of later processing [1]. Median
ﬁlter replaces each pixel with the median value of the surrounding N-by-N neighbor‐
hood. Erosion removes pixels on object boundaries, concretely the value of the output
pixel is the minimum value of all the pixels in the input pixel’s neighborhood. In a binary
image, if any of the pixels is set to 0, the output pixel is set to 0.
2.2
Edge Detection
The Canny edge detector is used to obtain the edges of the image [8]. Common edge
detection algorithms include Sobel, Canny, Prewitt, Roberts, and fuzzy logic methods
[1]. The Canny method diﬀers from the other edge-detection methods in that it uses two
diﬀerent thresholds (to detect strong and weak edges), and includes the weak edges in
the output only if they are connected to strong edges. This edge detector is therefore
more likely to detect true weak edges instead of noise.
A Vision-Based Strategy to Segment and Localize Ancient Symbols
253

2.3
Thresholding Phase
Finally, the Iterative Self-Organizing Data Analysis Technique (ISODATA) threshold
is used to detect the most important edges. ISODATA is a simple iterative technique
commonly used in image segmentation for choosing a threshold [14]. The objective is
to split non-homogeneous regions into two sub-regions (objects and background) as
follows:
(1) A threshold T is set to a random value.
(2) The image is binarized using T.
(3) The mean values 𝜇1, 𝜇2 of the two sub-regions (objects and background) generated
with T are obtained. 𝜇1 is the mean value of all values less or equal to T. 𝜇2 is the
mean value of all values greater to T.
(4) A new threshold is calculated as T =
(
𝜇1 + 𝜇2
)
∕2.
(5) Repeat steps (2) to (4) until T stops changing its value.
This technique reported good results in previous works compared to other thresh‐
olding techniques like Otsu, minimum error and k-means [15, 16]. Edges with less than
P pixels are deleted to reduce the noise. In this work P is proportional to 10% of the
width of the image after trial and error.
3
Localization Process
This section explains the method developed in order to localize the objects of interest
in the image. A search of symbols is realized and the technique used is described as
follows. A symbol is identiﬁed when at least T percentage of the points of the symbol’s
contour satisfy (a) and (b) conditions (T = 70% in this work):
(a) The Euclidean distance between a point of the symbol’s contour (Ps) and a point
of the background (Pb) is lower than d. d is set to 3 pixels for a symbol of 100 pixels
width. In Fig. 2, if the symbol’s contour is marked in orange and the background’s
contour one is colored in blue, the distance between Ps(3,4) and the image is 2
because Pb(4,2) and Pb(5,3) are the closest points of the symbol, and the distance
(a)
(b)
Fig. 2. Distance between points Ps (a) and points Pb (b).
254
J. Duque-Domingo et al.

to them is 2. The minimum distance from Ps(3,7) to the background is higher than
3 because the closest points are Pb(6,4), Pb(6,5) and Pb(6,6). To calculate the
distance it can be used a convolution mask over the central point, and it will be
increased each iteration.
(b) The angle of the contour line in Pb minus the angle of the contour line in Ps is less
than MA. The contours have been obtained by using the Canny algorithm, which
produces non-maximum suppression (the width of the contour is 1 pixel), so the
contours that include Pb and Ps have 1 pixel width. MA is calculated as in (2):
MA = (100 −T) ⋅𝜋∕2
100
(2)
In this work MA is equal to 0.15 𝜋. The angle ϕ of a point respect to their neighboring
points is obtained as in (3) where A is the number of pixels in the region, xi are the x
positions with pixels, and yi are the y positions with pixels. The gradient of a point is
not used in this proposal because the gradient gives the angle change of a point in a
segment.
Sx = ∑xiSy ; Sy = ∑yi
Sxx = ∑x2
i Syy ; Syy = ∑y2
i ; Sxy = ∑xiyi
Mxx = Sxx −
S2
x
A ; Myy = Syy −
S2
y
A ; Mxy = Sxy −
Sx ⋅Sy
A
𝜙= tan−1
⎡
⎢
⎢
⎢⎣
Mxx −Myy +
√(Mxx −Myy
)2 + 4 ⋅M2
xy
2 ⋅Mxy
⎤
⎥
⎥
⎥⎦
(3)
If a point Pb satisﬁed (a) and (b) conditions, Pb is assigned to Ps. If a background’s
point (Pbk) has been assigned to a symbol’s point (Psk) then Pbk is not used again in the
process.
The processed region is identiﬁed as a symbol if it is satisﬁed the T percentage of
the points assigned to the image and the number of continue lines that compose the
contour assigned to the image (i.e. Pb points that have been linked to Ps) is lower than
L (L = 20 in this work). This restriction prevents from some detected cases where noise
provoke errors in detection. As a summary, Fig. 3 shows a scheme of the segmentation
and localization processes described in this work.
Input (Image)
PRE-PROCESSING
•
RGB to YIQ
•
Median filter
•
Erosion
EDGE DETECTOR
•
Canny
THRESHOLDING
•
ISODATA
SYMBOL 
LOCALIZATION
Output (Symbol)
IMAGE SEGMENTATION
Fig. 3. Scheme of the segmentation and localization processes.
A Vision-Based Strategy to Segment and Localize Ancient Symbols
255

4
Results
This section presents the results obtained by the proposed method. The 152 images used
in this work were taken on diﬀerent days and therefore under varying lighting conditions
(controlled and natural lighting): 76 images were obtained from the Abydos King List
(group 1), 36 images from stonemason’s marks datasets [13, 17] (group 2), and 40
images from the castle of La Adrada (group 3). Image resolutions are: group
1 − 220 × 330, group 2 − 800 × 600, and group 3 − 2592 × 1456. The Abydos King
List was found on a wall in the Temple of Seti I at Abydos (Egypt). Nowadays, it is
exhibited at the British Museum in London (UK). It consists of three rows of thirty-eight
cartouches in each row. Figure 1(a–h) belongs to the Abydos King List. The castle of
La Adrada is located in the province of Ávila (Spain), and it was built in the XIV–XVI
centuries. Figure 1(i–p) belongs to the castle of La Adrada. For visualization purposes
the results of eight images are presented in Tables 1 and 2.
Table 1. Intermediate and ﬁnal visual results of four images of Egyptian cartouches.
For comparative purposes, the proposed method is compared against three methods
based on chain coding, neural networks and statistical correlation [17]. All methods have
been implemented in MATLAB. Chain codes are used for shape analysis because repre‐
sent a boundary by a connected sequence of straight-line segments of speciﬁed length
and direction [1]. In this work, this representation is based on 8-connectivity of the
segments as one can see in Fig. 4(a, b). The direction of each segment is coded by using
the following scheme: N, NE, E, SE, S, SW, W, and NW.
256
J. Duque-Domingo et al.

(a)
(b)
(c)
(d)
Fig. 4. (a, b) Chain code representation is based on 8-connectivity of the segments; Symbols
detection by means of: (c) MLP; (d) SCA.
Once chain codes are extracted, two neural networks are trained as in [17] in order
to detect symbols: Hopﬁeld network (HN) and multilayer perceptron (MLP). The stat‐
istical correlation algorithm (SCA) proposed in [17] and based on standard deviations
and covariance is also compared. Figure 4(c, d) shows two examples of symbols detec‐
tion by means of MLP and SCA methods, respectively.
Table 3 displays the averaged percentage of errors and standard deviations obtained
against ground truth images. Symbols belonging to Egyptian cartouches were better
detected than stonemason’s marks. The diﬀerences between the same cartouche in
diﬀerent writings are not very big because Egyptian scribes used a similar model to write
in the material. This makes easier the localization process as one can see from the results
obtained. Similar results were observed for MLP and SCA in terms of percentage and
low standard deviation. Although SCA obtains a good performance for stonemason’s
marks detection, the proposed method outperforms the other three methods tested.
Symbols belonging to stonemason’s marks do not present a speciﬁc pattern. For this
reason, the localization process of these symbols in the image becomes more diﬃcult
than for Egyptian cartouches. Nevertheless the averaged percentage of errors is 16.1.
Table 2. Intermediate and ﬁnal visual results of four images of stonemason’s marks.
A Vision-Based Strategy to Segment and Localize Ancient Symbols
257

For the whole set of images used in this work, the averaged percentage of errors of the
proposed method is 11.2.
Table 3. Averaged percentage of errors and standard deviations obtained through the proposed
method against HN, MLP and SCA.
%
𝜎
HN
27.9
0.6
Abydos King list dataset
24.6
0.6
Stonemason’s marks dataset
31.2
0.7
MLP
14.8
0.4
Abydos King list dataset
12.2
0.5
Stonemason’s marks dataset
17.5
0.4
SCA
13.9
0.7
Abydos King list dataset
12.7
1.2
Stonemason’s marks dataset
15.1
0.3
Proposed method
11.2
1.0
Abydos King list dataset
6.3
0.7
Stonemason’s marks dataset
16.1
1.3
5
Conclusions and Future Work
This work presents an automatic method for segmentation and localization of two kind
of ancient symbols: Egyptian cartouches and stonemason’s marks. The proposed method
takes into account well-known techniques used in computer vision, conveniently
adapted for this purpose. The two-stage method consists of segmentation and localiza‐
tion processes. Segmentation process includes a pre-processing step, edge detection and
thresholding. Morphological transformations and the Canny and the ISODATA tech‐
niques are applied in this stage. Localization process is based on two conditions that
take into account several parameters, like the distance between points, and the orienta‐
tion and the continuity of the edges.
The images used in this work were taken with controlled and natural lighting.
Besides, the majority of ancient symbols that have survived until the present day have
suﬀered the eﬀects of time and exposure. Hence the proposed method successfully copes
with several diﬃculties.
Aiming to deliver a new strategy for enriching the experience of users visiting a
museum, this method may be combined with a positioning system as shown in Fig. 5.
By means of the combination of WiFi Positioning Systems (WPS) and depth cameras
as in [18], users can be located in indoor environments. Portable devices employed in
this system and required to determine the position of users may be used to capture e.g.
fragments of Egyptian hieroglyphs.
258
J. Duque-Domingo et al.

Fig. 5. Components of the system based on RGD-B cameras and WiFi positioning.
Acknowledgements. This work has been developed with the help of the research projects
DPI2013-44776-R and DPI2016-77677-P of the MICINN. It also belongs to the activities carried
out within the framework of the research network CAM Robo-City2030 S2013/MIT-2748 of the
Comunidad de Madrid.
References
1. Gonzalez, R.C., Woods, R.E.: Digital Image Processing, 3rd edn. Prentice Hall, Upper Saddle
River (2008)
2. Papari, G., Petkov, N.: Edge and line oriented contour detection: state of the art. Image Vis.
Comput. 29, 79–103 (2011)
3. Sánchez-González, M., Cabrera, M., Herrera, P.J., Vallejo, R., Cañellas, I., Montes, F.: Basal
area and diameter distribution estimation using stereoscopic hemispherical images.
Photogram. Eng. Remote Sens. 82(8), 605–616 (2016)
4. Roman-Rangel, E., Pallan, C., Odobez, J.M., Gatica-Perez, D.: Analyzing ancient maya glyph
collections with contextual shape descriptors. Int. J. Comput. Vis. 94(1), 101–117 (2011)
5. Roman-Rangel, E., Marchand-Maillet, S.: Shape-based detection of Maya hieroglyphs using
weighted bag representations. Pattern Recogn. 48(4), 1161–1173 (2015)
6. Herrera, P.J., Pajares, G., Guijarro, M., Ruz, J.J., Cruz, J.M., Montes, F.: A featured-based
strategy for stereovision matching in sensors with ﬁsh-eye lenses for forest environments.
Sensors 9(12), 9468–9492 (2009)
7. Herrera, P.J., Dorado, J., Ribeiro, A.: A novel approach for weed type classiﬁcation based on
shape descriptors and a fuzzy decision-making method. Sensors 14, 15304–15324 (2014)
8. Canny, J.: A computational approach to edge detection. IEEE Trans. Pattern Anal. Mach.
Intell. 8(6), 679–698 (1986)
9. Smith, S.M., Brady, J.M.: SUSAN - a new approach to low level image processing. Int. J.
Comput. Vis. 23(1), 45–78 (1997)
10. Ballard, D.H.: Generalizing the hough transform to detect arbitrary shapes. Pattern Recogn.
13(2), 111–122 (1981)
11. Cootes, T.F., Edwards, G.J., Taylor, C.J.: Active appearance models. In: 5th European
Conference on Computer Vision, pp. 484–498. Springer, UK (1998)
A Vision-Based Strategy to Segment and Localize Ancient Symbols
259

12. Zhang, K., Zhang, L., Song, H., Zhou, W.: Active contours with selective local or global
segmentation: a new formulation and level set method. Image Vis. Comput. 28(4), 668–676
(2010)
13. Duque-Domingo, J.: Aplicación para reconocimiento visual y datación de jeroglíﬁcos
egipcios. Máster Universitario en Investigación en Ingeniería del Software y Sistemas
Informáticos, UNED. Final Master Project (2014)
14. El-Zaart, A.: Images thresholding using ISODATA technique with gamma distribution.
Pattern Recogn. Image Anal. 20(1), 29–41 (2010)
15. Gonzales-Barron, U., Butler, F.: A comparison of seven thresholding techniques with the k-
means clustering algorithm for measurement of bread-crumb features by digital image
analysis. J. Food Eng. 74, 268–278 (2006)
16. Macedo-Cruz, A., Pajares, G., Santos, M., Villegas-Romero, I.: Digital image sensor-based
assessment of the status of oat (Avena sativa L.) crops after frost damage. Sensors 11, 6015–
6036 (2011)
17. Jiménez, C.: Modelos simbólico-conexionistas para la segmentación y descripción de Marcas
de Cantero. Máster Universitario en Investigación en Ingeniería de Software y Sistemas
Informáticos, UNED. Final Master Project (2014)
18. Duque-Domingo, J., Cerrada, C., Valero, E., Cerrada, J.A.: Indoor positioning system using
depth maps and wireless networks. J. Sens. 2016, 1–8 (2016)
260
J. Duque-Domingo et al.

Robot Competitions

euRathlon and ERL Emergency: A Multi-domain
Multi-robot Grand Challenge for Search and
Rescue Robots
Alan F.T. Winﬁeld1(✉), Marta Palau Franco1, Bernd Brueggemann2, Ayoze Castro3,
Gabriele Ferri4, Fausto Ferreira4, Xingcun Liu5, Yvan Petillot5, Juha Roning6,
Frank Schneider2, Erik Stengler1, Dario Sosa3, and Antidio Viguria7
1 Bristol Robotics Lab, UWE Bristol, Bristol, UK
alan.winfield@uwe.ac.uk
2 Fraunhofer FKIE, Bonn, Germany
3 PLOCAN, Canary Island, Las Palmas, Spain
4 NATO Centre for Maritime Research and Exploration, La Spezia, Italy
5 Herriot Watt University, Edinburgh, UK
6 University of Oulu, Oulu, Finland
7 FADA-CATEC, Seville, Spain
Abstract. In this paper we outline the euRathlon 2015 and the ERL Emergency
2017 Grand Challenge robotics competitions, and the results and lessons learned
from euRathlon 2015. Staged at Piombino, Italy in September 2015, euRathlon
2015 was the world’s ﬁrst multi-domain (air, land and sea) multi-robot search and
rescue competition. In a mock disaster scenario inspired by the 2011 Fukushima
NPP accident the euRathlon 2015 Grand Challenge required teams of robots to
cooperate to map the area, and missing workers and stem a leak. The second
edition of the competition will be held also in Piombino in September 2017, under
the name of ERL Emergency and as part of the new European Robotics League
initiative.
Keywords: Field robotics · Multi-robot systems · Land air and sea robots ·
Competitions
1
Introduction
A high-level aim of the euRathlon and ERL Emergency projects is to help speed-up
progress towards practical, useable real-world intelligent autonomous robots through
competitions; toward this aim euRathlon and ERL Emergency has created real-world
robotics challenges for outdoor robots in demanding emergency response scenarios.
The euRathlon and ERL Emergency competitions aim to provide real-world robotics
challenges that test the intelligence and autonomy of outdoor robots in demanding mock
disaster-response scenarios inspired by the 2011 Fukushima accident. Focused on multi-
domain cooperation, the 2015 euRathlon and the 2017 ERL Emergency competitions
require ﬂying, land and marine robots acting together to survey the disaster, collect
environmental data, and identify critical hazards. The ﬁrst (land) competition was held
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_22

in 2013 in Berchtesgaden, Germany [1]. In September 2014, the second (sea) competi‐
tion was held in La Spezia, Italy [2, 3]. The ﬁnal euRathlon Grand Challenge (air, land
and sea) was held in Piombino, Italy, from 17th - 25th September 2015. Also, diﬀerent
workshops have been organized in the ﬁve years in order to promote the participation
of new teams in this challenging completion, for example see [4]. Moreover, a second
edition of the euRathlon Gran Challenge, now called ERL Emergency, will be held in
September 2017 also in Piombino, Italy.
This paper proceeds as follows. First we outline the Grand Challenge concept then,
in Sect. 3, we describe the location chosen for euRathlon 2015 and ERL Emrgency 2017
and how the requirements of the Grand Challenge map to the physical environment. The
paper concludes in Sect. 4 by evaluating ﬁrst the competition itself (euRathlon 2015),
including lessons learned, then the performance of the teams in rising to the Grand
Challenge.
2
The Grand Challenge
Inspired by the 2011 Fukushima accident and the subsequent eﬀorts to use robots to
assess internal damage to the NPP buildings [5], we sought to develop a scenario which
would - in some respects at least - provide teams with a comparable challenge. Clearly
there were aspects that we could not replicate, in particular the radiological environment
or chemical hazards - but we were able to oﬀer signiﬁcant challenges to radio commu‐
nication. Other challenges included the weather, which reduced underwater visibility to
less than 1 m, the rough terrain for land robots, and obstructed access routes inside the
building.
Fig. 1. Concept diagram for the euRathlon 2015 Grand Challenge scenario
264
A.F.T. Winﬁeld et al.

Figure 1 shows the concept diagram for the Grand Challenge scenario. The key
physical elements of the scenario are (1) a building on a shoreline which can act in the
role of the ‘reactor’ building, with an internal mock ‘machine room’, (2) valves (stop‐
cocks) in the machine room connected to pipes which lead out of the building and into
the sea, with corresponding underwater valves, (3) damage or debris blocking paths or
entrances outside or inside the building, (4) damage to the pipes and (5) missing workers.
The Grand Challenge scenario comprised three mission objectives – outlined as
follows.
• Mission A: Search for missing workers. Robots must search for two missing workers
represented by mannequins dressed in orange suits, which could be inside the
building, outside the building, ﬂoating on the sea surface near the coast, or trapped
underwater. Teams received bonus points if a worker was found during the ﬁrst
30 min of the Grand Challenge, because in a real scenario the probability of ﬁnding
a missing person alive decreases rapidly with time.
• Mission B: Reconnaissance and environmental survey of a building. Robots must
inspect a building to evaluate damage (represented by markers) and ﬁnd a safe path
to a machine room where valves were located. This required robots to survey the
area, create a map of the building and the outdoor area surrounding it, and locate
objects of potential interest (OPIs) in order to provide situational awareness to the
team.
• Mission C: Pipe inspection and stemming a leak. Robots must localize four pipe
sections on land, localize another four matching pipes underwater, look for damage
to the land pipes and identify a contaminant leak (represented by a marker), reach
the valves in the machine room and underwater, and close the two corresponding
valves in synchrony.
In the published scenario description1, we made it clear that the missions could be
undertaken in any order, or in parallel. The Grand Challenge would be successfully met
if all three mission objectives were met within 100 min, but importantly we did not
specify how the challenge should be met, or with what robots (only limiting their number
and kind).
3
Torre Del Sale - the Competition Site
Securing a location for euRathlon 2015 and ERL Emergency 2017 was challenging
given the requirements. We needed a suitable building on a shoreline and surrounding
areas with safe access for land and ﬂying robots, a safe shallow sea for marine robots
and suﬃcient space for team preparation, organisers and spectators. Equally importantly
we needed all of the necessary permissions to operate land, sea and air robots: for marine
robots from the Port Authority and for ﬂying robots from the Italian Civil Aviation
Authority (ENAC).
The venue selected was an area in front of the ENEL (Italian National Company for
Electricity) electrical power plant in Piombino, Italy. The location oﬀered all the areas
1 http://www.eurathlon.eu/index.php/compete2/eurathlon2015/scenarios2015/.
euRathlon and ERL Emergency
265

needed for the robots, space for hosting participants and public, and also oﬀered a
credible industrial context as a background for the competition. Permission was obtained
from the State Property Authority to make use of a disused historical building on the
sea shore, the Torre del Sale, as the mock reactor building with an internal room playing
the part of the machine room. Figure 2 shows the Torre Del Sale building, and Fig. 3
shows a satellite image of the competition site, with the outdoor land, air and sea robot
areas indicated.
Fig. 2. The Torre del Sale, with the ENEL power plant in the background, and beach to the right
Fig. 3. Competition site, with the Torre del Sale at the left. Image: Google Earth
266
A.F.T. Winﬁeld et al.

4
Evaluation of euRathlon 2015 Competition
4.1
The Competition
A total of 21 teams registered for euRathlon 2015 and, of these, 18 progressed success‐
fully through the qualiﬁcation process. Of those 18, two withdrew one week before the
competition for diﬀerent reasons; both teams did however attend euRathlon 2015 as
visitors.
The 16 teams that participated in euRathlon 2015 are detailed in Table 1. They
comprised a total of 134 team members from 10 countries with ~40 robots. A group
photo is shown in Fig. 4. As shown in Table 1 there were 9 single domain teams, 2 two-
domain teams and 3 three-domain teams. Through a team matching process we actively
encouraged single- and two-domain teams to form combined air, land and sea teams.
This process resulted in 3 new matched teams to complement the existing 3 multi-
domain teams. Thus, of the 16 teams at euRathlon 2015, 10 were able to compete in the
Grand Challenge scenario, as shown in Table 2.
Table 1. Teams with country of origin and domains of participation
Team name
Institution/company
Country
Land Sea
Air
AUGA
ACSM
ES
X
AUV Team TomKyle
University of Applied Sciences
Kiel
DE
X
AVORA
Universidad Las Palmas de Gran
Canaria
ES
X
bebot-team
Bern University of Applied
Sciences
CH
X
X
B.R.A.I.N. Robots
B.R.A.I.N. Robots e. V.
DE
X
X
Cobham
Cobham Mission Systems
DE
X
ENSTA Bretagne Team 1
ENSTA Bretagne (ex ENSIETA)
FR
X
X
X
ENSTA Bretagne Team 2
ENSTA Bretagne (ex ENSIETA)
FR
X
X
X
ISEP/INESC TEC Aerial
ISEP & INESC TEC
PT
X
ICARUS
ICARUS FP7 Project
BE, DE.
PL. PT,
ES
X
X
X
Team Nessie
Ocean Systems Laboratory/Heriot
Watt University
UK
X
OUBOT
Obuda University
HU
X
Robdos Team Underwater
Robdos SRL/Universidad Politc-
ES
X
Robotics
nica de Madrid
SARRUS - Search And
Rescue Robot of UPM &
Sener
UPM SENER
ES
X
UNIFI Team
University of Florence
IT
X
X
Universitat de Grtona
Universitat de Girona
ES
X
euRathlon and ERL Emergency
267

Fig. 4. Group photo of euRathlon 2015 participants
Table 2. Teams participating in the Grand Challenge, showing domains (L = Land, A = Air, S = Sea)
Grand Challenge Teams
AUV Team TomKyle (S) + bebot-team (L)(A)
B.R.A.I.N. Robots (L)(S) + UNIFI Team (S)(A)
Cobhatn (L) + Universitat de Girona (S) + ISEP/INESC TEC Aerial Team (A)
ENSTA Bretagne Team 1 (L)(S)(A)
ENSTA Bretagne Team 2 (L)(S)(A)
ICARUS (L)(S)(A)
The competition took place over 9 days. The ﬁrst three days were for practice, then
followed 2 days for single-domain trials, 2 days for two-domain subchallenges, and the
Grand Challenge in the nal two-days. Including single domain trials, sub-challenges and
the Grand Challenge a total of 48 runs were judged. It should be noted that the position
of missing workers, leaks, blocked routes and OPIs were randomised between GC runs,
and at no time during the competition were teams allowed access into the Torre del Sale
building or the machine room.
In parallel with the competition was a public programme, including evening lectures
and public demonstrations in the Piombino city centre and at the competition site.
Notably the programme included demonstrations from two ﬁnalists, including the
overall winner, of the DARPA Robotics Challenge (DRC). A total of ~1200 visitors
attended the competition and its public events, including several organised parties of
school children, families and VIPs.
The logistics and local organisation work of euRathlon 2015 was considerable. The
event was staﬀed by 78 people in total, including the organising staﬀ, judging team,
technical and safety team (including divers and safety pilots), media and ﬁlm crew,
stewards and volunteers; the judging team comprised 16 judges (12 from Europe and 4
from the USA). Despite the considerable challenges the event ran smoothly and - most
importantly given the risks inherent in an outdoor robotics event - safely.
268
A.F.T. Winﬁeld et al.

4.2
Grand Challenge Results
Using the methodology outlined in Sect. 4 of [6], the judges were able to assess the
performance of the 6 Grand Challenge teams. As summarised in Fig. 5 scores were
derived from 5 components: task achievements, optional task achievements, autonomy
class, penalties and key penalties. A number of the task achievements were scored on
the basis of judges witnessing an event, such as `robot reaches the unobstructed entrance
of the building’ or `robot enters the machine room’; others were scored following anal‐
ysis of data supplied by teams after the run had been completed, such as map data or
OPIs found. Optional achievements were bonus points awarded if, for instance, teams
found both missing workers within 30 min, robots transmitted live video/image data
during the run, or for direct robot-robot cooperation between domains. The autonomy
class was judged on the basis of observing teams, with 1 point awarded for full autonomy,
0.5 for semi-autonomous operation and 0 for tele-operation. Penalties were marked for
each manual intervention per achievement, or key penalties for mission critical errors
such as closing the wrong valve.
Fig. 5. Grand Challenge scores and ranking
In euRathlon, because of the unstructured nature of the environment and changes in
conditions between events the benchmarks are relatively coarse. However, our Bench‐
marking and Scoring methodology proved to be very successful in allowing a thorough
and transparent evaluation of the performance of teams during the euRathlon 2015
competition. Perhaps the best indicator of the success of the approach was the fact that
teams were clearly diﬀerentiated in both task and functionality benchmarks; notably no
scores were appealed. The detailed scores exposed strengths and weaknesses, both
between teams and of the state of the art as represented by competing teams and their
robots. The overall winners of the Grand Challenge, scoring 53 out of a maximum
achievable of 75 points, were team ISEP/INESC TEC (Air), Team Cobham (Land) and
Universitat de Girona (Sea), shown with their robots in Fig. 6. This was a particularly
impressive outcome given that these three teams had not worked together until arriving
at euRathlon 2015. However, of the teams entering the Grand Challenge ﬁve achieved
euRathlon and ERL Emergency
269

creditable performance in mapping, ﬁnding missing workers and closing valves in a
complex search and rescue scenario that placed great demands on both the robots and
the teamwork needed to coordinate those robots.
Fig. 6. Overall winners of the euRathlon 2015 Grand Challenge: ISEP/INESC TEC (Air), Team
Cobham (Land) and Universitat de Girona (Sea)
5
Conclusions and Lessons Learned
By all measures euRathlon 2015 was a very successful event. We attracted a larger
number of teams than originally planned, and the team matching process proved to be
very successful. Indeed perhaps the most signiﬁcant outcome of not just euRathlon 2015
but the whole project was in bringing together air, land and sea robotics domains to
create a new community. We estimate that we have, through workshops and competi‐
tions trained ~200 roboticists in outdoor multi-domain robotics. We expect to obtain
even a more successful event in ERL Emergency 2017.
From a technical point of view we were impressed by the performance of teams in
the Grand Challenge noting however that there were a number of common diﬃculties
that all teams experienced. The ﬁrst was radio communication. Most teams expected to
use WiFi networks to maintain communication with land robots, and despite some
innovative approaches to overcoming range limitations, such as dropping repeaters or
using several land robots as a multihop network, all teams experienced challenges. The
second was human-robot interfaces - many teams had poorly designed interfaces with
their robots which severely tested those operating or supervising robots from inside hot
control tents. The third limitation was human-human interaction. We did not specify
how the teams communicated between land, sea and air control stations, but it was clear
270
A.F.T. Winﬁeld et al.

that the most successful multi-domain teams were those who established and rehearsed
clear channels and protocols for human-human coordination between the domains. The
real challenges are often not technical but human.
In ERL Emergency 2017, we have updated missions and rules2 taking into consid‐
eration the presented lessons learned from euRathlon. ERL Emergency will be held from
15-23 September 2017, so due to the fact that the submission deadline was in July 2017,
we cannot incorporate the results of ERL Emergency 2017 in this paper. However, they
will be presented during the conference ROBOT 2017 to be held in November 2017.
Acknowledgements. The euRathlon project was funded within the EU FP7 programme, grant
agreement number 601205.
ERL Emergency is part ofROCKEU2 project that is funded by EU H2020 programme, gran
agreement number 688441.
References
1. Winfeld, A., Palau Franco, M., Brueggemann, B., Castro, A., Djapic, V., Ferri, G., Petillot,
Y., Roning, J., Schneider, F., Sosa, D., Viguria, A.: euRathlon outdoor robotics challenge: year
1 report Advances in Autonomous Robotics Systems: 15th Annual Conference, TAROS 2014.
Springer, Birmingham (2014)
2. Ferri, G., Ferreira, F., Sosa, D., Petillot, Y., Djapic, V., Franco, M. P., Winﬁeld, A., Viguria,
A., Castro, A., Schneider, F., Roning, J.: euRathon 2014 marine robotics competition analysis.
In: Eurocast 2015 Workshop on Marine Sensors and Manipulators, Las Palmas de Gran Canaria
(2015)
3. Petillot, Y., Ferreira, F., Ferri, G.: Performance measures to improve evaluation of teams in
the euRathlon 2014 sea robotics competition. IFAC-PapersOnLine 48(2), 224–230 (2015)
4. Röning, J., Kauppinen, M., Pitkänen, V., Kemppainen, A., Tikanmäki; A., Furci, M., Palau
Franco, M., Winﬁeld, A., Stengler, E., Brueggemann, B., Schneider, F., Castro, A., Cordero
Limon, M., Viguria, A., Ferri, G., Ferreira, F., Liu, X., Petillot, Y., Sosa, D.: The Challenge
of Preparing Teams for the European Robotics League. In: Electronic Imaging, Intelligent
Robotics and Industrial Applications using Computer Vision 2017, pp. 22–30 (9) (2017)
5. Nagatani, K., Kiribayashi, S., Okada, Y., Otake, K., Yoshida, K., Tadokoro, S., Nishimura, T.,
Yoshida, T., Koyanagi, E., Fukushima, M., Kawatsuma, S.: Emergency response to the nuclear
accident at the Fukushima Daiichi Nuclear Power Plants using mobile rescue robots. J. Field
Robot. 30(1), 44–63 (2013)
6. Winﬁeld, A.F., Franco, M.P., Brueggemann, B., Castro, A., Limon, M.C., Ferri, G., Schneider,
F.: euRathlon 2015: A multi-domain multi-robot grand challenge for search and rescue robots.
In: Conference Towards Autonomous Robotic Systems, pp. 351–363. Springer, June 2016
2 https://eu-robotics.net/robotics_league/erl-emergency/about/index.html.
euRathlon and ERL Emergency
271

Autonomous Landing of a Multicopter on a
Moving Platform Based on Vision Techniques
Jos´e Joaqu´ın Acevedo1(B), Manuel Garc´ıa1, Antidio Viguria1, Pablo Ram´on2,
Bego˜na C. Arrue2, and Anibal Ollero2
1 Advanced Center for Aerospace Technologies, Parque Tecnol´ogico y Aeron´autico de
Andaluc´ıa, c/ Wilbur y Orville Wright 19, 41309 La Rinconada, Seville, Spain
{jjacevedo,mgarcia,aviguria}@catec.aero
2 Robotics, Vision and Control Research Group, University of Seville,
Avda. de los Descubrimientos s/n, 41092 Seville, Spain
{prs,barrue,aollero}@us.es
Abstract. This paper proposes the whole system scheme designed for
the autonomous landing of a multicopter on a moving platform. The
technology used for the tracking and landing is the visual detection and
recognition of a marker placed on the platform. Both the hardware and
software architecture are explained and also the results of some succesful
tests are shown. In addition, the proposed system was validated and
compared during the MBZIRC robotics competition in March 2017.
Keywords: Autonomous landing · Multicopter · Vision control
1
Introduction
The interest in unmanned air vehicles (UAV) and particularly in multicopters
has risen signiﬁcantly in a great number of civil and military applications such
as remote sensing, traﬃc monitoring, search and rescue, surveillance, while new
application scenarios are being continuously discovered. One of the most chal-
lenging and promising application is the autonomous landing of multicopters on
moving platforms, for example on the deck of a ship.
Apart from that, many robotic competitions in which the so-called drones
are tested under many diﬀerent circumstances are recently appearing. In this
case, both CATEC and GRVC have participated in the International Robotic
Competition Challenge called MBZIRC. One of the challenges of the MBZIRC
competition is the autonomous landing of a multicopter on a moving platform.
The relative positioning between the landing platform, which position is
unknown, and the aerial vehicle is the key area of interest in this work. Dif-
ferent techniques are being studied and developed for this purpose, such as the
relative positioning sensor DECKFINDER based on radio frequency beacons
developed by AIRBUS D&S. This system is independent from GPS and involves
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_23

Autonomous Landing of a Multicopter on a Moving Platform
273
a signiﬁcant assistance in maritime operations of unmanned aircrafts. Another
interesting way to provide assistance in this kind of operations is the use of
tethered conﬁgurations. First references that use tether tension to guide the
helicopter landing operation are in [1,2]. Finally, the GPS sensor is replaced by
a tether-based helicopter position estimate during the landing operation in [3,4].
However, the most promising approach proposed to deal the autonomous
landing operation is the visual servoing. Two main techniques are deﬁned: Image-
Based Visual Servoing (IBVS), where the image features are directly used to
control the landing operation; and the Position-Based Visual Servoing (PBVS),
which estimate the target pose before including it in the control law. Authors
of [5,6] combine IBVS methods with inertial measurements and Proportional
Derivative controller for Attitude Control to land a VTOL aircraft on a sta-
tic platform. PBVS is applied in [7], using stereo-vision to measure the height
between the aerial vehicle and the landing area. Moreover, in [8] authors estimate
the pose of a 6-DOF moving platform with known dimensions, using a downward
looking camera. PBVS is combined with inertial and GPS measurements in [9].
This paper deals the landing operation based on PBVS visual servoing app-
roach. The strategy to track and land on the platform could be summarized as
follows. Based on GPS, the autonomous multicopter navigates a set of predeﬁned
waypoints in order to detect the platform. Once it is detected, the multicopter
tracks it until the landing platform on the ship is under its vertical. Then, the
multicopter descends on the platform taking into account its relative position.
2
System Integration
There are three main stages in the development process. The ﬁrst phase is about
the design of both the hardware and software architecture. The second phase
includes simulation work to validate the proposed algorithms to accomplish the
landing mission. Finally, the third phase includes ﬂight experiments in which
the whole system is tested with an experimental setup built for that purpose
(Fig. 1).
2.1
Hardware Setup
The
aerial
platform
used
in
the
experiments
is
an
hexarotor
of
size
1.18 × 1.18 × 0.5 m, and the weight is 6 kg approximately. The necessary equip-
ments for the multirotor to accomplish the landing mission are:
– An autopilot: a PIXHWAK autopilot is used. The autopilot is in charge of
executing ﬂight control autonomously, receiving the commands of the ﬂight
planner which is integrated on the on-board computer and giving the com-
mands to the engines.
– An on-board computer: the chosen computer is an Intel NUCi7. This PC
is used for vision data processing to estimate the relative position to the
platform. Moreover, it provides the autopilot with navigation commands.
– An on-board camera: ZED camera from Stereolabs to detect and track the
platform because of its wide ﬁeld of view and good exposure.

274
J.J. Acevedo et al.
Fig. 1. Multicopter fully equipped with onboard PC and camera.
2.2
Simulation Environment
A simulation environment has been developed in order to implement, test, debug
and validate the logic which manages the diﬀerent states during the autonomous
mission. In order to ease the ﬁnal integration, the simulation environment
considers the main features from the real system, including the emulation of
the communication protocols between the diﬀerent elements from the system.
ROS (http://www.ros.org/) is an environment which facilitates the informa-
tion exchange between nodes. It has received a great acceptance in the robotics
community and has been used to develop the system described in this paper.
Moreover, MAVLINK (https://pixhawk.ethz.ch/mavlink/) is a protocol compat-
ible with most autopilots (including PIXHAWK), which allows to communicate,
monitor and control them from external computers. So, the simulation envi-
ronment, which main architecture is shown in Fig. 2, has been developed to be
compatible with these protocols.
Fig. 2. Simulator architecture implemented to debug the system.

Autonomous Landing of a Multicopter on a Moving Platform
275
Main nodes from the architecture shown in Fig. 2 are described as follows:
– The application node is the software or algorithm to be tested in the simula-
tion environment.
– An autopilot simulator (which implements the APM-Copter v3.5 ﬁrmware
http://ardupilot.org/dev/docs/setting-up-sitl-on-linux.html) has been inte-
grated in the simulation environment.
– The mavros node is a MAVLINK parser which allows the autopilot simulator
to communicate with the rest of the system based on ROS services and topics.
– Gazebo is the graphical simulator which allows the integration of visual sen-
sors in the simulations and deﬁne diﬀerent scenarios.
– The Gazebo bridge node is in charge of updating the Gazebo object (multi-
copter) states based on the state provided by the autopilot simulator through
the mavros node.
Figure 3 shows a snapshot from a simulation.
Fig. 3. Snapshot taken from a simulation.
3
Software Architecture
The software integrated in the onboard computer is based on an Ubuntu 14.04
and ROS indigo architecture. The software architecture is shown Fig. 4.
It implements three main nodes: the manager, the platform tracker and the
vision ﬁlter. Following paragraphs explain this architecture.
The manager node handles the state machine described in the next Sub-
sect. 3.1. This node executes the following tasks: sending commands to the
autopilot (takeoﬀ, land, set mode, load waypoints list), monitoring autopilot
states and multicopter telemetry, and activating/deactivating platform tracker.

276
J.J. Acevedo et al.
Autonomous landing based on vision techniques
5
Fig. 4. Software architecture implemented in the onboard computer.
The platform tracker node detects and estimates its relative position with
respect to the moving platform based on images taken from the onboard camera.
The algorithm used to estimate this relative position is explained in Subsect. 3.2.
The vision ﬁlter node is in charge of commanding externally to the autopilot
the target position where the multicopter should ﬂy. It receives the relative
position estimated by the platform tracker and the multicopter actual position
and states provided by the autopilot. Basically, the vision ﬁlter node makes the
following tasks:
– It stabilizes the relative position estimation using a low-pass ﬁlter.
– It calculates the actual platform position based on the relative position and
the multicopter actual position (based on GPS).
– It estimates the platform velocity based on a linear regression model from the
last calculated platform positions.
– It predicts the future platform position using the estimated platform velocity.
This position will be sent to the autopilot as target position. The autopilot
is in charge of controlling the multicopter motion based on this position.
Finally, a slighted modiﬁed version of the mavros node has been included in
the system to provide communication capability between the onboard computer
and the autopilot based on the MAVLink protocol. The mavros node (https://
github.com/mavlink/mavros) creates a set of ROS services to send commands
via MAVLink to the autopilot that are used by the manager. Also, mavros parses
the MAVLINK messages provided by the autopilot through ROS topics accessi-
ble by the rest of the ROS nodes.

Autonomous Landing of a Multicopter on a Moving Platform
277
3.1
State Machine to Manage the Mission Strategy
The strategy of autonomously landing on a moving platform divides the mission
in three phases: searching, tracking and approaching. The manager handles these
phases using a ﬁnite state machine, as is shown in Fig. 5. It has been implemented
based on ROS and the SMACH package (http://wiki.ros.org/smach).
During the Init state, the multicopter is armed and a predeﬁned list of way-
points (which has been chosen to cover the whole area of interest) is uploaded to
the autopilot. Then, the ﬂight mode is set to AUTO and the multicopter takes
oﬀand starts the mission.
During the Search state, the multicopter ﬂies in AUTO mode executing a
GPS waypoint navigation in order to detect and locate the moving vehicle using
the information provided by the platform tracker. Assuming a previously known
and ﬁxed eight-shaped trajectory for the moving platform, the time to detect it
Fig. 5. Main state machine to handle the landing activity.

278
J.J. Acevedo et al.
could be bounded up if the multicopter waits in the center of the ﬁeld (an only
waypoint in the predeﬁned list).
Once the vehicle has been detected, the Track state starts and the ﬂight mode
is set to GUIDED. The multicopter tracks the target position provided by the
vision ﬁlter to get a very conﬁdent measure from the vehicle relative position.
Finally, when the relative position provided by the platform tracker is reliable
enough and the multicopter is close enough to the platform, the Approach state
starts and the multicopter starts descending on the platform until landing, using
the platform position estimated by the vision ﬁlter.
For safety reasons, there are four other states deﬁned. If GPS signal is lost,
the multicoper hovers on its actual position waiting to recover it (HoverWith-
outGPS state). During the Approach state, the aerial platform could go into
instable conditions (for instance, relative position not stable). In this case, the
approach and landing operation is aborted, starting Wait state. In this case, the
multicopter rises and waits until recovery of safety conditions. Apart from that,
in case of extreme low battery condition, the multicopter starts an emergency
land operation (EmergencyLand state). Finally, the operator can take control of
the multicopter at any time during the operation in case of dangerous circum-
stances (RemoteTeleoperation state).
3.2
Relative Position Estimation Based on Vision Techniques
The platform tracker node has to provide a relative position estimation in
real-time. The scheme is simple: searching for a cross-shaped contour in the
received image from the cameras. First of all, the source image needs to be
slightly processed to enhance its contrast. Then, the contours from the image
are extracted, and ﬁnally all these contours are ﬁltered to get a very speciﬁc one,
the cross. Once the cross is framed, as previously commented, the relative pose
can be computed. Regarding the computation time, 20 ms (50 Hz) are needed to
detect and compute the pose estimation. Several tests at diﬀerent altitudes have
been performed (see Fig. 6. At short and medium distances, the cross- ﬁnder
approach performs as expected and runs fast enough for control purposes.
4
Flight Tests
In order to replicate the scenario of a multicopter landing on a ship deck, an
experimental setup has been built in order to replicate the conditions. A landing
square platform of 1.5 m side has been acquired, on top of which a canvas has
been attached with a marker painted on it for the visual recognition of the
platform. The platform is made of ferrous material (steel) and is placed over the
luggage rack of a car, so it is possible to move it with diﬀerent trajectories. Several
magnetic elements have been placed on the landing skids in order to attach the
multicopter to the landing platform at the touch down moment (Fig. 7).
As a method of comparison of relative position to the platform based on
vision, a GPS receiver has been integrated on the top of the platform, so it is

Autonomous Landing of a Multicopter on a Moving Platform
279
Fig. 6. Relative localization algorithm tracking the landing cross at diﬀerent altitudes.
Fig. 7. On the left, the multicopter is landing on the mobile platform during ﬂight
tests. On the right, the landing marker mounted on the mobile platform is shown.
150
200
250
300
350
−10
−8
−6
−4
−2
0
2
4
6
8
10
time (s)
Relative Position X axis (m)
GPS
Vision
(a) x relative position
150
200
250
300
350
−10
−8
−6
−4
−2
0
2
4
6
8
10
time (s)
Relative Position Y axis (m)
GPS
Vision
(b) y relative position
Fig. 8. Comparison between relative position computed by the vision algorithm and
the based on the GPS positions.

280
J.J. Acevedo et al.
possible to track the car position during the experiments. When analyzing the
data oﬄine, it is possible to compare the relative position based on vision data
and relative position obtained by means of the GPS data of both the RPAS and
the landing platform, see Fig. 8.
Several successful tests of the whole landing strategy, assuming a maximum
car speed of 10 km/h, validated the proposed system. The system was proven
to be secure, since the logics implemented in the state machine always avoided
unsuccessful landings such as touch downs out of the platform. Hence, a risky
situation such as landing on a small moving platform has been achieved without
crashes and damages on the multicopter.
5
MBZIRC Competition
CATEC and GRVC as part of the Al-Robotics team have participated in the
MBZIRC international competition in March 2017. The ﬁrst challenge of this
competition consisted of the autonomous landing of a multicopter on a moving
platform which moves with a eight-shaped trajectory.
Al-Robotics team was selected from more than 100 candidates from all over
the world to participate in the challenge in Abu-Dhabi and was one of the only
ﬁve teams in achieving one successful landing in a complete autonomous way.
Figure 9 shows some results from the competition ﬂights: the multicopter posi-
tion since the detection of the landing platform until the touch down moment,
when the car was moving at 5 km/h. See the video in the MBZIRC oﬃ-
cial youtube stream (https://www.youtube.com/watch?v=EsqRXrFcaS4) since
minute 1:19:45.
0
10
20
30
40
50
60
70
80
90
−8
−6
−4
−2
0
2
4
time (s)
Relative Position (m)
x
y
Auto
Guided
(a) xy relative position
0
10
20
30
40
50
60
70
80
90
0
2
4
6
8
10
12
Relative Altitude (m)
time (s)
(b) Multicopter altitude
Fig. 9. Relative position between the multicopter and the landing marker, during the
MBZIRC competition.

Autonomous Landing of a Multicopter on a Moving Platform
281
6
Conclusions
The proposed system has been proven to be reliable and robust from a safety
point of view. A large amount of real test of the whole mission have been per-
formed and no crashes have been registered. Moreover, several successful landing
on the car have been completed, at car velocities between 5 and 10 km/h. Future
work will improve the robustness of the system in order to be able to achieve
landings at higher velocities of the moving platform.
CATEC and GRVC have used this system to participate in the MBZIRC
competition, in Challenge 1, achieving successful landings on the car moving
at 5 km/h. Only ﬁve teams got to land on the car in a fully autonomous way.
The interesting issue in the solution proposed in this paper is that it does not
take into account the landing platform trajectory, being a solution more generic
which can be used at any scenario.
Based on the experience of the authors during the ﬂight tests and the com-
petition, the future work will be focused on improving the system eﬃciency and
achieving landings at higher platform velocities, as well as making the ﬂight dur-
ing the tracking phase much smoother. The ﬁrst proposal would be to modify
the ﬂight mode in the tracking phase, so that the command from the external
PC would be based on attitude, instead of a target position. This would improve
the command rate of the PIXHAWK-PC interface and would be less restrictive
in terms of waypoint navigation parameters of the autopilot conﬁguration.
Acknowledgements. The work of the author Jose J. Acevedo was partially funded by
the SAVIRA project (PTQ-14-07083) in the framework of the Torres Quevedo national
program.
References
1. Oh, S.R., Pathak, K., Agrawal, S.K., Pota, H.R., Garratt, M.: Approaches for a
tether-guided landing of an autonomous helicopter. IEEE Trans. Rob. 22(3), 536–
544 (2006)
2. Ahmed, B., Pota, H.R.: Backstepping-based landing control of a ruav using tether
incorporating ﬂapping correction dynamics. In: 2008 American Control Conference,
pp. 2728–2733, June 2008
3. Alarcn, F., Santamara, D., Viguria, A., Ollero, A., Heredia, G.: Helicopter gnc
system for autonomous landing by using a tether in a gps denied scenario. In: 2015
International Conference on Unmanned Aircraft Systems (ICUAS), pp. 1067–1073,
June 2015
4. Sandino, L.A., Santamaria, D., Bejar, M., Viguria, A., Kondak, K., Ollero, A.:
Tether-guided landing of unmanned helicopters without gps sensors. In: 2014 IEEE
International Conference on Robotics and Automation (ICRA), pp. 3096–3101, May
2014
5. Chaumette, F., Hutchinson, S.: Visual servo control. i. basic approaches. IEEE
Robot. Autom. Mag. 13(4), 82–90 (2006)
6. Chaumette, F., Hutchinson, S.: Visual servo control. ii. advanced approaches [tuto-
rial]. IEEE Robot. Autom. Mag. 14(1), 109–118 (2007)

282
J.J. Acevedo et al.
7. Yu, Z., Nonami, K., Shin, J., Celestino, D.: 3D vision based landing control of a
small scale autonomous helicopter. Int. J. Adv. Robot. Syst. 4(1), 7 (2007), http://
dx.doi.org/10.5772/5710
8. Sanchez-Lopez, J.L., Pestana, J., Saripalli, S., Campoy, P.: An approach toward
visual autonomous ship board landing of a VTOL UAV. J. Intell. Robot. Syst.
74(1), 113–127 (2014), http://dx.doi.org/10.1007/s10846-013-9926-3
9. Ceren, Z., Altu, E.: Vision-based servo control of a quadrotor air vehicle. In: 2009
IEEE International Symposium on Computational Intelligence in Robotics and
Automation - (CIRA), pp. 84–89, December 2009

3D Mapping for a Reliable Long-Term
Navigation
Jonathan Gin´es1, Francisco Mart´ın1(B), Vicente Matell´an2, Francisco J. Lera3,
and Jes´us Balsa2
1 University of Rey Juan Carlos, M´ostoles, Spain
j.gines@alumnos.urjc.es, francisco.rico@urjc.es
2 University of Le´on, Le´on, Spain
{vicente.matellan,jesus.balsa}@unileon.es
3 Computer Science and Communications Research Unit (CSC), University of
Luxembourg, Luxembourg city, Luxembourg
francisco.lera@uni.lu
Abstract. The use of maps allows mobile robots to navigate between
known points in an environment. Using maps allows to calculate routes
avoiding obstacles and not being stuck in dead ends. This paper shows
how to integrate 3D perceptions on a map to obtain obstacle-free paths
when obstacles are not at the level of 2D sensors, but elevated. Chairs
and tables usually pose a problem when one can only see the legs with a
2D laser, although they present a high hurdle with a much larger area.
This approach builds a static map starting from the construction plans of
a building. A long-term map is started from the static map, and updated
when adding and removing furniture, or when doors are opened or closed.
A short-term map represents dynamic obstacles such as people. Obsta-
cles are perceived by merging all available information, both 2D laser
and RGB-D cameras, into a compact 3D probabilistic representation.
This approach is appropriate for fast deployment and long-term oper-
ations in oﬃce or domestic environments, able to adapt to changes in
the environment. This work is designed for domestic environments, and
has been tested in the RoboCup@home competition, where robots must
navigate in an environment that changes during the tests.
Keywords: Long-term navigation · 3D mapping · Mobile robot ·
Robocup
1
Introduction
Mobile robots navigate along the environment to perform the commanded tasks.
This capability is critic for the success of many applications, and it has to be
accomplished robustly. One of the elements of navigation is the map. Maps
represent the structure of the environment, the obstacles and the free space.
Using this information, robots update their pose and generate paths. Usually,
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_24

284
J. Gin´es et al.
maps remain unchanged once generated. This is not convenient in domestic
environment when operating for days because furniture can change its position
and new objects can appear. Even for short-term operation, doors can be open
or closed.
For this reason, we present a mapping method for long time operation in
domestic environment. Simultaneous Localization and Mapping (SLAM) tech-
niques also updates the map while operation, but they are more focused in
building maps, instead of deploying a robot in a known environment and start
operating immediately. Besides of this, SLAM techniques require a robustness
hard to get. Walls, doors and corridors are known a priori. Usually, we can obtain
an architectonic map of large environments likes oﬃces, universities, public build-
ings or hotels.
Our proposal is a mapping method starts from these kind of structural infor-
mation of walls, doors and corridors, building a static map which never changes.
This information is enough to maintain a good estimation about the robot pose,
when starting pose is known. Obstacles not present in the static part are incor-
porated to a short-term map. When this obstacle is persistent in time, it is
incorporated to a long-term map. In the same way, if a obstacle disappear, it is
removed from the long-term map. The robot uses the combined map of static
and long-term information to self-localize and build routes.
Mobile robots that travel on the ground often use 2D maps, since the z
component, or the roll or pitch are not taken into account to locate or generate
routes. For most navigation tasks, using a 2D laser is considered appropriate.
We have found that it is not enough, because a robot is a 3D volume that must
pass through an environment that may present obstacles that do not necessarily
have to be at the height of the laser. Using a 2D approach, we had problems
with shelves, tables, chairs and all kinds of obstacles that are partially detected
at the height of the laser. For this reason, one of the main contributions of our
work is to merge the information of a 2D laser with any other sensor, mainly
3D cameras, currently used in many robots to have a complete 3D information
which let us to safety navigate.
This method is applied in the RoboCup@home [1], RoCKIn@home [2] and
European Robotics League (ERL) competitions. These competitions propose
several tasks in a domestic environment which simulates a house. The tech-
nologies evaluated in the test includes navigation, manipulation, object/people
recognition, communication with humans and task planning. These testbeds let
us to compare our approach with the other participants’ solutions in the same
conditions.
This paper is organized as follows: in the Sect. 2 we describe the relevant
work in the area, with special attention to the last method used in the competi-
tion. The description of our contribution is in Sect. 3, where we brieﬂy describe
the ROS navigation system and present our mapping system. The experimental
validation is in Sect. 4, where the experiments in the simulator conﬁrm the result
in the competition. Finally, we discuss the results and work in Sect. 5.

3D Mapping for a Reliable Long-Term Navigation
285
2
Related Work
Mapping in the form of a grid has been a widely used approach. Most mobile
robots move on the ground, so they do not need more info to self-locate and
navigate than a 2D grid of obstacles. It is a compact and general way of rep-
resenting the environment. Maybe one of the ﬁrst successful works using this
approach is [3], and a full description of capabilities is shown in [4]. It is a widely
used mapping method in conjunction with self localization algorithms such as
Monte Carlo [5,6] or Markov [7], when sensory information can be reduced to
2D readings of obstacles. Most of these approaches start from a premise that
is not fulﬁlled when the operation of the robot can last for days or months,
as the environment can change. These jobs usually do the mapping once, and
assume that the environment never changes. In fact, if the robot is in an crowded
environment, techniques such as using the ceiling image as a map are used [8].
In [9], robots operation last for 1000 Km [10]. This approach takes as its basis
an architectural map using a Bayesian network to decide what type of feature
is being detected. In our case, we simply model the features on diﬀerent maps.
Our approach is simpler, but equally eﬀective. By not having to detect diﬀerent
types of obstacles, our method is more general and scalable.
Long-term navigation should keep in mind that environments may vary. In
our opinion, the only thing that does not change is the structure of the building,
meaning the walls The rest can change, and the robot has to adapt to calcu-
late better routes. In [11] a Dynamic Pose Graph SLAM is used to map low
dynamic environments. In this work, 2D maps are generated from a graph stor-
ing changes in the environment. The authors state that localization is improved
using this type of maps. In [12] a work is presented in which are maintained
spatial-temporal 3D maps. Each position stores the probability that a node is
busy depending on the time of day. In this work a robot plans which points
to visit depending on the entropy of surrounding areas, since this occupation
is repeated cyclically. Our work is focused on providing a map which allows us
to generate good routes to navigate. Adding these temporal characteristics is
future work and not included in the presented approach.
The appearance of 3D sensors has given rise new types of 3D maps [13].
Octomap [14] is a compact way to encode the environment about obstacles and
even colors. These maps are very useful for self-localization [15], although not so
much to navigation when the robot is on the ground and there are no obstacles
in height that prevent the robot from passing through.
3
Mapping System
Our mapping system is integrated in the navigation stack of ROS. The standard
ROS navigation system is designed for static environments. If a little change is
produced in the scenario, the localization component bear with this but when
signiﬁcant portions of the environment change, it can not deal with this situation.
This system has also problems dealing with doors, that sometimes can be open

286
J. Gin´es et al.
Fig. 1. ROS navigation system with our contributions in red (left). Diagram of the
proposed dynamic map module (right).
and sometimes closed, objects smaller than the position of the laser sensors and
tables because the robot can only perceive the thin legs of it.
To deal with these problems, we have replaced the original map server by a
node that implements our approach, as is shown in the left side of the Fig. 1.
The module map server is a critical component of the navigation stack of
ROS. It provides of occupancy information both to the navigation module, which
actually makes plans and send commands to the actuators, and the Amcl mod-
ule, which localizes the robot using a Monte Carlo [16] algorithm called KLD-
Sampling [17].
For representing occupancy maps in 3D, we use a compact representation
called octomap [14], based on octrees (Fig. 2).
Internally, our map module uses this 3D representation of the environment.
As we use the existing navigation and localization modules in the ROS navi-
gation stack, and their inputs are 2D occupation maps, our module produces
two outputs: the ﬁnal 3D occupancy map and a 2D occupancy map, built by
ﬂattening the 3D map. This ﬂattening process gets the value of each (x, y) cell
as the maximum occupancy value of cells (x, y, z)∀z ∈[zmin, zmax].
We want our robot to navigate days, weeks, or months through an envi-
ronment without remapping. We believe that the only really static part of an
environment is its walls. Anything else can change its position over time. For this
reason our robot starts from metric measures or from the architectonics maps,
Fig. 2. 3D occupancy representation using octomaps.

3D Mapping for a Reliable Long-Term Navigation
287
which we will call a static map. This map include the unchanged parts of the
scenario. All maps are 3D maps, as we’ll describe in the next point, and as a
ﬁnal step we converts them in a 2D occupancy grid.
When the robot perceives an obstacle that is not on the static map, it incor-
porates it into a short-term map whose 3D cells value varies rapidly over time.
If it is a person, it will quickly disappears and the short-term map will reﬂect it.
On the other hand, it is a new persistent obstacle, the robot will perceive it for a
long time and because of this the obstacle will become part of a long-term map.
Similarly, if an obstacle was on the map and it disappears, it will be removed
from the long-term map.
The long-term map is periodically saved to disk as an .ot ﬁle. When the
map server starts, it loads both the static map and the long-term map. As we
previously introduced, the map server publishes the combination of these maps
and convert it in a 2D occupancy grid map for the navigation and localization
modules.
Internally, map server is composed by the following elements also illustrated
on the right side of Fig. 1.:
– Map loader: This node loads the static and the long-term maps from disk.
– Sensors2octomap: This node combines the information of the sensors and
create an octomap with it.
– Obs detector: The main function of this node is to detect objects using the
octomap from the sensors. This information produces a 3D octomap, which
is the main information of the short-term map.
– Map controller: Node with the function of compose the ﬁnal octomap, pub-
lish it in /octomap topic and update the long-term map with the values of
the short-term map.
– Octomap2map: This node takes the information of the /octomap topic and
creates a 2D occupancy grid with it.
Maps are 3D grids of values representing obstacles or free space. The
dimension of each 3D cell in the grid is preconﬁgured and an usual value is
100 mm × 100 mm. It probabilistically represents obstacles using values in the
range [0, 1].
We use several maps at the same time for diﬀerent:
– static map (mapS): The 2D map is built from metric measures or from the
architectonics maps.
– short-term map (mapst): This octomap is initialized totally empty. When
running, cell values are updated with the obstacle information from sensors.
– long-term map (maplt): The long-term map includes the objects that per-
sist in time which could aﬀect when generating routes. This octomap is
updated with the information from the short-term map.
– resulting map (mapf): This is the resulting octomap published by the map
server.

288
J. Gin´es et al.
– projected map (mapp): Conversion of the mapf into a 2D map. To do this
conversion we take all 3D cells and we associate each one with a cell in 2D
map. The value of the 2D map cell will be the max value between all 3D
nodes associated with it.
map2D = Flat(map3D)
(1)
map2D
i,j = max(mapi,j,k∀k ∈[−∞, +∞])
(2)
Maps are created and updated following these operations:
mapf = mapS ⊕maplt
(3)
where ⊕is deﬁned as,
m1 ⊕m2 : max(m1i,j, m2i,j),
∀m1i,j ∈m1, ∀m2i,j ∈m2
(4)
and
maplt = maplt ⊗mapst
(5)
where ⊗is deﬁned as:
m1 ⊗m2 : m1 =

m1i,j
if m2i,j < 1,
m1i,j + 1
if m2i,j = 1
∀m1i,j ∈m1, ∀m2i,j ∈m2
(6)
where 1 value represent the probability that a cell is occupied.
After these operations we have set up a map (mapf) ready for being con-
verted in 2D map for being used by localization and navigation modules.
The 2D map, mapp, is built following this operation:
m1i,j = max(m2i,j,∀k),
∀m1i,j ∈m1, ∀m2i,j,k ∈m2
(7)
4
Experiments
In order to validate the work presented in this paper, we have carried out exper-
iments both in the simulator as the previous step for implementing in the real
robot and testing it in incoming real competitions.
The goal of these experiments is to validate the proposed algorithm in a
controlled environment. The fundamental aspect of our approach is that the
map update is performed correctly and we can avoid small objects:

3D Mapping for a Reliable Long-Term Navigation
289
– Detection of dynamic objects should not aﬀect the long-term map.
– The perception of static objects must be incorporated in a reasonably short
time to the long-term map, anchoring itself with successive detections.
– The disappearance of static objects that were already on the long-term map
should be eliminated quickly.
– Closed doors are assimilated to static obstacles that are incorporated into the
long-term map, being eliminated quickly when the doors are opened.
– Avoid objects that could presents diﬃculties to perceive with the laser sensor,
like a chairs or tables.
– Avoid smaller objects under the position of the laser sensor.
1. Static objects. In the ﬁrst experiment, the robot perceives a static object
(Fig. 3).
Fig. 3. Experiment with a static object in front of the robot.
At ﬁrst, the object will be added to the short-term map and 17s later, when
the value of the cells in short-term map reach 1, the long-term map will reﬂect
this change.
Figure 4 represents the evolution of the short-term map and long-term map
when the robot perceive a new object. If the object is removed from the scene,
the short-term map will reﬂects this change clearing the cells of the object and
when the short-term map its clean, the cells of the object in long-term map will
starts to decrease their value.
2. Impact of the mapping mechanism in the robot’s motion. If a door
is closed just when the robot go to cross it or if a person block the way of the
robot, local planner of the navigation will avoid the danger. This local planner
only uses the information of the laser sensor, because of this we had to create
a new laserscan to reﬂect the information of the combination between laser
and RGB-D sensors.
Figure 5 shows the statistics parameters of the time to add and delete objects
from long-term map and how much time is needed to inﬂuence in the global
planner.

290
J. Gin´es et al.
Fig. 4. Changes in cell’s value in presence of static objects. Up, the short-term map
and down the long-term map
Fig. 5. Statistics parameters of the time to change the long-term map and the global
planner.
In this experiment we compare the previous 2D navigation method with our
approach, where RGB-D sensors are used.
We commanded the robot to go to a position beyond a table. Figure 6 shows
how the robot went under the table. If the robot would taller than Kobuki, the
robot would had collide with the table.
In Fig. 7 we show how the robot avoid the table using our 3D mapping system.
The global planner takes the information from the 2D map, which is generated
with the information of all the sensors, and reﬂects the object in the navigation
global map. Because of this, the navigation module does not plan a new route
crossing the table, like shown in Fig. 6. Now the navigation module can plan a
route avoiding the obstacle.
3. Avoiding
little
objects.
In
a
domestic
environment
or
in
the
Robocup@Home you can ﬁnd some objects under the laser position in the
robot in your way. Our approach can avoid colliding with it while navigating.
In Fig. 8 we show how the ball is added to the octomap and how the robot
can avoid the ball.

3D Mapping for a Reliable Long-Term Navigation
291
Fig. 6. Planning a new route crossing a table with 2D approach.
Fig. 7. Planning a new route crossing a table with 3D aproach.
Fig. 8. Avoiding a little object.

292
J. Gin´es et al.
4. Doors opened and closed. In this experiment, we ask the robot to go near
to the blue table in the dinning room. It knows mapf (mapS ⊕maplt), so
it can compute the path: go straight to the wall, turn left and go straight
again to the table. In this experiment, we have blocked this way, simulating
a closed door (Fig. 9).
Fig. 9. Path blocked simulating a closed door.
The robot starts its way, but when it approximates to the closed door, maplt
starts to reﬂect this change. Path is recomputed, and the robot found an alter-
native way to reach its goal. The robot does not forget the closed door when
the robot is looking away, because of this the robot will not use this door to
calculate a new route.
Fig. 10. Path followed.

3D Mapping for a Reliable Long-Term Navigation
293
5
Conclusions
This paper has presented our mapping system for long-term navigation, based
on dynamic maps. Maps have permanent structure built from the constructions
plans. All the perceived obstacles are dynamically incorporated or removed from
the map, using diﬀerent levels of persistency. The described approach provides
a fast method for deploying a robot in a domestic environment, and for long
operation in which the obstacles can modify their position.
Among the strengths of our method, we highlight, besides the fast mapping,
the management of dynamic objects, the convenience to manage the opening of
doors and their ability to adapt to changes in the surrounding furniture.
The main contribution of this work is the use of a 3D representation for both
perceptions and the maps. This let us to perceive obstacles of any shapes, even
small ones, which are not correctly perceived using only 2D laser perceptions.
The proposed method has been implemented inside the ROS navigation stack
without any modiﬁcation of the other modules. Because of this, the implemen-
tation of this contribution can be easily tested by the robotics community. The
experiments shows the validity of this approach, and its use in competitions
probes its robustness.
This work can be extended by gradually migrating the localization and nav-
igation modules to use 3D information, although navigation beneﬁts will only
be eﬀective in robots with 6 DoF.
Acknowledgment. This work has been supported by the Spanish Government
TIN2016-76515-R Grant, supported with FEDER funds.
References
1. van der Zant, T., Wisspeintner, T.: RoboCup@Home: creating and benchmarking
tomorrows service robot applications. In: Lima, P. (ed) Robotic Soccer, pp. 521–
528. I-Tech Education and Publishing, Vienna (2007)
2. Lima,
P.U.,
Nardi,
D.,
Iocchi,
L.,
Kraetzschmar,
G.,
Matteucci,
M.:
RoCKIn@Home: benchmarking domestic robots through competitions. In: ICAR
2013, Montevideo, Uruguay (2013)
3. Thrun, S., Buckenz, A.: Integrating grid-based and topological maps for mobile
robot navigation. In: Proceedings of the Thirteenth National Conference on Arti-
ﬁcial Intelligence AAAI, Portland, Oregon, August 1996
4. Thrun, S., Burgard, W., Fox, D.: Probabilistic Robotics (Intelligent Robotics and
Autonomous Agents). The MIT Press, Cambridge (2005)
5. Dellaert, F., Fox, D., Burgard, W., Thrun, S.: Monte carlo localization for mobile
robots. In: ICRA, vol. 2, pp. 13221328 (1999)
6. Lenser, S., Veloso, M.: Sensor resetting localization for poorly modelled mobile
robots. In: International Conference on Robotics and Automation (2000). ISBN:
0780358864
7. Koenig, S., Simmons, R.: Xavier: a robot navigation architecture based on partially
observable markov decision process models. In: Artiﬁcial Intelligence Based Mobile
Robotics: Case Studies of Successful Robot Systems, pp. 91122 (1998)

294
J. Gin´es et al.
8. Thrun, S., Bennewitz, M., et al.: MINERVA: a second-generation museum tour-
guide robot. In: Proceedings of IEEE International Conference on Robotics and
Automation (ICRA99) (1999)
9. Biswas, J., Veloso, M.: Episodic non-Markov localization. Robot. Auton. Syst. 87,
162–176 (2016)
10. Biswas, J., Veloso, M.: The 1,000-km challenge: insights and quantitative and qual-
itative results. In: IEEE Intelligent Systems, pp. 1541–1672 (2016)
11. Walcott-Bryant, A., Kaess, M., Johannsson, H., Leonard, J.J.: Dynamic pose
graph SLAM: long-term mapping in low dynamic environments. In: Proceedings of
the 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,
Vilamoura, Portugal, pp. 1871–1878 (2012)
12. Santos, J.M., Krajnik, T., Fentanes, J.P., Duckett, T.: Lifelong information-driven
exploration to complete and reﬁne 4D spatio-temporal maps. IEEE Robot. Autom.
Lett. PP(99), 1–14 (2016)
13. Montemerlo, M., Thrun, S.: Large-scale robotic 3-D mapping of urban structures.
In: Experimental Robotics IX. Springer Tracts in Advanced Robotics, vol. 21, pp
141–150 (2006)
14. Hornung, A., Wurm, K., Bennewitz, M., Stachniss, C., and Burgard, W.: OctoMap:
an eﬃcient probabilistic 3D mapping framework based on octrees. Auton. Robots
(2013)
15. Mart´ın, F., Matelln, Lera, F.: Multi-thread impact on the performance of Monte
Carlo based algorithms for self-localization of robots using RGB-D sensors. In:
Workshop on Physical Agents (2016)
16. Dellaert, F., Fox, D., Burgard, W., Thrun, S.: Monte Carlo Localization for
Mobile Robots. In: IEEE International Conference on Robotics and Automation
(ICRA99), May 1999
17. Fox, D.: KLD-Sampling: adaptive particle ﬁlters and mobile robot localization. In:
Advances in Neural Information Processing Systems (NIPS) (2001)

A Lightweight Navigation System
for Mobile Robots
M.T. L´azaro1(B), G. Grisetti1, L. Iocchi1, J.P. Fentanes2, and M. Hanheide2
1 DIAG, Sapienza University of Rome, Rome, Italy
{mtlazaro,grisetti,iocchi}@dis.uniroma1.it
2 LCAS, University of Lincoln, Lincoln, UK
{jpulidofentanes,mhanheide}@lincoln.ac.uk
Abstract. In this paper, we describe a navigation system requiring very
few computational resources, but still providing performance comparable
with commonly used tools in the ROS universe. This lightweight naviga-
tion system is thus suitable for robots with low computational resources
and provides interfaces for both ROS and NAOqi middlewares. We have
successfully evaluated the software on diﬀerent robots and in diﬀerent
situations, including SoftBank Pepper robot for RoboCup@Home SSPL
competitions and on small home-made robots for RoboCup@Home Edu-
cation workshops. The developed software is well documented and easy
to understand. It is released open-source and as Debian package to facil-
itate ease of use, in particular for the young researchers participating in
robotic competitions and for educational activities.
Keywords: Navigation · Mobile robots · Open source · RoboCup
1
Introduction
Navigation is among the most important features of mobile robot applications
and is a primary functionality in many tasks. Navigation performance of mobile
robots is typically well established thanks to the availability of standard open-
source software (e.g., ROS amcl and move base) and many robotic platforms
are able to properly use such oﬀ-the-shelf tools after a reasonable conﬁguration
eﬀort. However, such solutions have some limitations: they are strongly embed-
ded in the ROS framework, they require substantial computational power, they
are sometimes not easy to tune and conﬁgure for a particular setting (robot
and environment). While ROS is a de-facto standard for many robotic plat-
forms, there are still cases in which ROS-based tools cannot be fully used. For
example, SoftBank Pepper robot has its own operating system and development
environment that does not support ROS on-board. Small home-made robots
controlled by Arduino and Raspberry boards have limited computational power
with respect to the requirements of typical ROS-based navigation modules.
In several robotic competitions, specially for young researchers, having
a lightweight open-source navigation system is very important to speed-up
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_25

296
M.T. L´azaro et al.
deployment of mobile robot applications. For example, as part of the RoboCup
competitions1, the RoboCup@Home Social Standard Platform, using SoftBank
Pepper robot, introduces a new challenge in navigation because of the need of
deploying a navigation system for RoboCup@Home tasks running with on-board
computational resources of the robot. Similarly, in the RoboCup@Home Educa-
tion initiative2 low-cost robots with limited computational resources are used to
keep the entry-level for new teams low.
Therefore, the release of a lightweight navigation system that can support
both ROS and other robotic development environments, achieving the same per-
formance of standard ROS-based tools, but requiring much less computational
resources, brings many beneﬁts in mobile robot applications specially for involv-
ing many young researchers to robotic competitions and to the use of educational
robots.
In this paper, we describe the development and release of an open-source
lightweight navigation system that has been integrated in and evaluated on
three diﬀerent robotic platforms of diverse proﬁles regarding their middleware
support and their available computational resources: (1) ROS-based robots with
good computational resources, (2) ROS-based robots with limited computational
resources, (3) NAOqi-based robots (in particular, SoftBank Pepper) with limited
computational resources. Notice that, while in the ﬁrst case our system achieves
similar performance as standard ROS-based tools (i.e., amcl and move base),
in the other two cases these tools are not suitable either because of too limited
resources or non-availability of ROS middle-ware.
The proposed solution has been also been deployed in the context of the
RoboCup@Home Social Standard Platform League, within the team SPQReL
that is a joint research collaboration between Sapienza University of Rome, Italy
and University of Lincoln, UK. Within the context of RoboCup@Home Social
Standard Platform League, the navigation package described in this paper over-
comes current limitations of the navigation system of the Pepper robot since
because (1) it is open-source, (2) it runs on the Pepper on-board PC, (3) it pro-
vides navigation performance comparable with ROS standard tools.
2
Related Work
Mobile robot navigation is one of the key competences a robot must have to
be fully autonomous. For this reason, it is usual to ﬁnd at least one naviga-
tion component in most robot software frameworks. Although there are multiple
frameworks for mobile robot software development, in general, it is safe to say
that robotics navigation speciﬁc software can be divided in three main com-
ponents: localization, global path planning and local path planning or reactive
navigation.
Although localization has been probably the most researched ﬁeld in mobile
robotics, most well known robotics frameworks like ROS or the Player project
1 www.robocup.org.
2 www.robocupathomeedu.org.

A Lightweight Navigation System for Mobile Robots
297
include a standard localization component based on 2D laser and an AMCL [3]
localization ﬁlter. Usually this component can be replaced depending on the
robot set-up, using, for example, EKF and GPS information for outdoor robot
localization or the integration of vision based localization systems.
The integration of both global and local path planners however has had more
variety of options. For example, the Player project uses a standard wavefront
propagation planner [8] for generating global plans that drive the robot towards
its goal, while, to deal with unmapped obstacles, it provides three diﬀerent meth-
ods that drive the robot away from obstacles and towards the goal using virtual
force ﬁelds [10] or decision trees [7].
More recently, the robot operating system (ROS) has proposed the use of a
very ﬂexible navigation stack [6] that uses a global dynamic window approach [2]
that not only plans a global plan to the goal and then follows it blindly but also
re-plans the path if the robot deviates from it to avoid unmapped obstacles.
In addition to this, ROS uses a roll-out trajectory planner [5] to send velocity
commands to the robot base that follows the planned trajectory as close as
possible given the robot constrains and unmapped obstacles. Alternatively to
this planner, ROS also provides a dynamic window planner [4] that is more
eﬃcient that the trajectory roll-out as it samples a smaller velocity space ﬁltering
out unachievable trajectories using the robot’s acceleration limits.
Other authors propose having a planning library [9] that can easily be inte-
grated in any system and provides multiple tools to create the best possible
planner, these proposals can create highly customizable navigation systems and
are not restricted to any speciﬁc software framework.
Our proposal is to provide an open source approach to navigation that can
be used both as an additional navigation system for ROS or as an open source
alternative for NAOqi-based robots that is lightweight and can be used directly
in robots with limited computational power.
The proposed robot navigation suite comprises a variant of Monte Carlo
Localization, a Dijkstra-based global path planner, and a policy-based dynamic
obstacle avoidance, all described in detail in the following. The suite is completed
by visualization and remote control tools.
3
Localization Filter
The localization module implements an eﬃcient version of the Monte Carlo
Localization. The idea is to track the robot position with a particle ﬁlter. The
state of the ﬁlter comprises the 2D robot pose and orientation xt = (xt; yt; θt)T
and the belief space is represented as a set of pose samples x(i). The denser the
samples in a region of the environment, the more likely is that the robot will
be in that region. The state transition is governed by odometry measurements
ut = (Δxt; Δyt; Δθt)T , that express the relative movement of the robot between
subsequent time steps.

298
M.T. L´azaro et al.
Each time a new odometry measurement is received, we generate a new set
of samples according to the following equation:
xi
t|t−1 = x(i)
t−1 ⊕(ut−1 + n(i)
t−1)
(1)
nt−1 ∼N(0, Σt−1).
(2)
Here nt−1 is a sample drawn from a zero mean Gaussian distribution with
covariance Σt−1 representing the additive noise aﬀecting the odometry. The
covariance of this distribution is adapted based on the magnitude of the odom-
etry motion. If the robot does not move, then Σt−1 = 0 and no sampling is
performed. If one of the samples falls in the invalid space of the environment
(e.g. unknown or inside a wall), it is replaced by a new sample drawn at a ran-
dom valid location. This substantially enhances the performance during global
localization.
When a sensor measurement zt becomes available, we reﬁne our predicted
belief {x(i)
t|t−1} through conditioning. To this end we assign each predicted par-
ticle a weight w(i), proportional to the likelihood of the measurement. The like-
lihood l(zt, x) ∈ℜis a function expressing how well the current measurement
zt approximates a predicted measurement obtained from the known map if the
robot was at location x. Ideally, if the predicted measurement and the actual
one are the same, the likelihood is maximal. Once the likelihood is computed for
each predicted sample, we generate the posterior distribution by replicating or
suppressing samples depending on their weight. More formally, we draw a set of
new indices from the weight distributions, as follows:
it ∼w(i)
(3)
Equation 3, means that the index it is obtained by sampling from the piece-
wise constant distribution of the weights. The index i has probability w(i)
t
to be
selected. The fact that an index is selected from the sampling means that the
corresponding particle will appear in the update distribution. Samples having
high weight can be drawn more than once. The above procedure is called resam-
pling and has the eﬀect of turning a weighted distribution in an unweighted one
by replicating likely samples and suppressing unlikely ones.
In our system, the sensor measurements zt are the laser endpoints. We use a
fast but robust procedure to calculate the likelihood. First, given a robot pose
hypothesis x(i)
t
and the current laser measurement, we compute the position of
the laser endpoints in the map as follows:
ˆz(i)
k
= x(i)
t|t−1 ⊕zk
(4)
where the index k represents one speciﬁc beam within the laser measurement zt.
Subsequently, for each endpoint we compute minimal distance between the end-
point and the closest obstacle in the map. This operation can be performed in
O(1), by using a precalculated grid that stores for each cell the minimal distance
to the obstacles: the distance map. To lessen the eﬀects of dynamic unpredicted

A Lightweight Navigation System for Mobile Robots
299
obstacles, we clamp the reported distance to a maximum value. Let d(i)
k
be the
distance of the kth beam from the closest obstacle w.r.t. particle i. If a measure-
ment is perfectly explained, the distances will be zero. The ﬁnal likelihood of a
particle computed as
w(i) = exp(−

σd(i)),
(5)
where σ is a scaling factor to account for diﬀerent sensor accuracies. Before the
resampling step the weights are normalized so that their sum is 1.
4
Path Planning and Obstacle Avoidance
The planner module implements a fast global path planner that adapts the com-
puted path to dynamic changes in the environment. The system takes advantage
of this eﬃcient computation to consider planning only at a global level, in con-
trast to other planners that manage both local and global maps, which prevents
from having a uniﬁed view of the environment.
This is facilitated by an eﬃcient implementation and management of the
distance map, which, as mentioned in previous Sect. 3, stores in each pixel the
distance to the closest occupied cell. Figure 1(middle) shows an example of dis-
tance map associated to a portion of map shown if Fig. 1(left). Once the distance
map from the static map is obtained, is it possible to add the information from
the dynamic obstacles in the same structure which are managed as explained in
the next Sect. 4.1.
Using this ﬁnal distance map that includes both static and dynamic infor-
mation, our system approaches planning by computing the minimal paths to the
goal on a 2D grid using the Dijkstra algorithm. Instead of reporting just a single
path, we compute a policy such that each cell of the grid points to the closest
cell to reach the goal. We assume that the robot can travel from a cell to its eight
neighbors, and that the cost of the transition decreases linearly with the distance
from the obstacles. Other parameters such as a safety distance from obstacles
or the robot radius inﬂuences the ﬁnal cost of each cell, which is saturated to a
maximum value. Figure 1(right) shows an example of such grid, called cost map.
Computing the cost on a grid 1000 × 1000 takes 20 ms on a Intel(R) Core(TM)
i7-6500U CPU @ 2.50 GHz.
The ﬁnal plan provided is executed by using the motion controller by [1] as
explained in Sect. 4.2.
4.1
Management of Dynamic Obstacles
The distance map is recomputed at each new measurement by incorporating the
detected obstacles. This operation can be performed eﬃciently in an additive
fashion by adding only the obstacles to the distance map representing the static
scene. In order to track dynamic obstacles we keep a list of unexplained laser
endpoints (or grid cells), that are those endpoints that fall far from an occupied
cell in the static map.

300
M.T. L´azaro et al.
Fig. 1. Left: Portion of an input map. Middle: Distance map. Right: Cost map.
Two diﬀerent policies are considered to manage the dynamic obstacles. The
ﬁrst policy is to suppress old obstacles when a new measurement conﬁrms that
they have been removed. Intuitively, this situation occurs when the new measure-
ment “goes through” an existing obstacle. More concretely, obstacle points are
transformed into the robot’s reference frame. Those points that are in the ﬁeld
of view of the robot are projected into a circular array of K bins that will store
the closest old and new obstacle falling in each bin together with their distance
w.r.t. the robot. Then, given an old po
k and new pn
k obstacle points falling in the
same bin k whose distances to the robot frame are do
k and dn
k respectively, po
k is
removed if the new point appears behind the old one or if it is in a Euclidean
distance lower than a threshold ϵ as
dn
k > do
k
,
|dn
k −do
k| < ϵ
(6)
This procedure is illustrated in Fig. 2.
The second policy suppresses obstacles points after a certain time passes. This
is done by assigning each obstacle point a time stamp of the moment it has been
seen for the last time. This intuitive but eﬀective policy allows to re-consider
paths that could have been discarded due to temporally-static obstacles.
Fig. 2. Management of dynamic obstacle points. Current (red) and old (green, blue)
points in the robot’s ﬁeld of view are projected on a circular array of ﬁxed number of
bins. Old points are removed if a new point corresponding to the same bin appears
behind the old ones (blue points).

A Lightweight Navigation System for Mobile Robots
301
4.2
Motion Generation
Once a ﬁnal path to the goal is provided, an intermediate waypoint is com-
puted in a short distance from the robot (e.g., 1m ahead from the current robot
pose). This waypoint is used to calculate an attractive virtual force F that is
applied to the robot to generate its movement. Then, using the motion controller
described in [1] it is possible to transform this force into the desired control input
u = (v
ω)T , the linear and angular velocities of the robot using the following
diﬀerential equation:
˙u = Au + BF
(7)
where
A = −2b

1 0
0 ki

B =

1 0
0 kih

F =

Fcosθ
Fsinθ

(8)
Details on the controller parameters b (viscous friction coeﬃcient), ki (inertial
coeﬃcient) and h (moment arm) are explained in [1]. Notice that the intermedi-
ate waypoint changes as the robot moves since it always refers to a ﬁxed point
w.r.t the current position of the robot, providing a smooth execution of the
computed trajectory.
5
Software Description
The software is released as a git repository at https://github.com/LCAS/spqrel
navigation/. Links and additional instructions are available from the repositories
Wiki (https://github.com/LCAS/spqrel navigation/wiki) and the SPQReL web
site3. The software is released as source code to be compiled with either ROS or
NAOqi, but also as Debian packages for Ubuntu 16.04LTS desktop development4.
Integration with other components of the robotic applications is performed
in diﬀerent ways depending on the platform-dependent wrapper used. The ROS
wrappers are compatible with amcl and move base, thus they use the same
name for ROS topics and actions allowing for an easy replacement in already
existing ROS applications. The main parameters also correspond to the equiva-
lent ones in standard ROS tools, although our navigation system has much less
parameters and thus requires less conﬁguration eﬀort. The NAOqi wrappers use
instead a diﬀerent mechanism that is based on the communication through the
NAOqi shared memory (ALMemory), either by means of writing and reading
data directly in memory or by raising/subscribing to memory events.
Both the wrappers use a map of the environment described in the same
YAML format used in standard ROS navigation applications. This map should
be generated beforehand with any available tool compatible with ROS standard
maps (e.g., gmapping).
3 https://sites.google.com/dis.uniroma1.it/spqrel.
4 For installation of binary packages see https://github.com/LCAS/rosdistro/wiki and
install ros-kinetic-spqrel-navigation.

302
M.T. L´azaro et al.
5.1
Visualization Tools
For ROS-based systems, RViz can be used to visualize the information relevant
to the task (localization particles, path planned, etc.) For non-ROS systems,
we have developed simple viewers to view the information processed by the
components and to tune the parameters. In particular, we will describe below
the localizer viewer and the planner viewer.
Fig. 3. Left: Localizer viewer. Red points represent the ﬁlter particles. Green points
are current laser points that can be explained by the map while blue points are laser
points not explained by the map (e.g., new obstacles not represented in the map).
Middle: Planner viewer. Right: Distance map.
Through the localizer viewer it is possible to see the outcome of the local-
ization system. As shown in Fig. 3 (left), the viewer shows the map, the current
laser scans and the particles that are currently stored in the ﬁlter. The viewer
also allows for setting the initial pose of the robot in a speciﬁc pose of the
environment or to call for a global localization phase if the pose is not known
accurately.
Similarly, the planner viewer allows to visualize the state of the navigation
system. As shown in Fig. 3 (middle), the viewer shows the map, current pose of
the robot provided by the localization system, current goal (if given) and the
computed path if the goal is reachable from the current pose. From this viewer
it is also possible to visualize the current distance map computed from the given
map and the added dynamic obstacles (see Fig. 3 (right)), cancel a goal or restore
the distance map to its original state (i.e., cancel the obstacles added during the
navigation task).
6
Experiments
In this section we present evaluation experiments of our navigation system. The
experiments aim at verifying that our navigation system achieves similar perfor-
mance of a typical ROS-based navigation system for mobile robots in oﬃce-like
environments, but with much less computational resources. The navigation soft-
ware has been validated on three diﬀerent robots in an oﬃce-like environment
as described in the next sections.

A Lightweight Navigation System for Mobile Robots
303
6.1
Experimental Environment
The environment considered in the experiments reported in this paper is a typical
indoor environment in our Department, representative of other similar environ-
ments, including RoboCup@Home scenarios.
The total size of the environment is about 50 m × 50 m for which a map
was acquired at a resolution of 5 cm, generating a grid of 1010 × 1070 pixels
(illustrated in Fig. 3) and given in input to the three robotic platforms.
6.2
Robots
Three diﬀerent kinds of robots have been used in these experiments, illustrated
in Fig. 4.
Fig. 4. Left: Robot Diago. Middle: Robot MARRtino. Right: SoftBank Robotics
Pepper.
Diago5 is a robot based on a Segway base and a home-made torso supporting
many sensors and mainly used for Human-Robot Interaction (HRI) tasks. It
includes a powerful laptop with Intel i7-6700 CPU @ 3.40 GHz, 16 GB RAM.
MARRtino6 is a low-cost home-made robot controlled with an Arduino
board and a Raspberry PI 3 Model B operated by a 1.2 GHz Quad core ARMv8
CPU with 1GB RAM and running Ubuntu 16.04 and ROS Kinetic.
Pepper7 is a robot developed by SoftBank, specially designed for HRI and
social interaction. It has an omni-directional drive platform and carries an Intel
Atom E3845 @ 1.91 GHz Quad core as processing unit with 4 GB RAM. Its
development environment is NAOqi, while ROS is not supported on-board of
the robot.
All the robots are equipped with a laser range ﬁnder for localization and
obstacle avoidance and a diﬀerential drive navigation mechanism. Although
other sensors are available on some of these robots and Pepper is an omni-
directional drive platform, these additional features are not used in the experi-
ments reported here, in order to compare the three platforms on a common set of
5 https://sites.google.com/a/dis.uniroma1.it/diago/.
6 https://sites.google.com/dis.uniroma1.it/marrtino.
7 https://www.softbank.jp/en/robot/.

304
M.T. L´azaro et al.
sensor and mobility features. It is worth to mention the challenge of approaching
laser-based navigation for Pepper robot due to its limited range (about 5 m) and
sparsity of the provided laser data (45 points in a ﬁeld of view of 240 degrees).
6.3
Experimental Results
The experiments reported in this paper consist in the execution of a path in
the environment with static and dynamic obstacles. The overall length of the
path followed by the robots is about 30m. The robots start from a known initial
pose and they come back to the same position. During this path we measured
execution time and computational resources. Moreover, we qualitatively observed
that the path performed by the robots and the way in which obstacles were
avoided was adequate to the situation. This is demonstrated through videos of
the use of the system available at the SPQReL website8.
We have compared performance of our navigation system with respect to
the ROS standard navigation modules amcl and move base. We decided to use
the default parameters for both the ROS modules and our components. This
choice was motivated by our interest in evaluating these tools from a non-expert
user perspective. We want to point out that the resources consumption of our
software is minimal for the localization process when the robot is not moving,
or for the path planning process when a goal is not active. For these reasons,
measurements reported in this section consider the situation in which a goal is
active and the robot is moving towards it.
Tables 1 and 2 summarize the computational resources consumption by the
diﬀerent software suites in terms of percentage of CPU and memory usage on
the three platforms. As shown by the results, our implementation for the local-
ization and path planning processes is more eﬃcient in terms percentage of CPU
usage with respect to the ROS standard modules while the diﬀerences on per-
centage of memory usage are anecdotal. It is worth to notice that the current
implementation is single-thread for each component (one thread for the localizer
and one thread for the planner), so other processes of the application can use
other available cores.
Table 1. Average computational resources required for the localization process on
the diﬀerent platforms with the ROS standard tools and our proposed software. Data
obtained using the Linux command top.
Localization
Robot
Software
%CPU %Mem
Diago
ROS - amcl
5.65
0.2
ROS - SPQReL
3.12
0.4
MARRtino ROS - amcl
7.14
3.2
ROS - SPQReL
9.06
5.6
Pepper
NAOqi - SPQReL 2.15
1.8
8 https://sites.google.com/dis.uniroma1.it/spqrel/multimedia.

A Lightweight Navigation System for Mobile Robots
305
Table 2. Average computational resources required for the path planning process on
the diﬀerent platforms with the ROS standard tools and our proposed software. Data
obtained using the linux command top.
Path planning
Robot
Software
%CPU
%Mem
Diago
ROS - move base
41.2
0.3
ROS - SPQReL
25.72
0.8
MARRtino ROS - move base
103.72∗
2.1
ROS - SPQReL
98.07
11.15
Pepper
NAOqi - SPQReL
38.46
2.8
We would like to point out that the values reported in Table 2 for the robot
MARRtino running move base are a best-case performance. We experienced
several issues and warnings when running move base on the Raspberry which,
in general, were due to missed rates in control loops, robot pose requests or map
updates. These issues make the experience of using move base on the Raspberry
diﬃcult and confusing for students or other non-expert users.
Furthermore, we have measured execution times of each cycle of our localiza-
tion and path planning processes. Times are sumarized in Table 3. As shown in
the table, if laser and odometry data are acquired at 10 Hz (typical case), only
one core will be used at 100 %.
Table 3. Average execution times of each cycle of the localization and path planning
processes on the diﬀerent platforms.
Cycle times
Robot
Localization Path planning
Diago
2.55 ms
36.1 ms
MARRtino 15 ms
320 ms
Pepper
7 ms
104.25 ms
7
Conclusions
The open-source lightweight navigation system described in this paper and
released as open-source and Debian package can improve and speed up develop-
ment of many mobile robot applications, specially when low-cost robots or robots
with limited computational resources are used. This feature is very important
to spread the use of mobile robot applications and for educational purposes.
The system described in this paper can thus be very useful to young
researchers that are willing to build robotic applications with limited resources.
Some examples in which our software have been successfully applied is within

306
M.T. L´azaro et al.
RoboCup@Home SSPL and RoboCup@Home Education competitions in which
standard navigation systems (e.g., ROS amcl and move base) cannot run. More-
over, the open-source distribution allows the research community to improve the
system (e.g., adding more features) and to increase its scope (e.g., porting it
to other robotic platforms). Finally, we believe this software provides a didac-
tic contribution when teaching subjects like autonomous robots or probabilistic
robotics, since the code was actually developed in such a context.
Acknowledgements. The work is partially supported by the European Community’s
funded project 732773 ‘ILIAD’, and the RoboCup Federation’s Collaboration funds.
References
1. Asensio, J.R., Montano, L.: A kinematic and dynamic model-based motion con-
troller for mobile robots. In: The 15th IFAC Triennial World Congress, Barcelona,
Spain, 21–26 July 2002 (2002)
2. Brock, O., Khatib, O.: High-speed navigation using the global dynamic window
approach. In: 1999 IEEE International Conference on Robotics and Automation,
1999, Proceedings, vol. 1, pp. 341–346. IEEE (1999)
3. Fox, D., Burgard, W., Dellaert, F., Thrun, S.: Monte carlo localization: eﬃcient
position estimation for mobile robots. AAAI/IAAI 1999(343–349), 2–2 (1999)
4. Fox, D., Burgard, W., Thrun, S.: The dynamic window approach to collision avoid-
ance. IEEE Robot. Autom. Mag. 4(1), 23–33 (1997)
5. Gerkey, B.P., Konolige, K.: Planning and control in unstructured terrain. In: ICRA
Workshop on Path Planning on Costmaps (2008)
6. Marder-Eppstein, E., Berger, E., Foote, T., Gerkey, B., Konolige, K.: The oﬃce
marathon: robust navigation in an indoor oﬃce environment. In: International
Conference on Robotics and Automation (2010)
7. Minguez, J., Montano, L.: Nearness diagram (ND) navigation: collision avoidance
in troublesome scenarios. IEEE Trans. Robot. Autom. 20(1), 45–59 (2004)
8. Murphy, R.R., Hughes, K., Noll, E.: An explicit path planner to facilitate reac-
tive control and terrain preferences. In: 1996 IEEE International Conference on
Robotics and Automation, 1996, Proceedings, vol. 3, pp. 2067–2072. IEEE (1996)
9. S¸ucan, I.A., Moll, M., Kavraki, L.E.: The open motion planning library. IEEE
Robot. Autom. Mag. 19(4), 72–82 (2012). http://ompl.kavrakilab.org
10. Ulrich, I., Borenstein, J.: VFH+: reliable obstacle avoidance for fast mobile robots.
In: IEEE International Conference on Robotics and Automation (ICRA 1998), pp.
1572–1577 (1998)

Visual Perception for Robotics

Bridge Mapping for Inspection Using
an UAV Assisted by a Total Station
Javier Prada Delgado(B), Pablo Ramon Soria, B.C. Arrue, and A. Ollero
University of Seville, 41092 Seville, Spain
javpradel@alum.us.es, {prs,barrue,aollero}@us.es
Abstract. In this paper it is proposed the use of a Total Station as
odometry helper to improve the localization of UAVs for 3D reconstruc-
tion of underside of bridges where typically lacks of GPS signal and steel
structures might produce interferences on the measures on the UAVs.
The information from the Total Station is sent to the UAV through a
WIFI network and is fused with data from the IMU as odometry. Robot
is also provided with a RGB-D camera which provides pointclouds for
building the map.
Keywords: Bridge inspection · Total Station · UAV
1
Introduction
UAVs (Unmanned Aerial Vehicles) have become a very useful technology for
doing autonomous tasks in a short time. Nowadays, this ﬁeld continues growing
and we can even begin to see the results in areas such as aerial ﬁlmography and
agriculture.
An interesting area of the use of UAVs is the inspection of places where
the access for people is diﬃcult. These can be environmental catastrophe areas
or just areas where, due to their orography, access to them is impossible using
ground vehicles. UAVs allow to access remotely these areas and, depending on
the hardware carried onboard, visualize the place, create a map, analyze the
environment and so on.
Positioning UAVs on the space is very important. This will allow them to
ﬂy in autonomous mode or even create a map at the same time, also know
as Simultaneous Localization and Mapping (SLAM). However, there are still
environments where the UAVs can not positionate themselves well with the
current positioning systems. This is for example in situations of low visibility
(when using visual systems) or where there is not GPS signal. In these cases,
it is dangerous to use the UAV in autonomous mode and it will probably be
necessary the intervention of a ﬂight operator. Moreover, if the GPS information
is incorrect it can distort the local state estimation. Thus, if it is used in the
software to create a 3D model it can produce sifts and missalignments.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_26

310
J. Prada Delgado et al.
Instead of relying the positioning system on the GPS, it can be improved
combining several methods. This will help the software to overcome the GPS
errors, so improving the positioning system and hence for the performance of
UAVs tasks as for example a full 3D model reconstruction.
This paper focus on bridge 3D reconstruction, which is important for inspec-
tion tasks where the access to the bridge is diﬃcult. This models generated are
typically used to visualize defects on the bridge remotely.
To do this, we will map a 3D model of the bridge using an onboard camera
that will give us a pointcloud. Due to being under the bridge, the system will
not have GPS positioning (Global Positioning System). This is by the reinforced
concrete that compose the bridge, since it hinders a good GPS signal. To solve
this problem, a Leica Total Station will be used which will facilitate a local
reference for positioning of the UAV in the space.
In addition to he total station the IMU (Inertial Measurement Unit) informa-
tion of the autopilot is used too. The combination of both systems remarkably
improve the 3D reconstruction.
The remainder of the article is structured as follows:
– Section 2 introduces the state of the art.
– Section 3 explains how to use the Total Station as a ground truth and the
basic conﬁguration of the used system.
– Section 4 details how is alignment process solved.
– Section 5 explains the results and the procedure followed to perform the exper-
iments.
– Section 6 ends with the conclusions of the article.
2
State of the Art
As mentioned above, accurate positioning with an UAV can become a complex
problem. There are currently many additional systems that allow the positioning
of an UAV to be relatively accurate. One of the sensors used is the optical
ﬂow [1,4,10]. This sensor allows, with the help of a camera, to measure the
relative movement of environment relative to the UAV. Usually the camera is
placed facing down.
Another widely used sensor is ultrasonic [3,6]. It is less accurate than the
optical ﬂow, instead, it cost less money and are easier to implement. The mea-
surement obtained with this system usually has interferences and can depend a
lot on the environment. However these ultrasonic sensors are widely extended in
commercial UAVs for amateur pilots.
Something less common but also interesting are the systems based on sig-
nal measurements and communications, also know as Range Only Simultane-
ous Localization and Mapping (RO-SLAM) [2,8]. An example are the Signals of
Opportunity (SOP) that can be emanating from a cellular tower [9]. The authors
demonstrate that fusing SOP pseudoranges in a GNSS-SOP-INS framework pro-
duces a better solution than a traditional GNSS-INS framework. Another exam-
ple are the LTE systems, which uses the cellular LTE signals. This method is

Bridge Mapping for Inspection Using an UAV Assisted
311
based on estimating the channel impulse response from the received LTE sig-
nal [15]. Authors in [12] used a Bluetooth framework for localizing teams of
robots without static Beacons using the range intensity signal of the modules.
Novadem company uses another diﬀerent method, the Local Positioning Sys-
tem (LPS) [11]. The LPS technology is close to the GPS technology but instead
of using satellites, it uses terrestrial beacons which create a local positioning
network. It has approximately an accuracy of 10 cm and does not need GPS.
The accuracy obtained with these systems is suﬃcient for navigation, but not
to improve the map of the environment by converging a series of pointclouds on
a complete map. To improve that it has been used a better system based on very
accuracy sensors. This is the case of an automatic theodolite also known as Total
Station (TS) from Leica Geosystems. This will be the system that will aid the
positioning which aim is to improve the convergence of diﬀerents pointclouds.
3
Total Station as a Ground Truth
Without the possibility of use GPS, it is needed a positioning method for improve
the 3D reconstruction of the bridge. Here is when is interesting the use of a Total
Station. This is a very precise laser positioning machine from Leica Geosystems,
one of the most important companies in spatial measurement and tophograpy.
It measure the position pointing to a 360◦prism with a laser. The light
is reﬂected and then we obtain polar coordinates of the prism, which can be
(a) Leica mini prism 360o.
(b) Leica Total Station MS50.
Fig. 1. Total Station system

312
J. Prada Delgado et al.
transformed into X, Y and Z axis. Later, the information is sent to the onboard
PC which collects all the data and make the 3D model. It can measure to a
maximum distance of 3 km with a accuracy of 1 mm.
The Total Station is connected to a computer that will communicate with the
UAV onboard system. It sends the position and the UAV computer saves that
info in a ROS bag, including the orientation (using mavros) and the pointcloud
info from a Kinect camera. Then we will use all that topics information to
compose a complete pointcloud (Fig. 1).
The system diagram is explained in Fig. 2. The odometry information is
sended to the onboard computed by WIFI. The UAV has the prism, a computer
and the kinect camera.
Fig. 2. System diagram.
Fig. 3. Pointcloud map taken by the Total Station.

Bridge Mapping for Inspection Using an UAV Assisted
313
The Total Station can create also a pointcloud without the help of an UAV.
It take a long time (about 15 min) to measure a simple pointcloud from one side.
This means that if we want to create a pointcloud of the whole bridge it will
take approximately 1 h. With the use of an UAV it can be performed in real
time and furthermore reach not visible areas from the perspective of the Total
Station (Fig. 3).
4
Alignment Process
The aim of this paper is to build bridge’s map from several pointclouds. To do
this, odometry and visual information is obtained from diﬀerent sensors:
– Kinect camera: it will get the images and pointclouds needed to create the
map (Fig. 4).
– Pixhawk autopilot [13]: using mavros [7] node from ROS (Robot Operating
System) to get the orientation of the UAV. That will allow to have the rotation
matrix and perform the position transformation of one pointcloud and the
previous one (Fig. 5).
– Total Station and 360◦prism: it will get the position (X, Y and Z) of the
UAV. This information is necessary for the transformation matrix (Fig. 6).
The more accurate the information is, the easier will be to build the map
and better quality it will have. That is the reason the Total Station is chosen to
obtain a stable local position of the UAV.
For the alignment process we will use a ros package called rtabmap [5,14].
This ros package does not use external odometry by default. But the problem of
Fig. 4. Kinect camera.
Fig. 5. Pixhawk autopilot.

314
J. Prada Delgado et al.
Fig. 6. Prism ﬁxed to the UAV.
not using external odometry is that if you visualize a white wall, where there are
not similar features between a pointcloud and the next one, the software will not
correctly simulate the visual odometry and will not ﬁt well a pointcloud with
the previous one. To solve this, external odometry is proposed.
5
Experimental Testing
5.1
Experiment with Total Station
To prove what is proposed above, we will perform a set of experiments on non
transit bridges. These bridges are in the Escuela T´ecnica Superior de Ingeniera
in the University of Seville.
One of the bridges is more exposed to sunlight and this is an inconvenient
for the Kinect when taking the pointclouds (see Fig. 7). The reason why it is an
inconvenient is because the Kinect camera projects a set of infrared dots and
then visualize them with an infrared camera. But the sunlight has a component
of infrared light in his spectrum, so the information is mixed and the experiment
is not valid at all.
The other bridge is less exposed to the sunlight (see Fig. 8), so that is the
one in which we are going to work. The aim is demonstrate that using external
odometry with a Total Station is easier and more accurate than just with visual
odometry by software (matching features between pointclouds).
We have focused on analyzing three columns of the bridge. If the odometry
is ﬁne, it should be able to ﬁx the deviation between pointclouds and the back-
ground should be perfectly straight, as well as the trajectory. As we can see in
Fig. 9, there are three vertical ﬂights corresponding to each of the columns.
The next ﬁgure is an image of the experiment zone whole map. In just one
pass we have mapped three columns with a good quality (Figs. 10 and 11).
We have taken several measurements to compare the real dimensions of the
bridge and the model 3D. On average, the error is about 3 cm.

Bridge Mapping for Inspection Using an UAV Assisted
315
Fig. 7. Total Station taking measurements for odometry position under one of the
bridges.
Fig. 8. Photo under the bridge used for mapping.
Fig. 9. UAV Trajectory under the bridge.

316
J. Prada Delgado et al.
Fig. 10. Single pointcloud of a column and UAV trajectory (color pink).
Fig. 11. Pointcloud map and UAV trajectory (blue color).
Fig. 12. Pointcloud map represented in Matlab with datatips.

Bridge Mapping for Inspection Using an UAV Assisted
317
5.2
Experiment Without Total Station
In this case the same bag ﬁle of the previous experiment has been used. The
diﬀerent is that now we have run the rtabmap package but without the Total
Station and IMU odometry. The package looks for matching features between
pointclouds and estimates the position of the UAV with that visual information
(Fig. 12).
Using the same ﬁle, the software can not create the map and loses the previous
pointcloud, due to the big discrepancy between the ﬁrst pointclouds (it starts on
a bright zone and there is not much information). From one pointcloud to the
next one there is such a few coincident points and the software is not be able to
follow the movement of the UAV (see Fig. 13). That is the reason why external
odometry is useful.
Fig. 13. Rtabmap package can’t estimate the position with visual odometry.
6
Discussion
It has been proven that the use of a total station remarkably improves the
odometry of the UAV for the mapping process. Additionally it is useful when
there is not much information in the pointcloud, i.e. ﬂat and textureless surfaces,
or just out of RGB-D camera’s range. If the ros package is not able to generate
an odometry from the point clouds, it will not be able to generate a bridge map.
Thus, the use of a total station is important to generate a reliable odometry
avoiding drifts that can be used to reconstruct the map. This map could be
improved using plane ﬁtting, which allows to approximate the pointclouds to

318
J. Prada Delgado et al.
bridge’s planes. This is possible due to the large number of ﬂat areas there are
on a bridge, which would ﬁx some alignment errors and reduce the 3 cm error [16].
In future step the system will be tested in larger and taller bridges in adverse
conditions. For example, sunlight might aﬀect to the quality and accuracy of the
clouds.
Furthermore, the comming work will focus on the use of these map and
information recorded by the UAV for detecting faults in the bridges. As the aerial
robot has the capabilities for reaching higher places and approximate to them,
it is able to obtain more accurate information without taking human risks. The
built maps will be used for detecting beam deﬂections and other inappropriate
bends on the structure.
Acknowledgements. This work has been supported by the H2020 Project AER-
OBI (H2020-ICT-2015-687384) funded by the European Commission and AEROMAIN
(DPI2014-5983-C2-1-R) Project, funded by the Spanish Ministerio de Economia, Indus-
tria y Competitividad.
References
1. Aasish, C., Ranjitha, E., Ridhwan, R., et al.: Navigation of UAV without GPS. In:
2015 International Conference on Robotics, Automation, Control and Embedded
Systems (RACE), pp. 1–3. IEEE (2015)
2. Djugash, J., Singh, S., Kantor, G., Zhang, W.: Range-only slam for robots operat-
ing cooperatively with sensor networks. In: Proceedings 2006 IEEE International
Conference on Robotics and Automation, ICRA 2006, pp. 2078–2084. IEEE (2006)
3. Gupta, N., Makkar, J.S., Pandey, P.: Obstacle detection and collision avoidance
using ultrasonic sensors for RC multirotors. In: 2015 International Conference on
Signal Processing and Communication (ICSC), pp. 419–423. IEEE (2015)
4. Hrabar, S., Sukhatme, G.S., Corke, P., Usher, K., Roberts, J.: Combined optic-
ﬂow and stereo-based navigation of urban canyons for a UAV. In: 2005 IEEE/RSJ
International Conference on Intelligent Robots and Systems, (IROS 2005), pp.
3309–3316. IEEE (2005)
5. Labbe, M., Michaud, F.: Online global loop closure detection for large-scale multi-
session graph-based SLAM. In: Proceedings of the IEEE/RSJ International Con-
ference on Intelligent Robots and Systems, pp. 2661–2666, September 2014
6. Lange, S., Sunderhauf, N., Protzel, P.: A vision based onboard approach for land-
ing and position control of an autonomous multirotor UAV in GPS-denied environ-
ments. In: International Conference on Advanced Robotics, ICAR 2009, pp. 1–6.
IEEE (2009)
7. Mavros node from ROS. http://wiki.ros.org/mavros
8. Menegatti, E., Zanella, A., Zilli, S., Zorzi, F., Pagello, E.: Range-only slam with
a mobile robot and a wireless sensor networks. In: IEEE International Conference
on Robotics and Automation, ICRA 2009, pp. 8–14. IEEE (2009)
9. Morales, J., Roysdon, P., Kassas, Z.: Signals of opportunity aided inertial naviga-
tion. In: Proceedings of ION GNSS Conference, pp. 1492–1501 (2016)
10. Muratet, L., Doncieux, S., Meyer, J.-A.: A biomimetic reactive navigation system
using the optical ﬂow for a rotary-wing UAV in urban environment. In: Proceedings
of the International Session on Robotics (2004)

Bridge Mapping for Inspection Using an UAV Assisted
319
11. Novadem Beacons (2014). https://www.suasnews.com/2014/12/novadem-unveils-
a-new-positioning-technology-for-uav-allowing-automatic-inspection-of-structures
-without-gps-coverage/
12. Ramon Soria, P., Felipe Palomino, A., Arrue, B.C., Ollero, A.: Bluetooth network
for micro-uavs for communication network and embedded range only localization.
In: 2017 International Conference on Unmanned Aircraft Systems (ICUAS), June
2017
13. Pixhawk web. https://pixhawk.org/
14. rtabmap node from ROS. http://wiki.ros.org/mavros
15. Shamaei, K., Khalife, J., Kassas, Z.: Performance characterization of positioning
in LTE systems. In: Proceedings of ION GNSS Conference, pp. 2262–2270 (2016)
16. Taguchi, Y., Jian, Y.-D., Ramalingam, S., Feng, C.: Slam using both points and
planes for hand-held 3d sensors. In: 2012 IEEE International Symposium on Mixed
and Augmented Reality (ISMAR), pp. 321–322. IEEE (2012)

Multi-view Probabilistic Segmentation of Pome
Fruit with a Low-Cost RGB-D Camera
Pablo Ramon Soria1(B), Fouad Sukkar2, Wolfram Martens3, B.C. Arrue1,
and Robert Fitch2
1 University of Seville, 41092 Seville, Spain
{prs,barrue}@us.es
2 University of Technology Sydney, Ultimo, NSW 2007, Australia
fouad.sukkar@student.uts.edu.au, rfitch@uts.edu.au
3 The University of Sydney, Sydney, NSW 2006, Australia
w.martens@acfr.usyd.edu.au
Abstract. Fruit harvesting is a topic of intereset in agricultural indus-
tries. In order to perform this task, robots should be able to recognise and
segment fruit in their perceptual environment. Particularly, apple trees
are often arranged as planar trellis structures in commercial orchards.
The vine-like branches have leaves that can occlude fruit and produce
noise in typical depth sensor data that also populates the scene with
objects that are not of interest. In this paper, we present a method that
uses a Dirichlet mixture of Gaussian processes and a Gibbs-Sampler for
segmenting clusters of apples to support selective autonomous harvest-
ing. Furthermore, the model provides probabilistic reconstruction of the
entire apple which can be used for better grasping of the fruit.
Keywords: Probabilistic segmentation · RGB-D · Agricultural robotics
1
Introduction
The emerging ﬁeld of agricultural robotics has gained increasing interest in
recent years. The growing demand for high-quality food worldwide, combined
with inherently limited natural resources, has forced the agriculture industry to
modernize assets in order to increase overall eﬃciency. The task of collecting
fruit from trees is typically characterized by large quantities, but comparably
low levels of selectivity, and an enormous requirement for manual labor. As a
result, large quantities of fruit must be sold at low value or wasted, as it cannot
be harvested at the right time (being either too mature or not ripe) or cannot be
harvested at all due to inavailability of workers. Increasing the level of automa-
tion allows for continuous harvesting over wide time windows, and the intelligent
selection of fruit to be picked at just the right time with lower labor cost.
Automation of agricultural processes using robots requires high levels of
robustness compared to processes that are performed by humans or closely mon-
itored by humans. In uncontrolled environments, such as typically encountered
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_27

MVPS of Pome Fruit with RGB-D Camera
321
in outdoor fruit harvest scenarios, a robot needs to cope with high levels of
uncertainty in the perception of its environment, and the need for representa-
tions of uncertainty is twofold. On the one hand, for a robot to operate safely,
it should not be steered into regions with high uncertainty, so that accidents are
prevented. On the other hand, large scale automation of agricultural processes
can only be achieved with increased levels of autonomy, where a robot actively
explores its environment to reduce uncertainty.
This paper discusses several crucial components of an automized pome fruit
harvesting pipeline using an RGB-D camera. We present an experimental setup
where an Intel RealSense SR300 camera is mounted to a commercial robot arm,
and collects multiple scans of an artiﬁcial apple trellis in order to increase the
accuracy of perception. Accurate knowledge of the location and shape of the fruit
reduces potential damage to the fruit and surrounding parts of the plant at the
time of harvesting. Harvesting with robots is challenging because of the varying
light conditions of outdoor environment and the large amount of occlusions by
the leaves and branches.
Authors in [1] used a RGB-D sensor for the detection of apples. They use
single scans from the camera and process them to quantify the amount of apples
and their size. They ﬁrst apply a color ﬁlter to remove the non-apple points,
and then use Euclidean clustering for initial segmentation of the apples. Finally,
apples that are close to the camera are segmented using a RANSAC algorithm
with a model of a sphere.
Authors in [2] develop a classiﬁcation algorithm for detection of peduncles of
sweet peppers. They use a Support Vector Machine classiﬁer for 3D points in a
reconstructed scene fed with color features and point feature histograms.
In [3] the authors use simple color cameras for the detection and 2D segmen-
tation of tomatoes using a set of ﬁlters and an adaptive threshold algorithm.
They achieve good results for the segmentation of tomatoes, but their work is
limited to 2D perception, and further steps are required for automatic harvesting
of the tomatoes.
In [4] authors develop a multisensory system consisting of a multi-spectral
camera, a TOF camera, an RGB-D camera and an artiﬁcial illumination system.
They combine a set of ﬁlters for the input data combined with a pixel-wise
classiﬁcation algorithm. They achieve good results for diﬀerent scenarios and
fruits, but the cost of the entire system is large considering the need for mass
employment as required in pome fruit harvesting.
In this paper we use probabilistic modeling of the fruit surface based on
Gaussian Process Implicit Surfaces (GPIS) [5]. State-of-the-art GPIS algorithms
typically assume a constant mean level function, which contains no prior knowl-
edge about the shape of an object. This restriction is limiting the applicability
of GPIS, especially when the data points for the surface reconstruction are not
evenly distributed or large fractions of the objects have not been observed. In [6]
authors propose the use of a priori learned shapes of objects to integrate prior
knowledge in the GPIS process. This improves the surface reconstruction and

322
P. Ramon Soria et al.
can be used to improve segmentation of the object when signiﬁcant parts of the
scene have not been observed.
Here, we use a Dirichlet mixture of GPIS for probabilistic segmentation of
the scene into distinct pieces of fruit and non-fruit components. Dirichlet mix-
ture models are a systematic way of describing data association problems where
latent components, such as an unknown number of objects in a scene, need
to be inferred. The resulting probability space increases super-exponentially,
and Markov-Chain Monte Carlo methods (such as Gibbs-sampling) are typically
employed.
The remainder of the paper is organized as follows. Section 2 introduces the
probabilistic segmentation model, and Sect. 3 presents the procedure of the mul-
tiple view selection for reducing the uncertainty on the segmentation and improv-
ing the object segmentation. In Sect. 4, the hardware system used for testing the
algorithm is shown, and results are presented for a real apple vine. In Sect. 5,
the paper concludes with a discussion of the presented results and further steps
to be investigated.
2
Dirichlet Segmentation
Segmentation is a common preprocessing step to higher-level tasks such as
object detection, classiﬁcation, or manipulation. Probabilistic representations
of pointcloud segmentation are essential for active methods such as targeted
exploration [7], decision making in grasping contexts [8] or probabilistic object
detection [9]. We formulate segmentation as a data association problem, where
each point n is assigned a label an, associating it to one of a set of objects
present in the scene. With common pointcloud sizes, data association on a point
level is generally impractical. Instead, we perform probabilistic segmentation on
an oversegmented scene, as obtained using the voxel cloud segmentation algo-
rithm presented in [10], with source code distributed as part of the Point Cloud
Library (PCL) [11].
The surfaces of fruit are modeled by Gaussian process implicit surfaces [5],
where observations of object surfaces are interpreted as points in the zero-level
set of an underlying GP. Let D = {xn, yn} for n = 1, · · · , N, be a set of observa-
tions yn of f(xn), characterized by the covariance function kN(xm, xn). Obser-
vations y∗of f ∗= f(x∗) at a query location x∗are distributed according to
y∗∼N(⟨y∗⟩, σ∗2),
(1)
with mean and variance
⟨y∗⟩= k∗T K−1yD,
(2)
σ∗2 = k∗∗−k∗T K−1k∗,
(3)
where yD ∈RN×1 denotes the concatenation of the observations for all data
points, and K ∈RN×N, k∗∈RN×1, k∗∗∈R denotes covariance terms. Further
information can be found in [5,6,12].

MVPS of Pome Fruit with RGB-D Camera
323
As we are interested in scenes consisting of multiple objects, we consider mix-
tures of GPs, where the association of data points to diﬀerent GPs is modelled
by a Dirichlet process (DP). DP mixture models have the advantage that they
readily deal with data sets where the number of latent clusters is not known.
Each GP is then characterised by a set of hyperparameters, which can be used
to represent the surface properties as well as prior shape and location [6].
Analytical representations of the resulting probability space are intractable,
even for reasonably small problems, as it consists of all possible combinations of
part associations. As a result, we employ a Markov-Chain Monte Carlo method
that sequentially explores the probability space by producing samples according
to a Gibbs-sampler. It is known that samples from a DP can be generated by
sampling from a Chinese Restaurant process (CRP): the prior distribution of a
data point’s association is conditioned on the association of all other data points
and is given by
p(an = k|a\n, α) =
Nk
N −1 + α
(4)
p(an = K + 1|a\n, α) =
α
N −1 + α,
(5)
where an denotes the association of point n, a\n denotes the association vector
for all other points, N denotes the total number of points (including point n),
Nk denotes the number of points associated to component K, and α denotes the
Dirichlet process concentration parameter. The second expression p(an = K +1)
represents the probability that point n is associated to a new object without
any points associated. The algorithm randomly selects a data point {xn, yn} ∈
DM and removes it from its current GP, before computing the likelihoods with
respect to all GPs currently present in the scene, i.e. with number of associated
data points Nk > 0. The GP likelihoods are weighted by Nk according to (4)
and (5) and used to compute posterior association probabilities p(an = k) by
normalisation. Our algorithm produces a desired number of samples for the
scene segmentation, which can be used to draw probabilistic conclusions about
the scene segmentation and the location and shape of objects.
3
Multiple View Segmentation
3.1
Pose Generation and Cloud Registration
The vine-like structure of apple trees on a trellis comprises leaves, branches and
fruit. The leaves can occlude the fruit and also induce noise in the observed
point clouds. For these reasons the input point clouds are ﬁrstly ﬁltered with a
standard noise removal. At this stage, the clouds have fragments of the apple but
morphologically these are identical to leaves since both are small planar surfaces.
In order to achieve better results in the probabilistic segmentation we per-
form registration of point clouds taken from multiple points of view. A pose
planner generates camera poses such that the scene is observed from diﬀerent

324
P. Ramon Soria et al.
Fig. 1. System block diagram.
angles. Figure 1 summarises the multiview probabilistic segmentation process. It
is assumed that the robot is positioned in front of the apple branches. First, a
snapshot is taken and used to infer the relative angle of the apple “vine” with
respect to the arm. Using this angle, we compute candidate poses reachable by
the arm arranged on a portion of a sphere. Then, at each step of the algorithm
a pose is selected that maximises the distance from the history of past poses
granting a good convergence of the mapping process. From each viewpoint a
new cloud is taken, which is registered using ICP plus information from the arm
controller that provides an initial estimate of the point cloud’s true pose.
Finally, the map of the scene is ﬁltered by color using Euclidean distance
between colors and a model value in HSV. Then, it is simpliﬁed using the voxel
cloud segmentation algorithm described in [10] before being introduced into the
Dirichlet segmentation process.
3.2
Arm Controller
Path Planning and Control. The arm path planning framework leverages
recent breakthroughs in trajectory optimization algorithms that perform very
well in high-dimensional conﬁguration space. The algorithms used in our system
include: TrajOpt [13], GPMP2 [14] and CHOMP [15]. In preliminary experiments
we found that none of the algorithms performed consistently better than any
other for all scenarios. CHOMP had the highest success rate, however, TrajOpt
had signiﬁcantly better computation time.
The algorithms can fail for several reasons, most often because they are
inherently prone to getting trapped in local minima and there is no guarantee
of ever ﬁnding a solution. Sampling based planners, such as RRT*, are another
Table 1. Average computation time and success rates per planner. Ranked planner
runs all three optimisers in parallel and, unless all planners fail, returns ﬁrst successful
solution.
Planner
Computation time (s) Success (%)
TrajOpt 0.09
94
GPMP2 0.09
68
Chomp
1.15
96
Ranked
0.088
98

MVPS of Pome Fruit with RGB-D Camera
325
Fig. 2. Arm hardware setup and simulation in OpenRAVE
eﬀective path planning method used in high-dimensional conﬁguration spaces.
A key property is that these planners are probabilistically complete, that is as
the planner continues running, the probability of not ﬁnding a solution (if one
exists) asymptotically approaches zero. Bi-directional RRT, a variant of RRT,
exhibits a high planning success rate at the cost of sub-optimal paths [16].
We performed comparison experiments with 100 random end-eﬀector poses
sampled over a sphere surface, centred on the middle of the trellis. As can be
seen from results in Table 1 the Ranked planner (which runs all optimisers in
parallel) performed the best, however still had a 2% failure rate. Therefore, the
Ranked planner is used in our system and supplemented by Bi-directional RRT,
which is used as a last resort if all others fail. The paths returned are geometric
and often sparse, hence they are ﬁrst time parameterised and up-sampled before
being sent to the arm for execution.
Arm Hardware. In order for a robot arm to achieve an arbitrary end eﬀector
position and orientation in 3D space, i.e. a 6D pose, six degrees of freedom
(DOF) are necessary [17]. Hence, manipulators suited for dexterous tasks, such
as active sensing, should have at least six DOF. Redundant manipulators have
seven or more DOF and oﬀer greater dexterity and ﬂexibility for maneuvering
around obstacles. Rethink Robotics’ Sawyer, a 7DOF robot arm, is used in our
system. An open source SDK for the Sawyer arm provides a convenient API for
executing trajectories and requesting state information.
To control the robot arm, its SDK provides a built-in Joint Trajectory Action
Server (JTAS) which facilitates commanding the arm through multiple way-
points [18]. JTAS takes as input a list of timestamped waypoints and then
determines appropriate joint velocity commands, through interpolation, to send
to the arm so that the given trajectory is followed.
Simulation Environment. The Sawyer arm and its environment are shown
in Fig. 2. The arm is simulated in OpenRAVE, an Open Robotics Automation
Virtual Environment for developing and testing motion planners [19]. Open-
RAVE can import standard robot models, such as COLLADA, allowing seamless

326
P. Ramon Soria et al.
integration with all of its interfaces, including motion planning libraries, inverse
kinematics solvers and collision checkers. The motion planners are implemented
as OpenRAVE planner plugins. Further, one of the strengths of OpenRAVE is
its ability to robustly run multiple environments simultaneously in the same
process. Hence, parallel path planning is well supported.
Function with Rest of the System. The Arm Controller provides a TCP
server front end which acts as the interface to the rest of the system. It listens on
a TCP socket for messages, containing serialised commands, and once received
deserialises and parses them for execution. The sequence of commands is as
follows. First the Cloud Registration sends a request to the Arm Controller for
the arm’s current pose. Then the Pose Planner computes a target pose based on
the current pose and map information, which is then sent to the Arm Controller
for planning and execution. The Arm Controller then lets the Cloud Registration
node know whether the execution succeeded or failed. This process is repeated
until no more viewpoints are needed.
4
Experimental Validation
In this section we discuss the experimental setup used for testing our system. We
use a 1.5 m by 1.5 m section of an artiﬁcial apple trellis with the same character-
istics as typical orchard trellises. The initial position of the arm with respect to
the trellis is unknown at the beginning of each experiment. The only assumption
we make is that the trellis is in the range of the camera (the Intel RealSense
SR300 has a maximum range of 1.5 m).
Figure 3 illustrates the map building result of an experiment using the regis-
tration procedure described in Sect. 3.
The ﬁrst row in Fig. 4 shows the input cloud that is fed into the Dirichlet
segmentation algorithm. In the second row the result of the segmentation process
is shown, where diﬀerent colours are used to show the association to the diﬀerent
objects.
Finally, the last row shows the centroids of the resulting objects. As the
sampler tends to create new candidates of groups with single parts at random
(a)
(b)
(c)
(d)
Fig. 3. Example of map building according to Sect. 3. The coordinate frames illustrate
the inferred poses from where the point clouds were taken, and each cloud shows the
reconstructed RGB-D pointcloud of the apple scene.

MVPS of Pome Fruit with RGB-D Camera
327
Fig. 4. Segmentation results for an increasing number of viewpoints from left to right.
The ﬁrst row shows the registered point clouds for multiple views, the second row
shows the labeled groups representing the segmented apples, and the last row displays
the centroids of apples that were segmented with a suﬃciently large number of parts
associated.
samples, a threshold has been set for a minimum number of parts for a group to
be considered an object. As the number of viewpoints increases, more parts are
added to the scene and an increasing number of apples are correctly located.
In order to validate the locations of the individual apples as identiﬁed by
our segmentation algorithm, the arm has been used to take a close up shot of
each fruit, representing a key step in a harvesting or fruit inspection pipeline.
Figure 5, shows a series of examples of these snapshots.
The
computation
time
of
the
Dirichlet
segmentation
scales
super-
exponentially with the number of parts in the scene. The supervoxelisation
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Fig. 5. Close-up view of the apples after complete segmentation process and validation
of their locations.

328
P. Ramon Soria et al.
Fig. 6. Eﬀect of the seed size and voxel resolution on the number of parts of the input
cloud for the Dirichlet segmentation and the eﬀect on the speed of the sampler. Note: the
legend describes the voxel resolution (number after ‘v’) and seed size (number after ‘s’).
(a)
(b)
(c)
(d)
Fig. 7. Segmentation results for diﬀerent distributions of the apples over the vine.
algorithm mentioned in Sect. 2 over-segments the input clouds into supervoxels
(which we refer to as parts). This algorithm depends on two input parameters
parameters: seed size and voxel resolution [10]. Figure 6(a) shows the eﬀect of
these parameters on the number of parts fed into the segmentation process for
the same dataset. Figure 6(b) shows the average time per sample in the Gibbs
sampler. Finer parameters (lower voxel resolution and seed size) produce more
parts, which increases the computation time.
Finally, Fig. 7 shows additional results of the probabilistic segmentation with
diﬀerent conﬁgurations and clusters of apples. All the apple vines were com-
pletely built and all the apples were successfully segmented using 1000 itera-
tions of the Gibbs Sampler. The centroids of the apples are distributed over the
samples within a range of 3 cm. This range depends on the parameters of the

MVPS of Pome Fruit with RGB-D Camera
329
sampler, and a speciﬁc centroid depends on the selected iteration step, as these
are updated sequentially to ensure convergence. The surfaces of the apples ﬁt
well to the data, demonstrating accurate reconstruction close to the observed
data points, as required for harvesting and manipulation.
5
Discussion
We have presented a system for probabilistic segmentation of pome fruit, includ-
ing hardware realisation consisting of an Intel RealSense SR300 camera mounted
on a commercial robotic arm, robust path planning algorithms, point cloud
acquisition and registration, preprocessing (color-based presegmentation and
cloud-based supervoxelisation) and MCMC-based proababilistic segmentation
and fruit detection. We have investigated the robustness of the path planning
and point cloud registration algorithms in hardware experiments and demon-
strated the feasibility of the probabilistic segmentation algorithm.
The results of the GPIS- and DP-based probabilistic segmentation algorithm
are promising in that they reliably detect apples and their exact location in the
Fig. 8. Results of the probabilistic segmentation. The bottom right ﬁgure shows the
association entropy map, where blue indicates low entropy and red indicates high
entropy.

330
P. Ramon Soria et al.
point cloud. Further, the probabilistic surface reconstruction using GPIS can be
used for robust grasping algorithms [20] that take into account uncertainty in
the fruit surface. Furthermore, probabilistic representations of the environment
are essential for active methods to improve perception.
Figure 8 shows the pointcloud and segmentation results at an intermediate
step of 5 registered scans. As was observed before, apples are correctly located
and segmented. Furthermore, the bottom right subplot shows the association
entropy map, where blue indicates low entropy and red indicates high entropy.
Those parts of the scene with high association entropy indicate that parts have
been less consistently associated to the same objects compared to regions of lower
entropy. Fruit that are densely cluttered (like in the center of the scene) generally
result in larger segmentation uncertainty, whereas the single apple at the top
right is conﬁdently identiﬁed as one object. Such results can immediately be
employed for grasping (start by picking the apple that is conﬁdently segmented)
and active perception (acquire more data in cluttered regions of the scene).
In future work we intend to increase the robustness of the segmentation
algorithm. We have already employed a noise model that rejects parts that do
not belong to objects, and in a more involved approach planar prior shapes
can be used to model the shape of leaves. Our general goal is to close the loop
around perception and provide an end-to-end system that makes observations
with a low cost camera, registers point clouds, chooses viewpoints based on
information theoretic measures and robustly moves the arm accordingly.
Acknowledgments. This work has been carried out in the framework of the
AEROARMS
(SI-1439/2015)
EU-funded
projects
and
the
Australian
Research
Council’s Discovery Projects funding scheme (DP140104203).
References
1. Nguyen, T.T., Vandevoorde, K., Kayacan, E., De Baerdemaeker, J., Saeys, W.:
Apple detection algorithm for robotic harvesting using an RGB-D camera. In:
Proceedings of International Conference of Agricultural Engineering, Zurich,
Switzerland (2014)
2. Sa, I., Lehnert, C., English, A., McCool, C., Dayoub, F., Upcroft, B., Perez, T.:
Peduncle detection of sweet pepper for autonomous crop harvesting combined color
and 3-D information. IEEE Robot. Autom. Lett. 2(2), 765–772 (2017)
3. Zhao, Y., Gong, L., Huang, Y., Liu, C.: Robust tomato recognition for robotic
harvesting using feature images fusion. Sensors 16(2), 173 (2016)
4. Fern´andez, R., Salinas, C., Montes, H., Sarria, J.: Multisensory system for fruit
harvesting robots: experimental testing in natural scenarios and with diﬀerent
kinds of crops. Sensors 14(12), 23885–23904 (2014)
5. Williams, O., Fitzgibbon, A.: Gaussian process implicit surfaces. In: Gaussian
Processes in Practice (2006)
6. Martens, W., Poﬀet, Y., Soria, P.R., Fitch, R., Sukkarieh, S.: Geometric priors
for Gaussian process implicit surfaces. IEEE Robot. Autom. Lett. 2(2), 373–380
(2017)

MVPS of Pome Fruit with RGB-D Camera
331
7. van Hoof, H., Kroemer, O., Peters, J.: Probabilistic segmentation and targeted
exploration of objects in cluttered environments. IEEE Trans. Robot. 30(5), 1198–
1209 (2014)
8. Pajarinen, J., Kyrki, V.: Decision making under uncertain segmentations. In: Pro-
ceedings of IEEE ICRA, pp. 1303–1309 (2015)
9. Cadena, C., Koseck´a, J.: Semantic parsing for priming object detection in indoors
RGB-D scenes. Int. J. Robot. Res. 34(4–5), 582–597 (2015)
10. Papon, J., Abramov, A., Schoeler, M., W¨org¨otter, F.: Voxel cloud connectivity
segmentation - supervoxels for point clouds. In: Proceedings of CVPR, pp. 2027–
2034 (2013)
11. Rusu, R.B., Cousins, S.: 3D is here: Point Cloud Library (PCL). In: Proceedings
of IEEE ICRA, pp. 1–4 (2011)
12. Rasmussen, C.E.: Gaussian processes for machine learning (2006)
13. Schulman, J., Ho, J., Lee, A., Awwal, I., Bradlow, H., Abbeel, P.: Finding locally
optimal, collision-free trajectories with sequential convex optimization. Robot. Sci.
Syst. 9, 1–10 (2013)
14. Dong, J., Mukadam, M., Dellaert, F., Boots, B.: Motion planning as probabilistic
inference using Gaussian processes and factor graphs. In: Robotics: Science and
Systems, vol. 12 (2016)
15. Zucker, M., Ratliﬀ, N., Dragan, A.D., Pivtoraiko, M., Klingensmith, M., Dellin,
C.M., Bagnell, J.A., Srinivasa, S.S.: Chomp: covariant hamiltonian optimization
for motion planning. Int. J. Robot. Res. 32(9–10), 1164–1193 (2013)
16. Kuﬀner, J., Lavalle, S.: RRT-connect: an eﬃcient approach to single-query path
planning. In: Proceedings of IEEE ICRA, pp. 995–1001 (2000)
17. Hayashi, A.: Geometrical motion planning for highly redundant manipulators using
a continuous model. Ph.D. thesis, Austin, TX, USA (1991)
18. InternaSDK. http://sdk.rethinkrobotics.com/intera/arm control systems
19. Diankov, R., Kuﬀner, J.: Introduction to the openrave architecture 0.9.0. http://
openrave.org/docs/latest stable/coreapihtml/architecture concepts.html
20. Dragiev, S., Toussaint, M., Gienger, M.: Gaussian process implicit surfaces for
shape estimation and grasping. In: Proceedings of IEEE ICRA, pp. 2845–2850
(2011)

Vision-Based Deﬂection Estimation
in an Anthropomorphic, Compliant and Lightweight
Dual Arm
Alejandro Suarez
(✉), Guillermo Heredia, and Anibal Ollero
Robotics, Vision and Control Group, University of Seville,
Camino de los Descubrimientos s/n, 41092 Seville, Spain
{asuarezfm,guiller,aollero}@us.es
Abstract. This paper proposes the application of a stereo vision system for esti‐
mating and controlling the Cartesian and joint deﬂection in an anthropomorphic,
compliant and ultra-lightweight dual arm designed for aerial manipulation. Each
arm provides four degrees of freedom (DOF) for end-eﬀector positioning in a
human-like kinematic conﬁguration. A simple and compact spring-lever mecha‐
nism introduced in all joints provides mechanical compliance to the arms. A color
marker attached at the end eﬀector of the arms is visually tracked by a stereo pair
installed over the shoulders. The Cartesian position and velocity of the markers
is estimated with an Extended Kalman Filter (EKF), while the corresponding
points in an equivalent stiﬀ-joint manipulator are obtained from the kinematic
model and the position of the servos. The Cartesian deﬂection is deﬁned as the
diﬀerence between these two measurements, obtaining the joint deﬂection from
the inverse kinematics. The vision-based deﬂection estimator is validated in test
bench experiments: position estimation accuracy, impact response, passive/active
compliance and contact force control.
Keywords: Compliance · Visual deﬂection estimation · Aerial manipulation
1
Introduction
Mechanical compliance is a highly desirable feature in an aerial manipulation robot
interacting with the environment, where contact forces and motion constraints generated
during certain operations may destabilize the aerial platform, and where impacts and
overloads may damage the manipulator [1, 2]. Imagine for example a situation in which
a robotic arm installed at the base of a multi-rotor vehicle is grabbed to a ﬁxed point on
ﬂight. Any small displacement of the aerial platform with respect to the operation point
will introduce an external torque that, if propagated through a stiﬀ manipulator to the
base of the aerial platform, may destabilize the attitude controller. Thus, joint compli‐
ance increases safety in the manipulation operation, as it provides a certain level of
tolerance against external forces and overloads, making even possible to estimate the
contact force/joint torque from the deﬂection of the elastic element. However, this is
done at expenses of decreasing the positioning accuracy of the end eﬀector due to the
inﬂuence of inertia and gravity over the compliant joints.
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_28

Aerial manipulation is a very recent research ﬁeld, which proposes the integration
of one or multiple [3] robotic arms in vertical take-oﬀ and landing (VTOL) unmanned
aerial vehicles (UAV), either helicopter [4] or multirotor [5]. The development of this
technology is motivated by the necessity in several industrial scenarios to reach areas
and workspaces out of the range for humans to be able to perform certain inspection and
maintenance tasks. Some typical examples include detection and insulation of leaks in
high altitude pipes in chemical plants, visual inspection and repair of cracks in blades
of wind turbines, contact-based measurement of beam deﬂection in bridges, or the
installation and retrieval of sensor devices in buildings. Several prototypes of aerial
manipulators have been developed and applied to grasping and transportation tasks. A
7-DOF industrial manipulator is integrated in a high payload autonomous helicopter in
[4]. Two compliant grippers at the base of another helicopter are used for object grasping
in [6]. Quadrotor platforms have been widely employed in diﬀerent tasks in indoors [7,
8] and outdoors [9] environments. Visual markers (tags) [10, 11] or visual features of
the target object [12] have been applied to visual servoing in the context of aerial
manipulation.
Dual arm manipulation has been also addressed, although only in a few works. The
valve turning operation described in [13] employs two simple 2-DOF arms and the
torque in the yaw angle generated by the propellers for this task. A human-size dual arm
manipulator is described in [14] and tested in outdoor ﬂights [15]. A commercially
available manipulator is shown in [16]. The beneﬁts of mechanical joint compliance in
robotic arms intended to aerial manipulation have been evidenced in our previous work.
Reference [1] describes a method for estimating the weight of grasped objects from the
deﬂection of extension springs acting as tendons in the elbow joint, as well as the possi‐
bility of detecting collisions with low energy transfers. The 3-DOF compliant joint arm
developed in [2] is applied to contact force control and to obstacle localization. However,
the integration of a deﬂection potentiometer in the joints is a hard task from the mecha‐
tronics point of view. This has motivated the evaluation of vision sensors (cameras) for
measuring the deﬂection of the end eﬀector, inspired by the work done with ﬂexible link
manipulators intended to space robotics [17, 18].
The main contribution of this paper is the development and experimental validation
of a vision system used for estimating and controlling the mechanical deﬂection of a
compliant joint dual arm designed for aerial manipulation with multirotor platform. A
stereo camera installed over the shoulder structure is focused on the color markers
attached to the end eﬀector of the arms. A modiﬁed version of the CAMShift (Contin‐
uously Adaptive Mean-Shift) algorithm [19, 20] provides the tracking points to an
Extended Kalman Filter (EKF), which estimates the 3D position and velocity of the
markers. The Cartesian deﬂection is then computed from this estimation and from the
forward kinematic model of the manipulator. Experimental results in testbench have
been conducted, demonstrating its application to impact detection and force control.
The rest of the paper is organized as follows. Section 2 describes the anthropomor‐
phic, compliant and lightweight dual arm designed for its integration in multirotor plat‐
form. The kinematics and dynamics of the manipulator are presented in Sect. 3 along
with the model of the vision system. Section 4 details the vision-based deﬂection esti‐
mator, presenting the results in Sect. 5 and the conclusions in Sect. 6.
Vision-Based Deﬂection Estimation
333

2
System Description
The range of operations that an aerial manipulation robot is able to perform will depend
on the level of dexterity of the robotic arms integrated in the aerial platform. Addition‐
ally, certain inspection and maintenance tasks typically conducted by human operators
imply bimanual manipulation. Therefore, it is desirable that the manipulator is able to
reproduce the kinematics of the human arms, as well as its features in terms of compli‐
ance, low weight and inertia, and robustness. The anthropomorphic, compliant and
lightweight dual arm system developed from the scratch at the Robotics, Vision and
Control Group of the University of Seville has been designed taking into account these
considerations. A picture of the prototype can be seen in Fig. 1. Each arm counts with
three joints in the shoulder and one joint in the elbow disposed in a human-like kinematic
conﬁguration. Also the length of the upper arm and the forearm links (25 cm), and the
separation between the arms (32 cm) are similar to the human arm. The actuators
employed are the Herkulex servos manufactured by Dongbu Robot, models DRS-0201
and DRS-0101. The aluminum frame structure has been carefully designed in such a
way that it isolates the servo actuators from impacts and overloads, and at the same time
allows the integration of a compliant spring-lever transmission mechanism in all the
joints. The end eﬀector is a simple gripper built with a Futaba S3003 servo and a micro
switch for contact-based grasping. A ZED stereo camera attached over the shoulder
structure and rotated 45 degrees downwards in the pitch angle allows the teleoperation
of the arms with visual feedback and the application of visual servoing algorithms for
automatic tasks.
Fig. 1. Anthropomorphic, compliant and lightweight dual arm.
3
Modeling
3.1
Kinematic Model
The reference frames, joint variables, lengths and vectors associated to the kinematic
model of the dual arm are represented in Fig. 2. Here XiYiZi is the coordinated system
of the i-th manipulator and XcYcZc is the camera frame to which the marker positions
334
A. Suarez et al.

are measured. Superscript i = {1, 2} denotes the arm (left/right), while subscript
j = {1, 2, 3, 4} indicates the joint in the following order: shoulder pitch, roll, and yaw,
and elbow pitch. The rotation angle of each link with respect to the previous link is
denoted by qi
j, while 𝜃i
j is the angular position of the corresponding servo. The joint
deﬂection Δ𝜃i
j is then deﬁned as the diﬀerence between these two angles:
Δ𝜃i
j = qi
j −𝜃i
j
(1)
Fig. 2. Kinematic model of the dual arm manipulator with stereo camera head.
The deﬂection of the joints is caused by the compression of the springs in the
compliant transmission mechanism [2], with a maximum amplitude around ± 20
degrees.
Let Fi:ℜ𝟒→ℜ𝟑 represent the forward kinematics of the i-th arm, that is, the function
that maps the joint space into the Cartesian space of the arm. Analogously, the inverse
kinematic model is deﬁned by function 𝜴i:ℜ𝟑→ℜ𝟒, which returns the joint variables
for a given Cartesian position. Note that the inverse kinematics has an inﬁnite number
of solutions due to the redundant degree of freedom of the manipulator. This work
assumes that the shoulder roll angle is a known parameter for simplicity, so qi
2 = 𝜑i. The
forward and inverse kinematics are then deﬁned as follows:
ri(qi) =
⎡
⎢
⎢
⎢⎣
xi
yi
zi
1
⎤
⎥
⎥
⎥⎦
= Fi(qi);
qi(ri) =
⎡
⎢
⎢
⎢⎣
qi
1
qi
2
qi
3
qi
4
⎤
⎥
⎥
⎥⎦
= 𝜴i(ri, 𝜑i)
(2)
Here ri is an arbitrary point in the Cartesian space of the i-th arm associated to the
vector of joint variables qi. Let us particularize the kinematic model to the red marker
attached to the wrist point of both arms shown in Fig. 2. The vision system provides the
3D position of each marker referred to the camera frame, that is, the vector rc
M,i. The
position and orientation of the camera with respect to each arm is known, and so the
Vision-Based Deﬂection Estimation
335

corresponding transformation matrix i
cT ∈ℜ𝟒×𝟒. Then, the position of the marker can
be referred to the coordinate system of each arm in the following way:
ri
M,i = i
cT ⋅rc
M,i
(3)
Imagine an equivalent stiﬀ joint manipulator, in which the compliant joints become
stiﬀ. The joint deﬂection is zero, and thus, the angular position of the output links is the
servo position vector 𝜽i. The Cartesian deﬂection Δlp ∈ℜ𝟑 evaluated at a certain point
p of the manipulator is deﬁned as the diﬀerence between the positions of this point in
the compliant joint manipulator w.r.t. the equivalent stiﬀ joint manipulator:
Δli
p = Fi
p
(qi) −Fi
p
(𝜽i)
(4)
The information available for estimating the deﬂection in the dual arm system is the
angular position of the servo actuators, represented by vector 𝜽i, and the Cartesian posi‐
tion of the markers attached at the end eﬀector provided by the vision system, rc
M,i.
Therefore, the deﬂection of the manipulator at this point is computed as follows:
Δli
M,i = i
cT ⋅rc
M,i −Fi
M,i
(𝜽i)
(5)
The interpretation of the right side in Eq. (5) is the following: the ﬁrst term represents
the position where the marker really is (measured by the camera), while the second one
corresponds to the position where the marker would be if the joints were stiﬀ (given the
position of the servos). The Cartesian deﬂection may be useful for estimating and
controlling directly the contact force exerted by the end eﬀector, or for estimating the
weight of a grasped object. The joint deﬂection can be estimated in a similar way,
applying the inverse kinematic model to the marker point given by the vision system
and subtracting the angular position of the servos:
Δ𝜽i = 𝜴i(
ri
M,i, 𝜑i)
−𝜽i
(6)
The joint torque can be finally derived from the static torque-deflection characteristic.
3.2
Dynamic Model
The most evident source of deﬂection in the developed compliant joint manipulator is
gravity. Take as reference that the deﬂection at the end eﬀector when the arm is fully
stretched is around 8 cm. The own weight of the upper arm and forearm links causes the
compression of the springs of the transmission mechanism. However, the compliant
joints will also suﬀer the eﬀect of inertia, Coriolis and centrifugal forces generated by
the motion of the links. Although the inﬂuence of these terms in practice is almost
negligible for normal operation velocities, it is necessary to take into account the
complete dynamics in the design of the control system.
As described in [21, 2], the dynamic model of a compliant joint manipulator can be
decomposed in two parts: the dynamics of the actuators, reduced to the inertia and
336
A. Suarez et al.

friction of the rotor and the torque transmitted to the output links through the transmis‐
sion mechanism, and the output link dynamics, which comprises the inertia, centrifugal
and Coriolis, and the gravity term along with the external forces and the torque received
from the transmission:
Bi ⋅̈𝜽
i + 𝝉i = 𝝉i
m −𝝉i
f
(7)
Mi(qi) ⋅̈qi + Ci(qi, ̇qi) + Gi(qi) = 𝝉i + 𝝉i
ext
(8)
Here Bi and Mi ∈ℜ𝟒×𝟒 are the generalized mass matrices of the rotors and the links,
respectively, Ci ∈ℜ𝟒 is the Coriolis and centrifugal term, and Gi ∈ℜ𝟒 is the gravity
torque. Vectors 𝝉i
m and 𝝉i
f ∈ℜ𝟒 model the torque and friction generated by the motors,
while 𝝉i
ext represents the torque due to exogenous forces, typically contact forces. It is
easily to observe in previous equations that the term 𝝉i ∈ℜ𝟒 is the torque transmitted
by the spring-lever mechanism, which is characterized by the stiﬀness and dampening
constants.
𝝉i = Ki ⋅(qi −𝜽i) + Di ⋅
(
̇qi −̇𝜽
i)
(9)
It is convenient to remark that the possibility to measure the deﬂection of the
compliant joints facilitates the development of applications based on force/torque
control.
3.3
Camera Model
The Extended Kalman Filter (EKF) implemented for estimating the 3D position and
velocity of the color markers makes use of the pin-hole camera model for computing
the expected projection of the markers into the image plane of the cameras. Each of the
cameras in the stereo pair will be identiﬁed by index k = {1, 2}. The intrinsic parameters,
obtained from calibration, are the focal length, 
(
f k
x , f k
y
)
, and the radial and tangential
distortion coeﬃcients. If rc
M,i = [xc
M,i, yc
M,i, zc
M,i
]T represents the position of the marker of
the i-th arm in the camera frame (see Fig. 2), then the projection of its centroid over the
image plane of the j-th camera is given by:
ck
M,i =
[ ck
x,i
ck
y,i
]
=
⎡
⎢
⎢⎣
f k
x ⋅
xc
M,i ± b
2
zc
M,i
f j
y ⋅
yc
M,i
zc
M,i
⎤
⎥
⎥⎦
T
(10)
were b = 12[cm] is the base line of the stereo pair. The distortion model described in [22]
is applied over the projected centroid for taking into account the deviation produced by
the wide-angle lenses of the ZED camera.
Vision-Based Deﬂection Estimation
337

4
Vision-Based Deﬂection Estimation
4.1
Structure of the Estimator
The block diagram of the developed deﬂection estimator is depicted in Fig. 3, and it is
based on the kinematic model described in Sect. 3.1. The estimator takes as input the
angular position of the servo actuators, 𝜽i, along with the centroid of the i-th marker
projected over the k-th camera given by the tracking algorithm, giving as output the
estimated Cartesian and joint deﬂection, Δli
M,i and Δ𝜽i, respectively. The EKF integrates
the measurements provided by the tracking algorithm, obtaining the XYZ position and
velocity of the markers, ri
M,i and vi
M,i. The Cartesian deﬂection is computed subtracting
the position of the marker in the equivalent stiﬀ-joint manipulator to the position given
by the EKF. The joint deﬂection is computed in a similar way, applying the inverse
kinematic model.
Fig. 3. Structure of the deﬂection estimation system.
4.2
Vision System
The vision system consists of a ZED stereo camera focused on the color markers attached
at the wrist point of both arms, and a program developed in C ++ that implements a
modiﬁed version of the CAMShift which allows the tracking loss detection and object
re-detection using color and geometric information. This modiﬁed version, called
Tracking-Searching CAMShift, was developed previously by the authors in the context
of a multi-UAV system that made use of color markers disposed over quadrotors for
detecting, identifying and recovering against positioning sensor faults [20]. The
CAMShift algorithm is a well suited solution for tracking color objects due to its
simplicity, low computational requirements in time and memory, robustness to blurring
and to changes in the illumination conditions. However, its main drawback is the neces‐
sity of a marker whose color is in high contrast with respect to the background. The
shape of the marker should be spherical so its projection of the image plane of the stereo
pair is independent from the point of view or pose of the arms.
A calibration process is carried out in an oﬄine phase for determining the focal
length, principal point as well as the distortion coeﬃcients of the ZED camera. The hue,
saturation and value (HSV) rejection thresholds used for isolating the markers on the
338
A. Suarez et al.

back-projection image were tuned experimentally from a picture of the markers at the
nominal observation distance.
4.3
Extended Kalman Filter
Although the position of the markers can be estimated geometrically from the pair of
projection centroids given by the CAMShift algorithm, the Kalman ﬁlter is preferred as
it also provides an estimation of the velocity of the marker, which can be exploited for
estimating the rate of energy exchange during the impact of the manipulator. The nonli‐
nearity associated to the pin-hole camera model requires the application of the extended
version of this algorithm. A simple constant velocity model is assumed for describing
the motion of the marker, so the state vector is deﬁned as follows:
x = [x
y
z
vx
vy
vz
]T
(11)
The measurement vector taken as input by the EKF is the centroid of the marker
projected on each camera, ck
M,i, along with the elapsed time since last update, Δt. The
accuracy in the position/velocity estimations has been tuned through the process noise
and measurement noise covariance matrices Q ∈ℜ6×6 and R ∈ℜ2×2, respectively.
Q =
[
diag3×3
(
𝜎2
p
)
𝟎3×3
𝟎3×3
diag3×3
(
𝜎2
v
)
]
;
R =
[
𝜎2
c 0
0 𝜎2
c
]
(12)
where 𝜎2
p = 2.5[m], 𝜎2
v = 100
[
m∕s
] and 𝜎2
c = 25
[
pixels
].
5
Experimental Results
5.1
Evaluation of the Performance of the Vision Module
The purpose of this experiment is to evaluate the time performance and the accuracy of
the developed vision system described in Sect. 4, considering for simplicity a single
marker. All the experiments have been conducted over a Sony VAIO laptop with an
Intel® CoreTM i7-3632QM CPU @ 2.20 GHz x 8 processor and Ubuntu 14.04 LTS. The
resolution of the ZED camera was set to 2560 × 720 pixels, at a frame rate of 30 fps.
The execution of the vision module comprises the image acquisition using the Video‐
Capture class provided by the OpenCV library, the application of the modiﬁed
CAMShift algorithm for each frame (left and right) along with the EKF update, and the
visualization of the images on the screen. The execution times measured in a 50 s time
interval are summarized in Table 1.
Vision-Based Deﬂection Estimation
339

Table 1. Execution times of the visual tracing algorithm.
Mean [ms]
Standard deviation [ms]
Acquisition time
7,76
2,71
Processing time
16,66
2,92
Iteration time
33,32
5,06
The accuracy in the position estimation of the marker provided by the vision system
was measured using a drylin® T rail guide system TS-04-15 and an igus® TW-04-15
sliding carriage. The marker can be linearly displaced along the Z axis of the camera
(depth), obtaining the ground truth from a ruler disposed parallel to the rail guide.
Figure 4 represents the position estimation error when the marker is displaced from 0.15
meters (minimum observable distance) up to 0.5 meters (maximum reach of the arms).
As it can be seen on the right side of Fig. 4, the error is nonlinear with the distance. This
is mainly due to errors in the calibration of the camera and in the projection of the
centroid of the marker into the image plane. In order to improve the accuracy of the
deﬂection controller, a calibration process is executed for obtaining the deﬂection oﬀset
around the normal operation position of the arms.
Fig. 4. Position estimation error along the X, Y and Z (depth) axes.
5.2
Impact Response: Cartesian Deﬂection and Velocity
In this experiment, the color marker is attached at the wrist point of the left arm (see
Fig. 2), so the Cartesian deﬂection and speed at this point are obtained applying the
estimation method described in Sects. 3.1 and 4.1. The impact is generated throwing the
TW-04-15 sliding carriage (62 grams weight) along the rail guide from a height of 0.5
meters, hitting the servo of the gripper. The left arm is in L-position, with the elbow
joint ﬂexed 90 degrees. The deﬂection and speed of the marker caused by the impact
are represented in Fig. 5. Note that the underdamped response is proper of a mass-spring-
damper system: the potential energy of the impact is transformed in elastic potential
energy and released at a lower rate.
340
A. Suarez et al.

0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-0.04
-0.02
0
0.02
0.04
IMPACT RESPONSE - CARTESIAN DEFLECTION AND VELOCITY
deflection [m]
 
 
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-0.2
-0.1
0
0.1
0.2
time
deflection velocity [m/s]
X
Y
Z
Fig. 5. Cartesian deﬂection (up) and velocity (down) at the marker position due to impact.
5.3
Passive Compliance Vs Active Compliance: Zero Deﬂection Control
This experiment illustrates the concepts of passive and active compliance, using the
developed vision system for this purpose. Let us consider that a pushing-pulling force
is applied over the end eﬀector of the left arm, causing the compression of the springs
employed in the compliant joints. At the moment when the force ceases, the excess of
elastic potential energy stored in the springs will be transformed into kinetic energy, so
the manipulator will reconﬁgure itself to reach the state of minimum energy. This passive
behavior can be identiﬁed in the left side of Fig. 6, where the end eﬀector of the left arm
was pushed by hand sequentially along the X, Y and Z axes. Note that the displacement
of the end eﬀector w.r.t. its initial position is the Cartesian deﬂection. Now, imagine that
the position of the marker is controlled for maintaining a zero Cartesian deﬂection
reference, what can be achieved solving the inverse kinematics of the manipulator (see
Sect. 3.1). When the arm is pushed, the deviation in the position of the marker measured
by the vision system is feedback to the controller, which adjust the position of the joint
servos according to the direction of the deﬂection. This operation mode is called active
compliance for indicating that the actuators actively contribute to release the excess of
elastic potential energy stored in the springs, as it can be seen in the right side of Fig. 6.
Fig. 6. Passive deﬂection of the left arm (left) and zero deﬂection control (right).
Vision-Based Deﬂection Estimation
341

When the end eﬀector is pushed, the apparent eﬀect is that the arm is guided without
resistance. A video of the experiments can be found in [23]. The zero deﬂection control
mode may be useful for reducing the inﬂuence of contact forces generated in grabbing
situations over the aerial platform.
5.4
Deﬂection (Force) Control
One of the main motivations for the development of a vision-based deﬂection estimation
system is the possibility of estimating and controlling the contact forces that the aerial
manipulator applies during the execution of certain operations or tasks, for example in
the installation of sensors, the assembly of structures or the manipulation of tools.
Although the analysis of the force-deﬂection characteristic is out of the scope of this
work, a proportional relationship in the form F = K ⋅Δl can be considered, where the
diagonal constant matrix K ∈ℜ𝟑×𝟑 will in general depend on the joint angles. The
control of the Cartesian deﬂection is evaluated here for simplicity [23]. Figure 7 shows
the reference and measured value in the XYZ axes (left), and the position of the marker
along with the joint position of the servos (right). The position of the arm is the same as
stated above (90 deg ﬂexion of the elbow joint). A simple proportional controller gener‐
ates an incremental position reference for the end eﬀector integrating the Cartesian error.
As seen on the left side of Fig. 7, the control error tends to zero.
Fig. 7. Cartesian deﬂection-reference (left) and marker/joints position (right).
6
Conclusion
This paper has demonstrated the application of a stereo vision system for estimating and
controlling the deﬂection of a lightweight and compliant dual arm designed for aerial
manipulation with multirotors. A small color marker attached at the end eﬀector is visu‐
ally tracked by the camera head, obtaining its position and velocity by means of an EKF.
The Cartesian deﬂection is deﬁned as the diﬀerence between the real position of the
marker given by the EKF, and the position in the equivalent stiﬀ joint manipulator. The
proposed method avoids the integration of a deﬂection sensor in the joints, facilitating
342
A. Suarez et al.

the development of tasks requiring contact force control. As future work, it would be
convenient to improve the accuracy of the position estimator.
Acknowledgement. This work has been funded by the Spanish MINECO Retos project
AEROMAIN (DPI2014-5983-C2-1-R) and by the H2020 AEROARMS Project, Grant
Agreement Nº 644271. The research activity of Alejandro Suarez is supported by the Spanish
Ministerio de Educacion, Cultura y Deporte FPU Program.
References
1. Suarez, A., Heredia, G., Ollero, A.: Lightweight compliant arm for aerial manipulation. In:
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015, pp.
1627–1632. IEEE, September 2015
2. Suarez, A., Heredia, G., Ollero, A.: Lightweight compliant arm with compliant ﬁnger for
aerial manipulation and inspection. In: IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2016, pp. 4449–4454. IEEE, October 2016
3. AEROARMS Project http://aeroarms-project.eu/
4. Kondak, K., Huber, F., Schwarzbach, M., Laiacker, M., Sommer, D., Bejar, M., Ollero, A.:
Aerial manipulation robot composed of an autonomous helicopter and a 7 degrees of freedom
industrial manipulator. In: IEEE International Conference on Robotics and Automation
(ICRA), 2014, pp. 2107–2112. IEEE, May 2014
5. Jimenez-Cano, A.E., Martin, J., Heredia, G., Ollero, A., Cano, R.: Control of an aerial robot
with multi-link arm for assembly tasks. In: IEEE International Conference on Robotics and
Automation (ICRA), 2013, pp. 4916–4921. IEEE, May 2013
6. Pounds, P.E., Bersak, D.R., Dollar, A.M.: The yale aerial manipulator: grasping in ﬂight. In:
IEEE International Conference on Robotics and Automation (ICRA), 2011, pp. 2974–2975.
IEEE, May 2011
7. Mellinger, D., Lindsey, Q., Shomin, M., Kumar, V.: Design, modeling, estimation and control
for aerial grasping and manipulation. In: IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2011, pp. 2668–2673. IEEE, September 2011
8. Cataldi, E., Muscio, G., Trujillo, M.A., Rodríguez, Y., Pierri, F., Antonelli, G., Ollero, A.:
Impedance Control of an aerial-manipulator: preliminary results. In: IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), 2016, pp. 3848–3853. IEEE, October
2016
9. Heredia, G., Jimenez-Cano, A.E., Sanchez, I., Llorente, D., Vega, V., Ollero, A.: Control of
a multirotor outdoor aerial manipulator. In: IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS 2014), 2014, pp. 3417–3422. IEEE, September 2014
10. Lippiello, V., Cacace, J., Santamaria-Navarro, A., Andrade-Cetto, J., Trujillo, M.A., Esteves,
Y.R., Viguria, A.: Hybrid visual servoing with hierarchical task composition for aerial
manipulation. IEEE Robot. Autom. Lett. 1(1), 259–266 (2016)
11. Santamaria-Navarro, A., Grosch, P., Lippiello, V., Sola, J., Andrade-Cetto, J.: Uncalibrated
visual servo for unmanned aerial manipulation. IEEE/ASME Trans. Mechatron. (2017)
12. Ramon Soria, P., Arrue, B.C., Ollero, A.: Detection, location and grasping objects using a
stereo sensor on UAV in outdoor environments. Sensors 17, 103 (2017)
13. Korpela, C., Orsag, M., Oh, P. Towards valve turning using a dual-arm aerial manipulator.
In: International Conference on Intelligent Robots and Systems (IROS 2014), pp. 3411–3416.
IEEE/RSJ (2014)
Vision-Based Deﬂection Estimation
343

14. Suarez, A., Jimenez-Cano, A.E., Vega, V., Heredia, G., Rodriguez-Castaño, A., Ollero, A.:
Lightweight and human-size dual arm aerial manipulator. In: International Conference on
Unmanned Aircraft Systems (ICUAS) (2017)
15. https://www.youtube.com/watch?v=4sjpmDOSZms
16. https://www.prodrone.jp/en/archives/1420/
17. Jiang, X., Konno, A., Uchiyama, M.: A vision-based endpoint trajectory and vibration control
for ﬂexible manipulators. In: IEEE International Conference on Robotics and Automation,
2007, pp. 3427–3432. IEEE, April 2007
18. Xu, Y., Ritz, E.: Vision based ﬂexible beam tip point control. IEEE Trans. Control Syst.
Technol. 17(5), 1220–1227 (2009)
19. Bradski, G.R.: Computer vision face tracking for use in a perceptual user interface (1998)
20. Suarez, A., Heredia, G., Ollero, A.: Cooperative sensor fault recovery in multi-UAV systems.
In: IEEE International Conference on Robotics and Automation (ICRA), 2016, pp. 1188–
1193. IEEE, May 2016
21. Albu-Schäﬀer, A., Ott, C., Herzinger, G.: A uniﬁed passivity-based control framework for
position, torque and impedance control of ﬂexible joint robots. Int. J. Robot. Res. 26(1), 23–
39 (2007)
22. OpenCV camera calibration. http://docs.opencv.org/2.4/doc/tutorials/calib3d/camera_calibration/
camera_calibration.html
23. Video of the experiments. https://youtu.be/yLR0DelSlXU
344
A. Suarez et al.

3D Navigation for a Mobile Robot
Jan ˇSkoda(B) and Roman Bart´ak
Faculty of Mathematics and Physics, Charles University, Prague, Czech Republic
skoda@jskoda.cz, bartak@ktiml.mff.cuni.cz
Abstract. We propose a novel 3D navigation system for autonomous
vehicle path-planning. The system processes a point-cloud data from an
RGB-D camera and creates a 3D occupancy grid with adaptable cell size.
Occupied grid cells contain normal distribution characterizing the data
measured in the area of the cell. The normal distributions are then used
for cell classiﬁcation, traversability, and collision checking. The space
of traversable cells is used for path-planning. The ability to work in
three-dimensional space allows autonomous robots to operate in highly
structured environments with multiple levels, uneven surfaces, and vari-
ous elevated and underground crossings. That is important for the usage
of robots in real-world scenarios such as in urban areas and for disaster
rescue missions.
Keywords: Robotics · Navigation · 3D · Path ﬁnding
1
Introduction
In robotic research, the problem of mobile robot navigation is among the most
important. It has been thoroughly studied, but mostly for a two-dimensional
space. Although there are many 3D mapping systems available for use, robust
and general 3D navigation systems, that are not constrainted only to some envi-
ronments, are still very rare. Mostly, such navigations work with a 2D map or
project the constructed 3D map into 2D for navigation.
The major issue of 3D navigation and the reason, why most navigation sys-
tems work in 2D, is the computational complexity of 3D sensor processing,
path-planning, and traversability estimation – the recognition of areas, where
the robot can drive. We therefore utilize several techniques to improve eﬃ-
ciency, especially octree [3] data structure for map representation and 3D normal
distribution transform [9] (3D-NDT) for data representation and traversability
estimation.
In this paper, we introduce a general 3D navigation system for ground-
vehicles equipped with an RGB-D camera. Together with 3rd-party libraries,
our system performs localization, mapping, traversability estimation, and nav-
igation to a given goal location. The system is even able to work in previously
unexplored indoor environment, to build the map simultaneously with naviga-
tion, and to navigate through small yet-unmapped areas.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_29

346
J. ˇSkoda and R. Bart´ak
1.1
Related Work
There are many production-grade navigation systems operating in 2D [7], which
are able to control the robot in dynamic environments. While mapping systems,
especially visual, have been able to create 3D maps for several years, state-of-
the-art navigations for ground vehicles still can’t eﬃciently use them and instead
project the 3D maps into a horizontal map, possibly enriched with elevation
information (2.5D) [1], or several horizontal maps [2]. 2D maps can describe a
ﬂoor or multiple ﬂoors of a building, but can’t represent stairs, inclined surfaces
or uneven terrain. 2.5D elevation maps add support for some cases of uneven
terrain, but are unable to represent more complicated structures such as bridges
or tunnels. 3D maps are commonly used by navigation systems for ﬂying robots
[12], however such UAV systems don’t recognize traversable surfaces, so they
can’t be used for ground vehicle navigation. Traversability was solved for example
in [1] by ﬁtting planes to heightmap regions and classifying regions into several
categories, which is very similar to our approach.
The ﬁeld of truly 3D navigation for ground robots contains just a few results
and published systems are mostly not well suitable for usage with a real robot.
For example Stoyanov et al. [10] uses an approach similar to our proposal with
the octree structure and 3D-NDT. However, it cannot be used as real-time nav-
igation as it cannot replan the path with respect to obstacles discovered when
following the planned path.
Morisset et al. [6] solved planning for a hexapod walking robot in a graph of
2.5D regions classiﬁed as ﬂat terrain, uneven terrain and stairs. Then, they use
diﬀerent map representation and planning strategies for the individual classes.
Their solution is signiﬁcantly faster than our approach on ﬂat terrain, but is also
much more complex. They didn’t implement traversability checking important
for wheeled vehicles. We however consider this approach of composing 3D map
out of graph of 2.5D regions to be a comparable alternative to the fully 3D map
we are presenting.
If we focus on individual subproblems – mapping, map representation, and
path-planning – there are interesting published results. State-of-the-art visual
mapping systems for RGB-D camera [4] are precise, and can correct accumulated
odometry errors in the map using loop closure techniques. They are however very
computationally expensive, and the large maps usually need to be preprocessed
in order to be eﬃciently usable. For the preprocessing, we use tree-structured
spatial occupancy grids called octrees [3], which are commonly used for dense
3D map representation. In addition to that, 3D-NDT [9] can be used for map
simpliﬁcation and traversability estimation.
1.2
Motivation
As robots operate in a 3D world, the simpliﬁcation of the world into a 2D map
causes a loss of information. That can prevent the robot from operating correctly
in more complicated situations. We provide three examples of types of situations,
that may happen in real-world scenarios.

3D Navigation for a Mobile Robot
347
In Fig. 1, there are two surface levels and the robot can choose to ride under
the bridge or on it. 2D navigation systems are unable to use the”bridge” and
mostly consider it an obstacle. The robot sometimes has to enter a yet unmapped
area. That can happen for example in rescue missions in an unknown environ-
ment or when some parts of environment were occluded during mapping. Finally,
2D navigation systems often cannot sense holes in the surface or ends of it.
In general, 3D navigation is necessary in any environment, where the robot
should traverse signiﬁcantly uneven surfaces. One of the typical robotic prob-
lems, where 3D navigation would be very useful, is autonomous or assisted rescue
mission in damaged buildings after various disasters.
2
System Outline
Fig. 1. A situation, where the robot can
choose to ride under the bridge or on it.
The input of the system is a partial
map and location estimate continu-
ously received from a localization and
mapping system (SLAM). The map
is composed of a set of nodes, which
correspond to a single captured frame
from the RGB-D camera. A node con-
tains a point-cloud representation of
the captured RGB-D frame, pose esti-
mate of the robot at the time of cap-
turing the respective RGB-D frame, and a timestamp. As that map can contain
millions of points, we process it into a 3D octree [3] grid (discussed in Sect. 3.1).
We can then quickly check any grid cell for traversability or presence of an
obstacle.
A classical occupancy grid would however not be suitable for traversability
check, as it would require too ﬁne grid resolution, which would ruin the perfor-
mance. We therefore propose a solution utilizing approximation of the measured
points with 3D-NDT [9] (3D normal distribution transform) for traversability
estimation. This is described in Sect. 3.2.
Traversable cells of the map can be used for path-planning. We utilize the
well-known A* algorithm with the Manhattan distance as the heuristic and we
propose a speciﬁc cost function (Sect. 4) as the optimality criterion. As the map
representation distinguishes free and unexplored areas, the plan can even traverse
unexplored areas. That is needed because maps often contain small unexplored
“gaps” for example due to occlusions.
The sensors are only able of inaccurate and incomplete perception of the
environment. Localization is sometimes imprecise, which means that the mea-
sured points will be added to a wrong part of the map. The system has to be
able to deal with these errors during navigation and to relocate map points when
an error is detected.

348
J. ˇSkoda and R. Bart´ak
3
Mapping Subsystem
Our system is receiving measurements from an RGB-D camera and from a SLAM
system in order to discover new obstacles or yet unexplored areas. RGB-D mea-
surements are received as point clouds annotated with timestamp and pose esti-
mate. For performance reasons, we use only some RGB-D measurements, that
might contain new information. We select them by thresholding transitional,
angular, and temporal distances from the previous used measurement.
3.1
Octree Structure
The internal representation of the map uses an octree structure implemented
in the Octomap library [3], which is frequently used in state-of-the-art robotic
mapping systems. Octree is a tree structure for accessing a grid with dynamic
size. Every node of the tree is a grid cell, that can contain none or exactly
8 children (therefore octree), which represent the 8 subcells with a two times
shorter edge. The root node’s cell contains the whole environment. The nodes
with no children are called leafs as usual in the graph theory terminology. The
structure is visualized in Fig. 2.
The octree allows the map to describe cluttered parts of the environment with
denser grid and use coarse grid in the free space. Octree therefore saves both
memory, which is an important aspect of 3D mapping, and processing time,
especially during map updates. Currently, only unexplored and empty space,
containing no points, is represented using coarser grid. Merging such cells for
coarser grid can be done simply by recursively deleting their children. Splitting
big cells can be done by creating children until the leaf level. That happens when
the camera measures a point in a merged cell.
Fig. 2. Octree data structure [3] visualization. One occupied cell is drawn in black, free
cells are in non-transparent gray, unexplored cells are transparent.
3.2
Grid Cells Classiﬁcation Using 3D-NDT
Each leaf cell of the map is classiﬁed into one of those disjoint classes: unexplored,
free, obstacle, and traversable. All cells are initially considered unexplored and
later classiﬁed as traversable or obstacle only when the cell contains enough
points for precise classiﬁcation. On the contrary, we perform raytracing to mark
cells as free space or remove previously detected obstacles. When no points were

3D Navigation for a Mobile Robot
349
measured in a cell and some points were seen through that cell, we mark the cell
as free.
To distinguish traversable and obstacle cells, we use a 3D normal distribution
approximation of the points measured in the cell. We basically estimate a 3D
normal distribution covering points measured in every cell. If the distribution is
ﬂat and horizontal, we consider the cell to be traversable (Fig. 3). Each normal
distribution is described by its mean value and a covariance matrix:
µ = 1
n
n

i=1
xi
(1)
C =
1
n −1
n

i=1
(xi −µ)(xi −µ)T
(2)
where x are the vectors of 3D coordinates of points in the cell, n is the number
of the points, µ ∈R3 is the mean value of x and C ∈R3×3 is the covariance
matrix.
3D-NDT can also be represented by ellipsoid describing distribution’s con-
ﬁdence region (generalization of conﬁdence interval for multiple dimensions),
which is the area, where certain fraction (conﬁdence) of points is located (see
Fig. 4). The shape of this ellipsoid is used for traversability estimation. The cen-
ter of the ellipsoid is given by the mean value µ. The directions of the principal
axes of the ellipsoid are given by the eigenvectors of the distribution’s covari-
ance matrix. The squared relative lengths of the principal axes are given by the
corresponding eigenvalues.
We deﬁne the normal vector of the surface measured in a cell as the shortest
axis of the distribution’s ellipsoid. The axis may be inverted (multiplied by −1) if
it doesn’t point in the direction of camera. That way, the normal vector always
points in the direction of free space and its size describes the ﬂatness of the
surface – ﬂat surface has very small normal. Finally, we use this normal vector
to estimate cell traversability: if the normal vector points upwards and is small,
the surface is considered traversable.
3.3
Map Update
There are two types of map update that have to be handled. The ﬁrst type is
an RGB-D measurement that can contain new obstacles or unexplored parts
of the map. For each cell, where enough points were sensed in the received
measurement, we compute a new 3D-NDT and update the type of that cell. We
only consider points received in the latest RGB-D measurement containing this
cell to ensure, that no duplicate points/surfaces caused by localization error will
be present, because they might corrupt the traversability estimation. Moreover,
during the execution of the plan, we periodically check each cell of the path and
its surroundings for obstacles that could be discovered after map update.
The second type of map update is a loop closure. Loop closure is a discovery
of an error in the map detected by the SLAM system. It happens for example

350
J. ˇSkoda and R. Bart´ak
Fig. 3. Visualization of the NDT usage
in two dimensions. Blue ellipses are tra-
versable cells, red ones are obstacles. Notice
the eﬀect of the octree cell merging. (We use
2D visualization for clarity.)
Fig. 4. The conﬁdence region ellipse of
an NDT representing a set of points.
(We use 2D visualization for clarity.)
when the localization provides imprecise location for some time, during which
we inserted obstacles into wrong places of the map. The localization failure can
be caused by odometry error when mapping unknown areas or when the robot
doesn’t recognize already known area.
When loop closure is detected, the SLAM system changes the location esti-
mate of some RGB-D measurements and updates the map accordingly to ﬁx
errors, such as obstacles inserted to wrong parts of the map. The system per-
forms reinsertion of corrected RGB-D measurements starting with the ﬁrst one
with the wrong location estimate until the last measurement. That means mark-
ing the cells corresponding to former measurements as unexplored and inserting
the corrected measurements into the map. The reinsertion can be very time con-
suming, because the upper bound of computational complexity is linear with
respect to the number of measurements. The loop closures in indoor environ-
ments (where we can use the RGB-D sensor) are usually small and contain just
a few latest measurements, so the loop closure handling doesn’t get, in practice,
linearly slower during time.
4
Path Planning
The task of the planning algorithm is to ﬁnd a path between the robot position
and a given goal coordinate in the map structure. The resulting path is a list of
subsequently adjacent map cells and has to fulﬁll several constraints to ensure
safety of the path and reliability of the system: it has to be traversable, obstacle
free, and mostly explored, as we will describe later.
In addition to the above constraints, we optimize the path with respect to a
cost metric, which expresses distance and accounts for possible problems along
the path such as the number of nearby obstacles, unexplored cells, and the angle
of slopes. The motivation for the speciﬁc cost metric is to minimize the chance
of possible problems during plan execution that would lead to re-planning. The
exact deﬁnition of the function can be found in the Master thesis [11].

3D Navigation for a Mobile Robot
351
We do not limit the shape of the path for any particular robot movement
constraints (such as for robots unable to turn in place) and we don’t consider the
robot orientation. That is appropriate for example for robots with diﬀerential
drive, but not for more complicated drives like the Ackermann steering.
For path planning, we currently use the well-known A* algorithm. The algo-
rithm traverses the traversable and unexplored cells of the map (see Sect. 3.2),
while preferring cells in the direction of goal. As the map representation distin-
guishes free and unexplored areas, the planner can even traverse unexplored
areas. That is needed because maps often contain small unexplored “gaps”
caused for example by occlusions.
4.1
Collision and Traversability Checking
The system checks traversability and possible collisions of the robot centered on
a particular cell for all cells traversed by the planning algorithm. We call those
two checks together a safety check.
The safety check can fail because of an obstacle cell that is present within a
certain distance from the checked cell, or by an absence of traversable surface
somewhere within that area. The distance depends on the dimensions of the
robot and in this implementation, we use a simple spherical model of a robot for
safety checking. Three examples of safety check are shown in Fig. 5.
During the checks, we also consider the 3D-NDT stored in the cells to increase
precision of the system. A traversable cell can satisfy the check only if its surfaces
altitude – the z coordinate of the 3D-NDT mean value – is close enough to
that of the safety-checked traversable cell. Therefore, the system can distinguish
between a small traversable asperity and a bigger stair as visualized in Fig. 5,
part A.
Fig. 5. Three cases of safety check. In A,
the traversability is not satisﬁed, as the tra-
versable cell to the left is too low and the
cell to the right is not traversable. In B, the
cell is considered safe, as there is only one
solitary obstacle. In C, the collision check
will fail because of two obstacles.
Fig. 6. The sampling local planner.
Selected path is shown as a thick blue
line. Corresponding linear and angular
velocity command (0.3; −0.1) will be
sent for execution.

352
J. ˇSkoda and R. Bart´ak
To increase robustness against errors in the map, we threshold the number
of obstacles and missing traversable area, so an error in one cell can not cause
the safety check to fail. The system requires at least two obstacle cells or at least
two missing traversable surfaces for reporting the checked cell as unsafe. See the
example in Fig. 5. That is extremely important during the post-planning checks
after map updates, which detect new obstacles along the path. Without this
thresholding, one incorrect cell measurement in that check would require recom-
puting the whole plan. We also ignore obstacles that are under some traversable
surface, e.g. points incorrectly measured below ﬂoor. Full details are given in the
Master thesis [11].
4.2
Plan Execution
We implemented only the global path-planning and used the ROS Navigation
[7] local planner for motion planning. This local planner receives the global plan
as a 3D curve and the robot location and outputs linear and angular velocity
command standardized in ROS. Its algorithm generates several possible velocity
commands, predicts the trajectory of the robot after 1.5 s of executing such
velocity command and chooses the velocity command after which the robot gets
closest to the requested trajectory and to the next vertex of the global path
curve. The weighted sum of those two errors is used as the optimality criterion.
Illustration of the local planner mechanism is in Fig. 6. During the execution of
the plan, the system checks the upcoming part of the plan for traversability and
collisions and triggers replanning if a problem is detected.
5
Implementation
The navigation system was implemented in C++11. We have chosen to use the
ROS (Robotic Operating System) as an abstraction layer. That allows us to use
the ROS mapping tools, simpliﬁes sharing of the navigation system with robotic
community, and enables the system to work on diﬀerent robotic platforms. Also,
we use the Octomap [3] and the PCL [8] libraries, and the rtabmap [4] mapping
system.
We have tested our implementation on our local robotic platform shown in
Fig. 7 equipped with diﬀerential drive and Microsoft Kinect camera. The camera
was positioned on an elevated mount and tilted downwards. The computations
were performed on a quadcore notebook carried by the robot.
The implementation is able to visualize the processed map and the planned
path as shown in Fig. 8.

3D Navigation for a Mobile Robot
353
Fig. 7. Our local robotic platform, on
which we have tested the implementation.
Fig. 8. Planning on an inclined board.
The robot is standing to the left of
the image and the goal is denoted by
a purple sphere. Unsafe cells are the
reachable cells, where collision checking
failed. The edges of board (highlighted
by the stripe pattern) were detected, so
the robot won’t fall from it.
6
Evaluation
To evaluate the results of the implementation, we have suggested several sce-
narios and observed the overall robustness and performance of the system. We
paid attention especially to the reliability of the system and to special situations,
when the navigation fails.
In the scenarios, we ask the system to navigate to given coordinates relative
to the robot starting position. The system has neither prior knowledge of the
environment nor the map. It plans the path after the ﬁrst RGB-D measurement.
As our robot unfortunately doesn’t provide 3D odometry but only 2D wheel
odometry, we use visual odometry tracked by the SLAM system.
6.1
Plan Execution
We repeated each of the scenarios several times and measured the number of
successful runs, Nav-f failures of our navigation system (the robot crashed or
was unable to ﬁnd a path) and Other-f failures of the underlaying systems
(localization was lost, the SLAM map was corrupted). The results can be found
in Table 1. We also identiﬁed several cases, where the system is unable to work.
The RGB-D camera can’t detect transparent surfaces such as glass. Also, the
narrow ﬁeld of view of the camera occasionally caused the used SLAM system
to fail after sharp turns.

354
J. ˇSkoda and R. Bart´ak
More details about the individual scenarios can be found in the Master the-
sis [11]. We have also recorded a few short videos of the scenarios, that are
available at http://www.janskoda.cz/navigation/. Note that for safety reasons,
we were conﬁrming the path plans manually before execution, so the timing in
videos doesn’t correspond with the implementation performance.
Table 1. The reliability evaluation results
Experiment
Ok
Nav-f Other-f N
Around slope
4
0
1
5
Slope
6
2
2
10
Into unexplored
5
0
0
5
Bridge traversal
7
1
2
10
Long operation
6
1
3
10
Navigation reliability 87.5%
Reliability overall
70%
In addition to the overall results, we present one of the scenarios, “bridge
traversal” in detail. In this scenario, the robot had to navigate into a part of
the environment, that was only accessible by a wooden “bridge” (Fig. 9). Other
paths were blocked. The system was able to plan the path, that is shown in
Fig. 10, in about 2 s. Later, it discovered an unsafe cell on the path and after
replanning, the robot arrived to the goal location. The replanning took about
3 s and the resulting path is shown in Fig. 11.
Fig. 9. The environment of the scenario.

3D Navigation for a Mobile Robot
355
Fig. 10. The initial map and the plan to the
temporary goal. The meaning of the used
cell colors is described in Fig. 8.
Fig. 11. The map source point cloud
and the resulting path as recorded by
the SLAM system.
7
Conclusion
We have proposed a 3D navigation system for ground vehicles. The design of our
system has several advantages compared to other state-of-the-art navigations. It
is well suited for uneven terrain, that can even have multiple levels. 3D navigation
is also safer than standard navigation systems in many situations, because it can
avoid surface edges and holes in the ground, that are invisible for most available
navigations. The novelty of the system lies in our map structure and map update
procedures, which are able to model the environment accurately, yet eﬃciently
with respect to map update and path planning speed. Also, we paid a lot of
attention to dealing with the problems caused by imprecise localization and
RGB-D input of the broadly available cameras.
On the other hand, our design results in several disadvantages as well. The
used RGB-D camera can’t sense any obstacles closer than approximately 0.4 m
and has relatively narrow ﬁeld of view, which together limits the ability of the
system to avoid obstacles. Also, the computational complexity and especially
the latency of the RGB-D stream processing both by the camera driver and our
system slows down the reaction speed of the system. It therefore takes about 2 s
to react to new or moving obstacles.
The results of the implementation were evaluated during a series of experi-
ments where a robot had to navigate in environments containing a single naviga-
tional problem. In the evaluation, we have shown that the implementation of the
system works as expected, assessed the reliability of the system and highlighted
its problems.

356
J. ˇSkoda and R. Bart´ak
Acknowledgements. We would like to thank Dr. Ohya from University of Tsukuba,
Japan, for his valuable insight that helped with this work and Dr. Obdˇz´alek for help
with the robotic platform. Research is supported by the Czech Science Foundation
under the contract P103-15-19877S.
References
1. Doroodgar, B., Yugang, L., Goldie, N.: A learning-based semi-autonomous con-
troller for robotic exploration of unknown disaster scenes while searching for vic-
tims. IEEE Trans. Cybern. 44, 12 (2014)
2. Hornung, A., Phillips, M., Jones, E.G., Bennewitz, M., Likhachev, M., Chitta, S.:
Navigation in three-dimensional cluttered environments for mobile manipulation.
In: IEEE International Conference on Robotics and Automation (ICRA) (2012)
3. Hornung, A., Wurm, K.M., Bennewitz, M., Stachniss, C., Burgard, W. OctoMap :
an eﬃcient probabilistic 3D mapping framework based on octrees. In: IEEE Inter-
national Conference on Autonomous Robots (2013)
4. Labbe, M., Michaud, F.: Online global loop closure detection for large-scale multi-
session graph-based SLAM. In: IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS) (2014)
5. Maier, D., Hornung, A., Bennewitz, M.: Real-time navigation in 3D environ-
ments based on depth camera data. In: IEEE/RAS International Conference on
Humanoid Robots (2012)
6. Morisset, B., et al.: Leaving ﬂatland: toward real-time 3D navigation. In: IEEE
International Conference on Robotics and Automation (ICRA) (2009)
7. Marder-Eppstein, E., Berger, E., Foote, T., Gerkey, B., Konolige, K.: The oﬃce
marathon: robust navigation in an indoor oﬃce environment. In: IEEE Interna-
tional Conference on Robotics and Automation (ICRA) (2010)
8. Rusu, R.B., Cousins, S.: 3D is here: point cloud library (PCL). In: IEEE Interna-
tional Conference on Robotics and Automation (ICRA) (2011)
9. Stoyanov, T., Magnusson, M., Almqvist, H., Lilienthal, A.: On the accuracy of the
3D normal distributions transform as a tool for spatial representation. In: IEEE
International Conference on Intelligent Robots and Systems (IROS) (2011)
10. Stoyanov, T., Magnusson, M., Andreasson, H., Lilienthal, A.: Path planning in
3D environments using the normal distributions transform. In: IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems (IROS) (2010)
11. ˇSkoda, J.: 3D Navigation for Mobile Robots, Master thesis, Charles University
(2017)
12. Xu, S., Honegger, D., Pollefeys, M., Heng, L.: Real-time 3D navigation for
autonomous vision-guided MAVs. In: IEEE International Conference on Intelli-
gent Robots and Systems (IROS) (2015)

Educational Robotics

Robobo: The Next Generation of Educational Robot
Francisco Bellas1(✉), Martin Naya1, Gervasio Varela2, Luis Llamas2, Moises Bautista1,
Abraham Prieto1, and Richard J. Duro1
1 Integrated Group for Engineering Research, Universidade da Coruña, A Coruña, Spain
{francisco.bellas,martin.naya,m.bautista,abprieto,
richard}@udc.es
2 Mytech, A Coruña, Spain
{gervasio.varela,luis.llamas}@mytechia.com
Abstract. This paper presents Robobo in the context of higher education.
Robobo is a low-cost educational mobile robot that combines a simple wheeled
base with a smartphone, which provides the latest technology to the robot. With
Robobo, students can develop their own projects through ROS using cameras,
microphones or high-resolution screens, bringing teaching closer to the real
requirements of the market they will ﬁnd when they ﬁnish their studies. In this
work, the hardware and software development that has been carried out is
described in detail. Furthermore, it is presented an exemplifying case of student
project that shows the potentiality of Robobo in this context.
Keywords: Educational robots · ROS · Human-robot interaction
1
Introduction
University degrees in computer science and engineering have been using mobile robots
in diﬀerent subjects from a long time [1–3]. They are a basic tool when teaching auton‐
omous robotics but, in addition, they have shown to be very useful in other subjects like
computer vision [4] or control systems. Up to now, mainly due to economical limitations,
these educational robots have been quite simple in technological terms. Thus, the most
popular educational robot in higher education is the Lego Mindstorms [5], a modular
robot equipped with an Arm926ej-S Core@300 MHz processor in its last version (EV3).
This robot is equipped with a sonar or infrared sensor, a gyroscope and a color sensor,
and it has an average price of 350–400€. A very popular and small mobile robot in
European Universities is the Thymio II [6], suitable for multi-robot teaching. It only
costs around 120€ and it is equipped with infrared sensors, a temperature sensor and an
accelerometer. It has a low performance PIC24@32 MHz microcontroller. As a conse‐
quence of this simplicity, the practical use of real robots in higher education has been
limited to simple control mechanisms and laboratory applications.
This issue has not been a real problem because robotics was not a real market in
industry, so robots were used in classes as simple prototypes, without a rigorous trans‐
lation to reality. In fact, the use of robotic simulators has been very popular. But, as it
is known, the current and near future situation is that robotics will be a key market for
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_30

engineers and computer scientists [7]. As a consequence, the robots used in University
classrooms must be updated to include all the features of the real robots that will be
present in our reality, both in industrial and domestic domains. This implies that robotic
classes must change simple platforms or simulated robots for realistic ones. But if one
moves to robots with more powerful sensors, the price increases. For example, the e-
puck robot [8] is a mobile robot that was very popular in the last decade in several
European Universities. It has infrared sensors too and a dsPIC30F6014A@64 MHz
controller, and it includes a low-resolution camera of 640 × 480 pixels. The average
price of one unit is 600€. A more powerful robot used in higher education is the Kephera
IV [9], which uses a 752 × 480 resolution camera, an ARM Cortex 800 MHz processor,
infrared and sonar sensors, but with a price of around 2500€. Considering high-end
mobile robots like the NAO [10], with an Atom Z530 1.6 Ghz CPU, two 1280 × 960
cameras, sonar and tactile sensors, the price becomes even higher (around 6000€), which
is unreachable for most higher education institutions when considering the large number
of robots that intensive teaching of these topics would imply (no more than two or three
students per robot, and preferably only one). The most remarkable platform in terms of
cost-features balance is the TurtleBot [11]. It is a low-cost (520€) and open-source
mobile robot, which is very popular for ROS (Robot Operating System) users as a testing
platform, mainly for SLAM (Simultaneous Localization and Mapping) research,
because it contains a 360º LIDAR system in the latest version. Its application to a wider
range of robotic domains, as human-robot interaction, using speech recognition, cameras
or tactile interfaces, is limited.
Consequently, a few years ago, the authors of this paper decided to start the devel‐
opment of low-cost educational mobile robot that can incorporate state-of-the-art tech‐
nology to be used in engineering and computer science degrees. This robot has been
called Robobo [12], and it combines a simple wheeled base with a smartphone. With
Robobo, students can develop engineering projects using cameras, microphones or high-
resolution screens, bringing teaching closer to the real market they will ﬁnd when they
ﬁnish their studies. This paper is focused on the presentation of the current version of
the Robobo hardware and software, which is explained in Sect. 2. Furthermore, its
potentiality in higher education will be illustrated through an exemplifying case of
student project, in Sect. 3. This project exploits the Human-robot Interaction (HRI)
features that are provided by the smartphone based on its visual, auditory and tactile
capabilities.
2
The Robobo Robot
The Robobo hardware is made up of two elements: a mobile wheeled base and a smart‐
phone that is attached to the base, as shown in Fig. 1. It must be pointed out that the
Robobo robot is not only the mobile base, but the combination of base and smartphone.
This conﬁguration provides the following features:
• Low-cost: The educational institution must acquire only the mobile base, because the
smartphone can be provided by the student. Our experience in this aspect is that
students perceive this extra application of their own smartphone as positive. Robobo
360
F. Bellas et al.

will be commercialized in classroom packs with a reference cost per unit that will be
lower than the most popular educational robots, TurtleBot 3 or LEGO EV3.
• Continuous update: The smartphone technology, both in hardware and software, is
in continuous update, so Robobo can be easily improved by simply upgrading the
smartphone. It is well-known that students usually have the latest models, so new
hardware features can be easily incorporated in classes. The Robobo base will support
new smartphone features by simply updating its ﬁrmware and software.
• Long-term duration: Related to the previous two features, it can be concluded that
the Robobo validity is longer than other educational robots, and the investment in
the mobile base lasts longer.
• State-of-the-art technology: Apart from the sensors and actuators that are in the
mobile base and that will be explained later in detail, students can use all the hardware
elements that are included in typical current smartphones:
– 2 high-resolution cameras
– Proximity, light and temperature sensors
– Gyroscope, accelerometer and magnetometer
– GPS
– Microphone
– High-resolution LCD screen
– Connectivity: 4G, WI-FI, USB, …
– Speaker
– ….
Fig. 1. The Robobo robot with the smartphone attached (left). The mobile base with a 3D printed
pusher (right)
In addition to the Robobo hardware, a software architecture that allows to program
the Robobo through ROS (Robot Operating System) [13] has been developed. In the
Robobo: The Next Generation of Educational Robot
361

following sections, the Robobo hardware and software design and implementation will
be described.
2.1
Robobo Hardware
The Robobo hardware is composed by two elements: the smartphone and the mobile
base. Any commercial smartphone with the following requirements is supported:
• Operating system: Android 5.1 or higher (iOS version is under development)
• Display size: minimum 4.7 inches and maximum 5.5 inches
• Thickness: 95 mm maximum
The main elements of the base hardware are displayed in Fig. 2. The mobile base
has two driving motors (MT1, MT2) which are in charge of performing the displacement
of the platform. They are 150:1 gear motors with encoder plus a pinion-crow design.
This extra ratio increases the maximum available torque and the robot’s acceleration.
At the back of the base, there are two caster balls which allow an omnidirectional move‐
ment. Furthermore, the base is equipped with two additional motors that allow an inde‐
pendent movement of the base and the smartphone:
Fig. 2. Main hardware elements of the Robobo base
Pan motor (PAN): It is a 150:1 gear motor with encoder with pinion-crown reduction.
This reduction provides the possibility of performing soft and precise movements, with
a 4.86º minimum angular displacement.
362
F. Bellas et al.

Tilt motor (TILT): It is a 1000:1 gear motor with encoder. This is a high ratio for two
reasons:
• 1000:1 reduction implies that the smartphone is self-supported. No current is needed
to maintain the smartphone in the desired position.
• This ratio gives us the required torque to move the smartphone in the worst-case
conditions. The gear motor gives a maximum of 44 Ncm, which is higher that the
13 Ncm in the worst considered case obtained in our tests.
These two motors provide Robobo with a many diﬀerent movements, and with a
richer type of HRI possibilities.
A speciﬁc Printed Circuit Board (PCB) has been designed for Robobo in order to
maximize the performance and capabilities of the base, and to adapt the electronics to
the external design. These are the main electronic components:
Microcontroller: The electronic structure of the Robobo is built around a PIC32
microcontroller. The CPU runs at 80 MHz, being enough to control the sensors, actuators
and carry out miscellaneous tasks that the PIC must perform. It is in charge of carrying
out simple calculations and operations, such as motor-encoder adjustments or analog
battery readings, leaving the complex and high cost computing tasks to the smartphone.
Sensors: Robobo uses 8 VCNL4040 Vishay I2C infrared sensors (IR1 to IR8) for
three purposes, which depend on their orientation:
• Object detection (IR1, IR3, IR5, IR7): They are placed in the front and rear part of
the base oriented parallel to the ground. They are used for obstacle detection and
avoidance in cluttered environments. Furthermore, they can perform a soft object
approximation thanks to their gradual detection range.
• Fall avoidance (IR2, IR4, IR6, IR8): Placed in the front and rear part of base, they
are placed at a 45º degree orientation towards the ground, with the aim of detecting
holes and avoiding falls when on tables or stairs.
• Ambient light sensing: The VCNL4040 can also be used as an ambient light sensor.
Actuators: As commented above, 4 DC-DC motors are used to move and control the
base. The 4 motors have a 6-pole magnetic encoder shaft to precisely measure the motor
revolutions. In addition, they are controlled by two motor drivers that are limited to a
maximum of 1.3A each. The motor drivers are controlled by a PWM signal from the
PIC microcontroller.
RGB LEDs: There are 7 RGB LEDs (L1 to L7 in Fig. 2). Five of them are placed in
the front part of the base and two at the rear. The LED driver has a 4096 bit resolution
per color, which provides a high color resolution. The LEDs have a user interface func‐
tionality, showing information to the user, such as the battery status or infrared obstacle
detection.
Bluetooth module: A Microchip BM78 Bluetooth module is used to establish
communication between the base and the smartphone. This module was selected for
being a simple, low cost, CE and FCC certiﬁed Bluetooth module. It uses the 4.2 version,
which supports the SPP (Serial Port Proﬁle) for serial communication with the micro‐
controller.
Robobo: The Next Generation of Educational Robot
363

Power system: A 2500 mAh and 3.7 V LiPo battery is mounted in the base. This
battery was selected for its high charge density, in order to provide a high level of
autonomy in the minimum possible space. The voltage range of these batteries is 4.2 V–
3.0 V. Two integrated circuits (IC) complete the rest of the power system:
• A Buck-Boost converter to obtain a ﬁxed 3.3 V used by the microcontroller, Blue‐
tooth, LEDs, etc. This IC is needed due to the variable voltage range of the battery.
• A Step-up converter, to boost the battery voltage to the motor’s 7 V.
Furthermore, there is a battery manager in the circuit board for recharging the battery
through a BATT USB connector (see Fig. 2).
2.2
Robobo Software
The Robobo software development can be organized into low-level (ﬁrmware) and high-
level (software architecture) blocks:
Low-level: Figure 3 shows the general hardware architecture of the mobile base,
where only the main connection paths are represented to clarify the scheme. The core
of the system is the PIC32 microcontroller, which is in charge of controlling the diﬀerent
integrated circuits, sensors and actuators on the base. Furthermore, the PIC32 manages
the communications between the microcontroller and the Bluetooth module, which
sends information to the smartphone and receives motor commands from it.
Fig. 3. Robobo electronic architecture
364
F. Bellas et al.

High-level: The main requirement of the Robobo software architecture was to
support the programming of the robot using diﬀerent programming paradigms like ROS,
Java or Scratch. A completely modular software architecture, shown in Fig. 4, has been
designed. It is based around the concept of Robobo module and a very lightweight
library, called the Robobo Framework, which provides the essential mechanisms to
manage and run those modules on an Android smartphone. On top of this library, two
diﬀerent kinds of modules can be loaded:
• Functionality modules (the orange ones in Fig. 4): Implemented in Java using the
Android standard API, they provide diﬀerent functionalities to the robot, like speech
recognition, face detection, environment sensing or physical movement. Further‐
more, these modules complement the native API of the robot, which can be directly
used for programming it using Java for Android, thus creating Robobo Apps that can
be run in any Android smartphone.
• A series of proxy modules (ros-proxy and remote-proxy modules in Fig. 4), also
implemented in Java, which provide particular interfaces to other programming
environments. These interfaces provide a translation between an external API or
protocol, and the native Robobo API in Java.
Fig. 4. Block diagram of the Robobo software architecture
Robobo comes by default with a particular set of these modules but, as they are
completely decoupled between them and with respect to the framework, advanced users
can customize the set of modules, and even implement new modules to support new
robot functionalities, or even new programming paradigms through proxy modules.
Finally, it is important to note that there exists a module for connecting the Robobo
framework and its modules to the Robobo robotic base. The rob-interface module, shown
in pink in Fig. 4, implements the Bluetooth communications protocol of the base and
provides a control API for other modules to use.
Robobo: The Next Generation of Educational Robot
365

Java programming is directly supported by the native API provided by the diﬀerent
modules. Using the Robobo framework users can create custom Android Apps that
control the behavior of the robot. These apps use the programming interfaces of the
Robobo modules to access the diﬀerent functionalities of the robot and build new robot
behaviors or solve new tasks.
For block programming, two diﬀerent approaches are currently supported: Scratch,
and our own block-based IDE that uses Blockly. As can be seen in Fig. 4, both
approaches are connected to the framework using the same proxy, the Robobo Remote
Access Protocol (RRAP). This is a simple JSON based protocol that allows remote
access to the Robobo modules’ APIs.
Finally, Robobo can also be programmed using the Robot Operating System (ROS),
which is the main programming language for teaching robotics at higher education
levels. ROS is supported by a ros-proxy module than translates between the native APIs
of the modules and ROS messages and topics.
3
Example of Student Project
To illustrate the type of project that can be carried out using Robobo in computer science
or engineering degrees, this section describes a speciﬁc example that was proposed to
students in the “Robotics” subject of the Computer Science degree, at the University of
Coruna, during the 2016/17 academic year. The project was focused on HRI, speciﬁ‐
cally, on programming the Robobo to act as a pet. That is, the robot must ask the user
for food, attention and “aﬀection”. The way it is performed must be selected by the
students, giving them the opportunity of creating highly realistic solutions.
The functionality modules required to solve this project use all the interactive
modalities of Robobo:
Speech: The speech capability of the robot was deployed using the Android libraries
and a speechProducer module was implemented.
Sound recognition: SpeechRecognition and soundRecognition were implemented
using the Pocketsphinx [14] and TarsosDSP [15] libraries.
Vision: A faceRecognition module was implemented using the Android libraries,
and a colorDetection module using the OpenCV library [16].
Tactile sense: The touchDetection module performs the recognition of tactile
gestures over the smartphone screen. It allows a tap (quick and simple touch on the
screen), a touch (sustained touch over the screen), a caress (slide over the screen) and
ﬂings (end a slide in a fast way). The Android libraries were used to deploy this module.
Visual emotion: The emotionProducer module allows to display diﬀerent images or
animations in the smartphone screen, giving a powerful interaction capability to users
to show the robot emotions.
Movements: The motorCommand module allows the user to move the four motors
of the Robobo base.
With these functionality modules and the programming language selected by
students (and supported by ROS), the speciﬁcations of the pet behavior were the
following:
366
F. Bellas et al.

• Basic instincts: Robobo must store hunger and thirst levels and, when they are below
a threshold, it must say “I am hungry” or “I am thirsty” until they are reﬁlled.
• Movement: Robobo must move in order to capture user attention if no event has been
executed for a predeﬁned time-lapse. For instance, it can spin or emit a sound.
• Touch: Two diﬀerent behaviors must be implemented depending on the type of
screen touch modality:
– Tap: If the user touches the screen with a single tap, Robobo must react diﬀerently
depending on the part of the “face” that is touched. If it is in the “eye” or in the
“mouth”, it will move the tilt motor backwards and show an angry expression (see
Fig. 5a and b for examples). If it is in any other point, it will show a laughing face
and emit a sound saying “tickles” (see Fig. 5c and d for examples)
(b)
(c)
(d)
(a)
Fig. 5. Expressions of the touching behavior of the Robobo pet
– Flip: If the user slides a ﬁnger over the screen, the pan-tilt unit moves accordingly.
The possible slide directions must be discretized into four pan-tilt movements: tilt
backwards or forwards and tilt rightwards or leftwards.
• Voice: Robobo must react to the following predeﬁned phrases:
– “Here comes the food”: The robot prepares to receive food
– “Here comes the drink”: The robot prepares to receive drink
– “Hello”: The robot answers by saying “Hello”
– “How are you?”: The robot will respond by saying its thirst and hunger levels
• Vision:
– Color: Robobo must detect 2 diﬀerent colors, green for food and blue for drink,
but only after the corresponding voice command has been detected. In both cases,
it must say whether the color is correct or not. For instance, if the user says “Here
comes the drink”, the robot must look for a blue color area of a predeﬁned size in
its ﬁeld of view (the left image of Fig. 6 shows an example), and after a time-
lapse, it must say “Thank you for the drink” or “I don’t see the drink”.
Robobo: The Next Generation of Educational Robot
367

Fig. 6. Green ball that represents “food” (left) and user face recognition (right)
– Face: Robobo must detect the user face. If it is below a threshold, the robot will
move backwards (Fig. 6 right)
This example provided an engaging experience for the students. As shown, it
addresses many topics within robotics and programming while, at the same time,
provides a fun way for the students to get acquainted with them. In fact, many of the
functionalities can be initially programmed without using the base, and the students
would often do this at home playing around with their mobile phone.
Summarizing, even though this example is simple and it is an initial test, it is easy
to see that there is a lot of potential in using this type of simple robots with high end
functionalities (sensing and actuation structures provided by the combination of the base
and the smartphone). It is our aim to progressively introduce this approach in more
courses during the next year.
4
Conclusions
Traditional robots used in higher education must be updated in order to prepare engi‐
neering and computer science students to the real market they will face in the near future.
In this line, a low-cost educational robot called Robobo has been designed, developed
and tested, which is made up of a simple mobile base that has a smartphone attached to
it. With this conﬁguration, state-of-the-art technology can be used by students in classes
now and in the future, because Robobo can be easily updated only by upgrading the
smartphone model. The robot will be available in November 2017 through the Robobo
Project web page: http://en.theroboboproject.com.
368
F. Bellas et al.

References
1. Schilling, K., Rösch, O.: Mobile mini-robots for engineering education. Global J. Eng. Educ.
6(1), 79–84 (2002). IGI
2. Fagin, B., Merkle, L.: Quantitative analysis of the eﬀects of robots on introductory Computer
Science education. J. Educ. Resour. Comput. 4, 1 (2002). Article 2
3. Williams, A.: The qualitative impact of using LEGO MINDSTORMS robots to teach
computer engineering. IEEE Trans. Educ. 46(1), 206 (2003)
4. Sankowski, D., Nowakowski, J.: Computer Vision in Robotics and Industrial Applications.
World Scientiﬁc, Singapore (2014)
5. Danahy, E., Wang, E., Brockman, J., Carberry, A., Shapiro, B., Rogers, C.B.: Lego-based
robotics in higher education: 15 years of student creativity. Int. J. Adv. Rob. Syst. 11(2), 27
(2014)
6. Mondada, F., et al.: Bringing robotics to formal education: the Thymio open-source hardware
robot. IEEE Robot. Autom. Mag. 24(1), 77–85 (2017)
7. Frey, C., Osborne, M.: The future of employment: how susceptible are jobs to
computerisation? Technol. Forecast. Soc. Chang. 114, 254–280 (2017)
8. Mondada, F., Bonani, M., Raemy, X., Pugh, J., Cianci, C., Klaptocz, A., Magnenat, S.,
Zuﬀerey, J., Floreano, D., Martinoli, A.: The e-puck, a robot designed for education in
engineering. In: Proceedings of the 9th Conference on Autonomous Robot Systems and
Competitions (2009)
9. Soares, J., Navarro, I., Martinoli, A.: The Khepera IV mobile robot: performance evaluation,
sensory data and software toolbox. In: Proceedings of Robot 2015: Second Iberian Robotics
Conference, pp. 767–781 (2016)
10. Shamsuddin, S. et al.: Initial response of autistic children in human-robot interaction therapy
with humanoid robot NAO. In: Proceedings of the 2012 IEEE 8th International Colloquium
on Signal Processing and its Applications, pp. 188–193 (2012)
11. TurtleBot we page. http://www.turtlebot.com
12. The Robobo Project web page. http://en.theroboboproject.com
13. ROS web page. http://www.ros.org
14. Pocketsphinx web page. https://github.com/cmusphinx/pocketsphinx
15. TarsosDSP web page. https://github.com/JorenSix/TarsosDSP
16. OpenCV web page. http://opencv.org
Robobo: The Next Generation of Educational Robot
369

The ROSIN Education Concept
Fostering ROS Industrial-Related Robotics Education
in Europe
Alexander Ferrein, Stefan Schiﬀer(B), and Stephan Kallweit
Mobile Autonomous Systems and Cognitive Robotics Institute (MASCOR),
FH Aachen University of Applied Sciences, Aachen, Germany
{ferrein,s.schiffer,kallweit}@fh-aachen.de
Abstract. ROS Industrial (ROS-I) is an eﬀort to deploy the Robot
Operating System (ROS) for industrial manufacturing applications. The
ROS-I activities are organised by the ROS Industrial consortium (RIC).
With the EU-funded project ROSIN, which started in 2017, the ROS-I
activities are further supported. The project will give out funds for devel-
oping ROS-I components. As a further important measure, the ROSIN
project focuses on education measures for training a large number of stu-
dents and industry professionals to become specialists in ROS-I. In this
paper, we outline the broad ROSIN education programme, which con-
sists of a series of summer schools, a professional academy and intends
to provide the course contents in Massive Open Online Courses as well.
Keywords: Robot Operating System (ROS) · ROS Industrial (ROS-I) ·
Robotics education · Professional training · Summer school · MOOC
1
Introduction
The inﬂuence of robotic systems is steadily increasing, in the industry and the
automation sector just as well as in areas such as mobile service robotics. The
development of an intelligent robot application is a complex and time-consuming
task that requires a lot of expertise in various ﬁelds. Hence, developers of such
applications need to be well-trained.
Middlewares help in developing applications. One of the most well-known
robotics middlewares is the Robot Operating System (ROS). The ROS Indus-
trial (ROS-I) branch of the ROS project aims at extending ROS with features
important for industrial manufacturing applications. In this paper we present
the EU-funded ROSIN project that aims at fostering the use of high-quality
ROS-based solutions in industry applications. We will focus on the educational
activities planned and already started in the ROSIN project.
The rest is organised as follows. We start with a brief introduction of the back-
ground and some related work. In the next section, we present the core ideas of the
ROSIN project. As this paper focusses on the educational aspects of the ROSIN
project, we review other existing ROS education initiatives in Sect. 4, before we
outline the ROSIN education activities in Sect. 5. We conclude with Sect. 6.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_31

ROSIN Education
371
2
Background and Related Work
Robot middlewares are facilitating the robot application design. They come with
tool support, they allow to debug the real-time software in a principled way and
they open the opportunity to simulate the application in a 3D physical simulation
environment. A number of middlewares from the academic sector are available.
Since 2000, the Open Robot Control Software (Orocos) is developed. It is quite
well known for its Kinematics and Dynamics Library and its Bayesian Filter
Library. The Player project [6] which also started in early 2000 generated a
widely used network server for cross-robot development which is often combined
with Stage [5]—a 2D simulator for robot hardware. Other approaches like Urbi [1]
are hardware dependant and belong more to the area of scripting languages.
The framework Fawkes is used in competitions like RoboCup but is not widely
deployed [8].
The aforementioned systems have in common that they are mostly used in
academia. None of these frameworks have been intensively applied for industrial
applications. In the above list, the very well-known middleware Robot Operat-
ing System (ROS) [9], which has celebrated its 10th anniversary in 2017, is yet
missing. ROS is one of the most accepted frameworks for robotic applications
world-wide and has a wide-spread community that supports it. The ROS-I [3]
branch started in 2012, with industrial robotic applications in mind. These appli-
cations should beneﬁt from components already available in ROS, such as, say,
perception, navigation or path planning (e.g. [7]). Up to now, the acceptance of
ROS-I in industrial environments is still low. This is despite the fact that ROS
can help solving challenging future industrial robot applications.
To steer and to support the development of ROS for an industrial setting,
the ROS Industrial Consortium America has been founded in 2013 at the South
West Research Institute (SwRI). The goal of the consortium is to foster the devel-
opment of the ROS-I core as well as ROS-I components for industrial robots.
By using ﬂexible and advanced techniques such as object detection, environment
modelling or path planning so far deployed in mobile robotics mainly in acad-
emia, industrial robot applications can heavily beneﬁt. This could also lead to
cutting the costs for developing industrial robot applications by (a) making use
of open-source software supported by a large community which allow commer-
cial use without restrictions (through using BSD or Apache 2.0 license models),
and (b) reducing the manufacturers’ technology “lock-in” risk by providing stan-
dardised robot and sensor interfaces through ROS Industrial. In the meantime,
there exists also a ROS Industrial Consortium Europe (RIC-EU), led by Fraun-
hofer IPA. Very recently, it has been agreed on founding the ROS Industrial
Consortium Asia Paciﬁc.
The eﬀort of pushing ROS Industrial recently gained more support. In the
beginning of 2017, the H2020 project ROSIN funded by the European Com-
mission was started. ROSIN has the goal to further push the ROS-I activities
in cooperation with the ROS Industrial Consortium Europe. This is pursued
by providing funds for ROS-I-related software projects, by improving the soft-
ware quality developed in those projects, and by a broad education initiative for

372
A. Ferrein et al.
training software engineers with ROS-I. We will describe the ROSIN project in
the following section in more detail.
3
The ROSIN Project
The Robot Operating System has gained much attention in the academic world
and has become a de-facto standard middleware for mobile robotics applications.
World-wide, many research institutes make use of this framework and provide
their research work as open-source software modules. To this end, the process to
develop a complex robotic application is much easier and less time-consuming
today than it was 10 years ago because many important components of such a
system are available as ROS nodes. However, many ROS components do not
quite meet the requirements of industrial robotics applications in terms of real-
time guarantees or software quality yet. The ROS-I eﬀort—as mentioned above—
was started to deploy ROS also for industrial robots. The beneﬁts are that a
larger developer community from diﬀerent stakeholders can provide software
solutions. Further, with ROS-I a form of standardisation of components, sensors
and algorithms takes place.
3.1
Overview
The goals of the ROSIN project are to change the availability of high-quality
intelligent robot software components especially for the European industry. To
reach this goal, the criticism brought forward by the industry against ROS, in
its current form, has to be addressed. The main points raised are:
– lack of (suﬃcient) real-time support;
– stability issues;
– no (software) quality guarantees of the framework, and
– a lack of professional training sites.
The ROSIN project’s work programme addresses this criticism with three
main focus areas. The development of relevant industrial software components
in high-quality is supported by ROSIN with a funding from the European
Commission. The consortium consists of the Robotics Institute of TU Delft,
the Netherlands, the Fraunhofer Institute for Manufacturing Engineering and
Automation (IPA), Germany, the IT University of Copenhagen, Denmark, the
Mobile Autonomous Systems & Cognitive Robotics Institute at FH Aachen Uni-
versity of Applied Sciences, Germany, Tecnalia Research & Innovation, Spain,
and ABB AB, Sweden. The ﬁrst measure of the ROSIN project is to fund so-
called Focussed Technical Projects (FTPs, see Sect. 3.2). Second, with special
tool support and dedicated training, the quality of the software developed in
FTPs will be improved (Sect. 3.3). Third, the development and QA measures
are ﬂanked with a broad education initiative for current and future ROS-I soft-
ware engineers (Sect. 3.4). In the following, we describe these measures in greater
detail.

ROSIN Education
373
3.2
Focussed Technical Projects
The idea of Focussed Technical Projects as promoted by the ROS Industrial
Consortium (RIC) is to kick-start relevant and needed ROS-I capabilities. A full
member of a RIC can champion a particular project and share the cost and
eﬀort with other RIC members who are interested in that particular capability.
Under the guidance of the Consortium Manager, the SwRI, a network of inter-
ested developers start with the implementation of the component as soon as the
funds are in place. The idea of FTPs is the fundamental concept to support
developments within the ROSIN project. With funds allocated by the European
Commission for ROSIN, the funding for FTPs is in place. Members of the ROSIN
project board decide, which projects should be funded. It further monitors the
progress of a respective FTP. The challenging goal of the ROSIN project is to
give out funds for 50 successful FTPs over a time span of four years. For more
information about how to apply for an FTP, please visit the ROSIN project’s
website.1
3.3
Quality Assurance Aspects
The ROSIN project proposal mentions three strategies how software quality
of the newly established ROS-I components could be improved. The quality
assurance strategy includes a Community-based quality management process. It
will be established in such a way that it proposes quality standards which may or
may not be obeyed, but it will leverage to force software developers to comply
with the given quality standard. There will also be workshops and tutorials
as well as some technical support. Within the ROSIN project, a Continuous
Integration infrastructure will be provided allowing code reviews, error reporting
and unit tests. Further, ROS-speciﬁc bug and quality trackers are to be developed
within the ROSIN project. Model-driven development has been identiﬁed as a
successful design and software production methodology. While there exist some
description languages such as the Uniﬁed Robot Description Format (URDF) in
ROS already, the concept of model-driven code generation will be strengthened
to provide another means to assure the software quality. The component testing
will also be improved including thorough testing of the ROS core. Besides test-
based validation methods, also program-analysis based validation and regression
methods will be applied to the developed ROS-I components.
3.4
Education
Even when a company agrees to use ROS-I as the middleware for their soft-
ware developments, the major question remains where to hire employees with
fundamental ROS-I knowledge. Despite the fact that many roboticists leaving
varsity these days have been in contact with ROS before, profound and certiﬁed
knowledge is desirable from a company’s viewpoint. Therefore, a third pillar of
1 http://www.rosin-project.eu/

374
A. Ferrein et al.
the ROSIN concept foresees an education and training programme to impart in-
depth knowledge of ROS Industrial. The diﬀerent ROSIN education measures,
which will be described in Sect. 5 in detail, will impart the required ROS-I knowl-
edge, needed by the industry. The project comes with a number of measures,
but is open to further educational ideas. Therefore, it is also possible to apply
for funds to run additional ROS-I education measures.
4
World-Wide ROS Training Activities
Even though ROS is quite well established and is the standard middleware for
mobile robotics, the possibilities to receive professional training are limited. The
academic world often uses ROS in the classroom, but the number of publicly
available courses world-wide is less than ten. The well-established ROS Industrial
Training by Fraunhofer IPA for Europe2 or the one held at SwRI3 in the USA
are known to be of high quality. They usually host around 40 participants with
the aim of building the bridge between ROS and ROS Industrial with a lot of
practical contents.4
(a) ROS Summer School 2016
held in Aachen, Germany
(b) ROS Industrial training
at Fraunhofer IPA
Fig. 1. ROS Summer School 2016 held in Aachen, Germany
The ROS Summer School in Aachen5 is well established and was already
held ﬁve times. The last summer school had more than 50 participants out of
27 nations and went on for twelve days. The program is structured in a ROS
introduction, basics, higher level applications of mobile robotics and a UAV
session. A small introduction to MoveIt! is given for interested participants.
The lectures and laboratory sessions are mixed equally with a high level of
practical work with custom hardware. A more detailed description of the concept,
2 http://rosindustrial.org/news/2016/9/26/ros-industrial-training-and-conference-2016-schedule-
now-online
3 http://aeswiki.datasys.swri.edu/rositraining/indigo/Exercises/
4 See, e.g., http://rosindustrial.org/news/2016/4/20/recap-ros-i-training-spring-2016
5 https://www.fh-aachen.de/en/faculties/mechanical-engineering-and-mechatronics/
international-topics/ros/

ROSIN Education
375
the organisation and the ﬁnancing of the Aachen ROS Summer School has been
described in [4]. Figure 1a gives an impression of the Summer School held in
Aachen last year. Figure 1b from a training at Fraunhofer IPA.6
The number of novel ROS training activities increased over the last two
years. For instance, in July 2015, a summer school was established in China,
Shanghai, East China Normal University with over 200 participants for the ﬁrst
time. It is a four day event and consists mostly of lectures without any practical
work with robots. In Turkey, at the Eskisehir Osmangazi University & Inovasyon
Muhendislik a three day event was held for the 2nd time with ten participants
focusing on ROS Basics, Simulation in ROS and ROS higher level applications.
Lectures and laboratory sessions are equally split. In the UK, a ROS summer
course is established in London, England, Middlesex at the University of London.
It hosts eleven participants for ﬁve days and consists mainly of laboratory ses-
sions, where the usage of Advanced ROS Packages and simulation is explained.
The hardware deployed in this course is the well-known Turtlebot system.
Another ROS training is held in Brazil, Sao Paulo, Centro Universitario
FEI, but details about the number of participants are missing. The course takes
ﬁve days and is structured in a ROS introduction, some basics, higher-level
applications for mobile robotics. Lectures and lab sessions are oﬀered but the
goal is to have a more practical approach to ROS. It was held for the ﬁrst time. In
Austria, Maribor, TEDUSAR a ROS course is held with less than 20 participants
for ﬁve days, focusing on RoboCup Rescue. This course concentrates on a ROS
introduction, basics and higher-level applications for mobile rescue robotics. It
is held for the 3rd time and consists of lectures and lab sessions. Another ROS
course takes place in Australia, Kioloa, Australian Centre for Robotic Vision
where the number of participants and the contents is not known. It seems to
focus on ROS Vision. The list of summer schools shows that there are only a
couple of ROS training facilities world-wide. The possibilities are increasing, but
ROS Industrial – or even components like MoveIt! – are not yet covered by the
mentioned courses.
5
Scope of Education Activities Within ROSIN
One of the main objectives of the ROSIN project is to kick-start industrial robot
applications based on the ROS middleware. With the aforementioned FTPs,
interested developers and industrial partners could start interesting development
projects. Another measure in ROSIN to facilitate ROS Industrial applications
are educational activities.
5.1
Overview
In order to strengthen the ROS Industrial community it is also of utmost
importance to educate and train current and future ROS-I software developers.
6 http://rosindustrial.org/news/2016/12/16/ric-europe-event-recap-part-1-ros-industrial-training
-and-conference-2016

376
A. Ferrein et al.
Therefore, the ROSIN Education programme targets diﬀerent end-user groups.
These are, in particular, students and professional developers but also members
of the broader ROS/ROS Industrial community.
For each of the end-user groups, the ROSIN Education programme provides
speciﬁc training and education activities tailored to the needs of that particular
group. Hence, the ROSIN education initiative includes the following diﬀerent
activities:
– ROS-I School oﬀers Classes for students.
– ROS-I Academy oﬀers Trainings for (industry) professionals.
– ROS-I MOOCs oﬀers Online Courses for diﬀerent stakeholders.
– ROS-I Market is a Central Hub where stakeholders can meet.
For the diﬀerent end-users (students, professionals, community) tailor-made
education activities exist which aim at optimally training the respective user
group. The education programme for professionals also foresees some form of
an accepted certiﬁcate which states the profound knowledge of the certiﬁcate
holder in particular ROS-I topics. For self-studies, online courses will be provided
(ROS-I MOOCs). Important for the success of the ROSIN project (FTPs as
well as educational activities) will be to bring together people from academia
and industry to work together on related ROS-I projects. For this purpose, the
foundation of the educational activities will be the ROS-I Market, which will
be a brokering platform where stakeholders from both sides can ﬁnd respective
peers. In the following, we describe the diﬀerent activities in greater detail.
5.2
Education Measures
The ROSIN education concept is based on three main measures and a supple-
mentary activities. A structural overview is depicted in Fig. 2a.
ROS-I School. The ROS-I School addresses university students and young pro-
fessionals to get an entry to the ROS Industrial eco-system. In regularly held
summer schools, participants could enroll for a one week teaching activity. The
training will give good insights into currently used ROS-I software tools such as
Moveit! or Decartes. As the activity aims at a very intense training, the number
of admitted students per training event is rather limited. Therefore, the activ-
ities will be held several times per year to give a large number of interested
students and young professionals the opportunity to participate in this educa-
tional activity. Further, we will also oﬀer workshops and summer schools with
varying topics. These are not only limited to teaching particular ROS-I compo-
nents and tools, but will also allow to address how good software engineering
works in order to improve the quality of future ROS-I components.
ROS-I Academy. As a second activity, the ROS-I Academy aims at establishing
a ROS-I certiﬁed engineer programme to certify certain skills within the ROS
Industrial software engineering eco-system, similar to MSE or CISCO certiﬁed

ROSIN Education
377
engineers. The certiﬁed skills comprise basic knowledge in ROS Industrial, as
taught in the ROS-I Schools activity, skills in code review and specialised ROS-I
topics inspired by FTPs or based on diﬀerent robot platform programming skills.
In close collaboration with industrial parties, the contents for the certiﬁed ROS-I
engineering programmes will be selected. Further, the course will be accredited
by some oﬃcial relevant body in some form to ensure that the required skills
will be taught and quality standards of the training are met. The number of
participants per course will also be limited to ensure that each engineer gets a
good hands-on experience with real hardware and gain good insights into ROS-
I components. Also, a course concept where courses with diﬀerent topics that
build upon each other will be oﬀered.
ROS-I MOOCs. To support the other two education activities, the ROS-I
MOOC’s objective is (1) to develop Massive Open Online Courses and (2) to
provide a learning platform. Each participant of the ROS-I Schools or the ROS-I
Academy could deepen their knowledge with this type of online courses in addi-
tion to presence courses held in ROS-I Schools or ROS-I Academies. Here, a
number of diﬀerent up-to-date topics will be presented ranging from ROS-I core
over software engineering and software quality assurance topics (cf. Sect. 3.3) to
single ROS-I components.
ROS-I Market. Finally, the ROS-I Market activity aims at providing a common
internet platform to bring together the interested stakeholders of ROS-I. The
objectives are to provide a brokerage platform, where industrial partners could
post their needs for staﬀor development actions, developers could oﬀer their
expertise and students will be provided with internships at industrial or academic
ROS Industrial stakeholders. The ROSIN Education Board will try and make
matches between oﬀers and demands from the diﬀerent stakeholder’s sides.
5.3
The Education Cube
The main educational activities are structured along three dimensions: topic,
level of proﬁciency, and type of activity. We give a description of each dimension
in the following.
Topic. The topics oﬀered in ROSIN education measures are oriented towards
industry needs. The ﬁrst and foremost industrial applications are dealing with
robotic manipulation. A second area then is mobility, dealing with mobile robots
moving around an industrial site. Finally, the combination of these two forms the
third area mobile manipulation where a robotic manipulator is moving around
on a mobile platform.
Level. Each topic can be dealt with at diﬀerent levels of proﬁciency. Beginners
will start at a basic level, moving via intermediate proﬁciency to an advanced
level. The levels will be oriented along the SOLO taxonmy, where basic corre-
sponds to SOLO 2, intermediate to SOLO 3, and advanced to SOLO 4.

378
A. Ferrein et al.
Type. Depending on the particular target group, we organise education in three
bodies. The ROS-I School provides university level Classes for students. The
ROS-I Academy oﬀers vocational Trainings for industry professionals. Finally,
for both groups mentioned above as well as for interested developers from the
(broader) ROS(-I) community, a collection of Online Courses are present in the
ROS-I MOOCs. Since the trainings in the ROS-I Academy and the classes in the
ROS-I School are quite similar, we should and will not keep them too separated.
The structure of the educational activities along the three dimensions
described above can be understood as a cube. The core instatiation of this edu-
cation cube can be found in Fig. 2b.
ROSIN
Education and Exchange
ROSIN Board
ROS-I
School
ROS-I
MOOC
ROS-I
Academy
Class
Online
Course
Training
Students
Professionals
Community
Academia
Industry
ROS-I Market
(a) The ROSIN education concept.
Basic
Intermediate
Advanced
Manipulation
Mobility
Mobile
Manipulation
ROS-I School
ROS-I MOOCs
ROS-I Academy
Basic
Manipulation
Basic
Mobility
Basic
Mobile
Manipulation
Intermediate
Manipulation
Intermediate
Mobility
Intermediate
Mobile
Manipulation
Advanced
Manipulation
Advanced
Mobility
Advanced
Mobile
Manipulation
[topic]
[level]
[type]
Advanced
Classes
Intermed.
Classes
Basic
Classes
Manipulation
Classes
Mobility
Classes
Mobile
Manipulation
Classes
Advanced
MOOCs
Intermed.
MOOCs
Basic
MOOCs
Manipulation
MOOCs
Mobility
MOOCs
Mobile
Manipulation
MOOCs
Advanced
Trainings
Intermed.
Trainings
Basic
Trainings
Manipulation
Trainings
Mobility
Trainings
Mobile
Manipulation
Trainings
(b) The ROSIN education cube.
Fig. 2. The ROSIN education concept and the education cube.
Extending the Cube. The core education cube can be extended along any of
its dimensions.
For the level of proﬁciency, for now we plan for three stages, namely basic,
intermediate, and advanced. At a later point in time, however, a suﬃcient
amount of participants will want to further deepen their understanding. Thus,
introducing an expert layer on top of the existing levels might be appropriate.
This level would then be corresponding to SOLO 5 in the SOLO taxonomy. Also,
not all interested participant will meet the entry requirements of the basic edu-
cation measures. As a consequence, we foresee preparatory courses that bring
those individuals up to speed. While oﬀering such prep courses as a MOOC will
have the maximal coverage, given suﬃcient demand, special classes or trainings
might be oﬀered in the ROS-I School or the ROS-I Academy respectively.
Just as the scope of industrial robotic applications will broaden, so will the
topics of our educational measures. Following industry needs, new topics will
be accomodated and accounted for, such as 3D perception or machine learning.
Besides particular topics stemming for applications, also more general subjects
like continuous integration or software architectures might be included.

ROSIN Education
379
The three types depicted in the core education cube are oriented towards the
primary target groups. Eventually, to reach maximum impact, the educational
measure could/should be franchised. This in turn requires training the instruc-
tors of such franchised educational activities. A ROS-I Teacher Training hence
is a straightforward extension of the core cube.
5.4
Planned Course Contents
The overall training contents will focus on industrial applications. As an exam-
ple, consider a standard pick-and-place scenario, where the individual positions
of single objects are taught and later processed using a position list. This prob-
lem will be solved by using ROS-I standard components. The next step includes
a ﬂexible vision-based approach, where the positions of the objects to manipu-
late are no longer pre-speciﬁed with a teach-in. Instead, they are determined via
standard tools available in the ROS environment such as OpenCV, PCL and the
alvar functionality. This approach enhances the standard industrial pick-and-
place procedure already by using concepts from mobile robotics such as percep-
tion and navigation. The next step includes collision avoidance with alternative
path planning, in order to improve this application beyond the industrial stan-
dard. The basic concept of a collaborating robot will be used to attract the
industrial users to the ROSIN activity. This setup focusses on the use of MoveIt!
in combination with necessary training in setting up a serial kinematic with
URDF, XACRO (XML Macros) and other description formats. Essential mod-
ules like the tf package will be explicitly explained for industrial serial kinematics
which are still the most common robots for industrial use.
Fundamental knowledge of the topics mentioned above should enable the
industrial user to cover numerous non-standard robotic applications using ROS-
I. These applications can be as well supported using the funding possibility
as FTPs. This way, the industrial users will be encouraged to make use of
ROS-I for solving complex robotic tasks with open-source technology instead
of going for proprietary cost intense solutions. The course contents above was
just one example. During the project, we will further develop the concept includ-
ing leading-edge components from successful FTPs. In general, training content
will be aligned with prevalent industrial use cases. Currently, these are very
often pick-and-place applications like sketched above. Another focus in the teach-
ing activities lies on the quality of the developed software components. There-
fore, workshops how to use state-of-the art development tools and methods (cf.
Sect. 3.3) will be part of the initiative. Finally, the performance, stability and
quality of the used software components will be evaluated during the ROSIN
project.
Intended Learning Outcomes. Upon reﬂection within the education board,
we are trying to use the SOLO taxonomy [2]. Let us give an example of the goals
formulated along the so-called Intended Learning Outcomes (ILO). Consider a
basic manipulation training in the ROS-I Academy. The training covers a UR5
application with path planning and collision avoidance. After completing the
training, the participant will be able to

380
A. Ferrein et al.
– deﬁne their robot by using URDF, kinematic chain etc.;
– conﬁgure the robot by setting parameters using the ROS reconﬁgure tools;
– visualize the robot by making use of rviz;
– make it move by deploying MoveIt!
– make it see by integrating additional cameras and recognizing objects using
OpenCV;
– make it clever by combining motion planning and object recognition into an
intelligent high-level control application;
– do something useful with ROS by ﬁnding task speciﬁc solutions to given
application problems.
This was an example for a basic to intermediate ROS-I Academy training.
Following SOLO taxonomy, for each diﬀerent education level diﬀerent ILOs will
be in focus.
5.5
Educating Future Teachers
The ROSIN Education Board will set up a special education activity in order
to educate future ROS-I instructors. To conduct education activities is not
restricted to consortium members of the ROSIN project. The goal is rather
to use the project as a multiplier to interest many other institutions to partici-
pate in ROS-I activities. However, a certain quality level of external education
measures is required. The ROSIN Education board will ensure the quality of
external education measures. This is done by oﬀering special train-the-trainer
courses within the ROSIN education measures.
In this measure, we need to distinguish between teachers and training instruc-
tors. This is because with teachers we can already assume a certain level of
didactical proﬁciency. With instructors for professional trainings that might not
be the case.
5.6
Measures Conducted by the ROSIN Consortium
Within the ROSIN project, a number of education activities will be carried out
by the ROSIN consortium members. The Mobile Autonomous Systems & Cog-
nitive Robotics Institute (MASCOR) will conduct eight editions of the ROS-I
school at their premises in Aachen, Germany. The Fraunhofer Institute for Man-
ufacturing Engineering and Automation (IPA) will oﬀer two kinds of professional
training activities. The ﬁrst one will be a basic training course teaching the fun-
damentals of ROS-I. This activity shall take place every two months. There will
be also more advanced courses every three month. The workshops will take place
either at the IPA training facilities or at MASCOR. Some workshops will also
be held at the consortium member TECNALIA in Spain. The courses will also
be accredited in the course of the ROSIN project. Further, the consortium mem-
bers will set up online courses at an appropriate platform (e.g. EdEx, Moodle,
or Coursera). At a later stage, an additional programme will be dedicated to
train future ROS-I trainers.

ROSIN Education
381
6
Conclusion and Future Work
In this paper, we presented the ROSIN project, in general, and its ROS
Industrial-related robotics education activities, in particular. The ROSIN project
aims to foster the acceptance and quality of ROS Industrial-based solutions
in industry and academia. To this end, the ROSIN project features a set of
educational activities to increase the number of ROS Industrial literate people
among students and professionals alike. Tailored oﬀers for the diﬀerent end-user
groups accommodate for particular interests. In so-called ROS-I Schools, stu-
dents receive training in developing high-quality applications using ROS Indus-
trial. Similarly, the ROS-I Academy aims at training and later also certifying
professionals in developing industrial-strength solutions with ROS Industrial for
industrial applications. ROS-I MOOCS are meant to oﬀer any interested party
improving their ROS Industrial proﬁciency with online training. Finally, the
ROS-I Market oﬀers a central platform for cooperation and exchange between
all stakeholders. The education activities are also open for third-parties who
can propose additional ROS-I education measures and who can apply for funds
through the ROSIN project from the European Commission.
Acknowledgements. The ROSIN project has received funding from the European
Unions Horizon 2020 research and innovation programme under grant agreement No
732287.
References
1. Baillie, J.C., Demaille, A., Nottale, M., Hocquet, Q., Tardieu, S.: The Urbi universal
platform for robotics. In: SIMPAR 2008, Venice, Italy (2008)
2. Biggs, J.B., Collis, K.F.: Evaluating the quality of learning: the SOLO taxonomy
(Structure of the Observed Learning Outcome). Academic Press, New York (1982)
3. Edwards, S., Lewis, C.: ROS-Industrial – applying the Robot Operating System
(ROS) to industrial applications. In: ICRA/ROSCon, St. Paul, Minnesota (2012)
4. Ferrein, A., Kallweit, S., Scholl, I., Reichert, W.: Learning to program mobile robots
in the ros summer school series. In: Proceedings of the 6th International Conference
on Robotics in Education (RIE-15) (2015)
5. Gerkey, B., Vaughan, R.T., Howard, A.: The player/stage project: tools for multi-
robot and distributed sensor systems. In: ICAR 2003, Coimbra, Portugal (2003)
6. Gerkey, B.P., Vaughan, R.T., Støy, K., Howard, A., Sukhatme, G.S., Mataric, M.J.:
Most valuable player: a robot device server for distributed control. In: IROS 2001,
Wailea, Hawaii (2001)
7. Michieletto, S., Tosello, E., Romanelli, F., Ferrara, V., Menegatti, E.: ROS-I inter-
face for COMAU robots. In: Proceedings SIMPAR, pp. 243–254. Springer (2014)
8. Niem¨uller, T., Ferrein, A., Beck, D., Lakemeyer, G.: Design principles of the
component-based robot software framework fawkes. In: Proceedings SIMPAR 2010,
LNCS, vol. 6472, pp. 300–311. Springer (2010)
9. Quigley, M., Conley, K., Gerkey, B.P., Faust, J., Foote, T., Leibs, J., Wheeler, R.,
Ng, A.Y.: ROS: an open-source Robot Operating System. In: ICRA Workshop on
Open Source Software (2009)

Mobile Robots as a Tool to Teach First Year
Engineering Electronics
Pedro Fonseca1(B)
, Paulo Pedreiras1
, and Filipe Silva2
1 IT, Universidade de Aveiro, 3810-193 Aveiro, Portugal
{pf,pbrp}@ua.pt
2 IEETA, Universidade de Aveiro, 3810-193 Aveiro, Portugal
fmsilva@ua.pt
Abstract. Engineering degrees require a strong background in Physical
Sciences and Mathematics, demanding a high level of conceptualization
and abstract reasoning that many students do not possess at the entry
level of their high education studies. This can cause students demotiva-
tion and dropout, a situation that Higher Education institutions have
felt the need to cope with. One methodology to address this problem is
to introduce the use of robots in the classes. This tool has unique char-
acteristics that may potentially contribute to increase students’ motiva-
tion and engagement, which are key factors on their academic success.
This paper presents the rationale, challenges and methodology used to
introduce robots as a tool to teach introductory electronics to ﬁrst year
students in a Electronics and Telecommunications Engineering Masters
degree. The paper also reports evaluation indicators that result from
two diﬀerent surveys, one generic, carried out in the scope of the Quality
Assurance System of the University, and another one developed specif-
ically to evaluate the course. The results conﬁrm that there is a clear
and overall positive impact. Particularly signiﬁcant are the gains on the
students motivation and subject comprehension, without a noticeable
impact on the course diﬃculty and required eﬀort. It is also specially
relevant that students are strongly in favour of keeping robot’s usage
due to its impact on both knowledge and motivation.
Keywords: Undergraduate/First
year ·
Engineering
curriculum ·
Learning environment/laboratory · Qualitative - 1. Case study ·
Motivation
1
Introduction
For many students, the starting of their studies at a Higher Education (HE) level
can be a challenging task. This is a problem that has been detected worldwide
[4,10,23]. Engineering degrees, as well as other degrees in the STEM (Science,
Technology, Engineering and Mathematics) area, are particularly prone to these
problems. On the one hand, the required preparation in experimental sciences,
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_32

Mobile Robots as a Tool to Teach First Year Engineering Electronics
383
such as Physics, Chemistry, and Mathematics, demands for a high level of
abstraction for which not all students are prepared to. As a result, many play-
ers in the education ﬁeld have felt the need to develop strategies to tackle with
the problems potentially faced by STEM students. Many of these proposals are
focused on increasing students’ motivation [13,14], as de-motivation and aban-
donment are known to be signiﬁcantly related [8].
The use of robots appears to be one of the most promising strategies for
improving STEM education as advances in technology reduced costs and facili-
tated their use in the classroom. One advantage is the fact that students can more
easily learn abstract concepts and gain a more functional level of understanding
[18]. Another argument for teaching with robots is that they stimulate creative
solutions and problem-solving skills [2]. Robots are built of hardware compo-
nents and software programs, each one of them depending on diﬀerent ﬁelds of
knowledge such as electronics, mechanics and programming. This interdiscipli-
nary nature of robots will inevitably give rise to the learning of diﬀerent subjects
and, consequently, how all parts of a complex system interact and depend upon
each other [2].
Case studies found in the literature positively document the use of robotics to
teach diﬀerent subjects [6,19,20] to a wide range of age groups [3,22]. In what
concerns HE, most studies illustrate the potential eﬀectiveness of robotics to
positively impact both learning and student motivation [1,15,21]. At the same
time, engaging students to active learning facilitates deeper understanding of
content knowledge and promotes interest in STEM subjects [5,11,16]. Further,
robotics promotes the application of the subject matters in a rigorous manner
through experimentation, allowing students to put their learning into action.
From the viewpoint of skills development, the use of robots requires problem
solving, appears to foster cooperative learning and teamwork and encourages
students to creatively solve problems [9,12,17].
The main objective of this paper is to provide a quantitative study that exam-
ines the use of mobile robots for teaching introductory electronics to ﬁrst-year
engineering students. Despite the positive motivational beneﬁts of introducing
robots for engineering education suggested by other studies, rigorous quanti-
tative research is missing from the literature in what concerns an Electronics
Curriculum. The results of this study involve a research design based on student
achievement data. In line with this, the remainder of the paper is as follows.
Section 2 presents the robot used in the classes and Sect. 3 presents the course
unit where the robot was used. Section 4 addresses the rationale and methodol-
ogy for obtaining the results, that are presented in Sect. 5, concluding in Sect. 6.
2
The DETI Robot
The DETI robot (Fig. 1) is a small, didactic robot developed at the Department
of Electronics, Telecommunications and Informatics (DETI) of the University of
Aveiro. Two opposed wheels provide diﬀerential driving and a ball caster guaran-
tees the robot stability. Wheels are driven by DC motors coupled to a gear box.

384
P. Fonseca et al.
Fig. 1. DETI robot
The robot is controlled by a board based on the PIC32 microcontroller from
Microchip. A bootloader allows the board to be programmed by connecting the
board to a PC via an USB cable. A C/C++ library with the basic functions to
operate the robot is made available to the students, with the objective of hiding
the microcontroller’s idiosyncrasies and provide a simple interface to the hard-
ware resources. This is important because, at this stage, student’s competences
on computer architectures are still rather limited.
3
The Course at the University of Aveiro
As many other HE institutions, the University of Aveiro found the need to
develop means to motivate new students in order to improve students’ satisfac-
tion and retention and to reduce attrition. In the case of the Electronics and
Telecommunications Engineering degree, a fundamental piece of this strategy
was the development of a new course unit, aimed at motivating ﬁrst-year stu-
dents and introduce them to the ﬁeld of Electronic and Telecommunications
Engineering. This course unit started in 2007, following the introduction of the
Bologna model. This course unit started to be a one-semester, 6 ECTS course
unit, 47260 - “T´ecnicas Laboratoriais em Electrotecnia” (Laboratory Techniques
in Electrotechnology), which, in the academic year 2013–14 was converted into a
two-semester, 8 ECTS course, 40331 - “Laborat´orios de Eletr´onica” (Electronics
Laboratories). We will present here the course outline; the interested reader can
ﬁnd more details in [7].
3.1
Course Structure
The course structure is presented in Fig. 2. In each semester, students have a
block of laboratory assignments, followed by a small project, which lasts for

Mobile Robots as a Tool to Teach First Year Engineering Electronics
385
Fig. 2. Course organization
approximately ﬁve to six weeks and that aims at building a simple electronic
circuit, with a clear and evident application. In the ﬁrst semester, this project
consists on the design of a small, stand-alone device: e.g., a LED that lights
up in the dark, a jig with reference current and voltage sources to test the lab
multimeters. Students’ work starts with the project outline and then proceeds
to design the circuit schematic and the printed circuit board, which is assembled
and tested. The outcome is a fully functional circuit that the students designed
and built and that they can keep.
The second semester follows the same pattern, but now the circuits are not
stand-alone devices, but rather to be applied to the DETI robot. This can be a
sensor, or some other means of interacting with the robot. The students’ tasks
include developing the software to program the robot.
3.2
Robot Projects
The challenge met by the lecturers, when preparing the course, can be described
by trying to fulﬁl the following set of requirements:
– to provide a meaningful development activity, involving some level of engi-
neering design, albeit simple enough to be tackled by ﬁrst year students;
– involve a robot, in order to provide the motivation that comes from working
with an interactive device;
– keep the focus on Electronics.
The solution adopted was to propose a project, based on the DETI robot,
where students are deﬁed to developed an electronic extension module, compris-
ing sensors or some other electronic means of interacting with the robot. When
planning the course activities, care was taken to guarantee that the students’
work is focused on Electronics, as 40331 is a course on Electronics. The robot is
a means to an end: it is seen as a tool for teaching electronics.
In this way, students are expected to follow a set of activities during the
project:
– design the electronics for the sensor circuit, involving simple design activities;
– prototype the circuit on a breadboard and validate its operation;

386
P. Fonseca et al.
– design and assemble the PCB;
– install the sensor; and
– program the robot.
4
Assessing the Utilization of Robots in the Classroom
As a result of the structure of 40331, with a small project at the end of each
semester, students are exposed to design activities in Electronic in two diﬀerent
contexts:
– in the ﬁrst semester, they develop their project as a stand-alone Electronic
device;
– in the second semester, they develop a similar set of activities, but now envolv-
ing the application of their design to the DETI robot.
This means that students taking 40331 experience, in a relatively short time
span, two diﬀerent ways of developing their Electronics project: with and without
the robot. As such, these students are particularly well positioned to assess
the impact of using a robot to promote the study of Electronics, as they have
experienced both situations.
Three sources were used to collect data to assess the impact of the robot use
in the classroom. The ﬁrst, subjective, is the lecturers perception of the students
reaction. This is a course with a three hours long weekly section, most of the time
in lab sessions, which allows for some degree of acquaintance with the students.
The second source is the University’s quality assurance data, including suc-
cess rates and the students survey. This questionnaire addresses several aspects
of every course unit, from student’s motivation and self-assessment to lectur-
ers’ performance, infrastructure conditions and the coordination of the several
activities. Every item assessed in a Likert scale from 1 (worst) to 9 (best). From
the whole survey, the questions in Table 1 were selected for analysis. Questions
addressing issues not directly related to the subject of this work where not con-
sidered for analysis.
Finally, the third source was a questionnaire aimed speciﬁcally at measuring
the robot usage impact. To understand how students perceived the impact of
the robot usage in their motivation and learning, a survey was set up (Table 2).
Questions in group 1 were asked both relative to the course unit and relative
to the robot. This allowed us to estimate the diﬀerence of perception by the
students of the stand alone projects and the robot related projects. The questions
in group 2 aim at understanding how students felt that the robot contributed to
their knowledge and, in group 3, about the value of the robots as a learning tool.
Finally, group 4 investigates whether the students value the robot’s contributions
concerning knowledge gains and motivation. For each question, students were
invited to qualify each of the statements in a Likert scale from “Totally disagree”
(−2) to “Totally agree” (+2). The questionnaire was sent to the 432 students
that took 40331 since 2013–14; from those, 66 responses were received1.
1 The questionnaire is still open at the moment of writing this text.

Mobile Robots as a Tool to Teach First Year Engineering Electronics
387
Table 1. Quality assurance survey questions.
Overall assessment
Q1
Motivation to the course unit
Q2
Satisfaction with own performance
Q12 Overall functioning of course unit
Learning activities
Q7
Coordination of course unit components
Q9
Suitability of proposed activities
Q14 Development of subject comprehension
Q15 Matching between course activities and previous skills
Diﬃculty and eﬀort required
Q16 Course contents diﬃculty
Q17 Level of eﬀort and time required for passing grade
Table 2. Students questionnaire
Group 1 – 40331/robot contributions
40331 unit course/robot project contributed to...
Q 1.1 ... get to know more about Electronics
Q 1.2 ... get to know more about other technical subjects
Q 1.3 ... get more non technical skills (soft skills)
Q 1.4 ... become more motivated toward Electronics
Q 1.5 ... conclude further course units more easily
Group 2 – Application of circuits to the robot
Applying the circuits to the robot. . .
Q 2.1 ... allows for a better understanding of their operation
Q 2.2 ... allows for a better understanding of their application
Q 2.3 ... allows for a greater motivation in their development
Group 3 – Suitability of robot usage
Q 3.1 The robot projects were an eﬀective way of teaching Electronics
Q 3.2 The learning outcomes obtained with the robot could not be easily obtained
otherwise
Group 4 – Continuation of robot usage
If I were responsible for the course, I would keep the usage of robots in classes, due
the the gains in...
Q 4.1 ... knowledge
Q 4.2 ... motivation

388
P. Fonseca et al.
5
Results
5.1
Lecturers Perception
As for the instructors perception goes, the students get quite motivated by the
robot based projects. When the sensor is working and connected to the robot,
and the interaction is possible, it can sometimes be diﬃcult to make the class
come to an end, due to the students involvement in their projects.
The possibility of designing the sensor, not only the circuit but also its phys-
ical realization, tends to spark the creativity in the students. For instance, in
one case, students designed the printed circuit board so that, by soldering the
connector directly to the board, the sensor would be placed in the correct posi-
tion when assembled in the robot, instead of using a standard interface cable
provided to all the groups.
5.2
Course Unit Quality Survey
Figure 3 presents the success rate in 40331, with the results extended to three
years before, referring to the previous, one semester, course unit (47260). It can
be seen that the introduction of the new structure in the academic year 2013–14
resulted in a drop of the success rate, recovering, in the last two years, to values
close to 80%. When 40331 was introduced, there was no previous experience in
lecturing annual course units, therefore this was also for the faculty an year of
learning.
Fig. 3. Success rates
The results concerning the overall assessment, including students self-assess-
ment, are presented in Fig. 4. The eﬀects of introducing the new course, in 2013–
14, are clearly visible, the most notable eﬀect being on the students’ self satis-
faction (Q2). In the following years, all these indicator values recover, with Q1

Mobile Robots as a Tool to Teach First Year Engineering Electronics
389
Fig. 4. Q.A. survey responses
(Motivation to the course unit) and Q12 (Overall functioning of the course unit)
attaining higher values than those achieved with 47260.
A similar pattern is perceived in the responses to the learning activities
(questions 7, 9, 14 and 15). After a drop in 2013–14, all indicators recover.
The improvement from 47260 to 40331 is specially visible in Q9 (Suitability
of proposed activities), Q14 (Development of subject comprehension) and Q15
(Matching between course activities and previous skills), denoting that the stu-
dents indeed appreciated the change and regard it as a positive factor. Finally,
in the last group, and coherently with the previous results, the perceived diﬃ-
culty and required eﬀort to successfully complete the course have a peak in the
academic year 2013–14, reducing in the following years.
5.3
Students Survey
In what concerns the questionnaire speciﬁcally designed to assess the robot use
impact, Fig. 5 presents the results for the mean value of the responses to Group
1 questions, comparing the responses concerning the course unit as a whole
(“LabE”), and the robot projects (“Robot”).
In the set of questions Q 1.1 to Q 1.4, the students responses rate the robot
project contribution with a mean value between 1 (“Partially agree”) and 2
(“Totally agree”). Comparing the course contribution to the robot projects con-
tribution, students rate more highly the course impact in questions Q 1.1 (con-
tribute to know more about Electronics) and Q 1.5 (contribute to complete

390
P. Fonseca et al.
Fig. 5. Responses to group 1.
successfully further course units). On the other hand, they rate the contribution
of robot projects for learning more about other subjects, either technical (Q 1.2)
and non-technical (Q 1.3), and to motivate the students to learn Electronics
higher than the overall course unit contribution.
In what concerns the gains perceived by the students from using the circuits
applied to a robot, the results are presented in Fig. 6. Here, the marks represent
the average response value and the bars are one standard deviation long (positive
and negative). For group 2, the values are clearly positive, between 1.31 and 1.66.
The result that is more positively perceived by the students is related to the
motivation. In what concerns groups 3 and 4, students consider the robot projects
as an eﬀective way of teaching Electronics, with an average value of 1.32. The
responses to the question whether the same learning outcomes could be obtained
employing other means is the one that gets the lowest average score (0.70).
When asked whether students would keep the robots in the classes if they
were in charge, the response is positive, with average values of 1.29 consider-
ing the gains in knowledge and of 1.74 considering the motivational aspects.
Fig. 6. Responses to groups 2 to 4.

Mobile Robots as a Tool to Teach First Year Engineering Electronics
391
Again, and coherent with the responses to Group 2, students value the robot
projects primarily as a motivational tool and recognize its value as a learning
tool.
By considering the responses to Q 1.4, Q 2.3 and Q 4.2, we can conclude
that students, most of all, value the robot projects as a motivational tool. The
gains in knowledge brought by the robot projects are also recognized, as it can
be seen by the responses to Q 1.2, Q 1.3, Q 2,1, Q 2.2 and Q 4.1.
6
Conclusions
Small mobile robots have unique and appealing characteristics that turn them
valuable helper teaching tools on many ﬁelds, among them introductory-level
electronics.
Engineering degrees are known for being particularly challenging for ﬁrst
year students. On the other hand, motivation and engagement are key factors to
increase student’s academic success. Therefore, taking the opportunity open by
a course restructuring carried out in 2013–2014, a robot-based project was intro-
duced on the Electronics Laboratories course of the Masters course in Electronics
and Telecommunications Engineering at the University of Aveiro.
A thorough evaluation of the impact of the robot’s introduction on the stu-
dents performance was carried out. The evaluation targeted several axes, such as
motivation, acquisition of technical competences, required eﬀort and impact and
articulation with other courses. The survey results are, overall, very positive. In
particular, the impact on students motivation and on the subject comprehension,
without implying more eﬀort nor increasing the course diﬃculty, are particularly
relevant. Therefore, it can be concluded that robotics can be an eﬀective tool
to improve the academic performance and satisfaction of electronics ﬁrst year
students. The experience also allowed the authors to gain insight about the rele-
vance of operational aspects. In particular, providing customized tools, tailored
to hide the idiosyncrasies of the robots’ hardware, and so expose simple and
concise interfaces, contributed decisively to keep the student’s focus on the core
course components and limit the complexity to acceptable levels. Moreover, the
broad range of scientiﬁc domains involved in the course is also challenging for
the faculty staﬀ, which usually tend to have relatively narrow areas of expertise,
typically closely related with their research areas.
Acknowledgment. The authors would like to acknowledge the work of Prof. Jos´e
Lu´ıs Azevedo, the main developer of DETI robot for his work in the development of
tools to promote robotics at student level.
References
1. Aroca, R.V., Watanabe, F.Y., Avila, M.T.D., Hernandes, A.C.: Mobile robot-
ics integration in introductory undergraduate engineering courses, pp. 139–144.
IEEE (2016). https://doi.org/10.1109/LARS-SBR.2016.30. http://ieeexplore.ieee.
org/document/7783516/

392
P. Fonseca et al.
2. Beer, R.D., Chiel, H.J., Drushel, R.F.: Using autonomous robotics to teach science
and engineering. Commun. ACM 42(6), 85–92 (1999). https://doi.org/10.1145/
303849.303866. http://portal.acm.org/citation.cfm?doid=303849.303866
3. Benitti,
F.B.V.:
Exploring
the
educational
potential
of
robot-
ics
in
schools:
a
systematic
review.
Comput.
Educ.
58(3),
978–988
(2012).
https://doi.org/10.1016/j.compedu.2011.10.006.
http://linkinghub.elsevier.com/retrieve/pii/S0360131511002508
4. Berzonsky,
M.D.,
Kuk,
L.S.:
Identity
status,
identity
processing
style,
and
the
transition
to
university.
J.
Adolesc.
Res.
15(1),
81–98
(2000).
https://doi.org/10.1177/0743558400151005
5. Eguchi, A.: RoboCupJunior for promoting STEM education, 21st century
skills, and technological advancement through robotics competition. Robot.
Autonom. Syst. 75, 692–699 (2016). https://doi.org/10.1016/j.robot.2015.05.013.
http://linkinghub.elsevier.com/retrieve/pii/S0921889015001281
6. Fagin, B., Merkle, L.: Measuring the eﬀectiveness of robots in teaching computer
science. In: SIGCSE 2003 Proceedings of the 34th SIGCSE Technical Symposium
on Computer Science Education, p. 307. ACM Press, Reno (2003). https://doi.org/
10.1145/611892.611994. http://portal.acm.org/citation.cfm?doid=611892.611994
7. Fonseca, P., Pedreiras, P., Cabral, P., Cunha, B., Silva, F., Matos, J.N.: Moti-
vating ﬁrst year students for an engineering degree. In: CISPEE 2016 – 2nd
International Conference of the Portuguese Society for Education in Engineer-
ing, Vila Real, Portugal (2016). https://doi.org/10.1109/CISPEE.2016.7777745.
http://ieeexplore.ieee.org/document/7777745/
8. French, B.F., Immekus, J.C., Oakes, W.C.: An examination of indicators of engi-
neering students’ success and persistence. J. Eng. Educ. 94(4), 419–425 (2005).
https://doi.org/10.1002/j.2168-9830.2005.tb00869.x
9. Greenwald, L., Kopena, J.: Mobile robot labs. IEEE Robot. Automat. Mag. 10(2),
25–32 (2003). https://doi.org/10.1109/MRA.2003.1213613
10. Kantanis, T.: The role of social transition in students’: adjustment to the ﬁrst-year
of university. J. Inst. Res. 9(1), 100–110 (2000)
11. Kim, C., Kim, D., Yuan, J., Hill, R.B., Doshi, P., Thai, C.N.: Robotics to promote
elementary education pre-service teachers’ STEM engagement, learning, and teach-
ing. Comput. Educ. 91, 14–31 (2015). https://doi.org/10.1016/j.compedu.2015.08.
005. http://linkinghub.elsevier.com/retrieve/pii/S0360131515300257
12. Lalonde, J., Bartley, C., Nourbakhsh, I.: Mobile robot programming in educa-
tion, pp. 345–350. IEEE (2006). https://doi.org/10.1109/ROBOT.2006.1641735.
http://ieeexplore.ieee.org/document/1641735/
13. Linnenbrink, E.A., Pintrich, P.R.: Motivation as an enabler for academic success.
Sch. Psychol. Rev. 31(3), 313 (2002)
14. Lynch, D.J.: Motivational factors, learning strategies and resource management as
predictors of course grades. Coll. Student J. 40(2), 423–428 (2006)
15. McGill, M.M.: Learning to program with personal robots: inﬂuences on student
motivation. ACM Trans. Comput. Educ. 12(1), 1–32 (2012). https://doi.org/10.
1145/2133797.2133801. http://dl.acm.org/citation.cfm?doid=2133797.2133801
16. McLurkin, J., Rykowski, J., John, M., Kaseman, Q., Lynch, A.J.: Using multi-robot
systems for engineering education: teaching and outreach with large numbers of
an advanced, low-cost robot. IEEE Trans. Educ. 56(1), 24–33 (2013). https://doi.
org/10.1109/TE.2012.2222646. http://ieeexplore.ieee.org/document/6363493/
17. Mirats Tur, J., Pfeiﬀer, C.: Mobile robot design in education. IEEE Robot.
Autom. Mag. 13(1), 69–75 (2006). https://doi.org/10.1109/MRA.2006.1598055.
http://ieeexplore.ieee.org/document/1598055/

Mobile Robots as a Tool to Teach First Year Engineering Electronics
393
18. Nourbakhsh, I.R., Crowley, K., Bhave, A., Hamner, E., Hsiu, T., Perez-Bergquist,
A., Richards, S., Wilkinson, K.: The robotic autonomy mobile robotics course:
robot design, curriculum design and educational assessment. Autonom. Robots
18(1), 103–127 (2005). https://doi.org/10.1023/B:AURO.0000047303.20624.02.
http://link.springer.com/10.1023/B:AURO.0000047303.20624.02
19. Oliver, J., Toledo, R.: On the use of robots in a PBL in the ﬁrst year of computer
science/computer engineering studies. In: Global Engineering Education Confer-
ence (EDUCON), pp. 1–6. IEEE (2012). https://doi.org/10.1109/EDUCON.2012.
6201026. http://ieeexplore.ieee.org/document/6201026/
20. Ortiz, O.O., Pastor Franco, J.A., Alcover Garau, P.M., Herrero Martin, R.: Inno-
vative mobile robot method: improving the learning of programming languages in
engineering degrees. IEEE Trans. Educ. 60(2), 143–148 (2017). https://doi.org/
10.1109/TE.2016.2608779. http://ieeexplore.ieee.org/document/7582486/
21. Sakata Jr., K., Olguin, G.S.: Robotics: a case study of contextualization in engi-
neering education. In: WEE2011 - 1st World Engineering Education Flash Week,
Lisbon, Portugal (2011)
22. Spolaˆor,
N.,
Benitti,
F.B.:
Robotics
applications
grounded
in
learn-
ing
theories
on
tertiary
education:
a
systematic
review.
Comput.
Educ.
112,
97–107
(2017).
https://doi.org/10.1016/j.compedu.2017.05.001.
http://linkinghub.elsevier.com/retrieve/pii/S0360131517300970
23. Tao, S., Dong, Q., Pratt, M.W., Hunsberger, B., Pancer, S.M.: Social support:
relations to coping and adjustment during the transition to University in the Peo-
ple’s Republic of China. J. Adolesc. Res. 15(1), 123–144 (2000). https://doi.org/
10.1177/0743558400151007

Methodology and Results on Teaching Maths
Using Mobile Robots
Paola Ferrarelli1(B), Tamara Lapucci2, and Luca Iocchi1
1 DIAG, Sapienza University of Rome, Rome, Italy
ferrarelli@diag.uniroma1.it
2 Department of Advanced Research, Clementoni Spa, Recanati, Italy
Abstract. In 58 Italian Public Comprehensive Institutes (Istituti Com-
prensivi), that include Primary and Elementary schools, 2911 students
experimented the use of a mobile robot, Sapientino Doc by Clementoni,
to learn curricula matters such as Mathematics, Geometry and Geog-
raphy (MGG). The project “A scuola di coding con Sapientino” was
developed during the 2016/2017 regular school year for about 3 months
(April–June 2017). The schools were distributed throughout Italy and
involved 2911 students from 5 to 8 years old, 155 classes, and 163 teach-
ers. The aim of the research is to demonstrate a learning gain in Mathe-
matics, Geometry and Geography, after the students use a mobile robot
during regular lessons held by their own teachers in their classrooms. In
this paper, we present the methodology used to develop the project and
the results of data analysis.
1
Introduction
A survey on the use of robots in educational ﬁeld was made by Benitti [2], who
reviewed the international literature on Educational Robotic (ER) published
over 10 years listing robots, students age and obtained results, and by Mubin [8]
who gives an overview on robot kits, robot roles and robot usage domains. About
robot kits, a recent study was conducted by Garcia [10] by listing characteristics
of robot kits and apps, currently available on the market for teachers of 4–14 age
students, and costs, that for Mondada [7] is one of the obstacles to the massive
use of robots in the schools. From the literature analysis, it emerges that many
of ER activities are extra-curricular, during summer camps or workshops. Few
studies show quantitative data, using statistical methods, or a controlled group to
compare the results of the test group, or a random choice of the users belonging
to each group. Few empirical data are available on the use of low cost robots since
in 90% Lego R
⃝robots are used. Few studies analyze the use of robots to teach
subjects diﬀerent from computer science or mechatronics. Moreover, the use of
robots in a class is not the only favorable condition for learning, but also: the
presence of the teacher, the spaces suitable to do the activities with the robot,
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_33

Methodology and Results on Teaching Maths Using Mobile Robots
395
the availability of one kit for each team of 2–3 students, short theory lessons
and tutorials to link theory and practice, realistic but aﬀordable tasks linked
with curricular subjects, teachers at ease with the robot, etc. So ER researchers
and teachers need to choose the more suitable robot kit for their students, and
carefully design where and how to use it and with which role. The analysis of
the interviews to 85 students by Shin [11] highlighted that young students prefer
a robot that acts as a peer during the learning process. In the ER research ﬁeld,
studies with students 5–12 years old have been conducted using diﬀerent robot
kits. For example, Bee-Bot was used by many authors, including: Highﬁeld [5]
with 33 children aged 3–5 years in a Primary school in a 12 weeks project for
2 h each lesson; Eck [4] with 4–5 years old children, 1 h a week for 6 weeks; Di
Lieto [3], on a sample of 12 children aged 5–6 years, 75 min twice a week for
6 weeks. Lego R
⃝WeDo was used by Kazakoﬀ, Sullivan and Bers [6] during a 10 h
intensive course in the robotic week with 29 children. Lego R
⃝Mindstorms was
used by Barker and Ansorge [1], in a post school program with 32 students aged
9–11 years, 1 h twice a week for 6 weeks, and by Zygouris [13] to teach geometry
concepts to 12 years old. Thymio was used by Friebroon and Ben-Ari [12] with
thirty 7–8 years old students of Elementary school, while Nulli and Di Stasio [9]
used Cubetto with 5–6 years old kids of Primary School during a school year.
Our research was conducted using the mobile robot Doc, during the project
“A scuola di coding con Sapientino”, with almost 3,000 5–8 years old students,
during the regular school lessons.
Fig. 1. Educational activity organization: (left) ﬁlling the questionnaire, (right) learn-
ing with robot Doc.
The project “A scuola di coding con Sapientino”1 was promoted by three dif-
ferent subjects: Clementoni, an Italian company leader in educational toys (the
Sapientino DOC inventor), DIAG Dept. (Department of Computer, Control, and
Management Engineering) at Sapienza University of Rome, and MIUR (The Ital-
ian Ministry of Education, University and Research). The project was born with
the main shared purpose to bring in the classrooms an easy-to-use and fun tool
1 Web site of the project with additional information, material and results: https://
sites.google.com/a/dis.uniroma1.it/doc-scuola/attivita.

396
P. Ferrarelli et al.
(the robot Sapientino Doc2 by Clementoni) to introduce the basic concepts of
programming and robotics. The educational activities with the robot (see Fig. 1
right) were designed to learn main concepts of Maths, Geometry, Geography
(MGG) measuring the learning gain through questionnaires (see Fig. 1 left).
2
Methodology
The methodology that we used for this project is illustrated below:
1. identify a robot that ﬁts Primary and Elementary schools needs
2. identify the curricula subjects in which we want to analyze the learning gain
after using the robot
3. write the educational activities within a scenario
4. write the initial and ﬁnal evaluation questionnaires to be distributed to stu-
dents for the assessment of the learning gain
5. Phase A: test the educational sessions and the teaching materials in few
sample schools and reﬁne the material before the national distribution
6. Phase B: run the educational sessions on a large distribution of Italian Pub-
lic Comprehensive Schools and collect the evaluation questionnaires data
through an on-line web system
7. analyze data and publish the results
Due to time and project constraints, we could not arrange a controlled exper-
iment to do a comparison with another method of teaching MGG. This further
experimental modality is planned as future work.
Table 1. Parameters of our experiment.
Number of students
2911
Where
intra-curricular activity inside the
classroom, during the regular lesson time
Robot role
tool to engage the students to learn
Teacher role
transfer base knowledge
Subjects
Maths, Geometry, Geography (MGG)
Background knowledge required none for students; provided teaching
material for teachers
Robot cost
low cost product, aﬀordable for public
schools
Table 1 summarizes the characteristic of the project in terms of number of
participants, teacher role, robot role and cost, domain and location of the learn-
ing activities, that are fundamental parameters to describe the experiment, as
underlined by [8].
2 http://www.clementoni.com/en/61323-doc-educational-smart-robot/.

Methodology and Results on Teaching Maths Using Mobile Robots
397
2.1
Mobile Robot Doc
Our research was conducted using the mobile robot Doc, even if the methodol-
ogy can be adopted using mobile robots with the same characteristics of Doc.
Doc is an easy programmable mobile robot. It is tall 12 cm and it looks like
an astronaut with a visor, see Fig. 2 (left). When Doc is switched on, 2 circles
illuminate the visor for simulating the eyes, so one can easily identify the front
of the robot. It is equipped with 2 driving wheels about 8 cm apart, with no
odometers and sensors, it has a step of 13 cm and it moves at a speed of about
11.5 cm/s. Doc can move linearly or turn itself, on the spot, by 90◦. It does not
have movable arms or manipulators. The keyboard for its programming is placed
on its head and is composed by 7 touch buttons through which it is possible to
program Doc to move a step forward or backward, to turn 90◦to the right or left,
to reproduce a fun sound eﬀect, to cancel the moves entered so far from robot’s
memory, and to make Doc undertake the command sequence just entered (see
Fig. 2 left bottom). There are three playing modes: Free, Game and Edu, that
can be selected by moving the selector behind robot head (see Fig. 2 left top).
In particular, using the Free mode, the robot is free to move around, either on
its puzzle mat or on any other clean and smooth surface. The strong quality of
the product is that the user can enter a sequence of commands and execute it
with one touch. The robot can interact with the user through a set of predeﬁned
sentences at the beginning of the activity, with an introductory message to invite
the user to start or at the end, to give feedback, or in the meanwhile, in case
of lack of activity, to catch the user’s attention, before to eventually freeze. Its
energy is supplied by 3 batteries 1.5 V AA alkaline, which guarantees continu-
ous use of the robot for 5–7 h. The Sapientino Doc product box contains also
rectangular double face puzzle mat of dimensions 91 cm × 65 cm, that recreates
a grid with boxes that adapt to Doc’s paces (13 cm). The analysis of the prod-
uct has highlighted several educational strengths among which the teaching of
mathematics, geography (orientation, points of view, directions) and geometry
(open and closed lines, geometric shapes) concepts. Scenarios and educational
activities were therefore designed according to these basic concepts, using the
robot in Free mode. We also designed an evaluation questionnaire for each sce-
nario, to analyze the student improvements after using the robot, and a feedback
Fig. 2. Left: Robot Doc with its keyboard and modes selector. Right: Educational
activity: commutative properties of the sum on the line of numbers.

398
P. Ferrarelli et al.
questionnaire for the teacher to evaluate the student experience in terms of inter-
est, motivation, active participation, fun, cooperation, frustration, rule respect,
anxiety, self-organization.
2.2
Experimental Design
The hypothesis we want to test with our research is that using a mobile robot
enables a learning gain in MGG with additional beneﬁts of being an amusement
experience and of facing and learning a new technology. We measure the learning
gain by evaluating the score of the initial and the ﬁnal evaluation questionnaires,
that consist in eight questions about MGG. The experimental method adopted
a within-subject design (repeated measures). In fact, each student executed the
questionnaires under two diﬀerent conditions: before using the robot (initial
questionnaire), after using the robot (ﬁnal questionnaire). The null hypothesis
is that there is no diﬀerence in the questionnaire scores.
2.3
Test of the Teaching Material (Phase A)
Five classes were selected for phase A of the project. Each class received 4 boxes
of Sapientino Doc, i.e. 4 robots, batteries, a grid board with white squares and
a ﬁrst version of the teaching material, which included the description of the
didactic activities (scenarios) and the evaluation questionnaires for the students.
During phase A, teachers were trained by Sapienza and Clementoni researchers,
through a meeting at school, without the students, at a time immediately before
the activities, and they chose two scenarios to be executed in the classroom.
After, the teachers, supported by the researchers, administered the initial eval-
uation questionnaire, managed the educational activities and administered the
ﬁnal evaluation questionnaire. During these educational sessions the researchers
could test each scenario, train the teachers, receive their comments and sugges-
tions on scenarios, questionnaires and designed acquisition tools. Researchers
also measured the time that took to do an educational session, that consists in
the execution of two scenarios. It takes 30 min. to ﬁll the initial evaluation ques-
tionnaires, about 2 h to execute the activity with the robot and 30 min. to ﬁll the
ﬁnal evaluation questionnaire. At the end of phase A, we improved the teaching
material and created the project website for teachers’ on-line training, with video
demonstration of educational activities, a community, a contacts section and a
frequent asked questions page. We also added the Edu and Game scenarios for
Primary schools, we created one more scenario and questionnaire for the ﬁrst
Elementary classes and added a feedback questionnaire for the teachers. Finally,
the proposed scenarios and the questionnaires associated with them are shown
in Table 2. In order to improve the questionnaire aesthetics and the compilation
experience, their ﬁnal version was done by Clementoni’s graphic staﬀand col-
ored printed. All this material (in Italian) is available in the already reference
project web site.

Methodology and Results on Teaching Maths Using Mobile Robots
399
Table 2. Scenarios and questionnaires for Primary and Elementary. The X value
indicate the association between them.
Scenarios
Questionnaires
Mathematics 1 Mathematics 2 Geography Geometry Primary
Mathematics 1 X
X
Mathematics 2
X
Geography
X
X
Geometry
X
X
Storytelling
X
X
X
Edu
X
X
X
Game
X
X
X
2.4
Run of the Educational Sessions at National Level (Phase B)
As already mentioned, a total of 58 Italian Public Comprehensive Institutes
(Istituti Comprensivi), including 155 Primary and Elementary classes, adhered
to the project on a voluntary basis. The schools were distributed throughout
Italy, involving 60% of Italian regions (see Fig. 3), 2,911 students from 5 to 8
years old, 163 teachers and about 600 robots.
Fig. 3. Geographic distribution of schools participating to the project.
As in Phase A, each class received 4 boxes of Sapientino Doc, i.e. 4 robots, bat-
teries, a grid board with white squares and the ﬁnal version of the teaching mate-
rial, which included the description of the scenarios and the evaluation question-
naires for the students. During Phase B, teachers were trained on-line. They chose

400
P. Ferrarelli et al.
two scenarios and executed their activities autonomously (i.e., without physical
presence of Clementoni or Sapienza researchers). Finally, they collected the evalu-
ation questionnaire data and ﬁlled in their on-line feedback questionnaire, through
the website adhered to the project on a voluntary basis (Table 3).
Table 3. Number of classes and questionnaires involved in the project, divided by school
grade.
Number of
Total Primary 5y.o. I Elem. 6y.o. II Elem. 7y.o. III Elem. 8y.o.
Classes
155
38
58
52
7
Questionnaires 4926
677
1938
2056
255
2.5
Questionnaires: Evaluation and Feedback
We created three sets of questionnaires: two paper evaluation questionnaires for
students and one on-line feedback questionnaire for teachers. An initial evaluation
questionnaire is ﬁlled by the students before the execution of the activities with the
robot, and a ﬁnal one is ﬁlled after the activities with the robot. Both are supposed
to be ﬁlled by the students in the classroom, just before and after the activities
with the robot. Teachers could read the questions to the Primary and ﬁrst Ele-
mentary students, if needed. The questions in the ﬁnal questionnaire are slightly
diﬀerent from the ones in the initial, but the diﬃculty level is the same. In fact,
during Phase A, we observed that with a ﬁnal questionnaire equal to the initial
one, the students took less time to ﬁll the ﬁnal questionnaire because they wrote
automatically the same answer of the initial one, without any cognitive eﬀort on
the question. The questionnaires regard Mathematics, Geometry or Geography
subjects and aim at evaluating students capacity to answer eight questions for
each scenario. For Primary school students, that cannot neither read nor write, we
created ad-hoc questionnaires with three questions, where students have to color
their answers. The protagonist of the initial questionnaire is a frog, Zap, while
in the ﬁnal one there is a robot, Doc, in order to create a cognitive link with the
performed activities. We collate together the initial and the ﬁnal questionnaires
because we want to be sure that they refer to the same student, in order to relate
individual performance before and after the teaching activity. The student can
use pencil, rubber and must ﬁll with the appropriate answer a red box near the
question.
For example, in the Mathematics 1 questionnaire, the ﬁrst two questions ask
how many steps will do Zap/Doc to reach a ﬂag, being the step equal to one unit
in Question 1 and two units in Question 2; Question 3 asks how many steps will
do Zap/Doc to reach the pool goggles, Question 4 to reach the swimming pool
and Question 5 asks for the total number of steps done. To complete Questions 6
and 7, the student must read a short story and ﬁll the gaps with appropriate given

Methodology and Results on Teaching Maths Using Mobile Robots
401
numbers. Finally, in Question 8, s/he must write which is the longest route made
by Zap/Doc to reach its home.
The on-line feedback questionnaire for the teachers have multiple goals: (1) col-
lect the student answers to the evaluation questionnaires; (2) collect the feeling of
the teachers about the student experience using the robot, in terms of interest,
motivation, active participation, fun, cooperation, frustration, rule respect, anx-
iety, self-organization; (3) collect the teachers feedback about coding tools.
2.6
Educational Activity Organization: Teachers and Teams
The teacher was free to decide which scenarios to execute during the educational
session, depending on his/her didactic objective. We asked to run 2 scenarios in
the same day, in order to minimize the bias eﬀect of the evaluation questionnaires
and the loss of data due to the absence of students. Moreover, we suggested to
the teachers to spend some time, in the days before the educational session and
without the students, to become familiar with the robot keyboard, modes and
actions. As already mentioned, at the beginning, the students ﬁlled in the initial
evaluation questionnaire associated with the chosen scenario (see Fig. 1 left), then
they were divided in teams of 4–5 students, in order to execute the activities of the
scenarios, using the robot (see Fig. 1 right). Each team could use one robot and a
billboard (the white one or the colored one). At the completion of the activities,
the students ﬁlled in the ﬁnal evaluation questionnaire. The teacher could ﬁll in
the feedback questionnaire in a diﬀerent day.
2.7
Scenarios
The design of educational scenarios took place alongside the analysis of robot Doc
and analyzing similar activities that took place in similar domains (Thymio3, Bee-
Bot4, MARRtino Sapienza mobile robot5). At the beginning, we wrote the follow-
ing 4 diﬀerent scenarios and the related evaluation questionnaires: (1) Mathemat-
ics, (2) Geography, (3) Geometry, (4) Storytelling.
After Phase A, we introduced a simpliﬁed Mathematics scenario for ﬁrst
elementary classes, splitting Mathematics in two scenarios: Mathematics 1 and
Mathematics 2. Moreover, we introduced Edu and Game scenarios to address the
needs of Primary students.
Mathematics 1 scenario includes a number of didactic activities related to the
baseline calculation on the line of numbers. The activities take place with the Doc
robot set on a grid board with numbers. Students have to program linear opened
routes. The didactic objective is the veriﬁcation on the line of numbers of sim-
ple math operations such as sum, subtraction and multiplication. The methodol-
ogy includes students programming the robot to move on the line of numbers and
checking the results of the maths operations. Figure 2 (right) shows an example
3 https://www.thymio.org/en:thymio.
4 https://www.bee-bot.us/.
5 http://tinyurl.com/marrtino.

402
P. Ferrarelli et al.
of an activity to teach the commutative property of the sum: (i) put the robot on
the blank mat at the initial position (number 0); (ii) program robot A to move for-
ward for 2 steps, turn on the right to watch the reached number, turn on the left
and move forward for 3 steps, turn on the right to watch the ﬁnal number position
(number 5): students must record the robot A ﬁnal number position; (iii) program
robot B to ﬁrst move forward 3 steps and then 2 steps: students must record the
ﬁnal number position of robot B and compare it with the ﬁnal number position of
robot A.
Mathematics 2 scenario diﬀers from the previous one in the methodology. In
fact, in this scenario the robot is used as a tool to verify the results of opera-
tions mentally calculated previously by the students. Example of activity: stu-
dents record on their notebook the result of 2+3 and 3+2 sums. Then they pro-
gram the robot to verify the correctness of their results.
Geometry scenario includes a number of tasks related to geometry concepts
that take place with the Doc robot on a grid board with white boxes and other
graphical elements. The didactic objectives is the veriﬁcation of the knowledge of
simple ﬂat geometric ﬁgures (square, rectangle and circumference) and the intro-
duction to the perimeter’s calculation. The methodology include students pro-
gramming the robot to move by making a route with a geometric shape. For exam-
ple, program the robot to move along a square route.
Geography scenario includes a series of activities related to geography concepts
that take place with the Doc robot set on a grid board with white boxes and other
graphical elements. The didactic objective is the veriﬁcation of basic elements of
geography such as diﬀerent points of view and reference systems. The method-
ology include students placing the robot on the grid board in an initial position
and orientation and then programming it to reach a ﬁnal position and orientation
being at diﬀerent relative orientations with respect to the robot. For example, the
robot is placed in the left corner of the grid, watching the students team and they
must program it to reach the opposite corner.
Storytelling scenario explore the ability to set and move the robot in scenes or
stories represented on the puzzle mat. It includes a series of activities related to a
story chosen by the teacher. It could be a story that the class has already developed
in recent months (for example going at the theater). It can be implemented with
the white grid board, enriched with the characters of the history (protagonist,
antagonist, aides, obstacles, etc.), or the boards provided by the product.
Edu and Game scenarios are the ones described in the instruction manual of
the product.
3
Data Analysis
Table 4 summarizes the number of classes that executed the questionnaires,
divided by school grade. In this paper we focus on the results of the Mathematics 1
questionnaire (Maths). The results of Primary, Geometry, Geography and Math-
ematics 2 questionnaires are published on the project website mentioned above.
By comparing the initial and the ﬁnal evaluation questionnaires, using sta-
tistical methods, we want to measure if there were improvements in the score

Methodology and Results on Teaching Maths Using Mobile Robots
403
Table 4. Number of classes that executed the questionnaires, divided by school grade.
Number of classes
Questionnaires
Mathematics 1
Mathematics 2
Geography
Geometry
Primary
Primary
0
0
0
0
52
I Elementary
44
2
28
32
0
II Elementary
3
37
36
34
0
III Elementary
1
5
3
5
0
Total
48
44
67
71
52
obtained by the students on each question. Maths questionnaires were executed
by 48 classes involving a total of 886 students (445 female and 441 male).
Global statistics. The ﬁrst analysis presented here considers the score to each
question by all the students. The results are presented in Table 5 showing the per-
centage of correct answers given in each question before and after the educational
activity with the robot, the diﬀerence, and the corresponding p-value of the two
distributions. As shown in the table, most of the results are extremely signiﬁcant,
two results are very signiﬁcant (Questions 4 and 7), one is signiﬁcant (Question
7) and one is not signiﬁcant (Question 8).
Table 5. Percentage of correct answers in 886 Math questionnaires before and after the
educational activity with the robot and p-value of the distribution.
Question Before act. After act. Diﬀerence p-value
Signiﬁcance
1
60.9%
68.5%
7.6%
8.56e-04 ***
2
53.6%
66.3%
12.7%
5.08e-08 ****
3
57.1%
65.5%
8.4%
3.03e-04 ***
4
54.0%
60.3%
6.3%
7.17e-03
**
5
35.0%
43.6%
8.6%
2.14e-04 ***
6
73.9%
80.4%
6.5%
1.25e-03
**
7
71.4%
76.5%
5.1%
1.48e-02
*
8
89.7%
88.1%
−1.6%
2.89e-01
-
From this analysis, it is possible to conclude that the educational activities
with the robot has a very signiﬁcant impact in the increase of score of the Math
questionnaire. The same results are shown in a graphical format in Fig. 4. In the
rest of the paper, we present other analysis only in a graphical format for better
readability. Note that, being the low number of II and III Elementary classes using
this questionnaire, Fig. 4 (left) represents basically I Elementary results.
By looking more speciﬁcally at the score of each student, we found some gen-
eral positive trend after using the robot: 47% of students got a better score in the

404
P. Ferrarelli et al.
ﬁnal evaluation questionnaire and 26% of them got the same score; the percentage
of students who achieved the maximum score (i.e., all correct answers) increased
from 19% to 25%; only a few students did not answer to the questions, but for
Questions 4 and 5 the number of blank answers decreased, while it remained equal
for Questions 2, 6 and 7; all the questions, except Question 5, got more than 50%
of correct answers before the activity, conﬁrmed that the questionnaire is well cal-
ibrated on the students level and this was possible mostly thanks to the teachers
feedback collected during Phase A; the general downward trend of score during
questionnaire time, is probably due to students eﬀort in doing eight Maths ques-
tions. In particular, the last question, Question 8, albeit with low signiﬁcance, kept
a negative diﬀerence for all the schools levels. Breaks should be foreseen during
questionnaire time.
Fig. 4. Percentage of students who gave correct answers to the questions. Left: a com-
parison of the results before and after the activity with the robot, for all the students.
Right: the comparison for III Elementary students only.
We would like to show in Fig. 4 (right), the data related to III Elementary stu-
dents only. While these data are not statistically relevant (only 1 class), we can
observe a highest percentage of correct answers (89% on average), with respect to
the I Elementary students (65% on average) due to the fact that the Mathemat-
ics questionnaire was written for I Elementary students principally, and the high
number of questions with negative diﬀerence (−6% means 1 student) are probably
due to loss of motivation in doing for the second time a trivial questionnaire for
them. Here the lesson learned is that the improvement of the score is also aﬀected
by the way in which the activities to be performed with the robot are designed,
not only the robot itself.
Teacher’s feedback. Analyzing the feedback questionnaires, we can distinguish
three types of teachers involved in the project: the neophytes, or beginners, not
expert in coding; the apprentices, teachers who attended coding courses but hadn’t
experience in classroom; the experts, teachers who had a medium or strong expe-
rience in coding and robotics in classroom. Overall, they all commented that Doc
is used by children with joy and fun, without fear to make mistakes. Most of them
underlined how all the students were active and interested, also the ones assessed
as usually not motivated.

Methodology and Results on Teaching Maths Using Mobile Robots
405
4
Conclusions
In this paper we presented our methodology to teach Mathematics using mobile
robots and the results of data analysis, collected from 2,911 students of 58 Ital-
ian Public schools, Primary (5 years old) and Elementary (from 6 to 8 years old).
Using a eight questions evaluation questionnaire on Maths, we found the improve-
ments in the score obtained by the students on most of the questions and this
learning gain is very signiﬁcantly correlated with the use of the robot. Overall, we
found general positive trend in the increase of better and maximum scores and
the decrease of blank answers. The lessons learned are to reduce the number of
questions and calibrate the questions with the level of the class. About teachers,
all their feedbacks have reported a good level of satisfaction about the experi-
ence. They discovered a new cheap and practical tool for educational robotics and
noticed high involvement and enthusiasm of their students.
As future work we are planning a controlled experiment to compare our
methodology with the traditional one and schedule a didactic competition asso-
ciated with the project activities between classes and schools.
Acknowledgements. The project “A scuola di coding con Sapientino” was promoted
by Clementoni, DIAG Department at Sapienza University of Rome and MIUR (Italian
Ministry of Education, University and Research). We would like to warmly thank all
the teachers that, on a voluntary base, supported and realized the project with us and
all the children that joyfully participated in the teaching activities.
References
1. Barker, B.S., Ansorge, J.: Robotics as means to increase achievement scores in an
informal learning environment. J. Res. Technol. Educ. 39(3), 229–243 (2007)
2. Benitti, F.B.V.: Exploring the educational potential of robotics in schools: a sys-
tematic review. Comput. Educ. 58(3), 978–988 (2012)
3. Di Lieto, M.C., Inguaggiato, E., Castro, E., Cecchi, F., Cioni, G., Dell’Omo, M.,
Laschi, C., Pecini, C., Santerini, G., Sgandurra, G., et al.: Educational robotics
intervention on executive functions in preschool children: a pilot study. Comput.
Hum. Behav. 71, 16–23 (2017)
4. Eck, J., Hirschmugl-Gaisch, S., Hofmann, A., Kandlhofer, M., Rubenzer, S.,
Steinbauer, G.: Innovative concepts in educational robotics: robotics projects for
kindergartens in Austria. In: Austrian Robotics Workshop 2013, vol. 14, p. 12 (2013)
5. Highﬁeld, K.: Robotic toys as catalyst for mathematical problem solving. Aust.
Prim. Math. Classroom 15(2), 22–27 (2010)
6. Kazakoﬀ, E.R., Sullivan, A., Bers, M.U.: The eﬀect of a classroom-based intensive
robotics and programming workshop on sequencing ability in early childhood (2013)
7. Mondada, F., Bonani, M., Riedo, F., Briod, M., Pereyre, L., Retornaz, P., Magne-
nat, S.: Bringing robotics to formal education: the thymio open-source hardware
robot. IEEE Robot. Autom. Mag. 24(1), 77–85 (2017)
8. Mubin, O., Stevens, C.J., Shahid, S., Al Mahmud, A., Dong, J.-J.: A review of the
applicability of robots in education. J. Technol. Educ. Learn. 1, 209-0015 (2013)
9. Nulli, G., Di Stasio, M.: Coding alla scuola dell’infanzia con docente esperto della
scuola primaria. Ital. J. Educ. Technol. 1(1) (2017)

406
P. Ferrarelli et al.
10. Rees, A.M., Garc´ıa-Pe˜nalvo, F.J., Toivonen, T., Hughes, J., Jormanainen, I.,
Vermeersh, J.: A survey of resources for introducing coding into schools (2016)
11. Shin, N., Kim, S.: Learning about, from, and with robots: students’ perspectives. In:
The 16th IEEE International Symposium on Robot and Human Interactive Com-
munication, RO-MAN 2007, pp. 1040–1045. IEEE (2007)
12. Yesharim, M.F., Ben-Ari, M.: Teaching robotics concepts to elementary school chil-
dren. In: Proceedings of International Conference on Robotics in Education (RiE)
(2017)
13. Zygouris, N.C., Striftou, A., Dadaliaris, A.N., Stamoulis, G.I., Xenakis, A.C.,
Vavougios, D.: The use of lego mindstorms in elementary schools. In: 2017 IEEE
Global Engineering Education Conference (EDUCON), pp. 514–516, April 2017

Autonomous Driving and Driver
Assistance Systems (I)

Application of Sideslip Estimation Architecture
to a Formula Student Prototype
Andr´e Antunes2, Carlos Cardeira1,2(B), and Paulo Oliveira1,2
1 IDMEC, Universidade de Lisboa, 1049-001 Lisboa, Portugal
2 Instituto Superior T´ecnico, Universidade de Lisboa, 1049-001 Lisboa, Portugal
{andre.antunes,carlos.cardeira,paulo.j.oliveira}@tecnico.ulisboa.pt
Abstract. This paper describes an estimator architecture for a For-
mula Student Prototype, based on data from an inertial measurement
unit (IMU), a global positioning system (GPS), and from the underlying
dynamic model of the car. A non-linear dynamic model of the car and
realistic models for the sensors are presented. The estimates of attitude,
rate-gyro bias, position, velocity and sideslip are based on Kalman ﬁlter-
ing techniques. The resulting system is validated on a Formula Student
prototype and assessed given ground truth data obtained by a set of
diﬀerential GPS receivers installed onboard.
Keywords: Kalman Filter · Sideslip · Sensors · Estimation
1
Introduction
Formula Student is an university competition that challenge students from
around the world to build a single-seat racing car. Along the years the com-
petition has been evolving with the implementation of new materials and tech-
nologies, always looking to follow the world evolution of automotive technol-
ogy. Recently it has approach the driverless cars starting a new parallel com-
petition for these vehicles, which associated with electric independent all-wheel
drive already used by the teams, opens a door for countless control approaches.
Control strategies like vehicle stability control and torque-vectoring [1] depend
widely on a sideslip observer to assure that the vehicle stays in a stable route.
This observer is especially needed when there are signiﬁcant diﬀerences between
the model and the true vehicle, something that usually happens when working
with road vehicles and tires.
This paper undertakes the implementation of an architecture proposed in [2]
for a Formula Student prototype to estimate the sideslip of the vehicle. During
a test day, data was acquired from an inertial measurement unit (IMU) con-
sisting of an accelerometer a magnetometer and a gyroscope, all of them with
3 axis, a global positioning system (GPS) and a steering encoder. These sen-
sors were already part of the vehicle. In order to verify the results obtained
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_34

410
A. Antunes et al.
Fig. 1. IST - FST06e
from the estimators, a secondary system using a diﬀerential global positioning
system (DGPS) was attached to the car. This one gives the heading and the
velocity components, values required to calculate the sideslip angle of the car.
Both systems, the DGPS and the car sensors are completely independent. The
acquired values are then processed in oﬄine to skip implementation issues. In
this document, the models and estimators are brieﬂy explained in Sects. 2 and 3
respectively, with only the more important equations presented, their deduction
can be found in [2,3]; followed by a closer look to the test platform (Fig. 1), its
technical characteristics, and the sensors are explored in Sect. 4; Sect. 5 illustrates
the results obtained in each estimator and a comparison is performed with the
values from the DGPS; Finally, Sect. 6 outlines some remarks and future work.
2
Car Model
This section presents the general non-linear equations for the vehicle model from
where the linear car model is obtained. These equations and their deduction are
explored in detail by [2], and as such, in this work only the resulting equations
are exposed.
The following model is named a planar model since it considers that no
pith (θ) or roll (φ) rotations exist, being these the major assumptions, which
also implies that no load transfer occurs. Besides this limitation, aerodynamics
forces acting on the car are neglected. This restricts all the forces to the tires,
as can be seen in Fig. 2(a), where each one produces a longitudinal and a lateral
force respectively Fx and Fy, with the indexation Front Left (FL), Front Right
(FR), Rear Left (RL) and Rear Right (RR). And where δ deﬁnes the wheel
steer angle, v is the velocity vector, β is the car sideslip angle, a is the distance
between the centre of gravity (CG) and the front axle, b the distance between the
CG and the rear axle, and tr is the length of the axle, where both are considered
equal. The resulting equations of this model are described by (1), where r is the

Sideslip Estimation
411
Fig. 2. (a) Forces applied on the Car; (b) Tyre angles and frames. Images from [2]
angular velocity around the z-axis, m is the mass of the car plus the driver and
Iz is the moment of inertia around the z-axis.
˙vx = vyr −1
m[F F
y sin δ −F R
x ]
(1a)
˙vy = −vxr + 1
m[F F
y cos δ + F R
y ]
(1b)
˙rz = 1
Iz
aF F
y cos δ −1
Iz
bF R
y
(1c)
The longitudinal force Fx on the car is assumed to be a direct input, without
considering the tires limitations. The lateral forces Fy on the tires are given by
a cornering stiﬀness approximation expressed by (2), where Cα is the cornering
stiﬀness constant, and αi is the slip angle of the tyre i, deﬁned as αi = βi −δi,
as seen in Fig. 2(b).
Fy = −Cααi
(2)
The sideslip angle of the wheel (βi) is the projection of the sideslip angle β of
the car in the wheel by vi = Bv + Br × Bri, where Bri is the vector of distance
between the CG and the wheel i. This cornering stiﬀness approximation contains
several assumptions that are explained in detail in [2].
2.1
Linear Car Model
The car model used in the estimator is an approximation of the Eqs. (1) and (2)
for small angles and the assumption of a constant longitudinal velocity (vx =
const), which has been widely used in the literature [4–6]. With the small angle
approximation is possible to deﬁne the wheel slip angle as:
αi = tg-1
vy + xir
vx −yir

−δi ≈vy + xir
vx
−δi
(3)

412
A. Antunes et al.
Using a diﬀerent cornering stiﬀness for front and rear wheels, respectively
Cαf and Cαr, is then possible to rewrite (1)–(3) in a state space form:
˙vy
˙r

=
⎡
⎣−Cαf +Cαr
vxm
−aCαf +bCαr
vxm
−vx
−aCαf +bCαr
Izvx
−a2Cαf −b2Cαr
Izvx
⎤
⎦
vy
r

+
⎡
⎣
Cαf
m
aCαf
Iz
⎤
⎦δ
(4)
3
Estimator Architecture
In this section, the proposed architecture for the estimation of the sideslip is
exposed. As depicted in Fig. 3, the estimation process is composed of three
sequential ﬁlters. An Attitude Complementary Filter (ACF), a Position Com-
plementary Filter (PCF), and a Car Estimation Model (CEM).
Fig. 3. Flowchart of Filter Scheme where mr, ωr, ar, pr and δr are respectively the
reading from the magnetometer, gyroscope, accelerometer, GPS and steering encoder.
Grey boxes represent required data processing before use in ﬁlters.
The ACF is used to correct the yaw reading from the magnetometer and to
estimate the bias of the gyroscope. The yaw is then used in the rotation matri-
ces inside the PCF, which uses the accelerometer and the GPS to estimate both
velocity components in the body reference frame. The CEM is used to include
the car dynamics in the sideslip estimation using the velocity components, the
corrected yaw rate and the steering angle. The three ﬁlters are implemented
using discrete Kalman Filter [7,8], where the CEM is discretized from a contin-
uous state space model. Both complementary ﬁlters are explained in more detail
in [2,3].
3.1
Attitude Complementary Filter
The ACF combines the yaw readings with the angular velocity around the z-axis
to give a more accurate yaw value and a bias estimation for the angular velocity
to correct the gyroscope signal. This assumes that the yaw rate ˙ψ = ωr,k reading
is aﬀected by random white noise (wωr,k) as well as a constant bias (bωk):
ωrk = ωk + bωk + wωr,k

Sideslip Estimation
413
With this, is then possible to write the discrete Kalman Filter in a state space
form as:
 ψk+1
bωk+1

=
1 −T
0 1
  ψk
bωk

+
T
0

ωr,k +
K1
K2

(yk −ˆyk)
ˆyk = ˆψk ,
yk = ψr,k + vk
where the index k deﬁnes the instant in time t = kT being T the sampling
time interval. The gyroscope reading is ωr,k, and the yaw reading from the
magnetometer is ψr,k which is corrupted with random white noise vk. The values
K1 and K2 are the Kalman gains associated to each state.
3.2
Position Complementary Filter
The PCF combines the readings of the accelerometer with the ones from the
GPS to give an estimate of the velocity components in the vehicle body frame
which in this case are unobservable states. Since the GPS uses a global reference
frame as the opposite of the accelerometer and the needed velocity components,
the yaw estimation from the ACF is used to convert between reference frames
using a rotation matrix deﬁned as Rk. The equations that rule the PCF are the
motion equations:
¯pk+1 = ¯pk + T ¯vk + T 2
2 Rk¯ak
¯vk+1 = ¯vk + TRk¯ak
where ¯p, ¯v, ¯a are respectively the vectors of position, velocity and acceleration.
Is then possible to write the PCF system as:

ˆpk+1
Bˆvk+1

=

I T ¯Rk
0
I
 
ˆpk
Bˆvk

+

T 2
2 ¯Rk
TI

¯ak +

K1p
¯R
′
kK2p

(ypk −ˆypk)
ˆypk = ˆpk ,
ypk = ¯pk + vpk
where I ∈R2 is the identity matrix, ¯ak is the readings from the accelerometer,
¯pk is the GPS readings of position that are corrupted with random white noise
vpk. The values K1 and K2 are 2 × 2 diagonal matrices with the Kalman gains
identiﬁed with a linear time-invariant system based on the above, explored in
[2,3]. The subscript B indicates the components in the vehicle body frame.
3.3
Car Estimator Model
The Car Estimator Model is based on system (4), which depends on two state
variables

vy r

. This system is time variant due to vx, which can generate com-
plications if the longitudinal velocity is zero, or close to zero due to numeric
problems. The CEM uses the velocity components estimations of the PCF, the

414
A. Antunes et al.
gyroscope reading corrected with the bias from the ACF, and the steering angle
of the wheel. The system for the ﬁlter is given by:
⎡
⎣
˙ˆvy
˙ˆr
⎤
⎦=
⎡
⎢⎣
−
Cαf +Cαr
vxm
−aCαf +bCαr
vxm
−vx
−aCαf +bCαr
Izvx
−
a2Cαf −b2Cαr
Izvx
⎤
⎥⎦

ˆvy
ˆr
	
+
⎡
⎣
Cαf
m
aCαf
Iz
⎤
⎦δ +

K1vy K1r
K2vy K2r
	
(yl −ˆyl)
ˆyl =
 ˆvy
ˆr

,
yl =
vy
r

+
vvy
vr

where vvy and vr are the noises associated to each measurement and the Ki
gains are the Kalman gains that relate the error of measured and estimated
data to each state variable.
4
Test Platform
In order to test the proposed estimation architecture, a real test was conducted.
At the time, these algorithms weren’t already implemented in a hardware capable
of real-time processing, so all the data was logged, and processed oﬄine after
the test. The test platform was FST06e (Fig. 1), an electric Formula Student
Prototype. This vehicle is propelled by two independent 50 kW motors at the
rear, one motor per wheel, with a single ﬁxed gear with no clutch. With a
weight of 280 Kg and a distance between axis of 1.59 m, is capable of achieve
0–100 km/h in 2.9 s, and a top speed of 150 km/h. The remaining parameters of
the car needed for the models and estimators are presented in Table 1.
Table 1. FST06e Parameters
Description
Var
Variable
Units
Front and rear track
−
1.24
m
Mass of the Car + Driver
m
356
kg
Yaw Inertia
Iz
120
kg.m2
Weight distribution
−
45.1–54.9
% −%
Static load at front wheels
F Fz 787.5
N
Static load at rear wheels
RFz
958.7
N
Cornering Stiﬀness front
Cαf
1.527 × 104 N/rad
Cornering Stiﬀness rear
Cαr
1.995 × 104 N/rad
The car relies on a distributed electronic circuit for all the monitoring, con-
trols and acquisition. This circuit spreads all along the car, and consists in several
modules interconnected by a CAN-BUS line working at 1Mbit/s. For this test
3 modules were essential, the GPS, the Steering and IMU, and the log unit.
Each of these modules has one dedicated micro-controller, a dsPIC30f4013 from

Sideslip Estimation
415
Texas Instruments, working at 30 MHz in a self-developed board. The GPS mod-
ule incorporates a SkyTraq S1216F8 chip conﬁgured to an update rate of 25 Hz
. The Steering and IMU module consists of a GY-80 IMU that includes a 3-axis
accelerometer (ADXL345), a 3-axis gyroscope (L3G4200D) and a 3-axis magne-
tometer (HMC5883L). The steering encoder is a 3-turn rotational potentiometer
used as a voltage divider with a 12bit ADC, that is attached to the steering col-
umn. The log unit simply reads one message at the time in the CAN-BUS line
and writes it to a ﬁle in an SD card.
In order verify the estimated data, a second independent system was used.
This one, wasn’t connected to the car’s previous system, and had a separated log
system. This system consisted of two GPS antennas placed on the front and rear
of the car separated by 2.5 m as seen in Fig. 4. The antennas were connected to
an Ashtech MB100 board that can deliver the heading angle and velocity com-
ponents needed to calculate the sideslip angle of the car. This data was logged
at 10 Hz.
The acquisition was made during a track day at the university campus using
a parking lot limited to an asphalt area of approximated 60 m × 25 m. The tra-
jectory of the car consisted in several circles in both ways, and turns after a long
straight.
DGPS Front 
antenna
DGPS 
Acquisition 
system
DGPS Rear 
antenna
Car GPS 
antenna
Car IMU and 
Steering 
encoder
Fig. 4. FST06e Acquisition system with DGPS. In orange are the sensors belonging to
the car, and in blue the elements of the DGPS.
5
Results
This section presents the results from the diﬀerent ﬁlters and estimators during
the test run using the FST06e with the DGPS (Fig. 4). Is also explained the
processing done to each signal before the ﬁlters.
For this test, the car was stripped of all aerodynamic elements, to better
correspond to the models, and for the antennas to have a clean view of the sky.

416
A. Antunes et al.
5.1
Attitude Complementary Filter
To feed the Attitude complementary ﬁlter, the data from gyroscope and the
magnetometer are required. Before the use of this data, a calibration to align
the axis of the IMU with the vehicle axis is done using the accelerometer with
data from a static acquisition. This calibration is used in the three sensor units
(accelerometer, gyroscope and magnetometer), and kept for all the ﬁlters. The
yaw angle is computed from the magnetometer after a 3-dimensional calibration
is done to correct hard and soft non-linearities [9]. The yaw angle is also corrected
from the magnetic declination, to match the position referential.
The ACF is fed with this data resulting in a corrected yaw angle of the
vehicle as can be seen in Fig. 5. The angle must be in a continuous or cumulative
form, since discontinuities in the transition from 0◦↔360◦or −180◦↔180◦
generate problems in the ACF. In Fig. 5, is possible to see the estimated heading
angle of the car, side by side with the raw value from the magnetometer. Besides
the yaw angle, also the bias of the gyroscope’s z-axis is estimated. In Fig. 6,
an overlapping of several responses to diﬀerent initial conditions are presented,
showing a convergence after some seconds. From the analysis of the raw data
of the sensor, an oﬀset of −0.62◦/s was expected for a stationary measurement,
something that is veriﬁed in the graph.
Fig. 5.
ACF
heading
angle
result
and
raw
heading
measurement
from
the
magnetometer.
Analysing Fig. 6, is possible to see that around the 160 s, the bias starts to
oscillate. This is when the vehicle started to move. One of the assumptions made
at the start is that no roll or pitch happens, but in the real vehicle this isn’t true.
Although small, these rotations exist and inﬂuence the IMU that is attached to
the car. Besides that, the track isn’t perfectly ﬂat containing a slight tilt in one
side. These unpredicted rotations can justify the oscillations when the car is
moving.
5.2
Position Complementary Filter
The position complementary ﬁlter relies on three major sources, the GPS, the
accelerometer and the corrected yaw angle or heading of the car from the ACF.

Sideslip Estimation
417
Fig. 6. Bias result from the ACF, and a overlapping of several responses for diﬀerent
initial conditions
A GPS receiver doesn’t give the required X and Y position coordinates, so a
transformation is done from ECEF (Earth-Centred, Earth-Fixed) coordinates to
ENU (East North Up) coordinates, using as origin a point along the track. The
East is deﬁned as the X coordinate, and North the Y coordinate, the Up or Z
is not used. Also, a correction is performed to the location of the GPS antenna,
this was considered necessary since the distance between the antenna and the
centre of gravity was substantial (≈1 m). The correction was performed using
(5) where ¯dgps is the vector with the distance from the GPS antenna to the
centre of gravity, and ψ is the yaw angle from the ACF.
xcg
ycg

=
xgps
ygps

+
cos ψ −sin ψ
sin ψ
cos ψ

¯dgps
(5)
The GPS only has a 25 Hz acquisition frequency compared with the 100 Hz
of the remaining sensors, this was overcome by using an interpolation for the
missing points to match the general frequency, and was only possible because
processing was done oﬄine. For an online application, a multi-rate solution must
be implemented, or the global frequency reduced to match the smallest frequency
available.
The accelerometer kept the axis calibration explained in the ACF section, and
was corrected from oﬀsets in the readings. It was also implemented a correction
from the inﬂuences of angular velocities due to the distance between the IMU
and the centre of gravity using Eq. (6) where ¯Aimu is the vector of accelerations
of the reading, ¯Acg is the vector of accelerations in the centre of gravity, ¯ω is
the vector of angular velocities, and ¯dimu the distance vector from the IMU to
the centre of gravity.
¯Aimu = ¯Acg + ¯ω × (¯ω × ¯dimu)
(6)
The results of the PCF using the values of position acceleration and yaw
angle can be seen in Fig. 7. This graph presents an overlapping of the veloc-
ity components acquired from the DGPS, and the estimates from the PCF. It
must be noticed that the PCF doesn’t use in anyway the data from the DGPS,

418
A. Antunes et al.
Fig. 7. Velocity components estimations resulting from the PCF compared with the
ones from the DGPS.
being the two signals completely independent one from the other. The noise is
clearly higher in the estimation, this is due to the sensor signals, specially the
accelerometer. Since the accelerometer is corrected with the gyroscope unﬁltered
data using (6), the combination of both signals results in a very noisy signal.
An example is the time window between [225 s; 250 s], where the vehicle was
performing several turns at constant radius and speed, and the resulting lateral
acceleration from (6) has a mean value of 10.2 m/s2 and a standard deviation
σ = 1.8 m/s2. In the overall, even with the noise, the estimation of both velocity
components is considerably close to the results from the DGPS with small errors.
5.3
Car Estimation Model
For this estimator, is necessary the velocity components from the PCF, the
steering angle of the front wheels, and the corrected gyroscope reading. The
steering encoder mentioned before doesn’t read the δ value of the wheel, but the
steering wheel angle, besides that, the encoder to achieve a greater resolution
relies on a gear ratio to convert the steering wheel range to make a better use
of the 3-turn encoder. Due to Ackerman geometry [10] both wheels rarely have
the same angle, and the ratio between the steering wheel angle and the wheel
angle isn’t linear, but for the sake of simplicity this angle is assumed linear and
equal. The linear car estimator, uses Eq. (4) where the transition matrix relies
on the longitudinal velocity vx. For values equal or close to 0, some problems
arise since some of the elements are dividing by zero. To overcome this situation,
is assumed that the longitudinal velocity used in the transition matrix of (4),
has a lower saturation of vx = 3 m/s.
A similar problem arises computing β = tg-1(vy/vx) when the velocities are
very close to zero, any small noise can generate huge angles. To get around this

Sideslip Estimation
419
Fig. 8. Sideslip angle (β) estimation from the CEM compared with the one given by
the DGPS
situation a rule was implemented that when vx < 3 m/s then vy = 0 m/s, which
means that for small velocities (under 3 m/s) no sideslip occurs. Note that 3 m/s
was just a value that seems small enough to not induce a great error, and worked
well during the tests.
The results obtained using the car model estimator for the sideslip angle
are presented in Fig. 8, as well as the sideslip angle measured by the DGPS.
This last one didn’t use any signal treatment, and is acquired at only 10 Hz.
Fig. 9. Graphic representation of the results obtained from the estimators with the car
represented by a triangle with the heading orientation, and a blue vector representing
the velocity vector.

420
A. Antunes et al.
The resulting signal is a bit noisy due to the input signals, in this case the
velocity components from the PCF, as seen previously. Once again, the estimator
doesn’t use any value from the DGPS. Analyzing both signals is possible that
the estimative is a little more conservative in terms of amplitude of the angle.
With some simple changes in the gains is possible to increase this amplitude,
but with the consequence of an increase in the noise. The presented result is a
compromise between accuracy and noise. It’s also important to remember that
one of the assumptions in this ﬁlter is the small angle approximation, which from
the DGPS values, was clearly exceeded. Even with some diﬀerences the estimator
keeps up with all the variations registered by the DGPS. In Fig. 9 is possible to
see a graphical representation of the car during a part of the track ([221 s; 227 s]),
where the car is simpliﬁed by a red triangle align with the heading angle. A blue
arrow represents the velocity vector of the car at the centre of gravity. The β,
is the angle between the heading and the velocity vector. It’s a well-known fact,
among all the drivers of this car, that it suﬀers a lot from understeer, something
visible in Fig. 9, where is possible to see that the velocity vector always points to
the inside of the curve in relation to the heading, showing that behaviour. This
situation was also clearly visible during the data acquisition.
6
Conclusion
The estimation architecture explored was validated on a Formula Student pro-
totype. The Attitude Complementary Filter was implemented with interesting
results of heading and an estimate that corresponds to the expected bias. The
results of the Position Complementary Filter were close to the ground truth
given the low-cost sensors used. The Car Estimation Model was able to pro-
duce a sideslip estimation close to the values acquired from the DGPS, and
the observed during the ﬁeld tests. The ﬁnal results were similar to the values
from the DGPS. Thus, the estimation architecture proposed is capable to pro-
duce the intended results. This architecture paves the way for the next stages of
the projects, namely control system design and driverless competition prototype
development, by providing an observer for the sideslip.
Acknowledgments. This work was supported by FCT, through IDMEC, under
LAETA, project UID/EMS/50022/2013. The authors thank the prompt and fruit-
ful cooperation of the IST Formula Student team, FST Lisboa and ISR - Dynamical
Systems and Ocean Robotics Lab, namely the assistance of Bruno Cardeira.
References
1. De Novellis, L., Sorniotti, A., Grubber, P., Pennycott, A.: Comparison of feed-
back control techniques for torque-vectoring control of fully electric vehicles. IEEE
Trans. Veh. Technol. 63(8), 3612–3623 (2014)
2. Antunes, A., Cardeira, C., Oliveira, P.: Sideslip estimation of a formula stu-
dent prototype through GPS/INS fusion. In: 2017 IEEE International Conference
on Autonomous Robot Systems and Competitions (ICARSC), 26–28 April 2017,
Coimbra, Portugal, pp. 184–191 (2017)

Sideslip Estimation
421
3. Vasconcelos, J.F., Cardeira, B., Silvestre, C., Oliveira, P., Batista, P.: Discrete-
time complementary ﬁlters for attitude and position estimation: design, analysis
and experimental validation. IEEE Trans. Control Syst. Technol. 19(1), 181–198
(2011)
4. Jazar, R.N.: Vehicle planar dynamics. In: Vehicle Dynamics: Theory and Applica-
tion, 1st edn. Springer, New York (2008). Chap. 10
5. Ryu, J., Rossetter, E.J., Gerdes, J.C.: Vehicle sideslip and roll parameter estima-
tion using GPS. In: 6th International Symposium on Advanced Vehicle Control,
Hiroshima (2002)
6. Nam, K., Oh, S., Fujimoto, H., Hori, Y.: Estimation of sideslip and roll angles
of electric vehicles using lateral tire force sensors through RLS and Kalman ﬁlter
approaches. IEEE Trans. Ind. Electron. 60(3), 988–1000 (2013)
7. Franklin, G.F., Powell, J.D., Workman, M.: Multivariable and optimal control.
In: Digital Control of Dynamic Systems, 3rd edn., pp. 389–391. Ellis-Kagle Press
(2006). Chap. 9
8. Brown, R., Hwang, P.: Discrete Kalman ﬁlter basics. In: Introduction to Ran-
dom Signals and Applied Kalman Filtering: with Matlab Exercises, 4th edn.,
pp. 249–262. Wiley, New York (2012). Chap. 7
9. Vasconcelos, J.F., Elkaim, G., Silvestre, C., Oliveira, P., Cardeira, B.: Geometric
approach to strapdown magnetometer calibration in sensor frame. IEEE Trans.
Aerospace Electron. Syst. 47(2), 1293–1306 (2011)
10. Abe, M.: Vehicle Handling Dynamics: Theory and Application, 1st edn, pp. 5–117.
Butterworth-Heinemann publications, Oxford (2009). Chap. 2–3

Torque Vectoring for a Formula Student
Prototype
Jo˜ao Antunes2, Carlos Cardeira1,2(B), and Paulo Oliveira1,2
1 IDMEC, Lisbon, Portugal
2 Instituto Superior T´ecnico, Universidade de Lisboa, Lisbon, Portugal
{joao.pedro.antunes,carlos.cardeira,paulo.j.oliveira}@tecnico.ulisboa.pt
Abstract. Torque Vectoring (TV) has the objective to substitute the
need of a mechanical diﬀerential, while improving the handling and
response of the wheeled vehicle. This work addresses the design of a
torque vectoring system in an rear wheel driven formula student pro-
totype. The proposed solution resorts to a PID controller for yaw rate
tracking with an evenly distributed torque to each wheel. Also an LQR
scheme is discussed, for tracking the yaw rate and the lateral velocity.
To assess and design, ﬁrst a 7 degree of freedom (DOF) non linear model
is constructed, followed by a linear 2 DOF model, both validated with
real data. The linear model, is used to design and simulate the proposed
controllers. When the controller is within the desired parameters it is
tested in the non linear model. Tests with the vehicle are performed to
verify the contribution of the controller to the overall performance of the
vehicle.
Keywords: Torque vectoring · Formula student · PID controller · LQR
controller · Vehicle model
1
Introduction
Each year that goes by sees an increase in sales of personal use of electric vehicles
(battery EVs, plug-in hybrids and regular hybrids), all of which rely on electric
motors as a basis or an aid to propulsion. It also opens the opportunity for
vehicle stability control systems directly at the motors, like Electronic Stability
Program (ESP) and Anti-lock Braking System (ABS) which gives the vehicle
better stability and maneuverability. This paper addresses another type of vehi-
cle stability control called Torque Vectoring (TV). By controlling the amount of
torque distributed to each driven wheel, the system has the potential to improve
both the stability and response of a vehicle without compromising safety and
drivability.
Developing a torque vectoring system can be tackled resorting to several
diﬀerent approaches. The most common [6,8] use the yaw rate of the vehicle as
the reference for the controller. More advanced solutions [7,9,11] use the sideslip
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_35

Torque Vectoring for a Formula Student Prototype
423
angle and/or a combination of yaw rate and sideslip angle. The choice on the
strategy will depend on the available sensors.
A variety of control system design approach can be used. The most basic
method is to distribute the left and right torque, proportional to the amount
of steering input ΔT = f(δ), where δ is the steering wheel angle. The propor-
tional integral derivative (PID) controller is the classic control structure and the
most commonly used in practical applications. It is a straightforward method to
implement and tune [9,12]. Sliding mode control is a non linear control design
methodology used by several researchers to achieve the objectives of tracking
the yaw rate and slip angle [9,11]. Predictive control estimates the future states
of the vehicle in order to ﬁnd the best control input [3,4]. Some authors also
try to implement Fuzzy control to create a set of rules for the allocation of the
torque [14].
2
Vehicle Model
Vehicle dynamics is the area devoted to the development of models that describe
the behavior of a vehicle for any given set of inputs and disturbances. Modeling
this type of system is a very complex and challenging task. A lot of diﬀerent
approaches and models can be needed. A complex multi-body system (with 20+
degrees of freedom), or a simple two degree of freedom model can be represen-
tative [5, p. 6].
The development of the torque vectoring controller will be tested in the
FST06e. The FST06e is a prototype electric vehicle powered by two Siemens
permanent magnetic synchronous motors each one with an RPM range of 0 to
8000, and producing a maximum torque of 107 Nm. With a planetary gear set
ﬁxed with a gear ratio of 4.1:1, amplify at the wheel with a total of 876 Nm.
2.1
Non Linear Model
The vehicle model used to study a torque vectoring control will typically have
seven degrees of freedom. The lateral and longitudinal velocities of the vehicle
(vx and vy respectively) and the yaw rate ˙ψ constitute three degrees of freedom
related to the vehicle body. The wheel velocities of the four wheels, the front left
wheel (wfl), front right wheel (wfr), rear left wheel (wrl), and rear right wheel
wrr) constitute the other four degrees of freedom [10].
The full model consists in a horizontal model which describes the model
position and orientation of the vehicle. A model of the steering kinematics, which
describes the relation between the steering wheel and the actual steering of each
wheel. A simpliﬁed tire model to calculate the forces acting on the wheels. A
balance at the wheel between the applied torque and the tire longitudinal force.
Lateral and longitudinal weight transfer, to model the loads at each wheel during
cornering, braking and acceleration. Figure 1 shows the block diagram of the
model, where some of the physical variables can be found in [2].

424
J. Antunes et al.
Fig. 1. Simulink Schematic of Non Linear Model
2.2
Linear Model
The linear model, is one of the most common models. The linear bicycle model
is a simpliﬁcation in which it is only being considered the lateral velocity vy
and yaw rate ˙ψ on the model [5,6,8,14]. The way the model is presented, the
equations only generate lateral force by the steering input. The goal with the
torque vectoring is to generate yaw moment based on controlling the torque
(longitudinal force) at the driven wheels. For this it is necessary to introduce a
new term Mz that represents the additional yaw moment. Table 1 summarizes
the values from the FST06e need for the model.
˙x = Ax + Bu1 + Eu2
˙x =
 ˙vy
¨ψ

;
u1 = Mz;
u2 = δ
A =
⎡
⎢⎢⎢⎣
−Cy,f + Cy,r
mvx0
−lfCy,f + lrCy,r
mvx0
−vx0
−lfCy,f + lrCy,r
Izzvx0
−
Cfl2
f + Crl2
r
Izzvx0
⎤
⎥⎥⎥⎦B =
⎡
⎣
0
1
Izz
⎤
⎦E =
⎡
⎢⎢⎢⎣
Cy,f
mvx0
lfCy,f
Izz
⎤
⎥⎥⎥⎦
(1)
2.3
Validation
The constant radius turn (skidpad) is deﬁned by ISO 4138 [1]. According to the
norm, the test should be performed with a minimum circle radius of 30 m. For
the FST06e, the available test track is at the campus in front of the main build

Torque Vectoring for a Formula Student Prototype
425
Table 1. Linear model values
Term
Symbol Value Units
Term
Symbol Value
Units
Yaw rate
˙ψ
-
[rads−1]
Rear wheelbase lr
0.717
[m]
Velocity
vx0
[0,40] [ms−1]
Steering angle
δ
[-3.3,3.3] [rad]
Rear Stiﬀness
Cy,r
21429 [Nrad−1] Yaw moment
Mz
-
[Nm]
Front Stiﬀness
Cy,f
15714 [Nrad−1] Lateral velocity vy
-
[ms−1]
Inertia moment Izz
120
[Kgm2]
Gear ratio
Gr
4.4
-
Mass
m
356
[Kg]
Half track
tr
0.65
m
Front wheelbase lf
0.873 [m]
Wheel Radius
Rw
0.265
m
in the parking lot, with a maximum circle radius of 7.5 m. Another test track is
at the International Kartodromo in Palmela, with a maximum radius of 15 m.
In the Formula Student competition, one of the disciplines is to perform a
constant radius turn in a track with a radius of 8.75 m. To be more close to the
competition conditions, the tests are modiﬁed and performed with a range of
5-9 m radius.
In total three data sets are available. “Test 1” and “Test 2” are both done at
the campus. The radius of the circle is the same, but the driver varied the velocity.
“Test 3” is a test with a bigger radius done in Palmela. Table 2 summarizes the
tests.
Table 2. Test setups
Test 1 Test 2 Test 3 Unit
Radius of circle
5.62
5.62
9.02
[m]
Global velocity
7
8.5
9.3
[m/s]
Steering angle
110
100
71
[deg]
Measured yaw rate 72.3
72
58.6
[deg/s]
During the tests some data are being monitored. The inputs are the global
velocity (vCG), the steering wheel angle (δ). The output values are the longitu-
dinal, lateral acceleration (ax, ay) and yaw rate ( ˙ψ). Although, many more data
can be logged from the car, these are suﬃcient to conclude if the models are
accurate enough [13, p. 345]. Figure 2 shows the comparison between the yaw
rate from the vehicle and the yaw rate from both linear and non linear simula-
tions. The simulated values are very close to the real data. The data presented
illustrates that the vehicle made 4 turns. Two to the left and two to the right (≈
5s each). Negative values of yaw rate represent the car cornering to the right (in
a clockwise way) while positive values represents the car cornering left (counter
clockwise).

426
J. Antunes et al.
3
Control System Design
Based on the availability, sampling time and quality of the sensors, the strategy
for the calculation of the reference value is presented next, followed by the choice
and tuning of the controllers.
3.1
Reference Value
Before the introduction of the control algorithm, it is necessary to deﬁne a ref-
erence signal. Various authors propose the calculation of the reference based on
the velocity and steering angle of the car. It is assumed that the vehicle is in a
steady state [10].
Ku =
lrm
Cy,f(lf + lr) −
lfm
Cy,r(lf + lr)
(2)
If the under-steer gradient, Ku is positive (Ku > 0): The car is said to have
an under-steer behavior (under yaw rate).
If Ku is negative (Ku < 0): The car is said to have an over-steer behavior
(over yaw rate). And if Ku = 0, it means that the car is neutral-steer (ideal yaw
rate).
A neutral-steer vehicle has the smallest possible turning radius for a given
velocity, which corresponds to optimal performance. Therefore, it would be
assumed that a neutral-steered vehicle should be chosen as the reference. How-
ever, this approach would put the car on the verge of over-steer instability, so
a slightly under-steered is taken as reference. This reference is much closer to
neutral-steered than the actual car, so the controller can improve the yaw rate.
Given the velocity and steering angle of the car and with the known steer
gradient and wheelbase it is possible to know the turning radius, combining with
145
150
155
160
165
170
175
180
time [s]
-150
-100
-50
0
50
100
150
yaw rate [deg/s]
Simulation Non Linear Model
FST06e data
Simulation Linear Model
Fig. 2. Comparison between real data form the FST06e and both linear and non linear
simulation during a skidpad to the left and right from test1

Torque Vectoring for a Formula Student Prototype
427
the under-steer gradient and the road radius, gives the desired yaw reference.
˙ψdesired =
vCG
(lr + lf) + Kuv2
GC
δ
(3)
The desired yaw rate is a function of the velocity, steering and characteristic
of the car. The under-steer gradient Ku can be tuned in for each driver prefer-
ence. The smaller the under-steer gradient the bigger the diﬀerence between the
desired and actual yaw rate, more will the car have neutral steer characteristics.
3.2
Maximum Yaw Value
With all the various possible implementations and control strategies, some limi-
tations are valid for all torque vectoring controllers. These limitations are related
to the physical properties of the vehicle like, the maximum yaw rate possible,
the maximum tire adhesion, microprocessor computing time, etc. Depending on
the entry speed of the car, it will be able or not to achieve the desired yaw. If
entering in a corner to fast the road may be unable to provide the necessary
tire forces, and the car just goes forward, thus under-steering. The solution is to
bound the force according to the tire-road coeﬃcient.
vCG ˙ψ + axβ +
vCG ˙β

1 + tan β2 ≤μg
(4)
In Eq. (4), if considering that the car has a small heading angle, the equation
can be further simpliﬁed and reduced to:
˙ψmax = σ μg
vCG
(5)
where σ represents a tunability factor to take into account changes in the friction
coeﬃcient from diﬀerent types of pavement. The yaw rate reference is used as
long as it doesn’t pass the maximum possible yaw rate.
X(m, n) =
 ˙ψdes,
| ˙ψdes| ≤˙ψmax|
± ˙ψmax,
otherwise
(6)
3.3
Proposed Controller
The added momentum results from the diﬀerence between the left wheel torque
Trl and the right wheel torque Trr. This diﬀerence multiplied by the half track of
the car will be the additional yaw momentum Mz in the model. If the right wheel
has more torque than the left wheel the car will have a positive yaw momentum,
thus turning to the left, if the opposite happens the car while have a negative
momentum and will turn right.
Mz = (Trr −Trl)tf
(7)

428
J. Antunes et al.
Table 3. PI values for diﬀerent velocities values
Vx (m/s) P
I
7
296.29 12716.7
10
392.23 12492.5
13
421.72 12040
16
479.87 11536.07
19
396.22 11058.5
22
404.79 13650
But the torque at the wheel is not the same torque at the motor. Between the
motor and the wheel there is a planetary gear set, this gear set multiplies by a
gear ratio Gr the torque from the motor Twheel = Gr ∗Tmotor. Then the torque
at the wheel has to be divided by the wheel radius Rw to obtain the torque at
the ground. These values can be found in Table 3.
Putting together all this information and with Eq. 7 the yaw moment from
the diﬀerence in torque is given by.
ΔT =
Rw
2trGr
Mz =
0.265
2 ∗0.65 ∗4.4Mz = 0.05Mz
(8)
Thus rewriting the state space equation from Eq. 1 in order to the delta
torque instead of the yaw moment the new input matrix is:
B =
⎡
⎣
0
1
0.05 ∗Izz
⎤
⎦
(9)
Taking into consideration that the linear model is dependent on the velocity,
the design of one controller will not be suﬃcient to ensure the control for full
range of operation. So a gain scheduled controller is implemented. Six diﬀerent
setting points are chosen. The main disadvantage of having a small number of
points is that when the gains change the driver could sense an unexpected yaw
rate change. If this occurs more setting points are necessary and thus the same
procedure is reﬁned or points are interpolated.
3.4
LQR
Linear quadratic regulator (LQR) is an optimal control strategy for linear sys-
tems. In the design of this type of control an optimal gain K is calculated based
on the performance index J. The advantage of the PI controller is its simplicity

Torque Vectoring for a Formula Student Prototype
429
in implementation and understanding of what is happening in terms of allo-
cated torque, but this simplicity also has its drawbacks. If the tire-road friction
is wrong or the calculated reference yaw rate is excessive for the current state of
the vehicle, the vehicle behavior may become unstable. To further improve the
torque control, a LQR controller is presented. The controller will also use both
models, both lateral velocity vy and yaw rate ˙ψ are considered state variables
and ΔT the control input. The diﬀerence is that now it will also be monitored
the lateral velocity, which will give a more robust control.
ΔT = Kr ˙ψ + Kvvy
(10)
The performance index may be written in the following way:
J(u) = 1
2
 tf
t0
[(Xd −X)T Q(Xd −X) + uT Ru]dt
(11)
where:
– u - control eﬀort, u = Mz
– R - weight factor of the control eﬀort
– Q - Penalization matrix for the states, lateral velocity vy, yaw rate ˙ψ and
desired yaw rate ˙ψdes
J =
 tf
t0
1
2( ˙ψ −˙ψdes)2 + 1
2wΔT 2
dt
(12)
where
˙ψdes is the desired yaw rate of the vehicle. Minimizing this will lead
to a vehicle with very close to neutral steer behaviour. Not forgetting that as
discussed, the control eﬀort ΔT must be constrained both due to the maximum
torque possible and the tires limit.
The K gains are calculated by solving the Riccati equation, choosing appro-
priate values of Q and R. The controller is tuned by varying both values, First
R and then Q.
AT P + PA −PBR−1BT P + Q = 0
(13)
Solving for our case will be:
2a11p11 + 2a21p12 −R−1 p2
12
Izz
= Q11
(a11 + a22)p12 + a21k22 + a12k11 −R−1 p12p22
Izz
= Q12
2a12p12 + 2a22p22 −R−1 p2
22
I2zz
= Q22
(14)

430
J. Antunes et al.
This system can be solved and the optimal feedback gain matrix K will be:
K = R−1BT P = R−1

0
1
Izz
 
p11 p12
p12 p22

= R−1 p12
Izz
p22
Izz

(15)
Equation 14 is solved for Q =
⎡
⎣
1 0
0
0 1
0
0 0 106
⎤
⎦and a value of R = 10−6.
4
Results
The results of the implemented controller are discussed. The car is tested in the
same conditions as in Sect. 2.3. The driver performs a skidpad with a radius of
5 m trying to match the same conditions that are used to validate the model
(same speed and steering angle). With the data acquired with the diﬀerent con-
trollers, a comparison is made, and the gain in vehicle performance evaluated.
4.1
Without Torque Vectoring Controller
Figure 3 shows the variables, desired yaw rate (blue color), current yaw rate (red
color), and the speed of the vehicle (yellow color). The driver does 5 laps of
approximately 5 s each (80s-110 s) to the left, cornering in a counter-clockwise
way, and from 110s-120 s the driver exists and starts cornering in the opposite
way in a clockwise way, from (120s-150 s). The vehicle speed is also plotted in
the graph to conﬁrm if the driver is maintaining its speed, recalling Eq. 3 the
reference is calculated based on the velocity and the steering angle, if both are
constant then the desired yaw will also be constant.
Focusing now just on the current yaw rate value and desired yaw rate value
it can be observed that the current value of the yaw rate is 70 deg/s, and the
desired yaw rate is 81deg/s, which means that with torque vectoring there can
be a possible gain of 11deg/s. Also, it is important the see that if we look at
test1 from Table 4, the yaw rate is quite similar, which makes sense because the
test track is the same.
4.2
With Torque Vectoring Controller
After the baseline test is performed, the test with the controllers are performed.
Figure 4 shows the same variables as in Fig. 3, but this time the torque vectoring
controller is acting on the vehicle. The data presented is from the car cornering
to the right. As the driver is starting the corner, like in Fig. 3 at ﬁrst (525s-535 s)
the driver is inconsistent but starts to became consistent the more time he is
in the corner (535s-555 s). That is when it can be seen that the controller is
improving the yaw rate of the car.
Table 4 shows that the proposed controller contributes to an increase in lat-
eral performance, the vehicle as more yaw rate, allowing to achieve a higher
cornering speed, which translates in a reduction of 7.6% of lap time.

Torque Vectoring for a Formula Student Prototype
431
80
90
100
110
120
130
140
150
time [s]
-100
-80
-60
-40
-20
0
20
40
60
80
100
yaw rate [deg/s]
4
5
6
7
8
9
Velocity [m/s]
RefYaw
yaw Filtrado
Vel
Fig. 3. Data logged form the vehicle during the test with no torque vectoring. The
variables presented are: Yaw rate reference, yaw rate, and global velocity
Fig. 4. Data logged from the vehicle during the test with the PI controller. The vari-
ables presented are: Yaw reference, yaw rate, and global velocity
Table 4. Comparison between torque vectoring and no torque vectoring for the PI
controller
Yaw rate [deg/s] Velocity [m/s] Time [s]
No TV 70
8.4
4.97
TV
74
8.8
4.59
4.3
LQR
Once the tests of the PI controller are done, the LQR controller is tested. Due to
tire wear and electronic faiulure it was not possible to test the LQR controller in
the car. Figure 5 compares the real data with the PI controller and the simulation
of the LQR controller, it can be seen that the LQR controller response would be
very similar to that of the PID controller.

432
J. Antunes et al.
520
525
530
535
540
545
550
555
560
Time [s]
-30
-20
-10
0
10
20
30
40
50
Torque at wheel [Nm]
Real Data Torque RR
Real Data Torque RL
Simulation PI Torque RL
Simulation PI Torque RR
Simulation LQR Torque RR
Simulation LQR Torque RL
Fig. 5. Comparison of torques between simulation and real data from the car for the
LQR controller and PI controller
4.4
Conclusion
A nonlinear model for simulation has been presented, as well as a linearized
model for yaw rate control through the application of a diﬀerential torque. Both
models are validated with logged data from the electric prototype.
Two control strategies are presented; (i) A gain scheduling PI controller for
controlling the wheel torques based on the yaw rate, (ii) and an LQR approach
for controlling the wheel torques based on the yaw rate and lateral velocity. It
was concluded that for low velocities the models, are not much aﬀected by the
lateral velocity gain, and in terms of response it was quite similar to the PI
controller. The main advantage of the LQR is its robustness and lack of a gain
scheduling when compared to the base solution.
The microcontroller developed by the team proved suﬃcient for receiving and
ﬁltering the data, and also run the controller every 0.2 s. The fail safe system
was crucial to ensure that the tests with the controller could be executed in safe
conditions.
Looking at all the diﬀerent test procedures, it is clear that the implementation
of the torque vectoring has an eﬀect in the vehicle behaviour. When the controller
is used a gain of 7.6% is achieved, which translates in a reduction of 0.38 s in a
skidpad when compared to the same situation but without the torque vectoring.
Acknowledgments. This work was supported by FCT, through IDMEC, under
LAETA, project UID/EMS/50022/2013. The authors thank the prompt and fruit-
ful cooperation of the IST Formula Student team, FST Lisboa and ISR - Dynamical
Systems and Ocean Robotics Lab, namely the assistance of Bruno Cardeira.

Torque Vectoring for a Formula Student Prototype
433
References
1. Passengers cars - Steady-state circular driving behaviour - Open-loop test methods
ISO 4138:2004. 4th edn., June 2012
2. Jazar, R.N.: Vehicle Dynamics: Theory and Application, 2nd edn. Springer (2014),
ISBN:978-1-4614-8544-5
3. Anwar, S.: Yaw stability control of an automotive vehicle via generalized predictive
algorithm. In: Proceedings of the 2005, American Control Conference, pp. 435–440,
June 2005, doi:10.1109/ACC.2005.1469974
4. Di Cairano, S., Tseng, H.E., Bernardini, D., Bemporad, A.: Vehicle yaw stabil-
ity control by coordinated active front steering and diﬀerential braking in the
tire sideslip angles domain. IEEE Trans. Control Syst. Technol. 21(4), 1236–1248
(2013), 1063-6536
5. Schramm, D., Hiller, M., Bardini, R.: Vehicle Dynamics: Modelling and Simulation,
1st edn. Springer (2014), ISBN 978-3-540-36045-2
6. Ghosh, J., Tonoli, A., Amati, N.: A torque vectoring strategy for improving the
performance of a rear wheel drive electric vehicle. In: 2015 IEEE Vehicle Power and
Propulsion Conference (VPPC), pp. 1–6, October 2015, doi:10.1007/11823285 121
7. Kaiser, G., Holzmann, F., Chretien, B., Korte, M., Werner, H.: Torque vectoring
with a feedback and feed forward controller – applied to a through the road hybrid
electric vehicle. In: 2011 IEEE Intelligent Vehicles Symposium (IV), vol. 4128, pp.
448–453, June 2011
8. Kaiser, G., Liu, Q., Hoﬀmann, C., Korte, M., Werner, H.: Torque vectoring for an
electric vehicle using an LPV drive controller and a torque and slip limiter. In:
2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pp. 5016–5021,
December 2012, doi:10.1109/CDC.2012.6426553
9. De Novellis, L., Sorniotti, A., Gruber, P., Pennycott, A.: Comparison of feed-
back control techniques for torque-vectoring control of fully electric vehicles. IEEE
Trans. Veh. Technol. 63(8), 3612–3623 (2014), doi:10.1007/11823285 121
10. Rajamani, R.: Vehicle Dynamics and Control, Mechanical Engineering Series, 1st
edn. Springer (2005), ISBN:978-0387263960
11. Rubin, D., Arogeti, S.: Vehicle yaw stability control using rear active diﬀerential
via Sliding mode control methods. In: 21st Mediterranean Conference on Control
and Automation, pp. 317–322, June 2013, doi:10.1109/MED.2013.6608740
12. Stoop, A.: Design and Implementation of Torque Vectoring for the Forze Racing
Car. Master’s thesis, Delft Center for Systems and Control (2014)
13. Kiencke, U., Nielsen, L.: Automotive Control Systems: For Engine, Driveline and
Vehicle, 2nd edn. Springer (2005), ISBN:978-3-540-26484-2
14. Zhao, C., Xiang, W., Richardson, P.: Vehicle Lateral Control and Yaw Stability
Control through Diﬀerential Braking, pp. 384–389, July 2006, doi:10.1109/ISIE.
2006.295624

Path and Velocity Trajectory Selection in an
Anticipative Kinodynamic Motion Planner for
Autonomous Driving
Jordi P´erez Talamino and Alberto Sanfeliu(B)
Institut de Rob´otica i Inform`atica Industrial (CSIC-UPC),
Parc Tecnol´ogic de Barcelona, 08028 Barcelona, Spain
{joperez,sanfeliu}@iri.upc.edu
http://www.iri.upc.edu/
Abstract. This paper presents an approach for plan generation, selec-
tion and pruning of trajectories for autonomous driving, capable of deal-
ing with dynamic complex environments, such as driving in urban scenar-
ios. The planner ﬁrst discretizes the plan space and searches for the best
trajectory and velocity proﬁle of the vehicle. The main contributions of
this work are the use of G2-splines for path generation and a method that
takes into account accelerations and passenger comfort for generating and
pruning velocity proﬁles based on 3rd order splines, both fulﬁlling kino-
dynamic constraints. The proposed methods have been implemented in
a motion planner in MATLAB and tested through simulation in diﬀerent
representative scenarios, involving obstacles and other moving vehicles.
The simulations show that the planner performs correctly in diﬀerent
dynamic scenarios, maintaining the passenger comfort.
Keywords: Autonomous driving · Urban · Anticipation · Kinodynamic
motion planning · Path planning · G2-splines, Velocity proﬁles
1
Introduction
Nowadays autonomous driving is becoming more and more popular due to its
numerous beneﬁts, and motion planning is a key element [1,2]. Motion plan-
ners need to generate candidate geometric paths that will be followed by a low
level control system. Popular techniques, such as B´ezier curves, present a high
complexity generation from the road shape, with the presence of non-intuitive
geometric waypoints and multitude of parameters [3].
On the other hand, velocity proﬁle generation methods often use trapezoidal
proﬁles due to their simplicity despite its dynamic limitations because they have
discontinuities in the acceleration [4]. Better approaches compute spline-based
velocity trajectories over time, ﬁxing the total maneuver time [5] or also over
the road length [6]. Nevertheless, these time conditions make the discretization
process diﬃcult and lead even to the need of a post-optimization.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_36

Trajectory Selection in an Anticipative Kinodynamic Vehicle Motion Planner
435
We explain in Sect. 2 of this article the anticipative kinodynamic motion
planner that is used; in Sect. 2.1, the G2-splines for path generation; in Sect. 2.2,
the velocity proﬁle generation; and in Sect. 2.3 the cost structure. Finally in
Sect. 3 we explain the simulations and in Sect. 4, the conclusions.
2
Anticipative Kinodynamic Motion Planner Framework
Figure 1 shows the general scheme of the proposed motion planner.
Fig. 1. Motion planner framework
First of all, the planner discretizes the environment choosing several end-
points, which are state conﬁgurations X = [x, y, θ, κ], where x and y are the
position coordinates, θ is the heading and κ is the curvature, which is related
with the steering wheel angle using the bicycle kinematic model.
Then candidate paths are generated from the host vehicle state to the end-
points (Sect. 2.1). The motion planner uses a novel approach to generate paths,
G2-splines [7]. This method only needs a basic geometric state X for the ini-
tial and the ﬁnal points. It can approximate any kind of path shape preserving
a smooth curvature continuity and minimizing curvature variability, and this
implies kinematic feasibility for a vehicle following it: circular segments, straight
lines, clothoids, complete lane changes, etc.
Once the planner has a set of candidate paths, for each one of them it com-
putes a set of velocity proﬁles in order to anticipate dynamic obstacles (Sect. 2.2).
Each velocity proﬁle has an associated cost and the one with the minimum cost
is chosen, for a certain path. Finally, all the candidate paths with its associated
velocity proﬁles are compared with a cost structure (Sect. 2.3) and the minimum
cost solution (path and velocity proﬁle) is chosen to be the executed one. The
velocity proﬁles are computed using 3rd order splines, taking into account the
dynamic restrictions of the candidate path and the road path shape, and they
also are kinematically validated.
The output of the proposed motion planner is a kinematically and dynamically
feasible path with an associated velocity proﬁle for the host vehicle, also fulﬁlling
comfort restrictions for the passengers, such as bounded accelerations and jerk.

436
J.P. Talamino and A. Sanfeliu
2.1
G2-splines Path Generation
The G2-splines are geometric polynomials of 5th order presenting second order
geometric continuity (G2), so the curvature κ is continuous [7]. In order to build
a general path p(u), understood as a path with arbitrary deﬁned starting end-
point XA = [xA, yA, θA, κA], and ending endpoint XB = [xB, yB, θB, κB], the
equations to deﬁne these splines are:
p(u) =
x(u)
y(u)

:=
x0 + x1u + x2u2 + x3u3 + x4u4 + x5u5
y0 + y1u + y2u2 + y3u3 + y4u4 + y5u5

(1)
where u ∈[0, 1] and:
x0 = xA
x1 = η1 cos θA
x2 = 1
2(η3 cos θA −η2
1κA sin θA)
x3 = 10(xB −xA) −(6η1 + 3
2η3) cos θA −(4η2 −1
2η4) cos θB+
+3
2η2
1κA sin θA −1
2η2
2κB sin θB
x4 = −15(xB −xA) + (8η1 + 3
2η3) cos θA + (7η2 −η4) cos θB−
−3
2η2
1κA sin θA + η2
2κB sin θB
x5 = 6(xB −xA) −(3η1 + 1
2η3) cos θA −(3η2 −1
2η4) cos θB+
+1
2η2
1κA sin θA −1
2η2
2κB sin θB
y0 = yA
y1 = η1 sin θA
y2 = 1
2(η3 sin θA + η2
1κA cos θA)
y3 = 10(yB −yA) −(6η1 + 3
2η3) sin θA −(4η2 −1
2η4) sin θB−
−3
2η2
1κA cos θA + 1
2η2
2κB cos θB
y4 = −15(yB −yA) + (8η1 + 3
2η3) sin θA + (7η2 −η4) sin θB+
+3
2η2
1κA cos θA −η2
2κB cos θB
y5 = 6(yB −yA) −(3η1 + 1
2η3) sin θA −(3η2 −1
2η4) sin θB−
−1
2η2
1κA cos θA + 1
2η2
2κB cos θB
The resulting path depends on the parameter vector η that aﬀect its shape,
η = [η1, η2, η3, η4].
An important characteristic of the resulting spline is its curvature κ(u), which
can be computed using the following equation:
κ(u) = ˙x(u)¨y(u) −¨x(u) ˙y(u)
( ˙x(u)2 + ˙y(u)2)3/2
(2)

Trajectory Selection in an Anticipative Kinodynamic Vehicle Motion Planner
437
Fig. 2. η3 4 variations in a lane change maneuver (top) and η1 2 iterations approximat-
ing a circular path (bottom)
G2-splines optimization algorithm. The general case for the optimization
of the η values requires numerical optimization [7]. However, due to the type
of the required vehicle motion maneuvers, we can apply a faster solution that
does not need this numerical optimization, but a short iterative process. We
have realized that these trajectories have symmetrical behaviour between the
starting and ending points, then we can impose the following constraints to the
η parameters. η1 = η2 and η3 = −η4 so only 2 parameters are needed to be
tuned: η1 2 = η1 = η2 and η3 4 = η3 = −η4.
η3 4 ∈(−∞, +∞), and it aﬀects to the curvature changes. It can be concluded
that the best value for this parameter is 0 in all the cases. In Fig. 2, diﬀerent
η3 4 values are tested performing a lane change maneuver. As it can be seen,
if η3 4 > 0 (green trajectories) the curvature changes are concentrated in the
center of the trajectory, whereas η3 4 < 0 (blue trajectories) concentrate it on
the extreme initial and ﬁnal points. The red trajectory represents η3 4 = 0,
and it is the smoothest and more balanced trajectory, presenting the minimum
curvature variability.
On the other hand, η1 2 ∈(0, +∞), and it forces θ and κ to stay close to the
initial and ﬁnal values. It has been found that the smoothest trajectory is reached
when η1 2 is close to the trajectory length (meters). For this reason, an iterative
method is performed in order to converge into the best possible trajectory, giving
to η1 2 the length of the path. After a few iterations, usually 3 or 4, all the com-
puted trajectories converge into the smoothest one (Fig. 2). This process obtains
similar results as the proposed numerical optimization in [7], which minimizes

438
J.P. Talamino and A. Sanfeliu
the curvature variability

min
η
dκ
du

, but with much less computation allowing
to reach real time performance.
2.2
Velocity Proﬁle Generation
We explain three diﬀerent situations for the velocity proﬁle generation.
3rd Order Spline Without Initial Acceleration. The proposed 3rd order
spline in velocity has 4 variables: a, b, c, d (Eq. 3). The ﬁfth unknown is the total
time of the trajectory, T. The equation system is solved by applying initial and
ﬁnal conditions, and also using the fact that the velocity spline is symmetric, so
the maximum acceleration is achieved at time equal to T/2.
x(t) = a
4t4 + b
3t3 + c
2t2 + dt + x0
v(t) = at3 + bt2 + ct + d
a(t) = 3at2 + 2bt + c
(3)
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
v(0) = v0
a(0) = 0
v(T) = vf
a(T) = 0
a( T
2 ) = amax
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
b =
4a2
max
3(vf −v0)
a = −
b2
3amax
c = 0
d = v0
T = 3(vf −v0)
2amax
(4)
3rd Order Spline with Arbitrary Initial Acceleration. The 3rd order
spline in velocity is the same as in the case without initial acceleration. The
equation system is slightly diﬀerent because the spline is not symmetric and the
maximum acceleration is reached in the time t1. The 6 unknowns are the spline
variables a, b, c, d, the total time of the trajectory T and the time t1.
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
v(0) = v0
a(0) = a0
v(T) = vf
a(T) = 0
a(t1) = amax
a′(t1) = 0
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0 =

4a2
0
a0 −amax
−3a0

T 2+
+

6(vf −v0) −12(vf −v0)a0
a0 −amax

T +

9(vf −v0)2
a0−amax

b = 1
T

3(vf −v0)
T
−2a0

a =
b2
3(a0 −amax)
c = a0
d = v0
t1 = −b
3a
(5)

Trajectory Selection in an Anticipative Kinodynamic Vehicle Motion Planner
439
We have to solve a 2nd order equation to ﬁnd T1 and T2. Not in all cases there
exists a solution, due to the denominator value. amax must be always greater
than a0 if accelerating or lower when decelerating.
When only one of the Ti is positive, then this is the unique solution. In the
case when both Ti are positive corresponds to two possible solutions: increasing
acceleration up to amax and ﬁnishing faster the maneuver, or decreasing the
acceleration from a0 and ﬁnishing the maneuver in longer time. In this case, the
fastest maneuver is always chosen, so T = min(T1, T2).
The cases when the spline has no solution correspond to situations when
the vehicle is accelerating in the opposite direction of the desired ﬁnal speed, or
contradictory cases. This issue is solved by adding a linear section that starts in
a0 and ends in 0 acceleration, with a desired slope that guarantees comfort and
feasibility.
Velocity Proﬁle Generation Algorithm. The complete algorithm for gen-
erating a velocity proﬁle is detailed in Algorithm 1.
Algorithm 1. Generate a velocity proﬁle
if a0 = 0 then
Spline case without a0
else
if vf = v0 then
Add linear acceleration from a0 to 0
Spline case without a0
else
if contradictory case then
Add linear acceleration from a0 to 0
Spline case without a0
else
Spline case with arbitrary a0
end if
end if
end if
Set of Velocity Proﬁle Candidates.
This spline-based method gives a
smooth transition between two diﬀerent velocities, caused by the derivative of
the acceleration, the jerk, which is continuous. In addition, the possibility of
directly adjusting the parameter of maximum desired acceleration in the tra-
jectory is very useful for this application, as it is directly related with the time
needed for the trajectory and also with the comfort of the passengers and its
dynamic feasibility.
For each candidate path, the motion planner computes a set of velocity proﬁle
candidates, discretizing the ﬁnal velocities and also the accelerations, taking into
account the kinodynamics of the vehicle. Starting in the current state of the

440
J.P. Talamino and A. Sanfeliu
Fig. 3. Sets of all the velocity proﬁles, with a discretization of 0.5 m/s and diﬀerent
accelerations. In the right ﬁgure, cases with a lower ﬁnal velocity are contradictory
cases
host vehicle, several ﬁnal velocities are chosen from 0 to the maximum allowed
road/street velocity. Then, for each ﬁnal velocity, diﬀerent accelerations are also
chosen, from the maximum deceleration limit to the desired acceleration value.
Examples of these velocities and accelerations can be seen in Fig. 3.
In order to reduce the computation, we impose some constraints in the veloc-
ity proﬁle using the splines to reject the non-feasible candidates and restrict the
search space. The constraints are:
1. The distance to collide with a static obstacle, by using the distance in length
of the track from the host vehicle. If a collision is detected, the only allowed
ﬁnal velocity is 0.
2. The maximum allowed velocity due to the planned path curvature.
3. The maximum allowed velocity due to the center lane path curvature. This
restriction assures safety, in order to be more conservative and check a static
property of the track geometry, because the planned path could be smoothed
when re-launching.
4. The longitudinal dynamics of the vehicle. This restricts the accelerations to
ensure feasibility.
With this approach, we can have the required accelerations, and then we can
reduce the search space and prune the velocity proﬁle selecting the best ones.
2.3
Cost Structure
The chosen velocity proﬁle is the one with minimum dynamic cost. The motion
planner uses two kind of cost terms: static and dynamic. Dynamic costs are
related to the temporal dimension and they are associated to a velocity proﬁle
and include the dynamic obstacles (Table 1) and is computed as Jdynamic =

i wici. The costs are based on the method proposed in [6], but adapted to ﬁt
the proposed approach.

Trajectory Selection in an Anticipative Kinodynamic Vehicle Motion Planner
441
Table 1. Dynamic costs
Cost
Formula
Physical interpretation
Impact
cv
1 −vf,vp/vmax,desired
Velocity
Behaviour
ca
abs(amax,vp/amax,braking) Acceleration
Comfort & Eﬃciency
cobs,d f · e(−1/λ)·d
Dynamic obstacles repulsion Safety & Behaviour
Table 2. Static costs
Cost
Formula
Physical interpretation
Impact
cl
l/smaneuver
Path length
Eﬃciency
cκ
max(κ) · rmin Maximum path κ
Comfort & Kinematic feasibility
c ˙κ
max( ˙κ) · rmin Maximum path ˙κ
Comfort & Kinematic feasibility
coff
o/omax
Lateral oﬀset from centerline Behaviour
cobs,s f · e(−1/λ)·d
Static obstacles repulsion
Safety
vf,vp is the ﬁnal velocity of the candidate velocity proﬁle. vmax,desired is the
maximum allowed velocity at the current moment. amax,vp is the maximum accel-
eration value of the candidate velocity proﬁle. amax,braking is the acceleration
value (always < 0) of the maximum allowed braking.
In the other hand, static costs are the terms related to path geometry and
static obstacles and they are associated to a candidate path (Table 2). The cost
function is computed as the weighted sum of all the statics terms, Jstatic =

i wici.
All the terms (static and dynamic) with the exception of the obstacle terms,
are normalized between 0 and 1. smaneuver is the longitudinal distance from
the host vehicle and the ending point of the path. κ is the curvature value of
a speciﬁc point in the path. rmin is the minimum turning radius of the host
vehicle. omax is the lateral distance from the farthest endpoint to the center of
the lane (the point with more lateral oﬀset). f is a scale value and λ is the decay
of the exponential function. d is the distance from a obstacle (static or dynamic)
to the host vehicle (they have been modelled as circles). If d is smaller than a
threshold, then the cost is penalized:
cobs =

f · e(−1/λ)·d + Penalization,
if
d < threshold
f · e(−1/λ)·d,
otherwise
(6)
Unlike other approaches that penalize collisions with an inﬁnite cost, as [6],
it is preferred to preserve the value. Then, in a situation where all the candidate
solutions present high cost (for instance, an unavoidable obstacle), the planner
will give always the minimum cost solution, so it will be the less dangerous one.
Finally, for several maneuver candidates, formed by a path-velocity proﬁle
pair, a total cost function Jtotal is computed as the minimum cost maneuver,
Jtotal = minmaneuver(Jdynamic + Jstatic).

442
J.P. Talamino and A. Sanfeliu
3
Simulations
The motion planner and the low level vehicle control have been implemented
in MATLAB. The Stanley’s lateral controller [8] has been used together with
a longitudinal controller which takes into account the dynamics of the vehicle.
The next ﬁgures show representative road scenarios (with straight and curves
lanes), with the lines (in black), its center-lines (in blue) and the path followed
by the host vehicle (in green). The host vehicle (in red and purple) and the
velocity proﬁle are plotted at each interval of time. The simulations were made
in MATLAB with a slow CPU and the average computation for each simulation
cycle with static and dynamic obstacles was of 250 ms. Taken into account that
programming in C++ is around 10 times faster, this method can run at 25 ms.
Fig. 4. Simulation without any obstacle. Followed path (left) and vehicle velocity proﬁle
(right) in red and planned in blue
Fig. 5. Detail of a static obstacle avoidance maneuver

Trajectory Selection in an Anticipative Kinodynamic Vehicle Motion Planner
443
Fig. 6. Overtaking of a slower vehicle in a one way track (left) and in a two way track
with oncoming traﬃc (right). Red vehicle is the host.
Static Environment. Figure 4 shows a simulation in a road environment with-
out any obstacle in the lanes. Figure 5 shows the same environment, but with
several static obstacles, represented by circles in the lanes. This simulates an
urban scenario, for instance with vehicles parked in double row.
Dynamic Environment. Figure 6 shows two diﬀerent overtaking maneuvers
involving dynamic obstacles.

444
J.P. Talamino and A. Sanfeliu
Fig. 7. Incorporation in a T-intersection, in several instants of time. Conservative case
Fig. 8. Incorporation in a T-intersection, in several instants of time. Aggressive case

Trajectory Selection in an Anticipative Kinodynamic Vehicle Motion Planner
445
T-Intersection. To enter in a T-intersection is mandatory to anticipate the
oncoming vehicles, which come in both directions. Figure 7 is a conservative
case, where the host vehicle slows down and let the traﬃc pass. On the other
hand, Fig. 8 is a more aggressive case, where the host vehicle takes advantage of
a gap between vehicles.
4
Conclusions
After analyzing the simulations, it is veriﬁed that the proposed methods of path
and velocity proﬁle generation behaves correctly and help to overcome success-
fully all the presented situations, which correspond to representative cases that
occur in on-road driving.
The proposed approach for the path generation, using the G2-splines, per-
forms above expectations and a real-time computation can be obtained.
The proposed method for the velocity proﬁle generation behaves smoothly
and provides a fully analytic solution that can deal with any arbitrary situation.
Moreover, it allows to directly adjust the desired acceleration instead of the
maneuver time and this helps to the selection and pruning of a candidate set of
velocity proﬁles, taking into account kinodynamic and dynamic constraints.
References
1. Paden, B., C´ap, M., Zheng Yong, S., Yershov, D., Frazzoli, E.: A survey of motion
planning and control techniques for self-driving urban vehicles. IEEE Trans. Intell.
Veh. 1(1), 33–55 (2016)
2. Katrakazas, C., Quddus, M., Chen, W.-H., Deka, L.: Real-time motion planning
methods for autonomous on-road driving: State-of-the-art and future research direc-
tions. Transp. Res. Part C 60, 416–442 (2015)
3. Bautista, D.G., Rastelli, J.P., Lattarulo, R., Milan´es, V., Nashashibi, F.: Continuous
curvature planning with obstacle avoidance capabilities in urban scenarios. In: 17th
International IEEE Conference on Intelligent Transportation Systems (ITSC), pp.
1430–1435 (2014)
4. Ferguson, D., Howard, T.M., Likhachev, M.: Motion Planning in urban environ-
ments, Part I and II. In: IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pp. 1063–1069, 1070–1076 (2008)
5. Gu, T., Dolan, J.M.: On-road motion planning for autonomous vehicles. In: Inter-
national Conference on Intelligent Robotics and Applications (ICIRA), vol. 3, pp.
588–597 (2012)
6. Xu, W., Wei, J., Dolan, J.M., Zhao, H., Zha, H.: A real-time motion planner with
trajectory optimization for autonomous vehicles. In: IEEE International Conference
on Robotics and Automation (ICRA), pp. 2061–2067 (2012)
7. Bianco, C.G., Piazzi, A.: Optimal trajectory planning with quintic G2- splines. In:
Proceedings of the IEEE Intelligent Vehicles Symposium, pp. 620–625 (2000)
8. Hoﬀmann, G.M., Tomlin, C.J., Montemerlo, M., Thrun, S.: Autonomous automobile
trajectory tracking for oﬀ-road driving: controller design, experimental validation
and racing. In: American Control Conference (ACC), pp. 2296–2301 (2007)

Deadzone-Quadratic Penalty Function
for Predictive Extended Cruise Control
with Experimental Validation
Seyed Amin Sajadi-Alamdari1(B), Holger Voos1, and Mohamed Darouach2
1 Interdisciplinary Centre for Security, Reliability and Trust (SnT),
University of Luxembourg, 29, avenue JF Kennedy,
1855 Luxembourg City, Luxembourg
{amin.sajadi,holger.voos}@uni.lu
2 Centre de Recherche en Automatique de Nancy (CRAN) UMR-CNRS 7039,
Universit´e de Lorraine, IUT de Longwy, 186 rue de Lorraine,
54400 Cosnes et Romain, France
mohamed.darouach@univ-lorraine.fr
https://www.uni.lu
Abstract. Battery Electric Vehicles have high potentials for the mod-
ern transportations, however, they are facing limited cruising range. To
address this limitation, we present a semi-autonomous ecological driver
assistance system to regulate the velocity with energy-eﬃcient tech-
niques. The main contribution of this paper is the design of a real-time
nonlinear receding horizon optimal controller to plan the online cost-
eﬀective cruising velocity. Instead of conventional ℓ2-norms, a deadzone-
quadratic penalty function for the nonlinear model predictive controller
is proposed. Obtained ﬁeld experimental results demonstrate the eﬀec-
tiveness of the proposed method for a semi-autonomous electric vehicle
in terms of real-time energy-eﬃcient velocity regulation and constraints
satisfaction.
Keywords: Deadzone penalty function · Nonlinear model predictive
control · Ecological advanced driver assistance system · Electric vehicles
1
Introduction
Battery Electric Vehicle (BEV) has one of the most promising powertrain tech-
nology for the predictable future transportations [1]. However, the BEVs have
limited onboard energy capacity, which limits their cruising range on a sin-
gle charge. Several methods have been developed to extend the cruising range
such as the Ecological Advanced Driver Assistance Systems (Eco-ADAS) with
anticipated driving style. For the automotive systems, receding horizon optimal
control also known as Model Predictive Control (MPC) has been an attractive
approach. In MPC, an Optimal Control Problem (OCP) is solved repeatedly in
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_37

NMPC with Deadzone-Quadratic Penalty Function
447
a receding horizon principle. The ﬁrst element in a sequence of ﬁnite control
actions is applied to the system at each sampling time.
The well-established modern Cruise Control (CC) systems automate the
throttle and brake control of the vehicle to retain the pre-set longitudinal veloc-
ity. Several works of literature may be founded, such as [2], where a sequen-
tial optimisation approach was presented for connected CC system. In [3,4], an
energy-eﬃcient linear MPC that use the energy consumption map of a BEV was
established. Despite the eﬀectiveness of the linear MPCs to some extent, they
have limitations. In order to improve the performance speciﬁcations, Nonlinear
MPC (NMPC) is distinguished by the use of nonlinear system models in the
OCP. An instance work of the NMPC for the Eco-CC system considering up-
down road slopes with an Internal Combustion Engine (ICE) fuel consumption
model was presented in [5]. An Extended Eco-CC (Ext-Eco-CC) system that
considers further road curves and traﬃc speed limit areas for the BEVs was
introduced in [6]. Comparison and assessment of energy consumption models,
cost functions, and solution methods of the Eco-CC system for passenger vehi-
cles were reviewed in [7,8]. Considering the general class of (residual) penalty
functions used in the NMPC, the ℓ2-norm is preferred in practice due to its eﬃ-
ciency in implementation. The quadratic penalty function yields least-square or
Euclidean norm approximation [9]. The ℓ2-norm is preferred for energy-eﬃciency
applications. However, the NMPC based on ℓ2-norm associated to states may
also lead to aggressive system behaviour [10]. As an alternative, a systematic
way of dealing with large state residuals based on Huber function was proposed
in [10]. The Huber function, φM(x) is equivalent to a ℓ2-norm within the region
[−M, M] and to a ℓ1-norm outside. The ℓ1-norm is preferred for robust regula-
tions where the absolute value penalty function yields ℓ1-norm approximation.
Thus, the sensitivity to outliers or large residuals is lower than the ℓ2-norm [9].
Although most of the mentioned NMPCs are based on agile and intuitive set-
point tracking, this may not be a suitable strategy for the energy-eﬃcient state
regulation. One of the main reason for high energy consumption of the system is
strict achieving and tracking the set-point. In this paper, we propose a deadzone-
quadratic and deadzone-linear penalty functions that have the advantages of ℓ2
and ℓ1-norms respectively. This method preserves the energy-eﬃcient behaviour
within the desired operating zone. The main idea of the deadzone-quadratic
penalty function is to assess low penalty or insensitivity for residuals smaller
than the deadzone width and quadratic or linear penalty for bigger residuals.
This motivates to ﬁnd a tradeoﬀbetween the agile set-point tracking and energy-
eﬃcient strategy.
The main contribution of this work is to design a real-time NMPC with
deadzone-quadratic penalty function to enhance the Ext-Eco-CC system for the
BEVs. For this purpose, the components are considered to develop a system
model. First, the BEV longitudinal dynamics, its energy consumption, as well
as road geometry and traﬃc sign information are modelled in a reasonably accu-
rate framework. Second, a real-time nonlinear receding horizon optimal controller
is designed to plan the online cost-eﬀective cruising velocity. Then, the NMPC

448
S.A. Sajadi-Alamdari et al.
takes advantage of a convex deadzone-quadratic penalty function for velocity
tracking within desired reference zone. Finally, the performance of the proposed
concept is evaluated in terms of energy-eﬃcient velocity regulation and con-
straints fulﬁlment.
The remainder of this paper is structured as follows: The system model is
introduced in Sect. 2. The NMPC formulation with deadzone-quadratic penalty
function is presented in Sect. 3. Section 4 includes ﬁeld experimental validation
of the proposed concept, followed by the conclusion and future work in Sect. 5.
Notation
Throughout this paper, Rn denotes the n-dimensional Euclidean space. R+ :=
[0, ∞). N = {1, 2, . . .} is set of natural numbers. N+ := N ∪{0} and Z[a,b] :=
{a, a + 1, . . . , b} is set of integers from a to b.
2
System Model
Safe and energy-eﬃcient velocity proﬁle identiﬁcation has a signiﬁcant improve-
ment on extending the cruising range of a BEV. The semi-autonomous BEV
concept that extends the functionalities of an Eco-CC system is presented in
Fig. 1. Similar to the modern CC systems, the driver pre-sets the desired veloc-
ity. The Semi-autonomous Ext-Eco-CC system predictively regulates the velocity
with respect to the longitudinal motion of the vehicle, its energy consumption
dynamics, road geometric navigation data, and traﬃc sign information. While
the driver handles the steering control of the vehicle, this system should plan a
proper energy-eﬃcient cruising velocity proﬁle autonomously for the entire trip
without requiring the driver interventions. For more details about the proposed
Ext-Eco-CC system, see [6].
Vehicle and
Energy Dynamics with
Position Information
Road and Trafﬁc
Information
Nonlinear Model
Predictive Controller
Traction Input
Discharging
Charging
GPS
Fig. 1. Semi-autonomous BEV Ext-Eco-CC system

NMPC with Deadzone-Quadratic Penalty Function
449
2.1
Vehicle and Energy Dynamics
The position (s) and velocity (v) along the longitudinal motion of the BEV can
be expressed by Newton’s second law of motion, which it is assumed to be a
point mass at the centre of gravity as follows:
˙s = v,
(1)
˙v = (Ftrac −Fres)/M,
(2)
where M, Ftrac(t), and Fres(t) are equivalent mass of the vehicle, traction force,
and total motion resistive forces, respectively [1]. The traction force (throttle and
brake pedals) depends on the equivalent mass and control input as Ftrac(t) :=
Mu(t). The control input is bounded (umin(v) ≤u(t) ≤umax(v)) by the physical
limits of the traction force the wheel-road contact can support without slip.
The main total resistive force including aerodynamic drag, gradient, and rolling
resistance forces represented by:
Fres = 1
2ρAfCDv2 + Mg sin(θ(s)) + Crr(v)Mg cos(θ(s)),
(3)
where ρ, Af, CD, g, θ(s), and Crr(v), are the air density, the vehicle frontal
area, the aerodynamic drag coeﬃcient, the gravitational acceleration, the road
slope angle as a function of the host vehicle position, and the velocity dependent
rolling resistance coeﬃcient, subsequently. The rolling resistance coeﬃcient for
passenger vehicles on a concrete road can be approximated as Crr(v) = 0.01(1+
v/576) [1].
Energy consumption of a BEV depends on a number of factors including
driven velocity, acceleration proﬁle, geometric characteristics of roads, and traﬃc
situations. For a given velocity at a given traction force, the operating point of
the electric machine and the related power consumption or regeneration could
be determined [6]. The energy consumption during cruising at constant speed
is equal to the resistive power. This can be approximated through the curve-
ﬁt process with measurement data by a polynomial of velocity as fcruise =
b3v3 +b2v2 +b1v +b0 and acceleration as fa = a2u2 +a1u+a0. Therefore, at any
given velocity and control input, a linear relation of the traction power-to-mass
ratio can describe the energy consumption of the BEV as:
˙e = fa (ptrac/M) + fcruise,
(4)
where ptrac = Ftracv, denotes the traction power. This model is capable of
capturing the energy consumption of a BEV including the regenerative braking
for the full-range of velocity and the control input (for more details, see [6]).
2.2
Road Geometry and Traﬃc Model
Road geometries and traﬃc information have favourable advantages for the
ADAS safety and energy management applications [11]. In [6], the road slopes,

450
S.A. Sajadi-Alamdari et al.
road curves, and traﬃc speed limit zone data are modelled as continuous and
diﬀerentiable functions. In that method, the road slope proﬁle (fslp(θ(s))) is pro-
posed to be the sum of quadratic functions of the vehicle position representing
each road segments slope data as follows:
fslp(θ(s)) :=
Nsgm

n=1
Hn
(s−sn−1)(ans2 + bns + cn)Hn
(s−sn),
(5)
where Nsgm is the number of road segments, Hn
(s−sn−1) and Hn
(s−sn) are hyper-
functions of the nth road segment. These functions represent the data points in
each segment of the road utilising hyper-function concept to interconnect the
estimated segments of the road at the boundaries positions, sn−1 and sn.
The road curves and traﬃc speed limits proﬁles are modelled in a similar
way [6]. The simple curve is used to express the total absolute road curve proﬁle
(fcrv(δ(s))) which is deﬁned as:
fcrv(δ(s)) :=
Ncrv

n=1
Hn
(s−sent)

1
Rcrvn(s)
 Hn
(s−sext),
(6)
where Ncrv is the number of road curves, and Rcrvn is the radius of a circle
valid for the curve’s arc length with two position points, sent and sext, at the
respective entrance and exit positions. Furthermore, the traﬃc speed limit proﬁle
(flmt(s)) can be modelled as:
flmt(s) :=
Nlmt

n=1
Hn
(s−sstr)(vlmt −vmax)Hn
(s−send) + vmax,
(7)
where Nlmt is the number of speed limit zones, and vlmt is the speciﬁed speed
limit value at positions starts from sstr up to the end of the zone send. The vmax
is the maximum speed value of the electric vehicle (for more details, see [6]).
3
Optimal Control and Penalty Functions
For the sake of completeness, a general NMPC formulation will be reviewed.
Next, the deadzone-quadratic and deadzone-linear penalty functions will be
introduced.
3.1
Nonlinear Model Predictive Control
Consider a general discrete-time system:
xt+1 = f(xt, ut),
(8)
where t ∈N+; xt ∈Rnx is the system states vector, and ut ∈U ⊂Rnu is a non-
empty measurable set for the inputs. The f(·) is nonlinear Borel-measurable

NMPC with Deadzone-Quadratic Penalty Function
451
vector of functions that describes the system dynamics. Let N ∈N be the both
state and control prediction horizon. Deﬁne an N-stage feedback control policy as:
πππ := {π0(·), π1(·), . . . , πN−1(·)},
(9)
where the Borel-measurable function πi(·) : R(i+1)nx →U, for all i = 0, . . . , N −1
is a general state feedback control law. The control input ui is selected as the
feedback control law ui = πi(·) at the ith stage of the control policy. In receding
horizon optimal control, the cost function of an OCP is commonly deﬁned as:
VN(xt,πππ) :=
N−1

i=0
Jc(ˆxi, ui) + Jf(ˆxN),
(10)
where Jc : Rnx × U →R+ and Jf : Rnx →R+ are the cost-per-stage function
and the ﬁnal cost function, respectively, and ˆxi denotes the predicted states at
time i given the initial states ˆx0 = xt, and control law {πi(·)}N−1
i=0 .
Using the cost function (10), the OCP for (8) is formulated as follows:
V ∗
N(xt) := minimise
π
VN(xt,πππ)
(11a)
subject to:
ˆxi+1 = f(ˆxi, πi),
for all i ∈Z[0,N−1],
(11b)
πi(·) ∈U,
for all i ∈Z[0,N−1],
(11c)
gj(ˆxi) ≤0,
for all j = 1, . . . , s, and i ∈Z[0,N−1],
(11d)
ˆx0 = xt,
(11e)
where V ∗
N(xt) denotes the optimal value function under the optimal control pol-
icy πππ∗. The inequality state constraints are denoted by gj(ˆxi) that are required
to be fulﬁlled. The OCP in receding horizon principle involves applying the ﬁrst
element of the control action sequence ut = πππ∗
0(·) repeatedly to the system at
each sampling time. For more details and the ﬁrst-order necessary conditions for
a solution of the OCP see e.g. [9,12].
3.2
Deadzone Penalty Functions
In many practical NMPC applications considering the energy-eﬃciency, it is
desirable to reach a region of reference set-points with relatively low-cost value
rather than costly but accurate and agile set-point tracking. This could be accom-
plished using a nonnegative and symmetric deadzone-quadratic penalty function
such as:
φq(x) :=

0
: |x| ≤z,
x2 −z2
: |x| > z,
(12)
where z is the edge of free zone that no penalty is assessed if |x| ≤z. The φq(·)
function agrees with least-square for any residual outside of the zone width. In
other words, the residuals smaller than the zone width are ignored which lead
to low-cost function value.

452
S.A. Sajadi-Alamdari et al.
In a case of energy-eﬃcient robust regulations, deadzone-linear penalty func-
tion agrees with absolute value for the residual outside of the zone width as
follows:
φl(x) :=

0
: |x| ≤z,
|x| −z
: |x| > z.
(13)
Unfortunately, these deadzone penalty functions are not convex which lead to
a challenging OCPs. However, a smooth approximation of deadzone penalty
function may address the challenge.
In this paper, a deadzone penalty function based on softplus rectiﬁer is pro-
posed. The softplus is an approximation to the activation function so-called
Rectiﬁed Linear Unit (ReLU) which is mostly utilised in the deep neural net-
works [13]. The proposed deadzone-linear penalty function is a combination of
the two softplus as follows:
ψl(x) := ln(1 + exp(x −z)) + ln(1 + exp(−x −z)).
(14)
The ψl(x) have advantages such as being a convex function with eﬃcient compu-
tation and gradient propagation [14]. The gradient of the deadzone-linear penalty
function is a combination of two sigmoid functions as follows:
dψl(x)
dx
=
exp(x −z)
1 + exp(x −z) −
exp(−x −z)
1 + exp(−x −z).
(15)
Similar to ψl(x), the deadzone-quadratic penalty function can be formulated as
follows:
ψq(x) := (ln(1 + exp(x −z)) + ln(1 + exp(−x −z)))2.
(16)
The gradient of the deadzone-linear penalty function is a linear continuous func-
tion with a deadzone area, [−z, z], as follows:
dψq(x)
dx
= 2ψl(x)dψl(x)
dx
.
(17)
For sake of simplicity, Fig. 2 shows the proposed ψq(x) and ψl(x) penalty func-
tions for a scalar residual with z = 5 in comparison with φq(x), φl(x), ℓ2, and
ℓ1-norms. Note that when the state residual is within the zone, the gradient is
non-zero and the optimality conditions are satisﬁed as ℓ1 and ℓ2-norms. In other
words, the states will converge to ﬁnal reference set-point values but slower than
conventional norms which lead to the energy-eﬃcient behaviour.
3.3
Case Study: Ext-Eco-CC Mathematical Optimisation Problem
The state vector for the Ext-Eco-CC system from Eqs. (1), (2), and (4), is deﬁned
as xt = [s, v, e]T ∈R3; the control input is the traction input applied on BEV,
ut = u ∈U ⊂R (for more details see [6]); Please note that all states are
measurable and the measurement noise is negligible.

NMPC with Deadzone-Quadratic Penalty Function
453
Fig. 2. Deadzone penalty functions with relative comparisons
The cost-per-stage function for Ext-Eco-CC system is deﬁned as:
Jc(ˆxi, ui) :=
N−1

i=0
1
2[ψq(ˆxi −xref)Q+ ∥ui −uref ∥2
R],
(18)
with corresponding weights (Q and R). The ﬁnal cost function for Ext-Eco-CC
system is deﬁned as:
Jf(ˆxN) := 1
2ψq(ˆxN −xref)Q.
(19)
The lateral acceleration of the BEV should be lower than the comfort level
(ˆωref) as inequality constraint as follows:
g1(ˆsi, ˆvi) := ˆv2
i /fcrv(δ(ˆsi)) ≤ˆωref,
(20)
The velocity of the BEV should also be lower than speed limit zones as:
g2(ˆsi, ˆvi) := ˆvi ≤flmt(ˆsi).
(21)
In addition, the velocity should be within the standstill and the reference set-
point so-called funnel concept (see e.g., [15]) as follows:
g3(ˆvi) := 0 ≤ˆvi ≤vref + vrlx
(22)
where vref is the reference set-point and vrlx is the relaxed velocity for the
inequality constraint. One of the eﬃcient methods to solve the resulting OCP in
the receding horizon manner can be based on Pontryagin’s Minimum Principle.
The obtained dual OCP can be solved eﬃciently in real-time by the Continua-
tion and Generalised Minimal RESidual (C/GMRES) with a proper inequality
constraints handling method (for more details see [12,16]).
4
System Evaluation
The proposed Ext-Eco-CC system has been evaluated with practical experiments
on a test track using realistic values of the parameters. A Smart Electric Drive

454
S.A. Sajadi-Alamdari et al.
Fig. 3. Smart fortwo electric drive
c⃝OpenStreetMap contributors
Start Point
1stCurve
2ndCurve
3rdCurve
4thCurve
s = 500
s = 850
Fig. 4. Centre de Formation pour Conducteurs (CFC) [17]
third generation (Smart-ED) commercial BEV, which is available for practical
experiments, is chosen here to model the dynamics of a BEV and its energy
consumption (Fig. 3). A closed test track located at Colmar-Berg, Luxembourg,
(CFC) is chosen to model the road geometry with traﬃc information (Fig. 4).
The test track has a total length of 1.255 km and includes curves and relative
slope proﬁle. This track has four main curves with 20 m, 25 m, 15 m, and 27 m
radius. Note that the speed limit zone is not imposed in system evaluation in
order to simplify the system evaluation (for more detail, see [6]).
4.1
Experimental Setup
In order to validate the proposed concept, the NMPC with deadzone-quadratic
penalty function is experimentally implemented on the Smart-ED vehicle and the
Ext-Eco-CC system is tested on the CFC test track. The position of the Smart-
ED is updated by the Global Positioning System (GPS) sensor. The velocity
and energy consumption of the vehicle including the battery current and voltage
information is updated by the Controller Area Network (CAN-bus) through the
On-Board Diagnose (OBD) interface. The onboard computational resource for
the Ext-Eco-CC concept is foreseen by a Linux operating system on the Intel R
⃝
Core
TM i7 with a memory of 7.7 GiB PC and connection panel. The connection

NMPC with Deadzone-Quadratic Penalty Function
455
Fig. 5. The Linux operated PC with connection panel
panel is developed for the system power supply and actuators communication
(Fig. 5).
The control input of the proposed NMPC with deadzone-quadratic penalty
function is realised by actuating either the accelerator pedal or brake actua-
tor. The accelerator pedal is replaced by an electronic board (E-accelerator) to
manipulate the required acceleration and imitates the electric signals generated
by the original accelerator pedal of the Smart-ED. The brake actuator is manip-
ulated by an electric stepper motor that is connected to the brake pedal by a
planetary gearbox and ﬂexible cable. The automatic brake actuation is designed
in a way that preserves the possibility for the driver to brake in emergency cases.
Figure 6 shows the conﬁguration of the E-accelerator and brake actuators for the
Ext-Eco-CC system.
Fig. 6. Automatic accelerator and brake actuators
4.2
Experimental Results Validation
In order to show the performance of the proposed Ext-Eco-CC system, a pre-
diction horizon for the predictive controller is set to T = 15 s, to cover upcom-
ing road geometry, and traﬃc speed limit zone with N = 30 discretized steps.

456
S.A. Sajadi-Alamdari et al.
The constants in performance index function are set as Q = diag[0, 2, 0], and
R = diag[450]. Note the weight for energy-consumption is set to zero since the
eﬀectiveness of the deadzone-quadratic penalty function in energy eﬃciency is
the main focus in this paper. The reference for the lateral acceleration comfort
level is ωref = 3.7 m/s2.
We have compared our proposed deadzone-quadratic NMPC (DQ-NMPC)
with the conventional NMPC with ℓ2-norm (C-NMPC) and human driver (HD)
in terms of velocity regulation, travel time (t), power consumption proﬁle and
total energy-consumption (e). For the sake of fair comparison, all of the tests
started from the standstill and the maximum reference velocity value is chosen,
vref = 100 km/h without imposing speed limit zone. The desired reference zone
for velocity tracking is chosen as z = 2 m/s. We have proposed human driver to
drive as fast and energy-eﬃcient as possible.
Figure 7a shows the performance of various tests in terms of velocity regula-
tions and total travel time. The DQ-NMPC and C-NMPC increase the velocity
up to reaching the ﬁrst curve (220 ≤s ≤270) where the lateral acceleration con-
straint should be satisﬁed. As it is shown, the human driver is faster than the
controllers. However, during the ﬁrst and second curves (320 ≤s ≤440), the con-
trollers and human drivers show similar behaviour. Afterwards, the controllers
increase over again the velocity up to the point the third curve (860 ≤s ≤930)
are in their prediction horizon. This leads to the beginning of slowing down
predictively to satisfy the upcoming constraints in an energy-eﬃcient way. The
human driver show late but sharper velocity reduction which may not be an
energy-eﬃcient technique. Finally, the controllers keep the velocity during the
fourth curve (930 ≤s ≤1045) and speed up once more to reach the starting
point on the test track. Thus, the presented result shows that the maximum
reference velocity is not reachable, however, the reference velocity for less than
Fig. 7. Experimental results of DQ-NMPC in compare with C-NMPC and Human
Driver for (a) Velocity regulation, and (b) Power consumption proﬁle.

NMPC with Deadzone-Quadratic Penalty Function
457
vref = 80 km/h is reachable on the experimental tests carried on the CFC track.
Figure 7b shows the power consumption proﬁle and total energy. Note that neg-
ative power consumption refers to energy recovery mechanism.
Figure 8a and b show the velocity and power consumption normalised
histogram information. The proposed DQ-NMPC beneﬁts from an improved
penalty function which leads to a denser velocity and power consumption distri-
bution compared to the C-NMPC and human driver. Based on achieved results,
it is shown that the set-point value is not reachable on the test track by the
controllers or the human driver. The DQ-NMPC leads to more steady velocity
proﬁle and consequently the better drive comfort with relatively small increased
travel time. The total energy consumption of DQ-NMPC is +13.65% more energy
eﬃcient than the human driver and +6.58% more energy eﬃcient than the C-
NMPC. In other words, for longer trips with more hilly and curvy roads, our
proposed method has higher potential to be more energy-eﬃcient. It is notewor-
thy that the OCP average calculation time for the DQ-NMPC is 2.35 ms which
indicate its real-time capability of the proposed controller.
Fig. 8. Experimental performance distribution of DQ-NMPC in compare with
C-NMPC and the Human Driver for (c) Velocity and (d) Power consumption.
5
Conclusion and Future Research
A semi-autonomous ecological driver assistance system was developed to regu-
late the velocity in an energy-eﬃcient manner for the electric vehicles. A real-
time nonlinear receding horizon optimal controller with approximate deadzone-
quadratic penalty function was proposed to plan online the cost-eﬀective velocity
proﬁle. The limited cruising range of the electric vehicles was improved by the
assessed low penalty value on set-point tracking zone and ecological driving tech-
niques. Various tests on a semi-autonomous electric vehicle in terms of real-time

458
S.A. Sajadi-Alamdari et al.
states regulation and constraints fulﬁlment were carried out. The eﬀectiveness
of the proposed method was demonstrated by the achieved ﬁeld experimental
results. Further practical experiments will be conducted including extending the
functionalities of semi-autonomous ecological driving.
References
1. Ehsani, M., Gao, Y., Emadi, A.: Modern Electric, Hybrid Electric, and Fuel Cell
Vehicles: Fundamentals, Theory, and Design, 2 illustr edn. CRC Press, Boca Raton
(2009)
2. Li, N.I., He, C.R., Orosz, G.: Sequential parametric optimization for connected
cruise control with application to fuel economy optimization. In: 2016 IEEE 55th
Conference on Decision and Control (CDC), pp. 227–232. IEEE, Las Vegas (2016).
https://doi.org/10.1109/CDC.2016.7798274
3. Schwickart, T., Voos, H., Hadji-Minaglou, J.R., Darouach, M., Rosich, A.: Design
and simulation of a real-time implementable energy-eﬃcient model-predictive
cruise controller for electric vehicles. J. Franklin Inst. 352(2), 603–625 (2015).
https://doi.org/10.1016/j.jfranklin.2014.07.001
4. Schwickart, T., Voos, H., Hadji-Minaglou, J.R., Darouach, M.: A fast model-
predictive speed controller for minimised charge consumption of electric vehicles.
Asian J. Control 18(1), 133–149 (2016). https://doi.org/10.1002/asjc.1251
5. Kamal, M.A.S., Mukai, M., Murata, J., Kawabe, T.: Ecological vehicle control
on roads with up-down slopes. IEEE Trans. Intell. Transp. Syst. 12(3), 783–794
(2011). https://doi.org/10.1109/TITS.2011.2112648
6. Sajadi-alamdari, S.A., Voos, H., Darouach, M.: Nonlinear model predictive
extended eco-cruise control for battery electric vehicles. In: 24th Mediterranean
Conference on Control and Automation (MED), pp. 467–472. IEEE, Athens (2016).
https://doi.org/10.1109/MED.2016.7535929
7. Saerens, B., Rakha, H., Diehl, M., Van den Bulck, E.: A methodology for assessing
eco-cruise control for passenger vehicles. Transp. Res. Part D Transport Environ.
19, 20–27 (2013). https://doi.org/10.1016/j.trd.2012.12.001
8. Sciarretta, A., De Nunzio, G., Ojeda, L.L.: Optimal ecodriving control: energy-
eﬃcient driving of road vehicles as an optimal control problem. IEEE Control
Syst. 35(5), 71–90 (2015). https://doi.org/10.1109/MCS.2015.2449688
9. Boyd, S., Vandenberghe, L.: Convex Optimization. Cambridge University Press,
Cambridge (2004)
10. Gros, S., Zanon, M.: Penalty functions for handling large deviation of quadrature
states in NMPC. IEEE Trans. Autom. Control 9286(c), 1–1 (2017). https://doi.
org/10.1109/TAC.2017.2649043
11. Eskandarian, A. (ed.): Handbook of Intelligent Vehicles, vol. 2. Springer, London
(2012). https://doi.org/10.1007/978-0-85729-085-4
12. Ohtsuka, T.: A continuation/GMRES method for fast computation of nonlinear
receding horizon control. Automatica 40(4), 563–574 (2004). https://doi.org/10.
1016/j.automatica.2003.11.005
13. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444
(2015)
14. Dugas, C., Bengio, Y., Belisle, F., Nadeau, C., Garcia, R.: Incorporating second-
order functional knowledge for better option pricing. In: Proceedings of the 13th
International Conference on Neural Information Processing Systems, NIPS 2000,
pp. 451–457. MIT Press, Cambridge (2000)

NMPC with Deadzone-Quadratic Penalty Function
459
15. Maciejowski, J.M.: Predictive Control: With Constraints. Prentice Hall, Pearson
Education (2002)
16. Huang, M., Nakada, H., Butts, K., Kolmanovsky, I.: Nonlinear model predictive
control of a diesel engine air path: a comparison of constraint handling and compu-
tational strategies. IFAC-PapersOnLine 48(23), 372–379 (2015). https://doi.org/
10.1016/j.ifacol.2015.11.308
17. Centre de Formation pour Conducteurs. http://www.cfc.lu/

Autonomous Driving and Driver
Assistance Systems (II)

Comparative Study of Visual Odometry
and SLAM Techniques
Ana Rita Gaspar(B), Alexandra Nunes, Andry Pinto, and Anibal Matos
INESC TEC, Porto, Portugal
{argaspar,apn,andry.m.pinto}@inesctec.pt, anibal@fe.up.pt
Abstract. The use of the odometry and SLAM visual methods in
autonomous vehicles has been growing. Optical sensors provide valu-
able information from the scenario that enhance the navigation of
autonomous vehicles. Although several visual techniques are already
available in the literature, their performance could be signiﬁcantly
aﬀected by the scene captured by the optical sensor. In this context,
this paper presents a comparative analysis of three monocular visual
odometry methods and three stereo SLAM techniques. The advantages,
particularities and performance of each technique are discussed, to pro-
vide information that is relevant for the development of new research
and novel robotic applications.
Keywords: Evaluation · Odometry · SLAM · Vision
1
Introduction
The increased use of the autonomous vehicles is related to the fact that their
properties allow its application in diverse tasks that can be dangerous and repet-
itive for the human. To ensure that vehicles are completely involved in vari-
ous applications it is relevant to augment their capacity to reliably navigate
autonomously even in unknown environments. In this context, a large eﬀort is
being made by the researchers to explore the concepts of the odometry and
SLAM (Simultaneous Localization and Mapping) in order to support the activ-
ity of mobile robots in diﬀerent scenarios. Odometry allows estimation of the
robot’s position from a single reference and the SLAM technique localizes the
robot and constructs a map of the environment. Therefore, SLAM techniques can
use odometry-based methods to provide an estimation of motion. The biggest
advantage of SLAM is related with the revisiting capability, which means that,
the technique reduces the positioning error along the navigation path once a
revisited area is detected. Optical systems have the ability to provide informa-
tion with high quality at a reasonable cost. Therefore, the development of visual
odometry and SLAM approaches have been an active line of research that was
followed by a large number of institutions worldwide. The appearance of several
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_38

464
A.R. Gaspar et al.
visual-based techniques have triggered a fundamental question: what technique
is suitable for a speciﬁc application? Therefore, the major contribution of this
paper include: to provide a comparative study of some of the visual odometry
and SLAM techniques that are currently available in the literature. Moreover,
this paper discusses the performance of these methods for typical applications
related to mapping and data fusing with other sensors. Thus, it is important to
highlight this kind of studies because it allows to understand the main properties,
advantages and disadvantages of each implementation, as well as its results in dif-
ferent environments and testing conditions. Therefore, it is possible to select the
best and more convenient method for particular applications. This paper is orga-
nized as follows: Sect. 2 presents conventional methods for visual odometry and
SLAM. The comparative study is presented in Sect. 3, where the results obtained
by several techniques are evaluated in common testing scenarios. Finally, Sect. 4
shows the major conclusions of this paper.
2
Methods
As a prerequisite of the many tasks that involve the robot motion, the localiza-
tion is the most crucial feature for an autonomous robot. In this sense, the visual
odometry estimates motion with only input images from of one or multiple cam-
eras. The use of visual odometry presents advantages compared to traditional
method (encoders in wheels) since it is more reliable in slipping events, namely
in rugged lands where drift errors can occur frequently. However, the analysis of
egomotion from sequence of images are very complicated due to the presence of
the external objects moving in the scenario (which violates the motion coherent
assumption [1]). It is also necessary to ensure that the rate of acquisition is fast
enough to avoid any aliasing phenomenon (and to increase the overlapping area
between images). On the other hand, the SLAM is a more soﬁscated approach
that constructs a local map according to the navigation of the robot in the envi-
ronment. Within this map, it provides the estimation of the robot’s position. The
implementation of visual approaches for both the odometry and SLAM (often
called as vOdometry and vSLAM, respectively) usually resorts to feature-based
analysis to increase the performance and, as a consequence, the frame rate of the
output data from the visual system. With the aim to simulate the human vision it
is possible to use stereo cameras to acquire the 3D information from the environ-
ment. It should be highlighted that non-structured and dynamic environments
usually impose severe challenges for visual-based techniques and, therefore, the
detection of revisited areas is a key point for the navigation stack of mobile
robots since it decreases the positioning error. Considering the high number of
the implementation of techniques available in the literature, the current research
had selected the most promising one by considering factors such as, performance
expectation reported in scientiﬁc articles, public availability of the methods and
other particularities (robustness of features, internal assumptions and others).
A comparative analysis is conducted by taking into consideration a set of three
monocular odometry methods and three stereo-based SLAM methods.

Visual Odometry and SLAM Techniques
465
For odometry method three algorithms were selected, namely mono-vo, viso2
and mORB-SLAM. Mono-vo [2] implementation was developed in 2015 and it is
based on OpenCV. Uses the FAST for features detection and the Kanade-Lucas-
Tomasi to search the correspondence in the next image. Incorporates a mecha-
nism that searches for new features and uses an outliers removal mechanism. This
implementation aims the use the scale information from an external source of
data and, therefore, it is possible to correct the previous estimations. Moreover,
the mono-vo follow a heuristic to estimate the forward motion as the dominant
motion. The viso2 [3] implementation was developed in 2011 and calculates the
camera position estimation using a set of rectiﬁed images. The method has avail-
able a large number of conﬁgurable parameters which increases the ﬂexibility,
but turns the method very diﬃcult to setup. Frequently, a motion estimation
system cannot estimate with a metric scale from monocular sequences. Thus,
viso2 assumes that the camera motion follows a ﬁxed and known height from
the ground (used to predict the scale factor). This method uses a bucketing
technique to the correct distribution of features in images however, it has a rel-
evant limitation related to pure rotations which degradates the estimation. The
mORB-SLAM is the monocular implementation of the ORB-SLAM presented
in [4]. Therefore, this method uses keyframes and ORB as features extractor,
detecting corners from the FAST and BRIEF descriptor, to ensure the (soft)
real-time capacity. A bundle adjustment is conducted with a new keyframe in
order to remove some erroneous estimations (and features) and provides a better
positioning.
For SLAM technique three algorithms were selected, namely RTAB-Map, S-
PTAM and ORB-SLAM2. The RTAB-Map [5] system was developed in 2014
to capture a Graph-Based SLAM implementation and to present an incremental
approach for loop closure detection. It is important to note that the calculation
of the egomotion, with the own method of odometry called “s odom”, presents
limitations in situations comprising “Empty Space Environments” (when the
features are a distance to the camera larger than 4 m). This means that, the
performance is aﬀected by image sequences presenting large egomotion (reduces
the feature matching in consecutive frames) however, it is possible to use other
visual odometry methods to solve this limitation such as, viso2. The RTAB-Map
was particularly developed for scenarios involving cars and based on two cam-
eras with large focal distances. Although being used in diﬀerent robotic applica-
tions, the method only estimates a new position when 6DOF motion is detected
between consecutive frames. The loop closure detection is constructed online
through a bag-of-words approach (with SURF descriptors). A Graph Optimiza-
tion approach is used to correct the map during the robot navigation. Consider-
ing that mapping large-scale environments during long-terms navigation paths
is constrained by the computational power available onboard. The RTAB-Map
implements a memory management approach that considers only part of the map
to fulﬁll online processing requirements of todays applications. The S-PTAM [6]
implementation was developed with the goal to obtain a stereoscopic system able
to help the robot navigation, by providing more reliable estimations. Divides the

466
A.R. Gaspar et al.
SLAM-based approach into two tasks: Tracking and Map Optimization. Uses
the BRIEF descriptor with binary features to reduce the storage requirements
and speedup the feature correspondence. The Shi-Tomas algorithm imposes a
good spatial distribution of the features. Like the ORB-SLAM, the S-PTAM
uses a keyframe-based approach to estimate the motion. During the creation
of map, the method adjusts the nearby points by excluding those points that
are considered erroneous. This task is presented as a maintenance process inde-
pendently of the Tracking, which is an advantage in terms of the processing
time. The ORB-SLAM2 [4] implementation was developed in 2015 and it is
able to use monocular, stereo and RGB-D cameras. It is a feature-based app-
roach that uses the ORB extractor because of time constraints (important in the
real-time applications). Therefore, the egomotion determination is characterized
by a reliable motion estimation, since it is invariant to view point and illu-
mination changes (FAST corners with BRIEF descriptors). In terms of motion
estimation, it is keyframe-based approach and avoids an excessive computational
demand. Allows a camera relocalization in real-time when the Tracking process
was lost, by following a bag-of-words approach. Uses the Covisibility Graph con-
cept to bring the possibility of adding new keyframes and, consequently, to obtain
an environment ideal reconstruction. The Covisibility Graph helps the closure
loop detection, because this detection can be achieved by a similarity measure
between bag-of-words vector and all neighbors of the Covisibility Graph. Finally,
an Essential Graph provides a real-time eﬀective loop closure since it maintains
the words that represent a strong match (assuming a vocabulary constructed
oﬄine using the DBoW2 library [7]).
3
Comparative Analysis
Two set of experiments were conducted in this section. The ﬁrst aims to provide
evidences about the accuracy of egomotion estimation considering monocular
images sequences - odometry trials. The second aims to provide a comparative
study of diﬀerent vSLAM-based approaches without any constraint about the
environment or navigation path. A simple but eﬀective comparative analysis
is performed by considering all techniques introduced in the previous section.
During the analysis some public datasets were considered to allow replicability
of results by other scientiﬁc works (when proposing other visual-based meth-
ods). The performance of the methods are discussed by taking into consideration
some metrics, namely the Central Processing Unit (CPU) utilization in percent,
processing time and normalized Euclidian error between the ground-truth and
estimated trajectories in the same conditions, for example data acquisition rate
and image size. Regarding the results obtained in the monocular odometry, a
normalization of trajectories are conducted due to the unknown scale factor
between diﬀerent methods. Moreover, the images sets that were choosen rep-
resent paths that do not evidence revisited areas since the analysis is focused
on the accuracy of the egomotion estimation. Following a similar methodology,
vSLAM techniques discussed in the previous section are evaluated by taking

Visual Odometry and SLAM Techniques
467
into account the same metrics used in the odometry evaluation. In this case, the
detection of the closure loop is also contemplated since it is a key feature for
all SLAM techniques. Quantitative analysis can be retrieved by measuring the
accuracy of each SLAM technique in two checkpoints (since not all techniques
provide estimatives in all frames), represented by “error point1;error point2” in
the Tables 4, 5 and 6. Qualitative discussion is made using graph representations
of the trajectories. A very relevant phenomenon that it is usually ignored by the
literature is the aliasing. This research discussed this phenomenon during the
trials which means, the inﬂuence of the processing time in the overall accuracy
of each method is investigated. To evaluate the methods were selected three test
scenarios, that represent indoor and outdoor environments. These environments
show a clear image of the behavior that will be expected for each technique. The
three public datasets comprise KITTI, MIT Stata Center and New College.
The KITTI1 dataset is composed of 22 stereo images sequences with diﬀerent
trajectories obtained, in urban and freeway environments, see Fig. 1(a). The
height of the camera in relation to the ground and the no-oscillation have been
taken into account. The camera calibration parameters are available as well as
the ground-truth of the trajectory made during the acquisition of high resolution
image sequences. The MIT Stata Center2 is an indoor dataset, obtained from
a robot, see Fig. 1(b). This dataset was made to support the development of
visual SLAM algorithms and, therefore, the trajectories are longer and present
various direction changes. The New College3 dataset provides data that was
acquired in gardens, see Fig. 1(c). All data is synchronized: images, laser, GPS
and IMU information and odometry data (ground-truth) are available.
Fig. 1. Illustrative example of the used datasets: (a) KITTI (b) MIT Stata Center (c)
New College
Table 1 presents a summary of the testing conditions and scenarios evaluated
to each technique.
1 Dataset available on http://www.cvlibs.net/datasets/kitti/eval odometry.php.
2 Dataset available on http://projects.csail.mit.edu/stata/downloads.php.
3 Dataset available on http://www.robots.ox.ac.uk/NewCollegeData/.

468
A.R. Gaspar et al.
Table 1. Test Conditions to methods evaluation
3.1
Results
Odometry
KITTI – Sequence 07. As visible in the Fig. 2, the mono-vo implementation
follows the movement of the camera for most of the time. However, the incor-
rect detection of a direction change caused a little error between the real and
estimated trajectories.
Fig. 2. Normalized trajectories obtained by KITTI dataset (07)
On the other hand, mORB-SLAM implementation estimates the camera posi-
tion correctly, but with lower error, since it captures all direction changes with
a satisfactory accuracy. It can be noticed in Table 2 that the mORB-SLAM
presents the better egomotion estimation but it takes longer processing time
(in average)4. In the majority of cases, the mono-vo implementation captures
4 KFr represents the number of keyframes used to egomotion estimation.

Visual Odometry and SLAM Techniques
469
the egomotion during for a large part of the trajectory made by the observer,
however the largest maximum error was caused by a wrong detection of one
direction change. The viso2 implementation presents a higher error, because of
the deviations of the ﬁrst positions.
Table 2. Comparison of the normalized trajectories obtained by KITTI dataset (07)
Processed frames
Normalized error
Time
CPU
Maximum
Average
Std
Mono-vo
1000
0,64
0,13
0,17
85 s
>35% max. = 40%
Viso2
999
0,62
0,32
0,18
74 s
>12% max. = 15%
mORB-SLAM
996 KFr = 374
0,36
0,10
0,10
109 s
>18% max. = 20%
New College. The mORB-SLAM and mono-vo implementations try to follow
the circular motion of the observer, see Fig. 3.
Fig. 3. Normalized trajectories obtained by New College dataset
The viso2 provides an incorrect estimation, that can be justiﬁed by some
oscillations in the camera during the motion of the observer as well as the rela-
tive changes of depth, according to the literature. Therefore, the performance of
viso2 is severely aﬀected by these conditions (which reduces its robustness and
reliability). From the Table 3, it is visible that, although all implementations
present errors, the mORB-SLAM is the best. However, neither this implemen-
tation nor mono-vo can obtain the ﬁnal position intended.
In terms of the processing time, the mono-vo implementation presents better
results, even using more frames but with a slightly higher CPU usage.

470
A.R. Gaspar et al.
Table 3. Comparison of the normalized trajectories obtained by New College dataset
Processed frames
Normalized error
Time
CPU
Maximum
Average
Std
Mono-vo
2500
1,22
0,61
0,26
127 s
>28% max. = 34%
Viso2
1765
1,11
0,85
0,19
124 s >13% max. = 18%
mORB-SLAM
1657 KFr = 293
0,69
0,37
0,16
273 s
>19% max. = 24%
SLAM
KITTI – Sequence 05. According to the Fig. 4, it is possible to observe that
the ORB-SLAM2 implementation is the only one that estimates all camera posi-
tions, detects the loops and adjusts its trajectory. It should be noticed that, as
expected, the RTAB-Map system (with the viso2 providing the egomotion esti-
mation) try to replicate the motion made by observer however, there are devia-
tions. These deviation could be justiﬁed by the susceptibility of the method in
situations with inclination changes or even with camera rotations.
Fig. 4. Trajectories obtained by KITTI dataset (05)
Taken into account the results presented in Table 4, it is safe to say that ORB-
SLAM2 lead to lower errors between the real trajectory and ground-truth. In
terms of the CPU utilization and processing time, the ORB-SLAM2 implemen-
tation presents higher values comparatively with the S-PTAM implementation.
KITTI – Sequence 09. Figure 5 depicts that ORB-SLAM2 has some drifts
during the estimation of the trajectory in this sequence. Moreover, the tech-
nique do not detect any revisited area and, as a consequence, the trajectory was
not adjusted by the closure loop detection mechanism. In relation to the other
implementations, is not possible to conclude about the eﬀectiveness of closure
loop, once the estimated ﬁnal position was not close enough of the initial posi-
tion (circular path). One relevant issue was the number of frames that were not

Visual Odometry and SLAM Techniques
471
Table 4. Comparison of the trajectories obtained by KITTI dataset (05)
ORB-SLAM2
RTAB-Map
S-PTAM
viso2
s odom
Processed frames
2761
1224
2745
1916
Error Maximum
1,64 m
420,91 m
—
123,29 m
Average
0,55 m;1,05 m 30,84 m;17,73 m
—
33,05 m;8,41 m
Processing time
5 min 32 s
4 min 57 s
2 min 19 s
4 min 43 s
CPU
47%
57%
28%
54%
Fig. 5. Trajectories obtained by KITTI dataset (09)
considered by the SLAM techniques, which lead to aliasing situation - and some
motion components were not captured by these visual techniques. Although the
other implementations try to follow the camera motion, they get lost along the
trajectory.
Table 5 demonstrates that the ORB-SLAM2 has the ability to characterize
the observer motion with better accuracy and reliability (no aliasing phenom-
enon because the entire image sequence was processed). The CPU utilization and
processing time have higher values, but it uses all frames to trajectory estima-
tion. On the other hand, the RTAB-Map (viso2) does not estimate the realistic
path, being diﬃcult to provide a quantitative analysis of the performance of this
method.
MIT Stata Center. Figure 6 shows that RTAB-Map, with the odometry incor-
porated directly by the implementation, is not able to provide data (lack of
inliers). It is important to emphasize that were modiﬁed some parameters to
the features extraction but without changes in the obtained results. This fact
can be explained because of the limitation of this method in indoor environ-
ments, in particular the presence of large homogeneous spaces (halls), represent
a challenging problem for this method.

472
A.R. Gaspar et al.
Table 5. Comparison of the trajectories obtained by KITTI dataset (09)
ORB-SLAM2
RTAB-Map
S-PTAM
viso2
s odom
Processed frames
1591
648
1557
884
Error Maximum
28,87 m
145,14 m
—
115,06 m
Average
11,46 m;37,62 m —;207,65 m
—
60,61 m;103,25 m
Processing time
3 min 4 s
2 min 49 s
1 min 20 s
2 min 40 s
CPU
36%
55%
30%
80%
Fig. 6. Trajectories obtained by MIT Stata Center dataset
The S-PTAM implementation estimates correctly the trajectory since the
loops are correctly detected and, therefore, the method redeﬁnes the trajectory
taken by the observer. In fact, this technique was able to estimate the entire path
with a realistic scale. The RTAB-Map implementation (viso2) tries to follow the
direction changes occured during the path. The result of RTAB-Map (viso2)
shows that the path has suﬀered from a wrong estimation at the beginning of
the sequence (y-axis) which caused a total deviation. This fact can be explained
by the existence of a high and fast rotation at the start of the trajectory which
clearly demonstrates that this method is quite susceptible to errors in these
situations. All methods have demonstrated similar CPU usages (see Table 6),
however, ORB-SLAM2 had the best performance in this metric. Although the
best CPU usage, the ORB-SLAM2 did not have the best accuracy since the
localization was lost for a while (during a transition to a darker area) and the
method was not able to detect the loop closure. Thus, it is possible to conclude
that was calculated wrongly the ﬁrst direction change and, consequently, it is
quite diﬃcult to characterize the error. The RTAB-Map (viso2) system was also
unable to determine the loop closure, because discards many frames, possibly
due to the lack of the 6DOF motion, between consecutive frames.

Visual Odometry and SLAM Techniques
473
Table 6. Comparison of the trajectories obtained by MIT Stata Center dataset
ORB-SLAM2
RTAB-Map
S-PTAM
viso2
Processed frames
2312
2730
2962
Error Maximum
38,92 m
26,95 m
2,57 m
Average
6,30 m;—
3,29 m;14,67 m 8,87 m;0,34 m
Processing time
4 min 47 s
4 min 23 s
4 min 10 s
CPU
33%
53%
68%
4
Conclusion
This article studies several visual-based techniques. It presents a comparative
analysis of odometry and SLAM approaches in realistic indoor and outdoor
scenarios. Moreover, a quantitative and qualitative discussion is presented by
taking into account several metrics such as, graphic representation of the navi-
gation path, processing time and the aliasing phenomenon (considering real-time
constraints). This phenomenon causes positions error along the trajectory, once
are lost some frames between consecutive samples.
The results showed that the mono-vo follows relatively well the motion of
the majority of the trajectories, but any motion detected provide a new position
even when the camera does not move, which causes an increase in the error. The
viso2 presents a good motion estimation always that detects 6DOF, however it
provides erroneous estimations when the camera presents oscillations or changes
its height in relation to the ground. The mORB-SLAM also generates good
results in most cases, The mORB-SLAM also generates good results in most
cases, with errors lower in relation to the others (approximately 45%). Thus, the
mORB-SLAM and viso2 are the most complete with the principal diﬀerence in
the requirement by the mORB-SLAM estimator of a vocabulary constructed `a
prior. In the case of the SLAM implementations, the ORB-SLAM2 presents, in
the majority of the cases, a good motion estimation and provides estimations
with lower errors (decrease more than 80%). It is important to reinforce that the
RTAB-Map with own odometry method is diﬃcult to parametrize and highly
dependent on the environment, such as in the case of the KITTI dataset that
can be possible to considered a “Empty Space Environment”. The S-PTAM is
suitable only for MIT Stata Center dataset and, in this case, it was the only
one that provided correct results. This fact can be explained by the higher time
between images input in relation to the KITTI dataset. Thus, it is notable that
the S-PTAM and ORB-SLAM2 are the most adequate. These implementations
are diﬀerentiated by the fact of the S-PTAM does not present an approach for
loop closure detection, but only Bundle Adjustment, which does not provide
results so good to known revisited areas.

474
A.R. Gaspar et al.
To future work, the authors will conduct novel datasets (and incorporate
these datasets in the discussion), including new environments, to support the sci-
entiﬁc community and intend to reinforce the study namely with other methods.
Acknowledgements. This work is ﬁnanced by the ERDF - European Regional Devel-
opment Fund through the Operational Programme for Competitiveness and Interna-
tionalisation - COMPETE 2020 Programme within project ≪POCI-01-0145-FEDER-
006961≫, and by National Funds through the FCT - Funda¸c˜ao para a Ciˆencia e a
Tecnologia (Portuguese Foundation for Science and Technology) as part of project
UID/EEA/50014/2013.
References
1. Pinto, A.M., Moreira, A.P., Correia, M.V., Costa, P.: A ﬂow-based motion per-
ception technique for an autonomous robot system. J. Intell. Robot. Syst. 75(3),
475–492 (2014). doi:10.1007/s10846-013-9999-z
2. Singh, A.: An OpenCV based implementation of Monocular Visual Odometry.
Indian Institute of Technology Kanpur. Technical report, Kanpur (2015)
3. Kitt, B., Geiger, A., Lategahn, H.: Visual odometry based on stereo image sequences
with RANSAC-based outlier rejection scheme. In: IEEE Intelligent Vehicles Sym-
posium. University of California, San Diego, CA, USA, pp. 486–492 (2010)
4. Mur-Artal, R., Montiel, J.M.M., Tard´os, J.D.: ORB-SLAM: a versatile and accurate
monocular SLAM system. IEEE Trans. Robot. 31(5), 1147–1163 (2015). doi:10.
1109/TRO.2015.2463671
5. Labb´e, M., Michaud, F.: T Online global loop closure detection for large-scale multi-
session graph-based SLAM. In: IEEE/RSJ International Conference on Intelligent
Robots and Systems (2014). doi:10.1109/IROS.2014.6942926
6. Pire, T., Fischer, T., Civera, J., Crist´oforis, P., Berlles, J.J.: Stereo parallel tracking
and mapping for robot localization. In: Intelligent Robots and Systems, pp. 1373–
1378 (2015). doi:10.1109/IROS.2015.7353546
7. Galvez-Lopez, D., Tard´os, J.D.: Bags of binary words for fast place recognition in
images sequences. Intell. Robots Syst. 28(5), 1188–1197 (2012). doi:10.1109/IROS.
2012.2197158

Real-Time Deep ConvNet-Based Vehicle Detection
Using 3D-LIDAR Reﬂection Intensity Data
Alireza Asvadi(B), Luis Garrote, Cristiano Premebida, Paulo Peixoto,
and Urbano J. Nunes
Department of Electrical and Computer Engineering, Institute of Systems and Robotics,
University of Coimbra, Coimbra, Portugal
{asvadi,garrote,cpremebida,peixoto,urbano}@isr.uc.pt
Abstract. This paper addresses the problem of vehicle detection using a little
explored LIDAR’s modality: the reﬂection intensity. LIDAR reﬂection measures
the ratio of the received beam sent to a surface, which depends upon the distance,
material, and the angle between surface normal and the ray. Considering a 3D-
LIDAR mounted on board a robotic vehicle, which is calibrated with respect to
a monocular camera, a Dense Reﬂection Map (DRM) is generated from the pro-
jected sparse LIDAR’s reﬂectance intensity, and inputted to a Deep Convolutional
Neural Network (ConvNet) object detection framework for the vehicle detection.
The performance on the KITTI is superior to some of the approaches that use
LIDAR’s range-value, and hence it demonstrates the usability of LIDAR’s reﬂec-
tion for vehicle detection.
Keywords: Vehicle detection · 3D-LIDAR reﬂection · Deep learning
1
Introduction and Motivation
Vehicle detection is one of the key tasks in artiﬁcial sensor-based perception systems
for intelligent robotic vehicles and intelligent transportation systems. Robust and reli-
able vehicle detection ﬁnds variety of practical applications including collision warn-
ing systems, collision avoidance system, autonomous cruise controls, advanced driver
assistance systems (ADAS), and autonomous driving systems.
Autonomous vehicles use different types of sensors (e.g., Camera, LIDAR and
RADAR) to have a redundant and robust perception system. Color cameras, being the
type of sensor most used, suffer from illumination variations, lacking direct object dis-
tance estimation, and inability of vision through the night which restricts the reliability
of safe driving. LIDAR and RADAR measure distance by emitting and receiving elec-
tromagnetic waves. RADAR is able to work efﬁciently in extreme weather conditions
but suffers from narrow ﬂied of view and low resolution, which limits its application in
tasks of detection and recognition of objects. In comparison with RADAR, 3D-LIDAR
has a precise range measurement and a full 360◦ﬁeld of view. Motivated by reduction
c⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_39

476
A. Asvadi et al.
in their cost and increase in resolution and range, 3D-LIDARs are becoming a reliable
solution for scene understanding.
In this paper, we propose a sensory perception solution for vehicle detection (herein
called RefCN, which stands for ‘Reﬂectance ConvNet’) using a less explored 3D-
LIDAR reﬂection and a Deep ConvNet-based detection framework (YOLOv2 [1]). The
approach introduced here depends on a front view Dense Reﬂection Map (DRM) that
is constructed from sparse 3D-LIDAR reﬂectance data. DRMs run through the trained
DRM-based YOLOv2 pipeline to achieve vehicle detection.
The paper is organized as follows. Related work is presented in Sect. 2. The pro-
posed approach is described in Sect. 3. Experimental results are discussed in Sect. 4,
and Sect. 5 brings concluding remarks and future work.
2
Related Work
This section reviews research works that relate to LIDAR reﬂection, and surveys previ-
ous work in LIDAR-based vehicle detection.
2.1
Perception Using 3D-LIDAR Reﬂection
To our knowledge only a few works exist that use LIDAR reﬂection for artiﬁcial percep-
tion tasks. Tatoglu and Pochiraju [2] use LIDAR reﬂection for point cloud segmentation
based on the diffuse and specular reﬂection behaviors. Hernandez et al. [3] detect trafﬁc
lanes on roads by exploiting the LIDAR reﬂection of lane marking. Caltagirone et al.
[4] use top view LIDAR elevation and reﬂection data for road detection.
2.2
3D-LIDAR-based Vehicle Detection
This subsection gives a concise overview of vehicle detection approaches using 3D-
LIDARs in IV and ITS domains.
Behley et al. [5] perform object detection with a hierarchical segmentation of
3D-LIDAR point cloud (PCD) followed by bag-of-word (BoW) classiﬁcation. Asvadi
et al. [6] use PCD clustering for object proposal generation and a depth map based Con-
vNet classiﬁer for vehicle detection. Wang and Posner [7] discretize LIDAR points and
reﬂectance values into a Voxel grid. A 3D sliding window with a linear SVM classi-
ﬁer is used for obtaining detection scores. Li et al. [8] use a 2D Fully-Convolutional
Network (FCN) in a 2D point map (top-view projection of 3D-LIDAR range data) and
trained it end-to-end to build a vehicle detection system. Li [9] extends it to a 3D Fully-
Convolutional Network (FCN) to detect and localize objects as 3D boxes from LIDAR
point cloud data. Gonzalez et al. [10] use images and LIDAR-based depth maps with
HOG and Local Binary Patterns (LBP) features. They split the training set in differ-
ent views to take into account different poses of objects and train a Random Forest
(RF) of local experts for each view for object detection. Chen et al. [11] use LIDAR
range data and images. A top view representation of point cloud is used for 3D pro-
posal generation. 3D proposals were projected to color image, LIDAR’s top and front
views. A ConvNet-based fusion network is used for 3D object detection. Oh et al. [12]

Real-Time Deep ConvNet-Based Vehicle Detection Using 3D-LIDAR Reﬂection
477
use segmentation-based object proposal generation from LIDAR depth maps and color
images. They use decision level fusion to combine detections from two independent
ConvNet-based classiﬁers in the depth map and color image. Table 1 provides a review
of vehicle detection approaches using 3D-LIDAR data.
Table 1. Related work on 3D-LIDAR-based vehicle detection. Col., Ran. and Ref. are abbrevia-
tions for Color, Range and Reﬂectance modalities, respectively.
Reference
Modality
Representation
Detection technique
Behley et al. [5]
Ran.
Full 3D: PCD
Hierarchical Seg. + BoW
Asvadi et al. [6]
Ran.
PCD + Front view
Clu. based Prop. + ConvNet
Wang and Posner [7] Ran. + Ref.
Full 3D: Voxel
Sliding-window + SVM
Li et al. [8]
Ran.
Top view
2D-FCN
Li [9]
Ran.
Full 3D: PCD
3D-FCN
Gonzalez et al. [10]
Col. + Ran
Front view
Sliding-window + RF
Chen et al. [11]
Col. + Ran. + Ref
Front + Top views
Top view 3D Prop. + ConvNet
Oh et al. [12]
Col. + Ran.
Front view
Seg. based Prop. + ConvNet
While most of the 3D-LIDAR-based vehicle detection systems were built on range
data, in this paper a less exploited modality, 3D-LIDAR reﬂection intensity data, is used
for vehicle detection. The front view Dense Reﬂection Map (DRM) is generated and
used with YOLOv2, and is shown that the DRM can be useful for the vehicle detection
purpose.
3
RefCN: Vehicle Detection Using 3D-LIDAR Reﬂection
and YOLOv2 Object Detection Framework
The architecture of the proposed RefCN vehicle detection system is shown in Fig. 1.
The 3D-LIDAR point cloud is projected to the camera coordinate system and a Sparse
Reﬂectance Map (SRM) is generated. The SRM is up-sampled for getting a Dense
Reﬂectance Map (DRM). The DRM is inputted to the trained DRM-YOLO to detect
vehicles. The RefCN has two steps: (1) DRM generation and (2) DRM-based YOLOv2
object detection framework.
3D-LIDAR
SRM
DRM
DRM-YOLOv2
Sensor
ConvoluƟon
ConvoluƟon
Max
Pooling
Max
Pooling
Detected Vehicles
Image Plane
Image Plane
Image Plane
Fig. 1. The pipeline of the RefCN: vehicle detection using reﬂection intensity data from a LIDAR
and the Deep ConvNet based method (YOLOv2).

478
A. Asvadi et al.
3.1
Delaunay Triangulation-Based DRM Generation
The DRM is generated by projection of the point cloud on the image plane, followed
by triangulation and interpolation as described in the sequel.
3D-LIDAR Image Projection. 4D LIDAR data, 3D point cloud with reﬂection inten-
sity P = {X,Y,Z,I}, is ﬁltered to the cameras ﬁeld of view, and projected onto the
2D-image plane using
P∗=
Projection Matrix



PC2I ×R0×PL2C × P
(1)
where PC2I is the projection matrix from the camera coordinate system to the image
plane, R0 is the rectiﬁcation matrix, and PL2C is LIDAR to camera coordinate system
projection matrix. Considering P∗= {X∗,Y ∗,Z∗,I∗}, using the row and column pixel
values {X∗,Y ∗} accompanied with reﬂectance data I∗, a compact Sparse Reﬂectance
Map (SRM) is computed, which has a lower density than the image resolution.
Delaunay Triangulation (DT). The Delaunay Triangulation (DT) is used for mesh
generation from the row and column values {X∗,Y ∗} of the projected 3D-LIDAR points
P∗. The DT produces a set of isolated triangles Δ = {δ1,··· ,δn}, each triangle δ com-
posed of three vertices nk,{k : 1,2,3}, useful for building the interpolating function f(·)
to perform interpolation on reﬂectance values I∗.
Interpolation of Sparse Reﬂectance Map (SRM). Unsampled (missing) intensity
value I of a pixel P which lie within a triangle δ, is estimated by interpolating
reﬂectance values of the surrounding triangle vertices nk,{k : 1,2,3} using Nearest
Neighbor interpolation (which means selecting the value of the closest vertex), end-
ing up in a DRM.
I = f(argmin
nk
||P −nk||),
{k : 1,2,3}
(2)
For more details on the interpolation of scattered points, please refer to [13]. Premebida
et al. [14] and Ashraf et al. [15] investigate the quality of the generated up-sampled
dense map representations from LIDAR data using different interpolation techniques.
3.2
DRM-based YOLOv2 Object Detection Framework
You Only Look Once (YOLO) [16] models object detection as a regression problem.
The most-recent version of YOLO, denoted as YOLOv2 [1], is used in this paper.
In the proposed RefCN framework, the DRM is divided into 13 × 13 grid regions,
where each grid cell is responsible for predicting ﬁve object BB centers with their asso-
ciated conﬁdence scores. A convolutional network runs once on the DRM to predict
object BBs. The network is composed by 19 convolutional layers and 5 max-pooling
layers. YOLO looks at the whole DRM during training and test time; therefore, in addi-
tion to vehicle appearances, its predictions are informed by contextual information in
the DRM. The constructed DRM run through the trained DRM-based YOLOv2 pipeline
to achieve vehicle detection. Figure 2 illustrates the different steps of the RefCN.

Real-Time Deep ConvNet-Based Vehicle Detection Using 3D-LIDAR Reﬂection
479
Fig. 2. Illustration of the RefCN process. Top to bottom: a color image with superimposed pro-
jected LIDAR points. The SRM with color coded reﬂectance values. The generated 2D trian-
gulation. The zoomed area within the red box of above image. The ﬁfth image represents the
constructed DRM. The last image shows the vehicle detection result with conﬁdence scores.

480
A. Asvadi et al.
4
Experimental Results and Analysis
For the RefCN evaluation, quantitative and qualitative experiments using the KITTI
dataset [17] were performed.
4.1
KITTI Object Detection Dataset
The KITTI object detection ‘training dataset’ (containing 7,481 frames) was partitioned
into two subsets: 80% as training set (5,985 frames) and 20% as validation set (1,496
frames). The ‘Car’ label was considered for the evaluation.
4.2
Evaluation Metrics
Following KITTI’s assessment methodology, the PASCAL VOC intersection-over-
union (IOU) metric on three difﬁculty levels was used as the evaluation criterion with
an overlap of 70% for car detection. The overlap rate in 2D is given by,
IOU = area(2D-BB∩2D-BBg)
area(2D-BB∪2D-BBg)
(3)
The precision-recall curve and Average Precision (AP), which corresponds to the area
under the precision-recall curve, were computed and reported over easy, moderate and
hard data categories to measure the detection performance.
4.3
Experimental Setup and Computational Analysis
The experiments were run on a computer with a Hexa-core 3.5 GHz processor, powered
with a GTX 1080 GPU and 64 GB RAM under Linux. The YOLOv21 [1] 416 × 416
detection framework, written in C, were used in the RefCN implementation.
YOLOv2 Training with DRMs.
For training, as an initial weight, convolutional
weights of the pre-trained ConvNet on ImageNet were used. DRM-YOLOv2 was
trained on DRMs, generated from KITTI training set, to adapt it to DRM-based vehicle
detection. The DRM-YOLOv2 is ﬁne-tuned for 80,200 iterations using stochastic gra-
dient descent with learning rate of 0.001, 64 as batch size, weight decay of 0.0005 and
momentum of 0.9.
Computational Complexity and Run-Time. Two versions of DRM were imple-
mented: a version using MATLAB scatteredInterpolant function and a much faster
reimplementation in C++. The DRM generation in MATLAB takes about 1.4 s while in
C++ it takes 34 ms. The implementation details and the computational load of DRM
generation and YOLO detection steps are reported in Table 2. The overall time for
processing each frame using C++ implementation is 49 ms (more than 20 frames per
second). Considering that the KITTI dataset was captured using a 10 Hz spinning Velo-
dyne HDL-64E, it can be concluded that RefCN can be performed in real-time.
1 https://pjreddie.com/darknet/yolo/.

Real-Time Deep ConvNet-Based Vehicle Detection Using 3D-LIDAR Reﬂection
481
Table 2. The RefCN processing time (in milliseconds).
Impl. details
Proc. time Environment
DRM generation 34
C++
YOLO detection 15
C
4.4
Quantitative Results
Quantitative experiments were conducted to assess the performance of the RefCN: (i)
Sparse Reﬂectance Map (SRM) versus DRM; (ii) comparison of DRMs with differ-
ent interpolation methods; (iii) DRM versus color and range data modalities; and (iv)
RefCN versus state-of-the-art methods.
Sparse Reﬂectance Map (SRM) vs DRM. The RefCN was trained on training set
and evaluated on the validation set. As can be seen from Fig. 3 and Table 3, the results
show that the DRM (with Nearest Neighbor interpolation) considerably improves the
detection performance.
Table 3. Detection accuracy with SRM vs DRM on validation-set.
Input data Easy
Moderate Hard
SRM
23.45 % 17.57 %
15.57 %
DRM
67.69 % 51.91 %
44.98 %
DRM with Nearest Neighbor, Linear and Natural Interpolations. The result from
the previous experiment shows that the use of dense up-sampled representation consid-
erably improves the detection rate. A question that can be raised is which interpolation
method gives the best performance. In this experiment, we evaluated three interpolation
methods: Nearest Neighbor (DRMnearest), Natural Neighbor (DRMnatural) and Linear
(DRMlinear). DRMnatural is based on Voronoi tessellation of the projected LIDAR points
which result in a continuous surface except at projected points. DRMlinear is based
on linear interpolation between sets of three points (of the projected LIDAR points)
for surfaces in DT format. Figure 4 shows an example of the generated DRMlinear
and DRMnatural. The detection performance for each interpolation method is reported
in Fig. 3 and Table 4. The best performance was attained, for all categories, with
DRMnearest.
Table 4. Detection accuracy using DRM with different interpolation methods on validation-set.
Input data
Easy
Moderate Hard
DRM (DRMnearest) 67.69 % 51.91 %
44.98 %
DRMlinear
60.60 % 45.71 %
40.79 %
DRMnatural
65.25 % 50.07 %
44.76 %

482
A. Asvadi et al.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision
Easy
Moderate
Hard
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision
Easy
Moderate
Hard
Fig. 3. Precision-Recall on KITTI Validation-set (Car Class). Left: SRM (dashed lines) versus
DRM (solid lines). Right: DRMnearest (solid lines), DRMnatural (dashed lines) and DRMlinear
(dotted lines).
Fig. 4. Top to bottom: an example of the generated DRMlinear and DRMnatural, respectively.
Please refer to Fig. 2 for the illustration of the corresponding color image and the generated
DRMnearest.
Fig. 5. An example of the generated dense depth map (please refer to Fig. 2 for the illustration of
the corresponding color image and DRM.

Real-Time Deep ConvNet-Based Vehicle Detection Using 3D-LIDAR Reﬂection
483
Table 5. Detection accuracy with color vs depth and DRM input on validation-set.
Input data
Easy
Moderate Hard
Color
73.54 % 61.41 %
53.71 %
Depth
66.86 % 54.13 %
45.94 %
Reﬂec. (DRM) 67.69 % 51.91 %
44.98 %
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision
Easy
Moderate
Hard
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Precision
Recall
Car
Easy
Moderate
Hard
Fig. 6. Left: Precision-Recall on KITTI validation-set (Car Class) using Color (dotted lines)
Depth (dashed lines) and DRM reﬂectance (solid lines). Right: Precision-Recall on KITTI test-set
(Car Class).
Table 6. RefCN evaluation on KITTI test-set.
Approach
Moderate Easy
Hard
RunTime Environment
MV3D (LID.) [11] 79.24%
87.00% 78.16% 0.24
GPU@2.5 Ghz
(Python+C/C++)
VeloFCN [8]
53.59%
71.06% 46.92% 1
GPU@2.5 Ghz
(Python+C/C++)
Vote3D [7]
47.99%
56.80% 42.57% 0.5
4cores@2.8 Ghz (C/C++)
Proposed RefCN
35.72%
50.28% 29.86% 0.05
GPU@3.5 Ghz (C/C++)
CSoR [18]
26.13%
34.79% 22.69% 3.5
4cores@3.5 Ghz
(Python+C/C++)
mBoW [5]
23.76%
36.02% 18.44% 10
1core@2.5 Ghz (C/C++)
DepthCN [6]
23.21%
37.59% 18.01% 2.3
GPU@3.5 Ghz
(MATLAB)
Reﬂectance vs Depth vs Color Data.
In this experiment, we have compared
reﬂectance, depth and color data modalities. DT with Nearest Neighbor interpolation
and Range Inverse encoding were used for dense depth map generation (see Fig. 5).
To have fair comparison among modalities, the images (DRM, dense depth map and
color image) were converted to JPEG ﬁles with 75% compression quality. The average

484
A. Asvadi et al.
Fig. 7. Example screenshots of RefCN results. Detection results are shown, as green BBs in the
color-images (top) and DRMs (bottom) compared to the ground-truth (dashed-magenta). Notice
that the depicted color-images are shown only for visualization purpose.

Real-Time Deep ConvNet-Based Vehicle Detection Using 3D-LIDAR Reﬂection
485
ﬁle size for each DRM, dense up-sampled depth map and color modalities is approxi-
mately 44 KB, 28 KB and 82 KB, respectively. As it can be seen in Fig. 6 and Table 5,
the results show that the DRM has a better performance than the depth modality in Easy
class.
Comparison with the State-of-the-Art. To compare with the state-of-the-art, the
RefCN was trained on the full KITTI object detection ‘training dataset’ and evaluated
on the test set. Results are reported in Fig. 6 and Table 6. Although the RefCN uses
only reﬂection data, it surpasses some of the approaches that use LIDAR’s range data,
which provides empirical evidence of the usability of LIDAR’s reﬂection intensity data
in vehicle detection systems.
Fig. 8. The effect of retroreﬂectivity of car license plates and trafﬁc signs in the DRM and the
corresponding color-image. The green BBs show a car license plate and trafﬁc signs.
4.5
Qualitative Results
Figure 7 shows some of the representative qualitative results with many cars in the
scene. A video result is available at https://youtu.be/1JJHihvp7NE. As can be seen,
for most cases, the RefCN correctly detects target vehicles.
5
Conclusions, Key Findings and Future Work
To fulﬁll the requirements of a complementary and multimodal perception system,
we presented a methodology for using 3D-LIDAR reﬂection intensity data for vehi-
cle detection, and we provide empirical evidence of the beneﬁts of the system through
quantitative and qualitative experiments. In this paper, we have observed that the dense
up-sampled representation (DRM) considerably improves the detection rate (in com-
parison with the SRM). We also observed that the DRM has a better performance than

486
A. Asvadi et al.
the depth modality in “Easy class”. The generated DRM can be used in multi-modal
data fusion systems for object detection and tracking. A direction for the future work is
to explore the DRM for car license plate and trafﬁc sign detection (see Fig. 8).
Acknowledgments. This work has been supported by “AUTOCITS - Regulation Study for Inter-
operability in the Adoption of Autonomous Driving in European Urban Nodes” - Action number
2015-EU-TM-0243-S, co-ﬁnanced by the European Union (INEA-CEF); and FEDER through
COMPETE 2020, Portugal 2020 program under grant UID/EEA/00048/2013.
References
1. Redmon, J., Farhadi, A.: YOLO9000: better, faster, stronger. In: IEEE CVPR (2017)
2. Tatoglu, A., Pochiraju, K.: Point cloud segmentation with LIDAR reﬂection intensity behav-
ior. In: IEEE ICRA (2012)
3. Hernandez, D.C., Hoang, V.D., Jo, K.-H.: Lane surface identiﬁcation based on reﬂectance
using laser range ﬁnder. In: IEEE/SICE SII (2014)
4. Caltagirone, L., Samuel, S., Svensson, L., Wahde, M.: Fast LIDAR-based road detection
using fully convolutional neural networks. In: IEEE IV (2017)
5. Behley, J., Steinhage, V., Cremers, A.B.: Laser-based segment classiﬁcation using a mixture
of bag-of-words. In: IEEE/RSJ IROS (2013)
6. Asvadi, A., Garrote, L., Premebida, C., Peixoto, P., Nunes, U.J.: DepthCN: vehicle detection
using 3D-LIDAR and convnet. In: IEEE ITSC (2017)
7. Wang, D.Z., Posner, I.: Voting for voting in online point cloud object detection. In: Robotics:
Science and Systems (2015)
8. Li, B., Zhang, T., Xia, T.: Vehicle detection from 3D Lidar using fully convolutional network.
In: Robotics: Science and Systems (2016)
9. Li, B.: 3D fully convolutional network for vehicle detection in point cloud. In: IEEE/RSJ
IROS (2017)
10. Gonzalez, A., Vazquez, D., Lopez, A.M., Amores, J.: On-board object detection: multicue,
multimodal, and multiview random forest of local experts. CYB IEEE Trans. 47, 3980–3990
(2016)
11. Chen, X., Ma, H., Wan, J., Li, B., Xia, T.: Multi-view 3d object detection network for
autonomous driving. In: IEEE CVPR (2017)
12. Oh, S.-I., Kang, H.-B.: Object detection and classiﬁcation by decision-level fusion for intel-
ligent vehicle systems. Sensors 17(1), 207 (2017)
13. Amidror, I.: Scattered data interpolation methods for electronic imaging systems: a survey.
J. Electron. Imaging 11(2), 157–176 (2002)
14. Premebida, C., Garrote, L., Asvadi, A., Ribeiro, A.P., Nunes, U.: High-resolution LIDAR-
based depth mapping using bilateral ﬁlter. In: IEEE ITSC (2016)
15. Ashraf, I., Hur, S., Park, Y.: An investigation of interpolation techniques to generate 2D
intensity image from LIDAR data. IEEE Access 5, 8250–8260 (2017)
16. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: uniﬁed, real-time
object detection. In: IEEE CVPR (2016)
17. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? The kitti vision
benchmark suite. In: IEEE CVPR (2012)
18. Plotkin, L.: PyDriver. https://github.com/lpltk/pydriver. Accessed Sep 2017

Modeling Traﬃc Scenes for Intelligent
Vehicles Using CNN-Based Detection
and Orientation Estimation
Carlos Guindel(B), David Mart´ın, and Jos´e Mar´ıa Armingol
Intelligent Systems Laboratory (LSI) Research Group,
Universidad Carlos III de Madrid, Legan´es, Spain
{cguindel,dmgomez,armingol}@ing.uc3m.es
Abstract. Object identiﬁcation in images taken from moving vehicles is
still a complex task within the computer vision ﬁeld due to the dynamism
of the scenes and the poorly deﬁned structures of the environment. This
research proposes an eﬃcient approach to perform recognition on images
from a stereo camera, with the goal of gaining insight of traﬃc scenes
in urban and road environments. We rely on a deep learning framework
able to simultaneously identify a broad range of entities, such as vehicles,
pedestrians or cyclists, with a frame rate compatible with the strong
requirements of onboard automotive applications. The results demon-
strate the capabilities of the perception system for a wide variety of
situations, thus providing valuable information to understand the traﬃc
scenario.
Keywords: Object detection · Viewpoint estimation · Deep learning ·
Intelligent vehicles
1
Introduction
Technology has adopted an increasingly important role in transportation sys-
tems over the past decades. Advanced Driver Assistance Systems (ADAS) have
been introduced in an attempt to deal with the fact that wrong decision-making
and distractions are the cause of a signiﬁcant proportion of traﬃc accidents.
These systems represent an increase in the degree of automation towards the
future goal of fully autonomous driving, which is expected to lead to signiﬁcant
improvements in several issues associated with transportation systems.
While automated cars have been already successfully tested in urban envi-
ronments [1], they are in most cases heavily dependent on oﬀ-line-built maps.
Navigation with scarce or non-existent prior knowledge remains an open chal-
lenge due to the wide range of complex situations which is required to be han-
dled (occluded landmarks, unexpected behaviors, etc.) within a highly dynamic,
semi-structured environment.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_40

488
C. Guindel et al.
Forthcoming self-driving systems will be demanded to understand complex
traﬃc situations by themselves. This requirement relies on a robust inference of
the position and motion of every traﬃc participant in the surrounding scene. Not
only the presence of obstacles must be accounted but also an accurate estimation
of the class to which every obstacle belongs (i.e. car, cyclist, etc.) is essential in
order to correctly understand and predict the traﬃc situation.
Vision-based approaches [2] have been proved to be highly cost-eﬀective while
enabling close-to-production assemblies given their compact size and ease of
integration. Furthermore, video frames provide a rich data source from which
additional information can be extracted.
In this paper, a vision-based approach enabling an enhanced onboard envi-
ronment perception is introduced. It is targeted to the detection and localization
of the diﬀerent road participants present in the surroundings of a movable plat-
form. Additionally, object detection is enriched through a viewpoint estimation,
enabling high-level inference about short-term behaviors. A modern convolu-
tional network-based framework is employed to perform the critical inference
steps according to appearance features; later, stereo information allows intro-
ducing spatial reasoning into the system.
The remainder of this paper is organized as follows. In Sect. 2, we brieﬂy
discuss related works. Section 3 gives a general overview of the work. Section 4
describes the obstacle detection approach, while Sect. 5 introduces the scene
modeling procedure. Results are reported in Sect. 6. Finally, Sect. 7 gives our
conclusions about the work.
2
Related Work
Obstacle detection is an essential feature for automated driving systems. Conse-
quently, a large number of algorithms have been historically developed to that
end. Eﬀort often focused on vehicle and pedestrian detection as these agents are
the most commonly found ones in traﬃc scenes.
According to the sensing device in use, vision-based methods have tradition-
ally fallen into two main categories: monocular-vision methods and stereo-vision
methods. Stereo-vision provides depth information about the scene and thus is
commonly used in driving applications [3].
Stereo-vision algorithms usually make some assumptions about the ground
or the expected free space on it [4]. However, rich geometry information about
the scene can be recovered, enabling the building of representations such as
probabilistic occupancy maps [5], elevation maps [6] or full 3D models [7], where
obstacles can be identiﬁed.
On the other hand, monocular obstacle detection is commonly based on
appearance features. Selection of suitable features was traditionally the most
crucial step in the performance of the detection pipeline and thus a lot of
application-speciﬁc features, such as HOG-DPM [8], have been proposed to per-
form detection of traﬃc participants (e.g. cyclists [9]). Orientation estimation of
the detected objects, while less frequent, has also been addressed [10].

Modeling Traﬃc Scenes for Intelligent Vehicles
489
Representation learning based on deep neural networks has delivered a para-
digm shift in recent years, showing vast improvements over hand-crafted features
in several kinds of recognition tasks. In particular, Convolutional Neural Net-
works (CNN) can learn hierarchical data representations that have been shown
useful in object classiﬁcation [11].
Instead of using the classical sliding-window approach, detection with CNNs
frequently relies on attention mechanisms to limit the number of proposals to
be eﬀectively classiﬁed. Within this tendency, Girshick et al. introduced the now
widely known recognition using regions (R-CNN) paradigm [12]. These regions
can be selected according to classical similarity-based segmentation methods;
however, much eﬀort has recently been devoted to end-to-end pipelines where
every stage, including region proposal, can be eﬀectively learned. Faster R-CNN
[13] take advantage of a Region Proposal Network (RPN) which feeds the R-CNN
responsible for the classiﬁcation task.
CNNs have been applied in several tasks involved in autonomous driving, such
as lane detection [14] and, certainly, object detection [15]. In some cases, orien-
tation is also predicted to increase the information about the detected instances;
thus, in [16], a CNN is used for object detection and viewpoint estimation. View-
point is also estimated in [17] through keypoint likelihood models.
3
System Overview
This work has been designed to become the core element of the perception mod-
ule in the IVVI 2.0 (Intelligent Vehicle based on Visual Information) research
platform [18]. IVVI 2.0 is a manned vehicle, equipped with cutting edge auto-
motive sensors, which is meant for the development and testing of ADAS.
Visual sensing units in the IVVI 2.0 include a trinocular stereo camera cov-
ering the ﬁeld of view in front of the vehicle, which is the source of the images
used by the presented approach. The processing unit includes a high-performance
GPU which enables high-parallel processing, such as that carried out in CNNs.
Robot Operative System (ROS)1 is used for inter-module cooperation.
The method presented here provides a step forward in vision-based detection
and classiﬁcation. The work consists of two main branches that are intended to
run in parallel, as shown in Fig. 1:
1. Object detection and viewpoint estimation based on appearance. Features are
extracted exclusively from the left stereo image.
2. Object localization, robust to changes in position and orientation of the vision
system resulting from the vehicle movement. A stereo-based 3D reconstruc-
tion is performed, and the extrinsic camera parameters are extracted under
a ﬂat-ground assumption.
As usual in deep learning frameworks, our object detection approach is meant
to be performed almost entirely in the GPU; on the other hand, the object
1 http://www.ros.org/.

490
C. Guindel et al.
Fig. 1. Proposed system overview
localization pipeline is expected to make an intensive use of a typical multi-
core CPU during the step of 3D reconstruction. This twofold process has been
designed to ﬁll the available computing capability, in order to meet the time
requirements inherent to the application.
4
Obstacle Detection
A wide variety of dynamic obstacles can be found in urban and road environ-
ments. Whereas object classiﬁcation aims to classify predeﬁned image regions,
object detection also requires localizing every object within the image coordi-
nates.
We adopt a state-of-the-art, CNN-based approach, Faster R-CNN [13], to
perform object detection. Based on the popular R-CNN detector [12], Faster
R-CNN provides an end-to-end trainable framework spanning from the image
pixels to the ﬁnal prediction. While outperforming classical detection pipelines,
Faster R-CNN can deal with a large number of classes with no major impact on
performance, making it particularly suitable for driving environments.
Faster R-CNN involves two diﬀerent stages: a Region Proposal Network
(RPN), which is responsible for identifying those image regions where objects
are located, and an R-CNN, where image regions from the previous RPN are
classiﬁed. Both components are based on CNN architectures and in fact, they
share the same set of convolutional layers. For this reason, Faster R-CNN enables
object detection at real-time frame rates.
We adopt the strategy introduced in [19] to incorporate the viewpoint infer-
ence into the detection framework. The basis of the idea is to beneﬁt from the
already computed convolutional features to obtain an estimation of the orienta-
tion of the objects with respect to the camera. Figure 2 illustrates the approach.
As with the region proposals from the RPN, viewpoint can be estimated at
almost no cost during test-time given that convolutions are computed only once.
According to the requirements of the application, only the yaw angle (i.e.
azimuth) from which objects are seen is to be estimated, since both relevant
obstacles and the ego-vehicle are assumed to move on the same ground plane.

Modeling Traﬃc Scenes for Intelligent Vehicles
491
Fig. 2. Proposed object detection and viewpoint estimation approach.
4.1
Discrete Viewpoint Approach
A discrete approach is adopted for the viewpoint estimation, so that the
full range of possible viewpoints (2π rad) gets divided into Nb bins Θi; i =
0, . . . , Nb −1 of which only one is employed to represent the object viewpoint.
Accordingly, objects with a ground-truth angle θ are assigned a viewpoint label
i during the training step such that θ ∈Θi:
Θi =

θ ∈[0, 2π)

2π
Nb
· i ≤θ < 2π
Nb
· (i + 1)

(1)
The proposed object viewpoint estimation system is aimed to provide a view-
point estimation consisting of the parameters of a categorical distribution over
Nb possible viewpoints, r. A single-valued ˆθ estimation can therefore be provided
as the center of the bin b∗with the greatest probability according to r:
ˆθ = π(2b∗+ 1)
Nb
(2)
4.2
Joint Detection and Viewpoint Estimation
In the R-CNN framework, image regions are propagated through the network
and, ﬁnally, a ﬁxed-length feature vector is extracted to predict the object class.
We introduce the viewpoint estimation straightforwardly: it is inferred from the
same feature vector that is also used to predict the class. This is motivated by
the fact that appearance is highly aﬀected by viewpoint, so a good set of features
should be able to discriminate among diﬀerent viewpoints.

492
C. Guindel et al.
Solutions introduced with Fast R-CNN [20] are adopted here, so the resulting
feature vectors are fed into a sequence of fully connected layers that are ﬁnally
split into three sibling layers. As in the original approach, the ﬁrst two sibling
layers provide a classiﬁcation and a bounding box regression, respectively. On
the other hand, the new third layer is responsible for giving an estimation of the
viewpoint, which is ultimately normalized through a softmax function. Given
that classiﬁcation is performed over K classes, the output of this branch is a
vector r composed of Nb · K elements, representing K categorical distributions
(one per class) over the Nb viewpoint bins:
rk = (rk
0, . . . , rk
Nb) for k = 0, . . . , K
(3)
4.3
Training
According to the results reported in [13], we adopt an approximate joint training
strategy, which has been shown to oﬀer the best time-precision trade-oﬀ. View-
point estimation loss is introduced as a logistic loss that only adopts non-zero
values for the ground-truth class; that is, from the NbK-dimensional output r,
we only take into account the Nb elements belonging to the ground-truth class
when computing the loss:
Lv =
1
NB

j∈B
Lcls(ru∗
j , b∗
j)
(4)
where NB is the size of the batch B used to train the Fast R-CNN stage, and
Lcls(ru∗
j , b∗
j) is the multinomial logistic loss computed with the Nb elements from
r corresponding to the ground-truth class u∗(i.e. the probability distribution of
the angular bins for the ground-truth class) and the ground-truth label for the
bin classiﬁcation b∗.
This summation is added to the existing four components of the loss in the
original Faster R-CNN framework, to get a ﬁve-component multi-task loss which
is used for training the CNN. Although diﬀerent weights might be applied to the
components of the loss function, we let every (normalized) loss have the same
contribution.
4.4
Implementation Details
As usual in classiﬁcation tasks, convolutional layers are expected to be initial-
ized from a model previously trained on the ImageNet classiﬁcation dataset [21],
while fully connected layers are given random values according to a Gaussian
distribution. In this paper, eight evenly spaced viewpoint bins are considered for
the viewpoint estimation (Nb = 8). Finally, a per-class non-maximum suppres-
sion (NMS) is applied to prune away the duplicated detections.

Modeling Traﬃc Scenes for Intelligent Vehicles
493
5
Scene Modeling
Object detection can be augmented with geometrical information in order to
retrieve an instantaneous, local model of the traﬃc participants in front of the
vehicle. To that end, we use the information from the two cameras in the stereo
rig to build a dense 3D reconstruction of the environment.
Initially, the 3D point cloud of the scene is represented in camera coordinates.
If the ground is assumed to be ﬂat in a relatively small neighborhood from
the vehicle, extrinsic parameters of the stereo system can be estimated in an
online fashion. Accordingly, the eﬀect of camera pose changes due to the vehicle
movement (e.g. traveling on uneven road surfaces) can be properly removed.
Through this process, obstacles ﬁrst detected through the object detection
stage can be localized in world coordinates and assigned an absolute yaw angle.
5.1
Stereo 3D Reconstruction
We adopt a semi-global approach [22] to perform dense stereo matching. Despite
this family of algorithms being more processing intensive than the traditional
block-matching methods, challenges posed by road environments, e.g. lack of
texture or changes of illumination, make them more suitable for the intended
application. As an example, the disparity map obtained from the scene in Fig. 3a
is shown in Fig. 3b.
As a result, a 3D point cloud is obtained (Fig. 3c). Then, a voxel grid down-
sampling, with a grid size of 20 cm, is performed. In addition to reducing the
amount of data to be processed, this ﬁltering is aimed to normalize the point
density along the depth axis.
5.2
Extrinsic Parameters Auto-Calibration
Coeﬃcients deﬁning the ground plane must be estimated as a ﬁrst step to obtain
the vision system extrinsic parameters. Two pass-through ﬁlters are applied in
order to remove points outside a 0–2 m range along the vertical axis and a 0–20
m range along the depth axis. Within that ranges, ﬂatness assumption is fulﬁlled
with a high probability.
Points comprising the ﬁltered point-cloud are then ﬁtted to a plane using
RANSAC [23] with a 10 cm threshold. Only planes perpendicular to a ﬁxed
direction, with a small angular tolerance, are considered. Since angles deﬁning
the camera pose are expected to be small, that axis is chosen as the vertical
direction in camera coordinates. Figure 3d illustrates the ground plane (shown
in green) obtained from the voxel-ﬁltered point cloud.
It can be shown [24] that, given a road plane deﬁned by axc+byc+czc+d = 0,
with (xc, yc, zc) being the coordinates of a point belonging to the plane, roll (ψ),
pitch (φ) and height (h) deﬁning the camera pose can be obtained as:
ψ = arcsin(a)
φ = arctan
−c
b

h = d
(5)

494
C. Guindel et al.
(a)
(b)
(c)
(d)
Fig. 3. Extrinsic parameters estimation pipeline: (a) left image; (b) disparity map; (c)
point cloud; (d) inliers for the plane, in green, over the voxelized cloud (d).
Yaw angle cannot be extracted solely from the plane, and thus it is assumed
to be nil. We choose not to translate the world coordinate frame along x and
y camera axes, although that displacement may be arbitrarily chosen (e.g. the
origin might be centered at the front end of the vehicle).
That set of extrinsic parameters deﬁnes a transformation which is then
applied to the non-ﬁltered point-cloud to get points in world coordinates.
5.3
Object Localization
To obtain the spatial location of the objects in the scene, the correspondence
between points in the image and points in the 3D cloud, preserved within an
organized point cloud structure, is exploited. Points belonging to the ground,
as well as those too close to the camera (e.g. from the hood of the car), are
removed beforehand. For every detection, the median values of the x, y and z
coordinates for the set of 3D points corresponding to the 11 central rows of the
object bounding box are computed and used as an estimation of the 3D location
of the object. Yaw angle, expressed as the rotation around a local vertical axis,
can be approximated taking into account the angle between the positive x axis of
the world coordinate frame and the point given by the coordinates of the object.
By using all the inferred information about the obstacles, a top-view model
of the vehicle surroundings is built, where every object in the ﬁeld of view is
included alongside their estimated orientation.
6
Results
Our joint object detection/viewpoint estimation pipeline was quantitatively eval-
uated according to the standard metrics on a well-established image benchmark,
while the performance of the scene modeling stage and, eventually, the full sys-
tem, have been tested in real traﬃc scenes using the IVVI 2.0 vehicle.
6.1
Object Detection and Viewpoint Estimation
Experiments for assess the object detection and viewpoint estimation branch
have been conducted on the KITTI object detection benchmark [25], taking

Modeling Traﬃc Scenes for Intelligent Vehicles
495
advantage of the available class and orientation labels. Since annotations for the
testing set are not publicly available, the labeled training set has been divided
into two splits, for train and validation, ensuring that images from the same
sequence are not used in both subsets. Overall, 5,576 images are used in training
whereas 2,065 are subsequently employed to test our algorithms.
Since our work focus on the simultaneous recognition of the diﬀerent agents of
the scene, our algorithm has been trained to detect the seven foreground classes
provided by the KITTI dataset. Special care was taken to avoid including regions
overlapping with DontCare and Misc regions, neither as positive nor negative
samples while training.
Given that our approach is independent of the particular architecture selected
for the convolutional layers, we tested the two baseline architectures from Faster
R-CNN in our application: ZF [26] and VGG16 [27]. On the other hand, even
though all the models were obtained scaling the input images to 500 pixels in
height during the training, diﬀerent scales were evaluated at test-time. In all
cases, training was carried out for 90k iterations, with a base learning rate of
0.0005, which was scaled by 0.1 every 30k iterations.
For the sake of brevity, we only evaluate Average Orientation Similarity
(AOS), as introduced in [25], which is intended to assess the joint detection and
viewpoint accuracy. Results for the diﬀerent architecture/scale combinations are
given in Table 1. Please note that results for the Person sitting and Tram classes
are not reliable due to the low number of samples and were therefore excluded.
Processing times are for our implementation using the Python interface of Caﬀe
[28] and a NVIDIA Titan Xp GPU.
Table 1. Average orientation similarity (%) and run times (ms) on the test split for
diﬀerent scales and architectures
Net
Scale Car
Pedest. Cyclist Van Truck mean Time
ZF
375
44.2 35.6
16.1
8.5
3.2
21.5
46
500
52.7 43.7
18.4
12.9
3.5
26.2
73
625
51.6 40.7
22.7
15.1
5.3
27.1
90
VGG 375
64.8 54.7
25.0
22.9
8.5
35.2
79
500
74.7 61.0
33.0
30.0 12.1
42.2
112
625
75.7 60.9
35.2
31.1 15.4
43.7
144
As shown, precision does not grow signiﬁcantly when the test-time scale is
raised beyond the original train-time scale, i.e., 500 pixels. On the other hand,
VGG16 considerably outperforms ZF for every analyzed class.
6.2
Scene Modeling
Tests for the scene modeling were performed using the IVVI 2.0 vehicle in real
traﬃc situations. According to the results in the previous section, we chose

496
C. Guindel et al.
the VGG16 architecture, and 500 as the image scale. Due to the generalization
capability featured by CNN structures, models trained on the KITTI dataset
were used without modiﬁcations. An ROI with 500 pixels in height, comprising
the area where objects are typically present in the images, was extracted from
the original 1024 × 768 images to be utilized by the CNN branch, while the full
frame is employed to build the point cloud at the modeling branch.
Figure 4 shows four examples of monocular detections (upper row) and their
resulting scene models, where obstacles are represented as dots on a top view of
the reconstructed point cloud (lower row). Object orientation is represented by
an arrow. Additionally, points belonging to the ground plane (RANSAC inliers)
are projected on the image and colored green; they provide a rough estimation
of the traversable area for the vehicle.
(a)
(b)
(c)
(d)
Fig. 4. Some examples of traﬃc scenes correctly identiﬁed by our system.
7
Conclusion
A computer vision framework designed to reach a full traﬃc scene understanding
has been presented. Traﬃc participants are identiﬁed by a CNN-enabled method,
showing the potential of this approach within automotive applications. Obstacle
viewpoint estimation is introduced as an additional information source to endow
the system with further insight into the scene features.
Because of the nature of the adopted approach, joint object detection and
viewpoint estimation can be performed simultaneously over all classes. Since
CNN parameters are shared across all categories and feature vectors computed
by the CNN are low-dimensional, computation times are compliant with real-
time requirements, yet achieving accurate results. This information can be fur-
ther augmented using a stereo vision 3D reconstruction to gather an accurate
situation assessment in complex traﬃc situations.

Modeling Traﬃc Scenes for Intelligent Vehicles
497
New categories of traﬃc elements, even those belonging to the infrastructure,
may be subsequently added to enhance the scene understanding. The presented
approach can be naturally extended to the time domain in order to make pre-
dictions about future behaviors of agents involved in the scene. In this regard,
viewpoint estimation provided by the presented method plays a fundamental
role to enable robust inference.
Additionally, the output provided by the system is suitable to be combined
with information from other perception modules, e.g., semantic segmentation,
to build an even further comprehensive model of the surroundings of the vehicle.
Acknowledges. Research supported by the Spanish Government through the CICYT
projects (TRA2015-63708-R and TRA2016-78886-C3-1-R), and the Comunidad de
Madrid through SEGVAUTO-TRIES (S2013/MIT-2713). The Titan Xp used for this
research was donated by the NVIDIA Corporation.
References
1. Broggi, A., Cerri, P., Debattisti, S., Laghi, M.C., Medici, P., Panciroli, M.,
Prioletti, A.: PROUD-public road urban driverless test: architecture and results.
In: Proceedings of IEEE Intelligent Vehicles Symposium (IV), pp. 648–654 (2014)
2. Zhu, H., Yuen, K.V., Mihaylova, L., Leung, H.: Overview of environment perception
for intelligent vehicles. IEEE Trans. Intell. Transp. Syst. (2017)
3. Franke, U., Pfeiﬀer, D., Rabe, C., Knoeppel, C., Enzweiler, M., Stein, F.,
Herrtwich, R.G.: Making Bertha See. In: IEEE International Conference on Com-
puter Vision Workshops (ICCVW), pp. 214–221 (2013)
4. Musleh, B., de la Escalera, A., Armingol, J.M.: U-V disparity analysis in urban
environments. In: Computer Aided Systems Theory - EUROCAST 2011. Springer,
Heidelberg, pp. 426–432 (2012)
5. Badino, H., Franke, U., Mester, R.: Free space computation using stochastic occu-
pancy grids and dynamic programming. In: IEEE International Conference on
Computer Vision Workshops (ICCVW) (2007)
6. Oniga, F., Nedevschi, S.: Processing dense stereo data using elevation maps: Road
surface, traﬃc isle, and obstacle detection. IEEE Trans. Veh. Technol. 59(3), 1172–
1182 (2010)
7. Broggi, A., Cattani, S., Patander, M., Sabbatelli, M., Zani, P.: A full-3D Voxel-
based dynamic obstacle detection for urban scenario using stereo vision. In: Pro-
ceedings of IEEE International Conference on Intelligent Transportation Systems
(ITSC), pp. 71–76 (2013)
8. Felzenszwalb, P.F., Girshick, R., McAllester, D., Ramanan, D.: Object detection
with discriminatively trained part-based models. IEEE Trans. Pattern Anal. Mach.
Intell. 32(9), 1627–1645 (2010)
9. Tian, W., Lauer, M.: Fast cyclist detection by cascaded detector and geometric
constraint. In: Proceedings of IEEE International Conference on Intelligent Trans-
portation Systems (ITSC), pp. 1286–1291 (2015)
10. Pepik, B., Stark, M., Gehler, P., Schiele, B.: Teaching 3D geometry to deformable
part models. In: Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 3362–3369 (2012)

498
C. Guindel et al.
11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with deep con-
volutional neural networks. In: Proceedings of Advances in Neural Information
Processing Systems (NIPS), pp. 1097–1105 (2012)
12. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-
rate object detection and semantic segmentation. In: Proceedings of IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR), pp. 580–587 (2014)
13. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object
detection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell.
39(6), 1137–1149 (2016)
14. Li, J., Mei, X., Prokhorov, D.: Deep neural network for structural prediction and
lane detection in traﬃc scene. IEEE Trans. Neural Netw. Learn. Syst. 28(3), 690–
703 (2017)
15. Yang, F., Choi, W., Lin, Y.: Exploit all the layers: fast and accurate CNN
object detector with scale dependent pooling and cascaded rejection classiﬁers.
In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2129–2137 (2016)
16. Yang, L., Liu, J., Tang, X.: Object detection and viewpoint estimation with auto-
masking neural network. In: Computer Vision - ECCV 2014, pp. 441–455 (2014)
17. Tulsiani, S., Malik, J.: Viewpoints and keypoints. In: Proceedings of IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR), pp. 1510–1519 (2015)
18. Mart´ın, D., Garc´ıa, F., Musleh, B., Olmeda, D., Pel´aez, G.A., Mar´ın, P., Ponz, A.,
Rodr´ıguez Garavito, C.H., Al-Kaﬀ, A., de la Escalera, A., Armingol, J.M.: IVVI
2.0: an intelligent vehicle based on computational perception. Expert Syst. Appl.
41(17), 7927–7944 (2014)
19. Guindel, C., Martin, D., Armingol, J.M.: Joint object detection and viewpoint
estimation using CNN features. In: Proceedings of IEEE International Conference
on Vehicular Electronics and Safety (ICVES), pp. 145–150 (2017)
20. Girshick, R.: Fast R-CNN. In: Proceedings of IEEE International Conference on
Computer Vision (ICCV), pp. 1440–1448 (2015)
21. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet large
scale visual recognition challenge. Int. J. Comput. Vision 115(3), 211–252 (2015)
22. Hirschm¨uller, H.: Stereo processing by semiglobal matching and mutual informa-
tion. IEEE Trans. Pattern Anal. Mach. Intell. 30(2), 328–341 (2008)
23. Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model
ﬁtting with applications to image analysis and automated cartography. Commun.
ACM 24(6), 381–395 (1981)
24. de la Escalera, A., Izquierdo, E., Mart´ın, D., Musleh, B., Garc´ıa, F., Armingol,
J.M.: Stereo visual odometry in urban environments based on detecting ground
features. Robot. Auton. Syst. 80(June), 1–10 (2016)
25. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for Autonomous Driving? The
KITTI Vision Benchmark Suite. In: Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 3354–3361 (2012)
26. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.
In: Computer Vision - ECCV 2014, pp. 818–833. Springer (2014)
27. Simonyan, K., Zisserman, A.: Very Deep Convolutional Networks for Large-Scale
Image Recognition. CoRR abs/1409.1 (2014)
28. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.,
Guadarrama, S., Darrell, T.: Caﬀe: convolutional architecture for fast feature
embedding. In: Proceedings of ACM International Conference on Multimedia, pp.
675–678 (2014)

Complete ROS-based Architecture
for Intelligent Vehicles
Pablo Marin-Plaza(B), Ahmed Hussein, David Martin,
and Arturo de la Escalera
Universidad Carlos III de Madrid, Leganes, Spain
{pamarinp,ahussein,dmgomez,escalera}@ing.uc3m.es
http://www.uc3m.es/islab
Abstract. In the Intelligent Transportation Systems Society (ITSS),
the research interest on intelligent vehicles is increasing during the last
few years. Accordingly, this paper presents the advances in the develop-
ment of the ROS-based (Robot Operating System) software architecture
for intelligent vehicles. The main contribution of the architecture is its
powerfulness, ﬂexibility, and modularity, which allows the researchers to
develop and test diﬀerent algorithms. The architecture has been tested
on diﬀerent platforms, autonomous ground vehicles from the iCab (Intel-
ligent Campus Automobile) project and in the intelligent vehicle based
on Advanced Driver Assistance Systems (ADAS) incorporated from IvvI
2.0 (Intelligent Vehicle based on Visual Information) project.
1
Introduction
The interest of the autonomous driving vehicles starts three decades ago, solving
simple problems related to the dynamics and stabilization of the vehicle; such
as Anti-lock Braking System (ABS) and Traction Control System (TCS) with
proprioceptive sensors like encoders and inertial measurement units. From late
1990 to the last decade diﬀerent Advanced Driver Assistance Systems (ADAS)
applications; for instance, park assistance, Adaptive Cruise Control (ACC), night
vision, and lane leave detection among others were developed and installed in
some commercial vehicles. The intention of this features was always to assist
the driver and not the autonomous driving. This decade, the revolution of sen-
sors and computers, allow the research groups focus on the diﬀerent levels of
autonomy, increasing the complexity of vehicle tasks. From collision avoidance,
automated highway driving and recently cooperative behavior on road, where
the rules of each country are applied for all vehicles. When talking about oﬀ-road
navigation, the rules for roads are not applicable and new challenges emerge due
to the unstructured environment and the lack of lanes or signs.
The intention of this paper is to present a software architecture, based on
modular and portable solution to solve each one of the problems that autonomous
vehicles have to face. Most of them are common from the on road; such as
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_41

500
P. Marin-Plaza et al.
localization, mapping, perception, and behavior decision. However, the most
important diﬀerence between on road and oﬀroad is the analysis of the map
during behavior selection. This architecture is easily conﬁgurable to work with
diﬀerent platforms and it is intended to compare and evaluate the solution of
problems by diﬀerent methods.
The document is structured as follows, Sect. 2 introduces the design of the
architecture and involved elements. Section 3 describes the platforms, where this
architecture has been tested. Section 4 exposes conclusions and future work.
2
Design
The main idea to create a whole architecture is to allow the researchers the
possibility to test and compare the implemented algorithms, in order to reduce
the time spent on implementation. With a modular architecture, it is possible to
change or select easily diﬀerent modules keeping the coherence of the connections
between nodes. The prototyping software is ROS (Robot Operating System),
and the most important aspect of the architecture is the connection between
packages. Additionally, standard messages for sensors, mapping and navigation
are used in order to keep the connections unbroken and ensure the compatibility
between nodes when a new package needs to be incorporated.
The ideal software for research platforms is focused on diﬀerent aspects; such
as modularity, ﬂexibility, simplicity, and usability. They are described as follows:
– Modularity: is the degree to which a system’s components may be separated
and recombined. Based on that aspect, each problem to solve, in the afore-
mentioned Sect. 1, has been simpliﬁed and isolated in order to work inde-
pendently of the system. Navigation, localization, control, and path planning
are isolated, in order to debug and test individually, as well as mapping, and
behavior decision making.
– Flexibility: means designs that can adapt when external changes occur. In
this case, the external change will be a researcher changing the main method,
in order to make a comparison between the old method and the new one. For
example, one case of use is the odometry ego-motion method. The sources like
wheel odometry from the encoders or GPS odometry may vary substantially
between them and a good ﬁlter will generate a better localization. The archi-
tecture allows the change from one ﬁlter to other or even generates both. The
use of managers, in order to select which method is active, is a fundamental
part of the architecture. One important fact is that if a new feature like a new
message, parameter or signal is necessary, the architecture is mature enough
to adapt and assimilate this change without modifying the rest of the system.
– Simplicity: is the state or quality of being simple. As the architecture grows in
complexity with many more modules, the conﬁguration ﬁle and the launch ﬁle
of the whole architecture maintain the tracking of which parts of the modules
are running.

ROS-based Architecture
501
Fig. 1. Extended overview of the architecture
– Usability: means the availability or convenience of use. This aspect is the most
important for sharing the knowledge with other researchers. Each package is
documented and available to be used by other researchers when necessary.
The overview of the whole system is shown in the Fig. 1. On the top left, there
are some examples of the inputs of the system as sensors or controllers. In the
top middle, a simpliﬁed diagram of the modules of the architecture is presented.
On its right side, possible actuators such as motors or speakers. Photos of iCab
1, IvvI 2.0 and iCab 2 are under the top blocks and described later on. Below the
photos, there is the description of the modules in more details. The connection

502
P. Marin-Plaza et al.
messages between them are always the standard of ROS repositories, for the
reasons aforementioned, to keep the compatibility and interchangeability of the
packages.
In order to conﬁgure which packages are active, roslaunch ﬁles have been used
with speciﬁc nodes. These ﬁles run the selected packages with speciﬁc parameters
under diﬀerent namespaces, in order to generate the whole system. If new sensor
or node is required, a new launch ﬁle is incorporated.
2.1
Inputs
The inputs of the system are based on which type of research platform is con-
ﬁgured. Each sensor is able to run under Linux or at least the driver is able to
communicate with the ROS core. This section describes the mounted sensors on
the platform.
– Laser Rangeﬁnder: LMS 291 and LD-MRS, both from Sick company. The
drivers are sick wrapper and sick ldmrs. Both lasers are conﬁgurable and
share information under a speciﬁc type of message called LaserScan.msg.
– Stereo Camera: The use of Point Grey Bumblebee 2 and Bumblebee xB3
allow the architecture to gather information under the speciﬁc messages of
Image.msg. Each camera is publishing left and right image and the post
process nodes, rectify them in order to create the disparity map.
– GPS + IMU + Compass: uBlox module is one solution to retrieve infor-
mation about the inner state of the vehicle with the IMU by Imu.msg and
Compass Float.msg. For the GPS module, this device publish information by
NavSatFix.msg. The driver is Mavros.
– Ultrasonic: In order to control them, it is necessary a microcontroller to send
and receive the time of ﬂight and the accordingly distance. The simple solution
is the use of Arduino and the driver to publish the information is rosserial
arduino. The messages to publish are Float.msg.
– Joystick: The manual driving needs a stable joystick to govern the car. The
driver Joy, which is generic from ROS libraries, is the perfect solution. The
message published is Joy.msg
– Lidar: The device is Velodyne VLP-16. This special sensor is able to provide
information from the environment with 360◦over 16 diﬀerent layers. The
driver Velodyne driver allows the architecture to receive messages of the type
PointCloud2.msg and LaserScan.msg
2.2
Actuators
The only method to move a vehicle is over the motor, brake and direction wheel,
therefore, the commands generated from the architecture are accordingly to stan-
dard messages in ROS AckermannDrive.msg. The importance to maintain this
message is reﬂected on the possibility to use with diﬀerent actuators. The only
change is the low-level driver outside of the architecture and outside the com-
puters.

ROS-based Architecture
503
The speakers use sound driver to play diﬀerent instructions for the passengers
by the type SoundRequest.msg.
The touchscreen is included here due to the GUI created that is part of the
architecture. In addition to taking inputs from passengers, related to transporta-
tion requests.
2.3
Perception
Image processing is a tool for environmental understanding, where features of
the scene are extracted and analyzed in order to create better decisions. The pre-
process is in charge of transforming the data extracted from the stereo camera
as image raw for both left and right and rectiﬁed accordingly with lookup tables.
The use of lookup tables or the oﬃcial ROS repository for camera calibration
is up to the user of the architecture. For speciﬁc Bumblebee Point Grey sensors
for the iCab and IvvI projects, the lookup tables have been selected due to
better performance. In order to understand the environment using the stereo
analysis, it is necessary to create the disparity map. Again, the architecture
provides the opportunity to select the method and as long as the use of standard
image messages maintains the same structure, the method to rectify or create
the disparity map is irrelevant from the point of view of the architecture and
only the performance is aﬀected. Towards the obstacle detection, some nodes
use the extrinsic calibration from the camera to the road plane, accordingly, it is
necessary to save computational power and perform only once, because the use
of point-clouds requires high CPU load.
2.4
Localization
The localization package is intended to solve the problem of the odometry of
the vehicle. The knowledge of vehicle pose, position, and orientation, during
the movement in the environment, is critical to making eﬀective decisions. Due
to this reason, the vehicle needs its own localization at all time and for this
architecture, several ways to obtain the odometry have been developed, which
is described as follows:
– Encoders: The accumulative displacement on longitudinal and latitudinal
coordinates are extracted from the rear wheels and the steering wheel. Sev-
eral algorithms has been tested in order to obtain the less accumulative error
on this type of odometry and afterwards, the use of a compass in the wheel
odometry calculation improved the overall eﬃciency. The geometry for the
look ahead distance of pure pursuit allows the node to calculate the distance
covered. Through the covered distance and the compass angle, it is possible
to identify the vehicle pose.
– GPS: Based on the latitude coordinates, longitude coordinates and X and
Y UTM values, the readings of this sensor provides the global movement of
the vehicle. These readings are highly dependent on the number of available
satellites in the testing area. In order to obtain the local odometry, the use of

504
P. Marin-Plaza et al.
the initial heading direction is possible to be selected by two methods, based
only on the GPS readings after the start of the movement or based directly
on the compass.
– Visual: The possibility to obtain the plane of the ground in front of the vehicle
opens the possibilities to create two diﬀerent approach for this objective. One
method based on the analysis of the ground plane to extract features using
PCL. The other method uses de UV-disparity analysis [10].
– Velodyne: Using the LOAM (Laser Odometry and Mapping) algorithm [12]
from ROS repositories and using diﬀerent conﬁgurations, this package pro-
vides one of the best odometries for the vehicle, especially near buildings and
structures around the path. This method is based on the 3D reconstruction
of the environment that the sensor provides.
– Wireless: The use of access points for wireless Internet communication pro-
vides the localization in a known environment [2].
The odometry manager is a fundamental part of this package. It is in charge
of selecting the ﬁnal odometry of the vehicle under the conﬁgured namespace.
All other nodes that use the odometry are directly connected to the output of
the odometry manager. Other odometries derived from this package are only for
testing and debugging. The messages used for all odometry generation nodes
are from nav msgs package and the type is Odometry.msg. It includes the ego
position of the vehicle, the velocity and the covariance for the conﬁdence level.
Moreover, the odometry manager is responsible for the fusion of the diﬀerent
type of odometry techniques in order to achieve better results with less error.
2.5
Mapping
The use of nav msgs package allows the system the use of OccupancyGrid.msg
to share with the diﬀerent processes. The main characteristic of this occupancy
grid map is the reconﬁgurability to work with diﬀerent sizes and resolutions. The
mapping library creates the grid map for each sensor and later fuse them to create
one map with all surroundings information. The use of TF (transformation tree)
is mandatory, therefore, proper transformations are estimated. The data fusion
algorithm is up to the researcher, in order to test the importance of each sensor
depending on the conﬁdence level. Finally, the visualizer adds some elements;
such as the icon for the vehicle in colors, waypoints and other features only for
debugging purposes. Each message contains the data array, where the diﬀerent
values between 0 and 100 are speciﬁed for the occupation probability of each cell.
2.6
Planning
In order to go from one point to other, the planning is divided into two main
layers. On the one hand, global planning creates a path for the vehicle from
the starting point to the destination point, avoiding all static obstacles in the
provided occupancy grid map. The message type Path.msg is used on this nodes
and it is from the package nav msgs. The most important ﬁeld is the waypoints,

ROS-based Architecture
505
where the car needs to pass by in order to send to the proper controller commands
to convert later into movement commands. Grid map message is mandatory
for each node in order to allow the use of diﬀerent algorithms; such as A* or
Simulated Annealing [3].
On the other hand, there is the local planning for the vehicle movement
among the pre-set waypoints by the global planner. The planner uses the local
grid map with the current information and current location as input to navigate
to the next waypoint. Accordingly, the better local grid map of the surroundings,
the better and more eﬃcient planning is obtained. The local planning traces the
path to follow, using diﬀerent methods; such as splines, Time Elastic Bands [11]
or Bezi´er curves [1]. The same workﬂow from the other package with the planning
(global or local) manager is used here. The manager is in charge of publishing
to ROS the selected path, based on the simple selection or ﬁlter and fused by
diﬀerent algorithms.
2.7
Control
The lateral and velocity controllers are generated in a closed-loop model. This
package generates the commands to navigate by ackermann msgs, where the
velocity and the steering angle are the results of the PID controller package from
ROS. This package is conﬁgured for each vehicle, due to the slight diﬀerences in
the actuators, therefore, the PID coeﬃcients are diﬀerent. The message Acker-
mannDrive.msg is generated in this package as the result of the current position
and the desired position, gathered by the message aforementioned Path.msg from
the local planning manager.
2.8
Communication
For the V2X communication, the use of Multimaster FKIE allows the archi-
tecture to discover other ROS core masters in the same network. The study
of comparison between 4G and WiFi networks is conducted in [7] along with
the diﬀerent conﬁgurations using Virtual Private Network (VPN). The Vehicle-
to-Vehicle (V2V) communication allows each agent in the system to know the
position of the other agents, among other related information for cooperation
purposes. The Vehicle-to-Infrastructure (V2I) creates the registry log to debug
and is the main component of interaction with the iCab project to save the
transportation requests for the vehicles to go from one point to another. Finally,
the Vehicle-to-Pedestrian (V2P) communication uses the network to alert both
nearby vulnerable pedestrians and the vehicle in case of possible collision [4].
All conﬁgurations are bidirectional, that means the infrastructure-to-vehicle and
the pedestrian-to-vehicle communications are implemented as well. Due to the
project speciﬁcations for each type of communication and not for the general
purpose, the messages shared through V2X conﬁgurations are not standard.

506
P. Marin-Plaza et al.
2.9
Cooperation
The package main purpose is to manage the cooperation and coordination among
all vehicles in the system. It follows the multi-robot task allocation (MRTA)
problem modeling, using the multi-traveling salesmen (mTSP) formulation. Cur-
rently, there are two proposed solutions, market-based approach and a meta-
heuristic based optimization approach. The cooperation manager selects which
method is used. Because of the complexity of the cooperation level, an extra
non-standard message is used in order to share all relevant information of the
vehicle [5].
2.10
Simulation
One of the most extended tools for 3D simulation open-source is Gazebo 2, due
to the physics motor, reconﬁgurability, portability, and usability. The model of
the vehicle has been created using an Ackermann vehicle 3D conﬁguration. The
control lib is loaded in the ROS core and communicates the transmission of the
movement of the links to the simulator by libroscontroller. The sensors; such
as laser rangeﬁnder, Velodyne, cameras, and ultrasonic are implemented to be
used by the model. For the returned odometry message of the vehicle inside
the simulator, an interface between gazebo and the architecture has been imple-
mented, which publishes the gazebo odometry by the Odometry.msg, allowing
the manager to select this odometry for simulation purposes to link with the
rest of nodes.
2.11
Decision Making
The logic core is placed in this package. For the iCab project, there are sim-
ple tasks developed, such as follow right boundary, follow left boundary, drive
between boundaries, path follower, and platooning. These tasks are based on
the grid map and the odometry, which allow the iCab vehicles to move around
the oﬀ-road environment. The selection of each task is based on a state machine
with diﬀerent levels of decision [6].
2.12
Debugging
The tools for recording the data using rosbag and its posterior analysis allow
the test and debug with multiple conﬁgurations. The launch ﬁles are conﬁgured
to select where to save the data, which topics to be analyzed, and the duration
of the bagﬁles to split. Other tools such as rqt plot, rqt graph, rosconsole, ros-
topic, roswtf, tf monitor among others are connected to the packages, in order to
provide useful information. The most common IDE for developing is QT, which
allows live debugging while ROS is running.

ROS-based Architecture
507
2.13
GUI
This package has the implementation of the graphical user interface (GUI). It
is possible to run diﬀerent interfaces related to the active project(iCab or IvvI),
along with other visualization options such as RVIZ. It has been developed with
QT-creator.
3
Platforms
Based on the previous description of the architecture, it has been tested the iCab
project vehicles [6,8] and in the IvvI 2.0 project [9].
3.1
ICab
This platform is described as an electric golf cart, where the direction wheel
has been removed and replaced with a motor encoder system. For the traction
movement, the power unit is governed by MOSFET PWM with the ability to go
forward and backward at (limited at this moment) maximum speed of 15 Km/h.
For the brake system has been incorporated a linear motor actuator to push the
on-wheel mechanism and create the correspondent friction to decelerate con-
trolled by Arduino motor shield. The steering angle and PWM traction control
are done by a microcontroller outside of the ROS scope. The sensor system is
composed of a conﬁgurable stereo camera (from 640 × 480 RGB to 1024 × 768
RGB @ 20 Hz) at the front, lidar single plane (180◦FOV @ 10 Hz) at the front,
GPS + IMU + compass on top of the vehicle, lidar 16 layers (360◦vertical +
30◦horizontal) on top of the vehicle, ultrasonic sensors x2 wide angle ±45◦at
the rear. The hardware to manage the sensors and the actuators are divided
into two computers, AAEON embedded i7 PC, and Intel NUC 6I7KYK, both
working with Ubuntu 16.04 and ROS Kinetic. For the emergency button, there
is an electronic device inside of the vehicle and a WiFi paired button. Exists two
vehicles iCab 1 and iCab 2 with the same setup. The Fig. 2 shows this elements.
3.2
IvvI 2.0
This platform is shown in the Fig. 3 and present the ADAS system for the drivers
without any autonomous intervention. The sensors are a conﬁgurable stereo
camera, (from 640 × 480 RGB to 1024 × 768 RGB @ 20 Hz) at the front, IR
camera (320 × 240 @ 15 Hz) at the front, lidar 16 layers (360◦vertical + 30◦
horizontal) on top of the vehicle and Kinect 2 inside of the vehicle pointing
at the driver in order to monitored the distractions and the drowsiness. One
powerful computer is mounted at the rear of the vehicle and it has the same
setup as in the iCab platforms, Ubuntu 16.04 and ROS Kinetic.

508
P. Marin-Plaza et al.
Fig. 2. iCab
Fig. 3. IvvI 2.0

ROS-based Architecture
509
4
Conclusions and Future Work
In this article, a complete ROS-based architecture for intelligent vehicles has
been studied and developed as a complex and essential task for intelligent trans-
portation systems society (ITSS). So, a reliable solution based on the ROS-based
software architecture has been proposed by fusing data and knowledge to adapt
and adjust overall vehicle parameters in diﬀerent platforms. The reconﬁgurable
architecture aids diﬀerent embedded systems in vehicles to increase the power-
fulness, ﬂexibility, and modularity, to allow the test of new algorithms easily.
The architecture has been exempliﬁed into two platforms, ﬁrst is an
autonomous ground vehicle, and second, an intelligent vehicle based on ADAS.
The usefulness of the presented architecture, and comparative advantages with
respect to non-adaptable architectures, have been extensively demonstrated
through results under demanding circumstances such as GPS outages, degraded
computer vision scenarios, or multi-path trajectories, while maintaining the posi-
tioning accuracy in complex urban scenarios.
Therefore, the aim of this research is that with the growing research in the
autonomous vehicle ﬁeld in the last years, multiple research groups develop pow-
erful algorithms only for speciﬁc platforms. That is, based on the dependency
level, when a new platform appears, the portability of this algorithms are not
simple. The work exposed in this paper, provides a solution to create a complete
and conﬁgurable architecture, from diﬀerent modules, that is not linked to the
sensor devices or the hardware low-level controller. Then, the provided archi-
tecture is able to manage two diﬀerent platforms, as exposed in the previous
sections, iCab platforms (autonomous vehicles) and IvvI 2.0 (ADAS vehicle).
As a future work, this architecture solution will be applied, using moderate-
cost available sensors, in the forthcoming autonomous vehicle industry, such as
cooperative driving, automatic maneuvers for safety, autonomous urban vehicles,
or collision avoidance, among other complex systems that require complete archi-
tectures. Moreover, the study of diﬀerent levels of compatibility is also required
to provide the solutions when a platform is not compatible enough to support
or execute the whole architecture. Accordingly, the future proposed work is the
use of tools for communication such as Lightweight Communications and Mar-
shalling (LCM).
Acnowledgement. This research is supported by Madrid Community project
SEGVAUTO-TRIES (S2013-MIT-2713) and by the Spanish Government CICYT
projects (TRA2013-48314-C3-1-R, TRA2015-63708-R and TRA2016-78886-C3-1-R).
References
1. Gonzlez, D., Prez, J., Lattarulo, R., Milans, V., Nashashibi, F.: Continuous cur-
vature planning with obstacle avoidance capabilities in urban scenarios. In: IEEE
17th International Conference on Intelligent Transportation Systems (ITSC), 2014,
pp. 1430–1435. IEEE (2014)

510
P. Marin-Plaza et al.
2. Hernandez, N., Ocaa, M., Alonso, J.M., Kim, E.: Continuous space estimation:
Increasing wiﬁ-based indoor localization resolution without increasing the site-
survey eﬀort. Sensors 17(147), 1–23 (2017)
3. Hussein, A., Mostafa, H., Badreldin, M., Sultan, O., Khamis, A.: Metaheuristic
optimization approach to mobile robot path planning, pp. 1–6 (2012)
4. Hussein, A., Garcia, F., Armingol, J.M., Olaverri-Monreal, C.: P2v and v2p com-
munication for pedestrian warning on the basis of autonomous vehicles. In: IEEE
International Conference on Intelligent Transportation Systems (ITSC), pp. 2034–
2039 (2016)
5. Hussein, A., Marin-Plaza, P., Garcia, F., Armingol, J.M.: Optimization-based app-
roach for cooperation and coordination of multi-autonomous vehicles. In: Sixteenth
International Conference on Computer Aided Systems Theory (EUROCAST), pp.
1–2 (2017)
6. Hussein, A., Marin-Plaza, P., Martin, D., de la Escalera, A., Armingol, J.M.:
Autonomous oﬀ-road navigation using stereo-vision and laser-rangeﬁnder fusion
for outdoor obstacles detection. In: IEEE Intelligent Vehicles Symposium (IV), pp.
104–109 (2016)
7. Kokuti, A., Hussein, A., Marin-Plaza, P., Garcia, F.: V2x communications architec-
ture for oﬀ-road autonomous vehicles (icab). In: Proceedings of IEEE International
Conference on Vehicular Electronics and Safety (ICVES) (2017)
8. Marin-Plaza, P., Beltran, J., Hussein, A., Musleh, B., Martin, D., de la Escalera,
A., Armingol, J.M.: Stereo vision-based local occupancy grid map for autonomous
navigation in ros. In: Joint Conference on Computer Vision, Imaging and Computer
Graphics Theory and Applications (VISIGRAPP), vol. 3, pp. 703–708 (2016)
9. Martin, D., Garcia, F., Musleh, B., Olmeda, D., Pelez, G., Marn, P., Ponz, A.,
Rodrguez, C., Al-Kaﬀ, A., de la Escalera, A., Armingol, J.M.: Ivvi 2.0: an intelligent
vehicle based on computational perception. Expert Syst. Appl. 41(17), 7927–7944
(2014)
10. Musleh, B., Martin, D., de la Escalera, A., Armingol, J.M.: Visual ego motion
estimation in urban environments based on uv disparity. In: Intelligent Vehicles
Symposium (IV), 2012 IEEE, pp. 444–449. IEEE (2012)
11. Rsmann, C., Feiten, W., Wsch, T., Hoﬀmann, F., Bertram, T.: Trajectory mod-
iﬁcation considering dynamic constraints of autonomous robots. In: 7th German
Conference on Robotics, Proceedings of ROBOTIK 2012, pp. 1–6. VDE (2012)
12. Zhang, J., Singh, S.: Loam: lidar odometry and mapping in real-time. In: Robotics:
Science and Systems, vol. 2 (2014)

Challenges in Medical Robotics in the
Frame of Industry 4.0

Health 4.0 Oriented to Non-surgical Treatment
Carles Soler
(✉)
Casiopea Robotics, Barcelona, Spain
carles@casiopearobotics.com
Abstract. The emerging technologies that are conforming the Industry 4.0 are
also impacting on health. Artiﬁcial intelligence, 3D printing, robotics, big data,
Internet of Things, augmented reality, among others, are adding a layer of digi‐
tization on classical processes, allowing to increase the eﬀectiveness and eﬃ‐
ciency in the processes related to health and opening a new space of possibilities.
In this article, some examples will show the state of art of Health 4.0 in the non-
surgical ﬁeld.
Keywords: Industry 4.0 · Artiﬁcial intelligence · 3D printing · Robotics · Big
data · Internet of Things · Augmented reality
1
Introduction
“Industry 4.0” is the name of an initiative, initially proposed by the German government,
which wants to take advantage of a renewed technological framework in which the use
of diﬀerent technologies (including robotics, artiﬁcial intelligence, cloud computing,
Internet of Things, big data, additive manufacturing or augmented reality) aims to raise
the production again competitive in countries with a long industrial tradition. From the
application of the concepts associated with Industry 4.0, industry, and especially the
manufacturing industry, is accelerating its automation process, adding a layer of digi‐
tization on the physical reality. At the end of the process, it is expected to achieve a
highly eﬃcient production environment, where the necessary human resources will be
considerably smaller in number to the present ones, and with professional proﬁles and
competencies required that are very diﬀerent from those that are needed today.
The concepts associated with Industry 4.0 are not exclusive to manufacturing
production processes, but can be applied to areas as diverse as logistics, transportation,
energy or health. Thus, the application of emerging technologies that imply the creation
of a layer of digitization and aim to increase the eﬃciency in the processes related to
health opens a new space of possibilities. Today it is possible to ﬁnd multiple operational
applications of what, in a non-original way, we can qualify with the label “Health 4.0”.
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_42

2
Some Health 4.0 Applications Today
Robotics is deeply impacting on surgical treatments, but non-surgical treatments are also
taking advantage of the emerging technologies. Without wishing to be exhaustive, some
signiﬁcant examples are presented in the following sections.
Disease prediction [1]
Heart attacks are hard to anticipate. In an eﬀort to predict them, many doctors use
guidelines similar to those of the American College of Cardiology/American Heart
Association (ACC/AHA), based on eight risk factors, including age, cholesterol level
and blood pressure.
The University of Nottingham (United Kingdom) has led an study comparing the
use of the ACC/AHA guidelines with machine-learning algorithms. First, they analyzed
data coming from the electronic medical records of 378,256 patients in the United
Kingdom. They used about 80% of the data to search for patterns and build their own
internal “guidelines”. Then they tested themselves on the remaining records. Using
record data available in 2005, they predicted which patients would have their ﬁrst cardi‐
ovascular event over the next 10 years, and checked the guesses against the 2015 records.
Neural networks correctly predicted 7.6% more events than the ACC/AHA method, and
raised 1.6% fewer false alarms. As prediction leads to prevention (through cholesterol-
lowering medication or changes in diet) that would have meant lots of lives that could
have been saved.
Diagnosis [2, 3]
Since 2013 Watson, the IBM artiﬁcial intelligence system, dedicates part of its activity
to support in decision making on cancer treatments, developing diagnoses and proposing
a ranking of possible personalized treatments for each patient, ranked by applicability.
The system analyzes patient data against thousands of historical cases, information
from more than 5,000 h of training by oncologists, nearly 300 medical journals, more
than 200 textbooks and 12 million pages of text.
After being initially tested at the Memorial Sloan-Kettering Cancer Center, several
hospitals around the world today have access to the services of Watson for Oncology.
The concordance of their diagnoses with teams of highly specialized oncologists exceeds
90%, and the time required to issue a diagnosis of high complexity has decreased from
weeks to hours.
Mobile diagnosis [4, 5]
Early 2017, after training with a database of nearly 130,000 skin disease images,
computer scientists at Stanford Artiﬁcial Intelligence Laboratory have presented an
artiﬁcially intelligent diagnosis algorithm for skin cancer. The ﬁnal product was tested
against 21 board-certiﬁed dermatologists. In its diagnoses of skin lesions, which repre‐
sented the most common and deadliest skin cancers, the algorithm matched the perform‐
ance of board-certiﬁed dermatologists.
Once the algorithm has been developed, since the diagnosis is based on an image, it
is immediate to think of taking advantage of the photographic capacity of the
514
C. Soler

smartphones to achieve to bring reliable skin cancer diagnoses ubiquitous Thus, the
developing team is already working on it.
The accessibility, intuitive user-interface and the inherent connectivity oﬀered by
smartphones can overcome the limiting factors of many point of care tests, creating new
and exciting possibilities, a major revolution that will transform the patient experience
and accelerate diagnostic decision making.
Remote medical care [6, 7]
Sometimes, in rural or small hospitals a medical specialist is physically unavailable. In
these cases, a robot can be deployed to check in on a patient with a physician from
elsewhere.
Since 2013, the InTouch Vita robot aims to facilitate remote medical care. Includes
built-in ports for diagnostic devices like ultrasound machines, stethoscopes, and
otoscopes, plus two cameras and the ability to navigate autonomously in a non-structured
environment.
New treatment plans [8, 9]
In 2013, Stanford researchers identiﬁed a possible new treatment for small cell lung
cancer, which is particularly deadly. The pipeline worked by scanning hundreds of
thousands of gene-expression proﬁles (gathered by multiple researchers and stored in
large databases) across many diﬀerent cell types and tissues, some normal and some
diseased, some treated with medications and some not. Alone, these proﬁles may not
mean much, but when viewed together, they appear unsuspected patterns and trends. It
came up that Imipramine, which is an anti-depressant, had the ability to help cure certain
types of lung cancer. And because the drug was already approved by the Food and Drug
Administration for use in humans, they’d been able to quickly and (relatively) inexpen‐
sively move into human trials.
Remote monitoring [10–12]
For years there has been interest in applications for remote monitoring of patients. For
instance, in-home monitoring of vital signs of patients with chronic obstructive pulmo‐
nary disease provides more proactive care and makes healthcare more convenient and
persistent. Up to now, these applications have been complex and expensive. Current
availability and price decrease of sensors are facilitating the appearance of new products
and services.
The Consumer Electronics Show (CES) in Las Vegas, the biggest consumer elec‐
tronics fair in the world, is a great showcase of the new proposals either from established
players and new start-ups. Some examples: HTC and Under Armour have presented a
wristworn activity tracker designed for athletes and a chest strap heart monitor to
measure workout intensity. Misﬁt introduced Specter, a pair of sleep-tracking head‐
phones. GreatCall unveiled the Lively Wearable, a device worn on the wrist or around
the neck that tracks activity and oﬀers a mobile emergency response service via a one-
touch button that connects seniors to trained agents in emergency situations. L’Oreal
teamed up with ﬂexible electronics company MC10 created MyUVPatch, an adhesive
patch users can apply to their skin and then consult a mobile app to track their UV
exposure. Cercacor launched Ember, a ﬁnger clip sensor that measure pulse rate and
Health 4.0 Oriented to Non-surgical Treatment
515

hemoglobin, and connects to an app via Bluetooth. Omron debuted a new blood pressure
monitor that also tracks activity and sleep, and connect to an app. Neogia oﬀered a
wearable that detects sleep apnea and improves sleeping quality via a personalized
artiﬁcial intelligence that learns about the user. TempTraq oﬀered a patch-like smart
device, which monitors body temperature 24/7. QardioCore presented hearth monitor
without patches and wires to record continuous ECG, heart rate, heart rate variability,
respiratory rate, skin temperature, and activity data, which can be shared with medical
professionals. Bloomlife developed a “pregnancy wearable” that measures contractions
and sends the information to a smart phone. Bodytrak presented a wearable that measures
biometric information from the ear: body temperature, heart rate, VO2, speed, distance
and cadence, continuously and in real-time.
Pre-surgical models [13, 14]
3D printed pre-surgical planning models, based on patients’ scans, are being used all
over the world with the goal of improving accuracy and eﬃcacy of complicated
surgeries. 3D printed replicas allow physicians to evaluate and interact with patient
anatomy in ways 2D images cannot, allowing unprecedented preparation for complex
surgical cases as doctors are able to examine all angles before even touching a scalpel.
Physicians know from ﬁrst-hand experience that 3D printed patient-speciﬁc models
improve surgery, improve outcomes and result in lower treatment costs.
While originally conceived of as a way to help surgeons with complicated or
extremely delicate surgical procedures, doctors are discovering some unexpected bene‐
ﬁts from using 3D printed replicas of patients’ organs for surgical-preplanning for more
common surgeries. 3D-printed anatomical models allow doctors to get a much clearer
idea of their patient’s internal anatomy, and make better treatment recommendations
(Fig. 1).
Fig. 1. 3D printed heart (Source: 3dprint.com)
Implants [15–17]
In 2014, the ﬁrst tests of replacements of knee and hip by 3D printed ones were
completed. That year, Peking University Third Hospital’s Orthopedics Department
announced clinical trials of 3D printed artiﬁcial vertebral bodies. But dental prostheses
were the ﬁrst to be introduced in a fully operational customized medical solutions. Thus,
516
C. Soler

in 2015 the Catalan company Avinent Implant System had already passed the testing
stage and regulatory issues and was oﬀering its customized implant solutions by 3D
printing. Today, they are oﬀering a complete digital solution of personalized implants
to hospitals and clinics in areas such as reconstructive surgery, orthognathic surgery,
neurosurgery, panfacial fractures, condyle reconstructions, and reconstructions of any
part of the body, such as the spinal column.
Implants have been used for decades. What is new is the way in which 3D printing
allows the shapes of the orthotics to be created. There is an incredible ﬂexibility in terms
of the forms that can be generated. Rather than relying on boxy shapes that are easier to
produce, the 3D printer’s ability to create extremely complex geometries means a reduc‐
tion in the amount of additional hardware necessary to force a generic implant to stay
in place. Another beneﬁt is the porous nature of the printed implant which allows bones
to grow into the implant creating a natural bond. As with anything introduced into the
body, these implants must be sterile and compatible with living cells. While it is a major
challenge in the ﬁeld, newer printable biocompatible materials are being developed for
use in humans, some are synthetic polymers and some are derived from natural products
like gelatin or seaweed (Fig. 2).
Fig. 2. 3D printed spine (Source: 3dprint.com)
Organ printing [18–20]
With lots of patients on the waiting list for an organ transplant and a shortage of donated
organs for transplants, organ printing is a promising revolutionary technology for saving
human lives. The goal is to print a working organ that can be transplanted into a human,
but according to the estimations of researchers, the ability to produce 3D printed
complex organs is at least a decade away.
Meanwhile, printing living structures with bio-ink using a 3D printer is already a
reality. In 2014, the company Organovo delivered the ﬁrst sample of its 3D bioprinted
liver tissue. Today, bioprinted lung, liver, skeletal muscle, cardiac, blood vessel, skin,
bone, cartilage and nerve tissue are available. These tissue samples can be used by
pharmaceutical companies in toxicology tests of new drug candidates.
It is also possible to create structures like ears or noses. In 2016, Wake Forest Institute
for Regenerative Medicine unveiled its system that allows, after a couple of months, to
Health 4.0 Oriented to Non-surgical Treatment
517

naturally form cartilage tissue and blood vessels into those bioprinted structures. But it
will take a while to start trials in humans.
Minimal invasive interventions [21, 22]
Nanorobots have huge potential for application in medical robotics, for which they oﬀer
accuracy in their performance of operations along with minimal invasion of the patient.
Researchers from Johns Hopkins University are developing “soft” robots with micro‐
grippers that are capable of adhering to speciﬁc body tissues. These could be used in
extraction procedures for biopsies or for the localized injection of drugs. In another area,
researchers from the University of Bristol are studying how swarms of nanorobots could
detect cancer cells and carry out non-invasive surgical interventions at cellular level in
patients with tumors. For their part, researchers from the Swiss Federal Institute of
Technology in Zurich (ETHZ) are working with nanorobots magnetically guided to
carry out eye operations on patients with cataracts and glaucoma.
Surgical support [23, 24]
The team of Dr Itaru Endo, head of the department of surgery of the digestive tract and
liver transplantation of Yokohama City University, in collaboration with the Fraunhofer
MEVIS, has developed a machine-vision system, based on an iPad app, to give surgical
support in liver operations. The application, which is now being clinically evaluated,
gives access to three-dimensional surgical data. Thanks to augmented reality techniques,
during the operation images of the vascular system are superimposed so as to allow the
pattern of blood circulation in the liver to be discerned, which would otherwise be
invisible to the naked human eye. The application also shows the areas of blood ﬂow
and assesses the potential risks in real time (Fig. 3).
Fig. 3. Fraunhofer iPad app guides liver surgery through augmented reality (Source:
www.engadget.com)
Sample processing [25]
Classical industrial robots are already present in laboratories, either for the processing
of samples, for the preparation of drugs dosages, or for preparing casework DNA
samples. These activities need multiple steps that, when done with manual workﬂow,
are tedious. Robots can automate most, if not all, of this processes. That allows to
improve eﬃciency, save time, reduce human error and improve documentation, enabling
to focus on analysis and interpretation rather than process tasks.
518
C. Soler

Prosthetics [26, 27]
3D printing and open source platforms are helping to engender low-cost prosthetic
devices that users can buy at a fraction of the cost of traditional commercial prostheses.
These prostheses may have reduced functionality, but they represent a clear advantage
over traditional non-movable prostheses. In the open source domain, pure magic can
happen when designers, experts in robotics, makers, and owners of 3D printers get
together to bring robotic prostheses to people who otherwise could not aﬀord them, and
especially to children.
The e-NABLE project involves a community of more than 3,200 people who have
already created more than 700 prostheses for users all over the world. The designs are
intended to be as functional as possible, rather than to mimic the human limb, and can
be customized to suit individual needs (ﬁngers, feet, whole arms, elbows…). The “Hand-
o-matic” software allows the user to design his or her own prosthesis and to obtain the
data ﬁles for its 3D printing, which they can do themselves or get another person in the
community who has a printer to do for them (Fig. 4).
Fig. 4. 3D printed prosthetic hand (Source: enablingthefuture.org)
Rehabilitation and mobility aid [28–30]
The ﬁrst working medical exoskeleton was created in 1972 by the Mihajlo Pupin Institute
in Belgrade, Yugoslavia. Although several labs demonstrated early success with
exoskeleton development for medical use, research in industrial robotics was far more
practical and proﬁtable. It wasn’t but decades later when exoskeleton development was
restarted.
There are two main categories of medical exoskeletons: rehabilitation and mobility
aid. A rehabilitation device is used as a tool to augment a physical rehabilitation program
and is no longer used after the completion of the program. A mobility aid wearable will
be used permanently and the user is not expected to progress in the recovery process. In
this case the exoskeleton becomes an augmentative device.
As examples, the Ekso GT by Ekso Bionics has to be used by a person that still has
some mobility and the user has to be able to shift weight from one leg to the other (its
software allows a gradual decrease in the assistive force as the patient becomes stronger
during rehabilitation). In contrast, the REX by REX Bionics takes full control of
Health 4.0 Oriented to Non-surgical Treatment
519

walking, including transferring weight from one leg to the other, so its operator “rides”
the suit and does not use their own muscles to walk.
Although with less presence in the market, there also exist upper body exoskeletons,
focusing on the arm (shoulder, elbow) or hand (ﬁngers, wrist) with strengthening or
augmentation functions (Fig. 5).
Fig. 5. Mobility exoskeleton (Source: www.medicaltourismitaly.com)
Assistive solutions [31–34]
Robotics is proposing solutions to make life easier for people with disabilities. The
company Whill has gone beyond the concept “Wheelchairs” to introduce the “Personal
electric vehicle”. Robot Care Systems has created LEA (Lean Empowering Assistant)
an elderly-walker that provides support to more active and safe movement, so elderly
can live longer independently in their own home. Camanio Care focus its activity looking
for positive mealtime experience for elderly or people with diﬀerent disabilities, oﬀering
assistive eating and drinking aids like Bestic and Drinc.
In the ﬁeld of support for people with neurological disorders there is an extensive
collection of advanced interactive robots. For years, robotic teddies looking like seals,
dogs, cats, bears or rabbits are helping in the treatment of patients with autism or
Alzheimer. For instance, since 2003 PARO stimulates interactions and improves the
socialization of patients (Fig. 6).
Fig. 6. PARO (Source: www.parorobots.com)
520
C. Soler

Delivery transport [35, 36]
Hospitals move an immense amount of materials through hallways, on elevators, in
basements and to patient units. This is a complex and demanding internal logistics chal‐
lenge that has implications on costs, quality and safety. Automating delivery of this
material generates considerable savings, eﬃciency and worker satisfaction, because if
robots perform the delivery and transportation tasks they release clinical and service
staﬀ to focus on patient care.
One of the ﬁrst was UCSF Medical Center at Mission Bay, that in 2017 incorporated
a ﬂeet of 25 autonomous robots made by Aethon for the transportation of medicines,
medical equipment, bedding, food and waste. These robots have an integrated mapping
system to orient themselves wherever they might be on the premises and they commu‐
nicate with the hospital information systems by means of Wi-Fi. Their infrared and
ultrasonic sensors help them to avoid collisions and they use radio waves to open the
doors of elevators, which they can only use if they are empty of people (Fig. 7).
Fig. 7. TUG (Source: www.aethon.com)
Economical eﬃciency [37]
The current evidence shows that for most patients, all drugs from a class called statins
used to prevent cardiovascular problems are equally safe and eﬀective, so doctors are
usually advised to use the cheapest.
In a project that cost almost nothing to build, by doing one simple analysis on publicly
available NHS prescriptions data, Mastodon C, a big data start-up company, found that
in one year in England an average of £27 m a month of potentially unnecessary expen‐
diture on the two proprietary statins took place. As Sir Bruce Keogh, Medical Director
of the NHS Commissioning Board, said: “Variation in prescribing habits costs the NHS
millions of pounds a year. Transparent sharing of information will help clinicians under‐
stand whether they are over or under prescribing. This will focus minds in a way that
will not only improve the quality of treatment for patients but also reduce cost and free
up money for re-investment in other parts of the NHS.”
Health 4.0 Oriented to Non-surgical Treatment
521

3
Conclusions and Further Developments
The examples previously presented are only a few examples of what these technologies
will allow us to do over the next decade, which will bring about a revolution in the ﬁeld
of medicine. We won’t have to wait longtime to see achievements today only featured
in sci-ﬁ.
But Health 4.0 is not only a technological issue. Legal and ethics aspects will have
to be faced with an open mind but in a very strict way: the border between what will be
feasible and what acceptable too often won’t be clear.
References
1. http://www.sciencemag.org/news/2017/04/self-taught-artiﬁcial-intelligence-beats-doctors-
predicting-heart-attacks
2. https://www.ibm.com/watson/health/oncology-and-genomics/oncology
3. https://www.mskcc.org/about/innovative-collaborations/watson-oncology
4. http://news.stanford.edu/2017/01/25/artiﬁcial-intelligence-used-identify-skin-cancer/
5. https://www.nature.com/nature/journal/v542/n7639/full/nature21056.html
6. http://robohub.org/the-state-of-telepresence-healthcare-and-telemedicine/
7. https://www.intouchhealth.com/
8. http://www.datapine.com/blog/big-data-examples-in-healthcare/
9. http://scopeblog.stanford.edu/2013/09/27/big-data-big-ﬁnds-clinical-trial-for-deadly-lung-
cancer-launched-by-stanford-study/
10. https://mapr.com/blog/5-big-data-trends-healthcare-2017/
11. http://www.mobihealthnews.com/content/ces-2016-running-list-health-and-wellness-
devices
12. http://medicalfuturist.com/10-best-health-technology-innovations-ces-2017/
13. http://blog.stratasys.com/2017/03/08/3dheart-surgical-model-study/
14. http://www.materialise.com/en/blog/how-3d-printed-renal-models-improve-pre-surgical-
decisions
15. https://digital-health.avinent.com/eng/default.cfm
16. https://3dprint.com/165236/3d-printed-vertebrae-india/
17. http://www.faseb.org/Portals/2/PDFs/opa/2016/FASEB-HorizonsInBioscience-3D-
Bioprinting.pdf
18. http://www.qmed.com/mpmn/gallery/image/1-3-d-printed-tissues-and-organs-promise-and-
challenges
19. https://www.economist.com/news/science-and-technology/21715638-how-build-organs-
scratch
20. http://gizmodo.com/3d-bioprinting-just-took-a-major-step-forward-1758803208
21. https://spectrum.ieee.org/robotics/medical-robots/medical-microbots-take-a-fantastic-
voyage-into-reality
22. http://robohub.org/minimally-invasive-eye-surgery-on-the-horizon-as-magnetically-
guided-microbots-move-toward-clinical-trials/
23. https://www.engadget.com/2013/08/22/fraunhofer-ipad-app-guides-liver-surgery/
24. https://www.apple.com/ipad/life-on-ipad/new-eyes-for-hands-on-surgery/
25. https://www.forensicmag.com/article/2016/01/robotics-rescue-automated-sample-
processing
522
C. Soler

26. http://enablingthefuture.org/
27. http://www.openhandproject.org/
28. http://exoskeletonreport.com/2016/06/medical-exoskeletons/
29. http://eksobionics.com/
30. https://www.rexbionics.com/
31. http://whill.us/
32. http://www.robotcaresystems.nl/
33. http://www.camanio.com/en/
34. http://www.parorobots.com/
35. http://www.ucsfmissionbayhospitals.org/articles/high-tech-tug-robots-do-heavy-lifting-at-
mission-bay.html
36. http://www.aethon.com/tug/tughealthcare/
37. https://theodi.org/news/prescription-savings-worth-millions-identiﬁed-odi-incubated-
company
Health 4.0 Oriented to Non-surgical Treatment
523

Collaborative Robots for Surgical Applications
´Alvaro Bertelsen1(B), Davide Scorza1, Camilo Cort´es1, Jon O˜nativia2,
´Alvaro Escudero2, Emilio S´anchez3, and Jorge Presa2
1 Vicomtech-IK4, San Sebasti´an, Spain
abertelsen@vicomtech.org
2 Egile Innovative Solutions, Mendaro, Spain
3 CEIT and TECNUN, San Sebasti´an, Spain
Abstract. This works presents the impact that collaborative robotic
technologies can oﬀer for surgical applications, with emphasis on the
tracking and execution steps. In particular, a new workﬂow for spine and
trauma surgery is presented, in which a miniature mechanical tracker is
attached directly to the patients’ bony structure (Patent pending). The
tracker is capable of following the patients’ motion with high precision,
measuring the deviation with respect to the trajectories deﬁned in the
surgical plan and providing a feedback channel to a robot which assists
the surgeon holding the surgical tools in place. The clinical application
of vertebral fusion has been chosen as testing scenario and preliminary
results are presented to demonstrate the feasibility of this concept.
1
Introduction
The recent advances in artiﬁcial intelligence have paved the way of the Industry
4.0 concept, loosely deﬁned as the fourth industrial revolution, preceded by
ones triggered by the introduction of steam power (ﬁrst industrial revolution),
assembly lines and electricity (second) and control and automation (third). In
the upcoming industrial revolution, robots and automated system should be
able to work in coordination, sharing information between them and taking
decentralised decisions. According to the deﬁnitions of Hermann, Pentek and
Otto, Industry 4.0 is centred on the concept of cyber-physical systems (CPS)
–deﬁned as networks of interacting physical and digital elements– with six key
design aspects of Industry 4.0: interoperability, virtualisation, decentralisation,
real-time capability, service orientation and modularity [1]. Among the possible
interaction modalities covered by the Industry 4.0, collaborative robots –deﬁned
as robotic assistants designed to work hand-to-hand with human workers– take
a relevant role to increase manufacturing eﬃciency and ﬂexibility (Fig. 1).
Surgery –and healthcare in general– is also experiencing the push of the
Industry 4.0 technologies, opening the way to a new era of intelligent and data-
driven surgery and giving birth to the new discipline of surgical data science [2].
According to Maier-Hein et al., three key clinical applications would beneﬁt
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_43

Collaborative Robots for Surgical Applications
525
Fig. 1. The evolution of surgical practice. In the past, the physician oﬀered treatment
with minimum equipment. In the present, a wealth of information is given by a set of
isolated sources, leaving the surgeon with the task of using all the available information
based on their experience and knowledge domain. In the future, surgery will be based
on a network of intelligent devices –among them, surgical robots– that will oﬀer a
holistic processing of all the available patient data. Picture originally published by
Maier-Hein et al. [2]
from the achievements of surgical data science, which are surgical training, deci-
sion support and context-aware assistance [2]. On the last area, autonomous
assistance can provide surgeons with timely information through surgical phase
recognition, decision-support through patient-speciﬁc simulations and collabo-
rative robots, the latter also identiﬁed as a key component of Industry 4.0 ini-
tiatives.
The introduction of robots into the operating rooms has been gradual and
slower than expected. In fact, tasks carried out during surgery are inherently
collaborative, as they are always exerted on a human person –the patient– and
with the assistance of the surgeon, nurses, anesthetists and the rest of surgical
staﬀ: in the operating room, there is no way to isolate the robots from persons,
as usually done in industry. These requires providing robots with superior sens-
ing, cognitive and interaction capabilities, to eﬀectively cope with the lack of
structure of the surgical situation and oﬀer the required level of safety to avoid
harm to the patient and surgical staﬀ.

526
´A. Bertelsen et al.
To achieve the capabilities envisaged by the Industry 4.0 and Surgical Data
Sciences paradigms, collaborative surgical robots ﬁrst need to oﬀer solid solutions
to two fundamental problems –registration and tracking– required to eﬀectively
sense the patients’ location and follow it over time. Registration consists in the
computation of the geometrical transform that maps the coordinate system of
the surgical plan to the coordinate system of the real-world scenario, whereas
tracking consists on the detection of displacements over time between the align-
ment computed in the registration. In this work, a novel surgical assistant is
presented, which oﬀers innovative solutions to these two basic problems. In the
following sections, an overall description is given as well as a their application
to the speciﬁc ﬁeld of spinal surgery, which has demanding requirements that
justify their development.
2
Bone-Mounted Mechanical Tracker
2.1
Need of More Precise Tracking Technology
The need of tracking technology in the operating room is twofold, as it is required
to follow the motion of the instrumental and the patient. Two tracking technolo-
gies are most widespread nowadays, which are optical and electromagnetic track-
ing. Optical tracking employs a set of cameras which detect groups of markers
attached to the surgical instruments and the patient’s anatomy, computing their
positions in the three-dimensional space. Electromagnetic tracking employs gen-
erator that emit a space-varying magnetic ﬁeld which is sensed by small detectors
ﬁtted on the instruments. Whereas optical tracking has relatively high precision,
it suﬀers from the need to keep a clear line of sight between the cameras and
the markers. On the other hand, electromagnetic tracking does not suﬀer from
occlusions, but its precision is lower and its performance is seriously aﬀected by
the proximity of metallic objects, which is a usual situation in many surgical
scenarios such as trauma surgery [3].
Although optical tracking is acknowledged as the tracking technology that
oﬀers the highest level of precision, its error levels are still insuﬃcient for cer-
tain procedures such as vertebral fusion, which requires submillimetric precision
to avoid nerve damage or vertebral fracture [6]. Determination of the optical
trackers’ error levels is a question that is diﬃcult to answer: a study pub-
lished by Elfring et al. compared multiple optical trackers –all of them avail-
able commercially– and estimated error levels between 0.22 and 1.02 mm [4].
Although the minimum error levels that can be attained are reduced, the optical
trackers’ performance is seriously degraded by the distance and angle between
the markers and cameras, which are factors that are diﬃcult to control in a real
surgical scenario.
On the other hand, mechanical tracking oﬀers considerable advantages for the
surgical ﬁeld which have not been adequately explored. These devices are based
on a mechanical device with sensors on each of their joints, which detect the dis-
placement of its end-eﬀector, which is attached to the patient anatomy. These
devices can be lightweight, are unaﬀected by occlusion and oﬀer unmatched levels

Collaborative Robots for Surgical Applications
527
of precision due to their stable build and highly precise pose-estimation meth-
ods based on the resolution of their forward kinematics equations. Their main
drawbacks are their limited working space and their obtrusiveness, although the
latter can be alleviated by an appropriate design that can be eﬀectively deployed
without interfering with the surgeons’ workspace.
2.2
Description of the Robotic Assisted Surgery System
The system that is presented here allows surgeons to plan a surgery before the
patient goes into the operating room on a preoperative CT scan. During the
surgery, the robotic assistant system provides a robust tool guide that will be
positioned and oriented according to the preplanned trajectories in order to
perform the intervention in a minimally invasive way. This is achieved by means
of a two step process. First, the position of the patient is registered accurately
by attaching a clamp with a registration ﬁducial on a non-critical and accessible
area of the targeted bone. Second, after the position of the patient is registered,
the relative movements between the robotic arm and the patient are constantly
monitored in real-time by an mechanical tracking system that is mounted on the
end eﬀector of the robotic arm. When bone motion is detected, the robotic arm
compensates such movement in order to keep the same position and orientation
of the tool guide with respect to the bone. Figures 2a and b illustrate the tracking
system attached to the vertebra with the tool guide pointing to the targeted area.
(a)
(b)
Fig. 2. Tool guide and mechanical tracking system. Both, the tracking system and the
tool guide are mounted on the end-eﬀector of a robotic arm. The distal end of the
tracking system is attached to the vertebra in order to monitor changes in position and
orientation of the targeted bone. The robotic arm keeps the tool guide at the same
trajectory at all time by compensating for bone movement.
The system is designed to achieve high accuracy without slowing down the
workﬂow of the surgeon in the operating room. To achieve this goal, the mecha-
nism to mount the registration ﬁducial or the tracking system on the clamp that

528
´A. Bertelsen et al.
is attached to the bone is based on a magnetic kinematic coupling system. The
base part of the coupling mechanism belongs to the clamp that is ﬁxed to the
bone. This base part presents three V-shaped groves with a magnet at the center.
The top part of the coupling mechanism that goes in the registration ﬁducial
and at the distal end of the tracker presents three spheres and another magnet.
This coupling mechanism creates a precise and repeatable interface between two
rigid bodies [5]. This system provides six contact points, two per sphere, in order
to guarantee that the coupling mechanism constrains six degrees of freedom of
the relative movement between the clamp and the tip of the tracker or the regis-
tration ﬁducial. Figure 3 illustrates the coupling mechanism and the registration
ﬁducial.
(a)
(b)
Fig. 3. Registration ﬁducial with magnetic kinematic coupling mechanism to mount
the ﬁducial on the clamp in a fast a precise manner. In (a) the base and the top portions
of the kinematic coupling mechanism can be seen. In (b) the ﬁducial is mounted on
the clamp.
The joint coupling of tracking system and tool guide on the robotic arm’s end
eﬀector makes the system very accurate and avoids the presence of an additional
elements such as cameras or magnetic ﬁeld generators. The tracker’s base and
the tool guide belong to the same rigid body and their geometrical relationship
is measured during the manufacturing process. Therefore, there is no need of
any extra calibration step during the intervention. Moreover, since the tracking
system is monitoring the exact positioning of the tool guide, the overall accuracy
of the system is higher than the accuracy of other systems where the tracker
monitors the position of the base of the robot. In cases, the overall accuracy
is directly inﬂuenced by the precision of the robotic arm, which is eﬀectively
compensated with the proposed conﬁguration.

Collaborative Robots for Surgical Applications
529
2.3
Mechanical Tracking System
The tracking device is a passive mechanical system with six angular joints that
are constantly monitored with rotary encoders. In order to monitor changes
in position and orientation of the bone with respect to the tool guide without
constraining the mobility of the bone the tracking system has to present at
least six degrees of freedom. The exact position and orientation of the tip of the
tracker are obtained by combining the readings of the rotary encoders with the
forward kinematic equations of the kinematic chain.
q1
q2
q3
q4
q5
q6
Tool guide
Tracker
Roboti carm
end effector
Fig. 4. Schematic representation of the kinematic chain of the mechanical tracking
device.
In Fig. 4a schematic representation of the tracking system is provided. The
system presents six rotary joints with its corresponding rotary encoders. The
ﬁrst, fourth and sixth encoders (q1, q4 and q6) track the rotational movement
along the longitudinal axes of the corresponding segments and the second, third
and ﬁfth encoders (q2, q3 and q5) track the rotational movement along axes that
are perpendicular to the segments. The accuracy of the tracker depends on the
length of the segments, the resolution of the encoders and the manufacturing
and mounting process of the tracker itself. With a combination of four segments
of lengths of the order 10 cm and six rotary encoders with resolutions of 16 bits
an overall theoretical accuracy of the order of 50 µm can be achieved. However,
experimental evaluation is required to include other factors such as calibration
errors, slacks and dead zones of the kinematic chain. Ongoing work is currently
being done on this subject (Fig. 5).

530
´A. Bertelsen et al.
3
Application on Spinal Surgery
3.1
Description of the Surgical Procedure
The proposed mechanical tracking device can have considerable impact on high-
precision surgical tasks, among which vertebral screw insertion is a relevant
example. Screw insertion is fundamental task on multiple spinal surgeries, such
as cervical body fusion and transpedicular ﬁxation, which consists in the inser-
tion of screws into the vertebrae through their pedicles and their anchoring on
their bodies’ cortical bone. The precision requirements for this task are quite
demanding, as there are considerable risks of nerve damage –which can lead to
intense pain or even disability– or rupture of surrounding blood vessels, leading
to internal hemorrhage. The required level of precision varies among vertebrae
–ranging from 0.1 mm up to 3.8 mm– with the largest majority of vertebrae
requiring sub-millimeter accuracy and with the particular case of the T5 verte-
bra which may require insertion of screws with diameters larger than the criti-
cal pedicular dimensions [6]. Unsurprisingly, the largest part of surgical robots
designed for spinal applications have focused on this particular task [7].
Fig. 5. Schematic of screw insertion. A misplaced screw can breach the spinal canal
producing nerve damage (left) or breach the outer vertebral walls damaging surround-
ing blood vessels (centre). A correctly placed screw should pass through the axis of the
pedicle and be ﬁrmly anchored on the cortical bone (right).
3.2
Surgical Planning
According to the proposed workﬂow, the patient has to undergo a Computerised
Axial Tomography (CAT) scan before the surgery. The acquired CAT scan is
loaded into 3D Slicer1, extended with a custom planner speciﬁcally designed
for this purpose [8]. The modules permit interactive placement of the screws,
deciding their with and length and ensuring that appropriate anchoring on the
cortical bone is achieved.
1 http://www.slicer.org, last visited on July 2017.

Collaborative Robots for Surgical Applications
531
3.3
Intra-operative Workﬂow
Once the patient enters the operating room and is put under anesthesia, a small
incision is made over the vertebrae to be operated and a clamp is attached to
one of the spinal processes. Then, a 3D printed ﬁducial object is attached to
the clamp and a set of two ﬂuoroscopic images is acquired. The ﬁducial object
projection is segmented automatically and the relative position between the
imaging plane and the origin of the ﬁducial object is computed. Then, a geometry
containing two three-dimensional planes is produced.
Once the geometry is computed, the pre-operative CAT scan is placed into
the geometrical scene by means of a 2D-3D registration algorithm. This generates
a set of Digitally Reconstructed Radiographs (DRR), projecting the CAT data
onto the radiographic planes. The DRRs and actual radiographs are compared
–computing a similarity metric– and the CAT image’s position and orientation is
updated until the similarity metric reaches a maximum value. A multi-resolution
scheme using 3 levels –with downsamplings of 4, 2 and 1– is used to prevent
falling into local minima.
After registration, the pre-operative and intra-operative are brought into
alignment, but patient’s motion has to be tracked to avoid misalignments. To
achieve this, the ﬁducial is removed from the clamp and the tracker’s end eﬀec-
tor is connected in its place. Thus, any subsequent motion of the patient will
be detected by the tracker and transmitted to the rest of the system. This feed-
back motion signal is fed into a robotic manipulator, which is commanded to
align the tool holder’s axis to the insertion trajectories over the pedicles. The
surgeon is able to mill the pedicle and then insert the screw using surgical tools
inserted through the cannula. Once a screw is successfully inserted, the robot is
commanded to place the instrument holder on the next pedicle and the insertion
procedure is repeated.
3.4
Registration Performance
Among the closed loop tracking system, the 2D-3D registration is the largest
source of error, so its accurate implementation is critical to locate the patient’s
anatomy with respect to the mechanical tracker. An inaccurate registration will
lead to a poor tracking, no matter how precise the mechanical tracker is. In the
presented workﬂow, the DRRs and the actual radiographs have similar contrast,
so the use of the Normalised Cross Correlation (NCC) metric is appropriate,
along with a Powell optimiser, with oﬀers good convergence properties without
the need to compute the metric’s gradient. The implemented 2D-3D registration
algorithm was tested following a public evaluation protocol –originally published
by the University of Ljubljana–, which deﬁnes the 2D and 3D images, starting
positions and performance evaluation metrics [9].
The accuracy of 2D-3D registration algorithms is aﬀected by the number
of 2D radiographs included in the registration and their relative angles [10].
Although better results are obtained increasing the number of images, surgeons

532
´A. Bertelsen et al.
are normally reluctant to acquire more than two radiographs during interven-
tions, so this number was ﬁxed at two. The registration performance was found
to be aﬀected not only by the relative angle between radiographs, but also by
the absolute angle, measured with respect to the patient axes. To measure this
lat-ap
50-130
50-150
30-110
30-130
50-AP
30-AP
0.4
0.45
0.5
0.55
0.6
L1
(a)
lat-ap
50-130
50-150
30-110
30-130
50-AP
30-AP
0.3
0.35
0.4
0.45
0.5
0.55
0.6
L2
(b)
lat-ap
50-130
50-150
30-110
30-130
50-AP
30-AP
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
L3
(c)
lat-ap
50-130
50-150
30-110
30-130
50-AP
30-AP
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
L4
(d)
lat-ap
50-130
50-150
30-110
30-130
50-AP
30-AP
0.3
0.35
0.4
0.45
0.5
0.55
0.6
L5
(e)
Fig. 6. Boxplots of observed registration errors on 2D-3D registrations performed on
each vertebra of the Ljublajana dataset, with diﬀerent pairs of 2D images. Lowest reg-
istration errors are obtained with pairs separated by 80 to 100◦–in accordance with
intuition– although the standard pair –formed by the antero-posterior and lateral views–
never obtains the best results. On each plot, the horizontal axis marks the angles of each
pair of 2D images and the vertical axis marks the target registration error in mm.

Collaborative Robots for Surgical Applications
533
Table 1. TRE for the precision evaluation carried out using the Ljubljana dataset.
For each vertebra, the mean TRE obtained for three image pairs –50–150, 30–110 and
30–130 separated by 100, 80 and 100◦respectively– are compared with the standard
AP-LAT pair. The standard pair did not obtain the most precise result in any case
(for each vertebra, the best result is marked with bold font).
Vertebra LAT-AP 50–150 30–110
30–130
L1
0.457
0.431
0.443
0.390
L2
0.374
0.302
0.4769
0.449
L3
0.359
0.213
0.31302 0.249
L4
0.275
0.315
0.252
0.156
L5
0.395
0.333
0.284
0.332
eﬀect an experiment was made using the Ljublajana dataset, running multiple
registrations using diﬀerent set of 2D images separated by 40, 60, 80 and 100◦.
For each included vertebra, a series of registrations –with initial mean Target
Registration Error (mTRE) up to 10 mm– were conducted, measuring their ﬁnal
mTRE. Results –summarised on Fig. 6 and Table 1– show that optimal angu-
lar separation between images is obtained with values between 80 and 100◦–in
accordance with the results published by Tomaˇzeviˇc et al. [10]–, but diﬀerent
image pairs with the same angular separation give considerably diﬀerent results.
In fact, the standard pair formed by the lateral and antero-posterior images
never gave the best precision, as illustrated on Fig. 6 and Table 1. Replacing the
standard pair by a more optimal one can yield error decrements between 0.07
and 0.12 mm.
It is hypothesised that the latter eﬀect is caused by the better view of the ver-
tebral pedicles and processes oﬀered by the antero-posterior and oblique images,
which could have more structure to help the registration to obtain a better ﬁt.
This lack of performance of registrations that included the lateral view was also
observed on phantom experiments, although those scenarios have additional fac-
tors which are not present in the Ljubljana dataset. These are the particular
geometry of the ﬁducial object, which –in the lateral view– has its ﬁducial beads
aligned in a quasi-linear pattern, which is an ill-posed situation for computation
of the radiographic plane position. In addition, the radiographs ﬁeld-of-view is
adjusted to cover the largest possible section of the vertebra, pushing the ﬁducial
beads’ close to the edges of the images which, in the case of radiographs acquired
with image-intensiﬁers, are more aﬀected by distortion. Further experiments are
required to better describe this eﬀect.
4
Discussion and Conclusions
A novel collaborative surgical robotic assistant has been presented, along with
its application for spinal interventions. A description of its components has
been given, with particular emphasis on its mechanical tracking and image

534
´A. Bertelsen et al.
registration systems. Laboratory experiments characterising their performance
have been presented, although additional experiments on phantoms and cadavers
are underway.
A novel mechanical tracking have been presented, giving a description of its
architecture and components. Preliminary testing has given error levels around
50 µm, which is below error levels oﬀered by currently available surgical track-
ing systems. Additional testing is required to fully characterise its performance
and integration tests are needed to evaluate its functionality in more realistic
conditions.
In addition, a characterisation of the image registration system –identiﬁed
as the largest contribution of error– has been given. Preliminary testing with a
publically available dataset has been presented, demonstrating that it precision
level is quite high in comparison with other algorithms. In addition, it was shown
that its performance is aﬀected not only by the angular separation between 2D
images, but also by their angular orientation with respect to the patient axes.
Additional experiments are required to better characterise this eﬀects, as well
as integration testing on phantoms and cadavers.
The presented collaborative robots falls in the category of intelligent tool
holders, capable of holding surgical instruments with minimal deviations and
unaﬀected by tremor, fatigue and patient motion. Although the presented track-
ing system permits eﬀective following of the patient’s movements, there is cer-
tain room for improvement in the degree of autonomy given to the robot, which
requires better sensing, cognitive capabilities and integration with the surgical
situation. The prospect of fully robotised surgeons is certainly far-fetched, but
the idea of autonomous robots capable of performing minor surgical tasks is not.
In fact, a milestone has been achieved by the STAR system, a robot capable
of performing fully automated sutures [11], and similar achievements could be
translated to the spinal surgical ﬁeld.
References
1. Hermann, M., Pentek, T., Otto, B.: Design principles for Industrie 4.0 scenarios. In:
2016 49th Hawaii International Conference on System Sciences (HICSS) 39283937.
IEEE (2016). doi:10.1109/HICSS.2016.488
2. Maier-Hein, L., et al.: Surgical Data Science: Enabling Next-Generation Surgery
(2017). http://arxiv.org/abs/1701.06482
3. Franz, A.M., et al.: Electromagnetic tracking in medicine: a review of technology,
validation, and applications. IEEE Trans. Med. Imaging 33, 17021725 (2014)
4. Elfring, R., de la Fuente, M., Radermacher, K.: Assessment of optical localizer
accuracy for computer aided surgery systems. Comput. Aided Surg. 15, 1–12 (2010)
5. Alexander, H.: Slocum: design of three-groove kinematic couplings. Precis. Eng.
14–2, 67–76 (1992)
6. Rampersaud, Y.R., Simon, D.A., Foley, K.T.: Accuracy requirements for image-
guided spinal pedicle screw placement. Spine (Phila. Pa. 1976) 26, 352–359 (2001)
7. Bertelsen, A., Melo, J., Snchez, E., Borro, D.: A review of surgical robots for spinal
interventions. Int. J. Med. Robot. Comput. Assist. Surg. 9, 407–422 (2013)

Collaborative Robots for Surgical Applications
535
8. Fedorov, A., et al.: 3D slicer as an image computing platform for the quantitative
imaging network. Magn. Reson. Imaging 30, 13231341 (2012)
9. Tomaeviˇc, D., Likar, B., Pernuˇs, F.: Gold standard data for evaluation and com-
parison of 3D/2D registration methods. Comput. Aided Surg. 9, 13744 (2004)
10. Tomaˇzeviˇc, D., Likar, B., Pernuˇs, F.: 3D/2D image registration: the impact of
X-ray views and their number. Med. Image Comput. Comput. Assist. Interv. 10,
4507 (2007)
11. Shademan, A., et al.: Supervised autonomous robotic soft tissue surgery. Sci.
Transl. Med. 8 (2016)
12. Knez, D., Likar, B., Pernus, F., Vrtovec, T.: Automated pedicle screw size and tra-
jectory planning by maximization of fastening strength. In: Computational Meth-
ods and Clinical Applications for Spine Imaging, CSI 2015, pp. 3–13 (2016)
13. Dao, T.T., et al.: Multimodal medical imaging (CT and dynamic MRI) data and
computer-graphics multi-physical model for the estimation of patient speciﬁc lum-
bar spine muscle forces. Data Knowl. Eng. 9697, 318 (2015)

New Technologies in Surgery
Alícia Casals
(✉), Narcís Sayols, and Josep Amat
Centre of Research in Biomedical Engineering (CREB), Universitat Politècnica de Catalunya,
(UPC), Barcelona, Spain
{alicia.casals,narcis.sayols,josep.amat}@upc.edu
Abstract. The progress of technology and robotics in industry has not yield to
an equivalent development in the medical ﬁeld. This paper analyses surgical
procedures from the point of view of industrial processes looking for analogies
in both ﬁelds so as to evaluate the possibilities of using equivalent technologies
in both of them. After analyzing surgical specialties from the mechanical point
of view, the actions to be performed, and the main requirements as precision and
working conditions, a look at the main challenges that surgical robotics should
face is presented.
Keywords: Surgical robotics · Image registration · Tracking
1
Introduction
Surgery is the medical practice that implies mechanical manipulation of anatomical
structures as a healing means. Since this treatment consists in mechanical manipulation
of tissues, a logical consequence is considering the possibility of applying production
techniques and particularly CAD/CAM and those related to the industry 4.0. However,
while technological progress has changed drastically the production processes in
industry, the application of technology in the medical ﬁeld, although having had a very
high impact in the medical practice, has not evolved at the same pace than in industry.
The reasons for this dephasing is manifold [1]. First, the variability among humans.
Patients diﬀer one from each other in size, shape or proportions, thus impeding the
possibility to routinely program a task on a predeﬁned object and of using frequently
used techniques as CAD/CAM. Humans diﬀer from machines or industrial parts also
since many tissues in the human body are soft, deformable, and even when dealing with
hard tissues they frequently cannot be ﬁxed to a static position. In this context trajectories
cannot be previously deﬁned and there is a continuous need of adapting the actuation to
changing environments. Unfortunately the modeling of human organs and tissues is not
yet good enough to allow the development of algorithms for the register of patient-
modalities and of tracking the human movements, so as to adequately positioning the
robot with respect to the human anatomical part. And last but not least, when putting
technology into practice in the medical ﬁeld many other issues arise referring to ethical,
legal and safety aspects which should be considered when designing an application or
system.
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_44

2
Technologies
Medical practice frequently carries with it three stages. A ﬁrst and previous stage is
diagnosis, which is critical since it determines the next steps in the process of healing
as well as the necessary way to guarantee the adequate treatment, Fig. 1. The second
stage is the one that should implement the best possible treatment that can give response
to the speciﬁc needs identiﬁed in the diagnosis phase. This second stage passes through
the medical knowledges, but not only this, it also relies on the clinical eye of the phys‐
icist, who needs a wide data-base of all symptoms, of the diagnosis and of the best
treatments already experienced. All these information is crucial to help taking the right
decisions and perform adequately. A treatment admits many variants:
– A psychotherapeutic treatment, in which the own organism ﬁghts against the disease
it suﬀers by means of the help of psychotherapy.
– A pharmacologic treatment, based on drugs, and chemotherapy addressed to chem‐
ically combat cancer.
– A kinesiotherapy treatment, which combines diﬀerent physical stimulus by means
of manual manipulation and, eventually, with the aid of other mechanical or elec‐
tronic elements or devices on the diﬀerent body parts, aiming to pursue therapeutic
outputs.
– Surgery, understood as the most severe physical actuation that can be of diﬀerent
types: Ectomies, Reparative-Reconstructive and Substitutive surgery, and Radio‐
therapy, Laser and US surgery applied over the speciﬁc aﬀected anatomy.
Fig. 1. Schema of the three steps involved in a healing process and the diﬀerent kind of treatments
Finally, a third phase, after a good diagnosis and a speciﬁc treatment may require a
rehabilitation step, both to complete the previous treatment and to recover the quality
of life after an accident, trauma or after damages caused by aging. This work addresses
basically severe treatments, that is, surgery. Their main characteristics are described in
the following subsections.
2.1
Ectomies
One of the older techniques applied is that of removing the anatomic part considered as
the cause of the disease. This technique, that uses the suﬃx Ectomy to name the concrete
treatment, are usually related to actions such as cutting away, amputation or surgical
ablation. This technique comes from the ancient Greeks, meaning the suﬃx ectomy, the
New Technologies in Surgery
537

act of cutting. This name is used then to refer to the kind of interventions that imply
cutting or removing some materials, parts or tissues from inside the body. This surgery
then implies a sequence of actions which are cutting, removing and a last action of
sewing or ﬁxing the parts. Actually, all technological advances are oriented to improve
the technology of the corresponding instruments (electro-cauterization, stapling …) as
well as to robotize the intervention when possible using CAD/CAM techniques.
However, as robotizing is normally diﬃcult, surgical robotics frequently relies on tele‐
operated robots.
Cutting and separating tissues, was performed traditionally in open surgery. It was
in the decade of 1980 when it was substituted in many cases by surgery through small
incisions using speciﬁc instruments to operate through trocars. This move from open
surgery to minimally invasive surgery, MIS, based on the use of speciﬁc instruments
opened the doors, on the one hand to the possibility that these instruments were robot‐
ically controlled and, on the other hand, to endow these instruments with more degrees
of freedom than those operated manually, Fig. 2. These instruments speciﬁcally devel‐
oped for MIS have been evolving facilitating a relatively comfortable operating posture
and gestures, even when accessing inside the body through a unique incision. They gave
place to the so called Single Port Surgery or Single Incision Laparoscopic Surgery
(SILS). Operation through a unique port into the abdominal cavity through the belly
button, that avoids new visible scars, has also evolved towards the Natural Oriﬁces
Transluminal endoscopic Surgery (NOTES). When this surgery, MIS, is used for joints
repair, it is named arthroscopic surgery.
Fig. 2. (a) Conventional manual instruments endowed with opening-closing movements and a
rotation (roll), and (b) instrument endowed with opening-closing movements plus the three
orientation degrees of freedom used in robotized laparoscopic surgery.
2.2
Reparative-Reconstructive-Substitutive Surgery
This kind of surgery refers basically to orthopedic interventions, those dealing with bone
tissues, such as: hip, knee or shoulder surgery, maxillofacial reconstruction, or spine
repair. This set of surgeries relies on mechanical procedures, closer to those going on
in industry as drilling, milling, sawing, holing or puncturing. Orthopedic surgery, or
orthopedics, is the branch of surgery concerned with conditions involving the muscu‐
loskeletal system. This kind of surgery comprises diﬀerent kind of operations: cutting,
modelling and inserting, aiming to complete damaged bone structures. Drilling, using
bolds and rods, plaques and scaﬀolds to join, immobilize or enforce bone parts. The
third technique consists in substituting naturals structures by artiﬁcial elements (pros‐
thesis).
538
A. Casals et al.

The ﬁrst case constitutes the application of CAD/CAM in medicine. A typical inter‐
vention is maxillofacial reconstruction, required after removal of a part of the jaw to
eliminate cancerous tissues. For this intervention, the planning of the optimal cut of part
of the bone or another body part, as the tibia, is used to extract the exact shape and size
of the missing part of the jaw bone. Once planned, the right execution of the cut is
achieved using CAM techniques, guaranteeing this process the optimization of both, the
extracted bone part and the posterior insertion. This greatly improves manual operation
that relies on the human visual perception and an iterative try and correct strategy.
A second type of surgeries, that of performing mechanical operations such as
inserting screws, plaques or scaﬀolds, is another example of how CAD/CAM techniques
can be applied to orthopedic surgery. In these surgeries CAD constitutes the valid
instrument for the 3D modelling of the devices, while simulation techniques provide the
necessary assistance to help deciding the exact place where perforations should be done,
both to assure patient integrity and to guarantee the maximum robustness and mechanical
eﬃciency. Referring to the execution, the CAM process can be performed manually or
robotized, depending on the surgical specialty, which may or may not allow immobili‐
zation of the anatomical part. When the anatomy cannot be immobilized, as in the case
of the femur or the knee, it is also possible to resort to molds, which are ﬁxed over the
bone in a way that the drillings and machining to perform are not subject anymore to
the manual imprecision of the surgeons.
In the third case, that of implanting prosthesis, the work to be done carries with it
some machining. But in this case, when the work is done manually, the diﬀerences
between the optimal hole and that really executed, the necessary to implant the pros‐
thesis, can be adjusted with the injection of biological cements, which are manufactured
from the own patient bone extracted during the machining previous to the ﬁtting of the
prosthesis. For that, in the previous phase the CAD permits the computing of the exact
geometrical position of the prosthesis to implant to achieve the maximum mobility and
robustness as well as assuring the best position of the prosthesis in the 3D space with
respect to the symmetric healthy joint.
In all cases, their robotization has been introduced at the time that the registering
problems have been solved. Register can imply CAD-images (TC and radiography) or
CAD-CAD (TC and 3D model based on taking ﬁducial points by means of a navigator).
As registration in real time is not always possible, when this happens a partial or total
immobilization of the anatomical part is required. These techniques are quite adequate
to be robotized. As happens when using CAD/CAM techniques, the automatic operation
can be programmed and thus the system can operate autonomously. However, due to
the many eventualities that may appear along an intervention, they are performed by
humans, but with the assistance of a teleoperated robot, or the more and more used
collaborative robots cooperating with humans. Working in cooperation, the human and
the robot both hold the same surgical instrument and while the human guides the instru‐
ment movements, the robot supervises the task according to a pre-established plan and
the perception of the environment conditions, allowing the human movements when
moving in a free space, slowing them down when operating close to critical areas or
steering them when approaching a target to facilitate its safe guidance.
New Technologies in Surgery
539

2.3
Radiotherapy
Radiotherapy constitutes a kind of treatment alternative or complementary to surgery
which as surgery has a direct incidence on the body tissues, but in contrast to surgery,
radiotherapy does not interact physically with the human body, being in this sense less
invasive. The challenge is to completely avoid any damage to healthy tissues. With this
aim, the radiation source, or radiation beam, should be highly focalized towards the
target tissues. This treatment should be imperatively robotized since it is not possible to
hold and guide the beam manually due to the high requirements in precise focalization,
together with its high weight. Due to the need of avoiding damaging healthy tissues, the
beam is not statically focusing the target from a given point, but following a trajectory
in space while keeping the target point as the center of the spherical surface that described
the end eﬀector tip. In this way, the damaged tissue receives a high radiation, while other
tissues are only slightly radiated avoiding their damage. Thus, the robot guarantees the
right focalization and minimizes the collateral eﬀects, Fig. 3. To facilitate the access of
the tool tip to all the desired volume, it is possible to add external degrees of freedom,
to control the bed position and orientation.
Fig. 3. Focalization of the target point with a robot that follows a trajectory with curvature radii
R and center of rotation on the target tissue.
2.4
Laser Surgery
Laser has become a multifunctional source of energy. Its name stands for Light Amplifica‐
tion by Stimulated Emission of Radiation). Its polarized light allows concentrating all the
power at a given point or trajectory producing different effects on the tissues according to the
characteristics and the parameters of the radiation. Laser can be used to cut, for ablation, for
welding or cauterizing. As soft tissues are rich in water, laser is good for cutting them. Laser
therapies are especially good to treat soft tissues. With laser it is possible to have a
destroying effect strongly and precisely localized, making precise cuts. With low power
beams the effect on the surface areas produces ablation. Laser can weld when phototherm‐
ically the extracellular components of connective tissues fuse themselves (retina, cornea,
plastic surgery). The main laser surgery is addressed to the eye. Lasers may be used to treat
nonrefractive conditions (e.g. to seal a retinal tear). Laser eye surgery or laser corneal
surgery is a medical procedure that uses a laser to reshape the surface of the eye. This is done
to correct myopia (short-sightedness), hypermetropia (long sightedness) and astigmatism
(uneven curvature of the eye’s surface). Being back pain one of the most prevalent affecta‐
tion, laser can also have an incidence on it.
540
A. Casals et al.

2.5
Ultrasound
The focalization of acoustic waves produces a local heating that provokes the cellular
death of the aﬀected tissues. It is comparatively the less invasive technique since there
are no incisions, and in contrast with radiotherapy, it does not produce radiation. Its
name HIFU comes from High Intensity Focussed UltraSound. There are three kind of
actuators for this treatment: (a) Piezoelectric actuators, which have limited energy and
produce very local eﬀects, as in prostate cancer, (b) Electromagnetic actuators, usually
arranged in arrays and focalizing the beams with mirrors towards the target point. As
they have to operate in an aqueous medium they are submerged in water, and (c)
Hydraulic actuators which provide higher energy and can be used to destroy stones
(lithotripsy). US treatment is mainly used in prostate cancer, to destroy solid tumors of
the bone, brain, liver, pancreas and so.
3
Challenges
While there are well consolidated application areas, other applications still require to
solve some technical limitations. Surgery has experimented a signiﬁcative progress in
the last years, not only due to the continuous growth of new medical knowledge, but
also pushed by the development of new surgical techniques. These technologies, have
allowed, up to diﬀerent degrees, the introduction of robotics in surgical practice. But
the contributions of robotics nowadays rely on its operating precision, its capacity of
tracking some determined trajectories and even in providing the assistance required to
avoid the physical and mental stress that surgeons suﬀer in many kind of surgeries, more
than in its intelligence, which is still very low, and in being able to operate under a
cognition based behavior.
With the aim of taking advantage of the robotics potential some bottlenecks should
still be overcome. These current operative limitations lie in the need of working over
soft anatomies, which are deformable and elastic, and they change their shape depending
on the patient’s position and due to the eﬀect of being manipulated by the surgical
instruments. These anatomical characteristics of soft tissues strongly complicate the
adaptation over the physical scene tasks, even those repetitive and mechanics as can be
a suture. On the other hand, operation over bone structures, that is, no deformable hard
tissues, can also present these diﬃculties. This problem appears when in many cases it
is not possible to achieve a suﬃcient immobilization level so as to be able to consider
the operating reference axis ﬁx in space. This forces the need to have available locali‐
zation systems operating in the 3D space and at high speed in order to obtain the 6
degrees of freedom of the anatomical structure with time delays low enough to adjust
the reference frames to the anatomy movements in real time. This means that the tasks
going on over moving anatomic parts can be executed as if they were ﬁxed, and without
producing neither strains nor ruptures on the surgical instruments, no mattering how
delicate they are. To tackle these problems, all the challenges concentrates on:
New Technologies in Surgery
541

(a) The capability of dynamically modelling both, rigid and ﬂexible elements, based
on the data obtained from the available TC images together with the information
provided by the sensors used for the perception of the surgical environment, Fig. 4.
Fig. 4. Schema of the localization and registration for robot surgery.
(b) The capability of registering in real time the model obtained dynamically from the
working scene, with the physical model, previously known, over which the inter‐
vention has been planned.
(c) The capability of tracking the anatomic elements that the perception systems use
for its registration with the model
The challenge stablished in the research ﬁeld in surgical robotics is achieving an
advance in each of these issues, both to plan and execute some determined surgical
interventions in an autonomous way, and to allow the introduction of aiding systems.
These aids, such as virtual reality, constitute an appropriate solution in teleoperation to
beneﬁt from the surgeons knowledge, experience and ability in surgery, counting at the
same time with the precision and tiredness performance of robotics.
3.1
Image Registration
Image registration is a key problem in medical imaging and in particular in image-guided
interventions, in which the objective is to align the preoperative and the intraoperative
data. Hence, surgical procedures beneﬁt from image registration since it allows
providing the surgeon with the current position of the instruments in relation with the
patient anatomy. In addition, surgeons can detect their approaching to nearby vulnerable
structures and the objective target at any time. In this context, it is necessary the use of
accurate and real time methods.
Mathematically, the registration of two images A and B to ﬁnd the best map ̃𝜑 from
a set of admissible transformations {𝜑} that best aligns these two images based on a
comparison criterion, usually by the minimization of an energy function 𝜀, i.e.,
̃𝜑= arg min𝜑𝜀.
Image registration has been usually classiﬁed following the criteria written in [2]
which basically are image dimensionality, image modality, nature of the registration and
geometric type of transformation.
542
A. Casals et al.

Registration is classiﬁed by the dimensionality of the images into 3D/3D, 2D/2D
and 2D/3D. In medical environments 3D/3D registration is usually used in diagnosis to
complement the information of diﬀerent images such as CT and SPECT. An example
of 2D/2D is the registration and display of the combined X-ray images in the diagnosis
of bone injuries. Instead, in orthopedic surgery, 2D/3D registration is used in navigation
based on 3D CT preoperative images and 2D ﬂuoroscopy intraoperative images, because
ﬂuoroscopy is a real-time X-ray image which is suitable for detecting bones. Hence,
both images must be transformed to the same dimensionality to be registered. In essence,
the dimensional correspondence is carried out by projecting the 3D data to a 2D space
or by transforming the 2D data into 3D. Three diﬀerent strategies can be used for this
transform which are: projection, back-projection and reconstruction.
The projection strategy transforms the 3D data into a 2D space by using a projection
matrix T ∈M3x4(ℝ). In the second strategy, back-projection, 2D data is transformed to
rays and then the distance between these rays and the 3D data is minimized to obtain
the best transformation function. And ﬁnally, reconstruction uses a number N > 1 of 2D
images to create a 3D image from all of them via a reconstruction function R and then,
this reconstruction is compared directly to the 3D image.
Referring to modalities, four types of registration techniques can be identiﬁed. First,
monomodal when all the images have the same modality. Second, quasi-intra-modal
which is a registration where the diﬀerent images are basically of the same modality,
but they diﬀer in the photon energy and in the detector characteristics. An example of
this type is a navigation using CT preoperative images and X-ray intraoperative images.
The third type are multi-modal registration. This type gathers registration methods that
use diﬀerent image modalities. This type is used in diagnosis using CT and SPECT
imaging. Finally, the last category of registration includes the registration methods that
tries to align images with a standard model (atlas).
Referring to the nature of registration, image Registration strategies are classiﬁed as
extrinsic and intrinsic according to the method used to achieve the correspondence
between the two data sets. The ﬁrst ones are called extrinsic because they rely on artiﬁcial
objects such as stereotactic frames or some external markers implanted into the bone,
soft tissues or skin. This type of techniques have as big advantage that they do not require
high computation because the optimization algorithm only has to compare a few
markers. However, the markers must be placed prior to the acquisition of the preoper‐
ative images and cannot be removed until the intervention is over to guarantee that their
position has not changed.
Intrinsic methods are so called because their strategy is the use of anatomical struc‐
tures taken from the own mages to be registered. These methods are usually divided into
feature-based, intensity-based and gradient-based.
As its name indicates, feature-based algorithms rely on ﬁnding the transformation
that optimizes the distance between some features extracted from the diﬀerent images.
These features can be points [3], curves [4], contours [5] and/or surfaces [6]. Then, the
applied algorithms usually follow an approach similar, or an adaptation, of the iterative
closest point(ICP) [7]. The ICP starts from a given initial transformation and tries to
resolve this minimization problem by alternatively optimizing feature correspondences
and transformations.
New Technologies in Surgery
543

The intensity-based strategy relies on ﬁnding the best correspondence between the
information contained in the pixel and voxels of the images. This strategy tries to mini‐
mize the diﬀerence between the two images and not the distance between some features
extracted from the images. Hence, it can only be used when the information in the pixels
and voxels of the images are comparable. Therefore it is not generally adequate for
aligning MR images with X-Ray images. There are some publications using these type
of images when contrast agents are applied. Nowadays, as the use of ﬂuoroscopy has
spread out, there are a lot of research works in this topic, registering 3D CT preoperative
images with 2D ﬂuoroscopy/X-ray intraoperative images. The method used to register
these data sets is usually based on the projection of digitally reconstructed radiographs
(DRRs). These DRR are generated from a CT image by emitting rays from the camera
position to the volume data and summing the intensities of the volume found along the
ray path.
Gradient-based methods register images using objective functions that are directly
dependent on the image gradient. These methods use the fact that there is a direct rela‐
tionship between the 3D gradient vectors of CT images and 2D gradient vectors of an
X-Ray image. The classiﬁcation in terms of geometric transformation are principally
divided into rigid and non-rigid registration. Rigid registration is based on a Euclidean
transformation, a transformation of a six degree of freedom composed by a translation
and a rotation. In non-rigid registration the transformation used is of any other type such
as aﬃne, polynomial, splines, elastic transformations, local models, etc.
Aiming to achieve real time and an adequate accuracy, we are working in a method
to improve the ICP algorithm computational time. The ICP algorithm is suited for
parallel architectures, stable and robust even for special cases. The principal idea behind
our work is the of use relevant points, those that better describe the object shape in order
to obtain better approximations to the solution. The points selected as relevant features
are the contour points with higher curvature. The proposed method merits come from
the search of a trade-oﬀ between the computational time that depends on the number of
iterations before reaching a solution and the achieved accuracy. Thus, the contribution
of the method is the improvement in the correspondence step that reduces the time per
iteration.
Once all the features have been extracted from the images, the algorithm iteratively
performs the following steps: First, projection of all the 3D points to the image plane
and selection of the contour points. Second, calculation of the curvature value of each
contour point. After, these points are classiﬁed by its curvature with respect to the nearby
points and ﬁnally, the most relevant points are selected as ﬁnal features to be used in the
ICP. In the optimization step of the ICP, these relevant points weighed according to their
curvature classiﬁcation. In the following iteration, the number of points ﬁnally selected
is increased. The algorithm has been tested in a scene using plastic models of diﬀerent
organs and in diﬀerent positions, Fig. 5. The experiments evaluate the accuracy and
computational time in relation to the increment in number of points used in each iteration
of the algorithm.
544
A. Casals et al.

Fig. 5. Registration of the 3D images of the three models with the 2D images obtained from
diﬀerent views.
These experiments show an improvement of about 10% in computational time with
respect to the standard ICP algorithm, obtaining the same accuracy. The computational
cost can be improved in relation to the application needs since this methodology opti‐
mizes the number of points according to the accuracy needed.
3.2
Modeling and Tracking
Position tracking is a key problem in many ﬁelds such as robotics, medical engineering
and virtual reality. In surgical treatments that rely on navigation systems. In these type
of procedures, the surgeon is assisted by a computer aided system, mainly in the phases
of planning and in image guided surgery. In consequence, these medical systems need
both, accuracy and real-time execution.
Tracking systems are based on diﬀerent technologies such as electromagnetic
systems, radio frequency, ultrasounds or medical imaging. The objective of tracking is
to determine the position and orientation of the tracked object. In the case of image
tracking, the objective is to track a target along a sequence of frames. So here, tracking
could be described as a registration with the addition of the temporal dimension. In
consequence, the tracking of an object can be carried out by continuously registering
the two images by giving as an initial solution for each new frame the position of the
previous frame.
These type of tracking algorithms are divided into deterministic and Bayesian.
Deterministic systems simply rely on the information extracted from the current frame
and from the solution of the previous image to iteratively calculate the new position.
Bayesian trackers try to estimate the best possible position of the target assuming that
the previous positions and measurements contain statistical noise and other inaccuracies.
Bayesian algorithms are divided into two steps called prediction and update. The predic‐
tion step uses the information from the prior position to estimate a new position, while
the updated state uses the information given by the new frame to get a better estimation.
In Bayesian trackers, the position of the tracked object is described as a possible state
which is characterized by the dynamic and measurement models. The dynamic model
determines how the state changes over time (physical laws of motion), and the meas‐
urement model characterizes how measurements are obtained from the state and its
statistics. The principal algorithms in Bayesian tracking are the Kalman and particle
ﬁlters. Kalman ﬁlter is a Bayesian algorithm that needs that the dynamic and
New Technologies in Surgery
545

measurement models are linear and Gaussian. If this assumptions holds, this algorithm
is optimal in terms of minimum mean square. If the models are not linear, a generali‐
zation of the algorithm named extended Kalman ﬁlter can be used. The particle ﬁlter is
a Bayesian algorithm that approximates the probability models by a sum of Dirac func‐
tions (particles) centered in a set of possible solutions. In consequence, its main
advantage is that it can be used with any type of stochastic model, neither Gaussian nor
linear, and when the model is unknown. Its principal drawback is that it needs many
particles to infer correctly the probabilistic distributions of the models what makes the
algorithm computationally ineﬃcient.
In this ﬁeld, we have implemented a set of Kalman trackers that uses the mentioned
registration algorithm to obtain new measurements of the possible position in real-time.
In our approach, we use a Kalman ﬁlter for each dimensionality of the solution (6 in
total) because we suppose that the trajectory of the target is continuous in time and, in
consequence, it has to be continuous in each dimension. This approach is a linearization
of the real model since it does not take into account the inﬂuence of one dimension on
the others which incorporates some non linearities to the problem and it permits the
parallelization of the system. The Kalman ﬁlter was selected for its low computational
cost with respect to other Bayesian trackers and because the error of our measurement
model are Gaussian.
We present our states not as the position in the frame, but as the trajectory of the last
points to impose to the system a continuity to the solutions and to reduce the variability
of the measurements. This imposition and the use of all the trajectory to the ﬁlter carries
with it the temporal response of the system. This ﬁlter was selected because in medical
applications it is advisable to prevent sudden movements. A response of 18 processing
cycles is obtained, that is, a response time of 0.5 s; time that has still to be reduced.
Taking into account that with the implemented system a time of 30 ms has been achieved,
its bandpass is of up to 1/20 × 30 ms, that is 1.6 Hz.
4
Work in Progress
Our research in surgical robotics started with the automatic guidance of the laparoscopic
camera by interpreting the area of interest to be visualized as the central point deﬁned
by the surgical instruments tip. The ﬁrst trials in the lab along 1994 and 95 led to the
ﬁrst vision based autonomous camera guidance system experimented clinically
(ICAR95). These ﬁrst results led to the design of a three arms robot, two operating arms
and the third for holding the camera. From this platform as a crane in the operating bed
a new robot was conceived in 2008, which was patented (patent) and motivated the
creation of a spin-oﬀ, RobSurgicalSystems S.L. in 2012. This is a modular robotic
system with 3, 4 or 5 arms in process of getting the EC mark. The challenges are: (a)
optimizing the ratio occupied volume with respect to accessible space, (b) design of a
telecontrol station that allows to approach the surgeon to the patient and the operative
ﬁeld, introduce virtual reality and haptic feedback, and (c) design instruments (avoiding
current patents) under the constraints imposed by the existing live patents that impedes
the use of elements as elementals as pulleys.
546
A. Casals et al.

In the ﬁeld of maxillofacial surgery, the works started in 2000 on an industrial robotic
platform still have to face the challenge of being able to operate on non-immobilized
parts, by integrating a quick enough registering system, in addition to the current assis‐
tive aids as virtual ﬁxtures. New challenges come from the need of introducing robotics,
and MIS, in fetal surgery. This application requires a small robotic system with arms to
hold two instruments but not the camera since it can be kept ﬁxed as the operating area
is small.
5
Conclusions
Robotics really constitutes an eﬀective assistant to the surgeon as robots can complement
humans thanks to their complementary qualities. The main goal now is to improve
surgical procedures and make possible new kind of interventions that could not be
possible without robotics. Due to the good human-system synergies, the question that
arise is whether we really want robots as substitute of humans. The complexity of
developing robotic applications which are autonomous enough to operate automatically
is still far, and many more cooperative tasks can be eﬃciently achieved.
Another point in the development of robotics is the low speed of development of
new systems and applications, which are due in part to legal issues. In this sense, the
lack of the best solutions frequently appear due to limitations due to existence of patents
frequently abusive that impede the use of the most reasonable technology.
In developing robotic systems for surgical applications, besides the safety issues well
stablished another main point that refers to ethics is the consciousness on developing
the technology just needed and no more than the necessary to achieve the expected
results without adding superﬂuous costs and dependencies. The goal is to achieve social
beneﬁts improving surgeries and widening the ﬁeld of feasible interventions.
References
1. Casals, A., Frigola, M., Amat, J.: La Robótica una valiosa herramienta en cirugía. Revista
Iberoamericana de Automática e Informática Industrial 6(1), 5–19 (2009)
2. van den Elsen, P.A., Pol, E.-J.D., Viergever, M.A.: Medical image matching - a review with
classiﬁcation. IEEE Eng. Med. Biol. 12(1), 26–39 (1993)
3. Bijhold, J.: Three-dimensional veriﬁcation of patient placement during radiotherapy using
portal images. Med. Phys. 20(2 Pt 1), 347–356 (1993)
4. Zikic, D., Groher, M., Khamene, A., Navab, N.: Deformable registration of 3D vessel structures
to a single projection image. In: Proceedings of SPIE, vol. 6914, pp. 691412–691412–12 (2008)
5. Guéziec, A., Kazanzides, P., Williamson, B., Taylor, R.H.: Anatomy-based registration of CT-
scan and intraoperative X-ray images for guiding a surgical robot. IEEE Trans. Med. Imaging
17(5), 715–728 (1998)
6. Yamazaki, T., Watanabe, T., Nakajima, Y., Sugamoto, K., Tomita, T., Yoshikawa, H., Tamura,
S.: Improvement of depth position in 2-D/3-D registration of knee implants using single-plane
ﬂuoroscopy. IEEE Trans. Med. Imaging 23(5), 602–612 (2004)
7. Besl, P., McKay, N.: A method for registration of 3-D shapes. IEEE Trans. Pattern Anal. Mach.
Intell. 14(2), 239–256 (1992)
New Technologies in Surgery
547

Collaborative Robotic System for Hand-Assisted
Laparoscopic Surgery
Carmen López-Casado1, Enrique Bauzano1, Irene Rivas-Blanco1, Víctor F. Muñoz1(✉),
and Juan C. Fraile2
1 Departamento Ingeniería de Sistemas y Automática. Lab. Robótica Médica,
Universidad de Málaga, Málaga, Spain
{mclopezc,vfmm}@uma.es
2 Departamento de Ingeniería de Sistemas y Automática, Universidad de Valladolid,
Valladolid, Spain
Abstract. Hand-assisted laparoscopic surgery is a Minimally Invasive Surgery
technique that is based on the insertion of one surgeon’s hand inside the abdominal
cavity. In this scenario, a robotic assistant can properly collaborate with the
surgeon, working side by side with him/her. This paper presents a robotic system
for this kind of technique, based on a cognitive architecture that makes possible
an eﬃcient collaboration with the surgeon, thanks to a better understanding of the
environment and the learning mechanisms included. This architecture includes a
hand gesture recognition module and two diﬀerent autonomous movement of the
robotic arms, one for the camera motion and the other for the tool movement. All
of these modules take advantage of the cognitive learning mechanisms of the
architecture, ﬁtting their behavior to the current user and procedure.
Keywords: Surgical robotics · Cognitive systems · Surgical procedure modelling ·
Smart surgical glove
1
Introduction
Hand-assisted laparoscopic surgery (HALS) is a new intermediate scenario between
conventional laparoscopy and laparotomy, in which the surgeon inserts one hand into
the abdominal cavity for organ manipulation, while handles the conventional minimally
invasive surgical tool with the other. This is a useful approach in complex situations
where conventional laparoscopic surgery cannot be performed. This kind of techniques
are widely used thanks to studies where it has been demonstrated that HALS techniques
do not imply longer patients’ recovery time [1].
The surgeons’ global requirements for this kind of technique is not covered using
single robotized tools, a robotic assistant concept is needed for HALS procedures. The
idea of robot co-worker proposed by Haddadin et al. [2], where robots work side by side
with humans, can be extrapolated to be used within surgical environments. In these
cases, the robot co-worker concept is based on cognitive architectures [3, 4] that make
possible an eﬃcient collaboration with the surgeon, thanks to a better understanding of
the environment and the learning mechanisms included. These mechanisms involve both
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_45

surgeon behavior and surgical procedure learning; and make possible to properly react
to unforeseen situations.
This kind of robotic systems should include interfaces in order to collaborate with
the surgeon in a natural way. The basis of these interfaces is the recognition of the
surgeon’s task to properly assist with autonomous tool movements or providing a suit‐
able view of the abdominal cavity. In HALS, the task recognition is usually based on
hand gesture recognition, using smart surgical gloves [5]. This kind of device allows
tracking the position and distances among phalanges and ﬁngertips. This information is
then used to recognize the hand gesture with techniques as Hidden Markov Models
(HMM) [6, 7] or Dynamic Time Warping (DTW) [8, 9] among others.
Regarding the suitable view of the abdominal cavity, the use of conventional endo‐
scopes makes the surgical environment to have many blind areas that are not reachable. To
enhance the field of view, intra-abdominal devices are used. These devices are inserted
through tool incisions and when having magnetic coupling [10] they can be moved through
the abdominal cavity, reaching areas not covered by conventional endoscopes.
The aim of the present paper is the development of a robotic system for HALS
procedures, using a cognitive co-worker architecture, where manipulators work side by
side with the surgeon, collaborating during the surgical maneuvers and learning from
practice. The workspace (Fig. 1) is composed by two robotic arms, one to handle an
articulated laparoscopic tool and the other to handle an intra-abdominal camera through
a magnetic coupling. The second tool is managed by the surgeon to perform laparoscopic
procedures that requires two tools (i.e. suture). The surgeon also wears a surgical glove
to track the hand movements within the abdominal cavity. The cognitive architecture
will recognize the present stage of the surgical workﬂow by means of the surgeon’s hand
gestures and the motions of the surgical instruments. Thanks to this information, the
robotic arm will be able to collaborate with the surgeon and to assist with the articulated
tool and placing the camera in the suitable location to provide a complete and appropriate
view of the surgical ﬁeld.
Fig. 1. HALS workspace.
Collaborative Robotic System for Hand-Assisted Laparoscopic Surgery
549

2
Cognitive Architecture
The cognitive architecture for HALS procedures is based on three layers (Fig. 2). The
lower one is the environment itself, including the patient, the surgeon and the robotic
assistant. Then, the second one is formed by the modules that allows the robot-world
interaction. The Perception System receives all the information of the patient and the
surgeon: the surgeon’s hand gesture, the tool position, etc. that it then processes. The
Action System commands the motion of the robotic assistant, receiving the new loca‐
tions from the cognition layer. A Human Machine Interface (HMI) is also included to
allow the surgeon to modify the autonomous movement of the robotic arms or even stop
the whole system in case of emergency using voice commands. Finally, the cognition
layer implements all the system knowledge and the learning mechanisms to suitable
assist the surgeon.
Fig. 2. Cognitive robotic architecture.
The cognition layer is divided into several memories. The long-term memory
includes all the system global knowledge and is formed by the semantic and the proce‐
dural memories. The semantic memory, modelled as a database, includes evidences,
meanings, concepts and any form of knowledge that is needed to understand the HALS
environment:
• List of HALS procedures and their division into diﬀerent stages as well as the trigger
signals that makes the procedure to change from one stage to the next one.
• List of actions to be completed by the action system. For each procedure stage, the
actions to be executed, have to be set.
• List of surgical tools to be used in the HALS procedures.
On the other hand, the procedural memory manages the actions that must be
executed. It contains the information on how, when and what actions must be performed
by the Action System. This knowledge is presented as a set of production rules (if-then
rules). Each rule has a condition (the if part) that evaluates the current state of the envi‐
ronment; and a set of actions (the then part) that include the actions to be done for each
550
C. López-Casado et al.

element in the robotic assistant. This knowledge can be improved using the reinforce‐
ment learning module, which makes possible to learn from practise, rewarding positively
the rules correctly selected and penalizing the wrong ones.
Finally, the working memory is the short-term memory of the architecture. It contains
the information inferred from the environment combined with the long-term memory
information, i.e. it oﬀers the description of the current situation of the system. The
Decision Procedure uses all this data to decide which production rule, within the set in
the procedural memory, must be executed in each cycle.
3
Perception System
The Perception System is formed by all the devices that acquire information of the
abdominal cavity. The most important one for HALS procedures is the data obtained by
the smart glove that the surgeon wears. This glove provides information about the posi‐
tion, orientation and distances among the ﬁngers and the phalanges that is then used to
recognize the hand gesture made by the surgeon.
The hand gesture recognition algorithm allows the robotic assistant to detect patterns
during the movement of the surgeon’s hand, which may correspond to a required action
of the surgical protocol or to a speciﬁc gesture to command the robotic assistant. All
data are acquired by the smart glove and is processed by a HMM algorithm. A set of
HMMs are ﬁrstly trained, one for each of the gestures to be recognized. Next, the move‐
ment of the surgeon’s hand is processed during a surgical protocol by each of all the
trained HMMs. This process outputs the probability of the gesture of being one of the
trained gestures. The recognized gesture will be the one with the highest probability.
Coupled with the recognized gesture, the algorithm provides a numerical value
informing about the conﬁdence of the recognition, the Conﬁdence Index.
4
Action System
The Action System oversees the movement of the two robotic arms. Each one has a
diﬀerent navigation strategy, but both are autonomous. The navigation strategy for the
camera is based on the combination of two simple strategies: one proactive and one
reactive. The ﬁrst one consists in moving the camera depending on the stage of the HALS
procedure, during this period the camera cannot be moved. On the contrary, the reactive
approach consists in tracking the surgical tools, without considering the procedure. Both
approaches are rigid and do not include any intelligence nor awareness to be considered
autonomous. But, if both approaches are combined, advantages of reactive and proactive
strategies will be oﬀered. This new strategy is based on computing the global focus of
attention of the image (PFoA), i.e. the position where the camera must be placed, as a
weighted combination of both contributions: tool tracking and stage ﬁeld of view.
PFoA = Kr ⋅PREACTIVE + Kp ⋅PPROACTIVE
(1)
Collaborative Robotic System for Hand-Assisted Laparoscopic Surgery
551

where Kr, Kp ∈[0, 1]∕Kr + Kp = 1 are the weight for the reactive and the proactive
contribution respectively; and PREACTIVE, PPROACTIVE are the target position of the camera
if reactive approach and proactive one is the only one used. The robot motion highly
depends on the value of these two constants (Kr, Kp). If Kr has a value close to 1, the
camera will follow the tools with no care of the current stage of the HALS procedure.
On the contrary, if Kp is high, the camera motion will depend on the stage of the proce‐
dure without considering the positions of the surgeon tools. First behavior is usually
comfortable for novices, meanwhile the second one is more suitable for expert surgeons.
Regarding movement of the surgical tool, it is autonomous in displacement and in
actuation of the laparoscopic grasp. This behavior is achieved using an auto-guided system
based on potential field algorithms. In this way, a low potential field is assigned to the target
position to attract the robot tool whereas high potential fields are located over the obstacles
(surgeon’s hand and tool) to create a repulsion effect. The robot action will depend on the
current state of the surgical procedure; however, its usual tasks are related to aid the surgeon
by holding tissue/organs or stretching the needle thread during a suture.
5
Reinforcement Learning
Reinforcement learning (RL) is a technique focus on the maximization of a numeric
reward signal. It is based on discovering which actions are the most rewarded by trying
them [11], what makes this kind of algorithm a trade-oﬀ between exploration and
exploitation: the system must exploit what it has already learnt, but it also has to explore
the new actions that has not been tested yet. This kind of learning is based on production
rules equivalent to the ones in the procedural memory. The diﬀerence between the
normal rules and the ones belonging to the RL algorithm is that the last ones are balanced.
Each RL rule has a value associated with it, the Q-value, that is obtained following a Q-
learning technique. This technique computes this value depending on the reward
received after executing the action associated with it and the discounted expected future
reward, which is obtained from the available actions in the next state [12]. The internal
goal of the system is to maximize the expected future reward. The reward received after
completing the action, must inform on how good the action executed was.
The RL algorithm has been used in two diﬀerent ﬁelds within this work. On one
hand, it has been used to learn the Kp and Kr weight of the autonomous movement of
the camera. And on the other hand, it has been used to enhance the hand gesture recog‐
nition algorithm. Regarding the camera movement, diﬀerent values for Kp and Kr have
been learnt for each user and for each stage of the HALS procedure. The reward signal
has been computed as a fuzzy model depending on the number of corrections made
through the HMI, the time spent to complete a stage and a numerical value corresponding
with the user satisfaction.
Regarding the recognition algorithm, the RL technique has been used to include new
on-line gesture records into the gesture library. This new record is weighted with respect
to the old ones, this weight is the one to be learnt. In this case, the reward signal is the
Conﬁdence Index obtained in each recognition.
552
C. López-Casado et al.

6
Implantation and Conclusions
This paper has described the cognitive architecture of a robotic assistant that can be used
for any HALS procedure, as well as the main modules that govern its behavior: the hand
gesture recognition modules and the autonomous movements module. Both, take
advantage of the learning mechanisms of the architecture, making possible to ﬁt the
robotic assistant performance to the users.
Most part of the Cognition Layer (Fig. 2) has been implemented in SOAR [13], a
general cognitive architecture that support all the capabilities required by a general
intelligent agent. It is based on a production system, and uses explicit production rules
to govern its behavior. This architecture has been encapsulated into a ROS node [14].
ROS is an open source software framework for robot software development. This
framework is based on nodes with a publishing/subscribing communication policy.
Thus, all the devices of the HALS environment have been integrated as a ROS node to
be connected one with each other.
References
1. Vogel, J.D., et al.: Hand-assisted laparoscopic right colectomy: how does it compare to
conventional laparoscopy? J. Am. Coll. Surg. 212(3), 367–372 (2011)
2. Haddadin, S., et al.: Towards the Robotic Co-Worker. Robotic Research. Springer Tracts in
Advance Robotics, vol. 70, pp. 261–282 (2011)
3. Chui, C.K., et al.: Development of a cognitive engine for balancing automation and human
control of surgical robotic system. In: 2014 11th International Conference on Ubiquitous
Robots and Ambient Intelligence (URAI), pp. 13–17 (2015)
4. Weede, O., et al.: Towards cognitive medical robotics in minimal invasive surgery. In:
Proceedings of Conference on Advances in Robotics, pp. 1–8 (2013)
5. Fahn, D.S., et al.: Development of a ﬁngertip glove equipped with magnetic tracking sensors.
Sensors 10(2), 1119–1140 (2010)
6. Galka, J., et al.: Inertial motion sensing glove for sign language gesture acquisition and
recognition. IEEE Sens. J. 16(16), 6310–6316 (2016)
7. Rossi, M., et al.: Hybrid EMG classiﬁer based on HMM and SVM for hand gesture recognition
in prosthetics. In: IEEE Conference Industrial Technology, pp. 1700–1705 (2015)
8. Plouﬀe, G., et al.: Static and dynamic hand gesture recognition in depth data using dynamic
time warping. IEEE Trans. Instrum. Meas. 65(2), 305–316 (2016)
9. Bautista, M.A., et al.: A gesture recognition system for detecting behavioral patterns of
ADHD. IEEE Trans. Cybern. 46(1), 136–146 (2016)
10. Valdastri, P., et al.: A magnetic internal mechanism for precise orientation of the camera in
wireless endoluminal applications. Endoscopy 42(6), 481–486 (2010)
11. Sutton, R.S., et al.: Reinforcement Learning: An Introduction, vol. 1(1). MIT Press,
Cambridge (2012)
12. Laird, J.E.: Tuning Procedural Knowledge: Reinforcement Learning. The Soar Cognitive
Architecture. MIT Press, pp. 181–202 (2012). ISBN:9780262301145
13. Laird, J.E.: The Soar Cognitive Architecture. MIT Press (2012)
14. Koubaa, A.: Robotic Operating System (ROS). SCI, vol. 625. Springer (2016)
Collaborative Robotic System for Hand-Assisted Laparoscopic Surgery
553

Rehabilitation and Assistive Robotics

Mechanical Design of a Novel Hand Exoskeleton
Driven by Linear Actuators
Jorge A. D´ıez(B), Andrea Blanco, Jos´e M. Catal´an, Arturo Bertomeu-Motos,
Francisco J. Badesa, and Nicol´as Garc´ıa-Aracil
Universidad Miguel Hern´andez, Elche, 03202 Alicante, Spain
jdiez@umh.es
http://nbio.umh.es/
Abstract. This paper presents the mechanical design of a novel hand
exoskeleton for assistance and rehabilitation therapies. As a solution for
the movement transmission, the proposed device uses modular linkage
that are attached to each ﬁnger by means of snap-in ﬁxations. The linkage
is kinematically and dynamically analyzed by means of simulations with
AnyBody Simulation Software to obtain an estimation of the range of
motion and admissible forces. In order to check the deviations of the real
performance respect to the simulated results, due to uncertain variables,
a ﬁrst prototype is built and tested.
Keywords: Hand exoskeleton · Assistive robotics · Rehabilitation
robotics
1
Introduction
1.1
State of the Art
In the current literature we can ﬁnd a wide diversity of robotic devices which
can actuate the movements of the human hand [1]. Part of these devices, such
as [2–4], are aimed to perform rehabilitation therapies; while many others are
pretended to assist the hand motion during activities of daily-living [5–7].
Depending on the application, a hand exoskeleton may require uneven fea-
tures. For example, a rehabilitation-aimed exoskeleton needs to be fairly back-
drivable and allow a wide range of movement, so it is ﬂexible enough to perform
diﬀerent rehabilitation exercises. In contrast, an assistance exoskeleton must be
stiﬀenough to assure a ﬁrm grasping of objects present during activities of daily
living and can sacriﬁce ﬂexibility of movement in favor of predeﬁned grasping
patterns.
These diﬀerent requirements result on diverse force transmission architec-
tures:
– Some devices use linkages in order to transmit the force from the actuator to
the human joints. This is a stiﬀarchitecture that requires a proper alignment
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_46

558
J.A. D´ıez et al.
between kinematic centers of the linkage and human joints, but allows a good
control of the hand pose. Due to the ﬂexibility of the design, with the correct
sizing, these mechanisms can achieve complex movement patterns with simple
actuators.
– Another extended architecture is the cable driven glove. These are more ﬂex-
ible and simpler alternatives, that rely on the own human joints to direct the
movement, so they are less prone to uncomfortable poses. In contrast, they
require pulleys to achieve high forces and are harder to control in intermedi-
ate positions. Additionally, this kind of exoskeletons need a pair of cables in
antagonist conﬁguration in order to assist both extension and ﬂexion move-
ments.
– Finally, some devices use deformable actuators, like pneumatic muscles or
shape-memory alloys, attached directly to the hand by means of a glove.
They result in very light and simple devices, but actuators are not placed in
the most advantageous place to achieve great forces.
In general, according to [1] there is a clear trend to use cable driven devices
and deformable gloves for assistance purposes while linkage architectures are
mainly used in rehabilitation devices.
1.2
Framework and Objective
AIDE Project (H2020) [8] is a project that is developing a multimodal system to
assist disabled people in the realization of a wide range of activities of the daily
living. One of the main research lines, is the analysis of usage of exoskeletons to
assist users during the interaction with their environment [9].
In this framework, we are developing a hand exoskeleton with the con-
cept described in the patent ES2558024B1 [10] as starting point. This device
is required to be able to grasp diﬀerent kind of everyday use objects, such as
glasses, adapted cutlery or handles, with a grip that must be ﬁrm and safe in
order to provide real autonomy to the user.
2
Mechanical Design
Since we consider the grip strength as a decisive factor to achieve our goals,
we have chosen an architecture based on a linkage that controls the pose of
the phalanxes. Using this type of power transmission, we can tune the leverage
between links in order to achieve a satisfying balance between the transmitted
force and range of movement; and therefore optimize the weight and power
required for the actuators.
As a ﬁrst approximation, we propose an exoskeleton with three active degrees-
of-freedom, corresponding to ﬂexion-extension of index ﬁnger, ﬂexion-extension
of middle ﬁnger and ﬂexion-extension of both ring and little ﬁnger. Thumb will
have a series of passive-degrees of freedom that will allow to place the thumb in
a suitable pose in the installation phase, and will be lockable to allow the thumb
to work in opposition during the grasp.

Mechanical Design of a Hand Exoskeleton
559
2.1
Finger Mechanism
The proposed solution (Fig. 1) consists on a ﬁve-bar-linkage that couples the
movement of both proximal and medial phalanxes so that they can be controlled
with a single active degree-of-freedom. This mechanical system is driven by a lin-
ear actuator ﬁxed to the exoskeleton frame and connected to the link L2. We have
decided not to actuate on the distal phalanx since it would restrain excessively
the movements of the ﬁnger, leading to a greater probability of uncomfortable
poses.
Fig. 1. Proposed linkage for a ﬁnger
This linkage forms closed kinematic chains with the ﬁnger, but this is only
true if the ﬁnger is perfectly ﬁxed to the linkage. Unfortunately, the interface
between the robot and the human body will introduce an uncertainty that,
due to the complexity and high grade of non-linearity of the mechanism, may
completely modify the kinematic behavior of the robot. In this regard, similarly
to the solution proposed by Ho et al. [11], we have added a pair of circular
guides, whose centers match with the joints of a reference ﬁnger. In this way,
the kinematic chain is closed exclusively by mechanical parts whose dimensional
parameters are known.
The addition of the circular guides carries extra dimensional restrictions, due
to the possible interference between guides and links. The design developed by
Ho et al. consists on a pair of slots in which a train of bearings slide, restrict-
ing the movement of the links. This architecture may be eﬀective, but requires
substantially good quality in the machined surfaces of the slot and the use of
miniature elements in order to be as compact as possible. As an alternative, we
propose the use of a doubled edged curved guide which slides between a group
of 4 V-shaped bearings (Fig. 2). This results on a simple but eﬀective option,
that can be easily built with technologies like 3D-printing or plastic moulding.
Regarding the linear actuator, size and weight are the main limiting design
factors. Despite pneumatic actuator may result attractive for this kind of devices
due to their reduced dimensions and high force, we consider that electric drives

560
J.A. D´ıez et al.
Fig. 2. Detailed view and section of the designed circular guide, using bearings with
V -shaped proﬁle
are more suitable for domestic environments, since it is easier and cheaper use
the household electric installation than providing a compressed air supply to
the user. Among the diﬀerent existing technologies, we consider that Actuonix
PQ12 Linear Actuator [12] provides an adequate balance between size, weight
and maximum force. Moreover, this actuator has two interesting features for this
application: in one hand, the screw transmission allows to have a high backdrive
force that avoid the releasing of the grasped object in case of accidental lose
of the power supply; in the other hand, it has a built in potentiometer, so no
additional space is required to set up an external one.
The sizing of the whole mechanism can not be easily done by mathematical
means, since complex and non-linear kinematics result on multiple solutions or
unstable convergence. Additionally, elements like screws or shafts can be found
just in speciﬁc sizes, so any optimization of the link sizes would hardly be com-
patible with these normalized parts. Therefore, we have used a parametric CAD
model of the device in order to obtain a satisfying solution by manually chang-
ing the geometric variables. Figure 3 shows a detailed view of the resulting ﬁn-
ger mechanism with the extreme theoretical pose of the ﬁnger. At this stage
of design, this linkage and dimensions will be the same for each ﬁnger except
thumb.
Unlike the rest of ﬁngers, thumb can not be approximated as a planar mech-
anism due to its degree of freedom of opposition. However, many common grasps
can be done by just maintaining ﬁrmly the position of the thumb while the rest of
ﬁngers perform the whole movement. So, at this design stage, we have designed
a mechanism that allow to place manually the user’s thumb in a comfortable
pose and then block it.

Mechanical Design of a Hand Exoskeleton
561
Fig. 3. Drawing of a ﬁnger mechanism with extreme positions. (Left: Maximum exten-
sion. Right: Maximum ﬂexion)
2.2
Hand Interface
Since the designed linkage is kinematically determinate by means of circular
slides, it is necessary to design an interface able to deal with misalignment
between human joints and robot rotation centers, as well as the deviation
between the reference ﬁnger size and actual user’s dimensions.
As an interface between moving parts of the exoskeleton and human ﬁngers,
we have designed a snap-in ﬁxing system by using elastic ring-like pieces that
can be inserted in certain points of the ﬁnger linkage (Fig. 4). This system allows
a quick attach-detachment process, reducing the time required in the set up and
allowing a fast reaction against any dangerous situation. The stiﬀness of the
ring can be designed to automatically release itself if a certain force threshold
is overcome. Additionally, the diameter of each ring can be chosen accordingly
to the real dimensions and misalignment, in order to provide the user with the
most comfortable experience.
Each ﬁnger mechanism must be attached to a frame ﬁxed with respect to the
metacarpal section of the hand, but the optimal placement varies signiﬁcantly
depending on hand’s dimension. As a solution, we propose an attachment similar
to the ﬁnger’s one (Fig. 5). In this case, the ﬁnger frame has a slot in which
a double wedge-like part is inserted. This wedge constraints two translation
degrees-of-freedom, but allows the movement along the slot direction, so that
the position of the linkage along the ﬁnger metacarpal axis can be adjusted.
Once the linkage is correctly placed, the friction between the frame and the
wedge prevents it from displacing.
Finally, each wedge is screwed to a semi-rigid hand orthosis that wraps both
palm and back of the hand. As a result, we obtain a modular hand exoskeleton

562
J.A. D´ıez et al.
Fig. 4. Finger-Exoskeleton ﬁxing system. Left: Section of the ring-box assembly with
the force needed for releasing. Right: Detailed view of the attachment points in the
ﬁnger linkage.
Fig. 5. Hand-Exoskeleton ﬁxing system. Left: Section that shows the wedge-like part
inserted in the slot of the ﬁnger frame. Right: Detailed view of the connection system
before attachment.
that can be easily modiﬁed to the user’s needs, since just the necessary phalanxes
or even ﬁnger linkages may be set up.
3
Biomechanical Analysis
In order to study how do exoskeleton kinematics aﬀect the ﬁnger kinematics,
we have decided to use the AnyBody Simulation Software [13] with the human
ﬁnger model proposed by Wu et al. [14]. We have included the kinematics and
dynamics of the ﬁnger linkage to this model (Fig. 6), paying attention in modeling
the ﬁnger-linkage interface as accurately as possible, to obtain realistic reaction
forces. In detail, the ring snap-in ﬁxation has been modeled as a cylindrical slide
whose axis is aligned with the phalanx longitudinal axis, attached rigidly to the
exoskeleton’s link through a bar with length equal to the radius of the ring.
With this numerical model, we can easily solve the kinematics of the mech-
anism and compute the relationship between the angles of both metacarpopha-
langeal (MCP) and proximal interphalangeal (PIP) joints. This result is shown

Mechanical Design of a Hand Exoskeleton
563
Fig. 6. Human ﬁnger [14] and exoskeleton model in AnyBody
in Fig. 7. Despite the internal non-linearity of the mechanism and interface, along
the working range deﬁned by the actuator’s stroke, the output angles present
a fairly linear behavior; so we may use two simple expressions to obtain an
estimation of the hand’s pose with just measuring the stroke of the motor.
According to Chen et al. [15] reaction torques in each joint of the ﬁnger, and
the ratio between them, may vary greatly depending on the user and the type
of grasp. Therefore, in order to compute in which working points (MCP torque,
PIP torque) our exoskeleton can work properly, we have performed an inverse
dynamics study computing the force that must exert the actuator to balance the
mechanism, applying diﬀerent torque combinations in the joints of the ﬁnger.
Taking the maximum force required in each simulation for each combination of
MCP and PIP torque, we have computed the admissible load assumptions for
our exoskeleton (Fig. 8).
We have performed a preliminary calculation in order to check whether these
admissible loads are suitable for the required applications:
The heaviest object that will be considered to be grasped will be a full bottle
of water with a capacity of 1.5 L, which can be considered as a mass of 1.5 kg.
According to the study of O’Meara and Smith [16], the human skin has a static
friction coeﬃcient of 0.8, so to lift a weight of 1.5 kg, a hand must exert a total
of approximately 2 kg of force over it.
As a simpliﬁcation, the applied force is distributed as follow: half the force
is exerted by the thumb (1 kg) and the other half is distributed equally between

564
J.A. D´ıez et al.
Fig. 7. Angles of the MCP and PIP joints for each actuator stroke. Solid lines represent
the output of the simulation. Dashed lines represent the corresponding linear ﬁt. Linear
ﬁt equations are provided, where y refers to the joint angle and x to the motor stroke.
Angles are measured as in Fig. 3.
Fig. 8. Module of the force required to the linear actuator in order to stand a spe-
ciﬁc torque combination. Green area shows the working points in which the actuator
can stand torques even without electrical power. Yellow zone deﬁnes working points
that necessarily require powered actuator. Red zone contains torque combinations not
standed by the mechanism.
index and middle ﬁnger (0.5 kg each). If the force is applied in the middle of each
phalanx and only the MCP and PIP joints are actuated, the required torque for
each joint can be summarized in Table 1. With this load hypothesis, each ﬁnger
would be working inside the safety zone of Fig. 8.

Mechanical Design of a Hand Exoskeleton
565
Table 1. Loads required to grasp 1.5 kg bottle with a simpliﬁed grasp model
Joint
Next phalanx Applied
Joint
Required torque
average
force (N) torque
(Nm) Safety
length (m)
(Nm)
factor >2
IndexMCP
0.058
2.5 N
0.0725
0.15
IndexPIP
0.022
2.5 N
0.0275
0.6
MiddleMCP
0.053
2.5 N
0.06625 0.14
MiddlePIP
0.026
2.5 N
0.0325
0.7
Thumb-MCP 0.032
10 N
0.16
0.32
4
Prototype Stage
There are many factors, such as clearances in the interfaces or joint misalignment,
that can not be easily evaluated with simulations, so it is mandatory to build a
prototype to check the real performance of the designed exoskeleton.
In this prototype, the exoskeleton will have 3 ﬁnger modules that control
index ﬁnger, middle ﬁnger and the pair formed by ring and little ﬁngers, in
addition to the aforementioned thumb blocking linkage.
Each ﬁnger module is built in Polylactic Acid (PLA) material by Fused Depo-
sition Modeling (FDM) 3D printing technology, which allows fast and cheap fab-
rication of complex geometries. Despite this fact, all parts have been designed
or subdivided in geometries that can be easily built by computer numerical con-
trol (CNC) machining in order to allow the production of further prototypes
using traditional materials. We have particular interest in studying the resis-
tance of the PLA 3D printed parts in order to check the feasibility of building
the exoskeleton using technical polymers, to reduce as much as possible the
weight of the ﬁnal device. Additionally to the ﬁnger modules, we have similarly
built a set of rings with diameters that cover the range from 15 mm to 25 mm
with a step of 1 mm. Finally, as an interface with the hand, we have chosen a
commercial hand orthosis, were the ﬁxations for the ﬁnger module and thumb
blocking linkage have been rigidly attached. Figure 9 displays the modules that
compose the whole exoskeleton (which can be easily attached and detached),
the device set up on a hand, and a grasp test in which the device is capable of
holding a bottle of half liter of water.
Once the user wears the hand exoskeleton, we have observed that, despite that
the mechanism holds its position, rings allow certain freedom of movement of the
ﬁngers so the user can accommodate them to a more natural pose. This adjust-
ment introduces an error in the kinematics that varies each time the devices is
attached and removed from the hand. Figure 10 shows the result of the measure-
ment of the angles of the joints for a same user after installing the exoskeleton
several times. These measures were taken approximately by photogrametry, with
a precision of about ± 1◦. For both joints we can observe that the error with
respect to the simulated values is remarkably high for low angles and becomes

566
J.A. D´ıez et al.
Fig. 9. Left: Disassembled exoskeleton with ﬁnger modules, hand orthosis and ﬁnger
rings. Center: Exoskeleton attached to a human hand. Right: Grasp test of a bottle of
0.5 L of water.
Fig. 10. Mismatch between simulated and measured angles due to clearances in the
rings and misalignment between joints. Sparse points correspond to measurements for
a same subject after diﬀerent exoskeleton set-ups.
lower in closed poses. This phenomenon is due to the alignment between both
rings of the ﬁnger, since it allows the ﬁnger to move along the axial direction
of the ring. When the exoskeleton becomes more closed, misalignment between
rings restrict the displacement along them and the real behavior becomes closer
to the simulated model.
5
Conclusion and Future Work
According to the ﬁrst tests with the prototype, the designed device has proven to
be feasible to grasp certain objects present in the activities of daily living, such

Mechanical Design of a Hand Exoskeleton
567
as small bottles, glasses or adapted cutlery. In comparison with other existing
devices, we want to remark several features that may suppose an improvement
in this kind of devices:
– Snap-in ﬁxations together with the modular ﬁnger mechanisms allow a quick
installation and removal of the device as well as eases the maintenance of the
hardware and the substitution of worn up parts.
– The use of elastic rings as interface with ﬁngers, despite that they introduce
uncertainty in the actual hand pose, provide a comfortable ﬁxation to the
user allowing the proper accommodation of the ﬁnger. Additionally, rings
add safety measure when extending the hand since they will detach at a
certain interaction force.
– As PLA has proven to be tough enough to stand the hand loads, the use of
technical plastics as structural materials seems to be feasible. These kind of
materials will result on lightweight devices as well as will allow cheap mass
manufacturing process like molding or injection.
– Since grasping contact forces mainly fall to rings, so they can be coated with
adherent materials to improve the safety of the grip.
Future works will consist in an exhaustive validation of the device as well as
in exploring additional potential applications, such as rehabilitation therapies.
Implementation of a force measurement system is a short-term objective, since
it will provide valuable data required for validation tests and studies with dis-
abled users. This force measurement will extend the functionality of the devices
because of the utility in control loops or detection of movement intention.
Acknowledgments. This work has been founded by the European Commission
through the project AIDE:Adaptive Multimodal Interfaces to Assist Disabled Peo-
ple in Daily Activities (Grant Agreement No: 645322); by the Spanish Ministerio de
Econom´ıa y Comptitividad through the project DPI2015-70415-C2-2-R; and by Consel-
leria d’Educaci´o, Cultura i Esport of Generalitat Valenciana through the grants ACIF
2016/216 and APOTIP 2016/021.
References
1. Heo, P., Gu, G.M., Lee, S.J., Rhee, K., Kim, J.: Current hand exoskeleton tech-
nologies for rehabilitation and assistive engineering. Int. J. Precis. Eng. Manuf.
13(5), 807–824 (2012)
2. Brokaw, E.B., Black, I., Holley, R.J., Lum, P.S.: Hand spring operated movement
enhancer (HandSOME): a portable, passive hand exoskeleton for stroke rehabili-
tation. IEEE Trans. Neural Syst. Rehabil. Eng. 19(4), 391–399 (2011)
3. Mulas, M., Folgheraiter, M., Gini, G.: An EMG-controlled exoskeleton for
hand rehabilitation. In: 9th International Conference on Rehabilitation Robotics,
ICORR 2005, pp. 371–374. IEEE, June 2005
4. Kinetic Muscles Inc., Hand Physical Therapy with the Hand MentorTM. http://
www.kineticmuscles.com/hand-physicaltherapy-hand-mentor.html

568
J.A. D´ıez et al.
5. Martinez, L.A., Olaloye, O.O., Talarico, M.V., Shah, S.M., Arends, R.J., BuSha,
B.F.: A power-assisted exoskeleton optimized for pinching and grasping motions.
In: Proceedings of the 2010 IEEE 36th Annual Northeast Bioengineering Confer-
ence, pp. 1–2. IEEE, March 2010
6. Yamada, Y., Morizono, T., Sato, S., Shimohira, T., Umetani, Y., Yoshida, T.,
Aoki, S.: Proposal of a SkilMate ﬁnger for EVA gloves. In: Proceedings of the
IEEE International Conference on Robotics and Automation, ICRA 2001, vol. 2,
pp. 1406–1412. IEEE (2001)
7. Tadano, K., Akai, M., Kadota, K., Kawashima, K.: Development of grip ampliﬁed
glove using bi-articular mechanism with pneumatic artiﬁcial rubber muscle. In:
2010 IEEE International Conference on Robotics and Automation (ICRA), pp.
2363–2368. IEEE, May 2010
8. AIDE: adaptive multimodal interfaces to assist disabled people in daily activities
(GA 645322). http://cordis.europa.eu/project/rcn/194307 es.html
9. D´ıez, J.A., Catal´an, J.M., Lled´o, L.D., Badesa, F.J., Garcia-Aracil, N.: Multimodal
robotic system for upper-limb rehabilitation in physical environment. Adv. Mech.
Eng. 8(9), 1–8 (2016). doi:10.1177/1687814016670282
10. Garcia-Aracil, N., Sabater, J.M., Fern´andez, E., Badesa, F.J., Morales, R., D´ıez,
J.A., Enriquez, S.C.: Self-adaptive for hand rehabilitation and method of use and
modular robotic device, ES Grant ES2558024B1, Oﬁcina Espa˜nola de patentes y
marcas (2016)
11. Ho, N.S.K., Tong, K.Y., Hu, X.L., Fung, K.L., Wei, X.J., Rong, W., Susanto,
E.A.: An EMG-driven exoskeleton hand robotic training device on chronic stroke
subjects: task training system for stroke rehabilitation. In: 2011 IEEE International
Conference on Rehabilitation Robotics (ICORR), pp. 1–5. IEEE, June 2011
12. Actuonix Motion Devices Inc. Miniature Linear Motion Series PQ12, Victoria,
Canada (2016). https://www.actuonix.com/Actuonix-PQ-12-P-Linear-Actuator-
p/pq12-p.htm
13. The AnyBody Modeling System (Version 7.0.0). [Computer software]. AnyBody
Technology, Aalborg, Denmark (2017). http://www.anybodytech.com
14. Wu, J.Z., An, K.N., Cutlip, R.G., Krajnak, K., Welcome, D., Dong, R.G.: Analysis
of musculoskeletal loading in an index ﬁnger during tapping. J. Biomech. 41(3),
668–676 (2008)
15. Chen, F.C., Favetto, A., Mousavi, M., Ambrosio, E.P., Appendino, S., Battezzato,
A., Manfredi, D., Pescarmona, F., Bona, B.: Human hand: kinematics, statics and
dynamics. In: 41st International Conference on Environmental Systems (ICES),
Portland, Oregon, USA, vol. 5249, pp. 17–21. AIAA, January 2011
16. O’Meara, D.M., Smith, R.M.: Static friction properties between human palmar
skin and ﬁve grabrail materials. Ergonomics 44(11), 973–988 (2001). doi:10.1080/
00140130110074882

Robotic Platform with Visual Paradigm
to Induce Motor Learning in Healthy Subjects
Guillermo As´ın-Prieto(B), Jos´e E. Gonz´alez, Jos´e L. Pons, and Juan C. Moreno
Neural Rehabilitation Group, Cajal Institute,
Spanish National Research Council (CSIC), Avda. Dr. Arce, 37, 28002 Madrid, Spain
guillermo.asin.prieto@csic.es
http://www.neuralrehabilitation.org/
Abstract. Recent projects highlight how motor learning and a high
level of attention control can potentially improve submaximal force pro-
duction during recovery after stroke. This study focuses on the assess-
ment of detailed metrics of force production and position control -healthy
subjects- and their correlation with submaximal force production con-
trol learning during a new task consisting in maintaining the position for
early rehabilitation after stroke.
We used a Motorized Ankle Foot Orthosis (MAFO) with zero-torque
control together with a visual paradigm interface to exert controlled
torque proﬁles to the ankle of the subject. The subject is asked to follow
the trajectories in the visual interface, while the robot disturbs the move-
ment. The aim of the exercise is to improve the motor control by learning
how to maintain the position to follow the trajectory, compensating the
perturbations, in three possible training paradigms: (1) ﬁxed torque, (2)
progressive increase of torque, and (3) modulated torque based on score
on the task.
All training paradigms led to an improvement in the score comparing
pre and post-training performances, so we concluded that this platform
induces learning on healthy subjects.
To sum up, we conclude that this tool is useful to induce learning in
healthy subjects, and thus will keep improving the training paradigms,
for the translation into a rehabilitative tool.
Keywords: Stroke recovery · CVA · Neural rehabilitation · Motor
control
1
Introduction
Recent projects highlight how motor learning and a high level of attention con-
trol can potentially improve submaximal force production during recovery after
stroke. Motivation and attention are key elements for a potentially success-
ful motor rehabilitation [1]. It is capital for the success of the therapy that
the patient understands the exercise and the potential outcome, to keep the
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_47

570
G. As´ın-Prieto et al.
engagement and the attention, thus enhancing motivation [2]. Furthermore, if
the patient lacks the capacity to understand how an exercise is executed correctly,
little eﬀect can be expected [3]. It is important to note that while talking about
stroke patients that their brain has suﬀered an injury, learning function may be
compromised. Nonetheless, it has been seen that after local brain cortex tissue
damage, rehabilitative training can produce a reorganization to some extent on
the adjacent intact cortex [4]. These ﬁndings suggest that undamaged motor
cortex might play an important role in motor recovery. One important factor is
that there seems to exist a direct relationship between impairment and function:
function of walking is correlated to lower-limb strength (impairment) [5].
Other key factor in a successful therapy towards motor recovery is the concept
of repetition. On the one hand, the idea of “repetition without repeating” is
important for the avoidance of disengagement during the progress of the task.
On the other hand, this need of repetition may lead to health problems for
physiotherapists, and robotics can be of huge help in providing them with a tool
to enhance the therapy by allowing them to ensure repeatability and providing
a tool to more objectively assess performance metrics [6].
The concept of “repetition without repeating” considers that training ses-
sions must include recording the modiﬁed parameters to challenge the patient [2].
Within this concept, the idea of interleaving diﬀerent exercises also arises, and
it has been seen that this mixing of learning tasks is more eﬀective for learning
than learning tasks one by one with speciﬁc focused trainings [7], and this mixed
schedule might aid recovery because it considers each movement as a problem
to solve and not as a pre-memorized muscular sequence [8]. In fact, Shea and
Kohl [9] stated that “retention of the test task was best facilitated by acquisi-
tion practice that included task variations rather than conditions that included
additional practice on the test task or even a condition identical to the reten-
tion condition.” The nervous system seems to rely on the concept of adaptation,
meaning that a previously learned skill can be recovered after a change in the
operating environment. This theory of adaptation says that after a change in the
environment dynamics, the control system does not need to relearn but adapt,
suggesting that the nervous system tunes a learned internal model and has the
ability to extrapolate this learning to new scenarios [10].
Robotics has been proposed and partially proven to be beneﬁcial in interven-
tions in acute and chronic stroke patients [11–13]. It is important to study the
better tradeoﬀbetween the learning rate and the amount of helping guidance
given by the robot during training [14]. Robotic guidance approaches also oﬀer
a powerful tool in rehabilitative and learning cases where large errors may be
dangerous or undesirable, as this guidance provides the user with a tunnel inside
of which the movement is allowed and controlled outside of it.
Robotics can be used not only as standalone devices, but also in combina-
tion with virtual environments, providing the possibility of showing the patient
and the physiotherapist the feedback of the performance of the task, which
has been proven to improve rehabilitation [3]. In the literature we can ﬁnd
that visual feedback through virtual environments helps to improve the per-

Robotic Platform to Induce Motor Learning
571
formance in speciﬁc tasks, even improving the results in comparison to real-task
training [15]. Indeed, this visual approach has been proven to improve robotic
haptic guidance therapy scenarios [16]. Furthermore, the idea of this virtual envi-
ronment or “video game” makes the implicit learning process transparent for the
user, not being aware of what it is being learned [6]. They also ease to oﬀer the
user with a goal-oriented task and possibility of repetition [17].
Robots are crucial for these haptic approaches, as they help the physiother-
apist to provide an objectively equal movement to the patient’s limb. Several
authors have tested the error augmentation approach [6,18–20] to enhance motor
learning. The nervous system learns forming the internal model of the dynam-
ics of the environment via a process of error reduction. Emken and Reikens-
meyer concluded that motor learning process can be accelerated by exploiting
the error-based learning mechanism. They used the approach of perturbing the
movement via a robotic device, to augment the error while the user was per-
forming the task. It seems that this learning process is indeed a minimization
problem, based on the last performed trials to accomplish the task [20], where
the minimization takes place between the weighted sum of the kinematic error
and the muscular eﬀort [21]. Combined haptic error augmentation and visual
feedback have provided better results than conventional direct training (pure
joint mobilization), exploiting the idea of using the after-eﬀects of a resistive
force to help perform the user the desired trajectory task [22]. Moreover, this
error-augmentation strategy produces, when the after-eﬀects are evaluated, the
correct muscle activation patterns to achieve the desired trajectory, and results
showed that subjects were able to reduce errors more rapidly than those that
received a robotic guidance approach training [14]. Reikensmeyer and Patton
suggest combining robotic guidance and error-augmentation techniques, start-
ing with guidance and gradually removing it and increasing error-augmentation.
Several studies have been conducted with stroke patients, showing promising
results comparable to those learning results seen in healthy subjects. For exam-
ple, Patton et al. [23] found some preliminary evidence that showed that the
approach of error-augmentation could be used to obtain smoother trajectories
in stroke patients. Krakauer [8] found that, although guidance approach leads
to better performance during the training, it is not optimal for retaining the
learning over time, so these training may improve performance in the clinical
environment, but may not be transferable to activities of daily living. Reikens-
meyer and Patoon demonstrated that with this error-augmentation approach,
stroke patients can potentially alter abnormal limb movement patterns that
appear relatively ﬁxed, although the reasons why people with stroke perform
abnormal patterns remain to be identiﬁed [14].
One important point to take into account in rehabilitative processes is that
“several types of exercise programs, including muscle strengthening, physical
conditioning, and task-oriented exercises have led to an improvement in bal-
ance and mobility” [24]. Thus, it makes sense to focus the approach of early
rehabilitation and motor recovery on simple tasks, towards the integration of
them into more complex processes while the recovery takes place over time.

572
G. As´ın-Prieto et al.
Patton et al. [19] also found evidence on stroke survivors retaining ability to
adapt to force ﬁelds, being better at adapting to real scenarios less demanding
than training scenarios (lower forces in the real world than in the training); and
more importantly, they can last longer if the exercise resembles normal move-
ments and the after-eﬀects can be perceived by the patient as an improvement [6].
Most studies are focused on upper-limb rehabilitation, but Duncan et al. [25]
demonstrated that similar motor recovery of upper and lower-limbs occur after
stroke. Thus, we focus our study, given this state-of-the-art analysis, on lower-
limb recovery, speciﬁcally on the ankle. It has been reported that both plantar
and dorsiﬂexion are compromised after a stroke: plantarﬂexor passive stiﬀness
is the cause for reduced plantarﬂexion torque before starting the swing phase in
gait, and may also be the cause to limited dorsiﬂexion, compromising foot clear-
ance [26]. It is also interesting to point out that these rehabilitative approaches
can be either used for chronic or acute patients, leading to performance improve-
ment and recovery beyond the current 6-month recovery plateau [27].
This study focuses on the assessment of detailed metrics of force production
and position control -healthy subjects- and their correlation with submaximal
force production control learning, as improving regularity in submaximal force
production might help improve functionality and reduce disability [28] during
a new task consisting in maintaining the position for early rehabilitation after
stroke. Several assessment measurements are taken during the learning task to
temporally determine the scale of this learning. Our aim is to characterize the
capacity to perform the precision task of maintaining the position with a sub-
maximal force production in healthy subjects.
2
Materials and Methods
In this section, we brieﬂy present the platform we use for this study and expose
the methodology. The analysis of the obtained data is performed with Math-
Works R⃝MATLAB R⃝.
2.1
Experimental Platform
A Motorized Ankle Foot Orthosis (MAFO) is used for this study (see Fig. 1),
with a zero-torque control. This robotic platform permits to exert controlled
torque proﬁles to the ankle joint of the subject. It is connected to a BeagleBone
Black board running Ubuntu and a custom application to interface the CAN-
enabled driver board of the robot to MathWorks Simulink R⃝, where the visual
paradigm is programmed, and to MATLAB, from where the control parameters
for the robot are sent.
2.2
Participants
Nine males and six females without any history of neuromuscular and cardio-
vascular disorders participated in this study; with ages 27.73 ± 3.58, and sport
activity more than 2 and less than 6 h per week.

Robotic Platform to Induce Motor Learning
573
(a)
(b)
Fig. 1. (a) Subject inside the robotic platform performing the experiment; and
(b) detail of the actuator of the robotic platform.
2.3
Task
The experiment consisted in following the trajectories depicted via the visual
paradigm (Fig. 2), while the robot disturbed the movement by performing plan-
tar and dorsiﬂex interleaved torque patterns (Fig. 3). The aim of the exercise
was to improve the motor control by learning how to maintain the position to
follow the trajectory in the screen, compensating the perturbations done by the
robot.
Training
Paradigms.
Three training paradigms were designed for the
experiment:
Fig. 2. Visual paradigm. The user controls the position of the bird with the angular
position of the ankle.

574
G. As´ın-Prieto et al.
0
200
400
600
0
0.2
0.4
0.6
0.8
1
Samples
Normalized amplitude
0
200
400
600
−1
−0.8
−0.6
−0.4
−0.2
0
Samples
Fig. 3. Torque patterns.
– Fixed torque: This training paradigm performed the thirty trajectories at a
ﬁxed torque (established at 16 N·m as a technical limitation of the set robot-
controller).
– Progressive: Progressive paradigm progressively increased the torque exerted
by the robot from a minimum of 1 N·m up to 16 N·m.
– Modulated: The peak torque exerted in each trajectory was increased in 2
N·m if score in the previous one was 100%, and decreased to the average
between the two last trajectories’ score (saturated in this way: decreased
at least 1 N·m and at most 2 N·m in comparison to previous torque). For
example, if score is lower that 100%, previous peak torque was 8 N·m, and
current torque was 10 N·m, then next torque will be (8 + 10)/2 = 9 N·m,
due to the diﬀerence 10 −9 = 1 N·m being higher or equal to 1 and lower or
equal to 2 N·m.
Task Description. The task consisted on forty trajectories, distributed as in
Fig. 4: (1) ﬁrst trajectory let the user move the foot freely to understand the
dynamics of the exercise; (2) the three trials from 2 to 4 assessed the performance
of the subject in the task before the training (at ﬁxed torque); (3) thirty train-
ing trials were performed (1 out of 3 diﬀerent training paradigms was randomly
selected for each user); (4) the three trials from 35 to 37 assessed the performance
of the subject in the task after the training, at 80% of the ﬁxed torque, e.g. 1.2 N·m
(in order to assess the performance at a diﬀerent level than that used for training
in the ﬁxed torque training paradigm); and (5) ﬁnally, the last three trials from
38 to 40 assessed the performance of the subject in the task after the training.
Fig. 4. Experiment phases.

Robotic Platform to Induce Motor Learning
575
40
60
80
100
First assessment
80 % assessment
Last assessment
Score (%)
Full torque
40
60
80
100
First assessment
80 % assessment
Last assessment
Progressive
40
60
80
100
First assessment
80 % assessment
Last assessment
Modulated
Fig. 5. Scores for the three training paradigms (average and standard deviation), for
the three assessments: (1) Before training at ﬁxed torque, (2) after training at 80% of
ﬁxed torque, and (3) after training at ﬁxed torque.
0
1
2
3
First assessment
80 % assessment
Last assessment
Root Mean Square Error (degrees)
Full torque
0
1
2
3
First assessment
80 % assessment
Last assessment
Progressive
0
1
2
3
First assessment
80 % assessment
Last assessment
Modulated
Fig. 6. Root mean squared error for the three training paradigms (average and standard
deviation), for the three assessments: (1) Before training at ﬁxed torque, (2) after
training at 80% of ﬁxed torque, and (3) after training at ﬁxed torque.

576
G. As´ın-Prieto et al.
3
Results
In Fig. 5 we depict the scores for the three training paradigms (average and
standard deviation) together with the trend (calculated as the ﬁtting of the two
ﬁxed torque assessment). We observe that the score increases after the training,
and that the score for the assessment at the 80% of the ﬁxed torque at the end
of the training is better for all training paradigms.
Figure 6 shows the root mean squared error for the three training paradigms.
Lower errors are obtained after the training.
In Fig. 7 we can see the mean interaction torque for the three training para-
digms, separated in dorsi and plantarﬂex trials. For (1) the ﬁxed torque training
paradigm, all trials reach the 16 N·m target; (2) in the progressive paradigm we
can observe this progressively increase in the torque; and (3) in the modulated
Fig. 7. Torque for the three training paradigms, separated in dorsiﬂexion and plan-
tarﬂexion. Average is represented with a solid black line, and standard deviation is
represented by the grey shadow in the ﬁgure.

Robotic Platform to Induce Motor Learning
577
torque training paradigm, the torque increases up to a plateau, not reaching the
established ﬁxed torque in any case. We had to remove from the analysis the ﬁrst
training trial due to a technical problem (ﬁxed torque of 16 N·m was incorrectly
applied in this ﬁrst trial for all the training paradigms).
We observed that all training paradigms led to an improvement in the score
comparing pre and post-training performances, so we concluded that this plat-
form induces learning on healthy subjects. Consistently, the error of the subject
when positioning the bird on the visual cue (e.g. maintaining the foot at 0◦),
before and after training, decreased.
All the users for the cases of ﬁxed torque and progressive torque para-
digms obviously reached the prescribed torque, but this was not the case for
the modulated torque paradigm, where none of the users was able to reach this
16 N·m torque during the training, so we concluded that this torque exceeds our
approach of using submaximal force production. Thus, we will study possible
methodologies to calculate this customized submaximal torque for each subject.
All the subjects reported that they liked the game, and most of them said
that it was too easy. This is also observable looking into the scores. We will test
more diﬃcult trajectories to be maintained, as well as diﬀerent torque proﬁles,
in order to increase the diﬃculty of the game.
To sum up, we conclude that this tool is useful to induce learning in healthy
subjects, and thus will keep improving the training paradigms, for the translation
into a rehabilitative tool.
Acknowledgements. This research has been funded by the Commission of the Euro-
pean Union under the BioMot project - Smart Wearable Robots with Bioinspired
Sensory-Motor Skills (Grant Agreement number IFP7-ICT- 2013-10-611695, and par-
tially supported with grant RYC-2014-16613 by Spanish Ministry of Economy and
Competitiveness.
References
1. Cramer, S.C., Sur, M., Dobkin, B.H., O’brien, C., Sanger, T.D., Trojanowski, J.Q.,
Rumsey, J.M., Hicks, R., Cameron, J., Chen, D., et al.: Harnessing neuroplasticity
for clinical applications. Brain 134(6), 1591–1609 (2011)
2. Cano-de-la Cuerda, R., Molero-S´anchez, A., Carratal´a-Tejada, M., Alguacil-Diego,
I.M., Molina-Rueda, F., Miangolarra-Page, J.C., Torricelli, D.: Theories and con-
trol models and motor learning: clinical applications in neurorehabilitation. Neu-
rolog´ıa (English Edition) 30(1), 32–41 (2015)
3. As´ın-Prieto, G., Cano-de-la Cuerda, R., L´opez-Larraz, E., Metrot, J., Molinari, M.,
van Dokkum, L.E.H.: Emerging perspectives in stroke rehabilitation. In: Emerging
Therapies in Neurorehabilitation, pp. 3–21. Springer (2014)
4. Nudo, R.J., Wise, B.M., SiFuentes, F., Milliken, G.W.: Neural substrates for the
eﬀects of rehabilitative training on motor recovery after ischemic infarct. Science
272(5269), 1791 (1996)
5. Langhorne, P., Coupar, F., Pollock, A.: Motor recovery after stroke: a systematic
review. Lancet Neurol. 8(8), 741–754 (2009)

578
G. As´ın-Prieto et al.
6. Patton, J.L., Mussa-Ivaldi, F.A.: Robot-assisted adaptive training: custom force
ﬁelds for teaching movement patterns. IEEE Trans. Biomed. Eng. 51(4), 636–646
(2004)
7. Shea, J.B., Morgan, R.L.: Contextual interference eﬀects on the acquisition, reten-
tion, and transfer of a motor skill. J. Exp. Psychol. Hum. Learn. Memory 5(2),
179 (1979)
8. Krakauer, J.W.: Motor learning: its relevance to stroke recovery and neurorehabil-
itation. Curr. Opin. Neurol. 19(1), 84–90 (2006)
9. Shea, C.H., Kohl, R.M.: Composition of practice: inﬂuence on the retention of
motor skills. Res. Q. Exerc. Sport 62(2), 187–195 (1991)
10. Conditt, M.A., Gandolfo, F., Mussa-Ivaldi, F.A.: The motor system does not learn
the dynamics of the arm by rote memorization of past experience. J. Neurophysiol.
78(1), 554–560 (1997)
11. Fasoli, S.E., Krebs, H.I., Stein, J., Frontera, W.R., Hogan, N.: Eﬀects of robotic
therapy on motor impairment and recovery in chronic stroke. Archives Phys. Med.
Rehabil. 84(4), 477–482 (2003)
12. Volpe, B.T., Krebs, H.I., Hogan, N., Edelstein, L., Diels, C., Aisen, M.: A novel
approach to stroke rehabilitation robot-aided sensorimotor stimulation. Neurology
54(10), 1938–1944 (2000)
13. Krebs, H.I., Hogan, N., Aisen, M.L., Volpe, B.T.: Robot-aided neurorehabilitation.
IEEE Trans. Rehabil. Eng. 6(1), 75–87 (1998)
14. Reinkensmeyer, D.J., Patton, J.L.: Can robots help the learning of skilled actions?
Exerc. Sport Sci. Rev. 37(1), 43 (2009)
15. Todorov, E., Shadmehr, R., Bizzi, E.: Augmented feedback presented in a virtual
environment accelerates learning of a diﬃcult motor task. J. Motor Behav. 29(2),
147–158 (1997)
16. Liu, J., Cramer, S.C., Reinkensmeyer, D.J.: Learning to perform a new movement
with robotic assistance: comparison of haptic guidance and visual demonstration.
J. Neuroeng. Rehabil. 3(1), 20 (2006)
17. Dobkin, B.H.: Strategies for stroke rehabilitation. Lancet Neurol. 3(9), 528–536
(2004)
18. Emken, J.L., Reinkensmeyer, D.J.: Robot-enhanced motor learning: accelerating
internal model formation during locomotion by transient dynamic ampliﬁcation.
IEEE Trans. Neural Syst. Rehabil. Eng. 13(1), 33–39 (2005)
19. Patton, J.L., Stoykov, M.E., Kovic, M., Mussa-Ivaldi, F.A.: Evaluation of robotic
training forces that either enhance or reduce error in chronic hemiparetic stroke
survivors. Exp. Brain Res. 168(3), 368–383 (2006)
20. Scheidt, R.A., Dingwell, J.B., Mussa-Ivaldi., F.A.: Learning to move amid uncer-
tainty. J. Neurophysiol. 86(2), 971–985 (2001)
21. Emken, J.L., Benitez, R., Sideris, A., Bobrow, J.E., Reinkensmeyer, D.J.: Motor
adaptation as a greedy optimization of error and eﬀort. J. Neurophysiol. 97(6),
3997–4006 (2007)
22. Bittmann, M.F., Patton, J.L.: Forces that supplement visuomotor learning: a “sen-
sory crossover” experiment. IEEE Trans. Neural Syst. Rehabil. Eng. 25(8), 1109–
1116 (2017)
23. Patton, J.L., Mussa-Ivaldi, F.A., Rymer, W.Z.: Altering movement patterns in
healthy and brain-injured subjects via custom designed robotic forces. In: Pro-
ceedings of the 23rd Annual International Conference of the IEEE Engineering in
Medicine and Biology Society, vol. 2, pp. 1356–1359. IEEE (2001)

Robotic Platform to Induce Motor Learning
579
24. Leroux, A., Pinet, H., Nadeau, S.: Task-oriented intervention in chronic stroke:
changes in clinical and laboratory measures of balance and mobility. Am. J. Phys.
Med. Rehabil. 85(10), 820–830 (2006)
25. Duncan, P.W., Goldstein, L.B., Horner, R.D., Landsman, P.B., Samsa, G.P.,
Matchar, D.B.: Similar motor recovery of upper and lower extremities after stroke.
Stroke 25(6), 1181–1188 (1994)
26. Lamontagne, A., Malouin, F., Richards, C.L., Dumas, F.: Mechanisms of disturbed
motor control in ankle weakness during gait after stroke. Gait Posture 15(3), 244–
255 (2002)
27. Page, S.J., Gater, D.R., Rita, P.B.: Reconsidering the motor recovery plateau in
stroke rehabilitation. Arch. Phys. Med. Rehabil. 85(8), 1377–1381 (2004)
28. Panjan, A., Simunic, B., Sarabon, N.: Maximal voluntary torque and the ability
to ﬁnely grade submaximal torque are not related/sposobnost ﬁnega uravnavanja
submaksimalnega navora ni povezana z najvecjim hotenim navorom. Kinesiologia
Slovenica 17(3), 5 (2011)

A Protocol Generator Tool for Automatic
In-Vitro HPV Robotic Analysis
Juan Pedro Dominguez-Morales1, Angel Jimenez-Fernandez1,
Saturnino Vicente-Diaz1, Alejandro Linares-Barranco1(B),
Asuncion Olmo-Sevilla2, and Antonio Fernandez-Enriquez3
1 Robotic and Technology of Computers Lab., University of Seville,
Av. Reina Mercedes s/n, 41012 Sevilla, Spain
{jpdominguez,alinares}@atc.us.es
2 Technical Department, Master Diagnostica S.L., Granada, Spain
asuncion.olmo@vitro.bio
3 Technical Department, VITRO S.A., Sevilla, Spain
antonio.fernandez@vitro.bio
Abstract. Human Papilloma Virus (HPV) could develop precancerous
lesions and invasive cancer, as it is the main cause of nearly all cases
of cervical cancer. There are many strains of HPV and current vaccines
can only protect against some of them. This makes the detection and
genotyping of HPV a research area of utmost importance. Several bio-
medical systems can detect HPV in DNA samples; however, most of
them do not have a procedure as fast, automatic or precise as it is actu-
ally needed in this ﬁeld. This manuscript presents a novel XML-based
hierarchical protocol architecture for biomedical robots to describe each
protocol step and execute it sequentially, along with a robust and auto-
matic robotic system for HPV DNA detection capable of processing from
1 to 24 samples simultaneously in a fast (from 45 to 162 min), eﬃcient
(100% markers eﬀectiveness) and precise (able to detect 36 diﬀerent HPV
genotypes) way. It includes an eﬃcient artiﬁcial vision process as the last
step of the diagnostic.
Keywords: Robotic arm · In-vitro analysis · Automatic diagnostic ·
Computer vision · OpenCV · Convolutional neural networks · XML
protocol
1
Introduction
Human papillomavirus (HPV) is a group of more than 150 DNA viruses from the
papillomavirus family. Most HPV infections will cause no physical symptoms,
but others could cause benign papillomas or premalignant lesions that could lead
to cancer, especially cervical cancer. HPV is transmitted through intimate skin-
to-skin contact and is the most common sexually transmitted infection (STI).
This virus is so common that nearly all sexually active men and women get it
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_48

A Protocol Generator Tool for Automatic In-Vitro HPV Robotic Analysis
581
at some point in their lives, making it a very high interest subject of study for
cervical cancer screening.
Persistent human papillomavirus infection is a necessary cause for the devel-
opment of cervical cancer [1,2]. Therefore, sensitive HPV detection is crucial
to establish the risk of progression to high grade intraepithelial lesions and cer-
vical carcinoma. In addition, HPV genotyping constitutes an important tool to
assess the speciﬁc genotype-associated risk of progression and subsequent patient
management.
HPV testing oﬀers greater sensitivity for the early detection of pre-cancerous
cervical lesions and has been recommended as a triage tool for eﬃcient patient
management. The HPV detection systems available on the market are mainly
based on hybridization assays [3], signal ampliﬁcation assays [4], or the ampliﬁ-
cation of viral sequences by Polymerase Chain Reaction (PCR) [4]. Several HPV
detection systems do not identify the speciﬁc genotype(s) present in the sam-
ple, and their results have a limited usefulness as they can only be reported as
high-risk positive, low-risk positive, or high-risk + low-risk positive. The market
requires highly sensitive and speciﬁc HPV genotyping systems. These methods
should also be automated, rapid and reliable.
There are several systems in the market for HPV DNA detection. Most of
them are time consuming (6–8 h minimum to get results) (CLART R
⃝Technol-
ogy from Genomica, Spain1), others are not fully automatic (FT-Pro Flow-
Through hybridization system from Diagcor BioScience, China2), or are not able
to identify a multiplex panel of HPV genotypes (Cobas from Roche Diagnostics,
Switzerland3). Other systems are not able to run a set of samples simultaneously,
but only one sample per run (GeneXpert from Cepheid, USA4).
HybriSpot 24 (HS24) platform5 is a robotic device intended for automatic
and rapid multiplexing analysis of biomarkers, by a reverse dot blot colorimet-
ric hybridization into a porous CHIP based on DNA Flow Technology. This
hybridization platform allows the target molecules (DNA amplicons) to verti-
cally ﬂow through and cross the membrane towards the complementary capture
probes that are immobilized inside the matrix pores. This approach enables the
reaction between the target molecules in a three-dimensional porous environment
rather than passive surface hybridization, allowing the reactions to be completed
in seconds. It generates much higher signal intensities in a very short period of
time, reducing reagent volumes and processing times from hours to minutes,
which represents an economic alternative for molecular diagnostic.
1 Genomica, “CLART
R
⃝Technology.” [Online]. Available: http://genomica.es/en/in
vitro diagnostics products.cfm.
2 Diagcor BioScience, FT-Pro Flow-Through hybridization system. [Online]. Avail-
able: http://www.diagcor.com/en/mdx-products/detail/ft-pro-auto-system.
3 Roche Diagnostics, Cobas. [Online]. Available: http://molecular.roche.com/assays/
Pages/cobasHPVTest.aspx.
4 Cepheid, GeneXpert. [Online]. Available: http://www.cepheid.com/us/cepheid-
solutions/systems/genexpert-systems/genexpert-i.
5 VITRO
S.A.,
Hybrispot
24.
[Online].
Available: www.masterdiagnostica.com/
products/moleculardiagnostickits/dnaﬂowtechnology/instruments.aspx.

582
J.P. Dominguez-Morales et al.
The rest of the manuscript is structured as follows: Sect. 2 presents hybriSpot
24, which is the biomedical robot that is used in this work to implement and exe-
cute XML protocols, as well as to obtain execution times. Then, Sect. 3 explains
the image analysis technique used in this work. Section 4 introduces XML proto-
cols and describes their hierarchical structure and components (Basic Commands
and High-Level Commands). Section 5 describes the process of building XML
protocols. Section 6 presents results for a HPV diagnosis protocol in terms of
execution times, performance, robustness and validation. Finally, Sect. 7 presents
the conclusions of this work.
2
HybriSpot 24
HybriSpot 24 (See Fig. 1 is a fully automatic biomedical robot developed by
VITRO S.A6, DropSens7, Master Diagnostica8 and RTC Lab9, and commercial-
ized by VITRO S.A., whose main aim is to detect and genotype viruses like
HPV. HS24 can process from 1 to 24 samples simultaneously in a period of
time that ranges from 45 to 162 min, respectively. It provides a diagnostic plat-
form for rapid detection and identiﬁcation of HPV genotypes by reverse dot blot
hybridization onto a microarray porous membrane (see Fig. 2). HS24 is based
on a Gilson R
⃝GX-271 Liquid Handler robotic arm. This arm is a very rigid
and robust Cartesian Coordinate Robot (CCR) that also has a dispenser, Fig. 1,
which is a syringe-like electronic device that allows to pour or to suction liq-
uids from various inputs automatically using speciﬁc commands. The dispenser
is connected to a probe located on the robot, in addition to a washing solution
container that is used to purge the system after particular steps of a protocol.
HS24 also has:
– A washing station with two positions: a waste position and a probe cleaning
position.
– Two 3 × 8 (three rows and eight columns) matrices to place DNA samples.
– 28 containers for reagents. Four of them are dedicated to a special reagent
(hybridization buﬀer), which is most widely used in every protocol, and have
two heaters so that the temperature can be set inside the containers. Setting
this reagent to a speciﬁc temperature at a particular step of the protocol is
essential for the speed and success of the analysis.
– Two 4 × 3-matrix (four rows and three columns) reaction chambers with one
associated Peltier cell. Each chamber allows the incubation process on the
membranes.
Using the robotic arm, the dispenser and the diverse elements presented
above, HS24 is able to combine reagents with DNA samples, incubate them and
execute a set of operations that will result on the spots color development at
6 VITRO S.A. Available: http://www.vitro.bio/.
7 DropSens. Available: http://www.dropsens.com/.
8 Master Diagnostica. Available: http://www.masterdiagnostica.com/.
9 RTC, Robotic and Tech. of Computers Lab. http://www.rtc.us.es/.

A Protocol Generator Tool for Automatic In-Vitro HPV Robotic Analysis
583
Fig. 1. HS24 instrument, composed of a Gilson
R
⃝GX-271 Liquid Handler robotic arm
with one dispenser attached, a washing station, a webcam, two reaction chambers with
peltier cells, two reagents zones (with heaters), two 12 samples areas and embedded
STM32-based USB PCBs for heaters / valves control.
the end of the process. In order to provide to the system with enough ﬂexibility
for other protocol implementations, or for tuning or debugging a protocol under
development, all these operations are described in XML. Thus, the protocol is a
sequence of XML sentences, which include information about every single exe-
cution step of the robot. Each membrane has a printed matrix with spots. Some
spots are always there for control, some others will show up depending on the
result of the detected genotypes. Figure 2 shows a membrane with highlighted
spots after successfully completing a HPV diagnosis protocol. HS24 includes a
high deﬁnition camera attached to the robotic arm that takes pictures of the
membranes. They are sent to the host computer software, where they are auto-
matically analyzed with two diﬀerent mechanisms to perform a diagnosis and
store it in a database: (a) OpenCV [5] artiﬁcial vision algorithms and (b) Convo-
lutional Neural Network (CNN) previously trained with an extensive database
collected in Master Diagnostica during the instrument testing period.

584
J.P. Dominguez-Morales et al.
Fig. 2. Membrane with highlighted spots after executing a HPV diagnosis protocol
(o). o+f, o+b and o+r are the output from diﬀerent synthetic samples generation
techniques.
3
Image Analysis
The principal approach implemented in this instrument is a classical one using
artiﬁcial vision techniques, through the OpenCV library; we aim to compare this
principal one to a deep learning based technique, using Convolutional Neural
Networks. The second one aim is for validating deep learning in this particular
application. The OpenCV library technique will be used as the ground truth for
the deep learning training procedure, since 100% of accuracy has been obtained
in the laboratory with proper camera conﬁguration and light conditions. Once
the protocol procedure of the samples have concluded, the camera installed in the
instrument, close to the syringe, is activated and a picture is taken. The following
operations are applied to the picture: (a) a smooth ﬁlter process, (b) a circle
detection for the membrane plus centering it, (c) remove from the image anything
not useful, (d) search for key spots in the membrane using template matching
techniques for three diﬀerent small circles detection, (e) distance calculation
between these spots (the maximum distance identiﬁes two of the ﬁve control
spots, and the other three follow logic rules to be located), (f) membrane rotation
to the same view for all membranes in the database, (g) diagnostic depending
on how many spots and which ones are detected in the membrane. In the other
hand, for the CNN technique, a database has to be created using the analyzed
samples during the testing period of the instrument for the HPV analysis only.
The images taken from the samples after the HPV analysis have been processed
using diﬀerent techniques like focus ﬁlters (o+f), brightness ﬁlters (o+b) and
rotation ﬁlters (o+r) to increase the number of images in the database in a
synthetic way. A classic CNN architecture extracted from LeNet-5 has been
implemented and trained with Caﬀe [13]. The CNN execution can be performed
in the HSHS software by using the open-source Caﬀe libraries.
4
XML Protocols
Our instruments have to execute a particular protocol to analyze a tissue
sample to make a diagnostic of the presence of a particular DNA sequence

A Protocol Generator Tool for Automatic In-Vitro HPV Robotic Analysis
585
(i.e. HPV, Zoonosis, Sepsis, etc.). This protocol is stored as a sequential list
of commands that the robot has to execute to perform a speciﬁc functionality.
XML (eXtensible Markup Language) [6] has been selected in this case. XML is a
language developed by W3C that is mainly used to store information. The main
beneﬁts of this language are its easy managing and its very legible structure.
The information in this format is saved under a hierarchical structure similar to
a tree: there is a root node and subtrees of children with a parent node. Thus,
we call it XML-protocol. The root node is the name of the protocol, and two dif-
ferent types of children could be below the root (by convention, trees are drawn
growing downwards):
– If the node is a leaf node (it does not have any children) then it is a Basic
Command, i.e. a simplest actuation of the robot.
– If the node is an inner node (it has at least one child) then it is a High-Level
Command. This command is a set of Basic Commands that describes a higher
level functionality.
4.1
Basic Commands
They are instructions that describe basic functionalities of the robot, like position
control, dispenser control, valves and pumps control and Peltier cells control. The
set of basic commands that an XML-protocol can use allows to execute all the
steps needed by a biomedical robot in the analysis process. These commands
are:
– RobotCmd. Used to command the arm motion in the X, Y and Z Cartesian
axes. The Z movement allow the syringe of the robot to go up and down in
order to suction or dispense liquids properly. The robot has a liquid detector
in the syringe. There are four additional parameters in this command that
allow to (1) select movements only in X and Y axes, (2) do a homing process
after the command is executed, (3) do a movement only in Z axis and (4)
enable liquid detection in the probe. XY movements are allowed only if Z
axis is under a security threshold to avoid syringe damage. As can be seen,
the speed of the movement cannot be set with this command. The robot speed
is controlled with a diﬀerent parameter and cannot be modiﬁed using these
kind of commands.
– DispCmd. Its purpose is to suction or dispense liquids like reagents or DNA
samples using the probe attached to a dispenser. Quantities used in this com-
mand should be checked carefully to avoid exceeding the maximum capacity
of the dispenser. The speed to execute this process can also be speciﬁed with
a value between 0.01 and 40 ml/min, in addition to the position of the valve
of the dispenser, which will allow to suction or dispense from the probe tip
or from the washing solution container. The speed in the absorbing process
should not be high, since the potential formation of air bubbles could aﬀect
the entire analysis [7].
– PeltierCmd. This command controls the temperature of the Peltier cells
located bellow the reaction chambers and the main reagents of the robot.

586
J.P. Dominguez-Morales et al.
This process is essential for the incubation of the samples. Some parameters
are required for executing this command, such as the ID of the Peltier cell that
will be aﬀected, the target temperature, and the waiting time after reaching
the temperature. This command will block the following command until this
wait is over.
– ValveCmd. On a biomedical robot, valves redirect the liquid ﬂow in the
system. ValveCmd command allows this task, adding some control over it by
using three parameters: (1) to specify the action (on/oﬀ) that will be applied
to (2) a speciﬁc valve or to a set of them and (3) how much time will the
system wait after this action until the next command starts its execution.
– PumpCmd. Pumps allow to turn on or oﬀthe liquid ﬂow in the robot. This
is very useful in washing and waste removal processes. This command has
the same parameters as ValveCmd, but it acts on the pumps instead of the
valves.
– WaitCmd. This command waits for a speciﬁc amount of time (minutes or
seconds.
– CameraCmd. This command takes a picture of the membrane whose posi-
tion is speciﬁed as an input. The captured picture is processed using an
OpenCV (Open Source Computer Vision) [5] algorithm, or a CNN previously
trained. OpenCV is an open source computer vision and machine learning
software library of programming functions mainly used for motion tracking
[8], facial or gesture recognition [9,10] and augmented reality [11]. This library
has been used to detect which spots of the matrix that is printed on the mem-
brane are highlighted, and their intensity. With this information, OpenCV
gives an automated diagnosis of the detected genotypes.
As they are focused on low-level functionalities, creating an entire protocol using
only these commands could be a slow and tedious process with a high rate of
human error on it, which may cause serious damage to the robot.
4.2
High-Level Commands
High-level commands are a set of basic commands that describe their whole basic
functionalities under a more complex and useful task. These commands allow
the creation of longer protocols faster, and since they need less parameters to
be speciﬁed, human mistakes in this process are reduced. The most important
high-level commands are:
– Wash. This command cleans the probe attached to the robot both internally
and externally, using a washing solution, which is pumped through the system
using basic commands to control valves and pumps. Throughout the execution
of the protocol, the robot uses diﬀerent reagents and samples, which could
spoil the whole process if they mix at some point. That is what makes a purge
command like this so important for a biomedical robot.
– Reagents dispensing. Before a protocol starts, the camera attached to the
robotic arm scans the workspace, to identify where the reagents needed in the

A Protocol Generator Tool for Automatic In-Vitro HPV Robotic Analysis
587
protocol are placed, and saves this information. Using this information plus
the localization of the membranes that will be used (set by the user), this
command is able to dispense a speciﬁc quantity of any of the reagents (both
values are speciﬁed as parameters for this functionality) in every membrane
used in the protocol. Figure 3 shows the execution ﬂowchart for this command.
– Samples dispensing. This high-level command is similar to the reagents
dispensing command described above. Instead of dispensing a speciﬁc quan-
tity of a reagent in the membranes, it dispenses the reagent along with a
small quantity of a sample, both speciﬁed as parameters.
– Camera Capture. After the protocol is ﬁnished, this command is launched
to take images of the membranes that were used, and to analyze them with
OpenCV algorithm. This command is a combination of multiple CameraCmd
basic commands.
– Incubation. As the incubation of the membranes is one of the main steps
in a protocol, this command was created for this purpose. The incubation
high-level command combines a PeltierCmd and a WaitCmd. This will set
the membranes to a temperature that is speciﬁed in the input parameters
and then it waits for a speciﬁc amount of time after the Peltier cell reaches
the target temperature.Incubation.
– Drain. After some particular steps of the protocol, the membranes need to
be washed from reagents and samples that were dispensed. This cleaning
process is done with the drain command, which uses pump and valve basic
commands to allow the ﬂow of washing solution into the system below the
reaction chambers where the membranes are placed.
– Remove waste. Usually, the quantity of reagent that is suctioned is higher
than the one that is dispensed. This adds a small security gap of reagent
to maintain the pressure in the pipe that connects the probe with the dis-
Reagent suction
  - Go to reagent position (RobotCmd)
  - Suction a specific quantity (DispCmd)
Enough reagent to 
dispense?
Yes
Dispense reagent in the 
membrane
- Go to membrane position (RobotCmd)
- Dispense a specific quantity (DispCmd)
No
All membranes 
dispensed?
No
Yes
End
Fig. 3. Dispense Reagents command ﬂowchart.

588
J.P. Dominguez-Morales et al.
penser while dispensing. This gap is added for each dispensation, generating
a remainder in the dispenser that keeps increasing its quantity after some of
these instructions. The remove waste high-level command empties the syringe.
It is important to execute this command before the remainder and the quan-
tity that will be suctioned exceed the dispenser maximum quantity.
5
Building XML Protocols
In an analysis of a virus like HPV, the number of Basic-Commands that the
protocol can have is 400 per sample, approximately. Creating an XML protocol
manually for an analysis with more than one sample could take an inconceivable
amount of time. This is why protocols are not created manually by researchers; it
would take many hours to create it and, due to the fact that it would have a lot of
Basic-Commands, the human error possibility is very high. To solve this problem,
a software tool was developed to generate the XML protocol from a template
written in CSV format [12] which consists of very high-level instructions that will
later be translated into basic and high-level commands. A CSV template can be
written using more than 30 diﬀerent instructions, allowing to generate protocols
that describe the same functionality as those created by using only Basic and
High-Level commands, but spending less time and eﬀort and reducing the error
probability. A CSV template for the HPV analysis has around 63 instructions
and the time spent to create it is 30 min approximately.
Therefore, the researcher needs to create a generic CSV protocol tem-
plate based on his/her experiments and validations. Then, after selecting which
reagents and samples are going to be used in the protocol, the software tool reads
the CSV ﬁle associated with the selected analysis and automatically converts it
to a valid XML protocol, which can be understood and executed by the robot
based on this information. As it is written in a generic way, the CSV template,
once created, is useful for the analysis regardless of which reagents, samples or
membranes are used, being completely independent from these values. However,
XML protocols can still be built manually or even modiﬁed after being generated
using a CSV for further control over it.
Basic and High-Level commands are programmed as classes and functions
in C#, respectively. This means that adding new commands that were not
described in Sect. 3 can be done just by creating a new class deﬁning the com-
mand functionality and binding it to the main architecture class. This easy
process allows a quick and eﬀortless adaptation of this protocol system to new
instruments or robot functionalities.
6
Results
The time that the analysis process takes to complete depends on two factors:
ﬁrst, the kind of analysis that will be executed and, second, the number of
samples that will be analyzed. Figure 4 shows execution times for the HPV virus
diagnosis protocol using 1, 2, 6, 12, 15 and 24 membranes. These six tests were

A Protocol Generator Tool for Automatic In-Vitro HPV Robotic Analysis
589
Fig. 4. Execution times of the HPV diagnosis protocol for 1, 2, 6, 12, 15 and 24 samples.
executed in HS24. As can be seen, time increases linearly except for the range
between 12 and 15 samples, where the second reaction chamber (12 samples per
reaction chamber) needs to be used, which is farther from the washing station and
the hybridization buﬀer containers than the ﬁrst one. Table 1 shows execution
times for diﬀerent number of samples, the number of instructions in the CSV ﬁle
and the number of Basic Commands in the XML protocol. As it is an automated
process without the need of human supervision, robots can be programmed with
protocols during nighttime, increasing the number of tests per day and reducing
human cost. The performance and robustness of the automatic HS24 system was
validated by testing10 low concentrations of the HPV genotypes included in the
detection kit for HPV (5 copies for HPV 16 and HPV 18, 50–500 copies for the
other genotypes).
Basically, this validation demonstrates the reproducibility of the results in
positions 1 to 24 on the HS24 device and the reproducibility of the results for
runs with diﬀerent number of samples. Moreover, the system demonstrated a
high sensitivity of detection for all the genotypes.
Table 1. Execution time, number of instructions and number of Basic Commands for
1, 2, 3, 6, 12, 15 and 24 samples.
Number of Execution time Number of
Number of
samples
(minutes)
instructions Basic Commands
1
45
63
967
2
49
63
1161
6
70
63
2325
12
96
63
3877
15
128
63
5065
24
162
63
7587
10 All tests were performed at Master Diagnostica.

590
J.P. Dominguez-Morales et al.
The reproducibility of the results for runs with diﬀerent number of samples
was evaluated. Replicates for a positive sample containing a limiting copy num-
ber of several genotypes (50 GE) were loaded at diﬀerent positions in the HS24
platform, according to four protocols: (1) Protocol for 2 samples (2 replicates),
(2) protocol for 12 samples (3 replicates), (3) protocol for 15 samples (4 repli-
cates), (4) protocol for 24 samples (6 replicates).
The results were automatically analyzed and all the samples were detected
without diﬀerences among positions and protocols.
In order to verify the reproducibility of the protocol in diﬀerent positions of
the HS24 reaction chambers, four replicates for each of the 36 HPV genotypes
detected by the method at a limiting copy number were loaded in diﬀerent posi-
tions of the two reaction chambers in HS24, and protocols for 24 samples were
performed. The results were automatically analyzed and showed 100% repro-
ducibility for all the genotypes analyzed in the diﬀerent positions of the reaction
chambers using classical image processing techniques during instruments val-
idation at Master Diagnostica Labs for around 2 k samples. A LeNet-5 CNN
has been implemented in Caﬀe and a the under construction database is being
created using images taken from membranes after applying HPV analysis. For
MNIST experiments 60k training samples were needed to obtain 98% accuracy.
We aim to obtain similar results with similar database length on these HPV
images.
7
Conclusions
This manuscript presents a novel XML-based hierarchical protocol architecture
for biomedical robots that allows to describe any execution step that a HPV diag-
nosis protocol could need. This architecture is already implemented in hybriSoft
(hybriSpot 24 sample management software from VITRO), allowing the HPV
DNA detection processing of up to 24 samples simultaneously in less than three
hours, which is a novelty in terms of speed in the ﬁeld of biomedical robotic
systems.
As XML ﬁles are generated automatically using a generic CSV template of
the protocol, regardless of the number of samples and the reagents that will be
used, the user does not have to create a new protocol for the same virus diagnosis
when using a diﬀerent number of samples, reducing time and eﬀort to zero in
these cases. This also leads to having a minimum human error probability when
creating the protocol. Adjusting and adapting speciﬁc steps of the protocol can
be done by modifying a single XML or CSV ﬁle, which means a fast and eﬀortless
way to change the robot behavior.
Although in this manuscript we focused on HPV DNA detection, this protocol
architecture allows the detection of other infections like sepsis, meningitis, etc.
which are already implemented on the hybriSpot 24 and hybriSpot 12-Auto
systems.
Acknowledgments. We would like to thank VITRO for their continuous support and
help during this work and for lending us a hybriSpot 24 for protocol testing purposes.

A Protocol Generator Tool for Automatic In-Vitro HPV Robotic Analysis
591
This work is supported by FIDETIA (P055-12/E03). This work is supported by the
Spanish government grant (with support from the European Regional Development
Fund) COFNET (TEC2016-77785-P).
References
1. Bosch, F.X., Lorincz, A., Munoz, N., Meijer, C.J.L.M., Shah, K.V.: The causal
relation between human papillomavirus and cervical cancer. J. Clin. Pathol. 55,
244–265 (2002)
2. Walboomers, J.M., Jacobs, M.V., Manos, M.M., Bosch, F.X., Kummer, J.A., Shah,
K.V., Munoz, N.: Human papillomavirus is a necessary cause of invasive cervical
cancer worldwide. J. Pathol. 189, 12–19 (1999)
3. Wong, A.A., Fuller, J., Pabbaraju, K., Wong, S., Zahariadis, G.: Comparison of
the hybrid capture 2 and cobas 4800 tests for detection of high-risk human papil-
lomavirus in specimens collected in PreservCyt medium. J. Clin. Microbiol. 50,
25–29 (2012)
4. Abreu, A.L.P., Souza, R.P., Gimenes, F., Consolaro, M.E.L.: A review of methods
for detect human Papillomavirus infection. Virol. J. 9, 262 (2012), http://doi.org/
10.1186/1743-422X-9-262
5. Bradski, G.: The opencv library. Doctor Dobbs J. 25, 120–126 (2000)
6. Bray, T., Paoli, J., Sperberg-McQueen, C.M., Maler, E., Yergeau, F.: Extensible
markup language (XML). World Wide Web J. 2, 27–66 (1997)
7. Savchenko, Y.: Supercavitation-problems and perspectives. In: CAV 2001, pp. 1–8
(2001)
8. Luo, Y., Yang, H., Hu, Z.: Human limb motion real-time tracking based on
CamShift for intelligent rehabilitation system. In: IEEE International Conference
on Robotics and Biomimetics (ROBIO), pp. 343–348 (2009)
9. Pingxian, Y., Rong, G., Peng, G., Zhaoju, F.: Research on lip detection based on
Opencv. In: International Conference on Transportation, Mechanical, and Electri-
cal Engineering (TMEE), pp. 1465–1468 (2011)
10. Li, X., Han, G., Ma, G., Chen, Y.: A new method for detecting fatigue driving
with camera based on OpenCV. In: International Conference on Wireless Commu-
nications and Signal Processing (WCSP), pp. 1–5 (2011)
11. Manaf, A. S., Sari, R. F.: Color recognition system with augmented reality concept
and ﬁnger interaction: case study for color blind aid system. In: 9th International
Conference on ICT and Knowledge Engineering, pp. 118–123 (2012)
12. Shafranovich, Y.: Common format and MIME type for comma-separated values
(CSV) ﬁles (2005)
13. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.,
Guadarrama, S., Darrell, T.: Caﬀe: Convolutional Architecture for Fast Feature
Embedding. arXiv preprint arXiv:1408.5093 (2014)

Robotics and Cyber-Physical Systems
for Industry 4.0 (I)

End-Eﬀector Precise Hand-Guiding
for Collaborative Robots
Mohammad Safeea1, Richard Bearee2, and Pedro Neto1,2(B)
1 University of Coimbra, 3000-033 Coimbra, Portugal
ms@uc.pt
2 Arts et Metiers ParisTech, LSIS Lille 8, Boulevard Louis XIV,
59046 LILLE Cedex, France
pedro.neto@dem.uc.pt
Abstract. Hand-guiding is a main functionality of collaborative robots,
allowing to rapidly and intuitively interact and program a robot. Many
applications require end-eﬀector precision positioning during the teach-
ing process. This paper presents a novel method for precision hand-
guiding at the end-eﬀector level. From the end-eﬀector force/torque mea-
surements the hand-guiding force/torque (HGFT) is achieved by com-
pensating for the tools weight/inertia. Inspired by the motion properties
of a passive mechanical system, mass subjected to coulomb/viscous fric-
tion, it was implemented a control scheme to govern the linear/angular
motion of the decoupled end-eﬀector. Experimental tests were conducted
in a KUKA iiwa robot in an assembly operation.
Keywords: Hand-guiding · Collaborative robot · End-eﬀector
1
Introduction
Human-robot interaction has been studied along the last decades, from text-
based programming to oﬀ-line programming [1] to the more recent intuitive
techniques in which humans interact with the robots like interact with each
other using natural means like speech, gestures and touch [2,3]. In the touch
interaction mode the robot can be hand guided to teach the required paths.
Hand-guiding is a representative functionality of collaborative robots, allow-
ing unskilled users to interact and program robots in a more intuitive way than
using the teach pendant. Existing collaborative robots include hand-guiding
functionality with limitations in terms of the accuracy required for many tasks
like assembly. For precision positioning (position and orientation of the end-
eﬀector) the teach pendant is still used (even for sensitive robots). The use of
the teach pendant limits the intuitiveness of the teaching process provided by the
hand guiding and it is time consuming (an important parameter on the factory
ﬂoor). When the operator is using the teach pendant, he/she has to visualize
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_49

596
M. Safeea et al.
mentally diﬀerent reference axes of the robot. In some scenarious, the teach pen-
dant based robot positioning can conduct to undesirable collisions, which can
cause damage in sensitive equipment.
This paper presents a novel method for end-eﬀector precision hand-guiding
to be applied in collaborative robots equipped with joint torque sensors or with
a force/torque sensor attached to the end-eﬀector. It was implemented a control
scheme that utilizes force feedback to compensate for the end-eﬀector weight, so
that a minimal eﬀort is required from the operator to perform the hand-guiding
motion. Inspired by the motion properties of a passive mechanical system, mass
subjected to coulomb/viscous friction, it was implemented a control scheme to
govern the linear/angular motion of the decoupled end-eﬀector. Experimental
tests were conducted in a KUKA iiwa 7 R800 robotic manipulator in an assem-
bly operation requiring precision and ﬁne tuning. It is demonstrated that the
proposed method allows precise hand guiding with smooth robot motion in terms
of position and orientation of the end-eﬀector.
1.1
State of the Art
The importance of collaborative robots and hand-guiding teaching is well known
in the robotics community. It is still a challenge to have robots and humans shar-
ing the same space performing collaborative tasks with the required safety levels
to minimize the risk of injuries [4]. Human subjects can interact physically with
a robot arm moving and guiding the robot into the desired grasping poses, while
the robot’s conﬁguration is recorded [5]. Diﬀerent control schemes for human-
robot cooperation have been proposed, for example physical interaction in 6
DOF space using a controller that allows asymmetric cooperation [6]. Robot
assistance through hand-guiding is also used to assist in the transport of heavy
parts. In [7] it is studied the application of a human-industrial robot coopera-
tive system to a production line. Safety, operability and the assistance of human
skills were studied as they relate to hand-guiding. Hand-guiding teaching for
conventional industrial robots are normally sensor based. In [8] it is proposed a
sensor less hand guiding method based on torque control. The dynamic model
of a robot along with the motor current and friction model is used to determine
the users intention to move the end-eﬀector of a robot instead of directly sensing
the external force by the user.
The problem of Cartesian impedance control of a redundant robot executing
a cooperative task with a human has been addressed in [9]. Redundancy was used
to keep robots natural behavior as close as possible to the desired impedance
behavior, by decoupling the end eﬀector equivalent inertia. The authors claim
that this allows easily ﬁnding a region in the impedance parameter space where
stability is preserved. A method for using impedance control of multiple robots
to aid a human moving an object and conducted an experiment with motion
along one degree of freedom (DOF) was presented in [10].
In [11] it is proposed an approach for collision detection and reaction for an
industrial manipulator with a closed control architecture and without external
sensors. The robot requirements are the joint velocity, motor currents and joint

End-Eﬀector Precise Hand-Guiding for Collaborative Robots
597
positions. Robot assisted surgery demands high precision hand-guiding accu-
racy. An approach that enables an operator to guide a robot along a predeﬁned
geometric path while taking required joint constraints into account and thus
implicitly considering the actuator dynamics is proposed in [12].
2
Work Principal
Assuming force feedback at the robot end-eﬀector (the robot may have joint
torque sensors or a force/torque sensor attached to the end-eﬀector), three groups
of robot hand-guiding motion are considered, Fig. 1:
1. The ﬁrst motion-group represent the positioning (with linear displacements)
of the end-eﬀector in the X, Y and Z coordinates system of the robot base;
2. The second motion-group is used for orienting the end-eﬀector in the Carte-
sian space;
3. The third motion group is used to rotate the end-eﬀector around its axis.
Those motion groups are introduced because they are the most intuitive for
humans when we perform a motion that requires precision.
Fig. 1. The proposed robot motion groups for precision hand-guiding
Let us consider the hand-guiding force the force applied by the operator
at the end-eﬀector for linear positioning, and the hand-guiding moment the
moment applied by the operator at the end-eﬀector for angular positioning.
The force/moment measurements represent the forces/moments due to (1) end-
eﬀector weight (2) the inertial forces/moments due to the acceleration of the
end-eﬀector, and (3) the external hand-guiding force/moment (to be applied by
the operator for achieving hand-guiding). To simplify the calculations we omit
the inertial forces/moments due to end-eﬀector acceleration because in precise
hand-guiding applications these inertial forces are relatively small compared to
its weight and to the external forces/moments. The measured forces and torques
can be approximated to be due to the hand-guiding forces and the weight of the
end-eﬀector. The components of the hand-guiding force described relative to the
robot base frame are used as inputs for the proposed linear motion controller,
i.e., for positioning the end-eﬀector according to the robot base frame.

598
M. Safeea et al.
2.1
Hand-Guiding Force
The components of the hand-guiding force described in robot base frame f b =
(fx, fy, fz) serve as input to move the robot end-eﬀector along the X, Y or Z
directions of the robot base, 1st motion group. The maximum of the components
(fx, fy, fz) is used to calculate the control command to control the end-eﬀector
one axis at a time. Those hand-guiding forces (f e
x, f e
y, f e
z ) described on the end-
eﬀector reference frame are calculated by subtracting the weight of the end-
eﬀector from the measured forces:
⎡
⎣
f e
x
f e
y
f e
z
⎤
⎦=
⎡
⎣
ue
x
ue
y
ue
z
⎤
⎦−Re
b
⎡
⎣
0
0
w
⎤
⎦
(1)
where (ue
x, ue
y, ue
z) are the components of end-eﬀector force/torque measure-
ments, described in sensor frame (in this study the frame of the force/torque
measurements is the same as the frame of the end-eﬀector, otherwise a con-
stant transform between the two frames can be introduced and the methodology
described is still valid). Here, w is the weight of the end-eﬀector and Re
b is the
rotation matrix from base frame to end-eﬀector frame. This matrix is obtained
from the transpose of the rotation matrix from the end-eﬀector to the base frame:
Re
b =

Rb
e
T
(2)
where Rb
e is obtained from the direct kinematics.
The hand-guiding force f b described in base frame is calculated from:
f b = Rb
e
⎡
⎣
f e
x
f e
y
f e
z
⎤
⎦
(3)
The force command f sent to the control algorithm is:
f =
⎡
⎣
a 0 0
0 b 0
0 0 c
⎤
⎦f b
(4)
where we have to respect the following conditions:
a b c Condition
1 0 0 |fx| > |fy| && |fx| > |fz|
0 1 0 |fy| > |fx| && |fy| > |fz|
0 0 1 |fz| > |fx| && |fz| > |fy|

End-Eﬀector Precise Hand-Guiding for Collaborative Robots
599
2.2
Hand-Guiding Moment
For end-eﬀector orientation (2nd and 3rd motion group) the hand-guiding
moment me shall be calculated from the end-eﬀector force/torque measurements:
me =
⎡
⎣
τ e
x
τ e
y
τ e
z
⎤
⎦−
⎡
⎣
μe
x
μe
y
μe
z
⎤
⎦
(5)
where

τ e
x, τ e
y, τ e
z
	
are measurements of the three components of torques from
the end-eﬀector force/torque measurements, and

μe
x, μe
y, μe
z
	
are components of
moment due to weight of end-eﬀector described in end eﬀector frame:
⎡
⎣
μe
x
μe
y
μe
z
⎤
⎦=
⎡
⎣
0
−ze
c
ye
c
ze
c
0
−xe
c
−ye
c xe
c
0
⎤
⎦Re
b
⎡
⎣
0
0
w
⎤
⎦
(6)
where (xe
c, ye
c, ze
c) are the coordinates of the center of mass of the end eﬀector,
described in the end-eﬀector reference frame.
In the 2nd motion group, the end-eﬀector axis zeef is oriented in space, Fig. 1.
For this type of motion the input to the controller is calculated from the hand-
guiding moment. The input command is the vector mxy. To calculate this vector,
the vector me
xy shall be calculated ﬁrst, where it represents the component of
the hand-guiding moment in the XY plane of the end-eﬀector frame, Fig. 2. The
components of vector me
xy are described in sensor reference frame. This vector
is calculated from:
me
xy =
⎡
⎣
1 0 0
0 1 0
0 0 0
⎤
⎦me
(7)
The input command for 2nd motion group is calculated from:
mxy = Rb
eme
xy
(8)
where mxy represents the vector me
xy after being transformed to base frame.
For the 3rd motion group, the end-eﬀector is allowed to rotate around its
axis zeef. For this type of motion the input command to the controller is the
vector me
z. To calculate this vector, the vector me
z shall be calculated ﬁrst, where
it represents the Z direction of the end eﬀector frame, as shown in the Fig. 3.
This vector is calculated from:
me
z = me −me
xy
(9)
The controller command for the 3rd motion group is calculated from:
mz = Rb
eme
z
(10)
where mz represents the vector me
z after being transformed to base frame.

600
M. Safeea et al.
Fig. 2. Hand-guiding moment in sensor reference frame for 2nd motion group (at left)
and for the 3rd motion group (at right)
Fig. 3. Damper-mass mechanical system
Only one of motion group (2nd and 3rd motion group) is allowed to be
performed once at a time. The motion command vector sent to the control
algorithm m is calculated from:
m =

mxy if ∥mxy∥> ∥mz∥
mz if ∥mz∥> ∥mxy∥
(11)
3
Controller
The robot is controlled at the end-eﬀector level, so that we consider the decoupled
end-eﬀector as a mass moving under the eﬀect of Coulomb and viscous friction.
The equation of linear motion of the center of gravity of the mass moving is:
m¨x + b ˙x + fr = f
(12)
where ¨x is the linear acceleration of the center of mass, ˙x is the linear velocity
of the center of mass, m is the mass, b is the damping coeﬃcient, fr is Coulomb

End-Eﬀector Precise Hand-Guiding for Collaborative Robots
601
friction, and f is the external force acting on the mass. The equation of angular
rotations around the center of gravity of the mass is:
i¨θ + β ˙θ + τr = τ
(13)
where ¨θ is the angular acceleration around rotation axes, ˙θ is the angular velocity,
i is the moment of inertia around the rotation axes, β is the damping coeﬃcient,
τr is the torque due to Coulomb friction, and τ is the external torque.
We consider that the Coulomb and viscous friction eﬀects are much bigger
than the eﬀect of the inertia of the mass. In this context, the inertial factors
from previous equations are omitted (Fig. 4) so that equation (12) becomes:
b ˙x + fr = f
|f| > |fr|
(14)
and:
˙x = 0
otherwise
(15)
where |f| is the absolute value of the external force.
Equation (13) becomes:
β ˙θ + τr = τ
|τ| > |τr|
(16)
and:
˙θ = 0
otherwise
(17)
where |τ| is the absolute value of the external torque.
3.1
Robot Control
The linear motion of the end-eﬀector is controlled by:
v =

f(∥f∥−∥fr∥)
b∥f∥
∥f∥> ∥fr∥
0
otherwise
(18)
where v is the linear velocity vector of the end-eﬀector, f is the force vector
command issued to the controller, fr is the sensitivity threshold (motion is only
executed when the force command reaches a value above this threshold), and b is
the motion constant that deﬁnes the rate of conversion from force measurement
to velocity.
The angular motion of the end-eﬀector is controlled by:
ϖ =

m(∥m∥−∥τr∥)
β∥m∥
∥m∥> ∥τr∥
0
otherwise
(19)
where ϖ is the angular velocity vector of the end-eﬀector, m is the moment
command issued to the controller, τr is the sensitivity threshold (where motion
is only valid when the force command value is higher than this threshold), and β

602
M. Safeea et al.
is a motion constant that deﬁnes the rate of conversion from force measurement
to velocity.
The end eﬀector velocity ˙x is:
˙x =

v
ϖ

(20)
After calculating the velocity of the end-eﬀector, angular and linear, joint
velocities are calculated using the pseudo inverse of the Jacobean J†:
˙q = J† ˙x
(21)
From the angular velocities and by using an iterative solution, the state space
vector can be calculated, which is used to control the manipulator, Fig. 4.
Fig. 4. Robot control algorithm
4
Experiments and Results
The experiments were performed in a KUKA iiwa 7 R800 robotic manipulator in
an assembly operation requiring precision positioning and ﬁne tuning. The oﬀ-
the-shelf hand-guiding functionality provided by KUKA lacks in the precision

End-Eﬀector Precise Hand-Guiding for Collaborative Robots
603
that is crucial for ﬁne and precise positioning of the end-eﬀector for many appli-
cations. The proposed method was implemented in a control loop updated at
each 8.5 ms. The controller compensates automatically for the piece weight such
that when no force is applied by the operator, the piece is held in place by the
robot. Diﬀerent tests demonstrated that in the 1st motion group the robot posi-
tion along X, Y and Z can be precisely adjusted, in the 2nd motion group the
end-eﬀector rotation around X, Y and Z is achieved, while in the 3rd motion
group we can rotate around the end-eﬀector axis with accuracy, Fig. 5. Figure 6
shows the end-eﬀector position along Y axis according to a given applied force.
It is clear to visualize the end-eﬀector displacement when the force is applied.
The proposed hand-guiding method accuracy was measured and compared with
a nominal path in plane xy, Fig. 7. The operator moves the end-eﬀector along
the y axis (250 mm) so that it was achieved a maximum error of 0.09 mm along x
Fig. 5. Precision hand-guiding for assembly operation
Fig. 6. End-eﬀector position along y axis according to applied force

604
M. Safeea et al.
Fig. 7. Nominal end-eﬀector path against the real robot end-eﬀector path in plane xy
direction. In summary, it was demonstrated that the precision hand-guiding abil-
ities of the proposed method work well and are useful in collaborative robotics.
5
Conclusion and Future Work
A novel precision hand-guiding method at end-eﬀector level for collaborative
robots was proposed. Experimental tests demonstrated that with the proposed
method it is possible to hand-guide the robot with accuracy, with no vibration,
and in a natural way by taking advantage of the proposed three motion groups. It
was also demonstrated that the system is intuitive and compares favorably with
oﬀ-the-shelf KUKA hand-guiding. Future work will be focused on optimizing
the control by utilizing the redundancy of the KUKA iiwa for achieving better
stiﬀness while hand-guiding, and also for avoiding collisions with obstacles while
hand-guiding. In addition the control can be optimized to give some feedback to
the user when reaching near the joint limits.
Acknowledgments. This research was partially supported by Portugal 2020 project
DM4Manufacturing POCI-01-0145-FEDER-016418 by UE/FEDER through the pro-
gram COMPETE 2020, the European Unions Horizon 2020 research and innovation
programme under grant agreement No 688807 - ColRobot project, and the Portuguese
Foundation for Science and Technology (FCT) SFRH/BD/131091/2017.

End-Eﬀector Precise Hand-Guiding for Collaborative Robots
605
References
1. Neto, P., Mendes, N.: Direct oﬀ-line robot programming via a common CAD pack-
age. Robot. Auton. Syst. 61(8), 896–910 (2013). doi:10.1016/j.robot.2013.02.005
2. Neto, P., Pereira, D., Pires, J.N., Moreira, A.P.: Real-time and continuous hand ges-
ture spotting: an approach based on artiﬁcial neural networks. In: 2013 IEEE Inter-
national Conference on Robotics and Automation (ICRA), pp. 178–183 (2013).
doi:10.1109/ICRA.2013.6630573
3. Simao, M.A., Neto, P., Gibaru, O.: Unsupervised gesture segmentation by motion
detection of a real-time data stream. IEEE Trans. Ind. Inform. 13(2), 473–481
(2017). doi:10.1109/TII.2016.2613683
4. Haddadin, S., Albu-Schaﬀer, A., Hirzinger, G.: Requirements for safe robots: mea-
surements, analysis and new insights. Int. J. Robot. Res. 28(11/12), 15071527
(2009). doi:10.1177/0278364909343970
5. Balasubramanian, R., Xu, L., Brook, P.D., Smith, J.R., Matsuoka, Y.: Physical
human interactive guidance: identifying grasping principles from human-planned
grasps. IEEE Trans. Robot. 28(4), 899–910 (2012). doi:10.1109/TRO.2012.2189498
6. Whitsell, B., Artemiadis, P.: Physical Human-Robot Interaction (pHRI) in 6 DOF
with asymmetric cooperation. IEEE Access 5, 10834–10845 (2017). doi:10.1109/
ACCESS.2017.2708658
7. Fujii, M., Murakami, H., Sonehara, M.: Study on application of a human-robot
collaborative system using hand-guiding in a production line. IHI Eng. Rev. 49(1),
24–29 (2016)
8. Lee, S.D., Ahn, K.H., Song, J.B.: Torque control based sensorless hand guiding for
direct robot teaching. In: 2016 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pp. 745–75 (2017). doi:10.1109/IROS.2016.7759135
9. Ficuciello, F., Villani, L., Siciliano, B.: Variable impedance control of redundant
manipulators for intuitive human robot physical interaction. IEEE Trans. Robot.
31(4), 850–863 (2015). doi:10.1109/TRO.2015.2430053
10. Kosuge, K., Yoshida, H., Fukuda, T.: Dynamic control for robot-human collabo-
ration. In: Proceedings of the 2nd IEEE International Workshop on Robot and
Human Communication, pp. 398–401 (1993). doi:10.1109/ROMAN.1993.367685
11. Geravand, M., Flacco, F., De Luca, A.: Human-robot physical interaction and
collaboration using an industrial robot with a closed control architecture. In:
2013 IEEE International Conference on Robotics and Automation, pp. 4000–4007
(2013). doi:10.1109/ICRA.2013.6631141
12. Hanses, M., Behrens, R., Elkmann, N.: Hand-guiding robots along predeﬁned geo-
metric paths under hard joint constraints. In: 2016 IEEE 21st International Confer-
ence on Emerging Technologies and Factory Automation (ETFA), pp. 1–5 (2016).
doi:10.1109/ETFA.2016.7733600

Integrating 3D Reconstruction and Virtual
Reality: A New Approach for Immersive
Teleoperation
Francisco Navarro, Javier Fdez, Mario Garz´on(B), Juan Jes´us Rold´an,
and Antonio Barrientos
Centro De Autom´atica y Rob´otica, UPM-CSIC,
Calle Jos´e Guti´errez Abascal, 2,28006 Madrid, Spain
{francisco.navarro.merino,javier.ferfernandez}@alumnos.upm.es,
{ma.garzon,jj.roldan,antonio.barrientos}@upm.es
Abstract. The current state of technology permits very accurate 3D
reconstructions of real scenes acquiring information through quite diﬀer-
ent sensors altogether. A high precision modelling that allows simulat-
ing any element of the environment on virtual interfaces has also been
achieved. This paper illustrates a methodology to correctly model a 3D
reconstructed scene, with either a camera RGB-D or a laser, and how to
integrate and display it in virtual reality environments based on Unity, as
well as a comparison between both results. The main interest regarding
this line of research consists in the automation of all the process from the
map generation to its visualisation with the VR glasses, although this
ﬁrst approach only managed to get results using several programs man-
ually. The long-term objective would be indeed a real-time immersion in
Unity interacting with the scene seen by the camera.
Keywords: Robotics · 3D reconstruction · Virtual reality · Immersive
teleoperation
1
Introduction
The promise of Virtual Reality (VR) lies mainly in its ability to transport a
user into a totally diﬀerent world and make him feel that he is immersed in it.
This immersion generally implies a total separation from the real world, meaning
that the virtual environment rarely, if ever, represents any real-world location.
In contrast with this, several 3D Reconstruction techniques are being developed
in order to create models of the real world, and also in general terms, those
reconstructions rarely include ﬁctional or virtual elements.
The motivation of this work is to combine the beneﬁts of both techniques, and
therefore, create a VR world from a reconstruction of a real location. With this
solution, it will be possible to immerse a user in a virtual representation of a real
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_50

Integrating 3D Reconstruction and Virtual Reality
607
location. For example, it would be possible to see and walk in an remote location,
and if this technology is combined with teleoperation of mobile manipulators,
actually interact with an unaccessible, dangerous or remote location.
1.1
Objectives
In order to achieve the goal of integrating 3D reconstructions into virtual worlds,
the following partial objectives have been deﬁned.
– Reconstruct 3D maps of a location using with two diﬀerent devices and tech-
niques.
– Create a mesh of the reconstruction.
– Contrast the real environment with the virtual maps.
– Integrate the reconstruction in a VR interface.
1.2
Related Works
Nowadays, there are three diﬀerent immersive technologies. First of all, the aug-
mented reality (AR), which overlaps virtual elements on the videos or images of
real scenes. Secondly, the virtual reality (VR), which is based on virtual models
of real or imaginary scenarios. The third technology is a combination of the ﬁrst
two, which is known as mixed reality (MR). Virtual reality is the option in which
this paper has been focused.
On the other hand, in robotics and other ﬁelds like medicine [1] 3D recon-
struction is used as a process of capturing the shape and appearance of real
objects. There are several devices, such as a lidar, RGB-D cameras or sonars
that can display this function. In addition, there are also several programs or
libraries that ﬁlter and reconstruct the point clouds that those devices generate.
Both 3D reconstruction and VR have been subject of study for many years
and there are hundreds of papers published on these topics separately. Never-
theless, the number of publications that connect both subjects is very low. Some
of the existing research work are listed next.
A successful integration of 3D reconstructions of antique objects in a virtual
museum, using a 3D scanner and a high-resolution camera was accomplished
by the University of Calabria [2]. Additionally, Stanford correctly managed to
import a mesh model of a reconstructed scene in Unity using the Kinect One [3].
Lastly, York University succeeded at modelling a 2D mapped trajectory, using a
laser along with wheel odometry, and display it in Unity using a ROS connection
based on Rosbridge [4].
1.3
Overview of the System
Figure 1 illustrates a general view of the process and the main devices and pro-
grams used along with this research. The environment information is acquired
by two diﬀerent devices and it is processed using diverse software tools in order
to obtain a mesh of the scene. Then, these meshes are imported into Unity and
visualised with the virtual reality glasses.

608
F. Navarro et al.
Fig. 1. Overview. This ﬁgure illustrates a general view of the process and the main
devices and programs used along with this research.
1.4
Main Contributions
This research encompasses diﬀerent ideas from previous works, but it also pro-
poses several novel contributions, listed next, that distinguish this research form
previous ones.
– It includes an interface and recreates a real world.
– It compares two diﬀerent 3D reconstructions and their results.
– It uses ROS, allowing to use this same method in real time just by doing a
socket between ROS and Unity.
2
3D Map Generation
This section presents the necessary steps to accomplish a 3D representation of
a real world scenario from two diﬀerent methods. The ﬁrst one processes the
information of a RGB-D camera at real-time while the second one creates the
reconstruction after the data acquisition from a 2D laser scanner.
2.1
Real-Time 3D Reconstruction
This technique uses a Microsoft’s Kinect One (v2) RGB-D camera in order to
acquire enough data from the environment. This device presents an RGB camera
to get colour information as well as a depth sensor, with an eﬀective range of
40–450 cm, to obtain distance estimation from the scene.
Once the environment data is available, colour and depth information are
processed with using the RTAB-map software [5], which uses the OpenCV, QT
and PCL libraries. One of the main advantages of this method is the creation
of a real-time point cloud, mapping the scene as the camera is moving through
the scenario.
Figure 2 shows the ﬁnal point cloud after the desired scene is completely
mapped manipulating this tool and the camera. Additionally, the odometry of
the camera has been overlapped in the ﬁgure for the purpose of illustrating the
mapping trajectory.

Integrating 3D Reconstruction and Virtual Reality
609
Fig. 2. Kinect Point Cloud. This ﬁgure shows the ﬁnal point cloud after the desired
scene is completely mapped manipulating this tool and the camera.
2.2
Oﬀ-line 3D Reconstruction
The second technique, is based on the use of an Hokuyo 2D laser scanner. This
sensor measures the distance to a target by illuminating it with a pulsed laser
light, and measuring the reﬂected pulses with a sensor. Diﬀerences in laser return
phases and wavelengths can be used to make digital 3D-representations of the
target.
Unlike the RGB-D camera, the map reconstruction is not done in real time.
Since the Lidar can only provide distance measurements in a plane, a rotating
mechanism was designed, it rotates the scanner on its x axis in order to get 3D
information. The data is recorded and stored in a rosbag. Then, it is transformed
to point clouds and stored in PDC ﬁles. Those point clouds can be grouped into
one large 3D reconstruction using the libpointmatcher [6] library, that uses an
Iterative Closest Point (ICP) algorithm to estimate the correct transformation
Fig. 3. Lidar Point Cloud. This ﬁgure shows a view of the reconstructed scene with
the Lidar.

610
F. Navarro et al.
between one point cloud and the next. Finally, the result is visualized, ﬁltered
and post-processed with Paraview software [7], as shown in Fig. 3.
3
Mesh Creation
Since the Unity VR engine does not support point clouds or PCD formats, it
is compulsory to use one that allows a correct integration. The mesh format is
the option selected in this work, a mesh is a collection of vertices, edges and
faces that deﬁnes a 3D model. In this research, a polygonal mesh, whose faces
are triangles, will be used. The mesh format was chosen not only because it is
compatible with Unity, but also due to the fact that a solid 3D mesh contains
more useful information than a point cloud. In short, a discrete 3D reconstruction
is converted into a continuous 3D model.
Before creating the mesh, a post-processing phase is applied to the map. It
consists on applying noise reduction and smoothing ﬁlters in order to obtain a
more realistic reconstruction. The ﬁltering can be applied to both on-line and
oﬀ-line maps, and it is based on the use of the Point Cloud Library [8].
Regarding the camera reconstruction, the obtained point cloud presents sev-
eral local and global ﬂaws, which result in a non-consistent 3D map. Firstly, to
reduce these defects, RANSAC algorithm is applied in order to reject outliers.
Once the noisy points are removed, more loop closures are detected along with
the Iterative Closest Point (ICP) algorithm. This step increases the correct iden-
tiﬁcation of features, avoiding mapping an already visited area twice, as well as
improving the alignment of object edges. Figure 4 presents the improved point
cloud with fairly corrected edges and objects.
The main parameters used for the organised meshing are the size and the
angle tolerance of the faces, which are, in this case, triangles. The angle tolerance
was set to 15 for checking whether or not an edge is occluded whereas the faces
Fig. 4. Post-processed map. Point cloud with corrected edges and objects.

Integrating 3D Reconstruction and Virtual Reality
611
size was 2 pixels. It should be pointed out that smaller triangles mean higher
resolution of the resulting mesh.
One of the main concerns during the meshing is the inclusion of textures,
being this step essential for a correct colour display in Unity. So as to accomplish
a complete integration, the triangulated point cloud along with the cameras
used are exported from RTAB-map to MeshLab [9] for a further post-processing
adding the corresponding textures. The ﬁnal camera mesh is shown in Fig. 5.
Fig. 5. Resulting Mesh. Final mesh form RGB-D camera map.
On the other hand, the Lidar mesh creation consists in two steps. First,
a ﬁlter to the point cloud, which reduces the number of points. Then the 3D
triangulation, which is speeded up by the previous ﬁlter.
In the ﬁrst step, the libpointmatcher library [6] was used, which is a modular
library that implements the ICP algorithm for aligning point clouds. Addition-
ally, it has been used the ﬁlter Voxel Grid Filter which down-samples the data
by taking a spatial average of the points in the cloud. The sub-sampling rate is
adjusted by setting the voxel size along each dimension. The set of points, which
lie within the bounds of a voxel, are assigned to that voxel and will be combined
into one output point.
There are two options in order to represent the distribution of points in
a voxel by a single point. In the ﬁrst, the centroid or spatial average of the
point distribution is chosen. In the second, the geometrical centre of the voxel
is taken. For this point cloud, it has been chosen the ﬁrst one for its higher
accuracy. Figure 6a shows the model after the triangulation.
The triangulation has been done with the Point Cloud Library (PCL). This
library includes a function called Greedy Projection Triangulation that creates
the Polygon File Format document. After the triangulation, with the program
MeshLab the ﬁles are converted into DAE or OBJ format so that the model could
b imported into Unity (as it only accepts certain formats). Figure 6 presents the
triangulation and the mesh that has been ﬁnally imported into Unity.

612
F. Navarro et al.
(a) Triangulation
(b) Mesh
Fig. 6. Lidar mesh creation. (a) shows the model after the triangulation. (b) presents
the mesh that has been ﬁnally imported into Unity
3.1
Integration with Unity
For the development of the virtual reality interface, the program selected has
been Unity and, in order to visualise the 3D reconstruction, it has been used the
HTC Vive glasses and the program Steam VR. Unity is a cross-platform game
engine which is primarily used to develop games and simulations for computers,
consoles and mobile devices. The package HTC Vive includes an HTC Vive
headset, two controllers with buttons and two base stations that can locate
objects inside the wok area. Last, Steam VR is a complement for Unity required
to compile and run programs in virtual reality.
In order to see the meshes into Unity, it was necessary to have a mesh and its
textures. These textures are necessary so as to visualise the mesh with colours.
Once the model is imported into Unity, it is possible to create scripts and interact
with it as it can be done with other objects.
The scale and localization of the 3D map can be modiﬁed in order to better
reﬂect the real world, or to allow a better immersion. Also, new elements, real
or virtual can be added to the scene, and it is possible to interact with them
using the Steam VR capabilities.

Integrating 3D Reconstruction and Virtual Reality
613
4
Experiments and Results
This section presents the experiments developed in order to test the proposed
system. Two diﬀerent scenarios, one for each of the mapping techniques have
been used. Their description and the results of the experiments are described
next.
4.1
Test Scenarios
The ﬁrst scenario, where there RGB-D camera was used, is an indoors laboratory
of the Department of Automatics at Technical University of Madrid (UPM).
This room is equipped with sensors for VR, what makes it interesting for an
immersion at the same place of the reconstruction. In addition, this immersion
could ensure an accurate validation of the dimensions and distances mapped of
the room to assess how faithful it is and its quality.
The second scenario, used with the laser reconstruction, took place in an
olive ﬁeld located in the Spanish province of Toledo. Unlike the previous case,
this experiment was done in an outdoor environment so that it was possible
to get other results and compare them. It is important to mention that the
test was done rotating a base connected to the Lidar in order to be able to
reconstruct 360◦.
4.2
Analysis and Results
One of the drawbacks regarding of the Kinect RGB-D mapping is its inability
to reliably operate outdoors, the reason for this is that the sensor can not deal
with direct and intense lightning. This factor limited the research and comparison
between both methods, being the camera reconstruction obliged to take place
indoors.
Focusing on the ﬁnal mesh obtained, which is shown in Fig. 7b, compared to
the real world, shown Fig. 7a, the global outcome is considered satisfying. The
vast majority of its objects are easily identiﬁed, and they have adequate shapes
and sizes. However, although the overall result from this RTAB-Map reconstruc-
tion is very good and usable, it presents some local defects that occurred during
the mapping stage. For example, most objects on the table are not correctly
reconstructed because they were not on the camera ﬁeld of vision. Moving on
to the Unity integration and immersion, as a consequence of reconstructing the
same room containing the VR equipment, the process of calibrating and validat-
ing the mesh was straightforward.
In the second scenario, diﬀerent results and conclusions were found. First of
all, as shown in Fig. 8, there is an area in the middle of the point cloud that
does not have any points. That is caused due to the structure that rotates the
lidar, which can not reach the full range and consequently leaves an area without
information. Furthermore, even tough this is not relevant from a visual point of
view, it causes many problems when creating the mesh reconstruction, because
of the discontinuities found.

614
F. Navarro et al.
(a) Real World
(b) Virtual World
Fig. 7. Kinect 3D reconstruction in Unity. (a) presents the real room and one of
the experiments. (b) illustrates the view of each eye of the virtual glasses.
Another important result of the use of lidar is that the quality of the virtual
map is worse than other created with other tools. This results from the fact
that the point cloud generated by the lidar does not have colour. Therefore, it is
not possible to import any textures in the virtual interface. However, the main
advantage is that it is able to reconstruct outdoor worlds, what makes that tool
really useful. Figure 8 displays the visualization of the mesh reconstructed from
the Lidar.
Finally, it is important to mention that there are two main diﬃculties in
the developing of the complete process. Firstly, the generation of the textures
in MeshLab and the point cloud triangulation imply a lot of time consumption.
Secondly, a large part of the process is manual which could hamper the future
automation of the method.

Integrating 3D Reconstruction and Virtual Reality
615
Fig. 8. Lidar 3D reconstruction in Unity. This ﬁgure displays the visualization of
the mesh reconstructed from the Lidar.
4.3
Comparison
The main diﬀerences observed objectively between both methods rely heavily on
two diﬀerent items, the sensor used and the scene mapped.
Regarding the sensor, the results with the camera are of a higher quality
due to the fact of colour information acquisition, along with the infrared depth
sensor. However, the laser is limited in this kind of reconstruction having to
rotate to correctly map the environment. Lastly, the ﬁeld of view of the laser
is much longer and wider, with a range of 30 m and an ﬁeld of view of more
than 180◦.
Referring to the scene reconstructed, the real-time mapping was done indoors
while the post-created reconstruction was achieved outdoors. Here the longer
range of the laser scanner, and its ability of handling sun light and other exterior
conditions, make it a better solution for the reconstruction of the scene, since
the RGB-D camera suﬀers in adverse outdoors conditions.
5
Conclusions
This work correctly integrated 3D reconstructions into virtual worlds. For this
purpose, two maps were reconstructed using two diﬀerent devices: Camera
RGB-D and Lidar. Both of them worked in a diﬀerent way since the ﬁrst one
managed to reconstruct with colour an indoor environment, whereas the second
could not. However, the laser worked satisfactorily in outdoor scenes.
Regarding the mesh creation, it turned out to be an adequate approach
not only because of a valid format for its integration into Unity but also for
improving the overall map resulting in a more accurate, continuous and faithful
3D representation.

616
F. Navarro et al.
Another necessary task was to contrast the virtual and real world because it
was essential to know if it was possible to identify the elements from the virtual
sight. Fortunately, even if there were parts that were not positioned exactly in
their pretended place, the virtual map allowed us to distinguish and recognise
the objects. It is important to mention the fact that this occurred in both maps.
Finally, future works will continue this line of research, trying to improve
the reconstruction of the map and the performance of the process. Furthermore,
next eﬀorts will be focused on the reconstruction and integration in Unity in real
time since the long-term objective is to be able to reconstruct an environment
while the robot is tracking a path, allowing this aim to see the elements of the
robot and interact with them from the distance thanks to the virtual reality.
Acknowledgments. This work was partially supported by the Robotics and Cyber-
netics Group at Universidad Polit´ecnica de Madrid (Spain), and it was funded under
the projects: PRIC (Proteccin Robotizada de Infraestructuras Crticas; DPI2014-
56985-R), sponsored by the Spanish Ministry of Economy and Competitiveness and
RoboCity2030-III-CM (Robtica aplicada a la mejora de la calidad de vida de los ciu-
dadanos. fase III; S2013/MIT-2748), funded by Programas de Actividades I+D en la
Comunidad de Madrid and cofunded by Structural Founds of the EU.
References
1. Cong, V., Linh, H.: 3D medical image reconstruction. Biomedical Engineering
Department Faculty of Applied Science, HCMC University of Technology, pp. 1–5
(2002)
2. Bruno, F., Bruno, S., De Sensi, G., Luchi, M., Mancuso, S., Muzzupappa, M.: From
3d reconstruction to virtual reality: a complete methodology for digital archaeolog-
ical exhibition. J. Cult. Heritage 11, 42–49 (2010)
3. Cazamias, J., Raj, A.: Virtualized reality using depth camera point clouds. Stanford
EE 267: Virtual Reality, Course Report (2016)
4. Codd-Downey, R., Forooshani, P., Speers, A., Wang, H., Jenkin, M.: From ROS to
unity: leveraging robot and virtual environment middleware for immersive teleop-
eration. In: 2014 IEEE International Conference on Information and Automation,
ICIA 2014, pp. 932–936 (2014)
5. Labb, M., Michaud, F.: Online global loop closure detection for large-scale multi-
session graph-based slam. In: IEEE International Conference on Intelligent Robots
and Systems, pp. 2661–2666 (2014)
6. Pomerleau, F., Colas, F., Siegwart, R., Magnenat, S.: Comparing ICP variants on
real-world data sets. Autonom. Robot. 34(3), 133–148 (2013)
7. Ahrnes, J., Geveci, B., Law, C.: ParaView: an end-user tool for large-data visualiza-
tion. In: Hansen, C.D., Johnson, C.R. (eds.) Visualization Handbook, pp. 717–731.
Butterworth-Heinemann, Burlington (2005)
8. Rusu, R., Cousins, S.: 3D is here: point cloud library. In: IEEE International Con-
ference on Robotics and Automation, pp. 1–4 (2011)
9. Cignoni, P., Callieri, M., Corsini, M., Dellepiane, M., Ganovelli, F., Ranzuglia,
G.: MeshLab: an open-source mesh processing tool. In: Sixth Eurographics Italian
Chapter Conference, pp. 129–136 (2008)

Enhancement of Industrial Logistic Systems
with Semantic 3D Representations
for Mobile Manipulators
C´esar Toscano1, Rafael Arrais1(B), and Germano Veiga1,2
1 INESC TEC, R. Dr. Roberto Frias, 4200-465 Porto, Portugal
{ctoscano,rafael.l.arrais,germano.veiga}@inesctec.pt
2 Faculty of Engineering, University of Porto,
R. Dr. Roberto Frias, 4200-465 Porto, Portugal
Abstract. This paper proposes a logistic planner with supplementary
3D spatial representations to enhance and interact with traditional logis-
tic systems on the context of mobile manipulators performing internal
logistics operations. By deﬁning a hierarchical structure, the logistic
world model, as the central entity synchronized between multiple sys-
tem components, the reliability and accuracy of the logistic system is
strengthened. The proposed approach aims at implementing a robust
and intuitive solution for the set-up of mobile manipulator based logistic
systems. The logistic planner includes a web based interface for fast setup
of the warehouse layout based on robot sensing, as well as the deﬁnition
of missions for the ﬂeet of robotic systems.
Keywords: Mobile manipulators · Cyber-physical systems · World
model · Logistic system · Service-oriented architectures · Vertical
integration
1
Introduction
With the advent of ﬂexible robots, able to sense and interact with increasingly
complex and unstructured industrial environments, there is a crucial need to
correctly identify, model and organize all their physical objects and functional
components, in order to successfully support robotic operations.
Vertical integration between Enterprise Information Systems (EIS) (e.g.
Manufacturing Execution System (MES)) and robotic systems, operating in com-
plex industrial environments, present great development challenges. One example
is the diﬃculty to reliably model and disseminate accurate information on the
pose of objects and physical components from the environment.
To support robot operation and to facilitate vertical integration, there is
an implicit need to conceive a dynamic and scalable structure to cope with
the necessity to model spatial properties of all physical objects and functional
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_51

618
C. Toscano et al.
components in a complex industrial environment. This structure can be seen as
an enhancement of traditional logistic systems, in the sense that it combines
semantic data with geometric information updated in real time.
This paper proposes a dynamic hierarchical structure, the logistic world
model, to augment traditional logistic data with 3D spatial information. Thus,
the logistic world model holds semantic data, spatial location, geometric dimen-
sions, and internal structure of every physical object present in the environment.
This dynamic structure provides support for the operation of mobile manipula-
tors and acts as the principal information provider to the planning, execution
and monitoring features of the logistic system.
The logistic world model structure exists and is maintained within a central
node, the logistic planner, responsible for orchestrating operations on the robotic
system and providing interfaces to handle software components interactions. The
logistic planner also manages integration with the robotic ﬂeet and the EIS or any
other information management system supporting the production system (e.g.
MES). The communication interface between components was developed with
a Service Oriented Architecture [1] in mind. As such, this approach makes the
functionality implemented by each entity accessible through a service interface.
This work was driven forward by the Sustainable and Reliable Robotics for
Part Handling in Manufacturing Automation (STAMINA) project, funded by
the European Unions Seventh Framework Programme for research, technologi-
cal development and demonstration. The project aimed at developing a mobile
manipulator to perform picking operations in a logistic supermarket located on
a car manufacturing industrial plant.
The purpose of this paper is twofold: on one hand it provides a comprehensive
deﬁnition and analysis of the high level architecture of the system, focusing on
the logistic planner component, which is the orchestration node and central hub
of communication between diﬀerent components of the system; on the other
hand, the logistic world model structure is presented, as a tool for synchronizing
data between diﬀerent system components.
The remainder of the paper is organized as follows: Sect. 2 describes related
work, Sect. 3 presents an overview of the high level software architecture of the
system, Sect. 4 describes the logistic planner component, Sect. 5 details the pro-
posed logistic world model, its creation and usability, and Sect. 6 draws the
conclusions.
2
Related Work
The usage of mobile manipulators in industrial applications have increased in
the last years [2]. Nevertheless, there are some barriers delaying the adoption of
the technology. Due to safety concerns, costs of the solutions, and to the lack
of ﬂexibility of current systems to adapt to production changeovers and layout
reconﬁgurations in a time-eﬃcient manner, it is diﬃcult to achieve the desired
performance to justify the admission of mobile manipulator in many industrial
applications [3]. Moreover, the vertical integration of robotics systems with EIS

Logistic Systems Semantic 3D Representations for Mobile Manipulators
619
is still a research challenge [4]. In this context, a model of the environment
specifying the semantics and geometric form of the physical objects that the
robot may interact with seems to be a ﬂexible and promising approach not only
to have control on its behaviour but also to establish an integration with an EIS
managing a given industrial environment.
On the context of representing 3D spatial data for robotics, the Simulation
Description File (SDF) format, an XML-based speciﬁcation created by the Open
Source Robotics Foundation, allows the deﬁnition of models of objects and envi-
ronments so as to support the simulation of robotic systems. SDF models a
physical environment as a set of world elements, describing scenes through a set
of properties addressing elements such as the ambient light, the sky, shadows,
and fog. A physical object is modelled as a hierarchical set of elements having
a position and orientation relative to a speciﬁed frame. However, the geometric
form of the object is not addressed. In a logistic supermarket the key need is not
only to specify the location of each physical object but also its internal structure
so that one can relate objects, constitute aggregates (e.g., a shelf has several
layers where small boxes are stored) and help robotics achieve their goals in an
easier and quicker way (e.g. to reach a small box contained inside a shelf). Also,
in its present format, SDF purposes are oriented to support the simulation of
the behaviour of a robotic system as a set of rigid bodies connected by joints and
not exactly to support its overall behaviour in a given physical environment.
3
High Level System Architecture
A service-oriented approach [5] was used to design and implement the STAMINA
logistic system. Information ﬂow between components was designed to be deﬁned
by the corresponding exposed services whose functionality are deﬁned through
the service’s Application Program Interface (API). Services were designed to be
stateless, with the objective of maximizing the loose coupling of logistic compo-
nents [6].
Figure 1(a) identiﬁes the components that comprise the STAMINA system
and the corresponding information ﬂow. On the top level, the MES organizes
factory data into object categories (possible types of physical things present
on the logistic area) and kitting orders, providing them to the logistic planner
whenever they are created and/or changed. A crucial function of the logistic
planner is to act as the major information provider to the planning and execution
functions performed by the mission planner and the components inside each
robot, through the logistic world model. The logistic planner will be analysed in
more detail in Sect. 4, and the logistic world model will be presented in Sect. 5.
The mission planner [7] interacts exclusively with the logistic planner and
is responsible for generating mission assignments for individual robots in the
ﬂeet, taking as major input the current state of the logistic system (stated in
the logistic world model) and a set of orders requesting the construction of kits
composed by several parts. These kits (kitting boxes) will be later on transported
to the automotive assembly line. Parts can be stored in small boxes, which are

620
C. Toscano et al.
Robot
Task Manager
Skills Manager
World Model Consistency Checker
Logistic Planner
Mission
Planner
world model
kitting orders
Manufacturing Execution System
object categories
kitting orders
kitting orders
alerts
mission
world model
mission
robot skills
world model
task status
World Model
-object categories
-object instances
-3D volumes
-robot skills
Order Manager
-kitting orders
-missions
-alerts
Ubuntu 14.04.1
Microsoft Windows 10 or Ubuntu 14.04.1
with Java Virtual Machine
Logistic Planner
<<ROS Node>>
ROS Bridge
<<ROS Node>>
Mission Planner
<<ROS Node>>
Task Manager
MongoDB
keeps data on
communicates with
communicates with
communicates with
Internet
Browser
provides user interface on
<<ROS Node>>
World Model
Consistency Checker
communicates with
(a)
(b)
Fig. 1. STAMINA architecture: (a) high level overview of the logistic system, high-
lighting main components and information ﬂow (colors are used to diﬀerentiate between
the two main categories of information elements, World Model and Order Manager).
(b) STAMINA components as ROS nodes (UML Deployment diagram).
placed on shelves, and also in large boxes (pallet style), which are placed on
the ground of the logistic area. According to the assignment, missions (which
may include the construction of one or two kits) are subsequently delivered to
individual robots in the ﬂeet.
Each robotic system in the ﬂeet is composed by an on-board task manager,
skills manager and world model consistency checker software components.
Upon the reception of a mission from the logistic planner, the task manager
[8] component identiﬁes and sequences the tasks that will be needed to fulﬁl
the mission, considering the skills [9] available on the robot. This is handled by
a novel multi-agent planning algorithm, invoked by general-purpose automated
planning techniques. The execution of the skills is coordinated by the skills
manager. The task manager is also responsible for keeping track of the robot’s
progress on its mission, and by sending progress updates to the logistic planner.
The skills manager performs and monitors the execution of skills on the robot.
A skill is composed by three stages [10,11]. The precondition stage checks are
performed to ensure that the conditions necessary for carrying out the requested
skill have been satisﬁed. The execution stage performs the action related to the
skill. Finally, a post-condition stage veriﬁes whether the requested skill achieved
its desired outcome successfully. The results of the assessment stages are reported
to the task manager, which can trigger re-planning, in case of a failure, or request
the execution of the following skill, in case of a success [9].
The world model consistency checker [12] is able to perform a real-time analy-
sis on the reliability of the information stored on the logistic world model (which
is provided to the task manager as the mission’s context). This assessment is
performed by comparing the sensed reality by the robotic ﬂeet with the corre-

Logistic Systems Semantic 3D Representations for Mobile Manipulators
621
sponding virtual representation. The objective of this component is to check for
inconsistencies on the logistic world model and either perform a correction on the
stored information or report the detected situation to the operator responsible
for the logistic supermarket.
4
Logistic Planner
The logistic planner is the coordinator node and the communications hub
between diﬀerent sub-systems, responsible for managing robotic operations and
providing service oriented interfaces for integrating diﬀerent sub-system compo-
nents. The management of the lifecycle of a kitting order, namely its creation,
planning, assignment, execution and monitoring, is addressed by the order man-
ager, a subcomponent within the logistic planner.
At the level of vertical integration, the logistic planner interacts with the
EIS [13] by providing the integration mechanisms to extract and handle data
from its MES and complementary systems, which manage information regarding
the logistic supermarket. Conversion logic with higher level planning functions
deﬁned in Enterprise Resource Planning (ERP) or MES type system comprises
a main goal of the logistic planner, as it hides the remaining sub-system compo-
nents from integration-related issues. In order to provide a more suitable expe-
rience to the responsible operator, the logistic planner was implemented as an
autonomous software component, running as a server on top of a Java virtual
machine. The logistic planner provides a JavaScript and HTML5 user interface
on standard Internet browsers.
Besides mediating between functions performed by the remaining compo-
nents in the system and integrating the logistic activities with the EIS, man-
aging the logistic world model is another feature of the logistic planner. The
logistic world model serves the purpose of providing the robotic ﬂeet data about
the working environment (i.e., the logistic area). As such, information from the
high-level systems is extracted, processed and organized in such fashion that
it is understandable by the logistic planner and, consequently, can be used to
create the logistic world model. Inversely, the semantic and format of data are
converted from the logistic planner back to the EIS in order to report actions
performed or information sensed by the robotic ﬂeet on the logistic supermarket.
The data repository generated and accessed by the logistic planner, such as
the logistic world model, is provided by MongoDB, a document-oriented Data
Base Management System (DBMS). This DBMS was chosen for its ability to
keep data in documents organized as data hierarchies as data on the logistic
world model is hierarchically organized. Additionally, this data base technology
oﬀers high degrees of performance and scalability [14].
Ubuntu 14.04.1 constitutes the operating system hosting the remaining com-
ponents, such as the mission planner, task manager and the world model consis-
tency checker. All of these components are available as Robot Operating System
(ROS) nodes (ROS Indigo release) and are programmed in the C++ language.
In this instance of the STAMINA architecture (see Fig. 1(b)), the services
correspond to ROS services and topics. Access to/from the logistic planner is

622
C. Toscano et al.
made through the ROS Bridge [15]. The ROS Bridge is a ROS node that facili-
tates communications from the ROS world (mission planner, task manager, skill
manager) to other entities running in diﬀerent computational environments. This
bidirectional communication channel is implemented as an HTML 5 web socket.
Thus, the logistic planner communicates with all remaining STAMINA compo-
nents through the ROS Bridge. Service invocations made by the Logistic Planner
are sent to the ROS Bridge and from there to the ROS node running the targeted
service (for example, to the one running the mission planner). The correspond-
ing response from the service is sent to the ROS Bridge and then to the logistic
planner. Inversely, a proxy of the services provided by the logistic planner is
available at the ROS Bridge so that an invocation of this proxy service will be
translated into the invocation of the real service available at the logistic planner.
In terms of communication, internal transactions between the logistic planner
and the ROS Bridge are hidden: the ROS Nodes available in the network are
not aware of the presence of the ROS Bridge. Additionally, the exchanged ROS
messages are translated by the ROS Bridge into the JSON format. The logistic
planner sends and receives JSON messages and the ROS Bridge translates these
messages into a ROS-speciﬁc format.
Figure 2 identiﬁes how the logistic planner is organized in terms of software
implementation. Communication with the logistic planner’s server element is
achieved through the HTTP protocol by invoking RESTfull web services using
the JSON data format. Invocation of these web services, done by both EIS and
the user interface running on an Internet browser, triggers the activation of Con-
trollers and Actions inside the server, which validate the data input and output
and maps JSON data into Java data. The Play framework, a high-productivity
Java and Scala web application framework integrating the components and APIs
currently needed for developing modern web applications, is used to implement
the reactive part of the server.
Fig. 2. Software implementation organization within the logistic planner.
At a lower level, the required functions are encapsulated as Services and
the Spring Data provides the mapping of Java objects into database objects.
Although all the usual DBMS, such as Microsoft SQL Server, Oracle and Sybase,
are supported, MongoDB is the one selected. The Object Model speciﬁes all the
Java objects and relationships corresponding to project speciﬁc structures, such

Logistic Systems Semantic 3D Representations for Mobile Manipulators
623
as kitting orders, missions, and the elements that comprise the logistic world
model.
Besides handling RESTfull requests coming from either the user interface or
the EIS, the logistic planner needs to have active threads in support of inter-
actions with the mission planner and task managers without any human inter-
vention. This mechanism is implemented by AKKA Actors [16], objects that
encapsulate state and behavior and that communicate exclusively by exchang-
ing messages, having their own light-weight thread. In practice, AKKA Actors
are used within the logistic model to control the planning of a set of kitting
orders by the mission planner, their assignment to the task manager and the
execution, monitoring and other minor functions.
5
Logistic World Model
As the planning and execution procedures implies a direct action or assessment
to be made on a physical object or functional component located in the environ-
ment, it is crucial to store information on all the entities located in the logistic
supermarket. The type of information exchanged between system components
mainly concerns the physical objects located in the logistic supermarket, such
as containers (shelves, small/large boxes), parts, packaging elements, kits, con-
veyors and robots. The geometry, internal organization and spatial location of
every physical object in the logistic supermarket are modelled in the logistic
world model with the objective of providing this information to the planning
and execution components.
The logistic world model is modelled through a hierarchical graph, where
each object is represented by a node and the link between two nodes refers to
a containment relationship between two physical things. Figure 3a depicts the
organization of the logistic world model and the types of nodes it may contain.
The root node represents the entire logistic supermarket. This is decomposed into
speciﬁc regions, represented by the child nodes underneath the root node, where
each region is further decomposed into diﬀerent instances of physical objects
such as shelves and large boxes. This decomposition of objects continues up to
the level that is appropriate to enable the operation of robots in the environ-
ment. Since the world model aims to accommodate a multi-robot system, Fig. 3b
shows how the objects on the logistic area are related with the reference robot’s
referential and with the other robots in the ﬂeet.
Each node in the logistic world model contains information specifying the cor-
responding object position, orientation and geometry, in the form of a bounding
volume. For example, bellow a speciﬁc region node there might be nodes to
model kits (containers decomposed into compartments), large boxes (contain-
ers organized in layers that contain parts), and shelves (object decomposed into
levels which contain cells where small boxes can be located).
The bounding volume of each node speciﬁes the volume of the represented
physical object (including the volume of every child node) and its location in the
logistic supermarket, by relating its location with the location of its container

624
C. Toscano et al.
Fig. 3. Logistic world model conceptual representation (cardinality represented
between any two nodes): (a) Implantation hierarchy for the logistic area. (b) Rela-
tionship between each robot’s referential.
object. As most of the objects represented in the logistic world model have the
geometric form of a parallelepiped, it is possible to have a common volume
speciﬁcation by deﬁning a two-dimensional closed polyline extruded by height
along the z-axis, as shown in Fig. 4.
The two-dimensional closed polyline is deﬁned in a local coordinate system
(of the object represented by the node), whose origin is an internal point of
the closed polyline. There is a transformation which relates the origin of the
local coordinate system to the origin of the parent’s coordinate system (i.e. the
container object). The translation component of the transformation is speciﬁed
by a 3D vector while the rotation component is given by a tuple specifying three
rotations (roll, pitch and yaw). The points that specify the polyline are deﬁned
in the clockwise direction. The model also allows a given volume to be empty (i.e.
its closed polyline is not deﬁned), in order to enable nodes that simply provide
a transformation between two coordinate systems.
The construction of the logistic world model in the logistic planner is accom-
plished in two distinct phases that are part of the set up process. The objective
of this set up process is to have a concrete virtual representation of each physical
object present in the logistic area, specifying their characteristics and their 3D
locations.
In the ﬁrst set up phase, all object types are deﬁned through either an inter-
action with the EIS or by using the user interface to manually create their 3D
representations. The outcome of this stage is the identiﬁcation and description
of shelves, large/small boxes, conveyors, parts and kits. Objects are described in
terms of their system identiﬁer, internal organization in terms of levels and cells
within each level, and geometric properties, such as width, depth and height.

Logistic Systems Semantic 3D Representations for Mobile Manipulators
625
Fig. 4. Logistic world model bounding volume concept. The origin of the local coor-
dinate system is related to the origin of the parent’s coordinate system by a spatial
transformation with translational and rotational components.
The second phase is performed in the logistic planner’s user interface by the
technician managing the logistic area. The goal for this stage is to correctly iden-
tify which objects actually exist in the logistic supermarket, which containment
relationship relate some of the objects and where they are located in space.
The main element supporting the second phase is a two-dimensional image of
the logistic supermarket area, provided to the technician by the logistic planner’s
user interface. This two-dimensional image can either be created by the robot
through its Simultaneous Localization and Mapping (SLAM) functionality, or
can be provided by a Computer-Aided Design (CAD) application. In both cases,
visual patterns on the image will indicate to the technician the location and
orientation of each physical object in the logistic supermarket (e.g. shelves and
large boxes) on a 2D editor in the logistic planner. This editor enables the tech-
nician to follow an iterative process where he/she tells the system the position
and orientation of shelf and large box, by using the graphical tool available on
the user interface. Instances of shelves and large boxes are created and dragged
and dropped into the background image on the proper places.
As a ﬁnal step, the technician establishes containment relationships between
the automotive parts and the possible containers (i.e., small/large boxes and
shelves). Also, small boxes are placed on shelves in a transversal or longitudinal
orientation.
After having been constructed by the set up process described above, the
logistic world model is provided to the interested components in the system (e.g.,
mission planner, task manager) throughout appropriate services. Actions having
a physical eﬀect on the logistic supermarket cause the robots (namely, their
task management components) to generate notiﬁcations informing the logistic
planner of changes in the logistic world model by the way of a service (e.g. a
part was picked from a large box and placed on a kitting box). Additionally, and
as a result of the real-time analysis performed by the world model consistency
checker, the logistic world model can also be updated when a robot detects

626
C. Toscano et al.
(a)
(b)
Fig. 5. Example of a logistic world model 3D representation of a PSA - Peugeot Citro¨en
Automobiles S.A. factory, in Rennes, France, given by: (a) the web browser interface;
(b) ROS 3D visualizer, RVIZ.
spatial inconsistencies between the sensed reality and the digital representation
(e.g. no small box is detected in the place stated in the logistic world model). The
logistic planner is responsible for keeping the logistic world model synchronized
between the diﬀerent system components.
Figure 5a is a 3D visualization of an example logistic world model obtained
using the logistic planner web browser interface. The image shown in the back-
ground (grey area) is the SLAM-based image built and retrieved from the robot.
Point (0,0,0) of the robot corresponds to the left bottom corner of the image.
This establishes the initial referential three (X,Y,Z) axes in the logistic world
model. On top of the background image, one can see nine shelves (the bigger
objects) and three large boxes (orange colour). These objects were placed in
these locations by the technician on the set-up phase describe above. The main
purpose of the 3D visualization is to help the technician conﬁrm that the model
is correct after its edition in the 2D editor and to visually follow the progress
of the robot ﬂeet. Figure 5b is the visualization of the same world model by the
ROS three dimensional visualizer, RVIZ.
6
Conclusion
This paper presents a system which provides vertical integration with existing
EIS. A hierarchical structure named logistic world model is proposed to augment
traditional logistic data with 3D spatial information, in particular to store the
pose of objects in the environment. A novel component called the logistic planner
is responsible for creating and updating this spatial information.
The logistic planner can be seen as the orchestrator and major information
provider to the planning and execution of missions performed by each robot
of the ﬂeet. The proposed web user interface provides a robust, reliable and
intuitive approach to specify, plan and monitor the execution of missions carried
out by a robotic ﬂeet.
This work is integrated in the EU project STAMINA, where a mobile manip-
ulator was developed to conduct automatic pick and place operations in a logistic
supermarket. Future work will include the usage and adaptation of the logistic
planner and the logistic world model in other industrial applications.

Logistic Systems Semantic 3D Representations for Mobile Manipulators
627
Acknowledgements. This work is ﬁnanced by the ERDF European Regional Devel-
opment Fund through the Operational Programme for Competitiveness and Interna-
tionalisation - COMPETE 2020 Programme, and by National Funds through the Por-
tuguese funding agency, FCT - Fundao para a Cincia e a Tecnologia, within project
SAICTPAC/0034/2015- POCI-01-0145-FEDER-016418. Project “TEC4Growth - Per-
vasive Intelligence, Enhancers and Proofs of Concept with Industrial Impact/NORTE-
01-0145-FEDER-000020” is ﬁnanced by the North Portugal Regional Operational Pro-
gramme (NORTE 2020), under the PORTUGAL 2020 Partnership Agreement, and
through the European Regional Development Fund (ERDF).
References
1. Perrey, R., Lycett, M.: Service-oriented architecture. In: Proceedings of the 2003
Symposium on Applications and the Internet Workshops, pp. 116–119 (2003).
https://doi.org/10.1109/SAINTW.2003.1210138
2. Bostelman, R., Hong, T., Marvel, J.: Survey of research for performance measure-
ment of mobile manipulators. J. Natl. Inst. Stand. Technol. (2016)
3. Shneier, M., Bostelman, R.: Literature review of mobile robots for manufacturing,
National Institute of Standards and Technology, US Department of Commerce,
May 2015
4. Krueger, V., Chazoule, A., Crosby, M., Lasnier, A., Pedersen, M.R., Rovida, F.,
Nalpantidis, L., Petrick, R., Toscano, C., Veiga, G.: A vertical and cyber-physical
integration of cognitive robots in manufacturing. Proc. IEEE 104(5), 1114–1127
(2016)
5. Bianco, P., Lewis, G., Merson, P., Simanta, S.: Architecting service-oriented sys-
tems, Technical report CMU/SEI-2011-TN-008, Software Engineering Institute,
Carnegie Mellon University, Pittsburgh, PA (2011)
6. Girbea, A., Suciu, C., Nechifor, S., Sisak, F.: Design and implementation of a
service-oriented architecture for the optimization of industrial applications. IEEE
Trans. Ind. Inf. 10(1), 185–196 (2014)
7. Crosby, M., Petrick, R.P.A.: Mission planning for a robot factory ﬂeet. In: IROS
2015 Workshop on Task Planning for Intelligent Robots in Service and Manufac-
turing, Hamburg, Germany (2015)
8. Rovida, F., Kruger, V.: Design and development of a software architecture for
autonomous mobile manipulators in industrial environments. In: 2015 IEEE Inter-
national Conference on Industrial Technology (ICIT), pp. 3288–3295. IEEE (2015)
9. Pedersen, M.R., Nalpantidis, L., Andersen, R.S., Schou, C., Bøgh, S., Kr¨uger, V.,
Madsen, O.: Robot skills for manufacturing: from concept to industrial deployment.
Robot. Comput. Integr. Manuf. 37, 282–291 (2016)
10. Pedersen, M.R., Nalpantidis, L., Bobick, A., Kr¨uger, V.: On the integration of
hardware-abstracted robot skills for use in industrial scenarios. In: 2nd Interna-
tional Workshop on Cognitive Robotics Systems: Replicating Human Actions and
Activities, pp. 1166–1171 (2013)
11. Pedersen, M.R., Herzog, D.L., Kr¨uger, V.: Intuitive skill-level programming of
industrial handling tasks on a mobile manipulator. In: 2014 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (IROS 2014), pp. 4523–4530.
IEEE (2014)
12. Arrais, R., Oliveira, M., Toscano, C., Veiga, G.: A mobile robot based sensing
approach for assessing spatial inconsistencies of a logistic system. J. Manuf. Syst.
43, 129–138 (2017)

628
C. Toscano et al.
13. Niu, N., Da Xu, L., Bi, Z.: Enterprise information systems architecture analysis
and evaluation. IEEE Trans. Ind. Inf. 9(4), 2147–2154 (2013)
14. Parker, Z., Poe, S., Vrbsky, S.V.: Comparing NoSQL MongoDB to an SQL DB.
In: Proceedings of the 51st ACM Southeast Conference, p. 5. ACM (2013)
15. Crick, C., Jay, G., Osentoski, S., Pitzer, B., Jenkins, O.C.: Rosbridge: ROS for
non-ROS users. In: Proceedings of the 15th International Symposium on Robotics
Research (2011)
16. Roestenburg, R., Bakker, R., Williams, R.: Akka in Action. Manning Publications
Co. (2015)

Human Intention Recognition in Flexible
Robotized Warehouses Based on Markov
Decision Processes
Tomislav Petkovi´c(B), Ivan Markovi´c, and Ivan Petrovi´c
Faculty of Electrical Engineering and Computing, University of Zagreb,
Zagreb, Croatia
{tomislav.petkovic2,ivan.markovic,ivan.petrovic}@fer.hr
Abstract. The rapid growth of e-commerce increases the need for larger
warehouses and their automation, thus using robots as assistants to
human workers becomes a priority. In order to operate eﬃciently and
safely, robot assistants or the supervising system should recognize human
intentions. Theory of mind (ToM) is an intuitive conception of other
agents’ mental state, i.e., beliefs and desires, and how they cause behav-
ior. In this paper we present a ToM-based algorithm for human intention
recognition in ﬂexible robotized warehouses. We have placed the ware-
house worker in a simulated 2D environment with three potential goals.
We observe agent’s actions and validate them with respect to the goal
locations using a Markov decision process framework. Those observa-
tions are then processed by the proposed hidden Markov model frame-
work which estimated agent’s desires. We demonstrate that the proposed
framework predicts human warehouse worker’s desires in an intuitive
manner and in the end we discuss the simulation results.
Keywords: Human intention recognition · Markov decision processes ·
Hidden Markov model · Theory of mind
1
Introduction
The European e-commerce turnover managed to increase 13.3% to 455.3e bil-
lion in 2015, compared to the 1.0% growth of general retail in Europe [1]. With
the internationalization of distribution chains, the key for success lies within
eﬃcient logistics, consequently increasing the need for larger warehouses and
their automation. There are many fully automated warehouse systems such as
the Swisslog’s CarryPick Mobile system and Amazon’s Kiva system [2]. They
use movable racks that can be lifted by small, autonomous robots. By bring-
ing the product to the worker, productivity is increased by a factor of two or
more, while simultaneously improving accountability and ﬂexibility [3]. How-
ever, current automation solutions based on strict separation of humans and
robots provide limited operation eﬃciency of large warehouses. Therefore, a new
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_52

630
T. Petkovi´c et al.
integrated paradigm arises where humans and robots will work closely together
and these integrated warehouse models will fundamentally change the way we
use mobile robots in modern warehouses. Besides immediate safety issues, exam-
ple of a challenge such models face, is to estimate worker’s intentions so that the
worker may be assisted and not impeded in his work. Furthermore, if the robot
is not intelligent, but controlled by a supervisory system, the supervisory system
needs to be able to estimate worker’s intentions correctly and control the robots
accordingly, so that the warehouse operation eﬃciency is ensured.
There exists a plethora of challenges in human intention recognition, because
of the subtlety and diversity of human behaviors [4]. Contrary to some more
common quantities, such as the position and velocity, the human intention is
not directly observable and needs to be estimated from human actions. Further-
more, the intentions should be estimated in real-time and overly complicated
models should be avoided. Having that in mind, only the actions with the great-
est inﬂuence on intention perception should be considered. For example, in the
warehouse domain, worker’s orientation and motion have large eﬀect on the goal
intention recognition. On the other hand, observing, e.g., worker’s heart rate
or perspiration could provide very few, if any, information on worker’s inten-
tions. Therefore, such measurements should be avoided in order to reduce model
complexity and ensure real-time operation [4].
Many models addressing the problem of human intention recognition success-
fully emulate human social intelligence using Markov decision processes (MDPs).
The examples of such models can be found in [4], where authors propose frame-
work for estimating pedestrian’s intention to cross the road and in [5] where
authors proposed framework for gesture recognition and robot assisted coﬀee
serving. There are multiple papers from the gaming industry perspective, propos-
ing methods for improving the non-playable character’s assisting eﬃciency [6,7].
An interesting approach is the Bayesian Theory of Mind (BToM) [8] where beliefs
and desires generate actions via abstract causal laws. BToM framework in [8]
observes agent’s actions and estimates agent’s desires to eat at a particular food-
truck. However, though impressive, BToM model does not predict the possibility
of agent’s change of mind during the simulation [9].
In this paper, we propose an algorithm for warehouse worker intention recog-
nition motivated by the BToM approach. We expand the BToM to accommodate
the warehouse scenario problem and we present formal and mathematical details
of the constructed MDP framework. The warehouse worker is placed in a sim-
ulated 2D warehouse environment with multiple potential goals, i.e., warehouse
items that need to be picked. The worker’s actions, moving and turning, are
validated based on the proposed MDP framework. Actions, resulting in motion
towards the goal, yield greater values than those resulting in moving away from
the goal. We introduce worker’s intention recognition algorithm based on the hid-
den Markov model (HMM) framework similarly to those presented in [10,11].
The proposed intention recognition algorithm observes action values generated
by the MDP and estimates potential goal desires. We have considered worker’s
tendency to change its mind during the simulation, as well as worker’s intention

Human Intention Recognition
631
of leaving the rack. In the end, we demonstrate that the proposed algorithm pre-
dicts human warehouse worker’s desires in an intuitive manner and we discuss
the simulation results.
2
Human Action Validation
In the integrated warehouse environment the worker’s position and orientation
need to be precisely estimated, and in the present paper we assume that these
quantities are readily available. Furthermore, we would like to emphasize that
most of warehouse worker duties, such as sorting and placing materials or items
on racks, unpacking and packing, include a lot of motion which is considered to be
inherently of the stochastic nature. Therefore, we model the worker’s perception
model P(O|S), with O and S representing observation and state, respectively, as
deterministic, and the action model P(S′|S, A), with A representing action, as
stochastic. A paradigm that encompasses uncertainty in agent’s motion and is
suitable for the problem at hand are MDPs [12]. The MDP framework is based
on the idea that transiting to state S yields an immediate reward R. Desirable
states, such as warehouse items worker needs to pick up, have high immediate
reward values, while undesirable states, such as warehouse parts that are of no
immediate interest, have low immediate reward values. The rational worker will
always take actions that will lead to the highest total expected reward and that
value needs to be calculated for each state. Before approaching that problem,
we deﬁne the MDP framework applicable to the warehouse domain. In order
to accomplish that, we have placed the worker (later referred as agent) in a
simulated 2D warehouse environment shown in Fig. 1. The environment is con-
structed using MATLAB R
⃝GUI development environment without predeﬁned
physical interpretation of the map tiles and the map size is chosen arbitrary
to be 20 × 20. There are three potential goals and the shortest path to each
goal is calculated. There are many oﬀ-the-shelf graph search algorithms we can
use to ﬁnd the optimal path to each goal, such as Dijkstra’s algorithm and A∗.
However, if there exist multiple optimal paths to the goal, there are no prede-
ﬁned rules which one to select and the selection depends on the implementation
details. Consider the example scenario with the warehouse worker in Fig. 2. It
is intuitive that the rational worker will tend to follow the green path because
the orange path would require the worker to either take the additional action of
turning or unnatural walking by not looking forward. Having this in mind, we
modify the A∗search algorithm which selects optimal path the agent currently
sees the most. This has been done by introducing the heuristic matrix H using
the Manhattan distance (L1) heuristics as follows:
Hx,y =

|xg −x| + |yg −y| −ϵ,
if the agent sees tile (x,y)
|xg −x| + |yg −y|,
otherwise
(1)
where ϵ is a small value. Subtracting a small value from the visible tiles directs the
search in their direction and does not disrupt heuristic’s admissibility. The cost

632
T. Petkovi´c et al.
Fig. 1. Agent (green tile) in simulation environment with three potential agent’s goals
(colored tiles). Unoccupied space is labeled with yellow tiles and occupied space (i.e.
warehouse racks) is labeled with black tiles. The optimal path to each goal is shown
with red dots and red solid lines denote agent’s vision ﬁeld. Visible tiles are colored
white. Red dashed line denotes agent’s current orientation and colored lines indicate
direction of the average orientation of the visible optimal path to each goal calculated
using (2).
of each movement is also modiﬁed in a similar way by subtracting a small value
ϵ from the base movement cost of 1, if the tile is visible. Example of the modiﬁed
A∗search algorithm results can be seen in Fig. 3. The average orientation of the
visible path to each goal is deﬁned as follows:
θgoal =
⎧
⎨
⎩
atan2

N

n=1
sin(θn),
N

n=1
cos(θn)

,
if N > 0
θa + π,
otherwise
(2)
where N is the number of visible optimal path tiles, θa is agent’s orientation and
θn are relative orientations of each visible optimal path tile (x, y) with respect
to the agent (xa, ya):
θn = atan2(y −ya, x −xa).
(3)
We propose a mathematical model for validating agent’s actions based on the
assumption that the rational agent tends to (i) move towards the goal it desires
most by taking the shortest possible path, and (ii) orients in a way to minimize
diﬀerence between its orientation and most desirable goal’s average orientation
of the visible optimal path calculated in (2). The proposed model goal is to
assign large value to the actions compatible with the mentioned assumptions,

Human Intention Recognition
633
Fig. 2. Warehouse worker’s (blue
circle) shortest path to the red goal
is ambiguous because both orange
dashed and green dotted paths are
optimal. The black arrow denotes
worker’s orientation.
(a)
(b)
Fig. 3.
The
proposed
A∗
modiﬁcation
yields diﬀerent optimal paths with the
agent’s orientation change.
and small values to the actions deviating from them. These values will be used
to develop agent’s intention recognition algorithm in the sequel. We can notice
that the introduced validation problem is actually a path planning optimization
problem. Perfectly rational agent will always choose the action with the greatest
value and consequently move towards the goal. We approach the agent’s action
values calculation by introducing the agent’s action validation MDP framework.
We assume that agent’s position and orientation are fully observable and create
the MDP state space S as:
Sx,y,k =
⎡
⎣
x
y
θ
⎤
⎦.
(4)
The agent’s orientation space Θ must be discrete because the MDP frame-
work assumes a ﬁnite number of states. We have modeled Θ to include orienta-
tions divisible with π
4 and it can be arbitrary expanded:
Θ = {0, π
4 , π
2 , 3π
4 , π, 5π
4 , 3π
2 , 7π
4 }.
(5)
The action space A includes actions ‘Up’, ‘Down’, ‘Left’, ‘Right’, ‘Turn Clock-
wise’, ‘Turn Counterclockwise’ and ‘Stay’, labeled in order as follows:
A = (∧, ∨, <, >, R, L, S).
(6)
It has already been stated that the agent’s actions are fully observable but
stochastic. In order to capture stochastic nature of the agent’s movement, we
deﬁne the transition matrix T of agent’s movement:

634
T. Petkovi´c et al.
T =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 −2ϵ
0
ϵ
ϵ
0
0
0
0
1 −2ϵ
ϵ
ϵ
0
0
0
ϵ
ϵ
1 −2ϵ
0
0
0
0
ϵ
ϵ
0
1 −2ϵ
0
0
0
0
0
0
0
1 −ϵ
0
ϵ
0
0
0
0
0
1 −ϵ
ϵ
0
0
0
0
0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
(7)
where element Tij denotes realization probability of the action Aj, if the wanted
action is Ai. Moving actions have small probability 2ϵ of resulting in lateral
movement, and turning actions have small probability ϵ of failing. The value of
the constant ϵ is obtained experimentally and equals to 0.1. If the agent’s action
cannot be completed, because of the occupied space blocking the way, column
responding to the impossible action is added to last column and is set to zero
vector afterwards. We deﬁne three hypotheses, Hi, i = 1 . . . 3, one for each goal
state as follows: “Agent wants to go to the goal i and other potential goals are
treated as unoccupied tiles”. The immediate reward values R for each hypothesis
and state are calculated as follows:
Ri,S′ =

π,
if S′ is the goal state according to the Hi
−(ϵ + |θi −θa|),
otherwise
(8)
where ϵ is a small number and |θi−θa| represents the absolute diﬀerence between
average orientation of the visible path to the goal i and agent’s orientation.
Note that we have taken the angle periodicity into account while calculating the
angle diﬀerence in (8). The goal state is rewarded and other states are punished
proportionally to the orientation diﬀerence. If the agent does not see path to
the goal i, the reward is set to the lowest value, −π which is derived from (3).
One of the most commonly used algorithms for solving the MPD optimal policy
problem is the value iteration algorithm [4], which assigns calculated value to
the each state. The optimal policy is derived by choosing the actions with the
largest expected value gain. The value iteration algorithm iteratively solves the
Bellman’s equation [13] for each hypothesis Hi:
Vj+1(Hi, S) = max
a {

S′
PS,S′(RHi,S′ + γVj(Hi, S′))}
(9)
where S is the current state, S′ adjacent state, and PS,S′ element of the row Ta
in transition matrix T which would cause transitioning from state S to S′. The
algorithm stops once the criteria:

i,k
||Vj(Hi, Sk) −Vj−1(Hi, Sk)|| < η
(10)
is met, where the threshold η is set to 0.01. State values, if the goal state is the
dark yellow (southern) goal and agent’s orientation of 3π
2 , is shown in Fig. 4. The
agent’s behavior consistent with the hypothesis Hi is deﬁned as follows.

Human Intention Recognition
635
Fig. 4. State values for the agent’s orientation of 3π
2 if the goal state is the southern
goal labeled with the red circle.
Deﬁnition 1 (Consistent behavior). If the agent in state S takes the action a
under the hypothesis Hi, with the expected value gain greater or equal than the
expected value gain of the action “Stay”, its behavior is considered consistent
with the hypothesis Hi. Otherwise, its behavior is considered inconsistent with
the hypothesis Hi.
Behavior consistency is an important factor in determining agent’s rationality,
which will be further discussed in next section. While calculating the immediate
rewards and state values has O(n4) complexity and can be time consuming, it
can be done oﬄine, before the simulation start. Optimal action, Π∗(Hi, S), for
each state is the action that maximizes expected value gain and, on the other
hand, the worst possible action, ¯Π∗(Hi, S), is the action that minimizes expected
value gain.
3
Human Intention Recognition
Once the state values VH,S are obtained, model for solving agent’s intention
recognition is introduced. While agent’s actions are fully observable, they depend
on agent’s inner states (desires), which cannot be observed and need to be esti-
mated. We propose framework based on hidden Markov model for solving the
agent’s desires estimation problem. HMMs are especially known for their applica-
tion in temporal pattern recognition such as speech, handwriting, gesture recog-
nition [14] and force analysis [15]. They are an MDP extension including the case
where the observation (agent’s action) is a probabilistic function of the hidden
state (agent’s desires) which cannot be directly observed. We propose a model
with ﬁve hidden states, which is shown in Fig. 5 and listed in Table 1.

636
T. Petkovi´c et al.
Table 1. HMM framework components
Symbol Name
Description
G1
Goal 1
Agent wants to go to the northern goal
G2
Goal 2
Agent wants to go to the eastern goal
G3
Goal 3
Agent wants to go to the southern goal
G?
Unknown goal
Agent’s desires are not certain
Gx
Irrational agent Agent behaves irrationally
Fig. 5. Hidden states and transition probabilities. The used constant values are as
follows: α = 0.2, β = 0.1, γ = 0.65, δ = 0.1.
In order to avoid confusion caused by MDP and HMM frameworks, both
having similar or the same element names, MDP states will be referred as states
and HMM states will be referred to as hidden states, or simply desires. All of
the other similarly named elements in this chapter refer to the HMM unless
stated otherwise. Hidden state G? indicates that the agent behaves consistent
with multiple goal hypotheses and the model cannot decide between them with
enough certainty. On the other hand, hidden state Gx indicates that the agent
behaves inconsistently with every goal hypothesis. This hidden state includes
the cases of the agent being irrational or agent’s desire to go to an a priori
unknown goal. The proposed model cannot distinguish between these cases. The
agent’s change of mind during the simulation is allowed, but with very low
probability. The constant values in Fig. 5 are obtained experimentally and we
introduce HMM transition matrix THMM:

Human Intention Recognition
637
THMM =
⎡
⎢⎢⎢⎢⎣
0.8
0
0
0.2
0
0
0.8
0
0.2
0
0
0
0.8
0.2
0
0.1
0.1
0.1
0.65
0.05
0
0
0
0.1
0.9
⎤
⎥⎥⎥⎥⎦
.
(11)
During the simulation, each agent’s action a ∈A generates a three element
observation vector OS,a, each element belonging to one hypothesis. Observation
vector element OS,a,i is calculated as follows:
OS,a,i =
E[V (Hi, S)|a] −E[V (Hi, S)| ¯Π∗]
E[V (Hi, S)|Π∗] −E[V (Hi, S)| ¯Π∗],
(12)
where E[·] denotes expected value gain. Calculated observations are used to
generate the HMM emission matrix B. The emission matrix is expanded with
each agent’s action (simulation step) with the row B′, where the element B′
i
stores the probability of observing observation vector O from hidden state Si.
Last three observations are averaged and maximum average value φ is selected.
It is used as an indicator if the agent is behaving irrationally. Each expansion
row B′ is calculated as follows:
B′ = ζ ·
⎧
⎨
⎩

tanh(O1) tanh(O2) tanh(O3) tanh(0.55) 0

,
if φ > 0.5

tanh( O1
2 ) tanh( O2
2 ) tanh( O3
2 ) tanh(0.1) tanh(1 −φ)

,
otherwise
(13)
where ζ is a normalizing constant and O is i-th observation vector. The initial
probabilities of agent’s desires are:
Π =

0 0 0 1 0

,
(14)
indicating that the initial state is G?. After each agent’s action, the agent’s
desires are estimated using the Viterbi algorithm [16] which is often used for
solving HMM human intention recognition models [17]. The Viterbi algorithm
outputs the most probable hidden state sequence and the probabilities of each
hidden state in each step. These probabilities are the agent’s desire estimates.
4
Simulation Results
In the previous sections, we have introduced the MDP and HMM frameworks for
modeling human action validation and intention recognition. We have obtained
the model parameters empirically and conducted multiple simulations evaluat-
ing the proposed human intention recognition algorithm. The proposed algo-
rithm is tested in a scenario, where the most important simulation steps are
shown in Fig. 4 and the corresponding desire estimates are shown in Fig. 7. The
starting position is (x1, y1, θ1) = (5, 5, 3π
2 ). The agent behaves consistently with
all the hypotheses and proceeds to the state (x6, y6, θ6) = (8, 5, 0). Because of
the mentioned hypothesis consistency, the desire estimates for all of the goal

638
T. Petkovi´c et al.
(a) Simulation step 2.
(b) Simulation step 3.
(c) Simulation step 6.
(d) Simulation step 7.
(e) Simulation step 12.
(f) Simulation step 13.
(g) Simulation step 18.
(h) Simulation step 25.
(i) Simulation step 31.
Fig. 6. Representative simulation steps (best viewed in color).
states increase. The actions from simulation step 7 to step 12 are consistent only
with the hypothesis H3 which manifests as the steep rise of the P(G3) and fall of
probabilities related to other goal hypotheses. In the step 13, action “Stay” is the
only action consistent with the hypothesis H3 and because the agent chooses the
action “Right”, the P(G3) instantly falls towards the zero and P(G?) and P(G2)
rise. While it might seem obvious that the agent now actually wants to go to the
Goal 2, it has previously chosen actions inconsistent with that hypothesis and
the model initially gives greater probability value to the desire G? than to G2.

Human Intention Recognition
639
Next few steps are consistent with the hypothesis H2 and the P(G2) rises until
the simulation step 18, when it enters steady state of approximately 0.85. The
goal desires will never obtain value of 1 because the element B′
4 is never zero,
thus allowing agent’s change of mind. In the state (x25, y25, θ25) = (16, 7, π
4 )
agent can decide to go to the Goal 1 or Goal 2. However, it chooses to take
the turn towards the dead end in the simulation step 31. The proposed model
recognizes that this behavior is inconsistent with all of the hypotheses and the
P(Gx) steeply rises to value slightly smaller than 1, declaring the agent irrational
(Fig. 7).
5
10
15
20
25
30
0
0.2
0.4
0.6
0.8
1
Simulation step
Agent’s desires estimates
Fig. 7. Hidden state (desires) probabilities. Probabilities of the goal states are colored
according to the goal tile’s color. The unknown goal state probability is colored black
and irrational agent state probability is colored red.
5
Conclusion
In this paper we have proposed a feasible human intention recognition algo-
rithm. Our goal was to estimate the intention of a human worker, i.e., agent,
inside of a robotized warehouse, where we assumed that the agent’s position and
orientation are known, as well as the potential goals. The proposed approach is
based on the Markov decision process, where ﬁrst we run oﬄine the value itera-
tion algorithm for known agent goals and discretized possible agent states. The
resulting state values are then used within the hidden Markov model framework
to generate observations and estimate the ﬁnal probabilities of agent’s intentions.
Simulations have been carried out within a simulated 2D warehouse with three
potential goals, modeling a situation where the human worker should need to
enter the robotized part of the warehouse and pick an item from a rack. Results
suggest that the proposed framework predicts human warehouse worker’s desires
in an intuitive manner and within reasonable expectations.

640
T. Petkovi´c et al.
Acknowledgement. This work has been supported from the European Union’s Hori-
zon 2020 research and innovation programme under grant agreement No 688117
“Safe human-robot interaction in logistic applications for highly ﬂexible warehouses
(SafeLog)” and has been carried out within the activities of the Centre of Research
Excellence for Data Science and Cooperative Systems supported by the Ministry of
Science, Education and Sports of the Republic of Croatia.
References
1. E-commerce Europe. European B2C E-commerce Report 2016 - Facts, Figures,
Infographic & Trends of 2015 and the 2016 Forecast of the European B2C
E-commerce Market of Goods and Services (2016)
2. D’Andrea, R.: Guest editorial: a revolution in the warehouse: a retrospective on
kiva systems and the grand challenges ahead. IEEE Trans. Autom. Sci. Eng. 9(4),
638–639 (2012)
3. Wurman, P.R., D’Andrea, R., Mountz, M.: Coordinating hundreds of cooperative,
autonomous vehicles in warehouses. AI Mag. 29(1), 9 (2008)
4. Bandyopadhyay, T., Won, K.S., Frazzoli, E., Hsu, D., Lee, W.S., Rus, D.: Intention-
aware motion planning. In: Springer Tracts in Advanced Robotics, vol. 86, pp.
475–491 (2013)
5. Lin, H., Chen, W.: Human intention recognition using Markov decision processes.
In: CACS International Automatic Control Conference, (Cacs), pp. 340–343 (2014)
6. Nguyen, T.-H.D., Hsu, D., Lee, W.-S., Leong, T.-Y., Kaelbling, L.P., Lozano-Perez,
T., Grant, A.H.: CAPIR: collaborative action planning with intention recognition,
pp. 61–66 (2012)
7. Fern, A., Tadepalli, P.: A computational decision theory for interactive assistants.
In: Advances in Neural Information Processing Systems 23 (NIPS), pp. 577–585
(2011)
8. Baker, C.L., Tenenbaum, J.B.: Modeling human plan recognition using Bayesian
theory of mind. In: Plan, Activity, and Intent Recognition, pp. 1–24 (2014)
9. Chater, N., Oaksford, M., Hahn, U., Heit, E.: Bayesian models of cognition. Wiley
Interdisc. Rev. Cogn. Sci. 1(6), 811–823 (2010)
10. Wang, Z., Peer, A., Buss, M.: An HMM approach to realistic haptic Human-Robot
interaction. In: Proceedings - 3rd Joint EuroHaptics Conference and Symposium
on Haptic Interfaces for Virtual Environment and Teleoperator Systems, World
Haptics 2009, pp. 374–379 (2009)
11. He, L., Zong, C., Wang, C.: Driving intention recognition and behaviour prediction
based on a double-layer hidden Markov model. J. Zhejiang Univ. Sci. C 13(3), 208–
217 (2012)
12. Thrun, S., Burgard, W., Fox, D.: Probabilistic Robotics (1999)
13. Bellman, R.: A Markovian decision process. J. Math. Mech. 6, 679–684 (1957)
14. Rabiner, L.R.: A tutorial on hidden Markov models and selected applications in
speech recognition. Proc. IEEE 77(2), 257–286 (1989)
15. Chen, C.S., Xu, Y., Yang, J.: Human action learning via Hidden Markov Model.
IEEE Trans. Syst. Man Cybern. Part A Syst. Hum. 27(1), 34–44 (1997)
16. Forney Jr., G.D.: The Viterbi algorithm. Proc. IEEE 61(3), 268–278 (1973)
17. Zhu, C., Cheng, Q., Sheng, W.: Human intention recognition in smart assisted
living systems using a hierarchical Hidden Markov Model. In: IEEE International
Conference on Automation Science and Engineering, pp. 253–258 (2008)

Robotics and Cyber-Physical Systems
for Industry 4.0 (II)

Dynamic Collision Avoidance System
for a Manipulator Based on RGB-D Data
Thadeu Brito1,2(B), Jose Lima2,3, Pedro Costa3,4, and Luis Piardi1,2
1 Federal University of Technology - Paran´a, Apucarana, Brazil
thadeu brito@hotmail.com, luis piardi@outlook.com
2 Polytechnic Institute of Bragan¸ca, Bragan¸ca, Portugal
jllima@ipb.pt
3 Faculty of Engineering of University of Porto, Porto, Portugal
pedrogc@fe.up.pt
4 INESC-TEC, Centre for Robotics in Industry and Intelligent Systems,
Porto, Portugal
Abstract. The new paradigms of Industry 4.0 demand the collabora-
tion between robot and humans. They could help and collaborate each
other without any additional safety unlike other manipulators. The robot
should have the ability of acquire the environment and plan (or re-plan)
on-the-ﬂy the movement avoiding the obstacles and people. This paper
proposes a system that acquires the environment space, based on a kinect
sensor, performs the path planning of a UR5 manipulator for pick and
place tasks while avoiding the objects, based on the point cloud from
kinect. Results allow to validate the proposed system.
Keywords: Collaborative
robots ·
Manipulator
path
planning ·
Collision avoidance · RGB-D
1
Introduction
One of the most important task in Industry 4.0 related to cooperation is the
ability to estimate and avoidance of collision for a robot manipulator. Collabo-
rative robotics is a topic addressed in Industry 4.0 where humans and robots can
share and help each other in a cooperative way. Collaborative robot can be used
without any additional safety unlike other manipulators. This means, the robot
should have the capacity of acquire the environment and plan the movement
avoiding the obstacles and people. The cooperation between human and robot
requires that the robot could re-plan the path to reach the target position that
avoids the collision with human parts and obstacles in real time, this means on
the ﬂy, while the arm is moving. Such process can be called dynamic collisions
avoidance.
Nowadays RGB-D sensors help this environment acquisition and perception
so that the system can do the path planning with constraints. The depth cameras
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_53

644
T. Brito et al.
are increasing its popularity and decreasing its prices. The well-known kinect
sensor is an example of that. This paper proposes a system that acquires the
environment space, based on a kinect sensor, performs the path planning of a
UR5 manipulator while avoiding the objects. Two algorithms were tested in
real acquired situations with a simulated UR5 robot and the results point the
advantages for this approach.
2
Related Work
An important step to be considered when developing the manipulator system is
the path planning. Path planning is a key area of robotics. It comprises planning
algorithms, conﬁguration space discretization strategies and related constraints.
It is well known that path planning for robots with many degrees of freedom
is a complex task. Barraquand and Latombe [1], in 1991, proposed a new app-
roach to robot path planning that consisted of building and searching a graph
connecting the local minima of the potential function deﬁned over the robot’s
conﬁguration space. This new approach was proposed considering robots with
multiple degrees of freedom. Later Ralli and Hirzinger [2] reﬁned that same
algorithm accelerating the system, calculating solutions with a lower estimated
executing time. Probabilistic methods were introduced by Kavraki et al. [3] with
the objective to reduce the conﬁguration free space complexity. This method is
not adapted for dynamic environments since a change in the environment causes
the reconstruction of the whole graph. Several variants of these methods were
proposed: Visibility based PRM [4], Medial axis PRM [5], Lazy PRM [6] and
sampling based roadmap of trees [7]. Other methods are used and Helguera et al.
used a local method to plan paths for manipulator robots and solved the local
minima problem by making a search in a graph describing the local environ-
ment using and A* algorithm until the local minima are avoided [8]. The path
planning becomes more complex when there are inserted obstacles in a given
environment. Blackmore and Williams in 2006 presents a complete algorithm by
posing the problem as disjunctive programming. They are able to use existing
constrained optimization methods to generate optimal trajectories for manipula-
tor path planning with obstacles [9]. Path planning in real-time is introduced by
Samir et al. in 2006 in the dynamic environment. This approach is based on the
constraints method coupled with a procedure to avoid local minima by bypass-
ing obstacles using a boundary following strategy [10] More recent Tavares et al.
use a double A* algorithm for multiple industrial manipulators. This approach
uses one A* algorithm to approach the target and an another A* more reﬁning
to reduce the error [11].
3
System Architecture
Many eﬀorts have been done to achieve a system that acquires the environment
by means of an RGB-D sensor, planning a way for an UR5 manipulator to reach
its end point with the ability to avoid obstacles. Figure 1, presents a simpliﬁed
block diagram of the system.

Dynamic Collision Avoidance System for a Manipulator
645
Fig. 1. Diagram of system functions. Image adapted from [22].
3.1
ROS
Robotic Operating System (ROS) is a framework that contains a wide range
of use in developing programs for robots. ROS makes interactions between the
functionalities of a robot (sensors, locomotion, vision, navigation and location)
with contribution of libraries and services, facilitating the robotic application.
The philosophy is to make a piece of software that could work in other robots
by making little changes in the code [16]. Several ROS modules are used in the
present work. Next subsections address the main ones.
3.2
Rviz
The ROS framework comes with a great number of powerful tools to help the
user and developer in the process of debugging the code, and detecting problems
with both the hardware and software. This comprises debugging facilities such
as log messages as well as visualization and inspection capabilities which allows
the user to see what is going on in the system easily [16].
Rviz, presented in Fig. 2 is a 3D visualizer to make a virtual simulation of
robotic models in ROS. During the simulation it is possible to create scenarios
with obstacles, to change positions of pose of a robot or to move them through
a virtual “world”. It is also possible to insert sensors, such as Kinect, change
positions of sensors or robots. This way it is conﬁrmed that the application is
ready to be implemented in a real application, avoiding possible problems in real
robots.

646
T. Brito et al.
Fig. 2. Screenshot of a Rviz 3D visualizer with a simple application. On the left a
display panel, in the middle a simulated UR5 on the table and on the right a views
panel.
3.3
RGB-D Sensor
Develop a robotic application requires the use of sensors. There are currently
several types of devices that ROS supports. This package is deﬁned in diﬀer-
ent categories: 2D range ﬁnders, 3D sensors, Pose estimation, Cameras, Sensor
Interfaces, and other ones [20]. The 3D sensor package, contains the RGB-Depth
(RGB-D) sensors such as Kinect.
RGB-D cameras consist of an RGB and a depth sensor that capture color
images along with per-pixel depth information (depth map). These features have
promoted the wide adoption of low-cost RGB-D cameras in numerous at-home
applications, such as body tracking, gait monitoring for tele-rehabilitation, track-
ing of facial expressions, object and gesture recognition among the others [12].
3.4
MoveIt!
MoveIt! is a well-known software for planning mobile manipulation movements,
incorporating the latest advances in motion planning, manipulation, 3D per-
ception, kinematics, control and navigation. It provides an easy-to-use platform
for developing advanced robotics applications, evaluating new robot designs and
building integrated robotics products for industrial, commercial, R&D and other
domains [13]. Figure 3 shows a screenshot of the MoveIt! performing a path
planning.
The main node of this software is the move group that integrates among sev-
eral other tools. A good example of how move group works is the path planning,
where it is necessary to collect information from a point cloud and turn it into
obstacles in the simulation. MoveIt! uses C++ or Phyton language which makes
it easy to establish commands and create interface when viewing some movement

Dynamic Collision Avoidance System for a Manipulator
647
Fig. 3. Screenshot of a MoveIt! with a planned path executed.
in 3D. The algorithms embedded in MoveIt! can be used by many planners:
Open Motion Planning Library (OMPL), Stochastic Trajectory Optimization
for Motion Planning (STOMP), Search-Based Planning Library (SBPL) and
Covariant Hamiltonian Optimization for Motion Planning (CHOMP).
3.5
Camera Calibration
The calibration of consumer-grade depth sensors has been widely investigated
since the release of the ﬁrst-generation Kinect in 2010. Various calibration meth-
ods, particularly for the depth sensor, have been studied by diﬀerent research
groups [14].
The availability of aﬀordable depth sensors in conjunction with common RGB
cameras (even in the same device, e.g. the Microsoft Kinect) provides robots
with a complete and instantaneous representation of both the appearance and
the 3D structure of the current surrounding environment. This type of infor-
mation enables robots to perceive and actively interact with other agents inside
the working environment. To obtain a reliable and accurate measurements, the
intrinsic parameters of each sensors should be precisely calibrated and also the
extrinsic parameters relating the two sensors should be precisely known. The
calibration must be done because there are no integrated sensors able to provide
both color and depth information yet (sensors are separated).
These sensors provide colored point clouds that suﬀer from a non accu-
rate association between depth and RGB data, due to a non perfect alignment
between the camera and the depth sensor. Moreover, depth images suﬀer from a
geometric distortion, typically irregular and position dependent. These devices
are factory calibrated, so each sensor is sold with its own calibration parameter
set stored inside a non-volatile memory. On the other side, the depth distor-
tion is not modeled in the factory calibration. So, a proper calibration method

648
T. Brito et al.
Fig. 4. Calibration procedure to obtain the intrinsic and extrinsic parameters.
for robust robotics applications should precisely estimate the misalignment and
both the systematic and distortion errors [15]. Figure 4 shows the calibration
procedure.
4
Path Planning
Diﬀerent methods of path planning can be exploited in an application of robotic
manipulators. An interesting planner is the OMPL, a library for many trajectory
calculation algorithms. However, to check for collisions, the FCL library (Flexible
Collision Library, included in MoveIt!) is used.
The OMPL planner works with two ways to create a path, one uses dif-
ferential constraints (Control-based planners) and the other establishes a path
through the geometric and kinematic constraints of the system (Geometric Plan-
ners) which is addressed in this paper [23].
A widely used algorithm is the multiple query of scripts created from the
environment, known as PRM (Probabilistic Roadmap Method). These multiple
scripts are based on sampling algorithms, which can have a higher cost frame-
work. Another good algorithm is RRT (Rapidly-exploring Random Trees), that
is very simple to implement: it has low cost of framework and has good outputs
that accomplishes its work by making state trees.
Therefore, with these algorithms it is possible to carry out a path planning
from an initial pose to a ﬁnal pose. The steps to perform this path planning
are indicated in Fig. 1, so the use of a RGB-D image generated by a Kinect can
check for possible obstacles.
Images are introduced into the system through interconnected nodes, which
make the calculations necessary to have non-collision paths between the start
and the goal poses. Finally, the execution of the movement is done, if there is

Dynamic Collision Avoidance System for a Manipulator
649
a trajectory planning without collisions. Otherwise the MoveIt! informs that it
is not possible to carry out the collisions free movement. If the environment
changes at any time, for example if obstacles change places or new obstacles
are inserted into the work environment, the system (OMPL) will recalculate the
trajectory to reach the ﬁnal pose. By this way, the aim of a system of avoiding
collisions dynamically arises, that is, the whole collision avoidance system adapts
the planning routes according to the environment.
5
Results
To verify that the dynamic collision avoiding system works, it was made a lab
simulation with a Kinect sensor and a virtual model of a UR5 manipulator. In
this way, the purpose is to create a scenario with real obstacles to guarantee
the operation of the system. The main idea is to make the Kinect sensor to
create a point cloud of real obstacles and indicates to MoveIt! where the virtual
manipulator can not collide, that is, where the manipulator can move to reach the
goal pose. The working environment for the validation of the dynamic collision
avoidance system was developed with a simple table as the base for one box
that form the real obstacles. The virtual manipulator model has been conﬁgured
to be ﬁxed to the center of the table, so that the manipulator stays between
the box and human, without maintaining contact with them. At this stage the
use of the RGB-D sensor was important to generate the point cloud and to be
able to calibrate the positioning of all the objects. In order to avoid shadow
interference in point cloud generation, the best RGB-D sensor ﬁxture is at the
top of the working environment. In Fig. 5, it is possible to observe the real
working environment and the processes of transformation of this environment
to the MoveIt! as a point cloud in order to perform the perception of the real
Fig. 5. Figure that shows the scenario created, the obstacles in the real world and the
steps to transform the scenario in a way that the software perceives.

650
T. Brito et al.
obstacles and to make a simulation of path planning without collisions in a
virtual model of the UR5 manipulator.
For the MoveIt! to perform the path planning it is necessary to conﬁgure
the algorithm that will do the routes of the manipulator. For the tests, two
algorithms, PRM and RRT, were chosen. Both algorithms were chosen because
they are widely used in path planning, so these algorithms can be inserted into
the dynamic collision avoidance system.
In order to guarantee the consistency of the comparison between two algo-
rithms, the same path planning conﬁguration was performed in the tests of
each algorithm. Only two parameters were changed, planning time and planning
attempt while the other parameters remained with the MoveIt! default conﬁgu-
ration.
The planning time has been set to 10 s. This parameter indicates the time
limit at which the system will take to ﬁnd a path planning. In the parameter
planning attempts was set to 15. This parameter indicates to the system how
many path planning should be done within the set time. In case the system
does not ﬁnd a path planning within the timeout, the system will not move the
manipulator. The same manipulator pose conﬁgurations (start and goal poses)
were used in both algorithms. The start and goal poses are indicated in Fig. 6.
Fig. 6. The state where the UR5 manipulator is transparent is the start pose, as the
state where the manipulator is orange is the goal pose.
Therefore, during each test the algorithm must ﬁnd a route solution within
the time limit and create states (or poses) to realize the trajectory. The state
sequence that is expected for each algorithm to ﬁnd can be visualized in Fig. 7.
The ﬁrst test uses the PRM algorithm. Figure 8 shows the real scenario cre-
ated and the transformation of this scenario to the perception of the software.
It consists of: a point cloud transformed into Octotree (blue and purple boxes),
the planning trail (in Gray) and the remaining points are the point cloud not
considered for simulation.
To ensure that the ﬁrst test does not interfere with the next test, all nodes
have been restarted. The second test, with the RRT algorithm presented in

Dynamic Collision Avoidance System for a Manipulator
651
Fig. 7. Example of a sequence of states forming a trajectory.
Fig. 8. Path planning using the PRM algorithm.
Fig. 9, shows the same scenario created and the trail that the planner choose to
reach the goal pose.
At the end of each path planning performed by the algorithms through
MoveIt!, the time and amount of states (or poses) were used to ﬁnd a route
to the ﬁnal pose. These data were collected and analyzed in graph format, as
shown in Fig. 10. Although both algorithms ﬁnd a certain amount of states to
perform the path planning, it is not necessary to use all found states.
The logs presented in graph format show that the algorithm PRM use all the
time that was conﬁgured to ﬁnd a path planning. This has resulted in a higher
value of pose states. However, the RRT algorithm uses less time to ﬁnd a path
planning solution, resulting in smaller amounts of pose states.

652
T. Brito et al.
Fig. 9. Path planning using the RRT algorithm.
Fig. 10. Results of logs generated by the system.
6
Conclusion and Future Work
In the presented paper a collaborative manipulator and two path planning algo-
rithms were stressed and compared allowing to develop a system that helps and
collaborates with humans, according to the new paradigms of Industry 4.0. The
system uses ROS and a RGB-Depth sensor (Kinect) to acquire the environment
such as objects and humans positions. The implemented system allows to re-plan
the movement avoiding collisions while guaranteeing the execution of operations.
The logs generated by the system, show a diﬀerence between the analyzed algo-
rithms. While RRT uses agility in ﬁnding a solution to plan the path to the goal
pose, the PRM uses all the time it has been assigned to ﬁnd a trajectory. There-
fore the use of these algorithms must be adequate to the objective of the project
in which it is implemented. The simulation of an UR5 robot with acquired point
cloud validates the approach of both algorithms. In future works, it will be pos-
sible to optimize the point cloud for the system to have a faster response to the
introduction of new objects in the working environment of the UR5 robot.

Dynamic Collision Avoidance System for a Manipulator
653
Acknowledgment. Project “TEC4Growth - Pervasive Intelligence, Enhancers and
Proofs
of
Concept
with
Industrial
Impact/NORTE-01-0145-FEDER-000020”
is
ﬁnanced by the North Portugal Regional Operational. Programme (NORTE 2020),
under the PORTUGAL 2020 Partnership Agreement, and through the European
Regional Development Fund (ERDF).
This work is also ﬁnanced by the ERDF – European Regional Development Fund
through the Operational Programme for Competitiveness and Internationalisation -
COMPETE 2020 Programme within project POCI-01-0145-FEDER-006961, and by
National Funds through the FCT – Funda¸cao para a Ciˆencia e a Tecnologia (Portuguese
Foundation for Science and Technology) as part of project UID/EEA/50014/2013.
References
1. Barraquand, J., Latombe, J.C.: Robot motion planning. a distributed representa-
tion approach. Int. J. Robot. Res. 10(6), 628–649 (1991)
2. Ralli, E., Hirzinger, G.: Fast path planning for robot manipulators using numerical
potential ﬁelds in the conﬁguration space, vol. 3, pp. 1922–1929 (1994)
3. Kavraki, L., Svestka, P., Latombe, J.C., Overmars, M.: Probabilistic roadmaps
for path planning in high dimensional conﬁguration spaces. IEEE Trans. Robot.
Autom. 12(4), 566–580 (1996), ISSN: 1042-296X
4. Sim´eon, T., Laumond, J.P., Nissoux, C.: Visibility based probabilistic roadmaps
for motion planning. Adv. Robot. 14(6), 477–494 (2000)
5. Wilmarth, S., Amato, N., Stiller, P.: Maprm: a probabilistic roadmap planner
with sampling on the medial axis of the free space. In: Proceedings of the IEEE
International Conference on Robotics and Automation, pp. 1024–1031 (1999)
6. Bohlin, R., Kavraki, L.E.: Path planning using lazy PRM. In: Proceedings of the
IEEE International Conference on Robotics and Automation, San Fransisco, vol.
1, pp. 521–528 (2000)
7. Plaku, E., Bekris, K.E., Chen, B.Y., Ladd, A.M., Kavraki, L.E.: Sampling based
roadmap of trees for parallel motion planning. IEEE Trans. Rob. 21(4), 597–608
(2005)
8. Helguera, C., Zeghloul, S.: A local-based method for manipulators path planning
in heavy cluttered environments. In: Proceedings of IEEE International Conference
on Robotics and Automation, San Francisco, pp. 3467–3472 (2000)
9. Blackmore, L., Williams, B.: Optimal manipulator path planning with obstacles
using disjunctive programming. In: American Control Conference, Minneapolis
(2006)
10. Lahouar, S., Zeghloul, S., Romdhane, L.: Real-time path planning for multi-DoF
manipulators in dynamic environment. Int. J. Adv. Robot. Syst. 3(2) (2006)
11. Tavares, P., Lima, J., Costa, P., Moreira, A.P.: Multiple manipulators path plan-
ning using double A*. Ind. Robot: Int. J. 43(6), 657–664 (2016). https://doi.org/
10.1108/IR-01-2016-0006
12. Staranowicz, A., Brown, G.R., Morbidi, F., Mariottini, G.L.: Easy-to-use and accu-
rate calibration of RGB-D cameras from spheres. In: Klette, R., Rivera, M., Satoh,
S. (eds.) Image and Video Technology, PSIVT 2013. LNCS, vol. 8333. Springer,
Heidelberg (2014)
13. Chitta, S.: MoveIt!: an introduction. In: Koubaa, A. (ed.) Robot Operating System
(ROS). SCI, vol. 625. Springer, Cham (2016)

654
T. Brito et al.
14. Walid Darwish, W., Tang, S., Wenbin, L., Chen, W.: A new calibration method
for commercial RGB-D sensors. Sensors 17, 1204 (2017). https://doi.org/10.3390/
s17061204
15. Basso, F., Pretto, A., Menegatti, E.: Unsupervised intrinsic and extrinsic cali-
bration of a camera-depth sensor couple. In: Proceedings of IEEE International
Conference on Robotics and Automation (ICRA) (2014)
16. Martinez, A., Fern´andez, E.: Learning ROS for Robotics Programming. Packt Pub-
lishing, Birmingham (2013)
17. Joseph, L.: Mastering ROS for Robotics Programming. Packt Publishing, Birm-
ingham (2015)
18. The
Robotic
Operation
System
Wiki,
http://wiki.ros.org/ROS/Tutorials/
UnderstandingTopics
19. ROS Industrial Training Exercises with version Kinetic, https://github.com/
ros-industrial/industrial training/wiki
20. Sensor supported by ROS, http://wiki.ros.org/Sensors
21. Sucan, I.A., Chitta, S.: MoveIt, http://moveit.ros.org
22. Sucan,
I.A.,
Chitta,
S.:
MoveIt,
http://picknik.io/moveit wiki/index.php?
title=High-level Overview Diagram
23. Sucan, I.A., Moll, M., Kavraki, L.E.: The Open Motion Planning Library. IEEE
Robot. Autom. Mag. (2012), http://ompl.kavrakilab.org

Development of a Dynamic Path
for a Toxic Substances Mapping Mobile
Robot in Industry Environment
Luis Piardi1,2(B), Jos´e Lima2,4, Paulo Costa3,4, and Thadeu Brito1,5
1 Federal University of Technology - Paran´a, Toledo, Brazil
luis piardi@outlook.com, thadeu brito@hotmail.com
2 Polytechnic Institute of Bragan¸ca, Bragan¸ca, Portugal
jllima@ipb.pt
3 Faculty of Engineering of University of Porto, Porto, Portugal
paco@fe.up.pt
4 INESC-TEC, Centre for Robotics in Industry and Intelligent Systems,
Porto, Portugal
5 Federal University of Technology - Paran´a, Campo Mour˜ao, Brazil
Abstract. Some industries have critical areas (dangerous or hazardous)
where the presence of a human must be reduced or avoided. In some cases,
there are areas where humans should be replaced by robots. The present
work uses a robot with diﬀerential drive to scan an environment with
known and unknown obstacles, deﬁned in 3D simulation. It is important
that the robot be able to make the right decisions about its way without
the need of an operator. A solution to this challenge will be presented in
this paper. The control application and its communication module with a
simulator or a real robot are proposed. The robot can perform the scan,
passing through all the waypoints arranged in a grid. The results are
presented, showcasing the robot’s capacity to perform a viable trajectory
without human intervention.
Keywords: Simulation · Mobile robotics · Path planning
1
Introduction
The new paradigms of industry demand the collaboration between robot and
humans. It is desired that both could help each other and also to collaborate. The
impact on Industry 4.0 and Cyber-Physical Systems is a new technical systems
paradigm based on collaboration [1] and will contribute to several areas, such
as, new value chain models in industry, human security, eﬃciency, comfort and
health, among the others. Some industries have critical areas where the presence
of a human must be reduced or avoided. In these cases, there are some danger
or hazardous areas where humans should be replaced by robots.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_54

656
L. Piardi et al.
In the present case, the developed robot should be able to map toxic sub-
stances in an area (replacing humans) and outside that area, the robot should
share the space with humans. This dynamic environment, with moving obstacles
must be handled by the robot.
This paper presents an approach that can be used to scan a dangerous area in
an industry. It is composed by a planning method that routes the path avoiding
the known obstacles and also re-plan, on the ﬂy, avoiding previously unknown
obstacles that are detected by Time oﬀFlight sensors, during the scanning
process. It is also presented a simulation environment that allows to test and
validate the proposed approach and algorithms. Further, these algorithms will
be implemented in a real robot and the toxic mapping task will be done in a
real environment. Due to some conﬁdential issues, the company name, photos
and toxic elements are omitted. Nevertheless, the layout of the mapping area of
the simulation environment is presented in Fig. 1.
Fig. 1. Layout of the scanning area in simulation environment.
The remaining of the paper is organized as follows: after a brief introduction
in this section, the state of the art of path planning is addressed in next section.
Section 3 presents the system architecture where the communication between
diﬀerent modules and the robot model are stressed. The connectivity grid, the
path planning and robot control are presented in Sect. 4 whereas Sect. 5 shows
the results. Finally, Sect. 6 rounds up the paper with conclusions and points
some future work direction.
2
Related Work
This chapter cites references that use techniques related to planning of dynamic
paths of robots, highlighting the resolution of traveling salesman problem. In

Dynamic Path Mapping Robot for Industry Environment
657
addition to the problem of the path planning performed by the robot, it is also
proposed a model of the trajectory that connect the points. These waypoints
followed by the robot, using parametrized curves by polynomials with splines
for the trajectory.
2.1
Planning
The classic travelling salesman problem (TSP), regarding a group of n cities,
where the purpose of this problem is to start the route in city deﬁned, visiting
the other cities only once, and them returning to the ﬁrst city [2]. Considering
the possibility of the existence of several cities, the TSP becomes complex with
(n −1)! possible routes to be calculated. The work of Pereira et al. [3] presents
a problem with characteristics similar to the TSP, where they seek to determine
the most eﬃcient way to collect golf balls scattered through an open ﬁeld using
a robot with diﬀerential drive.
Similar to the TSP is the scanning of a room with toxics substances as
addressed in the present work. Using a connectivity grid with standard space
between each point or distributed points conﬁgurable by user, the robot needs
to ﬁnd an optimal path between the starting and the destination point, crossing
all points of the grid while avoiding repeat points. Moreover, obstacle avoidance
should also be performed.
For robot navigation and scanning, Hirakawa et al. [4] presents research using
the Adaptive Automaton theory to model, identifying and classifying the robot
decision. Our work presents a similar approach, where we seek to ﬁnd the next
movement of the robot in the exploration process, with four options of move-
ments: north, south, east, west. It is possible to modify the decision due to the
existence of unmapped objects, which are detected based on a Time of Flight
sensor (ToF). There is a great deal of eﬀort by the academic community in
researching path planning, where they use diﬀerent approaches such as Rapidly-
exploring Random Tree [5], Road Map [6], Cell Decomposition [7], Potential
Field [8], and others.
2.2
Trajectory
Many works with mobile robotics use smooth curves because they are performed
by robots with non-holomonic constraints. The B´ezier curves are an example.
As presented in the works [6,9,10], the robots can perform the B´ezier curves
autonomously. However, this approach, despite implementing a smooth curve,
does not require the robot to pass over at all points. It is only an approximation
to the points, which is not interesting for the mapping of toxic substances inside
a room or industry.
Magid et al. [8] use potential ﬁeld for path planning. For building a smooth
curve, they use splines in order to pass at all points and avoid obstacles [8].
In our approach, we will use an improved splines approach so that the curve
performed by the robot is smooth and also precise on the waypoints.

658
L. Piardi et al.
3
System Architecture
3.1
Robot
For the present work a model of mobile robot with diﬀerential steering system
was used. It consists of two drive wheels mounted on a common axis, and each
wheel can independently be driven either forward or back-ward. By this way, it
is possible control the speed and rotation of the robot. The real robot was also
developed as presented in right side of Fig. 2 whereas left ﬁgure shows the robot
developed in a realistic simulation system, the SimTwo. Further explanations
about the implementation and use of this software can be seen in [11,12]. The
real robot will be used in future works for the toxic mapping after the validation
through the simulation of the approach presented in this work.
Fig. 2. Model of simulated robot at left and the real robot at right.
3.2
Communication Between the Simulator and the Application
For this project, it was used two diﬀerent environments. The ﬁrst one
(ControlApp) is an application, developed in Lazarus, that interfaces the user
and controls the robot, based on its position (x,y,θ) provided by the second one,
the SimTwo a 3D simulated environment.
The ControlApp has an interface for the user to interact with the robot. It
provides to the operator the speed, position and orientation of the robot in real-
time. Moreover, ControlApp shows an image that represents the environment of
the robot. The interface is shown in the Fig. 3.
The operator can control the robot through two methods. The ﬁrst one, user
can insert the waypoints, clicking with the mouse on the image in the desired
position. The second method computes a grid of waypoints by selecting the dx
and dy as the distance of waypoints in x and y direction. Then, it is automatically
inserted a grid of possible waypoints according to the values input by the user.
The ControlApp is responsible for all processing that involves the robot, such
as calculations for the spline trajectory and planning, calculations for the control
of the speed of the wheels and recalculating a new route when an unmapped
obstacle is detected.

Dynamic Path Mapping Robot for Industry Environment
659
Fig. 3. Interface of Lazarus application, ControlApp.
In the simulation environment, the physical structure of the robot is deﬁned
as width, weight, length, thickness, size and position of the wheels and sen-
sors. The motor and sensors simulation models are addressed in previous works
[13–15].
Figure 4 represents the SimTwo simulation environment for the developed
software interface shown in Fig. 3.
Fig. 4. SimTwo simulation 3D environment.
The communication between both applications is based on a UDP/IP net-
work protocol. A packet encoding the robot position, orientation and distance
sensor data is sent from the SimTwo to the ControlApp, whereas a packet con-
taining the right and left speed wheels is sent from ControlApp to the SimTwo,
as presented in Fig. 5.

660
L. Piardi et al.
Fig. 5. Communication between user, ControlApp and the SimTwo.
4
Application Development
With intention of to replace human presence by robots in high risk environ-
ments (dangerous industrial environment), it is necessary that such robots are
autonomous. Robots must have the ability to check unmapped obstacles and
make eﬃcient decisions without the help of human. On the other hand, the
Human Machine Interface (HMI) is necessary to perform some conﬁgurations,
such as waypoints, grid accuracy, speeds and visualization.
4.1
Connectivity Grid
The connectivity grid is an approach used for the robot to scan all desired areas of
a room or environment, considering the distance between the waypoints deﬁned
by the user. This grid will establish all the possible trajectories for the robot.
Figure 6a shows a connectivity grid for a environment with 3 meters length and
width and the waypoints with a distance of 0.5 meters (dx,dy).
The robot is constrained to perform only horizontal or vertical trajectories in
the north, south, east and west directions (no diagonal trajectories). Initially a
mapped object existence is veriﬁed. If a mapped object is veriﬁed, the waypoint
that intercepts this object is excluded from the connectivity grid and, as a result
(a)
(b)
Fig. 6. (a) Connectivity Grid. (b) Connectivity grid with the presence of obstacle.

Dynamic Path Mapping Robot for Industry Environment
661
all trajectories that provide access to this point are excluded. Figure 6b illustrates
the eﬀect of an obstacle on the connectivity grid. Based on this condition will be
projected the path for the robot to reach the end point and crossing all available
waypoints.
During the course, the robot checks for unmapped obstacles through a dis-
tance sensor (in practice, it will be used a ToF sensor). When an unmapped
obstacle less than 0.15 meters is detected, the robot returns to the last waypoint
(avoiding a collision with the unmapped obstacle) while plans a new route, thus
eliminating the point where there is an obstacle. With this algorithm the robot
will go through all the waypoints until reaching the end point and avoiding
obstacles.
4.2
Robot Controller
Robot controller uses the kinematics model of the robot. It receives the position
(x,y,θ) from the simulator and then controls the speed of the right and left wheel
(Vr and Vl). In simulation environment, SimTwo is responsible for the dynamics
presents in the system, since it uses the ODE (Open Dynamics Engine) library
for simulating rigid body dynamics [12]. Figure 7 shows an outline of the robot
and represents the characteristics of its movement along a room considering the
speed of the robot’s wheels.
Fig. 7. Considerations for calculating the linear velocity of the robot wheels.
Since V is the linear velocity, w is the angular velocity of the robot ( δθ
δt ) and
b is the distance between wheels of the robot, we obtain the Eq. 1.
V = Vl + Vr
2
;
w = Vl −Vr
2
(1)
For the orientation of the robot to the next point of its trajectory, the value
of w must follow a reference (wref) in function of a constant of proportionality

662
L. Piardi et al.
(Kp) and error between the robot orientation and trajectory. The velocity V
must be constant (Vref) as shown in Eq. 2 and complemented by Fig. 7. The
coordinate (x, y) indicates the actual robot position and (x1, y1) represents the
coordinates of the next robot trajectory point.
Vref = K1
;
wref = Kp · error
(2)
Finally, the velocity of wheels can be calculated by Eq. 3.
Vl = Vref −wref · b
2
;
Vr = Vref + wref · b
2
(3)
4.3
Trajectories Deﬁned by Splines
Splines in two-dimensional space were used in this work to represent the trajec-
tories performed by the robot. The trajectory is the path that the robot follows
between two distinct waypoints. The total path that the robot will perform, from
the starting point until reaching the end point is divided into several trajectories,
which will depend on the layout of the mapped and unmapped obstacles. For n
waypoints there are n −1 trajectories.
The implemented algorithm calculates a cubic spline for the mathematical
representation of the trajectories(calculations and mathematical proofs can be
found in Chap. 18 Sect. 6 of the book [16]), which are feasible by the robot. It’s
purpose is to smooth the trajectory that connects the waypoints through a third
degree function.
To ensure a smooth curve, the angle of entry into a waypoint should be the
same as the angle of exit. The implemented algorithm determines this angle as
a function of the slope of the straight line deﬁned by the waypoint that precedes
and succeeds the point in question.
Fig. 8. Trajectory deﬁned by Spline. The green curves indicate the trajectories to
be followed by the robot. The black lines indicate the angle of entry and exit of the
waypoints.
Figure 8a represents the calculation for the entry and exit angle of point 1,
whereas Fig. 8b represents the estimate for waypoint 2. Equation 4 describes the
calculation to determine the angle.
θi = atan2(yi+1 −yi−1, xi+1 −xi−1)
(4)

Dynamic Path Mapping Robot for Industry Environment
663
4.4
Path Planning Considering the Grid of Connectivity
Considering the connectivity grid and the waypoints with their respective pos-
sible trajectories (Fig. 6a), an algorithm was developed to path planning of the
robot knowing the start and end point. There are eight priorities for choosing
a path for the robot (Fig. 9). The path planning is calculated for all priorities,
one at a time. The priority that has the smallest number of iterations and con-
sequently the smallest trajectory will be selected to be executed by the robot.
The algorithm checks which waypoint adjacent to the current waypoint was
less visited by the robot. If only one waypoint was less visited, the robot will move
to it. When the smallest number of visits is veriﬁed in more than one adjacent
waypoint, the choice will be the one of these least visited waypoints according
to the current priority of the robot’s trajectory (Fig. 9). This algorithm is used
for all eight priorities. Switching from one priority to the next occurs when the
algorithm ﬁnds the ﬁnal waypoint and all other waypoints have been visited at
least once. When the eight priorities are ﬁnalized, the one with the shortest path
will be selected.
If an obstacle is detected, the waypoint that intercepts the obstacle will
be deleted as well as the possible trajectories as shown in Fig. 6b. Then, the
algorithm calculates a new route in the same way as described above. However
it will take into consideration the points already visited as well as the waypoints
that can not be visited due to the existence of obstacles.
Fig. 9. Priority of robot trajectories.
5
Results
In order to validate the developed application, some tests were accomplished
using the SimTwo simulator and the ControlApp.
5.1
Result of Trajectory Spline
The ﬁrst test was performed to verify the quality of the trajectories described
by the spline algorithm. It consists of inserting the robot in an environ-
ment simulated by the SimTwo whose dimensions are three meters width

664
L. Piardi et al.
Fig. 10. Result of the spline trajectory algorithm.
by two meters height. In ControlApp are inserted the waypoints that will deﬁne
the trajectories by the user as observed in Fig. 10. The results obtained from
the simulator by the algorithm that calculates the spline trajectories shows that
it reaches the objective of constructing a discrete path, segmented by contin-
uous parametric curves, constituting a smooth route feasible by a robot with
diﬀerential drive.
5.2
Result of the Planning Algorithm Applying
a Grid of Connectivity
With the objective of analyzing the path planning and a grid of connectivity
with obstacles, three tests were made in a simulation environment with 3 meters
width by 3 meters height having a spacing dx and dy of the connectivity grid of
0.35 meters for all tests performed below. The identiﬁcation of mapped obstacles
is highlighted through the red color in SimTwo simulation and ControlApp and
the identiﬁcation of unmapped obstacles is highlighted through the purple color
in SimTwo simulation and ControlApp.
Result of the Path Planning in Environments Without Obstacles: The
algorithm for the path planning is executed only once because there are none
obstacles in the environment and having an outstanding result. The robot (starts
at left bottom corner) explored the entire room through all the waypoints present
in the ﬁrst seven columns of the connectivity grid only once. In the eighth column
the robot passed twice in each waypoint to reach the end point (right upper
corner) as Fig. 11 shows. Given the proximity of waypoints, this case does not
have the concavities of the curve well deﬁned as in the example presented in the
Fig. 10.
Result of the Path Planning in Environments with Mapped Obstacles:
The Fig. 12 presents mapped obstacles simulating a room, having only a single
path for entry or exit. The waypoints that are in the regions of the mapped obsta-
cles are excluded from the connectivity grid before executing the path planning

Dynamic Path Mapping Robot for Industry Environment
665
Fig. 11. Result of planning algorithm without obstacle.
Fig. 12. Result of planning algorithm with mapped obstacles.
algorithm, i.e., only waypoints free of obstacles will be computed for path plan-
ning. Consequently the algorithm is executed once. The results for this robot
path planning validates the approach, since to complete the planning, the robot
will pass a maximum of two times in each waypoint.
Result of the Path Planning in Environments with Mapped and
Unmapped Obstacles: In the case of environments with the presence of
unmapped obstacles (Fig. 13), the path planning algorithm for the robot will be
executed the number of times the robot identiﬁes an unknown obstacle, recal-
culating a new path to reach the end point. Therefore, the arrangement of the
unmapped obstacles will result in a change in the route of the robot and also the
number of times that each waypoint of the connectivity grid will be visited. Com-
paring Figs. 12 and 13, it can be observed that the robot’s path in the presence
of unmapped obstacles is considerably more complex and nevertheless the robot
performs its test with success and safety without colliding against obstacles.

666
L. Piardi et al.
Fig. 13. Result of planning algorithm with mapped and unmapped obstacle.
6
Conclusion and Future Work
This paper presents a system for explorer and scanning a dangerous area in
an industry, where the presence of humans should be reduced. The planning
method that routes the path avoiding the known and unknown obstacles that
are detected by ToF sensors during the mapping process is implemented in a
simulation environment. The results are convincing since the robot maps the
selected waypoints, introduced by user or grid.
As future work, the replacement of the SimTwo by the real robot (already
developed) will be done. Once the protocol communication between ControlApp
and SimTwo will be adopted by the real robot, this task is simpliﬁed.
Acknowledgment. Project “TEC4Growth - Pervasive Intelligence, Enhancers and
Proofs
of
Concept
with
Industrial
Impact/NORTE-01-0145-FEDER-000020”
is
ﬁnanced by the North Portugal Regional Operational. Programme (NORTE 2020),
under the PORTUGAL 2020 Partnership Agreement, and through the European
Regional Development Fund (ERDF).
This work is also ﬁnanced by the ERDF European Regional Development Fund
through the Operational Programme for Competitiveness and Internationalisation -
COMPETE 2020 Programme within project POCI-01-0145-FEDER-006961, and by
National Funds through the FCT Funda¸cao para a Ciˆencia e a Tecnologia (Portuguese
Foundation for Science and Technology) as part of project UID/EEA/50014/2013.
References
1. Mosterman, P.J., Zander, J.: Industry 4.0 as a cyber-physical system study. Softw.
Syst. Model. 15(1), 17–29 (2016)
2. Miller, C.E., Tucker, A.W., Zemlin, R.A.: Integer programming formulation of
traveling salesman problems. J. ACM 7(4), 326–329 (1960). New York
3. Pereira, N., Ribeiro, F., Lopes, G., Whitney, D., Lino, J.: Autonomous golf ball
picking robot design and development. Ind. Robot Int. J. 39(6), 541–550 (2012)

Dynamic Path Mapping Robot for Industry Environment
667
4. Hirakawa, A. R., Saraiva, A. M., Cugnasca, C. E.: Autˆomatos Adaptativos Aplica-
dos em Automa¸cao e Robtica(A4R). IEEE Lat. Am. Trans. 5(7), 539–543 (2007).
S˜ao Paulo
5. Vaz, D.A.B.O.: Planejamento de movimento cinem´atico-dinˆamico para robˆos mveis
com rodas deslizantes. Universidade de S˜ao Paulo (2011)
6. Yang, X., Zeng, Z., Xiao, J., Zheng, Z.: Trajectory planning for RoboCup MSL
mobile robots based on B´ezier curve and Voronoi diagram. In: IEEE International
Conference on Information and Automation, pp. 2552–2557 (2015)
7. Kloetzer, M., Mahuela, C., Gonzalez, R.: Optimizing cell decomposition path plan-
ning for mobile robots using diﬀerent metrics. In: 19th International Conference
on System Theory, Control and Computing (ICSTCC), pp. 565–570 (2015)
8. Magid, E., Karen, D.,Rivlin, E., Yavneh,I.: Spline-based robot navigation. In:
IEEE/RSJ International Conference on Intelligent Robots and System, pp. 2296–
2301 (2006)
9. Hwang, J.H., Arkin, R.C., Kwon, D.S.: Mobile robots at your ﬁngertip: Bezier curve
on-line trajectory generation for supervisory control. In: IEEE/RSJ International
Conference on Intelligent Robots and Systems, pp. 1444–1449 (2003)
10. Simba, K.R., Uchiyama, N., Sano, S.: Real-time trajectory generation for movile
robots in a corridor-like space using Bezier curves. In: IEEE/SICE International
Symposium Systrem Integration, pp. 37–41 (2013)
11. Costa, P., Gon¸calvez, J., Lima, J.: SimTwo realistic simulator: a tool for the devel-
opment and validation of robot software. Int. J. Theor. Appl. Math. Comput. Sci.
17–33 (2011)
12. Nascimento, T.P., Moreira, A.P., Costa, P., Costa, P, Concei¸c˜ao, A.G.S.: Modeling
omnidirectiona mobile robots: an approach using SimTwo. In: 10th Portuguese
Conference on Automatic Control, Funchal, pp. 117–223 (2012)
13. Lima, J., Gon¸calves, J., Costa, P., Moreira, A.: Modeling and simulation of a
laser scanner sensor: an industrial application case study. In: Azevedo, A. (ed.)
Advances in Sustainable and Competitive Manufacturing Systems. Lecture Notes
in Mechanical Engineering, pp. 697–705. Springer, Heidelberg (2013)
14. Lima, J., Gon¸calves, J., Costa, P.: Modeling of a low cost laser scanner sensor.
In: Moreira, A., Matos, A., Veiga, G. (eds.) CONTROLO2014 Proceedings of the
11th Portuguese Conference on Automatic Control, vol. 321, pp. 697–705. Springer,
Cham (2015)
15. Gon¸calves, J., Lima, J., Costa, P., Moreira, A.: Modeling and simulation
of the EMG30 geared motor with encoder resorting to SimTwo: the oﬃcial
Robot@Factory simulator. In: Azevedo, A. (ed.) Advances in Sustainable and Com-
petitive Manufacturing Systems, pp. 307–314. Springer, Heidelberg (2013)
16. Chapra, C.S., Canale, R.P.: Numerical Methods for Engineers, 6th edn. The
McGraw-Hill Companies, New York (2010)

Poses Optimisation Methodology for High
Redundancy Robotic Systems
Pedro Tavares1,3(B), Pedro Costa1,2, Germano Veiga1,2,
and António Paulo Moreira1,2
1 FEUP - Faculty of Engineering of University of Porto, Porto, Portugal
pedro.ms.tavares@gmail.com
2 INESC TEC - INESC Technology and Science formerly INESC Porto,
Porto, Portugal
3 SARKKIS Robotics, Porto, Portugal
Abstract. The need for eﬃcient automation methods has prompted the
fast development in the ﬁeld of Robotics. However, most robotic solutions
found in industrial environments lack in both ﬂexibility and adaptabil-
ity to be applied to any generic task. A particular problem arises when
robots are integrated in work cells with extra degrees of freedom, such
as external axis or positioners. The speciﬁcation/design of high redun-
dancy systems, including robot selection, tool and ﬁxture design, is a
multi-variable problem with strong inﬂuence in the ﬁnal performance
of the work cell. This work builds on top of optimisation techniques to
deal with the optimal poses reachability for high redundancy robotic sys-
tems. In this paper, it will be proposed a poses optimisation approach
to be applicable within high redundancy robotic systems. The proposed
methodology was validated by using real environment existent infrastruc-
tures, namely, the national CoopWeld project.
Keywords: Optimisation · Poses determination · Cost functions ·
Metaheuristics · High redundancy robotic systems
1
Introduction
The robotic ﬁeld’s expansion leads to the desire of developing highly autonomous
and eﬃcient methodologies to address the prominent problems of not only research
and development but also the needs pointed by the industrial corporations.
One of those problems in robotic systems is task management, that involves
the integration of the motion planning in the work cell context, including tech-
nological process limitations, communication with external devices, automatic
work cell calibration among others. Currently, there is no optimal tool to create
an action sequence to complete a given task (currently handled by human expe-
rience). However, some studies point to the usage of optimisation functions to
attend this problem [1,3].
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_55

Poses Optimisation Methodology
669
Recently, a wide range of applications have been developed towards the
robotic implementation in harsh and/or repetitive tasks, such as welding, cut-
ting and transportation, commonly found in the construction industries. These
tasks are fairly complex as they need to cope with several parameters. Robotic
systems have to be autonomously capable of performing any coded task in a
precise and eﬃcient way. Thus, when developing systems that are going to be
included into dangerous situations, the architecture that embraces the robotic
system needs to consider robust elements. From the presented tasks, cutting and
transportation seem to have a clear idea on where to improve and deﬁne a bet-
ter and cleaner strategy while welding still needs to have some re-visitation on
sensing, planning and adequate optimisation to any required task.
The speciﬁcation of optimal design methodology of high redundancy systems,
including robot selection, tools and ﬁxtures design, is a multi-variable problem
with strong inﬂuence in the ﬁnal performance of the work cell. An active example
of this problem is related to selection of ﬂexible elements (such as the torching
cables for welding operations) and their constraints.
In 2015, Graetz and Michaels, in their work entitled “Robots at work”,
analysed for the ﬁrst time the economic impact of industrial robots pointing
that robots signiﬁcantly added increased value to industries [9]. Thus, this opti-
misation research topic applied to industrial work cells seems to be adequate to
current state of industrial development.
Throughout this project, a novel design methodology capable of deﬁning the
correct work cell for a given operation in robotic applications is being developed
in order to achieve industrial requirements and potentiate production goals. Fur-
thermore some considerations on work cell motion and components reposition
will assure an eﬃcient function of the system.
The current paper is structured in ﬁve main section. Section 2 aims to provide
an overview of the current state of optimisation technique appliance in research
or industrial processes. Following, Sect. 3, System Architecture and Heuristic-
Based Approach, intends to present the deﬁned optimisation structure, as well as
the detail model description of a generic work cell submitted to the selected algo-
rithms. Then, Sect. 4, Optimisation Methodologies, will describe a set of algo-
rithms to be applied in order to accurately ﬁnd an optimal solution. Section 5,
Validation, will provide preliminary results of the proposed optimisation app-
roach and a comparison between selected algorithms. Section 6, Discussion and
Future Perspectives, will summarise the contribution of this project to the sci-
entiﬁc community and intended iterative of it.
2
Related Work
Robotic applications have become key to ensure process eﬃciency in a wide range
of scientiﬁc ﬁelds. Several studies have suggested that there can be considerable
gains in terms of proﬁtability and reduced operation times on ﬁelds ranging from
medical surgery to manufacturing proceedings [5,8,12].

670
P. Tavares et al.
Currently, optimisation methodologies have grown interest within research
and industrial communities due to their success in solving multi-variable prob-
lems. Several algorithms have been deﬁned and implemented showing great
adaptability to complex tasks [17]. The main trend of such optimisation method-
ologies is related to heuristic-based algorithms. By deﬁning detailed goals and
constraints, such algorithm can iterative search for a ideal solution, and has been
applied to several applications [7,15].
Another key topic connected to optimisation and focusing in robotics is
related to the work cell design, that comprises, among others, robot selection
and ﬁxture design. Cheng mentioned the simulation tool’s advantages in order
to develop a robotic work cell [4].
Furthermore, there have been authors claiming to ﬁnd powerful enough
methodologies to handle machining [2] and welding challenges [10,11]. Still,
despite its importance, existent robotic systems are focused on solving separate
necessities while there is no optimal tool to properly design a generic robotic
work cell.
The nearest solution to an optimal design methodology was presented by
Kamoun et al., with an approach concerning the display of equipment over a
given area [13]. Recently, Pelleginelli et al., proposed an extended formalisa-
tion of design and motion planning problems for spot-welding multi-robot cells
[16]. However, both solutions only considered previously selected equipment or
reduced features and did not include the optimal selection of such equipment.
Despite the tremendous utility that optimisation methodologies have shown
when applicable to other principals, the robotic industrial world still lacks a
ﬂexible and autonomous solution regarding the complete optimisation of work
cell and its properties.
3
System Architecture and Heuristic-Based Approach
The optimisation approached presented is directly linked to an user interface soft-
ware. This software follows the paradigm of MVVM (Model-View-View Model),
a three layer software architecture. Each layer has its own importance. The
Model is the centralized database of all relevant content. Here we can detail
system components and their relations. The View Model is responsible for the
communication management between the raw data on the model and the user
interface that is referenced as View.
Thus, in order to use the proposed optimisation approach, each user has
to follow a script of inserting work cell components, deﬁning relations between
them and then generating a robotic Kinematic Chain that will be storage as the
system Model.
The View Model of the proposed approach is related to the optimisation
methodology. The raw data inserted in the Model will be de-serialized and the
kinematic chain’s components will be classiﬁed as ﬁx parts, conveyor, external
axis, robots or tools. Then an world map is created accordingly to the found
components and converted to a visual interface, View.

Poses Optimisation Methodology
671
Once loaded the system, the optimise algorithm is fully deﬁned. This algo-
rithm can then be described as a set of steps (see Algorithm 1).
Algorithm 1. Optimization Proceeding
Inputs: Model Data, Optimisation Algorithm;
Outputs: Optimized Pose;
Map = GenerateMap(Model Data);
Job = LoadJob();
Points = IdentifyPoints(Job);
OptimisedPoses = CreateStructure(Optimisation Algorithm);
RunOptimisationAlgorithm(Points,Optimisation Algorithm);
The terminal part of the optimisation proceeding diﬀers accordingly to the
chosen algorithm. An insight on what algorithms have been selected and vali-
dated will follow in Sect. 4.
However, all of the selected algorithms follow an ideology of heuristic-based
solutions and share a common goal, minimization of eﬀort and maximization
of present and subsequent poses. In that regard, a cost function was developed
based on six features:
1. External Axis Motion: While performing a task, it is pretended to minimize
the external axis moves as they may insert instability within the robotic
system.
2. Singularities: Robots’ behaviour becomes unstable during singularities, thus,
they should be avoided.
3. Conﬁguration Change: Robots should whenever possible keep an original con-
ﬁguration to avoid sudden uncontrolled movements.
4. Joints’ Eﬀort: Minimization of system eﬀort smooths moves and protects
components.
5. Reachability: The distance between robot’s base and goal position should be
minimized to increase the reaching probability for future poses.
6. Joints’ Limits: Similar to the previous criteria, in order to increase the reach-
ing probability of future poses, an ideal pose should maximize the interval
between joint position and limits.
Thus, the cost function that untimely dictates the viability of a random pose
results in a weighed sum and can be described using Eq. 1.

(w1 ∗ExternalMove + w2 ∗Singularities
COST = + w3 ∗ConfigurationChange + w4 ∗JointsAmplitude
+ w5 ∗Reachability + w6 ∗JointsLimits)
(1)

672
P. Tavares et al.
Although not all variables stated in the equation are not continuous (Exter-
nalMove and ConﬁgurationChange), the cost function value is well deﬁned in all
its domain. That is accomplish by processing each feature separately as described
below.
External Axis Motion is consider a boolean state, on whether the generated
solution requires external axis moves. In that case, the value ExternalMove will
be set to 1, otherwise it will be 0. The associated weight w1 is fairly high due to
the fact that this is one of the feature that we desire to avoid the most.
Singularities are determined and analysed using the robot Jacobian where its
determinant is a indicator of singularities. When close to 0, the robot is approach
a singularity state. Thus, the inverse of that value (will be higher when closer to
that singularity state) is used as the Singularities parameter.
Each robot manufacture has a conﬁguration deﬁnition for a given joint state.
Generically this is linked with the wrist-shoulder-elbow conﬁgurations. Ideally,
when ﬁnding a new solution for a speciﬁed position, robots should avoid chang-
ing conﬁguration as it prevents uncontrolled movements. Once again, Conﬁgu-
rationChange is a binary value, 0 when conﬁguration change is not require, 1
otherwise.
Regarding joints’ eﬀort, the weight associated to each joint is not linear since
that some joints have higher implications than other. Considering as an example,
an anthropomorphic robot, the initial three joints’ amplitude is more relevant
than the wrist joints. Therefore the parameter JointsAmplitude is obtained from
a weighed sum with decreasing weight for each joint starting from base to end.
The Reachability value is calculated based on the robot full length and is
deﬁned as the quotient between base to goal position distance and the full length
value.
Finally, the joints’ limits follow the same pattern as the previous parameter,
as it results from a quotient between estimated joint value and its limits. How-
ever, another layer is inserted here as for each joint the weight is diﬀerent for
similar reasons presented for the joints’ eﬀort parameter.
One ﬁnal comment is related to the usage of the cost function when there
is no available solution. In that cases the testing hypothesis is discard without
even being weighed.
By combining all values it is possible to classify any given pose of the sys-
tem and, thus, ﬁnding the optimal solution following one of the methodologies
presented next.
4
Optimisation Methodologies
To face the proposed challenge there were selected four mains optimisation
techniques: Linear Scanning of available poses, Genetic Algorithms, Simulated
Annealing and Potential Fields.
Each of the proposed methods returns a heap storing the best outcomes of
the cost function. Considering that each solution results in an array of joint
values throughout the kinematic chain, a dynamic structure is built upon the

Poses Optimisation Methodology
673
map generation. Every heap element will follow the parameters deﬁne within
that same structure.
The reason behind using a multi-solution heap is related to the continuous
path of the robotic system following the array of solutions. Even if a random
position is validated and optimised, along the path between poses, there might
be an extra constraint such as obstacles or speed eﬀort. Thus, in those cases, the
initial best solution has to be discarded and new one will be searched within the
heap.
Each method also can be divided in two main phases: creation of hypothesis
and validation. Since the idea is to ﬁnd the optimal robotic system pose for a
pre-deﬁned position, the hypothesis initial focus the external axis values and
then using a path planner validates and determines the robot positioning for
those external axis values.
The implemented methods will be synthesise in the following subsections.
4.1
Linear Scanning
The standard and easier way to do a search for an optimal solution is linearly
go through all hypothesis while saving the best one. Since the ﬁnal return is
expected to be a heap, the saving results need to be extended to its size.
The key step of this method is selecting the discretization step that balances
memory usage and time consumption. As expected this method raises prob-
lems for high redundancy systems as computational capacities are limited and
considerations on memory usage need to be consider. Thus, when creating the
hypothesis to test a linear discretization method for each element of interest was
implemented, bounding the number of hypothesis.
The ideal algorithm can be described as following (Algorithm 2).
Algorithm 2. Linear Scanning Algorithm
Inputs: PointsToOptimise, Model Data;
Outputs: Optimised Poses;
Hypothesis = Create_Testing_Hyphotesis();
Optimised Poses = Create_Poses_Structure();
foreach PointsToOptimise do
foreach Hypothesis do
EvaluteHypothesisCost(Hypothesis.Current, PointToOptimise.Current);
Update_Optimised_Poses();
end
end
return Optimised Poses;

674
P. Tavares et al.
4.2
Genetic Algorithms
This method can be deﬁned as a search and optimisation tool able to solve multi-
constraint problems [6]. Genetic algorithms recur to genes (variable of interest)
to store a sequence or solution of interest. Most of common applications using
this method start with two set of solutions and iteratively swap (exchange of
genes between solutions) or mutate (random or methodical change of a given
gene), creating a population of solutions.
Within our proposed methodology we start with a higher number of ran-
domly generated genes. Each gene is deﬁned as a vector resulting of the external
axis values. Then, each gene undergoes a reachability validation of the deﬁned
position. Iteratively new genes are generated throughout a ﬁxed number of iter-
ations and the optimised heap is built. The generation of each gene is based
on the swap and mutation operations, that are randomly selected. In case of
swap procedure, the second gene is also randomly chosen from the multi-gene
population. The algorithm can be describe as following (Algorithm 3).
Algorithm 3. Genetic Algorithm
Inputs: PointsToOptimise, Model Data;
Outputs: Optimised Poses;
Population = Generate_Genes();
Optimised Poses = Create_Poses_Structure();
foreach PointsToOptimize do
for NumberOfIterations do
foreach Gene in Population do
EvaluteHypothesisCost(Gene, PointToOptimize.Current);
Update_Optimised_Poses();
end
Population = Generate_NewGenes(Population, mutationRate,
swapRate);
end
end
return Optimised Poses;
4.3
Simulated Annealing
Another optimisation technique is the simulated annealing method, which is a
probabilistic to ﬁnd the global optima of a given function [14]. This method starts
from a random solution and iteratively searches its neighbourhood to deﬁne new
possible solutions. Then, probabilistically decides to which solution it should
iterate until untimely ﬁnds the global optimum.

Poses Optimisation Methodology
675
However, when dealing with high redundancy system this method entails a
high time and computation eﬀort. Thus, a minor adjustment to the method was
implemented in order to reduce the execution time of the method. A threshold
was deﬁned in order that the method runs iteratively until reaching a ﬁxed num-
ber of solutions that verify such constraint, stopping without fully completing
the algorithm, giving a secure and acceptable list of solutions while minimizing
time consumption.
Another add-in was related to the initial point. Since that this is neighbour-
based, if the initial point and its neighbours do not produce a valid solution,
the method would stop and a erroneous value would be found. As such, the
initial point is randomly ﬁxed within half robot’s length to the goal point. The
algorithm is shown next.
Algorithm 4. Simulated Annealing
Inputs: PointsToOptimise, Model Data;
Outputs: Optimised Poses;
Solution = Generate_Initial_AcceptablePosition();
Optimised Poses = Create_Poses_Structure();
foreach PointsToOptimise do
while SolutionNumber < IntendedSolutionNumber do
EvaluateSolution(Solution);
Update_Optimised_Poses();
Neighbours = Get_Solution_Neighbours();
Solution = Select_Next_Solution(Neighbours);
end
end
return Optimised Poses;
4.4
Potential Gradient
Similar to Simulated Annealing, Potential Gradient is an algorithm based on
surrounding solutions of the current iteration. However, this algorithm stops at
local optimums.
The iteration direction is deﬁned by the sum of directional derivatives framed
with the optimisation function. Once determined what is the best directional
vector a new solution is generated until reaching a local optimum.
Despite being computational light, this algorithm does not guarantee optimal
solutions for any given problem.

676
P. Tavares et al.
5
Validation
Attempting to ensuring a multi disciplinary validation process, two work cells
with diﬀerent properties were modelled and inserted in a custom simulator.
Those work cells are displayed below in Fig. 1.
Fig. 1. At the left - a work cell with a cartesian external axis, a fanuc ic30 and a
cutting torch; at the left - a work cell with two kinematic chain, one composed by a
rotative external axis (Ring) and a Motoman MH5 robot and the second by a external
positioner.
A third cell was used, although it is not presented above. That is due to
the fact of this cell is currently being physically implemented according to the
national project CoopWeld. Thus, the third cell was only test and validated in
a simulated environment.
The work cells here presented are focused in these redundancy systems com-
posed by external axis, robot and operation tool. However the proposed approach
is also applicable to simpler robotic cells.
In order to validate the methodology a set of Cutting and Welding Jobs
were generated using a CAM (Computer Aided Manufacturing) software for
the production of beams, MetroID and CLARiSSA, proprietary of SARKKIS
robotics. This software generates a set of vectors containing poses that describe
the given operation (see Fig. 2).
Fig. 2. MetroID user interface.

Poses Optimisation Methodology
677
Using these software it was then created several test beams. Examples are
presented next in Fig. 3. The idea behind the creation of those beams was to
deﬁne diﬀerent jobs that required external axis/positioners movement.
Fig. 3. Beams examples.
In order to evaluate each of the implemented algorithms there were consider
three parameters: reachability percentage, time consumption and cost of reached
solution. The results are summarized in Table 1. These are according to a vali-
dation test of 15 beams, each with 5 to 130 operations, which resulted in a total
of 563 points to be optimised. The results provide in the table are means per
operation of the correct achieved solutions.
Table 1. Results summary of the validation test
Optimisation methodology Solution reachability (%) Time consumption (s) Cost value
Linear scanning
100%
18.031
0.103
Genetic algorithms
100%
1.022
0.098
Simulated annealing
96.4%
0.740
0.147
Potential ﬁelds
81.4%
0.412
0.134
These results were achieved once established the proper parameters for each
algorithm. Those were determined by considering the best testing performance
for random positions for each algorithm when concerning memory management,
time consumption and solution reachability.
Concerning Linear Scanning was implemented a discretization in 50 equally
spaced hypothesis of each external axis/positioners based on their interval range.
The Genetic Algorithm methodology was implemented using a cross rate of 50%
and a mutation rate of 10%, for a random generated population of 1000, through-
out 25 iterations. This was the set of parameters that produced the best results
in a exhaustive study done with diﬀerent parametrizations. We also limited Sim-
ulated Annealing reaching goal to a maximum cost of 0.15 in order to reach a
higher number of solutions. Moreover, Simulated Annealing was implemented
using a multi dimensional neighbour radius of 8 increments. Each increment is
considered to be the interval value of each external part when discretized into
1000 equally spaced hypothesis.

678
P. Tavares et al.
6
Discussion and Future Perspectives
In this article we presented a solution with enough ﬂexibility to be applied to
all robotic installations. This solution allows one to avoid the common pitfalls
associated with robotic poses conﬁguration.
Once deﬁned the cost function containing key elements for poses description,
a set of algorithms can be applied to identify the correct conﬁguration for a
pre-deﬁned pose.
Here, the optimisation algorithms prove to be the best methodology.
Throughout the project, four were implemented. Tests shows that each can be
valuable within the scientiﬁc community.
Directional algorithms, such as Potential Gradient, proved to be faster to
achieve a solution. However, they fail to reach a reasonable solution to all cases.
Energy gradient based, such as Simulated Annealing, reach all solutions, however
the time consumption to reach the optimal one sometimes is too high. Thus,
ﬁxing a limit may help reducing time eﬀorts but compromises eﬃciency.
Linear Scanning is set to ﬁnd the optimal solution within a discretization step.
However, the time consumption is too high and the mentioned discretization
in order to avoid memory issues might have to be enlarge reducing eﬃciency
to ﬁnd the optimal position. A recursive Linear Scanning algorithm could be
implemented however it would forfeit at each iteration a set of solution (not
ensuring that the optimal one is not discarded) and still remains with the time
consumption problem.
Thus, Genetic Algorithms, seem to be the best methodology as they reach a
viable solution for all cases with a reduce time and computation eﬀort.
Future work concerning optimisation in robotic work cells is related to com-
ponents positioning and choosing, thus, providing a complete software solution
to optimal design a high redundancy robotic work cell.
In conclusion, the work presented here formalizes a ﬂexible approach for poses
optimisation methodology, ensuring optimal conﬁguration for robotic elements
to perform a given task in a faster and eﬃcient way.
Acknowledgments. A special word to SARKKIS robotics and INESC-TEC (in par-
ticular the ERDF – European Regional Development Fund through the Operational
Programme for Competitiveness and Internationalisation - COMPETE 2020 Pro-
gramme, and the FCT – Fundação para a Ciência e a Tecnologia (Portuguese Foun-
dation for Science and Technology) within project «POCI-01-0145-FEDER-006961»)
for their commitment in research and development of revolutionary state-of-the-art
algorithms and for their contribution regarding software tools and engineering hours
availability.
References
1. Alatartsev, S., Stellmacher, S., Ortmeier, F.: Robotic task sequencing problem: a
survey. J. Intell. Robot. Syst. 80(2), 279–298 (2015)

Poses Optimisation Methodology
679
2. Andrisano, A.O., Leali, F., Pellicciari, M., Pini, F., Vergnano, A., Pini, F.: Inte-
grated design of robotic workcells for high quality machining (2011)
3. Bennewitz, M., Burgard, W., Thrun, S.: Optimizing schedules for prioritized path
planning of multi-robot systems, vol. 1, pp. 271–276 (2001)
4. Cheng, F.S.: Methodology for developing robotic workcell simulation models, vol.
2, pp. 1265–1271 (2000)
5. Chu, B., Jung, K., Chu, Y., Hong, D., Lim, M.T., Park, S., Lee, Y., Lee, S.U.,
Min Chul, K., Kang Ho, K.: Robotic automation system for steel beam assembly
in building construction, pp. 38–43 (2009)
6. Deb, K.: Introduction to genetic algorithms. Sadhana - Acad. Proc. Eng. Sci. 24(4–
5), 293–315 (1999)
7. Fathi, M., Álvarez, M., Rodríguez, V.: A new heuristic-based bi-objective simulated
annealing method for U-shaped assembly line balancing. Eur. J. Ind. Eng. 10(2),
145–169 (2016)
8. Geller, E., Matthews, C.: Impact of robotic operative eﬃciency on proﬁtability.
Am. J. Obstet. Gynecol. 209(1), 20e1–20e5 (2013)
9. Graetz, G., Michaels, G.: Robots at Work (2015)
10. Gueta, L., Chiba, R., Arai, T., Ueyama, T., Ota, J.: Compact design of work cell
with robot arm and positioning table under a task completion time constraint, pp.
807–813 (2009)
11. Hauer, S., M.V.H.C.S.K.: Design and simulation of modular robot work cells, pp.
1801–1802 (2009)
12. Kamezaki, M., Hashimoto, S., Iwata, H., Sugano, S.: Development of a dual robotic
arm system to evaluate intelligent system for advanced construction machinery, pp.
1299–1304 (2010)
13. Kamoun, H., Hall, N., Sriskandarajah, C.: Scheduling in robotic cells: heuristics
and cell design. Oper. Res. 47(6), 821–835 (1999)
14. Kirkpatrick, S., Gelatt Jr., C., Vecchi, M.: Optimization by simulated annealing.
Science 220(4598), 671–680 (1983)
15. Mansouri, S.: A multi-objective genetic algorithm for mixed-model sequencing on
JIT assembly lines. Eur. J. Oper. Res. 167(3), 696–716 (2005)
16. Pellegrinelli, S., Pedrocchi, N., Tosatti, L.M., Fischer, A., Tolio, T.: Multi-robot
spot-welding cells for car-body assembly: design and motion planning. Robot.
Comput.-Integr. Manufact. 44, 97–116 (2017)
17. Yang, J., Yang, J.: Intelligence optimization algorithms: a survey 3(4), 144–152
(2011)

Oﬄine Programming of Collision Free
Trajectories for Palletizing Robots
Ricardo Silva1, Lu´ıs F. Rocha1, Pedro Relvas1, Pedro Costa1,2,
and Manuel F. Silva1,3(B)
1 INESC TEC - INESC Technology and Science, Porto, Portugal
{rfsilva,luis.f.rocha,pedro.m.relvas}@inesctec.pt
2 FEUP - Faculty of Engineering of the University of Porto, Porto, Portugal
pedrogc@fe.up.pt
3 ISEP - School of Engineering of the Porto Polytechnic, Porto, Portugal
mss@isep.ipp.pt
Abstract. The use of robotic palletizing systems has been increasing in
the so-called Fast Moving Consumer Goods (FMCG) industry. However,
because of the type of solutions developed, focused on high performance
and eﬃciency, the degree of adaptability of packaging solutions from one
type of product to another is extremely low. This is a relevant problem,
since companies are changing their production processes from low vari-
ety/high volume to high variety/low volume. This environment requires
companies to perform the setup of their robots more frequently, which has
been leading to the need to use oﬄine programming tools that decrease
the required robot stop time. This work addresses these problems and, in
this paper, is described the solution proposed for the automated oﬄine
development of collision free robot programs for palletizing applications.
Keywords: Palletizing · Industrial robots · Simulation · Oﬄine robot
programming
1
Introduction
The use of robotic systems has been increasing in particular in the so-called Fast
Moving Consumer Goods (FMCG) industry. Among the various applications of
robotic systems in this industry, it should be highlighted the automated systems
for the movement of raw materials and ingoing products and, fundamentally, the
robotic systems for palletizing and packaging of ﬁnished products.
In particular, the palletizing activities are specially demanding not only in
terms of operation speed but also due to the inherent complexity in the han-
dling of diﬀerent types of packaged products. Despite the common application
of robotic technology solutions, these are generally designed to ensure high per-
formance under well deﬁned operating conditions (i.e. very speciﬁc in terms
of operating standards). Therefore, a trade-oﬀbetween the operational perfor-
mance and the desired degree of ﬂexibility is normally seen. In fact, because of
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_56

Oﬄine Programming of Palletizing Robots
681
the type of solutions developed, focused on high performance and eﬃciency, the
degree of adaptability of packaging solutions from one type of product to another
is extremely low. Also, there is also high rigidity as regards a possible change
in the conﬁguration of the packaging system to new operating requirements, for
example, integrated into a new layout of the manufacturing process.
From the perspective of the production process, this type of limitations sig-
niﬁcantly aﬀects the ﬂexibility of the production system itself, often preventing
the development of production processes in a low volume/high variability envi-
ronment [1]. In fact, after the eﬀorts made in recent years by several industrial
units to make production processes more ﬂexible, allowing for a diversiﬁed and
competitive response, it is clear that packaging/palletizing systems should follow
this development: become more adaptable and ﬂexible.
Following this set of identiﬁed problems, INESC TEC is developing a project
having as its main objective, to investigate, study, explore and develop a frame-
work that allows the design and development of adaptive palletizing solutions
based on robotic technologies. The framework includes a set of innovative ele-
ments that eﬃciently create and evaluate palletizing solutions and, at a later
stage, design and develop the solution to be produced, based on an innovative
concept of modularity (modular functional elements) and advanced technology
for oﬄine programming of the robotic systems.
This project constitutes a unique opportunity for a qualitative leap in the
industry equipment, which provides palletizing solutions, evolving from a par-
adigm based on the use of conventional design and development practices to a
new paradigm of solution creation, based on technologically advanced modular
architectures that are adaptive and intelligent (aligned with the principles and
concepts of industry 4.0).
Bearing these ideas in mind, the next section describes the main ideas behind
this project, followed by Sect. 3 that presents the related work to the problem
addressed in this paper which is described in Sect. 4. Section 5 presents the con-
ceptual architecture proposed for the solutions and, in the sequel, Sect. 6 intro-
duces the main results achieved and its discussion. Finally, in Sect. 7 the main
conclusions of this work are referred, as well as some ideas for future develop-
ments.
2
Proposed Solution Concept
The development of ﬂexible and conﬁgurable products that can evolve in the
future is a complex task. The limitations of conventional design and development
approaches are recognized in this context.
Unlike conventional product development, where architectures/platforms
with well-structured and detailed interfaces are not used, the concept under-
lying the proposed solution is based on the principles of integrating advanced
modularity techniques into the design and development of products. In this way,
it will be possible to make the integrated and modular development of new prod-
ucts by drastically reducing the time of design, development and installation of
new equipments.

682
R. Silva et al.
The “modular product” project assumes to guarantee two main characteris-
tics. First, each functional element of the product consists of “parts” which have
well deﬁned interactions with other parts of the same product. Second, modu-
lar product design allows to perform changes to be made in one part, or even
replace it (ex: conveyor, robot, etc.) without this change aﬀecting other product
functions.
As in the automotive industry, modularity should consider the entire design
process. While standardization involves design-to-production optimization, the
product architecture inﬂuences the use and the sale of the ﬁnal product, as it
contributes for the enhancement of its ﬂexibility and adaptability.
The concepts of modularity considered here aim at the decomposition of what
would be a robotized palletizing complex monolithic solution, in subsystems that
constitute complete functional units, that can be designed and produced inde-
pendently (allowing the construction of diﬀerent products by means of a combi-
nation of subsystems) and with a reduced complexity, but keeping its integrated
operation. This innovative concept will be materialized in a technological kit
that will validate the application of the developed research in the design and
development of robotic adaptive palletizers, and will be supported by a set of
software and hardware prototypes (see Fig. 1).
Fig. 1. Diagram of the proposed project solution modules
Considering the proposed project solution architecture depicted in Fig. 1,
this paper speciﬁcally focus on the “Palletizing System Simulator” and “Robot
Trajectory Generation System” Modules. More in detail, is presented the simu-
lation environment, and how this simulator can be used for quickly designing a
palletizing cell and programming the respective robots.

Oﬄine Programming of Palletizing Robots
683
3
Related Work
Robot palletizing cells have captured a lot of attention in the last decade in the
scientiﬁc sphere. Two essential issues aﬀecting the performance of palletizing
robots, that should be considered when designing a ﬂexible automatic robot
palletizing production line, are: the proper robotic cell layout arrangement and
the path planning of the industrial robot.
In [2,3] is presented an oﬄine programming robot palletizing simulator that
consist of: (i) a fast algorithm to generate the palletizing product mosaic, (ii)
a 3D robot simulator, and (iii) an optimized trajectory generation algorithm.
According to its authors, with this system it becomes possible to program the
industrial robot without the need of using the teach pendant or writing a single
line of code. The authors focus their work on the optimization of the computa-
tional time of their path planner, based on A*.
In [4] the authors deal with facility layout analysis and path planning of a
robotic palletizing production line, using a 4-DOF partial closed-loop palletiz-
ing robot and two carton conveyors. The layout of the robotic cell is arranged
considering the reachability of the same, but at the same time minimizing the
robot footprint. In terms of path planning, the authors propose a straight-line
and arc based motion planning approach under the constraints of joint velocity
and acceleration to achieve a higher productivity and an easier implementation.
Ryosuke et al. [6] also deal with the design of a palletizing cell and with
the robot path planning problem. To solve both, the authors propose a design
method to improve the performance of a palletizing manipulator. The working
environment is optimized regarding the base position of the manipulator and
the shape and position of the pallet. To reduce the computational time, the
parameters of an environment are quickly evaluated with the proposed method,
in which are set passing points to reduce the computation time of path planning.
Focusing on the path planning problem, the authors in [5] present an algo-
rithm that computes a collision free trajectory considering a 6-axis industrial
robot. The target applications for the developed approach are palletizing and
handling robotic cells. Given a 3D environment, the proposed algorithm decom-
poses the Cartesian workspace of the robot into 3 dimensional cylinder slices
centered around the robot base.
For its turn, Nan Luan et al. [7] present a maximum speed algorithm for
serial palletizing robots. As the authors state, operation speed is one of the
most important performance index of a robotic palletizing cell. To meet the
required performance, the authors propose an iterative control algorithm that
optimizes the maximum robot speed given a certain positional tolerance. The
authors refer that, with this algorithm it is possible to obtain maximum speed
and acceleration for the robot manipulator regardless of manipulator position,
arm posture or joint couplings, thus improving palletizing eﬃciency and bringing
more economic beneﬁts into practical application.

684
R. Silva et al.
4
Problem Description
The problem addressed in this paper is the identiﬁcation, deﬁnition and develop-
ment of the “Palletizing System Simulator” and “Robot Trajectory Generation
System” modules. In Fig. 1 is presented the project solution architecture.
As mentioned, this project foresees the inclusion of a 3D robotic cell sim-
ulator as an integral part of the proposed modular architecture. As its main
requirements, this simulator should allow the design and simulation of the all
palletizing cell, and also the oﬄine programming and the download of the gen-
erated program code for robots from distinct brands.
The inclusion of this software in the project aims not only for the reduction
in the design time of the palletizing cells, but also in the development and setup
times of the robots programs. Firstly, this software allows the oﬄine program-
ming of the mentioned robots, as well as the simulation of its operation. Secondly,
it makes possible to expand the software components library by designing new
models of customized equipment or even whole palletizing cells. This way, their
inclusion in each project can be performed in a simple way. Indeed, by simulat-
ing the robots virtual models and the palletizing systems in which they operate,
the eﬃciency of design of a robotic cell can be signiﬁcantly improved. Also, con-
sidering the accuracy and level of detail oﬀered by the simulation software, it is
possible to achieve models having a behaviour very similar to the actual equip-
ment, enabling the study and test of a solution before its implementation, with
very high reliability.
Moreover, using the simulator is also ensured the concept of adaptability
proposed in the project. After the oﬄine programming stage, through the soft-
ware’s own language, the goal is to translate this language into the ones used
by the robot’s controllers of diﬀerent vendors (namely ABB, KUKA, FANUC
and Yaskawa Motoman). For this purpose, the idea is to deﬁne a so-called neu-
tral language, to which is translated the software’s language. Then, the next
step is to perform the translation of this neutral language to each considered
robot’s manufacturer language. The neutral language adopted in this Project is
the Industrial Robot Language (IRL - DIN Standard 66312), since the software
already allows the translation of its instructions set to this standard.
This simulator will be part of the “Palletizing System Simulator” module
(see Fig. 1), whose main goal is to make the programming stage of each palletiz-
ing cell’s robot. Therefore, using the Python’s API provided by the simulation
software, it was developed a plug-in that generates the robot’s program only by
selecting the pick and place points of the products to palletize.
This module, on its turn, will exchange information with the “Robot Trajec-
tory Generation System” module. It aims to program the movement of the robot
between the points of pick and place, considering its operation in the projected
cell, in a way that results in collision free trajectories. Each of these two modules
are detailed in the next section.

Oﬄine Programming of Palletizing Robots
685
5
Modules Conceptual Architecture
The conceptual architecture of the two developed modules is presented in this
section. Firstly, the “Palletizing System Simulator” module will be presented,
followed by the “Robot Trajectory Generation System” module.
5.1
Palletizing System Simulator
The conceptual architecture of the simpliﬁed robot’s programming module for a
palletization task follows the scheme of Fig. 2.
Fig. 2. Palletizing System Simulator architecture scheme
As the ﬁgure shows, an Init button is used to start the process. After that,
the OnSelection.py function was developed to wait for the selection of the picking
point, by clicking on the corresponding components (which must be a simulator
predeﬁned type of object - a product), and placing points (which must also be
a simulator predeﬁned type of object - a pallet). If the selected components are
diﬀerent than expected, the function rejects the selections.
After the selections are made successfully, the function stores the coordinates
of the selected objects which are passed to the Palletize.py function. Besides the
mentioned coordinates, this function has two input ﬁles: Conﬁgs.txt, which has
information about the dimensions of the products to be transported and the
dimension of the pallet on which the palletization will occur; and Mosaic.txt,
which contains the points that deﬁne the products mosaic to be palletized. As
output the function sets the program instructions for the robot’s movements and
gripper actuation, and starts the simulation.
5.2
Robot Trajectory Generation System
For the generation of collision free trajectories the work presented by [8] was con-
sidered. This planner is similar to the ones from the A* family and has associated
the need of space discretization and the deﬁnition of the kinematics equations
that rule the robot’s movement. Based on this algorithm, the implemented func-
tionality follows the architecture scheme of Fig. 3.

686
R. Silva et al.
For the generation of the robot’s collision free trajectory, the A* Path button
is pressed to start planning. Thus, the PathPlanning.py function ﬁrstly receives
the start and end point coordinates of the trajectory from the OnSelection.py
function, resulting from the selection process referred in Subsect. 5.1. These are
deﬁned as points above the product to be palletized and above the chosen pallet,
respectively, meaning that the path planning is made between these two points
of the palletizing cell. Then, the algorithm process starts. For its operation, the
RobotKinematics.py function is ﬁrst identiﬁed, which is responsible for handling
the kinematics of the robot. For its turn, the CollisionDetector.py function ﬁnds
information of collision occurrences in the 3D robot workspace using the simula-
tor own collision detector. This way, it is possible to identify collisions between
the set composed of the robot, gripper and transported product, and any other
component part of the palletizing cell in each discretized space. After that, all
the discretized joint space information required for A * algorithm’s logic is stored
in a data structure whose query functions are deﬁned in DataHandler.py.
Finally, as Fig. 3 shows, the output path points are handled by Palletize.py.
This function, described in Subsect. 5.1, is responsible for its inclusion in the
robot’s movement instructions for the palletizing process.
Fig. 3. Robot Trajectory Generation System architecture scheme
6
Tests and Results
This section presents the obtained results with the two developed modules. A
simple palletizing cell, depicted in Fig. 4(a), is used as an example. This cell has
a 100 kg payload KUKA palletizing robot, two separate picking point feeders
(identiﬁed by the green numbers) and two pallets representing two diﬀerent
placing points (identiﬁed by the red numbers). Also, a customized menu for
this work was deﬁned and added to the simulator top tab menu to access the
developed functions of the plugin (see Fig. 4(b)).

Oﬄine Programming of Palletizing Robots
687
(a) Palletizing cell
(b) Customized project simulator menu
Fig. 4. Palletizing cell example for testing purposes
As in the previous section, this one is divided in order to present the results
of the “Palletizing System Simulator” module in the ﬁrst subsection, and the
“Robot Trajectory Generation System” module in the second one.
6.1
Palletizing System Simulator
With the developed functions mentioned in Subsect. 5.1, it actually becomes
possible to program the palletizing robot in a very simple way, just by selecting
the pick and place points of the product to palletize. The process starts with
the press of the Init button, located in the Palletize tab of the project simula-
tor menu (Fig. 5(a)). This action starts the simulation automatically until the
products of both feeders reach the two picking points (Fig. 5(b)). Selecting the
product at the desired picking location (Fig. 5(c)), sets the coordinates of the
picking point for the robot’s instruction generation (Fig. 5(e)). Then, by clicking
on the desired pallet, the coordinates of the placing point are also set (Fig. 5(d)).
As stated in Subsect. 5.1, the exact coordinates of the placing points (Fig. 5(f))
are deﬁned in the Mosaic.txt ﬁle. After this last selection, the simulation resets
and only the feeder corresponding to the deﬁned picking point works and the
robot starts the palletization process. In Figs. 5(g), (h) and (i) three moments
of a three layer palletization process can be seen.
6.2
Robot Trajectory Generation System
Path planning comes as a result of the developed application described in Sub-
sect. 5.2. The ﬁrst test performed was the trajectory planning for the palletizing
task shown in Fig. 5. After selecting the pick and place points and clicking the
A* Path button located in the Path Planning tab of the project simulator menu,
the algorithm process starts. When it ﬁnishes, it returns the points of the dis-
cretized space that must be included in the trajectory. To illustrate the result of
the planned trajectory for the given example, Fig. 6(a) shows the points of the
discretized space included in the resulting trajectory shown in Fig. 6(b).

688
R. Silva et al.
(a) Init button to start the
process
(b) Products stop at picking
points
(c) Picking point selection
by clicking in the product
(d) Placing point selection
by clicking in the pallet
(e) Robot’s generated pick-
ing points
(f) Robot’s generated plac-
ing points
(g) Palletization: ﬁrst layer (h)
Palletization:
second
layer
(i)
Palletization:
complete
pallet
Fig. 5. Palletizing System Simulator steps
The second test performed to the trajectory planning plugin was its behav-
iour in collision situations. Thus, metallic fences were located in the robot’s
workspace, resulting the layout depicted in Fig. 7. After the algorithm applica-
tion under these conditions, the obtained result is shown in Fig. 8. Again, in
Fig. 8(a) are illustrated the points added to the trajectory and in Fig. 8(b) the
resulting trajectory.
Finally, in order to compare the two illustrated cases, Fig. 9 shows the result
of the collision’s detector inclusion in the application architecture. As can be
seen, the robot’s movement is adjusted having a collision-free trajectory.

Oﬄine Programming of Palletizing Robots
689
(a) Trajectory added points
(b) Resulting trajectory
Fig. 6. Planned trajectory for the palletizing example of Fig. 5.
Fig. 7. Palletizing cell with fences as obstacles
(a) Trajectory added points
(b) Resulting trajectory
Fig. 8. Planned trajectory in a palletizing cell with obstacles

690
R. Silva et al.
(a) Without collision detector
(b) With collision detector
Fig. 9. Trajectory planning comparison
7
Conclusions
The use of robotic palletizing systems has been increasing in the FMCG industry.
Due to marked demands, most of these companies are changing their produc-
tion processes from low variety/high volume to high variety/low volume, which
demands frequent reprogrammings of their equipments with the inherent pro-
duction breakdowns and also the need to frequently customize and/or change the
palletizing solutions. To overcome these problems, this work proposes a solution
based on the principles of integrating advanced modularity techniques into the
design and development of automated palletizing solutions.
This paper addressed the development of a system for the automated oﬄine
generation of collision free robot programs for palletizing applications in the
scope of this project. It has been detailed the automatic generation of robot
programs, based on the speciﬁcation by the user of the picking and placing posi-
tion coordinates and on the pallet mosaic conﬁguration, and the robot collision
free trajectory generation system for the implementation of this program. Based
on preliminary tests, the system works as predicted and has been considered
user friendly by the people who have tested it. However, future improvements
have to be performed in the collision free trajectory generation system in order
to lower its computational burden and allow to decrease the computational time
required to compute collision free trajectories.
Acknowledgements. This work is ﬁnanced by the ERDF – European Regional
Development Fund through the Operational Programme for Competitiveness and Inter-
nationalisation – COMPETE 2020 under the PORTUGAL 2020 Partnership Agree-
ment, and through the Portuguese National Innovation Agency (ANI) as a part of
project AdaptPack — POCI-01-0247-FEDER-017887.
References
1. Yaman, H., Sen, A.: Manufacturer’s mixed pallet design problem. Eur. J. Oper. Res.
186, 826–840 (2008)

Oﬄine Programming of Palletizing Robots
691
2. Yu, S., Lim, S., Kang, M., Han, C., Kim, S.: Oﬀ-line robot palletizing simulator
using optimized pattern and trajectory generation algorithm. In: 2007 IEEE/ASME
International Conference on Advanced Intelligent Mechatronics, Zurich, pp. 1–6
(2007)
3. Lim, S., Yu, S., Han, C., Kang, M.: Palletizing simulator using optimized pattern and
trajectory generation algorithm. In: Di Paola, A.M.D., Cicirelli, G. (eds.) Mecha-
tronic Systems Applications. InTech. (2010)
4. Zhang, L., et al.: Layout analysis and path planning of a robot palletizing production
line. In: 2008 IEEE International Conference on Automation and Logistics, Qingdao,
pp. 2420–2425 (2008)
5. Scheurer, C., Zimmermann, U.E.: Path planning method for palletizing tasks using
workspace cell decomposition. In: 2011 IEEE International Conference on Robotics
and Automation, Shanghai, pp. 1–4 (2011)
6. Chiba, R., Arai, T., Ueyama, T., Ogata, T., Ota, J.: Working environment design
for eﬀective palletizing with a 6-DOF manipulator. Int. J. Adv. Robot. Syst. 13,
1–8 (2016)
7. Luan, N., Zhang, H., Tong, S.: Optimum motion control of palletizing robots based
on iterative learning. Ind. Robot Int. J. 39(2), 162–168 (2012). https://doi.org/10.
1108/01439911211201627
8. Tavares, P., Lima, J., Costa, P.: Double A* path planning for industrial manipula-
tors. In: Robot 2015: Second Iberian Robotics Conference: Advances in Robotics,
Lisbon, vol. 2, pp. 119–130 (2016)

Manipulation

Estimating Objects’ Weight in Precision Grips
Using Skin-Like Sensors
Francisco Azevedo, Joana Carmona, Tiago Paulino, and Plinio Moreno(B)
Institute for Systems and Robotics (ISR/IST), LARSyS, Instituto Superior T´ecnico,
Univ Lisboa, Torre Norte Piso 7, Av. Rovisco Pais 1, 1049-001 Lisboa, Portugal
francisco.n.azevedo@gmail.com, joana.pcarmona@gmail.com,
tiago paulino @hotmail.com, plinio@isr.tecnico.ulisboa.pt
http://vislab.isr.tecnico.ulisboa.pt/
Abstract. The estimation of object’s weight is a very challenging prob-
lem due to limitations on tactile technology and robust and fast con-
trollers that can adapt to friction state changes. In this article we study
the weight estimation using skin-like tactile sensors, which provide accu-
rate 3 dimensional force measurements on the ﬁnger tips of Vizzy, a
robot with human-like hands. The sensor reading from the ﬁngertips are
analyzed and segmented in order to ﬁnd the most adequate stages of
the movement for the computation of the weight. The analysis is based
on the diﬀerence of the load force between the grasping and holding
up stages, which provides a good estimation of the object’s weight for
diﬀerent objects and various repetitions of the experiments.
Keywords: Weight estimation · Humanoid robots · Tactile sensors ·
Vizzy robot
1
Introduction
Humans are able to execute manipulation actions that aim at adapting the grip-
ping force while grasping objects, using rough weight guesses from vision as
initial estimation, followed by tactile aﬀerent control loop that provides robust
and precise grasp such as the precision grip. The capability of a fast adaptation
using the tactile sensors relies on the high density of tactile aﬀerents (about 140
aﬀerents/cm2) in the ﬁngertips and the specialized action-phase controllers [3],
which allow to sense accurately large areas of the objects when compared to the
current technologies for robotic hands tactile sensing. Nevertheless, studies on
grip control and slippage detection for robotic hands have shown the plausibil-
ity of haptic feedback for robots in simpliﬁed experimental setups. In addition
to the technological limitations, adaptive object manipulation requires robust
switching control algorithms and independent-mode controllers that provide a
fast response, and at the same time model unstable states such as grasping in
the presence of dynamic friction. All these challenges are closely related to the
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_57

696
F. Azevedo et al.
weight estimation of objects by humanoid robots in uncontrolled environments,
which is the long-term objective of our work.
Fig. 1. Example of the initial robot conﬁguration before the execution of a precision
grip experiment for weight estimation
In this article we address the weight estimation of objects, by executing
manipulation actions with the humanoid robot Vizzy [4], which has two hands
very similar to their human counterparts. Figure 1 shows the robot right at the
beginning of a weight estimation experiment. Although the robot was mechan-
ically designed for power grasps, Vizzy is able to execute precision grips for
medium size objects. We focus on the weight estimation of objects during the
execution of precision grips, using skin-like tactile sensors at the ﬁngertips [5].
The sensors provide an estimation of the force from the changes in the magnetic
ﬁeld, considering three main elements: (i) a 3 dimensional hall eﬀect sensor, (ii)
a magnet and (iii) a silicon cover for the magnet. The changes in magnetic ﬁeld
due to the deformation of the silicon part are mapped onto 3 dimensional forces,
which provide the tactile perception to the silicon cover. These 3 dimensional
forces estimated by the sensor are analyzed over time in order to ﬁnd the diﬀer-
ent stages of the precision grip execution. The sequence of stages is as follows: (1)
initial positioning of the hand around the object, (2) object grasping and lifting,
(3) holding, (4) returning the object to the initial position and (5) returning
the robotic hand to the initial position. These stages are segmented for all the
sensors that touch the object, and the estimation of the weight of each object
was based on the assumption that the diﬀerence between the forces exerted by
the robotic hand in the stages 2 and 3 along the three directions sum up to
the weight of the object (i.e. the change in the load force due to gravity). In
addition, we consider that the friction force keeps a constant value during the
stages 2 and 3, which is valid if there is no relative movement between the object
and the sensors (no slips nor oscillations). Therefore, the friction component of
the force is removed by calculating the diﬀerence between the forces in stages 2
and 3.

Estimating Objects’ Weight Using Skin-Like Sensors
697
We evaluate the weight estimation algorithm on two objects with diﬀerent
size and shape, and also increasing the weight of the objects by adding water.
The results show that the tactile sensor provide good estimation of the weight
while having skin-like skills.
2
Related Work
Recent developments in tactile sensing such as BioTac1, OptoForce2 and the
skin-line magnetic-based [5] sensors have opened the door to explore the chal-
lenging area of grip control [1,7] and slippage detection [6] for robotic hands.
Since autonomous grip control using tactile sensors is still an open research prob-
lem due to technology limitations and development of controllers that are able to
operate in diﬀerent conditions such as the changes between static and dynamic
friction. A robust set of controllers should be able to switch between states and
adaptively change the control reference in order to manipulate autonomously
objects with diﬀerent materials and shapes. Thus, the autonomous weight esti-
mation is a very challenging problem that has to consider physical properties
(i.e. static and dynamic friction), autonomous switching between control modes
and the contact points on the object. Thus, the weight estimation of objects is
usually done in simpliﬁed settings where for instance the object shape ﬁts very
well the gripper shape and the robot arm is carefully designed for the weight
estimation by following the moving crane approach [2]. In that work, the manip-
ulation action is a power grasp where the friction problems are not present,
but the authors are able to estimate online the object weight in a very short
time (0.5–0.7 s). In [2], the voltage signals of the load cell (sensors) are analyzed
oﬄine in order to characterize the diﬀerent manipulation stages. Then, asso-
ciations between the voltage response and the manipulation stages lead to an
ad-hoc algorithm for the segmentation of the signal. Finally, the average value of
the signal in the selected interval is utilized to learn the parameters of a regres-
sion function that maps load cell voltages onto weights. As in [2], we analyze
the signal response over time and identify the manipulation stages. However, we
address a more challenging manipulation problem, the two-ﬁngertip precision
grip of a humanoid robot hand, where the friction issues arise. In the following
we describe the characteristics of the hand.
3
Vizzy’s Hand Design
In this work is used the robot Vizzy [4] to perform the grasps. Vizzy was designed
as a human assistant for social interaction tasks. Vizzy has an anthropomorphic
upper body with similar degrees of freedom and motion execution skills of a
human. Regarding its hands, the palm and ﬁnger sizes and number of limbs are
1 https://www.google.com/patents/US7658119.
2 https://optoforce.com/ﬁle-contents/OMD-20-SE-40N-DATASHEET-V2.2.pdf?
v14.

698
F. Azevedo et al.
also similar to an adult person, but having only four ﬁngers capable of grasping
objects. The thumb and index ﬁngers are actuated each one by a single motor,
while the middle and ring ﬁngers are coupled to one motor. The motor of a ﬁnger
is coupled to a pulley, that pulls a ﬁshing line string. The ﬁshing line string is
attached from the pulley to the last limb of the ﬁnger, such that the motion of
one motor moves in an under-actuated manner the three limbs of each ﬁnger.
In this work we only used two ﬁngers: thumb and index, in order to perform
the precision grasp. Regarding the sensors, the thumb has three sensors and the
rest of the ﬁngers have four sensors each. The sensors are presented in orange in
Fig. 2 and the ones used in this experiment are numbered from 1 to 4.
Fig. 2. Indexes of the force sensors in Vizzy’s hand
These tactile sensors [5] are composed by a soft elastomer body with a small
permanent magnet inside. Below the magnet there is a magnetic ﬁeld sensing
element (i.e. Hall-eﬀect sensor). When an external force is applied on the elas-
tomer, the relative magnet position changes and the Hall-eﬀect sensor detects
the magnetic ﬁeld variation, that can be converted in a measure of the applied
force. An air gap is left between the elastomer and the magnetic sensor in order
to increase the sensitivity for small forces. The use of a 3-axis Hall-eﬀect sensor
allows the detection of the magnetic ﬁeld variations along the 3 axis, meaning
that the sensor is capable of measuring the force magnitude and direction in
3D. The presented tactile sensors are dependent on the contact area, that is
unknown. The feedback of the measured Hall sensor provides the magnetic ﬁeld
vector. To achieve the force vector, some assumptions of the contact area are
needed during the calibration process. The sensors are calibrated for a contact
with a plane surface perpendicular to the Z axis of each sensor. The 2 sensors
near the ﬁngertips are covered with same elastomer piece but each sensor has its
own individual calibration, made with a planar surface on top of that sensor.

Estimating Objects’ Weight Using Skin-Like Sensors
699
4
Experiment Setup
The experimental tests were performed using two diﬀerent objects (plastic con-
tainers) with similar shapes and surface characteristics but slightly diﬀerent sizes
(Fig. 5). In order to increase the variability of the test objects regarding the vari-
able of interest, these objects were used in two diﬀerent conﬁgurations: empty
and partially ﬁlled with water. The movement executed by the robotic arm dur-
ing data acquisition can be divided into a series of sequential steps described in
the Introduction (1) (Fig. 4).
Fig. 3. Test objects: object 1 (left) and object 2 (right).
(a) Stage 1
(b) Stage 2
(c) Stages 3 and 4
(d) Stage 5 (beginning)
(e) Stage 5 (end)
Fig. 4. Movement phases: 1-initial position; 2-object grasping and lifting; 3-holding;
4-landing the object; 5-return to the initial position.
At the beginning of the test (stage 1, on Fig. 4a), the palm of the robotic
hand was placed perpendicularly to the surfaces of the object where the contact
would be established. The initial position of the object and the hand was deﬁned
according to two main criteria. On the one hand, there was no initial contact

700
F. Azevedo et al.
between them in order to assure that the sensors would not detect any signiﬁcant
force at this stage of the sequence. On the other hand, the relative position of the
thumb and the ﬁnger had to be optimized in a way that the grip forces exerted
by these ﬁngers were approximately perpendicular to the surface of the object
and parallel to the palm of the robotic hand. This was achieved by setting
the thumb to an abducted position and the index ﬁnger in opposition to it.
According to the GRASP Tanoxomy [1], this type of grasp corresponds to a
precision grasp with pad opposition, which is naturally executed by humans to
grasp small objects. The optimization of the initial relative position of the hand
and the object has proven to be critical for the success of the grasp, particularly
to prevent oscillations in posterior stages of the movement and to guarantee
the appropriate contact between the sensors and the object. The following stage
of the movement (stage 2, on Fig. 4) consisted in the movement of the thumb
and the ﬁnger against the surface of the object without changing signiﬁcantly
their initial conﬁguration. The ﬁnal position of the ﬁngers at this stage was
tuned to optimize the compromise between minimizing the potential occurrence
of slipping events during the lifting phase while not inducing any signiﬁcant
degree of deformation in the object. The following stage of the sequence was the
lifting and holding of the object (stage 3, on Fig. 4). During these phases, the
most relevant issue was to minimize the motion artifacts resulting from small
oscillations of the object, which was accomplished by controlling the velocity of
the movement of the robot’s joints. Finally, both the objects and the robotic
hand were returned to their initial positions (stages 4 and 5 on Fig. 4) and the
robotic hand also returned to the conﬁguration described in the ﬁrst stage. The
sequential movement was repeated over several trials for each one of the objects
and the acquisition of the data from the sensors was performed using the Arduino
Nano board. The raw data consisted of the magnitude of the forces along the
three directions measured in each sensor’s reference frame. Matlab was used for
real-time visualization and monitoring of the results as well as post-acquisition
signal processing and extraction of the results.
5
Results
Figure 5 displays the results obtained in one of the trials performed with the
test object 1, consisting of the magnitude of the three components of the force
(Fx, Fy and Fz) over time, for one of the sensors of the robotic hand. The
sampling frequency used for data acquisition was 20 Hz and each trial lasted
for approximately 12.5 s, which was enough to perform the previously described
sequence of movements and to obtain an appropriate number of samples in each
one of the stages. The estimation of the weight of each object was based on the
assumption that the diﬀerence between the forces exerted by the robotic hand
in the stages (2) and (3) along the three directions sum up to the weight of the
object. Another necessary assumption is that the friction (and resultant force)
is constant during the movement, which is valid if there is no relative movement
between the object and the sensors (no slips nor oscillations). Therefore, the fric-
tion component of the force is nulliﬁed by calculating the diﬀerence between the

Estimating Objects’ Weight Using Skin-Like Sensors
701
Fig. 5. Illustrative example of the results obtained for one trial performed with test
object 1. The graphic represents the magnitude of the three components of the force
(Fx, Fy, Fz) for one sensor over time as the sequence of movements is executed. The
vertical lines along the time axis represent the temporal sequence of stages that con-
stitute the overall movement: (1) initial position of the robotic hand; (2) lifting and
grasping (3) holding; (4) returning the object to its initial position (in contact with
the table); (5) opening the robotic hand.
forces in stages (2) and (3). It was necessary to manually identify stages (2) and
(3) for each one of the trials and compute the average force along the directions
X, Y and Z for sensors 1, 2, 3 and 4 during those stages of the movement. The
weight of the object was then estimated as the sum of the absolute diﬀerence
between the average load and grip forces measured during stages (3) and (2),
respectively Eq. 1. The mass was computed according to Eq. 2, where m stands
for the mass of the object in grams, FT otal is the diﬀerence between the average
forces in stages (2) and (3) that was identiﬁed as the weight of the object and
g is the acceleration due to gravity. Table 1 summarizes the results of the mass
for objects 1 and 2 in both conﬁgurations.
FT otal =
4

i=1
[|F 3
x,i −F 2
x,i| + |F 3
y,i −F 2
y,i| + |F 3
z,i −F 2
z,i|]
(1)
m = 1000FT otal
g
(2)
The experimental results are a reasonable approximation of the actual mass
of both objects with water. However, the standard deviation for these objects
is considerable, which is mainly due to ﬂuctuations in the initial positioning
of the object that resulted in a non-optimal contact between the sensors and
the surface of the objects that compromised data acquisition. Moreover, due to
technical constraints, it was not possible to achieve the desired number of tri-
als. On the other hand, the experimental mass of the test objects in the empty

702
F. Azevedo et al.
Table 1. Experimental mass of test objects 1 and 2 in both conﬁgurations (E-Empty
and W-With Water)
Object Index Object
Conﬁguration
Mass (g) Experimental
Mass (g)
Standard
Deviation (g)
1
E
40.0
73.8
8.1
W
94.0
94.6
13.8
2
E
26.0
52.0
19.2
W
76.0
78.5
7.4
conﬁguration exhibits a signiﬁcant deviation from its actual value. Since the
sequence of movements was maintained for both conﬁgurations, it was veriﬁed
that, regarding the test objects in the ﬁlled conﬁguration, the added weight
resulted in more stable contact points and, in general, more reproducible mea-
surements. This fact contributes for the diﬀerence in accuracy observed between
the experimental results for both conﬁgurations. Another relevant conclusion is
the systematic overestimation of the mass for both objects and conﬁgurations.
This overestimation can be explained by the acquisition of data from two sen-
sors at each ﬁngertip that do not contact the surface of the object in an optimal
position. In fact, as can be observed in [reference to an image that illustrate the
grasp], the contact is established in an intermediate position between the two
sensors, which represents a deviation from the optimal position (at the center
of the sensor’s surface). Nevertheless, if only one sensor was used under similar
circumstances, an underestimation of the mass would be expected.
6
Conclusions and Future Work
This experimental work allowed us to obtain satisfactory results regarding the
estimation of objects’ weight on precision grips using skin-like sensors integrated
on the humanoid robot Vizzy. However, more accurate and reproducible results
will require an optimization of the experimental protocol concerning the posi-
tioning of the contact points between the sensors and the object’s surface. For
future work, one possible direction would be the adaptation of the data process-
ing to achieve an estimation of the weight of deformable objects, for which the
assumption of a constant friction force during the holding stage of the movement
is no longer valid. Another relevant direction would be a more autonomous data
processing in order to allow a real-time identiﬁcation of the stages of the move-
ment useful for weight estimation. This would be a crucial advancement towards
an online estimation of the objects’ weight, which could ultimately be used for
real-time adjustments of the grip forces in order to avoid the occurrence of slip
events during grasping. The elastomer body that contains the sensor is not tai-
lored to measure weight. In an ideal scenario the sensor is at equal distance
to the surface of contact. A spherical shape would be more suitable for this
measurements instead of the one portrayed in Fig. 2.

Estimating Objects’ Weight Using Skin-Like Sensors
703
Acknowledgements. This work was supported by FCT [UID/EEA/50009/2013],
partially funded by POETICON++ [STREP Project ICT-288382], the FCT Ph.D.
programme RBCog and FCT project AHA [CMUP-ERI/HCI/0046/2013] and IEEE-
IST EMBS Student Chapter Colab Sessions.
References
1. Chitta, S., Sturm, J., Piccoli, M., Burgard, W.: Tactile sensing for mobile manipu-
lation. IEEE Trans. Robot. 27(3), 558–568 (2011)
2. Copot, D., Ionescu, C., Nascu, I., De Keyser, R.: Online weight estimation in a
robotic gripper arm. In: 2016 IEEE International Conference on Automation, Qual-
ity and Testing, Robotics (AQTR), pp. 1–6. IEEE (2016)
3. Johansson, R.S., Flanagan, J.R.: Coding and use of tactile signals from the ﬁngertips
in object manipulation tasks. Nat. Rev. Neurosci. 10(5), 345–359 (2009)
4. Moreno, P., Nunes, R., de Figueiredo, R.P., Ferreira, R., Bernardino, A., Santos-
Victor, J., Beira, R., Vargas, L., Arag˜ao, D., Arag˜ao, M.: Vizzy: a humanoid on
wheels for assistive robotics. In: Robot (1), pp. 17–28 (2015)
5. Paulino, T., Ribeiro, P., Neto, M., Cardoso, S., Schmitz, A., Santos-Victor, J.,
Bernardino, A., Jamone, L.: Low-cost 3-axis soft tactile sensors for the human-
friendly robot vizzy. In: 2017 IEEE International Conference on Robotics and
Automation (ICRA) (2017)
6. Su, Z., Fishel, J.A., Yamamoto, T., Loeb, G.E.: Use of tactile feedback to control
exploratory movements to characterize object compliance. Front. Neurorobot. 6,
1–20 (2012)
7. Wettels, N., Parnandi, A.R., Moon, J.-H., Loeb, G.E., Sukhatme, G.S.: Grip control
using biomimetic tactile sensing systems. IEEE/ASME Trans. Mechatron. 14(6),
718–723 (2009)

Kinematic Estimator for Flexible Links
in Parallel Robots
Pablo Bengoa1, Asier Zubizarreta1(B), Itziar Cabanes1, Aitziber Mancisidor1,
and Charles Pinto2
1 Automatic Control and System Engineering Department,
University of the Basque Country (UPV/EHU),
Ingeniero Torres Quevedo Plaza, 1, 48013 Bilbo, Spain
{pablo.bengoa,asier.zubizarreta}@ehu.eus
2 Mechanical Engineering Department,
University of the Basque Country (UPV/EHU),
Ingeniero Torres Quevedo Plaza, 1, 48013 Bilbo, Spain
Abstract. Control of ﬂexible link parallel manipulators is still an open
area of research. The ﬂexibility and deformations of the limbs make the
estimation of the Tool Center Point (TCP) position a non-trivial area,
being one of the main challenges on this type of robots. In the literature
diﬀerent approaches to estimate this deformation and determine the loca-
tion of the TCP have been proposed. However, most of these approaches
require the use of high computational cost integration methods or expen-
sive measurement systems. This work presents a novel approach which
can not only estimate precisely the deformation of the ﬂexible links (less
than 3% error), but also its derivatives (less than 4% error). The validity
of the developed estimator is tested in a Delta Robot, resulting in less
than 0.025% error in the estimation of the TCP position in comparison
with the results obtained with ADAMS Multibody software.
Keywords: Kinematic estimator · Flexible link · Parallel robot
1
Introduction
Nowadays, higher quality and smaller production time is required in industry
due to globalization. However, for those robots traditionally used in industry, the
serial robots, the increase in speed implies, usually, a loss in accuracy. Hence,
when both, quality and speed need to be met, the use of Parallel Kinematic
Robots (PKR) is more appropriate [11]. These type of robots are composed by
multiple kinematic chains which connect a ﬁxed or base platform and a mobile
one, where the Tool Center Point (TCP) is located. This structure provides
higher precision, stiﬀness and load/weight ratio [3].
Trying to achieve higher productivity, manufacturers have reduced the cross-
section of the limbs in order to decrease the moving mass of the robot. This,
in addition to the high accelerations demanded in low operation cycles causes a
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_58

Kinematic Estimator for Flexible Links in Parallel Robots
705
certain degree of elastic deformation of the limbs [3]. These deformations have
inﬂuence in the kinematics and dynamics of the robots, causing in some cases
substantial errors in motion control. Hence, the tracking of the TCP’s trajectory
is one of the most challenging problems caused by elastic deformation, since it
must be compensated so that the accuracy requirements are fulﬁlled.
When stiﬀlinks are considered, the location of the TCP is usually deter-
mined by the kinematic model of the robot, which uses the data from the sen-
sors attached to the actuated joints of the robot. In the case of ﬂexible robots,
however, the TCP location depends not only on the joint positions but also on
the deformation of its ﬂexible links. Therefore, as the deformation of diﬀerent
ﬂexible elements must be considered, the stiﬀelement assumption may not be
accurate enough [14].
In the literature, diﬀerent types of sensors have been considered for measuring
link ﬂexibility. One of the most accurate approaches is the use of artiﬁcial vision
[4]. However, most of visual servoing approaches require the robot and image
Jacobian to map end-eﬀector velocity [9], and, in ﬂexible manipulators, these
Jacobians need the information of the ﬂexible variables [12], so they still need
to be measured [7]. Hence, even if precise global data can be derived from vision
systems, their use is limited by its computational cost, the low sampling rate
and the eﬀects of visual obstruction.
To overcome these limitations, accelerometers and strain gauges have been
widely used. Accelerometers are usually placed at the tip of each ﬂexible link, and
their measurements are traditionally used to stabilize the oscillation of the TCP
by vibration control approaches [13]. Hence, it is possible to estimate the TCP
location of the manipulator if joint motions are sensorized [6], since the tip accel-
eration contains both information of ﬂexible-link vibrations and the rigid-body
motions. This approach, however, requires to integrate the acceleration signal
twice for velocity and position estimations which provide noisy measurements
and may contains biases, resulting in high accumulation of errors [5].
On the other hand, the use of strain gauges has been widely used to measure
local deformation along the link. Strain gauges are an attractive approach since
they do not require the knowledge of dynamic parameters, such as link masses or
inertias, to estimate the deformation of the ﬂexible links. However, strain gauges
suﬀer from biasing due to the electromagnetic interferences and they are prone
to temperature variations and measurement noises [1].
Thus, even with the use of additional sensors, measurement of the deforma-
tion is not a trivial task. Based on this fact, some authors have proposed the use
of sensor fusion techniques to improve the measurement. In [8], the forward kine-
matics of a 6 degrees of freedom (DOF) robot is estimated by using an extended
Kalman ﬁlter to process accelerometer and encoder data, while in [16] the fusion
of accelerometer and encoder signals using a disturbance observer to compensate
the nonlinearities of the deformation was proposed. However, even though the
use of sensor fusion techniques improves the accuracy of the measurement, those
techniques require, usually, high computational load [15].

706
P. Bengoa et al.
In this work, a novel approach to estimate the deformation of ﬂexible links is
proposed for control purposes. The approach is based on the fusion of a mathe-
matical model and a single high resolution optical encoder. Using this measure,
the position and orientation of the TCP of ﬂexible robots can be estimated
accurately. In addition, the proposed approach estimates the deformation of the
ﬂexible links with minimal computational cost, providing signiﬁcant advantages
for Real-Time implementation over the aforementioned approaches.
The rest of the paper is organized as follows: Sect. 2 outlines both the concept
and theoretical development of the presented estimator. In Sect. 3, the theoretical
approach is applied into a Delta robot and its simulation results are discussed
in Sect. 4. Finally, the most important ideas are summarized.
2
Theoretical Development of the Kinematic Estimator
In robotics, the Direct Kinematic Model allows to calculate the trajectory in
the operational space x(t) for any given joint space trajectory q(t), this is,
f(q(t)) = x(t). Therefore, it is easy to see that for any ﬂexible link manipulator,
the direct kinematic equation admits multiple solutions, as the deformation of
the link provides more than one end-eﬀector position/orientation for the same
conﬁguration of joints q (Fig. 1).
Fig. 1. A ﬂexible manipulator with the same conﬁguration of rigid joints qr(q1, q2)
but multiple end-eﬀector position/orientation due to the ﬂexibility of the links,
qf(qfd, qfs).(qfs link tip transverse ﬂexural deﬂection and qfs link tip ﬂexural slope).
Therefore, the Direct Kinematic Model has to be deﬁned not only in terms
of rigid joint motion variables qr but also in terms of the ﬂexible variables qf,
x(t) = f(qr, qf)
(1)
Measurement of rigid joints can be easily carried out using extra sensors
attached to joints. However, as stated in the introduction, the measurement of
the ﬂexible DOF can be a challenging task. In this section, a novel estimator
based on a normal mode analysis approach is proposed which allows to estimate,
with minimal computational cost, the deformation with high accuracy.

Kinematic Estimator for Flexible Links in Parallel Robots
707
2.1
Fundamentals of the Estimator
Normal mode analysis is a mathematical tool which represents a pattern of
motion in which all parts of a system move with the same frequency and ﬁxed
phase relation. Hence, this mathematical tool provides the kinematic relation
between the diﬀerent DOF of the system based on its dynamic properties, such as
their structure, material and boundary conditions. Moreover, the general motion
of a system is composed as a superposition of its normal modes, being all normal
modes orthogonal to each others, since the excitation of one mode will not aﬀect
to a diﬀerent mode. This way, if a particular mode is considered and the modal
motion of a single degree of freedom is measured, the complete set of DOF qDOF
modal motions can be estimated,
qDOF =
nDOF

k=1
Xk ϵk
(2)
where Xk is the kth mode shape or eigenvector and ϵk is the kth modal motion
[10].
Several researchers [2] have suggested to consider only the ﬁrst few modes
in the model neglecting high frequency since their amplitude are much smaller.
Furthermore, in control applications, the bandwidth of the working frequencies
is limited by the application itself and the actuator system. This, usually ensure
that even the second lowest resonant frequency of the manipulator is out of the
working bandwidth in all robot workspace. So, the deﬂection of the limbs can be
estimated considering just the ﬁrst natural frequency mode of the manipulator.
Hence, (2) can be simpliﬁed to,
qDOF =
nDOF

k=1
Xk ϵk ≈X1 ϵ1
(3)
This constitutes the basis of the proposed estimator. Lets assume that a
robot is composed by a series of ﬂexible links i = 1, . . . n connected with several
stiﬀlinks. If the modal analysis of each ﬂexible link is carried out based on the
previous considerations, the ﬁnal deﬂection of the ith link can be estimated with
a properly placed sensor that measures a single ﬂexible DOF.
Consider, for instance, the example of Fig. 2, where the ﬂexible link b is
connected with two stiﬀlinks (a, c) using rotary joints. If Euler-Bernouilli Beam
Theory and Finite Element Method approach is used to model the ﬂexible link,
one of the ﬂexible variables, qfsi, would deﬁne the deformation slope at the tip
of the ﬂexible link. If this angular deformation is measured by high resolution
optical encoder and its data is introduced in (3), by substituting ϵ1 = qfsi,
the total deﬂection at the tip of the link could be estimated, and therefore the
kinematics of the TCP.
The developed approach presents several advantages over previous ones.
First, since each ﬂexible link is considered, the obtained kinematic relation of
the deformations is constant for a given mode, resulting in a low computa-
tional cost approach. Hence, the normal modes of each link are independent of

708
P. Bengoa et al.
Fig. 2. Schematic of a ﬂexible link (b) connected with two stiﬀlinks (a, c) using
rotatory joint.
the manipulator conﬁguration which allows to calculate the required matrices
oﬀ-line, reducing the time to compute link deformation. Second, the approach
provides the required accuracy for control purposes, as long as the deﬂections are
small enough (small deﬂection assumption). Finally, the relationship holds not
only for the position problem, but also for velocity and acceleration problems,
as vector Xk deﬁnes the relationship between the kth mode of the ﬂexible DOFs
motion of the of the ith link.
Hence, the full kinematic relations between the ﬂexible variables are,
qfi = nDOF
k=1
Xki ϵki ≈X1i ϵ1i
˙qfi = nDOF
k=1
Xki ˙ϵki ≈X1i ˙ϵ1i
¨qfi = nDOF
k=1
Xki ¨ϵki ≈X1i ¨ϵ1i
(4)
In the following section, the procedure to calculate the generalised inertia
matrix and the stiﬀness matrix of each link is detailed, so that the modal analysis
and their eigenvalues can be calculated.
2.2
Modelling of Flexible Links
When modelling ﬂexible link robots, one of the most used approaches is Finite
Element Method (FEM). According to this procedure, each ﬂexible link i is
an assemblage of a ﬁnite number, ni, of small elements of length li which are
interconnected at certain points called nodes. Each element is referenced as ij,
where subscript j denotes the number of the element, being the more number of
elements per link (hence, smaller elements), the more precise the overall solution
of the system will be, making it converge to the exact solution as precisely as
desired at the cost of higher computational cost.
In order to solve the dynamic problem, the energy of each element is to
be analysed using the Euler-Lagrange method. However, before calculating the
potential and kinematic energy, the position vector r0
i of each element must
be deﬁned (Fig. 3). For simplicity, this vector will be expressed in terms of a
local coordinate vector ri and then translated it into the inertial frame. This is
accomplished by using the transformation matrix T i
0,

Kinematic Estimator for Flexible Links in Parallel Robots
709
Fig. 3. Schematics of the DOF of each link’s element.
r0
i = T 1
0
⎡
⎣
L1
0
u2n1+1
⎤
⎦+ T 2
0
⎡
⎣
L2
0
u2n2+1
⎤
⎦+ . . . + T i
0 ri
(5)
where,
ri =
⎡
⎣
(j −1) li + xij
0
zij
⎤
⎦
(6)
According to Euler-Bernoulli beam theory, all elements possess two DOF at
each end of the element: a transverse ﬂexural deﬂection (u2j−1 and u2j+1) and
a ﬂexural slope (u2j and u2j+2) (Fig. 3). These ﬂexible DOF are related by the
shape functions φk(x), which describe the ﬂexural displacement z, as
z(x, t) =
4

k=1
φk(x) u2j−2+k(t)
(7)
where, for element j of the ith link the shape functions are deﬁned by Hermitian
polynomials,
φ1(x) = 1 −3 x2
l2
ij
+ 2 x3
l3
ij
,
φ2(x) = x −2 x2
lij
+ 2 x3
l2
ij
φ3(x) = 3 x2
l2
ij
−2 x3
l3
ij
,
φ2(x) = −x2
lij
+ 2 x3
l2
ij
(8)
where lij is the length of the element and the boundary conditions to be satisfy
by the displacement variables x and z(x) are,
z(0) = u2j−1,
∂z(0)
∂x
= u2j, z(l) = u2j+1,
∂z(l)
∂x
= u2j+2
(9)
After deﬁning the position vector, potential energy Vij and the kinetic energy
Tij of each element are computed in terms of the generalised variables of the

710
P. Bengoa et al.
system q = (q1, q2, ...qn)T and their time derivatives ˙q. Afterwards, total kinetic
T and potential V energies for the entire system is obtained by summing those
energies. This is,
T(q, ˙q) =
m

i=1
ni

j=1
Tij
V (q) =
m

i=1
ni

j=1
Vij
(10)
where m is the number of links and Tij is the kinetic energy which is obtained
by,
Tij = 1
2
 lij
0
ρi Ai
∂rT
∂t
∂r
∂t
	
dx = 1
2 ˙zT
j Mij ˙zj
(11)
where matrix Mij is the generalised inertia matrix of element ij. This matrix is
symmetrical and each (k, o) element of it is deﬁned as,
Mij(k, o) =
 li
0
ρi Ai
 ∂r
∂zjk
	T  ∂r
∂zjk
	
dxij,
o, k = 1, 2, ...nq
(12)
where ρi is the mass density, Ai is the cross-section area of the element, zjk is
the kth element of zj = [qr, u2j−1, u2j, u2j+1, u2j+2]T , nq is the number of the
variables of the system and qr = [q1, q2...qnr] is the vector of variables associated
to the stiﬀmodel.
Following a similar procedure, the overall potential energy of the system
is obtained, (10). The potential energy is divided into two components: the
potential energy due to the elasticity (Veij) and the potential energy due to the
gravity (Vgij). This is,
Vij = Vgij + Veij =
 lij
0
ρij Aij g [0 0 1] r dxij + 1
2
 lij
0
E Ii
∂2yij
∂x2
ij
2
dxij
(13)
By rewriting the elasticity term of (13), the following expression is obtained,
Veij = 1
2 zT
j Kij zj
(14)
where Kij is the stiﬀness matrix of the j element of the ith link.
In order to determine the deﬂection of each link, the generalised stiﬀness and inertia
ﬂexible submatrices (Kif and Mif , respectively) have to be deﬁned. These matrices
are obtained by selecting the submatrices associated to the ﬂexible variables qf from
the generalised inertia and stiﬀness matrices.
After obtaining the generalised inertia submatrix Mif and the stiﬀness submatrix
Kif of each limb, the Boundary Conditions (BC) of each ﬂexible link have to be deﬁned,
this is, the deﬂection (w), the slope (w′), the shear force (v) and/or the bending moment
(M) in both ends of each link (x = 0 and x = lTi). Thus, depending on the link’s
conﬁguration, diﬀerent BC are deﬁned.

Kinematic Estimator for Flexible Links in Parallel Robots
711
2.3
Modal Analysis of the Flexible Links
After calculating the inertia and stiﬀness submatrices, the modal analysis procedure
can be applied. For that purpose, a reduced set of equations of motion is required in
which the damping and the applied load are not taken into account,

Mif

BC ¨q +

Kif

BC q = 0
(15)
To solve (15), a harmonic solution based on (16) is proposed.
qf = X sin(ω t)
(16)
where ω is the natural frequency vector.
The harmonic solution deﬁnes the deﬂection relationship between all the DOF of
the link. Therefore, since the structural conﬁguration does not change during motion,
for a given mode is possible to relate the eﬀects of ﬂexibility on each DOF if just one
DOF amplitude is measured. This is the key concept to develop the proposed estimator,
as stated in the beginning of Sect. 2.
By substituting (16) into (15) and simplifying it, the following equation is obtained.

Kif

BC −ω2
i

Mif

BC
	
Xi = 0
i = 1, 2, 3...nDOF
(17)
where the number of eigenvalues and eigenvectors nDOF is equal to the number of DOF
of the discretised limb has.
Hence, by solving (17), the eigenvector Xi can be calculated. This eigenvector, as
it has been shown in (4), represents the relationship between the ﬂexible DOF of the
link. As stated in the introduction of Sect. 2, this calculation can be carried out oﬀ-line,
since the inertia and stiﬀness submatrices do not depend on the robot’s position. This
way, if one of the ﬂexible DOF is measured, the rest can be estimated based on the
calculated eigenvalues.
3
Case of Study: Delta Robot
In the following section the developed estimator will be implemented into a ﬂexible-link
parallel Delta Robot in order to validate it. For that purpose, the procedure detailed
in the previous section will be applied.
3.1
The Delta Robot Description
The Delta robot is a widely used, three DOF parallel robot whose lightweight structure
provides fast dynamic capabilities, being mainly used for pick and place applications.
This robot is composed of 3 uniformly distributed arms (i = 1, 2, 3) that connect
the mobile platform with the ﬁxed base. Each of these arms is composed by two limbs:
the upper one is connected to the ﬁxed base by an actuated rotational joint called qai
(for i = 1, 2, 3) and the lower ones are composed by an articulated parallelogram which
allows passive rotations in two directions (qnai and αi for i = 1, 2, 3), as it is shown in
Fig. 4. This conﬁguration limits the motion of the TCP, which is located in the mobile
platform, to a pure three dimensional translational motion.
For the study case, the main parameters of the robots (Table 1) have been obtained
from an Omron Mini Delta CR-UGD4MINI-NR commercial robot, whose ﬂexible model
has also been implemented in ADAMS Multibody Software.
In order to increase the deformations, AW5083/H111 aluminium platens, whose
mechanical properties are summarised in Table 2, have been used in the lower limbs.

712
P. Bengoa et al.
Fig. 4. Robot schematic with the most important parameters and variables.
Table 1. Parameters of Omron Mini Delta CR-UGD4MINI-NR
Length (m) Mass (kg) Inertia (kg m2)
Fixed base
|ai| = 0.100
Upper link Li
0.150
0.0365
ILixx = 2.2781 10−6 ILiyy = 8.7001 10−5 ILizz = 8.6422 10−5
Lower link lTi
0.400
0.1319
Ilixx = 2.0023 10−6
Iliyy = 0.0010
Ilizz = 0.0010
Mobile platform |di| = 0.040 0.1278
Ipxx = 4.6225 10−5
Ipyy = 4.6225 10−5
Ipzz = 9.1472 10−5
3.2
Simulation Setup
In order to analyse the ﬂexible behaviour of the ﬂexible links and validate the proposed
approach, a ﬂexible simulation model of the robot developed in ADAMS Multibody
Software has been excited with a periodical movement in the z axes. Hence, a sinusoidal
motion has been applied in the actuated joints qai,
qa1 = qa2 = qa3 = 0.15 −0.1 sin(6 π t)
(18)
Two factors have been considered to carry out the validation of the proposed estima-
tor. First, the deformation estimation capability of the developed approach is analysed.
Based on FEM, certain number of nodes compose each ﬂexible link. Each of those nodes
have two DOF, the deﬂection displacement and the slope deﬂection. Hence, in order
to analyse the accuracy of the approach, the estimation of each DOF is compared with
the data obtained from ADAMS Multibody Software. Since the developed sensor not
only estimates the position and the orientation of the DOF, but also the speed and the
acceleration of them, the accuracy of estimator will be analysed in position, speed and
acceleration.
Results are shown in Fig. 5. As it can be seen the estimation is accurate either in
position, speed and acceleration. Hence, the developed approach is able to estimate the
deﬂection of the tip with a maximum error of less than 0.005 mm, which is about 3% of
the maximum deﬂection amplitude, less than 10−4 m/s (4%) in speed estimation error
and around 2 · 10−3 m/s2 (4%) in acceleration estimation error.
Table 2. Mechanical properties of the aluminium platens.
Length
Width
Thickness Young’s modulus Mass density
0.400 m 0.015 m 0.003 m
71 GPa
2740 Kg/m3

Kinematic Estimator for Flexible Links in Parallel Robots
713
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
−1
−0.5
0 x 10
−5
(a) Position Estimation Error at the end effector
Time [s]
Deflection [m]
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
−1
0
1
x 10
−4
(b) Speed Estimation Error at the end−effector
Time [s]
Deflection [m/s]
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
−5
0
5 x 10
−3
(c) Acceleration Estimation Error at the end−effector
Time [s]
Deflection [m/s]
Fig. 5. Estimator link deﬂection estimation compared with the real deﬂection at the
tip of the link obtained with ADAMS Multibody Software: (a) position, (b) speed and
(c) acceleration.
On the other hand, the performance of the estimator calculating the Direct Kine-
matic Problem will be also evaluated. This model is derived from the loop closure
equations of the mechanism (Fig. 6). For the deﬁned Delta robot, these equations are,
Γi(x, q) = ai + Li(qa) + lTi(qa, qna, αi) + δi −di −px = 0
(19)
where i = 1, 2, 3 is the link identiﬁcation and δi is the variable to be estimated which
deﬁnes the deﬂection distance in the ﬂexible link end which respect to its analogous
rigid link.
Hence, the position of the TCP of the robot, is calculated by solving px. Figure 7
shows the error between the estimated TCP position and the obtained from the ﬂexible
ADAMS Multibody Software model for the x, y and z component. As it can be seen,
the estimation error of the developed approach rises up to 8 · 10−6m for the x axis,
Fig. 6. Graphical representation of the loop closure of a Delta Robot.

714
P. Bengoa et al.
Fig. 7. TCP deﬂection estimation error done by the developed approach.
1.7 · 10−6m for the y axis and 0.5 · 10−6m for the z axis, which means an error of less
than the 0.025%, 0.005%, 0.001% of the applied movement, respectively.
4
Conclusions
This work presents a novel approach for the deformation estimation of ﬂexible links.
The approach is based on the use of modal analysis, the Euler-Bernouilli Beam The-
ory and the Finite Element Method approach. This estimator, which is focused on
Parallel robots control applications, provides a accurate and computationally eﬀective
estimations in comparison with other approaches.
The proposed estimator has been validated in a ﬂexible Delta Robot which has been
modelled on ADAMS Multibody Software. The deformation data obtained by the esti-
mator was compared with the one obtained by ADAMS, resulting in a maximum error
of less than 3%, 4% and 4% for the deformation position, speed and acceleration esti-
mation, respectively. Furthermore, the TCP position has also been estimated, resulting
in a maximum TCP error of 0.025%.
Acknowledgement. This work was supported in part by the Spanish Ministry of
Economy and Competitiveness under grant BES-2013-066142, UPV/EHU’s PPG17/56
projects, Spanish Ministry of Economy and Competitiveness’ MINECO & FEDER
inside DPI-2012-32882 project and the Basque Country Government’s (GV/EJ)
under PRE-2014-1-152 and BFI-2012-223 grants and under recognized research group
IT914-16.

Kinematic Estimator for Flexible Links in Parallel Robots
715
References
1. Bascetta, L., Rocco, P.: End-point vibration sensing of planar ﬂexible manipula-
tors through visual servoing. Mechatronics 16(3–4), 221–232 (2006). doi:10.1016/
j.mechatronics.2005.11.005
2. Book, W.: Modeling, design, and control of ﬂexible manipulator arms: a tutorial
review. In: 29th IEEE Conference on Decision and Control, pp. 500–506, vol. 2.
IEEE (1990). doi:10.1109/CDC.1990.203648
3. Chen, X., Li, Y., Deng, Y., Li, W., Wu, H.: Kinetoelastodynamics modeling and
analysis of spatial parallel mechanism. Shock Vib. 2015, 1–10 (2015). doi:10.1155/
2015/938314
4. Corke, P.I., Hutchinson, S.A.: Real-time vision, tracking and control. In: 2000
IEEE International Conference on Robotics and Automation, 1 April, pp. 622–629
(2000). doi:10.1109/ROBOT.2000.844122
5. Dubus, G.: On-line estimation of time varying capture delay for vision-based vibra-
tion control of ﬂexible manipulators deployed in hostile environments. In: 2010
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 3765–
3770. IEEE (2010). doi:10.1109/IROS.2010.5651211
6. Gu, M., Asokanthan, S.F.: Combined discrete-distributed control of a single-link
ﬂexible manipulator using a Lyapunov approach. J. Dyn. Syst. Meas. Contr.
121(3), 448 (1999). doi:10.1115/1.2802495
7. Gu, M., Piedboeuf, J.C.: A ﬂexible-link as an endpoint position and force detection
unit. IFAC Proc. Volumes (IFAC-PapersOnline) 15(1), 361–366 (2002). doi:10.
1016/S0967-0661(03)00105-9
8. Henriksson, R., Norrlof, M., Moberg, S., Wernholt, E., Schon, T.B.: Experimen-
tal comparison of observers for tool position estimation of industrial robots. In:
Proceedings of the 48th IEEE Conference on Decision and Control (CDC) Held
Jointly with 2009 28th Chinese Control Conference, pp. 8065–8070. IEEE (2009).
doi:10.1109/CDC.2009.5400313
9. Hutchinson, S., Hager, G., Corke, P.: A tutorial on visual servo control. IEEE
Trans. Robot. Autom. 12(5), 651–670 (1996). doi:10.1109/70.538972
10. Maia, N.M.M., Montalvao e Silva, J.M.J.M.: Theoretical and Experimental Modal
Analysis. Research Studies Press (1997)
11. Merlet, J.: Parallel Robots, 2nd edn. Springer Science & Business Media (2006)
12. Piedboeuf, J.C.: The jacodian matrix for a ﬂexble manipulator. J. Robotic Syst.
12(11), 709–726 (1995). doi:10.1002/rob.4620121102
13. Rodriguez-Donate, C., Morales-Velazquez, L., Osornio-Rios, R.A., Herrera-Ruiz,
G., Romero-Troncoso, R.d.J.: FPGA-based fused smart sensor for dynamic and
vibration parameter extraction in industrial robot links. Sensors 10(4), 4114–4129
(2010). doi:10.3390/s100404114
14. Rognant, M., Courteille, E., Maurine, P.: A systematic procedure for the elastody-
namic modeling and identiﬁcation of robot manipulators. IEEE Trans. Rob. 26(6),
1085–1093 (2010). doi:10.1109/TRO.2010.2066910
15. Trejo-Hernandez,
M.,
Osornio-Rios,
R.A.,
de
Jesus
Romero-Troncoso,
R.,
Rodriguez-Donate, C., Dominguez-Gonzalez, A., Herrera-Ruiz, G.: FPGA-based
fused smart-sensor for tool-wear area quantitative estimation in CNC machine
inserts. Sensors 10(4), 3373–3388 (2010). doi:10.3390/s100403373
16. Zheng, J., Fu, M.: A reset state estimator using an accelerometer for enhanced
motion control with sensor quantization. IEEE Trans. Control Syst. Technol. 18(1),
79–90 (2010). doi:10.1109/TCST.2009.2014467

Tactile-Based In-Hand Object Pose Estimation
David ´Alvarez1(B), M´aximo A. Roa2, and Luis Moreno1
1 Robotics Lab, Carlos III University of Madrid, Leganes, Spain
{dasanche,moreno}@ing.uc3m.es
2 German Aerospace Center - DLR, Wessling, Germany
maximo.roa@dlr.de
Abstract. This paper presents a method to estimate the pose of an
object inside a robotic hand by exploiting contact and joint position
information. Once an initial visual estimation is provided, a Bootstrap
Particle Filter is used to evaluate multiple hypothesis for the object
pose. The function used to score the hypothesis considers feasibility
and physical meaning of the contacts between the object and the hand.
The method provides a good estimation of in-hand pose for diﬀerent 3D
objects.
Keywords: Robotic grasping · Object pose estimation · Tactile sensing
1
Introduction
Grasp planning is generally considered as the computation of the locations where
the ﬁngers should contact an object in order to obtain a stable grasp. However,
those theoretical locations are often hard to reach in real life due to uncertainties
in the sensing, modeling, planning or execution phases, leading to a hand-object
conﬁguration diﬀerent from the planned one.
In-hand object pose estimation is an online cognitive process that humans
perform while grasping or manipulating objects. Information acquired from dif-
ferent sources (including action, vision, tactile, and semantic knowledge) is linked
in an interactive manner [1]. Tactile sensing is specially important when the
object is inside the hand; in fact, experiments have shown that humans fail to
accurately perform manipulation tasks when their sense of touch is impaired [2].
The problem of ﬁnding the pose of an object when it is grasped by a robotic
hand has become a topic of high interest. A common approach starts with a
vision-based pose estimation, which is later improved by using information com-
ing from tactile sensors in the hand. In [3] the vision estimate is reﬁned by
This work has received funding from the Spanish Ministry of Economy, Industry and
Competitiveness under the projects DPI2013-47944-C4-3-R and DPI2016-80077-R,
and the RoboCity2030-III-CM project, stage III, S2013/MIT-2748), cofunded by
“Programas de Actividades I+D en la Comunidad de Madrid”, and by Structural
Funds of the EU.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_59

Tactile-Based In-Hand Object Pose Estimation
717
minimizing the distance from the object’s point cloud to the contact locations
in the hand. An oﬄine description of the object’s facets to ﬁnd possible combi-
nations that match the current sensor measurements is used in [4]. The idea of
preventing physically unfeasible solutions by discarding collisions between the
object and the hand was included in [5] with an estimation based on a particle
ﬁlter. A similar idea was presented in [6], in which a manifold of state space
that respects the contact constraints is used for a 2D analysis when the hand is
pushing an object. An approach that uses arrays of tactile sensors to recognize
the object and its 6D pose by exploring the object’s surface and edges, using a
particle ﬁlter combined with an Iterative Closest Point approach, was used in [7].
Estimation of an object’s pose combining stereo vision and a force-torque sensor
mounted on the wrist of a robot was reported in [8]. It also used joint positions
to estimate the location of the ﬁngers with respect to the object’s faces, an idea
also exploited in [9] in the context of a global search of the pose of the object.
Statement of contributions: this work presents the in-hand estimation of the
6D pose for a known object by exploiting tactile and joint information in the
robotic hand, starting with an initial vision-based estimation of the pose. Based
on ideas presented in [5] for 2D objects, geometrically meaningful constraints
are included in a particle ﬁlter, where each particle represents a pose hypothesis
of the object. In comparison to previous works, the estimation not only uses the
contact locations, but also the force measurements provided by pressure-based
sensors in the hand. Moreover, properties of a stable grasp are included in the
cost function that evaluates the best estimation.
2
Problem Description
The overall process of a grasp execution starts with a visual estimation of the
object’s pose in the robot frame. This is used to plan the grasp, and then the
robotic arm is generally sent to a pre-grasp pose. The initial pose estimation of
the object can be described with respect to the reference frame of the wrist by
x = [q, t]T = [qx, qy, qz, qw, tx, ty, tz]T
(1)
where q is a rotation expressed as a quaternion, and t is a translation vector.
The grasping process is then triggered, and the ﬁngers start closing towards
the object. When any contact with the object is detected, the pose estimation
starts (after that moment the object might be moved by the hand). Therefore,
the problem of ﬁnding the pose of an object with respect to the hand (wrist)
can be formulated as ﬁnding a set of parameters that deﬁne a transformation
(1) that matches the information provided by the tactile sensors.
The ReFlex TakkTile hand used in this work (Fig. 1) is equipped with two types
of sensors: pressure sensors located along the ﬁngers (9 along each ﬁnger) and the
palm (11 sensors), and magnetic encoders in the proximal joint and the tendon
that moves the distal joint, which allow computing the location of both phalanges
in each ﬁnger [10]. Therefore, the position of the tactile sensors with respect to the

718
D. ´Alvarez et al.
Fig. 1. Experimental setup for in-hand object pose estimation. A reference frame
parallel to the wrist reference frame is shown in the lower right corner.
wrist and the normal vector to the surface of each sensor can be inferred. The sen-
sors deliver force measurements based on the pressure transmitted by the rubber
that covers the ﬁngers. However, since the pressure ﬂows through the rubber, one
single contact with an object may be detected by two consecutive sensors. When
this happens, a linear combination of measurements is performed to compute the
actual contact location ci with respect to the wrist, as follows:
ci = ti +
1 −
fi
fi + fi+1
 × (ti+1 −ti)
(2)
where ti is the position of sensor i and fi its force measurement. Note that this
assumes that only one contact per link with the object is possible.
The initial pose estimation of the object with respect to the hand is obtained
by computing the relative transformation between two Apriltags located in the
hand wrist and on the object (Fig. 1). Assuming that the CAD model of the
object is available, the problem of in-hand pose estimation is tackled by com-
bining the following general ideas:
I When a contact is detected by a sensor, the estimated position of the object
must produce a contact at the same location. The opposite situation is illus-
trated in the left side of Fig. 2, where the estimated object pose cannot
explain the contact readings in two ﬁngers.
II The estimated position of the object should not be in collision with the hand
(just in contact). Besides, the object cannot ﬂoat without contacting the
hand at all when at least one sensor reading is positive.
III The inward normal of the object surface at the contact location and the
outward normal at the contact surface in the hand should have the same
direction. When friction is considered, the normals do not necessarily have
to be aligned, but, since the friction coeﬃcient is in general not known, the
angle between normals should be as close to 0 as possible (Fig. 2).
3
Implementation of the Particle Filter
A particle ﬁlter is a type of Bayesian ﬁlter, a general probabilistic approach for
recursively estimating an unknown probability density function using incoming

Tactile-Based In-Hand Object Pose Estimation
719
Fig. 2. Left: two contacts are detected (red arrows), but the estimated pose of the
object does not produce contacts. Right: friction cone at a contact location.
measurements and a mathematical modeling process. One of the most pop-
ular methods for performing Bayesian ﬁltering is the Kalman Filter, usually
applied to optimally solve a linear and Gaussian dynamic model [11]. However,
for highly non-linear and non-Gaussian problems it fails to provide a reasonable
estimate [12]. Besides, Kalman ﬁlters (regular or extended) perform better with
an analytic description of the measurement model [5], not available here.
The estimation method used here is the Bootstrap Particle Filter, a type of
Sequential Monte Carlo method (SMC) [13]. These are a set of simulation-based
methods that provide a convenient approach to compute posterior distributions
sequentially, avoiding linearity or normality assumptions. In order to mathemat-
ically deﬁne how the Bayesian ﬁltering works, let us start by deﬁning two sets
of random (vector) processes:
Xt := x(0), ..., x(t)
Yt := y(0), ..., y(t)
(3)
where Xt is a set of random variables of interest and Yt is a set of measurements
of the process of interest. Bayes’ theorem for conditional distribution can be
expressed as
Pr(Xt|Yt) = Pr(Yt|Xt) × Pr(Xt)
Pr(Yt)
.
(4)
The principal idea under SMC techniques is representing a probability dis-
tribution as a set of samples from this distribution. Therefore, the more samples
taken from the distribution, the better representation is achieved. Besides, if
the system is assumed to be Markovian, using Sequential Importance Sampling
(SIS), an estimate of ˆPr(Xt|Yt) can be computed using the past simulated sam-
ples Xt−1 ∼q(Xt−1|Yt−1) [14]. The desired sequential solution of the update of
the weight of each particle at each time-step can be written as:
Wi(t) = Wi(t −1) × Pr(Yt|Xi,t−1) × Pr(Xi,t|Xi,t−1)
q(Xi,t|Xi,t−1, Yt)
(5)
where i refers to each particle. This weighting update enables the formulation
of a generic Bayesian Sequential Importance Sampling algorithm, a constrained
version of importance sampling.

720
D. ´Alvarez et al.
Finally, in order to avoid the possible degeneration of the particles, i.e. that
most particles have a weight value of 0, the Bootstrap Particle Filter uses Sequen-
tial Importance Resampling (SIR) to eliminate particles with low importance
weights while preserving particles with large probabilities [12]. After this step,
the weights of the new particles are all set to be equal to 1/N, which means
they all have the same importance. Algorithm 1 shows the implementation of
the Bootstrap Particle Filter used in this work.
Algorithm 1. Bootstrap Particle Filter
1: procedure BPF(Np, prior estimation)
Initialization:
2:
xi(0) ∼Pr(x(0)),
Wi(0) =
1
Np
Importance Sampling:
3:
xi(t) ←system model(xi(t −1), inputt)
4:
Wi ∼Pr(Wi(t))
Weight Update:
5:
Wi(t) = Wi(t −1) × measurement(y(t)|xi(t))
Weight Normalization:
6:
Wi(t) =
Wi(t)
Np
i=1 Wi(t)
Resampling:
7:
if ˆ
Neff(t) ≤Nthresh, then ˆxi(t) ⇒xj(t)
8: end procedure
3.1
Importance Sampling: System Model
In a Particle Filter, the system model relates the state at time i with respect
to the state at time i −1 and the input to the system at time i. In this case,
since the state is the object’s pose, this relation is given by the displacement. In
order to compute the object’s displacement based on ﬁnger movements, only the
location of sensors that detect contact is taken into account. Let si = [xi, fi >
contact threshold] be a sensor location with positive contact. Then, Δxi =
[Δqi, Δti]T is the displacement and rotation of sensor i between two consecutive
readings. The total object movement is then computed as the average translation
and rotation, Δxia, among the sensors in which a contact is detected.
3.2
Weight Update: Measurement Model
The measurement model gives to each of the particles a weight that quantiﬁes
how similar is the state expressed by that particle to the truth, based on the
measurements provided by the position and tactile sensors in the hand. The
measurement model used in this work is inspired by [5]. Three new features have
been added: ﬁrst, not only the sensor location but also the force measurements
are used to compute the real contact locations. Second, the evaluation method

Tactile-Based In-Hand Object Pose Estimation
721
considers diﬀerently each sensor depending on whether it is in contact or not.
And third, the friction cone of a contact is considered to evaluate the feasibility
of a contact direction with the object.
The estimation method works in parallel with the grasping execution using
a simulated environment. The Flexible Collision Library (FCL) [16] is used to
compute the shortest distance (no collision, positive value) or deepest penetra-
tion (in collision, negative value) between each sensor and the object. Taking
into account this information, three kind of measurements are considered:
– For every sensor that does not detect contact with the object, a probability is
assigned to each particle (representing the object’s pose) based on its distance
to the object do
i by:
pnc,i(do
i ) = 0.5 ∗

1 + erf

do
i
√
2σnc

(6)
where σd is a standard deviation value that can be adjusted to match the
inaccuracy of the measurements, and erf corresponds to the error function.
This function is chosen to assign high weight values to positive distances
and small values to negative distances, which helps to avoid estimations that
predict unreal collisions. The resulting function is represented in Fig. 3.
– For each sensor that detects a contact, the distance do
i is used to associate a
probability to the measurement with:
pc,i(do
i ) = e
−0.5
 do
i
σc
2
(7)
where σc can be adjusted to account for the uncertainty in the force sensors.
This function assigns high weights to values that are close to zero, i.e. close
to contact. The function is represented in Fig. 3.
– Assuming the grasp is stable, the normal of the surface of the object at
the contact point (for the sensors that detect a contact) should lie within
the friction cone around each contact point in the hand. The contact force
measured by the hand is considered to be normal to its surface, therefore, the
angle αi between the normals to the surfaces can be computed, and afterward
evaluated with:
pa,i(α) = e−0.5(
αi
σa )
2
(8)
where σa accounts for the friction between the surfaces. This function assigns
high weights to values that are close to 0.
Finally, a combined weight for each particle (Wi) can be expressed as:
Wi =
Nm

k=1
pnc,i ∗pc,i ∗pa,i
(9)
where Nm is the number of measurements for each particle. This weight is cal-
culated for every particle during the update step.

722
D. ´Alvarez et al.
Fig. 3. Left: weight function used when a contact is not detected, σnc = 0.005 m.
Right: weight function used when a contact is detected, σc = 0.005 m.
4
Experimental Setup
The ReFlex TakkTile Hand was used for experimental tests. Figure 1 shows the
setup when the hand is about to start closing the ﬁngers. Note that both hand
and object have Apriltags [17] on their surfaces (36 h 11 family and 0.0163513 m
of side length), which are used to compute the ground truth pose of the object
with respect to the hand (but are not used in the pose estimation process). The
experiments follow the next steps:
– Hand and object are placed on a table surface, and the scene is perceived
with an Asus Xtion Pro sensor.
– Data recording is started, an operator picks up the hand and positions it at a
seemingly good location to execute the grasp. The fact that the robotic hand
is held by a human operator has no inﬂuence on the estimation provided by
the ﬁlter (since this work deals with the estimation of the pose of an object
with respect to the hand, no robot arm is actually needed to prove the beneﬁts
of the technique).
– 10 seconds later, the ﬁngers close around the object. Once all of them are in
contact, a constant “closing” velocity is maintained in all ﬁngers to make the
grasp stable (Non-stable grasps are not studied in this work).
– The particle ﬁlter is started as soon as the ﬁrst contact between any ﬁnger
and object occurs. The initial population is built out of the visual estimation
of the pose of the object (based on AprilTags), adding Gaussian noise in every
axis according to the expected error in the visual estimation.
– When the grasp is completed, the object is lifted to check if it is stable. New
estimations are computed until the hand is commanded to open.
One important issue when using a particle ﬁlter is deciding the number of
particles to use. It must be a compromise between a very big population, which
increases the likelihood of obtaining a good estimate, and a small one, which
allows higher frequency of the estimation process. In order to optimize this

Tactile-Based In-Hand Object Pose Estimation
723
Table 1. Average update time.
Particles Time [s]
100
0.3170
150
0.4787
200
0.6116
250
0.7727
300
0.9034
600
1.8772
1000
2.9871
Fig. 4. Variance of the estimated pose.
choice, the iteration period is analyzed against the number of particles by per-
forming 10 tests using 100, 150, 200, 250, 300, 600 and 1000 particles; the average
computation times on a standard desktop PC (Intel Core i5 with 4 cores) are
shown in Table 1. Time grows almost proportionally with the number of par-
ticles. Assuming a desirable minimum update frequency of 1 Hz, this is only
fulﬁlled with up to 300 particles.
Then, additional tests are performed to study the variance of the posterior
distribution over 10 diﬀerent repetitions, as shown in Fig. 4. Blue and red lines
correspond to the variance in the estimation of position and orientation, respec-
tively. Theoretically, as the number of particles grows, the estimation of the
particle ﬁlter is more accurate and the variance of the estimations should be
smaller. Results show a small change in variance because the number of parti-
cles changes very little. However, a trend can be appreciated: estimations get
better until 200 particles, when this improvement slows down or disappears. For
this reason, further tests are performed using 200 particles on three diﬀerent
objects selected from the YCB database [15]; their characteristics can be seen
in Fig. 5 and Table 2. The parameters used in the particle ﬁlter are summarized
in Table 3. For the input to the system (displacement between iterations), a
Gaussian noise, with 0 mean and a diagonal covariance matrix, is added both in
position and orientation. No artiﬁcial noise is added to the measurements, since
the data provided by the sensors is quite noisy itself. The values used in the
weighting function have been empirically chosen. For instance, σd and σc were
initially set to 0.5 mm, but this made all the particles to have a weight value
of 0 in most of the iterations, which is useless for the estimation process. On
the other hand, the greater the σ values are, the less importance is given to the
measurements.
4.1
Results
Figure 6 shows an example of the grasping process of the Pringles can. The ﬁrst
image presents the starting pose of the object and the hand. In the second image,

724
D. ´Alvarez et al.
Table 2. Characteristics of the objects
used in the experiments.
Object
Size [mm]
Weight [kg]
Cheez-it 60 × 160 × 230 0.453
Pringles 75 × 250
0.205
Jell-o
35 × 110 × 89
0.187
Fig. 5. Objects used for testing.
Table 3. Parameters used for the pose estimation with the proposed ﬁlter.
Input Noise
Position - Mean
0
Position - Standard Deviation
0.7 mm
Rotation - Mean
0
Rotation - Standard Deviation 0.35◦
Measurement Noise Mean
0
Standard Deviation
0
Weighting Function σd
1 mm
σc
1 mm
σa
8◦
the proximal links of the ﬁngers make contact with the object; the ﬁngers keep
closing until the distal link also makes contact, as shown in the third image.
In the fourth and ﬁfth images, the hand is robustly grasping the object, which
is lifted and turned towards the camera. Note that this movement does not
aﬀect the computation of the actual pose of the object with respect to the hand
(provided there is no slippage). Then, in the next images, the hand is moved
down, the object is placed back on the table, and the ﬁngers are opened.
The pose estimation process was repeated 10 times. Figure 7 shows the results
for this scenario, including the average of all the trials, and absolute values of
the estimation for position (cm) and rotation (◦) in the three axes. All the given
data are for poses of a frame at the center of mass of the object with respect to
the base of the hand; the orientation of the axes of this frame is indicated in the
ﬁrst image of Fig. 6, to help interpreting the results. As an output of the ﬁlter,
the best particle of the population at each iteration is drawn in a purple line.
Besides, the weighted average of the posterior and its standard deviation are
shown as a blue line and a shaded green area, respectively. Finally, the ground
truth (GT) computed using the Apriltag system is included as a red line.
Both the ground truth and the estimation show slight movements in X and
Y axis. The motion in Y axis can be considered as noise, since is has a range of
less than 1 mm. The motion in X axis is very likely induced by the asynchronous

Tactile-Based In-Hand Object Pose Estimation
725
(a) a
(b) b
(c) c
(d) d
(e) e
(f) f
(g) g
(h) h
Fig. 6. Pringles can, grasped from the side with a robotic hand held by a human
operator. The sequence is ordered left to right, top to bottom.
contact of the ﬁngers on the object, and by the force applied to stabilize the
grasp, reﬂected also in the Z axis.
Although the ground truth does not detect almost any rotation, the estima-
tions show slight variations around 0◦. Note that Y is the axis of revolution of
the cylinder. When no textures are considered for the 3D model, the object can
rotate around the Y axis while still fulﬁlling the geometrical constraints of the
contacts when being grasped, so rotations around this axis do not make any dif-
ference. Rotations estimated around X and Z axis are much more noisy, reaching
a maximum error of 1.2% (with respect to all possible orientations in 360◦).
Similar results are obtained for the other two objects. While the position is
generally well estimated, the estimation of rotations has higher uncertainties.
Table 4 summarizes the results of pose estimation for the three objects, compar-
ing the ground truth to the best and to the average particles. Average error and
standard deviation are presented for the errors in position and orientation.
These results can be compared, at least in a qualitative way, with other
similar works. [3,6] evaluate similar techniques to this one for 2D applications.
In both cases, average position errors are around 2 to 10 times bigger than ours,
while having similar computation times. Orientation errors are also around 5
to 10 times bigger. In [5], results in 2D for real experiments and in 3D for
simulated ones are presented. Their ﬁlter was tried with 5 to 80 particles, which
obviously leads to better computation times; however, this results in larger errors
in position and specially in orientation, which were improved here by adding
information on the force measurement and the orientation of the surfaces in
contact.

726
D. ´Alvarez et al.
0
2
4
6
8
10
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Time (s)
Position X (cm)
 
 
μ
σ
Best
GT
0
2
4
6
8
10
−6
−4
−2
0
2
4
6
Time (s)
Orientation X (°)
 
 
μ
σ
Best
GT
0
2
4
6
8
10
−4.4
−4.2
−4
−3.8
−3.6
−3.4
Time (s)
Position Y (cm)
 
 
μ
σ
Best
GT
0
2
4
6
8
10
−4
−2
0
2
4
6
Time (s)
Orientation Y (°)
 
 
μ
σ
Best
GT
0
2
4
6
8
10
14.7
14.8
14.9
15
15.1
15.2
15.3
15.4
15.5
15.6
15.7
Time (s)
Position Z (cm)
 
 
μ
σ
Best
GT
0
2
4
6
8
10
−8
−6
−4
−2
0
2
Time (s)
Orientation Z (°)
 
 
μ
σ
Best
GT
Fig. 7. Results of the pose estimation for the Pringles can. Left column corresponds
to positions in X, Y, and Z axes. Right column corresponds to rotations around the
same axes.

Tactile-Based In-Hand Object Pose Estimation
727
Table 4. Average errors and standard deviation in the pose estimation for three
selected objects.
Object
Particle
Position error
Orientation error
Pringles can
Best
μ = 0.2140 cm μ = 1.739◦
σ = 0.0583
σ = 0.0571
Average μ = 0.1759 cm μ = 2.153◦
σ = 0.0571
σ = 0.106
Jell-o box
Best
μ = 0.3704 cm μ = 3.392◦
σ = 0.1013
σ = 0.871
Average μ = 0.3638 cm μ = 3.508◦
σ = 0.10951
σ = 0.954
Cheez-it box Best
μ = 0.7310 cm μ = 0.812◦
σ = 0.2069
σ = 0.131
Average μ = 0.7187 cm μ = 0.905◦
σ = 0.2397
σ = 0.154
5
Conclusions
This paper proposes and implements an in-hand pose estimation algorithm to
detect the movement of the object that occurs while performing a grasp. The
estimation process is based on the measurements provided by the sensors of
the hand: pressure sensors for contact detection and encoders for proprioceptive
information. The measurement model relies on a virtual simulation of the rela-
tive position of the hand and the object, which uses a collision detection library,
to compare real measurements with simulation-based ones. The approach uses
a Bootstrap particle ﬁlter, selected mainly because of its ﬂexibility in the mea-
surement model.
The algorithm is tested in real grasping scenarios with three diﬀerent objects.
Results show that the main movements produced while stabilizing the grasp can
be estimated. The main deviations occur in the degrees of freedom in which there
is no measurement update. The estimation might be improved by introducing
a new measurement term that penalizes movements higher than those used in
the control input of the system. However, increasing the number of variables in
the measurement computation increases also the computation time required to
predict the behavior of the object. Also, the experiments presented here compare
results to a ground truth given by an Apriltag system, which greatly limited the
variability of motions. Further experiments using a better system for ground
truth prediction that allows larger motion variations is a future work.
An ongoing work is the combination of the proposed tactile scheme with
a visual-based pose estimator, robust to medium and high levels of occlu-
sions (which commonly happen after the object is grasped with the hand).
Visual information can also provide useful insights for the degrees of freedom
that cannot be detected with the tactile-based pose estimation.

728
D. ´Alvarez et al.
References
1. Macura, Z., Cangelosi, A., Ellis, R., Bugmann, D., Fischer, M., Myachykov, A.: A
cognitive robotic model of grasping. In: Proceedings of International Conference
on Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems, pp.
89–96 (2009)
2. Rothwell, J., Traub, M., Day, B., Obeso, J., Thomas, P., Marsden, C.: Manual
motor performance in a deaﬀerented man. Brain 105, 515–542 (1982)
3. Bimbo, J., Seneviratne, L., Althoefer, K., Liu, H.: Combining touch and vision
for the estimation of an object’s pose during manipulation. In: Proceedings of
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 4021–
4026 (2013)
4. Haidacher, S., Hirzinger, G.: Estimating ﬁnger contact location and object pose
from contact measurements in 3-D grasping. In: Proceedings of IEEE International
Conference on Robotics and Automation, pp. 1805–1810 (2003)
5. Chalon, M., Reinecke, J., Pfanne, M.: Online in-hand object localization. In: Pro-
ceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems,
pp. 2977–2984 (2013)
6. Koval, M., Dogar, M., Pollard, N., Srinivasa, S.: Pose estimation for contact manip-
ulation with manifold particle ﬁlters. In: Proceedings of IEEE/RSJ International
Conference on Intelligent Robots and Systems, pp. 4541–4548 (2013)
7. Aggarwal, A., Kirchner, F.: Object recognition and localization: the role of tactile
sensors. Sensors 14, 3227–3266 (2014)
8. Hebert, P., Hudson, N., Ma, J., Burdick, J.: Fusion of stereo vision, force-torque,
and joint sensors for estimation of in-hand object location. In: Proceedings of IEEE
International Conference on Robotics and Automation, pp. 5935–5941 (2011)
9. Bimbo, J., Kormushev, P., Althoefer, K., Liu, H.: Global estimation of an object’s
pose using tactile sensing. Adv. Rob. Syst. 29, 37–41 (2015)
10. Tenzer, Y., Jentoft, L., Howe, R.: The feel of MEMS barometers: inexpensive and
easily customized tactile array sensors. IEEE Rob. Autom. Magaz. 21(3), 89–95
(2014)
11. Kalman, R.: A new approach to linear ﬁltering and prediction problems. Trans.
ASME J. Basic Eng. 82, 35–45 (1960)
12. Doucet, A., de Freitas, N., Gordon, N.: An Introduction to Sequential Monte Carlo
Methods. Springer, New York (2001)
13. Candy, J.: Bootstrap particle ﬁltering. IEEE Signal Process. Mag. 24, 73–85 (2007)
14. Ristic, B., Arulampalam, S., Gordon, N.: Beyond the Kalman Filter: Particle Fil-
ters for Tracking Applications. Artech House, Norwood (2004)
15. Calli, B., Singh, A., Walsman, A., Srinivasa, S., Abbeel, P., Dollar, A.: The YCB
object and model set: towards common benchmarks for manipulation research. In:
Proceedings of IEEE International Conference on Advanced Robotics, pp. 510–517
(2015)
16. Pan, J., Chitta, S., Manocha, D.: FCL: a general purpose library for collision
proximity queries. In: Proceedings of IEEE International Conference on Robotics
and Automation, pp. 3859–3866 (2012)
17. Olson, E.: AprilTag: a robust and ﬂexible visual ﬁducial system. In: Proceedings
of IEEE International Conference on Robotics and Automation, pp. 3400–3407
(2011)

Legged Locomotion Robots

Study of Gait Patterns for an Hexapod Robot
in Search and Rescue Tasks
Jorge De Le´on(B), Mario Garz´on, David Garz´on-Ramos,
and Antonio Barrientos
Centro De Autom´atica y Rob´otica, UPM-CSIC, Calle Jos´e Guti´errez Abascal,
2, 28006 Madrid, Spain
{jorge.deleon,ma.garzon,antonio.barrientos}@upm.es, dgarzon@etsii.upm.es
Abstract. This paper presents a study about gait patterns for hexa-
pod robots with extremities called C-legs. The study analyses several
modes of gait that diﬀerent animals use to move through the terrestrial
environment, and another new ones that arise when looking at the lim-
itations that present the existing ones. The whole study is reinforced
with a series of simulations carried out, where the obtained results are
analysed to select the best gait pattern for a speciﬁc situation.
Keywords: Hexapod robots · C-legs · Gait patterns · USAR
1
Introduction
Urban Search and Rescue (USAR) is one of the most challenging applications
for mobile robotics. The complexity of the tasks and the scenarios where they
have to be carried on, require novel and robust solutions. This paper presents
a contribution towards this goal, by proposing a new approach for the use of
mobile terrestrial robots in this application.
One of the most problematic issues regarding the use of ground robots in
USAR applications is their locomotion mode. Most conventional ground robots,
which have locomotion based on wheels or caterpillars, are not suitable because
they may not be able to overcome the obstacles found in their way and therefore
they can not complete their missions. The new design presented in this paper is
intended as a guide for developments of new bio-inspired robots, and thus reduce
failure due to this and other type of diﬃculties.
A wide variety of bio-inspired robots may be found in literature, however, a
high percentage of those robots are based on the physiognomy of the hexapods.
The class of the hexapods is a classiﬁcation of the arthropods, and is the class
that groups the larger number of species, including among them a variety of
insects. The hexapods found in nature can have terrestrial or aerial locomotion,
however, robots inspired in them are mainly terrestrial. Hexapod locomotion is
widely used in robotics because of its robustness, stability and the facility in
which they are adapted to the ﬁeld of engineering.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_60

732
J. De Le´on et al.
There are many developments of hexapod robots, The FZI (Research Centre
for Information Technology) in Karlsruhe, Germany, has created the LAURON
family, whose development goes into the ﬁfth generation [1] (See Fig. 1a). This
robot is equipped with diﬀerent walking modes and is able to maintain stability
even on a terrain with numerous obstacles. Another example is the DIGbot robot,
designed by Case Western Reserve University in Cleveland, United States, and
whose leg design makes it possible to advance through tree trunks and vertical
walls, shown in Fig. 1b.
(a) Robot LAURON IV
(b) Robot DIGbot
(c) Robot RHex
Fig. 1. Example of hexapod robots
But, although the kinematics of this type of robots is well studied and very
simple to implement, the mechanical implementation and its control is very
complex [7], therefore, a new series of robots have been developed. These robot
continue being hexapods but present a new physiognomy in their extremities.
These new extremities have the shape of a letter C. Robots using this legs
can move at much higher speeds and they have less mechanical requirements.
Another advantage of this type of robots is their capacity of “locomotion without
legs” by which robots can overcome obstacles when none of their legs touch the
ground [2]. A robot that uses this type of design is the RHex (See Fig. 1c)
developed in USA under DARPA funding.
However, despite the great proliferation of robots using C-shaped legs, there
have not been studies that deepen into the kinematic of the legs ([5,8,10]).
Even tough this study is fundamental for this type of robots because it largely
conditions the process of design and construction.
The objective of this paper is to contribute in this regard, by presenting a
study of the diﬀerent modes and gaits used for locomotion of this type of robots.
The results of those studies have been validated through simulations, and they
provide insightful details as to how to work with this type of robots.
The structure of the article begins with this introduction, and then continue
describing the model of the robot. Subsequently, the gait patterns are deﬁned
and, ﬁnally, are the tests and simulations performed, ending with the conclusions
of the analysis.

Study of Gait Patterns for an Hexapod Robot in Search and Rescue Tasks
733
2
Design of the Robot and Its Legs
The robot that has been used is a prototype developed by the Robotics and
Cybernetics group of the Polytechnic University of Madrid ([8,9]).
This robot has been designed with a series of speciﬁcations to be able to
perform search and rescue tasks (USAR [6]) and also robotic protection of critical
infrastructures (PRIC [4]) in environments that present diﬀerent obstacles such
as buildings, industrial complexes, . . . where a conventional robot with wheels
cannot move around the area to be monitored.
Fig. 2. Render of the designed robot
The ﬁrst requirement of design is that the robot must be able to overcome a
stair, so according to the conclusions of the analysed works it is necessary that
the diameter of the leg is equal to or greater than the height of the step. So, to
make the leg, a 200 mm diameter acetal tube with 4.5 mm thick wall thickness
and a 50 mm wide width have been selected. Then the legs are cut in the form
of letter C.
With this imposition of design, a new restriction appears, the minimum
dimension of the robot, which is the sum of the three radii of the legs plus
the thickness of them.
lengthrobot > 3 · (diameterlegs + thicknesslegs)
(1)
lengthrobot > 613.5 mm
(2)
In addition to these two restrictions, there is another one depending in the
deployability of the robot. It should be classiﬁed as a man-portable or man-
packable robot.
Man-packable. Robot systems that can be carried in one or two backpacks
along with the person’s personal protective gear. Man-packable robots can be
further subdivided into micro and mini. Micro-robots ﬁt completely in one
pack and can be deployed in spaces where humans or canines cannot ﬁt.
Man-portable. Robot systems that can be lifted or carried by one or two people
but do not ﬁt on a backpack or would distort the person’s center of gravity.
Typically are robots tank-ike or with a manipulator arm.
With this, the resulting design is the one that can be observed in Fig. 2.

734
J. De Le´on et al.
3
Deﬁnition and Evaluation of the Gait Patterns
All the walking hexapods have the characteristic of being stable, both dynami-
cally and statically; that is to say that with the six extremities that they have a
perfect support on the ground, this way when withdrawing the support of one,
two or three extremities, the ﬁve, four or three remaining legs continue giving
sustenance. Which means, that the hexapod has advantages compared to bipeds
and quadrupeds by having more points of support and give an excellent support
at the time of generating diﬀerent patterns of walking.
Fig. 3. Scheme of the position and deﬁnition of the legs of our hexapod
3.1
Gait Patterns from Animals
Based on the observation of nature, it is possible to distinguish three types of gait
for the hexapods: alternate tripod, tetrapod and wave, whose walking patterns
can be observed in the Fig. 4.
Alternate Tripod: Legs I1, D2, and I3 (see Fig. 3) are moved simultaneously
for a period of time (either forward or backward) in the air, black area; While
the legs D1, I2 and D3 remain in contact with the ground having a movement
contrary to the other tripod, white area.
Tetrapod: The legs are moved in pairs of opposing extremities. That is, I1 and
D3 move (either forward or backward), black areas, while the rest remain on
the ground. When the cycle of I1 and D3 is ending the leg D2 start moving
in the same way as in the previous one. Then the legs I3 and D1 begin to
perform the same action as above but during their duty cycle. And, ﬁnally,
the leg I2 realizes the movement.
Wave: In this type of walking the legs are independent, each one moves for
a period of time (either forward or backward) and continues the adjacent
extremity.
3.2
New Gait Patterns
However, as the human has been demonstrated throughout history, is possible
to develop things that nature has not been able to. Therefore, in this section
are presented two novel gait modes that have not been found in any biological
study to date.

Study of Gait Patterns for an Hexapod Robot in Search and Rescue Tasks
735
Fig. 4. Gait patterns. (From top to bottom: Alternate tripod, tetrapod and wave)
The ﬁrst one (new mode 1) could be deﬁned as a variant of the tetrapod gait,
but in this case the combination of legs does not present a smooth transition.
Instead of setting the gait with a smooth transition between pairs of legs, a
pattern of movement will be generated in which each pair of legs will always
move, ﬁrst the front, then the intermediate, and ﬁnally the rear, and again the
pattern will start again.
When implementing this mode is necessary to have a good calculation of
the torques and eﬀorts generated in the pair that acts, because in this type of
movement there is not present a support as large as in other movements.
The second designed movement (new mode 2) may seem rather rudimentary
but nonetheless not less eﬀective, it is intended for very irregular terrain. In this
case all the legs of the robot will move together in each cycle, that means that
the robot will be completely supported and then to not present any support and
fall on the ground.
This movement may have the disadvantage of being aggressive with the robot
because of the large number of impacts it receives in each transition.
Fig. 5. New gait patterns.

736
J. De Le´on et al.
4
Simulations and Results
In order to obtain a complete analysis of the gait patterns, a series of tests have
been carried out where all the patterns are tested under diﬀerent conditions. The
tests were carried out on the virtual robot programming platform, ROS (Robot
Operating System), and the Gazebo simulator.
As in any design process, the simulation part provides a lot of data, mainly
to detect possible elements that have been poorly designed (both hardware and
software).
In the case that is discussed, a set of four tests has been designed where the
gait patterns algorithms that have been programmed will be tested. The ﬁrst is
a test of displacement on ﬂat ground, without slopes or obstacles; the second is
a test of overcoming a stair; the third simulates an uneven terrain and the last
one is the overcoming of an inclined plane.
As the simulation in a computer can be considered a simulation in a controlled
environment and therefore, if it is always performed under the same conditions,
the results obtained will be identical, at the beginning of each test the robot will
be aﬀected by a disturbance provoked purposely by the system. In this way we
try to expose the robot to diﬀerent cases in each repetition of the tests.
Each test will be repeated a total of 100 times to have a large sample and to
obtain more conclusive results. In addition, a series of data are collected for their
analysis: duration of the test, distance travelled, average velocity, displacement
in bodies/second, initial position (x, y), ﬁnal position (x, y) and height of the
center of mass of the robot.
4.1
Trial 1
The objective of the ﬁrst test is to analyse if all the pattern gaits are able to
realize a displacement of at least one body per second, which means that they
exceed in this ability to numerous already designed hexapod robots. Besides that
quality, it is tried to analyse which is the maximum speed that is able to reach
each mode, to achieve this, the speed of the robot will be increased successively
until it is unable to travel a distance of X meters with an error in the Transverse
axis less than ten percent of the distance travelled (Fig. 6). The duration of the
test is 60 s. In the Table 1 the results can be analysed.
Fig. 6. Maximum error allowed in test 1.

Study of Gait Patterns for an Hexapod Robot in Search and Rescue Tasks
737
Table 1. Results of the ﬁrst test.
Pattern
Distance (m)
Velocity (m/s)
Bodies
Final pos. (x, y)
Average height (m)
Tripod
4,5
0,075
2,16
4.5; 0,066
0,0971
Tetrapod
7,53
0,125
3,62
6.95; −2.9
0,093
Wave
4,45
0,074
2,13
−0.05; 4.45
0,088
Wave V2
5,57
0,093
2,68
5.46; 1.12
0,088
New mode 1
4,15
0,069
1,99
3.98; −1.19
0,085
New mode 2
12,07
0,201
5,79
12.02; 1.16
0,062
Numerous data of interest can be obtained from test number 1, the ﬁrst one
can be obtained from the analysis of the highest speed by the robot, the new
mode 1 is three times faster than the other modes of running, the second that
more average speed has had is the pattern gait that imitates the tetrapods, but
far below. Although the velocities may seem low, they are quite the opposite, in
robotics it is considered excellent that a mobile robot can reach a speed of one
body per second. In the Table 2 you can see a comparison with the other robots
designed to date [8].
Table 2. Comparative of speed with other hexapod robots.
Robot
Length (m) Mass (kg) Velocity (m/s) Bodies/second
CW Robot 0,5
1
0,0833
0,16
Dante II
3
770
0,017
0,006
Atilla
0,36
2,5
0,03
0,083
ASV
5
3200
1,1
0,22
Boadicea
0,5
4,9
0,11
0,22
Sprawlita
0,17
0,27
0,42
2,5
Robot
0,48
9,5
12,07
5,79
In Fig. 7 are drawn the diﬀerent paths that each gait pattern realized for the
test 1. It is highlighted, in white, the path of the Wave Mode, making almost a
half-turn, in order to try to solve this error by the algorithm of the pattern, the
sequence of movement of the legs (Fig. 8) is changed and the result can be see
in yellow colour.
Another fact that comes from the data is that only the tripod mode is able
to maintain a more or less rectilinear, the other modes are aﬀected by the imbal-
ances that occur at some point in the transitions when the center of gravity loose
the stability margin (not all the gait patterns are stable all the time) or by the
lag that occurs in the ﬁrst step.
The comparison of the height of the center of mass of the robot is also inter-
esting (Fig. 9). There are some patterns that maintain a more constant height

738
J. De Le´on et al.
Fig. 7. Trajectories followed by the diﬀerent gait modes. (1) Alternate tripod (2) Tetra-
pod (3.1) Wave (3.2) Wave V2 (4) New Mode (5) New Mode 2
Fig. 8. New wave mode design.
while others are constantly rising and falling, this can become a determining
factor when selecting the mode of operation according to the surface where the
robot is going to act.
Fig. 9. Height of the center of mass of the robot in the gait patterns. (A) Altern tripod
(B) Tetrapod (C) Wave (D) Wave V2 (E) New Mode (F) New Mode 2.
4.2
Trial 2
The second test try to ﬁnd out what types of pattern are capable of overcoming a
stair. This test is a very important feature that the robot must have since one of

Study of Gait Patterns for an Hexapod Robot in Search and Rescue Tasks
739
the main limitations that the current USAR robots have is the inability to climb
stairs, therefore, getting the robot to be able to overcome this test becomes an
essential objective to overcome.
The stair designed for the test is designed to the standards in architecture
[3], where the footprint is required to be at least 28 cm. And the riser in the
interval 13 to 18 cm, fulﬁlling the following relation:
54 < 2 · stepheight + 1 · stepwidth < 70
(3)
In the case covered, the riser has a height of 17 cm and the footprint 34 cm,
giving a result of 68 cm.
Fig. 10. Proﬁle of the two patterns that overcome test 2. (A) New Mode (B) New
Mode 2
In this case the new mode 2 was able to overcome the obstacle, although
with diﬃculties, the robot had to be perpendicular to the step that was going to
climb, otherwise it did not surpass it and had to be replaced, however, if there
was no deviation in the path is capable of climbing the whole stair at once. The
new mode 1 also achieved the goal but provided that it was placed in the same
position as the previous mode and also, only exceeded it if the movement of the
legs was made front pair. The results of the two modes that surpassed the test
are detailed in the Fig. 10, where it is seen that the new mode 2 is much faster
despite having a greater number of attempts to climb each step. For its part, the
new mode 1 presents that slowness because in each step that surpasses, it has
to wait until the sequence of movement arrives again to the pair ahead.
As the others locomotion patterns were still unable to climb the stair it was
decided to make a change in the control strategy, since one of the reasons why
the robot could not climb the ladder is the lack of torque against the step. To do
this, the algorithm was redesigned to perform a sequence control in time interval,
the positive side of this control is that when one leg forces against a step, after
the deﬁned interval of time, the second front leg start to apply force too, helping
to solve the obstacle. The consequence of using this running mode is that the
synchronization between legs is lost.

740
J. De Le´on et al.
The conclusion of this test clearly indicates that overcoming obstacles
presents two key factors, the ﬁrst is the relationship between leg diameter and
step, where the second can not be greater than the ﬁrst; And the second means
that the running mode used must apply a torque with the two front legs simul-
taneously in order to overcome the obstacle.
4.3
Trial 3
USAR robots are usually designed to move around irregular terrain, but as
already discussed in the introduction a large number are unable to overcome
this obstacle. The test does not try to ﬁnd out only who is able to overcome the
abrupt terrain but also to do it in a faster way.
The terrain modeled for the test is observed in the Fig. 11a according to the
mode of travel selected the robot will follow the path according to the torque
that can be applied in each sequence and the possible collisions between the
terrain and the body (Fig. 11b).
(a) Model of the terrain for
test 3.
(b) Results of test 3.
Fig. 11. Test 3.
As can be seen from the results of this test, alternate tripod and the new
mode 1 are those that have followed a more rectilinear path, this can be due to
two main factors, which have an average height respect the other modes, with
which they have been able to overcome obstacles that the other modes have had
to surround it, or, that motion sequences are able to generate more torque to
overcome obstacles.
Finally, in the Fig. 12 the proﬁle of the route followed by each mode can be
seen.
4.4
Trial 4
Often, after earthquakes or collapses of buildings, walls are torn down but form-
ing inclined planes, or USAR members have to access the interior of a house
through the roof, as it happens when there are landslides that bury houses.
This test is designed to simulate these situations and ﬁnd out which sequence
of moves is able to overcome a more inclined plane. Robots were started by

Study of Gait Patterns for an Hexapod Robot in Search and Rescue Tasks
741
Fig. 12. Proﬁle of the route followed by each running mode in test 3. (A) Alternate
tripod (B) Tetrapod (C) Wave (D) Wave V2 (E) New Mode (F) New Mode 2
testing a constant slope of 10% with a length of 4 meters in the horizontal
projection, and then increasing the slope to reach a maximum slope of 50%. The
results obtained in this test have been very satisfactory, all robots have been
able to overcome it without any inconvenience. Table 3 shows the time that all
the patterns need to overcame the trial with a slope of 50%.
Table 3. Results of passing test 4 for a slope of 50%.
Mode
Time (s)
Alternate tripode 80
Tetrapod
51
Wave
79
New mode 1
71
New mode 2
20
5
Conclusions
A study on the locomotion gaits for an C-leg based hexapod robot have been
carried out, it shows that those gaits largely aﬀect the performance of the robot,
and therefore it is a ﬁeld that needs to be studied in detail. Two novel gait modes
have been proposed and compared with those found in literature.
It is worth highlighting that the results obtained by the two gait modes pro-
posed, especially the second of them, new mode 2, to the best of our knowledge,
have not been studied before, maybe because of the unnatural movement that

742
J. De Le´on et al.
it performs. However, it has been by far the most versatile movement oﬀered,
much more than expected, being able to overcome all the tests with almost no
drawbacks.
It has also been observed that the alternate tripod is able to perform a more
rectilinear trajectories and maintain a more constant height than any of the
other gaits, which is why it is the one that predominates in hexapods, both
robotic and those found in nature.
It would be very interesting to be able to analyze a mixed behavior, that in
terrains where obstacles are easy to dodge or with a size smaller than half the
diameter of the leg, use the alternate tripod and when a high obstacle has to be
overcome, change the mode to the new mode 1 or new mode 2.
The relevance of the study presented in this work has been demonstrated,
moreover, it has open new development routes to terrestrial rescue robots, where
the elements that, to date, have been limited to robots that use conventional
wheels and caterpillars.
References
1. Roennau, A., Heppner, G., Nowicki, M., Dillmann, R.: LAURON V: a versatile six-
legged walking robot with advanced maneuverability. In: IEEE ASME, pp. 82–87,
Julio 2014
2. Balasubramanian,
R.:
Legless
locomotion:
concept
and
analysis.
Technical
Report, The Robotics Institute. Carnegie Mellon University, Pittsburgh, Pennsyl-
vania 15213. https://pdfs.semanticscholar.org/207c/599e4d5c9bbbc71e109645d79
fbc9def52e6.pdf
3. de Espana, G.: Codigo tecnico de la ediﬁcacion. db-si. Technical report, Gobierno
de Espana
4. G´omez, J.J.R., Oviedo, M.G., de Le´on Rivas, J., Ramos, D.A.G., Barrio, A.M.,
Terrile, S., Au˜n´on, P.G., Giner, J.D.C., Rossi, C., Cruz, A.B.: Proyecto pric: pro-
tecci´on robotizada de infraestructuras cr´ıticas. In: Libro de actas de las Jornadas
Nacionales de Rob´otica 2017, vol. 1, pp. 1–6. CEA-IFAC, Valencia, Junio 2017.
http://oa.upm.es/46819/, rob´otica y Cibern´etica RobCib
5. Moore, E.Z.: Leg design and stair climbing control for the RHex robotic hexapod.
Master’s thesis, Department of Mechanical Engineering McGill University, January
2002
6. Murphy, R.R.: Disaster Robotics. The MIT Press, Cambridge (2014)
7. Nie, C., Corcho, X.P., Spenko, M.: Robots on the move: versatility and complex-
ity in mobile robot locomotion. IEEE Robot. Autom. Mag. 20(4), 72–82 (2013).
http://ieeexplore.ieee.org/document/6582554/
8. Rivas, J.D.L.: Deﬁnicion y analisis de los modos de marcha de un robot hexapodo
para tareas de busqueda y rescate. Master’s thesis, Escuela Superior de Ingeniera
Industrial. Universidad Politecnica de Madrid, October 2015
9. Tordesillas, J.: Diseno y simulacion del sistema de locomocion de un robot
hexapodo para tareas de busqueda y rescate. Master’s thesis, Universidad Politec-
nica de Madrid, July 2016. http://oa.upm.es/42893/
10. Tordesillas, J., Leon, J.D., Cerro, J.D., Barrientos, A.: Modelo cinematico de un
robot con C-LEGS. Jornadas de Automatica, pp. 267–275, September 2016. http://
ja2016.uned.es/assets/ﬁles/ActasJA2016.pdf

A Hybrid ZMP-CPG Based Walk Engine
for Biped Robots
S. Mohammadreza Kasaei(B), David Sim˜oes, Nuno Lau, and Artur Pereira
IEETA/DETI, University of Aveiro, 3810-193 Aveiro, Portugal
{mohammadreza,david.simoes,nunolau,artur}@ua.pt
Abstract. Developing an optimized omnidirectional walking for biped
robots is a challenging task due to their complex dynamics and kinemat-
ics. This paper proposes a hierarchical walk engine structure to generate
fast and stable walking. In particular, this structure provides a closed-
loop CPG-based omnidirectional walking that takes into account two
human-inspired push recovery strategies. In addition, this structure is
fully parametric and allows using a policy search algorithm to ﬁnd the
optimum parameters for the walking. To show the performance of the
proposed structure, a set of experiments on a simulated NAO robot has
been carried out. Experimental results demonstrate that the proposed
structure is able to generate fast and stable omnidirectional walking. The
maximum speed of forward walking that we have achieved was 59 cm/s.
Keywords: Humanoid robots · Omnidirectional walking · Hierarchical
structure · Central Pattern Generator(CPG) · CREPS-CMA · 3D Soccer
simulation
1
Introduction
The ability to eﬃciently move in dynamic environments is an essential property
for robots which want to operate in human environments. Humanoid robots are
the most appropriate type of robots to operate in these environments due to
their similarity in kinematic architecture with a human. Hence, developing a
fast and stable walking has become a hot topic for several research groups such
as robotics and biologists [10,16,23].
Over the past decades, several approaches have been presented to construct a
stable walking for biped robots and can be generally divided into four categories:
trajectory-based methods, passive dynamics control, heuristic based methods
and Central Pattern Generators [11,18,21]. Trajectory-based methods employ
dynamics models of robots with some stability criterion to produce stable walk-
ing. These approaches are computationally expensive due to calculating com-
plex geometrical equations at each time step. Most of the approaches in this
group consider some constraints in dynamics to achieve simplicity. For example,
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_61

744
S.M. Kasaei et al.
keeping the center of mass at a constant height is one of the general assumptions
in these approaches. This constraint is not only harmful to the knee joint but it
also increase the consumption of energy. In addition, these kinds of approaches
require predeﬁning positions for placing the feet. Passive dynamics approaches
try to describe behaviors of robots by their passive dynamics with no sensing
or control. These types of approaches fall beyond the scope of this paper. Some
others are based on heuristic approaches such as genetic algorithms, reinforce-
ment learning, etc. In these approaches, the learning phase takes a considerable
amount of time because of the requirement of a large amount of samples [21].
These approaches are not suitable for a real robot due to the high potential of
damaging the hardware during the training period [12].
Central Pattern Generator (CPG) approaches are biologically inspired con-
trol methods. These methods try to produce stable walking by generating some
rhythmic patterns for each limb instead of using physical dynamics model of
the robot. They not only require less computational power but also provide
energy eﬃcient walking. In addition, these approaches generate more human-
like motions [19].
In this paper, an omnidirectional CPG-based walk engine is proposed to gen-
erate fast and stable walking for a simulated soccer humanoid robot. Towards
this goal, Partial Fourier Series (PFS) oscillators are used to generate smooth
output trajectories. In order to achieve more stable walking, sensory feedback
signals are used to modify oscillators outputs while interacting with the envi-
ronment. In addition, an optimization technique based on Contextual Relative
Entropy Policy Search with Covariance Matrix Adaptation (CREPS-CMA) [1]
is used to appropriately tune the parameters.
The remainder of this paper is structured as follows. In Sect. 2, we review
related work. The architecture of our step generator is explained in Sect. 3.
The hierarchical structure of our walk engine and detailed methodologies are
explained in Sect. 4. Optimization method and the results are then presented in
Sect. 5. Finally, conclusions are presented and future research is discussed.
2
Related Work
Over the past decade, several researches have been conducted on CPG-based
walk engine for bipeds and quadrupeds [10,12,14]. Recently, interests of using
bio-inspired approaches as possible alternatives to generate gait locomotion are
growing due to their successful results and fewer requirements [8]. The main focus
of this paper is on biped robots. A brief literature survey concerning CPG-based
walk engines is presented in the following of this section.
Picado et al. [18] proposed a forward walk generator for a simulated humanoid
robot. They used PFS oscillators to generate joint trajectories. Genetic Algo-
rithms were then used to ﬁnd optimum parameters for each trajectory. They
chose a simple but eﬀective ﬁtness function which considered forward walking
velocity and the torso average oscillation. Their approach has been tested on
a NAO simulated humanoid robot. Simulation results showed their approach

A Hybrid ZMP-CPG BasedWalk Engine for Biped Robots
745
is able to generate forward walking with good velocity (51 cm/s). Afterward,
Shahriar et al. [2] extended their approach to produce forward and side walk for
the agent. As they argued, their approach was able to produce faster forward
walking (55 cm/s).
Song et al. [19] developed a CPG-based forward walk generator with a balance
controller for a humanoid robot. In their approach, onboard gyro and accelerom-
eter sensors are used to measure the angles of upper-body. By using these angles,
a controller was developed to adjust the tilt angle of the upper-body. To verify
the eﬀectiveness of their controller, real Zero Momentum Point (ZMP) data has
been recorded during walking on diﬀerent slopes between −7◦to 7◦. Experimen-
tal results with a real NAO humanoid robot showed their approach can generate
stable forward walking on unknown slopes.
Cristiano et al. [4] implemented a CPG-based walk engine with sensory feed-
back that enabled biped robots to walk on uneven terrain. In order to develop
a feedback walk engine, inertial and force sensors were used. Their system con-
sisted of two stages including the trajectories generator (trunk, legs, and arms)
and a stability controller to automatically adjust these trajectories using sensors
data. To evaluate the abilities of their walk engine, simulation experiments with
a NAO humanoid robot have been carried out. Simulation results showed the
robot was able to walk on stairs with a step height of 1 cm and was also able to
walk on sloped terrain.
Liu and Urbann [14] presented and implemented a CPG-based walk engine
which was able to model the walking pattern using continuous and diﬀerentiable
mathematical functions. In addition, they used ZMP criterion and a body pos-
ture controller to stabilize the robot during walking. The proposed controller
was based on sensory feedback and tried to modify walking pattern in real time.
They have performed several experiments on a NAO humanoid robot to verify
their method. The results showed their method could provide energy eﬃcient
and stable dynamic walking.
Most of the above approaches were able to provide unidirectional walk engine
and some of them used sensory feedback to increase its stability when walking.
To use humanoid robots in a real human environment, we believe they should
not only be able to generate omnidirectional human-like walking but also be
able to react appropriately against disturbances (e.g., external pushes, inclined
surfaces, uneven terrain, etc.). In the remainder of this paper, a CPG-based
walking engine is presented that is able to generate omnidirectional walking
patterns in an appropriate manner. Besides, two human-inspired push recovery
strategies are developed to provide robustness against disturbances. Moreover,
this walking engine is structured in hierarchical layers to fade the complexities.
3
Step Generator Architecture
Biped locomotion is inherently periodic and be commonly generate by repeating
a series of steps. A step can commonly be generated by two approaches, the
engineering-oriented approach based on Zero Moment Point (ZMP) [22] and

746
S.M. Kasaei et al.
the biologically-inspired approach based on CPG [17]. In the reminder of this
section, a brief overview of the Three-Dimensional Linear Inverted Pendulum
(3D-LIPM) [9] is presented, and our CPG-based step generator is introduce.
3.1
3D-LIPM and ZMP
Fig. 1. 3D-Linear inverted pen-
dulum
3D-LIPM is widely used to analyze the behavior
of Center of Mass (CoM) during walking. In this
model, overall dynamics of a humanoid robot is
approximated by a single mass that is connected
to a certain point on the ground via a mass-
less rod. To achieve simplicity and linearity in
dynamics, the length of the rod is assumed to
be constant, therefore, the single mass is lim-
ited to move along a horizontally deﬁned plane
(See Fig. 1). According to this assumption, the
motion equations of the CoM are decoupled in
sagittal and lateral plane. Moreover, the ground reaction force always passes
through the CoM and can not generate angular momentum. The motion equa-
tion of this pendulum is as follows:
¨x(t) = g
zc
x(t) −
1
m × zc
τ(t),
(1)
where t represents the time which is reset at the end of each step, m is the mass
of the pendulum, g describes the gravity acceleration, zc is the height of CoM
and τ is the torque around the axis. ZMP is the most commonly used criterion
for controlling a biped robot during walking. ZMP concept is used to specify
a point on the ground which does not produce any momentum (due to gravity
and body inertia) in the horizontal direction. The ZMP equation for 3D-LIPM
is obtained by the following equation:
px(t) =
τ(t)
m × g ,
(2)
where px is the location of the ZMP. By substituting Eq. (1) to the ZMP Eq. (2)
we obtain:
px(t) = x(t) −zc
g ¨x(t).
(3)
Traditionally, by using a preplanned ZMP trajectory and solving this diﬀerential
equation, a static walking can be obtained [9]. Although this approach has been
successfully tested on several real robots, it has several drawbacks. It is com-
putationally expensive and requires to preplanned motions. In addition, to keep
the CoM at a constant height, knee joints have to be bent during walking which
is not only energy eﬃcient but also harmful to the knee actuators. Moreover, it
does not generate very human-like motion [3,12,17].

A Hybrid ZMP-CPG BasedWalk Engine for Biped Robots
747
3.2
CPG Control Architecture
The latest neurophysiological studies on invertebrate and vertebrate animals
showed that rhythmic locomotion patterns (e.g. walking, running, etc.) are gen-
erated by CPGs at the spinal cord [6]. Furthermore, they proved the basic build-
ing block of a CPG is an oscillator and a CPG can be modeled by coupling some
oscillators in a particular arrangement. Hence, CPGs are able to generate several
endogenously rhythmic signals without a rhythmic sensory input.
During last decade, several CPG-based walk engines have been proposed
which were based on diﬀerent oscillator models such as Partial Fourier Series
(PFS), Hopf, Matsuoka, etc. [15,18,19]. In most of them, an oscillator is allo-
cated at each limb and its output is directly used as a set point command (torque,
position, etc.). Humanoid robots have several Degrees of Freedom (DOF), there-
fore adjusting the parameters of each oscillator is not only diﬃcult but also
trial-intensive. In addition, adapting sensory information and obtaining appro-
priate feedback pathways for all the oscillators is a complex subject [19].
In this paper, to overcome these diﬃculties, a speciﬁc arrangement for the
oscillators is designed which takes into account the dynamics and kinematics
model of the robot. Our arrangement is composed of only seven oscillators by
considering the symmetry motion pattern between the legs and arms. These
oscillators generate the Cartesian coordinate positions of feet and arms to pro-
duce overall stride trajectories. In order to develop a CPG trajectory generator,
PFS oscillators are used as the main oscillator since they are able to generate
multi-frequency shape signals. PFS try to decompose a periodic signal into a sum
of simple oscillators (e.g. sines or cosines). A PFS oscillator model is deﬁned as
follows:
f(t) = A0 +
N

n=1
An sin(nωt + φn),
ω = 2π
T
∀t ∈ℜ,
(4)
where N is the number of frequencies, An, ω and φn are the amplitude, the
angular velocity and the phase of the nth term, respectively. These parameters
can be adjusted to obtain diﬀerent shapes.
As we previously mentioned, walking is inherently periodic. Therefore,
according to Eq. (3), if the trajectory of the ZMP is periodic, then the tra-
jectory of the CoM is also periodic. Therefore, Let us consider trajectories of the
CoM and the ZMP can be generated by PFS oscillators, then we have:
px(t) = A0 +
N

n=1
An sin(nωt + φn),
ω = 2π
T
∀t ∈ℜ,
(5)
x(t) = B0 +
N

n=1
Bn sin(nωt + φn),
ω = 2π
T
∀t ∈ℜ.
(6)
Considering ω2
0 =
g
Zc and by plugging Eqs. (5) and (6) into Eq. (3) we obtain:

748
S.M. Kasaei et al.
A0 +
N

n=1
An sin(nωt + φn) = B0 +
N

n=1
(Bn + Bnn2ω2
ω2
0
) sin(nωt + φn).
(7)
This equation shows the ZMP trajectories and the CoM trajectories have the
same form with diﬀerent amplitudes. Now by comparing the left-hand coeﬃ-
cients with the right-hand coeﬃcients of Eq. (7), the relationship between the
amplitudes will be obtained:

A0 = B0
An = Bn(1 + ( n×ω
ω0 )2).
(8)
These relationships declare several features for biped walking which are very
useful for adjusting the oscillators and walking parameters. For instance, by
increasing the ZMP frequency, the amplitudes of the CoM will be decreased.
Generally, each step consists of two phases, single support and double sup-
port. To achieve fastest walking speed, we considered the double support period
is very short. In our target structure, a step is deﬁned by Length (L), Width
(W), Orientation (θ) and Step Duration (T). According to these parameters,
ZMP trajectory can be generated. The best intuitive choice for the ZMP tra-
jectory is the middle of the supporting foot [5]. Hence, ZMP trajectories with
respect to the torso location can be generated using two distinct periodic sym-
metric square functions. Therefore, ZMP trajectories are deﬁned as follow:
yZMP (t) =

−(D + W)
−T ≤t < 0
D + W
0 ≤t < T,
xZMP (t) =

−L
−T ≤t < 0
L
0 ≤t < T,
(9)
where D is the distance between the feet. Since yZMP and xZMP are odd func-
tions, a speciﬁc method exists to approximate them by a PFS. By using this
method, the coeﬃcients of yZMP and xZMP can be calculated as follows:
Cy =
⎧
⎪
⎨
⎪
⎩
A0 = 0
an =
 4(D+W )
nπ
for n odd
0
for n even,
Cx =
⎧
⎪
⎨
⎪
⎩
A0 = 0
an =

4L
nπ
for n odd
0
for n even.
(10)
By substituting these coeﬃcients into Eq. (8), the trajectories of CoM can be
approximated. An approximation to ZMP and corresponding CoM trajectory in
Y direction when the sum is truncated at the N = 31 are shown in Fig. 2(a).
As is shown in this ﬁgure, the output of nth PFS has large oscillations near
the target ZMP. To overcome this issue and to generate a smooth output, the
Lanczos sigma factors [7] are employed which are deﬁned as follows:
sinc
 n
m
	
= sin

 nπ
m

nπ
m
,
(11)

A Hybrid ZMP-CPG BasedWalk Engine for Biped Robots
749
0
0.5
1
1.5
2
2.5
3
Time (s)
-0.1
-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
CoM and ZMP Refrence in Y (m)
ZMP and CoM
ZMP
CoM
0
0.5
1
1.5
2
2.5
3
Time (s)
-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
CoM and ZMP Refrence in Y (m)
ZMP and CoM
ZMP
CoM
)
b
(
)
a
(
Fig. 2. ZMP trajectory and corresponding CoM trajectory in Y-direction: (a) the out-
puts of oscillators when the sum is truncated at the N = 31; (b) the outputs of
oscillators after applying the Lanczos sigma factors.
where m = N+1. By multiplying these factors into each terms of PFS, the proper
smooth outputs will be achieved. The obtained results are shown in Fig. 2(b).
In the next section, we will design an omnidirectional walk engine by using this
CPG step generator.
4
Hierarchical Walk Engine
Biped locomotion can generally be generated based on the trajectories of ZMP
and CoM. An inverse kinematic method should then be used to calculate the
joint angles. In this section, a hierarchical structure is proposed to develop an
omnidirectional walk engine which has three levels, including the Trajectories
generator, the Push recovery controllers and the Low level controllers. The over-
all architecture of this walk engine is depicted in Fig. 3.
Fig. 3. Overall architecture of our walk engine.
4.1
Trajectories Generator
ZMP trajectories can be deﬁned by substituting the parameters of a step
into Eq. (10) to obtain the coeﬃcients for Eq. (5). Then, by substituting these

750
S.M. Kasaei et al.
coeﬃcients into Eq. (8) and using Eq. (6) the trajectories of CoM can be gener-
ated. According to the sagittal symmetry of walking in frontal and lateral planes,
the same trajectories with a half-period phase shift are generated for the swing
leg movements. Moreover, three separate oscillators are allocated to generate Z,
θ and arms trajectories for both legs and both arms in a symmetrical manner. All
of the reference trajectories of walking have already been generated and by using
an inverse kinematic method a feed-forward walking engine can be achieved.
4.2
Push Recovery Controller
Feed-forward walk engines are able to provide walking only on even surfaces or on
predeﬁned inclined slopes. This type of walk engines generates rhythmic gaits
for given inputs regardless of the environment. Therefore, due to unexpected
errors which can be raised from several sources, such as external disturbances,
inaccurate dynamic model and etc., they can not be robust. For instance, during
walking on rough terrain environments, several forces will be applied to the
robots. Hence, to operate on these environments, robots should be able to do
some recovery strategies to keep their stability. To develop stable walking, several
criteria have been deﬁned [5,13] and the most important criterion for stability is
keeping the Center of Gravity (CoG is the vertical projection of the CoM on the
ground) inside a polygon that is deﬁned by the foot or feet touching the ground
(support polygon). Real position of the ZMP, position and velocity of the CoM
are generally used to estimate the state of the robot. We have developed two
independent human-inspired strategies to keep the stability during walking and
regain balance after external pushes.
Fig. 4. Ankle strategy
Ankle Strategy. According to Eq. (1), the position of
the CoG is proportional to the ankle torque. Therefore,
to keep the CoG inside support polygon, a compensat-
ing torque can be applied at the ankle joints. Ankle
torque will saturate whenever the CoG is at the bound-
aries of the support polygon. Therefore, by using this
strategy, balance can be recovered whenever the CoG
does not reach at the edge of the support polygon. To
provide more restoring torque, the arms can also be
used. To implement this strategy a PD controller is
used.
θc = Kp × ω + Kd × d
dtω,
(12)
where θc is the compensation angle that is added to the angle of the ankle. Kp
and Kd are the proportional and derivative gains and ω is the angular velocity
of CoM. Kp and Kd are set based on expert knowledge and experiments. This
strategy is shown in Fig. 4.

A Hybrid ZMP-CPG BasedWalk Engine for Biped Robots
751
Fig. 5. Hip strategy
Hip Strategy. During fast walking, the CoM acceler-
ates forward, meaning the Center of Pressure (CoP) moves
behind the CoG. Ankle strategy tries to decelerate the
CoM and keep the CoG inside the support polygon. It
causes the CoP to move in front of the CoG and to reach
to the boundary of the support foot. In this situation, the
hip will start to roll over and, as a consequence, the robot
can not regain its stability only by using the ankle strat-
egy. As is shown in Fig. 5, hip angle error can be a good
feedback to detect this condition. Using this feedback an
indirect reference ZMP controller can be developed, which
is trying to follow reference ZMP without measuring the real ZMP by keeping
the hip parallel with the ground [20]. Since the humanoids have many joints,
to control the acceleration of CoM, beyond the ankle torque, the hip and waist
joints can be used to apply more torque around the CoM. Therefore, to prevent
a fall, in addition to the ankle and arm joints, the joints of the waist and the
hips should be used. Hip strategy tries to compensate the disturbance by moving
CoM and hip quickly. To implement this strategy, two PD controllers are used.
One of them is used to apply torque to the hip joints to create the acceleration
of the upper body and another one is used to decelerate the body. The set points
for both controllers and the gains for each of them are selected depending on
the amplitude of the disturbance.
4.3
Low Level Controller
This level is the lowest level of the proposed structure and consists of three
main modules, including state estimator, inverse kinematics solvers, and joints
position controller. Low level controller tries to estimate position, velocity, and
acceleration of the CoM by using the data of the IMU, which is mounted on
the hip, and the forward kinematic model of the robot, which uses the values of
the joint encoders. This level also provides PID position controllers to control
the actuators. Moreover, to increase the portability, each module which has
dependency on the robot’s hardware is kept in this level. Most humanoid robots
have standard 6DoF legs with diﬀerent conﬁgurations (3DoF in the hip, 1DoF
in the knee and 2DoF in the ankle). Therefore, to apply the proposed walking
engine to a new humanoid robot, only this layer need to be conﬁgured.
5
Optimization of Walk Engine Parameters
As described in previous sections, the walk engine is parameterized by 7 para-
meters, representing the expected step movement in the x, y, and z axes, along
with the turning and inclination angles, the step duration and the center of mass
height. The parameters are ﬁltered in order to avoid out-of-bounds values, by
using their absolute values when parameters must be positive. This creates a
smooth ﬁtness landscape for the algorithm to explore, instead of simply limiting
the parameters within a certain range, which creates sharp ridges and ﬂat areas.

752
S.M. Kasaei et al.
These parameters have been tuned in order to obtain both an initial solution
that was very stable and that could move forward, albeit slowly, as well as the
fastest hand-tuned solution. CREPS-CMA [1] is then used to optimize the slow
and stable solution, and compared it against the best hand-tuned version.
CREPS-CMA is a state-of-the-art contextual policy search algorithm that
iteratively generates sets of candidate solutions sampled from a parameter dis-
tribution based on a given context. Candidate solutions are evaluated against
a given task and context, and the parameter distribution is updated based on
the candidates’ ﬁtness, thus improving the quality of the samples in the next
iteration. Due to CREPS-CMA’s parallel nature, we could take advantage of a
distributed optimization architecture to achieve a speed-up of 30x, thus enabling
us to optimize the parameters in a practical amount of time.
5.1
Maximum Speed Scenario
This scenario focused on optimizing the humanoid agent to achieve the highest
speed, moving straightforward, without being unstable enough to fall. The agent
is allowed to move during 15 s, and the ﬁtness function f with parameters θ is
deﬁned as
f(θ) = −δx + δy + ε,
(13)
where δx represents the total distance covered in the x-axis, δy represents the
total distance deviated in the y-axis, and ε is 0 if the agent did not fall and
30 otherwise. With this function, the agent is rewarded for moving along the
x-axis, and it is penalized for deviating to the sides or falling. After 5000 epochs
of training, CREPS-CMA improved upon our initial speed of 11 cm/s and reach-
ing a maximum stable velocity of 59 cm/s. In comparison, the hand-tuned best
solution achieved only a top speed of 50 cm/s, resulting in an improvement of
18% faster speed. In addition, our optimized agent is faster than both agents
in [2,18]. The distances covered by the optimized, hand-tuned and initial solu-
tions are shown in Fig. 6.
5.2
Omnidirectional Scenario
In our second scenario, we focused on optimizing the humanoid agent to achieve
high mobility and turning speed, without being unstable enough to fall. We allow
the agent to start moving with maximum speed forwards for a random amount
of time (from 1 to 3 s), this way assuring our parameters allow the agent to turn
when he is already moving. The agent is then allowed to turn to a speciﬁc angle
during 10 s, or less if he starts moving away from the target. We deﬁned the
angle as a random value in the interval [0, 45]◦, and the agent is given a target
1 unit of distance away in that direction. Using a positive interval instead of a
symmetrical one is a simple trick to speed up optimization, which we exploit by
negating the y and θ parameters when, in actual play, the target stands in a
direction with a negative angle. Fitness function f with parameters θ is deﬁned
as
f(θ) = δd20
3 + δθ 10
180 + δt + ε,
(14)

A Hybrid ZMP-CPG BasedWalk Engine for Biped Robots
753
Fig. 6. The top panel shows the distance covered in the same amount of time by the
agent after optimization. The middle panel shows the distance covered by our hand-
tuned solution. The bottom panel shows the stable initial hand-tuned solution given
to CREPS-CMA.
where δd represents the total distance to the intended target, δθ represents the
diﬀerence between the ﬁnal and target orientations of the agent, δt represents
the time taken, and ε is 0 if the agent did not fall and 30 otherwise. With this
function, we reward the agent for moving as fast as possible and with as much
accuracy as possible to the target pose, without falling. We use some ratios to
scale each segment of the reward, such that the distance, angle, and time have
a similar importance, penalty-wise. After 2000 epochs of training, we achieve a
fast omni-directional walk, where the agent can turn up to 45◦in a single unit
of distance, as can be seen in Fig. 7.
Fig. 7. An exemplary omni-directional walk after the optimization process has ﬁnished.
The agent is given several turning targets, starting with left turns and ﬁnishing with
right ones, all of these with increasing angle. The agent walks forward for 2 s after every
turn.
6
Conclusion
This paper proposed a hierarchical structure to generate an omnidirectional walk
engine. This structure decoupled the walking motion into three hierarchy lev-

754
S.M. Kasaei et al.
els to increase ﬂexibility and portability. In the ﬁrst level, 3D-LIPM and 7 PFS
oscillators are used to generate the trajectory of the feet. In the second level, two
human-inspired push recovery strategies are developed to increase stability dur-
ing walking. To achieve highly portable walk engine, all modules which depend
on the robot’s platform, are developed in the lowest level. The proposed walk
engine is developed and completely tested on a simulated Nao humanoid robot.
CREPS-CMA is used to ﬁnd the optimize parameters. Experimental results
showed that this structure using the optimized parameters provided fast and
stable walking (59 cm/s) which was 18% faster then the hand-tuned best solu-
tion. In the future, we intend to develop knee push recovery strategy to provide
a more stable walking.
Acknowledgment. This research is supported by: Portuguese National Funds
through Foundation for Science and Technology (FCT), in the context of the
project UID/CEC/00127/2013; and by European Union’s FP7 under EuRoC grant
agreement CP-IP 608849. The second author is supported by FCT under grant
PD/BD/113963/2015.
References
1. Abdolmaleki, A., Simoes, D., Lau, N., Reis, L.P., Neumann, G.: Contextual relative
entropy policy search with covariance matrix adaptation. In: Proceedings of 2016
IEEE International Conference on Autonomous Robot Systems and Competitions
(ICARSC), pp. 94–99. IEEE, May 2016
2. Asta, S., Sariel-Talay, S.: Nature-inspired optimization for biped robot locomotion
and gait planning. In: Applications of Evolutionary Computation, pp. 434–443
(2011)
3. Behnke, S.: Online trajectory generation for omnidirectional biped walking. In:
Proceedings of 2006 IEEE International Conference on Robotics and Automation,
ICRA 2006, pp. 1597–1603. IEEE (2006)
4. Cristiano, J., Puig, D., Garcıa, M.: Locomotion control of biped robots on uneven
terrain through a feedback CPG network. In: Proceedings of the XIV Workshop
of Physical Agents, pp. 1–6 (2013)
5. Erbatur, K., Okazaki, A., Obiya, K., Takahashi, T., Kawamura, A.: A study on
the zero moment point measurement for biped walking robots. In: Proceedings of
2002 7th International Workshop on Advanced Motion Control, pp. 431–436. IEEE
(2002)
6. Guertin, P.A.: The mammalian central pattern generator for locomotion. Brain
Res. Rev. 62(1), 45–56 (2009)
7. Hamming, R.: Lanczos’ σ factors and the σ factors in the general case 32.6 and
32.7. In: Numerical Methods for Scientists and Engineers, pp. 534–536 (1986)
8. Ijspeert, A.J.: Central pattern generators for locomotion control in animals and
robots: a review. Neural Netw. 21(4), 642–653 (2008)
9. Kajita, S., Kanehiro, F., Kaneko, K., Fujiwara, K., Harada, K., Yokoi, K.,
Hirukawa, H.: Biped walking pattern generation by using preview control of zero-
moment point. In: Proceedings of 2003 IEEE International Conference on Robotics
and Automation, ICRA 2003, vol. 2, pp. 1620–1626. IEEE (2003)

A Hybrid ZMP-CPG BasedWalk Engine for Biped Robots
755
10. Kajita, S., Tan, K.: Study of dynamic biped locomotion on rugged terrain-
derivation and application of the linear inverted pendulum mode. In: Proceedings
of 1991 IEEE International Conference on Robotics and Automation, pp. 1405–
1411. IEEE (1991)
11. Kasaei, S.M., Lau, N., Pereira, A., Shahri, E.: A reliable model-based walking
engine with push recovery capability. In: proceedings of 2017 IEEE International
Conference on Autonomous Robot Systems and Competitions (ICARSC), pp. 122–
127, April 2017
12. Kasaei, S.M., Kasaei, S.H., Shahri, E., Ahmadi, A., Lau, N., Pereira, A.: How
to select a suitable action against strong pushes in adult-size humanoid robot:
Learning from past experiences. In: Proceedings of 2016 International Conference
on Autonomous Robot Systems and Competitions (ICARSC), pp. 80–86. IEEE
(2016)
13. Komura, T., Leung, H., Kudoh, S., Kuﬀner, J.: A feedback controller for biped
humanoids that can counteract large perturbations during gait. In: Proceedings
of the 2005 IEEE International Conference on Robotics and Automation, ICRA
2005, pp. 1989–1995. IEEE (2005)
14. Liu, J., Urbann, O.: Bipedal walking with dynamic balance that involves three-
dimensional upper body motion. Robot. Auton. Syst. 77, 39–54 (2016)
15. Missura, M., Behnke, S.: Self-stable omnidirectional walking with compliant joints.
In: Proceedings of 8th Workshop on Humanoid Soccer Robots, IEEE International
Conference on Humanoid Robots, Atlanta, USA (2013)
16. Mohades Kasaei, S.H., Kasaei, M., Alireza Kasaei, S.: Design and implementation
of a fully autonomous humanoid soccer robot. Ind. Robot. Int. J. 39(1), 17–26
(2012)
17. Or, J.: A hybrid cpg-zmp control system for stable walking of a simulated ﬂexible
spine humanoid robot. Neural Netw. 23(3), 452–460 (2010)
18. Picado, H., Gestal, M., Lau, N., Reis, L., Tom´e, A.: Automatic generation of biped
walk behavior using genetic algorithms. In: Proceedings of Bio-Inspired Systems:
Computational and Ambient Intelligence, pp. 805–812 (2009)
19. Song, K.T., Hsieh, C.H.: CPG-based control design for bipedal walking on unknown
slope surfaces. In: Proceedings of 2014 IEEE International Conference on Robotics
and Automation (ICRA), pp. 5109–5114. IEEE (2014)
20. Song, S., Ryoo, Y., Hong, D.: Development of an omni–directional walking engine
for full–sized lightweight humanoid robots. In: IDETC/CIE Conference (2011)
21. Torres, E., Garrido, L.: Automated generation of CPG-based locomotion for robot
NAO. In: RoboCup 2011: Robot Soccer World Cup XV, pp. 461–471 (2012)
22. Vukobratovic, M., Frank, A., Juricic, D.: On the stability of biped locomotion.
IEEE Trans. Biomed. Eng. 1, 25–36 (1970)
23. Yu, J., Tan, M., Chen, J., Zhang, J.: A survey on CPG-inspired control models and
system implementation. IEEE Trans. Neural Networks Learn. Syst. 25(3), 441–456
(2014)

Modelling, Trajectory Planning
and Control of a Quadruped Robot
Using Matlab R
⃝/Simulink
TM
Italo Oliveira1, Ramiro Barbosa2, and Manuel Silva3(B)
1 Universidade Tecnol´ogica Federal do Paran´a, Campo Mour˜ao, Paran´a, Brazil
italo fernando10@hotmail.com
2 Instituto Superior de Engenharia do Porto, Porto, Portugal
rsb@isep.ipp.pt
3 Instituto Superior de Engenharia do Porto and INESC TEC, Porto, Portugal
mss@isep.ipp.pt
Abstract. Due to the diﬃculty of building and making control tests in
real robots, it is usual to ﬁrst have a simulated model that provides a
good approach of a real robot’s behaviour. The importance of a good
control system in execution of a planned trajectory inspired this work,
whose purpose is to design a control system for a quadruped robot and
test its performance.
Keywords: MATLAB
R
⃝· Modelling · Trajectory Planning · Control
system · Simulation · Quadruped robots
1
Introduction
Mobile robotics are an important tool in today’s society, since many scien-
tiﬁc researches rely on instruments that should move in inhospitable places for
humans. For the stated reasons, it is necessary to create means of locomotion
capable of withstanding several adversities, such as high temperature and pres-
sure, steep and irregular terrain, and places with high radioactivity.
Mobile robots are mainly divided into locomotion by wheels, by tracks and
locomotion by legs. Although wheeled locomotion has more eﬃciency in regular
terrain and indoor environments, a legged locomotion is more robust when obsta-
cles appear on its path [1,2]. Inside legged locomotion, there is the sub-group
of quadruped robots, which are robots that use four legs to move. Compared
to other types of legged robots, quadrupeds take advantage of bipedal robots in
terms of stability, and compared to hexapod and octopod robots, they are more
eﬃcient [3,4].
There are also a lot of studies about quadruped robot models, specially in
control and gait planning, as in [5] which describes a Central Pattern Generator
to generate its gait or in [6] that implements a free gait based on posture control.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_62

Control of a Quadruped Robot
757
Still, [7] presents an on-line technique to construct walking gaits in irregular
terrains using a control basis and [8] investigates an operational space control
based on hierarchical task optimisation for quadruped locomotion.
The quadruped robot presented in this work adopts the trot gait, originally
found in horses [12], for its locomotion. Although factors such energy eﬃciency,
stability, speed and mobility are still far from ideal, quadruped robots provide
a good approach of the biological model when appropriate locomotion patterns
are implemented [9].
The problem in the control of mobile systems is the proper application of
parameters according to the goals to be achieved. Deﬁning the sampling rate,
which controller to use, how much accuracy the locomotion pattern requires, the
energy eﬃciency and the speciﬁcations of the actuators and sensors, are necessary
eﬀorts that not always are a trivial solution. In most cases the mathematical
models of the systems are complex and non-linear, and to ﬁnd a proper controller
for such systems require the application of linearization techniques in order to
reach a simple and similar plant for the robot.
Having these ideas in mind, this paper presents an overview of the imple-
mented robot model in Sect. 2, considerations about the planned trajectory are
shown in the Sect. 3, the implementation and analysis of the control system
using Matlab R
⃝/Simulink
TM are presented in Sect. 4 and ﬁnally the conclusion
and future developments are given in Sect. 5.
2
Model Overview
The model of the robot used in this work is made of a four-part body with two
rotational joints in each leg, which provide angular movements along the Z axis.
Between each two adjacent parts of the body there’s a spring and damper system
associated with a 2 DOF joint that gives more ﬂexibility to the movements of
the body [13]. In order to improve the simulated model, a foot-ground contact
system, using equations of reaction forces in X and Y axis, was implemented.
Each part of the robot is associated with an inertia matrix according to its shape.
The robot has a total weight of 4 kg. Figure 1a shows the aspect of the robot
model in SimMechanics
TM. The simpliﬁed diagram with connections between
body, legs and foot-ground systems is presented in Fig. 1b. The diagram also has
a GND block that is needed to provide the robot its position in relation to the
ground [15].
2.1
Body Subsystems
The body has four interconnected blocks, each with its own rectangular shape
inertia matrix Ib with mass mb = 0.9 kg, which is described in Eq. (1) due to its
symmetry around coordinated axis [13]. The robot body dimensions are width
(wb) = 0.30 m, depth (db) = 0.20 m and height (hb) = 0.15 m. The complete
body subsystem is shown in Fig. 2.

758
I. Oliveira et al.
(a) Simulated quadruped robot aspect
in SimMechanics
TM.
(b) Simpliﬁed diagram of the model.
Fig. 1. Quadruped robot model overview.
Fig. 2. Body scheme overview in SimMechanics.
Ib = 1
12
⎡
⎣
mb(h2
b + d2
b)
0
0
0
mb(w2
b + d2
b)
0
0
0
mb(w2
b + h2
b)
⎤
⎦
(1)
The CS2 port present in each body block is used to connect the corresponding
leg subsystem, and the ground reference is connected to the center of gravity of
the BODY 1.
2.2
Leg Subsystems
As mentioned, each leg has two rotational joints: one in the hip and one in the
knee. The implemented leg subsystem is depicted in Fig. 3.

Control of a Quadruped Robot
759
Fig. 3. SimMechanics diagram of the leg subsystems.
The two IC blocks provides the initial angle of each joint in the simulation.
The subsystem is connected in its left to the corresponding body block and in
its right to its foot-ground block. The control system sends the torque signal to
the actuator blocks and its feedback is provided by the sensor blocks.
The cylindrical inertia matrix associated with the body blocks leg sup and
leg inf has radius r = 0.02 m, length L = 0.2 m and mass ml = 0.05 kg. Its
equation, for the applied case of alignment in X axis, is provided by (2) [13].
Il =
⎡
⎣
1
2mlr2
0
0
0
1
12ml(3r2 + L2)
0
0
0
1
12ml(3r2 + L2)
⎤
⎦
(2)
2.3
Foot-Ground Subsystems
The foot-ground subsystems are included in the model to get a closer approach
to a real world robot. Its architecture is shown in Fig. 4.
The sensor provides the velocity and position of the robot feet in order to
compute friction and normal forces that the robot applies in the ground. The
applied forces are calculated through Eqs. (3) and (4), which happens only when
foot touch the ground.
Fx = −Kx(x −x0) −Bx ˙x
(3)
Fy = −Kyy −By(−y)m ˙y
(4)
The equation used to compute friction forces is based on a linear spring-
damper system in which Fx is the applied force, Kx is the elastic constant of
the spring, x and x0 are the current and previous X axis position in relation to
the ground, respectively, Bx is the constant of the damper and ˙x is the velocity
in X axis provided by the body sensor.
For the normal force calculation was chosen a non-linear spring-damper equa-
tion because it presented better results in tests [11]. In this case, Fy is the applied

760
I. Oliveira et al.
Fig. 4. SimMechanics diagram of the foot-ground subsystems.
normal force, Ky is the elastic constant of the spring, y is the current Y axis posi-
tion, By is the damper constant, m is a factor that depends on the characteristics
of the soil and ˙y is the velocity in Y axis provided by the body sensor.
3
Trajectory Planning
Trajectory planning is crucial to the robot’s locomotion. A good planning means
higher eﬃciency because it maximises the generated movement and minimises
the spent energy used to generate it. With this in mind, a good option to legged
locomotion is to use a cycloid trajectory during the transfer phase of the leg. In
this way, the implemented trajectory is given by the expressions:
PT =

VF

t −tT0
2π sin(2πt
tT0
)

, Fc
2

1 −cos(2πt
tT0
)
T
(5)
PS = [VF T0, 0]T
(6)
where T0 is the cycle time, VF is the frontal velocity of the body deﬁned by
VF = LS/T0, tT0 is the transfer phase time given by tT0 = (1 −β)T0 (where β is
the duty factor of the leg), t is the time variable and Fc is the maximum height
of the cycloid.
In order to make possible the implementation of the gait it is needed to
analyse the kinematics of the robot. Kinematics are responsible for providing
the reference angles to the controller based on the trajectory planned positions.
The study of kinematics was performed for one leg only, since the others present
the same shapes and sizes. The kinematic model of a leg with two degrees of
freedom was used in the case.

Control of a Quadruped Robot
761
4
Control System and Tuning
Due to the diﬃculty of ﬁnding a model that describes the robot’s non-linear
behaviour, the design of the control system appeals to linearization methods to
ﬁnd a state-space representation that describes the main dynamics of the robot.
This was made in order to ﬁnd a proper solution for the tuning of controller
parameters. After the tuning, the controller will act in robot’s non-linear system.
The simpliﬁed block diagram of the system with the controller is shown in Fig. 5.
Fig. 5. Overall control system diagram.
The adopted procedure for the tuning of the controller was the following:
(a) Select the IO points of
(b) Select the parameters
the robot for linearization;
that will be trimmed;
(c) Obtain the linear state space of
(d) Find a proper compensator for the linearized
the plant using the trim model;
plant using classic control methods;
(e) Test the selected parameters
(f) If necessary, make ﬁne adjustments
in the non linear model of the robot;
and repeat the last step.
The linearization of the system was made using the Simulink Linear Analysis
tool. The linearized model was obtained using the trim model technique in the
hip and knee of leg 1. All the joint’s positions and velocities remained untrimmed,
while all the other parameters were setted to steady state. Since all legs have
the same length and shape, there was no need to reproduce the analysis for all
of them. The step-by-step linearization process is not in the scope of this work,
and both techniques can be found in Simulink Control Design reference [14].
After linearization, the space state model was used in the Simulink Control
System Analysis in order to obtain a proper compensator. The parameters were
tuned using the PID Tuning tool shown in Fig. 6. A good option to this case is
to use the PD controller with ﬁrst order ﬁlter due to the varying reference θd(t).
The transfer function of the PD controller with ﬁlter for the leg i and joint
j is given by a lead type compensator formula, as described in Eq. (7).
Cij(s) = Kij
s + Zij
s + Pij
, Zij < Pij
(7)
Due to its resemblance, the controller Ci1 and the controller Ci2 were tuned
once and used in all hip and knee joints, respectively.

762
I. Oliveira et al.
Fig. 6. Tuning interface for the compensator.
5
Tests and Results
The tests were carried while the robot was adopting the trot gait. The velocity
VF and the parameters Kij, Zij and Pij of the controllers were varied in tests
for a saturation limit of ±20 Nm in order to obtain a ﬁrst approach. After that,
the best tuning was applied to diﬀerent values of saturation and the results
were compared. The number of simulated cycles was nc = 5 corresponding to
a simulation time of t = nc ∗T0. The sampling period was 0.005 s and the step
length was LS = 0.2 m. A total of ten tests were performed varying the controller
parameters, and each test considers ﬁve diﬀerent periods. Finally, for the best
case the system was tested for ﬁve diﬀerent values of saturation.
5.1
Performance Indices
The used performance indices were the Integral of Absolute Error (Iae), the
Integral of Squared Error (Ise) and its percentage in relation to the reference θd
area, %Iae and %Ise, given respectively by expressions (8)–(11).
Iaehip =
4

i=1
	 nT0
0
|θdi1 −θi1|dt
,
Iaeknee =
4

i=1
	 nT0
0
|θdi2 −θi2|dt
(8)
Isehip =
4

i=1
	 nT0
0
|θdi1 −θi1|2dt
,
Iseknee =
4

i=1
	 nT0
0
|θdi2 −θi2|2dt
(9)

Control of a Quadruped Robot
763
%Iae =
4

i=1
2

j=1

 nT0
0
|θdij −θij|dt

 nT0
0
|θdij|dt
∗100
(10)
%Ise =
4

i=1
2

j=1

 nT0
0
|θdij −θij|2dt

 nT0
0
|θdij|dt
∗100
(11)
where θdij(t) is the desired angle and θij(t) is the measured angle for the leg i
and joint j.
As a measure of eﬃciency of the system is used the average energy consump-
tion by travelled distance, Eav, given by:
Eav = 1
d
4

i=1
2

j=1
	 T0
0
|τaij(t) ˙θij(t)|dt
(12)
where τaij is the applied torque and ˙θij is the measured velocity in each leg i
and joint j of the robot. Note that Eav assumes that energy regeneration is not
available by actuators doing negative work [10].
For actuator size considerations, the maximum torque of the hips τhip and
knees τknee, which corresponds to the stall torque needed to move the joint
during a brief period of time, are also computed.
5.2
Results
After several tests with diﬀerent control parameters, the results presented in
Table 1 are given by test number 5, which best satisﬁed the desired performance
in terms of error and eﬃciency. The best parameters are the ones that have the
lowest values of Eav∗%Iae and Eav∗%Ise, as shown in Figs. 7 and 8, respectively.
Table 1. Best results of the tests.
VF
Iaehip
Iaeknee Isehip Iseknee %Iae %Ise d
Eav
(ms−1) (deg)
(deg)
(deg)
(deg)
(m)
(Jm−1)
0.1
6.176 2.931
2.170
0.401
0.161 0.045 0.745 22681.668
0.05
6.571 4.344
1.548
0.414
0.096 0.017 1.126
9756.350
0.033
8.110 5.993
1.347
0.441
0.083 0.011 1.030
7425.555
0.025
11.249 7.892
4.216
0.631
0.084 0.021 0.981
5942.543
0.02
13.717 9.802
5.039
0.810
0.083 0.021 1.000
4645.231
The controller parameters that correspond to the results of Table 1 are shown
in Table 2.
Using the parameters of Table 2, the next test was to vary the saturation
limit. For all ﬁve tests performed, the results are shown in Table 3. As expected,

764
I. Oliveira et al.
Fig. 7. Values of Eav multiplied by %Iae for each test.
Fig. 8. Values of Eav multiplied by %Ise for each test.
Table 2. Controller parameters for the best case.
Parameter Hip joint Knee joint
Kij
77
204
Zij
20.88
20.88
Pij
150
150
Table 3. Saturation tests in the best case of the controller.
τsat
Iaehip
Iaeknee Isehip
Iseknee %Iae %Ise
d
Eav
(Nm) (deg)
(deg)
(deg)
(deg)
(m)
(Jm−1)
20
6.571
4.344
1.548
0.414
0.096
0.017 1.126
9756.350
15
6.867
4.814
2.368
0.639
0.103
0.027 1.071
9869.309
10
8.810
5.401
5.488
1.130
0.125
0.058 0.827
9789.381
5
73.618 11.784
652.621
8.986
0.753
5.836 0.862
5179.345
4
103.717 14.724
1292.455 17.459
1.045 11.555 0.905Z 3974.033
Fig. 9 shows that the higher energy eﬃciency Eav is obtained for the lowest
saturation. On the other hand, higher precision leads to higher torques, as shown
in Fig. 10. The joint trajectories for a maximum torque of 10 Nm are shown in

Control of a Quadruped Robot
765
Fig. 9. Values of Eav versus saturation torque.
Fig. 10. Values of %Iae versus saturation torque.
Fig. 11. Joint trajectories of leg 1 for a saturation of 10 Nm.
Fig. 12. Total distance travelled by the robot for a saturation of 10 Nm in the joints.

766
I. Oliveira et al.
Fig. 11 while in the Fig. 12 the position of the robot’s body is illustrated. As
can be seen, the robot follows the desired joint trajectories with good accuracy,
travelling a total distance of approximately 1 m.
6
Conclusions
The presented work focused in the implementation of a control system for a
quadruped robot using linear analysis and trajectory planning. The planned tra-
jectory presents good results when adopted the trot gait, especially for a velocity
of VF = 0.05 ms−1. The linearization provides a good response to the behaviour
of the controllers, but still require ﬁne adjustments to get better results. It is
also concluded that the error is directly associated with the joint’s torque, which
may vary with the presence of saturation in the controllers. Although several
parameters can still be changed to improve the results, this work provided an
analysis of some relevant aspects of modelling and control of a quadruped robot
in simulation environment and represents the ﬁrst steps for further developments.
Acknowledgments. This work is ﬁnanced by the ERDF - European Regional Devel-
opment Fund through the Operational Programme for Competitiveness and Interna-
tionalisation - COMPETE 2020 Programme within project POCI-01-0145-FEDER-
006961, and by National Funds through the FCT - Funda¸c˜ao para a Ciˆencia e a
Tecnologia (Portuguese Foundation for Science and Technology) as part of project
UID/EEA/50014/2013.
References
1. Hutter, M., et al.: ANYmal - a highly mobile and dynamic quadrupedal robot.
In: 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), Daejeon, pp. 38–44 (2016). https://doi.org/10.1109/IROS.2016.7758092
2. Raibert, M.H.: Legged Robots that Balance. MIT press, Cambridge (1986)
3. Cao, Q., van Rijn, A.T., Poulakakis, I.: On the control of gait transitions in
quadrupedal running. In: 2015 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), Hamburg, pp. 5136–5141 (2015). https://doi.org/10.
1109/IROS.2015.7354100
4. Gao, F., Qi, C., Sun, Q., Chen, X., Tian, X.: A quadruped robot with parallel mech-
anism legs. In: 2014 IEEE International Conference on Robotics and Automation
(ICRA), Hong Kong, p. 2566 (2014). https://doi.org/10.1109/ICRA.2014.6907223
5. Rutishauser, S., Sprowitz, A., Righetti, L., Ijspeert, A.J.: Passive compliant
quadruped robot using Central Pattern Generators for locomotion control. In:
2008 2nd IEEE RAS & EMBS International Conference on Biomedical Robot-
ics and Biomechatronics, Scottsdale, AZ, pp. 710–715 (2008). https://doi.org/10.
1109/BIOROB.2008.4762878
6. Igarashi, H., Machida, T., Harashima, F., Kakikura, M.: Free gait for quadruped
robots with posture control. In: 9th IEEE International Workshop on Advanced
Motion Control (2006). https://doi.org/10.1109/AMC.2006.1631698
7. MacDonald, W.S., Grupen, R.A.: Building walking gaits for irregular terrain from
basis controllers. In: 1997 IEEE International Conference on Robotics and Automa-
tion, Proceedings, vol. 1. IEEE (1997)

Control of a Quadruped Robot
767
8. Hutter, M., et al.: Quadrupedal locomotion using hierarchical operational space
control. Int. J. Robot. Res. 33(8), 1047–1062 (2014)
9. Suzuki, H., Nishi, H., Aburadani, A., Inoue, S.: Animal gait generation for
quadrupedal robot. In: Second International Conference on Innovative Comput-
ing, Informatio and Control (ICICIC 2007), Kumamoto, p. 20 (2007). https://doi.
org/10.1109/ICICIC.2007.169
10. Silva, M.F., Machado, J.A.T.: Kinematic and dynamic performance analysis of
artiﬁcial legged systems. Robotica 26(1), 19–39 (2008)
11. Silva,
M.,
Machado,
J.,
Lopes,
A.:
Modelling
and
simulation
of
artiﬁcial
locomotion
systems.
Robotica
23(5),
595–606
(2005).
https://doi.org/10.1017/S0263574704001195
12. Muybridge, E.: Animals in Motion. Courier Corporation (2012)
13. Silva, M., Barbosa, R., Castro, T.: Multi-legged walking robot modelling in MAT-
LAB/Simmechanics and its simulation. In: 2013 8th EUROSIM Congress on Mod-
elling and Simulation (EUROSIM). IEEE (2013)
14. Mathworks. Simulink Control Design Reference. Version R2016b. User guide (2016)
15. Mathworks. Simscape Multibody Reference. Version R2016b. User guide (2016)

Communication-Aware Robotics (I)

Cooperative Perimeter Surveillance Using
Bluetooth Framework Under Communication
Constraints
J.M. Aguilar(B), Pablo Ramon Soria, B.C. Arrue, and A. Ollero
University of Seville, 41092 Seville, Spain
josagulop@alum.us.es, {prs,barrue,aollero}@us.es
Abstract. The work presented focuses in the simulations and real
experiments of perimeter surveillance under communication constraints,
performed by teams of UAVs using a Bluetooth communication frame-
work. When UAVs work in a colaborative manner, communication among
them is essential to properly perform their task. Moreover, energy con-
sumption and weight of the devices equipped in a UAV are important to
be reduced at minimum possible, particularly in micro-UAVs. A coordi-
nation variables strategy is implemented to perform the perimeter divi-
sion.
Keywords: Bluetooth low energy · UAV · Communication · Coordina-
tion variables
1
Introduction
Nowadays, there is a great development and investment in the ﬁeld of the
unmanned air systems or UAS, commonly called drones. There are uncountable
applications thanks to their adaptability to diﬀerent tasks, as for instance in
agriculture, mapping, delivery services, border defense, search and rescue oper-
ations, etc. Due to this, they have been the subject of much research to improve
their autonomy, obstacle avoidance, localization, cooperation, path planing.
This article focuses in a group of robots which divides a perimeter to surveil
it in a cooperative manner. Any surveillance system is made of many activities
that can be summarized in three main activities:
– The detection of new events, intruders or information of interest. This task
is strongly dependent on the movement planiﬁcation strategy of the aerial
robots, which is based on the information available and the estimations about
the problem, the environment and the situation of the rest of the aerial robots.
– The communication between the elements of the system, so each of them
is aware of the whole system. This also involves the movement planiﬁcation
strategy as it needs the system status.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_63

772
J.M. Aguilar et al.
– The assignment of the best robot to handle every detected event. This can be
made of two manners: as a centralized system, in which case a control station
decides which aerial robot should handle each task; and as a distributed
system, in which case a dynamic target allocation among the aerial robots
can be addressed to assign the targets in the most eﬃcient way.
The work of this article is focused in the test of communication among the
robots of a team when range constraints exist. Real experiments have been per-
formed with UAVs equipped with Bluetooth devices in order to allow them
to communicate themselves, and also with an algorithm based on coordination
variables to partition the total perimeter assigned to the group.
This system has been tested on simulations and on a real location in the
Escuela Tecnica Superior de Ingenieria of Seville (Spain). The remain of the
article is structured as follows:
– Sect. 2 summarizes the state of the art work.
– Sect. 3 introduces and explains how works the Bluetooth devices chosen for
the experiments, and a brief description of the operating framework.
– Sect. 4 details the developed perimeter division protocol used.
– Sect. 5 shows both simulation and real experiments, and their evaluation.
– Sect. 6 presents the conclusions of the article.
2
State of the Art
The cooperation among robots to carry out a widely range of tasks has been
studied in all kind of robots. In [14,17] authors develop a system with multiple
ground robots capable of covering area and tracking targets in a cooperative
manner. One robot behaves as the master and reaches its objectives by other
robots who behave as the slaves. In [19] authors paid attention to aerial robots,
which track targets in the optimal way using the information of their neighbors.
In [15], the system is also conformed with UAVs, the algorithm asigns missions
in autonomously way based on their level of importance.
The way the robots communicate among them depends on the application
pursued. For instance, Zigbee is an standard protocol widely used for DIY
projects. In [3,18] authors used this protocol for communicating nodes and robots
in relative large distance, but its main disadvantage is its greater energy require-
ments in comparison with other systems.
Authors in [6,13] used bluetooth technology to connect the robots as opposed
due to its lower energy consumption and weight. In [5,11,16] authors worked in
the localization of the robots given the communication framework, though due
to being focused in ground robots the task was simpler.
In this article the commmunication framework used is the one exposed in
[8], based on standard Bluetooth Low Energy 4.1 connections. It works with the
Nordic nRF51 chipset in combination with the S130 SoftDevice [12].

Cooperative Perimeter Surveillance Using Bluetooth Framework
773
The perimeter surveillance has been a ﬁeld of study in lately years. A way
of approach to this problem is through UAS and ground stations in the perime-
ter to manage the communication [2], or also using a method based on linear
programming and Markow chain as it is posed in [4].
Other methods, more interesting when there are communication constrains,
are distributed and decentralized: authors in [9] proposed a robust solution based
on behaviour control of the multi-robot system. Another possibility are the coor-
dination variables methods, which are proved to be fast convergent solution. In
[7], it is developed an algorithm to coordinate a team of small homogeneous UAS
to perform cooperatively a perimeter partitioning strategy, using coordination
variables. This aforenamed algorithm slightly modiﬁed to consider heterogeneous
robots is the one used in this paper, as poses in [?].
3
Bluetooth Devices and Framework
The use of Bluetooth Low Energy (BLE) has been steadily growing just since
its initial launch in the market of the wireless communications, aimed at appli-
cations such of Internet of Things (IoT). This involves monitoring of wireless
sensor networks and control of applications that are in continuous communica-
tion of state variables. The coordination of UAVs problem is very similar to this
statement.
In this work it is taken as a development framework an opensource imple-
mentation of a mesh network oﬀered by M-Way Solutions [10]. This approach is
placed in the upper layers of application and host of the Nordic nRF51 chipset
in combination with the S130 SoftDevide so that it manages the roles of the
diﬀerent devices in the communications including self-healing capabilities and
implementation with battery powered devices. This device is advantageous small
and low power consumption, which suits for micro-uav teams.
The S130 SoftDevice from Nordic enable the use of three central device con-
nections and one peripheral. To manage the communication, the device estab-
lishes connections with its neighbors and also assigns group identiﬁers. All the
nodes that got the same group identiﬁer will be considered part of a swarm,
so they can exchange information between them. Due to this, this structure
conforms a scatternet topology (Fig. 1) which interconnects all the robots.
Everytime a connection is made, the SDK serves an ID and the RSSI for that
connection, which is corresponded to a power indicator in the received signal in
a radio communication, and this can be used in diﬀerent wireless protocols.
About the speciﬁcations of the BLE technology, there is one specially impor-
tant and remarkable, its data transfer limit. The conception of the BLE is focused
on the transmission of small amounts of data at 1 Mbps, and in short distances
up to 10 m. Due to the management of limited data packages in the connections,
the BLE technology has another fundamental characteristic: a transmission time
under the 3 ms, letting it to work in real time developments.

774
J.M. Aguilar et al.
Fig. 1. Example of the scatternet topology. Blue dots are the nodes, and the blue
circles represent the range of the communications
4
Perimeter Division Protocol Based on Coordination
Variables
The algorithm implemented to perform the perimeter surveillance is the one
exposed in [1] based on coordination variables. It is a distributed and decentral-
ized algorithm. Due to the short range of communication most of the time a
robot of the patrol will be isolated from the others robots, so this kind of model
allow the multi-robot system to converge to the ﬁnal path partitioning through
local decisions and asynchronous information exchanges, without any kind of
hierarchical levels among the robots.
Being B the entire perimeter deﬁned as:
B := {b(s) ∈Rk : s ∈[0, L]}
(1)
where b is a curve to cover the whole path B, s is deﬁned as the distance to the
initial path position b(0) along the curve b, and L is the length of the path B,
being b(L) the ﬁnal position.
Each robot Qi decides autonomously its own segment Bi := [b(sinf
i
), b(ssup
i
)]
in order to the total amount of robots patrol the entire perimeter B. Thus, each
Qi uses a back and forth motion between its own ﬁrst segment point b(sinf
i
) and
its own last segment point b(ssup
i
).
Hence, any robot Qi knows its maximum motion speed vmax
i
, its current
direction movement (right or left in its own segment) di, and its current position
si into the curve b.
The algorithm implemented uses a set of variables called coordination vari-
ables, which represent the minimum information necessary for each robot Qi to
calculate its own segment Li. These variables are:
lengthi = L
speedsum
i
=
n

j=1
vmax
j
, ∀i = 1, 2, ..., n
(2)

Cooperative Perimeter Surveillance Using Bluetooth Framework
775
Being lengthi the length of the total perimeter, and speedsum
i
the sum of
all robots maximum speed. In addition to these, each robot Qi has a set of
intermediate variables which are required to calculate the coordination variables:
– lengthleft
i
is the length of perimeter that Qi has currently on its left.
– lengthright
i
is the length of perimeter that Qi has currently on its right.
– speedleft
i
is the sum of the speed of all the robot which are at left of Qi.
– speedright
i
is the sum of the speed of all the robot which are at right of Qi.
The sequence of the algorithm is as following: the robot Qi moves at its
maximum speed vmax
i
along its segment Bi. In the case that it reachs the end
of Bi, Qi does not stop its movement but continues till it communicates with
another Qj, and both exchange their variable information and updates their
segments, or it arrives at the end of the perimeter.
If robot Qj meets robot Qi by Qi right side, Qj sends it all the information
about its right side, scilicet, its sum of speeds speedright
j
and its length on its
right side lengthright
j
. Qi does the same but with the left side variables.
Then, both Qi and Qj use this new information to update their coordination
variables, lengthi and speedsum
i
:
speedsum
i
= speedleft
i
+ speedright
i
+ vmax
i
lengthi = lengthleft
i
+ lengthright
i
(3)
And with this update they can calculate their segment [sinf
i
, ssup
i
] as follows:
sinf
i
= speedleft
i
lengthi
speedsum
i
ssup
i
= sinf
i
+ vmax
i
lengthi
speedsum
i
(4)
If one robot reachs the initial si = 0 or ending si = L position in the perimeter,
it also updates its variables according to its direction di and turns back.
The algorithm minimizes the information exchanges because robots only has
to communicate with their neighbors.
Fig. 2. A team of 4 UAVs with diﬀerent speeds implementing a perimeter division
strategy

776
J.M. Aguilar et al.
5
Experimental Testing
A set of simulation and experimental results are provide to demonstrate the eﬀec-
tiveness of the communication framework works and validate Bluetooth devices.
Results show that UAVs divide the perimeter proportionally to their own capa-
bilities.
5.1
Simulation Results
The proposed distributed algorithm is based on the coordination variables.
UAVs’ model and communication framework simulation have been developed
in C++. The initial positions has been deﬁned randomly, but the speeds have
been chosen proportionally between them in order to obtain a visual result under-
stable at one glance.
The ﬁrst simulation consist on 4 UAVs patrolling single line of 160 m, similar
to Fig. 2, being their speeds 1 m/s, 2 m/s, 3 m/s, and 4 m/s. Figure 3(a) shows
the result. As it can be observed, each UAV patrol a length proportional to their
speed. For instance, the slower UAV patrol a length which is a quarter of the
faster UAV, accordingly to the relation between their speeds. It is highlighted
that the ﬁnal solution is not a unique point but a cycle. This will be discussed
in the next simulation.
(a) Patrolling a line
(b) Patrolling a square
Fig. 3. Graphic representation of the UAVs movements in the experiments
The second simulation is the same 4 UAVs patrolling not a single line, but
the square made of the previous line of 160 m, namely, a square of side 40 m.
Figs. 3(b) and 4 shows the result of this simulation.
Though it seems that the UAVs have converged in no solution, the ﬁnal
solution of this division problem is not a single point but a cycle. Indeed, taking
a careful glimpse, for instance, the left image of Fig. 4, we can easily see how
the system converges quickly to a cycle of 10 points which are clearly signaled
in Fig. 5. Through diﬀerent simulation experiments, modifying the number of
robots, their speeds, the perimeter to surveil and other sets of variables, it is
enough clear that all them aﬀects the number of points of the ﬁnal solution of

Cooperative Perimeter Surveillance Using Bluetooth Framework
777
Fig. 4. Simulation results of 4 UAVs patrolling a square of side 40 m. Images only
show the position of 2 of the 4 UAVs for clarity. Left image shows position X in meters
against time; right image shows position Y in meters against time
Fig. 5. Signalization of the cycle of ten points
the system, but which is the exact relation is not utterly determined, because of
being out of the scope of this article.
The algorithm proposes is robust enough to variations in the system. The
last simulation performed present the same 4 UAVs patrolling the square of
side 40 m, but two of them get lost at the middle of the mission. As it can be
observed in the Figs. 6 and 7, the other two UAVs patrolling the whole perimeter
converging in a new cycle solution.
Fig. 6. Simulation results of 4 UAVs patrolling a square of side 40 m, getting lost two
of them

778
J.M. Aguilar et al.
Fig. 7. Simulation results of 4 UAVs patrolling a square of side 40 m. Images only
show the position of 3 of the 4 UAVs for clarity. Left image shows position X in meters
against time; right image shows position Y in meters against time
5.2
Real Experiment
The location selected to carry out the real experiment was an outdoor zone of the
laboratories of the University of Seville Escuela Tecnica Superior de Ingenieria
that is shown in Fig. 8(a). Only two UAVs were used in the experiment, in order
to demonstrate the communication framework. Thanks to the scalability of the
perimeter division algorithm, which was proved in the simulations, two UAVs
are enough: more robots would increase the number of point of the ﬁnal cycle
solution, and would make the graphics more diﬃcult to read.
(a) Zone of the experiments
(b) Perimeter division
Fig. 8. Satellite images of the experiments
Both UAVs were equipped with the required sensor to know their own GPS
position, and the aforenamed Nordic BLE device. Also they have an Intel NUC
computer to compute the perimeter division algorithm and the communication
instructions load in them. The experiment takes approximately 3 min to com-
plete.
Figure 8(b) shows the result of the experiment over the satellite image of the
experiment zone. The UAVs patrol the perimeter till they meet themselves or

Cooperative Perimeter Surveillance Using Bluetooth Framework
779
Fig. 9. The 3D representation is turned in order to ease the visualization of the con-
nections (continuous line) and disconnections (dashed line) of the UAVs
reach the end of the perimeter, in both case they updates their coordination vari-
ables, recalculate their routes and turn back to continue their patrol. Figure 9 is
included to show the connection and disconnection between both UAVs. It is a
remarkable fact that the bluetooth devices are able to maintain the connection
further away than the 10 m speciﬁed, even though to establish that connection
they do need to be in the 10 m range. Finally, the Fig. 10 is the same 3D rep-
resentation but projected to over the latitude-longitude plane: in this image is
easier to see the perimeter, but slightly more diﬃcult to see when they connect
or disconnect.
Fig. 10. 3D representation of the connections (continuous line) and disconnections
(dashed line) of the UAVs projected over the longitude-latitude plane
6
Conclusions
Real outdoor experiments have been performed with satisfactory results. Both
communication framework and perimeter division algorithm have been demon-
strated to be robust and eﬃcient. The Nordic bluetooth devices are proved to

780
J.M. Aguilar et al.
work properly, being an excellent option to use for UAVs communication thanks
to their low consumption energy and weight. The devices connect within the
10 m range, and even maintain the communication till the 20 m. As future work
it would be interesting evaluate the existent relation between the number of
UAVs, their capabilities, the length and shape of the perimeter, etc., and the
number of points of the ﬁnal solution cycle. Likewise, another future work could
be an experiment with more UAVs not only performing a perimeter division but
also allocating tasks, as tracking diﬀerent moving targets. Also measuring the
energy consumption of the BLE devices along the experiment would be a good
subject of study.
Acknowledgements. This work was partially supported by the European Commis-
sion H2020-ICT Programme under the project MULTIDRONE (731667).
This work has been developed in the framework of the project AEROARMS (SI-
1439/2015) EU-funded project and the project AEROMAIN (DPI2014-59383-C2-1-R).
References
1. Acevedo, J.J., Arrue, B.C., Maza, I., Ollero, A.: Cooperative perimeter surveillance
with a team of mobile robots under communication constraints. In: 2013 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS), pp. 5067–
5072. IEEE (2013)
2. Beard, R.W., McLain, T.W., Nelson, D.B., Kingston, D., Johanson, D.: Decentral-
ized cooperative aerial surveillance using ﬁxed-wing miniature UAVs. Proc. IEEE
94(7), 1306–1324 (2006)
3. Benavidez, P., Nagothu, K., Ray, A.K., Shaneyfelt, T., Kota, S., Behera, L.,
Jamshidi, M.: Multi-domain robotic swarm communication system. In: IEEE Inter-
national Conference on System of Systems Engineering, SoSE 2008, pp. 1–6. IEEE
(2008)
4. Darbha, S., Krishnamoorthy, K., Pachter, M., Chandler, P.: State aggregation
based linear programming approach to approximate dynamic programming. In:
2010 49th IEEE Conference on Decision and Control (CDC), pp. 935–941. IEEE
(2010)
5. Esteves, Y.R., Concejero, J.B., Jim´enez, A.V.: Indoor localization of the points of
interest using ro-slam. In: 2015 12th International Joint Conference on e-Business
and Telecommunications (ICETE), vol. 1, pp. 35–42. IEEE (2015)
6. Herbrechtsmeier, S., Witkowski, U., R¨uckert, U.: Bebot: a modular mobile minia-
ture robot platform supporting hardware reconﬁguration and multi-standard com-
munication. In: FIRA RoboWorld Congress, pp. 346–356. Springer (2009)
7. Kingston, D., Beard, R.W., Holt, R.S.: Decentralized perimeter surveillance using
a team of UAVs. IEEE Trans. Rob. 24(6), 1394–1404 (2008)
8. M-Way solutions (2015), https://github.com/mwaylabs/fruitymesh/wiki
9. Marino, A., Parker, L., Antonelli, G., Caccavale, F.: Behavioral control for multi-
robot perimeter patrol: a ﬁnite state automata approach. In: IEEE International
Conference on Robotics and Automation, ICRA 2009, pp. 831–836. IEEE (2009)
10. Heil, M.: Bluerange meshing now open source. @ ONLINE 2015 (2015)
11. Menegatti, E., Zanella, A., Zilli, S., Zorzi, F., Pagello, E.: Range-only slam with
a mobile robot and a wireless sensor networks. In: IEEE International Conference
on Robotics and Automation, ICRA 2009, pp. 8–14. IEEE (2009)

Cooperative Perimeter Surveillance Using Bluetooth Framework
781
12. Nordic
Semiconductor
(2013),
https://www.nordicsemi.com/eng/Products/
Bluetooth-low-energy/nRF51822
13. P´asztor, A., Kov´acs, T., Istenes, Z.: Compass and odometry based navigation of a
mobile robot swarm equipped by bluetooth communication. In: 2010 International
Joint Conference on Computational Cybernetics and Technical Informatics (ICCC-
CONTI), pp. 565–570. IEEE (2010)
14. Patil, D.A., Upadhye, M.Y., Kazi, F., Singh, N.: Multi robot communication and
target tracking system with controller design and implementation of swarm robot
using arduino. In: 2015 International Conference on Industrial Instrumentation
and Control (ICIC), pp. 412–416. IEEE (2015)
15. Sampedro, C., Bavle, H., Sanchez-Lopez, J.L., Fern´andez, R.A.S., Rodr´ıguez-
Ramos, A., Molina, M., Campoy, P.: A ﬂexible and dynamic mission planning
architecture for UAV swarm coordination. In: 2016 International Conference on
Unmanned Aircraft Systems (ICUAS), pp. 355–363. IEEE (2016)
16. Sun, D., Kleiner, A., Wendt, T.M., et al.: Multi-robot range-only slam by active
sensor nodes for urban search and rescue. In: RoboCup, pp. 318–330. Springer
(2008)
17. Yang, B., Ding, Y., Hao, K.: Area coverage searching for swarm robots using
dynamic voronoi-based method. In: 2015 34th Chinese Control Conference (CCC),
pp. 6090–6094. IEEE (2015)
18. Zhang, B., Gao, S.: The study of zigbee technology’s application in swarm robotics
system. In: 2011 2nd International Conference on Artiﬁcial Intelligence, Manage-
ment Science and Electronic Commerce (AIMSEC), pp. 1763–1766. IEEE (2011)
19. Zhou, Y., Dong, X., Zhong, Y.: Time-varying formation tracking for UAV swarm
systems with switching interaction topologies. In: 2016 35th Chinese Control Con-
ference (CCC), pp. 7658–7665. IEEE (2016)

Development of an Adaptable
Communication Layer with QoS Capabilities
for a Multi-Robot System
Hannes Harms(B), Julian Schmiemann, Jan Schattenberg, and Ludger Frerichs
Institute of Mobile Machines and Commercial Vehicles, TU Braunschweig,
38106 Braunschweig, Germany
{hannes.harms,j.schmiemann,j.schattenberg,ludger.frerichs}@tu-bs.de
Abstract. In this paper we present an approach to connect a multi-
robot system of unmanned ground and aerial vehicles inside a mobile
ad-hoc network to exchange sensor data requiring a high bandwidth.
The introduced communication layer extends the messaging system of
an underlying middleware (ROS) to work with non-ideal wireless links.
Every connection can be conﬁgured with a user deﬁned data rate, pro-
tocol and priority to control the overall traﬃc load in the network. If
required, messages can be buﬀerd during communication outages or due
to low available bandwidth. Moreover, the message transport system con-
tains mechanisms for lossless and lossy data compression as well as an
user interface to quickly react to diﬀerent application conditions.
1
Introduction
One key challenge in developing a multi-robot system, is to provide an appro-
priate communication system ﬁrstly for robot interaction and secondly for link-
ing human machine interfaces. The characteristics of the communication system
highly depend on the scope and area of application. However, many middleware
frameworks used nowadays have a central service for establishing connections
between software components that has to be reachable. They do not provide
mechanisms for data compressions on-the-ﬂy and the QoS (Quality of Service)
parameters are only accessible for developers whereby human operators have no
possibility to adapt communication behaviour to their needs. Within the joint
development project ANKommEn a semi-automatic machine cluster consisting
of three unmanned aerial vehicles (UAV), two unmanned ground vehicles (UGV)
and a human operator with a control interface is developed to collect, process and
visualize diﬀerent sensor data on-the-ﬂy. Every ground vehicle is equipped with
a 3D-LiDAR sensor and a camera. In contrast each aerial vehicles has diﬀerent
sensor setup e.g. a high resolution camera for aerial images, a 3D-LiDAR sensor
for mapping and a thermal camera for ﬁreﬁghting support. The sensor data is
used to create a data base to decide about adequate rescue actions or to coor-
dinate human rescue forces. The therefore used network is based upon a VHT
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_64

Development of an Adaptable Communication Layer with QoS Capabilities
783
(Very High Throughput) 802.11ac mesh network in ad-hoc mode. Mobile ad-hoc
networks (MANETs) can be used without any ﬁxed networks infrastructure,
which makes them suitable for disaster scenarios. Moreover, MANETS provide
a very ﬂexible mechanism to form a network between participants that meet
in a certain area. The wireless communication has to deal with a high network
load and should be as robust as possible e.g. to deal with lossy links and link
failures. Therefore a communication architecture is proposed, which is capable
of controlling the delivery rate for each data channel, adjusting QoS parameters
for reliability and priority, compressing data on the ﬂy, and changing the under-
lying transport protocol. When using the multi-robot system for a cooperative
mapping task, data links can be conﬁgured for high reliability, because mapping
algorithm are mainly not fault tolerate to message loss. On the other hand when
using the multi-robot system for a search and rescue mission the operator might
be more interested in an live sensor feedback and can therefore adjust the QoS
parameters with the user interface.
2
Related Work
During the last years several communication frameworks for robotics have been
developed. They can be distinguished in connection oriented and connection-less
communication methods. The Player/Stage framework [1] is a connection ori-
ented middleware based on a client server architecture, which has the drawback
that data loss will occur if a connection fails. Connection-less frameworks often
implement a publisher/subscriber mechanism to pass messages between compo-
nents. As described by Eugster et al. [2] a publisher subscriber mechanism can
provide a space, time and synchronization decoupling of the sender and receiver,
which makes the approach more fault tolerant for a multi-robot system. One of
the frameworks with the most valuable impact on the community that implement
a publisher/subscriber concept for message transport is ROS (Robert Operating
System) [3]. ROS allows the usage of TCP or UDP for the message transport
and comes with a comprehensive sensor and visualisation support. One of the
biggest disadvantages of ROS is the centralized approach of the roscore for man-
aging subscriber and publisher connections. ROS was never been designed to run
on a distributed network including non-ideal wireless links. LCM (Lightweight
communications and marshalling) [4] also implements the publisher/subscriber
concept, but in contrast to ROS a decentralized approach is used and LCM
is capable of soft real-time message transmission. The decentralized approach
of LCM is achieved by sending all messages as UDP multicast. However UDP
multicast messages are unsuitable for a mobile ad-hoc network when streaming
large ﬁles e.g. point clouds from mapping tasks. To improve the drawbacks of
ROS currently ROS2 is under development, that will relay on diﬀerent imple-
mentations of the Data Distribution Service (DDS)1, like RTI Connext DDS2 or
1 http://www.omg.org/spec/DDS/1.4/.
2 http://www.rti.com/products/dds/index.html.

784
H. Harms et al.
OpenSplice3. ROS 2 enables the setting of diﬀerent QoS proﬁles for topics e.g.
to achieve a reliable or a best-eﬀort connection, to set delivery deadlines and to
conﬁgure diﬀerent levels for data durability [13].
2.1
Multi-Robot Frameworks
In order to avoid the drawbacks of current middleware frameworks for multi-
robot systems several approaches have been developed. Hartanto et al. [7] are
using a cloud-based publisher/subscriber system to enable a reliable communica-
tion for cooperative mapping task using the Data Distribution Service (DDS) for
inter-robot communication. As described in [7] the inter-process communication
is based on ROS and ROS/DDS Proxy is used for sharing data between robots.
Another concept for a ROS multi-master system was developed by Tiderko et al.
[5] called multimaster fkie. The multimaster fkie package provides methods to
automatically discover independent roscores via UDP multicast and synchronis-
ing their states by exchanging the publisher and subscriber list. Schwarz et al.
[6] developed a set of ROS packages called nimbro network to exchange top-
ics and services via an unreliable Wiﬁlink between two independent roscores.
The topic transport of nimbro network can be subdivided into subscribing the
topic, serializing the messages, sending the data to the receiver, and publishing
the message. Furthermore nimbro network is able to compress messages using
BZip2, switching the underlying protocol (TCP/UDP), and it provides mecha-
nisms for FEC (Forward Error Correction) when using UDP. Another approach
for a multi-robot ROS system called Pound was presented by Tardioli et al. [11].
Pound uses a similar approach to connect multiple ROS cores, but the communi-
cation is realized over a single UDP ﬂow. Messages are inserted in the UDP send
queue depending on the priority of the topic. Therefore Pound is able to reduce
the jitter and meet bandwidth requirements, which makes the solution suitable
for tight control loops. Furthermore Tardioli et al. [14] presented a multi-robot
framework based on the routing protocol RT-WMP [12] to implement network
wide ﬁxed message priorities.
3
Communication Architecture
The presented communication layer is designed to work on top of a middleware
system (in our case ROS) that handles the inter-process communication between
applications in a subnetwork that can reliable communicate, e.g. some clients
connected through Ethernet. Each swarm robot runs an independent roscore on
an communication board. ROS nodes executed on local wired machines hook up
with the roscore on the communication board. For transmitting and receiving
ROS topics of wireless connected mesh neighbors we have extended the publisher
and subscriber mechanism of ROS. Therefore the regular topic transport mech-
anism is splitted up and enlarged with a set of nodes for sending and receiving
3 http://www.prismtech.com/dds-community.

Development of an Adaptable Communication Layer with QoS Capabilities
785
topics via an unreliable WiFi connection, as shown in Fig. 1. In a similar app-
roach Schwarz et al. [6] build up the communication for the NimbRo rescue
robot. In contrast to Schwarz et al. [6] our approach introduces an additional set
of state nodes. The Sender State and Receiver State nodes are used to distribute
the local available topics of a single roscore to all other swarm neighbours, sim-
ilar to the ROS master discovery developed by Tiderko et al. [5]. To exchange
messages between the mesh robots the Sender State node requests the current
state of the local roscore and publishes a message containing a list of all avail-
able topics and services. The Topic Sender node subscribes the sender state
messages for transmitting the message via UDP multicast periodically to all
other network participants. To avoid an endless loop of topics transmissions, the
Sender State node takes also into account the already received state messages
from other swarm clients. By doing this the sender state message only contains
the original topics and services of the local roscore, therefore retransmissions
of external topics are avoided. Based on the status message, the Topic Sender
node is capable of establishing a unique connection through TCP or UDP for
each data channel that is requested. As outlined in Fig. 2 the topic transport
process can be subdivided into some subroutines. First of all, the message is
serialized and if required the message can also be compressed using zstd [8], a
real-time compression algorithm. Following on from that the message is send out
via the Wiﬁlink and received by the Topic Receiver node. Before publishing the
message on the receiver side, the original message has to be recovered through
decompressing and deserialization of the incoming byte stream.
Node
Node
Topic
Publication
Subscription
Sender
State
Receiver
State
Exchange States via UDP Multicast
Node
Node
Topic
Sender
Node
Topic
Reciever
Node
Publication
Subscription
Publication
Subscription
Wiﬁ
/my topic
/my topic
/my topic
/IP/my topic
Fig. 1. ROS topic transport extension

786
H. Harms et al.
Sender State
Receiver State
Topic
Sender
serialization()
compression()
TCP
send()
UDP
send()
Topic
Receiver
deserialization()
decompression()
TCP
receive()
UDP
receive()
conﬁgure()
conﬁgure()
Subscribe
Topics
Publish
Topics
Fig. 2. Software layout for the topic transport
Every connection between the sender and receiver node is handled by an
unique socket and can be conﬁgured with a user deﬁned data rate to control the
overall traﬃc load in the network. Furthermore each connection can be adjusted
with an individual buﬀer size on the sender side, to achieve QoS in terms of reli-
ability. In case of a connection loss, data can be buﬀered for a certain period of
time. If the connection recovers, the buﬀered data is send out with an appropri-
ate rate to make a trade-oﬀbetween the conﬁgured transmission frequency and
the available network capacity. Image topics like aerial or thermal images could
be either transferred using the raw data, or if the compression mode is enabled,
a JPEG image compression is used by subscribing the ROS built-in compressed
image topic. To achieve QoS in terms of priority, every socket on the sender
node can be adjusted within three priority levels, matching the three bands of
the pﬁfo fast [9] queuing disciplines of Linux kernel. Moreover the Topic Sender
node also implements an additional threading model to prefer topics with higher
priority. Therefore messages are divided in chunks of 0.1 MB. Whenever a mes-
sage with higher priority is ready for transmission, topics with lower priority will
pause sending after current chunk has ﬁnished.
3.1
Data Compression
One of the main problems in our use cases is that the utilized sensors can easily
overload the network capacity. To prevent this the Topic Sender and Topic
Receiver nodes contain the already mentioned mechanism for compressing and
decompressing messages via zstd [8]. In contrast to Pound or nimbro network
that are using Bzip2, our approach utilizes zstd (compression level 6) for message
compression. Figure 3 depicts the compression time and ration for three typical

Development of an Adaptable Communication Layer with QoS Capabilities
787
data types exchanged in robotic applications: A status msg (size 3 KB), a smaller
point cloud (size 0.9 MB) and a larger point cloud (size 3.3 MB). In all cases zstd
provides a signiﬁcant better compression time while the compression ratio is on
a similar level compared to Bzip2. Therefore zstd seems more suitable especially
for real-time applications.
Status Msg
Cloud a
Cloud b
0
100
200
300
0.2
25
135
0.9
90
328
Compression Time ms
zstd
bzip2
3 KB
0.9 MB
3.3 MB
(a)
Status Msg
Cloud a
Cloud b
2
2.5
3
1.8
2.7
2.3
1.8
2.6
2.4
Compression Ratio
zstd
bzip2
3 KB
0.9 MB
3.3 MB
(b)
Fig. 3. Comparing compression time and ratio for zstd and Bzip2
3.2
Video Streaming
To save an additional amount of bandwidth, image topics can also be transferred
as a h.264 encoded video stream. For this purpose the already introduced Sender
State nodes transmits a conﬁguration message to a Video Server node that
subscribes the image topic. The Video Server node is able to inject every image
in a Gstreamer pipeline by using a Gstreamer appsrc element. The h.264 video
streams can be transferred with the RTP (Real-Time Transport) protocol from
the UAV/UGV to the ground station.
As shown in Fig. 4 the Video Client node receives the RTP video stream data
and converts the video frames back to ROS image messages.
3.3
User Interface
The message transfer between each swarm participant can either be established
automatically through predeﬁned conﬁguration ﬁles for the Sender and Receiver
State nodes or manipulated via a user interface. The interface is implemented as
a plugin for rviz and lets the operator switch between the UDP, TCP and RTP
protocol for each topic. Apart from this the transmission rate, the buﬀer size,
the priority and the compression mode can be adjusted. For each data channel
the actual bitrate is calculated and displayed to give the user a better under-
standing of the current network load. Figure 5 shows the developed rviz interface
during an experiment with two connected vehicles. Enabled topics for inter-robot

788
H. Harms et al.
Sender State
Receiver State
Video
Server
Gstreamer
Appsrc
RTP
send()
Video
Client
Gstreamer
Appsink
RTP
receive()
conﬁgure()
conﬁgure()
Subscribe
Video
topics
Publish
Video
topics
Fig. 4. Software layout for video streaming
communication are plotted in green. The columns contain parameters in the fol-
lowing order: Transmission Protocol (TCP/UDP), Delivery Rate (Hz), Buﬀer
Size (Number of messages), Priority (Low, Mid, High), Compression (On/Oﬀ).
Fig. 5. User interface for controlling network traﬃc
4
Case Studies
To evaluate the performance of the developed communication layer several case
studies were done. For all experiments two ground vehicles (Fig. 6b) and a sta-
tionary control station for a human operator were used. The hardware for the
communication consists of an embedded ARM board (Gateworks GW 5520) that

Development of an Adaptable Communication Layer with QoS Capabilities
789
contains the WiFi module (Compex WLE900VX), conﬁgured in ad-hoc mode.
For the routing algorithm we considered BATMAN [10] (Better Approach To
Mobile Adhoc Networking) and open802.11s. Both protocols have shown similar
throughput results in our use cases, but open802.11s prevents frequent route
changes due to the air time link metric of the protocol.
A
B
C
D
(a) Travelled path of the ground vehicle. The control station is
located at point A, inside the building.
(b) The unmanned ground ve-
hicle equipped with a stereo
camera, a 3D-LiDAR sensor,
Wifi and GPS antennas.
Fig. 6. Testing area and ground vehicle
4.1
Priority Based QoS
In order to test the priority based QoS two test data streams were transmitted
from a single ground vehicle to the control station, sharing the available band-
width. As described in Fig. 7 both streams have a setpoint rate of 10 Hz. Due to
the bandwidth constrains of the WiFi link the setpoint rate is not achieved. If
both streams have the same priority they are sharing the available bandwidth
resulting in a delivery rate, which does not reach the constraints of 10 Hz. If one
stream is prioritized over the other, the setpoint rate can be reached and the
prioritized stream consumes all of the available bandwidth.
4.2
Reliability Based QoS
To test the reliability based QoS implementation the pattern A - B - A shown
in Fig. 6a was driven by a ground vehicle. During the experiment a data stream
was transmitted to the control station with a setpoint rate of 8 Hz. With increas-
ing distance between the vehicle and the control station, the message delivery
decreases due to the limited available bandwidth of the link. When reaching
point B the communication between the vehicle and the ground station has an
outage. During the communication outage messages are pushed into a waiting
queue, for later delivery. As depicted in Fig. 8 the message delivery rate for the
data streams increases again when reducing the distance towards the control sta-
tion. Using the described reliability option, messages that can not be delivered

790
H. Harms et al.
0
20
40
60
80
100
120
140
160
180
0
5
10
Time s
Delivery Rate Hz
Stream 1, no priority, setpoint rate 10 Hz (8 MB
s
)
Stream 2, no priority, setpoint rate 10 Hz (11 MB
s
)
0
20
40
60
80
100
120
140
160
180
0
5
10
Time s
Delivery Rate Hz
Stream 1, high priority, setpoint rate 10 Hz (8 MB
s
)
Stream 2, low priority, setpoint rate 10 Hz (11 MB
s
)
Fig. 7. Streaming data with diﬀerent priority levels
0
50
100
150
200
250
0
5
10
15
20
Time s
Delivery Rate Hz
1. Stream 2.5 MB/s
QoS - buﬀering
Setpoint rate 8 Hz
2. Stream 2.5 MB/s
no QoS
Setpoint rate 8 Hz
A
B
A
Fig. 8. Streaming data with reliability based QoS during an communication outage
during the communication outage are buﬀered and all outstanding messages are
transmitted when the link recovers. During the transfer of all outstanding mes-
sages the delivery ratio can exceed the current setpoint rate and is only limited
by the network throughput.

Development of an Adaptable Communication Layer with QoS Capabilities
791
0
50
100
150
200
250
300
350
400
0
10
20
Time s
Delivery Rate Hz
TCP point cloud stream, setpoint rate 10 Hz (2.8 MB
s
)
A
B
C
D
C
B
A
0
50
100
150
200
250
300
350
400
0
10
20
Time s
Delivery Rate Hz
UDP video stream, setpoint rate 10 Hz (0.7 MB
s
)
A
B
C
D
C
B
A
Fig. 9. Streaming data during a multi hop case study
4.3
Multi Hop Data Streaming
To evaluate the multi hop capability of the mobile ad-hoc network the pattern
setup shown in Fig. 6a is used again. In contrast to the experiment described
before now a relay station (a second ground vehicle) is placed at point B. The
second vehicle is driving from point A towards the turning point D and back
again. During the ride the ground vehicle is passing the relay station at point
B. Figure 9 shows the message delivery rate of two data streams that are trans-
ferred during the experiment from the ground vehicle towards the control station.
One of the data streams is send via TCP (a 3D-LiDAR point cloud message)
with reliability based QoS enabled and the other one is send via UDP (live
video stream). Figure 9 shows the delivery rate for both data streams. The TCP
point cloud stream is often missing the setpoint rate, especially during multi-hop
communication between point B and D. When the vehicle comes close to the
control station outstanding messages can be delivered. The UDP data stream
mostly meets the conﬁgured setpoint rate, but it has also some drop outs with
increasing link distance and during route changes when passing the relay station.
Further multi hop experiments for a mapping application within our communi-
cation setup are described in Harms et al. [15]. The test have shown that smaller
network traﬃc like control messages and live video (0.3 MB/s) for teleopera-
tion could mostly be delivered in multi hop stages. Instead larger map updates
(1.5 MB–100 MB per map update) were only delivered during single hop stages,
thanks to the implemented mechanisms for priority handling.
5
Conclusions
It is shown that the proposed communication architecture is capable to extend
the underlying middleware architecture to exchange messages in an unreliable

792
H. Harms et al.
network. In addition some methods were presented to reduce network load and
meet bandwidth requirements. The presented user interface is another option to
control data streams and manage diﬀerent situations. In the future we would
like to test if sensor streams can be transferred via multiple hops from aerial
vehicles to the ground station to exploit the full potential of the mobile ad-hoc
network. Further tests for detecting link failures and reconnection time are nec-
essary, especially for TCP connections. It is also thinkable to spent more work on
adjusting transmission rates automatically depending on the available network
bandwidth in order to avoid preconﬁgured rates. In the long term another option
would be to use some of the presented feature like data compression, delivery
rate control and topic prioritization in combination with a ROS 2 interface, to
get access to the comprehensive QoS options of the middleware.
Acknowledgements. All presented work was done within the joint development
project ANKommEn (german acronym for: Automated Navigation and Communica-
tion for Exploration) funded by the German Federal Ministry of Economic Aﬀairs
and Energy administrated by the Space Administration of the DLR (funding code:
50NA1518).
References
1. Gerkey, B., Vaughan, R., Howard, A.: The player/stage project: tools for multi-
robot and distributed sensor systems. In: Proceedings of the 11th International
Conference on Advanced Robotics, pp. 317–323. Citeseer (2003)
2. Eugster, P.T., Felber, P.A., Guerraoui, R., Kermarrec, A.M.: The many faces of
publish/subscribe. ACM Comput. Surv. (CSUR) 35(2), 114–131 (2003)
3. Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T.B., Leibs, J., Wheeler, R.,
Ng, A.Y.: ROS: an open-source robot operating system. In: Proceedings of ICRA
Open-Source Software. Workshop (2009)
4. Huang, A., Olson, E., Moore, D.: LCM: lightweight communications and mar-
shalling. In: IEEE/RSJ International Conference on Intelligent Robots and Sys-
tems, pp. 4057–4062, October 2010
5. Tiderko, A., Hoeller, F., Rhling, T.: The ROS multimaster extension for simpli-
ﬁed deployment of multi-robot systems. In: Robot Operating System (ROS): The
Complete Reference, vol. 1, pp. 629–650. Springer International Publishing (2016)
6. Schwarz, M., Beul, M., Droeschel, D., Schller, S., Periyasamy, A., Lenz, C.,
Schreiber, M., Behnke, S.: Supervised autonomy for exploration and mobile manip-
ulation in rough terrain with a centaur-like robot. Front. Robot. AI 3, 57 (2016)
7. Hartanto, R., Eich, M.: Reliable, cloud-based communication for multi-robot sys-
tems. In: IEEE International Conference on Technologies for Practical Robot
Applications (TePRA). Massachusetts (2014)
8. Zstandard - Fast real-time compression algorithm (2017). https://github.com/
facebook/zstd
9. Linux Classless Queuing Disciplines (qdiscs) (2017). http://linux-ip.net/articles/
Traﬃc-Control-HOWTO/classless-qdiscs.html
10. Johnson, D., Ntlatlapa, N., Aichele, C.: A simple pragmatic approach to mesh
routing using BATMAN. In: 2nd IFIP International Symposium on Wireless Com-
munications and Information Technology in Developing Countries, Pretoria, South
Africa (2008)

Development of an Adaptable Communication Layer with QoS Capabilities
793
11. Tardioli, D., Parasuraman, R., ¨Ogren, P.: Pound: a ROS node for reducing delay
and jitter in wireless multi-robot networks. CoRR, 1707.07540 (2017)
12. Tardioli, D., Sicignano, D., Villarroel, J.L.: A wireless multi-hop protocol for real-
time applications. Comput. Commun. 55, 4–21 (2015)
13. Maruyama, Y., Kato, S., Azumi, T.: Exploring the performance of ROS2. In: Com-
puter Communications, EMSOFT, pp. 5:1–5:10. ACM (2016)
14. Tardioli, D., Sicignano, D., Riazuelo, L., Romeo, A., Villarroel, J., Montano, L.:
Robot teams for intervention in conﬁned and structured environments. J. Field
Robot. 33, 765–801 (2016)
15. Schmiemann, J., Harms, H., Schattenberg, J., Becker, M., Batzdorfer, S., Frerichs,
L.: A distributed online 3D-lidar mapping system. In: ISPRS Archives of the Pho-
togrammetry, Remote Sensing and Spatial Information Sciences (2017)

Trajectory Planning Under Time-Constrained
Communication
Yaroslav Marchukov(B) and Luis Montano(B)
Instituto Universitatio de Investigaci´on en Ingenier´ıa de Arag´on,
University of Zaragoza, Zaragoza, Spain
{yamar,montano}@unizar.es
Abstract. In the present paper we address the problem of trajectory
planning for scenarios in which some robot has to exchange informa-
tion with other moving robots for at least a certain time, determined
by the amount of information. We are particularly focused on scenarios
where a team of robots must be deployed, reaching some locations to
make observations of the environment. The information gathered by all
the robots must be shared with an operation center (OP), thus some
robots are devoted to retransmit to the OP the data of their teammates.
We develop a trajectory planning method called Time-Constrained RRT
(TC-RRT). It computes trajectories to reach the assigned primary goals,
but subjected to the constraint determined by the need of communicat-
ing with another robot acting as moving relay, just during the time it
takes to fulﬁll the data exchange. Against other methods in the literature,
using this method it is not needed a task allocator to assign beforehand
speciﬁc meeting points or areas for communication exchange, because
the planner ﬁnds the best area to do it, simultaneously minimizing the
time to reach the goal. Evaluation and limitations of the technique are
presented for diﬀerent system parameters.
Keywords: Trajectory planning · Time-constrained communication ·
Cooperative scenarios
1
Introduction
Lets consider a scenario where a team of robots must be deployed to inspect
an environment. The mission of the robots is to reach some points of interest
and make observations of the environment (taking pictures, environmental sens-
ing...). The information must be transmitted to a static operation center (OP),
where human operators monitor the mission. The robots and the OP have a
limited signal range, so they cannot be continuously communicated. Each robot
is equipped with a wireless device, thus it creates a dynamic communication
This work was partially supported by Spanish projects DPI2012-32100, DPI2016-
76676-R-AEI/FEDER-UE and T04-DGA-FSE.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_65

Trajectory Planning Under Time-Constrained Communication
795
area, allowing to extend during the motion the communication area determined
by the OP. This fact can be exploited by the system to exchange information
between the robots. The exchange can be made in diﬀerent ways. The simplest
one, and probably the costliest in terms of time and resources, is that robots
periodically visit the OP. For large scenarios this solution wastes a lot of time
only on information exchange. Another option is to devote some robots to relay
tasks, re-transmitting the information of the mates to the OP. However, in a
multi-robot application, minimizing the number of these relay robots, maximizes
the number of robots visiting the allocated primary tasks. Therefore, a better
solution, is to allocate the nearest goals to some robot, which is responsible of
visiting these goals and, at the same time, to communicate directly with the OP.
This way, the robots devoted to reach farthest goals, share the collected data
with the relay robot, which will upload this information to the OP when going to
share its own data. In the same way, the OP updates the mission tasks and this
information is shared with the team, through the relay robot. This procedure is
illustrated in Fig. 1(a)–(c).
R1
R2
R3
(a)
R1
R3
R2
(b)
R1
R2
R3
(c)
Robot
Relay
Goal 1
TRobot
y
x
Goal 2
tCmin
TRelay
(d)
Fig. 1.
In (a), 3 robots must make observations of the environment (red crosses).
The OP allocates the tasks to each robot (the colored areas associated to each in
the Fig. (a). Additionally R2 (black) is assigned to communicate directly with OP.
Each robot computes its trajectory, sharing it with the mates. Figure (b)–(c) represent
the trajectories planned by R1 and R3 to meet R2 and share data when they make
observations of the environment. Figure (d) illustrates a simpliﬁed example for two
robots in the x-y-time space, in which Relay is assigned for communications while
moving towards its assigned goal, and Robot for primary tasks. The latter computes
its trajectory towards its goal, constrained by the time needed for information exchange
(tcmin).
Furthermore, a constant connectivity between robots, requires coordination,
considerably constraining the motion. Nevertheless, based on the amount of col-
lected data and the achievable bit-rate, the robot knows the time required to
exchange the information. So, it can plan the trajectory, synchronizing with the
relay only the required time to fulﬁll the transmission. As a result, the maximum
deviation of the robot from its trajectory to the primary goal, is the time needed
to exchange information with the relay, see Fig. 1(d).
We assume that a task allocation algorithm has assigned the sequence of
goals and the role of the robots, some of them for both, primary and relay tasks.

796
Y. Marchukov and L. Montano
The relay robots compute their trajectories and share it with the rest of the
robots. Thus, we focus this work on developing a Time-Constrained trajectory
planner based on a Rapidly exploring Random Tree algorithm (RRT), called TC-
RRT. It computes a trajectory which leads the robot to a primary goal through
the communication area of the relay robot. The planner attempts to reduce the
time of the entire trajectory, remaining synchronized with the relay the time
needed to complete the transmission.
The advantages of the presented algorithm are:
– the planner does not require a task allocator to deﬁne beforehand meeting
points or areas for communication: itself computes the time-constrained tra-
jectory towards its assigned primary goal.
– it also obtains the best area in which the exchange can be made, so the
trajectory is obtained through that area.
– the time for communication can be adapted to the requirements of the amount
of collected data for each application and the employed communication tech-
nology. The robot is able to ﬁnd out the time required for each speciﬁc trans-
mission, applying it to the best trajectory computation.
The rest of the paper is organized as follows: in Sect. 2, we present the related
works. The problem is described in Sect. 3. In Sect. 4, we deﬁne the dynamic
communication area concept and its parts, used in the planner. We present TC-
RRT to solve the time-constrained trajectory planning problem in Sect. 5. In
Sect. 6, we discuss the results obtained by our method in simulations. Finally, in
Sect. 7, we present several conclusions and the future work.
2
Related Work
The centralized approach for exploration missions is the most secure method to
assure the coordination and the connectivity among the robots of the team. In
works as [1] and [2], the teams are coordinated to accomplish a deployment mis-
sion. Both methods fully coordinate the team and are constrained to continuous
connectivity between members, which in many cases might be a very strict con-
straint. We ﬁnd interesting the works [3–6] in how they solve Time-Constrained
planning. In [3], the authors avoid coordination, exploiting the signal of other
members, but the algorithm is still restricted to a permanent communication.
With our approach, the robot should be free to decide when to transmit the
information and the period of this transmission. The communication time is
relaxed in [4] and [5], but coordinating the robots is still needed speciﬁcally to
this end. In [4], the team periodically establish line-of-sight communication, but
does not take int account the time of the data exchange. In [5], a robot com-
municates with the team only when it makes an observation, but it needs to
be coordinated with the teammates. In [6], a multi-robot synchronization prob-
lem is addressed, where the robot trajectories are preﬁxed loops and the areas
for inter-robot communication are also a priori established between each pair of
trajectories. In our work, on the contrary to those previous works, the robots

Trajectory Planning Under Time-Constrained Communication
797
share information without disrupting the trajectories of the relays, and without
pre-ﬁxed communication areas between trajectories. Furthermore, the aforemen-
tioned techniques require a speciﬁc allocation algorithm for connectivity tasks.
The presented trajectory planner avoids the usage of this speciﬁc task allocator
for connectivity, because how and where to achieve the coordination is computed
by the trajectory planner.
The trajectory computation involves the time in which the path is traversed,
so the planner explores diﬀerent temporary options to reach the goal. The run-
time of methods such as Dijkstra, Fast Marching Method (FMM), or A* scale
with the number of dimensions. Therefore, we consider sampling-based methods,
which does not require the grid discretization. The randomized multiple-query
algorithms such as Probabilistic Roadmaps (PRM) [7], are still computation-
ally heavy for our problem. Therefore, we employ as the base method RRT [8].
Due to its single-query and randomized nature, it ﬁts well to explore several
trajectories, choosing the best one among them.
There are many variations of RRTs in the literature. The RRT-Connect [9]
raises two trees, one from the initial position of the robot, and another from the
goal. This way, the environment is explored faster and it reduces the probability
of getting trapped. The Dynamic-Domain RRTs (DD-RRT) [10] limit the sam-
pling domain, generating random nodes in areas which do not lead towards the
obstacles and minimizing the execution time. The solution cost is improved by
RRT* [11] and Informed RRT* [12]. RRT* improves the parent selection and
includes a reconnection routine of neighboring nodes, reducing the cost of possi-
ble paths, with respect to the basic RRT. The Informed RRT* employs heuristics
to delimit the sampling domain, ﬁnding out solutions faster and in more prob-
lematic scenarios where the randomized algorithms are not usually eﬀective, as
narrow passages. Both mentioned techniques increase the computation time with
respect to the basic RRT.
The presented TC-RRT, described in Sect. 5, takes into account some of the
features of the cited methods. We delimit the sampling space to lead the robot
through the communication area, as occurs with DD-RRT and Informed RRT*.
We use a parent selection routine, similar to RRT*, not to reduce the solution
cost, but to avoid deadlock in a local minima, so increasing the success rate of
the algorithm.
3
Problem Setup
The problem that we solve is the computation of a trajectory to some location,
which is synchronized with the trajectory of a relay robot, in order to exchange
data. So this involves a spatio-temporal planning. Thus, let us deﬁne some loca-
tion x as a duple of [x y]T . As each position is visited at some time t, we can
deﬁne a node n = [x t]T . Throughout the paper x(n) and t(n) will denote posi-
tion and time of some node n. Since the space is three-dimensional, the distance
between a pair of nodes will include the diﬀerence between times, besides the
Euclidean distance. Thus, the distance between any two nodes n1 and n2 is
expressed as:

798
Y. Marchukov and L. Montano
d(n1, n2) =
∥x(n1) −x(n2)∥
|t(n1) −t(n2)|

(1)
We denote τ as a trajectory travelled by a robot, that can be deﬁned as
a sequence of contiguous nodes: τ = [n0, n1, ...nN], where N denotes the end
of the trajectory. The time of a trajectory to reach the goal is expressed with
t(τ). The robot must communicate with a mate, which is assigned for relaying
the information to the operation center. The trajectory of the mate is expressed
as τm, and it generates a dynamic communication area A(τm). The details of
computation of this area are explained in Sect. 4. An example with a scenario
in presence of obstacles is depicted in Fig. 2(a). Knowing the size of the package
to transmit, the robot determines the minimum time to fulﬁll the transmission,
tcmin. Then, the time that the trajectory of the robot is synchronized with τm,
is expressed as tc(τ). In summary, the problem to solve is to ﬁnd the fastest
trajectory which leads the robot to its primary goal, while it is synchronized the
required time to accomplish the information transmission, Fig. 1(d). Formally
expressed as:
τ ∗= argmin
τ
(t(τ))
subject to
tc(τ) ≥tcmin.
(2)
4
Dynamic Communication Area
A basic wireless antenna, located at some position x produces a communication
area, denoted as A(x). This area is composed by all the positions where the
received signal strength is higher than some speciﬁed threshold, which assures
the communication. Thus, it is possible to obtain the distance dth, where the
communication is guaranteed, from the following expression [14]:
PRX = PT X −10γlog10(dth);
dth = 10
PT X −PRX
10γ
(3)
where PT X, PRX are the transmitted/received signal strength and γ is the path-
loss exponent. Moreover, we want to assure always the communication, so we
discard the positions which are obstructed by obstacles, considering only the
line-of-sight (LoS) component. When a relay robot, with an antenna, follows
its trajectory, it extends the static communication area with the movement. So,
this dynamic area A(τm) can be expressed as all the nodes which are within the
communication distance through the trajectory τm, Fig. 2(a):
A(τm) = {n | dist(nmi, n) ∧LoS(nmi, n), nmi ∈τm, i = 0, ..., N}
(4)
where
dist(nmi, n)
is
a
boolean
function
that
computes
d(nmi, n)
≤
[dth Δtrelay]T using Eq. 1, and Δtrelay is the maximum timestep of the relays
trajectory. And LoS(τm, n) is a boolean function that computes if exists line-of-
sight between the trajectory and the node, using the Bresenham algorithm [13].

Trajectory Planning Under Time-Constrained Communication
799
0
20
40
0
20
40
0
100
200
300
x
y
Time (s)
0
50
100
150
200
(a) A(τm)
0
20
40
0
20
40
0
100
200
300
x
y
Time (s)
100
150
200
(b) Areach
0
20
40
0
20
40
0
100
200
300
x
y
Time (s)
0
50
100
150
(c) Aca
0
20
40
0
20
40
0
100
200
300
x
y
Time (s)
100
120
140
160
(d) Afeas
Fig. 2. Diﬀerent communication area parts.
The robot and the relay robot start from diﬀerent initial positions. Thus,
some nodes of A(τm) are impossible to reach and must be discarded. So, we
deﬁne the reachable area by the robot, Fig. 2(b), as:
Areach = {n ∈A(τm)
|
∥x0 −x(n)∥/vmax ≤t(n)}
(5)
where x0 is the initial position of the robot and vmax its maximum attainable
speed.
As explained in the previous sections, the size of the package to transmit and
the speed of the wireless link are known. So only those nodes of the communica-
tion area, which will guarantee the transmission of the data, are considered by
the planner. So we deﬁne the communication assurance area Aca, Fig. 2(c), as:
Aca = {n ∈A(τm)
|
t(n) ≤t(nN) −tcmin}
(6)
where t(nN) is the time of the last position of the trajectory of the mate τm.
Therefore, the feasible area, Fig. 2(d), which assures that the robot is able to
reach the relay robot at time and guarantees a minimal time for the data
exchange, is the intersection of the areas of Eqs. (5)–(6):
Afeas = Areach ∩Aca
(7)
If ∃nfeas ∈Afeas, it means that the mission may be accomplished if the location
of x(nfeas) is reached no later than t(nfeas). However, if Afeas = ∅, there is no
solution. TC-RRT guides the search to the goal through the communication area,
in order to synchronize the robot with the relay.
5
Trajectory Planning with Limited Communication
Time
The trajectory planner uses as base method the basic RRT. The proposed TC-
RRT (Algorithm 1) has the same structure, but with diﬀerent sampling and
parent selection functions. First of all, let us deﬁne each node in the tree as
z = [x p t a tc]T , where x(z) and p(z) are the position and the parent of z,
t(z) is the time to reach z, a(z) is a boolean to indicate if z is within A(τm),
and tc(z) is the communication time accumulated up to z in the tree. All these

800
Y. Marchukov and L. Montano
variables are computed when the node is inserted in the tree, with InsertNode.
The algorithm generates random samples x in the workspace outside the obsta-
cles (AreaSample), l.3 in Algorithm 1. The way to generate samples depends on if
they are within or outside Afeas. This sample is connected to a parent in the tree
by means of AreaNearest procedure (l.4), selecting this way the node znear that
provides the fastest movement. As in the basic RRT, the Steer function cuts the
stretch of the line between the generated sample (xsample) and the selected par-
ent (znear), l.5. If the new branch is not obstructed by some obstacle, the node
is inserted in the tree T , l.6-8. After expanding K nodes, there may be several
branches or trajectories which achieve the goal. All of them fulﬁll the condition
of the communication time, thus we select the fastest trajectory, as deﬁned in
Eq. 2. The algorithm iterates until expaning the number of nodes speciﬁed by
the user. This way, the environment is rapidly explored, although without the
optimal solution, because of the randomized nature of the algorithm.
Two examples of tree computation and their respective trajectories are shown
in Fig. 3. Let us explain in more detail the algorithm.
Algorithm 1. TC-RRT
1: T ←InsertNode(xini, 0)
2: for i=1:K do
3:
xsample ←AreaSample
4:
znear ←AreaNearest(xsample)
5:
xnew ←Steer(xsample, x(znear))
6:
if ObstacleFree(xnew) then
7:
T ←InsertNode(xnew, znear)
8:
end if
9: end for
10: return T
Algorithm 2 . InsertNode(xnew, znear)
1: Δt = ∥xnew−x(znear)∥
v(xnew,x(znear))
2: tnew = t(znear) + Δt
3: anew = ∃n
:
d([[xnew tnew]T , n]) ≤
[dϵ Δtrelay]T , n ∈A(τm)
4: if anew & a(znear) then
5:
tcnew = tc(znear) + Δt
6: else
7:
tcnew = 0
8: end if
9: T ←[xnew znear tnew anew tcnew]T
InsertNode routine (Algorithm 2) receives the position of a new node and the
parent, and computes the time to reach the node from the parents position (l.1),
where v represents the velocity of the robot. Then it computes the total time to
reach it from the root (l.2). It checks the presence in the area by means Eq. 1
in l.3, where dϵ represents a small value of distance. Note that this operation is
quite lightweight, because Δtrelay selects only one horizontal slice of A(τm). If
both, the parent and the node, are within A(τm), the time of l.1 is accumulated
as communication time (l.4-8).
AreaSample (Algorithm 3) works as follows. When some node is introduced
into the communication area on time, i.e. in Afeas (l.1), and does not fulﬁll the
condition of communication time of Eq. (2) (tc(z) < tcmin), the tree is expanded
through A(τm). The algorithm selects the node which accumulates the maximal
communication time, za in l.2, and choose a greater time of the relays trajectory,
using tmin, tmax in l.3, and delimiting the sampling space by the distance dth. The
selection of time limits is made in accordance with the relative speed between

Trajectory Planning Under Time-Constrained Communication
801
Algorithm 3. AreaSample
1: if ∃z : {dist([x(z) t(z)]T , n), n ∈Afeas ∧tc(z) < tcmin} then
2:
za = argmaxz(tc(z))
▷Node which accumulates the maximal communication time
3:
x−
m = x : ∥x−xm(t(za)+tmin)∥≤dth
x+
m = x : ∥x−xm(t(za)+tmax)∥≤dth
4:
xsample ←rand(x−
m, x+
m)
5: else
6:
xsample ←rand(xmin, xmax)
▷xmin, xmax are the limits of the scenario
7: end if
8: return xsample
(a)
0
20
40
0
20
40
0
200
400
600
800
y
x
Time (s)
(b)
(c)
0
20
40
0
50
0
200
400
y
x
Time (s)
(d)
Fig. 3. Tree (trajectory) computation for diﬀerent scenarios. In (a), a robot that has
collected data at [45,5] (red circle), must share information with some static mate at
[5,5], then return to the initial position. In (b), the tree explores the environment,
looking for the static communication area of the mate. When A(τm) is achieved, the
robot remains within A(τm) until ﬁnish data exchange. Finally, T is expanded to the
goal. The computed trajectory is depicted in green. In (c), the mate is moving from [5,5]
to [45,45], thus the area is dynamic. The robot must transmit data, before reaching the
goal at [5,45]. In (d), T intercepts the communication trail area of the mate as fast as
possible, remains in A(τm) until the end of communication, then achieves the primary
goal.
the relay and the robot, setting tmin = Δtrelayvrelay/vrobot and tmax = tmin +
Δtrelay, see Fig. 4(a). In the opposite case, if there exist no nodes which have
entered within the communication area, Afeas, or the communication time has
already been accumulated, so that tc(z) ≥tcmin, the samples are generated
outside the obstacles in all the scenario, l.5-7.
AreaNearest (Algorithm 4) ﬁnds out the best parent node of the tree to con-
nect the generated sample. The procedure is represented in Fig. 4(b). If there
are no nodes of the tree inside A(τm), l.1, it is connected to parents that provide
the fastest movement, l.2. In the case that some node is within A(τm), l.3, and
accomplishes the condition of Eq. 2, l.4, it selects all the nodes accomplishing
that condition, l.5, and chooses the parent of the fastest movement, l.6. When
there are no nodes in A(τm) which accomplish tc(z) ≥tcmin, l.7, the suitable can-
didates to parent are those that have accumulated the maximum communication
time of the entire tree, l.8. To increase the number of suitable candidate parents,
a relaxation time tr is applied in l.9. It is computed as tr = ntsdmax/vmax, where
nts is the number of timesteps and dmax and vmax are the maximum attainable
step and speed of the robot, respectively. Finally, the chosen parent is that one,

802
Y. Marchukov and L. Montano
Algorithm 4. AreaNearest(xsample)
1: if ∄z : a(z) then
2:
znear = argminz
 ∥xsample−x(z)∥
v(xsample,x(z))

3: else
4:
if ∃z : tc(z) ≥tcmin then
5:
zc = {z | tc(z) ≥tcmin}
6:
znear = argminz∈zc
 ∥xsample−x(z)∥
v(xsample,x(z))

7:
else
8:
z∗= argmaxz(tc(z)) : a(z)
▷Maximum communication time of the entire tree
9:
zc = {z ∈z∗| tc(z) ≥tc(z∗) −tr}
10:
znear = argminz∈zc(t(z))
11:
end if
12: end if
13: return znear
which provides the minimal time, l.10. Therefore, the selected parent is one that
reduces the nodes time, maintaining the tree within A(τm), see Fig. 4(c).
t(za)+tmax3
t(za)+tmin3
m
za
t(za)+tmax1
t(za)+tmin1
t(za)+tmax2
t(za)+tmin2
(a) Sampling area
z1 xsample
A( m )
- z1∉A( m )
- t(z2 )≥tcmin
- t(z3 )<tcmin
z2
z3
znear = z2
(b) Parent selection
nts=0
nts=1
nts=2
t(z*)
t(z*)-tr
xsample
A( m )
znear
z*
(c) Relaxation time
Fig. 4. Sampling area and nearest node selection. (a) shows the sampling space within
communication area A(τm). The maximum communication time is achieved with the
node za, green and red slices depict the visible and obstructed parts of A(τm), respec-
tively. The obstructed parts produce deadlock situations, because the tree attempts to
expand a branch which collides with an obstacle. (b) depicts the selection of suitable
parent candidate. In this case, the best one is z2, because it accomplishes the condition
of minimal communication time, of Eq. (2). In (c), xsample is generated in the sampling
space (green slice), but connecting to the best parent z∗(in red), the tree leaves A(τm).
Using a relaxation time tr, it connects to a suitable parent znear (in blue), which keeps
the tree within A(τm).
6
Simulations and Discussion
In this section we discuss the performance of the TC-RRT and present its limita-
tions. We test the method in the scenario of Fig. 3(c), in which the robot has to
capture a moving mate to transmit the collected data. The technique is evaluated

Trajectory Planning Under Time-Constrained Communication
803
for diﬀerent system parameters. These parameters are the number of extended
nodes, the communication time tcmin and the relative velocity between the robot
and the relay. The communication time is selected as the percentage of the time
available to transmit data, that is, the time of Areach. We select: (1000, 2000,
5000, 10000) nodes, (25%, 50%, 75% and 90%) of time of Areach and (−10%,
0%, +10%) for the speed of the robot with respect to the relays. Varying the
relative speed, the slope of A(τm) changes. Consequently, the time of Areach
changes as well, so that, the times for the diﬀerent relative speeds are (182, 163
and 148) seconds, respectively. Increasing the speed of the relay or reducing the
speed of the robot, the slope is reduced, which means, that it is more diﬃcult
to remain synchronized. The performance of the method is tested in terms of
the solution cost, i.e. total time for the trajectory to the goal (t(τ)), success rate
and the execution time. Note that the success is that the algorithm obtains a
trajectory fulﬁlling the communication time condition of Eq. 2. The algorithm is
implemented in MatLab and tested on a machine Intel Core i7 clocked at 3.40
Ghz and 8 Gb of memory. The results are presented in Fig. 5 and an example of
tree expansion for the tested scenario can be found in the video1.
1000
2000
5000
10000
100
120
140
160
180
200
220
240
260
280
300
Nodes
Solution cost (s)
25%
50%
75%
90%
(a)
1000
2000
5000
10000
100
120
140
160
180
200
220
240
260
280
300
Nodes
Solution cost (s)
25%
50%
75%
90%
(b)
1000
2000
5000
10000
100
120
140
160
180
200
220
240
260
280
300
Nodes
Solution cost (s)
25%
50%
75%
90%
(c)
1000
2000
5000
10000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Nodes
Success rate
25%
50%
75%
90%
(d)
1000
2000
5000
10000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Nodes
Success rate
25%
50%
75%
90%
(e)
1000
2000
5000
10000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Nodes
Success rate
25%
50%
75%
90%
(f)
Fig. 5. Results for 100 random trials in the scenario of Fig. 3(c). Each column represents
−10%, 0% and +10% of relative speed, with respect to the speed of the mate. The red
circles in (a)–(c) represent the mean value of the costs of the trajectories.
The cost of the solution does not change signiﬁcantly with the number of
nodes, Fig. 5(a)–(c). This is because the base method is the basic RRT, thus
when some branch reaches Areach, as well as the goal, it is unlikely that some
other drastically diﬀerent branch will reach the same positions. However, the
1 http://robots.unizar.es/data/videos/robot17yamar.mp4.

804
Y. Marchukov and L. Montano
success rate increases when some nodes threshold is exceeded, Fig. 5(d)–(f). This
is clear in the transition from 1000 to 2000 nodes and for 90% of time of Areach.
When the speed of the robot increases with respect to the relay, the success
of the algorithm increases. Logically, the robot reaches the communication area
and the goal faster, thereby reducing the solution cost. High values of tcmin
reduce the time range to achieve the communication area on time. This, together
with the randomized nature of the algorithm makes impossible to guarantee the
communication. Even in the best case, 10% of extra speed, the success rate is
just 60% for 90% of Areach (equivalent to an interval of 8 s).
Expanding over 2000 nodes for this scenario, the solution is stabilized, the
cost and success rate remain practically constant. The medium values of exe-
cution times were (1.3, 2.25, 5.47, 11.76) seconds for (1000, 2000, 5000, 10000)
nodes respectively. Considering that for this scenario the results are good enough
with few nodes (2000), it takes about 2 s to obtain quite steady solution.
As described in Sect. 5, we employ a relaxation time tr to increase the success
rate of the algorithm. We set nts = (0, 1, 2), and the results are depicted in
Table 1. The solution cost does not vary signiﬁcantly, it is slightly lower without
adding tr. Logically, increasing nts, the number of evaluated candidates increases,
but the execution time do not raises signiﬁcantly. At the same time, using the
relaxation factor increases the success rate, because the tree expansion avoids
getting stuck within coverage area. This means that it is worthwhile to use this
parameter. The simulations of Fig. 5, were performed with nts = 2.
Table 1. Results for diﬀerent timesteps, nts = 0, 1, 2.
Execution time
Success rate
Solution cost
nts 1000 2000 5000 10000 1000 2000 5000 10000 1000
2000
5000
10000
0
1.164 2.064 5.16
11.16
0.74
0.88
0.9
0.91
186.1
186.87 186.58 185.81
1
1.27
2.166 5.13
11.27
0.76
0.95
0.95
0.96
187.09 188.36 187.05 187.14
2
1.32
2.21
5.29
11.39
0.75
0.99
0.99
0.99
188.05 188.05 187.38 187.06
The method works under the hypothesis that the global plan for the robots
will be maintained. If the scenario or the strength of the signal change, it might
produce variations in the planned trajectories executed by the teammates. In
this case, if the robot trajectories do no change a lot, only are deviated from
the original trajectory, the technique could still work by extending the commu-
nication area to be explored around the original communication planned area.
In case of this fails, another more costly solution can be that the mate returns
to a previous communicated position in its trajectory, relaunching the planner.
This is obviously a problem that will be formally dealt in future work.

Trajectory Planning Under Time-Constrained Communication
805
7
Conclusions
In the present work, we have developed a trajectory planning algorithm, called
TC-RRT, constrained by a communication time for exchange information. The
planner computes a trajectory that guides the robot to some location, and at the
same time synchronizes with another robot, just the time required to transmit
the data. The presented technique is aimed to be used in scenarios where a
team of robots is exploring an environment and some robots are used as relays.
But unlike other solution in the literature, the relay robots are collecting data
as well. So that, they retransmit the gathered data of their teammates to the
OP when go to share its own data. This way, the coordination and permanent
communication between the entire team is not required. Knowing the trajectory
of the relay, each robot is able to plan the better place and instant to share its
data. Likewise, the trajectory is obtained in accordance with the size of the data
to share, so reduces substantially the total time for communication tasks.
The success rate of TC-RRT does not increase signiﬁcantly with the number
of nodes, exceeding a certain number of them. The reason is the usage of a basic
RRT as the base of the method, it maintains the ﬁrst result found and does not
improve it. Therefore, an obvious future work is the adaptation to TC-RRT*,
which includes an improvement in parent selection AreaNearest, as well as a
rewiring routine to focus the tree construction within the communication area.
The sampling procedure and parent selection guide the trajectory within com-
munication area of the relay. However, we will explore to employ a potential-like
function to consider the displacement of the communication area, and compute
better the possible movements within it.
References
1. Fink, J., Ribeiro, A., Kumar, V.: Robust control of mobility and communications
in autonomous robot teams. IEEE Access 1, 290–309 (2013)
2. Tardioli, D., Sicignano, D., Riazuelo, L., Romeo, A., Villarroel, J.L., Montano, L.:
Robot teams for intervention in conﬁned and structured environments. J. Field
Robot. 33(6), 765–801 (2016)
3. Flushing, E.F., Kudelski, M., Gambardella, L.M., Di Caro, G.A.: Spatial predic-
tion of wireless links and its application to the path control of mobile robots. In:
Proceedings of the 9th IEEE International Symposium on Industrial Embedded
Systems (SIES 2014), pp. 218–227, June 2014. ISSN 2150-3109
4. Hollinger, G.A., Singh, S.: Multirobot coordination with periodic connectivity:
theory and experiments. IEEE Trans. Robot. 28(4), 967–973 (2012)
5. Banﬁ, J., Li, A.Q., Basilico, N., Rekleitis, I., Amigoni, F.: Asynchronous multiro-
bot exploration under recurrent connectivity constraints. In: IEEE International
Conference on Robotics and Automation (ICRA), pp. 5491–5498 (2016)
6. D´ıaz-B´a˜nez, J.M., Caraballo, L.E., Lopez, M.A., Bereg, S., Maza, I., Ollero, A.:
A general framework for synchronizing a team of robots under communication
constraints. IEEE Trans. Robot. PP(99), 1–8 (2017)
7. Amato, N.M., Wu, Y.: A randomized roadmap method for path and manipulation
planning. In: IEEE International Conference on Robotics and Automation, pp.
113–120 (1996)

806
Y. Marchukov and L. Montano
8. LaValle, S.M.: Rapidly-exploring random trees: a new tool for path planning, TR
98-11, Computer Science Department, Iowa State University, October 1998
9. Kuﬀner, J.J., LaValle, S.M.: RRT-connect: an eﬃcient approach to single-query
path planning. In: IEEE International Conference on Robotics and Automation,
vol. 2, pp. 995–1001 (2000)
10. Yershova, A., Jaillet, L., Simeon, T., LaValle, S.: Dynamic-Domain RRTs: eﬃcient
exploration by controlling the sampling domain. In: IEEE International Conference
on Robotics and Automation, pp. 3856–3861, April 2005
11. Karaman, S., Frazzoli, E.: Sampling-based algorithms for optimal motion planning.
Int. J. Robot. Res. 30(7), 846–894 (2011)
12. Gammell, J.D., Srinivasa, S.S., Barfoot, T.D.: Informed RRT*: optimal sampling-
based path planning focused via direct sampling of an admissible ellipsoidal heuris-
tic. In: IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), pp. 2997–3004, September 2014
13. Joy, K.I.: Breshenhams algorithm. In: Visualization and Graphics Research Group,
Department of Computer Science, University of California, Davis (1999). http://
graphics.idav.ucdavis.edu/education/GraphicsNotes/Bresenhams-Algorithm.pdf
14. Goldhirsh, J., Vogel, W.J.: Handbook of propagation eﬀects for vehicular and per-
sonal mobile satellite systems: overview of experimental and modelling results,
December 1998

Balancing Packet Delivery to Improve
End-to-End Multi-hop Aerial Video Streaming
Luis Ramos Pinto1,2(B), Luis Almeida2, and Anthony Rowe1
1 Carnegie Mellon University, Pittsburgh, PA 15217, USA
p@cmu.edu, agr@ece.cmu.edu
2 Faculdade de Engenharia, Instituto de Telecomunica¸c˜oes, Universidade do Porto,
Rua Roberto Frias, 4200-465 Porto, Portugal
lda@fe.up.pt
Abstract. Unmanned Aerial Vehicles (UAVs) are becoming an impor-
tant tool for facilities monitoring, target tracking, surveillance, etc. In
many of these contexts, a UAV needs to reach an area of interest (AOI)
while streaming data to a ground station (GS) where one or more drones
are controlled by an operator. In remote areas, intermediate UAVs can
act as relays and form a line network to extend range. Interactive con-
trol requires a live video stream where both throughput and delay are
important, which limits the number of relays and thus maximum range.
However, throughput also depends on the quality of the links. It is known
that in open space scenarios, where links can be considered similar, max-
imum throughput is achieved with equal spacing of the relays. This is not
the case near large obstacles or high interference areas, in which links can
be rather asymmetric. In this paper, we address this scenario and show
that maximum throughput is achieved with uneven relay spacing but
balanced packet delivery ratios. We propose a new distributed protocol
ﬁt to high throughput streams on line networks that tracks and balances
the quality of neighboring links by moving the relay nodes accordingly.
Real world experiments with multiple AR.Drone 2.0 UAVs show that
variable link length in a scenario with asymmetric links generates gains
in packet delivery up to 40% and throughput up to 300%.
Keywords: IEEE 802.11 · Multi-hop · Packet delivery ratio · Relay
network · TDMA · Throughput · UAV · Wireless networks
1
Introduction
Monitoring large infrastructures, e.g., bridges, buildings and dams, searching
and rescue, or outdoor surveillance typically rely on online video provided to a
base station where an operator controls the video source interactively to tune
the video content. The commoditization of Unmanned Aerial Vehicles (UAV or
drones) made these devices, particularly multirotors, an obvious technological
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_66

808
L.R. Pinto et al.
choice for those scenarios, given their high maneuverability and ad-hoc commu-
nication capability, avoiding the need for a pre-deployed network infrastructure.
Previous work [11] showed the potential of using relay drones to increase com-
munication range. Unfortunately, streaming with best-eﬀort approaches based
on immediate forwarding leads to poor results with high delays and losses.
In this paper, we design and analyze a new protocol that adjusts the positions
of the relays to improve throughput balancing packet delivery ratio (PDR) in
multi-hop online video streaming applications (Fig. 1). We ﬁrst show that a naive
solution using commodity 802.11 hardware on commercial multirotors struggles
to stream data at high speeds when links are created based on distance. In fact,
obstacles, antennas, local interference, etc., can cause signiﬁcant asymmetries
among links, even with similar length, causing diﬀerent error rates and through-
put, thus negatively impacting the network end-to-end features. Therefore, we
propose a protocol to improve the network by balancing the quality of its links,
thus their capacity, moving the relay nodes adequately.
Our protocol relies on distributed online PDR measurement in which each
node assesses both of its links, in-bound and out-bound directions, and deter-
mines where to move to, along the network axis, to balance the respective PDR.
We analytically and experimentally show the advantage of our protocol when
compared with an equal-link-length approach on a three hop network of real live
video streaming drones. Our contributions are:
– a new TDMA overlay protocol named Dynamic Relay Placement (DRP),
– experimental validation in a real scenario streaming video over three hops.
The next section discusses related work with the problem statement shown
in Sect. 3. Section 4 shows the formal grounds for the asymetric relay placement,
while Sect. 5 presents the DRP protocol implementation architecture. Section 6
shows experimental results and Sect. 7 concludes the paper.
AoI
SENSOR
GS
R3
R2
R1
SENSOR
Fig. 1. Multi-source aerial stream to a ground sink using UAV relays to transmit data.
2
Related Work
Numerous UAV test-beds have been developed for commercial, military and
research purposes, some of which explored using UAVs as ﬂying wireless sensor

Balancing Packet Delivery
809
networks, especially image/video sensing. For example, [3] proposes using UAVs
to collect aerial imagery for mapping and localization. More related is the work
in [4] that discusses multiple ways to organize UAVs in a sensor network, whether
the sensors can be temporarily disconnected from the base station, whether sen-
sors are all directly connected to the base station, or if relays are used to increase
communication range. The authors in [1] provide extensive experimental data
for 802.11g UDP throughput, testing diﬀerent PHY bit-rates and distances, and
relative velocities. They show that bit-rate should be set manually to improve
throughput. Conversely, the authors of [8] propose a UAV wireless multi-source
video streaming system in which transmitters adapt their PHY rate according
to the network load and link conditions, thus improving performance.
Multi-hop wireless network improvements are generally done at management
level, improving routing (AODV, DSDV, BATMAN and their variants), con-
trolling the traﬃc in the network, and/or coping with unreliable links using
redundancy. In contrast to our work, none of the above approaches explores
moving relay nodes dynamically to gain an advantage, since some applications
use uncontrollable or ﬁxed nodes or their applications do not allow it to happen.
In turn, online PDR analysis on mobile networks has been addressed by several
works but mostly with low throughput scenarios with static nodes [6,14]. Thus,
in-network buﬀer overﬂow is not necessarily a problem, unlike our case where
the video streaming needs high throughput, requiring adequate traﬃc manage-
ment. On the other hand, the work in [2] addresses the eﬀect of relative speed
in a vehicular network, including PDR during data streaming, but it does not
address multi-hop communication.
Another line of work that bears similarities to our work is that of robotic
networks, despite typically focusing on ground autonomous mobile robots. The
work in [7] investigates how robots motion can be controlled so to maintain
high throughput for streaming data to a base-station using a multi-hop net-
work. Instead of transmitting all the way, the authors propose concentrating
transmissions in areas where/when the channel is good, slowing the robot, while
moving faster in areas of poor channel characteristics. In [5], the authors ana-
lyze mobile robotic networks performance as a function of distance from the base
station and required data-rate/delay requested by the users. They also consider
using relay nodes but, when robots move far from the base, they propose swap-
ping towards a data mule model leveraging delay tolerant networking. However,
breaking connectivity is incompatible with live stream scenarios and the work
does not provide experimental PDR analysis. The works in [12,13] also aim at
establishing a line topology of relays to support a multimedia connection but
they focus on the speciﬁc characteristics of tunnels and pipelines. For exam-
ple, they explore the fact that tunnels, under certain circumstances, behave like
wave-guides, which does not apply to our context.
Hence, to the best of our knowledge, we still miss a thorough packet delivery
and throughput analysis of multi-hop IEEE802.11 UAV networks using relay
location to improve end-to-end network performance for live video streaming.

810
L.R. Pinto et al.
3
Problem Statement
Consider a multi-hop wireless network architecture with n UAV nodes and a sink
(Fig. 2) aiming at online video streaming produced by one source (eventually
more) to a unique sink i.e. a ground station. Each node buﬀers received packets
and retransmits them in dedicated slots as explained later. We assume a buﬀer
size based on the amount of delay that can be tolerated for remote operation.
Fig. 2. Multi-hop line network model. The PDR of link i, between nodes i and i+1, is
Pi with parameters Ri, αi, and its length is di.
The work in [11] considered each link PDR to depend on link-length, orien-
tation and packet size. The operational scenario considered open space, similar
packets and aligned UAVs leading to similar link properties. In such case, the
general function describing the probability (P) of a packet being delivered on
a single link depends on link length (d) as in Eq. 1, where R is the link-length
with a PDR of 50% and α the curve decay.
P(d) = e−(ln 2)( d
R)
α
(1)
In more complex operational scenarios, e.g., with large metallic surfaces or
local electro-magnetic interference, the individual links are expected to diﬀer,
each with its unique PDR function. However, we consider these functions mod-
eled by R and α parameters that remain constant or vary slowly according to our
protocol dynamics. To keep the links (nearly) independent we applied an overlay
TDMA protocol on top of 802.11 CSMA/CA, as in [11], which strongly reduces
mutual-interference among network nodes. Consequently, the protocol minimizes
packet loss and guarantees end-to-end timeliness [10], having a visible impact
on network performance under high multi-hop traﬃc load. Figure 3 shows how
packets are routed through the network. The source (slot 1) transmits during its
own slot as many packets as possible to its neighbor. The latter, in its own slot,
relays these packets to the next neighbor, and so on until packets reach their
ﬁnal destination (sink). The sink has no dedicated slot. Slot IDs are ordered
from source to sink to minimize delay of the video stream. Packets that are lost
along the way are not recovered. Figure 2 depicts the network and link model.
The PDR at each link Pi depends on the link length di and the link parameters
Ri and αi. Assuming links independence, based on the TDMA protocol isolat-
ing transmissions of each UAV, the end-to-end network PDR (Pnet) is given by
the probability of a packet being successfully transmitted in every link, i.e., the
product of all individual link PDR. Given n hops in a line network with length

Balancing Packet Delivery
811
Fig. 3. TDMA overlay protocol with each node transmitting in its slot. Packets are
received and relayed till reaching the base station. Packet loss is distinct at each link.
L, our problem (Eq. 2) is to ﬁnd a relay placement (d) that maximizes end to
end packet delivery (Pnet).
max
d
Pnet =
n

i=1
Pi(di)
s.t.
L =
n

i=1
di
(2)
4
Optimal Relay Placement
The maximum of the product of exponentials is hard to derive algebraically, even
for two curves. However, for low numbers of nodes, say 5 or less as imposed by
the end-to-end bandwidth and delay requirements of interactive video streaming,
numerical solutions can be easily found. Figure 4 provides an insightful example
of the behavior of this function, concerning a network with two links with dif-
ferent PDR functions and L = 100 m. The relay node is placed at xm from the
source and thus d1 = x and d2 = L −x. The network PDR Pnet(x) (black line)
is the product of P1(x) (red line) and P2(L −x) (blue line) as in Eq. 3.
Pnet(x) = P1(x).P2(L−x) =
e
−(ln 2)

x
R1
α1
e
−(ln 2)

L−x
R2
α2
= e
−(ln 2)

x
R1
α1+

L−x
R2
α2
= e−(ln 2).f(x)
(3)
Maximizing Pnet(x) is the same as minimizing the variable part of the expo-
nential argument f(x) (Eq. 4), since the exponential is monotonic.
max
x
Pnet(x) = min
x f(x) = min
x
 x
R1
α1
+
L −x
R2
α2
(4)
This solution is not trivial and it does not correspond to the intersection of
the curves (cf. red dot at 53.7 m in Fig. 4), which would be the case with similar
links, i.e., R1 = R2 and α1 = α2, leading to f(x) having a minimum at x = L/2
(cf. scenarios explored in [11]). In the general case, the derivative of f(x) must
be found or an iterative search performed to ﬁnd the solution of Eq. 4. In this
case, Pnet(x) is maximum at approximately x = 57.8 m (green dot).

812
L.R. Pinto et al.
Fig. 4. Optimal solution (green dot) for the relay position problem in a two-link 100 m-
length network. Best possible PDR is 57.1%.
Figure 5(top) shows the end-to-end PDR of a two-link network as a function
of end-to-end network length L, assuming each link is modeled by the curves
showed in Fig. 4. Clearly, the bold line, corresponding to the optimal relay posi-
tion, dominates the dashed line obtained with the relay placed in the network
midpoint. The particular example of a 100 m network is highlighted in green.
Figure 5(bottom) shows the optimal relay placement (distance from the source)
as a fraction of the network length (bold line). This plot shows that as the net-
work length grows, it is more favorable to place the relay closer to the sink than
the source. Only in the case of a network of ≈70 m, it is desirable to have the
relay right in the middle. This happens using this particular choice of parameters,
where the second link is worse than the ﬁrst (P1(d) > P2(d)).
Fig. 5.
(top) End-to-end PDR on a two-link network as a function of end-to-end
network length L (links modeled as in Fig. 4). (bottom) Optimal position of the relay
as fraction of L (bold curve).

Balancing Packet Delivery
813
5
System Architecture
We implemented our system following a three-tiered architecture that we sum-
marise below. Describing the full implementation is out of this paper’s scope.
5.1
Application Layer (AL) – Video Capture and Collection
The AL is just used by the source and sink devices, the only nodes that can
generate or consume data. The source grabs image frames directly from the
camera device and passes them to the AL that fragments frames into multiple
packets as needed and sends them to the layer below. Each packet also carries
a corresponding header to facilitate re-assembly at the sink. In the sink, the AL
receives packets from the layer below, extracts their header, re-assembles them
into a full image frame and delivers it to the application for display.
5.2
Packet Manager (PM) Layer – Routing
The PM layer is in charge of ﬁnding the next hop of an incoming packet, whether
it comes from the AL or from another node. The PM passes all packets that are
to be forwarded along the network to the layer below that handles transmissions.
Packets aimed at the node itself are passed to the AL above.
5.3
TDMA Layer – Transmission Shaper
The TDMA layer is in charge of shaping the outgoing communications, accessing
the network interface directly. Periodically, i.e., every round period, enqueued
packets are passed to the wireless card to be sent through the air during a given
interval of time called a slot, only. A new packet is only sent if it is within the
slot and when the network card informs the TDMA module that the last packet
has been sent, which may take a variable time depending on whether there was
an access delays or retransmissions (a maximum of two were conﬁgured).
The TDMA layer is also in charge of synchronizing the network nodes. We
follow an adaptive distributed clockless approach similar to that in [9]. Every
node measures the average delay of the last set of received packets and adjusts the
phase of its own slot to initiate transmissions right after the end of the previous
slot. Figure 6 exampliﬁes this process showing packet reception and transmission
instants in a relay node. Each line in the y-axis contains events occurred within a
round period. Yellow squares represent received packets and their x-axis position
the time in the round when they were received. Gray squares represent packets
transmitted (relayed) by the node. The node time slots are shown as long thin
black rectangles. Figure 6 also shows the slot phase adjustment, visible through
the slot shifts to the right, caused by delays in the packets received in the previous
slot due to retransmissions and interference.

814
L.R. Pinto et al.
Fig. 6. Packet reception (yellow) and transmission (black) in a relay, using slots 1 and
2 of a TDMA round. Phase misalignment is due to adaptive and clockless synchroniza-
tion. Blue packets are DRP protocol feedback PDR packets.
5.4
Dynamic Relay Placement (DRP) Protocol
To maximize end-to-end PDR, each relay runs a distributed protocol to track its
best placement. The protocol main idea is that every UAV constantly estimates
the PDR of its incoming link and shares this value, together with its GPS posi-
tion, with the node before in the line topology. This allows each node to acquire
the PDR and length estimates of both its incoming and outgoing links and, after
collecting multiple estimates, perform a function regression and infer the links
R and α parameters. Then it computes its optimal relative position between its
neighbors and adjusts its position accordingly. Diﬀerential GPS units are to be
used to minimize link length errors. As visible in Fig. 4, an error of 5 m, typical
in standard GPS, leads to a deterioration of near 10% from optimal PDR.
To facilitate these computations, node i TDMA layer adds a link i unique
sequence number (sqi) to the packets header. This way, receiving node i + 1
can determine which packets were lost from sender node i. An array of the last
M sequence numbers of received packets (from every node) is kept updated
(currently M = 200). The DRP protocol periodically sweeps this array verifying
diﬀerences between consecutively registered sequence numbers, determining how
many packets from node i to i + 1 were lost. The estimated PDR (Pi) is the
number of missing packets divided by the window size M. The TDMA layer of
node i + 1 then sends a PDR packet upstream, i.e. to node i, containing the
PDR estimate of link i (cf. blue packet in Fig. 6). This way, every node is able to
estimate the quality of its incoming link (source side) directly, and its outgoing
link (sink side) indirectly by means of the PDR packet.
6
Experimental Results
This section conﬁrms the capability of the proposed DRP protocol to assess the
PDR of each link at each relay node, and how the PDR of the links and end-
to-end network vary with the relative position of the relay in the line topology.

Balancing Packet Delivery
815
We also show the end-to-end throughput, i.e., video frame rate, to guarantee
that we do not improve PDR at the expense of throughput.
6.1
Setup
Figure 7 shows the top view of the experimental setup where we have a single
source UAV that streams images through a line network of two other UAVs that
eventually deliver them to the base station. We used AR.Drone 2.0 as the UAV
platform. The experiment was carried out indoors and distances between nodes
taken from a tape measure. We chose a TDMA round period of T = 100 ms, and
dedicated one equal slot to each transmitting UAV. The network hop count is 3
(n = 3), and routing tables are hard-coded accordingly, namely a UAV-source,
two UAV-relays and a computer base station. The distance between source and
second relay (node 3) is ﬁxed to D = 18 m, and the ﬁrst relay (node 2) is the only
moving node (Fig. 7). Thus, for the purpose of analysing network performance,
we will focus on the ﬁrst three nodes and respective links, only, considering relay
node 3 as the eﬀective sink. Queues in the relays are limited, thus the system
will drop oldest packets ﬁrst if new incoming packets ﬁnd a queue full.
Fig. 7. Experiment layout (top view) of the UAVs and computer base station. All
nodes are ﬁxed except the ﬁrst relay (node 2). End-to-end length set to D = 18 m.
All nodes were set to operate on the same 802.11g ad-hoc network channel
at 24 Mbps ﬁxed bit rate. At this speed, we guarantee that we can produce data
fast enough to saturate the channel, thus not being limited by the processing
power of our platform. The transmission power was equally set among the UAVs
(10 dBm) as well as the maximum number of 802.11 MAC retries (2, the mini-
mum the platform allows). Each video streaming experiment lasted for roughly
100 consecutive seconds due to limited local log ﬁle storage. Each image frame,
in raw format, has 57600 bytes and is divided into 50 packets by the AL that
then passes them to the PM layer. Each packet corresponds to one horizontal
line of pixels in an image frame. The source is programmed to capture a new
frame only if the PM queue has room for at least 50 packets.
6.2
Packet Delivery Ratio
Figure 8 shows the eﬀect of relative distance of a relay to its upstream (source
side) and downstream (base station side) nodes on the PDR of the respective

816
L.R. Pinto et al.
links. As the relay moves farther from the source, and closer to its upstream
neighbor, PDR from the ﬁrst link oscillates around 95%, and the one from the
second link improves dramatically. The best end-to-end condition is when this
product is maximum. If one of the two links has low PDR, this translates to a
low end-to-end PDR since packets have to be successfully transported at every
hop to reach the sink. As discussed in Sect. 4, the asymmetric links lead to a
best end-to-end scenario with the relay node away from the middle distance
between its upstream and downstream nodes. In that midpoint the network
PDR is 90% × 79% ≈72%. Figure 9(a) shows a snapshot of the video stream
received at the sink when the relay is placed exactly in the midpoint between
its two neighbors. When the relay moves away from the source, the network
PDR eventually increases to ≈100%, i.e., a 40% improvement. Figure 9(b) shows
a snapshot of the video stream with the relay close to the optimal position,
revealing less losses as expected. Moving the relay farther away again deteriorates
the network performance since the ﬁrst link worsens signiﬁcantly.
Fig. 8. Experimentally measured packet delivery ratio at the ﬁrst (orange) and second
(purple) links, with respective standard deviation, at diﬀerent static relay positions.
Best scenario when relay is closer to sink (x = 13 m) leading to end-to-end PDR ≈100%.
Dashed lines are superimposed regressions of Eq. 1 on experimental data.
6.3
Throughput
The second metric we analyzed is throughput, measuring the data received at
the sink per second, corresponding to the throughput of the second link, since
all data that the relay transmits is originally generated at the source. Figure 10
shows that placing the relay at x = 13 m, ≈0.6 of the source to sink distance,
maximizes throughput. Interestingly, the achieved throughput is near three times
higher than at the mid-point x = 9 m.
Overall, the experimental results validate our initial assumption that end-to-
end network performance in a line topology with asymmetric links is maximized
moving the relay nodes to balance the PDR of the respective links.

Balancing Packet Delivery
817
Fig. 9. Video stream snapshot with relay positioned at: (a) midpoint between source
and sink, showing plenty of packet losses (black lines); and (b) 0.6 of the distance
between source and sink (x = 13 m), leading to balanced links and fewer packet losses.
Relay Position (m)
0
2
4
6
8
10
12
14
16
18
End-to-End Throughput (KB/s)
0
50
100
150
200
250
300
Fig. 10. Measured end-to-end throughput. Placing the relay at midpoint (x = 9 m) is
not optimal, again. At x = 13 m, throughput is three times higher.
7
Conclusion
UAVs are a promising tool to provide live video streaming to a ground station for
multiple purposes. Due to limited radio range, other UAVs can act as relays and
form a line network with multiple UAVs. The relays positioning has a signiﬁcant
impact on the end-to-end network performance, particularly PDR and through-
put. We showed that in the presence of asymmetries in the links, adjusting UAV
positions balancing the links PDR maximizes network performance. Experimen-
tal results, at diﬀerent static positions and with a two-link line network, showed
a PDR increase of 40% and 3-fold increase in throughput when compared with
placing the relay node in the network mid-point. We also described and imple-
mented an online PDR estimation protocol that will be able to guide the relays
dynamically to their best positions. We are currently addressing the solution of
the general case of positioning n relays and proving convergence of the protocol
when operating iteratively. Moreover, determining the optimal number of relays
in asymmetrical conditions is still an open problem.

818
L.R. Pinto et al.
Acknowledgments. This work is a result of the project NanoSTIMA (NORTE-01-
0145-FEDER-000016), supported by Norte Portugal Regional Operational Programme
(NORTE 2020), under the PORTUGAL 2020 Partnership Agreement, through the
European Regional Development Fund (ERDF). The work was also partially supported
by UID/EEA/50008/2013.
References
1. Asadpour, M., Giustiniano, D., Hummel, K.A., Heimlicher, S., Egli, S.: Now or
later?: delaying data transfer in time-critical aerial communication. In: Proceedings
of Ninth ACM Conference on Emerging Networking Experiments and Technologies
(CoNEXT), Santa Barbara, CA, USA, pp. 127–132. ACM Press (2013)
2. Bohm, A., Lidstrom, K., Jonsson, M., Larsson, T.: Evaluating CALM M5-based
vehicle-to-vehicle communication in various road settings through ﬁeld trials. In:
Proceedings of IEEE Local Computer Networks Conference (LCN), Denver, CO,
pp. 613–620. IEEE (2010)
3. Forster, C., Lynen, S., Kneip, L., Scaramuzza, D.: Collaborative monocular slam
with multiple micro aerial vehicles. In: 2013 IEEE/RSJ International Conference
on Intelligent Robots and Systems, pp. 3962–3970 (2013). doi:10.1109/IROS.2013.
6696923
4. Goddemeier, N., Daniel, K., Wietfeld, C.: Role-based connectivity management
with realistic air-to-ground channels for cooperative uavs. IEEE J. Sel. Areas Com-
mun. 30(5), 951–963 (2012). doi:10.1109/JSAC.2012.120610
5. Henkel, D., Brown, T.X.: Delay-tolerant communication using mobile robotic
helper nodes. In: Proceedings of International Symposium Modeling and Opti-
mization in Mobile, Ad Hoc, and Wireless Networks and Workshops (WiOPT),
Berlin, Germany, pp. 657–666. IEEE (2008)
6. Jia, F., Shi, Q., Zhou, G.M., Mo, L.F.: Packet delivery performance in dense wire-
less sensor networks. In: Proceedings of International Conference on Multimedia
Technology (ICMT), Ningbo, China, pp. 1–4. IEEE (2010)
7. Lindhe, M., Johansson, K.: Using robot mobility to exploit multipath fading. IEEE
Wirel. Commun. 16(1), 30–37 (2009)
8. Muzaﬀar, R., Vukadinovic, V., Cavallaro, A.: Rate-adaptive multicast video
streaming from teams of micro aerial vehicles. In: 2016 IEEE International Con-
ference on Robotics and Automation (ICRA), pp. 1194–1201 (2016). doi:10.1109/
ICRA.2016.7487250
9. Oliveira, L., Almeida, L., Lima, P.: Multi-hop routing within TDMA slots for teams
of cooperating robots. In: 2015 IEEE World Conference on Factory Communication
Systems (WFCS), pp. 1–8 (2015). doi:10.1109/WFCS.2015.7160566
10. Pinto, L.R., Almeida, L., Rowe, A.: Video streaming in multi-hop aerial networks:
demo abstract. In: Proceedings of the 16th ACM/IEEE International Conference
on Information Processing in Sensor Networks, IPSN 2017, pp. 283–284. ACM,
New York, NY, USA (2017). doi:10.1145/3055031.3055047
11. Pinto, L.R., Moreira, A., Almeida, L., Rowe, A.: Characterizing multihop aerial
networks of cots multirotors. IEEE Trans. Industrial Inf. 13(2), 898–906 (2017).
doi:10.1109/TII.2017.2668439
12. Rizzo, C., Tardioli, D., Sicignano, D., Riazuelo, L., Villarroel, J.L., Montano, L.:
Signal-based deployment planning for robot teams in tunnel-like fading environ-
ments. Int. J. Rob. Res. 32(12), 1381–1397 (2013)

Balancing Packet Delivery
819
13. Sicignano, D., Tardioli, D., Cabrero, S., Villarroel, J.L.: Real-time wireless multi-
hop protocol in underground voice communication. Ad Hoc Netw. 11(4), 1484–1496
(2013). Special Issue on Wireless Communications and Networking in Challenged
Environments
14. Zhao, J., Govindan, R.: Understanding packet delivery performance in dense wire-
less sensor networks. In: Proceedings of the First International Conference on
Embedded Networked Sensor Systems (SenSys), Los Angeles, CA, USA, p. 1. ACM
Press (2003)

Communication-Aware Robotics (II)

Discrete Robot Localization in Tunnels
Teresa Seco1(B), Carlos Rizzo2, Jes´us Espelos´ın1, and Jos´e Luis Villarroel3
1 Instituto Tecnol´ogico de Arag´on, (ITAINNOVA), Zaragoza, Spain
{tseco,jespelosin}@itainnova.es
2 EURECAT Technology Center, Barcelona, Spain
carlos.rizzo@eurecat.org
3 Arag´on Institute for Engineering Research (I3A),
University of Zaragoza, Zaragoza, Spain
jlvilla@unizar.es
Abstract. Tunnel-like environments represent a challenge in terms of
robot localization due to the special conditions of this type of scenarios.
Traditional robot localization systems based on laser, camera or odom-
etry sensors do not perform well due to the hostile conditions and the
general lack of distinctive features. In this paper we present a discrete
localization system which takes advantage of the periodicity nature of
the RF signal fadings that appears inside tunnels under certain conﬁgu-
rations. Experimental results show the feasibility of the proposed method
in order to periodically correct the robot position during its displacement
along the tunnel.
Keywords: Localization · RF · Tunnel-like environments · RLS · Maha-
lanobis distance
1
Introduction
There are several labors that need to be carried out inside tunnel-like envi-
ronments such as inspections, rescue missions and even construction. The hard
conditions, as well as the implicit risk for people of executing some of these tasks,
make robots the most adequate devices to perform them.
In this context, localization of the robot along the tunnel represents a chal-
lenge. Due to the absence of satellite signal in underground scenarios, outdoor
methods based on GPS sensors can not be considered. Additionally, the dark-
ness and lack of distinguishable features make the most common techniques for
indoor localization — based on cameras or laser sensors — to perform errati-
cally. Other localization methods rely on wheel odometers, but besides suﬀering
cumulative errors, they can be inaccurate due to uneven surfaces and presence
of ﬂuids. Taking into account the aforementioned diﬃculties, other alternatives
must be explored.
From the point of view of the RF (Radio Frequency) signal propagation,
several studies have been performed in tunnel scenarios [1–3]. In the cited works,
two diﬀerent behaviours of the RF signal are presented: on the one hand tunnel-
like environments behave as waveguides, extending the communication range in
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_67

824
T. Seco et al.
comparison to free space but on the other hand, the signal suﬀers from strong
fading phenomena. The authors demonstrate that it is possible to obtain periodic
fadings under certain transmitter-receiver conﬁgurations (i.e. position in the
tunnel cross section and operating frequency).
In this paper we propose the use of the periodic nature of the fadings for
localization purposes, extending the methodology presented in [4]. Our approach
consists of identifying the minimums of the RF signal during the displacement
of the robot along the tunnel, and subsequently match these features with a RF
map generated from the known signal propagation model. The robot position is
periodically adjusted with the position reference provided by the map during the
matching process. In order to accomplish these tasks, the combination of two
strategies has been proposed as a ﬁrst approach: Recursive Least Square (RLS)
algorithm for the features identiﬁcation [5], and the Mahalanobis Distance for
the matching step.
The remaining of the paper is structured as follows. In the next Section,
the fundamental aspects of the electromagnetic propagation in tunnels relevant
for this work are described. Section 3 presents the proposed discrete localization
method, with a detailed description of each step of the algorithm. In Sect. 4, the
preliminary results of the experimental tests and the algorithm implementation
are shown. Finally, the conclusions are summarized in Sect. 5.
2
Fundamentals of Electromagnetic Propagation in
Tunnels
Propagation in tunnels diﬀers from regular indoor and outdoor scenarios. For
operating frequencies high enough with free space wavelength much smaller
than the tunnel cross-section dimensions, tunnels behave as hollow dielectric
waveguides. If an emitting antenna is placed inside a long tunnel, the spherical
radiated wavefronts will be multiply scattered by the surrounding walls. The
superposition of all these scattered waves is itself a wave that propagates in one
dimension —along the tunnel length— with a quasi-standing wave pattern in
the transversal dimensions. This allows extending the communication range, but
aﬀects the signal with the appearance of strong fadings.
There are many diﬀerent possible transversal standing wave patterns for a
given tunnel shape. Each one is called a mode and has its own wavelength, close
to —but diﬀerent from— the free space one, and with its own decay rate. For a
detailed explanation, a good online source can be found in [6].
The electromagnetic ﬁeld radiated from the antenna starts propagating along
the tunnel distributed among many of the possible propagating modes supported
by this waveguide. After a long-enough travel distance, only the lowest order
modes survive (i.e. those with the lowest attenuation rate), giving rise to the
slow fadings in the so-called far sector. These fadings are caused by the pairwise
interaction between the propagating modes, and therefore, the higher the number
of modes, the more complex the fading structure. On the transmitter side, the
position of the antenna allows to maximize or minimize the power coupled to

Discrete Robot Localization in Tunnels
825
a given mode, favoring the interaction between certain modes, and allowing to
produce a speciﬁc fading structure.
In this speciﬁc work, given the tunnel dimensions and transmitter-receiver
setup, the dominant modes are the ﬁrst three modes (the ones that survive in
the far sector). By placing the transmitter antenna close to a tunnel wall, we
maximize the power coupled to the ﬁrst and second modes while minimizing
the excitation of the third one. In the receiving side, this produces a strictly
periodic fading structure. The superposition of the ﬁrst and second propagation
modes (called EHy
11 and EHy
21 respectively) creates a 512 m periodic fading
structure (Fig. 1(a)). In the very center of the tunnel there is no contribution
from the second mode, and the third mode (EHy
31), with lower energy, becomes
observable, creating another fading structure with a diﬀerent period. However,
the received power associated to the fadings maxima is lower compared to the
previous fading structure. The situation is illustrated in Fig. 1(b), which shows
the data collected by having one antenna in each half of the tunnel, and another
located in the center. It can be seen that there is a spatial phase diﬀerence of
180◦between both halves of the tunnel (i.e. a maximum of one fading matches a
minimum of the other) caused by the transversal structure of the second mode.
For a more detailed explanation, see [3].
Fig. 1. Measured Received Power at 2.4 GHz inside the Somport tunnel, from [3].
The transmitter was kept ﬁxed and the receiver was displaced along 4 Km from the
transmitter. In (b), the same experiment was repeated for three diﬀerent receiver’s
cross-section positions: left half, center and right half. The solid lines represent the
modal theory simulations, and the dotted lines the experimental results.
3
Discrete Localization Algorithm Based on RF
Minimum Detection
We consider that the most relevant features of the RF waveform are the valleys,
where a sharp change in the RSSI (Received Signal Strength Indicator) value

826
T. Seco et al.
is clearly distinguishable. Using the propagation model described in Chaps. 2.1
and 3.2 of [7], it is possible to know in advance the position of each valley along
the tunnel. The proposed algorithm is based on the detection of these relevant
features of the measured RF signal during the robot displacement. Thus, it is
possible to correct the estimated position with the known positions correspond-
ing to each fading feature, removing the odometric cumulative error periodically.
The proposed localization approach is represented in Fig. 2. These steps are
described in more detail in the next subsections.
Fig. 2. General stages of the localization algorithm
3.1
Minimums Map Generation
The map generation step consists of determining the location of the theoretical
minimums from the known fadings model, which relates the RSSI to the distance
along the tunnel. Although the map generation is executed in an oﬄine process,
the solution applied to this process is the same as the one selected for the online
process.
In order to accomplish the online minimums detection, a change in the slope
of the signal from negative to positive must be detected. For this purpose, we
propose to approximate the signal with a ﬁrst order function (ˆy = θ0 + θ1u)
using the resultant coeﬃcients to check the change in slope. Therefore, a system
identiﬁcation method is needed to obtain the linear approximation at each time-
stamp. Although Least Squares (LS) method is the most widely applied solution
for linear optimization problems, it requires an important quantity of previously
recorded data samples for the solution to converge, which means that it is not
suitable to solve problems with real time requirements. For this reason, the
Recursive Least Square algorithm has been proposed as the solution. The RLS
method, which is the recursive formulation of the LS algorithm, is capable of
updating the solution online each time new data is available.
The general formulation of the RLS algorithm is based on the following
equations:
e(k) = y(k) −ˆy(k)
(1)
ˆyk = uT (k)θ(k −1)
(2)

Discrete Robot Localization in Tunnels
827
θ(k) = θ(k −1) + γ(k)e(k)
(3)
γ(k) =
1
uT (k)P(k −1)u(k) + λP(k −1)u(k)
(4)
P(k) = (I −γ(k)uT )P(k −1)
λ
(5)
being e the error, computed as the diﬀerence between the measured data y
and the model output ˆy, calculated using the input sample u and the previous
coeﬃcient estimation θ. P is the inverse correlation matrix and it is proportional
to the covariance matrix of the parameter estimates θ. λ is the forgetting factor,
a positive scalar taking the form (0 < λ ≤1). The smaller the λ, the smaller the
contribution of previous samples to the covariance matrix. This makes the ﬁlter
more sensitive to recent samples, which means more ﬂuctuations in the ﬁlter
coeﬃcients with noisy signals.
As a result of applying the RLS algorithm to the theoretical fadings model
and the slope change detection process, an array of n distance values is obtained
(xm1, xm2, .., xmn). These positions are considered as global localization refer-
ences and will be use as a prior known information (Fig. 3(a)).
Fig. 3. Algorithm ﬂow chart: (a) Map generation, (b) Prediction and Estimation steps

828
T. Seco et al.
3.2
Prediction and Estimation Using RLS and Mahalanobis
Distance
The position of the vehicle ˆxt is predicted using the information provided by the
odometry sensors (vt, wt) and the position computed in the previous iteration
xt−1. In our speciﬁc case the vehicle travels in a straight line along the center
of the tunnel, and therefore slightly heading variations are observed during its
displacement.
ˆxt = xt−1 + v ∗dt
(6)
In the estimation step, the existence of a minimum of the actual RSSI signal
(zi) is checked using the previously described RLS algorithm. If a potential min-
imum ˆxpot appears, a data association technique in order to match the potential
minimum with the minimums of the map Xm is required. In this work, the use of
the Mahalanobis distance in combination with the individual compatibility test
(IC) is proposed. These techniques are the basis for some of the most popular
data association algorithm used in SLAM in order to determine the subset of
map features that are compatible with a measurement [8].
The Mahalanobis distance is computed between the position of the potential
minimum ˆxpot and each of the positions of the minimum map (Xm):
DM(ˆxpot, xmj) =

(ˆxpot −xmj)T S−1(ˆxpot −xmj)
∀xmj ∈Xm,
j = 1..n
(7)
It is important to notice that the covariance matrix (S) must take into
account not only the covariances of the pose estimation and the minimum map,
but also the covariance of the sensor which in this case is a ‘minimums’ detector:
S = CovposeEst + Covmap + CovminSensor
(8)
In order to obtain CovminSensor, a calibration process is required. The data
provided by this sensor are the minimum values obtained through the application
of the RLS algorithm to the actual RSSI data (including the false positives). A
ﬁrst approach of the calibration procedure consists of measuring the distance
between two minimums of the actual data (dmin), that would correspond to a
known minimum. Figure 4 shows the results of the calibration procedure.
The covariance is then obtained through the following expression:
CovminSensor =
dmin
2
2
(9)
The estimated position can be considered corresponding to an actual mini-
mum if the Mahalanobis distance satisﬁes the individual compatibility test (IC):
D2(ˆxpot, xmj) < χ2
d,1−α,
∀xmj ∈Xm,
j = 1..n
(10)
where d is the dimension, and 1−α is the desired conﬁdence level (usually more
than 95%). The value for χ is obtained from the Student’s t Distribution table.

Discrete Robot Localization in Tunnels
829
3250
3300
3350
3400
3450
3500
−80
−75
−70
−65
Distance (m)
rssi (dB)
dmin
Fig. 4. Distance calculation between minimums of the actual signal
It is important to remark the noisy nature of RF signal in tunnels, which
results in false positives during the minimum detection process, even though
a digital ﬁlter is used. The proposed technique is capable of reducing or even
removing the false positives during the matching process.
Finally, if the potential minimum passes the IC test, the estimated posi-
tion of the vehicle along its displacement xt is corrected with the position that
comes from the corresponding minimum of the theoretical map xmj. Therefore,
the accumulated odometry error is reset after identifying a minimum. Also, the
prediction and estimation steps (Fig. 3(b)) are carried out iteratively.
4
Results
In order to test the proposed discrete localization method, the algorithm was
implemented in Matlab. The main goal was to validate the solution using real
data, and for this purpose a dataset logged in the Somport tunnel was used and
processed oﬄine.
4.1
Scenario and Experimental Setup
The test scenario selected for carrying out the experiments was the Somport
railway tunnel, which is 7 Km long and connects Spain with France. It is straight
but there is a change in slope at approximately 4 km from the Spanish entrance,
and the cross-section is horseshoe-shaped (approximately 5 m high and 4.65 m
wide).
An all-terrain vehicle was utilized as the mobile platform. It was equipped
with an array of twelve TPLINK wireless receivers. The antennas were spaced
0.125 m apart and the working frequency was 2.412 GHz. All of them were ver-
tically polarized with a 4 dBi gain. The receiver array was placed at a height of
2.17 m on the moving platform. The radio signal was generated with a TPLINK
tl-wn7200nd wireless adapter with Ralink chipset. It was placed 100 m form the
entrance of the tunnel, 0.25 m from the right wall and 2 m above the ground.
With these transmitter-receiver settings, the period expected for the fadings is
around 512 m.

830
T. Seco et al.
In order to obtain the position of the mobile platform along the tunnel,
the vehicle was equipped with two odometers and a SICK laser sensor. The
vehicle localizes itself fusing all the sensor data with a localization algorithm
on a previously built map [9]. It is only feasible to use this approach due to
a very speciﬁc characteristic of this tunnel, which is lateral emergency shelters
every 25 m, acting as landmarks. Without these shelters, the uncertainty along
the tunnels grows indeﬁnitely as there are no relevant features in the map as to
match and reset the errors. The experimental setup can be seen in Fig. 5.
(a) The Somport tunnel
(b) Instrumented platform
Fig. 5. Experimental setup
The mobile platform moved up to 4000 m from the transmitter position along
the center of the tunnel. During the displacement of the robot, the data pro-
vided by the sensors were streaming and logging with a laptop running Robot
Operating System (ROS) [10] over Ubuntu 12-04.
The real data used to validate the proposed algorithm are the RSSI val-
ues provided by the rightmost antenna. This method is intended to solve the
localization problem in the far sector where the periodic fadings are observable.
Therefore, the values corresponding to the near sector have been removed from
the data set in order to avoid the fast-fadings eﬀect (Fig. 6).
4.2
Localization Algorithm Implementation
The RLS algorithm needs to be implemented in order to get the minimums of
the RF signal (fadings) along the tunnel. For this particular case, the variables
involved in RLS method are deﬁned as follows:

Discrete Robot Localization in Tunnels
831
0
1000
2000
3000
4000
5000
−100
−80
−60
−40
−20
Distance (m)
RSSI (dB)
fadings model
real data
Fig. 6. Received signal strength from antenna 12. The vertical dashed line denotes the
limit between the fast fadings and the selected work zone.
– Measured process output: y = RSSIvalues
– Inputs: u = (1 τ)T , where τ is the time between two consecutive RSSI
received values
– Coeﬃcients: θ = (θ0 θ1)T
– Model output: ˆy = uT θ = θ0 + θ1τ, ﬁrst order polynomial
– Initialization: θ(0) = 0 and P(0) = δI. Since the knowledge of θ at t = 0 is
very vague, a very high covariance matrix of the coeﬃcients is to be expected,
and thus a high value to δ must be assigned.
A signal minimum is identiﬁed when a change in the slope is detected, i.e.
when the slope of previous iteration is negative (θ1(k −1) < 0) and the slope of
the current iteration is positive (θ1(k) > 0).
Minimums Map Generation: In this case, the RSSI values of the fadings
model are used as the process output. The forgetting factor λ is selected as 0.5
because of the lack of noise in the theoretical model. For the initialization of P
a value of 1000 is assigned to δ. The position corresponding to each minimum
are stored (xm1, xm2, ..., xmn).
Minimums Detection of the Actual RF Signal Values: The RLS algo-
rithm is applied using the RSSI value provided by the RF receivers as the process
output. Due to the noisy nature of the signal, a major contribution of previous
samples in the parameter estimation is needed. For this reason, a 0.95 value is
assigned to λ. The value selected for δ is 1000 as in the previous case.
Mahalanobis distance and IC test: The covariance of the minimum sensor
obtained during the calibration process is (20/2)2. For the IC test (10) it is
necessary to select a value for χ. In this case, for a 95% conﬁdence level with
d = 1 and α = 0.05, the χ value provided by the Student’s Distribution table is
6.314 m.

832
T. Seco et al.
1500
2000
2500
3000
3500
4000
4500
−80
−75
−70
−65
−60
−55
−50
Distance (m)
rssi (dB)
fadings model
min rls
2366
1854
3901
3389
2877
(a) Theoretical minimum map
0
50
100
150
200
250
300
350
−80
−75
−70
−65
−60
−55
−50
time (s)
RSSI (dB)
real data
filtered data
(b) Real RSSI data vs ﬁltered data
0
50
100
150
200
250
300
350
−80
−75
−70
−65
−60
−55
−50
RSSI (dB)
time (s)
filtered data
pot min rls
min model
(c) RLS application to RF Signal
0
50
100
150
200
250
300
350
−80
−75
−70
−65
−60
−55
−50
RSSI (dB)
time (s)
filtered data
min rls + mah
min model
A
(d) RLS and individual compatibil-
ity test
152
152.5
153
153.5
154
154.5
155
2850
2860
2870
2880
2890
2900
2910
2920
2930
time (s)
distance (m)
ground truth
x est
x odom
A
(e) Detail of the position correction
0
50
100
150
200
250
300
0
5
10
15
20
25
30
35
time (s)
error (m)
odom
RLS + Mah
A
(f) Position error
Fig. 7. Results of the propose algorithm based on RLS and Mahalanobis distance:
(a) minimums map from fadings model used as known information, (b) ﬁltered signal
obtained by a ﬁrst order ﬁlter, (c) results of the application of the RLS method to actual
data, (d) ﬁnal minimums detection result after the IC Test, (e) position correction
during the displacement of the vehicle, (e) position error comparison between odometry
method and the presented approach
4.3
Preliminary Results
The result of the minimums detection of the fadings model is shown in Fig. 7(a).
The vector of reference positions (green circles) is used as a prior information

Discrete Robot Localization in Tunnels
833
in the online process. Due to the noisy nature of the RF signal, a ﬁrst order
butterworth ﬁlter has been applied to the real signal (Fig. 7(b)).
The result of the RLS algorithm application to the actual RF signal is shown
in Fig. 7(c). The potential minimums provided by the algorithm are represented
as blue asteriks. The green circles are the minimums provided by the map. As it
can be observed, only the use of the RLS method is not enough because several
false positives are returned as signal valleys. Figure 7(d) shows the result of the
combination of the RLS algorithm with the individual compatibility test based
on Mahalanobis distance. It is clearly shown how most of the false positives are
discarded using the proposed method.
When a minimum of the RF signal is identiﬁed, the estimated position of
the robot is set to the reference position of the minimums map. A detail of the
position correction corresponding to detected minimum denoted by A is shown
in Fig. 7(e). As expected, the position error of the odometry estimation increases
reaching a value of 30 m whereas the error of the proposed method (around 8 m)
is reduced each time a valley is identiﬁed (Fig. 7(f)).
5
Conclusions
In this paper, a discrete localization system based on RF fadings for tunnel-
like environments has been presented. The proposed method is based on the
periodic nature of the RF fadings in this environment. It takes advantage of the
RF propagation model to generate a minimums map used as position references.
The algorithm has been validate with real data collected during an experimental
test in a real scenario.
The online identiﬁcation of singular characteristics of the RF signal along
the tunnel allows the robot position to be corrected. The empirical preliminary
results show that it is possible to reset the odometry accumulated error period-
ically, being this period the known fadings signal period.
The implemented discrete localization algorithm overcomes the lack of dis-
tinctive features in tunnels needed by other algorithms based on laser or cameras.
Moreover, the only equipment required are the odometers and the RF transmit-
ter and receiver.
Although this approach is a discrete method, it provides enough localization
corrections for some applications where a continuous precise localization is not
critical. A future challenge to be faced is to extend the discrete solution to a
continuous localization system based on the periodic fadings and incorporating
the information provided by the proposed algorithm.
References
1. Rizzo, C., Tardioli, D., Sicignano, D., Riazuelo, L., Villarroel, J.L., Montano, L.:
Signal based deployment planning for robot teams in tunnel-like fading environ-
ments. Int. J. Robot. Res. 32, 1381–1397 (2013)

834
T. Seco et al.
2. Rizzo, C., Lera, F., Villarroel, J.L.: UHF and SHF fading analysis using wavelets in
tunnel environments. In: 2013 IEEE 78th Vehicular Technology Conference (VTC
Fall), pp. 1–6, September 2013
3. Rizzo, C., Lera, F., Villarroel, J.: Transversal fading analysis in straight tunnels
at 2.4 GHz. In: 2013 13th International Conference on ITS Telecommunications
(ITST), pp. 313–318, November 2013
4. Rizzo, C., Lera, F., Villarroel, J.: A methodology for localization in tunnels based
on periodic RF signal fadings. In: 2014 IEEE Military Communications Conference
(MILCOM), pp. 317–324, October 2014
5. Nelles, O.: Nonlinear System Identiﬁcation: From Classical Approaches to Neural
Networks and Fuzzy Models. Engineering Online Library. Springer (2001). https://
books.google.es/books?id=7qHDgwMRqM4C
6. Orfanidis, S.J.: Electromagnetic waves and antennas. Rutgers University (2014).
http://www.ece.rutgers.edu/∼orfanidi/ewa/
7. Rizzo, C.: Propagation, Localization and Navigation in Tunnel-like Environments.
Ph.D. thesis, University of Zaragoza, July 2015. https://zaguan.unizar.es/record/
31897?ln=en
8. Castellanos, J.A., Neira, J., Tard´os, J.: Map building and slam algorithms. In:
Autonomous Mobile Robots, pp. 335–371, May 2006
9. Lazaro, M., Castellanos, J.: Localization of probabilistic robot formations in
SLAM. In: 2010 IEEE International Conference on Robotics and Automation
(ICRA), pp. 3179–3184, May 2010
10. Quigley, M., Conley, K., Gerkey, B.P., Faust, J., Foote, T., Leibs, J., Wheeler, R.,
Ng, A.Y.: ROS: an open-source robot operating system. In: ICRA Workshop on
Open Source Software (2009)

Low-Bandwidth Telerobotics in Fading Scenarios
Samuel Barrios1(B), Natalia Ayuso1, Danilo Tardioli2(B), Luis Riazuelo1,
Alejandro R. Mosteo2, Francisco Lera1, and Jos´e Luis Villarroel1
1 Arag´on Institute for Engineering Research (I3A),
University of Zaragoza, Zaragoza, Spain
{666180,nayuso,riazuelo,lera,jlvilla}@unizar.es
2 Arag´on Institute for Engineering Research (I3A),
Centro Universitario de la Defensa, Zaragoza, Spain
{dantard,amosteo}@unizar.es
Abstract. Sensor networks can monitor wide areas to gather informa-
tion and relay alerts about concerning events. Response robotic missions
in conﬁned scenarios where such a network existed, like tunnels or mines,
could take advantage of it as a backbone. This paper addresses challenges
arising from the combined characteristics of nodes, typically of low power
and bandwidth, and signal propagation in such scenarios, that exhibits
extended range but also blind spots due to waveguide self-interference.
Firstly, a measurement campaign is reported that characterized RSSI
and PDR performance of XBee nodes in the Somport tunnel, enabling
improved placement of nodes. Then, a teleoperation mission is demon-
strated in which a mobile robot that relays rangeﬁnder readings is com-
manded thought a backbone multi-hop network, within the restrictions
of the extremely limited bandwidth of the IEEE 802.15.4 protocol.
1
Introduction
Sensor based networks have registered a constant growth in the last 20 years.
The use of battery-powered nodes with communication capabilities has sim-
pliﬁed the surveying of not easily accessible or dangerous environments. This
includes mines, nuclear plants or tunnels where the access of human beings can
be sometimes harmful as, for example, in case of ﬁre or gas leaks. The deployment
of low-power wireless nodes allows to monitor this type of scenarios without the
need to install long and expensive cables, with a battery life of months or even
years. In this way, the information collected by the nodes is sent to a so-called
sink node that acts as an aggregator and can be connected to the wired network
and, possibly, to the Internet. The data can be checked remotely but sometimes
it can be useful to have the possibility of checking the situation on site especially
if the readings provided by the sensors show some unexpected or worrying value.
This can be made using a mobile robot that can be teleoperated taking
advantage of the pre-existing sensors’ infrastructure that would be in charge of
routing teleoperation-related information from the robot to the operator (possi-
bly through the Internet).
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_68

836
S. Barrios et al.
However, on the one hand, the bandwidth oﬀered by such small devices is
extremely limited especially if the communication is required to be multi-hop
and, on the other hand, the propagation in this kind of conﬁned environments
has substantially diﬀerent characteristics than in open space. Speciﬁcally, as
has been studied in [1], in the ultra high frequency band (300–3000 MHz) the
wireless transmission along tunnel-like environments takes the form of waveguide
propagation. This allows the communication over a range of several kilometers
thanks to the reduced attenuation per unit length but favouring the appearance
of fadings, periodic zones where the signal strength is weaker.
These characteristics pose two diﬀerent challenges: (1) the positioning of the
nodes must be based on a criterium other than the simple distance among peers
and (2) it is necessary to reduce the amount of data needed for teleoperation.
The contribution of this paper is twofold. On the one hand, we provide a
method to position a set of sensor nodes along a long tunnel taking into account
the characteristics of the propagation inside the tunnel itself and verifying the
results with a measuring campaign. On the other hand, we propose a scheme
to teleoperate a robot along the tunnel using the extremely reduced bandwidth
available and demonstrate this achievement with a real-world experiment.
The remainder of this paper is organized as follows. In Sect. 2 the related
works are presented. Section 3 describes the propagation of the communication
signal in tunnels as well as a nodes deployment based on experimental analysis.
Section 4 addresses the teleoperation issue, dealing with bandwidth limitations.
In Sect. 5 we develop a proof of concept experiment in which we set up a robot
teleoperation. Finally, conclusions form Sect. 6.
2
Related Work
Prior research on robots and wireless sensor-network interactions has not con-
sidered the use of preexisting communication infrastructure for the mobile robot
teleoperation in fading scenarios, to the best of our knowledge.
Dunbabin et al. presented an Autonomous Underwater Vehicle (UAV) vision
based navigation, able to collect data from many static Underwater Sensor Nodes
(USN) networked together optically and acoustically [2]. Palmer et al. considered
USN with depth adjustment capabilities that allow IEEE 802.15.4 communica-
tion between surface nodes as well as between a node and the UAV remotely
controlled with an additional 802.15.4 radio [3].
Teleoperation of Unmanned Ground Vehicles (UGVs) in harsh environments
through the wireless LAN has been studied, facing the inherent problems due
to the propagation characteristics in such environments. The performance and
stability of even the simplest teleoperation systems involving vehicles mounted
cameras’ video feeds and hand controlled motion commands can be committed.
Shim and Ro faced the problem of random and unbounded communication time
delay [4]. This work proposed measuring the round trip time (RTT) on real-time
to switch the best control mode. In [5], the radio signal strength (RSS) gradient
at the UGV location has been graphically represented for naturally guiding the

Low-Bandwidth Telerobotics in Fading Scenarios
837
Fig. 1. The somport tunnel.
human operators to drive the mobile robot (UGV) for reliable communications.
In [6] the authors propose a complete system for tunnel intervention where a
team of robots is teleoperated over a multi-hop network. Finally, because of
the intrinsic bandwidth ﬂuctuations, Gomes et. at, proposed an approach based
on ROS (Robot Operating Systems) that increases the operators overall per-
formance in low bandwidth situations by strategies to optimize all displayed
information transmission quality [7].
3
Signal Propagation and Nodes Deployment
Wireless communication systems have become increasingly important as they
represent, most of the time, a faster, more economical, and sometimes the only
option to deploy a network, as in the case involving mobile agents. A huge eﬀort
has been made in terms of mathematical modeling and measuring campaigns
in order to predict radio-wave propagation in diﬀerent types of scenarios, from
indoor, outdoor, urban to even underground, with the ﬁnal goal of ensuring a
good quality communication link and to increase the channel capacity. Among
these scenarios, tunnels have attracted the attention for train applications, vehic-
ular networks, and even service and surveillance missions in both military and
civilian context [8–11]. Wireless propagation in these environments is described
as strongly multipath, and if the wavelenght of the signal is much smaller than
the tunnel cross section they act as an oversized dielectric waveguide, extending
the communication range but aﬀecting the signal with strong fadings [1,12].
The analysis of electromagnetic waves propagation inside a tunnel with arbi-
trary cross section is not analytically feasible. Even for simple geometries, such as
rectangular or circular cross sections, no exact closed form solutions are available.
To obtain approximate solutions, the most common approaches are the Modal
Theory, which takes into consideration the interaction between the propagating
modes [13,14] and the Geometrical Optics Theory (such as the Ray Tracing
approach), which models radio signals as rays [15,16]. For these reasons, we
proceed to analyze experimentally the signal propagation in a speciﬁc environ-
ment: the Somport Railway Tunnel. Based on the propagation already analyzed
in [17,18], the objective is to characterize the communication link, ﬁnding a
distance between nodes which allows ensuring a correct operation of the net-
work. In this way, the experiment focuses on two measurements. First, the RSSI

838
S. Barrios et al.
(a) XBee 802.15.4
(b) XBee-Pro 802.15.4
(c) XBIB-U-DEV
Fig. 2. RF modules (a, b) and USB interface board (c).
(Received Signal Strength Indicator) along the tunnel, obtaining the maximum
communication range for diﬀerent emission powers as well as the location of the
fadings. Second, the RSSI - PDR (Packet Delivery Ratio) relationship, obtain-
ing the minimum RSSI that guarantees the correct reception of the information
exchanged between nodes. Finally, by joining both measurements, a deployment
strategy is proposed.
3.1
The Somport Railway Tunnel
The Somport Railway Tunnel is the location chosen to perform the experiment.
It is an out-of-service tunnel which connects Spain and France through the
Pyrenees.
It is 7.7 km long and has a horseshoe-shaped section, approximately 5 m high
and 4.65 m wide (Fig. 1). The tunnel is straight along its entire length, suﬀering a
change in slope at approximately 4 km from the Spanish entrance. The walls are
limestone with short sections covered with a thin concrete layer. Thanks to its
characteristics, it is an ideal environment to perform experiments and emulate
long tunnels, common in transport or mining applications.
3.2
Measurement Setup
As previously mentioned, the behavior of the tunnel as waveguide depends on
the wavelength being much smaller than the transverse dimension of the tunnel.
In other words, the higher the signal frequency, the lower the attenuation factor
per unit length, disregarding wall roughness eﬀects. An experimental analysis
of the signals propagation in diﬀerent frequencies of the ISM band (Industrial,
Scientiﬁc and Medical) is performed in [17]. As a result, it is determined that the
2.4 GHz and 5.2 GHz signals have a much lower attenuation factor than those of
433 MHz and 868 MHz. For this reason, 2.4 GHz is the selected frequency band,
since it has a high propagation range, a reasonable wall roughness tolerance and
a wide variety of hardware available on the market.
Regarding the communications protocol, since one of the objectives of this
work is the development of a low energy consumption wireless sensor network

Low-Bandwidth Telerobotics in Fading Scenarios
839
(a) Instrumented vehicle
(b) Emitter module
Fig. 3. Receiver modules on mobile platform (a) and emitter module setup (b).
(WSN), IEEE 802.15.4 is the chosen one [19]. This technical standard speciﬁes
the physical and media access control layers for LR-WPANs (Low-Rate Wireless
Personal Area Networks). It is suitable for communication in networks with low
cost devices and low transfer rates, up to 250 kbit/s. In this way, and in exchange
for a lower bandwidth, it allows a much lower power consumption than other
standards such as IEEE 802.11 [20], maximizing the battery life of each node.
According to the speciﬁcations described, the chosen RF (radio frequency)
modules are Digi’s XBee 802.15.4 (Fig. 2a) and XBee-Pro 802.15.4 (Fig. 2b), due
to their ﬂexibility and low cost. These devices communicate over IEEE 802.15.4,
emitting in a power range of -10 to 0 dBm and 10 to 18 dBm, respectively. They
allow a maximum payload of 100 bytes, even tough the maximum size of the
802.15.4 MAC frame is limited to 127 bytes. In addition, they are mounted on
XBIB-U-DEV interface boards (Fig. 2c), allowing the connection to a computer
via USB. Concerning the antennas, Fig. 2 shows 2 diﬀerent types depending on
the module: XBee 802.15.4 uses a whip antenna with a gain of 1.5 dBi. On the
other hand, the XBee-Pro 802.15.4 has a 2.1 dBi dipole antenna.
Regarding the location of the devices, the emitter (Tx) module is placed on
a 180 cm tripod separated 160 cm from the wall (Fig. 3b). This position is chosen
in order to produce better deﬁned and predictable fadings, as [21] reports. In
order to avoid the eﬀect of the change in slope, the emitter is placed close to the
highest point of the tunnel, at approximately 4 km from the Spanish entrance
(Fig. 1).
The selected emission powers are -10, 0 and 18 dBm. The Tx module broad-
casts packets of 60 bytes with a numeric identiﬁer, which are captured by an
array of two XBee-Pro 802.15.4 modules, allowing to characterize the inﬂuence
of transversal fadings. The array modules have a sensitivity of -100 dBm and
are at 200 cm height and separated 120 cm apart. As in [22] and [21], the spac-

840
S. Barrios et al.
Position (m)
3500
4000
4500
5000
5500
RSSI (dBm)
-100
-90
-80
-70
-60
-50
XBee1 (Tx)
XBee2
(a) 0 dBm, Galleries 9-4
Position (m)
3500
4000
4500
5000
5500
6000
6500
7000
RSSI (dBm)
-85
-75
-65
-55
-45
-35
XBee1 (Tx)
XBee2
(b) 18 dBm, Galleries 9-1
Fig. 4. RSSI measured during the experiment. The closest module to the sending side
is indicated by (Tx).
ing between two successive modules is ﬁxed to be greater than one half of the
wavelength, so that the coupling between antennas is minimized.
The receiver array is moved along the tunnel using an oﬀ-road vehicle as
mobile platform (Fig. 3a). In order to synchronize the received packets with the
position in the tunnel, it is equipped with two 0.5 degree resolution encoders and
a Scanning Laser Rangeﬁnder. The vehicle localizes itself along the tunnel using
a localization algorithm on a previously built map [23], allowing to maintain the
same position reference for all the experiments.
The experiment consists of six sweeps along the tunnel. For each emission
power of the selected set (−10, 0 and 18 dBm), a route is made from the emitter,
moving away until the loss of the signal. At this point, the vehicle returns to the
starting point, resulting in two sweeps per power.
3.3
Results of the Experiment
First, the signal propagation is analyzed in the regions near the emitter. The
packets have been received by the modules, located at the ends of the array
(XBee1 and XBee2), diﬀerentiating between the closest to the emitter side (Tx)
and the furthest one, since it is not placed at the center of the tunnel. Figure 4
shows the RSSI of the packets collected during the sweeps 3 and 5. The phe-
nomena of the fadings can be appreciated mainly in the signal received by the
closest module to the emitter (Fig. 4a). In correspondence with [21], the fadings
have a spatial period of about 500 m. Regarding the signal received by the other
module, it does not present fadings as sharp as the ﬁrst one. However, the RSSI
level is enough to ensure a suitable communications link (Fig. 4b).
The next step is the measurement of the link quality. The objective is to ﬁnd
a minimum RSSI which allows ensuring the correct communication between two
adjacent nodes. The chosen quality estimator is Packet Delivery Ratio (PDR),
which is deﬁned as the proportion of number of packets delivered against the

Low-Bandwidth Telerobotics in Fading Scenarios
841
Table 1. Results of the link quality study.
Range (dBm) No. of Packets % PDR = 1
[−105, −100)
227
11.31
[−100, −95)
1805
30.93
[−95, −90)
5454
79.17
[−90, −85)
5717
91.81
[−85, −80)
5215
96.56
[−80, −75)
4839
98.68
[−75, −70)
4149
98.84
[−70, −65)
3838
98.98
[−65, −60)
1880
99.73
[−60, −55]
975
100
number of packets sent. Since it is a global measure and it is necessary to analyze
the PDR during the sweep, a moving average of 20 packets is applied in order
to obtain its evolution during the path. Thus, the packets received during the
whole experiment are grouped in diﬀerent ranges according to their RSSI. For
each range, the percentage of windows with a PDR equal to 1 is found. This gives
a measure of how likely it is that all packets reach their destination for a given
RSSI range. The results are shown in Table 1. It can be observed that for RSSI
values higher than −80 dBm the percentage of windows with a maximum PDR
is greater than 98%. Depending on the application, it will be necessary to prior-
itize a greater reliability of packet delivery or to sacriﬁce it for a greater range of
coverage per node. In this work a 98% probability is considered suﬃcient, which
means that each node receives the packets of the adjacent one with a RSSI higher
than −80 dBm. Table 2 shows the maximum range achieved during the experi-
ments meeting the cited minimum RSSI requirement. It can be veriﬁed that even
for the same power, there are diﬀerent ranges depending on the module and the
path. This is due to the fact that the transverse separation of the receivers to
the wall has not been constant during the experiment. Hence, the transversal
fadings have a strong inﬂuence on the RSSI of the received packets. Therefore,
in order to achieve the indicated maximum ranges, a careful positioning of the
transverse separation with respect to the emitter must be performed.
Assuming the nodes are placed in the optimal transverse position, the number
of devices needed to cover this tunnel (7.7 km) would be 61, 5 and 3, for emission
powers of −10, 0 and 18 dBm, respectively. As detailed in following sections,
the addition of repeaters reduces the network bandwidth in a factor (n −1),
being n the total number of nodes, since there is not spatial reuse. This is not
problematic when monitoring the scenario, because the size of the measured data
is usually small, as well as the frequency of sensing. However, for a high number
of nodes, it implies a diﬃculty in establishing a continuous communication, as
is the teleoperation of a robot.

842
S. Barrios et al.
Table 2. Maximum distance where a RSSI of −80 dBm is obtained. Underlined mea-
surements are obtained in the modules next to the sending side.
Sweep Power (dBm) XBee1 Range (m) XBee2 Range (m)
1
−10
46
127
2
−10
123
102
3
0
1557
1013
4
0
379
385
5
18
3380
3378
6
18
1613
2780
Hence, there are two diﬀerent problems with diﬀerent requirements that must
be addressed separately. First, the teleoperation of a robot needs the highest cov-
erage range using the least number of nodes. On the other hand, the monitoring
of the scenario requires a higher number of nodes emitting with a low power.
For this reason, the use of devices with several diﬀerent emission levels is pro-
posed. In this way, they can work in low power during a monitoring situation,
where there is not high bandwidth requirement. Besides, in an emergency con-
text where the teleoperation of a robot is needed, a minimum number of nodes
can raise their power, diminishing the number of hops and providing a higher
bandwidth to the network.
Besides, it is necessary to consider that the RSSI is not kept above the min-
imum desired value during the whole path. Due to the phenomenon of fadings,
there are areas where the link quality is lower than desired. In this way, forbid-
den regions are established, where the deployment of a node is not advisable.
Figure 5 shows these zones for emission powers of 0 and 18 dBm. In the case of
the greater power, it is observed that the zones coincide with the valleys of the
fadings, being periodic and predictable. On the other hand, the smaller powers
from these regions in higher areas of the fadings, making them comparable in
length with the regions suitable for deployment.
Therefore, a simple deployment strategy consists of placing the node i + 1 in
the coverage range of the node i, avoiding the forbidden regions. Since monitoring
applications usually require a periodic separation distance d between sensors, a
node position may coincide with a forbidden region. In this case, it would be
deployed in the closest previous suitable zone.
4
A Proof of Concept
In this section, we are going to show the results of a proof of concept experiment
in which we set up a robot teleoperation over a backbone network made of XBee
nodes. The goal is to control the movement of a Pioneer P3DX robot using a
joystick while receiving LIDAR reading feedback (Fig. 6).
The system is built on top of the ROS (Robotics Operating System) [24]
middleware and consists in several nodes connected over the cited network using

Low-Bandwidth Telerobotics in Fading Scenarios
843
Distance to emitter (m)
0
500
1000
1500
2000
RSSI (dBm)
-100
-90
-80
-70
-60
-50
(a) 0 dBm, XBee1 (Tx)
Distance to emitter (m)
0
500
1000
1500
2000
2500
3000
RSSI (dBm)
-90
-80
-70
-60
-50
-40
(b) 18 dBm, XBee1 (Tx)
Fig. 5. Forbidden regions for the node deployment in the sweeps 3 and 5, respectively.
Zones where the RSSI is below the minimum required (−80 dBm) are plotted in red
while Tx indicates that the module was the closest to the sending side.
Base
Robot
LIDAR
Rpt
Rpt
Rpt
0
1
n
Laser
Joystick
Joystick
Fig. 6. Teleoperation scheme.
a multi-core scheme. In Fig. 7 it is possible to observe the conceptual design
of the system: the JOY node connected with the joystick publishes the stick
movements messages through the /joy topic; it is read by the TEO BASE node
that serializes the information (see below) and sends it over the network. On the
other side, the messages are received and deserialized by the TEO ROBOT node
and published on the /cmd vel topic that, in turn, is read by the ROSARIA
node, which is in charge of sending the velocity commands to the actual robot. In
the other direction the LIDAR readings are published by the SICK node on the
/scan topic and read by the TEO ROBOT node that, again, serializes the data
before sending it over the network. On the base station side, it is deserialized
and published in the /comp scan node to be shown by the RV IZ node. Notice
that the nodes on the two ends do not share any roscore since each of them
consists in a independent ROS system connected through a non-ROS network.
Thus, they share in a periodic and asynchronous way the information about the
LIDAR and joystick readings.

844
S. Barrios et al.
/joy
/comp_scan
/scan
/cmd_vel
JOY
TEO_BASE
RVIZ
SICK
TEO_ROBOT
ROSARIA
Fig. 7. ROS nodes and topics involved in the system.
4.1
Adapting to the Available Bandwidth
Although the theoretical bandwidth of the XBee modules is 250 kbps, the max-
imum measured experimentally is only 32.5 kbps, due to the bottleneck that
their serial port communication introduce. Additionally, the use of repeaters in
a network reduces the raw bandwidth by a factor of 1/(n −1), being n the
total number of nodes. Considering the case of the Somport tunnel, emitting at
the maximum power would require the use of 3 nodes, which would suppose a
bandwidth of 16 kbps. On the other hand, in the case of 10 dBm, the result-
ing bandwidth would be 8 kbps, since 5 nodes would be required. As described,
the system is conceptually very simple but being the bandwidth so limited, it
has been necessary to reduce the information sent in order to be able to obtain
an acceptable loop rate. For example, the SICK node conﬁgured as in our
experiment (and connected via serial port to the computer) produces 180 range
readings (one per degree) at a frequency of 4 Hz. This corresponds, taking also
into account the additional data published (timestamp, max and min range,
etc.), to approximately 0.8 KB of data, 4 times per second that is, 3.2 KB/s.
Since the maximum payload of the XBee modules is 100 bytes, sending the raw
data would imply splitting the LIDAR message in at least 8 parts and then
merge them at the receiver side to rebuild it. This has two implications: on the
one hand sending 8 diﬀerent messages over a network would take several mil-
liseconds (especially over a multi-hop one) reducing the loop rate. On the other
hand, the loss of a single message would mean being unable to reconstruct the
information. A similar consideration can be made with the joystick commands,
even if in this case the information ﬁts in a single packet. Additionally, the more
the bandwidth used for these tasks, the less bandwidth available for others. To
mitigate this problem, we decided to compress the data as described below.
4.2
Compressing the Data
Having high precision and resolution data is usually indicative of a correct system
performance. However, if the bandwidth is limited it might not be possible or
convenient. Since this is the case, we decided to reduce the information sent to
the minimum necessary to ﬁnd the data themselves useful. To do so, the precision
and the resolution of the data is decreased, maintaining a more than acceptable

Low-Bandwidth Telerobotics in Fading Scenarios
845
Table 3. Packet Delivery Ratio (PDR) and Inter-Arrival Time (IAT) measured during
the experiment. Full indicates the percentage of laser packets recomposed using their
two halves.
LIDAR
Joystick
PDR (%) Full (%) μ IAT (ms) σ IAT (ms) PDR (%) μ IAT (ms) σ IAT (ms)
100
76.6
207
80
91
282
105
level. Speciﬁcally, the ranges representation is reduced from ﬂoat64 (8 bytes) to
a single byte of information. The length of each reading is thus scaled to 256
possible values and the maximum range is reduced to 4 m independently on the
actual range of the LIDAR sensor. All the values above that will be assigned
the out-of-range value of 0xFF. This provides an accuracy of 4 m/254 = 1.5 cm,
only 0.5 cm worse than that of the Hokuyo URG-04LX, just to give an example,
and more that enough for a teleoperation task where the maximum speed of the
robot is limited (0.5 m/s in our experiments).
However, even with this reduction, the whole laser scan does not ﬁt in a single
packet (we need at least 180 byte, and the maximum payload is 100 bytes), so
it is split in 2 halves, using 1 byte as identiﬁer. In order to avoid losing all the
information in case one of the two packets does not reach the destination, we
send in the ﬁrst packet the even laser readings (0◦, 2◦, etc.) and then the uneven.
If a message cannot be completed with the second part or only the second part
is received, the message is published anyway ﬁlling the missing information with
the out-of-range value. In this way it is possible to have a valuable feedback
information also in lossy networks.
5
Real-World Testing
In order to demonstrate the correct functioning of the system, the following
experiment is proposed. Given a room with two doors communicated externally
by a corridor, the goal is to teleoperate the robot, making it exit through one and
return by the other using only the visual LIDAR feedback to perform the task.
Both the LIDAR and joystick data were sent every 200 ms which is fast enough
for an eﬀective teleoperation while this period allows leaving free bandwidth for
other possible ﬂows. The information is sent through a network formed by 4 XBee
modules. Figure 8 shows the path achieved, where the map has been generated
using a SLAM node running in the robot itself while Table 3 shows the results
in terms of inter-arrival time and PDR for each ﬂow. As can be checked, it was
possible to teleoperate the Pioneer P3DX, which is 40 cm wide approximately,
through a door of comparable size.
6
Conclusions
Wireless sensor networks have allowed in the last 20 years the surveying of hostile
or dangerous environments like mines, nuclear plants or tunnels where the access

846
S. Barrios et al.
Fig. 8. ROS nodes and topics involved in the system.
of human beings can be dangerous as in case of ﬁre or gas leaks. However,
the distribution of the nodes in the environment is subject to many factors as
communication range, sensing requirements and so on. Also, they are usually
static which means that in case of unexpected readings there is no way to know
what’s actually happening. In this paper we proposed two diﬀerent contributions.
On the one hand, we present a way of positioning a set of sensor nodes in
fading environments, like tunnels or mines, taking into account their peculiar
propagation pattern and as a function of the transmission power that directly
aﬀects the battery life. On the other hand, we propose the use of the sensor
nodes as an emergency backbone network to provide network communication
to a mobile robot with the end of teleoperating it for on-site intervention in
case of unexpected reading or necessity, for example, with the extremely limited
bandwidth provided by the 802.15.4 protocol commonly used by sensor networks.
The measuring campaign performed allow us to provide parameters that can be
useful in similar settings. Additionally, the real-world teleoperation experiment
demonstrates that such teleoperation is possible allowing a remote operator to
explore the environment with the sole LIDAR feedback.
References
1. Emslie, A., Lagace, R., Strong, P.: Theory of the propagation of UHF radio waves
in coal mine tunnels. IEEE Trans. Antennas Propag. 23, 192–205 (1975)
2. Dunbabin, M., Corke, P., Vasilescu, I., Rus, D.: Data muling over underwater
wireless sensor networks using autonomous underwater vehicle. In: 2006 IEEE
International Conference on Robotics and Automation, pp. 2091–2098, May 2015
3. Palmer, J., Yuen, N., Ore, J.-P., Detweiler, C., Basha, E.: On air-to-water radio
communication between UAVs and water sensor networks. In: 2015 IEEE Interna-
tional Conference on Robotics and Automation, pp. 5311–5317, July 2015
4. Shim, K., Ro, Y.: The mobile Robot Teleoperation to consider the stability over
the time-delay of wireless network. In: 2003 7th Korea-Russia International Sym-
posium, vol. 2, pp. 457–461, July 2003

Low-Bandwidth Telerobotics in Fading Scenarios
847
5. Caccamo, S., Parasuraman, R., B˚aberg, F., ¨Ogren, P.: Extending a UGV Tele-
operation FLC interface with wireless network connectivity information. In: 2015
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 4305–
4312, September 2015
6. Tardioli, D., Sicignano, D., Riazuelo, L., Romeo, A., Villarroel, J.L., Montano, L.:
Robot teams for intervention in conﬁned and structured environments. J. Field
Robot. 33(6), 765–801 (2016)
7. Gomes, J., Marques, F., Louren¸co, A., Mendon¸ca, R., Barata, J.: Gaze-directed
telemetry in high latency wireless communications: the case of robot teleoperation.
In: 42nd Annual Conference of the IEEE Industrial Electronics Society, pp. 704–
709, December 2016
8. Masson, E., Cocheril, Y., Combeau, P., Aveneau, L., Berbineau, M., Vauzelle, R.,
Fayt, E.: Radio wave propagation in curved rectangular tunnels at 5.8 GHz for
metro applications. In: 2011 11th International Conference on ITS Telecommuni-
cations (ITST), pp. 81–85. IEEE (2011)
9. Bernad´o, L., Roma, A., Paier, A., Zemen, T., Czink, N., Karedal, J., Thiel, A.,
Tufvesson, F., Molisch, A.F., Mecklenbrauker, C.F.: In-tunnel vehicular radio chan-
nel characterization. In: 2011 IEEE 73rd Vehicular Technology Conference (VTC
Spring), pp. 1–5. IEEE (2011)
10. Cerasoli, C.: RF propagation in tunnel environments. In: IEEE Military Commu-
nications Conference, MILCOM 2004, vol. 1, pp. 363–369, October 2004
11. Kjeldsen, E., Hopkins, M.: An experimental look at RF propagation in narrow
tunnels. In: IEEE Military Communications Conference, MILCOM 2006, pp. 1–7.
IEEE (2006)
12. Delogne, P.: EM propagation in tunnels. IEEE Trans. Antennas Propag. 39(3),
401–406 (1991)
13. Mahmoud, S.F., Wait, J.R.: Guided electromagnetic waves in a curved rectangular
mine tunnel. Radio Sci. 9(5), 567–572 (1974)
14. Dudley, D.G., Lienard, M., Mahmoud, S.F., Degauque, P.: Wireless propagation
in tunnels. IEEE Antennas Propag. Mag. 49(2), 11–26 (2007)
15. Mahmoud, S.F., Wait, J.R.: Geometrical optical approach for electromagnetic wave
propagation in rectangular mine tunnels. Radio Sci. 9(12), 1147–1158 (1974)
16. Chen, S.-H., Jeng, S.-K.: SBR image approach for radio wave propagation in tun-
nels with and without traﬃc. IEEE Trans. Veh. Technol. 45(3), 570–578 (1996)
17. Rizzo, C., Lera, F., Villarroel, J.L.: UHF and SHF fading analysis using wavelets in
tunnel environments. In: 2013 IEEE 78th Vehicular Technology Conference (VTC
Fall), pp. 1–6, September 2013
18. Rizzo, C., Lera, F., Villarroel, J.L.: Transversal fading analysis in straight tunnels
at 2.4 GHz. In: 2013 13th International Conference on ITS Telecommunications
(ITST), pp. 313–318, November 2013
19. IEEE Computer Society, IEEE Standard for Information Technology - Telecommu-
nications and Information Exchange Between Systems - Local and Metropolitan
Area Networks Speciﬁc Requirements Part 15.4: Wireless Medium Access Control
(MAC) and Physical Layer (PHY) Speciﬁcations for Low-Rate Wireless Personal
Area Networks (LR-WPANs), IEEE Std 802.15.4-2003, pp. 1–670, October 2003
20. IEEE Computer Society, IEEE Standard for Information Technology - Telecommu-
nications and Information Exchange Between Systems - Local and Metropolitan
Area Networks - Speciﬁc Requirements - Part 11: Wireless LAN Medium Access
Control (MAC) and Physical Layer (PHY) Speciﬁcations, IEEE Std 802.11-2007
(Revision of IEEE Std 802.11-1999), pp. 1–1076, June 2007

848
S. Barrios et al.
21. Rizzo, C., Villarroel, J.L., Lera, F.: Propagation, Localization and Navigation in
Tunnel-like Environments. PhD thesis, University of Zaragoza (2015)
22. Molina-Garcia-Pardo, J.-M., Lienard, M., Degauque, P.: Propagation in tunnels:
experimental investigations and channel modeling in a wide frequency band for
MIMO applications. EURASIP J. Wireless Commun. Netw. 2009(1), 560571
(2009)
23. L´azaro, M.T., Castellanos, J.A.: Localization of probabilistic robot formations in
SLAM. In: 2010 IEEE International Conference on Robotics and Automation, pp.
3179–3184, May 2010
24. Quigley, M., Conley, K., Gerkey, B.P., Faust, J., Foote, T., Leibs, J., Wheeler, R.,
Ng, A.Y.: ROS: an open-source robot operating system. In: ICRA Workshop on
Open Source Software (2009)

Wireless Propagation Characterization
of Underground Sewers Towards Autonomous
Inspections with Drones
Carlos Rizzo1(B), Pedro Cavestany1, Fran¸cois Chataigner1, Marcel Soler1,
German Moreno1, Daniel Serrano1, Francisco Lera2, and Jose Luis Villarroel2
1 EURECAT Technology Center, Barcelona, Spain
carlos.rizzo@eurecat.org
2 Robotics, Perception and Real-Time Group (RoPeRT), Universidad de Zaragoza,
Zaragoza, Spain
Abstract. In tunnel-like environments such as sewers, road tunnels and
mines, a robot or team of networked mobile robots can provide monitor-
ing services, surveillance or search and rescue. However, these scenarios
pose multiple challenges from the robotics and from the communica-
tion points of view. Structurally, sewers are GPS-denied and narrow sce-
narios with lack of illumination and presence of sediments, and in the
communication context, the multipath propagation causes strong fad-
ing phenomena. In this work we characterize a sewer scenario from the
communications point of view, based on a measuring campaign carried
out in the sewers of the city of Barcelona, Spain, in the context of an
ECHORD++ project towards future inspection with drones.
Keywords: Tunnel-like environments · Communications · Sewers ·
Wireless propagation · MAVs · Inspection
1
Introduction
The sewer network is one of the essential infrastructures of a city. Its charac-
teristics — a very wide underground grid of pipelines and galleries, frequently
narrow and worn out — along with the presence of big amounts of waste, yield
a hostile working environment.
Sewer inspections require many humans to work in risky and unhealthy con-
ditions. Introducing a robotics solution in this process aims at reducing the
labour risks, improving the precision of sewer inspections and optimizing sewer
cleaning resources of the city, not only in terms of economic expenses but also
in terms of water required for the cleaning process and of machinery needed.
In this context, the ECHORD++ PDTI Urban Robotics: Sewer Inspection
project1, with the ﬁnal goal of developing an automated robotics solution to
1 ECHORD++ http://echord.eu/pdti/pdti-urban-robotics-sewer-inspection/.
c
⃝Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2_69

850
C. Rizzo et al.
address the described problem, has chosen the sewer network of the city of
Barcelona (Spain) as use-case and test site to develop a solution in a three years
time-span. The aspired robotics solution will be able to determine the state of
the sewer in order to identify segments where the sewers functionality has been
compromised, either by sediments or by structural defects.
Automated collection of data in an environment of this nature is a complex
task: in many points of the sewer network the ground is highly irregular and
full of obstacles which, combined with the high levels of wastewater and litter,
impedes greatly the operability of terrestrial vehicles. In this context, the ARSI
team (Aerial Robot for Sewer Inspection)2 has proposed and is developing a
solution based on the use of a Micro Aerial Vehicle (MAV) for inspection tasks.
It avoids the mobility constraints from which a ground robot would suﬀer, such as
paths with steps, steep drops and even objects in the way. Additionally, a ﬂying
platform is able to move faster through the sewer compared to the terrestrial
alternative and needs simpler logistics in deployment and operation. On the other
hand, a MAV solution has to overcome strong constraints of size, weight and
energy, as its ﬂying space is bounded by sections less than 1 m wide. Therefore
its size, and consequently its payload, are limited to minimal dimensions.
The ARSI consortium has addressed the problem of sewer inspection with
the integral design of an aerial platform, multi-rotor type, endowed with sensors
for semi-autonomous navigation and data collection within the network, and
capable of communicating with an on-surface operator (see Fig. 1).
The main features of the design consist of:
– Optimal layout of the payload on the platform, (embedded PC, 2D laser scan,
multiple image sensors, LED lights, gas sensor and antenna) which minimizes
the weight and energy requirements.
– Localization and motion planning methods in GPS-denied environments.
– Careful design of the image sensors and their conﬁguration in order to ensure
a detailed view of the whole section for structure assessment and defect detec-
tion purposes.
– Use of the morphology of the sewer network to enable out-of-sight communi-
cation.
– Integration of the mission planner within the operational software of sewer
maintenance companies.
The MAV needs to communicate to a base station or operator, located in
the manhole of the sewer. However, electromagnetic waves do not propagate in
tunnels as in regular indoor scenarios nor in free space, even if Line-of-Sight
(LoS) is maintained between emitter and receiver [1,2]. In tunnels, the multi-
path propagation aﬀects the radio signal. Free space solutions or regular indoor
propagation models do not work well, and therefore special techniques must be
adopted in order to ensure reliable communications. This aspect must be taken
into account when planning a communications-aware robotic application to avoid
jeopardizing the success of the mission.
2 ARSI http://echord.eu/essential grid/arsi/.

Wireless Propagation Characterization of Underground Sewers
851
IDS ueye camera intel NUC
external compass
Pointgrey Fireﬂy MV camera
Hokuyo 
UST - 20LX
Pointgrey Fireﬂy 
MV camera
Pixhawk autopilot
ENVIRA
air quality
sensor
IDS ueye camera
(a) Sensor placement on MAV.
(b) Equipped platform ﬂying during a ﬁeld test.
Fig. 1. ARSI platform.
In this paper we show the results of a measuring campaign carried out in the
sewers network, with the ﬁnal goal of characterizing propagation to overcome
the communication issues. The remaining of the paper is organized as follows:
in the next Section, the fundamentals of electromagnetic propagation in tunnels
are presented. Then, in Sect. 3, the test scenario and experimental setup are
described. In Sect. 4, the results of the measuring campaign are presented and
analyzed. Finally, Sect. 5 summarizes the main conclusions and future work.

852
C. Rizzo et al.
2
Radio Wave Propagation in Tunnels
Propagation in tunnels diﬀers from regular indoor and outdoor scenarios. For
operating frequencies high enough with free space wavelength much smaller
than the tunnel cross-section dimensions, tunnels behave as hollow dielectric
waveguides. If an emitting antenna is placed inside a long tunnel, the spherical
radiated wavefronts will be multiply scattered by the surrounding walls. The
superposition of all these scattered waves is itself a wave that propagates in one
dimension —along the tunnel length— with a quasi-standing wave pattern in
the transversal dimensions. This allows extending the communication range, but
aﬀects the signal with the appearance of strong fadings.
There are many diﬀerent possible transversal standing wave patterns for a
given tunnel shape. Each one is called a mode and has its own wavelength, close
to —but diﬀerent from— the free space one, and with its own decay rate [3]. For
a detailed explanation, a good online source can be found in [4], and a complete
survey about radio propagation modeling in these scenarios is presented in [5].
The electromagnetic ﬁeld radiated from the antenna starts propagating along
the tunnel distributed among many of the possible propagating modes supported
by this waveguide. After a long-enough travel distance, only the lowest order
modes survive (i.e. those with the lowest attenuation rate), giving rise to the slow
fadings in the so-called far sector [6]. These fadings are caused by the pairwise
interaction between the propagating modes, and therefore, the higher the number
of modes, the more complex the fading structure. On the transmitter side, the
position of the antenna allows to maximize or minimize the power coupled to
a given mode, favoring the interaction between certain modes, and allowing to
produce a speciﬁc fading structure.
Fig. 2. Measured Received Power at 2.4 GHz in the Somport Tunnel, from [2]. The
transmitter was kept ﬁxed and the receiver was displaced along 4 km from the trans-
mitter.

Wireless Propagation Characterization of Underground Sewers
853
In our previous works in road tunnels, given the tunnel dimensions and the
transmitter-receiver setup, the dominant modes were the ﬁrst three modes. By
placing the transmitter antenna close to a tunnel wall, we maximize the power
coupled to the ﬁrst and second modes while minimizing the excitation of the third
one. In the receiving side, this produces a strictly periodic fading structure. The
superposition of the ﬁrst and second propagation modes (called EHy
11 and EHy
21
respectively) created a 512 m periodic fading structure in the Somport tunnel
(Fig. 2). See [2] for a detailed explanation.
3
Experimental Setup
The Environment. The measuring campaign was carried out in the evaluation
area of the project. It is composed of the sewers network, currently in use, of
a residential building block around Passeig Sant Joan street in Barcelona. The
area of interest is composed of three segments: Valencia Street, with a length
of 132 m, Bailen Street, 129 m long, and Mallorca Street (41 m), as shown in
Fig. 3(a). Each of them has a slightly diﬀerent tunnel cross-section shape and
dimensions, as shown in Fig. 3(b). At last, it is composed of two diﬀerent kind
of joints between two adjacent streets: abrupt 90◦turns, and smooth turns, in
which a smooth concrete curvature guides the water (Fig. 4(b)).
PSSG. SANT JOAN
VALENCIA
132m
T174
BAILÉN
129m
T162B
MALLORCA
41m
T133
smooth turns
abrupt
90 deg.
turn
(a) Test Environment.
(b) Diﬀerent cross-section types
(T174 and T162B).
Fig. 3. Scenario description.
The Communication Nodes. To collect the information about the measured
received power and bandwidth, an ad-hoc network was established. The com-
munication nodes (base station - tx and, mobile receiver - rx) are composed of
a Laptop with an external WiFi network card (Ralink rt2770 chipset), with a

854
C. Rizzo et al.
(a) Straight segment.
(b) Smooth turns.
Fig. 4. Sewer scenario structure.
sensitivity of −95 dBm (see Fig. 5). The tests were conducted at 2.4 and 5.2 GHz,
with the transmitter broadcasting frames every 5 ms at a power of 20 dBm. All
antennas used were dipoles, with a 2.15 dBi gain. The transmitter was placed at
diﬀerent points according to the test, and will be speciﬁed in each case. Finally,
the human driven receiver played as the mobile platform and an ad-hoc system
based on a measuring tape provided the position estimation.
Fig. 5. Deployed communication nodes.
4
Simulations and Experimental Results
In [1,2,7] we have shown that, depending on the transmitter-receiver setup (i.e.
frequency, position), predictable periodic fadings can be obtained in tunnel envi-
ronments, which subsequently can be used for network planning and deployment
[7], navigation under connectivity constraints [8], and even with localization pur-
poses [9,10]. These experiments where performed in a 7 km long tunnel with a
cross-section of 4.65 × 5 m, and periodic fadings of 190 to 500 m of period were
obtained at 2.4 GHz under diﬀerent conﬁgurations (see Fig. 2). However, in the
setup presented in this work, the cross section dimension is much smaller, causing
a higher attenuation but shorter fadings according to simulations.

Wireless Propagation Characterization of Underground Sewers
855
4.1
Measured Received Power
Using the Modal Theory approach with the approximate expressions for the elec-
tromagnetic ﬁeld modes and the corresponding propagation constants obtained
by [11] for rectangular hollow dielectric waveguides — which are valid for high
enough frequencies (i.e. with free space wavelength much smaller than the tunnel
cross section dimensions) — and following the procedure in [11] and Chap. 3.5
of [12], by placing the transmitter and receiver close to the wall, 26 and 24 m of
period fadings should be obtained in Valencia and Bailen street, respectively.
Valencia Street. In the ﬁrst test the transmitter was placed at point A, and the
receiver was displaced from point A to point B, along 132 m from the transmitter,
logging the RSSI (Fig. 6). Results are shown in Fig. 7(a), where we can appreciate
spatial fadings with a period of about 25 m, and an attenuation of about 8 dB
in 100 m. The free space path loss (as described in antenna theory textbooks,
a good online reference can be found in [4]), at 2.4 GHz is approximately 82 dB
for a 132 m long path. With these results, we corroborate both the waveguide
behavior, which allows to greatly extend the coverage in comparison to free
space, and the presence of fadings.
A 
B
D
F
PSSG. SANT JOAN
C
E
VALENCIA
132m
BAILÉN
129m
MALLORCA
41m
smooth turn
abrupt
90 deg.
turn
Fig. 6. Experiment paths.
90◦Abrupt Turn. Following the previous test, the receiver was moved from
point B to point C, while the transmitter was kept at point A. In this condition,
we have non LoS between the transmitter and the receiver. As soon as the
receiver loses the LoS with the transmitter, the connectivity is lost (in less than
3 m). No results are shown, due to the fact that if a frame is not received, the
RSSI cannot be determined (i.e. non-existence of RSSI = 0).
Bailen Street + Smooth Turn + Mallorca Street. To analyze the inﬂu-
ence of the smooth turn, the transmitter was moved to point B and the receiver
was displaced from point B to point F, passing through point E (Fig. 6).

856
C. Rizzo et al.
Fig. 7. Measured Received Power at 2.4 GHz along diﬀerent segments of the sewer.
Results are shown in Fig. 7(b). From point B to point D we can appreciate
a similar behavior as in Valencia street: well deﬁned periodic fadings with a
mean period of about 22 m, and an attenuation of about 9 dB/100 m. The dif-
ferences with respect to Valencia street are due to the fact that the cross section
is slightly diﬀerent. Later on, at point E (where the LoS with the transmitter is
lost), it can be seen that the signal suﬀers great attenuation. Nevertheless, the
received signal still remains above the sensitivity of the receiver. This means that
the smooth turn guides the wave softly, making it able to provide communication
coverage with a perpendicular street.
It can be seen, in both cases, the appearance of periodic fadings along the
straight segments. It is more clear along Bailen street Fig. 7(b) due to the transit

Wireless Propagation Characterization of Underground Sewers
857
conditions. Transiting along Valencia street was more diﬃcult because of the
conditions and the receiver didn’t keep a constant cross-section position at all
times. However, the fadings are appreciated.
Table 1 collects the results from both simulations and the measuring cam-
paign. Regarding the attenuation rate, it can be seen that in the real case the
attenuation is higher than in simulations. This is caused by the roughness and
irregularities along the sewer. Nevertheless, the attenuation is much lower com-
pared to free space, and hence the sewer is acting as a waveguide. With respect
to the fadings period, the simulation and the experimental results match quite
well.
Table 1. Propagation Results
Street
Cross-section Simulated
path loss
(dB/100 m)
Measured
path loss
(dB/100 m)
Simulated
period (m)
Measured
period (m)
Valencia T174
6.06
8
26
25
Bailen
T162B
6.36
9
24
22
Based on the tests performed, we can summarize that:
– Similarly to studies of propagation in tunnels, sewers behave as waveguides:
the communication range is extended in comparison to free space, but the
signal suﬀers from strong fading phenomena. Under certain conﬁgurations,
these fadings are strictly periodic.
– In the abrupt 90◦turns, the communication link is broken almost as soon as
the LoS between the transmitter and receiver is lost, making it impossible to
maintain communication along perpendicular streets.
– The smooth turns attenuate the signals, but also guide it, making it possible
to provide coverage along perpendicular streets.
4.2
Wireless Network Full Coverage Test
Considering the previous results, a communication design test was performed
in order to provide full coverage along Valencia, Bailen and Mallorca streets.
First, two communication nodes were placed at points A and F, and a repeater
was deployed at point B to be able to make an abrupt 90◦turn (following the
convention of Fig. 6). Ping and text streaming was successfully achieved between
both extremes.
To make a more detailed study, a bandwidth measuring campaign was per-
formed on a diﬀerent day.

858
C. Rizzo et al.
4.3
Bandwidth Measurement
Following the wireless propagation characterization, an extensive measurement
campaign was performed to determine the bandwidth of the link between the
robot (drone) and the base station (BS) (tx →rx, rx →tx), speciﬁcally in eight
strategic points (A to I in Fig. 8), under LoS and N-LoS conditions, with and
without the use of a network repeater to overcome the problems related to the
turns, described in the previous section. Two frequencies were tested: 2.4 GHz
and 5.2 GHz.
To measure the bandwidth, the JPERF software was used3, which gener-
ates traﬃc to measure the capacity of the link by estimating the time required
to send and receive large amounts of data. In each of the points, a sequential
bi-directional link bandwidth measurement was performed. The drone was left
static for a period of 120 s. In the ﬁrst 60 s, the bandwidth of the BS-drone link
was measured, followed by the drone-BS link. The base station was kept ﬁxed
at point A, and the repeater at point C. The robot was moved from point B
trough I.
Results are showed in Fig. 8. The letter U represents the upload link (BS-
drone), while letter D represents the contrary. 2G and 5G represent the fre-
quency, 2.4 and 5.2 GHz respectively. The units are represented in Mbps. As an
example, U 2G = 85 means BS-robot link, at 2.4 GHz, and the obtained mean
bandwidth was 85 Mbps. Finally, the bold letters represent the path (eg. AH is
the link between points A and H without using the repeater, while ACG is the
link between A and G, passing through the repeater located at C).
After the tests performed, we can summarize that:
– Under LoS conditions, the bandwidth is higher at 5.2 GHz compared to
2.4 GHz.
– As in the previous Section, the signal is not able to make the abrupt 90 turn
(point C + 3 m towards point D in Fig. 8), at 2.4 GHz nor at 5.2 GHz.
– At 2.4 GHz, the smooth curved structure is able to guide the signal to the
perpendicular street (point A to point H and I, or point C to point F and
G). At 5.2 GHz, the signal is not guided by the smooth turn. This can be
explained by the fact that the wavelength of the signal is much smaller and
gets aﬀected by surface irregularities and roughness.
4.4
Video Streaming
At last, a video streaming test was performed. The compression used was JPEG
at 90%, with the camera working at 11.42 Hz, generating a traﬃc of about 13.8
Mbps. The test was performed from point A to point I, at 2.4 GHz, to be able
to guide the signal with the smooth turn. Until point H, the video streaming is
smooth with no interruptions. From point H to point I, some interruptions can
be observed (which matches the bandwidth results, as the video streaming was
using more bandwidth than the one available). This issue can be addressed by
optimizing video quality, rate and compression settings.
3 https://iperf.fr/.

Wireless Propagation Characterization of Underground Sewers
859
Fig. 8. Bandwidth measurement results. The letter U represents the BS-drone link,
while letter D represents the contrary. 2G and 5G represent the frequency, 2.4 and
5.2 GHz respectively. The units are represented in Mbps and the bold letters represent
the path.
5
Conclusions and Future Work
In this work we have presented the results of a communications ﬁeld tests in the
sewers of Barcelona, in order to characterize the received power as well as the
bandwidth of a communication link between a base station and a mobile robot.
Results show that, similar to road tunnels [1], periodic fadings are also
obtained inside the sewers networks with an attenuation that would allow to
cover several hundreds of meters at 2.4 and 5.2 GHz. Also, as the attenuation
is higher compared to our previous road tunnel scenarios, the near sector is
shorter and almost negligible, obtaining clear fadings since the very beginning.
Finally, the bandwidth measurement tests made us realize that in the smooth
turns (which are mostly the same and appear in the whole sewer network), the
curvature radius is high enough to softly guide propagation and overcome the
turn. This does not happens at a higher frequency (5.2 GHz), and it might be
due to fact that the wavelength of the signal is smaller and gets aﬀected by the
structure.
Some topics remain open as future work for the upcoming experiments, such
as exploring the spatial diversity as in [2] to verify the extend of this study
and take advantage of spatial diversity schemes, as well as deeply characterizing
the eﬀect of the smooth turns at diﬀerent operating frequencies, which would
allow to minimize the use of repeaters to make turns. At last, we would like to
introduce the periodicity of the fadings as a source in the localization algorithm,
as a more robust implementation that in [9,10,13], which in this case is more

860
C. Rizzo et al.
feasible due to the period of the fadings inside sewers, which was from 20–30 m
compared to 200–500 m in road tunnels at the same frequency.
Acknowledgments. This work has received funding from the European Union’s Sev-
enth Framework Programme for research, technological development and demonstra-
tion under grant agreement No. 601116. The authors would like to thank Mr. Raul
Hernandez and Mr. Albert Figueras from FCC (Fomento de Construcciones y Con-
tratas) for their logistic support during the ﬁeld tests.
References
1. Rizzo, C., Lera, F., Villarroel, J.L.: UHF and SHF fading analysis using wavelets in
tunnel environments. In: Vehicular Technology Conference (VTC Fall), 2013 IEEE
78th. pp. 1–6 (Sept 2013)
2. Rizzo, C., Lera, F., Villarroel, J.L.: Transversal fading analysis in straight tunnels
at 2.4 GHz. In: ITS Telecommunications (ITST), 2013 13th International Confer-
ence on. pp. 313–318 (Nov 2013)
3. Dudley, D., Lienard, M., Mahmoud, S., Degauque, P.: Wireless propagation in
tunnels. Antennas and Propagation Magazine, IEEE 49(2), 11–26 (april 2007)
4. Orfanidis, S.J.: Electromagnetic waves and antennas. Rutgers University (2014),
http://www.ece.rutgers.edu/∼orfanidi/ewa/
5. Hrovat, A., Kandus, G., Javornik, T.: A survey of radio propagation modeling for
tunnels. Communications Surveys Tutorials, IEEE 16(2), 658–669 (Second 2014)
6. Delogne, P.: EM propagation in tunnels. Antennas and Propagation, IEEE Trans-
actions on 39(3), 401 –406 (mar 1991)
7. Rizzo, C., Sicignano, D., Riazuelo, L., Tardioli, D., Lera, F., Villarroel, J.L.,
Montano, L.: Robot 2015: Second Iberian Robotics Conference: Advances in Robot-
ics, Volume 1, chap. Guaranteeing Communication for Robotic Intervention in
Long Tunnel Scenarios, pp. 691–703. Springer International Publishing, Cham
(2016)
8. Rizzo, C., Tardioli, D., Sicignano, D., Riazuelo, L., Villarroel, J.L., Montano, L.:
Signal-based deployment planning for robot teams in tunnel-like fading environ-
ments. The International Journal of Robotics Research 32(12), 1381–1397 (2013)
9. Rizzo, C., Kumar, V., Lera, F., Villarroel, J.: RF odometry for localization in pipes
based on periodic signal fadings. In: Intelligent Robots and Systems (IROS), 2014
IEEE/RSJ International Conference on (2014)
10. Rizzo, C., Lera, F., Villarroel, J.: A methodology for localization in tunnels based
on periodic RF signal fadings. In: Military Communications Conference (MIL-
COM), 2014 IEEE. pp. 317–324 (Oct 2014)
11. Laakmann, K.D., Steier, W.H.: Waveguides: characteristic modes of hollow rectan-
gular dielectric waveguides. Appl. Opt. 15(5), 1334–1340 (May 1976)
12. Rizzo, C.: Propagation, Localization and Navigation in Tunnel-like Environments.
Ph.D. thesis, University of Zaragoza (7 2015), https://zaguan.unizar.es/record/
31897?ln=en
13. Seco, T., Rizzo, C., Espelosin, J., Villarroel, J.L.: A Robot Localization System
based on RF Fadings using Particle Filters inside Pipes. In: 2016 International
Conference on Autonomous Robot Systems and Competitions (ICARSC). pp. 28–
34 (May 2016)

Author Index
A
Abad-Fraga, Francisco J., 43, 53
Acevedo, José Joaquín, 272
Adán, Antonio, 227
Aguilar, J.M., 771
Alenyà, Guillem, 141
Almeida, Ana Filipa, 117
Almeida, Luis, 807
Álvarez, David, 716
Amaral, Filipe, 15
Amat, Josep, 536
Andújar, Dionisio, 239
Antunes, André, 409
Antunes, João, 422
Aparicio, José Luis, 43
Aparicio-Rodriguez, Jose-Luis, 53
Armingol, José María, 487
Arrais, Rafael, 617
Arrue, Begoña C., 272, 309, 320, 771
Asín-Prieto, Guillermo, 569
Asvadi, Alireza, 475
Ayuso, Natalia, 835
Azevedo, Francisco, 695
Azevedo, José Luís, 15
B
Badesa, Francisco J., 557
Balsa, Jesús, 283
Balsa-Comerón, Jesús, 67
Barbosa, Ramiro, 756
Bárcena, Guillermo, 43
Barrientos, Antonio, 606, 731
Barrios, Samuel, 835
Barták, Roman, 345
Bautista, Moises, 359
Bauzano, Enrique, 548
Bearee, Richard, 595
Bellas, Francisco, 359
Bengoa, Pablo, 704
Bengochea-Guevara, José M., 239
Bernal-Polo, P., 79
Bernardino, Alexandre, 117
Bertelsen, Álvaro, 524
Bertomeu-Motos, Arturo, 557
Blanco, Andrea, 557
Blanco-Medina, Pablo, 102
Brito, Thadeu, 643, 655
Brueggemann, Bernd, 263
C
Cabanes, Itziar, 704
Cañada, Jesús, 3
Cantuña, Karla, 239
Cardeira, Carlos, 409, 422
Carlos, Socarras Bertiz, 190
Carmona, Joana, 695
Casals, Alícia, 536
Castro, Ayoze, 263
Catalán, José M., 557
Cavestany, Pedro, 849
Cerrada, Carlos, 251
Cerrada, José A., 251
Chataigner, François, 849
Chavera, Pedro, 43, 53
Colomé, Adrià, 141
Cortés, Camilo, 524
Costa, Carlos M., 153
Costa, Paulo, 655
Costa, Pedro, 643, 668, 680
Cristiano, Julián, 91
Culla, David, 3
Cunha, Bernardo, 15
D
Darouach, Mohamed, 446
de la Escalera, Arturo, 499
de la Rubia, David, 227
de León, Jorge, 731
Dias, Ricardo, 15
Diaz-Cano, Ignacio, 53
© Springer International Publishing AG 2018
A. Ollero et al. (eds.), ROBOT 2017: Third Iberian Robotics Conference, Advances in Intelligent
Systems and Computing 694, https://doi.org/10.1007/978-3-319-70836-2
861

Díez, Jorge A., 557
Dominguez-Morales, Juan Pedro, 580
Duque-Domingo, Jaime, 251
Duro, Richard J., 359
E
Escudero, Álvaro, 524
Espelosín, Jesús, 823
F
Fdez, Javier, 606
Fentanes, J.P., 295
Fernandez-Enriquez, Antonio, 580
Fernández-Jiménez, Francisco J., 200
Fernández-Llamas, Camino, 67
Fernández-Lozano, J.J., 190
Ferrarelli, Paola, 394
Ferrein, Alexander, 370
Ferreira, Fausto, 263
Ferri, Gabriele, 263
Figueiredo, Rui, 117
Fitch, Robert, 320
Foix, Sergi, 141
Fonseca, Pedro, 382
Fraile, Juan C., 548
Frerichs, Ludger, 782
G
Galindo, Pedro L., 43, 53
Gandarias, Juan M., 165
García, Manuel, 272
García, Miguel Angel, 91
García-Aracil, Nicolás, 557
García-Cerezo, Alfonso J., 165, 190
Garrote, Luis, 475
Garzón, Mario, 606, 731
Garzón-Ramos, David, 731
Gaspar, Ana Rita, 463
Ginés, Jonathan, 283
Gomes, Rui, 213
Gómez-de-Gabriel, Jesús M., 165
Gomez-Ruiz, J.A., 190
González, José E., 569
Gorrotxategi, Jose, 3
Grisetti, G., 295
Guerrero-Higueras, Ángel Manuel, 67
Guindel, Carlos, 487
H
Hanheide, M., 295
Harms, Hannes, 782
Heredia, Guillermo, 332
Herrera, P. Javier, 251
Hervé, Pierre Ellie, 3
Hussein, Ahmed, 499
I
Iocchi, Luca, 295, 394
Izard, Jean Baptiste, 3
J
Jimenez-Fernandez, Angel, 580
K
Kallweit, Stephan, 370
Kasaei, S. Mohammadreza, 743
L
Lapucci, Tamara, 394
Lau, Nuno, 15, 129, 743
Lázaro, M.T., 295
Lera, Francisco J., 283
Lera, Francisco, 835, 849
Lim, Gi Hyun, 15
Lima, José, 643, 655
Linares-Barranco, Alejandro, 580
Liu, Xingcun, 263
Llamas, Luis, 359
López-Casado, Carmen, 548
M
Mancisidor, Aitziber, 704
Marchukov, Yaroslav, 794
Marin-Plaza, Pablo, 499
Marković, Ivan, 629
Martens, Wolfram, 320
Martín, David, 487, 499
Martín, Francisco, 283
Martín-Ávila, Juan, 190
Martínez-Barberá, H., 79
Martínez-de Dios, J. Ramiro, 179, 200
Martín-Guzmán, Miguel, 190
Matellán, Vicente, 67, 283
Matos, Anibal, 463
Montano, Luis, 794
Moreira, António Paulo, 668
Moreno, German, 849
Moreno, Juan C., 569
Moreno, Luis, 716
Moreno, Plinio, 695
Morgado-Estévez, Arturo, 43, 53
Mosteo, Alejandro R., 835
Muñoz, Luis Miguel, 102
Muñoz, Víctor F., 548
N
Navarro, Francisco, 606
Naya, Martin, 359
Neto, Pedro, 595
862
Author Index

Nunes, Alexandra, 463
Nunes, Urbano J., 475
O
Oliveira, Italo, 756
Oliveira, Paulo, 409, 422
Ollero, Anibal, 179, 272, 309, 332, 771
Olmo-Sevilla, Asuncion, 580
Oñativia, Jon, 524
P
Palau Franco, Marta, 263
Paneque, Julio L., 179
Paulino, Tiago, 695
Pedreiras, Paulo, 382
Pedrosa, Eurico, 15
Peixoto, Paulo, 475
Pereira, Artur, 15, 743
Pereira, Fernando Lobo, 213
Petillot, Yvan, 263
Petković, Tomislav, 629
Petrović, Ivan, 629
Piardi, Luis, 643, 655
Pinto, Andry, 463
Pinto, Charles, 704
Pinto, Luis Ramos, 807
Pons, José L., 569
Ponsa, Pere, 102
Prada Delgado, Javier, 309
Premebida, Cristiano, 475
Presa, Jorge, 524
Prieto, Abraham, 359
Puig, Domènec, 91
R
Ramírez, Juan Ramón Astorga, 179
Ramon Soria, Pablo, 309, 320
Ramón, Pablo, 272
Reis, Luís Paulo, 15, 129
Relvas, Pedro, 680
Riazuelo, Luis, 835
Ribeiro, Angela, 239
Rioja-del-Rio, Carlos, 53
Rivas-Blanco, Irene, 548
Rizzo, Carlos, 823, 849
Roa, Máximo A., 716
Rocha, Luís F., 680
Rodríguez, Mariola, 3
Rodríguez-Lera, Francisco Javier, 67
Rodríguez-Sedano, Francisco, 102
Rojas-de-Silva, Abiud, 28
Roldán, Juan Jesús, 606
Roning, Juha, 263
Rowe, Anthony, 807
S
Safeea, Mohammad, 595
Sajadi-Alamdari, Seyed Amin, 446
Sánchez, Emilio, 524
Sanchez-Sardana, Francisco L., 239
Sanfeliu, Alberto, 434
Santos-Victor, José, 117
Sayols, Narcís, 536
Schattenberg, Jan, 782
Schiffer, Stefan, 370
Schmiemann, Julian, 782
Schneider, Frank, 263
Scorza, Davide, 524
Seco, Teresa, 823
Serrano, Daniel, 849
Silva, Filipe, 382
Silva, Manuel, 680, 756
Silva, Ricardo, 680
Simões, David, 129, 743
Škoda, Jan, 345
Soler, Carles, 513
Soler, Fernando, 28
Soler, Marcel, 849
Soria, Pablo Ramon, 771
Sosa, Dario, 263
Soto-Núñez, José Andrés, 43, 53
Sousa, Armando, 153
Stengler, Erik, 263
Suarez, Alejandro, 332
Suárez, Raúl, 28
Sukkar, Fouad, 320
T
Talamino, Jordi Pérez, 434
Tardioli, Danilo, 835
Tavares, Pedro, 668
Torras, Carme, 141
Torres-González, Arturo, 179
Toscano, César, 617
V
Varela, Gervasio, 359
Vázquez, Andrés S., 227
Veiga, Germano, 153, 617, 668
Vicente-Diaz, Saturnino, 580
Viguria, Antidio, 263, 272
Villarroel, José Luis, 823, 835, 849
Voos, Holger, 446
W
Winﬁeld, Alan F.T., 263
Z
Zubizarreta, Asier, 704
Author Index
863



Applied Linear Regression
TLFeBOOK


Applied Linear Regression
Third Edition
SANFORD WEISBERG
University of Minnesota
School of Statistics
Minneapolis, Minnesota
A JOHN WILEY & SONS, INC., PUBLICATION

Copyright 2005 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form
or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee
to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400,
fax 978-646-8600, or on the web at www.copyright.com. Requests to the Publisher for permission
should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street,
Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts
in preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and speciﬁcally disclaim any implied warranties of
merchantability or ﬁtness for a particular purpose. No warranty may be created or extended by sales
representatives or written sales materials. The advice and strategies contained herein may not be
suitable for your situation. You should consult with a professional where appropriate. Neither the
publisher nor author shall be liable for any loss of proﬁt or any other commercial damages, including
but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services please contact our Customer Care
Department within the U.S. at 877-762-2974, outside the U.S. at 317-572-3993 or fax 317-572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print,
however, may not be available in electronic format.
Library of Congress Cataloging-in-Publication Data:
Weisberg, Sanford, 1947–
Applied linear regression / Sanford Weisberg.—3rd ed.
p. cm.—(Wiley series in probability and statistics)
Includes bibliographical references and index.
ISBN 0-471-66379-4 (acid-free paper)
1. Regression analysis. I. Title. II. Series.
QA278.2.W44 2005
519.5′36—dc22
2004050920
Printed in the United States of America.
10 9 8 7 6 5 4 3 2 1

To Carol, Stephanie
and
to the memory of my parents


Contents
Preface
xiii
1
Scatterplots and Regression
1
1.1
Scatterplots, 1
1.2
Mean Functions, 9
1.3
Variance Functions, 11
1.4
Summary Graph, 11
1.5
Tools for Looking at Scatterplots, 12
1.5.1
Size, 13
1.5.2
Transformations, 14
1.5.3
Smoothers for the Mean Function, 14
1.6
Scatterplot Matrices, 15
Problems, 17
2
Simple Linear Regression
19
2.1
Ordinary Least Squares Estimation, 21
2.2
Least Squares Criterion, 23
2.3
Estimating σ 2, 25
2.4
Properties of Least Squares Estimates, 26
2.5
Estimated Variances, 27
2.6
Comparing Models: The Analysis of Variance, 28
2.6.1
The F-Test for Regression, 30
2.6.2
Interpreting p-values, 31
2.6.3
Power of Tests, 31
2.7
The Coefﬁcient of Determination, R2, 31
2.8
Conﬁdence Intervals and Tests, 32
2.8.1
The Intercept, 32
2.8.2
Slope, 33
vii

viii
CONTENTS
2.8.3
Prediction, 34
2.8.4
Fitted Values, 35
2.9
The Residuals, 36
Problems, 38
3
Multiple Regression
47
3.1
Adding a Term to a Simple Linear Regression Model, 47
3.1.1
Explaining Variability, 49
3.1.2
Added-Variable Plots, 49
3.2
The Multiple Linear Regression Model, 50
3.3
Terms and Predictors, 51
3.4
Ordinary Least Squares, 54
3.4.1
Data and Matrix Notation, 54
3.4.2
Variance-Covariance Matrix of e, 56
3.4.3
Ordinary Least Squares Estimators, 56
3.4.4
Properties of the Estimates, 57
3.4.5
Simple Regression in Matrix Terms, 58
3.5
The Analysis of Variance, 61
3.5.1
The Coefﬁcient of Determination, 62
3.5.2
Hypotheses Concerning One of the Terms, 62
3.5.3
Relationship to the t-Statistic, 63
3.5.4
t-Tests and Added-Variable Plots, 63
3.5.5
Other Tests of Hypotheses, 64
3.5.6
Sequential Analysis of Variance Tables, 64
3.6
Predictions and Fitted Values, 65
Problems, 65
4
Drawing Conclusions
69
4.1
Understanding Parameter Estimates, 69
4.1.1
Rate of Change, 69
4.1.2
Signs of Estimates, 70
4.1.3
Interpretation Depends on Other Terms in the Mean
Function, 70
4.1.4
Rank Deﬁcient and Over-Parameterized Mean
Functions, 73
4.1.5
Tests, 74
4.1.6
Dropping Terms, 74
4.1.7
Logarithms, 76
4.2
Experimentation Versus Observation, 77

CONTENTS
ix
4.3
Sampling from a Normal Population, 80
4.4
More on R2, 81
4.4.1
Simple Linear Regression and R2, 83
4.4.2
Multiple Linear Regression, 84
4.4.3
Regression through the Origin, 84
4.5
Missing Data, 84
4.5.1
Missing at Random, 84
4.5.2
Alternatives, 85
4.6
Computationally Intensive Methods, 87
4.6.1
Regression Inference without Normality, 87
4.6.2
Nonlinear Functions of Parameters, 89
4.6.3
Predictors Measured with Error, 90
Problems, 92
5
Weights, Lack of Fit, and More
96
5.1
Weighted Least Squares, 96
5.1.1
Applications of Weighted Least Squares, 98
5.1.2
Additional Comments, 99
5.2
Testing for Lack of Fit, Variance Known, 100
5.3
Testing for Lack of Fit, Variance Unknown, 102
5.4
General F Testing, 105
5.4.1
Non-null Distributions, 107
5.4.2
Additional Comments, 108
5.5
Joint Conﬁdence Regions, 108
Problems, 110
6
Polynomials and Factors
115
6.1
Polynomial Regression, 115
6.1.1
Polynomials with Several Predictors, 117
6.1.2
Using the Delta Method to Estimate a Minimum or a
Maximum, 120
6.1.3
Fractional Polynomials, 122
6.2
Factors, 122
6.2.1
No Other Predictors, 123
6.2.2
Adding a Predictor: Comparing Regression Lines, 126
6.2.3
Additional Comments, 129
6.3
Many Factors, 130
6.4
Partial One-Dimensional Mean Functions, 131
6.5
Random Coefﬁcient Models, 134
Problems, 137

x
CONTENTS
7
Transformations
147
7.1
Transformations and Scatterplots, 147
7.1.1
Power Transformations, 148
7.1.2
Transforming Only the Predictor Variable, 150
7.1.3
Transforming the Response Only, 152
7.1.4
The Box and Cox Method, 153
7.2
Transformations and Scatterplot Matrices, 153
7.2.1
The 1D Estimation Result and Linearly Related
Predictors, 156
7.2.2
Automatic Choice of Transformation of Predictors, 157
7.3
Transforming the Response, 159
7.4
Transformations of Nonpositive Variables, 160
Problems, 161
8
Regression Diagnostics: Residuals
167
8.1
The Residuals, 167
8.1.1
Difference Between ˆe and e, 168
8.1.2
The Hat Matrix, 169
8.1.3
Residuals and the Hat Matrix with Weights, 170
8.1.4
The Residuals When the Model Is Correct, 171
8.1.5
The Residuals When the Model Is Not Correct, 171
8.1.6
Fuel Consumption Data, 173
8.2
Testing for Curvature, 176
8.3
Nonconstant Variance, 177
8.3.1
Variance Stabilizing Transformations, 179
8.3.2
A Diagnostic for Nonconstant Variance, 180
8.3.3
Additional Comments, 185
8.4
Graphs for Model Assessment, 185
8.4.1
Checking Mean Functions, 186
8.4.2
Checking Variance Functions, 189
Problems, 191
9
Outliers and Inﬂuence
194
9.1
Outliers, 194
9.1.1
An Outlier Test, 194
9.1.2
Weighted Least Squares, 196
9.1.3
Signiﬁcance Levels for the Outlier Test, 196
9.1.4
Additional Comments, 197
9.2
Inﬂuence of Cases, 198
9.2.1
Cook’s Distance, 198

CONTENTS
xi
9.2.2
Magnitude of Di, 199
9.2.3
Computing Di, 200
9.2.4
Other Measures of Inﬂuence, 203
9.3
Normality Assumption, 204
Problems, 206
10 Variable Selection
211
10.1 The Active Terms, 211
10.1.1 Collinearity, 214
10.1.2 Collinearity and Variances, 216
10.2 Variable Selection, 217
10.2.1 Information Criteria, 217
10.2.2 Computationally Intensive Criteria, 220
10.2.3 Using Subject-Matter Knowledge, 220
10.3 Computational Methods, 221
10.3.1 Subset Selection Overstates Signiﬁcance, 225
10.4 Windmills, 226
10.4.1 Six Mean Functions, 226
10.4.2 A Computationally Intensive Approach, 228
Problems, 230
11 Nonlinear Regression
233
11.1 Estimation for Nonlinear Mean Functions, 234
11.2 Inference Assuming Large Samples, 237
11.3 Bootstrap Inference, 244
11.4 References, 248
Problems, 248
12 Logistic Regression
251
12.1 Binomial Regression, 253
12.1.1 Mean Functions for Binomial Regression, 254
12.2 Fitting Logistic Regression, 255
12.2.1 One-Predictor Example, 255
12.2.2 Many Terms, 256
12.2.3 Deviance, 260
12.2.4 Goodness-of-Fit Tests, 261
12.3 Binomial Random Variables, 263
12.3.1 Maximum Likelihood Estimation, 263
12.3.2 The Log-Likelihood for Logistic Regression, 264

xii
CONTENTS
12.4 Generalized Linear Models, 265
Problems, 266
Appendix
270
A.1
Web Site, 270
A.2
Means and Variances of Random Variables, 270
A.2.1
E Notation, 270
A.2.2
Var Notation, 271
A.2.3
Cov Notation, 271
A.2.4
Conditional Moments, 272
A.3
Least Squares for Simple Regression, 273
A.4
Means and Variances of Least Squares Estimates, 273
A.5
Estimating E(Y|X) Using a Smoother, 275
A.6
A Brief Introduction to Matrices and Vectors, 278
A.6.1
Addition and Subtraction, 279
A.6.2
Multiplication by a Scalar, 280
A.6.3
Matrix Multiplication, 280
A.6.4
Transpose of a Matrix, 281
A.6.5
Inverse of a Matrix, 281
A.6.6
Orthogonality, 282
A.6.7
Linear Dependence and Rank of a Matrix, 283
A.7
Random Vectors, 283
A.8
Least Squares Using Matrices, 284
A.8.1
Properties of Estimates, 285
A.8.2
The Residual Sum of Squares, 285
A.8.3
Estimate of Variance, 286
A.9
The QR Factorization, 286
A.10 Maximum Likelihood Estimates, 287
A.11 The Box-Cox Method for Transformations, 289
A.11.1 Univariate Case, 289
A.11.2 Multivariate Case, 290
A.12 Case Deletion in Linear Regression, 291
References
293
Author Index
301
Subject Index
305

Preface
Regression analysis answers questions about the dependence of a response variable
on one or more predictors, including prediction of future values of a response, dis-
covering which predictors are important, and estimating the impact of changing a
predictor or a treatment on the value of the response. At the publication of the sec-
ond edition of this book about 20 years ago, regression analysis using least squares
was essentially the only methodology available to analysts interested in questions
like these. Cheap, widely available high-speed computing has changed the rules for
examining these questions. Modern competitors include nonparametric regression,
neural networks, support vector machines, and tree-based methods, among others.
A new ﬁeld of computer science, called machine learning, adds diversity, and con-
fusion, to the mix. With the availability of software, using a neural network or any
of these other methods seems to be just as easy as using linear regression.
So, a reasonable question to ask is: Who needs a revised book on linear regres-
sion using ordinary least squares when all these other newer and, presumably,
better methods exist? This question has several answers. First, most other mod-
ern regression modeling methods are really just elaborations or modiﬁcations of
linear regression modeling. To understand, as opposed to use, neural networks or
the support vector machine is nearly impossible without a good understanding of
linear regression methodology. Second, linear regression methodology is relatively
transparent, as will be seen throughout this book. We can draw graphs that will
generally allow us to see relationships between variables and decide whether the
models we are using make any sense. Many of the more modern methods are much
like a black box in which data are stuffed in at one end and answers pop out at
the other, without much hope for the nonexpert to understand what is going on
inside the box. Third, if you know how to do something in linear regression, the
same methodology with only minor adjustments will usually carry over to other
regression-type problems for which least squares is not appropriate. For example,
the methodology for comparing response curves for different values of a treatment
variable when the response is continuous is studied in Chapter 6 of this book. Anal-
ogous methodology can be used when the response is a possibly censored survival
time, even though the method of ﬁtting needs to be appropriate for the censored
response and not least squares. The methodology of Chapter 6 is useful both in its
xiii

xiv
PREFACE
own right when applied to linear regression problems and as a set of core ideas
that can be applied in other settings.
Probably the most important reason to learn about linear regression and least
squares estimation is that even with all the new alternatives most analyses of data
continue to be based on this older paradigm. And why is this? The primary reason
is that it works: least squares regression provides good, and useful, answers to
many problems. Pick up the journals in any area where data are commonly used
for prediction or estimation and the dominant method used will be linear regression
with least squares estimation.
What’s New in this Edition
Many of the examples and homework data sets from the second edition have been
kept, although some have been updated. The fuel consumption data, for example,
now uses 2001 values rather than 1974 values. Most of the derivations are the
same as in the second edition, although the order of presentation is somewhat
different. To keep the length of the book nearly unchanged, methods that failed to
gain general usage have been deleted, as have the separate chapters on prediction
and missing data. These latter two topics have been integrated into the remaining
text.
The continuing theme of the second edition was the need for diagnostic methods,
in which ﬁtted models are analyzed for deﬁciencies, through analysis of residuals
and inﬂuence. This emphasis was unusual when the second edition was published
and important quantities like Studentized residuals and Cook’s distance were not
readily available in the commercial software of the time.
Times have changed, and so has the emphasis of this book. This edition stresses
graphical methods including looking at data both before and after ﬁtting models.
This is reﬂected immediately in the new Chapter 1, which introduces the key idea of
looking at data with scatterplots and the somewhat less universal tool of scatterplot
matrices. Most analyses and homework problems start with drawing graphs. We
tailor analyses to correspond to what we see in the graphs, and this additional
step can make modeling easier and ﬁtted models reﬂect the data more closely.
Remarkably, this also lessens the need for diagnostic methods.
The emphasis on graphs leads to several additional methods and procedures that
were not included in the second edition. The use of smoothers to help summarize
a scatterplot is introduced early, although only a little of the theory of smoothing
is presented (in Appendix A.5). Transformations of predictors and the response
are stressed, and relatively unfamiliar methods based both on smoothing and on
generalization of the Box–Cox method are presented in Chapter 7.
Another new topic included in the book is computationally intensive methods
and simulation. The key example of this is the bootstrap, in Section 4.6, which
can be used to make inferences about ﬁtted models in small samples. A somewhat
different computationally intensive method is used in an example in Chapter 10,
which is a completely rewritten chapter on variable selection.
The book concludes with two expanded chapters on nonlinear and logistic regres-
sion, both of which are generalizations of the linear regression model. I have

PREFACE
xv
included these chapters to provide instructors and students with enough informa-
tion for basic usage of these models and to take advantage of the intuition gained
about them from an in-depth study of the linear regression model. Each of these
can be treated at book-length, and appropriate references are given.
Mathematical Level
The mathematical level of this book is roughly the same as the level of the second
edition. Matrix representation of data is used, particularly in the derivation of the
methodology in Chapters 2–4. Derivations are less frequent in later chapters, and so
the necessary mathematics is less. Calculus is generally not required, except for an
occasional use of a derivative, for the discussion of the delta method, Section 6.1.2,
and for a few topics in the Appendix. The discussions requiring calculus can be
skipped without much loss.
Computing and Computer Packages
Like the second edition, only passing mention is made in the book to computer
packages. To help the reader make a connection between the text and a com-
puter package for doing the computations, we provide several web companions for
Applied Linear Regression that discuss how to use standard statistical packages for
linear regression analysis. The packages covered include JMP, SAS, SPSS, R, and
S-plus; others may be included after publication of the book. In addition, all the
data ﬁles discussed in the book are also on the website. The web address for this
material is
http://www.stat.umn.edu/alr
Some readers may prefer to have a book that integrates the text more closely
with a computer package, and for this purpose, I can recommend R. D. Cook and
S. Weisberg (1999), Applied Regression Including Computing and Graphics, also
published by John Wiley. This book includes a very user-friendly, free computer
package called Arc that does everything that is described in that book and also
nearly everything in Applied Linear Regression.
Teaching with this Book
The ﬁrst ten chapters of the book should provide adequate material for a one-quarter
course on linear regression. For a semester-length course, the last two chapters can
be added. A teacher’s manual, primarily giving solutions to all the homework
problems, can be obtained from the publisher by instructors.
Acknowledgments
I am grateful to several people who generously shared their data for inclusion
in this book; they are cited where their data appears. Charles Anderson and Don
Pereira suggested several of the examples. Keija Shan, Katherine St. Clair, and
Gary Oehlert helped with the website and its content. Brian Sell helped with the

xvi
PREFACE
examples and with many administrative chores. Several others helped with earlier
editions: Christopher Bingham, Morton Brown, Cathy Campbell, Dennis Cook,
Stephen Fienberg, James Frane, Seymour Geisser, John Hartigan, David Hinkley,
Alan Izenman, Soren Johansen, Kenneth Koehler, David Lane, Michael Lavine,
Kinley Larntz, John Rice, Donald Rubin, Joe Shih, Pete Stewart, Stephen Stigler,
Douglas Tiffany, Carol Weisberg, and Howard Weisberg.
Sanford Weisberg
St. Paul, Minnesota
April 13, 2004

C H A P T E R 1
Scatterplots and Regression
Regression is the study of dependence. It is used to answer questions such as Does
changing class size affect success of students? Can we predict the time of the next
eruption of Old Faithful Geyser from the length of the most recent eruption? Do
changes in diet result in changes in cholesterol level, and if so, do the results depend
on other characteristics such as age, sex, and amount of exercise? Do countries with
higher per person income have lower birth rates than countries with lower income?
Regression analysis is a central part of many research projects. In most of this book,
we study the important instance of regression methodology called linear regression.
These methods are the most commonly used in regression, and virtually all other
regression methods build upon an understanding of how linear regression works.
As with most statistical analyses, the goal of regression is to summarize observed
data as simply, usefully, and elegantly as possible. In some problems, a theory may
be available that speciﬁes how the response varies as the values of the predictors
change. In other problems, a theory may be lacking, and we need to use the data to
help us decide on how to proceed. In either case, an essential ﬁrst step in regression
analysis is to draw appropriate graphs of the data.
In this chapter, we discuss the fundamental graphical tool for looking at regres-
sion data, a two-dimensional scatterplot. In regression problems with one predictor
and one response, the scatterplot of the response versus the predictor is the starting
point for regression analysis. In problems with many predictors, several simple
graphs will be required at the beginning of an analysis. A scatterplot matrix is a
convenient way to organize looking at many scatterplots at once. We will look at
several examples to introduce the main tools for looking at scatterplots and scat-
terplot matrices and extracting information from them. We will also introduce the
notation that will be used throughout the rest of the book.
1.1
SCATTERPLOTS
We begin with a regression problem with one predictor, which we will generi-
cally call X and one response variable, which we will call Y. Data consists of
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
1

2
SCATTERPLOTS AND REGRESSION
values (xi, yi), i = 1, . . . , n, of (X, Y) observed on each of n units or cases. In
any particular problem, both X and Y will have other names such as Temperature
or Concentration that are more descriptive of the data that is to be analyzed. The
goal of regression is to understand how the values of Y change as X is varied
over its range of possible values. A ﬁrst look at how Y changes as X is varied is
available from a scatterplot.
Inheritance of Height
One of the ﬁrst uses of regression was to study inheritance of traits from generation
to generation. During the period 1893–1898, E. S. Pearson organized the collection
of n = 1375 heights of mothers in the United Kingdom under the age of 65 and
one of their adult daughters over the age of 18. Pearson and Lee (1903) published
the data, and we shall use these data to examine inheritance. The data are given in
the data ﬁle heights.txt1.
Our interest is in inheritance from the mother to the daughter, so we view the
mother’s height, called Mheight, as the predictor variable and the daughter’s height,
Dheight, as the response variable. Do taller mothers tend to have taller daughters?
Do shorter mothers tend to have shorter daughters?
A scatterplot of Dheight versus Mheight helps us answer these questions. The
scatterplot is a graph of each of the n points with the response Dheight on the
vertical axis and predictor Mheight on the horizontal axis. This plot is shown in
Figure 1.1. For regression problems with one predictor X and a response Y, we
call the scatterplot of Y versus X a summary graph.
Here are some important characteristics of Figure 1.1:
1. The range of heights appears to be about the same for mothers and for daugh-
ters. Because of this, we draw the plot so that the lengths of the horizontal
and vertical axes are the same, and the scales are the same. If all mothers and
daughters had exactly the same height, then all the points would fall exactly
on a 45◦line. Some computer programs for drawing a scatterplot are not
smart enough to ﬁgure out that the lengths of the axes should be the same,
so you might need to resize the plot or to draw it several times.
2. The original data that went into this scatterplot was rounded so each of the
heights was given to the nearest inch. If we were to plot the original data,
we would have substantial overplotting with many points at exactly the same
location. This is undesirable because we will not know if one point represents
one case or many cases, and this can be very misleading. The easiest solution
is to use jittering, in which a small uniform random number is added to each
value. In Figure 1.1, we used a uniform random number on the range from
−0.5 to +0.5, so the jittered values would round to the numbers given in the
original source.
3. One important function of the scatterplot is to decide if we might reasonably
assume that the response on the vertical axis is independent of the predictor
1See Appendix A.1 for instructions for getting data ﬁles from the Internet.

SCATTERPLOTS
3
55
60
Dheight
65
70
75
55
60
Mheight
65
70
75
FIG. 1.1
Scatterplot of mothers’ and daughters’ heights in the Pearson and Lee data. The original data
have been jittered to avoid overplotting, but if rounded to the nearest inch would return the original
data provided by Pearson and Lee.
on the horizontal axis. This is clearly not the case here since as we move
across Figure 1.1 from left to right, the scatter of points is different for each
value of the predictor. What we mean by this is shown in Figure 1.2, in which
we show only points corresponding to mother–daughter pairs with Mheight
rounding to either 58, 64 or 68 inches. We see that within each of these three
strips or slices, even though the number of points is different within each
slice, (a) the mean of Dheight is increasing from left to right, and (b) the
vertical variability in Dheight seems to be more or less the same for each of
the ﬁxed values of Mheight.
4. The scatter of points in the graph appears to be more or less elliptically
shaped, with the axis of the ellipse tilted upward. We will see in Section 4.3
that summary graphs that look like this one suggest use of the simple linear
regression model that will be discussed in Chapter 2.
5. Scatterplots are also important for ﬁnding separated points, which are either
points with values on the horizontal axis that are well separated from the
other points or points with values on the vertical axis that, given the value
on the horizontal axis, are either much too large or too small. In terms of
this example, this would mean looking for very tall or short mothers or,
alternatively, for daughters who are very tall or short, given the height of
their mother.

4
SCATTERPLOTS AND REGRESSION
Mheight
Dheight
55
60
65
70
75
55
60
65
70
75
FIG. 1.2
Scatterplot showing only pairs with mother’s height that rounds to 58, 64 or 68 inches.
These two types of separated points have different names and roles in a
regression problem. Extreme values on the left and right of the horizontal
axis are points that are likely to be important in ﬁtting regression models
and are called leverage points. The separated points on the vertical axis, here
unusually tall or short daughters give their mother’s height, are potentially
outliers, cases that are somehow different from the others in the data.
While the data in Figure 1.1 do include a few tall and a few short mothers
and a few tall and short daughters, given the height of the mothers, none
appears worthy of special treatment, mostly because in a sample size this
large we expect to see some fairly unusual mother–daughter pairs.
We will continue with this example later.
Forbes’ Data
In an 1857 article, a Scottish physicist named James D. Forbes discussed a series of
experiments that he had done concerning the relationship between atmospheric pres-
sure and the boiling point of water. He knew that altitude could be determined from
atmospheric pressure, measured with a barometer, with lower pressures correspond-
ing to higher altitudes. In the middle of the nineteenth century, barometers were
fragile instruments, and Forbes wondered if a simpler measurement of the boiling
point of water could substitute for a direct reading of barometric pressure. Forbes

SCATTERPLOTS
5
195
200
205
210
22
24
26
28
30
Temperature
(a)
Pressure
195
200
205
210
−0.2
0.0
0.2
0.4
0.6
Temperature
(b)
Residuals
FIG. 1.3
Forbes data. (a) Pressure versus Temp; (b) Residuals versus Temp.
collected data in the Alps and in Scotland. He measured at each location pressure
in inches of mercury with a barometer and boiling point in degrees Fahrenheit
using a thermometer. Boiling point measurements were adjusted for the difference
between the ambient air temperature when he took the measurements and a standard
temperature. The data for n = 17 locales are reproduced in the ﬁle forbes.txt.
The scatterplot of Pressure versus Temp is shown in Figure 1.3a. The gen-
eral appearance of this plot is very different from the summary graph for the
heights data. First, the sample size is only 17, as compared to over 1300 for the
heights data. Second, apart from one point, all the points fall almost exactly on a
smooth curve. This means that the variability in pressure for a given temperature
is extremely small.
The points in Figure 1.3a appear to fall very close to the straight line shown
on the plot, and so we might be encouraged to think that the mean of pressure
given temperature could be modelled by a straight line. Look closely at the graph,
and you will see that there is a small systematic error with the straight line: apart
from the one point that does not ﬁt at all, the points in the middle of the graph
fall below the line, and those at the highest and lowest temperatures fall above the
line. This is much easier to see in Figure 1.3b, which is obtained by removing the
linear trend from Figure 1.3a, so the plotted points on the vertical axis are given
for each value of Temp by
Residual = Pressure −point on the line
This allows us to gain resolution in the plot since the range on the vertical axis in
Figure 1.3a is about 10 inches of mercury while the range in Figure 1.3b is about
0.8 inches of mercury. To get the same resolution in Figure 1.3a, we would need
a graph that is 10/0.8 = 12.5 as big as Figure 1.3b. Again ignoring the one point
that clearly does not match the others, the curvature in the plot is clearly visible in
Figure 1.3b.

6
SCATTERPLOTS AND REGRESSION
195
200
205
210
1.35
1.40
1.45
Temperature
(a)
log(pressure)
195
200
205
210
0.000
0.005
0.010
Temperature
(b)
Residuals
FIG. 1.4
(a) Scatterplot of Forbes’ data. The line shown is the ols line for the regression of
log(Pressure) on Temp. (b) Residuals versus Temp.
While there is nothing at all wrong with curvature, the methods we will be
studying in this book work best when the plot can be summarized by a straight
line. Sometimes we can get a straight line by transforming one or both of the plotted
quantities. Forbes had a physical theory that suggested that log(Pressure) is linearly
related to Temp. Forbes (1857) contains what may be the ﬁrst published summary
graph corresponding to his physical model. His ﬁgure is redrawn in Figure 1.4.
Following Forbes, we use base ten common logs in this example, although in
most of the examples in this book we will use base-two logarithms. The choice of
base has no material effect on the appearance of the graph or on ﬁtted regression
models, but interpretation of parameters can depend on the choice of base, and
using base-two often leads to a simpler interpretation for parameters.
The key feature of Figure 1.4a is that apart from one point the data appear to
fall very close to the straight line shown on the ﬁgure, and the residual plot in
Figure 1.4b conﬁrms that the deviations from the straight line are not systematic
the way they were in Figure 1.3b. All this is evidence that the straight line is a
reasonable summary of these data.
Length at Age for Smallmouth Bass
The smallmouth bass is a favorite game ﬁsh in inland lakes. Many smallmouth bass
populations are managed through stocking, ﬁshing regulations, and other means,
with a goal to maintain a healthy population.
One tool in the study of ﬁsh populations is to understand the growth pattern of
ﬁsh such as the dependence of a measure of size like ﬁsh length on age of the ﬁsh.
Managers could compare these relationships between different populations with
dissimilar management plans to learn how management impacts ﬁsh growth.
Figure 1.5 displays the Length at capture in mm versus Age at capture for n =
439 small mouth bass measured in West Bearskin Lake in Northeastern Minnesota
in 1991. Only ﬁsh of age seven or less are included in this graph. The data were
provided by the Minnesota Department of Natural Resources and are given in the

SCATTERPLOTS
7
Age
Length
50
100
150
200
250
300
350
2
4
6
8
FIG. 1.5
Length (mm) versus Age for West Bearskin Lake smallmouth bass. The solid line shown
was estimated using ordinary least squares or ols. The dashed line joins the average observed length
at each age.
ﬁle wblake.txt. Fish scales have annular rings like trees, and these can be
counted to determine the age of a ﬁsh. These data are cross-sectional, meaning
that all the observations were taken at the same time. In a longitudinal study, the
same ﬁsh would be measured each year, possibly requiring many years of taking
measurements. The data ﬁle gives the Length in mm, Age in years, and the Scale
radius, also in mm.
The appearance of this graph is different from the summary plots shown for last
two examples. The predictor Age can only take on integer values corresponding to
the number of annular rings on the scale, so we are really plotting seven distinct
populations of ﬁsh. As might be expected, length generally increases with age, but
the longest ﬁsh at age-one ﬁsh exceeds the length of the shortest age-four ﬁsh,
so knowing the age of a ﬁsh will not allow us to predict its length exactly; see
Problem 2.5.
Predicting the Weather
Can early season snowfall from September 1 until December 31 predict snowfall
in the remainder of the year, from January 1 to June 30? Figure 1.6, using data
from the data ﬁle ftcollinssnow.txt, gives a plot of Late season snowfall
from January 1 to June 30 versus Early season snowfall for the period September
1 to December 31 of the previous year, both measured in inches at Ft. Collins,
Colorado2. If Late is related to Early, the relationship is considerably weaker than
2The data are from the public domain source http://www.ulysses.atmos.colostate.edu.

8
SCATTERPLOTS AND REGRESSION
0
10
20
30
40
50
60
0
10
20
30
40
50
60
Early
Late
FIG. 1.6
Plot of snowfall for 93 years from 1900 to 1992 in inches. The solid horizontal line is drawn
at the average late season snowfall. The dashed line is the best ﬁtting (ordinary least squares) line of
arbitrary slope.
in the previous examples, and the graph suggests that early winter snowfall and
late winter snowfall may be completely unrelated, or uncorrelated. Interest in this
regression problem will therefore be in testing the hypothesis that the two variables
are uncorrelated versus the alternative that they are not uncorrelated, essentially
comparing the ﬁt of the two lines shown in Figure 1.6. Fitting models will be
helpful here.
Turkey Growth
This example is from an experiment on the growth of turkeys (Noll, Weibel, Cook,
and Witmer, 1984). Pens of turkeys were grown with an identical diet, except
that each pen was supplemented with a Dose of the amino acid methionine as a
percentage of the total diet of the birds. The methionine was provided using either
a standard source or one of two experimental sources. The response is average
weight gain in grams of all the turkeys in the pen.
Figure 1.7
provides
a
summary
graph
based
on
the
data
in
the
ﬁle
turkey.txt. Except at Dose = 0, each point in the graph is the average response
of ﬁve pens of turkeys; at Dose = 0, there were ten pens of turkeys. Because aver-
ages are plotted, the graph does not display the variation between pens treated alike.
At each value of Dose > 0, there are three points shown, with different symbols
corresponding to the three sources of methionine, so the variation between points
at a given Dose is really the variation between sources. At Dose = 0, the point has
been arbitrarily labelled with the symbol for the ﬁrst group, since Dose = 0 is the
same treatment for all sources.
For now, ignore the three sources and examine Figure 1.7 in the way we have
been examining the other summary graphs in this chapter. Weight gain seems

MEAN FUNCTIONS
9
0.00
0.05
0.10
0.15
0.20
0.25
650
700
750
800
Amount (percent of diet)
Weight gain (g)
1
2
3
FIG. 1.7
Weight gain versus Dose of methionine for turkeys. The three symbols for the points refer
to three different sources of methionine.
to increase with increasing Dose, but the increase does not appear to be linear,
meaning that a straight line does not seem to be a reasonable representation of
the average dependence of the response on the predictor. This leads to study of
mean functions.
1.2
MEAN FUNCTIONS
Imagine a generic summary plot of Y versus X. Our interest centers on how the
distribution of Y changes as X is varied. One important aspect of this distribution
is the mean function, which we deﬁne by
E(Y|X = x) = a function that depends on the value of x
(1.1)
We read the left side of this equation as “the expected value of the response when
the predictor is ﬁxed at the value X = x;” if the notation “E( )” for expectations
and “Var( )” for variances is unfamiliar, please read Appendix A.2. The right side
of (1.1) depends on the problem. For example, in the heights data in Example 1.1,
we might believe that
E(Dheight|Mheight = x) = β0 + β1x
(1.2)
that is, the mean function is a straight line. This particular mean function has two
parameters, an intercept β0 and a slope β1. If we knew the values of the βs, then
the mean function would be completely speciﬁed, but usually the βs need to be
estimated from data.
Figure 1.8 shows two possibilities for βs in the straight-line mean function (1.2)
for the heights data. For the dashed line, β0 = 0 and β1 = 1. This mean function

10
SCATTERPLOTS AND REGRESSION
55
60
65
70
55
60
65
70
Mheight
Dheight
FIG. 1.8
The heights data. The dashed line is for E(Dheight|Mheight) = Mheight, and the solid line
is estimated by ols.
would suggest that daughters have the same height as their mothers on average.
The second line is estimated using ordinary least squares, or ols, the estimation
method that will be described in the next chapter. The ols line has slope less
than one, meaning that tall mothers tend to have daughters who are taller than
average because the slope is positive but shorter than themselves because the slope
is less than one. Similarly, short mothers tend to have short daughters but taller
than themselves. This is perhaps a surprising result and is the origin of the term
regression, since extreme values in one generation tend to revert or regress toward
the population mean in the next generation.
Two lines are shown in Figure 1.5 for the smallmouth bass data. The dashed
line joins the average length at each age. It provides an estimate of the mean
function E(Length|Age) without actually specifying any functional form for the
mean function. We will call this a nonparametric estimated mean function; some-
times we will call it a smoother. The solid line is the ols estimated straight line
(1.1) for the mean function. Perhaps surprisingly, the straight line and the dashed
lines that join the within-age means appear to agree very closely, and we might
be encouraged to use the straight-line mean function to describe these data. This
would mean that the increase in length per year is the same for all ages. We cannot
expect this to be true if we were to include older-aged ﬁsh because eventually the
growth rate must slow down. For the range of ages here, the approximation seems
to be adequate.
For the Ft. Collins weather data, we might expect the straight-line mean function
(1.1) to be appropriate but with β1 = 0. If the slope is zero, then the mean function
is parallel to the horizontal axis, as shown in Figure 1.6. We will eventually test
for independence of Early and Late by testing the hypothesis that β1 = 0 against
the alternative hypothesis that β1 ̸= 0.

SUMMARY GRAPH
11
Not all summary graphs will have a straight-line mean function. In Forbes’
data, to achieve linearity we have replaced the measured value of Pressure by
log(Pressure). Transformation of variables will be a key tool in extending the
usefulness of linear regression models. In the turkey data and other growth models,
a nonlinear mean function might be more appropriate, such as
E(Y|Dose = x) = β0 + β1[1 −exp(−β2x)]
(1.3)
The βs in (1.3) have a useful interpretation, and they can be used to summarize the
experiment. When Dose = 0, E(Y|Dose = 0) = β0, so β0 is the baseline growth
without supplementation. Assuming β2 > 0, when the Dose is large, exp(−β2Dose)
is small, and so E(Y|Dose) approaches β0 + β1 for large Dose. We think of β0 + β1
as the limit to growth with this additive. The rate parameter β2 determines how
quickly maximum growth is achieved. This three-parameter mean function will be
considered in Chapter 11.
1.3
VARIANCE FUNCTIONS
Another characteristic of the distribution of the response given the predictor is
the variance function, deﬁned by the symbol Var(Y|X = x) and in words as the
variance of the response distribution given that the predictor is ﬁxed at X = x. For
example, in Figure 1.2 we can see that the variance function for Dheight|Mheight
is approximately the same for each of the three values of Mheight shown in the
graph. In the smallmouth bass data in Figure 1.5, an assumption that the variance
is constant across the plot is plausible, even if it is not certain (see Problem 1.1). In
the turkey data, we cannot say much about the variance function from the summary
plot because we have plotted treatment means rather than the actual pen values, so
the graph does not display the information about the variability between pens that
have a ﬁxed value of Dose.
A frequent assumption in ﬁtting linear regression models is that the variance
function is the same for every value of x. This is usually written as
Var(Y|X = x) = σ 2
(1.4)
where σ 2 (read “sigma squared”) is a generally unknown positive constant. We will
encounter later in this book other problems with complicated variance functions.
1.4
SUMMARY GRAPH
In all the examples except the snowfall data, there is a clear dependence of the
response on the predictor. In the snowfall example, there might be no dependence
at all. The turkey growth example is different from the others because the average
value of the response seems to change nonlinearly with the value of the predictor
on the horizontal axis.

12
SCATTERPLOTS AND REGRESSION
TABLE 1.1
Four Hypothetical Data Sets. The Data Are Given in the File
anscombe.txt
X1
Y1
Y2
Y3
X2
Y4
10
8.04
9.14
7.46
8
6.580
8
6.95
8.14
6.77
8
5.760
13
7.58
8.74
12.74
8
7.710
9
8.81
8.77
7.11
8
8.840
11
8.33
9.26
7.81
8
8.470
14
9.96
8.1
8.84
8
7.040
6
7.24
6.13
6.08
8
5.250
4
4.26
3.1
5.39
19
12.500
12
10.84
9.13
8.15
8
5.560
7
4.82
7.26
6.42
8
7.910
5
5.68
4.74
5.73
8
6.890
The scatterplots for these examples are all typical of graphs one might see in
problems with one response and one predictor. Examination of the summary graph
is a ﬁrst step in exploring the relationships these graphs portray.
Anscombe (1973) provided the artiﬁcial data given in Table 1.1 that consists
of 11 pairs of points (xi, yi), to which the simple linear regression mean function
E(y|x) = β0 + β1x is ﬁt. Each data set leads to an identical summary analysis
with the same estimated slope, intercept, and other summary statistics, but the
visual impression of each of the graphs is very different. The ﬁrst example in
Figure 1.9a is as one might expect to observe if the simple linear regression model
were appropriate. The graph of the second data set given in Figure 1.9b suggests
that the analysis based on simple linear regression is incorrect and that a smooth
curve, perhaps a quadratic polynomial, could be ﬁt to the data with little remaining
variability. Figure 1.9c suggests that the prescription of simple regression may be
correct for most of the data, but one of the cases is too far away from the ﬁtted
regression line. This is called the outlier problem. Possibly the case that does not
match the others should be deleted from the data set, and the regression should be
reﬁt from the remaining ten cases. This will lead to a different ﬁtted line. Without
a context for the data, we cannot judge one line “correct” and the other “incorrect”.
The ﬁnal set graphed in Figure 1.9d is different from the other three in that there
is not enough information to make a judgment concerning the mean function. If
the eighth case were deleted, we could not even estimate a slope. We must distrust
an analysis that is so heavily dependent upon a single case.
1.5
TOOLS FOR LOOKING AT SCATTERPLOTS
Because looking at scatterplots is so important to ﬁtting regression models, we
establish some common vocabulary for describing the information in them and
some tools to help us extract the information they contain.

TOOLS FOR LOOKING AT SCATTERPLOTS
13
y
4
6
8
10
12
4
6
8
10
12
x
5
10
15
20
5
(a)
(b)
(c)
(d)
10
15
20
FIG. 1.9
Four hypothetical data sets (from Anscombe, 1973).
The summary graph is of the response Y versus the predictor X. The mean
function for the graph is deﬁned by (1.1), and it characterizes how Y changes on
the average as the value of X is varied. We may have a parametric model for the
mean function and will use data to estimate the parameters. The variance function
also characterizes the graph, and in many problems we will assume at least at ﬁrst
that the variance function is constant. The scatterplot also will highlight separated
points that may be of special interest because they do not ﬁt the trend determined
by the majority of the points.
A null plot has constant mean function, constant variance function and no sep-
arated points. The scatterplot for the snowfall data appears to be a null plot.
1.5.1
Size
To extract all the available information from a scatterplot, we may need to interact
with it by changing scales, by resizing, or by removing linear trends. An example
of this is given in Problem 1.2.

14
SCATTERPLOTS AND REGRESSION
1.5.2
Transformations
In some problems, either or both of Y and X can be replaced by transformations so
the summary graph has desirable properties. Most of the time, we will use power
transformations, replacing, for example, X by Xλ for some number λ. Because
logarithmic transformations are so frequently used, we will interpret λ = 0 as cor-
responding to a log transform. In this book, we will generally use logs to the base
two, but if your computer program does not permit the use of base-two logarithms,
any other base, such as base-ten or natural logarithms, is equivalent.
1.5.3
Smoothers for the Mean Function
In the smallmouth bass data in Figure 1.5, we computed an estimate of
E(Length|Age) using a simple nonparametric smoother obtained by averaging the
repeated observations at each value of Age. Smoothers can also be deﬁned when
we do not have repeated observations at values of the predictor by averaging the
observed data for all values of X close to, but not necessarily equal to, x. The
literature on using smoothers to estimate mean functions has exploded in recent
years, with good fairly elementary treatments given by H¨ardle (1990), Simonoff
(1996), Bowman and Azzalini (1997), and Green and Silverman (1994). Although
these authors discuss nonparametric regression as an end in itself, we will gen-
erally use smoothers as plot enhancements to help us understand the information
available in a scatterplot and to help calibrate the ﬁt of a parametric mean function
to a scatterplot.
For example, Figure 1.10 repeats Figure 1.1, this time adding the estimated
straight-line mean function and smoother called a loess smooth (Cleveland, 1979).
Roughly speaking, the loess smooth estimates E(Y|X = x) at the point x by ﬁtting
55
60
65
70
55
60
65
70
Mheight
Dheight
FIG. 1.10
Heights data with the ols line and a loess smooth with span = 0.10.

SCATTERPLOT MATRICES
15
a straight line to a fraction of the points closest to x; we used the fraction of 0.20
in this ﬁgure because the sample size is so large, but it is more usual to set the
fraction to about 2/3. The smoother is obtained by joining the estimated values of
E(Y|X = x) for many values of x. The loess smoother and the straight line agree
almost perfectly for Mheight close to average, but they agree less well for larger
values of Mheight where there is much less data. Smoothers tend to be less reliable
at the edges of the plot. We brieﬂy discuss the loess smoother in Appendix A.5,
but this material is dependent on the results in Chapters 2–4.
1.6
SCATTERPLOT MATRICES
With one potential predictor, a scatterplot provides a summary of the regression
relationship between the response and the potential predictor. With many potential
predictors, we need to look at many scatterplots. A scatterplot matrix is a convenient
way to organize these plots.
Fuel Consumption
The goal of this example is to understand how fuel consumption varies over the
50 United States and the District of Columbia, and, in particular, to understand the
effect on fuel consumption of state gasoline tax. Table 1.2 describes the variables
to be used in this example; the data are given in the ﬁle fuel2001.txt. The
data were collected by the US Federal Highway Administration.
Both Drivers and FuelC are state totals, so these will be larger in states with
more people and smaller in less populous states. Income is computed per person.
To make all these comparable and to attempt to eliminate the effect of size of the
state, we compute rates Dlic = Drivers/Pop and Fuel = FuelC/Pop. Additionally,
we replace Miles by its (base-two) logarithm before doing any further analysis.
Justiﬁcation for replacing Miles with log(Miles) is deferred to Problem 7.7.
TABLE 1.2
Variables in the Fuel Consumption Dataa
Drivers
Number of licensed drivers in the state
FuelC
Gasoline sold for road use, thousands of gallons
Income
Per person personal income for the year 2000, in thousands of dollars
Miles
Miles of Federal-aid highway miles in the state
Pop
2001 population age 16 and over
Tax
Gasoline state tax rate, cents per gallon
State
State name
Fuel
1000 × Fuelc/Pop
Dlic
1000 × Drivers/Pop
log(Miles)
Base-two logarithm of Miles
Source: “Highway Statistics 2001,” http://www.fhwa.dot.gov/ohim/hs01/index.htm.
aAll data are for 2001, unless otherwise noted. The last three variables do not appear in the data ﬁle
but are computed from the previous variables, as described in the text.

16
SCATTERPLOTS AND REGRESSION
Tax
700
900
12
16
10
20
700
900
Dlic
Income
25
35
12
16
logMiles
10
20
25
35
300
600
300
600
Fuel
FIG. 1.11
Scatterplot matrix for the fuel data.
The scatterplot matrix for the fuel data is shown in Figure 1.11. Except for the
diagonal, a scatterplot matrix is a 2D array of scatterplots. The variable names on
the diagonal label the axes. In Figure 1.11, the variable log(Miles) appears on the
horizontal axis of the all the plots in the fourth column from the left and on the
vertical axis of all the plots in the fourth row from the top3.
Each plot in a scatterplot matrix is relevant to a particular one-predictor regres-
sion of the variable on the vertical axis, given the variable on the horizontal axis.
For example, the plot of Fuel versus Tax in the last plot in the ﬁrst column of the
scatterplot matrix is relevant for the regression of Fuel on Tax ; this is the ﬁrst plot
in the last row of Figure 1.11. We can interpret this plot as we would a scatterplot
for simple regression. We get the overall impression that Fuel decreases on the
average as Tax increases, but there is lot of variation. We can make similar quali-
tative judgments about the each of the regressions of Fuel on the other variables.
The overall impression is that Fuel is at best weakly related to each of the variables
in the scatterplot matrix.
3The scatterplot matrix program used to draw Figure 1.11, which is the pairs function in R, has the
diagonal running from the top left to the lower right. Other programs, such as the splom function in
R, has the diagonal from lower-left to upper-right. There seems to be no strong reason to prefer one
over the other.

PROBLEMS
17
Does this help us understand how Fuel is related to all four predictors
simultaneously? The marginal relationships between the response and each of the
variables are not sufﬁcient to understand the joint relationship between the response
and the predictors. The interrelationships among the predictors are also important.
The pairwise relationships between the predictors can be viewed in the remain-
ing cells of the scatterplot matrix. In Figure 1.11, the relationships between all
pairs of predictors appear to be very weak, suggesting that for this problem the
marginal plots including Fuel are quite informative about the multiple regression
problem. General considerations for other scatterplot matrices will be developed in
later chapters.
PROBLEMS
1.1. Smallmouth bass data Compute the means and the variances for each of the
eight subpopulations in the smallmouth bass data. Draw a graph of average
length versus Age and compare to Figure 1.5. Draw a graph of the stan-
dard deviations versus age. If the variance function is constant, then the
plot of standard deviation versus Age should be a null plot. Summarize the
information.
1.2. Mitchell data The data shown in Figure 1.12 give average soil temperature
in degrees C at 20 cm depth in Mitchell, Nebraska, for 17 years beginning
January 1976, plotted versus the month number. The data were collected by
K. Hubbard and provided by O. Burnside.
1.2.1. Summarize the information in the graph about the dependence of soil
temperature on month number.
0
50
100
150
200
5
0
5
10
15
20
25
Months after January 1976
Average Soil Temperature
FIG. 1.12
Monthly soil temperature data.

18
SCATTERPLOTS AND REGRESSION
1.2.2. The data used to draw Figure 1.12 are in the ﬁle Mitchell.txt.
Redraw the graph, but this time make the length of the horizontal axis
at least four times the length of the vertical axis. Repeat Problem 1.2.1.
1.3. United Nations The data in the ﬁle UN1.txt contains PPgdp, the 2001 gross
national product per person in US dollars, and Fertility, the birth rate per 1000
females in the population in the year 2000. The data are for 193 localities,
mostly UN member countries, but also other areas such as Hong Kong that are
not independent countries; the third variable on the ﬁle called Locality gives
the name of the locality. The data were collected from http://unstats.un.org/
unsd/demographic. In this problem, we will study the conditional distribution
of Fertility given PPgdp.
1.3.1. Identify the predictor and the response.
1.3.2. Draw the scatterplot of Fertility on the vertical axis versus PPgdp on
the horizontal axis and summarize the information in this graph. Does
a straight-line mean function seem to be a plausible for a summary of
this graph?
1.3.3. Draw the scatterplot of log(Fertility) versus log(PPgdp), using logs to
the base two. Does the simple linear regression model seem plausible
for a summary of this graph?
1.4. Old Faithful The data in the data ﬁle oldfaith.txt gives information
about eruptions of Old Faithful Geyser during October 1980. Variables are
the Duration in seconds of the current eruption, and the Interval, the time
in minutes to the next eruption. The data were collected by volunteers and
were provided by R. Hutchinson. Apart from missing data for the period from
midnight to 6 AM, this is a complete record of eruptions for that month.
Old Faithful Geyser is an important tourist attraction, with up to several
thousand people watching it erupt on pleasant summer days. The park ser-
vice uses data like these to obtain a prediction equation for the time to the
next eruption.
Draw the relevant summary graph for predicting interval from duration,
and summarize your results.
1.5. Water run-off in the Sierras
Can Southern California’s water supply in
future years be predicted from past data? One factor affecting water availability
is stream run-off. If run-off could be predicted, engineers, planners and policy
makers could do their jobs more efﬁciently. The data in the ﬁle water.txt
contains 43 years’ worth of precipitation measurements taken at six sites in
the Sierra Nevada mountains (labelled APMAM, APSAB, APSLAKE, OPBPC,
OPRC, and OPSLAKE), and stream run-off volume at a site near Bishop,
California, labelled BSAAM. The data are from the UCLA Statistics WWW
server.
Draw the scatterplot matrix for these data and summarize the information
available from these plots.

C H A P T E R 2
Simple Linear Regression
The simple linear regression model consists of the mean function and the variance
function
E(Y|X = x) = β0 + β1x
Var(Y|X = x) = σ 2
(2.1)
The parameters in the mean function are the intercept β0, which is the value of
E(Y|X = x) when x equals zero, and the slope β1, which is the rate of change in
E(Y|X = x) for a unit change in X; see Figure 2.1. By varying the parameters, we
can get all possible straight lines. In most applications, parameters are unknown
and must be estimated using data. The variance function in (2.1) is assumed to be
constant, with a positive value σ 2 that is usually unknown.
Because the variance σ 2 > 0, the observed value of the ith response yi will
typically not equal its expected value E(Y|X = xi). To account for this dif-
ference between the observed data and the expected value, statisticians have
invented a quantity called a statistical error, or ei, for case i deﬁned implicitly
by the equation yi = E(Y|X = xi) + ei or explicitly by ei = yi −E(Y|X = xi).
The errors ei depend on unknown parameters in the mean function and so are not
observable quantities. They are random variables and correspond to the vertical dis-
tance between the point yi and the mean function E(Y|X = xi). In the heights data,
page 2, the errors are the differences between the heights of particular daughters
and the average height of all daughters with mothers of a given ﬁxed height.
If the assumed mean function is incorrect, then the difference between the
observed data and the incorrect mean function will have a non random component,
as illustrated in Figure 2.2.
We make two important assumptions concerning the errors. First, we assume
that E(ei|xi) = 0, so if we could draw a scatterplot of the ei versus the xi, we
would have a null scatterplot, with no patterns. The second assumption is that the
errors are all independent, meaning that the value of the error for one case gives
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
19

20
SIMPLE LINEAR REGRESSION
3
3
2
2
1
1
1
b0 = intercept
X
Y
b1
FIG. 2.1
Equation of a straight line E(Y|X = x) = β0 + β1x.
Y
Fixed lack-of-fit error
True relationship
Straight line
X
FIG. 2.2
Approximating a curved mean function by straight line cases adds a ﬁxed component to the
errors.
no information about the value of the error for another case. This is likely to be
true in the examples in Chapter 1, although this assumption will not hold in all
problems.
Errors are often assumed to be normally distributed, but normality is much
stronger than we need. In this book, the normality assumption is used primarily
to obtain tests and conﬁdence statements with small samples. If the errors are
thought to follow some different distribution, such as the Poisson or the Binomial,

ORDINARY LEAST SQUARES ESTIMATION
21
other methods besides ols may be more appropriate; we return to this topic in
Chapter 12.
2.1
ORDINARY LEAST SQUARES ESTIMATION
Many methods have been suggested for obtaining estimates of parameters in a
model. The method discussed here is called ordinary least squares, or ols, in
which parameter estimates are chosen to minimize a quantity called the residual
sum of squares. A formal development of the least squares estimates is given in
Appendix A.3.
Parameters are unknown quantities that characterize a model. Estimates of
parameters are computable functions of data and are therefore statistics. To keep
this distinction clear, parameters are denoted by Greek letters like α, β, γ and σ,
and estimates of parameters are denoted by putting a “hat” over the corresponding
Greek letter. For example, ˆβ1, read “beta one hat,” is the estimator of β1, and ˆσ 2 is
the estimator of σ 2. The ﬁtted value for case i is given by E(Y|X = xi), for which
we use the shorthand notation ˆyi,
ˆyi = E(Y|X = xi) = ˆβ0 + ˆβ1xi
(2.2)
Although the ei are not parameters in the usual sense, we shall use the same hat
notation to specify the residuals: the residual for the ith case, denoted ˆei, is given
by the equation
ˆei = yi −E(Y|X = xi) = yi −ˆyi = yi −( ˆβ0 + ˆβ1)
i = 1, . . . , n
(2.3)
which should be compared with the equation for the statistical errors,
ei = yi −(β0 + β1xi)
i = 1, . . . , n
All least squares computations for simple regression depend only on aver-
ages, sums of squares and sums of cross-products. Deﬁnitions of the quantities
used are given in Table 2.1. Sums of squares and cross-products have been cen-
tered by subtracting the average from each of the values before squaring or tak-
ing cross-products. Appropriate alternative formulas for computing the corrected
sums of squares and cross products from uncorrected sums of squares and cross-
products that are often given in elementary textbooks are useful for mathematical
proofs, but they can be highly inaccurate when used on a computer and should be
avoided.
Table 2.1 also lists deﬁnitions for the usual univariate and bivariate summary
statistics, the sample averages (x, y), sample variances (SD2
x, SD2
y), and estimated
covariance and correlation (sxy, rxy). The “hat” rule described earlier would suggest
that different symbols should be used for these quantities; for example, ˆρxy might
be more appropriate for the sample correlation if the population correlation is ρxy.

22
SIMPLE LINEAR REGRESSION
TABLE 2.1
Deﬁnitions of Symbolsa
Quantity
Deﬁnition
Description
x
 xi/n
Sample average of x
y
 yi/n
Sample average of y
SXX
(xi −x)2 = (xi −x)xi
Sum of squares for the x’s
SD2x
SXX/(n −1)
Sample variance of the x’s
SDx
√SXX/(n −1)
Sample standard deviation of the x’s
SYY
(yi −y)2 = (yi −y)yi
Sum of squares for the y’s
SD2y
SYY/(n −1)
Sample variance of the y’s
SDy
√SYY/(n −1)
Sample standard deviation of the y’s
SXY
(xi −x)(yi −y) = (xi −x)yi
Sum of cross-products
sxy
SXY/(n −1)
Sample covariance
rxy
sxy/(SDxSDy)
Sample correlation
aIn each equation, the symbol  means to add over all the n values or pairs of values in the data.
This inconsistency is deliberate since in many regression situations, these statistics
are not estimates of population parameters.
To illustrate computations, we will use Forbes’ data, page 4, for which n = 17.
The data are given in Table 2.2. In our analysis of these data, the response will
be taken to be Lpres = 100 × log10(Pressure), and the predictor is Temp. We have
used the values for these variables shown in Table 2.2 to do the computations.
TABLE 2.2
Forbes’ 1857 Data on Boiling Point and Barometric Pressure for 17
Locations in the Alps and Scotland
Case Number
Temp (◦F)
Pressure (Inches Hg)
Lpres = 100 × log(Pressure)
1
194.5
20.79
131.79
2
194.3
20.79
131.79
3
197.9
22.40
135.02
4
198.4
22.67
135.55
5
199.4
23.15
136.46
6
199.9
23.35
136.83
7
200.9
23.89
137.82
8
201.1
23.99
138.00
9
201.4
24.02
138.06
10
201.3
24.01
138.04
11
203.6
25.14
140.04
12
204.6
26.57
142.44
13
209.5
28.49
145.47
14
208.6
27.76
144.34
15
210.7
29.04
146.30
16
211.9
29.88
147.54
17
212.2
30.06
147.80

LEAST SQUARES CRITERION
23
Neither multiplication by 100 nor the base of the logarithms has important effects
on the analysis. Multiplication by 100 avoids using scientiﬁc notation for numbers
we display in the text, and changing the base of the logarithms merely multiplies
the logarithms by a constant. For example, to convert from base-ten logarithms
to base-two logarithms, multiply by 3.321928. To convert natural logarithms to
base-two, multiply by 1.442695.
Forbes’ data were collected at 17 selected locations, so the sample variance
of boiling points, SD2
x = 33.17, is not an estimate of any meaningful population
variance. Similarly, rxy depends as much on the method of sampling as it does
on the population value ρxy, should such a population value make sense. In the
heights example, page 2, if the 1375 mother–daughter pairs can be viewed as a
sample from a population, then the sample correlation is an estimate of a population
correlation.
The usual sample statistics are often presented and used in place of the corrected
sums of squares and cross-products, so alternative formulas are given using both
sets of quantities.
2.2
LEAST SQUARES CRITERION
The criterion function for obtaining estimators is based on the residuals, which
geometrically are the vertical distances between the ﬁtted line and the actual y-
values, as illustrated in Figure 2.3. The residuals reﬂect the inherent asymmetry in
the roles of the response and the predictor in regression problems.
0
1
2
3
4
−1
0
1
2
3
4
5
Predictor = X
Response = Y
Residuals are the signed lengths of
the vertical lines
FIG. 2.3
A schematic plot for ols ﬁtting. Each data point is indicated by a small circle, and the solid
line is a candidate ols line given by a particular choice of slope and intercept. The solid vertical lines
between the points and the solid line are the residuals. Points below the line have negative residuals,
while points above the line have positive residuals.

24
SIMPLE LINEAR REGRESSION
The ols estimators are those values β0 and β1 that minimize the function1
RSS(β0, β1) =
n

i=1

yi −(β0 + β1xi)
2
(2.4)
When evaluated at ( ˆβ0, ˆβ1), we call the quantity RSS( ˆβ0, ˆβ1) the residual sum of
squares, or just RSS.
The least squares estimates can be derived in many ways, one of which is
outlined in Appendix A.3. They are given by the expressions
ˆβ1 = SXY
SXX = rxy
SDy
SDx
= rxy
 SYY
SXX
2
ˆβ0 = y −ˆβ1x
(2.5)
The several forms for ˆβ1 are all equivalent.
We emphasize again that ols produces estimates of parameters but not the actual
values of the parameters. The data in Figure 2.3 were created by setting the xi to be
random sample of 20 numbers from a N(2, 1.5) distribution and then computing
yi = 0.7 + 0.8xi + ei, where the errors were N(0, 1) random numbers. For this
graph, the true values of β0 = 0.7 and β1 = 0.8 are known. The graph of the true
mean function is shown in Figure 2.3 as a dashed line, and it seems to match
the data poorly compared to ols, given by the solid line. Since ols minimizes
(2.4), it will always ﬁt at least as well as, and generally better than, the true mean
function.
Using Forbes’ data, we will write x to be the sample mean of Temp and y to be
the sample mean of Lpres. The quantities needed for computing the least squares
estimators are
x = 202.95294
SXX = 530.78235
SXY = 475.31224
y = 139.60529
SYY = 427.79402
(2.6)
The quantity SYY, although not yet needed, is given for completeness. In the rare
instances that regression calculations are not done using statistical software or a
statistical calculator, intermediate calculations such as these should be done as
accurately as possible, and rounding should be done only to ﬁnal results. Using
(2.6), we ﬁnd
ˆβ1 = SXY
SXX = 0.895
ˆβ0 = y −ˆβ1x = −42.138
1We abuse notation by using the symbol for a ﬁxed though unknown quantity like βj as if it were a
variable argument. Thus, for example, RSS(β0, β1) is a function of two variables to be evaluated as its
arguments β0 and β1 vary. The same abuse of notation is used in the discussion of conﬁdence intervals.

ESTIMATING σ2
25
The estimated line, given by either of the equations
E(Lpres|Temp) = −42.138 + 0.895Temp
= 139.606 + 0.895(Temp −202.953)
was drawn in Figure 1.4a. The ﬁt of this line to the data is excellent.
2.3
ESTIMATING σ 2
Since the variance σ 2 is essentially the average squared size of the e2
i , we should
expect that its estimator ˆσ 2 is obtained by averaging the squared residuals. Under
the assumption that the errors are uncorrelated random variables with zero means
and common variance σ 2, an unbiased estimate of σ 2 is obtained by dividing
RSS =  ˆe2
i by its degrees of freedom (df), where residual df = number of cases
minus the number of parameters in the mean function. For simple regression,
residual df = n −2, so the estimate of σ 2 is given by
ˆσ 2 = RSS
n −2
(2.7)
This quantity is called the residual mean square. In general, any sum of squares
divided by its df is called a mean square. The residual sum of squares can be
computed by squaring the residuals and adding them up. It can also be computed
from the formula (Problem 2.9)
RSS = SYY −SXY 2
SXX = SYY −ˆβ2
1SXX
(2.8)
Using the summaries for Forbes’ data given at (2.6), we ﬁnd
RSS = 427.79402 −475.312242
530.78235
= 2.15493
(2.9)
σ 2 = 2.15493
17 −2 = 0.14366
(2.10)
The square root of ˆσ 2, ˆσ =
√
0.14366 = 0.37903 is often called the standard error
of regression. It is in the same units as is the response variable.
If in addition to the assumptions made previously, the ei are drawn from a
normal distribution, then the residual mean square will be distributed as a multiple
of a chi-squared random variable with df = n −2, or in symbols,
(n −2) ˆσ 2
σ 2 ∼χ2(n −2)

26
SIMPLE LINEAR REGRESSION
This is proved in more advanced books on linear models and is used to obtain the
distribution of test statistics and also to make conﬁdence statements concerning σ 2.
In particular, this fact implies that E(ˆσ 2) = σ 2, although normality is not required
for unbiasedness.
2.4
PROPERTIES OF LEAST SQUARES ESTIMATES
The ols estimates depend on data only through the statistics given in Table 2.1.
This is both an advantage, making computing easy, and a disadvantage, since any
two data sets for which these are identical give the same ﬁtted regression, even if
a straight-line model is appropriate for one but not the other, as we have seen in
Anscombe’s examples in Section 1.4. The estimates ˆβ0 and ˆβ1 can both be written
as linear combinations of y1, . . . , yn, for example, writing ci = (xi −x)/SXX (see
Appendix A.3)
ˆβ1 =
 xi −x
SXX

yi =

ciyi
The ﬁtted value at x = x is
E(Y|X = x) = y −ˆβ1x + ˆβ1x = y
so the ﬁtted line must pass through the point (x, y), intuitively the center of the
data. Finally, as long as the mean function includes an intercept,  ˆei = 0. Mean
functions without an intercept will usually have  ˆei ̸= 0.
Since the estimates ˆβ0 and ˆβ1 depend on the random eis, the estimates are also
random variables. If all the ei have zero mean and the mean function is correct,
then, as shown in Appendix A.4, the least squares estimates are unbiased,
E( ˆβ0) = β0
E( ˆβ1) = β1
The variance of the estimators, assuming Var(ei) = σ 2, i = 1, . . ., n, and
Cov(ei, ej) = 0, i ̸= j, are from Appendix A.4,
Var( ˆβ1) = σ 2 1
SXX
Var( ˆβ0) = σ 2

1
n + x2
SXX
	
(2.11)
The two estimates are correlated, with covariance
Cov( ˆβ0, ˆβ1) = −σ 2 x
SXX
(2.12)

ESTIMATED VARIANCES
27
The correlation between the estimates can be computed to be
ρ( ˆβ0, ˆβ1) =
−x

SXX/n + x2
=
−x

(n −1)SD2
x/n + x2
This correlation can be close to plus or minus one if SDx is small compared to |x|
and can be made to equal zero if the predictor is centered to have sample mean
zero.
The Gauss–Markov theorem provides an optimality result for ols estimates.
Among all estimates that are linear combinations of the ys and unbiased, the ols
estimates have the smallest variance. If one believes the assumptions and is inter-
ested in using linear unbiased estimates, the ols estimates are the ones to use.
When the errors are normally distributed, the ols estimates can be justiﬁed
using a completely different argument, since they are then also maximum likelihood
estimates, as discussed in any mathematical statistics text, for example, Casella and
Berger (1990).
Under the assumption that errors are independent, normal with constant variance,
which is written in symbols as
ei ∼NID(0, σ 2)
i = 1, . . . , n
ˆβ0 and ˆβ1 are also normally distributed, since they are linear functions of the yis
and hence of the ei, with variances and covariances given by (2.11) and (2.12).
These results are used to get conﬁdence intervals and tests. Normality of estimates
also holds without normality of errors if the sample size is large enough2.
2.5
ESTIMATED VARIANCES
Estimates of Var( ˆβ0) and Var( ˆβ1) are obtained by substituting ˆσ 2 for σ 2 in (2.11).
We use the symbol 
Var( ) for an estimated variance. Thus

Var( ˆβ1) = ˆσ 2 1
SXX

Var( ˆβ0) = ˆσ 2

1
n + x2
SXX
	
The square root of an estimated variance is called a standard error, for which we
use the symbol se( ). The use of this notation is illustrated by
se( ˆβ1) =


Var( ˆβ1)
2The main requirement for all estimates to be normally distributed in large samples is that
maxi

(xi −x)2/SXX

must get close to zero as the sample size increases (Huber, 1981).

28
SIMPLE LINEAR REGRESSION
2.6
COMPARING MODELS: THE ANALYSIS OF VARIANCE
The analysis of variance provides a convenient method of comparing the ﬁt of
two or more mean functions for the same set of data. The methodology developed
here is very useful in multiple regression and, with minor modiﬁcation, in most
regression problems.
An elementary alternative to the simple regression model suggests ﬁtting the
mean function
E(Y|X = x) = β0
(2.13)
The mean function (2.13) is the same for all values of X. Fitting with this mean
function is equivalent to ﬁnding the best line parallel to the horizontal or x-axis, as
shown in Figure 2.4. The ols estimate of the mean function is E
(Y|X) = ˆβ0, where
ˆβ0 is the value of β0 that minimizes (yi −β0)2. The minimizer is given by
ˆβ0 = y
(2.14)
The residual sum of squares is

(yi −ˆβ0)2 =

(yi −y)2 = SYY
(2.15)
This residual sum of squares has n −1 df, n cases minus one parameter in the
mean function.
Next, consider the simple regression mean function obtained from (2.13) by
adding a term that depends on X
E(Y|X = x) = β0 + β1x
(2.16)
Fitting this mean function is equivalent to ﬁnding the best line of arbitrary slope,
as shown in Figure 2.4. The ols estimates for this mean function are given by
0
1
2
3
4
−1
0
1
2
3
4
5
Predictor = X
Response = Y
Fit of (2.13)
Fit of (2.16)
FIG. 2.4
Two mean functions compared by the analysis of variance.

COMPARING MODELS: THE ANALYSIS OF VARIANCE
29
(2.5). The estimates of β0 under the two mean functions are different, just as the
meaning of β0 in the two mean functions is different. For (2.13), β0 is the average
of the yis, but for (2.16), β0 is the expected value of Y when X = 0.
For (2.16), the residual sum of squares, given in (2.8), is
RSS = SYY −(SXY)2
SXX
(2.17)
As mentioned earlier, RSS has n −2 df.
The difference between the sum of squares at (2.15) and that at (2.17) is the
reduction in residual sum of squares due to enlarging the mean function from (2.13)
to the simple regression mean function (2.16). This is the sum of squares due to
regression, SSreg, deﬁned by
SSreg = SYY −RSS
= SYY −

SYY −(SXY)2
SXX

= (SXY)2
SXX
(2.18)
The df associated with SSreg is the difference in df for mean function (2.13),
n −1, and the df for mean function (2.16), n −2, so the df for SSreg is (n −
1) −(n −2) = 1 for simple regression. These results are often summarized in an
analysis of variance table, abbreviated as ANOVA, given in Table 2.3. The column
marked “Source” refers to descriptive labels given to the sums of squares; in
more complicated tables, there may be many sources, and the labels given may be
different in some computer programs. The df column gives the number of degrees of
freedom associated with each named source. The next column gives the associated
sum of squares. The mean square column is computed from the sum of squares
column by dividing sums of squares by the corresponding df. The mean square on
the residual line is just ˆσ 2, as already discussed.
The analysis of variance for Forbes’ data is given in Table 2.4. Although this
table will be produced by any linear regression software program, the entries in
Table 2.4 can be constructed from the summary statistics given at (2.6).
The ANOVA is always computed relative to a speciﬁc larger mean function, here
given by (2.16), and a smaller mean function obtained from the larger by setting
TABLE 2.3
The Analysis of Variance Table for Simple Regression
Source
df
SS
MS
F
p-value
Regression
1
SSreg
SSreg/1
MSreg/ˆσ 2
Residual
n −2
RSS
ˆσ 2 = RSS/(n −2)
Total
n −1
SYY

30
SIMPLE LINEAR REGRESSION
TABLE 2.4
Analysis of Variance Table for Forbes’ Data
Source
df
SS
MS
F
p-value
Regression on Temp
1
425.639
425.639
2962.79
≈0
Residual
15
2.155
0.144
some parameters to zero, or occasionally setting them to some other known value.
For example, equation (2.13) was obtained from (2.16) by setting β1 = 0. The line
in the ANOVA table for the total gives the residual sum of squares corresponding
to the mean function with the fewest parameters. In the next chapter, the analysis
of variance is applied to a sequence of mean functions, but the reference to a ﬁxed
large mean function remains intact.
2.6.1
The F-Test for Regression
If the sum of squares for regression SSreg is large, then the simple regression
mean function E(Y|X = x) = β0 + β1x should be a signiﬁcant improvement over
the mean function given by (2.13), E(y|X = x) = β0. This is equivalent to saying
that the additional parameter in the simple regression mean function β1 is different
from zero or that E(Y|X = x) is not constant as X varies. To formalize this notion,
we need to be able to judge how large is “large.” This is done by comparing the
regression mean square, SSreg divided by its df, to the residual mean square ˆσ 2.
We call this ratio F:
F = (SYY −RSS)/1
ˆσ 2
= SSreg/1
ˆσ 2
(2.19)
F is just a rescaled version of SSreg = SYY −RSS, with larger values of SSreg
resulting in larger values of F. Formally, we can consider testing the null hypothesis
(NH) against the alternative hypothesis (AH)
NH:
E(Y|X = x) = β0
AH:
E(Y|X = x) = β0 + β1x
(2.20)
If the errors are NID(0, σ 2) or the sample size is large enough, then under NH
(2.19) will follow an F-distribution with df associated with the numerator and
denominator of (2.19), 1 and n −2 for simple regression. This is written F ∼
F(1, n −2). For Forbes’ data, we compute
F = 425.639
0.144
= 2963
We obtain a signiﬁcance level or p-value for this test by comparing F to the
percentage points of the F(1, n −2)-distribution. Most computer programs that ﬁt
regression models will include functions to computing percentage points of the F

THE COEFFICIENT OF DETERMINATION, R2
31
and other standard distributions and will include the p-value along with the ANOVA
table, as in Table 2.4. The p-value is shown as “approximately zero,” meaning that,
if the NH were true, the change of F exceeding its observed value is essentially
zero. This is very strong evidence against NH and in favor of AH.
2.6.2
Interpreting p-values
Under the appropriate assumptions, the p-value is the conditional probability of
observing a value of the computed statistic, here the value of F, as extreme or
more extreme, here as large or larger, than the observed value, given that the NH
is true. A small p-value provides evidence against the NH.
In some research areas, it has become traditional to adopt a ﬁxed signiﬁcance
level when examining p-values. For example, if a ﬁxed signiﬁcance level of α is
adopted, then we would say that an NH is rejected at level α if the p-value is
less than α. The most common choice for α is 0.05, which would mean that, were
the NH to be true, we would incorrectly ﬁnd evidence against it about 5% of the
time, or about 1 test in 20. Accept–reject rules like this are generally unnecessary
for reasonable scientiﬁc inquiry. Simply reporting p-values and allowing readers
to decide on signiﬁcance seems a better approach.
There is an important distinction between statistical signiﬁcance, the observation
of a sufﬁciently small p-value, and scientiﬁc signiﬁcance, observing an effect of
sufﬁcient magnitude to be meaningful. Judgment of the latter usually will require
examination of more than just the p-value.
2.6.3
Power of Tests
When the NH is true, and all assumptions are met, the chance of incorrectly declar-
ing an NH to be false at level α is just α. If α = 0.05, then in 5% of tests where
the NH is true we will get a p-value smaller than or equal to 0.05.
When the NH is false, we expect to see small p-values more often. The power
of a test is deﬁned to be the probability of detecting a false NH. For the hypothesis
test (2.20), when the NH is false, it is shown in more advanced books on linear
models (such as Seber, 1977) that the statistic F given by (2.19) has a noncentral
F distribution, with 1 and n −2 df, and with noncentrality parameter given by
SXXβ2
1/σ 2. The larger the value of the non centrality parameter, the greater the
power. The noncentrality is increased if β2
1 is large, if SXX is large, either by
spreading out the predictors or by increasing the sample size, or by decreasing σ 2.
2.7
THE COEFFICIENT OF DETERMINATION, R2
If both sides of (2.18) are divided by SYY, we get
SSreg
SYY = 1 −RSS
SYY
(2.21)
The left-hand side of (2.21) is the proportion of variability of the response explained
by regression on the predictor. The right-hand side consists of one minus the

32
SIMPLE LINEAR REGRESSION
remaining unexplained variability. This concept of dividing up the total variability
according to whether or not it is explained is of sufﬁcient importance that a special
name is given to it. We deﬁne R2, the coefﬁcient of determination, to be
R2 = SSreg
SYY = 1 −RSS
SYY
(2.22)
R2 is computed from quantities that are available in the ANOVA table. It is a scale-
free one-number summary of the strength of the relationship between the xi and
the yi in the data. It generalizes nicely to multiple regression, depends only on the
sums or squares and appears to be easy to interpret. For Forbes’ data,
R2 = SSreg
SYY = 425.63910
427.79402 = 0.995
and
thus
about
99.5%
of
the
variability
in
the
observed
values
or
100 × log(Pressure) is explained by boiling point. Since R2 does not depend on
units of measurement, we would get the same value if we had used logarithms with
a different base, or if we did not multiply log(Pressure) by 100.
By appealing to (2.22) and to Table 2.1, we can write
R2 = SSreg
SYY =
(SXY)2
SXX × SYY = r2
xy
and thus R2 is the same as the square of the sample correlation between the
predictor and the response.
2.8
CONFIDENCE INTERVALS AND TESTS
When the errors are NID(0, σ 2), parameter estimates, ﬁtted values, and predictions
will be normally distributed because all of these are linear combinations of the
yi and hence of the ei. Conﬁdence intervals and tests can be based on the t-
distribution, which is the appropriate distribution with normal estimates but using
an estimate of variance ˆσ 2. Suppose we let t(α/2, d) be the value that cuts off
α/2 × 100% in the upper tail of the t-distribution with d df. These values can be
computed in most statistical packages or spreadsheet software3.
2.8.1
The Intercept
The intercept is used to illustrate the general form of conﬁdence intervals for nor-
mally
distributed
estimates.
The
standard
error
of
the
intercept
is
se(β0) = ˆσ(1/n + x2/SXX)1/2. Hence a (1 −α) × 100% conﬁdence interval for
the intercept is the set of points β0 in the interval
ˆβ0 −t(α/2, n −2)se( ˆβ0) ≤β0 ≤ˆβ0 + t(α/2, n −2)se( ˆβ0)
3Such as the function tinv in Microsoft Excel, or the function pt in R or S-plus.

CONFIDENCE INTERVALS AND TESTS
33
For Forbes’ data, se( ˆβ0) = 0.37903(1/17 + (202.95294)2/530.78235)1/2 = 3.340.
For a 90% conﬁdence interval, t(0.05, 15) = 1.753, and the interval is
−42.138 −1.753(3.340) ≤β0 ≤−42.136 + 1.753(3.340)
−47.993 ≤β0 ≤−36.282
Ninety percent of such intervals will include the true value.
A hypothesis test of
NH:
β0 = β∗
0,
β1 arbitrary
AH:
β0 ̸= β∗
0,
β1 arbitrary
is obtained by computing the t-statistic
t =
ˆβ0 −β∗
0
se( ˆβ0)
(2.23)
and referring this ratio to the t-distribution with n −2 df. For example, in Forbes’
data, consider testing the NH β0 = −35 against the alternative that β0 ̸= −35. The
statistic is
t = −42.138 −(−35)
3.340
= 2.137
which has a p-value near 0.05, providing some evidence against NH. This hypoth-
esis test for these data is not one that would occur to most investigators and is used
only as an illustration.
2.8.2
Slope
The standard error of ˆβ1 is se( ˆβ1) = ˆσ/
√
SXX = 0.0164. A 95% conﬁdence inter-
val for the slope is the set of β1 such that
0.8955 −2.131(0.0164) ≤β1 ≤0.8955 + 2.131(0.0164)
0.867 ≤β1 ≤0.930
As an example of a test for slope equal to zero, consider the Ft. Collins snowfall
data presented on page 7. One can show, Problem 2.11, that the estimated slope is
ˆβ1 = 0.2035, se( ˆβ1) = 0.1310. The test of interest is of
NH:
β1 = 0
AH:
β1 ̸= 0
(2.24)
For the Ft. Collins data, t = (0.20335 −0)/0.1310 = 1.553. To get a signiﬁcance
level for this test, compare t with the t(91) distribution; the two-sided p-value is

34
SIMPLE LINEAR REGRESSION
0.124, suggesting no evidence against the NH that Early and Late season snowfalls
are independent.
Compare the hypothesis (2.24) with (2.20). Both appear to be identical. In fact,
t2 =

ˆβ1
se( ˆβ1)
	2
=
ˆβ2
1
ˆσ 2/SXX =
ˆβ2
1SXX
ˆσ 2
= F
so the square of a t statistic with d df is equivalent to an F-statistic with (1, d) df.
In nonlinear and logistic regression models discussed later in the book, the analog
of the t test will not be identical to the analog of the F test, and they can give
conﬂicting conclusions. For linear regression models, no conﬂict occurs and the
two tests are equivalent.
2.8.3
Prediction
The estimated mean function can be used to obtain values of the response for given
values of the predictor. The two important variants of this problem are prediction
and estimation of ﬁtted values. Since prediction is more important, we discuss it
ﬁrst.
In prediction we have a new case, possibly a future value, not one used to
estimate parameters, with observed value of the predictor x∗. We would like to
know the value y∗, the corresponding response, but it has not yet been observed.
We can use the estimated mean function to predict it. We assume that the data
used to estimate the mean function are relevant to the new case, so the ﬁtted
model applies to it. In the heights example, we would probably be willing to apply
the ﬁtted mean function to mother–daughter pairs alive in England at the end of
the nineteenth century. Whether the prediction would be reasonable for mother–
daughter pairs in other countries or in other time periods is much less clear. In
Forbes’ problem, we would probably be willing to apply the results for altitudes
in the range he studied. Given this additional assumption, a point prediction of y∗,
say ˜y∗, is just
˜y∗= ˆβ0 + ˆβ1x∗
˜y∗predicts the as yet unobserved y∗. The variability of this predictor has two
sources: the variation in the estimates ˆβ0 and ˆβ1, and the variation due to the
fact that y∗will not equal its expectation, since even if we knew the parameters
exactly, the future value of the response will not generally equal its expectation.
Using Appendix A.4,
Var( ˜y∗|x∗) = σ 2 + σ 2
1
n + (x∗−x)2
SXX

(2.25)
Taking square roots and estimating σ 2 by ˆσ 2, we get the standard error of prediction
(sepred) at x∗,
sepred( ˜y∗|x∗) = ˆσ

1 + 1
n + (x∗−x)2
SXX
1/2
(2.26)

CONFIDENCE INTERVALS AND TESTS
35
A prediction interval uses multipliers from the t-distribution. For prediction of
100 × log(Pressure) for a location with x∗= 200, the point prediction is ˜y∗=
−42.13778 + 0.89549(200) = 136.961, with standard error of prediction
sepred( ˜y∗|x∗= 200) = 0.37903

1 + 1
17 + (200 −202.95294)2
530.78235
1/2
= 0.393
Thus a 99% predictive interval is the set of all y∗such that
136.961 −2.95(0.393) ≤y∗≤136.961 + 2.95(0.393)
135.803 ≤y∗≤138.119
More interesting would be a 99% prediction interval for Pressure, rather than for
100 × log(Pressure). A point prediction is just 10(136.961/100) = 23.421 inches of
Mercury. The prediction interval is found by exponentiating the end points of the
interval in log scale. Dividing by 100 and then exponentiating, we get
10135.803/100 ≤Pressure ≤10138.119/100
22.805 ≤Pressure ≤24.054
In the original scale, the prediction interval is not symmetric about the point
estimate.
For the heights data, Figure 2.5 is a plot of the estimated mean function given
by the dashed line for the regression of Dheight on Mheight along with curves at
ˆβ0 + ˆβ1x∗± t(.025, 15)sepred(
˜
Dheight∗|Mheight∗)
The vertical distance between the two solid curves for any value of Mheight cor-
responds to a 95% prediction interval for daughter’s height given mother’s height.
Although not obvious from the graph because of the very large sample size, the
interval is wider for mothers who were either relatively tall or short, as the curves
bend outward from the narrowest point at Mheight = Mheight.
2.8.4
Fitted Values
In rare problems, one may be interested in obtaining an estimate of E(Y|X = x). In
the heights data, this is like asking for the population mean height of all daughters
of mothers with a particular height. This quantity is estimated by the ﬁtted value
ˆy = β0 + β1x, and its standard error is
seﬁt( ˜y∗|x∗) = ˆσ
1
n + (x∗−x)2
SXX
1/2

36
SIMPLE LINEAR REGRESSION
55
60
65
70
55
60
65
70
Mheight
Dheight
FIG. 2.5
Prediction intervals (solid lines) and intervals for ﬁtted values (dashed lines) for the heights
data.
To obtain conﬁdence intervals, it is more usual to compute a simultaneous interval
for all possible values of x. This is the same as ﬁrst computing a joint conﬁdence
region for β0 and β1, and from these, computing the set of all possible mean
functions with slope and intercept in the joint conﬁdence set (Section 5.5). The
conﬁdence region for the mean function is the set of all y such that
( ˆβ0 + ˆβ1x) −seﬁt( ˆy|x)[2F(α; 2, n −2)]1/2 ≤y
≤( ˆβ0 + ˆβ1x) + seﬁt( ˆy|x)[2F(α; 2, n −2)]1/2
For multiple regression, replace 2F(α; 2, n −2) by p′F(α; p′, n −p′), where p′
is the number of parameters estimated in the mean function including the intercept.
The simultaneous band for the ﬁtted line for the heights data is shown in Figure 2.5
as the vertical distances between the two dotted lines. The prediction intervals are
much wider than the conﬁdence intervals. Why is this so (Problem 2.4)?
2.9
THE RESIDUALS
Plots of residuals versus other quantities are used to ﬁnd failures of assumptions.
The most common plot, especially useful in simple regression, is the plot of resid-
uals versus the ﬁtted values. A null plot would indicate no failure of assumptions.
Curvature might indicate that the ﬁtted mean function is inappropriate. Residuals
that seem to increase or decrease in average magnitude with the ﬁtted values might
indicate nonconstant residual variance. A few relatively large residuals may be
indicative of outliers, cases for which the model is somehow inappropriate.
The plot of residuals versus ﬁtted values for the heights data is shown in
Figure 2.6. This is a null plot, as it indicates no particular problems.

THE RESIDUALS
37
60
62
64
66
68
−5
0
5
Fitted values
Residuals
FIG. 2.6
Residuals versus ﬁtted values for the heights data.
135
140
145
0.0
0.5
1.0
Fitted values
Residuals
12
FIG. 2.7
Residual plot for Forbes’ data.
The ﬁtted values and residuals for Forbes’ data are plotted in Figure 2.7. The
residuals are generally small compared to the ﬁtted values, and they do not fol-
low any distinct pattern in Figure 2.7. The residual for case number 12 is about
four times the size of the next largest residual in absolute value. This may sug-
gest that the assumptions concerning the errors are not correct. Either Var(100 ×
log(Pressure)|Temp) may not be constant or for case 12, the corresponding error
may have a large ﬁxed component. Forbes may have misread or miscopied the
results of his calculations for this case, which would suggest that the numbers in

38
SIMPLE LINEAR REGRESSION
TABLE 2.5
Summary Statistics for Forbes’
Data with All Data and with Case 12 Deleted
Quantity
All Data
Delete Case 12
ˆβ0
−42.138
−41.308
ˆβ1
0.895
0.891
se( ˆβ0)
3.340
1.001
se( ˆβ1)
0.016
0.005
ˆσ
0.379
0.113
R2
0.995
1.000
the data do not correspond to the actual measurements. Forbes noted this possi-
bility himself, by marking this pair of numbers in his paper as being “evidently a
mistake”, presumably because of the large observed residual.
Since we are concerned with the effects of case 12, we could reﬁt the data, this
time without case 12, and then examine the changes that occur in the estimates
of parameters, ﬁtted values, residual variance, and so on. This is summarized in
Table 2.5, giving estimates of parameters, their standard errors, ˆσ 2, and the coef-
ﬁcient of determination R2 with and without case 12. The estimates of parameters
are essentially identical with and without case 12. In other regression problems,
deletion of a single case can change everything. The effect of case 12 on standard
errors is more marked: if case 12 is deleted, standard errors are decreased by a
factor of about 3.1, and variances are decreased by a factor of about 3.12 ≈10.
Inclusion of this case gives the appearance of less reliable results than would be
suggested on the basis of the other 16 cases. In particular, prediction intervals of
Pressure are much wider based on all the data than on the 16-case data, although
the point predictions are nearly the same. The residual plot obtained when case
12 is deleted before computing indicates no obvious failures in the remaining 16
cases.
Two competing ﬁts using the same mean function but somewhat different data
are available, and they lead to slightly different conclusions, although the results of
the two analyses agree more than they disagree. On the basis of the data, there is
no real way to choose between the two, and we have no way of deciding which is
the correct ols analysis of the data. A good approach to this problem is to describe
both or, in general, all plausible alternatives.
PROBLEMS
2.1. Height and weight data The table below and in the data ﬁle htwt.txt
gives Ht = height in centimeters and Wt = weight in kilograms for a sample
of n = 10 18-year-old girls. The data are taken from a larger study described
in Problem 3.1. Interest is in predicting weight from height.

PROBLEMS
39
Ht
Wt
169.6
71.2
166.8
58.2
157.1
56.0
181.1
64.5
158.4
53.0
165.6
52.4
166.7
56.8
156.5
49.2
168.1
55.6
165.3
77.8
2.1.1. Draw a scatterplot of Wt on the vertical axis versus Ht on the horizontal
axis. On the basis of this plot, does a simple linear regression model
make sense for these data? Why or why not?
2.1.2. Show that x = 165.52, y = 59.47, SXX = 472.076, SYY = 731.961,
and SXY = 274.786. Compute estimates of the slope and the intercept
for the regression of Y on X. Draw the ﬁtted line on your scatterplot.
2.1.3. Obtain the estimate of σ 2 and ﬁnd the estimated standard errors of
ˆβ0 and ˆβ1. Also ﬁnd the estimated covariance between ˆβ0 and ˆβ1.
Compute the t-tests for the hypotheses that β0 = 0 and that β1 = 0
and ﬁnd the appropriate p-values using two-sided tests.
2.1.4. Obtain the analysis of variance table and F-test for regression. Show
numerically that F = t2, where t was computed in Problem 2.1.3 for
testing β1 = 0.
2.2. More with Forbes’ data An alternative approach to the analysis of Forbes’
experiments comes from the Clausius–Clapeyron formula of classical ther-
modynamics, which dates to Clausius (1850). According to this theory, we
should ﬁnd that
E(Lpres|Temp) = β0 + β1
1
Ktemp
(2.27)
where Ktemp is temperature in degrees Kelvin, which equals 255.37 plus
(5/9) × Temp. If we were to graph this mean function on a plot of Lpres
versus Ktemp, we would get a curve, not a straight line. However, we can
estimate the parameters β0 and β1 using simple linear regression methods by
deﬁning u1 to be the inverse of temperature in degrees Kelvin,
u1 =
1
Ktemp =
1
(5/9)Temp + 255.37

40
SIMPLE LINEAR REGRESSION
Then the mean function (2.27) can be rewritten as
E(Lpres|Temp) = β0 + β1u1
(2.28)
for which simple linear regression is suitable. The notation we have used
in (2.28) is a little different, as the left side of the equation says we are
conditioning on Temp, but the variable Temp does not appear explicitly on
the right side of the equation.
2.2.1. Draw the plot of Lpres versus u1, and verify that apart from case 12
the 17 points in Forbes’ data fall close to a straight line.
2.2.2. Compute the linear regression implied by (2.28), and summarize your
results.
2.2.3. We now have two possible models for the same data based on the
regression of Lpres on Temp used by Forbes, and (2.28) based on the
Clausius–Clapeyron formula. To compare these two, draw the plot of
the ﬁtted values from Forbes’ mean function ﬁt versus the ﬁtted values
from (2.28). On the basis of these and any other computations you
think might help, is it possible to prefer one approach over the other?
Why?
2.2.4. In his original paper, Forbes provided additional data collected by
the botanist Dr. Joseph Hooker on temperatures and boiling points
measured often at higher altitudes in the Himalaya Mountains. The
data for n = 31 locations is given in the ﬁle hooker.txt. Find the
estimated mean function (2.28) for Hooker’s data.
2.2.5. This problem is not recommended unless you have access to a pack-
age with a programming language, like R, S-plus, Mathematica, or
SAS IML. For each of the cases in Hooker’s data, compute the pre-
dicted values ˆy and the standard error of prediction. Then compute
z = (Lpres −ˆy)/sepred. Each of the zs is a random variable, but if
the model is correct, each has mean zero and standard deviation close
to one. Compute the sample mean and standard deviation of the zs,
and summarize results.
2.2.6. Repeat Problem 2.2.5, but this time predict and compute the z-scores
for the 17 cases in Forbes data, again using the ﬁtted mean func-
tion from Hooker’s data. If the mean function for Hooker’s data
applies to Forbes’ data, then each of the z-scores should have zero
mean and standard deviation close to one. Compute the z-scores,
compare them to those in the last problem and comment on the
results.
2.3. Deviations from the mean Sometimes it is convenient to write the simple
linear regression model in a different form that is a little easier to manipu-
late. Taking equation (2.1), and adding β1x −β1x, which equals zero, to the

PROBLEMS
41
right-hand side, and combining terms, we can write
yi = β0 + β1x + β1xi −β1x + ei
= (β0 + β1x) + β1(xi −x) + ei
= α + β1(xi −x) + ei
(2.29)
where we have deﬁned α = β0 + β1x. This is called the deviations from the
sample average form for simple regression.
2.3.1. What is the meaning of the parameter α?
2.3.2. Show that the least squares estimates are
ˆα = y,
ˆβ1 as given by (2.5)
2.3.3. Find expressions for the variances of the estimates and the covariance
between them.
2.4. Heights of mothers and daughters
2.4.1. For the heights data in the ﬁle heights.txt, compute the regres-
sion of Dheight on Mheight, and report the estimates, their standard
errors, the value of the coefﬁcient of determination, and the esti-
mate of variance. Give the analysis of variance table that tests the
hypothesis that E(Dheight|Mheight) = β0 versus the alternative that
E(Dheight|Mheight) = β0 + β1Mheight, and write a sentence or two
that summarizes the results of these computations.
2.4.2. Write the mean function in the deviations from the mean form as in
Problem 2.3. For this particular problem, give an interpretation for the
value of β1. In particular, discuss the three cases of β1 = 1, β1 < 1
and β1 > 1. Obtain a 99% conﬁdence interval for β1 from the data.
2.4.3. Obtain a prediction and 99% prediction interval for a daughter whose
mother is 64 inches tall.
2.5. Smallmouth bass
2.5.1. Using the West Bearskin Lake smallmouth bass data in the ﬁle
wblake.txt, obtain 95% intervals for the mean length at ages 2, 4
and 6 years.
2.5.2. Obtain a 95% interval for the mean length at age 9. Explain why this
interval is likely to be untrustworthy.
2.5.3. The ﬁle wblake2.txt contains all the data for ages one to eight
and, in addition, includes a few older ﬁshes. Using the methods we
have learned in this chapter, show that the simple linear regression
model is not appropriate for this larger data set.
2.6. United Nations data Refer to the UN data in Problem 1.3, page 18.

42
SIMPLE LINEAR REGRESSION
2.6.1. Using base-ten logarithms, use a software package to compute the
simple linear regression model corresponding to the graph in Prob-
lem 1.3.3, and get the analysis of variance table.
2.6.2. Draw the summary graph, and add the ﬁtted line to the graph.
2.6.3. Test the hypothesis that the slope is zero versus the alternative that it
is negative (a one-sided test). Give the signiﬁcance level of the test
and a sentence that summarizes the result.
2.6.4. Give the value of the coefﬁcient of determination, and explain its
meaning.
2.6.5. Increasing log(PPgdp) by one unit is the same as multiplying PPgdp
by ten. If two localities differ in PPgdp by a factor of ten, give a 95%
conﬁdence interval on the difference in log(Fertility) for these two
localities.
2.6.6. For a locality not in the data with PPgdp = 1000, obtain a point pre-
diction and a 95% prediction interval for log(Fertility). If the interval
(a, b) is a 95% prediction interval for log(Fertility), then a 95% pre-
diction interval for Fertility is given by (10a, 10b). Use this result to
get a 95% prediction interval for Fertility.
2.6.7. Identify (1) the locality with the highest value of Fertility; (2) the
locality with the lowest value of Fertility; and (3) the two localities
with the largest positive residuals from the regression when both vari-
ables are in log scale, and the two countries with the largest negative
residuals in log scales.
2.7. Regression through the origin Occasionally, a mean function in which the
intercept is known a priori to be zero may be ﬁt. This mean function is
given by
E(y|x) = β1x
(2.30)
The residual sum of squares for this model, assuming the errors are indepen-
dent with common variance σ 2, is RSS = (yi −ˆβ1xi)2.
2.7.1. Show that the least squares estimate of β1 is ˆβ1 =  xiyi/  x2
i .
Show that ˆβ1 is unbiased and that Var( ˆβ1) = σ 2/  x2
i . Find an
expression for ˆσ 2. How many df does it have?
2.7.2. Derive the analysis of variance table with the larger model given by
(2.16), but with the smaller model speciﬁed in (2.30). Show that the
F-test derived from this table is numerically equivalent to the square
of the t-test (2.23) with β∗
0 = 0.
2.7.3. The data in Table 2.6 and in the ﬁle snake.txt give X = water
content of snow on April 1 and Y = water yield from April to July
in inches in the Snake River watershed in Wyoming for n = 17 years
from 1919 to 1935 (from Wilm, 1950).

PROBLEMS
43
TABLE 2.6
Snake River Data for Problem 2.7
X
Y
X
Y
23.1
10.5
32.8
16.7
31.8
18.2
32.0
17.0
30.4
16.3
24.0
10.5
39.5
23.1
24.2
12.4
52.5
24.9
37.9
22.8
30.5
14.1
25.1
12.9
12.4
8.8
35.1
17.4
31.5
14.9
21.1
10.5
27.6
16.1
Fit a regression through the origin and ﬁnd ˆβ1 and σ 2. Obtain a
95% conﬁdence interval for β1. Test the hypothesis that the intercept
is zero.
2.7.4. Plot the residuals versus the ﬁtted values and comment on the ade-
quacy of the mean function with zero intercept. In regression through
the origin,  ˆei ̸= 0.
2.8. Scale invariance
2.8.1. In the simple regression model (2.1), suppose the value of the predictor
X is replaced by cX, where c is some non zero constant. How are ˆβ0,
ˆβ1, ˆσ 2, R2, and the t-test of NH: β1 = 0 affected by this change?
2.8.2. Suppose each value of the response Y is replaced by dY, for some
d ̸= 0. Repeat 2.8.1.
2.9. Using Appendix A.3, verify equation (2.8).
2.10. Zipf’s law Suppose we counted the number of times each word was used in
the written works by Shakespeare, Alexander Hamilton, or some other author
with a substantial written record (Table 2.7). Can we say anything about the
frequencies of the most common words?
Suppose we let fi be the rate per 1000 words of text for the ith most
frequent word used. The linguist George Zipf (1902–1950) observed a law
like relationship between rate and rank (Zipf, 1949),
E(fi|i) = a/ib
and further observed that the exponent is close to b = 1. Taking logarithms
of both sides, we get approximately
E(log(fi)| log(i)) = log(a) −b log(i)
(2.31)

44
SIMPLE LINEAR REGRESSION
TABLE 2.7
The Word Count Data
Word
The word
Hamilton
Rate per 1000 words of this word in the writings of Alexander Hamilton
HamiltonRank
Rank of this word in Hamilton’s writings
Madison
Rate per 1000 words of this word in the writings of James Madison
MadisonRank
Rank of this word in Madison’s writings
Jay
Rate per 1000 words of this word in the writings of John Jay
JayRank
Rank of this word in Jay’s writings
Ulysses
Rate per 1000 words of this word in Ulysses by James Joyce
UlyssesRank
Rank of this word in Ulysses
Zipf’s law has been applied to frequencies of many other classes of objects
besides words, such as the frequency of visits to web pages on the internet
and the frequencies of species of insects in an ecosystem.
The data in MWwords.txt give the frequencies of words in works from
four different sources: the political writings of eighteenth-century American
political ﬁgures Alexander Hamilton, James Madison, and John Jay, and the
book Ulysses by twentieth-century Irish writer James Joyce. The data are
from Mosteller and Wallace (1964, Table 8.1-1), and give the frequencies of
165 very common words. Several missing values occur in the data; these are
really words that were used so infrequently that their count was not reported
in Mosteller and Wallace’s table.
2.10.1. Using only the 50 most frequent words in Hamilton’s work (that is,
using only rows in the data for which HamiltonRank ≤50), draw the
appropriate summary graph, estimate the mean function (2.31), and
summarize your results.
2.10.2. Test the hypothesis that b = 1 against the two-sided alternative and
summarize.
2.10.3. Repeat Problem 2.10.1, but for words with rank of 75 or less, and
with rank less than 100. For larger number of words, Zipf’s law may
break down. Does that seem to happen with these data?
2.11. For the Ft. Collins snow fall data discussed in Example 1.1, test the hypoth-
esis that the slope is zero versus the alternative that it is not zero. Show that
the t-test of this hypothesis is the same as the F-test; that is, t2 = F.
2.12. Old Faithful Use the data from Problem 1.4, page 18.
2.12.1. Use simple linear regression methodology to obtain a prediction
equation for interval from duration. Summarize your results in a
way that might be useful for the nontechnical personnel who staff
the Old Faithful Visitor’s Center.
2.12.2. Construct a 95% conﬁdence interval for
E(interval|duration = 250)

PROBLEMS
45
2.12.3. An individual has just arrived at the end of an eruption that lasted 250
seconds. Give a 95% conﬁdence interval for the time the individual
will have to wait for the next eruption.
2.12.4. Estimate the 0.90 quantile of the conditional distribution of
interval|(duration = 250)
assuming that the population is normally distributed.
2.13. Windmills Energy can be produced from wind using windmills. Choosing
a site for a wind farm, the location of the windmills, can be a multimillion
dollar gamble. If wind is inadequate at the site, then the energy produced
over the lifetime of the wind farm can be much less than the cost of building
and operation. Prediction of long-term wind speed at a candidate site can be
an important component in the decision to build or not to build. Since energy
produced varies as the square of the wind speed, even small errors can have
serious consequences.
The data in the ﬁle wm1.txt provides measurements that can be used
to help in the prediction process. Data were collected every six hours for
the year 2002, except that the month of May 2002 is missing. The values
Cspd are the calculated wind speeds in meters per second at a candidate
site for building a wind farm. These values were collected at tower erected
on the site. The values RSpd are wind speeds at a reference site, which is
a nearby location for which wind speeds have been recorded over a very
long time period. Airports sometimes serve as reference sites, but in this
case, the reference data comes from the National Center for Environmental
Modeling; these data are described at http://dss.ucar.edu/datasets/ds090.0/.
The reference is about 50 km south west of the candidate site. Both sites
are in the northern part of South Dakota. The data were provided by Mark
Ahlstrom and Rolf Miller of WindLogics.
2.13.1. Draw the scatterplot of the response CSpd versus the predictor RSpd.
Is the simple linear regression model plausible for these data?
2.13.2. Fit the simple regression of the response on the predictor, and present
the appropriate regression summaries.
2.13.3. Obtain a 95% prediction interval for CSpd at a time when RSpd =
7.4285.
2.13.4. For this problem, we revert to generic notation and let x = CSpd and
y = CSpd and let n be the number of cases used in the regression
(n = 1116 in the data we have used in this problem) and x and
SXX deﬁned from these n observations. Suppose we want to make
predictions at m time points with values of wind speed x∗1, .., x∗m
that are different from the n cases used in constructing the prediction
equation. Show that (1) the average of the m predictions is equal to
the prediction taken at the average value x∗of the m values of the

46
SIMPLE LINEAR REGRESSION
predictor, and (2) using the ﬁrst result, the standard error of the
average of m predictions is
se of average prediction =

ˆσ 2
m + ˆσ 2
1
n + (x∗−x)2
SXX

(2.32)
If m is very large, then the ﬁrst term in the square root is negligible,
and the standard error of average prediction is essentially the same
as the standard error of a ﬁtted value at x∗.
2.13.5. For the period from January 1, 1948 to July 31, 2003, a total of
m = 62039 wind speed measurements are available at the reference
site, excluding the data from the year 2002. For these measurements,
the average wind speed was x∗= 7.4285. Give a 95% prediction
interval on the long-term average wind speed at the candidate site.
This long-term average of the past is then taken as an estimate of
the long-term average of the future and can be used to help decide
if the candidate is a suitable site for a wind farm.

C H A P T E R 3
Multiple Regression
Multiple linear regression generalizes the simple linear regression model by
allowing for many terms in a mean function rather than just one intercept and
one slope.
3.1
ADDING A TERM TO A SIMPLE LINEAR REGRESSION MODEL
We start with a response Y and the simple linear regression mean function
E(Y|X1 = x1) = β0 + β1x1
Now suppose we have a second variable X2 with which to predict the response.
By adding X2 to the problem, we will get a mean function that depends on both
the value of X1 and the value of X2,
E(Y|X1 = x1, X2 = x2) = β0 + β1x1 + β2x2
(3.1)
The main idea in adding X2 is to explain the part of Y that has not already been
explained by X1.
United Nations Data
We will reconsider the United Nations data discussed in Problem 1.3. To the regres-
sion of log(Fertility), the base-two log fertility rate on log(PPgdp), the base-two
log of the per person gross domestic product, we consider adding Purban, the per-
centage of the population that lives in an urban area. The data in the ﬁle UN2.txt
give values for these three variables, as well as the name of the Locality for 193
localities, mostly countries, for which the United Nations provides data.
Figure 3.1 presents several graphical views of these data. Figure 3.1a can
be viewed as a summary graph for the simple regression of log(Fertility) on
log(PPgdp). The ﬁtted mean function using ols is
E(log(Fertility)| log(PPgdp)) = 2.703 −0.153 log(PPgdp)
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
47

48
MULTIPLE REGRESSION
8
10
12
14
0.0
0.5
1.0
1.5
2.0
log(PPgdp)
log(Fertility)
20
40
60
80
100
0.0
0.5
1.0
1.5
2.0
Percent urban
(a)
(b)
log(Fertility)
8
10
12
14
20
40
60
80
100
log(PPgdp)
Percent urban
−60
−20
−40
0
20
40
−1.0
−0.5
0.0
0.5
ê from Purban on log(PPgdp)
(c)
(d)
ê from log(Fertility) on log(PPgdp)
FIG. 3.1
United Nations data on 193 localities, mostly nations. (a) log(Fertility) versus log(PPgdp);
(b) log(Fertility) versus Purban; (c) Purban versus log(PPgdp); (d) Added-variable plot for Purban
after log(PPgdp).
with R2 = 0.459, so about 46% of the variability in log(Fertility) is explained
by log(PPgdp). An increase of one unit in log(PPgdp), which corresponds to a
doubling of PPgdp, is estimated to decrease log(Fertility) by 0.153 units.
Similarly, Figure 3.1b is the summary graph for the regression of log(Fertility)
on Purban. This simple regression has ﬁtted mean function
E(log(Fertility)|Purban) = 1.750 −0.013 Purban
with R2 = 0.348, so Purban explains about 35% of the variability in log(Fertility).
An increase of one percent urban implies a change on the average in log(Fertility)
of −0.13.
To get a summary graph of the regression of log(Fertility) on both log(PPgdp)
and Purban would require a three-dimensional plot of these three variables, with
log(PPgdp) on one of the horizontal axes, Purban on the other horizontal axis,
and log(Fertility) on the vertical axis. Although such plots are possible by using

ADDING A TERM TO A SIMPLE LINEAR REGRESSION MODEL
49
either perspective or motion to display the third dimension, using them is much
more difﬁcult than using two-dimensional graphics, and their successful use is
not widespread. Cook and Weisberg (1999a) discuss using motion to understand
three-dimensional graphics for regression.
As a partial substitute for looking at the full three-dimensional plot, we add
a third plot to the ﬁrst two in Figure 3.1, namely, the plot of Purban versus
log(PPgdp) shown in Figure 3.1c. This graph does not include the response, so it
only shows the relationship between the two potential predictors. In this problem,
these two variables are positively correlated, and the mean function for Figure 3.1c
seems to be well approximated by a straight line.
The inference to draw from Figure 3.1c is that to the extent that Purban can
be predicted by log(PPgdp), these two potential predictors are measuring the same
thing, and so the role of these two variables in predicting log(Fertility) will be
overlapping, and they will both, to some extent, be explaining the same variability.
3.1.1
Explaining Variability
Given these graphs, what can be said about the proportion of variability in
log(Fertility) explained by log(PPgdp) and Purban? We can say that the total
explained variation must exceed 46 percent, the larger of the two values explained
by each variable separately, since using both log(PPgdp) and Purban must surely
be at least as informative as using just one of them. The total variation will be
additive, 46% + 35% = 91%, only if the two variables are completely unrelated
and measure different things. The total can be less than the sum if the terms are
related and are at least in part explaining the same variation. Finally, the total can
exceed the sum if the two variables act jointly so that knowing both gives more
information than knowing just one of them. For example, the area of a rectangle
may be only poorly determined by either the length or width alone, but if both are
considered at the same time, area can be determined exactly. It is precisely this
inability to predict the joint relationship from the marginal relationships that makes
multiple regression rich and complicated.
3.1.2
Added-Variable Plots
The unique effect of adding Purban to a mean function that already includes
log(PPgdp) is determined by the relationship between the part of log(Fertility)
that is not explained by log(PPgdp) and the part of Purban that is not explained
by log(PPgdp). The “unexplained parts” are just the residuals from these two
simple regressions, and so we need to examine the scatterplot of the residuals from
the regression of log(Fertility) on log(PPgdp) versus the residuals from the regres-
sion of Purban on log(PPgdp). This plot is shown in Figure 3.1d. Figure 3.1b is
the summary graph for the relationship between log(Fertility) and Purban ignor-
ing log(PPgdp), while Figure 3.1d shows this relationship, but after adjusting for
log(PPgdp). If Figure 3.1d shows a stronger relationship than does Figure 3.1b,
meaning that the points in the plot show less variation about the ﬁtted straight line,

50
MULTIPLE REGRESSION
then the two variables act jointly to explain extra variation, while if the relationship
is weaker, or the plot exhibits more variation, then the total explained variability
is less than the additive amount. The latter seems to be the case here.
If we ﬁt the simple regression mean function to Figure3.1d, the ﬁtted line has
zero intercept, since the averages of the two plotted variables are zero, and the
estimated slope via ols is ˆβ2 = −0.0035 ≈−0.004. It turns out that this is exactly
the estimate ˆβ2 that would be obtained using ols to get the estimates using the
mean function (3.1). Figure 3.1d is called an added-variable plot.
We now have two estimates of the coefﬁcient β2 for Purban:
ˆβ2 = −0.013 ignoring log(PPgdp)
ˆβ2 = −0.004 adjusting for log(PPgdp)
While both of these indicate that more urbanization is associated with lower fer-
tility, adjusting for log(PPgdp) suggests that the magnitude of this effect is only
about one-fourth as large as one might think if log(PPgdp) were ignored. In other
problems, slope estimates for the same term but from different mean functions
can be even more wildly different, changing signs, magnitude, and signiﬁcance.
This naturally complicates the interpretation of ﬁtted models, and also comparing
between studies ﬁt with even slightly different mean functions.
To get the coefﬁcient estimate for log(PPgdp) in the regression of log(Fertility)
on both predictors, we would use the same procedure we used for Purban and
consider the problem of adding log(PPgdp) to a mean function that already includes
Purban. This would require looking at the graph of the residuals from the regression
of log(Fertility) on Purban versus the residuals from the regression of log(PPgdp)
on Purban (see Problem 3.2).
3.2
THE MULTIPLE LINEAR REGRESSION MODEL
The general multiple linear regression model with response Y
and terms
X1, . . . , Xp will have the form
E(Y|X) = β0 + β1X1 + · · · + βpXp
(3.2)
The symbol X in E(Y|X) means that we are conditioning on all the terms on the
right side of the equation. Similarly, when we are conditioning on speciﬁc values
for the predictors x1, . . . , xp that we will collectively call x, we write
E(Y|X = x) = β0 + β1x1 + · · · + βpxp
(3.3)
As in Chapter 2, the βs are unknown parameters we need to estimate. Equation
(3.2) is a linear function of the parameters, which is why this is called linear
regression. When p = 1, X has only one element, and we get the simple regression
problem discussed in Chapter 2. When p = 2, the mean function (3.2) corresponds

TERMS AND PREDICTORS
51
Y
X2
X1
b1
1
1
b0
b2
FIG. 3.2
A linear regression surface with p = 2 predictors.
to a plane in three dimensions, as shown in Figure 3.2. When p > 2, the ﬁtted
mean function is a hyperplane, the generalization of a p-dimensional plane in a
(p + 1)-dimensional space. We cannot draw a general p-dimensional plane in our
three-dimensional world.
3.3
TERMS AND PREDICTORS
Regression problems start with a collection of potential predictors. Some of these
may be continuous measurements, like the height or weight of an object. Some
may be discrete but ordered, like a doctor’s rating of overall health of a patient on
a nine-point scale. Other potential predictors can be categorical, like eye color or
an indicator of whether a particular unit received a treatment. All these types of
potential predictors can be useful in multiple linear regression.
From the pool of potential predictors, we create a set of terms that are the
X-variables that appear in (3.2). The terms might include:
The intercept The mean function (3.2) can we rewritten as
E(Y|X) = β0X0 + β1X1 + · · · + βpXp
where X0 is a term that is always equal to one. Mean functions without an
intercept would not have this term included.
Predictors The simplest type of term is equal to one of the predictors, for
example, the variable Mheight in the heights data.
Transformations of predictors Sometimes the original predictors need to be
transformed in some way to make (3.2) hold to a reasonable approxima-
tion. This was the case with the UN data just discussed, in which PPgdp was

52
MULTIPLE REGRESSION
used in log scale. The willingness to replace predictors by transformations of
them greatly expands the range of problems that can be summarized with a
linear regression model.
Polynomials Problems
with
curved
mean
functions
can
sometimes
be
accommodated in the multiple linear regression model by including poly-
nomial terms in the predictor variables. For example, we might include as
terms both a predictor X1 and its square X2
1 to ﬁt a quadratic polynomial
in that predictor. Complex polynomial surfaces in several predictors can be
useful in some problems1.
Interactions and other combinations of predictors Combining several predictors
is often useful. An example of this is using body mass index, given by height
divided by weight squared, in place of both height and weight, or using a total
test score in place of the separate scores from each of several parts. Products
of predictors called interactions are often included in a mean function along
with the original predictors to allow for joint effects of two or more variables.
Dummy variables and factors A categorical predictor with two or more levels
is called a factor. Factors are included in multiple linear regression using
dummy variables, which are typically terms that have only two values, often
zero and one, indicating which category is present for a particular observation.
We will see in Chapter 6 that a categorical predictor with two categories can
be represented by one dummy variable, while a categorical predictor with
many categories can require several dummy variables.
A regression with say k predictors may combine to give fewer than k terms or
expand to require more than k terms. The distinction between predictors and terms
can be very helpful in thinking about an appropriate mean function to use in a
particular problem, and in using graphs to understand a problem. For example, a
regression with one predictor can always be studied using the 2D scatterplot of the
response versus the predictor, regardless of the number of terms required in the
mean function.
We will use the fuel consumption data introduced in Section 1.6 as the primary
example for the rest of this chapter. As discussed earlier, the goal is to understand
how fuel consumption varies as a function of state characteristics. The variables
are deﬁned in Table 1.2 and are given in the ﬁle fuel2001.txt. From the six
initial predictors, we use a set of four combinations to deﬁne terms in the regression
mean function.
Basic summary statistics for the relevant variables in the fuel data are given in
Table 3.1, and these begin to give us a bit of a picture of these data. First, there
is quite a bit of variation in Fuel, with values between a minimum of about 626
gallons per year and a maximum of about 843 gallons per year. The gas Tax varies
1This discussion of polynomials might puzzle some readers because in Section 3.2, we said the linear
regression mean function was a hyperplane, but here we have said that it might be curved, seemingly
a contradiction. However, both of these statements are correct. If we ﬁt a mean function like E(Y|X =
x) = β0 + β1x + β2x2, the mean function is a quadratic curve in the plot of the response versus x but
a plane in the three-dimensional plot of the response versus x and x2.

TERMS AND PREDICTORS
53
TABLE 3.1
Summary Statistics for the Fuel Data
Variable
N
Average
Std Dev
Minimum
Median
Maximum
Tax
51
20.155
4.5447
7.5
20.
29.
Dlic
51
903.68
72.858
700.2
909.07
1075.3
Income
51
28.404
4.4516
20.993
27.871
40.64
logMiles
51
15.745
1.4867
10.583
16.268
18.198
Fuel
51
613.13
88.96
317.49
626.02
842.79
from only 7.5 cents per gallon to a high of 29 cents per gallon, so unlike much of
the world gasoline taxes account for only a small part of the cost to consumers of
gasoline. Also of interest is the range of values in Dlic: The number of licensed
drivers per 1000 population over the age of 16 is between about 700 and 1075.
Some states appear to have more licensed drivers than they have population over
age 16. Either these states allow drivers under the age of 16, allow nonresidents to
obtain a driver’s license, or the data are in error. For this example, we will assume
one of the ﬁrst two reasons.
Of course, these univariate summaries cannot tell us much about how the fuel
consumption depends on the other variables. For this, graphs are very helpful. The
scatterplot matrix for the fuel data is repeated in Figure 3.3. From our previous
Tax
0.7
0.9
12
16
10
20
0.7
0.9
Dlic
Income
25
35
12
16
logMiles
10
20
25
35
300
600
300
600
Fuel
FIG. 3.3
Scatterplot matrix for the fuel data.

54
MULTIPLE REGRESSION
TABLE 3.2
Sample Correlations for the Fuel Data
Sample Correlations
Tax
Dlic
Income logMiles
Fuel
Tax
1.0000 -0.0858 -0.0107
-0.0437 -0.2594
Dlic
-0.0858
1.0000 -0.1760
0.0306
0.4685
Income
-0.0107 -0.1760
1.0000
-0.2959 -0.4644
logMiles -0.0437
0.0306 -0.2959
1.0000
0.4220
Fuel
-0.2594
0.4685 -0.4644
0.4220
1.0000
discussion, Fuel decreases on the average as Tax increases, but there is lot of
variation. We can make similar qualitative judgments about each of the regressions
of Fuel on the other variables. The overall impression is that Fuel is at best weakly
related to each of the variables in the scatterplot matrix, and in turn these variables
are only weakly related to each other.
Does this help us understand how Fuel is related to all four predictors simultane-
ously? We know from the discussion in Section 3.1 that the marginal relationships
between the response and each of the variables is not sufﬁcient to understand the
joint relationship between the response and the terms. The interrelationships among
the terms are also important. The pairwise relationships between the terms can be
viewed in the remaining cells of the scatterplot matrix. In Figure 3.3, the relation-
ships between all pairs of terms appear to be very weak, suggesting that for this
problem the marginal plots including Fuel are quite informative about the multiple
regression problem.
A more traditional, and less informative, summary of the two-variable relation-
ships is the matrix of sample correlations, shown in Table 3.2. In this instance,
the correlation matrix helps to reinforce the relationships we see in the scatter-
plot matrix, with fairly small correlations between the predictors and Fuel, and
essentially no correlation between the predictors themselves.
3.4
ORDINARY LEAST SQUARES
From the initial collection of potential predictors, we have computed a set of
p + 1 terms, including an intercept, X = (X0, X1, . . . , Xp). The mean function
and variance function for multiple linear regression are
E(Y|X) = β0 + β1X1 + · · · + βpXp
(3.4)
Var(Y|X) = σ 2
Both the βs and σ 2 are unknown parameters that need to be estimated.
3.4.1
Data and Matrix Notation
Suppose we have observed data for n cases or units, meaning we have a value of Y
and all of the terms for each of the n cases. We have symbols for the response and

ORDINARY LEAST SQUARES
55
the terms using matrices and vectors; see Appendix A.6 for a brief introduction.
We deﬁne
Y =


y1
y2
...
yn


X =


1
x11
· · ·
x1p
1
x21
· · ·
x2p
...
...
...
...
1
xn1
· · ·
xnp


(3.5)
so Y is an n × 1 vector and X is an n × (p + 1) matrix. We also deﬁne β to be
a (p + 1) × 1 vector of regression coefﬁcients and e to be the n × 1 vector of
statistical errors,
β =


β0
β1
...
βp


and
e =


e1
e2
...
en


The matrix X gives all of the observed values of the terms. The ith row of X will
be deﬁned by the symbol x′
i, which is a (p + 1) × 1 vector for mean functions that
include an intercept. Even though xi is a row of X, we use the convention that all
vectors are column vectors and therefore need to write x′
i to represent a row. An
equation for the mean function evaluated at xi is
E(Y|X = xi) = x′
iβ
= β0 + β1xi1 + · · · + βpxip
(3.6)
In matrix notation, we will write the multiple linear regression model as
Y = Xβ + e
(3.7)
The ith row of (3.7) is yi = x′
iβ + ei.
For the fuel data, the ﬁrst few and the last few rows of the matrix X and the
vector Y are
X =


1
18.00
1031.38
23.471
16.5271
1
8.00
1031.64
30.064
13.7343
1
18.00
908.597
25.578
15.7536
...
...
...
...
...
1
25.65
904.894
21.915
15.1751
1
27.30
882.329
28.232
16.7817
1
14.00
970.753
27.230
14.7362


Y =


690.264
514.279
621.475
...
562.411
581.794
842.792


The terms in X are in the order intercept, Tax, Dlic, Income and ﬁnally log(Miles).
The matrix X is 51 × 5 and Y is 51 × 1.

56
MULTIPLE REGRESSION
3.4.2
Variance-Covariance Matrix of e
The 51 × 1 error vector is an unobservable random vector, as in Appendix A.6.
The assumptions concerning the eis given in Chapter 2 are summarized in matrix
form as
E(e) = 0
Var(e) = σ 2In
where Var(e) means the covariance matrix of e, In is the n × n matrix with ones
on the diagonal and zeroes everywhere else, and 0 is a matrix or vector of zeroes
of appropriate size. If we add the assumption of normality, we can write
e ∼N(0, σ 2In)
3.4.3
Ordinary Least Squares Estimators
The least squares estimate ˆβ of β is chosen to minimize the residual sum of squares
function
RSS(β) =

(yi −x′
iβ)2 = (Y −Xβ)′(Y −Xβ)
(3.8)
The ols estimates can be found from (3.8) by differentiation in a matrix analog to
the development of Appendix A.3. The ols estimate is given by the formula
ˆβ = (X′X)−1X′Y
(3.9)
provided that the inverse (X′X)−1 exists. The estimator ˆβ depends only on the
sufﬁcient statistics X′X and X′Y, which are matrices of uncorrected sums of squares
and cross-products.
Do not compute the least squares estimates using (3.9)! Uncorrected sums of
squares and cross-products are prone to large rounding error, and so computations
can be highly inaccurate. The preferred computational methods are based on matrix
decompositions as brieﬂy outlined in Appendix A.8. At the very least, computations
should be based on corrected sums of squares and cross-products.
Suppose we deﬁne X to be the n × p matrix
X =


(x11 −x1)
· · ·
(x1p −xp)
(x21 −x1)
· · ·
(x2p −xp)
...
...
...
(xn1 −x1)
· · ·
(xnp −xp)


This matrix consists of the original X matrix, but with the ﬁrst column removed
and the column mean subtracted from each of the remaining columns. Similarly,
Y is the vector with typical elements yi −y. Then
C =
1
n −1
	 X ′X
X ′Y
Y′X
Y′Y

(3.10)

ORDINARY LEAST SQUARES
57
is the matrix of sample variances and covariances. When p = 1, the matrix C is
given by
C =
1
n −1
	 SXX
SXY
SXY
SYY

The elements of C are the summary statistics needed for ols computations in simple
linear regression. If we let β∗be the parameter vector excluding the intercept β0,
then for p ≥1,
ˆβ
∗= (X ′X)−1X ′Y
ˆβ0 = y −ˆβ
∗′x
where x is the vector of sample means for all the terms except for the intercept.
Once ˆβ is computed, we can deﬁne several related quantities. The ﬁtted values
are ˆY = X ˆβ and the residuals are ˆe = Y −ˆY. The function (3.8) evaluated at ˆβ is
the residual sum of squares, or RSS,
RSS = ˆe′ˆe = (Y −X ˆβ)′(Y −X ˆβ)
(3.11)
3.4.4
Properties of the Estimates
Additional properties of the ols estimates are derived in Appendix A.8 and are only
summarized here. Assuming that E(e) = 0 and Var(e) = σ 2In, then ˆβ is unbiased,
E( ˆβ) = β, and
Var( ˆβ) = σ 2(X′X)−1
(3.12)
Excluding the intercept term,
Var( ˆβ
∗) = σ 2(X ′X)−1
(3.13)
and so (X ′X)−1 is all but the ﬁrst row and column of (X′X)−1. An estimate of σ 2
is given by
ˆσ 2 =
RSS
n −(p + 1)
(3.14)
which is the residual sum of squares divided by its df = n −(p + 1). Several
formulas for RSS can be computed by substituting the value of ˆβ into (3.11) and
simplifying:
RSS = Y′Y −ˆβ
′(X′X) ˆβ
= Y′Y −ˆβ
′X′Y
= Y′Y −ˆβ
∗′(X ′X) ˆβ
∗
(3.15)
= Y′Y −ˆβ
′(X′X) ˆβ + ny2

58
MULTIPLE REGRESSION
Recognizing that Y′Y = SYY, (3.15) has the nicest interpretation, as it writes RSS
as equal to the total sum of squares minus a quantity we will call the regression
sum of squares, or SSreg. In addition, if e is normally distributed, then the residual
sum of squares has a Chi-squared distribution,
(n −(p + 1))ˆσ 2/σ 2 ∼χ2(n −(p + 1))
By substituting ˆσ 2 for σ 2 in (3.12), we ﬁnd the estimated variance of ˆβ, 
Var( ˆβ),
to be

Var( ˆβ) = ˆσ 2(X′X)−1
(3.16)
3.4.5
Simple Regression in Matrix Terms
For simple regression, X and Y are given by
X =


1
x1
1
x2
...
...
1
xn


Y =


y1
y2
...
yn


and thus
(X′X) =
	
n
 xi
 xi
 x2
i

X′Y =
	  yi
 y2
i

By direct multiplication, (X′X)−1 can be shown to be
(X′X)−1 =
1
SXX
	  x2
i /n
−x
−x
1

(3.17)
so that
ˆβ =
	 ˆβ0
ˆβ1

= (X′X)−1X′Y =
1
SXX
	  x2
i /n
−x
−x
 xiyi

 	  yi
 y2
i

=
	
y −ˆβ1x
SXY/SXX

as found previously. Also, since  x2
i /(nSXX) = 1/n + x2/SXX, the variances and
covariances for ˆβ0 and ˆβ1 found in Chapter 2 are identical to those given by
σ 2(X′X)−1.
The results are simpler in the deviations from the sample average form, since
X ′X = SXX
X ′Y = SXY

ORDINARY LEAST SQUARES
59
and
ˆβ1 = (X ′X)−1X ′Y = SXY
SXX
ˆβ0 = y −ˆβ1x
Fuel Consumption Data
We will generally let p equal the number of terms in a mean function excluding the
intercept, and p′ = p + 1 equal if the intercept is included; p′ = p if the intercept
is not included. We shall now ﬁt the mean function with p′ = 5 terms, including
the intercept for the fuel consumption data. Continuing a practice we have already
begun, we will write Fuel on Tax Dlic Income log(Miles) as shorthand for using
ols to ﬁt the multiple linear regression model with mean function
E(Fuel|X) = β0 + β1Tax + β2Dlic + β3Income + β4log(Miles)
where conditioning on X is short for conditioning on all the terms in the mean
function. All the computations are based on the summary statistics, which are the
sample means given in Table 3.1 and the sample covariance matrix C deﬁned at
(3.10) and given by
Tax
Dlic
Income logMiles
Fuel
Tax
20.6546
-28.4247
-0.2162
-0.2955 -104.8944
Dlic
-28.4247 5308.2591
-57.0705
3.3135 3036.5905
Income
-0.2162
-57.0705
19.8171
-1.9580 -183.9126
logMiles
-0.2955
3.3135
-1.9580
2.2103
55.8172
Fuel
-104.8944 3036.5905 -183.9126
55.8172 7913.8812
Most statistical software will give the sample correlations rather than the covari-
ances. The reader can verify that the correlations in Table 3.2 can be obtained from
these covariances. For example, the sample correlation between Tax and Income
is −0.2162/√(20.6546 × 19.8171) = −0.0107 as in Table 3.2. One can convert
back from correlations and sample variances to covariances; the square root of the
sample variances are given in Table 3.1.
The 5 × 5 matrix (X′X)−1 is given by
Intercept
Tax
Dlic
Income
logMiles
Intercept
9.02151 -2.852e-02 -4.080e-03 -5.981e-02 -1.932e-01
Tax
-0.02852
9.788e-04
5.599e-06
4.263e-05
1.602e-04
Dlic
-0.00408
5.599e-06
3.922e-06
1.189e-05
5.402e-06
Income
-0.05981
4.263e-05
1.189e-05
1.143e-03
1.000e-03
logMiles
-0.19315
1.602e-04
5.402e-06
1.000e-03
9.948e-03
The elements of (X′X)−1 often differ by several orders of magnitude, as is the case
here, where the smallest element in absolute value is 3.9 × 10−6 = 0.0000039, and
the largest element is 9.02. It is the combining of these numbers of very different
magnitude that can lead to numerical inaccuracies in computations.

60
MULTIPLE REGRESSION
The lower-right 4 × 4 sub-matrix of (X′X)−1 is (X ′X)−1. Using the formulas
based on corrected sums of squares in this chapter, the estimate ˆβ
∗is computed
to be
ˆβ
∗= (X ′X)−1X ′Y =


ˆβ1
ˆβ2
ˆβ3
ˆβ4

=


−4.2280
0.4719
−6.1353
18.5453


The estimated intercept is
ˆβ0 = y −ˆβ
∗′x = 154.193
and the residual sum of squares is
RSS = Y′Y −ˆβ
∗′(X ′X)Oβ
∗= 193,700
so the estimate of σ 2 is
ˆσ 2 =
RSS
n −(p + 1) = 193,700
51 −5 = 4211
Standard errors and estimated covariances of the ˆβj are found by multiplying ˆσ
by the square roots of elements of (X′X)−1. For example,
se( ˆβ2) = ˆσ

3.922 × 10−6 = 0.1285
Virtually all statistical software packages include higher-level functions that will
ﬁt multiple regression models, but getting intermediate results like (X′X)−1 may be
a challenge. Table 3.3 shows typical output from a statistical package. This output
gives the estimates ˆβ and their standard errors computed based on ˆσ 2 and the
TABLE 3.3
Edited Output from the Summary Method in R for
Multiple Regression in the Fuel Data
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 154.1928
194.9062
0.791 0.432938
Tax
-4.2280
2.0301
-2.083 0.042873
Dlic
0.4719
0.1285
3.672 0.000626
Income
-6.1353
2.1936
-2.797 0.007508
logMiles
18.5453
6.4722
2.865 0.006259
Residual standard error: 64.89 on 46 degrees of freedom
Multiple R-Squared: 0.5105
F-statistic: 11.99 on 4 and 46 DF,
p-value: 9.33e-07

THE ANALYSIS OF VARIANCE
61
diagonal elements of (X′X)−1. The column marked t-value is the ratio of the
estimate to its standard error. The column labelled Pr(>|t|) will be discussed
shortly. Below the table are a number of other summary statistics; at this point
only the estimate of σ called the residual standard error and its df are familiar.
3.5
THE ANALYSIS OF VARIANCE
For multiple regression, the analysis of variance is a very rich technique that is
used to compare mean functions that include different nested sets of terms. In the
overall analysis of variance, the mean function with all the terms
E(Y|X = x) = β′x
(3.18)
is compared with the mean function that includes only an intercept:
E(Y|X = x) = β0
(3.19)
For simple regression, these correspond to (2.16) and (2.13), respectively. For mean
function (3.19), ˆβ0 = y and the residual sum of squares is SYY. For mean function
(3.18), the estimate of β is given by (3.9) and RSS is given in (3.11). We must
have RSS < SYY, and the difference between these two
SSreg = SYY −RSS
(3.20)
corresponds to the sum of squares of Y explained by the larger mean function that
is not explained by the smaller mean function. The number of df associated with
SSreg is equal to the number of df in SYY minus the number of df in RSS, which
equals p, the number of terms in the mean function excluding the intercept.
These results are summarized in the analysis of variance table in Table 3.4.
We can judge the importance of the regression on the terms in the larger model
by determining if SSreg is sufﬁciently large by comparing the ratio of the mean
square for regression to ˆσ 2 to the F(p, n −p′) distribution2 to get a signiﬁcance
TABLE 3.4
The Overall Analysis of Variance Table
Source
df
SS
MS
F
p-value
Regression
p
SSreg
SSreg/1
MSreg/ˆσ 2
Residual
n −(p + 1)
RSS
ˆσ 2 = RSS/(n −2)
Total
n −1
SYY
2Reminder: p′ = p for mean functions with no intercept, and p′ = p + 1 for mean functions with an
intercept.

62
MULTIPLE REGRESSION
level. If the computed signiﬁcance level is small enough, then we would judge that
the mean function (3.18) provides a signiﬁcantly better ﬁt than does (3.19). The
ratio will have an exact F distribution if the errors are normal and (3.19) is true.
The hypothesis tested by this F-test is
NH:
E(Y|X = x) = β0
AH:
E(Y|X = x) = x′β
3.5.1
The Coefﬁcient of Determination
As with simple regression, the ratio
R2 = SSreg
SYY = 1 −RSS
SYY
(3.21)
gives the proportion of variability in Y explained by regression on the terms. R2
can also be shown to be the square of the correlation between the observed values
Y and the ﬁtted values ˆY; we will explore this further in the next chapter. R2 is
also called the multiple correlation coefﬁcient because it is the maximum of the
correlation between Y and any linear combination of the terms in the mean function.
Fuel Consumption Data
The overall analysis of variance table is given by
Df Sum Sq Mean Sq F value
Pr(>F)
Regression 4 201994
50499
11.992
9.33e-07
Residuals 46 193700
4211
Total
50 395694
To get a signiﬁcance level for the test, we would compare F = 11.992 with the
F(4, 46) distribution. Most computer packages do this automatically, and the result
is shown in the column marked Pr(>F) to be about 0.0000009, a very small
number, leading to very strong evidence against the null hypothesis that the mean
of Fuel does not depend on any of the terms. The value of R2 = 201994/395694 =
0.5105 indicates that about half the variation in Fuel is explained by the terms.
The value of F, its signiﬁcance level, and the value of R2 are given in Table 3.3.
3.5.2
Hypotheses Concerning One of the Terms
Obtaining information on one of the terms may be of interest. Can we do as
well, understanding the mean function for Fuel if we delete the Tax variable? This
amounts to the following hypothesis test of
NH:
β1 = 0,
β0, β2, β3, β4 arbitrary
AH:
β1 ̸= 0,
β0, β2, β3, β4 arbitrary
(3.22)
The following procedure can be used. First, ﬁt the mean function that excludes the
term for Tax and get the residual sum of squares for this smaller mean function.

THE ANALYSIS OF VARIANCE
63
Then ﬁt again, this time including Tax, and once again get the residual sum of
squares. Subtracting the residual sum of squares for the larger mean function from
the residual sum of squares for the smaller mean function will give the sum of
squares for regression on Tax after adjusting for the terms that are in both mean
functions, Dlic, Income and log(Miles). Here is a summary of the computations
that are needed:
Df
SS
MS
F
Pr(>F)
Excluding Tax
47
211964
Including Tax
46
193700
Difference
1
18264
18264
4.34
0.043
The row marked “Excluding Tax” gives the df and RSS for the mean function
without Tax, and the next line gives these values for the larger mean function
including Tax. The difference between these two given on the next line is the sum
of squares explained by Tax after adjusting for the other terms in the mean function.
The F-test is given by F = (18,264/1)/ˆσ 2 = 4.34, which, when compared to the
F distribution with (1, 46) df gives a signiﬁcance level of about 0.04. We thus have
modest evidence that the coefﬁcient for Tax is different from zero. This is called a
partial F-test. Partial F-tests can be generalized to testing several coefﬁcients to
be zero, but we delay that generalization to Section 5.4.
3.5.3
Relationship to the t-Statistic
Another reasonable procedure for testing the importance of Tax is simply to com-
pare the estimate of the coefﬁcient divided by its standard error to the t(n −p′)
distribution. One can show that the square of this t-statistic is the same number of
the F-statistic just computed, so these two procedures are identical. Therefore, the
t-statistic tests hypothesis (3.22) concerning the importance of terms adjusted for
all the other terms, not ignoring them.
From Table 3.3, the t-statistic for Tax is t = −2.083, and t2 = (−2.083)2 =
4.34, the same as the F-statistic we just computed. The signiﬁcance level for Tax
given in Table 3.3 also agrees with the signiﬁcance level we just obtained for the
F-test, and so the signiﬁcance level reported is for the two-sided test. To test the
hypothesis that β1 = 0 against the one-sided alternative that β1 < 0, we could again
use the same t-value, but the signiﬁcance level would be one-half of the value for
the two-sided test.
A t-test that βj has a speciﬁc value versus a two-sided or one-sided alternative
(with all other coefﬁcients arbitrary) can be carried out as described in Section 2.8.
3.5.4
t-Tests and Added-Variable Plots
In Section 3.1, we discussed adding a term to a simple regression mean function.
The same general procedure can be used to add a term to any linear regression
mean function. For the added-variable plot for a term, say X1, plot the residuals
from the regression of Y on all the other X’s versus the residuals for the regression

64
MULTIPLE REGRESSION
of X1 on all the other Xs. One can show (Problem 3.2) that (1) the slope of the
regression in the added-variable plot is the estimated coefﬁcient for X1 in the
regression with all the terms, and (2) the t-test for testing the slope to the zero in
the added-variable plot is essentially the same as the t-test for testing β1 = 0 in the
ﬁt of the larger mean function, the only difference being a correction for degrees
of freedom.
3.5.5
Other Tests of Hypotheses
We have obtained a test of a hypothesis concerning the effect of Tax adjusted for
all the other terms in the mean function. Equally well, we could obtain tests for the
effect of Tax adjusting for some of the other terms or for none of the other terms.
In general, these tests will not be equivalent, and a variable can be judged useful
ignoring variables but useless when adjusted for them. Furthermore, a predictor
that is useless by itself may become important when considered in concert with
the other variables. The outcome of these tests depends on the sample correlations
between the terms.
3.5.6
Sequential Analysis of Variance Tables
By separating Tax from the other terms, SSreg is divided into two pieces, one
for ﬁtting the ﬁrst three terms, and one for ﬁtting Tax after the other three. This
subdivision can be continued by dividing SSreg into a sum of squares “explained”
by each term separately. Unless all the terms are uncorrelated, this breakdown is
not unique. For example, we could ﬁrst ﬁt Dlic, then Tax adjusted for Dlic, then
Income adjusted for both Dlic and Tax, and ﬁnally log(Miles) adjusted for the other
three. The resulting table is given in Table 3.5a. Alternatively, we could ﬁt in the
order log(Miles), Income, Dlic and then Tax as in Table 3.5b. The sums of squares
can be quite different in the two tables. For example, the sum of squares for Dlic
ignoring the other terms is about 25% larger than the sum of squares for Dlic
adjusting for the other terms. In this problem, the terms are nearly uncorrelated,
see Table 3.2, so the effect of ordering is relatively minor. In problems with high
sample correlations between terms, order can be very important.
TABLE 3.5
Two Analysis of Variance Tables with Different Orders of Fitting
(a) First analysis
(b) Second analysis
Df Sum Sq Mean Sq
Dlic
1
86854
86854
Tax
1
19159
19159
Income
1
61408
61408
logMiles
1
34573
34573
Residuals 46 193700
4211
Df Sum Sq Mean Sq
logMiles
1
70478
70478
Income
1
49996
49996
Dlic
1
63256
63256
Tax
1
18264
18264
Residuals 46 193700
4211

PROBLEMS
65
3.6
PREDICTIONS AND FITTED VALUES
Suppose we have observed, or will in the future observe, a new case with its own
set of predictors that result in a vector of terms x∗. We would like to predict the
value of the response given x∗. In exactly the same way as was done in simple
regression, the point prediction is ˜y∗= x′
∗ˆβ, and the standard error of prediction,
sepred( ˜y∗|x∗), using Appendix A.8, is
sepred( ˜y∗|x∗) = ˆσ

1 + x′∗(X′X)−1x∗
(3.23)
Similarly, the estimated average of all possible units with a value x for the terms is
given by the estimated mean function at x, ˆE(Y|X = x) = ˆy = x′ ˆβ with standard
error given by
seﬁt( ˆy|x) = ˆσ

x′(X′X)−1x
(3.24)
Virtually all software packages will give the user access to the ﬁtted values, but
getting the standard error of prediction and of the ﬁtted value may be harder. If a
program produces seﬁt but not sepred, the latter can be computed from the former
from the result
sepred( ˜y∗|x∗) =

ˆσ 2 + seﬁt( ˜y∗|x∗)2
PROBLEMS
3.1. Berkeley Guidance Study
The Berkeley Guidance Study enrolled children
born in Berkeley, California, between January 1928 and June 1929, and then
measured them periodically until age eighteen (Tuddenham and Snyder, 1954).
The data we use is described in Table 3.6, and the data is given in the data
ﬁles BGSgirls.txt for girls only, BGSboys.txt for boys only, and
BGSall.txt for boys and girls combined. For this example, use only the
data on the girls.
3.1.1. For the girls only, draw the scatterplot matrix of all the age two vari-
ables, all the age nine variables and Soma. Write a summary of the
information in this scatterplot matrix. Also obtain the matrix of sample
correlations between the height variables.
3.1.2. Starting with the mean function E(Soma|WT9) = β0 + β1WT9, use
added-variable plots to explore adding LG9 to get the mean function
E(Soma|WT9, LG9) = β0 + β1WT9 + β2LG9. In particular, obtain the
four plots equivalent to Figure 3.1, and summarize the information in
the plots.
3.1.3. Fit the multiple linear regression model with mean function
E(Soma|X) = β0 + β1HT2 + β2WT2 + β3HT9 + β4WT9 + β5ST9
(3.25)

66
MULTIPLE REGRESSION
TABLE 3.6
Variable Deﬁnitions for the Berkeley Guidance Study in the Files
BGSgirls.txt, BGSboys.txt, and BGSall.txt
Variable
Description
Sex
0 for males, 1 for females
WT2
Age 2 weight, kg
HT2
Age 2 height, cm
WT9
Age 9 weight, kg
HT9
Age 9 height, cm
LG9
Age 9 leg circumference, cm
ST9
Age 9 strength, kg
WT18
Age 18 weight, kg
HT18
Age 18 height, cm
LG18
Age 18 leg circumference, cm
ST18
Age 18 strength, kg
Soma
Somatotype, a scale from 1, very thin, to 7, obese, of body type
Find ˆσ, R2, the overall analysis of variance table and overall F-test.
Compute the t-statistics to be used to test each of the βj to be zero
against two-sided alternatives. Explicitly state the hypotheses tested and
the conclusions.
3.1.4. Obtain the sequential analysis of variance table for ﬁtting the variables
in the order they are given in (3.25). State the hypotheses tested and
the conclusions for each of the tests.
3.1.5. Obtain analysis of variance again, this time ﬁtting with the ﬁve terms
in the order given from right to left in (3.25). Explain the differences
with the table you obtained in Problem 3.1.4. What graphs could help
understand the issues?
3.2. Added-variable plots
This problem uses the United Nations example in
Section 3.1 to demonstrate many of the properties of added-variable plots.
This problem is based on the mean function
E(log(Fertility)|log(PPgdp) = x1, Purban = x2) = β0 + β1x1 + β2x2
There is nothing special about the two-predictor regression mean function, but
we are using this case for simplicity.
3.2.1. Show that the estimated coefﬁcient for log(PPgdp) is the same as the
estimated slope in the added-variable plot for log(PPgdp) after Purban.
This correctly suggests that all the estimates in a multiple linear regres-
sion model are adjusted for all the other terms in the mean function.
Also, show that the residuals in the added-variable plot are identical to
the residuals from the mean function with both predictors.

PROBLEMS
67
3.2.2. Show that the t-test for the coefﬁcient for log(PPgdp) is not quite the
same from the added-variable plot and from the regression with both
terms, and explain why they are slightly different.
3.3. The following questions all refer to the mean function
E(Y|X1 = x1, X2 = x2) = β0 + β1x1 + β2x2
(3.26)
3.3.1. Suppose we ﬁt (3.26) to data for which x1 = 2.2x2, with no error. For
example, x1 could be a weight in pounds, and x2 the weight of the
same object in kg. Describe the appearance of the added-variable plot
for X2 after X1.
3.3.2. Again referring to (3.26), suppose now that Y and X1 are perfectly
correlated, so Y = 3X1, without any error. Describe the appearance of
the added-variable plot for X2 after X1.
3.3.3. Under what conditions will the added-variable plot for X2 after X1
have exactly the same shape as the scatterplot of Y versus X2?
3.3.4. True or false: The vertical variation in an added-variable plot for X2
after X1 is always less than or equal to the vertical variation in a plot
of Y versus X2. Explain.
3.4. Suppose we have a regression in which we want to ﬁt the mean function (3.1).
Following the outline in Section 3.1, suppose that the two terms X1 and X2
have sample correlation equal to zero. This means that, if xij, i = 1, . . . , n,
and j = 1, 2 are the observed values of these two terms for the n cases in the
data, n
i=1(xi1 −x1)(xi2 −x2) = 0.
3.4.1. Give the formula for the slope of the regression for Y on X1, and for
Y on X2. Give the value of the slope of the regression for X2 on X1.
3.4.2. Give formulas for the residuals for the regressions of Y on X1 and for
X2 on X1. The plot of these two sets of residuals corresponds to the
added-variable plot in Figure 3.1d.
3.4.3. Compute the slope of the regression corresponding to the added-variable
plot for the regression of Y on X2 after X1, and show that this slope
is exactly the same as the slope for the simple regression of Y on X2
ignoring X1. Also ﬁnd the intercept for the added-variable plot.
3.5. Refer to the data described in Problem 1.5, page 18. For this problem, consider
the regression problem with response BSAAM, and three predictors as terms
given by OPBPC, OPRC and OPSLAKE.
3.5.1. Examine the scatterplot matrix drawn for these three terms and the
response. What should the correlation matrix look like (that is, which
correlations are large and positive, which are large and negative, and
which are small)? Compute the correlation matrix to verify your results.
Get the regression summary for the regression of BSAAM on these three
terms. Explain what the “t-values” column of your output means.

68
MULTIPLE REGRESSION
3.5.2. Obtain the overall test if the hypothesis that BSAAM is independent of
the three terms versus the alternative that it is not independent of them,
and summarize your results.
3.5.3. Obtain three analysis of variance tables ﬁtting in the order (OPBPC,
OPRC and OPSLAKE), then (OPBPC, OPSLAKE and OPRC), and
ﬁnally (OPSLAKE, OPRC and OPBPC). Explain the resulting tables,
and discuss in particular any apparent inconsistencies. Which F-tests
in the Anova tables are equivalent to t-tests in the regression output?
3.5.4. Using the output from the last problem, test the hypothesis that the coef-
ﬁcients for both OPRC and OPBPC are both zero against the alternative
that they are not both zero.

C H A P T E R 4
Drawing Conclusions
The computations that are done in multiple linear regression, including drawing
graphs, creation of terms, ﬁtting models, and performing tests, will be similar in
most problems. Interpreting the results, however, may differ by problem, even if
the outline of the analysis is the same. Many issues play into drawing conclusions,
and some of them are discussed in this chapter.
4.1
UNDERSTANDING PARAMETER ESTIMATES
Parameters in mean functions have units attached to them. For example, the ﬁtted
mean function for the fuel consumption data is
E(Fuel|X) = 154.19 −4.23 Tax + 0.47 Dlic −6.14 Income + 18.54 log(Miles)
Fuel is measured in gallons, and so all the quantities on the right of this equation
must also be in gallons. The intercept is 154.19 gallons. Since Income is measured
in thousands of dollars, the coefﬁcient for Income must be in gallons per thousand
dollars of income. Similarly, the units for the coefﬁcient for Tax is gallons per cent
of tax.
4.1.1
Rate of Change
The usual interpretation of an estimated coefﬁcient is as a rate of change: increas-
ing Tax rate by one cent should decrease consumption, all other factors being
held constant, by about 4.23 gallons per person. This assumes that a predictor
can in fact be changed without affecting the other terms in the mean function
and that the available data will apply when the predictor is so changed. The
fuel data are observational since the assignment of values for the predictors was
not under the control of the analyst, so whether increasing taxes would cause
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
69

70
DRAWING CONCLUSIONS
a decrease in fuel consumption cannot be assessed from these data. From these
data, we can observe association but not cause: states with higher tax rates are
observed to have lower fuel consumption. To draw conclusions concerning the
effects of changing tax rates, the rates must in fact be changed and the results
observed.
The coefﬁcient estimate of log(Miles) is 18.55, meaning that a change of one unit
in log(Miles) is associated with an 18.55 gallon per person increase in consumption.
States with more roads have higher per capita fuel consumption. Since we used
base-two logarithms in this problem, increasing log(Miles) by one unit means that
the value of Miles doubles. If we double the amount of road in a state, we expect
to increase fuel consumption by about 18.55 gallons per person. If we had used
base-ten logarithms, then the ﬁtted mean function would be
E(Fuel|X) = 154.19 −4.23 Tax + 0.47 Dlic −6.14 Income + 61.61 log10(Miles)
The only change in the ﬁtted model is for the coefﬁcient for the log of Miles, which
is now interpreted as a change in expected Fuel consumption when log10(Miles)
increases by one unit, or when Miles is multiplied by 10.
4.1.2
Signs of Estimates
The sign of a parameter estimate indicates the direction of the relationship between
the term and the response. In multiple regression, if the terms are correlated, the sign
of a coefﬁcient may change depending on the other terms in the model. While this
is mathematically possible and, occasionally, scientiﬁcally reasonable, it certainly
makes interpretation more difﬁcult. Sometimes this problem can be removed by
redeﬁning the terms into new linear combinations that are easier to interpret.
4.1.3
Interpretation Depends on Other Terms in the Mean Function
The value of a parameter estimate not only depends on the other terms in a mean
function but it can also change if the other terms are replaced by linear combinations
of the terms.
Berkeley Guidance Study
Data from the Berkeley Guidance Study on the growth of boys and girls are given
in Problem 3.1. As in Problem 3.1, we will view Soma as the response, but con-
sider the three predictors WT2, WT9, WT18 for the n = 70 girls in the study. The
scatterplot matrix for these four variables is given in Figure 4.1. First look at the
last row of this ﬁgure, giving the marginal response plots of Soma versus each of
the three potential predictors. For each of these plots, we see that Soma is increasing
with the potential predictor on the average, although the relationship is strongest
at the oldest age and weakest at the youngest age. The two-dimensional plots of
each pair of predictors suggest that the predictors are correlated among themselves.
Taken together, we have evidence that the regression on all three predictors cannot

UNDERSTANDING PARAMETER ESTIMATES
71
WT2
25
35
45
3
4
5
6
7
10
12
14
16
25
35
45
WT9
WT18
50
70
90
10
12
14
16
3
4
5
6
7
50
70
90
Soma
FIG. 4.1
Scatterplot matrix for the girls in the Berkeley Guidance Study.
be viewed as just the sum of the three separate simple regressions because we must
account for the correlations between the terms.
We will proceed with this example using the three original predictors as terms
and Soma as the response. We are encouraged to do this because of the appearance
of the scatterplot matrix. Since each of the two-dimensional plots appear to be well
summarized by a straight-line mean function, we will see later that this suggests
that the regression of the response on the original predictors without transformation
is likely to be appropriate.
The parameter estimates for the regression of Soma on WT2, WT9, and WT18
given in the column marked “Model 1” in Table 4.1 leads to the unexpected con-
clusion that heavier girls at age two may tend to be thinner, have lower expected
somatotype, at age 18. We reach this conclusion because the t-statistic for testing
the coefﬁcient equal to zero, which is not shown in the table, has a signiﬁcance level
of about 0.06. The sign, and the weak signiﬁcance, may be due to the correlations
between the terms. In place of the preceding variables, consider the following:
WT2 = Weight at age 2
DW9 = WT9 −WT2 = Weight gain from age 2 to 9
DW18 = WT18 −WT9 = Weight gain from age 9 to 18

72
DRAWING CONCLUSIONS
TABLE 4.1
Regression of Soma on Different
Combinations of Three Weight Variables for the n = 70
Girls in the Berkeley Guidance Study
Term
Model 1
Model 2
Model 3
(Intercept)
1.5921
1.5921
1.5921
WT2
−0.1156
−0.0111
−0.1156
WT9
0.0562
0.0562
WT18
0.0483
0.0483
DW9
0.1046
NA
DW18
0.0483
NA
Since all three original terms measure weight, combining them in this way is
reasonable. If the variables measured different quantities, then combining them
could lead to conclusions that are even less useful than those originally obtained.
The parameter estimates for Soma on WT2, DW9, and DW18 are given in the
column marked “Model 2” in Table 4.1. Although not shown in the table, summary
statistics for the regression like R2 and ˆσ 2 are identical for all the mean functions
in Table 4.1 but coefﬁcient estimates and t-tests are not the same. For example,
the slope estimate for WT2 is about −0.12, with t = −1.87 in the column “Model
1,” while in Model 2, the estimate is about one-tenth the size, and the t-value is
−0.21. In the former case, the effect of WT2 appears plausible, while in the latter
it does not. Although the estimate is negative in each, we would be led in the latter
case to conclude that the effect of WT2 is negligible. Thus, interpretation of the
effect of a variable depends not only on the other variables in a model but also
upon which linear transformation of those variables is used.
Another interesting feature of Table 4.1 is that the estimate for WT18 in Model
1 is identical to the estimate for DW18 in Model 2. In Model 1, the estimate
for WT18 is the effect on Soma of changing WT18 by one unit, with all other
terms held ﬁxed. In Model 2, the estimate for DW18 is the change in Soma when
DW18 changes by one unit, when all other terms are held ﬁxed. But the only way
DW18 = WT18 −WT9 can be changed by one unit with the other variables includ-
ing WT9 = DW9 −WT2 held ﬁxed is by changing WT18 by one unit. Conse-
quently, the terms WT18 in Model 1 and DW18 in Model 2 play identical roles
and therefore we get the same estimates.
The linear transformation of the three weight variables we have used so far
could be replaced by other linear combinations, and, depending on the context,
others might be preferred. For example, another set might be
AVE = (WT2 + WT9 + WT18)/3
LIN = WT18 −WT2
QUAD = WT2 −2WT9 + WT18
This transformation focuses on the fact that WT2, WT9 and WT18 are ordered in
time and are more or less equally spaced. Pretending that the weight measurements

UNDERSTANDING PARAMETER ESTIMATES
73
are equally spaced, AVE, LIN and QUAD are, respectively, the average, linear, and
quadratic time trends in weight gain.
4.1.4
Rank Deﬁcient and Over-Parameterized Mean Functions
In the last example, several combinations of the basic predictors WT2, WT9, and
WT18 were studied. One might naturally ask what would happen if more than three
combinations of these predictors were used in the same regression model. As long as
we use linear combinations of the predictors, as opposed to nonlinear combinations
or transformations of them, we cannot use more than three, the number of linearly
independent quantities.
To see why this is true, consider adding DW9 to the mean function including
WT2, WT9 and WT18. As in Chapter 3, we can learn about adding DW9 using an
added-variable plot of the residuals from the regression of Soma on WT2, WT9
and WT18 versus the residuals from the regression of DW9 on WT2, WT9 and
WT18. Since DW9 can be written as an exact linear combination of the other
predictors, DW9 = WT9 −WT2, the residuals from this second regression are all
exactly zero. A slope coefﬁcient for DW9 is thus not deﬁned after adjusting for the
other three terms. We would say that the four terms WT2, WT9, WT18, and DW9
are linearly dependent, since one can be determined exactly from the others. The
three variables WT2, WT9 and WT18 are linearly independent because one of them
cannot be determined exactly by a linear combination of the others. The maximum
number of linearly independent terms that could be included in a mean function is
called the rank of the data matrix X.
Model 3 in Table 4.1 gives the estimates produced in a computer package when
we tried to ﬁt using an intercept and the ﬁve terms WT2, WT9, WT18, DW9, and
DW18. Most computer programs, including this one, will select the ﬁrst three, and
the estimated coefﬁcients for them. For the remaining terms, this program sets the
estimates to “NA,” a code for a missing value; the word aliased is sometimes
used to indicate a term that is a linear combination of terms already in the mean
function, and so a coefﬁcient for it is not estimable.
Mean functions that are over-parameterized occur most often in designed experi-
ments. The simplest example is the one-way design. Suppose that a unit is assigned
to one of three treatment groups, and let X1 = 1 if the unit is in group one and
zero otherwise, X2 = 1 if the unit is in group two and zero otherwise, and X3 = 1
if the unit is in group three and zero otherwise. For each unit, we must have
X1 + X2 + X3 = 1 since each unit is in only one of the three groups. We therefore
cannot ﬁt the model
E(Y|X) = β0 + β1X1 + β2X2 + β3X3
because the sum of the Xj is equal to the column of ones, and so, for example,
X3 = 1 −X1 −X2. To ﬁt a model, we must do something else. The options are:
(1) place a constraint like β1 + β2 + β3 = 0 on the parameters; (2) exclude one of
the Xj from the model, or (3) leave out an explicit intercept. All of these options
will in some sense be equivalent, since the same R2, σ 2 and overall F-test and

74
DRAWING CONCLUSIONS
predictions will result. Of course, some care must be taken in using parameter
estimates, since these will surely depend on the parameterization used to get a full
rank model. For further reading on matrices and models of less than full rank, see,
for example, Searle (1971, 1982).
4.1.5
Tests
Even if the ﬁtted model were correct and errors were normally distributed, tests and
conﬁdence statements for parameters are difﬁcult to interpret because correlations
among the terms lead to a multiplicity of possible tests. Sometimes, tests of effects
adjusted for other variables are clearly desirable, such as in assessing a treatment
effect after adjusting for other variables to reduce variability. At other times, the
order of ﬁtting is not clear, and the analyst must expect ambiguous results. In most
situations, the only true test of signiﬁcance is repeated experimentation.
4.1.6
Dropping Terms
Suppose we have a sample of n rectangles from which we want to model log(Area)
as a function of log(Length), perhaps through the simple regression mean function
E(log(Area)|log(Length)) = η0 + η1log(Length)
(4.1)
From elementary geometry, we know that Area = Length × Width, and so the “true”
mean function for log(Area) is
E(log(Area)|log(Length), log(Width)) = β0 + β1log(Length) + β2log(Width)
(4.2)
with β0 = 0, and β1 = β2 = 1. The questions of interest are: (1) can the incorrect
mean function speciﬁed by (4.1) provide a useful approximation to the true mean
function (4.2), and if so, (2) what are the relationships between ηs, in (4.1) and the
βs in (4.2)?
The answers to these questions comes from Appendix A.2.4. Suppose that the
true mean function were
E(Y|X1 = x1, X2 = x2) = β0 + β′
1x1 + β′
2x2
(4.3)
but we want to ﬁt a mean function with X1 only. The mean function for Y|X1 is
obtained by averaging (4.3) over X2,
E(Y|X1 = x1) = E [E(Y|X1 = x1, X2)|X1 = x1]
= β0 + β′
1x1 + β′
2E(X2|X1 = x1)
(4.4)
We cannot, in general, simply drop a set of terms from a correct mean function,
but we need to substitute the conditional expectation of the terms dropped given
the terms that remain in the mean function.
In the context of the rectangles example, we get
E(log(Area)|log(Length)) = η0 + η1log(Length) + β2E(log(Width)|log(Length))
(4.5)

UNDERSTANDING PARAMETER ESTIMATES
75
The answers to the questions posed depend on the mean function for the regression
of log(Width) on log(Length). This conditional expectation has little to do with the
area of rectangles, but much to do with the way we obtain a sample of rectangles
to use in our study. We will consider three cases.
In the ﬁrst case, imagine that each of the rectangles in the study is formed by
sampling a log(Length) and a log(Width) from independent distributions. If the
mean of the log(Width) distribution is W, then by independence
E(log(Width)|log(Length)) = E(log(Width)) = W
Substituting into (4.5),
E(log(Area)|log(Length)) = β0 + β1log(Length) + β2W
= (β0 + β2W) + β1log(Length)
= W + log(Length)
where the last equation follows by substituting β0 = 0, β1 = β2 = 1. For this case,
the mean function (4.1) would be appropriate for the regression of log(Area) on
log(Width). The intercept for the mean function (4.1) would be W, and so it depends
on the distribution of the widths in the data. The slope for log(Length) is the same
for ﬁtting (4.1) or (4.2).
In the second case, suppose that
E(log(Width)|log(Length)) = γ0 + γ1log(Length)
so the mean function for the regression of log(Width) on log(Length) is a straight
line. This could occur, for example, if the rectangles in our study were obtained
by sampling from a family of similar rectangles, so the ratio Width/Length is
the same for all rectangles in the study. Substituting this into (4.5) and simplify-
ing gives
E(log(Area)|log(Length)) = β0 + β1log(Length) + β2(γ0 + γ1log(Length))
= (β0 + β2γ0) + (β1 + β2γ1)log(Length)
= γ0 + (1 + γ1)log(Length)
Once again ﬁtting using (4.1) will be appropriate, but the values of η0 = γ0
and η1 = 1 + γ1 depend on the parameters of the regression of log(Width) on
log(Length). The γ s are a characteristic of the sampling plan, not of rectangles.
Two experimenters who sample rectangles of different shapes will end up estimating
different parameters.
For a ﬁnal case, suppose that the mean function
E(log(Width)|log(Length)) = γ0 + γ1log(Length) + γ2log(Length)2

76
DRAWING CONCLUSIONS
is quadratic. Substituting into (4.5), setting β0 = 0, β1 = β2 = 1 and simplify-
ing gives
E(log(Area)|log(Length)) = β0 + β1log(Length)
+ β2

γ0 + γ1log(Length) + γ2log(Length)2
= γ0 + (1 + γ1)log(Length) + γ2log(Length)2
which is a quadratic function of log(Length). If the mean function is quadratic, or
any other function beyond a straight line, then ﬁtting (4.1) is inappropriate.
From the above three cases, we see that both the mean function and the parame-
ters for the response depend on the mean function for the regression of the removed
terms on the remaining terms. If the mean function for the regression of the removed
terms on the retained terms is not linear, then a linear mean function will not be
appropriate for the regression problem with fewer terms.
Variances are also affected when terms are dropped. Returning to the true mean
function given by (4.3), the general result for the regression of Y on X1 alone is,
from Appendix A.2.4,
Var(Y|X1 = x1) = E [Var(Y|X1 = x1, X2)|X1 = x1]
+ Var [E(Y|X1 = x1, X2)|X1 = x1]
= σ 2 + β′
2Var(X2|X1 = x1)β2
(4.6)
In the context of the rectangles example, β2 = 1 and we get
Var(log(Area)|log(Length)) = σ 2 + Var(log(Width)|log(Length))
Although ﬁtting (4.1) can be appropriate if log(Width) and log(Length) are linearly
related, the errors for this mean function can be much larger than those for (4.2)
if Var(log(Width)|log(Length)) is large. If Var(log(Width)|log(Length)) is small
enough, then ﬁtting (4.2) can actually give answers that are nearly as accurate as
ﬁtting with the true mean function (4.2).
4.1.7
Logarithms
If we start with the simple regression mean function,
E(Y|X = x) = β0 + β1x
a useful way to interpret the coefﬁcient β1 is as the ﬁrst derivative of the mean
function with respect to x,
dE(Y|X = x)
dx
= β1
We recall from elementary geometry that the ﬁrst derivative is the rate of change,
or the slope of the tangent to a curve, at a point. Since the mean function for

EXPERIMENTATION VERSUS OBSERVATION
77
simple regression is a straight line, the slope of the tangent is the same value β1
for any value of x, and β1 completely characterizes the change in the mean when
the predictor is changed for any value of x.
When the predictor is replaced by log(x), the mean function as a function of x
E(Y|X = x) = β0 + β1 log(x)
is no longer a straight line, but rather it is a curve. The tangent at the point x > 0 is
dE(Y|X = x)
dx
= β1
x
The slope of the tangent is different for each x and the effect of changing x on
E(Y|X = x) is largest for small values of x and gets smaller as x is increased.
When the response is in log scale, we can get similar approximate results by
exponentiating both sides of the equation:
E(log(Y)|X = x) = β0 + β1x
E(Y|X = x) ≈eβ0eβ1x
Differentiating this second equation gives
dE(Y|X = x)
dx
= β1E(Y|X = x)
The rate of change at x is thus equal to β1 times the mean at x. We can also write
dE(Y|X = x)/dx
E(Y|X = x)
= β1
is constant, and so β1 can be interpreted as the constant rate of change in the
response per unit of response.
4.2
EXPERIMENTATION VERSUS OBSERVATION
There are fundamentally two types of predictors that are used in a regression
analysis, experimental and observational. Experimental predictors have values that
are under the control of the experimenter, while for observational predictors, the
values are observed rather than set. Consider, for example, a hypothetical study of
factors determining the yield of a certain crop. Experimental variables might include
the amount and type of fertilizers used, the spacing of plants, and the amount of
irrigation, since each of these can be assigned by the investigator to the units, which
are plots of land. Observational predictors might include characteristics of the plots
in the study, such as drainage, exposure, soil fertility, and weather variables. All of
these are beyond the control of the experimenter, yet may have important effects
on the observed yields.
The primary difference between experimental and observational predictors is in
the inferences we can make. From experimental data, we can often infer causation.

78
DRAWING CONCLUSIONS
If we assign the level of fertilizer to plots, usually on the basis of a randomization
scheme, and observe differences due to levels of fertilizer, we can infer that the fer-
tilizer is causing the differences. Observational predictors allow weaker inferences.
We might say that weather variables are associated with yield, but the causal link
is not available for variables that are not under the experimenter’s control. Some
experimental designs, including those that use randomization, are constructed so
that the effects of observational factors can be ignored or used in analysis of
covariance (see, e.g., Cox, 1958; Oehlert, 2000).
Purely observational studies that are not under the control of the analyst can
only be used to predict or model the events that were observed in the data, as
in the fuel consumption example. To apply observational results to predict future
values, additional assumptions about the behavior of future values compared to the
behavior of the existing data must be made. From a purely observational study,
we cannot infer a causal relationship without additional information external to the
observational study.
Feedlots
A feedlot is a farming operation that includes large number of cattle, swine or
poultry in a small area. Feedlots are efﬁcient producers of animal products, and can
provide high-paying skilled jobs in rural areas. They can also cause environmental
problems, particularly with odors, ground water pollution, and noise.
Taff, Tiffany, and Weisberg (1996) report a study on the effect of feedlots on
property values. This study was based on all 292 rural residential property sales
in two southern Minnesota counties in 1993–94. Regression analysis was used.
The response was sale price. Predictors included house characteristics such as
size, number of bedrooms, age of the property, and so on. Additional predictors
described the relationship of the property to existing feedlots, such as distance to
the nearest feedlot, number of nearby feedlots, and related features of the feedlots
such as their size. The “feedlot effect” could be inferred from the coefﬁcients for
the feedlot variables.
In the analysis, the coefﬁcient estimates for feedlot effects were generally pos-
itive and judged to be nonzero, meaning that close proximity to feedlots was
associated with an increase in sale prices. While association of the opposite sign
was expected, the positive sign is plausible if the positive economic impact of the
feedlot outweighs the negative environmental impact. The positive effect is esti-
mated to be small, however, and equal to 5% or less of the sale price of the homes
in the study.
These data are purely observational, with no experimental predictors. The data
collectors had no control over the houses that actually sold, or siting of feed-
lots. Consequently, any inference that nearby feedlots cause increases in sale price
is unwarranted from this study. Given that we are limited to association, rather
than causation, we might next turn to whether
we can generalize the results.
Can we infer the same association to houses that were not sold in these coun-
ties during this period? We have no way of knowing from the data if the same

EXPERIMENTATION VERSUS OBSERVATION
79
relationship would hold for homes that did not sell. For example, some home-
owners may have perceived that they could not get a reasonable price and may
have decided not to sell. This would create a bias in favor of a positive effect of
feedlots.
Can we generalize geographically, to other Minnesota counties or to other places
in the Midwest United States? The answer to this question depends on the char-
acteristics of the two counties studied. Both are rural counties with populations of
about 17,000. Both have very low property values with median sale price in this
period of less than $50,000. Each had different regulations for operators of feedlots,
and these regulations could impact pollution problems. Applying the results to a
county with different demographics or regulations cannot be justiﬁed by these data
alone, and additional information and assumptions are required.
Joiner (1981) coined the picturesque phrase lurking variable to describe a pre-
dictor variable not included in a mean function that is correlated with terms in the
mean function. Suppose we have a regression with predictors X that are included
in the regression and a lurking variable L not included in the study, and that the
true regression mean function is
E(Y|X = x, L = ℓ) = β0 +
p

j−1
βjxj + δℓ
(4.7)
with δ ̸= 0. We assume that X and L are correlated and for simplicity we assume
further that E(L|X = x) = γ0 +  γjxj. When we ﬁt the incorrect mean function
that ignores the lurking variable, we get, from Section 4.1.6,
E(Y|X = x) = β0 +
p

j−1
βjxj + δE(L|X = x)
= (β0 + δγ0) +
p

j−1
(βj + δγj)xj
(4.8)
Suppose we are particularly interested in inferences about the coefﬁcient for X1,
and, unknown to us, β1 in (4.7) is equal to zero. If we were able to ﬁt with the
lurking variable included, we would probably conclude that X1 is not an important
predictor. If we ﬁt the incorrect mean function (4.8), the coefﬁcient for X1 becomes
(β1 + δγ1), which will be non zero if γ1 ̸= 0. The lurking variable masquerades
as the variable of interest to give an incorrect inference. A lurking variable can
also hide the effect of an important variable if, for example, β1 ̸= 0 but β1 +
δγ1 = 0.
All large observational studies like this feedlot study potentially have lurking
variables. For this study, a casino had recently opened near these counties, creating
many jobs and a demand for housing that might well have overshadowed any effect
of feedlots. In experimental data with random assignment, the potential effects of
lurking variables are greatly decreased, since the random assignment guarantees that

80
DRAWING CONCLUSIONS
the correlation between the terms in the mean function and any lurking variable is
small or zero.
The interpretation of results from a regression analysis depend on the details
of the data design and collection. The feedlot study has extremely limited scope,
and is but one element to be considered in trying to understand the effect of
feedlots on property values. Studies like this feedlot study are easily misused.
As recently as spring 2004, the study was cited in an application for a permit
to build a feedlot in Starke county, Indiana, claiming that the study supports the
positive effect of feedlots on property values, confusing association with causation,
and inferring generalizability to other locations without any logical foundation for
doing so.
4.3
SAMPLING FROM A NORMAL POPULATION
Much of the intuition for the use of least squares estimation is based on the assump-
tion that the observed data are a sample from a multivariate normal population.
While the assumption of multivariate normality is almost never tenable in practical
regression problems, it is worthwhile to explore the relevant results for normal
data, ﬁrst assuming random sampling and then removing that assumption.
Suppose that all of the observed variables are normal random variables, and the
observations on each case are independent of the observations on each other case.
In a two-variable problem, for the ith case observe (xi, yi), and suppose that
 xi
yi

∼N
 µx
µy

,

σ 2
x
ρxyσxσy
ρxyσxσy
σ 2
y

(4.9)
Equation (4.9) says that xi and yi are each realizations of normal random variables
with means µx and µy, variances σ 2
x and σ 2
y and correlation ρxy. Now, suppose
we consider the conditional distribution of yi given that we have already observed
the value of xi. It can be shown (see e.g., Lindgren, 1993; Casella and Berger,
1990) that the conditional distribution of yi given xi, is normal and,
yi|xi ∼N

µy + ρxy
σy
σx
(xi −µx), σ 2
y (1 −ρ2
xy)

(4.10)
If we deﬁne
β0 = µy −β1µx
β1 = ρxy
σy
σx
σ 2 = σ 2
y (1 −ρ2
xy)
(4.11)
then the conditional distribution of yi given xi is simply
yi|xi ∼N(β0 + β1xi, σ 2)
(4.12)
which is essentially the same as the simple regression model with the added assump-
tion of normality.

MORE ON R2
81
Given random sampling, the ﬁve parameters in (4.9) are estimated, using the
notation of Table 2.1, by
ˆµx = x
ˆσ 2
x = SD2
x
ˆρxy = rxy
ˆµy = y
ˆσ 2
y = SD2
y
(4.13)
Estimates of β0 and β1 are obtained by substituting estimates from (4.13) for
parameters in (4.11), so that ˆβ1 = rxySDy/SDx, and so on, as derived in Chapter 2.
However, ˆσ 2 = [(n −1)/(n −2)]SD2
y(1 −r2
xy) to correct for degrees of freedom.
If the observations on the ith case are yi and a p × 1 vector xi not including a
constant, multivariate normality is shown symbolically by
 xi
yi

∼N
 µx
µy

,
 xx
xy
xy
σ 2
y

(4.14)
where xx is a p × p matrix of variances and covariances between the elements
of xi and xy is a p × 1 vector of covariances between xi and yi. The conditional
distribution of yi given xi is then
yi|xi ∼N

(µy −β∗′µx) + β∗′xi, σ 2
(4.15)
If R2 is the population multiple correlation,
β∗= −1
xx xy;
σ 2 = σ 2
y ′
xy−1
xx xy = σ 2
y (1 −R2)
The formulas for ˆβ
∗and σ 2 and the formulas for their least squares estimators
differ only by the substitution of estimates for parameters, with (n −1)−1(X ′X)
estimating xx, and (n −1)−1(X ′Y) estimating xy.
4.4
MORE ON R2
The conditional distribution in (4.10) or (4.15) does not depend on random sam-
pling, but only on normal distributions, so whenever multivariate normality seems
reasonable, a linear regression model is suggested for the conditional distribution
of one variable, given the others. However, if random sampling is not used, some
of the usual summary statistics, including R2, lose their connection to population
parameters.
Figure 4.2a repeats Figure 1.1, the scatterplot of Dheight versus Mheight for
the heights data. These data closely resemble a bivariate normal sample, and so
R2 = 0.24 estimates the population R2 for this problem. Figure 4.2b repeats this
last ﬁgure, except that all cases with Mheight between 61 and 64 inches—the lower
and upper quartile of the mother’s heights rounded to the nearest inch—have been
removed form the data. The ols regression line appears similar, but the value of
R2 = 0.37 is about 50% larger. By removing the middle of the data, we have made

82
DRAWING CONCLUSIONS
D height
55
60
65
70
55
60
65
70
55
60
65
70
55
60
65
70
(b) Outer
(a) All
(c) Inner M height
FIG. 4.2
Three views of the heights data.
R2 larger, and it no longer estimates a population value. Similarly, in Figure 4.2c,
we exclude all the cases with Mheight outside the quartiles, and get R2 = 0.027,
and the relationship between Dheight and Mheight virtually disappears.
This example points out that even in the unusual event of analyzing data drawn
from a multivariate normal population, if sampling of the population is not random,
the interpretation of R2 may be completely misleading, as this statistic will be
strongly inﬂuenced by the method of sampling. In particular, a few cases with
unusual values for the predictors can largely determine the observed value of this
statistic.
We have seen that we can manipulate the value of R2 merely by changing our
sampling plan for collecting data: if the values of the terms are widely dispersed,

MORE ON R2
83
then R2 will tend to be too large, while if the values are over a very small range,
then R2 will tend to be too small. Because the notion of proportion of variability
explained is so useful, a diagnostic method is needed to decide if it is a useful
concept in any particular problem.
4.4.1
Simple Linear Regression and R2
In simple regression linear problems, we can always determine the appropriateness
of R2 as a summary by examining the summary graph of the response versus the
predictor. If the plot looks like a sample from a bivariate normal population, as in
Figure 4.2a, then R2 is a useful measure. The less the graph looks like this ﬁgure,
the less useful is R2 as a summary measure.
Figure 4.3 shows six summary graphs. Only for the ﬁrst three of them is R2
a useful summary of the regression problem. In Figure 4.3e, the mean function
appears curved rather than straight so correlation is a poor measure of depen-
dence. In Figure 4.3d the value of R2 is virtually determined by one point, making
R2 necessarily unreliable. The regular appearance of the remaining plot suggests
a different type of problem. We may have several identiﬁable groups of points
caused by a lurking variable not included in the mean function, such that the
mean function for each group has a negative slope, but when groups are com-
bined the slope becomes positive. Once again R2 is not a useful summary of this
graph.
Predictor or y^
(a)
(b)
(e)
(f)
(c)
Response
(d)
FIG. 4.3
Six summary graphs. R2 is an appropriate measure for a–c, but inappropriate for d–f.

84
DRAWING CONCLUSIONS
4.4.2
Multiple Linear Regression
In multiple linear regression, R2 can also be interpreted as the square of the
correlation in a summary graph, this time of Y versus ﬁtted values ˆY. This plot can
be interpreted exactly the same way as the plot of the response versus the single
term in simple linear regression to decide on the usefulness of R2 as a summary
measure.
For other regression methods such as nonlinear regression, we can deﬁne R2 to
be the square of the correlation between the response and the ﬁtted values, and use
this summary graph to decide if R2 is a useful summary.
4.4.3
Regression through the Origin
With regression through the origin, the proportion of variability explained is given
by 1 −SSreg/  y2
i , using uncorrected sums of squares. This quantity is not invari-
ant under location change, so, for example, if units are changed from Fahrenheit to
Celsius, you will get a different value for the proportion of variability explained.
For this reason, use of an R2-like measure for regression through the origin is not
recommended.
4.5
MISSING DATA
In many problems, some variables will be unrecorded for some cases. The methods
we study in this book generally assume and require complete data, without any
missing values. The literature on analyzing incomplete data problems is very large,
and our goal here is more to point out the issues than to provide solutions. Two
recent books on this topic are Little and Rubin (1987) and Schafer (1997).
4.5.1
Missing at Random
The most common solution to missing data problems is to delete either cases or
variables so the resulting data set is complete. Many software packages delete
partially missing cases by default, and ﬁt regression models to the remaining,
complete, cases. This is a reasonable approach as long as the fraction of cases
deleted is small enough, and the cause of values being unobserved is unrelated to
the relationships under study. This would include data lost through an accident like
dropping a test tube, or making an illegible entry in a logbook. If the reason for not
observing values depends on the values that would have been observed, then the
analysis of data may require modeling the cause of the failure to observe values.
For example, if values of a measurement are unrecorded if the value is less than the
minimum detection limit of an instrument, then the value is missing because the
value that should have been observed is too small. A simple expedient in this case
that is sometimes helpful is to substitute a value less than or equal to the detection
limit for the unobserved values. This expedient is not always entirely satisfactory
because substituting, or imputing, a ﬁxed value for the unobserved quantity can
reduce the variation on the ﬁlled-in variable, and yield misleading inferences.

MISSING DATA
85
As a second example, suppose we have a clinical trial that enrolls subjects
with a particular medical condition, assigns each subject a treatment, and then the
subjects are followed for a period of time to observe their response, which may
be time until a particular landmark occurs, such as improvement of the medical
condition. Subjects who do not respond well to the treatment may drop out of
the study early, while subjects who do well may be more likely to remain in the
study. Since the probability of observing a value depends on the value that would
have been observed, simply deleting subjects who drop out early can easily lead to
incorrect inferences because the successful subjects will be overrepresented among
those who complete the study.
In many clinical trials, the response variable is not observed because the study
ends, not because of patient characteristics. In this case, we call the response times
censored; so for each patient, we know either the time to the landmark or the time
to censoring. This is a different type of missing data problem, and analysis needs
to include both the uncensored and censored observations. Book-length treatments
of censored survival data are given by Kalbﬂeisch and Prentice (1980) and Cox
and Oakes (1984), among others.
As a ﬁnal example, consider a cross-cultural demographic study. Some demo-
graphic variables are harder to measure than others, and some variables, such as
the rate of employment for women over the age of 15, may not be available for
less-developed countries. Deleting countries that do not have this variable measured
could change the population that is studied by excluding less-developed countries.
Rubin (1976) deﬁned data to be missing at random (MAR) if the failure to
observe a value does not depend on the value that would have been observed.
With MAR data, case deletion can be a useful option. Determining whether an
assumption of MAR is appropriate for a particular data set is an important step in
the analysis of incomplete data.
4.5.2
Alternatives
All the alternatives we brieﬂy outline here require strong assumptions concerning
the data that may be impossible to check in practice.
Suppose ﬁrst that we combine the response and predictors into a single vector
Z. We assume that the distribution of Z is fully known, apart from unknown
parameters. The simplest assumption is that Z ∼N(µ, ). If we had reasonable
estimates of µ and , then we could use (4.15) to estimate parameters for the
regression of the response on the other terms. The EM algorithm (Dempster, Laird,
and Rubin , 1977) is a computational method that is used to estimate the parameters
of the known joint distribution based on data with missing values.
Alternatively, given a model for the data like multivariate normality, one could
impute values for the missing data and then analyze the completed data as if it were
fully observed. Multiple imputation carries this one step further by creating several
imputed data sets that, according to the model used, are plausible, ﬁlled-in data
sets, and then “average” the analyses of the ﬁlled-in data sets. Software for both
imputation and the EM algorithm for maximum likelihood estimate is available

86
DRAWING CONCLUSIONS
in several standard statistical packages, including the “missing” package in S-plus
and the “MI” procedure in SAS.
The third approach is more comprehensive, as it requires building a model for
the process of interest and the missing data process simultaneously. Examples of
this approach are given by Ibrahim, Lipsitz, and Horton (2001), and Tang, Little,
and Raghunathan (2003).
The data described in Table 4.2 provides an example. Allison and Cicchetti
(1976) presented data on sleep patterns of 62 mammal species along with several
other possible predictors of sleep. The data were in turn compiled from several
other sources, and not all values are measured for all species. For example, PS, the
number of hours of paradoxical sleep, was measured for only 50 of the 62 species in
the data set, and GP, the gestation period, was measured for only 58 of the species.
If we are interested in the dependence of hours of sleep on the other predictors,
then we have at least three possible responses, PS, SWS, and TS, all observed on
only a subset of the species. To use case deletion and then standard methods to
analyze the conditional distributions of interest, we need to assume that the chance
of a value being missing does not depend on the value. For example, the four
missing values of GP are missing because no one had (as of 1976) published this
value for these species. Using the imputation or the maximum likelihood methods
are alternatives for these data, but they require making assumptions like normality,
which might be palatable for many of the variables if transformed to logarithmic
scale. Some of the variables, like P and SE are categorical, so other assumptions
beyond multivariate normality might be needed.
TABLE 4.2
The Sleep Dataa
Number
Percent
Variable
Type
Observed
Missing
Description
BodyWt
Variate
62
0
Body weight in kg
BrainWt
Variate
62
0
Brain weight in g
D
Factor
62
0
Danger index, 1 = least danger,
. . . , 5 = most
GP
Variate
58
6
Gestation time, days
Life
Variate
58
6
Maximum life span, years
P
Factor
62
0
Predation index, 1 = lowest , . . . ,
5 = highest
SE
Factor
62
0
Sleep exposure index, 1 = more
exposed, . . . , 5 = most protected
PS
Response
50
19
Paradoxical dreaming sleep, hrs/day
SWS
Response
48
23
Slow wave nondreaming sleep,
hrs/day
TS
Response
58
6
Total sleep, hrs/day
Species
Labels
62
0
Species of mammal
a10 variables, 62 observations, 8 patterns of missing values; 5 variables (50%) have at least one missing
value; 20 observations (32%) have at least one missing value.

COMPUTATIONALLY INTENSIVE METHODS
87
4.6
COMPUTATIONALLY INTENSIVE METHODS
Suppose we have a sample y1, . . . , yn from a particular distribution G, for example
a standard normal distribution. What is a conﬁdence interval for the population
median?
We can obtain an approximate answer to this question by computer simulation,
set up as follows:
1. Obtain a simulated random sample y∗
1, . . . ., y∗
n from the known distribution
G. Most statistical computing languages include functions for simulating
random deviates (see Thisted, 1988 for computational methods).
2. Compute and save the median of the sample in step 1.
3. Repeat steps 1 and 2 a large number of times, say B times. The larger the
value of B, the more precise the ultimate answer.
4. If we take B = 999, a simple percentile-based 95% conﬁdence interval for
the median is the interval between the 25th smallest value and the 975th
largest value, which are the sample 2.5 and 97.5 percentiles, respectively.
In most interesting problems, we will not actually know G and so this simu-
lation is not available. Efron (1979) pointed out that the observed data can be
used to estimate G, and then we can sample from the estimate ˆG. The algorithm
becomes:
1. Obtain a random sample y∗
1, . . . , y∗
n from ˆG by sampling with replacement
from the observed values y1, . . . , yn. In particular, the i-th element of the
sample y∗
i is equally likely to be any of the original y1, . . . , yn. Some of
the yi will appear several times in the random sample, while others will not
appear at all.
2. Continue with steps 2–4 of the ﬁrst algorithm. A test at the 5% level con-
cerning the population median can be rejected if the hypothesized value of
the median does not fall in the conﬁdence interval computed at step 4.
Efron called this method the bootstrap, and we call B the number of bootstrap sam-
ples. Excellent references for the bootstrap are the books by Efron and Tibshirani
(1993), and Davison and Hinkley (1997).
4.6.1
Regression Inference without Normality
Bootstrap methods can be applied in more complex problems like regression. Infer-
ences and accurate standard errors for parameters and mean functions require either
normality of regression errors or large sample sizes. In small samples without nor-
mality, standard inference methods can be misleading, and in these cases a bootstrap
can be used for inference.

88
DRAWING CONCLUSIONS
Transactions Data
The data in this example consists of a sample of branches of a large Australian
bank (Cunningham and Heathcote, 1989). Each branch makes transactions of two
types, and for each of the branches we have recorded the number of transactions
T1 and T2, as well as Time, the total number of minutes of labor used by the branch
in type 1 and type 2 transactions. If βj is the average number of minutes for a
transaction of type j, j = 1, 2, then the total number of minutes in a branch for
transaction type j is βjTj, and the total number of minutes is expected to be
E(Time|T1, T2) = β0 + β1T1 + β2T2
(4.16)
possibly with β0 = 0 because zero transactions should imply zero time spent. The
data are displayed in Figure 4.4, and are given in the data ﬁle transact.txt.
The key features of the scatterplot matrix are: (1) the marginal response plots in
the last row appear to have reasonably linear mean functions; (2) there appear to
be a number of branches with no T1 transactions but many T2 transactions; and
(3) in the plot of Time versus T2, variability appears to increase from left to right.
The errors in this problem probably have a skewed distribution. Occasional
transactions take a very long time, but since transaction time is bounded below by
T1
0
2000
4000
6000
0
500
1000
1500
0
2000
4000
6000
T2
0
500
1000
1500
0
5000
15,000
0
5000
15,000
Time
FIG. 4.4
Scatterplot matrix for the transactions data.

COMPUTATIONALLY INTENSIVE METHODS
89
TABLE 4.3
Summary for B = 999 Case Bootstraps for the Transactions Data,
Giving 95% Conﬁdence Intervals, Lower to Upper, Based on Standard Normal
Theory and on the Percentile Bootstrap
Normal Theory
Bootstrap
Estimate
Lower
Upper
Estimate
Lower
Upper
Intercept
144.37
−191.47
480.21
136.09
−254.73
523.36
T1
5.46
4.61
6.32
5.48
4.08
6.77
T2
2.03
1.85
2.22
2.04
1.74
2.36
zero, there can not be any really extreme “quick” transactions. Inferences based on
normal theory are therefore questionable.
Following the suggestion of Pardoe and Weisberg (2001) for this example, a
bootstrap is computed as follows:
1. Number the cases in the data set from 1 to n. Take a random sample with
replacement of size n from these case numbers. Thus, the i-th case number
in the sample is equally likely to be any of the n cases in the original data.
2. Create a data set from the original data, but repeating each row in the data
set the number of times that row was selected in the random sample in
step 1. Some cases will appear several times and others will not appear at
all. Compute the regression using this data set, and save the values of the
coefﬁcient estimates.
3. Repeat steps 1 and 2 a large number of times, say, B times.
4. Estimate a 95% conﬁdence interval for each of the estimates by the 2.5 and
97.5 percentiles of the sample of B bootstrap samples.
Table 4.3 summarizes the percentile bootstrap for the transactions data. The column
marked Estimate gives the ols estimate under “Normal theory” and the average
of the B bootstrap simulations under “Bootstrap.” The difference between these
two is called the bootstrap bias, which is quite small for all three terms relative to
the size of the conﬁdence intervals. The 95% bootstrap intervals are consistently
wider than the corresponding normal intervals, indicating that the normal-theory
conﬁdence intervals are probably overly optimistic. The bootstrap intervals given
in Table 4.3 are random, since if the bootstrap is repeated, the answers will be a
little different. The variability in the end-points of the interval can be decreased by
increasing the number B of bootstrap samples.
4.6.2
Nonlinear Functions of Parameters
One of the important uses of the bootstrap is to get estimates of error variability
in problems where standard theory is either missing, or, equally often, unknown
to the analyst. Suppose, for example, we wanted to get a conﬁdence interval for
the ratio β1/β2 in the transactions data. This is the ratio of the time for a type 1

90
DRAWING CONCLUSIONS
transaction to the time for a type 2 transaction. The point estimate for this ratio is
just ˆβ1/ ˆβ2, but we will not learn how to get a normal-theory conﬁdence interval for
a nonlinear function of parameters like this until Section 6.1.2. Using the bootstrap,
this computation is easy: just compute the ratio in each of the bootstrap samples and
then use the percentiles of the bootstrap distribution to get the conﬁdence interval.
For these data, the point estimate is 2.68 with 95% bootstrap conﬁdence interval
from 1.76 to 3.86, so with 95% conﬁdence, type 1 transactions take on average
from about 1.76 to 3.86 times as long as do type 2 transactions.
4.6.3
Predictors Measured with Error
Predictors and the response are often measured with error. While we might have
a theory that tells us the mean function for the response, given the true values
of the predictors, we must ﬁt with the response, given the imperfectly measured
values of the predictors. We can sometimes use simulation to understand how the
measurement error affects our answers.
Here is the basic setup. We have a true response Y ∗and a set of terms X∗and
a true mean function
E(Y ∗|X∗= x∗) = β′x∗
In place of Y ∗and X∗we observe Y = Y ∗+ δ and X = X∗+ η, where δ and η
are measurement errors. If we ﬁt the mean function
E(Y|X = x) = γ ′x
what can we say about the relationship between β and γ ? While there is a sub-
stantial theoretical literature on this problem (for example, Fuller, 1987), we shall
attempt to get an answer to this question using simulation. To do so, we need to
know something about δ and η.
Catchability of Northern Pike
One of the questions of interest to ﬁsheries managers is the difﬁculty of catching a
ﬁsh. A useful concept is the idea of catchability. Suppose that Y ∗is the catch for
an angler for a ﬁxed amount of effort, and X∗is the abundance of ﬁsh available
in the population that the angler is ﬁshing. Suppose further that
E(Y ∗|X∗= x∗) = β1x∗
(4.17)
If this mean function were to hold, then we could deﬁne β1 to be the catchability
of this particular ﬁsh species.
The data we use comes from a study of Northern Pike, a popular game ﬁsh
in inland lakes in the United States. Data were collected on 16 lakes by Rob
Pierce of the Minnesota Department of Natural Resources. On each lake we have
a measurement called CPUE or catch per unit effort, which is the catch for a

COMPUTATIONALLY INTENSIVE METHODS
91
speciﬁc amount of ﬁshing effort. Abundance on the lake is measured using the ﬁsh
Density that is deﬁned to be the number of ﬁsh in the lake divided by the surface
area of the lake. While surface area can be determined with reasonable accuracy,
the number of ﬁsh in the lake is estimated using a capture–recapture experiment
(Seber, 2002). Since both CPUE and Density are experimentally estimated, they
both have standard errors attached to them. In terms of (4.17), we have observed
CPUE = Y ∗+ δ and Density = x∗+ η. In addition, we can obtain estimates of the
standard deviations of the δs and ηs from the properties of the methods used to
ﬁnd CPUE and Density. The data ﬁle npdata.txt includes both the CPUE and
Density and their standard errors SECPUE and SEdens.
Figure 4.5 is the plot of the estimated CPUE and Density. Ignoring the lines on
the graph, a key characteristic of this graph is the large variability in the points. A
straight line mean function seems plausible for these data, but many other curves
are equally plausible. We continue under the assumption that a straight-line mean
function is sensible.
The two lines on Figure 4.5 are the ols simple regression ﬁts through the origin
(solid line) and not through the origin (dashed line). The F-test comparing them
has a p-value of about 0.13, so we are encouraged to use the simpler through-
the-origin model that will allow us to interpret the slope as the catchability. The
estimate is ˆβ1 = 0.34 with standard error 0.035, so a 95% conﬁdence interval for
β1 ignoring measurement errors is (0.250, 0.0399).
To assess the effect of measurement error on the estimate and on the conﬁdence
interval, we ﬁrst make some assumptions. First, we suppose that the estimated stan-
dard errors of the measurements are the actual standard errors of the measurements.
Second, we assume that the measurement errors are independently and normally
0
10
20
30
40
50
0
5
10
15
20
25
Estimated density
Estimated CPUE
FIG. 4.5
Scatterplot of estimated CPUE versus Density for the northern pike data. Solid line is the
ols mean function through the origin, and the dashed line is the ols line allowing an intercept.

92
DRAWING CONCLUSIONS
TABLE 4.4
Simulation Summary for the Northern Pike
Data
Point Estimate
95% ConﬁdenceInterval
Normal theory
0.324
(0.250, 0.399)
Simulation
0.309
(0.230, 0.387)
distributed. Neither of these assumptions are checkable from these data, but for the
purposes of a simulation these seem like reasonable assumptions.
The simulation proceeds as follows:
1. Generate a pseudo-response vector given by ˜Y = CPUE + ˜δ, where the i-th
element of ˜δ is a normal random number with mean zero and variance given
by the square of the estimated standard error for the i-th CPUE value. In this
problem, each observation has its own estimated error variance, but in other
problems there may be a common estimate for all elements of the response.
2. Repeat step 1, but for the predictor to get ˜x = Density + ˜η.
3. Fit the simple regression model of ˜Y on ˜x and save the estimated slope.
4. Repeat steps 1–3 B times. The average of the B values of the slope estimate
is an estimate of the slope in the problem with no measurement error. A con-
ﬁdence interval for the slope is found using the percentile method discussed
with the bootstrap.
The samples generated in steps 1–2 are not quite from the right distribution, as
they are centered at the observed values of CPUE and Density rather than the
unobserved values of Y ∗and x∗, but the observed values estimate the unobserved
true values, so this substitution adds variability to the results, but does not affect
the validity of the methodology.
The results for B = 999 simulations are summarized in Table 4.4. The results of
the normal theory and the simulation that allows for measurement error are remark-
ably similar. In this problem, we judge the measurement error to be unimportant.
PROBLEMS
4.1. Fit the regression of Soma on AVE, LIN and QUAD as deﬁned in Section 4.1
for the girls in the Berkeley Guidance Study data, and compare to the results
in Section 4.1.
4.2.
4.2.1. Starting with (4.10), we can write
yi = µy + ρxy
σy
σx
(xi −µx) + εi

PROBLEMS
93
Ignoring the error term εi, solve this equation for xi as a function of
yi and the parameters.
4.2.2. Find the conditional distribution of xi|yi. Under what conditions is
the equation you obtained in Problem 4.2.1, which is computed by
inverting the regression of y on x, the same as the regression of
x on y?
4.3. For the transactions data described in Section 4.6.1, deﬁne A = (T1 + T2)/2
to be the average transaction time, and D = T1 −T2, and ﬁt the following
four mean functions
M1 : E(Y|T1, T2) = β01 + β11T1 + β21T2
M2 : E(Y|T1, T2) = β02 + β32A + β42D
M3 : E(Y|T1, T2) = β03 + β23T2 + β43D
M4 : E(Y|T1, T2) = β04 + β14T1 + β24T2 + β34A + β44D
4.3.1. In the ﬁt of M4, some of the coefﬁcients estimates are labelled as
either “aliased” or as missing. Explain what this means.
4.3.2. What aspects of the ﬁtted regressions are the same? What is different?
4.3.3. Why is the estimate for T2 different in M1 and M3?
4.4. Interpreting coefﬁcients with logarithms
4.4.1. For the simple regression with mean function E(log(Y)|X = x) =
β0 + β1 log(x), provide an interpretation for β1 as a rate of change in
Y for a small change in x.
4.4.2. Show that the results of Section 4.1.7 do not depend on the base of
the logarithms.
4.5. Use the bootstrap to estimate conﬁdence intervals of the coefﬁcients in the
fuel data.
4.6. Windmill data For the windmill data in the data ﬁle wm1.txt discussed in
Problem 2.13, page 45, use B = 999 replications of the bootstrap to estimate
a 95% conﬁdence interval for the long-term average wind speed at the candi-
date site and compare this to the prediction interval in Problem 2.13.5. See the
comment at the end of Problem 2.13.4 to justify using a bootstrap conﬁdence
interval for the mean as a prediction interval for the long-term mean.
4.7. Suppose we ﬁt a regression with the true mean function
E(Y|X1 = x1, X2 = x2) = 3 + 4x1 + 2x2
Provide conditions under which the mean function for E(Y|X1 = x1) is linear
but has a negative coefﬁcient for x1.

94
DRAWING CONCLUSIONS
4.8. In a study of faculty salaries in a small college in the Midwest, a linear
regression model was ﬁt, giving the ﬁtted mean function
E(

Salary|Sex) = 24697 −3340Sex
(4.18)
where Sex equals one if the faculty member was female and zero if male.
The response Salary is measured in dollars (the data are from the 1970s).
4.8.1. Give a sentence that describes the meaning of the two estimated
coefﬁcients.
4.8.2. An alternative mean function ﬁt to these data with an additional term,
Years, the number of years employed at this college, gives the esti-
mated mean function
E(

Salary|Sex, Years) = 18065 + 201Sex + 759Years
(4.19)
The important difference between these two mean functions is that the
coefﬁcient for Sex has changed signs. Using the results of this chapter,
explain how this could happen. (Data consistent with these equations
are presented in Problem 6.13).
4.9. Sleep data
4.9.1. For the sleep data described in Section 4.5, describe conditions under
which the missing at random assumption is reasonable. In this case,
deleting the partially observed species and analyzing the complete data
can make sense.
4.9.2. Describe conditions under which the missing at random assumption
for the sleep data is not reasonable. In this case, deleting partially
observed species can change the inferences by changing the deﬁnition
of the sampled population.
4.9.3. Suppose that the sleep data were fully observed, meaning that values
for all the variables were available for all 62 species. Assuming that
there are more than 62 species of mammals, provide a situation where
examining the missing at random assumption could still be important.
4.10. The data given in longley.txt were ﬁrst given by Longley (1967) to
demonstrate inadequacies of regression computer programs then available.
The variables are:
Def = GNP price deﬂator, in percent
GNP = GNP, in millions of dollars
Unemployed = Unemployment, in thousands of persons
Armed.Forces = Size of armed forces, in thousands
Population = Population 14 years of age and over, in thousands

PROBLEMS
95
Employed = Total derived employment in thousands the response
Year = Year
4.10.1. Draw the scatterplot matrix for these data excluding Year, and explain
from the plot why this might be a good example to illustrate numer-
ical problems of regression programs. (Hint: Numerical problems
arise through rounding errors, and these are most likely to occur
when terms in the regression model are very highly correlated.)
4.10.2. Fit the regression of Employed on the others excluding Year.
4.10.3. Suppose that the values given in this example were only accurate to
three signiﬁcant ﬁgures (two ﬁgures for Def). The effects of mea-
surement errors can be assessed using a simulation study in which
we add uniform random values to the observed values, and recom-
pute estimates for each simulation. For example, Unemp for 1947
is given as 2356, which corresponds to 2,356,000. If we assume
only three signiﬁcant ﬁgures, we only believe the ﬁrst three digits.
In the simulation we would replace 2356 by 2356 + u, where u is a
uniform random number between −5 and +5. Repeat the simulation
1000 times, and on each simulation compute the coefﬁcient estimates.
Compare the standard deviation of the coefﬁcient estimates from the
simulation to the coefﬁcient standard errors from the regression on
the unperturbed data. If the standard deviations in the simulation are
as large or larger than the standard errors, we would have evidence
that rounding would have important impact on results.

C H A P T E R 5
Weights, Lack of Fit, and More
This chapter introduces a number of additional basic tools for ﬁtting and using
multiple linear regression models.
5.1
WEIGHTED LEAST SQUARES
The assumption that the variance function Var(Y|X) is the same for all values of
the terms X can be relaxed in a number of ways. In the simplest case, suppose we
have the multiple regression mean function given for the ith case by
E(Y|X = xi) = β′xi
(5.1)
but rather than assume that errors are constant, we assume that
Var(Y|X = xi) = Var(ei) = σ 2/wi
(5.2)
where w1, . . . , wn are known positive numbers. The variance function is still char-
acterized by only one unknown positive number σ 2, but the variances can be
different for each case. This will lead to the use of weighted least squares, or wls,
in place of ols, to get estimates.
In matrix terms, let W be an n × n diagonal matrix with the wi on the diagonal.
The model we now use is
Y = Xβ + e
Var(e) = σ 2W−1
(5.3)
We will continue to use the symbol ˆβ for the estimator of β, even though the
estimate will be obtained via weighted, not ordinary, least squares. The estimator
ˆβ is chosen to minimize the weighted residual sum of squares function,
RSS(β) = (Y −Xβ)′W(Y −Xβ)
(5.4)
=

wi(yi −x′
iβ)2
(5.5)
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
96

WEIGHTED LEAST SQUARES
97
The use of the weighted residual sum of squares recognizes that some of the errors
are more variable than others since cases with large values of wi will have small
variance and will therefore be given more weight in the weighted RSS. The wls
estimator is given by
ˆβ = (X′WX)−1X′WY
(5.6)
While this last equation can be found directly, it is convenient to transform the
problem speciﬁed by (5.3) to one that can be solved by ols. Then, all of the results
for ols can be applied to wls problems.
Let W1/2 be the n × n diagonal matrix with ith diagonal element √wi, and so
W−1/2 is a diagonal matrix with 1/√wi on the diagonal, and W1/2W−1/2 = I.
Using Appendix A.7 on random vectors, the covariance matrix of W1/2e is
Var(W1/2e) = W1/2Var(e)W1/2
= W1/2(σ 2W−1)W1/2
= W1/2(σ 2W−1/2W−1/2)W1/2
= σ 2(W1/2W−1/2)(W−1/2W1/2)
= σ 2I
(5.7)
This means that the vector W1/2e is a random vector but with covariance matrix
equal to σ 2 times the identity matrix. Multiplying both sides of equation (5.3) by
W1/2 gives
W1/2Y = W1/2Xβ + W1/2e
(5.8)
Deﬁne Z = W1/2Y, M = W1/2X, and d = W1/2e, and (5.8) becomes
Z = Mβ + d
(5.9)
From (5.7), Var(d) = σ 2I, and in (5.9) β is exactly the same as β in (5.3). Model
(5.9) can be solved using ols,
ˆβ = (M′M)−1M′Z
=

(W1/2X)′(W1/2X)
−1
(W1/2X)′(W1/2Y)
=

X′W1/2W1/2X
−1
(X′W1/2W1/2Y)
=

X′WX
−1 (X′WY)
which is the estimator given at (5.6).

98
WEIGHTS, LACK OF FIT, AND MORE
To summarize, the wls regression of Y on X with weights given by the diagonal
elements of W is the same as the ols regression of Z on M, where
M =


√w1
√w1x11
· · ·
√w1x1p
√w2
√w2x21
· · ·
√w2x2p
...
...
...
...
√wn
√wnxn1
· · ·
√wnxnp


Z =


√w1y1
√w2y2
...
√wnyn


Even the column of ones gets multiplied by the √wi. The regression problem is
then solved using M and Z in place of X and Y.
5.1.1
Applications of Weighted Least Squares
Known weights wi can occur in many ways. If the ith response is an average of ni
equally variable observations, then Var(yi) = σ 2/ni, and wi = ni. If yi is a total
of ni observations, Var(yi) = niσ 2, and wi = 1/ni. If variance is proportional to
some predictor xi, Var(yi) = xiσ 2, then wi = 1/xi.
Strong Interaction
The purpose of the experiment described here is to study the interactions of unstable
elementary particles in collision with proton targets (Weisberg et al., 1978). These
particles interact via the so-called strong interaction force that holds nuclei together.
Although the electromagnetic force is well understood, the strong interaction is
somewhat mysterious, and this experiment was designed to test certain theories of
the nature of the strong interaction.
The experiment was carried out with beam having various values of incident
momentum, or equivalently for various values of s, the square of the total energy
in the center-of-mass frame of reference system. For each value of s, we observe
the scattering cross-section y, measured in millibarns (µb). A theoretical model of
the strong interaction force predicts that
E(y|s) = β0 + β1s−1/2 + relatively small terms
(5.10)
The theory makes quantitative predictions about β0 and β1 and their dependence on
particular input and output particle type. Of interest, therefore, are: (1) estimation of
β0 and β1, given (5.10) and (2) assessment of whether (5.10) provides an accurate
description of the observed data.
The data given in Table 5.1 and in the ﬁle physics.txt summarize the
results of experiments when both the input and output particle was the π−meson.
At each value of s, a very large number of particles was counted, and as a result
the values of Var(y|s = si) = σ 2/wi are known almost exactly; the square roots of
these values are given in the third column of Table 5.1, labelled SDi. The variables
in the ﬁle are labelled as x, y, and SD, corresponding to Table 5.1.
Ignoring the smaller terms, mean function (5.10) is a simple linear regression
mean function with terms for an intercept and x = s−1/2. We will need to use wls,

WEIGHTED LEAST SQUARES
99
TABLE 5.1
The Strong Interaction Data
x = s−1/2
y (µb)
SDi
0.345
367
17
0.287
311
9
0.251
295
9
0.225
268
7
0.207
253
7
0.186
239
6
0.161
220
6
0.132
213
6
0.084
193
5
0.060
192
5
TABLE 5.2
wls Estimates for the Strong Interaction Data
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
148.473
8.079
18.38 7.91e-08
x
530.835
47.550
11.16 3.71e-06
Residual standard error: 1.657 on 8 degrees of freedom
Multiple R-Squared: 0.9397
Analysis of Variance Table
Df Sum Sq Mean Sq F value
Pr(>F)
x
1 341.99
341.99
124.63 3.710e-06
Residuals
8
21.95
2.74
because the variances are not constant but are different for each value of s. In
this problem, we are in the unusual situation that we not only know the weights
but also know the value of σ 2/wi for each value of i. There are 11 quantities
w1, . . . , w10 and σ 2 that describe the values of only 10 variances, so we have too
many parameters, and we are free to specify one of the 11 parameters to be any
nonzero value we like. The simplest approach is to set σ 2 = 1. If σ 2 = 1, then the
last column of Table 5.1 gives 1/√wi, i = 1, 2, . . . , n, and so the weights are just
the inverse squares of the last column of this table.
The ﬁt of the simple regression model via wls is summarized in Table 5.2. R2
is large, and the parameter estimates are well determined. The next question is
whether (5.10) does in fact ﬁt the data. This question of ﬁt or lack of ﬁt of a model
is the subject of the next section.
5.1.2
Additional Comments
Many statistical models, including mixed effects, variance components, time series,
and some econometric models, will specify that Var(e) = , where  is an n × n
symmetric matrix that depends on a small number of parameters. Estimates for

100
WEIGHTS, LACK OF FIT, AND MORE
the generalized least squares problem minimize (5.4), with W replaced by −1.
Pinheiro and Bates (2000) is one recent source for discussion of these models.
If many observations are taken at each value of x, the inverse of the sample
variance of y given x can provide useful estimated weights. This method was used
to get weights in the strong interaction data, where the number of cases per value
of x was extremely large. Problem 5.6 provides another example using estimated
weights as if they were true weights. The usefulness of this method depends on
having a large sample size at each value of x.
In some problems, Var(Y|X) will depend on the mean E(Y|X). For example,
if the response is a count that follows a Poisson distribution, then Var(Y|X) =
E(Y|X), while if the response follows a gamma distribution, Var(Y|X) =
σ 2(E(Y|X))2. The traditional approach to ﬁtting when the variance depends on
the mean is to use a variance stabilizing transformation, as will be described in
Section 8.3, in which the response is replaced by a transformation of it so that the
variance function is approximately constant in the transformed scale.
Nelder and Wedderburn (1972) introduced generalized linear models that extend
linear regression methodology to problems in which the variance function depends
on the mean. One particular generalized linear model when the response is a bino-
mial count leads to logistic regression and is discussed in Chapter 12. The other
most important example of a generalized linear model is Poisson regression and
log-linear models and is discussed by Agresti (1996). McCullagh and Nelder (1989)
provide a general introduction to generalized linear models.
5.2
TESTING FOR LACK OF FIT, VARIANCE KNOWN
When the mean function used in ﬁtting is correct, then the residual mean square
ˆσ 2 provides an unbiased estimate of σ 2. If the mean function is wrong, then ˆσ 2
will estimate a quantity larger than σ 2, since its size will depend both on the errors
and on systematic bias from ﬁtting the wrong mean function. If σ 2 is known, or
if an estimate of it is available that does not depend on the ﬁtted mean function,
a test for lack of ﬁt of the model can be obtained by comparing ˆσ 2 to the model-
free value. If ˆσ 2 is too large, we may have evidence that the mean function is
wrong.
In the strong interaction data, we want to know if the straight-line mean function
(5.10) is correct. As outlined in Section 5.1, the inverses of the squares of the values
in column 3 of Table 5.1 are used as weights when we set σ 2 = 1, a known value.
From Table 5.2, ˆσ 2 = 2.744. Evidence against the simple regression model will be
obtained if we judge ˆσ 2 = 2.744 large when compared with the known value of
σ 2 = 1. To assign a p-value to this comparison, we use the following result.
If the ei ∼NID(0, σ 2/wi), i = 1, 2, . . . , n, with the wi and σ 2 known, and
parameters in the mean function are estimated using wls, and the mean function
is correct, then
X2 = RSS
σ 2 = (n −p′)ˆσ 2
σ 2
(5.11)

TESTING FOR LACK OF FIT, VARIANCE KNOWN
101
is distributed as a chi-squared random variable with n −p′ df. As usual, RSS is
the residual sum of squares. For the example, from Table 5.4,
X2 = 21.953
1
= 21.953
Using a table or a computer program that computes quantiles of the χ2 distribution,
χ2(0.01, 8) = 20.09, so the p-value associated with the test is less than 0.01, which
suggests that the straight-line mean function may not be adequate.
When this test indicates the lack of ﬁt, it is usual to ﬁt alternative mean functions
either by transforming some of the terms or by adding polynomial terms in the
predictors. The available physical theory suggests this latter approach, and the
quadratic mean function
E(y|s) = β0 + β1s−1/2 + β2(s−1/2)2 + smaller terms
≈β0 + β1x + β2x2
(5.12)
with x = s−1/2 should be ﬁt to the data. This mean function has three terms, an
intercept, x = s−1/2, and x2 = s−1. Fitting must use wls with the same weights
as before, as given in Table 5.3. The ﬁtted curve for the quadratic ﬁt is shown in
Figure 5.1. The curve matches the data very closely. We can test for lack of ﬁt of
this model by computing
X2 = RSS
σ 2 = 3.226
1
= 3.226
Comparing this value with the percentage points of χ2(7) gives a p-value of about
0.86, indicating no evidence of lack of ﬁt for mean function (5.12).
TABLE 5.3
wls Estimates for the Quadratic Mean Function for the
Strong Interaction Data
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
183.830
6.459
28.46
1.7e-08
x
0.971
85.369
0.01
0.99124
x^2
1597.505
250.587
6.38
0.00038
Residual standard error: 0.679 on 7 degrees of freedom
Multiple R-Squared: 0.991
Analysis of Variance Table
Response: y
Df
Sum Sq Mean Sq F value
Pr(>F)
x
1 341.991 341.991 742.1846 2.3030e-08
x^2
1
18.727
18.727
40.6413 0.00037612
Residuals
7
3.226
0.461

102
WEIGHTS, LACK OF FIT, AND MORE
0.05
0.10
0.15
0.20
0.25
0.30
0.35
200
250
300
350
Cross section, y
x = s (−1/2)
FIG. 5.1
Scatterplot for the strong interaction data. Solid line: simple linear regression mean function.
Dashed line: quadratic mean function.
Although (5.10) does not describe the data, (5.12) does result in an adequate
ﬁt. Judgment of the success or failure of the model for the strong interaction force
requires analysis of data for other choices of incidence and product particles, as
well as the data analyzed here.
On the basis of this further analysis, Weisberg
et al. (1978) concluded that the theoretical model for the strong interaction force
is consistent with the observed data.
5.3
TESTING FOR LACK OF FIT, VARIANCE UNKNOWN
When σ 2 is unknown, a test for lack of ﬁt requires a model-free estimate of the vari-
ance. The most common model-free estimate makes use of the variation between
cases with the same values on all of the predictors. For example, consider the artiﬁ-
cial data with n = 10 given in Table 5.4. The data were generated by ﬁrst choosing
the values of xi and then computing yi = 2.0 + 0.5xi + ei, i = 1, 2, . . . , 10, where
the ei are standard normal random deviates. If we consider only the values of yi
corresponding to x = 1, we can compute the average response y and the standard
deviation with 3 −1 = 2 df, as shown in the table. If we assume that variance is
the same for all values of x, a pooled estimate of the common variance is obtained
by pooling the individual standard deviations into a single estimate. If n is the
number of cases at a value of x and SD is the standard deviation for that value of
x, then the sum of squares for pure error, symbolically SSpe, is given by
SSpe =

(n −1)SD2
(5.13)

TESTING FOR LACK OF FIT, VARIANCE UNKNOWN
103
TABLE 5.4
An Illustration of the Computation of Pure Error
X
Y
y
(n −1)SD2
SD
df
1
1
1
2.55
2.75
2.57



2.6233
0.0243
0.1102
2
2
2.40
2.4000
0
0
0
3
3
4.19
4.70

4.4450
0.1301
0.3606
1
4
4
4
4
3.81
4.87
2.93
4.52



4.0325
2.2041
0.8571
3
2.3585
6
where the sum is over all groups of cases. For example, SSpe is simply the sum of
the numbers in the fourth column of Table 5.4,
SSpe = 0.0243 + 0.0000 + 0.1301 + 2.2041 = 2.3585
Associated with SSpe is its df, dfpe = (n −1) = 2 + 0 + 1 + 3 = 6. The pooled,
or pure error estimate of variance is ˆσpe = SSpe/dfpe = 0.3931. This is the same
estimate that would be obtained for the residual variance if the data were analyzed
using a one-way analysis of variance, grouping according to the value of x. The pure
error estimate of variance makes no reference to the linear regression mean function.
It only uses the assumption that the residual variance is the same for each x.
Now suppose we ﬁt a linear regression mean function to the data. The analysis
of variance is given in Table 5.5. The residual mean square in Table 5.5 provides
an estimate of σ 2, but this estimate depends on the mean function. Thus we have
two estimates of σ 2, and if the latter is much larger than the former, the model is
inadequate.
We can obtain an F-test if the residual sum of squares in Table 5.5 is divided
into two parts, the sum of squares for pure error, as given in Table 5.4, and
the remainder, called the sum of squares for lack of ﬁt, or SSlof = RSS −SSpe =
4.2166 −2.3585 = 1.8581 with df = n −p′ −dfpe. The F-test is the ratio of the
TABLE 5.5
Analysis of Variance for the Data in Table 5.4
Analysis of Variance Table
Df Sum Sq Mean Sq F value
Pr(>F)
Regression
1 4.5693
4.5693 11.6247 0.01433
Residuals
8 4.2166
0.5271
Lack of fit
2 1.8582
0.9291
2.3638 0.17496
Pure error
6 2.3584
0.3931

104
WEIGHTS, LACK OF FIT, AND MORE
mean square for lack of ﬁt to the mean square for pure error. The observed F = 2.36
is considerably smaller than F(0.05; 2, 6) = 5.14, suggesting no lack of ﬁt of the
model to these data.
Although all the examples in this section have a single predictor, the ideas used
to get a model-free estimate of σ 2 are perfectly general. The pure-error estimate
of variance is based on the sum of squares between the values of the response for
all cases with the same values on all of the predictors.
Apple Shoots
Many types of trees produce two types of morphologically different shoots. Some
branches remain vegetative year after year and contribute considerably to the size
of the tree. Called long shoots or leaders, they may grow as much as 15 or 20 cm
in one growing season. Short shoots, rarely exceeding 1 cm in total length, produce
fruit. To complicate the issue further, long shoots occasionally change to short in
a new growing season and vice versa. The mechanism that the tree uses to control
the long and short shoots is not well understood.
Bland (1978) has done a descriptive study of the difference between long and
short shoots of McIntosh apple trees. Using healthy trees of clonal stock planted in
1933 and 1934, he took samples of long and short shoots from the trees every few
days throughout the 1971 growing season of about 106 days. The shoots sampled
are presumed to be a sample of available shoots at the sampling dates. The sampled
shoots were removed from the tree, marked and taken to the laboratory for analysis.
Among the many measurements taken, Bland counted the number of stem units
in each shoot. The long and the short shoots could differ because of the number of
stem units, the average size of stem units, or both. Bland’s data is given in the data
ﬁles longshoots.txt for data on long shoots, shortshoots.txt for data
on short shoots, and allshoots.txt for both long and short shoots. We will
consider only the long shoots, leaving the short shoots to the problems section.
Our goal is to ﬁnd an equation that can adequately describe the relationship
between Day = days from dormancy and Y = number of stem units. Lacking
a theoretical form for this equation, we ﬁrst examine Figure 5.2, a scatterplot of
average number of stem units versus Day. The apparent linearity of this plot should
encourage us to ﬁt a straight-line mean function,
E(Y|Day) = β0 + β1Day
(5.14)
If this mean function were adequate, we would have the interesting result that the
observed rate of production of stem units per day is constant over the growing
season.
For each sampled day, Table 5.6 reports n = number of shoots sampled, y =
average number of stem units on that day, and SD = within-day standard devia-
tion. Assuming that residual variance is constant from day to day, we can do the
regression in two ways.
On the basis of the summaries given, since Var(y|Day) = σ 2/n, we must com-
pute the wls regression of y on Day with weights given by the values of n. This is

GENERAL F TESTING
105
0
20
40
60
80
100
10
15
20
25
30
Days since dormancy
Number of stem units
FIG. 5.2
Scatterplot for long shoots in the apple shoot data.
summarized in Table 5.7a. Alternatively, if the original 189 data points were avail-
able, we could compute the unweighted regression of the original data on Day.
This is summarized in Table 5.7b. Both methods give identical intercept, slope,
and regression sum of squares. They differ on any calculation that uses the resid-
ual sum of squares, because in Table 5.7b the residual sum of squares is the sum
of SSpe and SSlof. For example, the standard errors of the coefﬁcients in the two
tables differ because in Table 5.7a the apparent estimate of variance is 3.7196 with
20 d.f., while in Table 5.7b it is 1.7621 with 187 df. Using pure error alone to
estimate ˆσ 2 may be appropriate, especially if the model is doubtful; this would
lead to a third set of standard errors. The SSpe can be computed directly from
Table 5.6 using (5.13), SSpe = (n −1)SD2 = 255.11 with (n −1) = 167 df.
The F-test for lack of ﬁt is F = 2.43. Since F(0.01; 20, 167) = 1.99, the p-value
for this test is less than 0.01, indicating that the straight-line mean function (5.14)
does not appear to be adequate. However, an F-test with this many df is very pow-
erful and will detect very small deviations from the null hypothesis. Thus, while
the result here is statistically signiﬁcant, it may not be scientiﬁcally important, and
for purposes of describing the growth of apple shoots, the mean function (5.14)
may be adequate.
5.4
GENERAL F TESTING
We have encountered several situations that lead to computation of a statistic that
has an F distribution when a null hypothesis (NH) and normality hold. The theory
for the F-tests is quite general. In the basic structure, a smaller mean function
of the null hypothesis is compared with a larger mean function of the alternative

106
WEIGHTS, LACK OF FIT, AND MORE
TABLE 5.6
Bland’s Data for Long and Short Apple Shootsa
Long Shoots
Short Shoots
Day
n
y
SD
Len
Day
n
y
SD
Len
0
5
10.200
0.830
1
0
5
10.000
0.000
0
3
5
10.400
0.540
1
6
5
11.000
0.720
0
7
5
10.600
0.540
1
9
5
10.000
0.720
0
13
6
12.500
0.830
1
19
11
13.360
1.030
0
18
5
12.000
1.410
1
27
7
14.290
0.950
0
24
4
15.000
0.820
1
30
8
14.500
1.190
0
25
6
15.170
0.760
1
32
8
15.380
0.510
0
32
5
17.000
0.720
1
34
5
16.600
0.890
0
38
7
18.710
0.740
1
36
6
15.500
0.540
0
42
9
19.220
0.840
1
38
7
16.860
1.350
0
44
10
20.000
1.260
1
40
4
17.500
0.580
0
49
19
20.320
1.000
1
42
3
17.330
1.520
0
52
14
22.070
1.200
1
44
8
18.000
0.760
0
55
11
22.640
1.760
1
48
22
18.460
0.750
0
58
9
22.780
0.840
1
50
7
17.710
0.950
0
61
14
23.930
1.160
1
55
24
19.420
0.780
0
69
10
25.500
0.980
1
58
15
20.600
0.620
0
73
12
25.080
1.940
1
61
12
21.000
0.730
0
76
9
26.670
1.230
1
64
15
22.330
0.890
0
88
7
28.000
1.010
1
67
10
22.200
0.790
0
100
10
31.670
1.420
1
75
14
23.860
1.090
0
106
7
32.140
2.280
1
79
12
24.420
1.000
0
82
19
24.790
0.520
0
85
5
25.000
1.010
0
88
27
26.040
0.990
0
91
5
26.600
0.540
0
94
16
27.120
1.160
0
97
12
26.830
0.590
0
100
10
28.700
0.470
0
106
15
29.130
1.740
0
aLen = 0 for short shoots and 1 for long shoots.
hypothesis (AH), and the smaller mean function can be obtained from the larger
by setting some parameters in the larger mean function equal to zero, equal to
each other, or equal to some speciﬁc value. One example previously encountered
is testing to see if the last q terms in a mean function are needed after ﬁtting the
ﬁrst p′ −q. In matrix notation, partition X = (X1, X2), where X1 is n × (p′ −q),
X2 is n × q, and partition β′ = (β′
1, β′
2), where β1 is (p′ −q) × 1, β2 is q × 1,
so the two hypotheses in matrix terms are
NH:
Y = X1β1 + e
AH:
Y = X1β1 + X2β2 + e
(5.15)

GENERAL F TESTING
107
TABLE 5.7
Regression for Long Shoots in the Apple Data
(a) wls regression using day means
Estimate Std. Error t value Pr(>|t|)
(Intercept) 9.973754
0.314272
31.74
<2e-16
Day
0.217330
0.005339
40.71
<2e-16
Residual standard error: 1.929 on 20 degrees of freedom
Multiple R-Squared: 0.988
Analysis of Variance Table
Df Sum Sq Mean Sq F value
Pr(>F)
Day
1 6164.3
6164.3
1657.2 < 2.2e-16
Residuals 20
74.4
3.7
(b) ols regression of y on Day
Estimate Std. Error t value Pr(>|t|)
(Intercept) 9.973754
0.21630
56.11
<2e-16
Day
0.217330
0.00367
59.12
<2e-16
Residual standard error: 1.762 on 187 degrees of freedom
Multiple R-Squared: 0.949
Analysis of Variance Table
Df Sum Sq Mean Sq F value
Pr(>F)
Regression
1 6164.3
6164.3
1657.2 < 2.2e-16
Residual
187
329.5
1.8
Lack of fit
20
74.4
3.7
2.43
0.0011
Pure error
167
255.1
1.5
The smaller model is obtained from the larger by setting β2 = 0.
The most general approach to computing the F-test is to ﬁt two regressions.
After ﬁtting NH, ﬁnd the residual sum of squares and its degrees of freedom
RSSNH and dfNH. Similarly, under the alternative mean function compute RSSAH
and dfAH. We must have dfNH > dfAH, since the alternative mean function has
more parameters. Also, RSSNH −RSSAH > 0, since the ﬁt of the AH must be at
least as good as the ﬁt of the NH. The F-test then gives evidence against NH if
F = (RSSNH −RSSAH)/(dfNH −dfAH)
RSSAH/dfAH
(5.16)
is large when compared with the F(dfNH −dfAH, dfAH) distribution.
5.4.1
Non-null Distributions
The numerator and denominator of (5.16) are independent of each other. Assuming
normality, apart from the degrees of freedom, the denominator is distributed as σ 2
times a χ2 random variable under both NH and AH. Ignoring the degrees of

108
WEIGHTS, LACK OF FIT, AND MORE
freedom, when NH is true the numerator is also distributed as σ 2 times a χ2, so
the ratio (5.16) has an F-distribution under NH because the F is deﬁned to be
the ratio of two independent χ2 random variables, each divided by their degrees
of freedom. When AH is true, apart from degrees of freedom the numerator is
distributed as a σ 2 times a noncentral χ2. In particular, the expected value of the
numerator of (5.16) will be
E(numerator of (5.16)) = σ 2(1 + noncentrality parameter)
(5.17)
For hypothesis (5.15), the noncentrality parameter is given by the expression
β′
2X′
2(I −X1(X′
1X1)−1X′
1)X2β2
qσ 2
(5.18)
To help understand this, consider the special case of X′
2X2 = I and X′
1X2 = 0 so
the terms in X2 are uncorrelated with each other and with the terms in X1. Then
(5.17) becomes
E(numerator) = σ 2 + β′
2β2
(5.19)
For this special case, the expected value of the numerator of (5.16), and the power
of the F-test, will be large if β2 is large. In the general case where X′
1X2 ̸= 0,
the results are more complicated, and the size of the noncentrality parameter, and
power of the F-test, depend not only on σ 2 but also on the sample correlations
between the variables in X1 and those in X2. If these correlations are large, then
the power of F may be small even if β′
2β2 is large. More general results on F-tests
are presented in advanced linear model texts such as Seber (1977).
5.4.2
Additional Comments
The F distribution for (5.16) is exact if the errors are normally distributed, and in
this case it is the likelihood ratio test for (5.15). The F-test is generally robust to
departures from normality of the errors, meaning that estimates, tests, and conﬁ-
dence procedures are only modestly affected by modest departures from normality.
In any case, when normality is in doubt, the bootstrap described in Section 4.6 can
be used to get signiﬁcance levels for (5.16).
5.5
JOINT CONFIDENCE REGIONS
Just as conﬁdence intervals for a single parameter are based on the t distribution,
conﬁdence regions for several parameters will require use of an F distribution. The
regions are elliptical.
The (1 −α) × 100% conﬁdence region for β is the set of vectors β such that
(β −ˆβ)′(X′X)(β −ˆβ)
p′ ˆσ 2
≤F(α; p′, n −p′)
(5.20)

JOINT CONFIDENCE REGIONS
109
The conﬁdence region for β∗, the parameter vector excluding β0, is, using the
notation of Chapter 3, the set of vectors β∗such that
(β∗−ˆβ
∗)′(X ′X)(β∗−ˆβ
∗)
p ˆσ 2
≤F(α; p, n −p)
(5.21)
The region (5.20) is a p′-dimensional ellipsoid centered at β, while (5.21) is a
p-dimensional ellipsoid centered at β∗.
For example, the 95% conﬁdence region for (β1, β2) in the regression of
log(Fertility) on log(PPgdp) and Purban in the UN data is given in Figure 5.3.
This ellipse is centered at (−0.13, −0.0035). The orientation of the ellipse, the
direction of the major axis, is negative, reﬂecting the negative correlation between
the estimates of these two coefﬁcients. The horizontal and vertical lines shown of
the plot are the marginal 95% conﬁdence intervals for each of the two coefﬁcient
estimates. From the graph, it is apparent that there are values of the coefﬁcients
that are in the 95% joint conﬁdence region that would be viewed as implausible if
we examined only the marginal intervals.
A slight generalization is needed to get a conﬁdence ellipsoid for an arbitrary
subset of β. Suppose that β1 is a subvector of β with q elements. Let S be the
q × q submatrix of (X′X)−1 corresponding to the q elements of β1. Then the 95%
conﬁdence region is the set of points βl such that
(β1 −ˆβ1)′S−1(β1 −ˆβ1)
q ˆσ 2
≤F(α; q, n −p′)
(5.22)
−0.18
−0.16
−0.14
−0.12
−0.1
−0.08
−0.06
−0.01
−0.008
−0.006
−0.004
−0.002
0
0.002
Coefficient for log (PPgdp)
Coefficient for Purban
FIG. 5.3
95% conﬁdence region for the UN data.

110
WEIGHTS, LACK OF FIT, AND MORE
The bootstrap can also be used to get joint conﬁdence regions by generalization
of the method outlined for conﬁdence intervals in Section 4.6, but the number
of bootstrap replications B required is likely to be much larger than the number
needed for intervals. The bootstrap conﬁdence region would be the smallest set
that includes (1 −α) × 100% of the bootstrap replications.
PROBLEMS
5.1. Galton’s sweet peas Many of the ideas of regression ﬁrst appeared in the
work of Sir Francis Galton on the inheritance of characteristics from one gen-
eration to the next. In a paper on “Typical Laws of Heredity,” delivered to the
Royal Institution on February 9, 1877, Galton discussed some experiments on
sweet peas. By comparing the sweet peas produced by parent plants to those
produced by offspring plants, he could observe inheritance from one generation
to the next. Galton categorized parent plants according to the typical diameter
of the peas they produced. For seven size classes from 0.15 to 0.21 inches, he
arranged for each of nine of his friends to grow 10 plants from seed in each
size class; however, two of the crops were total failures. A summary of Gal-
ton’s data was later published by Karl Pearson (1930) (see Table 5.8 and the
data ﬁle galtonpeas.txt). Only average diameter and standard deviation
of the offspring peas are given by Pearson; sample sizes are unknown.
5.1.1. Draw the scatterplot of Progeny versus Parent.
5.1.2. Assuming that the standard deviations given are population values,
compute the weighted regression of Progeny on Parent. Draw the ﬁtted
mean function on your scatterplot.
5.1.3. Galton wanted to know if characteristics of the parent plant such as
size were passed on to the offspring plants. In ﬁtting the regression,
a parameter value of β1 = 1 would correspond to perfect inheritance,
while β1 < 1 would suggest that the offspring are “reverting” toward
“what may be roughly and perhaps fairly described as the average
ancestral type” (The substitution of “regression” for “reversion” was
TABLE 5.8
Galton’s Peas Data
Parent
Progeny
Diameter (.01 in)
Diameter (.01 in)
SD
21
17.26
1.988
20
17.07
1.938
19
16.37
1.896
18
16.40
2.037
17
16.13
1.654
16
16.17
1.594
15
15.98
1.763

PROBLEMS
111
probably due to Galton in 1885). Test the hypothesis that β1 = 1 versus
the alternative that β1 < 1.
5.1.4. In his experiments, Galton took the average size of all peas produced
by a plant to determine the size class of the parental plant. Yet for seeds
to represent that plant and produce offspring, Galton chose seeds that
were as close to the overall average size as possible. Thus, for a small
plant, the exceptional large seed was chosen as a representative, while
larger, more robust plants were represented by relatively smaller seeds.
What effects would you expect these experimental biases to have on
(1) estimation of the intercept and slope and (2) estimates of error?
5.2. Apple shoots Apply the analysis of Section 5.3 to the data on short shoots
in Table 5.6.
5.3. Nonparametric lack of ﬁt The lack-of-ﬁt tests in Sections 5.2–5.3 require
either a known value for σ 2 or repeated observations for a given value of the
predictor that can be used to obtain a model-free, or pure-error, estimate of
σ 2. Loader (2004, Sec. 4.3) describes a lack-of-ﬁt test that can be used with-
out repeated observations or prior knowledge of σ 2 based on comparing the
ﬁt of the parametric model to the ﬁt of a smoother. For illustration, consider
Figure 5.4, which uses data that will be described later in this problem. For
each data point, we can ﬁnd the ﬁtted value ˆyi from the parametric ﬁt, which
is just a point on the line, and ˜yi, the ﬁtted value from the smoother, which is
200
400
600
800
100
200
300
400
Dbh
Height
FIG. 5.4
Height versus Dbh for the Upper Flat Creek grand ﬁr data. The solid line is the ols ﬁt. The
dashed line is the loess ﬁt with smoothing parameter 2/3.

112
WEIGHTS, LACK OF FIT, AND MORE
a point on the dashed line. If the parametric model is appropriate for the data,
then the differences ( ˆyi −˜yi) should all be relatively small. A suggested test
statistic is based on looking at the squared differences and then dividing by
an estimate of σ 2,
G =
n
i=1( ˆyi −˜yi)2
ˆσ 2
(5.23)
where ˆσ 2 is the estimate of variance from the parametric ﬁt. Large values of
G provide evidence against the NH that the parametric mean function matches
the data. Loader (2004) provides an approximation to the distribution of G
and also a bootstrap for computing an approximate signiﬁcance level for a
test based on G. In this problem, we will present the bootstrap.
5.3.1. The appropriate bootstrap algorithm is a little different from what we
have seen before, and uses a parametric bootstrap. It works as follows:
1. Fit the parametric and smooth regression to the data, and compute G
from (5.23). Save the residuals, ˆei = yi −ˆyi from the parametric ﬁt.
2. Obtain a bootstrap sample ˆe∗
1, . . . , ˆe∗
n by sampling with replacement
from ˆe1, . . . , ˆen. Some residuals will appear in the sample many
times, some not at all.
3. Given the bootstrap residuals, compute a bootstrap response Y∗with
elements y∗
i = ˆyi + ˆe∗
i . Use the original predictors unchanged in
every bootstrap sample. Obtain the parametric and nonparametric
ﬁtted values with the response Y∗, and then compute G from (5.23).
4. Repeat steps 2–3 B times, perhaps B = 999.
5. The signiﬁcance level of the test is estimated to be the fraction
of bootstrap samples that give a value of (5.23) that exceed the
observed G.
The important problem of selecting a smoothing parameter for the
smoother has been ignored. If the loess smoother is used, selecting
the smoothing parameter to be 2/3 is a reasonable default, and statis-
tical packages may include methods to choose a smoothing parameter.
See Simonoff (1996), Bowman, and Azzalini (1997), and Loader (2004)
for more discussion of this issue.
Write a computer program that implements this algorithm for regres-
sion with one predictor.
5.3.2. The data ﬁle ufcgf.txt gives the diameter Dbh in millimeters at 137
cm perpendicular to the bole, and the Height of the tree in decimeters
for a sample of grand ﬁr trees at Upper Flat Creek, Idaho, in 1991,
courtesy of Andrew Robinson. Also included in the ﬁle are the Plot
number, the Tree number in a plot, and the Species that is always “GF”
for these data. Use the computer program you wrote in the last subprob-
lem to test for lack of ﬁt of the simple linear regression mean function
for the regression of Height on Dbh.

PROBLEMS
113
5.4. An F-test In simple regression, derive an explicit formula for the F-test of
NH:
E(Y|X = x) = x
(β0 = 0, β1 = 1)
AH:
E(Y|X = x) = β0 + β1x
5.5. Snow geese
Aerial surveys sometimes rely on visual methods to estimate
the number of animals in an area. For example, to study snow geese in their
summer range areas west of Hudson Bay in Canada, small aircraft were used
to ﬂy over the range, and when a ﬂock of geese was spotted, an experienced
person estimated the number of geese in the ﬂock.
To investigate the reliability of this method of counting, an experiment
was conducted in which an airplane carrying two observers ﬂew over n = 45
ﬂocks, and each observer made an independent estimate of the number of birds
in each ﬂock. Also, a photograph of the ﬂock was taken so that a more or less
exact count of the number of birds in the ﬂock could be made. The resulting
data are given in the data ﬁle snowgeese.txt (Cook and Jacobson, 1978).
The three variables in the data set are Photo = photo count, Obs1 = aerial
count by observer one and Obs2 = aerial count by observer 2.
5.5.1. Draw scatterplot matrix of three variables. Do these graphs suggest
that a linear regression model might be appropriate for the regression
of Photo on either of the observer counts, or on both of the observer
counts? Why or why not? For the simple regression model of Photo
on Obs1, what do the error terms measure? Why is it appropriate to
ﬁt the regression of Photo on Obs1 rather than the regression of Obs1
on Photo?
5.5.2. Compute the regression of Photo on Obs1 using ols, and test the
hypothesis of Problem 5.4. State in words the meaning of this hypoth-
esis and the result of the test. Is the observer reliable (you must deﬁne
reliable)? Summarize your results.
5.5.3. Repeat Problem 5.5.2, except ﬁt the regression of Photo1/2 on Obs11/2.
The square-root scale is used to stabilize the error variance.
5.5.4. Repeat Problem 5.5.2, except assume that the variance of an error is
Obs1 × σ 2.
5.5.5. Do both observers combined do a better job at predicting Photo than
either predictor separately? To answer this question, you may wish to
look at the regression of Photo on both Obs1 and Obs2. Since from
the scatterplot matrix the two terms are highly correlated, interpretation
of results might be a bit hard. An alternative is to replace Obs1 and
Obs2 by Average = (Obs1 + Obs2)/2 and Diff = Obs1 −Obs2. The
new terms have the same information as the observer counts, but they
are much less correlated. You might also need to consider using wls.
As a result of this experiment, the practice of using visual counts of
ﬂock size to determine population estimates was discontinued in favor
of using photographs.

114
WEIGHTS, LACK OF FIT, AND MORE
TABLE 5.9
Jevons Gold Coinage Data
Age,
Sample
Average
Minimum
Maximum
Decades
Size n
Weight
SD
Weight
Weight
1
123
7.9725
0.01409
7.900
7.999
2
78
7.9503
0.02272
7.892
7.993
3
32
7.9276
0.03426
7.848
7.984
4
17
7.8962
0.04057
7.827
7.965
5
24
7.873
0.05353
7.757
7.961
5.6. Jevons’ gold coins The data in this example are deduced from a diagram
in a paper written by W. Stanley Jevons (1868) and provided by Stephen
M. Stigler. In a study of coinage, Jevons weighed 274 gold sovereigns that
he had collected from circulation in Manchester, England. For each coin, he
recorded the weight after cleaning to the nearest 0.001 g, and the date of
issue. Table 5.9 lists the average, minimum, and maximum weight for each
age class. The age classes are coded 1 to 5, roughly corresponding to the age
of the coin in decades. The standard weight of a gold sovereign was supposed
to be 7.9876 g; the minimum legal weight was 7.9379 g. The data are given
the ﬁle jevons.txt.
5.6.1. Draw a scatterplot of Weight versus Age, and comment on the appli-
cability of the usual assumptions of the linear regression model. Also
draw a scatterplot of SD versus Age, and summarize the information in
this plot.
5.6.2. Since the numbers of coins n in each age class are all fairly large, it is
reasonable to pretend that the variance of coin weight for each Age is
well approximated by SD2, and hence Var(Weight) is given by SD2/n.
Compute the implied wls regression.
5.6.3. Compute a lack-of-ﬁt test for the linear regression model, and summa-
rize results.
5.6.4. Is the ﬁtted regression consistent with the known standard weight for
a new coin?
5.6.5. For previously unsampled coins of Age = 1, 2, 3, 4, 5, estimate the
probability that the weight of the coin is less than the legal minimum.
(Hints: The standard error of prediction is a sum of two terms, the
known variance of an unsampled coin of known Age and the estimated
variance of the ﬁtted value for that Age. You should use the normal
distribution rather than a t to get the probabilities.)
5.7. The data ﬁle physics1.txt gives the results of the experiment described
in Section 5.1.1, except in this case the input is the π−meson as before, but
the output is the π+ meson.
Analyze these data following the analysis done in the text, and summarize
your results.

C H A P T E R 6
Polynomials and Factors
6.1
POLYNOMIAL REGRESSION
If a mean function with one predictor X is smooth but not straight, integer powers
of the predictors can be used to approximate E(Y|X). The simplest example of this
is quadratic regression, in which the mean function is
E(Y|X = x) = β0 + β1x + β2x2
(6.1)
Depending on the signs of the βs, a quadratic mean function can look like either
of curves shown in Figure 6.1. Quadratic mean functions can therefore be used
when the mean is expected to have a minimum or maximum in the range of the
predictor. The minimum or maximum will occur for the value of X for which the
derivative dE(Y|X = x)/dx = 0, which occurs at
xM = −β1/(2β2)
(6.2)
xM is estimated by substituting estimates for the βs into (6.2).
Quadratics can also be used when the mean function is curved but does not have
a minimum or maximum within the range of the predictor. Referring to Figure 6.1a,
if the range of X is between the dashed lines, then the mean function is everywhere
increasing but not linear, while in Figure 6.1b it is decreasing but not linear.
Quadratic regression is an important special case of polynomial regression. With
one predictor, the polynomial mean function of degree d is
E(Y|X) = β0 + β1X + β2X2 + · · · + βdXd
(6.3)
If d = 2, the model is quadratic, d = 3 is cubic, and so on. Any smooth func-
tion can be estimated by a polynomial of high-enough degree, and polynomial
mean functions are generally used as approximations and rarely represent a physi-
cal model.
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
115

116
POLYNOMIALS AND FACTORS
X
E(Y|X)
(a)
(b)
FIG. 6.1
Generic quadratic curves. A quadratic is the simplest curve that can approximate a mean
function with a minimum or maximum within the range of possible values of the predictor. It can also
be used to approximate some nonlinear functions without a minimum or maximum in the range of
interest, possibly using the part of the curve between the dashed lines.
The mean function (6.3) can be ﬁt via ols with p′ = d + 1 terms given by
an intercept and X, X2, . . . , Xd. Any regression program can be used for ﬁtting
polynomials, but if d is larger than three, serious numerical problems may arise
with some computer packages, and direct ﬁtting of (6.3) can be unreliable. Some
numerical accuracy can be retained by centering, using terms such as Zk = (X −
x)k, k = 1, . . . , d. Better methods using orthogonal polynomials are surveyed by
Seber (1977, Chapter 8).
An example of quadratic regression has already been given with the physics
data in Section 5.1.1. From Figure 5.1, page 102, a maximum value of the mean
function does not occur within the range of the data, and we are in a situation like
Figure 6.1b with the range of x between the dashed lines. The approximating mean
function may be very accurate within the range of X observed in the data, but it
may be very poor outside this range; see Problem 6.15.
In the physics example, a test for lack of ﬁt that uses extra information about
variances indicated that a straight-line mean function was not adequate for the
data, while the test for lack of ﬁt after the quadratic model indicated that this
model was adequate. When a test for lack of ﬁt is not available, comparison of the
quadratic model
E(Y|X) = β0 + β1X + β2X2
to the simple linear regression model
E(Y|X) = β0 + β1X
is usually based on a t-test of β2 = 0. A simple strategy for choosing d is to
continue adding terms to the mean function until the t-test for the highest-order term
is nonsigniﬁcant. An elimination scheme can also be used, in which a maximum
value of d is ﬁxed, and terms are deleted from the mean function one at a time,
starting with the highest-order term, until the highest-order remaining term has a

POLYNOMIAL REGRESSION
117
signiﬁcant t-value. Kennedy and Bancroft (1971) suggest using a signiﬁcance level
of about 0.10 for this procedure. In most applications of polynomial regression, only
d = 1 or d = 2 are considered. For larger values of d, the ﬁtted polynomial curves
become wiggly, providing an increasingly better ﬁt by matching the variation in
the observed data more and more closely. The curve is then modeling the random
variation rather than the overall shape of the relationship between variables.
6.1.1
Polynomials with Several Predictors
With more than one predictor, we can contemplate having integer powers and
products of all the predictors as terms in the mean function. For example, for the
important special case of two predictors the second-order mean function is given by
E(Y|X1 = x1, X2 = x2) = β0 + β1x1 + β2x2 + β11x2
1 + β22x2
2 + β12x1x2
(6.4)
The new term in (6.4) is the multiplicative term x1x2 called an interaction. With k
predictors, the second-order model includes an intercept, k linear terms, k quadratic
terms, and k(k + 1)/2 interaction terms. If k = 5, the second-order mean function
has 26 terms, and with k = 10, it has 76 terms. A usual strategy is to view the
second-order model as consisting of too many terms and use testing or other selec-
tion strategies such as those to be outlined in Section 10.2.1 to delete terms for
unneeded quadratics and interactions. We will provide an alternative approach in
Section 6.4.
The most important new feature of the second-order model is the interaction.
Return to the k = 2 predictor mean function (6.4). If x1 is changed to x1 + δ, then
the value of the mean function is
E(Y|X1 = x1 + δ, X2 = x2) = β0 + β1(x1 + δ) + β2x2 + β11(x1 + δ)2
+ β22x2
2 + β12(x1 + δ)x2
(6.5)
The change in the expected response is the difference between (6.5) and (6.4),
E(Y|X1 = x1 + δ, X2 = x2) −E(Y|X1 = x1, X2 = x2)
= (β11δ2 + β1δ) + 2β11δx1 + β12δx2
(6.6)
If β12 = 0, the expected change is the same for every value of x2. If β12 ̸= 0, then
β12δx2 will be different for each value of x2, and so the effect of changing x1
will depend on the value of x2. Without the interaction, the effect of changing one
predictor is the same for every value of the other predictor.
Cakes
Oehlert (2000, Example 19.3) provides data from a small experiment on baking
packaged cake mixes. Two factors, X1 = baking time in minutes and X2 = baking
temperature in degrees F, were varied in the experiment. The response Y was the
average palatability score of four cakes baked at a given combination of (X1, X2),
with higher values desirable. Figure 6.2 is a graphical representation of the experi-
mental design, from which we see that the center point at (35, 350) was replicated

118
POLYNOMIALS AND FACTORS
32
33
34
35
36
37
38
335
340
345
350
355
360
365
X1
X2
FIG. 6.2
Central composite design for the cake example. The center points have been slightly jittered
to avoid overprinting.
six times. Replication allows for estimation of pure error and tests for lack of ﬁt.
The experiment consisted of n = 14 runs.
The estimated mean function based on (6.4) and using the data in the ﬁle
cakes.txt is
E(Y|X1, X2) = −2204.4850 + 25.9176X1 + 9.9183X2
−0.1569X2
1 −0.0120X2
2 −0.0416X1X2
(6.7)
Each of the coefﬁcient estimates, including both quadratics and the interaction, has
signiﬁcance level of 0.005 or less, so all terms are useful in the mean function
(see Problem 6.1). Since each of X1 and X2 appears in three of the terms in
(6.7), interpreting this mean function is virtually impossible without the aid of
graphs. Figure 6.3 presents a useful way of summarizing the ﬁtted mean function.
In Figure 6.3a, the horizontal axis is the baking time X1, and the vertical axis is
the response Y. The three curves shown on the graph are obtained by ﬁxing the
value of temperature X2 at either 340, 350, or 360, and substituting into (6.7). For
example, when X2 = 350, substitute 350 for X2 in (6.7), and simplify to get
E(Y|X1, 
X2 = 350) = ˆβ0 + ˆβ2(350) + ˆβ22(350)2
+ ˆβ1X1 + ˆβ12(350)X1
+ ˆβ11X2
1
(6.8)
= −196.9664 + 11.3488X1 −0.1569X2
1

POLYNOMIAL REGRESSION
119
32
34
36
38
4
5
6
7
8
Y
X2 = 340
X2 = 350
X2 = 360
335
345
355
365
4
5
6
7
8
Y
X1 = 33
X1 = 35
X1 = 37
X1
X2
(a)
(b)
FIG. 6.3
Estimated response curves for the cakes data, based on (6.7).
Each of the lines within a plot is a quadratic curve, because both the X2
1 and X2
2
terms are in the mean function. Each of the curves has a somewhat different shape.
For example, in Figure 6.3a, the baking time X1 that maximizes the response is
lower at X2 = 360 degrees than it is at X2 = 340 degrees. Similarly, we see from
Figure 6.3b that the response curves are about the same for baking time of 35 or 37
minutes, but the response is lower at the shorter baking time. The palatability score
is perhaps surprisingly sensitive to changes in temperature of 10 or 15 degrees and
baking times of just a few minutes.
If we had ﬁt the mean function (6.4), but with β12 = 0 so the interaction is
absent, we would get the ﬁtted response curves shown in Figure 6.4. Without the
32
34
36
38
4
5
6
7
8
Y
X2 = 340
X2 = 350
X2 = 360
335
345
355
365
4
5
6
7
8
Y
X1 = 33
X1 = 35
X1 = 37
X1
X2
(a)
(b)
FIG. 6.4
Estimated response curves for the cakes data, based on ﬁtting with β12 = 0.

120
POLYNOMIALS AND FACTORS
interaction, all the curves within a plot have the same shape, and all are maximized
at the same point. Without the interaction, we could say, for example, that for
all temperatures the response is maximized for baking time of around 36 min,
and for all baking times, the response is maximized for temperature around 355
degrees. While this mean function is simpler, F-testing would show that it does not
adequately match the data, and so (6.8) and Figure 6.3 give appropriate summaries
for these data.
6.1.2
Using the Delta Method to Estimate a Minimum or a Maximum
We have seen at (6.2) that the value of the predictor that will maximize or mini-
mize a quadratic, depending on the signs of the βs, is xM = −β1/(2β2). This is a
nonlinear combination of the βs, and so its estimate, ˆxM = −ˆβ1/(2 ˆβ2) is a nonlin-
ear combination of estimates. The delta method provides an approximate standard
error of a nonlinear combination of estimates that is accurate in large samples. The
derivation of the delta method, and possibly its use, requires elementary calculus.
We will use different notation for this derivation to emphasize that the results
are much more general than just for ratios of coefﬁcient estimates in multiple linear
regression. Let θ be a k × 1 parameter vector, with estimator ˆθ such that
ˆθ ∼N(θ, σ 2D)
(6.9)
where D is a known, positive deﬁnite, matrix. Equation (6.9) can be exact, as it
is for the multiple linear regression model with normal errors, or asymptotically
valid, as in nonlinear or generalized linear models. In some problems, σ 2 may be
known, but in the multiple linear regression problem it is usually unknown and
also estimated from data.
Suppose g(θ) is a nonlinear continuous function of θ that we would like to
estimate. Suppose that θ∗is the true value of θ. To approximate g(ˆθ), we can use
a Taylor series expansion (see Section 11.1) about g(θ∗),
g(ˆθ) = g(θ∗) +
k

j=1
∂g
∂θj
( ˆθj −θ∗
j ) + small terms
≈g(θ∗) + ˙g(θ∗)′(ˆθ −θ∗)
(6.10)
where we have deﬁned
˙g(θ∗) = ∂g
∂θ =
 ∂g
∂θ1
, . . . , ∂g
∂θk
′
evaluated at θ∗. The vector ˙g has dimension k × 1. We have expressed in (6.10)
our estimate g(ˆθ) as approximately a constant g(θ∗) plus a linear combination of

POLYNOMIAL REGRESSION
121
data. The variance of a constant is zero, as is the covariance between a constant
and a function of data. We can therefore approximate the variance of g(ˆθ) by
Var(g(ˆθ)) = Var(g(θ∗)) + Var

˙g(θ∗)′(ˆθ −θ∗)

= ˙g(θ∗)′Var(ˆθ)˙g(θ∗)
= σ 2˙g(θ∗)′D˙g(θ∗)
(6.11)
This equation is the heart of the delta method, so we will write it out again as a
scalar equation. Let ˙gi be the i-th element of ˙g( ˆθ), so ˙gi is the partial derivative
of g(θ) with respect to θi, and let dij be the (i, j)-element of the matrix D. Then
the estimated variance of g( ˆθ) is
Var(g( ˆθ)) = σ 2
k

i=1
k

j=1
˙gi ˙gjdij
(6.12)
In practice, all derivatives are evaluated at ˆθ, and σ 2 is replaced by its estimate.
In large samples and under regularity conditions, g(ˆθ) will be normally dis-
tributed with mean g(θ∗) and variance (6.11). In small samples, the normal approx-
imation may be poor, and inference based on the bootstrap, Problem 6.16, might
be preferable.
For quadratic regression (6.1), the minimum or maximum occurs at g(β) =
−β1/(2β2), which is estimated by g( ˆβ). To apply the delta method, we need the
partial derivative, evaluated at ˆβ,
 ∂g
∂β
′
=

0, −1
2 ˆβ2
,
ˆβ1
2 ˆβ2
2

Using (6.12), straightforward calculation gives
Var(g(β)) =
1
4 ˆβ2
2

Var( ˆβ1) +
ˆβ2
1
ˆβ2
2
Var( ˆβ2) −2 ˆβ1
ˆβ2
Cov( ˆβ1, ˆβ2)

(6.13)
The variances and covariances in (6.13) are elements of the matrix σ 2(X′X)−1,
and so the estimated variance is obtained from ˆσ 2D = ˆσ 2(X′X)−1.
As a modestly more complicated example, the estimated mean function for
palatability for the cake data when the temperature is 350 degrees is given by
(6.8). The estimated maximum palatability occurs when the baking time is
ˆxM = −
ˆβ1 + ˆβ12(350)
2 ˆβ11
= 36.2 min

122
POLYNOMIALS AND FACTORS
which depends on the estimate ˆβ12 for the interaction as well as on the linear and
quadratic terms for X1. The standard error from the delta method can be computed
to be 0.4 minutes. If we can believe the normal approximation, a 95% conﬁdence
interval for xM is 36.2 ± 1.96 × 0.4 or about 35.4 to 37 min.
Writing a function for computing the delta method is not particularly hard using
a language such as Maple, Mathematica, MatLab, R or S-plus that can do sym-
bolic differentiation to get ˙g. If your package will not do the differentiation for
you, then you can still compute the derivatives by hand and use (6.12) to get the
estimated standard error. The estimated variance matrix ˆσ 2(X′X)−1 is computed
by all standard regression programs, although getting access to it may not be easy
in all programs.
6.1.3
Fractional Polynomials
Most problems that use polynomials use only integer powers of the predictors as
terms. Royston and Altman (1994) considered using fractional powers of predictors
in addition to integer powers. This provides a wider class of mean functions that
can be approximated using only a few terms and gives results similar to the results
we will get in choosing a transformation in Section 7.1.1.
6.2
FACTORS
Factors allow the inclusion of qualitative or categorical predictors in the mean
function of a multiple linear regression model. Factors can have two levels, such
as male or female, treated or untreated, and so on, or they can have many levels,
such as eye color, location, or many others.
To include factors in a multiple linear regression mean function, we need a way
to indicate which particular level of the factor is present for each case in the data.
For a factor with two levels, a dummy variable, which is a term that takes the value
1 for one of the categories and 0 for the other category, can be used. Assignment
of labels to the values is generally arbitrary, and will not change the outcome of
the analysis. Dummy variables can alternatively be deﬁned with a different set of
values, perhaps −1 and 1, or possibly 1 and 2. The only important point is the
term has only two values.
As an example, we return to the sleep data described in Section 4.5. This is
an observational study of the sleeping patterns of 62 mammal species. One of the
response variables in the study is TS, the total hours of sleep per day. Consider
here as an initial predictor the variable D, which is a categorical index to measure
the overall danger of that species. D has ﬁve categories, with D = 1 indicating
species facing the least danger from other animals, to D = 5 for species facing the
most danger. Category labels here are the numbers 1, 2, 3, 4, and 5, but D is not
a measured variable. We could have just as easily used names such as “lowest,”
“low,” “middle,” “high,” and “highest” for these ﬁve category names. The data are
in the ﬁle sleep1.txt. TS was not given for three of the species, so this analysis
is based on the 59 species for which data are provided.

FACTORS
123
6.2.1
No Other Predictors
We begin this discussion by asking how the mean of TS varies as D changes from
category to category. We would like to be able to write down a mean function
that allows each level of D to have its own mean, and we do that with a set of
dummy variables. Since D has ﬁve levels, the j-th dummy variable Uj for the
factor, j = 1, . . . , 5 has ith value uij , for i = 1, . . . , n, given by
uij =
	 1
if Di = jth category of D
0
otherwise
(6.14)
If the factor had three levels rather than ﬁve, and the sample size n = 7 with cases
1, 2, and 7 at the ﬁrst level of the factor, cases 4 and 5 at the second level, and
cases 3 and 6 at the third level, then the three dummy variables would be
U1
U2
U3
1
0
0
1
0
0
0
0
1
0
1
0
0
1
0
0
0
1
1
0
0
If these three dummy variables are added together, we will get a column of ones.
This is an important characteristic of a set of dummy variables for a factor: their
sum always adds up to the same value, usually one, for each case because each
case has one and only one level of the factor.
Returning to the sleep data, we can write the mean function as
E(TS|D) = β1U1 + β2U2 + β3U3 + β4U4 + β5U5
(6.15)
and we can interpret βj as the population mean for all species with danger index
equal to j. Mean function (6.15) does not appear to include an intercept. Since the
sum of the Uj is just a column of ones, the intercept is implicit in (6.15). Since
it is usual to have an intercept included explicitly, we can leave out one of the
dummy variables, leading to the factor rule:
The factor rule A factor with d levels can be represented by at most d dummy
variables. If the intercept is in the mean function, at most d −1 of the dummy
variables can be used in the mean function.
One common choice is to delete the ﬁrst dummy variable, and get the mean function
E(TS|D) = η0 + η2U2 + η3U3 + η4U4 + η5U5
(6.16)
where we have changed the names of the parameters because they now have
different meanings. The means for the ﬁve groups are now η0 + ηj for levels

124
POLYNOMIALS AND FACTORS
j = 2, 3, 4, 5 of D, and η0 for D = 1. Although the parameters have different
meanings in (6.15) and (6.16), both are ﬁtting a separate mean for each level of
D, and so both are really the same mean function. The mean function (6.16) is a
usual mean function for one-way analysis of variance models, which simply ﬁts a
separate mean for each level of the classiﬁcation factor.
Most computer programs allow the user to use a factor1 in a mean function
without actually computing the dummy variables. For example, in the packages
S-plus and R, D would ﬁrst be declared to be a factor, and then the mean function
(6.16) would be speciﬁed by
TS ∼1 + D
(6.17)
where the “1” speciﬁes ﬁtting the intercept, and the D speciﬁes ﬁtting the terms that
are created for the factor D. As is common in the speciﬁcation of mean functions
in linear regression computer programs, (6.17) speciﬁes the terms in the mean
function but not the parameters. This will work for linear models because each
term has one corresponding parameter.
Since most mean functions include an intercept, the speciﬁcation
TS ∼D
is equivalent to (6.17)2; to ﬁt (6.15) without an explicit intercept, use
TS ∼D −1
Sets of dummy variables are not the only way to convert a factor into a set of
terms, and each computer package can have its own rules for getting the terms that
will represent the factor, and it is important to know what your package is doing
if you need to interpret and use coefﬁcient estimates. Some packages, like R and
S-plus, allow the user to choose the way that a factor will be represented.
Figure 6.5 provides a scatterplot of TS versus D for the sleep data. Some com-
ments about this plot are in order. First, D is a categorical variable, but since
the categories are ordered, it is reasonable to plot them in the order from one
to ﬁve. However, the spacings on the horizontal axis between the categories are
arbitrary. Figure 6.5 appears to have an approximately linear mean function, but
we are not using this discovery in the one-way analysis of variance model (but
see Problem 6.3). If D had unordered categories, the graph could be drawn with
the categories on the horizontal axis so that the average response within group is
increasing from left to right. Also from Figure 6.5, variability seems to be more
or less the same for each group, suggesting that ﬁtting with constant variance is
appropriate.
Table 6.1 summarizes the ﬁt of the one-way analysis of variance, ﬁrst using
(6.15), then using (6.16). In Table 6.1a, the coefﬁcient for each Uj is the cor-
responding estimated mean for level j of D, and the t-value is for testing the
1A factor is called a class variable in SAS.
2In SAS, the equivalent model speciﬁcation would be TS=D.

FACTORS
125
Danger index
Total sleep (h)
 5
10
15
20
1
2
3
4
5
FIG. 6.5
Total sleep versus danger index for the sleep data.
TABLE 6.1
One-Way Mean Function for the Sleep Data Using
Two Parameterizations
Estimate
Std. Error
t-value
Pr(>|t|)
(a) Mean function (6.15)
U1
13.0833
0.8881
14.73
0.0000
U2
11.7500
1.0070
11.67
0.0000
U3
10.3100
1.1915
8.65
0.0000
U4
8.8111
1.2559
7.02
0.0000
U5
4.0714
1.4241
2.86
0.0061
Df
Sum Sq
Mean Sq
F-value
Pr(>F)
D
5
6891.72
1378.34
97.09
0.0000
Residuals
53
752.41
14.20
Estimate
Std. Error
t-value
Pr(>|t|)
(b) Mean function (6.16)
Intercept
13.0833
0.8881
14.73
0.0000
U2
−1.3333
1.3427
−0.99
0.3252
U3
−2.7733
1.4860
−1.87
0.0675
U4
−4.2722
1.5382
−2.78
0.0076
U5
−9.0119
1.6783
−5.37
0.0000
Df
Sum Sq
Mean Sq
F-value
Pr(>F)
D
4
457.26
114.31
8.05
0.0000
Residuals
53
752.41
14.20

126
POLYNOMIALS AND FACTORS
hypothesis that the mean for level j is zero versus the alternative that the mean is
not zero. In Table 6.1b, the estimate for the intercept is the mean for level one of
D, and the other estimates are the differences between the mean for level one and
the jth level. Similarly, the t-test that the coefﬁcient for Uj is zero for j > 1 is
really testing the difference between the mean for the jth level of D and the ﬁrst
level of D.
There are also differences in the analysis of variance tables. The analysis of
variance in Table 6.1a corresponding to (6.15) is testing the null hypothesis that all
the βs equal zero or that E(TS|D) = 0, against the alternative (6.15). This is not
the usual hypothesis that one wishes to test using the overall analysis of variance.
The analysis of variance in Table 6.1b is the usual table, with null hypothesis
E(TS|D) = β0 versus (6.16).
In summary, both the analyst and a computer package have considerable ﬂexi-
bility in the way that dummy variables for a factor are deﬁned. Different choices
have both advantages and disadvantages, and the analyst should be aware of the
choice made by a particular computer program.
6.2.2
Adding a Predictor: Comparing Regression Lines
To the sleep data, suppose we add log(BodyWt), the base-two logarithm of the
species average body weight, as a predictor. We now have two predictors, the
danger index D, a factor with ﬁve levels, and log(BodyWt). We assume for now
that for a ﬁxed value of D,
E(TS|log(BodyWt) = x, D = j) = β0j + β1jx
(6.18)
We distinguish four different situations:
Model 1: Most general Every level of D has a different slope and intercept,
corresponding to Figure 6.6a. We can write this most general mean function
in several ways. If we include a dummy variable for each level of D, we can
write
E(TS|log(BodyWt) = x, D = j) =
d

j=1

β0jUj + β1jUjx

(6.19)
Mean function (6.19) has 2d terms, the d dummy variables for d intercepts
and d interactions formed by multiplying each dummy variable by the con-
tinuous predictor for d slope parameters. In the computer packages R and
S-plus, if D has been declared to be a factor, this mean function can be ﬁt
by the statement
TS ∼−1 + D + D:log(BodyWt)

FACTORS
127
TS
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
3
3
3
3
3
33
3
3
3
4
4
4
4
4
4
4
4
4
5
5
5
5
5
5 5
(a) General
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2 2
2
2
2
2
2
2
3
3
3
3
3
33
3
3
3
4
4
4
4
4
4
4
4
4
5
5
5
5
5
5 5
5
10
15
20
(b) Parallel
−5
0
5
10
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2 2
2
2
2
2
2
2
3
3
3
3
3
33
3
3
3
4
4
4
4
4
4
4
4
4
5
5
5
5
5
5 5
5
10
15
20
−5
0
5
10
(c) Common intercept
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2 2
2
2
2
2
2
2
3
3
3
3
3
33
3
3
3
4
4
4
4
4
4
4
4
4
5
5
5
5
5
5 5
(d) Common regression
1
2
3
4
5
log2(BodyWt)
FIG. 6.6
Four models for the regression of TS on log(BodyWt) with ﬁve groups determined by D.
where the “−1” explicitly deletes the intercept, the term “D” ﬁts a separate
intercept for each level of D, and the term “D:log(BodyWt)” speciﬁes inter-
actions between each of the dummy variables for D and log(BodyWt)3.
Using a different letter for the parameters, this mean function can also be
written as
E(TS|log(BodyWt) = x, D = j) = η0 + η1x +
d

j=2

η0jUj + η1jUjx

(6.20)
Comparing the two parameterizations, we have η0 = β01, η1 = β11, and for
j > 1, η0j = β0j −β01 and η1j = β1j −β11. The parameterization (6.19) is
more convenient for getting interpretable parameters, while (6.20) is useful
for comparing mean functions. Mean function (6.20) can be speciﬁed in R
3Other programs such as SAS may replace the “:” with a “*”.

128
POLYNOMIALS AND FACTORS
and S-plus by
log(TS) ∼log(BodyWt) + D + D:log(BodyWt)
where again the overall intercept is implicit in the mean function and need
not be speciﬁed.
In Figure 6.6a, the estimated mean functions for each level of D appear to
be nearly parallel, so we should expect a simpler mean function might be
appropriate for these data.
Model 2: Parallel regressions For this mean function, all the within-group mean
functions are parallel as in Figure 6.6b, so β11 = β12 = · · · = β1d in (6.19),
or η12 = η12 = · · · = η1d = 0 in (6.20). Each level of D can have its own
intercept. This mean function can be speciﬁed as
log(TS) ∼D + log(BodyWt)
The difference between levels of D is the same for every value of the con-
tinuous predictor because no dummy variable by predictor interactions is
included in the mean function. This mean function should only be used if it is
in fact appropriate for the data. This mean function is ﬁt with terms for the
intercept, log(BodyWt) and D. The number of parameters estimated is d + 1.
We can see from Figure 6.6b that the ﬁtted mean function for D = 5 has the
smallest intercept, for D = 1 the intercept is largest, and for the three inter-
mediate categories, the mean functions are very nearly the same. This might
suggest that the three middle categories could be combined; see Problem 6.3.
Model 3: Common intercept In this mean function, the intercepts are all equal,
β01 = · · · = β0d in (6.19) or η02 = · · · = η0d = 0 in (6.20), but slopes are
arbitrary, as illustrated in Figure 6.6c. This particular mean function is prob-
ably inappropriate for the sleep data, as it requires that the expected hours of
TS given that a species is of 1-kg body weight, so log(BodyWt) = 0, is the
same for all levels of danger, and this seems to be totally arbitrary. The mean
function would change if we used different units, like grams or pounds.
This mean function is ﬁt with terms for the intercept, log(BodyWt) and the
log(BodyWt) × D interaction, for a total of d + 1 parameters. The R or S-plus
speciﬁcation of this mean function is
TS ∼1 + D:log(BodyWt)
Model 4: Coincident regression lines Here, all lines are the same, β01 = · · · =
β0m and β11 = · · · = β1m in (6.19) or η02 = · · · = η0d = η12 = · · · = η1d =
0 in (6.20). This is the most stringent model, as illustrated in Figure 6.6d. This
mean function requires only a term for the intercept and for log(BodyWt),
for a total of 2 parameters and is given by
TS ∼log(BodyWt)

FACTORS
129
TABLE 6.2
Residual Sum of Squares and df for the Four Mean
Functions for the Sleep Data
df
RSS
F
P(>F)
Model 1, most general
48
565.46
Model 2, parallel
52
581.22
0.33
0.853
Model 3, common intercept
52
709.49
3.06
0.025
Model 4, all the same
56
866.23
3.19
0.006
It is usually of interest to test the plausibility of models 4 or 2 against a different,
less stringent model as an alternative. The form of these tests is from the formulation
of the general F-test given in Section 5.4.
Table 6.2 gives the RSS and df for each of the four models for the sleep data.
Most tests concerning the slopes and intercepts of different regression lines will use
the general mean function of Model 1 as the alternative model. The usual F-test
for testing mean functions 2, 3, and 4 is then given for ℓ= 2, 3, 4 by
Fℓ= (RSSℓ−RSS1)(dfℓ−df1)
RSS1/df1
(6.21)
If the hypothesis provides as good a model as does the alternative, then F will be
small. If the model is not adequate when compared with the general model, then F
will be large when compared with the percentage points of the F(dfℓ−df1, df1)
distribution. The F-values for comparison to the mean function for Model 1 are
given in Table 6.2. Both the common intercept mean function and the coincident
mean function are clearly worse than Model 1, since the p-values are quite small.
However, the p-value for the parallel regression model is very large, suggesting
that the parallel regression model is appropriate for these data. The analysis is
completed in Problem 6.3.
6.2.3
Additional Comments
Probably the most common problem in comparing groups is testing for parallel
slopes in simple regression with two groups. Since this F-test has 1 df in the
numerator, it is equivalent to a t-test. Let ˆβj, ˆσ 2
j , nj and SXXj be, respectively, the
estimated slope, residual mean square, sample size, and corrected sum of squares
for the ﬁt of the mean function in group j, j = 1, 2. Then a pooled estimate of
σ 2 is
ˆσ 2 =

(n1 −2)ˆσ 2
1 + (n2 −2)ˆσ 2
2
n1 + n2 −4

(6.22)
and the t-test for equality of slopes is
t =
ˆβ1 −ˆβ2
ˆσ(1/SXX1 + 1/SXX2)1/2
(6.23)

130
POLYNOMIALS AND FACTORS
with n1 + n2 −4 df. The square of this t-statistic is numerically identical to the
corresponding F-statistic.
The model for common intercept, Model 3 of Section 6.2.2, can be easily
extended to the case where the regression lines are assumed common at any ﬁxed
point, X = c. In the sleep data, suppose we wished to test for concurrence at c = 2,
close to the average log body weight in the data. Simply replace log(BrainWt) by
z = log(BrainWt) −2 in all the models. Another generalization of this method is
to allow the regression lines to be concurrent at some arbitrary and unknown point,
so the point of equality must be estimated from the data. This turns out to be a
nonlinear regression problem (Saw, 1966).
6.3
MANY FACTORS
Increasing the number of factors or the number of continuous predictors in a mean
function can add considerably to complexity but does not really raise any new
fundamental issues. Consider ﬁrst a problem with many factors but no continu-
ous predictors. The data in the ﬁle wool.txt are from a small experiment to
understand the strength of wool as a function of three factors that were under
the control of the experimenter (Box and Cox, 1964). The variables are summa-
rized in Table 6.3. Each of the three factors was set to one of three levels, and
all 33 = 27 possible combinations of the three factors were used exactly once in
the experiment, so we have a single replication of a 33 design. The response vari-
able log(Cycles) is the logarithm of the number of loading cycles to failure of
worsted yarn. We will treat each of the three predictors as a factor with three
levels.
A main-effects mean function for these data would include only an intercept and
two dummy variables for each of the three factors, for a total of seven parame-
ters. A full second-order mean function would add all the two-factor interactions
to the mean function; each two-factor interaction would require 2 × 2 = 4 dummy
variables, so the second-order model will have 7 + 3 × 4 = 19 parameters. The
third-order model includes the three-factor interaction with 2 × 2 × 2 = 8 dummy
variables for a total of 19 + 8 = 27 parameters. This latter mean function will ﬁt
the data exactly because it has as many parameters as data points. R and S-plus
speciﬁcation of these three mean functions are, assuming that Len, Amp and Load
TABLE 6.3
The Wool Data
Variable
Deﬁnition
Len
Length of test specimen (250, 300, 350 mm)
Amp
Amplitude of loading cycle (8, 9, 10 mm)
Load
Load put on the specimen (40, 45, 50 g)
log(Cycles)
Logarithm of the number of cycles until the specimen fails

PARTIAL ONE-DIMENSIONAL MEAN FUNCTIONS
131
have all been declared as factors,
log(Cycles) ∼Len + Amp + Load
log(Cycles) ∼Len + Amp + Load
+ Len:Amp + Len:Load + Amp:Load
log(Cycles) ∼Len + Amp + Load
+ Len:Amp + Len:Load + Amp:Load
+ Len:Amp:Load
Other mean functions can be obtained by dropping some of the two-factor interac-
tions. Problems with many factors can be neatly handled using analysis of variance
methods given, for example, by Oehlert (2000), and in many other books. The anal-
ysis of variance models are the same as multiple linear regression models, but the
notation is a little different. Analysis of the wool data is continued in Problems 6.20
and 7.6.
6.4
PARTIAL ONE-DIMENSIONAL MEAN FUNCTIONS
A problem with several continuous predictors and factors requires generalization of
the four mean functions given in Section 6.2.2. For example, suppose we have two
continuous predictors X1 and X2 and a single factor F. All of the following are
generalizations of the parallel regression mean functions, using a generic response
Y and the computer notation of showing the terms but not the parameters:
Y ∼1 + F + X1
Y ∼1 + F + X2
Y ∼1 + F + X1 + X2
Y ∼1 + F + X1 + X2 + X1X2
These mean functions differ only with respect to the complexity of the dependence
of Y on the continuous predictors. With more continuous predictors, interpreting
mean functions such as these can be difﬁcult. In particular, how to summarize these
ﬁtted functions using a graph is not obvious.
Cook and Weisberg (2004) have provided a different way to look that problems
such as this one can be very useful in practice and can be easily summarized
graphically. In the basic setup, suppose we have terms X = (X1, . . . , Xp) created
from the continuous terms and a factor F with d levels. We suppose that
1. The mean function depends on the X-terms through a single linear com-
bination, so if X = x, the linear combination has the value x′β∗for some
unknown β∗. A term for the intercept is not included in X.

132
POLYNOMIALS AND FACTORS
2. For an observation at level j of the factor F, the mean function is
E(Y|X = x, F = j) = η0j + η1j(x′β∗)
(6.24)
This is equivalent to the most general Model 1 given previously, since each
level of the factor has its own intercept and slope. We can then summarize
the regression problem with a graph like one of the frames in Figure 6.6,
with x′β∗or an estimate of it on the horizontal axis. The generalization of
the parallel mean functions is obtained by setting all the η1j equal, while the
generalization of the common intercepts mean function sets all the η0j to be
equal.
There is an immediate complication: the mean function (6.24) is not a linear
mean function because the unknown parameter η1j multiplies the unknown param-
eter β∗, and so the parameters cannot be ﬁt in the usual way using linear least
squares software. Even so, estimating parameters is not particularly hard. In Prob-
lem 6.21, we suggest a simple computer program that can be written that will use
standard linear regression software to estimate parameters, and in Problem 11.6,
we show how the parameters can be estimated using a nonlinear least squares
program.
Australian Athletes
As an example, we will use data provided by Richard Telford and Ross Cunningham
collected on a sample of 202 elite athletes who were in training at the Australian
Institute of Sport. The data are in the ﬁle ais.txt. For this example, we are
interested in the conditional distribution of the variable LBM, the lean body mass,
given three terms, Ht, height in cm, Wt, weight in kg, and RCC, the red cell count,
separately for each sex. The data are displayed in Figure 6.7, with “m” for males
and “f” for females. With the exception of RCC, the variables are all approximately
linearly related; RCC appears to be at best weakly related to the others.
We begin by computing an F-test to compare the null hypothesis given by
E(LBM|Sex, Ht, Wt, RCC) = β0 + β1Sex + β2Ht + β3Wt + β4RCC
to the alternative mean function
E(LBM|Sex, Ht, Wt, RCC)=β0 + β1Sex + β2Ht + β3Wt + β4RCC + β12(Sex×Ht)
+ β13(Sex × Wt) + β14(Sex × RCC)
(6.25)
that has a separate mean function for each of the two sexes. For the ﬁrst of these
two, we ﬁnd RSS = 4043.6 with 202 −4 = 198 df. For the second mean function,
RSS = 1136.8 with 202 −8 = 194 df. The value of the test statistic is F = 104.02
with (4, 194) df, for a corresponding p-value that is zero to three decimal places.
We have strong evidence that the smaller of these mean functions is inadequate.

PARTIAL ONE-DIMENSIONAL MEAN FUNCTIONS
133
LBM
150
180
210
f
f
f
ff
f
f
f
f
f
f
f
f
f
ffff
f
f
f
f
f
f
f
ff
f
f
ff
ff f f
f
f
f
f
f
ff
fff
f f
ff
f
ff
f
ff
ff
fff
f
ff
f
f
f
f f
f
f
ff f
f
f
f
f
f
f
f
f
f f
f
f
f
f
f f
f
f
f
f
f
f
fff
ff
m
m m
m
m
mm
m
m
m
m
m
m
m
m m
m
m m
m
m
mmm
m
m
m
m
m
m
m
m
m
m
m
m
m
m m
m
m
m
mm
mm
m
m
m
m
m
m
m
m
m
m m
m
m
m
m
m
m
m
m
m m
m
m m
mmm
m m
m m
m
m
m
m
m
m
m
m mm
m
m
m
m
m
m
m m
m
m
m
m
m
m
m
f
f
f f
ff
f
f
ff
f
f
f
f
fff
f
f
f
ff
ff
f
f ff
f
ff
f fff
f
f
f
f
f
ff f
ff
f
ff f
f
ff f
f
f
f
f f
f
f
f f
ff
f
f ff
f
f
f ff
f
f
f
f
f
f
f
f
f fff
f
fff
ff
f
f
f
f
ff
f
f f
m
mm
m
m
m
m
mm
m
m
m
m
m
m
m
mm
m
m
mm
mmm
m
m
m
m
m
m
m
mm
m
m
mm
m
m
mmm
m
m
m
mmm
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
mm
mmm
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
mmm
m
m
m
m
m
m
4.0
5.0
6.0
40 60 80
f
f
ff
f
f
f
f
f f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f f
f
f
f
f
f
fff
f
f
f
f f
fff
f f
f
f
f f
f
f
f ff
f
f
f
ff
f
f
ff fff
f
f
f
f
f
f
f f
f
f
f
f
f
f
f
f
f f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
m
m m
m
m
m m
m
m
m
m
m
m
mm
m
m
m
mm
m
m
m m
m
m m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
mm m
mm
m
m
m
m
m
m
m
m
m
mm
mm
m
m
m
m
m
m
m
m
m
m
m m
m
mm
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
mm
m
m
m
mm
m
m
m
m
m
m
150
180
210
f
f
f
f
f
f
f
f f
f
f
f
f
f
ffff
f
f ff
f
f
f
f
f
f
f
ff
ffff
f
f f
f
f
f
fff
f
f
f
ff
f
f ff
f f
ff
fff
f
ff
f
f
f
f
f
f
f
ff f
f
f
f
f
f
f
f
f
f f
f
f
f
f
ff f
f
f
f
f
f
ff
f
f f
m m
m
m
m
m
m m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
m m
m
m
m
m
m
m m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
mm m
m m
m
m
m
m
m
m
m
m
Ht
f
f
f
f
f
f
f
f f
f
f
f
f
f
fff
f
f
fff
f
f
f
f ff
f
ff
f fff
ff f
f
f
ff f
ff
f
f
f f
f
ff f
f
ff
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f ff
f
f
f
f
f
f
f
f
f f
f
f
f f
ff
f
f
f
f
f
f
ff
f
f f
m m
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
m m
m
m
m
m
m
m
m
m
m
m
mmm
m
m
m
m
m
m
m
m
m
m
m
m
m m
m
m
m
m
m
m m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
f
f
f
f
f
f
f
ff
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
fff
f
f
ff
f
f
ff
f f
f
f
f
f
f
f
f ff
f
ff
f
f
f
f
f
f f
f
f
f
f
f
f
f f
f f
f
f
f
f
f
f
f
f
f f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
m
m
m
m
m
m m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
mm m
mm
m
m
m
m
mm
m
m
m m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
mmm
m
m
m
m
m
mm
m
mmm
m
m
m
m
m
mm
m
m
m
m
f
fff
ff
f
f ff
f
f
f
f
ffff
f
f ff
ff
f
f
f
f
f
ff
ffff
f
f f
f
ff
f
ff
f
f
fff
f
f ff
f
f
f
ffff
ffff
f
f ff
f
f
ff f
f
f
f
f
f
f
f
f
f
f ff
f
fff
ff
f f
f
f
ff
f
f
f
m
mm
m
m
m
m
m
m
m
m
m
m
mm
m
mm
m
m
m
m
mmm
m
m m
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
mmm
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
mm
mmm
m
m
m
mm
m
m
mm
m
m
m
m
mm
m
m
mm
m
m
mmm
m
m
m
m
m m
f
f
f
f
f
f
f
ff
f
f
f
f
f
ffff
f
ff
ff
f
f
ff
f
f
ff
ff f f
f
f f
f
f
ff
fff
f
f
ff
f
ff
f
f
ff
f
fff
f
ff
f
f
f
f f
f
f
ff f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f f
f
f
f
f
f
f
fff
f
f
m
m m
m
m
mm
m
m
m
m
m
m
m
m m
m
m m
m
m
mmm
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
mm
m
m
m
m m
m
m
m
m
m
m
m
m
m m
m
m m
mmm
m
m
m m
m
m
m
m
m
m
m
m
mm
m
mm
m
m
m
m m
m
m
m
m
m
m
m
Wt
40
80
120
f
f
ff
f
f
f
ff f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
fff
f
f
ff f
fff
f f
f
f
f f
f
f
f ff
f
ff
ff
f
f
ff fff
f
f
f
f
f
f
f f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
m
m m
m
m
m m
m
m
m
m
m
m
mm
m
m
m
mm
m
m
m m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
mm m
m
m
m
m
m
m
mm
m
m
m
mm
mm
m
m
m
m
m
m
m
m
m
m
m m
m
mm
m
m
m
m
m
m
m
m
m
m
m
mm
m
mmm
m
m
m
mm
m
m
m
m
m m
40 60 80
4.0
5.0
6.0
f
f
ff
f
f
f
f f
f
f
f
f
f
f
f
f
f f
f
f
f
f
f
ff
f
f
f
f
f
fff
f
f
f f
f
ff
f
ff
f f
ff
f
f
f
ff
f
f
f
ff
f
f
ff
ff
f
f
f
f
f
f
f
f
f
f
f
f
f
f f
f
f
f f
f
f
f
f
ff
f
f
f
f
f f
f
f
f
f
f
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
mm
m
m
m
m
m m
mmm
m
m
m
m m
m
m
m
m
m
m
m
m
m
m
f
f
f
f
f
f
f
ff
f
f
f
f
f
f
f
f
f f
f
f
f
f
f
f
f
f
f
f
f
f
ff f
f
f
f f
f
f
ff
ff
f f
f
f
f
f
f
f
f
f
ff
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f f
f
f
f f
f
f
f
f
f f
f
f
f
f
f
f
f
f
f
f
f
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m mm
m
m
m
m
mm
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
mm
m
m
m
m
m
m mm
m
mm
mm
m
m
m
m
mm
m
m
m
m
40
80
120
f
f
f f
f
f
f
f f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f ff
f
f
f f
f
f
ff
f
f
f f
ff
f
f
f
f f
f
ff
f f
f
f
f f
ff
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f f
f
f
f
f
ff
f
f
f
f
f
f
f
f
f
f
f
m
m
m m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
m
m
m
RCC
FIG. 6.7
Scatterplot matrix for the Australian athletes data, using “m” for males and “f” for females.
Interpretation of the larger mean function is difﬁcult because we cannot draw simple
graphs to summarize the results.
The partial one-dimensional (POD) mean function for these data is given by
E(LBM|Sex, Ht, Wt, RCC) = β0 + β1Sex + β2Ht + β3Wt + β4RCC
+ η0Sex + η1Sex × (β2Ht + β3Wt + β4RCC) (6.26)
which is a modest reparameterization of (6.24). We can ﬁt the mean function
(6.26) using either the algorithm outlined in Problem 6.21 or the nonlinear least
squares method outlined in Problem 11.6. The residual sum of squares is 1144.2
with 202 −6 = 196 df. The F-test for comparing this mean function to (6.25) has
value F = 0.63 with (2, 194) df, with a p-value of about 0.53. The conclusion is
the POD mean function matches the data as well as the more complicated (6.25).
The major advantage of the POD mean function is that we can draw the sum-
marizing graph given in Figure 6.8. The horizontal axis in the graph is the single
linear combination of the predictors ˆβ2Ht + ˆβ3Wt + ˆβ4RCC from the ﬁt of (6.26).
The vertical axis is the response LBM, once again with “m” for males and “f” for

134
POLYNOMIALS AND FACTORS
f
f
f
f
ff
f
f
ff
f
f
f
f
ff f
f
f
f
ff
ff
f
f
f
f
f
ff
f
f ff
f
f
f
f
f
f
f
f
f
f
f
ff f
f
ff f
f
f
f
f f
f
f
ff
ff
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f f
ff
f
f
f
f
ff
f
f
f
m
m
m
m
m
mm
mm
m
m
m
m
m
mm
m
mm
m
m
mm
mm
m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
mm m
m
m
m
m
mm
m
m
m
m
m
mm
m m
m
m
m
m
m
m
m
m
mm
m
m
mm
m
m
m
m
m
m
m
m
m
m
m
m
m m
m
m
m
m
m
m
m
mm
m
m
m
m
m
m
50
60
70
80
90
100
120
40
60
80
100
Linear Predictor, pod mean function
LBM, Groups = Sex
FIG. 6.8
Summary graph for the POD mean function for the Australian athletes data.
females. The two lines shown on the graph are the ﬁtted values for males in the
solid line and females in the dashed line. We get the interesting summary that LBM
depends on the same linear combination of the terms for each of the two sexes,
but the ﬁtted regression has a larger slope for males than for females.
6.5
RANDOM COEFFICIENT MODELS
We conclude this chapter with a brief introduction to problems in which the method-
ology of this chapter seems appropriate, but for which different methodology is to
be preferred.
Little is known about wetland contamination by road salt, primarily NaCl. An
exploratory study examined chloride concentrations in ﬁve roadside marshes and
four marshes isolated from roads to evaluate potential differences in chloride con-
centration between marshes receiving road runoff and those isolated from road
runoff, and to explore trends in chloride concentrations across an agricultural grow-
ing season, from about April to October.
The data in the ﬁle chloride.txt, provided by Stefanie Miklovic and Susan
Galatowitsch, summarize results. Repeated measurements of chloride level were
taken in April, June, August, and October, during 2001. Two of the marshes were
dry by August 2001, so only April and June measurements were taken on those
two. The variables in the ﬁle are Cl, the measured chloride level in mg/liter, Month,
the month of measurement, with April as 4, June as 6, and so on; Marsh, the marsh
number, and Type, either isolated or roadside.
Following the methodology of this chapter, we might contemplate ﬁtting mul-
tiple linear regression models with a separate intercept and slope for each level of

RANDOM COEFFICIENT MODELS
135
Type, following the outline in Section 6.2.2. This mean function ignores the possi-
bility of each marsh having a different intercept and slope. Were we to include a
factor Marsh in the mean function, we would end up ﬁtting up to 18 parameters,
but the data include only 32 observations. Furthermore, ﬁtting a separate regression
for each marsh does not directly answer the questions of interest that average over
marshes.
The data are shown in Figure 6.9. A separate graph is given for the two types,
and the points within a wetland are joined in each graph. While it is clear that
the overall chloride level is different in the two types of wetlands, the lines within
a graph do not tell a coherent story; we cannot tell if there is a dependence on
Month, or if the dependence is the same for the two types.
To examine data such as these, we use a random coefﬁcients model, which in this
problem assumes that the marshes within a type are a random sample of marshes
that could have been studied. Using a generic notation, suppose that yijk is the value
of the response for the jth marsh of type i at time xk. The random coefﬁcients
model speciﬁes that
yijk = β0i + β1ixk + b0ij + b1ijxk + eijk
(6.27)
As in other models, the βs are ﬁxed, unknown parameters that specify separate
linear regression for each of the two types, β01 + β11xk for Type = isolated, and
β02 + β12xk for Type = roadside. The errors eijk will be taken to be independent
and identically distributed with variance σ 2. Mean function (6.27) is different from
other mean functions we have seen because of the inclusion of the bs. We assume
Month number
Cl (mg/l)
0
20
40
60
80
100
4
5
6
7
8
9
10
Isolated
Roadside
4
5
6
7
8
9
10
FIG. 6.9
The chloride concentration data.

136
POLYNOMIALS AND FACTORS
that the bs are random variables, independent of the eijk, and that
 b0ij
b1ij

∼N
 0
0

,
 τ 2
0
τ01
τ01
τ 2
1

According to this model, a particular marsh has intercept β0i + b0ij and slope
β1i + b1ij , while the average intercept and slope for type i are β0i and β1i, respec-
tively. The two variances τ 2
0 and τ 2
1 model the variation in intercepts and slopes
between marshes within a type, and τ01 allows the bs to be correlated within marsh.
Rather than estimate all the bs, we will instead estimate the τs that characterize
the variability in the bs.
One of the effects of a random coefﬁcient model is that the yijk are no longer
independent as they are in the linear regression models we have considered so far.
Using Appendix A.2.2:
Cov(yijk, yi′j′k′) =



0
i ̸= i′
0
i = i′, j ̸= j′
τ 2
0 + xkxk′τ 2
1 + (xk + xk′)τ01
i = i′, j = j′, k ̸= k′
σ 2 + τ 2
0 + x2
kτ 2
1 + 2xkτ01
i = i′, j = j′, k = k′
(6.28)
The important point here is that repeated observations on the same marsh are cor-
related, while observations on different marshes are not correlated. If we consider
the simpler random intercepts model given by
yijk = β0i + β1ixk + b0ij + eijk
(6.29)
we will have b1ij = τ 2
1 = τ01 = 0, and the correlation between observations in the
same marsh will be τ 2
0 /(σ 2 + τ 2
0 ), which is known as the intra-class correlation.
Since this is always positive, the variation within one marsh will always be smaller
than the variation between marshes, as often makes very good sense.
Methods for ﬁtting with a model such as (6.27) is beyond the scope of this
book. Software is widely available, however, and books by Pinheiro and Bates
(2000), Littell, Milliken, Stroup, and Wolﬁnger (1996), and Verbeke and Molen-
berghs (2000) describe the methodology and the software. Without going into detail,
generalization of the testing methods discussed in this book suggest that the random
intercepts model (6.29) is appropriate for these data with β11 = β12, as there is no
evidence that either the slope is different for the two types or that the slope varies
from marsh to marsh. When we ﬁt (6.29) using the software described by Pinheiro
and Bates, we get the summaries shown in Table 6.4. The difference between the
types is estimated to be between about 28 and 72 mg/liter, while the level of Cl
appears to be increasing over the year by about 1.85 mg/liter per month. The esti-
mates of the variances τ0 and σ are shown in the standard deviation scale. Since
these two are of comparable size, therefore a substantial gain in precision in the
analysis by accounting for, and removing, the between marsh variation.
The random coefﬁcients models are but one instance of a general class of linear
mixed models. These are described in Pinheiro and Bates (2000), Littell, Milliken,

PROBLEMS
137
TABLE 6.4
Approximate 95% Conﬁdence Intervals for Parameters
of the Random Coefﬁcients Model Using R for the Chloride Data
Fixed effects:
lower
est.
upper
(Intercept)
-21.50837 -5.5038 10.5007
Month
0.75487
1.8538
2.9527
Type
28.69165 50.5719 72.4521
Random Effects:
lower
est.
upper
sd((Intercept))
7.5761 13.335 23.473
Within-group standard error:
4.7501 6.3872 8.5885
Stroup, and Wolﬁnger (1996), and Diggle, Heagerty, Liang, and Zeger (2002). This
is a very rich and important class of models that allow ﬁtting with a wide variety
of correlation and mean structures.
PROBLEMS
6.1. Cake data The data for this example are in the data ﬁle cakes.txt.
6.1.1. Fit (6.4) and verify that the signiﬁcance levels are all less than 0.005.
6.1.2. Estimate the optimal (X1, X2) combination ( ˜X1, ˜X2) and ﬁnd the stan-
dard errors of ˜X1 and ˜X2.
6.1.3. The cake experiment was carried out in two blocks of seven obser-
vations each. It is possible that the response might differ by block.
For example, if the blocks were different days, then differences in
air temperature or humidity when the cakes were mixed might have
some effect on Y. We can allow for block effects by adding a fac-
tor for Block to the mean function, and possibly allowing for Block
by term interactions. Add block effects to the mean function ﬁt in
Section 6.1.1 and summarize results. The blocking is indicated by the
variable Block in the data ﬁle.
6.2. The data in the ﬁle lathe1.txt are the results of an experiment on char-
acterizing the life of a drill bit in cutting steel on a lathe. Two factors were
varied in the experiment, Speed and Feed rate. The response is Life, the total
time until the drill bit fails, in minutes. The values of Speed in the data have
been coded by computing
Speed = (Actual speed in feet per minute −900)
300
Feed = (Actual feed rate in thousandths of an inch per revolution −13)
6

138
POLYNOMIALS AND FACTORS
The coded variables are centered at zero. Coding has no material effect on
the analysis but can be convenient in interpreting coefﬁcient estimates.
6.2.1. Draw a scatterplot matrix of Speed, Feed, Life, and log(Life), the base-
two logarithm of tool life. Add a little jittering to Speed and Feed to
reveal over plotting. The plot of Speed versus Feed gives a picture of
the experimental design, which is called a central composite design.
It is useful when we are trying to ﬁnd a value of the factors that
maximizes or minimizes the response. Also, several of the experi-
mental conditions were replicated, allowing for a pure-error estimate
of variance and lack of ﬁt testing. Comment on the scatterplot matrix.
6.2.2. For experiments in which the response is a time to failure or time to
event, the response often needs to be transformed to a more useful
scale, typically by taking the log of the response, or sometimes by
taking the inverse. For this experiment, log scale can be shown to be
appropriate (Problem 9.7).
Fit the full second-order mean function (6.4) to these data using
log(Life) as the response, and summarize results.
6.2.3. Test for the necessity of the Speed × Feed interaction, and summa-
rize your results. Draw appropriate summary graphs equivalent to
Figure 6.3 or Figure 6.4, depending on the outcome of your test.
6.2.4. For Speed = 0.5, estimate the value of Feed that minimizes log(Life),
and obtain a 95% conﬁdence interval for this value using the delta
method.
6.3. In the sleep data, do a lack of ﬁt test for D linear against the one-way Anova
model. Summarize results.
6.4. The data in the ﬁle twins.txt give the IQ scores of identical twins, one
raised in a foster home, IQf, and the other raised by birth parents, IQb.
The data were published by Burt (1966), and their authenticity has been
questioned. For purposes of this example, the twin pairs can be divided into
three social classes C, low, middle or high, coded in the data ﬁle 1, 2, and
3, respectively, according to the social class of the birth parents. Treat IQf
as the response and IQb as the predictor, with C as a factor.
Perform an appropriate analysis of these data. Be sure to draw and discuss
a relevant graph. Are the within-class mean functions straight lines? Are there
class differences? If there are differences, what are they?
6.5. Referring to the data in Problem 2.2, compare the regression lines for Forbes’
data and Hooker’s data, for the mean function E(log(Pressure)|Temp) = β0 +
β1Temp.
6.6. Refer to the Berkeley Guidance study described in Problem 3.1. Using the
data ﬁle BGSall.txt, consider the regression of HT18 on HT9 and the
grouping factor Sex.

PROBLEMS
139
6.6.1. Draw the scatterplot of HT18 versus HT9, using a different symbol for
males and females. Comment on the information in the graph about
an appropriate mean function for these data.
6.6.2. Fit the four mean function suggested in Section 6.2.2, perform the
appropriate tests, and summarize your ﬁndings.
6.7. In the Berkeley Guidance Study data, Problem 6.6, consider the response
HT18 and predictors HT2 and HT9. Model 1 in Section 6.2.2 allows each
level of the grouping variable, in this example the variable Sex, to have its
own mean function. Write down at least two generalizations of this model
for this problem with two continuous predictors rather than one.
6.8. Continuing with Problem 6.7 and assuming no interaction between HT2 and
HT9, obtain a test for the null hypothesis that the regression planes are parallel
for boys and girls versus the alternative that separate planes are required for
each sex.
6.9. Refer to the apple shoot data, Section 5.3, using the data ﬁle all-
shoots.txt, giving information on both long and short shoots.
6.9.1. Compute a mean square for pure error separately for long and short
shoots, and show that the pure-error estimate of variance for long
shoots is about twice the size of the estimate for short shoots. Since
these two estimates are based on completely different observations,
they are independent, and so their ratio will have an F distribution
under the null hypothesis that the variance is the same for the two
types of shoots. Obtain the appropriate test, and summarize results.
(Hint: the alternative hypothesis is that the two variances are unequal,
meaning that you need to compute a two-tailed signiﬁcance level, not
one-tailed as is usually done with F-tests). Under the assumption that
the variance for short shoots is σ 2 and the variance for long shoots is
2σ 2, obtain a pooled pure-error estimate of σ 2.
6.9.2. Draw the scatterplot of ybar versus Day, with a separate symbol for
each of the two types of shoots, and comment on the graph. Are
straight-line mean functions plausible? Are the two types of shoots
different?
6.9.3. Fit models 1, 3 and 4 from Section 6.2.2 to these data. You will need
to use weighted least squares, since each of the responses is an average
of n values. Also, in light of Problem 6.9.1, assume that the variance
for short shoots is σ 2, but the variance for long shoots is 2σ 2.
6.10. Gothic and Romanesque cathedrals The data in the data ﬁle cathe-
dral.txt gives Height = nave height and Length = total length, both in
feet, for medieval English cathedrals. The cathedrals can be classiﬁed accord-
ing to their architectural style, either Romanesque or, later, Gothic. Some

140
POLYNOMIALS AND FACTORS
cathedrals have both a Gothic and a Romanesque part, each of differing
height; these cathedrals are included twice. Names of the cathedrals are also
provided in the ﬁle. The data were provided by Gould S.J. based on plans
given by Clapham (1934).
6.10.1. For these data, it is useful to draw separate plots of Length versus
Height for each architectural style. Summarize the differences appar-
ent in the graphs in the regressions of Length on Height for the two
styles.
6.10.2. Use the data and the plots to ﬁt regression models that summarize the
relationship between the response Length and the predictor Height
for the two architectural styles.
6.11. Windmill data
In Problem 2.13, page 45, we considered data to predict
wind speed CSpd at a candidate site based on wind speed RSpd at a nearby
reference site where long-term data is available. In addition to RSpd, we also
have available the wind direction, RDir, measured in degrees. A standard
method to include the direction data in the prediction is to divide the direc-
tions into several bins and then ﬁt a separate mean function for of CSpd
on RSpd in each bin. In the wind farm literature, this is called the measure,
correlate, predict method, Derrick (1992). The data ﬁle wm2.txt contains
values of CSpd, RSpd, RDir, and Bin for 2002 for the same candidate and
reference sites considered in Problem 2.13. Sixteen bins are used, the ﬁrst bin
for cases with RDir between 0 and 22.5 degrees, the second for cases with
RDir between 22.5 and 45 degrees, . . . , and the last bin between 337.5 and
360 degrees. Both the number of bins and their starting points are arbitrary.
6.11.1. Obtain tests that compare ﬁtting the four mean functions discussed
in Section 6.2.2 to the 16 bins. How many parameters are in each of
the mean functions?
6.11.2. Do not attempt this problem unless your computer package has a
programming language.
Table 6.5 gives the number of observations in each of the 16 bins
along with the average wind speed in that bin for the reference site
for the period January 1, 1948 to July 31, 2003, excluding the year
2002; the table is also given in the data ﬁle wm3.txt. Assuming the
most general model of a separate regression in each bin is appropri-
ate, predict the average wind speed at the candidate site for each of
the 16 bins, and ﬁnd the standard error. This will give you 16 pre-
dictions and 16 independent standard errors. Finally, combine these
16 estimates into one overall estimate (you should weight according
to the number of cases in a bin), and then compare your answer to
the prediction and standard error from Problem 4.6.
6.12. Land valuation Taxes on farmland enrolled in a “Green Acres” program in
metropolitan Minneapolis–St. Paul are valued only with respect to the land’s
value as productive farmland; the fact that a shopping center or industrial park

PROBLEMS
141
TABLE 6.5
Bin Counts and Means for the Windmill Dataa
Bin
Bin.count
RSpd
Bin
Bin.count
RSpd
0
2676
6.3185
8
4522
7.7517
1
2073
5.6808
9
32077
6.4943
2
1710
5.4584
10
2694
6.1619
3
1851
5.4385
11
2945
6.5947
4
2194
5.8763
12
4580
7.6865
5
3427
6.6539
13
6528
8.8078
6
5201
7.8756
14
6705
8.5664
7
6392
8.4281
15
4218
7.5656
aThese data are also given in the ﬁle wm3.txt.
has been built nearby cannot enter into the valuation. This creates difﬁculties
because almost all sales, which are the basis for setting assessed values, are
priced according to the development potential of the land, not its value as
farmland. A method of equalizing valuation of land of comparable quality
was needed.
One method of equalization is based on a soil productivity score P , a
number between 1, for very poor land, and 100, for the highest quality
agricultural land. The data in the ﬁle prodscore.txt, provided by Doug
Tiffany, gives P along with Value, the average assessed value, the Year,
either 1981 or 1982 and the County name for four counties in Minnesota,
Le Sueur, Meeker, McLeod, and Sibley, where development pressures had
little effect on assessed value of land in 1981–82. The unit of analysis is a
township, roughly six miles square.
The goal of analysis is to decide if soil productivity score is a good
predictor of assessed value of farmland. Be sure to examine county and year
differences, and write a short summary that would be of use to decision
makers who need to determine if this method can be used to set property
taxes.
6.13. Sex discrimination
The data in the ﬁle salary.txt concern salary and
other characteristics of all faculty in a small Midwestern college collected
in the early 1980s for presentation in legal proceedings for which discrim-
ination against women in salary was at issue. All persons in the data hold
tenured or tenure track positions; temporary faculty are not included. The data
were collected from personnel ﬁles and consist of the quantities described in
Table 6.6.
6.13.1. Draw an appropriate graphical summary of the data, and comment
of the graph.
6.13.2. Test the hypothesis that the mean salary for men and women is the
same. What alternative hypothesis do you think is appropriate?

142
POLYNOMIALS AND FACTORS
TABLE 6.6
The Salary Data
Variable
Description
Sex
Sex, 1 for female and 0 for male
Rank
Rank, 1 for Assistant Professor, 2 for Associate Professor, and 3 for
Full Professor
Year
Number of years in current rank
Degree
Highest degree, 1 if Doctorate, 0 if Masters
YSdeg
Number of years since highest degree was earned
Salary
Academic year salary in dollars
6.13.3. Obtain a test of the hypothesis that salary adjusted for years in cur-
rent rank, highest degree, and years since highest degree is the same
for each of the three ranks, versus the alternative that the salaries are
not the same. Test to see if the sex differential in salary is the same
in each rank.
6.13.4. Finkelstein (1980), in a discussion of the use of regression in dis-
crimination cases, wrote, “. . . [a] variable may reﬂect a position or
status bestowed by the employer, in which case if there is discrim-
ination in the award of the position or status, the variable may be
‘tainted’.” Thus, for example, if discrimination is at work in promo-
tion of faculty to higher ranks, using rank to adjust salaries before
comparing the sexes may not be acceptable to the courts.
Fit two mean functions, one including Sex, Year, YSdeg and
Degree, and the second adding Rank. Summarize and compare the
results of leaving out rank effects on inferences concerning differen-
tial in pay by sex.
6.14. Using the salary data in Problem 6.13, one ﬁtted mean function is
E(Salary|Sex, Year) = 18223 −571Sex + 741Year + 169Sex × Year
6.14.1. Give the coefﬁcients in the estimated mean function if Sex were
coded so males had the value 2 and females had the value 1 (the
coding given to get the above mean function was 0 for males and 1
for females).
6.14.2. Give the coefﬁcients if Sex were codes as −1 for males and +1 for
females.
6.15. Pens of turkeys were grown with an identical diet, except that each pen
was supplemented with an amount A of an amino acid methionine as a
percentage of the total diet of the birds. The data in the ﬁle turk0.txt
give the response average weight Gain in grams of all the turkeys in the pen
for 35 pens of turkeys receiving various levels of A.

PROBLEMS
143
6.15.1. Draw the scatterplot of Gain versus A and summarize. In particular,
does simple linear regression appear plausible?
6.15.2. Obtain a lack of ﬁt test for the simple linear regression mean function,
and summarize results. Repeat for the quadratic regression mean
function.
6.15.3. To the graph drawn in Problem 6.15.1, add the ﬁtted mean functions
based on both the simple linear regression mean function and the
quadratic mean function, for values of A in the range from 0 to 0.60,
and comment.
6.16. For the quadratic regression mean function for the turkey data discussed in
Problem 6.15, use the bootstrap to estimate the standard error of the value
of D that maximizes gain. Compare this estimated standard error with the
answer obtained using the delta method.
6.17. Refer to Jevons’ coin data, Problem 5.6. Determine the Age at which the
predicted weight of coins is equal to the legal minimum, and use the delta
method to get a standard error for the estimated age. This problem is called
inverse regression, and is discussed by Brown (1994).
6.18. The data in the ﬁle mile.txt give the world record times for the one-
mile run. For males, the records are for the period from 1861–2003, and for
females, for the period 1967–2003. The variables in the ﬁle are Year, year of
the record, Time, the record time, in seconds, Name, the name of the runner,
Country, the runner’s home country, Place, the place where the record was
run (missing for many of the early records), and Gender, either Male or
Female. The data were taken from http://www.saunalahti.ﬁ/˜sut/eng/.
6.18.1. Draw a scatterplot of Time versus Year, using a different symbol for
men and women. Comment on the graph.
6.18.2. Fit separate simple linear regression mean functions to each sex, and
show that separate slopes and intercepts are required. Provide an
interpretation of the slope parameters for each sex.
6.18.3. Find the year in which the female record is expected to be 240 sec-
onds, or four minutes. This will require inverting the ﬁtted regression
equation. Use the delta method to estimate the standard error of this
estimate.
6.18.4. Using the model ﬁt in Problem 6.18.2, estimate the year in which the
female record will match the male record, and use the delta method
to estimate the standard error of the year in which they will agree.
Comment on whether you think using the point at which the ﬁtted
regression lines cross as a reasonable estimator of the crossing time.
6.19. Use the delta method to get a 95% conﬁdence interval for the ratio β1/β2
for the transactions data, and compare with the bootstrap interval obtained
at the end of Section 4.6.1.

144
POLYNOMIALS AND FACTORS
6.20. Refer to the wool data discussed in Section 6.3.
6.20.1. Write out in full the main-effects and the second-order mean func-
tions, assuming that the three predictors will be turned into factors,
each with three levels. This will require you to deﬁne appropriate
dummy variables and parameters.
6.20.2. For the two mean functions in Problem 6.20.1, write out the expected
change in the response when Len and Amp are ﬁxed at their middle
levels, but Load is increased from its middle level to its high level.
6.21. A POD model for a problem with p predictors X = (X1 . . . , Xp) and a factor
F with d levels is speciﬁed, for the jth level of F, by
E(Y|X = x, F = j) = η0j + η1j(x′β∗)
(6.30)
This is a nonlinear model because ηij multiplies the parameter β∗. Estimation
of parameters can use the following two-step algorithm:
1. Assume that the η1j, j = 1, . . . , d are known. At the ﬁrst step of the
algorithm, set η1j = 1, j = 1, . . . , d. Deﬁne a new term zj = η1jx, and
substituting into (6.30),
E(Y|X = x, F = j) = η0j + z′
jβ∗
We recognize this as a mean function for parallel regressions with com-
mon slopes β∗and a separate intercept for each level of F. This mean
function can be ﬁt using standard ols linear regression software. Save the
estimate ˆβ
∗of β∗.
2. Let v = x′ ˆβ
∗, where ˆβ
∗was computed in step 1. Substitute v for x′β∗in
(6.30) to get
E(Y|X = x, F = j) = η0j + η1jv
which we recognize as a mean function with a separate intercept and slope
for each level of F. This mean function can also be ﬁt using ols linear
regression software. Save the estimates of η1j and use them in the next
iteration of step 1.
Repeat this algorithm until the residual sum of squares obtained at the two
steps is essentially the same. The estimates obtained at the last step will be
the ols estimates for the original mean function, and the residual sum of
squares will be the residual sum of squares that would be obtained by ﬁtting
using nonlinear least squares. Estimated standard errors of the coefﬁcients
will be too small, so t-tests should not be used, but F-tests can be used to
compare models.
Write a computer program that implements this algorithm.

PROBLEMS
145
6.22. Using the computer program written in the last problem or some other compu-
tational tool, verify the results obtained in the text for the Australian Athletes
data. Also, obtain tests for the general POD mean function versus the POD
mean function with parallel mean functions.
6.23. The Minnesota Twins professional baseball team plays its games in the
Metrodome, an indoor stadium with a fabric roof. In addition to the large air
fans required to keep to roof from collapsing, the baseball ﬁeld is surrounded
by ventilation fans that blow heated or cooled air into the stadium. Air is
normally blown into the center of the ﬁeld equally from all directions.
According to a retired supervisor in the Metrodome, in the late innings
of some games the fans would be modiﬁed so that the ventilation air would
blow out from home plate toward the outﬁeld. The idea is that the air ﬂow
might increase the length of a ﬂy ball. For example, if this were done in the
middle of the eighth inning, then the air-ﬂow advantage would be in favor
of the home team for six outs, three in each of the eighth and ninth innings,
and in favor of the visitor for three outs in the ninth inning, resulting in a
slight advantage for the home team.
To see if manipulating the fans could possibly make any difference, a
group of students at the University of Minnesota and their professor built
a “cannon” that used compressed air to shoot baseballs. They then did the
following experiment in the Metrodome in March 2003:
1. A ﬁxed angle of 50 degrees and velocity of 150 feet per second was
selected. In the actual experiment, neither the velocity nor the angle could
be controlled exactly, so the actual angle and velocity varied from shot
to shot.
2. The ventilation fans were set so that to the extent possible all the air was
blowing in from the outﬁeld towards home plate, providing a headwind.
After waiting about 20 minutes for the air ﬂows to stabilize, 20 balls were
shot into the outﬁeld, and their distances were recorded. Additional vari-
ables recorded on each shot include the weight (in grams) and diameter
(in cm) of the ball used on that shot, and the actual velocity and angle.
3. The ventilation fans were then reversed, so as much as possible air was
blowing out toward the outﬁeld, giving a tailwind. After waiting 20 min-
utes for air currents to stabilize, 15 balls were shot into the outﬁeld, again
measuring the ball weight and diameter, and the actual velocity and angle
on each shot.
The data from this experiment are available in the ﬁle domedata.txt,
courtesy of Ivan Marusic. The variable names are Cond, the condition, head
or tail wind; Velocity, the actual velocity in feet per second; Angle, the actual
angle; BallWt, the weight of the ball in grams used on that particular test;
BallDia, the diameter in inches of the ball used on that test; Dist, distance
in feet of the ﬂight of the ball.
6.23.1. Summarize any evidence that manipulating the fans can change the
distance that a baseball travels. Be sure to explain how you reached

146
POLYNOMIALS AND FACTORS
your conclusions, and provide appropriate summary statistics that
might be useful for a newspaper reporter (a report of this experiment
is given in the Minneapolis StarTribune for July 27, 2003).
6.23.2. In light of the discussion in Section 6.5, one could argue that this
experiment by itself cannot provide adequate information to decide if
the fans can affect length of a ﬂy ball. The treatment is manipulating
the fans; each condition was set up only once and then repeatedly
observed. Unfortunately, resetting the fans after each shot is not prac-
tical because of the need to wait at least 20 minutes for the air ﬂows
to stabilize.
A second experiment was carried out in May 2003, using a similar
experimental protocol. As before, the fans were ﬁrst set to provide a
headwind, and then, after several trials, the fans were switched to a
tailwind. Unlike the ﬁrst experiment, however, the nominal Angle and
Velocity were varied according to a 3 × 2 factorial design. The data
ﬁle domedata1.txt contains the results from both the ﬁrst exper-
iment and the second experiment, with an additional column called
Date indicating which sample is which. Analyze these data, and write
a brief report of your ﬁndings.

C H A P T E R 7
Transformations
There are exceptional problems for which we know that the mean function E(Y|X)
is a linear regression mean function. For example, if (Y, X) has a joint normal
distribution, then as in Section 4.3, the conditional distribution of Y|X has a linear
mean function. Sometimes, the mean function may be determined by a theory, apart
from parameter values, as in the strong interaction data in Section 5.1.1. Often, there
is no theory to tell us the correct form for the mean function, and any parametric
form we use is little more than an approximation that we hope is adequate for the
problem at hand. Replacing either the predictors, the response, or both by nonlinear
transformations of them is an important tool that the analyst can use to extend the
number of problems for which linear regression methodology is appropriate. This
brings up two important questions: How do we choose transformations? How do
we decide if an approximate model is adequate for the data at hand? We address
the ﬁrst of these questions in this chapter, and the second in Chapters 8 and 9.
7.1
TRANSFORMATIONS AND SCATTERPLOTS
The most frequent purpose of transformations is to achieve a mean function that
is linear in the transformed scale. In problems with only one predictor and one
response, the mean function can be visualized in a scatterplot, and we can attempt
to select a transformation so the resulting scatterplot has an approximate straight-
line mean function. With many predictors, selection of transformations can be
harder, as the criterion to use for selecting transformations is less clear, so we
consider the one predictor case ﬁrst. We seek a transformation so if X is the
transformed predictor and Y is the transformed response, then the mean function
in the transformed scale is
E(Y|X = x) ≈β0 + β1x
where we have used “≈” rather than “=” to recognize that this relationship may
be an approximation and not exactly true.
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
147

148
TRANSFORMATIONS
0
1000
2000
3000
4000
5000
6000
0
2000
4000
Body weight (kg)
Brain weight (g)
Asian_elephant
Human
African_elephant
FIG. 7.1
Plot of BrainWt versus BodyWt for 62 mammal species.
Figure 7.1 contains a plot of body weight BodyWt in kilograms and brain weight
BrainWt in grams for 62 species of mammals (Allison and Cicchetti, 1976), using
the data in the ﬁle brains.txt. Apart from the three separated points for two
species of elephants and for humans, the uneven distribution of points hides any
useful visual information about the mean of BrainWt, given BodyWt. In any case,
there is little or no evidence for a straight-line mean function here. Both variables
range over several orders of magnitude from tiny species with body weights of just
a few grams to huge animals of over 6600 kg. Transformations can help in this
problem.
7.1.1
Power Transformations
A transformation family is a collection of transformations that are indexed by one
or a few parameters that the analyst can select. The family that is used most often
is called the power family, deﬁned for a strictly positive variable U by
ψ(U, λ) = Uλ
(7.1)
As the power parameter λ is varied, we get the members of this family, including
the square root and cube root transformations, λ = 1/2 or 1/3, and the inverse,
λ = −1. We will interpret the value of λ = 0 to be a log transformation. The usual
values of λ that are considered are in the range from −2 to 2, but values in the range
from −1 to +1 are ordinarily selected. The value of λ = +1 corresponds to no
transformation. The variable U must be strictly positive for these transformations
to be used, but we will have more to say later about transforming variables that
may be zero or negative. We have introduced this ψ-notation1 because we will
later consider other families of transformations, and having this notation will allow
more clarity in the discussion.
1ψ is the Greek letter psi.

TRANSFORMATIONS AND SCATTERPLOTS
149
Some statistical packages include graphical tools that can help you select power
transformations of both the predictor and the response. For example, a plot could
include slidebars to select values of the transformation parameters applied to the
horizontal and vertical variables. As different values of the parameters are selected
in the slidebars, the graph is updated to reﬂect the transformation of the data
corresponding to the currently selected value of the transformation parameter. If a
graphical interface is not available in your package, you can draw several ﬁgures
to help select a transformation. A mean smoother and the ols line added to each
of the plots may be helpful in looking at these plots.
Figure 7.2 shows plots of ψ(BrainWt, λ) versus ψ(BodyWt, λ) with the same λ
for both variables, for λ = −1, 0, 1/3, 1/2. There is no necessity for the transfor-
mation to be the same for the two variables, but it is reasonable here because both
variables are the same type of measurements, one being the weight of an object,
and the other a weight of a component of the object. If we allowed each variable
to have its own transformation parameter, the visual search for a transformation is
harder because more possibilities need to be considered.
0
50
100
150
200
0
1
2
3
4
5
6
7
−4 −2
0
2
4
6
8
−2
0
2
4
6
8
0
5
10
15
0
5
10
15
0
20
40
60
80
0
20
40
60
Brain Wt−1
Body Wt−1
loge (Brain Wt)
loge (Body Wt)
Brain Wt0.33
Body Wt0.33
Brain Wt0.5
Body Wt0.5
(a)
(b)
(c)
(d)
FIG. 7.2
Scatterplots for the brain weight data with four possible transformations. The solid line on
each plot is the ols line; the dashed line is a loess smooth.

150
TRANSFORMATIONS
From the four graphs in Figure 7.2, the clear choice is replacing the weights by
their logarithms. In this scale, the mean function appears to be a straight line, with
the smoother closely matching the ols line shown on the graph in log scale but
matching less well for the other transformations. As a bonus, the variance function
in the log plot appears to be constant.
The use of logarithms for the brain weight data may not be particularly surpris-
ing, in light of the following two empirical rules that are often helpful in linear
regression modeling:
The log rule If the values of a variable range over more than one order of
magnitude and the variable is strictly positive, then replacing the variable by
its logarithm is likely to be helpful.
The range rule If the range of a variable is considerably less than one order of
magnitude, then any transformation of that variable is unlikely to be helpful.
The log rule is satisﬁed for both BodyWt, with range 0.005 kg to 6654 kg, and
for BrainWt, with range 0.14 g to 5712 g, so log transformations would have been
indicated as a starting point for examining these variables for transformations.
Simple linear regression seems to be appropriate with both variables in log scale.
This corresponds to the physical model
BrainWt = α × BodyWt β1 × δ
(7.2)
where δ is a multiplicative error, meaning that the actual average brain weight for
a particular species is obtained by taking the mean brain weight for species of a
particular body weight and multiplying by δ. We would expect that δ would have
mean 1 and a distribution concentrated on values close to 1. On taking logarithms
and setting β1 = log(α) and e = log(δ),
log(BrainWt) = β0 + β1log(BodyWt) + e
which is the simple linear regression model. Scientists who study the relation-
ships between attributes of individuals or species call (7.2) an allometric model
(see, for example, Gould, 1966, 1973; Hahn, 1979), and the value of β1 plays an
important role in allometric studies. We emphasize, however, that not all useful
transformations will correspond to interpretable physical models.
7.1.2
Transforming Only the Predictor Variable
In the brain weight example, transformations of both the response and the predictor
are required to get a linear mean function. In other problems, transformation of only
one variable may be desirable. If we want to use a family of power transformations,
it is convenient to introduce the family of scaled power transformations, deﬁned
for strictly positive X by
ψS(X, λ) =

(Xλ −1)/λ
if λ ̸= 0
log(X)
if λ = 0
(7.3)

TRANSFORMATIONS AND SCATTERPLOTS
151
The scaled power transformations ψS(X, λ) differ from the basic power transfor-
mations ψ(X, λ) in several respects. First ψS(X, λ) is continuous as a function of
λ. Since limλ→0 ψS(X, λ) = loge(X), the logarithmic transformation is a member
of this family with λ = 0. Also, scaled power transformations preserve the direction
of association, in the sense that if (X, Y) are positively related, then (ψS(X, λ), Y)
are positively related for all values of λ. With basic power transformations, the
direction of association changes when λ < 0.
If we ﬁnd an appropriate power to use for a scaled power transformation, we
would in practice use the basic power transformation ψ(X, λ) in regression mod-
eling, since the two differ only by a scale, location, and possibly sign change. The
scaled transformations are used to select a transformation only.
If transforming only the predictor and using a choice from the power family,
we begin with the mean function
E(Y|X) = β0 + β1ψS(X, λ)
(7.4)
If we know λ, we can ﬁt (7.4) via ols and get the residual sum of squares, RSS(λ).
The estimate ˆλ of λ is simply the value of λ that minimizes RSS(λ). As a practical
matter, we do not need to know λ very precisely, and selecting λ to minimize
RSS(λ) from
λ ∈{−1, −1/2, 0, 1/3, 1/2, 1}
(7.5)
is usually adequate.
As an example of transforming only the predictor, we consider the dependence
of tree Height in decimeters on Dbh, the diameter of the tree in mm at 137 cm
above the ground, for a sample of western cedar trees in 1991 in the Upper Flat
Creek stand of the University of Idaho Experimental Forest (courtesy of Andrew
Robinson). The data are in the ﬁle ufcwc.txt. Figure 7.3 is the scatterplot of
200
400
600
800
1000
10
15
20
25
30
35
40
Dbh
Height
1
0
1
.
.
FIG. 7.3
Height versus Dbh for the red cedar data from Upper Flat Creek.

152
TRANSFORMATIONS
7.0
7.5
8.0
8.5
9.0
9.5
10.0
10
15
20
25
30
35
40
Height
log2(Dbh)
FIG. 7.4
The red cedar data from Upper Flat Creek transformed.
the data, and on this plot we have superimposed three curved lines. For each λ, we
computed ﬁtted values ˆy(λ) from the ols regression of Height on ψS(Dbh, λ). The
line for a particular value of λ is obtained by plotting the points (Dbh, ˆy(λ)) and
joining them with a line. Only three values of λ are shown in the ﬁgure because it
gets too crowded to see much with more lines, but among these three, the choice of
λ = 0 seems to match the data most closely. The choice of λ = 1 does not match
the data for large and small trees, while the inverse is too curved to match the data
for larger trees. This suggests replacing Dbh with log(Dbh), as we have done in
Figure 7.4.
As an alternative approach, the value of the transformation parameter can be
estimated by ﬁtting using nonlinear least squares. The mean function (7.4) is a
nonlinear function of the parameters because β1 multiplies the nonlinear function
ψS(X, λ) of the parameter λ. Using the methods described in Chapter 11, the
estimate of λ turns out to be ˆλ = 0.05 with a standard error of 0.15, so λ = 0 is
close enough to believe that this is a sensible transformation to use.
7.1.3
Transforming the Response Only
A transformation of the response only can be selected using an inverse ﬁtted value
plot, in which we put the ﬁtted values from the regression of Y on X on the vertical
axis and the response on the horizontal axis. In simple regression the ﬁtted values
are proportional to the predictor X, so an equivalent plot is of X on the vertical
axis versus Y on the horizontal axis. The method outlined in Section 7.1.2 can then
be applied to this inverse problem, as suggested by Cook and Weisberg (1994).
Thus, to estimate a transformation ψS(Y, λy), start with the mean function
E( ˆy|Y) = α0 + α1ψS(Y, λy)

TRANSFORMATIONS AND SCATTERPLOT MATRICES
153
and estimate λy. An example of the use of an inverse response plot will be given
in Section 7.3.
7.1.4
The Box and Cox Method
Box and Cox (1964) provided another general method for selecting transformations
of the response that is applicable both in simple and multiple regression. As with
the previous methods, we will select the transformation from a family indexed by
a parameter λ. For the Box–Cox method, we need a slightly more complicated
version of the power family that we will call the modiﬁed power family, deﬁned
by Box and Cox (1964) for strictly positive Y to be
ψM(Y, λy) = ψS(Y, λy) × gm(Y)1−λy
(7.6)
=

gm(Y)1−λy × (Y λy −1)/λy
if λy ̸= 0
gm(Y) × log(Y)
if λy = 0
where gm(Y) is the geometric mean of the untransformed variable2.
In the Box–Cox method, we assume that the mean function
E(ψM(Y, λy)|X = x) = β′x
(7.7)
holds for some λy. If λy were known, we could ﬁt the mean function (7.7) using ols
because the transformed response ψM(Y, λy) would then be completely speciﬁed.
Write the residual sum of squares from this regression as RSS(λy). Multiplica-
tion of the scaled power transformation by gm(Y)1−λ guarantees that the units of
ψM(Y, λy) are the same for all values of λy, and so all the RSS(λy) are in the
same units. We estimate λy to be the value of the transformation parameter that
minimizes RSS(λy). From a practical point of view, we can again select λy from
among the choices in (7.5).
The Box–Cox method is not transforming for linearity, but rather it is trans-
forming for normality: λ is chosen to make the residuals from the regression of
ψ(Y, λy) on X as close to normally distributed as possible. Hernandez and Johnson
(1980) point out that “as close to normal as possible” need not be very close to
normal, and so graphical checks are desirable after selecting a transformation. The
Box and Cox method will also produce a conﬁdence interval for the transformation
parameter; see Appendix A.11.1 for details.
7.2
TRANSFORMATIONS AND SCATTERPLOT MATRICES
The data described in Table 7.1 and given in the data ﬁle highway.txt are
taken from an unpublished master’s paper in civil engineering by Carl Hoffstedt.
2If the values of Y are y1, . . . , yn, the geometric mean of Y is gm(Y) = exp( log(yi)/n), using
natural logarithms.

154
TRANSFORMATIONS
TABLE 7.1
The Highway Accident Dataa
Variable
Description
Rate
1973 accident rate per million vehicle miles
Len
Length of the segment in miles
ADT
Estimated average daily trafﬁc count in thousands
Trucks
Truck volume as a percent of the total volume
Slim
1973 speed limit
Shld
Shoulder width in feet of outer shoulder on the roadway
Sigs
Number of signalized interchanges per mile in the segment
aAdditional variables appear in the ﬁle highway.txt, and will be described in Table 10.5.
They relate the automobile accident rate in accidents per million vehicle miles to
several potential terms. The data include 39 sections of large highways in the state
of Minnesota in 1973. The goal of this analysis was to understand the impact of
the design variables, Acpts, Slim, Sigs, and Shld that are under the control of the
highway department, on accidents. The other variables are thought to be important
determinants of accidents but are more or less beyond the control of the highway
department and are included to reduce variability due to these uncontrollable fac-
tors. We have no particular reason to believe that Rate will be a linear function of
the predictors, or any theoretical reason to prefer any particular form for the mean
function.
An important ﬁrst step in this analysis is to examine the scatterplot matrix of all
the predictors and the response, as given in Figure 7.5. Here are some observations
about this scatterplot matrix that might help in selecting transformations:
1. The variable Sigs, the number of trafﬁc lights per mile, is zero for freeway-
type road segments but can be well over 2 for other segments. Transforma-
tions may help with this variable, but since it has non positive values, we
cannot use the power transformations directly. Since Sigs is computed as the
number of signals divided by Len, we will replace Sigs by a related variable
Sigs1 deﬁned by
Sigs1 = Sigs × Len + 1
Len
This variable is always positive and can be transformed using the power
family.
2. ADT and Len have a large range, and logarithms are likely to be appropriate
for them.
3. Slim varies only from 40 mph to 70 mph, with most values in the range 50
to 60. Transformations are unlikely to be much use here.
4. Each of the predictors seems to be at least modestly associated with Rate,
as the mean function for each of the plots in the top row of Figure 7.5 is
not ﬂat.

TRANSFORMATIONS AND SCATTERPLOT MATRICES
155
Rate
10
30
6
10
2
6
10
2
6
10
30
Len
ADT
0
30
70
6
10
Trks
Slim
40
55
70
2
6
10
Shld
2
6
0
30
70
40
55
70
0.0
1.5
0.0
1.5
Sigs
FIG. 7.5
The highway accident data, no transformations.
5. Many of the predictors are also related to each other. In some cases, the
mean functions for the plots of predictor versus predictor appear to be linear;
in other cases, they are not linear.
Given these preliminary views of the scatterplot matrix, we now have the daunting
task of ﬁnding good transformations to use. This raises immediate questions: What
are the goals in selecting transformations? How can we decide if we have made a
good choice?
The overall goal of transforming in linear regression is to ﬁnd transformations
in which multiple linear regression matches the data to a good approximation. The
connection between this goal and choosing transformations that make the 2D plots
of predictors have linear mean functions is not entirely obvious. Important work
by Brillinger (1983) and Li and Duan (1989) provides a theoretical connection.
Suppose we have a response variable Y and a set of predictors X, and suppose it
were true that
E(Y|X = x) = g(β′x)
(7.8)

156
TRANSFORMATIONS
for some completely unknown and unspeciﬁed function g. According to this, the
mean of Y depends on X only through a linear combination of the terms in X, and
if we could draw a graph of Y versus β′x, this graph would have g as its mean
function. We could then either estimate g, or we could transform Y to make the
mean function linear. All this depends on estimating β without specifying anything
about g. Are there conditions under which the ols regression of Y on X can help
us learn about β?
7.2.1
The 1D Estimation Result and Linearly Related Predictors
Suppose that A = a′X and B = b′X were any two linear combinations of the terms
in X, such that
E(A|B) = γ0 + γ1B
(7.9)
so the graph of A versus B has a straight-line mean function. We will say that X
is a set of linear predictors if (7.9) holds for all linear combinations A and B. The
condition that all the graphs in a scatterplot matrix of X have straight-line mean
functions is weaker than (7.9), but it is a reasonable condition that we can check in
practice. Requiring that X has a multivariate normal distribution is much stronger
than (7.9). Hall and Li (1993) show that (7.9) holds approximately as the number
of predictors grows large, so in very large problems, transformation becomes less
important because (7.9) will hold approximately without any transformations.
Given that (7.9) holds at least to a reasonable approximation, and assuming
that E(Y|X = x) = g(β′x), then the ols estimate ˆβ is a consistent estimate of cβ
for some constant c that is usually nonzero (Li and Duan, 1989; see also Cook,
1998). Given this theorem, a useful general procedure for applying multiple linear
regression analysis is:
1. Transform predictors to get terms for which (7.9) holds, at least approxi-
mately. The terms in X may include dummy variables that represent factors,
which should not be transformed, as well as transformations of continuous
predictors.
2. We can estimate g from the 2D scatterplot of Y versus ˆβ
′x, where ˆβ is the
ols estimator from the regression of Y on X. Almost equivalently, we can
estimate a transformation of Y either from the inverse plot of ˆβ
′x versus Y
or from using the Box–Cox method.
This is a general and powerful approach to building regression models that match
data well, based on the assumption that (7.8) is appropriate for the data. We have
already seen mean functions in Chapter 6 for which (7.8) does not hold because of
the inclusion of interaction terms, and so transformations chosen using the methods
discussed here may not provide a comprehensive mean function when interactions
are present.
The Li–Duan theorem is actually much more general and has been extended to
problems with interactions present and to many other estimation methods beyond

TRANSFORMATIONS AND SCATTERPLOT MATRICES
157
ols. See Cook and Weisberg (1999a, Chapters 18–20) and, at a higher mathematical
level, Cook (1998).
7.2.2
Automatic Choice of Transformation of Predictors
Using the results of Section 7.2.1, we seek to transform the predictors so that all
plots of one predictor versus another have a linear mean function, or at least have
mean functions that are not too curved. Without interactive graphical tools, or some
automatic method for selecting transformations, this can be a discouraging task,
as the analyst may need to draw many scatterplot matrices to get a useful set of
transformations.
Velilla (1993) proposed a multivariate extension of the Box and Cox method to
select transformations to linearity, and this method can often suggest a very good
starting point for selecting transformations of predictors. Starting with k untrans-
formed strictly positive predictors X = (X1, . . . , Xk), we will apply a modiﬁed
power transformation to each Xj, and so there will be k transformation parame-
ters collected into λ = (λ1, λ2, . . . , λk)′. We will write ψM(X, λ) to be the set of
variables
ψM(X, λ) = (ψM(X1, λ1), . . . , ψM(Xk, λk))
Let V(λ) be the sample covariance matrix of the transformed data ψM(X, λ). The
value ˆλ is selected as the value of λ that minimizes the logarithm of the determinant
of V(λ). This minimization can be carried using a general function minimizer
included in high-level languages such as R, S-plus, Maple, Mathematica, or even
Excel. The minimizers generally require only speciﬁcation of the function to be
minimized and a set of starting values for the algorithm. The starting values can
be taken to be λ = 0, λ = 1, or some other appropriate vector of zeros and ones.
Returning to the highway data, we eliminate Slim as a variable to be transformed
because its range is too narrow. For the remaining terms, we get the summary of
transformations using the multivariate Box–Cox method in Table 7.2. The table
TABLE 7.2
Power Transformations to Normality for the Highway Data
Box Cox Transformations to Multivariate normality
Est.Power Std.Err. Wald(Power=0) Wald(Power=1)
Len
0.1429
0.2124
0.6728
-4.0349
ADT
0.0501
0.1204
0.4162
-7.8904
Trks
-0.7019
0.6132
-1.1447
-2.7754
Shld
1.3455
0.3630
3.7065
0.9518
Sigs1
-0.2440
0.1488
-1.6402
-8.3621
L.R. test, all powers = 0:
23.373
df = 5
p = 3e-04
L.R. test, all powers = 1:
133.179
df = 5
p = 0
L.R. test, of (0,0,0,1,0):
6.143
df = 5
p = 0.29

158
TRANSFORMATIONS
gives the value of ˆλ in the column marked “Est. power.” The standard errors are
computed as outlined in Appendix A.11.2. For our purposes, the standard errors
can be treated like standard errors of regression coefﬁcients. The next two columns
are like t-tests of the transformation parameter equal to zero or to one. These tests
should be compared with a normal distribution, so values larger in absolute value
than 1.96 correspond to p-values less than 0.05. The power parameters for Len,
ADT, Trks and Sigs1 do not appear to be different from zero, and Shld does not
appear to be different from one. At the foot of the table are three likelihood ratio
tests. The ﬁrst of these tests is that all powers are zero; this is ﬁrmly rejected as the
approximate χ2(5) is very large. Similarly, the test for no transformation (λ = 1)
is ﬁrmly rejected. The test that the ﬁrst three variables should be in log scale, the
next untransformed, and the last in log scale, has a p-value 0.29 and suggests using
these simple transformations in further analysis with these data. The predictors in
transformed scale, along with the response, are shown in Figure 7.6. All these 2D
plots have a linear mean function, or at least are not strongly nonlinear. They
provide a good place to start regression modeling.
Rate
2
4
2.6
3.4
2
6
10
2
6
2
4
logLen
logADT
0
3
6
2.6
3.4
logTrks
Slim
40
55
70
2
6
10
Shld
2
6
0
3
6
40
55
70
−4
−1
4
1
logSigs1
FIG. 7.6
Transformed predictors for the highway data.

TRANSFORMING THE RESPONSE
159
7.3
TRANSFORMING THE RESPONSE
Once the terms are transformed, we can turn our attention to transforming the
response. Figure 7.7 is the inverse ﬁtted value plot for the highway data using the
transformed terms determined in the last section. This plot has the response Rate
on the horizontal axis and the ﬁtted values from the regression of Rate on the
transformed predictors on the vertical axis. Cook and Weisberg (1994) have shown
that if the predictors are approximately linearly related, then we can use the method
of Section 7.1.2 to select a transformation for Rate. Among the three curves shown
on this plot, the logarithmic seems to be the most appropriate.
The Box–Cox method provides an alternative procedure for ﬁnding a trans-
formation of the response. It is often summarized by a graph with λy on the
horizontal axis and either RSS(λy) or better yet −(n/2) log(RSS(λy)/n) on the
vertical axis. With this latter choice, the estimate ˆλy is the point that maximizes
the curve, and a conﬁdence interval for the estimate is given by the set of all
λy with log(L(ˆλy)) −log(L(λy) < 1.92; see Appendix A.11.1. This graph for the
highway data is shown in Figure 7.8, with ˆλ ≈−0.2 and the conﬁdence interval of
about −0.8 to +0.3. The log transformation is in the conﬁdence interval, agreeing
with the inverse ﬁtted value plot.
In the highway data, the two transformation methods for the response seem to
agree, but there is no theoretical reason why they need to give the same transforma-
tion. The following path is recommended for selecting a response transformation:
1. With approximately linear predictors, draw the inverse response plot of ˆy
versus the response. If this plot shows a clear nonlinear trend, then the
response should be transformed to match the nonlinear trend. There is no
reason why only power transformations should be considered. For example,
2
4
6
8
0
2
4
6
8
Rate
Fitted values
1
0
1
.
FIG. 7.7
Inverse ﬁtted value plot for the highway data.

160
TRANSFORMATIONS
−1
0
1
2
−90
−85
−80
−75
−70
log−Likelihood
95%
λy
−2
FIG. 7.8
Box–Cox summary graph for the highway data.
the transformation could be selected using a smoother. If there is no clear
nonlinear trend, transformation of the response is unlikely to be helpful.
2. The Box–Cox procedure can be used to select a transformation to normality.
It requires the use of a transformation family.
For the highway data, we now have a reasonable starting point for regression, with
several of the predictors and the response all transformed to log scale. We will
continue with this example in later chapters.
7.4
TRANSFORMATIONS OF NONPOSITIVE VARIABLES
Several transformation families for a variable U that includes negative values have
been suggested. The central idea is to use the methods discussed in this chapter for
selecting a transformation from a family but to use a family that permits U to be
non positive. One possibility is to consider transformations of the form (U + γ )λ,
where γ is sufﬁciently large to ensure that U + γ is strictly positive. We used a
variant of this method with the variable Sigs in the highway data. In principle, (γ, λ)
could be estimated simultaneously, although in practice estimates of γ are highly
variable and unreliable. Alternatively, Yeo and Johnson (2000) proposed a family
of transformations that can be used without restrictions on U that have many of the
good properties of the Box–Cox power family. These transformations are deﬁned by
ψYJ(U, λ) =
 ψM(U + 1, λ)
if U ≥0
ψM(−U + 1, 2 −λ)
if U < 0
(7.10)
If U is strictly positive, then the Yeo–Johnson transformation is the same as the
Box–Cox power transformation of (U + 1). If U is strictly negative, then the
Yeo–Johnson transformation is the Box–Cox power transformation of (−U + 1),

PROBLEMS
161
−2
0
2
4
−4
−6
−8
−4
−2
0
2
4
y
Transform, λ = 0.5
(c)
5
0
−5
−10
−15
−4
−2
0
2
4
y
Transform, λ = 0
(b)
−20 −10
0
10
−30
−40
−50
−4
−2
0
2
4
y
Transform, λ = −1
(a)
FIG. 7.9
Comparison of Box–Cox (dashed lines) and Yeo–Johnson (solid lines) power transfor-
mations for λ = −1, 0, 0.5. The Box–Cox transformations and Yeo–Johnson transformations behave
differently for values of y close to zero.
but with power 2 −λ. With both negative and positive values, the transformation is
a mixture of these two, so different powers are used for positive and negative values.
In this latter case, interpretation of the transformation parameter is difﬁcult, as it
has a different meaning for U ≥0 and for U < 0. Figure 7.9 shows the Box–Cox
transformation and Yeo–Johnson transformation for the values of λ = −1, 0, .5.
For positive values, the two transformations differ in their behavior with values
close to zero, with the Box–Cox transformations providing a much larger change
for small values than do the Yeo–Johnson transformations.
PROBLEMS
7.1. The data in the ﬁle baeskel.txt were collected in a study of the effect of
dissolved sulfur on the surface tension of liquid copper (Baes and Kellogg,

162
TRANSFORMATIONS
1953). The predictor Sulfur is the weight percent sulfur, and the response
is Tension, the decrease in surface tension in dynes per cm. Two replicate
observations were taken at each value of Sulfur. These data were previously
discussed by Sclove (1968).
7.1.1. Draw the plot of Tension versus Sulfur to verify that a transformation
is required to achieve a straight-line mean function.
7.1.2. Set λ = −1, and ﬁt the mean function
E(Tension|Sulfur) = β0 + β1Sulfurλ
using ols; that is, ﬁt the ols regression with Tension as the response
and 1/Sulfur as the predictor. Let new be a vector of 100 equally
spaced values between the minimum value of Sulfur and its maximum
value. Compute the ﬁtted values from the regression you just ﬁt, given
by Fit.new = ˆβ0 + ˆβ1newλ. Then, add to the graph you drew in Prob-
lem 7.1.1 the line joining the points (new, Fit.new). Repeat for λ = 0, 1.
Which of these three choices of λ gives ﬁtted values that match the data
most closely?
7.1.3. Replace Sulfur by its logarithm, and consider transforming the response
Tension. To do this, draw the inverse ﬁtted value plot with the ﬁtted
values from the regression of Tension on log log(Sulphur) on the verti-
cal axis and Tension on the horizontal axis. Repeat the methodology of
Problem 7.1.2 to decide if further transformation of the response will
be helpful.
7.2. The (hypothetical) data in the ﬁle stopping.txt give stopping times for
n = 62 trials of various automobiles traveling at Speed miles per hour and the
resulting stopping Distance in feet (Ezekiel and Fox, 1959).
7.2.1. Draw the scatterplot of Distance versus Speed. Add the simple regres-
sion mean function to your plot. What problems are apparent? Compute
a test for lack of ﬁt, and summarize results.
7.2.2. Find an appropriate transformation for Distance that can linearize this
regression.
7.2.3. Hald (1960) has suggested on the basis of a theoretical argument that the
mean function E(Distance|Speed) = β0 + β1Speed + β2Speed2, with
Var(Distance|Speed) = σ 2Speed2 is appropriate for data of this type.
Compare the ﬁt of this model to the model found in Problem 7.2.2. For
Speed in the range 0 to 40 mph, draw the curves that give the predicted
Distance from each model, and qualitatively compare them.
7.3. This problem uses the data discussed in Problem 1.5. A major source of water
in Southern California is the Owens Valley. This water supply is in turn
replenished by spring runoff from the Sierra Nevada mountains. If runoff
could be predicted, engineers, planners, and policy makers could do their

PROBLEMS
163
jobs more efﬁciently. The data in the ﬁle water.txt contains 43 years’ of
precipitation measurements taken at six sites in the mountains, in inches of
water, and stream runoff volume at a site near Bishop, California. The three
sites with name starting with “O” are fairly close to each other, and the three
sites starting with “A” are also fairly close to each other.
7.3.1. Load the data ﬁle, and construct the scatterplot matrix of the six
snowfall variables, which are the predictors in this problem. Using
the methodology for automatic choice of transformations outlined in
Section 7.2.2, ﬁnd transformations to make the predictors as close to
linearly related as possible. Obtain a test of the hypothesis that all
λj = 0 against a general alternative, and summarize your results. Do
the transformations you found appear to achieve linearity? How do
you know?
7.3.2. Given log transformations of the predictors, show that a log transfor-
mation of the response is reasonable.
7.3.3. Consider the multiple linear regression model with mean function
given by
E(log(y)|x) = β0 + β1 log(APMAM) + β2 log(APSAB)
+β3 log(APSLAKE) + β4 log(OPBPC)
+β5 log(OPRC) + β6 log(OPSLAKE)
with constant variance function. Estimate the regression coefﬁcients
using ols. You will ﬁnd that two of the estimates are negative; Which
are they? Does a negative coefﬁcient make any sense? Why are the
coefﬁcients negative?
7.3.4. In the ols ﬁt, the regression coefﬁcient estimates for the three predictors
beginning with “O” are approximately equal. Are there conditions under
which one might expect these coefﬁcients to be equal? What are they?
Test the hypothesis that they are equal against the alternative that they
are not all equal.
7.3.5. Write one or two paragraphs that summarize the use of the snowfall
variables to predict runoff. The summary should discuss the important
predictors, give useful graphical summaries, and give an estimate of
variability. Be creative.
7.4. The data in the ﬁle salarygov.txt give the maximum monthly salary for
495 non-unionized job classes in a midwestern governmental unit in 1986.
The variables are described in Table 7.3.
7.4.1. The data as given has as its unit of analysis the job class. In a study of
the dependence of maximum salary on skill, one might prefer to have
as unit of analysis the employee, not the job class. Discuss how this
preference would change the analysis.

164
TRANSFORMATIONS
TABLE 7.3
The Governmental Salary Data
Variable
Description
MaxSalary
Maximum salary in dollars for employees in this job class, the response
NE
Total number of employees currently employed in this job class
NW
Number of women employees in the job class
Score
Score for job class based on difﬁculty, skill level, training requirements and
level of responsibility as determined by a consultant to the governmental
unit. This value for these data is in the range between 82 to 1017.
JobClass
Name of the job class; a few names were illegible or partly illegible
7.4.2. Examine the scatterplot of MaxSalary versus Score. Find transforma-
tion(s) that would make the mean function for the resulting scatterplot
approximately linear. Does the transformation you choose also appear
to achieve constant variance?
7.4.3. According to Minnesota statutes, and probably laws in other states
as well, a job class is considered to be female dominated if 70% of
the employees or more in the job class are female. These data were
collected to examine whether female-dominated positions are compen-
sated at a lower level, adjusting for Score, than are other positions.
Create a factor with two levels that divides the job classes into female
dominated or not, ﬁt appropriate models, and summarize your results.
Be mindful of the need to transform variables and the possibility of
weighting.
7.4.4. An alternative to using a factor for female-dominated jobs is to use a
term NW/NE, the fraction of women in the job class. Repeat the last
problem, but encoding the information about sex using this variable in
place of the factor.
7.5. World cities
The Union Bank of Switzerland publishes a report enti-
tled Prices and Earnings Around the Globe on their internet web site,
www.ubs.com. The data in the ﬁle BigMac2003.txt and described in
Table 7.4 are taken from their 2003 version for 70 world cities.
7.5.1. Draw the scatterplot with BigMac on the vertical axis and FoodIndex
on the horizontal axis. Provide a qualitative description of this graph.
Use an inverse ﬁtted value plot and the Box–Cox method to ﬁnd a
transformation of BigMac so that the resulting scatterplot has a linear
mean function. Two of the cities, with very large values for BigMac,
are very inﬂuential for selecting a transformation. You should do this
exercise with all the cities and with those two cities removed.
7.5.2. Draw the scatterplot matrix of the three variables (BigMac, Rice, Bread),
and use the multivariate Box–Cox procedure to decide on normaliz-
ing transformations. Test the null hypothesis that λ = (1, 1, 1)′ against a

PROBLEMS
165
TABLE 7.4
Global Price Comparison Data
Variable
Description
BigMac
Minutes of labor to buy a Big Mac hamburger based on a typical wage
averaged over 13 occupations
Bread
Minutes of labor to buy 1 kg bread
Rice
Minutes of labor to buy 1 kg of rice
Bus
Lowest cost of 10 km public transit
FoodIndex
Food price index, Zurich=100
TeachGI
Primary teacher’s gross annual salary, thousands of US dollars
TeachNI
Primary teacher’s net annual salary, thousands of US dollars
TaxRate
100 × (TeachGI −TeachNI)/TeackGI. In some places, this is negative,
suggesting a government subsidy rather than tax
TH
Teacher’s hours per week of work
Apt
Monthly rent in US dollars of a typical three-room apartment
City
City name
Source: Most of the data are from the Union Bank of Switzerland publication Prices and Earnings
Around the Globe, 2003 edition, from www.ubs.com.
general alternative. Does deleting Karachi and Nairobi change your con-
clusions?
7.5.3. Set up the regression using the four terms, log(Bread), log(Bus),
log(TeachGI), and Apt0.33, and with response BigMac. Draw the inverse
ﬁtted value plot of ˆy versus BigMac. Estimate the best power trans-
formation. Check on the adequacy of your estimate by reﬁtting the
regression model with the transformed response and drawing the inverse
ﬁtted value plot again. If transformation was successful, this second
inverse ﬁtted value plot should have a linear mean function.
7.6. The data in the ﬁle wool.txt were introduced in Section 6.3. For this prob-
lem, we will start with Cycles, rather than its logarithm, as the response.
7.6.1. Draw the scatterplot matrix for these data and summarize the informa-
tion in this plot.
7.6.2. View all three predictors as factors with three levels, and without trans-
forming Cycles, ﬁt the second-order mean function with terms for all
main effects and all two-factor interactions. Summarize results.
7.6.3. Fit the ﬁrst-order mean function consisting only of the main effects.
From Problem 7.6.2, this mean function is not adequate for these data
based on using Cycles as the response. Use both the inverse ﬁtted value
plot and the Box–Cox method to select a transformation for Cycles
based on the ﬁrst-order mean function.
7.6.4. In the transformed scale, reﬁt the second-order model, and show that
none of the interactions are required in this scale. For this problem,
the transformation leads to a much simpler model than is required for

166
TRANSFORMATIONS
the response in the original scale. This is an example of removable
nonadditivity.
7.7. Justify transforming Miles in the Fuel data.
7.8. The data ﬁle UN3.txt contains data described in Table 7.5. There are data
for n = 125 localities, mostly UN member countries, for which values are
observed for all the variables recorded.
Consider the regression problem with ModernC as the response variable
and the other variables in the ﬁle as deﬁning terms.
7.8.1. Select appropriate transformations of the predictors to be used as terms.
(Hint: Since Change is negative for some localities, the Box–Cox fam-
ily of transformations cannot be used directly.)
7.8.2. Given the transformed predictors as terms, select a transformation for
the response.
7.8.3. Fit the regression using the transformations you have obtained, and
summarize your results.
TABLE 7.5
Description of Variables in the Data File UN3.txt
Variable
Description
Locality
Country/locality name
ModernC
Percent of unmarried women using a modern method of contraception
Change
Annual population growth rate, percent
PPgdp
Per capita gross national product, US dollars
Frate
Percent of females over age 15 economically active
Pop
Total 2001 population, 1000s
Fertility
Expected number of live births per female, 2000
Purban
Percent of population that is urban, 2001
Source: The data were collected from http://unstats.un.org/unsd/demographic and refer to values col-
lected between 2000 and 2003.

C H A P T E R 8
Regression Diagnostics: Residuals
So far in this book, we have mostly used graphs to help us decide what to do
before ﬁtting a regression model. Regression diagnostics are used after ﬁtting to
check if a ﬁtted mean function and assumptions are consistent with observed data.
The basic statistics here are the residuals or possibly rescaled residuals. If the ﬁtted
model does not give a set of residuals that appear to be reasonable, then some
aspect of the model, either the assumed mean function or assumptions concerning
the variance function, may be called into doubt. A related issue is the importance
of each case on estimation and other aspects of the analysis. In some data sets, the
observed statistics may change in important ways if one case is deleted from the
data. Such a case is called inﬂuential, and we shall learn to detect such cases. We
will be led to study and use two relatively unfamiliar diagnostic statistics, called
distance measures and leverage values. We concentrate on graphical diagnostics
but include numerical quantities that can aid in interpretation of the graphs.
8.1
THE RESIDUALS
Using the matrix notation outlined in Chapter 3, we begin by deriving the properties
of residuals. The basic multiple linear regression model is given by
Y = Xβ + e
Var(e) = ˆσ 2I
(8.1)
where X is a known matrix with n rows and p′ columns, including a column of 1s
for the intercept if the intercept is included in the mean function. We will further
assume that we have selected a parameterization for the mean function so that X
has full column rank, meaning that the inverse (X′X)−1 exists; as we have seen
previously, this is not an important limitation on regression models because we can
always delete terms from the mean function, or equivalently delete columns from
X, until we have full rank. The p′ × 1 vector β is the unknown parameter vector.
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
167

168
REGRESSION DIAGNOSTICS: RESIDUALS
The vector e consists of unobservable errors that we assume are equally variable
and uncorrelated, unless stated otherwise.
In ﬁtting model (8.1), we estimate β by ˆβ = (X′X)−1X′Y, and the ﬁtted values
ˆY corresponding to the observed values Y are then given by
ˆY = X ˆβ
= X(X′X)−1X′Y
= HY
(8.2)
where H is the n × n matrix deﬁned by
H = X(X′X)−1X′
(8.3)
H is called the hat matrix because it transforms the vector of observed responses
Y into the vector of ﬁtted responses ˆY. The vector of residuals ˆe is deﬁned by
ˆe = Y −ˆY
= Y −X ˆβ
= Y −X(X′X)−1X′Y
= (I −H)Y
(8.4)
8.1.1
Difference Between ˆe and e
The errors e are unobservable random variables, assumed to have zero mean and
uncorrelated elements, each with common variance σ 2. The residuals ˆe are com-
puted quantities that can be graphed or otherwise studied. Their mean and variance,
using (8.4) and Appendix A.7, are
E(ˆe) = 0
Var(ˆe) = σ 2(I −H)
(8.5)
Like the errors, each of the residuals has zero mean, but each residual may have
a different variance. Unlike the errors, the residuals are correlated. From (8.4), the
residuals are linear combinations of the errors. If the errors are normally distributed,
so are the residuals. If the intercept is included in the model, then the sum of the
residuals is 0, ˆe′1 =  ˆei = 0. In scalar form, the variance of the ith residual is
Var(ˆei) = ˆσ 2(1 −hii)
(8.6)
where hii is the ith diagonal element of H. Diagnostic procedures are based on the
computed residuals, which we would like to assume behave as the unobservable
errors would. The usefulness of this assumption depends on the hat matrix, since it
is H that relates e to ˆe and also gives the variances and covariances of the residuals.

THE RESIDUALS
169
8.1.2
The Hat Matrix
H is n × n and symmetric with many special properties that are easy to verify
directly from (8.3). Multiplying X on the left by H leaves X unchanged, HX = X.
Similarly, (I −H)X = 0. The property HH = H2 = H also shows that H(I −H) =
0, so the covariance between the ﬁtted values HY and residuals (I −H)Y is
Cov(ˆe, ˆY) = Cov(HY, (I −H)Y)
= σ 2H(I −H) = 0
Another name for H is the orthogonal projection on the column space of X. The
elements of H, the hij , are given by
hij = x′
i(X′X)−1xj = x′
j(X′X)−1xi = hji
(8.7)
Many helpful relationships can be found between the hij . For example,
n

i=1
hii = p′
(8.8)
and, if the mean function includes an intercept,
n

i=1
hij =
n

j=1
hij = 1
(8.9)
Each diagonal element hii is bounded below by 1/n and above by 1/r, if r is the
number of rows of X that are identical to xi.
As can be seen from (8.6), cases with large values of hii will have small values
for Var(ˆei); as hii gets closer to 1, this variance will approach 0. For such a case,
no matter what value of yi is observed for the ith case, we are nearly certain to
get a residual near 0. Hoaglin and Welsch (1978) pointed this out using a scalar
version of (8.2),
ˆyi =
n

j=1
hij yj = hiiyi +
n

j̸=i
hij yj
(8.10)
In combination with (8.9), equation (8.10) shows that as hii approaches 1, ˆyi gets
closer to yi. For this reason, they called hii the leverage of the ith case.
Cases with large values of hii will have unusual values for xi. Assuming that
the intercept is in the mean function, and using the notation of the deviations from
the average cross-products matrix discussed in Chapter 3, hii can be written as
hii = 1
n + (x∗
i −x)′(X ′X)−1(x∗
i −x)
(8.11)
The second term on the right-hand side of (8.11) is the equation of an ellipsoid
centered at x, and x′
i = (1, x∗
i
′).

170
REGRESSION DIAGNOSTICS: RESIDUALS
Purban
log(PPgdp)
−50
0
50
100
150
200
0
5
10
15
20
25
Liechtenstein
FIG. 8.1
Contours of constant leverage in two dimensions.
For example, consider again the United Nations data, Section 3.1. The plot of
log(PPgdp) versus Purban is given in the scatterplot in Figure 8.1. The ellipses
drawn on graph correspond to elliptical contours of constant hii for hii = 0.02,
0.04, 0.06, 0.08, and 0.10. Any point that falls exactly on the outer contour would
have hii = 0.10, while points on the innermost contour have hii = 0.02. Points
near the long or major axis of the ellipsoid need to be much farther away from x,
in the usual Euclidean distance sense, than do points closer to the minor axis, to
have the same values for hii 1.
In the example, the localities with the highest level of urbanization, which are
Bermuda, Hong Kong, Singapore, and Guadalupe, all with 100% urbanization, do
not have particularly high leverage, as all the points for these places are between
the contour for hii = 0.04 and 0.06. None of the hii is very large, with the largest
value for the marked point for Liechtenstein, which has relatively high income for
relatively low urbanization. In other problems, high-leverage points with values
close to 1 can occur, and identifying these cases is very useful in understanding a
regression problem.
8.1.3
Residuals and the Hat Matrix with Weights
When Var(e) = σ 2W−1 with W a known diagonal matrix of positive weights as
in Section 5.1, all the results so far in this section require some modiﬁcation. A
useful version of the hat matrix is given by (see Problem 8.9)
H = W1/2X(X′WX)−1X′W1/2
(8.12)
1The term Purban is a percentage between 0 and 100. Contours of constant leverage corresponding
to Purban > 100 are shown to give the shape of the contours, even though in this particular problem
points could not occur in this region.

THE RESIDUALS
171
and the leverages are the diagonal elements of this matrix. The ﬁtted values are
given as usual by ˆY = X ˆβ, where now ˆβ is the wls estimator.
The deﬁnition of the residuals is a little trickier. The “obvious” deﬁnition of a
residual is, in scalar version, yi −ˆβ
′xi, but this choice has important deﬁciencies.
First, the sum of squares of these residuals will not equal the residual sum of
squares because the weights are ignored. Second, the variance of the ith residual
will depend on the weight of case i.
Both of these problems can be solved by deﬁning residuals for weighted least
squares for i = 1, . . . , n by
ˆei = √wi(yi −ˆβ
′xi)
(8.13)
The sum of squares of these residuals is the residual sum of squares, and the
variance of the residuals does not depend on the weight. When all the weights
are equal to 1, (8.13) reduces to (8.4). In drawing graphs and other diagnostic
procedures discussed in this book, (8.13) should be used to deﬁne residuals. Some
computer packages use the unweighted residuals rather than (8.13) by default.
There is no consistent name for these residuals. For example, in R and S-plus,
the residuals deﬁned by (8.13) are called Pearson residuals in some functions, and
weighted residuals elsewhere. In this book, the symbols ˆei or ˆe always refer to the
residuals deﬁned by (8.13).
8.1.4
The Residuals When the Model Is Correct
Suppose that U is equal to one of the terms in the mean function, or some linear
combination of the terms. Residuals are generally used in scatterplots of the resid-
uals ˆe against U. The key features of these residual plots when the correct model
is ﬁt are as follows:
1. The mean function is E(ˆe|U) = 0. This means that the scatterplot of residuals
on the horizontal axis versus any linear combination of the terms should have
a constant mean function equal to 0.
2. Since Var(ˆe|U) = σ 2(1 −hii) even if the ﬁtted model is correct, the variance
function is not quite constant. The variability will be smaller for high-leverage
cases with hii close to 1.
3. The residuals are correlated, but this correlation is generally unimportant and
not visible in residual plots.
When the model is correct, residual plots should look like null plots.
8.1.5
The Residuals When the Model Is Not Correct
If the ﬁtted model is based on incorrect assumptions, there will be a plot of residuals
versus some term or combination of terms that is not a null plot. Figure 8.2 shows
several generic residual plots for a simple linear regression problem. The ﬁrst plot is

172
REGRESSION DIAGNOSTICS: RESIDUALS
0
(a)
yi or xi
∧
0
ri
ri
(b)
yi or xi
∧
0
(c)
yi or xi
∧
ri
0
(d)
yi or xi
∧
ri
0
(e)
yi
∧
ri
0
(f)
yi
∧
ri
0
(g)
yi
∧
ri
0
(h)
yi
∧
FIG. 8.2
Residual plots: (a) null plot; (b) right-opening megaphone; (c) left-opening megaphone; (d)
double outward box; (e)–(f) nonlinearity; (g)–(h) combinations of nonlinearity and nonconstant variance
function.
a null plot that indicates no problems with the ﬁtted model. From Figures 8.2b–d, in
simple regression, we would infer nonconstant variance as a function of the quantity
plotted on the horizontal axis. The curvature apparent in Figures 8.2e–h suggests
an incorrectly speciﬁed mean function. Figures 8.2g–h suggest both curvature and
nonconstant variance.
In models with many terms, we cannot necessarily associate shapes in a residual
plot with a particular problem with the assumptions. For example, Figure 8.3 shows
a residual plot for the ﬁt of the mean function E(Y|X = x) = β0 + β1x1 + β2x2
for the artiﬁcial data given in the ﬁle caution.txt from Cook and Weisberg

THE RESIDUALS
173
0.05
0.15
0.25
0.35
−0.2
0.0
0.2
0.4
Fitted values
Residuals
FIG. 8.3
Residual plot for the caution data.
(1999b). The right-opening megaphone is clear in this graph, suggesting noncon-
stant variance. But these data were actually generated using a mean function
E(Y|X = x) =
|x1|
2 + (1.5 + x2)2
(8.14)
with constant variance, with scatterplot matrix given in Figure 8.4. The real problem
is that the mean function is wrong, even though from the residual plot, nonconstant
variance appears to be the problem. A nonnull residual plot in multiple regres-
sion be indicates that something is wrong but does not necessarily tell what is
wrong.
Residual plots in multiple regression can be interpreted just as residual plots
in simple regression if two conditions are satisﬁed. First, the predictors should be
approximately linearly related, Section 7.2.1. The second condition is on the mean
function: we must be able to write the mean function in the form E(Y|X = x) =
g(β′x) for some unspeciﬁed function g. If either of these conditions fails, then
residual plots cannot be interpreted as in simple regression (Cook and Weisberg,
1999b). In the caution data, the ﬁrst condition is satisﬁed, as can be veriﬁed by
looking at the plot of X1 versus X2 in Figure 8.4, but the second condition fails
because (8.14) cannot be written as a function of a single linear combination of
the terms.
8.1.6
Fuel Consumption Data
According to theory, if the mean function and other assumptions are correct, then
all possible residual plots of residuals versus any function of the terms should
resemble a null plot, so many plots of residuals should be examined. Usual choices

174
REGRESSION DIAGNOSTICS: RESIDUALS
X1
−1.0
0.0
0.5
1.0
−0.5
0.0
0.5
1.0
−1.0
0.0
0.5
1.0
X2
−0.5
0.0
0.5
1.0
0.0
0.2
0.4
0.6
0.8
0.0
0.2
0.4
0.6
0.8
Y
FIG. 8.4
Scatterplot matrix for the caution data.
include plots versus each of the terms and versus ﬁtted values, as shown in
Figure 8.5 for the fuel consumption data. None of the plots versus individual terms
in Figure 8.5a–d suggest any particular problems, apart from the relatively large
positive residual for Wyoming and large negative residual for Alaska. In some
of the graphs, the point for the District of Columbia is separated from the oth-
ers. Wyoming is large but sparsely populated with a well-developed road system.
Driving long distances for the necessities of life like going to see a doctor will
be common in this state. While Alaska is also very large and sparsely populated,
most people live in relatively small areas around cities. Much of Alaska is not
accessible by road. These conditions should result in lower use of motor fuel than
might otherwise be expected. The District of Columbia is a very compact urban
area with good rapid transit, so use of cars will generally be less. It has a small
residual but unusual values for the terms in the mean function, so it is separated
horizontally from most of the rest of the data. The District of Columbia has high
leverage (h9,9 = 0.415), while the other two are candidates for outliers. We will
return to these issues in the next chapter.

THE RESIDUALS
175
10
15
20
25
−150
0
100
Tax
Residuals
AK
WY
DC
700
800
900
1000
−150
0
100
Dlic
Residuals
AK
WY
DC
25
30
35
40
−150
0
100
Income
Residuals
AK
WY
DC
12
14
16
18
−150
0
100
log(Miles)
Residuals
AK
WY
DC
400
500
600
700
−150
0
100
Fitted values
Residuals
AK
WY
DC
400
500
600
700
300
500
700
Fitted values
(a)
(b)
(c)
(d)
(e)
(f)
Fuel
AK
WY
DC
FIG. 8.5
Residual plots for the fuel consumption data.
Figure 8.5e is a plot of residuals versus the ﬁtted values, which are just a linear
combination of the terms. Some computer packages will produce this graph as the
only plot of residuals, and if only one plot were possible, this would be the plot
to draw, as it contains some information from all the terms in the mean function.
There is a hint of curvature in this plot, possibly suggesting that the mean function
is not adequate for the data. We will look at this more carefully in the next section.
Figure 8.5f is different from the others because it is not a plot of residuals but
rather a plot of the response versus the ﬁtted values. This is really just a rescaling
of Figure 8.5e. If the mean function and the assumptions appear to be sensible,
then this ﬁgure is a summary graph for the regression problem. The mean function
for the graph should be the straight line shown in the ﬁgure, which is just the ﬁtted
ols simple regression ﬁt of the response on the ﬁtted values.

176
REGRESSION DIAGNOSTICS: RESIDUALS
TABLE 8.1
Signiﬁcance Levels for the Lack-of-Fit
Tests for the Residual Plots in Figure 8.5
Term
Test Stat.
Pr(> |t|)
Tax
−1.08
0.29
Dlic
−1.92
0.06
Income
−0.09
0.93
log(Miles)
−1.35
0.18
Fitted values
−1.45
0.15
8.2
TESTING FOR CURVATURE
Tests can be computed to help decide if residual plots such as those in Figure 8.5
are null plots or not. One helpful test looks for curvature in this plot. Suppose
we have a plot of residuals ˆe versus a quantity U on the horizontal axis, where
U could be a term in the mean function or a combination of terms2. A simple
test for curvature is to reﬁt the original mean function with an additional term for
U2 added. The test for curvature is then based on the t-statistic for testing the
coefﬁcient for U2 to be 0. If U does not depend on estimated coefﬁcients, then
a usual t-test of this hypothesis can be used. If U is equal to the ﬁtted values so
that it depends the estimated coefﬁcients, then the test statistic should be compared
with the standard normal distribution to get signiﬁcance levels. This latter case is
called Tukey’s test for nonadditivity (Tukey, 1949).
Table 8.1 gives the lack-of-ﬁt tests for the residual plots in Figure 8.5. None of
the tests have small signiﬁcance levels, providing no evidence against the mean
function.
As a second example, consider again the United Nations data from Section 3.1
with response log(Fertility) and two predictors log(PPgdp) and Purban. The appar-
ent linearity in all the frames of the scatterplot matrix in Figure 8.6 suggests that
the mean function
E(log(Fertility)|log(PPgdp), Purban) = β0 + β1log(PPgdp) + β2Purban (8.15)
should be appropriate for these data. Plots of residuals versus the two terms and
versus ﬁtted values are shown in Figure 8.7. Without reference to the curved
lines shown on the plot, the visual appearance of these plots is satisfactory, with
no obvious curvature or nonconstant variance. However, the curvature tests tell
a different story. In each of the graphs, the value of the test statistics shown
in Table 8.2 has a p-value of 0 to 2 decimal places, suggesting that the mean
function (8.15) is not adequate for these data. We will return to this example in
Section 8.4.
2If U is a polynomial term, for example, U = X2
1, where X1 is another term in the mean function, this
procedure is not recommended.

NONCONSTANT VARIANCE
177
log(PPgdp)
20
40
60
80
100
8
10
12
14
20
40
60
80
100
Purban
8
10
12
14
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
log(Fertility)
FIG. 8.6
Scatterplot matrix for three variables in the UN data.
TABLE 8.2
Lack-of-Fit Tests for the UN Data
Test Stat.
Pr(>|t|)
log(PPgdp)
3.22
0.00
Purban
3.37
0.00
Tukey test
3.65
0.00
8.3
NONCONSTANT VARIANCE
A nonconstant variance function in a residual plot may indicate that a constant
variance assumption is false. There are at least four basic remedies for nonconstant
variance. The ﬁrst is to use a variance stabilizing transformation since replacing
Y by YT may induce constant variance in the transformed scale. A second option
is to ﬁnd empirical weights that could be used in weighted least squares. Weights

178
REGRESSION DIAGNOSTICS: RESIDUALS
8
10
12
14
−1.0
−0.5
0.0
0.5
log(PPgdp)
 Residuals
20
40
60
80
100
−1.0
−0.5
0.0
0.5
Purban
Residuals
0.4
0.8
1.2
1.6
−1.0
−0.5
0.0
0.5
Fitted values
Residuals
FIG. 8.7
Residual plots for the UN data. The dotted curved lines are quadratic polynomials ﬁt to
the residual plot and do not correspond exactly to the lack-of-ﬁt tests that add a quadratic term to the
original mean function.
that are simple functions of single predictors, such as Var(Y|X) = σ 2X1, with
X1 > 0, can sometimes be justiﬁed theoretically. If replication is available, then
within group variances may be used to provide approximate weights. Although
beyond the scope of this book, it is also possible to use generalized least squares
and estimate weights and coefﬁcients simultaneously (see, for example, Pinheiro
and Bates, 2000, Section 5.1.2).
The third option is to do nothing. Estimates of parameters, given a misspeciﬁed
variance function, remain unbiased, if somewhat inefﬁcient. Tests and conﬁdence
intervals computed with the wrong variance function will be inaccurate, but the
bootstrap may be used to get more accurate results.
The ﬁnal option is to use regression models that account for the nonconstant
variance that is a function of the mean. These are called generalized linear models
and are discussed in the context of logistic regression in Chapter 12. In this section,
we consider primarily the ﬁrst two options.

NONCONSTANT VARIANCE
179
8.3.1
Variance Stabilizing Transformations
Suppose that the response is strictly positive, and the variance function before
transformation is
Var(Y|X = x) = σ 2g(E(Y|X = x))
(8.16)
where g(E(Y|X = x)) is a function that is increasing with the value of its argument.
For example, if the distribution of Y|X has a Poisson distribution, then g(E(Y|X =
x)) = E(Y|X = x), since for Poisson variables, the mean and variance are equal.
For distributions in which the mean and variance are functionally related as
in (8.16), Scheff´e (1959, Section 10.7) provides a general theory for determining
transformations that can stabilize variance, so that Var(YT |X = x) will be approx-
imately constant. Table 8.3 lists the common variance stabilizing transformations.
Of course, transforming away nonconstant variance can introduce nonlinearity into
the mean function, so this option may not always be reasonable.
The square root, log(Y), and 1/Y are appropriate when variance increases or
decreases with the response, but each is more severe than the one before it. The
square-root transformation is relatively mild and is most appropriate when the
response follows a Poisson distribution, usually the ﬁrst model considered for errors
in counts. The logarithm is the most commonly used transformation; the base of
the logarithms is irrelevant. It is appropriate when the error standard deviation
is a percent of the response, such as ±10% of the response, not ±10 units, so
Var(Y|X) ∝σ 2[E(Y|X)]2.
The reciprocal or inverse transformation is often applied when the response is a
time until an event, such as time to complete a task, or until healing. This converts
times per event to a rate per unit time; often the transformed measurements may be
multiplied by a constant to avoid very small numbers. Rates can provide a natural
measurement scale.
TABLE 8.3
Common Variance Stabilizing Transformations
YT
Comments
√
Y
Used when Var(Y|X) ∝E(Y|X), as for Poisson distributed data.
YT =
√
Y +
√
Y + 1 can be used if all the counts are small (Freeman
and Tukey, 1950).
log(Y)
Use if Var(Y|X) ∝[E(Y|X)]2. In this case, the errors behave like a
percentage of the response, ±10%, rather than an absolute deviation,
±10 units.
1/Y
The inverse transformation stabilizes variance when
Var(Y|X) ∝[E(Y|X)]4. It can be appropriate when responses are
mostly close to 0, but occasional large values occur.
sin−1(
√
Y)
The arcsine square-root transformation is used if Y is a proportion
between 0 and 1, but it can be used more generally if y has a limited
range by ﬁrst transforming Y to the range (0, 1), and then applying
the transformation.

180
REGRESSION DIAGNOSTICS: RESIDUALS
8.3.2
A Diagnostic for Nonconstant Variance
Cook and Weisberg (l983) provided a diagnostic test for nonconstant variance.
Suppose now that Var(Y|X) depends on an unknown vector parameter λ and a
known set of terms Z with observed values for the ith case zi. For example, if
Z = Y, then variance depends on the response. Similarly, Z may be the same as X,
a subset of X, or indeed it could be completely different from X, perhaps indicating
spatial location or time of observation. We assume that
Var(Y|X, Z = z) = σ 2 exp(λ′z)
(8.17)
This complicated form says that (1) Var(Y|Z = z) > 0 for all z because the expo-
nential function is never negative; (2) variance depends on z and λ but only through
the linear combination λ′z; (3) Var(Y|Z = z) is monotonic, either increasing or
decreasing, in each component of Z; and (4) if λ = 0, then Var(Y|Z = z) = σ 2.
The results of Chen (1983) suggest that the tests described here are not very sensi-
tive to the exact functional form used in (8.17), and so the use of the exponential
function is relatively benign, and any form that depends on the linear combination
λ′z would lead to very similar inference.
Assuming that errors are normally distributed, a score test of λ = 0 is particularly
simple to compute using standard regression software. The test is carried out using
the following steps:
1. Compute the ols ﬁt with the mean function
E(Y|X = x) = β′x
as if
Var(Y|X, Z = z) = σ 2
or equivalently, λ = 0. Save the residuals ˆei.
2. Compute scaled squared residuals ui = ˆe2
i /˜σ 2 = nˆe2
i /[(n −p′)ˆσ 2], where
˜σ 2 =  ˆe2
j/n is the maximum likelihood estimate of σ 2, and differs from
ˆσ 2 only by the divisor of n rather than n −p′. We combine the ui into a
variable U.
3. Compute the regression with the mean function E(U|Z = z) = λ0 + λ′z.
Obtain SSreg for this regression with df = q, the number of components
in Z. If variance is thought to be a function of the responses, then in this
regression, replace Z by the ﬁtted values from the regression in step 1. The
SSreg then will have 1 df.
4. Compute the score test, S = SSreg/2. The signiﬁcance level for the test can
be obtained by comparing S with its asymptotic distribution, which, under
the hypothesis λ = 0, is χ2(q). If λ ̸= 0, then S will be too large, so large
values of S provide evidence against the hypothesis of constant variance.

NONCONSTANT VARIANCE
181
If we had started with a set of known weights, then the score test could be based
on ﬁtting the variance function
Var(Y|X, Z = z) = σ 2
w exp(λ′z)
(8.18)
The null hypothesis for the score test is then Var(Y|X, Z = x) = σ 2/w versus the
alternative given by (8.18). The test is exactly the same as outlined above except
that in step one, the wls ﬁt with weights w is used in place of the ols ﬁt, and in
the remaining steps, the weighted or Pearson residuals given by (8.13) are used,
not unweighted residuals.
Snow Geese
The relationship between photo = photo count, obs1 = count by observer 1, and
obs2 = count by observer 2 of ﬂocks of snow geese in the Hudson Bay area
of Canada is discussed in Problem 5.5 of Chapter 5. The data are displayed in
Figure 8.8. We see in the graph that (1) there is substantial disagreement between
the observers; (2) the observers cannot predict the photo count very well, and (3)
the variability appears to be larger for larger ﬂocks.
photo
0
200
400
0
200
400
0
200
400
obs1
0
200
400
0
200
400
0
200
400
obs2
FIG. 8.8
The snow geese data. The line on each plot is a loess smooth with smoothing parameter 2/3.

182
REGRESSION DIAGNOSTICS: RESIDUALS
Using the ﬁrst observer only, we illustrate computation of the score test for
constant variance. The ﬁrst step is to ﬁt the ols regression of photo on obs1. The
ﬁtted ols mean function is ˆE(photo|obs1) = 26.55 + 0.88obs1. From this, we can
compute the residuals ˆei, ˜σ =  ˆe2
i /n, and then compute the ui = ˆe2
i /˜σ. We then
compute the regression of U on obs1, under the hypothesis suggested by Figure 8.8
that variance increases with obs1. The analysis of variance table for this second
regression is:
Response: U
Df
Sum Sq Mean Sq F value
Pr(>F)
obs1
1 162.826 162.826
50.779 8.459e-09
Residuals
43 137.881
3.207
The score test for nonconstant variance is S = (1/2)SSreg = (1/2)162.83 = 81.41,
which, when C compared with the chi-squared distribution with one df, gives
an extremely small p-value. The hypothesis of constant residual variance is not
tenable. The analyst must now cope with the almost certain nonconstant variance
evident in the data. Two courses of action are outlined in Problems 5.5.3 and 5.5.4.
Sniffer Data
When gasoline is pumped into a tank, hydrocarbon vapors are forced out of the
tank and into the atmosphere. To reduce this signiﬁcant source of air pollution,
devices are installed to capture the vapor. In testing these vapor recovery systems,
a “sniffer” measures the amount recovered. To estimate the efﬁciency of the system,
some method of estimating the total amount given off must be used. To this end, a
laboratory experiment was conducted in which the amount of vapor given off was
measured under controlled conditions. Four predictors are relevant for modeling:
TankTemp = initial tank temperature (degrees F)
GasTemp = temperature of the dispensed gasoline (degrees F)
TankPres = initial vapor pressure in the tank (psi)
GasPres = vapor pressure of the dispensed gasoline (psi)
The response is the hydrocarbons Y emitted in grams. The data, kindly provided by
John Rice, are given in the data ﬁle sniffer.txt, and are shown in Figure 8.9.
The clustering of points in many of the frames of this scatterplot is indicative of
the attempt of the experimenters to set the predictors at a few nominal values, but
the actual values of the predictors measured during the experiment was somewhat
different from the nominal. We also see that (1) the predictors are generally lin-
early related, so transformations are unlikely to be desirable here, and (2) some
of the predictors, notably the two pressure variables, are closely linearly related,
suggesting, as we will see in Chapter 10, that using both in the mean function may
not be desirable. For now, however, we will use all four terms and begin with
the mean function including all four predictors as terms and ﬁt via ols as if the

NONCONSTANT VARIANCE
183
TankTemp
40
70
3
5
7
30
70
40
70
GasTemp
TankPres
3
5
7
3
5
7
GasPres
30
70
3
5
7
20
40
20
40
Y
FIG. 8.9
Scatterplot matrix for the sniffer data.
variance function were constant. Several plots of the residuals for this regression
are shown in Figure 8.10.
Figure 8.10a is the plot of residuals versus ﬁtted values. While this plot is far
from perfect, it does not suggest the need to worry much about the assumption
of nonconstant variance. Figures 8.10b and c, which are plots of residuals against
TankTemp and GasPres, respectively, give a somewhat different picture, as partic-
ularly in Figure 8.10c variance does appear to increase from left to right. Because
none of the graphs in Figure 8.9 have clearly nonlinear mean functions, the infer-
ence that variance may not be constant can be tentatively adopted from the residual
plots.
Table 8.4 gives the results of several nonconstant variance score tests, each
computed using a different choice for Z. Each of these tests is just half the sum
of squares for regression for U on the choice of Z shown. The plot shown in
Figure 8.10d has the ﬁtted values from the regression of U on all four predictors,
and corresponds to the last line of Table 8.4.
From Table 8.4, we would diagnose nonconstant variance as a function of var-
ious choices of Z. We can compare nested choices for Z by taking the difference
between the score tests and comparing the result with the χ2 distribution with df

184
REGRESSION DIAGNOSTICS: RESIDUALS
20
30
40
50
−6
−2
0
2
4
6
Residuals
30
50
70
90
−6
−2
0
2
4
6
Tank temperature
Residuals
3
4
5
6
7
−6
−2
2
0
4
6
Gas pressure
Residuals
−0.5
0.5
1.0
1.5
2.0
−6
−2
0
2
4
6
Linear combination
Residuals
Y
(b)
(c)
(d)
(a)
∧
FIG. 8.10
Residuals plots for the sniffer data with variance assumed to be constant.
TABLE 8.4
Score Tests for Sniffer Data
Choice for Z
df
S
p-value
TankTemp
1
5.50
.019
GasPres
1
9.71
.002
TankTemp, GasPres
2
11.78
.003
TankTemp, GasTemp TankPres, GasPres
4
13.76
.008
Fitted values
1
4.80
.028
equal to the difference in their df (Hinkley, 1985). For example, to compare the 4 df
choice of Z to Z = (TankTemp, GasPres), we can compute 13.76 −11.78 = 1.98
with 4 −2 = 2 df, to get a p-value of about 0.37, and so the simpler Z with
two terms is adequate. Comparing Z = (TankTemp, GasPres) with Z = GasPres,

GRAPHS FOR MODEL ASSESSMENT
185
the test statistic is 11.78 −9.71 = 2.07 with 2 −1 = 1 df, giving a p-value of
about .15, so once again the simpler choice of Z seems adequate. Combining these
tests, we would be led to assessing that the variance is primarily a function of
GasPres.
A reasonable approach to working with these data is to assume that
Var(Y|X, Z) = σ 2 × GasPres
and use 1/GasPres as weights in weighted least squares.
8.3.3
Additional Comments
Some computer packages will include functions for the score test for nonconstant
variance. With other computer programs, it may be more convenient to compute the
score test as follows: (1) Compute the residuals ˆei from the regression of Y on X; let
ˆσ 2 be the usual residual mean square from this regression. (2) Compute the regres-
sion of ˆe2
i on Z for the choice of Z of interest, and let SSreg(Z) be the resulting
sum of squares for regression. (3) Compute S = (1/2)SSreg(Z)/[(n −p′)ˆσ 2/n]2.
Pinheiro and Bates (2000, Section 5.2.1) present methodology and software for
estimating weights using models similar to those discussed here.
8.4
GRAPHS FOR MODEL ASSESSMENT
Residual plots are used to examine regression models to see if they fail to match
observed data. If systematic failures are found, then models may need to reformu-
lated to ﬁnd a better ﬁtting model.
A closely related problem is assessing how well a model matches the data.
Let us ﬁrst think about a regression with one predictor in which we have ﬁtted
a simple linear regression model, and the goal is to summarize how well a ﬁtted
model matches the observed data. The lack-of-ﬁt tests developed in Section 5.3
and 6.1 approach the question of goodness of ﬁt from a testing point of view. We
now look at this issue from a graphical point of view using marginal model plots.
We illustrate the idea ﬁrst with a problem with just one predictor. In
Section 7.1.2, we discussed the regression of Height on Dbh for a sample of western
red cedar trees from Upper Flat Creek, Idaho. The mean function
E(Height|Dbh) = β0 + β1Dbh
(8.19)
was shown to be a poor summary of these data, as can be seen in Figure 8.11.
Two smooths are given on the plot. The ols ﬁt of (8.19), shown as a dashed line,
estimates the mean function only if the simple linear regression model is correct.
The loess ﬁt, shown as a solid line, estimates the mean function regardless of the
ﬁt of the simple linear regression model. If we judge these two ﬁts to be different,
then we have visual evidence against the simple linear regression mean function.

186
REGRESSION DIAGNOSTICS: RESIDUALS
200
400
600
800
1000
100
200
300
400
Diameter, Dbh
Height
FIG. 8.11
Model checking plot for the simple linear regression for western red cedar trees at Upper
Flat Creek. The dashed line is the ols simple linear regression ﬁt, and the solid line is a loess smooth.
The loess ﬁt is clearly curved, so the mean function (8.19) is not a very good
summary of this regression problem.
Although Figure 8.11 includes the data, the primary focus in this plot is com-
paring the two curves, using the data as background mostly to help choose the
smoothing parameter for the loess smooth, to help visualize variation, and to locate
any extreme or unusual points.
8.4.1
Checking Mean Functions
With more than one predictor, we will look at marginal models to get a sequence
of two-dimensional plots to examine. Suppose that the model we have ﬁtted has
mean function E(Y|X = x) = β′x, although in what follows the exact form of the
mean function is not important. We will draw a plot with the response Y on the
vertical axis. On the horizontal axis, we will plot a quantity U that will consist of
any function of X we think is relevant, such as ﬁtted values, any of the individual
terms in X, or even transformations of them. Fitting a smoother to the plot of Y
versus U estimates E(Y|U) without any assumptions. We want to compare this
smooth to an estimate of E(Y|U) based on the model.
Following Cook and Weisberg (1997), an estimate of E(Y|U), given the model,
can be based on application of equation (A.4) in Appendix A.2.4. Under the model,
we have
E(Y|U = u) = E[E(Y|X = x)|U = u]
We need to substitute an estimate for E(Y|X = x). On the basis of the model, this
expectation is estimated at the observed values of the terms by the ﬁtted values ˆY.

GRAPHS FOR MODEL ASSESSMENT
187
We get
E(Y 
|U = u) = E( ˆY|U = u)
(8.20)
The implication of this result is that we can estimate E(Y|U = u) by smoothing
the scatterplot with U on the horizontal axis, and the ﬁtted values ˆY on the vertical
axis. If the model is correct, then the smooth of Y versus U and the smooth of ˆY
versus U should agree; if the model is not correct, these smooths may not agree.
As an example, we return to the United Nations data discussed in Section 8.2,
starting with the mean function given by (8.15),
E(log(Fertility)|log(PPgdp), Purban) = β0 + β1log(PPgdp) + β2Purban (8.21)
and suppose that U = log(PPgdp), one of the terms in the mean function.
Figure 8.12 shows plots of log(Fertility) versus U and of ˆY versus U. The
smooth in Figure 8.12a estimates E(log(Fertility)|log(PPgdp)) whether the model
is right or not, but the smooth in Figure 8.12b may not give a useful estimate of
E(log(Fertility)|log(PPgdp)) if the linear regression model is wrong. Comparison
of these two estimated mean functions provides a visual assessment of the adequacy
of the mean function for the model. Superimposing the smooth in Figure 8.12b on
Figure 8.12a gives the marginal model plot shown in Figure 8.13a. The two ﬁtted
curves do not match well, suggesting that the mean function (8.21) is inadequate.
Three additional marginal model plots are shown in Figure 8.13. The plots versus
Purban and versus ﬁtted values also exhibit a curved mean smooth based on the data
compared to a straight smooth based on the ﬁtted model, conﬁrming the inadequacy
of the mean function. The ﬁnal plot in Figure 8.13d is a little different. The quantity
on the horizontal axis is a randomly selected linear combination of Purban and
8
10
12
14
0.0
0.5
1.0
1.5
2.0
log(PPgdp)
log(Fertility)
8
10
12
14
0.4 0.6 0.8 1.0 1.2 1.4 1.6
log(PPgdp)
(a)
(b)
Fitted values
FIG. 8.12
Plots for log(Fertility) versus log(PPgdp) and ˆy versus log(PPgdp). In both plots, the
curves are loess smooths with smoothing parameters equal to 2/3. If the model has the correct mean
function, then these two smooths estimate the same quantity.

188
REGRESSION DIAGNOSTICS: RESIDUALS
8
10
12
14
0.0
0.5
1.0
1.5
2.0
log(PPgdp)
log(Fertility)
20
40
60
80
100
0.0
0.5
1.0
1.5
2.0
Purban
log(Fertility)
0.4 0.6 0.8 1.0 1.2 1.4 1.6
0.0
0.5
1.0
1.5
2.0
Fitted values
log(Fertility)
−1.0
−0.5
0.0
0.5
1.0
0.0
0.5
1.0
1.5
2.0
Random direction
(a)
(b)
(c)
(d)
log(Fertility)
FIG. 8.13
Four marginal model plots, versus the two terms in the mean function, ﬁtted values, and a
random linear combination of the terms in the mean function.
log(PPgdp). In this direction, both smooths are curved, and they agree fairly well,
except possibly at the left edge of the graph. If a ﬁtted model is appropriate for
data, then the two smooths in the marginal model plots will agree for any choice of
U, including randomly selected ones. If the model is wrong, then for some choices
of U, the two smooths will disagree.
Since the mean function (8.21) is inadequate, we need to consider further modiﬁ-
cation to get a mean function that matches the data well. One approach is to expand
(8.15) by including both quadratic terms and an interaction between log(PPgdp)
and Purban. Using the methods described elsewhere in this book, we conclude that
the mean function
E(log(Fertility)|log(PPgdp), Purban) = β0 + β1log(PPgdp) + β2Purban
+ β22Purban2
(8.22)

GRAPHS FOR MODEL ASSESSMENT
189
8
10
12
14
0.0
0.5
1.0
1.5
2.0
log(PPgdp)
log(Fertility)
20
40
60
80
100
0.0
0.5
1.0
1.5
2.0
Purban
log(Fertility)
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
Fitted values
log(Fertility)
−1.0
−0.5
0.0
0.5
1.0
0.0
0.5
1.0
1.5
2.0
Random direction
(a)
(b)
(c)
(d)
log(Fertility)
FIG. 8.14
Marginal model plots for the United Nations data, including a quadratic term in Purban.
matches the data well, as conﬁrmed by the marginal model plots in Figure 8.14.
Evidently, adding the quadratic in Purban allows the effect of increasing Purban
on log(Fertility) to be smaller when Purban is large than when Purban is small.
8.4.2
Checking Variance Functions
Model checking plots can also be used to check for model inadequacy in the vari-
ance function, which for the multiple linear regression problem means checking the
constant variance assumption. We call the square root of the variance function the
standard deviation function. The plot of Y versus U can be used to get the estimate
SDdata(Y|U) of the standard deviation function, as discussed in Appendix A.5.
This estimate of the square root of the variance function does not depend on a
model.

190
REGRESSION DIAGNOSTICS: RESIDUALS
We need a model-based estimate of the standard deviation function. Applying
(A.5) and again substituting ˆY ≈E(Y|U = u),
Var(Y|U) = E[Var(Y|X)|U] + Var[E(Y|X)|U]
(8.23)
≈E[σ 2|U] + Var[ ˆY|U]
= σ 2 + Var[ ˆY|U]
(8.24)
Equation (8.23) is the general result that holds for any model. Equation (8.24) holds
for the linear regression model in which the variance function Var(Y|X) = σ 2 is
constant. According to this result, we can estimate Var(Y|U) under the model by
getting a variance smooth of ˆY versus U, and then adding to this an estimate of
σ 2, for which we use ˆσ 2 from the ols ﬁt of the model. We will call the square
root of this estimated variance function SDmodel(Y|U). If the model is appropriate
for the data, then apart from sampling error, SDdata(Y|U) = SDmodel(Y|U), but if
the model is wrong, these two functions need not be equal.
For visual display, we show the mean function estimated from the plot ±SDdata
(Y|U) using solid lines and the mean function estimated from the model ±SDmodel
(Y|U) using dashed lines; colored lines would be helpful here. The same smooth-
ing parameter should be used for all the smooths, so any bias due to smoothing
will tend to cancel. These smooths for the United Nations example are shown in
Figure 8.15, ﬁrst for the ﬁt of (8.21) and then for the ﬁt of (8.22). For both, the
horizontal axis is ﬁtted values, but almost anything could be put on this axis. Apart
from the edges of the plot where the smooths are less accurate, these plots do not
suggest any problem with nonconstant variance, as the estimated variances func-
tions using the two methods are similar, particularly for the mean function (8.22)
that matches the data.
0.4 0.6 0.8 1.0 1.2 1.4 1.6
0.0
0.5
1.0
1.5
2.0
Fitted values, original model
log(Fertility)
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
Fitted values, quadratic term added
(a)
(b)
log(Fertility)
FIG. 8.15
Marginal model plots with standard deviation smooths added. (a) The ﬁt of (8.21). (b) The
ﬁt of (8.22).

PROBLEMS
191
The marginal model plots described here can be applied in virtually any regres-
sion problem, not just for the linear regression. For example, Pan, Connett, Porzio,
and Weisberg (2001) discuss application to longitudinal data, and Porzio (2002)
discusses calibrating marginal model plots for binary regression.
PROBLEMS
8.1. Working with the hat matrix
8.1.1. Prove the results given by (8.7).
8.1.2. Prove that 1/n ≤hii ≤1/r, where hii is a diagonal entry in H, and
r is the number of rows in X that are exactly the same as xi.
8.2. If the linear trend were removed from Figure 8.5f, what would the resulting
graph look like?
8.3. This example compares in-ﬁeld ultrasonic measurements of the depths of
defects in the Alaska oil pipeline to measurements of the same defects in a
laboratory. The lab measurements were done in six different batches. The goal
is to decide if the ﬁeld measurement can be used to predict the more accurate
lab measurement. In this analysis, the ﬁeld measurement is the response vari-
able and the laboratory measurement is the predictor variable. The data, in the
ﬁle pipeline.txt, were given at www.itl.nist.gov/div898/handbook/pmd/
section6/pmd621.htm. The three variables are called Field, the in-ﬁeld mea-
surement, Lab, the more accurate in-lab measurement, and Batch, the batch
number.
8.3.1. Draw the scatterplot of Lab versus Field, and comment on the appli-
cability of the simple linear regression model.
8.3.2. Fit the simple regression model, and get the residual plot. Compute
the score test for nonconstant variance, and summarize your results.
8.3.3. Fit the simple regression mean function again, but this time assume
that Var(Lab|Field) = σ 2/Field. Get the score test for the ﬁt of this
variance function. Also test for nonconstant variance as a function of
batch; since the batches are arbitrarily numbered, be sure to treat Batch
as a factor. (Hint: Both these tests are extensions of the methodology
outlined in the text. The only change required is to be sure that the
residuals deﬁned by (8.13) are used when computing the statistic.)
8.3.4. Repeat Problem 8.3.3, but with Var(Lab|Field) = σ 2/Field2.
8.4. Refer to Problem 7.2. Fit Hald’s model, given in Problem 7.2.3, but with
constant variance, Var(Distance|Speed) = σ 2. Compute the score test for
nonconstant variance for the alternatives that (a) variance depends on the
mean; (b) variance depends on Speed; and (c) variance depends on Speed
and Speed2. Is adding Speed2 helpful?

192
REGRESSION DIAGNOSTICS: RESIDUALS
8.5. Consider the simple regression model, E(Y|X = x) = β0 + β1x with Var
(Y|X = x) = σ 2.
8.5.1. Find a formula for the hij and for the leverages hii.
8.5.2. In a 2D plot of the response versus the predictor in a simple regression
problem, explain how high-leverage points can be identiﬁed.
8.5.3. Make up a predictor X so that the value of the leverage in simple
regression for one of the cases is equal to 1.
8.6. Using the QR factorization deﬁned in Appendix A.12, show that H = QQ′.
Hence, if qi is the ith row of Q,
hii = q′
iqi
hij = q′
iqj
Thus, if the QR factorization of X is computed, the hii and the hij are easily
obtained.
8.7. Let U be an n × 1 vector with 1 as its ﬁrst element and 0s elsewhere. Consider
computing the regression of U on an n × p′ full rank matrix X. As usual,
let H = X(X′X)−1X′ be the Hat matrix with elements hij .
8.7.1. Show that the elements of the vector of ﬁtted values from the regres-
sion of U on X are the h1j, j = 1, 2, . . . , n.
8.7.2. Show that the vector of residuals have 1 −h11 as the ﬁrst element,
and the other elements are −h1j, j > 1.
8.8. Two n × n matrices A and B are orthogonal if AB = BA = 0. Show that
I −H and H are orthogonal. Use this result to show that as long as the
intercept is in the mean function, the slope of the regression of ˆe on ˆY is 0.
What is the slope of the regression of ˆe on Y?
8.9. Suppose that W is a known positive diagonal matrix of positive weights, and
we have a weighted least squares problem,
Y = Xβ + e
Var(e) = ˆσ 2W−1
(8.25)
Using the transformations as in Section 5.1, show that the hat matrix is given
by (8.12).
8.10. Draw residuals plots for the mean function described in Problem 7.3.3 for
the California water data, and comment on your results. Test for curvature
as a function of ﬁtted values. Also, get marginal model plots for this model.
8.11. Refer to the transactions data discussed in Section 4.6.1. Fit the mean func-
tion (4.16) with constant variance, and use marginal model plots to examine
the ﬁt. Be sure to consider both the mean function and the variance function.
Comment on the results.

PROBLEMS
193
TABLE 8.5
Crustacean Zooplankton Species Data
Variable
Description
Species
Number of zooplankton species
MaxDepth
Maximum lake depth, m
MeanDepth
Mean lake depth, m
Cond
Speciﬁc conductance, micro Siemans
Elev
Elevation, m
Lat
N latitude, degrees
Long
W longitude, degrees
Dist
distance to nearest lake, km
NLakes
number of lakes within 20 km
Photo
Rate of photosynthesis, mostly by the 14C method
Area
Surface area of the lake, in hectares
Lake
Name of lake
Source: From Dodson (1992).
8.12. The number of crustacean zooplankton species present in a lake can be dif-
ferent, even for two nearby lakes. The data in the ﬁle lakes.txt, provided
by S. Dodson and discussed in part in Dodson (1992), give the number of
known crustacean zooplankton species for 69 world lakes. Also included are
a number of characteristics of each lake. There are some missing values,
indicated with a “?” in the data ﬁle. The goal of the analysis is to understand
how the number of species present depends on the other measured variables
that are characteristics of the lake. The variables are described in Table 8.5.
Decide on appropriate transformations of the data to be used in this prob-
lem. Then, ﬁt appropriate linear regression models, and summarize your
results.

C H A P T E R 9
Outliers and Inﬂuence
9.1
OUTLIERS
In some problems, the observed response for a few of the cases may not seem
to correspond to the model ﬁtted to the bulk of the data. In a simple regression
problem such as displayed in Figure 1.9c, page 13, this may be obvious from a
plot of the response versus the predictor, where most of the cases lie near a ﬁtted
line but a few do not. Cases that do not follow the same model as the rest of the
data are called outliers, and identifying these cases can be useful.
We use the mean shift outlier model to deﬁne outliers. Suppose that the ith case
is a candidate for an outlier. We assume that the mean function for all other cases is
E(Y|X = xj) = x′
jβ, but for case i the mean function is E(Y|X = xi) = x′
iβ + δ.
The expected response for the ith case is shifted by an amount δ, and a test of
δ = 0 is a test for a single outlier in the ith case. In this development, we assume
Var(Y|X) = σ 2.
Cases with large residuals are candidates for outliers. Not all large residual cases
are outliers, since large errors ei will occur with the frequency prescribed by the
generating probability distribution. Whatever testing procedure we develop must
offer protection against declaring too many cases to be outliers. This leads to the
use of simultaneous testing procedures. Also, not all outliers are bad. For example,
a geologist searching for oil deposits may be looking for outliers, if the oil is in
the places where a ﬁtted model does not match the data. Outlier identiﬁcation is
done relative to a speciﬁed model. If the form of the model is modiﬁed, the status
of individual cases as outliers may change. Finally, some outliers will have greater
effect on the regression estimates than will others, a point that is pursued shortly.
9.1.1
An Outlier Test
Suppose that the ith case is suspected to be an outlier. First, deﬁne a new term, say
U, with the jth element uj = 0 for j ̸= i, and the ith element ui = 1. Thus, U is
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
194

OUTLIERS
195
a dummy variable that is zero for all cases but the ith. Then, simply compute the
regression of the response on both the terms in X and U. The estimated coefﬁcient
for U is the estimate of the mean shift δ. The t-statistic for testing δ = 0 against
a two-sided alternative is the appropriate test statistic. Normally distributed errors
are required for this test, and then the test will be distributed as Student’s t with
n −p′ −1 df.
We will now consider an alternative approach that will lead to the same test,
but from a different point of view. The equivalence of the two approaches is left
as an exercise.
Again suppose that the ith case is suspected to be an outlier. We can proceed
as follows:
1. Delete the ith case from the data, so n −1 cases remain in the reduced data
set.
2. Using the reduced data set, estimate β and σ 2. Call these estimates ˆβ(i) and
ˆσ 2
(i) to remind us that case i was not used in estimation. The estimator ˆσ 2
(i)
has n −p′ −1 df.
3. For the deleted case, compute the ﬁtted value ˆyi(i) = x′
i ˆβ(i). Since the ith
case was not used in estimation, yi and ˆyi(i) are independent. The variance
of yi −ˆyi(i) is given by
Var(yi −ˆyi(i)) = σ 2 + σ 2x′
i(X′
(i)X(i))−1xi
(9.1)
where X(i) is the matrix X with the ith row deleted. This variance is estimated
by replacing σ 2 with ˆσ 2
(i) in (9.1).
4. Now E(yi −ˆyi(i)) = δ, which is zero under the null hypothesis that case i
is not an outlier but nonzero otherwise. Assuming normal errors, a Student’s
t-test of the hypothesis δ = 0 is given by
ti =
yi −ˆyi(i)
ˆσ(i)

1 + x′
i(X′
(i)X(i))−1xi
(9.2)
This test has n −p′ −1 df, and is identical to the t-test suggested in the ﬁrst
paragraph of this section.
There is a simple computational formula for ti in (9.2). We ﬁrst deﬁne an
intermediate quantity often called a standardized residual, by
ri =
ˆei
ˆσ√1 −hii
(9.3)
where the hii is the leverage for the ith case, deﬁned at (8.7) . Like the residuals
ˆei, the ri have mean zero, but unlike the ˆei, the variances of the ri are all equal to

196
OUTLIERS AND INFLUENCE
one. Because the hii need not all be equal, the ri are not just a rescaling of the ˆei.
With the aid of Appendix A.12, one can show that ti can be computed as
ti = ri

n −p′ −1
n −p′ −r2
i
1/2
=
ˆei
ˆσ(i)
√1 −hii
(9.4)
A statistic divided by its estimated standard deviation is usually called a studentized
statistic, in honor of W. S. Gosset, who ﬁrst wrote about the t-distribution using
the pseudonym Student1. The residual ti is called a studentized residual. We see
that ri and ti carry the same information since one can be obtained from the other
via a simple formula. Also, this result shows that ti can be computed from the
residuals, the leverages and ˆσ 2, so we don’t need to delete the ith case, or to add
a variable U, to get the outlier test.
9.1.2
Weighted Least Squares
If we initially assumed that Var(Y|X) = σ 2/w for known positive weights w, then
in equation (9.3), we compute the residuals ˆei using the correct weighted formula
(8.13) and leverages are the diagonal elements of (8.12). Otherwise, no changes
are required.
9.1.3
Signiﬁcance Levels for the Outlier Test
If the analyst suspects in advance that the ith case is an outlier, then ti should be
compared with the central t-distribution with the appropriate number of df. The
analyst rarely has a prior choice for the outlier. Testing the case with the largest
value of |ti| to be an outlier is like performing n signiﬁcance tests, one for each
of n cases. If, for example, n = 65, p′ = 4, the probability that a t statistic with
60 df exceeds 2.000 in absolute value is 0.05; however, the probability that the
largest of 65 independent t-tests exceeds 2.000 is 0.964, suggesting quite clearly
the need for a different critical value for a test based on the maximum of many
tests. Since tests based on the ti are correlated, this computation is only a guide.
Excellent discussions of this and other multiple-test problems are presented by
Miller (1981).
The technique we use to ﬁnd critical values is based on the Bonferroni inequality,
which states that for n tests each of size a, the probability of falsely labeling at least
one case as an outlier is no greater than na. This procedure is conservative and
provides an upper bound on the probability. For example, the Bonferroni inequality
speciﬁes only that the probability of the maximum of 65 tests exceeding 2.00 is
no greater than 65(0.05), which is larger than 1. Choosing the critical value to
be the (α/n) × 100% point of t will give a signiﬁcance level of no more than
n(α/n) = α. We would choose a level of .05/65 = .00077 for each test to give an
overall level of no more than 65(.00077) = .05.
1See www-gap.dcs.st-and.ac.uk/˜history/Mathematicians/Gosset.html for a biography of Student.

OUTLIERS
197
Standard functions for the t-distribution can be used to compute p-values for
the outlier test: simply compute the p-value as usual and then multiply by the
sample size. If this number is smaller then one, then this is the p-value adjusted
for multiple testing. If this number exceeds one, then the p-value is one.
In Forbes’ data, Example 1.1, case 12 was suspected to be an outlier because
of its large residual. To perform the outlier test, we ﬁrst need the standardized
residual, which is computed using (9.3) from ˆei = 1.36, ˆσ = 0.379, and h12,12 =
0.0639,
r12 =
1.3592
0.379
√
1 −.0639
= 3.7078
and the outlier test is
ti = 3.7078

17 −2 −1
17 −2 −3.70782
1/2
= 12.40
The nominal two-sided p-value corresponding to this test statistic when com-
pared with the t(14) distribution is 6.13 × 10−9. If the location of the outlier was
not selected in advance, the Bonferroni-adjusted p-value is 17 × 6.13 × 10−9 =
1.04 × 10−7. This very small value supports case 12 as an outlier.
The test locates an outlier, but it does not tell us what to do about it. If we
believe that the case is an outlier because of a blunder, for example, an unusually
large measurement error, or a recording error, then we might delete the outlier and
analyze the remaining cases without the suspected case. Sometimes, we can try to
ﬁgure out why a particular case is outlying, and ﬁnding the cause may be the most
important part of the analysis. All this depends on the context of the problem you
are studying.
9.1.4
Additional Comments
There is a vast literature on methods for handling outliers, including Barnett and
Lewis (2004), Hawkins (1980), and Beckman and Cook (1983). If a set of data
has more than one outlier, a sequential approach can be recommended, but the
cases may mask each other, making ﬁnding groups of outliers difﬁcult. Cook and
Weisberg (1982, p. 28) provide the generalization of the mean shift model given
here to multiple cases. Hawkins, Bradu, and Kass (1984) provide a promising
method for searching all subsets of cases for outlying subsets. Bonferroni bounds
for outlier tests are discussed by Cook and Prescott (1981). They ﬁnd that for one-
case-at-a-time methods the bound is very accurate, but it is much less accurate for
multiple-case methods.
The testing procedure helps the analyst in ﬁnding outliers, to make them avail-
able for further study. Alternatively, we could design robust statistical methods
that can tolerate or accommodate some proportion of bad or outlying data; see, for
example, Staudte and Sheather (1990).

198
OUTLIERS AND INFLUENCE
9.2
INFLUENCE OF CASES
Single cases or small groups of cases can strongly inﬂuence the ﬁt of a regression
model. In Anscombe’s examples in Figure 1.9d, page 13, the ﬁtted model depends
entirely on the one point with x = 19. If that case were deleted, we could not
estimate the slope. If it were perturbed, moved around a little, the ﬁtted line would
follow the point. In contrast, if any of the other cases were deleted or moved
around, the change in the ﬁtted mean function would be quite small.
The general idea of inﬂuence analysis is to study changes in a speciﬁc part
of the analysis when the data are slightly perturbed. Whereas statistics such as
residuals are used to ﬁnd problems with a model, inﬂuence analysis is done as if
the model were correct, and we study the robustness of the conclusions, given a
particular model, to the perturbations. The most useful and important method of
perturbing the data is deleting the cases from the data one at a time. We then study
the effects or inﬂuence of each individual case by comparing the full data analysis
to the analysis obtained with a case removed. Cases whose removal causes major
changes in the analysis are called inﬂuential.
Using the notation from the last section, a subscript (i) means “with the ith case
deleted,” so, for example, β(i) is the estimate of β computed without case i, X(i)
is the (n −1) × p′ matrix obtained from X by deleting the ith row, and so on. In
particular, then,
ˆβ(i) = (X′
(i)X(i))−1X′
(i)Y(i)
(9.5)
Figure 9.1 is a scatterplot matrix of coefﬁcient estimates for the three parameters
in the UN data from Section 3.1 obtained by deleting cases one at a time. Every
time a case is deleted, different coefﬁcient estimates may be obtained. All 2D plot
in Figure 9.1 are more or less elliptically shaped, which is a common characteristic
of the deletion estimates. In the plot for the coefﬁcients for log(PPgdp) and Purban,
the points for Armenia and Ukraine are in one corner and the point for Djibouti is
in the opposite corner; deleting any one of these localities causes the largest change
in the values of the estimated parameters, although all the changes are small.
While the plots in Figure 9.1 are informative about the effects of deleting cases
one at a time, looking at these plots can be bewildering, particularly if the number
of terms in the model is large. A single summary statistic that can summarize these
pictures is desirable, and this is provided by Cook’s distance.
9.2.1
Cook’s Distance
We can summarize the inﬂuence on the estimate of β by comparing ˆβ to ˆβ(i).
Since each of these is a p′ vector, the comparison requires a method of combining
information from each of the p′ components into a single number. Several ways
of doing this have been proposed in the literature, but most of them will result in
roughly the same information, at least for multiple linear regression. The method
we use is due to Cook (1977). We deﬁne Cook’s distance Di to be
Di = ( ˆβ(i) −ˆβ)′(X′X)( ˆβ(i) −ˆβ)
p′ ˆσ 2
(9.6)

INFLUENCE OF CASES
199
Intercept
−0.132
−0.126
−0.120
2.56 2.58 2.60 2.62
−0.132
−0.126
−0.120
log(PPgdp)
2.56 2.58 2.60 2.62
−0.0040
−0.0034
−0.0040
−0.0034
Purban
FIG. 9.1
Estimates of parameters in the UN data obtained by deleting one case at a time.
This statistic has several desirable properties. First, contours of constant Di are
ellipsoids, with the same shape as conﬁdence ellipsoids. Second, the contours can
be thought of as deﬁning the distance from ˆβ(i) to ˆβ. Third, Di does not depend
on parameterization, so if the columns of X are modiﬁed by linear transformation,
Di is unchanged. Finally, if we deﬁne vectors of ﬁtted values as ˆY = X ˆβ and
ˆY(i) = X ˆβ(i), then (9.6) can be rewritten as
Di = ( ˆY(i) −ˆY)′( ˆY(i) −ˆY)
p′ ˆσ 2
(9.7)
so Di is the ordinary Euclidean distance between ˆY and ˆY(i). Cases for which Di
is large have substantial inﬂuence on both the estimate of β and on ﬁtted values,
and deletion of them may result in important changes in conclusions.
9.2.2
Magnitude of Di
Cases with large values of Di are the ones whose deletion will result in substantial
changes in the analysis. Typically, the case with the largest Di, or in large data sets
the cases with the largest few Di, will be of interest. One method of calibrating

200
OUTLIERS AND INFLUENCE
Di is obtained by analogy to conﬁdence regions. If Di were exactly equal to the
α × 100% point of the F distribution with p′ and n −p′ df, then deletion of the
ith case would move the estimate of ˆβ to the edge of a (1 −α) × 100% conﬁdence
region based on the complete data. Since for most F distributions the 50% point
is near one, a value of Di = 1 will move the estimate to the edge of about a 50%
conﬁdence region, a potentially important change. If the largest Di is substantially
less than one, deletion of a case will not change the estimate of ˆβ by much. To
investigate the inﬂuence of a case more closely, the analyst should delete the large
Di case and recompute the analysis to see exactly what aspects of it have changed.
9.2.3
Computing Di
From the derivation of Cook’s distance, it is not clear that using these statistics is
computationally convenient. However, the results sketched in Appendix A.12 can
be used to write Di using more familiar quantities. A simple form for Di is
Di = 1
p′ r2
i
hii
1 −hii
(9.8)
Di is a product of the square of the ith standardized residual ri and a monotonic
function of hii. If p′ is ﬁxed, the size of Di will be determined by two different
sources: the size of ri, a random variable reﬂecting lack of ﬁt of the model at the
ith case, and hii, reﬂecting the location of xi relative to x. A large value of Di
may be due to large ri, large hii, or both.
Rat Data
An experiment was conducted to investigate the amount of a particular drug present
in the liver of a rat. Nineteen rats were randomly selected, weighed, placed under
light ether anesthesia and given an oral dose of the drug. Because large livers would
absorb more of a given dose than smaller livers, the actual dose an animal received
was approximately determined as 40 mg of the drug per kilogram of body weight.
Liver weight is known to be strongly related to body weight. After a ﬁxed length
of time, each rat was sacriﬁced, the liver weighed, and the percent of the dose
in the liver determined. The experimental hypothesis was that, for the method of
determining the dose, there is no relationship between the percentage of the dose in
the liver (Y) and the body weight BodyWt, liver weight LiverWt, and relative Dose.
The data, provided by Dennis Cook and given in the ﬁle rat.txt, are shown in
Figure 9.2. As had been expected, the marginal summary plots for Y versus each
of the predictors suggests no relationship, and none of the simple regressions is
signiﬁcant, all having t-values less than one.
The ﬁtted regression summary for the regression of Y on the three predictors
is shown in Table 9.1. BodyWt and Dose have signiﬁcant t-tests, with p < 0.05 in
both cases, indicating that the two measurements combined are a useful indicator
of Y; if LiverWt is dropped from the mean function, the same phenomenon appears.
The analysis so far, based only on summary statistics, might lead to the conclusion

INFLUENCE OF CASES
201
BodyWt
6
7
8
9
0.20
0.35
0.50
150
170
190
6
7
8
9
LiverWt
Dose
0.75 0.85 0.95
150
170
190
0.20
0.35
0.50
0.75 0.85 0.95
y
FIG. 9.2
Scatterplot matrix for the rat data.
TABLE 9.1
Regression Summary for the Rat Data
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
0.265922
0.194585
1.367
0.1919
BodyWt
-0.021246
0.007974
-2.664
0.0177
LiverWt
0.014298
0.017217
0.830
0.4193
Dose
4.178111
1.522625
2.744
0.0151
Residual standard error: 0.07729 on 15 degrees of freedom
Multiple R-Squared: 0.3639
F-statistic:
2.86 on 3 and 15 DF,
p-value: 0.07197
that while neither BodyWt or Dose are associated with the response when the
other is ignored, in combination they are associated with the response. But, from
Figure 9.2, Dose and BodyWt are almost perfectly linearly related, so they measure
the same thing!
We turn to case analysis to attempt to resolve this paradox. Figure 9.3 displays
diagnostic statistics for the mean function with all the terms included. The outlier

202
OUTLIERS AND INFLUENCE
Case number
0.0
0.4
0.8
0
5
10
15
20
Cook’s distance
0.2 0.4 0.6 0.8
Leverage
−1
0
1
2
Outlier test
FIG. 9.3
Diagnostic statistics for the rat data.
statistics are not particularly large. However, Cook’s distance immediately locates
a possible cause: case three has D3 = .93; no other case has Di bigger than 0.27,
suggesting that case number three alone may have large enough inﬂuence on the
ﬁt to induce the anomaly. The value of h33 = 0.85 indicates that the problem is an
unusual set of predictors for case 3.
One suggestion at this point is to delete the third case and recompute the regres-
sion. These computations are given in Table 9.2. The paradox dissolves and the
apparent relationship found in the ﬁrst analysis can thus be ascribed to the third
case alone.
Once again, the diagnostic analysis ﬁnds a problem, but does not tell us what
to do next, and this will depend on the context of the problem. Rat number three,
with weight 190 g, was reported to have received a full dose of 1.000, which was a
larger dose than it should have received according to the rule for assigning doses;
for example, rat eight with weight of 195 g got a lower dose of 0.98. A number
of causes for the result found in the ﬁrst analysis are possible: (1) the dose or
weight recorded for case 3 was in error, so the case should probably be deleted

INFLUENCE OF CASES
203
TABLE 9.2
Regression Summary for the Rat Data with Case 3 Deleted
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
0.311427
0.205094
1.518
0.151
BodyWt
-0.007783
0.018717
-0.416
0.684
LiverWt
0.008989
0.018659
0.482
0.637
Dose
1.484877
3.713064
0.400
0.695
Residual standard error: 0.07825 on 14 degrees of freedom
Multiple R-Squared: 0.02106
F-statistic: 0.1004 on 3 and 14 DF,
p-value: 0.9585
from the study, or (2) the regression ﬁt in the second analysis is not appropriate
except in the region deﬁned by the 18 points excluding case 3. This has many
implications concerning the experiment. It is possible that the combination of dose
and rat weight chosen was fortuitous, and that the lack of relationship found would
not persist for any other combinations of them, since inclusion of a data point
apparently taken under different conditions leads to a different conclusion. This
suggests the need for collection of additional data, with dose determined by some
rule other than a constant proportion of weight.
9.2.4
Other Measures of Inﬂuence
The added-variable plots introduced in Section 3.1 provide a graphical diagnostic
for inﬂuence. Cases corresponding to points at the left or right of an added-variable
plot that do not match the general trend in the plot are likely to be inﬂuential for
the variable that is to be added. For example, Figure 9.4 shows the added-variable
plots for BodyWt and for Dose for the rat data. The point for case three is clearly
−8
−6
−4
−2
0
2
−0.10
−0.05
0.00
0.05
0.10
0.15
0.20
BodyWt | others
y | others
−0.01
0.01 0.02 0.03 0.04
−0.10
−0.05
0.00
0.05
0.10
0.15
0.20
Dose | others
y | others
FIG. 9.4
Added-variable plots for BodyWt and Dose.

204
OUTLIERS AND INFLUENCE
separated from the others, and is a likely inﬂuential point based on these graphs.
The added-variable plot does not correspond exactly to Cook’s distance, but to
local inﬂuence deﬁned by Cook (1986).
As with the outlier problem, inﬂuential groups of cases may serve to mask
each other and may not be found by examination of cases one at a time. In some
problems, multiple-case methods may be desirable; see Cook and Weisberg (1982,
Section 3.6).
9.3
NORMALITY ASSUMPTION
The assumption of normal errors plays only a minor role in regression analysis. It
is needed primarily for inference with small samples, and even then the bootstrap
outlined in Section 4.6 can be used for inference. Furthermore, nonnormality of the
unobservable errors is very difﬁcult to diagnose in small samples by examination
of residuals. From (8.4), the relationship between the errors and the residuals is
ˆe = (I −H)Y
= (I −H)(X ˆβ + e)
= (I −H)e
because (I −H)X = 0. In scalar form, the ith residual is
ˆei = ei −


m

j=1
hijej


(9.9)
Thus, ˆei is equal to ei, adjusted by subtracting off a weighted sum of all the errors.
By the central limit theorem, the sum in (9.9) will be nearly normal even if the
errors are not normal. With a small or moderate sample size n, the second term
can dominate the ﬁrst, and the residuals can behave like a normal sample even if
the errors are not normal. Gnanadesikan (1997) refers to this as the supernormality
of residuals.
As n increases for ﬁxed p′, the second term in (9.9) has small variance compared
to the ﬁrst term, so it becomes less important, and residuals can be used to assess
normality; but in large samples, normality is much less important. Should a test of
normality be desirable, a normal probability plot can be used. A general treatment
of probability plotting is given by Gnanadesikan (1997). Suppose we have a sample
of n numbers z1, z2, . . . , zn, and we wish to examine the hypothesis that the z’s
are a sample from a normal distribution with unknown mean µ and variance σ 2.
A useful way to proceed is as follows:
1. Order the z’s to get z(1) ≤z(2) ≤. . . ≤z(n). The ordered zs are called the
sample order statistics.

NORMALITY ASSUMPTION
205
2. Now, consider a standard normal sample of size n. Let u(1) ≤u(2) ≤. . . ≤
u(n) be the mean values of the order statistics that would be obtained if we
repeatedly took samples of size n from the standard normal. The u(i)s are
called the expected order statistics. The u(i) are available in printed tables,
or can be well approximated using a computer program2.
3. If the zs are normal, then
E(z(i)) = µ + σu(i)
so that the regression of z(i) on u(i) will be a straight line. If it is not straight,
we have evidence against normality.
Judging whether a probability plot is sufﬁciently straight requires experience.
Daniel and Wood (1980) provided many pages of plots to help the analyst learn to
use these plots; this can be easily recreated using a computer package that allows
one quickly to look at many plots. Atkinson (1985) used a variation of the bootstrap
to calibrate probability plots.
Many statistics have been proposed for testing a sample for normality. One of
these that works extremely well is the Shapiro and Wilk (1965) W statistic, which
is essentially the square of the correlation between the observed order statistics
and the expected order statistics. Normality is rejected if W is too small. Royston
(1982abc) provides details and computer routines for the calculation of the test and
for ﬁnding p-values.
Figure 9.5 shows normal probability plots of the residuals for the heights data
(Section 1.1) and for the transactions data (Section 4.6.1). Both have large enough
−3
−2
−1
0
1
2
3
−5
0
5
(a) Heights data
Sample Quantiles
−3
−2
−1
0
1
2
3
−4000 −2000
0
2000 4000 6000
(b) Transaction data
Sample Quantiles
FIG. 9.5
Normal probability plots of residuals for (a) the heights data and (b) the transactions data.
2Suppose (x) is a function that returns the area p to the left of x under a standard normal distribution,
and −1(p) computes the inverse of the normal, so for a given value of p, it returns the associated
value of x. Then the ith expected normal order statistic is approximately −1[(i −(3/8))/(n + (1/4))]
(Blom, 1958).

206
OUTLIERS AND INFLUENCE
samples for normal probability plots to be useful. For the heights data, the plot is
very nearly straight, indicating no evidence against normality. For the transactions
data, normality is in doubt because the plot is not straight. In particular, there are
very large positive residuals well away from a ﬁtted line. This supports the earlier
claim that the errors for this problem are likely to be skewed with too many large
values.
PROBLEMS
9.1. In an unweighted regression problem with n = 54, p′ = 5, the results inclu-
ded ˆσ = 4.0 and the following statistics for four of the cases:
ˆei
hii
1.000
0.9000
1.732
0.7500
9.000
0.2500
10.295
0.185
For each of these four cases, compute ri, Di, and ti. Test each of the four
cases to be an outlier. Make a qualitative statement about the inﬂuence of
each case on the analysis.
9.2. In the fuel consumption data, consider ﬁtting the mean function
E(Fuel|X) = β0 + β1Tax + β2Dlic + β3Income + β4log(Miles)
For this regression, we ﬁnd ˆσ = 64.891 with 46 df, and the diagnostic statis-
tics for four states and the District of Columbia were:
Fuel
ˆei
hii
Alaska
514.279
−163.145
0.256
New York
374.164
−137.599
0.162
Hawaii
426.349
−102.409
0.206
Wyoming
842.792
183.499
0.084
District of Columbia
317.492
−49.452
0.415
Compute Di and ti for each of these cases, and test for one outlier. Which
is most inﬂuential?
9.3. The matrix (X′
(i)X(i)) can be written as (X′
(i)X(i)) = X′X −xix′
i, where x′
i
is the ith row of X. Use this deﬁnition to prove that (A.37) holds.

PROBLEMS
207
9.4. The quantity yi −x′
i ˆβ(i) is the residual for the ith case when β is estimated
without the ith case. Use (A.37) to show that
yi −x′
i ˆβ(i) =
ˆei
1 −hii
This quantity is called the predicted residual, or the PRESS residual.
9.5. Use (A.37) to verify (9.8).
9.6. Suppose that interest centered on β∗rather than β, where β∗is the parameter
vector excluding the intercept. Using (5.21) as a basis, deﬁne a distance
measure D∗
i like Cook’s Di and show that (Cook, 1979)
D∗
i = r2
i
p

hii −1/n
1 −hii + 1/n

where p is the number of terms in the mean function excluding the intercept.
9.7. Refer to the lathe data in Problem 6.2.
9.7.1. Starting with the full second-order model, use the Box–Cox method
to show that an appropriate scale for the response is the logarithmic
scale.
9.7.2. Find the two cases that are most inﬂuential in the ﬁt of the quadratic
mean function, and explain why they are inﬂuential. Delete these
points from the data, reﬁt the quadratic mean function, and compare
to the ﬁt with all the data.
9.8. Florida election 2000 In the 2000 election for US president, the counting of
votes in Florida was controversial. In Palm Beach county in south Florida,
for example, voters used a so-called butterﬂy ballot. Some believe that the
layout of the ballot caused some voters to cast votes for Buchanan when
their intended choice was Gore.
The data in the ﬁle florida.txt3 has four variables, County, the
county name, and Gore, Bush, and Buchanan, the number of votes for each
of these three candidates. Draw the scatterplot of Buchanan versus Bush, and
test the hypothesis that Palm Beach county is an outlier relative to the sim-
ple linear regression mean function for E(Buchanan|Bush). Identify another
county with an unusual value of the Buchanan vote, given its Bush vote, and
test that county to be an outlier. State your conclusions from the test, and its
relevance, if any, to the issue of the butterﬂy ballot.
Next, repeat the analysis, but ﬁrst consider transforming the variables
in the plot to better satisfy the assumptions of the simple linear regression
model. Again test to see if Palm Beach County is an outlier and summarize.
3Source: http://abcnews.go.com/sections/politics/2000vote/general/FL county.html.

208
OUTLIERS AND INFLUENCE
9.9. Refer to the United Nations data described in Problem 7.8 and consider the
regression with response ModernC and predictors (log(PPgdp), Change, Pop,
Fertility, Frate, Purban).
9.9.1. Examine added-variable plots for each of the terms in the regres-
sion model and summarize. Is it likely that any of the localities are
inﬂuential for any of the terms? Which localities? Which terms?
9.9.2. Are there any outliers in the data?
9.9.3. Complete analysis of the regression of ModernC on the terms in the
mean function.
9.10. The data in the data ﬁle landrent.txt were collected by Douglas Tiffany
to study the variation in rent paid in 1977 for agricultural land planted to
alfalfa. The variables are Y = average rent per acre planted to alfalfa, X1 =
average rent paid for all tillable land, X2 = density of dairy cows (number
per square mile), X3 = proportion of farmland used as pasture, X4 = 1 if
liming is required to grow alfalfa; 0, otherwise.
The unit of analysis is a county in Minnesota; the 67 counties with appre-
ciable rented farmland are included. Alfalfa is a high protein crop that is
suitable feed for dairy cows. It is thought that rent for land planted to alfalfa
relative to rent for other agricultural purposes would be higher in areas with
a high density of dairy cows and rents would be lower in counties where
liming is required, since that would mean additional expense. Use all the
techniques learned so far to explore these data with regard to understanding
rent structure. Summarize your results.
9.11. The data in the ﬁle cloud.txt summarize the results of the ﬁrst Florida
Area Cumulus Experiment, or FACE-1, designed to study the effectiveness of
cloud seeding to increase rainfall in a target area (Woodley, Simpson, Bion-
dini, and Berkeley, 1977). A ﬁxed target area of approximately 3000 square
miles was established to the north and east of Coral Gables, Florida. During
the summer of 1975, each day was judged on its suitability for seeding. The
decision to use a particular day in the experiment was based primarily on a
suitability criterion S depending on a mathematical model for rainfall. Days
with S > 1.5 were chosen as experimental days; there were 24 days chosen
in 1975. On each day, the decision to seed was made by ﬂipping a coin;
as it turned out, 12 days were seeded, 12 unseeded. On seeded days, silver
iodide was injected into the clouds from small aircraft. The predictors and
the response are deﬁned in Table 9.3.
The goal of the analysis is to decide if there is evidence that cloud seeding
is effective in increasing rainfall. Begin your analysis by drawing appropri-
ate graphs. Obtain appropriate transformations of predictors. Fit appropriate
mean functions and summarize your results. (Hint: Be sure to check for
inﬂuential observations and outliers.)

PROBLEMS
209
TABLE 9.3
The Florida Area Cumulus Experiment on Cloud Seeding
Variable
Description
A
Action, 1 = seed, 0 = do not seed
D
Days after the ﬁrst day of the experiment (June 16, 1975=0)
S
Suitability for seeding
C
Percent cloud cover in the experimental area, measured using radar in
Coral Gables, Florida
P
Prewetness, amount of rainfall in the hour preceding seeding in 107 cubic
meters
E
Echo motion category, either l or 2, a measure of the type of cloud
Rain
Rainfall following the action of seeding or not seeding in 107 cubic meters
9.12. Health plans use many tools to try to control the cost of prescription
medicines. For older drugs, generic substitutes that are equivalent to name-
brand drugs are sometimes available at a lower cost. Another tool that
may lower costs is restricting the drugs that physicians may prescribe. For
example, if three similar drugs are available for treating the same symptoms,
a health plan may require physicians to prescribe only one of them. Since
the usage of the chosen drug will be higher, the health plan may be able to
negotiate a lower price for that drug.
The data in the ﬁle drugcost.txt, provided by Mark Siracuse, can
be used to explore the effectiveness of these two strategies in controlling
drug costs. The response variable is COST, the average cost of drugs per
prescription per day, and predictors include GS (the extent to which the
plan uses generic substitution, a number between zero, no substitution, and
100, always use a generic substitute if available) and RI (a measure of the
restrictiveness of the plan, from zero, no restrictions on the physician, to 100,
TABLE 9.4
The Drug Cost Data
Variable
Description
COST
Average cost to plan for one prescription for one day, dollars
RXPM
Average number of prescriptions per member per year
GS
Percent generic substitution used by the plan
RI
Restrictiveness index (0=none, 100=total)
COPAY
Average member copayment for prescriptions
AGE
Average member age
F
Percent female members
MM
Member months, a measure of the size of the plan
ID
An identiﬁer for the name of the plan

210
OUTLIERS AND INFLUENCE
the maximum possible restrictiveness). Other variables that might impact cost
were also collected, and are described in Table 9.4. The data are from the
mid-1990s, and are for 29 plans throughout the United States with pharmacies
administered by a national insurance company.
Provide a complete analysis if these data, paying particular regard to
possible outliers and inﬂuential cases. Summarize your results with regard
to the importance of GS and RI. In particular, can we infer that more use of
GS and RI will reduce drug costs?

C H A P T E R 10
Variable Selection
We live in an era of cheap data but expensive information. A manufacturer studying
the factors that impact the quality of its product, for example, may have many mea-
sures of quality, and possibly hundreds or even thousands of potential predictors of
quality, including characteristics of the manufacturing process, training of employ-
ees, supplier of raw materials, and many others. In a medical setting, to model the
size of tumor, we might have many potential predictors that describe the status of
the patient, treatments given, and environmental factors thought to be relevant. In
both of these settings, and in many others, we can have too many predictors.
One response to working with problems with many potential predictors is to
try to identify the important or active predictors and the unimportant or inactive
ones. Variable selection methods are often used for this purpose, and we will study
them in this chapter. Estimates and predictions will generally be more precise from
ﬁtted models based only on relevant terms, although selection tends to overestimate
signiﬁcance. Sometimes, identifying the important predictors can be an end in
itself. For example, learning if the supplier of raw materials impacts quality of a
manufactured product may be more important than attempting to measure the exact
form of the dependence.
Linear regression with variable selection is not the only approach to the problem
of modeling a response as a function of a very large number of terms or predictors.
The ﬁelds of machine learning and to some extent data mining provide alternative
techniques for this problem, and in some circumstances, the methods developed
in these areas can give superior answers. An introduction to these areas is given
by Hastie, Tibshirani, and Friedman (2001). The methods described in this chapter
are important in their own right because they are so widely used, and also because
they can provide a basis for understanding newer methods.
10.1
THE ACTIVE TERMS
Given a response Y and a set of terms X derived from the predictors, the idealized
goal of variable selection is to divide X into two pieces X = (XA, XI), where XA
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
211

212
VARIABLE SELECTION
is the set of active terms, while XI is the set of inactive terms that are not relevant
to the regression problem, in the sense that the two mean functions E(Y|XA, XI)
and E(Y|XA) would give exactly the same values. Identifying the active predictors
can be surprisingly difﬁcult.
Dividing X = (XA, XI) into active and inactive terms, suppose the mean
function
E(Y|X = x) = β′X = β′
AxA + β′
IxI
(10.1)
was a correct speciﬁcation. For example, using the methods of the previous chapters
in this book, we might have selected transformations, added interactions, and possi-
bly deleted a few outliers so that (10.1) holds at least to a reasonable approximation.
For the inactive terms, we will have βI = 0. If we have a sufﬁciently large sample
to estimate β, then identifying XA seems easy: the terms in XA will have non zero
corresponding elements in ˆβ, and the terms in XI will correspond to elements in ˆβ
close to zero.
Simulated Data
To illustrate, we consider two cases based on artiﬁcial data, each with ﬁve terms
in X, including the intercept. For both cases, the response is obtained as
y = 1 + x1 + x2 + 0x3 + 0x4 + error
where the error is N(0, 1), so σ 2 = 1. For this mean function, XA is the intercept
plus ﬁrst two components of X, and XI is the remaining two components. In
the ﬁrst case we consider, X1 = (x1, x2, x3, x4) are independent standard normal
random variables, and so the population covariance matrix for X1 is
Var(X1) =


1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1


In the second case, X2 = (x1, x2, x3, x4) are again normal with mean zero, but the
population covariance matrix is
Var(X2) =


1
0
.95
0
0
1
0
−.95
.95
0
1
0
0
−.95
0
1


(10.2)
so the ﬁrst and third variables are highly positively correlated, and the second and
fourth variables are highly negatively correlated.
Table 10.1 summarizes one set of simulated data for the ﬁrst case and with
n = 100. ˆβ is reasonably close to the true value of 1 for the ﬁrst three coefﬁcients
and 0 for the remaining two coefﬁcients. The t-values for the ﬁrst three terms are
large, indicating that these are clearly estimated to be nonzero, while the t-values

THE ACTIVE TERMS
213
TABLE 10.1
Regression Summary for the Simulated Data with No Correlation
between the Predictors
Estimate
Std. Error
t-value
Pr(>|t|)
(Intercept)
0.8022
0.0919
8.73
0.0000
x1
0.9141
0.0901
10.14
0.0000
x2
0.9509
0.0861
11.04
0.0000
x3
−0.0842
0.1091
−0.77
0.4423
x4
−0.2453
0.1109
−2.21
0.0294
ˆσ = 0.911, df = 95, R2 = 0.714
Var( ˆβ) =
1
100


0.84
0.09
0.01 −0.05
0.02
0.09
0.81 −0.03 −0.04 −0.06
0.01 −0.03
0.74 −0.16 −0.07
−0.05 −0.04 −0.16
1.19
0.02
0.02 −0.06 −0.07
0.02
1.23


for the remaining two terms are much smaller. As it happens, the t-test for β4 has
a p-value of about 0.03, suggesting incorrectly that β3 ̸= 0. If we do tests at 5%
level, then 5% of the time we will make errors like this. Also shown in Table 10.1
is Var( ˆβ), which is approximately equal to 1/n times a diagonal matrix; ignoring
the entries in the ﬁrst row and column of this matrix that involve the intercept, the
remaining 4 × 4 matrix should be approximately the inverse of Var(X1), which is
the identity matrix. Apart from the intercept, all the estimates are equally variable
and independent. If the sample size were increased to 1000, Var( ˆβ) would be
approximately the same matrix multiplied by 1/1000 rather than 1/100.
Table 10.2 gives the summary of the results when n = 100, and the covariance
of the terms excluding the intercept is given by (10.2). XA and XI are not clearly
identiﬁed. Since x2 and x4 are almost the same variable, apart from a sign change,
identiﬁcation of x4 as more likely to be the active predictor is not surprising; the
choice between x2 and x4 can vary from realization to realization of this simulation.
All of the t-values are much smaller than in the ﬁrst case, primarily because with
covariance between the terms, variances of the estimated coefﬁcients are greatly
inﬂated relative to uncorrelated terms. To get estimates with about the same vari-
ances in case 2 as we got in case 1 requires about 11 times as many observations.
The simulation for case 2 is repeated in Table 10.3 with n = 1100. Apart from
the intercept, estimates and standard errors are now similar to those in Table 10.1,
but the large correlations between some of the estimates, indicated by the large
off-diagonal elements in the covariance matrix for ˆβ, remain. Identiﬁcation of the
terms in XA and XI with correlation present can require huge sample sizes relative
to problems with uncorrelated terms.
Selection methods try to identify the active terms and then reﬁt, ignoring the
terms thought to be inactive. Table 10.4a is derived from Table 10.2 by deleting the
two terms with small t-values. This seems like a very good solution and summary
of this problem, with one exception: it is the wrong answer, since x4 is included

214
VARIABLE SELECTION
TABLE 10.2
Regression Summary for the Simulated Data with High Correlation
between the Predictors
Estimate
Std. Error
t-value
Pr(>|t|)
(Intercept)
0.8022
0.0919
8.73
0.0000
x1
1.1702
0.3476
3.37
0.0011
x2
0.2045
0.3426
0.60
0.5519
x3
−0.2696
0.3494
−0.77
0.4423
x4
−0.7856
0.3553
−2.21
0.0294
ˆσ = 0.911, df = 95, R2 = 0.702
Var( ˆβ) =
1
100


0.84
0.25
0.08 −0.17
0.07
0.25
12.08
0.14 −11.73
−0.34
0.08
0.14
11.73
−0.36
11.78
−0.17 −11.73
−0.36
12.21
0.17
0.07
−0.34
11.78
0.17
12.63


TABLE 10.3
Regression Summary for the Simulated Data, Correlated Case But
with n = 1100
Estimate
Std. Error
t-value
Pr(>|t|)
(Intercept)
1.0354
0.0305
33.92
0.0000
x1
1.0541
0.0974
10.83
0.0000
x2
1.1262
0.0989
11.39
0.0000
x3
−0.0106
0.0978
−0.11
0.9136
x4
0.1446
0.1006
1.44
0.1511
ˆσ = 1.01, df = 1095, R2 = 0.68
Var( ˆβ) =
1
1100


1.02
−0.10
0.00
0.09
−0.05
−0.10
10.43
−0.07
−9.97
−0.05
0.00
−0.07
10.75
0.06
10.41
0.09
−9.97
0.06
10.52
0.06
−0.05
−0.05
10.41
0.06
11.14


rather than x2. Table 10.4b is the ﬁt of the mean function using only the correct
XA as terms. The ﬁt of this choice for the mean function is somewhat worse, with
larger ˆσ and smaller R2.
10.1.1
Collinearity
Two terms X1 and X2 are exactly collinear, or linearly dependent, if there is a
linear equation such as
c1X1 + c2X2 = c0
(10.3)
for some constants c0, c1 and c2 that is true for all cases in the data. For example,
suppose that X1 and X2 are amounts of two chemicals and are chosen so that

THE ACTIVE TERMS
215
TABLE 10.4
Regression Summary for Two Candidate Subsets in the Simulated
Data, Correlated Cases, with n = 100
Estimate
Std. Error
t-value
Pr(>|t|)
(a) Candidate terms are intercept, x1 and x4
(Intercept)
0.7972
0.0912
8.74
0.0000
x1
0.9146
0.0894
10.23
0.0000
x4
−0.9796
0.0873
−11.22
0.0000
ˆσ = 0.906, df = 97, R2 = 0.711
Estimate
Std. Error
t-value
Pr(>|t|)
(b) Candidate terms are intercept, x1 and x2
(Intercept)
0.8028
0.0933
8.60
0.0000
x1
0.9004
0.0915
9.84
0.0000
x2
0.9268
0.0861
10.76
0.0000
ˆσ = 0.927, df = 97, R2 = 0.691
X1 + X2 = 50 ml, then X1 and X2 are exactly collinear. Since X2 = 50 −X1,
knowing X1 is exactly the same as knowing both X1 and X2, and only one of
X1 or X2 can be included in a mean function. Exact collinearities can occur by
accident when, for example, weight in pounds and in kilograms are both included
in a mean function, or with sets of dummy variables. Approximate collinearity is
obtained if the equation (10.3) nearly holds for the observed data. For example,
the variables Dose and BodyWt in the rat data in Section 9.2.3 are approximately
collinear since Dose was approximately determined as a multiple of BodyWt. In the
ﬁrst of the two simulated cases in the last section, there is no collinearity because
the terms are uncorrelated. In the second case, because of high correlation x1 ≈x3
and x2 ≈−x4, so these pairs of terms are collinear.
Collinearity between terms X1 and X2 is measured by the square of their sam-
ple correlation, r2
12. Exact collinearity corresponds to r2
12 = 1, and noncollinearity
corresponds to r2
12 = 0. As r2
12 approaches 1, approximate collinearity becomes
generally stronger. Most discussions of collinearity are really concerned with
approximate collinearity.
The deﬁnition of approximate collinearity extends naturally to p > 2 terms. A set
of terms, X1, X2, . . ., Xp are approximately collinear if, for constants c0, c1, . . ., cp,
c1X1 + c2X2 + · · · + cpXp ≈c0
with at least one cj ̸= 0. For that j, we can write
Xj ≈1
cj

c0 −

ℓ̸=j
cℓXℓ

= c0
cj
+

ℓ̸=j

−cℓ
cj
	
Xℓ

216
VARIABLE SELECTION
which is similar to a linear regression mean function with intercept c0/cj and
slopes −cℓ/cj. A simple diagnostic analogous to the squared correlation for the
two-variable case is the square of the multiple correlation between Xj and the
other X’s, which we will call R2
j. This number is computed from the regression of
Xj on the other X’s. If the largest R2
j is near 1, we would diagnose approximate
collinearity.
When a set of predictors is exactly collinear, one or more predictors must be
deleted, or else unique least squares estimates of coefﬁcients do not exist. Since
the deleted predictor contains no information after the others, no information is lost
by this process although interpretation of parameters can be more complex. When
collinearity is approximate, a usual remedy is again to delete variables from the
mean function, with loss of information about ﬁtted values expected to be minimal.
The hard part is deciding which variables to delete.
10.1.2
Collinearity and Variances
We have seen in the initial example in this chapter that correlation between the
terms increases the variances of estimates. In a mean function with two terms
beyond the intercept,
E(Y|X1 = x1, X2 = x2) = β0 + β1x1 + β2x2
suppose the sample correlation between X1 and X2 is r12, and deﬁne the symbol
SXjXj = 
(xij −xj)2 to be the sum of squares for the jth term in the mean
function. It is an exercise (Problem 10.7) to show that, for j = 1, 2,
Var( ˆβj) =
σ 2
1 −r2
12
1
SXjXj
(10.4)
The variances of ˆβ1 and ˆβ2 are minimized if r2
12 = 0, while as r2
12 nears 1, these
variances are greatly inﬂated; for example, if r2
12 = .95, the variance of β1 is 20
times as large as if r2
12 = 0. The use of collinear predictors can lead to unacceptably
variable estimated coefﬁcients compared to problems with no collinearity.
When p > 2, the variance of the j-th coefﬁcient is (Problem 10.7)
Var( ˆβj) =
σ 2
1 −R2
j
1
SXjXj
(10.5)
The quantity 1/(1 −R2
j) is called the jth variance inﬂation factor, or VIFj (Mar-
quardt, 1970). Assuming that the Xj’s could have been sampled to make R2
j = 0
while keeping SXjXj constant, the VIF represents the increase in variance due to
the correlation between the predictors and, hence, collinearity.
In the ﬁrst of the two simulated examples earlier in this section, all the terms are
independent, so each of the R2
j should be close to zero, and the VIF are all close

VARIABLE SELECTION
217
to their minimum value of one. For the second example, each of the R2
j ≈0.952,
and each VIF should be close to 1/(1 −.952) = 10.256. Estimates in the second
case are about 10 or 11 times as variable as estimates in the ﬁrst case.
10.2
VARIABLE SELECTION
The goal of variable selection is to divide X into the set of active terms XA and
the set of inactive terms XI. For this purpose, we assume that the mean function
(10.1) is appropriate for the data at hand. If we have k terms in the mean function
apart from the intercept, then there are potentially 2k possible choices of XA
obtained from all possible subsets of the terms. If k = 5, there are only 25 = 32
choices for XA and all 32 possible can be ﬁt and compared. If k = 10, there are
1024 choices, and ﬁtting such a large number of models is possible but still an
unpleasant prospect. For k as small as 30, the number of models possible is much
too large to consider them all.
There are two basic issues. First, given a particular candidate XC for the active
terms, what criterion should be used to compare XC to other possible choices for
XA? The second issue is computational: How do we deal with the potentially huge
number of comparisons that need to be made?
10.2.1
Information Criteria
Suppose we have a particular candidate subset XC. If XC = XA, then the ﬁtted
values from the ﬁt of the mean function
E(Y|XC = xC) = β′
CxC
(10.6)
should be very similar to the ﬁt of mean function (10.1), and the residual sum of
squares for the ﬁt of (10.6) should be similar to the residual sum of squares for
(10.1). If XC misses important terms, the residual sum of squares should be larger;
see Problem 10.9.
Criteria for comparing various candidate subsets are based on the lack of ﬁt
of a model and its complexity. Lack of ﬁt for a candidate subset XC is measured
by its residual sum of squares RSSC. Complexity for multiple linear regression
models is measured by the number of terms pC in XC, including the intercept1.
The most common criterion that is useful in multiple linear regression and many
other problems where model comparison is at issue is the Akaike Information
Criterion, or AIC. Ignoring constants that are the same for every candidate subset,
AIC is given by Sakamoto, Ishiguro, and Kitagawa (1987),
AIC = n log(RSSC/n) + pC
(10.7)
1The complexity may also be deﬁned as the number of parameters estimated in the regression as a
whole, which is equal to the number of terms plus one for estimating σ 2.

218
VARIABLE SELECTION
Small values of AIC are preferred, so better candidate sets will have smaller RSS
and a smaller number of terms pC. An alternative to AIC is the Bayes Information
Criterion, or BIC, given by Schwarz (1978),
BIC = n log(RSSC/n) + pC log(n)
(10.8)
which provides a different balance between lack of ﬁt and complexity. Once again,
smaller values are preferred.
Yet a third criterion that balances between lack of ﬁt and complexity is Mallows’
Cp (Mallows, 1973), where the subscript p is the number of terms in candidate
XC. This statistic is deﬁned by
CpC = RSSC
ˆσ 2
+ 2pC −n
(10.9)
where ˆσ 2 is from the ﬁt of (10.1). As with many problems for which many solutions
are proposed, there is no clear choice between the criteria for preferring a subset
mean function. There is an important similarity between all three criteria: if we
ﬁx the complexity, meaning that we consider only the choices XC with a ﬁxed
number of terms, then all three will agree that the choice with the smallest value
of residual sum of squares is the preferred choice.
Highway Accident Data
We will use the highway accident data described in Section 7.2. The initial terms
we consider include the transformations found in Section 7.2.2 and a few others and
are described in Table 10.5. The response variable is, from Section 7.3, log(Rate).
This mean function includes 14 terms to describe only n = 39 cases.
TABLE 10.5
Deﬁnition of Terms for the Highway Accident Data
Variable
Description
log(Rate)
Base-two logarithm of 1973 accident rate per million vehicle miles,
the response
log(Len)
Base-two logarithm of the length of the segment in miles
log(ADT)
Base-two logarithm of average daily trafﬁc count in thousands
log(Trks)
Base-two logarithm of truck volume as a percent of the total volume
Slim
1973 speed limit
Lwid
Lane width in feet
Shld
Shoulder width in feet of outer shoulder on the roadway
Itg
Number of freeway-type interchanges per mile in the segment
log(Sigs1)
Base-two logarithm of (number of signalized interchanges per mile
in the segment + 1)/(length of segment)
Acpt
Number of access points per mile in the segment
Hwy
A factor coded 0 if a federal interstate highway, 1 if a principal
arterial highway, 2 if a major arterial, and 3 otherwise

VARIABLE SELECTION
219
TABLE 10.6
Regression Summary for the Fit of All Terms in the Highway Accident
Dataa
Estimate
Std. Error
t-value
Pr(>|t|)
Intercept
5.7046
2.5471
2.24
0.0342
logLen
−0.2145
0.1000
−2.15
0.0419
logADT
−0.1546
0.1119
−1.38
0.1792
logTrks
−0.1976
0.2398
−0.82
0.4178
logSigs1
0.1923
0.0754
2.55
0.0172
Slim
−0.0393
0.0242
−1.62
0.1172
Shld
0.0043
0.0493
0.09
0.9313
Lane
−0.0161
0.0823
−0.20
0.8468
Acpt
0.0087
0.0117
0.75
0.4622
Itg
0.0515
0.3503
0.15
0.8842
Lwid
0.0608
0.1974
0.31
0.7607
Hwy1
0.3427
0.5768
0.59
0.5578
Hwy2
−0.4123
0.3940
−1.05
0.3053
Hwy3
−0.2074
0.3368
−0.62
0.5437
ˆσ = 0.376 on 25 df, R2 = 0.791
aThe terms Hwy1, Hwy2, and Hwy3 are dummy variables for the highway factor.
The regression on all the terms is summarized in Table 10.6. Only two of the
terms have t-values exceeding 2 in absolute value, in spite of the fact that R2 =
0.791. Few of the predictors adjusted for the others are clearly important even
though, taken as a group, they are useful for predicting accident rates. This is
usually evidence that XA is smaller than the full set of available terms.
To illustrate the information criteria, consider a candidate subset XC consisting
of the intercept and (log(Len), Slim, Acpt, log(Trks), Shld). For this choice, RSSC =
5.016 with pC = 6. For the mean function with all the terms, RSSX = 3.537 with
pX = 14, so ˆσ 2 = 0.1415. From these, we can compute the values of AIC, BIC,
and Cp for the subset mean function,
AIC = 39 log(5.016/39) + 2 × 6 = −67.99
BIC = 39 log(5.016/39) + 6 log(39) = −58.01
Cp = 5.016
0.1415 + 2 × 6 −39 = 8.453
and for the full mean function,
AIC = 39 log(3.5377/39) + 2 × 14 = −65.611
BIC = 39 log(3.5377/39) + 14 log(39) = −42.322
Cp = 3.537
0.1415 + 2 × 14 −39 = 14

220
VARIABLE SELECTION
All three criteria are smaller for the subset, and so the subset is preferred over
the mean function with all the terms. This subset need not be preferred to other
subsets, however.
10.2.2
Computationally Intensive Criteria
Cross-validation can also be used to compare candidate subset mean functions.
The most straightforward type of cross-validation is to split the data into two parts
at random, a construction set and a validation set. The construction set is used
to estimate the parameters in the mean function. Fitted values from this ﬁt are
then computed for the cases in the validation set, and the average of the squared
differences between the response and the ﬁtted values for the validation set is used
as a summary of ﬁt for the candidate subset. Good candidates for XA will have
small cross-validation errors. Correction for complexity is not required because
different data are used for ﬁtting and for estimating ﬁtting errors.
Another version of cross-validation uses predicted residuals for the subset mean
function based on the candidate XC. For this criterion, compute the ﬁtted value for
each i from a regression that does not include case i. The sum of squares of these
values is the predicted residual sum of squares, or PRESS (Allen, 1974; Geisser
and Eddy, 1979),
PRESS =
n

i=1
(yi −x′
Ci ˆβC(i))2 =
n

i=1

ˆeCi
1 −hCii
	2
(10.10)
where ˆeCi and hCii are, respectively, the residual and the leverage for the ith case
in the subset model. For the subset mean function and the full mean function
considered in the last section, the values of PRESS are computed to be 7.688 and
11.272, respectively, suggesting substantial improvement of this particular subset
over the full mean function because it gives much smaller errors on the average.
The PRESS method for linear regression depends only on residuals and lever-
ages, so it is fairly easy to compute. This simplicity does not carry over to problems
in which the computation will require reﬁtting the mean function n times. As a
result, this method is not often used outside the linear regression model.
10.2.3
Using Subject-Matter Knowledge
The single most important tool in selecting a subset of variables is the analyst’s
knowledge of the area under study and of each of the variables. In the highway
accident data, Hwy is a factor, so all of its levels should probably either be in
the candidate subset or excluded. Also, the variable log(Len) should be treated
differently from the others, since its inclusion in the active predictors may be
required by the way highway segments are deﬁned. Suppose that highways consist
of “safe stretches” and “bad spots,” and that most accidents occur at the bad spots.
If we were to lengthen a highway segment in our study by a small amount, it is
unlikely that we would add another bad spot to the section, assuming bad spots are

COMPUTATIONAL METHODS
221
rare, but the computed response, accidents per million vehicle miles on the section
of roadway, will decrease. Thus, the response and log(Len) should be negatively
correlated, and we should consider only subsets that include log(Len). Of the 14
terms, one is to be included in all candidate subsets, and three, the dummy variables
for Hwy, are all to be included or excluded as a group. Thus, we have 10 terms
(or groups of terms) that can be included or not, for a total of 210 = 1024 possible
subset mean functions to consider.
10.3
COMPUTATIONAL METHODS
With linear regression, it is possible to ﬁnd the few candidate subsets of each subset
size that minimize the information criteria. Furnival and Wilson (1974) provided
an algorithm called the leaps and bounds algorithm that uses information from
regressions already computed to bound the possible value of the criterion function
for regressions as yet not computed. This trick allows skipping the computation of
most regressions. The algorithm has been widely implemented in statistical pack-
ages and in subroutine libraries. It cannot be used with factors, unless the factors are
replaced by sets of dummy variables, and it cannot be used with computationally
intensive criteria such as cross-validation or PRESS.
For problems other than linear least squares regression, or if cross-validation is
to be used as the criterion function, exhaustive methods are generally not feasible,
and computational compromise is required. Stepwise methods require examining
only a few subsets of each subset size. The estimate of XA is then selected from
the few subsets that were examined. Stepwise methods are not guaranteed to ﬁnd
the candidate subset that is optimal according to any criterion function, but they
often give useful results in practice.
Stepwise methods have three basic variations. For simplicity of presentation, we
assume that no terms beyond the intercept are forced into the subsets considered.
As before, let k be the number of terms, or groups of terms, that might be added
to the mean function. Forward selection uses the following procedure:
[FS.1] Consider all candidate subsets consisting of one term beyond the inter-
cept, and ﬁnd the subset that minimizes the criterion of interest. If an infor-
mation criterion is used, then this amounts to ﬁnding the term that is most
highly correlated with the response because its inclusion in the subset gives
the smallest residual sum of squares. Regardless of the criterion, this step
requires examining k candidate subsets.
[FS.2] For all remaining steps, consider adding one term to the subset selected
at the previous step. Using an information criterion, this will amount to
adding the term with the largest partial correlation2 with the response given
the terms already in the subset, and so this is a very easy calculation. Using
2The partial correlation is the ordinary correlation coefﬁcient between the two plotted quantities in an
added-variable plot.

222
VARIABLE SELECTION
cross-validation, this will require ﬁtting all subsets consisting of the subset
selected at the previous step plus one additional term. At step ℓ, k −ℓ+ 1
subsets need to be considered.
[FS.3] Stop when all the terms are included in the subset, or when addition of
another term increases the value of the selection criterion.
If the number of terms beyond the intercept is k, this algorithm will consider at
most k + (k −1) + · · · + 1 = k(k −1)/2 of the 2k possible subsets. For k = 10,
the number of subsets considered is 45 of the 1024 possible subsets. The subset
among these 45 that has the best value of the criterion selected is tentatively selected
as the candidate for XA.
The algorithm requires modiﬁcation if a group of terms is to be treated as all
included or all not included, as would be the case with a factor. At each step, we
would consider adding the term or the group of terms that produces the best value
on the criterion of interest. Each of the information criteria can now give different
best choices because at each step, as we are no longer necessarily examining mean
functions with pC ﬁxed.
The backward elimination algorithm works in the opposite order:
[BE.1] Fit ﬁrst with candidate subset XC = X, as given by (10.1).
[BE.2] At the next step, consider all possible subsets obtained by removing
one term other than those to be forced to be in all mean functions from the
candidate subset selected at the last step. Using an information criterion, this
amounts to removing the term with the smallest t-value in the regression
summary because this will give the smallest increase in residual sum of
squares. Using cross-validation, all subsets formed by deleting one term from
the current subset must be considered.
[BE.3] Continue until all terms but those forced into all mean functions are
deleted, or until the next deletion increases the value of the criterion.
As with the forward selection method, only k(k −1)/2 subsets are considered,
and the best among those considered is the candidate for XA. The subsets consid-
ered by forward selection and by backward elimination may not be the same. If
factors are included among the terms, as with backward elimination, the information
criteria need not all select the same subset of ﬁxed size as the best.
The forward and backward algorithms can be combined into a stepwise method,
where at each step, a term is either deleted or added so that the resulting can-
didate mean function minimizes the criterion function of interest. This will have
the advantage of allowing consideration of more subsets, without the need for
examining all 2k subsets.
Highway Accidents
Table 10.7 presents a summary of the 45 mean functions examined using forward
selection for the highway accident data, using PRESS as the criterion function for
selecting subsets. The volume of information in this table may seem overwhelming,

COMPUTATIONAL METHODS
223
TABLE 10.7
Forward Selection for the Highway Accident Data. Subsets within a
Step Are Ordered According to the Value of PRESS
Step 1:
Base terms: (logLen)
df
RSS
|
p
C(p)
AIC
BIC
PRESS
Add: Slim
36
6.11216
|
3
10.20 -66.28 -61.29
6.93325
Add: Shld
36
7.86104
|
3
22.56 -56.46 -51.47
9.19173
Add: Acpt
36
7.03982
|
3
16.76 -60.77 -55.78
9.66532
Add: Hwy
34
8.62481
|
5
31.96 -48.85 -40.53
10.4634
Add: logSigs1
36
9.41301
|
3
33.53 -49.44 -44.45
10.8866
Add: logTrks
36
9.89831
|
3
36.96 -47.48 -42.49
11.5422
Add: logADT
36
10.5218
|
3
41.37 -45.09 -40.10
12.0428
Add: Itg
36
10.962
|
3
44.48 -43.50 -38.51
12.5544
Add: Lane
36
10.8671
|
3
43.81 -43.84 -38.84
12.5791
Add: Lwid
36
11.0287
|
3
44.95 -43.26 -38.27
15.3326
Step 2:
Base terms: (logLen Slim)
df
RSS
|
p
C(p)
AIC
BIC
PRESS
Add: logTrks
35
5.5644
|
4
8.33 -67.94 -61.29
6.43729
Add: Hwy
33
5.41187
|
6
11.25 -65.02 -55.04
6.79799
Add: logSigs1
35
5.80682
|
4
10.04 -66.28 -59.62
6.94127
Add: Itg
35
6.10666
|
4
12.16 -64.31 -57.66
7.07987
Add: Lane
35
6.10502
|
4
12.15 -64.32 -57.67
7.15826
Add: logADT
35
6.05881
|
4
11.82 -64.62 -57.97
7.18523
Add: Shld
35
6.0442
|
4
11.72 -64.71 -58.06
7.28524
Add: Acpt
35
5.51181
|
4
7.96 -68.31 -61.66
7.77756
Add: Lwid
35
6.07752
|
4
11.96 -64.50 -57.85
8.9025
Step 3:
Base terms: (logLen Slim logTrks)
df
RSS
|
p
C(p)
AIC
BIC
PRESS
Add: Hwy
32
4.82665
|
7
9.12 -67.49 -55.84
6.28517
Add: Itg
34
5.55929
|
5
10.29 -65.98 -57.66
6.54584
Add: logADT
34
5.46616
|
5
9.64 -66.63 -58.32
6.6388
Add: logSigs1
34
5.45673
|
5
9.57 -66.70 -58.38
6.65431
Add: Lane
34
5.56426
|
5
10.33 -65.94 -57.62
6.66387
Add: Shld
34
5.41802
|
5
9.30 -66.98 -58.66
6.71471
Add: Acpt
34
5.15186
|
5
7.41 -68.94 -60.63
7.341
Add: Lwid
34
5.51339
|
5
9.97 -66.30 -57.98
8.12161
Step 4:
Base terms: (logLen Slim logTrks Hwy)
df
RSS
|
p
C(p)
AIC
BIC
PRESS
Add: logSigs1
31
3.97747
|
8
5.11 -73.03 -59.73
5.67779
Add: Itg
31
4.8187
|
8
11.06 -65.55 -52.24
6.49463
Add: Lane
31
4.8047
|
8
10.96 -65.66 -52.36
6.54448
Add: logADT
31
4.82664
|
8
11.12 -65.49 -52.18
6.73021
Add: Shld
31
4.82544
|
8
11.11 -65.50 -52.19
6.88205
Add: Acpt
31
4.61174
|
8
9.60 -67.26 -53.95
7.72218
Add: Lwid
31
4.80355
|
8
10.95 -65.67 -52.37
8.05348
Step 5:
Base terms: (logLen Slim logTrks Hwy logSigs1)
df
RSS
|
p
C(p)
AIC
BIC
PRESS
Add: Itg
30
3.90937
|
9
6.63 -71.71 -56.74
5.70787
Add: Lane
30
3.91112
|
9
6.64 -71.69 -56.72
5.7465
(Continued overleaf )

224
VARIABLE SELECTION
TABLE 10.7
(Continued)
Step 5:
Base terms: (logLen Slim logTrks Hwy logSigs1)
Add: logADT
30
3.66683
|
9
4.92 -74.21 -59.23
5.86015
Add: Shld
30
3.97387
|
9
7.09 -71.07 -56.10
6.25166
Add: Acpt
30
3.92837
|
9
6.77 -71.52 -56.55
7.15377
Add: Lwid
30
3.97512
|
9
7.10 -71.06 -56.08
7.68299
Add: Itg
30
3.90937
|
9
6.63 -71.71 -56.74
5.70787
Add: Lane
30
3.91112
|
9
6.64 -71.69 -56.72
5.7465
Add: logADT
30
3.66683
|
9
4.92 -74.21 -59.23
5.86015
Add: Shld
30
3.97387
|
9
7.09 -71.07 -56.10
6.25166
Add: Acpt
30
3.92837
|
9
6.77 -71.52 -56.55
7.15377
Add: Lwid
30
3.97512
|
9
7.10 -71.06 -56.08
7.68299
Step 6:
Base terms: (logLen Slim logTrks Hwy logSigs1 Itg)
df
RSS
|
p
C(p)
AIC
BIC
PRESS
Add: Lane
29
3.86586
|
10
8.32 -70.14 -53.51
5.78305
Add: logADT
29
3.66672
|
10
6.92 -72.21 -55.57
6.1424
Add: Shld
29
3.90652
|
10
8.61 -69.74 -53.10
6.30147
Add: Acpt
29
3.86515
|
10
8.32 -70.15 -53.52
7.17893
Add: Lwid
29
3.90718
|
10
8.62 -69.73 -53.09
7.73347
Step 7:
Base terms: (logLen Slim logTrks Hwy logSigs1 Itg Lane)
df
RSS
|
p
C(p)
AIC
BIC
PRESS
Add: logADT
28
3.65494
|
11
8.83 -70.33 -52.03
6.31797
Add: Shld
28
3.86395
|
11
10.31 -68.16 -49.86
6.40822
Add: Acpt
28
3.8223
|
11
10.02 -68.59 -50.29
7.32972
Add: Lwid
28
3.86487
|
11
10.32 -68.15 -49.85
7.89833
Step 8: Base terms: (logLen Slim logTrks Hwy logSigs1 Itg Lane logADT)
df
RSS
|
p
C(p)
AIC
BIC
PRESS
Add: Shld
27
3.654
|
12
10.83 -68.34 -48.38
6.93682
Add: Acpt
27
3.55292
|
12
10.11 -69.44 -49.47
8.2891
Add: Lwid
27
3.63541
|
12
10.70 -68.54 -48.58
8.3678
Step 9:
Base terms: (logLen Slim logTrks Hwy logSigs1 Itg Lane
logADT Shld)
df
RSS
|
p
C(p)
AIC
BIC
PRESS
Add: Lwid
26
3.61585
|
13
12.56 -66.75 -45.12
8.85795
Add: Acpt
26
3.55037
|
13
12.09 -67.46 -45.84
9.33926
Step 10:
Base terms: (logLen Slim logTrks Hwy logSigs1 Itg Lane
logADT Shld Lwid)
df
RSS
|
p
C(p)
AIC
BIC
PRESS
Add: Acpt
25
3.53696
|
14
14.00 -65.61 -42.32
11.2722
so some description is in order. At Step 1, the mean function consists of the single
term log(Len) beyond the intercept because this term is to be included in all mean
functions. Ten mean functions can be obtained by adding one of the remaining 10-
candidate terms, counting Hwy as a single term. For each candidate mean function,
the df, RSS, and number of terms p = pC in the mean function are printed, as are

COMPUTATIONAL METHODS
225
PRESS and the three information criteria. If none of the terms were factors, then all
three information criteria would order the terms identically. Since Hwy is a factor,
the ordering need not be the same on all criteria. PRESS may choose a different
ordering from the other three. All the criteria agree for the “best” term to add at
the ﬁrst step, since adding Slim gives the smallest value of each criterion.
Step 2 starts with the base mean function consisting of log(Len) and Slim, and
PRESS selects log(Trks) at this step. Both Cp and BIC would select a different
term at this step, leading to different results (see Problem 10.10). This process is
repeated at each step.
The candidate mean function with the smallest value of PRESS is given by
(log(Len), Slim, log(Trks), Hwy, log(Sigs1)), with a value of PRESS = 5.678. Sev-
eral other subsets have values of PRESS that differ from this one by only a trivial
amount, and, since the values of all the criteria are random variables, declaring
this subset to the “best” needs to be tempered with a bit of common sense. The
estimated active predictors should be selected from among the few essentially
equivalent subsets on some other grounds, such as agreement with theory. The
candidate for the active subset has R2 = 0.765, as compared to the maximum pos-
sible value of 0.791, the R2 for the mean function using all the predictors. Further
analysis of this problem is left to homework problems.
10.3.1
Subset Selection Overstates Signiﬁcance
All selection methods can overstate signiﬁcance. Consider another simulated
example. A data set of n = 100 cases with a response Y and 50 predictors
X = (X1, . . . , X50) was generated using standard normal random deviates, so
there are no active terms, and the true multiple correlation between Y and X
is also exactly zero. The regression of Y on X is summarized in the ﬁrst line
of Table 10.8. The value of R2 = 0.54 may seem surprisingly large, considering
that all the data are independent random numbers. The overall F-test, which is
in a scale more easily calibrated, gives a p-value of .301 for the data; Rencher
and Pun (1980) and Freedman (1983) report similar simulations with the over-
all p-value varying from near 0 to near 1, as it should since the null hypothesis
of β = 0 is true. In the simulation reported here, 11 of 50 terms had |t| >
√
2,
while 7 of 50 had |t| > 2. Line 2 of Table 10.8 displays summary statistics from
the ﬁt of the mean function that retains all the terms with |t| >
√
2. The value
of R2 drops to 0.28. The major change is in the perceived signiﬁcance of the
result. The overall F now has p-value of about .001, and the |t|-values for ﬁve
terms exceed 2. The third line is similar to the second, except a more stringent
|t| > 2 was used. Using seven terms, R2 = 0.20, and four terms have |t|-values
exceeding two.
This example demonstrates many lessons. Signiﬁcance is overstated. The coefﬁ-
cients for the terms left in the mean function will generally be too large in absolute
value, and have t- or F-values that are too large. Even if the response and the
predictors are unrelated, R2 can be large: when β = 0, the expected value of R2
is k/(n −1). With selection, R2 can be much too large.

226
VARIABLE SELECTION
TABLE 10.8
Results of a Simulated Example with 50 Terms and n = 100
Number of
p-value of
Number
Number
Method
Terms
R2
Overall F
|t| >
√
2
|t| > 2
No selection
50
0.48
0.301
11
7
|t| >
√
2
11
0.28
0.001
7
5
|t| > 2
5
0.20
0.003
5
4
10.4
WINDMILLS
The windmill data discussed in Problems 2.13, 4.6 and 6.11 provide another case
study for model selection. In Problem 2.13, only the wind speed at the reference
site was used in the mean function. In Problem 6.11, wind direction at the reference
site was used to divide the data into 16 bins, and a separate regression was ﬁt in
each of the bins, giving a mean function with 32 parameters. We now consider
several other potential mean functions.
10.4.1
Six Mean Functions
For this particular candidate site, we used as a reference site the closest site where
the National Center for Environmental Modeling data is collected, which is south-
west of the candidate. There are additional possible reference sites to the northwest,
the northeast, and the southeast of the candidate site. We could use data from all
four of these candidates to predict wind speed at the candidate site. In addition,
we could consider the use of lagged variables, in which we use the wind speed
at the reference site six hours before the current time to model the current wind
speed at the candidate site. Lagged variables are commonly used with data collected
at regularly spaced intervals and can help account for serial correlations between
consecutive measurements.
In all, we will consider six very different mean functions for predicting CSpd,
using the terms deﬁned in Table 10.9:
[Model 1] E(CSpd|Spd1) = β0 + β1Spd1. This was ﬁt in Problem 2.13.
[Model 2] Fit as in Model 1, but with a separate intercept and slope for each
of 16 bins determined by the wind direction at reference site 1.
[Model 3] This mean function uses the information about the wind directions
in a different way. Writing θ for the wind direction at the reference site, the
mean function is
E(CSpd|X) = β0 + β1Spd1 + β2 cos(θ) + β3 sin(θ)
= +β4 cos(θ)Spd1 + β5 sin(θ)Spd1
This mean function uses four terms to include the information in the wind
direction. The term cos(θ)Spd1 is the wind component in the east–west

WINDMILLS
227
TABLE 10.9
Description of Data in the Windmill Data in the File wm4.txt
Label
Description
Date
Date and time of measurement. “2002/3/4/12” means March 4, 2002 at 12
hours after midnight
Dir1
Wind direction θ at reference site 1 in degrees
Spd1
Wind speed at reference site 1 in meters per second. Site 1 is the closest
site to the candidate site
Spd2
Wind speed at reference site 2 in m/s
Spd3
Wind speed at reference site 3 in m/s
Spd4
Wind speed at reference site 4 in m/s
Spd1Lag1
Wind speed at reference site 1 six hours previously
Spd2Lag1
Wind speed at reference site 2 six hours previously
Spd3Lag1
Wind speed at reference site 3 six hours previously
Spd4Lag1
Wind speed at reference site 4 six hours previously
Bin
Bin number
Spd1Sin1
Spd1 × sin(θ), site 1
Spd1Cos1
Spd1 × cos(θ), site 1
CSpd
Wind speed in m/s at the candidate site
direction, while sin(θ)Spd1 is the component in the north–south direction.
The terms in sine and cosine alone are included to allow information from
the wind direction alone.
[Model 4] This model uses the mean function
E(CSpd|X) = β0 + β1Spd1 + β2Spd1Lag1
that ignores information from the angles but includes information from the
wind speed at the previous period.
[Model 5] This model uses wind speed from all four candidate sites,
E(CSpd|X) = β0 + β1Spd1 + β2Spd2 + β3Spd3 + β4Spd4
[Model 6] The ﬁnal mean function starts with model 5 and then adds informa-
tion on the lagged wind speeds:
E(CSpd|X) = β0 + β1Spd1 + β2Spd2 + β3Spd3 + β4Spd4
+ β5Spd1Lag1 + β6Spd2Lag1 + β7Spd3Lag1
+ β8Spd4Lag1
All six of these mean functions were ﬁt using the data in the ﬁle wm4.txt. The
ﬁrst case in the data does not have a value for the lagged variables, so it has been
deleted from the ﬁle. Since the month of May 2002 is also missing, the ﬁrst case
in June 2002 was also deleted.

228
VARIABLE SELECTION
TABLE 10.10
Summary Criteria for the Fit of Six
Mean Function to the Windmill Data
df
AIC
BIC
PRESS
Model 1
2
2014.9
2024.9
6799.0
Model 2
32
1989.3
2149.8
6660.2
Model 3
6
2020.7
2050.8
6836.3
Model 4
3
1920.6
1935.6
6249.1
Model 5
5
1740.6
1765.7
5320.2
Model 6
9
1711.2
1756.3
5188.5
Table 10.10 gives the information criteria for comparing the ﬁt of these six mean
functions, along with PRESS. All three criteria agree on the ordering of the models.
The simplest model 1 is preferred over model 3; evidently, the information in the
sine and cosine of the direction is not helpful. Adding the lagged wind speed in
Model 4 is clearly helpful, and apparently is more useful than the information from
binning the directions used in Model 2. Adding information from four reference
sites, as in models 5 and 6, gives a substantial improvement, with about a 15%
decrease in the criterion statistics. Model 6, which includes lags but not information
on angles, appears to be the most appropriate model here.
10.4.2
A Computationally Intensive Approach
The windmill data provides an unusual opportunity to look at model selection by
examining the related problem of estimating the long-term average wind speed not
at the candidate site but at the closest reference site. The data ﬁle wm5.txt3 gives
55 years of data from all four candidate sites. We can simulate the original problem
by estimating a regression model for predicting wind speed at site 1, given the data
from the remaining three sites, and then see how well we do by comparing the
prediction to the actual value, which is the known average over 55 years of data.
We used the following procedure:
1. The year 2002 at the candidate site had n = 1116 data points. We begin by
selecting n time points at random from the 55 years of data to comprise the
“year” of complete data.
2. For the sample of times selected, ﬁt the models we wish to compare. In
this simulation, we considered only the model with one site as the predictor
without binning; one site as predictor with wind directions binned into 16
bins, and using the wind speeds at all three remaining sites as predictors
without using bins or lagged variables. Data from the simulated year were
used to estimate the parameters of the model, predict the average wind speed
3Because this ﬁle is so large, it is not included with the other data ﬁles and must be downloaded
separately from the web site for this book.

WINDMILLS
229
over the remaining time points, and also compute the standard error of the
prediction, using the methodology outlined previously in this section, and in
Problems 2.13 and 6.11.
3. Repeat the ﬁrst two steps 1000 times, and summarize the results in his-
tograms.
The summarizing histograms are shown in Figure 10.1. The ﬁrst column shows
the histograms for the estimates of the long-term average wind speed for the three
mean functions. The vertical dashed lines indicate the true mean wind speed at
site 1 over the 55 years of data collection. All three methods have distributions
of estimates that are centered very close to the true value and appear to be more
or less normally distributed. The second column gives the standard errors of the
estimated mean for the 1000 simulations, and the dashed line corresponds to the
Reference = Spd2, ignore bins
Frequency
7.2
7.3
7.4
7.5
7.6
7.7
0 50
150
Reference = Spd2, 16 bins
Frequency
7.2
7.3
7.4
7.5
7.6
7.7
0 50
150
250
Three references
Frequency
7.2
7.3
7.4
7.5
7.6
7.7
0
100
200
300
Reference = Spd2, ignore bins, SE
Frequency
0.060
0.070
0.080
0
50 100
200
Reference = Spd2, 16 bins, SE
Frequency
0.060
0.070
0.080
0
50
150
Three references, SE
Frequency
0.060
0.070
0.080
0 50
150
250
FIG. 10.1
Summary of the simulation for the windmill data. The ﬁrst column gives a histogram of
the estimated mean wind speed at reference site 1 for 1000 simulations using three mean functions. The
second column gives a histogram of the 1000 standard errors. The dashed lines give the true values,
the average of the wind speed measurements from 1948 to 2003 for the averages, and the standard
deviation of the 1000 averages from the simulation for the standard errors.

230
VARIABLE SELECTION
“true value,” actually the standard deviation of the 1000 means in the ﬁrst column.
In each case, most of the histogram is to the right of the dashed line, indicating
that the standard formulas will generally overestimate the actual standard error,
perhaps by 5%. Also, the mean functions that use only one reference, with or
without binning, are extremely similar, suggesting only a trivial improvement due
to binning. Using three references, however, shifts the distribution of the standard
errors to the left, so this method is much more precise than the others.
Generalizing these results to the candidate site from reference site 1 does not
seem to be too large a leap. This would suggest that we can do better with more
reference sites than with one and that the information about wind direction, at least
at this candidate site, is probably unimportant.
PROBLEMS
10.1. Generate data as described for the two simulated data sets in Section 10.1,
and compare the results you get to the results given in the text.
10.2. Using the “data” in Table 10.11 with a response Y and three predictors X1,
X2 and X3 from Mantel (1970) in the ﬁle mantel.txt, apply the BE and
FS algorithms, using Cp as a criterion function. Also, ﬁnd AIC and Cp for
all possible models, and compare results. What is XA?
TABLE 10.11
Mantel’s Data for Problem 10.2
Y
X1
X2
X3
1
5.00
1.00
1004.00
6.00
2
6.00
200.00
806.00
7.30
3
8.00
−50.00
1058.00
11.00
4
9.00
909.00
100.00
13.00
5
11.00
506.00
505.00
13.10
10.3. Use BE with the highway accident data and compare with the results in
Table 10.7.
10.4. For the boys in the Berkeley Guidance Study in Problem 3.1, ﬁnd a model
for HT18 as a function of the other variables for ages 9 or earlier. Perform
a complete analysis, including selection of transformations and diagnostic
analysis, and summarize your results.
10.5. An experiment was conducted to study O2UP, oxygen uptake in milligrams of
oxygen per minute, given ﬁve chemical measurements shown in Table 10.12
(Moore, 1975). The data were collected on samples of dairy wastes kept in
suspension in water in a laboratory for 220 days. All observations were on the

PROBLEMS
231
TABLE 10.12
Oxygen Update Experiment
Variable
Description
Day
Day number
BOD
Biological oxygen demand
TKN
Total Kjeldahl nitrogen
TS
Total solids
TVS
Total volatile solids
COD
Chemical oxygen demand
O2UP
Oxygen uptake
same sample over time. We desire an equation relating log(O2UP) to the other
variables. The goal is to ﬁnd variables that should be further studied with the
eventual goal of developing a prediction equation; day cannot be used as a
predictor. The data are given in the ﬁle dwaste.txt.
Complete the analysis of these data, including a complete diagnostic
analysis. What diagnostic indicates the need for transforming O2UP to a
logarithmic scale?
10.6. Prove the results (10.4)-(10.5). To avoid tedious algebra, start with an added-
variable plot for Xj after all the other terms in the mean function. The
estimated slope ˆβj is the ols estimated slope in the added-variable plot.
Find the standard error of this estimate, and show that it agrees with the
given equations.
10.7. Gal´apagos Islands The Gal´apagos Islands off the coast of Ecuador provide
an excellent laboratory for studying the factors that inﬂuence the develop-
ment and survival of different life species. Johnson and Raven (1973) have
presented data in the ﬁle galapagos.txt, giving the number of species
and related variables for 29 different islands (Table 10.13). Counts are given
for both the total number of species and the number of species that occur
only on that one island (the endemic species).
Use these data to ﬁnd factors that inﬂuence diversity, as measured by
some function of the number of species and the number of endemic species,
and summarize your results. One complicating factor is that elevation is
not recorded for six very small islands, so some provision must be made
for this. Four possibilities are: (1) ﬁnd the elevations; (2) delete these six
islands from the data; (3) ignore elevation as a predictor of diversity, or (4)
substitute a plausible value for the missing data. Examination of large-scale
maps suggests that none of these elevations exceed 200 m.
10.8. Suppose that (10.1) holds with βI = 0, but we ﬁt a subset model using the
terms XC ̸= XA; that is, XC does not include all the relevant terms. Give
general conditions under which the mean function E(Y|XC) is a linear mean
function. (Hint: See Appendix A.2.4.)

232
VARIABLE SELECTION
TABLE 10.13
Gal´apagos Island Data
Variable
Description
Island
Island name
NS
Number of species
ES
Number of endemic species (occurs only on that island)
Area
Surface area of island, hectares
Anear
Area of closest island, hectares
Dist
Distance to closest island, kilometers
DistSC
Distance from Santa Cruz Island, kilometers
Elevation
Elevation in m, missing values given as zero
EM
1 if elevation is observed, 0 if missing
10.9. For the highway accident data, ﬁt the regression model with active predictors
given by the subset with the smallest value of PRESS in Table 10.7. The
coefﬁcient estimate of Slim is negative, meaning that segments with higher
speed limits lower accident rates. Explain this ﬁnding.
10.10. Re¨express Cp as a function of the F-statistic used for testing the null hypoth-
esis (10.6) versus the alternative (10.1). Discuss.
10.11. In the windmill data discussed in Section 10.4, data were collected at the
candidate site for about a year, for about 1200 observations. One issue
is whether the collection period could be shortened to six months, about
600 observations, or three months, about 300 observations, and still give a
reliable estimate of the long-term average wind speed.
Design and carry out a simulation experiment using the data described
in Section 10.4.2 to characterize the increase in error due to shortening the
collection period. For the purpose of the simulation, consider site #1 to be
the “candidate” site and site #2 to be the reference site, and consider only
the use of Spd2 to predict Spd1. (Hint: The sampling scheme used in Section
10.4.2 may not be appropriate for time periods shorter than a year because
of seasonal variation. Rather than picking 600 observations at random to
make up a simulated six-month period, a better idea might be to pick a
starting observation at random, and then pick 600 consecutive observations
to comprise the simulated six months.)

C H A P T E R 11
Nonlinear Regression
A regression mean function cannot always be written as a linear combination of
the terms. For example, in the turkey diet supplement experiment described in
Section 1.1, the mean function
E(Y|X = x) = θ1 + θ2(1 −exp(−θ3x))
(11.1)
where Y was growth and X the amount of supplement added to the turkey diet,
was suggested. This mean function has three parameters, θ1, θ2, and θ3, but only
one predictor, X. It is a nonlinear mean function because the mean function is not
a linear combination of the parameters. In (11.1), θ2 multiplies 1 −exp(−θ3x), and
θ3 enters through the exponent.
Another nonlinear mean function we have already seen was used in estimating
transformations of predictors to achieve linearity, given by
E(Y|X = x) = β0 + β1ψS(x, λ)
(11.2)
where ψS(x, λ) is the scaled power transformation deﬁned by (7.3), page 150.
This is a nonlinear model because the slope parameter β1 multiplies ψS(x, λ),
which depends on the parameter λ. In Chapter 7, we estimated λ visually and
then estimated the βs from the linear model assuming λ is ﬁxed at its estimated
value. If we estimate all three parameters simultaneously, then the mean function
is nonlinear.
Nonlinear mean functions usually arise when we have additional information
about the dependence of the response on the predictor. Sometimes, the mean func-
tion is selected because the parameters of the function have a useful interpretation.
In the turkey growth example, when X = 0, E(Y|X = 0) = θ1, so θ1 is the expected
growth with no supplementation. Assuming θ3 > 0, as X increases, E(Y|X = x)
will approach θ1 + θ2, so the sum of the ﬁrst two parameters is the maximum
growth possible for any dose called an asymptote, and θ2 is the maximum additional
growth due to supplementation. The ﬁnal parameter θ3 is a rate parameter; for larger
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
233

234
NONLINEAR REGRESSION
values of θ3, the expected growth approaches its maximum more quickly than it
would if θ3 were smaller.
11.1
ESTIMATION FOR NONLINEAR MEAN FUNCTIONS
Here is the general setup for nonlinear regression. We have a set of p terms X,
and a vector θ = (θ1, . . . , θk)′ of parameters such that the mean function relating
the response Y to X is given by
E(Y|X = x) = m(x, θ)
(11.3)
We call the function m a kernel mean function. The two examples of m we
have seen so far in this chapter are in (11.1) and (11.2) but there are of course
many other choices, both simpler and more complex. The linear kernel mean
function, m(x, θ) = x′θ is a special case of the nonlinear kernel mean function.
Many nonlinear mean functions impose restrictions on the parameters, like θ3 > 0
in (11.1).
As with linear models, we also need to specify the variance function, and for
this we will use exactly the same structure as for the linear model and assume
Var(Y|X = xi) = σ 2/wi
(11.4)
where, as before, the wi are known, positive weights and σ 2 is an unknown positive
number. Equations (11.3) and (11.4) together with the assumption that observations
are independent of each other deﬁne the nonlinear regression model. The only
difference between the nonlinear regression model and the linear regression model
is the form of the mean function, and so we should expect that there will be many
parallels that can be exploited.
The data consist of observations (xi, yi), i = 1, . . . , n. Because we have retained
the assumption that observations are independent and that the variance function
(11.4) is known apart from the scale factor σ 2, we can use least squares to estimate
the unknown parameters, so we need to minimize over all permitted values of θ
the residual sum of squares function,
RSS(θ) =
n

i=1
wi(yi −m(xi, θ))2
(11.5)
We have ols if all the weights are equal and wls if they are not all equal.
For linear models, there is a formula for the value ˆθ of θ that minimizes RSS(θ),
given at (A.21) in the Appendix. For nonlinear regression, there generally is no for-
mula, and minimization of (11.5) is a numerical problem. We present some theory
now that will approximate (11.5) at each iteration of a computing algorithm by a
nearby linear regression problem. Not only will this give one of the standard com-
puting algorithms used for nonlinear regression but will also provide expressions

ESTIMATION FOR NONLINEAR MEAN FUNCTIONS
235
for approximate standard errors and point out how to do approximate tests. The
derivation uses some calculus.
We begin with a brief refresher on approximating a function using a Taylor
series expansion1. In the scalar version, suppose we have a function g(β), where
β is a scalar. We want to approximate g(β) for values of β close to some ﬁxed
value β∗. The Taylor series approximation is
g(β) = g(β∗) + (β −β∗)dg(β)
dβ
+ 1
2(β −β∗)2 d2g(β)
dβ2
+ Remainder
(11.6)
All the derivatives in equation (11.6) are evaluated at β∗, and so Taylor series
approximates g(β), the function on the left side of (11.6) using the polynomial in
β on the right side of (11.6). We have only shown a two-term Taylor expansion
and have collected all the higher-order terms into the remainder. By taking enough
terms in the Taylor expansion, any function g can be approximated as closely as
wanted. In most statistical applications, only one or two terms of the Taylor series
are needed to get an adequate approximation. Indeed, in the application of the
Taylor expansion here, we will mostly use a one-term expansion that includes the
quadratic term in the remainder.
When g(θ) is a function of a vector-valued parameter θ, the two-term Taylor
series is very similar,
g(θ) = g(θ∗) + (θ −θ∗)′u(θ∗) + 1
2(θ −θ∗)′H(θ∗)(θ −θ∗) + Remainder (11.7)
where we have deﬁned two new quantities in (11.7), the score vector u(θ∗) and the
Hessian matrix H(θ∗). If θ∗has k elements, then u(θ∗) also has k elements, and its
jth element is given by ∂g(x, θ)/∂θj, evaluated at θ = θ∗. The Hessian is a k × k
matrix whose (ℓ, j) element is the partial second derivative ∂2g(x, θ)/(∂θℓ∂θj),
evaluated at θ = θ∗.
We return to the problem of minimizing (11.5). Suppose we have a current guess
θ∗of the value of θ that will minimize (11.5). The general idea is to approximate
m(θ, xi) using a Taylor approximation around θ∗. Using a one-term Taylor series,
ignoring the term with the Hessian in (11.7), we get
m(θ, xi) ≈m(θ∗, xi) + ui(θ∗)′(θ −θ∗)
(11.8)
We have put the subscript i on the u because the value of the derivatives can
be different for every value of xi. The ui(θ∗) play the same role as the terms
in the multiple linear regression model. There are as many elements of ui(θ∗)
as parameters in the mean function. The difference between nonlinear and linear
models is that the ui(θ∗) may depend on unknown parameters, while in multiple
linear regression, the terms depend only on the predictors.
1Jerzy Neyman (1894–1981), one of the major ﬁgures in the development of statistics in the twenti-
eth century, often said that arithmetic had ﬁve basic operations: addition, subtraction, multiplication,
division, and Taylor series.

236
NONLINEAR REGRESSION
Substitute the approximation (11.8) into (11.5) and simplify to get
RSS(θ) =
n

i=1
wi

yi −m(θ, xi)
2
≈
n

i=1
wi

yi −m(θ∗, xi) −ui(θ∗)′(θ −θ∗)
2
=
n

i=1
wi

ˆe∗
i −ui(θ∗)′(θ −θ∗)
2
(11.9)
where ˆe∗
i = yi −m(θ∗, xi) is the ith working residual that depends on the current
guess θ∗. The approximate RSS(θ) is now in the same form as the residual sum
of squares function for multiple linear regression (5.5), with response given by the
ˆe∗
i , terms given by ui(θ∗), parameter given by θ −θ∗, and weights wi. We switch
to matrix notation and let U(θ∗) be an n × k matrix with ith row ui(θ∗)′, W is
an n × n diagonal matrix of weights, and ˆe∗= (ˆe∗
1, . . . , ˆe∗
n)′. The least squares
estimate is then

θ −θ∗= [U(θ∗)′WU(θ∗)]−1U(θ∗)′Wˆe∗
(11.10)
ˆθ = θ∗+ [U(θ∗)′WU(θ∗)]−1U(θ∗)′Wˆe∗
(11.11)
We will use (11.10) in two ways, ﬁrst to get a computing algorithm for estimat-
ing θ in the rest of this section and then as a basis for inference in the next
section.
Here is the Gausss–Newton algorithm that is suggested by (11.10)–(11.11):
1. Select an initial guess θ(0) for θ, and compute RSS(θ(0)).
2. Set the iteration counter at j = 0.
3. Compute U(θ(j)) and ˆe(j) with ith element yi −m(xi, θ(j)). Evaluating
(11.11) requires the estimate from a weighted linear least squares problem,
with response ˆe(j), predictors U(θ(j)), and weights given by the wi. The new
estimator is θ(j+1). Also, compute the residuals sum of squares RSS(θ(j+1)).
4. Stop if RSS(θ(j)) −RSS(θ(j+1)) is sufﬁciently small, in which case there is
convergence. Otherwise, set j = j + 1. If j is too large, stop, and declare
that the algorithm has failed to converge. If j is not too large, go to step 3.
The Gauss–Newton algorithm estimates the parameters of a nonlinear regression
problem by a sequence of approximating linear wls calculations.
Most statistical software for nonlinear regression uses the Gauss–Newton algo-
rithm, or a modiﬁcation of it, for estimating parameters. Some programs allow
using a general function minimizer based on some other algorithm to minimize
(11.5). We provide some references at the end of the chapter.

INFERENCE ASSUMING LARGE SAMPLES
237
There appear to be two impediments to the use of the Gauss–Newton algo-
rithm. First, the score vectors, which are the derivatives of m with respect to the
parameters, are needed. Some software may require the user to provide expressions
for the derivatives, but many packages compute derivatives using either symbolic
or numeric differentiation. Also, the user must provide starting values θ(0); there
appears to be no general way to avoid specifying starting values. The optimization
routine may also converge to a local minimum of the residuals sum of squares
function rather than a global minimum, and so ﬁnding good starting values can be
very important in some problems. With poor starting values, an algorithm may fail
to converge to any estimate. We will shortly discuss starting values in the context
of an example.
11.2
INFERENCE ASSUMING LARGE SAMPLES
We repeat (11.11), but now we reinterpret θ∗as the true, unknown value of θ.
In this case, the working residuals ˆe∗are now the actual errors e, the differences
between the response and the true means. We write
ˆθ = θ∗+ [U(θ∗)′WU(θ∗)]−1U(θ∗)′We
(11.12)
This equation is based on the assumption that the nonlinear kernel mean function
m can be accurately approximated close to θ∗by the linear approximation (11.8),
and this can be guaranteed only if the sample size n is large enough. We then see
that ˆθ is equal to the true value plus a linear combination of the elements of e, and
by the central limit theorem ˆθ under regularity conditions will be approximately
normally distributed,
ˆθ ∼N(θ∗, σ 2[U(θ∗)′WU(θ∗)]−1)
(11.13)
An estimate of the large-sample variance is obtained by replacing the unknown θ∗
by ˆθ on the right side of (11.13),

Var(ˆθ) = ˆσ 2[U(ˆθ)′WU(ˆθ)]−1
(11.14)
where the estimate of σ 2 is
ˆσ 2 = RSS(ˆθ)
n −k
(11.15)
where k is the number of parameters estimated in the mean function.
These results closely parallel the results for the linear model, and consequently
the inferential methods such as F- and t-tests and the analysis of variance for
comparing nested mean functions, can be used for nonlinear models. One change
that is recommended is to use the normal distribution rather than the t for inferences
where the t would be relevant, but since (11.13) is really expected to be valid only

238
NONLINEAR REGRESSION
in large samples, this is hardly important. We emphasize that in small samples,
large-sample inferences may be inaccurate.
We can illustrate using these results with the turkey growth experiment. Methio-
nine is an amino acid that is essential for normal growth in turkeys. Depending on
the ingredients in the feed, turkey producers may need to add supplemental methio-
nine for a proper diet. Too much methionine could be toxic. Too little methionine
could result in malnourished birds.
An experiment was conducted to study the effects on turkey growth of different
amounts A of methionine, ranging from a control with no supplementation to
0.44% of the total diet. The experimental unit was a pen of young turkeys, and
treatments were assigned to pens at random so that 10 pens get the control (no
supplementation) and 5 pens received each of the other ﬁve amounts used in the
experiment, for a total of 35 pens. Pen weights, the average weight of the turkeys
in the pen, were obtained at the beginning and the end of the experiment three
weeks later. The response variable is Gain, the average weight gain in grams per
turkey in a pen. The weight gains are shown in Table 11.1 and are also given in the
ﬁle turk0.txt (Cook and Witmer, 1985). The primary goal of this experiment
is to understand how expected weight gain E(Gain|A) changes as A is varied. The
data are shown in Figure 11.1.
In Figure 11.1, E(Gain|A) appears to increase with A, at least over the range
of values of A in the data. In addition, there is considerable pen-to-pen variation,
reﬂected by the variability between repeated observations at the same value of A.
The mean function is certainly not a straight line since the difference in the means
when A > 0.3 is much smaller than the difference in means when A < 0.2. While
a polynomial of degree two or three might well match the mean at the six values of
A in the experiment, it will surely not match the data outside the range of A, and
the parameters would have little physical meaning (see Problem 6.15). A nonlinear
mean function is preferable for this problem.
For turkey growth as a function of an amino acid, the mean function
E(Gain|A) = θ1 + θ2(1 −exp(−θ3A))
(11.16)
TABLE 11.1
The Turkey Growth Data
Amount, A
Gain
0.00
644, 631, 661, 624, 633
610, 615, 605, 608, 599
0.04
698, 667, 657, 685, 635
0.10
730, 715, 717, 709, 707
0.16
735, 712, 726, 760, 727
0.28
809, 796, 763, 791, 811
0.44
767, 771, 799, 799, 791

INFERENCE ASSUMING LARGE SAMPLES
239
0.0
0.1
0.2
0.3
0.4
600
650
700
750
800
Amount (percent of diet)
Weight gain (g)
FIG. 11.1
Turkey data.
was suggested by Parks (1982). To estimate the parameters in (11.16), we need
starting values for θ. While there is no absolute rule for selecting starting values,
the following approaches are often useful:
Guessing Sometimes, starting values can be obtained by guessing values for the
parameters. In the turkey data, from Figure 11.1, the intercept is about 620
and the asymptote is around 800. This leads to starting values θ(0)
1
= 620
and θ(0)
2
= 800 −620 = 180. Guessing a value for the rate parameter θ3 is
harder.
Solving equations for a subset of the data Select as many distinct data points as
parameters, and solve the equations for the unknown parameters. The hope is
that the equations will be easy to solve. Selecting data points that are diverse
often works well. In the turkey data, given θ(0)
1
= 620 and θ(0)
2
= 180 from
the graph, we can get an initial estimate for θ3 by solving only one equation
in one unknown. For example, when D = 0.16, a plausible value of Gain is
Gain = 750, so
750 = 620 + 180(1 −exp(−θ(0)
3 (.16)))
which is easily solved to give θ(0)
3
≈8. Thus, we now have starting values
for all three parameters.
Linearization If possible, transform to a multiple linear regression mean func-
tion, and ﬁt it to get starting values. In the turkey data, we can move the
parameters θ1 and θ2 to the left side of the mean function to get
(θ1 + θ2) −yi
θ2
= exp(−θ3D)

240
NONLINEAR REGRESSION
Taking logarithms of both sides,
log
(θ1 + θ2) −yi
θ2

= −θ3D
Substituting initial guesses θ(0)
1
= 620 and θ(0)
2
= 180 on the left side of this
equation, we can compute an initial guess for θ3 by the linear regression
of log[(yi −800)/180] on −D, through the origin. The ols estimate in this
approximate problem is θ(0)
3
≈12.
Many computer packages for nonlinear regression require speciﬁcation of the
function m using an expression such as
y ~ th1 + th2*(1 - exp(-th3*A))
In this equation, the symbol ~ can be read “is modeled as,” and the mathematical
symbols +, -, * and / represent addition, subtraction, multiplication and division,
respectively. Similarly, exp represents exponentiation, and ^, or, in some programs,
**, is used for raising to a power. Parentheses are used according to the usual
mathematical rules. Generally, the formula on the right of the model speciﬁcation
will be similar to an expression in a computer programming language such as
Basic, Fortran, or C. This form of a computer model should be contrasted with the
model statements described in Sections 6.2.1–6.2.2. Model statements for nonlinear
models include explicit statement of both the parameters and the terms on the right
side of the equation. For linear models, the parameters are usually omitted and
only the terms are included in the model statement.
If the starting values are adequate and the nonlinear optimizer converges, output
including the quantities in Table 11.2 will be produced. This table is very similar
to the usual output for linear regression. The column marked “Estimate” gives ˆθ.
Since there is no necessary connection between terms and parameters, the lines
of the table are labeled with the names of the parameters, not the names of the
terms. The next column labeled “Std. Error” gives the square root of the diagonal
entries of the matrix given at (11.14), so the standard errors are based on large-
sample approximation. The column labeled “t-value” is the ratio of the estimate
to its large-sample standard error and can be used for a test of the null hypothesis
TABLE 11.2
Nonlinear Least Squares Fit of (11.16)
Formula: Gain ~ th1 + th2 * (1 - exp(-th3 * A))
Parameters:
Estimate Std. Error t-value Pr(>|t|)
th1
622.958
5.901
105.57
< 2e-16
th2
178.252
11.636
15.32 2.74e-16
th3
7.122
1.205
5.91 1.41e-06
---
Residual standard error: 19.66 on 32 degrees of freedom

INFERENCE ASSUMING LARGE SAMPLES
241
0.0
0.1
0.2
0.3
0.4
600
650
700
750
800
Amount (percent of diet)
Weight gain (g)
FIG. 11.2
Fitted mean function.
that a particular parameter is equal to zero against either a general or one-sided
alternative. The column marked “P(> |t|)” is the signiﬁcance level for this test,
using a normal reference distribution rather than a t-distribution. Given at the foot
of the table is the estimate ˆσ and its df, which is the number of cases minus the
number of elements in θ that were estimated, df = 35 −3 = 32.
Since this example has only one predictor, Figure 11.1 is a summary graph
for this problem. Figure 11.2 repeats this ﬁgure, but with the ﬁtted mean function
ˆE(Gain|A = a) = 622.958 + 178.252(1 −exp(−7.122a)) added to the graph. The
ﬁtted mean function does not reproduce the possible decline of response for the
largest value of A because it is constrained to increase toward an asymptote. For
A = 0.28, the ﬁtted function is somewhat less than the mean of the observed values,
while at A = 0.44, it is somewhat larger than the mean of the observed values. If
we believe that an asymptotic form is really appropriate for these data, then the
ﬁt of this mean function seems to be very good. Using the repeated observations
at each level of A, we can perform a lack-of-ﬁt test for the mean function, which
is F = 2.50 with (3, 29) df, for a signiﬁcance level of 0.08, so the ﬁt appears
adequate.
Three Sources of Methionine
The purpose of this experiment was not only to estimate the weight gain response
curve as a function of amount of methionine added but also to decide if the source
of methionine was important. The complete experiment included three sources
that we will call S1, S2, S3. We can imagine a separate response curve such as
Figure 11.2 for each of the three sources, and the goal might be to decide if the
three response curves are different.
Suppose we create three dummy variables Si, i = 1, 2, 3, so that Si is equal to
one if an observation is from source i, and it is zero otherwise. Assuming that the

242
NONLINEAR REGRESSION
mean function (11.16) is appropriate for each source, the largest model we might
contemplate is
E(Gain|A = a, S1, S2, S3) = S1[θ11 + θ21(1 −exp(−θ31a))]
+ S2[θ12 + θ22(1 −exp(−θ32a))]
+ S3[θ13 + θ23(1 −exp(−θ33a))]
(11.17)
This equation has a separate intercept, rate parameter, and asymptote for each
group, and so has nine parameters. For this particular problem, this function has
too many parameters because a dose of A = 0 from source 1 is the same as A = 0
with any of the sources, so the expected response at A = 0 must be the same
for all three sources. This requires that θ11 = θ12 = θ13 = θ1, which is a model of
common intercepts, different asymptotes, and different slopes,
E(Gain|A = a, S1, S2, S3) = θ1 + S1[θ21(1 −exp(−θ31a))]
+S2[θ22(1 −exp(−θ32a))]
+S3[θ23(1 −exp(−θ33a))]
(11.18)
Other reasonable mean functions to examine include common intercepts and asymp-
totes but separate rate parameters,
E(Gain|A = a, S1, S2, S3) = θ1 +
θ2{S1[1 −exp(−θ31a)] +
S2[1 −exp(−θ32a)] +
S3[1 −exp(−θ33a)]}
(11.19)
= θ1 + θ2

1 −exp(−

θ3iSia)

and ﬁnally the mean function of identical mean functions, given by (11.16).
The data from this experiment are given in the data ﬁle turkey.txt. This
ﬁle is a little different because it does not give the response in each pen, but rather
for each combination of A and source it gives the number m of pens with the
combination, the mean response for those pens, and SD the standard deviation of
the m pen responses. From Section 5.4, we can use these standard deviations to
get a pure error estimate of σ 2 that can be used in lack-of-ﬁt testing,
ˆσ 2
pe = SSpe
dfpe
=
	(m −1)SD2
	(m −1)
= 19916
70
= 284.5
The data are shown in Figure 11.3. A separate symbol was used for each of the
three groups. Each point shown is an average over m pens, where m = 5 for every

INFERENCE ASSUMING LARGE SAMPLES
243
0.00
0.05
0.10
0.15
0.20
0.25
650
700
750
800
A
Gain
FIG. 11.3
Turkey growth as a function of methionine added for three sources of methionine. The
lines shown on the ﬁgure are for the ﬁt of (11.18), the most general reasonable mean function for these
data.
point except at A = 0, where m = 10. The point at A = 0 is common to all three
groups.
The four mean functions (11.16)–(11.19) can all be ﬁt using nonlinear weighted
least squares, with weights equal to the ms. Starting values for the estimates can
be obtained in the same way as for ﬁtting for one group. Table 11.3 summarizes
the ﬁt of the four mean functions, giving the RSS and df for each. For comparing
the mean functions, we start with a lack-of-ﬁt test for the most restrictive, common
mean function, for which the F-test for lack of ﬁt is
F = 4326.1/10
284.5
= 1.52
which, when compared with the F(10, 70) distribution, gives a p-value of about
0.65, or no evidence of lack of ﬁt of this mean function. Since the most restrictive
TABLE 11.3
Four Mean Functions Fit to the Turkey Growth Data with Three
Sources of Methionine
Change in
Source
df
SS
df
SS
Common mean function, (11.16)
10
4326.1
Equal intercept and asymptote, (11.19)
8
2568.4
2
1757.7
Common intercepts, (11.18)
6
2040.0
2
528.4
Separate regressions, (11.17)
4
1151.2
2
888.9
Pure error
70
19916.0

244
NONLINEAR REGRESSION
mean function is adequate, we need not test the other mean functions for lack
of ﬁt.
We can perform tests to compare the mean functions using the general F-
testing procedure of Section 5.4. For example, to compare as a null hypothesis
the equal intercept and slope mean function (11.19) versus the common intercept
mean function (11.18), we need to compute the change in RSS, given in the table
as 528.4 with 2 df. For the F-test, we can use ˆσ 2pe in the denominator, and so
F = 528.4/2
ˆσ 2pe
= 0.93 ∼F(2, 70)
for which p = 0.61, suggesting no evidence against the simpler mean function.
Continued testing suggests that the simplest mean function of no difference between
sources appears appropriate for these data, so we conclude that there is no evidence
of a difference in response curve due to source. If we did not have a pure error
estimate of variance, the estimate of variance from the most general mean function
would be used in the F-tests.
11.3
BOOTSTRAP INFERENCE
The inference methods based on large samples introduced in the last section may
be inaccurate and misleading in small samples. We cannot tell in advance if the
large-sample inference will be accurate or not, as it depends not only on the mean
function but also on the way we parameterize it, since there are many ways to write
the same nonlinear mean function, and on the actual values of the predictors and
the response. Because of this possible inaccuracy, computing inferences in some
other way, at least as a check on the large-sample inferences, is a good idea.
One generally useful approach is to use the bootstrap introduced in Section 4.6.
The case resampling bootstrap described in Section 4.6.1 can be applied in non-
linear regression. Davison and Hinkley (1997) describe an alternative bootstrap
scheme on the basis of resampling residuals, but we will not discuss it here.
We illustrate the use of the bootstrap with data in the ﬁle segreg.txt, which
consists of measurements of electricity consumption in KWH and mean tempera-
ture in degrees F for one building on the University of Minnesota’s Twin Cities
campus for 39 months in 1988–1992, courtesy of Charles Ng. The goal is to model
consumption as a function of temperature. Higher temperature causes the use of air
conditioning, so high temperatures should mean high consumption. This building
is steam heated, so electricity is not used for heating. Figure 11.4, a plot of C =
consumption in KWH/day versus Temp, the mean temperature in degrees F.
The mean function for these data is
E(C|Temp) =

θ0
Temp ≤γ
θ0 + θ1(Temp −γ )
Temp > γ
This mean function has three parameters, the level θ0 of the ﬁrst phase; the slope
θ1 of the second phase, and the knot, γ , and assumes that energy consumption is

BOOTSTRAP INFERENCE
245
10
20
30
40
50
60
70
80
60
70
80
90
Mean temperature (F)
KWH/day
FIG. 11.4
Electrical energy consumption per day as a function of mean temperature for one building.
The line shown on the graph is the least squares ﬁt.
unaffected by temperature when the temperature is below the knot, but the mean
increases linearly with temperature beyond the knot. The goal is to estimate the
parameters.
The mean function can be combined into a single equation by writing
E(C|Temp) = θ0 + θ1(max(0, Temp −γ ))
Starting values can be easily obtained from the graph, with θ(0)
0
= 70, θ(0)
1
= 0.5
and γ (0) = 40. The ﬁtted model is summarized in Table 11.4. The baseline elec-
trical consumption is estimated to be about ˆθ0 ≈75 KWH per day. The knot is
estimated to be at ˆγ ≈42◦F, and the increment in consumption beyond that tem-
perature is about ˆθ2 ≈0.6 KWH per degree increase.
From Figure 11.4, one might get the impression that information about the knot
is asymmetric: γ could be larger than 42 but is unlikely to be substantially less
than 42. We might expect that in this case conﬁdence or test procedures based on
asymptotic normality will be quite poor. We can conﬁrm this using the bootstrap.
TABLE 11.4
Regression Summary for the Segmented Regression Example
Formula: C ~ th0 + th1 * (pmax(0, Temp - gamma))
Parameters:
Estimate Std. Error t value Pr(>|t|)
th0
74.6953
1.3433
55.607
< 2e-16
th1
0.5674
0.1006
5.641 2.10e-06
gamma
41.9512
4.6583
9.006 9.43e-11
Residual standard error: 5.373 on 36 degrees of freedom

246
NONLINEAR REGRESSION
q1
0.4
0.8
1.2
70
72
74
76
78
0.4
0.8
1.2
q2
70
72
74
76
78
30
40
50
60
30
40
50
60
g
FIG. 11.5
Scatterplot matrix of estimates of the parameters in the segmented regression example,
computed from B = 999 case bootstraps.
Figure 11.5 is a scatterplot matrix of B = 999 bootstrap replications. All three
parameters are estimated on each replication. This scatterplot matrix is a little
more complicated than the ones we have previously seen. The diagonals contain
histograms of the 999 estimates of each of the parameters. If the normal approxi-
mation were adequate, we would expect that each of these histograms would look
like a normal density function. While this may be so for θ1, this is not the case for
θ2 and for γ . As expected, the histogram for γ is skewed to the right, meaning that
estimates of γ much larger than about 40 occasionally occur but smaller values
almost never occur. The univariate normal approximations are therefore poor.
The other graphs in the scatterplot matrix tell us about the distributions of the
estimated parameters taken two at a time. If the normal approximation were to
hold, these graphs should have approximately straight-line mean functions. The
smoothers on Figure 11.5 are generally far from straight, and so the large-sample
inferences are likely to be badly in error.

BOOTSTRAP INFERENCE
247
q1
160
200
240
610
620
630
640
160
200
240
q2
610
620
630
640
4
5
6
7
8
9
11
4
5
6
7 8
9
11
q3
FIG. 11.6
Scatterplot matrix of bootstrap estimates for the turkey growth data. Two of the replicates
were very different from the others and were deleted before graphing.
In contrast, Figure 11.6 is the bootstrap summary for the ﬁrst source in the turkey
growth data. Normality is apparent in histograms on the diagonal, and a linear
mean function seems plausible for most of the scatterplots, and so the large-sample
inference is adequate here.
Table 11.5 compares the estimates and conﬁdence intervals produced by large-
sample theory, and by the bootstrap. The bootstrap standard errors are the standard
TABLE 11.5
Comparison of Large-Sample and Bootstrap Inference for the
Segmented Regression Data
Large Sample
Bootstrap
θ0
θ1
γ
θ0
θ1
γ
Estimate
74.70
0.57
41.95
Mean
74.92
0.62
43.60
SE
1.34
0.10
4.66
SD
1.47
0.13
4.81
2.5%
72.06
0.37
32.82
2.5%
71.96
0.47
37.16
97.5%
77.33
0.76
51.08
97.5%
77.60
0.99
55.59

248
NONLINEAR REGRESSION
deviation of the bootstrap replicates, and the ends of the bootstrap 95% conﬁdence
interval are the 0.025 and 0.0975 quantiles of the bootstrap replicates. The large-
sample theory conﬁdence interval is given by the usual rule of estimate plus or
minus 1.96 times the standard error computed from large-sample theory. Although
the bootstrap SDs match the large-sample standard errors reasonably well, the
conﬁdence intervals for both θ1 and for γ are shifted toward smaller values than
the more accurate bootstrap estimates.
11.4
REFERENCES
Seber and Wild (1989) and Bates and Watts (1988) provide textbook-length treat-
ments of nonlinear regression problems. Computational issues are also discussed
in these references, and in Thisted (1988, Chapter 4). Ratkowsky (1990) provides
an extensive listing of nonlinear mean functions that are commonly used in various
ﬁelds of application.
PROBLEMS
11.1. Suppose we have a response Y, a predictor X, and a factor G with g levels. A
generalization of the concurrent regression mean function given by Model 3
of Section 6.2.2, is, for j = 1, . . . , g,
E(Y|X = x, G = j) = β0 + β1j(x −γ )
(11.20)
for some point of concurrence γ .
11.1.1. Explain why (11.20) is a nonlinear mean function. Describe in words
what this mean function speciﬁes.
11.1.2. Fit (11.20) to the sleep data discussed in Section 6.2.2, so the mean
function of interest is
E(TS|log(BodyWt) = x, D = j) = β0 + β1j(x −γ )
(Hint: To get starting values, ﬁt the concurrent regression model with
γ = 0. The estimate of γ will be very highly variable, as is often
the case with centering parameters like γ in this mean function.)
11.2. In ﬁsheries studies, the most commonly used mean function for expected
length of a ﬁsh at a given age is the von Bertalanffy function (von Bertalanffy,
1938; Haddon, 2001), given by
E(Length|Age = t) = L∞(1 −exp(−K(t −t0))
(11.21)
The parameter L∞is the expected value of Length for extremely large ages,
and so it is the asymptotic or upper limit to growth, and K is a growth rate
parameter that determines how quickly the upper limit to growth is reached.
When Age = t0, the expected length of the ﬁsh is 0, which allows ﬁsh to
have nonzero length at birth if t0 < 0.

PROBLEMS
249
11.2.1. The data in the ﬁle lakemary.txt give the Age in years and
Length in millimeters for a sample of 78 bluegill ﬁsh from Lake
Mary, Minnesota, in 1981 (courtesy of Richard Frie). Age is deter-
mined by counting the number of rings on a scale of the ﬁsh. This
is a cross-sectional data set, meaning that all the ﬁsh were measured
once. Draw a scatterplot of the data.
11.2.2. Use nonlinear regression to ﬁt the von Bertalanffy function to these
data. To get starting values, ﬁrst guess at L∞from the scatterplot
to be a value larger than any of the observed values in the data.
Next, divide both sides of (11.21) by the initial estimate of L∞,
and rearrange terms to get just exp(−K(t −t0) on the right of the
equation. Take logarithms, to get a linear mean function, and then
use ols for the linear mean function to get the remaining starting
values. Draw the ﬁtted mean function on your scatterplot.
11.2.3. Obtain a 95% conﬁdence interval for L∞using the large-sample
approximation, and using the bootstrap.
11.3. The data in the ﬁle walleye.txt give the length in mm and the age in
years of a sample of over 3000 male walleye, a popular game ﬁsh, cap-
tured in Butternut Lake in Northern Wisconsin (LeBeau, 2004). The ﬁsh are
also classiﬁed according to the time period in which they were captured,
with period = 1 for pre-1990, period = 2 for 1990–1996, and period = 3
for 1997–2000. Management practices on the lake were different in each of
the periods, so it is of interest to compare the length at age for the three time
periods.
Using the von Bertalanffy length at age function (11.21), compare the
three time periods. If different, are all the parameters different, or just some
of them? Which ones? Summarize your results.
11.4. A quadratic polynomial as a nonlinear model
The data in the ﬁle
swan96.txt were collected by the Minnesota Department of Natural Re-
sources to study the abundance of black crappies, a species of ﬁsh, on Swan
Lake, Minnesota in 1996. The response variable is LCPUE, the logarithm of
the catch of 200 mm or longer black crappies per unit of ﬁshing effort. It
is believed that LCPUE is proportional to abundance. The single predictor
is Day, the day on which the sample was taken, measured as the number of
days after June 19, 1996. Some of the measurements were taken the follow-
ing spring on the same population of ﬁsh before the young of the year are
born in late June. No samples are taken during the winter months when the
lake surface was frozen.
11.4.1. For these data, ﬁt the quadratic polynomial
E(LCPUE|Day = x) = β0 + β1x + β2x2
assuming Var(LCPUE|Day = x) = σ 2. Draw a scatterplot of LCPUE
versus Day, and add the ﬁtted curve to this plot.

250
NONLINEAR REGRESSION
11.4.2. Using the delta method described in Section 6.1.2, obtain the estimate
and variance for the value of Day that maximizes E(LCPUE|Day).
11.4.3. Another parameterization of the quadratic polynomial is
E(Y|X) = θ1 −2θ2θ3x + θ3x2
where the θs can be related to the βs by
θ1 = β0,
θ2 = −β1/2β2,
θ3 = β2
In this parameterization, θ1 is the intercept, θ2 is the value of the
predictor that gives the maximum value of the response, and θ3 is a
measure of curvature. This is a nonlinear model because the mean
function is a nonlinear function of the parameters. Its advantage is
that at least two of the parameters, the intercept θ1 and the value
of x that maximizes the response θ2, are directly interpretable. Use
nonlinear least squares to ﬁt this mean function. Compare your results
to the ﬁrst two parts of this problem.
11.5. Nonlinear regression can be used to select transformations for a linear regres-
sion mean function. As an example, consider the highway accident data,
described in Table 7.1, with response log(Rate) and two predictors X1 = Len
and X2 = ADT. Fit the nonlinear mean function
E(log(Rate)|X1 =x1, X2 =x2, X3 =x3)=β0+β1ψS(X1, λ1)+β2ψS(X2, λ2)
where the scaled power transformations ψS(Xj, λj) are deﬁned at (7.3).
Compare the results you get to results obtained using the transformation
methodology in Chapter 7.
11.6. POD models Partial one-dimensional mean functions for problems with both
factors and continuous predictors were discussed in Section 6.4. For the Aus-
tralian athletes data discussed in that section, the mean function (6.26),
E(LBM|Sex, Ht, Wt, RCC) = β0 + β1Sex + β2Ht + β3Wt + β4RCC
+ η0Sex + η1Sex × (β2Ht + β3Wt + β4RCC)
was suggested. This mean function is nonlinear because η1 multiplies each
of the βs. Problem 6.21 provides a simple algorithm for ﬁnding estimates
using only standard linear regression software. This method, however, will
not produce the large-sample estimated covariance matrix that is available
using nonlinear least squares.
11.6.1. Describe a reasonable method for ﬁnding starting values for ﬁtting
(6.26) using nonlinear least squares.
11.6.2. For the cloud seeding data, Problem 9.11, ﬁt the partial one-
dimensional model using the action variable A as the grouping
variable, and summarize your results.

C H A P T E R 12
Logistic Regression
A storm on July 4, 1999 with winds exceeding 90 miles per hour hit the Boundary
Waters Canoe Area Wilderness (BWCAW) in northeastern Minnesota, causing seri-
ous damage to the forest. Roy Rich studied the effects of this storm using a very
extensive ground survey of the area, determining for over 3600 trees the status,
either alive or dead, species, and size. One goal of this study is to determine
the dependence of survival on species, size of the tree, and on the local severity.
Figure 12.1a shows a plot for 659 Balsam Fir trees, with the response variable Y
coded as 1 for trees that were blown down and died and 0 for trees that survived,
versus the single predictor log(D), the base-two logarithm of the diameter of the
tree. To minimize overprinting, the plotted values of the variables were slightly
jittered before plotting. Even with the jittering, this plot is much less informative
than most of the plots of response versus a predictor that we have seen earlier
in this book. Since the density of ink is higher in the lower-left and upper-right
corners, the probability of blowdown is apparently higher for large trees than for
small trees, but little more than that can be learned from this plot.
Figure 12.1b is an alternative to Figure 12.1a. This graph displays density esti-
mates, which are like smoothed histograms for log(D), separately for the survivors,
the solid line, and for the trees blown down, the dashed line1. Both densities are
roughly shaped like a normal density. The density for the survivors, Y = 1, is
shifted to the right relative to the density for the density for Y = 0, meaning that
the trees that blew down are generally larger. If the histograms had no overlap,
then the quantity on the horizontal axis, log(D), would be a perfect predictor of
survival. Since there is substantial overlap of the densities, log(D) is not a perfect
predictor of blowdown. For values of log(D) where the two densities have the same
height, the probability of survival will be about 0.5. For values of log(D) where
the height of the density for survivors is higher than the density for blowdown,
then the probability of surviving exceeds 0.5; when the height of the density of
1Looking at overlapping histograms is much harder than looking at overlapping density estimates.
Silverman (1986) and Bowman and Azzalini (1997), among others, provide discussions of density
estimation.
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
251

252
LOGISTIC REGRESSION
2.5
3.0
3.5
4.0
4.5
5.0
0.0
0.2
0.4
0.6
0.8
1.0
Log2(Diameter)
Blowdown indicator
2
3
4
5
0.0
0.2
0.4
0.6
0.8
log2(D)
(a)
(b)
Density
Y = 0
Y = 1
FIG. 12.1
Blowdown data for Balsam Fir. (a) Scatterplot of Y versus log(D). The solid line is the
ols line. The dotted line is the ﬁt of a smoothing spline. The dashed line is the logistic regression ﬁt.
Data have been jittered in both variables to minimize overprinting. (b) Separate density estimates for
log(D) for survivors, Y = 0, and blowdown, Y = 1.
blowdown is higher, the probability of surviving is less than 0.5. In Figure 12.1b,
the probability of survival is greater than 0.5 if log(D) is less than about 3.3, and
less than 0.5 if log(D) exceeds 3.3.
More generally, suppose in a problem with predictor X we let θ(x) =
Pr(Y = 1|X = x) be the conditional probability that Y = 1 given the value of
the predictor. This conditional probability plays the role of the mean function in
regression problems when the response is either one or zero. For the blowdown
data in Figure 12.1, the probability of blowdown increases from left to right.
We can visualize θ(log(D)) by adding a smoother to Figure 12.1a. The straight
line on this graph is the ols regression of Y on log(D). It includes estimated values

BINOMIAL REGRESSION
253
of θ(log(D)) outside the range (0, 1) for very small or large trees, and so the ols
line cannot be a good representation of θ(log(D)) for all values of log(D). The
ols line is often inappropriate for a bounded response because it will produce
ﬁtted values outside the permitted range. The dotted smoother in Figure 12.1 uses
a smoother, so it estimates the mean function without a model. This estimate of the
mean function has the characteristic shape of binary regression, with asymptotes
at 0 and 1 for extreme values of the predictor. The logistic regression models we
will study next also have this shape.
As with other regression problems, with a binary response we also have a set
of terms or predictors X, and we are interested in the study of Pr(Y = 1|X = x)
= θ(x) as x is varied. The response variable is really a category, like success or
failure, alive or dead, passed or failed, and so on. In some problems, the ith value of
the response yi will be a count of the number of successes in mi independent trials
each with the same probability of success. If all the mi = 1, then each element of
Y has a Bernoulli distribution; if some of the mi > 1, then each element of Y has
a Binomial distribution if each of the trials has the same probability of “success”
and all trials are independent. Bernoulli regression is a special case of binomial
regression with all the mi = 1.
12.1
BINOMIAL REGRESSION
We recall the basic facts about binomial random variables. Let y be the number of
successes out of m independent trials, each with the same probability θ of success,
so y can have any integer value between 0 and m. The random variable y has
a binomial distribution. We write this as y ∼Bin(m, θ). The probability that Y
equals a speciﬁc integer j = 0, 1, . . . , m, is given by
Pr(y = j) =
 m
j

θj(1 −θ)(m−j)
(12.1)
where (m
j ) = m!/(j!(m −j)!) is the number of different orderings of j successes
in m trials. Equation (12.1) is called the probability mass function for the binomial.
The mean and variance of a binomial are
E(y) = mθ;
Var(y) = mθ(1 −θ)
(12.2)
Since m is known, both the mean and variance are determined by one parameter θ.
In the binomial regression problem, the response yi counts the number of “suc-
cesses” in mi trials, and so mi −yi of the trials were “failures.” In addition, we
have p′ terms or predictors xi possibly including a constant for the intercept, and
assume that the probability of success for the ith case is θ(xi). We write this
compactly as
(Y|X = xi) ∼Bin(mi, θ(xi)), i = 1, . . . , n
(12.3)
We use yi/mi, the observed fraction of successes at each i, as the response because
the range of yi/mi is always between 0 and 1, whereas the range of yi is between

254
LOGISTIC REGRESSION
0 and mi and can be different for each i. Using (12.2), the mean and variance
functions are
E(yi/mi|xi) = θ(xi)
(12.4)
Var(yi/mi|xi) = θ(xi)(1 −θ(xi))/mi
(12.5)
In the multiple linear regression model, the mean function and the variance function
generally have completely separate parameters, but that is not so for binomial
regression. The value of θ(xi) determines both the mean function and the variance
function, so we need to estimate θ(xi). If the mi are all large, we could simply
estimate θ(xi) by yi/mi, the observed proportion of successes at xi. In many
applications, the mi are small—often mi = 1 for all i—so this simple method will
not always work.
12.1.1
Mean Functions for Binomial Regression
As with linear regression models, we assume that θ(xi) depends on xi only through
a linear combination β′xi for some unknown β. This means that any two cases for
which β′x is equal will have the same probability of “success.” We can write θ(x)
as a function of β′x,
θ(xi) = m(β′xi)
The quantity β′xi is called the linear predictor. As in nonlinear models, the function
m is called a kernel mean function. m(β′xi) should take values in the range (0, 1)
for all β′x. The most frequently used kernel mean function for binomial regression
is the logistic function,
θ(xi) = m(β′xi) =
exp(β′xi)
1 + exp(β′xi) =
1
1 + exp(−β′xi)
(12.6)
A graph of this kernel mean function is shown in Figure 12.2. The logistic mean
function is always between 0 and 1, and has no additional parameters.
Most presentations of logistic regression work with the inverse of the kernel
mean function called the link function. Solving (12.6) for β′x, we ﬁnd
log

θ(x)
1 −θ(x)

= β′x
(12.7)
The left side of (12.7) is called a logit and the right side is the linear predictor β′x.
The logit is a linear function of the terms on the right side of (12.7). If we were
to draw a graph of log(θ(x)/(1 −θ(x))) versus β′x, we would get a straight line.
The ratio θ(x)/(1 −θ(x)) is the odds of success. For example, if the probability
of success is 0.25, the odds of success are .25/(1 −.25) = 1/3, one success to
each three failures. If the probability of success is 0.8, then the odds of success are
0.8/0.2 = 4, or four successes to one failure. Whereas probabilities are bounded

FITTING LOGISTIC REGRESSION
255
−6
−4
−2
0
2
4
6
0.0
0.2
0.4
0.6
0.8
1.0
b′x
Probability
FIG. 12.2
The logistic kernel mean function.
between 0 and 1, odds can be any nonnegative number. The logit is the logarithm
of the odds; natural logs are used in deﬁning the logit. According to equation
(12.7), the logit is equal to a linear combination of the terms.
In summary, the logistic regression model consists of the data and distribution
speciﬁed by (12.3), and a ﬁxed component that connects the response to the mean
through (12.6).
12.2
FITTING LOGISTIC REGRESSION
Many standard statistical packages allow estimation for logistic regression models.
The most common computational method is outlined in Section 12.3.2; for now,
we return to our example.
12.2.1
One-Predictor Example
Consider ﬁrst logistic regression with one predictor using the Balsam Fir data
from the BWCAW blowdown shown in Figure 12.1. The data are given in the ﬁle
blowBF.txt. The single predictor is log(D), using base-two logarithms. All the
mi = 1. We ﬁt with two terms, the intercept and log(D). The results are summarized
in Table 12.1.
TABLE 12.1
Logistic Regression Summary for the Balsam Fir
Blowdown Data
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
-7.8923
0.6301
-12.53
<2e-16
logD
2.2626
0.1907
11.86
<2e-16
---
Residual deviance: 655.24
on 657
degrees of freedom
Pearson’s X^2: 677.44
on 657
degrees of freedom

256
LOGISTIC REGRESSION
The output reports estimate ˆβ0 = −7.8923 for the intercept and ˆβ1 = 2.2626 for
the slope for log(D). The dashed curve drawn on Figure 12.1a corresponds to the
ﬁtted mean function
ˆE(Y|log(D)) =
1
1 + exp[−(−7.8923 + 2.2626log(D))]
The logistic ﬁt matches the nonparametric spline ﬁt fairly closely, except for the
largest and smallest trees where smoothers and parametric ﬁts are often in disagree-
ment. It is not always easy to tell if the logistic ﬁt is matching the data without
comparing it to a nonparametric smooth.
Equation (12.7) provides a basis for understanding coefﬁcients in logistic regres-
sion. In the example, the coefﬁcient for log(D) is about 2.26. If log(D) were
increased by one unit (since this is a base-two logarithm, increasing log(D) by one
unit means that the value of D doubles), then the natural logarithm of the odds will
increase by 2.26 and the odds will be multiplied by exp(2.26) = 9.6. Thus, a tree
with diameter 10 in. is 9.6 times as likely to blow down as a tree of diameter ﬁve
inches, or a tree of 2-in. diameter is 9.6 times as likely to blow down as a tree of
1 in. diameter. In general, if ˆβj is an estimated coefﬁcient in a logistic regression,
then if xj is increased by one unit, the odds of success, that is, the odds that Y = 1,
are multiplied by exp( ˆβj).
Table 12.1 also reports standard errors of the estimates, and the column marked
z-value shows the ratio of the estimates to their standard errors. These values can
be used for testing coefﬁcients to be zero after adjusting for the other terms in the
model, as in linear models, but the test should be compared to the standard normal
distribution rather than a t-distribution. The deviance and Pearson’s X2 reported in
the table will be discussed shortly. Since the variance of a binomial is determined
by the mean, there is not a variance parameter that can be estimated separately.
While an equivalent to an R2 measure can be deﬁned for logistic regression, its
use is not recommended.
12.2.2
Many Terms
We introduce a second predictor into the blowdown data. The variable S is a local
measure of severity of the storm that will vary from location to location, from
near 0, with very few trees effected, to near 1, with nearly all trees blown down.
Figure 12.3 shows two useful plots. Figure 12.3a gives the density for S for each of
the two values of Y. In contrast to log(D), the two density estimates are much less
nearly normal in shape, with one group somewhat skewed, and the other perhaps
bimodal, or at least very diffuse. The two densities are less clearly separated, and
this indicates that S is a weaker predictor of blowdown for these Balsam Fir trees.
Figure 12.3b is a scatterplot of S versus log(D), with different symbols for Y = 1
and for Y = 0. This particular plot would be much easier to use with different
colors indicating the two classes. In the upper-right of the plot, the symbol for
Y = 1 predominates, while in the lower-left, the symbol for Y = 0 predominates.

FITTING LOGISTIC REGRESSION
257
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0.0
0.5
1.0
1.5
2.0
S
Density
Y = 0
Y = 1
2.5
3.0
3.5
4.0
4.5
5.0
0.0
0.2
0.4
0.6
0.8
1.0
log2(D)
(a)
(b)
S
Y = 0
Y = 1
FIG. 12.3
(a) Density estimates for S for survivors, Y = 0 and blowdown, Y = 1, for the Balsam Fir
data. (b) Plot of S versus log(D) with separate symbols for points with Y = 1 and Y = 0. The values
of log(D) have been jittered.
This suggests that the two predictors have a joint effect because the prevalence of
symbols for Y = 1 changes along a diagonal in the plot. If the prevalence changed
from left to right but not down to up, then only the variable on the horizontal
axis would have an effect on the probability of blowdown. If the prevalence of
symbols for Y = 1 were uniform throughout the plot, then neither variable would
be important.
Fitting logistic regression with the two predictors log(D) and S means that the
probability of Y = 1 depends on these predictors only through a linear combination
β1log(D) + β2S for some (β1, β2). This is like ﬁnding a sequence of parallel lines
like those in Figure 12.4a so that the probability of blowdown is constant on the

258
LOGISTIC REGRESSION
log2(D)
S
2.5
3.0
3.5
4.0
4.5
5.0
0.0
0.2
0.4
0.6
0.8
1.0
log2(D)
(a)
(b)
S
2.5
3.0
3.5
4.0
4.5
5.0
0.0
0.2
0.4
0.6
0.8
1.0
FIG. 12.4
Scatterplots with contours of estimated probability of blowdown. (a) No interaction mean
function. (b) Mean function with interaction.
lines, and increases (or, in other problems, decreases) from lower left to upper
right. The lines shown on Figure 12.4a come from the logistic regression ﬁt that
is summarized in Table 12.2a. From the table, we see that all points that have the
same values of 2.2164log(D) + 4.5086S have the same estimated probability of
blowdown. For example, for the points near the line marked 0.5, we would expect
about 50% symbols for Y = 0 and 50% symbols for Y = 1, but near the line 0.1,
we would expect 90% symbols for Y = 0.
Figure 12.4b and Table 12.2b correspond to ﬁtting with a mean function that
includes the two terms and their interaction. The lines of constant estimated prob-
ability of Y = 1 shown on Figure 12.4b are now curves rather than straight lines,

FITTING LOGISTIC REGRESSION
259
TABLE 12.2
Logistic Regressions for the Balsam Fir Data
(a) No interaction
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
-9.5621
0.7499
-12.75
<2e-16 ***
logD
2.2164
0.2079
10.66
<2e-16 ***
S
4.5086
0.5159
8.74
<2e-16 ***
---
Residual deviance: 563.9
on 656
degrees of freedom
Pearson’s X^2: 715.3
on 656
degrees of freedom
(b) Mean function with interaction
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
-3.6788
1.4209
-2.589
0.00963
logD
0.4009
0.4374
0.916
0.35941
S
-11.2026
3.6143
-3.100
0.00194
logD:S
4.9098
1.1319
4.338 1.44e-05
---
Residual deviance: 541.75
on 655
degrees of freedom
Pearson’s X^2: 885.44
on 655
degrees of freedom
but otherwise the interpretation of this plot is the same as Figure 12.4a. Visually
deciding which of these two mean functions matches the data more closely is
difﬁcult, and we will shortly develop a test for this comparison.
Interpretation of estimates of parameters in mean functions with no interac-
tion works the same way with many predictors as it does with one predictor. For
example, the estimated effect of increasing log(D) by one unit using Table 12.2a is
to multiply the odds of blowdown by exp(2.2164) ≈9.2, similar to the estimated
effect when S is ignored. Interpretation of estimates is complicated by interactions.
Using the estimates in Table 12.2b, if log(D) is increased by one unit, then the
odds of blowdown are multiplied by exp(0.4009 + 4.9098S), which depends on
the value of S. For the effect of S, since S is bounded between 0 and 1, we cannot
increase S by one unit, so we can summarize the S-effect by looking at an increase
of 0.1 units. The odds multiplier for S is then exp(.1[−11.2026 + 4.9098log(D)]).
These two functions are graphed in Figure 12.5. Big trees were much more likely to
blow down in severe areas than in areas where severity was low. For ﬁxed diameter,
increasing severity by 0.1 has a relatively modest effect on the odds of blowdown.
As with logistic regression with a single term, the estimates are approximately
normally distributed if the sample size is large enough, with estimated standard
errors in Table 12.2. The ratios of the estimates to their standard errors are called
Wald tests. For the no-interaction mean function in Table 12.2a, the p-values for
all three Wald tests are very small, indicating all terms are important when adjust-
ing for the other terms in the mean function. For the interaction mean function
in Table 12.2b, the p-values for S and for the interaction are both small, but
the main effect for log(D) has a large p-value. Using the hierarchy principle,

260
LOGISTIC REGRESSION
0.0
0.2
0.4
0.6
0.8
1.0
0
50
100
150
200
S
Odds multiplier
5
10
15
20
25
30
1.0
1.5
2.0
2.5
3.0
3.5
D
(a)
(b)
Odds multiplier
FIG. 12.5
Blowdown odds multiplier for (a) doubling the diameter of a Balsam Fir tree as a function
of local severity, S, and (b) increasing S by 0.1 as a function of diameter.
however, we recommend that whenever an interaction is included in a mean
function, all the main effects in that interaction be included as well, so in light
of the signiﬁcant interaction the test for log(D) is not relevant, and we would
retain log(D).
In the next section, we derive tests analogous to F-tests for linear regression.
Unlike the linear model where F-tests and Wald tests are equivalent, in logistic
regression they can give conﬂicting results. The tests in the next section are to be
preferred over the Wald tests.
12.2.3
Deviance
In multiple linear regression, the residual sum of squares provides the basis for tests
for comparing mean functions. In logistic regression, the residual sum of squares
is replaced by the deviance, which is often called G2. The deviance is deﬁned for
logistic regression to be
G2 = 2
n

i=1

yi log
yi
ˆyi

+ (mi −yi) log
mi −yi
mi −ˆyi

(12.8)
where ˆyi = mi ˆθ(xi) are the ﬁtted number of successes in mi trials. The df asso-
ciated with the deviance is equal to the number of cases n used in the calcu-
lation minus the number of elements of β that were estimated; in the example,
df = 659 −4 = 655.
Methodology for comparing models parallels the results in Section 5.4. Write
β′x = β′
1x1 + β′
2x2, and consider testing
NH:
θ(x) = m(β′
1x1)
AH:
θ(x) = m(β′
1x1 + β′
2x2)

FITTING LOGISTIC REGRESSION
261
TABLE 12.3
Analysis of Deviance for Balsam Fir Blowdown Data
Terms
df
Deviance
Change in df
Deviance
P(> |Chi|)
1, log(D)
657
655.24
1, log(D), S, S × log(D)
655
541.75
2
113.50
0.0000
TABLE 12.4
Sequential Analysis of Deviance for Balsam Fir Blowdown Data
Terms
df
Deviance
Change in df
Deviance
P(> |Chi|)
Intercept
658
856.21
Add log(D)
657
655.24
1
200.97
0.0000
Add S
656
563.90
1
91.34
0.0000
Add interaction
655
541.75
1
22.16
0.0000
to see if the terms in x2 have zero coefﬁcients. Obtain the deviance G2
NH and degrees
of freedom dfNH under the null hypothesis, and then obtain G2
AH and dfAH under
the alternative hypothesis. As with linear models, we will have evidence against
the null hypothesis if G2
NH −G2
AH is too large. To get p-values, we compare the
difference G2
NH −G2
AH with the χ2 distribution with df = dfNH −dfAH, not with
an F-distribution as was done for linear models.
If we set x1 = (Ones, log(D)), where Ones is the vector of ones to ﬁt the
intercept, and x2 = (S, S × log(D)) to test that only log(D) and the intercept are
required in the mean function. Fitting under the null hypothesis is summarized
in Table 12.1, with G2
NH = 655.24 with dfNH = 657. The alternative hypothesis
is summarized in Table 12.2, where we see that G2
AH = 541.75 with dfAH = 655.
These results can be summarized in an analysis of deviance table, as in Table 12.3.
The interpretation of this table parallels closely the results for multiple linear regres-
sion models in Section 5.4. The table includes the deviance and df for each of the
models. The test statistic depends on the change in deviance and the change in
df, as given in the table. The p-value is 0 to 4 decimals, and the larger model is
preferred.
We could also have a longer sequence of models, for example, ﬁrst ﬁtting with
an intercept only, then the intercept and log(D), then adding S, and ﬁnally the
interaction. This would give an analysis of deviance table like Table 12.4 that
parallels the sequential analysis of variance tables discussed in Section 3.5.6. The
table displays the tests for comparing two adjacent mean functions.
12.2.4
Goodness-of-Fit Tests
When the number of trials mi > 1, the deviance G2 can be used to provide a
goodness-of-ﬁt test for a logistic regression model, essentially comparing the null
hypothesis that the mean function used is adequate versus the alternative that a
separate parameter needs to be ﬁt for each value of i (this latter case is called the

262
LOGISTIC REGRESSION
saturated model). When all the mi are large enough, G2 can be compared with the
χ2
n−p distribution to get an approximate p-value. The goodness-of-ﬁt test is not
applicable in the blowdown example because all the mi = 1.
Pearson’s X2 is an approximation to G2 deﬁned for logistic regression by
X2 =
n

i=1

(yi −ˆyi)2
 1
ˆyi
+
1
mi −ˆyi

=
n

i=1
mi(yi/mi −ˆθ(xi))2
ˆθ(xi)(1 −ˆθ(xi))
(12.9)
X2 and G2 have the same large-sample distribution and often give the same infer-
ences. In small samples, there may be differences, and sometimes X2 may be
preferred for testing goodness-of-ﬁt.
Titanic
The Titanic was a British luxury passenger liner that sank when it struck an iceberg
about 640 km south of Newfoundland on April 14–15, 1912, on its maiden voyage
to New York City from Southampton, England. Of 2201 known passengers and
crew, only 711 are reported to have survived. The data in the ﬁle titanic.txt
from Dawson (1995) classify the people on board the ship according to their Sex as
Male or Female, Age, either child or adult, and Class, either ﬁrst, second, third, or
crew. Not all combinations of the three factors occur in the data, since no children
were members of the crew. For each age/sex/class combination, the number of
people M and the number surviving Surv are also reported. The data are shown in
Table 12.5.
Table 12.6 gives the value of G2 and Pearson’s X2 for the ﬁt of ﬁve mean
functions to these data. Since almost all the mi exceed 1, we can use either G2
or X2 as a goodness-of-ﬁt test for these models. The ﬁrst two mean functions,
the main effects only model, and the main effects plus the Class × Sex interac-
tion, clearly do not ﬁt the data because the values of G2 and X2 are both much
larger then their df, and the corresponding p-values from the χ2 distribution are
TABLE 12.5
Data from the Titanic Disaster of 1912. Each Cell
Gives Surv/M, the Number of Survivors, and the Number of
People in the Cell
Female
Male
Class
Adult
Child
Adult
Child
Crew
20/23
NA
192/862
NA
First
140/144
1/1
57/175
5/5
Second
80/93
13/13
14/168
11/11
Third
76/165
14/31
75/462
13/48

BINOMIAL RANDOM VARIABLES
263
TABLE 12.6
Fit of Four Mean Functions for the Titanic Data. Each of the Mean
Functions Treats Age, Sex, and Class as Factors, and Fits Different Main Effects and
Interactions
Mean Function
df
G2
X2
Main effects only
8
112.57
103.83
Main effects + Class × Sex
5
45.90
42.77
Main effects + Class × Sex + Class × Age
3
1.69
1.72
Main effects + all two-factor interactions
2
0.00
0.00
Main effects, two-factor and three-factor interactions
0
0.00
0.00
0 to several decimal places. The third model, which adds the Class × Age inter-
action, has both G2 and X2 smaller than its df, with p-values of about 0.64,
so this mean function seems to match the data well. Adding more terms can
only reduce the value of G2 and X2, and adding the third interaction decreases
these statistics to 0 to the accuracy shown. Adding the three-factor interaction ﬁts
one parameter for each cell, effectively estimating the probability of survival by
the observed probability of survival in each cell. This will give an exact ﬁt to
the data.
The analysis of these data is continued in Problem 12.7.
12.3
BINOMIAL RANDOM VARIABLES
In this section, we provide a very brief introduction to maximum likelihood esti-
mation and then provide a computing algorithm for ﬁnding maximum likelihood
estimates for logistic regression.
12.3.1
Maximum Likelihood Estimation
Data can be used to estimate θ using maximum likelihood estimation. Suppose we
have observed y successes in m independent trials, each with the same probability
θ of success. The maximum likelihood estimate or mle of θ is the value ˆθ of θ
that maximizes the probability of observing y successes in m trials. This amounts
to rewriting (12.1) as a function of θ, with y held ﬁxed at its observed value,
L(θ) =
 m
y

θy(1 −θ)(m−y)
(12.10)
L(θ) is called the likelihood function for θ. Since the same value maximizes both
L(θ) and log(L(θ)), we work with the more convenient log-likelihood, given by
log (L(θ)) = log
 m
y

+ y log(θ) + (m −y) log(1 −θ)
(12.11)

264
LOGISTIC REGRESSION
Differentiating (12.11) with respect to θ and setting the result to 0 gives
d log(L(θ))
dθ
= y
θ −m −y
1 −θ = 0
Solving for θ gives the mle,
ˆθ = y
m = Observed number of successes
Observed ﬁxed number of trials
which is the observed proportion of successes. Although we can ﬁnd the variance
of this estimator directly, we use a result that gives the large-sample variance of
the mle for most statistical problems. Suppose the parameter θ is a vector. Then
in large samples,
Var( ˆθ) = −

E
∂2 log(L(θ))
∂θ(∂θ)′
−1
(12.12)
For the binomial example, θ is a scalar, and

−E
d2 log(L(θ))
dθ2
−1
=

−E
 y
θ2 −m −y
(1 −θ)2
−1
=

m
θ(1 −θ)
−1
= θ(1 −θ)
m
(12.13)
This variance is estimated by substituting
ˆθ for θ. In large samples, the
mle ˆθ is approximately normally distributed with mean θ and variance given
by (12.12).
12.3.2
The Log-Likelihood for Logistic Regression
Equation (12.10) provides the likelihood function for a single binomial random
variable y with m trials and probability of success θ. We generalize now to having
n independent random variables (y1, . . . , yn) with yi a binomial random variable
with mi trials and probability of success θ(xi) that depends on the value of xi and
so may be different for each i. The likelihood based on (y1, . . . , yn) is obtained
by multiplying the likelihood for each observation,
L =
n

i=1
 mi
yi

(θ(xi))yi(1 −θ(xi))mi−yi
∝
n

i=1
(θ(xi))yi(1 −θ(xi))mi−yi

GENERALIZED LINEAR MODELS
265
In the last expression, we have dropped the binomial coefﬁcients (mi
yi ) because they
do not depend on parameters. After minor rearranging, the log-likelihood is
log(L) ∝
n

i=1

yi log

θ(xi)
1 −θ(xi)

+ mi log(1 −θ(xi))

Next, we substitute for θ(xi) using equation (12.7) to get
log(L(β)) =
n

i=1

(β′xi)yi −mi log(1 + exp(β′xi))

(12.14)
The log-likelihood depends on the regression parameters β explicitly, and we can
maximize (12.14) to get estimates. An iterative procedure is required. The usual
methods using either the Newton–Raphson algorithm or Fisher scoring attain con-
vergence in just a few iterations, although problems can arise with unusual data
sets, for example, if one or more of the predictors can determine the value of the
response exactly; see Collett (2002, Section 3.12). Details of the computational
method are provided by McCullagh and Nelder (1989, Section 2.5), Collett (2002),
and Agresti (1996, 2002), among others.
The estimated covariance matrix of the estimates is given by
Var( ˆβ) = (X′ 	
WX)−1
where 	
W is a diagonal matrix with entries mi ˆθ(xi)(1 −ˆθ(xi)), and X is a matrix
with ith row x′.
12.4
GENERALIZED LINEAR MODELS
Both the multiple linear regression model discussed earlier in this book and the
logistic regression model discussed in this chapter are particular instances of a gen-
eralized
linear
model.
Generalized
linear
models
all
share
three
basic
characteristics:
1. The distribution of the response Y, given a set of terms X, is distributed
according to an exponential family distribution. The important members of
this class include the normal and binomial distributions we have already
encountered, as well as the Poisson and gamma distributions. Generalized
linear models based on the Poisson distributions are the basis of the most
common models for contingency tables of counts; see Agresti (1996, 2002).
2. The response Y depends on the terms X only through the linear combina-
tion β′X.
3. The mean E(Y|X = x) = m(β′x) for some kernel mean function m. For the
multiple linear regression model, m is the identity function, and for logistic
regression, it is the logistic function. There is considerable ﬂexibility in select-
ing the kernel mean function. Most presentations of generalized linear models
discuss the link function, which is the inverse of m rather than m itself.

266
LOGISTIC REGRESSION
These three components are enough to specify completely a regression problem
along with methods for computing estimates and making inferences. The methodol-
ogy for these models generally builds on the methods in this book, usually with only
minor modiﬁcation. Generalized linear models were ﬁrst suggested by Nelder and
Wedderburn (1972) and are discussed at length by McCullagh and Nelder (1989).
Some statistical packages use common software to ﬁt all generalized linear models,
including the multiple linear regression model. Book-length treatments of binomial
regression are given by Collett (2002) and by Hosmer and Lemeshow (2000).
PROBLEMS
12.1. Downer data For unknown reasons, dairy cows sometimes become
recumbent—they lay down. Called downers, these cows may have a seri-
ous illness that may lead to death of the cow. These data are from a study
of blood samples of over 400 downer cows studied at the Ruakura New
Zealand Animal Health Laboratory during 1983–1984. A variety of blood
tests were performed, and for many of the animals, the outcome (survived,
died, or animal was killed) was determined. The goal is to see if survival
can be predicted from the blood measurements. The variables in the data ﬁle
downer.txt are described in Table 12.7. These data were collected from
veterinary records, and not all variables were recorded for all cows.
12.1.1. Consider ﬁrst predicting Outcome from Myopathy. Find the fraction
of surviving cows of Myopathy = 0 and for Myopathy = 1.
12.1.2. Fit the logistic regression with response Outcome, and the single
predictor Myopathy. Obtain a 95% conﬁdence interval for coefﬁcient
for Myopathy, and compute the estimated decrease in odds of survival
when Myopathy = 1. Obtain the estimated probability of survival
when Myopathy = 0 and when Myopathy = 1, and compare with the
observed survival fractions in Problem 12.1.1.
TABLE 12.7
The Downer Data
Variable
n
Description
AST
429
Serum asparate amino transferase (U/l at 30C)
Calving
431
0 if measured before calving, 1 if after
CK
413
Serum creatine phosphokinase (U/l at 30C)
Daysrec
432
Days recumbent when measurements were done
Inﬂamat
136
Is inﬂammation present? 0=no, 1=yes
Myopathy
222
Is muscle disorder present? 1=yes, 0=no
PCV
175
Packed cell volume (hematocrit), percent
Urea
266
Serum urea (mmol/l)
Outcome
435
1 if survived, 0 if died or killed
Source: Clark, Henderson, Hoggard, Ellison, and Young (1987).

PROBLEMS
267
12.1.3. Next, consider the regression problem with only CK as a predic-
tor (CK is observed more often than is Myopathy, so this regres-
sion will be based on more cases than were used in the ﬁrst two
parts of this problem). Draw separate density estimates of CK, for
Outcome = 0 and for Outcome = 1. Also, draw separate density
estimates for log(CK) for the two groups. Comment on the graphs.
12.1.4. Fit the logistic regression mean function with log(CK) as the only
term beyond the intercept. Summarize results.
12.1.5. Fit the logistic mean function with terms for log(CK), Myopathy and
a Myopathy × log(CK) interaction. Interpret each of the coefﬁcient
estimates. Obtain a sequential deviance table for ﬁtting the terms
in the order given above, and summarize results. (Missing data can
cause a problem here: if your computer program requires that you ﬁt
three separate mean functions to get the analysis of deviance, then
you must be sure that each ﬁt is based on the same set of observations,
those for which CK and Myopathy are both observed.)
12.2. Starting with (12.6), prove (12.7).
12.3. Electric shocks A study carried out by R. Norell was designed to learn about
the effect of small electrical currents on farm animals, with the eventual goal
of understanding the effects of high-voltage power lines near farms. A total
of m = 70 trials were carried out at each of six intensities, 0, 1, 2, 3, 4, and
5 mA (shocks on the order of 15 mA are painful for many humans, Dalziel,
Lagen, and Thurston 1941). The data are given in the ﬁle shocks.txt
with columns Intensity, number of trials m, which is always equal to 70, and
Y, the number of trials out of m for which the response, mouth movement,
was observed.
Draw a plot of the fraction responding versus Intensity. Then, ﬁt the
logistic regression with predictor Intensity, and add the ﬁtted curve to your
plot. Test the hypothesis that the probability of response is independent of
Intensity, and summarize your conclusions. Provide a brief interpretation of
the coefﬁcient for Intensity. (Hint: The response in the logistic regression is
the number of successes in m trials. Unless the number of trials is one for
every case, computer programs will require that you specify the number of
trials in some way. Some programs will have an argument with a name like
“trials” or “weights” for this purpose. Others, like R and JMP, require that
you specify a bivariate response consisting of the number of successes Y and
the number of failures m −Y.)
12.4. Donner party In the winter of 1846–1847, about 90 wagon train emigrants
in the Donner party were unable to cross the Sierra Nevada Mountains of
California before winter, and almost half of them starved to death. The data
in ﬁle donner.txt from Johnson (1996) include some information about
each of the members of the party. The variables include Age, the age of
the person, Sex, whether male or female, Status, whether the person was a

268
LOGISTIC REGRESSION
member of a family group, a hired worker for one of the family groups, or
a single individual who did not appear to be a hired worker or a member
of any of the larger family groups, and Outcome, coded
1 if the person
survived and 0 if the person died.
12.4.1. How many men and women were in the Donner Party? What was
the survival rate for each sex? Obtain a test that the survival rates
were the same against the alternative that they were different. What
do you conclude?
12.4.2. Fit the logistic regression model with response Outcome and pre-
dictor Age, and provide an interpretation for the ﬁtted coefﬁcient
for Age.
12.4.3. Draw the graph of Outcome versus Age, and add both a smooth and a
ﬁtted logistic curve to the graph. The logistic regression curve appar-
ently does not match the data: Explain what the differences are and
how this failure might be relevant to understanding who survived
this tragedy. Fit again, but this time, add a quadratic term in Age.
Does the ﬁtted curve now match the smooth more accurately?
12.4.4. Fit the logistic regression model with terms for an intercept, Age,
Age2, Sex, and a factor for Status. Provide an interpretation for the
parameter estimates for Sex and for each of the parameter estimates
for Status. Obtain tests on the basis of the deviance for adding each
of the terms to a mean function that already includes the other terms,
and summarize the results of each of the tests via a p-value and a
one-sentence summary of the results.
12.4.5. Assuming that the logistic regression model provides an adequate
summary of the data, give a one-paragraph written summary on the
survival of members of the Donner Party.
12.5. Counterfeit banknotes The data in the ﬁle banknote.txt contains infor-
mation on 100 counterfeit Swiss banknotes with Y = 0 and 100 genuine
banknotes with Y = 1. Also included are six physical measurements of the
notes, including the Length, Diagonal and the Left and Right edges of the
note, all in millimeters, and the distance from the image to the Top edge and
Bottom edge of the paper, all in millimeters (Flury and Riedwyl, 1988). The
goal of the analysis is to estimate the probability or odds that a banknote is
counterfeit, given the values of the six measurements.
12.5.1. Draw a scatterplot matrix of six predictors, marking the points dif-
ferent colors for the two groups (genuine or counterfeit). Summarize
the information in the scatterplot matrix.
12.5.2. Use logistic regression to study the conditional distribution of y,
given the predictors.
12.6. Challenger
The ﬁle challeng.txt from Dalal, Fowlkes, and Hoadley
(1989) contains data on O-rings on 23 U. S. space shuttle missions prior

PROBLEMS
269
to the Challenger disaster of January 20, 1986. For each of the previous
missions, the temperature at take-off and the pressure of a pre-launch test
were recorded, along with the number of O-rings that failed out of six.
Use these data to try to understand the probability of failure as a function
of temperature, and of temperature and pressure. Use your ﬁtted model to
estimate the probability of failure of an O-ring when the temperature was
31◦F, the launch temperature on January 20, 1986.
12.7. Titanic Refer to the Titanic data, described in Section 12.2.4, page 262.
12.7.1. Fit a logistic regression model with terms for factors Sex, Age and
Class. On the basis of examination of the data in Table 12.5, explain
why you expect that this mean function will be inadequate to explain
these data.
12.7.2. Fit a logistic regression model that includes all the terms of the
last part, plus all the two-factor interactions. Use appropriate test-
ing procedures to decide if any of the two-factor interactions can
be eliminated. Assuming that the mean function you have obtained
matches the data well, summarize the results you have obtained by
interpreting the parameters to describe different survival rates for var-
ious factor combinations. (Hint: How does the survival of the crew
differ from the passengers? First class from third class? Males from
females? Children versus adults? Did children in ﬁrst class survive
more often than children in third class?)
12.8. BWCAW blowdown The data ﬁle blowAPB.txt contains the data for
Rich’s blowdown data, as introduced at the beginning of this chapter, but for
the two species SPP = A for aspen, and SPP = PB for paper birch.
12.8.1. Fit the same mean function used for Balsam Fir to each of these
species. Is the interaction between S and logD required for these
species?
12.8.2. Ignoring the variable S, compare the two species, using the mean
functions outlined in Section 6.2.2.
12.9. Windmill data For the windmill data in the data ﬁle wm4.txt, use the
four-site data to estimate the probability that the wind speed at the candidate
site exceeds six meters per second, and summarize your results.

Appendix
A.1
WEB SITE
The web address for material for this book is
http://www.stat.umn.edu/alr
The web site includes free text primers on how to do the computations described
in the book with several standard computer programs, all the data ﬁles described
in the book, errata for the book, and scripts for some of the packages that can
reproduce the examples in the book.
A.2
MEANS AND VARIANCES OF RANDOM VARIABLES
Suppose we let u1, u2, . . . , un be random variables and also let a0, a1, . . . , an be
n + 1 known constants.
A.2.1
E Notation
The symbol E(ui) is read as the expected value of the random variable ui. The
phrase “expected value” is the same as the phrase “mean value.” Informally, the
expected value of ui is the average value of a very large sample drawn from the
distribution of ui. If E(ui) = 0, then the average value we would get for ui if
we sampled its distribution repeatedly is 0. Since ui is a random variable, any
particular realization of ui is likely to be nonzero.
The expected value is a linear operator, which means
E(a0 + a1u1) = a0 + a1E(u1)
E

a0 +

aiui

= a0 +

aiE(ui)
(A.1)
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
270

MEANS AND VARIANCES OF RANDOM VARIABLES
271
For example, suppose that u1, . . . , un are a random sample from a population, and
E(ui) = µ, i = 1, . . . , n. The sample mean is u =  ui/n = (1/n)ui, and the
expectation of the sample mean is
E(u) = E
 1
nui

= 1
n

E(ui) = 1
n(nµ) = µ
We say that u is an unbiased estimate of the population mean µ, since its expected
value is µ.
A.2.2
Var Notation
The symbol Var(ui) is the variance of ui. The variance is deﬁned by the equation
Var(ui) = E[ui −E(ui)]2 =, the expected squared difference between an observed
value for ui and its mean value. The larger Var(ui), the more variable observed
values for ui are likely to be. The symbol σ 2 is often used for a variance, or σ 2
u
might be used for the variance of the identically distributed ui if several variances
are being discussed.
The general rule for the variance of a sum of uncorrelated random variables is
Var

a0 +

aiui

=

a2
i Var(ui)
(A.2)
The a0 term vanishes because the variance of a0 + u is the same as the variance of
u since the variance of a constant is 0. Assuming that Var(ui) = σ 2, we can ﬁnd
the variance of the sample mean of independently, identically distributed ui:
Var(u) = Var
 1
nui

= 1
n2

E(ui) = 1
n2 (nσ 2) = σ 2
n
A.2.3
Cov Notation
The symbol Cov(ui, uj) is read as the covariance between the random variables
ui and uj and is deﬁned by the equation
Cov(ui, uj) = E

ui −E(ui))(uj −E(uj)
	
= Cov(uj, ui)
The covariance describes the way two random variables vary jointly. If the two
variables are independent, then Cov(ui, uj) = 0, but zero correlation does not imply
independence. The variance is a special case of covariance, since Cov(ui, ui) =
Var(ui). The rule for covariance is
Cov(a0 + a1u1, a3 + a2u2) = a1a2Cov(u1, u2)

272
APPENDIX
The correlation coefﬁcient is deﬁned by
ρ(ui, uj) =
Cov(ui, uj)
Var(ui)Var(uj)
The correlation does not depend on units of measurement and has a value between
−1 and 1.
The general form for the variance of a linear combination of correlated random
variables is
Var

a0 +

aiui

=
n

i=1
a2
i Var(ui) + 2
n−1

i=1
n

j=i+1
aiajCov(ui, uj)
(A.3)
A.2.4
Conditional Moments
Throughout the book, we use notation like E(Y|X = x) to denote the mean of the
random variable Y in the population for which the value of X is ﬁxed at the value
X = x. Similarly, Var(Y|X = x) is the variance of the random variable Y in the
population for which X is ﬁxed at X = x.
There are simple relationships between the conditional mean and variance of Y
given X and the unconditional mean and variances (see, for example, Casella and
Berger, 1990):
E(Y) = E[E(Y|X = x)]
(A.4)
Var(Y) = E[Var(Y|X = x)] + Var(E(Y|X = x)]
(A.5)
For example, suppose that when we condition on the predictor X we have a
simple linear regression mean function with constant variance, E(Y|X = x) =
β0 + β1x, Var(Y|X = x) = σ 2. In addition, suppose the unconditional moments of
the predictor are E(X) = µx and Var(X) = τ 2
x . Then for the unconditional random
variable Y,
E(Y) = E[E(Y|X = x)]
= E[β0 + β1x]
= β0 + β1µx
Var(Y) = E[Var(Y|X = x)] + Var[E(Y|X = x)]
= E[σ 2] + Var[β0 + β1x]
= σ 2 + β2
1τ 2
x
The mean of the unconditional variable Y is obtained by substituting the mean of the
unconditional variable X into the conditional mean formula, and the unconditional
variance of Y equals the conditional variance plus a second quantity that depends
on both β2
1 and on τ 2
x .

MEANS AND VARIANCES OF LEAST SQUARES ESTIMATES
273
A.3
LEAST SQUARES FOR SIMPLE REGRESSION
The ols estimates of β0 and β1 in simple regression are the values that minimize
the residual sum of squares function,
RSS(β0, β1) =
n

i=1
(yi −β0 −β1xi)2
(A.6)
One method of ﬁnding the minimizer is to differentiate with respect to β0 and β1,
set the derivatives equal to 0, and solve
∂RSS(β0, β1)
β0
= −2
n

i=1
(yi −β0 −β1xi) = 0
∂RSS(β0, β1)
β1
= −2
n

i=1
xi(yi −β0 −β1xi) = 0
Upon rearranging terms, we get
β0n + β1
 xi =  yi
β0
 xi + β1
 x2
i =  xiyi
(A.7)
Equations (A.7) are called the normal equations for the simple linear regression
model (2.1). The normal equations depend on the data only through the sufﬁcient
statistics  xi,  yi,  x2
i and  xiyi. Using the formulas
SXX =
(xi −x)2
=  x2
i −nx2
SXY = (xi −x)(yi −y) =  xiyi −nxy
(A.8)
equivalent and numerically more stable sufﬁcient statistics are given by x, y, SXX
and SXY. Solving (A.7), we get
ˆβ0 = y −ˆβ1x,
ˆβ1 = SXY
SXX
(A.9)
A.4
MEANS AND VARIANCES OF LEAST SQUARES ESTIMATES
The least squares estimates are linear combinations of the observed values
y1, . . . , yn of the response, so we can apply the results of Appendix A.2 to the
estimates found in Appendix A.3 to get the means, variances, and covariances of
the estimates. Assume the simple regression model (2.1) is correct. The estimator ˆβ1
given at (A.9) can be written as ˆβ1 =  ciyi, where for each i, ci = (xi −x)/SXX.

274
APPENDIX
Since we are conditioning on the values of X, the ci are ﬁxed numbers. By (A.1),
E( ˆβ1|X) = E

ciyi|X = xi

=

ciE(yi|X = xi)
=

ci(β0 + β1xi)
= β0

ci + β1

cixi
By direct summation,  ci = 0 and  cixi = 1, giving
E( ˆβ1|X) = β1
which shows that ˆβ1 is unbiased for β1. A similar computation will show that
ˆβ0 = β0.
Since the yi are assumed independent, the variance of ˆβ1 is found by an appli-
cation of (A.2),
Var( ˆβ1|X) = Var

ciyi|X = xi

=

c2
i Var(Y|X = xi)
= σ 2 
c2
i
= σ 2/SXX
This computation also used  c2
i = (xi −x)2/SXX2 = 1/SXX. Computing the
variance of ˆβ0 requires an application of (A.3). We write
Var( ˆβ0) = Var(y −ˆβ1x|X)
= Var(y|X) + x2Var( ˆβ1|X) −2xCov(y, ˆβ1|X)
(A.10)
To complete this computation, we need to compute the covariance,
Cov(y, ˆβ1|X) = Cov
1
n

yi,

ciyi

= 1
n

ciCov(yi, yi)
= σ 2
n

ci
= 0

ESTIMATING E(Y|X) USING A SMOOTHER
275
because the yi are independent, and  ci = 0. Substituting into (A.10) and
simplifying,
Var( ˆβ0) = σ 2

1
n + x2
SXX

Finally,
Cov( ˆβ0, ˆβ1|X) = Cov(y −ˆβ1x, ˆβ1|X)
= Cov(y, ˆβ1) −xCov( ˆβ1, ˆβ1)
= 0 −σ 2 x
SXX
= −σ 2 x
SXX
Further application of these results gives the variance of a ﬁtted value,
ˆy = ˆβ0 + ˆβ1x:
Var( ˆy|X = x) = Var( ˆβ0 + ˆβ1x|X = x)
= Var( ˆβ0|X = x) + x2Var( ˆβ1|X = x) + 2xCov( ˆβ0, ˆβ1|X = x)
= σ 2

1
n + x2
SXX

+ σ 2x2 1
SXX −2σ 2x x
SXX
= σ 2
1
n + (x −x)2
SXX

(A.11)
A prediction ˜y∗at the future value x∗is just ˆβ0 + ˆβ1x∗. The variance of a prediction
consists of the variance of the ﬁtted value at x∗given by (A.11) plus σ 2, the variance
of the error that will be attached to the future value,
Var( ˜y∗|X = x∗) = σ 2
1
n + (x −x)2
SXX

+ σ 2
as given by (2.25).
A.5
ESTIMATING E(Y|X) USING A SMOOTHER
For a 2D scatterplot of Y versus X, a scatterplot smoother provides an estimate of
the mean function E(Y|X = x) as x varies, without making parametric assumptions
about the mean function, so we do not need to assume that the mean function is a
straight line or any other particular form. We very brieﬂy introduce one of many
types of local smoothers and provide references to other approaches to smoothing.

276
APPENDIX
The smoother we use most often in this book is the simplest case of the loess
smoother (Cleveland, 1979; see also the ﬁrst step in Algorithm 6.1.1 in H¨ardle,
1990, p. 192). This smoother estimates E(Y|X = xg) by ˜yg at the point xg via a
weighted least squares simple regression, giving more weight to points close to xg
than to points distant from xg. Here is the method:
1. Select a value for a smoothing parameter f , a number between 0 and 1.
Values of f close to 1 will give curves that are too smooth and will be close
to a straight line, while small values of f give curves that are too rough
and match all the wiggles in the data. The value of f must be chosen to
balance the bias of oversmoothing with the variability of undersmoothing.
Remarkably, for many problems, f ≈2/3 is a good choice. There is a sub-
stantial literature on the appropriate ways to estimate a smoothing parameter
for loess and for other smoothing methods, but for the purposes of using a
smoother to help us look at a graph, optimal choice of a smoothing parameter
is not critical.
2. Find the f n closest points to xg. For example, if n = 100, and f = 0.6, then
ﬁnd the f n = 60 closest points to xg. Every time the value of xg is changed,
the points selected may change.
3. Among these f n nearest neighbors to xg, compute the wls estimates for
the simple regression of Y on X, with weights determined so that points
close to xg have the highest weight, and the weights decline toward 0 for
points farther from xg. We use a triangular weight function that gives max-
imum weight to data at xg, and weights that decrease linearly to 0 at the
edge of the neighborhood. If a different weight function is used, answers are
somewhat different.
4. The value of ˜yg is the ﬁtted value at xg from the wls regression using the
nearest neighbors found at step 2 as the data, and the weights from step 3
as weights.
5. Repeat 1–4 for many values of xg that form a grid of points that cover the
interval on the x-axis of interest. Join the points.
Figure A.1 shows a plot of Y versus X, along with four smoothers. The ﬁrst
smoother is the ols simple regression line, which does not match the data well
because the mean function for the data in this ﬁgure is probably curved, not straight.
The loess smooth with f = 0.1 is as expected very wiggly, matching the local
variation rather than the mean. The line for f = 2/3 seems to match the data very
well, while the loess ﬁt for f = .95 is nearly the same as for f = 2/3, but it tends
toward oversmoothing and attempts to match the ols line. We would conclude from
this graph that a straight-line mean function is likely to be inadequate because it
does not match the data very well. Loader (2004) discusses a formal lack-of-ﬁt
test on the basis of comparing parametric and nonparametric estimates of the mean
function that is presented in Problem 5.3.
The loess smoother is an example of a nearest neighbor smoother. Local poly-
nomial regression smoothers and kernel smoothers are similar to loess, except they

ESTIMATING E(Y|X) USING A SMOOTHER
277
200
400
600
800
1000
100
150
200
250
300
350
400
Dbh
Height
f = .1
f = 2/3
f = .95
OLS
FIG. A.1
Three choices of the smoothing parameter for a loess smooth. The data used in this plot are
discussed in Section 7.1.2.
give positive weight to all cases within a ﬁxed distance of the point of interest
rather than a ﬁxed number of points.
There is a large literature on nonparametric regression, for which scatterplot
smoothing is a primary tool. Recent reference on this subject include Simonoff
(1996), Bowman and Azzalini (1997), and Loader (2004).
The literature on estimating a variance function from a scatterplot is much
smaller than the literature on estimating the mean (but see Ruppert, Wand, Holst
and H¨ossjer, 1997). Here is a simple algorithm that can produce a smoother that
estimates the standard deviation function, which is the square root of the variance
function:
1. Smooth the yi on the xi to get an estimate say ˜yi for each value of X = xi.
Compute the squared residuals, ri = (yi −˜yi)2. Under normality of errors,
the expectation E(ri|xi) = Var(Y|X = xi), so a mean smooth for the squared
residuals estimates the variance smooth for Y.
2. Smooth the ri on xi to estimate Var(Y|X = xi) by s2
i at each value xi. Then
s2
i is the smoothed estimate of the variance, and si is a smoothed estimate
of the standard deviation.
3. Add three lines to the scatterplot: The mean smooth (xi, ˜yi), the mean smooth
plus one standard deviation, (xi, ˜yi + si) and the mean smooth minus one
standard deviation, (xi, ˜yi −si).

278
APPENDIX
200
400
600
800
1000
100
150
200
250
300
350
400
Dbh
Height
FIG. A.2
loess smooth for the mean function, solid line, and mean ± one standard deviation, dashed
lines.
Figure A.2 shows the loess smooth for the mean function and the mean plus and
minus one standard deviation for the same data as in Figure A.1. The variability
appears to be a bit larger in the middle of the range than at the edges.
A.6
A BRIEF INTRODUCTION TO MATRICES AND VECTORS
We provide only a brief introduction to matrices and vectors. More complete ref-
erences include Graybill (1969), Searle (1982), Schott (1996), or any good linear
algebra book.
Boldface type is used to indicate matrices and vectors. We will say that X is an
r × c matrix if it is an array of numbers with r rows and c columns. A speciﬁc
4 × 3 matrix X is
X =


1
2
1
1
1
5
1
3
4
1
8
6

=


x11
x12
x13
x21
x22
x23
x31
x32
x33
x41
x42
x43

=

xij
	
(A.12)
The element xij of X is the number in the ith row and the jth column. For example,
in the preceding matrix, x32 = 3.

A BRIEF INTRODUCTION TO MATRICES AND VECTORS
279
A vector is a matrix with just one column. A speciﬁc 4 × 1 matrix y, which is
a vector of length 4, is given by
y =


2
3
−2
0

=


y1
y2
y3
y4


The elements of a vector are generally singly subscripted; thus, y3 = −2. A row
vector is a matrix with one row. We do not use row vectors in this book. If a
vector is needed to represent a row, a transpose of a column vector will be used
(see below).
A square matrix has the same number of rows and columns, so r = c. A square
matrix Z is symmetric if zij = zji for all i and j. A square matrix is diagonal if
all elements off the main diagonal are 0, zij = 0, unless i = j. The matrices C and
D below are symmetric and diagonal, respectively:
C =


7
3
2
1
3
4
1
−1
2
1
6
3
1
−1
3
8

D =


7
0
0
0
0
4
0
0
0
0
6
0
0
0
0
8


The diagonal matrix with all elements on the diagonal equal to 1 is called the
identity matrix, for which the symbol I is used. The 4 × 4 identity matrix is
I =


1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1


A scalar is a 1 × 1 matrix, an ordinary number.
A.6.1
Addition and Subtraction
Two matrices can be added or subtracted only if they have the same number of
rows and columns. The sum C = A + B of r × c matrices is also r × c. Addition
is done elementwise:
C = A + B =


a11
a12
a21
a22
a31
a32

+


b11
b12
b21
b22
b31
b32

=


a11 + b11
a12 + b12
a21 + b21
a22 + b22
a31 + b31
a32 + b32


Subtraction works the same way, with the “+” signs changed to “−” signs. The
usual rules for addition of numbers apply to addition of matrices, namely commu-
tativity, A + B = B + A, and associativity, (A + B) + C = A + (B + C).

280
APPENDIX
A.6.2
Multiplication by a Scalar
If k is a number and A is an r × c matrix with elements (aij), then kA is an r × c
matrix with elements (kaij ). For example, the matrix σ 2I has all diagonal elements
equal to σ 2 and all off-diagonal elements equal to 0.
A.6.3
Matrix Multiplication
Multiplication of matrices follows rules that are more complicated than are the
rules for addition and subtraction. For two matrices to be multiplied together in
the order AB, the number of columns of A must equal the number of rows of B.
For example, if A is r × c, and B is c × q, then C = AB is r × q. If the elements
of A are (aij) and the elements of B are (bij ), then the elements of C = (cij ) are
given by the formula
cij =
c

k=1
aikbkj
This formula says that cij is formed by taking the ith row of A and the jth column
of B, multiplying the ﬁrst element of the speciﬁed row in A by the ﬁrst element
in the speciﬁed column in B, multiplying second elements, and so on, and then
adding the products together.
If A is 1 × c and B is c × 1, then the product AB is 1 × 1, an ordinary number.
For example, if A and B are
A = (1
3
2
−1)
B =


2
1
−2
4


then the product AB is
AB = (1 × 2) + (3 × 1) + (2 × −2) + (−1 × 4) = −3
AB is not the same as BA. For the preceding matrices, the product BA will be a
4 × 4 matrix:
BA =


2
6
4
−2
1
3
2
−1
−2
−6
−4
2
4
12
8
−4


The following small example illustrates what happens when all the dimensions
are bigger than 1. A 3 × 2 matrix A times a 2 × 2 matrix B is given as


a11
a12
a21
a22
a31
a32


 b11
b12
b21
b22

=


a11b11 + a12b21
a11b12 + a12b22
a21b11 + a22b21
a21b12 + a22b22
a31b11 + a32b21
a31b12 + a32b22



A BRIEF INTRODUCTION TO MATRICES AND VECTORS
281
Using numbers, an example of multiplication of two matrices is


3
1
−1
0
2
2


 5
1
0
4

=


15 + 0
3 + 4
−5 + 0
−1 + 0
10 + 0
2 + 8

=


15
4
−5
−1
10
10


In this example, BA is not deﬁned because the number of columns of B is not equal
to the number of rows of A. However, the associative law holds: If A is r × c, B
is c × q, and C is q × p, then A(BC) = (AB)C, and the result is an r × p matrix.
A.6.4
Transpose of a Matrix
The transpose of an r × c matrix X is a c × r matrix called X′ such that if the
elements of X are (xij ), then the elements of X′ are (xji). For the matrix X given
at (A.12),
X′ =


1
1
1
1
2
1
3
8
1
5
4
6


The transpose of a column vector is a row vector. The transpose of a product (AB)′
is the product of the transposes, in opposite order, so (AB)′ = B′A′.
Suppose that a is an r × 1 vector with elements a1, . . . , ar. Then the product
a′a will be a 1 × 1 matrix or scalar, given by
a′a = a2
1 + a2
2 + · · · + a2
r =
r

i=1
a2
i
(A.13)
Thus, a′a provides a compact notation for the sum of the squares of the elements
of a vector a. The square root of this quantity (a′a)1/2 is called the norm or length
of the vector a. Similarly, if a and b are both r × 1 vectors, then we obtain
a′b = a1b1 + a2b2 + · · · + anbn =
r

i=1
aibi =
r

i=1
biai = b′a
The fact that a′b = b′a is often quite useful in manipulating the vectors used in
regression calculations.
Another useful formula in regression calculations is obtained by applying the
distributive law
(a −b)′(a −b) = a′a + b′b −2a′b
(A.14)
A.6.5
Inverse of a Matrix
For any scalar c ̸= 0, there is another number called the inverse of c, say d, such
that the product cd = 1. For example, if c = 3, then d = 1/c = 1/3, and the inverse

282
APPENDIX
of 3 is 1/3. Similarly, the inverse of 1/3 is 3. The number 0 does not have an
inverse because there is no other number d such that 0 × d = 1.
Square matrices can also have an inverse. We will say that the inverse of a
matrix C is another matrix D, such that CD = I, and we write D = C−1. Not all
square matrices have an inverse. The collection of matrices that have an inverse are
called full rank, invertible, or nonsingular. A square matrix that is not invertible
is of less than full rank, or singular. If a matrix has an inverse, it has a unique
inverse.
The inverse is easy to compute only in special cases, and its computation in
general can require a very tedious calculation that is best done on a computer.
High-level matrix and statistical languages such as Matlab, Maple, Mathematica,
R and S-plus include functions for inverting matrices, or returning an appropriate
message if the inverse does not exist.
The identity matrix I is its own inverse. If C is a diagonal matrix, say
C =


3
0
0
0
0
−1
0
0
0
0
4
0
0
0
0
1


then C−1 is the diagonal matrix
C =


1
3
0
0
0
0
−1
0
0
0
0
1
4
0
0
0
0
1


as can be veriﬁed by direct multiplication. For any diagonal matrix with nonzero
diagonal elements, the inverse is obtained by inverting the diagonal elements. If
any of the diagonal elements are 0, then no inverse exists.
A.6.6
Orthogonality
Two vectors a and b of the same length are orthogonal if a′b = 0. An r × c matrix
Q has orthonormal columns if its columns, viewed as a set of c ≤r different
r × 1 vectors, are orthogonal and in addition have length 1. This is equivalent to
requiring that Q′Q = I, the r × r identity matrix. A square matrix A is orthogonal
if A′A = AA′ = I, and so A−1 = A′. For example, the matrix
A =


1
√
3
1
√
2
1
√
6
1
√
3
0
−2
√
6
1
√
3
−1
√
2
1
√
6



RANDOM VECTORS
283
can be shown to be orthogonal by showing that A′A = I, and therefore
A−1 = A′ =


1
√
3
1
√
3
1
√
3
1
√
2
0
−1
√
2
1
√
6
−2
√
6
1
√
6


A.6.7
Linear Dependence and Rank of a Matrix
Suppose we have a n × p matrix X, with columns given by the vectors x1, . . . , xp;
we consider only the case p ≤n. We will say that x1, . . . , xp are linearly dependent
if we can ﬁnd multipliers a1, . . . , ap, not all of which are 0, such that
p

i=1
aixi = 0
(A.15)
If no such multipliers exist, then we say that the vectors are linearly independent,
and the matrix is full rank. In general, the rank of a matrix is the maximum number
of xi that form a linearly independent set.
For example, the matrix X given at (A.12) can be shown to have linearly inde-
pendent columns because no ai not all equal to zero can be found that satisfy
(A.15). On the other hand, the matrix
X =


1
2
5
1
1
4
1
3
6
1
8
11

= (x1, x2, x3)
(A.16)
has linearly dependent columns and is singular because x3 = 3x1 + x2, or 3x1 +
x2 −x3 = 0. This matrix is of rank two because the linearly independent subset of
the columns with the most elements, consisting of any two of the three columns,
has two elements.
The matrix X′X is a p × p matrix. If X has rank p, so does X′X. Full-rank
square matrices always have an inverse. Square matrices of less than full rank
never have an inverse.
A.7
RANDOM VECTORS
An n × 1 vector Y is a random vector if each of its elements is a random variable.
The mean of an n × 1 random vector Y is also an n × 1 vector whose elements
are the means of the elements of Y. The variance of an n × 1 vector Y is an n × n
square symmetric matrix, often called a covariance matrix, written Var(Y) with
Var(yi) as its (i, i) element and Cov(yi, yj) = Cov(yj, yi) as both the (i, j) and
(j, i) element.

284
APPENDIX
The rules for means and variances of random vectors are matrix equivalents
of the scalar versions in Appendix A.2. If a0 is a vector of constants, and A is a
matrix of constants,
E(a0 + AY) = a0 + AE(Y)
(A.17)
Var(a0 + AY) = AVar(Y)A′
(A.18)
A.8
LEAST SQUARES USING MATRICES
The multiple linear regression model can be written as
E(Y|X = x) = β′x
Var(Y|X = x) = σ 2
In matrix terms, we will write the model using errors as
Y = Xβ + e
where Y is the n × 1 vector of response values and X is a n × p′ matrix. If the
mean function includes an intercept, then the ﬁrst column of X is a vector of ones,
and p′ = p + 1. If the mean function does not include an intercept, then the column
of one is not included in X and p′ = p. The ith row of the n × p′ matrix X is x′
i,
β is a p′ × 1 vector of parameters for the mean function, e is the n × 1 vector of
unobservable errors, and σ 2 is an unknown positive constant.
The ols estimate ˆβ of β is given by the arguments that minimize the residual
sum of squares function,
RSS(β) = (Y −Xβ)′(Y −Xβ)
Using (A.14), we obtain
RSS(β) = Y′Y + β′(X′X)β −2Y′Xβ
(A.19)
RSS(β) depends on only three functions of the data: Y′Y, X′X, and Y′X. Any
two data sets that have the same values of these three quantities will have the
same least squares estimates. Using (A.8), the information in these quantities is
equivalent to the information contained in the sample means of the terms plus the
sample covariances of the terms and the response.
To minimize (A.19), differentiate with respect to β and set the result equal to
0. This leads to the matrix version of the normal equations,
X′Xβ = X′Y
(A.20)
The ols estimates are any solution to these equations. If the inverse of (X′X) exists,
as it will if the columns of X are linearly independent, the ols estimates are unique
and are given by
ˆβ = (X′X)−1X′Y
(A.21)

LEAST SQUARES USING MATRICES
285
If the inverse does not exist, then the matrix (X′X) is of less than full rank, and
the ols estimate is not unique. In this case, most computer programs will use a
linearly independent subset of the columns of X in ﬁtting the model, so that the
reduced model matrix does have full rank. This is discussed in Section 4.1.4.
A.8.1
Properties of Estimates
Using the rules for means and variances of random vectors, (A.17) and (A.18), we
ﬁnd
E( ˆβ|X) = E((X′X)−1X′Y|X)
= (X′X)−1X′E(Y|X)
= (X′X)−1X′Xβ
= β
(A.22)
so ˆβ is unbiased for β, as long as the mean function that was ﬁt is the true mean
function. The variance of ˆβ is
Var( ˆβ|X) = Var((X′X)−1X′Y|X)
= (X′X)−1X′ [Var(Y|X)] X(X′X)−1
= (X′X)−1X′ 
σ 2I

X(X′X)−1
= σ 2(X′X)−1X′X(X′X)−1
= σ 2(X′X)−1
(A.23)
The variances and covariances are compactly determined as σ 2 times a matrix
whose elements are determined only by X and not by Y.
A.8.2
The Residual Sum of Squares
Let ˆY = X ˆβ be the n × 1 vector of ﬁtted values corresponding to the n cases in the
data, and ˆe = Y −ˆY is the vector of residuals. One representation of the residual
sum of squares, which is the residual sum of squares function evaluated at ˆβ, is
RSS = (Y −ˆY)′(Y −ˆY) = ˆe′ˆe =
n

i=1
ˆe2
i
which suggests that the residual sum of squares can be computed by squaring the
residuals and adding them up. In multiple linear regression, it can also be computed
more efﬁciently on the basis of summary statistics. Using (A.19) and the summary
statistics X′X, X′Y and Y′Y, we write
RSS = RSS( ˆβ) = Y′Y + ˆβ′X′X ˆβ −2Y′X ˆβ

286
APPENDIX
We will ﬁrst show that ˆβ′X′X ˆβ = Y′X ˆβ. Substituting for one of the ˆβs, we get
ˆβ′X′X(X′X)−1X′Y = ˆβ′X′Y = Y′X ˆβ
the last result following because taking the transpose of a 1 × 1 matrix does not
change its value. The residual sum of squares function can now be rewritten as
RSS = Y′Y −ˆβ′X′X ˆβ
= Y′Y −ˆY′ ˆY
where ˆY = X ˆβ are the ﬁtted values. The residual sum of squares is the difference
in the squares of the lengths of the two vectors Y and ˆY. Another useful form for
the residual sum of squares is
RSS = SYY(1 −R2)
where R2 is the square of the sample correlation between ˆY and Y.
A.8.3
Estimate of Variance
Under the assumption of constant variance, the estimate of σ 2 is
ˆσ 2 = RSS
d
(A.24)
with d df, where d is equal to the number of cases n minus the number of terms with
estimated coefﬁcients in the model. If the matrix X is of full rank, then d = n −p′,
where p′ = p for mean functions without an intercept, and p′ = p + 1 for mean
functions with an intercept. The number of estimated coefﬁcients will be less than
p′ if X is not of full rank.
A.9
THE QR FACTORIZATION
Most of the formulas given in this book are convenient for derivations but can be
inaccurate when used on a computer because inverting a matrix such as (X′X)
leaves open the possibility of introducing signiﬁcant rounding errors into cal-
culations. Most statistical packages will use better methods of computing, and
understanding how they work is useful.
We start with the basic n × p′ matrix X of terms. Suppose we could ﬁnd an n ×
p′ matrix Q and a p′ × p′ matrix R such that (1) X = QR; (2) Q has orthonormal
columns, meaning that Q′Q = Ip′ and (3) R is an upper triangular matrix, meaning
that all the entries in R below the diagonal are equal to 0, but those on or above
the diagonal can be nonzero.

MAXIMUM LIKELIHOOD ESTIMATES
287
Using the basic properties of matrices, we can write
X = QR
X′X = (QR)′(QR) = R′R
(X′X)−1 = (R′R)−1 = R−1(R′)−1
(A.25)
ˆβ = X(X′X)−1X′Y = R−1(Q′Y)
(A.26)
H = X(X′X)−1X′ = QQ′
(A.27)
Equation (A.25) follows because R is a square matrix, and the inverse of the
product of square matrices is the product of the inverses in opposite order. From
(A.26), to compute ˆβ, ﬁrst compute Q′Y, which is a p′ × 1 vector, and multiply
on the left by R to get
R ˆβ = Q′Y
(A.28)
This last equation is very easy to solve because R is a triangular matrix and so we
can use backsolving. For example, to solve the equations


7
4
2
0
2
1
0
0
1

ˆβ =


3
2
1


ﬁrst solve the last equation, so ˆβ3 = 1, substitute into the equation above it, so
2 ˆβ2 + 1 = 2, so ˆβ2 = 1/2. Finally, the ﬁrst equation is 7 ˆβ1 + 2 + 2 = 3, so ˆβ3 =
−1/7.
Equation (A.27) shows how the elements of the n × n hat matrix H can be
computed without inverting a matrix and without using all the storage needed to
save H in full. If qi is the ith column of Q, then an element hij of the H matrix
is simply computed as hij = q′
iqj.
Golub and Van Loan (1996) provide a complete treatment on computing and
using the QR factorization. Very high quality computer code for computing this
and related quantities for statistics is provided in the publicly available Lapack
package, described on the internet at www.netlib.org/lapack/lug/. This code is also
used in many standard statistical packages.
A.10
MAXIMUM LIKELIHOOD ESTIMATES
Maximum likelihood estimation is probably the most frequently used method of
deriving estimates in statistics. A general treatment is given by Casella and Berger
(1990, Section 7.2.2); here we derive the maximum likelihood estimates for the
linear regression model assuming normality, without proof or much explanation.
Our goal is to establish notation and deﬁne quantities that will be used in the

288
APPENDIX
discussion of Box–Cox transformations, and estimation for generalized linear mod-
els in Chapter 12.
The normal multiple linear regression model speciﬁes for the ith observation
that
(yi|xi) ∼N(β′xi, σ 2)
Given this model, the density for the ith observation yi is the normal density
function,
fyi(yi|xi, β, σ 2) =
1
√
2πσ
exp

−(yi −β′xi)2
2σ 2

Assuming the observations are independent, the likelihood function is just the
product of the densities for each of the n observations, viewed as a function of the
parameters with the data ﬁxed rather than a function of the data with the parameters
ﬁxed:
L(β, σ 2|Y) =
n

i=1
fyi(yi|xi, β, σ 2)
=
n

i=1
1
√
2πσ
exp

−(yi −β′xi)2
2σ 2

=

1
√
2πσ
n
exp

−1
σ 2
n

i=1
(yi −β′xi)2

The maximum likelihood estimates are simply the values of β and σ 2 that maximize
the likelihood function.
The values that maximize the likelihood will also maximize the logarithm of
the likelihood
log

L(β, σ 2|Y)

= −n
2 log(2π) −n
2 log(σ 2) −
1
2σ 2
n

i=1
(yi −β′xi)2
(A.29)
The log-likelihood function (A.29) is a sum of three terms. Since β is included
only in the third term and this term has a negative sign in front of it, we recognize
that maximizing the log-likelihood over β is the same as minimizing the third term,
which, apart from constants, is the same as the residual sum of squares function
(see Section 3.4.3). We have just shown that the maximum likelihood estimate of
β for the normal linear regression problem is the same as the ols estimator. Fixing
β at the ols estimator ˆβ, (A.29) becomes
log

L( ˆβ, σ 2|Y)

= −n
2 log(2π) −n
2 log(σ 2) −
1
2σ 2 RSS
(A.30)

THE BOX–COX METHOD FOR TRANSFORMATIONS
289
and differentiating (A.30) with respect to σ 2 and setting the result to 0 gives the
maximum likelihood estimator for σ 2 as RSS/n, the same estimate we have been
using, apart from division by n rather than n −p′.
Maximum likelihood estimation has many important properties that make them
useful. These estimates are approximately normally distributed in large samples,
and the large-sample variance achieves the lower bound for the variance of all
unbiased estimates.
A.11
THE BOX–COX METHOD FOR TRANSFORMATIONS
A.11.1
Univariate Case
Box and Cox (1964) derived the Box–Cox method for selecting a transformation
using a likelihood-like method. They supposed that, for some value of λ, ψM(Y, λ)
given by (7.6), page 153, is normally distributed. With n independent observations,
therefore, the log-likelihood function for (β, σ 2, λ) is given by (A.29), but with yi
replaced by ψM(Y, λ)1,
log

L(β, σ 2, λ|Y)

= −n
2 log(2π) −n
2 log(σ 2) −
1
2σ 2
n

i=1
(ψM(yi, λ) −β′xi)2
(A.31)
For a ﬁxed value of λ, (A.31) is the same as (A.29), and so the maximum likelihood
estimates for β and σ 2 are obtained from the regression of ψM(Y, λ) on X, and
the value of the log-likelihood evaluated at these estimates is
log

L(β(λ), σ 2(λ), λ|Y)

= −n
2 log(2π) −n
2 log(RSS(λ)/n) −n
2
(A.32)
where RSS(λ) is the residual sum of squares in the regression of ψM(Y, λ) on X,
as deﬁned in Section 7.1.4. Only the second term in (A.32) involves data, and so
the global maximum likelihood estimate of λ minimizes RSS(λ).
Standard likelihood theory can be applied to get a (1 −α) × 100% conﬁdence
interval for λ to be the set

λ
2

log(L(β(ˆλ), σ 2(ˆλ), ˆλ|Y)) −log(L(β(λ), σ 2(λ), λ|Y))

< χ2(1, 1 −α)

Or, setting α = .05 so χ2(1, .95) = 3.84, and using (A.32)

λ|(n/2)(log(RSS(λ)) −log(RSS(ˆλ)) < 1.92

(A.33)
1As λ is varied, the units of ψM(Y, λ) can change, and so the joint density of the transformed data should
require a Jacobian term; see Casella and Berger (1990, Section 4.3). The modiﬁed power transformations
are deﬁned so the Jacobian of the transformation is always equal to 1, and it can therefore be ignored.

290
APPENDIX
Many statistical packages will have routines that will provide a graph of RSS(λ)
versus λ, or of (n/2) log(RSS(λ)) versus λ as shown in Figure 7.8, for the highway
accident data. Equation (A.32) shows that the conﬁdence interval for λ includes
all values of λ for which the log-likelihood is within 1.92 units of the maximum
value of the log-likelihood, or between the two vertical lines in the ﬁgure.
A.11.2
Multivariate Case
Although the material in this section uses more mathematical statistics than most
of this book, it is included because the details of computing the multivariate exten-
sion of Box–Cox transformations are not published elsewhere. The basic idea was
proposed by Velilla (1993).
Suppose X is a set of p variables we wish to transform and deﬁne
ψM(X, λ) = (ψM(X1, λ1), . . . , ψM(Xk, λk))
Here, we have used the modiﬁed power transformations (7.6) for each element of
X, but the same general idea can be applied using other transformations such as the
Yeo–Johnson family introduced in Section 7.4. In analogy to the univariate case,
we assume that for some λ, we will have
ψM(X, λ) ∼N(µ, V)
where V is some unknown positive deﬁnite symmetric matrix that needs to be esti-
mated. If xi is the observed value of X for the ith observation, then the likelihood
function is given by
L(µ, V, λ|X) =
n

i=1
1
(2π|V|)1/2
× exp

−1
2(ψM(xi, λ) −µ)′V−1(ψM(xi, λ) −µ)

(A.34)
where |V| is the determinant2. After rearranging terms, the log-likelihood is given by
log(L(µ, V, λ|X)) = −n
2 log(2π) −n
2 log(|V|)
−1
2
n

i=1
V−1(ψM(xi, λ) −µ)(ψM(xi, λ) −µ)′ (A.35)
If we ﬁx λ, then (A.35) is the standard log-likelihood for the multivariate normal
distribution. The values of V and µ that maximize (A.35) are the sample mean and
2The determinant is deﬁned in any linear algebra textbook.

CASE DELETION IN LINEAR REGRESSION
291
sample covariance matrix, the latter with divisor n rather than n −1,
µ(λ) = 1
n
n

i=1
ψM(xi, λ)
V(λ) = 1
n
n

i=1
(ψM(xi, λ) −µ(λ))(ψM(xi, λ) −µ(λ))′
Substituting these estimates into (A.35) gives the proﬁle log-likelihood for λ,
log(L(µ(λ), V(λ), λ|X)) = −n
2 log(2π) −n
2 log(|V(λ)|) −n
2
(A.36)
This equation will be maximized by minimizing the determinant of V(λ) over
values of λ. This is a numerical problem for which there is no closed-form solution,
but it can be solved using a general-purpose function minimizer.
Standard theory for maximum likelihood estimates can provide tests concerning
λ and standard errors for the elements of λ. To test the hypothesis that λ = λ0
against a general alternative, compute
G2 = 2

log(L(µ(ˆλ), V(ˆλ), ˆλ)) −log(L(µ(λ0), V(λ0), λ0))

and compare G2 to a Chi-squared distribution with k df. The standard error of ˆλ is
obtained from the inverse of the expected information matrix evaluated at ˆλ. The
expected information for ˆλ is just the matrix of second derivatives of (A.36) with
respect to λ evaluated at ˆλ. Many optimization routines, such as optim in R, will
return the matrix of estimated second derivatives if requested; all that is required
is inverting this matrix, and then the square roots of the diagonal elements are the
estimated standard errors.
A.12
CASE DELETION IN LINEAR REGRESSION
Suppose X is the n × p′ matrix of terms with linearly independent columns. We
use the subscript “(i)” to mean “without case i,” so that X(i) is an (n −1) × p′
matrix. We can compute (X′
(i)X(i))−1 from the remarkable formula
(X′
(i)X(i))−1 = (X′X)−1 + (X′X)−1xix′
i(X′X)−1
1 −hii
(A.37)
where hii = x′
i(X′X)−1xi is the ith leverage value, a diagonal value from the hat
matrix. This formula was used by Gauss (1821); a history of it and many variations
is given by Henderson and Searle (1981). It can be applied to give all the results

292
APPENDIX
that one would want relating multiple linear regression with and without the ith
case. For example,
ˆβ(i) = ˆβ −(X′X)−1xi ˆei
1 −hii
(A.38)
Writing ri = ˆei/ˆσ√1 −hii, the estimate of variance is
ˆσ 2
(i) = ˆσ 2

n −p′ −1
n −p′ −r2
i
−1
(A.39)
and the studentized residual ti is
ti = ri

n −p′ −1
n −p′ −r2
i
1/2
(A.40)
The diagnostic statistics examined in this book were ﬁrst thought to be practical
because of simple formulas used to obtain various statistics when cases are deleted
that avoided recomputing estimates. Advances in computing in the last 20 years
or so have made the computational burden of recomputing without a case much
less onerous, and so diagnostic methods equivalent to those discussed here can be
applied to problems other than linear regression where the updating formulas are
not available.

References
Agresti, A. (1996). An Introduction to Categorical Data Analysis. New York: Wiley.
Agresti, A. (2002). Categorical Data Analysis, Second Edition. New York: Wiley.
Allen, D. M. (1974). The relationship between variable selection and prediction. Techno-
metrics, 16, 125–127.
Allison, T. and Cicchetti, D. V. (1976). Sleep in mammals: Ecological and constitutional
correlates. Science, 194, 732–734.
Anscombe, F. (1973). Graphs in statistical analysis. Am. Stat., 27, 17–21.
Atkinson, A. C. (1985). Plots, Transformations and Regression. Oxford: Oxford University
Press.
Baes, C. and Kellogg, H. (1953). Effects of dissolved sulphur on the surface tension of
liquid copper. J. Metals, 5, 643–648.
Barnett, V. and Lewis, T. (2004). Outliers in Statistical Data, Third Edition. Chichester:
Wiley.
Bates, D. and Watts, D. (1988). Relative curvature measures of nonlinearity (with discus-
sions). J. R. Stat. Soc. Ser. B, 22, 41–88.
Beckman, R. and Cook, R. D. (1983). Outliers. Technometrics, 25, 119–149.
Bland, J. (1978). A comparison of certain aspects of ontogeny in the long and short shoots
of McIntosh apple during one annual growth cycle. Unpublished Ph. D. Dissertation,
University of Minnesota, St. Paul.
Blom, G. (1958). Statistical Estimates and Transformed Beta Variates. New York: Wiley.
Bowman, A. and Azzalini, A. (1997). Applied Smoothing Techniques for Data Analysis.
Oxford: Oxford University Press.
Box, G. E. P. and Cox, D. R. (1964). An analysis of transformations. J. R. Stat. Soc., Ser.
B, 26, 211–246.
Brillinger, D. (1983). A generalized linear model with “Gaussian” regression variables. In
Bickel, P. J., Doksum, K. A., and Hodges Jr., J. L., eds., A Festschrift for Erich L.
Lehmann. New York: Chapman & Hall, 97–114.
Brown, P. (1994). Measurement, Regression and Calibration. Oxford: Oxford University
Press.
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
293

294
REFERENCES
Burt, C. (1966). The genetic determination of differences in intelligence: A study of monozy-
gotic twins reared together and apart. Br. J. Psychol., 57, 137–153.
Casella, G. and Berger, R. (1990). Statistical Inference. Paciﬁc Grove: Wadsworth & Brooks-
Cole.
Chen, C. F. (1983). Score tests for regression models. J. Am. Stat. Assoc., 78, 158–161.
Clapham, A. W. (1934). English Romanesque Architecture After the Conquest. Oxford:
Clarendon Press.
Clark, R., Henderson, H. V., Hoggard, G. K., Ellison, R., and Young, B. (1987). The ability
of biochemical and haematological tests to predict recovery in periparturient recumbent
cows. N. Z. Vet. J., 35, 126–133.
Clausius, R. (1850). ¨Uber die bewegende Kraft der W¨arme und die Gezetze welche sich
daraus f¨ur die W¨armelehre selbst abeiten lassen. Annalen der Physik, 79, 500–524.
Cleveland, W. (1979). Robust locally weighted regression and smoothing scatterplots. J. of
the Amer. Stat. Assoc., 74, 829–836.
Collett, D. (2002). Modelling Binary Data, Second Edition. Boca Raton: CRC Press.
Cook, R. D. (1977). Detection of inﬂuential observations in linear regression. Technometrics,
19, 15–18.
Cook, R. (1979). Inﬂuential observations in linear regression. J. Amer. Stat. Assoc., 74,
169–174.
Cook, R. D. (1986). Assessment of local inﬂuence (with discussion). J. R. Stat. Soc., Ser. B,
48, 134–169.
Cook, R. D. (1998). Regression Graphics: Ideas for Studying Regressions through Graphics.
New York: Wiley.
Cook, R. and Jacobson, J. (1978). Analysis of 1977 West Hudson Bay snow goose surveys.
Unpublished Report, Canadian Wildlife Services.
Cook, R. D. and Prescott, P. (1981). Approximate signiﬁcance levels for detecting outliers
in linear regression. Technometrics, 23, 59–64.
Cook, R. D. and Weisberg, S. (1982). Residuals and Inﬂuence in Regression. London: Chap-
man & Hall.
Cook, R. D. and Weisberg, S. (1983). Diagnostics for heteroscedasticity in regression.
Biometrika, 70, 1–10.
Cook, R. D. and Weisberg, S. (1994). Transforming a response variable for linearity.
Biometrika, 81, 731–737.
Cook, R. D. and Weisberg, S. (1997). Graphics for assessing the adequacy of regression
models. J. of the Amer. Stat. Assoc., 92, 490–499.
Cook, R. D. and Weisberg, S. (1999a). Applied Regression Including Computing and Graph-
ics. New York: Wiley.
Cook, R. D. and Weisberg, S. (1999b). Graphs in statistical: Analysis: Is the medium the
message? Amer. Stat., 53, 29–37.
Cook, R. D. and Weisberg, S. (2004). Partial one-dimensional regression models. Amer.
Stat., 58, 110–116.
Cook, R. D. and Witmer, J. (1985). A note on parameter-effects curvature. Journal of the
American Statistical Association, 80, 872–878.
Cox, D. R. (1958). The Planning of Experiments. New York: Wiley.
Cox, D. R. and Oakes, D. (1984). Analysis of Survival Data. London: Chapman & Hall.

REFERENCES
295
Cunningham, R. and Heathcote, C. (1989). Estimating a non-Gaussian regression model
with multicollinearity. Australian Journal of Statistics, 31, 12–17.
Dalziel, C., Lagen, J., and Thurston, J. (1941). Electric shocks. Transactions of the IEEE,
60, 1073–1079.
Daniel, C. and Wood, F. (1980). Fitting Equations to Data, Second Edition. New York:
Wiley.
Davison, A. and Hinkley, D. (1997). Bootstrap Methods and their Application. Cambridge:
Cambridge University Press.
Dawson, R. (1995). The “Unusual Episode” Data Revisited. Journal of Statistical Education,
3, an electronic journal available at www.stat.ncsu.edu/info/jse.
Dempster, A., Laird, N., and Rubin, D. (1977). Maximum likelihood estimation from incom-
plete data via the EM algorithm. J. R. Stat. Soc., Ser. B, 29, 1–38.
Derrick, A. (1992). Development of the measure-correlate-predict strategy for site assess-
ment. Proceedings of the 14th BWEA Conference, Nottingham, 259–265.
Diggle, P., Heagerty, P., Liang, K. Y., and Zeger, S. (2002). Analysis of Longitudinal Data,
Second Edition. Oxford: Oxford University Press.
Dodson, S. (1992), Predicting crustacean zooplankton species richness. Limnology and
Oceanography, 37, 848–856.
Efron, B. (1979). Bootstrap methods: another look at the jackknife. Ann. Stat., 7, 1–26.
Efron, B. and Tibshirani, R. (1993). An Introduction to the Bootstrap. Boca Raton: Chapman
& Hall.
Ezekiel, M. and Fox, K. A. (1959). Methods of Correlation Analysis, Linear and Curvilinear.
New York: Wiley.
Finkelstein, M. (1980). The judicial reception of multiple regression studies in race and sex
discrimination cases. Columbia Law Review, 80, 734–757.
Flury, B. and Riedwyl, H. (1988). Multivariate Statistics: A Practical Approach. London:
Chapman & Hall.
Forbes, J. (1857). Further experiments and remarks on the measurement of heights by boiling
point of water. Trans. R. Soc. Edinburgh, 21, 235–243.
Freedman, D. (1983). A note on screening regression equations. Amer. Stat., 37, 152–157.
Freeman, M. and Tukey, J. (1950). Transformations related to angular and the square root.
Ann. Math. Stat., 21, 607–677.
Fuller, W. (1987). Measurement Error Models. New York: Wiley.
Furnival, G. and Wilson, R. (1974). Regression by leaps and bounds. Technometrics, 16,
499–511.
Gauss, C. (1821–1826). Theoria Combinationis Observationum Erroribus Minimis Obnoxiae
(Theory of the combination of observations which leads to the smallest errors). Werke,
4, 1–93.
Geisser, S. and Eddy, W. F. (1979). A predictive approach to model selection. J. Am. Stat.
Assoc., 74, 753–160.
Gnanadesikan, R. (1997). Methods for Statistical Analysis of Multivariate Data, Second
Edition. New York: Wiley.
Golub, G. and van Loan, C. (1996). Matrix Computations, Third Edition. Baltimore: Johns
Hopkins.
Gould, S. J. (1966). Allometry and size in ontogeny and phylogeny. Biol. Rev., 41, 587–640.

296
REFERENCES
Gould, S. J. (1973). The shape of things to come. Syst. Zool., 22, 401–404.
Graybill, F. (1969). Introduction to Matrices with Statistical Applications. Belmont, CA:
Wadsworth.
Green, P. and Silverman, B. (1994). Nonparametric Regression and Generalized Linear
Models: A Roughness Penalty Approach. London: Chapman & Hall.
Haddon, M. (2001). Modelling and Quantitative Methods in Fisheries. Boca Raton: Chapman
& Hall.
Hahn, M., ed. (1979). Development and Evolution of Brain Size. New York: Academic Press.
Hald, A. (1960). Statistical Theory with Engineering Applications. New York: Wiley.
Hall, P. and Li, K. C. (1993). On almost linearity of low dimensional projections from high
dimensional data. Ann. Stat., 21, 867–889.
H¨ardle, W. (1990). Applied Nonparametric Regression. Cambridge: Cambridge University
Press.
Hastie, T., Tibshirani, R., and Friedman, J. (2001). The Elements of Statistical Learning.
New York: Springer.
Hawkins, D. M. (1980). Identiﬁcation of Outliers. London: Chapman & Hall.
Hawkins, D. M., Bradu, D., and Kass, G. (1984). Locations of several outliers in multiple
regression using elemental sets. Technometrics, 26, 197–208.
Henderson, H. V. and Searle, S. R. (1981). On deriving the inverse of a sum of matrices.
SIAM Rev., 23, 53–60.
Hernandez, F. and Johnson, R. A. (1980). The large sample behavior of transformations to
normality. J. of the Amer. Stat. Assoc., 75, 855–861.
Hinkley, D. (1985). Transformation diagnostics for linear models. Biometrika, 72, 487–496.
Hoaglin, D. C. and Welsch, R. (1978). The hat matrix in regression and ANOVA. Am. Stat.,
32, 17–22.
Hosmer, D. and Lemeshow, S. (2000). Applied Logistic Regression, Second Edition. New
York: Wiley.
Huber, P. (1981). Robust Statistics. New York: Wiley.
Ibrahim, J., Lipsitz, S., and Horton, N. (2001). Using auxiliary data for parameter estimation
with non-ignorably missing outcomes. Applied Statistics, 50, 361–373.
Jevons, W. S. (1868). On the condition of the gold coinage of the United Kingdom, with
reference to the question of international currency. J. [R.] Stat. Soc., 31, 426–464.
Johnson, K. (1996). Unfortunate Emigrants: Narratives of the Donner Party. Logan: Utah
State University Press.
Johnson, M. P. and Raven, P. H. (1973). Species number and endemism: The Gal´apagos
Archipelago revisted. Science, 179, 893–895.
Joiner, B. (1981). Lurking variables: Some examples. Amer. Stat., 35, 227–233.
Kalbﬂeisch, J. D. and Prentice, R. L. (1980). The Statistical Analysis of Failure Time Data.
New York: Wiley.
Kennedy, W. and Bancroft, T. (1971). Model building for prediction in regression based on
repeated signiﬁcance tests. Ann. Math. Stat., 42, 1273–1284.
LeBeau, M. (2004). Evaluation of the intraspeciﬁc effects of a 15-inch minimum size limit
on walleye populations in Northern Wisconsin. Unpublished Ph. D. Dissertation, Uni-
versity of Minnesota.

REFERENCES
297
Li, K. C. and Duan, N. (1989). Regression analysis under link violation. Ann. Stat., 17,
1009–1052.
Lindgren, B. L. (1993). Statistical Theory, Fourth Edition. New York: Macmillan.
Loader, C. (2004). Smoothing: Local regression techniques. In Gentle, J., H¨ardle, W., Mori,
Y., Handbook of Computational Statistics. New York: Springer-Verlag.
Littell, R., Milliken, G., Stroup, W.Wolﬁnger, R. (1996). SAS System for Mixed Models.
Cary: SAS Institute.
Little, R. and Rubin, D. (1987). Statistical Analysis with Missing Data. New York: Wiley.
Longley, J. (1967). An appraisal of least squares programs for the electronic computer from
the point of view of the user. J. Am. Stat. Assoc., 62, 819–841.
Mallows, C. (1973). Some comments on Cp. Technometrics, 15, 661–676.
Mantel, N. (1970). Why stepdown procedures in variable selection? Technometrics, 12,
621–625.
Marquardt, D. W. (1970). Generalized inverses, ridge regression and biased linear estimation.
Technometrics, 12, 591–612.
McCullagh, P. and Nelder, J. (1989). Generalized Linear Models, Second Edition. London:
Chapman & Hall.
Miller, R. (1981). Simultaneous Inference, Second Edition. New York: Springer.
Moore, J. A. (1975). Total biomedical oxygen demand of animal manures. Unpublished Ph.
D. Dissertation, University of Minnesota.
Mosteller, F. and Wallace, D. (1964). Inference and Disputed Authorship: The Federalist.
Reading: Addison-Wesley.
Nelder, J. A. and Wedderburn, R. W. M. (1972). Generalized linear models. Journal of the
Royal Statistical Society, Series A, 135, 370–384.
Noll, S., Weibel, P., Cook, R. D., and Witmer, J. (1984). Biopotency of methionine sources
for young turkeys. Poultry Science, 63, 2458–2470.
Oehlert, G. (2000). A First Course in the Design and Analysis of Experiments. New York:
W.H. Freeman.
Pan, Wei, Connett, J., Porzio, G., and Weisberg, S. (2001). Graphical model checking using
marginal model plots with longitudinal data. Statistics in Medicine, 20, 2935–2945.
Pardoe, I. and Weisberg, S. (2001). An Introduction to bootstrap methods using Arc. Unpub-
lished Report available at www.stat.umn.edu/arc/bootmethREV.pdf.
Parks, J. (1982). A Theory of Feeding and Growth of Animals. New York: Springer.
Pearson, K. (1930). Life and Letters and Labours of Francis Galton, Vol IIIa. Cambridge:
Cambridge University Press.
Pinheiro, J. and Bates, D. (2000). Mixed-Effects Models in S and S-plus. New York: Springer.
Pearson, E. S. and Lee, S. (1903). On the laws of inheritance in man. Biometrika, 2, 357–463.
Porzio, G. (2002). A simulated band to check binary regression models. Metron, 60, 83–95.
Ratkowsky, D. A. (1990). Handbook of Nonlinear Regression Models. New York: Marcel
Dekker.
Rencher, A. and Pun, F. (1980). Inﬂation of R2 in best subset regression. Technometrics,
22, 49–53.
Royston, J. P. (1982a). An extension of Shapiro and Wilk’s W test for normality to large
samples. Appl. Stat., 31, 115–124.

298
REFERENCES
Royston, J. P. (1982b). Expected normal order statistics (exact and approximate), algorithm
AS177. Appl. Stat., 31, 161–168.
Royston, J. P. (1982c). The W test for normality, algorithm AS181. Appl. Stat., 31, 176–180.
Royston, P. and Altman, D. (1994). Regression using fractional polynomials of continuous
covariates: parsimonious parametric modeling. Applied Statistics, 43, 429–467.
Rubin, D. (1976). Inference and missing data. Biometrika, 63, 581–592.
Ruppert, D., Wand, M., Holst, U., and H¨ossjer, O. (1997). Local polynomial variance-
function estimation. Technometrics, 39, 262–273.
Sakamoto, Y., Ishiguro, M., and Kitagawa, G. (1987). Akaike Information Criterion Statistics.
Dordrecht: Reidel.
Saw, J. (1966). A conservative test for concurrence of several regression lines and related
problems. Biometrika, 53, 272–275.
Schafer, J. (1997). Analysis of Incomplete Multivariate Data. Boca Raton: Chapman &
Hall/CRC.
Scheff´e, H. (1959). The Analysis of Variance. New York: Wiley.
Sclove, S. (1968). Improved estimators for coefﬁcients in linear regression. J. Am. Stat.
Assoc., 63, 596–606.
Schott, J. (1996). Matrix Analysis for Statistics. New York: Wiley.
Schwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics, 6, 461–464.
Searle, S. R. (1971). Linear Models. New York: Wiley.
Searle, S. R. (1982). Matrix Algebra Useful for Statistics. New York: Wiley.
Seber, G. A. F. (1977). Linear Regression Analysis. New York: Wiley.
Seber, G. A. F. (2002). The Estimation of Animal Abundance. Caldwell, NJ: Blackburn Press.
Seber, G. and Wild, C. (1989). Nonlinear Regression. New York: Wiley.
Silverman, B. (1986). Density Estimation for Statistics and Data Analysis. Boca Raton:
CRC/Chapman & Hall.
Simonoff, J. (1996). Smoothing Methods in Statistics. New York: Springer-Verlag.
Shapiro, S. S. and Wilk, M. B. (1965). An analysis of variance test for normality (complete
samples). Biometrika, 52, 591–611.
Staudte, R. and Sheather, S. (1990). Robust Estimation and Testing. New York: Wiley.
Taff, S., Tiffany, D., and Weisberg, S. (1996). Measured effects of feedlots on residential
property values in Minnesota: A report to the legislature. Technical Report, Department
of Applied Economics, University of Minnesota, agecon.lib.umn.edu/mn/p96-12.pdf.
Tang, G., Little, R., and Raghunathan, T. (2003). Analysis of multivariate missing data with
nonignorable nonresponse. Biometrika, 90, 747–764.
Thisted, R. (1988). Elements of Statistical Computing. New York: Chapman & Hall.
Tuddenham, R. and Snyder, M. (1954). Physical growth of California boys and girls from
birth to age 18. Calif. Publ . Child Dev., 1, 183–364.
Tukey, J. (1949). One degree of freedom for nonadditivity. Biometrics, 5, 232–242.
Velilla, S. (1993). A note on the multivariate Box-Cox transformation to normality. Stat.
Probab. Lett., 17, 259–263.
Verbeke, G. and Molenberghs, G. (2000). Linear Mixed Models for Longitudinal Data. New
York: Springer-Verlag.

REFERENCES
299
von Bertalanffy, L. (1938). A quantitative theory of organic growth. Human Biology, 10,
181–213.
Weisberg, H., Beier, H., Brody, H., Patton, R., Raychaudhari, K., Takeda, H., Thern, R., and
Van Berg, R. (1978). s-dependence of proton fragmentation by hadrons. II. Incident
laboratory momenta, 30–250 GeV/c. Phys. Rev. D, 17, 2875–2887.
Wilm, H. (1950). Statistical control in hydrologic forecasting. Research Notes, 71, Paciﬁc
Northwest Forest Range Experiment Station, Oregon.
Woodley, W. L., Simpson, J., Biondini, R., and Berkeley, J. (1977). Rainfall results 1970–75:
Florida area cumulus experiment. Science, 195, 735–742.
Yeo, I. and Johnson, R. (2000). A new family of power transformations to improve normality
or symmetry. Biometrika, 87, 954–959.
Zipf, G. (1949). Human Behavior and the Principle of Least-Effort. Cambridge: Addison-
Wesley.


Author Index
Agresti, A., 100, 265, 293
Ahlstrom, M., 45
Allen, D. M., 220, 293
Allison, T., 86, 148, 293
Altman, D., 122, 298
Anscombe, F., 26, 12, 293
Atkinson, A. C., 205, 293
Azzalini, A., 112, 251, 277, 293
Baes, C., 161, 293
Bancroft, T., 117, 296
Barnett, V., 197, 293
Bates, D., 100, 136, 177, 185, 248, 293, 297
Beckman, R., 197, 293
Beier, H., 299
Berger, R., 27, 80, 287, 289, 294
Berkeley, J., 209, 299
Bickel, P. J., 293
Biondini, R., 209, 299
Bland, J., 104, 293
Blom, G., 205, 293
Bowman, A., 112, 251, 277, 293
Box, G. E. P., 130, 153, 289, 293
Bradu, D., 197, 296
Brillinger, D., 155, 293
Brody, H., 299
Brown, P., 143, 293
Burnside, O., 17
Burt, C., 138, 294
Campeyron, E., 39
Casella, G., 27, 80, 287, 289, 294
Chen, C. F., 180, 294
Cicchetti, D., 86, 148, 293
Clapham, A. W., 140, 294
Clark, R., 266, 294
Clausius, R., 39
Cleveland, W., 14, 276, 294
Collett, D., 265–266, 294
Connett, J., 191, 297
Cook, R. D., 8, 49, 113, 131, 152, 156–157, 159,
172–173, 180, 186, 197–198, 204, 238,
293–294, 297
Cox, D. R., 294, 78, 85, 130, 153, 289, 293–294
Cunningham, R., 88, 132, 294
Dalal, S., 268
Dalziel, C., 267, 295
Daniel, C., 205, 295
Davison, A., 87, 244, 295
Dawson, R., 262, 295
Dempster, A., 85, 295
Derrick, A., 140, 295
Diggle, P., 137, 295
Dodson, S., 193, 295
Doksum, K. A., 293
Duan, N., 155–156, 296
Eddy, W., 220, 295
Efron, B., 87, 295
Ellison, R., 266, 294
Ezekiel, M., 162, 295
Finkelstien, M., 295
Flury, B., 268, 295
Forbes, J., 4, 6, 295
Fowlkes, E. B., 268
Fox, K. A., 162, 295
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN 0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
301

302
AUTHOR INDEX
Freedman, D., 225, 295
Freeman, M., 179, 295
Frie, R., 249
Friedman, J., 211, 296
Fuller, W., 90, 295
Furnival, G., 221, 295
Galatowitsch, S., 134
Galto, F., 110
Gauss, C., 291, 295
Geisser, S., 220, 295
Gentle, J., 297
Gnanadesikan, R., 204, 295
Golub, G., 287, 295
Gosset, W. S., 196
Gould, S. J., 140, 150, 295
Graybill, F., 278, 296
Green, P., 14, 296
Haddon, M., 248, 296
Hahn, M., 150, 296
Hald, A., 162, 296
Hall, P., 156, 296
H¨ardle, W., 14, 276, 296–297
Hastie, T., 211, 296
Hawkins, D. M., 197, 296
Heagerty, P., 137, 295
Heathcote, C., 88, 294
Henderson, H. V., 266, 291, 294, 296
Hernandez, F., 153, 296
Hinkley, D., 87, 184, 244, 295–296
Hoadley, B., 268
Hoaglin, D. C., 169, 296
Hodges Jr., J. L., 293
Hoffstedt, C., 153
Hoggard, G. K., 266, 294
Holst, U., 277, 298
Horton, N., 86, 296
Hosmer, D., 266, 296
H¨ossjer, O., 277, 298
Hubbard, K., 17
Huber, P., 27, 296
Hutchinson, R., 18
Ibrahim, J., 86, 296
Ishiguro, M., 217, 298
Jacobson, J., 113, 294
Jevons, W. S., 114, 296
Johnson, K., 267, 296
Johnson, M. P., 231
Johnson, R. A., 153, 296, 299
Joiner, B., 79, 296
Kalbﬂeisch, J. D., 85, 296
Kass, G., 197, 296
Kellogg, H., 161, 293
Kennedy, W., 117, 296
Kitagawa, G., 217, 298
Lagen, J., 295
Laird, N., 85, 295
LeBeau, M., 249, 296
Lee, S., 2, 297
Lemeshow, S., 266, 296
Lewis, T., 197, 293
Li, K. C., 155–156, 296
Liang, K. Y., 137, 295
Lindgren, B., 80, 297
Lipsitz, S., 86, 296
Littell, R., 136, 297
Little, R., 84, 86, 297–298
Loader, C., 111–112, 276–277, 297
Longley, J., 94, 297
Mallows, C., 218, 297
Mantel, N., 230, 297
Marquardt, D. W., 216, 297
McCullagh, P., 100, 265–266, 297
Miklovic, S., 134
Miller, R., 45, 196, 297
Milliken, G., 136, 297
Molenberghs, G., 136, 298
Moore, J., 231, 297
Mori, Y., 297
Mosteller, F., 44, 297
Nelder, J. A., 100, 265–266, 297
Neyman, J., 235
Ng, C., 244
Noll, S., 8, 297
Norell, R., 267
Oakes, D., 85, 294
Oehlert, G., 78, 117, 131, 297
Pan, W., 191, 297
Pardoe, I., 89, 297
Parks, J., 238, 297
Patton, R., 299
Pearson, E. S., 2, 297
Pearson, K., 110, 297
Pierce, R., 90
Pinheiro, J., 100, 136, 177, 185, 297
Porzio, G., 191, 297
Prentice, R. L., 85, 296

AUTHOR INDEX
303
Prescott, P., 197, 294
Pun, F., 225, 297
Raghunathan, T., 86, 298
Ratkowsky, D. A., 248, 297
Raven, P. H., 231, 296
Raychaudhari, K., 299
Rencher, A., 225, 297
Rice, J., 182
Rich, R., 251
Riedwyl, H., 268, 295
Robinson, A., 112, 151
Royston, J. P., 122, 205, 297–298
Rubin, D., 84–85, 295, 297–298
Ruppert, D., 277, 298
Sakamoto, Y., 217, 298
Saw, J., 130, 298
Schafer, J., 84, 298
Scheff´e, H., 179, 298
Schott, J., 278, 298
Schwarz, G., 218, 298
Sclove, S., 162, 298
Searle, S. R., 74, 278, 291, 296, 298
Seber, G. A. F., 31, 91, 108, 116, 248, 298
Shapiro, S. S., 205, 298
Sheather, S., 197, 298
Silverman, B., 14, 251, 296, 298
Simonoff, J., 14, 112, 277, 298
Simpson, J., 209, 299
Siracuse, M., 210
Snyder, M., 65, 298
Staudte, R., 197, 298
Stigler, S., 114
Stroup, W., 136–137, 297
Taff, S., 78, 298
Takeda, H., 299
Tang, G., 86, 298
Telford, R., 132
Thern, R., 299
Thisted, R., 87, 248, 298
Thurston, J., 295
Tibshirani, R., 87, 211, 295–296
Tiffany, D., 78, 141, 208, 298
Tuddenham, R., 65, 298
Tukey, J., 176, 179, 295, 298
Van Berg, R., 299
Van Loan, C., 287, 295
Velilla, S., 157, 290, 298
Verbeke, G., 136, 298
Von Bertalanffy, L., 248, 299
Wallace, D., 44, 297
Wand, M., 277, 298
Watts, D., 248, 293
Wedderburn, R. W. M., 100, 266, 297
Weibel, P., 8, 297
Weisberg, H., 98, 102, 299
Weisberg, S., 49, 78, 89, 131, 152, 157, 159,
172–173, 180, 186, 191, 197, 204, 294,
297–298
Welsch, R., 169, 296
Wild, C., 248, 298
Wilk, M. B., 205, 298
Wilm, H., 42, 299
Wilson, R., 221, 295
Witmer, J., 8, 238, 294, 297
Wolﬁnger, R., 136–137, 297
Wood, F., 205, 295
Woodley, W. L., 209, 299
Yeo, I., 160, 299
Young, B., 266, 294
Zeger, S., 137, 295
Zipf, G., 43, 299


Subject Index
Added-variable plot, 49–50, 63, 66, 73, 203, 221
AIC, 217
Akaike Information Criterion, 217
Aliased, 73
Allometric, 150
Analysis of variance, 28–29, 131
overall, 61
sequential, 62, 64
anova, 28–29
overall, 61
sequential, 62,64
Arcsine square-root, 179
Asymptote, 233
Backsolving, 287
Backward elimination algorithm, 222
Bayes Information Criterion, 218
Bernoulli distribution, 253
BIC, 218
Binary regression, 251
Binomial distribution, 253, 263
Binomial regression, 251, 253
Bonferroni inequality, 196
Bootstrap, 87, 110, 112, 143, 178, 244
bias, 89
measurement error, 90
nonlinear regression, 244
ratio of estimators, 89
Box–Cox, 153, 157, 159–160, 289
Cases, 2
Causation, 69, 78
Censored regression, 85
Central composite design, 138
Class variable, 124
Coefﬁcient of determination, 31, 62
Collinearity, 211, 214
Comparing groups
linear regression, 126
nonlinear regression, 241
random coefﬁcient model, 134
Complexity, 217
Computer packages, 270
Computer simulation, 87
Conﬁdence interval, 32
intercept, 32
slope, 33
Conﬁdence regions, 108
Cook’s distance, 198
Correlation, 31, 54, 272
Covariance matrix, 283
Cross-validation, 220
Curvature
in residual plot, 176
Data
cross-sectional, 7
longitudinal, 7
Data ﬁles, 270
ais.txt, 132
allshoots.txt, 104, 139
anscombe.txt, 12
baeskel.txt, 161
banknote.txt, 268
BGSall.txt, 65–66, 138
BGSboys.txt, 65–66
BGSgirls.txt, 65–66
BigMac2003.txt, 164
Applied Linear Regression, Third Edition, by Sanford Weisberg
ISBN0-471-66379-4
Copyright 2005 John Wiley & Sons, Inc.
305

306
SUBJECT INDEX
Data ﬁles (Continued)
blowAPB.txt, 269
blowBF.txt, 255
brains.txt, 148
cakes.txt, 118, 137
cathedral.txt, 139
caution.txt, 172
challeng.txt, 268
chloride.txt, 134
cloud.txt, 208
domedata.txt, 145
domedata1.txt, 146
donner.txt, 267
downer.txt, 266
drugcost.txt, 209
dwaste.txt, 231
florida.txt, 207
forbes.txt, 5
ftcollinssnow.txt, 7
fuel2001.txt, 15, 52
galapagos.txt, 231
galtonpeas.txt, 110
heights.txt, 2, 41
highway.txt, 153–154
hooker.txt, 40
htwt.txt, 38
jevons.txt, 114
lakemary.txt, 249
lakes.txt, 193
landrent.txt, 208
lathe1.txt, 137
longley.txt, 94
longshoots.txt, 104
mantel.txt, 230
mile.txt, 143
Mitchell.txt, 18
MWwords.txt, 44
npdata.txt, 91
oldfaith.txt, 18
physics.txt, 98
physics1.txt, 114
pipeline.txt, 191
prodscore.txt, 141
rat.txt, 200
salary.txt, 141
salarygov.txt, 163
segreg.txt, 244
shocks.txt, 267
shortshoots.txt, 104
sleep1.txt, 122
snake.txt, 42
sniffer.txt, 182
snowgeese.txt, 113
stopping.txt, 162
swan96.txt, 249
titanic.txt, 262
transact.txt, 88
turk0.txt, 142, 238
turkey.txt, 8, 242
twins.txt, 138
ufcgf.txt, 112
ufcwc.txt, 151
UN1.txt, 18
UN2.txt, 47
UN3.txt, 166
walleye.txt, 249
water.txt, 18, 163
wblake.txt, 7, 41
wblake2.txt, 41
wm1.txt, 45, 93
wm2.txt, 140
wm3.txt, 140
wm4.txt, 227, 269
wm5.txt, 228
wool.txt, 130, 165
Data mining, 211
Degrees of freedom, 25
Delta method, 120, 143
Density estimate, 251
Dependence, 1
Deviance, 260
Diagnostics, 167–187, 194–204
Dummy variable, 52, 122
EM algorithm, 85
Errors, 19
Examples
Alaska pipeline faults, 191
Anscombe, 12, 198
Apple shoot, 139
Apple shoots, 104, 111
Australian athletes, 132, 250
Banknote, 268
Berkeley guidance study, 65, 70, 92, 138–139,
230
Big Mac data, 164–165
Black crappies, 249
Blowdown, 251, 255, 269
Brain weight, 148
Cake data, 137
Cakes, 117
California water, 18, 162, 192
Cathedrals, 139
Challenger data, 268
Cloud seeding, 209, 250
Donner party, 267
Downer, 266
Drug costs, 209

SUBJECT INDEX
307
Electric shocks, 267
Feedlots, 78
Florida election 2000, 207
Forbes, 4, 22–24, 29, 33–34, 37, 39, 138
Ft. Collins snowfall, 7, 33, 44
Fuel consumption, 15, 52, 69, 93, 166, 173,
206
Gal´apagos species, 231
Galton’s peas, 110
Government salary, 163
Heights, 2, 19, 23, 34–36, 41, 51, 205
Highway accidents, 153, 159, 218, 222, 230,
232, 250
Hooker, 40, 138
Jevons’ coins, 114, 143
Lake Mary, 248
Land productivity, 140
Land rent, 208
Lathe, 137
Lathe data, 207
Longley, 94
Mantel, 230
Metrodome, 145
Mitchell, 17
Northern Pike, 90
Old Faithful, 18, 44
Oxygen uptake, 230
Physics data, 116
Rat, 200, 203
Segmented regression, 244
Sex discrimination, 141
Sleep, 86, 122, 126, 138, 248
Smallmouth bass, 6, 17, 41
Snake River levels, 42
Sniffer, 182
Snow geese, 113, 181
Stopping distances, 162
Strong interaction, 98, 114
Surface tension, 161
Titanic, 262
Transactions, 88, 143, 205
Turkey growth, 8, 142–143, 233, 238, 247
Twin study, 138
United Nations, 18, 41, 47, 51, 66, 166, 170,
176, 187, 198
Upper Flat Creek, 151, 112
Walleye growth, 249
Windmills, 45, 93, 140, 226, 232, 269
Wool data, 130, 165
World cities, 164
Zipf’s Law, 43
Zooplankton species, 193
Expected information matrix, 291
Expected order statistics, 205
Experiment, 77
Exponential family distribution, 265
F-test, 28
overall anova, 30
partial, 63
power, 107
robustness, 108
Factor, 52, 122–136
Factor rule, 123
Fisher scoring, 265
Fitted value, 21, 35, 57, 65
Fixed signiﬁcance level, 31
Forward selection, 221
Full rank matrix, 283
Gauss–Markov theorem, 27
Gauss–Newton algorithm, 236
Generalized least squares, 100, 178
Generalized linear models, 100, 178, 265
Geometric mean, 153
Goodness of ﬁt, 261
Hat matrix, 168–169
Hat notation, 21
Hessian matrix, 235
Histogram, 251
Hyperplane, 51
Independent, 2, 19
Information matrix, 291
Inﬂuence, 167, 198
Inheritance, 2
Interaction, 52, 117
Intercept, 9, 19, 51
Intra-class correlation, 136
Inverse ﬁtted value plot, 152, 159, 165
Inverse regression, 143
Jittering, 2
Kernel mean function, 234, 254
logistic function, 254
Lack of ﬁt
F test, 103
for nonlinear models, 241
nonparametric, 111
sum of squares, 103
variance known, 100
variance unknown, 102
Lagged variables, 226
Lapack, 287

308
SUBJECT INDEX
Leaps and bounds, 221
Least squares
nonlinear, 234
ordinary, 6
weighted, 96
Leverage, 4, 169, 196
Li–Duan theorem, 156
Likelihood function, 263
Likelihood ratio test, 108, 158, 260
Linear
dependence, 73, 214, 283
independence, 73, 283
mixed model, 136
operator, 270
predictor, 156, 254
regression, 1
Link function, 254
Local inﬂuence, 204
loess, 14–15, 111–112, 149, 181, 185, 187,
276–277
Logarithms, 70
and coefﬁcient estimates, 76
choice of base, 23
log rule, 150
Logistic function, 254
Logistic regression, 100, 251–265
log-likelihood, 264
model comparisons, 261
Logit, 254
Lurking variable, 79
Machine learning, 211
Main effects, 130
Mallows’ Cp, 218
Marginal model plot, 185–190
Matrix
diagonal, 279
full rank, 282
identity, 279
inverse, 282
invertible, 282
nonsingular, 282
norm, 281
notation, 54
orthogonal, 282
orthogonal columns, 282
singular, 282
square, 279
symmetric, 279
Maximum likelihood, 27, 263–264, 287–288
Mean function, 9–11
Mean square, 25
Measure, correlate, predict, 140
Measurement error, 90
Median, 87
Missing
at random, 85
data, 84
multiple imputation, 85
values in data ﬁles, 270
Modiﬁed power family, 153
Multiple correlation coefﬁcient, 62
Multiple linear regression, 47
Multivariate normal, 80
Multivariate transformations, 157
Nearest neighbor, 276
Newton–Raphson, 265
NID, 27
Noncentral χ2, 108
Noncentral F, 31
Nonconstant variance, 177, 180
Nonlinear least squares, 152, 234
Nonlinear regression, 233–250
comparing groups, 241
large-sample inference, 237
starting values, 237
Nonparametric, 10
Nonparametric regression, 277
Normal distributions, 80
Normal equations, 273, 284
Normality, 20, 58, 204
large sample, 27
Normal probability plot, 204
Null plot, 13, 36
Observational study, 69, 77
Odds of success, 254
OLS, see Ordinary least squares
One-dimensional estimation result, 156
One-way analysis of variance, 124
Order statistics, 205
Ordinary least squares, 6–7, 10, 21–28, 116, 144,
231, 234, 240, 284–285, 288
same as maximum likelihood estimate, 288
Orthogonal, 192
polynomials, 116
projection, 169
Outlier, 4, 36, 194–197
mean shift, 194
Over-parameterized, 73
Overplotting, 2
p′, 59
p-value, 268
interpreting, 31

SUBJECT INDEX
309
Parameters, 9, 21
interpreting, 69
not the same as estimates, 21, 24
terms in log scale, 70
Parametric bootstrap, 112
Partial correlation, 221
Partial one-dimensional, 131, 144, 250
Pearson residual, 171
Pearson’s X2, 262
POD model, 131–133, 144, 250
Polynomial regression, 115
Power, 31, 108
Power family, 148
Power transformations, 14
Predicted residual, 207, 220
Prediction, 34, 65
Predictor, 1, 51
PRESS, 220
residual, 207
Probability mass function, 253
Proﬁle log-likelihood, 291
Pure error, 103, 242
Quadratic regression, 115
estimated minimum or maximum, 115
R2, 31–32, 62, 81
interpretation, 83
Random coefﬁcients model, 135
Random intercepts model, 136
Random sampling, 81
Random vector, 283
Range rule, 150
Rectangles, 74
Regression, 1, 10
binomial, 251
multiple linear, 47
nonlinear, 233
sum of squares, 58
through the origin, 42, 84
Removable nonadditivity, 166
Residual, 21, 23, 36, 57
degrees of freedom, 25
mean square, 25
Pearson, 171
properties, 168
standardized, 195
studentized, 196
sum of squares, 21, 24, 57
sum of squares function, 234
weighted, 170–171
Residual plots, 171
curvature, 176
Tukey’s test, 176
Response, 1
Sample correlations, 54
Sample covariance matrix, 57
Sample order statistics, 204
Saturated model, 262
Scalar, 279
Scaled power transformation, 233
Scatterplot, 1
Scatterplot matrix, 15–17
Score test, 180
nonconstant variance, 180
Score vector, 235
se, 27
seﬁt, 35
sepred, 34
Second-order mean function, 117
Segmented regression, 244
Separated points, 3
Signiﬁcance level
ﬁxed, 31
Simple regression
model, 19
deviations form, 41
matrix version, 58
Slices, 3
Slope, 9, 19
Smoother, 10, 14, 275
Smoothing parameter, 276
Standard error, 27
of ﬁtted value, 2
of prediction, 2
of regression, 25
Standardized residual, 195
Statistical error, 19
Stepwise methods, 221–226
backward elimination, 222
forward selection, 221
Straight lines, 19
Studentized residual, 196
Studentized statistic, 196
Sum of squares
due to regression, 29
for lack of ﬁt, 103
regression, 58
Summary graph, 1–2, 11–12, 175
multiple regression, 84
Supernormality, 204
Taylor series, 120, 235
Terms, 47, 51–54
Three-dimensional plot, 48

310
SUBJECT INDEX
Transformations, 52, 147–160
arcsine square-root, 179
Box–Cox, 153
families, 148
for linearity, 233
log rule, 150
many predictors, 153–158
modiﬁed power, 153
multivariate Box–Cox, 157
nonpositive variables, 160
power family, 148
predictor, 150
range rule, 150
response, 159
scaled, 150
using a nonlinear model, 233
Yeo–Johnson, 160
t-tests, 32
Tukey’s test for nonadditivity, 176
Unbiased, 271
Uncorrelated, 8
Variability explained, 83
Variable selection methods, 211
Variance estimate
pure error, 103
Variance function, 11, 96
Variance inﬂation factor, 216
Variance stabilizing transformation, 100, 177
Vector, 279
length, 281
Web site, 270
www.stat.umn.edu/alr, 270
Weighted least squares, 96–99, 114, 234
outliers, 196
Weighted residual, 171
Wind farm, 45
WLS, see Weighted least squares
Yeo–Johnson transformation family, 160, 290
Zipf’s law, 43










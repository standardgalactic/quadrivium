Introduction to Reconﬁgurable Computing

Introduction
to Reconﬁgurable
Computing
Architectures, Algorithms,
and Applications
by
Christophe Bobda
University of Kaiserslautern, Germany

A C.I.P. Catalogue record for this book is available from the Library of Congress.
ISBN 978-1-4020-6088-5 (HB)
ISBN 978-1-4020-6100-4 (e-book)
Published by Springer,
P.O. Box 17, 3300 AA Dordrecht, The Netherlands.
www.springer.com
Printed on acid-free paper
All Rights Reserved
c⃝2007 Springer
No part of this work may be reproduced, stored in a retrieval system, or transmitted
in any form or by any means, electronic, mechanical, photocopying, microﬁlming, recording
or otherwise, without written permission from the Publisher, with the exception
of any material supplied speciﬁcally for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work.

To Lewin, Jan and Huguette for being so patient

Foreword
"Christophe Bobda’s book is an all-embracing introduction to the fundamentals
of the entire discipline of Reconﬁgurable Computing, also seen with the eyes
of a software developer and including a taxonomy of application areas.
Reconﬁgurable Computing is a disruptive innovation currently going to com-
plete the most important breakthrough after introduction of the von Neumann
paradigm. On software to FPGA migrations a dazzling array of publications
from a wide variety application areas reports speed-up factors between 1 and
4 orders of magnitude and promises to reduce the electricity bill by at least an
order of magnitude. Facing the tcyberinfrastructure’s growing electricity con-
sumption (predicted to reach 35–50% of the total electricity production by the
year 2020 in the USA) also this energy aspect is a strategic issue.
The focal point of worldwide almost 15 million software developers will
shift toward new solutions of the productivity problems which stem from pro-
gramming the coming many-core microprocessors and from educational deﬁcits.
Currently Reconﬁgurable Computing for High Performance computing is even
disorienting the supercomputing scene. Their tremulous question is: Do we
need to learn hardware design?
In the past, when students asked for a text book, we had to refer to a col-
lection of specialized books and review articles focused on individual topics or
special application areas, where Reconﬁgurable Computing takes only a corner
or a chapter, sometimes even treating FPGAs as exotic technology (although
in important areas it is mainstream for a decade). The typical style of those
books or articles assumes that the reader has a hardware background: a leap
too far for the existing software developers community.
The book by Christophe Bobda, however, has also been written for peo-
ple with a software background, substantially reducing the educational leap
bybridging the gap. His book has the potential to become a best-seller and to

viii
Foreword
stimulate the urgently needed transformation of the software developer popu-
lation’s mindset, by playing a similar role as known from the famous historic
Mead-&-Conway textbook for the VLSI design revolution.
Reiner Hartenstein, IEEE fellow,
Professor, TU Kaiserslautern "

Contents
Foreword
vii
Preface
xiii
About the Author
xv
List of Figures
xvii
List of Tables
xxv
1. INTRODUCTION
1
1
General Purpose Computing
2
2
Domain-Speciﬁc Processors
5
3
Application-Speciﬁc Processors
6
4
Reconﬁgurable Computing
8
5
Fields of Application
9
6
Organization of the Book
11
2. RECONFIGURABLE ARCHITECTURES
15
1
Early Work
15
2
Simple Programmable Logic Devices
26
3
Complex Programmable Logic Device
28
4
Field Programmable Gate Arrays
28
5
Coarse-Grained Reconﬁgurable Devices
49
6
Conclusion
65
3. IMPLEMENTATION
67
1
Integration
68
2
FPGA Design Flow
72
3
Logic Synthesis
75
4
Conclusion
98

vi
Contents
4. HIGH-LEVEL SYNTHESIS FOR RECONFIGURABLE DEVICES
99
1
Modelling
100
2
Temporal Partitioning Algorithms
120
3
Conclusion
148
5. TEMPORAL PLACEMENT
149
1
Ofﬂine Temporal Placement
151
2
Online Temporal Placement
160
3
Managing the Device’s Free Space with Empty Rectangles
161
4
Managing the Device’s Occupied Space
165
5
Conclusion
179
6. ONLINE COMMUNICATION
181
1
Direct Communication
181
2
Communication Over Third Party
182
3
Bus-based Communication
183
4
Circuit Switching
183
5
Network on Chip
188
6
The Dynamic Network on Chip (DyNoC)
199
7
Conclusion
212
7. PARTIAL RECONFIGURATION DESIGN
213
1
Partial Reconﬁguration on Virtex Devices
214
2
Bitstream Manipulation with JBits
216
3
The Modular Design Flow
217
4
The Early Access Design Flow
225
5
Creating Partially Reconﬁgurable Designs
234
6
Partial Reconﬁguration using Handel-C Designs
244
7
Platform design
246
8
Enhancement in the Platform Design
256
9
Conclusion
257
8. SYSTEM ON A PROGRAMMABLE CHIP
259
1
Introduction to SoPC
259
2
Adaptive Multiprocessing on Chip
268
3
Conclusion
284

Contents
vii
9. APPLICATIONS
285
1
Pattern Matching
286
2
Video Streaming
294
3
Distributed Arithmetic
298
4
Adaptive Controller
307
5
Adaptive Cryptographic Systems
310
6
Software Deﬁned Radio
313
7
High-Performance Computing
315
8
Conclusion
317
References
319
Appendices
336
A
Hints to Labs
337
1
Prerequisites
338
2
Reorganization of the Project Video8 non pr
338
B
Party
345
C
Quick Part-Y Tutorial
349

Preface
One indicator of the growing importance of Reconﬁgurable Computing is
the large number of events (conferences, workshops, meetings) organized and
devoted to this topic in the last couple of years. Also, the growth observed in
the market share of programmable logic devices, particulary the FPGAs, is an
indicator of the strong interest in reconﬁgurable logic.
Following this development, teaching reconﬁgurable computing, which was
initiated in many Universities a couple of years before, has gained more impor-
tance. The curricula in reconﬁgurable computing varies from simple seminars
to more heavy syllabus including lectures, exercises and labs.
Many people among whom Reiner Hartenstein have been advocating years
ago in favour of a normalized reconﬁgurable computing syllabus. Aware of
the importance of a teaching book in this normalization, during a bi-annual
meeting on reconﬁgurable computing held in Dagstuhl in 2003 [11], Harten-
stein coined the importance of a text book in reconﬁgurable computing and
proposed the attendees to write one. He suggested to have several people writ-
ing to minimize the work and have the book published as faster as possible.
Unfortunately, this initiative was not pursued.
A couple of months after this initiative, in the summer term 2004, I started
teaching a course in reconﬁgurable computing at the university of Erlangen-
Nuremberg. With the difﬁculties of acquiring teaching materials and labs, I
started writing a script to ease the learning process of students. The positive
feedback gained from the student encouraged me to continue writing. Further
repetitions of the course in winter term 2004 and in winter term 2006 were
used to improve the course contents. It should be mentioned that in a couple
of books [153] [220] [134], reconﬁgurable computing were published in be-
tween. However, none of them were found to cover the complete aspects of
reconﬁgurable computing as I use to teach in my course.
My goal in writing this book was to provide a strong theoretical and practi-
cal background, as a contribution for a syllabus in reconﬁgurable computing.

xiv
Preface
A short overview on the content of each chapter is provided in Section 6 of
Chapter 1.
This book targets graduate students and lecturers in computer engineering,
computer science and electrical engineering. Also professional in the afore
mentioned ﬁeld can use the book as well. We supply the book with teaching
materials (slides and labs) to ease the course preparation for those willing to
introduce a reconﬁgurable computing curricula. The teaching material as well
as the labs can be downloaded from the course Web page at www.bobda.net/
rc-book.
Finally, despite all the effort place in the review, we cannot be sure that all
the mistakes were ﬁltered out. We will therefore be grateful to receive your
comments and feedback on possible errors.
Kaiserslautern, June 2007
Christophe Bobda

About the Author
Dr. Bobda received the Licence degree in mathematics from the Univer-
sity of Yaounde, Cameroon, in 1992, the diploma of computer science and the
Ph.D. degree (with honors) in computer science from the University of Pader-
born in Germany in 1999 and 2003, respectively. In June 2003, he joined the
department of computer science at the University of Erlangen-Nuremberg in
Germany as post doc. In October 2005, he moved to the University of Kaiser-
slautern as Junior Professor, where he leads the working group Self-Organizing
Embedded Systems in the department of computer science. His research inter-
ests include reconﬁgurable computing, self-organization in embedded systems,
multiprocessor on chip and adaptive image processing.
Dr. Bobda received the Best Dissertation Award 2003 from the University of
Paderborn for his work on synthesis of reconﬁgurable systems using temporal
partitioning and temporal placement.
Dr. Bobda is member of The IEEE Computer Society, the ACM and the
GI. He has also served in the program committee of several conferences (FPL,
FPT, RAW, RSP, ERSA, DRS) and in the DATE executive committee as pro-
ceedings chair (2004, 2005, 2006, 2007). He served as reviewer of several
journals (IEEE TC, IEEE TVLSI, Elsevier Journal of Microprocessor and Mi-
crosystems, Integration the VLSI Journal ) and conferences (DAC, DATE, FPL,
FPT, SBCCI, RAW, RSP, ERSA).

List of Figures
1.1
The Von Neumann Computer architecture
2
1.2
Sequential and pipelined execution of instructions on a
Von Neumann Computer
4
1.3
ASIP implementation of Algorithm 1
7
1.4
Flexibility vs performance of processor classes
8
2.1
Structure of the Estrin Fix-Plus Machine
17
2.2
The basic building blocks of Fix-Plus Machine
17
2.3
The wiring harness of the Estrin-Machine
18
2.4
The motherboard of the Estrin’s Fix-Plus
18
2.5
Estrin at work: Hand-Controlled Reconﬁguration
19
2.6
Structure of the Rammig machine
20
2.7
META-46 GOLDLAC
20
2.8
General architecture of the XPuter as implemented in
the Map oriented Machine (MOM-3) prototype
21
2.9
Structure of the XPuter’s reconﬁgurable ALU
22
2.10
Programmable Active Memory (PAM) architecture as
array of (PABs)
23
2.11
Architecture of the SPLASH II array board
25
2.12
PAL and PLA implementations of the functions F1 =
A · C + A · B and F2 = A · B + B · C
27
2.13
Structure of a CPLD device
28
2.14
Structure of an FPGA
29
2.15
Antifuse FPGA Technology
30
2.16
A Xillinx SRAM cell
31
2.17
Use of SRAM in FPGA-Conﬁguration
31

xviii
List of Figures
2.18
EEPROM Technology
32
2.19
Implementation of f=ab in a 2-input MUX
33
2.20
Implementation of a Full adder using two 4-input one
output MUX
35
2.21
The Actel basic computing blocks uses multiplexers as
function generators
36
2.22
2-input LUT
36
2.23
Implementation of a full adder in two 3-input LUTs
37
2.24
Basic block of the Xilinx FPGAs
38
2.25
CLB in the newer Xilinx FPGAs (Spartan 3, Virtex 4
and Virtex 5)
38
2.26
Logic Element in the Cyclone II
39
2.27
Stratix II Adaptive Logic Module
40
2.28
The four basic FPGA structures
41
2.29
Symmetrical array arrangement in a) the Xilinx and b)
the Atmel AT40K FPGAs
42
2.30
Virtex routing resource
42
2.31
Local connection of an Atmel Cell
42
2.32
Row based arrangement on the Actel ACT3 FPGA Family
43
2.33
Actel’s ACT3 FPGA horizontal and vertical routing resources 44
2.34
Actel ProASIC local routing resources
45
2.35
Hierarchical arrangement on the Altera Stratix II FPGA
46
2.36
LAB connection on the Altera Stratix devices
46
2.37
General structure of an I/O component
47
2.38
Structure of a Xilinx Virtex II Pro FPGA with two Pow-
erPC 405 Processor blocks
49
2.39
Structure of the PACT XPP device
51
2.40
The XPP ALU Processing Array Element. The struc-
ture of the RAM ALU is similar.
52
2.41
Structure of the NEC Dynamically Reconﬁgurable Processor
53
2.42
The DRP Processing Element
54
2.43
Structure of the picoChip device
55
2.44
The Quicksilver ACM hierarchical structure with 64 nodes
56
2.45
ACM Node and Routing resource
57
2.46
IPﬂex DAP/DNA reconﬁgurable processor
59
2.47
The Stretch 5530 conﬁgurable processor
59

List of Figures
xix
2.48
Pipeline Reconﬁguration: Mapping of a 5 stage virtual
pipeline auf eine 3 stage
63
3.1
Architecture of a run-time reconﬁgurable system
69
3.2
A CPU-RPU conﬁguration and computation step
70
3.3
The FPGA design ﬂow
73
3.4
A structured digital system
75
3.5
Example of a boolean network with: y1 = x1 + x2,
y2 = x3 ·x4, y3 = x5 · x6, y4 = y1 + y2, z1 = y1 +y4,
and z2 = y2 ⊕y3
76
3.6
BDD-representation of the function f = abc + bd + bcd
79
3.7
Example of a K-feasible cone Cv at a node v
83
3.8
Example of a graph covering with K-feasible cone and
the corresponding covering with LUTs
83
3.9
Chortle two-level decomposition
84
3.10
Example of multi-level decomposition
86
3.11
Exploiting reconvergent paths to reduce the amount of
LUTs used
87
3.12
Logic replication at fan-out nodes to reduce the number
of LUTs used
88
3.13
Construction of the network Nt from the cone Ct at
node t
90
3.14
Minimum height 3-feasible cut and node mapping
91
3.15
Illustration of the two cases in the proof of Lemma 3.8
92
3.16
Transforming Nt into N ′t by node collapsing
94
3.17
Transforming the node cut constraints into the edge cut ones
94
3.18
Improvement of the FlowMap algorithm through efﬁ-
cient predecessor packing
97
4.1
Dataﬂow Graph for Quadratic Root
102
4.2
Sequencing graph with a branching node linking to two
different sub graphs
103
4.3
Transformation of a sequential program into a FSMD
105
4.4
Transformation of the greatest common divisor pro-
gram into an FSMD
106
4.5
The datapath and the corresponding FSM for the GCD-FSMD107
4.6
Dataﬂow graph of the functions: x = ((a × b) −(c ×
d))+((c×d)−(e−f)) and y = ((c×d)−(e−f))−
((e −f) + (g −h))
109

xx
List of Figures
4.7
HLS of the graph in ﬁgure 4.6 on a an architecture with
one instances of the resource types +, ∗and −
110
4.8
HLS of the graph in ﬁgure 4.6 on a reconﬁgurable device
111
4.9
Partitioning of a coarse-grained node in the dataﬂow graph.
113
4.10
Example of conﬁguration graph
116
4.11
Wasted resources
117
4.12
Partitioning of the graph G with connectivity 0.24 with
an algorithm that produces a quality of 0.25
119
4.13
Partitioning of the graph G with connectivity 0.24 with
an algorithm that produces a quality of 0.45
119
4.14
Scheduling example with the ASAP-algorithm
122
4.15
ALAP Scheduling example
122
4.16
An example of list scheduling using the depth of a node
as priority
124
4.17
Levelizing effect on the list-scheduling on a dataﬂow graph
128
4.18
Partitioning with a better quality than the list-scheduling
128
4.19
Dataﬂow graph transformation into a network
134
4.20
Transformation and partitioning steps using the network
ﬂow approach
135
4.21
1-D and 2-D spectral-based placement of a graph
136
4.22
Dataﬂow graph of f = ((a + b) ∗c) −((e + f) + (g ∗h))
140
4.23
3-D spectral placement of the DFG of ﬁgure 4.22
141
4.24
Derived partitioning from the spectral placement of ﬁg-
ure 4.23
141
4.25
Internal and external edges of a given nodes
143
4.26
Partitioning of a graph into two sets with common sets
of operators
146
4.27
Logical partitioning of the graph of ﬁgure 4.26
147
4.28
Implementation of conﬁguration switching with the par-
titions of ﬁgure 4.27
147
5.1
Temporal placement as 3-D placement of blocks
150
5.2
First-ﬁt temporal placement of a set of clusters
154
5.3
Valid two dimensional packing
157
5.4
A non valid two dimensional packing
158
5.5
3-D placement and corresponding interval graphs, com-
plement graphs and oriented packing
159
5.6
Various types of empty rectangles
162

List of Figures
xxi
5.7
Increase of the number of MER through the insertion
of a new component
163
5.8
Two different non-overlapping rectangles representations
164
5.9
Splitting alternatives after new insertion
164
5.10
IPR of a new module v relative to a placed module v′
167
5.11
Impossible and possible placement region of a compo-
nent v prior to its insertion
168
5.12
Nearest possible feasible point of an optimal location
that falls within the IPR
170
5.13
Moving out of consecutive overlapping IPRs
172
5.14
Expanding existing modules and shrinking chip area
and the new
173
5.15
Characterization of IPR of a given component: The set
of contours (left), the contour returned by the modiﬁed
CUR (middle), the contour returned by the CUR (right)
174
5.16
Placement of a component on the chip (left) guided by
the communication with its environment (right)
176
5.17
Computation of the union of contours. The point on
the boundary represent the potentially moves from the
median out of the IPR.
178
6.1
Direct communication between placed modules on a
reconﬁgurable device
182
6.2
Drawback of circuit switching in temporal placement
Placing a component using 4 PEs will not be possible,
although enough free resources are available
185
6.3
The RMBoC architecture
186
6.4
RMBoC FPGA implementation
187
6.5
Crosspoint architecture
187
6.6
A Network on Chip on a 2-D Mesh
189
6.7
Router Architecture
190
6.8
A general FIFO Implementation
191
6.9
General format of a packet
192
6.10
Arbiter to control the write access at output data lines
193
6.11
A general wrapper architecture
194
6.12
Implementation of a large reconﬁgurable module on a
Network on Chip
199
6.13
The communication infrastructure on a DyNoC
201
6.14
A impossible placement scenario
203

xxii
List of Figures
6.15
A strongly connected conﬁguration on a DyNoC
204
6.16
Obstacle avoidance in the horizontal direction
206
6.17
Obstacle avoidance in the vertically direction
207
6.18
Placement that cause an extreme long routing path
208
6.19
Router guiding in a DyNoC
209
6.20
DyNoC implementation of a trafﬁc light controller on
a VirtexII-1000
212
7.1
Generation of bitstreams for partial reconﬁguration
215
7.2
Routing tools can route the same signals on different paths
217
7.3
The recommended directory structure for the modular
design ﬂow
219
7.4
Limitation of a PR area to a block (dark) and the actual
dimensions (light)
225
7.5
Scheme of a PR application with a traversing bus that
may not be interrupted
226
7.6
Improved directory structure for the Early Access De-
sign Flow
227
7.7
Usage of the new EA bus macros
229
7.8
Narrow (a) and wide (b) bus macro spanning two or
four CLBs
230
7.9
Three nested bus macros
230
7.10
Scheme of Animated Patterns
235
7.11
Two patterns that can be created with modules of An-
imated Patterns.
Left: the middle beam is moving.
Right: the diagonal stripes are moving
236
7.12
Scheme of Video8
236
7.13
Reconstructing example Video8 to place the partially
reconﬁgurable part and connected modules in the top-level
238
7.14
Moving a partially reconﬁgurable module to the top-
level design on the example of Video8
238
7.15
Modules using resources (pins) not available in their
placement area (the complete column) must use feed-
through signals to access those resources
246
7.16
Illustration of th pin problematique on the RC200-Board
248
7.17
Architecture of the ESM-Baby board
251
7.18
Architecture of the ESM MotherBoard
253
7.19
Intermodule communication possibilities on the ESM
254
7.20
SRAM-based intermodule communication on the ESM
255

List of Figures
xxiii
7.21
Possible enhancement of the Erlangen Slot Machine on
the Xilinx Virtex 4 and Virtex 5 FPGAs
257
8.1
Integration of PCB modules into a single chip: from
system on PCB to SoC
260
8.2
Example of system ingration with CoreConnect buses
266
8.3
Implementation of the OPB for two maters and two slaves
268
8.4
General adaptive multiprocessor hardware infrastructure
271
8.5
Structure of the on chip network
273
8.6
Implementation of the transceiver
274
8.7
The communication protocoll
275
8.8
Automatic hardware generation ﬂow
277
8.9
The Platform-independent Hardware generation tool (PinHat) 279
8.10
The PinHaT framework
280
8.11
software conﬁguration ﬂow
282
8.12
4-Processor infrastructure for the SVD on the ML310-Board 283
9.1
Sliding windows for the search of three words in parallel
288
9.2
FSM recognizers for the word ‘conte’: a) sate diagram,
b) transition table, c) basis structure the hardware im-
plementation: 4 ﬂip ﬂops will be need to code a 5 × 6
transition table
291
9.3
a) Use of the common preﬁx to reduce the number
of ﬂip ﬂops of the common word detector for ‘partir’,
‘paris’, ‘avale’,‘avant’. b) implementation without use
of common preﬁx and common comparator set
291
9.4
Basic structure of a FSM-based words recognizer that
exploits the common preﬁx and a common set of characters
292
9.5
Processing steps of the FSM for the word ‘tictic’
293
9.6
Implementation of a 5 × 5 sliding windows
296
9.7
A modular architecture for video streaming on the ESM
297
9.8
Architecture of a distributed arithmetic datapath
300
9.9
k-parallel distributed arithmetic datapath
301
9.10
Datapath of the distributed arithmetic computation for
ﬂoating-point numbers
304
9.11
An optical multimode waveguide is represented by a
multiport with several transfer paths
305
9.12
Screenshot of the 6-parallel DA implementation of the
recursive convolution equation on the Celoxica RC100-
PP platform
306

xxiv
List of Figures
9.13
Adaptive controller architecture
308
9.14
Adaptive controller architecture. Left: the one slot im-
plementation, and right: the two slot implemenation
310
9.15
Architecture of an adaptive cryptographic system
312
9.16
Architecture of a software deﬁned radio system
315
C.1
Tree View after Top Assembly
350

List of Tables
2.1
Truth table of the Full adder
35
3.1
Language and tools overview for coarse-grained RPUs
72
3.2
Overview of FPGA manufacturers and tool providers
74
4.1
Laplacian matrix of the graph of ﬁgure 4.22
140
6.1
Router Statistics
210
6.2
TLC and CG Statistics
211
7.1
New address ranges to be set in EDK
241
9.1
Results of the recursive convolution equation on differ-
ent platforms
306

Chapter 1
INTRODUCTION
Research in architecture of computer systems has always been a central
preoccupation of the computer science and computer engineering communi-
ties. The investigation goals vary according to the target applications, the price
of the ﬁnal equipment, the programmability of the system, the environment in
which processors will be deployed and many others.
For processors to be used in parallel machines for high-performance com-
puting as it is the case in weather simulation, the focus is placed on high clock
rates, parallelism and high communication bandwidth at the expense of power.
In many embedded systems, the price of the ﬁnal equipment is the governing
factor during the development. A small microcontroller is usually used to con-
trol data acquisition from sensors and provide data to actuators at a very low
frequency. In many other embedded systems, in particular in untethered sys-
tems, power and cost optimization are the central goals. In those systems, the
growing need of more computation power that contradict with power and cost
optimization put a lot of pressure on engineers who must ﬁnd a good balance
of all contradicting goals. For an autonomous cart used to explore a given en-
vironment, the processing unit must be able to capture images, compress the
images and send the compressed images to a base station for control. Parallel
to this, the system must perform other actions such as obstacle detection and
avoidance. In such a system, power must be optimized to allow the system to
run as long as possible. On the other hand, the processor must process image
frames as fast as possible, to avoid important frames to be missed. Obstacle
detection and avoidance must also be done as faster as possible to avoid a pos-
sible crash of the cart. The multiplicity of goals has led to the development
of several processing architectures, each optimized according to a given goal.
Those architectures can be categorized in three main groups according to their
degree of ﬂexibility: the general purpose computing group that is based on the

2
Reconﬁgurable Computing
Von Neumann (VN) computing paradigm; domain-speciﬁc processors, tailored
for a class of applications having in common a great range of characteristics;
application-speciﬁc processors tailored for only one application.
1.
General Purpose Computing
In 1945, the mathematician John Von Neumann demonstrated in a study
of computation that a computer could have a simple, ﬁxed structure, able to
execute any kind of computation, given a properly programmed control, with-
out the need for hardware modiﬁcation. The VN contribution was universally
adopted and quickly became the fundament of future generations of high-speed
digital computers. One of the reasons for the acceptance of the VN approach is
its simplicity of programming that follows the sequential way of human think-
ing.
The general structure of a VN machine as shown in ﬁgure 1.1 consists of:
A memory for storing program and data. Harvard architectures contain two
parallel accessible memories for storing program and data separately.
A control unit (also called control path) featuring a program counter that
holds the address of the next instruction to be executed.
An arithmetic and logic unit (also called data path) in which instructions
are executed.
Figure 1.1.
The Von Neumann Computer architecture

Introduction
3
A program is coded as a set of instructions to be executed sequentially,
instruction after instruction. At each step of the program execution, the next
instruction is fetched from the memory at the address speciﬁed in the program
counter and decoded. The required operands are then collected from the mem-
ory before the instruction is executed. After execution, the result is written
back into the memory. In this process, the control path is in charge of setting
all signals necessary to read from and write to the memory, and to allow the
data path to perform the right computation. The data path is controlled by the
control path, which interprets the instructions and sets the data path’s signals
accordingly to execute the desired operation.
In general, the execution of an instruction on a VN computer can be done in
ﬁve cycles: Instruction Read (IR) in which an instruction is fetched from the
memory; Decoding (D) in which the meaning of the instruction is determined
and the operands are localized; Read Operands (R) in which the operands
are read from the memory; Execute (EX) in which the instruction is executed
with the read operands; Write Result (W) in which the result of the execution
is stored back to the memory. In each of those ﬁve cycles, only the part of
the hardware involved in the computation is activated. The rest remains idle.
For example if the IR cycle is to be performed, the program counter will be
activated to get the address of the instruction, the memory will be addressed
and the instruction register to store the instruction before decoding will be
also activated. Apart from those three units (program counter, memory and
instruction register), all the other units remain idle. Fortunately, the structure
of instructions allows several of them to occupy the idle part of the processor,
thus increasing the computation throughput.
1.1
Instruction Level Parallelism
Pipelining is a transparent way to optimize the hardware utilization as well
as the performance of programs. Because the execution of one instruction
cycle affects only a part of the hardware, idle parts could be activated by having
many different cycles executing together. For one instruction, it is not possible
to have many cycles being executed together. For instance, any attempt to
perform the execute cycle (EX) together with the reading of an operand (R)
for the same instruction will not work, because the data needed for the EX
should ﬁrst be provided by R cycle. Nevertheless, the two cycles EX and D
can be performed in parallel for two different instructions. Once the data have
been collected for the ﬁrst instruction, its execution can start while the data are
being collected for the second instruction. This overlapping in the execution
of instructions is called pipelining or instruction level parallelism (ILP), and it
is aimed at increasing the throughput in the execution of instructions as well as
the resource utilization. It should be mentioned that ILP does not reduce the
execution latency of a single execution, but increases the throughput of a set

4
Reconﬁgurable Computing
of instructions. The maximum throughput is dictated by the impact of hazards
in the computation. Those Hazards can be reduced for example by the use of a
Harvard architecture.
If tcycle is the time needed to execute one cycle, then the execution of one
instruction will require 5∗tcycle to perform. If three instructions have to be exe-
cuted, then the time needed to perform the execution of those three instructions
without pipelining is 15 ∗tcycle, as illustrated in ﬁgure 1.2. Using pipelining,
the ideal time needed to perform those three instruction, when no hazards have
to be dealt with, is 7∗tcycle. In reality, we must take hazards into account. This
increases the overall computation time to 9 ∗tcycle.
The main advantage of the VN computing paradigm is its ﬂexibility, be-
cause it can be used to program almost all existing algorithms. However, each
algorithm can be implemented on a VN computer only if it is coded according
to the VN rules. We say in this case that ‘The algorithm must adapt itself to the
hardware’. Also because of the temporal use of the same hardware for a wide
variety of applications, VN computation is often characterized as ‘temporal
computation’.
With the fact that all algorithms must be sequentially programmed to run on
a VN computer, many algorithms cannot be executed with their potential best
performance. Algorithms that usually perform the same set of inherent parallel
operations on a huge set of data are not good candidates for implementation on
a VN machine.
Figure 1.2.
Sequential and pipelined execution of instructions on a Von Neumann Computer

Introduction
5
If the class of algorithms to be executed is known in advance, then the pro-
cessor can be modiﬁed to better match the computation paradigm of that class
of application. In this case, the data path will be tailored to always execute the
same set of operations, thus making the memory access for instruction fetching
as well as the instruction decoding redundant. Moreover, the memory access
for data fetching and storing can also be avoided if the sources and destinations
of data are known in advance. A bus could for instance provide sensor data to
the processor, which in turn sends back the computed data to the actuators
using another bus.
2.
Domain-Speciﬁc Processors
A domain-speciﬁc processor is a processor tailored for a class of algorithms.
As mentioned in the previous section, the data path is tailored for an optimal
execution of a common set of operations that mostly characterizes the algo-
rithms in the given class. Also, memory access is reduced as much as possible.
Digital Signal Processor (DSP) belong to the most used domain-speciﬁc pro-
cessors.
A DSP is a specialized processor used to speed-up computation of repeti-
tive, numerically intensive tasks in signal processing areas such as telecommu-
nication, multimedia, automobile, radar, sonar, seismic, image processing, etc.
The most often cited feature of the DSPs is their ability to perform one or more
multiply accumulate (MAC) operations in single cycle. Usually, MAC opera-
tions have to be performed on a huge set of data. In a MAC operation, data
are ﬁrst multiplied and then added to an accumulated value. The normal VN
computer would perform a MAC in 10 steps. The ﬁrst instruction (multiply)
would be fetched, then decoded, then the operand would be read and multiply,
the result would be stored back and the next instruction (accumulate) would be
read, the result stored in the previous step would be read again and added to
the accumulated value and the result would be stored back. DSPs avoid those
steps by using specialized hardware that directly performs the addition after
multiplication without having to access the memory.
Because many DSP algorithms involve performing repetitive computations,
most DSP processors provide special support for efﬁcient looping. Often a
special loop or repeat instruction is provided, which allows a loop implementa-
tion without expending any instruction cycles for updating and testing the loop
counter or branching back to the top of the loop. DSPs are also customized for
data with a given width according to the application domain. For example if a
DSP is to be used for image processing, then pixels have to be processed. If the
pixels are represented in Red Green Blue (RGB) system where each colour is
represented by a byte, then an image processing DSP will not need more than
8 bit data path. Obviously, the image processing DSP cannot be used again for
applications requiring 32 bits computation.

6
Reconﬁgurable Computing
This specialization of the DSPs increases the performance of the processor
and improves the device utilization. However, the ﬂexibility is reduced, be-
cause it cannot be used anymore to implement other applications other than
those for which it was optimally designed.
3.
Application-Speciﬁc Processors
Although DSPs incorporate a degree of application-speciﬁc features such
as MAC and data width optimization, they still incorporate the VN approach
and, therefore, remain sequential machines. Their performance is limited. If
a processor has to be used for only one application, which is known and ﬁxed
in advance, then the processing unit could be designed and optimized for that
particular application. In this case, we say that ‘the hardware adapts itself to
the application’.
In multimedia processing, processors are usually designed to perform the
compression of video frames according to a video compression standard. Such
processors cannot be used for something else than compression. Even in com-
pression, the standard must exactly match the one implemented in the proces-
sors. A processor designed for only one application is called an Application-
Speciﬁc Processor (ASIP). In an ASIP, the instruction cycles (IR, D, EX, W)
are eliminated. The instruction set of the application is directly implemented
in hardware. Input data stream in the processor through its inputs, the proces-
sor performs the required computation and the results can be collected at the
outputs of the processor. ASIPs are usually implemented as single chips called
Application-Speciﬁc Integrated Circuit (ASIC)
Example 1.1 If algorithm 1 has to execute on a Von Neumann computer,
then at least 3 instructions are required.
Algorithm 1
if a < b then
d = a + b
c = a · b
else
d = b + 1
c = a −1
end if
With tcycle being the instruction cycle, the program will be executed in 3 ∗
5 ∗tcycles = 15 ∗tcycle without pipelining.
Let us now consider the implementation of the same algorithm in an ASIP.
We can implement the instructions d = a + b and c = a ∗b in parallel. The
same is also true for d = b + 1, c = a −1 as illustrated in ﬁgure 1.3

Introduction
7
Figure 1.3.
ASIP implementation of Algorithm 1
The four instructions a+b, a∗b, b+1, a−1 as well as the comparison a < b
will be executed in parallel in a ﬁrst stage. Depending on the value of the com-
parison a < b, the correct values of the previous stage computations will be
assigned to c and d as deﬁned in the program. Let tmax be the longest signal
needed by a signal to move from one point to another in the physical implemen-
tation of the processor (this will happen on the path Input-multiply-multiplex).
tmax is also called the cycle time of the ASIP processor. For two inputs a and
b, the results c and d can be computed in time tmax. The VN processor can
compete with this ASIP only if 15 ∗tcycle < tmax, i.e. tcycle < tmax/15. The
VN must be at least 15 times faster than the ASIP to be competitive. Obviously,
we have assumed a VN without pipeline. The case where a VN computer with
a pipeline is used can be treated in the same way.
ASIPs use a spatial approach to implement only one application. The func-
tional units needed for the computation of all parts of the application must be
available on the surface of the ﬁnal processor. This kind of computation is
called ‘Spatial Computing’.
Once again, an ASIP that is built to perform a given computation cannot be
used for other tasks other than those for which it has been originally designed.

8
Reconﬁgurable Computing
4.
Reconﬁgurable Computing
From the discussion in the previous sections, where we studied three
different kinds of processing units, we can identify two main means to charac-
terize processors: ﬂexibility and performance.
The VN computers are very ﬂexible because they are able to compute any
kind of task. This is the reason why the terminology GPP (General Pur-
pose Processor) is used for the VN machine. They do not bring so much
performance, because they cannot compute in parallel. Moreover, the ﬁve
steps (IR, D, R, EX, W) needed to perform one instruction becomes a major
drawback, in particular if the same instruction has to be executed on huge
sets of data. Flexibility is possible because ‘the application must always
adapt to the hardware’ in order to be executed.
ASIPs bring much performance because they are optimized for a particular
application. The instruction set required for that application can then be
built in a chip. Performance is possible because ‘the hardware is always
adapted to the application’.
If we consider two scales, one for the performance and the other for the ﬂex-
ibility, then the VN computers can be placed at one end and the ASIPs at the
other end as illustrated in ﬁgure 1.4.
-
-
Figure 1.4.
Flexibility vs performance of processor classes

Introduction
9
Between the GPPs and the ASIPs are a large numbers of processors. De-
pending on their performance and their ﬂexibility, they can be placed near or
far from the GPPs on the two scales.
Given this, how can we choose a processor adapted to our computation
needs? If the range of applications for which the processor will be used is large
or if it is not even deﬁned at all, then the GPP should be chosen. However, if
the processor is to be used for one application like it is the case in embedded
systems, then the best approach will be to design a new ASIP optimized for
that application.
Ideally, we would like to have the ﬂexibility of the GPP and the performance
of the ASIP in the same device. We would like to have a device able ‘to adapt
to the application’ on the ﬂy. We call such a hardware device a reconﬁgurable
hardware or reconﬁgurable device or reconﬁgurable processing unit (RPU)
in analogy the Central Processing Unit (CPU). Following this, we provide a
deﬁnition of the term reconﬁgurable computing. More on the taxonomy in
reconﬁgurable computing can be found in [111] [112].
Definition 1.2 (Reconfigurable Computing) Reconﬁgurable com-
puting is deﬁned as the study of computation using reconﬁgurable devices.
For a given application, at a given time, the spatial structure of the device
will be modiﬁed such as to use the best computing approach to speed up that
application. If a new application has to be computed, the device structure will
be modiﬁed again to match the new application. Contrary to the VN comput-
ers, which are programmed by a set of instructions to be executed sequentially,
the structure of reconﬁgurable devices are changed by modifying all or part
of the hardware at compile-time or at run-time, usually by downloading a so-
called bitstream into the device.
Definition 1.3 (Configuration, Reconfiguration) Conﬁguration
respectively reconﬁguration is the process of changing the structure of a recon-
ﬁgurable device at star-up-time respectively at run-time
Progress in reconﬁguration has been amazing in the last two decades. This is
mostly due to the wide acceptance of the Field Programmable Gate Array (FP-
GAs) that are now established as the most widely used reconﬁgurable devices.
The number of workshops, conferences and meetings dealing with this topics
has also grown following the FPGA evolution. Reconﬁgurable devices can be
used in a wide number of ﬁelds, from which we list some in the next section.
5.
Fields of Application
In this section, we would like to present a non-exhaustive list of ﬁelds, where
the use of reconﬁguration can be of great interest. Because the ﬁeld is still
growing, several new ﬁelds of application are likely to be developed in the
future.

10
Reconﬁgurable Computing
5.1
Rapid Prototyping
Rapid prototyping is certainly one of the most important ﬁelds of application
of reconﬁguration. The development of an ASIC that can be seen as physical
implementation of ASIPs is a cumbersome process consisting of several steps,
from the speciﬁcation down to the layout of the chip and the ﬁnal production.
Because of the different optimization goals in development, several teams of
engineers are usually involved. This increases the Non-recurring engineering
(NRE) cost that can only be amortized if the ﬁnal product is produced in a
very large quantity. Contrary to software development, errors discovered af-
ter the production mean enormous loss because the produced pieces become
either unusable or a great adaptation effort must be spent for the deployment.
Rapid prototyping allows a device to be tested in real hardware before the ﬁnal
production. In this sense, errors can be corrected without affecting the pieces
already produced. A reconﬁgurable device is useful here, because it can be
used several times to implement different versions of the ﬁnal product until
an error-free state. One of the concepts related to rapid prototyping is hard-
ware emulation, in analogy to software simulation. With hardware emulation,
a hardware module is tested under real operating conditions in the environment
where it will be deployed later.
5.2
In-System Customization
Time to market has become one of the main challenges that electronic man-
ufacturers face today. In order to secure market segments, manufacturers must
release their products as quickly as possible. In many cases, a well working
product can be released with less functionalities. The manufacturer can then
upgrade the product on the ﬁeld to incorporate new functionalities. While this
approach works with normal VN processors, it is not the case with ASIPs. A
reconﬁgurable device provides such capabilities of being upgraded in the ﬁeld,
by changing the conﬁguration.
In-system customization can also be used to upgrade systems that are de-
ployed into non-accessible or very difﬁcult to access locations. One example
is the Mars rover vehicle in which some FPGAs, which can be modiﬁed from
the earth, are used.
5.3
Multi-modal Computation
The number of electronic devices we interact with is permanently increas-
ing. Many people hold besides a mobile phone, other devices such as hand-
helds, portable mp3 player, portable video player, etc. Besides those mobile
devices, ﬁxed devices such as navigation systems, music and video players as
well as TV devices are available in cars and at home. All those devices are
equipped with electronic control units that run the desire application on the

Introduction
11
desired device. Furthermore, many of the devices are used in a time multi-
plexed fashion. It is difﬁcult to imagine someone playing mp3 songs while
watching a video clip and given a phone call. For a group of devices used
exclusively in a time multiplexed way, only one electronic control unit can be
used. Whenever a service is needed, the control unit is connected to the cor-
responding device at the correct location and reconﬁgured with the adequate
conﬁguration. For instance, a domestic mp3, a domestic DVD player, a car
mp3, a car DVD player as well as a mobile mp3 player and a mobile video
player can all share the same electronic unit, if they are always used by the
same person. However, if several persons have access to the same devices (this
can happen in a household with several people), then sharing of the electronic
control unit will be rather difﬁcult. In the ﬁrst case, the user just needs to re-
move the control unit from the domestic devices and connect them to one car
device when going to work. The control unit can be removed from the car and
connected to a mobile device if the user decides to go for a walk. Coming back
home, the electronic control unit is removed from the mobile device and used
for watching video.
5.4
Adaptive Computing Systems
Advances in computation and communication are helping the development
of ubiquitous and pervasive computing. Computer anytime and everywhere is
now becoming reality.
The design of ubiquitous computing system is cumbersome task that cannot
be dealt with only at compile time. Because of uncertainty and unpredictability
of such systems, it is impossible, at compile time, to address all scenarios that
can happen at run-time, because of unpredictable changes in the environment.
We need computing systems that are able to adapt their behavior and structure
to change operating and environmental conditions, to time-varying optimizing
objectives, and to physical constraints such as changing protocols and new
standards. We call those computing systems Adaptive Computing System.
Reconﬁguration can provide a good fundament for the realization of adap-
tive systems, because it allows system to quickly react to changes by adopting
the optimal behavior for a given run-time scenario.
6.
Organization of the Book
The rest of the book is organized in nine chapters, ranging from the archi-
tecture to the applications. In those chapters, we try to provide a mixture of the
theoretical as well as the practical background needed to understand, develop
and deploy reconﬁgurable systems.
Architecture of reconﬁgurable systems: After a brief tour in the earlier
systems, this chapter considers the technology as well as the coupling pos-
sibilities of reconﬁgurable systems, from the ﬁne-grained look up table

12
Reconﬁgurable Computing
(LUT)-based reconﬁgurable systems like the ﬁeld programmable gate ar-
rays (FPGA) to the new coarse-grained technology.
Design and implementation: This chapter deals with the implementation
on reconﬁgurable system. It covers the steps needed (design entry, func-
tional simulation, logic synthesis, technology mapping, place and route and
bit stream generation) to implement today’s FPGAs. We focus deeply on
the logic synthesis for FPGAs, in particular LUT technology mapping.
High-Level Synthesis for Reconﬁgurable Devices: This chapter consid-
ers the high-level synthesis for reconﬁgurable systems, also known as tem-
poral partitioning. It covers the implementation of large functions, which
cannot ﬁt into one RPU. Several temporal partitioning techniques are pre-
sented and explained.
Temporal placement: In this chapter, stand-alone reconﬁgurable systems
are considered. We assume that a kind of operating systems for reconﬁg-
urable systems is in charge of managing the resources of a given system
and allocate space on a device for the computation of incoming tasks. We
therefore present several temporal placement approaches for ofﬂine as well
as online placement.
Online Communication: Modules dynamically placed at run-time on a
given device need to communicate with each other in order to exchange
data. Therefore, they dynamically create a need of communication channels
on the chip. This chapter reviews and explains the different approaches to
solve this dynamic intercommunication need.
Designing for Partial Reconﬁguration on Xilinx Virtex FPGA: This
chapter considers the implementation for partial reconﬁguration on the Xil-
inx FPGAs that are one of the few one on the market with this feature. We
present the different possibilities to produce the partial bitstream needed
at run-time to reconﬁgure only part of the device. Also, based on a case
study, some hints are provided on the design of viable platforms for partial
reconﬁguration.
System on Programmable Chip: System on programmable chip is a hot
topic in reconﬁgurable computing. This is mainly the integration of a sys-
tem made upon some peripheral (UART, Ethernet, VGA, etc.), but also
computational (coding, ﬁlter, etc.) hardware modules on one programmable
chip. We present the current usable solutions. Furthermore, we focus on
the development of adaptive multiprocessors on chip. Those are systems
consisting of a set of Processors and exchangeable hardware accelerators
connected in a network for a parallel implementation of applications on the
chip.

Introduction
13
Applications: This section presents applications of reconﬁgurable systems.
It covers the use of reconﬁgurable system in computer architecture (rapid
prototyping, reconﬁgurable supercomputer, reconﬁgurable massively par-
allel computers) and algorithm better adapted for reconﬁgurable systems
(distributed arithmetic, pattern matching, control, software deﬁned radio,
cryptography, etc.).

Chapter 2
RECONFIGURABLE ARCHITECTURES
In the previous chapter, we presented the reconﬁgurable devices as piece of
hardware able to dynamically adapt to algorithms. How can it be possible that
a hardware device, whose structure is normally ﬁxed at fabrication time and
cannot be changed anymore during the lifetime, can be readapted at run-time
to dynamically match the application requirements? In this chapter, we bring
some light into this issue by presenting the architecture of the commonly used
reconﬁgurable devices.
After taking a brief tour of the evolution of reconﬁgurable computing archi-
tectures, we will cover in the ﬁrst part of the chapter the so-called ﬁne-grained
reconﬁgurable devices. Those are hardware devices, whose functionality can
be modiﬁed on a very low level of granularity. For instance, the device can
be modiﬁed such as to add or remove a single inverter or a single two input
NAND-gate. Fine-grained reconﬁgurable devices are mostly represented by
programmable logic devices (PLD). This consists of programmable logic ar-
rays (PLA), programmable array logics (PAL), complex programmable logic
devices (CPLD) and the ﬁeld programmable gate arrays (FPGA).
In the second part of the chapter, we cover the pseudo reconﬁgurable de-
vice also called coarse-grained reconﬁgurable devices. Those devices usually
consist of a set of coarse-grained elements such as ALUs that allow for the
efﬁcient implementation of a dataﬂow function.
1.
Early Work
Attempts to have a ﬂexible hardware structure that can be dynamically mod-
iﬁed at run-time to compute a desired function are almost as older as the de-
velopment of other computing paradigms. To overcome the non ﬂexibility
of the ﬁrst computer, the ENIAC (electronical numerical integrator and com-
puter) that could be programmed only by handwriting an algorithm, Jon Von

16
Reconﬁgurable Computing
Neumann proposed a ﬁrst universal architecture made upon three main blocks
(memory, datapath and control path), able to run any given and well-coded
program. The Von Neumann approach was not intended to provide the most
efﬁcient hardwired structure for each application, but a platform to run all type
of programs without spending too much effort in the rearrangement of the un-
derlying hardware. The previous effort of Von Neumann was then pursued
by other researchers with goal of always having the best computation struc-
ture of a given application. We present some of those investigations in this
section.
1.1
The Estrin Fix-plus machine
In 1959, Gerald Estrin, a computer scientist of the university of California
at Los Angeles, introduced the concept of reconﬁgurable computing. The fol-
lowing fragment of an Estrin publication in 1960 [81] the ﬁx-plus machine,
deﬁnes the concept of reconﬁgurable computing paradigm.
“Pragmatic problem studies predicts gains in computation speeds in a va-
riety of computational tasks when executed on appropriate problem-oriented
conﬁgurations of the variable structure computer. The economic feasibility of
the system is based on utilization of essentially the same hardware in a variety
of special purpose structures. This capability is achieved by programmed or
physical restructuring of a part of the hardware.”
To implement its vision, Estrin designed a computing system, the ﬁx-plus
machine [81]. Like many reconﬁgurable computing systems available today,
the ﬁx-plus machine consists of three main elements as shown in ﬁgure 2.1.
A high-speed general purpose computer, the ﬁxed part (F) that can be im-
plemented on any general purpose processor. In its ﬁx-plus machine, the
IBM 7090 were used.
A variable part (V) consisting of various size high-speed digital substruc-
tures that can be reorganized in problem-oriented special purpose conﬁgu-
rations.
The supervisory control (SC) coordinate operations between the ﬁx module
and the variable module.
The variable part (V) was made upon a set of problem-speciﬁc optimized
functional units in the basic conﬁguration (trigonometric functions, logarithm,
exponentials, n-th power, roots, complex arithmetic, hyperbolic, matrix
operation).
The basic block modules (ﬁgure 2.2) could be inserted into any of 36 po-
sitions on a motherboard (ﬁgure 2.12(a)) that provides the functionality for a
given application. The functionality of the motherboard could be manually

Reconﬁgurable Architectures
17
Figure 2.1.
Structure of the Estrin Fix-Plus Machine
modiﬁed by replacing some basic blocks by new ones. Two types of basic
building were available in the ﬁx-plus machine: The ﬁrst basic element that
entailed four ampliﬁers and associated input logic for signal inversion, ampli-
ﬁcation, or high-speed storage. The second basic block used as combinatoric
that was made upon ten diodes and four output drivers.
Figure 2.2.
The basic building blocks of Fix-Plus Machine

18
Reconﬁgurable Computing
Figure 2.3.
The wiring harness of the Estrin-Machine
The connection between the modules is done by wiring harness (ﬁgure 2.3).
With this architecture, the reconﬁguration was done manually by replacing
some modules on the motherboard (ﬁgure 2.4) or by changing a wiring harness
for a new connection among the existing modules.
The ﬁx-plus machine was intended to be used for accelerating Eigenval-
ues computation of matrices and has shown a speed gain of 2.5–1000 over the
IBM7090 [80] [79]. The available technology at that time however made the
use of the ﬁx-plus machine difﬁcult. Reconﬁguration had to be done manu-
ally, and substantial software efforts were required to implement applications
(ﬁgure 2.5).
Figure 2.4.
The motherboard of the Estrin’s Fix-Plus

Reconﬁgurable Architectures
19
Figure 2.5.
Estrin at work: Hand-Controlled Reconﬁguration
1.2
The Rammig Machine
In the year 1977, Franz J. Rammig, a researcher at the university of Dort-
mund proposed a concept for editing hardware [182]. The goal was the “in-
vestigation of a system, which, with no manual or mechanical interference,
permits the building, changing, processing and destruction of real (not simu-
lated) digital hardware”.
Rammig materialized his concept by developing a hardware editor similar
to the today’s FPGA architecture. The editor was made upon a set of modules,
a set of pins and a one-to-one mapping function on the set of pins. The circuitry
of a given function was then deﬁned as a “string” on an alphabet of two letters
(w = “wired” and u = “unwired”). To build the hardware editor, selectors were
provided with the modules’ outputs connected to the input of the selectors and
the output of the selectors connected to the input of the modules. The overall
system architecture is shown in ﬁgure 2.6.
The implementation of the {wired, unwired} property was done through
a programmable crossbar switch, made upon an array of selectors. The bit
strings were provided by storing the selector control in registers, and by mak-
ing these registers accessible from a host computer, the PDP11 in those days.
The modules were provided on a library board similar to that of Estrin’s Fix-
Plus. Each board could be selected under software control. The mapping from
module I/Os to pins was realized manually, by a wiring of the provided library
boards, i.e. ﬁxed per library board.

20
Reconﬁgurable Computing
Figure 2.6.
Structure of the Rammig machine
The Rammig machine was heavily used as emulation platform, which is
also one of the largest application ﬁelds of today’s FPGAs. It was therefore
possible to control the complete behaviour of a circuit under observation from
the software side. This was done by buffering module outputs in registers
and transferring the contents of these registers to the host before clocking the
system. The system was implemented with a 128 × 192 crossbar and got the
name META-46 GOLDLAC (ﬁgure 2.7).
1.3
Hartenstein’s XPuter
The Xputer’s [113] [216] concept was presented in early 1980s by Reiner
Hartenstein, a researcher at the University of Kaiserslautern in Germany.
Figure 2.7.
META-46 GOLDLAC

Reconﬁgurable Architectures
21
The goal was to have a very high degree of programmable parallelism in the
hardware, at lowest possible level, to obtain performance not possible with the
Von Neumann computers. Instead of sequencing the instructions, the Xputer
used to sequence data, thus exploiting the regularity in the data dependen-
cies of some class of applications like in image processing, where a repeti-
tive processing is performed on a large amount of data. An Xputer consists
of three main parts: the data sequencer, the data memory and the reconﬁg-
urable ALU (rALU) that permits the run-time conﬁguration of communica-
tion at levels below instruction set level. Within a loop, data to be processed
were accessed via a data structure called scan window. Data manipulation
was done by the rALU that had access to many scan windows. The most es-
sential part of the data sequencer was the generic address generator (GAG)
that was able to produce address sequences corresponding to the data of up to
three nested loops. An rALU subnet that could be conﬁgured to perform all
computations on the data of a scan window was required for each level of a
nested loop.
The general XPuter architecture is presented in ﬁgure 2.8. Shown is the
realization of the XPuter as a map oriented machine (MoM). The overall sys-
tem was made upon a host processor, whose memory was accessible by the
MoM.
The rALU subnets received their data directly from local memory or from
the host main memory via the MoM bus. Communication was also possible
Figure 2.8.
General architecture of the XPuter as implemented in the Map oriented Machine
(MOM-3) prototype

22
Reconﬁgurable Computing
among the rALUs via direct serial connections. Several XPuters could also be
connected to provide more performance.
For executing a program, the hardware had to be conﬁgured ﬁrst. If no
reconﬁguration would take place at run-time, then only the data memory would
be necessary. Otherwise, a conﬁguration memory would be required to hold
all the conﬁgurations to be used at run-time.
The basic building block of the reconﬁgurable ALU was the so-called recon-
ﬁgurable datapath unit (rDPU) (ﬁgure 2.9). Several rDPUs were used within
an rALU for data manipulation. Each rDPU had two registered inputs and two
registered outputs with a datawidth of 32 bit. Input data were provided either
from the north or from the west, while the south and east were used for the
output. Besides the interconnection lines for the rALUs, a global I/O-Bus is
available for the connection of designs to the external world. The I/O bus was
principally used for accessing the scan windows.
The global view of the reconﬁgurable datapath attached to a host processor
is given on ﬁgure 2.9. It consists of two main parts: the control unit and a ﬁeld
of rDPUs. The register ﬁle was used for optimizing the memory access when
the GAG operated with overlapping scan windows. In this case, data in the
actual scan window position will be reused in the following positions. Those
data could therefore be temporary stored in registers and copied back into the
memory if they were no more needed.
The control implemented a program that is loaded on reconﬁguration to
control different units of the rALU. Its instruction set consisted of instructions
for loading the data as well as instructions for collecting results from the ﬁeld.
Application of the XPuters was in image processing, systolic array and sig-
nal processing.
Figure 2.9.
Structure of the XPuter’s reconﬁgurable ALU

Reconﬁgurable Architectures
23
The introduction of commercial FPGAs by Xilinx in the mid-1980s in-
creased interest in reconﬁgurable computing. Many systems in high-performance
computing were built with farm of FPGAs on different boards. The “draw-
backs" of FPGAs also inspired many research groups that propose new archi-
tectures. We brieﬂy present some of the most popular experiments. Included
are the PAM developed at the DEC research center in Paris, the SPLASH sys-
tem of the Supercomputer Research Center, the PRISM, the DISC and the
GARP.
1.4
The PAM Machine
Introduced by Bertin et al. [25] from the Digital Equipment corporation, a
programmable active memory (PAM) is deﬁned as a uniform array of identical
cells, the processing array blocks (PAB) that are connected in a given regular
fashion. The architecture is built on the FPGA model that will be presented
later in this chapter. The basic building block in a PAM is a cell, which consist
of a ﬂip ﬂop and a combinatorial element with four inputs and four outputs.
The combinatorial element consists of ﬁve look-up tables, each of which im-
plements an output as a function of the ﬁve inputs. One hundred and sixty bits
are, therefore, necessary to conﬁgure a combinatorial element. The ﬂip ﬂop
provides a memory for the realization of ﬁnite state machines but can be used
for local data storage. Figure 2.10 shows a PAM consisting of a 4 × 4 array of
PAB.
Like in the FPGAs, the PAM conﬁguration is deﬁned by a bitstream that set
the functions of the combinatorial elements as well as the state of the ﬂip ﬂops.
A prototype of the PAM, the Perle was built using a 5 × 5 array of Logic
Array Cell (LCA) , a CMOS cell designed by Xillinx. Perle-0 features a VME
Figure 2.10.
Programmable Active Memory (PAM) architecture as array of (PABs)

24
Reconﬁgurable Computing
bus to interface a host CPU to which it is coupled. The host is in charge
of downloading the bitstream to the Perle. The conﬁguration controller and
the host-bus communication protocol are programmed into two extra LCAs,
statically conﬁgured at power-up time from a PROM.
Example of applications for which the Perle was used to implement are data
compression, cryptography are image processing and, energetic physics.
1.5
The SPLASH II
SPLASH II [97], [39] is a computing system made upon a set of FPGA
modules solded on different printed circuit boards, connected together to build
a massive parallel computer. The system consists of a set of boards, called
array boards (ﬁgure 2.11), that can be reconﬁgured according to the applica-
tion currently being implemented. The array boards are connected to a host
computer (in this case a SUN SPARC Station 2) via a system Bus (SBUS)
and an interface board. Each array board features 17 Xilinx XC4010 FPGAs.
Although sixteen of the seventeen FPGAs are used for computation, the seven-
teenth is use for data broadcasting. Each of the FPGAs has access to a separate
512-Kbyte memory. The SLASH II can be viewed as a large serial data path
from the interface board to the array board. The SIMD bus is used to transmit
data to the ﬁrst FPGA (X0) on each board, which can then broadcast the data
to the other sixteen FPGAs on its array. Alternatively, the data can also be
moved in a linear way on an array board, from FPGA to FPGA. In this case,
the data are received by the ﬁrst FPGA (X1) that serially transmit the data to
the next FPGAs on the board. The data are transmitted to the next board by the
last FPGA in the chain.
Some examples of applications that have beneﬁted from the SPLASH II
performance are image processing, text searching in genetic database and ﬁn-
gerprint matching.
1.6
The PRISM Paradigm
PRISM is an acronym for processor reconﬁguration through instruction set
metamorphosis. Developed by Athanas et al [12], the idea behind the PRISM
is to use information extracted at compile-time from a given application and to
build a new set of instructions tailored to that application. The so-generated in-
structions can then be implemented in a reconﬁgurable hardware unit, tightly
coupled to the processor. To ease the programmability of such systems, the
identiﬁcation and synthesis of the operations in hardware should be done au-
tomatically and transparent to the user. This task is performed by a conﬁgura-
tion compiler, which automatically performs a hardware/software partitioning.
This results in the generation of a software and a hardware image. The software
image is a sequential program to be executed on the processor, whereas the

Reconﬁgurable Architectures
25
Figure 2.11.
Architecture of the SPLASH II array board
hardware image deﬁnes the conﬁguration of the attached reconﬁgurable hard-
ware. As proof of concept, a PRISM prototype consisting of a 10-MHz proces-
sor M68010 attached to a board with four Xilinx 3090 FPGAs was constructed
and tested.
1.7
The Garp Approach
The approach proposed by Hauser and Wawrzynek [114] is very close to
today’s embedded FPGA systems. The Garp was proposed as attempt to over-
come the obstacles of reconﬁgurable computing machines, mostly consisting
of a processor attached to a set of FPGA boards. The problems identiﬁed
were for instance, the small capacity of FPGA, their expensive reconﬁgura-
tion time, the non availability of internal memory to temporally stored data
to be computed and the lack of a clear deﬁned standard for coupling FPGAs
to processor. Hauser and Wawrzynek proposed a system consisting of a mi-
croprocessor and a reconﬁgurable array on the same die. The system was
not different than those already proposed. However, having the processor on
the same die with the reconﬁgurable device was a great beneﬁt. The time
for transferring data to the reconﬁgurable array could be drastically reduced.
Instructions and data caches were also foreseen on the chip. Almost all the
FPGAs available today on the market offer a similar structure, with processors
that can be synthesized according to the user’s need. Despite the nice con-
cept and positive simulation results, a Garp-chip could not be produced and
commercialized.

26
Reconﬁgurable Computing
1.8
DISC
The purpose of the dynamic instruction set computer (DISC) presented in
[219] is to support demand-driven instruction set modiﬁcations. In contrast to
the PRISM approach, where the speciﬁc instructions are synthesized and ﬁxed
at compiled-time, the DISC approach uses partial reconﬁguration of FPGAs
to place on the FPGA, hardware modules, each of which implements a given
instruction. The relocation was also proposed as a means to reduced defrag-
mentation of the FPGA. Because of its partial reconﬁguration capabilities, the
national semiconductor conﬁgurable logic array (Clay) was chosen for build-
ing a prototype consisting of a printed circuit board with two CLA31 FPGA
and some memory. Although the ﬁrst FPGA was used to control the conﬁg-
uration process, the second FPGA was used for implementing the instruction
speciﬁc hardware blocks. Via an ISA Bus, the board was attached to a host
processor running on Linux. A simple image mean ﬁlter ﬁrst implemented as
application-speciﬁc module, and later as sequence of general purpose instruc-
tions, was used to show the viability of the platform.
1.9
The DPGA
With similar goals as the Garp architecture, Dehon’s dynamically programm-
able gate array was proposed as a means to extend the capability of micro-
processor [68]. However, DGPAs provide a better implementation of the re-
conﬁgurable array proposed in the Garp. The concept proposed by Dehon
is used in many coarse-grained devices available today.
To reduce the re-
conﬁguration time, one of the main bottlenecks of early FPGAs, the DGPA
should be able to quickly switch among preloaded conﬁgurations.
There-
fore, redundant look-up tables are used to broadcast conﬁgurations in a local
area on a cycle-by-cycle basis, thus allowing a clockwise reconﬁguration of
the FPGAs.
Despite the recent development of ﬂexible coarse-grained device, almost
all systems that require ﬂexibility in the hardware structure rely on the pro-
grammable devices to which the PALs and PLAs, the CPLDs and the FPGA
belongs. In the next section, we provide a short view on the structure of those
devices.
2.
Simple Programmable Logic Devices
Programmable logic arrays (PLA) and programmable array logic (PAL)
consist of a plane of AND-gates connected to a plane of OR-gates. The in-
puts signals as well as their negations are connected to the inputs of the AND-
gates in the AND-plane. The outputs of the AND-gates are use as input for the

Reconﬁgurable Architectures
27
OR-gate in the OR-plane whose outputs correspond to those of the PAL/PLA.
The connections in the two planes are programmable by the user.
Because every Boolean function can be written as a sum of products, the
PLAs and PALs can be used to program any Boolean function after fabrica-
tion. The products are implemented in the AND-plane by connecting the wires
accordingly, and the sums of the product are realized in the OR-plane. While in
the PLAs both ﬁelds can be programmed by the user, this is not the case in the
PALs where only the AND-plane is programmable. The OR-plane is ﬁxed by
the manufacturer. Therefore, the PALs can be seen as a subclass of the PLAs.
Example 2.1 Figure 2.12 shows the PLA and PAL implementations of the
functions F1 = A · C + A · B and F2 = A · B + B · C. While the OR-
plane of the PAL is no more programmable, by modifying the connection in the
OR-Plane of the PLA, different sums of the products can be generated.
Further enhancements (feeding the OR-output to a ﬂip ﬂop or feeding back a
OR-output to a AND-input) are made on PLAs to allow more complex circuits
to be implemented.
PALs and PLAs are well suited to implement two-level circuits; those are
circuits made upon the sum of product as described earlier. At the ﬁrst level,
all the products are implemented and all the sum are implemented on the
second level.
The main limitation of PLAs and PALs is their low capacity, which is due to
the nature of the AND-OR-plane. The size of the plane grows too quickly as
the number of inputs increases. Because of their low complexities, PALs and
PLAs belong to the class of devices called simple programmable logic devices
(SPLD).
(a) PAL: The OR-connections are ﬁxed
(b) PLA: Programmable OR-connections
Figure 2.12.
PAL and PLA implementations of the functions F1 = A · C + A · B and
F2 = A · B + B · C

28
Reconﬁgurable Computing
3.
Complex Programmable Logic Device
As stated in the previous section, PALs and PLAs are only available in small
sizes, equivalent to a few hundred logic gates. For large logic circuits, complex
programmable logic devices (CPLD) can be used.
A CPLD consists of a set of macro cells, input/output blocks and an in-
terconnection network. The connection between the input/output blocks and
the macro cells and those between macro cells and macro cells can be made
through the programmable interconnection network (ﬁgure 2.13). A macro
cell typically contains several PLAs and ﬂip ﬂops. Despite their relative large
capacity (few hundreds thousands of logic gates), compared to those of PLAs,
CPLDs are still too small for using in reconﬁgurable computing devices. They
are usually used as glue logic, or to implement small functions. Because of
their non volatility, CPLDs are used in many systems for conﬁguration of the
main reconﬁgurable device at start up.
Figure 2.13.
Structure of a CPLD device
4.
Field Programmable Gate Arrays
Introduced in 1985 by the company Xilinx, an FPGA is a programmable
device consisting, like the CPLDs, of three main parts. a set of programmable
logic cells also called logic blocks or conﬁgurable logic blocks, a programmable
interconnection network and a set of input and output cells around the device
(ﬁgure 2.14). A function to be implemented in FPGA is partitioned in mod-
ules, each of which can be implemented in a logic block. The logic blocks are
then connected together using the programmable interconnection. All three
basic components of an FPGA (logic block, interconnection and input output)

Reconﬁgurable Architectures
29
Figure 2.14.
Structure of an FPGA
can be programmed by the user in the ﬁeld. FPGAs can be programmed once
or several times depending on the technology used.
4.1
Technology
The technology deﬁnes how the different blocks (logic blocks, interconnect,
input/output) are physically realized. Basically, two major technologies exist:
antifuse and memory-based. Whereas the antifuse paradigm is limited to the
realization of interconnections, the memory-based paradigm is used for the
computation as well as the interconnections. In the memory-based category,
we can list the SRAM the EEPROM and the Flash based FPGAs.
4.1.1
Antifuse
Contrary to a fuse, an antifuse is normally an open circuit. An antifuse-
based FPGA uses special antifuses included at each connection customization
point. The two-terminal elements are connected to the upper and lower layer
of the antifuse, in the middle of which a dielectric is placed (ﬁgure 2.15). In its
initial state, the high resistance of the dielectric does not allow any current to
ﬂow between the two layers. Applying a high voltage causes large power dis-
sipation in a small area, which melts the dielectric. This operation drastically
reduces the resistance and a link can be built, which permanently connects
the two layers. The two types of antifuses actually commercialized are: The
Programmable Low-Impedance Circuit Element (PLICE) (2.15(a)), which is

30
Reconﬁgurable Computing
(a) Q-Logic Vialink antifuse
(b) Actel PLICE antifuse
Figure 2.15.
Antifuse FPGA Technology
manufactured by the company Actel and the Metal Antifuse also called ViaLink
made by the company QuickLogic.
The PLICE antifuse consists of an Oxygen-Nitrogen-Oxygen (ONO) dielec-
tric layer sandwiched between a polysilicon and an n+ diffusion layer that
serves as conductor. The ViaLink antifuse (2.15(b)) is composed of a sandwich
of very high resistance layer of programmable amorphous silicon between two
metal layers. When a programming voltage is applied, a metal-to-metal link is
formed by permanently converting the silicon to a low resistance state.
The main advantage of the antifuse chips is their small area and their signif-
icantly lower resistance and parasitic capacitance compared with transistors.
This helps to reduce the RC delays in the routing. However, anti-fuse-based
FPGAs are not suitable for devices that must be frequently reprogrammed, as
it is the case in reconﬁgurable computing. Antifuse FPGAs are normally pro-
grammed once by the user and will not change anymore. For this reason, they
are also known as one-time programmable FPGAs.
4.1.2
SRAM
Unlike the antifuse that is used mostly used to conﬁgure the connection, a
static RAM (SRAM) is use to conﬁgure the logic blocks and the connection as
well. SRAM-based FPGAs are the most widely used.
In an SRAM-based FPGA, the states of the logic blocks, i.e. their function-
ality bits as well as that of the interconnections, are controlled by the output of
SRAM cells (ﬁgure 2.16).
As shown in ﬁgure 2.17(a), a connection between two wires can be done
by a pass transistor connected to the wires (one wire on a terminal). The
state of the transistor can then be controlled by the output of an SRAM cell
that allows the current to ﬂow or not to ﬂow between the two wires. Copy-
ing different values into the SRAM allows for changing the behaviour of the
connection.

Reconﬁgurable Architectures
31
Figure 2.16.
A Xillinx SRAM cell
The same principle applies for the programming of the logic modules, the
so-called function generators. The selector input of a multiplexer (ﬁgure 2.17(b))
as well as the content of a look-up table representing the values of a function
for different input combinations (ﬁgure 2.17(c)) can be reprogrammed by just
copying new values into the corresponding SRAM cells.
The basic SRAM conﬁguration cell (ﬁgure 2.16) is constructed from two
cross-coupled inverters and uses a standard CMOS-process. The Q-output of
the SRAM cell is connected to the module (pass transistor, multiplexer or Look
up table) that is controlled by conﬁguration. The read-and-write word line is
use either to read or to write data from or to the SRAM through the data line.
The major advantage of this technology is that FPGAs can be programmed
(conﬁgured) indeﬁnitely. We just need to change the value into the SRAM-
cells to realize a new connection or a new function. Moreover, the device can
be done in-circuit very quickly and allow the reconﬁguration to be done on
the ﬂy.
The disadvantages of SRAM-based FPGAs are the chip area required by the
SRAM approach is relatively large. The total size of an SRAM-conﬁguration
cell plus the transistor switch that the SRAM-cell drives is also larger than the
programming devices used in the antifuse technology. Furthermore, the device
is volatile, i.e. the conﬁguration of the device stored in the SRAM-cells is lost
if the power is cut off. Therefore, external storage or non-volatile devices such
(a) Wire connection
(b) MUX programming
(c) Function generation
Figure 2.17.
Use of SRAM in FPGA-Conﬁguration

32
Reconﬁgurable Computing
as CPLDs, EPROM or Flash devices, are required to store the conﬁguration
and load it into the FPGA-device at power-on.
4.1.3
EPROM
Erasable programmable read only memory (EPROM) devices are based on
a ﬂoating gate (ﬁgure 2.18(a)). The device can be permanently programmed
by applying a high voltage (10–21 V) between the control gate and the drain
of the transistor (12 V). This causes the ﬂoating gate to be permanently and
negatively charged. This negative potential on the ﬂoating gate compensates
the voltage on the control gate and keeps the transistor closed.
In an ultra violet (UV) PROM, the programming process can be reversed
by exposing the ﬂoating gate to UV-light. This process reduces the threshold
voltage and makes the transistor function normally. For this purpose, the device
must be removed from the system in which it operates and plug into a special
device.
In electrically erasable and programmable ROM (EEPROMs) as well as
in ﬂash-EPROM, the erase operation is accomplished electrically, rather than
by exposure to ultraviolet light. A high negative voltage must therefore be
applied at the control gate. This process is faster than using a UV lamp, and
the chip does not have to be removed from the system. In EEPROM-based
devices, two or more transistors are typically used in a ROM cell: one access
and one programmed transistor. The programmed transistor performs the same
function as the ﬂoating gate in an EPROM, with both charge and discharge
being done electrically.
In the ﬂash-EEPROMs that are used as logic tile cell in the Actel ProASIC
chips (ﬁgure 2.18(b)), two transistors share the ﬂoating gate, which store the
programming information. The sensing transistor is only used for writing and
veriﬁcation of the ﬂoating gate voltage whereas the other is used as switch.
This can be used to connect or disconnect routing nets to or from the conﬁgured
logic. The switch is also used to erase the ﬂoating gate.
(a) EPROM Cell
(b) Actel Flash-EEPROM
Figure 2.18.
EEPROM Technology

Reconﬁgurable Architectures
33
4.2
Function generators
A reconﬁgurable hardware device should provide the users with the possibil-
ity to dynamically implement and reimplement new functions. This is usually
done by means of function generators that can be seen as the basic computing
unit in the device. Two types of function generators are in use in commercial
FPGAs: the multiplexers and the look-up tables.
4.2.1
Multiplexer
A 2:1 (2n-input-1) multiplexer (MUX) is a selector circuit with 2n inputs
and one output. Its function is to allow only one input line to be fed at the
output. The line to be fed at the output can be selected using some selector
inputs. To select one of the 2n possible inputs, n selectors lines are required.
An MUX can be used to implement a given function. The straightforward way
is to place the possible results of the function at the 2n inputs of the MUX and
to place the function arguments at the selector inputs. In this case, the MUX
will work like a look-up table that will be explained in the next section. Several
possibilities exist to implement a function in a MUX, for instance by using
some arguments as inputs and other as selectors. Figure 2.19 illustrates this
case for the function f = a×b. The argument a is used as input in combination
with a second input 0 and the second argument b is used as selector.
The Shannon expansion theorem can be used to decompose a function and
implement it into a MUX. This theorem states that a given Boolean logic func-
tion F(x1, . . . , xn) of n variables can be written as shown in the following
equation.
F(x1, · · · , xn) = F(x1, · · · , xi = 1, · · · , xn) × xi +
F(x1, · · · , xi = 0, · · · , xn) × xi
Where F(x1, · · · , xi = 1, · · · , xn) is the function obtained by replacing xi
with one and F(x1, · · · , xi = 0, · · · , xn) the function obtained by replacing
xi by zero in F. The functions F1 = F(x1, · · · , xi = 1, · · · , xn) and F2 =
F(x1, · · · , xi = 0, · · · , xn) are called cofactors.
Figure 2.19.
Implementation of f=ab in a 2-input MUX

34
Reconﬁgurable Computing
A 2:1 multiplexer with inputs I0, I1, output O and selector S is deﬁned by
the following equation: O(S, I0, I1) = S(I0) + S(I1). The function F can be
implemented on a 2:1 MUX with inputs I0 = F0 and I1 = F1, and selector
S = xi.
A complex function can be implemented in many multiplexers connected
together. The function is ﬁrst broken down into small pieces. Each piece is
then implemented on a multiplexer. The multiplexers will then be connected
to build the given function. This process is called technology mapping and is
part of the logic synthesis that we consider in the next chapter.
To implement a function on a 2n:1 MUX, the Shannon expansion theorem
should be reﬁned such as to have n ﬁxed variables and 2n cofactors. For n = 2,
the function of the multiplexer is deﬁned as follows:
O(S0, S1, I0, I1, I2, I3) = S0S1(I0)+S0S1(I1)+S0S1(I2)+S0S1(I3) (4.1)
This is equivalent to the Shannon reﬁnement of a function with two ﬁxed
variables as described below:
F (x1,··· ,xn)=F (x1,··· ,xi=1,··· ,xn)·xi+F (x1,··· ,xi=0,··· ,xn)·xi
=[F (x1,··· ,xi=1,··· ,xj=1,..xn)·xi+F (x1,··· ,xi=0,··· ,xj=1,··· ,xn)·xi]xj
+[F (x1,··· ,xi=1,··· ,xj=0,··· ,xn)·xi+F (x1,··· ,xi=0,··· ,xj=0,··· ,xn)·xi]xj
=F (x1,··· ,xi=1,··· ,xj=1,··· ,xn)·xixj+F (x1,··· ,xi=0,··· ,xj=1,··· ,xn)·xixj
+F (x1,··· ,xi=1,··· ,xj=0,··· ,xn)·xixj+F (x1,··· ,xi=0,··· ,xj=0,··· ,xn)·xixj
I0=F0=F (x1,··· ,xi=0,··· ,xj=0,··· ,xn),
I1=F1=F (x1,··· ,xi=0,··· ,xj=1,··· ,xn),
I2=F2=F (x1,··· ,xi=1,··· ,xj=0,··· ,xn),
I3=F3=F (x1,··· ,xi=1,··· ,xj=1,··· ,xn)
Example 2.2 We consider the implementation of a full adder using 4:1 MUX.
Let ai, bi be the operand and ci−1 be the carry on a previous level, then si is
the sum and ci the carry for the next level. According to the truth table of the
full adder (table 2.1), we can write si and ci as:
si = aibici−1 + aibici−1 + aibici−1 + aibici−1
ci = aibi ∗0 + aibici−1 + aibici−1 + aibi ∗1
According to those two equations and to the function of a 4:1 MUX given
above, two multiplexers can be conﬁgured as shown in ﬁgure 2.20.
As stated earlier, an MUX provides much more possibilities than look-up
tables, because the inputs of the MUX, which are equivalent of the SRAM

Reconﬁgurable Architectures
35
ai
bi
ci−1
si
ci
0
0
0
0
0
0
0
1
1
0
0
1
0
1
0
0
1
1
0
1
1
0
0
1
0
1
0
1
0
1
1
1
0
0
1
1
1
1
1
1
Table 2.1.
Truth table of the Full adder
output to a look-up table, are not reserved to the function result like as it is the
case in look-up tables. Function arguments can be connected to the MUX-input
as well as to he selector inputs.
The Actel ACT X Logic module.
Multiplexer are used as function genera-
tors in the Actel FPGA devices (ﬁgure 2.21). In the Actel ACT1 device, the
basic computing element is the logic module. It is an 8-input 1-output logic
circuit that can implement a wide range of functions. Besides combinatorial
functions, the logic module can also implement a variety of D-latches. The
C-modules present in the second generation of Actel devices, the ACT2, are
similar to the logic module. The S-modules, which are found in the second and
third generation of actel devices, contain an additional dedicated ﬂip-ﬂop. This
avoids the building of ﬂip ﬂop from the combinatorial logic as it is the case in
the logic module.
4.2.2
Look-Up Tables
A look-up tables (LUT) is a group of memory cells, which contain all the
possible results of a given function for a given set of input values. Usually,
Figure 2.20.
Implementation of a Full adder using two 4-input one output MUX

36
Reconﬁgurable Computing
Figure 2.21.
The Actel basic computing blocks uses multiplexers as function generators
the set of possible function values corresponds to the possible combinations of
the inputs. The values of the function are stored in such a way that they can
be retrieved by the corresponding input values. An n-input LUT can be used
to implement up to 22n different functions, each of which can take 2n possi-
ble values. Therefore, an n-input LUT must provide 2n cells for storing the
possible values of an n-input function. In FPGAs, an LUT physically consists
of a set of SRAM-cells to store the values and a decoder that is used to ac-
cess the correct SRAM location and retrieve the result of the function, which
corresponds to the input combination (ﬁgure 2.22).
To implement a complex function in an LUT-based FPGA, the function must
be divided into small pieces, each of which can be implemented in a single
Figure 2.22.
2-input LUT

Reconﬁgurable Architectures
37
LUT. The interconnections are used to connect small pieces together and form
the complete function
Example 2.3 The full adder of the previous section can be implemented us-
ing two 3-inputs, 1 output (3:1) LUT as shown in ﬁgure 2.23. The sum is
implemented in the ﬁrst LUT and the carry-out in the second LUT.
The ﬁrst three columns of table 2.1 represent the input values. They build
the address used to retrieve the function value (corresponding to the value
in the third column) from the corresponding LUT location. The content of
the fourth and ﬁfth columns of the truth table must therefore be copied in the
corresponding LUTs as shown in ﬁgure 2.23. The sum values are copied in the
upper LUT, while the carry values are compiled in the lower LUT.
SRAM-based LUT is used in the most commercial FPGAs as function gen-
erators. Several LUTs are usually grouped in a large module in which other
functional elements such as ﬂip ﬂops and multiplexers are available. The con-
nection between the LUTs inside such modules is faster than connections via
the routing network, because dedicated wires are used. We consider examples
of devices using LUT as function generator, the Xilinx FPGA and those of
Altera, and we next explain how the LUT is used in those devices.
The Xilinx Conﬁgurable Logic Block.
The basic computing block in the
Xilinx FPGAs consists of an LUT with variable number of inputs, a set of
multiplexers, arithmetic logic and a storage element (ﬁgure 2.24).
The LUT is used to store the conﬁguration whereas the multiplexers select
the right inputs for the LUT and the storage element as well as the right output
of the block.
The arithmetic logic provides some facilities such as XOR-gate and faster
carry chain to build faster adder without wasting too much LUT-resources.
Figure 2.23.
Implementation of a full adder in two 3-input LUTs

38
Reconﬁgurable Computing
Figure 2.24.
Basic block of the Xilinx FPGAs
Several basic computing blocks are grouped in a coarse-grained element
called the conﬁgurable logic block (CLB)(ﬁgure 2.25). The number of basic
blocks in a CLB varies from device to device. In the older devices such as the
4000 series, the Virtex and Virtex E and the Spartan devices, two basic blocks
were available in a CLB. In the newer devices such as the Spartan 3, the Virtex
II, the Virtex II-Pro and the Virtex 4, the CLBs are divided into four slices each
of which contains two basic blocks. The CLBs in the Virtex 5 devices contain
only two slices, each of which contains four basic blocks.
In the newer devices, the left part slices of a CLB, also called SLICEM, can
be conﬁgured either as combinatorial logic, or can be use as 16-bit SRAM or
Figure 2.25.
CLB in the newer Xilinx FPGAs (Spartan 3, Virtex 4 and Virtex 5)

Reconﬁgurable Architectures
39
as shift register while right-hand slices, the SLICEL, can only be conﬁgured as
combinatorial logic.
Except for the Virtex 5, all LUTs in Xilinx devices have four inputs and one
output. In the Virtex 5 each LUT has six inputs and two outputs. The LUT can
be conﬁgured either as a 6-input LUT, in which case only one output can be
used, or as two 5-input LUTs, in which case each of the two outputs is used as
output of a 5-input LUT.
The Altera Logic Array Block.
Like the Xilinx devices, Altera’s FPGAs
(Cyclone, FLEX and Stratix) are also LUT-based. In the Cyclone II as well
as in the FLEX architecture, the basic unit of logic is the logic element (LE)
that typically contains an LUT, a ﬂip ﬂop, a multiplexer and additional logic
for carry chain and register chain. Figure 2.26 shows the structure of the logic
element in the Cyclone FPGA. This structure is very similar to that of the
Altera FLEX devices. The LEs in the cyclone can operate in different modes
each of which deﬁnes different usage of the LUT inputs.
In the Stratix II devices, the basic computing unit is called adaptive logic
module (ALM) (ﬁgure 2.27). The ALM is made upon a mixture of 4-input
and 3-input LUTs that can be used to implement logic functions with variable
number of inputs. This ensures a backward compatibility to 4-input-based
designs, while providing the possibility to implement coarse-grained module
with variable number (up to 8) inputs. Additional modules including ﬂip ﬂops,
adders and carry logic are also provided.
Altera logic cells are grouped to form coarse-grained computing elements
called logic array blocks (LAB). The number of logic cells per LAB varies
from the device to device. The Flex 6000 LABs contains ten logic elements
while the FLEX 8000 LAB contains only eight. Sixteen LEs are available for
each LAB in the cyclone II while the Stratix II LAB contains eight ALMs.
Figure 2.26.
Logic Element in the Cyclone II

40
Reconﬁgurable Computing
Figure 2.27.
Stratix II Adaptive Logic Module
4.3
FPGA structures
FPGAs consist of a set of programmable logic cells placed on the device
such as to build an array of computing resources. The resulting structure is
vendor-dependant. According to the arrangement of logic blocks and the inter-
connection paradigm of the logic blocks on the device, FPGAs can be classiﬁed
in four categories: symmetrical array, row-based, hierarchy-based and sea of
gates (ﬁgure 2.28). We next explain each of the mentioned structure based on
a commercial example.
4.3.1
Symmetrical Array: The Xilinx Virtex and Atmel AT40K
Families
A symmetrical array-based FPGA consists of a two-dimensional array of
logic blocks immersed in a set of vertical and horizontal lines. Switch elements
exist at the intersections of the vertical and horizontal lines to allow for the
connections of vertical and horizontal lines.
Examples of FPGAs arranged in a symmetrical array-based are the Xilinx
Virtex FPGA and the Atmel (ﬁgure 2.29).
On the Xilinx devices, CLBs are embedded in the routing structure that
consists of vertical and horizontal wires. Each CLB element is tied to a switch
matrix to access the general routing structure, as shown in ﬁgure 2.30(a). The
switch matrix provides programmable multiplexers, which are used to select
the signals in the given routing channel that should be connected to the CLB
terminals. The switch matrix can also connect vertical and horizontal lines,
thus making routing possible on the FPGA.
Each CLB has access to two tri-state driver (TBUF) over the switch matrix.
Those can be used to drive on-chip busses. Each tri-state buffer has its own tri-

Reconﬁgurable Architectures
41
Figure 2.28.
The four basic FPGA structures
state control pin and its own input pin that are controlled by the logic built in the
CLB. Four horizontal routing resources per CLB are provided for on-chip tri-
state busses. Each tri-state buffer has access alternately to two horizontal lines,
which can be partitioned as shown in ﬁgure 2.30(b). Besides the switch matrix,
CLBs connect to their neighbours using dedicated fast connexion tracks.
The routing is done on the Atmel chips using a set of busing planes. Seven
busing planes are available on the AT40K. Figure 2.31 depicts a part of the
plane with ﬁve identical busing planes. Each plane has three bus resources: a
local-bus resource (the middle bus) and two express-bus resources (both sides).
Repeaters are connected to two adjacent local-bus segments and two express-
bus segments. Local bus segments span four cells whereas an express bus
segments span eight cells. Long tri-state bus can be created by bypassing a
repeater.

42
Reconﬁgurable Computing
(a) The Xilinx Virtex II
(b) Atmel’s symmetrical array arrangement
Figure 2.29.
Symmetrical array arrangement in a) the Xilinx and b) the Atmel AT40K FPGAs
(a) CLB connexion to the switch matrix
(b) Tri-state buffer connection to horizontal lines
Figure 2.30.
Virtex routing resource
Figure 2.31.
Local connection of an Atmel Cell

Reconﬁgurable Architectures
43
Locally, the Atmel chip provides a star-like connection resource that allows
each cell (which is the basic unit of computation) to be connected directly to
all its eight neighbours. Figure 2.31 depicts direct connections between a cell
and its eight nearest neighbours.
4.3.2
Row-Based FPGAs: The Actel ACT3 Family
A row-based FPGA consists of alternating rows of logic block or macro
cells and channels (ﬁgure 2.32). The space between the logic blocks is called
channel and is used for signal routing.
The Actel ACT3 FPGA family (ﬁgure 2.32) is an example of row-based
FPGA. The macro cells are the C-elements and S-elements presented earlier in
this chapter. The routing is done via the horizontal direction using the chan-
nels. In the vertical direction, dedicated vertical tracks are used. As shown
in ﬁgure 2.33, a channel consists of several routing tracks divided into seg-
ments. The minimum length of a segment is the width of a module pair and
its maximum length is the length of a complete channel, i.e. the width of the
device.
Any segment that spans more than one-third of the row length is considered
a long horizontal segment. Non dedicated horizontal routing tracks are used
to route signal nets. Dedicated routing tracks are used for the global clock
networks and for power and ground tracks. Vertical tracks (ﬁgure 2.33) are of
Figure 2.32.
Row based arrangement on the Actel ACT3 FPGA Family

44
Reconﬁgurable Computing
Figure 2.33.
Actel’s ACT3 FPGA horizontal and vertical routing resources
three types: input, output and long. They are also divided into more segments.
Each segment in an input track is dedicated to the input of a particular module,
and each segment in the output track is dedicated to the output of a particular
module. Long segments are uncommitted and can be assigned during routing.
Each output segment spans four channels (two above and two below) except
near the top and the bottom of the array. Vertical input segments span only the
channel above or the channel below. The tracks dedicated to module inputs
are segmented by pass transistors in each module row. During normal user
operation the pass transistors are inactive, which isolate the inputs of a module
from the inputs of the module above it.
The connections inside Actel FPGAs are established using antifuse. Four
types of antifuse connections exist for the ACT3: horizontal-to-vertical (XF)
connection, horizontal-to-horizontal (HF) connection, vertical-to-vertical (FF)
connection and fast-vertical connection (ﬁgure 2.33).
4.3.3
Sea-of-gates: The Actel ProASIC family
Like in symmetrical arrays, the macro cells are arranged on a two-dimensional
array structure such that an entry in the array correspond to the coordinate of
a given macro cell. The difference between the symmetrical array and the
sea-of-gate is that there is no space left aside between the macro cells for
routing.
The interconnection wires are fabricated on top of the cells. The Actel
ProASIC FPGA family is an implementation of the sea-of-gate approach. The
ProASIC core consists of a sea-of-gates called sea-of-tiles. The macro cells
are the EEPROM-based tiles previously seen. The device uses a four level of
hierarchy routing resource to connect the logic tiles: the local resources, the
long-line resources, the very long-line resources and the global networks. The
local resources allow the output of the tile to be connected to the inputs of one

Reconﬁgurable Architectures
45
Figure 2.34.
Actel ProASIC local routing resources
of the eight surrounding tiles (ﬁgure 2.34). The long-line resources provide
routing for longer distances and higher fanout connections. These resources,
which vary in length (spanning one, two, or four tiles), run both vertically and
horizontally and cover the entire device. The very long lines span the entire
device. They are used to route very long or very high fanout nets.
4.3.4
Hierarchical-based: The Altera Cyclone, Flex and Stratix
families
In hierarchical based FPGAs, macro cells are hierarchically placed on the
device. Elements with the lowest granularity are at the lowest level hierarchy.
They are grouped to form the elements of the next level. Each element of a
level i consists of a given number of elements from level i −1.
Altera FPGAs (FLEX, Cyclone II and Stratix II) have two hierarchical lev-
els. The logic cells (in the Cyclone II and FLEX) and the ALM in the Stratix
II are on the lowest level of the hierarchy. The logic array blocks (LABs) build
the higher level (ﬁgure 2.35). Each LAB contains a given number of logic ele-
ments (eight for the FLEX8000, ten for the FLEX6000, sixteen for the Cyclone
and eight AMLs for the Stratix II). The LABs in turn are arranged as array on
the device.
Signal connections to and from device pin are provided via a routing struc-
ture called FastTrack in the FLEX and MultiTrack in Cyclone II and Stratix
II). The FastTrack as well as the MultiTrack interconnects consist of a series of
fast, continuous row and column channels that run the entire length and width
of the device.

46
Reconﬁgurable Computing
Figure 2.35.
Hierarchical arrangement on the Altera Stratix II FPGA
Signals between LEs or ALMs in the same LAB and those in the adjacent
LABs are routed via local interconnect signals (ﬁgure 2.36). Each row of a
LAB is served by a dedicated row interconnect, which routes signals between
LABs in the same row. The column interconnect routes signals between rows
and routes signals from I/O pin rows.
Figure 2.36.
LAB connection on the Altera Stratix devices

Reconﬁgurable Architectures
47
A row channel can be driven by an LE (or ALM in the Stratix II) or by
one of two column channels. Each column of LABs is served by a dedicated
column interconnect. The LEs in an LAB can drive the column interconnect,
which can then drive another row’s interconnect, to route the signals to other
LABs in the device. A signal from the column interconnect must be routed to
the row interconnect before it can enter an LAB (ﬁgure 2.36).
LEs can drive global control signals. This is helpful for distributing the
internally generated clock, asynchronous clear and asynchronous preset signals
and high-fan-out data signals.
4.3.5
Programmable I/O
Located around the periphery of the device, I/O components allow for the
communication of a design inside the FPGA with off-chip modules. Like the
logic cells and the interconnections, FPGA I/Os are programmable, which al-
lows designs inside the FPGA to conﬁgure a single interface pin as input, out-
put or bidirectional.
The general structure of an I/O component is shown in ﬁgure 2.37: It con-
sists of an input block, an output block and an output enable block for driving
the tri-state buffer. Two registers that are activated either by the falling or by
the rising edge of the clock are available in each block.
The I/Os can be parameterized for a single data rate (SDR) or a double data
rate (DDR) operation mode. Whereas in the SDR-mode, data are copied into
the I/O registers on the rising clock edge only, the DDR mode exploits the
falling clock edge and the rising clock edge to copy data into the I/O registers.
On the input, output, tri-state, one of the double data rate (DDR) register can be
Figure 2.37.
General structure of an I/O component

48
Reconﬁgurable Computing
used. The double data rate is directly accomplished by the two registers on each
path, clocked by the rising edge (or the falling edge) from different clock nets.
DDR input can be done using both input registers whereas DDR output will
use both output registers. In the Altera Stratix II, the I/O component is called
I/O element (IOE). The IOEs are located in I/O blocks around the periphery of
the device.
The I/O blocks, which contain up to four IOEs, are used to drive the rows
and columns interconnects.
They are divided in two groups: The row I/O
blocks, which drive row, column or direct link interconnects, whereas the col-
umn I/O blocks drive column interconnects.
The Xilinx Virtex I/O components are called I/O block (IOB), and they are
provided in groups of two or four on the device boundary. The IOB can be used
independent from each other as input and/or output, or they can be combined
in group of two to be used as differential pair directly connected to a switch
matrix.
4.4
Hybrid FPGAs
The process technology as well as the market demand is pushing manufac-
turers to include more and more pre-designed and well-tested hard macros in
their chips. Resources, such as memory, that are used in almost all designs can
be directly be found on the chip. This allows the designer to use well-tested and
efﬁcient modules. Moreover, hard macros are more efﬁciently implemented
and are faster than macro implemented on the universal function generators.
The resources often available on hybrid FPGAs are RAMs, clock managers,
arithmetic modules, network interface modules and processors. The market
demand has pushed almost all the manufactures to include hard macros in all
their devices. Because the resources required by the users vary with their ap-
plication class, some manufacturers, such as Xilinx, provided different classes
of FPGAs, each of which is suited for a given purpose. The most emerging
classes as classiﬁed by Xilinx are the system on chip (SoC), digital signal pro-
cessing (DSP) and pure logic. The system on chip class to which the Virtex
4 FX belongs is characterized by embedded processors directly available on
the chip, memory and dedicated bus resources. Reference designs also exist,
which efﬁciently use the chip resource to get the maximum performance. The
DSP class, which contains the Virtex 4 SX is characterized by the abundance of
multipliers macros and the pure logic class, to which the Virtex 4 LX belongs,
is dominated by LUT function generators.
Figure 2.38 depicts the architecture of the Xilinx Virtex II Pro, one of the
ﬁrst FPGAs to include Hard macro, among which complete processors.
The Xililnx Virtex II Pro contains up to four embedded IBM Power PC 405
RISC hard core processors, which can be clocked at more than 300 MHZ.
Embedded high-speed serial RocketIO transceivers with up to 3.125 Gb/s per

Reconﬁgurable Architectures
49
Figure 2.38.
Structure of a Xilinx Virtex II Pro FPGA with two PowerPC 405 Processor blocks
channel, internal BlockRAM memory module to be used as dual-ported RAM,
embedded 18 × 18-bit multiplier blocks, digital clock manager (DCM) to pro-
vide self-calibrating, fully digital solution for clock distribution delay com-
pensation, clock multiplication and division, and coarse-and ﬁne-grained clock
phase shifting are available on the chip.
5.
Coarse-Grained Reconﬁgurable Devices
FPGAs allow for programming any kind of function as far as this can ﬁt
onto the device. This is only possible because of the low granularity of the
function generators (LUT and MUX). However, the programmable intercon-
nections used to connect the logic blocks reduce the performance of FPGAs.
A way to overcome this is to provide frequently used module as hard macro,
as it is the case in hybrid FPGAs, and therefore, to allow programmable inter-
connections only between processing elements available as hard macros on the
chip. Coarse-grained reconﬁgurable devices follow this approach. In general,
those devices are made upon a set of hard macros (8-bit, 16-bit or even a 32-bit
ALU), usually called processing element (PE). The PEs are able to carry few
operations such as addition, subtraction or even multiplication. The intercon-
nection is realized either through switching matrices or dedicated busses. The
conﬁguration is done by deﬁning the operation mode of the PEs and program-
ming the interconnection between the processing elements.
In the last couple of years, several coarse-grained architectures were built
by different companies, some of which went bankrupt. Despite all the hope
placed in coarse-grained reconﬁgurable device, they fail until now to provide

50
Reconﬁgurable Computing
a large acceptance, and their future is not really easy to predict. This does not
mean that the philosophy behind coarse-grained device is wrong. Companies
investigating coarse-grained reconﬁgurable devices must also face the FPGA
competition, dominated by large companies that provides many coarse-grained
elements in their devices according to the market need.
A wide variety of coarse-grained reconﬁgurable devices that we classify
into three categories will be presented in this section: in the ﬁrst category,
the dataﬂow machines, functions are usually built by connecting some PEs to
build a functional unit that is used to compute on a stream of data. In the sec-
ond category are the network-based devices in which the connection between
the PEs is done using messages instead of wires. The third category are the
embedded FPGA devices, which consist of a processor core that cohabit with
a programmable logic on the same chip.
5.1
Dataﬂow Machines
Dataﬂow machines are the most dominating coarse-grained reconﬁgurable
devices. In this section, we present three of those architectures: the PACT-XPP,
the NEC-DRP and the PicoChip devices.
5.1.1
The PACT XPP device
The idea behind the PACT XPP architecture [17] [176] is to efﬁciently com-
pute streams of data provided from different sources such as A/D converters
rather than single instructions as it is the case in the Von-Neumann computers.
Because the computation should be done while data are streaming through the
processing elements, it is suitable to conﬁgure the PEs to adapt to the natural
computation paradigm of a given application or part of it at a given time. The
eXtreme Processing Platform (XPP) architecture of PACT consist of:
An array of processing array elements (PAE) grouped in processing array
(PA)
A communication network
A hierarchical conﬁguration tree
Memory elements aside the PAs
A set of I/O elements on each side of the device.
One conﬁguration manager (CM) attached to a local memory is responsible
for writing conﬁguration onto a PA. The conﬁguration manager together with
PA build the processing array cluster (PAC). An XPP chip contains many PACs
arranged as grid array on the device. Figure 2.39 shows an XPP device with
four PACs, each of which contains 4 PAEs and surrounded by memory blocks.
The CMs at a lower level are controlled by a CM at the next higher level. The

Reconﬁgurable Architectures
51
Figure 2.39.
Structure of the PACT XPP device
root CM at the highest level is attached to an external conﬁguration memory
and supervises the whole device conﬁguration.
The Processing Array Element (PAE).
There exist two different kinds of
PAEs: the ALU PAE and the RAM-PAE. An ALU-PAE contains an ALU that
can be conﬁgured to perform basic arithmetic operations, whereas the RAM-
PAE is used for storing data.
The back-register (BREG) provides routing
channels for data and events from bottom to top, additional arithmetic and
register functions whereas the forward-register (FREG) is used for routing the
signals from top to bottom and for the control of dataﬂow using event sig-
nals. All objects can be connected to horizontal routing channels using switch-
objects. Dataﬂow register (DF-Registers) can be used at the object output for
data buffering in case of a pipeline stall. Input registers can be pre-loaded by
conﬁguration data and always provide single cycle stall.
A RAM-PAE is similar to an ALU-PAE. However, instead of an ALU, a dual
ported RAM is used for storing data. The RAM generates a data packet after
an address was received at the input. Writing to the RAM requires two data
packet: one for the address and the other for the data to be written. Figure 2.40
shows an ALU-PAE. The structure is the same for a RAM-PAE; however, an
RAM is used instead of the ALU.
Routing and Communication.
The XPP interconnection network consists
of two independent networks: one for data transmission and the other for event
transmission (ﬁgure 2.40). These two networks consist of horizontal and ver-
tical channels. The vertical channels are controlled by the BREG and FREG
whereas connection to horizontal channel is done via switch elements. Besides
the horizontal and vertical channels a conﬁguration bus exists, which allows the
CMs to conﬁgure the PAEs.

52
Reconﬁgurable Computing
Figure 2.40.
The XPP ALU Processing Array Element. The structure of the RAM ALU is
similar.
Horizontal buses are used to connect a PAE within a row whereas the ver-
tical buses are used to connect objects to a given horizontal bus. Vertical con-
nections are done using conﬁgurable switch objects that segment the vertical
communication channels. The vertical routing is enabled using register-objects
integrated into the PAEs.
Interfaces.
XPP devices provide communication interfaces aside the chip.
The number may vary from device to device. The XPP64-A1 for example
contain six external interfaces consisting of four identical general purpose I/O
interfaces on the chip corner (bottom left, upper left, bottom right and upper
right), one conﬁguration manager interface and a JTAG compliant interface for
debugging and testing purpose.
The I/O interfaces can operate independently from each other either in RAM,
or in streaming mode. In streaming mode, each I/O element provides two bidi-
rectional ports for data streaming. Handshake signals are used for synchro-
nization of data packets to external ports. In RAM mode, each port can access
external synchronous SRAMs with 24-bit addresses and 24-bit data. Control
signals for the SRAM transactions are available such that no extra logic is
required.
The conﬁguration manager interface consists of three subgroups of signals:
code, message send and message receive.
The code group provides chan-
nels over which conﬁguration data can be downloaded to the device whereas
the send and receive groups provide communication channels with a host
processor.
5.1.2
The NEC DRP Architecture
We now present the NEC dynamically reconﬁgurable processor (DRP), which
operates in a similar way as the PACT. However, the reconﬁguration control is

Reconﬁgurable Architectures
53
Figure 2.41.
Structure of the NEC Dynamically Reconﬁgurable Processor
not
done
in
a hierarchical
fashion
such
as
in the
PACT devices.
Figure 2.41 shows the overall structure of a NEC DRP. It consist of
an array of byte-oriented processing elements
a programmable interconnection network to connect the processing ele-
ments
a sequencer (State Transition Controller) which can be programmed as ﬁ-
nite state machine to control the dynamic reconﬁguration process on the
device
memory blocks to store conﬁguration and computation data. The memory
blocks are arranged around the device
various interfaces and RAM controllers such as PCI, PLL, SDRAM/SRAM
As shown in ﬁgure 2.42, a DRP processing element contains an ALU for
byte-oriented arithmetic and logic operation, a data management unit to han-
dle byte selects, shift, mask and constant generation.
The operand can be
fetched from the PE’s register ﬁle or collected directly from the PE’s inputs.
The results can be stored in the PE’s register ﬁle or can be sent to the output.
The conﬁguration is done by the state transition controller that set a pointer
to a corresponding instruction register according to the operation mode of the

54
Reconﬁgurable Computing
Figure 2.42.
The DRP Processing Element
ALU. Having many conﬁguration registers allow for storing conﬁguration data
directly in the proximity of the PEs and allow a fast switching from one con-
ﬁguration to the next one.
5.1.3
The picoChip Reconﬁgurable Device
A picochip consists of a picoArray core and a set of different interfaces for
external connection of various module such as memory and processors. The
picoArray core has a similar structure such as the NEC DRP and the PACT;
however, the connection between the PES is done at the column and row in-
tersection. The PC102 Chip for example contains hundreds of array elements,
each with a versatile 16-bit processor with local data memory connected by
a programmable interconnect structure (ﬁgure 2.43). The architecture is het-
erogeneous, with four types of processing element all having a common basic
structure, but optimized for different tasks: the standard AE (STAN), the con-
trol AE (CTRL), the Memory AE (MEM) and the function accelerator unit
(FAU).
A standard AE type includes multiply-accumulate peripheral as well as spe-
cial instructions optimized for CDMA operations. The memory AE contains
multiply unit and additional memory. The function accelerator unit is a co-
processor optimized for speciﬁc signal-processing tasks. The control AE is
equipped with a multiply unit and larger amounts of data and instruction mem-
ory optimized for the implementation of base station control functionality.
Multiple elements can be programmed together as a group to perform par-
ticular function. The device can be reconﬁgured at run-time to run different
applications such as wireless protocols. Several interfaces are available for

Reconﬁgurable Architectures
55
Figure 2.43.
Structure of the picoChip device
seamless external connection (ﬁgure 2.43): each chip has four inter processor
communications links that can be used to build an array of several picoChips to
implement larger and complex functions that cannot ﬁt in only one picoChip.
The communication on those links is based on a time division multiplexing
(TDM) protocol scheme. Besides the inter processor communication, a micro-
processor interface is available for connecting an external processor that can
be used to conﬁgured the device and stream data into the device. External stor-
age (EEPROM, FLASH) can be connected as well to this interface to allow
a self reconﬁguration of the device on start up. Other interfaces are provided
among which a memory interface for connection of external memory and a
JTAG interface for debugging purpose.
5.2
Network-oriented architectures
Despite the interest on networks on chip as communication paradigm has
grown recently, very few reconﬁgurable devices rely on message passing for
data exchange among the PEs. One of the few companies to have implemented
this concept is Quicksilver Tech, a start-up that failed to commercialized their
technology and, recently, went bankrupt. Nevertheless, we believe that their
architecture merits more attention. We therefore decide to present some details
on it in this section.

56
Reconﬁgurable Computing
5.2.1
The Quicksilver ACM Architecture
The adaptive computing machine (ACM) is based on a revolutionary net-
work on chip paradigm and is one of the very few devices that work on such a
principle.
The Quicksilver ACM consists of a set of heterogeneous computing nodes
hierarchically arranged on a device. At the lowest level, four computing nodes
are placed in a cluster and connected locally together.
Many clusters at a
given level are put together to build bigger clusters at the next higher level
(ﬁgure 2.44).
An ACM chip consists of the following elements:
a set of heterogeneous processing nodes (PN)
an homogenous matrix interconnect network (MIN)
a system controller
various I/O interfaces.
The ACM Processing Node structure.
An ACM processing nodes con-
sist of
an algorithmic engine that deﬁnes the node type. The node type can be
customized at compile-time or at run-time by the user to match a given al-
gorithm. Four types of nodes exist: The Pprogrammable scalar node (PSN)
Figure 2.44.
The Quicksilver ACM hierarchical structure with 64 nodes

Reconﬁgurable Architectures
57
provides a standard 32-bit RISC architecture with 32 general purpose reg-
isters, the adaptive execution node (AXN) provides variable word size mul-
tiply accumulate (MAC) and ALU operations, the domain bit manipulation
( DBN) node provides bit manipulation and byte oriented operations, and
the external memory controller node provides DDRRAM, SRAM, memory
random access and DMA control interfaces for off-chip memory access
The node memory for data storage at node level
A node wrapper that hides the complexity of the network architecture. It
contains an MIN interface to support communication, a hardware task man-
ager for task managements at node level and a DMA engine. The wrapper
envelops the algorithmic engine and presents an identical interface to the
neighbouring nodes. It also incorporates dedicated I/O circuitry, memory,
memory controllers and data distributors and aggregators (ﬁgure 2.45).
The Matrix Interconnect Network (MIN).
The communication inside an
ACM chip is done via an MIN, which is organized hierarchically. At a given
level, the MIN connects many lower level MINs. The top level MIN, the MIN
root, is used to access the nodes from outside and to control the conﬁguration
of the nodes. The communication among nodes is done via the MIN with
the help of the node wrapper. The MIN provides a diversity of services such
as point-to-point dataﬂow streaming, real-time broadcasting, direct memory
access and random memory access. The ACM chip also contains various I/O
interfaces accessible via the MIN for testing (JTAG) and communication with
off-chip devices (ﬁgure 2.45(b)).
(a) The ACM node structure
(b) The ACM QS2412 Resources
Figure 2.45.
ACM Node and Routing resource

58
Reconﬁgurable Computing
System Controller.
The system management is done via an embedded sys-
tem controller (ﬁgure 2.45(b)).
The system controller loads tasks into the
node’s ready-to-run queue for execution, statically or dynamically sets the
communication channels between the processing nodes.
Any node can be
adapted (reconﬁgured) at run-time by the system controller to perform a new
function, in a cycle-by-cycle manner.
5.3
Embedded FPGA
Embedded programmable logic devices, also known under the name embed-
ded FPGA, usually integrate a processor core, a programmable logic or FPGA
and memory on the same chip. Strictly seen, several existing FPGAs such as
the Xilinx Virtex 4 FX fall under this category, as they provides the same ele-
ments like any other embedded FPGA device. However, the processor in the
Xilinx Virtex 4 FPGAs is immersed in the programmable logic, whereas it is
strictly decoupled from the logic in common embedded FPGAs. Two exam-
ples of such devices are the DAP/DNA from IPﬂex and the S500 series from
Stretch.
5.3.1
The IPﬂex DAP/DNA Reconﬁgurable Processor
The DAP/DNA consists of an integrated DAP RISC processor,1
a
distributed network architecture (DNA) matrix and some interfaces (ﬁgure
2.46). The processor controls the system, conﬁgures the DNA, performs com-
putations in parallel to the DNA and manages the data exchange on the device.
The DNA Matrix 2.46 is a dataﬂow accelerator with more than hundred dy-
namic reconﬁgurable processing elements. The wiring among elements can be
changed dynamically, therefore, providing the possibility to build and quickly
change parallel/pipelined processing system tailored to each application. The
DNA conﬁguration data are stored in the conﬁguration memory from where it
can be downloaded to the DNA on a clock-by-clock basis. Like in other recon-
ﬁgurable devices, several interfaces exist for connecting the chip to external
devices.
Large applications can be partitioned and sequentially executed on a
DAP/DNA chip. While the processor controls the whole execution process,
the DNA executes critical parts of the application.
5.3.2
The Stretch Processor
The second example of embedded FPGA that we present is the S5000 series
from Stretch. The device consists of a 32-bit Xtensa RISC processor from
1That is a 32-bit RISC processor operating at 100 MHz with a data cache of 8K byte and an instruction
cache of 8K byte.

Reconﬁgurable Architectures
59
Figure 2.46.
IPﬂex DAP/DNA reconﬁgurable processor
Tensilica, operating at 300 MHz and featuring a single precision ﬂoating point
unit, an instruction set extension fabric (ISEF), embedded memory and a set
of peripheral control modules (ﬁgure 2.47).
Like the DAP/DNA, the processor controls the whole system and conﬁgures
the ISEF. The ISEF is used to augment the processor capacity by implementing
Figure 2.47.
The Stretch 5530 conﬁgurable processor

60
Reconﬁgurable Computing
additional instructions that are directly accessible by a program running on the
processor. The S5 device uses thirty-two 128-bit wide registers coupled with
128-bit wide access to memory to feed data to the ISEF. Several interfaces,
connected to a peripheral bus exist for communication with the external world.
The S5530 peripherals are divided into three categories as follows: the low
speed peripherals that includes UART, IrDA, serial peripheral interface (SPI).
The second category consists of the mid-speed peripherals that include parallel
generic interface bus (GIB) for Flash and SRAM connection. The last category
are the high-speed peripherals that include a 64-bit PCI/PCI-X port and one
64-bit SDRAM port.
5.4
Academic Efforts
Besides commercial coarse-grained reconﬁgurable devices, several other
coarse-grained devices were developed in the academic world. Starting from
1990, several approaches for coarse-grained device, some of which are de-
scribed in this section, have been published [110].
5.4.1
The MATRIX Architecture
The multiple ALU architecture with reconﬁgurable interconnect experiment
(MATRIX) [159] comprises an array of identical basic functional units (BFUs).
Each BFU contains an 8-bit ALU, 256 words of 8-bit memory and control
logic. The ALU features the standard set of arithmetic and logic functions and
a multiplier. A conﬁgurable carry-chain between adjacent ALUs can be used to
cascade ALUs for wide-word operations. The control logic can generate local
control signals from ALU output by a pattern matcher. A reduction network
can be employed for control generated from neighbouring data. Finally, a 20-
input, 8-output NOR block may be used as half of a PLA to produce control
signals. According to these features, a BFU can serve as an instruction mem-
ory, a data memory, a register-ﬁle-ALU combination or an independent ALU
function. Instructions can be routed over the array to several ALUs. The rout-
ing fabric provides three levels of 8-bit buses: eight nearest neighbour and four
second nearest neighbour connections of length four, and global lines spanning
an entire row or column.
5.4.2
RAW: Reconﬁgurable Architecture Workstation
The idea of RAW [210] is to provide a simple and highly parallel comput-
ing architecture composed of several repeated tiles connected to each other
by nearest neighbor connections. The tiles comprise computation facilities as
well as memory, thus implementing a distributed memory model. A RAW mi-
croprocessor is a homogeneous array of processing elements called tiles. The
prototype chip features 16 tiles arranged in a 4 × 4 array. Each tile comprises

Reconﬁgurable Architectures
61
a simple RISC-like processor consisting of ALU, register ﬁle and program
counter, SRAM-based instruction and data memories, and a programmable
switch supplying point-to-point connections to NN. The CPU in each tile of
the prototype is a modiﬁed 32-bit MIPS R2000 processor with an extended 6-
stage pipeline, a ﬂoating point unit and a register ﬁle of 32 general purpose and
16 ﬂoating point registers. Both the data memory and the instruction memory
consist of 32-kilobytes of SRAM. While the instruction memory is uncached,
the data memory can operate in cached and uncached mode. Interconnection of
the RAW architecture is done over nearest neighbour connections being opti-
mized for single data word transfer. Communication between tiles is pipelined
over these connections and appears at register level between processors, mak-
ing it different from multiprocessor systems. A programmable switch on each
tile connects the four nearest neighbour links to each other and to the proces-
sor. The RAW architecture provides both a static and a dynamic network with
wormhole routing for the forwarding of data.
5.4.3
REMARC: Reconﬁgurable Multimedia Array Coprocessor
REMARC [160] is a reconﬁgurable coprocessor that is tightly coupled to a
main RISC processor. It consists of an 8 × 8 array of 16-bit programmable
logic units called nanoprocessor, which is attached to a global control unit.
The control unit manages data transfers between the main processor and the
reconﬁgurable array and controls the execution on the nanoprocessors. It com-
prises an instruction RAM with 1024 entries, 64-bit data registers and four
control registers. The nanoprocessor consists of a 32-entry local instruction
RAM, an ALU with 16-bit datapath, a data RAM with 16 entries, an instruc-
tion register, eight data registers, four 16-bit data input registers and a 16-bit
data output register. The ALUs can execute 30 instructions, including addition,
subtraction, logical operations, shift instructions, as well as some operations
often found in multimedia applications such as minimum, maximum, average,
absolute and add. Each ALU can use data from the data output registers of
the adjacent processors via nearest neighbor connect, from a data register, a
data input register, or from immediate values as operands. The result of the
ALU operation is stored in the data output register. The communication lines
consist of nearest neighbour connections between adjacent nanoprocessors and
additional horizontal and vertical buses in each row and column. The nearest
neighbour connections allow the data in the data output register of a nanopro-
cessor to be sent to any of its four adjacent neighbors. The horizontal and
vertical buses have double width (32-bit) and allow data from a data output
register to be broadcasted to processors in the same row or column, respec-
tively. Furthermore, the buses can be used to transfer data between processors,
which are not adjacent to each other.

62
Reconﬁgurable Computing
5.4.4
MorphoSys
The complete MorphoSys [194] chip comprises a control processor
(TinyRISC), a frame buffer (data buffer), a DMA controller, a context mem-
ory (conﬁguration memory) and an array of 64 reconﬁgurable cells (RC). Each
RC comprises an ALU-multiplier, a shift unit, and two multiplexers at the RC
inputs. Each RC also has an output register, a feedback register and a reg-
ister ﬁle. A context word, loaded from the conﬁguration memory and stored
in the context register, deﬁnes the functionality of the RC. Besides standard
logic/arithmetic functions, the ALU has other functions such as computation
of absolute value of the difference of two operands and a single cycle multiply-
accumulate operation. There are a total of 25 ALU functions. The RC intercon-
nection network features three layers. In the ﬁrst layer, all cells are connected
to their four nearest neighbours. In the second layer, each cell can access data
from any other cell in the same row or column of the same array quadrant.
The third layer of hierarchy consists of buses spanning the whole array and
allowing transfer of data from a cell in a row or column of a quadrant to any
other cell in the same row or column in the adjacent quadrant. In addition, two
horizontal 128-bit buses connect the array to the frame buffer.
5.4.5
NISC Processor: No-Instruction-Set Computer
The NISC [92] processor consists of a Controller and Datapath on which
any C program can be executed on it. The datapath consists of a set of storage
elements (registers, register ﬁles, memories), functional units (ALUs, multi-
pliers, shifters, custom functions) and a set of busses. Each component may
take one or more clock cycles to execute, each component may be pipelined
and each component may have input or output latches or registers. The entire
Datapath can be pipelined in several stages in addition to components being
pipelined themselves. The Controller deﬁnes the state of the processor and
issues the control signals for the Datapath. The Controller can be ﬁxed or
programmable, whereas the Datapath can be reprogrammable and reconﬁg-
urable. Reprogrammable means that the Datapath can be extended or reduced
by adding or omitting some components, while reconﬁgurable means that the
Datapath can be reconnected with the same components. To speed up the NISC
pipelining, a control register (CR) and a status register (SR) has been inserted
between the Controller and the Datapath.
5.4.6
Virtual Pipelines: The PipeRench
Pipeline reconﬁguration was proposed by Goldstein et al. [98] as a method
of virtualizing pipelined hardware application designs. The single static re-
conﬁguration is broken into pieces that correspond to pipeline stages in the
application. The resulting conﬁgurations are then loaded into the device on a

Reconﬁgurable Architectures
63
cycle-by-cycle basis. With pipeline virtualization, an application is pipelined
implemented on a given amount of virtual resources. The virtual resources are
then mapped to the physical resources in a ﬁnal step. The physical resources
are computing units modules, whose functionality can be changed by reconﬁg-
uration.
The process is somehow similar to the implementation on parallel machines,
where virtual processors are ﬁrst used to easily capture the inherent parallel
structure of an application. The virtual processors are then mapped to the phys-
ical one in the next step. Virtualization provides the designer freedom to better
explore the parallelism in the implementation, and he/she does not need to face
the resource constraint of a given platform.
Figure 2.48 illustrates the virtualization processing on the mapping of a ﬁve-
stage virtual pipeline on a three-stage physical fabric. On the top part of this
ﬁgure, we see the ﬁve-stage application and the state of each of the stages
of the pipeline in the ﬁve consecutive cycles. The bottom half of the ﬁgure
shows the mapping of the virtual blocks on the physical modules of the device.
Because the conﬁguration of each single unit in the pipeline is independent
from the other, the reconﬁguration process can be broken down to a cycle-by-
cycle conﬁguration. In this way, part of the pipeline can be reconﬁgured while
the rest is computing.
Figure 2.48.
Pipeline Reconﬁguration: Mapping of a 5 stage virtual pipeline auf eine 3 stage

64
Reconﬁgurable Computing
Based on the pipeline reconﬁguration concept, a class of reconﬁgurable de-
vices called PipeRench was proposed in [98] as co-processor in multimedia
applications. A PipeRench device consists of a set of physical pipeline stages
also called stripes. A stripe is composed of interconnect and processing ele-
ments (PE), each of which contains registers and LUT-based ALUs. The PEs
access operands from registered outputs of the previous stripe as well as reg-
istered or unregistered outputs of the other PEs in the stripe. All PipeRench
devices have four global busses, two of which are dedicated to storing and
restoring stripe state during hardware virtualization (conﬁguration). The other
two are used for input and output.
5.4.7
RaPiD
The last work that we consider in this section is the one of Ebeling et al.
[100] whose goal was to overcome the handicap of FPGA.2 A one-dimensional
coarse-grained reconﬁgurable architecture called RaPiD, an acronym for re-
conﬁgurable pipelined datapath is proposed for this purpose.
The structure of RaPiD datapaths resembles that of systolic arrays. The
structure is made upon linear arrays of functional units communicating in
mostly a nearest neighbour fashion. This can be used for example, to con-
struct a hardware module, which comprises different computations at different
stages and at different times resulting in a linear array of functional units that
can be conﬁgured to form a linear computational pipeline. The resulting ar-
ray of functional units is divided into identical cells that are replicated to form
a complete array. A RaPiD-cell consists of an integer multiplier, two inte-
ger ALUs, and six general purpose registers and three small local memories.
Interconnections among the functional units are realized using a set of ten seg-
mented busses that run the length of the datapath. Many of the registers in a
pipelined computation can be implemented using the bus pipeline registers.
Functional unit outputs are registered. However, the output registers can
be bypassed via conﬁguration control. Functional units may additionally be
pipelined internally depending on their complexity.
The control of the datapath is done using two types of signals: the static
control signals that are deﬁned by the conﬁguration memory as in ordinary
FPGAs and dynamic control that must be provided on every cycle. To program
an application on the RaPiD, a mapping of functional blocks to computing el-
ements of the datapath must be done, which result on the generation of a static
programming bitstream. This will be used to construct the pipeline, and the dy-
namic programming bits are used to schedule the operations of the computation
2In this case, the large amount of resources deployed to build macro instructions and the difﬁculty in
programming the FPGAs.

Reconﬁgurable Architectures
65
onto the datapath over time. A controller is programmed to generate the dy-
namic information needed to produce the dynamic programming bits.
5.5
Capacity / Chip Size
The last point we deal with in the architecture is the deﬁnition of a measure
of comparison between the RPUs. The most used measure of comparison is the
device capacity, which is usually provided by the manufacturer as the number
of gates that can be used to build a function into the device. On ﬁne-grained
reconﬁgurable devices, the number of gates is often used as unit of measure for
the capacity of an RPU. A gate equivalent corresponds to a two-input NAND
gate, i.e. a circuit that performs the function F = A · B. The gate density
deﬁnes the number of gates per unit area. Besides the capacity of an RPU,
others factors such as the number of pins and the device speed may also play a
big role in the comparison.
Coarse-grained reconﬁgurable devices do not have any established measure
of comparison. However, the number of PEs on a device as well as their char-
acteristics such as the granularity, the speed of the communication link, the
amount of memory on the device and the variety of peripherals may provide
an indication on the quality of the device.
Although the capacity of the device deﬁnes the “amount of parallelism”,
which can be implement in the device, the speed gives an indication on the
throughput of data in the device. Designers should however bear in mind that
the real speed and real size that a design can achieve depends on the imple-
mentation style and the compilers used to produce the designs.
6.
Conclusion
Our goal in this chapter was not to provided all possible details on the single
architectures of the reconﬁgurable computer chips presented. We rather focus
on the main characteristics in the technology as well as the operations. More
details on the single devices can be found in the corresponding datasheet. De-
spite the large amount of architecture presented in this section, the market is
still dominated by the FPGAs, in particular those from Xilinx and Altera. The
coarse-grained reconﬁgurable device’s market has not really taken-off so far,
despite the large amount of concept and prototypes developed in this direction.
Research and development in the architecture of reconﬁgurable computing
systems is very dynamic. By the time this book is published, some concept
presented in this section will probably be no more actual. However, as we
have seen with the FPGAs, the basic structure of reconﬁgurable device will
still remain the same in the future. In the case of FPGA, we will experience
some changes in the amount of input of the LUT, modiﬁcations in the I/O el-
ements and also the inclusion of various coarse-grained element in the chip.

66
Reconﬁgurable Computing
However, LUTs will still be used as computational connected to each other
using programmable crossbar switches. I/O elements will still be used for ex-
ternal communication. Coarse-grained device will still have the same structure
consisting of ALU-computing elements, programmable interconnections and
I/O components. Understanding the concept presented here will therefore help
to understand better and faster the changes that will be made on the devices in
the future.

Chapter 3
IMPLEMENTATION
In the ﬁrst part of this chapter, we present the different possibilities for the
use of reconﬁgurable devices in a system. According to the way those devices
are used, the target applications and the systems in which they are integrated,
different terminologies will be deﬁned for the systems. We follow up by pre-
senting the design ﬂow, i.e. the steps required to implement an application on
those devices. Because the programming of coarse-grained reconﬁgurable de-
vices is very similar to that of processors, we will not focus on the variety of
tool that exists for this purpose. The implementation on FPGA devices is rather
unusual in two points. First, the programming is not aimed at generating a set
of instructions to be executed sequentially on a given processor. We seek the
generation of the hardware components that will be mapped at different time
on the available resources. According to the application, the resources needed
for the computation of the application will be built as components to be down-
loaded to the device at run-time. The generation of such components is called
logic synthesis. It is an optimization process whose goal is to minimize some
cost functions aimed at producing, for instance, the fastest hardware with the
smallest amount of resources and the smallest power consumption. The map-
ping of the application to the FPGA resources is a step of the logic synthesis
called technology mapping. The second unusual point with FPGAs is that the
technology mapping targets look-up tables rather than NAND Gate as it is the
case with many digital devices. In the last part of the chapter, we will therefore
shortly present the steps required in logic synthesis and focus in more details
on technology mapping for FPGAs.

68
Reconﬁgurable Computing
1.
Integration
Reconﬁgurable devices are usually used in three different ways:
Rapid prototyping: In this case, the reconﬁgurable device is used as an
emulator for another digital device, usually an ASIC. The emulation pro-
cess allows to functionally test the correctness of the ASIC device to be pro-
duced, sometimes in real operating and environmental conditions, before
production. The reconﬁgurable device is only reconﬁgured to emulate a
new implementation of the ASIC device.
Non-frequently reconﬁgurable systems: The reconﬁgurable device is in-
tegrated in a running system where it is used as an application-speciﬁc pro-
cessor. These systems are usually stand-alone systems. The reconﬁguration
is used for testing and mg initialization at start-up and for upgrading pur-
pose. The device conﬁguration is usually stored in an EEPROM of Flash
from which it is downloaded at start-up to reconﬁgure the device. No con-
ﬁguration happens during operation.
Frequently reconﬁgurable systems: This third category comprises sys-
tems, which are frequently reconﬁgured. Those systems are usually cou-
pled with a host processor, which is used to reconﬁgure the device and
control the complete system.
With the increasing size and speed of reconﬁgurable processor,1 it is possi-
ble to implement many large modules on a reconﬁgurable device at the same
time. Moreover, for some reconﬁgurable devices, only a part of the device can
be conﬁgured while the rest continues to operate. This partial reconﬁgura-
tion capability enables many functions to be temporally implemented on the
device. Depending on the time at which the reconﬁguration sequence are de-
ﬁned, the computation and conﬁguration ﬂow on a reconﬁgurable devices can
be classiﬁed into two categories:
Compile-time reconﬁguration: In this case, the computation and conﬁg-
uration sequences as well as the data exchange are deﬁned at compile time
and never change during a computation. This approach is more interest-
ing for devices, which can only be fully reconﬁgured. However, it can
be applied to partial reconﬁgurable devices that are logically or physically
partitioned in a set of reconﬁgurable bins.
Run-time reconﬁguration: The computation and conﬁguration sequences
are not known at compile time. Request to implement a given task is known
at run-time and should be handled dynamically.The reconﬁguration process
1Today, leading edge reconﬁgurable devices contain million of gates with few hundreds MHz.

Implementation
69
exchanged part of the device to accommodate the system to changing oper-
ational and environmental conditions. Run-time reconﬁguration is a difﬁ-
cult process that must handle side-effect factors such as defragmentation of
the device and communication between newly placed modules. The man-
agement of the reconﬁgurable device is usually done by a scheduler and a
placer that can be implemented as part of an operating system running on a
processor (ﬁgure 3.1). The processor can either reside inside or outside the
reconﬁgurable chip.
The scheduler manages the tasks and decides when a task should be exe-
cuted. The tasks that are available as conﬁguration data in a database are
characterized through their bounding box and their run-time. The bounding
box deﬁnes the area that a task occupies on the device. The management of
task execution at run-time is therefore a temporal placement problem that
will be studied in detail in chapter 5. The scheduler determines which task
should be executed on the RPU and then gives the task to the placer that
will try to place it on the device, i.e. allocate a set of resources for the im-
plementation of that task. If the placer is not able to ﬁnd a site for the new
task, then it will be sent back to the scheduler that can then decide to send
it later and to send another task to the placer. In this case, we say that the
task is rejected.
Figure 3.1.
Architecture of a run-time reconﬁgurable system

70
Reconﬁgurable Computing
Figure 3.2.
A CPU-RPU conﬁguration and computation step
No matter if we are dealing with a compiled-time or run-time reconﬁgurable
system, the computation and reconﬁguration ﬂow is usually the one shown on
Figure 3.2. The host CPU is used for device conﬁguration and data trans-
fer. Usually, the reconﬁgurable device and the host processor communicates
through a bus that is used for the data transfer between the processor and the
reconﬁgurable device. In supercomputing, systems are made upon a high-
speed processor from Intel or AMD and an FPGA board attached to a bus such
as the PCI. Several systems (Cray XD1, SGI, Nallatech SRC Computers), in
which FPGAs cohabits and communicate using a dedicated high-speed bus,
are available to buy. In embedded systems, the processors are more and more
integrated in the reconﬁgurable devices and are heavily used for management
purpose rather than for computation. The RPU acts like a coprocessor with
varying instruction sets accessible by the processor in a function call. The
computation ﬂow can be summarized as shown in algorithm 2.
At the beginning of a computation, the host processor conﬁgures the recon-
ﬁgurable device.2 Then it downloads the segment of the data to be processed by
2It is already possible to trigger the reconﬁguration of the Xilinx FPGAs from within the device using
their ICAP-port. This might allow a self-reconﬁguration of a system from a processor running within the
device.

Implementation
71
Algorithm 2 CPU-RPU conﬁguration and computation steps
1: Start
2: Initialize the RPU
3: while (1) do
4:
Conﬁgure the RPU to implement a new task
5:
Download Data for RPU computation into RPU-memory
6:
Computes in parallel with the RPU if necessary
7:
Upload the data computed by the RPU from the RPU-memory
8: end while
9: Stop
the RPU3 and give the start signal to the RPU. The host and the RPU can then
process in parallel on their segments of data. At the end of its computation,
the host reads the ﬁnish signal of the RPU. At this point, the data (computation
result) can be collected from the RPU memory by the processor. The RPU
can also send the data directly to an external sink. In the computation ﬂow
presented above, the RPU is conﬁgured only once. However, in frequently re-
conﬁgured systems, several conﬁgurations might be done. If the RPU has to be
conﬁgured more than once, then the body of the while loop must be run again
according to the number of reconﬁgurations to be done before.
The design ﬂow of a dynamic reconﬁgurable system is primary a hard-
ware/software partitioning process in which:
The part of the code to be executed on the host processor is determined.
This part is usually control dominated.
The parts of the code to be executed on the reconﬁgurable device are iden-
tiﬁed. Those are usually data-dominated parts for which efﬁcient dataﬂow
computation modules are required.
The interface between the processor and the reconﬁgurable device is im-
plemented.
The implementation of the control part on the CPU is done in software using
the common development languages and tools. It is usually a C-program with
system calls to access the device for reconﬁguration. We will not focus on the
software development details in this book, because enough material that cover
software development exist.
No matter if the device is partially reconﬁgurable or not, the development
of the module to be downloaded later on the reconﬁgurable device follows the
3The data might also be collected by the RPU itself from an external source.

72
Reconﬁgurable Computing
Manufacturer
Language
Tool
Description
PACT
NML (Structural)
XPP-VC
C into NML and conﬁguration
Quicksilver
SilverC
InSpire SDK
SiverC into Conﬁguration
NEC-DRP
C
DRP Compiler
C into conﬁguration
picoChip
C
Picochip Toolchain
C into conﬁguration
IpFlex
C/MATLAB
DAP/DNA FW
C/MATLAB into conﬁguration
Table 3.1.
Language and tools overview for coarse-grained RPUs
same approach. The goal is to generate a dataﬂow computing block that best
matches the inherent parallelism of the part of the application to be imple-
mented as module.
Almost all manufacturers of coarse-grained reconﬁgurable devices provide
proprietary language and tools to implement such modules. It is usually a
C-like language with an appropriate scheduler that deﬁnes the function of
the processing elements as well as their interconnections as function of the
time. Table 3.1 gives an overview of the languages and tools used by some
manufacturers.
We will not further consider the programming on coarse-grained devices,
but rather refer to the corresponding manufacturers for more details on their
implementation languages and tools. In the next section, we will brieﬂy present
the standard design ﬂow for implementing digital circuits such as ASIC. We
will focus in much detail on technology mapping for FPGA because it differs
from that of other digital devices.
2.
FPGA Design Flow
The standard implementation methodology for FPGA designs is borrowed
from the ASIC design ﬂow. The usual steps are presented in Figure 3.3. Note
that those steps concern only the modules of the application that has been iden-
tiﬁed by the hardware/software co-design process to be executed on the FPGA.
The software part is implemented using standard software development ap-
proaches, which are well covered in several textbooks.
2.1
Design Entry
The description of the function is made using either a schematic editor, a
hardware description language (HDL), or a ﬁnite state machine (FSM) editor.
A schematic description is made by selecting components from a given library
and connecting them together to build the function circuitry. This process has
the advantage of providing a visual environment that facilitates a direct map-
ping of the design functions to selected computing blocks. The ﬁnal circuit is
built in a structural way. However, designs with very large amount of function
will not be easy to manage graphically. Instead, a HDL may be used to cap-
ture the design either in a structural or in a behavioral way. Besides VHDL

Implementation
73
Design Entry
Functional
Simulation
Synthesis
Place and
Route
Configuration
Bitstream
Figure 3.3.
The FPGA design ﬂow
and Verilog, which are the most established HDLs several C-like languages,
mostly tied to just one compiler manufacturer exist to describe hardware. Here
we can cite the languages Handel-C [125] and ImpulseC [175] and, in some
extend, SystemC [170].
2.2
Functional Simulation
After the design entry step, the designer can simulate the design to check
the correctness of the functionality. This is done by providing test patterns to
the inputs of the design and observing the outputs. The simulation is done in
software by tools that emulate the behaviour of the components used in the
design. During the simulation, the inputs and outputs of the design are usually
shown on a graphical interface, which describes the signal evolution in time.
2.3
Logic Synthesis
After the design description and the functional simulation, the design can
be compiled and optimized. It is ﬁrst translated into a set of Boolean equa-
tions. Technology mapping is then used to implement the functions with the
available modules in function library of the target architecture. In case of FP-
GAs, this step is called LUT-based technology mapping, because LUTs are the
modules used in the FPGA to implement the boolean operators. The result of
the logic synthesis is called the netlist. A netlist describes the modules used to

74
Reconﬁgurable Computing
implement the functions as well as their interconnections. There exist different
netlist formats to help exchange data between different tools. The most known
are the Electronic Design Interchange Format (EDIF). Some FPGA manufac-
turers provide proprietary formats. This is the case the Xilinx Netlist Format
(XNF) for the Xilinx FPGAs.
2.4
Place and Route
For the netlist generated in the logic synthesis process, operators (LUTs,
Flip-Flopss, Multiplexers, etc.) should be placed on the FPGA and connected
together through routing. Those two steps are normally achieved by CAD
tools provided by the FPGA vendors. After the placement and routing of a
netlist, the CAD tools generate a ﬁle called a bitstream. A bitstream provides
the description of all the bits used to conﬁgure the LUTs, the interconnect
matrices, the state of the multiplexer and I/O of the FPGA. The full and partial
bitstreams can now be stored in a database to be downloaded later according
to the paradigm described in section 1.
2.5
Design Tools
The design entry, the functional simulation and the logic synthesis are done
using the CAD tools from Xilinx, Synopsys, Synplicity, Cadence, ALTERA
and Mentor Graphics. The place and route as well as the generation of con-
ﬁguration data is done by the corresponding vendor tools. Table 3.2 provides
some information on the tool capabilities of some vendors.
The logic synthesis step is an important part in FPGA design. It is done by
tools, which solve a given number of optimization problems on the path from
the design entry to the generation of the conﬁguration data. In the next section,
we will take a look inside those tools in order to understand how they work.
As the technology mapping process of FPGA differs from that of ASIC, we
will pay more attention to this step and present in detail some of technology
mapping algorithms, developed for FPGA devices. Each of the algorithms is
best adapted for the optimization of a given cost function.
Manufacturer
Tool
Description
Synopsys
FPGA Compiler
Synthesis
Mentor
FPGA
Synthesis, place and route
Synplicity
Sinplify
Synthesis, place and route
Xilinx
ISE
Synthesis, place and route (only Xilinx products)
Altera
Quartus II
Synthesis, place and route (only Altera products)
Actel
Libero
Synthesis, place and route (only Actel products)
Atmel
Figaro
Synthesis, place and route (only Atmel products)
Table 3.2.
Overview of FPGA manufacturers and tool providers

Implementation
75
3.
Logic Synthesis
A function, assigned to the hardware in the hardware/software co-design
process, can be described as a digital structured system. As shown in
Figure 3.4, such a digital structured system consists of a set of combinatorial
logic modules (the nodes), memory (the registers), inputs and outputs.
The inputs provide data to the system, whereas the outputs carry data out of
the system. Computation is performed in the combinatorial parts and the re-
sults might be temporally stored in registers that are placed between the com-
binatorial blocks. A clock is used to synchronize the transfer of data from
register to register via combinatorial parts. The description of a design at this
level is usually called register transfer description, because of the register to
register operation mode previously described.
For such a digital system, the goal of the logic synthesis is to produce an
optimal implementation of the system on a given hardware platform. In the
case of FPGA, the goal is the generation of conﬁguration data that satisﬁes
a set of given constraints such as the maximal speed, the minimum area, the
minimum power consumption, etc. In a structured system, each combinatorial
block is a node that can be represented as a two-level function or as multi-level
function. Depending on the node representations, the two following synthesis
approaches exist:
Two-Level Logic Synthesis: Two-level synthesis deals with the synthesis of
designs represented in two-level logic. Those are representations in which
the longest path from input to output, in term of number of gates crossed on
the path, is two. Two-level logic is the natural and straightforward approach
Figure 3.4.
A structured digital system

76
Reconﬁgurable Computing
to implement a Boolean function, because each Boolean function can be
represented as a sum of product terms. In the ﬁrst level, the products are
built using the AND primitives. The sums of the resulting products are built
in the second level with the OR-primitives.
Multi-Level Logic Synthesis: In the multi-level synthesis, functions are rep-
resented using a multi-level logic. Those are circuits in which the longest
path from input to output goes through more than two gates.
Most of the circuits used in practice are implemented using multi-level logic.
Multi-level circuits are smaller, faster in most cases and consume less power
than two-level circuits. Two-level logic is most appropriate for PAL and PLA
implementations, whereas multi-level is used for standard cell, mask-programmable
or ﬁeld-programmable devices.
We formally represent a node of the structured system as a Boolean net-
work, i.e. a network of Boolean operators that reﬂects the structure and func-
tion of the nodes. A Boolean network is deﬁned as a directed acyclic graph
(DAG) in which a node represents an arbitrary Boolean function and an edge
(i, j) represents the data dependency between the two nodes i and j of the
network.
Figure 3.5 shows a Boolean network example for the functions.
3.1
Node representation
The representation of a node is of great importance in the synthesis process.
In two-level logic, the question on how to represent the node of a Boolean net-
work is not relevant, because the ﬁnal representation is the same sum ofproducts
Figure 3.5.
Example of a boolean network with: y1 = x1 + x2, y2 = x3 · x4, y3 = x5 · x6,
y4 = y1 + y2, z1 = y1 + y4, and z2 = y2 ⊕y3

Implementation
77
as the initial representation. Although any valid representation is allowed, in
multi-level logic, representation are sought, which efﬁciently use the memory
and for which a correlation with the ﬁnal representation exists. Furthermore,
the representation should be easy to manipulate.
Logic synthesis can be done in two different approaches: the technology-
dependent synthesis and the technology independent. In the ﬁrst case, only
valid gates chosen from the target library are used in the node representation.
The ﬁnal implementation matches the node representation. In the second case,
the representation is technology independent, i.e the design is not tied to any
library. A ﬁnal mapping must be done to the ﬁnal library in order to have
an implementation. The technology-independent method is most used, be-
cause of the large set of available optimization methods. With a technology-
independent representation, synthesis for FPGA devices is done in two steps.
In the ﬁrst step, all the Boolean equations are minimized, independent of the
function generators used. In the second step, the technology mapping process
maps the parts of the Boolean network to a set of LUTs.
In general, the following choices are made for the representation of a node:
Sum of products form: A Sum of Product (SOP) is the most trivial form
to represent a Boolean function. It consists of a sum of product terms, and
it is well adapted for two-level logic implementation on PALs and PLAs.
Example: f = xyz + xyz + wxy.
This representation has the advantage that it is well understood, and it is
easy to manipulate. Many optimization algorithms are available (AND,
OR, Tautology, two-level minimizers). The main disadvantage is the non-
representativity of the logic complexity. In fact, designs represented as
sum of products are not easy to estimate as the complexity of the design
decreases through manipulation. Therefore, estimation of progress during
logic minimization on SOPs is difﬁcult.
Factored form: A factored form is deﬁned recursively either as a single
literal or as a product or a sum of two factored forms: a product is either
a single literal or the product of two factored forms and a sum is either a
single literal or the sum of two factored forms. c [a + b(d + e)] is a product
of the factored forms c and a + b(d + e), and a + b(d + e) is a sum of the
factored forms a and b(d + e).
Factored forms are representative of the logic complexity. In many design
styles, the implementation of a function corresponds to its factored form.
Therefore, factored forms are good estimation of complexity of the logic
implementation. Their main disadvantage is the lack of manipulation algo-
rithms. They are usually converted in SOPs before manipulation.

78
Reconﬁgurable Computing
Binary decision diagram: A binary decision diagram (BDD) is a rooted
directed acyclic graph used to represent a boolean function. Two kinds of
nodes exist in BDDs: variable and constant nodes.
–
A variable node v is a non-terminal having as attribute its argument
index4 index(v) ∈{1, ..., n} and its two children low(v) and high(v).
–
A constant node v is a terminal node with a value value(v) ∈{0, 1}.
A BDD in which an ordering relation among the nodes exists is called a
ordered BDD (OBDD). The non-terminal nodes are ordered from the root
to the terminal nodes. Formally, for each non-terminal node v, if low(v)
is non terminal, then index(low(v)) < index(v). Similarly, if high(v) is
non terminal, then index(high(v)) < index(v).
The correspondence between a BDD and a Boolean relation is deﬁne as
follow: A BDD with root v denotes a function fv.
–
If v is a terminal node, then if value(v) = 1, then fv = 1, else
[value(v) = 0] fv = 0.
–
If v is a non-terminal node with index(v) = i , the Shannon expan-
sion theorem is used to express the function fv as fv = xiflow(v) +
xifhigh(v), where flow(v) respectively fhigh(v) denote the function rooted
at low(v) respectively high(v). The value of fv for a given assignment
is obtained by traversing the graph from the root terminal according to
the assignment values of the nodes.
A BDD G is a reduced ordered BDD (ROBDD) if the following holds:
–
low(v) ̸= high(v), ∀v in G
–
∀v, v′ ∈G, the subtrees rooted at v and the subtree routed at v′ are not
isomorphic5
Figure 3.6 shows the optimal BDD representation of the function f = abc+
bd + bcd.
Basically, BDDs are not canonical, i.e. there might exist several BDD
representations of the same Boolean function.
Reduced ordered BDDs
are canonical and compact; thus, they are good replacements of truth ta-
bles. For a good ordering, reduced ordered BDDs remain reasonably small
4An index i deﬁnes a variable xi.
5Two BDDs G1 and G2 are isomorph ⇐⇒there exists a bijective function σ from G1 in G2 such
that: 1) for a terminal node v ∈G1, σ(v) = w is a terminal node in G2 with value(v) = value(w); 2)
for a non terminal node v ∈G1, σ(v) = w is a non-terminal node of G2 with index(v) = index(w),
σ(low(v)) = low(w) and σ(high(v)) = high(w).

Implementation
79
Figure 3.6.
BDD-representation of the function f = abc + bd + bcd
for complicated functions. Manipulations of BDDs are well deﬁned and
efﬁcient.
3.2
Node manipulation
Having represented the node in one of the formats previously presented op-
timization algorithms can be applied to the Boolean network. The goal is to
generate an optimized network with either a less amount of gates or the lowest
depth of the gates. A given number of transformations can be applied to the
network for this purpose. Among them, we can cite the following:
Decomposition: The decomposition takes a single Boolean function and
replace it with a collection of new expressions. We say that a Boolean
function f(X) is decomposable if we can ﬁnd a function g(X) such that
f(X) = f ′(g(X), X).
The function f = abc+abd+acd+bcd for example operates on 12 literals.
Using decomposition, f can be written as: f = ab(c + d) + (a + b)cd =
ab(c + d) + ab(c + d) = XY + XY with X = ab and Y = c + d. This
decomposition reduces f the operation of f to only 8 literals.
Extraction: The extraction is used to identify common intermediate sub-
functions of a set of given functions in order to avoid redundancy.
Example: f = (a + bc)d + e and g = (a + bc)e can be rewritten as
f = xd + e and g = xe with x = a + bc. The output x of the common part
of the circuit will be used as common input for f and g.

80
Reconﬁgurable Computing
Factoring: Factoring is the transformation of SOP-expressions in factored
form. For example the function f = ac+ ad + bc + bd + e can be rewritten
as f = (a + b)(c + d) + e
Substitution: Substitution replace an expression e within a function f with
the value of an equivalent function g(X) = e. For example the function
f = (a + bc)(d + e) can be rewritten as f = g(d + e), with g = a + bc.
Collapsing: Also called elimination, collapsing is the reverse operation of
the substitution. Collapsing is used to eliminate levels in order to meet the
timing constraints. The function f = ga + gb for example will be replaced
by f = ac + ad + bcd with g = c + d.
3.3
LUT-based Technology Mapping
The technology-independent optimization phase ends with a reduced Boolean
network in which the fanin and fanout of gates vary. The next step consists of
allocating LUTs, which are the FPGA library elements for the implementation
of the different nodes. Several goals might be followed here. If the goal is to
minimize the chip area used by the circuit, then the mapping will try to allo-
cate the less possible amount of LUTs. If the goal is to minimize the delay of
the ﬁnal circuit, then the mapping will try to minimize the depth of the LUTs
used. Other goals such as testability and low power might also be followed.
Several LUT technology mapping algorithms have been developed in the past.
Depending on their optimization goals, those algorithms can be classiﬁed in
three categories:
The ﬁrst category contains the algorithms, whose goal is to minimize the
area. This category includes the Chortle-crf [87][86], the MIS-fpga [163]
and the Xmap [133]. Because of its popularity and the proof of area opti-
mality for LUTs with less than ﬁve inputs, the Chortle algorithm will be
presented in this section.
The algorithms in the second category target the delay minimization. Al-
gorithms in this category include the FlowMap [53], the Chortle-d [86],
the DAG-map [46] and the MIS-pga-delay [164]. The FlowMap algorithm
was a breakthrough in delay minimization because the authors were able
to present an optimal algorithm for LUT technology mapping with delay
minimization as well as a proof of the polynomial-time complexity of their
algorithm. We present the FlowMap in detail in this section.
The third category contains algorithms that focuses on maximizing the
routability. This category includes the Bhat and Hill [26] work as well
as the Schlag [54], Kong and Chang approach [54]. None of those methods
will be presented here.

Implementation
81
Besides the algorithms presented in this section, a large variety of high-quality
LUT technology mapping algorithms have been developed in the last couple of
years. Also research on LUT-based technology mapping keeps going on. All
those algorithms cannot be presented here. We therefore referred the interested
readers to the large amount of available publications. A starting point is the
survey provided by Cong et al. [54]. We will ﬁrst present some deﬁnitions
needed to better understand the algorithms that we present. We start providing
a formal deﬁnition of the problem that the algorithms we introduce intend to
solve. The LUT technology mapping problem can be stated as follows:
Definition 3.1 [LUT-Based Technology mapping problem]
Given a Boolean network representing a function f and an integer k. Find an
implementation of f using only k-inputs LUTs, such that
1 The amount of LUTs use is minimal or
2 The delay of the resulting circuit is minimal.
The LUT-based technology mapping is the problem of covering a given graph
(the Boolean network) with a set of k-input LUTs. Because the area of the ﬁnal
implementation is deﬁned through the amount of LUT used, the ﬁrst condition
is equivalent to having a covering with a minimal amount of LUTs. The delay
in an FPGA is inﬂuenced by two main factors: The delay in LUTs and the
interconnection delay. Although the delay in LUT is known, the interconnec-
tion delay can only be accurately known after the place and route phase. Delay
estimation at this stage is done using the depth of LUTs in the design, thus
assuming the interconnection delay to be one for each wire. The more LUTs
are available on a path from the input to the outputs the higher the delay in the
circuit.
Definition 3.2 (Primary Input, Primary Output, Node Level,
Node Depth, Fan-in, Fan-out) Given a boolean network G, we deﬁne
the following:
1 A primary input (PI) node is a node without any predecessor.
2 A primary output (PO) node is a node without any successor.
3 The level l(v) of a node v is the length of the longest path from the primary
inputs to v.
4 The depth of a network G is the largest level of a node in G.
5 The fan-in of a node v is the set of gates whose outputs are inputs of v.
6 The fan-out of v is the set of gates that use the output of v as input.

82
Reconﬁgurable Computing
7 Given a node v ∈G , input(v) is deﬁned as the set of node of G, which
are fan-in of v, i.e. the set of predecessors of v.
Definition 3.3 (Trees, Leaf-Dag)
1 A tree or fan-out-free circuit is
one in which each node has a maximal fan-out of one.
2 A leaf-DAG is a combinational circuit in which the only gates with a fan-in
greater than one are the primary inputs.
Definition 3.4 (K-Bounded network) Given a Boolean network G
and a subgraph H of G.
With input (H), we denote the set of all nodes not included in H, which
are predecessors of some nodes in H.
G is K-bounded if |input(v)| ≤K for all nodes of G.
A K-bounded Boolean network can be directly mapped to a set of k-inputs
LUT by assigning an LUT for each node. However, this straightforward ap-
proach may not produce the expected optimal results.
Definition 3.5 (Cone at a node) Given a Boolean network G,
1 A cone Cv at a node v is the tree with root v which spans from v to its
primary inputs.
2 The cone Cv is K-feasible if:
input(Cv) ≤K
any path connecting two nodes in Cv to v lies entirely in Cv
An illustration of K-feasible cone at a node v is given in ﬁgure 3.7.
With the previous deﬁnition of K-feasible cones, the LUT technology map-
ping becomes the problem of covering the graph with a set of K-feasible cones
that are allowed to overlap. The technology mapping results in a new DAG in
which nodes are K-feasible cones and edges represent communication among
the cones. Figure 3.8 shows the covering of a graph with 3-feasible cones and
the resulting LUT-mapping to 3-input LUTs.
Next, we present some existing LUT-technology mapping algorithms and
explain their advantage as well as their drawbacks.
3.3.1
The Chortle Algorithm
The Chortle algorithm was developed by Francis et al. [86, 87] at the Uni-
versity of Toronto in 1991 with the aim of minimizing the amount of LUTs in

Implementation
83
Figure 3.7.
Example of a K-feasible cone Cv at a node v
3−LUT
3−LUT
3−LUT
Figure 3.8.
Example of a graph covering with K-feasible cone and the corresponding covering
with LUTs
the implementation of a given circuit. It operates in two steps: In the ﬁrst step,
the original Boolean network is partitioned into a forest of trees that are then
separately mapped into circuits of K-input LUTs. The second step assembles
the circuits implementing the trees to produce the ﬁnal circuit.
The transformation of the original network into a forest is done by partition-
ing each fan-out node v. Therefore, sub-network rooted at v is duplicated for
each input triggered by the fan-out nodes of v. The resulting sub-networks are
either trees or leaf-DAGs. The leaf-DAGs are converted in trees by creating a
unique instance of a primary input for each of its fan-out edges.
Mapping the trees.
The strategy used by Chortle to map a tree is a combina-
tion of bin packing and dynamic programming. Each tree is traversed from the
primary inputs to the primary outputs. At each node v, a circuit referred to as
the best circuit, implementing the cone at v extending from the node to the pri-
mary inputs of the network, is constructed. The best circuit is characterized by

84
Reconﬁgurable Computing
two main factors: The tree rooted at v and represented by a cone must contain
the minimum number of LUTs and the output LUT (the root-LUT) implement-
ing v should contain the maximum number of unused input pins. For a primary
input p, the best circuit is a single LUT whose function is a buffer. Using the
dynamic programming, the best circuit at a node v can be constructed from
its fan-in nodes, because each of them is already optimally implemented. The
procedure enforces the use of the minimum number of LUTs at a given node.
The best-circuit is then constructed from the minimum number of LUTs used
to implement its fan-in nodes. The secondary goal is to minimize the number
of unused inputs of the circuit rooted at node v.
The construction of the tree is done in two steps. First a two-level decompo-
sition of the cone at v is constructed, and then, this decomposition is converted
into a multi-level decomposition.
Two-level decomposition.
The two-level decomposition consists of a single
ﬁrst-level node and several two-level nodes (ﬁgure 3.9(a)). Each second level
node implements the operation of the node being decomposed over a subset of
the fan-in LUTs. The ﬁrst level node is not implemented at this stage. This will
be done in the second phase of the algorithm where the two-level representation
is converted into a multi-level implementation.
The two-level decomposition is constructed using a bin packing algorithm
approach. In the traditional bin packing algorithm, the goal is to ﬁnd the min-
imum number of bins with a given capacity, into which a set of boxes can be
packed. In the Chortle algorithm, the bins are the second-level LUTs and the
boxes are the fan-in LUTs. The capacity of each bin is the number k of LUT
inputs. The packing at this stage consist of combining two fan-in LUTs, LUT1
that realizes the function f1 and LUT2 that realizes the function f2 into a new
LUT LUTr that implements the function f1 ⊘f2, where ⊘is the operation
implemented by the fan-out node. Figure 3.9(a) shows an original graph and
its decomposition is shown in 3.9(b). In example, the ⊘is the OR function.
(a) Fan-in LUTs at a node
(b) The two-level decomposition
Figure 3.9.
Chortle two-level decomposition

Implementation
85
Algorithm 3 Chortle’s two-level decomposition
Start with an empty list of LUTs
while there are unpacked fan-in LUTs do
if the largest unpacked fan-in LUT will
not ﬁt within any LUT in the list then
create an empty LUT and add
it to the end of the list
end if
pack the largest unpacked fan-in LUT
into the ﬁrst LUT it will ﬁt within
end while
The pseudocode of the two-level decomposition algorithm is given in
algorithm 3.
This algorithm uses a ﬁrst-ﬁt-decreasing (FFD) method, which places a fan-
in LUT to be packed into the ﬁrst LUT it will ﬁt within. However, a best-ﬁt
(BF) approach that packed the fan-in LUTs into the LUT they best ﬁt in can
also be used.
Multi-level Decomposition.
In the second step, the ﬁrst-level nodes are im-
plemented using a tree of LUTs. The number of LUTs used is minimized by
using second-level LUTs that have unused pins to implement a portion of the
ﬁrst-level tree as shown in ﬁgure 3.10. The detailed procedure for converting a
two-level decomposition into a multi-level decomposition is given in algorithm
4, and ﬁgure 3.10 provides an illustration of this process.
The fact that the most ﬁlled unconnected LUTs are always selected pushes
less ﬁlled LUTs to the root of the tree being built. Therefore, the LUTs with
the most unused inputs will be found near the root of the tree.
Algorithm 4 Chortle’s multi-level decomposition
while there is more than one unconnected LUT do
if there are no free inputs among
the remaining unconnected LUTs then
create an empty LUT and add
it to the end of the LUT list
end if
connect the most ﬁlled unconnected LUT
to the next unconnected LUT with a free input
end while

86
Reconﬁgurable Computing
Figure 3.10.
Example of multi-level decomposition
Improvement of the Chortle algorithm.
Before applying the decomposi-
tion on the trees in the forest, the Chortle algorithm performs a pre-processing
step in which De Morgan’s Theorem as well as the associativity rules are ap-
plied in order to insure that
the only inverted edges in the trees are those originating from the leaf nodes.
no consecutive OR and no consecutive AND exist in the trees.
Subject to this, Francis et al. [87] could prove that the Chortle algorithm con-
struct an area optimal algorithm for LUTs with less than or equal ﬁve inputs.
Exploiting the reconvergent paths.
A further optimization of the Chortle
consists of optimizing the reconvergent paths to improve the implementation
at a given node. A reconvergent path is caused by a leaf node with a fan-out
greater than one. This produces two paths in the leaf-DAG that terminate at
a given node. The goal of the optimization is to pack the reconvergent paths
caused by a given input into just one LUT.

Implementation
87
Figure 3.11.
Exploiting reconvergent paths to reduce the amount of LUTs used
This strategy is illustrated in ﬁgure 3.11. To see are two paths going through
the blocks g and h and placed in the same LUT, thus reducing the number of
LUTs used from ﬁve to four.
If more than one pair of fan-in LUTs share inputs, there will be several
pairs of reconvergent paths. To determine which one should be packed in the
same LUT, two approaches exist in the Chortle algorithm. The ﬁrst one is an
exhaustive approach that ﬁrst ﬁnds all pairs of fan-in LUTs that share inputs.
Then every possible combination is constructed by ﬁrst merging the fan-in
LUTs and then proceeding with the FFD bin packing algorithm. The two-level
decomposition that produces the fewest bins and the smallest least ﬁlled bins
is retained as the solution.
A large amount of pairs of fan-in LUTs sharing the same inputs cause the
algorithm to be impracticable. To overcome this limitation, a second heuristic
called the maximum share decreasing (MSD) can be used. The goal is to max-
imize the sharing of inputs when fan-in LUTs (boxes) are packed into bins.
The MSD iteratively chooses the next box to be packed into bins according to
the following criteria: 1) the box has the greatest number of inputs, 2) the box
shares the greatest number of inputs with any existing bin and 3) it shares the
greatest number of inputs with any of the remaining boxes.
The ﬁrst criterion insures that the MSD algorithm works like the FFD if no
reconvergent path exists. The second and third criteria help to place boxes that
share inputs into the same bins. The chosen box is then packed into the bins
with which it shares the most input without exceeding the bin capacity, i.e. the
number of inputs. If no more bins exist for packing the chosen bin, then a new
bin is created, and the chosen box is packed into the new bin.
Logic replication at fan-out LUTs.
The replication of the logic at fan-out
nodes can also help to reduce the amount of LUTs used. As stated earlier,
the Chortle technology mapping ﬁrst decomposes the Boolean network into a
forest, then maps each tree separately and ﬁnally assembles the ﬁnal circuit

88
Reconﬁgurable Computing
Figure 3.12.
Logic replication at fan-out nodes to reduce the number of LUTs used
from the mapped threes. In the assembling phase, logic replication is used to
reduce the amount of LUTs in the ﬁnal circuit as illustrated in ﬁgure 3.12.
The chortle algorithm exists in two versions. One with the replication of all
nodes and the other without replication. The solution that produces the less
amount of LUT is retained.
The Chortle algorithm presented in this section is known in the literature
[87] as the Chortle-crf.6
As seen in ﬁgure 3.10, the path generated by the Chorlte algorithm can be-
come very long if no effort is spent in reducing the delay. This problem is
addressed in another version of the Chrotle algorithm, the chortle-d.7 Instead
of using the bin packing to minimize the amount of LUTs used, the Chortle-d
focus in the bin packing strategy, on the reduction of the number of levels in the
ﬁnal design. Because we investigate a delay optimal algorithm in the next sec-
tion, the Chortle-d algorithm will not be considered further. The FlowMap
algorithm that we next present focuses on delay minimization using a net-
work ﬂow approach, a technique that was also use in the MIS-pga algorithm of
Murgai et al. [163].
3.3.2
The FlowMap Algorithm
Cong and Ding proposed a polynomial time algorithm for LUT-based tech-
nology mapping with the goal of delay minimization [53]. In their algorithm,
the FlowMap uses the notion of cut in a network to construct an optimal solu-
tion in polynomial time.
Because the delay of the ﬁnal circuit is determined by the delays in the LUTs
as well as those of the interconnections, an algorithm seeking to minimize the
delay should consider those two factors. However, as the components are not
placed and routed on the chip before the technology mapping phase, no real
accurate estimation of the interconnection delays can be done. The technology
6c is for the constructive bin packing, r for the reconvergent path and f for the logic replication.
7d stays for delay

Implementation
89
mapping with delay minimization as goal can be done only on the basis of the
LUT delays. Delay minimization is equivalent to the minimization of the depth
of the resulting DAG, i.e. the minimization of the number of LUTs on a path
from the primary inputs to the primary outputs of the ﬁnal circuit.
In contrast to most existing algorithms that ﬁrst partitioned the Boolean net-
work into a forest of trees and then map each tree separately, the FlowMap
algorithm can be directly applied to the Boolean network. The precondition
is that the network must be K-bounded. If this is not the case, then a pre-
processing must be performed on the network to transform it to a K-bounded
one. This can be done for example with the same approach used in the Chortle
to generate the trees.
The FlowMap algorithm is a two-step method, which ﬁrst determines the
labels of the nodes in the ﬁrst phase and then, in the second phase, assembles
the nodes in the LUTs according to their level number. We present the two
steps in the next paragraphs.
First Phase: Node Labeling.
Labeling is based on the notion of cut in a
network. We therefore ﬁrst recall some deﬁnitions related to the notion of
network and ﬂow in a network.
Definition 3.6 (Network, cut, label, height, volume)
Given a network N = (V, E) with a source s and a sink t, a cut is a
partition (X, X) of the graph N such that s ∈X and t ∈X).
The cut-size n(X, X) of a cut (X, X) is the number of nodes in X adjacent
to some nodes in X
A cut (X, X) is K-feasible if n(X, X) ≤K
The edge cut-size e(X, X) of (X, X) is the sum of the crossing edges ca-
pacities.
The label l(t) of a node t is deﬁned as the depth of the LUT, which contains
t in an optimal mapping of the cone at t.
The height h(X, X) of a cut (X, X) is the maximum label in X.
h(X, X) = max {l(x) : x ∈X}
The volume vol(X, X) of a cut (X, X) is the number of nodes in X.
vol(X, X) =
X

The level of the K-LUT containing the node t in an optimal mapping of N
is at least l(t), and the label of all primary outputs of N is the depth of the
optimal mapping of N.

90
Reconﬁgurable Computing
The ﬁrst phase of the algorithm computes the label of the nodes in a topolog-
ical order, starting with the primary inputs of the graph. The topological order
guarantees that each node is processed after all its predecessors have been pro-
cessed. The labeling is done as follows: Each primary input u is assigned the
label l(u) = 0. For a given node to be processed at a given level, the sub-
graph Nt representing the cone at node t is transformed into a network Nt by
inserting a source node s which is connected to all inputs of Nt as shown in ﬁg-
ure 3.13. For the sake of simplicity, the resulting network is still denoted as Nt
Assuming that t is implemented in the K-LUT LUT(t) in an optimal map-
ping of Nt, the cut (X(t), X(t)), where X(t) is the set of nodes in LUT(t)
and X(t) = Nt −X(t), is a K-feasible cut between s and t and the level of
LUT(t) is l(u) + 1, where u is the node of X(t) with the maximum level.
With the previous reﬂection, the K-LUT mapping with minimal delay is
reduced to the problem of ﬁnding a K-feasible cut with minimum height8 for
each node in the graph. The level l(t) of the node t is therefore given by the
following formula:
l(t) = min{(X,X) is K−feasible}(h(X, X) + 1)
(3.1)
The following Lemma results from the previous discussion, and therefore, it
will be given without proof.
Figure 3.13.
Construction of the network Nt from the cone Ct at node t
8It is assumed here that no cut (X, X) is computed with primary input nodes in X.

Implementation
91
Lemma 3.7 The minimum depth of any mapping solution of Nt is given by:
l(t) = min{(X,X) is K−feasible}(h(X, X) + 1)
Figure 3.14 illustrates the FlowMap labelling method. Because there is a min-
imum height 3-feasible cut in Nt, we have l(t) = 2 and the optimal 3-LUT
mapping solution for Nt is given in the ﬁgure.
The goal of minimizing the delay can be reduced to efﬁciently compute
the minimum height K-feasible cut for each node visited in the graph. The
FlowMap algorithm constructs a mapping solution with minimum delay in
time O(Km), where m is the number of nodes in the network. Further trans-
formations are required on the networks Nt in order to reach this goal. The
node labels deﬁned by the FlowMap scheme satisfy the following property.
Lemma 3.8 Let l(t) be the label of node t, then l(t) = p or l(t) = p + 1,
where p is the maximum label of the nodes in input(t).
Proof : Let t′ be any node in input(t). Then for any cut (X, X) in Nt, either
1 t′ ∈X or
2 (X, X) also determines a K-feasible cut (X′, X′) in Nt′ with
h(X′, X′) ≤h(X, X), where X′ = X ∩N ′
t and X′ = X ∩N ′
t. Those two
cases are illustrated in ﬁgure 3.15
Figure 3.14.
Minimum height 3-feasible cut and node mapping

92
Reconﬁgurable Computing
Figure 3.15.
Illustration of the two cases in the proof of Lemma 3.8
In the ﬁrst case, we have l(t) = h(X, X) + 1 ≥l(t′) + 1 and therefore
l(t) ≥l(t′)
In the second case we have l(t′) −1 ≤h(X′, X′) ≤h(X, X) = l(t) −1,
which implies l(t) ≥l(t′). Therefore, l(t) ≥p. This part of the proof shows
that the label of a node cannot be smaller than those of its predecessors.
Because the network Nt is K-bounded, input(t) ≤K. Therefore, (Nt −
t, {t}) is a K-feasible cut. As each node in Nt −{t} is either in input(t) or
is a predecessor of some node in input(t), the maximum label of the nodes in
Nt −{t} is p. Therefore, h(Nt −{t}, {t}) = p, i.e l(t) ≤p + 1.
According to Lemma 3.8, a delay optimal mapping algorithm could ﬁrst
check whether there is a K-feasible cut (X, X) of height p −1 in Nt. If such
a cut exists, then l(t) is assigned the value p and the node t will be packed
in the second phase in a common LUT with the nodes in X. If such a cut
does not exist, then the minimum height of the K-feasible cuts in Nt is p and
Nt −{t} , {t} is such a cut. The value of l(t) is set to p + 1 in this case, and
a new LUT will be used for t in the second phase of the algorithm. This is the
way the FlowMap algorithm works.
We now face the next problem, namely ﬁnding out if a network has a K-
feasible cut with a given height h. In order to provide an answer to this ques-
tion, further transformations are required. The ﬁrst one transforms Nt into N ′t
as described in Lemma 3.9.
Lemma 3.9 Let N ′t be the graph obtained from Nt by applying a transfor-
mation which collapses all the nodes in Nt with maximum label p −1 together
with t in a new node t′. Nt has a K-feasible cut of height p −1 if N ′t has a
K-feasible cut.
Proof : Let Ht denote the set of node in Nt that are collapsed into t′.
⇐If N ′t has a K-feasible cut(X′, X′), let X = X′ and X = (X′ −{t′} ∪
Ht), then (X, X) is a K-feasiblecut of Nt. Because no node in X′(= X) has a

Implementation
93
label p or larger, we have h(X, X) ≤p−1. Furthermore, according to Lemma
3.8, l(t) ≥p, which implies h(X, X) ≥p −1. Therefore, h(X, X) = p −1.
⇒If Nt has a cut (X, X) of height p −1, then X cannot contain any node
of label p or higher. Therefore, Ht ⊆X, meaning that (X, (X −Ht) ∪{t′})
forms a K-feasible cut of N ′
t.
Now that the problem of ﬁnding a minimum height K-feasible cut in the
network Nt is reduced to the problem of ﬁnding a K-feasible cut in N ′
t, exist-
ing network ﬂow algorithms can be used to compute cuts in N ′
t and then check
if they are feasible. Unfortunately, for those algorithms, the cuts are deﬁned
on crossing edges. Moreover, it is difﬁcult to establish a relation between the
number of crossing edges and the number of adjacent nodes in the original net-
work. A second transformation, called node splitting, is therefore applied on
N ′
t. The goal of this second step is to reduce the node cut-size constraint in N ′
t
to an edge cut-size constraint in the new graph N ′′
t and then use well-known
existing edge-cut computation algorithms.
The node splitting transforms the graph N ′
t into a graph N ′′
t as follows:
For each node v in N ′
t other than s and t′, two new nodes v1 and v2 are
introduced and connected by a bridging edge (v1, v2).
The source s and sink t′ are also introduced in N ′′
t . For each edge (s, v)
respectively (v, t′) in N ′
t, an edge (s, v1) respectively (v2, t′) is introduced
in N ′′
t .
For each edge (u, v) in N ′
t with u ̸= s and v ̸= t′, an edge (u2, v1) is
introduced in N ′′
t . The capacity of each bridging edge is set to one and
those of the non bridging edges is set to inﬁnity.
The constructions of the graphs N ′
t and N ′′
t are illustrated on ﬁgures 3.16
and 3.17.
The transformation of the graph N ′
t into N ′′
t insures that if a cut exists in
N ′′
t with capacity less than K, then no edge with inﬁnite capacity will be a
crossing one. The only edges that would be crossing the cut are the bridging
ones. Because each bridging edge represents a node of N ′
t and the capacity of
each bridging edge is one, the edge size cut in N ′′
t is equivalent to the node cut
size in N ′
t. Based on this observation, the following Lemma can be stated.
Lemma 3.10 N ′
t has a K-feasible cut if N ′′
t has a cut whose edge cut size is
no more than K.
The Ford and Fulkerson method [84, 55] can be used to check if a cut with
edge cut-size smaller or equal K exists. We ﬁrst brieﬂy provide some more
background on networks, which are important to understand the testing proce-
dure.

94
Reconﬁgurable Computing
Figure 3.16.
Transforming Nt into N ′
t by node collapsing
In a network N with source s and sink t, a ﬂow can be seen as a streaming
of data on different direction on the edges of the network. A node might have
data coming in and data going out through its edges, each of which has a given
capacity. Data streaming in the s-direction (resp. in the t-direction) caused a
Figure 3.17.
Transforming the node cut constraints into the edge cut ones

Implementation
95
negative (resp. positive) value of the ﬂow at a given node. The value of the
ﬂow in the network is therefore the sum of the ﬂows on the network edges.
Formally, a ﬂow f in N can be deﬁned as a function of N × N in
 .
A residual value exists on an edge if the ﬂow at that edge has a lower value
than the edge capacity. The residual value is then the difference between the
capacity and the ﬂow’s value. This can be added to the ﬂow in order to saturate
the edge. The capacity of a cut in the network is deﬁned as the sum of all
positive crossing edge capacities. Negative crossing edges do not inﬂuence the
capacity of a cut. An edge not saturated by the ﬂow is called a residual edge.
The residual network of a network is made upon all the residual edges as well
as the nodes that they connect. An augmenting path in the residual network is
a path from source s to the sink t that contains only residual edges, i.e. a path
in the residual network.
A very important relationship between a ﬂow and a cut in a network is given
by the following corollary:
Corollary 3.11 The value of any ﬂow in the network is bounded from
above by the capacity of any cut in the network.
This implies the following: the maximum ﬂow value in the network is
bounded from above by the minimum cut of the network.
Based on the above observation, the notion of cut and that of residual net-
work, the famous max-ﬂow min-cut theorem [55] of Ford and Fulkerson states
that a ﬂow is maximum in the network if the residual network does not contain
any augmenting path. The value of the ﬂow is then equal to the capacity of the
minimum cut.
Applying the max-ﬂow min-cut theorem to our problem, we can state the
following: If a cut with edge cut-size smaller or equal K exists in N ′′
t , then the
maximum value of any ﬂow between s and t′′ in N ′′
t will be less than K.
Because we are only interested in testing if the value of the cut is smaller
than K, the augmenting path of Ford an Fulkerson can be applied to compute
the maximum ﬂow. The approach starts with a ﬂow f, whose value is set to
0 and then iteratively ﬁnd some augmenting path P in the residual network
and increase the ﬂow on P by the residual capacity cf(P), that is the value
by which the ﬂow on each edge of P can be increased. If no path from s to t
exists, then the computing stops and return the maximum ﬂow value.
Because each bridging edge in N ′′
t have a capacity of one, each augmenting
path in the ﬂow residual graph of N ′′
t from s to t′′ increases the ﬂow by one.
Therefore, the augmenting path can be recursively used to check whether the
maximum value for a ﬂow associated to a cut is less than K. For a given cut,
if a K + 1 augmenting paths could be found, then the maximum ﬂow in N ′′
t
has a value more than K. Otherwise, the residual graph will not contain a
(K + 1)th path.

96
Reconﬁgurable Computing
Testing if N ′′
t has a cut, whose value is no more than K, can therefore be
done through a depth ﬁrst search starting at s and including in X′ all nodes
reachable by s. The run-time of the Ford and Fulkerson method is O(m|f ∗|,
where |f ∗| is the value of the maximal ﬂow computed. In the FlowMap algo-
rithm, this value is K, which corresponds to the number of iterations that were
performed to ﬁnd the K augmenting paths. Since ﬁnding an augmenting path
takes O(m) (m being the number of edges of N ′′
t ), testing if a cut with edge
cut-size less or equal K exists can be determined in time O(Km). The result-
ing cut (X′′, X′′) in N ′′
t induces a cut (X′, X′) in N ′
t which in turn induces a
K-feasible cut (X, X) in Nt.
Because the above procedure must be repeated for each node in the original
Boolean network, we conclude that the labelling phase, i.e. computing the
label of all edges in the graph, can be done in O(Kmn) where n is the number
of nodes and m the number of edges in N.
Node Mapping.
In its second phase, the FlowMap algorithm maps the nodes
into K-LUTs. The algorithm works on a set L of node outputs to be imple-
mented in the LUTs. Each output will therefore be implemented as a LUT-
output.
Initially, L contains all primary outputs. For each node v in L, it is assumed
that the minimum K-feasible cut (X, X) in Nv has been computed in the ﬁrst
phase. A K-LUT LUT v is then generated to implement the function of v as
well as that of all nodes in X. The inputs of LUT v are the crossing edges
from X to X which are less than K, because the cut is K-feasible. L is then
updated to be (L−{v})∪input(X). Those nodes w belonging to two different
cut-set Xu and Xv will be automatically duplicated. Algorithm 5 provides the
pseudocode that summarizes all the steps of the FlowMap presented here.
Improvements performed in the FlowMap algorithm have the goal of re-
ducing the amount of LUTs used, while keeping the delay minimal. The ﬁrst
improvement is used during the mapping of the nodes into the LUTs. The al-
gorithm tries to ﬁnd the K-feasible cut with maximum volume. This allows
the ﬁnal LUTs to contain more nodes, thus reducing the area used. The second
possibility is the used of the so-called ﬂow-pack method, which generalizes the
predecessor packing used in the DAG-Map [46]. The idea of predecessor pack-
ing is to pack K-inputs LUT v in the same K-inputs LUT u, if v is a fan-out
free fan-in LUT of u and if the total amount the inputs of u and v is less than
K. The ﬂow-pack method generalizes the predecessor packing method to the
set of predecessors Pu of u (including u) of the node u, provide that Pu has a
number of inputs less or equal to K.
Figure 3.18 illustrates the predecessor packing approach. The gate decom-
position presented in the Chortle algorithm can be used as well for the reduc-
tion of the LUT amount.

Implementation
97
Figure 3.18.
Improvement of the FlowMap algorithm through efﬁcient predecessor packing
Algorithm 5 Pseudocode of the FlowMap algorithm
/* phase 1: node labeling */
for each PI node v do
l(v) := 0
end for
T := list of non-PI nodes in topological order
while T is not empty do
remove the ﬁrst node t from T
construct the network Nt
let p = max(l(u)) : u ∈input(t)
transform Nt into N ′
t using collapsing
transform N ′
t into N ′′
t using node splitting
compute a cut (X′′, X′′) in N ′′
t with e(X′′, X′′) ≤K
using the augmenting path method
if (X′′, X′′) with e(X′′, X′′) ≤K is not found in N ′′
t then
Xt := {t}
l(t) := p + 1
else
Induce a cut (X, X) in Nt from the cut (X′′, X′′) in N ′′t
Xt := X
l(t) := p
end if
end while
/* phase 2: mapping nodes to LUTs */
L := list of PO nodes
while L contains non-PI nodes do
take a non-PI node v from L
generate a K-LUT LUT(v′) to implement the function of v
such that input(v′) = input(X)
L := (L −{v}) ∪input(LUT(v′))
end while

98
Reconﬁgurable Computing
4.
Conclusion
In this chapter, the general use of FPGAs in different systems was explained
as well as the different steps needed to synthesize the hardware modules that
will be executed in the FPGAs at run-time. We have taken a brief tour in
logic synthesis with the focus placed on LUT technology mapping, which is
the step by which FPGAs differ from other logic devices. Our goal was to
present two algorithms that best match the optimization requirements, namely
minimal delay and minimal area. Research in FPGA synthesis keep on go-
ing and we several LUT-based technology mapping algorithms are expected to
be developed in the future. The dynamic development in FPGAs introduces
new challenges for the synthesis tools, which has to deal with new elements
such as embedded arithmetic modules, embedded memory, different clocking
schemes, etc. Therefore, good technology mappers are required to cope with
the on-going innovations. Our goal in this chapter was to provide some back-
ground to better understand the challenges and provide better solutions for the
next generations of FPGAs.

Chapter 4
HIGH-LEVEL SYNTHESIS
FOR RECONFIGURABLE DEVICES
One of the key points for the success of microprocessors is the ease in pro-
gramming such systems. This is in part due to the maturity of compilers as
well as the operation mode of microprocessors, the Von Neumann paradigm
that allows any sequential program to be executed on the underlying hardware.
In almost three decades of progress in compilers and microprocessors, a huge
amount of algorithms have been developed, coded and deployed in high-level
languages such as FORTRAN, C, C++, Java. Most of those existing programs
can be executed with low modiﬁcations and low porting efforts on new plat-
forms. With the very attractive nature of software programming, a very large
community of programmers has grown, thus providing software code for the
most existing problem. The consequence of this development is that high ex-
pectation in programmability is placed on new hardware platforms. A new and
highly innovative hardware computing platform, providing the best architec-
tural organization to speed-up applications will certainly fail to be adopted, if
its programmability is poor. A less-competitive hardware platform in turn will
certainly succeed if the portability of existing algorithms is shown to be easy
with a small increase in the computation time. This concept has led to the de-
velopment of languages and frameworks, which allows for the compiler to be
generated for a speciﬁed hardware description [168] [187].
Although a certain degree of maturity has been reached in software pro-
gramming, or better said in sequential programming languages and compilers,
parallel programming has not experienced the same advancements. The par-
allel implementation of a given application requires two main steps. First, the
application must be partitioned to identify the dependencies among the dif-
ferent parts and set the parts that can be executed in parallel. This partition-
ing step requires a good knowledge on the structure of the application.After

100
Reconﬁgurable Computing
partitioning, a mapping phase is required to allocate the independent blocks
of the application to the computing hardware resources. For this purpose, a
good understanding of the underlying hardware structure is required. As a
consequence, the community of programmers is not very enthusiastic when-
ever parallel programming is concerned.
Reconﬁgurable computing also faces the disadvantages of parallel comput-
ing. To program most of the existing systems, one must write code in a given
hardware description language such as VHDL or Verilog and sometimes in a
C-like language, which syntax is not so far from VHDL or Verilog. It is there-
fore not easy to decouple the implementation process from the hardware. The
consequence is that FPGA programming is still considered a close ﬁeld for
electrical engineers, despite all the beneﬁts in performance and ﬂexibility that
they provided.
High-level synthesis has been introduced for more than a decade as an at-
tempt to describe electronic systems using a high-level language that can be
compiled down to the hardware without manual intervention of the user. Hav-
ing such systems, software developers could be encouraged to develop more
applications for reconﬁgurable systems, and existing applications could be
ported on such systems with low effort, thus increasing the popularity of such
systems.
High-level synthesis for reconﬁgurable devices is slightly different from that
of other electronic devices in that the resources on which part of the application
must be mapped are not ﬁxed. This provides a greater ﬂexibility during the
temporal mapping of code segments to computing blocks.
In this chapter, we present the high-level synthesis problem in general and
focus on the high-level synthesis for reconﬁgurable devices, also known as
temporal partitioning. Afterwards, we will investigate the algorithms that were
developed for this purpose.
1.
Modelling
High-level synthesis (HLS) deals with the speciﬁcation of a given applica-
tion at a very high level of abstraction as well as its implementation on a given
platform. The starting point is to specify the application in a given high-level
language or tool. For this modelling step, several possibilities exist for cap-
turing the behaviour of a systems, from the very simple ﬁnite state machines
(FSM) and their extensions such as State charts, Control Dataﬂow Graphs, to
very complex tools such as the Petri Nets, each of which has a different level of
powerfulness. We ﬁrst present the dataﬂow graph that is used as model in this
chapter. We also consider two extensions of dataﬂow graphs, the sequencing
graphs and the ﬁnite state machine with datapath and explain why those two
models are not used in this context.

High-Level Synthesis for Reconﬁgurable Devices
101
1.1
Dataﬂow Graphs
A dataﬂow graph (DFG) provides a means to describe a computing task
in a streaming mode. Given a code segment in a high-level language such as
C or C++, each operator represents a node in the dataﬂow graph. The inputs
of the nodes are the operand on which the corresponding operator is applied.
The output of a node represents the result of the operation on that node. The
output of a node can be used as input to other nodes, thus deﬁning a data de-
pendency in the graph. Nodes that they depend on others are called successors
of the nodes on which they depend. Nodes on which other nodes depend are
the predecessors of the nodes that depend on. Some nodes in the graph have no
predecessors and others have no successors. Dataﬂow graphs might be normal-
ized by inserting in the graph two fake nodes, whose operation has no effect
on the dataﬂow computation. The ﬁrst fake node is connected as predecessor
of all nodes without any predecessor, and the second fake node is connected as
successor of all nodes that have no successors.
Example 4.1 As example of dataﬂow graph, consider the computation of
the quadratic root using the formula x =
(b2−4ac)
1
2 −b
2a
. The corresponding
dataﬂow graph is shown in ﬁgure 4.1. Data are streamed through the inputs
a, b, c, d, 4, 2 and the result is collected at the output of the graph.
Formally, we deﬁne a dataﬂow graph as follows:
Definition 4.2 (Dataflow Graph)
Given a set of tasks T = {T1, ...., Tk},
a dataﬂow graph (DFG) is a directed acyclic graph G = (V, E), where
V = T is the set of nodes representing operators and E is the set of edges.
An edge e = (vi, vj) ∈E is deﬁned through the (data)dependency between
task Ti and task Tj.
We assume that for each task, an equivalent hardware implementation exists,
which occupies a rectangular area on the chip. Therefore, the nodes as well as
the edges in a DFG possess some characteristics such as height, length, area,
latency and width that are derived from the hardware resources used later to
implement those tasks. Those values are formally deﬁned as follows:
Definition 4.3 (Latency, length, height, area, weight of
nodes and edges)
Given a node vi ∈V and its implementation Hvi as rectangular shape module
in hardware.
1 li denotes the length and hi the height of Hvi
2 ai = li × hi denotes the area Hvi

102
Reconﬁgurable Computing
Figure 4.1.
Dataﬂow Graph for Quadratic Root
3 The latency ti of vi is the time it takes to compute the function of vi using
the module Hvi
4 For a given edge eij = (vi, vj), which deﬁnes a data dependency between
vi and vj, we deﬁne the weight wij of eij as the width of bus connecting
two components Hvi and Hvj.
5 The latency tij of eij is the time needed to transmit data from Hvi to Hvj.
For sake of simplicity, we will just use the notation vi to denote a node of the
graph as well as its hardware implementation Hvi.
Any program written in a high-level language can be compiled into a dataﬂow
graph, provided that the program is free of loops and branching instructions.
This restriction does not match with the reality, as loops and branch instruc-
tions are available in most of the programs, for the evaluation of branching
conditions at run-time, and to decide on the segment to be executed according
to the value of the condition variables. In case of non nested loops, the body of
a loop is always a set of instructions that can be represented using a dataﬂow
data structure. Several extensions of dataﬂow graph exist to capture program
with control structures and loop. We will consider two of them in this chapter:
the sequencing graphs and the ﬁnite state machines with datapath.

High-Level Synthesis for Reconﬁgurable Devices
103
1.2
Sequencing Graph
A sequencing graph [156] is a hierarchical dataﬂow graph with two differ-
ent types of nodes: The operation nodes corresponding to normal ‘task nodes’
in a dataﬂow graph and the link nodes or branching nodes that point to another
sequencing graph in a lower level of the hierarchy. Linking nodes evaluate
conditional clauses. They are placed at the tail of alternative paths correspond-
ing to possible branches. Loops can be modelled by using a branching as a
tail of two paths, one for the exit from the loop and the other for the return to
the body of the loop, which is the sub-sequencing graph associated with the
link node.
Example 4.4 Figure 4.2 shows an example of sequencing graph with a
branching node BR containing two branching paths.
According to the conditions that node BR evaluates, one of the two sub-
sequencing (1 or 2) graphs can be activated. To implement a loop, only one
sub-sequencing graph in which the body of the loop is implemented, will be
considered. The node BR will then evaluate the exit condition and branch to the
next node of the hierarchy level, if the condition holds. Otherwise, the body of
the loop is re-entered by reactivating the corresponding sub-sequencing graph.
1.3
Finite State Machine with Datapath
Another extension can be done on a dataﬂow by integrating a ﬁnite state
machine in the model to control the execution on a datapath deﬁned by the
dataﬂow graph. The result is the so-called ﬁnite state machine with datapath
Figure 4.2.
Sequencing graph with a branching node linking to two different sub graphs

104
Reconﬁgurable Computing
(FSMD).1 We adopt in this section the FSMD deﬁnitions and terminology from
Vahid et al. [203]. Thereafter, a FSMD can be formally deﬁned as a 7-tuple
< S, I, O, V, F, H, s0 > where:
S = {s0, s1 · · · , sl} is a set of states,
I = {i0, i1 · · · , im} is a set of inputs,
O = {o0, o1 · · · , on} is a set of outputs,
V = {v0, v1 · · · , vn} is a set of variables,
F : S × I × V →S is a transition function that maps a tupel (states, input
variable, output variable) to a state,
H : S →O + V is an action function that maps the current state to output
and variable,
s0 is an initial state.
FSMDs have some fundamental differences with traditional ﬁnite state ma-
chines. First, the transition function operates on arbitrary complex data type
like in high-level programming language; second, the transition and action
functions may include arithmetic operations rather than just Boolean opera-
tions. The arithmetic operations and the complex data types implicitly deﬁne
a datapath structure in the speciﬁcation.
The transformation of a program into an FSMD is done by transforming the
statements of the program into FSMD states. The statements are ﬁrst classiﬁed
into three categories:
the assignment statements: For an assignment statement, a single state is
created that executes the assignment action. An arc connecting the so cre-
ated state with the state corresponding to the next program statement is
created.
the branch statements: For a branch statement, a condition state C and
a join state J both with no action are created. An arc is added from the
conditional state to the ﬁrst statement of the branch. This branch is labelled
with the ﬁrst branch condition. A second arc, labelled with the complement
of the ﬁrst condition ANDed with the second branch condition is added
from the conditional state to the ﬁrst statement of the branch. This process
is repeated until the last branch condition. Each state corresponding to the
last statement in a branch is then connected to the join state. The join state
1In the original deﬁnition from Gasky [91], the extension is done on an FSM to support more complex
data types and variables as well as complex operators.

High-Level Synthesis for Reconﬁgurable Devices
105
is ﬁnally connected to the state corresponding to the ﬁrst statement after the
branch.
and the loop statements: For a loop statement, a condition state C and a join
state J, both with no action are created. An arc, labelled with the loop con-
dition and connecting the conditional state C with the state corresponding
to the ﬁrst statement in the loop body is added to the FSMD. Accordingly,
another arc is added from C to the state corresponding to the ﬁrst statement
after the loop body. This arc is labelled with the complement of the loop
condition. Finally, an edge is added from the state corresponding to the last
statement in the loop to the join state and another edge is added from the
join state back to the conditional state.
The transformation steps of a given program to an FSMD is illustrated in
ﬁgure 4.3.
Example 4.5 Let us model the greatest common divisor (GCD) of two num-
bers, using an FSMD for as explained [203]. The sequential version of the
GCD is given in algorithm 6 and the corresponding FSMD is shown in
ﬁgure 4.4.
The loop state (C1) and the branching state within the loop are white and
other states are grey. Also, we labelled the states with the action to be taken in
those states. An additional label corresponding to the line of the instruction in
the program is placed on each state.
Figure 4.3.
Transformation of a sequential program into a FSMD

106
Reconﬁgurable Computing
Algorithm 6 The greatest common divisor sequential algorithm
1: variable a, b, gcd: integer;
2: done := FALSE;
3: while (!done) do
4:
if (a > b) then
5:
a := a - b;
6:
else
7:
if (b > a) then
8:
b := b - a;
9:
else
10:
done = TRUE;
11:
end if
12:
end if
13: end while
14: gcd := a;
Figure 4.4.
Transformation of the greatest common divisor program into an FSMD

High-Level Synthesis for Reconﬁgurable Devices
107
After the program transformation into an FSMD, a datapath must be created
that will be controlled by a ﬁnite state machine derived from the FSMD. The
process of creating the datapath is straightforward. First, a register must be
instantiated for each variable in the program. An output port implicitly declares
a variable for which a register must be created. In the second step, a functional
unit will be created for each arithmetic operation in the state diagram. The
third step consists of connecting the ports of the functional unit with those of
the variable. In the fourth step, a unique identiﬁer is created for each control
input and output of the functional units in the datapath. Sharing the operator
can be done using multiplexers.
Figure 4.5 shows the datapath of the GCD programmed together with con-
trol ﬁnite state machine resulting from the FSMD. The control signals LDA
and LDB are to load the two registers A and B with the values coming from
the subtractor, the SELL and SELR to select the corresponding value from the
multiplexer. Those signals are controlled by the FSM that set them accord-
ing to its current state. The status signals AGTB and ALTB are used by the
FSM to decide about the next state to move to, depending on the value of the
comparison between A and B.
Having speciﬁed a system using one of the modelling tools previously pre-
sented, the next step will consist of the compilation of the speciﬁcation into a
Figure 4.5.
The datapath and the corresponding FSM for the GCD-FSMD

108
Reconﬁgurable Computing
set of hardware components. This compilation step, also known as synthesis is
usually done in three different steps. The ﬁrst one is the allocation, which de-
ﬁnes the type of resources required by the design, and for each type the number
of instances. In the GCD case, the types of resources needed are comparators,
subtractors, registers and multiplexers. Two registers are instantiated and two
multiplexers are instantiated whereas only one subtractor and one comparator
are used. Instead of using just one subtractor and two multiplexers, we could
choose to use two subtractors and no multiplexers. The next step after the allo-
cation is the binding. In this step, each operation is mapped to an instance of a
given resource. As many operators can be mapped to the same instance of a re-
source, a schedule is used in the third step to decide on which operator should
be assigned a given resource at a given period of time. Formally, allocation
binding and scheduling can be deﬁned as follow:
Definition 4.6 (Allocation) For a given speciﬁcation with a set of op-
erators or task T = {t1, t2, · · · , tn} to be implemented on a set of resource
types R = {r1, r2, · · · , rt}. An Allocation is a function α : R →
+, where
α(r) = zi denotes the number of available instances of resource type ri
Definition 4.7 (Binding) For a given speciﬁcation with a set of opera-
tors or task T = {t1, t2, · · · , tn} to be implemented on a set of resource types
R = {r1, r2, · · · , rt}. A binding is a function β : T →R ×
+, where
β(ti) = (ri, bi), (1 ≤bi ≤α(ri)) denotes the instance of the resource type ri
on which ti is mapped to.
Definition 4.8 (Schedule) For a given speciﬁcation with a set of oper-
ators T = {t1, t2, · · · , tn}, a schedule is a function ς : V →
+, where ς(ti)
denotes the starting time of the tast ti.
1.4
Fundamental differences in HLS for reconﬁgurable
computing
The high-level synthesis for reconﬁgurable computing differs from that of
other architectures2 in two main points. First, the fundamental problems that
must be solved in common high-level synthesis are the binding and scheduling.
The allocation is usually not necessary, in particular in those systems in which
the resource types and their numbers are already ﬁxed. The binding will just
map operator to resources and the scheduling will decide which operator owns
the resource at a given time. This is not the case in reconﬁgurable computing,
where the architectural resources are created on the reconﬁgurable device ac-
cording to the resource types needed to map the operators at a given time. As
2Also known as architectural synthesis.

High-Level Synthesis for Reconﬁgurable Devices
109
long as basic computing resources are available on the device, new resources
can be built. With the uniformity of resources available on a reconﬁgurable
device, it is possible to implement any task on a given part of a reconﬁgurable
device, provided that the available resource are enough. The problem of bind-
ing an operator to a given resource then becomes an area assignment problem.
Example 4.9 Let us illustrate this major difference with using dataﬂow graph
for the computation of the functions x = ((a×b)−(c×d))+((c×d)−(e−f))
and y = ((c × d) −(e −f)) −((e −f) + (g −h)) as shown on ﬁgure 4.6.
We consider the implementation on a common architecture. Assume an al-
location that selects one instance of each resource type (multiplier, adder and
subtractor). Consider that the multiplication needs 100 basic resource units.
This can be 100 NAND-gate for example, but we express the basic unit of com-
putation in look-up table (LUT). The multiplier therefore needs 100 LUTs, the
adder and the subtractor need 50 LUTs each. Because only one instance for
each resource type is available, the two subtractions in the ﬁrst level of the
graph corresponding to the nodes 3 and 4 cannot be executed in the same step,
although enough basic resources are available. But, those resources were used
to implement one adder and one subtractor instead of two subtractors. The
adder cannot be used in the ﬁrst step, because it depends on a subtractor that
Figure 4.6.
Dataﬂow graph of the functions: x = ((a × b) −(c × d)) + ((c × d) −(e −f))
and y = ((c × d) −(e −f)) −((e −f) + (g −h))

110
Reconﬁgurable Computing
Figure 4.7.
HLS of the graph in ﬁgure 4.6 on a an architecture with one instances of the
resource types +, ∗and −
must be ﬁrst executed. The minimum execution delay is four steps if we use
chaining3 in the schedule (ﬁgure 4.7).
Consider now the implementation on an reconﬁgurable device having the
same amount of basic resources, namely 200 LUTs. Because the basic re-
sources are not bounded to any operator, they can be conﬁgured according to
our need. We are therefore able to map one multiplier and the two subtrac-
tors in the ﬁrst step. In the second step, we map one multiplier, one adder and
one subtractor. In the third step, the rest of operators are map onto the basic
resources. The resulting implementation is shown on ﬁgure 4.8. This imple-
mentation is one step faster than the previous one, although they use the same
amount of resource.
The second major difference concerns the control of the computation steps
of a given application. In general high-level synthesis, the application is speci-
ﬁed using a structure that encapsulates a datapath (set of computational
3Chaining operations means sequentially executing a set of operations in the same time slot, provided
that their overall delay is less that the clock delay.

High-Level Synthesis for Reconﬁgurable Devices
111
Figure 4.8.
HLS of the graph in ﬁgure 4.6 on a reconﬁgurable device
resources and their interconnections) and a control part. The synthesis pro-
cess then allocates the resources to operators at different time according to
a computed schedule. This temporal resource assignment is controlled by a
separate part of the system for which a synthesis is required. In reconﬁg-
urable devices, a set of hardware modules implemented as datapath normally
compete for execution on the chip. Instead of a separate control module to
set the control lines of the datapath modules, a processor is used to control
the selection process of the hardware modules by means of reconﬁguration.
The same processor is also in charge of activating the single resources in the
corresponding hardware accelerators.
For the modelling of the datapath, a
dataﬂow graph is usually sufﬁcient, as loops and branch control is left to the
processor. Although the general high-level synthesis has to control each sin-
gle operator, the high-level synthesis for reconﬁgurable devices deals with a
set of operators with different execution delay at a time. This means that for
a given application, the resources on the device are not allocated to only one
operator but to a set of operators that must be placed at the same time and
removed at the same time. With this, an application must be partitioned in
sets of operators. The partitions will then be successively implemented at dif-
ferent time on the device. This process, called temporal partitioning, allows
an application to be sequentially computed, by allowing a temporal sharing
of resource among different sets of operators. We next present the tempo-
ral partitioning problem and some of the solution approaches developed to
solve it.

112
Reconﬁgurable Computing
1.5
Temporal Partitioning
We assume that for each task ti ∈T = {t1, t2, · · · , tn} a core4 Ci is avail-
able in a given library. This can be a simple description of the task ti in a
hardware description language or the ﬁnal implementation of the task, i.e. the
ﬁnal partial bitstream for the given task. Recall that the implementation of a
given task for a given device describes all the resources used to implement the
given task in the device.
In temporal partitioning, complete partitions are downloaded onto the de-
vice rather than just a single component. Whenever a new partition is down-
loaded, it is assigned all the resources onto the device and the partition that
was running is destroyed. Two possibilities exist to build the implementation
of a given partition. The ﬁrst one is to compose the partition from the imple-
mentations of the single components in the partition. This exercise requires
some knowledge about the structure of the bitstream to place the component at
the correct locations and perform the right interconnections. The second pos-
sibility is to instantiate the HDL description of each single component in the
HDL description of the partition and to let the compiler do the rest of the job.
Preserving the boundary of each component is usually not required. Therefore,
the complete design is usually ﬂattened and optimized across the component
boundaries before the technology mapping, the placement and routing phases.
Each task is characterized by its latency, the height, the length and area of
its bounding box. Those values are estimated values on the basis of the im-
plementation of the task in the region deﬁned by its bounding box. It may not
be the same in the ﬁnal implementation if the complete partition design is ﬁrst
ﬂattened before optimization. The ﬂattening of a design before optimization
provides more room for optimization. The resulting design is usually smaller
than the non-ﬂattened. However, placement algorithms tend to place compo-
nents belonging to the same group in the same region. If some modules in
a given component are tied to other modules in another component, then the
placer will tend to place tied components in the same group. This may result
in an increase in the latency of the two involved components. We next provide
some deﬁnitions before stating the temporal partitioning problem.
Definition 4.10 (Configuration) Given a reconﬁgurable processing
unit H and a set of tasks T = {t1, ...., tn} available as cores C = {c1, ...., cn},
we deﬁne the conﬁguration ζi of the RPU at time ti to be the set cores
{ci1, ..., cik} ⊆C running in H at time ti. We set ζi = {ci1, ..., cik}
We consider a model in which the execution control is left to the processor
that is also in charge of controlling the reconﬁguration process on the chip.
4Hardware module.

High-Level Synthesis for Reconﬁgurable Devices
113
We therefore use the dataﬂow model as entry point for the part of the design
to be implemented in the reconﬁgurable device. Because the control loops are
no more available in the design, cycles can be only available in the dataﬂow
graph, if the outputs of some components are used as input to some of their
predecessors. This is for instance the case with the GCD-datapath in ﬁgure 4.5,
where the output of the subtractor is used as inputs for the two registers A and
B. In this case, the part of the datapath containing the loop is encapsulated into
a single component that will be executed for a given number of clock cycles
ﬁxed by the processor. We can also imagine a coarse-grained structure in which
a smaller controller supervises the execution process until completion. Upon
completion of the execution, status signals are used to notify the processor.
If a coarse-grained element containing loops ﬁts into the device, then it may
be entirely placed into a partition with other component. However, we may
also face the situation, where a coarse-grained element containing a loop is too
large to ﬁt onto the device. In this case, we will partition only this coarse-
grained element in parts that will be sequentially implemented in the chip. The
number of iterations will then be done in the partitioned version as required
in the original version. This approach is illustrated in ﬁgure 4.9, where the
coarse-grained node C with a loop in its implementation is partitioned in two
part P1 and P2. P1 will be downloaded before P2 in one loop iteration.
The rest of the chapter is based on the assumption that an underlying dataﬂow
graph model is available. We therefore redeﬁne the schedule on the basis of
the dataﬂow graph.
Definition 4.11 (Schedule and Ordering Relation) Given a
DFG G = (V = {v1, v2, · · · , vn}, E),
A schedule is a function ς : V →
+, where ς(vi) denotes the starting time
of the node vi that implements a task ti.
Figure 4.9.
Partitioning of a coarse-grained node in the dataﬂow graph.

114
Reconﬁgurable Computing
A schedule ς is feasible if: ∀eij = (vi, vj) ∈E, such that eij deﬁnes a data
dependency between tasks ti and tj, ς(tj) ≥ς(ti) = ti + tij (tij denotes
the latency of the edge eij and ti is the time it takes the node vi to complete
execution).
We deﬁne an ordering relation ≤among the nodes of G, such that vi ≤
vj ⇐⇒∀schedule ς, ς(vi) ≤ς(vj).
≤is a partial ordering, as it is not deﬁned for all pairs of nodes in G.
Definition 4.12 (Partition) A partition P of the graph G = (V, E) is
its division into some disjoint subsets P1, ..., Pm such that m
k=1 Pk = V
A partition is feasible in accordance to a reconﬁgurable device H with area
a(H) and pin count p(H) if:
∀Pk ⊆P: a(Pk) = (
vi∈Pk ai) ≤a(H)
1/2(
{eij∈E:(eij∩Pk)̸=∅and (eij−Pk)̸=∅}(wij)) ≤p(H)
The run-time t(Pi) of a partition Pi is the time it takes to complete the exe-
cution of that partition. It is the maximum time from the input of the data to
the output of the result. The deﬁnition of the run-time is limited to only one
execution step of the complete partition. If a partition is use in a loop, then we
must multiply the run-time of the partition by the number of iterations.
Each partitioning is submitted to the restriction that the total amount of re-
sources assigned to it, i.e. the sum of the resources of all its components does
not exceed the available amount of resources. The total area of a partition must
therefore be smaller than the device area and the weighted sum of all crossing
edges5 is less than the number of terminals or pins. This restriction on the
crossing edges can be relaxed by multiplexing the use of input output pins in
order to allow a design with a greater amount of pins to communicate with the
external world.
Definition 4.13 (Ordered Partitions) We can extend the ordering
relation ≤to partitions as follows: Pi ≤Pj
⇐⇒
∀vi ∈Pi, ∀vj ∈Pj,
either vi ≤vj or vi and vj are not in relation. Thereafter, a partitioning P is
ordered ⇐⇒an ordering relation ≤exists on P.
An ordered partitioning is characterized by the fact that for a pair of par-
titions one can always be implemented after the other with respect to any
scheduling relation.
5A crossing edge is an edge that connects one component in a partition with another component out of
the partition

High-Level Synthesis for Reconﬁgurable Devices
115
Definition 4.14 (Temporal Partitioning) Given a dataﬂow graph
G = (V, E) and a reconﬁgurable device H, a temporal partitioning of G on H
is an ordered partitioning P of G with respect to the reconﬁgurable device H.
Applying a temporal partitioning on a dataﬂow graph generates another
graph that we call conﬁguration graph. With the necessary control part of
an application, a ﬁnal schedule must be applied on the conﬁguration graph to
compute the original application. Formally, we deﬁne a conﬁguration graph as
follows:
Definition 4.15 (Configuration Graph) Given a DFG G = (V, E)
and a temporal partitioning P = {P1, ..., Pn} of G, we deﬁne a Conﬁguration
graph of G relative to the partition P, with notation Γ(G/P) to be the graph
Γ(G/P) = (P, EP ) in which the nodes are partitions in P. An edge eP =
(Pi, Pj) ∈EP ⇐⇒∃e = (vi, vj) ∈E with vi ∈Pi and vj ∈Pj.
For a given partition P, each node Pi ∈P has an associated conﬁguration ζi
that is the implementation of Pi for the given device H. Because two partitions
are never running at the same time, communication data between the partitions
must be stored in a memory external to the device. We call this memory that is
used for communication between partitions the communication memory. Phys-
ically, the communication memory might not be on a different chip than the
main system memory. It can be a reserved part of the main memory.
The computation steps for two partitions Pi and Pj such that Pi ≤Pj is
done as follows: The corresponding conﬁguration for Pi is ﬁrst downloaded
into the device. Pi copies all the data it needs to send to other partitions into
the communication memory. Upon completion, the device is reconﬁgured to
implement the partition Pj, which can then access the communication mem-
ory and collect the data that were sent to it. By interconﬁguration registers,
we denote those registers that are usually used at the boundary of the device
to hold the input and output values. Interconﬁguration registers are usually
mapped into a processor address space that can be used by the processor for
communication with the device. An example of conﬁguration graph with in-
terconﬁguration registers is given in ﬁgure 4.10.
According to the goal to be reached, several objective functions can be de-
ﬁned for the temporal partitioning problem.
One objective could be
the minimization of the number of partitions to reduce the overall reconﬁg-
uration overhead. Another objective could be the minimization of the compu-
tation time. This can be expressed for example through the minimization of
the maximum computational delay across all the partitions. A third objective
could be the minimization of the overall amount of wasted resources on the
chip. One of the major drawbacks of temporal partitioning is the non-efﬁcient
use of the device area. Temporal partitioning normally targets non-partial re-
conﬁguration devices. It has the advantage that the resulting partition can be

116
Reconﬁgurable Computing
Figure 4.10.
Example of conﬁguration graph
implemented in just one circuit. Therefore, ﬂoorplanning efforts are left to the
synthesis tools. However, when components with shorter run-times are placed
in the same partition with other components with longer run-time, those with
the shorter components remain idle for a longer period of time, resulting in a
waste of the device resources. The wasted resource of a given partition can be
formally deﬁned as follows:
Definition 4.16 (Wasted Resource) Given a dataﬂow graph G =
(V, E) and a temporal partitioning P = {P1, ..., Pk} of G:
The wasted resource wr(vi) of a node vi is the unused area occupied by
the node vi during the computation of a partition. It is the area occupied
by the node times the idle time of the node.
wr(vi) = (t(Pi)−ti)×ai, where t(Pi) is the run-time of the partition. The
idle time t(Pi) −ti of vi is the run-time of the partition minus the running
time of the component.
The wasted resource wr(Pi) of a partition Pi = {vi1, .., vin} is deﬁned as
the sum of wasted resources of its components:
wr(Pi) = n
j=1(t(Pi) −(tij) × aj
The wasted resource of a partitioning P is the sum of the wasted resource
of all its partitions.
wr(P) = k
i=1 wr(Pi)
Figure 4.11 graphically illustrates the waste resource of a partition. The run-
time of the partition is determined by the component v1. The shaded area
deﬁnes the overall wasted resource of the partition.

High-Level Synthesis for Reconﬁgurable Devices
117
Figure 4.11.
Wasted resources
The minimization of the overall run-time is difﬁcult to capture in an equa-
tion, as it can be affected by many factors. The reduction of the number of
partition automatically reduces the overall conﬁguration time, but not neces-
sarily the overall computation time. If the conﬁguration time is smaller than the
computation time of some components, an additional packing of those compo-
nents in a partition will reduce the number of partition on one side. However,
the run-time of the partitions in which the components were packed will in-
crease by a factor greater than the reconﬁguration time. Fortunately, this is not
the case with the existing technology, where the reconﬁguration time is usually
higher than the computation time of single components.
The execution time does not only depend on the partition but also on the
processor used for reconﬁguration and the speed of data exchange. Recall that
fully reconﬁgurable devices are usually coupled with a processor, which man-
ages the complete system. The processor reconﬁgures the device and manages
the data exchange. One of the most important goal to be considered in tempo-
ral partitioning is the communication overhead. This goal can be captured in
several ways. We present here a simple model to capturing the communication
cost during temporal partitioning: the graph connectivity.
Definition 4.17 (Graph Connectivity) Given a dataﬂow graph G =
(V, E), we deﬁne the connectivity, con(G) =
2×|E|
|V |2−|V |, of G as the rapport be-
tween the number of edges in E over the number of all edges that can be built
with the nodes of G.

118
Reconﬁgurable Computing
For a given subset V ′ of V , the connectivity of V ′ is deﬁned as the relation
between the number of edges connecting the nodes of V ′ over the set of all
edges that can be built with the nodes of V ′.
The connectivity of a set provides a means to measure how strongly the
components of a set are connected. High connectivity means a strongly con-
nected set, whereas low connectivity reﬂects a graph in which many modules
are not connected together. The connectivity may be used to deﬁne how good
a partitioning algorithm, whose goal is the minimization of the communica-
tion cost, has performed. We formally deﬁne the quality of a partitioning as
follows:
Definition 4.18 (Quality) Given a dataﬂow graph G = (V, E) and
a partitioning P = {P1, ..., Pn} of G, we deﬁne the quality Q(P) =
1
n ×
n
i=1(con(Pi)) of P as the average connectivity over all the partitions Pi, 1 ≤
i ≤n .
Most of the target architectures for which temporal partitioning is used are
made upon a reconﬁgurable device connected to a host processor by a bus such
as, for instance, the PCI.6 The number of bus lines is limited. Therefore, the
communication has to be time-multiplexed on the bus, if many data have to
be transported on the bus. This happens for example when a partition has to
be replaced. Recall that the communication between the partitions is done by
a set of registers inside the reconﬁgurable devices. All the temporary data in
those registers have to be saved in the communication memory before recon-
ﬁguration. The device is then reconﬁgured, and the data are copied back into
the reconﬁgurable device registers. Minimizing the communication overhead
can be done by minimizing the weighted sum of crossing edges among the
partitions. This will also minimize the set of registers needed to communi-
cate between the generated partitions, thus reducing on one side the size of the
communication memory and the communication time on the other side. This
goal is likely to be reached if highly connected components are placed in the
same partition.
After a partitioning by a given algorithm, the quality of the partition will de-
termine whether the algorithm performed well. If a graph is highly connected
and the partitioning algorithm performs with low quality, then there will be
more edges connecting different partitions and therefore more data exchange
among the partitions. But if the graph is highly connected and the partitioning
algorithm performs with high quality, then the components in the partitions
are highly connected, and therefore, there will be fewer edges connecting the
partitions.
6Peripheral Component Interconnect.

High-Level Synthesis for Reconﬁgurable Devices
119
1
3
4
6
7
5
10
9
2
8
8
4
5
3
6
9
2
1
10
7
G:
P0
P1
Figure 4.12.
Partitioning of the graph G with connectivity 0.24 with an algorithm that pro-
duces a quality of 0.25
Example 4.19 Figures 4.12 and 4.13 illustrate the connectivity of a graph
and the quality of the algorithm that was used for the partitioning. In ﬁgure
4.12, a graph with a connectivity of 0.24 is partitioned by a ﬁrst algorithm,
which produces a quality of 0.25. The same graph is partitioned by another
algorithm in ﬁgure 4.13 with a quality of 0.45. In the ﬁrst case, we have six
edges connecting the two partitions, while there are only two edges connecting
the partitions in the second case. The second case is, therefore, better than the
ﬁrst case for data communication between the partitions.
In the next section, we present contributions of authors who developed var-
ious techniques and methodologies for temporal partitioning. The methods
can be grouped into four different categories. In the ﬁrst category are the list-
scheduling based methods. The second category encounters exact methods,
7
10
9
8
1
3
4
6
5
2
G:
1
3
6
2
4
5
7
10
9
8
P0
P1
Figure 4.13.
Partitioning of the graph G with connectivity 0.24 with an algorithm that pro-
duces a quality of 0.45

120
Reconﬁgurable Computing
which use integer linear programming equations for capturing the optimization
problem.
The solution of the equations provides the exact solution of the
temporal partitioning problem. The third category covers network ﬂow-based
algorithms, whereas, in the fourth category, we have spectral method based on
the computation of eigenvalues of a matrix derived from the dataﬂow graph.
The methods in the ﬁrst, third and fourth categories can also be grouped un-
der the umbrella of recursive bipartition, because the partitions are iteratively
generated through the bipartition of a remaining set of components.
2.
Temporal Partitioning Algorithms
Several heuristics such as the list scheduling algorithm use the mobility
range of components to perform further optimizations. This range can be deter-
mined on the basis of the earliest and the latest starting time of the correspond-
ing node resulting from the solution of the unconstrained scheduling using
the ASAP and ALAP methods. We therefore ﬁrst present the unconstrained
scheduling before focussing on the temporal partitioning methods.
2.1
Unconstrained Scheduling
In general high-level synthesis, unconstrained scheduling [156] is the allo-
cation of starting time to components under the assumption that an unlimited
amount of resources is available. In the case of reconﬁgurable devices, this
corresponds to placing modules in a device with unlimited size that allows any
partition to be implemented. Because the devices in reality have only a lim-
ited amount of resources, unconstrained scheduling cannot be used as such.
Instead, it is usually used as pre-processing step for other algorithms. It can
be used for instance for the computation of the upper and lower bounds on the
starting time of operations in a dataﬂow graph. Whereas the lower bound pro-
vides the earliest time at which a module can be scheduled, the upper bound
deﬁnes the latest time at which a module can be started. The difference be-
tween the upper and the lower bound of a module is its mobility range. The
mobility interval is given by [ASAP(v) ALAP(v)], where ASAP(v) resp.
ALAP(v) is the starting time of v computed with the ASAP resp. ALAP-
algorithm, that we present later in this section. The starting time of a node
cannot be out of its mobility range.
To compute the earliest starting time of each component, the as soon as
possible (ASAP)-Algorithm can be used. The ASAP-algorithm traverses the
dataﬂow in topological order starting from the primary input. Each primary
input is assigned the starting time 0. A node vi is processed only when all
its predecessors have been processed. The starting time of the node vi is the
highest end time among all its predecessors. The ASAP algorithm idealizes
the binding process by assuming an unlimited amount of available resource,

High-Level Synthesis for Reconﬁgurable Devices
121
and assigns each operation as soon as it is ready to be executed, therefore
providing the lowest starting time of the tasks in the graphs. The pseudocode
for the ASAP-algorithm is provided in algorithm 7 for the resulting schedule
ς.
Algorithm 7 ASAP-Algorithm for unconstrained scheduling
1: for each node v ∈V do
2:
if v has no predecessors then
3:
ς(v) := 0
4:
V := V −v
5:
end if
6: end for
7: while V ̸= ∅do
8:
select a vertex vi ∈V whose predecessors are all scheduled
9:
schedule vi by setting ς(vi) := max(vj,vi)∈E(ς(vj) + tj)
10:
V := V −vi
11: end while
Example 4.20 Assuming a latency of 100 clocks for the multiplication and
50 clocks for the addition as well as for the subtraction, an example of ASAP-
scheduling is provided in ﬁgure 4.14. The nodes are labelled with their number
as well as their starting time as computed by the algorithm. The label on
the edges represent the delay computation delay of the previous node. The
data transmission delay is neglected. The number on the nodes represent their
starting times.
Contrary to the ASAP-algorithm, the as late as possible (ALAP)-algorithm
traverses the graph from the primary outputs to the primary inputs. A node is
processed only if all its successors have been processed. The primary outputs
are assigned the maximum starting time, which is an upper bound on the com-
putation time of the dataﬂow graph. In the simple case, this upper bound is the
length (latest starting time −earliest starting time) of the schedule computed
by the ASAP-algorithm. The starting time of the node vi is assigned the min-
imum value among all the staring times of its predecessor minus the latency
of vi. Algorithm 8 provides the pseudocode of the ALAP-algorithm for the
resulting schedule ς and an upper bound λ on the overall computation time.
Example 4.21 With the previous assumption on the module latencies, we
schedule the example of ﬁgure 4.14 using the ALAP-algorithm with upper
bound, thus providing the result of ﬁgure 4.15.

122
Reconﬁgurable Computing
Figure 4.14.
Scheduling example with the ASAP-algorithm
Figure 4.15.
ALAP Scheduling example

High-Level Synthesis for Reconﬁgurable Devices
123
Algorithm 8 ALAP-Algorithm for unconstrained scheduling
1: for each node v ∈V do
2:
if v has no successors then
3:
ς(v) := λ
4:
V := V −v
5:
end if
6: end for
7: while V ̸= ∅do
8:
select a vertex vi ∈V whose successors are all scheduled
9:
schedule vi by setting ς(vi) := min(vi,vj)∈E(ς(vj) −ti)
10:
V := V −v
11: end while
The constrained scheduling problem considers the restriction on the archi-
tecture. If there are many tasks competing for a given resource at a given
time, one of them must be chosen according to a given criteria and the rest
will be scheduled later. Sometimes, a task that could be executed at a given
time cannot be run because the resource type needed to implement that task is
not available at that time. This may delay the execution of the complete set of
remaining tasks, if the rest of the tasks depends on that task.
2.2
The List Scheduling Approach
List Scheduling.
One of the most used approaches used to solve the tem-
poral partitioning problem is the list scheduling (LS) method. List scheduling
was ﬁrst used as a method for microcode compaction to generate efﬁcient mi-
crocode from high-level languages [65]. The idea behind list scheduling in
architectural synthesis is to ﬁrst sort the nodes in topological order and then
assign some priority to the node of a dataﬂow graph. There are several meth-
ods to assign a priority to a node. A common strategy is to use the latency
weighted depth of the node as its priority [96][143]. The depth of a node v
is deﬁned as the length (number of nodes) of the longest path from an input
to v. The latency weighted depth is the same as the latency depth; however,
the path from the input to v is weighted using the latency of the operation to
be executed by the nodes on the path. Also, the mobility of a given node, i.e.
the difference between its ALAP-value and its ASAP-value, can be used as its
priority.
At any time step t, the so-called ready set, that is the set of operations ready
to be scheduled, is constructed. The ready set contains operations whose pre-
decessors have already been scheduled early enough to complete their execu-
tion at time t. The algorithm checks whether there are enough resources of a
given type k to implement all the operations of type k. If so, the operations are

124
Reconﬁgurable Computing
assigned the resources, otherwise, higher priority tasks are assigned the avail-
able resources and the rest of the operations will be scheduled later, when some
resources will be available. If the mobility of a node is used as priority criteria,
it is possible that all operators in the ready list are on a critical paths, which
means that their mobility is zero. As a consequence, the complete depth of
each operators is increased by one, thus increasing the latency of the graph’s
execution.
Example 4.22 We consider the graph of ﬁgure 4.16 on the left side, in which
each node is labelled with its priority. The nodes must be scheduled on a
resource set made upon an adder and a multiplier. With the priority of a node
deﬁned as its depth, the list scheduling is shown on the right side.
Forced-Directed List Scheduling.
The force-directed
list scheduling
(FDLS) from Paulin and Knight [174] is an extension of the list-scheduling
algorithm. Instead of computing the priority for each node at the beginning,
priorities are dynamically computed at each step. Force-directed list schedul-
ing is a resource-constrained scheduling method aimed at ﬁnding a minimum
latency schedule for a given resource set. The concept of force is used to select
Figure 4.16.
An example of list scheduling using the depth of a node as priority

High-Level Synthesis for Reconﬁgurable Devices
125
the operation to be assigned a given resource. First, the possible time frame,
i.e. the mobility interval of each node is computed using the ALAP and ASAP
scheduling. Next, the probability p(v, t) of a node v to be scheduled at step t is
computed for each node v of the graph. The probability p(v, t) is zero outside
the mobility interval of v and is equal the reciprocal of the mobility within the
mobility interval. Formally, we have
f(n) =

1
mob(v)+1
if t ∈[ASAP(v) ALAP(v)]
0
otherwise
(2.1)
where mob(v) = ALAP(v) −ASAP(v).
A distribution function d(t, k) is calculated as the sum of probabilities of all
the operations with a resource type k. Formally, we have
d(t, k) =

{v∈V,α(v)=k}
p(v, t)
(2.2)
This can be plotted into a graph called the distribution graph that indicates
the concurrency of similar operations over the schedule steps. Whenever many
operators compete for a fewer amount of resources, the operations that produce
a global increase of concurrency in the graph are selected and assigned the
resources.
Example 4.23 Consider once again the graph of ﬁgure 4.6 whose ASAP
and ALAP time is computed in ﬁgures 4.14 and 4.15. Nodes v1, v2, v5 and
v8 have each a mobility of zero. Therefore, p(v1, i) = p(v2, i) = p(v5, i) =
p(v8, i) = 0, ∀i ∈[0 , 200].
As stated earlier, the scheduling is done on the basis of the force concept,
whose role is to attract or repel operators in that scheduling step. Two types
of forces are deﬁned: the self-force and the predecessor–successor force. The
self-force is the one that relates an operation to different control steps in which
it can be scheduled. For a given node v, the self-force is formally deﬁned as
follows:
self-force(v, t) = d(t, k) −
1
mob(v) + 1
ALAP (v)

m=ASAP (v)
d(m, k)
(2.3)
The selection of an operator inﬂuences the mobility interval of other opera-
tors. This inﬂuence can be expressed as a force, called predecessor–successor
force that we will not present in detail here. The predecessor–successor force
is added to the self-force to build the total forces that are used for the selection
process. More details on the computation of forces can be found in [174].

126
Reconﬁgurable Computing
List Scheduling for Reconﬁgurable Devices.
In high-level synthesis for re-
conﬁgurable devices, the resource type does not play a big role. Only the total
amount of basic resources is important. We do not have operators competing
for a resource type but for a given surface on the device. Furthermore, we
are interested in building complete partitions describing a set of computational
steps that will be performed at the same time. Despite the fact that any compo-
nent has a different starting time and a different end time, only the starting time
and the end time of the complete partition is usually considered. Several au-
thors [173, 41, 171, 45, 202, 181] have used this fact to adapt known architec-
tural
synthesis
algorithm
to
reconﬁgurable
devices.
The list-scheduling algorithm in reconﬁgurable devices works in the same
way as the common list-scheduling algorithm, by building and updating a list
of ready operators. Components are then removed from the ready list and
assigned to partitions. The only assignment criterion is that there should be
enough places left on the device to accommodate the new component. If this
is the case, then the component is placed on the partition currently being built.
Otherwise, a new partition is built and the process is repeated until all the
nodes of the graph are placed in partitions. The pseudocode for this approach
is provided in algorithm 9.
Algorithm 9 List-scheduling algorithm for reconﬁgurable devices
sort the nodes of v according to their priorities
P0 := ∅
while V ̸= ∅do
select a vertex v ∈V with highest priority and whose predecessors are all
placed
if (a partition Pi exists with s(Pi) + s(v) ≤s(H)) then
Pi = Pi ∪{v}
else
create a new partition Pi+1 and set Pi+1 = {v}
end if
end while
Because the method is an iterative heuristic, several improvements can be
done during the construction to generate optimal solutions.
Optimization usually considers the minimization of the overall computation
latency of the dataﬂow graph. The two main factors that inﬂuence the overall
latency are the latency of each segment and the conﬁguration overhead. If k

High-Level Synthesis for Reconﬁgurable Devices
127
segments are generated by an algorithm, then the overall computation time can
be formally written as shown in equation 2.4.
tDF G = k × CH +
n

i=1
(tPi)
(2.4)
where tDF G is the overall computation latency for a dataﬂow graph DFG, CH
is the reconﬁguration time of the device H and tPi is the computation time of
partition i.
The minimization of the overall computation time of the given function can
be done by tuning with the different parameters in equation 2.4. If the re-
conﬁguration time of the device is too big, then the optimization will tend to
minimize the number of partitions to avoid a lot for reconﬁgurations. However,
if the reconﬁguration time can be neglected, then only the delay in partitions
will be of great interest and a good algorithm will tend to avoid long paths in
partitions.
The main advantage of the list-scheduling method is its linear run-time in
the number of nodes in the graph. Furthermore the method allows for local
optimizations while selecting the nodes to be placed in partitions. This fact
is exploited in [173] and [30] to compute a minimum set of resources used
by consecutive partitions. In [202], a pair wise interchange is done between
adjacent segments to minimize a cost function deﬁned as the sum of squares of
the number of nets in each segments.
List scheduling has a major drawback called the levelization. Levelization
means that the partitions are built on the basis of the level number of the com-
ponents in the dataﬂow graph. Modules are assigned to partitions based more
on their level number rather than their interconnectivity with other components
already placed in the partition. This may have a bad effect on the quality of
the partitioning because of a bad connectivity inside the partitions and between
the partitions. With this, the goal of minimizing the number of nets connect-
ing two different partitions, i.e. the minimization of the data exchange among
partitions becomes difﬁcult to reach.
Example 4.24 Consider the circuit of ﬁgure 4.17 partitioned by a list-
scheduling algorithm that produces three partitions. Although the components
are less connected inside the partitions, the partitions have more edges among
each other, thus leading to a poor quality. A better partitioning is shown in
ﬁgure 4.18 for the same graph. The connectivity is better preserved inside and
outside the partitions.
Despite its drawback in connectivity, the list-scheduling algorithm remains,
thanks to its linear computation time, a good temporal partitioning candidate,

128
Reconﬁgurable Computing
Figure 4.17.
Levelizing effect on the list-scheduling on a dataﬂow graph
Figure 4.18.
Partitioning with a better quality than the list-scheduling

High-Level Synthesis for Reconﬁgurable Devices
129
in particular for low-connected graphs. Also, it can be used to construct ini-
tial solutions that can be improved by more complex iterative improvement
optimization heuristics.
2.3
Integer Linear Programming
As deﬁned in [49], an instance of an integer linear programming (ILP) prob-
lem is deﬁned through the equation 2.5.
min cT x
(2.5)
Ax = b
x ≥0
with A, b and c and x being matrices of integers. It consists of ﬁnding a vari-
able x, which minimizes cT x under the side constraints Ax = b and x ≥0.
The ILP method provides an exact approach for solving the general architec-
tural synthesis problem and therefore can be used for the temporal partitioning
problem as well.
The approach consists of formulating the temporal partitioning problem as
an ILP problem instance and then to use well-known tools to compute the
solution to the equation. All problem-related constraints are deﬁned as a set
of equations involving only integer numbers. The equations must be solved
subject to a given goal to be minimized.
The most important constraints that need to be captured in an ILP problem
instance are the unique assignment constraint, the precedence constraint and
the resource constraint. The integer requirement in the equation vector entries
is due to the fact that we deal with indivisible variables and therefore no real
number must be involved.
Before we present the formal description of the constraints, we provide the
deﬁnitions of the variables that will be used in the equations.
Definition 4.25 (0-1 variable) Given a dataﬂow graph G = (V, E)
and a partitioning P = P1, ..., Pn of G sorted in order of precedence. We
deﬁned 0-1 variable y and w as follow:
yvi = 1 ⇔v ∈Pi
wuv = 1 ⇔((u, v) ∈E) ∧(u ∈Pi) ∧(v ∈Pj) ∧(Pi ̸= Pi)
The role of the y variables is to deﬁne the memberships of a given node
to a partition. A value of one means that the node is in the corresponding
partition, whereas a value of zero means that the node is not in the partition. In
the same way, a 0-1 variable for an edge determines if that edge connects two
different partitions. The 0-1 variables provide the basis for deﬁning the set of
ILP equations that we next formulate.

130
Reconﬁgurable Computing
1 Unique assignment constraint: This constraint is used to control the as-
signment of a node to a single partition. Each node must be assigned exactly
to one partition. The unique assignment constraint is formally deﬁned by
equation 2.6.
∀v ∈V,
n

i=1
yvi = 1
(2.6)
The sum of the assignment index of a given variable over all partitions is
exactly one, which means that there is exactly one value i for which the y
variable is one. This is the index i of the partition in which v is placed.
2 Precedence constraint: This constraint is used to control the assignment
of data-dependent nodes of the dataﬂow graph to partitions. A node v that
is data dependent from a node u must be placed in a partition with index
bigger or equal to that of the partition into which the node u is placed. This
constraint is captured by the following equation:
∀(u, v) ∈E,
n

i=1
i × yui ≤
n

i=1
i × yvi
(2.7)
The two sums deﬁne the index of the partition in which the components are
placed.
3 Resource constraint: The resource constraint deﬁnes the constraint on the
architecture used. This can be limited to the reconﬁgurable device or to a
complete system into which the reconﬁgurable device is integrated. For a
reconﬁgurable device, the area constraint as well as the constraint on the
number of terminals must be speciﬁed.
The area constraint states that the total amount of resources assigned to a
given partition must not exceed the amount of available resource on the
chip.
Recall that the computation resources in a reconﬁgurable device
are deﬁned in terms of area occupied on the device. The constraint can
therefore be expressed in terms of device area: the total area assigned to a
given partition must not exceed the device area. This is formally deﬁned by
equation 2.8
∀Pi ∈P,

v∈Pi
a(v) ≤a(H)
(2.8)
The terminal constraint deﬁned in equation 2.9 states that the total number
of input and output in a partition must not exceed the total number of pins
on the device.
∀Pi ∈P,

(u∈Pi)∧(v/∈Pi)
wuv +

(u/∈Pi)∧(v∈Pi)
wuv ≤p(H)
(2.9)
where p(H) is the number of terminals (pins) of the device H.

High-Level Synthesis for Reconﬁgurable Devices
131
4 communication memory constraint: The total amount of data to be tem-
porally saved must not exceed the size of the communication memory used.
This constraint is captured by the following equation:

(u,v)∈E and (u∈Pi)∧(v∈Pj(j>i))
wuv ≤Ms
(2.10)
where wuv is the width of the edge (u, v) and Ms is the size of the commu-
nication memory.
Having formulated the constraint as ILP equations, commercial solvers can
be invoked to compute the solutions of the equations. ILP in temporal parti-
tioning was investigated in [76] [135, 136], [93] and [141].
Example 4.26 Considering the partitioning of ﬁgure 4.18, let us check if
the ILP-constraints hold.
Unique assignment constraint:
Partition P1: y22 = y23 = 0, y21 = 1 and y32 = y33 = 0, y31 = 1 and
y42 = y43 = 0, y41 = 1
Partition P2: y11 = y13 = 0, y12 = 1 and y51 = y53 = 0, y52 = 1 and
y61 = y63 = 0, y62 = 1
Partition P3: y71 = y72 = 0 and y73 = 1
Precedence constraint: 1 = n
i=1 y4i ≤n
i=1 y5i = 2 and 2 = n
i=1 y6i ≤
n
i=1 y7i = 3
Resource constraint assuming a device with a size of 200 LUTs, and 100
LUTs for the multiplication, 50 LUTs each for the addition, the comparison
and the multiplexer, we have:
Partition P1: 7
u=1 yu1 = (100 + 50 + 50) ≤a(H) = 200
Partition P2: 7
u=1 yu2 = (100 + 50 + 50) ≤a(H) = 200
Partition P3: 7
u=1 yu3 = (100) ≤a(H) = 200
Communication memory constraint: let us assume that a memory with 50
bytes is available for communication and each datum has a 32-bit width.
We have communication between partition P1 and P2 and between P2 and
P3.
For P1 to P2: w45 = 32Bytes ≤Ms = 8 × 50
For P2 to P3: w67 = 32Bytes ≤Ms = 8 × 50
The general problem of the ILP approaches for partitioning a DFG is the
computation time, which grows drastically with the size of the problem instance.

132
Reconﬁgurable Computing
The algorithm can be applied only to small examples. Branch and bound strate-
gies are sometimes used to limit the search space. In this case, the bounds can
be computed using the ASAP/ALAP approach previously described. To over-
come this problem, some authors reduce the size of the model by reducing
the set of constraints in the problem formulation, but the number of variables
and precedence constraints to be considered still remains high. Reconﬁgurable
devices are no longer those tiny devices that could not hold more than four
multipliers. Their sizes have increased very fast in the past, and this will con-
tinue in the future. Temporal partitioning algorithms should therefore be able
to partition very large graphs (graphs with thousands of nodes). Trying to for-
mulate all the precedence constraints with the ILP approach can drastically
increase the size of the model, thus making the algorithm intractable.
2.4
Network Flow
The next method that we consider is the network ﬂow approach for solving
temporal partitioning. The network ﬂow methodology is based on the Ford
and Fulkerson min-cut max-ﬂow theorem [84]. This method has been used
in circuit partitioning [231] [123] and also in LUT-technology mapping by
the FlowMap method presented in section 3.3.2. The use of the network ﬂow
approach to solve temporal partitioning problems was ﬁrst proposed in [149],
[148].
The method is a recursive bipartition approach that successively partitions a
set of remaining nodes in two sets, one of which is a ﬁnal partition, whereas a
further partition step must be applied on the second one.
The goal to be reached during partitioning is the minimization of the com-
munication overhead among the partitions, which also means the minimization
of the communication memory. The goal is formulated as the minimization of
the overall cut-size among the partitions. A little cut-size among the partitions
means fewer edges connecting the partitions, less communication and there-
fore a good partitioning quality.
Formally, we deﬁne ˜Pi as the set of nodes to be partitioned at step i. We
call ˜Pi the restgraph of the original dataﬂow graph. ˜P0 = G is the complete
graph G. The partitions P0, P1, ..., Pk are successively generated using recur-
sive bisection of the sets ˜Pi until the set ˜Pk fulﬁlls the device size and terminal
limitations, i.e. the a( ˜Pk) ≤a(H) and p( ˜Pk) ≤p(H).
We consider partitions that generate cycle-free conﬁguration graphs. Con-
ﬁguration graph with cycles can be treated in the same way; however, the im-
plementation of the feedback loops is left to the processor that controls the
reconﬁguration and the data streaming on the datapaths. With this, the bi-
partition of the set ˜Pi produces two new sets Pi and ˜Pi+1 with the following
characteristics:

High-Level Synthesis for Reconﬁgurable Devices
133
1 s(Pi) ≤s(H) and p(Pi) ≤p(H), i.e. Pi must not be further partitioned.
2 With the ordering of the relation of deﬁnition 4.13, only one of the follow-
ing condition should hold:
(Pi ≤˜Pi+1)
( ˜Pi+1 ≤Pi)
Pi and ˜Pi+1 are not in relation.
Those conditions ensure that no cycles exist between Pi and ˜Pi+1. The net-
work ﬂow method is used in each step of the recursive bipartitioning process
to compute a cycle-free bipartition. At each step, the following processing
operations are applied to the rest graph ˜Pi:
1 ˜Pi is ﬁrst transformed into a network graph ˜P ′
i by introducing two new
nodes into the graph ˜Pi. The ﬁrst one is an input-free source node, whose
outputs are connected to all the primary inputs. In the same way, a sink node
is inserted in the dataﬂow graph and all the primary outputs are connected
as input to it.
2 A second transformation is done on the resulting network ˜P ′
i to generate
the graph ˜P 2
i .
For an edge (v1, v2) ∈˜Pi
′ × ˜Pi
′ two edges e1 = (v1′, v2′), with a
capacity of c1 = 1 and e2 = (v2′, v1′) with a capacity of c2 = ∞, are
added to ˜P 2
i .
For a multi-terminal edge in ˜Pi
′ × ˜Pi
′, a bridging node is added to ˜Pi
2.
An edge weighted with 1 connects the source node with the bridging
node in ˜Pi
2 × ˜Pi
2. For each sink node in the multi-terminal net, an
edge weighted with ∞is added between the bridging edge and the sink
nodes and between the sink nodes and the source node.
This transformation is similar to the one done by the FlowMap algorithm
in section 3.3.2 before applying the min-cut maxﬂow theorem. Figure 4.19
illustrates the transformation previously described.
Having transformed the rest graph into a network graph, the maximum ﬂow
on the resulting network is computed, leading to a min-cut (X2, X2) of the
network ˜P 2
i , all the forward edges (from X2 to X2) must be saturated (ﬂow
equal to the capacity) and all backward edges (from X2 to X2) should have
no ﬂow. If a net is cut, then only bridging edges can connect X2 to X2, thus

134
Reconﬁgurable Computing
Figure 4.19.
Dataﬂow graph transformation into a network
preserving the precedence constraints. The computation of the maximal ﬂow
is done using the augmenting path method described in [186].
For a computed min-cut (X2, X2) in ˜P 2
i , the corresponding cut (X, X) is
induced in ˜Pi by inserting the equivalent nodes from (X2, X2) in (X, X). The
four steps for transformation and partitioning using the network ﬂow approach
are illustrated on ﬁgure 4.20.
The min-cut max-ﬂow theorem of Ford and Fulkerson is a powerful tool to
minimize the communication in a cut in polynomial time. However, the model
is constructed by inserting a great amount of nodes and edges in the original
graph. The resulting graph ˜P 2
i may grow too big. In the worst case, the number
of nodes in the new graph can be twice the number of the nodes in the original
graph. The number of additional edges also grows dramatically and become
difﬁcult to handle.
2.5
Spectral Methods
In the case where the goal in temporal partitioning is the minimization of
communication between partitions, the spectral approach [30][29][28] can be
used for temporal partitioning.
The connectivity of a graph can be minimized by placing components in
an n-dimensional space in such a way that the sum of the distance between
component pairs is minimized. This approach is called the wire length model.
As the sum of the distances between component pairs can be minimized if
connected components are placed close to each other, the wire length model is
likely to provide an optimal placement of the components in an n-dimensional
space. Using the wire length model to solve the temporal partitioning with the
minimization of communication among the partitions can be done by solving
the following two sub-problems:

High-Level Synthesis for Reconﬁgurable Devices
135
Figure 4.20.
Transformation and partitioning steps using the network ﬂow approach
1 Placement of the components in an n-dimensional vector space such as to
minimize the sum of the distances between the components. This procedure
is known as an n-dimensional spectral embedding [5] [7] [6].
2 Derivation of a partition from an optimal placement that minimizes the sum
of the distance between the components.

136
Reconﬁgurable Computing
0.8
0.6
0.4
0.2
0.0
−0.2
−0.4
−0.8 −0.6
1
4
2
3
b)
1
2
3
4
1
3
4
2
a)
Node
c)
Bounding box
Node
d)
0.6
0.4
0.2
0.0
−0.2
−0.4
−0.6
0.8
0.6
0.4
0.2
0.0
−0.2
−0.4
−0.8 −0.6
1
−1
1
2
4
3
Figure 4.21.
1-D and 2-D spectral-based placement of a graph
We start with the ﬁrst problem by considering the one-dimensional7 version
as deﬁned by Hall in [107] and illustrated in ﬁgure 4.21 [195]. (The subﬁg-
ure (a) shows an example of a directed graph with four nodes and three edges
borrowed from [195], whereas subﬁgure (b) shows the one-dimensional place-
ment on a line. The small black squares represent the centres of the logic cells.
Subﬁgure (c) shows the two-dimensional placement of the same nodes. The
method does not take account of sizes of the logic cells or actual location of
logic cell connectors. The scaling of the computed position (by the width and
height of the node bounding box). In subﬁgure (d), a complete layout is made
by placing the logic cells on valid locations.
The spectral embedding problem can be stated as follows: Given a dataﬂow
graph G = (V, E), ﬁnd locations for the |V | nodes that minimize the weighted
sum of squared distances between the nodes. If xi denotes the X-coordinates
of node vi ∈V and r denotes the weighted sum of squared distances between
the nodes, then the one-dimensional problem is to ﬁnd the row vector XT =
(x1, x2, ..., xn), which minimizes
r =
n

i=1
n

j=1
(xi −xj)2wij
(2.11)
7Placement on a line.

High-Level Synthesis for Reconﬁgurable Devices
137
To avoid the trivial case in which xi = 0 for all i, we impose the following
condition (normalization):
XT X = 1
(2.12)
We assume that the non-interesting solution xi = xj (for all i, j ∈{1, .., n})
is to be avoided. This leads to all components placed at the same location.
Next, we deﬁne the connection matrix, the degree matrix and the Laplacian or
disconnection matrix of G as follows:
Definition 4.27 (Connection Matrix, Degree Matrix,
Laplacian Matrix) Given a DFG G = (V, E), we deﬁne:
The connection matrix of G as the symmetric matrix C = (ci,j) with (1 ≤
i, j ≤| V |) and ci,j = 1 if (vi, vj) ∈E and ci,j = 0 otherwise.
The degree matrix of G as the diagonal matrix D = (di,j), (1 ≤i, j ≤| V |)
with i ̸= j →di,j = 0 and di,i = |V |
j=1 ci,j.
The Laplacian matrix of G as the matrix B = D −C.
For two nodes vi and vj of the DFG connected by an edge, the connection
matrix will have an entry one in line i and column j. The degree matrix is
a diagonal matrix. An entry in the diagonal (line i, column i) corresponds to
the number of nodes adjacent to vi. The Laplacian matrix is the difference
between the degree and the connection. Hall has proved in [107] that:
r = XT BX
(2.13)
As B is positive semi-deﬁnite (B ≥0) and B is of rank |V | −1, whenever G
is connected [107], the initial problem is now reduced to the following:

minimize r = XT BX with B ≥0
subject to XT X = 1
(2.14)
This is a standard constraint optimization problem that can be solved using the
method of the Lagrange multipliers, which is a standard method used to ﬁnd the
extrema of a function f(x1, ..., xn) subject to a constraint g(x1, ..., xn) = 0.
An extrema exists if equations (2.15) and (2.16) are satisﬁed.
df = ∂f
∂x1
dx1 + · · · + ∂f
∂xn
dxn = 0
(2.15)
dg = ∂g
∂x1
dx1 + · · · + ∂g
∂xn
dxn = 0
(2.16)

138
Reconﬁgurable Computing
Multiplying (2.16) by a parameter λ to be determined and subtracting the result
from (2.15), we obtain equation (2.17):
( ∂f
∂x1
−λ ∂g
∂x1
)dx1 + · · · + ( ∂f
∂xn
−λ ∂g
∂xn
)dxn = 0
(2.17)
Because the differentials are all independent, we can set any combination equal
to zero and the remainder must still be zero. Therefore,
( ∂f
∂xk
−λ ∂g
∂xk
)dxk = 0 , ∀k ∈1, ..., n
(2.18)
The constant λ to be computed is called the Lagrange multiplier.
To solve problem 2.14, we apply the method of the Lagrange multipliers
with f = XT BX and g = XT X −1. We introduce the Lagrange multiplier λ
and form the Lagrangian L = XT BX −λ(XT X −1). Taking the ﬁrst partial
derivative of L with respect to X and setting the result equal to zero yields
equation 2.19:
2BX −2λX = 0 ⇐⇒(B −λI)X = 0
(2.19)
Multiplying equation (2.19) by XT and applying the constraint (2.12), equa-
tion (2.19) yields a non-trivial solution if and only if X is the eigenvector
of B which minimizes r and λ(= r) is the corresponding eigenvalue. Be-
cause the minimum eigenvalue λ0 = 0 yields the non-interesting solution
XT = (1, 1, ..., 1)/

(n), the second smallest eigenvalue λ1 should be cho-
sen. The eigenvector X1 related to the eigenvalue λ1 is the solution to the
one-dimensional problem (2.14).
For a placement in a k-dimensional vector space, the problem is formulated
similar to the one-dimensional version. The entire dimensions involved have
to be considered. The following equation must therefore hold.

minimize R = X1T BX1 + X2T BX2 + ... + XkT BXk
subject to X1T X1 = X2T X2 = ... = XkT Xk = 1
(2.20)
(Xi deﬁnes the coordinates of the nodes of V in the i-th dimension) has
to be solved. Analog to the one-dimensional case, the Lagrange multiplier
method will be applied with the k (each for one dimension) Lagrange multipli-
ers λ1, λ2, ...., λk. The solution is the eigenvectors associated to the k smallest
non-zero eigenvalues λ1, λ2, ...., λk. This approach is known in the literature
as spectral method. Spectral methods have been widely used in the past for
partitioning and placement [6] [7] [118] [44] [74] [73]. Its run-time is domi-
nated by the computation of the eigenvalues, which can be done using various
methods on different architectures. The most used algorithm for computing

High-Level Synthesis for Reconﬁgurable Devices
139
the eigenvalues of a matrix is the Golub Kahan method [99] that needs O(n3)
on a single processor for an n × n matrix. Using O(n) processors, the eigen-
values can be computed with a parallel version of the Hestenes method [200]
[172] [119] [38] in O(n2S) where S is the number of so-called sweeps [119]
[38]. Brent and Luk [38] conjectured that S = log(n) and therefore S ≤10 in
general. For sparse and quadratic matrices, the eigenvalues can be computed
in O(n1.4) using the more efﬁcient Lanczos method [99, 6].
2.6
Application to the Temporal Partitioning Problem
So far we have shown that the spectral method can be used to solve the ﬁrst
part of our problem. The second part, the generation of partitions from a place-
ment with minimum wire length, has to be considered yet. One approach is to
consider the placement of the components in a three-dimensional vector space
in which the X axis and Y axis represent the device surface and the Z axis rep-
resents the time at which each component should be mapped inside the device.
In the time dimension, the precedence constraints, ∀e = (vi, vj) ∈E, zi ≤zj,
between the components of the dataﬂow graph must hold. A possible solu-
tion will consist of ﬁrst placing the component in the space without taking the
precedence constraints into consideration. Thereafter, the partition can be built
using an iterative approach in which the constraint can be checked.
Assuming that three-dimensional spectral placement of the component is
computed, the partitions P0, P1, ... and Pk can incrementally be generated us-
ing a recursive bisection of the sets ˜Pi, which is the set of the modules that
still needs to be placed in partitions. Initially we set ˜P0 = V . At step i, the
partition ˜Pi is divided into two partitions Pi and ˜Pi+1. This process is repeated
until ˜Pi+1 = ∅.
The partition Pi is built at step i by picking components along the Z axis and
placing them in Pi until the size of Pi reaches a given limit (the device size).
This process creates a bipartition (Pi, ˜Pi−Pi = ˜Pi+1) of the set of components
not yet assigned to a partition. For each computed bipartition (Pi, ˜Pi+1), we
seek exactly one of the following situations: either (Pi ≤˜Pi+1), or ( ˜Pi+1 ≤
Pi), or there is no relation between Pi and ˜Pi+1. This means that either no edge
exists that connects a component of one partition with a component of another
partition or all the edges connecting components between the two partitions
have the same direction. This is not always true, as we have used an undirected
graph for the placement, and we did not consider the precedence constraint.
The bisection has to be improved to ﬁt the cycle-free constraint. We do this by
moving nodes from one side of the bisection to the other until a strict order is
reached between Pi and ˜Pi+1.

140
Reconﬁgurable Computing
Example 4.28 To illustrate our temporal partitioning using the spectral
method, we consider the graph of ﬁgure 4.22 whose laplacian matrix is given
in matrix 4.1.
Figure 4.22.
Dataﬂow graph of f = ((a + b) ∗c) −((e + f) + (g ∗h))
The three smallest eigenvalues related to them are λ6 = 0.112586, λ3 =
0.267949 and λ1 = 0.438447 and the related eigenvectors are (0.0410593,
−0.152761, 0.225048, −0.241072,
−0.241072,
0.355147),(0.0, 0.0,0.0,
0.325058, −0.325058, 0.0),
(0.34188, 0.0749482, 0.191984,−0.079481,
−0.079481, −0.191984).
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
0
0
0
0
0
0
−1
0
0
0
0
0
1
0
0
0
0
0
−1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
−1
0
0
0
0
0
1
0
0
0
0
0
0
−1
0
0
0
0
0
0
0
1
0
0
0
0
0
−1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
−1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
−1
0
0
0
0
0
0
0
0
3
−1
−1
0
0
0
0
0
0
0
0
0
0
0
−1
3
0
−1
−1
0
0
−1
0
0
0
0
0
0
−1
0
3
0
0
−1
0
0
0
0
−1
−1
0
0
0
−1
0
3
0
0
0
0
0
−1
0
0
−1
0
0
−1
0
0
3
0
0
0
0
0
0
0
0
−1
0
0
−1
0
0
3
−1
0
0
0
0
0
0
0
0
0
−1
0
0
−1
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
Table 4.1.
Laplacian matrix of the graph of ﬁgure 4.22

High-Level Synthesis for Reconﬁgurable Devices
141
Figure 4.23.
3-D spectral placement of the DFG of ﬁgure 4.22
In this example, the primary inputs and primary outputs were taken into
account in the building of the matrices. This is useful in that it allows to con-
sider the placement of components in the vicinity of the pins that they use.
However, the eigenvectors only deﬁne the position of the components in the
different dimensions.
Using those three eigenvectors, we computed the three-dimensional place-
ment of ﬁgure 4.23.
By picking the component along the Z axis in increasing order of their
Z-coordinates, we constructed the two partitions P0 and P1 of ﬁgure 4.24.
Figure 4.24.
Derived partitioning from the spectral placement of ﬁgure 4.23

142
Reconﬁgurable Computing
This partition is not ordered as the edges (3, 6) and (4, 6) have their source
in P0 and their destination in P1, whereas the edge (5, 3) has its source in P1
and its destination in P0. This produces a conﬁguration graph with a cycle. By
exchanging the nodes 3 and 5 from P0 to P1 and vice versa, the partitioning
becomes an ordered one.
2.7
Elimination of Cycles in the Conﬁguration Graph
Elimination of cycles in the conﬁguration graph is important in methods
that perform an iterative bisection of a rest graph. This is the case for the list-
scheduling, the network ﬂow method and the spectral method. The approach
that we present here is more general than just limited to one particular method.
The cycle elimination method is based on the well-known iterative improve-
ment procedure of Kernighan and Lin (KL) described in [137] [83] [144]. As
the KL-algorithm deals only with undirected graphs and because we target di-
rected graphs in this work, some modiﬁcations have to be done on the original
KL algorithm.
The fundamental idea behind the KL-algorithm is the deﬁnition of a cut
of a bisection as well as the notion of the gain of moving a vertex from one
side of the bisection to the other. For an undirected graph, a cut is deﬁned as
the weighted sum of all the edges crossing from one partition to another. By
moving a node from one partition into the other, the number of crossing edges
is also modiﬁed and the value of the cut changed. The KL-algorithm allows a
series of moves, which reduce the bisection cut. If the gain of moving a vertex
is positive, then making that move will reduce the total cost of the cut in the
partition. During one iteration of the KL-algorithm, nodes are moved from
one side of the bisection and locked on the other side. The cost of swapping
unlocked nodes in opposite parts is then computed, and the nodes with the
best gain (greatest decrease or less increase of the cut) are swapped. If all
the nodes are locked, the lowest cost partition is set to the current computed
partition, if it improves the cost of the cut. One iteration of the KL-algorithms
is called a pass. After one pass, all the nodes are unlocked and a new pass is
computed. The iteration terminates if a pass produces no further improvement
on the cut. For a more detailed description of the KL-methods and its extension
by Fiduccia and Mattheyses, refer to [137] [83] [144].
The KL-method works on undirected graphs and does not differentiate be-
tween the direction of edges. It does not matter if an edge crosses from the
ﬁrst partition to the second partition or vice versa. In temporal partitioning,
the graph is directed because of precedence constraints; thus, the original KL-
algorithm has to be modiﬁed to ﬁt our needs. To better explain how the modi-
ﬁcation can be done on the original KL-method, we ﬁrst provide more deﬁni-
tions.

High-Level Synthesis for Reconﬁgurable Devices
143
Definition 4.29 Let G = (V, E) be a dataﬂow graph. For two nodes vi
and vj in V and a bipartition {P, Q} of G, we have:
EP (vi) = 
{(vi,vj)|vj∈P } wij is the weighted sum of the edges from vi to
P, i.e. edges connecting vi to nodes in P (ﬁgure 4.25).
IP (vi) = 
{(vj,vi)|vj∈P } wji is the weighted sum of the edges from P to
vi, i.e. edges connecting nodes in P to vi (ﬁgure 4.25) and
EP,Q = {(vi, vj) ∈E | vi ∈P and vj ∈Q} is the set of edges crossing
from partition P to partition Q, i.e. edges having their sources in P and
their destination in Q.
At step i of the temporal partitioning, a bipartition (Pi, ˜Pi+1) is generated.
The precedence constraint (Pi ≤˜Pi+1 or ˜Pi+1 ≤Pi) can be captured through
the following equation: EPi, ˜Pi+1 = ∅or E ˜Pi+1,Pi = ∅. Intuitively, we would
like to combine the two variables EPi, ˜Pi+1 and E ˜Pi+1,Pi in the deﬁnition of
our cut and try to decrease one of them to zero using the KL-algorithm. But
decreasing one variable could have the negative effect of increasing the second
one, thus producing a cycle of improvement and alteration on the cost of the
cut. To avoid this, we apply two instances of the KL-algorithm on the same
Figure 4.25.
Internal and external edges of a given nodes

144
Reconﬁgurable Computing
bisection in parallel. The objective is to have EPi, ˜Pi+1 = ∅on the ﬁrst com-
putation path and E ˜Pi+1,Pi = ∅on the second one. Obviously, the cost of the
cut is set to |EPi, ˜Pi+1| on the ﬁrst path and |E ˜Pi+1,Pi| on the second path. After
one pass on each path, we check whether the objective has been reached on
one path. If this is the case, then the result is set to be the partition generated
on that path. Otherwise, a new pass is computed on the two computation paths.
The gain of moving a node is deﬁned differently on the two computation paths.
On the ﬁrst path where the goal is to have |EPi, ˜Pi+1| = 0, the gain of
moving a node j from Pi to ˜Pi+1 is IPi(vj) −E ˜Pi+1(vj) and the one of
moving a node k from ˜Pi+1 to Pi is E ˜Pi+1(vk) −IPi(vk).
On the second path where to goal is to have |E ˜Pi+1,Pi| = 0, the gain of
moving a node j from Pi to ˜Pi+1 is EPi(vj) −I ˜Pi+1(vj) and the gain of
moving a node k from ˜Pi+1 to Pi is I ˜Pi+1(vk) −EPi(vk).
The gain deﬁned on each computation path is the same as the one deﬁned in the
original KL-algorithm. The modiﬁed version of the KL-algorithm presented
here will produce the desired result on one path. Because the targeted graphs
are acyclic dataﬂow graphs, there exists a partition in which all the edges cross
from the ﬁrst one to the second. Such a partition is provided for example by a
list-scheduling algorithm.
2.8
Local optimization: Context Switching, Time
Multiplexing and Conﬁguration Switching
Reconﬁguration time has always been a major problem in reconﬁgurable
computing. Switching from one conﬁguration to the next can take up to a frac-
tion of a second in large FPGAs, according to the technical realization of the
system. With millions of iterations, the reconﬁguration overhead becomes too
high to allow a use of the device in real-life problem solving. To decrease re-
conﬁguration overhead, the architectural concepts of time-multiplex and con-
text switching FPGAs have been proposed [202] [188]. The general idea be-
hind those two concepts is to store a given number of conﬁgurations directly
on the chip. This avoids the downloading of a large amount of reconﬁguration
data when required.
Trimberger [202] proposed an FPGA architecture in which designs are mod-
elled as Mealy state machines. Microregisters are used to temporally hold
computation data when switching from one conﬁguration to the next. The
combinational logic receives its inputs from the device inputs and from the ﬂip
ﬂop outputs, and the device outputs come from combinational logic and from
ﬂip ﬂops outputs.In logic engine mode, the device emulates a large design in

High-Level Synthesis for Reconﬁgurable Devices
145
many microcycles. In each microcycle, resources are allocated to a new con-
ﬁguration. A similar architecture has been proposed by Scalera et al. [188].
Data pipes are used here to exchange data between different conﬁgurations also
called contexts. A data pipe contains a plurality of context switching logic ar-
rays (CSLA), which can be used to process two 16-bit words. An incoming
context can then pick its input data where its predecessor left off by acquir-
ing the intermediate data deposited on the rightmost portion of the pipe and
processing it in a pipeline from right to left. Unfortunately, the methods de-
veloped have remained in a conceptual stage. Neither time multiplexing nor
context-switching FPGAs have ever been commercialized.
A more practical approach has been proposed in [30] to reduce the recon-
ﬁguration time. It is based on the observation that traditional list-scheduling
based temporal partitioning algorithms can produce a series of conﬁgurations
based on the same set of operators if the components are ‘well ordered’8 in
the list. For example, for two consecutive conﬁgurations ζi = {C1, ..., Cki}
and ζi+1 = {C1′, ..., Cki+1
′} representing two partitions Pi and Pi+1, one can
be the subset of the other one, i.e either ζi ⊆ζi+1, or ζi+1 ⊆ζi. If one
of those two situations arises, then the reconﬁguration overhead can be re-
duced by implementing the two partitions Pi and Pi+1 in one conﬁguration
ζnew = ζi ∪ζi+1. The components of ζnew will then be shared between the
two partitions Pi and Pi+1. That means, the modules required for both conﬁg-
urations are placed on the device and wired in two different ways. Each way
corresponds to one conﬁguration. Figure 4.26 shows a partitioning of a graph
into two partitions P0 and P1. The set of components required to implement
P1 is a subset of the set of components required to implement P0.
With the use of multiplexers on the inputs of the operators in and the use
of selection signals to connect the corresponding signals to the module inputs,
it is then possible to implement the connections deﬁned in conﬁguration ζi as
well as those deﬁned in ζi+1 together. Switching from the conﬁguration ζi
to the conﬁguration ζi+1 can be done by setting the corresponding value of
the selection signals. The device is therefore ‘reconﬁgured’ without changing
the physical conﬁguration. We call this process conﬁguration switching. With
conﬁguration switching, there is no need to save the registers of the FPGA in
the processor address space during reconﬁguration because no register is al-
tered. With this, conﬁguration switching will help to reduce the data exchange
between the processor and the reconﬁgurable.
Conﬁguration switching can be extended to a series of conﬁgurations ζ1, ..., ζr
by extracting the smallest amount of common operators needed to implement
all the conﬁgurations and implement the conﬁguration switching on ζ1, ..., ζr.
8A well order deﬁnes the way components with the same level number should be place in the list before
partitioning.

146
Reconﬁgurable Computing
Figure 4.26.
Partitioning of a graph into two sets with common sets of operators
But the amount of multiplexers needed to assign the operators to a particu-
lar conﬁguration as well as the difﬁculty to route all the possible conﬁgura-
tions can jeopardize the implementation of conﬁguration switching for a high
number of conﬁgurations. The amount of partitions to be implemented using
conﬁguration switching must be kept small.
Example 4.30 Figures 4.28 shows the implementation of conﬁguration
switching on a device for the two partitions of ﬁgure 4.27.
During reconﬁguration, the corresponding values for the selection signal
are set on the multiplexers. The output of the corresponding partitions are
then selected and sent to the operators.
Although conﬁguration switching can save time and reduce the data trans-
fer between the processor and the reconﬁgurable device, its implementation
usually requires additional resources. If the amount of additional resource be-
comes too big, then it makes no sense to have a smaller number of components
implemented, and a larger logic area on the device just to realize the switch.
The tradeoffs between the numbers of additional resources and the number of
conﬁgurations to be implemented has to be chosen carefully. The search for
a tradeoff between the number of conﬁgurations to reside in the device and
the complexity of the ﬁnal design is an optimization problem that we do not
address in this book, namely the exploration of the design space.

High-Level Synthesis for Reconﬁgurable Devices
147
Figure 4.27.
Logical partitioning of the graph of ﬁgure 4.26
Figure 4.28.
Implementation of conﬁguration switching with the partitions of ﬁgure 4.27

148
Reconﬁgurable Computing
3.
Conclusion
High-level synthesis for reconﬁgurable devices has beneﬁtted from the var-
ious method provided in the past for the general high-level synthesis problem.
The main difference between the two approaches is in the freedom of choice on
the operator type in the case of reconﬁgurable device. However, one must deal
with temporal partitioning, which allows the reuse of chip resource over time.
Time multiplex reuse of resources was a great matter of concern for a decade,
as the capacity of FPGAs was very small to accommodate one of the complete
functions to be implemented. Meanwhile, the size of FPGAs has grown a lot,
and it is quite difﬁcult to ﬁnd those functions to be temporally partitioned to
exploit chip resources. The interest on temporal partitioning methods has con-
siderably decreased as a consequence of this growth in the size of FPGAs. This
does not mean that temporal partitioning is useless. We still have areas of com-
putation such as rapid prototyping where reconﬁgurable device can provide a
very cheap alternative to the very expensive existing systems. Rapid proto-
typing systems are usually done with expensive machines containing a set of
FPGAs. Applications are then partitioned among the FPGAs, which compute
each part of the function to be implemented. With temporal partitioning, a few
number of FPGAs can be used by several parts of the applications that must
not be computed at the same time.

Chapter 5
TEMPORAL PLACEMENT
In the last chapter, we presented the high-level synthesis problem for recon-
ﬁgurable devices and some solution approaches. The result is a set of partitions
that are used to reconﬁgure the complete device. While the implementation of
single partitions is easy, the amount of waste resources in partitions can be very
high. Recall that the waste resource of a component is the amount resources
occupied by that component multiplied by the time where the component is
idle on the device.
Wasting resources on the chip can be avoided if any single component is
placed on the chip only when its computation is required and remains on the
device only for time it is active. With this, idle components can be replaced by
new ones, ready to run at a given point of time. Exchanging a single compo-
nent on the chip means reconﬁguring the chip only on the location previously
occupied by that component. This process is called partial reconﬁguration
in contrast to full reconﬁguration where the full device must be reconﬁgured,
even for the replacement of a single component. To be exploited, the partial
reconﬁguration must be technically supported by the device, which is not the
case for all available devices. While most of the existing devices support full
reconﬁguration, only few are able to be partially reconﬁgured.
For a given set of operations to be executed, the resource allocation on the
device is a time-dependant process in which not only the placement of the com-
ponents on the device is deﬁned, but also the time slot in which the execution
of the task must be performed. The time-dependant placement of task on the
device is called temporal placement.
Temporal placement can be graphically illustrated through an arrangement
of rectangular boxes in a 3-dimensional container whose base is deﬁned by
the surface of the device and the high is the time axis. Each box represents a

150
Reconﬁgurable Computing
components with a given surface placed at a given location on the device at a
time deﬁned on the time axis.
Example 5.1 In ﬁgure 5.1, a temporal placement of a set of components
{v1, v2, v3, v4, v5, v6, v7, v8} is given. Component v1 for example occupies a
surface on the deﬁned by its length and height for a time slot 0 to 55, which
corresponds to its computation latency. Component v5 starts its execution at
time 90 and occupies among others, part of the device that was perviously
occupied by component v4. Component v8 occupies a part of the device for the
whole computation time.
The computation of a temporal placement for a given set of components rep-
resenting some tasks to be implemented can be done ofﬂine, at compile-time
or online, while the device is executing. In the ﬁrst case, for an application
speciﬁed as dataﬂow graph that must be computed on the device, the com-
putation sequence is deﬁned at compile-time and remains ﬁxed for the given
function. In online temporal placement or simple said, online placement, the
computation sequence is not known in advance. The creation of task that must
be placed on the device is done dynamically at run-time. It is therefore not pos-
sible to capture the speciﬁcation of an algorithm for online placement through
a given model at compile-time.
Figure 5.1.
Temporal placement as 3-D placement of blocks

Temporal Placement
151
Temporal placement has the advantage of being highly ﬂexible and efﬁcient
in terms of device utilization; however, it also has a major drawback. Efﬁcient
temporal placement algorithms are difﬁcult and cost-intensive. Online place-
ment requires to solve some computational intensive problems at run-time in a
fraction of millisecond. Those are the efﬁcient management of the free space,
the selection of the best site for a new component and the management of com-
munication. For a new module to be placed at run-time, it is not sufﬁcient to
only compute the best placement site. The communication between the mod-
ules running on the chip must also be considered. The communication between
modules running on the chip and the external world must be taken into account
as well. A reconﬁgurable device must provide a viable online communication
mechanisms to help establish the communication among modules on the chip
at run-time. This problem is not easy to solve and requires most of the time,
some prerequisites on the device architecture.
Although communication aspects such as the distance among components
are considered in some of the methods provided in this chapter, we do not deal
with the technical realization of communication here. Chapter 6 is devoted
to this topic. We assume that communication among modules placed on the
device is somehow possible.
The ﬁrst part of this chapter addresses the ofﬂine temporal placement, whereas
the second part deals with the online temporal placement. In both cases, we
start with some deﬁnitions related to the part being addressed, and then we
present existing approaches to solve the corresponding problem.
1.
Ofﬂine Temporal Placement
We ﬁrst provide a formal deﬁnition of the temporal placement problem be-
fore investigating existing solution approaches.
For sake of simplicity and
compatibility, we use the deﬁnitions provided in [199] [82].
Definition 5.2 [(Off-line) Temporal placement]
Given a dataﬂow graph G = (V, E) and a reconﬁgurable device H with length
(Hx and width Hy), a temporal placement is a 3-dimensional vector function:
p = (px, py, pt) : V →
 3, where pt deﬁnes a feasible schedule.
For a given node vi, the values px(vi), py(vi) and pt(vi) denote the coordi-
nates of the node vi in a 3-dimensional vector space. px(vi) and py(vi) are the
coordinates of vi on the device H, while pt(vi) deﬁnes the starting time of vi
on H.
We next present some solution approaches for the ofﬂine temporal place-
ment problem. We ﬁrst introduce a simple incremental method based on the
ﬁrst-ﬁt/best-ﬁt concept. In the second method, a clustering of components in
blocks that can be placed together on the chip is done. Finally, we introduce

152
Reconﬁgurable Computing
an exact method that characterized valid partitioning as a set of constraint
equations, used to describe the 3-D packing of boxes in containers.
1.1
First-Fit and Best-Fit Placement
Given a dataﬂow graph for which a temporal placement must be computed,
a simple approach will consist of continuously keeping track of free locations
on the chip. The nodes to be placed will then be selected according to their
readiness. A node is said to be ready to be placed, if all its predecessors are
already placed. For each selected node, one of the free locations on the chip is
selected, and the node is placed at that location, provides that the space is large
enough to accommodate the selected node. If the selection procedure chooses
the ﬁrst free place on the chip to place the new component, then we call it a
ﬁrst-ﬁt.
The ﬁrst-ﬁt procedure is fast and run in linear time in the number of free
locations. However, if we assume that each selected location can accommodate
only one component, the amount of unused resource created by the component
can be very high, if a location is selected whose surface is bigger than that of
the component. To avoid this situation, we may choose a best-ﬁt strategy in
which the component is placed on the location whose surface is the closest to
that of the component. We call this approach a best-ﬁt. Best-ﬁt is much time
consuming than ﬁrst-ﬁt, because the complete list of free locations must be
searched for the best location for the component to be placed.
The deﬁnition of a location has a great importance in the ﬁrst-ﬁt and best-
ﬁt strategy. What is a location? A simple rectangle? or is it a shape with
an arbitrary contour? For sake of simplicity, we assume that a location is a
rectangle, in which case the test of inclusion of a given component in a free
location is easy. What happens with the new free locations that results from the
placement of a component on a selected free area of the device? Just ignoring
those free fragmented locations can be inefﬁcient. In many case, the total
amount of fragmented area can be enough to hold some components. If we
do not keep track of the fragmented area and if those fragmented area are not
consecutive, then many components cannot be placed on the device, although
enough free space exist to accommodate those components. Managing the
fragmented locations in turn requires a very large book keeping efforts. In
Section 2, we will present some algorithms that deal with the fragmentation
issues in the context of online temporal placement.
A possibility to keep the ﬁrst-ﬁt and the best-ﬁt approaches simple is to
segment the device in sectors, each of which is deﬁned as a locations. Such
a location is able to hold a given amount of components, whose total area
cannot exceed the size of that sector. This approach is attractive for many
devices in which the partial reconﬁguration is done with some restrictions, as
it is the case for example with the Xilinx Virtex and Virtex II FPGA, where the

Temporal Placement
153
reconﬁguration can be done only column wise. Whenever a component has to
be exchanged, all the components sharing some columns with that component
are affected. The situation is better in the Virtex 4 and Virtex 5 families, where
the reconﬁguration does not affect the complete device column, but only the
complete height of a rectangular block within the chip. By deﬁning the location
to span the complete device column in the case of the Virtex and Virtex II
and the block height for the Virtex 4 and Virtex 5, we avoid to disturb the
components already running on the device during the reconﬁguration process.
Given a dataﬂow graph and a device partitioned in locations, we would then
like to place several components at the same time at a given location on the
device. The dataﬂow graph must then be partitioned in sets of components such
that each set can ﬁt into a location. Several partitioning strategies exist in the
literature. However, most of them does not take the precedence constraints into
account. Partitioning strategies are not presented in this section. We refer to
the temporal partitioning algorithms presented in the previous chapter that can
also be used here. In this case, the size constraint must be the size of a location.
To limit the amount of wasted resources on the chip, a clustering algorithms
that tries to place components in partitions according to their ﬁnishing time
was presented in [30]. This has the advantage that all the components ﬁnish at
the same time, thus minimizing the wasted amount of resources.
Let us now assume that for given a dataﬂow graph G = (V, E), a parti-
tioning or better say a clustering C = {C1, ..., Cn} is computed such that
all the clusters fulﬁll the size constraints. Algorithm 10 computes a tempo-
ral placement of C in a ﬁrst-ﬁt manner. For a new cluster Cact to be placed
on the device, the algorithm checks for the cluster Ctop with the minimum
run-time among the clusters allocated to the device for which the precedence
constraint Ctop ≤Cact between Ctop and Cact holds. The cluster Cact is
placed on top of Ctop. The placement of cluster Cact on top Ctop simply means
that the resources allocated to Ctop will be allocated to Cact after Ctop has
completed its execution. The algorithm stops when all the clusters have been
placed.
Example 5.3 Figure 5.2 illustrates the ﬁrs-ﬁt temporal cluster placement on
a set of ten clusters. We assume that the precedence constraints are satisﬁed
Algorithm 10 First-ﬁt temporal placement of clusters
while while all the clusters are not placed do do
Select one cluster Cact from the list of ready to run clusters
From the clusters already placed, select the cluster Ctop
with the smallest ﬁnishing-time such that Ctop ≤Cact
place the cluster Cact on top of Ctop
end while

154
Reconﬁgurable Computing
Figure 5.2.
First-ﬁt temporal placement of a set of clusters
during the placement. At the beginning, the clusters C0, C1, C2, C3 to C4
are completely placed on the device. To place cluster C5, cluster C0 with the
earliest ﬁnish time is selected, and cluster C5 is placed on top of C0. Cluster
C6 occupies the space freed by cluster C2, and cluster C7 is placed on top
of C3. C8 is then placed on top of C4, C9 on top of C1 and C10 on top of
C5. The conﬁguration sequence produced by this example is ({0, 1, 2, 3, 4} →
{5, 1, 2, 3, 4} →{5, 1, 6, 7, 4} →{5, 1, 6, 7, 8} →{5, 9, 6, 7, 8} →{10, 9, 6, 7, 8}).
The ﬁrst-ﬁt and best-ﬁt approaches provide fast method to compute a tem-
poral placement solution. However, no effort is spent on the efﬁciency of the
space management. Integer linear programming can be used as exact method
to solve the temporal placement problem. In this case, a set of constraint equa-
tions that must be fulﬁlled by each solution must be formulated. Solving the
equations will then provide a solution to the temporal placement problem. Be-
cause the computation of a ILP-solution is computational intensive, integer
linear programming is usually suitable only for small-size problems. An exact
method that we next presented was proposed in [199] [82] to solve the tempo-
ral placement for large-size problems. Equations are formulated in a similar
way as in integer linear programming. However, a branch and bound strategy
is used to reduce the search space and allow much large problems to be solved.

Temporal Placement
155
1.2
Packing Approach for Temporal Placement
The idea of modeling the temporal placement problem as a 3-D packing
problem, which is the problem of deciding if a given set of boxes can be placed
within a given container of size (x, y, h) representing the length, the width and
the height of the container, was ﬁrst presented in [199] [82]. According to the
goals seeked, several variations can be made in the formulation of the problem.
A goal can be for example the minimization of the overall execution time or the
minimization of the amount of wasted resources for a given algorithm. Let us
consider two possible variations of the packing problem: the base minimization
problem (BMP) and the the strip packing problem (SPP)
1 Base Minimization Problem: Given a set of boxes B and a height h, the
problem of ﬁnding a container with minimal size (x, y, h) that can accom-
modate the set of boxes B is called the BMP. The corresponding goal in
temporal placement is to ﬁnd the device with minimum size (x, y) on which
a set of components can be implemented given an overall run-time t = h.
2 Strip Packing Problem: Given a set of boxes and a base (x, y), the problem
of ﬁnding the minimum height h container with size (x, y, h) that can hold
all the boxes is called the SPP. The analogy with the temporal placement
is to ﬁnd the minimum run-time t = h for a set of components given a
reconﬁgurable device with size (x, y).
With the optimal solution of the BMP, one can select the best reconﬁgurable
device, in term of size, on which a set of tasks can be computed in a limited
amount of time. This device is optimized for the given set of task and for the
given run-time. In reconﬁgurable computing, the main task is not necessary to
choose an optimal device for a ﬁxed set of tasks, but to have a ﬁxed device on
which various set of task can be optimally implemented. The BMP is therefore
not the right tool to be used.
The SPP that we next focus on best matches the requirements of having only
one device on which different set of tasks that can be implemented at different
time. In its original version, the goal of the SPP was to minimize the overall
computation delay. This may be changed according to the objectives seek.
As stated earlier in this section, integer linear programming can be used to
formulate all constraints that must be fulﬁlled for a given solution, as a set of
equation to be solved. However, this approach can be used only for small-size
problems. The concept of packing classes that we next present was used in
[199] [82] as a mean to deﬁne a feasible solution for the SPP.

156
Reconﬁgurable Computing
1.2.1
Packing Classes
The solution of an instance of the temporal placement problem can ﬁrst be
characterized through a valid packing, using the concept of packing classes.
This approach is limited to the pure packing of boxes in a container and does
not capture the precedence constraints in the dataﬂow graph.
Providing an orientation to the packing classes can then be done in a sec-
ond step to capture the precedence constraints of the dataﬂow graph in a valid
solution. The description provided in [199] [82] considers packing in arbitrary
dimension. We are interested only in a 3-D packing and will therefore limit our
description accordingly. We start by providing some deﬁnitions of the terms
needed in this section.
Definition 5.4 (Interval Graph) Given a dataﬂow graph G = (V, E),
a reconﬁgurable device H and a packing of V into H, i.e a 3-D placement of
the components of V on the device H. An interval graph of G is a graph
Gi = (V, Ei), ∀i ∈{1, 2, 3} such that: (vk, vl) ∈Ei ⇐⇒the projections of
the nodes vk and vl overlap in the i-th dimension, i.e on the i-th axis.
Definition 5.5 (Complement Graph) Given a dataﬂow graph G =
(V, E), a reconﬁgurable device H and a packing of V into H, The complement
graph of an interval graph Gi = (V, Ei) is the graph Gi = (V, Ei), ∀i ∈
{1, 2, 3}.
The strategy presented here consists of ﬁrst providing a better characteri-
zation of a solution. With this, potential solutions can be checked for validity.
The second step will then consists of constructing arrangements, each of which
is tested for its validity as solution. The validity of a solution can be enforced
with the two following conditions that must be fulﬁlled by the corresponding
interval graphs of a given arrangement:
Any independant1 set S ⊆G is i-admissible. That is 
v∈S pri(v) ≤
hi, where pri(v) is the length of projection of component vi on the i-th
dimension and hi is the length of the device in the same dimension.
3
i=1 Ei = ∅
The ﬁrst restriction states that in a direction i, the total length of components
placed on the device and pairwise overlapping in the perpendicular dimension
to i should not exceed the length of the device in dimension i. To better un-
derstand this restriction, consider a perpendicular line to direction i that run
through the device. All the components that are cut by the line pairwise over-
lap in the perpendicular direction to i. The total sum of their width in the
1∀v, w ∈S, v overlaps with w in dimension (3 −i), i ∈{1, 2}.

Temporal Placement
157
i direction should therefore not exceed the line segment which is in the device,
i.e. the size of the device in dimension i. Placing many such lines at the dif-
ferent coordinate points in direction i of the device will help to capture all the
restrictions stated by the ﬁrst condition.
With the second restrictions, we avoid placements in which components
overlap in all dimensions. A placement can only be valid if for any pair of
components, at least 1-D must exists in which the two components do not
overlap.
Definition 5.6 (packing Class) A 3-tuple of interval graphs that sat-
isfy the two previous deﬁned restriction is called a packing class.
Example 5.7 The two-dimensional placement of ﬁgure 5.3 is a valid pack-
ing with the corresponding packing classes G1 and G2. In ﬁgure 5.4, we have
an example of invalid packing for two reasons. First, part of the component
v5 is placed with a part out of the device and second, components v3 and v4
overlap in all directions.
Using the pure packing classes as previously deﬁned help to deﬁne the
placement aspect of the temporal placement problem, without caring about
the constraints in the dataﬂow graph. Because those constraints must be pre-
served in a valid temporal placement, a modiﬁcation of the packing classes
must be done to insure that a component depending on another one starts its
execution only after the component on which it depends has completed its
Figure 5.3.
Valid two dimensional packing

158
Reconﬁgurable Computing
Figure 5.4.
A non valid two dimensional packing
execution. The modiﬁcation that we present here are the orientation of the
packing classes.
1.2.2
Orientation of Packing Classes
An undirected edge {v1, v2} in a component graph Gi corresponds to an
intersection between the projections of component v1 and v2 in the i-th dimen-
sion. However, it does not provide any information on the relative position of
component v1 compared to v2. The two nodes v1 and v2 can be compared ac-
cording to their placement in one of the three dimensions. In the x-dimension,
we have the relation ‘left to’ and ‘upper’ in the y-dimension. However, we
are only interested in the 3-rd dimension, i.e the time axis, where we deﬁne
the relation ‘place after’. For each pair of components v1 and v2, either the
components v1 is placed after v2 (pt(v1) + t1 ≤pt(v2)) or vice versa, if their
placement overlaps in the 3-rd dimension. We use this fact to deﬁne the com-
parability graph of an interval graph.
Definition 5.8 (Comparability Graph) Given
a
dataﬂow
graph
G = (V, E), a reconﬁgurable device H and a packing of V into H.
The comparability graph of an interval graph Gi = (V, Ei), ∀i ∈{1, 2}
is the directed graph Gi = (V, Ei), where (vk, vl) ∈Ei ⇐⇒vl is ‘placed
after’ vk in 3-th dimension, i.e. (pt(vk) + tk ≤pt(vl)).
The relation ‘place after’ deﬁned by the comparability graph is a transitive
relation also known as transitive orientation that can be used for orienting the
packing classes.

Temporal Placement
159
Definition 5.9 (Packing Class Orientation) Given
a
dataﬂow
graph G = (V, E), a reconﬁgurable device H and a packing of V into H.
The orientation of the packing class corresponding to the packing is deﬁned
through the comparability graph of the interval graph in the time dimension.
The orientation of a packing is used to reﬁne information about the place-
ment of components on the time axis. This reﬁnement is the information need
to check if the precedence constraint is fulﬁlled. Precedence constraints do
not play a role for the placement of component on the x and y directions.
Therefore, a large variety of packing classes exist that lead to different 3-D
placements with precedence constraints.
Example 5.10 Figure 5.5 provides a 3-D placement and its characteriza-
tion through interval graphs, complement graphs and the orientation of the
packing class associated to the placement. Shown is only the graph related to
the dimensions x and y.
Now that a good description of a solution is available, we have the possibil-
ity to check if a given arrangement of nodes forms a valid packing with some
precedence constraints fulﬁlled. We only need a method to build arrangements
on which we can apply the formulas previously described to check whether
Figure 5.5.
3-D placement and corresponding interval graphs, complement graphs and ori-
ented packing

160
Reconﬁgurable Computing
or not the arrangement is a solution. Moreover, if we have a set of available
solutions, we also need to extract the optimal one according to the optimality
criteria. A solution or better said an arrangement that can be checked for valid-
ity can be constructed by arbitrarily ﬁxing the edges of the different graphs as
well as their orientations. The set of all possible arrangements that can be built
in this constructive fashion called the solution space. This can be extremely
large and very difﬁcult to explored, if the size of the problem is big.
In [199] [82], a branch and bound strategy for iteratively prune the set of all
possible arrangements is presented. The approach is an incremental that aim
at constructing a tree from the root to the leaf. The root of the tree consists of
the set of available nodes. The tree is constructed from the root to the leaf by
inserting edges to the already existing graph. Two different insertions of edges
on the same node lead to two different branches in the tree. In each step of the
algorithm, a new edge is included into the graphs, and the validity of the result-
ing placement is checked. If the resulting placement will not be valid, then all
possible arrangement in which the introduced edges exist are discarded from
the search. Otherwise, a new edge is added to the constructed graph. When-
ever a branch will not lead to valid leaf, the complete subtree resulting from
the introduction of a new branch is discarded. This helps to avoid exhaustive
and expensive checks on branches that do not lead to a solution. The formal
characterization of a valid solution previously done provides at the same time
a means to check if the full or partial arrangement constructed can lead to a
solution. In [199] [82], the authors use the concept of induced cycle to identify
and discard paths that will lead to non-valid solutions.
The method presented in this section target problems well speciﬁed at
compiled-time with no change on the computational ﬂow at run-time. In many
systems, in particular real-time ones, the set of task as well as their interconnec-
tivity is not known at compiled-time. Run-time or online placement possibility
must be provided to deal with unpredictable event that creates tasks, which
must be executed at run-time. The next sections deal with this issue.
2.
Online Temporal Placement
In Section 1, we presented the architecture of a dynamic run-time reconﬁg-
urable system (ﬁgure 3.1). The dynamic management of the device resources
is done by two main components: a scheduler and a placer, each of which can
be implemented as part of an operating system running on a processor. While
the scheduler determines which task should be executed on the RPU, the placer
tries to place the associated bounding box associated to a task on the device,
thus allocating rectangular resource area on the device for the execution of that
task. Whenever the placer receives a request from the scheduler to place a task
and it is not successful in doing that, the scheduler must decide on what to do

Temporal Placement
161
with the task. It can later try again to run the task on the RPU or it can de-
cide to let the task run with a lower speed on the CPU. In any case, not being
able to satisfy a placement request creates additional delay in the execution of
the program. To avoid this penalty, care should be taken on the design of the
placer. The placer should be able to place as much tasks as possible on the de-
vice to avoid delay penalties. This goal can be reached if the amount of wasted
resource is kept small. We ﬁrst state the online placement problem and then
present some solution approaches.
Definition 5.11 (Online Placement) Given a reconﬁgurable pro-
cessing unit H at time t with a given conﬁguration Ct. Find an optimal posi-
tion for a new incoming task v such that v does not overlap with any running
component in the conﬁguration Ct.
As deﬁned in the mathematics, the goal when solving an online problem
does not only consist of providing an optimal method at a given time step, but a
method that is optimal for a sequence of computation not ﬁxed in advance. The
method should be developed to cope with unknown parameters that may arise
in the future. A non-optimal partial solution can be preferred to an optimal one
at time t, if this partial sub-optimal solution contributes on a global optimal
in the future. In this section, we limit the online deﬁnition to the stepwise
optimization of a partial problem in a given time slot, without caring about the
global optimal solution.
We present in the next sections two approaches for online placement of in-
coming components on the device. While the ﬁrst approach manages the free
space on the device by keeping track of all empty rectangles from which one
will be selected to place the new module, the second one keeps only track of
the occupied area on the device and try to place a new component in such a
way that no overlap with a running module occurs.
3.
Managing the Device’s Free Space with Empty
Rectangles
One possibility to implement the placer is to always keep track of the free
space on the device. On request to place a new component, a free location that
can accommodate the new component is chosen and the module is placed at
the selected location.
Bazargan et al. [18] proposed the used of a list of empty rectangles on the
chip to capture the free locations, an approach that was used in computational
geometry to solve rectangle layout problems as it is the case in glass cutting
and paper cutting industries [116].
The main task to solve is to place a given set of rectangles, so-called pieces,
in a given larger rectangular area, the slot sheet, such as to minimize the amount

162
Reconﬁgurable Computing
of wasted space in the resulting layout. Once again, we start with deﬁnitions
and then continue with solution approaches.
Definition 5.12 (Empty Rectangle) An Empty Rectangle (ER) is a
rectangle that does not overlap a placed module on th chip. A maximum empty
rectangle (MER) or Largest Empty Rectangle (LER) is an empty rectangle that
is not included in any other rectangle than the device bounding box.
Example 5.13 An illustration of empty rectangles is provided in ﬁgure 5.6.
(E, D), (A,D) and (E,C) are example of empty rectangles. While (E, D) and
(A,D) are maximal, (E,C) is not because it is included in (E,D). (E,F) is not an
empty rectangle because it overlaps module v2.
On the basis of empty rectangle concept, a strategy to solved the online
temporal placement problem was proposed in [18]. The method, which has the
name keeping all maximum empty rectangles (KAMER), permanently keeps
track of all MERs. Whenever a request for placing a component v arrives, the
list of MERs is searched for a rectangle that can accommodate v. Because
it is possible to have many MERs in which v can ﬁt, strategies such as the
ﬁrst-ﬁt or best-ﬁt are used to select one rectangle from the set of possible free
rectangles. Once a rectangle is chosen, the candidate points that can be chosen
as reference point for placing the new component are those that do not allow
an overlap with the external part of the rectangle. In [18], the authors use the
bottom left point as the reference one to place the component.
Figure 5.6.
Various types of empty rectangles

Temporal Placement
163
The great advantage of the KAMER approach is that all the free space on
the device is captured, thus providing a good quality2. This happens at the cost
of efﬁciency in the computation time. As the following example shows, the
number of empty rectangle does not grow linear with the number of compo-
nents included. Whenever a new component is placed on large, depending on
the conﬁguration.
Example 5.14 Consider the conﬁguration shown in ﬁgure 5.7(a) with 11
MERs ((D,I), (C,H), (A,P), (A,O), (F,O), (E,N), (Q,G), (Q,H), (E,K), (M,I),
(L,J)) before the placement of a new component of the device. After the place-
ment of module v4, the number of MERs grows to 14 ((D,I), (C,H), (N,P),
(A,U), (N,O), (A,S), (F,S), (E,R), (E,K), (L,H), (L,J), (M,S), (V,I), (T,I)), thus in-
creasing the complexity by a factor more than the number of included modules.
Also the number of free rectangles can be drastically reduced after the removal
of a module running on the chip. The insertion of components on the chip as
well as their later removal creates large ﬂuctuations on the number of empty
rectangles to be managed, thus increasing the complexity of the algorithm. In
[116], the run-time of the free rectangle placement is shown to be O(n2).
To avoid the quadratic run-time of the KAMER, a simple heuristic was pro-
posed in [18], with a lower quality and a linear run-time. The strategy con-
sist of keeping only the non-overlapping empty rectangles, thus reducing the
number of empty rectangles to be managed. This allows a linear run-time
at the cost of the quality.
The problem with this approach is that several
(a) The conﬁguration contains 11 modules
befog the insertion of a new task
(b) The number of modules grows to 14 after
the insertion of a new task v4
Figure 5.7.
Increase of the number of MER through the insertion of a new component
2If a free space exist in which the module can be placed, then there must be a free rectangle that can
accommodate that module, and therefore, the MER approach will ﬁnd a placement.

164
Reconﬁgurable Computing
Figure 5.8.
Two different non-overlapping rectangles representations
non-overlapping rectangles representation of a conﬁguration may exists lead-
ing to multiple possible choices in the representation as illustrated in ﬁgure 5.8.
Shown are two different representations of the free space with different sets of
non-overlapping free rectangles.
Because the non-overlapping empty rectangles are not necessary maximal, a
module may exists that could ﬁt onto the device, but cannot be placed because
of a bad non-overlapping representation.
As shown in ﬁgure 5.9, whenever a new component v1 is placed in a non-
overlapping rectangle, two possibilities exist to split that rectangle. A horizon-
tal split that is done using segment Sa and a vertical split using the segment Sb.
Choosing to select one the two split directions may have a negative impact
on the placement of the next components. Assuming for example that the al-
gorithm keeps selecting the horizontal split, after the placement of module v1
(ﬁgure 5.9), the free rectangles left are (A,D) and (C,E). Assume now that a
new module, whose width is the same as (C,E) but with a height slightly bigger
than that of (C,E) is chosen. The algorithm will not be able to place the compo-
Figure 5.9.
Splitting alternatives after new insertion

Temporal Placement
165
nent, although enough free space is available on the chip to accommodate the
new module. Bazargan et al. proposed several strategies for choosing one split
direction with the goal is to favour those splits that create quadratic space on
the chip. To increase the quality of the non-overlapping free rectangle heuris-
tic, a strategy is proposed in [197] that simply consist of delaying the split
decision for a number of steps later. This may allow a bad choice that could
have been done earlier to avoid.
While the KAMER algorithm always ﬁnd a rectangle to place a new com-
ponent, if one exists, the position to place the component within the rectangle
must be selected from a set of points whose number is the area of the rectangle
in worst case. An optimal algorithm must choose one point from the set of
possible points according to some optimization criteria. In [116], the bottom
left position is chosen. Because no relation exists between the rectangle to be
placed and those already placed, an arbitrary position can be chosen for the
new rectangle. In reconﬁgurable devices where the communication between
pairs of tasks on one side and between a task and the device boundary on the
other side plays an important role, components should be placed in such a way
that the communication will be efﬁciently realized. If a position in the middle
of the selected rectangle is better adapted for optimizing the goal seeked, then
the component should be placed there, no mater if the number of empty rect-
angle increases. An optimal algorithm should consider any single point where
the placement is possible and develop a strategy to choose the best position.
This approach is followed by the next algorithms that we present. Instead of
managing the free space, the strategy consists of managing the occupied space
on the device and use the set of running components for computing the best
position to place the incoming component.
4.
Managing the Device’s Occupied Space
The observation that has guided the development of this approach is that the
number of placed modules on the device grows linearly contrary to the number
of free rectangles that has a quadratic grows. Each insertion of a new compo-
nent on the chip increases the number of placed components by only one, the
same as any removal of a component decreases the list by only one. By man-
aging the number of placed modules, we do not face large ﬂuctuations in the
list of modules like it is the case with empty rectangles. It is therefore possible
to develop a linear time optimal algorithm for the online placement problem.
With this, the temporal placement can be splitted into two subproblems as ﬁrst
presented in [3].

166
Reconﬁgurable Computing
1 Identify the set of potential sites to place the new component.
2 Select the best site to place the component according to a set of given cri-
teria.
Assuming that a set of possible placement sites is identiﬁed, several criteria
can be used to choose between the feasible positions. Here, we consider the
communication cost between the task and its environment as the objective to
be minimized. The connections between two different components as well as
those between modules on the device and the device boundary are of great
importance. While the ﬁrst one allows two modules to communicate together,
the second one allows a modules within the device to communicate with a
module out of the device.
The straightforward way to solve this subproblem is to use a brute force
Algorithm. For each new component c to be placed, the brute force algorithm
solves the ﬁrst subproblem by scanning all the positions on the device. For
each position p = (xp, yp), it checks if an overlapping will occur between c
and a placed module, if the component c is placed at location p.
Having solved subproblem through the scanning of all possible positions,
the optimal placement position is computed from the set of feasible positions
by computing the placement cost for each of the locations found in the ﬁrst
step and then select the best one as the optimal solution.
The Brute force requires O(H × W × n) time to solve subproblem 1. H
being the height, W the width of the reconﬁgurable device and n is the number
of running tasks on the hardware. This approach is not practicable for large
reconﬁgurable devices.
Without loss of generality, we will consider that components are placed rel-
atively to their lower left positions. Later, we will use the middle of the device
as reference point. We next provide some deﬁnitions that are important to
understand the placement strategy explained in this section.
Definition 5.15 (IPR relative to placed modules) For
a
new
component v to be placed on the device and a placed component v′, the Im-
possible Placement Region (IPR) Iv′(v) of v relative to v′ is the region on the
chip, where v cannot be placed without overlapping with v′.
For a set V ′ of placed components, the impossible placement region IV ′(v)
of v relative to the set V ′ is the region on the chip, where v cannot be placed
without overlapping with an element of V ′: IV ′(v) = 
v′∈V ′ Iv′(v).
Besides the impossible placement region relative to components of the de-
vice, an impossible placement region relative to the device itself exists and is
deﬁned as follow:

Temporal Placement
167
Definition 5.16 (IPR relative to the device) The
Impossible
Placement Region IH(v) of a module v relative to a reconﬁgurable device
H is the region of the device where v cannot be placed without overlapping
with the external area of the device.
Having deﬁned the impossible placement region relative to the device and
that relative to other components on the chip, we can now deﬁne the overall
impossible placement region of a component on a device with a running set of
components.
Definition 5.17 (Impossible and Possible Placement Regions)
The impossible placement region IPR I(v) of a new component v on a recon-
ﬁgurable device H is the region on the device, where v is not allowed to be
placed, if the placement has to be valid. I(v) = IV ′(v) ∪IH(v), where V ′ is
the set of all the components running on the chip
The possible placement region (PPR) P(v) of a module v is obtained by
subtracting the IPR of the module from device from the device area. If L is the
set of all locations on the device, then P(v) = L −I(v).
As illustrated in ﬁgure 5.10, for a new component v with height hv and
width wv to be placed on the device and a placed component v′, the IPR Iv′(v)
of v relative to v′ can be computed by ﬁrst deﬁning a left-side margin with size
wv −1 and a bottom margin with size hv −1 for the component v′.
The IPR of v relative to v′ is the sum of the augmented margin and the area
of v′.
The computation of the IPR relative to the device is slightly different from
that of a component. Instead of computing a left-side and a bottom margin, we
compute a right-side and an upper margin with. The right-side margin has the
Figure 5.10.
IPR of a new module v relative to a placed module v′

168
Reconﬁgurable Computing
Figure 5.11.
Impossible and possible placement region of a component v prior to its insertion
width wv −1, whereas the upper margin has the height hv −1. Figure 5.11
shows the impossible placement region of a component v on a device with four
running components v1, v2, v3 and v4.
Computing the IRP of a given module can be done in linear time O(n),
where n is the number of components currently running on the chip. For each
component, a constant amount of operations is required to identify the IPR rel-
ative to that component.
Now that we know how to compute the region (set of points) where the
component can be placed, the next sub-problem consists of choosing the best
location for the new component. The position where the new component must
be placed should be a feasible position that minimizes the communication cost.
A simple approach consists of scanning all the possible placement positions
and to compute the communication cost for each position and then selecting the
optimal one. This straightforward but inefﬁcient approach requires O(|PPR|∗
n) where n is the number of placed components and |PPR| is the size (number
of point) of the possible placement region. This can be very large according to
the current conﬁguration of the device.
Alternatively, we can ﬁrst compute the point popt that gives us the optimal
placement cost. If popt is located within the PPR of v, then we have the solu-
tion. Otherwise, the optimal position is not in the PPR and we should choose

Temporal Placement
169
the closest point to popt which is located in the PPR and select it as the optimal
placement position.
The deﬁnition of the optimal point is done according to the communica-
tion cost that can be minimized by placing the new component closed to other
components with which it communicates. If the component to be placed has
off-chip connections, then the boundary of the device that connects to the
component is assumed to be a placed module at the corresponding location.
The communication cost that we also called routing cost is therefore formally
deﬁned in terms of weighted distance between modules and device boundary.
We consider that routing cost between two modules is deﬁned by the Euclid
distance. Later we will investigate the Manhattan distance that is better adapted
for the routing cost on a given chip.
Definition 5.18 (Routing Cost) For two modules vi and vj, we deﬁne
the routing cost between modules vi and vj as follows:
Routing Cost(Rij) =
[(xj + wj
2 −xi −wi
2 )2 + (yj + hj
2 −yi −hi
2 )2] × wij
In other words, the routing cost between two modules is the weighted Euclid
distance between the two modules. To compute the routing cost, we use the
center point of the components instead of using the bottom left corner point as
the reference point. For module vi, this is deﬁned by the pair (xi + wi/2, y +
hi/2) where (xi, yi) deﬁne the lower left position of vi, wi the width and hi
the height of vi . If there is no communication between two modules vi, vj,
then the routing cost between the two modules is zero.
With (n−1) modules placed on the device and an n-th module to be placed,
the routing cost is the sum of the routing cost between the n-th module and
each of the n −1 modules running on the chip. The optimal point is therefore
the one that provides a solution to equation 4.1:
min(
n−1

i=1
{[(xn + wn
2 −xi −wi
2 )2 + (yn + hn
2 −yi −hi
2 )2] × win}) (4.1)
In Equation 4.1, xn and yn are variables and other parameters are ﬁxed. Be-
cause xn and yn are positive and independent from each other, we can replace
equation 4.1 by the two equations 4.2 and 4.3.
min{
n−1

i=1
[(xn + wn
2 −xi −wi
2 )2 × win]}
(4.2)
min{
n−1

i=1
[(yn + hn
2 −yi −hi
2 )2 × win]}
(4.3)

170
Reconﬁgurable Computing
The minimums can be computed through the partial derivative of the sums in
equations 4.2 and 4.3. We therefore have for xn
∂{n−1
i=1 [(xn + wn
2 −xi −wi
2 )2 × win]}
∂xn
= 0
(4.4)
This leads to the optimal value:
xn =
n−1
i=1 win(xi + wi
2 −wn
2 )
n−1
i=1 win
(4.5)
In a similar way, we can compute the optimal value for yn:
yn =
n−1
i=1 win(yi + hi
2 −hn
2 )
n−1
i=1 win
(4.6)
The computation of this optimal weighted sum of Euclid distance is fast.
However, the computed point may fall into the IPR of the component. If this
is the case, we need a strategy to quickly move out of the IPR and select the
closest feasible point to optimal. As shown in ﬁgure 5.12, there are four nearest
possible points (NPP) associated with a component in whose IPR the optimal
point falls. Four comparisons are necessary for selecting the closest one.
In the worst case, we may face the situation, where several IPRs overlap
such that moving out of the IPR related to a component bring us in the next
IPR of another component. In this case, we simple keep moving out of the IPR
until we get a point in the PPR. However, the price of doing such a successive
move can be very high. A situation could be constructed such that we have
to go through all IPRs, thus creating a quadratic run-time O(n2). Because
we need O(n) time to compute all the IPRs and O(n2) in the worst case for
moving out of all IPRs if the computed optimal point falls within one of them,
the run-time of the algorithm is quadratic.
Figure 5.12.
Nearest possible feasible point of an optimal location that falls within the IPR

Temporal Placement
171
Algorithm 11 provides a pseudocode for the approach presented here.
Algorithm 11 Pseudocode of the online placement algorithm
1: Compute optimal point (new component)
2: if optimal point is feasible then
3:
place the component at the optimal point
4: else
5:
Find the 4 near points outside the actual IPR, in which the selected point
is located
6:
Insert these points into the optimal points list
7:
Select the closest point to the optimal point from the near points list
8:
if the selected point is feasible then
9:
place the component at the optimal point
10:
else
11:
remove it from the list, and go to 5
12:
end if
13: end if
The implementation of this method can be done using a linked list of size
O(n) (for n running tasks) to store the placed and running modules on the
reconﬁgurable device.
A second structure that can be used is a 2-D matrix with the same size as
that of the reconﬁgurable device. This matrix represents the state of the device.
An entry in the matrix deﬁnes the value of the point with the same coordinate
on the device. Whenever a point on the device is occupied by a module, the
corresponding element in the matrix will have a pointer to the related module.
Otherwise, the point in the matrix shows its emptiness. Also for identifying
PPR, the effects of extending and deleting margins will be applied on this ma-
trix. Using the matrix allows to access each element and obtain its state in just
one computing step.
For ﬁnding optimal or near optimal point for placing a new module, a linked
list for saving near optimal points can be used. The next element to be removed
from the list is always the one closest to the optimal point. If the removed point
is feasible (this is tested by checking the corresponding entry in the matrix),
then we have the NPP, otherwise the selected point is occupied by a running
module or its margin. In this case, the nearest points, out of the border of that
running module are inserted into the list.
Example 5.19 Figure 5.13 shows an example of recursive move out of the
IPRs. The optimal point falls in an IPR relative to module A. The four nearest
points inserted in the list are 1, 2, 3 and 4. The closet point to the optimal is 4
that is next selected but falls in the IPR of module B.

172
Reconﬁgurable Computing
Figure 5.13.
Moving out of consecutive overlapping IPRs
The points 5, 6 and 7 are inserted into the list. The closest point to the
optimum after the removal of the point 4 is the point 1 that is selected and
maintained as solution, because it is in the PPR.
The approach presented here has the drawback of moving from IPR to IPR
until a valid placement point is found. In the normal case, we need only a
few number of steps. However, the worst case still remains and in the case, it
happens, the penalty is a quadratic run-time. A better characterization of the
total set of IPRs can help to improve the efﬁciency as well as the quality of the
method. This is the topic of the next section.
4.1
Using the Contour of Rectangles
The main drawback of the approach previously presented is the delay that
may arise, when moving a point out of the IPRs. This is because the IPRs
of all components are considered separately. If we could merge the complete
IPRs of that component in only one region, then moving out of an occupied
region can be done in constant time, thus improving the run-time of the al-
gorithm. Several IPRs can be merged in only by computing the contour of
the IPRs.
Like in the previous section, the approach presented here is based on the
observation that the occupied space consists of very simple geometric objects,

Temporal Placement
173
namely n placed rectangular modules. The difference is in the use of the Man-
hattan Distance metric for capturing the exact routing costs.
The method was ﬁrst presented in [2] as extension of the work in [3]. The
management of the occupied space is done through a modiﬁcation of the well-
known algorithm ContourOfUnionOfRectangles (CUR) [106] [147]
[179], for ﬁnding the contour of a union of axis-parallel rectangles. As the
number of contour segments is linear in n, a running time of O(n log n) can be
achieved. Note that the method does not require the contour to be connected,
it works even on a set of disjoint contours.
In contrast to the previous section and without loss of generality, the impos-
sible placement region of a component v relative to v′ is now computed with
the reference to the middle point of v, by blowing up the module v′ by half the
width and half the height of v. The impossible placement region of v relative
to the device H is however computed by shrinking the device half the width
and half the height of v.
As deﬁned in the previous section, the new module v is reduced to a point
pv; thus, the original problem of ﬁnding free space to place a rectangle be-
comes the problem of ﬁnding a free location to place a point.
Example 5.20 In ﬁgure 5.14, the result of the transformation for a new com-
ponent that must be placed is shown. All points out of the impossible placement
regions are feasible placement locations from which the optimal one must be
selected.
Figure 5.14.
Expanding existing modules and shrinking chip area and the new

174
Reconﬁgurable Computing
Among all feasible locations, the points on the contour of the occupied
space, i.e those at the boundary of the free space and the occupied space, form
a set from which the nearest to optimal point will be selected, if the optimal
point falls in an occupied region. This helps to avoid the recursive jumping
out of the IPRs of the components as presented in the previous section. The
computation of the contour is therefore at the center of our interest.
4.1.1
Computing the Contour of the Occupied Space
In general ﬁnding, the contour of a set of n axis-aligned rectangles can be
done in O(n log n + s) by using the CUR algorithm as described in [106]
[147] [179]. Here s is the complexity of the resulting contour. The algorithm
presented in [2] is not simply an implementation of CUR. There are a few sub-
tleties that must be considered. All differences stem from the above-mentioned
fact that the points on the contour are feasible locations. As a consequence, the
algorithm has to ﬁnd free space of height and width 0 as shown on ﬁgure 5.15.
The CUR returns an exact contour of a set of placed rectangle, whereas the
modiﬁed algorithm return the exact set of feasible location. Shown is a set of
rectangles (left), representing expanded modules (dashed, light). At the center,
we have the contour as returned by CUR and the resulting feasible space for
placing the center point of the new module (dark, together with the boundary).
The right part of the ﬁgure shows the correct feasible positions for placing the
center point of the new module as returned by the modiﬁed CUR (dark, with
boundary). The difference is a short segment representing the boundary of two
IPR of the two components.
Figure 5.15.
Characterization of IPR of a given component: The set of contours (left), the
contour returned by the modiﬁed CUR (middle), the contour returned by the CUR (right)
In the following, we will describe CUR and the modiﬁcation done in [2].

Temporal Placement
175
The building block of CUR is an algorithmic technique from computational
geometry called plane sweep and a data structure called segment tree. For an
in-depth introduction to both, refer to [66].
A plane-sweep algorithm is an algorithm that scans the plane and a set of ob-
jects in it: It consists of moving an axis-parallel line in an orthogonal direction
across the plane and keep track of the structure of the intersection with the set
of objects. The key observation is to notice that updates to this structure only
occur at a discrete set of critical positions called events. By pre-sorting these
events (in time Θ(n log n)), only the updates have to be performed, which can
be done efﬁciently for all events by using an appropriate data structure. For
our purposes, such a data structure is a segment tree: This is a balanced binary
tree for dynamically storing a set of n intervals. The number of endpoints of
these intervals must be known at construction time. Because it is bounded by
2n, the segment tree can be constructed in O(n). Insertion and deletion of
intervals can be done in O(log n). For more details on segment trees, refer to
[24] [22, 179] [23].
One has to be careful when constructing the segment tree. To ﬁnd free
space of height and width 0, we have to make sure that two modules starting
or ending on the same coordinate are separated by an elementary interval in
the segment tree. This can be done by disturbing the top left corner of each
module by a sufﬁciently small value ϵ > 0.
The algorithm uses two plane sweeps: one horizontal sweep that discovers
all the vertical contour segments and one vertical sweep that ﬁnds all hori-
zontal segments. As the horizontal and the vertical sweeps differ only in the
initialization, only the horizontal one will be described.
For the horizontal sweep, we construct a list L of 2n quadruples, denoted
by (pi, ti, bi, ei): for each module v′
i ∈V ′, where V ′ represents the set of
extended modules that form the IPRs of the components on the chip, we add
two elements to L — one for the left side (x′
i, Open, y′
i, y′
i + h′
i) and one for
the right side (x′
i + w′
i, Close, y′
i, y′
i + h′
i). This list is sorted lexicographically
and we assume that Close < Open.
In the sweep, we process all 2n elements of L. In case of an Open event,
the corresponding contour points are retrieved from the segment tree and the
segment [y′
j, y′
j + h′
j] is added to the tree. For a Close event, the segment
[y′
i, y′
i +h′
i] is removed from the tree, and the corresponding contour points are
retrieved. Another vertical plane sweep is necessary to discover all horizontal
segments.
4.2
Routing-Conscious Placement
After describing how to ﬁnd all feasible locations to place a new module, we
turn to ﬁnd an optimal location, which minimizes the weighted communication
between the modules. Instead of using the Euclid distance as in the previous

176
Reconﬁgurable Computing
section, the Manhattan Distance is used. The Euclid distance between two
components deﬁnes the shortest distance between two points. However, this
shortest distance may be a diagonal line that cannot be realized as signal on the
chip. The Manhattan Distance captures the smallest routing distance between
two components on the chip. Such a route is made only upon vertical and
horizontal segments weighted by width of the communication segments.
This can still be achieved in time Θ(n log n), making use of local optimal-
ity properties, the occupied space manager, and another application of plane
sweep techniques.
4.2.1
Communication Cost
Given a reconﬁgurable device H and a component v, the objective of the
placer is to ﬁnd a point in free space that minimizes communication cost for
the new module v.
As in the previous section, the communication cost of an additional module
m is the sum of the Manhattan distance between the center point of m and the
center points of the modules that communicate with m.
Also, we consider communication with the chip boundary as indicated in
Figure 5.16. We therefore have to consider the set C = {(x1, y1), . . . , (xk, yk)}
of demand points for communication. We assume that the number k of con-
nections to be established with module m is linear in n, i.e the new component
is connected to a running component by a maximum of one link. Obviously,
the width wij of the communication link between modules i and j must be
considered, thus, we get the objective function
min{c(xi, yi) =
k

j=1
wij∥(xi, yi) −(xj, yj)∥1}
m
4
2
1
2
Figure 5.16.
Placement of a component on the chip (left) guided by the communication with
its environment (right)

Temporal Placement
177
Because we are dealing with the Manhattan metric, this can be reformulated to
c(x, y) = cx(x) + cy(y)
with
cx(x) =
k

i=1
wij|xi −x|,
and
cy(y) =
k

i=1
wij|yi −y|.
If we would allow vi to be placed anywhere on the chip, this can be reformu-
lated to
min{cx(xi) : xi ∈[li
2 , Hl−li
2 ]}+
min{cy(yi) : yj ∈[hi
2 , Hh−hi
2 ]}.
As a consequence, we can consider two separate minimization problems, one
for each coordinate. If we ignore feasibility, both minima are attained in the
respective weighted medians, so they can be computed in linear time [27]; as
we already sort the coordinates for performing plane sweep, this running time
is not critical, so we may as well use a trivial method. Note that only medians
satisfy unconstrained local optimality, as the gradients for cx and cy are simply
∇cx(x) =

xi<x
wij −

xi>x
wij,
the sum of the required bandwidths to the left minus the sum of the required
bandwidths to the right and
∇cy(y) =

yi<y
wij −

yi>y
wij,
the sum of the required bandwidths to the bottom minus the sum of the required
bandwidths to the top.

178
Reconﬁgurable Computing
4.2.2
Local Optimality
The optimal point that produces the minimal routing cost with the Manhat-
tan distance is called the median (ﬁgure 5.17). It is the global optimum that
may falls in the impossible placement region. If it is in the occupied space,
there are only two other types of points where the nearest possible point to
optimum could be located.
Median
Figure 5.17.
Computation of the union of contours. The point on the boundary represent the
potentially moves from the median out of the IPR.
All points of the ﬁrst type can be found by intersecting the contour of the
occupied space with the median axes lx = {(xmed, y) : y ∈[0, Hl]} and
ly = {(x, ymed) : x ∈[0, Hh]}. In these points, one of the gradients ∇cx and
∇cy vanishes. We cannot move in the direction of a better solution because
that way is blocked by either a vertical or a horizontal segment of the contour.
The second type of points are some of the vertices of the contour. These
points are the intersections of horizontal and vertical segments forming an in-
terior angle of π
2 pointing in the direction of the median. In these points, neither
of the gradients vanishes. Either of the directions indicated by the gradient are
blocked by contour segments.
By simply inspecting all the local optima, one ﬁnds the one closest to the
global optimum. In the next subsection, we describe how this can be done
efﬁciently.
4.2.3
Algorithm for Global Optimality
The occupied space manager described in the previous section computes the
contour of the occupied space in O(n log n). A simple algorithm that ﬁnds the
optimal point to place a new module v would compute the median and check
its feasibility; if the outcome is positive, we have found the optimum. Other-
wise, we need to check communication cost for all other possible local minima,

Temporal Placement
179
i.e., for every vertex of the contour and every intersection point of the contour
with one of the median axes. Let L denote this set of points. Computing the
communication cost for a single point takes O(n), so evaluating all objective
values in a brute-force manner would take O(n2) time. However, by means of
two more plane sweeps, we can achieve a complexity of O(n log n).
For this purpose, we observed that communication cost for the x- and y-
coordinate of the contour segments can be computed separately, then add the
precomputed values for every point of L. The crucial step is to use the fact that
we only need to compute the communication cost for the leftmost x-coordinate
and for the bottommost y-coordinate; the other values can be obtained by doing
appropriate fast updates during the plane sweep.
5.
Conclusion
This chapter has provided some of the most important approaches for tem-
poral placements. While very solid mathematical backgrounds were developed
and simulated, a physical investigation on reconﬁgurable chips has failed so
far. This is mainly due to the lack of devices providing partial reconﬁguration
facilities, a precondition for the implementation of temporal placement algo-
rithms. One and almost the only device type to provide partial reconﬁguration
is very limited and does not allows for evaluating the large set of available al-
gorithms. Besides the difﬁculty to designed for partial reconﬁguration, many
restriction are done on the use of the device resources during and after the
reconﬁguration. Some hope were placed on coarse-grained reconﬁgurable de-
vices. However, they have failed so far to take-off.

Chapter 6
ONLINE COMMUNICATION
Several methods were presented in the last chapter for the placement of
components at run-time on a reconﬁgurable device. The last two ones use the
communication costs between a component to be placed and its environments,
i.e. the set of modules on the chip and out of the chip that exchange data with
this component, as a mean to select the best position for the new component.
While this communication costs, deﬁned as the average minimum distance of
the new component to its neighbour is useful to guide the placement process,
it does not tell us how data should be exchanged among the different compo-
nents on the chip. In this chapter, we provide some answers to this question by
presenting some approaches to enable the communication at run-time between
modules on the chip. The approaches can be classiﬁed on different categories,
depending on the way the communication is realized. We can cite the direct in-
terconnection, the communication over third party, bus-based communication,
the circuit-switching approach and network on chip-oriented communication.
1.
Direct Communication
Direct communication paradigm allows modules placed on the chip to com-
municate using dedicated physical channels, conﬁgured at compile-time. The
conﬁguration of the channels remains until the next full reconﬁguration of the
device. A conﬁguration deﬁnes the set of physical lines to be used, their di-
rection, their bandwidth and speed as well as the terminal, i.e the components
that are connected by the lines. Components must be designed and placed on
the device in such a way that their ports can be connected to the predeﬁned
terminals. Feedthrough channels must also be available in each component to
allow signal used by modules aside the component to cross the components.

182
Reconﬁgurable Computing
Figure 6.1.
Direct communication between placed modules on a reconﬁgurable device
Example 6.1 The conﬁguration of ﬁgure 6.1 provides an example of 1-D
communication using direct lines. For this purpose, a set of 10 predeﬁne chan-
nels is ﬁxed and the component must adapt their position and direction to make
use of the channels. The physical channels 1, 5, 6 and 7 are not used. Line 3
is used by module C2 for connection with the device pins on the left and right
sides. It is fed through component C1 that provides the necessary channel for
the signal to cross. Line 4 is used by the two components C1 and C2 for a
direct communication. Additionally, the two components use the same line to
access the device pins. Lines 8 and 10 are used by component C5 to access
the device pins on the left and right sides. They cross components C3 and
C4. Line 9 is used to connect the components C3 and C4 and runs through
component C5.
The main disadvantage of this approach is the restriction imposed on the de-
sign of components. For each component, dedicated channels must be foreseen
to allow signals that are not used by this component in its placement location to
cross. This increases the amount of resources needed to implement the compo-
nent. Also, the placement algorithm must deal with additional restrictions like
the availability of signals in a given location. This increases its complexity and
makes the approach only possible for an ofﬂine temporal placement, where all
the conﬁgurations can be deﬁned and implemented at compile-time.
2.
Communication Over Third Party
Communication over third party is used for example by Brebner in [37] and
Walder et al. in [211] [196] [212]. A central component exists that behaves as
a message reﬂector. Each message is ﬁrst sent to the central module, which for-
wards it to the destination. Brebner in [37] uses this approach to allow the com-
munication between a reconﬁgurable module connected to a processor through

Online Communication
183
a bus and a user program running on the host processor. The module inputs and
outputs are controlled by registers that are mapped into the address space of the
processor. This approach can be used not only to allow the communication be-
tween a reconﬁgurable module and a user program but also between several
reconﬁgurable modules connected together through a bus. The system is con-
trolled by an operating system, whose role is to manage the device resources,
control the reconﬁguration process and allow the communication to happen
between the components temporally placed on, or removed from the device.
All modules willing to send a message must ﬁrst copy those messages in their
sending register. Thereafter, the operating system copies the message from
those registers to the input register of the destination module. In [211] [212],
the central module is not an operating system running on a separate processor,
but a set of ﬁxed resources on the device. It provides dedicated channels to
access peripheral devices and also to connect direct neighbour modules. The
communication between non-adjacent modules is done through ﬁxed resources
implemented on the device.
3.
Bus-based Communication
The communication between the reconﬁgurable modules on a given device
can also be done using a common bus. To avoid the bus resources to be de-
stroyed at run-time by components dynamically placed on the device, prede-
ﬁned slots must be available, where the modules can be placed at run-time,
such that no alteration of the bus is possible. On the predeﬁned slots, con-
nection ports must be available to dynamically attach the placed component
to the bus. While the predeﬁnition of locations where to place components
allows a simplicity of the placement algorithms, it is not ﬂexible at all. Using
a common bus reduces the amount of resources needed in the system because
only one medium is required for all component. However, the additional delay
increased by the bus arbitration can drastically affect the performance of the
system. The approaches of Walder et al. in [211] [212] as well as that of Breb-
ner are both based on restricted bus in which no arbitration module is required
to manage the bus access.
4.
Circuit Switching
Introduced in the 1980s under the name reconﬁgurable massively parallel
computers [146], [205], circuit switching is the art of dynamically establishing
a connection between two processing elements (PE) at run-time, using a set
of physical lines connected by switches. The system consists of a set of pro-
cessing elements arranged in a mesh. Switches are available at the column and
line intersection to allow a longer connection using the vertical and horizontal
lines at an intersection point. In this way, two arbitrary processing elements

184
Reconﬁgurable Computing
can be dynamically connected at run-time by dynamically setting the switches
on the path from the ﬁrst processor to the second one. Once the connection is
established, the data are transmitted from the source PE to the destination in
just one clock.
Circuit routing was experienced in several systems such as the YUPPIE
[146] in the 1980s. It is available today in many systems such as the PACT-
XPP device [176]. Also the connection mechanism in FPGA follows the same
paradigm. Although dedicated lines exist in some devices to allow connection
between remote modules on the device, longer connections are normally estab-
lished using the switch matrices to connect segmented lines in other to realize
the long communication path.
Despite its application in ﬁne-grained parallel image computing systems,
where it helped to dynamically change the topology of a parallel computer
to accommodate the best computation structure of an application under com-
putation, using the circuit switching in reconﬁgurable devices presents some
drawbacks:
Long communication delay: This will happen, if the connection between
two processing elements has to go through many other processors. The
number of switches on the communication path therefore increases, thus
increasing communication delay and therefore slowing down the clock.
Dynamic computation of routes: In dynamically changing environment,
the communication need between components changes with the placement
of components. Computing new routes at run-time is very time-consuming,
and therefore, the performance of the overall computation may be drasti-
cally affected by the run-time routing.
Exclusive use of chip space: Because of the complexity of the synthesis al-
gorithm, components that will be downloaded later onto the reconﬁgurable
device are implemented at compile-time. The synthesis process constrains
a module on a given area of the device, where it uses all the resources,
i.e. the processing elements and the interconnects in that area. This has
the consequence that the placement of a module at run-time in a region,
where a route is running, will destroy the route, because the connection
used by the route will be assigned to the component. To avoid this, we
can place components only at locations where no route will destroy them,
a restriction that will however increase the chip fragmentation. Consider
the placement of ﬁgure 6.2 with three routes to connect PE1 to PE2, PE3 to
PE4 and PE5 to PE6. For a new component that needs to use four consecu-
tive processing elements in a quadratic surface, the region where the routes
are implemented must be prohibited. No region that can accommodate the
new component is therefore left on the device, and the component must be
rejected, although enough resources are available on the device.

Online Communication
185
Figure 6.2.
Drawback of circuit switching in temporal placement Placing a component using
4 PEs will not be possible, although enough free resources are available
The previous example has shown how circuit switching might increase the
device fragmentation in devices allowing a 2-D placement. In device allowing
only a 1-D placement like it is the case of Xilinx VirtexII FPGAs, which are
only column wise reconﬁgurable, circuit switching can be used to connect a
few number of modules, usually 2–8, and allow dynamic communication to be
established between the components running onto the device at run-time.
4.1
1-D Circuit Switching: The RMB
In [4] [32], a 1-D circuit switching model called the reconﬁgurable multi-
ple bus (RMB), well suited for the column-wise reconﬁgurable Xilinx Virtex
FPGA is designed and implemented. The approach is based on a concept pre-
viously introduced in [204] and [77].
As ﬁgure 6.3 shows, the communication structure is made upon a set of
switches, locally attached to a component using vertical lines. The connection
between the switches is done through a bus, which consists of a set of seg-
mented horizontal lines. The bandwidth of the horizontal connection may vary
according to the application to be implemented. The function of the switches
is to allow the component on which they are connected to dynamically access
the bus and establish communication with other components.
The request for a new connection is done in a wormhole fashion. A sender
components Pk, located in position k and willing to establish a communication
with a receiver component Pt located in position t, must ﬁrst send a request for
communication to its own switch at location k. Switch k forwards the re-
quest to switch k + 1 in the direction of the receiver, which in turn forwards
the request to switch k + 2, until the request arrives at the switch t. Each
switch on the path from the sender to the receiver checks if the request for a

186
Reconﬁgurable Computing
Figure 6.3.
The RMBoC architecture
communication line can be satisﬁed, i.e. if a free communication channel is
still available on the switch. If this is not the case, then the request is either
queued or rejected. Otherwise, the switch sets the connection and sends an
acknowledge. The ﬁrst acknowledge is generated by the last switch on a path
(in this case switch t) and sent back to switch t −1, which in turn send the
acknowledgement to switch t −2, the process is repeated until the switch at
the sender location. Upon receiving the acknowledgement, the sender can start
with the communication. This approach was ﬁrst presented in [204] and ex-
tended later in [77] with a compaction mechanisms for a quick ﬁnding of a free
segment. A redesign as well as an implementation of this approach, called the
RMBoC (RMB on Chip) for the Xilinx FPGA devices was presented in [4] and
[32].
In of the RMBoC on the Xilinx Virtex FPGAs, the switch controllers are
separated from the processor. In this way, a uniform interface is provided to
designers for connecting their modules on the Bus.
To facilitate the partial reconﬁguration on Xilinx FPGA devices, the device
is segmented in a set of horizontal slots, each of which can accommodate a
given module at run-time. The reconﬁgurable modules can then be placed
only on the foreseen slots. Large modules that cannot ﬁt on one slot can be
implemented using a given amount of consecutive slots. The segmentation in
slots is enforced using dedicated modules, the so-called bus macros (ﬁgure
6.4), at the boundaries of the slots. A bus macro is a special hardware module
that allows established connections between neighbour component not to be
destroyed during the reconﬁguration process. The crosspoint locally manages
the connection of the component to the Bus as well as the setting of switches
according to the requests and the availability of the channels.

Online Communication
187
Figure 6.4.
RMBoC FPGA implementation

		
		
		


	




		







Figure 6.5.
Crosspoint architecture
As depicted in ﬁgure 6.5, the crosspoint used in the RMBoC consists of
three principal components.
Controller: The controller manages the switch at a local level according
to requests from the left or the right crosspoint as well as that of the local
module. Four kinds of command (REQUEST, REPLY, CANCEL and DE-
STROY) can be locally processed on a single crosspoint, thus deﬁning the
interface between a processing node and its crosspoint.
A communication process starts by a REQUEST command from the sender
to its local corresponding crosspoint with the destination address. The com-
mand is successively sent to the next crosspoint in the destination direc-
tion until the receiver location or the earliest crosspoint with no free chan-
nel. Upon reaching the destination, a connection will be established and a
positive answer (REPLY) is sent back successively until the source. Each

188
Reconﬁgurable Computing
processing element that receives a REPLY sets its switch accordingly. If a
processor cannot establish a connection in the destination direction, a neg-
ative answer (CANCEL) is sent back and the connection will not be estab-
lished. Whenever a connection is successfully established, the communica-
tion is done on a point-to-point manner. At the end of the communication,
the sender sends a DESTROY command to its crosspoint that successively
passes it to the crosspoints involved in the communication. Each cross-
point frees the data channel after sending the DESTROY command to the
next crosspoint.
Data network: The function of data network is just to connect corre-
sponding data channels according to the conﬁgurations modiﬁed by the
controller. The original RMB foreseen that data must be transferred within
one clock cycle from source to destination, once the connection is estab-
lished. However, to avoid long delay and therefore slow clocks, a pipelined
communication is realized in the RMBoC, using registers between the slots.
FIFOs: The purpose of FIFOs is to provide buffer for commands coming
from different side of the crosspoint (left, right, local). The FIFO Selector
sends the command from the FIFOs of each side to the main FIFO. The
policy of arbitration in the FIFO Selector is Round-Robin in the following
order: left, right and local.
5.
Network on Chip
Many well-reputed authors [21] [117][60] have predicted that wiring mod-
ules on chip will not be a viable solution in the billion transistor chips in the
future. Instead, they proposed Network on Chip (NoC) as a good solution
to support communication on System on Chip in the future. NoCs encounter
many advantages (performance, structure and modularity) towards global sig-
nal wiring. A chip employing an NoC consists of a set of network clients such
as DSP, memory, peripheral controller, custom logic that communicate on a
packet base instead of using direct connection. As shown on ﬁgure 6.6, several
modules (network client) placed at ﬁxed locations on the chip can exchange
packets in the common network. This provides a very high ﬂexibility, because
no route has to be computed before allowing components to start communicat-
ing. Components just send packets, and they do not care on how the packets are
routed in the network. NoC is viewed as the ultimate solution to avoid prob-
lems that will arise because of the growing size of the chip. The Quicksilver
chip [89] can be seen as an example of NoC-based architecture.
A generic NoC architecture is characterized by the number of routers, each
of which is attached to processing elements in the array, the bandwidth of the
communication channels between the routers, the topology of the network and
the mechanism used for packet forwarding. The topology of the network is

Online Communication
189
Figure 6.6.
A Network on Chip on a 2-D Mesh
deﬁned through the arrangement of routers and processor on the device and the
way those processors are connected together. One of the most used topology
is the 2-D mesh network, because it naturally ﬁts the tile-based architecture on
the chip.
Compared to typical macro networks, a network on chip is by far more
resource limited. To minimize the implementation cost, the network should be
implemented with little area overhead. This is especially important for those
architectures composed of tiles with ﬁne-level granularity as it is the case in
FPGAs. Thus, instead of having huge memories (e.g. SRAM or DRAM) as
buffer space for the routers like in the macro network, only few registers are
available to be used as buffers for on-chip routers. This leads to a much simpler
model with little overhead compared with its macro network peer.
We next take a look on the design and implementation of major NoC com-
ponents, which are the routers and the processing elements.
5.1
The Router
The basic building blocks of an NoC are routers that consist of a given set
of components like buffers to temporarily store packets, a controller that deter-
mines how to forward the packet, usually according to the destination address.
The performance and the efﬁciency of the NoC depend on the underlying com-
munication infrastructure, which in turn depends on the performance (latency
and throughput) of the on-chip routers. Thus, the design of efﬁcient, high

190
Reconﬁgurable Computing
Figure 6.7.
Router Architecture
performance routers represents a critical issue for the success of the NoC ap-
proach. While several possibilities may exist to implement a given router, we
focus on a very simple and intuitive model consisting, as depicted in ﬁgure 6.7,
of the following main components:
1 Five input buffers, usually implemented as FIFOs, to temporally store mes-
sages coming from ﬁve directions left, right, north, south and local PE,
before forwarding. Each router willing to send a message in a given direc-
tion copies it into the FIFO of the neighbour router in the direction where
the message will be forwarded. The data are therefore placed on the data
lines and the control signals are used to perform the handshaking between
the two neighbour routers.
2 The controller, which is in charge of managing the dataﬂow inside the
router. It deﬁnes in which direction a message should be sent and sets
the signals in the router to allow this to happen.
3 Five Output Arbiters, each for one direction and the local PE. The output
arbiters manage the assignment of the message to output channels.
We now provide a much detailed description of the router components:

Online Communication
191
5.1.1
The FIFO
A ﬁrst-in, ﬁrst-out (FIFO) is a data storage to model a queue in such a way
that the data that come in ﬁrst will be read out ﬁrst. It consists of a set of
registers for storing the data. The registers are organized in a serial way that
allowed data to be shifted from one register to the next one. Data are pushed
(written) on one side and read from the other side. Figure 6.8 provides the
general architecture of a FIFO.
It consists of two counters to count the written and read registers. One write
enable signals allow data to be written into the registers and one read enable
signal to notify about the readiness of the data from the registers. Two signals
are also used as ﬂags to inform about the fullness and emptiness of the FIFO.
If full, then no more data can be written to the FIFO. An empty FIFO means
that no data are available in the FIFO registers. If a common clock is used for
reading and writing the data then the FIFO is said to be synchronous. Other-
wise, there will be two different clocks for reading and writing the data. In this
case, we said that the FIFO is asynchronous. The FIFO can be parameterized
with the data width (number of bits in a register) and FIFO depth (number of
registers in a FIFO).
5.1.2
Router Control
Each router is identiﬁed through its position in the network. This position
is determined by the (x,y)-coordinate of the processing element on which the
router is attached. Messages are sent in packets, each of which consists of the
address of destination router, control bits and the payload (data). A possible
format for the packet being sent in to the network is shown in ﬁgure 6.9
Figure 6.8.
A general FIFO Implementation

192
Reconﬁgurable Computing
Figure 6.9.
General format of a packet
The ﬁrst part is the packet address, which is used by the router control to
determine the direction where to send the packet. The controller has an address
decoder that decodes the address into (x,y) coordinate of destination router or
PE. In the simple case of XY-Routing as we will see later, a comparator is used
to compare the (x,y) coordinate of the destination PE to that of the router to
compute the direction (LOCAL, EAST, WEST, SOUTH, and NORTH) where
the packet will be sent. The packet is then sent out by writing in the input
FIFO of the corresponding neighbor FIFO if this is not full. If the FIFO in the
neighbour router is full, then the router can decide to take some action. It can
for instance block all incoming packets or send the packet in another direction
to decongest a given data line.
5.1.3
Output Arbiter
To increase performance in the router, input FIFOs in all directions must be
read concurrently if they are not empty, i.e. there is at least one packet in a
FIFO. After reading the packets in parallel, the control logic decides in which
directions the packets should be sent. The packets are then written into the
neighbour router in the chosen directions. The case where the control decides
to forward many packets in the same direction leads to a contention, because
only one output data line is available and many packets cannot be written at
the same time. For a contention free routing, an arbiter must be provided at
every output port. A simple arbiter may consists of a multiplexer and a Finite
State Machine, which will make the decision in a Round-Robin fashion. If the
data must be written on the local output line, the order can be EAST, WEST,
SOUTH, and NORTH as shown in ﬁgure 6.10.
The incoming packets from the EAST will be written before the one com-
ing from the WEST, which in turn is written before the packet coming from
SOUTH that is written before the one coming from NORTH. Incoming packet
from LOCAL is not considered here, because we assume the a packet coming
from a given direction will not be sent back in that direction. This assumption
is only a design decision that can be made or not. However, we can choose
to give the router the freedom to decide where to forward packets. In this
case, a packet can be returned in the same direction from where it was sent.
This capability may lead in some routing algorithm to a ping-pong game be-
tween two routers. However, it may also help to decongest some over-ﬁlled
channels.

Online Communication
193
Figure 6.10.
Arbiter to control the write access at output data lines
5.2
The Processing Element
A processing element can be either a processor core, a memory block, an
embedded programmable logic or any custom hardware block. Processing el-
ements are usually connected to the upper right router through an interface in
the network. Each processing element has a unique address, which is the same
address of router to which it is connected. Thus, any PE can be plugged into
the network if its design ﬁts into an available slot. To make this process easy,
a wrapper is usually used to decouple the network activities from the compu-
tation within a processing element. The wrapper controls all the transaction
on the network and provides to the component a simple interface to access the
network.
5.2.1
Wrapper Design
The PE port is connected through a wrapper to the router. The port width (or
channel width) determines the size of the packet (message) that can be moved
in the network through routers. For consistency, wrappers use the same port
width as the router port width. A wrapper consists of only one FIFO at in-
put port as shown in ﬁgure 6.11. The main functionality of the wrapper is to
provide an interface to the processing element or the module connected to the
network. Its logic includes decoding and encoding the received and sent pack-
ets, respectively. Incoming packets to the PE carries the address along with
the payload. This address is removed in the wrapper, and only the payload is
passed to the PE. In the same way, wrapper gets the payload from the PE, adds
the address of the destination PE to the payload and formats the packet before
giving it to the connected router. Always an incoming packet is written to the
FIFO in the wrapper, and an outgoing packet is written to the FIFO in router to

194
Reconﬁgurable Computing
Figure 6.11.
A general wrapper architecture
which the PE is connected. The wrapper reads the packet from the FIFO only
when there is a packet in that FIFO. In the implementation, the processing
element is instantiated as functional block within the wrapper.
5.3
NoC Design Constraints
During the design of an NoC, important decision must be taken according
to a goal seek by the user. Each design decision inﬂuences a given parameter
in the ﬁnal implementation. The area overhead, i.e. the amount of resources
that the ﬁnal design consumes as well as the latency, i.e. how long in average
it takes to route a packet to destination are the most important parameters that
guide the design process.
5.3.1
Area Overhead
When considering a NoC architecture, the area used by the routing hard-
ware is important. Costumers will probably not be interested in those design,
where the routing hardware occupies 90% of the device area and only 10%
is left for the implementation of the PEs. The area consumed by the routers
depends on the bandwidth requirements, i.e. the packet width, the buffer size
and the complexity of the control algorithm. The packet size determines the
width of connection between routers. This makes it directly proportional to
the amount of internal wire required. The buffer size determines the amount of
memory used for storing the packets within the router before forwarding. The
complexity of the routing algorithm determines how much additional resources
the router consumes.
5.3.2
Latency
The latency consists of the time needed to setup a route and the time need
to transfer the payload to destination. In circuit switching, route setup time

Online Communication
195
includes request and acknowledgment latency, but in packet routing, there is
no such set up time. Only the address ﬂit takes initial setup time to reach the
destination based on the routing algorithm, thereafter for every cycle, the data
ﬂit will be delivered to the destination in a deadlock free network. In deadlock-
free store-and-forward packet switching, each packet spends one cycle to pass
through each router. The total number of routers between source and destina-
tion will determine the latency. For example, if the source and destination are
placed at diagonally opposite corners of a 4 × 4 mesh, each packet being trans-
ferred will take 8 clock cycles to reach the destination. In destination, IP block
perspective, for every clock cycle a packet will be received. The latency com-
pletely depends on the chosen routing algorithm (adaptive or deterministic)
and the buffering method.
5.3.3
Performance Metrics
The performance of a network provides a mean to compare two different
implementations of the network. The two main means used are the latency and
the throughput.
Latency: This is the time a message needs from its source to its destination.
It is the difference between the time where the last packet of the message
arrives at destination and the time when the ﬁrst packet of the message is
output from the source.
Throughput: Often used in conjunction with latency. This is the maximum
trafﬁc a network can accept per unit of time, typically measured as bytes or
packets per node per cycle.
5.4
Routing Techniques
Routing is one of the most understood area in computer science. A large
variety of routing algorithms exist in the literature, each with its advantages
and drawback. We provide some of the most used class of routing algorithms
in this section.
5.4.1
Circuit Switching
Circuit switching was presented in Section 4. This approach allows a com-
munication path to be created from the source to the destination before
transmitting any data. The procedure starts by a routing probe traversing the
network and reserving links to transmit the data. This routing probe contains
the source and destination addresses. Once the routing probe reaches the des-
tination address, an acknowledgment is sent back to the source address, then
the data are transferred at the full bandwidth of the hardware. The circuit re-
mains operational until the end of data to be transmitted. The lock on the
links may be released once all the data have reached the destination by sending

196
Reconﬁgurable Computing
back another acknowledgment through the same route to the source. The dis-
advantage of this technique is the time required to establish a dedicated link
from source to destination. It can be advantageous when the time to set up the
path is minimal, compared with the transfer time of the messages, and when
long messages are present, especially continuous data streams.
5.4.2
Store-and-Forward
At each node, the packets are stored in memory and the routing informa-
tion examined to determine which output channel to direct the packet. This is
why the technique is referred to as store-and-forward (SAF). Additionally, by
transferring an entire packet at a time, the latency for a packet is the number of
routers through which the packet must travel multiplied by the time to transfer
the packet between the routers.
5.4.3
Virtual Cut-Through
Virtual cut-through (VCT) was conceptualized to address the deﬁciency in
SAF-based packet routing, which results from the buffering of messages at
each node along the communication path. As the routing information is car-
ried in the header of a packet, the packet should not be stored in the cur-
rent node’s memory if an output buffer is available. The packet simply cuts
through the router of the node to an available output channel. This alleviates
the need for an excessive amount of memory along the path of a message, but
enough memory has to be allocated if an output channel is not available. Of
course at high volumes of messages on the network, VCT will behave simi-
larly to SAF packet switching. The unit of message ﬂow control for VCT is
the packet [184].
5.4.4
Wormhole Routing
Wormhole routing was conceived to address the deﬁciency in VCT, that is,
if an output channel is not available, the packet must be stored in the current
node’s memory. Wormhole routing divides a message into smaller ﬂow-control
digits than packets, which are called ﬂits. Each message contains one header
ﬂit that carries the routing and control information and the remaining data for
the message are stored in data ﬂits. The header ﬂit always goes ﬁrst to allocate
a path for the data ﬂits. Thus, smaller memory requirements exist for each
node on the network (buffers ﬂits instead of packets). If an output channel
is available, the header ﬂit is routed and the remaining data ﬂits follow in a
pipeline style fashion. During any instance of a message traversing a network,
the ﬂits of a message will be located in multiple routers. This routing technique
got its name in that it looks like a worm traversing the network. While this
method beneﬁts from very low latency and buffer space, blocking and deadlock

Online Communication
197
problems can occur. Therefore, other techniques, such as virtual channels,
are needed. An advantage of virtual channels is the ability to share a single
physical channel.
5.4.5
Deadlock and Livelock
Deadlock is a situation that occurs when a packet is waiting for an event that
can never happen because of a circular dependence on resources. Livelock, on
the other hand, is a conﬁguration of the network in which packets continue to
move, but never reach their destination.
A routing algorithm should determine the optimal routing path to transport
packets from source to destination through the network. The optimality is
usually deﬁned according to the following parameters: high performance, low
overhead, Deadlock and livelock freeness, fault-tolerance and ﬂexibility.
In general, routing can be done in two ways: deterministic routing and adap-
tive routing.
5.5
Deterministic Routing
A deterministic routing algorithm provides a unique path from a source to
destination. XY-routing also called dimension ordering routing is a simple
deterministic routing algorithm, where messages are transmitted fully in each
dimension, beginning with the lowest dimension available. In a 2-D mesh
network, XY-routing ﬁrst routes packets along the X-axis. Once it reaches
the destination’s column, the packet is then routed along the Y-axis until the
destination’s line. Therefore, any packet moving in the Y-direction can never
return to the X-direction. The XY-routing algorithm routes the packets based
on the destination address, irrespective of the trafﬁc pattern on the link and
the link delay. The router compares its own address (Xrouter, Yrouter) to the
destination address of a packet. Packets are forwarded based on destination
address (Xdest, Ydest) as follows.
1 If Xrouter < Xdest, the packet ist forwarded in the east direction
2 If Xrouter > Xdest, the packet ist forwarded in the west direction
3 If Xrouter = Xdest and Yrouter > Ydest, the packet is sent to the south of
the current router
4 If Xrouter = Xdest and Yrouter < Ydest, the packet is sent to the north of
the current router
5 If Xrouter = Xdest and Yrouter = Ydest, the packet is sent to the local PE

198
Reconﬁgurable Computing
5.6
Adaptive Routing
In adaptive routing algorithms, the direction where to send an incoming
packet is not ﬁxed a priori. The routing algorithm may decides to use more
complex approaches to decide on the further direction of a packet. Adap-
tive algorithms are usually used to improve the performance in the presence
of localized trafﬁc or to provide fault-tolerance in the network. Packets are
not always routed along the shortest path. The longest routing path may be
preferred to the shortest one, if the longest path provides the best better fault
tolerance result or if deadlock and livelock problems can be avoided. An ex-
ample of adaptive routing is the Q-routing, an adaptive routing algorithm based
on Q-learning, which is a form of reinforcement learning [36]. It routes pack-
ets based on the learnt routing information from its neighbours. Initially, this
algorithm builds a routing table based on the delivery times (Q values) of the
packets to every router in the network. These delivery times are updated ev-
ery time a router forwards a packet for a particular destination, which changes
depending on the trafﬁc at a given time to the destination.
On the basis of this information, a router can choose an alternative route
when the queues are congested in the intermediate routers, thus resulting in
faster delivery compared to the XY-routing algorithm.
With reinforcement learning in general and Q-learning in particular, each
router is an autonomous structure that learns with the time the efﬁcient route
to all possible destinations. On communication request, the best router, i.e. the
router through which the fastest communication is possible (from the point of
view of that router) is selected and the packet is sent there. With this method,
all the routers need to learn the new structure of the network whenever a change
is done.
With a frequently changeable network, the router will spend most of its
time for learning the new network structure, thus decreasing the network per-
formance.
Adaptive routing algorithms are more complex than deterministic routing
techniques, which means that the amount of resources consumed by the router
that implements adaptive routing is much higher than the amount of resources
consumed by router implementing deterministic routing. Therefore, the com-
plexity of those algorithms does not qualify them to be used on a chip. The
XY-algorithm usually performs well in the practice and routes packets accord-
ing to the lower Manhattan distance.
Network on a chip presents a viable communication infrastructure for com-
munication among task dynamically placed on a reconﬁgurable device. How-
ever, it is still too inﬂexible to dynamically support the communication among
modules in a changing network. When placing a component on a reconﬁg-
urable device, we face two possibilities: First, the components ﬁts on one PE,
i.e. the resources needed by the component is no more than that available on

Online Communication
199
A
3
4
2
5
1
6
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
PE
Figure 6.12.
Implementation of a large reconﬁgurable module on a Network on Chip
a PE. In this case, we place the component on one PE and attach it to the cor-
responding router to allow a communication with other components. In the
second case, the component does not ﬁt on one PE, i.e. more than the amount
of resources offered by a PE is necessary to implement the component. In this
case, the component will be splitted in pieces, each of which can ﬁt on a PE.
The communication between the pieces placed on different PE will use the
router network for communication. In other words, the communication within
a component boundary must be done using packets that are sent and routed in
the network. This increases the complexity of the module, and more resources
are wasted than needed. This situation is best illustrated in the ﬁgure 6.12.
There, a large module needs 12 PEs to be implemented. It is splitted in parts,
each of which is implemented on one PE. The connection among the parts is
done using routers. A better implementation would connect all the PE using
direct wiring. This has no disadvantage, because the PEs are close to each
other, and therefore, long connections are avoided. In ﬁgure 6.12, the routers
inside the area of the components (1,2,3,4,5,6) become redundant.
To overcome this handicap, the concept of dynamic network on chip that we
next presented was developed. In the dynamic network on chip, the redundant
routers can be used as additional resource to implement an even bigger module.
6.
The Dynamic Network on Chip (DyNoC)
A Dynamic Network on Chip (DyNoC) [35] [32] is a Network on Chip
whose structure can be dynamically changed at run-time.
In a DyNoC, a
routers is a programmable element basically conﬁgured as router but that can
be conﬁgured at run-time to implement any function that can ﬁt on it. This
concept provides a mean to overcome the deﬁciency of network on chip as
communication paradigm in reconﬁgurable computing (ﬁgure 6.12).

200
Reconﬁgurable Computing
Whenever a module is placed at a given location where it uses the resources
of several PEs, the redundant routers within the module boundary can be used
as additional resources for the module. The placed module only needs one
router to access the network. With this, the routers in the area of a placed
component are no more accessible by other components in the network. Upon
completion, the module is removed and the network must be reactivated in the
area where the component where previously placed. This can be done quickly,
because the router are programmable components that can be quickly reset to
their basic conﬁguration, i.e. the routers.
With this, we have a network in which some parts may be deactivated at a
given period of time and reactivated in the future. In order for such a network
to efﬁciently operate, some prerequisites on the communication infrastructure
of the network must be fulﬁlled. We explain these next.
6.1
Communication infrastructure
The goal is to have a communication infrastructure in which the reachabil-
ity of packets is insured, independent on the changing topology which occurs
when components are placed and removed on the chip. In its basic state, the
communication infrastructure is a normal NoC. Processing elements access
the network through corresponding routers. Additionally, direct communica-
tion lines exist between neighbour PEs. In this way, the routers are only used
for communication between non-neighbour PEs. With this modiﬁcation, com-
ponent splitted across several PEs with router inbetween do not need the router
anymore for communication, because direct lines already exist to connect the
neighbour PEs. The direct lines can also be modiﬁed to connect two PEs not
directly neighbour and allow more ﬂexibility. The cost of realizing such an
hybrid architecture with a network on one side and direct routing on the other
side will be more than that of a normal NoC; however, the gain in ﬂexibility is
enormous.
Routers in those area where a component is placed become useless, and
therefore, they can be used to provide more resources (logic and memory) to
the component. The realization of such routers on the current programmable
logic device is too costly, thus making the realization of a DyNoC communi-
cation infrastructure only possible if the router are directly available as hard
macros on the device, as it is the case with the switch matrices, the embedded
multipliers or the block memories on FPGA devices. Whenever a component
is placed in a given region, only one router is necessary for this element to ac-
cess the network. We use without loss of generality the router attached to the
upper right PE of the module (ﬁgure 6.12).
Figure 6.13 shows a communication infrastructure as described below on a
reconﬁgurable device. Note that the arrangement of the router is done in such
a way that the device is surrounded by a ring of routers. This arrangement

Online Communication
201
Figure 6.13.
The communication infrastructure on a DyNoC
increases the ﬂexibility and, as we will see later, is a prerequisite for a accessi-
bility of placed components.
A further requirements on the structure of the network is that routers should
be able to notify their neighbours about their activity. This can be done using
an additional activation line that is set to one if the router is active, i.e. no
component is placed on top of that router and zero if a component is using
that router in its internal implementation. In this case, the activation line is
controlled by the component, which notiﬁes the routers around on the states of
the routers that it internally uses.
6.2
Network access
As stated earlier, we would like to have a network in which all component
are reachable, despite the dynamic modiﬁcation of the network. Let us ﬁrst
recall the way component are developed for partial reconﬁguration. Each task
that can be executed at run-time is implemented as a component, represented
by a rectangular box and stored in a database. The bounding box delimits the
area of resources used by that component. In fact, the component is available as
partial bitstream that will be downloaded onto the device at run-time, whenever
the corresponding task needs to be executed.
Those components are hardware modules that required to be synthesized
from a given description. As synthesis is a time-consuming task, it cannot be
done online. Therefore, the synthesis of components is done at compiled time.

202
Reconﬁgurable Computing
The result of the synthesis of a component is box that encapsulates the circuit
implemented with the resources in a given area (routers logic and PEs).
After the placement of a new component on the device at run-time, its coor-
dinate is set to that of its corresponding router. As the placement destroys some
part of the network, we must insure that packets sent to other components will
still reach their destination.
6.2.1
Reachability of Components and Pins
In this section, we deﬁne some conditions that must be fulﬁlled by a tem-
poral placement of components on the device to enforce the reachability of all
the components on the device an those of the pins around the device.
Definition 6.2 (reachability of component and pin) Given a re-
conﬁgurable device and a conﬁguration, i.e. a set of components actually run-
ning on the device, we said that a component (pin) on reconﬁgurable device
at a given time is reachable if each message sent to this component (pin) can
reach the component (pin).
Because the communications between components are established at run-
time and because the conﬁguration of the chip is not known in advance, we
must insure that all components and pins on the device are reachable at any
time during the temporal placement. This condition is fulﬁlled if at any time
the set of components and pins on the device is strongly connected.
Definition 6.3 (strongly connected configuration) Given a re-
conﬁgurable device, we said that a conﬁguration of the device is strongly con-
nected if for each pair of components A and B, a path of active routers that
connects the two components, exists on the device.
One way to enforce the strong connectedness in a DyNoC is to require that
each component placed on the chip must always be surrounded by a ring of
active routers. This can be reached either by synthesizing components in such
a way that when placed on the device, they are always surrounded by a ring of
routers. The second possibility is to let the job to a temporal placer. This will
considerably increase the complexity of the placer. Besides the computation
of free space to place a new component, it must insure that all placements are
strongly connected.
Theorem 6.4 If each component is synthesized in such a way that it is in-
ternally surrounded only by processing elements, then all placements on the
reconﬁgurable device are strongly connected.
Proof : Assume that a set of components developed as required in theorem
6.4 and placed on the device is not strongly connected. In this case, at least

Online Communication
203
one pair of components abuts or a component abut the device boundary. Let
us consider the ﬁrst case. The second one can be handled in a similar way.
Either the two components overlap or at least one component uses some routers
on its internal boundary (this is illustrated in ﬁgure 6.14) . The ﬁrst case is
impossible because only overlapping free placements are valid. The second
case contradicts our requirement of the theorem, thus completing the proof.
Example 6.5 Consider the placement of ﬁgure 6.14 with two abutting com-
ponents. The ﬁrst component attached to router A is not implemented accord-
ing to the guideline of theorem 6.4, because routers are used in its internal
boundary. The second component attached to router B is developed according
to the rule of theorem 6.4. No route is available between those two component,
because the only routers available are consumed by the ﬁrst component.
The conﬁguration of ﬁgure 6.15 allows the reachability of all components
and pins. All the components are implemented according to the guideline of
theorem 6.4. Therefore, no matter where a component is placed on the device,
it will always be surrounded by a ring of routers.
Figure 6.14.
A impossible placement scenario

204
Reconﬁgurable Computing
Figure 6.15.
A strongly connected conﬁguration on a DyNoC
While in a static NoC, each router always has four active neighbour routers,1
this is not always the case in the DyNoC presented here. Whenever a compo-
nent is placed on the device, it covers the routers in its area. As those routers
cannot be used, they are deactivate. The component therefore sets the activa-
tion signal to the neighbour routers to notify them not to send packets in its
direction. Upon completing its execution, the deactivated routers are set to
their default state. A routing algorithm used for common NoC cannot work
on the DyNoC, without modiﬁcation. An improvement or an extension of the
existing routing algorithms should be done in such a way that packets may be
able to surround the components on their way to the destinations.
6.3
Routing Packets
Because the dynamic placement and removal of modules on the chip creates
unpredictable obstacles in a DyNoC, adaptive routing algorithms must be used
to handle the dynamic changing conﬁguration of the network. The routing
algorithm must be fully local-decisive,2 deadlock free and live lock free. We
must insure that each packet sent in the network will reach its destination after
a ﬁnite number of steps. While very complex and intelligent algorithm may
be developed, a very simple strategy was proposed in [35] [32] to allow the
packets to surround the obstacle, whenever they encounter one.
1Routers around the chip does not have four neighbours. The package pins can be considered as further
neighbours where the routers can send a message. This leads to the availability of the four neighbour routers
everywhere on the chip.
2The decision where to send a packet is taken locally.

Online Communication
205
The strategy is based on the XY-routing, because of its simplicity, its efﬁ-
ciency and its deadlock freeness, the XY-routing algorithm presented in
Section 5.5 was adapted for the DyNoC. Recall that in a full mesh, the XY-
routing is a deadlock free, shortest path routing algorithm that ﬁrst routes pack-
ets in X-direction to the correct X-coordinate and then in the Y-direction un-
til the correct Y-coordinate. The adapted XY-routing, the S-XY-Routing (Sur-
rounding XY-routing) operates in three different modes:
The N-XY (Normal XY) mode. In this mode, the router behaves as a normal
XY router.
The SH-XY (Surround horizontal XY) mode. The router enter this mode
when its left neighbour or its right neighbour is deactivated.
The SA-XY (surround vertical XY) mode. The router enter this mode when
its upper neighbour or its lower neighbour is deactivated.
The normal mode is available when all the neighbours of a routers are active.
In this case, it is the XY-routing that is used.
6.3.1
The SH-XY mode: Surrounding Obstacles in the X-direction
Assume without loss of generality that a packet moving from right to left
is blocked by an obstacle. As shown in ﬁgure 6.16, there exist two alternative
paths for the packet to reach its destination.
According to the XY-routing, which is the base of this algorithm, the ﬁrst
path is chosen if the Y-coordinate of destination of the packet is greater or equal
than that of the router and the packet is sent upward. Otherwise, the second
path is chosen and the packet is sent downward. One problem occurs when a
packet with destination Ydest is sent for example upward and riches a router
r with coordinate Yr > Ydest. According to the previously deﬁned scheme,
the packet will be sent downwards to the router with coordinate Yr −1, which
will send it upward, thus producing a ‘ping-pong’ game. Figure 6.16 illustrates
this situation. In this case, the ping-pong game happens between routers 1 and
router 2.
To avoid this, a strategy consists of stamping the packet by setting a “stamp-
bit” to 1 to notify the next routers that the packet is willing to surround the
component, and it should not be sent back. In our example, the packet is
stamped by the router 1.
Upon reaching the router upper right of the module to be surrounded, the
stamp is removed and the packet is sent left, until its destination column or until
another obstacle is found. Because each component is always surrounded by a
ring of routers, this algorithm for surrounding a component in the X-direction
will always work and a packet will never be blocked. In the example of ﬁgure
6.16, path 1 will be chosen.

206
Reconﬁgurable Computing
Figure 6.16.
Obstacle avoidance in the horizontal direction
6.3.2
Surrounding Obstacles in the Y-direction
The situation is similar to the one where a packet moving in the X-direction
is blocked. Assume without loss of generality that a packet moving from top
to bottom is blocked by a placed component. Dealing with this case as the
previous one, the packet will be sent left or right. No preference is done here,
because the packet is already in its right column. Let us assume that the packet
is sent to next router right to the current one. Because the basic routing algo-
rithm is the XY-routing, the next router will ﬁrst compare the X position of
the packet with its own position. With the packet X-destination being smaller,
it will send the packet back to the router from which it was received. The
two routers will keep sending the same packet to each other, thus creating a
deadlock. Figure 6.17 in which a ‘ping-pong’ game will then happen between
router 2 and router 4 illustrates this situation.
As in the previous case, the packet is stamped to notify all the routers above
the obstacle that the packet is willing to surround the component.
In our
example, the packet will then be sent right until the last router (5) above the
component. There, the router removes the stamp and sends the packet down-
ward. From there on, we have the same situation as deﬁned in the previous
section (surrounding obstacles in the X-direction).

Online Communication
207
1
3
2
4
5
6
Obstacle
Component
Destination
Component
Routing
Path1
Routing
Path 2
Ping-Pong
Game
Figure 6.17.
Obstacle avoidance in the vertically direction
In the two cases, a path will be always available from the source to destina-
tion. With the XY-routing used as underlying routing technique, we conclude
that each packet sent will reach its destination in a ﬁnite number of steps.
Theorem 6.6 With very high probability, the S-XY algorithm presented here
is deadlock free and livelock free.
Proof : We need to ﬁrst prove that there is always a path from packet to desti-
nation. With this, the only possibility to have a livelock is that the packet moves
permanently through the network and never reaches its destination, despite the
availability of a path.
Second, we must prove that each packet will reach its destination after a ﬁ-
nite number of steps. The ﬁrst requirement is guaranteed through theorem 6.4.
We now assume that a packet never reaches its destination. This will hap-
pen only if the packet is blocked or if the packet is looping in a given region.
Because a path always exists from one active router to all other active routers,
no packet can be blocked in the network, i.e. a packet is looping. As this sit-
uation is not possible in the normal XY, it can only arise in the surrounding
phase. When a packet is blocked in a given direction, it takes the perpendicu-
lar direction. This is done until the last router on the component boundary that
is at one corner of the module to be surrounded. From there on, the normal

208
Reconﬁgurable Computing
XY-routing resumes. A looping of a packet around a component is therefore
not possible.
It may be possible to construct a placement sequence of components that
blocks a single packet whenever that packet moves in a given direction, forcing
it to take the perpendicular direction. Such a sequence may lead to a packet that
will loop in a given region. However, doing this will only block a single packet,
and other packets will move to their destination as previously described. This
leas to a very low livelocking because only one packet out of an inﬁnite number
is livelocking. However, such placement sequences are very unlikely to happen
in the reality.
In the S-XY routing, the direction where to send a packet whenever an ob-
stacle is encountered is ﬁxed for all routers a priori. In this particular case,
packets are always sent to the right, whenever they are blocked in the verti-
cal direction. This may lead to extreme long routing paths like that of ﬁgure
ﬁgure 6.18. Those extreme paths are not usual, but may occur as result of some
placements.
6.3.3
Router Guiding
A simple and efﬁcient strategy to avoid such path consists of instructing
the router around a component on the direction where a packet must be sent,
Figure 6.18.
Placement that cause an extreme long routing path

Online Communication
209
00
00
01
01
01
01
01
00
00
00
00
01
01
01
01
01
00
00
Figure 6.19.
Router guiding in a DyNoC
whenever the incoming packet is blocked by the component in a given direc-
tion. Only one line is necessary to realize this concept.
Figure 6.19 illustrates this approach that we call the router guiding. Be-
sides the activation line, which provides information on the state of a router
(1 = activate, 0 = deactivated), an additional line is used for instructing the router
on the direction where they should send packet to, in order for them to surround
the component. A value of 0 (= east or south) will mean that a packet blocked
in the vertical direction must be forwarded east and a packet blocked in the
horizontal direction must be forwarded south. A value of 1(= west or north)
means that a packet blocked in the vertical direction must be forwarded west,
while a packet blocked in the horizontal direction must be forwarded north.
The best direction where to send packets whenever they are blocked by a
component can easily be determined by the component shape, a process that
can be managed at compile-time.
This approach considerably limits the complexity of the routers, and there
is no need for stamping anymore. We call this modiﬁcation the router guiding
because the routers are guided by the components.
6.4
Analysis of Efﬁciency
We deﬁne the communication latency of a packet as the number of routers
on a path from its source to its destination. On a reconﬁgurable device with
width W and height H, the maximum latency is given by W + H. However,

210
Reconﬁgurable Computing
the unpredictable structure of the network that changes with the placement of
components creates unpredictable delays. The worst case delay Dwc(p) of a
packet going through k obstacles from its source to its destination is given by
equation
Dwc(p) = H + W +
k

i=0
Di
wc(p)
(6.1)
Di
wc(p) is the worst case additional delay caused by the obstacle i. If wi
is the weight of the components i and hi its height, then we have Di
wc(p) =
max(wi, hi). The half perimeter of the component is not added here, because
it is already in the sum H + W. Only the maximum between the components
height and its width is considered. This additional delay is unpredictable and
depends on the temporal placement of components on the device.
In a static NoC where each processing element is assigned a ﬁxed place, the
delay is also ﬁxed. Packet communication can be seen as pipelined compu-
tation in which the length of the pipeline is the packet delay from source to
destination. Once the pipeline if ﬁlled, one packet is received at the destination
on each clock.
In the DyNoC, we have the same situation; however, delays created by
newly placed components create new situations for which the path must ﬁrst be
ﬁlled with packets. It is even possible that some packets sent later reach their
destination before others, which were sent earlier, but which had to go through
some obstacles. This makes the problem difﬁcult for a formal analysis.
6.5
DyNoC Implementation on FPGAs
DyNoCs with different size and width were prototyped on two FPGAs [35]
[32], the VirtexII-1000 and VirtexII-6000 from Xilinx. While the prototype on
the VirtexII-6000 were mostly for statistical (area, latency) purpose, the im-
plementation on the VirtexII-1000 was done on the RC200 FPGA-board from
Celoxica [43]. The result is given in table 6.1 in terms of area (A) occupation
for different bit-widths, memory (M) usage and speed (S) in MHz.
Two video applications with a VGA controller running at 25 Mhz for normal
640 × 480 VGA were implemented. In the ﬁrst one, a colour generator module
Table 6.1.
Router Statistics
VirtexII-1000
VirtexII-6000
A/M/S(8 bit)
8% /4% / 77.2
1% /0% / 77.2
A/M/S(16 bit)
12% 7% / 75.4
2% /1% / 75.4
A/M/S(32 bit)
21% / 12% / 77.3
3% /2% / 74.9
A/M/S(64 bit)
46% / 28% / 70.1
7% /4% / 73.7

Online Communication
211
(CG) communicates with the VGA controller (VC). The colour generator gets
the X and Y coordinates of the current pixel position from the VGA module,
computes the colour to be placed at that position and sent it back to the VGA
module that displays the colour at the corresponding position. The CG appli-
cation is a nice method to detect changes in the communication, because this
will directly have a visual effect on the screen. The X and Y positions are
each 12 bits wide and the colour is 24 bits wide. Therefore, packets with 32
bits width in each direction were used. Implemented on the RC200 board with
the DyNoC, no change was detected in the displayed pattern, event with a full
network trafﬁc because of the communication among remaining routers.
The second application is the implementation of a trafﬁc light controller
(TLC) containing three modules. A VGA controller (VGA), a Trafﬁc light vi-
sual module (LV) and a trafﬁc control module (TC) to capture the pedestrians
wishes. As in the ﬁrst case, the VGA module is used to display the state of a
trafﬁc intersection on which the light and the button used by the pedestrians can
be seen. The trafﬁc visual module is in charge of building the trafﬁc light in-
frastructure, which is then displayed by the VGA module. The VGA sends the
X and Y pixel scan positions to the trafﬁc visual module and receive a colour to
be displayed. According to the pixel positions, the trafﬁc light visual computes
the pattern to be placed at that position. This generates the trafﬁc light infras-
tructure. The last module is a FSM that monitors the pedestrian inputs (two
push buttons on the board) and sent a message for the transition of state of the
trafﬁc infrastructure to the trafﬁc light visual, which in turn generates the corre-
sponding colour to be seen. The trafﬁc light controller was successfully imple-
mented on 3 × 3 DyNoC. The router at position (2,2) were disable to enforce a
surrounding. All the remaining routers keep communicating with each other to
keep the trafﬁc high in the network. Also here the application runs as it uses to
do without any interruption and without any malfunction. Table 6.2 provides
some statistics on the implementation on VirtexII-1000 and VirtexII-6000. The
implementation of the TLC on a 3 × 3 DyNoC is shown in ﬁgure 6.20.
As we can see, the size of the routers are very large, making the used of
NoCs very difﬁcult on FPGAs.
This is somehow because the routing re-
sources in the router are built on top the programmable resources available
(LUT and routing matrices) in an FPGA. A direct access to the available
resources in the FPGA will reduce the amount of the resources need to a
Table 6.2.
TLC and CG Statistics
VirtexII-1000
VirtexII-6000
A/M/S(TLC)
53% /32% / 100.2
8% / 5% / 100.2
A/M/S(CG)
47% 33% / 91.2
20% /11% / 121.9

212
Reconﬁgurable Computing
Figure 6.20.
DyNoC implementation of a trafﬁc light controller on a VirtexII-1000
minimum. However, this can be done only if the vendors provide the necessary
knowledge to control the resources on a very low level.
7.
Conclusion
In this chapter, we have addressed the communication paradigm needed to
allow components dynamically placed at run-time on a reconﬁgurable devices
to communicate with their environment. The most promising approach is the
Network on Chip, which has attracted a lot of attention in the research com-
munity in the last decade. A large amount of work is available, each with its
own level of complexity. The DyNoC is an extension of the NoC paradigm.
In our opinion, it provides the best NoC-based approaches to solve the dy-
namic need of communication in temporal placement. We have also presented
a circuit switching approach and show why it is not a good solution in tem-
poral placement. However, the 1-D RMB circuit switching approaches can be
used to connect a small number of components in predeﬁned location on a 1-D
reconﬁgurable device.

Chapter 7
PARTIAL RECONFIGURATION DESIGN ∗
Partial reconﬁguration is one the prerequisite in reconﬁgurable computing,
as it allows for swapping modules into and out of the devices without having
to reset the complete device for a total reconﬁguration. As we will see later in
chapter applications, the possibilities offered by the partial reconﬁgurable de-
vices in the implementation of system adaptivity are enormous. Unfortunately,
very few devices on the market have the capability to be partially reconﬁgured
to replace some modules, whereas the rest of the design keeps working. One
of the few devices falling in the category of partial reconﬁgurable chips are the
FPGAs from Xilinx, in particular those from the Virtex class.
Designing for partial reconﬁguration on Xilinx device is a complex exer-
cise that requires to be well prepared, technically, but also psychologically.
This chapter is much like a tutorial for designing partial reconﬁgurable appli-
cations on the Xilinx Virtex FPGA-devices. Our goal in this chapter is to guide
the designer in the design process for partial reconﬁguration by providing the
essential steps and materials in order for him to avoid long search and long
introduction phases, which sometimes leads to a possible discouragement.
We therefore explained the three design ﬂows used for the partial reconﬁg-
uration on Xilinx devices: The JBits approach, the Modular Design Flow and
the Early Access Design Flow. Although the JBits approach apply for very
few old Virtex FPGA, the modular design and the early access design ﬂows
support newer devices. We therefore explain those two design ﬂows on the
basis of working examples, whose sources can be downloaded from the book
home page.
∗This chapter and the corresponding Appendix were prepared by Christophe Bobda and Dominik Murr.

214
Reconﬁgurable Computing
The development of partial reconﬁgurable applications on the Xilinx plat-
form is done according to the Modular Design Flow provided by Xilinx in
[227]. The modular design ﬂow provides a guided ﬂow on how to generate the
constraints required by the placement and conﬁguration tools for the genera-
tion of full and partial bitstreams representing modules to be downloaded at
run-time into the device for a partial or for a full conﬁguration.
Several tutorials for partial reconﬁguration on the Xilinx devices exist to be
downloaded from the internet. Despite their usefulness, almost all of them tar-
get only VHDL and Verilog designs. With the increasing interest in software-
like hardware design language such as SystemC and Handel-C, the need for
incorporating those languages in partial reconﬁguration design ﬂows is high.
We therefore describe in this chapter how to incorporate Handel-C designs in
a partial reconﬁguration design ﬂow.
Finally, to efﬁciently use the capability of partial reconﬁgurable chips, the
platform that hosts the chip, i.e. the board, must be designed in such a way
that the use of the ﬂexibility provided by the reconﬁgurable chip can be max-
imized. This issue is addressed at the end of the chapter where we provide
some guidelines in the design of partial reconﬁgurable platforms. As example
of such platforms, we present the Erlangen Slot Machine, which was designed
to allow the maximal use of partial reconﬁguration.
1.
Partial Reconﬁguration on Virtex Devices
Virtex FPGAs like other FPGAs from Xilinx are organized as a two-dimen-
sional array of CLBs containing a certain amount of logic. They are conﬁgured
with conﬁguration data called a bitstream, which can be downloaded into the
device via the conﬁguration port.
The idea behind partial reconﬁguration is to realize reconﬁguration by only
making the changes needed to bring the device in the desired state. Fragments
of the complete bitstream, also called packets, are sent to the device to re-
conﬁgure the needed part of the device. A copy of the last conﬁguration is
maintained in a dedicated part of the processor memory, called the conﬁgura-
tion memory. Partial reconﬁguration is done by synchronization between the
conﬁguration memory and the device. Changes made between the last con-
ﬁguration and the present one is marked as dirty packets that are then sent to
the device for partial reconﬁguration. A packet is a sequence of (command,
data) pairs of varying length that are used to read or write internal registers and
conﬁguration state data.
Virtex devices are organized in frames or column-slices that are the smallest
unit of reconﬁguration. Partial reconﬁguration can then be performed by just
downloading the conﬁguration data needed to modify the part of the device
needed. Up to the Virtex II Pro, a frame used to span a complete column,
with the consequence that the reconﬁguration of one component affects all the
components, which share a common column with that component. From the
Virtex 4 and upwards, the frames do not span a complete column any more,

Partial reconﬁguration design
215
but only a complete tile. Therefore, the reconﬁguration of a component affects
only those components in the same blocks, which share a common column
with the reconﬁgurable module.
It is then the matter of the partial reconﬁguration design ﬂow to produce the
partial bitstream, i.e. the set of data required to conﬁgure the needed frames
and therefore to allow the replacement of a component on the chip. The ex-
tracted partial bitstreams are then used to move the device from one conﬁgura-
tion state to the next one without reconﬁguring the whole device.
Two possibilities exist to extract the partial bitstream representing a given
module: A constructive approach that allows us to ﬁrst implement any single
component separately using common development tools. The so-developed
components are then constrained to be placed at a given location using bound-
ing box and position constraints. The complete bitstream is ﬁnally built as the
sum of all partial bitstreams. This approach is the one followed by the mod-
ular design and the early access design ﬂows. The second possibility consists
of ﬁrst implementing the complete bitstreams separately. The ﬁx parts as well
as the reconﬁgurable parts are implemented with components constrained at
the same location in all the bitstreams, which differs from each other only on
the reconﬁgurable parts. The difference of two bitstreams is then computed
to obtain the partial bitstream needed to move from one conﬁguration to the
next one.
Example 7.1 Consider for instance the two designs of ﬁgure 7.1, where the
full designs are named Top1 and Top2. The ﬁx part consists of the module
Figure 7.1.
Generation of bitstreams for partial reconﬁguration

216
Reconﬁgurable Computing
F-Module 1, F-Module 2, and F-Module 3, which are placed at the same lo-
cation in the two designs. In Top1 , the reconﬁgurable module is R-Module 1,
which is placed at the same location, with the same bounding box constraints
as the reconﬁgurable module R-Module 2 in Top2.
The addition of R-Module 2 to the ﬁx parts of the two bitstreams generate
the full bitstream Top2. The same is true for R-Module 2 and Top1. Also,
as shown in the ﬁgure, adding the two bitstreams Top1 and R-Module 2 will
produce the bitstream Top2, because R-Module 2 will replace R-Module 1 in
Top1. The substraction of Top2 from Top1 produce the partial bitstream for
R-Module 1.
2.
Bitstream Manipulation with JBits
JBits [102] is an application interface developed by Xilinx to allow the end
user to set the content of the LUT and make connections inside the FPGA
without the need for other CAD tools. JBits does permit the changes to be
made directly to the device, but to the conﬁguration data.
The JBits API is constructed from a set of java classes, methods and tools
that can be used to set the LUT-values as well as the interconnections. This is
all what is required to implement a function in an FPGA. JBits also provides
functions to read back the content of a FPGA currently in use.
The content of an LUT can be set in JBits using the set function:
set(row, col, SliceNumber Type, value)
which means that the content of the LUT Type (Type is either the LUT F or
the LUT G of the corresponding CLB) in the slice SliceNumber (SliceNumber
can be 0 or 1) of the CLB in position row and col should be set to the value of
the array value. The Virtex LUTs have four inputs and 16 entries representing
the 16 possible values a 4-input function can take.
A connection is deﬁned using the function connect:
connect(outpin, inpin)
This function uses the JBits routing capabilities to connect the pin outpin to
the pin inpin anywhere inside the FPGA. outpin and inpin should be one of the
CLB terminals.
Connecting the output of an LUT to a CLB terminal can be done with the
function.
set(row, col, terminal control, LUT output)
where terminal control sets the correct terminal to be connected to the out-
put LUT output of the corresponding LUT. JBits provides hundreds of prede-
ﬁne cores such as adders, subtractors, multipliers, CORDIC Processor, encoder

Partial reconﬁguration design
217
and decoders, network modules, which can directly be used in designs. They
can also be combined to generate more complex cores.
Although any function can be implemented entirely in JBits using the basic
primitives previously described as well as the predeﬁned library modules, it
is usually not the best approach, because of the high complexity in building
components from scratch. Instead, the bitstream substraction described in the
previous section is used to extract the part of the bitstream needed at run-time
for partial reconﬁguration.
The approaches used to extract partial bitstream of the component that will
be replaced at run-time, consist of implementing as many full bitstream, also
called top-level, as required and perform the substraction among them to ex-
tract the differences. Those are the parts that will be used to conﬁgure the
device later. Implementing the substraction using JBits is straightforward. The
two bitstreams need to be scanned and compared for each element. Only the
difference is copied in the resulting bitstream.
3.
The Modular Design Flow
One of the main drawback of JBits is the difﬁculty to provide ﬁxed routing
channels for a direct connection between two modules. This is important be-
cause one must ensure that signals will not be routed on the wrong paths after
the reconﬁguration. Consider for instance the two full designs of ﬁgure 7.2 in
which the ﬁxed as well as the reconﬁgurable components are placed at the same
locations. The connection F-Module 1 ↔R-Module 1 is running on a different
path than the connection F-Module 1 ↔R-Module 2. On reconﬁguration, the
design will probably not work correctly.
One of the main contribution in the modular design ﬂow [128] is the use of
the so-called Bus Macro primitive to guarantee ﬁxed communication channels
Figure 7.2.
Routing tools can route the same signals on different paths

218
Reconﬁgurable Computing
among components that will be reconﬁgured at run-time and the ﬁxed part of
the design.
The modular design ﬂow however was not initially developed to support par-
tial reconﬁguration. It is an approach that allows several engineers to coopera-
tively work on the same project. The project leader identiﬁes the components
of the whole project, estimates the amount of resources that will be consumed
by each component, deﬁnes the placement locations for the components on the
device and lets the single parts be developed by the different engineers. Fi-
nally, the single developed parts are integrated into the whole design to build
a workable circuit. The modular design ﬂow is a 4-step method (design en-
try and synthesis, initial budgeting, active implementation and assembly) for
which a viable directory structure must be used.
We recommend to use the ISE version 6.3, which is in our opinion the most
stable version for implementing partial reconﬁguration with the modular de-
sign ﬂow.
3.1
Directory Structure
Xilinx recommends a structure of design directories as shown in ﬁgure 7.3:
The HDL Design directory (HDL): This directory is used to store the HDL
descriptions of the modules and that of the top level designs. The HDL
description can be in VHDL, Verilog or any other hardware description
language.
Synthesis directory (synth): In this directory the team leader synthesizes
the top-level design, which includes the appropriate HDL ﬁle, the project
ﬁle, and project directories. In the same way, each team member will also
have a local synthesis directory for his or her synthesized module.
Modules directory (Modules): In this directory, the active implementation
of individual modules is done in the respective module’s subdirectory.
Implementation directory (Top): It includes two subdirectories, one for the
initial budgeting phase and one for the ﬁnal assembly phase. In the initial
directory, the team leader performs the initial budgeting operations for the
corresponding top-level design. In ﬁnal assembly directory, the team leader
assembles the top-level design from the individual modules implemented
and published in PIMs directory.
PIMs (Physically Implemented Modules) directory (PIMs): The active im-
plementation of individual module creates appropriate module directory in
PIMs directory where implemented module ﬁles are copied.
We next focus on the single steps of the modular design ﬂow:

Partial reconﬁguration design
219
Figure 7.3.
The recommended directory structure for the modular design ﬂow
3.2
Design Entry and Synthesis
This step must be carried out before going to the modular design imple-
mentation step. In this step, the members of the team develop modules using
a hardware description language (HDL) and synthesize them using synthesis
tools. The team leader completes top-level design entry and synthesis before
moving to the initial budgeting phase. Team members complete individual
module design and synthesis before moving to the active module implemen-
tation phase for that module. However, the team members can work on their
modules whereas the team leader is working on initial budgeting phase of mod-
ular design implementation. The restriction that must be done on the top-level
design are the following:
All global logic, including I/Os, must be included in the top-level descrip-
tion.
All design modules are instantiated as black boxes in the top-level design,
with only ports and port directions speciﬁed.
Signals that connect modules to each other and to I/O pins must be deﬁned
in the top-level design.
Top-level design and individual modules are synthesized as described in the
documentation of the synthesis tool selected. In synthesis properties, the insert

220
Reconﬁgurable Computing
I/O pad settings must be enabled for top-level design and disabled for each
submodule. Separate unique netlists will be created for each of the modules
and for the top-level design. For each instantiated module, exactly the same
module name must be used as its ﬁle name. Some synthesis tools such as the
Xilinx Xst will not be able to match the module names speciﬁed in the top-level
netlist to the module netlists.
3.3
Initial Budgeting
In this stage, the team leader assigns top-level constraints (pin locations,
area constraints for each modules, timing constraints, etc.) to the top-level
design, which results in topX.ucf ﬁle (user constrain ﬁle). The same topX.ucf
ﬁle is used for the active module implementation of individual modules.
Only the netlist for the top-level design ﬁle (topX.ngc, if compiled with
Xilinx Xst) and the constraint ﬁle (topX.ucf) are copied to the initial directory
in implementation directory of the corresponding top-level design.
The same top-level design name should be used for both ﬁles. The following
command is then ﬁrst executed
ngdbuild -modular initial design name
In this case design name is top.ngc. In this step, the compiler ngdbuild pro-
duces two ﬁles, design name.ngd and design name.ngo. The design name.ngd
ﬁle is used during subsequent modular design steps, while design name.ngo is
not. The constraint editor that is used to edit design constraints such as clock
periods, in the top-level design, can be invoked with the following command:
constraints editor design name.ngd
The following command invokes Floorplaner.
ﬂoorplanner design name.ngd
Using Floorplanner, modules sizes, module locations and information on
module ports can be created. The modiﬁcations are copied in the top-level
constraint ﬁle topX.ucf. This ﬁle can also be directly edited with the constraints
as speciﬁed in the user manual. Constraints that must be set in the .ucf ﬁle,
either by hand or by the ﬂoorplanner are the following:
1 Module area constraints:
each module must have a minimum width of four-slices.
a module’s width is always a multiple of four slices (e.g. 4, 8, 12)

Partial reconﬁguration design
221
the module bounding box must span the full height of the device.
each module must be placed on an even four-slice boundary.
Besides those constraints, area group must be deﬁned for each component
and speciﬁc properties for the partial reconﬁguration design ﬂow must be
added to the to the area group of components.
2 Input Output Block constraints: The IOB must be constrained within the
columnar space of their associated reconﬁgurable modules. This condition
is important to avoid the connection between a module and a pin to be
destroyed during the reconﬁguration of another component. Also, all IOBs
must be locked down to exact sites.
3 Global logic constraint: All global logic such as clocks, power and ground
lines must be constrained in the top level. No global logic should be locked
in the module topX.ucf ﬁle.
4 Insertion of Bus macros
Partial reconﬁguration is the process of transiting from one full conﬁgura-
tion to a new full conﬁguration using a partial conﬁguration bitstream. This
partial bitstream represents a component or a set of components present
in the ﬁnal bitstream, but not in the ﬁrst one. The two bitstreams have a
common ﬁx part, which does not change. The reconﬁgurable parts are con-
nected to the ﬁx part by deﬁning signal and connecting them to the ports.
The path followed by the signals in two designs may be different because
of the routing tools. On partial reconﬁguration, the resulting application
may not work anymore, if the signals connecting the ﬁx and dynamic part
are not using the same paths in the two designs. The role of the bus macros
in the Xilinx FPGAs is to provide ﬁx communication channels, which can
be used by reconﬁgurable module. Those are tri-state lines that help to
keep the signal integrity of modules being reconﬁgured. Those channels
are not affected by the reconﬁguration and the signals keep their integrity,
thus ensuring a correct operation of the application.
The approach consists of connecting the two components at the two ends of
a dedicated long line segment within the FPGA. The two components use
tri-state buffers to deﬁne the direction of the communication. A bus macro
is a construct that allows such connection to be speciﬁed in the design tool.
Each bus macro occupies a 1-row by 8-column section of tri-buffer site
space.
Besides the constraints speciﬁed, global-level timing constraints can be cre-
ated for the overall design.
The output of the initial budgeting phase is a
topX.ucf ﬁle containing all placements and timing constraints. Each module is

222
Reconﬁgurable Computing
implemented using this topX.ucf ﬁle, extended with the module-speciﬁc con-
straint requirements.
To ﬁnalize the initial budgeting phase, the following command is run in the
initial subdirectory of the corresponding top-level design, after copying the
ﬁles topX.ucf ﬁle and topX.ngc ﬁle to the same directory.
ngdbuild -p device type -modular initial topX.ngc
3.4
Implementing Active Modules
During this phase, the team members implement the top-level design with
only the active module expanded using the top-level topX.ucf ﬁle. Team mem-
bers can implement their modules in parallel. Once each design has been syn-
thesized, ﬂoorplanned and constrained, the implementation (mapping, place
and route) of all modules (both ﬁxed and partially reconﬁgurable) for the de-
sign can start. Each module will be implemented separately, but always in the
context of the top-level logic and constraints. Bitstreams will be generated
for all reconﬁgurable modules. This section describes the overview on how to
independently implement each module.
The following instructions should be performed for each of the module in
the design.
1 Copy the module implementation ﬁles to the active implementation direc-
tories for each module (active/module name) under the module directory in
which active module will be implemented:
Synthesized module netlist ﬁle module name.ngc
The ﬁle topX.ucf, which the team leader created in the initial budgeting
phase must be copied in the module local directory and renamed to
module name.ucf.
2 Build the module in active module mode as follows:
ngdbuild -uc module name.ngc -modular module -active topX.ucf \\
top level design directory path/topX.ngo
The output NGD ﬁle is named after the top-level design and contains im-
plementation information for both the top-level design and the individual
module. The -uc option ensures that the constraints from the local topX.ucf
ﬁle are annotated.
3 Map the module to the library elements using the following command.
map topX.ngd

Partial reconﬁguration design
223
In this step, the logic of the design only with expanded active module is
mapped.
4 The place and route of the library elements assigned to the module is done
by invoking the placer and router with:
par -w module name.ncd top routed.ncd
The -w option ensures that any previous versions of the produced ﬁles de-
sign name routed.ncd are overwritten. If the area speciﬁed for the module
cannot contain the physical logic for the module because it is sized incor-
rectly, then resizing must be done again and the .ucf ﬁle generated during
the initial budgeting phase must be regenerated with the changes made.
The entire initial budgeting phase should be performed again. This would
then imply that new .ucf ﬁles would need to be copied to each module and
that each module needs to be reimplemented. If everything went ﬁne, then
the module can be placed and routed at the correct location in the given
bounding box.
5 Run trace on the implemented design to check the timing report to verify
that the timing constraints are met.
trce top routed.ncd
If necessary, the Floorplanner is used to reposition logic, if the module
implementation is unsatisfactory, for example if it does not meet timing
constraints. In this case the Map, place and route steps must be rerun again
as described in the previous steps.
6 The last step in the active implementation is the publishing of the imple-
mented module ﬁle to the central located PIMs directory, which was set up
by the team leader. This step is performed using the command:
pimcreate pim directory path -ncd top routed.ncd
This command creates the appropriate module directory inside the PIMs
directory. It then copies the local, implemented module ﬁles, including the
.ngo, .ngm and .ncd ﬁles, to the module directory inside the PIMs direc-
tory and renames the .ngd and .ngm ﬁles to module name.ncd and mod-
ule name.ngm. The -ncd option speciﬁes the fully routed NCD ﬁle that
should be published.

224
Reconﬁgurable Computing
3.5
Module Assembling
Module assembling is the ﬁnal phase of modular design. Here the team
leader assembles the previously implemented modules into one top-level de-
sign. In this phase previously implemented modules, which were published to
the PIMs directory, and the top-level design ﬁle are used. The resulting full de-
sign can be used to generate a bitstream. As before, we provide the necessary
steps.
1 To incorporate all the logic for each module into the top-level design, run
ngdbuild as follows:
ngdbuild -modular assemble -pimpath pim directory path topX.ngo
Ngdbuild generates an topX.ngd ﬁle from the top-level topX.ucf ﬁle, the
top-level .ngo ﬁle and each PIM’s topX.ngo ﬁle.
2 The logic for the full design can then be mapped as follows:
map topX.ngd
MAP uses the topX.ncd and topX.ngm ﬁles from each of the module direc-
tories inside the PIMs directory to accurately recreate the module logic.
3 The full design can then be placed and routed using the following com-
mand:
par -w topX.ncd topX routed.ncd
The place and route command, par, uses the topX.ncd ﬁle from each of the
module directories inside the PIMs directory to accurately reimplement the
module logic.
4 To verify whether timing constraints were met, the trace command must be
invoked as follows:
trce top routed.ncd
Partial reconﬁguration with the modular design ﬂow can be performed as
batch job using one or more scripts that contain the large amount of commands
that must be invoked. An example of such scripts and design examples are
available for download on the book’s website. Also, tools such as Part-Y [71]
provide a conformable graphical user interface on top of the modular design.

Partial reconﬁguration design
225
4.
The Early Access Design Flow
With the introduction of the Early Access Design Flow for creating partially
reconﬁgurable designs, Xilinx updated and enhanced the existing Modular De-
sign Flow [227]. The package consists of a number of scripts that modify the
user’s ISE installation. Up to now the update is only viable with the exact
ISE version 8.1i with service pack 1. It can be obtained by registered users
from [226].
The main changes compared to the Modular Design Flow concern usability
and the relaxation of some rigid constraints for partial reconﬁguration designs.
Also, new bus macros improve constraint setting and allow more possibilities
for reconﬁguration.
First, Virtex-4 platform support is added to the PR tools. Besides the known
Spartan 3, Virtex-II and -II Pro, this enables users to deploy the current top
product of Xilinx’s linecard.
Partially reconﬁgurable areas may now also be deﬁned block-wise and do
not have to span over the full chip height. This provides the possibility, for
Virtex-4 devices and later to reconﬁgure the tiles rather than whole columns.
The Virtex-II and Spartan FPGAs, in contrast, physically lack this ability.
When a small region is reconﬁgured, the whole column is written, but the
conﬁguration controller on the FPGA checks whether the reconﬁguration would
actually change the content of the conﬁgurable logic block and perform re-
conﬁguration only where changes are necessary. The reconﬁguration will not
touch blocks that would remain unchanged at all as depicted in ﬁgure 7.4.
Figure 7.4.
Limitation of a PR area to a block (dark) and the actual dimensions (light)

226
Reconﬁgurable Computing
Figure 7.5.
Scheme of a PR application with a traversing bus that may not be interrupted
With this, applications may also be realized that require a bus that is placed
through the whole width of the chip and will not be destroyed when recon-
ﬁguration occurs somewhere in between. Figure 7.5 shows an example of the
design for a video streaming application that uses several stages of ﬁlters and
a bus that transports the pictures from one module to the next. The single ﬁl-
ters in the modules are partially reconﬁgurable and may be swapped without
interrupting and stalling the bus. In this case, we will not want to alter the bus
while conﬁguring a module on the chain.
Another relaxation is that signals crossing partially reconﬁgurable blocks
without interacting will not have to be passed through bus macros anymore.
The routing algorithm is now capable of determining those signals and using
longer range communication lines that will not be touched by a partial recon-
ﬁguration. This greatly alleviates timing constraints to be met and simpliﬁes
the designing process as a whole. Before, the developer had to build up the
design, have the signals be routed and check for signals that had been guided
through the partially reconﬁgurable area. Also, resources that used to lie within
this block, but have to be used in another ﬁxed module, would have had to be
fed signals through some bus macros. Both cases, given the signal, will not
exchange information with the PR area and are now fortunately handled by the
new design ﬂow. Now, traversing signals must not be guided through a bus
macro, a tedious handwork.
4.1
New directory structure
The Early Access Tools now support a modiﬁed, more sensible directory
structure for PR projects as depicted in ﬁgure 7.6. There, the folders’ contents
are presented in the order of usage in the design ﬂow.

Partial reconﬁguration design
227
Figure 7.6.
Improved directory structure for the Early Access Design Flow
The non pr folder should contain a working version of every combination
of modules that can be later used for partial reconﬁguration. Although not
required, it is strongly recommended to verify the functionality of the dif-
ferent variants of the design before trying to do any partial reconﬁguration!
synth holds the HDL-deﬁnitions of the modules. There should be at least
one subfolder for every partially reconﬁgurable module, a folder base for
all the ﬁxed modules and top for the top-level design. The modules are then
synthesized herein. Synth thus replaces the folders hdl and synth from the
Modular Design Flow.
The initial budgeting phase on the top-level module is done in top using the
synthesized netlist from synth/top, the UCF and the Bus Macro deﬁnition
ﬁles.
Initial budgeting for the static modules is accomplished in static.
The activation phase takes place in the reconﬁgmodules’ subdirectories for
each partially reconﬁgurable module.

228
Reconﬁgurable Computing
Finally, the design is assembled in merges and full and partial bitstreams
are generated.
4.2
Design Entry and Synthesis
Except the slightly changed locations because of the modiﬁed directory
structure, the general implementation work stays the same as with the Mod-
ular Design. Thus, top-level modules contain
all global logic, including I/Os,
all design modules, instantiated as black boxes with only ports and port
directions speciﬁed and
signals that connect modules to each other and to I/O pins.
Again, top-level modules include no logic except clock generation and dis-
tribution circuits.
4.3
Bus Macros
Xilinx ships a new set of Bus Macros to go with the Early Access package.
They are now distinguished between device type, direction, synchronicity and
width. They are now directed and not deployable in any direction and either
clocked to run synchronously with the data or unclocked as the old bus macros.
The naming convention yields to terms such as busmacro device direction
synchronicity width.nmc where
device can be
xc2vp
–
for a Virtex-II Pro,
xc2v
–
for a Virtex-II or
xc4v
–
for a Virtex-4 FPGA.
direction is either one of
r2l
–
right-to-left
l2r
–
left-to-right
b2t
–
bottom-to-top (Virtex-4 only)
t2b
–
top-to-bottom (Virtex-4 only).
synchronicity may be
sync
–
synchronous or
async
–
asynchronous
width is either
wide
–
spanning four CLBs or
narrow
–
reaching over two CLBs.
A sample name is busmacro xc2vp r2l async narrow.nmc.

Partial reconﬁguration design
229
Figure 7.7.
Usage of the new EA bus macros
The direction of the macro is to be seen geographically: right-to-left means
data ﬂowing from the right side of a border to the left side. If the right side
is inside a partially reconﬁgurable area, the bus macro is an output bus macro,
otherwise it is an input bus macro. The same applies to the other cases with
left-to-right and the bottom-to-top and top-to-bottom bus macros. The usage is
shown in ﬁgure 7.7.
Figure 7.8 shows the difference between a wide- and a narrow-type bus
macro. The classiﬁcation concerns only the range of CLBs that one of these
bus macros bridges. Both kinds offer eight signal lines to cross a boundary.
The unoccupied CLBs between wide bus macros can be used by user logic
or other bus macros. They can also be nested up to three times in the same y-
row as depicted in ﬁgure 7.9. With the old bus macros, only four signals could
traverse the boundary at this position whereas the new macros put through up
to 24 lines. The three staggered bus macros do not have to be of the same type
or direction.
Especially during the reconﬁguration, signals from partially reconﬁgurable
modules might toggle unpredictably. The new bus macros now provide ver-
sions that may therefore be disabled when appropriate to output a stable zero
instead of the ﬂuctuant signals.
4.4
Initial Budgeting
For the initial budgeting as well as for the activation phase, similar steps
have to be taken as with the Modular Design. The UCF generation will have
a different outcome as area constraints may now be set more freely and bus
macros have to be deployed more carefully concerning their type and direction.

230
Reconﬁgurable Computing
(a)
(b)
Figure 7.8.
Narrow (a) and wide (b) bus macro spanning two or four CLBs
Figure 7.9.
Three nested bus macros
The placement rules are generally deﬁned as follow:
the X-coordinate of the CLBs on which the bus macros is locked to, has to
be divisible by two
the Y-coordinate has also to be divisible by two
Virtex-4 devices require coordinates divisible by four because of the loca-
tion of block RAMs and DSPs

Partial reconﬁguration design
231
bus macros have to traverse a region boundary. Each of the endpoints has
to completely lie in a different area.
Further actions for the Initial Budgeting such as setting the timing con-
straints follow the same guidelines as the Modular Design. One must keep
in mind that signals that will not interfere with a module can cross that module
without having to run through a bus macro.
To close the Initial Budgeting Phase, the top-level design has to be imple-
mented. In directory pr project/top, the command
ngdbuild -uc system.ucf -modular initial system.ngc
has to be invoked. pr project/static now budgets the static portion of the de-
sign. The .nmc ﬁles for the used bus macros have to be placed in this directory.
The necessary commands are
ngdbuild -uc ../top/system.ucf -modular initial ../top/system.ngc
map system.ngd
par -w system.ncd sytem base routed.ncd
The routed outputs are placed in pr project/top resp. pr project/static. The
process yields another ﬁle, static.used, that contains a list of routes within the
partially reconﬁgurable regions that are already occupied with static design
connections. This is used as an input for the placement and routing of the
partially reconﬁgurable modules later on. The ﬁle allows for guiding signals
through PRMs without using bus macros.
4.5
Activation
In the Activation Phase,
the partially
reconﬁgurable
modules are
implemented.
First, the previously generated static.used is copied to each
of the modules directories (e.g. pr project/ reconﬁgmodules/ prm a1) and re-
named to arcs.exclude. Each module is then run through the known sequence
of ngdbuild, map and par as follows for the prm a1. In directory pr project/
reconﬁgmodules/ prm a1, do the following:
ngdbuild -uc ../../top/system.ucf -modular module -active \\
prm component name ../../top/system.ngc
map system.ngd
par -w system.ncd prm des routed.ncd
Note that prm component name is the component name and not the instance
name of the module. par automatically looks for and incorporates the arcs.
exclude ﬁle.

232
Reconﬁgurable Computing
4.6
Final Assembly
The last step is to assemble the static and reconﬁgurable modules into the
top-level design.
This work is greatly facilitated by two new scripts:
pr verifydesign and pr assemble. For this, a routed top-level design, a set
of the static modules and one PRM design have to be copied to a separate
subdirectory for each PRM under pr project/merges. For every PRM (e.g. in
pr project/merges/prm a1), the following has to be executed:
pr veriﬁydesign sytem base routed.ncd
prm a1 routed.ncd
pr assemble sytem base routed.ncd
prm a1 routed.ncd
It should be checked whether the ﬁnal netlists (e.g. pr project/merges/prm a1/
static routed.ncd) meet timing constraints. The timing analysis is performed
with the .pcf ﬁle for the static design.
The partial and full bitstreams to reconﬁgure the targeted FPGA are then
found in merges. Before using them, the developer has to make sure that the
assembly process produced proper ﬁles by examining the .summary ﬁles gen-
erated.
4.7
Merging Multiple Partially Reconﬁgurable Areas
To merge more than one partially reconﬁgurable region in one design, an
iterative invocation of pr verifydesign and pr assemble is needed. With
each iteration, a new partially reconﬁgurable module is merged into the exist-
ing overall design.
The corresponding actions are
1 Merge each PRM of one reconﬁgurable area with the static design
pr verifydesign and pr assemble generate intermediate designs with
just the one PRM contained and partial bitstreams that can be used to re-
conﬁgure the FPGA at this particular region afterwards. The necessary
commands are
merges/prmA/pr veriﬁydesign sytem base routed.ncd \\
prm a1 routed.ncd
merges/prmA/pr assemble sytem base routed.ncd
prm a1 routed.ncd
merges/prmA/pr veriﬁydesign sytem base routed.ncd \\
prm a2 routed.ncd
merges/prmA/pr assemble sytem base routed.ncd
prm a2 routed.ncd

Partial reconﬁguration design
233
2 Merge additional modules for other partially reconﬁgurable areas
For every additional region, the output of the previous proceeding is taken
as input: the partially reconﬁgurable module b1 is to be merged with the
combined a1 and base design.
merges/prmB1withA1/pr verifydesign \\
../../prmA/prm a1/base routed full.ncd \\
../../reconﬁgmodules/prm b1/prm b1 routed.ncd
merges/prmB1withA1/pr assemble \\
../../prmA/prm a1/base routed full.ncd \\
../../reconﬁgmodules/prm b1/prm b1 routed.ncd
Every combination of partially reconﬁgurable modules have to be merged
together. This requires for two partially reconﬁgurable regions accordingly:
merges/prmB1withA2/pr verifydesign \\
../../prmA/prm a2/base routed full.ncd \\
../../reconﬁgmodules/prm b1/prm b1 routed.ncd
merges/prmB1withA2/pr assemble \\
../../prmA/prm a2/base routed full.ncd \\
../../reconﬁgmodules/prm b1/prm b1 routed.ncd
merges/prmB2withA1/pr verifydesign \\
../../prmA/prm a1/base routed full.ncd \\
../../reconﬁgmodules/prm b2/prm b2 routed.ncd
merges/prmB2withA1/pr assemble \\
../../prmA/prm a1/base routed full.ncd \\
../../reconﬁgmodules/prm b2/prm b2 routed.ncd
merges/prmB2withA2/pr verifydesign \\
../../prmA/prm a2/base routed full.ncd \\
../../reconﬁgmodules/prm b2/prm b2 routed.ncd
merges/prmB2withA2/pr assemble \\
../../prmA/prm a2/base routed full.ncd \\
../../reconﬁgmodules/prm b2/prm b2 routed.ncd

234
Reconﬁgurable Computing
With the iterations, several ﬁles that will be created are equivalent to each
other. In this case, for partially reconﬁgurable area B
prmB1withA1/prm b1 blank.bit,
prmB1withA2/prm b1 blank.bit,
prmB2withA1/prm b1 blank.bit and
prmB2withA2/prm b1 blank.bit
are equal. Equivalent bitstreams for module b1 are
prmB1withA1/prm b1 routed partial.bit and
prmB1withA2/prm b1 routed partial.bit
and for module b2
prmB2withA1/prm b2 routed partial.bit and
prmB2withA2/prm b2 routed partial.bit.
Four variations of the full bitstream are generated representing the four
combinations of the two partially reconﬁgurable modules in this example:
prmB1withA1/base routed full.bit,
prmB1withA2/base routed full.bit,
prmB2withA1/base routed full.bit and
prmB2withA2/base routed full.bit
In practice, only one full bitstream is needed to load the FPGA. The re-
conﬁguration can then be conducted with the partial bitstreams. Which full
bitstream to take is incumbent on the user.
5.
Creating Partially Reconﬁgurable Designs
This chapter gives a detailed description of the steps to take when recreating
a design to be partially reconﬁgurable. The next part summarizes two sample
designs to explain the single tasks to accomplish. The understanding of this
guide depends on a profound knowledge of VDHL, ISE and EDK. To get a
quick glance of VHDL, see [208], for an in depth introduction consult, e.g.
[131]. Introductions to ISE and EDK are provided in [229] and [230]. For the
Early Access Reconﬁguration Flow software to work, it is crucial to deploy the
correct version of ISE. The current issue of the update package requires ISE

Partial reconﬁguration design
235
8.1.01i (Service Pack 1) and updates it to ISE 8.1.01i PR8 resp. PR12. The
used EDK system has version 8.1.02i.
5.1
Animated Patterns
The sample design Animated Patterns is a partially reconﬁgurable pattern
generator for the XUP development board [228]. It is based on a project taken
from a previously published tutorial [40]. The design creates a simple mov-
ing pattern and directs it out to a VGA-compatible screen. Depending on the
current pixel position that is drawn on the monitor, the reconﬁgurable module
Produce Color evaluates the color to be set for the next pixel. The system
with its two areas, reconﬁgurable and ﬁxed, is depicted in ﬁgure 7.10.
Figure 7.10.
Scheme of Animated Patterns
Partially reconﬁguring the device at run-time here means exchanging
Produce Color while the output to the monitor is continuing uninterruptedly.
Control for the user is provided through a simple C-program accessible over
a serial terminal. If a new partial bitstream is chosen, the program loads the
necessary partial bitstream from a Compact-Flash card connected to the devel-
opment board and transmits the data to the ICAP, the Internal Conﬁguration
Access Port, to reconﬁgure the device. The ICAP allows certain families of
FPGAs1 to be reconﬁgured from within the FPGA by a processor, in this case
the PowerPC sitting in the Virtex II Pro on the XUP development board itself.
The design can be found on the book’s Web page.
1Currently, the Xilinx Virtex-II, -II Pro, Virtex-4 and Virtex-5.

236
Reconﬁgurable Computing
Figure 7.11.
Two patterns that can be created with modules of Animated Patterns. Left: the
middle beam is moving. Right: the diagonal stripes are moving
5.2
Video8
The second sample design is called Video8 and is used for the code and
command execution examples in the following guide. It deploys the Digilent
VDEC1 extension card [70] for the Virtex-II Pro XUP Development Board
[228] to digitize an analog video stream from a camera. The extension card
is connected to the XUP via a proprietary slot and is technically connected
through an I2C Bus.
As shown in ﬁgure 7.12, the data stream is taken into the system by video in.
The module converts to the RGB color space and hands over to a ﬁlter, employ-
ing, e.g. a Sobel-implementation for edge detection. This ﬁlter module will be
Figure 7.12.
Scheme of Video8

Partial reconﬁguration design
237
reconﬁgurable at the end of this tutorial. The ﬁltered data are then processed
in vga out to be sent to a VGA compliant screen.
The embedded hardcore processor on the FPGA is used to conﬁgure the
video digitizer card and the ICAP on start up. It is then used to run a simple
program to choose which partial bitstream to load next. The input and output
is brought to the user’s console on a remote PC through the serial port.
5.3
Measures to take for PRM creation
Initially, a completely working project should mark the starting point for the
creation of a partially reconﬁgurable design. The initial project Video8 non pr
marks the starting point for the instructions to follow.
1.
Move partially reconﬁgurable parts to the top-level module
Partially reconﬁgurable modules may not be submodules to other than the
top-level module. To move its instatiation to the top-level, generally two
approaches are possible:
Reconstruct the initial design to place the reconﬁgurable parts and also
therewith connected module instantiations in the top-level. On the one
hand, the resulting code is much easier to understand and modules can
be developed by different engineers according to the modular design
ﬂow straight forward. On the other hand, the facilitation that comes
for example from using EDK for the basic design might be outweighed
by the lot of work involved when extracting and reconnecting all the
necessary modules. This militates all the more regarding the synthesis
process. The placement tool places modules according to their connec-
tivity with each other and the constraints given in the UCF not accord-
ing to their actual position in the hierarchy. Thus, reconstructing the
design might be useless as the placement is done independently.
In the Video8 non pr example, video in and vga out are directly af-
ﬁliated to the partially reconﬁgurable module to be. As suggested in
ﬁgure 7.13, these two modules at least would have to be instantiated in
the top-level module. Additional difﬁculties arise from the use of the
OPB to connect the ﬁlter module. Either the developer has to adapt the
ﬁlter module and provide other means of connection to the rest of the
system or the OPB has to be brought up to the top-level as well. This
again affords work and other modules to be changed accordingly.
The other solution is to only replace the old instantiations of the par-
tially reconﬁgurable modules with instantiations on the top-level and
amend the entities that formerly hosted the modules: Ports have to be
added to redirect the input data to the partially reconﬁgurable modules
up the module hierarchy to its new location at the top-level module and

238
Reconﬁgurable Computing
Figure 7.13.
Reconstructing example Video8 to place the partially reconﬁgurable part and
connected modules in the top-level
the output data back down to be fed into the system again. Figure 7.14
depicts the new structures. The big advantage with this approach is the
minimal adaption work to be done. An initial EDK design may be used
as is and can easily be changed.
The aimed at module for partial reconﬁguration, rgbfilter, is initially
instantiated in entity system, the top-level module of the initial EDK de-
sign. To place rgbfilter in the top-level module according to the second
approach, video in has to be expanded as follows:
entity video_in is {
...
//ports going out to the rgbfilter
Figure 7.14.
Moving a partially reconﬁgurable module to the top-level design on the example
of Video8

Partial reconﬁguration design
239
LLC_CLOCK_to_filter : out std_logic;
R_out_to_filter : out std_logic_vector(0 to 9);
G_out_to_filter : out std_logic_vector(0 to 9);
B_out_to_filter : out std_logic_vector(0 to 9);
h_counter_out_to_filter : out std_logic_vector(0 to 9);
v_counter_out_to_filter : out std_logic_vector(0 to 9);
valid_out_to_filter : out std_logic;
//ports coming back from rgbfilter
R_in_from_filter : in std_logic_vector(0 to 9);
G_in_from_filter : in std_logic_vector(0 to 9);
B_in_from_filter : in std_logic_vector(0 to 9);
h_counter_in_from_filter : in std_logic_vector(0 to 9);
v_counter_in_from_filter : in std_logic_vector(0 to 9);
valid_in_from_filter : in std_logic
...
};
As the ports of a submodule cannot be connected directly to those of its
hosting module, intermediate signals have to be introduced in the hosting
module. The entity vide in from the sample design has to be appended
accordingly after deﬁning the signals themselves:
...
-- signals out to the filter:
R_out_to_filter <= R1;
G_out_to_filter <= G1;
B_out_to_filter <= B1;
h_counter_out_to_filter <= h_counter1;
v_counter_out_to_filter <= v_counter1;
valid_out_to_filter <= valid1;
-- processed signals in from the filter:
R2 <= R_in_from_filter;
G2 <= G_in_from_filter;
B2 <= B_in_from_filter;
h_counter2 <=
h_counter_in_from_filter;
v_counter2 <=
v_counter_in_from_filter;
valid2 <= valid_in_from_filter;
...
The top-level entity must contain a similar construction.
2.
Base design rebuilding
Many designs for the Xilinx FPGAs take advantage of the use of EDK and

240
Reconﬁgurable Computing
its advanced wizards. Complete functioning systems on chip including the
software controlling can be created with a simple sequence of point-and-
clicks. In many cases the static base system, including the microprocessor,
the communication over a serial lines, RAM-controller and so forth, is cre-
ated with EDK. The controlling software that runs on the microprocessor,
may also be part of the system, leading to the creation of a bistream ﬁle
in the .elf format. This is a ﬁle supported by the EDK, which describe
the hardware conﬁguration and the memory mapping with the software that
will be executed on the processor.
Up to now, EDK does not support partial reconﬁguration directly. The nec-
essary phases for the design ﬂow still have to be done manually. Systems
implemented with EDK have to be exported to an ISE-format project, syn-
thesized as required and then be plugged into the top-level module as a
submodule.
In the case of Video8 non pr a basic system consisting of the infrastructure
for the PowerPC on the Virtex-II Pro, a RS-232 controller, a Video-in and
a VGA-out module. The design is created so that the partial bitstreams to
reconﬁgure the FPGA are loaded from a Compact Flash Card attached to
the FPGA.
Because the reconﬁgurable module in Video8 non pr has been included as a
core (Pcore) in the EDK project, the additional port deﬁnitions previously
described have to be applied to the system entity generated by the EDK
as well. This can be easily accomplished by adding External Ports to the
project. These changes will be written to the project’s MHS ﬁle similar to
the following sample lines:
PORT LLC_CLOCK_OUT = VIDEO_IN_PR_0_LLC_CLOCK_to_filter, DIR = O
PORT R_in = VIDEO_IN_PR_0_R_out_to_filter, DIR = O, VEC = [0:9]
PORT G_in = VIDEO_IN_PR_0_G_out_to_filter, DIR = O, VEC = [0:9]
PORT B_in = VIDEO_IN_PR_0_B_out_to_filter, DIR = O, VEC = [0:9]
PORT h_counter_in = VIDEO_IN_PR_0_h_counter_out_to_filter, \\
DIR = O, VEC = [0:9]
PORT v_counter_in = VIDEO_IN_PR_0_v_counter_out_to_filter, \\
DIR = O, VEC = [0:9]
PORT valid_in = VIDEO_IN_PR_0_valid_out_to_filter, DIR = O
PORT R_out = RGBFILTER_0_R_out, DIR = I, VEC = [0:9]
PORT G_out = RGBFILTER_0_G_out, DIR = I, VEC = [0:9]
PORT B_out = RGBFILTER_0_B_out, DIR = I, VEC = [0:9]
PORT h_counter_out = RGBFILTER_0_h_counter_out, \\
DIR = I, VEC = [0:9]
PORT v_counter_out = RGBFILTER_0_v_counter_out, \\
DIR = I, VEC = [0:9]
PORT valid_out = RGBFILTER_0_valid_out, DIR = I

Partial reconﬁguration design
241
2.1.
Additional Modules
To make the best use of the EDK, some of the needed components can
be added and connected in the EDK. A JTAG-PPC controller can help
to debug components connected to the PowerPC core. The opb sysace
module is the necessary interface to the SystemACE device that is de-
ployed for the management of a Compact Flash Card attached to the
FPGA. opb timer is used here to measure the time the reconﬁgura-
tion takes. Finally, the opb hwicap functions as an interface to the
ICAP for self-reconﬁguration.
2.2.
Bus Connections
The needed bus connections for the additional modules can also be
set up with EDK using the graphical user interface. This prevents a
lot of errors originating from misspelling of variables and names. It
also eases the task for less-experienced users. Thus, opb hwicap and
opb timer have to be connected to the same opb on which the Sys-
temACE module is connected to.
2.3.
Setting Address Ranges and Software Locations
To ensure the correct function with the parameters set in the control-
ling software, some of the project’s modules have to be set to a certain
address as stated in table 7.1.
docm ctlr and iocm ctrl are the controllers for data and instructions
to be stored in on-chip block RAMs. The program that controls the
reconﬁguration of the FPGA in this case is too large to ﬁt in a smaller
RAM area. It can generally be put into the DDR-RAM as well, but
in this special design, the framebuffer for the VGA output is placed in
the DDR-RAM, consuming a great share of the bandwidth of the bus it
is connected to. Thus, placing the program in the DDR-RAM as well
generates unwanted distortions in the VGA output.
2.4.
Rewriting the Control Software
The program deployed to control the non-pr design has to be extended
Module Name
Start Address
Size
opb sysace
0x41800000
64KB
opb timer
0x41c00000
64KB
opb hwicap
0x40200000
64KB
docm ctlr
0xe8800000
32KB
iocm ctrl
0xffff8000
32KB
Table 7.1.
New address ranges to be set in EDK

242
Reconﬁgurable Computing
by the initialization of the ICAP module and a mean to choose which
partial bitstream to load next.
For the sample design, the control program main.c contains just the
initialization of the video decoder card. The initialization routine for
the ICAP and the SystemACE has to be added as well as a menu that
prompts the user for the next partial bitstream.
2.5.
Export to ISE
The generated design has to be synthesized to be a submodule to the
top-level design. For this, EDK supports the export of a project to the
ISE where detailed options for the synthesis can be set. Alternatively,
the module can be synthesized using xst. To export the project in
Project|Project-Options|Hierarchy and Flow, the following
settings have to be made as follows:
Design is a sub-module with name system i
use project navigator implementation ﬂow (ISE)
do not add modules to an existing ISE-project
3.
Creating a Top-Level Module
To integrate the different modules, a top-level module has to be created. It
should contain just module instantiations and clocks. Here, I/O-buffers are
put up and bus macros are instantiated. Input and output ports are simply di-
rected through the appropriate buffer instance. Bidirectional inout-signals
are split up in in-, out- and a tristate control signals, using three correspond-
ing buffers.
Instances have to be named, especially those that will be static and recon-
ﬁgurable because their deﬁnition is placed in the UCF and the connection
to the design is established over the instance name.
The partially reconﬁgurable module rgbfilter requires bus macros for
signals R,G and B, v and h counter and valid from and to the rgbfilter.
Tools such as PlanAhead or FPGA-Designer can facilitate this work. For
detailed information on how and why to use bus macros see sections 3.3 and
4.3. An example of the resulting UCF can be found on the book web page.
4.
Synthesis
The synthesis of the partially reconﬁgurable project takes place in the sub-
folder synth of the project base folder. Here, corresponding subfolders for
static and reconﬁgurable modules and the top-level module can be found.
It should be mentioned again that any submodules, including the partially
reconﬁgurable ones, have to be synthesized without automatically adding
I/O-buffers. Thus the option -IOBUF has be set to no. I/O-buffers only have
to be included in the top-level module. The exported project contains the

Partial reconﬁguration design
243
system as a submodule system i of type system and should already in-
clude the needed ports for the data ﬂow to and from the reconﬁgurable mod-
ule. system i is ordered under a generated super module system stub
and can therefore directly be included in a top-level module. These ports
should be created by EDK when entering and wiring external ports as de-
scribed above. If not so, the generated HDL ﬁles describing and instantiat-
ing entity system resp. system i (system.vhd resp. system stub.vhd)
have to be extended with the necessary ports.
For the sample design folders, the reconﬁgurable modules (mod *) as well
as a top and an edk directory must be created. There is no separate static
module, except the EDK system. Therefore, no static folder is required.
The ports to add to entity system are the same as when changing the former
super module of the one that should be partially reconﬁgurable.
5.
Appending the UCF Taking a UCF built for example from EDK, several
amendments have to be made to let the partial reconﬁguration work. As
stated before, the deﬁnition of the area to place the partial and static blocks
and the location of the bus macros have to be declared in the UCF.
To set a region to be partially reconﬁgurable, the UCF is extended by lines
such as
INST "reconfig_module" AREA_GROUP = "pblock_recon_module";
AREA_GROUP "pblock_recon_module" RANGE=SLICE_X30Y64:SLICE_X45Y95;
AREA_GROUP "pblock_recon_module" MODE=RECONFIG;
With this, the reconﬁgurable module reconfig module is associated with
the area group pblock recon module whose bounding box is deﬁned by
the rectangle between slices X30Y64 and X45Y95. It is then marked recon-
ﬁgurable. A ﬁxed part like a basic system created with EDK, can be set
with:
INST "system_i" AREA_GROUP = "pblock_fixed_area"; AREA_GROUP
"pblock_fixed_area" RANGE=SLICE_X46Y16:SLICE_X87Y149;
Omitting MODE=RECONFIG tells the parser that the block will be ﬁxed. Al-
though the UCF is a simple text ﬁle and can be edited manually, tools such
as PlanAhead can be a big help with their architecture-dependant graphical
user interfaces.
6.
The Build Process
The build process starts with the synthesis of all the involved modules. The
submodules should not include I/O-buffers, which must all be included only
in the top-level module. The initial budgeting is partly done already, and
must be complete now. All other phases, from the activation to the ﬁnal
assembly, follow the steps described in section 4.

244
Reconﬁgurable Computing
6.
Partial Reconﬁguration using Handel-C Designs
The structure of the directory previously described is the same, if another
HDL such as Handel-C or even SystemC have to be used. Using Handel-C,
the goal is to provide the implementation of top-level designs and separately
the implementation of each module in a much comfortable way. The Handel-C
that must code for the entire system must be divided in code for modules, each
with its own interface for connecting other modules. The integration is then
done by connecting the modules together using signals in the top-level design
as shown in ﬁgure 7.1. For each module to be reconﬁgured later, a separate top-
level must be produced. The transition from one design to the next one will
then be done later using either full reconﬁguration or partial reconﬁguration
with modules representing the difference from one top-level to the next one.
We explain this modular design for Handel-C by means of one example
provided below.
We consider a system containing an adder in its ﬁrst conﬁguration. The
system can be partially reconﬁgured to act implement a subtractor in place
of the adder. Both modules (adder and subtractor) have to be implemented
separately. The implementation of the adder is provided in the following code
segment in Handel-C.
void main() {
unsigned 32 res;
// Interface definition
interface port_in(unsigned 32 var_1
with {busformat="B<I>"}) Invar_a();
interface port_in(unsigned 32 var_2
with {busformat="B<I>"}) Invar_b();
interface port_out() Outvar(unsigned 32 Res = res
with {busformat="B<I>"});
// Addition
res = Invar_a.var_1 + Invar_b.var_2; }
The ﬁrst part of the code is the deﬁnition of the interfaces for communication
with other modules and the second part realizes the addition of the input values
coming from the input interface, and the output values are sent to the output
interface. We need not provide the implementation of the subtractor, because
it is the same like that of the adder, but instead of adding we subtract. Having
implemented the two modules separately, each module will be inserted in a
separated to-level. Up to the use of the adder or subtractor, the two top-levels

Partial reconﬁguration design
245
are the same. The design can now be reconﬁgured later to change the adder
against the subtractor. Connecting the module in a top-level design is done as
shown in the following code segment.
unsigned 32 operand1, operand2;
unsigned 32 result;
interface adder(unsigned 32 Res)
my_adder(unsigned 32 var_1 = operand1,
unsigned 32 var_2 = operand2)
with {busformat="B<I>"};
void main() {
operand1 = produceoperand( 0);
operand2 = produceoperand( 3);
result = my_adder.Res;
dosethingwithresult(result);
}
According to the top-level design being implemented, the adder will be re-
placed by a subtracter. The next question is how to keep the signal integrity
of the interfaces. As there is no bus macro available in Handel-C, bus-macros
provided by Xilinx must be used as VHDL-component in a Handel-C design.
For instruction on integrating a VHDL code in Handel-C, consult the Celoxica
Handel-C reference manuals [125]. Bus Macros are provided with the Xilinx
application note on partial reconﬁguration [128] [227]. Before setting con-
straints on Handel-C design, the design has to be compiled ﬁrst. Afterwards,
the resulting EDIF-ﬁles must be opened with any text-editor and the longest
pattern that contains the name of the module as declared in the top-level mod-
ule is selected. This is useful because the Handel-C compiler generates EDIF-
code and automatically adds some characters to the module name used in the
original design. If the original module name is used, it will not be recognized
in the following steps of the Modular Design Flow. With the EDIF for the
modules and that of the top-level designs, we now have all what we need to
run the modular design ﬂow explained in section 3.

246
Reconﬁgurable Computing
7.
Platform design
One of the main factors that have hindered the implementation of on-line
placement algorithms developed in the past and presented in chapter 5, is the
development process of reconﬁgurable modules for the Xilinx platforms. As
we have seen in the previous section, partial reconﬁguration design is done
with many restrictions, which make a systematic development process for par-
tial reconﬁguration very difﬁcult. Each module placed at a given location on
the device is implicitly assigned all the resources in that area. This includes
the device pins, clock managers and other hard macro components such as the
embedded multipliers and the embedded BlockRAMs.
As illustrated in ﬁgure 7.15, a module using resource outside its place-
ment area must use connection running through other modules to access its
resources. We call those signals used by a given module and crossing other
modules feed-through signals. Using feed-through lines to access resources
have two negative consequences:
Cumbersome design automation: Each module must be implemented with
all possible feed-through channels needed by other modules to reach their
resources. This problem was already identiﬁed in section 1, where dedi-
cated channels are preconﬁgured on the device to be used by modules at
run-time. Because we only know at run-time which module needs to feed
the signal through, many channels reserved for a possible feed-through be-
come redundant.
Non-relocation of modules: Modules accessing resources, like pins in a
given area of the chip, are no more relocatable, because they are compiled
for ﬁxed locations where a direct access to their pins must be available.
Figure 7.15.
Modules using resources (pins) not available in their placement area (the com-
plete column) must use feed-through signals to access those resources

Partial reconﬁguration design
247
Most of the FPGA platforms available on the market do not provide solution
to the problems previously mentioned. Many systems on the market offer vari-
ous interfaces for audio and video capturing and rendering, for communication
and so forth. However, each interface is connected to the FPGA using dedi-
cated pins in a ﬁx location. Modules willing to access a given interface such
as the VGA must be placed in the area of the chip where the VGA signals are
available such that they can be assigned those signals among other resources.
This makes a relocation of modules at run-time impossible. Relocation pro-
vides the possibility to have only one bitstream for a module representing the
implementation of that module for a given location. At run-time, the coordi-
nate of the module is modiﬁed to match the location assigned to the module
by the on-line placer. Without relocation, each module must be compiled for
each possible position on the device where it can be later placed. A copy of
the component bitstream must therefore be available for each location on the
chip. The amount of storage needed for such a solution does not make such an
approach attractive.
A research platform, the Erlangen Slot Machine (ESM) that we next present,
was developed at the university of Erlangen to overcome the drawbacks of
existing platforms, and therefore allow unrestricted on-line placement on an
FPGA platform. Through the presentation of the ESM, we also hope to high-
light some of the requirements in the platform design for reconﬁgurable com-
puting.
The goals that the ESM-designers had in mind while developing a new plat-
form was to overcome the deﬁciency of existing FPGA-Platforms by
providing
A new highly ﬂexible FPGA Platform in which each component must not
be ﬁxed all the time at a given chip location.
A suitable tool support which goal is to ease the development process of
modules for run-time reconﬁguration and communication and to make an
efﬁcient use of the architecture.
Although the tooling aspect is important, it is not part of the basic require-
ments for designing the platform itself. We therefore just focus on the archi-
tectural aspect of the platform design in the next sections.
7.0.1
Drawback of existing systems
The practical realization and use of partial and dynamic reconﬁguration on
the current existing FPGA-based reconﬁgurable platforms is highly restricted,
because of the following factors:
1.
Limitation of reconﬁguration on actual FPGAs: Very few FPGAs al-
lowing partial reconﬁguration exist on the market. Those few FPGAs such

248
Reconﬁgurable Computing
as the Virtex series allow only a restricted partial reconﬁguration. As stated
in section 1, up to the Virtex II Pro, the reconﬁguration had to be done col-
umn wise meaning that loading a module in a given area will affect all the
modules on top and below the module. From the Virtex 4 and upwards,
the reconﬁguration of a module affects only those components in the same
blocks, which share a common column with the reconﬁgurable module.
The block does not span the complete device column, but only a few num-
ber of lines.
2.
I/O-Pin-Problematic: The design of existing FPGA platforms makes it
more difﬁcult to design for partial reconﬁguration. Most of the existing
platforms have the peripheral components such as Video, RAMs, Audio,
ADC and DAC connected at ﬁxed location on the device. This has the
consequence that a module must be stuck on a given region where it will
have access to his pins, and therefore making a relocation very difﬁcult.
Another problem related to the pin connection is that the pins belonging to
a given logical group such as video, audio, etc... are not physically grouped
together. On many platform, they are spread around the device. A module
willing to use a device will have to feed many lines through many different
components.
This situation is illustrated on ﬁgure 7.16 for the Celoxica RC200 platform.
To see are two modules among which a VGA module, which is imple-
mented on the FPGA. The VGA module uses a large number of pins at the
bottom part of the device and also on the right side. It is possible to imple-
ment a module without feed-through only on the two ﬁrst columns on the
left side. Most of the reconﬁgurable modules will need more than the two
available column of resources to be implemented. Feed-through lines that
Figure 7.16.
Illustration of th pin problematique on the RC200-Board

Partial reconﬁguration design
249
run through the reconﬁgurable module must be used, in order for the VGA
module to access the pins cover by the reconﬁgurable module.
Similar situation is not only present on the Celoxica boards. The XESS
boards [56], the Nallatech boards [165], the Alpha boards [8] face the same
limitations.
On the XF-Board [177] [211] from the ETH in Zurich, the connection to
the peripherals are done on the side of the device. Each module accesses
its peripheral through an operating system (OS) layer implemented on the
left and right part of the device and between the component swapped in
and out. This approach provides only a restricted solution to the problem,
because all modules must be implemented with a given number of feed-
through lines and interfaces to access the OS layer for communication with
the peripheral. The intermodule communication as well as the communica-
tion between a module and its peripheral is done via buffers provided by the
OS. This indirect communication will affect the performance of the system.
Many other existing platforms such as the RAPTOR-Board [132], Celoxica
RC1000 and RC2000 [43] are PCI systems, which require a workstation for
operation. The use in stand-alone systems as it is the case in embedded is
not possible.
3.
Intermodule communication: One of the biggest problems not considered
in the design of reconﬁgurable platform is the intermodule communication.
Modules placed at run-time on the device may need to exchange some data
among each other. This request of communication is a dynamic task be-
cause of the dynamism in on line placement. For module placed far from
each other, it will not be possible to feed communication line through sev-
eral modules to establish a connection. New paths are then necessary to
allow a dynamic communication to take place at run-time.
The previous limitations were the primary motivation in the design of the
Erlangen Slot Machine (ESM), whose concept we next present.
7.0.2
The Erlangen Slot Machine
The Erlangen Slot machine (ESM) is made upon a BabyBoard mounted on
a MotherBoard. The separation of the system in two boards allows the Baby-
Board that contains the reconﬁgurable module to be used on a wide variety
of systems. For the integration of the ESM in a new platform, no redesign of
the complete system must be done. Only a new MotherBoard must be pro-
vided according to the computational requirement in the new environment. A
multimedia system for example will provide a MotherBoard with multimedia
devices such as video and audio. In an automotive system, the MotherBoard
will mostly contain sensor and actuators. The BabyBoard can be mounted on

250
Reconﬁgurable Computing
a PCI MotherBoard in a computer system or on a stand alone MotherBoard in
an embedded system. Actually, only one variety of MotherBoard is available,
that is mainly targeted for applications in multimedia and signal processing. In
the next, we provide a detailed description of the boards functionality.
7.0.3
The BabyBoard: Computation and Reconﬁgurable Engine
The reconﬁgurable engine of the ESM platform is a BabyBoard, which fea-
tures an FPGA Virtex II-6000 FPGA from Xilinx, several SRAMs and a con-
ﬁguration circuit. Because of the restriction2 in the reconﬁguration of the Vir-
tex FPGAs, the architecture was adapted to match the following:
Free relocation of modules: Online placement of modules on a recon-
ﬁgurable device, in this case the FPGA, is done by downloading a partial
bitstream implementing a given task in the FPGA. The full and partial bit-
streams represent circuits implemented on a given region of the device at
compile time. In order to place a module in another location other than
the location for which it was compiled, a relocation of the module must be
done. The relocation of a given module therefore modiﬁes the coordinates
of all the resources used by the module in its previous location (the location
in which the module was constrained at compile time) with the coordinates
of the module in the new location. Relocation can be done only if all the
resources are available and structured in the same way in the previous lo-
cation and in the new location. This include the device pins used by the
module. If the connection of a module with its peripheral is done via some
pins ﬁxed at a given chip location, the module must be constrained at that
location. A relocation will be possible, but the module must feed signals
through all the other modules between the pins and the new location.
This problem is solved on the ESM by avoiding a direct connection of pe-
ripheral on the FPGA. As shown in ﬁgure 7.17, all the bottom pins from
the FPGA are connected to an interface controller (crossbar). The Interface
devices are connected to the interface controller as well. This makes it pos-
sible to establish any connection from one module to its peripheral via the
crossbar. No matter where the module resides in the device, the crossbar
can be set at run-time to connect the module to its peripheral.
Uniform repartition of resource: The ESM like any other reconﬁgurable
platform is primarily a parallel computing engine in which each module
carries its own computation and communicates with other modules. There-
fore a certain amount of resources must be allocated to each module for its
operation, at least for a given period of time. Memory is very important in
2The reconﬁguration can be done only column wise

Partial reconﬁguration design
251
Figure 7.17.
Architecture of the ESM-Baby board
application such as video streaming in which a given module must exclu-
sively access a picture at time for computation. However, the capacity of
the available BlockRAMs in FPGA is limited. External memory (SRAM
or DRAM) must therefore be added to allow the storage of large amount of
data by each module. To allow a module to exclusively accesses its mem-
ory, the SRAM are connected at the top part of the FPGA. In this way,
a module will have its connection to its peripheral from the bottom part,
and the top part will be used to temporally store the computation data. No
module will need to feed its lines through the other modules for access-
ing its resources. According to the number of memory banks that can be
connected on the top level, the device is then divided into a set of exchange-
able slots, each of which have access to the SRAM on the top part and to
the crossbar at the bottom.
The name Erlangen Slot Machine was chosen because it was developed at
the University of Erlangen-Nuremberg in Germany, but mainly because of
this organization in slots. This modular organization of the device simpli-
ﬁes the relocation, primary condition for a viable reconﬁgurable computing
system. Each module moved from one slot to another one will ﬁnd the same
resources there. The architecture is illustrated in ﬁgure 7.17. The Baby-
Board is logically divided into columns of 2 CLBs called micro slots. The
micro slots are the smallest allocatable units in the system. Each module
must therefore be implemented in a given number of micro slots. Because
of the number of pins needed to access one external SRAM, each module

252
Reconﬁgurable Computing
willing to use an SRAM module must be implemented in a multiple of 3
micro slots.
7.0.4
The conﬁguration manager
A part from the main FPGA, the BabyBoard also contains the conﬁgura-
tion circuitry. This consists of a CPLD, a conﬁguration FPGA, that is a small
Spartan II-FPGA, and a Flash.
The CPLD is used to download the Spartan II’s conﬁguration from the ﬂash
on power-up. It also contains board initialization routines for the on board
PLL and the Flash.
The conﬁguration program for the main FPGA is implemented in the Spar-
tan II. Because of its small size, this reconﬁguration program could be di-
rectly implemented in the CPLD, which could conﬁgure the main FPGA
at power on. But a much larger device was chosen to increase the de-
gree of freedom in the reconﬁguration management. As stated before, the
relocation is an important operation of the ESM. This can be done in dif-
ferent ways: The ﬁrst approach is to keep a bitstream for each possible
module and each possible position in memory. However, the size of the
Flash cannot allow us to implement this solution. The second approach
is the online modiﬁcation of the coordinate of the resources used in the
module’s bitstream to match the new position. This modiﬁcation can be
done for example through a manipulation of the bitstream ﬁle with the Java
program JBits [102] previously presented. However, ﬁle manipulation is a
time-consuming operation that will increase the reconﬁguration time. The
last and most efﬁcient solution is to compute the new module’s position
while the bitstream is being downloaded, using a dedicated circuitry. This
requires a much larger hardware code than the simple reconﬁguration. This
is why the Spartan II was chosen.
The Flash provides a capacity of 64 MByte, thus enabling the storage of up
to 32 Full conﬁgurations and few hundreds partial bitstreams for the FPGA
Virtex II 6000 that was used.
7.0.5
Memory
Six SRAM banks with 2 MBytes each are vertically attached to the board
on the top side of the device, thus providing enough memory space to six dif-
ferent slots for temporal data storage. The SRAMs can also be used for shared
memory communication between neighbour modules, e.g for streaming appli-
cations. They are connected to the FPGA in such a way that the reconﬁguration
of a given module will not affect the access to other modules.

Partial reconﬁguration design
253
7.0.6
Debug lines
Debugging capabilities are offered through general purpose I/O provided in
regular distance between the basic slots. A JTAG port provides debug capabil-
ities for the main FPGA, the CPLD and the Spartan II.
7.0.7
The MotherBoard
The MotherBoard provides programmable links from the FPGA to all pe-
ripherals. The physical connections are established at run-time through a pro-
grammable crossbar implemented in a Spartan FPGA on the MotherBoard.
This crossbar functionality basically solves the I/O pin dilemma of many ex-
isting FPGA platforms, thus allowing free relocation of modules requiring I/O
pin connectivity. Besides the run-time programmable crossbar, many peripher-
als for multimedia and communication are available on the board (ﬁgure 7.18).
Video capture and rendering interfaces as well as high-speed communica-
tion links are available on the MotherBoard. This interface allows the plat-
form to be used for instance in autonomous system, where a robot may collect
pictures from the environment and send them after a preprocessing step to a
central station for evaluation.
Figure 7.18.
Architecture of the ESM MotherBoard

254
Reconﬁgurable Computing
7.1
Intermodule communication
One of the central points in dynamic reconﬁguration is the communication.
Each module that is placed on the device must collect its data from a given
source and send its results to a given destination. This problem is given an
importance only in few systems such as the XF-Board of Zurich [177] [211].
However, the XF-Board implements only a communication through a third
party, which is slow because of the long path that messages have to go.
In the ESM, the communication among different modules (ﬁgure 7.19) can
be realized in three different ways: The ﬁrst one is a direct communication us-
ing bus macros between adjacent modules. The second one is the shared mem-
ory using the external SRAMs or the internal BlockRAMs. However, only
neighbour modules can use those two communication modes. For modules
placed in non-adjacent slots, we provide a dynamic signal-switching commu-
nication, the reconﬁgurable multiple bus (RMB) presented in section 4.1. Also,
the crossbar switch can be use as well for communication between modules.
FPGA
M1
M2
M3
M1
M2
M3
SRAM
SRAM
SRAM
FPGA
M1
M2
M3
FPGA
M1
M2
M3
FPGA
Crossbar
Figure 7.19.
Intermodule communication possibilities on the ESM

Partial reconﬁguration design
255
7.1.1
Adjacent Communication
Adjacent communication is done between two modules, which abut each
other. Bus macros must be used for a direct communication between neigh-
bouring modules. The number of Bus-macros needed is limited by the amount
of signal that can go through the Bus-macros and the number of signals to be
connected.
7.1.2
Communication via shared memory
The communication between two neighbouring modules can be done in two
different ways using shared memory:
Communication using BlockRAM: The BlockRAMs are dual ported RAMs
used in the FPGA mostly to implement FIFOs and other memory. It is a
nice possibility to implement communication among two neighbour mod-
ules, working in two different clock domains. The sender will just write on
one side with its own frequency, whereas the receiver will read the data at
the other end with its own frequency.
Communication using external RAM: The communication through the ex-
ternal RAM is particularly useful in applications in which each module
must process a large amount of data and then send the processed data to
the next module. This is mostly the case in video-streaming application in
which a ﬁrst module captures a stream, image by image. The images are
alternately stored in two different memory, which are accessed by the next
module for reading. On the ESM, each SRAM can be accessed by three
modules as shown in ﬁgure 7.20.
Figure 7.20.
SRAM-based intermodule communication on the ESM

256
Reconﬁgurable Computing
Because an SRAM can only be accessed by one module at a time, a con-
troller is then used to manage the concurrent SRAM access between the
three modules. Depending on the application, the user may set the priority
of accessing the SRAM for the three modules.
7.1.3
Communication via RMB
The communication between non-adjacent modules happens through the 1-
D circuit-switching paradigm presented in section 4.1.
7.1.4
Communication via the crossbar
The last possibility to establish a communication among modules is to use
the crossbar switch available on the mother board. Because all modules are
connected to the crossbar via the pins at the bottom of the FPGA, the commu-
nication among the modules can be set in the crossbar as well. This possibility
should be used only as emergency solution, because of the high latency caused
by the of-chip routing.
8.
Enhancement in the Platform Design
The design of the Erlangen Slot Machine was strongly inﬂuenced by the ar-
chitectural limitations of the Xilinx Virtex II FPGA, in particular their colum-
nwise reconﬁguration. This limitation was the primary motivation for the im-
plementation of the crossbar switch on a separate FPGA. All the connections
between running modules and their peripherals could be implemented in the
crossbar switch and be safe from the reconﬁguration of the module. Substan-
tial efforts were therefore placed in the design of the two boards previously
described.
With the introduction of the Virtex 4, Xilinx has introduced a new paradigm
in which complete column must not be replaced on reconﬁguration. This new
development can lead to a very great enhancement on the platform design in
the two following aspects:
First, the crossbar switch can now be implemented in the FPGA rather than
on a separated device. In this case, we need to attach all the peripheral on
one side of the block in which the crossbar is implemented. Modules can
then be connected on the other side of the block. We can even think of a
system in which all components are attached around the chip. The crossbar
can then be implemented as a ring, distributed module around the chip.
The distribution of the resources, such as the external RAM must not be
done in a column-wise manner anymore. Resources should now be homo-
geneously spread around the device, to allow different modules, which are
placed on different blocks to access their own resources.

Partial reconﬁguration design
257
Figure illustrates the enhancements on the Erlangen Slot Machine, using the
Virtex 4 and Virtex 5 FPGAs as previously explained.
Figure 7.21.
Possible enhancement of the Erlangen Slot Machine on the Xilinx Virtex 4 and
Virtex 5 FPGAs
Despite the advantages provided by the new class of Virtex devices, a great
disadvantage that results from this architectural enhancement is the communi-
cation: for the columnwise reconﬁgurable devices, the 1-D circuit was a simple
and efﬁcient possibility to enforce communication among different modules
running on the FPGA. In a two-dimensional reconﬁgurable device as is the
case with the Virtex 4 and Virtex 5, the difﬁculty of implementing the com-
munication increases. In 2-D, we have a quadratic growth in the amount of
resources needed to implement the communication infrastructure. That is the
amount of resources needed to implement a 2-D communication infrastruc-
ture is not only twice the amount needed to implement a 1-D communication
infrastructure but four times.
9.
Conclusion
We have presented in this chapter the different possibilities to design for par-
tial reconﬁguration on Xilinx devices. Our goal was not to rewrite a complete
manual on partial reconﬁguration, because several descriptions on this exist
[128] [227]. The Xillinx manuals as well as the work of Sedcole [190] provide
very good descriptions on using the modular design ﬂow and the early access.
Our primary motivation was to provide a king of tutorial, based on our experi-

258
Reconﬁgurable Computing
ence and for which a workable design, not too complex, but also not so easy,
exists. The designs as well as all the scripts needed for compiling are available
to download from the book’s web page. The difﬁculty in designing for partial
reconﬁguration can be reduced, if the target platform is well designed. One of
such platform is the Erlangen Slot Machine that was presented, with the goal
to emphasize the challenges in designing such a platform. The ESM is how-
ever strongly inﬂuenced by the column-wise reconﬁgurable Virtex. The price
we pay for ﬂexibility is very high. With the advent of new Virtex 4 and virtex
5, enhancements can be made in the design to increase the ﬂexibility, while
reducing the costs.
One of the main problems is the communication between the module at run-
time. While this problem is somehow better solved in a 1-D reconﬁgurable
device through the use of circuit switching and dedicated channels on the mod-
ule, its extension on a 2-D reconﬁgurable device is not feasible because of the
amount of resources needed. Viable communication approaches such as the
DyNoC was presented in chapter 6; however, with the amount of resources
needed by those approaches, their feasibility is only possible if manufacturers
provide coarse-grained communication element in their devices.

Chapter 8
SYSTEM ON A PROGRAMMABLE CHIP
Developments in the ﬁeld of FPGA have been very amazing in the last two
decades. FPGAs have moved from tiny devices, with few thousands of gates,
only able to implement some ﬁnite state machines and glue-logic to very com-
plex devices with millions of gates as well as coarse-grained cores. In year
2003, a growth rate of 200% was observed in the capacity of the Xilinx FPGA
in less than 10 years, while in the meantime, a 50% reduction rate in the power
consumption could be reached with the prices also having the same decrease
rate. Other FPGA vendors have faced similar development, and this trend is
likely to continue for a while. This development, together with the progress
in design tools, has boosted the acceptance of FPGAs in different computation
ﬁelds. With the coarse-grained elements such as CPU, memory, arithmetic
units, available in recent FPGAs, it is now possible to build a complete sys-
tem consisting of one or more processors, embedded memory, peripherals and
custom hardware blocks in a single FPGA. This opportunity limits the amount
of components that must be soldered on a printed circuit board to get a FPGA
system working.
In this chapter, we present the different possibilities that exist to build those
systems consisting of one or more processors, peripherals and custom hard-
ware component.
We start with an introduction in system on programmable chip, and then,
we present some of the various elements usually needed to build those sys-
tems. We then present at the end of the chapter a design approach for adaptive
multiprocessor systems on chip.
1.
Introduction to SoPC
A system on chip (SoC) is usually deﬁned as a chip that integrates the ma-
jor functional elements of a complete end product. Figure 8.1 presents the

260
Reconﬁgurable Computing
Figure 8.1.
Integration of PCB modules into a single chip: from system on PCB to SoC
achievement of a system on chip, namely the replacement of a complete printed
circuit board by a single chip.
A system on chip usually consists of a processor, memory, peripheral mod-
ules and custom hardware modules.
Contrary to a system on chip that is
manufactured on an integrated circuit and cannot be modiﬁed again, a sys-
tem on programmable chip (SoPC), or programmable system on chip, is built
on programmable logic. Its structure can be modiﬁed by the end user, either at
compile-time or at run-time by means of full or partial reconﬁguration.
In the next sections, we take a brief look on the main components of sys-
tem on programmable chip and present some key implementations of those
components by manufacturers.
1.1
Processor
Processors are the central processing and coordination units in system on
programmable chips. Besides the coordination of the complete system, they
are in charge of collecting the data from different peripheral modules or from
the memory, process those data and store them into the memory or send them
to the peripheral modules. Also the processor initializes the peripherals as
well as the dedicated hardware modules on the chip and manages the memory.
The most widely used processors are those used on Xilinx and Altera devices,
because those two companies control the largest part of the programmable de-
vice market. Besides the processors offered by those two companies, other
platform-independent implementations exist. We list some of the available
processors next.

System on a Programmable Chip
261
1.1.1
The MicroBlaze Processor Core
The MicroBlaze [127] is a soft 32-bit RISC processor core designed and
commercialized by Xilinx. The processor is not directly available on a chip. It
exists only as reference design, optimized for the Xilinx FPGA. With a clock
frequency of up to 200 MHz and a possibility to include a 32-bit single pre-
cision ﬂoating point unit (FPU) in the IEEE-754 format into the datapath, the
MicroBlaze is one of the fastest available soft core processor. It consists of a
set of ﬁxed components and a set of optional features that can be conﬁgured at
design time by the user. The main ﬁxed features of the MicroBlaze are:
An Arithmetic and Logic (ALU) Block unit featuring a shifter
thirty two 32-bit general purpose registers
Harvard architecture with separate 32-bit address bus and 32-bit data bus
a 5-stage pipeline in the version 5.1a
seven fast simplpex links (FSL) that can be used to connect the processors
to custom hardware
an interface for connecting the processor to an on-chip peripheral bus (OPB)
Besides these ﬁxed features, the MicroBlaze processor can be parameterized
by the users at compile-time to include additional functionalities. The latest
version of the MicroBlaze, the version v5.1a, can be conﬁgured to support the
following additional features:
a barrel shifter, a multiplier and a divider can be added to the ALU-Block
a 32-bit single precision FPU in the IEEE-754 format
A data cache and an instruction cache
an interface to the local memory bus (LMB), a high-speed synchronous bus
used primarily to access on-chip block RAM.
A CacheLink for accessing external memory
debug logic
The MicroBlaze soft processor core is available as part of the Xilinx em-
bedded development kit (EDK), which includes a comprehensive set of system
tools to design an embedded application in a Xilinx FPGA.

262
Reconﬁgurable Computing
1.1.2
The PowerPC 405 Processor
Contrary to the MicroBlaze that exists only as reference design compiled to a
netlist, the PowerPC 405 processor [126] is immersed into some Xilinx Virtex
FPGAs (the Virtex II Pro, the Virtex 4 and the Virtex 5). The integration of the
PowerPC directly into the chip at fabrication allows for a great performance
increase compare conﬁgurable processor cores.
The PowerPC 405 processor core used in the Xilinx devices is a 32-bit im-
plementation of thePowerPC embedded environment architecture that can be
clocked at up to 450 MHZ. It provides a dual mode operation, which allows
programs to run either in privileged mode or in user mode. The main compo-
nents of the PowerPC 405 are:
the central processing unit (CPU) with the following features:
–
a ﬁve-stage (fetch, decode, execute, write-back) pipelined datapath.
A fetch queue consisting of two prefetch and a decode buffer is used
to queued instructions in case of stalls. Instructions ﬂow directly to the
decode buffer if the prefetch buffers are empty.
–
a general purpose register ﬁle (GPR) consisting of thirty two 32-bit reg-
isters accessible via three reads and two write ports. The ﬁve ports on
the GPR allow the processor to execute load/store operations in parallel
with ALU or MAC operations.
–
an issue execution unit consisting of ALU and a single cycle throughput
multiply accumulate unit (MAC).
an exception handling module for servicing a total of 19 critical and non-
critical exceptions that can be caused by error conditions, the internal timers,
debug events and the external interrupt controller (EIC) interface.
a memory management unit, which provides address translation, protection
functions, and storage-attribute control for a total of 4 GB non-segmented
address space.
a 16 KB instruction-cache and a 16 KB data-cache, which can be accessed
respectively through the data cache and instruction cache units.
additional resources such as timers and debug units.
a processor local bus (PLB) interface providing a 32-bit address and three
64-bit data buses attached to the instruction-cache and data-cache units.
a device control register (DCR) interface for the attachment of on-chip de-
vice control, clock and power management interfaces for clock distribution
and power management.

System on a Programmable Chip
263
a JTAG debugging port, on-chip interrupt controller interface and on-chip
memory controller interface for attaching additional memory.
an auxiliary processor unit (APU) interface and a APU controller that are
used for extending the native PowerPC 405 instruction set with custom
instructions, implemented as coprocessor on the FPGA Fabric. With the
APU a tighter integration of application-speciﬁc function into the processor
is possible. The APU can be used for processor interconnection as well.
The PowerPC 405 does not have a ﬂoating point unit; however, Xilinx
provides in its module library a dedicated ﬂoating point unit that can be
attached to the processor through the APU interface.
1.1.3
The Nios Processor Core
Like the MicroBlaze, Altera’s Nios II [124] processor core is available as
reference design to be synthesized and to be downloaded into FPGAs. The
architecture supports operation in user mode as well as in supervisor mode,
thus allowing for a protection of the control registers. The Nios II core can
be customized by the designer by adding optional functionalities to the basic
system. Its main features are
a register ﬁle that consists of thirty two 32-bit general purpose integer reg-
isters, and six 32-bit control registers. Floating point registers can be added
to the register ﬁle by the customization of the processor.
an ALU supporting the main arithmetic and logic operations as well addi-
tional shift and rotate operations.
a single precision ﬂoating point as speciﬁed by the IEEE Std 754-1985,
which can be added to the core by customization.
optional, a NIOS core may include one instruction cache and/or one data
cache, depending on the user application. Caches can be avoided to keep
the design small.
one data bus selector module and one instruction-bus selector interfaces,
each of which can be used to provide memory and I/O access. Each con-
troller provides an Avalon master port for connecting the memory via the
Avalon switch fabric, and an interface to fast memory outside the Nios II
core.
various interface for debugging and interrupt handling.
a custom instruction port that can be used to extend the native instruction
set. A set of instructions implemented in a hardware module can be acces-
sible by connecting that hardware module to this port.

264
Reconﬁgurable Computing
1.1.4
The LEON Processor Core
Developed by the company Gaisler Research, the LEON processor core [1]
is an open source 32-bit processor core based on the SPARC V8 architecture.
Several versions of the LEON exists; however, we present only the LEON3 in
this section. The LEON3 is available as a synthesizable VHDL model, which
means that it can be implemented on any FPGA platform with the correspond-
ing vendor tool. The complete source code is open and available under the
GNU GPL license. The main features of the LEON3 processor, which can be
clocked at up to 125 MHz, are
Advanced 7-stage pipelined datapath
Hardware multiply, divide and MAC units
a fully pipelined ﬂoating point unit in the IEEE-754 format
Separated instruction and data cache, whose size, associativity, and replace-
ment policy, can be conﬁgured
a memory management unit with conﬁgurable TLB
an interface to the AMBA-AHB bus
support for Symmetric Multiprocessor (SMP)
various interfaces for debugging, clocking and power-down mode
1.2
Memory
A system on programmable chip needs memory for storing instructions and
data. Memory elements are available in small amount on almost all modern
FPGAs. This is usually used to build the caches directly on the chip. For ap-
plications that require more memory, external SRAM or DRAM must be used.
On-chip memory can be built from the memory elements, the block RAMs, or
from the LUTs. The use of the LUT in the second case to built memory has
two drawbacks: First, the amount of available computing resources decreases,
thus reducing the complexity of the logic that can be implemented on the chip.
Second, the memory built with LUTs is distributed across the chip, meaning
that chip interconnection must be used to connect the spread LUTs. This leads
to decrease in performance in the memory access. On-chip block RAMs are
usually dual-ported. They therefore provide a nice possibility to integrate a
custom hardware module, in particular those working in two different clock
domains. The memory is used in this case to synchronize the module with
each other.

System on a Programmable Chip
265
1.3
Peripheral
Peripherals are used in the system for communication with the external
world and for debugging purpose. The peripheral components are usually pro-
vided by the board manufacturers as ready to the module that can be inserted
in the SoPC-design. Because of their lower frequency, compared with the one
of processors, peripherals should be connected to the low-speed bus, which in
turn can be connected to the processor bus through a bridge. The communi-
cation between the processor and the available peripheral can be done either
through the I/O mapping paradigm or through the memory map paradigm. In
the ﬁrst case, the processor addresses each peripheral using special instructions
that directly address the peripheral on the bus. In the second, the peripheral is
assigned an address space in the memory and the communication happens in
this space with the processor writing in control register that can be read by the
peripheral. The peripheral responds by writing in status registers that can be
read by the processor.
1.4
Custom hardware
The ﬁnal types of component in a system on programmable chip are the
custom hardware.
Those are hardware components that implement special
functions for which one intend to speed-up the computation. Custom hard-
ware modules are usually streaming-based, meaning that the computation is
performed on a stream of data that ﬂow through the hardware module. Care
should be taken while integrating custom hardware modules in a system on
programmable chip. Connecting a custom hardware module on a peripheral
bus may result on a performance decrease, if the data rate of the computation
is much higher than that of the bus, which is usually the case. Direct connec-
tion and communication using dual-ported RAM can help in this case to allow
the custom module to work with its maximal frequency.
1.5
Interconnection
The interconnection provides the mechanism for integrate all the compo-
nents previously described in a workable system on programmable chip. The
interconnection mechanism provides the communication channels, the inter-
face speciﬁcation, the communication protocols and the arbitration policy on
the channels. The design of the interconnection infrastructure depends on the
target application. In traditional system on chip, where the infrastructure is
ﬁxed at chip production, the communication infrastructure must be as gen-
eral as possible to serve all class of applications that can be implemented on
the chip. In programmable devices, the ﬂexibility can be used to modify the
interconnection infrastructure according to the type of application to be im-
plemented. In this case, each application can ﬁrst be analysed to derive and

266
Reconﬁgurable Computing
implement the best communication infrastructure for its computation. Despite
the great interest in network on chip in the last couple of years, interconnection
on chip is dominated by the SoC communication paradigm which is in most of
the case bus-based. Leading existing solutions were previously developed for
SoCs before adapted to SoPCs. The general connection mechanism is provided
in ﬁgure 8.2.
It usually consists of two different buses. A high-performance bus that is
used by the processor to access the memory and a slow bus used to connect the
peripherals. High-performance dedicated hardware modules are connected to
the high-performance bus, whereas low-performance custom hardware compo-
nents are connected to the low-performance bus. A bridge is used to allow for
communication to happen between two modules attached on the two different
Figure 8.2.
Example of system ingration with CoreConnect buses

System on a Programmable Chip
267
busses. Besides those two buses, several possibilities exist to directly connect
the component. Dedicated lines or crossbar switches can be used, even in co-
habitation with the two previously described buses. The two well-established
bus systems in the world of programmable system on chip are the CoreConnect
from IBM and the ARM AMBA.
1.5.1
The IBM CoreConnect
As shown on ﬁgure 8.2, the IBM CoreConnect communication infrastruc-
ture consists of three different buses:
the PLB, which is a high-performance bus, used to connect high-bandwidth
devices such as high-performance processor cores, external memory inter-
faces and DMA controllers.
the OPB, which is a secondary bus that can be used to decoupled the pe-
ripherals from the PLB to avoid a lost of system performance. Peripherals
such as serial ports, parallel ports, UARTs, GPIO, timers and other low-
bandwidth devices should be attached to the OPB. Access to the periph-
erals on the OPB bus by PLB masters is done through a bridge, which is
used as a slave device on the PLB and as master on the OPB. The bridge
performs dynamic bus sizing, to allow devices with different data widths to
communicate.
Figure 8.3 illustrates the implementation of the OPB. Note that no tri-state
is required. The address and date buses are instead implemented using
a distributed multiplexer. This is a common technique for implementing
buses in programmable logic devices. All the master inputs to the bus are
ORed, and the result is provided to all the slaves. The arbitration module
deﬁnes which master is granted the bus. All other masters must then place
a zero signal on their output. The ORed is then used to write the value of
the bus master on the bus.
the DCR bus to allow lower performance status and conﬁguration registers
to be read and written. It is a fully synchronous bus that provides a maxi-
mum throughput of one read or write transfer every two cycles. The DCR
bus removes conﬁguration registers from the memory address map, reduces
loading and improves bandwidth of the processor local bus.
1.5.2
The AMBA
The Advanced Microcontroller Bus Architecture (AMBA) from ARM shares
many similarities with the IBM CoreConnect. Both architectures support data
bus widths of 32-bits and higher, they utilize separate read and write data

268
Reconﬁgurable Computing
Figure 8.3.
Implementation of the OPB for two maters and two slaves
channels, they allow multiple masters and support split transactions and burst
transfers. The AMBA 2 interconnection infrastructure consists of two main
buses:
the advance high-speed bus (AHB) that is used as high-performance system
interconnect to allow communication between high-performance modules
and the processor. The AHB plays the same role in the AMBA that the
PLB plays in the CoreConnect. A bridge from AHB or ASP is required to
interface to APB.
the advance peripheral bus (APB), used to connect the slower peripherals.
A bridge is used to interface between the AHB and APB buses and allow
low peripheral to be accessed from components hung on the AHB.
2.
Adaptive Multiprocessing on Chip
The continuous performance improvement of the last decades in micropro-
cessor is not likely to hold in the future. This positive trend observed in past,
and also known Moore’s law, was due to two main factors: high-speed clocks
and improvement in instruction level parallelism.
While the speed was increased by constantly growing clock frequency, the
capacity has always been boosted by the reduction of transistor size. Sequential
programs could therefore be speeded-up through instruction level parallelisms
(ILP) in a transparent way for the user. The increase in speed is getting more
and more difﬁcult to maintain. Enhancement on ILP-design for datapath is
not coupled anymore with a performance increase of the system [169] [213].
Moreover, the power is becoming one of the main limitations. To solve this

System on a Programmable Chip
269
problem, the research community and the industry have started using the large
amount of available chip area to implement several processors, thus creating
the so called chip multi processing systems.
Basically, two paradigms exist for parallel computation on multiproces-
sors. The ﬁrst one is the message passing interface (MPI), usually available
where communication among processors happens in a network. The second
paradigm, the shared memory or symmetrical multi processing (SMP) is ap-
plied in bus-based systems where communication happens through a share
memory. Multiprocessor on chip has been studied according to the second
paradigm. In most of the existing works, the purpose is to have an efﬁcient
mapping of a set of threads onto the processors. Two possibilities have been
presented that address this purpose: simultaneous multithreading (SMT) [75,
120, 103] and chip multiprocessor (CMP) [169, 109, 139, 94, 16]. An SMT-
Chip is based on a superscalar processor with a set of units for a parallel ex-
ecution of instructions. Given a set of threads, the resources are dynamically
allocated by extracting several instructions to be executed in parallel. Threads
that need long memory access are preempted to avoid idle states of processors.
Based on an Alpha 21164 processor with 10 function units and 8 instructions
per clock, an SMT-system was shown to provide a speedup of up to 4 com-
pared with common superscalar implementations [75]. In Hirata et al. [120],
ray-tracing-application on SMT-architecture could be simulated with consid-
erable performance improvement. Gulati and Bagherzadeh [103] could also
provide simulation results of SMT with performance improvements.
Instead of using only one superscalar architecture to execute instructions in
parallel, CMP use many processor cores to compute threads in parallel. Each
processor has a small ﬁrst-level local instruction and data cache. A second-
level cache is available for all the processors on the chip. CMPs target applica-
tions consisting of a set of independent computations that can be easily mapped
to threads. This is usually the case in database or webserver where transactions
do not rely on each other. In CMPs, threads having long memory access are
preempted to allow others to use the processor. Barroso et al. [16] have pre-
sented a scalable CMP architecture called Piranha in which the processors –
in this case alpha processors – use a crossbar switch for communication. De-
spite the good simulation results no physical chip could be manufactured. A
most concrete approach is the work of Lammon and Olukutun that leads to
the design of the Hydra Chip [109], whose technology is used in the Niagara
chip [139]. In this architecture, a hardware support for speculation is avail-
able, besides the parallel execution of threads. The communication among the
processors is done using a two-level on-chip cache. The simulation provides
almost a linear performance increase in the number of processors. Other devel-
opments in multiprocessor on chip include the IBM Cell-Chip, consisting of a
64-Bit Power processor and eight so-called synergistic processors. The Intel

270
Reconﬁgurable Computing
Pentium D and the AMD Dual Core Athlon 64 Processor are other examples
of multiprocessor on chip.
The work in [167] proposed an adaptive chip-multiprocessor architecture,
where the number of active processors is dynamically adjusted to the current
workload needed to save energy while preserving performance. The proposed
architecture is a shared memory based, with a ﬁx number of embedded pro-
cessor cores. The adaptivity results in the possibility of switching the single
processors on and off.
Programmable logic devices in general and FPGA in particular have experi-
enced a continuous growth in their capacity, a constant decrease in their power
consumption and permanent reduction of their price. This trend, which has
increased interest in using FPGAs as ﬂexible hardware accelerators, is likely
to continue for a while.
After an unsuccessful attempt in the 1990s to use FPGA as co-processor in
computing system, the ﬁeld FPGA in high-performance computing is experi-
encing a renaissance with manufacturers like Cray who now produce systems
made upon high-speed processors couple to FPGAs that act as hardware ac-
celerators [58]. However, actual FPGA solutions for high-performance com-
puting still use fast microprocessors, and the results are systems with a very
high-power consumption and power dissipation.
A combination of the chip multiprocessor paradigm and ﬂexible hardware
accelerators can be used to increase the computation speed of applications. In
this case, FPGAs can be used as target devices in which a set of processors
and a set of custom hardware accelerators are implemented and communicate
together in a network.
FPGA manufacturers such as Xilinx have been very active in this ﬁeld by
providing ready to use components (soft of hard-cores processors, bus-based
intercommunication facilities, and peripherals) on the base of which, multipro-
cessor on chip can be built. The Virtex II Pro FPGA for example provides up to
four PowerPC processors and several features like memory and DSP-modules.
The adaptivity of the whole system can be reached by modifying the com-
putation infrastructure. This can be done at compile-time using full reconﬁg-
uration or at run-time by means of partial device reconﬁguration. This results
in a multiprocessor on-chip, whose structure can be adapted at run-time to the
computation paradigm of a given application.
Unfortunately, the capabilities of those devices are not exploited. FPGA
devices like the Xilinx Virtex Pro are usually used only to implement a small
hardware component. The two available Power PC processors remain most of
the time unused. This is in part due to the lack of support for the multiprocessor
design.
In this section, we investigate the use of Adaptive Multiprocessor on Chip
(AMoC) and present a design approach for such systems. We also present an

System on a Programmable Chip
271
environment developed at the University of Kaiserslautern in Germany, for a
seamless generation and conﬁguration of the hardware infrastructure to pro-
duce a complete system.
One of the main applications of such infrastructure is in high-performance
computing in embedded systems. We next present our understanding of the
system architecture that must be ﬁrst generated onto the device. Thereafter, we
present a two-step design approach and the design automation for AMoCs.
2.1
Hardware Infrastructure
Many possibilities exist to build a multiprocessor hardware infrastructure on
a chip. In this section, we however focus on the general computing infrastruc-
ture shown on ﬁgure 8.4. For the sake of modularity, the infrastructure consists
of a set of processor blocks, each having a custom hardware accelerator and
memory directly attached the processor, either through the available fast pro-
cessor bus or through a dedicated communication link. The processor blocks
are connected in a network and are accessible from off-chip components via
peripherals. Additional external RAM might be available.
One of the main tasks in the design and implementation of a system like
that of ﬁgure 8.4, is the interconnection network. Contrary to other modules
(processors, peripherals, memory controllers and some custom hardware) that
Figure 8.4.
General adaptive multiprocessor hardware infrastructure

272
Reconﬁgurable Computing
are available in the vendor libraries, communication facilities, in particular
network on chip modules are not available at all. Also, most of the existing
tools provide only a very limited support for multiprocessing. We therefore
focus on the communication aspect next.
2.2
Communication Infrastructure
The communication between the processor blocks can basically be done in
two different ways: Shared-memory and message passing; in the ﬁrst case, the
processor blocks are attached to a common bus on which a memory is also
attached. The exchanged of message is done through the common accessible
memory. Although such a system can be built with the current vendor tools,
the amount of handwork required to have a system working is quite high. Fur-
thermore, some modules needed to build a workable system, for instance a
module to handle the cache coherence amount, the processors is not provided
and must be designed and implemented by the user. In the second case, where
message passing is used, the processor blocks are accessible through a net-
work on which they hung. The network is in charge of routing messages from
source to destination. Although message passing can be implemented with
shared-memory, we focus in this section on those implementations where a set
of independent processors communicate with messages that are sent through a
network.
The design of the communication network should be carefully done, to avoid
a high consumption of chip resources, while maximizing the speed and the
throughput. A network on chip (NoC) normally consists of set of routers each
connected to a processor and direct connections between the routing elements.
Each router provides a processor an interface to the network for sending and
receiving messages in the form of packets. Also, the routers are in charge of
placing incoming packets on the best tracks and insure a smooth routing to
destination.
While the routers have the advantage of ﬂexibility, thus allowing any topol-
ogy to be built, their resource consumption has been shown to be quite high in
FPGAs, because of the large amount of multiplexers used [32]. To avoid wast-
ing too much chip resources, a limited amount of routers may be used. This
can be reached by using a ring architecture. Like a Bus, a ring provides a great
advantage in the resource consumption. The disadvantages of the bus, like the
exclusive bus access by masters and the lower fault tolerance can be can be
compensated by having several masters on different ring segments. In [108],
a ring architecture was designed for the interconnection of a set of processors
blocks on a FPGA.
As ﬁgure 8.5 illustrates, the processors are primarily connected using a ring.
However, because a ring allows only one kind of topology to be built, routers
are available for building cross-connections over different rings. The routers

System on a Programmable Chip
273
Figure 8.5.
Structure of the on chip network
therefore provide a means to build other topologies than the ring. A router
is able to connect up to four rings. Building a mesh for example is done by
connecting two ‘perpendicular and disconnected rings’ through the router. In
ﬁgure 8.5, two rings connected to a router are shown.
A ring has the advantage that components connected to it do not have to
deal with routing, and therefore, the amount of multiplexers (one of the most
resource consuming element) needed to implement the complete infrastructure
decreases drastically. Components only need transceivers to access the ring
for sending and receiving messages (ﬁgure 8.5). Each transceiver consists of
a sender (S) and a receiver (R) that operate independent from each other. The
receivers behave on the ring as slave and on the local bus as master, whereas
the sender behaves on the local bus as slave and on the ring as master. The
ring slave in the receiver collects data from the ring and passes them to local
bus master in the same receiver that. The message can then be copied into the
local memory through the local bus. The local bus slave in the sender collects
data from the local bus and send them to ring master in the sender, which then
place the message onto the ring (ﬁgure 8.6).
Within a transceiver, each slave and each master is basically a ﬁnite state ma-
chine (FSM), which is in charge of initiating and handle the required

274
Reconﬁgurable Computing
Figure 8.6.
Implementation of the transceiver
transactions (ﬁgure 8.6). This modularization helps to decouple the complex
bus operations from the cumbersome ring transactions.
The transceivers operate in two different modes: active copying and DMA
(direct memory access). The active copy mode is used to send small number of
packets while DMA deals with the transmission of large data segments. With
the DMA capability on transceivers, a remote DMA between two processors
is implemented, to quickly transfer large amount of data from one processor to
another one, without loss of performance. The communication protocol used
in this case is next explained.
2.3
Data Transfer and Communication Protocol
In active copy mode, the processors pass each single byte to be sent to the
transceiver whereas in DMA mode, the processor just speciﬁes the destina-
tion address and provides only the start address of the data block to be sent.
The transceiver can then access the memory and transmits the data over the
network, while the processor continues its computation.
A sending operation is initiated by a processor by copying the destination
address together with some control data to the control register of the FSM
acting as local bus slave. While this ﬁrst FSM collects data to be sent from
the local bus, the second FSM, acting as master on the ring, builds packets and
send them through the ring. In case of an active copy, the data are placed on
the bus by the processor, while in DMA, the slave FSM accesses the memory
itself to collect data, thus freeing the processor. This second possibility is used
to implement a remote DMA (RDMA), which allows a processor to directly
copy a chunk of data in the memory of another processor on the network. The
destination processor must therefore dynamically allocate enough space in its
local memory for incoming data from the sender processors. A processor Ps,
willing to send data to another processor Pr, ﬁrst sends a request of RDMA
data transfer to processor Pr. The request speciﬁes the length (in bytes) of

System on a Programmable Chip
275
data to transfer. Pr then tries to allocate enough memory for the data and
allows Ps to transfer data, if the reservation is successful. The transfer is done
by the sender and receiver modules of the corresponding transceivers while the
two processors might perform other tasks.
To implement the handshaking between the two processors, a communica-
tion area is foreseen on each processor. This is a table with two entries for
each processor. The ﬁrst one speciﬁes the request/acknowledge of data trans-
fer and the second one speciﬁes the amount of bytes to transfer. Each processor
performs a request/acknowledge through a remote writing in the table of the
sender/destination processor at the corresponding location.
A possible communication scenario is shown in ﬁgure 8.7; The network
consists of n + 1 processors also called clients. client0 and clientn consec-
utively request a RDMA-transfer of data to client1 (step 1 and step 2). Ac-
cording to a ﬁrst-request-ﬁrst-grant mechanisms, client1 ﬁrst grants RDMA
access to client0 by writing the granted-code at the right location in the hand-
shaking table of client0 (step 3). From this point on, the sender transceiver
and the receiver transceiver are used to realize the data transfer, while the two
processors keep computing (step 4). Upon completion, client0 removes its
request (step 5), and client1 removes its grant. clientn is now granted the
RDMA-access (step 6) and the data transfer is done according to the scheme
previously described (step 7 and step 8).
Figure 8.7.
The communication protocoll

276
Reconﬁgurable Computing
2.4
Design Automation
The goal in design automation is to ease the implementation of an adaptive
multiprocessor system described in ﬁgure 8.4, on an FPFA.
The generation of such a system is a complex task whose support through
the current vendor tools is very limited. A substantial amount of hand work
must be done to have many processors working in an FPGA. The only paradigm
supported by the current tools, albeit very limited, is the SMP-like computation
paradigm. For the design of MPI-like systems, the user must do everything
from scratch. The network components must be provided, also support for
cache coherence in the case of SMP-like computation. This requires a great
amount of hand work and strong experience in hardware design. To make
multiprocessing implementation on chip easier, in particular for software de-
signers, but also for those hardware designers not willing to spend too much
time in the hardware part of the system, a seamless approach must be available
to generate the hardware infrastructure. The designer can then focus only on
parallelizing he’s code by deﬁning the part to be executed by each processor
and the communication points in the code.
The work in [130] [183] has contributed, besides the development of the
communication infrastructure to the design and implementation of a vendor
independent framework in which the user, no matter if beginner or experienced,
can specify the complete requirements of he’s system and let the tool produce
the necessary conﬁguration and start-up code.
The approach that followed is a two-step one, consisting of ﬁrst providing
an abstract speciﬁcation of the system. The abstract speciﬁcation will then
be reﬁned and mapped to a concrete description according to the component
library available in the second step.
2.5
Automatic Generation of the Hardware Infrastructure
In the ﬁrst step, the hardware is automatically generated from an abstract
speciﬁcation. This speciﬁcation may be provided by the user or it can be the
result of an analysis process that generates the best multiprocessor infrastruc-
ture as well as the best topology to compute a given application.
As shown in ﬁgure 8.8, the description at the highest level is very abstract
and speciﬁes only the amount of the processors, their interconnection paradigm
and the characteristics of the computing blocks used in the system.
The description is done in XML in conjunction with a document type deﬁ-
nition ﬁle (DTD). The document type deﬁnition describes the syntax as well as
valid components and their attributes. At this level, no information about spe-
cialized and platform-dependent system components has to be supplied. Valid
modules that can be described at this stage are for instance CPU, Memory,
CommMedium, Periphery and HWAccelerator. CommMedium is an identiﬁer

System on a Programmable Chip
277
Figure 8.8.
Automatic hardware generation ﬂow
for the whole class of communication media. It comprises buses, networks and
point-to-point connections. Periphery and HWAccelerator differ only in their
type of connections. While a HWAccelerator is used for internal components,
Periphery is used to describe modules with external connections.
Having speciﬁed the system, the description may be functionally simulated
to observe the system behaviour and afterwards be validated.
In the last step, the ﬁnal system can be created by supplying a concrete
system description for the target platform.
When transforming an abstract system description into a concrete one, trace-
ability must be maintained. Therefore, in each of both steps, the respective
XML input ﬁle is validated against its document type deﬁnition to prevent in-
valid component speciﬁcations.
Subsequently, the given speciﬁcation is processed by a Java application
consisting of three consecutive phases. In phase one, an XML parser extracts

278
Reconﬁgurable Computing
required information from the XML sources and an internal representation of
the resulting XML tree is built up for further use. For each component, an ap-
propriate class is instantiated, which knows how to process its own parameters.
Obviously, the application can be easily extended to support new user-designed
IP-cores by just adding a corresponding class.
Both parsers – the abstract one as well as the concrete one – share this ap-
proach. Whereas the abstract parser has ﬁnished its work at this point, the
concrete XML parser can now go on processing the gathered information to
create the platform-speciﬁc hardware information ﬁles. Individual mappers
are created in the second phase, one for each component and each target plat-
form. In the third and ﬁnal step, a mapper is invoked to create the desired
platform-dependent hardware description ﬁles. Again, it has to be emphasized
that extending the application to support a new platform can be simply accom-
plished by adding another mapper class.
Mapper classes for system components are derived from an abstract platform-
dependent super class that contains a generic algorithm, which enables derived
classes to inherit mapper functions. The current system component that has
to be mapped is passed from the corresponding class itself to its mapper class
through constructor. The resulting hardware description ﬁles mentioned above
are then passed to the vendor’s tool chain, for example to Xilinx EDK to gen-
erate the bitstream for the conﬁguration of the FPGA.
The system does not only enable the user to create hardware information
ﬁles from a given speciﬁcation. Existing design can also be loaded from ﬁles in
which they were saved. Also, system components can be added to or removed
from the system in a very comfortable way, and the changes can be rewritten
back to the XML ﬁle.
A tool called PinHat (Platform-independent hardware generation tool) was
developed with the goal to integrate all the functionalities and features de-
scribed above into one single, easy-to-use application.
PinHat provides a
graphical user interface (ﬁgure 8.9) that allows the user to comfortably edit
the desired system settings and guides him through the process of deriving the
concrete system description from the abstract one.
It integrates seamlessly with vendor-supplied toolchains and allows the gen-
eration of a bitstream to conﬁgure any supported FPGA based on the created
concrete speciﬁcation.
After having adapted the abstract speciﬁcation to the end-user’s needs, Pin-
Hat offers a transformation wizard, which supports the procedure of reﬁning
abstract node elements to concrete ones. First of all, the root element has to
be reﬁned by specifying the target toolchain and the target board. This piece
of information is used to identify correct speciﬁed components when reﬁning
all the other abstract nodes. Selecting for instance Xilinx as target toolchain
and ML 310 as target board would allow an abstract CPU node to be reﬁned
to either a Microblaze or alternatively a PowerPC 405 concrete node element.

System on a Programmable Chip
279
Figure 8.9.
The Platform-independent Hardware generation tool (PinHat)
Subsequently, the reﬁnement of all lower-level nodes can be done. PinHat
has integrated parsers to read out component and board description ﬁles where
information about peripherals, FPGA pin connections, board architecture, in-
ternal component parameters and much more can be stored and thus does not
need to be entered manually. This increases the comfort by avoiding the users
to enter hundreds of parameters for a single component. Furthermore, this pro-
cedure guarantees that no non-existing parameters can be speciﬁed. Parameters
that are not speciﬁed are initialized with standard values that are also part of
the parsed description ﬁles. The role of PinHat in the design ﬂow is illustrated
in ﬁgure 8.10, where the abstract speciﬁcation is mapped later to the Xilinx
Platform.
2.6
Automatic conﬁguration of the hardware
infrastructure
In contrast to the downloading of the bitstream into the FPGA, we under-
stand under automatic conﬁguration the generation of the binary code that will
be run on each processor, the conﬁguration of the operating system and the
generation of the ﬁles needed for system start-up.

280
Reconﬁgurable Computing
Figure 8.10.
The PinHaT framework
Like in the Automatic generation of the hardware infrastructure, we have a
SW-speciﬁcation, which may be – as the HW-speciﬁcation – provided by the
user or it can be the result of an analysis and code generation process. The SW-
speciﬁcation describes whether a given processor is to be run with a standalone
application or if an operating system is foreseen for that processor. For each
standalone application, additional information is given about the task it exe-
cutes; the task allocation results from splitting the application to be computed
into a separated set of independent and communicating tasks. The distribution
of the jobs and the mapping to the processors is stored in XML, complementing

System on a Programmable Chip
281
the abstract component input described in Section 2.5. We are not focusing on
the partitioning process here. We assume that this part has been done and that
the segments of the overall code that will be implemented on each processor
are available.
As the abstract system description is transformed into a concrete one, the
mapping ‘task →abstract processor’ is reﬁned into a mapping ‘task →con-
crete processor’ (ﬁgure 8.11).
Thereafter, parameters of the software for each processor must be speciﬁed:
this includes information about either the application (source-code, libraries,
etc.) or about the operating system(s) (type, conﬁguration, require function-
alities and modules, etc.). Once those information are provided, scripts and
compiling ﬁles for building the standalone software and the operating systems
are generated. Building the standalone software is straightforward because it
requires only compiling and linking the ﬁles, using vendor tools on the basis of
the hardware generated in the previous step. The building of the operating sys-
tem is however a more complex issue. Depending on the OS chosen, different
steps are necessary; for some operating systems, only the kernel-parameters
may be conﬁgured explicitly, while the ﬁle-system and therewith the available
programs is pre-build. Other operating systems allow for the complete conﬁgu-
ration of the kernel, ﬁle-system and the available programs. The conﬁguration
of the operating systems is done by generated scripts.
The result of the conﬁguration step is one executable ﬁle for each processor
in the system. This ﬁle can be the same for a pair of processors in the system,
if the parameters provided to build the ﬁle are the same.
Using vendor tools, the FPGA-conﬁguration bitstream, resulting from the
generation of the hardware infrastructure is complemented with the generated
executable ﬁles. These results in a conﬁguration ﬁle containing hardware and
software conﬁguration of the whole system for the chosen FPGA.
According to the operation paradigm chosen, we may have the operating
system running on all the nodes or on just one node as illustrated in
ﬁgure 8.12. The last approach makes more sense. In this case, the node at-
tached to the peripherals uses the operating system for a better control of those
peripherals. This node will then behave as a master in the system, collect-
ing the data and dispatching them to all the processors involved in the parallel
computation of a given application, collect the partial results from the single
processors and send them through the peripherals to off-chip components. The
operating system can also be made available on many processors, if those pro-
cessors manage some peripherals separately.
As case study, several implementations were done on FPGA, using the
PinHat tool. A three processor system consisting of a PowerPC, and two
MicroBlaze processors on the Xilinx ML310 board, featuring a VirtexII Pro
30 FPGA, were built. The primary goal was to test the bandwidth performance

282
Reconﬁgurable Computing
Figure 8.11.
software conﬁguration ﬂow
of a system with multiple rings and a router for the ring interconnection. The
design, in a ﬁrst non-optimized implementation, reveals an average real band-
width of 36 Mb/s. This includes the complete transaction need to code, send
and decode a message.
To have the system running with a real-life application, the singular value
decomposition (SVD) on the ML310-Board is implemented, with the num-
ber of processors varying from one to eight, and with matrices varying from
(4×200) to (128×200). The performance increase because of the use of mul-
tiprocessors was shown to be almost linear in the number of microprocessors.
As the bulk of the computations in the SVD is the computation of the
dot-products of the column pairs, which is a good candidate for hardware
implementation. Custom multiply accumulate (MAC) modules for computing

System on a Programmable Chip
283
Figure 8.12.
4-Processor infrastructure for the SVD on the ML310-Board
those column-pair multiplications were designed and directly attached con-
nected to the MicroBlaze processors through the fast simplex link (FSL) ports
(ﬁgure 8.12, to increase the performance of the whole system.
2.7
Adaptivity
As stated at the beginning of the chapter, the adaptivity can be used for
several purposes. According to the application, a physical topology consisting
of a given arrangement of routers and processors may be built to match the best
computation paradigm of that application. The topology can be modiﬁed, even
at run-time to better adapt to the changes observed in the application.
On the other hand, the adaptivity of the system can be reached by using
reconﬁgurable hardware accelerators, provide that the target device supports
partial reconﬁguration.
Designing partially reconﬁgurable systems with unlimited capabilities is a
complex engineering task that requires a lot of hand work. To avoid this and
keep the thing very simple, a template for partial reconﬁguration can ﬁrst be
created for the target device. In this template, ﬁxed locations must be selected,
where the reconﬁguration is allowed to take place. It acts as place holder to
accommodate reconﬁgurable modules at run-time. Any hardware accelerator
can be plugged on the hardware block at run-time by means of partial recon-
ﬁguration. This step can even be performed from within the device using the
appropriated port like the ICAP in the case of Xilinx devices.

284
Reconﬁgurable Computing
3.
Conclusion
In this chapter, we have addressed the system on programmable chip paradigm
and provided some advantages of using those architectures. The main compo-
nents needed to build such a system were presented, with a focus on the leading
manufacturers. The goal was not a comparative study on the different system
on programmable chip infrastructure. We rather wanted to provide the user a
brief view on the existing solution.
In the second part of the chapter, we have presented a design approach for
multiprocessor systems on FPGAs. Using the reconﬁguration, it is possible to
modify the hardware infrastructure to adapt it to the best computation paradigm
of the application being computed.
We have presented a framework to specify and implement a complete sys-
tem without knowledge of the vendor tools. Obviously, all what is feasible
with the PinHat tool is also feasible with current vendor tools. However, the
complexity and the difﬁculty of using those tools do not allow any new comer
to generate design with it. The goal here was to hide the complexity in the de-
sign of multiprocessor, which is very limited in the current tool to the user and
allow him to focus on the efﬁcient analysis and partitioning of its application.
We believe that multiprocessing on FPGA has a great potential to speed-
up computation in embedded system. To facilitate their use, systems like the
PinHat are welcome to hide the complexity of the hardware generation and let
the designer focus on the analysis of the application and the deﬁnition of the
best adapted structure for its computation.
The adaptivity of the whole system is provided through the use of partial re-
conﬁguration to exchange running hardware modules at run-time. However, to
overcome the difﬁculty of designing for partial reconﬁguration with the current
tool, a template-based approach is recommended, which foresee predeﬁned lo-
cation for placing components at run-time.

Chapter 9
APPLICATIONS
Up to now, we have focussed on the technology behind reconﬁgurable de-
vices, their integration in different systems and their programmability at dif-
ferent level of abstraction. The natural question which arises is the one to
know, in which area does reconﬁgurable computing could be helpful? If areas
of application exist, then we would also like to know what the gain of using
reconﬁgurable technology is, compared with other processing devices used be-
fore? The gain of using a solution rather another one is usually expressed as
the computation speed, or as the improvement in power consumption, or as
the price, or even as the easiness in the programmability. In [57], Craven and
Athanas provided a performace/price comparative study between FPGA-based
high-performance computing machines and traditional supercomputers. On the
basis of the gain in accelerating ﬂoating-point computations, they came to the
conclusion that the price to pay was too high compared with the marginal gain
obtained, when using FPGAs in supercomputers. Other researchers have re-
ported amazing gains with FPGA-based computing machines in several ﬁelds
of computation, and several companies that have been offering FPGA-based
computing solutions a decade ago are still operating and even growing. Our
goal in this chapter is to present some applications that beneﬁt somehow from
an implementation on reconﬁgurable devices. Almost all the experiments re-
ported were done on FPGA platforms. We will not report all the experiments in
all details. We rather focus on the coarse-grained structure of the applications
and point out where the reconﬁguration can be helpful. We are primary con-
cerned with the speed improvement and the use of ﬂexibility of reconﬁgurable
computing systems.
Despite the very large number of applications that could beneﬁt from an
implementation in FPGAs, we have selected only 6 domains of application

286
Reconﬁgurable Computing
that we present here: pattern matching, video streaming, signal processing
using distributed arithmetic, control, and super computing.
1.
Pattern Matching
Pattern matching can be deﬁned as the process of checking if a character
string is part of a longer sequence of characters. Pattern matching is used in a
large variety of ﬁelds in computer science. In text processing programs such
as Microsoft Word, pattern matching is used in the search function. The pur-
pose is to match the keyword being searched against a sequence of characters
that build the complete text. In database information retrieval, the content of
different ﬁelds of a given database entry are matched against the sequence of
characters that build the user request. Searching in genetical database also use
pattern matching to match the content of character sequence from a database
entries with the sequence of characters that build a given query. In this case,
the alphabet is built upon a set of a genomic characters. Speech recognition
and other pattern recognition tools also use pattern matching as basic oper-
ations on top of which complex intelligent functions may be built, to better
classify the audio sequences. Other applications using pattern matching are:
dictionary implementation, spam avoidance, network intrusion detection and
content surveillance.
Because text mining, whose primary role is the categorization of documents,
makes a heavy use of pattern matching, we choose in this section to present
the use of pattern matching in text mining and to point out the possible use of
reconﬁguration.
Documents categorization is the process of assigning a given set of docu-
ments from a database to a given set of categories. The categories can either be
manually deﬁned by a user or be computed automatically by a software tool.
In the ﬁrst case, the categorization is supervised and in the second case, we
have an unsupervised categorization. In categorization, the ﬁrst step usually
consist of indexing the collection of available documents. This is usually done
through the so-called vector space representation. In this model, a document is
represented as a vector of key words or term present in the document. A com-
plete collection of n documents over a list of m keywords is then represented
by a term by documents matrix A ∈Rm × Rn. An entry aij in A represents
the frequency of the word i in the document j. The term by documents ma-
trix is then used for indexing purpose or statistical analysis like latent semantic
indexing (LSI) [67]. Building a term by documents matrix is done by scan-
ning the documents of the given collection to ﬁnd the appearance of key words
and return the corresponding entry in the matrix for each document. Pattern
matching is therefore used for this purpose.
The ﬁrst advantage of using the reconﬁgurable device here is the inherent
ﬁne-grained parallelism that characterizes the search. Many different words

Applications
287
can be searched for in parallel by matching the input text against a set of words
on different paths. The second advantage is the possibility of quickly exchang-
ing the list of searched words by means or reconﬁguration.
Pattern matching was investigated in different work using different approa-
ches on FPGAs [52] [85] [104] [151] [101] [180] [34], each with a different
overhead in compiling the set of words down to the hardware and different ca-
pacities. We deﬁne the capacity of a search engine as the number of words that
can de searched for in parallel. A large capacity also means a high complexity
of the function to be implemented in hardware, which in turn means a large
amount of resources. The goal in implementing a search engine in hardware is
to have a maximal hardware utilization, i.e. as many words as possible that can
be searched for in parallel. We present in this section various hardware imple-
mentation of the pattern matching, each with its advantages and drawbacks.
1.1
The Sliding Windows Approach
One approach for text searching is the so-called sliding window (SL) [85]
[151]. In the 1-keyword version, the target word is stored in one register, each
character being stored in one register ﬁeld consisting of a byte. The length of
the register is equal to the length of the word it contains. The text is streamed
through a separate shift register, whose length is the same as that of the key-
word. For each character of the given keyword stored as a byte in one register
ﬁeld, an 8-bit comparator is used to compare this character with the corre-
sponding character of the text, which streams through the shift register. A hit
occurs when all the comparators return the value true.
The sliding window can be extended to check a match of multiple patterns
in parallel. Each target word will then be stored in one register and will have as
many comparators as required. The length of the window (the number of char-
acters in the shift register) is deﬁned by the segment of the text to be processed.
Words with length bigger than the length of the window cannot be handled. To
overcome this problem, one can deﬁne the length of text to be considered as
the maximum length over all the target words. In this case, all the words with a
length smaller than the maximum length should be ﬁlled with do not care char-
acters to be handled correctly. The example of ﬁgure 9.1 shows the structure
of a sliding windows with three key words.
The main advantage of this approach resides in the reconﬁguration. Be-
cause each keyword is stored in an independent register, the reconﬁguration
can happen without affecting the other words, thus providing the possibility to
gradually and quickly modify the dictionary.1
1With dictionary, we mean here the set of keyword compiled into the reconﬁgurable device.

288
Reconﬁgurable Computing
Figure 9.1.
Sliding windows for the search of three words in parallel
This method however has a main drawback, which is the redundancy in the
amount of register ﬁelds used to store the words as well as the number of com-
parators. Redundancy in the sliding windows approach reduce the capacity of
such implementation, thus making its use not competitive. In the case of the
Xilinx Virtex FPGA XCV300 with a maximum of 6144 ﬂip ﬂops, a sliding
windows of length 10 needs 10 × 8 = 80 ﬂip ﬂops to store one target word in
the device. Consider that the device placement and routing permits an utiliza-
tion of 80%. The number of words that can be folded in such an FPGA will
theoretically be in the range of 60, which is small compared with the number
of words that can be folded in the same divide with a more efﬁcient implemen-
tation. A nice approach would be to just perform the comparison on a given
alphabet consisting of the set of common character and to use the result in the
evaluation of each single word. This would lead to a reduce amount of register
ﬁelds and a reduce amount of comparators.
1.2
Hashed Table-Based Text Searching
The hashed table-based text searching approach was implemented on the
SPLASH II Platform [180]. The text to be processed is streamed in a pair of
consecutive characters called superbyte. An incoming superbyte is mapped on
one character of the target alphabet. Each non-alphabet character is mapped to
a delimiter. To determine whether or not a word is in the target list, a hashing

Applications
289
technique is used. A hash table in memory is used to store the value of a pres-
ence bit for a particular target word. An entry zero in this table means that
the word is not present and a value one means that the word will probably be
present. A hash register (with a length of 22 bit in the SPLASH implementa-
tion), which is initially set to zero, is incrementally modiﬁed by the incoming
superbytes. At the end of a word marked by a delimiter, the hash register is
set to zero to allow the next word to be processed. The hash register is used to
address the hash table of 222 pseudo-random mappings of words. The modiﬁ-
cation of the hash register happens as follows: When a non-delimiter superbyte
is encountered, the contents of the 22-bit hash register is updated by ﬁrst XOR-
ing the upper 16-bit of the register with the incoming superbyte and a value of a
hash function. The modiﬁed value of the hash register is then circularly shifted
by a ﬁxed number of bits. Upon reaching the end of a word, the content of the
register is used to address the hash table and determine the value of the pres-
ence bit. To reduce the likelihood of false hits, a number of independent hash
functions is calculated for each word of the text, with the stipulation that each
hash function lookup of a word must result in a hit for that word to be counted
as a target word.
Estimations on performance of the SPLASH text searching have been done
mainly on the basis of the critical path length returned by the place and route
tool. Communication between the host computer and the boards such as mem-
ory latency has not been taken into consideration.
1.3
Automaton-Based Text Searching
It is well known that any regular grammar can be recognized by a deter-
ministic ﬁnite state machine (FSM). In an automaton-based search algorithm,
a ﬁnite state machine is built on the basis of the target words. The target words
deﬁne a regular grammar that is compiled in an automaton acting as a recog-
nizer for that grammar. When scanning a text, the automaton changes its state
with the appearance of characters. Upon reaching an end state, a hit occurs and
the corresponding word is set to be found. One advantage of the FSM-based
search machine is the elimination of the preprocessing step done in many other
methods to remove stop words (such as ‘the’, ‘to’, ‘for’ etc., which does not
affect the meaning of statements) from documents. In [52], the implementation
of the hardware-based retrieval machine is done using a memory to store the
transition table. Two registers hold the current state and the incoming charac-
ter. The logic is only used to decode the address of the next state in memory
and to set the correct values in the registers.
The performance (speed) of such an implementation is governed by the path
(RAM output →state register →RAM address →RAM output), which can
considerably slow down the search process, because of the multiple memory
accesses. To increase the speed of an FSM implementation, memory accesses

290
Reconﬁgurable Computing
can be suppressed by compiling the transition table in hardware and having
a mechanism to execute the state machine. Folding big transition tables in
FPGAs can be difﬁcult because of the limited capacity of those devices. Transi-
tion tables for word recognizers are often full of backtracking edges and cross-
ing edges (edge that connects two nodes on different paths in the FSM-tree).
With the sequential structure of the memory-based recognizer, it is almost im-
possible to eliminate the backtracking edges, which constitute in most cases
the biggest part of the transition table.
The SPACE (Scalable Parallel Architecture for Concurrency Experiments)
machine [104] [105] makes use of the parallel structure of FPGAs to eliminate
backtracking edge from the FSM. This search machine is a FSM-like imple-
mentation for text searching capable of handling a search over a list of almost
100 target words. The search is performed simultaneously on a board with 16
CAL1024 FPGAs without using a memory to store the transition table. Each
key word is compiled in a separate FSM that is mapped to the hardware device.
The text is then streamed into the device, character by character in ASCII form.
For the case insensitivity purpose the 8-bit characters are mapped by the pre-
processor to 5-bit characters. With the incoming characters, which clock the
ﬂip ﬂops, all the FSM move simultaneously to their next state. Upon reaching
an end state, the corresponding accumulator is incremented and the result is
returned. The backtracking edges appearing in the formal representation of the
FSM become redundant and will not be taken into consideration. Figure 9.2a
shows a transition graph of the FSM for the word ‘conte’. The transition table
requires 5 × 6 memory locations to be stored (ﬁgure9.2b). For the same FSM,
the hardware representation will need only 4 ﬂip ﬂops, 4 AND gates, and 5
comparators (ﬁgure 9.2c).
While having the advantage of replacing a word already folded in the FPGA
with a new target word of the same length, the SPACE implementation presents
two major inconveniences: First, it does not take into consideration words that
share a common preﬁx, thus increasing the amount of resources used to store
the character states. Second, it uses one comparator for each character of a tar-
get word, which increases the resource needed to implement the comparators.
Those two factors lead to redundancy in ﬂip ﬂop and look-up table utilization
as it is the case in the sliding window recognizer. On the SPACE board, with
a total of 8192 ﬂip ﬂops, it has been possible to implement a list of only 96
target words with average length 12.
When taking into consideration the common preﬁx of words, it is possible to
save a considerable amount of ﬂip ﬂops. For this purpose, the search machine
could be implemented as an automaton recognizer, common to all the target
words. This approach was ﬁrst presented in [34]. As shown in ﬁgure 9.3, the
resulting structure is a tree in which a path from the root to a leaf determines the
appearance of a corresponding key word in the streamed text. Words that share

Applications
291
Figure 9.2.
FSM recognizers for the word ‘conte’: a) sate diagram, b) transition table, c) basis
structure the hardware implementation: 4 ﬂip ﬂops will be need to code a 5 × 6 transition table
a common preﬁx use a common path from the root corresponding to the length
of their common preﬁx. A split occurs at the node where the common preﬁx
ends. In the hardware implementation of a group of words with a common
preﬁx, common ﬂip ﬂops will be used to implement the common path (ﬁgure
9.3a).
For each character in the target alphabet, only one comparator is needed.
The comparison occurs in this case, once at a particular location in the device.
Figure 9.3.
a) Use of the common preﬁx to reduce the number of ﬂip ﬂops of the common
word detector for ‘partir’, ‘paris’, ‘avale’,‘avant’. b) implementation without use of common
preﬁx and common comparator set

292
Reconﬁgurable Computing
Incoming characters can be directly sent to a set of all possible comparators.
Upon matching a particular one, a corresponding signal is sent to all the ﬂip
ﬂops, which need the result of that comparison. This method will reduce the
number of comparators needed almost by the sum of the length of all target
words.
The overall structure of the search engine previously explained is given in
ﬁgure 9.4. It consists of an array of comparators to decode the characters of
the FSM alphabet, a state decoder that moves the state machine in the next
states and a preprocessor to map incoming 8-bit characters to 5-bit charac-
ters, thus mapping all the characters to lower cases. As case insensitivity is
considered in most application in information retrieval, the preprocessor is
designed to map upper and lower case characters to the binary code of 1 to
26 and the rest of character to 0 [104]. Characters are streamed to the de-
vice in ASCII notation. An incoming 8-bit character triggers the clock and
is mapped to a 5-bit character that is sent to the comparator array. All the
5-bit signals are distributed to all the comparators that operate in parallel to
check if a match of the incoming character with an alphabet character occurs.
If a match occurs for the comparator k, the output signal chark is set to one
and all the others are set to zero. If no match occurs, all the output signals
are set to be zero. Figure 9.5 illustrates this concept on the basis of the word
‘tictic’.
The one-hot approach (ﬁgure 9.2c) is probably the best implementation for
the kind of automaton realized in the state decoder. First, it takes a major
advantage toward the regular structure of the FPGA, and it is the fastest way to
implement FSM in hardware [142]. Moreover, there is no need to incorporate
backtracking edges in a suitable hardware representation. To implement a set
of different words, the resulting tree structure of the common FSM is mapped
in hardware. All the leafs corresponding to target words are connected to the
hit port. The width of the hit port is determined by the number of positions
needed to code the number of target words. Upon matching a particular word,
Figure 9.4.
Basic structure of a FSM-based words recognizer that exploits the common preﬁx
and a common set of characters

Applications
293
Figure 9.5.
Processing steps of the FSM for the word ‘tictic’
one leaf of the automaton is reached, and the hit output is set as to the index of
the matched target word.
1.4
Automaton generation
A tool was provided in [34] to allow for an automatic generation of a term by
document matrix from a document collection with a given list of target words.
This tool automatically generates a VHDL description of the search machine,
and the synthesis can be done with available vendors or third party tools. The
tool is based on the method described in [14] to generate the corresponding
tree structure that is then mapped in hardware. The resulting state machine is
optimized (in terms of number of states) and is able to solve the Mississippi
problem2 for a set of target words. With the union, concatenation, Kleene star
(Kleene closure) operations, all the basic FSMs (FSM related to one word) are
used to build one FSM capable of recognizing all the target words. A set of
ﬁnal states is linked to the appearance of target words in a streaming text. The
resulting transition table is then translated into VHDL code in such a way that
2Can the state machine recognize the word ‘issip’ in a ﬁle containing the word Mississippi?[52]

294
Reconﬁgurable Computing
no backtracking and crossing edges appear in the conﬁguration. The resulting
VHDL code is then synthesized, mapped, placed, routed and conﬁgured to be
downloaded into the FPGA with the help of the generated scripts. The docu-
ment collection is then scanned and a column vector representing the weight
of target words in this document is returned for each document. The perfor-
mance of this approach was evaluated using the Spyder Virtex board from the
FZI Karlsruhe [214], a PCI board featuring a Xilinx Virtex XCV300. A list of
330 key words with an average of 10 characters each could then be compiled
into the FPGA, leading to an automaton with 2370 states. The utilization of
the FPGA was 59% (1817 CLB slices out of 3072 slices), including bus inter-
face. The stream rate of the characters through the FPGA was by 3 million3
characters per second.
2.
Video Streaming
Video streaming is the process of performing computations on video data,
which are streamed through a given system, picture after picture.
Implementation of video streaming on FPGA have attracted several resear-
chers in the past, resulting in the building of various platforms [39] [115] [198]
[72] [207] [140] [201] for various computations. Two main reasons are always
stated as motivation for implementation video streaming in FPGAs: the perfor-
mance, which results from using the exploitation of the inherent parallelism in
the target applications to build a tailored hardware architecture, and the adap-
tivity, which can be used to adjust the overall system by exchanging parts of
the computing blocks with better adapted ones.
In [193], Richard G.S discusses the idea of parameterized program gener-
ation of convolution ﬁlters in an FPGA. A 2-D ﬁlter is assembled from a set
of multipliers and adders, which are in turn generated from a canonical serial-
parallel multiplier stage. Atmel application notes [13] discuss 3 x 3 convolu-
tion implementations with run-time reconﬁgurable vector multiplier in Atmel
FPGAs. To overcome the difﬁculties of programming devices with classic
Hardware Description Languages (HDL) such as VHDL and Verliog, Celox-
ica has developed the Handel-C language. Handel-C is essentially an extended
subset of the standard ANSI-C language, speciﬁcally designed for use in a
hardware environment. The Celoxica Handel-C compiler, the DK1 develop-
ment environment includes a tool, the PixelStream, for an easy generation of
video processing functions for FPGA implementations. PixelStream offers a
set of basic modules and functions that can be (graphically) combined to build
a more complex datapath in FPGA. The resulting design can be mapped to one
of the Celoxica FPGA development boards.
34 million if less than 255 words are compiled into the word recognizer.

Applications
295
We will not focus in this section on the details of video or image process-
ing. A comprehensive description of video processing algorithms and their
hardware implementation on FPGA is provided in [153].
The Sonic architecture [115] is a conﬁgurable computing platform for accel-
eration and real-time video image processing. The platform consists of plug-
in processing elements (PIPEs), which are FPGA daughter card that can be
mounted on a PCI-board plugged on a PCI-slot of a workstation. A PIPEﬂow
bus exists to connect adjacent PIPES, while the available PIPE provide global
connection to the PIPES. Sonic’s architecture exploits the reconﬁguration ca-
pabilities of the PIPES to adapt part of the computation ﬂow at run-time.
The Splash [97] architecture was also used in video processing. Its systolic
array structure makes it well suited to image processing.
The ESM platform introduced in Section 7.2 presents an optimal pipelined
architecture for the modular implementation of video streams. Its organization
in exchangeable slots, each of which can be reconﬁgured at run-time to per-
form a different computation, makes it a viable platform in video streaming.
Moreover, the communication infrastructure available on the platform provides
an unlimited access of modules to their data, no matter on which slot they are
placed, thus increasing the degree of ﬂexibility in the system.
The computation on video frames is usually performed in different steps,
while the pictures stream through the datapath. It therefore presents a nice
structure for a pipelined computation. This has led to a chained architecture on
the basis of which most video streaming systems are built. The chain consist
of a set of modules, each of which is specialized for a given computation. The
ﬁrst module on the chain is in charge of capturing the video frames, while the
last module output the processed frames. Output can be rendered on a monitor
or can be sent as compressed data over a network. Between the ﬁrst and the
last modules, several computational modules can be used according to the algo-
rithm implemented. A module can be implemented in hardware or in software
according to the goals. While software provides a great degree of ﬂexibility,
it is usually not fast enough to carry the challenging computations required in
video applications. ASICs can be used to implement the computational de-
manding parts; however, ASIC does not provide the ﬂexibility needed in many
systems. On a reconﬁgurable platform, the ﬂexibility of a processor can be
combined with the efﬁciency of ASICs to build a high-performance ﬂexible
system. The partial reconﬁguration capability of reconﬁgurable devices pro-
vides the possibility to replace a given module on the chain at run-time.
Most of the video capture modules provide the frames to the system on a
pixel by pixel manner, leading to a serial computation on the incoming pix-
els. As many algorithms need the neighbourhood of a pixel to compute its new
value, a complete block must be stored and processed for each pixel. Capturing

296
Reconﬁgurable Computing
the neighbourhood of a pixel is often done using a sliding window data struc-
ture with varying size. The size of the sliding window corresponds to that of
neighbour region of a pixel needed for the computation of the new value. As
shown in ﬁgure 9.6, a sliding window is a data structure used to sequentially
capture the neighbourhood of pixels in a given image.
A given set of buffers (FIFO) is used to update the windows. The number
of FIFOs vary according to the size of the window. In each step, a pixel is
read from the memory and placed in the lower left cell of the window. Up to
the upper right pixel that is disposed, i.e. outputted, all the pixels in the right
part of the window are placed at the queue of the FIFOs one level higher. The
processing part of the video is a normal image processing algorithm combining
some of the basic operators like:
Median ﬁltering
Basic morphological operations
Convolution
Edge detection.
In the ﬁeld of video compression, the processing is usually done in a block
by block manner, different from the sliding window. However, the overall
structure is almost the same. The neighbourhood must always be saved to
provide the necessary data for the computation of a new pixel value.
As explained earlier in this section and as ﬁgure 9.7 shows, the architec-
ture for a video streaming system is usually built on a modular and chained
basis.
The ﬁrst module deals with the image captured from an image source. This
can be a camera or a network module that collects the picture through a net-
work channel, or any other source. The frames are alternately written to the
RAM1 and RAM2 by the capture module. The second module collects the pic-
tures from the RAM1 or RAM2, if this is not in use by the ﬁrst module, builds
FIFO1
FIFO2
W11
W12
W13
W21
W22
W23
W31
W32
W33
Disposed
W14
W15
W24
W25
W34
W35
W41
W42
W43
W51
W52
W53
W44
W45
W54
W55
RAM
FIFO3
FIFO4
Figure 9.6.
Implementation of a 5 × 5 sliding windows

Applications
297
Figure 9.7.
A modular architecture for video streaming on the ESM
the sliding windows and passes it to the third module, which processes the
pixel and saves it in its own memory or directly passes it to the next module.
This architecture presents a pipelined computation in which the computational
blocks are the modules that process the data frames. RAMs are used to tempo-
rally store frames between two modules, thus allowing a frame to stream from
RAM to RAM and the processed pictures to the output.
2.1
Enabling Adaptivity through Reconﬁguration
An adaptive video streaming system is characterized by its ability to opti-
mize the computation according to changing environmental condition. In most
cases, only one module on the computation chain must be changed while the
rest keep running. The video capture module for example can be changed
if we want to optimize the conversion of pixel to match the current bright-
ness or the current landscape. It is also possible to change the video source
from camera to a new one with different characteristics or the source can be
for instance a network channel collecting the frames in a compressed form.
In an adaptive system, the functionality of a module on the computation path
should be changed very fast and without affecting the rest of the system. Two
possibilities exist for this purpose. The ﬁrst one consists of providing some
parameters to the module to instruct it to switch from one algorithm to the
next one. However, the structure of the basic algorithms are not always the
same. A Sobel ﬁlter cannot be changed in a Laplace ﬁlter by just changing
the parameters. This is also true for a median operator that cannot be replaced

298
Reconﬁgurable Computing
by a Gauss operator by just changing the parameters. The network capture
module and the camera capture module require two different algorithms for
collecting the pixels and bringing them in the system. The second possibility
consists of replacing the complete module at run-time with a module of the
same size, but different in its structure, while the rest of the system keeps run-
ning. Reconﬁgurable devices in general and FPGAs in particular fulﬁll this
requirements. Many available FPGAs can be partly conﬁgured, while the rest
of the system is running. Moreover, many FPGAs provide small size on-chip
memories, able to hold part of a frame (the so-called region of interest). It
is therefore possible to perform many computations in parallel on different
regions of interest, thus increasing the performance and ﬂexibility of video
applications.
3.
Distributed Arithmetic
Distributed arithmetic (DA) is certainly one of the most powerful tool for the
computation of the product of two vector products, one of which is constant,
i.e it consists of constant values. DA exploits the nature of LUT-based com-
putation provided by the FPGAs by storing in an LUT, all the possible results
for a set of variable combinations. Computation at run-time only consists of
retrieving the results from the LUT, where they were previously stored. The el-
ements of the variable vector are used to address the look-up table and retrieve
partial sums in a bit-serial (1-BAAT, 1 Bit At A Time) manner. Investigations
in distributed arithmetic have been done in [215] [158] [223] [224] [33] [95]
[63] [62] [31].
One of the notorious DA-contribution is the work of White [215]. He pro-
posed the use of ROMs to store the precomputed values. The surrounding logic
to access the ROM and retrieve the partial sums had to be implemented on a
separate chip. Because of this moribund architecture, distributed arithmetic
could not be successful. With the appearance of SRAM based FPGAs, DA
became an interesting alternative to implement signal processing applications
in FPGA [158] [223] [224]. Because of the availability of SRAMs in those
FPGAs, the precomputed values could now be stored in the same chip as the
surrounding logic.
In [33], the design automation of DA architecture was investigated, and a
tool was designed to help the user in the code generation of a complete DA
design, and to perform investigations on the various tradeoffs. Also, an initial
attempt to implement DA for ﬂoating-point numbers to increase the accuracy
was presented. However, the amount of memory required to implement such a
solution makes it applicable only on small examples.

Applications
299
The idea behind the distributed arithmetic is to distribute the bits of one
operand across the equation to be computed. This operation results in a new
one, which can then be computed in a more efﬁcient way. The product of two
vectors X and A is given by the following equation:
Z = X × A =
n

i=0
(Xi × Ai)
(3.1)
We assume that A = (A1, · · · , An) is a constant vector of dimension n and
X = (X1, · · · Xn) is a variable vector of the same dimension. With the binary
representation, w−1
j=0 Xij2j, of Xi, where w is the width of the variables and
Xij ∈{0, 1} is the j-th bit of Xi, equation (3.1) can be rewritten as:
Z =
n

i=0
Ai ×
w−1

j=0
Xij2j =
w−1

j=0
2j
n

i=0
XijAi
(3.2)
Spreading equation 3.2 for a better understanding leads to equation 3.3
Z = (X00 × A0 + X10 × A1 + ... + Xn0 × An)20
+ (X01 × A0 + X11 × A1 + ... + Xn1 × An)21
...
+ (X0(w−1) × A0 + X1(w−1) × A1 + ... + Xn(w−1) × An)2(w−1)
(3.3)
Equation 3.2 represents the general form of a distributed arithmetic.
It
shows how the variable operands are distributed in the equation. Each bit of
each variable operand contributes only once to the sum n
i=0 XijAi. Because
Xij ∈{0, 1}, the number of possible values n
i=0 XijAi can take is limited to
2n. Therefore, they can be precomputed and stored in a look-up table, provide
that enough place is available in the LUT. Lets call such a look-up table a dis-
tributed arithmetic look-up table (DALUT). The DALUT has the size w × 2n
bits. At run-time, the n-tupel (X1j, X2j, ..., Xnj) will be used to address the
DALUT and retrieve the partial sums n
i=0 XijAi. the n-tupel (0, 0, ..., 0) will
be used to address the location 0, (0, 0, ..., 1) is used to address the location 1,
and (1, 1, ..., 1) is used to address the location 2n −1.
The complete dot-product Z requires w steps to be computed in a 1-BAAT
manner. At step j, the j-th bit of each variable are used to address the DA-
LUT and retrieved the value n
i=0 XijAi. This value is then left shifted by a
factor j (which corresponds to a multiplication by 2j) and accumulated. Af-
ter w accumulations, the values of the dot-product can be collected. The DA
datapath for this computation is obvious as ﬁgure 9.8 shows.

300
Reconﬁgurable Computing
Figure 9.8.
Architecture of a distributed arithmetic datapath
The DALUT address consists of a set of bits, each of which belongs to a
component of the input vector. A barrel shifter is used to shift the number
retrieved from the DALUT at the j-th step on j positions. The shifted value
is then added to the value in the accumulator. After w steps, the results are
collected from the accumulator.
Many enhancements can be done on a DA implementation. The size of the
DALUT for example can be halved if only positive values are stored. In this
case, the sign bit, which is the ﬁrst bit of a number, will be used to decide
whether the retrieved value should be added to or subtracted from the accumu-
lated sum. On the other hand, it is obvious that all the bit operations, i.e. the
retrievals of a value from the DALUT are independent from each other. The
computation can therefore be performed in parallel. The degree of parallelism
in a given implementation depends on the available memory to implement the
DALUTs. In the case where w DALUTs and datapaths can be instantiated in
parallel, the retrieval of all partial sums can be done in only one step, meaning
that the complete computation can be done in only one step instead of w as in
the serial case.
In general, if k DALUTs are instantiated in parallel, i.e. for a computation
performed on a k-BAAT basis, then w/k steps are required to retrieve all the
partial sums, which can be directly accumulated, thus leading to a run-time of
w/k steps. Figure 9.9 shows a datapath for the computation of the DA.
The input values are segmented in different ﬁelds, each of which is assigned
to a datapath for the retrieval of the partial sums from the corresponding DA-
LUT. The retrieved values from the k DALUTs are shifted by the correspond-
ing values and added to the accumulated sum. After w/k steps, the result can
be collected from the accumulator.

Applications
301
Figure 9.9.
k-parallel distributed arithmetic datapath
3.1
Distributed Arithmetic in High Dimension
In many computational areas such as in mechanical control, computations
are not only limited to dot-product. Matrix operations are usually required,
whose general form can be deﬁned by equation 3.4.
⎛
⎜
⎜
⎝
z1
z2
...
zs
⎞
⎟
⎟
⎠=
⎛
⎝
a11
...
a1r
...
a1s
...
asr
⎞
⎠
⎛
⎜
⎜
⎝
x1
x2
...
xr
⎞
⎟
⎟
⎠
(3.4)
This equation can be implemented using s DALUTs. The i-th (i ∈{1, .., s})
DALUT is used to compute the vector product zi = r
j=0 xj × aij with
the constants ai1 to air. If there is enough space on the chip to hold all the
DALUTs, then equation 3.4 can be computed in few clocks. As we will see
later, partial reconﬁguration can also be used to implement many dot-products,
if there is not enough memory on the chip to hold all the DALUTs.
3.2
Distributed Arithmetic and Real Numbers
The straightforward approach to handle real numbers in distributed arith-
metic is the use of ﬁxed-point. The representation of a real number using the
ﬁxed-point approach is done by deﬁning an integer part and a fractional part

302
Reconﬁgurable Computing
for each number. The integer part is separated from the fractional part using
a point. Because the point is only an imaginary and not physically available
in the number representation, operations on ﬁxed-point numbers are not differ-
ent than those on normal integer numbers. The datapath previously presented
can be used without modiﬁcations for real numbers represented as ﬁxed-point.
Representing real numbers as ﬁxed-point is advantageous in the computation
speed and the amount of resources required to implement the datapath. How-
ever, the ranges of ﬁxed-point representations as well as their precisions are
small compared to those of a ﬂoating-point representations. Therefore, we
would like to handle real numbers as ﬂoating-point.
In this section, we present a concept ﬁrst investigated in [33] for handling
real numbers, represented as ﬂoating-point in the IEEE 754 format, using dis-
tributed arithmetic. In IEEE 754 format, a number X is represented as follows:
X = (−1)SX2eX × 1.mX
(3.5)
In this equation, eX is the exponent (we consider that the substraction with
the bias is done and the result is eX), mX is the mantissa and SX is the sign
of X. Without loss of generality, we will consider the sign to be part of the
mantissa; thus, the new representation is X = 2eX × mX. With A, X and
Z being all ﬂoating-point numbers, the ﬂoating-point counterpart of equation
(3.1) is given by:
Z = X × A =
n

i=0
(Xi × Ai) =
n

i=0
(2eAi × mAi) × (2eXi × mXi)
=
n

i=0
(2eAi × 2eXi) × (mAi × mXi) =
n

i=0
(2eAi+eXi) × (mAi × mXi)
(3.6)
The goal here is to compute and provide Z as ﬂoating-point number, us-
ing the same principle of the partial sums previously described. At each step
of the computation, a ﬂoating-point value Fi must be read from the ﬂoating-
point DALUT and added to an accumulated sum, which is also a ﬂoating-
point number. As the adder used at this stage is a ﬂoating-point adder, we
assume that issues such as rounding and normalization have been integrated
in its implementation.
From the last part of equation 3.6, we see that the
value (2eAi+eXi) × (mAi × mXi) represents a ﬂoating-point number with ex-
ponent eAi + eXi and mantissa mAi × mXi. By setting eFi = eAi + eXi and
mFi = mAi × mXi, we have the requested values at each computation step.
Instead of computing the exponential part (2eAi+eXi) of Fi as well as its man-
tissa (mAi × mXij) online, the idea here consists of using two ﬂoating-point

Applications
303
DALUTS for each constant Ai. The values eAi + eXi will then be precom-
puted and saved in the ﬁrst DALUT and mAi × mXi in the second one. Let
us call the ﬁrst DALUT that stores the exponents the EDALUT and the sec-
ond DALUT that stores the mantissas the MDALUT. The size of EDALUT,
size(EDALUT), as well as that of MDALUT, size(MDALUT), is given in
equations (3.7).
size(EDALUT) = n × 2|E| × |E| bits
(3.7)
size(MDALUT) = n × 2|M| × |M| bits
(3.8)
|E| is the exponent width and |M| is the mantissa width of the ﬂoating-point
representation.
The main argument against this approach is the size of the DALUTs used
for this ﬂoating-point DA implementation, which can be so large that an im-
plementation cannot be possible. However, if we consider a DA implementa-
tion involving ﬁve variables and ﬁve coefﬁcients represented in the IEEE 754
ﬂoating-point format with 8 bits exponent and 10 bits mantissa, the total mem-
ory requirement for the EDALUTs and the MDALUTs is: ((5×28 ×8)+(5×
210 × 10))/1024 = 60 Kbits. Therefore, the EDALUTS and the MDALUTs
will easily ﬁt into a very small low cost FPGA such as the Xilinx Spartan III
50 with a total of 72 Kbits Block RAM [225].
For large vectors with numbers represented on large mantissas and expo-
nents, this approach will require a large amount of memory, certainly not di-
rectly available on the chip. An external memory must therefore be used.
Having the EDALUTs and the MDALUTs for each coefﬁcient, the datapath
will not be implemented as in the ﬁxed-point or integer case. The variables are
no more input in a bit-serial way. At step i, the variable Xi is used to address
the two i-th EDALUT and MDALUT. The bits of eXi are used to access the
EDALUT, while the bits of mXi are used to access the MDALUT in parallel.
The values collected from the EDALUT and the MDALUT are used to build
the ﬂoating point number Fi. After n steps, the ﬂoating-point dot-product is
computed. Figure 9.10 shows the datapath for the ﬂoating-point DA. As all
the DALUTs for the n coefﬁcients are available on the device, they can be
accessed in parallel to compute the dot-product in only one step.
Signal processing applications are usually based on the computation of the
dot-product of one vector of variables with a vector of constants. They are ideal
candidates for a DA implementation in reconﬁgurable hardware. As stated ear-
lier in this section, to facilitate the design of DA applications, a tool was im-
plemented in [33] for the design automation. Also, the possibility is given to

304
Reconﬁgurable Computing
eA1
eA1 + 1
eA1
2|E|
+
2|E|
eA2
2|E|
m x0
e x0
e xn
m xn
e x1
m x1
+
m
e Fi
Fi
m
e
Zi
Zi
LUTs 2
LUTs n
LUTs 1
A1
A1
A1
2|M|
m
m
m
+
e
An
An
2|M|
m
m
+
eA2
A2
A2
2|M|
m
m
Control
An
eAn
* 2
*
*
*
Figure 9.10.
Datapath of the distributed arithmetic computation for ﬂoating-point numbers
investigate the different tradeoffs for a given application. For a given device
and a given set of constants, different DA implementations can be generated,
from the full parallel implementation (w-BAAT), which can be computed in
only one step, to the full sequential DA (1-BAAT). For each possibility, the
user is provided the area and speed of the design. Real numbers can be han-
dled either as ﬁxed-point or as ﬂoating-point in the IEEE 754 format with the
technique previously deﬁned. The width of the mantissa as well as that of the
exponent has to be provided. For the ﬁnal design, the tool generates a descrip-
tion in the Handel-C language.
3.3
Use of Reconﬁguration
The modiﬁcation of design implemented as distributed arithmetic can be
done at run-time through the replacement of the DALUTs. The remaining part
of the design must not be changed, because it is the same for all designs. In
this case, the designer must not have to deal with all the cumbersome aspects
of partial reconﬁguration design. However, if the datawidth of the operators
changes, then the computing structure (shifter, adder, accumulator) must be
replaced as well. In this case, the partial reconﬁguration of the device cannot
be avoided. In Section 4, we present the use of reconﬁguration in DA designs,
using the concept of adaptive controller concept, which makes use of DA and
partial reconﬁguration.

Applications
305
3.4
Applications
In this section, we present one application, the implementation of recur-
sive convolution algorithm for time domain simulation of optical multimode
intrasystem interconnects that was substantially speeded-up through the use of
distributed arithmetic. Another application that has beneﬁt from the imple-
mentation as DA is the adaptive controller. Because Section 4 is devoted to
adaptive controller, we explain only the ﬁrst application here.
3.4.1
Recursive Convolution Algorithm of Time Domain Simulation
of Optical Multimode Intrasystem Interconnects
In general, an optical intrasystem interconnect contains several receivers
with optical inputs driven by transmitters with optical outputs. The intercon-
nections of transmitter and receivers are made by a passive optical waveguide,
which are represented as a multiport (ﬁgure 9.11) using ray tracing approach.
The transfer of an optical signal along the waveguide can be computed by a
multiple convolution process. Frequency domain simulation methods are not
applicable regarding the high number of frequency. Pure time domain simu-
lation methods are more efﬁcient if the pulse responses can be represented by
exponential functions. The application of a recursive method for the convolu-
tion of the optical stimulus signals at the input ports with the corresponding
pulse responses enables a time efﬁcient computation of the optical response
signals at the belonging output ports. The recursive formula to be implemented
in three different intervals is given by equation 3.9.
y(tn)
=
f0 · y(tn−1) +
(3.9)
f4 · x0 −f5 · x1 + f24 · x2 + f53 · x3 .
Because the values f0, f4, f4, f24 and f53 are constants, while tn−1, x0, x1,
x2, x3 are variables, equation 3.9 is best adapted for a DA-implementation.
 
 
 
  
   
  
   
  
   
Figure 9.11.
An optical multimode waveguide is represented by a multiport with several trans-
fer paths

306
Reconﬁgurable Computing
Workstation
1 interval
3 intervals
Sun Ultra 10
73.8 ms
354.2 ms
Athlon (1.53 GHZ)
558 ms
1967.4 ms
FPGA (time)
1 interval
3 intervals
Custom dot-product design
25.6 ms
76.8 ms
Sequential DA
19.4 ms
19.4 ms
3-parallel DA
6.4 ms
6.4 ms
FPGA (area)
1 interval
3 intervals
Custom dot-product design
could not ﬁt
could not ﬁt
Sequential DA
yes (7% )
yes (14% )
3-parallel DA
yes (14% )
yes (42% )
Table 9.1.
Results of the recursive convolution equation on different platforms
For this equation, different tradeoffs were investigated in the framework pre-
sented in [33], for generation and evaluation of DA-trade-off implementation.
A Handel-C code was generated, and the complete design was implemented on
a system made upon the Celoxica RC1000-PP board equipped with a Xilinx
Virtex 2000E FPGA and plugged into a workstation.
Table 9.1 provides the implementation results on different platforms. The
workstation is used for sending the variable to the FPGA and collecting the
result of the computation. The implementation of equation 3.9 in three in-
tervals occupies about 14% of the FPGA area while running at 65 MHZ. To
obtain a maximal parallelism in the computation, 6 DALUT could be imple-
mented in parallel leading to 6 times speed-up. As can be seen on ﬁgure 9.12,
Figure 9.12.
Screenshot of the 6-parallel DA implementation of the recursive convolution
equation on the Celoxica RC100-PP platform

Applications
307
the 6-parallel DALUT and the corresponding adder three occupy about 76% of
the FPGA area. Enough space is left for the routing.
The same design was implemented without the use of DA. It could not
ﬁt into the same FPGA and the run-time was much bigger that of the DA-
implementation. The performance as well as the area consumption of our the
DA-implementation is more efﬁcient than that of all the other architectures.
4.
Adaptive Controller
In this section, we investigate the next ﬁeld of application of reconﬁgurable
devices, namely the implementation of adaptive controllers, also identiﬁed here
as multi-controller, using partial reconﬁguration.
We will ﬁrst introduce the multi-controller concept. Thereafter, we inves-
tigate its implementation using the distributed arithmetic. Finally, the imple-
mentation of the whole design using partial reconﬁguration is shown.
4.1
Adaptive Control
The task of a controller is to inﬂuence the dynamic behaviour of a system
referred to as plant. A control feedback is available, if the input values for the
plant are calculated on basis of the plant’s outputs.
Adaptive controllers deal with the control of a plant, for instance a com-
plex mechatronic system, in a changing environment. In this case, the control
is modeled as a physical process that operates in a limited set of operating
regimes. Depending on the environmental conditions, the operation regime
of the plant must be changed to adapt to new conditions. With conventional
methods, it might be possible to design one robust controller that optimally
controls the plant in all operating regimes. According to the conditions, the
control regime may be changed using a set of parameters. Besides the possible
slow response of such a controller [166], the size can be too big due the large
amount of possible cases to be considered. To overcome this handicap, the
controller modules can be implemented as separate modules and stored in a
database. They will be used at run-time to replace the active controller, when-
ever a new control algorithm must be implemented. The price to pay for this
solution is the amount of memory to store all possible module implementations
as well as the time needed to switch from one module to the next one.
The general architecture of an adaptive controller as described in [161] is
shown in ﬁgure 9.13. It consists of a set of controller modules (CM), each of
which is optimized for an operating regime of the plant. A supervisor com-
ponent is used to switch from module to module and set the active one.4 The
decision to switch from one CM to the next one is made on the basis of mea-
4At a given time, the active controller is the one that controls the plant

308
Reconﬁgurable Computing
Plant
MUX
i
Multi−Controller
CM 1
CM i
CM n
Supervisor
Figure 9.13.
Adaptive controller architecture
surements of physical values of the plant. The strategy of the supervisor can
vary from simple Boolean combinations of the input values to very complex
reasoning techniques [206].
The computation of the plant inputs is normally done in a time discrete
manner, using a set of discrete sample points tk . . . tk+1. The time between two
sampling points is constant and deﬁnes the sampling period T. The controller
application can therefore be seen as a periodic real-time task, which must be
performed at every sampling point and must be completed within one period
T.
While in a Von Neumann processor-based implementation of the system,
each module is implemented as sequence of instructions, in a reconﬁgurable
device, the tasks are implemented as hardware modules to be downloaded onto
the device at run-time. Let us take a look on the behaviour of the single mod-
ules and their implementations.
4.2
Controller-Module Speciﬁcation
A common approach is to model the plant as a linear time-invariant system.
Based on this model and the requirements of the desired system behaviour, a
linear controller is systematically derived using formal design methods.
Let us assume that every controller module CMi of ﬁgure 9.13 is designed
as a linear time-invariant system, optimized for a given operating regime of
the plant. After a time discretization has been performed, the behaviour can be
captured in equation 4.1.
The input vector of the controller module, representing the measurements
from sensors of the plant, is represented by u, y is the output vector of the
controller, which is used to drive the actuators of the plant, and x is the inner

Applications
309
state vector of the controller. The matrices A, B, C and D are used for the
calculation of the outputs based on the inputs.
x(k + 1)
=
Ax(k) + Bu(k)
y(k)
=
Cx(k) + Du(k)
(4.1)
p = dim(u),
s = dim(x),
q = dim(y)
The task of the digital controller is to calculate equation 4.1 during one
sampling interval. The new state xk+1 and the output yk must be computed
before the next sampling point k + 1.
The state space equations of a digital linear time invariant controller (equa-
tion 4.1) can be written as a product of a matrix of constant coefﬁcients and a
vector of variables (equations 4.2 and 4.3).
z(s+q,1) = M(s+q,s+p)v(s+p,1)
(4.2)
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x1(k + 1)
...
xs(k + 1)
y1(k)
...
yq(k)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦



z
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a11
. . .
a1s
b11
. . .
b1p
...
...
...
...
...
...
as1
. . .
ass
bs1
. . .
bsp
c11
. . .
c1n
d11
. . .
d1p
...
...
...
...
...
...
cq1
. . .
cqs
dq1
. . .
dqp
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦



M
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x1(k)
...
xs(k)
u1(k)
...
uq(k)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦



v
(4.3)
Equations 4.2 and 4.3 deﬁne a classical distributed arithmetic equation in
higher dimension. Each of the s + q entries of the resulting vector v deﬁnes
a separate DA equation. We therefore need to compute s + q different DA-
equations, all independent from each other. The computation of the s + q
equations can then be performed in parallel, provided that enough resources
are available on the chip to store all the DALUTs. If enough memory is not
available for storing all the DALUTs as well as the corresponding logic for the
datapath, then the different equations must be implemented sequentially. In
general, compromises must be found between the amount of parallelism that
one need to have and the speed of the design.
4.3
Use of Reconﬁguration
As we explained earlier, the controller modules can be implemented as hard-
ware modules and stored in a database from which they will be downloaded at
run-time by the supervisor module.
The straightforward approach to realize this is to build a structure consisting
of a ﬁxed part in which the supervisor resides. A reconﬁgurable slot is then
foreseen as place holder for the controller module that will be downloaded at
run-time (ﬁgure 9.14 left). Each controller module is downloaded at run-time

310
Reconﬁgurable Computing
Figure 9.14.
Adaptive controller architecture. Left: the one slot implementation, and right:
the two slot implemenation
into the reconﬁgurable slot, which has predeﬁned interfaces to the supervisor
and to the plant.
The problem with this approach is the vacuum, which arises on switching
from one module to the next one. Because only one slot is available, the recon-
ﬁguration of this slot will place the plant in a ‘ﬂoating state’ on reconﬁguration,
where it is not really controlled. To avoid this situation, the approach devel-
oped in [64] was the use of two slots. One active slot and a passive one (ﬁgure
9.14 right). The active slot is in charge of controlling the plant, while the re-
conﬁguration takes place in the passive slot. Whenever the supervisor decides
to replace a controller module, the reconﬁguration will ﬁrst take place on the
passive slot. Thereafter, the control of the plant will be given to the conﬁg-
ured module, which becomes the active one, while the former active becomes
passive.
5.
Adaptive Cryptographic Systems
Sensitive data exchange is always coupled with security issues. Cryptogra-
phy provides a means to allow two communicating entities to exchanged data
in such a way that a third party that intercepts the data will not be able to under-
stand their meaning. A protocol is therefore required, which allows the sender
to crypt the data in such a way that only the receiver will be able decrypt them.
Considerable advances have been done in communication in the past, and
this trend is likely to continue. The development of the internet has also pushed
the development in several branches where a network processing could not be

Applications
311
tough off few years ago. Applications like e-commerce, e-government, virtual
private network, on-line banking must provide a high degree of security.
Over years, a large variety of standards, such as Triple-DES, Advanced En-
cryption Standard (AES), Data Encryption Standard (DES), RSA, OpenPGP,
CipherSaber, IPsec, Blowﬁsh, ANSI X9.59, CAST, RC4 and RC6, have been
developed to provide high security. With this large variety of standards and the
customized implementation possibilities for each standard, cryptography can
be seen as one of the most versatile application domains of computer science.
Depending on criteria such as speed, degree of ﬂexibility and degree of secu-
rity, single implementations of cryptography application were developed either
as software or as intellectual property component.
In [221] the advantages of using reconﬁgurable hardware in cryptography
are listed. The author focus on the traditional advantage of ﬂexibility and per-
formance. The ﬂexibility is so far important because it offers the possibility
to use the same hardware to switch from one algorithm to the next one at
run-time, according to factors such as the degree of security, the computa-
tional speed, the power consumption. Also, according to some parameters,
a given algorithm can be tuned. Moreover, algorithms that has been broken
and where the security is no more insured can be changed by means of re-
conﬁguration. The system can easily be upgraded to include new standards,
developed while the system was already deployed. The corresponding algo-
rithm can therefore be compiled and included in the library of bitstreams for
the device conﬁguration. The second advantage provided by reconﬁgurable
hardware, namely the performance, can be used to efﬁciently implement the
components, by using the inherent parallelism and building efﬁcient operators
for computing Boolean operation on a very large amount of data. This results
on a large throughput and a cost efﬁciency. Experiments reported a throughput
of 12 GBit/s for an implementation of the block cipher AES on an FPGA Virtex
1000, using 12,600 slices and 80 RAMs [90], while an ASIC implementation,
the Amphion CS5240TK [150] clocked at 200 MHz, could provided twice
the throughput of the FPGA solution. The same algorithm, implemented on a
DSP TI TMS320C6201 provided a throughput of 112.3 Mbits/s [222], while a
throughput of 718.4 Mbit/s could be reached on a counterpart implementation
on a on a Pentium III [10].
The general architecture of an adaptive cryptographic engine proposed by
Prasanna and Dandalis [61] [178] basically consists of a database to hold the
different conﬁguration that can be downloaded at run-time onto the device,
like an FPGA for instance, to perform the computation and a conﬁguration
controller to perform the reconﬁguration, i.e. downloading the corresponding
bitstream form the database into the FPGA. Each bitstream represents a given
algorithm implementing a given standard and tuned with some parameters ac-
cording to the current user’s need.

312
Reconﬁgurable Computing
With the recent development in FPGA, it is possible to have a complete sys-
tem on programmable chip (processor, peripherals, custom hardware compo-
nents, interconnection) implemented on an FPGA. The conﬁguration controller
therefore must no more resides off chip. It can be implemented as custom on-
chip hardware module or as software running on an embedded processor. Also
the possibility to reconﬁgure only part of the chip opens new possibilities. In
the architecture presented in [61] [178], the whole device must be reset on re-
conﬁguration, thus increasing the power consumption because of the amount
of the data that must be downloaded on a chip. Power consumption is usually
a big problem in mobile environments, and the implementation must consider
such issues. Besides this power saving, partial reconﬁguration also provides
the possibility of keeping a skeleton structure into the device and perform only
few modiﬁcations at run-time on the basic structure to move from one algo-
rithm to the next one. In [42], a cryptographic application is implemented as
exchangeable module of a partial reconﬁgurable platform. The system, which
is based on the AES algorithm consumes only 954 Virtex slices and 3 block
RAMS. The cryptographic algorithm is used in this case just as a block to test
the partial reconﬁgurable platform, instead of using the partial reconﬁguration
to enhance the ﬂexibility of the cryptographic algorithm.
Figures 9.15 presents a possible architecture of an adaptive cryptography
system, using the previous mentioned advantages of partial reconﬁgurable de-
vices.
The main difference with the architecture presented in [61] is the use of
partial reconﬁguration, which allows for an integration of all components on a
Figure 9.15.
Architecture of an adaptive cryptographic system

Applications
313
single chip. Also, in contrast to the adaptive architecture for a control system
presented in ﬁgure 9.14, the loader module resides into the device. Depend-
ing on the implementation chosen, the loader can reside inside or outside the
device. If a Xillinx Virtex chip is used and the conﬁguration takes place via
the ICAP port, then the loader, which is the ICAP module in this case, is au-
tomatically available inside the device. However, if the conﬁguration happens
through the normal SelectMap port, then we need an external loader module
for collecting conﬁguration data from the database and copy them on the con-
ﬁguration port.
In ﬁgure 9.15, the architecture is logically divided into two main blocks.
A ﬁx one, which remains continuously on the chip. It consist of the parts,
which are common to all the cryptographic algorithms in general or common
to the algorithms in a given class only. On the ﬁgure, we show only one recon-
ﬁgurable slot; however, it can be implemented as set of conﬁgurable blocks,
each of which can be changed by means of reconﬁguration to realize a given
customized standard.
The last point concerns the development of building blocks that will be com-
piled in bitstreams to be downloaded into the device at run-time. A designer
is no more required to focus on the hardware implementation of the crypto-
graphic algorithm. A lot of work was done in this direction, and the results are
available. We need mostly to focus on the architecture of the overall system
and ﬁnd out how a viable partitioning can be done, according to the reconﬁgu-
ration scheme.
Most of the work have focussed in various implementations of a given ap-
proach like the RSA [185] in [155] [88] [154] [232] [50] or the implementa-
tions described in [78] [19] [20] [47] [145] [152] [15], mostly parameterizable
and based on the Elliptic Curve approach [138] [157]. Generators for pro-
ducing a customized description in a hardware description language have been
developed for example in [47]. This can be used to generate various conﬁgu-
rations that will be used at run-time to move from one implementation to the
next one.
6.
Software Deﬁned Radio
Software deﬁned radio (SDR) was deﬁned in 1991 by Joe Mitola [129] as
follow:
A software radio is a radio whose channel modulation waveforms are de-
ﬁned in software. That is, waveforms are generated as sampled digital signals,
converted from digital to analog via a wideband DAC and then possibly upcon-
verted from IF to RF. The receiver, similarly, employs a wideband Analog to
Digital Converter (ADC) that captures all of the channels of the software radio
node. The receiver then extracts, downconverts and demodulates the channel
waveform using software on a general purpose processor.

314
Reconﬁgurable Computing
As it is the case in many new technologies, SDR was initiated by the mil-
itary in an attempt to alleviate numerous problems associated with traditional
radio systems [162].
In the project SPEAKeasy [162], which can be seen
as precursor of the SDR technology, several military entities (DARPA, AIR
FORCE/AFRL, ARMY/CECOM, NAVY/NRaD/SPAWAR, NSA) joined ef-
forts in the development of ‘programmable’ radios to demonstrate the feasibil-
ity of a Multiband, Multimode Radio (MBMMR) and to develop technologies
that facilitate programmability and implementation of MBMMR. The achieve-
ment was a system with two programmable channels, the Texas Instrument
quad-TMS320C40 multi-chip module for digital signal processing, and a SUN
Sparc 10 workstation for interfacing with human.
The main goal in SDR is to move the analog–digital signal conversion as
closer as possible to the antenna and to process the digital signals in software
rather than using a set of dedicated hardware components. The expected bene-
ﬁts of using SDR have drained great attention and interest from governmental
entities, from the academia and from the industry.
SDR is the next evolutionary stage of wireless technology. Many believe
that it is just a question of time up to the adoption of SDR in the broad range
of communication facilities, and several initiatives like [121] [189] were born
with the goal of accelerating the proliferation of SDR.
The advantages of SDR can be categorized as follows:
Interoperability to provide support of multiple standards through multi-
mode, multiband radio capabilities
Flexibility for an efﬁcient shift of technology and resources
Adaptivity to provide a faster migration towards new standards and tech-
nologies through programmability and reconﬁguration
Sustainability for increasing resource utilization through generic hardware
platforms
Reduced ownership costs because of the reduced infrastructure, the lower
maintenance and the easier deployment
The general architecture of a SDR is presented in ﬁgure 9.16.
At least one digital to analog (DAC) and one analog to digital converter
(ADC) must ba available in a SDR system. While the ADC performs conver-
sion of the incoming analog signals to digital signals, the DAC performs the
inverse operation.
The signal processing part collects digital signals from the ADC or from
another source like a base station input, process them and send them to the
DAC.

Applications
315
Figure 9.16.
Architecture of a software deﬁned radio system
The ideal realization of a SDR would push the ADC and DAC as close as
possible to the antenna. The practical realization is, however, as explained in
[217], very challenging.
6.1
Use of Reconﬁguration
Reconﬁgurable devices provide the best prerequisites to fulﬁll the expecta-
tions (interoperability, ﬂexibility, adaptability, sustainability, and reduced own-
ership costs) placed on SDR systems. As explained in the previous section, it
is possible to realize a complete system consisting of a processor, peripher-
als and reconﬁgurable custom hardware modules as system on programmable
chip. In such a system, the maximal ﬂexibility will be insured by the processor,
while lower degree of ﬂexibility can be realized by means of reconﬁguration.
For many algorithm in signal processing, software alone will not be able to
provide the required performance. A migration of functions in hardware is
then required. The simple approach as realized in [9] [122] [48] is to pro-
vide a single coprocessor for computation intensive tasks, like the CORDIC
[209] implementation, directly attached to the processor. This solution pro-
vides the possibility to conﬁgure the system as whole, either to implement a
new functions at run-time or event to upgrade the system structure according
to the availabilities of new standards. The use of partial reconﬁguration [69]
will be welcomed, to allow the upgrading to take place without interrupting the
system, while saving a considerable amount of power.
7.
High-Performance Computing
One of the ﬁrst and most investigated ﬁelds of application of FPGAs in the
1990s was high-performance computing. The goal of many platform built at
that time was to provide acceleration of application such as searching in ge-
nomic databases, signal and image processing, matrix factorization. Several
architectures were developed, mostly consisting of a workstation on which one
or more FPGA were attached. By reconﬁguration of the FPGAs, custom in-
structions could be added to the computing system, sometimes even at run-
time. While remarkable speed-ups were reported for some applications, the
resulting custom computing machines (CCMs) could not be widely adopted as
viable computing platforms. This was mainly due to architectural limitations

316
Reconﬁgurable Computing
and lack of high-level programming tools. The capacity of FPGAs was too
small to allow real migration of software components in hardware. Further-
more, the performance bottleneck posed by the slow communication between
host processor and FPGAs often busted speed-ups. Finally, there were no use-
ful software-like description languages that could encourage software design-
ers to write programs for such machines and implement efﬁcient libraries for
reusable data stream management. Programming CCMs meant tedious low-
level hardware design, which was not attractive at all.
The last couple of years brought a pivotal change. FPGA’s logic densities
increased dramatically, now allowing for massive bit-level parallelism. Avail-
able high-level languages like Handel-C [125] and ImpulseC [175] as well
as progress in compilation technology makes the logic resources available to
high-level application programmers.
Boosted by the recent engagement of companies such as Cray [58], SRC
Computers [59], and ClearSpeed [51], Nallatech [165], and SGI [191], CCMs
are experiencing a renaissance. Despite the use of new high-speed interfaces
and efﬁcient datastream management protocols between the components in the
new systems, the new architectures are built on the same models like the old
ones in the 1990s. It usually consist of a set of FPGAs, on the same board
with one or more processors. The processors control the whole systems and
conﬁgured the FPGAs at run-time with dedicated function to be accelerated in
hardware.
While considerable progress has been done, the rentability of the machines
provided by the ﬁrms previously mentioned still have to be proven. The rentabil-
ity cannot be evaluated anymore only on the basis of speed-ups observed in
some class of applications. It is not sure that a comparison made in terms of
performance/$, performance/J, and performance/sqm will favour further de-
ployment of those systems. Craven and Athanas recently provided in [57] a
study on the viability of the FPGA in supercomputers. Their study is based on
the cost of realizing ﬂoating-point into FPGAs. The cost includes not only the
performance gain but also the price to pay. The authors came to the conclu-
sion that the price to pay for a FPGA-based supercomputer is too high for the
marginal speed-ups.
High-precision ﬂoating-point arithmetic is usually the bulk of computa-
tion in high-performance computing. Despite the progress done in develop-
ing ﬂoating-point units in hardware, Floating-point computation and FPGA is
still a difﬁcult union. The advantage provided by FPGA in customized im-
plementation of ﬂoating-point as described Shirazi [192] is unlikely to help
here because the datapath must provide customization that consume the largest
amount of resources. Coarse-grained elements like the embedded multipliers
in FPGAs or coarse-grained reconﬁgurable device provide the best prerequi-
sites for the use of reconﬁgurable device in supercomputers.

Applications
317
8.
Conclusion
We have presented in this chapter a couple of applications that can take
advantage of the ﬂexibility as well as performance of reconﬁgurable devices.
For each application targeted, we placed the focus mostly on the architectural
organization because the goal was not to necessary present the detailed im-
plementation of an application. However, we have presented a comprehensive
description of the pattern-matching application, while keeping the other pre-
sentation short. This is due to the low attention paid on pattern matching in
hardware in available textbooks. With the large amount of literature in other
presented topics such as image processing and control, we choose not to focus
in details of those applications.
A lot of work have been done in the last two decades, and a large number
of applications were implemented in FPGA, which is the main representative
of reconﬁgurable devices. We could not present nor cite all available imple-
mentations here. We have rather focussed on few ones where we could show
the usefulness of reconﬁgurability. Finally, we would like to emphasize that
despite two decades of research, the existence of ‘killer applications’ could not
be shown for reconﬁgurable device, thus limiting the acceptance of reconﬁgu-
ration in the industry. The existence of a killer application will certainly boost
the development of reconﬁgurable devices, leading to new class of devices,
tools and programmers. Despite this missing step, research keeps going, and
the support of the industry is needed more than it have ever be.

References
[1] G. R. AB, MicroBlaze Processor Reference Guide: Embedded Development Kit EDK
8.2i, 2005, http://www.gaisler.com. [Online]. Available: http://www.gaisler.com
[2] A. Ahmadinia, C. Bobda, S. Fekete, J. Teich, and J. van der Veen, “Optimal routing-
conscious dynamic placement for reconﬁgurable computing,” in 14th International
Conference on Field-Programmable Logic and Application, ser. Lecture Notes in
Computer Science, vol. 3203.
Springer-Verlag, 2004, pp. 847–851, available at
http://arxiv.org/abs/cs.DS/0406035.
[3] A. Ahmadinia, C. Bobda, M. Bednara, and J. Teich, “A new approach for on-line place-
ment on reconﬁgurable devices,” in Proceedings of the 18th International Parallel and
Distributed Processing Symposium (IPDPS) / Reconﬁgurable Architectures Workshop
(RAW), 2004.
[4] A. Ahmadinia, C. Bobda, J. Ding, M. Majer, J. Teich, S. P. Fekete, and J. C. van der
Veen, “A practical approach for circuit routing on dynamic reconﬁgurable devices,”
in RSP ’05: Proceedings of the 16th IEEE International Workshop on Rapid System
Prototyping (RSP’05).
Washington, DC, USA: IEEE Computer Society, 2005, pp.
84–90.
[5] C. Alpert and A. Kahng, “Geometric embeddings for faster and better multi-way netlist
partitioning,” 1993.
[6] C. J. Alpert and A. B. Kahng, “Multi-way partitioning via spaceﬁlling curves and dy-
namic programming,” in Design Automation Conference, 1994, pp. 652–657.
[7] C. J. Alpert and S.-Z. Yao, “Spectral partitioning: The more eigenvectors, the better,” in
Design Automation Conference, 1995, pp. 195–200.
[8] ADM-XRC-II Xilinx Virtex-II PMC, Alpha Data Ltd., 2002, http://www.alpha-
data.com/adm-xrc-ii.html.
[9] Altera, “Software deﬁned radio,” www.altera.com/literature/cp/fpga-cores-for-sdr.pdf.
[Online]. Available: www.altera.com/literature/cp/fpga-cores-for-sdr.pdf
[10] K. Aoki and H. Lipmaa, “Fast Implementations of AES Candidates,” in The
Third Advanced Encryption Standard Candidate Conference.
New York, NY,

320
References
USA:
National
Institute
of
Standards
and
Technology,
13–14
Apr.
2000,
pp.
106–120,
entire
proceedings
available
from
the
conference
homepage
http://csrc.nist.gov/encryption/aes/round2/conf3/aes3conf.htm. [On-
line]. Available: citeseer.ist.psu.edu/aoki00fast.html
[11] P. Athanas, J. Becker, G. Brebner, and H. ElGindy, “Dynamically reconﬁgurable archi-
tectures,” in Dagstuhl Semiar 03301, 2003.
[12] P.
M.
Athanas
and
H.
F.
Silverman,
“Processor
reconﬁguration
through
instruction-set
metamorphosis,”
IEEE
Computer,
vol.
26,
no.
3,
pp.
11–18,
1993,
citeseer.nj.nec.com/athanas93processor.html.
[Online].
Available:
citeseer.nj.nec.com/athanas93processor.html
[13] Atmel, AT6000 FPGA conﬁguration guide.
Atmel Inc.
[14] R. Baeza-Tates, Handbook of Algorithms and Data Structures.
Addison-Wessley,
1991.
[15] S. Bajracharya, C. Shu, K. Gaj, and T. El-Ghazawi, “Implementation of elliptic curve
cryptosystems over gf(2n) in optimal normal basis on a reconﬁgurable computer,” in
FPGA ’04: Proceedings of the 2004 ACM/SIGDA 12th international symposium on
Field programmable gate arrays.
New York, NY, USA: ACM Press, 2004, pp. 259–
259.
[16] L. Barroso, K. Gharachorloo, R. McNamara, A. Nowatzyk, S. Qadeer, B. Sano,
S. Smith, R. Stets, and B. Verghese, “Piranha: A scalable architecture based on
single-chip multiprocessing,” 2000. [Online]. Available: http://citeseer.ist.psu.edu/
barroso00piranha.html
[17] V. Baumgarte, G. Ehlers, F. May, A. Nueckel, M. Vorbach, and M. Weinhardt, “PACT
XPP A self-reconﬁgurable data processing architecture,” J. Supercomput., vol. 26, no. 2,
pp. 167–184, 2003.
[18] K. Bazargan, R. Kastner, and M. Sarrafzadeh, “Fast template placement for reconﬁg-
urable computing systems,” In IEEE Design and Test - Special Issue on Reconﬁgurable
Computing, vol. January-March, pp. 68–83, 2000.
[19] M. Bednara, M. Daldrup, J. Shokrollahi, J. Teich, and J. von zur Gathen, “Tradeoff anal-
ysis of fpga based elliptic curve cryptography,” in Proc. of the IEEE International Sym-
posium on Circuits and Systems (ISCAS-02), Scottsdale, Arizona, U.S.A, May 2002.
[20] M. Bednara, M. Daldrup, J. von zur Gathen, J. Shokrollahi, and J. Teich, “Reconﬁg-
urable implementation of elliptic curve crypto algorithms.” in IPDPS, 2002.
[21] L. Benini and G. Micheli, “Network on chips: A new soc paradigm,” IEEE Computer,
January 2001.
[22] J. L. Bentley, “Multidimensional binary search trees used for associative searching,”
Commun. ACM, vol. 18, no. 9, pp. 509–517, 1975.
[23] J. L. Bentley, “Solutions to Klee’s rectangle problems,” Carnegie-Mellon Univ., Pitts-
burgh, PA, Technical Report, 1977.

References
321
[24] J. L. Bentley and D. Wood, “An optimal worst case algorithm for reporting intersections
of rectangles,” IEEE Trans. Comput., vol. C-29, pp. 571–577, 1980.
[25] P. Bertin, D. Roncin, and J. Vuillemin, “Introduction to programmable ac-
tive
memories,”
in
Systolic
Array
Processors,
J.
McCanny,
J.
McWhirter,
and
E.
Swartzlander,
Eds.
Prentice
Hall,
1989.
[Online].
Available:
citeseer.ist.psu.edu/bertin89introduction.html
[26] N. B. Bhat and D. D. Hill, “Routable technologie mapping for lut fpgas,” in ICCD ’92:
Proceedings of the 1991 IEEE International Conference on Computer Design on VLSI
in Computer & Processors.
Washington, DC, USA: IEEE Computer Society, 1992,
pp. 95–98.
[27] M. Blum, R. W. Floyd, V. Pratt, R. L. Rivest, and R. E. Tarjan, “Linear time bounds for
median computations,” in Proc. Fourth Annual ACM Symposium on Theory of Comput-
ing, 1972, pp. 119–124.
[28] C. Bobda, “Temporal partitioning and sequencing of dataﬂow graphs on reconﬁgurable
systems,” in International IFIP TC10 Stream on Distributed and Parallel Embedded
Systems (DIPES 2002).
Montreal, Canada: IFIP, 2002, pp. 185–194.
[29] C. Bobda and N. Steenbock, “A rapid prototyping environment for distributed recon-
ﬁgurable systems,” in 13th IEEE International Workshop On Rapid System Prototyp-
ing(RSP’02), Darmstadt Germany.
IEEE Computer Society, 2002.
[30] C. Bobda, “Synthesis of dataﬂow graphs for reconﬁgurable systems using temporal par-
titioning and temporal placement,” Dissertation, Universit¨at Paderborn, Heinz Nixdorf
Institut, Entwurf Paralleler Systeme, 2003, p. 35,-, ISBN 3-935433-37-9.
[31] C. Bobda, “Coremap: a rapid prototyping environment for distributed reconﬁgurable
systems,” International Journal of Embedded Systems, vol. 2, no. 3-4, pp. 274 – 290,
2006.
[32] C. Bobda and A. Ahmadinia, “Dynamic interconnection of reconﬁgurable modules on
reconﬁgurable devices.” IEEE Design & Test of Computers, vol. 22, no. 5, pp. 443–451,
2005.
[33] C. Bobda, A. Ahmadinia, and J. Teich, “Generation of distributed arithmetic designs
for reconﬁgurable application.” in ARCS Workshops, 2004, pp. 205–214.
[34] C. Bobda and T. Lehmann, “Efﬁcient building of word recongnizer in fpgas for term-
document matrices construction,” in Field Programmable Logic and Aplications FPL
2000, R. Hartenstein and H. Gr¨unbacher, Eds.
Villach, Austria: Springer, 2000, pp.
759–768.
[35] C. Bobda, M. Majer, D. Koch, A. Ahmadinia, and J. Teich, “A dynamic noc approach
for communication in reconﬁgurable devices,” in Proceedings of International Con-
ference on Field-Programmable Logic and Applications (FPL), ser. Lecture Notes in
Computer Science (LNCS), vol. 3203.
Antwerp, Belgium: Springer, Aug. 2004, pp.
1032–1036.
[36] J. A. Boyan and M. L. Littman, “Packet routing in dynamically changing
networks: A reinforcement learning approach,” in Advances in Neural Information

322
References
Processing Systems, J. D. Cowan, G. Tesauro, and J. Alspector, Eds., vol. 6.
Morgan Kaufmann Publishers, Inc., 1994, pp. 671–678.
[Online]. Available:
citeseer.ist.psu.edu/boyan94packet.html
[37] G. J. Brebner, “A virtual hardware operating system for the xilinx xc6200,” in FPL ’96:
Proceedings of the 6th International Workshop on Field-Programmable Logic, Smart
Applications, New Paradigms and Compilers.
London, UK: Springer-Verlag, 1996,
pp. 327–336.
[38] R. P. Brent and F. T. Luk, “The solution of singular-value and eigen-value problems on
multiprocessor arrays,” SIAM J. Sci. Stat. Comput., vol. 6, no. 1, pp. 69–84, 1985.
[39] D. A. Buel, J. M. Arnold, and W. J. Kleinfelder, Splash 2 FPGAs in a Custom Comput-
ing Machine.
IEEE Computer Society Press, Los Alamitos, California, 1996.
[40] C. Bobda, M. Huebner, A. Niyonkuru, B. Blodget, M. Majer, A. Ahmadinia, “Design-
ing partial and dynamically reconﬁgurable applications on xilinx virtex-ii fpgas using
handelc,” in International Workshop on Field Programmable Logic and Applications
(FPL), Antwerp, Belgium.
[41] J. M. P. Cardoso and H. C. Neto, “An enhance static-list scheduling algorithm for tem-
poral partitioning onto rpus,” in IFIP TC10 WG10.5 10 Int. Conf. on Very Large Scale
Integration(VLSI’99).
Lisboa, Portugal: IFIP, 1999, pp. 485 – 496.
[42] J. Castillo, P. Huerta, V. López, and J. I. Martínez, “A secure self-reconﬁguring archi-
tecture based on open-source hardware,” reconﬁg, vol. 0, p. 10, 2005.
[43] RC2000
Development
Board,
Celoxica
Ltd.,
2004,
http://www.celoxica.com/products/boards/rc2000.asp.
[44] P. K. Chan, M. D. F. Schlag, and J. Y. Zien, “Spectral-based multi-way fpga partition-
ing,” in FPGA ’95: Proceedings of the 1995 ACM third international symposium on
Field-programmable gate arrays.
New York, NY, USA: ACM Press, 1995, pp. 133–
139.
[45] D. Chang and M. Marek-Sadowska, “Partitioning sequential circuits on dynamicaly
reconﬁgurable fpgas,” in International Symposium on Field Programmable Gate Ar-
rays(FPGA 98).
Monterey, California: ACM/SIGDA, 1998, pp. 161 – 167.
[46] K.-C. Chen, J. Cong, Y. Ding, A. B. Kahng, and P. Trajmar, “Dag-map: Graph-based
fpga technology mapping for delay optimization,” IEEE Design and Test of Computers,
vol. 09, no. 3, pp. 7–20, 1992.
[47] R. C. C. Cheung, N. J. Telle, W. Luk, and P. Y. K. Cheung, “Customizable elliptic curve
cryptosystems.” IEEE Trans. VLSI Syst., vol. 13, no. 9, pp. 1048–1059, 2005.
[48] F. Christensen, “A scalable software-deﬁned radio development system,” Xcell Journal,
Oct. 2004.
[49] K. S. Christos H. Papadimitrio, Combinatorial Optimization.
Englewood Cliffs, NY:
Prentice Hall, 1982.
[50] M. Ciet, M. Neve, E. Peeters, and J.-J. Quisquater, “Parallel fpga implementation of
rsa with residue number systems - can side-channel threats be avoided? - extended

References
323
version,” Cryptology ePrint Archive, Report 2004/187, 2004. [Online]. Available:
http://eprint.iacr.org
[51] p. ClearSpeed Technology, http://www.clearspeed.com/.
[52] W. Cockshott and P. Foulk, “A low-cost text retrieval machine,” IEEE PROCEEDINGS,
vol. 136, no. 4, pp. 271–276, July 1989.
[53] J. Cong and Y. Ding, “Flowmap: an optimal technology mapping algorithm for delay
optimization in lookup-table based fpga designs.” IEEE Trans. on CAD of Integrated
Circuits and Systems, vol. 13, no. 1, pp. 1–12, 1994.
[54] J. Cong and Y. Ding, “Combinational logic synthesis for lut based ﬁeld programmable
gate arrays,” ACM Trans. Des. Autom. Electron. Syst., vol. 1, no. 2, pp. 145–204, 1996.
[55] T. H. Cormen, C. Stein, R. L. Rivest, and C. E. Leiserson, Introduction to Algorithms.
McGraw-Hill Higher Education, 2001.
[56] X. Corp, http://www.xess.com.
[57] S. Craven and P. Athanas, “Examining the viability of fpga supercomputing,” EURASIP
Journal on Embedded Systems, vol. 2007, pp. Article ID 93 652, 8 pages, 2007,
doi:10.1155/2007/93652.
[58] Cray, “Cray xd1 supercomputer,” GNU’s home page. [Online]. Available: http:
//www.cray.com/products/xd1/
[59] I. CRC Computers, “General purpose reconﬁgurable computing systemsr.” [Online].
Available: http://www.srccomp.com/
[60] W. J. Dally and B. Towles, “Route packets, not wires: on-chip interconnection net-
works,” in Proceedings of the Design Automation Conference, Las Vegas, NV, Jun.
2001, pp. 684–689.
[61] A. Dandalis and V. K. Prasanna, “An adaptive cryptographic engine for internet protocol
security architectures,” ACM Trans. Des. Autom. Electron. Syst., vol. 9, no. 3, pp. 333–
353, 2004.
[62] K. Danne, “Distributed arithmetic FPGA design with online scalable size and perfor-
mance,” in Proceedings of 17th SYMPOSIUM ON INTEGRATED CIRCUITS AND SYS-
TEMS DESIGN (SBCCI04).
ACM Press, New York, NY, USA, 7 - 11 Sep. 2004, pp.
135–140.
[63] K. Danne and C. Bobda, “Dynamic reconﬁguration of distributed arithmetic controllers:
Design space exploration and trade-off analysis,” ipdps, vol. 04, p. 140a, 2004.
[64] K. Danne, C. Bobda, and H. Kalte, “Run-time exchange of mechatronic controllers
using partial hardware reconﬁguration,” in Proc. of the International Conference on
Field Programmable Logic and Applications (FPL2003), Lisbon, Portugal, Sep. 2003.
[65] S. Davidson, D. Landskov, B. Shriver, and P. W. Mallett, “Some experiments in local
microcode compaction for horizontal machines.” IEEE Trans. Computers, vol. 30, no. 7,
pp. 460–477, 1981.

324
References
[66] M. de Berg, M. van Kreveld, M. Overmars, and O. Schwarzkopf, Computational Geom-
etry: Algorithms and Applications, 2nd ed.
Berlin, Germany: Springer-Verlag, 2000.
[67] S. Deerwester, S. Dumai, G. Furnas, T. Landauer, and R. Harshmann, “Indexing by
latent semantic analysis,” Journal of American Society for Information Science, vol. 41,
no. 6, pp. 391–407, 1990.
[68] A. DeHon, “DPGA-coupled microprocessors: Commodity ICs for the early 21st
century,” in IEEE Workshop on FPGAs for Custom Computing Machines, D. A. Buell
and K. L. Pocek, Eds.
Los Alamitos, CA: IEEE Computer Society Press, 1994, pp.
31–39. [Online]. Available: citeseer.ist.psu.edu/dehon94dpgacoupled.html
[69] J. P. Delahaye, G. Gogniat, C. Roland, and P. Bomel, “Software radio and
dynamic
reconﬁguration on a dsp/fpga platform,” 2004.
[Online]. Available:
http://hal.ccsd.cnrs.fr/ccsd-00089395/en/
[70] Digilent
Inc.,
“The
vdec1
video
decoder
board.”
[Online].
Available:
http:
//www.digilentinc.com/Products/Detail.cfm?Prod=VDEC1
[71] F. Dittmann, A. Rettberg, and F. Schulte, “A y-chart based tool for reconﬁgurable sys-
tem design,” in Workshop on Dynamically Reconﬁgurable Systems (DRS).
Innsbruck,
Austria: VDE Verlag, 17 Mar. 2005, pp. 67–73.
[72] P. S. B. do Nascimento, M. E. de Lima, S. M. da Silva, and J. L. Seixas, “Mapping
of image processing systems to fpga computer based on temporal partitioning and de-
sign space exploration,” in SBCCI ’06: Proceedings of the 19th annual symposium on
Integrated circuits and systems design.
New York, NY, USA: ACM Press, 2006, pp.
50–55.
[73] W. E. Donath, “Hierarchical placement method,” IBM Technical disclosure Bulletin,
vol. 17, no. 10, pp. 3121–3125, 1975.
[74] W. E. Donath and A. J. Hoffman, “Algorithms for partitioning of graphs and computer
logic based on eigenvectors of connection matrices,” IBM Technical disclosure Bulletin,
vol. 15, no. 3, pp. 938–944, 1972.
[75] S. J. Eggers, J. S. Emer, H. M. Levy, J. L. Lo, R. L. Stamm, and D. M.
Tullsen, “Simultaneous multithreading: A platform for next-generation processors,”
IEEE Micro, vol. 17, no. 5, 1997. [Online]. Available: http://citeseer.ist.psu.edu/
eggers97simultaneous.html
[76] Ejnioui and N. Ranganathan, “Circuit scheduling on time-multiplexed fpgas.” [Online].
Available: citeseer.nj.nec.com/17858.html
[77] H. A. ElGindy, A. K. Somani, H. Schroeder, H. Schmeck, and A. Spray, “RMB - A Re-
conﬁgurable Multiple Bus Network,” in Proceedings of the Second International Sym-
posium on High-Performance Computer Architecture (HPCA-2), San Jose, California,
Feb. 1996, pp. 108–117.
[78] M. Ernst, M. Jung, F. Madlener, S. Huss, and R. Bl&#252;mel, “A reconﬁgurable sys-
tem on chip implementation for elliptic curve cryptography over gf(2n),” in CHES ’02:
Revised Papers from the 4th International Workshop on Cryptographic Hardware and
Embedded Systems.
London, UK: Springer-Verlag, 2003, pp. 381–399.

References
325
[79] G. Estrin and C. R. Viswanathan, “Organisation of a "ﬁxed-plus-variable" structure
computer for computation of eigenvalues and eigenvectors of real symetric matrices,”
Journal of the ACM, vol. 9, pp. 41–60, 1962.
[80] G. Estrin, B. Bussell, R. Turn, and J. Bibb, “Parallel processing in a restructurable
computer system,” IEEE Transactions on Electronic Computers, vol. 12, no. 5, pp. 747–
755, 1963.
[81] G. Estrin and R. Turn, “Automatic assignment of computations in a variable structure
computer system,” IEEE Transactions on Electronic Computers, vol. 12, no. 5, pp. 755–
773, 1963.
[82] S. P. Fekete, E. Köhler, and J. Teich, “Optimal fpga module placement with temporal
precedence constraints,” Technische Universit¨at Berlin, Tech. Rep. 696.2000, 2000.
[83] C. M. Fiduccia and R. Mattheyses, “A linear-time heuristic for improving network par-
titions,” in Proceedings of the 19th Design Automation Conference, 1982, pp. 175–181.
[84] L. Ford and D. Fulkerson, Flows in Networks.
Princeton University Press, 1962.
[85] P. Foulk, “Data-folding in SRAM conﬁgurable FPGA,” in IEEE Workshop on FPGAs
for Custom Computing Machines.
IEEE, 1993, pp. 163–171.
[86] R. FRANCIS, “Technology mapping for lookup-table based ﬁeldprogrammable ga
te arrays,” Ph.D. dissertation, University of Toronto, 1992. [Online]. Available:
citeseer.ist.psu.edu/francis93technology.html
[87] R. J. Francis, J. Rose, and K. Chung, “Chortle: a technology mapping program for
lookup table-based ﬁeld programmable gate arrays,” in DAC ’90: Proceedings of the
27th ACM/IEEE conference on Design automation. New York, NY, USA: ACM Press,
1990, pp. 613–619.
[88] J. Fry and M. Langhammer, “Rsa and public key cryptography in fpgas.” [Online].
Available: www.altera.com/literature/cp/rsa-public-key-cryptography.pdf
[89] F. C. Furtek, E. Hogenauer, and J. Scheuermann, “Interconnecting heterogeneous nodes
in an adaptive computing machine.” in Proceedings of International Conference on
Field-P rogrammable Logic and Applications (FPL), ser. Lecture Notes in Computer
Science (LNCS), vol. 3203.
Antwep, Belgium: Springer, Aug. 2004, pp. 125–134.
[90] K. Gaj and P. Chodowiec, “Comparison of the hardware performance of the aes candi-
dates using reconﬁgurable hardware.” in AES Candidate Conference, 2000, pp. 40–54.
[91] D. D. Gajski and L. Ramachandran, “Introduction to high-level synthesis,” IEEE Des.
Test, vol. 11, no. 4, pp. 44–54, 1994.
[92] D. D. Gajski and M. Reshadio, “Application and advantages,” CECS, UC Irvin, Tech-
nical Report 04-12, 2004.
[93] C. H. Gebotys, “An optimal methodology of synthesis of dsp multichip architectures,”
Journal of VLSI Signal Processing, vol. 11, pp. 9–19, 1995.
[94] L. Geppert, “Sun’s big splash,” IEEE Spectrum Magazine, pp. 21–29, M, January 2005
2005.

326
References
[95] J. Gerling, K. Danne, C. Bobda, and J. Schrage, “Distributed arithmetics for recursive
convolution of optical intercannects,” in EOS Topical Meeting, Optics in Computing
(OIC), Engelberg (Switzerland), Apr. 2004, pp. 65–66.
[96] P. B. Gibbons and S. S. Muchnick, “Efﬁcient instruction scheduling for a pipelined ar-
chitecture,” in SIGPLAN ’86: Proceedings of the 1986 SIGPLAN symposium on Com-
piler construction.
New York, NY, USA: ACM Press, 1986, pp. 11–16.
[97] M. Gokhale, W. Holmes, A. Kopser, D. Kunze, D. P. Lopresti, S. Lucas, R. Minnich,
and P. Olsen, “Splash: A reconﬁgurable linear logic array.” in ICPP (1), 1990, pp. 526–
532.
[98] S. C. Goldstein, H. Schmit, M. Moe, M. Budiu, S. Cadambi, R. R. Taylor, and
R. Laufer, “Piperench: A coprocessor for streaming multimedia acceleration,” in ISCA,
1999, pp. 28–39. [Online]. Available: citeseer.ist.psu.edu/goldstein99piperench.html
[99] G. H. Golub and C. F. V. Loan, Matrix Computations.
North Oxford Academic Pub-
lisching, 1983.
[100] C. E. D. C. Green and P. Franklin, “RaPiD – reconﬁgurable pipelined datapath,”
in Field-Programmable Logic: Smart Applications, New Paradigms, and Compilers.
6th International Workshop on Field-Programmable Logic and Applications, R. W.
Hartenstein and M. Glesner, Eds.
Darmstadt, Germany: Springer-Verlag, 1996, pp.
126–135. [Online]. Available: citeseer.ist.psu.edu/ebeling96rapid.html
[101] S. Guccione, “Programming ﬁne-grained reconﬁgurable architechture,” Ph.D. disserta-
tion, The University of Texas at Austin, May 1995.
[102] S. Guccione, D. Levi, and P. Sundararajan, “Jbits: A java-based interface for reconﬁg-
urable computing,” 1999.
[103] M. Gulati and N. Bagherzadeh, “Performance study of a multithreaded superscalar mi-
croprocessor,” in Proc. Intl. Symp. High-Performance Computer Architecture, ACM,
1996, pp. 291–301.
[104] B. Gunther and G. Milne, “Accessing document relevance with run-time reconﬁgurable
machines,” in IEEE Workshop on FPGAs for Custom Computing Machines.
Napa
California: IEEE, 1996, pp. 9–16.
[105] B. Gunther and G. Milne, “Hardware-based solution for message ﬁltering,” school of
computer and information science, Tech. Rep., 1996.
[106] R. H. G¨uting, “An optimal contour algorithm for iso-oriented rectangles,” J. Algorithms,
vol. 5, pp. 303–326, 1984.
[107] K. Hall, “An r-dimensional quadratic dimensional quadratic placement algorithm,”
Journal of Management Science, vol. 17, no. 3, pp. 219–229, 1970.
[108] T. Haller, “Adaptive multiprocessing on reconﬁgurable chip,” 2006, master Thesis.
[109] L.
Hammond,
B.
A.
Nayfeh,
and
K.
Olukotun,
“The
hydra
chip,”
IEEE
MICRO
Magazine,
pp.
71–83,
March-April
2000.
[Online].
Available:
http:
//citeseer.ist.psu.edu/287939.html

References
327
[110] R. Hartenstein, “A decade of reconﬁgurable computing: A visionary Retrospective,” in
Proceedings of the International Conference on Design, Automation and Test in Europe
(DATE).
IEEE Computer Society, March 2001, pp. 290–295.
[111] R. Hartenstein, Morphware and Conﬁgware, A. Y. Zomaya, Ed.
New York: Springer-
Verlag, 2006.
[112] R. Hartenstein, Basics of Reconﬁgurable Computing, S. P. J. Henkel, Ed.
New York:
Springer-Verlag, 2007.
[113] R. Hartenstein, A. Hirschbiel, and M.Weber, “Xputers - an open family of non von neu-
mann architectures,” in 11th ITG/GI Conference on Architektur von Rechensystemen.
VDE-Verlag, 1990.
[114] J. R. Hauser and J. Wawrzynek, “Garp: A MIPS processor with a reconﬁgurable
coprocessor,” in IEEE Symposium on FPGAs for Custom Computing Machines, K. L.
Pocek and J. Arnold, Eds.
Los Alamitos, CA: IEEE Computer Society Press, 1997,
pp. 12–21. [Online]. Available: citeseer.nj.nec.com/hauser97garp.html
[115] S. D. Haynes, J. Stone, P. Y. K. Cheung, and W. Luk, “Video image processing with the
sonic architecture,” Computer, vol. 33, no. 4, pp. 50–57, 2000.
[116] P. Healy and M. Creavin, “An Optimal Algorithm for Rectangle Placement,” Dept. of
Computer Science and Information Systems, University of Limerick, Limerick, Ireland,
Tech. Rep. UL-CSIS-97-1, Aug. 1997.
[117] A. Hemani, A. Jantsch, S. Kumar, A. Postula, J. Oberg, M. Millberg, and D. Lindqvist,
“Network on chip: An architecture for billion transistor era,” in In Proceeding of the
IEEE NorChip Conference, November 2000., 2000.
[118] B. Hendrickson and R. Leland, “An improved spectral graph partitioning algorithm for
mapping parallel computations,” SIAM Journal on Scientiﬁc Computing, vol. 16, no. 2,
pp. 452–469, 1995. [Online]. Available: citeseer.nj.nec.com/hendrickson95improved.
html
[119] M. R. Hestenes, “Inversion of matrices by biorthogonalization and related results,” J.
Soc. Indust. Appl. Math., vol. 6, no. 1, pp. 51–90, 1958.
[120] Hirata et al., “An elementary processor architecture with simultaneous instruction is-
suing from multiple threads,” in Proc. Intl. Symp. Computer Architecture, Assoc. of
Computing Machinery, 1992, pp. 136–145.
[121] G.
home
page,
“Gnu
radio
-
the
gnu
software
radio,”
http://www.gnu.org/software/gnuradio/.
[Online].
Available:
http://www.gnu.org/
software/gnuradio/
[122] J. Huie, P. DŠAntonio, R. Pelt, and B. Jentz, “Synthesizing fpga cores for software-
deﬁned radio.” [Online]. Available: www.altera.com/literature/cp/fpga-cores-for-sdr.
pdf
[123] S. Iman, M. Pedram, C. Fabian, and J. Cong, “Finding uni-directional cuts based on
physical partitioning and logic restructuring,” in Fourth International Workshop on
Physical Design.
IEEE, 1993.

328
References
[124] A. Inc, Nios II Processor Reference Handbook, November, 2006. [Online]. Available:
http://www.altera.com/literature/lit-nio2.jsp
[125] C. inc, Handel-C Reference Manual, 2000, www.celoxica.com/techlib/ﬁles/CEL-
W0410251JJ4-60.pdf.
[Online].
Available:
www.celoxica.com/techlib/ﬁles/
CEL-W0410251JJ4-60.pdf
[126] X. Inc, PowerPC 405 Processor Block Reference Guide: Embedded Development Kit,
July 20, 2005. [Online]. Available: www.xilinx.com/bvdocs/userguides/ug018.pdf
[127] X. Inc, MicroBlaze Processor Reference Guide: Embedded Development Kit EDK 8.2i,
June 1, 2006. [Online]. Available: www.xilinx.com/ise/embedded/mb ref guide.pdf
[128] X. Inc, XAPP290: Two Flows for Partial Reconﬁguration: Module Based or Difference
Based, September 9, 2004. [Online]. Available: www.xilinx.com/bvdocs/appnotes/
xapp290.pdf
[129] J. M. Joseph Mitola, Software Radio Architecture: Object-Oriented Approaches to
Wireless Systems Engineering.
Wiley, John & Sons, Incorporated, 2001.
[130] S. Jung, “Entwurf eines verfahrens und einer umgebung zur durchgängigen konﬁgura-
tion adativer on-chip multiprozessor,” 2006, master Thesis.
[131] J¨urgen Reichardt, Bernd Schwarz, VHDL-Synthese, 3rd ed.
Oldenbourg-Verlag, Dec
2003. [Online]. Available: http://users.etech.fh-hamburg.de/users/reichardt/buch.html
[132] H. Kalte, M. Pormann, and U. Rueckert, “Rapid prototyping system f"ur dynamisch
rekonﬁgurierbarer hardware struckturen,” in AES 2000, 2000, pp. 149–157.
[133] K. Karplus, “Xmap: A technology mapper for table-lookup ﬁeld-programmable gate
arrays,” in DAC ’91: Proceedings of the 28th conference on ACM/IEEE design automa-
tion.
New York, NY, USA: ACM Press, 1991, pp. 240–243.
[134] R. Kastner, A. Kaplan, and M. Sarrafzadeh, Synthesis Techniques and Optimizations
for Reconﬁgurable Systems.
Amsterdam: Kluwer Academic Publishers, 2003.
[135] M. Kaul and R. Vemuri, “Optimal temporal partitioning and synthesis for reconﬁg-
urable architectures,” 1998.
[136] M. Kaul, R. Vemuri, S. Govindarajan, and I. Ouaiss, “An automated temporal
partitioning tool for a class of dsp applications,” 1998. [Online]. Available:
citeseer.nj.nec.com/160625.html
[137] B. W. Kernighan and S. Lin, “An efﬁcient heuristic procedure for partitioning graphs,”
The Bell System Technical Journal, vol. 49, no. 2, pp. 291–307, 1970.
[138] N. KOBLITZ, “Elliptic curve cryptosystems,” Mathematics of Computation, vol. 48,
pp. 203–209, 1987.
[139] P. Kongetira, K. Aingaran, and K. Olukotun, “The hydra chip,” IEEE MICRO
Magazine, pp. 21–29, March-April 2005. [Online]. Available: http://citeseer.ist.psu.
edu/287939.html

References
329
[140] H. Kropp, C. Reuter, M. Wiege, T.-T. Do, and P. Pirsch, “An fpga-based prototyping
system for real-time veriﬁcation of video processing schemes.” in FPL, 1999, pp. 333–
338.
[141] K. Kucukcakar and A. Parker, “Chop: A constraint-driven-system-level partitioner,” in
Design Automation Conference, 1991, pp. 514–519.
[142] P. Kurup and T. Abbasi, Logic Synthesis Using Synopsys. Kluwer Akademic publisher,
1997.
[143] D. Landskov, S. Davidson, B. Shriver, and P. W. Mallett, “Local microcode compaction
techniques,” ACM Comput. Surv., vol. 12, no. 3, pp. 261–294, 1980.
[144] T. Lengauer, Combinatorial Algorithm for Integrated Circuit Layout.
Teubner
Stuttgart, 1990.
[145] K. H. Leung, K. W. Ma, W. K. Wong, and P. H. W. Leong, “Fpga implementation
of a microcoded elliptic curve cryptographic processor,” in FCCM ’00: Proceedings
of the 2000 IEEE Symposium on Field-Programmable Custom Computing Machines.
Washington, DC, USA: IEEE Computer Society, 2000, p. 68.
[146] H. Li and Q. Stout, Reconﬁgurable Massively Parallel computers, H. Li and Q. Stout,
Eds.
Glasgow, UK: Prentice-Hall, 1991.
[147] W. Lipski, Jr. and F. P. Preparata, “Finding the contour of a union of iso-oriented rect-
angles,” J. Algorithms, vol. 1, pp. 235–246, 1980, errata in 2(1981), 105; corrigendum
in 3(1982), 301–302.
[148] H. Liu and D. F. Wong, “Network ﬂow-based circuit partitioning for time-multiplexed
FPGAs,” in IEEE/ACM International Conference on Computer-Aided Design, 1998,
pp. 497–504.
[149] H. Liu and D. F. Wong, “Circuit partitioning for dynamicaly reconﬁgurable fpgas,” in
International Symposium on Field Programmable Gate Arrays(FPGA 98).
Monterey,
California: ACM/SIGDA, 1999, pp. 187 – 194.
[150] A.
S.
Ltd.,
High
Performance
AES
Encryption
Cores.
[Online].
Available:
http://http://www.jat.co.kr/newsletter/2001 11 24/ds5210-40.pdf
[151] S.-M. Ludwig, “Hades-fast hardware synthesis tools and a reconﬁgurable coprocessor,”
Ph.D. dissertation, Swiss Federal Institute of Technologie, Z¨urich, 1997.
[152] J. Lutz and A. Hasan, “High performance fpga based elliptic curve cryptographic co-
processor,” in ITCC ’04: Proceedings of the International Conference on Information
Technology: Coding and Computing (ITCC’04) Volume 2.
Washington, DC, USA:
IEEE Computer Society, 2004, p. 486.
[153] P. S. G. Maya Gokhale, Reconﬁgurable Computing: Accelerating Computation with
Field-programmable Gate Arrays.
Berlin: Springer, 2005.
[154] A. Mazzeo, L. Romano, G. P. Saggese, and N. Mazzocca, “Fpga-based implementation
of a serial rsa processor,” in DATE ’03: Proceedings of the conference on Design, Au-
tomation and Test in Europe.
Washington, DC, USA: IEEE Computer Society, 2003,
p. 10582.

330
References
[155] A. Michalski and D. Buell, “A scalable architecture for rsa cryptography on large
fpgas,” in FCCM ’06: Proceedings of the 14th Annual IEEE Symposium on Field-
Programmable Custom Computing Machines (FCCM’06).
Washington, DC, USA:
IEEE Computer Society, 2006, pp. 331–332.
[156] G. D. Micheli, Synthesis and Optimization of Digital Circuits.
McGraw-Hill Higher
Education, 1994.
[157] V. S. Miller, “Use of elliptic curves in cryptography,” in Lecture notes in computer sci-
ences; 218 on Advances in cryptology—CRYPTO 85.
New York, NY, USA: Springer-
Verlag New York, Inc., 1986, pp. 417–426.
[158] L. Minzer, “Programmable silicon for embedde signal processing,” Embedded Systems
Programming, pp. 110–133, March 2000.
[159] E. Mirsky and A. DeHon, “MATRIX: A reconﬁgurable computing architecture with
conﬁgurable instruction distribution and deployable resources,” in IEEE Symposium
on FPGAs for Custom Computing Machines, K. L. Pocek and J. Arnold, Eds.
Los
Alamitos, CA: IEEE Computer Society Press, 1996, pp. 157–166. [Online]. Available:
citeseer.ist.psu.edu/mirsky96matrix.html
[160] T. Miyamori and K. Olukotun, “REMARC: Reconﬁgurable multimedia array
coprocessor (abstract),” in FPGA, 1998, p. 261. [Online]. Available: citeseer.ist.psu.
edu/miyamori98remarc.html
[161] A. Morse, “Control using logic-based switching,” Trends in Control, Springer, London,
1995.
[162] M. C.-G. H. Mr. Adam Harrington and D. S. S. Jones, “Software-deﬁned radio: The
revolution of wireless communication,” Annual Review of Communications, vol. 58,
2005.
[163] R. Murgai, Y. Nishizaki, N. Shenoy, R. K. Brayton, and A. Sangiovanni-Vincentelli,
“Logic synthesis for programmable gate arrays,” in DAC ’90: Proceedings of the 27th
ACM/IEEE conference on Design automation.
New York, NY, USA: ACM Press,
1990, pp. 620–625.
[164] R. Murgai, N. V. Shenoy, R. K. Brayton, and A. L. Sangiovanni-Vincentelli, “Perfor-
mance directed synthesis for table look up programmable gate arrays.” in ICCAD, 1991,
pp. 572–575.
[165] I. Nallatech, http://www.nallatech.com.
[166] Narendra and Balakrishnan, “Adaptive control using multiple models: Switching and
tuning,” Yale Workshop on Adaptive and Learning Systems, 1994.
[167] M. Nikitovic and M. Brorsson, “An adaptive chip-multiprocessor architecture for future
mobile terminals.” in CASES, 2002, pp. 43–49.
[168] A. Nohl, G. Braun, O. Schliebusch, R. Leupers, H. Meyr, and A. Hoffmann, “A uni-
versal technique for fast and ﬂexible instruction-set architecture simulation,” in DAC
’02: Proceedings of the 39th conference on Design automation.
New York, NY, USA:
ACM Press, 2002, pp. 22–27.

References
331
[169] K. Olukotun and L. Hammond, “The future of microprocessors,” ACM Queue, vol. 3,
no. 7, pp. 26–29, September 2005. [Online]. Available: http://doi.acm.org/10.1145/
1095408.1095418
[170] OSCI, SystemC Reference Manual, 2003. [Online]. Available: www.systemc.org
[171] I. Ouaiss, S. Govindarajan, V. Srinivasan, M. Kaul, and R. Vemuri, “An integrated
partitioning and synthesis system for dynamically reconﬁgurable multi-FPGA
architectures,” in IPPS/SPDP Workshops, 1998, pp. 31–36. [Online]. Available:
citeseer.nj.nec.com/ouaiss98integrated.html
[172] Y. Pan and M. Hamdi, “Singular value decomposition on processors arrays with a
pipelined bus system,” Journal of Network and Computer Applications, vol. 19, pp.
235–248, 1996.
[173] A. Pandey and R. Vemuri, “Combined temporal partitioning and scheduling for recon-
ﬁgurable architectures,” in Reconﬁgurable Technology: FPGAs for Computing and Ap-
plications, Proc. SPIE 3844, J. Schewel, P. M. Athanas, S. A. Guccione, S. Ludwig, and
J. T. McHenry, Eds.
Bellingham, WA: SPIE – The International Society for Optical
Engineering, 1999, pp. 93–103.
[174] P. G. Paulin and J. P. Knight, “Force-directed scheduling for the behavioral synthesis of
asic’s.” IEEE Transactions on CAD, vol. 6, no. 8, pp. 661–679, 1989.
[175] D. Pellerin and S. Thibault, Practical FPGA Programming in C.
Prentice Hall„ April
2005.
[176] M. Petrov, T. Murgan, F. May, M. Vorbach, P. Zipf, and M. Glesner, “The XPP ar-
chitecture and its co-simulation within the simulink environment.” in Proceedings of
International Conference on Field-P rogrammable Logic and Applications (FPL), ser.
Lecture Notes in Computer Science (LNCS), vol. 3203.
Antwep, Belgium: Springer,
Aug. 2004, pp. 761–770.
[177] M. Platzner and L. Thiele, “XFORCES - executives for reconﬁgurable embedded
systems.” [Online]. Available: http://www.ee.ethz.ch/\∼platzner
[178] V. K. Prasanna and A. Dandalis, “Fpga-based cryptography for internet security.”
[Online]. Available: halcyon.usc.edu/∼pk/prasannawebsite/papers/dandalisOSEE00.
pdf
[179] F. P. Preparata and M. I. Shamos, Computational Geometry: An Introduction.
New
York, NY: Springer-Verlag, 1985.
[180] D. Pryor, M.R.Thisle, and N.shirazi, “Text searching on splash 2,” in IEEE Workshop
on FPGAs for Custom Computing Machines.
IEEE, 1993, pp. 172–177.
[181] K. M. G. Purna and D. Bhatia, “Temporal partitioning and scheduling data ﬂow graphs
for reconﬁgurable computers,” IEEE Transactions on Computers, vol. 48, no. 6, pp.
579–590, 1999.
[182] F. J. Rammig, “A concept for the editing of hardware resulting in an automatic
hardware-editor,” in Proceedings of 14th Design Automation Conference, New Orleans,
1977, pp. 187–193.

332
References
[183] D. Rech, “Automatische generierung und konﬁguration von adaptiven on-chip symmet-
rical multiprocessing (smp) systemen,” 2006, master Thesis.
[184] E. Rijpkema, K. G. W. Goossens, A. Radulescu, J. Dielissen, J. van Meerbergen,
P. Wielage, and E. Waterlander, “Trade offs in the design of a router with both guar-
anteed and best-effort services for networks on chip,” in DATE ’03: Proceedings of the
conference on Design, Automation and Test in Europe.
Washington, DC, USA: IEEE
Computer Society, 2003, p. 10350.
[185] R. L. Rivest, A. Shamir, and L. Adleman, “A method for obtaining digital signatures and
public-key cryptosystems,” Communications of the ACM, vol. 21, no. 2, pp. 120–126,
1978.
[186] R. L. Rivest and C. E. Leiserson, Introduction to Algorithms.
New York, NY, USA:
McGraw-Hill, Inc., 1990.
[187] C. Rowen and S. Leibson, “Flexible architectures for engineering successful socs.” in
DAC, 2004, pp. 692–697.
[188] S. M. Scalera and J. R. Vázquez, “The design and implementation of a context switch-
ing fpga,” in IEEE Symposium on FPGAs for Custom Computing Machines.
Napa
Valey, CA: IEEE Computer Society Press, April 1998, pp. 78–85.
[189] SDR, “The sdr forum’s website.” [Online]. Available: http://www.gnu.org/software/
gnuradio/
[190] P. Sedcole, “Reconﬁgurable platform-based design in fpgas for video image process-
ing,” Ph.D. dissertation, Imperial College, London, January 2006.
[191] SGI, http://www.sgi.com/products/rasc/.
[192] N. Shirazi, A. Walters, and P. M. Athanas, “Quantitative analysis of ﬂoating point arith-
metic on fpga based custom computing machines.” in FCCM, 1995, pp. 155–163.
[193] R. G. Shoup, “Parameterized convolution ﬁltering in an FPGA,” in Proceedings of In-
ternational Conference on Field Programmable Logic and Arrays, 1993, pp. 274–280.
[194] H. Singh, M.-H. Lee, G. Lu, F. J. Kurdahi, N. Bagherzadeh, and E. M. C. Filho,
“Morphosys: An integrated reconﬁgurable system for data-parallel and computation-
intensive applications,” IEEE Transactions on Computers, vol. 49, no. 5, pp. 465–481,
2000.
[195] M. J. S. Smith, Application-speciﬁc integrated circuits.
Addison-Wesley Longman
Publishing Co., Inc., 1997.
[196] C. Steiger, “Operating systems for reconﬁgurable embedded platforms: Online schedul-
ing of real-time tasks,” IEEE Trans. Comput., vol. 53, no. 11, pp. 1393–1407, 2004,
member-Herbert Walder and Member-Marco Platzner.
[197] C. Steiger, H. Walder, M. Platzner, and L. Thiele, “Online Scheduling and Placement
of Real-time Tasks to Partially Reconﬁgurable Devices,” in Proceedings of the 24th
International Real-Time Systems Symposium (RTSS’03), December 2003.

References
333
[198] M. A. Tahir, A. Bouridane, and F. Kurugollu, “An fpga based coprocessor for glcm and
haralick texture features and their application in prostate cancer classiﬁcation,” Analog
Integr. Circuits Signal Process., vol. 43, no. 2, pp. 205–215, 2005.
[199] J. Teich, S. P. Fekete, and J. Schepers, “Optimizing dynamic hardware reconﬁguration,”
Angewante Mathematic Und Informatik Universit¨at zu K¨oln, Tech. Rep. 97.228, 1998.
[200] G. R. G. S. J. Thomas, “An optimal parallel jacobi-like solution method for the singular
value decomposition,” in Proc. Int. Conf. on Parallel Processing, January 1988.
[201] C. Torres-Huitzil and M. Arias-Estrada, “Fpga-based conﬁgurable systolic architecture
for window-based image processing,” EURASIP Journal on Applied Signal Processing,
vol. 2005, no. 7, pp. 1024–1034, 2005, doi:10.1155/ASP.2005.1024.
[202] S. Trimberger, “Scheduling designs into a time-multiplexed fpga,” in International
Symposium on Field Programmable Gate Arrays(FPGA 98).
Monterey, California:
ACM/SIGDA, 1998, pp. 153 – 160.
[203] F. Vahid and T. Givargis, Embedded System Design: A Uniﬁed Hardware/Software In-
troduction.
New York, NY, USA: John Wiley & Sons, Inc., 2001.
[204] R. Vaidyanathan and J. L. Trahan, Dynamic Reconﬁguration: Architectures and Algo-
rithms.
IEEE Computer Society, 2003.
[205] R. Vaidyanathan and J. L. Trahan, Dynamic Reconﬁguration: Architectures and Algo-
rithms (Series in Computer Science (Kluwer Academic/Plenum Publishers).).
Plenum
Publishing Co., 2004.
[206] A. van Breemen and T. de Vries, “An agent-based framework for designing multi-
controller systems,” Proc. of the Fifth International Conference on The Practical Ap-
plications of Intelligent Agents and Multi-Agent Technology, pp. 219-235, Manchester,
U.K, Apr. 2000.
[207] C. Veciana-Nogu&#233;s and J. Domingo-Pascual, “Adaptive video on demand service
on rsvp capable network,” in ECMAST ’99: Proceedings of the 4th European Confer-
ence on Multimedia Applications, Services and Techniques.
London, UK: Springer-
Verlag, 1999, pp. 212–228.
[208] “Vhdl online.” [Online]. Available: http://www.vhdl-online.de/
[209] J. E. Volder, “The birth of cordic,” J. VLSI Signal Process. Syst., vol. 25, no. 2, pp.
101–105, 2000.
[210] E. Waingold, M. Taylor, D. Srikrishna, V. Sarkar, W. Lee, V. Lee, J. Kim, M. Frank,
P. Finch, R. Barua, J. Babb, S. Amarasinghe, and A. Agarwal, “Baring it all to
software: Raw machines,” Computer, vol. 30, no. 9, pp. 86–93, 1997. [Online].
Available: citeseer.ist.psu.edu/waingold97baring.html
[211] H. Walder, S. Nobs, and M. Platzner, “Xf-board: A prototyping platform for reconﬁg-
urable hardware operating systems.” in ERSA, 2004, p. 306.
[212] H. Walder and M. Platzner, “A runtime environment for reconﬁgurable hardware oper-
ating systems.” in FPL, 2004, pp. 831–835.

334
References
[213] D. W. Wall, “Limits of instruction-level parallelism,” in Proceedings of the 4th
International Conference on Architectural Support for Programming Languages and
Operating System (ASPLOS), vol. 26, no. 4.
New York, NY: ACM Press, 1991, pp.
176–189. [Online]. Available: http://citeseer.ist.psu.edu/wall90limits.html
[214] K. Weiss, T. Steckstor, C. ¨Otker, I. Katchan, C. Nitsch, and J. Philipp, Spyder - Virtex -
X2 User’s Manual, 1999.
[215] S. A. White, “Application of distributed arithmetic to digital signal processing: A tuto-
rial review,” IEEE ASSP Magazine, pp. 4–19, July 1989.
[216] G. Wigley and D. Kearney, “The development of an operating system for reconﬁgurable
computing,” in Proceedings of the 9th IEEE Symposium Field-Programmable Custom
Computing Machines(FCCM’01).
IEEE-CS Press, April 2001.
[217] S. E. G. William Lehr, Fuencisla Merino, “Software radio: Implications for wireless
services, industry structure, and public policy,” Massachusetts Institute of Technology,
Program on Internet and Telecoms Convergence, Tech. Rep., Aug. 2002.
[218] J. W. Williams and N. Bergmann, “Embedded linux as a platform for dynamically
self-reconﬁguring systems-on-chip,” in Proceedings of the International Conference
on Engineering of Reconﬁgurable Systems and Algorithms, June 2004, pp. 163–169.
[Online]. Available: http://espace.library.uq.edu.au/view.php?pid=UQ:410
[219] M. Wirthlin and B. Hutchings, “A dynamic instruction set computer,” in IEEE
Symposium on FPGAs for Custom Computing Machines, P. Athanas and K. L. Pocek,
Eds.
Los Alamitos, CA: IEEE Computer Society Press, 1995, pp. 99–107. [Online].
Available: citeseer.nj.nec.com/wirthlin95dynamic.html
[220] P. L. E. Wolfgang Rosenstiel, New Algorithms. Architectures and Applications for Re-
conﬁgurable Computing.
Berlin: Springer, 2005.
[221] T. Wollinger, J. Guajardo, and C. Paar, “Security on fpgas: State-of-the-art implementa-
tions and attacks,” Trans. on Embedded Computing Systems, vol. 3, no. 3, pp. 534–574,
2004.
[222] T. J. Wollinger, M. Wang, J. Guajardo, and C. Paar, “How well are high-end dsps suited
for the aes algorithms? aes algorithms on the tms320c6x dsp.” in AES Candidate Con-
ference, 2000, pp. 94–105.
[223] Xilinx, “A guide to using ﬁeld programmable gate arrays (fpgas) for application-speciﬁc
digital signal processing performance,” http://www.xilinx.com, 1995.
[224] Xilinx, “The role of distributed arithmetic design in fpga-based signal processing,”
http://www.xilinx.com, 2000.
[225] Xilinx, “Spartan-3 fpgas,” http://www.xilinx.com, 2000.
[226] Xilinx Inc., “The early access partial reconﬁguration lounge,” (registration required).
[Online]. Available: http://www.xilinx.com/support/prealounge/protected/index.htm
[227] Xilinx Inc., “Early access partial reconﬁguration user guide,” xilinx User Guide
UG208, Version 1.1, March 6, 2006. [Online]. Available: http://www.xilinx.com/
bvdocs/appnotes/xapp290.pdf

References
335
[228] Xilinx Inc., “Virtex-II Pro XUP Development Board,” http://www.xilinx.com/
univ/xupv2p.html.
[229] Xilinx Inc., “Xilinx ISE 8 Software Manuals and Help - PDF Collection,” 2005.
[Online]. Available: http://toolbox.xilinx.com/docsan/xilinx8/books/manuals.pdf
[230] Xilinx Inc., “Edk platform studio documentation,” 2007. [Online]. Available:
http://www.xilinx.com/ise/embedded/edk docs.htm
[231] H. Yang and D. F. Wong, “Efﬁcient network ﬂow based min-cut balanced partitioning,”
in International Conference on Computer-Aided Design, 1994.
[232] Y. Yang, Z. Abid, and W. Wang, “Two-prime rsa immune cryptosystem and its fpga im-
plementation,” in GLSVSLI ’05: Proceedings of the 15th ACM Great Lakes symposium
on VLSI.
New York, NY, USA: ACM Press, 2005, pp. 164–167.

Appendix A
Hints to Labs
This chapter gives a step-by-step guide in note form to create a partially reconﬁgurable
system. It has been developed while reconstructing the Video8 example and is intended to
give hints to the reader on how to create own designs. A more detailed description of the
demonstration project can be found in 5.2 on page 236. The sources of the guide can be found
on the book’s Web page. Following the directory and entity names, refer to the Video8 project.
A profound knowledge of VHDL, ISE and EDK will be needed to comprehend the instructions.

338
Appendix A
Tutorial
Creation of partially reconﬁgurable designs
On the Example of Video8
Version 1.0
1.
Prerequisites
Following basic conditions should be present when accomplishing this guide. In case of
deviations the given approach might not be feasible.
ISE 8.1.01i PR8 or PR12 (Early Access Partial Reconﬁguration patch for ISE)
EDK 8.1.02i
Abbreviations:
PR
partially reconﬁgurable
PRM
partially reconﬁgurable module
TL
top-level
TLM
top-level module
DIR
directory
<base>
root directory of the project
2.
Reorganization of the Project Video8 non pr
1.
elevate PRM (rgbfilter) to the TLM (top.vhd): initially rgbfilter is instantiated in
entity video in. But PRM may not be sub-modules of a static part. This change now
is only the ﬁrst step. rgbfilter will have to be brought up completely to the top-level
module.
1.1.
In video in.vhd: additional ports have to be adjoined to the entity declaration of
video in so that rgbfilter can be instantiated outside this entity but the rest may
stay the same as before.
entity video_in is {
...
LLC_CLOCK_to_filter : out std_logic;
R_out_to_filter : out std_logic_vector(0 to 9);
G_out_to_filter : out std_logic_vector(0 to 9);
B_out_to_filter : out std_logic_vector(0 to 9);
h_counter_out_to_filter : out std_logic_vector(0 to 9);
v_counter_out_to_filter : out std_logic_vector(0 to 9);
valid_out_to_filter : out std_logic;
R_in_from_filter : in std_logic_vector(0 to 9);
G_in_from_filter : in std_logic_vector(0 to 9);
B_in_from_filter : in std_logic_vector(0 to 9);
h_counter_in_from_filter : in std_logic_vector(0 to 9);
v_counter_in_from_filter : in std_logic_vector(0 to 9);
valid_in_from_filter : in std_logic
...
};

Appendix A
339
1.2.
In video in.vhd: modules that formerly accessed the rgbfilter instance have to be
redirected to outward ports. For example simply by rewriting corresponding signals:
-- signals out to the filter:
R_out_to_filter <= R1;
G_out_to_filter <= G1;
B_out_to_filter <= B1;
h_counter_out_to_filter <= h_counter1;
v_counter_out_to_filter <= v_counter1;
valid_out_to_filter <= valid1;
-- processed signals in from the filter:
R2 <= R_in_from_filter;
G2 <= G_in_from_filter;
B2 <= B_in_from_filter;
h_counter2 <=
h_counter_in_from_filter;
v_counter2 <=
v_counter_in_from_filter;
valid2 <= valid_in_from_filter;
2.
making the PRM available to EDK (not necessary; good for testing purposes)
2.1.
adapt .mpd and .pao (PCORE-deﬁnition ﬁle und -synthesizing directives) for the
video in-module:
create .mpd from .vhd with:
psfutil -hdl2mpd video_in_pr.vhd -bus opb m \\
-lang vhdl -o VIDEO_IN_PR_v2_1_0.mpd
In .pao: delete rgbfilter since it’s a PRM. It’ll be synthesized separately.
lib VIDEO_IN_v1_00_a rgbfilter
löschen.
rename video in to video in pr
2.2.
Create an own PCORE for rgbfilter:
copy video in
ﬁt .pao:
lib RGBFILTER_v1_00_a rgbfilter
use only rgbfilter.vhd and create .mpd from it
2.3.
rename VIDEO IN v1 00 a to VIDEO IN PR v1 00 a
2.4.
include Video in pr and rgbfilter in the EDK project.
The custom PCOREs
can be found under ‘IP Catalog’, ‘Project Repository’. Erase the PCORE video in
from the EDK project. Now wire the ports: rgbfilter receives several signals from
video in. video in takes in the results from rbgfilter.
2.5.
Attention has to be paid to the correct address ranges of opb2plb-bridge :
0x0 - 0x0fffffff
The modules accessible over these addresses in this block will be supplied to the bus
participants on every side of the bridge. This is important because the data ﬂows from
video in to the frame buffer situated in the RAM (→Memory-Mapped I/O).

340
Appendix A
2.6.
Synthesis in EDK (for testing). If errors occur like
ERROR:HDLParsers:3317 - "D:/murr/edkProjects/ \\
Video8_81_PR/hdl/video_in_pr_0_wrapper.vhd" Line 10.
Library VIDEO_IN_PR_v1_00_a cannot be found.
ERROR:HDLParsers:3014 - "D:/murr/edkProjects/ \\
Video8_81_PR/hdl/video_in_pr_0_wrapper.vhd" Line 11.
Library unit VIDEO_IN_PR_v1_00_a \\
is not available in library work.
then the .pao ﬁles have not been altered correctly as stated above (e.g. not all occur-
rences of video in have been changed to video in pr)
2.7.
add additional ﬁlters if desired (e.g. a mean-value ﬁlter)
2.8.
place the address of the frame buffer at the end of the DDR-RAMs (at address
0x00001111111000000000000000000000) to allow the Linux System to start from
0x0
→adapt ﬁles, since Video8 unfortunately received hard-coded addresses from its au-
thor:
In rgbstream2framebuffer.vhd:
next_wb_counter <= ram_out(24 to 33) + \\
(ram_out(34 to 43) * "00000000001010000000") ;
changes to:
next_wb_counter <= ram_out(24 to 33) + \\
(ram_out(34 to 43) * "00000000001010000000") + \\
"000011111110000000000000000000" ;
adapt simple timer unit.vhd:
reader_addr_plb_unsynced <= (others => ’0’);
changes twice to:
reader_addr_plb_unsynced <= \\
"00001111111000000000000000000000"
3.
prepare the EDK-project for PR:
3.1.
supply additional IP-Cores:
JTAG-PPC controller (debugging)
opb sysace
opb timer (to measure the time the reconﬁguration process takes)
opb hwicap
3.2.
Bus connections
connect opb timer with the opb the sysace is already placed at
connect opb hwicap with the same opb
3.3.
ﬁll in addresses:

Appendix A
341
sysace: 0x41800000 - 64K
opb timer: 0x41c00000 - 64K
opb hwicap: 0x40200000 - 64K
docm ctlr: 0xE8800000 - 32K →so that the program ﬁts into the BRAMs.
If put in the RAM unwanted deviations occur in the resulting picture.
iocm ctrl: 0xFFFF8000 - 32K /
3.4.
extend the .ucf with sysace-pinouts. Entries of the form:
Net fpga_0_SysACE_CompactFlash_SysACE_CLK_....
→see appendix B
3.5.
connect ports:
opb sysace:
OPB_Clk - sys_clk_s
SysACE_MPA -> make external
SysACE_CLK -> make external
SysACE_MPD -> make external
SysACE_MPD_I -> no connection
SysACE_MPD_O -> no connection
SysACE_MPD_T -> no connection
SysACE_CEN -> make external
SysACE_OEN -> make external
SysACE_WEN -> make external
SysACE_MPIRQ -> no connection
→not to forget to adapt the pins in the .ucf
opb timer:
OPB_clk -> sys_clk_s
also ﬁt the external ports to the ones in the .ucf
3.6.
Extend the C-program for PR with ICAP:
copy xhwicap* ﬁles from John William’s ICAP driver [218]
adapt main.c: ﬁrst conﬁgure the video decoder card, then present the PR menu
can e.g.
sysace stdio.h not be found add the xilfatfs-library under
Software|Software Platform Settings|Platform Settings
create a linker script for the application (if possible put all the program parts to
iocm- resp. docm-memory (→RAM very often causes problems ⇒if necessary
extend the address space of the corresponding controller)
3.7.
Testing: completely compile with EDK and see if the system works so far
3.8.
enter ports for the PRM in the port declaration of entity system as external ports:
close the EDK project
redirect ports to and from rgbfilter from the local instance to the corresponding
external ports:
→add to system.mhs at the external port (at the beginning of the ﬁle):
PORT LLC_CLOCK_OUT = VIDEO_IN_PR_0_LLC_CLOCK_to_filter,
DIR = O

342
Appendix A
PORT R_in = VIDEO_IN_PR_0_R_out_to_filter, DIR = O, VEC
= [0:9]
PORT G_in = VIDEO_IN_PR_0_G_out_to_filter, DIR = O, VEC
= [0:9]
PORT B_in = VIDEO_IN_PR_0_B_out_to_filter, DIR = O, VEC
= [0:9]
PORT h_counter_in = VIDEO_IN_PR_0_h_counter_out_to_
filter, \\
DIR = O, VEC = [0:9]
PORT v_counter_in = VIDEO_IN_PR_0_v_counter_out_to_
filter, \\
DIR = O, VEC = [0:9]
PORT valid_in = VIDEO_IN_PR_0_valid_out_to_filter, DIR = O
PORT R_out = RGBFILTER_0_R_out, DIR = I, VEC = [0:9]
PORT G_out = RGBFILTER_0_G_out, DIR = I, VEC = [0:9]
PORT B_out = RGBFILTER_0_B_out, DIR = I, VEC = [0:9]
PORT h_counter_out = RGBFILTER_0_h_counter_out, \\
DIR = I, VEC = [0:9]
PORT v_counter_out = RGBFILTER_0_v_counter_out, \\
DIR = I, VEC = [0:9]
PORT valid_out = RGBFILTER_0_valid_out, DIR = I
reopen EDK project
erase instance rgbfilter in System Assembly|Bus Interface
3.9.
export to ISE:
→in Project|Project-Options|Hierarchy and Flow:
Design is a sub-module: name: system i
do not synthesize non-Xilinx IPs
use project navigator implementation ﬂow (ISE)
do not add modules to an existing ISE
4.
Synthetizing the project with ISE
4.1.
copy the content of the EDK-directory to <base>/synth/edk
4.2.
Verify the ISE project:
→open system.ise in base/synth/edk/projnav with ISE:
→verify if the design is structured as follows:
system stub.vhd (system stub)
system.vhd (system i)
ppc405 0 wrapper (ppc405 0)
ppc405 1 wrapper (ppc405 1)
jtagppc 0 wrapper (jtagppc 0)
reset block wrapper (reset block)
...
system stub.bmm
→if the structure is not like the above, check for the correct export settings in EDK.
Mind the ‘do not add modules to an existing ISE project’-setting!
4.3.
mark and rightclick system stub, select ’Synthesize - XST’-> properties: Xilinx Spe-
ciﬁc Options-> do not add I/O Buffers (since the system is to be a sub-module)

Appendix A
343
4.4.
create netlists for the EDK-module with doubleclicking ‘Synthesize - XST’
4.5.
the suiting Block-RAM memory mapping ﬁle (system stub bd.bmm) can be created
with doubleclicking ‘Generate Programming File’ (every step necessary will be done,
including synthetization)
→if that does not work out there is probably something wrong (a workaround might be
compiling with I/O-buffers to get the system stub bd.bmm and then again without to gen-
erate the appropriate .ngc-ﬁles)
5.
Changes to the skript framework: in directory synth
The example Video8 comes with a bunch of scripts to support up the PRM generation. This
part gives hints on how to change those appropriately.
Directories:
mod * directories according to the modules (sobel, sharpen, mean-value, unﬁltered)
ändern
top-directory remains
static - not needed (in this case contained in edk
edk - copy previously generated design here
Files:
adapt doit.cmd →descend in new DIR and execute xst
in these DIR: adapt .prj and .xst (are the same in all the DIR)
6.
Changes to the project ﬁles
6.1.
top.vhd (takes the longest time to accomplish):
adapt the entity-deﬁnition (ports) that are connected to the I/Os and further to the
ucf
not to forget the I, O and I/O buffers →IBUFs resp. OBUFs remain simple,
IOBUFs are extended to I, O and T (tristate)-line (e.g. for Sda decoder: one pin
to the outside, but * I, * O and * T towards the entity system i →very time and
work consuming!
add/erase components that are (not) needed:
→adapt component system ⇒comes up to the system from the EDK project →
can be copied from <base>/synth/edk/hdl/system.vhd (system stub.vhd
here is only a dummy to be able to instantiate system.vhd as a sub-module in
the ISE project)
→guide the PRM signals as ports out in entity system
add/remove corresponding instances
care for intermediate signals if signals have to run directly from component in-
stances to I/Os
insert bus macros for signals coming from/going to the PRM (here: R,G u. B to
and from rbgfilter, v and h counter to and from rgbfilter and valid to
and from rgbfilter
→again, add intermediate signals
additional instances, that are to be static or reconﬁgurable later on have to be
named →the deﬁnition will be done in the ucf (e.g. recon module: rgbfilter
portmap( ....)

344
Appendix A
6.2.
top.ucf:
place bus macros (PlanAhead or FPGA-Designer can pose a big help here)
assign areas for static and reconﬁgurable modules (→in PlanAhead this can be
done with drag-and-drop manner!)
assign which instance from top.vhd is member of which group (static ↔recon-
ﬁgurable)
7.
The Build Process:
7.1.
synthesize all ﬁles:
if not done yet: synthesize the ISE project in
<base>/synth/edk/projnav
(system stub bd.bmm) may not be omitted!)
execute <base>/synth/synthesize all.cmd
7.2.
begin with the Early Access Partial Reconﬁguration Design Flow:
→execute buildit.cmd ⇒the full and partial bitstreams should be built if the script
has been adapted properly.

Appendix B
Example of a User Constraints File
The following chapter gives an excerpt of a User Constraints File (UCF). It has been
created for the partial reconﬁguration of the Video8 sample design. The complete ﬁle can be
found on the book’s Web site.
# area group for reconfig module
INST "recon_module" AREA_GROUP = "pblock_recon_module";
AREA_GROUP "pblock_recon_module" RANGE=MULT18X18_X3Y8:MULT18X18_X3Y11;
AREA_GROUP "pblock_recon_module" RANGE=RAMB16_X3Y8:RAMB16_X3Y11;
AREA_GROUP "pblock_recon_module" RANGE=SLICE_X30Y64:SLICE_X45Y95;
AREA_GROUP "pblock_recon_module" RANGE=TBUF_X30Y64:TBUF_X44Y95;
AREA_GROUP "pblock_recon_module" MODE=RECONFIG;
# INST "fixed_area" AREA_GROUP = "pblock_fixed_area";
INST "system_i" AREA_GROUP = "pblock_fixed_area";
INST "alibi" AREA_GROUP = "pblock_fixed_area";
INST "dcm_25_MHZ" AREA_GROUP = "pblock_fixed_area";
AREA_GROUP "pblock_fixed_area" RANGE=SLICE_X46Y16:SLICE_X87Y149;
# Clock and reset constraints
Net sys_clk_pin LOC=AJ15;
Net sys_clk_pin PERIOD = 10000 ps;
Net reset LOC=AG5 | IOSTANDARD = LVTTL; # Center
Net reset TIG;
Net sys_rst_pin LOC=AH5;
Net sys_rst_pin IOSTANDARD = LVTTL;

346
Appendix B
INST "dcm_25_MHZ" loc="DCM_X2Y0";
# leds
net LED_0 loc = AC4 | IOSTANDARD = LVTTL;
net LED_1 loc = AC3 | IOSTANDARD = LVTTL;
net LED_2 loc = AA6
| IOSTANDARD = LVTTL;
net LED_3 loc = AA5
| IOSTANDARD = LVTTL;
# push buttons
net button_start loc = AH2 | IOSTANDARD = LVTTL; # Right
net button_stop loc = AH1 | IOSTANDARD = LVTTL; # Left
#### Diligent general purpose expansion port (where the VDEC1 is
connected)
NET "YCrCb_in*
LOC = "AA8" | IOSTANDARD = LVTTL ;
.
.
.
NET "LLC_CLOCK"
LOC = "B16" | IOSTANDARD = LVCMOS25 ;
NET "RESET_VDEC1_Z"
LOC = "AF6" | IOSTANDARD = LVTTL
| DRIVE = 8 ;
NET "VDEC1_PWRDN_Z"
LOC = "G1" | IOSTANDARD = LVTTL
| DRIVE = 8 ;
NET "VDEC1_OE_Z"
LOC = "AF3" | IOSTANDARD = LVTTL
| DRIVE = 8 ;
NET "Sda_decoder_pin"
LOC = "AE5" | IOSTANDARD = LVTTL
| DRIVE = 8 |
PULLUP = TRUE;
NET "Scl_decoder_pin"
LOC = "AB8" | IOSTANDARD = LVTTL
| DRIVE = 8 |
PULLUP = TRUE;
#### VGA
NET "BLANK_Z" LOC = A8 | DRIVE = 12 | SLEW = SLOW | IOSTANDARD
= LVTTL;
NET "COMP_SYNC" LOC = G12 | DRIVE = 12 | SLEW = SLOW | IOSTANDARD
= LVTTL;
NET "H_SYNC_Z" LOC = B8 | DRIVE = 12 | SLEW = SLOW | IOSTANDARD
= LVTTL;
NET "V_SYNC_Z" LOC = D11 | DRIVE = 12 | SLEW = SLOW | IOSTANDARD
= LVTTL;
NET "PIXEL_CLOCK" LOC = H12 | DRIVE = 12 | SLEW = SLOW | IOSTANDARD
= LVTTL;
INST "red_out_DAC*
INST "green_out_DAC*
INST "blue_out_DAC*

Appendix B
347
#### Bus Macros
INST "BM_1" LOC = SLICE_X44Y94;
INST "BM_3" LOC = SLICE_X44Y92;
INST "BM_2" LOC = SLICE_X44Y90;
INST "BM_4" LOC = SLICE_X44Y88;
INST "BM_5" LOC = SLICE_X44Y86;
INST "BM_6" LOC = SLICE_X44Y84;
INST "BM_7" LOC = SLICE_X44Y82;
INST "BM_8" LOC = SLICE_X44Y80;
INST "BM_9" LOC = SLICE_X44Y78;
INST "BM_10" LOC = SLICE_X44Y76;
INST "BM_11" LOC = SLICE_X44Y74;
INST "BM_12" LOC = SLICE_X44Y72;
INST "BM_13" LOC = SLICE_X44Y70;
INST "BM_14" LOC = SLICE_X44Y68;
INST "BM_1" AREA_GROUP = "AG_BM_1";
INST "BM_2" AREA_GROUP = "AG_BM_2";
INST "BM_3" AREA_GROUP = "AG_BM_3";
INST "BM_4" AREA_GROUP = "AG_BM_4";
INST "BM_5" AREA_GROUP = "AG_BM_5";
INST "BM_6" AREA_GROUP = "AG_BM_6";
INST "BM_7" AREA_GROUP = "AG_BM_7";
INST "BM_8" AREA_GROUP = "AG_BM_8";
INST "BM_9" AREA_GROUP = "AG_BM_9";
INST "BM_10" AREA_GROUP = "AG_BM_10";
INST "BM_11" AREA_GROUP = "AG_BM_11";
INST "BM_12" AREA_GROUP = "AG_BM_12";
INST "BM_13" AREA_GROUP = "AG_BM_13";
INST "BM_14" AREA_GROUP = "AG_BM_14";
#### Module RS232_Uart_1 constraints
Net fpga_0_RS232_Uart_1_RX_pin LOC=AJ8;
Net fpga_0_RS232_Uart_1_TX_pin LOC=AE7;
#### Module SysACE_CompactFlash constraints
Net fpga_0_SysACE_CompactFlash_SysACE_*
.
.
.
#### Module DDR_256MB_32MX64_rank1_row13_col10_cl2_5 constraints
Net fpga_0_DDR_256MB_32MX64_rank1_row13_col10_cl2_5_DDR_*
.
.
.

Appendix C
Quick Part-Y Tutorial
This quick tutorial gives an introduction to the basic features of the Part-Y [71] tool devel-
oped by Florian Dittmann from the ‘Heinz Nixdorf Institute’ at the University of Paderborn.
It is intended to guide a novice to Part-Y and partial reconﬁguration through a ﬁrst example
of a PR design. The targeted system was the Xilinx XUP Development Board with a Virtex-II
Pro FPGA.
Prerequisites:
–
running version of Part-Y (this tutorial was created with version 1.2)
–
the tutorial sources.zip ﬁle
–
ISE 6.1 or higher installed (tutorial is known to work on 7.1.04i)
Notes:
–
always hit OK before changing the tab in a view – changes will not be saved otherwise
–
StdOut will be directed to Miscellaneous∥Standard Output
–
you might have extended the Part-Y Project by the proper deﬁnition for your FPGA.
These can be found under
de.upb.cs.roichen.party.device.concrete
–
encountering problems you might ﬁnd Part-Y FAQ.txt useful
Proceeding:
1.
Open the level conﬁguration window and ensure that the following items from the
different views are checked:
–
BehavioralModuleSelectionController
–
BehavioralTopLevelController
–
BehavioralDownloadController
–
StructureTopAssemblyController
–
StructureTreeLevelController
–
GeoSystemLevelController

350
Appendix C
–
MiscStorageController
–
MiscGenerateBitstreamsController
–
MiscOutputController
2.
In Module Selection import, the modules rt1.vhd, rt2.vhd and ﬁxed.vhd from tuto-
rial sources.zip. Add them to current modules and mark each of them as reconﬁg-
urable. Hit OK!
3.
In Top, import global.vhd and global2.vhd. Add global.ucf to each of the Top-Level-
Designs marking one at a time and adding the .ucf separately. OK!
4.
Top Assembly: mark Top0 global and Top1 global2 each at a time and add every
module to both of them. You should see the added modules at the ‘modules instances
of selected top’-window.
5.
If everything worked out up to now, you will see a hierarchy like the one exposed in
ﬁgure [C.1] on page 350. in Structural View|Tree.
Figure C.1.
Tree View after Top Assembly

Appendix C
351
6.
In Geometrical View|System: set the correct target platform and a proper bus macro
ﬁle.
7.
Miscellaneous|Storage: set a name for the Part-Y project and a project path. Clicking
OK will create a ﬁle hierarchy at the given spot, INTEGRATE will copy the vhdl ﬁles
and others to the correct places. Be sure not to use spaces in path names because
Part-Y as well as ISE cannot cope with them properly. If everything went right, there
should be a bunch of subfolders in your speciﬁed directory containing copies of the
vhdl ﬁles you declared and a couple of additional ﬁles.
8.
In Behavioral View|Synthesis, all of the VHDL-ﬁles have to be synthesized.
9.
With Miscellaneous|Bitstream Generation, you can conduct the initialization phase,
the activation as well as the ﬁnal assembly phase by clicking on the run buttons. For
some reason, this will not look pressed when they are – might be a bug in the GUI.
You can check if the action is taking place in the ‘standard output’ window.
10.
After everything has been accomplished, the bitstreams can be downloaded to the de-
vice with Behavioral|Download using iMPACT.

Index
0-1 variable, 129
ACM, 56, 57
Actel, 30, 32, 35, 36, 43, 44, 45, 74
Active copy, 274
Active module, 219, 220, 222, 223
Adaptive computing machine, 56
Adaptive computing system, 11
Adaptive controller, 304, 305, 307, 308, 310
Adaptive cryptographic system, 310, 312
Adaptive execution node, 57
Adaptive logic module, 39
Adjacent communication, 255
Admissible, 156
ALAP, 120, 121, 122, 123, 125, 132
Algorithmic engine, 56, 57
Allocation, 108, 109, 120, 149, 280
ALM, 39, 45, 47
Altera, 37, 39, 45, 46, 48, 65, 74, 260, 263
ALU, 21, 22, 49, 51, 52, 53, 54, 57, 60, 61, 62,
66, 261, 262, 263
AMBA, 264, 267, 268
Antifuse, 29, 30, 31, 44
Application, 2, 5, 6, 7, 8, 9, 10, 15, 16, 20, 22,
24, 26, 48, 50, 58, 62, 63, 64, 67,
68, 72, 99, 100, 110, 111, 115, 139,
150, 176, 184, 185, 211, 216, 221,
226, 245, 251, 255, 256, 261, 263,
265, 269, 270, 276, 277, 278, 280,
281, 282, 283, 284, 285, 292, 294,
304, 305, 307, 308, 311, 312, 315,
316, 317
Architecture, 1, 2, 4, 11, 13, 15, 16, 18, 19, 21,
23, 25, 26, 39, 48, 50, 52, 54, 55,
56, 57, 58, 60, 61, 64, 65, 69, 73,
109, 110, 123, 130, 144, 145, 151,
160, 186, 187, 188, 189, 190, 191,
194, 200, 243, 247, 250, 251, 253,
261, 262, 263, 264, 267, 269, 270,
271, 272, 279, 290, 294, 295, 296,
297, 298, 300, 307, 308, 310, 311,
312, 313, 314, 315
Area, 26, 29, 30, 31, 65, 69, 75, 80, 81, 86, 96,
98, 101, 109, 112, 114, 115, 116,
130, 146, 152, 160, 161, 165, 167,
173, 184, 189, 194, 195, 199, 200,
201, 202, 204, 210, 220, 221, 223,
225, 226, 229, 231, 232, 234, 241,
243, 246, 247, 248, 269, 275, 285,
304, 306, 307
ARM, 267
ASAP, 120, 121, 122, 123, 125, 132
ASIC, 6, 10, 68, 72, 74, 295, 311
ASIP, 6, 7, 9
Atmel, 40, 41, 42, 43, 74, 294
Augmenting path, 95, 96, 97, 134
Automaton, 289, 290, 292, 293, 294
AXN, 57
BabyBoard, 249, 250, 252
Bandwidth, 1, 181, 185, 188, 194, 195, 241,
267, 281, 282
Base minimization, 155
basic blocks, 17, 38
BDD, 78, 79
Best-ﬁt, 85, 151, 152, 154, 162
Binding, 108, 109, 120
BlockRAM, 49, 255
Boolean, 27, 33, 73, 76, 77, 78, 79, 80, 81, 82,
83, 87, 89, 96, 104, 308, 311
equation, 73, 77
network, 76, 77, 79, 80, 81, 82, 83, 87,
89, 96
Bottom left, 52, 162, 165, 169

354
Index
Bus Macro, 186, 217, 221, 225, 226, 227, 228,
229, 230, 231, 242, 243, 245, 254
Bus-based, 181, 183, 266, 269, 270
Busing plane, 41
Capacity, 25, 27, 28, 59, 65, 84, 87, 93, 94, 95,
133, 148, 251, 252, 259, 268, 270,
287, 288, 290, 316
Channel, 40, 43, 44, 47, 49, 51, 182, 186, 187,
188, 193, 196, 197, 296, 297, 313
Chip multiprocessor, 269, 270
Chortle, 80, 82, 83, 84, 85, 86, 87, 88, 89, 96
Circuit switching, 183, 184, 185, 194, 195,
212, 258
CLB, 38, 40, 41, 42, 216, 294
CMOS, 23, 31
Collapsing, 80, 94, 97
Combinatorial, 23, 35, 38, 39, 75
Communication cost, 117, 118, 166, 168, 169,
176, 178, 179, 181
Communication memory, 115, 118, 131, 132
constraint, 131
Comparability graph, 158, 159
Compile-time, 9, 24, 56, 150, 181, 182, 184,
209, 260, 261, 270
Complement graph, 156, 159
Complex programmable logic device, 15, 28
Condition, 81, 102, 103, 104, 105, 133, 137,
157, 202, 221, 251, 297
Cone, 82, 83, 84, 89, 90
Conﬁgurable logic bloc, 28, 37, 38, 225
Conﬁguration, 9, 10, 11, 16, 21, 22, 23, 24, 25,
26, 28, 31, 32, 37, 49, 50, 51, 52,
53, 54, 57, 58, 62, 63, 64, 68, 69,
70, 71, 72, 74, 75, 112, 115, 116,
117, 126, 132, 142, 144, 145, 146,
147, 154, 161, 163, 164, 168, 181,
182, 197, 200, 202, 203, 204, 214,
215, 216, 221, 225, 235, 240, 244,
250, 252, 267, 271, 276, 278, 279,
281, 282, 294, 311, 312, 313
graph, 115, 116, 132, 142
manager, 50, 52, 252
memory, 22, 51, 58, 62, 64, 214
switching, 145, 146, 147
tree, 50
Connection matrix, 137
Connectivity, 117, 118, 119, 127, 134, 237, 253
Constant node, 78
Context switching, 144, 145
Contour of rectangle, 172
Control path, 2, 3, 16
Convolution, 294, 296, 305, 306
CoreConnect, 266, 267, 268
CPLD, 15, 28, 252, 253
Critical path, 124, 289
Customization, 10, 29, 263, 316
Cut, 31, 88, 89, 90, 91, 92, 93, 94, 95,
96, 97, 132, 133, 134, 142, 143,
144, 156, 196
Cut-size, 89, 93, 95, 96, 132
Cyclone, 39, 45
DALUT, 299, 300, 301, 302, 303, 306, 307
DAP/DNA, 58, 59, 72
Data management unit, 53
Dataﬂow graph, 100, 101, 102, 103, 109,
111, 113, 115, 116, 117, 118,
120, 121, 123, 126, 127, 128, 129,
130, 132, 133, 134, 136, 139, 140,
143, 144, 150, 151, 152, 153, 156,
157, 158, 159
Dataﬂow machines, 50
Datapath, 16, 22, 61, 62, 64, 65, 100, 102, 103,
104, 107, 110, 111, 113, 261, 262,
264, 268, 294, 295, 299, 300, 301,
302, 303, 304, 309, 316
DBN, 57
DCM, 49
DDR, 47, 48, 241
Deadlock,
195, 196, 197, 198, 204, 205,
206, 207
DEC, 23
Decomposition, 79, 84, 85, 86, 96, 282
Defragmentation, 26, 69
Degree matrix, 137
Dependency, 76, 101, 102, 114
Design ﬂow, 67, 71, 72, 73, 213, 214, 215, 217,
218, 221, 226, 240, 245, 279
DFG, 101, 113, 115, 131, 137, 141
Digital clock manager, 49
Direct communication, 181, 182, 200, 254, 255
Distributed arithmetic, 13, 286, 298, 299, 300,
301, 302, 304, 305, 307, 309
Distributed network architecture, 58
DMA, 57, 62, 267, 274
DNA, 58, 59
Domain bit manipulation, 57
DRP, 50, 52, 53, 54, 72
DSP, 5, 48, 188, 270, 311
Dynamic, 12, 26, 53, 58, 61, 64, 65, 71, 83, 84,
98, 160, 185, 199, 201, 204, 212,
221, 247, 249, 254, 267, 307
Dynamic Network on Chip, 199
DyNoC, 199, 200, 201, 202, 204, 205, 209, 210,
211, 212, 258
Early access design ﬂow., 213
EDALUT, 303
EDIF, 74, 245
EDK, 234, 235, 237, 238, 239, 240, 241, 242,
243, 261, 278
EEPROM, 29, 32, 44, 55, 68
Eigenvalues, 18, 120, 138, 139, 140
Elimination, 80, 142, 289

Index
355
Embedded Development Kit, 261
Empty rectangle, 161, 162, 163, 165
EPROM, 32
Erlangen Slot Machine, 214, 249, 251, 256,
257, 258
ESM, 247, 249, 250, 251, 252, 253, 254, 255,
258, 295, 297
Estrin, 16, 17, 18, 19
Extraction, 79
Factored form, 77, 80
Factoring, 80
Fan-in, 81, 82, 84, 85, 87, 96
Fan-out, 47, 81, 82, 83, 84, 86, 87, 88, 96
FastTrack, 45
Field Programmable Gate Arrays, 12, 15, 28
Final Assembly, 218, 232, 243
Finite state machine, 23, 53, 72, 100, 102, 103,
104, 107, 259, 273, 289
Fix part, 215, 216, 221
Fix-plus machine, 16, 17, 18
Flash, 29, 32, 60, 68, 235, 240, 241, 252
FLEX, 39, 45
Flip Flop, 23, 27, 28, 35, 37, 39, 144, 288, 290,
291, 292
FlowMap, 80, 88, 89, 91, 92, 96, 97, 132, 133
Flow-pack, 96
Force-Directed List scheduling, 124
Forward-Register, 51
FPGA, 9, 12, 15, 19, 23, 24, 25, 26, 28, 29, 30,
31, 32, 35, 36, 37, 39, 40, 41, 43,
44, 46, 47, 49, 50, 58, 64, 65, 67,
70, 72, 73, 74, 75, 77, 80, 81, 98,
100, 144, 145, 152, 184, 185, 186,
187, 200, 210, 211, 213, 216, 221,
225, 228, 232, 234, 235, 237, 240,
241, 242, 247, 248, 250, 251, 252,
253, 254, 255, 256, 257, 259, 261,
263, 264, 270, 272, 276, 278, 279,
281, 284, 285, 288, 290, 292, 294,
295, 298, 303, 306, 307, 311, 312,
315, 316, 317
Frame, 62, 125, 214, 297, 298
Free space, 151, 152, 161, 163, 164, 165, 173,
174, 175, 176, 202
Frequently, 30, 49, 68, 71, 198
FSM, 72, 100, 104, 107, 211, 273, 274, 289,
290, 292, 293
FSMD, 104, 105, 106, 107
Full reconﬁguration, 149, 181, 244, 270
Function generator, 31, 33, 35, 36, 37, 48, 49, 77
Functionality, 15, 16, 30, 54, 62, 63, 73, 193,
227, 250, 253, 297
Gate, 9, 12, 15, 26, 27, 32, 37, 44, 65, 67,
96, 109
General purpose, 1, 16, 26, 52, 57, 61, 64, 253,
261, 262, 263, 313
Graph, 76, 78, 81, 82, 83, 84, 89, 90, 91, 92,
93, 95, 96, 100, 101, 102, 103, 109,
110, 111, 113, 114, 115, 117, 118,
119, 120, 121, 124, 125, 126, 127,
132, 133, 134, 136, 139, 140, 142,
145, 146, 147, 153, 156, 158, 159,
160, 290
Handel-C, 73, 214, 244, 245, 294, 304, 306, 316
Hardware emulation, 10
Hashed Table, 288
Height, 89, 91, 92, 93, 101, 112, 136, 150, 153,
155, 164, 166, 167, 168, 169, 173,
174, 175, 209, 210, 221, 225
High-level synthesis, 12, 100, 108, 110, 111,
120, 126, 148, 149
Host, 19, 20, 21, 22, 24, 26, 52, 68, 70, 71, 118,
183, 289, 316
Hybrid, 48, 49, 200
ICAP, 70, 235, 237, 241, 242, 283, 313
ILP, 3, 129, 131, 132, 154, 268
Implementation, 4, 5, 6, 7, 10, 12, 15, 19, 26, 27,
34, 44, 54, 63, 65, 67, 68, 69, 71,
72, 75, 76, 77, 80, 81, 83, 84, 86,
99, 100, 101, 102, 109, 110, 112,
113, 115, 132, 146, 149, 171, 174,
179, 186, 187, 189, 194, 195, 199,
201, 210, 211, 212, 213, 218, 219,
220, 222, 223, 228, 236, 242, 244,
246, 247, 256, 260, 262, 267, 269,
271, 272, 276, 281, 282, 285, 286,
287, 288, 289, 290, 291, 292, 294,
295, 300, 302, 303, 304, 305, 306,
307, 308, 310, 311, 312, 313, 314,
315, 316, 317
Impossible placement region, 166, 167, 168,
173, 178
ImpulseC, 73, 316
Initial Budgetting, 220
Instruction, 2, 3, 4, 5, 6, 8, 21, 22, 24, 26, 53, 54,
58, 59, 60, 61, 70, 105, 245, 261,
262, 263, 264, 268, 269
Instruction Level Parallelism, 3, 268
Instruction set extension fabric, 59
Integer linear programming, 120, 129, 154, 155
Interconnection, 22, 28, 29, 40, 44, 49, 51, 53,
62, 81, 88, 181, 263, 264, 265, 266,
268, 271, 272, 276, 282, 312
Interfaces, 52, 53, 54, 55, 56, 57, 58, 60, 243,
244, 245, 247, 249, 253, 262, 263,
264, 267, 310, 316
Interval graph, 156, 157, 158, 159
IPﬂex, 58, 59
IPR, 166, 167, 168, 170, 171, 172,
174, 178
ISEF, 59, 60

356
Index
JBits, 213, 216, 217, 252
JTAG, 52, 55, 57, 241, 253, 263
KAMER, 162, 163, 165
Keyword, 286, 287
K-feasible, 82, 83, 89, 92
LAB, 39, 45, 46, 47
Label, 89, 90, 91, 92, 93, 96, 105, 121
Laplacian, 140
Latency, 3, 101, 102, 112, 114, 121, 123, 124,
126, 127, 150, 189, 194, 195, 196,
209, 210, 256, 289
LE, 39, 47
Level, 3, 12, 15, 21, 27, 34, 44, 45, 50, 51,
56, 57, 61, 75, 76, 77, 81, 84, 85,
89, 90, 99, 100, 101, 102, 103, 104,
105, 107, 108, 109, 111, 113, 115,
117, 119, 121, 123, 125, 127, 129,
131, 133, 135, 137, 139, 141, 143,
145, 147, 148, 187, 189, 212, 217,
218, 219, 220, 221, 222, 224, 227,
228, 231, 232, 237, 238, 239, 242,
243, 244, 245, 251, 268, 269, 276,
279, 285, 296, 316
Link node, 103
List scheduling, 120, 123, 124, 127
Livelock, 197, 198, 207
Logic array block, 39, 45
Logic element, 39, 45
Logic module, 31, 35, 39, 75
Logic replication, 87, 88
Long line, 45, 221
Look up table, 11, 31
LUT, 12, 35, 36, 37, 39, 48, 49, 64, 65, 73, 80,
81, 82, 83, 84, 85, 86, 87, 88, 89,
90, 91, 92, 96, 97, 98, 109, 132,
211, 216, 264, 298, 299
Macro cell, 28, 43, 44, 45
Manipulation, 21, 22, 57, 77, 79, 216, 252
Mapping, 12, 19, 63, 64, 67, 72, 74, 77, 80, 81,
82, 88, 89, 90, 91, 92, 97, 98, 100,
222, 240, 265, 269, 280, 281, 292
Matrix, 16, 40, 56, 58, 120, 137, 139, 140, 171,
286, 293, 309, 315
Matrix interconnect network, 56, 57
Max-ﬂow, 95, 132, 134
MDALUT, 303
Memory, 2, 3, 5, 16, 21, 22, 23, 24, 25, 26, 29,
32, 35, 48, 49, 50, 53, 54, 55, 57,
58, 59, 60, 61, 62, 65, 71, 75, 77,
98, 115, 118, 131, 188, 193, 194,
196, 200, 210, 214, 240, 251, 252,
255, 259, 260, 261, 262, 263, 264,
265, 266, 267, 269, 270, 271, 272,
273, 274, 275, 289, 290, 296, 297,
298, 300, 301, 303, 307, 309
META-46 GOLDLAC, 20
MicroBlaze, 261, 263, 281, 283
MIN, 56, 57, 95, 132, 133, 134, 176, 177
Min-cut, 95, 132, 133, 134
Mobility, 120, 123, 124, 125
Modelling, 100, 107, 111
Modular design ﬂow, 213, 214, 217, 218, 219,
224, 225, 227, 237, 245, 257
Module assembling, 224
MotherBoard, 16, 18, 249, 250, 253
Multi-level, 75, 76, 77, 84, 85, 86
Multi-level decomposition, 84, 85, 86
Multiplexer, 31, 33, 34, 35, 39, 74, 107, 131,
192, 267
Multiply accumulate, 5, 57, 262, 282
MultiTrack, 45
MUX, 33, 34, 35, 49, 308
Nearest possible point, 170, 178
NEC, 50, 52, 53, 54, 72
Netlist, 73, 74, 220, 222, 227, 262
Network, 12, 28, 37, 48, 50, 51, 53, 56, 57, 58,
60, 61, 62, 76, 79, 81, 82, 83, 88,
89, 90, 91, 92, 93, 94, 95, 97, 120,
132, 133, 134, 135, 142, 181, 188,
189, 191, 193, 195, 196, 197, 198,
199, 200, 201, 202, 204, 207, 210,
211, 217, 266, 269, 270, 271, 272,
273, 274, 275, 276, 286, 295, 296,
297, 298, 310, 311
ﬂow, 88, 93, 120, 132, 133, 134, 135, 142
Network on Chip (NoC), 56, 181, 188, 189, 194,
199, 200, 204, 210, 212, 266, 272
Nios, 263
Node, 56, 57, 58, 75, 76, 77, 78, 79, 81, 82,
83, 84, 86, 89, 90, 91, 92, 93, 94,
95, 96, 97, 101, 102, 103, 113, 114,
115, 116, 120, 121, 123, 124, 125,
129, 130, 133, 136, 142, 144, 151,
152, 160, 187, 195, 196, 278, 281,
291, 313
Labeling, 89, 97
mapping, 91
representation, 75, 76, 77
NRE, 10
Occupied space, 165, 172, 173, 174, 176, 178
One-time programmable, 30
Operation node, 103
Orientation, 156, 158, 159
Overlapping empty rectangle, 163, 164
Oxygen-Nitrogen-Oxygen, 30
PAC, 50
Packet, 51, 188, 189, 191, 192, 193, 194, 195,
196, 197, 198, 204, 205, 206, 207,
208, 209, 210, 214

Index
357
Packing, 83, 84, 87, 88, 96, 117, 152, 155, 156,
157, 158, 159
Packing Class, 155, 156, 157, 158, 159
PACT, 50, 51, 52, 53, 54, 72
PAE, 50, 51, 52
PAL, 15, 26, 27, 76
Parallelism, 1, 3, 21, 63, 65, 72, 286, 294, 300,
306, 309, 311, 316
Partial reconﬁguration, 12, 26, 68, 115, 149,
152, 179, 186, 201, 213, 214, 215,
217, 218, 219, 221, 223, 224, 225,
226, 227, 229, 231, 233, 235, 237,
238, 239, 240, 241, 243, 244, 245,
246, 247, 248, 249, 251, 253, 255,
257, 258, 260, 283, 284, 295, 301,
304, 307, 312, 315
Partition, 89, 112, 113, 114, 115, 116, 117, 118,
120, 126, 127, 129, 130, 131, 132,
135, 139, 142, 143, 144
Pass transistor, 30, 31, 44
Path, 2, 3, 5, 7, 24, 48, 74, 75, 76, 81, 82, 86, 87,
88, 89, 95, 96, 123, 144, 184, 185,
186, 195, 196, 197, 198, 202, 205,
207, 208, 209, 210, 217, 221, 222,
223, 224, 254, 289, 290, 291, 297
Pattern Matching, 13, 286, 287, 317
Peripheral, 12, 54, 59, 60, 183, 188, 248, 249,
250, 251, 256, 260, 261, 265, 268
PicoArray, 54
PicoChip, 54, 55, 72
PinHat, 278, 279, 281, 284
Pipelining, 3, 4, 6, 62
PLA, 15, 26, 27, 60, 76, 278
Place, 12, 22, 26, 33, 69, 74, 81, 87, 112, 145,
152, 153, 154, 158, 160, 161, 162,
165, 166, 171, 173, 175, 178, 183,
184, 199, 202, 210, 222, 223, 224,
227, 237, 238, 242, 243, 244, 249,
250, 267, 273, 283, 289, 299, 309,
310, 313, 315
PlanAhead, 242, 243
Plane sweep, 175, 176, 177, 179
Platform design, 246, 247, 256
PLICE, 29, 30
Possible placement region, 167, 168
PowerPC, 49, 235, 240, 241, 262, 263, 270,
278, 281
Precedence constraint, 129, 130, 131, 132, 134,
139, 142, 143, 153, 156, 159
Predecessor, 81, 92, 96, 97, 101, 121, 125, 145
Predecessor packing, 96, 97
Primary input, 81, 82, 83, 84, 89, 90, 120, 121,
133, 141
Primary output, 83, 89, 96, 121, 133, 141
ProASIC, 32, 44, 45
Processing array element, 50
Processing element, 49, 50, 53, 54, 58, 60, 64,
72, 183, 184, 188, 189, 191, 193,
194, 202, 210, 295
Program, 2, 3, 4, 6, 7, 16, 22, 24, 27, 60, 61, 62,
64, 71, 99, 100, 102, 104, 105, 106,
107, 161, 183, 235, 237, 241, 242,
252, 294
Programmable Array Logic, 15, 26
Programmable I/O, 47
Programmable Logic Array, 15
Prototyping, 10, 68, 148
PSN, 56
Quicksilver, 55, 56, 72, 188
Rammig, 19, 20
Rapid Prototyping, 10, 13, 148
RDMA, 274, 275
Reachability, 200, 202, 203
Ready set, 123
Receiver, 185, 187, 255, 273, 275, 310, 313
Reconﬁgurable computing, 2, 4, 6, 8, 9, 10, 12,
15, 16, 18, 20, 22, 23, 24, 25, 26,
28, 30, 32, 34, 36, 38, 40, 42, 44,
46, 48, 50, 52, 54, 56, 58, 60, 62,
64, 65, 66, 68, 70, 72, 74, 76, 78,
80, 82, 84, 86, 88, 90, 92, 94, 96,
98, 100, 102, 104, 106, 108, 110,
112, 114, 116, 118, 120, 122, 124,
126, 128, 130, 132, 134, 136, 138,
140, 142, 144, 146, 148, 150, 152,
154, 155, 156, 158, 160, 162, 164,
166, 168, 170, 172, 174, 176, 178,
182, 184, 186, 188, 190, 192, 194,
196, 198, 199, 200, 202, 204, 206,
208, 210, 212, 213, 214, 216, 218,
220, 222, 224, 226, 228, 230, 232,
234, 236, 238, 240, 242, 244, 246,
247, 248, 250, 251, 252, 254, 256,
258, 260, 262, 264, 266, 268, 270,
272, 274, 276, 278, 280, 282, 284,
285, 286, 288, 290, 292, 294, 296,
298, 300, 302, 304, 306, 308, 310,
312, 314, 316
Reconﬁgurable device, 9, 10, 15, 25, 28, 49,
50, 55, 58, 60, 64, 65, 67, 68, 69,
70, 71, 72, 100, 108, 109, 110,
111, 113, 114, 115, 117, 118, 120,
126, 130, 146, 148, 149, 151, 155,
156, 158, 159, 165, 166, 167, 171,
176, 179, 181, 182, 184, 198, 200,
202, 209, 212, 213, 250, 257, 258,
285, 286, 287, 295, 307, 308, 312,
316, 317
Reconﬁgurable hardware, 9, 24, 25, 33, 283,
303, 311
Reconﬁgurable processing unit, 9, 112, 161

358
Index
Reconﬁguration, 9, 10, 11, 12, 18, 19, 22, 24,
25, 26, 31, 52, 53, 55, 62, 63, 64,
68, 69, 70, 71, 111, 112, 115, 117,
118, 127, 132, 144, 145, 146, 149,
153, 179, 183, 186, 213, 214, 215,
217, 218, 221, 225, 226, 227, 229,
234, 241, 243, 244, 245, 246, 247,
248, 250, 252, 254, 256, 257, 270,
283, 284, 285, 286, 287, 295, 297,
304, 307, 309, 310, 311, 312, 313,
314, 315, 317
Reconvergent paths, 86, 87
Register transfer, 75
Relocation,
26, 246, 247, 248, 250, 251,
252, 253
Residual network, 95
Residual value, 95
Resource, 3, 41, 42, 43, 44, 48, 57, 63, 108, 109,
110, 111, 116, 120, 123, 124, 125,
126, 129, 130, 131, 146, 148, 149,
152, 160, 189, 199, 246, 250, 272,
273, 290, 314
constraint, 63, 129, 130, 131
Restgraph, 132
RMB, 185, 186, 188, 212, 254, 256
RMBoC, 186, 187, 188
ROBDD, 78
Route, 12, 43, 45, 47, 74, 81, 146, 176, 184,
188, 194, 196, 198, 203, 217, 222,
223, 224, 289
Router, 189, 190, 191, 192, 193, 194, 195, 196,
197, 198, 199, 200, 201, 202, 203,
204, 205, 206, 207, 208, 209, 210,
211, 223, 272, 273, 282
guiding, 209
Routing cost, 169, 173, 178
Routing network, 37
Routing track, 43
Routing-Conscious Placement, 175
Row-based, 40, 43
Run-time, 9, 11, 12, 15, 21, 22, 54, 56, 58, 67,
68, 69, 70, 96, 98, 102, 114, 116,
117, 127, 138, 150, 151, 153, 155,
160, 163, 170, 172, 181, 183, 184,
185, 186, 199, 201, 202, 212, 214,
217, 218, 235, 246, 247, 249, 250,
253, 260, 270, 283, 284, 294, 295,
298, 299, 300, 304, 307, 308, 309,
311, 312, 313, 315, 316
Schedule, 64, 108, 110, 111, 113, 114, 115, 121,
123, 124, 125, 151
Scheduling, 108, 114, 120, 122, 123, 124, 125,
126, 127, 128, 144, 145
Schematic, 72
Sea of gates, 40
Segment tree, 175
S-element, 43
Self-force, 125
Sender, 185, 186, 187, 188, 255, 273, 274,
275, 310
Sequencer, 21, 53
Sequencing graph, 100, 102, 103
Shannon, 33, 34, 78
Shared memory, 252, 254, 255, 269, 270
Simple programmable logic devices, 27
Simulation, 1, 10, 12, 25, 73, 74, 269, 305
Size, 16, 27, 31, 57, 65, 68, 89, 93, 118, 120,
131, 132, 139, 148, 152, 153, 154,
155, 157, 160, 167, 168, 171, 188,
193, 194, 210, 211, 241, 252, 264,
268, 296, 298, 299, 300, 303, 307
Slice, 216, 221
Sliding Window, 287, 288, 290, 296, 297
SoC, 48, 259, 260, 266
Software Deﬁned Radio, 13, 313, 315
Sonic, 295
SoPC, 259, 260, 265
SPACE, 12, 43, 44, 115, 132, 134, 135, 138,
139, 145, 146, 151, 152, 154, 160,
161, 162, 163, 165, 174, 178, 183,
184, 189, 196, 221, 236, 252, 262,
265, 274, 286, 290, 301, 307, 309
Spartan, 38, 225, 252, 253, 303
Spectral, 120, 134, 135, 136, 138, 139, 140,
141, 142
SPLASH, 23, 24, 25, 288, 289
SPLD, 27
SRAM, 29, 30, 31, 34, 36, 37, 38, 52, 53, 57,
61, 189, 251, 252, 254, 255, 256,
264, 298
Static, 30, 61, 62, 64, 204, 210, 227, 231, 232,
240, 242, 243
Store-and-Forward, 195, 196
Stratix, 39, 40, 45, 46, 47, 48
Stretch, 58, 59
Strip Packing, 155
Substitution, 80
Successor, 81, 101, 125
Sum of products, 27, 77
Superbyte, 288, 289
Supercomputing, 70
Supervisory control, 16
Surrounding Obstacle, 205, 206
Switch Matrix, 40, 41, 42, 48
Switching matrice, 49
Symetrical array, 42
Synthesis, 12, 24, 34, 67, 73, 74, 75, 76, 77, 98,
100, 108, 111, 116, 123, 126, 129,
148, 184, 201, 202, 218, 219, 220,
237, 242, 243, 293
System, 1, 5, 10, 11, 12, 13, 16, 19, 20, 21, 23,
24, 25, 32, 48, 56, 58, 59, 67, 68,
69, 70, 71, 75, 76, 107, 111, 115,
117, 130, 144, 160, 183, 188, 213,

Index
359
231, 235, 236, 237, 238, 240, 242,
243, 244, 249, 250, 251, 253, 256,
259, 260, 261, 263, 264, 265, 266,
267, 268, 269, 270, 271, 272, 273,
275, 276, 277, 278, 279, 280, 281,
282, 283, 284, 294, 295, 296, 297,
298, 306, 307, 308, 311, 312, 313,
314, 315
System on Chip, 48, 188, 259, 260, 265, 267
System on Programmable Chip, 12, 259, 260,
264, 265, 284, 312, 315
SystemC, 73, 214, 244
Technology, 11, 12, 18, 29, 30, 31, 32, 34, 48,
55, 65, 67, 72, 73, 74, 77, 80, 81,
82, 87, 88, 98, 112, 117, 132, 269,
285, 314, 316
mapping, 12, 34, 67, 72, 73, 74, 77, 80,
81, 82, 87, 88, 98, 112, 132
Temporal, 4, 12, 69, 100, 111, 112, 115, 116,
117, 118, 119, 120, 123, 127, 129,
131, 132, 134, 139, 140, 142, 143,
145, 148, 149, 150, 151, 152, 153,
154, 155, 156, 157, 159, 160, 161,
162, 163, 165, 167, 169, 171, 173,
175, 177, 179, 182, 185, 202, 210,
212, 252
Temporal partitioning, 12, 100, 111, 112, 115,
116, 117, 118, 119, 120, 123, 127,
129, 131, 132, 134, 140, 142, 143,
145, 148, 153
Temporal placement, 12, 69, 149, 150, 151, 152,
153, 154, 155, 156, 157, 162, 165,
179, 182, 185, 202, 210, 212
Tensilica, 59
Third party, 181, 182, 254, 293, 310
Time multiplexing, 145
Top-Level Module, 227, 228, 237, 238, 240,
242, 243, 245
Topological, 90, 97, 120, 123
Transceiver, 273, 274, 275
Tree, 50, 82, 83, 84, 85, 87, 89, 160, 175, 278,
290, 292, 293
Tri-state, 40, 41, 42, 47, 221, 267
Two-level, 27, 75, 76, 77, 84, 85, 87, 269
decomposition, 84, 85, 87
UCF, 220, 221, 222, 223, 224, 227, 229, 231,
237, 242, 243
Unconstrained scheduling, 120, 121, 123
Unique assignment constraint, 129, 130, 131
Variable node, 78
Variable part, 16
Verilog, 73, 100, 214, 218
VHDL, 72, 100, 214, 218, 234, 245, 264,
293, 294
ViaLink, 30
Video Streaming, 226, 251, 286, 294, 295, 296,
297
Virtex, 12, 38, 39, 40, 42, 48, 49, 58, 152, 153,
185, 186, 213, 214, 216, 225, 228,
230, 235, 236, 240, 248, 250, 252,
256, 257, 258, 262, 270, 288, 294,
306, 311, 312, 313
Virtual Cut-Through, 196
Volume, 89, 96
Von Neumann, 2, 4, 6, 16, 21, 99, 308
Wasted resource, 115, 116, 117, 153, 155, 161
Weight, 101, 102, 210, 294
Wormhole Routing, 61, 196
Wrapper, 57, 193, 194
Xilinx, 12, 23, 24, 25, 28, 37, 38, 39, 40, 42, 48,
49, 58, 65, 70, 74, 152, 185, 186,
210, 213, 214, 216, 218, 220, 221,
225, 228, 235, 239, 245, 246, 250,
256, 257, 259, 260, 261, 262, 263,
270, 278, 279, 281, 283, 288, 294,
303, 306
XNF, 74
XPP, 50, 51, 52, 72
XPuter, 20, 21, 22
Xtensa, 58


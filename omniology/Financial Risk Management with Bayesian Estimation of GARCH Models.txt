Lecture Notes in Economics
and Mathematical Systems
Founding Editors:
M. Beckmann
H.P. K√ºnzi
Managing Editors:
Prof. Dr. G. Fandel
Fachbereich Wirtschaftswissenschaften
Fernuniversit√§t Hagen
Feithstr. 140/AVZ II, 58084 Hagen, Germany
Prof. Dr. W. Trockel
Institut f√ºr Mathematische Wirtschaftsforschung (IMW)
Universit√§t Bielefeld
Universit√§tsstr. 25, 33615 Bielefeld, Germany
Editorial Board:
A. Basile, A. Drexl, H. Dawid, K. Inderfurth, W. K√ºrsten
612

with Bayesian Estimation
of GARCH Models
Financial Risk Management
David Ardia
Theory and Applications

The use of general descriptive names, registered names, trademarks, etc. in this publication does 
protective laws and regulations and therefore free for general use.
not imply, even in the absence of a specific statement, that such names are exempt from the relevant 
Production: le-tex Jelonek, Schmidt & V√∂ckler GbR, Leipzig
Cover design: WMX Design GmbH, Heidelberg
Printed on acid-free paper
9 8 7 6 5 4 3 2 1
springer.com
Springer-Verlag Berlin Heidelberg
¬© 2008
Lecture Notes in Economics and Mathematical Systems ISSN 0075-8442
University of Fribourg
1700 Fribourg
Switzerland
ISBN 978-3-540-78656-6
e-ISBN 978-3-540-78657-3
DOI 10.1007/978-3-540-78657-3
Library of Congress Control Number: 2008927201
Regime-Switching GARCH Models. Applications to Financial Risk Management‚Äù presented to the
Faculty of Economics and Social Sciences at the University of Fribourg Switzerland by the author. 
Accepted by the Faculty Council on 19 February 2008. The Faculty of Economics and Social Sciences
This book is the Ph.D. dissertation with the original title ‚ÄúBayesian Estimation of Single-Regime and
A E
Department of Quantitative Economics
Dr. David Ardia
at the University of Fribourg Switzerland neither approves nor disapproves the opinions expressed
in a doctoral dissertation. They are to be considered those of the author. (Decision of the Faculty
Council of 23 January 1990).
Typeset with LT X. Copyright ¬© 2008 David Ardia. All rights reserved.
Bd. de P√©rolles 90
david.ardia@unifr.ch

To my nonno, Riziero.

Preface
This book presents in detail methodologies for the Bayesian estimation of single-
regime and regime-switching GARCH models. These models are widespread
and essential tools in Ô¨Ånancial econometrics and have, until recently, mainly
been estimated using the classical Maximum Likelihood technique. As this study
aims to demonstrate, the Bayesian approach oÔ¨Äers an attractive alternative
which enables small sample results, robust estimation, model discrimination
and probabilistic statements on nonlinear functions of the model parameters.
The author is indebted to numerous individuals for help in the preparation
of this study. Primarily, I owe a great debt to Prof. Dr. Philippe J. Deschamps
who inspired me to study Bayesian econometrics, suggested the subject, guided
me under his supervision and encouraged my research. I would also like to thank
Prof. Dr. Martin Wallmeier and my colleagues of the Department of Quantitative
Economics, in particular Michael Beer, Roberto Cerratti and Gilles Kaltenrieder,
for their useful comments and discussions.
I am very indebted to my friends Carlos Ord¬¥as Criado, Julien A. Straubhaar,
J¬¥erÀÜome Ph. A. Taillard and Mathieu Vuilleumier, for their support in the Ô¨Åelds of
economics, mathematics and statistics. Thanks also to my friend Kevin Barnes
who helped with my English in this work.
Finally, I am greatly indebted to my parents and grandparents for their
support and encouragement while I was struggling with the writing of this thesis.
Thanks also to Margaret for her support some years ago. Last but not least,
thanks to you Sophie for your love which puts equilibrium in my life.
Fribourg, April 2008
David Ardia

Table of Contents
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .XIII
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
2
Bayesian Statistics and MCMC Methods . . . . . . . . . . . . . . . . . . . .
9
2.1
Bayesian inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.2
MCMC methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.2.1
The Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2.2
The Metropolis-Hastings algorithm . . . . . . . . . . . . . . . . . . . . 12
2.2.3
Dealing with the MCMC output . . . . . . . . . . . . . . . . . . . . . . 13
3
Bayesian Estimation of the GARCH(1, 1) Model with
Normal Innovations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.1
The model and the priors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.2
Simulating the joint posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.2.1
Generating vector Œ± . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.2.2
Generating parameter Œ≤ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.3
Empirical analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.3.1
Model estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
3.3.2
Sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.3.3
Model diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.4
Illustrative applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.4.1
Persistence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.4.2
Stationarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4
Bayesian Estimation of the Linear Regression Model with
Normal-GJR(1, 1) Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.1
The model and the priors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
4.2
Simulating the joint posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2.1
Generating vector Œ≥ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2.2
Generating the GJR parameters . . . . . . . . . . . . . . . . . . . . . . . 42
Generating vector Œ± . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
Generating parameter Œ≤ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

X
Table of Contents
4.3
Empirical analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.3.1
Model estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.3.2
Sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.3.3
Model diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.4
Illustrative applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
5
Bayesian Estimation of the Linear Regression Model with
Student-t-GJR(1, 1) Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
5.1
The model and the priors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
5.2
Simulating the joint posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.2.1
Generating vector Œ≥ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.2.2
Generating the GJR parameters . . . . . . . . . . . . . . . . . . . . . . . 60
Generating vector Œ± . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Generating parameter Œ≤ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.2.3
Generating vector œñ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.2.4
Generating parameter ŒΩ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.3
Empirical analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
5.3.1
Model estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
5.3.2
Sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.3.3
Model diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.4
Illustrative applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
6
Value at Risk and Decision Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 73
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
6.2
The concept of Value at Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
6.2.1
The one-day ahead VaR under the GARCH(1, 1) dynamics 77
6.2.2
The s-day ahead VaR under the GARCH(1, 1) dynamics . 77
6.3
Decision theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
6.3.1
Bayes point estimate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
6.3.2
The Linex loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
6.3.3
The Monomial loss function . . . . . . . . . . . . . . . . . . . . . . . . . . 90
6.4
Empirical application: the VaR term structure . . . . . . . . . . . . . . . . 91
6.4.1
Data set and estimation design . . . . . . . . . . . . . . . . . . . . . . . . 92
6.4.2
Bayesian estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
6.4.3
The term structure of the VaR density . . . . . . . . . . . . . . . . . 95
6.4.4
VaR point estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
6.4.5
Regulatory capital . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
6.4.6
Forecasting performance analysis . . . . . . . . . . . . . . . . . . . . . . 102
6.5
The Expected Shortfall risk measure . . . . . . . . . . . . . . . . . . . . . . . . . 104
7
Bayesian Estimation of the Markov-Switching GJR(1, 1)
Model with Student-t Innovations . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7.1
The model and the priors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.2
Simulating the joint posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
7.2.1
Generating vector s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
7.2.2
Generating matrix P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
7.2.3
Generating the GJR parameters . . . . . . . . . . . . . . . . . . . . . . . 118

Table of Contents
XI
Generating vector Œ± . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
Generating vector Œ≤ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
7.2.4
Generating vector œñ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7.2.5
Generating parameter ŒΩ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7.3
An application to the Swiss Market Index . . . . . . . . . . . . . . . . . . . . 122
7.4
In-sample performance analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
7.4.1
Model diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
7.4.2
Deviance information criterion . . . . . . . . . . . . . . . . . . . . . . . . 134
7.4.3
Model likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
7.5
Forecasting performance analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
7.6
One-day ahead VaR density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
7.7
Maximum Likelihood estimation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
8
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
A
Recursive Transformations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
A.1 The GARCH(1, 1) model with Normal innovations . . . . . . . . . . . . . 161
A.2 The GJR(1, 1) model with Normal innovations . . . . . . . . . . . . . . . . 162
A.3 The GJR(1, 1) model with Student-t innovations . . . . . . . . . . . . . . 163
B
Equivalent SpeciÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
C
Conditional Moments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
Computational Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
Abbreviations and Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201

Summary
This book presents in detail methodologies for the Bayesian estimation of single-
regime and regime-switching GARCH models. Our sampling schemes have the
advantage of being fully automatic and thus avoid the time-consuming and
diÔ¨Écult task of tuning a sampling algorithm. The study proposes empirical ap-
plications to real data sets and illustrates probabilistic statements on nonlinear
functions of the model parameters made possible under the Bayesian framework.
The Ô¨Årst two chapters introduce the work and give a short overview of the
Bayesian paradigm for inference. The next three chapters describe the estima-
tion of the GARCH model with Normal innovations and the linear regression
models with conditionally Normal and Student-t-GJR errors. For these mod-
els, we compare the Bayesian and Maximum Likelihood approaches based on
real Ô¨Ånancial data. In particular, we document that even for fairly large data
sets, the parameter estimates and conÔ¨Ådence intervals are diÔ¨Äerent between the
methods. Caution is therefore in order when applying asymptotic justiÔ¨Åcations
for this class of models. The sixth chapter presents some Ô¨Ånancial applications of
the Bayesian estimation of GARCH models. We show how agents facing diÔ¨Äerent
risk perspectives can select their optimal VaR point estimate and document that
the diÔ¨Äerences between individuals can be substantial in terms of regulatory cap-
ital. Finally, the last chapter proposes the estimation of the Markov-switching
GJR model. An empirical application documents the in- and out-of-sample su-
periority of the regime-switching speciÔ¨Åcation compared to single-regime GJR
models. We propose a methodology to depict the density of the one-day ahead
VaR and document how speciÔ¨Åc forecasters‚Äô risk perspectives can lead to diÔ¨Äer-
ent conclusions on the forecasting performance of the MS-GJR model.
JEL ClassiÔ¨Åcation: C11, C13, C15, C16, C22, C51, C52, C53.
Keywords and phrases: Bayesian, MCMC, GARCH, GJR, Markov-switching,
Value at Risk, Expected Shortfall, Bayes factor, DIC.

1
Introduction
(...) ‚Äúskedasticity refers to the volatility or wiggle of a
time series. Heteroskedastic means that the wiggle itself
tends to wiggle. Conditional means the wiggle of the
wiggle depends on its own past wiggle. Generalized means
that the wiggle of the wiggle can depend on its own past
wiggle in all kinds of wiggledy ways.‚Äù
‚Äî Kent Osband
Volatility plays a central role in empirical Ô¨Ånance and Ô¨Ånancial risk management
and lies at the heart of any model for pricing derivative securities. Research on
changing volatility (i.e., conditional variance) using time series models has been
active since the creation of the original ARCH (AutoRegressive Conditional
Heteroscedasticity) model in 1982. From there, ARCH models grew rapidly into
a rich family of empirical models for volatility forecasting during the last twenty
years. They are now widespread and essential tools in Ô¨Ånancial econometrics.
In the ARCH(q) speciÔ¨Åcation originally introduced by Engle [1982], the con-
ditional variance at time t, denoted by ht, is postulated to be a linear function
of the squares of past q observations {yt‚àí1, yt‚àí2, . . . , yt‚àíq}. More precisely:
ht .= Œ±0 +
q
X
i=1
Œ±iy2
t‚àíi
(1.1)
where the parameters Œ±0 > 0 and Œ±i ‚©æ0 (i = 1, . . . , q) in order to ensure a pos-
itive conditional variance. In many of the applications with the ARCH model,
a long lag length and therefore a large number of parameters are called for.
To circumvent this problem, Bollerslev [1986] proposed the Generalized ARCH,
or GARCH(p, q), model which extends the speciÔ¨Åcation of the conditional vari-
ance (1.1) as follows:
ht .= Œ±0 +
q
X
i=1
Œ±iy2
t‚àíi +
p
X
j=1
Œ≤jht‚àíj

2
1 Introduction
where Œ±0 > 0, Œ±i ‚©æ0 (i = 1, . . . , q) and Œ≤j ‚©æ0 (j = 1, . . . , p). In this case,
the conditional variance depends on its past values which renders the model
more parsimonious. Indeed, in most empirical applications it turns out that the
simple speciÔ¨Åcation p = q = 1 is able to reproduce the volatility dynamics of
Ô¨Ånancial data. This has led the GARCH(1, 1) model to become the ‚Äúworkhorse
model‚Äù by both academics and practitioners.
Numerous extensions and reÔ¨Ånements of the GARCH model have been pro-
posed to mimic additional stylized facts observed in Ô¨Ånancial markets. These
extensions recognize that there may be important nonlinearity, asymmetry, and
long memory properties in the volatility process. Many of these models are
surveyed in Bollerslev, Chou, and Kroner [1992], Bollerslev, Engle, and Nelson
[1994], Engle [2004]. Among them, we may cite the popular Exponential GARCH
model by Nelson [1991] as well as the GJR model by Glosten, Jaganathan, and
Runkle [1993] which both account for the asymmetric relation between stock re-
turns and changes in variance [see Black 1976]. An additional class of GARCH
models, referred to as regime-switching GARCH, has gained particular attention
in recent years. In these models, the scedastic function‚Äôs parameters can change
over time according to a latent (i.e., unobservable) variable taking values in the
discrete space {1, . . . , K}. The interesting feature of these models lies in the
fact that they provide an explanation of the high persistence in volatility, i.e.,
nearly unit root process for the conditional variance, observed with single-regime
GARCH models [see, e.g., Lamoureux and Lastrapes 1990]. Furthermore, these
models are apt to react quickly to changes in the volatility level which leads
to signiÔ¨Åcant improvements in volatility forecasts as shown by Dueker [1997],
Klaassen [2002], Marcucci [2005]. Further details on regime-switching GARCH
models can be found in Haas, Mittnik, and Paolella [2004], Hamilton and Susmel
[1994].
The Maximum Likelihood (henceforth ML) estimation technique is the gen-
erally favored scheme of inference for GARCH models, although semi- and non-
parametric techniques have also been applied by some authors [see, e.g., Gallant
and Tauchen 1989, Pagan and Schwert 1990]. The primary appeal of the ML
technique stems from the well-known asymptotic optimality conditions of the
resulting estimators under ideal conditions [see Bollerslev et al. 1994, Lee and
Hansen 1994]. In addition, the ML procedure is straightforward to implement
and is nowadays available in econometric packages. However, while conceptually
simple, we may encounter practical diÔ¨Éculties when dealing with the ML esti-
mation of GARCH models. First, the maximization of the likelihood function
must be achieved via a constrained optimization technique. The model param-
eters must indeed be positive to ensure a positive conditional variance and it

1 Introduction
3
is also common to require that the covariance stationarity condition holds (this
condition is Pq
i=1 Œ±i + Pp
j=1 Œ≤j < 1 for the GARCH(p, q) model [see Boller-
slev 1986, Thm.1, p.310]). The optimization procedure subject to inequality
constraints can be cumbersome and does not necessarily converge if the true pa-
rameter values are close to the boundary of the parameter space or if the process
is nearly non-stationary. The maximization is even more diÔ¨Écult to achieve in
the context of regime-switching GARCH models where the likelihood surface is
multimodal. Depending on the numerical algorithm, ML estimates often prove
to be sensitive with respect to starting values. Moreover, the covariance matrix
at the optimum can be extremely tedious to obtain and ad-hoc approaches are
often required to get reliable results (e.g., Hamilton and Susmel [1994] Ô¨Åx some
transition probabilities to zero in order to determine the variance estimates for
some model parameters). Second, as noted by Geweke [1988, p.77], in classical
applications of GARCH models, the interest usually does not center directly on
the model parameters but on possibly complicated nonlinear functions of the
parameters. For instance, in the case of the GARCH(p, q) model, one might be
interested in the unconditional variance, denoted by hy, which is given by:
hy .=
Œ±0
1 ‚àíPq
i=1 Œ±i ‚àíPp
j=1 Œ≤j
provided that the covariance stationarity condition is satisÔ¨Åed. To assess the
uncertainty of this quantity, classical inference involves tedious delta methods,
simulation from the asymptotic Normal approximation of the parameter esti-
mates or the bootstrap methodology. However, none of these techniques is com-
pletely satisfactory. The delta method is an approximation which can be crude
if the function of interest is highly nonlinear. The simulation and the bootstrap
approaches can deal with nonlinear functions of the model parameters and give
a full description of their distribution. Nevertheless, the former technique relies
on asymptotic justiÔ¨Åcations and the latter method is very demanding since at
each step of the procedure, a GARCH model is Ô¨Åtted to the bootstrapped data.
Finally, in the case of regime-switching GARCH models, testing the null of K
versus K‚Ä≤ states is not possible within the classical framework. The regularity
conditions for justifying the œá2 approximation of the likelihood ratio statistic
do not hold as some parameters are undeÔ¨Åned under the null hypothesis [see
Fr¬®uhwirth-Schnatter 2006, Sect.4.4].
Fortunately, diÔ¨Éculties disappear when Bayesian methods are used. First,
any constraints on the model parameters can be incorporated in the model-
ing through appropriate prior speciÔ¨Åcations. Moreover, the recent development
of computational methods based on Markov chain Monte Carlo (henceforth

4
1 Introduction
MCMC) procedures can be used to explore the joint posterior distribution of the
model parameters. These techniques avoid local maxima commonly encountered
via ML estimation of regime-switching GARCH models. Second, exact distribu-
tions of nonlinear functions of the model parameters can be obtained at low cost
by simulating from the joint posterior distribution. In particular, we will show
in Chap. 6 that, upon assuming that the underlying process is of GARCH type,
the well known Value at Risk risk measure (henceforth VaR) can be expressed
as a function of the model parameters. Therefore, the Bayesian approach gives
an adequate framework to estimate the full density of the VaR. In conjunction
with the decision theory framework, this allows to optimally choose a single
point estimate within the density of the VaR, given our risk preferences. Hence,
the Bayesian approach has a clear advantage in combining estimation and de-
cision making. Lastly, in the Bayesian framework, the issue of determining the
number of states can be addressed by means of model likelihood and Bayes fac-
tors. All these reasons strongly motivate the use of the Bayesian approach when
estimating GARCH models.
The choice of the algorithm is the Ô¨Årst issue when dealing with MCMC meth-
ods and it depends on the nature of the problem under study. In the case of
GARCH models, due to the recursive nature of the conditional variance, the
joint posterior and the full conditional densities are of unknown forms, what-
ever distributional assumptions are made on the model disturbances. Therefore,
we cannot use the simple Gibbs sampler and need more elaborate estimation
procedures. The initial approaches have been implemented using importance
sampling [see Geweke 1988, 1989, Kleibergen and van Dijk 1993]. More re-
cent studies include the Griddy-Gibbs sampler [see Aus¬¥ƒ±n and Galeano 2007,
Bauwens and Lubrano 1998] or the Metropolis-Hastings (henceforth M-H) algo-
rithm with some speciÔ¨Åc choice of the proposal densities. The Normal random
walk Metropolis is used in M¬®uller and Pole [1998], Vrontos, Dellaportas, and
Politis [2000], Adaptive Radial-Based Direction Sampling (henceforth ARDS)
is proposed by Bauwens, Bos, van Dijk, and van Oest [2004] while Nakatsuma
[1998, 2000] constructs proposal densities from an auxiliary process. In the con-
text of regime-switching ARCH models, Kaufmann and Fr¬®uhwirth-Schnatter
[2002], Kaufmann and Scheicher [2006] use the method of Nakatsuma [1998,
2000] while Bauwens, Preminger, and Rombouts [2006], Bauwens and Rom-
bouts [2007] rely on the Griddy-Gibbs sampler for regime-switching GARCH
models.
In the importance sampling approach, a suitable importance density is re-
quired for eÔ¨Éciency which can be a bit of an art, especially if the posterior
density is asymmetric or multimodal. In the random walk and independence M-

1 Introduction
5
H strategies, preliminary runs and tuning are necessary. Therefore, the method
cannot be completely automatic which is not a desirable property. The Griddy-
Gibbs sampler of Ritter and Tanner [1992] is used by Bauwens and Lubrano
[1998] in the context of GARCH models to get rid of these diÔ¨Éculties. This
methodology consists in updating each parameter by inversion from the distri-
bution computed by a deterministic integration rule. However, the procedure is
time consuming and this can become a real burden for regime-switching mod-
els which involve many parameters. Moreover, for computational eÔ¨Éciency, we
must limit the range where the probability mass is computed so that the prior
density has to be somewhat informative. In the case of the ARDS algorithm of
Bauwens et al. [2004], the method involves a reparametrization in order to en-
hance the eÔ¨Éciency of the estimation. This technique requires a large number of
evaluations, which signiÔ¨Åcantly slows down the estimation procedure compared
to usual M-H approaches. Lastly, one could also use a Bayesian software such as
BUGS [see Spiegelhalter, Thomas, Best, and Gilks 1995, Spiegelhalter, Thomas,
Best, and Lunn 2007] for estimating GARCH models. However, this becomes
extremely slow as the number of observations increases mainly due to the recur-
sive nature of the conditional variance process. Moreover, the implementation
of speciÔ¨Åc constraints on the model parameters is diÔ¨Écult and extensions to
regime-switching speciÔ¨Åcations are limited.
In the rest of the book, we will use the approach suggested by Nakatsuma
[1998, 2000] which relies on the M-H algorithm where some model parameters
are updated by blocks. The proposal densities are constructed from an auxiliary
ARMA process for the squared observations. This methodology has the advan-
tage of being fully automatic and thus avoids the time-consuming and diÔ¨Écult
task, especially for non-experts, of choosing and tuning a sampling algorithm.
We obtained very high acceptance rates with this M-H algorithm, ranging from
89% to 95% for the single-regime GARCH(1, 1) model, which indicates that the
proposal densities are close to the full posteriors. In addition, the approach of
Nakatsuma [1998, 2000] is easy to extend to regime-switching GARCH models.
In this case, the parameters in each regime can be regrouped and updated by
blocks which may enhance the sampler‚Äôs eÔ¨Éciency.
Organization of the book
A short introduction to Bayesian inference and MCMC methods is given in
Chap. 2. The rest of the book treats in detail the methodologies for the Bayesian
estimation of single-regime and regime-switching GARCH models, proposes em-
pirical applications to real data sets and illustrates some probabilistic state-

6
1 Introduction
ments on nonlinear functions of the model parameters made possible under the
Bayesian framework.
In Chap. 3, we propose the Bayesian estimation of the parsimonious but
eÔ¨Äective GARCH(1, 1) model with Normal innovations. We detail the MCMC
scheme based on the methodology of Nakatsuma [1998, 2000]. An empirical
application to a foreign exchange rate time series is presented where we compare
the Bayesian and the ML estimates. In particular, we show that even for a fairly
large data set, the point estimates and conÔ¨Ådence intervals are diÔ¨Äerent between
the methods. Caution is therefore in order when applying the asymptotic Normal
approximation for the model parameters in this case. We perform a sensitivity
analysis to check the robustness of our results with respect to the choice of
the priors and test the residuals for misspeciÔ¨Åcation. Finally, we compare the
theoretical and sample autocorrelograms of the process and test the covariance
and strict stationarity conditions.
In Chap. 4, we consider the linear regression model with conditionally
heteroscedastic errors and exogenous or lagged dependent variables. We ex-
tend the symmetric GARCH model to account for asymmetric responses to
past shocks in the conditional variance process. To that aim, we consider the
GJR(1, 1) model of Glosten et al. [1993]. We Ô¨Åt the model to the Standard and
Poors 100 (henceforth S&P100) index log-returns and compare the Bayesian and
the ML estimations. We perform a prior sensitivity analysis and test the resid-
uals for misspeciÔ¨Åcation. Finally, we test the covariance stationarity condition
and illustrate the diÔ¨Äerences between the unconditional variance of the process
obtained through the Bayesian approach and the delta method. In particular,
we show that the Bayesian framework leads to a more precise estimate.
In Chap. 5, we extend the linear regression model with conditionally hetero-
scedastic errors by considering Student-t disturbances, which allows to model
extreme shocks in a convenient manner. In the Bayesian approach, the heavy-
tails eÔ¨Äect is created by the introduction of latent variables in the variance
process as proposed by Geweke [1993]. An empirical application based on the
S&P100 index log-returns is proposed with a comparison between the estimated
joint posterior and the asymptotic Normal approximation of the distribution of
the estimates. We perform a prior sensitivity analysis and test the residuals for
misspeciÔ¨Åcation. Finally, we analyze the conditional and unconditional kurtosis
of the underlying time series.
In Chap. 6, we present some Ô¨Ånancial applications of the Bayesian estima-
tion of GARCH models. We introduce the concept of Value at Risk risk measure
and propose a methodology to estimate the density of this quantity for diÔ¨Äer-
ent risk levels and time horizons. This gives us the possibility to determine the

1 Introduction
7
VaR term structure and to characterize the uncertainty coming from the model
parameters. Then, we review some basics in decision theory and use this frame-
work as a rational justiÔ¨Åcation for choosing a point estimate of the VaR. We
show how agents facing diÔ¨Äerent risk perspectives can select their optimal VaR
point estimate and document, in an illustrative application, that the diÔ¨Äerences
between individuals, in particular between fund managers and regulators, can
be substantial in terms of regulatory capital. We show that the common testing
methodology for assessing the performance of the VaR is unable to discrimi-
nate between the point estimates but the deviations are large enough to imply
substantial diÔ¨Äerences in terms of regulatory capital. This therefore gives an
additional Ô¨Çexibility to the user when allocating risk capital. Finally, we extend
our methodology to the Expected Shortfall risk measure.
In Chap. 7, we extend the single-regime GJR model to the regime-switching
GJR model (henceforth MS-GJR); more precisely, we consider an asymmetric
version of the Markov-switching GARCH(1, 1) speciÔ¨Åcation of Haas et al. [2004].
We introduce a novel MCMC scheme which can be viewed as an extension of
the sampler proposed by Nakatsuma [1998, 2000]. Our approach allows to gen-
erate the parameters of the MS-GJR model by blocks which may enhance the
sampler‚Äôs eÔ¨Éciency. As an application, we Ô¨Åt a single-regime and a Markov-
switching GJR model to the Swiss Market Index log-returns. We use the random
permutation sampler of Fr¬®uhwirth-Schnatter [2001b] to Ô¨Ånd suitable identiÔ¨Åca-
tion constraints for the MS-GJR model and show the presence of two distinct
volatility regimes in the time series. The generalized residuals are used to test
the models for misspeciÔ¨Åcation. By using the Deviance information criterion
of Spiegelhalter, Best, Carlin, and van der Linde [2002] and by estimating the
model likelihoods using the bridge sampling technique of Meng and Wong [1996],
we show the in-sample superiority of the MS-GJR model. To test the predictive
performance of the models, we run a forecasting analysis based on the VaR.
In particular, we compare the MS-GJR model to a single-regime GJR model
estimated on rolling windows and show that both models perform equally well.
However, contrary to the single-regime model, the Markov-switching model is
able to anticipate structural breaks in the conditional variance process and needs
to be estimated only once. Then, we propose a methodology to depict the density
of the one-day ahead VaR by simulation and document how speciÔ¨Åc forecasters‚Äô
risk perspectives can lead to diÔ¨Äerent conclusions on forecasting performance
of the model. A comparison with the traditional ML approach concludes the
chapter.
Finally, we summarize the main results of the book and discuss future av-
enues of research in Chap. 8.

2
Bayesian Statistics and MCMC Methods
‚ÄúThe people who don‚Äôt know they are Bayesian are called
non-Bayesian.‚Äù
‚Äî Irving J. Good
This chapter gives a short introduction to the Bayesian paradigm for inference
and an overview of the Markov chain Monte Carlo (henceforth MCMC) algo-
rithms used in the rest of the book. For a more thorough discussion on Bayesian
statistics, the reader is referred to Koop [2003], for instance. Further details on
MCMC methods can be found in Chib and Greenberg [1996], Smith and Roberts
[1993], Tierney [1994]. The reader who is familiar with these topics can skip this
part of the book and go to the Ô¨Årst chapter dedicated to the Bayesian estimation
of GARCH models, on page 17.
The plan of this chapter is as follows. The Bayesian paradigm is introduced
in Sect. 2.1. MCMC techniques are presented in Sect. 2.2 where we introduce
the Gibbs sampler as well as the Metropolis-Hastings algorithm. We also brieÔ¨Çy
discuss some practical implementation issues.
2.1 Bayesian inference
As in the classical approach to inference, the Bayesian estimation assumes a
T √ó 1 vector y .= (y1 ¬∑ ¬∑ ¬∑ yT )‚Ä≤ of observations described through a probability
density p(y | Œ∏). The parameter Œ∏ ‚ààŒò serves as an index of the family of
possible distributions for the observations. It represents the characteristics of
interest one would wish to know in order to obtain a complete description of the
generating process for y. It can be a scalar, a vector, a matrix or even a set of
these mathematical objects. For simplicity, we will consider Œ∏ as a d-dimensional
vector, hence Œ∏ ‚ààŒò ‚äÜRd in what follows.
The diÔ¨Äerence between the Bayesian and the classical approach lies in the
mathematical nature of Œ∏. In the classical framework, it is assumed that there
exists a true and Ô¨Åxed value for parameter Œ∏. Conversely, the Bayesian approach

10
2 Bayesian Statistics and MCMC Methods
considers Œ∏ as a random variable which is characterized by a prior density de-
noted by p(Œ∏). The prior is speciÔ¨Åed with the help of parameters called hyper-
parameters which are initially assumed to be known and constant. Moreover,
depending on the researcher‚Äôs prior information, this density can be more or less
informative. Then, by coupling the likelihood function of the model parameters,
L(Œ∏ | y) ‚â°p(y | Œ∏), with the prior density, we can invert the probability density
using Bayes‚Äô rule to get the posterior density p(Œ∏ | y) as follows:
p(Œ∏ | y) =
L(Œ∏ | y)p(Œ∏)
R
Œò L(Œ∏ | y)p(Œ∏)dŒ∏ .
(2.1)
This posterior is a quantitative, probabilistic description of the knowledge about
the parameter Œ∏ after observing the data.
It is often convenient to choose a prior density which is conjugate to the like-
lihood. That is, a density that leads to a posterior which belongs to the same
distributional family as the prior. In eÔ¨Äect, conjugate priors permit posterior
densities to emerge without numerical integration. However, the easy calcula-
tions of this speciÔ¨Åcation comes with a price due to the restrictions they impose
on the form of the prior. In many cases, it is unlikely that the conjugate prior
is an adequate representation of the prior state of knowledge. In such cases, the
evaluation of (2.1) is analytically intractable, so asymptotic approximations or
Monte Carlo methods are required. Deterministic techniques can provide good
results for low dimensional models. However, when the dimension of the model
becomes large, simulation is the only way to approximate the posterior density.
2.2 MCMC methods
The idea of MCMC sampling was Ô¨Årst introduced by Metropolis, Rosenbluth,
Rosenbluth, Teller, and Teller [1953] and was subsequently generalized by Hast-
ings [1970]. For ease of exposition, we will restrict the presentation to the context
of Bayesian inference. A general and detailed statistical theory of MCMC meth-
ods can be found in Tierney [1994].
The MCMC sampling strategy relies on the construction of a Markov chain
with realizations Œ∏[0], Œ∏[1], . . . , Œ∏[j], . . . in the parameter space Œò. Under appropri-
ate regularity conditions [see Tierney 1994], asymptotic results guarantee that
as j tends to inÔ¨Ånity, then Œ∏[j] tends in distribution to a random variable whose
density is p(Œ∏ | y). Hence, the realized values of the chain can be used to make
inference about the joint posterior. All we require are algorithms for construct-
ing appropriately behaved chains. The best known MCMC algorithms are the

2.2 MCMC methods
11
Gibbs sampler and the Metropolis-Hastings (henceforth M-H) algorithm. These
samplers are nowadays essential tools to perform realistic Bayesian inference.
2.2.1 The Gibbs sampler
The Gibbs sampler is possibly the MCMC sampling technique which is used
most frequently. In the statistical physics literature, it is known as the heat bath
algorithm. Geman and Geman [1984] christened it in the mainstream statisti-
cal literature as the Gibbs sampler. An elementary exposition can be found in
Casella and George [1992]. See also Gelfand and Smith [1990], Tanner and Wong
[1987] for practical examples.
The Gibbs sampler is an algorithm based on successive generations from
the full conditional densities p(Œ∏i | Œ∏Ã∏=i, y), i.e., the posterior density of the ith
element of Œ∏ .= (Œ∏1 ¬∑ ¬∑ ¬∑ Œ∏d)‚Ä≤, given all other elements, where elements of Œ∏ can be
scalars or sub-vectors. In practice the sampler works as follows:
1. Initialize the iteration counter of the chain to j = 1 and
set an initial value Œ∏[0] .= (Œ∏[0]
1 ¬∑ ¬∑ ¬∑ Œ∏[0]
d )‚Ä≤;
2. Generate a new value Œ∏[j] from Œ∏[j‚àí1] through successive
generation values:
Œ∏[j]
1 ‚àºp(Œ∏1 | Œ∏[j‚àí1]
Ã∏=1
, y)
Œ∏[j]
2 ‚àºp(Œ∏2 | Œ∏[j]
1 , Œ∏[j‚àí1]
3
, . . . , Œ∏[j‚àí1]
d
, y)
...
Œ∏[j]
d ‚àºp(Œ∏d | Œ∏[j]
Ã∏=d, y);
3. Change counter j to j + 1 and go back to step 2 until
convergence is reached.
As the number of iterations increases, the chain approaches its stationary dis-
tribution and convergence is then assumed to hold approximately [see Tierney
1994]. SuÔ¨Écient conditions for the convergence of the Gibbs sampler are given
in Roberts and Smith [1994, Sect.4]. As noted in Chib and Greenberg [1996,
p.414], these conditions ensure that each full conditional density is well deÔ¨Åned
and that the support of the joint posterior is not separated into disjoint regions
since this would prevent exploration of the full parameter space. Although these
are only suÔ¨Écient conditions for the convergence of the Gibbs sampler, they are
extremely weak and are satisÔ¨Åed in most applications.
The Gibbs sampler is the most natural choice of MCMC sampling strategy
when it is easy to write down full conditionals from which we can easily generate

12
2 Bayesian Statistics and MCMC Methods
draws. When the expression of p(Œ∏i | Œ∏Ã∏=i, y) is nonstandard, we might consider
rejection methods [see, e.g., Ripley 1987], the Griddy-Gibbs sampler when Œ∏i is
univariate [see Ritter and Tanner 1992], adaptive rejection sampling [see Gilks
and Wild 1992] or M-H sampling as shown in the next section.
2.2.2 The Metropolis-Hastings algorithm
Some complicated Bayesian problems cannot be solved by using the Gibbs sam-
pler. This is the case when it is not easy to break down the joint density into
full conditionals or when the full conditional densities are of unknown form.
The M-H algorithm is a simulation scheme which allows to generate draws from
any density of interest whose normalizing constant is unknown. The algorithm
consists of the following steps.
1. Initialize the iteration counter to j = 1 and set an initial
value Œ∏[0];
2. Move the chain to a new value Œ∏‚ãÜgenerated from a pro-
posal (candidate) density q(‚Ä¢ | Œ∏[j‚àí1]);
3. Evaluate the acceptance probability of the move from
Œ∏[j‚àí1] to Œ∏‚ãÜgiven by:
min

p(Œ∏‚ãÜ| y)
p(Œ∏[j‚àí1] | y)
q(Œ∏[j‚àí1] | Œ∏‚ãÜ)
q(Œ∏‚ãÜ| Œ∏[j‚àí1]), 1

.
If the move is accepted, set Œ∏[j] .= Œ∏‚ãÜ, if not, set Œ∏[j] .= Œ∏[j‚àí1]
so that the chain does not move;
4. Change counter from j to j+1 and go back to step 2 until
convergence is reached.
As in the Gibbs sampler, the chain approaches its equilibrium distribution as
the number of iterations increases [see Tierney 1994]. The power of the M-H
algorithm stems from the fact that the convergence of the chain is obtained
for any proposal q whose support includes the support of the joint posterior
[see Roberts and Smith 1994, Sect.5]. It is however crucial that q approximates
closely the posterior to guarantee an acceptance rate which is reasonable.
With no intention of being exhaustive, some comments are in order here.
If we choose a symmetric proposal density, i.e., q(Œ∏[j] | Œ∏‚ãÜ) = q(Œ∏‚ãÜ| Œ∏[j]), the
acceptance probability of the M-H algorithm reduces to:
min
 p(Œ∏‚ãÜ| y)
p(Œ∏[j] | y), 1


2.2 MCMC methods
13
so that the proposal does not need to be evaluated. This simpler version of
the M-H algorithm is known as the Metropolis algorithm because it is the orig-
inal algorithm by Metropolis et al. [1953]. A special case consists of a pro-
posal density which only depends on the distance between Œ∏‚ãÜand Œ∏[j‚àí1], i.e.,
q(Œ∏‚ãÜ| Œ∏[j‚àí1]) = q(Œ∏‚ãÜ‚àíŒ∏[j‚àí1]). The resulting algorithm is referred to as the
random walk Metropolis algorithm. For instance, q could be a multivariate Nor-
mal density centered at previous draw Œ∏[j‚àí1] and whose covariance matrix is
calibrated to take steps which are reasonably close to Œ∏[j‚àí1] such that the prob-
ability of accepting the candidate is not too low, but with a step size large
enough to ensure a suÔ¨Écient exploration of the parameter space. The drawback
of this method is that it is not fully automatic since the covariance matrix needs
to be chosen carefully; thus preliminary runs are required.
Another special case of the M-H sampler is the independence M-H algorithm,
in which proposal draws are generated independently of the current position of
the chain, i.e., q(Œ∏‚ãÜ| Œ∏[j‚àí1]) = q(Œ∏‚ãÜ). This algorithm is often used with a Normal
or a Student-t proposal density whose moments are estimated from previous runs
of the MCMC sampler. This approach works well for well-behaved unimodal
posterior densities but may be very ineÔ¨Écient if the posterior is asymmetric or
multimodal.
Finally, we note that in the form of the M-H algorithm we have presented,
the vector Œ∏ is updated in a single block at each iteration so that all elements
are changed simultaneously. However, we could also consider componentwise
algorithms where each component is generated by its own proposal density [see
Chib and Greenberg 1995, Tierney 1994]. In fact, the Gibbs belongs to this class
of samplers where each component is updated sequentially, and where proposal
densities are the full conditionals. In this case, new draws are always accepted
[see Chib and Greenberg 1995]. The M-H algorithm is often used in conjunction
with the Gibbs sampler for those components of Œ∏ that have a conditional density
that cannot be sampled from directly, typically because the density is known
only up to a scale factor [see Tierney 1994].
2.2.3 Dealing with the MCMC output
Having examined the building-blocks for the standard MCMC samplers, we now
discuss some issues associated with their practical implementation. In particular,
we comment on the manner we can assess their convergence, the way we can
account for autocorrelation in the chains and how we can obtain characteristics
of the joint posterior from the MCMC output. Further details can be found in
Kass, Carlin, Gelman, and Neal [1998], Smith and Roberts [1993].

14
2 Bayesian Statistics and MCMC Methods
Several statistics have been devised for assessing convergence of MCMC out-
puts. The basic idea behind most of them is to compare moments of the sampled
parameters at diÔ¨Äerent parts of the chain. Alternatively, we can compare several
sequences drawn from diÔ¨Äerent starting points and check that they are indistin-
guishable as the number of iterations increases. We refer the reader to Cowles
and Carlin [1996], Gelman [1995] for a comparative review of these techniques.
In the rest of the book, we will use a methodology based on the analysis of
variance developed by Gelman and Rubin [1992]. More precisely, the approxi-
mate convergence is diagnosed when the variance between diÔ¨Äerent sequences
is no larger than the variance within each individual sequence. Apart from for-
mal diagnostic tests, it is also often convenient to check convergence by plotting
the parameters‚Äô draws over iterations (trace plots) as well as the cumulative or
running mean of the drawings.
Regarding the Monte Carlo (simulation) error, it is crucial to understand
that the draws generated by a MCMC method are not independent. The auto-
correlation either comes from the fact that the new draw depends on the past
value of the chain or that the old element is duplicated. When assessing the pre-
cision of an estimator, we must therefore rely on estimation techniques which
account for this autocorrelation [see, e.g., Geweke 1992, Newey and West 1987].
In the rest of the book, we will estimate the numerical standard errors, that is
the variation of the estimates that can be expected if the simulations were to be
repeated, by the method of Andrews [1991], using a Parzen kernel and AR(1)
pre-whitening as presented in Andrews and Monahan [1992]. As noted by De-
schamps [2006], this ensures easy, optimal, and automatic bandwidth selection.
After the run of a Markov chain and its convergence to the stationary distri-
bution, a sample {Œ∏[j]}J
j=1 from the joint posterior density p(Œ∏ | y) is available.
We can thus approximate the posterior expectation of any function Œæ(Œ∏) of the
model parameters:
EŒ∏|y

Œæ(Œ∏)

=
Z
Œò
Œæ(Œ∏)p(Œ∏ | y)dŒ∏
(2.2)
by averaging over the draws from the posterior distribution in the following
manner:
Œæ .= 1
J
J
X
j=1
Œæ(Œ∏[j]) .
Under mild conditions, the sample average Œæ converges to the posterior expec-
tation by the law of large numbers, even if the draws are generated by a MCMC
sampler [see Tierney 1994]. Some particular cases of (2.2) allow to obtain char-
acteristics of the joint posterior. For instance, when Œæ(Œ∏) .= Œ∏ we obtain the

2.2 MCMC methods
15
posterior mean vector Œ∏; for Œæ(Œ∏) .= (Œ∏ ‚àíŒ∏)(Œ∏ ‚àíŒ∏)‚Ä≤ we obtain the posterior co-
variance matrix; for Œæ(Œ∏) .= I{Œ∏‚ààC}, where I{‚Ä¢} denotes the indicator function
which is equal to one if the constraint holds and zero otherwise, we obtain the
posterior probability of a set C. Finally, if we are interested in the marginal
posterior density of a single component of Œ∏, we can estimate it through a his-
togram or a kernel density estimate of sampled values [see Silverman 1986]. By
contrast, deterministic numerical integration is often intractable.

3
Bayesian Estimation of the GARCH(1, 1) Model
with Normal Innovations
‚ÄúLarge changes tend to be followed by large changes (of
either sign) and small changes tend to be followed by
small changes.‚Äù
‚Äî BenoÀÜƒ±t Mandelbrot
(...) ‚Äúit is remarkable how large a sample is required for
the Normal distribution to be an accurate approximation.‚Äù
‚Äî Robert McCulloch and Peter E. Rossi
In this chapter, we propose the Bayesian estimation of the parsimonious but ef-
fective GARCH(1, 1) model with Normal innovations. We sample the joint poste-
rior distribution of the parameters using the approach suggested by Nakatsuma
[1998, 2000]. As a Ô¨Årst step, we Ô¨Åt the model to foreign exchange log-returns
and compare the Bayesian and the Maximum Likelihood estimates. Next, we
analyze the sensitivity of our results with respect to the choice of the priors
and test the residuals for misspeciÔ¨Åcation. Finally, we illustrate some appealing
aspects of the Bayesian approach through probabilistic statements made on the
parameters.
The plan of this chapter is as follows. We set up the model in Sect. 3.1.
The MCMC scheme is detailed in Sect. 3.2. The empirical results are presented
in Sect. 3.3. We conclude with some illustrative applications of the Bayesian
approach in Sect. 3.4.
3.1 The model and the priors
A GARCH(1, 1) model with Normal innovations may be written as follows:
yt = Œµth1/2
t
for t = 1, . . . , T
Œµt
iid
‚àºN(0, 1)
ht .= Œ±0 + Œ±1y2
t‚àí1 + Œ≤ht‚àí1
(3.1)

18
3 The GARCH(1, 1) Model with Normal Innovations
where Œ±0 > 0, Œ±1 ‚©æ0 and Œ≤ ‚©æ0 to ensure a positive conditional variance
and h0 = y0 .= 0 for convenience; N(0, 1) is the standard Normal density. In
this setting, the conditional variance ht is a linear function of the squared past
observation and the past variance.
In order to write the likelihood function, we deÔ¨Åne the vectors y .= (y1 ¬∑ ¬∑ ¬∑ yT )‚Ä≤
and Œ± .= (Œ±0 Œ±1)‚Ä≤ and we regroup the model parameters into œà .= (Œ±, Œ≤) for
notational purposes. In addition, we deÔ¨Åne the T √ó T diagonal matrix:
Œ£ .= Œ£(œà) = diag
 {ht(œà)}T
t=1

where:
ht(œà) .= Œ±0 + Œ±1y2
t‚àí1 + Œ≤ht‚àí1(œà) .
From there, the likelihood function of œà can be expressed as follows:
L(œà | y) ‚àù(det Œ£)‚àí1/2 exp

‚àí1
2y‚Ä≤Œ£‚àí1y

.
We propose the following proper priors on the parameters Œ± and Œ≤ of the
preceding model:
p(Œ±) ‚àùN2(Œ± | ¬µŒ±, Œ£Œ±)I{Œ±>0}
p(Œ≤) ‚àùN(Œ≤ | ¬µŒ≤, Œ£Œ≤)I{Œ≤>0}
where ¬µ‚Ä¢ and Œ£‚Ä¢ are the hyperparameters, I{‚Ä¢} is the indicator function which
equals unity if the constraint holds and zero otherwise, 0 is a 2 √ó 1 vector of
zeros and Nd is the d-dimensional Normal density (d > 1). In addition, we
assume prior independence between parameters Œ± and Œ≤ which implies that
p(œà) = p(Œ±)p(Œ≤). Then, we construct the joint posterior density via Bayes‚Äô rule:
p(œà | y) ‚àùL(œà | y)p(œà) .
(3.2)
3.2 Simulating the joint posterior
The recursive nature of the variance equation in model (3.1) does not allow
for conjugacy between the likelihood function and the prior density in (3.2).
Therefore, we rely on the M-H algorithm to draw samples from the joint posterior
distribution. The algorithm in this section is a special case of the algorithm
described by Nakatsuma [1998, 2000]. We draw an initial value œà[0] .= (Œ±[0], Œ≤[0])
from the joint prior and we generate iteratively J passes for œà. A single pass is

3.2 Simulating the joint posterior
19
decomposed as follows:
Œ±[j] ‚àºp(Œ± | Œ≤[j‚àí1], y)
Œ≤[j] ‚àºp(Œ≤ | Œ±[j], y) .
Since no full conditional density is known analytically, we sample parameters
Œ± and Œ≤ from two proposal densities. These densities are obtained by noting
that the GARCH(1, 1) model can be written as an ARMA(1, 1) model for {y2
t }.
Indeed, by deÔ¨Åning wt .= y2
t ‚àíht, we can transform the expression of the condi-
tional variance as follows:
ht = Œ±0 + Œ±1y2
t‚àí1 + Œ≤ht‚àí1
‚áîy2
t = Œ±0 + (Œ±1 + Œ≤)y2
t‚àí1 ‚àíŒ≤wt‚àí1 + wt
(3.3)
where wt can be written as:
wt .= y2
t ‚àíht =
y2
t
ht
‚àí1

ht =
 œá2
1 ‚àí1

ht
with œá2
1 denoting a Chi-squared variable with one degree of freedom. Hence, by
construction, {wt} is a Martingale DiÔ¨Äerence process with a conditional mean
of zero and a conditional variance of 2h2
t since a œá2
1 variable has a unit mean
and a variance equal to two.
Following Nakatsuma [1998, 2000], we construct an approximate likelihood
for parameters Œ± and Œ≤ from expression (3.3). The procedure consists in ap-
proximating Ô¨Årst the variable wt by a variable zt which is Normally distributed
with a mean of zero and a variance of 2h2
t. This leads to the following auxiliary
model:
y2
t = Œ±0 + (Œ±1 + Œ≤)y2
t‚àí1 ‚àíŒ≤zt‚àí1 + zt .
Then, by noting that zt is a function of œà given by:
zt(œà) = y2
t ‚àíŒ±0 ‚àí(Œ±1 + Œ≤)y2
t‚àí1 + Œ≤zt‚àí1(œà)
(3.4)
and by deÔ¨Åning the T √ó 1 vector z .= (z1 ¬∑ ¬∑ ¬∑ zT )‚Ä≤ as well as the T √ó T diagonal
matrix:
Œõ .= Œõ(œà) = diag
 {2h2
t(œà)}T
t=1

we can approximate the likelihood function of œà from the auxiliary model as
follows:
L(œà | y) ‚àù(det Œõ)‚àí1/2 exp

‚àí1
2z‚Ä≤Œõ‚àí1z

.
(3.5)

20
3 The GARCH(1, 1) Model with Normal Innovations
As will be shown hereafter, the construction of the proposal densities for pa-
rameters Œ± and Œ≤ is based on this approximate likelihood function.
3.2.1 Generating vector Œ±
Recursive transformations initially proposed by Chib and Greenberg [1994] allow
to express the function zt(œà) in (3.4) as a linear function of the 2 √ó 1 vector Œ±.
Let us deÔ¨Åne vt .= y2
t for notational convenience. The recursive transformations
are deÔ¨Åned as follows:
l‚àó
t
.= 1 + Œ≤ l‚àó
t‚àí1
v‚àó
t
.= vt‚àí1 + Œ≤ v‚àó
t‚àí1
where l‚àó
0 = v‚àó
0
.= 0. As shown in Prop. A.1 (see App. A), upon deÔ¨Åning the
2√ó1 vector ct .= (l‚àó
t v‚àó
t )‚Ä≤, the function zt can be expressed as zt = vt‚àíc‚Ä≤
tŒ±. Then,
by considering the T √ó 1 vector v .= (v1 ¬∑ ¬∑ ¬∑ vT )‚Ä≤ and the T √ó 2 matrix C whose
tth row is c‚Ä≤
t, we get z = v ‚àíCŒ±. Therefore, we can express the approximate
likelihood function of parameter Œ± as follows:
L(Œ± | Œ≤, y) ‚àù(det Œõ)‚àí1/2 exp

‚àí1
2(v ‚àíCŒ±)‚Ä≤Œõ‚àí1(v ‚àíCŒ±)

.
The proposal density to sample vector Œ± is obtained by combining this likelihood
function and the prior density by the usual Bayes update:
qŒ±(Œ± | eŒ±, Œ≤, y) ‚àùN2(Œ± | b¬µŒ±, bŒ£Œ±)I{Œ±>0}
with:
bŒ£‚àí1
Œ±
.= C‚Ä≤eŒõ‚àí1C + Œ£‚àí1
Œ±
b¬µŒ±
.= bŒ£Œ±(C‚Ä≤eŒõ‚àí1v + Œ£‚àí1
Œ± ¬µŒ±)
where the T √óT diagonal matrix eŒõ .= diag
 {2h2
t(eŒ±, Œ≤)}T
t=1

and eŒ± is the previous
draw of Œ± in the M-H sampler. A candidate Œ±‚ãÜis sampled from this proposal
density and accepted with probability:
min
p(Œ±‚ãÜ, Œ≤ | y)
p(eŒ±, Œ≤ | y)
qŒ±(eŒ± | Œ±‚ãÜ, Œ≤, y)
qŒ±(Œ±‚ãÜ| eŒ±, Œ≤, y), 1

.
3.2.2 Generating parameter Œ≤
The function zt(œà) in (3.4) could be expressed, in the previous section, as a
linear function of parameter Œ± but cannot be expressed as a linear function of

3.2 Simulating the joint posterior
21
Œ≤. To overcome this problem, we linearize zt(Œ≤) by a Ô¨Årst order Taylor expansion
at point eŒ≤:
zt(Œ≤) ‚âÉzt(eŒ≤) + dzt
dŒ≤

Œ≤=eŒ≤
√ó (Œ≤ ‚àíeŒ≤)
where eŒ≤ is the previous draw of parameter Œ≤ in the M-H sampler. Furthermore,
let us deÔ¨Åne the following:
rt .= zt(eŒ≤) + eŒ≤‚àát
,
‚àát .= ‚àídzt
dŒ≤

Œ≤=eŒ≤
where the terms ‚àát can be computed by the following recursion:
‚àát .= y2
t‚àí1 ‚àízt‚àí1(eŒ≤) + eŒ≤‚àát‚àí1
with ‚àá0 .= 0. This recursion is simply obtained by diÔ¨Äerentiating (3.4) with
respect to Œ≤. Then, we regroup these terms into the T √ó1 vectors r .= (r1 ¬∑ ¬∑ ¬∑ rT )‚Ä≤
and ‚àá.= (‚àá1 ¬∑ ¬∑ ¬∑ ‚àáT )‚Ä≤ and we approximate the term within the exponential
in (3.5) by z ‚âÉr‚àíŒ≤‚àá. This yields the following approximate likelihood function
for parameter Œ≤:
L(Œ≤ | Œ±, y) ‚àù(det Œõ)‚àí1/2 exp

‚àí1
2(r ‚àíŒ≤‚àá)‚Ä≤Œõ‚àí1(r ‚àíŒ≤‚àá)

.
The proposal density to sample Œ≤ is obtained by combining this likelihood and
the prior density by Bayes‚Äô update:
qŒ≤(Œ≤ | Œ±, eŒ≤, y) ‚àùN(Œ≤ | b¬µŒ≤, bŒ£Œ≤)I{Œ≤>0}
with:
bŒ£‚àí1
Œ≤
.= ‚àá‚Ä≤eŒõ‚àí1‚àá+ Œ£‚àí1
Œ≤
b¬µŒ≤
.= bŒ£Œ≤(‚àá‚Ä≤eŒõ‚àí1r + Œ£‚àí1
Œ≤ ¬µŒ≤)
where the T √ó T diagonal matrix eŒõ .= diag
 {2h2
t(Œ±, eŒ≤)}T
t=1

. A candidate Œ≤‚ãÜis
sampled from this proposal density and accepted with probability:
min
(
p(Œ±, Œ≤‚ãÜ| y)
p(Œ±, eŒ≤ | y)
qŒ≤(eŒ≤ | Œ±, Œ≤‚ãÜ, y)
qŒ≤(Œ≤‚ãÜ| Œ±, eŒ≤, y)
, 1
)
.
We end this section with some comments regarding the implementation of the
MCMC scheme. The program is written in the R language [see R Development
Core Team 2007] with some subroutines implemented in C in order to speed up

22
3 The GARCH(1, 1) Model with Normal Innovations
the simulation procedure. The validity of the algorithm as well as the correctness
of the computer code are veriÔ¨Åed by a variant of the method proposed by Geweke
[2004]. We sample œà from a proper joint prior and generate some passes of the
M-H algorithm. At each pass, we simulate the dependent variable y from the
full conditional p(y | œà) which is given by the conditional likelihood. This way,
we draw a sample from the joint density p(y, œà). If the algorithm is correct, the
resulting replications of œà should reproduce the prior. The Kolmogorov-Smirnov
empirical distribution test does not reject this hypothesis at the 1% signiÔ¨Åcance
level.
3.3 Empirical analysis
We apply our Bayesian estimation method to daily observations of the Deutsch-
mark vs British Pound (henceforth DEM/GBP) foreign exchange log-returns.
The sample period is from January 3, 1985, to December 31, 1991, for a total of
1‚Äô974 observations. The nominal returns are expressed in percent as in Bollerslev
and Ghysels [1996]. This data set has been proposed as an informal benchmark
for GARCH time series software validation and is available from the Journal of
Business and Economic Statistics at ftp://www.amstat.org/. From this time
series, the Ô¨Årst 750 observations, which is about three Ô¨Ånancial years, are used
to illustrate the Bayesian approach. The data set is large enough to perform
classical Maximum Likelihood (henceforth ML) estimation and apply asymp-
totic justiÔ¨Åcations. Hence, we have an interesting point of view from which to
compare classical and Bayesian approaches. The remaining data set will be used
in an empirical analysis proposed in Chap. 6.
The observation window excerpt from our data set is plotted in the upper
part of Fig. 3.1. We test for autocorrelation in the time series by testing the
joint nullity of autoregressive coeÔ¨Écients for {yt}. We estimate the regression
with autoregressive coeÔ¨Écients up to lag 20 and compute the covariance matrix
using the White estimate. The p-values of the Wald test is 0.377 which does
not support the presence of autocorrelation. However, from Fig. 3.1, we clearly
observe clusters of high and low variability in the time series. This phenomenon
is well known in Ô¨Ånancial data and is referred to as volatility clustering. This
eÔ¨Äect is emphasized in the lower part of the Ô¨Ågure where the sample autocorrelo-
gram of squared observations is displayed. In this case, the Ô¨Årst autocorrelations
are large and signiÔ¨Åcant, indicating GARCH eÔ¨Äects; the Wald test strongly re-
jects the null hypothesis of the absence of autocorrelation in the squares. As an
additional data analysis, we test for unit root using the test by Phillips and Per-
ron [1988]. The test strongly rejects the I(1) hypothesis. From this preliminary

3.3 Empirical analysis
23
1
250
500
750
‚àí3
‚àí2
‚àí1
0
1
2
3
time
index
Daily log‚àíreturns
(in percent)
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
Sample autocorrelogram
time
lag
Fig. 3.1. DEM/GBP foreign exchange daily log-returns (upper graph) and sample
autocorrelogram of the squared log-returns (lower graph).
analysis, we conclude that the time series is not integrated and does not exhibit
autocorrelation. However, we strongly suspect the presence of GARCH eÔ¨Äects
in the data.

24
3 The GARCH(1, 1) Model with Normal Innovations
3.3.1 Model estimation
We Ô¨Åt the parsimonious GARCH(1, 1) model to the data for this observation
window. As prior densities for the Bayesian estimation, we choose truncated
Normal densities with zero mean vectors and diagonal covariance matrices. The
variances are set to 10‚Äô000 so we do not introduce tight prior information in our
estimation (see Sect. 3.3.2 for a formal check). Finally, we recall that the joint
prior is constructed by assuming prior independence between Œ± and Œ≤. We run
two chains for 10‚Äô000 passes each. We emphasize the fact that only positivity
constraints are implemented in the M-H algorithm, through the prior densities;
no stationarity conditions are imposed in the simulation procedure. In addition,
we estimate the model by the usual ML technique for comparison purposes.
In Fig. 3.2, the running means are plotted over iterations. For all param-
eters, we notice a convergence of the two chains toward a constant value after
something like 5‚Äô000 iterations. As a formal check, we follow Gelman and Rubin
[1992] where the authors elaborated the idea that the chain trajectories should
be the same after convergence using analysis of variance techniques. Considering
m parallel chains and a real function Œæ .= Œæ(œà) of the model parameters, there
are m trajectories of length J given by {Œæ[j]
i }J
j=1, i = 1, . . . , m. The variances
between chains and within chains, respectively, denoted by B and W, are then
deÔ¨Åned as follows:
B .=
J
m ‚àí1
m
X
i=1
(Œæi ‚àíŒæ)2
W .=
1
m(J ‚àí1)
m
X
i=1
J
X
j=1
(Œæ[j]
i
‚àíŒæi)2
where Œæi is the average of observations of the ith chain and Œæ is the average of
these averages. After convergence, all these mJ values for Œæi are drawn from the
posterior distribution, and œÉ2
Œæ, the variance of Œæ, can be consistently estimated
by W, B as well as the following weighted average:
bœÉ2
Œæ
.=

1 ‚àí1
J

W + 1
J B .
If the chains have not yet converged, then initial values will still be inÔ¨Çuencing
the trajectories and, due to their overdispersion, will force bœÉ2
Œæ to overestimate
œÉ2
Œæ until stationarity is reached. On the other hand, before convergence, W will
tend to underestimate œÉ2
Œæ because each chain will not have adequately traversed
the complete state space. Following this reasoning, Gelman and Rubin [1992]
construct an indicator of convergence; this is the estimator of potential scale

3.3 Empirical analysis
25
reduction factor given by:
bR .=
s
bœÉ2
Œæ
W .
As the simulation converges, the potential scale reduction declines to one, mean-
ing that the m parallel chains are essentially overlapping. Gelman and Rubin
[1992] suggests accepting convergence when the value of bR is below 1.2. Since
this indicator is subject to estimation error, asymptotic conÔ¨Ådence bands can be
constructed and the 97.5th percentile is used as a conservative point estimate.
In our context, we test the convergence of the chains by using the following
functions:
Œæ(œà) = Œ±0
,
Œæ(œà) = Œ±1
and
Œæ(œà) = Œ≤ .
For these three functions, the diagnostic test by Gelman and Rubin [1992] does
not lead to the rejection of the convergence if we consider the second half of the
simulated values; the 97.5th percentile values for bR indeed belong to the interval
[1.04, 1.05]. We can therefore be conÔ¨Ådent that the generated parameters are
drawn from the joint posterior distribution.
Complementary analyses of the MCMC output are also worth mentioning at
this point. In particular, we note that the one-lag autocorrelations in the chains
range from 0.75 for parameter Œ±1 to 0.95 for Œ≤ which is reasonable. Moreover,
the sampling algorithm allows to reach very high acceptance rates ranging from
89% for vector Œ± to 95% for Œ≤, suggesting that the proposal densities are close
to the full conditionals. On the basis of these results, we discard the Ô¨Årst 5‚Äô000
draws from the overall MCMC output as a burn-in period and merge the two
chains to get a Ô¨Ånal sample of length 10‚Äô000.
The posterior statistics as well as the ML results are reported in Table 3.1.
First, we note that even though the number of observations is large, the ML es-
timates and the Bayesian posterior means are diÔ¨Äerent; the ML point estimate
is lower for components of vector Œ± and higher for parameter Œ≤. We also notice
a diÔ¨Äerence between the 95% conÔ¨Ådence intervals. Whereas the conÔ¨Ådence band
is symmetric in the ML case due to the asymptotic Normality assumption, this
is not true for the posterior conÔ¨Ådence intervals. The reason can be explained
through Fig. 3.3 where the marginal posterior densities of the parameters are
displayed. We clearly notice the asymmetric shape of the histograms for parame-
ters Œ±0 and Œ±1; the skewness values are 0.46 and 0.39, both signiÔ¨Åcantly diÔ¨Äerent
from zero at the 1% signiÔ¨Åcance level. Therefore the ML conÔ¨Ådence band has
a tendency to underestimate the right boundary of the 95% conÔ¨Ådence interval
for these parameters. In the case of parameter Œ≤, the skewness is ‚àí0.09, also
signiÔ¨Åcant; in this case, the Maximum Likelihood approach overestimates the

26
3 The GARCH(1, 1) Model with Normal Innovations
Table 3.1. Estimation results for the GARCH(1, 1) model with Normal
innovations.‚ãÜ
œà
œàMLE
œà
œà0.5
œà0.025
œà0.975
min
max
IF
Œ±0
0.039
0.048
0.047
0.022
0.080
0.011
0.119
9.79
[0.014,0.064]
(0.448)
Œ±1
0.198
0.226
0.223
0.128
0.337
0.083
0.499
5.85
[0.102,0.294]
(1.284)
Œ≤
0.686
0.636
0.636
0.476
0.795
0.338
0.849
40.79
[0.538,0.833]
(5.021)
‚ãÜœàMLE: Maximum Likelihood estimate; œà: posterior mean; œàœÜ: estimated pos-
terior quantile at probability œÜ; min: minimum value; max: maximum value; IF:
ineÔ¨Éciency factor (i.e., ratio of the squared numerical standard error and the
variance of the sample mean from a hypothetical iid sampler); [‚Ä¢]: Maximum
Likelihood 95% conÔ¨Ådence interval; (‚Ä¢): numerical standard error (√ó103). The
posterior statistics are based on 10‚Äô000 draws from the joint posterior sample.
left boundary of the 95% conÔ¨Ådence band. Moreover, as shown in the bottom
right-hand side of the Ô¨Ågure, the joint density of parameters Œ±0 and Œ≤ is slightly
diÔ¨Äerent from the ellipsoid obtained with the asymptotic Normal approximation.
Therefore, these results warn us against the abusive use of asymptotic justiÔ¨Å-
cations. In the present case, even 750 observations do not suÔ¨Éce to justify the
asymptotic Normal approximation for the parameters estimates.
The last column of Table 3.1 reports the ineÔ¨Éciency factors (IF) for the
diÔ¨Äerent parameters. Their values are computed as the ratio of the squared nu-
merical standard error of the posterior sample and the variance estimate divided
by the number of iterations (i.e., the variance of the sample mean from a hy-
pothetical iid sequence). The numerical standard errors are estimated by the
method of Andrews [1991], using a Parzen kernel and AR(1) pre-whitening as
presented in Andrews and Monahan [1992]. As noted by Deschamps [2006], this
ensures easy, optimal, and automatic bandwidth selection. In our estimation,
using 10‚Äô000 simulations out of the posterior distribution seems appropriate if
we require that the Monte Carlo error in estimating the mean is smaller than
0.4% of the variation of the error due to the data. The larger ineÔ¨Éciency factor
reported for parameter Œ≤ is reÔ¨Çected in a larger autocorrelation in the simulated
values.

3.3 Empirical analysis
27
Parameter Œ±0
0
2
4
6
8
10
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
iterations
(x 1000)
Parameter Œ±1
0
2
4
6
8
10
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
iterations
(x 1000)
Parameter Œ≤
0
2
4
6
8
10
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
iterations
(x 1000)
Fig. 3.2. Running means of the chains over iterations (up to 10‚Äô000). The acceptance rate ranges from 89% for vector Œ± to 95% for
parameter Œ≤. The autocorrelations range from 0.75 for Œ±1 to 0.95 for Œ≤. The convergence diagnostic test by Gelman and Rubin [1992]
indicates convergence of the chains from iteration 5‚Äô000; the 97.5th percentile of the potential reduction factor ranges from 1.04 to 1.05.

28
3 The GARCH(1, 1) Model with Normal Innovations
Parameter Œ±0
0.02
0.04
0.06
0.08
0.10
0.12
0
500
1000
1500
Parameter Œ±1
0.1
0.2
0.3
0.4
0.5
0
500
1000
1500
Fig. 3.3. Marginal posterior densities of the GARCH(1, 1) parameters; upper graph:
parameter Œ±0; lower graph: parameter Œ±1. The histograms are based on 10‚Äô000 draws
from the joint posterior sample.

3.3 Empirical analysis
29
Parameter Œ≤
0.3
0.4
0.5
0.6
0.7
0.8
0
200
400
600
800
1000
0.02
0.04
0.06
0.08
0.10
0.12
0.4
0.5
0.6
0.7
0.8
Parameter Œ±0
Parameter Œ≤
Fig. 3.3. (cont.) Marginal posterior densities of the GARCH(1, 1) parameters; upper
graph: parameter Œ≤; lower graph: scatter plot of (Œ±0, Œ≤). Both graphs are based on
10‚Äô000 draws from the joint posterior sample.

30
3 The GARCH(1, 1) Model with Normal Innovations
3.3.2 Sensitivity analysis
The Bayesian approach is often criticized on the grounds that the choice of the
prior density may have a non negligible impact on the posterior density and,
consequently, bias the posterior results. It is therefore important to determine
the extent of this impact through a sensitivity analysis. To that aim, we follow
Geweke [1999] who proposes a methodology to estimate the Bayes factors for
the initial model against a model with an alternative prior. While the Bayes
factor is a quantity which is often diÔ¨Écult to estimate, Geweke [1999, Sect.2]
shows that it is possible to approximate the Bayes factor between two models
diÔ¨Äering only by their prior densities using the posterior simulation output from
just one of the models. This approach provides an attractive way of performing
sensitivity analysis since it does not require the estimation of the alternative
model.
More precisely, let us denote by pI(œà) the initial prior density for œà .= (Œ±, Œ≤)
and by pA(œà) the alternative prior used to test the sensitivity of the posterior
density. Based on the T √ó1 vector of observations y .= (y1 ¬∑ ¬∑ ¬∑ yT )‚Ä≤, the Bayes fac-
tor in favor of the alternative model A over the initial model I can be expressed
as follows:
BFA‚âªI = p(y | A)
p(y | I)
where the marginal densities are found by integrating out the parameters:
p(y | ‚Ä¢) =
Z
L(œà | y)p‚Ä¢(œà)dœà .
Developing the Bayes factor using the expression of the marginal densities yields:
BFA‚âªI =
R
L(œà | y)pA(œà)dœà
R
L(œà | y)pI(œà)dœà
=
Z
L(œà | y)pI(œà) pA(œà)
pI(œà) dœà
R
L(œà | y)pI(œà)dœà
=
Z pA(œà)
pI(œà)

L(œà | y)pI(œà)
R
L(œà | y)pI(œà)dœà

dœà
=
Z pA(œà)
pI(œà) p(œà | y, I)dœà
= Eœà|(y,I)
pA(œà)
pI(œà)

where the notation Eœà|(y,I) emphasizes the fact that the posterior expectation
is calculated with respect to the initial prior pI. In this simple context, we
thus notice that the Bayes factor is nothing else than the posterior expectation

3.3 Empirical analysis
31
under the initial prior of the ratio of prior densities. The posterior expectation
can therefore be estimated using the joint posterior sample {œà[j]}J
j=1 as follows:
BFA‚âªI = Eœà|(y,I)
pA(œà)
pI(œà)

‚âà1
J
J
X
j=1
pA(œà[j])
pI(œà[j]) .
(3.6)
We test the sensitivity of our posterior results by considering three alterna-
tive priors which are truncated Normal densities as the initial prior. We choose
however diÔ¨Äerent hyperparameters, in particular larger variances in the covari-
ance matrices. Formally, the alternative priors may be expressed as follows:
p(Œ±) ‚àùN2(Œ± | ¬µ Œπ2, œÉ2I2)I{Œ±>0}
p(Œ≤) ‚àùN(Œ≤ | ¬µ, œÉ2)I{Œ≤>0}
where Œπ2 is a 2 √ó 1 vector of ones, I2 is a 2 √ó 2 identity matrix, ¬µ is the prior
mean and œÉ2 the prior variance; their values are given in the Ô¨Årst two columns
of Table 3.2. The Bayes factors are estimated using approximation (3.6) based
on 10‚Äô000 draws from the joint posterior sample. The discrimination between
models is then based on the JeÔ¨Ärey‚Äôs scale of evidence [see Kass and Raftery
1995, Sect.3.2] which can be summarized as follows:
‚Ä¢ Strong evidence in favor of the initial prior compared to the alter-
native prior:
BFA‚âªI < 0.1
‚Ä¢ Moderate evidence in favor of the initial prior compared to the
alternative prior:
0.1 ‚©ΩBFA‚âªI < 0.3125
‚Ä¢ Weak evidence in favor of the initial prior compared to the alter-
native prior:
0.3125 ‚©ΩBFA‚âªI < 1 .
Estimated BF are reported in the last column of Table 3.2. The numerical
standard errors are not shown since their values are negligible. First, we note
that a change in the prior mean has no impact on the BF. On the contrary,
larger variances in the alternative covariance matrices diminishes the value of
Bayes factors to 0.866; this indicates a weak evidence for the initial speciÔ¨Åca-
tion relative to the alternative priors. Therefore, for each alternative prior, the
estimated BF conÔ¨Årms that our initial choice is vague enough and does not
introduce signiÔ¨Åcant information in our estimation.

32
3 The GARCH(1, 1) Model with Normal Innovations
Table 3.2. Results of the sensitivity
analysis.‚ãÜ
Alternative priors
¬µ
œÉ2
BF
1.00
10‚Äô000
1.000
0.00
11‚Äô000
0.866
1.00
11‚Äô000
0.866
‚ãÜThe alternative priors are truncated
Normal densities; ¬µ: prior mean; œÉ2 prior
variance; BF: Bayes factor.
3.3.3 Model diagnostics
We test the residuals for possible misspeciÔ¨Åcation. The standardized residuals
are deÔ¨Åned by:
bŒµt .=
yt
bh1/2
t
for t = 1, . . . , 750, where bht is the conditional variance computed with œà0.5, the
median of the posterior sample. If the statistical assumptions in (3.1) are sat-
isÔ¨Åed, these residuals should be independent and Normally distributed asymp-
totically.
In the upper part of Fig. 3.4, we display the residuals over time. No au-
tocorrelation or heteroscedasticity are visually apparent. We test for autocor-
relation using the Ljung-Box test up to lag 20 [see Ljung and Box 1978]. The
test does not reject the null hypothesis of absence of autocorrelation at the 5%
signiÔ¨Åcance level (p-value = 0.652). This is also true for the squared residuals
(p-value = 0.961). Therefore, the GARCH(1, 1) process has been able to Ô¨Ålter
the heteroscedastic nature of the data. We form a quantile-quantile plot of the
residuals against the Normal distribution in the lower graph of the Ô¨Ågure. The
distribution is almost Normal at its center whereas the tails are slightly fatter,
especially the left one. The Kolmogorov-Smirnov Normality test rejects the null
hypothesis at the 5% signiÔ¨Åcance level (p-value = 0.008). The tails of the innova-
tions‚Äô distribution are not fat enough to fully capture the distributional nature
of the data. This point will be addressed in Chap. 5 with the introduction of
Student-t disturbances in the modeling.

3.3 Empirical analysis
33
‚àí6
‚àí4
‚àí2
0
2
4
6
1
250
500
750
time
index
Residuals
Normal quantiles
Sample quantiles
‚àí3
‚àí2
‚àí1
0
1
2
3
‚àí4
‚àí3
‚àí2
‚àí1
0
1
2
3
4
Quantile‚àíquantile plot
Fig. 3.4. Residuals time series (upper graph) and Normal quantile-quantile plot (lower
graph).

34
3 The GARCH(1, 1) Model with Normal Innovations
3.4 Illustrative applications
In this section, we illustrate some probabilistic statements made possible under
the Bayesian framework. The joint posterior sample is used to simulate nonlinear
functions of the model parameters.
3.4.1 Persistence
As pointed out in Sect. 3.2, a GARCH(1, 1) process for {yt} is equivalent
to an ARMA(1, 1) process for {y2
t } with an autoregressive coeÔ¨Écient (Œ±1 + Œ≤)
and a moving average coeÔ¨Écient ‚àíŒ≤. Consequently, the autocorrelation function
(henceforth ACF) of the squared observations comes from the standard formulae
for the ARMA(1, 1) model. It is recursively given by:
œÅi .= (Œ±1 + Œ≤) √ó œÅi‚àí1
for i > 1, where the Ô¨Årst order autocorrelation is:
œÅ1 .= Œ±1(1 ‚àíŒ≤2 ‚àíŒ±1Œ≤)
1 ‚àíŒ≤2 ‚àí2Œ±1Œ≤
.
The term (Œ±1 + Œ≤) is the degree of persistence in the autocorrelation of the
squares which controls the intensity of the clustering in the variance process.
With a value close to one, past shocks and past variances will have a longer im-
pact on the future conditional variance. An autoregressive coeÔ¨Écient (Œ±1+Œ≤) = 1
corresponds to a unit root process for squared observations.
To make inference on the persistence and ACF of the squared process, we
simply use the posterior sample and generate (Œ±[j]
1 + Œ≤[j]) as well as œÅ[j]
i
for
j = 1, . . . , 10‚Äô000 and i = 1, . . . , 20. The posterior density of the persistence
(Œ±1 + Œ≤) is plotted in the upper part of Fig. 3.5. The histogram is left-skewed
with a median value of 0.865 and a maximum value of 0.992. In this case, the
integration for the variance process is not supported by the data. In the lower
part of the Ô¨Ågure, we display the posterior ACF with its 95% and 99% conÔ¨Ådence
bands together with the sample autocorrelations of the squared observations.
Although a single observation, at lag 11, lies outside the conÔ¨Ådence bands, the
autocorrelation structure of the estimated GARCH(1, 1) model is in line with
the data.

3.4 Illustrative applications
35
Œ±1 + Œ≤
0.7
0.8
0.9
1.0
0
500
1000
1500
5
10
15
20
0.0
0.2
0.4
0.6
0.8
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1
5
10
15
20
Theoretical and
sample autocorrelograms
time
lag
 
 
 
+
median
95% confidence band
99% confidence band
sample autocorrelation
Fig. 3.5. Posterior density of the persistence (upper graph) and posterior autocor-
relogram (lower graph) of the squared observations. Both graphs are based on 10‚Äô000
draws from the joint posterior sample.

36
3 The GARCH(1, 1) Model with Normal Innovations
3.4.2 Stationarity
In the case of the GARCH(1, 1) model with Normal innovations, Bollerslev [1986,
Thm.1, p.310] and Nelson [1990, Thm.2, p.320] gave the conditions for covari-
ance stationarity (CSC) and strict stationarity (SSC), respectively. These con-
ditions are given by:
CSC .= Œ±1 + Œ≤ ‚àí1 < 0
SSC .= E

ln(Œ±1Œµ2
t + Œ≤)

< 0
where the error term Œµt is Normally distributed. As pointed out in Sect. 3.3,
no stationarity condition has been imposed in the M-H algorithm. The joint
posterior sample can therefore be used to estimate the posterior density of these
functions by generating:
CSC[j] .= Œ±[j]
1 + Œ≤[j] ‚àí1
SSC[j] .= 1
K
K
X
k=1
ln
 Œ±[j]
1 (Œ∑[k])2 + Œ≤[j]
for j = 1, . . . , 10‚Äô000, where Œ∑[k] is a draw from a standard Normal distribution
and K is set large enough (we choose K = 1‚Äô000 in our application). In Fig. 3.6,
we present the Gaussian kernel density estimates of the posterior densities for
CSC and SSC. As we can notice, none of these values exceed zero in our sim-
ulation study. Thus, the estimated model is covariance stationary and strictly
stationary.
We conclude this section by noting that other probabilistic statements on
interesting functions of the model parameters can be obtained using the joint
posterior sample. For instance, the posterior median is 0.341 for the uncon-
ditional variance and 4.54 for the unconditional kurtosis. They approximately
correspond to the sample estimations of 0.323 and 4.63.

3.4 Illustrative applications
37
‚àí0.5
‚àí0.4
‚àí0.3
‚àí0.2
‚àí0.1
0.0
0
2
4
6
8
10
‚àí0.5
‚àí0.4
‚àí0.3
‚àí0.2
‚àí0.1
0.0
0
2
4
6
8
10
Covariance stationarity
Strict stationarity
Fig. 3.6. Posterior densities of the covariance stationarity and strict stationarity con-
ditions. Gaussian kernel density estimates with bandwidth selected by the‚ÄúSilverman‚Äôs
rule of thumb‚Äù criterion [see Silverman 1986, p.48]. Both kernel density estimates are
based on 10‚Äô000 draws from the joint posterior sample.

4
Bayesian Estimation of the Linear Regression
Model with Normal-GJR(1, 1) Errors
‚ÄúOverall, these results show a greater impact on volatility
of negative, rather than positive return shocks.‚Äù
‚Äî Robert F. Engle and Victor K. Ng
In this chapter, we propose the Bayesian estimation of the linear regression
model with conditionally heteroscedastic errors. In the context of time series
regressions, the regression part can include exogenous or lagged dependent vari-
ables. Moreover, we extend the traditional GARCH speciÔ¨Åcation of the errors
to account for asymmetric movements between the conditional variance and the
underlying process. The volatility tends to rise more in response to bad news
than to good news and this phenomenon is especially true on equity markets.
This eÔ¨Äect was Ô¨Årst observed by Black [1976] and is referred to as the lever-
age eÔ¨Äect in the Ô¨Ånancial literature. One explanation of this empirical fact is
that negative returns increase Ô¨Ånancial leverage which extends the company‚Äôs
risk and therefore the variance. To cope with this stylized fact, we use the GJR
model of Glosten et al. [1993]. In this setting, the conditional variance can react
asymmetrically depending on the sign of the past shocks due to the introduction
of dummy variables. The appealing aspect of this model is that it encompasses
the symmetric GARCH. In addition, the MCMC scheme presented in Sect. 3.2
can easily be extended for this asymmetric model in order to Ô¨Ånd proposal den-
sities for the parameters. As a Ô¨Årst illustration, we Ô¨Åt the model to the S&P100
index log-returns and compare the Bayesian and the Maximum Likelihood es-
timates. Next, we perform a prior sensitivity analysis and test the residuals for
misspeciÔ¨Åcation. Finally, we estimate the density of the unconditional variance
of the process.
The plan of this chapter is as follows. We set up the model in Sect. 4.1.
The MCMC scheme is detailed in Sect. 4.2. The empirical results are presented
in Sect. 4.3. We conclude with some illustrations of the Bayesian approach in
Sect. 4.4.

40
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
4.1 The model and the priors
A linear regression model with Normal-GJR(1,1) errors may be written as fol-
lows:
yt = x‚Ä≤
tŒ≥ + ut
for t = 1, . . . , T
ut = Œµth1/2
t
Œµt
iid
‚àºN(0, 1)
ht .= Œ±0 + (Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0})u2
t‚àí1 + Œ≤ht‚àí1
(4.1)
where Œ±0 > 0, Œ±1 ‚©æ0, Œ±2 ‚©æ0 and Œ≤ ‚©æ0 to ensure a positive conditional
variance and h0 = y0 .= 0 for convenience; yt is a scalar dependent variable; xt
is a m√ó1 vector of exogenous or lagged dependent variables; Œ≥ is a m√ó1 vector
of regression coeÔ¨Écients; N(0, 1) is the standard Normal density. In this setting,
the conditional variance ht is a linear function of the squared past shock and
the past variance but contrary to the GARCH model, the conditional variance
can react asymmetrically to past shocks depending on their signs. The leverage
eÔ¨Äect is present if Œ±2 > Œ±1 so that the conditional variance is higher after a
negative shock than a positive shock.
In order to write the likelihood function, we deÔ¨Åne the vectors y .= (y1 ¬∑ ¬∑ ¬∑ yT )‚Ä≤
and Œ± .= (Œ±0 Œ±1 Œ±2)‚Ä≤ as well as the T √ó m matrix X whose tth row is given by
x‚Ä≤
t. We regroup the model parameters into œà .= (Œ≥, Œ±, Œ≤) for notational purposes
and deÔ¨Åne the T √ó T diagonal matrix:
Œ£ .= Œ£(œà) = diag
 {ht(œà)}T
t=1

where:
ht(œà) .= Œ±0 + (Œ±1I{ut‚àí1(Œ≥)‚©æ0} + Œ±2I{ut‚àí1(Œ≥)<0})u2
t‚àí1(Œ≥) + Œ≤ht‚àí1(œà)
ut(Œ≥) .= yt ‚àíx‚Ä≤
tŒ≥ .
Then, we regroup the error terms ut(Œ≥) into the T √ó 1 vector u .= (u1 ¬∑ ¬∑ ¬∑ uT )‚Ä≤
and express the likelihood function of œà as follows:
L(œà | y, X) ‚àù(det Œ£)‚àí1/2 exp

‚àí1
2u‚Ä≤Œ£‚àí1u

.
(4.2)
We propose the following proper priors on the parameters Œ≥, Œ± and Œ≤ of the
preceding model:

4.2 Simulating the joint posterior
41
p(Œ≥) = Nm(Œ≥ | ¬µŒ≥, Œ£Œ≥)
p(Œ±) ‚àùN3(Œ± | ¬µŒ±, Œ£Œ±)I{Œ±>0}
p(Œ≤) ‚àùN(Œ≤ | ¬µŒ≤, Œ£Œ≤)I{Œ≤>0} .
where ¬µ‚Ä¢ and Œ£‚Ä¢ are the hyperparameters, 0 is a 3 √ó 1 vector of zeros, I{‚Ä¢} is
the indicator function and Nd is the d-dimensional Normal density (d > 1). In
addition, we assume prior independence between parameters Œ≥, Œ± and Œ≤ which
yields the following joint prior:
p(œà) = p(Œ≥)p(Œ±)p(Œ≤) .
Then, we construct the joint posterior density via Bayes‚Äô rule:
p(œà | y, X) ‚àùL(œà | y, X)p(œà) .
4.2 Simulating the joint posterior
As in the GARCH model of Chap. 3, the recursive nature of the variance equa-
tion does not allow for conjugacy between the likelihood function and the joint
prior density. Hence, we rely again on the M-H algorithm to draw samples from
the joint posterior distribution. We draw an initial value œà[0] .= (Œ≥[0], Œ±[0], Œ≤[0])
from the joint prior and we generate iteratively J passes for œà. A single pass is
decomposed as follows:
Œ≥[j] ‚àºp(Œ≥ | Œ±[j‚àí1], Œ≤[j‚àí1], y, X)
Œ±[j] ‚àºp(Œ± | Œ≥[j], Œ≤[j‚àí1], y, X)
Œ≤[j] ‚àºp(Œ≤ | Œ≥[j], Œ±[j], y, X) .
Since no full conditional density is known analytically, we sample the parameters
Œ≥, Œ± and Œ≤ from three proposal densities.
4.2.1 Generating vector Œ≥
The proposal density to sample the m √ó 1 vector Œ≥ is obtained by combining
the likelihood function (4.2) and the prior density by the usual Bayes update:
qŒ≥(Œ≥ | eŒ≥, Œ±, Œ≤, y, X) = Nm(Œ≥ | b¬µŒ≥, bŒ£Œ≥)
with:

42
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
bŒ£‚àí1
Œ≥
.= X‚Ä≤eŒ£‚àí1X + Œ£‚àí1
Œ≥
b¬µŒ≥
.= bŒ£Œ≥(X‚Ä≤eŒ£‚àí1y + Œ£‚àí1
Œ≥ ¬µŒ≥)
where the T √ó T diagonal matrix eŒ£ .= diag
 {ht(eŒ≥, Œ±, Œ≤)}T
t=1

and eŒ≥ is the
previous draw of Œ≥ in the M-H sampler. A candidate Œ≥‚ãÜis sampled from this
proposal density and accepted with probability:
min
p(Œ≥‚ãÜ, Œ±, Œ≤ | y, X)
p(eŒ≥, Œ±, Œ≤ | y, X)
qŒ≥(eŒ≥ | Œ≥‚ãÜ, Œ±, Œ≤, y, X)
qŒ≥(Œ≥‚ãÜ| eŒ≥, Œ±, Œ≤, y, X), 1

.
4.2.2 Generating the GJR parameters
The proposal densities to generate the parameters Œ± and Œ≤ are obtained in
the same manner as in Sect. 3.2. However, since we have a regression term
which appears in the model, we estimate the GJR parameters from the errors
ut .= yt ‚àíx‚Ä≤
tŒ≥ instead of yt. An approximate likelihood function for (Œ±, Œ≤) is
then constructed from the process {u2
t}. Note that in the case of a GJR model
for {ut}, we do not end up with an ARMA process for {u2
t} as in the GARCH
model since we have two dummy variables which appear in the expression of the
conditional variance. Indeed, by deÔ¨Åning wt .= u2
t ‚àíht, we can transform the
expression of the conditional variance as follows:
ht = Œ±0 + (Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0})u2
t‚àí1 + Œ≤ht‚àí1
‚áîu2
t = Œ±0 + (Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0} + Œ≤)u2
t‚àí1 ‚àíŒ≤wt‚àí1 + wt
where wt can be written as wt = (œá2
1 ‚àí1)ht, œá2
1 denoting a Chi-squared variable
with one degree of freedom. As in the GARCH case, the sequence {wt} is a
Martingale DiÔ¨Äerence process where the variable wt has a conditional mean of
zero and a conditional variance of 2h2
t.
Following the methodology of Sect. 3.2, we approximate the variable wt by
a variable zt which is Normally distributed with a mean of zero and a variance
of 2h2
t. This leads to the following auxiliary model:
u2
t = Œ±0 + (Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0} + Œ≤)u2
t‚àí1 ‚àíŒ≤zt‚àí1 + zt .
Then, by noting that zt is a function of (Œ±, Œ≤) given by:
zt(Œ±, Œ≤) = u2
t ‚àíŒ±0 ‚àí(Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0} + Œ≤)u2
t‚àí1
+ Œ≤zt‚àí1(Œ±, Œ≤)
(4.3)

4.2 Simulating the joint posterior
43
and by deÔ¨Åning the the T √ó1 vector z .= (z1 ¬∑ ¬∑ ¬∑ zT )‚Ä≤ as well as the T √óT diagonal
matrix:
Œõ .= Œõ(Œ±, Œ≤) = diag
 {2h2
t(Œ±, Œ≤)}T
t=1

we can express the approximate likelihood function of (Œ±, Œ≤) as follows:
L(Œ±, Œ≤ | Œ≥, y, X) ‚àù(det Œõ)‚àí1/2 exp

‚àí1
2z‚Ä≤Œõ‚àí1z

.
As will be shown hereafter, the construction of the proposal densities for pa-
rameters Œ± and Œ≤ is based on this approximate likelihood function.
Generating vector Œ±
We extend the recursive transformations presented in Sect. 3.2.1 to express
the function zt(Œ±, Œ≤) in (4.3) as a linear function of the 3√ó1 vector Œ±. Since the
conditional variance is an asymmetric function of past errors, we have to dis-
tinguish between positive and negative shocks in the recursive transformations.
Let us deÔ¨Åne vt
.= u2
t for notational convenience. The appropriate recursive
transformations are then deÔ¨Åned as follows:
l‚àó
t
.= 1 + Œ≤ l‚àó
t‚àí1
v‚àó
t
.= vt‚àí1I{ut‚àí1‚©æ0} + Œ≤ v‚àó
t‚àí1
v‚àó‚àó
t
.= vt‚àí1I{ut‚àí1<0} + Œ≤ v‚àó‚àó
t‚àí1
where l‚àó
0 = v‚àó
0 = v‚àó‚àó
0
.= 0. As shown in Prop. A.2 (see App. A), upon deÔ¨Åning
the 3√ó1 vector ct .= (l‚àó
t v‚àó
t v‚àó‚àó
t )‚Ä≤, the function zt can be expressed as zt = vt‚àíc‚Ä≤
tŒ±.
Then, by considering the T √ó1 vector v .= (v1 ¬∑ ¬∑ ¬∑ vT )‚Ä≤ as well as the T √ó3 matrix
C whose tth row is c‚Ä≤
t, it turns out that z = v ‚àíCŒ±. Therefore, we can express
the approximate likelihood function of parameter Œ± as follows:
L(Œ± | Œ≥, Œ≤, y, X) ‚àù(det Œõ)‚àí1/2 exp

‚àí1
2(v ‚àíCŒ±)‚Ä≤Œõ‚àí1(v ‚àíCŒ±)

.
The proposal density to sample vector Œ± is obtained by combining this likelihood
function and the prior density by Bayes‚Äô update:
qŒ±(Œ± | Œ≥, eŒ±, Œ≤, y, X) ‚àùN3(Œ± | b¬µŒ±, bŒ£Œ±)I{Œ±>0}
with:
bŒ£‚àí1
Œ±
.= C‚Ä≤eŒõ‚àí1C + Œ£‚àí1
Œ±
b¬µŒ±
.= bŒ£Œ±(C‚Ä≤eŒõ‚àí1v + Œ£‚àí1
Œ± ¬µŒ±)

44
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
where the T √ó T diagonal matrix eŒõ .= diag
 {2h2
t(Œ≥, eŒ±, Œ≤)}T
t=1

and eŒ± is the
previous draw of Œ± in the M-H sampler. A candidate Œ±‚ãÜis sampled from this
proposal density and accepted with probability:
min
p(Œ≥, Œ±‚ãÜ, Œ≤ | y, X)
p(Œ≥, eŒ±, Œ≤ | y, X)
qŒ±(eŒ± | Œ≥, Œ±‚ãÜ, Œ≤, y, X)
qŒ±(Œ±‚ãÜ| Œ≥, eŒ±, Œ≤, y, X), 1

.
Generating parameter Œ≤
The methodology is the same as the one presented in Sect. 3.2.2. The function
zt(Œ≤) given by:
zt(Œ≤) = u2
t ‚àíŒ±0 ‚àí(Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0} + Œ≤)u2
t‚àí1 + Œ≤zt‚àí1(Œ≤)
is approximated by a Ô¨Årst order Taylor expansion at point eŒ≤, the previous draw
of parameter Œ≤ in the M-H sampler. The proposal density qŒ≤(Œ≤ | ‚Ä¢) is then
obtained by combining the approximate likelihood of Œ≤ and the prior density
via Bayes‚Äô update. A candidate Œ≤‚ãÜis generated from this density and accepted
with probability:
min
(
p(Œ≥, Œ±, Œ≤‚ãÜ| y, X)
p(Œ≥, Œ±, eŒ≤ | y, X)
qŒ≤(eŒ≤ | Œ≥, Œ±, Œ≤‚ãÜ, y, X)
qŒ≤(Œ≤‚ãÜ| Œ≥, Œ±, eŒ≤, y, X)
, 1
)
.
Finally, we conclude this section by noting that the validity of the algo-
rithm and the correctness of the computer code are veriÔ¨Åed by the methodology
detailed at the end of Sect. 3.2.2.
4.3 Empirical analysis
We apply our Bayesian estimation method to daily observations of the Standard
& Poors 100 (henceforth S&P100) index log-returns. The sample period is from
January 2, 1990, to December 17, 1992, for a total of 750 observations. The
log-returns are expressed in percent. The S&P100 index is one of the major
benchmarks of U.S. equity performance. It consists of one hundred stocks in a
representative sample of leading companies chosen for market size, liquidity and
industry group. We choose this data set since it is an equity index and is therefore
susceptible to exhibit leverage eÔ¨Äects. Moreover, a volatility index of the S&P100
index, the VIX, is computed by the Chicago Board of Exchange. This index
aims to give a fear gauge to investors and can be viewed as a proxy for the
conditional variance. This volatility index gained particular attention in recent
years as it provides an interesting asset for hedging downside market movements.

4.3 Empirical analysis
45
The two data sets are freely available from http://www.finance.yahoo.com.
Note that in 2003, the formula for the VIX‚Äôs calculation has been modiÔ¨Åed and
the underlying index has changed from the S&P100 index to the S&P500 index.
This is of no consequence here since we consider the old VIX deÔ¨Ånition which
is based on the S&P100 index.
The S&P100 index log-returns are displayed in the upper part of Fig. 4.1. In
the lower part, the VIX index is plotted for the same time period. The correlation
between log-returns and gross rates of squared VIX, which can be viewed as a
proxy for the variance, is -0.52, indeed suggesting the presence of the leverage
eÔ¨Äect.

46
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
Daily log‚àíreturns
(in percent)
1
250
500
750
‚àí2
‚àí1
0
1
2
time
index
VIX level
(in percent)
1
250
500
750
10
20
30
40
time
index
Fig. 4.1. S&P100 index log-returns (upper graph) and VIX level (lower graph).
4.3.1 Model estimation
As noted by Campbell, Lo, and MacKinlay [1996, p.104] for instance, Ô¨Ånancial
time series such as equity indices sometimes present positive Ô¨Årst order auto-
correlation. This eÔ¨Äect is stronger for high-frequency data such as intra-daily

4.3 Empirical analysis
47
or even daily time series due to market micro-structures. Based on that evi-
dence, we estimate the model (4.1) with a constant term and an autoregressive
parameter:
yt = Œ≥0 + Œ≥1yt‚àí1 + ut
(4.4)
where the errors {ut} are modeled by the Normal-GJR(1, 1) process intro-
duced in (4.1). As a prior density for the regression parameters, we choose a
bi-dimensional Normal density. In the case of the GJR parameters, the priors
are truncated Normal densities. Both priors have zero mean vectors and diago-
nal covariance matrices whose variances are set to 10‚Äô000 so we do not introduce
tight prior information in our estimation (see Sect. 4.3.2 for a formal check).
Finally, we recall that the joint prior is constructed by assuming prior indepen-
dence between Œ≥, Œ± and Œ≤.
We run two chains for 10‚Äô000 passes each where only positivity constraints
for the GJR parameters are implemented in the M-H algorithm, through the
prior densities. We test the convergence of the chains using the diagnostic test
by Gelman and Rubin [1992]. The convergence diagnostic based on the two
chains shows no evidence against convergence of the sampler for the last 5‚Äô000
iterations (the values of the 97.5th percentile of the potential scale reduction
factor ranges from 1.01 to 1.09). The one-lag autocorrelations in the chains
range from 0.30 for parameter Œ≥1 to 0.96 for Œ≤. The sampling algorithm allows
to reach acceptance rates of 66% for vector Œ±, 77% for Œ≥ and 95% for Œ≤. From
the overall MCMC output, we discard the Ô¨Årst 5‚Äô000 draws and merge the two
chains to get a Ô¨Ånal sample of length 10‚Äô000. In addition, we estimate the model
by the usual ML technique for comparison purposes.
The posterior statistics as well as the ML results are reported in Table 4.1.
First, we note a diÔ¨Äerence between the Bayesian and ML point estimates for
the GJR parameters; the ML estimates are lower for the components of vector
Œ± and higher for parameter Œ≤. In the case of vector Œ≥, the diÔ¨Äerence between
the Bayesian and the classical approaches is more pronounced for the compo-
nent Œ≥1. For both components of Œ≥, the posterior 95% conÔ¨Ådence bands are
centered around zero which suggests a zero expectation for yt and no need to
model an autoregressive component for this data set. Second, we note strong
asymmetries in the marginal posterior densities for the parameters Œ± and Œ≤,
as shown in Fig. 4.2. The marginal posteriors for components of vector Œ± are
right-skewed while left-skewed for parameter Œ≤. In the ML case, the asymptotic
Normal approximation of the parameter estimates leads to negative values for
the left boundary of Œ±0 and Œ±1. Caution is therefore in order when applying
the asymptotic Normal approximation in this context. Finally, the values of the

48
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
ineÔ¨Éciency factor (IF) reported in the last column of Table 4.1 indicate that
using 10‚Äô000 simulations is appropriate if we require that the Monte Carlo error
in estimating the mean is smaller than 0.33% of the variation of the error due
to the data. The larger ineÔ¨Éciency factor reported for parameter Œ≤ is reÔ¨Çected
in a larger autocorrelation in the simulated values.
Table 4.1. Estimation results for the linear regression model with Normal-
GJR(1, 1) errors.‚ãÜ
œà
œàMLE
œà
œà0.5
œà0.025
œà0.975
min
max
IF
Œ≥0
0.001
0.002
0.002
-0.025
0.028
-0.048
0.047
1.92
[-0.025,0.028]
(0.190)
Œ≥1
0.003
0.004
0.004
-0.075
0.084
-0.160
0.163
1.84
[-0.076,0.082]
(0.546)
Œ±0
0.018
0.023
0.022
0.012
0.041
0.006
0.071
16.52
[-0.008,0.028]
(0.299)
Œ±1
0.022
0.038
0.034
0.002
0.096
0.000
0.193
3.45
[-0.020,0.064]
(0.463)
Œ±2
0.155
0.180
0.175
0.101
0.286
0.058
0.393
3.96
[0.077,0.234]
(0.939)
Œ≤
0.799
0.750
0.754
0.620
0.853
0.423
0.918
33.62
[0.713,0.885]
(3.45)
‚ãÜœàMLE: Maximum Likelihood estimate; œà: posterior mean; œàœÜ: estimated pos-
terior quantile at probability œÜ; min: minimum value; max: maximum value; IF:
ineÔ¨Éciency factor (i.e., ratio of the squared numerical standard error and the
variance of the sample mean from a hypothetical iid sampler); [‚Ä¢]: Maximum
Likelihood 95% conÔ¨Ådence interval; (‚Ä¢): numerical standard error (√ó103). The
posterior statistics are based on 10‚Äô000 draws from the joint posterior sample.
Given the expression of the scedastic function in (4.1), the leverage eÔ¨Äect can
be measured by ‚ñ≥Œ± .= (Œ±2 ‚àíŒ±1). The posterior density of ‚ñ≥Œ± is displayed in
Fig. 4.3. The mean value is 0.142 and the median is 0.139, indicating a stronger
impact of negative shocks to the conditional variance, as expected. Using the
posterior sample, we can also estimate the probability of the presence of the
leverage eÔ¨Äect, i.e., P(‚ñ≥Œ± > 0 | y, X). The estimation gives a probability value
of 0.998 with a 95% conÔ¨Ådence band of [0.9976,0.9996]. Therefore, the data
strongly support the presence of the leverage eÔ¨Äect for the S&P100 index.

4.3 Empirical analysis
49
Parameter Œ±0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0
500
1000
1500
2000
2500
Parameter Œ±1
0.00
0.05
0.10
0.15
0.20
0
500
1000
1500
Fig. 4.2. Marginal posterior densities of the GJR(1, 1) parameters; upper graph: pa-
rameter Œ±0; lower graph: parameter Œ±1. The histograms are based on 10‚Äô000 draws
from the joint posterior sample.

50
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
Parameter Œ±2
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0
500
1000
1500
Parameter Œ≤
0.4
0.5
0.6
0.7
0.8
0.9
0
200
400
600
800
1000
1200
1400
Fig. 4.2. (cont.) Marginal posterior densities of the GJR(1, 1) parameters; upper
graph: parameter Œ±2; lower graph: parameter Œ≤. The histograms are based on 10‚Äô000
draws from the joint posterior sample.

4.3 Empirical analysis
51
‚àí0.1
0.0
0.1
0.2
0.3
0.4
0
500
1000
1500
‚àÜŒ±
Fig. 4.3. Posterior density of the leverage eÔ¨Äect parameter ‚ñ≥Œ± .= (Œ±2 ‚àíŒ±1). The
vertical line is set at ‚ñ≥Œ± = 0. The histogram is based on 10‚Äô000 draws from the joint
posterior sample.

52
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
4.3.2 Sensitivity analysis
As in Sect. 3.3.2, we test the sensitivity of our posterior results with respect
to the choice of the prior density. We consider three alternative priors by either
modifying the mean and/or increasing the variance relative to our initial prior.
Formally, the alternative prior densities can be expressed as follows:
p(Œ≥) ‚àùN2(Œ≥ | ¬µ Œπ2, œÉ2I2)
p(Œ±) ‚àùN3(Œ± | ¬µ Œπ3, œÉ2I3)I{Œ±>0}
p(Œ≤) ‚àùN(Œ≤ | ¬µ, œÉ2)I{Œ≤>0}
where Œπd is a d √ó 1 vector of ones, Id is a d √ó d identity matrix, ¬µ is the prior
mean and œÉ2 the prior variance.
The sensitivity results are reported in Table 4.2; the Ô¨Årst two columns give
the hyperparameters‚Äô values of the alternative priors while the last column report
the estimated Bayes factors. In all cases, the Bayes factors belong to the interval
[0.3125, 1] which implies a weak evidence in favor of our initial speciÔ¨Åcation
relative to the alternative priors. This indicates that our initial prior is vague
enough and does not introduce signiÔ¨Åcant information in our estimation.
Table 4.2. Results of the sensitivity
analysis.‚ãÜ
Alternative priors
¬µ
œÉ2
BF
1.00
10‚Äô000
0.999
0.00
11‚Äô000
0.751
1.00
11‚Äô000
0.751
‚ãÜThe alternative priors are (truncated)
Normal densities; ¬µ prior mean; œÉ2 prior
variance; BF: Bayes factor.
4.3.3 Model diagnostics
We test the residuals for possible misspeciÔ¨Åcation. The standardized residuals
are deÔ¨Åned by:
bŒµt .= yt ‚àíx‚Ä≤
tbŒ≥
bh1/2
t
for t = 1, . . . , 750, where bŒ≥ is the posterior median of vector Œ≥ and bht is the
conditional variance computed with the median of the posterior sample. If the

4.4 Illustrative applications
53
statistical assumptions in (4.1) are satisÔ¨Åed, these residuals should be indepen-
dent and Normally distributed asymptotically.
We test the residuals for autocorrelation using the Ljung-Box test up to lag
20 [see Ljung and Box 1978]. The test does not reject the null hypothesis of
the absence of autocorrelation at the 5% signiÔ¨Åcance level (p-value = 0.365).
This is also true for the squared residuals (p-value = 0.780). The Kolmogorov-
Smirnov Normality test does not reject the null hypothesis at the 5% signiÔ¨Åcance
level with a p-value of 0.0514. On the contrary, the Jarque-Bera Normality test
strongly rejects the null. Hence, while the model is able to Ô¨Ålter the hetero-
scedasticity, it is not Ô¨Çexible enough to account for the high kurtosis of the
residuals. This point will be addressed in Chap. 5 with the introduction of
Student-t errors in the modeling.
4.4 Illustrative applications
We end this chapter with the estimation of the unconditional variance of the
underlying process. Under model speciÔ¨Åcation (4.4), the process is covariance
stationary if the following conditions are satisÔ¨Åed:
CSC1 .= Œ≥2
1 ‚àí1 < 0
CSC2 .= (Œ± + Œ≤) ‚àí1 < 0
where we deÔ¨Åne Œ± .=
Œ±1+Œ±2
2
for notational purposes. If both conditions are
satisÔ¨Åed, the unconditional variance hy exists and is given by:
hy .=
Œ±0
CSC1 √ó CSC2
.
The joint posterior sample can be used to estimate the posterior density of these
functions by generating:
CSC[j]
1
.=
 Œ≥[j]
1
2 ‚àí1
CSC[j]
2
.= (Œ±[j] + Œ≤[j]) ‚àí1
and then:
h[j]
y
.=
Œ±[j]
0
CSC[j]
1 √ó CSC[j]
2
for j = 1, . . . , 10‚Äô000. In our simulation study, none of the values CSC1 and CSC2
exceed zero, thus indicating that the process is covariance stationary and that

54
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
the unconditional variance exists. The posterior density of the unconditional
variance is displayed in Fig. 4.4 together with the ML asymptotic Normal
approximation. The posterior mean and the posterior median are respectively
0.169 and 0.1653. The value of the unconditional variance computed from the
ML point estimates is slightly lower with a value of 0.1608. The 95% conÔ¨Ådence
band given by the Bayesian approach is [0.1373,0.2197]. In the classical ap-
proach, the conÔ¨Ådence band computed via the delta method is [0.0725,0.2491].
In this case, the asymptotic Normal approximation highly overestimates the
size of the conÔ¨Ådence band, especially the left part of the interval. As shown
in Fig. 4.4, the asymptotic approximation is Ô¨Çat and symmetric whereas the
posterior density is more peaked and exhibits a positive skewness (the skewness
is 2.01 and signiÔ¨Åcantly diÔ¨Äerent from zero).
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0
5
10
15
20
hy
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0
5
10
15
20
Delta approximation
Posterior density
Fig. 4.4. Posterior density of the unconditional variance and asymptotic Normal ap-
proximation. The histogram is based on 10‚Äô000 draws from the joint posterior sample.

5
Bayesian Estimation of the Linear Regression
Model with Student-t-GJR(1, 1) Errors
‚ÄúThis
development
(i.e.,
the
Student-t
distribution)
permits
a
distinction
between
conditional
heteroskedasticity
and
a
conditional
leptokurtic
distribution,
either
of
which
could
account
for
the
observed unconditional kurtosis in the data.‚Äù
‚Äî Tim Bollerslev
In this chapter, we extend the linear regression model with conditionally hetero-
scedastic errors. The conditional variance is again described by the GJR process
introduced in Chap. 4. However, in the new speciÔ¨Åcation, the errors are no
longer Normally distributed but follow a Student-t distribution. Therefore, the
model incorporates the possibility of heavy-tailed disturbances. Indeed, while
the Normal distribution is used routinely, it has been widely recognized that
Ô¨Ånancial markets exhibit signiÔ¨Åcant non-Normalities, in particular asset returns
exhibit heavy tails. A distribution with fat tails makes extreme outcomes such
as crashes relatively more likely than does a Normal distribution which assigns
virtually zero probability to events that are greater than three standard devi-
ations. Since one of the objectives of Ô¨Ånancial risk management models is to
measure severe losses, i.e., events appearing in the tails of the distribution, this
is a serious shortcoming and the alternative of the Student-t distribution is a
parsimonious way to incorporate fat tails in the modeling. In the Bayesian ap-
proach, the heavy-tails eÔ¨Äect is created by the introduction of latent variables
in the variance process as proposed by Geweke [1993]; this approach allows the
Bayesian estimation of the degrees of freedom parameter in a convenient man-
ner. As a Ô¨Årst illustration, we Ô¨Åt the model to the S&P100 index log-returns
and compare the Bayesian and the Maximum Likelihood estimations. Next, we
perform a prior analysis and test the residuals for misspeciÔ¨Åcation. Finally, we
estimate the conditional and unconditional kurtosis of the underlying time se-
ries.

56
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
The plan of this chapter is as follows. We set up the model in Sect. 5.1.
The MCMC scheme is detailed in Sect. 5.2. The empirical results are presented
in Sect. 5.3. We conclude with some illustrations of the Bayesian approach in
Sect. 5.4.
5.1 The model and the priors
A linear regression model with Student-t-GJR(1, 1) errors may be written as
follows:
yt = x‚Ä≤
tŒ≥ + ut
for t = 1, . . . , T
ut = Œµt(œ±ht)1/2
Œµt
iid
‚àºS(0, 1, ŒΩ)
œ± .= ŒΩ ‚àí2
ŒΩ
ht .= Œ±0 + (Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0})u2
t‚àí1 + Œ≤ht‚àí1
(5.1)
where Œ±0 > 0, Œ±1 ‚©æ0, Œ±2 ‚©æ0, Œ≤ ‚©æ0, ŒΩ > 2 and h0 = y0 .= 0 for convenience;
yt is a scalar dependent variable; xt is a m √ó 1 vector of exogenous or lagged
dependent variables; Œ≥ is a m √ó 1 vector of regression coeÔ¨Écients; S(0, 1, ŒΩ) is
the standard Student-t density with ŒΩ degrees of freedom, i.e., its variance is
ŒΩ
ŒΩ‚àí2. From model speciÔ¨Åcation (5.1) we note that œ± is a scaling factor which
normalizes the variance of the Student-t density so that ht is the variance of yt
given by the GJR scedastic function. The restriction on the degrees of freedom
parameter ensures the conditional variance to be Ô¨Ånite and the restrictions on
the GJR parameters guarantee its positivity.
In order to write the likelihood function, we deÔ¨Åne the vectors y .= (y1 ¬∑ ¬∑ ¬∑ yT )‚Ä≤
and Œ± .= (Œ±0 Œ±1 Œ±2)‚Ä≤ as well as the T √ó m matrix X of observations whose
tth row is x‚Ä≤
t. For notational purposes, we regroup the model parameters into
œà .= (Œ≥, Œ±, Œ≤, ŒΩ). In addition, we deÔ¨Åne the T √ó T diagonal matrix:
Œ£ .= Œ£(œà) = diag
 {œ±ht(Œ≥, Œ±, Œ≤)}T
t=1

where:
ht(Œ≥, Œ±, Œ≤) .= Œ±0 + (Œ±1I{ut‚àí1(Œ≥)‚©æ0} + Œ±2I{ut‚àí1(Œ≥)<0})u2
t‚àí1(Œ≥)
+ Œ≤ht‚àí1(Œ≥, Œ±, Œ≤)
ut(Œ≥) .= yt ‚àíx‚Ä≤
tŒ≥ .
(5.2)

5.1 The model and the priors
57
Then, we regroup the error terms ut(Œ≥) into the T √ó 1 vector u .= (u1 ¬∑ ¬∑ ¬∑ uT )‚Ä≤
and express the likelihood function of œà as follows:
LS(œà | y, X) ‚àù
"
Œì
  ŒΩ+1
2

Œì
  ŒΩ
2

ŒΩ1/2
#T
(det Œ£)‚àí1/2
T
Y
t=1

1 +
u2
t
ŒΩœ±ht
‚àí(ŒΩ+1)
2
.
(5.3)
The notation LS emphasizes the fact that the likelihood function is constructed
from the Student-t density.
While the likelihood function (5.3) can be used in classical inference, it is
not convenient in the Bayesian framework. It is indeed diÔ¨Écult to Ô¨Ånd proposal
densities for the parameters, especially if we aim to sample the degrees of free-
dom ŒΩ as well. To overcome this problem, we express the innovations process
{ut} in another speciÔ¨Åcation as proposed by Geweke [1993]. In this new setting,
the variable ut is expressed as follows:
ut = Œµt(œñtœ±ht)1/2
for t = 1, . . . , T
Œµt
iid
‚àºN(0, 1)
œñt
iid
‚àºIG
ŒΩ
2, ŒΩ
2

.
(5.4)
Hence, the error term ut, conditional on œñt, follows a Normal distribution with
a mean of zero and a variance of œñtœ±ht; the scedastic function œ±ht is multiplied
by a latent variable œñt which is Inverted Gamma distributed. The degrees of
freedom parameter ŒΩ characterizes the density of œñt as follows:
p(œñt | ŒΩ) =
ŒΩ
2
 ŒΩ
2 h
Œì
ŒΩ
2
i‚àí1
œñ
‚àíŒΩ
2 ‚àí1
t
exp

‚àíŒΩ
2œñt

.
(5.5)
Let us now regroup the latent variables into the T√ó1 vector œñ .= (œñ1 ¬∑ ¬∑ ¬∑ œñT )‚Ä≤
and deÔ¨Åne the augmented set of parameters Œò .= (œà, œñ). Upon deÔ¨Åning the T √óT
diagonal matrix:
Œ£ .= Œ£(Œò) = diag
 {œñtœ±ht(Œ≥, Œ±, Œ≤)}T
t=1

where ht is given in expression (5.2), we can express the likelihood function of
Œò as follows:
L(Œò | y, X) ‚àù(det Œ£)‚àí1/2 exp

‚àí1
2u‚Ä≤Œ£‚àí1u

.
(5.6)
This speciÔ¨Åcation is equivalent to (5.3). However, in that case, the Bayesian
estimation can be handled for all parameters in a convenient manner.

58
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
We propose the following proper priors on the parameters Œ≥, Œ±, Œ≤ of the
preceding model:
p(Œ≥) = Nm(Œ≥ | ¬µŒ≥, Œ£Œ≥)
p(Œ±) ‚àùN3(Œ± | ¬µŒ±, Œ£Œ±)I{Œ±>0}
p(Œ≤) ‚àùN(Œ≤ | ¬µŒ≤, Œ£Œ≤)I{Œ≤>0}
where we recall that ¬µ‚Ä¢ and Œ£‚Ä¢ are the hyperparameters, I{‚Ä¢} is the indicator
function, 0 is a 3√ó1 vector of zeros and Nd is the d-dimensional Normal density
(d > 1).
The prior density of vector œñ conditional on ŒΩ is found by noting that the
components œñt are independent and identically distributed from (5.5), which
yields:
p(œñ | ŒΩ) =
ŒΩ
2
 T ŒΩ
2 h
Œì
ŒΩ
2
i‚àíT
 T
Y
t=1
œñt
!‚àíŒΩ
2 ‚àí1
exp
"
‚àí1
2
T
X
t=1
ŒΩ
œñt
#
.
We follow Deschamps [2006] in the choice of the prior density on the degrees
of freedom parameter. The density is a translated Exponential with parameters
Œª > 0 and Œ¥ ‚©æ2:
p(ŒΩ) = Œª exp

‚àíŒª(ŒΩ ‚àíŒ¥)

I{ŒΩ>Œ¥} .
For large values of Œª, the mass of the prior is concentrated in the neighborhood
of Œ¥ and a constraint on the degrees of freedom can be imposed in this manner.
The Normality of the errors is obtained when Œ¥ becomes large. As pointed out
by Deschamps [2006], this prior density is useful for two reasons. First, it is
potentially important, for numerical reasons, to bound the degrees of freedom
parameter away from two to avoid explosion of the conditional variance. Second,
we can approximate the Normality of the errors while maintaining a reasonably
tight prior which can improve the convergence of the MCMC sampler.
Finally, we assume prior independence between Œ≥, Œ±, Œ≤ and (œñ, ŒΩ) which
yields the following joint prior:
p(Œò) = p(Œ≥)p(Œ±)p(Œ≤)p(œñ | ŒΩ)p(ŒΩ)
and, by combining the likelihood function (5.6) and the joint prior, we construct
the posterior density via Bayes‚Äô rule:
p(Œò | y, X) ‚àùL(Œò | y, X)p(Œò) .

5.2 Simulating the joint posterior
59
5.2 Simulating the joint posterior
Once again, we rely on the M-H algorithm to draw samples from the joint
posterior distribution. We draw an initial value:
Œò[0] .= (Œ≥[0], Œ±[0], Œ≤[0], œñ[0], ŒΩ[0])
from the joint prior and we generate iteratively J passes for Œò. A single pass is
decomposed as follows:
Œ≥[j] ‚àºp(Œ≥ | Œ±[j‚àí1], Œ≤[j‚àí1], œñ[j‚àí1], ŒΩ[j‚àí1], y, X)
Œ±[j] ‚àºp(Œ± | Œ≥[j], Œ≤[j‚àí1], œñ[j‚àí1], ŒΩ[j‚àí1], y, X)
Œ≤[j] ‚àºp(Œ≤ | Œ≥[j], Œ±[j], œñ[j‚àí1], ŒΩ[j‚àí1], y, X)
œñ[j] ‚àºp(œñ | Œ≥[j], Œ±[j], Œ≤[j], ŒΩ[j‚àí1], y, X)
ŒΩ[j] ‚àºp(ŒΩ | œñ[j]) .
Only vector œñ can be simulated from a known expression. Draws of parameters
Œ≥, Œ± and Œ≤ are made using a method similar to the one presented in Sect. 4.2.
Sampling parameter ŒΩ is more technical and relies on an optimized rejection
technique.
5.2.1 Generating vector Œ≥
The proposal density to sample the m √ó 1 vector Œ≥ is obtained by combining
the likelihood function (5.3) and the prior density by Bayes‚Äô update:
qŒ≥(Œ≥ | eŒ≥, Œ±, Œ≤, œñ, ŒΩ, y, X) = Nm(Œ≥ | b¬µŒ≥, bŒ£Œ≥)
with:
bŒ£‚àí1
Œ≥
.= X‚Ä≤eŒ£‚àí1X + Œ£‚àí1
Œ≥
b¬µŒ≥
.= bŒ£Œ≥(X‚Ä≤eŒ£‚àí1y + Œ£‚àí1
Œ≥ ¬µŒ≥)
where the T √ó T diagonal matrix eŒ£ .= diag
 {œñtœ±ht(eŒ≥, Œ±, Œ≤)}T
t=1

, eŒ≥ is the pre-
vious draw of Œ≥ in the M-H sampler and œ± .= ŒΩ‚àí2
ŒΩ . A candidate Œ≥‚ãÜis sampled
from this proposal density and accepted with probability:
min
p(Œ≥‚ãÜ, Œ±, Œ≤, œñ, ŒΩ | y, X)
p(eŒ≥, Œ±, Œ≤, œñ, ŒΩ | y, X)
qŒ≥(eŒ≥ | Œ≥‚ãÜ, Œ±, Œ≤, œñ, ŒΩ, y, X)
qŒ≥(Œ≥‚ãÜ| eŒ≥, Œ±, Œ≤, œñ, ŒΩ, y, X), 1

.

60
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
5.2.2 Generating the GJR parameters
The methodology is similar to the one exposed in Sect. 4.2.2. Let us deÔ¨Åne:
wt .= u2
t
œÑt
‚àíht
where œÑt .= œñtœ± for convenience. From there, we can transform the expression
of the conditional variance as follows:
ht = Œ±0 + (Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0})u2
t‚àí1 + Œ≤ht‚àí1
‚áîu2
t
œÑt
= Œ±0 + (Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0})u2
t‚àí1
+ Œ≤ u2
t‚àí1
œÑt‚àí1
‚àíŒ≤wt‚àí1 + wt
‚áîvt = Œ±0 + (Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0})œÑt‚àí1vt‚àí1
+ Œ≤vt‚àí1 ‚àíŒ≤wt‚àí1 + wt
where we deÔ¨Åne vt .= u2
t
œÑt for notational purposes. Moreover, we note that the
variable wt can also be expressed as follows:
wt .= u2
t
œÑt
‚àíht =
 u2
t
œÑtht
‚àí1

ht =
 œá2
1 ‚àí1

ht
where œá2
1 denotes a Chi-squared variable with one degree of freedom. The last
equality results from the expression for ut in (5.4). Therefore, the variable wt
has a conditional mean of zero and a conditional variance of 2h2
t. In addition,
we note that the sequence {wt} is again a Martingale DiÔ¨Äerence process.
Approximating the variable wt by a variable zt which is Normally distributed
with a mean of zero and a variance of 2h2
t yields the following auxiliary model
for vt:
vt = Œ±0 + (Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0})œÑt‚àí1vt‚àí1 + Œ≤vt‚àí1
‚àíŒ≤zt‚àí1 + zt .
Then, by noting that zt is a function of (Œ±, Œ≤) given by:
zt(Œ±, Œ≤) = vt ‚àíŒ±0 ‚àí

(Œ±1I{ut‚àí1‚©æ0} + Œ±2I{ut‚àí1<0})œÑt‚àí1 + Œ≤

vt‚àí1
+ Œ≤zt‚àí1(Œ±, Œ≤)
(5.7)
and by deÔ¨Åning the T √ó 1 vector z .= (z1 ¬∑ ¬∑ ¬∑ zT )‚Ä≤ as well as the T √ó T diagonal
matrix:

5.2 Simulating the joint posterior
61
Œõ .= Œõ(Œ±, Œ≤) = diag
 {2h2
t(Œ±, Œ≤)}T
t=1

we can approximate the likelihood function of (Œ±, Œ≤) as follows:
L(Œ±, Œ≤ | Œ≥, œñ, y, X) ‚àù(det Œõ)‚àí1/2 exp

‚àí1
2z‚Ä≤Œõ‚àí1z

.
(5.8)
As will be shown hereafter, the construction of the proposal densities for pa-
rameters Œ± and Œ≤ is based on this approximate likelihood function.
Generating vector Œ±
Our aim is to express the function zt(Œ±, Œ≤) in (5.7) as a linear function of the
3√ó1 vector Œ±. To that aim, let us deÔ¨Åne the following recursive transformations:
l‚àó
t
.= 1 + Œ≤ l‚àó
t‚àí1
v‚àó
t
.= u2
t‚àí1I{ut‚àí1‚©æ0} + Œ≤ v‚àó
t‚àí1
v‚àó‚àó
t
.= u2
t‚àí1I{ut‚àí1<0} + Œ≤ v‚àó‚àó
t‚àí1
where l‚àó
0 = v‚àó
0 = v‚àó‚àó
0
.= 0. As shown in Prop. A.3 (see App. A), upon deÔ¨Åning
the 3 √ó 1 vector ct .= (l‚àó
t v‚àó
t v‚àó‚àó
t )‚Ä≤, it turns out that the function zt can be
expressed as zt = vt ‚àíc‚Ä≤
tŒ±. Hence, by deÔ¨Åning the T √ó 1 vectors z .= (z1 ¬∑ ¬∑ ¬∑ zT )‚Ä≤
and v .= (v1 ¬∑ ¬∑ ¬∑ vT )‚Ä≤ as well as the T √ó 3 matrix C whose tth row is c‚Ä≤
t, we get
z = v ‚àíCŒ±. Therefore, we can express the approximate likelihood function of
parameter Œ± as follows:
L(Œ± | Œ≥, Œ≤, œñ, ŒΩ, y, X) ‚àù(det Œõ)‚àí1/2 exp

‚àí1
2(v ‚àíCŒ±)‚Ä≤Œõ‚àí1(v ‚àíCŒ±)

.
The proposal density to sample vector Œ± is obtained by combining this likelihood
function and the prior density by Bayes‚Äô update:
qŒ±(Œ± | eŒ≥, Œ±, Œ≤, œñ, ŒΩ, y, X) ‚àùN3(Œ± | b¬µŒ±, bŒ£Œ±)I{Œ±>0}
with:
bŒ£‚àí1
Œ±
.= C‚Ä≤eŒõ‚àí1C + Œ£‚àí1
Œ±
b¬µŒ±
.= bŒ£Œ±(C‚Ä≤eŒõ‚àí1v + Œ£‚àí1
Œ± ¬µŒ±)
where the T √ó T diagonal matrix eŒõ .= diag
 {2h2
t(Œ≥, eŒ±, Œ≤)}T
t=1

and eŒ± is the
previous draw of Œ± in the M-H sampler. A candidate Œ±‚ãÜis sampled from this
proposal density and accepted with probability:

62
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
min
p(Œ±‚ãÜ, Œ≥, Œ≤, œñ, ŒΩ | y, X)
p(eŒ±, Œ≥, Œ≤, œñ, ŒΩ | y, X)
qŒ±(eŒ± | Œ≥, Œ±‚ãÜ, Œ≤, œñ, ŒΩ, y, X)
qŒ±(Œ±‚ãÜ| Œ≥, eŒ±, Œ≤, œñ, ŒΩ, y, X), 1

.
Generating parameter Œ≤
Contrary to the parameter Œ±, we cannot express the function zt(Œ±, Œ≤) in (5.7)
as a linear function of Œ≤. To bypass this problem, we approximate the function
zt(Œ≤) by a Ô¨Årst order Taylor expansion at point eŒ≤:
zt(Œ≤) ‚âÉzt(eŒ≤) + dzt
dŒ≤

Œ≤=eŒ≤
√ó (Œ≤ ‚àíeŒ≤)
where eŒ≤ is the previous draw of Œ≤ in the M-H sampler. From there, we deÔ¨Åne
the following:
rt .= zt(eŒ≤) + eŒ≤‚àát
,
‚àát .= ‚àídzt
dŒ≤

Œ≤=eŒ≤
where the terms ‚àát can be computed by the following recursion:
‚àát .= v2
t‚àí1 ‚àízt‚àí1(eŒ≤) + eŒ≤‚àát‚àí1
with ‚àá0 .= 0. This recursion is simply obtained by diÔ¨Äerentiating (5.7) with
respect to Œ≤. Then, we regroup these terms into the T √ó1 vectors r .= (r1 ¬∑ ¬∑ ¬∑ rT )‚Ä≤
and ‚àá.= (‚àá1 ¬∑ ¬∑ ¬∑ ‚àáT )‚Ä≤ and we approximate the term within the exponential
in (5.8) by z ‚âÉr‚àíŒ≤‚àá. This yields the following approximate likelihood function
for parameter Œ≤:
L(Œ≤ | Œ≥, Œ±, œñ, ŒΩ, y, X) ‚àù(det Œõ)‚àí1/2 exp

‚àí1
2(r ‚àíŒ≤‚àá)‚Ä≤Œõ‚àí1(r ‚àíŒ≤‚àá)

.
This likelihood function is combined with the prior density by Bayes‚Äô update to
construct the proposal qŒ≤(Œ≤ | ‚Ä¢). A candidate Œ≤‚ãÜis sampled from this proposal
density and accepted with probability:
min
(
p(Œ≥, Œ±, Œ≤‚ãÜ, œñ, ŒΩ | y, X)
p(Œ≥, Œ±, eŒ≤, œñ, ŒΩ | y, X)
qŒ≤(eŒ≤ | Œ≥, Œ±, Œ≤‚ãÜ, œñ, ŒΩ, y, X)
qŒ≤(Œ≤‚ãÜ| Œ≥, Œ±, eŒ≤, œñ, ŒΩ, y, X)
, 1
)
.
5.2.3 Generating vector œñ
The components of œñ are independent a posteriori and the full conditional
posterior of œñt is obtained as follows:

5.2 Simulating the joint posterior
63
p(œñt | Œ≥, Œ±, Œ≤, ŒΩ, y, X) ‚àùL(Œò | y, X)p(œñt | ŒΩ)
‚àùœñ
‚àí(ŒΩ+3)
2
t
exp

‚àíbt
œñt

(5.9)
with:
bt .= 1
2
(yt ‚àíx‚Ä≤
tŒ≥)2
œ±ht
+ ŒΩ

where we recall that ht .= ht(Œ≥, Œ±, Œ≤) and œ± .= ŒΩ‚àí2
ŒΩ . Expression (5.9) is the kernel
of an Inverted Gamma density with parameters ŒΩ+1
2
and bt.
5.2.4 Generating parameter ŒΩ
Draws from p(ŒΩ | œñ) are made by optimized rejection sampling from a translated
Exponential source density. The target density is:
p(ŒΩ | œñ) ‚àùp(œñ | ŒΩ)p(ŒΩ)
‚àù
ŒΩ
2
 T ŒΩ
2 h
Œì
ŒΩ
2
i‚àíT
exp [‚àíœïŒΩ] I{ŒΩ>Œ¥}
with:
œï .= 1
2
T
X
t=1
 ln œñt + œñ‚àí1
t

+ Œª .
Following Deschamps [2006], we sample a candidate ŒΩ‚ãÜfrom a translated Expo-
nential source density:
g(ŒΩ; ¬Ø¬µ, Œ¥) .= ¬Ø¬µ exp

‚àí¬Ø¬µ(ŒΩ ‚àíŒ¥)

I{ŒΩ>Œ¥}
where ¬Ø¬µ maximizes the acceptance probability. The choice of ¬Ø¬µ is found by
solving:
T
2

ln
1 + ¬µŒ¥
2¬µ

+ 1 ‚àíŒ®
1 + ¬µŒ¥
2¬µ

+ ¬µ ‚àíœï = 0
for ¬µ, where Œ®(z) .=
d ln Œì(z)
dz
is the Digamma function. The candidate ŒΩ‚ãÜis
accepted with probability:
p‚ãÜ.=
k(ŒΩ‚ãÜ)
s(¬Ø¬µ, Œ¥)g(ŒΩ‚ãÜ; ¬Ø¬µ, Œ¥)
(5.10)
where k(ŒΩ) is the kernel of the target density:
k(ŒΩ) .=
ŒΩ
2
 T ŒΩ
2 h
Œì
ŒΩ
2
i‚àíT
exp [‚àíœïŒΩ]

64
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
and s(¬µ, Œ¥) is given by:
s(¬µ, Œ¥) .= k
1 + ¬µŒ¥
¬µ
 
g
1 + ¬µŒ¥
¬µ
; ¬µ, Œ¥
‚àí1
=
1 + ¬µŒ¥
2¬µ
 T (1+¬µŒ¥)
2¬µ

Œì
1 + ¬µŒ¥
2¬µ
‚àíT
¬µ‚àí1 exp

1 ‚àíœï(1 + ¬µŒ¥)
¬µ

.
Substituting for k(ŒΩ‚ãÜ), s(¬Ø¬µ, Œ¥) and g(ŒΩ‚ãÜ; ¬Ø¬µ, Œ¥) in expression (5.10) yields:
p‚ãÜ=
Ô£Æ
Ô£∞
Œì

1+¬Ø¬µŒ¥
2¬Ø¬µ

Œì
  ŒΩ‚ãÜ
2

Ô£π
Ô£ª
T ŒΩ‚ãÜ
2
 T ŒΩ‚ãÜ
2
1 + ¬Ø¬µŒ¥
2¬Ø¬µ
 ‚àíT (1+¬Ø
¬µŒ¥)
2¬Ø
¬µ
√ó exp

(ŒΩ‚ãÜ‚àíŒ¥)(¬Ø¬µ ‚àíœï) + œï
¬Ø¬µ ‚àí1

.
To end this section, we note that a slight modiÔ¨Åcation of Geweke [1993]
allows to generate draws from a Student-t distribution with conditional variance
ht without requiring the introduction of a scaling parameter œ± .= ŒΩ‚àí2
ŒΩ . This is
done by replacing the speciÔ¨Åcation for the latent variable œñt in (5.4) by:
œñt
iid
‚àºIG
ŒΩ
2, ŒΩ ‚àí2
2

.
The use of this new speciÔ¨Åcation requires some modiÔ¨Åcations of the eÔ¨Écient
rejection scheme. We refer the reader to App. B for further details.
Finally, we note that the validity of the algorithm and the correctness of
the computer code are veriÔ¨Åed by the methodology detailed at the end of
Sect. 3.2.2.
5.3 Empirical analysis
To illustrate our Bayesian estimation method, we Ô¨Åt the Student-t-GJR(1, 1)
model to the data set used in the empirical analysis of Chap. 4. Based on
previous results, we do not include the regression part in the current estimation.
5.3.1 Model estimation
As prior densities for the GJR parameters, we choose truncated Normal densities
with zero mean vectors and diagonal covariance matrices whose variances are
set to 10‚Äô000. For the prior on the degrees of freedom parameter, we set the
hyperparameters to Œª = 0.01 and Œ¥ = 2; the prior mean is therefore 102 and the

5.3 Empirical analysis
65
prior variance 10‚Äô000. Note that the value of the hyperparameter Œ¥ is determined
so that the conditional variance exists. Moreover, we recall that the joint prior
is constructed by assuming prior independence between Œ±, Œ≤ and (œñ, ŒΩ).
We run two chains for 10‚Äô000 passes each and control the convergence of the
sampler using the diagnostic test by Gelman and Rubin [1992]. The convergence
diagnostic shows no evidence against convergence for the last 5‚Äô000 iterations
(the value of the 97.5th percentile of the potential scale reduction factor ranges
from 1.001 to 1.1). The one-lag autocorrelations in the chains range from 0.59
for parameter Œ±1 to 0.97 for parameter ŒΩ. The acceptance rate is 73% for vector
Œ± and 95% for parameter Œ≤. The optimized rejection technique allows to draw
a new value of ŒΩ at each pass in the M-H algorithm. From the overall MCMC
output, we discard the Ô¨Årst 5‚Äô000 draws and merge the two chains to get a Ô¨Ånal
sample of length 10‚Äô000.
The posterior statistics as well as the ML results are reported in Table 5.1.
First, we note that results for the GJR parameters are close to the results
of Table 4.1 (see p.48). The posterior means of the parameters are slightly
higher in the Student-t case (except for parameter Œ±0) as well as the numerical
standard errors. Second, the marginal posterior densities (not shown) are still
clearly skewed and the 95% conÔ¨Ådence band of the parameters obtained through
the asymptotic Normal approximation leads to a negative left boundary for
component Œ±1. The ML point estimate for the degrees of freedom parameter
is 9.9 while the posterior mean is 7.15 and the posterior median is 7.11. This
low value indicates a departure from Normality for the errors. In addition, the
95% conÔ¨Ådence band given by the ML approach is much wider than the one
estimated via the Bayesian approach. The left boundary is 0.14 which rejects the
existence of the conditional variance. In the case of the Bayesian estimation, the
minimum value for the degrees of freedom is 3.84, which supports the existence
of the conditional variance. The values of the ineÔ¨Éciency factor (IF) range from
3.65 for parameter Œ±1 to 111.98 for parameter ŒΩ, indicating that in the worst
case, the numerical errors represent about 1.12% of the variation of the errors
due to the data.
In Fig. 5.1, we present a comparison between the classical and the Bayesian
approaches. The upper graphs show a scatter plot of the draws from the asymp-
totic Normal approximation of the model parameters; the Normal density is
centered at the ML estimates œàMLE and its covariance matrix is estimated as
the inverse of the Hessian matrix evaluated at œàMLE. The lower graphs present a
scatter plot of draws from the joint posterior sample. In both cases, the number
of draws is 10‚Äô000. The Ô¨Årst part of the Ô¨Ågure depicts the draws for (Œ±0, Œ≤). By
comparing the ML and Bayesian outputs, we can notice a clear diÔ¨Äerence in the

66
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
Table 5.1. Estimation results for the Student-t-GJR(1, 1) model.‚ãÜ
œà
œàMLE
œà
œà0.5
œà0.025
œà0.975
min
max
IF
Œ±0
0.012
0.018
0.017
0.008
0.036
0.003
0.060
18.15
[0.002,0.021]
(0.314)
Œ±1
0.026
0.041
0.037
0.008
0.107
0.000
0.194
3.65
[-0.015,0.067]
(0.516)
Œ±2
0.153
0.203
0.194
0.105
0.350
0.055
0.533
6.93
[0.063,0.242]
(1.641)
Œ≤
0.834
0.776
0.785
0.634
0.870
0.408
0.908
54.98
[0.740,0.929]
(4.636)
ŒΩ
9.90
7.549
7.11
4.54
13.60
3.84
18.85
111.98
[0.14,19.65]
(236.50)
‚ãÜœàMLE: Maximum Likelihood estimate; œà: posterior mean; œàœÜ: estimated posterior
quantile at probability œÜ; min: minimum value; max: maximum value; IF: ineÔ¨É-
ciency factor (i.e., ratio of the squared numerical standard error and the variance of
the sample mean from a hypothetical iid sampler); [‚Ä¢]: Maximum Likelihood 95%
conÔ¨Ådence interval; (‚Ä¢): numerical standard error (√ó103). The posterior statistics
are based on 10‚Äô000 draws from the joint posterior sample.
tails of the joint density. Indeed, the Bayesian posterior exhibits larger values
for parameter Œ±0 together with lower values for parameter Œ≤. In addition, when
drawing a vertical line at Œ±0 = 0, we note that some draws are negative with
the asymptotic Normal approximation. In the posterior sample, the draws are
positive as this is required by the prior density. In the second part of Fig. 5.1,
the two graphs show the draws for (Œ±2, Œ≤). For these parameters, the posterior
sample exhibits a clear departure from the ellipsoid shape obtained with the
Normal approximation.
In Fig. 5.2, we display the prior and the posterior densities of the degrees
of freedom parameter. While the prior density is almost Ô¨Çat (we recall that
the hyperparameters are set to Œª = 0.01 and Œ¥ = 2 so that the prior mean is
102 and the prior variance 10‚Äô000), the shape of the posterior density is peaked
and concentrated around its mean value. In addition, the density is signiÔ¨Åcantly
right-skewed.

5.3 Empirical analysis
67
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Parameter Œ±0
Parameter Œ≤
Normal approximation
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Parameter Œ±0
Parameter Œ≤
Bayesian approach
Fig. 5.1. Comparison between the ML (upper graph) and the Bayesian (lower graph)
approaches. For both graphs, the number of draws is 10‚Äô000.

68
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
0.0
0.1
0.2
0.3
0.4
0.5
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Parameter Œ±2
Parameter Œ≤
Normal approximation
0.0
0.1
0.2
0.3
0.4
0.5
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Parameter Œ±2
Parameter Œ≤
Bayesian approach
Fig. 5.1. (cont.) Comparison between the ML (upper graph) and the Bayesian (lower
graph) approaches. For both graphs, the number of draws is 10‚Äô000.

5.3 Empirical analysis
69
5
10
15
20
25
30
35
0.00
0.05
0.10
0.15
0.20
Parameter ŒΩ
5
10
15
20
25
30
35
0.00
0.05
0.10
0.15
0.20
Prior density
Posterior density
Fig. 5.2. Prior and posterior densities of the degrees of freedom parameter. The
vertical line is centered at ŒΩ = 4, the value required for the conditional kurtosis of the
errors to exist. The histogram is based on 10‚Äô000 draws from the posterior sample.

70
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
5.3.2 Sensitivity analysis
As in previous chapters, we test the robustness of our results with respect to
the choice of the prior density. To that aim, we consider the same alternative
priors of Sect. 4.3.2 for parameters Œ± and Œ≤:
p(Œ±) ‚àùN3(Œ± | ¬µ Œπ3, œÉ2I3)I{Œ±>0}
p(Œ≤) ‚àùN(Œ≤ | ¬µ, œÉ2)I{Œ≤>0}
where we recall that Œπ3 is a 3√ó1 vector of ones, I3 is a 3√ó3 identity matrix, ¬µ is
the prior mean and œÉ2 the prior variance. For the alternative prior on the degrees
of freedom parameter, we consider a translated Exponential with Œª = 0.008 and
Œ¥ = 2, which implies a prior mean of 127 and a prior variance of 15‚Äô625.
The results of Table 5.2 indicate that the prior on the degrees of freedom
has the largest impact on Bayes factors. Moreover, in all cases we conclude to a
weak evidence in favor of the initial speciÔ¨Åcation relative to the alternative priors
since the Bayes factors belong to the interval [0.3125, 1]. This indicates that our
initial prior is vague enough and does not introduce signiÔ¨Åcant information in
our estimation.
Table 5.2. Results of the sensitivity analysis.‚ãÜ
Alternative priors
¬µ
œÉ2
Œª
BF
1.00
10‚Äô000
0.01
1.000
0.00
11‚Äô000
0.01
0.826
0.00
10‚Äô000
0.008
0.809
1.00
11‚Äô000
0.008
0.668
‚ãÜThe alternative priors on the parameters Œ± and Œ≤
are truncated Normal densities; ¬µ prior mean; œÉ2 prior
variance; The alternative prior on the parameter ŒΩ is
a translated Exponential with hyperparameters Œª and
Œ¥ = 2; BF: Bayes factor.
5.3.3 Model diagnostics
We test the standardized residuals for possible model misspeciÔ¨Åcation. The
Ljung-Box test does not reject the absence of autocorrelation in the residuals
at the 5% signiÔ¨Åcance level (p-value = 0.4727). This is also true for the squared
residuals (p-value = 0.8724). The Kolmogorov-Smirnov Normality test slightly

5.4 Illustrative applications
71
rejects the Normality assumption at the 5% signiÔ¨Åcance level with a p-value
of 0.0437. However, when comparing the standardized residuals to a Student-t
distribution whose degrees of freedom parameter is set to the posterior median
bŒΩ = 7.11, the Kolmogorov-Smirnov empirical distribution test does not reject
the null hypothesis at the 5% signiÔ¨Åcance level (p-value = 0.4296). Hence, the
model accounts for the conditional heteroscedasticity and for the high kurtosis
in the residuals.
5.4 Illustrative applications
We end this chapter by illustrating some probabilistic statements made on the
conditional and unconditional kurtosis of the underlying process. Under speci-
Ô¨Åcation (5.1), the conditional kurtosis Œ∫Œµ is deÔ¨Åned as follows:
Œ∫Œµ .= 3(ŒΩ ‚àí2)
ŒΩ ‚àí4
provided that ŒΩ > 4. Using the joint posterior sample, we estimate the posterior
probability of the existence for the conditional kurtosis, P(ŒΩ > 4 | y, X), to 0.999.
Therefore, the existence is clearly supported by the data. The posterior mean
of the kurtosis is 6.82 and the 95% conÔ¨Ådence interval is [3.72,13.8], indicating
heavier tails than for the Normal distribution.
Finally, we extend the analysis to the unconditional kurtosis of the process.
Let us deÔ¨Åne Œ± .= Œ±1+Œ±2
2
for notational convenience. As demonstrated by He
and Ter¬®asvirta [1999], the expression of the unconditional kurtosis Œ∫y is given
by:
Œ∫y .= Œ∫Œµ(1 + Œ± + Œ≤)(1 ‚àíŒ± ‚àíŒ≤)
1 ‚àíŒ∫Œµ
(Œ±2
1+Œ±2
2)
2
‚àí2Œ≤(Œ± + Œ≤)
provided that Œ∫Œµ is Ô¨Ånite and:
Œ∫Œµ(Œ±2
1 + Œ±2
2)
2
+ 2Œ≤(Œ± + Œ≤) < 1 .
The posterior probability of the latter condition is 0.007, meaning that there is
a 0.7% chance that the unconditional kurtosis exists.

6
Value at Risk and Decision Theory
‚ÄúDensity forecasting is fast becoming an important tool
for decision makers in situations where loss functions
are asymmetric and forecast errors follow non-Gaussian
distributions.‚Äù
‚Äî Allan Timmermann
6.1 Introduction
Since the Group of Thirty report in 1996, the Value at Risk (henceforth VaR)
has become the corner-stone in any risk management framework and is essential
in allocating capital as a cushion for market risk exposures. This measure gives,
for a given time horizon and a given conÔ¨Ådence level œÜ, the portfolio‚Äôs loss that
is expected to be exceeded with probability œÜc .= (1 ‚àíœÜ). The VaR is in many
aspects an attractive measure of risk, being relatively easy to implement and
easy to explain to non-expert audiences. While primarily designed for market
risk exposures, the VaR methodology now underpins the credit and operational
risk recommendations. From the internal models approach endorsed by the Basel
Committee on Banking and Supervision of Banks for Internal Settlement [see
Basel Committee on Banking Supervision 1995] and later adopted by US bank
regulators, banks are allowed to use their own models to estimate the VaR and
keep aside regulatory capital.
From a statistical viewpoint, the VaR is nothing else than a given percentile
of the proÔ¨Åt and loss (henceforth P&L) distribution over a Ô¨Åxed horizon. To
be acceptable by regulators, the conÔ¨Ådence level must be 99% and the holding
period must be two weeks (i.e., ten trading days). This is motivated by the fear
of a liquidity crisis where a Ô¨Ånancial institution might not be able to liquidate
its holdings for ten days straights. However, market participants consider the
99% conÔ¨Ådence level and the two weeks horizon to be too conservative. As an
additional tool for internal risk controlling, both the holding period and the
conÔ¨Ådence level can be selected to Ô¨Åt the needs of analysts; in practice, it is
common to limit the conÔ¨Ådence level to 95% and the holding period to one day.

74
6 Value at Risk and Decision Theory
Evidently VaR can only be constructed by statistical methods. But in most
applications, the true P&L distribution is not known and VaR can only be
estimated from sample data. The underlying assumption of all the VaR estima-
tion methods is that the risk associated with a particular portfolio for a Ô¨Åxed
time horizon is encapsulated within the P&L distribution. If this distribution is
known, the VaR can be obtained directly by reading the appropriate percentile
value from this distribution. If the P&L is unknown, it must be estimated. As
noted by McNeil and Frey [2000, p.272]:
(...) ‚Äúthe existing approaches for estimating the P&L distribution can
be divided into three groups: the non-parametric historical simulation
method; fully parametric methods based on an econometric model for
volatility dynamics and the assumption of the conditional distribution,
e.g., GARCH models; and Ô¨Ånally methods based on extreme value the-
ory.‚Äù
We focus on the second approach in the current application.
Within the fully parametric literature, many papers either attempt to fore-
cast the VaR at diÔ¨Äerent time horizons or use the VaR to assess the forecasting
performance of a particular model. In both cases, the methodology is the same.
First, a statistical model which describes the P&L dynamics is determined. The
model parameters are estimated by the Maximum Likelihood technique for a
given estimation window. Then, based on these estimations, a VaR point fore-
cast is determined for a given horizon. The procedure is repeated again over a
testing window by rolling the estimation window; in this manner, we obtain a
time series of VaR forecasts. Then, the model is backtested, i.e., the predictions
are compared with the realized P&Ls. Often, a statistical test is used to assess
the performance of the model, i.e., to determine whether the model captures
the true VaR [see, e.g., ChristoÔ¨Äersen 1998, Kupiec 1995].
While this methodology is accepted by academics and is widely implemented
in practice, we note that few empirical studies account for the uncertainty in
the VaR predictions. Nevertheless, this issue is important in a risk management
framework where some measure of the forecasts‚Äô accuracy is also needed; assess-
ing the uncertainty of the VaR will allow the portfolio managers to make more
informed decisions when dictating a portfolio re-balance, for instance. We may
distinguish two sources of uncertainty which can inÔ¨Çuence the VaR accuracy:
‚Ä¢ The parameter uncertainty within the context of a given model;
‚Ä¢ The model uncertainty; via a probability function deÔ¨Åned on a class
of M possibly non-nested models Mi (i = 1, . . . , M).

6.1 Introduction
75
The former source of uncertainty, also referred to as estimation risk, is straight-
forwardly handled in Bayesian inference since the complete characterization of
the parameter uncertainty is contained in the joint posterior. The latter, known
as model risk, is a natural concept in the Bayesian framework. However, in
practice, its estimation involves many diÔ¨Éculties. In eÔ¨Äect, the methodology re-
quires the estimation of the model likelihood p(y | Mi) which can be diÔ¨Écult
to estimate. Several estimation methods have been proposed but their cost is
far from negligible. In addition, the method may not work properly and can be
sensitive to the choice of the prior density. For these reasons, we concentrate
our attention on the estimation risk where the parameter uncertainty is used to
determine the VaR density instead of a single VaR point estimate.
Some approaches have been proposed to quantify the VaR uncertainty when
the P&L dynamics is described by GARCH models. Basically, these techniques
rely on the bootstrap methodology as in ChristoÔ¨Äersen and Gon¬∏calves [2004] or
on some asymptotic justiÔ¨Åcations as in Bams, Lehnert, and WolÔ¨Ä[2005]. The
former approach is computationally very demanding since at each step in the
procedure, a GARCH model is Ô¨Åtted to the bootstrapped data. While technically
more convenient, the latter approach relies on an asymptotic approximation of
the distribution of the parameter estimates. The Bayesian approach gives a nat-
ural answer to these problems, as noted by Miazhynskaia and Aussenegg [2006].
As will be shown hereafter, the s-day ahead VaR (s ‚©æ1) can be expressed as a
function of the model parameters; hence, for each parameter in the joint poste-
rior sample, we can obtain a VaR point forecast. By repeating the estimation for
each draw in the posterior sample, we obtain an estimation for the VaR density
itself. When the forecast horizon is one day, the Bayesian approach gives the ex-
act VaR density. For forecasting horizons larger than one day, an approximation
based on the Ô¨Årst four moments of the future P&L density can be obtained.
In the Bayesian framework, we can either integrate out the parameter un-
certainty or choose a Bayes point estimate within the VaR density. The former
case is achieved by simulating from the predictive density and estimating the
VaR from empirical percentiles. The latter case yields an interesting problem
of decision theory: the choice of a Bayes point estimate which is optimal given
a particular loss function. In decision theory, the common practice is to use a
symmetric squared error loss function. While this loss function is appropriate in
many statistical applications, it may however not be Ô¨Çexible enough for Ô¨Ånan-
cial purposes, where over- and underestimation may have diÔ¨Äerent consequences.
Hence a Ô¨Çexible asymmetric loss function is required.
The contributions of this chapter to the existing literature are as follows.
First, we provide a manner to approximate the multi-day ahead VaR density

76
6 Value at Risk and Decision Theory
when the underlying process is described by a GARCH model. Since this class
of models is a workhorse in Ô¨Ånancial risk management, we therefore give the
possibility to determine the VaR term structure and to characterize the uncer-
tainty coming from the parameters. Second, we give a rational justiÔ¨Åcation to
the choice of a point estimate within the VaR density based on the decision
theory framework. We document how agents facing diÔ¨Äerent risk perspectives
can select their optimal VaR point estimate and show that the diÔ¨Äerences across
agents (e.g., fund and risk managers) can be substantial in terms of regulatory
capital. Lastly, we extend our methodology to the Expected Shortfall alternative
risk measure, and show that our simulation procedure can also be applied in a
straightforward manner.
The plan of this chapter is as follows. In Sect. 6.2, we formally deÔ¨Åne the
concept of VaR and derive the s-day ahead VaR expression under GARCH
dynamics. In Sect. 6.3, we review some fundamentals of Bayesian decision
theory and introduce the asymmetric Linex loss function. In Sect. 6.4, we
propose an empirical application with the estimation of the VaR term structure.
Finally, we extend the methodology to the Expected Shortfall risk measure in
Sect. 6.5.
6.2 The concept of Value at Risk
In this section, we formally deÔ¨Åne the concept of VaR and determine the density
of the one-day ahead VaR under the GARCH(1, 1) dynamics with both Normal
and Student-t disturbances. The density of the VaR for time horizons larger
than one day is obtained by explicitly estimating the Ô¨Årst four moments of the
conditional P&L density and approximating the percentile of interest by either
using the Cornish-Fisher expansion [see Cornish and Fisher 1937] or a Student-t
approximation. We consider the GARCH(1, 1) model for ease of exposition but
the methodology can be extended, upon modiÔ¨Åcations, to higher order GARCH
models as well as asymmetric speciÔ¨Åcations.
DeÔ¨Ånition 6.1 (Value at Risk). Let Y be a univariate random variable (not
necessarily continuous) with a distribution function FY . For a given risk level
œÜ, which belongs for risk management purposes to the interval [0.90, 0.995], the
VaR of Y is deÔ¨Åned by:
VaRœÜ .= inf

y ‚ààR | FY (y) ‚©æœÜc	
where œÜc .= (1 ‚àíœÜ) for notational purposes.

6.2 The concept of Value at Risk
77
Hence, the VaR is nothing else than a percentile of the distribution of Y . When
the variable Y follows a standard Normal distribution, the VaR with conÔ¨Ådence
level œÜ is the œÜcth percentile denoted by zœÜc; e.g., z0.95 = ‚àí1.64. When the
variable Y follows a standard Student-t distribution with ŒΩ degrees of freedom,
the œÜcth percentile is denoted by tœÜc(ŒΩ); e.g., t0.95(5) = ‚àí2.01. We emphasize
the notation in the Student-t case where the VaR depends on the parameter ŒΩ.
6.2.1 The one-day ahead VaR under the GARCH(1, 1) dynamics
Under a GARCH(1, 1) model with Normal disturbances, the one-day ahead VaR
at risk level œÜ, estimated at time t, is given by:
VaRœÜ
t (œà) = h1/2
t+1(Œ±, Œ≤) √ó zœÜc
where œà .= (Œ±, Œ≤) and ht+1 is the conditional variance which is computed by
recursion given Ft, the information set at time t. Hence, under Normal distur-
bances, the one-day ahead VaR is nothing else than a given percentile of the
standard Normal distribution scaled by the conditional standard deviation.
In the case of Student-t disturbances, the one-day ahead VaR at risk level
œÜ, estimated at time t, is given by:
VaRœÜ
t (œà) =

œ±(ŒΩ) √ó ht+1(Œ±, Œ≤)
1/2 √ó tœÜc(ŒΩ)
where in this case œà .= (Œ±, Œ≤, ŒΩ). In addition to the scale factor œ±(ŒΩ) .= ŒΩ‚àí2
ŒΩ ,
the œÜcth percentile of the standard Student-t distribution depends on the model
parameter ŒΩ. For both Normal and Student-t cases, the joint posterior sample
can be used to simulate the density of the one-day ahead VaR at any conÔ¨Ådence
level œÜ.
6.2.2 The s-day ahead VaR under the GARCH(1, 1) dynamics
If the horizon is larger than one day, predictions for the cumulative returns
are needed, which in turn requires multi-step predictions. The cumulative re-
turns over an s-day horizon (starting at time t) henceforth denoted as yt,s, are
straightforwardly calculated from the single period log-returns yt+i (i = 1, . . . , s)
as:
yt,s .= yt+1 + yt+2 + . . . + yt+s .
This follows from the deÔ¨Ånition of the one-day log-return, calculated as the
logarithmic diÔ¨Äerence of asset prices.
As in the one-day ahead case, our aim is to express the VaR of the variable
yt,s as a function of the model parameters œà. However, it is well known that

78
6 Value at Risk and Decision Theory
under GARCH dynamics, no expression in closed form exists for the density of
yt+i when i > 1; hence no closed form expression is available for the density of
yt,s either. To overcome this problem, we might use Monte Carlo simulations
to generate the density of interest. That is, for a given set of parameters œà and
information set Ft, we could simulate B paths for the P&L over s days. The
VaR would then be approximated by the empirical percentile of the distribution.
In order to obtain a density for the VaR itself, this evaluation would have to be
handled for each œà in the joint posterior sample. However, since the quantity
of interest describes the tail of the distribution, a large amount of simulations
B would be required to get an accurate VaR estimate, which might lead to an
extremely costly simulation scheme.
Therefore, in order to simplify and accelerate the estimation procedure, we
propose an approximation of the VaR based on the Ô¨Årst four conditional mo-
ments of the variable yt,s which can be calculated analytically when œà is known.
To that aim, let us deÔ¨Åne the pth conditional moment of yt,s as follows:
Œ∫p(œà) .= Et,œà
 yp
t,s

where Et,œà(‚Ä¢) .= E(‚Ä¢ | œà, Ft) is the conditional expectation given œà and Ft.
The notation Œ∫p(œà) emphasizes the fact that the pth conditional moment is a
function of œà and the time index is suppressed to simplify the notation. The
explicit calculation of the Ô¨Årst four moments is possible using the multinomial
formula which gives the pth power of the cumulative return as follows:
yp
t,s =
 s
X
i=1
yt+i
!p
=
X
i1, ... ,is
i1+ ... +is=p
p !
i1! ¬∑ ¬∑ ¬∑ is! √ó yi1
t+1 ¬∑ ¬∑ ¬∑ yis
t+s .
Calculations given in Props. C.1 and C.3 (see App. C) show that under the
GARCH(1, 1) speciÔ¨Åcation, the Ô¨Årst and third conditional moments are zero
which implies that the conditional density of yt,s is symmetric around zero. Cal-
culations for the second conditional moment of yt,s in Prop. C.2 (see App. C)
yield:
Œ∫2(œà) =
s
X
i=1
Et,œà(ht+i)
where:

6.2 The concept of Value at Risk
79
Et,œà(ht+i) = Œ±0 + œÅ1 Et,œà(ht+i‚àí1)
(6.1)
and œÅ1
.= (Œ±1 + Œ≤). Expression (6.1) can be evaluated recursively from
Et,œà(ht+1) = ht+1(œà) since this value is known given œà and Ft. For the fourth
conditional moment, the calculations in Prop. C.4 (see App. C) yield the
following expression:
Œ∫4(œà) = Œ∫Œµ
s
X
i=1
Et,œà(h2
t+i) + 6
s‚àí1
X
i=1
s
X
j=i+1
Et,œà(y2
t+i y2
t+j)
(6.2)
where:
Et,œà(h2
t+i) = Œ±2
0 + œÑ1 Et,œà(ht+i‚àí1) + œÑ2 Et,œà(h2
t+i‚àí1)
(6.3)
and:
Et,œà(y2
t+i y2
t+j) = Œ±0
 
1 ‚àíœÅj‚àíi
1
1 ‚àíœÅ1
!
Et,œà(ht+i) + œÅj‚àíi‚àí1
1
œÅ2 Et,œà(h2
t+i) .
(6.4)
In expression (6.2), the parameter Œ∫Œµ denotes the fourth moment of the distur-
bances in the GARCH(1, 1) process; in the case of Normal disturbances, Œ∫Œµ = 3,
while for (scaled) Student-t disturbances, Œ∫Œµ = 3(ŒΩ‚àí2)
ŒΩ‚àí4 . The parameters œÑ1, œÑ2
and œÅ2 are functions of the set of parameters œà, respectively given by:
œÑ1 .= 2Œ±0(Œ±1 + Œ≤)
œÑ2 .= Œ∫ŒµŒ±2
1 + Œ≤(2Œ±1 + Œ≤)
and:
œÅ2 .= Œ∫ŒµŒ±1 + Œ≤ .
The conditional expectations of expressions (6.3) and (6.4) can be evaluated
recursively from Et,œà(ht+1) = ht+1(œà) and Et,œà(h2
t+1) = h2
t+1(œà) since these
values are known given œà and Ft.
As previously stated, the conditional moments Œ∫i (i = 1, . . . , 4) are used to
estimate the percentile of the conditional density of the cumulative return over
an s-day horizon. We propose two approaches to determine this percentile. The
Ô¨Årst method is the well-known Cornish-Fisher expansion by [see Cornish and
Fisher 1937] which consists in a transformation of the percentile of the standard
Normal density to account for non-zero skewness (i.e., asymmetry) and excess

80
6 Value at Risk and Decision Theory
kurtosis (i.e., fat tails). In our context, Œ∫1 and Œ∫3 are zero so that the Cornish-
Fisher formula simpliÔ¨Åes. We thus obtain the following approximation for the
s-day ahead VaR (s > 1) at risk level œÜ, estimated at time t:
VaRœÜ
t,s(œà) ‚âàŒ∫1/2
2
(œà) √ó

zœÜc + 1
24(z3
œÜc ‚àí3zœÜc)
Œ∫4(œà)
Œ∫2
2(œà) ‚àí3

(6.5)
where we recall that œÜc .= (1 ‚àíœÜ) and zœÜc is the œÜcth percentile of the standard
Normal distribution. From expression (6.5), we can notice the impact of the
excess kurtosis
  Œ∫4
Œ∫2
2 ‚àí3

on the VaR. Since conditional moments are functions
œà, so is the VaR.
The Cornish-Fisher expansion is widely used in practice due to its simplicity,
and its accuracy is suÔ¨Écient in many situations, especially when the distribution
of interest is close to the Normal. In this case, the Cornish-Fisher expansion pro-
vides a small correction for the non-zero skewness and excess kurtosis. However,
as pointed out by Jaschke [2002], the Cornish-Fisher expansion may suÔ¨Äer from
important deÔ¨Åciencies in pathological situations, for instance, when the kurtosis
of the distribution we aim to approximate is high. We note in particular that:
‚Ä¢ The approximation may yield a distribution which is not necessar-
ily monotone. Hence, we may be faced with situations where the
risk capital allocated for a 1% chance event would be lower than
the capital allocated for a 5% chance event!
‚Ä¢ The approximation has the wrong tail behavior, i.e., the Cornish-
Fisher approximation for the VaR at risk level œÜ becomes less and
less reliable for œÜ ‚Üí{0, 1}.
These drawbacks can have serious consequences for risk management systems
and we propose therefore a second method to approximate the percentiles of
interest. Since the density we aim to approximate is symmetric around zero, we
simply Ô¨Åt a Student-t density to the second and fourth conditional moments Œ∫2
and Œ∫4. First, we determine the conditional kurtosis of yt,s, denoted by bŒ∫, as
follows:
bŒ∫(œà) = Œ∫4(œà)
Œ∫2
2(œà) .
From there, we estimate the degrees of freedom parameter bŒΩ of the Student-t
density. The relation between bŒ∫ and bŒΩ is given by:
bŒΩ(œà) = 6 ‚àí4 bŒ∫(œà)
3 ‚àíbŒ∫(œà)
.

6.2 The concept of Value at Risk
81
Finally, the VaR is estimated by the appropriate percentile of the standard
Student-t density scaled by the conditional standard deviation Œ∫1/2
2
. This yields
the following approximation for the s-day ahead VaR (s > 1) at risk level œÜ,
estimated at time t:
VaRœÜ
t,s(œà) ‚âà
bŒΩ(œà) ‚àí2
bŒΩ(œà)
1/2
√ó tœÜc bŒΩ(œà)

√ó Œ∫1/2
2
(œà) .
(6.6)
This approximation is a function of the set of parameters œà. Hence, as with the
Cornish-Fisher approximation, the density of the VaR can be estimated at low
cost by simulating from the joint posterior sample.
In the Bayesian context, we can integrate out the parameter uncertainty
to end up with a single VaR point estimate. This problem is solved by the
estimation of the predictive density which is deÔ¨Åned as the density of future
s-day ahead observations, yt:s
.= (yt+1 ¬∑ ¬∑ ¬∑ yt+s)‚Ä≤ for s ‚©æ1, conditioned on past
observations y0:t
.= (y1 ¬∑ ¬∑ ¬∑ yt)‚Ä≤ (also denoted by Ft), but marginalized over œà.
More formally, the predictive density is deÔ¨Åned as follows:
p(yt:s | y0:t) =
Z
p(yt:s | œà, y0:t)p(œà | y0:t)dœà
(6.7)
where p(yt:s | œà, y0:t) is the conditional density of yt:s given (œà, y0:t) and the
marginalization is with respect to the posterior density p(œà | y0:t). In general,
the predictive density is not available in closed form. However, one can use the
posterior sample in conjunction with the method of composition to produce a
sample of draws from the predictive density. We simulate a draw y[j]
t:s from the
density p(yt:s, œà | y0:t) as follows:
œà[j] ‚àºp(œà | y0:t)
y[j]
t:s ‚àºp(yt:s | œà[j], y0:t)
(6.8)
where the second step in the simulation process is possible by using the method
of composition:
p(yt:s | œà, y0:t) =
s
Y
i=1
p(yt+i | œà, y0:(t+i‚àí1)) .
(6.9)
The collection of simulated values {y[j]
t:s}J
j=1 is generated from the predictive
density in (6.7) and the predictive VaR is a percentile of this density. For the
one-day ahead VaR, we only need to consider the Ô¨Årst component of vector y[j]
t:s
whereas for the s-day ahead VaR, we must sum the components of y[j]
t:s to sim-
ulate the predictive density for yt,s. From (6.7), we notice that the predictive

82
6 Value at Risk and Decision Theory
VaR is a quantile of a mixture density. Therefore, it can be viewed as an exten-
sion of the case where there is no parameter uncertainty (i.e., œà is constant);
in this case, the predictive VaR would simply be estimated by a percentile of
p(yt:s | œà, y0:t).
To end this section, we illustrate the quality of the Cornish-Fisher and
Student-t approximations through a simulation study. To that aim, we estimate
the GARCH(1, 1) model with Student-t disturbances for the 750 Deutschmark
vs British Pound foreign exchange log-returns used in the empirical analysis of
Chap. 3. We arbitrarily select a set of parameters œà .= (Œ±, Œ≤, ŒΩ) in the joint
posterior sample:
Œ± =
 
0.036
0.297
!
,
Œ≤ = 0.626
and
ŒΩ = 5.4
and estimate the VaR using formulae (6.5) and (6.6) for s = 10 and œÜ ranging
from 0.001 to 0.999 with a step size of 0.001. This procedure allows to draw two
approximations for the distribution of y750,10. These distributions are compared
with the distribution obtained by simulating 10‚Äô000 paths of the process over ten
days, using (6.9). Under the model speciÔ¨Åcation and the selected œà, we obtain
Œ∫2 = 3.8 and Œ∫4 = 492, implying a conditional kurtosis bŒ∫ = 34 and a degrees of
freedom parameter bŒΩ = 4.2. The simulated distribution clearly exhibits heavier
tails than the Normal distribution.
On the left-hand side of Fig. 6.1, we display the two approximations to-
gether with the distribution obtained by simulation. The Cornish-Fisher ap-
proximation is shown in dotted line, the Student-t approximation in dashed line
and the empirical distribution in solid line. From this Ô¨Ågure, it is almost im-
possible to distinguish the approximation based on the Student-t distribution
from the simulated distribution. In contrast to this, the Cornish-Fisher expan-
sion produces a S-shaped, non-monotone distribution. The four shaded regions
delimit the extreme quantiles, at risk level œÜ ‚àà{0.01, 0.05, 0.95, 0.99}. We notice
that the approximations for œÜ ‚àà{0.05, 0.95} are quite similar for the Cornish-
Fisher and the Student-t approaches. However, the diÔ¨Äerence is substantial in
the case where œÜ ‚àà{0.01, 0.99}. In the middle graph of Fig. 6.1, we show a
zoom of the previous graph over the domain [2, 6] √ó [0.94, 1]. We can see that
the Student-t approximation Ô¨Åts the distribution of interest well. Hence, in this
particular example, the graphical comparison indicates that the Cornish-Fisher
expansion fails in approximating the distribution of interest. On the other hand,
the approximation based on the Student-t distribution seems to provide an ad-
equate approximation of the whole distribution. To complete the simulation
study, we display, on the right-hand side of Fig. 6.1, the diÔ¨Äerence between

6.2 The concept of Value at Risk
83
the simulated distribution and the Student-t approximation as a function of œÜc.
The dotted lines delimit the 95% conÔ¨Ådence band for the simulation, estimated
by replicating 500 times the empirical distribution. We note that the diÔ¨Äerence
lies within the [‚àí0.1, 0.1] interval for risk levels ranging from 0.05 to 0.95. For
other percentiles, the error increases together with the width of the conÔ¨Ådence
band. However, the conÔ¨Ådence interval still contains the value of zero, indicating
a good approximation in the tails too.
Finally, we note that other approximation methods of the whole density
for yt,s can be obtained [see, e.g., HighÔ¨Åeld and Zellner 1988]. This is of inter-
est when the density we aim to approximate is skewed, since in this case, the
Student-t approximation would fail. Such asymmetric densities arise, e.g., with
asymmetric GARCH models [see Engle 2004, p.415].

84
6 Value at Risk and Decision Theory
Approximations
VaRœÜ
œÜc
‚àí8
‚àí6
‚àí4
‚àí2
0
2
4
6
8
0.0
0.2
0.4
0.6
0.8
1.0
Predictive distribution
Cornish‚àíFisher
Student‚àít
VaRœÜ
œÜc
2
3
4
5
6
0.94
0.95
0.96
0.97
0.98
0.99
1.00
Zoom
0.0
0.2
0.4
0.6
0.8
1.0
‚àí0.4
‚àí0.2
0.0
0.2
0.4
0.0
0.2
0.4
0.6
0.8
1.0
‚àí0.4
‚àí0.2
0.0
0.2
0.4
0.0
0.2
0.4
0.6
0.8
1.0
‚àí0.4
‚àí0.2
0.0
0.2
0.4
œÜc
Prediction minus Student‚àít approximation
Approximation error
Fig. 6.1. Cornish-Fisher and Student-t approximations. On the left-hand side, we display the distribution given by the Cornish-Fisher (in
dotted line) and the Student-t (in dashed line) approximations together with the simulated distribution based on 10‚Äô000 paths (in solid
line). In the middle graph, we zoom the plot over the [2, 6] √ó [0.94, 1] domain. On the right-hand side, we plot the diÔ¨Äerence between
the simulated distribution and the Student-t approximation. The dotted lines delimit the conÔ¨Ådence band obtained by bootstrapping the
simulated distribution 500 times. The shaded regions indicate the 1st, 5th, 95th and 99th percentiles.

6.3 Decision theory
85
6.3 Decision theory
Using the Bayesian approach leads to an interesting problem of decision theory:
the choice of a Bayes point estimate within the whole VaR posterior density.
In this section, we present a short review of decision theory and introduce the
asymmetric Linex loss function. The use of asymmetric loss functions better
characterizes the views of market participants where the impact of underesti-
mation and overestimation can be signiÔ¨Åcantly diÔ¨Äerent. The Linex loss function
has proved to be advantageous in many Ô¨Åelds, especially for Ô¨Ånancial applica-
tions.
To keep the notation as general as possible, the decisions are formulated in
terms of œâ which can either be viewed as a one-dimensional parameter or a point
forecast.
6.3.1 Bayes point estimate
As is the case in economics, statistical decisions are made based on expected
ranking. In economics this ranking is achieved with the help of a utility func-
tion while we use a loss function in statistics. The Bayesian statistical decision
consists in the choice of a point estimate over the posterior density of the pa-
rameters.
Let us assume that a decision maker needs to choose a point estimate bœâ
and the true state of nature is œâ. In the Bayesian framework, the parameter
œâ is random and its uncertainty is fully characterized by its posterior density
p(œâ | y). Furthermore, we deÔ¨Åne the loss function L (bœâ, œâ) which is the loss
incurred when œâ is the true state of nature and bœâ is a point estimate. Then, the
Bayes estimate, also referred to as the optimal point estimate, denoted by bœâL ,
is the parameter which minimizes the posterior risk RL (bœâ | y). Formally, the
Bayes estimate is deÔ¨Åned as follows:
bœâL .= arg min
bœâ RL (bœâ | y)
(6.10)
where:
RL (bœâ | y) .=
Z
L (bœâ, œâ)p(œâ | y)dœâ .
The problem of point estimation of a location parameter or forecast is most
often treated as a symmetric problem in which positive and negative estimation
errors of the same magnitude are considered to be equally serious; thus, the loss
function L is symmetric. The most used loss functions are the squared error loss

86
6 Value at Risk and Decision Theory
(henceforth SEL), L (bœâ, œâ) = (bœâ ‚àíœâ)2, and the absolute error loss (henceforth
AEL), L (bœâ, œâ) = |bœâ ‚àíœâ|. Indeed most of the existing VaR literature ignores
the asymmetric loss relevant for diÔ¨Äerent economic agents. However, the impact
of overestimating or underestimating VaR can be quite diÔ¨Äerent. As quoted by
Knight, Satchell, and Wang [2003, p.335]:
‚ÄúFrom the perspective of the fund manager in a bank, the loss of over-
estimating is usually much greater than that of underestimating, as the
reserve capital exceeds the capital required by regulation and earns little
or no return at all. On the other hand, from the regulator‚Äôs perspective,
systematic failure would be increasing in the degree to which each bank‚Äôs
losses actually exceed their capital reserves. So underestimating will re-
sult in more loss for the regulator.‚Äù
This suggests the need of an appropriate asymmetric loss function when choosing
a point estimate within the VaR density.
We point out that, in light of the capital structure theory, the relevance of
an asymmetric loss function for banks is questionable since capital reserves do
not have to be held in cash. In this case, if regulators demand a higher capital
reserve, the bank will just have to rearrange its capital structure, which does
not necessarily increase capital cost. This suggests that bank managers should
in fact not be interested in minimizing regulatory capital. While this argument
is valid for the bank as a whole, it does not hold at the trading desk level since
it is common that traders and fund managers need a buÔ¨Äer in cash for facing
market risk exposures.
6.3.2 The Linex loss function
The Linex loss function is employed in the analysis of several central statistical
estimation and prediction problems. Varian [1974] motivates the use of the Linex
loss function on the basis of an example in which there is a natural imbalance
in the economic results of estimation errors of the same magnitude. Varian
argues that the Linex loss is a rational manner to formulate the consequences
of estimation errors in real estate assessment. ChristoÔ¨Äersen and Diebold [1996,
1997] use the Linex loss function in a study of optimal point prediction where
diÔ¨Äerent asymmetric loss functions are tested. More recently, Hwang, Knight,
and Satchell [1999, 2001] derive the Linex one-day ahead volatility forecast for
various volatility models. The empirical results of these authors suggest the
Linex loss function to be particularly well-suited in Ô¨Ånancial applications. In
addition, we note that other Ô¨Åelds than quantitative Ô¨Ånance make use of the
Linex loss function. An example is given in the Ô¨Åeld of hydrology with the

6.3 Decision theory
87
estimation of peak water level in the construction of dams or levies. In that
case, overestimation represents a conservative error which increases construction
costs, while underestimation corresponds to the much more serious error in
which overÔ¨Çows might lead to huge damages in the adjacent communities.
In its reduced form, the Linex loss function is given by:
L (bœâ, œâ) = exp[a‚àÜ] ‚àía‚àÜ‚àí1
(6.11)
where a ‚ààR‚àóand ‚àÜ.= (bœâ ‚àíœâ) denotes the scalar estimation error in using bœâ
when estimating œâ. From expression (6.11), we note that:
‚Ä¢ L is a convex function of ‚àÜ;
‚Ä¢ L is decreasing for ‚àÜ‚àà] ‚àí‚àû, 0[ and increasing for ‚àÜ‚àà]0, ‚àû[;
‚Ä¢ for a > 0, L grows exponentially in positive ‚àÜbut behaves ap-
proximately linearly for negative values of ‚àÜ. In this case, the Linex
loss function imposes a substantial penalty for overestimation, i.e.,
when bœâ > œâ;
‚Ä¢ for |a| ‚âÉ0, L is almost symmetric and not far from a squared
error loss function. Indeed, on expanding:
exp[a‚àÜ] ‚âÉ1 + a‚àÜ+ (a‚àÜ)2
2
and replacing it in expression (6.11), the loss function becomes
proportional to the SEL function. Thus for small values of a, the
SEL function is approximately nested within the Linex function.
In the upper graph of Fig. 6.2, we display the Linex loss function for parameter
a = 0.5 in dotted line, a = 1 in dashed line and a = 2 in solid line. We can
notice the impact on the shape of the loss function of larger values of parameter
a. Indeed, as a increases, the asymmetry accentuates. In the lower part of the
Ô¨Ågure, we show the Linex loss function for parameter a = 0.1 together with the
appropriately scaled SEL function. The loss functions are almost the same on
the interval.
As veriÔ¨Åed by Zellner [1986], the derivation of the Bayes estimator of œâ is
straightforward under the Linex loss function (6.11). The optimization prob-
lem (6.10) yields:
bœâL = ‚àí1
a ln
Z
exp[‚àíaœâ]p(œâ | y)dœâ


88
6 Value at Risk and Decision Theory
provided that the integral is Ô¨Ånite. Hence, the key to Linex estimation is to Ô¨Ånd
the moment generating function of p(œâ | y), which can be estimated using the
posterior sample.

6.3 Decision theory
89
‚àí1.5
‚àí1.0
‚àí0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
‚àí1.5
‚àí1.0
‚àí0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
‚àí1.5
‚àí1.0
‚àí0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
Linex loss function
‚àÜ
a=0.5
a=1
a=2
‚àí1.5
‚àí1.0
‚àí0.5
0.0
0.5
1.0
1.5
0.000
0.005
0.010
0.015
‚àí1.5
‚àí1.0
‚àí0.5
0.0
0.5
1.0
1.5
0.000
0.005
0.010
0.015
Linex and approximation
‚àÜ
Linex (a=0.1)
Approximation
Fig. 6.2. Linex loss function. In the upper graph we plot the Linex function for
diÔ¨Äerent values of parameter a. In the lower part, we show the Linex loss function for
parameter a = 0.1 in solid line together with the SEL function in dashed line. We
recall that ‚àÜ.= (bœâ ‚àíœâ) where bœâ is the point estimate and œâ is the true parameter
value.

90
6 Value at Risk and Decision Theory
6.3.3 The Monomial loss function
We end this section by noting that other interesting asymmetric loss functions
are readily available in the statistical literature. A general class of asymmetric
loss functions, referred to as Monomial-splined functions by Thompson and Basu
[1995], is deÔ¨Åned as follows:
L (‚àÜ) .=
(
a1 √ó |‚àÜ|p
if
‚àÜ‚©æ0
a2 √ó |‚àÜ|p
if
‚àÜ< 0
(6.12)
where ‚àÜ.= (bœâ‚àíœâ), ai ‚ààR+ (i = 1, 2) and p ‚ààN‚àó. This class provides asymmet-
ric loss functions for a1 Ã∏= a2; when a1 > a2, an overestimation incurs more loss
than an underestimation and inversely when a1 < a2. From expression (6.12),
we note the two following special cases:
‚Ä¢ p = 1: linear-linear loss function; when a1 = a2 we obtain the AEL
function;
‚Ä¢ p = 2: quadratic-quadratic loss function; when a1 = a2 we obtain
the SEL function.
It is often easier to work with a reparametrization of expression (6.12). Let us
deÔ¨Åne q .=
a1
a1+a2 and make use of the homogeneity property so that we obtain
the following loss function:
L (‚àÜ) =
 q + (1 ‚àí2q)I{‚àÜ<0}

|‚àÜ|p .
(6.13)
This function is particularly interesting when p = 1. In this case, it turns out
that the optimal point estimate ÀÜœâL is nothing else than the qth percentile
of the posterior density p(œâ | y). Hence, expression (6.13) gives a statistical
justiÔ¨Åcation to the choice of a posterior percentile as Bayes point estimate.
E.g., choosing the 95th percentile of the VaR density would be optimal for an
agent whose loss function is given by (6.13) with q = 0.95. Finally, we note
that numerical methods are needed to Ô¨Ånd Bayes point estimates for the loss
function (6.13) when p > 1.
In Fig. 6.3, we display the loss function given in expression (6.13) for pa-
rameter q = 0.5 in dotted line, q = 0.75 in dashed line and q = 0.95 in solid
line. As parameter q increases, the asymmetry becomes more pronounced and
the impact of an overestimation, i.e., ‚àÜ> 0, is larger compared to the impact
of an underestimation. The function is symmetric for q = 0.5.

6.4 Empirical application: the VaR term structure
91
‚àí1.5
‚àí1.0
‚àí0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
‚àí1.5
‚àí1.0
‚àí0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
‚àí1.5
‚àí1.0
‚àí0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
Monomial loss function
‚àÜ
q=0.5
q=0.75
q=0.95
Fig. 6.3. Monomial loss function for diÔ¨Äerent values of parameter q. We recall that
‚àÜ.= (bœâ ‚àíœâ) where bœâ is the point estimate and œâ is the true parameter value.
6.4 Empirical application: the VaR term structure
In this section, we estimate the term structure of the VaR when the P&L dy-
namics is described by a GARCH(1, 1) model with Normal and Student-t dis-
turbances. Our analysis is inspired by the paper of Guidolin and Timmermann
[2006] which considers the impact of diÔ¨Äerent econometric speciÔ¨Åcations to the
shape of the VaR term structure. While the authors report signiÔ¨Åcant diÔ¨Äer-
ences between the models, they do not account for parameter uncertainty in
their analysis. This is indeed a weakness of their approach, as recognized by the
authors [see Guidolin and Timmermann 2006, p.307]:
‚ÄúWe ignored parameter estimation uncertainty in our analysis, but this
could have important eÔ¨Äects on the results.‚Äù
The Bayesian approach provides a natural framework for investigating this
point. As shown in Sects. 6.2.1 and 6.2.2, the VaR can be expressed as a
function of the GARCH(1, 1) parameters under both Normal and Student-t

92
6 Value at Risk and Decision Theory
speciÔ¨Åcations. Consequently, the parameter uncertainty estimated by the joint
posterior sample can be used to estimate the density of the VaR in a convenient
manner.
An additional justiÔ¨Åcation for the use of the Bayesian approach in our con-
text is given by Miazhynskaia and Aussenegg [2006] who compare the Bayesian
and traditional techniques for estimating GARCH models. In particular, they
conclude that the Bayesian approach is an adequate framework with less un-
certainty in VaR estimates compared to other VaR methods such as resampling
technique and asymptotic Normal approximation. They also mention the inter-
esting issue of determining a single VaR point estimate [see Miazhynskaia and
Aussenegg 2006, p.262]:
‚ÄúOpen questions for future research are how the total VaR distribution
can be used in market risk management and how to account for VaR
uncertainty in choosing traditional VaR point estimates used to calculate
capital requirements for Ô¨Ånancial institution.‚Äù
This is precisely what we aim to achieve in a rational manner through the
decision theory framework.
6.4.1 Data set and estimation design
Our empirical analysis uses the Deutschmark vs British Pound exchange rate
daily log-returns over a sample period ranging from January 3, 1985, to Decem-
ber 31, 1991, for a total of 1‚Äô974 observations. This data set was used in the
empirical analysis of Chap. 3.
We consider daily log-returns so that the VaR term structure focuses on
short-term horizons. This is of primary interest for traders and risk managers
who adjust the bank‚Äôs portfolios on a daily basis. The methodology can also be
applied to longer time span log-returns as this is done in Guidolin and Timmer-
mann [2006]. In this manner, a term structure for mid- and long-term horizons is
obtained. Note however that modeling monthly or quarterly Ô¨Ånancial data would
probably require more complicated models than the GARCH(1, 1) speciÔ¨Åcation,
to account for structural breaks in the time series, for instance. This could dras-
tically complicate the approximation methodology developed in Sect. 6.2.2, in
particular to Ô¨Ånd the Ô¨Årst four moments conditioned on the model parameters
and information set.
The estimation of the GARCH(1, 1) models is achieved by using the rolling
window methodology. This procedure is heavily used in Ô¨Ånance and Ô¨Ånancial
risk management. The rationale behind it is to act as if we were moving over
time, using past observations to estimate the model and test the performance

6.4 Empirical application: the VaR term structure
93
over a prediction window. This is based on the assumption that older data are
not available or are irrelevant due to structural breaks, which are so complicated
that they cannot be modeled. Conceptually, this method aims to take account
for more recent information in a simpliÔ¨Åed framework and it has proved to be
eÔ¨Äective in many Ô¨Ånancial applications.
We structure the estimation procedure as follows: 750 log-returns, which is
about three trading years, are used to estimate the models. Then, the next 50
log-returns, which is slightly less than one quarter, are used as a forecasting
window. In the next step, the estimation and forecasting windows are moved
together by 50 days ahead, so that the forecasting windows do not overlap. In
this manner, the model parameters are updated every quarter and the estimation
methodology fulÔ¨Ålls the recommendations of the Basel Committee in the use of
internal models [see Basel Committee on Banking Supervision 1996b]. When
applied to our data set, the estimation design leads to the generation of 24
estimation windows. The non-overlapping forecasting windows represent a total
of 24 √ó 50 = 1‚Äô200 observations. An illustration of the methodology is shown
in Fig. 6.4 where we plot the Ô¨Årst three observation windows excerpt from our
data set; the vertical lines separate the estimation and the forecasting windows.
Note that the standard practice when using the rolling window methodology
in the context of GARCH models consists in moving the window by a single
day ahead. While this procedure can be achieved quite rapidly when estimating
the model by the Maximum Likelihood technique, this can become a computa-
tional burden with the Bayesian approach, since at each step, we need to run
the MCMC scheme again. This problem is however only relevant in an ex-post
framework; a portfolio or risk manager could run the Bayesian estimation of the
model every day, without encountering these computational diÔ¨Éculties.

94
6 Value at Risk and Decision Theory
0
200
400
600
800
1000
‚àí2
‚àí1
0
1
2
0
200
400
600
800
1000
‚àí2
‚àí1
0
1
2
Observations windows
Daily log‚àíreturns
(in percent)
time
index
0
200
400
600
800
1000
‚àí2
‚àí1
0
1
2
0
200
400
600
800
1000
‚àí2
‚àí1
0
1
2
0
200
400
600
800
1000
‚àí2
‚àí1
0
1
2
0
200
400
600
800
1000
‚àí2
‚àí1
0
1
2
Fig. 6.4. Estimation and forecasting windows (Ô¨Årst 3 windows out of 24). The 750
log-returns used for the estimation are shown in solid line and the 50 out-of-sample
log-returns are shown in dotted line. The vertical line separate the estimation and the
forecasting windows. At each step in the procedure, both windows are moved together
by 50 days ahead.
6.4.2 Bayesian estimation
As prior densities for the scedastic function‚Äôs parameters Œ± and Œ≤, we choose
truncated Normal densities with zero mean vectors and diagonal covariance
matrices whose variances are set to 10‚Äô000. In the case of Student-t disturbances,
we use the translated Exponential as a prior density for the degrees of freedom
parameter; the hyperparameters are set to Œª = 0.01 and Œ¥ = 4; the prior mean
is therefore 104 and the prior variance 10‚Äô000. The parameter Œ¥ is set so that the
conditional variance and conditional fourth moment exist, which allows the use
of the approximation for the predictive density based on the Ô¨Årst four moments
(see Sect. 6.2.2 for details). For each estimation window, two chains are run for
10‚Äô000 passes each and the convergence diagnostic test by Gelman and Rubin
[1992] is applied to guarantee a good convergence of the algorithm. From the

6.4 Empirical application: the VaR term structure
95
overall MCMC output, we discard the Ô¨Årst 5‚Äô000 draws and merge the two chains
to get a Ô¨Ånal sample of length 10‚Äô000.
6.4.3 The term structure of the VaR density
As a preliminary analysis, we consider the Ô¨Årst observation window excerpt from
our data set and estimate the (conditional) term structure of the VaR at risk level
œÜ = 0.95. We consider risk horizons ranging from one day to Ô¨Åfteen days for the
VaR density estimated under both GARCH(1, 1) Normal and Student-t models
using approximation (6.6). The two term structures are depicted in Fig. 6.5; the
lines give the median point estimates while the shaded regions depict the 95%
conÔ¨Ådence intervals of the densities. From this graph, we note that the VaR is a
monotone decreasing function of the time horizon for both models. The Student-t
speciÔ¨Åcation leads to lower median point estimates for time horizons larger than
Ô¨Åve days while the diÔ¨Äerences for smaller horizons are less pronounced. We also
notice that the VaR uncertainty increases for both models with respect to the
time horizon. Furthermore, the GARCH(1, 1) model with Student-t disturbances
leads to higher uncertainty in VaR at each horizon compared to the Normal
speciÔ¨Åcation. Finally, we note that the VaR density is almost symmetric for all
horizons in the Normal case while the density is left-skewed for the Student-t
model.
This Ô¨Årst static analysis indicates that the density of the VaR is inÔ¨Çuenced by
the time horizon as well as the speciÔ¨Åcation of the model disturbances; leptokur-
tic disturbances lead to a larger uncertainty in the VaR as well as a left-skewed
density.

96
6 Value at Risk and Decision Theory
VaR term structure
(in percent)
time horizon
(in days)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
‚àí4.5
‚àí4.0
‚àí3.5
‚àí3.0
‚àí2.5
‚àí2.0
‚àí1.5
‚àí1.0
 
 
 
 
Posterior median (Normal disturbances)
Posterior median (Student‚àít disturbances)
95% CI (Normal disturbances)
95% CI (Student‚àít disturbances)
Fig. 6.5. Term structures of the VaR density at risk level œÜ = 0.95 for the
GARCH(1, 1) model with Normal and Student-t disturbances. Both densities are based
on 10‚Äô000 draws from the joint posterior sample of the models‚Äô parameters.
6.4.4 VaR point estimates
We now investigate the diÔ¨Äerences in VaR point estimates under diÔ¨Äerent loss
functions of the forecasters. The comparison of point estimates over the out-of-
sample window has two purposes. First, it will provide a statistical counterpart
to the graphical Ô¨Åndings observed in the preceding section. Second, since the
VaR point estimates are used to calculate capital requirements for Ô¨Ånancial in-
stitutions, this analysis will give a Ô¨Årst idea on how large the diÔ¨Äerences in risk
capital are between agents facing diÔ¨Äerent risk perspectives. In what follows, we
concentrate the analysis on horizon s ‚àà{1, 5, 10} and risk level œÜ ‚àà{0.95, 0.99}.
Note that the particular case (s = 1, œÜ = 0.95) corresponds to the criterion
employed by the popular RiskMetrics benchmark [see RiskMetrics Group 1996].
The case (s = 10, œÜ = 0.99) is recommended by the Basel Committee on Bank-
ing Supervision [1996a] and aims to take in consideration liquidity constraints
encountered by the bank.

6.4 Empirical application: the VaR term structure
97
To compare the VaR point estimates resulting from the use of diÔ¨Äerent loss
functions, we use the following methodology: for each point in the out-of-sample
data set, we estimate the density of the VaR for the three diÔ¨Äerent time hori-
zons and the two risk levels using approximation (6.6). Then, for each den-
sity, we determine a point estimate for the VaR which solves the optimization
problem (6.10) for a given loss function L ; this point estimate is denoted by
d
VaR
œÜ
L ,t,s. In what follows, we use the asymmetric Linex loss, the absolute error
loss (AEL) as well as the squared error loss (SEL) functions for L , the latter
being considered as the benchmark in our analysis. As an additional point es-
timate, we use the predictive VaR deÔ¨Åned in (6.7). In this case, for each draw
in the joint posterior sample, we generate a draw from the predictive density
using (6.8) and the predictive VaR is obtained by calculating the appropriate
percentile of the simulated density.
Some comments regarding the diÔ¨Äerent perspectives of VaR point estimation
are in order here. In the Ô¨Årst approach, we estimate the density of the VaR by
simulating from the joint posterior sample and choose a point estimate which
is optimal for a given loss function (Linex, SEL and AEL). The parameter
uncertainty is integrated out in the second step of the procedure, when the
posterior risk is minimized (see Sect. 6.3.1). This methodology is natural in
combining estimation and decision making, and gives therefore an additional
Ô¨Çexibility to the user. In the second approach, the parameter uncertainty is
integrated out by averaging the conditional densities of the cumulated returns
over the joint posterior density of the model parameters. In this case, the VaR
point estimate (i.e., the predictive VaR) is not related to the risk preferences of
an agent and is the same for both regulators and fund managers. Hence, agents
diÔ¨Äer not in the estimation of the VaR, but in the way they would make use of
the point estimate afterwards. This approach is natural in Bayesian statistics
but it is not the most sophisticated. It can be viewed as an extension of the
case where there is no parameter uncertainty (i.e., œà is constant); in this case,
the predictive VaR would simply be estimated by a percentile of the conditional
density of future observations. The justiÔ¨Åcation of the predictive VaR on the
grounds of the decision theory still needs to be established.
To Ô¨Ånd an optimal VaR point estimate under the Linex loss function, we
need Ô¨Årst to choose a value for the parameter a in expression (6.11). The most
direct way is to elicit the parameter through detailed discussion with a fund
or risk manager. Due to the unavailability of this type of data, we will thus
rely on the estimation of Knight et al. [2003] where the authors found a ‚âà3
based on Standard and Poors 500 index data. In our framework, since the VaR
estimates are negative percentages, this positive parameter a implies a larger

98
6 Value at Risk and Decision Theory
penalty when the estimated VaR is underestimated (in absolute value) compared
to the true VaR, i.e., d
VaR > VaR. Hence, the Linex optimal point estimate
will be conservative in the sense that it will be located in the left tail of the
density to avoid underestimation. Such loss function can thus be attributed to
a regulator or a risk manager whose aim is to avoid systematic failure in VaR
estimation. For comparison purposes, we also consider the Linex function with
parameter a = ‚àí3. In that case, overestimating (in absolute value) the VaR,
i.e., d
VaR < VaR, leads to a larger penalty so that the Linex point estimate will
be located in the right tail of the VaR density. This is the loss perspective of
a trader or fund manager whose aim is to save regulatory capital since it earns
little or no return at all (as pointed out previously, traders hold a buÔ¨Äer in
cash for facing market risk exposures). The AEL and SEL functions correspond
to the perspective of an agent for which under- and overestimation are equally
serious; the SEL leads however to a larger penalty for large deviations from the
true VaR, compared to the AEL function.
Once the time series of VaR point estimates for a given loss function has
been obtained, we compare its values to the SEL benchmark. More precisely,
for a given risk level œÜ and time horizon s we compute a time series of diÔ¨Äerences
between the VaR point estimates obtained with the loss function L and the SEL
benchmark. Then, we estimate the average of deviations over the N = 1‚Äô200
out-of-sample observations as follows:
1
N
N
X
t=1

d
VaR
œÜ
L ,t,s ‚àíd
VaR
œÜ
SEL,t,s

.
Results of the average deviations are reported in Table 6.1 where the table
entries are given in hundredth percent, i.e., multiplied by 100, for convenience.
The upper panel gives the results for the GARCH(1, 1) model with Normal
disturbances while the lower panel presents the results for the Student-t case.
From this table, we Ô¨Årst note that the average deviations vary considerably
between table entries; the minimum value is 0.0001% in the case of the predictive
VaR for Normal disturbances with (s = 1, œÜ = 0.95) while the maximum is
0.496% in the case of the Linex VaR (a = 3) for Student-t disturbances with
(s = 10, œÜ = 0.99). Moreover, we note that the average size of deviations increase
with respect to the forecasting horizon. The deviations are also larger for risk
level œÜ = 0.99 and Student-t disturbances (except for the predictive VaR at risk
level œÜ = 0.99).
Deviations from the SEL benchmark are expected for the two Linex func-
tions. Indeed, the asymmetric nature of the function leads to point estimates
which are necessarily lower or larger than the mean point estimate. In the case

6.4 Empirical application: the VaR term structure
99
of the AEL function, departure from the SEL indicates asymmetric shapes for
the VaR densities over the out-of-sample window. More precisely, the AEL point
estimates are less conservative than the SEL on average, indicating left-skewed
densities for the VaR. This asymmetry can also be captured by comparing the
average deviations for the Linex loss functions. While a symmetric density for
the VaR would imply similar values (of opposite sign) for the two Linex func-
tions, this is clearly not the case here, especially for the Student-t density at
time horizons s = 5 and s = 10. Finally, we can notice that the predictive VaR
point estimates are close to the SEL point estimates for almost all risk levels
and time horizons; for these cases, choosing a quantile of the predictive distri-
bution or choosing the posterior mean of the VaR leads to the same VaR point
estimates, on average.
In summary, the VaR is left-skewed and the asymmetry as well as the un-
certainty increase with respect to the time horizon and the risk level. The av-
erage deviations are also larger when the GARCH(1, 1) model disturbances are
Student-t distributed. At Ô¨Årst sight, the deviations seem negligible (we recall
that the maximum deviation is half a percent). As will be shown later in this
chapter, the common testing methodology for assessing the performance of the
VaR is unable to discriminate between the point estimates but the deviations
are large enough to imply substantial diÔ¨Äerences in terms of regulatory capital.
This therefore gives an additional Ô¨Çexibility to the user when allocating risk
capital.

100
6 Value at Risk and Decision Theory
Table 6.1. Average deviations of the VaR point estimates from the SEL
benchmark.‚ãÜ
GARCH(1, 1) with Normal disturbances
œÜ = 0.95
œÜ = 0.99
Loss L
s = 1
s = 5
s = 10
s = 1
s = 5
s = 10
Linex (a = 3)
‚àí0.233
‚àí1.385
‚àí3.605
‚àí0.467
‚àí4.041
‚àí12.790
Linex (a = ‚àí3)
0.230
1.332
3.226
0.459
3.602
9.470
AELa
0.066
0.256
0.604
0.093
0.637
1.628
Predictiveb
0.011
‚àí0.148
‚àí0.145
‚àí0.344
‚àí1.603
‚àí2.309
GARCH(1, 1) with Student-t disturbances
œÜ = 0.95
œÜ = 0.99
Loss L
s = 1
s = 5
s = 10
s = 1
s = 5
s = 10
Linex (a = 3)
‚àí0.318
‚àí2.496
‚àí11.598
‚àí1.087
‚àí9.911
‚àí49.603
Linex (a = ‚àí3)
0.312
2.185
6.695
1.043
7.739
22.548
AELa
0.091
0.570
1.863
0.241
1.204
3.498
Predictiveb
0.013
‚àí0.104
1.353
‚àí0.027
0.645
0.847
‚ãÜThe tables entries are given in hundredth percent, i.e., multiplied by 100. L :
loss function; s: time horizon (in days); œÜ: risk level.
a Absolute error loss.
b In the case of the predictive VaR, the point estimate is the œÜcth percentile of
the predictive density for the s-day ahead cumulative return; œÜc .= 1 ‚àíœÜ.
6.4.5 Regulatory capital
In this section, we assess the Ô¨Ånancial consequences resulting from the use of a
particular loss function when determining a VaR point estimate. To that aim,
we will base our analysis on the notion of the regulatory capital as deÔ¨Åned by the
Basel II approach for market risk [see Basel Committee on Banking Supervision
1996b]. This capital is a cushion for market risk exposures and its value is based
on the ten-day ahead VaR at risk level œÜ = 0.99. Formally, the regulatory capital
allocated at time t by an agent facing a loss function L can be expressed as
follows:
c
RCL ,t .= min
(
d
VaR
0.99
L ,t,10, Œ∂
60
59
X
i=0
d
VaR
0.99
L ,t‚àíi,10
)
(6.14)
where d
VaR
0.99
L ,t,10 denotes the ten-day ahead VaR point estimate at time t, for
risk level œÜ = 0.99 and loss function L . The value Œ∂ ‚àà[3, 4] is a stress factor
determined by the quality of the model; it is Ô¨Åxed by the regulators and is based
on the forecasting performance of the model. We will set Œ∂ to 3.5 for simplicity
in what follows. From formula (6.14), we note that the regulatory capital is

6.4 Empirical application: the VaR term structure
101
smoothed over time in order to avoid frequent adjustments of the balance sheet
(which is costly for the bank) but can also react quickly enough to market news
such as crashes.
As this was done in Sect. 6.4.4 for the VaR point estimates, we calculate the
time series of diÔ¨Äerences between the regulatory capital obtained under loss L
and the SEL benchmark and then compute the average of the deviations over the
out-of-sample window. Results are reported in Table 6.2 where the table entries
are given in percent. First, we can notice that the deviations are much larger
than for the VaR point estimates; the average deviations range from 0.023% in
the case of the predictive VaR to 1.741% in the case of the Linex (a = 3), both
for the GARCH(1, 1) model with Student-t disturbances. In general, deviations
from the SEL are larger when the disturbances are Student-t distributed. The
percentage of capital obtained with the AEL function is, on average, lower than
with the SEL, indicating a left-skewed density for the regulatory capital. The
largest deviation is obtained for the Linex function with parameter a = 3; in this
case, a risk manager or regulator will keep aside a capital which is 1.741% larger
than the SEL agent, for which under- or overestimation are equally serious. In
contrast to this, a fund manager will be able to invest 0.79% more capital on
Ô¨Ånancial markets. For this special case, there is a diÔ¨Äerential of about 2.5%
in risk capital allocation between a risk manager and a fund manager; this is
substantial if we imagine the amounts invested on Ô¨Ånancial markets by Ô¨Ånancial
institutions.
Table 6.2. Average deviations of the regulatory capital
point estimates from the SEL benchmark.‚ãÜ
GARCH(1, 1) disturbances
Loss L
Normal
Student-t
Linex (a = 3)
‚àí0.446
‚àí1.741
Linex (a = ‚àí3)
0.331
0.790
AELa
0.056
0.122
Predictiveb
‚àí0.084
0.023
‚ãÜThe tables entries are given in percent. L : loss function.
a Absolute error loss.
b In the case of the predictive VaR, the point estimate is the
œÜcth percentile of the predictive density for the s-day ahead
cumulative return; œÜc .= 1 ‚àíœÜ.

102
6 Value at Risk and Decision Theory
6.4.6 Forecasting performance analysis
To test the ability of our models to capture the true VaR, we compare the real-
ization of the cumulated returns {yt,s}t with our VaR estimates for time horizon
s ‚àà{1, 5, 10} and risk level œÜ ‚àà{0.95, 0.99}. To that aim, we adopt the (back-
testing) methodology proposed by ChristoÔ¨Äersen [1998] which has become the
standard practice in Ô¨Ånancial risk management. When the forecasting horizon
is one day, this approach is based on the study of the random sequence {V œÜ
t }
where:
V œÜ
t
.=
Ô£±
Ô£≤
Ô£≥
1
if
yt+1 < VaRœÜ
t
0
else .
A sequence of VaR forecasts at risk level œÜ has correct conditional coverage if
{V œÜ
t } is an independent and identically distributed sequence of Bernoulli random
variables with parameter œÜc .= (1‚àíœÜ). In practice, this hypothesis can be veriÔ¨Åed
by testing jointly the independence on the series and the unconditional coverage
of the VaR forecasts, i.e., E(V œÜ
t ) = œÜc.
In order to test the performance of the s-day ahead VaR (s > 1), we use
a similar methodology, based now on the study of the random sequence {V œÜ
t,s}t
where:
V œÜ
t,s
.=
Ô£±
Ô£≤
Ô£≥
1
if
yt,s < VaRœÜ
t,s
0
else .
In this case however, since the cumulative returns yt,s and yœÑ,s overlap for
|t ‚àíœÑ| ‚©Ωs, the variables V œÜ
t,s and V œÜ
œÑ,s are not independent and the usual test
by ChristoÔ¨Äersen [1998] cannot be applied directly. However, we can exploit the
structure of dependence between yt,s to yœÑ,s to get rid of this diÔ¨Éculty. Indeed,
the construction of cumulative returns leads to the creation of spurious moving
average eÔ¨Äects of order (s ‚àí1) in the time series {yt,s}t. We can therefore follow
Diebold and Mariano [1995] and correct the test by ChristoÔ¨Äersen [1998] for
serial correlation via Bonferroni bounds. To that aim, we partition the series
{yt,s}t into groups for which we expect independence and correct unconditional
coverage. Under the assumption that the series {V œÜ
t,s}t is (s‚àí1) dependent, each
of the following s sub-series:

6.4 Empirical application: the VaR term structure
103
{V œÜ
1,s, V œÜ
1+s,s, V œÜ
1+2s,s, . . .}
{V œÜ
2,s, V œÜ
2+s,s, V œÜ
2+2s,s, . . .}
...
{V œÜ
s,s, V œÜ
2s,s, V œÜ
3s,s, . . .}
will be iid Bernoulli distributed if the model for the underlying process is correct.
Thus, a formal test with size bounded by Œ± can be obtained by performing
s tests, each of size Œ±/s, on each of the s sub-series, and rejecting the null
hypothesis if the null is rejected for any of the s sub-series.
Forecasting results are reported in Table 6.3 where we give the p-values
of the unconditional coverage (UC), independence (IND) as well as conditional
coverage (CC) tests; for time horizons s = 5 and s = 10, we report the lowest
p-value computed from the s series of VaR forecasts. Our results indicate that
Table 6.3. Forecasting results of the VaR point estimates.‚ãÜ
GARCH(1, 1) model with Normal disturbances
œÜ = 0.95
œÜ = 0.99
s
UC
IND
CC
UC
IND
CC
1
0.026
0.761
0.081
0.018
0.052
0.009
5
0.392
0.062
0.121
0.306
NA
NA
10
0.412
0.544
0.594
0.162
NA
NA
GARCH(1, 1) model with Student-t disturbances
œÜ = 0.95
œÜ = 0.99
s
UC
IND
CC
UC
IND
CC
1
0.222
0.903
0.470
0.572
NA
NA
5
0.392
0.050
0.101
0.344
NA
NA
10
0.983
0.280
0.558
0.162
NA
NA
‚ãÜForecasting test by ChristoÔ¨Äersen [1998] based on the SEL point
estimates. œÜ: risk level; s time horizon (in days); UC: p-value for the
correct uncoverage test; IND: p-value for the independence test; CC:
p-value for the correct conditional coverage test; NA: not applicable.
the GARCH(1, 1) model with Normal disturbances fails, at the 5% signiÔ¨Åcance
level, in forecasting the one-day ahead VaR for both risk levels. Indeed, the un-
conditional coverage gives a p-value of 0.026 for œÜ = 0.95 and 0.018 for œÜ = 0.99.
The joint test of correct unconditional coverage and independence is however
only rejected for risk level œÜ = 0.99. In contrast to this, the GARCH(1, 1) model
with Student-t disturbances performs well. For longer time horizons, the models
behave similarly well and neither the unconditional coverage nor the indepen-

104
6 Value at Risk and Decision Theory
dence tests are rejected at the 5% signiÔ¨Åcance level. We point out, however, that
the test by ChristoÔ¨Äersen [1998] is powerful when the number of observations is
large. In our context of 1‚Äô200 observations, the test of the ten-day ahead VaR
is based on ten sequences of (only) 120 observations. At risk level œÜ = 0.99,
a single violation is thus expected. The forecasting results should therefore be
taken with caution in this case.
We emphasize the fact that the test has been applied to the time series of SEL
point estimates. For comparison purposes, we have also analyzed the forecasting
performance of the alternative VaR point estimates, obtained with the Linex and
AEL functions. In all cases, the testing methodology gave similar p-values for
the diÔ¨Äerent risk levels and time horizons. This is not surprising. Indeed, the
diÔ¨Äerences between the VaR point estimates are small (we recall that the largest
deviation is -0.496%) and the test by ChristoÔ¨Äersen [1998] focuses on the number
of times the VaR is exceeded instead of testing the size of discrepancy between
predictions and realizations. In addition, the case where the diÔ¨Äerences between
point estimates are important was observed for a forecasting horizon of ten days
at risk level œÜ = 0.99, precisely the case where the power of the test is weak.
Therefore, alternative (more powerful) tests should be developed, as recently
pursued by Zumbach [2006]. In Sect. 7.6 of the next chapter, we will document
that the diÔ¨Äerences between the one-day ahead VaR point estimates are large
when the P&L dynamics is described by a Markov-switching GJR model. In this
context, the loss function of the forecaster leads to diÔ¨Äerent conclusions on the
forecasting performance of the model, even when relying on the common testing
methodology of ChristoÔ¨Äersen [1998].
6.5 The Expected Shortfall risk measure
While being now a standard tool in Ô¨Ånancial risk management, the VaR has
been criticized in the research literature for several reasons, in particular:
‚Ä¢ the VaR does not tell anything about the potential size of loss that
exceeds its level and, as a result, it is Ô¨Çawed;
‚Ä¢ the VaR is not a coherent measure of risk in the sense of Artzner,
Delbaen, Eber, and Heath [1999]. In particular, it lacks the prop-
erty of sub-additivity.
To circumvent these problems, the concept of Expected Shortfall (henceforth
ES) also known as Conditional VaR or CVaR has been introduced by Artzner
et al. [1999].

6.5 The Expected Shortfall risk measure
105
DeÔ¨Ånition 6.2 (Expected Shortfall). Let Y be a univariate random vari-
able with distribution FY , assumed continuous for simplicity. Then the Expected
Shortfall at risk level œÜ is deÔ¨Åned as the expected value of Y below the VaRœÜ
level. Formally:
ESœÜ .= E(Y | Y ‚©ΩVaRœÜ) =
R VaRœÜ
‚àí‚àû
y dFY (y)
œÜc
(6.15)
where we recall that œÜc .= (1‚àíœÜ) for convenience and VaRœÜ is given in Def. 6.1
(see p.76).
Basically, the ES risk measure is the expectation of the P&L below the VaR
level.
In the case of the GARCH(1, 1) model with Normal and Student-t distribu-
tions, the integral on the right-hand side of expression (6.15) can be calculated
explicitly given the set of parameters œà. Indeed, when the model disturbances
are Normally distributed, the one-day ahead ES at risk level œÜ, estimated at
time t, is given by:
ESœÜ
t (œà) = h1/2
t+1(Œ±, Œ≤) √ó
‚àíexp
h
‚àí
z2
œÜc
2
i
‚àö
2œÄœÜc
(6.16)
where we recall that œà .= (Œ±, Œ≤), ht+1 is the conditional variance which is com-
puted by recursion given the information set Ft and zœÜc is the œÜc percentile of
the standard Normal distribution. In the case of a Student-t distribution with
ŒΩ degrees of freedom, the one-day ahead ES at risk level œÜ, estimated at time t,
can be expressed as follows:
ESœÜ
t (œà) =

œ±(ŒΩ) √ó ht+1(Œ±, Œ≤)
1/2 √ó Œ®œÜc(ŒΩ)
(6.17)
with:
Œ®œÜc(ŒΩ) .=
Œì( ŒΩ+1
2 )
Œì( ŒΩ
2)
‚àöŒΩœÄ

ŒΩ
ŒΩ‚àí1
 
1 +
t2
œÜc(ŒΩ)
ŒΩ
 1‚àíŒΩ
2
œÜc
where in this case œà .= (Œ±, Œ≤, ŒΩ), œ±(ŒΩ) .= ŒΩ‚àí2
ŒΩ
and tœÜc is the œÜcth percentile of the
Student-t distribution. Once a joint posterior sample of the model parameters
is obtained, expressions (6.16) and (6.17) can be used to simulate the density of
the one-day ahead ES risk measure at any risk level œÜ.

106
6 Value at Risk and Decision Theory
In order to Ô¨Ånd the expression for the s-day ahead ESœÜ (s > 1), we note that
the Expected Shortfall can also be viewed as the average of VaR below the risk
level œÜc.
Proposition 6.3. Assuming that E(|Y |) < ‚àûand FY is continuous, we can
express the Expected Shortfall as follows:
E(Y | Y ‚©ΩVaRœÜ) =
R œÜc
0
VaRu du
œÜc
.
Proof. The integral in (6.15) is transformed by the change of variable
y 7‚Üíu .= FY (y), so that:
Z VaRœÜ
‚àí‚àû
y dFY (y) =
Z œÜc
0
VaRu du
since FY (‚àí‚àû) = 0, FY (VaRœÜ) = œÜc, du = dFY (y) and y = VaRu by Def. 6.1
(see p.76).
‚äì‚äî
Using Prop. 6.3, we can estimate the s-day ahead ES at any risk level œÜ by
integrating the s-day ahead VaR over the (0, œÜc] interval. Formally:
ESœÜ
t,s(œà) =
R œÜc
0
VaRu
t,s(œà)du
œÜc
(6.18)
where VaRu
t,s(œà) is calculated using approximation (6.6) on page 81. The integral
in expression (6.18) can be estimated by conventional quadrature methods. As
in the one-day ahead VaR, the joint posterior sample can be used to simulate
the density for the s-day ahead ES using formula (6.18).
In Fig. 6.6, we display the (conditional) term structure of the ES density
at risk level œÜ = 0.95 for the Ô¨Årst observation window excerpt from our data
set. As in the VaR illustration of Sect. 6.4.3, the lines give the median point
estimates and the shaded regions depict the 95% conÔ¨Ådence intervals of the ES
density. From this graph, we note that the GARCH(1, 1) model with Student-t
disturbances leads to lower median point estimates for every time horizon. In
addition, the ES uncertainty increases for both models with respect to the time
horizon. The Student-t model leads to higher uncertainty in ES at each horizon
compared to the Normal speciÔ¨Åcation. Finally, the left asymmetry of the density
is visually apparent for horizons larger than Ô¨Åve days for both models.
A comparison with the VaR term structure displayed in Fig. 6.5 (see p.96)
indicates that the ES density has heavier tails and a skewness which is more

6.5 The Expected Shortfall risk measure
107
ES term structure
(in percent)
time horizon
(in days)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
‚àí7.0
‚àí6.5
‚àí6.0
‚àí5.5
‚àí5.0
‚àí4.5
‚àí4.0
‚àí3.5
‚àí3.0
‚àí2.5
‚àí2.0
‚àí1.5
‚àí1.0
 
 
 
 
Posterior median (Normal disturbances)
Posterior median (Student‚àít disturbances)
95% CI (Normal disturbances)
95% CI (Student‚àít disturbances)
Fig. 6.6. Term structures of the ES density at risk level œÜ = 0.95 for the GARCH(1, 1)
model with Normal and Student-t disturbances. The density is based on 10‚Äô000 draws
from the joint posterior sample of the models‚Äô parameters.
pronounced. Therefore, given preferences in risk perspectives lead to larger dif-
ferences in ES point estimates.

7
Bayesian Estimation of the Markov-Switching
GJR(1, 1) Model with Student-t Innovations
(...) ‚Äúthe application of GARCH to long time series of
stock-return data will yield a high measure of persistence
because of the presence of deterministic shifts in the
unconditional variance and the subsequent failure of the
econometrician to model these shifts.‚Äù
‚Äî Christopher G. Lamoureux and William D. Lastrapes
In this chapter, we address the problem of estimating GARCH models subject
to structural changes in the parameters; namely, the Markov-switching GARCH
models (henceforth MS-GARCH). In this framework, a hidden Markov sequence
{st} with state space {1, . . . , K} allows discrete changes in the model param-
eters. Such processes have received a lot of attention in recent years as they
provide an explanation of the high persistence in volatility (i.e., nearly unit root
process for the conditional variance) observed with single-regime GARCH mod-
els [see, e.g., Lamoureux and Lastrapes 1990]. Furthermore, the MS-GARCH
models allow for a quick change in the volatility level which leads to signiÔ¨Åcant
improvements in volatility forecasts, as shown by Dueker [1997], Klaassen [2002],
Marcucci [2005].
Following the seminal work of Hamilton and Susmel [1994], diÔ¨Äerent para-
metrizations have been proposed to account for discrete changes in the GARCH
parameters [see, e.g., Dueker 1997, Gray 1996, Klaassen 2002]. However, these
parametrizations for the conditional variance process lead to computational dif-
Ô¨Åculties. Indeed, the evaluation of the likelihood function for a sample of length
T requires the integration over all KT possible paths, rendering the estima-
tion infeasible. As a remedy, approximation schemes have been proposed to
shorten the dependence on the state variable‚Äôs history. While this diÔ¨Éculty is
not present in ARCH type models, lower order GARCH speciÔ¨Åcation of the con-
ditional variance oÔ¨Äers a more parsimonious representation than higher order
ARCH models.

110
7 MS-GJR(1, 1) Model with Student-t Innovations
In order to avoid any diÔ¨Éculties related to the past inÔ¨Ånite history of the
state variable, we adopt a recent parametrization due to Haas et al. [2004]. In
their model, the authors hypothesize K separate GARCH(1, 1) processes for the
conditional variance of the MS-GARCH process {yt}. The conditional variances
at time t can be written in vector form as follows:
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
h1
t
h2
t
...
hK
t
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
.=
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œ±1
0
Œ±2
0
...
Œ±K
0
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
+
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œ±1
1
Œ±2
1
...
Œ±K
1
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
y2
t‚àí1 +
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œ≤1
Œ≤2
...
Œ≤K
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
‚äô
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
h1
t‚àí1
h2
t‚àí1
...
hK
t‚àí1
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
(7.1)
where ‚äôdenotes the Hadamard product, i.e., element-by-element multiplication.
The MS-GARCH process {yt} is then simply obtained by setting:
yt = Œµt(hst
t )1/2
where Œµt is an error term with zero mean and unit variance. The parameters Œ±k
0,
Œ±k
1 and Œ≤k are therefore the GARCH(1, 1) parameters related to the kth state of
the nature. Under this speciÔ¨Åcation, the conditional variance is solely a function
of the past data and current state st, which avoids the problem of inÔ¨Ånite history.
In the context of the Bayesian estimation, this allows to simulate the state
process in a multi-move manner which enhances the sampler‚Äôs eÔ¨Éciency.
In addition to its appealing computational aspects, the MS-GARCH model
of Haas et al. [2004] has conceptual advantages. In eÔ¨Äect, one reason for spec-
ifying Markov-switching models that allow for diÔ¨Äerent GARCH behavior in
each regime is to capture the diÔ¨Äerence in the variance dynamics in low- and
high-volatility periods. As pointed out by Haas et al. [2004, p.498]:
(...) ‚Äúa relatively large value of Œ±k
1 and relatively low values of Œ≤k in
high-volatility regimes may indicate a tendency to over-react to news,
compared to regular periods, while there is less memory in these sub-
processes. Such an interpretation requires a parametrization of Markov-
switching GARCH models that implies a clear association between the
GARCH parameters within regime k, that is Œ±k
0, Œ±k
1 and Œ≤k and the
corresponding {hk
t } process.‚Äù
The speciÔ¨Åcation of the conditional variance in equation (7.1) allows for a clear-
cut interpretation of the variance dynamics in each regime. Moreover, Haas
et al. [2004] show that results on the single-regime GARCH(1, 1) model can be
extended to their speciÔ¨Åcation; in particular, they derive explicit formulae for
the covariance stationarity condition, the unconditional variance as well as the
dependence structure of the squared process {y2
t }.

7.1 The model and the priors
111
To account for additional stylized facts observed in Ô¨Ånancial time series, es-
pecially for stock indices (see Chap. 4), we will consider an asymmetric exten-
sion of (7.1) in which the GARCH(1, 1) processes are replaced by GJR(1, 1)
processes. More precisely, in this Markov-switching GJR model (henceforth
MS-GJR), the conditional variances at time t can be written in vector form
as follows:
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
h1
t
h2
t
...
hK
t
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
.=
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œ±1
0
Œ±2
0
...
Œ±K
0
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
+
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œ±1
1
Œ±2
1
...
Œ±K
1
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
I{yt‚àí1‚©æ0} +
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œ±1
2
Œ±2
2
...
Œ±K
2
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
I{yt‚àí1<0}
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
y2
t‚àí1
+
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Œ≤1
Œ≤2
...
Œ≤K
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
‚äô
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
h1
t‚àí1
h2
t‚àí1
...
hK
t‚àí1
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
(7.2)
where I{‚Ä¢} denotes the indicator function. In this setting, the conditional vari-
ance in every regime can react asymmetrically depending on the sign of the
past shocks due to the introduction of dummy variables. The leverage eÔ¨Äect is
present for a given state k as soon as Œ±k
2 > Œ±k
1. An interesting feature of the
parametrization (7.2) lies in the fact that we can estimate whether the response
to past negative shock on the conditional variance is diÔ¨Äerent across regimes.
The plan of this chapter is as follows. We set up the model in Sect. 7.1.
The MCMC scheme is detailed in Sect. 7.2. The MS-GJR model as well as a
single-regime GJR model are applied to the Swiss Market Index log-returns in
Sect. 7.3. In Sect. 7.4, we test the models for misspeciÔ¨Åcation by using the
generalized residuals and assess the goodness-of-Ô¨Åt through the calculation of
the Deviance information criterion and the model likelihoods. In Sect. 7.5, we
test the predictive performance of the models by running a forecasting analysis
based on the VaR. In Sect. 7.6, we propose a methodology to depict the one-
day ahead VaR density and document how speciÔ¨Åc forecasters‚Äô risk perspectives
can lead to diÔ¨Äerent conclusions in terms of the forecasting performance of the
model. We conclude with some comments regarding the ML estimation of the
MS-GJR model in Sect. 7.7.
7.1 The model and the priors
A Markov-switching GJR(1, 1) model with Student-t innovations may be written
as follows:

112
7 MS-GJR(1, 1) Model with Student-t Innovations
yt = Œµt(œ±ht)1/2
for t = 1, . . . , T
Œµt
iid
‚àºS(0, 1, ŒΩ)
œ± .= ŒΩ ‚àí2
ŒΩ
ht .= e‚Ä≤
tht
(7.3)
where et is a K√ó1 vector deÔ¨Åned by et .=
 I{st=1} ¬∑ ¬∑ ¬∑ I{st=K}
‚Ä≤, I{‚Ä¢} is the indica-
tor function; the sequence {st} is assumed to be a stationary, irreducible Markov
process with discrete state space {1, . . . , K} and transition matrix P
.= [Pij]
where Pij
.= P(st+1 = j | st = i); S(0, 1, ŒΩ) denotes the standard Student-t
density with ŒΩ degrees of freedom and œ± is a scaling factor which ensures that
the conditional variance is given by ht. Moreover, we deÔ¨Åne the K √ó 1 vector of
GJR(1, 1) conditional variances in a compact form as follows:
ht .= Œ±0 + (Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0})y2
t‚àí1 + Œ≤ ‚äôht‚àí1
where ht .= (h1
t ¬∑ ¬∑ ¬∑ hK
t )‚Ä≤, Œ±j .= (Œ±1
j ¬∑ ¬∑ ¬∑ Œ±K
j )‚Ä≤ for j = 0, 1, 2 and Œ≤ .= (Œ≤1 ¬∑ ¬∑ ¬∑ Œ≤K)‚Ä≤.
In addition, we require that Œ±0 > 0, Œ±1 ‚©æ0, Œ±2 ‚©æ0 and Œ≤ ‚©æ0, where 0
is a K √ó 1 vector of zeros, in order to ensure the positivity of the conditional
variance in every regime and set h0 .= 0 and y0 .= 0 for convenience.
The use of a Student-t instead of a Normal distribution is quite popu-
lar in standard single-regime GARCH literature. For regime-switching mod-
els, a Student-t distribution might be seen as superÔ¨Çuous since the switching
regime can account for large unconditional kurtosis in the data [see, e.g., Haas
et al. 2004]. However, as empirically observed by Klaassen [2002], allowing for
Student-t innovations within regimes can enhance the stability of the states and
allows to focus on the conditional variance‚Äôs behavior instead of capturing some
outliers. Moreover, the Student-t distribution includes the Normal distribution
as the limiting case where the degrees of freedom parameter goes to inÔ¨Ånity. We
have therefore an additional Ô¨Çexibility in the modeling and can impose Normal-
ity by constraining the lower boundary for the degrees of freedom parameter
through the prior distribution.
As pointed out in Sect. 5.1, the Student-t speciÔ¨Åcation (7.3) needs to be
re-expressed to perform a convenient Bayesian estimation. This is achieved as
follows:
yt = Œµt(œñtœ±ht)1/2
for t = 1, . . . , T
Œµt
iid
‚àºN(0, 1)
œñt
iid
‚àºIG
ŒΩ
2, ŒΩ
2


7.1 The model and the priors
113
where N(0, 1) is the standard Normal and IG denotes the Inverted Gamma
density. The degrees of freedom parameter ŒΩ characterizes the density of œñt as
follows:
p(œñt | ŒΩ) =
ŒΩ
2
 ŒΩ
2 h
Œì
ŒΩ
2
i‚àí1
œñ
‚àíŒΩ
2 ‚àí1
t
exp

‚àíŒΩ
2œñt

.
(7.4)
For a parsimonious expression of the likelihood function, we deÔ¨Åne the T √ó1
vectors y .= (y1 ¬∑ ¬∑ ¬∑ yT )‚Ä≤, œñ .= (œñ1 ¬∑ ¬∑ ¬∑ œñT )‚Ä≤ as well as s .= (s1 ¬∑ ¬∑ ¬∑ sT )‚Ä≤ and regroup
the ARCH parameters into the 3K √ó 1 vector Œ± .= (Œ±‚Ä≤
0 Œ±‚Ä≤
1 Œ±‚Ä≤
2)‚Ä≤. The model pa-
rameters are then regrouped into the augmented set of parameters Œò .= (œà, œñ, s)
where œà .= (Œ±, Œ≤, ŒΩ, P). Finally, we deÔ¨Åne the T √ó T diagonal matrix:
Œ£ .= Œ£(Œò) = diag
 {œñtœ± e‚Ä≤
tht}T
t=1

where we recall that œ±, et and ht are both functions of the model parameters,
respectively given by:
œ±(ŒΩ) .= ŒΩ ‚àí2
ŒΩ
et(st) .=
 I{st=1} ¬∑ ¬∑ ¬∑ I{st=K}
‚Ä≤
and:
ht(Œ±, Œ≤) .= Œ±0 + (Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0})y2
t‚àí1 + Œ≤ ‚äôht‚àí1(Œ±, Œ≤) .
We can now express the likelihood function of Œò as follows:
L(Œò | y) ‚àù(det Œ£)‚àí1/2 exp

‚àí1
2y‚Ä≤Œ£‚àí1y

.
(7.5)
In the Bayesian approach, the vector of hidden states is considered as a param-
eter as implied by expression (7.5).
The likelihood function (7.5) is invariant with respect to relabeling the states
(i.e., the labeling of the states can be interchanged without aÔ¨Äecting the like-
lihood value), which leads to a lack of identiÔ¨Åcation of the state-speciÔ¨Åc pa-
rameters. So, without a prior inequality restriction on some state-speciÔ¨Åc pa-
rameters, a multimodal posterior is obtained and is diÔ¨Écult to interpret and
summarize. To overcome this problem, we make use of the permutation sampler
of Fr¬®uhwirth-Schnatter [2001b] to Ô¨Ånd suitable identiÔ¨Åcation constraints. The
permutation sampler requires priors that are labeling invariant. Furthermore,
we cannot be completely non-informative about the state speciÔ¨Åc parameters
since, from a theoretical viewpoint, this would result in improper posteriors [see

114
7 MS-GJR(1, 1) Model with Student-t Innovations
Diebolt and Robert 1994]. These points have therefore to be taken into account
when choosing the prior densities.
Conditionally on the K √óK transition probabilities matrix P .= [Pij] where:
Pij .= P(st+1 = j | st = i)
the prior on vector s is Markov:
p(s | P) = œÄ(s1)
K
Y
i=1
K
Y
j=1
P Nij
ij
where Nij .= #{st+1 = j | st = i} is the number of one-step transitions from
state i to j in the T √ó 1 vector s. The mass function for the initial state, œÄ(s1),
is obtained by calculating the ergodic probabilities of the Markov chain. The
vector of ergodic probabilities can be obtained as the sum of the columns of
matrix (A‚Ä≤A)‚àí1, where the matrix A is deÔ¨Åned as follows:
A .=
 
IK ‚àíP ‚Ä≤
Œπ‚Ä≤
K
!
where IK is a K√óK identity matrix and ŒπK a K√ó1 vector of ones [see Hamilton
1994, Sect.22.2].
The prior density for the K √óK transition matrix P is obtained by assuming
that the K rows are independent and that the density of the ith row is Dirichlet
with parameter Œ∑i
.= (Œ∑i1 ¬∑ ¬∑ ¬∑ Œ∑iK):
p(P) =
K
Y
i=1
D(Œ∑i)
‚àù
K
Y
i=1
K
Y
j=1
P Œ∑ij‚àí1
ij
.
Due to the labeling invariance assumption, we require that Œ∑ii
.= Œ∑p for
i = 1, . . . , K and Œ∑ij
.= Œ∑q for i, j ‚àà{1, . . . , K; i Ã∏= j}. A prior density with
Œ∑p > Œ∑q could be used to model the belief that the probability of persistence is
bigger than the probability of transition.
For the scedastic function‚Äôs parameters Œ± and Œ≤, we use truncated Normal
densities:
p(Œ±) ‚àùN3K(Œ± | ¬µŒ±, Œ£Œ±)I{Œ±>0}
p(Œ≤) ‚àùNK(Œ≤ | ¬µŒ≤, Œ£Œ≤)I{Œ≤>0}

7.2 Simulating the joint posterior
115
where we recall that ¬µ‚Ä¢ and Œ£‚Ä¢ are the hyperparameters, 0 is a vector of ze-
ros of appropriate size and Nd is the d-dimensional Normal density (d > 1).
The assumption of labeling invariance is fulÔ¨Ålled if we assume further that the
hyperparameters are the same for all states. In particular, we set:
[¬µŒ±]i
.= ¬µŒ±0 ,
[Œ£Œ±]ii
.= œÉ2
Œ±0
,

¬µŒ≤

i
.= ¬µŒ≤
,
[Œ£Œ≤]ii
.= œÉ2
Œ≤
for i = 1, . . . , K, and:
[¬µŒ±]i
.= ¬µŒ±1
,
[Œ£Œ±]ii
.= œÉ2
Œ±1
for i = K + 1, . . . , 2K, and:
[¬µŒ±]i
.= ¬µŒ±2
,
[Œ£Œ±]ii
.= œÉ2
Œ±2
for i = 2K + 1, . . . , 3K, where ¬µŒ±j, œÉ2
Œ±j (j = 0, 1, 2), and ¬µŒ≤, œÉ2
Œ≤ are Ô¨Åxed
hyperparameters. We note that matrices Œ£Œ± and Œ£Œ≤ are diagonal in this case.
The prior density of the T √ó 1 vector œñ conditional on ŒΩ is found by noting
that œñt are independent and identically distributed from (7.4), which yields:
p(œñ | ŒΩ) =
ŒΩ
2
 T ŒΩ
2 h
Œì
ŒΩ
2
i‚àíT
 T
Y
t=1
œñt
!‚àíŒΩ
2 ‚àí1
exp
"
‚àí1
2
T
X
t=1
ŒΩ
œñt
#
.
Following Deschamps [2006], we choose a translated Exponential with pa-
rameters Œª > 0 and Œ¥ ‚©æ2 for the degrees of freedom parameter:
p(ŒΩ) = Œª exp[‚àíŒª(ŒΩ ‚àíŒ¥)]I{Œ¥<ŒΩ<‚àû} .
Finally, we form the joint prior by assuming prior independence between Œ±,
Œ≤, (œñ, ŒΩ) and (s, P) as follows:
p(Œò) = p(Œ±)p(Œ≤)p(œñ | ŒΩ)p(ŒΩ)p(s | P)p(P)
and by combining the likelihood function (7.5) with the joint prior above, we
obtain the posterior density via Bayes‚Äô rule:
p(Œò | y) ‚àùL(Œò | y)p(Œò) .
(7.6)
7.2 Simulating the joint posterior
We draw an initial value:

116
7 MS-GJR(1, 1) Model with Student-t Innovations
Œò[0] .= (Œ±[0], Œ≤[0], œñ[0], ŒΩ[0], s[0], P [0])
from the joint prior and we generate iteratively J passes for Œò. A single pass is
decomposed as follows:
s[j] ‚àºp(s | Œ±[j‚àí1], Œ≤[j‚àí1], œñ[j‚àí1], ŒΩ[j‚àí1], P [j‚àí1], y)
P [j] ‚àºp(P | s[j])
Œ±[j] ‚àºp(Œ± | Œ≤[j‚àí1], œñ[j‚àí1], ŒΩ[j‚àí1], s[j], y)
Œ≤[j] ‚àºp(Œ≤ | Œ±[j], œñ[j‚àí1], ŒΩ[j‚àí1], s[j], y)
œñ[j] ‚àºp(œñ | Œ±[j], Œ≤[j], ŒΩ[j‚àí1], s[j], y)
ŒΩ[j] ‚àºp(ŒΩ | œñ[j]) .
(7.7)
In (7.7), only œñ and P can be simulated from known expressions. Draws of Œ±
and Œ≤ are achieved by a multivariate extension of the methodology proposed by
Nakatsuma [1998, 2000]. The generation of state vector s is made by using the
Forward Filtering Backward Sampling (henceforth FFBS) algorithm described
in Chib [1996]. Finally, sampling ŒΩ is achieved by an eÔ¨Écient rejection technique.
As pointed out previously, the likelihood function and the joint prior are
labeling invariant. Consequently, the joint posterior density in (7.6) will also be
invariant and hence exhibit, at least theoretically, K! diÔ¨Äerent modes. Therefore,
it is important to carefully select constraints to identify the model. In eÔ¨Äect, a
constraint that ignores the geometry of the posterior density will not lead to
a unique labeling and can introduce a bias toward the constraint, as shown in
Fr¬®uhwirth-Schnatter [2001b]. If a suitable identifying restriction is not available
or is not known a priori, an elegant solution to determine these constraints
is to use the random permutation sampler proposed by Fr¬®uhwirth-Schnatter
[2001b]. In this version of the permutation sampler, each pass of the MCMC
scheme is followed by a random permutation of the regime deÔ¨Ånitions. Formally,
a random permutation {Œ†1, . . . Œ†K} of {1, . . . , K} is selected with probability
1
K!. Then, for i, j ‚àà{1, . . . , K}, the element (i, j) of P is replaced by the element
with indices (Œ†i, Œ†j). The hidden states process {st} is substituted by {Œ†st}.
Finally, for k = 1, . . . , K, parameter Œ±k
0 is replaced by Œ±Œ†k
0 , parameter Œ±k
1 by
Œ±Œ†k
1 , parameter Œ±k
2 by Œ±Œ†k
2
and parameter Œ≤k by Œ≤Œ†k. Hence, relabeling only
aÔ¨Äects the scedastic function‚Äôs parameters, the state process and the transition
probabilities while the vector œñ and the degrees of freedom parameter ŒΩ remain
unchanged.
The random permutation sampler by Fr¬®uhwirth-Schnatter [2001b] is used to
improve the mixing of the MCMC sampler and to explore the full unconstrained
parameter space. Then post-processing the MCMC output of the random per-

7.2 Simulating the joint posterior
117
mutation sampler in an exploratory way, by plotting scatter plots for instance,
can suggest an appropriate identiÔ¨Åcation constraint, such as:
Œ≤1 < . . . < Œ≤K
(7.8)
meaning, in this particular case, that the MS-GJR model can be identiÔ¨Åed
through inequalities on the parameter Œ≤ between regimes. At this stage, the
model is estimated again under the constraint (7.8) by enforcing the corre-
sponding permutation of the regimes. This version of the permutation sampler
is referred to as the constrained permutation sampler. At each sweep of the
sampler, we test whether the constraint is fulÔ¨Ålled. If not, we order the pairs
{1, Œ≤1}, . . . , {K, Œ≤K} with respect to the second component. The Ô¨Årst component
{Œ†1, . . . , Œ†K} of the ordered pairs deÔ¨Ånes the correct permutation of reordering
the state parameters and this permutation is applied to the state-speciÔ¨Åc com-
ponents, as this was done for the random permutation sampler. If the model is
identiÔ¨Åable up to permutations of the states and satisÔ¨Åes certain regularity con-
ditions, the constrained posterior density will exhibit a single mode. Note that
the selection of the constraint (7.8) is arbitrary, because there exist K! diÔ¨Äerent
ways of formulating constraints which render the model identiÔ¨Åed, namely:
Œ≤Œ†1 < . . . < Œ≤Œ†K
for all permutations {Œ†1, . . . , Œ†K} of {1, . . . , K}. At this stage, if label switch-
ing still occurs, this might indicate that the inequality restriction (7.8) is not
well suited or that the number K of chosen regimes is too large [see Fr¬®uhwirth-
Schnatter 2006, Sect.4.2]. We will now present the derivation for the full condi-
tionals appearing in the MCMC scheme (7.7).
7.2.1 Generating vector s
The generation of posterior samples for the T √ó 1 vector s is carried out in
a multi-move manner by using the FFBS algorithm. We refer the reader to
Chib [1996] and Fr¬®uhwirth-Schnatter [2006, Chap.11] for a detailed presenta-
tion of this procedure. We mention however that the FFBS approach can be
used since the conditional density of yt only depends on the current regime
which is a consequence of the deÔ¨Ånition for the conditional variance ht .= e‚Ä≤
tht.
Other speciÔ¨Åcations for the conditional variance in Gray [1996] or Klaassen
[2002] for instance, do not allow for such an approach, as noted in Kaufmann
and Fr¬®uhwirth-Schnatter [2002, Sect.6.3]. The application of the FFBS algo-
rithm has the potential advantage that the states are updated as a single block,

118
7 MS-GJR(1, 1) Model with Student-t Innovations
which avoids superÔ¨Çuous correlation in the vector‚Äôs components, and therefore
enhances the sampler‚Äôs eÔ¨Éciency [see Fr¬®uhwirth-Schnatter 2006, Sect.11.5.6].
7.2.2 Generating matrix P
The full conditional density of the transition matrix can be derived without
regard to the sampling model since P becomes independent of Œò and y given
the vector of states. Indeed, the posterior density is obtained as follows:
p(P | s) ‚àùp(s | P)p(P)
‚àù
Ô£´
Ô£≠œÄ(s1)
K
Y
i=1
K
Y
j=1
P Nij
ij
Ô£∂
Ô£∏√ó
Ô£´
Ô£≠
K
Y
i=1
K
Y
j=1
P Œ∑ij‚àí1
ij
Ô£∂
Ô£∏
‚àù
K
Y
i=1
K
Y
j=1
P Nij+Œ∑ij‚àí1
ij
‚àù
K
Y
i=1
D(bŒ∑i)
(7.9)
where bŒ∑i
.= (Ni1 +Œ∑i1 ¬∑ ¬∑ ¬∑ NiK +Œ∑iK) and Nij .= #{st+1 = j | st = i} is the total
number of one-step transitions from state i to state j in the vector s. The rows
of matrix P are independent a posteriori and the ith row follows a Dirichlet
density with parameter bŒ∑i.
7.2.3 Generating the GJR parameters
The methodology used to draw vectors Œ± and Œ≤ can be viewed as a multivariate
extension of the approach proposed in Chap. 4 for the single-regime GJR model.
Let us consider the following K √ó 1 vector:
wt .= y2
t ŒπK
œÑt
‚àíht
where we deÔ¨Åne œÑt .= œñtœÅ for convenience and recall that ŒπK is a K √ó1 vector of
ones. In order to simplify the notations further, we deÔ¨Åne vt .= y2
t
œÑt which yields
wt = vtŒπK ‚àíht. From there, we can transform the expression for the vector of
conditional variances as follows:

7.2 Simulating the joint posterior
119
ht = Œ±0 + (Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0})y2
t‚àí1 + Œ≤ ‚äôht‚àí1
‚áî(vtŒπK ‚àíwt) = Œ±0 + (Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0})y2
t‚àí1
+ Œ≤ ‚äô(vt‚àí1ŒπK ‚àíwt‚àí1)
‚áîvtŒπK = Œ±0 +

œÑt‚àí1(Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0}) + Œ≤

‚äôvt‚àí1ŒπK
‚àíŒ≤ ‚äôwt‚àí1 + wt
‚áîwt = vtŒπK ‚àíŒ±0 ‚àí

œÑt‚àí1(Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0}) + Œ≤

‚äôvt‚àí1ŒπK
+ Œ≤ ‚äôwt‚àí1 .
Moreover, let us deÔ¨Åne wt .= e‚Ä≤
twt and note that wt can be written as follows:
wt .= e‚Ä≤
twt
= vt ‚àíht =

y2
t
œñtœ±ht
‚àí1

ht
=
 œá2
1 ‚àí1

ht
where œá2
1 denotes a Chi-squared variable with one degree of freedom. This comes
from the fact that the conditional distribution of yt is Normal with zero mean
and variance œñtœ±ht. Therefore, the conditional mean of wt is zero and the con-
ditional variance is 2h2
t. As in the single-regime GJR model, this variable can
be approximated by zt, a Normal variable with a mean of zero and a variance of
2h2
t. The variable zt can be further expressed as zt .= e‚Ä≤
tzt where zt is a function
of vectors Œ± and Œ≤ given by:
zt(Œ±, Œ≤) = vtŒπK ‚àíŒ±0 ‚àí

œÑt‚àí1(Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0}) + Œ≤

‚äôvt‚àí1ŒπK
+ Œ≤ ‚äôzt‚àí1(Œ±, Œ≤) .
(7.10)
Then, we construct the T √ó 1 vector z .= (z1 ¬∑ ¬∑ ¬∑ zT )‚Ä≤ where zt .= e‚Ä≤
tzt as well as
the T √ó T diagonal matrix:
Œõ .= Œõ(Œ±, Œ≤) = diag
 {2e‚Ä≤
th2
t(Œ±, Œ≤)}T
t=1

and express the approximate likelihood function of (Œ±, Œ≤) as follows:
L(Œ±, Œ≤ | œñ, ŒΩ, s, y) ‚àù(det Œõ)‚àí1/2 exp

‚àí1
2z‚Ä≤Œõ‚àí1z

.
(7.11)
As will be shown hereafter, the construction of the proposal densities for pa-
rameters Œ± and Œ≤ is based on this likelihood function.

120
7 MS-GJR(1, 1) Model with Student-t Innovations
Generating vector Œ±
First, we note that the function zt(Œ±, Œ≤) in (7.10) can be expressed as a lin-
ear function of vector Œ±. To show this, we simply extend the argument of the
single-regime GJR model by using appropriate recursive transformations. More
precisely, the ith component of the K √ó 1 vector zt can be written as follows:
[zt]i = vt ‚àí
 l‚àó
t (Œ≤i) v‚àó
t (Œ≤i) v‚àó‚àó
t (Œ≤i)

Ô£´
Ô£¨
Ô£≠
Œ±i
0
Œ±i
1
Œ±i
2
Ô£∂
Ô£∑
Ô£∏
with the recursive transformations l‚àó
t , v‚àó
t and v‚àó‚àó
t
given by:
l‚àó
t (Œ≤i) .= 1 + Œ≤il‚àó
t‚àí1(Œ≤i)
v‚àó
t (Œ≤i) .= y2
t‚àí1I{yt‚àí1‚©æ0} + Œ≤iv‚àó
t‚àí1(Œ≤i)
v‚àó‚àó
t (Œ≤i) .= y2
t‚àí1I{yt‚àí1<0} + Œ≤iv‚àó‚àó
t‚àí1(Œ≤i)
(7.12)
where l‚àó
0 = v‚àó
0 = v‚àó‚àó
0
.= 0. We notice that l‚àó
t (‚Ä¢), v‚àó
t (‚Ä¢) and v‚àó‚àó
t (‚Ä¢) in (7.12) are
similar to the recursive transformations used for the single-regime GJR model.
Let us now regroup the recursive values into a K √ó 3K matrix Ct as follows:
Ct .=
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
l‚àó
t (Œ≤1)
0
¬∑¬∑¬∑
0
v‚àó
t (Œ≤1)
0
¬∑¬∑¬∑
0
v‚àó‚àó
t (Œ≤1)
0
¬∑¬∑¬∑
0
0
l‚àó
t (Œ≤2)
0
...
0
v‚àó
t (Œ≤2)
0
...
0
v‚àó‚àó
t (Œ≤2)
0
...
...
0
...
0
...
0
...
0
...
0
...
0
0
¬∑¬∑¬∑
0
l‚àó
t (Œ≤K)
0
¬∑¬∑¬∑
0
v‚àó
t (Œ≤K)
0
¬∑¬∑¬∑
0
v‚àó‚àó
t (Œ≤K)
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏.
It is straightforward to show that zt = vtŒπK ‚àíCtŒ±, and since zt .= e‚Ä≤
tzt we
get zt = vt ‚àíe‚Ä≤
tCtŒ±. Then, by deÔ¨Åning the T √ó 1 vectors z .= (z1 ¬∑ ¬∑ ¬∑ zT )‚Ä≤ and
v .= (v1 ¬∑ ¬∑ ¬∑ vT )‚Ä≤ as well as the T √ó 3K matrix C whose tth row is e‚Ä≤
tCt, we
end up with z = v ‚àíCŒ± which is the desired linear expression for z. The
proposal density to sample vector Œ± is obtained by combining the approximate
likelihood (7.11) and the prior density by Bayes‚Äô update:
qŒ±(Œ± | eŒ±, Œ≤, œñ, ŒΩ, s, y) ‚àùN3K(Œ± | b¬µŒ±, bŒ£Œ±)I{Œ±>0}
(7.13)
with:
bŒ£‚àí1
Œ±
.= C‚Ä≤eŒõ‚àí1C + Œ£‚àí1
Œ±
b¬µŒ±
.= bŒ£Œ±(C‚Ä≤eŒõ‚àí1v + Œ£‚àí1
Œ± ¬µŒ±)

7.2 Simulating the joint posterior
121
where the T √ó T diagonal matrix eŒõ .= diag
 {2e‚Ä≤
th2
t(eŒ±, Œ≤)}T
t=1

and eŒ± is the
previous draw of Œ± in the M-H sampler. A candidate Œ±‚ãÜis sampled from this
proposal density and accepted with probability:
min
p(Œ±‚ãÜ, Œ≤, œñ, ŒΩ, s, P | y)
p(eŒ±, Œ≤, œñ, ŒΩ, s, P | y)
qŒ±(eŒ± | Œ±‚ãÜ, Œ≤, œñ, ŒΩ, s, y)
qŒ±(Œ±‚ãÜ| eŒ±, Œ≤, œñ, ŒΩ, s, y), 1

.
Generating vector Œ≤
The function zt(Œ±, Œ≤) in (7.10) could be expressed, in the previous section, as
a linear function of Œ± but cannot be expressed as a linear function of vector Œ≤.
To overcome this problem, we linearize the K √ó 1 vector zt(Œ≤) by a Ô¨Årst order
Taylor expansion at point eŒ≤:
zt(Œ≤) ‚âÉzt(eŒ≤) + dzt
dŒ≤‚Ä≤

Œ≤=eŒ≤
√ó (Œ≤ ‚àíeŒ≤)
where eŒ≤ is the previous draw of Œ≤ in the M-H sampler. Furthermore, let us
deÔ¨Åne the following:
rt .= zt(eŒ≤) + GteŒ≤
,
Gt .= ‚àídzt
dŒ≤‚Ä≤

Œ≤=eŒ≤
(7.14)
where the K √ó K matrix Gt can be computed by the following recursion:
Gt .= vt‚àí1IK ‚àíZt‚àí1 + Gt‚àí1eŒ≤
where Zt‚àí1 is a K √ó K diagonal matrix with zt‚àí1(eŒ≤) in its diagonal, IK is a
K √ó K identity matrix and G0 is a K √ó K matrix of zeros. This recursion is
simply obtained by diÔ¨Äerentiating (7.10) with respect to Œ≤. From the deÔ¨Ånitions
in (7.14) we get zt ‚âÉrt ‚àíGtŒ≤ and the approximation for zt is obtained as
zt ‚âÉrt‚àíe‚Ä≤
tGtŒ≤ where rt .= e‚Ä≤
trt. Let us now deÔ¨Åne the T√ó1 vector r .= (r1 ¬∑ ¬∑ ¬∑ rT )‚Ä≤
as well as the T √óK matrix G whose tth row is e‚Ä≤
tGt. It turns out that z ‚âÉr‚àíGŒ≤,
thus we can approximate the exponential of the approximate likelihood (7.11)
with:
exp

‚àí1
2(r ‚àíGŒ≤)‚Ä≤Œõ‚àí1(r ‚àíGŒ≤)

.
The proposal density to sample vector Œ≤ is obtained by combining this approx-
imation with the prior density by Bayes‚Äô update:
qŒ≤(Œ≤ | Œ±, eŒ≤, œñ, ŒΩ, s, y) ‚àùNK(Œ≤ | b¬µŒ≤, bŒ£Œ≤)I{Œ≤>0}
(7.15)
with:

122
7 MS-GJR(1, 1) Model with Student-t Innovations
bŒ£‚àí1
Œ≤
.= G‚Ä≤eŒõ‚àí1G + Œ£‚àí1
Œ≤
b¬µŒ≤ .= bŒ£Œ≤(G‚Ä≤eŒõ‚àí1r + Œ£‚àí1
Œ≤ ¬µŒ≤)
where the T √ó T diagonal matrix eŒõ .= diag
 {2e‚Ä≤
th2
t(Œ±, eŒ≤)}T
t=1

. A candidate Œ≤‚ãÜ
is sampled from this proposal density and accepted with probability:
min
(
p(Œ±, Œ≤‚ãÜ, œñ, ŒΩ, s | y)
p(Œ±, eŒ≤, œñ, ŒΩ, s | y)
qŒ≤(eŒ≤ | Œ±, Œ≤‚ãÜ, œñ, ŒΩ, s, y)
qŒ≤(Œ≤‚ãÜ| Œ±, eŒ≤, œñ, ŒΩ, s, y)
, 1
)
.
7.2.4 Generating vector œñ
The components of œñ are independent a posteriori and the full conditional
posterior of œñt is obtained as follows:
p(œñt | Œ±, Œ≤, ŒΩ, s, y) ‚àùL(Œò | y)p(œñt | ŒΩ)
‚àùœñ
‚àí(ŒΩ+3)
2
t
exp

‚àíbt
œñt

(7.16)
with:
bt .= 1
2
 y2
t
œ±ht
+ ŒΩ

where we recall that ht .= e‚Ä≤
tht(Œ±, Œ≤) and œ± .=
ŒΩ‚àí2
ŒΩ . Expression (7.16) is the
kernel of an Inverted Gamma density with parameters ŒΩ+1
2
and bt.
7.2.5 Generating parameter ŒΩ
Draws from p(ŒΩ | œñ) are made by optimized rejection sampling from a trans-
lated Exponential source density. This is achieved by following the lines of
Sect. 5.2.4.
Finally, we note that the computer code and the correctness of the algorithm
are tested as in previous chapters; the testing methodology is applicable to the
constrained as well as unconstrained versions of the permutation sampler.
7.3 An application to the Swiss Market Index
We apply our Bayesian estimation method to demeaned daily log-returns {yt} of
the Swiss Market Index (henceforth SMI). The sample period is from November
12, 1990, to December 16, 2005 for a total of 3‚Äô800 observations and the log-
returns are expressed in percent. The data set is freely available from the website

7.3 An application to the Swiss Market Index
123
http://www.finance.yahoo.com. Note that September 11, 2001, has not been
recorded by the data provider since the stock markets closed after the terrorist
attacks for a few days. From this time series, the Ô¨Årst 2‚Äô500 observations (up
to November 2001), which represent slightly less than two third of the data set,
are used to estimate the model while the remaining 1‚Äô300 log-returns are used
in a forecasting performance analysis.
The time series under investigation is plotted in the upper part of Fig. 7.1
where the vertical line delimits the in- and out-of-sample observation windows.
We test for autocorrelation in the times series by testing the joint nullity of au-
toregressive coeÔ¨Écients for {yt}. We estimate the regression with autoregressive
coeÔ¨Écients up to lag 15 and compute the covariance matrix using the White
estimate. The p-value of the Wald test is 0.5299 which does not support the
presence of autocorrelation. When testing for the autocorrelation in the series
of squared observations {y2
t }, we strongly reject the absence of autocorrelation.
This is in line with the autocorrelogram of {y2
t } plotted in the lower part of
Fig. 7.1. The autocorrelations are large and signiÔ¨Åcantly diÔ¨Äerent from zero up
to lag 70. As an additional data analysis, we test for unit root using the test by
Phillips and Perron [1988]. The test strongly rejects the I(1) hypothesis.
We estimate the single-regime GJR(1, 1) model as well as the two-state
Markov-switching GJR(1, 1) model henceforth referred to as GJR and MS-GJR
for convenience. Both models are estimated using the MCMC scheme presented
in Sect. 7.2. The estimation of the GJR model is obtained as a simpliÔ¨Åed ver-
sion of the algorithm when K = 1 by setting the T √ó1 vector s to a vector of ones
and omitting the generation of the transition matrix. For the hyperparameters
on priors p(Œ±) and p(Œ≤), we set ¬µŒ±i(i = 0, 1, 2) and ¬µŒ≤ to zero mean vectors and
choose diagonal covariance matrices for Œ£Œ±i(i = 0, 1, 2) and Œ£Œ≤. The variances
are set to œÉ2
Œ±i = œÉ2
Œ≤ = 10‚Äô000 (i = 0, 1, 2) so we do not introduce tight prior
information in our estimation. In the case of the prior on the degrees of freedom
parameter, we choose Œª = 0.01 and Œ¥ = 2; this therefore ensures the existence
for the conditional variance. Finally, the hyperparameters for the prior on the
transition probabilities are set to Œ∑ii = 2 and Œ∑ij = Œ∑ji = 1 for i, j ‚àà{1, 2} so
that we have a prior belief that the probabilities of persistence are bigger than
the probabilities of transition.
For both models, we run two chains for 50‚Äô000 iterations each and assess the
convergence of the sampler by using the diagnostic test by Gelman and Rubin
[1992]. The convergence appears rather quickly, but we nevertheless consider the
Ô¨Årst half of the iterations as a burn-in phase for precaution. For the GJR model,
the acceptance rates range from 88% for vector Œ± to 97% for Œ≤ indicating that
the proposal densities are close to the exact conditional posteriors. The one-

124
7 MS-GJR(1, 1) Model with Student-t Innovations
‚àí7.5
‚àí5.0
‚àí2.5
0.0
2.5
5.0
7.5
1991
1993
1995
1997
1999
2001
2003
2005
Daily log‚àíreturns
(in percent)
year
0
20
40
60
80
100
0.0
0.1
0.2
0.3
0.4
Sample autocorrelogram
lag
Fig. 7.1. SMI daily log-returns (upper graph) and sample autocorrelogram of the
squared log-returns up to lag 100 (lower graph). The vertical line in the upper graph
delimits the in-sample and out-of-sample observation windows.
lag autocorrelations in the chain range from 0.52 for Œ±1 to 0.96 for Œ≤ which is
reasonable. For the MS-GJR model, the random permutation sampler is run Ô¨Årst
to determine suitable identiÔ¨Åcation constraints. In Fig. 7.2, we show the contour

7.3 An application to the Swiss Market Index
125
plots of the posterior density for (Œ≤k, Œ±k
0), (Œ≤k, Œ±k
1) and (Œ≤k, Œ±k
2), respectively.
Note that the state value k is arbitrary since all marginal densities contain
the same information [see Fr¬®uhwirth-Schnatter 2001b]. As we can notice, the
bimodality of the posterior density is clear for the parameter Œ≤k on the three
graphs, suggesting a constraint of the type Œ≤1 < Œ≤2 for identiÔ¨Åcation. Therefore,
the model is estimated again by imposing this constraint at each sweep in the
sampler and the deÔ¨Ånition of the states is permuted if the constraint is violated.
In that case, label switching only appeared 16 times after the burn-in phase thus
conÔ¨Årming the suitability of the identiÔ¨Åcation constraint. The acceptance rates
obtained with the constrained version of the permutation sampler range from
22% for the vector Œ± to 93% for Œ≤. The one-lag autocorrelations range from 0.82
for Œ±2
1 to 0.97 for Œ≤2. We keep every Ô¨Åfth draw from the MCMC output for both
models in order to diminish the autocorrelation in the chains. The two chains
are then merged to get a Ô¨Ånal sample of length 10‚Äô000. Finally, we note that
a three-state Markov-switching GJR model has also been estimated. However,
post-processing the MCMC output has not allowed to Ô¨Ånd a clear identiÔ¨Åcation
constraint.
The posterior statistics for both models are reported in Table 7.1. In the
case of the GJR model (upper panel), we note the high persistence for the con-
ditional variance process, measured by Œ± + Œ≤ where Œ± .= Œ±1+Œ±2
2
, as well as the
presence of the leverage eÔ¨Äect. The estimation of the probability P(Œ±2 > Œ±1 | y)
is 0.999, supporting the asymmetric behavior of the conditional variance. The
low value for the estimated degrees of freedom parameter indicates conditional
leptokurtosis in the data set. In the MS-GJR case (lower panel), we note also
the presence of the leverage eÔ¨Äect in both states. A comparison of the scedastic
function‚Äôs parameters between regimes indicates similar 95% conÔ¨Ådence intervals
for the components of the vectors Œ±1 and Œ±2 while the diÔ¨Äerence for compo-
nents of the Œ±0 vector is more pronounced. Indeed, for i = 0, 1, 2, the estimated
probabilities P(Œ±1
i > Œ±2
i | y) are respectively 0.774, 0.397 and 0.543. As in the
single-regime model, the posterior density for the degrees of freedom parameter
indicates conditional leptokurtosis. We note however that the posterior mean
and median are larger than for the GJR model. The posterior means for proba-
bilities p11 and p22 are respectively 0.997 and 0.995 indicating infrequent mixing
between states. Finally, the ineÔ¨Éciency factors (IF) reported in the last column
of Table 7.1 indicate that using 10‚Äô000 draws out of the MCMC sampler seems
appropriate if we require that the Monte Carlo error in estimating the mean
is smaller than one percent of the variation of the error due to the data. We
recall that the IF are computed as the ratio of the squared numerical standard
error (NSE) of the MCMC simulations and the variance estimate divided by the

126
7 MS-GJR(1, 1) Model with Student-t Innovations
number of iterations (i.e., the variance of the sample mean from a hypothetical
iid sampler). The NSE are estimated by the method of Andrews [1991], using a
Parzen kernel and AR(1) pre-whitening as presented in Andrews and Monahan
[1992]. As noted by Deschamps [2006], this ensures easy, optimal, and automatic
bandwidth selection.
In Fig. 7.3, we display the marginal posterior densities for the MS-GJR
model parameters. First, we note that the use of the constrained permutation
sampler leads to marginal densities which are unimodal. Furthermore, we clearly
notice that most of these densities are skewed. More precisely, the densities for
the components of vector Œ± are right-skewed while components of Œ≤ are left-
skewed. In the case of parameters Œ±1
1 and Œ±2
1, the modes of the densities are close
to the lower boundary of the parameter‚Äôs space, suggesting that the parameters
are close to zero. Finally, we can notice that the posterior densities for p11 and
p22 are strongly left-skewed.
Some probabilistic statements on nonlinear functions of the parameters can
be straightforwardly obtained by simulation from the joint posterior sample
{œà[j]}J
j=1. In particular, we can test the covariance stationarity condition and es-
timate the density of the unconditional variance when this condition is satisÔ¨Åed.
Under the GJR speciÔ¨Åcation, the process is covariance stationary if Œ± + Œ≤ < 1
where we recall that Œ± .= Œ±1+Œ±2
2
for notational purposes. The estimated prob-
ability P(Œ± + Œ≤ < 1 | y) is one. Hence, the unconditional variance exists and
is given by
Œ±0
1‚àíŒ±‚àíŒ≤ ; the estimation of its posterior mean is 1.179 with a 95%
conÔ¨Ådence interval given by [1.173,1.189]. These estimations can be compared
with the empirical variance of the process which is 1.136. In this case, the single-
regime model slightly overestimates the variability of the underlying time series.
For the Markov-switching model, our simulation study indicates that the process
is covariance stationary in each state. The posterior mean of the unconditional
variances is 0.56 in state 1 and 2.00 in state 2 with 95% conÔ¨Ådence intervals
respectively given by [0.557,0.563] and [1.992,2.012]. The unconditional vari-
ance of the process in state 1 is about four times lower than the one in state
2; we will therefore refer state 1 as the low-volatility regime and state 2 as the
high-volatility regime. As found by Haas et al. [2004, Eq.11, p.500], the Markov-
switching GARCH process is covariance stationary if Œæ(M) < 1, where Œæ(M)
denotes the largest eigenvalue in modulus of matrix M. This matrix is con-
structed from the model parameters and, in the case of the MS-GJR model, it
is given by:

7.3 An application to the Swiss Market Index
127
Table 7.1. Estimation results for the GJR model (upper panel) and
MS-GJR model (lower panel).‚ãÜ
GJR model
œà
œà
œà0.5
œà0.025
œà0.975
min
max
NSE
IF
Œ±0
0.066
0.065
0.041
0.099
0.021
0.156
0.356
5.58
Œ±1
0.060
0.059
0.028
0.098
0.005
0.162
0.237
1.81
Œ±2
0.207
0.205
0.148
0.278
0.097
0.359
0.690
4.33
Œ≤
0.809
0.809
0.750
0.861
0.656
0.911
1.163
16.22
ŒΩ
8.083
7.954
6.258
10.580
4.871
13.930
34.643
9.79
MS-GJR model
œà
œà
œà0.5
œà0.025
œà0.975
min
max
NSE
IF
Œ±1
0
0.245
0.241
0.149
0.362
0.100
0.518
2.407
19.26
Œ±2
0
0.184
0.178
0.089
0.327
0.046
0.518
1.939
10.45
Œ±1
1
0.020
0.015
0.001
0.063
0.000
0.145
0.276
2.61
Œ±2
1
0.027
0.023
0.001
0.073
0.000
0.135
0.302
2.33
Œ±1
2
0.229
0.224
0.123
0.361
0.074
0.534
1.278
4.21
Œ±2
2
0.220
0.215
0.136
0.332
0.090
0.462
1.140
5.21
Œ≤1
0.436
0.440
0.212
0.642
0.004
0.746
4.454
16.80
Œ≤2
0.782
0.785
0.670
0.866
0.582
0.907
2.090
18.33
ŒΩ
9.459
9.264
7.051
12.880
5.881
23.740
55.931
13.45
p11
0.997
0.997
0.992
0.999
0.982
1.000
0.022
1.23
p12
0.003
0.003
0.001
0.008
0.001
0.018
0.022
1.23
p21
0.005
0.004
0.001
0.011
0.001
0.023
0.027
1.13
p22
0.995
0.996
0.989
0.999
0.978
1.000
0.027
1.13
‚ãÜœà: posterior mean; œàœÜ: estimated posterior quantile at probability œÜ;
min: minimum value; max: maximum value; NSE: numerical standard
error (√ó103); IF: ineÔ¨Éciency factor (i.e., ratio of the squared numerical
standard error and the variance of the sample mean from a hypotheti-
cal iid sampler). The posterior statistics are based on 10‚Äô000 draws from
the constrained posterior sample.
M .=
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
p11(Œ±1 + Œ≤1)
0
p21(Œ±1 + Œ≤1)
0
p11Œ±2
1
p11Œ≤2
p21Œ±2
1
p21Œ≤2
p12Œ≤1
p12Œ±1
1
p22Œ≤1
p22Œ±1
1
0
p12(Œ±2 + Œ≤2)
0
p22(Œ±2 + Œ≤2)
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏
(7.17)
where Œ±k .= Œ±k
1+Œ±k
2
2
. Using the posterior sample we can thus estimate the density
of Œæ(M) by substituting the values of the draws for the model parameters in the
deÔ¨Ånition (7.17). In the upper part of Fig. 7.4, we present the posterior density
for Œæ(M). As we can notice, none of the values exceed one in our simulation.
Thus, the model is covariance stationary. Therefore, the unconditional variance
of the MS-GJR process exists and is given by:

128
7 MS-GJR(1, 1) Model with Student-t Innovations
hy .= (vec P)‚Ä≤ √ó (I4 ‚àíM)‚àí1 √ó (œÄ ‚äóŒ±0)
(7.18)
where œÄ is the 2 √ó 1 vector of ergodic probabilities of the Markov chain, I4
is a 4 √ó 4 identity matrix, vec denotes the vectorization operator which stacks
the columns of a matrix one underneath the other and ‚äódenotes the Kro-
necker product. Derivation of formula (7.18) can be found in Haas et al. [2004,
p.501]. The posterior density of the unconditional variance is shown in the lower
part of Fig. 7.4. Its posterior mean is 1.134 with a 95% conÔ¨Ådence interval of
[1.128,1.139]. In this case, the conÔ¨Ådence band for the mean contains the em-
pirical variance of 1.136 contrary to the one in the GJR model. This suggests
that the Markov-switching model is more apt to reproduce the variability of the
data.
Finally, since the states vector s .= (s1 ¬∑ ¬∑ ¬∑ sT )‚Ä≤ is considered as a parameter in
the MCMC procedure, the draws {s[j]}J
j=1 can also be stored and used to make
inference about the smoothed probabilities. Theses probabilities are estimated
as the percentage of replications of st corresponding to regime k:
P(st = k | y) ‚âà1
J
J
X
j=1
I{s[j]
t =k} .
In Fig. 7.5, we present the smoothed probabilities for the high-volatility regime
(solid line, left axis) together with the in-sample daily log-returns (circles, right
axis). The 95% conÔ¨Ådence bands are shown in dashed lines but are almost indis-
tinguishable from the point estimates. The beginning of year 1991 is associated
with the high-volatility state. Then, from the second half of 1991 to 1997, the
returns are clearly associated with the low-volatility regime, with the exception
of 1994. From 1997 to 2000, the model remains in the high-volatility regime with
a transition during the second semester 2000 to the low-volatility state.

7.3 An application to the Swiss Market Index
129
Parameter Œ≤k
Parameter Œ±0
k
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
Parameter Œ≤k
Parameter Œ±1
k
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
Parameter Œ≤k
Parameter Œ±2
k
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Fig. 7.2. Contour plots for (Œ≤k, Œ±k
0), (Œ≤k, Œ±k
1) and (Œ≤k, Œ±k
2), respectively. The choice of k is arbitrary since all marginal densities contain
the same information [see Fr¬®uhwirth-Schnatter 2001b]. The graphs are based on 10‚Äô000 draws from the joint posterior sample.

130
7 MS-GJR(1, 1) Model with Student-t Innovations
Parameter Œ±0
1
0.1
0.2
0.3
0.4
0.5
0
200
400
600
800
1000
1200
1400
Parameter Œ±0
2
0.1
0.2
0.3
0.4
0.5
0
200
400
600
800
1000
1200
1400
Parameter Œ±1
1
0.00
0.05
0.10
0.15
0
500
1000
1500
2000
2500
3000
3500
Parameter Œ±1
2
0.00
0.04
0.08
0.12
0
200
400
600
800
1000
1200
Parameter Œ±2
1
0.1
0.2
0.3
0.4
0.5
0
200
400
600
800
1000
1200
Parameter Œ±2
2
0.1
0.2
0.3
0.4
0
500
1000
1500
Parameter Œ≤1
0.0
0.2
0.4
0.6
0
500
1000
1500
Parameter Œ≤2
0.60
0.70
0.80
0.90
0
500
1000
1500
Parameter ŒΩ
5
10
15
20
0
500
1000
1500
2000
2500
Parameter p11
0.985
0.990
0.995
1.000
0
500
1000
1500
2000
Parameter p12
0.000
0.005
0.010
0.015
0
500
1000
1500
2000
Parameter p21
0.000
0.010
0.020
0
500
1000
1500
Parameter p22
0.980
0.990
1.000
0
500
1000
1500
Fig. 7.3. Marginal posterior densities of the MS-GJR parameters. The histograms are based on 10‚Äô000 draws from the constrained posterior
sample.

7.3 An application to the Swiss Market Index
131
Œæ(M)
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0
200
400
600
800
1000
1200
hy
0.5
1.0
1.5
2.0
2.5
3.0
0
500
1000
1500
Fig. 7.4. Posterior densities of the covariance stationarity condition (upper graph)
and the unconditional variance (lower graph) of the MS-GJR process. The histograms
are based on 10‚Äô000 draws from the constrained posterior sample.

132
7 MS-GJR(1, 1) Model with Student-t Innovations
0.00
0.25
0.50
0.75
1.00
‚àí7.5
‚àí5.0
‚àí2.5
0.0
2.5
5.0
7.5
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
Pr(st=2 | y)
Daily log‚àíreturns
(in percent)
year
Smoothed probabilities
0.00
0.25
0.50
0.75
1.00
‚àí7.5
‚àí5.0
‚àí2.5
0.0
2.5
5.0
7.5
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
Pr(st=2 | y)
Daily log‚àíreturns
(in percent)
year
Smoothed probabilities
Fig. 7.5. Smoothed probabilities of the high-volatility state (solid line, left axis) together with the in-sample log-returns (circles, right
axis). The 95% conÔ¨Ådence bands are shown in dashed lines (they are almost indistinguishable from the point estimates).

7.4 In-sample performance analysis
133
7.4 In-sample performance analysis
7.4.1 Model diagnostics
We check for model misspeciÔ¨Åcation by analyzing the predictive probabilities re-
ferred to as probability integral transforms or p-scores in the literature [see, e.g.,
Diebold, Gunther, and Tay 1998, Kaufmann and Fr¬®uhwirth-Schnatter 2002]. We
make use of a simpler version of this method, as proposed by Kim, Shephard,
and Chib [1998], which consists in conditioning on point estimates of œà. To
be meaningful, the point estimate has to be chosen when the identiÔ¨Åcation is
imposed. Hence, we consider the posterior mean œà of the constrained poste-
rior sample. Upon deÔ¨Åning Ft‚àí1 as the information set up to time t ‚àí1, the
(approximate) p-scores are deÔ¨Åned as follows:
zt .=
K
X
k=1
P(Yt ‚©Ωyt | st = k, œà, Ft‚àí1) P(st = k | œà, Ft‚àí1) .
The probability P(Yt ‚©Ωyt | st = k, œà, Ft‚àí1) can be estimated by the Student-t
integral and the Ô¨Åltered probability P(st = k | œà, Ft‚àí1) is obtained as a byprod-
uct from the FFBS algorithm [see Chib 1996, p.83]. Under a correct speciÔ¨Åcation,
the p-scores should have independent uniform distributions asymptotically [see
Rosenblatt 1952]. A further transformation through the Normal integral is often
applied for convenience. In this case, we consider ut .= Œ¶‚àí1(zt) where Œ¶‚àí1(‚Ä¢)
denotes the inverse cumulative standard Normal function. If the model is cor-
rect, these generalized residuals {ut} should be independent standard Normal
and common tests can be used to check these features. In particular, we test the
presence of autocorrelation in the series {ut} and {u2
t} using a Wald test. We also
report the results of a joint test for zero mean, unit variance, zero skewness, and
the absence of excess kurtosis, employing the likelihood ratio framework pro-
posed by Berkowitz [2001]. For precisions on the testing methodology, we refer
the reader to Haas et al. [2004, p.516].
In the case of the GJR model, the Wald statistic for testing the joint nullity
of autoregressive coeÔ¨Écients, up to lag 15, for ut has a p-value of 0.0868 and for
u2
t, a p-value of 0.399. In the case of the MS-GJR model, the p-values are 0.0745
and 0.464, respectively. Therefore, both models seem adequate in removing the
volatility clustering present in the data set. The likelihood ratio framework for
testing the Ô¨Årst four moments of the transformed residuals yields p-values of
0.125 for the GJR model and 0.0635 for the MS-GJR model. Overall, these
results indicate no evidence of misspeciÔ¨Åcation at the 5% signiÔ¨Åcance level for
both models.

134
7 MS-GJR(1, 1) Model with Student-t Innovations
7.4.2 Deviance information criterion
In order to evaluate the goodness-of-Ô¨Åt of the models, we use Ô¨Årst the Deviance
information criterion (henceforth DIC) introduced by Spiegelhalter et al. [2002].
The DIC is not intended for identiÔ¨Åcation of the correct model, but rather
merely as a method of comparing a collection of alternative formulations (all of
which may be incorrect) and determining the most appropriate. This criterion
follows from an extension of the Deviance proposed by Dempster [1997]. A recent
article by Berg, Meyer, and Yu [2004] has illustrated the potential advantages
of this information criterion in determining the appropriate stochastic volatility
model. This criterion presents an interesting alternative to the Bayes factor
which is often diÔ¨Écult to calculate, especially for models that involve many
random eÔ¨Äects, large number of unknowns or improper priors.
Let us denote the model parameters by Œ∏ for the moment. Based on the
posterior density of the Deviance D(Œ∏) .= ‚àí2 ln L(Œ∏ | y) where L(Œ∏ | y) is the
likelihood function, the DIC consists of two terms: a component that measures
the goodness-of-Ô¨Åt and a penalty term for any increase in model complexity. The
measure of Ô¨Åt is obtained by taking the posterior expectation of the Deviance:
D .= EŒ∏|y

D(Œ∏)

(7.19)
where EŒ∏|y[‚Ä¢] denotes the expectation with respect to the joint posterior p(Œ∏ | y).
Provided that D(Œ∏) is available in closed form, D can easily be approximated
using the posterior sample by estimating the sample mean of the simulated
values of D(Œ∏). The second component measures the complexity of the model
using the eÔ¨Äective number of parameters, denoted by pD, and is deÔ¨Åned as
the diÔ¨Äerence between the posterior mean of the Deviance and the Deviance
evaluated at a point estimate eŒ∏:
pD .= D ‚àíD(eŒ∏) .
(7.20)
A natural candidate for eŒ∏ is the posterior mean EŒ∏|y(Œ∏), as suggested by Spiegel-
halter et al. [2002]. When the density is log-concave, this point estimate ensures
a positive pD due to Jensen‚Äôs inequality. The DIC is then simply deÔ¨Åned as
DIC .= D + pD and, given a set of models, the one with the smallest DIC has
the best balance between goodness-of-Ô¨Åt and model complexity.
As noted in Celeux, Forbes, Robert, and Titterington [2006], the deÔ¨Ånition
eŒ∏ .= EŒ∏|y(Œ∏) is not appropriate in mixture models when no identiÔ¨Åcation is im-
posed. Furthermore, when the state variable is discrete and considered as a
parameter in Œ∏, the posterior expectation usually fails to take one of the dis-
crete values. To overcome these diÔ¨Éculties, we integrate out the state vector by

7.4 In-sample performance analysis
135
considering the observed likelihood instead [see Celeux et al. 2006, Sect.3.1] and
make use of the constrained posterior sample in the estimation. In the context
of MS-GARCH models, the observed likelihood, also referred to as the marginal
likelihood in Kaufmann and Fr¬®uhwirth-Schnatter [2002, p.457] is obtained as
follows:
L(œà | y) =
T
Y
t=1
" K
X
k=1
p(yt | œà, st = k, Ft‚àí1) P(st = k | œà, Ft‚àí1)
#
(7.21)
where p(yt | œà, st = k, Ft‚àí1) can be estimated by the Student-t density and
the Ô¨Åltered probability P(st = k | œà, Ft‚àí1) is obtained as a byproduct from the
FFBS algorithm [see Chib 1996, p.83]. The DIC is then deÔ¨Åned as the sum of
components (7.19) and (7.20), which yields:
DIC .= 2
n
ln L
 œà | y

‚àí2Eœà|y

ln L(œà | y)
o
where we recall that œà .= Eœà|y(œà) with œà .= (Œ±, Œ≤, ŒΩ, P).
In order to make statements about the goodness-of-Ô¨Åt of one model relative
to another, it is important to consider the uncertainty in the DIC. While the
conÔ¨Ådence interval for D can be easily obtained from the MCMC output by
using spectral methods as this is done for the posterior mean, the task is more
tedious in the case of pD and hence for the DIC itself. Approximation methods
have been experimented in Zhu and Carlin [2000] but the brute force approach
is still the most accurate. In this method, the variability of the DIC is estimated
by running several MCMC chains and calculating the DIC‚Äôs variance from the
diÔ¨Äerent runs. Obviously, this is extremely costly. A simpler alternative consists
in running few MCMC runs and reporting the minimum and maximum DIC
obtained. This gives however a crude idea of DIC‚Äôs variability. In what follows,
we make use of a methodology which estimates the whole distribution for the
DIC based on a resampling technique. More precisely, from the joint posterior
sample {œà[j]}J
j=1, we generate randomly B new posterior samples of size J
by using the block bootstrap technique and estimate DIC‚Äôs components for
these samples. By comparing the 95% conÔ¨Ådence interval of the diÔ¨Äerent DICs,
we can Ô¨Ånd statistical evidence of diÔ¨Äerences in the Ô¨Åtting quality. With this
methodology, the MCMC procedure does not need to be re-run which strongly
diminishes the computing time. The choice of the block length is an important
issue in the block bootstrap technique. For the block bootstrap to be eÔ¨Äective,
the length should be large enough so that it includes most of the dependence
structure, but not too large so that the number of blocks becomes insuÔ¨Écient.
In our analysis, we use the stationary bootstrap of Politis and Romano [1994]

136
7 MS-GJR(1, 1) Model with Student-t Innovations
and select the block length following the algorithm based on the spectral density
estimation, as proposed by Politis and White [2004]. We apply the block length
selection algorithm to each parameter‚Äôs output. The maximum value is then
deÔ¨Åned as the optimal block length used for block bootstrapping the constrained
posterior sample. This ad-hoc procedure allows to keep the autocorrelation in
the chains as well as the cross-dependence structure between the parameters.
Results for the DIC and its components are reported in Table 7.2. They are
based on 10‚Äô000 draws from the constrained posterior distribution. In squared
brackets we give the 95% conÔ¨Ådence interval obtained by the resampling tech-
nique using B = 100 replications. We keep every tenth draw from the joint
posterior sample in the resampling technique in order to speed up the calcula-
tions and diminish the autocorrelation in the chains. For comparison purposes,
we also consider the Bayesian information criterion introduced by Schwarz [1978]
which is deÔ¨Åned as follows:
BIC(œà) .= 2 ln L(œà | y) ‚àín ln T
where n is the number of parameters and T the number of observations. In our
context, T = 2‚Äô500, n = 5 for the GJR model and n = 11 for the MS-GJR model
(since parameters p12 and p21 are redundant due to the summability constraint).
This criterion promotes model parsimony by penalizing models with increased
model complexity (larger n) and sample size T. Hence, a model with the largest
BIC is preferred. The computation of the Bayesian information criterion is based
on the posterior mean Eœà|y

BIC(œà)

obtained over the 10‚Äô000 draws of the
constrained posterior sample.
Table 7.2. Results of the DIC and BIC criteria.‚ãÜ
Model
DIC
D
pD
Eœà|y(BIC)
GJR
6770.4
6765.6
4.76
-6806.07
[6769.9,6770.8]
[6765.3,6765.8]
[4.49,4.93]
(7.12)
MS-GJR
6713.3
6704.4
8.84
-6804.73
[6712.6,6713.8]
[6793.9,6794.9]
[8.49,9.04]
(12.55)
‚ãÜDIC: Deviance information criterion; D: Deviance evaluated at the
posterior mean œà (see Table 7.1, p.127); pD: eÔ¨Äective number of pa-
rameters; Eœà|y(BIC): posterior mean of BIC(œà) obtained over the 10‚Äô000
draws of the constrained posterior sample; [‚Ä¢]: 95% conÔ¨Ådence interval
based on B = 100 replications of the constrained posterior sample; (‚Ä¢):
numerical standard error (√ó102).

7.4 In-sample performance analysis
137
From Table 7.2, we can notice that both DIC and BIC criteria favor the
MS-GJR model. Indeed, the DIC estimates based on the initial joint posterior
sample is 6770.4 for the GJR model and 6713.3 for the MS-GJR model. Both
95% conÔ¨Ådence intervals do not overlap which suggests signiÔ¨Åcant improvement
of the Markov-switching model. In the case of BIC, the diÔ¨Äerences between the
criterion‚Äôs values are less pronounced but still the Markov-switching model is
favored compared to the single-regime model. If we consider now the estimations
of pD, we note that the estimated value is somewhat lower than Ô¨Åve in the GJR
model while about nine in the MS-GJR case. Hence, in the single-regime model,
every parameter seems to be eÔ¨Äective (or informative) when Ô¨Åtting the model
to the data set. In the Markov-switching model however, about two third of the
13 parameters are eÔ¨Äective. This is in line with the estimation results where
it was shown that parameters Œ±1 and Œ±2 are almost identical across regimes.
Furthermore, the 2 √ó 2 transition matrix only contains two free parameters due
to the summability constraint. This suggests that the nine eÔ¨Äective parameters
of the MS-GJR model are Œ±1
0, Œ±2
0, Œ±1, Œ±2, Œ≤1, Œ≤2, ŒΩ, p11 and p22.
Finally, we point out that we have also considered the posterior mode:
eœà .= arg max
œà
L(œà | y)
in the deÔ¨Ånition of pD, as suggested by Celeux et al. [2006, Sect.3.1]. It is argued
that such a point estimate is more relevant since it depends on the posterior
distribution of the whole parameter œà, rather than on the marginal posterior
distributions of its elements. The values of pD obtained with this new deÔ¨Ånition
are larger for both models with 95% conÔ¨Ådence intervals respectively given by
[5.17,5.66] and [10.06,11.12] for the single-regime and Markov-switching models.
While the preferred model remains the MS-GJR, the interpretation of parameter
pD is now questionable in the GJR case since the value of pD exceeds the total
number of parameters.
7.4.3 Model likelihood
As a second criterion to discriminate between the models under study, we con-
sider the model likelihood which may be expressed as follows:
p(y) =
Z
L(œà | y)p(œà)dœà
where L(œà | y) is the marginal likelihood given in (7.21) and p(œà) is the joint
prior density on œà .= (Œ±, Œ≤, ŒΩ, P). It is clear that the model likelihood is equal
to the normalizing constant of the posterior density:

138
7 MS-GJR(1, 1) Model with Student-t Innovations
p(œà | y) = L(œà | y)p(œà)
p(y)
.
The estimation of p(y) requires the integration over the whole set of parameters
œà, which is a diÔ¨Écult task in practice, especially for complex statistical models
such as ours. A full investigation of the various approaches available to esti-
mate the model likelihood for Ô¨Ånite mixture models can be found in Fr¬®uhwirth-
Schnatter [2004]. In particular, the author documents that the bridge sampling
technique using the MCMC output of the random permutation sampler and
an iid sample from an importance density q(œà) which approximates the un-
constrained posterior yields the best estimator of the model likelihood (i.e.,
the estimator with the lowest variance). Moreover, the variance of the bridge
sampling estimator depends on a ratio that is bounded regardless of the tail be-
haviour of the importance density. This renders the estimator robust and gives
more Ô¨Çexibility in the choice of the importance density.
First, let us recall that the bridge sampling technique of Meng and Wong
[1996] is based on the following result:
1 =
R
a(œà)p(œà | y)q(œà)dœà
R
a(œà)q(œà)p(œà | y)dœà = Eq

a(œà)p(œà | y)

Eœà|y

a(œà)q(œà)

(7.22)
where a(œà) is an arbitrary function such that
R
a(œà)p(œà | y)q(œà)dœà > 0 and Eq
denotes the expectation with respect to the importance density q(œà). Replacing
p(œà | y) by
L(œà|y)p(œà)
p(y)
in expression (7.22) yields the key identity for bridge
sampling:
p(y) = Eq

a(œà)L(œà | y)p(œà)

Eœà|y

a(œà)q(œà)

.
We can estimate the model likelihood for a given function a(œà) by replacing the
expectations on the right-hand side of the latter expression by sample averages.
More precisely, we use MCMC draws {œà[m]}M
m=1 from the joint posterior p(œà | y)
and iid draws {œà[l]}L
l=1 from the importance sampling density q(œà) to get the
following approximation:
p(y) ‚âà
1
L
PL
l=1 a(œà[l])L(œà[l] | y)p(œà[l])
1
M
PM
m=1 a(œà[m])q(œà[m])
.
(7.23)
Meng and Wong [1996] discuss an asymptotically optimal choice for a(œà), which
minimizes the expected relative error of the p(y) estimator for iid draws from
p(œà | y) and q(œà). This function is given by:

7.4 In-sample performance analysis
139
a(œà) ‚àù
1
Lq(œà) + Mp(œà | y) .
This special case of bridge sampling estimator is referred to as the optimal bridge
sampling estimator by Fr¬®uhwirth-Schnatter [2001a] and will be used in what
follows. As the optimal choice depends on the normalized posterior p(œà | y),
Meng and Wong [1996] use an iterative procedure to estimate p(y) as a limit of
a sequence {pt(y)}. Based on an estimate pt‚àí1(y) of the normalizing constant,
the posterior is normalized as follows:
pt‚àí1(œà | y) .= L(œà | y)p(œà)
pt‚àí1(y)
and a new estimate pt(y) is computed using approximation (7.23). This leads
to the following recursion:
pt(y) .= pt‚àí1(y) √ó
1
L
PL
l=1
pt‚àí1(œà[l]|y)
Lq(œà[l])+Mpt‚àí1(œà[l]|y)
1
M
PM
m=1
q(œà[m])
Lq(œà[m])+Mpt‚àí1(œà[m]|y)
which can be initialized, e.g., with the reciprocal importance sampling estimator
of Gelfand and Dey [1994] given by:
p0(y) =
"
1
M
M
X
m=1
q(œà[m])
L(œà[m] | y)p(œà[m])
#‚àí1
.
Note that this latter estimator is only based on MCMC draws from the joint
posterior. Convergence of the bridge sampling technique is typically very fast in
practice. In our case, the estimates converged after 3‚Äì4 iterations.
The remaining task consists in choosing an appropriate importance density
to apply the bridge sampling technique. To that aim, we follow Kaufmann and
Fr¬®uhwirth-Schnatter [2002, pp.438‚Äì439] and Kaufmann and Scheicher [2006,
pp.9‚Äì10]. The importance density is constructed in an unsupervised manner
from the MCMC output of the random permutation sampler using a mixture of
the proposal and conditional densities. Its construction is fully automatic and
is easily incorporated in the MCMC sampler [see Fr¬®uhwirth-Schnatter 2001a,
p.39]. Formally, the importance density is deÔ¨Åned as follows:
q(œà) .=
 1
R
R
X
r=1
qŒ±(Œ± | Œ±[r], Œ≤[r], œñ[r], ŒΩ[r], s[r], y)
√ó qŒ≤(Œ≤ | Œ±[r], Œ≤[r], œñ[r], ŒΩ[r], s[r], y) √ó p(P | s[r])

√ó qŒΩ(ŒΩ)
(7.24)

140
7 MS-GJR(1, 1) Model with Student-t Innovations
where:
Œ±[r]
,
Œ≤[r]
,
œñ[r]
,
ŒΩ[r]
,
s[r]
for r = 1, . . . , R
are draws from the unconstrained posterior sample, qŒ±(Œ± | ‚Ä¢) is the proposal
density for parameter Œ± given in (7.13), qŒ≤(Œ≤ | ‚Ä¢) is the proposal density for
parameter Œ≤ given in (7.15) (the normalizing constants are easily obtained as
the proposals are truncated multivariate Normal densities), p(P | ‚Ä¢) is the prod-
uct of Dirichlet posterior densities for the transition probabilities given in (7.9).
For the degrees of freedom parameter ŒΩ, the optimized rejection technique of
Sect. 7.2.5 does not lead to a known expression for the marginal posterior
on ŒΩ. To tackle this problem, we approximate the marginal posterior by us-
ing a truncated skewed Student-t density whose parameters are estimated by
Maximum Likelihood from the posterior sample {ŒΩ[j]}J
j=1. More precisely, the
approximation may be written as follows:
qŒΩ(ŒΩ) ‚àùSS(ŒΩ | b¬µ, bœÉ2, bœÑ, bŒ≥)I{ŒΩ>Œ¥}
where:
SS(ŒΩ | ¬µ, œÉ2, œÑ, Œ≥) .=
2
Œ≥ + 1
Œ≥
Œì
  œÑ+1
2

Œì
  œÑ
2

(œÄœÑœÉ2)1/2
√ó

1 + (ŒΩ ‚àí¬µ)2
œÑœÉ2
 1
Œ≥ I{ŒΩ‚àí¬µ‚©æ0} + Œ≥2I{‚àí‚àû<ŒΩ‚àí¬µ}
‚àíœÑ+1
2
(7.25)
is the skewed Student-t density as deÔ¨Åned in Fern¬¥andez and Steel [1998, Eq.13,
p.363]. The parameters of the density deÔ¨Åned in (7.25) are: the location pa-
rameter ¬µ, the scale factor œÉ2 > 0, the degrees of freedom parameter œÑ ‚©æ1
and the asymmetry coeÔ¨Écient Œ≥ > 0. For Œ≥ = 1, the density coincides with the
symmetric Student-t density. In cases where Œ≥ > 1, the density is right-skewed
while it is left-skewed when Œ≥ < 1. Therefore, parametrization (7.25) allows for a
wide range of asymmetric and heavy-tailed densities. Moreover, the normalizing
constant for qŒΩ(ŒΩ) is easily obtained by conventional quadrature methods.
Some comments are in order here. First, the generation of draws from the
proposal densities qŒ±(Œ± | ‚Ä¢) and qŒ≤(Œ≤ | ‚Ä¢) is achieved by the rejection technique.
While we obtain good acceptance rates in our case, this method can become
very ineÔ¨Écient if the mass of the density is close to the domain of truncation.
For these cases, we would need a more sophisticated algorithm, as proposed in
Philippe and Robert [2003], Robert [1995], to draw eÔ¨Éciently from a truncated

7.4 In-sample performance analysis
141
multivariate Normal distribution. Second, the density qŒΩ(ŒΩ) is constructed in two
steps. The parameters of the the skewed Student-t are Ô¨Årst estimated by ML
from the MCMC output and then the density is truncated to construct qŒΩ(ŒΩ).
An alternative approach would be to Ô¨Åt directly the truncated skewed Student-t
density by ML. This is however not necessary in our case since the mass of the
posterior on the degrees of freedom is far from the truncation domain. Finally,
generating draws from qŒΩ(ŒΩ) is achieved by the rejection technique. In cases
where the boundary is close to the high probability mass, alternative approaches,
such as the inversion technique, are required [see, e.g., Geweke 1991].
As indicated previously, the parameters of the skewed Student-t density are
estimated by ML using the posterior sample of ŒΩ. In the case of the MS-GJR
model, we obtain the following ML estimates:
b¬µ = 9.49
,
bœÉ2 = 1.50
,
bœÑ = 16.67
and
bŒ≥ = 1.53 .
In the upper part of Fig. 7.6, we display the Ô¨Åtted truncated skewed Student-t
density (in dashed line) together with the density of the posterior sample for
ŒΩ (in solid line) obtained through Gaussian kernel density estimates [see Sil-
verman 1986]. We can notice that the truncated skewed Student-t density ap-
proximates the marginal closely. In the lower part of the Ô¨Ågure, we show the
marginal posterior for parameter Œ≤1 together with the importance density com-
puted with R = 1‚Äô000. As the construction of the mixture (7.24) is based on
averaging over proposal densities, where the state process is sampled from the
unconstrained posterior with balanced label switching, the mixture importance
density is multimodal. We also notice that the importance density provides a
good approximation of the marginal posterior.
In Table 7.3, we report the natural logarithm of the model likelihoods
obtained using the reciprocal sampling estimator (second column) and the bridge
sampling estimator (last column) for M = L = 1‚Äô000 draws. From this table, we
can notice that both estimators are higher for the MS-GJR model, indicating
a better in-sample Ô¨Åt for the regime-switching speciÔ¨Åcation. As an additional
discrimination criterion, we compute the (transformed) Bayes factor in favor of
the MS-GJR model [see Kass and Raftery 1995, Sect.3.2]. The estimated value
is 2 √ó ln BF = 2 √ó (‚àí3389.66 ‚àí(‚àí3408.04)) = 36.76, which strongly supports
the in-sample evidence in favor of the regime-switching model.
A Ô¨Ånal word about the robustness of these results is in order. It is indeed
recognized that the model likelihood is sensitive to the choice of the prior density.
We must therefore test whether an alternative joint prior speciÔ¨Åcation would
have modiÔ¨Åed the conclusion of our analysis. To answer this question, we modify
the hyperparameters‚Äô values and run the sampler again. This time, we consider

142
7 MS-GJR(1, 1) Model with Student-t Innovations
Table 7.3. Results of the model likelihood estimators.‚ãÜ
Model
ln p0(y)
ln p(y)
GJR
-3405.33 (2.979)
-3408.04 (2.644)
MS-GJR
-3386.14 (3.109)
-3389.66 (3.191)
‚ãÜln p0(y): natural logarithm of the model likelihood esti-
mate using reciprocal sampling; ln p(y): natural logarithm
of the model likelihood estimate using bridge sampling; (‚Ä¢)
numerical standard error of the estimators (√ó102).
slightly more informative priors for the vectors Œ± and Œ≤ by choosing diagonal
covariance matrices whose variances are set to œÉ2
Œ±i = œÉ2
Œ≤ = 1‚Äô000 (i = 0, 1, 2). As
an alternative prior on the degrees of freedom parameter, we choose Œª = 0.02
and Œ¥ = 2, which implies a prior mean of 52. Finally, the hyperparameters for
the prior on the transition probabilities are set to Œ∑ii = 3 and Œ∑ij = Œ∑ji = 1 for
i, j ‚àà{1, 2}. We recall that the hyperparameters of the initial joint prior were
set to œÉ2
Œ±i = œÉ2
Œ≤ = 10‚Äô000, Œª = 0.01, Œ¥ = 2, Œ∑ii = 2 and Œ∑ij = Œ∑ji = 1. In this
case, the results are similar to those obtained previously. The natural logarithm
of the bridge sampling estimator is -3402.11 for the GJR model and -3388.09
for the MS-GJR model, implying a (transformed) Bayes factor of 28.04. These
results are in line with the conclusion of the previous section and conÔ¨Årm the
better Ô¨Åt of the Markov-switching model.

7.4 In-sample performance analysis
143
6
8
10
12
14
16
18
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Parameter ŒΩ
Posterior density
Truncated skewed Student‚àít density
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
Parameter Œ≤1
Posterior density
Importance density
Fig. 7.6. Importance density (in dashed line) and marginal posterior density (in solid
line) comparison. Gaussian kernel density estimates with bandwidth selected by the
‚ÄúSilverman‚Äôs rule of thumb‚Äùcriterion [see Silverman 1986, p.48]. Both graphs are based
on 10‚Äô000 draws from the unconstrained posterior sample.

144
7 MS-GJR(1, 1) Model with Student-t Innovations
7.5 Forecasting performance analysis
In order to evaluate the ability of the competing models to predict the future
behavior of the volatility process, we study the forecasted one-day ahead Value
at Risk (henceforth VaR), which is a common tool to measure Ô¨Ånancial and
market risks. The one-day ahead VaR at risk level œÜ ‚àà(0, 1), denoted by VaRœÜ,
is estimated by calculating the œÜcth percentile of the one-day ahead predictive
distribution, where œÜc
.= (1 ‚àíœÜ) for convenience. The predictive density is
obtained by simulation from the joint posterior sample {œà[j]}J
j=1 as follows:
s[j]
t+1 ‚àºp(st+1 | œà[j], Ft)
y[j]
t+1 ‚àºp(yt+1 | œà[j], s[j]
t+1, Ft)
and VaRœÜ is then simply estimated by calculating the œÜcth percentile of the
empirical distribution {y[j]
t+1}J
j=1.
In order to simulate from the predictive density over the out-of-sample obser-
vation window, the posterior sample {œà[j]}J
j=1 should be updated using the most
recent information. Consequently, forecasting the one-day ahead VaR would ne-
cessitate the estimation of the joint posterior sample at each time point in the
out-of-sample observation window. However, such an approach is computation-
ally impractical for a large data set such as ours. Combination of MCMC and
importance sampling to estimate eÔ¨Éciently this predictive density is proposed
by Gerlach, Carter, and Kohn [1999]. Nevertheless, for the sake of simplicity, we
will consider the same joint posterior sample, based on the in-sample data set,
when forecasting the VaR.
In addition to the static GJR and MS-GJR models, we consider a GJR
model estimated on rolling windows which is the standard practice in Ô¨Ånancial
risk management. This methodology relies on the assumption that older data are
not available or are irrelevant due to structural breaks, which are so complicated
that they cannot be modeled. We refer the reader to Sect. 6.4.1 for a detailed
presentation of this procedure. For this approach, we use 750 log-returns to es-
timate the model and the next 50 log-returns are used as a forecasting window.
Then, the estimation and forecasting windows are moved together by 50 days
ahead, so that the forecasting windows do not overlap. In this manner, the es-
timation methodology fulÔ¨Ålls the recommendations of the Basel Committee in
the use of internal models [see Basel Committee on Banking Supervision 1996b].
When applied to our data set, this estimation design leads to the generation of
26 estimation windows for a total of 26√ó50 = 1‚Äô300 out-of-sample observations.
In the case of the static GJR and MS-GJR models, the Ô¨Årst 2‚Äô500 observations
of our data set are used to estimate the models while the remaining 1‚Äô300 obser-

7.5 Forecasting performance analysis
145
vations are used to test their predictive performance. For the three models, the
VaR predictions are obtained for the same 1‚Äô300 out-of-sample daily log-returns.
To verify the accuracy of the VaR estimates for the analyzed models, we
adopt the testing methodology proposed by ChristoÔ¨Äersen [1998]. This approach
is based on the study of the random sequence {V œÜ
t } where:
V œÜ
t
.=
Ô£±
Ô£≤
Ô£≥
1
if
yt+1 < VaRœÜ
t
0
else .
A sequence of VaR forecasts at risk level œÜ has correct conditional coverage if
{V œÜ
t } is an independent and identically distributed sequence of Bernoulli ran-
dom variables with parameter œÜc. This hypothesis can be veriÔ¨Åed by testing
jointly the independence on the series and the unconditional coverage of the
VaR forecasts, i.e., E(V œÜ
t ) = œÜc, as proposed by ChristoÔ¨Äersen [1998].
Forecasting
results
for
the
VaR
are
reported
in
Table
7.4
for
œÜ ‚àà{0.90, 0.95, 0.99} which are typical risk levels used in Ô¨Ånancial risk manage-
ment. The second and third columns give the expected and observed number
of violations. The last three columns report the p-values for the tests of correct
unconditional coverage (UC), independence (IND) and correct conditional cov-
erage (CC). From this table, we Ô¨Årst note that the observed number of violations
for the MS-GJR model are closer to the expected values than for the static GJR
model. Indeed, at the 1% signiÔ¨Åcance level, the test of correct unconditional
coverage is not rejected for the Markov-switching model while it is strongly re-
jected for the GJR model at risk level œÜ = 0.95. The test of independence is
not rejected for both models at the 1% signiÔ¨Åcance level. We can notice that
for risk level œÜ = 0.99 this test is not applicable since no consecutive violations
have been observed. The joint hypothesis of correct unconditional coverage and
independent sequence is obtained via the test of correct conditional coverage. In
the case of the MS-GJR model, p-values are close to 0.10 for risk levels œÜ = 0.9
and œÜ = 0.95 while it is 0.030 and 0.013 in the GJR case. We therefore reject
the correct conditional coverage hypothesis for the static GJR model at the 5%
signiÔ¨Åcance level. These results indicate the better out-of-sample performance
of the Markov-switching model compared to the static GJR model.
When comparing the MS-GJR model with the rolling GJR model, we can
notice that both approaches perform equally well. Indeed, for both models, the
test of independence is rejected at risk level œÜ = 0.90 while the correct condi-
tional coverage hypothesis is not rejected at the 5% signiÔ¨Åcance level. Although
the two models are successful in forecasting the conditional variance of the SMI
log-returns, the MS-GJR model has two advantages over the rolling window

146
7 MS-GJR(1, 1) Model with Student-t Innovations
Table 7.4. Forecasting results of the VaR.‚ãÜ
GJR model (static approach)
œÜ
E(V œÜ
t )
#
UC
IND
CC
0.99
13
14
0.783
NA
NA
0.95
65
89
0.004
0.624
0.013
0.90
130
143
0.236
0.018
0.030
GJR model (rolling windows approach)
œÜ
E(V œÜ
t )
#
UC
IND
CC
0.99
13
15
0.586
NA
NA
0.95
65
73
0.318
0.547
0.506
0.90
130
126
0.710
0.032
0.093
MS-GJR model (static approach)
œÜ
E(V œÜ
t )
#
UC
IND
CC
0.99
13
13
1.000
NA
NA
0.95
65
80
0.065
0.323
0.112
0.90
130
132
0.854
0.035
0.107
‚ãÜœÜ: risk level; E(V œÜ
t ): expected number of violations;
#: observed number of violations; UC: p-value for the
correct unconditional coverage test; IND: p-value for
the independence test; CC: p-value for the correct con-
ditional coverage test; NA: not applicable.
approach. First, it is able to anticipate structural breaks in the conditional vari-
ance process. This is achieved through the estimation of the Ô¨Åltered probabilities
P(st = k | œà, Ft‚àí1), as shown in Fig. 7.7. On the contrary, the rolling window
methodology is merely an ad-hoc approach which is unable to forecast structural
breaks. The updating frequency as well as the length of the rolling window are
subjective quantities, albeit some ranges are recommended by regulators, so that
diÔ¨Äerent choices might lead to signiÔ¨Åcant diÔ¨Äerences in the model‚Äôs performance.
Second, the MS-GJR model needs only to be estimated once. On the contrary,
the parameters of the GJR model must be updated frequently to account for
structural breaks in the time series and this can have practical consequences for
risk management systems of Ô¨Ånancial institutions. This is a deÔ¨Ånite advantage
of the regime-switching approach compared to the traditional rolling window
methodology.

7.5 Forecasting performance analysis
147
‚àí5.0
‚àí2.5
0.0
2.5
5.0
0.00
0.25
0.50
0.75
1.00
1
101
201
301
401
501
601
701
801
901
1001
1101
1201
Daily log‚àíreturns
(in percent)
Pr(st=2 | œà, Ft‚àí1)
time index
Filtered probabilities
Fig. 7.7. Filtered probabilities of the high-volatility state (solid line, left axis) together with the out-of-sample log-returns (circles, right
axis). The 95% conÔ¨Ådence bands are shown in dashed lines.

148
7 MS-GJR(1, 1) Model with Student-t Innovations
7.6 One-day ahead VaR density
As emphasized in Chap. 6, the one-day ahead VaR risk measure can be ex-
pressed as a function of the model parameters when the underlying time series
is described by a single-regime GARCH(1, 1) model. It turns out that this is also
the case in the context of Markov-switching GARCH models. In eÔ¨Äect, the one-
day ahead VaR at risk level œÜ, estimated at time t, can be explicitly calculated
for given œà and future state st+1 as follows:
VaRœÜ
t (œà, st+1) .=

œ±(ŒΩ) √ó e‚Ä≤
t+1(st+1)ht+1(Œ±, Œ≤)
1/2 √ó tœÜc(ŒΩ)
(7.26)
where we recall that œ±(ŒΩ) .= ŒΩ‚àí2
ŒΩ
and tœÜc(ŒΩ) denotes the œÜcth percentile of a
Student-t distribution with ŒΩ degrees of freedom. Hence, the VaR risk measure
can be simulated from the joint posterior sample {œà[j]}J
j=1 by Ô¨Årst generating
s[j]
t+1 from the Ô¨Åltered probability density p(st+1 | œà[j], Ft), and then inputting
the joint draw (œà[j], s[j]
t+1) in expression (7.26).
The result of this procedure is shown in Fig. 7.8 where we plot the one-
day ahead VaR density of the MS-GJR model for two distinct time points in
the out-of-sample observation window. We can notice that both densities are bi-
modal, which is a consequence of the Markov-switching nature of the conditional
variance process. At time t = 2‚Äô501, the VaR density gives a higher probability
to larger (in absolute value) VaR values. This suggests that, at that particular
point in time, the probability of being in the high volatility state is higher than
being in the low-volatility regime. At time t = 3‚Äô500, the bimodality of the den-
sity is slightly less pronounced. In this case, the VaR density puts more mass on
smaller VaR values (in absolute value). This graph shows that the density of the
VaR has a particular shape in the case of the MS-GJR model. In this context, it
would be interesting to determine if the loss function of an agent, and therefore
the location of his optimal Bayes estimate within the VaR density, would have
any inÔ¨Çuence on the forecasting performance of the model.
In order to address this question, we consider diÔ¨Äerent loss functions and
determine the Bayes point estimates for the VaR by solving the optimization
problem (6.10) of page 85. The loss functions we consider are the Linex with a
parameter a ‚àà{‚àí3, 3}, the absolute error loss (AEL) as well as the squared error
loss (SEL); the reader is referred to Sect. 6.4.4 for further details. We recall
however that the Linex function with a positive parameter could be attributed
to a regulator or risk manager whose aim is to avoid systematic failure in risk
measure estimation. On the contrary, a negative parameter could be attributed
to a fund manager who seeks to save risk capital since it earns little or no
return at all (see Sect. 6.3.1 for details). The AEL and SEL correspond to the

7.6 One-day ahead VaR density
149
perspective of an agent for whom under- and overestimation are equally serious.
The SEL leads, however, to a larger penalty for larger deviations from the true
value compared to the AEL function.
The VaR risk measure obtained with the diÔ¨Äerent loss functions are then
tested over the 1‚Äô300 out-of-sample observations. To test the adequacy of the
point estimates to reproduce the true VaR, we rely on the forecasting method-
ology of ChristoÔ¨Äersen [1998] as this was done in the preceding section. The
results are reported in Table 7.5 whose second column gives the observed num-
ber of violations and the third, fourth and Ô¨Åfth columns report the p-values for
the tests of correct unconditional coverage (UC), independence (IND) and cor-
rect conditional coverage (CC), respectively. From this table, we note Ô¨Årst that
the observed number of violations is close to the expected value for the Linex
function with parameter a = 3. In this case, the test of correct unconditional
coverage, at the 5% signiÔ¨Åcance level, is never rejected. On the contrary, the
Linex function with parameters a = ‚àí3 leads to the rejection of the null for risk
levels œÜ = 0.95 and œÜ = 0.99. The null hypothesis is also rejected for the AEL
and SEL point estimates at risk level œÜ = 0.95, where the estimates systemat-
ically underestimate (in absolute value) the true VaR. The joint hypothesis of
correct unconditional coverage and independence is rejected at the 5% signiÔ¨Å-
cance level for all functions, except the Linex with a = 3 and the SEL at risk
level œÜ = 0.9.
From what precedes, we can thus conclude that parameter uncertainty has to
be taken seriously in the context of MS-GARCH models. In particular, the choice
of a given point estimate within the VaR density has a signiÔ¨Åcant impact on
the forecasting performance of the model. A regulator (Linex a = 3) whose VaR
point estimate are conservative, would conclude to a good performance of the
model while a fund manager (Linex a = ‚àí3) would systematically underestimate
(in absolute value) the true VaR.

150
7 MS-GJR(1, 1) Model with Student-t Innovations
Table 7.5. Forecasting results of the VaR point esti-
mates for the MS-GJR model.‚ãÜ
œÜ = 0.90, E(V œÜ
t ) = 130;
Loss L
#
UC
IND
CC
Linex (a = 3)
130
1.000
0.018
0.061
Linex (a = ‚àí3)
140
0.361
0.011
0.025
AELa
133
0.782
0.011
0.039
SELb
131
0.926
0.015
0.053
œÜ = 0.95, E(V œÜ
t ) = 65;
Loss L
#
UC
IND
CC
Linex (a = 3)
71
0.452
0.270
0.410
Linex (a = ‚àí3)
87
0.008
0.171
0.011
AELa
84
0.020
0.228
0.033
SELb
83
0.028
0.249
0.046
œÜ = 0.99, E(V œÜ
t ) = 13;
Loss L
#
UC
IND
CC
Linex (a = 3)
11
0.567
NA
NA
Linex (a = ‚àí3)
21
0.041
NA
NA
AELa
17
0.287
NA
NA
SELb
14
0.783
NA
NA
‚ãÜœÜ: risk level; E(V œÜ
t ): expected number of violations;
#: observed number of violations; UC: p-value for the
correct uncoverage test; IND: p-value for the indepen-
dence test; CC: p-value for the correct conditional cov-
erage test; NA: not applicable.
a Absolute error loss function.
b Squared error loss function.

7.6 One-day ahead VaR density
151
‚àí2.2
‚àí2.0
‚àí1.8
‚àí1.6
‚àí1.4
‚àí1.2
‚àí1.0
0
1
2
3
4
‚àí2.2
‚àí2.0
‚àí1.8
‚àí1.6
‚àí1.4
‚àí1.2
‚àí1.0
0
1
2
3
4
VaR at t = 2501
VaR at t = 3500
One‚àíday ahead 95% VaR
Fig. 7.8. Density of the one-day ahead VaR at risk level œÜ = 0.95 for the MS-GJR
model at two time points in the out-of-sample observation window. Gaussian kernel
density estimates with bandwidth selected by the ‚ÄúSilverman‚Äôs rule of thumb‚Äù criterion
[see Silverman 1986, p.48]. Both graphs are based on 10‚Äô000 draws from the joint
posterior density of the MS-GJR model parameters.

152
7 MS-GJR(1, 1) Model with Student-t Innovations
7.7 Maximum Likelihood estimation
We conclude this chapter with some comments regarding the Maximum Likeli-
hood (henceforth ML) estimation of Markov-switching GARCH models. In this
case, the estimation is handled as in Hamilton [1994, p.692], where the algorithm
turns out to be a special case of the Expectation Maximization (henceforth EM)
algorithm developed by Dempster, Laird, and Rubin [1977]. The classical ML
approach cannot be applied directly, as the marginal likelihood where the latent
process {st} is integrated out, is not available in closed form. The estimation
procedure is therefore decomposed into two stages. The Ô¨Årst step consists in
estimating the sequence of Ô¨Åltered probabilities {P(st = k | œà, Ft‚àí1)}T
t=1 for a
Ô¨Åxed set of of parameters œà. The second step maximizes the observed likelihood
L(œà | y) in expression (7.21) given this sequence of probabilities. The procedure
is iterated until a given convergence criterion is satisÔ¨Åed. General results avail-
able for the EM algorithm indicate that the likelihood function increases in the
number of iterations.
While apparently straightforward to handle, the ML estimation has practical
drawbacks. Indeed, the EM algorithm guarantees a convergence to a local maxi-
mum of the likelihood, but not necessarily to the global optimum. As reported in
Hamilton and Susmel [1994], many starting points are required to end up with
a global maximum. Furthermore, the covariance matrix at the optimum can be
extremely tedious to obtain and ad-hoc procedures are often required to get re-
liable results. E.g., Hamilton and Susmel [1994] Ô¨Åx some transition probabilities
to zero in order to determine the variance estimates for some model parameters.
Finally, testing the null of K versus K‚Ä≤ states is not possible within the ML
framework since the regularity conditions for justifying the œá2 approximation of
the likelihood ratio statistic do not hold.
For comparison purposes, we estimate the MS-GJR model via the ML tech-
nique. The iterative procedure described previously has been run using 20 ran-
dom starting values. In all cases, the optimizer has been trapped in a local
maximum or even did not converge. The convergence has only been achieved
by starting the ML optimizer at the posterior mean œà (see Table 7.1, p.127)
obtained with the Bayesian approach.
In Fig. 7.9, we display the marginal densities obtained via Gaussian kernel
density estimates, for the model parameters obtained through the Bayesian ap-
proach (in solid lines) and the ML approach (in dashed lines). From these graphs,
we note that the ML estimation leads to more peaked density estimates and
therefore underestimates the parameter uncertainty. Furthermore, compared to
the Bayesian approach, the ML approach underestimates the values of the com-
ponents of vector Œ± whereas the components of Œ≤ are overestimated.

7.7 Maximum Likelihood estimation
153
0.0
0.2
0.4
0
2
4
6
8
Parameter Œ±0
1
0.0
0.2
0.4
0
2
4
6
8
Parameter Œ±0
2
0.00
0.05
0.10
0.15
0
10
20
30
40
50
Parameter Œ±1
1
0.00
0.05
0.10
0
5
10
15
20
25
30
Parameter Œ±1
2
0.0
0.2
0.4
0
1
2
3
4
5
6
7
Parameter Œ±2
1
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
Parameter Œ±2
2
0.0
0.2
0.4
0.6
0.8
0
1
2
3
4
Parameter Œ≤1
0.6
0.7
0.8
0.9
0
2
4
6
8
10
Parameter Œ≤2
5
10
15
20
25
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Parameter ŒΩ
0.985
0.995
0
100
200
300
400
Parameter p11
0.000
0.010
0
100
200
300
400
Parameter p12
0.000
0.010
0.020
0
50
100
150
200
250
Parameter p21
0.980
0.990
1.000
0
50
100
150
200
250
Parameter p22
Fig. 7.9. Marginal posterior densities of the MS-GJR model parameters and comparison with the asymptotic Normal approximation.
Results obtained via the Bayesian approach are given in solid lines while the ML estimates are shown in dashed lines. Gaussian kernel
density estimates with bandwidth selected by the ‚ÄúSilverman‚Äôs rule of thumb‚Äù criterion [see Silverman 1986, p.48]. The graphs are based
on 10‚Äô000 draws from the constrained posterior sample.

8
Conclusion
Single-regime and regime-switching GARCH models are widespread and essen-
tial tools in Ô¨Ånancial econometrics and have, until recently, mainly been es-
timated using the Maximum Likelihood (henceforth ML) technique. However,
the Bayesian estimation of these models has several advantages over the classical
approach. First, computational methods based on Markov chain Monte Carlo
(henceforth MCMC) procedures avoid the common problem of local maxima
encountered in the ML estimation of these models. Second, the exploration of
the joint posterior distribution gives a complete picture of the parameter un-
certainty and this cannot be achieved via the classical approach. Third, exact
distributions of nonlinear functions of the model parameters can be obtained at
low cost by simulating from the joint posterior distribution. Fourth, constraints
on the model parameters can be incorporated through appropriate prior speciÔ¨Å-
cations; in such a setting, imposing the constraint of covariance stationarity for
the regime-switching GARCH model, for instance, is straightforward. Finally,
discrimination between models can be achieved through the calculation of model
likelihoods and Bayes factors. All these reasons strongly motivate the use of the
Bayesian approach when estimating GARCH models.
The choice of the algorithm is the Ô¨Årst issue when dealing with MCMC
methods and it depends on the nature of the problem under study. In the case
of GARCH models, due to the recursive nature of the conditional variance, the
joint posterior and the full conditional densities are of unknown forms, whatever
distributional assumptions are made on the model disturbances. Therefore, we
cannot use the simple Gibbs sampler and need more elaborate estimation proce-
dures. The sampling schemes adopted in this book are based on the approach of
Nakatsuma [1998, 2000] which has the advantage of being fully automatic and
thus avoids the time-consuming and diÔ¨Écult task of tuning a sampling algo-
rithm. In addition, this approach is easy to extend to regime-switching GARCH

156
8 Conclusion
models. In this case, the parameters in each regime can be regrouped and up-
dated by blocks which may enhance the sampler‚Äôs eÔ¨Éciency.
This book presented in detail methodologies for the Bayesian estimation of
single-regime and regime-switching GARCH models. It proposed empirical ap-
plications to real data sets and illustrated some interesting probabilistic state-
ments on nonlinear functions of the model parameters made possible under the
Bayesian framework.
The work was introduced in Chap. 1 with a review of GARCH modeling
and a presentation of the advantages of the Bayesian approach compared to
the traditional ML technique. In Chap. 2, we proposed a short introduction to
the Bayesian paradigm for inference and gave an overview of the basic MCMC
algorithms used in the rest of the book.
In Chap. 3, we considered the Bayesian estimation of the parsimonious but
eÔ¨Äective GARCH(1, 1) model with Normal innovations. We detailed the MCMC
scheme based on the methodology of Nakatsuma [1998, 2000]. An empirical
application to a foreign exchange rate time series was presented where we com-
pared the Bayesian and the ML point estimates. In particular, we showed that
even for a fairly large data set, the estimates and conÔ¨Ådence intervals are dif-
ferent between the methods. Caution is therefore in order when applying the
asymptotic Normal approximation for the model parameters in this case. We
performed a sensitivity analysis to check the robustness of our results with re-
spect to the choice of the priors and tested the residuals for misspeciÔ¨Åcation.
Finally, we compared the theoretical and sample autocorrelograms of the process
and tested the covariance and strict stationarity conditions.
In Chap. 4, we analyzed the linear regression model with conditionally
heteroscedastic errors which allowed the introduction of lagged dependent vari-
ables in the modeling; moreover, we considered the GJR(1, 1) model to account
for asymmetric responses to past shocks in the conditional error variance pro-
cess. We Ô¨Åtted the model to the Standard and Poors 100 (henceforth S&P100)
index log-returns and compared the Bayesian and the ML estimations. We per-
formed a prior sensitivity analysis and tested the residuals for misspeciÔ¨Åcation.
Finally, we tested the covariance stationarity condition and illustrated the dif-
ferences between the unconditional variance of the process obtained through
the Bayesian approach and the delta method. In particular, we showed that the
Bayesian framework leads to a more precise estimate.
In Chap. 5, we extended the linear regression model further with the in-
troduction of Student-t-GJR(1, 1) errors. An empirical application based on the
S&P100 index log-returns was proposed with a comparison between the joint
posterior and the asymptotic Normal approximation of the parameter estimates.

8 Conclusion
157
We performed a prior sensitivity analysis and tested the residuals for misspeci-
Ô¨Åcation. Finally, we analyzed the conditional and unconditional kurtosis of the
underlying time series.
In Chap. 6, we presented some Ô¨Ånancial applications of the Bayesian esti-
mation of GARCH models. We introduced the concept of Value at Risk (hence-
forth VaR) risk measure and proposed a methodology to estimate the density
of this quantity for diÔ¨Äerent risk levels and time horizons. We reviewed some
basics in decision theory and used this framework as a rational justiÔ¨Åcation for
choosing a point estimate of the VaR. We showed how agents facing diÔ¨Äerent
risk perspectives could select their optimal VaR point estimate and documented
substantial diÔ¨Äerences in terms of regulatory capital between individuals. Fi-
nally, we extended our methodology to the Expected Shortfall (henceforth ES)
risk measure.
In Chap. 7, we extended the single-regime GJR model to the regime-
switching GJR model (henceforth MS-GJR); more precisely, we considered an
asymmetric version of the Markov-switching GARCH(1, 1) speciÔ¨Åcation of Haas
et al. [2004]. We introduced a novel MCMC scheme which can be viewed as a
multivariate extension of the sampler proposed by Nakatsuma [1998, 2000]. As
an application, we Ô¨Åtted a single-regime and a Markov-switching GJR model to
the Swiss Market Index log-returns. We used the random permutation sampler
of Fr¬®uhwirth-Schnatter [2001b] to Ô¨Ånd suitable identiÔ¨Åcation constraints for the
MS-GJR model and showed the presence of two distinct volatility regimes in
the time series. By using the Deviance information criterion of Spiegelhalter
et al. [2002] and by estimating the model likelihood using the bridge sampling
technique of Meng and Wong [1996], we showed the in-sample superiority of the
MS-GJR model. To test the predictive performance of the models, we ran a fore-
casting performance analysis based on the VaR. In particular, we compared the
MS-GJR model to a single-regime GJR model estimated on rolling windows and
concluded to the superiority of the MS-GJR speciÔ¨Åcation. Finally, we proposed
a methodology to depict the density of the one-day ahead VaR and presented a
comparison with the traditional ML approach.
This book proposed two main contributions which are of practical relevance
for both market participants and academics. First, we proposed a novel MCMC
scheme to perform the Bayesian estimation of a Markov-switching model with
Student-t innovations and asymmetric GJR speciÔ¨Åcations for the conditional
variance in each regime. It allows to reproduce many stylized facts observed
in Ô¨Ånancial time series, such as volatility clustering, conditional leptokurticity
and Markov-switching dynamics. Furthermore, it helps to identify whether the
leverage eÔ¨Äect is diÔ¨Äerent across regimes. Our multivariate extension of the ap-

158
8 Conclusion
proach proposed by Nakatsuma [1998, 2000] leads to a fast, fully automatic and
eÔ¨Écient estimation procedure compared to alternative approaches such as the
Griddy-Gibbs sampler. Practitioners who need to run the estimation frequently
and/or for a large number of time series should Ô¨Ånd the procedure helpful. Sec-
ond, we provided a manner to approximate the multi-day ahead VaR and ES
densities when the underlying process is described by a GARCH model. Our
methodology gives the possibility to determine the term structure of these risk
measures and to characterize the uncertainty coming from the model parame-
ters. In our empirical application, we documented that the choice of the model
disturbances has a signiÔ¨Åcant impact on the shape of both risk measures‚Äô den-
sities and this eÔ¨Äect gets larger as the time horizon increases. Moreover, the
densities are strongly left-skewed, which implies substantial diÔ¨Äerences in risk
capital allocation for agents facing diÔ¨Äerent risk perspectives (e.g., risk and fund
managers).
Suggestions for further work
This study has raised many questions and suggests interesting further avenues
of research.
First, in light of the results obtained in Chap. 6, additional work is required
to assess the performance of multi-day ahead VaR models. This is essential for
risk management purposes since the multi-day ahead VaR lies at the heart of the
risk capital allocation‚Äôs framework. The development of powerful methodologies
for testing the VaR is the subject of current researches and we refer the reader to
Berkowitz, ChristoÔ¨Äersen, and Pelletier [2006], Kaufmann [2004], Seiler [2006],
Zumbach [2006] for details. A natural extension of our analyses would consider
the model uncertainty in addition to the parameter uncertainty. The Bayesian
approach provides a natural framework for tackling this issue.
Second, regime-switching GARCH models might be compared to the class
of stochastic volatility (henceforth SV) models [see, e.g., Jacquier, Polson, and
Rossi 1994, Kim et al. 1998]. While SV models are highly Ô¨Çexible (two diÔ¨Äerent
processes drive the dynamics of the underlying time series and the dynamics
of the volatility), they are more diÔ¨Écult to estimate eÔ¨Éciently. Determining
whether this additional Ô¨Çexibility results in a superior predictive ability would
therefore be of interest.
Finally, in our study of the Markov-switching GJR model, we have consid-
ered a Ô¨Åxed transition matrix for the state process. Consequently, the expected
persistence of the regimes is constant over time, which is questionable. In a
more general formulation, we could allow the transition probabilities to change

8 Conclusion
159
over time depending on some observables [see, e.g., Bauwens et al. 2006, Gray
1996]. This would allow determining whether some exogenous variables trigger
the switching mechanism of the volatility process. The transition probabilities
could also depend on the past level of the volatility. In this case, we could re-
produce an additional feature of the volatility behavior, namely the fact that
the probability of returning to a normal (i.e., low or medium) volatility regime
increases after a high upward jump in the volatility level [see, e.g., Bauwens
et al. 2006, Dueker 1997].

A
Recursive Transformations
In this appendix, we demonstrate the recursive transformations introduced in
Chaps. 3, 4 and 5 which are used to express the function zt(Œ±) as a linear
function of parameter Œ±. The process for the conditional variance is based on
observations {yt}. Results can be straightforwardly extended when a linear com-
ponent is included in the model by considering instead the process {ut} where
ut .= yt ‚àíx‚Ä≤Œ≥.
A.1 The GARCH(1, 1) model with Normal innovations
First, let us recall that, in the case of the GARCH(1, 1) process, the expression
for the conditional variance of yt is given by:
ht .= Œ±0 + Œ±1y2
t‚àí1 + Œ≤ht‚àí1
where h0 = y0 .= 0 for convenience. As shown in Sect. 3.2, the GARCH(1, 1)
model with Normal innovations can be expressed as an ARMA(1, 1) model for
the squared observations {y2
t } and approximated as follows:
y2
t = Œ±0 + (Œ±1 + Œ≤)y2
t‚àí1 ‚àíŒ≤zt‚àí1 + zt
where {zt} is a Martingale DiÔ¨Äerence process. Let us deÔ¨Åne vt .= y2
t for notational
purposes. The variable zt can then be written as:
zt = vt ‚àíŒ±0 ‚àí(Œ±1 + Œ≤)vt‚àí1 + Œ≤zt‚àí1
(A.1)
where v0 = z0 = 0.
Proposition A.1. Upon deÔ¨Åning the following recursive transformations:
l‚àó
t
.= 1 + Œ≤ l‚àó
t‚àí1
v‚àó
t
.= vt‚àí1 + Œ≤ v‚àó
t‚àí1
(A.2)
where l‚àó
0 = v‚àó
0
.= 0, expression (A.1) can be written as follows:

162
A Recursive Transformations
zt = vt ‚àí(l‚àó
t v‚àó
t )Œ±
(A.3)
where Œ± .= (Œ±0 Œ±1)‚Ä≤. The function zt(Œ±) in (A.1) can therefore be expressed as
a linear function of the 2 √ó 1 vector Œ±.
Proof. By induction:
Beginning step:
For t = 1, we have:
v1 ‚àí(l‚àó
1 v‚àó
1)Œ±
(A.2)
=
v1 ‚àí(1 0)Œ± = v1 ‚àíŒ±0
(A.1)
=
z1 .
Assumption step:
Let us assume that expression (A.3) is satisÔ¨Åed for t = k.
Induction step:
For t = k + 1 we have:
vk+1 ‚àí(l‚àó
k+1 v‚àó
k+1)Œ±
(A.2)
=
vk+1 ‚àí(1 + Œ≤l‚àó
k
vk + Œ≤v‚àó
k)Œ±
=
vk+1 ‚àíŒ±0 ‚àíŒ±1vk ‚àíŒ≤(Œ±0l‚àó
k + Œ±1v‚àó
k)
=
vk+1 ‚àíŒ±0 ‚àíŒ±1vk ‚àíŒ≤(l‚àó
k v‚àó
k)Œ±
(A.3)
=
vk+1 ‚àíŒ±0 ‚àíŒ±1vk ‚àíŒ≤(vk ‚àízk)
=
vk+1 ‚àíŒ±0 ‚àí(Œ±1 + Œ≤)vk + Œ≤zk
(A.1)
=
zk+1 .
‚äì‚äî
A.2 The GJR(1, 1) model with Normal innovations
First, let us recall that, in the case of the GJR(1, 1) model, the expression for
the conditional variance of yt is given by:
ht .= Œ±0 + (Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0})y2
t‚àí1 + Œ≤ht‚àí1
where h0 = y0 .= 0 for convenience. As shown in Sect. 4.2.2, the GJR(1, 1)
model with Normal innovations can be transformed for the squared observations
{y2
t } and approximated as follows:
y2
t = Œ±0 + (Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0} + Œ≤)y2
t‚àí1 ‚àíŒ≤zt‚àí1 + zt
where {zt} is a Martingale DiÔ¨Äerence process. Let us deÔ¨Åne vt .= y2
t for notational
purposes. Then, the variable zt can be written as:
zt = vt ‚àíŒ±0 ‚àí(Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0} + Œ≤)vt‚àí1 + Œ≤zt‚àí1
(A.4)
where v0 = z0 = 0.

A.3 The GJR(1, 1) model with Student-t innovations
163
Proposition A.2. Upon deÔ¨Åning the following recursive transformations:
l‚àó
t
.= 1 + Œ≤l‚àó
t‚àí1
v‚àó
t
.= vt‚àí1I{yt‚àí1‚©æ0} + Œ≤ v‚àó
t‚àí1
v‚àó‚àó
t
.= vt‚àí1I{yt‚àí1<0} + Œ≤ v‚àó‚àó
t‚àí1
(A.5)
where l‚àó
0 = v‚àó
0 = v‚àó‚àó
0
.= 0, expression (A.4) can be written as follows:
zt = vt ‚àí(l‚àó
t v‚àó
t v‚àó‚àó
t )Œ±
(A.6)
where Œ± .= (Œ±0 Œ±1 Œ±2)‚Ä≤. The function zt(Œ±) in (A.4) can therefore be expressed
as a linear function of the 3 √ó 1 vector Œ±.
Proof. By induction:
Beginning step:
For t = 1, we have:
v1 ‚àí(l‚àó
1 v‚àó
1 v‚àó‚àó
1 )Œ±
(A.5)
=
v1 ‚àí(1 0 0)Œ± = v1 ‚àíŒ±0
(A.4)
=
z1 .
Assumption step:
Let us assume that expression (A.6) is satisÔ¨Åed for t = k.
Induction step:
For t = k + 1, we have:
vk+1 ‚àí(l‚àó
k+1 v‚àó
k+1 v‚àó‚àó
k+1)Œ±
(A.5)
=
vk+1 ‚àí(1 + Œ≤l‚àó
k
vkI{yk‚©æ0} + Œ≤v‚àó
k
vkI{yk<0} + Œ≤v‚àó‚àó
k )Œ±
=
vk+1 ‚àíŒ±0 ‚àíŒ±1vkI{yk‚©æ0} ‚àíŒ±2vkI{yk<0}
‚àíŒ≤(Œ±0l‚àó
k + Œ±1v‚àó
k + Œ±2v‚àó‚àó
k )
=
vk+1 ‚àíŒ±0 ‚àíŒ±1vkI{yk‚©æ0} ‚àíŒ±2vkI{yk<0} ‚àíŒ≤(l‚àó
k v‚àó
k v‚àó‚àó
k )Œ±
(A.6)
=
vk+1 ‚àíŒ±0 ‚àíŒ±1vkI{yk‚©æ0} ‚àíŒ±2vkI{yk<0} ‚àíŒ≤(vk ‚àízk)
=
vk+1 ‚àíŒ±0 ‚àí(Œ±1I{yk‚©æ0} + Œ±2I{yk<0} + Œ≤)vk + Œ≤zk
(A.4)
=
zk+1 .
‚äì‚äî
A.3 The GJR(1, 1) model with Student-t innovations
As shown in Sect. 5.2.2, the GJR(1, 1) model with Student-t innovations, de-
noted by {yt}, can be transformed in a new sequence {vt}, where vt .= y2
t
œÑt with
œÑt .= œñtœ± and œ± .= ŒΩ‚àí2
ŒΩ . The process {vt} can then be approximated as follows:
vt = Œ±0 + (Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0})œÑt‚àí1vt‚àí1 + Œ≤vt‚àí1 ‚àíŒ≤zt‚àí1 + zt

164
A Recursive Transformations
where {zt} is a Martingale DiÔ¨Äerence process. From this expression, the variable
zt can be written as:
zt = vt ‚àíŒ±0 ‚àí

(Œ±1I{yt‚àí1‚©æ0} + Œ±2I{yt‚àí1<0})œÑt‚àí1 + Œ≤

vt‚àí1 + Œ≤zt‚àí1
(A.7)
where v0 = z0 = 0.
Proposition A.3. Upon deÔ¨Åning the following recursive transformations:
l‚àó
t
.= 1 + Œ≤l‚àó
t‚àí1
v‚àó
t
.= y2
t‚àí1I{yt‚àí1‚©æ0} + Œ≤ v‚àó
t‚àí1
v‚àó‚àó
t
.= y2
t‚àí1I{yt‚àí1<0} + Œ≤ v‚àó‚àó
t‚àí1
(A.8)
where l‚àó
0 = v‚àó
0 = v‚àó‚àó
0
.= 0, expression (A.7) can be written as follows:
zt = vt ‚àí(l‚àó
t v‚àó
t v‚àó‚àó
t )Œ±
(A.9)
where Œ± .= (Œ±0 Œ±1 Œ±2)‚Ä≤. The function zt(Œ±) in (A.7) can therefore be expressed
as a linear function of the 3 √ó 1 vector Œ±.
Proof. By induction:
Beginning step:
For t = 1, we have:
v1 ‚àí(l‚àó
1 v‚àó
1 v‚àó‚àó
1 )Œ±
(A.8)
=
v1 ‚àí(1 0 0)Œ± = v1 ‚àíŒ±0
(A.7)
=
z1 .
Assumption step:
Let us assume that expression (A.9) is satisÔ¨Åed for t = k.
Induction step:
For t = k + 1, we have:
vk+1 ‚àí(l‚àó
k+1 v‚àó
k+1 v‚àó‚àó
k+1)Œ±
(A.8)
=
vk+1 ‚àí(1 + Œ≤l‚àó
k
y2
kI{yk‚©æ0} + Œ≤v‚àó
k
y2
kI{yk<0} + Œ≤v‚àó‚àó
k )Œ±
=
vk+1 ‚àíŒ±0 ‚àíŒ±1y2
kI{yk‚©æ0} ‚àíŒ±2y2
kI{yk<0}
‚àíŒ≤(Œ±0l‚àó
k + Œ±1v‚àó
k + Œ±2v‚àó‚àó
k )
=
vk+1 ‚àíŒ±0 ‚àíŒ±1y2
kI{yk‚©æ0} ‚àíŒ±2y2
kI{yk<0} ‚àíŒ≤(l‚àó
k v‚àó
k v‚àó‚àó
k )Œ±
(A.9)
=
vk+1 ‚àíŒ±0 ‚àíŒ±1y2
kI{yk‚©æ0} ‚àíŒ±2y2
kI{yk<0} ‚àíŒ≤(vk ‚àízk)
=
vk+1 ‚àíŒ±0 ‚àíŒ±1y2
kI{yk‚©æ0} ‚àíŒ±2y2
kI{yk<0} ‚àíŒ≤vk + Œ≤zk
=
vk+1 ‚àíŒ±0 ‚àí

(Œ±1I{yk‚©æ0} + Œ±2I{yk<0})œÑk + Œ≤

vk + Œ≤zk
(A.7)
=
zk+1 .
‚äì‚äî

B
Equivalent SpeciÔ¨Åcation
In this appendix, we demonstrate the equivalent speciÔ¨Åcation introduced at the
end of Sect. 5.2.
Proposition B.1. The following model:
yt = Œµt(œñtht)1/2
for t = 1, . . . , T
Œµt
iid
‚àºN(0, 1)
œñt
iid
‚àºIG
ŒΩ
2, ŒΩ ‚àí2
2

(B.1)
where ht .= ht(Œ±, Œ≤) is a GARCH scedastic function, is equivalent to:
yt = Œµth1/2
t
for t = 1, . . . , T
Œµt
iid
‚àºS‚àó(0, 1)
where S‚àó(0, 1) denotes the standardized Student-t density, i.e., its variance is
one.
Proof. First, let us regroup the model parameters into œà .= (Œ±, Œ≤, ŒΩ) for nota-
tional purposes. In speciÔ¨Åcation (B.1), the variables œñt are independent and
identically distributed from an Inverted Gamma density with parameters ŒΩ
2 and
ŒΩ‚àí2
2 :
p(œñt | ŒΩ) =
ŒΩ ‚àí2
2
 ŒΩ
2 h
Œì
ŒΩ
2
i‚àí1
œñ
‚àíŒΩ
2 ‚àí1
t
exp

‚àí(ŒΩ ‚àí2)
2œñt

and the joint density of the T √ó1 vector œñ .= (œñ1 ¬∑ ¬∑ ¬∑ œñT )‚Ä≤ is therefore given by:
p(œñ | ŒΩ) =
ŒΩ ‚àí2
2
 T ŒΩ
2 h
Œì
ŒΩ
2
i‚àíT
T
Y
t=1
œñ
‚àíŒΩ
2 ‚àí1
t
exp

‚àí(ŒΩ ‚àí2)
2œñt

.
(B.2)
Based on the T √ó 1 vector of observations y .= (y1 ¬∑ ¬∑ ¬∑ yT )‚Ä≤, we can express the
likelihood function of (œà, œñ) as follows:

166
B Equivalent SpeciÔ¨Åcation
L(œà, œñ | y) ‚àù
T
Y
t=1
(œñtht)‚àí1/2 exp

‚àí1
2
y2
t
œñtht

and, by using the Bayes theorem, we obtain the following joint posterior:
p(œà, œñ | y) ‚àù
ŒΩ ‚àí2
2
 T ŒΩ
2 h
Œì
ŒΩ
2
i‚àíT
 T
Y
t=1
h‚àí1/2
t
!
√ó
T
Y
t=1
œâ
‚àí(ŒΩ+3)
2
t
exp

‚àí1
2
y2
t
œâtht

.
(B.3)
Now, by using the following result:
Z ‚àû
0
x‚àía/2 exp

‚àíb
2x

dx = Œì
a ‚àí2
2

√ó
2
b
 a‚àí2
2
we can integrate (B.3) with respect to vector œñ to get the following expression:
p(œà | y) ‚àù
ŒΩ ‚àí2
2
 T ŒΩ
2 h
Œì
ŒΩ
2
i‚àíT 
Œì
ŒΩ + 1
2
T
√ó 2
T (ŒΩ+1)
2
T
Y
t=1
h‚àí1/2
t
b
‚àí(ŒΩ+1)
2
t
(B.4)
where:
bt .= y2
t
ht
+ (ŒΩ ‚àí2) = (ŒΩ ‚àí2) √ó

1 +
y2
t
(ŒΩ ‚àí2)ht

.
Some simpliÔ¨Åcations of expression (B.4) yield:
"
Œì
  ŒΩ+1
2

Œì
  ŒΩ
2

(ŒΩ ‚àí2)1/2
#T
T
Y
t=1
h‚àí1/2
t

1 +
y2
t
(ŒΩ ‚àí2)ht
‚àí(ŒΩ+1)
2
which if proportional to the likelihood function of parameters œà when observa-
tions yt
iid
‚àºS‚àó(0, ht) where ht .= ht(Œ±, Œ≤).
‚äì‚äî
The speciÔ¨Åcation (B.1) gives an additional way of handling the Bayesian estima-
tion of GARCH models with Student-t innovations. It has the appealing aspect
that no additional scale factor œ± .= ŒΩ‚àí2
ŒΩ
needs to be included in the modeling.
However, the simulation scheme presented in Deschamps [2006], Geweke [1993]
needs to be slightly modiÔ¨Åed, as shown hereafter.
In our application, we aim to generate eÔ¨Éciently draws for the degrees of
freedom parameter ŒΩ. The target density is obtained as follows:

B Equivalent SpeciÔ¨Åcation
167
p(ŒΩ | œñ) ‚àùp(œñ | ŒΩ)p(ŒΩ)
=
ŒΩ ‚àí2
2
 T ŒΩ
2 h
Œì
ŒΩ
2
i‚àíT
 T
Y
t=1
œñ
‚àí(ŒΩ+2)
2
t
!
√ó exp
"
‚àí(ŒΩ ‚àí2)
2
T
X
t=1
œñ‚àí1
t
#
Œª exp

‚àíŒª(ŒΩ ‚àíŒ¥)

I{ŒΩ>Œ¥}
where we can express QT
t=1 œñ
‚àí(ŒΩ+2)
2
t
as:
T
Y
t=1
œñ
‚àí(ŒΩ+2)
2
t
=
T
Y
t=1
exp

ln œñ
‚àí(ŒΩ+2)
2
t

=
T
Y
t=1
exp

‚àí(ŒΩ + 2)
2
ln œñt

= exp
"
‚àí(ŒΩ + 2)
2
T
X
t=1
ln œñt
#
= exp
"
‚àíŒΩ
2
T
X
t=1
ln œñt ‚àí
T
X
t=1
ln œñt
#
‚àùexp
"
‚àíŒΩ
2
T
X
t=1
ln œñt
#
.
This allows to express the kernel of the target density as follows:
k(ŒΩ) .=
ŒΩ ‚àí2
2
 T ŒΩ
2 h
Œì
ŒΩ
2
i‚àíT
exp[‚àíœïŒΩ]I{ŒΩ>Œ¥}
where:
œï .= 1
2
T
X
t=1

ln œñt + œñ‚àí1
t

+ Œª .
Note that, since the function ln œñ + œñ‚àí1 is minimized at œñ = 1, we have that
œï ‚©æT
2 + Œª > T
2 .
Following Deschamps [2006], the sampling density is a translated Exponen-
tial with kernel density function given by:
g(ŒΩ; ¬µ, Œ¥) .= ¬µ exp [‚àí¬µ(ŒΩ ‚àíŒ¥)] I{ŒΩ>Œ¥}
(B.5)
where the parameter ¬µ is chosen to maximize the acceptance probability. Fol-
lowing Geweke [1993], we can determine the value for this parameter. Given the
usual regularity conditions, a necessary condition is that ¬µ is part of a solution
of the following system:

168
B Equivalent SpeciÔ¨Åcation
‚àÇ
‚àÇŒΩ

ln k(ŒΩ) ‚àíln g(ŒΩ; ¬µ, Œ¥)

= 0
(B.6a)
‚àÇ
‚àÇ¬µ ln g(ŒΩ; ¬µ, Œ¥) = 0 .
(B.6b)
Expliciting (B.6a) yields:
T
2

ln
ŒΩ ‚àí2
2

+

ŒΩ
ŒΩ ‚àí2

‚àíŒ®
ŒΩ
2

‚àíœï + ¬µ = 0
(B.7)
where Œ®(z)
.=
d ln Œì(z)
dz
denotes the Digamma function, while solving (B.6b)
yields:
ŒΩ = 1
¬µ + Œ¥ = 1 + ¬µŒ¥
¬µ
.
(B.8)
Furthermore, we note that in expression (B.7), the function:
ln
ŒΩ ‚àí2
2

+

ŒΩ
ŒΩ ‚àí2

‚àíŒ®
ŒΩ
2

is monotone decreasing from ‚àûto 1 on the ]2, ‚àû[ interval. Hence, since œï > T
2 ,
there exists an unique ¬µ satisfying (B.7). Now, inserting (B.8) in expression (B.7)
yields:
T
2

ln
1 + ¬µ(Œ¥ ‚àí2)
2¬µ

+
1 + ¬µŒ¥
1 + ¬µ(Œ¥ ‚àí2) + Œ®
1 + ¬µŒ¥
2¬µ

+ ¬µ ‚àíœï = 0
and solving for ¬µ gives the optimal parameter ¬Ø¬µ for the eÔ¨Écient sampling scheme.
The value ¬Ø¬µ can be found by standard iterative methods. Then, a candidate ŒΩ‚ãÜ
is sampled from (B.5) with parameter ¬Ø¬µ and accepted with probability:
p‚ãÜ.=
k(ŒΩ‚ãÜ)
s(¬Ø¬µ, Œ¥)g(ŒΩ‚ãÜ; ¬Ø¬µ, Œ¥)
where s(¬µ, Œ¥) is given by:
s(¬µ, Œ¥) .= k
1 + ¬µŒ¥
¬µ
 
g
1 + ¬µŒ¥
¬µ
; ¬µ, Œ¥
‚àí1
=
1 + ¬µ(Œ¥ ‚àí2)
2¬µ
 T (1+¬µŒ¥)
2¬µ

Œì
1 + ¬µŒ¥
2¬µ
‚àíT exp
h
1 ‚àíœï(1+¬µŒ¥)
¬µ
i
¬µ
.
Substituting for k(ŒΩ‚ãÜ), s(¬µ, Œ¥) and g(ŒΩ‚ãÜ; ¬µ, Œ¥) in the expression of the acceptance
probability yields:

B Equivalent SpeciÔ¨Åcation
169
p‚ãÜ=
ŒΩ‚ãÜ‚àí2
2
 T ŒΩ‚ãÜ
2

Œì
ŒΩ‚ãÜ
2
‚àíT
exp[‚àíœïŒΩ‚ãÜ]
1 + ¬Ø¬µ(Œ¥ ‚àí2)
2¬Ø¬µ
‚àíT (1+¬Ø
¬µŒ¥)
2¬Ø
¬µ
√ó

Œì
1 + ¬Ø¬µŒ¥
2¬Ø¬µ
T
¬Ø¬µ exp
œï(1 + ¬Ø¬µŒ¥)
¬µ
‚àí1
 exp

¬Ø¬µ(ŒΩ‚ãÜ‚àíŒ¥)

¬Ø¬µ
=
Ô£Æ
Ô£∞
Œì

1+¬Ø¬µŒ¥
2¬Ø¬µ

Œì
  ŒΩ‚ãÜ
2

Ô£π
Ô£ª
T ŒΩ‚ãÜ‚àí2
2
 T ŒΩ‚ãÜ
2
1 + ¬Ø¬µ(Œ¥ ‚àí2)
2¬Ø¬µ
‚àíT (1+¬Ø
¬µŒ¥)
2¬Ø
¬µ
√ó exp

(ŒΩ‚ãÜ‚àíŒ¥)(¬Ø¬µ ‚àíœï) + œï
¬Ø¬µ ‚àí1

.

C
Conditional Moments
In this appendix, we demonstrate the propositions for the conditional moments
of the cumulative return yt,s .= Ps
i=1 yt+i used in Sect. 6.2.2. We consider the
case where the process {yt} is described by a GARCH(1, 1) model for ease of
exposition but the methodology can be extended, upon modiÔ¨Åcations, to higher
order GARCH models as well as asymmetric speciÔ¨Åcations. We recall that the
scedastic function of the GARCH(1, 1) model is given by:
ht .= Œ±0 + Œ±1y2
t‚àí1 + Œ≤ht‚àí1
where h0 = y0 .= 0 for convenience. For the model disturbances, we consider
standardized Normal and Student-t innovations. We deÔ¨Åne Et(‚Ä¢) .= E(‚Ä¢ | Ft)
and suppress the dependence of the model parameters for notational pur-
poses. The GARCH(1, 1) parameters Œ± .= (Œ±0 Œ±1)‚Ä≤ and Œ≤ are regrouped into
œà .= (Œ±, Œ≤). In the case of Student-t innovations, œà .= (Œ±, Œ≤, ŒΩ). Moreover, the
p-th conditional moment of yt,s is denoted by Œ∫p.
The following properties will be used henceforth:
A. the errors Œµt are iid (i.e., independent and identically distributed);
B. Et(Œµt+i) = 0 for i ‚©æ1 (i.e., centered distribution);
C. Et(Œµ2
t+i) = 1 for i ‚©æ1 (i.e., unit variance);
D. Et(Œµ3
t+i) = 0 for i ‚©æ1 (i.e., symmetric distribution);
E. the conditional variance ht is known given Ft‚àí1, i.e., it is pre-
dictable with respect to the natural Ô¨Åltration of the process {yt}.
To keep the calculations similar for both Normal and Student-t innovations,
we assume unit variance for the disturbances Œµt as emphasized in property C.
This has an implication for the fourth conditional moment of the innovations
Œ∫Œµ
.= Et(Œµ4
t+i) for i ‚©æ1. Indeed, in the Normal case, Œ∫Œµ = 3 while in the
normalized Student-t case, Œ∫Œµ = 3(ŒΩ ‚àí2)/(ŒΩ ‚àí4). In comparison, the fourth
moment of the usual Student-t is 3ŒΩ2/(ŒΩ ‚àí4)(ŒΩ ‚àí2).

172
C Conditional Moments
Proposition C.1 (First conditional moment). For horizon s ‚©æ1, the value
of the Ô¨Årst conditional moment Œ∫1 is zero.
Proof.
Œ∫1 .= Et(yt,s) = Et
 s
X
i=1
yt+i
!
=
s
X
i=1
Et(yt+i) = 0
since for 1 ‚©Ωi ‚©Ωs we have:
Et(yt+i) = Et(Œµt+ih1/2
t+i)
A= Et(Œµt+i)Et(h1/2
t+i)
B= 0 .
‚äì‚äî
Proposition C.2 (Second conditional moment).
For horizon s ‚©æ2, the
value of the second conditional moment Œ∫2 is:
Œ∫2 =
s
X
i=1
Et(ht+i)
where Et(ht+i) = Œ±0 + œÅ1Et(ht+i‚àí1) with œÅ1 .= (Œ±1 + Œ≤).
Proof. For horizon s ‚©æ2, the second power of the cumulative log-returns yt,s is
given by:
y2
t+s =
X
i1, ... ,is
i1+ ... +is=2
2 !
i1! . . . is! √ó yi1
t+1 ¬∑ ¬∑ ¬∑ yis
t+s
=
s
X
i=1
y2
t+i + 2
X
1‚©Ωi,j‚©Ωs
i<j
yt+i yt+j
and the second conditional moment Œ∫2 is obtained by taking the conditional
expectation as follows:
Œ∫2 =
s
X
i=1
Et(y2
t+i) + 2
X
1‚©Ωi,j‚©Ωs
i<j
Et(yt+i yt+j) .
(C.1)
Let us consider the Ô¨Årst term in expression (C.1). For 1 ‚©Ωi ‚©Ωs, we have:
Et(y2
t+i) = Et(Œµ2
t+iht+i)
A= Et(Œµ2
t+i)Et(ht+i)
C= Et(ht+i) .
For the second term in expression (C.1), since i < j, we have:
Et(yt+i yt+j) = Et(yt+i Œµt+jh1/2
t+j)
A= Et(yt+ih1/2
t+j)Et(Œµt+j)
B= 0 .
Hence, expression (C.1) simpliÔ¨Åes to Œ∫2 = Ps
i=1 Et(ht+i) where Et(ht+i) can be
expressed recursively as follows:

C Conditional Moments
173
Et(ht+i) = Œ±0 + œÅ1Et(ht+i‚àí1)
with Et(ht+1) = ht+1 since this value is known given Ft. The parameter œÅ1 is a
function of the model parameters œà. Its expression is found by noting Ô¨Årst that
the conditional variance at time t can be written as follows:
ht+i = Œ±0 + Œ±1y2
t+i‚àí1 + Œ≤ht+i‚àí1
= Œ±0 + (Œ±1Œµ2
t+i‚àí1 + Œ≤) ht+i‚àí1
= Œ±0 + œït+i‚àí1ht+i‚àí1
(C.2)
where œït
.= (Œ±1Œµ2
t + Œ≤). By taking the conditional expectation of (C.2), we
obtain:
Et(ht+i) = Œ±0 + Et(œït+i‚àí1ht+i‚àí1)
A= Œ±0 + Et(œït+i‚àí1)Et(ht+i‚àí1)
= Œ±0 + œÅ1Et(ht+i‚àí1)
where œÅ1 .= Et(œït+i‚àí1) = Et(Œ±1Œµ2
t+i‚àí1 + Œ≤) = Œ±1 Et(Œµ2
t+i‚àí1) + Œ≤
C= Œ±1 + Œ≤.
‚äì‚äî
Proposition C.3 (Third conditional moment).
For horizon s ‚©æ3, the
value of the third conditional moment Œ∫3 is zero.
Proof. For s ‚©æ3, the third power of the cumulative log-returns yt,s is given by:
y3
t,s =
X
i1, ... ,is
i1+ ... +is=3
3 !
i1! . . . is! √ó yi1
t+1 ¬∑ ¬∑ ¬∑ yis
t+s
=
s
X
i=1
y3
t+i + 3
X
1‚©Ωi,j‚©Ωs
iÃ∏=j
y2
t+i yt+j + 6
X
1‚©Ωi,j,k‚©Ωs
i<j<k
yt+i yt+j yt+k
and the third conditional moment Œ∫3 is obtained by taking the conditional ex-
pectation as follows:
Œ∫3 =
s
X
i=1
Et(y3
t+i) + 3
X
1‚©Ωi,j‚©Ωs
iÃ∏=j
Et(y2
t+i yt+j) + 6
X
1‚©Ωi,j,k‚©Ωs
i<j<k
Et(yt+i yt+j yt+k) .
(C.3)
Let us consider the Ô¨Årst term in expression (C.3). For 1 ‚©Ωi ‚©Ωs, we have:
Et(y3
t+i) = Et(Œµ3
t+ih3/2
t+i)
A= Et(Œµ3
t+i)Et(h3/2
t+i)
D= 0 .
For the second term, we need to distinguish two cases. First, when i < j, we
obtain:
Et(y2
t+i yt+j) = Et(y2
t+i Œµt+jh1/2
t+j)
A= Et(y2
t+ih1/2
t+j)Et(Œµt+j)
B= 0 .

174
C Conditional Moments
In the case where j < i, we have:
Et(y2
t+i yt+j) = Et(Œµ2
t+iht+i yt+j)
A= Et(Œµ2
t+i)Et(ht+iyt+j)
C= Et(ht+iyt+j)
= Et(ht+iŒµt+jh1/2
t+j)
= Et
 Et+j‚àí1[ht+iŒµt+jh1/2
t+j]

E= Et
 h1/2
t+j Et+j‚àí1[ht+iŒµt+j]

(C.4)
where the conditional expectation Et+j‚àí1[‚Ä¢] in (C.4) can be expressed as:
Et+j‚àí1[Œµt+jht+i]
(C.2)
=
Et+j‚àí1

Œµt+j(Œ±0 + œït+i‚àí1ht+i‚àí1)

B=
Et+j‚àí1[Œµt+jœït+i‚àí1ht+i‚àí1]
A=
Et+j‚àí1[Œµt+jht+i‚àí1]Et+j‚àí1[œït+i‚àí1]
=
œÅ1Et+j‚àí1[Œµt+jht+i‚àí1]
...
=
œÅi‚àíj
1
Et+j‚àí1[Œµt+jœït+jht+j]
E=
œÅi‚àíj
1
ht+jEt+j‚àí1[Œµt+jœït+j]
=
0 .
(C.5)
The last equality follows from:
Et+j‚àí1

Œµt+jœït+j

= Et+j‚àí1

Œµt+j(Œ±1Œµ2
t+j + Œ≤)

= Œ±1 Et+j‚àí1[Œµ3
t+j] + Œ≤ Et+j‚àí1[Œµt+j]
B,D
= 0 .
Finally, let us consider the last term in expression (C.3). Since i < j < k, we
have:
Et(yt+i yt+j yt+k)
A= Et(yt+i yt+jh1/2
t+k)Et(Œµt+k)
B= 0 .
Each term in expression (C.3) vanishes so that the third conditional moment is
zero.
‚äì‚äî
Proposition C.4 (Fourth conditional moment).
For horizon s ‚©æ4, the
value of the fourth conditional moment Œ∫4 is:
Œ∫4 = Œ∫Œµ
s
X
i=1
Et(h2
t+i) + 6
s‚àí1
X
i=1
s
X
j=i+1
Et(y2
t+i y2
t+j)

C Conditional Moments
175
where:
Et(h2
t+i) = Œ±2
0 + œÑ1 Et(ht+i‚àí1) + œÑ2 Et(h2
t+i‚àí1)
Et(y2
t+i y2
t+j) = Œ±0
 
1 ‚àíœÅj‚àíi
1
1 ‚àíœÅ1
!
Et(ht+i) + œÅj‚àíi‚àí1
1
œÅ2 Et(h2
t+i)
with œÅ1 .= (Œ±1 + Œ≤), œÅ2 .= (Œ∫ŒµŒ±1 + Œ≤), œÑ1 .= 2Œ±0œÅ1 and œÑ2 .= Œ∫ŒµŒ±2
1 + Œ≤(2Œ±1 + Œ≤).
Proof. For s ‚©æ4, the fourth power of the cumulative log-returns yt,s is given
by:
y4
t,s =
X
i1, ... ,is
i1+ ... +is=4
4 !
i1! . . . is! √ó yi1
t+1 ¬∑ ¬∑ ¬∑ yis
t+s
=
s
X
i=1
y4
t+i + 4
X
1‚©Ωi,j‚©Ωs
iÃ∏=j
y3
t+i yt+j + 6
X
1‚©Ωi,j‚©Ωs
i<j
y2
t+i y2
t+j
+ 6
X
1‚©Ωi,j,k‚©Ωs
iÃ∏=jÃ∏=k
y2
t+i yt+j yt+k
+ 24
X
1‚©Ωi,j,k,l‚©Ωs
i<j<k<l
yt+i yt+j yt+k yt+l .
and the fourth conditional moment Œ∫4 is obtained by taking the conditional
expectation as follows:
Œ∫4 =
s
X
i=1
Et(y4
t+i) + 4
X
1‚©Ωi,j‚©Ωs
iÃ∏=j
Et(y3
t+i yt+j) + 6
X
1‚©Ωi,j‚©Ωs
i<j
Et(y2
t+i y2
t+j)
+ 6
X
1‚©Ωi,j,k‚©Ωs
iÃ∏=jÃ∏=k
Et(y2
t+i yt+j yt+k)
+ 24
X
1‚©Ωi,j,k,l‚©Ωs
i<j<k<l
Et(yt+i yt+j yt+k yt+l)
(C.6)
Let us Ô¨Årst consider the terms in (C.6) which vanish. For the second term in
expression (C.6), when i < j, we have:
Et(y3
t+i yt+j) = Et(y3
t+i Œµt+jh1/2
t+j)
A= Et(y3
t+i h1/2
t+j)Et(Œµt+j)
B= 0
while in the case where j < i, we obtain:
Et(y3
t+i yt+j) = Et(Œµ3
t+ih3/2
t+i yt+j)
A= Et(Œµ3
t+i)Et(h3/2
t+i yt+j)
D= 0 .

176
C Conditional Moments
Let us now consider the fourth term in expression (C.6). Assuming that i < j < k
yields:
Et(y2
t+i yt+j yt+k)
A= Et(y2
t+i yt+j h1/2
t+k)Et(Œµt+k)
B= 0 .
The same holds for i < k < j, j < i < k and k < i < j. It remains to consider
the cases where i is the greatest integer, i.e., k < j < i and j < k < i. By
assuming (without loss of generality) that j < k < i we have:
Et(y2
t+i yt+j yt+k)
=
Et(Œµ2
t+iht+i yt+j yt+k)
A=
Et(Œµ2
t+i)Et(ht+i yt+j yt+k)
C=
Et(ht+i yt+j yt+k)
=
Et
 Et+j‚àí1[ht+i Œµt+jh1/2
t+j yt+k]

F=
Et
 h1/2
t+j Et+j‚àí1[ht+i Œµt+j yt+k]

=
Et

h1/2
t+j Et+j‚àí1

Et+k‚àí1{ht+i Œµt+j Œµt+kh1/2
t+k}

F=
Et

h1/2
t+j Et+j‚àí1

h1/2
t+k Œµt+j Et+k‚àí1{ht+i Œµt+k}

(C.5)
=
0 .
Finally, let us consider the last term in expression (C.6). By assuming (without
loss of generality) that i < j < k < l, we have:
Et(yt+i yt+j yt+k yt+l)
A= Et(yt+i yt+j yt+k h1/2
t+l)Et(Œµt+l)
B= 0 .
With these intermediate results, expression (C.6) can be simpliÔ¨Åed as follows:
Œ∫4 =
s
X
i=1
Et(y4
t+i) + 6
X
1‚©Ωi,j‚©Ωs
i<j
Et(y2
t+i y2
t+j)
=
s
X
i=1
Et(y4
t+i) + 6
s‚àí1
X
i=1
s
X
j=i+1
Et(y2
t+i y2
t+j) .
(C.7)
Let us explicit the Ô¨Årst term in expression (C.7). We have:
Et(y4
t+i) = Et(Œµ4
t+ih2
t+i)
A= Et(Œµ4
t+i)Et(h2
t+i) = Œ∫Œµ Et(h2
t+i) .
The term Et(h2
t+i) can be estimated by recursion as follows:

C Conditional Moments
177
Et(h2
t+i)
(C.2)
=
Et
 [Œ±0 + œït+i‚àí1ht+i‚àí1]2
=
Œ±2
0 + 2Œ±0 Et(œït+i‚àí1ht+i‚àí1) + Et(œï2
t+i‚àí1h2
t+i‚àí1)
A=
Œ±2
0 + 2Œ±0 Et(œït+i‚àí1)Et(ht+i‚àí1) + Et(œï2
t+i‚àí1)Et(h2
t+i‚àí1)
=
Œ±2
0 + œÑ1 Et(ht+i‚àí1) + œÑ2 Et(h2
t+i‚àí1)
(C.8)
where œÑ1 .= 2Œ±0œÅ1, œÅ1 .= Et(œït+i‚àí1) and œÑ2 .= Et(œï2
t+i‚àí1). Expression (C.8) can
be computed recursively since ht+1 is known given Ft. Furthermore, we can
explicit the term œÑ2 as follows:
œÑ2
.= Et(œï2
t+i‚àí1)
= Et(Œ±2
1Œµ4
t+i‚àí1 + 2Œ±1Œ≤Œµ2
t+i‚àí1 + Œ≤2)
= Œ±2
1 Et(Œµ4
t+i‚àí1) + 2Œ±1Œ≤ Et(Œµ2
t+i‚àí1) + Œ≤2
C,E
= Œ∫ŒµŒ±2
1 + 2Œ±1Œ≤ + Œ≤2
= Œ∫ŒµŒ±2
1 + Œ≤(2Œ±1 + Œ≤) .
It remains to consider the last term in expression (C.6). Since i < j, we have:
Et(y2
t+i y2
t+j) = Et(y2
t+i Œµ2
t+jht+j)
A= Et(y2
t+i ht+j)Et(Œµ2
t+j)
C= Et(y2
t+i ht+j)
= Et
 Et+i‚àí1[Œµ2
t+iht+i ht+j]

F= Et
 ht+i Et+i‚àí1[Œµ2
t+i ht+j]

(C.9)
where the conditional expectation Et+i‚àí1[‚Ä¢] in the last equality can be developed
as follows:

178
C Conditional Moments
Et+i‚àí1[Œµ2
t+i ht+j]
(C.2)
=
Et+i‚àí1

Œµ2
t+i (Œ±0 + œït+j‚àí1ht+j‚àí1)

=
Œ±0 Et+i‚àí1[Œµ2
t+i] + Et+i‚àí1[Œµ2
t+iœït+j‚àí1ht+j‚àí1]
A,C
=
Œ±0 + Et+i‚àí1[Œµ2
t+iht+j‚àí1]Et+i‚àí1[œït+j‚àí1]
=
Œ±0 + œÅ1Et+i‚àí1[Œµ2
t+iht+j‚àí1]
...
=
Œ±0(1 + . . . + œÅj‚àíi‚àí2
1
) + œÅj‚àíi‚àí1
1
Et+i‚àí1[Œµ2
t+iht+i+1]
=
Œ±0
j‚àíi‚àí2
X
k=0
œÅk
1 + œÅj‚àíi‚àí1
1
Et+i‚àí1

Œµ2
t+i (Œ±0 + œït+iht+i)

C=
Œ±0
j‚àíi‚àí2
X
k=0
œÅk
1 + Œ±0œÅj‚àíi‚àí1
1
+ œÅj‚àíi‚àí1
1
Et+i‚àí1[Œµ2
t+iœït+iht+i]
E=
Œ±0
j‚àíi‚àí1
X
k=0
œÅk
1 + œÅj‚àíi‚àí1
1
ht+i Et+i‚àí1[Œµ2
t+iœït+i]
=
Œ±0
 
1 ‚àíœÅj‚àíi
1
1 ‚àíœÅ1
!
+ œÅj‚àíi‚àí1
1
œÅ2 ht+i
(C.10)
with œÅ2 .= Et+i‚àí1[Œµ2
t+iœït+i]. The parameter œÅ2 is a function of the model pa-
rameters:
œÅ2
.= Et+i‚àí1[Œµ2
t+iœït+i]
= Et+i‚àí1

Œµ2
t+i(Œ±1Œµ2
t+i + Œ≤)

= Œ±1 Et+i‚àí1[Œµ4
t+i] + Œ≤ Et+i‚àí1[Œµ2
t+i]
C,E
= Œ∫ŒµŒ±1 + Œ≤ .
Replacing (C.10) in expression (C.9) yields:
Et(y2
t+i y2
t+j) = Et
 ht+i Et+i‚àí1[Œµ2
t+i ht+j]

= Œ±0
 
1 ‚àíœÅj‚àíi
1
1 ‚àíœÅ1
!
Et(ht+i) + œÅj‚àíi‚àí1
1
œÅ2 Et(h2
t+i) .
which can be computed recursively since ht+1 is known given Ft.
‚äì‚äî

Computational Details
The algorithms have been written in the R language, version 2.4.1 [see R De-
velopment Core Team 2007], with some subroutines implemented in C in order
to speed up the simulation procedure; this is required in the case of GARCH
models due to the recursive nature of the conditional process which drastically
slows down the computations with high level interpreted languages. Moreover,
the validity of the algorithms as well as the correctness of the computer code
were veriÔ¨Åed by a variant of the method proposed by Geweke [2004]. We refer
the reader to the end of Sect. 3.2.2 for further details. The R packages boa
0-1.3, mvtnorm 0-7.5, coda 0.10-7, sandwich 2.0-0, lmtest 0.9-18, MASS
7.2-32 have also been used to perform speciÔ¨Åc tasks. The R program itself and
packages are available from CRAN at http://CRAN.R-project.org.
Regarding the ML estimation of the models, the likelihood functions were
maximized using the R function nlminb which performs unconstrained and con-
strained optimization using PORT routines. In some cases, the procedure was
initialized using the DEoptim function provided by the R package DEoptim
1.01-2 [see Ardia 2007b]. This function performs a global (robust) optimization
based on the DiÔ¨Äerential Evolution algorithm. The reader is referred to Price,
Storn, and Lampinen [2006] for further details.
Finally, we note that the R package bayesGARCH will soon be available
from CRAN [see Ardia 2007a]. This package allows the Bayesian estimation of
the GARCH(1, 1) model with Student-t innovations. The underlying algorithm is
based on Nakatsuma [1998, 2000] for the generation of the scedastic function‚Äôs
parameters. The generation of the degrees of freedom parameter is achieved
following Deschamps [2006], Geweke [1993]. By using a translated Exponential as
a prior on the degrees of freedom parameter, Normal innovations can be obtained
as a special case of the sampler. Moreover, the function addPriorConditions
allows to add any constraints on the model parameters in the M-H sampler.

Abbreviations and Notations
Most of the notation as well as dimensions of vectors and matrices are clearly
deÔ¨Åned in the text where it is used. Occasionally, a mathematical symbol has
been assigned to a diÔ¨Äerent object.
Abbreviations
ACF
Autocorrelation function
AEL
Absolute error loss
ARCH
AutoRegressive Conditional Heteroscedasticity
ARDS
Adaptive Radial-Based Direction Sampling
AR
AutoRegressive
ARMA
AutoRegressive Moving Average
BIC
Bayesian information criterion
BF
Bayes factor
BUGS
Bayesian analysis Using Gibbs Software
cont.
Continued
CC
Conditional coverage
CSC
Covariance stationarity condition
DEM/GBP
Deutschmark vs British Pounds
DIC
Deviance information criterion
EM
Expectation Maximization
ES
Expected Shortfall
FFBS
Forward Filtering Backward Sampling
GARCH
Generalized ARCH
GJR
Asymmetric GARCH
IF
IneÔ¨Éciency factor
IND
Independence
Linex
Linex loss
MCMC
Markov Chain Monte Carlo
M-H
Metropolis-Hastings
continued on the next page

182
Abbreviations and Notations
Abbreviations (cont.)
ML
Maximum Likelihood
MS-GARCH
Markov-switching GARCH
MS-GJR
Markov-switching GJR
NA
Not applicable
NSE
Numerical standard error
P&L
ProÔ¨Åt and loss
SEL
Squared error loss
SMI
Swiss Market Index
S&P100
Standard & Poors 100
S&P500
Standard & Poors 500
SSC
Strict stationarity condition
UC
Unconditional Coverage
VaR
Value at Risk
VIX
Volatility index of the S&P100 index
Densities
N(0, 1)
Univariate standard Normal density
Nd(¬µ, Œ£)
d-dimensional Normal density with mean vector ¬µ and
covariance matrix Œ£
S(¬µ, œÉ2, ŒΩ)
Univariate standard Student-t density with mean ¬µ, scale
parameter œÉ2 and degrees of freedom ŒΩ
SS(¬µ, œÉ2, œÑ, Œ≥)
Univariate skewed Student-t density with mean ¬µ, scale
parameter œÉ2, degrees of freedom œÑ and asymmetry pa-
rameter Œ≥
IG(a, b)
Inverted Gamma density with shape parameter a and rate
parameter b
D(Œ∑)
Dirichlet density with parameter Œ∑
œá2
k
Chi-squared density with k degrees of freedom
Notations used throughout the book
‚àº
‚Äúis generated from‚Äù
‚àù
‚Äúis proportional to‚Äù
‚âª
‚Äúis preferred over‚Äù
‚âà
‚Äúis approximately equal to‚Äù
.=
‚Äúis deÔ¨Åned to‚Äù
‚â°
‚Äúis equivalent to‚Äù
‚àà
‚Äúbelongs to‚Äù
iid
Independent and identically distributed
R
Set of real numbers
R‚àó
Set of non-zero real numbers
R+
Set of real positive numbers
continued on the next page

Abbreviations and Notations
183
Notations used throughout the book (cont.)
N‚àó
Set of non-zero natural numbers
‚Ä¢
A scalar, a vector, a matrix, a parameter or a set of pa-
rameters (depending on the context)
exp[‚Ä¢]
Exponential
ln[‚Ä¢]
Natural logarithm
‚Ä¢!
Factorial
| ‚Ä¢ |
Absolute value
P
Summation operator
Q
Product operator
√ó
Multiplication operator or Cartesian product
R
Integral
‚Ä¢Ã∏=i
Vector without its ith component
{‚Ä¢}
Sequence (or set) of variables (or observations)
max{‚Ä¢}
Maximum value of a set
min{‚Ä¢}
Minimum value of a set
inf{‚Ä¢}
InÔ¨Åmum of a set
#{‚Ä¢}
Number of elements in a set
det(‚Ä¢)
Determinant of a square matrix
‚Ä¢‚àí1
Inverse of a number or a square matrix
Œì(‚Ä¢)
Gamma function
Œ®(‚Ä¢)
Digamma function
Id
Identity matrix of size d
Ft
Information set up to time t
I(d)
Integrated of order d
I{‚Ä¢}
Indicator function
‚Ä¢‚Ä≤
Transpose of a vector or a matrix
‚Ä¢MLE
Maximum Likelihood estimate
‚Ä¢
Sample average
‚Ä¢œÜ
œÜth percentile of a sample
L(‚Ä¢)
Likelihood function
L(‚Ä¢ | y)
Marginal (observed) likelihood function
f‚Ä¢(‚Ä¢)
Density function
F‚Ä¢(‚Ä¢)
Distribution function
p(‚Ä¢ | y)
Posterior density
P(‚Ä¢)
Probability
P(‚Ä¢ | ‚Ä¢)
Conditional probability
E(‚Ä¢)
Expectation
E(‚Ä¢ | ‚Ä¢), E‚Ä¢(‚Ä¢)
Conditional expectations
bR
Potential scale reduction factor
0
Vector of zeros
Œ±0 Œ±1 (Œ±2)
ARCH parameters
Œ±
Vector of ARCH parameters
Œ≤
GARCH parameter
Œ≥
Vector of linear regression‚Äôs parameters
continued on the next page

184
Abbreviations and Notations
Notations used throughout the book (cont.)
ŒΩ
Degrees of freedom parameter of the Student-t density
œñt
Latent scale variable at time t
œñ
Vector of latent scale variables
Œ∏ œà Œò
Sets (or vectors) of the model parameters
Œ±
Œ± .= (Œ±1 + Œ±2)/2
‚ñ≥Œ±
Leverage eÔ¨Äect coeÔ¨Écient: ‚ñ≥Œ± .= (Œ±2 ‚àíŒ±1)
T
Length of the underlying time series (number of observa-
tions)
yt
Dependent variable at time t
y
Vector containing the observations of yt
Œµt
Model innovation at time t
bŒµt
Model residual at time t
ut
Linear regression‚Äôs error at time t
u
Vector of linear regression‚Äôs errors
m
Number of exogenous or lagged dependent variables
xt
Vector of exogenous or lagged dependent variables at time
t
X
Matrix whose tth row is x‚Ä≤
t
ht
Conditional variance at time t
hy
Unconditional variance of the underlying process
Œ£ Œõ
Diagonal matrices of conditional variances
œ±
A scaling factor: œ± .= ŒΩ‚àí2
ŒΩ
J
Number of draws in the posterior sample
q‚Ä¢(‚Ä¢)
Proposal density
e‚Ä¢
Previous draw in the sampler
‚Ä¢[j]
jth draw in the sampler
‚Ä¢‚ãÜ
New draw in the sampler
¬µ‚Ä¢ Œ£‚Ä¢
Hyperparameters of the (truncated) Normal priors
b¬µ‚Ä¢ bŒ£‚Ä¢
Parameters of the (truncated) Normal proposals
Œª Œ¥
Hyperparameters of the translated Exponential prior
Œ∫Œµ
Kurtosis of the innovations
Œ∫y
Unconditional kurtosis of the underlying process
l‚àó
t v‚àó
t v‚àó‚àó
t
Recursive transformations
ct
Vector of recursive transformations
C
Matrix whose tth row is c‚Ä≤
t
V œÜ
t
Indicator variable at time t for the violation of the one-
day ahead VaR at risk level œÜ
Notations speciÔ¨Åc to Chap. 6
œÜ œÜc
Risk level of the VaR and œÜc .= 1 ‚àíœÜ
s
Time horizon (in days)
yt,s
Cumulated return over a s-day ahead horizon at time t
continued on the next page

Abbreviations and Notations
185
Notations speciÔ¨Åc to Chap. 6 (cont.)
yt:s
Vector of s-day ahead log-returns at time t
Œ∫p(‚Ä¢)
pth conditional moment
zœÜ
œÜth percentile of the N(0, 1) density
tœÜ(ŒΩ)
œÜth percentile of the S(0, 1, ŒΩ) density
L
Loss function
œâ
True value of the forecast (or one-dimensional parameter)
bœâ
Point estimate of œâ
bœâL
Optimal (Bayes) point estimate under loss function L
RL (‚Ä¢ | y)
Posterior risk function under loss L
a
Parameter of the Linex loss function
a1 a2 q
Parameters of the Monomial loss function
‚àÜ
Statistical error: ‚àÜ.= (bœâ ‚àíœâ)
N
Number of out-of-sample observations
d
VaR
VaR point estimate
VaRœÜ
t
One-day ahead VaR at time t for risk level œÜ
VaRœÜ
t,s
s-day ahead VaR at time t for risk level œÜ
d
VaR
œÜ
L ,t,s
s-day ahead VaR point estimate at time t for risk level œÜ
and loss function L
c
RCL ,t
Regulatory capital point estimate at time t for loss func-
tion L
V œÜ
t,s
Indicator variable at time t for the violation of the s-day
ahead VaR at risk level œÜ
ESœÜ
t
One-day ahead ES at time t for risk level œÜ
ESœÜ
t,s
s-day ahead ES at time t for risk level œÜ
Notations speciÔ¨Åc to Chap. 7
‚äó
Kronecker product
‚äô
Hadamard product, i.e., element-by-element multiplica-
tion
vec(‚Ä¢)
Vectorization (column stacking) of a matrix
tr(‚Ä¢)
Trace of a square matrix
K
Number of regimes
Œ±k
0 Œ±k
1 Œ±k
2
ARCH parameters in state k
Œ±0 Œ±1 Œ±2
Vectors of ARCH parameters
Œ±
Vector containing the vectors of ARCH parameters
Œ≤k
GARCH parameter in state k
Œ≤
Vector of GARCH parameters
et
stth column of the matrix IK
ŒπK
Vector of ones of size K
ht
Vector of conditional variances at time t
Œ∑i
Parameter‚Äôs vector of the ith Dirichlet density
continued on the next page

186
Abbreviations and Notations
Notations speciÔ¨Åc to Chap. 7 (cont.)
[‚Ä¢]i
ith component of a vector
œÄ
Vector of ergodic probabilities
P
Matrix of transition probabilities
st
Discrete state variable estimated at time t
s
Vector of state variables
zt
Approximate p-score at time t
ut
Generalized residual at time t
D(‚Ä¢)
Deviance function
pD
EÔ¨Äective number of parameters
B
Number of replications for the block bootstrap
q(œà)
Importance density for parameter œà
p(y)
Model likelihood
pt(y)
tth estimate of the model likelihood in the bridge sampling
p0(y)
Reciprocal sampling estimator of p(y)

List of Tables
3.1
Estimation results for the GARCH(1, 1) model with Normal
innovations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.2
Results of the sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
4.1
Estimation results for the linear regression model with
Normal-GJR(1, 1) errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
4.2
Results of the sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
5.1
Estimation results for the Student-t-GJR(1, 1) model . . . . . . . . . . . 66
5.2
Results of the sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.1
Average deviations of the VaR point estimates from the SEL
benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
6.2
Average deviations of the regulatory capital point estimates
from the SEL benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.3
Forecasting results of the VaR point estimates . . . . . . . . . . . . . . . . . 103
7.1
Estimation results for the GJR and MS-GJR models . . . . . . . . . . . . 127
7.2
Results of the DIC and BIC criteria . . . . . . . . . . . . . . . . . . . . . . . . . . 136
7.3
Results of the model likelihood estimators . . . . . . . . . . . . . . . . . . . . . 142
7.4
Forecasting results of the VaR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
7.5
Forecasting results of the VaR point estimates for the MS-GJR
model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150

List of Figures
3.1
DEM/GBP foreign exchange daily log-returns and sample
autocorrelogram of the squared log-returns . . . . . . . . . . . . . . . . . . . . 23
3.2
Running means of the chains over iterations . . . . . . . . . . . . . . . . . . . 27
3.3
Marginal posterior densities of the GARCH(1, 1) parameters . . . . . 28
3.3
Marginal posterior densities of the GARCH(1, 1) parameters
(cont.) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.4
Residuals time series and Normal quantile-quantile plot . . . . . . . . . 33
3.5
Posterior density of the persistence and posterior
autocorrelogram of the squared observations . . . . . . . . . . . . . . . . . . . 35
3.6
Posterior densities of the covariance stationarity and strict
stationarity conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.1
S&P100 index log-returns and VIX level . . . . . . . . . . . . . . . . . . . . . . 46
4.2
Marginal posterior densities of the GJR(1, 1) parameters . . . . . . . . 49
4.2
Marginal posterior densities of the GJR(1, 1) parameters (cont.) . . 50
4.3
Posterior density of the leverage eÔ¨Äect parameter . . . . . . . . . . . . . . . 51
4.4
Posterior density of the unconditional variance and asymptotic
Normal approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
5.1
Comparison between the ML and the Bayesian approaches . . . . . . 67
5.1
Comparison between the ML and the Bayesian approaches (cont.)
68
5.2
Prior and posterior densities of the degrees of freedom parameter . 69
6.1
Cornish-Fisher and Student-t approximations . . . . . . . . . . . . . . . . . . 84
6.2
Linex loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
6.3
Monomial loss function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
6.4
Estimation and forecasting windows . . . . . . . . . . . . . . . . . . . . . . . . . . 94
6.5
Term structures of the VaR density . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
6.6
Term structures of the ES density . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
7.1
SMI daily log-returns and sample autocorrelogram of the
squared log-returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124

190
List of Figures
7.2
Contour plots for (Œ≤k, Œ±k
0), (Œ≤k, Œ±k
1) and (Œ≤k, Œ±k
2) . . . . . . . . . . . . . . . 129
7.3
Marginal posterior densities of the MS-GJR parameters . . . . . . . . . 130
7.4
Posterior densities of the covariance stationarity condition and
the unconditional variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
7.5
Smoothed probabilities of the high-volatility state together with
the in-sample log-returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
7.6
Importance density and marginal posterior density comparison . . . 143
7.7
Filtered probabilities of the high-volatility state together with
the out-of-sample log-returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
7.8
Density of the one-day ahead VaR . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
7.9
Marginal posterior densities of the MS-GJR model parameters
and comparison with the asymptotic Normal approximation . . . . . 153

References
Andrews DWK (1991).
‚ÄúHeteroskedasticity and Autocorrelation Consistent
Covariance Matrix Estimation.‚Äù Econometrica, 59(3), 817‚Äì858.
Cited on
pages 14, 26, and 126.
Andrews DWK, Monahan JC (1992). ‚ÄúAn Improved Heteroskedasticity and Au-
tocorrelation Consistent Covariance Matrix Estimator.‚Äù Econometrica, 60(4),
953‚Äì966. Cited on pages 14, 26, and 126.
Ardia D (2007a). bayesGARCH: Bayesian Estimation of the GARCH(1,1) Model
with Student-t Innovations in R. R Foundation for Statistical Computing,
Vienna, Austria.
In preparation, URL http://stat.ethz.ch/CRAN/doc/
packages/bayesGARCH.PDF. Cited on page 179.
Ardia D (2007b). DEoptim: DiÔ¨Äerential Evolution Optimization. R Foundation
for Statistical Computing, Vienna, Austria. Version 1.1-8, URL http://stat.
ethz.ch/CRAN/doc/packages/DEoptim.PDF. Cited on page 179.
Artzner P, Delbaen F, Eber JM, Heath D (1999). ‚ÄúCoherent Measures of Risk.‚Äù
Quantitative Finance, 9(3), 203‚Äì228. Cited on page 104.
Aus¬¥ƒ±n MC, Galeano P (2007). ‚ÄúBayesian Estimation of the Gaussian Mixture
GARCH Model.‚Äù Computational Statistics and Data Analysis, 51(5), 2636‚Äì
2652. doi:10.1016/j.csda.2006.01.006. Cited on page 4.
Bams D, Lehnert T, WolÔ¨ÄCCP (2005). ‚ÄúAn Evaluation Framework for Alter-
native VaR Models.‚Äù Journal of International Money and Finance, 24(6),
944‚Äì958. doi:10.1016/j.jimonÔ¨Ån.2005.05.004. Cited on page 75.
Basel Committee on Banking Supervision (1995).
An Internal Model-Based
Approach to Market Risk Capital Requirements. The Bank for International
Settlements, Basel, Switzerland. URL http://www.bis.org/publ/bcbs17.
htm. Cited on page 73.
Basel Committee on Banking Supervision (1996a). Amendment to the Capital
Accord to Incorporate Market Risk. The Bank for International Settlements,
Basel, Switzerland. URL http://www.bis.org/publ/bcbs24.htm. Cited on
page 96.
Basel Committee on Banking Supervision (1996b).
Supervisory Framework
for the Use of Backtesting in Conjunction with the Internal Approach to
Market Risk Capital Requirement. The Bank for International Settlements,

192
References
Basel, Switzerland. URL http://www.bis.org/publ/bcbs22.htm. Cited on
pages 93, 100, and 144.
Bauwens L, Bos CS, van Dijk HK, van Oest RD (2004). ‚ÄúAdaptive Radial-
Based Direction Sampling: Some Flexible and Robust Monte Carlo Integration
Methods.‚Äù Journal of Econometrics, 123(2), 201‚Äì225. doi:10.1016/j.jeconom.
2003.12.002. Special issue on recent advances in Bayesian econometrics. Cited
on pages 4 and 5.
Bauwens L, Lubrano M (1998). ‚ÄúBayesian Inference on GARCH Models Using
the Gibbs Sampler.‚Äù The Econometrics Journal, 1(1), C23‚ÄìC46. doi:10.1111/
1368-423X.11003. Cited on pages 4 and 5.
Bauwens L, Preminger A, Rombouts JVK (2006). ‚ÄúRegime Switching GARCH
Models.‚Äù Discussion paper 2006/11, CORE and Department of Economics,
Universit¬¥e catholique de Louvain. Cited on pages 4 and 159.
Bauwens L, Rombouts JVK (2007). ‚ÄúBayesian Inference for the Mixed Condi-
tional Heteroskedasticity Model.‚Äù The Econometrics Journal, 10(2), 408‚Äì425.
doi:10.1111/j.1368-423X.2007.00213.x. Cited on page 4.
Berg A, Meyer R, Yu J (2004). ‚ÄúDeviance Information Criterion for Comparing
Stochastic Volatility Models.‚Äù Journal of Business and Economic Statistics,
22(1), 107‚Äì120. Cited on page 134.
Berkowitz J (2001). ‚ÄúTesting Density Forecasts, with Applications to Risk Man-
agement.‚Äù Journal of Business and Economic Statistics, 19(4), 465‚Äì474. Cited
on page 133.
Berkowitz J, ChristoÔ¨Äersen PF, Pelletier D (2006). ‚ÄúEvaluating Value-at-Risk
Model with Desk-Level Data.‚Äù Working Paper Series 010, Department of
Economics, North Carolina State University.
URL http://ideas.repec.
org/p/ncs/wpaper/010.html. Cited on page 158.
Black F (1976). ‚ÄúThe Pricing of Commodity Contracts.‚Äù Journal of Financial
Economics, 3(1‚Äì2), 167‚Äì179. doi:10.1016/0304-405X(76)90024-6. Cited on
pages 2 and 39.
Bollerslev T (1986). ‚ÄúGeneralized Autoregressive Conditional Heteroskedastic-
ity.‚Äù Journal of Econometrics, 31(3), 307‚Äì327.
doi:10.1016/0304-4076(86)
90063-1. Cited on pages 1, 3, and 36.
Bollerslev T, Chou RY, Kroner K (1992). ‚ÄúARCH Modeling in Finance: A
Review of the Theory and Empirical Evidence.‚Äù Journal of Econometrics,
52(1‚Äì2), 5‚Äì59. doi:10.1016/0304-4076(92)90064-X. Cited on page 2.
Bollerslev T, Engle RF, Nelson DB (1994). ‚ÄúARCH Models.‚Äù In RF Engle,
DL McFadden (eds.), ‚ÄúHandbook of Econometrics,‚Äù volume 4, chapter 49, pp.
2959‚Äì3038. North Holland, Amsterdam, NL, Ô¨Årst edition. ISBN 0444887660.
doi:10.1016/S1573-4412(05)80019-4. Cited on page 2.
Bollerslev T, Ghysels E (1996). ‚ÄúPeriodic Autoregressive Conditional Hetero-
scedasticity.‚Äù Journal of Business and Economic Statistics, 14(2), 139‚Äì151.
Cited on page 22.
Campbell JY, Lo AW, MacKinlay AC (1996). The Econometrics of Financial
Markets. Princeton University Press, Princeton, USA, Ô¨Årst edition. ISBN
0691043019. Cited on page 46.

References
193
Casella G, George EI (1992). ‚ÄúExplaining the Gibbs Sampler.‚Äù The American
Statistician, 46(3), 167‚Äì174. Cited on page 11.
Celeux G, Forbes F, Robert CP, Titterington M (2006). ‚ÄúDeviance Information
Criterion for Missing Data Models.‚Äù Bayesian Analysis, 1(4), 651‚Äì706. With
discussion and rejoinder. Cited on pages 134, 135, and 137.
Chib S (1996). ‚ÄúCalculating Posterior Distributions and Modal Estimates in
Markov Mixture Models.‚Äù Journal of Econometrics, 75(1), 79‚Äì97. doi:10.
1016/0304-4076(95)01770-4. Cited on pages 116, 117, 133, and 135.
Chib S, Greenberg E (1994).
‚ÄúBayes Inference in Regression Models with
ARMA(p,q) Errors.‚Äù
Journal of Econometrics, 64(1‚Äì2), 183‚Äì206.
doi:
10.1016/0304-4076(94)90063-9. Cited on page 20.
Chib S, Greenberg E (1995).
‚ÄúUnderstanding the Metropolis-Hasting Algo-
rithm.‚Äù The American Statistician, 49(4), 327‚Äì335. Cited on page 13.
Chib S, Greenberg E (1996). ‚ÄúMarkov Chain Monte Carlo Simulation Methods
in Econometrics.‚Äù Econometric Theory, 12(3), 409‚Äì431. Cited on pages 9
and 11.
ChristoÔ¨Äersen PF (1998). ‚ÄúEvaluating Interval Forecasts.‚Äù International Eco-
nomic Review, 39(4), 841‚Äì862. Cited on pages 74, 102, 103, 104, 145, and 149.
ChristoÔ¨Äersen PF, Diebold FX (1996). ‚ÄúFurther Results on Forecasting and
Model Selection Under Asymmetric Loss.‚Äù
Journal of Applied Econo-
metrics, 11(5), 561‚Äì571.
doi:10.1002/(SICI)1099-1255(199609)11:5<561::
AID-JAE406>3.0.CO;2-S. Special issue in econometric forecasting. Cited
on page 86.
ChristoÔ¨Äersen PF, Diebold FX (1997). ‚ÄúOptimal Prediction under Asymmetric
Loss.‚Äù Econometric Theory, 13(6), 808‚Äì817. Cited on page 86.
ChristoÔ¨Äersen PF, Gon¬∏calves S (2004).
‚ÄúEstimation Risk in Financial Risk
Management.‚Äù CIRANO Working Papers 2004s‚Äì15, CIRANO. URL http:
//ideas.repec.org/p/cir/cirwor/2004s-15.html. Cited on page 75.
Cornish EA, Fisher RA (1937). ‚ÄúMoments and Cumulants in the SpeciÔ¨Åcation
of Distributions.‚Äù Revue de l‚ÄôInstitut International de Statistique, 4, 1‚Äì14.
Reprinted in: Contributions to Mathematical Statistics, Fisher, R. A. (Ed.),
New York: Whiley, 1950. Cited on pages 76 and 79.
Cowles MK, Carlin BP (1996). ‚ÄúMarkov Chain Monte Carlo Convergence Diag-
nostics: A Comparative Review.‚Äù Journal of the American Statistical Associ-
ation, 91(434), 883‚Äì904. Cited on page 14.
Dempster AP (1997).
‚ÄúThe Direct Use of Likelihood for SigniÔ¨Åcance Test-
ing.‚Äù Statistics and Computing, 7(4), 247‚Äì252. doi:10.1023/A:1018598421607.
Cited on page 134.
Dempster AP, Laird NM, Rubin DB (1977). ‚ÄúMaximum Likelihood from Incom-
plete Data via the EM Algorithm.‚Äù Journal of the Royal Statistical Society,
39(1), 1‚Äì38. Cited on page 152.
Deschamps PJ (2006). ‚ÄúA Flexible Prior Distribution for Markov Switching
Autoregressions with Student-t Errors.‚Äù Journal of Econometrics, 133(1),
153‚Äì190. doi:10.1016/j.jeconom.2005.03.012. Cited on pages 14, 26, 58, 63,
115, 126, 166, 167, and 179.

194
References
Diebold FX, Gunther TA, Tay AS (1998). ‚ÄúEvaluating Density Forecasts with
Applications to Financial Risk Management.‚Äù International Economic Re-
view, 39(4), 863‚Äì883. Cited on page 133.
Diebold FX, Mariano RS (1995). ‚ÄúComparing Predictive Accuracy.‚Äù Journal of
Business and Economic Statistics, 13(3), 253‚Äì263. Cited on page 102.
Diebolt J, Robert CP (1994).
‚ÄúEstimation of Finite Mixture Distributions
through Bayesian Sampling.‚Äù Journal of the Royal Statistical Society, 56(2),
363‚Äì375. Cited on page 114.
Dueker MJ (1997).
‚ÄúMarkov Switching in GARCH Processes and Mean-
Reverting Stock-Market Volatility.‚Äù Journal of Business and Economic Statis-
tics, 15(1), 26‚Äì34. Cited on pages 2, 109, and 159.
Engle RF (1982).‚ÄúAutoregressive Conditional Heteroscedasticity with Estimates
of the Variance of United Kingdom InÔ¨Çation.‚Äù Econometrica, 50(4), 987‚Äì1008.
Cited on page 1.
Engle RF (2004).
‚ÄúRisk and Volatility: Econometric Models and Financial
Practice.‚Äù The American Economic Review, 94(3), 405‚Äì420.
doi:10.1257/
0002828041464597. Cited on pages 2 and 83.
Fern¬¥andez C, Steel MF (1998). ‚ÄúOn Bayesian Modeling of Fat Tails and Skew-
ness.‚Äù Journal of the American Statistical Association, 93(441), 359‚Äì371.
Cited on page 140.
Fr¬®uhwirth-Schnatter S (2001a). ‚ÄúFully Bayesian Analysis of Switching Gaussian
State Space Models.‚Äù Annals of the Institute of Statistical Mathematics, 53(1),
31‚Äì49. Special issue on nonlinear non-Gaussian models and related Ô¨Åltering
methods. Cited on page 139.
Fr¬®uhwirth-Schnatter S (2001b). ‚ÄúMarkov Chain Monte Carlo Estimation of Clas-
sical and Dynamic Switching and Mixture Models.‚Äù Journal of the American
Statistical Association, 96(453), 194‚Äì209. Cited on pages 7, 113, 116, 125,
129, and 157.
Fr¬®uhwirth-Schnatter S (2004). ‚ÄúEstimating Marginal Likelihoods for Mixture
and Markov Switching Models Using Bridge Sampling Techniques.‚Äù
The
Econometrics Journal, 7(1), 143‚Äì167. doi:10.1111/j.1368-423X.2004.00125.x.
Cited on page 138.
Fr¬®uhwirth-Schnatter S (2006). Finite Mixture and Markov Switching Models.
Springer Series in Statistics. Springer Verlag, New York, USA, Ô¨Årst edition.
ISBN 0387329099. Cited on pages 3, 117, and 118.
Gallant AR, Tauchen G (1989).
‚ÄúSeminonparametric Estimation of Condi-
tionally Constrained Heterogeneous Processes: Asset Pricing Applications.‚Äù
Econometrica, 57(5), 1091‚Äì1120. Cited on page 2.
Gelfand AE, Dey DK (1994). ‚ÄúBayesian Model Choice: Asymptotics and Exact
Calculations.‚Äù Journal of the Royal Statistical Society, 56(3), 501‚Äì514. Cited
on page 139.
Gelfand AE, Smith AFM (1990). ‚ÄúSampling-Based Approaches to Calculating
Marginal Densities.‚Äù Journal of the American Statistical Association, 85(410),
398‚Äì409. Cited on page 11.
Gelman A (1995). ‚ÄúInference and Monitoring Convergence.‚Äù In WR Gilks,
S Richardson, DJ Spiegelhalter (eds.), ‚ÄúMarkov Chain Monte Carlo in Prac-

References
195
tice,‚Äù chapter 8, pp. 131‚Äì143. Chapman and Hall, London, UK, Ô¨Årst edition.
ISBN 0412055511. Cited on page 14.
Gelman A, Rubin DB (1992). ‚ÄúInference from Iterative Simulation Using Multi-
ple Sequences.‚Äù Statistical Science, 7(4), 457‚Äì472. Cited on pages 14, 24, 25,
27, 47, 65, 94, and 123.
Geman S, Geman D (1984). ‚ÄúStochastic Relaxation, Gibbs Distributions, and
the Bayesian Restoration of Images.‚Äù IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6, 721‚Äì741. Cited on page 11.
Gerlach RH, Carter C, Kohn R (1999). ‚ÄúDiagnostics for Time Series Analy-
sis.‚Äù Journal of Time Series Analysis, 20(3), 309‚Äì330. doi:10.1111/1467-9892.
00139. Cited on page 144.
Geweke JF (1988).
‚ÄúExact Inference in Models with Autoregressive Condi-
tional Heteroscedasticity.‚Äù In ER Berndt, HL White, WA Barnett (eds.),
‚ÄúDynamic Econometric Modeling,‚Äù Number 3 in International Symposium in
Economic Theory and Econometrics, pp. 73‚Äì103. Cambridge University Press,
New York, USA. ISBN 0521333954. Cited on pages 3 and 4.
Geweke JF (1989).
‚ÄúExact Predictive Densities in Linear Models with
ARCH Disturbances.‚Äù Journal of Econometrics, 40(1), 63‚Äì86. doi:10.1016/
0304-4076(89)90030-4. Cited on page 4.
Geweke JF (1991). ‚ÄúEÔ¨Écient Simulation From the Multivariate Normal and
Student-t Distributions Subject to Linear Constraints and the Evaluation of
Constraint Probabilities.‚Äù In EM Keramidas, SM Kaufman (eds.), ‚ÄúCom-
puting Science and Statistics: The 23rd Symposium on the Interface,‚Äù pp.
571‚Äì578. Seattle, Washington, USA. Special issue on critical applications of
scientiÔ¨Åc computing. Cited on page 141.
Geweke JF (1992). ‚ÄúEvaluating the Accuracy of Sampling-Based Approaches
to the Calculation of Posterior Moments.‚Äù
In JO Berger, JM Bernardo,
AP Dawid, AFM Smith (eds.), ‚ÄúBayesian Statistics,‚Äù volume 4, pp. 169‚Äì194.
Oxford University Press, Oxford, UK. ISBN 0198522665. Fourth Valencia
international meeting. Cited on page 14.
Geweke JF (1993). ‚ÄúBayesian Treatment of the Independent Student-t Linear
Model.‚Äù Journal of Applied Econometrics, 8(S1), S19‚ÄìS40. doi:10.1002/jae.
3950080504.
Special issue on econometric inference using simulation tech-
niques. Cited on pages 6, 55, 57, 64, 166, 167, and 179.
Geweke JF (1999). ‚ÄúUsing Simulation Methods for Bayesian Econometric Mod-
els: Inference, Development and Communication.‚Äù
Econometric Reviews,
18(1), 1‚Äì73. doi:10.1080/07474939908800428. Cited on page 30.
Geweke JF (2004). ‚ÄúGetting it Right: Joint Distribution Tests of Posterior
Simulators.‚Äù Journal of the American Statistical Association, 99(467), 799‚Äì
804. Cited on pages 22 and 179.
Gilks WR, Wild P (1992). ‚ÄúAdaptive Rejection Sampling for Gibbs Sampling.‚Äù
Applied Statistics, 41(2), 337‚Äì348. Cited on page 12.
Glosten LR, Jaganathan R, Runkle DE (1993). ‚ÄúOn the Relation Between the
Expected Value and the Volatility of the Nominal Excess Return on Stocks.‚Äù
Journal of Finance, 48(5), 1779‚Äì1801. Cited on pages 2, 6, and 39.

196
References
Gray SF (1996). ‚ÄúModeling the Conditional Distribution of Interest Rates as a
Regime-Switching Process.‚Äù Journal of Financial Economics, 42(1), 27‚Äì62.
doi:10.1016/0304-405X(96)00875-6. Cited on pages 109, 117, and 159.
Guidolin M, Timmermann A (2006). ‚ÄúTerm Stucture of Risk under Alternative
Econometric SpeciÔ¨Åcations.‚Äù Journal of Econometrics, 131(1‚Äì2), 285‚Äì308.
doi:10.1016/j.jeconom.2005.01.033. Cited on pages 91 and 92.
Haas M, Mittnik S, Paolella MS (2004). ‚ÄúA New Approach to Markov-Switching
GARCH Models.‚Äù Journal of Financial Econometrics, 2(4), 493‚Äì530. doi:
10.1093/jjÔ¨Ånec/nbh020. Cited on pages 2, 7, 110, 112, 126, 128, 133, and 157.
Hamilton JD (1994). Time Series Analysis. Princeton University Press, Prince-
ton, USA, Ô¨Årst edition. ISBN 0691042896. Cited on pages 114 and 152.
Hamilton JD, Susmel R (1994). ‚ÄúAutoregressive Conditional Heteroskedasticity
and Changes in Regime.‚Äù Journal of Econometrics, 64(1‚Äì2), 307‚Äì333. doi:
10.1016/0304-4076(94)90067-1. Cited on pages 2, 3, 109, and 152.
Hastings WK (1970). ‚ÄúMonte Carlo Sampling Methods Using Markov Chains
and their Applications.‚Äù Biometrika, 57(1), 97‚Äì109. doi:10.1093/biomet/57.
1.97. Cited on page 10.
He C, Ter¬®asvirta T (1999). ‚ÄúProperties of Moments of a Family of GARCH Pro-
cesses.‚ÄùJournal of Econometrics, 92(1), 173‚Äì192. doi:10.1016/S0304-4076(98)
00089-X. Cited on page 71.
HighÔ¨Åeld RA, Zellner A (1988). ‚ÄúCalculation of Maximum Entropy Distribu-
tions and Approximation of Marginal Posterior Distributions.‚Äù Journal of
Econometrics, 37(2), 195‚Äì209. doi:10.1016/0304-4076(88)90002-4. Cited on
page 83.
Hwang S, Knight J, Satchell SE (1999). ‚ÄúForecasting Volatility Using Linex Loss
Functions.‚Äù Working paper 99-07, Financial Econometrics Research Center,
Warwick Business School. Cited on page 86.
Hwang S, Knight J, Satchell SE (2001). ‚ÄúForecasting Nonlinear Functions of
Returns Using Linex Loss Functions.‚Äù Annals of Economics and Finance,
2(1), 187‚Äì213. Cited on page 86.
Jacquier E, Polson NG, Rossi PE (1994). ‚ÄúBayesian Analysis of Stochastic
Volatility Models.‚Äù Journal of Business and Economic Statistics, 12(4), 371‚Äì
389. Cited on page 158.
Jaschke SR (2002). ‚ÄúThe Cornish-Fisher Expansion in the Context of Delta-
Gamma-Normal Approximations.‚Äù Journal of Risk, 4(4), 33‚Äì52. Cited on
page 80.
Kass RE, Carlin BP, Gelman A, Neal RM (1998). ‚ÄúMarkov Chain Monte Carlo
in Practice: A Roundtable Discussion.‚Äù The American Statistician, 52(2),
93‚Äì100. Cited on page 13.
Kass RE, Raftery AE (1995). ‚ÄúBayes Factors.‚Äù Journal of the American Statis-
tical Association, 90(430), 773‚Äì795. Cited on pages 31 and 141.
Kaufmann R (2004).
Long-Term Risk Management.
Phd thesis, Swiss Fed-
eral Institute of Technology Z¬®urich. Dissertation ETH No. 15595. Cited on
page 158.

References
197
Kaufmann S, Fr¬®uhwirth-Schnatter S (2002). ‚ÄúBayesian Analysis of Switching
ARCH Models.‚Äù Journal of Time Series Analysis, 23(4), 425‚Äì458. doi:10.
1111/1467-9892.00271. Cited on pages 4, 117, 133, 135, and 139.
Kaufmann S, Scheicher M (2006). ‚ÄúA Switching ARCH Model for the German
DAX Index.‚Äù Studies in Nonlinear Dynamics and Econometrics, 10(4), 1‚Äì
35. Article nr. 3, URL http://www.bepress.com/snde/vol10/iss4/art3/.
Cited on pages 4 and 139.
Kim S, Shephard N, Chib S (1998). ‚ÄúStochastic Volatility: Likelihood Inference
and Comparison with ARCH Models.‚Äù Review of Economic Studies, 65(3),
361‚Äì393. doi:10.1111/1467-937X.00050. Cited on pages 133 and 158.
Klaassen F (2002).
‚ÄúImproving GARCH Volatility Forecasts with Regime-
Switching GARCH.‚Äù
Empirical Economics, 27(2), 363‚Äì394.
doi:10.1007/
s001810100100. Cited on pages 2, 109, 112, and 117.
Kleibergen F, van Dijk HK (1993). ‚ÄúNon-Stationarity in GARCH Models: A
Bayesian Analysis.‚Äù Journal of Applied Econometrics, 8(S1), S41‚ÄìS61. doi:
10.1002/jae.3950080505. Cited on page 4.
Knight J, Satchell S, Wang G (2003). ‚ÄúValue at Risk Linear Exponent Forecasts
-VARLINEX-.‚Äù Quantitative Finance, 3, 332‚Äì344. Cited on pages 86 and 97.
Koop G (2003). Bayesian Econometrics. Wiley-Interscience, London, UK, Ô¨Årst
edition. ISBN 0470845678. Cited on page 9.
Kupiec PH (1995). ‚ÄúTechniques for Verifying the Accuracy of Risk Measurement
Models.‚Äù Journal of Derivatives, 3, 73‚Äì84. Cited on page 74.
Lamoureux CG, Lastrapes WD (1990). ‚ÄúPersistence in Variance, Structural
Change, and the GARCH Model.‚Äù Journal of Business and Economic Statis-
tics, 8(2), 225‚Äì243. Cited on pages 2 and 109.
Lee SW, Hansen BE (1994). ‚ÄúAsymptotic Theory for the GARCH(1,1) Quasi-
Maximum Likelihood Estimator.‚Äù Econometric Theory, 10(1), 29‚Äì52. Cited
on page 2.
Ljung GM, Box GEP (1978). ‚ÄúOn a Measure of Lack of Fit in Time Series
Models.‚Äù Biometrika, 65(2), 297‚Äì303. doi:10.1093/biomet/65.2.297. Cited on
pages 32 and 53.
Marcucci J (2005). ‚ÄúForecasting Stock Market Volatility with Regime-Switching
GARCH Models.‚Äù Studies in Nonlinear Dynamics and Econometrics, 9(4),
1‚Äì53. Article nr. 6, URL http://www.bepress.com/snde/vol9/iss4/art6/.
Cited on pages 2 and 109.
McNeil AJ, Frey R (2000).
‚ÄúEstimation of Tail-Related Risk Measures for
Heteroscedastic Financial Time Series: an Extreme Value Approach.‚Äù Journal
of Empirical Finance, 7(3‚Äì4), 271‚Äì300. doi:10.1016/S0927-5398(00)00012-8.
Cited on page 74.
Meng XL, Wong WH (1996). ‚ÄúSimulating Ratios of Normalizing Constants via
a Simple Identity: A Theoretical Exploration.‚Äù Statistica Sinica, 6, 831‚Äì860.
Cited on pages 7, 138, 139, and 157.
Metropolis N, Rosenbluth AW, Rosenbluth MN, Teller AH, Teller E (1953).
‚ÄúEquations of State Calculations by Fast Computing Machines.‚Äù Journal of
Chemical Physics, 21(6), 1087‚Äì1092. Cited on pages 10 and 13.

198
References
Miazhynskaia T, Aussenegg W (2006). ‚ÄúUncertainty in Value at Risk Estimates
under Parametric and Non-Parametric Modeling.‚Äù Financial Markets and
Portfolio Management, 20(3), 243‚Äì264. doi:10.1007/s11408-006-0020-8. Cited
on pages 75 and 92.
M¬®uller P, Pole A (1998). ‚ÄúMonte Carlo Posterior Integration in GARCH Mod-
els.‚Äù Sankhya: The Indian Journal of Statistics, 60, 127‚Äì144. Cited on page 4.
Nakatsuma T (1998). ‚ÄúA Markov-Chain Sampling Algorithm for GARCH Mod-
els.‚Äù Studies in Nonlinear Dynamics and Econometrics, 3(2), 107‚Äì117. Algo-
rithm nr.1, URL http://www.bepress.com/snde/vol3/iss2/algorithm1/.
Cited on pages 4, 5, 6, 7, 17, 18, 19, 116, 155, 156, 157, 158, and 179.
Nakatsuma T (2000). ‚ÄúBayesian Analysis of ARMA-GARCH Models: A Markov
Chain Sampling Approach.‚Äù Journal of Econometrics, 95(1), 57‚Äì69.
doi:
10.1016/S0304-4076(99)00029-9. Cited on pages 4, 5, 6, 7, 17, 18, 19, 116,
155, 156, 157, 158, and 179.
Nelson DB (1990). ‚ÄúStationarity and Persistence in the GARCH(1,1) Model.‚Äù
Econometric Theory, 6(3), 318‚Äì334. Cited on page 36.
Nelson DB (1991). ‚ÄúConditional Heteroskedasticity in Asset Returns: A New
Approach.‚Äù Econometrica, 59(2), 347‚Äì370. Cited on page 2.
Newey WK, West KD (1987). ‚ÄúA Simple, Positive Semi-DeÔ¨Ånite, Heteroskedas-
ticity and Autocorrelation Consistent Covariance Matrix.‚Äù
Econometrica,
55(3), 703‚Äì708. Cited on page 14.
Pagan AR, Schwert GW (1990). ‚ÄúAlternative Models for Conditional Stock
Variability.‚Äù
Journal of Econometrics, 45(1‚Äì2), 267‚Äì290.
doi:10.1016/
0304-4076(90)90101-X. Cited on page 2.
Philippe A, Robert CP (2003). ‚ÄúPerfect Simulation of Positive Gaussian Distri-
butions.‚Äù Computer Science and Mathematics and Statistics, 13(2), 179‚Äì186.
doi:10.1023/A:1023264710933. Cited on page 140.
Phillips PCB, Perron P (1988). ‚ÄúTesting for a Unit Root in Time Series Re-
gression.‚Äù Biometrika, 75(2), 335‚Äì346. doi:10.1093/biomet/75.2.335. Cited
on pages 22 and 123.
Politis DN, Romano JP (1994). ‚ÄúThe Stationary Bootstrap.‚Äù Journal of the
American Statistical Association, 89(428), 1303‚Äì1313. Cited on page 135.
Politis DN, White H (2004).
‚ÄúAutomatic Block-Length Selection for the
Dependent Bootstrap.‚Äù Econometric Reviews, 23(1), 53‚Äì70.
doi:10.1081/
ETC-120028836. Cited on page 136.
Price KV, Storn RM, Lampinen JA (2006). DiÔ¨Äerential Evolution: A Practical
Approach to Global Optimization. Springer Verlag, Berlin, Germany. ISBN
3540209506. Cited on page 179.
R Development Core Team (2007). R: A Language and Environment for Statis-
tical Computing. R Foundation for Statistical Computing, Vienna, Austria.
Version 2.4.1, URL http://www.R-project.org/. Cited on pages 21 and 179.
Ripley B (1987). Stochastic Simulation. Probability and Mathematical Statis-
tics. Whiley, New York, USA, Ô¨Årst edition.
ISBN 0471818844.
Cited on
page 12.

References
199
RiskMetrics Group (1996).
RiskMetrics Technical Document.
J. P. Mor-
gan/Reuters, fourth edition. URL http://www.riskmetrics.com/rmcovv.
html. Cited on page 96.
Ritter C, Tanner MA (1992). ‚ÄúFacilitating the Gibbs Sampler: the Gibbs Stopper
and the Griddy-Gibbs Sampler.‚Äù Journal of the American Statistical Associ-
ation, 87(419), 861‚Äì868. Cited on pages 5 and 12.
Robert CP (1995). ‚ÄúSimulation of Truncated Normal Variables.‚Äù Statistics and
Computing, 5(2), 121‚Äì125. doi:10.1007/BF00143942. Cited on page 140.
Roberts GO, Smith AFM (1994). ‚ÄúSimple Conditions for the Convergence of
the Gibbs Sampler and Metropolis-Hastings Algorithm.‚Äù Stochastic Processes
and their Applications, 49(2), 207‚Äì216. doi:10.1016/0304-4149(94)90134-1.
Cited on pages 11 and 12.
Rosenblatt M (1952). ‚ÄúRemarks on a Multivariate Transformation.‚Äù Annals of
Mathematical Statistics, 23, 470‚Äì472. Cited on page 133.
Schwarz G (1978). ‚ÄúEstimating the Dimension of a Model.‚Äù The Annals of
Statistics, 6(2), 461‚Äì464. Cited on page 136.
Seiler D (2006). Backtesting Multiple-Period Forecasting Models. Master thesis,
Swiss Federal Institute of Technology Z¬®urich. Cited on page 158.
Silverman BW (1986). Density Estimation for Statistics and Data Analysis.
Chapman and Hall, New York, USA, Ô¨Årst edition. ISBN 0412246201. Cited
on pages 15, 37, 141, 143, 151, and 153.
Smith AFM, Roberts GO (1993). ‚ÄúBayesian Computation via the Gibbs Sampler
and Related Markov Chain Monte Carlo Methods.‚Äù Journal of the Royal
Statistical Society, 55(1), 3‚Äì24. Cited on pages 9 and 13.
Spiegelhalter DJ, Best NG, Carlin BP, van der Linde A (2002).
‚ÄúBayesian
Measures of Model Complexity and Fit.‚Äù Journal of the Royal Statistical
Society, 64(4), 583‚Äì639. doi:10.1111/1467-9868.00353. Cited on pages 7, 134,
and 157.
Spiegelhalter DJ, Thomas A, Best NG, Gilks WR (1995).
BUGS: Bayesian
Inference Using Gibbs Sampling. The BUGS project. Version 0.50. Cited on
page 5.
Spiegelhalter DJ, Thomas A, Best NG, Lunn D (2007). WinBUGS User‚Äôs man-
ual. The BUGS project. Version 1.4.1, URL http://www.mrc-bsu.cam.ac.
uk/bugs/. Cited on page 5.
Tanner MA, Wong WH (1987). ‚ÄúThe Calculation of Posterior Distributions
by Data Augmentation.‚Äù Journal of the American Statistical Association,
82(398), 528‚Äì540. Cited on page 11.
Thompson RD, Basu AP (1995). ‚ÄúAsymmetric Loss Functions for Estimat-
ing System Reliability.‚Äù
In DA Berry, KM Chaloner, JF Geweke (eds.),
‚ÄúBayesian Analysis in Statistics and Econometrics,‚Äù Probability and Statis-
tics, pp. 471‚Äì482. John Wiley and Sons Ltd., New York, USA, Ô¨Årst edition.
ISBN 0471118567. Essays in honor of Arnold Zellner. Cited on page 90.
Tierney L (1994). ‚ÄúMarkov Chains for Exploring Posterior Distributions.‚Äù The
Annals of Statistics, 22(4), 1701‚Äì1762. With discussion. Cited on pages 9,
10, 11, 12, 13, and 14.

200
References
Varian HR (1974). ‚ÄúA Bayesian Approach to Real Estate Assessment.‚Äù In
SE Fienberg, A Zellner (eds.), ‚ÄúStudies in Bayesian Econometrics and Statis-
tics,‚Äù Number 86 in Contributions to economic analysis, pp. 195‚Äì208. North
Holland, Amsterdam, NL, Ô¨Årst edition. Cited on page 86.
Vrontos ID, Dellaportas P, Politis DN (2000).
‚ÄúFull Bayesian Inference for
GARCH and EGARCH Models.‚Äù Journal of Business and Economic Statis-
tics, 18(2), 187‚Äì198. Cited on page 4.
Zellner A (1986). ‚ÄúBayesian Estimation and Prediction Using Asymmetric Loss
Functions.‚Äù Journal of the American Statistical Association, 81(394), 446‚Äì
451. Cited on page 87.
Zhu L, Carlin BP (2000).
‚ÄúComparing Hierarchical Models for Spatio-
Temporally Misaligned Data Using the Deviance Information Criterion.‚Äù
Statistics in Medicine, 19, 2265‚Äì2278. doi:10.1002/1097-0258(20000915/30)
19:17/18<2265::AID-SIM568>3.0.CO;2-6. Cited on page 135.
Zumbach G (2006). ‚ÄúBacktesing Risk Methodologies from one Day to one Year.‚Äù
The Journal of Risk, 9(2), 0‚Äì36. Cited on pages 104 and 158.

Index
A
absolute error loss (AEL)
see loss functions
Adaptive Radial-Based
Direction Sampling
(ARDS) . . . . . . . . . see
algorithms
algorithms
Adaptive Radial-Based
Direction Sampling
(ARDS) . . . . . . . . . . . 4
componentwise . . . . . . 13
Expectation Maxi-
mization (EM)
152
Forward Filtering
Backward Sampling
(FFBS). . . . .116, 117
Gibbs . . . . . . . . . . . . 11‚Äì13
Griddy-Gibbs . . 4, 5, 158
importance sampling . 4,
144
independence M-H. . .13
Markov chain Monte
Carlo (MCMC) . . . 3,
10‚Äì15, 155
Metropolis-Hastings
(M-H) . . . . . . 4, 12‚Äì13
permutation sampler
constrained . . 116, 117,
138, 139
random. . . . . . . . .7, 157
random walk Metropolis
13
ARCH . . . . . . . . . . . . . 1, 109
asymmetric. . . . . .see GJR
autocorrelogram. . .22, 123
B
backtesting . . see Value at
Risk
Bayes
(optimal) point
estimate. . . . . . . . . .85
factor (BF) 4, 30, 52, 70,
134, 141, 142, 155
rule. . .10, 18, 41, 58, 115
Bayesian
information (BIC) . . see
information criteria
statistics . . . . . . . . . . 9‚Äì10
bootstrap. . . . . . . . . . . .3, 75
block. . . . . . . . . . . . . . .135
stationary . . . . . . . . . . 135
bridge sampling see model
likelihood
BUGS . . . . . . see softwares
burn-in. . . . . . . . . . . .25, 123
C
clustering . . . see volatility
clustering
componentwise . . . . . . . .see
algorithms
conditional moments
78‚Äì80, 171‚Äì178
conjugate prior . . . . . . . . 10
constrained permutation
sampler. . . . . . . . . .see
algorithms
convergence . . . . . see tests
Cornish-Fisher. .see Value
at Risk
covariance stationary. .see
stationarity
cumulative returns. . . . .77
D
data sets
Deutschmark vs British
Pound (DEM/GBP)
22, 82, 92
Standard & Poors 100
(S&P100) . . . . . . . . 44
Swiss Market Index
(SMI) . . . . . . . . . . . 122
decision theory . . . . . 85‚Äì86
Deutschmark vs British
Pound (DEM/GBP)
see data sets
Deviance information
(DIC) . . . . . . . . . . . see
information criteria
diagnostics . . . . . . see tests
disturbances . . . . . . . . . . see
innovations
E
eÔ¨Äective number of
parameters . . . . . . 134
Expectation Maximiza-
tion (EM) . . . . . . . see
algorithms
Expected Shortfall (ES)
104‚Äì107

202
Index
term structure. . . . . .106
Exponential GARCH . . . 2
F
factor . . . . . . . . . . see Bayes
Ô¨Åltered probabilities. .133,
146, 152
forecasting performance
102‚Äì104, 144‚Äì149,
see also backtesting
Forward Filtering Back-
ward Sampling
(FFBS). . . . . . . . . .see
algorithms
G
GARCH . . . . . . . 1, 109, 155
Normal innovations
17‚Äì36, 77
Gaussian . . . . . see Normal
generalized residuals . . see
residuals
Gibbs. . . . . .see algorithms
GJR . . . . . . . . . . . . . . . . . . . . 2
leverage eÔ¨Äect 39, 40, 44,
48, 111, 125, 157
linear regression. .39‚Äì54
Normal errors . . . . 39‚Äì54
Student-t errors . . 55‚Äì71
Griddy-Gibbs . . . . . . . . . see
algorithms
H
hyperparameters. . . . . . .10
I
identiÔ¨Åcation . . . . . . . . . . see
Markov-switching
importance
density. .4, 138, 139, 141
sampling see algorithms
independence M-H . . . . see
algorithms
ineÔ¨Éciency factor (IF) . 26
information criteria
Bayesian (BIC). . . . .136
Deviance (DIC)134‚Äì137
innovations
Normal . . . . . . . . . . . . . see
GARCH,GJR
Student-t. . . . . .see GJR
invariance . . . . . . . . . . . . .see
Markov-switching
J
Jarque-Bera . . . . . see tests
JeÔ¨Ärey‚Äôs scale of evidence
31
K
Kolmogorov-Smirnov . . see
tests
kurtosis.see leptokurtosis,
unconditional
L
label invariance . . . . . . . see
invariance
latent variable . . 55, 57, 64
leptokurtosis. .95, 125, 157
leverage eÔ¨Äect . . . see GJR
likelihood
approximated 20, 42, 43,
61, 62, 119, 120
function . . 10, 18, 40, 57,
113, 165, 179
maximum (ML) . . 2, 152
model. . . . . . . .see model
likelihood
likelihood ratio statistic 3,
152
linear regression . see GJR
Linex loss . . . . . . . . see loss
functions
Ljung-Box . . . . . . . see tests
loss functions
absolute error (AEL)86,
97, 148
Linex . . . . 86‚Äì88, 97, 148
Monomial . . . . . . . . . . . 90
squared error (SEL).85,
97, 148
M
marginalsee unconditional
Markov chain Monte
Carlo (MCMC) . . see
algorithms
Markov process . . . . . . . 112
Markov-switching
GARCH (MS-GARCH)
109‚Äì110
GJR (MS-GJR)109‚Äì152
identiÔ¨Åcation. . .113, 117
invariance. . . . . .114, 115
multimodality . . 3, 4, 13,
113, 141
state variable . . 109, 134
Maximum Likelihood
(ML). .see likelihood
Metropolis-Hastings
(M-H)see algorithms
model likelihood . 137‚Äì142
bridge sampling138, 139
reciprocal importance
sampling . . . . . . . . 139
model uncertainty . 74, 158
Monomial . . . . . . . . see loss
functions
Monte Carlo . . . . . . . . . . see
algorithms
multi-day ahead Value at
Risk. . . .see Value at
Risk
multimodality. . . . . . . . .see
Markov-switching
N
Normal see GARCH, GJR
numerical standard error
(NSE). . . . . . . . . . . .14
O
one-day ahead Value at
Risk. . . .see Value at
Risk
optimal point estimate see
Bayes
P
p-scores . . . . . see residuals
permutation sampler . . see
algorithms
persistence.2, 34, 109, 114,
158
point estimate . . see Bayes
posterior
density . . . . . . . . . . . . . . 10
risk. . . . . . . . . . . . . . . . . .85
potential scale reduction
factor . . . . . . . . . . . . 25
predictive density . . . . . . 81
prior density. . . . . . . . . . .10

Index
203
probability integral
transforms. . . . . . .see
residuals
R
R . . . . . . . . . . . see softwares
random
permutation samplersee
algorithms
walk Metropolis. . . . .see
algorithms
reciprocal importance
sampling. .see model
likelihood
recursive transformations
20, 43, 61, 120,
161‚Äì164
regime-switching . . . . . . . 2,
155, 158, see also
Markov-switching
regression . . . . . . . see GJR
regulatory capital 100‚Äì101
residuals . . . . . . . . 32, 52, 70
generalized . . . . . . . . . 133
p-scores . . . . . . . . . . . . 133
probability integral
transforms . . . . . . 133
risk measure . . . . . . . . . . see
Expected Shortfall,
Value at Risk
rolling window . 74, 92, 144
S
sampler. . . .see algorithms
sensitivity analysis.30‚Äì31,
52, 70, 141
smoothed probabilities128
softwares
BUGS . . . . . . . . . . . . . . . . 5
R . . . . . . . . . . . . . . . 21, 179
Standard & Poors 100
(S&P100) . . see data
sets
squared error loss (SEL)
see loss functions
state variable . . . . . . . . . see
Markov-switching
stationarity
covariance 3, 36, 53, 110,
126
strict . . . . . . . . . . . . . . . . 36
stochastic volatility . . 134,
158
strict stationary. . . . . . .see
stationarity
Student-t . . . . . . . . see GJR
sub-additivitysee Value at
Risk
Swiss Market Index (SMI)
see data sets
T
term structure . . . . . . . . see
Expected Shortfall,
Value at Risk
tests
convergence. . .14, 24‚Äì25
Jarque-Bera Normality
53
Kolmogorov-Smirnov
empirical distribution
22, 71
Normality . . . 32, 53, 70
Ljung-Box . . . . 32, 53, 70
Wald . . . . . . . 22, 123, 133
transition matrix 112, 114,
118, 158
U
unconditional
kurtosis . . . . . . . . . . 36, 71
variance.36, 53, 126, 127
V
Value at Risk (VaR)73‚Äì83
backtesting 74, 102‚Äì104,
144‚Äì149
Cornish-Fisher expan-
sion . . . . . . . . . . 79, 80,
82
multi-day ahead . 77‚Äì83,
158
one-day ahead . . . . . . 77,
144‚Äì149
sub-additivity . . . . . . 104
term structure. . . . . . .95
volatility clustering . . . . 22
W
Wald . . . . . . . . . . . . see tests
White . . . . . . . . . . . . . 22, 123

Lecture Notes in Economics
and Mathematical Systems
please contact your bookseller or Springer-Verlag
Vol. 525: S. Spinler, Capacity Reservation for Capital-
Intensive Technologies. XVI, 139 pages. 2003.
Vol. 526: C. F. Daganzo, A Theory of Supply Chains. VIII,
123 pages. 2003.
Vol. 527: C. E. Metz, Information Dissemination in
Currency Crises. XI, 231 pages. 2003.
Vol.
528:
R.
Stolletz,
Performance
Analysis
and
Optimization of InboundCallCenters. X, 219 pages. 2003.
Vol. 529: W. Krabs, S. W. Pickl, Analysis, Controllability
and
Optimization
of
Time-Discrete
Systems
and
Dynamical Games. XII, 187 pages. 2003.
Vol. 530: R. Wapler, Unemployment, Market Structure
and Growth. XXVII, 207 pages. 2003.
Vol. 531: M. Gallegati, A. Kirman, M. Marsili (Eds.), The
Complex Dynamics of Economic Interaction. XV, 402
pages, 2004.
Vol. 532: K. Marti, Y. Ermoliev, G. PÔ¨Çug (Eds.), Dynamic
Stochastic Optimization. VIII, 336 pages. 2004.
Vol. 533: G. Dudek, Collaborative Planning in Supply
Chains. X, 234 pages. 2004.
Vol. 534: M. Runkel, Environmental and Resource Policy
for Consumer Durables. X, 197 pages. 2004.
Vol.535:X.Gandibleux,M.Sevaux,K.S√∂rensen,V.T‚Äôkindt
(Eds.), Metaheuristics for Multiobjective Optimisation.
IX, 249 pages. 2004.
Vol. 536: R. Br√ºggemann, Model Reduction Methods for
Vector Autoregressive Processes. X, 218 pages. 2004.
Vol. 537: A. Esser, Pricing in (In)Complete Markets. XI,
122 pages, 2004.
Vol. 538: S. Kokot, The Econometrics of Sequential Trade
Models. XI, 193 pages. 2004.
Vol. 539: N. Hautsch, Modelling Irregularly Spaced
Financial Data. XII, 291 pages. 2004.
Vol. 540: H. Kraft, Optimal Portfolios with Stochastic
2004.
Vol. 541: G.-y. Chen, X. Huang, X. Yang, Vector
Optimization. X, 306 pages. 2005.
Vol. 542: J. Lingens, Union Wage Bargaining and
Economic Growth. XIII, 199 pages. 2004.
Vol. 543: C. Benkert, Default Risk in Bond and Credit
Derivatives Markets. IX, 135 pages. 2004.
Vol. 544: B. Fleischmann, A. Klose, Distribution Logistics.
X, 284 pages. 2004.
Vol. 547: M. Wildi, Signal Extraction. XI, 279 pages. 2005.
Vol. 548: D. Kuhn, Generalized Bounds for Convex
Multistage Stochastic Programs. XI, 190 pages. 2005.
Vol. 549: G. N. Krieg, Kanban-Controlled Manufacturing
Systems. IX, 236 pages. 2005.
Vol. 550: T. Lux, S. Reitz, E. Samanidou, Nonlinear
Dynamics and Heterogeneous Interacting Agents. XIII,
327 pages. 2005.
Vol. 551: J. Leskow, M. Puchet Anyul, L. F. Punzo, New
Tools of Economic Dynamics. XIX, 392 pages. 2005.
Vol. 552: C. Suerie, Time Continuity in Discrete Time
Models. XVIII, 229 pages. 2005.
Vol. 553: B. M√∂nch, Strategic Trading in Illiquid Markets.
XIII, 116 pages. 2005.
Vol. 554: R. Foellmi, Consumption Structure and Macro-
economics. IX, 152 pages. 2005.
Vol. 555: J. Wenzelburger, Learning in Economic Systems
with Expectations Feedback (planned) 2005.
Vol. 556: R. Branzei, D. Dimitrov, S. Tijs, Models in
Cooperative Game Theory. VIII, 135 pages. 2005.
Vol. 557: S. Barbaro, Equity and EfÔ¨Åciency Considerations
of Public Higer Education. XII, 128 pages. 2005.
Vol. 558: M. Faliva, M. G. Zoia, Topics in Dynamic Model
Analysis. X, 144 pages. 2005.
Vol. 559: M. Schulmerich, Real Options Valuation. XVI,
357 pages. 2005.
Vol. 560: A. von Schemde, Index and Stability in Bimatrix
Games. X, 151 pages. 2005.
Vol. 561: H. Bobzin, Principles of Network Economics.
XX, 390 pages. 2006.
Vol.
562:
T.
Langenberg,
Standardization
and
Expectations. IX, 132 pages. 2006.
Vol.
563:
A.
Seeger
(Ed.),
Recent
Advances
in
Optimization. XI, 455 pages. 2006.
Vol. 564: P. Mathieu, B. BeauÔ¨Åls, O. Brandouy (Eds.),
ArtiÔ¨Åcial Economics. XIII, 237 pages. 2005.
Vol. 565: W. Lemke, Term Structure Modeling and
Estimation in a State Space Framework. IX, 224 pages.
2006.
Vol. 566: M. Genser, A Structural Framework for the
Pricing of Corporate Securities. XIX, 176 pages. 2006.
Vol. 545: R. Hafner, Stochastic Implied Volatility. XI, 229
pages. 2004.
Vol. 567: A. Namatame, T. Kaizouji, Y. Aruga (Eds.), The
Complex Networks of Economic Interactions. XI, 343
pages. 2006.
Interest Rates and
Assets. X, 173 pages.
Defaultable
Vol. 546: D. Quadt, Lot-Sizing and Scheduling for Flexible
Flow Lines. XVIII, 227 pages. 2004.
Vol. 568: M. Caliendo, Microeconometric Evaluation of
Labour Market Policies. XVII, 258 pages. 2006.
For information about Vols. 1‚Äì524

Vol.
569: L.
Neubecker, Strategic
Competition
in
Vol. 570: J. Woo, The Political Economy of Fiscal Policy.
X, 169 pages. 2006.
Vol. 571: T. Herwig, Market-Conform Valuation of
Options. VIII, 104 pages. 2006.
Vol. 572: M. F. J√§kel, Pensionomics. XII, 316 pages. 2006
Vol. 573: J. Emami Namini, International Trade and
Multinational Activity. X, 159 pages, 2006.
Vol. 574: R. Kleber, Dynamic Inventory Management in
Reverse Logistics. XII, 181 pages, 2006.
Vol. 575: R. Hellermann, Capacity Options for Revenue
Management. XV, 199 pages, 2006.
Vol. 576: J. Zajac, Economics Dynamics, Information and
Equilibnum. X, 284 pages, 2006.
Vol. 577: K. Rudolph, Bargaining Power Effects in
Financial Contracting. XVIII, 330 pages, 2006.
Vol. 578: J. K√ºhn, Optimal Risk-Return Trade-Offs of
Commercial Banks. IX, 149 pages, 2006.
Vol. 579: D. Sondermann, Introduction to Stochastic
Calculus for Finance. X, 136 pages, 2006.
Vol. 580: S. Seifert, PostedPrice Offers in InternetAuction
Markets. IX, 186 pages, 2006.
Vol. 581: K. Marti; Y. Ermoliev; M. Makowsk; G. PÔ¨Çug
(Eds.), Coping with Uncertainty. XIII, 330 pages, 2006.
Vol. 582: J. Andritzky, Sovereign Default Risks Valuation:
Implications of Debt Crises and Bond Restructurings.
VIII, 251 pages, 2006.
Vol. 583: I.V. Konnov, D.T. Luc, A.M. Rubinov‚Ä† (Eds.),
Generalized Convexity and Related Topics. IX, 469 pages,
2006.
Vol. 584: C. Bruun, Adances in ArtiÔ¨Åcial Economics: The
Economy as a Complex Dynamic System. XVI, 296 pages,
2006.
Vol. 585: R. Pope, J. Leitner, U. Leopold-Wildburger, The
Knowledge AheadApproachtoRisk. XVI, 218 pages, 2007
(planned).
Vol. 586: B.Lebreton, Strategic Closed-Loop Supply Chain
Management. X, 150 pages, 2007 (planned).
Vol. 587: P. N. Baecker, Real Options and Intellectual
Property: Capital Budgeting Under Imperfect Patent
Protection. X, 276 pages , 2007.
Vol. 588: D. Grundel, R. Murphey, P. Panos , O. Prokopyev
(Eds.), Cooperative Systems: Control and Optimization.
IX, 401 pages , 2007.
Vol. 589: M. Schwind, Dynamic Pricing and Automated
Resource
Allocation
for
Information
Services:
Reinforcement Learning and Combinatorial Auctions.
XII, 293 pages , 2007.
Vol. 592: A. C.-L. Chian, Complex Systems Approach to
Economic Dynamics. X, 95 pages, 2007.
Vol.
593:
J.
Rubart,
The
Employment
Effects
of
Technological
Change:
Heterogenous
Labor,
Wage
Vol. 594: R. H√ºbner, Strategic Supply Chain Management
in Process Industries: An Application to Specialty
Chemicals Production Network Design. XII, 243 pages,
Vol. 595: H. Gimpel, Preferences in Negotiations: The
Vol. 596: M. M√ºller-Bungart, Revenue Management
with Flexible Products: Models and Methods for the
Vol. 597: C. Barz, Risk-Averse Capacity Control in
Vol. 598: A. Ule, Partner Choice and Cooperation in
Networks: Theory and Experimental Evidence. Approx.
Vol. 599: A. Consiglio, ArtiÔ¨Åcial Markets Modeling:
Vol.
600:
M.
Hickman,
P.
Mirchandani,
S.
Voss
(Eds.): Computer-Aided Scheduling of Public Transport.
Vol. 601: D. Radulescu, CGE Models and Capital Income
TaxReforms:TheCaseofaDualIncomeTaxforGermany.
Vol. 602: N. Ehrentreich, Agent-Based Modeling: The
Santa
Fe Institute
ArtiÔ¨Åcial
Stock
Market
Model
Vol. 603: D. Briskorn, Sports Leagues Scheduling: Models,
Combinatorial Properties, andOptimization Algorithms.
Vol. 604: D. Brown, F. Kubler, Computational Aspects
of General Equilibrium Theory: Refutable Theories of
Vol. 606: S. von Widekind, Evolution of Non-Expected
Vol. 608: P. Nicola, Experimenting with Dynamic
Vol. 609: X. Fang, K.K. Lai, S. Wang, Fuzzy Portfolio
Vol. 610: M. Hillebrand, Pension Systems, Demographic
Vol. 590: S. H. Oda, Developments on Experimental
Economics: New Approaches to Solving Real-World
Problems. XVI, 262 pages, 2007.
Vol. 607: M. Bouziane, Pricing Interest Rate Derivatives:
A Fourier-Transform Based Approach. XII, 191 pages,
 
 
Vol. 611: R. Brosch, Portfolios of Real Options. XVI, 174
Vol. 605: M. Puhle, Bond Portfolio Optimization. XIV, 137
 
 
 
 
 
 
 
 
 
 
Vol. 612:
Risk Management with
Vol.
591:
M.
Lehmann-Waffenschmidt,
Economic
Evolution and Equilibrium: Bridging the Gap. VIII, 272
pages, 2007.
Oligopolies with Fluctuating Demand. IX, 233 pages,  2006.
Bayesian Estimation of GARCH Models. XII, 203 pages,
D. Ardia, Financial
Inequality and Unemployment. XII, 209 pages, 2007.
2007.
Attachment Effect. XIV, 268 pages, 2007.
Broadcasting Industry. XXI, 297 pages, 2007.
Revenue Management. XIV, 163 pages, 2007.
200 pages, 2007.
Methods and Applications. XV, 277 pages, 2007.
Approx. 424 pages, 2007.
XVI, 168 pages, 2007.
Revisited. XVI, 225 pages, 2007.
XII, 164 pages, 2008.
Value. XII, 202 pages, 2008.
pages, 2008.
Utility Preferences. X, 130 pages, 2008.
2008.
Macromodels: Growth and Cycles. XIII, 241 pages, 2008.
Optimization: Theory and Models. IX, 173 pages, 2008.
Change, and the Stock Market. X, 176 pages, 2008 .
pages, 2008.
2008.


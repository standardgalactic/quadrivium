Lecture Notes in Economics
and Mathematical Systems
Founding Editors:
M. Beckmann
H.P. Künzi
Managing Editors:
Prof. Dr. G. Fandel
Fachbereich Wirtschaftswissenschaften
Fernuniversität Hagen
Feithstr. 140/AVZ II, 58084 Hagen, Germany
Prof. Dr. W. Trockel
Institut für Mathematische Wirtschaftsforschung (IMW)
Universität Bielefeld
Universitätsstr. 25, 33615 Bielefeld, Germany
Editorial Board:
A. Basile, A. Drexl, H. Dawid, K. Inderfurth, W. Kürsten
612

with Bayesian Estimation
of GARCH Models
Financial Risk Management
David Ardia
Theory and Applications

The use of general descriptive names, registered names, trademarks, etc. in this publication does 
protective laws and regulations and therefore free for general use.
not imply, even in the absence of a specific statement, that such names are exempt from the relevant 
Production: le-tex Jelonek, Schmidt & Vöckler GbR, Leipzig
Cover design: WMX Design GmbH, Heidelberg
Printed on acid-free paper
9 8 7 6 5 4 3 2 1
springer.com
Springer-Verlag Berlin Heidelberg
© 2008
Lecture Notes in Economics and Mathematical Systems ISSN 0075-8442
University of Fribourg
1700 Fribourg
Switzerland
ISBN 978-3-540-78656-6
e-ISBN 978-3-540-78657-3
DOI 10.1007/978-3-540-78657-3
Library of Congress Control Number: 2008927201
Regime-Switching GARCH Models. Applications to Financial Risk Management” presented to the
Faculty of Economics and Social Sciences at the University of Fribourg Switzerland by the author. 
Accepted by the Faculty Council on 19 February 2008. The Faculty of Economics and Social Sciences
This book is the Ph.D. dissertation with the original title “Bayesian Estimation of Single-Regime and
A E
Department of Quantitative Economics
Dr. David Ardia
at the University of Fribourg Switzerland neither approves nor disapproves the opinions expressed
in a doctoral dissertation. They are to be considered those of the author. (Decision of the Faculty
Council of 23 January 1990).
Typeset with LT X. Copyright © 2008 David Ardia. All rights reserved.
Bd. de Pérolles 90
david.ardia@unifr.ch

To my nonno, Riziero.

Preface
This book presents in detail methodologies for the Bayesian estimation of single-
regime and regime-switching GARCH models. These models are widespread
and essential tools in ﬁnancial econometrics and have, until recently, mainly
been estimated using the classical Maximum Likelihood technique. As this study
aims to demonstrate, the Bayesian approach oﬀers an attractive alternative
which enables small sample results, robust estimation, model discrimination
and probabilistic statements on nonlinear functions of the model parameters.
The author is indebted to numerous individuals for help in the preparation
of this study. Primarily, I owe a great debt to Prof. Dr. Philippe J. Deschamps
who inspired me to study Bayesian econometrics, suggested the subject, guided
me under his supervision and encouraged my research. I would also like to thank
Prof. Dr. Martin Wallmeier and my colleagues of the Department of Quantitative
Economics, in particular Michael Beer, Roberto Cerratti and Gilles Kaltenrieder,
for their useful comments and discussions.
I am very indebted to my friends Carlos Ord´as Criado, Julien A. Straubhaar,
J´erˆome Ph. A. Taillard and Mathieu Vuilleumier, for their support in the ﬁelds of
economics, mathematics and statistics. Thanks also to my friend Kevin Barnes
who helped with my English in this work.
Finally, I am greatly indebted to my parents and grandparents for their
support and encouragement while I was struggling with the writing of this thesis.
Thanks also to Margaret for her support some years ago. Last but not least,
thanks to you Sophie for your love which puts equilibrium in my life.
Fribourg, April 2008
David Ardia

Table of Contents
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .XIII
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
2
Bayesian Statistics and MCMC Methods . . . . . . . . . . . . . . . . . . . .
9
2.1
Bayesian inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.2
MCMC methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.2.1
The Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2.2
The Metropolis-Hastings algorithm . . . . . . . . . . . . . . . . . . . . 12
2.2.3
Dealing with the MCMC output . . . . . . . . . . . . . . . . . . . . . . 13
3
Bayesian Estimation of the GARCH(1, 1) Model with
Normal Innovations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.1
The model and the priors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.2
Simulating the joint posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.2.1
Generating vector α . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.2.2
Generating parameter β . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.3
Empirical analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
3.3.1
Model estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
3.3.2
Sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.3.3
Model diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.4
Illustrative applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.4.1
Persistence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.4.2
Stationarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4
Bayesian Estimation of the Linear Regression Model with
Normal-GJR(1, 1) Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.1
The model and the priors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
4.2
Simulating the joint posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2.1
Generating vector γ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.2.2
Generating the GJR parameters . . . . . . . . . . . . . . . . . . . . . . . 42
Generating vector α . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
Generating parameter β . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

X
Table of Contents
4.3
Empirical analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.3.1
Model estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.3.2
Sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.3.3
Model diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.4
Illustrative applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
5
Bayesian Estimation of the Linear Regression Model with
Student-t-GJR(1, 1) Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
5.1
The model and the priors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
5.2
Simulating the joint posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.2.1
Generating vector γ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.2.2
Generating the GJR parameters . . . . . . . . . . . . . . . . . . . . . . . 60
Generating vector α . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Generating parameter β . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.2.3
Generating vector ϖ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.2.4
Generating parameter ν . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.3
Empirical analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
5.3.1
Model estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
5.3.2
Sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.3.3
Model diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.4
Illustrative applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
6
Value at Risk and Decision Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 73
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
6.2
The concept of Value at Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
6.2.1
The one-day ahead VaR under the GARCH(1, 1) dynamics 77
6.2.2
The s-day ahead VaR under the GARCH(1, 1) dynamics . 77
6.3
Decision theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
6.3.1
Bayes point estimate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
6.3.2
The Linex loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
6.3.3
The Monomial loss function . . . . . . . . . . . . . . . . . . . . . . . . . . 90
6.4
Empirical application: the VaR term structure . . . . . . . . . . . . . . . . 91
6.4.1
Data set and estimation design . . . . . . . . . . . . . . . . . . . . . . . . 92
6.4.2
Bayesian estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
6.4.3
The term structure of the VaR density . . . . . . . . . . . . . . . . . 95
6.4.4
VaR point estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
6.4.5
Regulatory capital . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
6.4.6
Forecasting performance analysis . . . . . . . . . . . . . . . . . . . . . . 102
6.5
The Expected Shortfall risk measure . . . . . . . . . . . . . . . . . . . . . . . . . 104
7
Bayesian Estimation of the Markov-Switching GJR(1, 1)
Model with Student-t Innovations . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7.1
The model and the priors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.2
Simulating the joint posterior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
7.2.1
Generating vector s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
7.2.2
Generating matrix P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
7.2.3
Generating the GJR parameters . . . . . . . . . . . . . . . . . . . . . . . 118

Table of Contents
XI
Generating vector α . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
Generating vector β . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
7.2.4
Generating vector ϖ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7.2.5
Generating parameter ν . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7.3
An application to the Swiss Market Index . . . . . . . . . . . . . . . . . . . . 122
7.4
In-sample performance analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
7.4.1
Model diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
7.4.2
Deviance information criterion . . . . . . . . . . . . . . . . . . . . . . . . 134
7.4.3
Model likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
7.5
Forecasting performance analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
7.6
One-day ahead VaR density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
7.7
Maximum Likelihood estimation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
8
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
A
Recursive Transformations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
A.1 The GARCH(1, 1) model with Normal innovations . . . . . . . . . . . . . 161
A.2 The GJR(1, 1) model with Normal innovations . . . . . . . . . . . . . . . . 162
A.3 The GJR(1, 1) model with Student-t innovations . . . . . . . . . . . . . . 163
B
Equivalent Speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
C
Conditional Moments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
Computational Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
Abbreviations and Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201

Summary
This book presents in detail methodologies for the Bayesian estimation of single-
regime and regime-switching GARCH models. Our sampling schemes have the
advantage of being fully automatic and thus avoid the time-consuming and
diﬃcult task of tuning a sampling algorithm. The study proposes empirical ap-
plications to real data sets and illustrates probabilistic statements on nonlinear
functions of the model parameters made possible under the Bayesian framework.
The ﬁrst two chapters introduce the work and give a short overview of the
Bayesian paradigm for inference. The next three chapters describe the estima-
tion of the GARCH model with Normal innovations and the linear regression
models with conditionally Normal and Student-t-GJR errors. For these mod-
els, we compare the Bayesian and Maximum Likelihood approaches based on
real ﬁnancial data. In particular, we document that even for fairly large data
sets, the parameter estimates and conﬁdence intervals are diﬀerent between the
methods. Caution is therefore in order when applying asymptotic justiﬁcations
for this class of models. The sixth chapter presents some ﬁnancial applications of
the Bayesian estimation of GARCH models. We show how agents facing diﬀerent
risk perspectives can select their optimal VaR point estimate and document that
the diﬀerences between individuals can be substantial in terms of regulatory cap-
ital. Finally, the last chapter proposes the estimation of the Markov-switching
GJR model. An empirical application documents the in- and out-of-sample su-
periority of the regime-switching speciﬁcation compared to single-regime GJR
models. We propose a methodology to depict the density of the one-day ahead
VaR and document how speciﬁc forecasters’ risk perspectives can lead to diﬀer-
ent conclusions on the forecasting performance of the MS-GJR model.
JEL Classiﬁcation: C11, C13, C15, C16, C22, C51, C52, C53.
Keywords and phrases: Bayesian, MCMC, GARCH, GJR, Markov-switching,
Value at Risk, Expected Shortfall, Bayes factor, DIC.

1
Introduction
(...) “skedasticity refers to the volatility or wiggle of a
time series. Heteroskedastic means that the wiggle itself
tends to wiggle. Conditional means the wiggle of the
wiggle depends on its own past wiggle. Generalized means
that the wiggle of the wiggle can depend on its own past
wiggle in all kinds of wiggledy ways.”
— Kent Osband
Volatility plays a central role in empirical ﬁnance and ﬁnancial risk management
and lies at the heart of any model for pricing derivative securities. Research on
changing volatility (i.e., conditional variance) using time series models has been
active since the creation of the original ARCH (AutoRegressive Conditional
Heteroscedasticity) model in 1982. From there, ARCH models grew rapidly into
a rich family of empirical models for volatility forecasting during the last twenty
years. They are now widespread and essential tools in ﬁnancial econometrics.
In the ARCH(q) speciﬁcation originally introduced by Engle [1982], the con-
ditional variance at time t, denoted by ht, is postulated to be a linear function
of the squares of past q observations {yt−1, yt−2, . . . , yt−q}. More precisely:
ht .= α0 +
q
X
i=1
αiy2
t−i
(1.1)
where the parameters α0 > 0 and αi ⩾0 (i = 1, . . . , q) in order to ensure a pos-
itive conditional variance. In many of the applications with the ARCH model,
a long lag length and therefore a large number of parameters are called for.
To circumvent this problem, Bollerslev [1986] proposed the Generalized ARCH,
or GARCH(p, q), model which extends the speciﬁcation of the conditional vari-
ance (1.1) as follows:
ht .= α0 +
q
X
i=1
αiy2
t−i +
p
X
j=1
βjht−j

2
1 Introduction
where α0 > 0, αi ⩾0 (i = 1, . . . , q) and βj ⩾0 (j = 1, . . . , p). In this case,
the conditional variance depends on its past values which renders the model
more parsimonious. Indeed, in most empirical applications it turns out that the
simple speciﬁcation p = q = 1 is able to reproduce the volatility dynamics of
ﬁnancial data. This has led the GARCH(1, 1) model to become the “workhorse
model” by both academics and practitioners.
Numerous extensions and reﬁnements of the GARCH model have been pro-
posed to mimic additional stylized facts observed in ﬁnancial markets. These
extensions recognize that there may be important nonlinearity, asymmetry, and
long memory properties in the volatility process. Many of these models are
surveyed in Bollerslev, Chou, and Kroner [1992], Bollerslev, Engle, and Nelson
[1994], Engle [2004]. Among them, we may cite the popular Exponential GARCH
model by Nelson [1991] as well as the GJR model by Glosten, Jaganathan, and
Runkle [1993] which both account for the asymmetric relation between stock re-
turns and changes in variance [see Black 1976]. An additional class of GARCH
models, referred to as regime-switching GARCH, has gained particular attention
in recent years. In these models, the scedastic function’s parameters can change
over time according to a latent (i.e., unobservable) variable taking values in the
discrete space {1, . . . , K}. The interesting feature of these models lies in the
fact that they provide an explanation of the high persistence in volatility, i.e.,
nearly unit root process for the conditional variance, observed with single-regime
GARCH models [see, e.g., Lamoureux and Lastrapes 1990]. Furthermore, these
models are apt to react quickly to changes in the volatility level which leads
to signiﬁcant improvements in volatility forecasts as shown by Dueker [1997],
Klaassen [2002], Marcucci [2005]. Further details on regime-switching GARCH
models can be found in Haas, Mittnik, and Paolella [2004], Hamilton and Susmel
[1994].
The Maximum Likelihood (henceforth ML) estimation technique is the gen-
erally favored scheme of inference for GARCH models, although semi- and non-
parametric techniques have also been applied by some authors [see, e.g., Gallant
and Tauchen 1989, Pagan and Schwert 1990]. The primary appeal of the ML
technique stems from the well-known asymptotic optimality conditions of the
resulting estimators under ideal conditions [see Bollerslev et al. 1994, Lee and
Hansen 1994]. In addition, the ML procedure is straightforward to implement
and is nowadays available in econometric packages. However, while conceptually
simple, we may encounter practical diﬃculties when dealing with the ML esti-
mation of GARCH models. First, the maximization of the likelihood function
must be achieved via a constrained optimization technique. The model param-
eters must indeed be positive to ensure a positive conditional variance and it

1 Introduction
3
is also common to require that the covariance stationarity condition holds (this
condition is Pq
i=1 αi + Pp
j=1 βj < 1 for the GARCH(p, q) model [see Boller-
slev 1986, Thm.1, p.310]). The optimization procedure subject to inequality
constraints can be cumbersome and does not necessarily converge if the true pa-
rameter values are close to the boundary of the parameter space or if the process
is nearly non-stationary. The maximization is even more diﬃcult to achieve in
the context of regime-switching GARCH models where the likelihood surface is
multimodal. Depending on the numerical algorithm, ML estimates often prove
to be sensitive with respect to starting values. Moreover, the covariance matrix
at the optimum can be extremely tedious to obtain and ad-hoc approaches are
often required to get reliable results (e.g., Hamilton and Susmel [1994] ﬁx some
transition probabilities to zero in order to determine the variance estimates for
some model parameters). Second, as noted by Geweke [1988, p.77], in classical
applications of GARCH models, the interest usually does not center directly on
the model parameters but on possibly complicated nonlinear functions of the
parameters. For instance, in the case of the GARCH(p, q) model, one might be
interested in the unconditional variance, denoted by hy, which is given by:
hy .=
α0
1 −Pq
i=1 αi −Pp
j=1 βj
provided that the covariance stationarity condition is satisﬁed. To assess the
uncertainty of this quantity, classical inference involves tedious delta methods,
simulation from the asymptotic Normal approximation of the parameter esti-
mates or the bootstrap methodology. However, none of these techniques is com-
pletely satisfactory. The delta method is an approximation which can be crude
if the function of interest is highly nonlinear. The simulation and the bootstrap
approaches can deal with nonlinear functions of the model parameters and give
a full description of their distribution. Nevertheless, the former technique relies
on asymptotic justiﬁcations and the latter method is very demanding since at
each step of the procedure, a GARCH model is ﬁtted to the bootstrapped data.
Finally, in the case of regime-switching GARCH models, testing the null of K
versus K′ states is not possible within the classical framework. The regularity
conditions for justifying the χ2 approximation of the likelihood ratio statistic
do not hold as some parameters are undeﬁned under the null hypothesis [see
Fr¨uhwirth-Schnatter 2006, Sect.4.4].
Fortunately, diﬃculties disappear when Bayesian methods are used. First,
any constraints on the model parameters can be incorporated in the model-
ing through appropriate prior speciﬁcations. Moreover, the recent development
of computational methods based on Markov chain Monte Carlo (henceforth

4
1 Introduction
MCMC) procedures can be used to explore the joint posterior distribution of the
model parameters. These techniques avoid local maxima commonly encountered
via ML estimation of regime-switching GARCH models. Second, exact distribu-
tions of nonlinear functions of the model parameters can be obtained at low cost
by simulating from the joint posterior distribution. In particular, we will show
in Chap. 6 that, upon assuming that the underlying process is of GARCH type,
the well known Value at Risk risk measure (henceforth VaR) can be expressed
as a function of the model parameters. Therefore, the Bayesian approach gives
an adequate framework to estimate the full density of the VaR. In conjunction
with the decision theory framework, this allows to optimally choose a single
point estimate within the density of the VaR, given our risk preferences. Hence,
the Bayesian approach has a clear advantage in combining estimation and de-
cision making. Lastly, in the Bayesian framework, the issue of determining the
number of states can be addressed by means of model likelihood and Bayes fac-
tors. All these reasons strongly motivate the use of the Bayesian approach when
estimating GARCH models.
The choice of the algorithm is the ﬁrst issue when dealing with MCMC meth-
ods and it depends on the nature of the problem under study. In the case of
GARCH models, due to the recursive nature of the conditional variance, the
joint posterior and the full conditional densities are of unknown forms, what-
ever distributional assumptions are made on the model disturbances. Therefore,
we cannot use the simple Gibbs sampler and need more elaborate estimation
procedures. The initial approaches have been implemented using importance
sampling [see Geweke 1988, 1989, Kleibergen and van Dijk 1993]. More re-
cent studies include the Griddy-Gibbs sampler [see Aus´ın and Galeano 2007,
Bauwens and Lubrano 1998] or the Metropolis-Hastings (henceforth M-H) algo-
rithm with some speciﬁc choice of the proposal densities. The Normal random
walk Metropolis is used in M¨uller and Pole [1998], Vrontos, Dellaportas, and
Politis [2000], Adaptive Radial-Based Direction Sampling (henceforth ARDS)
is proposed by Bauwens, Bos, van Dijk, and van Oest [2004] while Nakatsuma
[1998, 2000] constructs proposal densities from an auxiliary process. In the con-
text of regime-switching ARCH models, Kaufmann and Fr¨uhwirth-Schnatter
[2002], Kaufmann and Scheicher [2006] use the method of Nakatsuma [1998,
2000] while Bauwens, Preminger, and Rombouts [2006], Bauwens and Rom-
bouts [2007] rely on the Griddy-Gibbs sampler for regime-switching GARCH
models.
In the importance sampling approach, a suitable importance density is re-
quired for eﬃciency which can be a bit of an art, especially if the posterior
density is asymmetric or multimodal. In the random walk and independence M-

1 Introduction
5
H strategies, preliminary runs and tuning are necessary. Therefore, the method
cannot be completely automatic which is not a desirable property. The Griddy-
Gibbs sampler of Ritter and Tanner [1992] is used by Bauwens and Lubrano
[1998] in the context of GARCH models to get rid of these diﬃculties. This
methodology consists in updating each parameter by inversion from the distri-
bution computed by a deterministic integration rule. However, the procedure is
time consuming and this can become a real burden for regime-switching mod-
els which involve many parameters. Moreover, for computational eﬃciency, we
must limit the range where the probability mass is computed so that the prior
density has to be somewhat informative. In the case of the ARDS algorithm of
Bauwens et al. [2004], the method involves a reparametrization in order to en-
hance the eﬃciency of the estimation. This technique requires a large number of
evaluations, which signiﬁcantly slows down the estimation procedure compared
to usual M-H approaches. Lastly, one could also use a Bayesian software such as
BUGS [see Spiegelhalter, Thomas, Best, and Gilks 1995, Spiegelhalter, Thomas,
Best, and Lunn 2007] for estimating GARCH models. However, this becomes
extremely slow as the number of observations increases mainly due to the recur-
sive nature of the conditional variance process. Moreover, the implementation
of speciﬁc constraints on the model parameters is diﬃcult and extensions to
regime-switching speciﬁcations are limited.
In the rest of the book, we will use the approach suggested by Nakatsuma
[1998, 2000] which relies on the M-H algorithm where some model parameters
are updated by blocks. The proposal densities are constructed from an auxiliary
ARMA process for the squared observations. This methodology has the advan-
tage of being fully automatic and thus avoids the time-consuming and diﬃcult
task, especially for non-experts, of choosing and tuning a sampling algorithm.
We obtained very high acceptance rates with this M-H algorithm, ranging from
89% to 95% for the single-regime GARCH(1, 1) model, which indicates that the
proposal densities are close to the full posteriors. In addition, the approach of
Nakatsuma [1998, 2000] is easy to extend to regime-switching GARCH models.
In this case, the parameters in each regime can be regrouped and updated by
blocks which may enhance the sampler’s eﬃciency.
Organization of the book
A short introduction to Bayesian inference and MCMC methods is given in
Chap. 2. The rest of the book treats in detail the methodologies for the Bayesian
estimation of single-regime and regime-switching GARCH models, proposes em-
pirical applications to real data sets and illustrates some probabilistic state-

6
1 Introduction
ments on nonlinear functions of the model parameters made possible under the
Bayesian framework.
In Chap. 3, we propose the Bayesian estimation of the parsimonious but
eﬀective GARCH(1, 1) model with Normal innovations. We detail the MCMC
scheme based on the methodology of Nakatsuma [1998, 2000]. An empirical
application to a foreign exchange rate time series is presented where we compare
the Bayesian and the ML estimates. In particular, we show that even for a fairly
large data set, the point estimates and conﬁdence intervals are diﬀerent between
the methods. Caution is therefore in order when applying the asymptotic Normal
approximation for the model parameters in this case. We perform a sensitivity
analysis to check the robustness of our results with respect to the choice of
the priors and test the residuals for misspeciﬁcation. Finally, we compare the
theoretical and sample autocorrelograms of the process and test the covariance
and strict stationarity conditions.
In Chap. 4, we consider the linear regression model with conditionally
heteroscedastic errors and exogenous or lagged dependent variables. We ex-
tend the symmetric GARCH model to account for asymmetric responses to
past shocks in the conditional variance process. To that aim, we consider the
GJR(1, 1) model of Glosten et al. [1993]. We ﬁt the model to the Standard and
Poors 100 (henceforth S&P100) index log-returns and compare the Bayesian and
the ML estimations. We perform a prior sensitivity analysis and test the resid-
uals for misspeciﬁcation. Finally, we test the covariance stationarity condition
and illustrate the diﬀerences between the unconditional variance of the process
obtained through the Bayesian approach and the delta method. In particular,
we show that the Bayesian framework leads to a more precise estimate.
In Chap. 5, we extend the linear regression model with conditionally hetero-
scedastic errors by considering Student-t disturbances, which allows to model
extreme shocks in a convenient manner. In the Bayesian approach, the heavy-
tails eﬀect is created by the introduction of latent variables in the variance
process as proposed by Geweke [1993]. An empirical application based on the
S&P100 index log-returns is proposed with a comparison between the estimated
joint posterior and the asymptotic Normal approximation of the distribution of
the estimates. We perform a prior sensitivity analysis and test the residuals for
misspeciﬁcation. Finally, we analyze the conditional and unconditional kurtosis
of the underlying time series.
In Chap. 6, we present some ﬁnancial applications of the Bayesian estima-
tion of GARCH models. We introduce the concept of Value at Risk risk measure
and propose a methodology to estimate the density of this quantity for diﬀer-
ent risk levels and time horizons. This gives us the possibility to determine the

1 Introduction
7
VaR term structure and to characterize the uncertainty coming from the model
parameters. Then, we review some basics in decision theory and use this frame-
work as a rational justiﬁcation for choosing a point estimate of the VaR. We
show how agents facing diﬀerent risk perspectives can select their optimal VaR
point estimate and document, in an illustrative application, that the diﬀerences
between individuals, in particular between fund managers and regulators, can
be substantial in terms of regulatory capital. We show that the common testing
methodology for assessing the performance of the VaR is unable to discrimi-
nate between the point estimates but the deviations are large enough to imply
substantial diﬀerences in terms of regulatory capital. This therefore gives an
additional ﬂexibility to the user when allocating risk capital. Finally, we extend
our methodology to the Expected Shortfall risk measure.
In Chap. 7, we extend the single-regime GJR model to the regime-switching
GJR model (henceforth MS-GJR); more precisely, we consider an asymmetric
version of the Markov-switching GARCH(1, 1) speciﬁcation of Haas et al. [2004].
We introduce a novel MCMC scheme which can be viewed as an extension of
the sampler proposed by Nakatsuma [1998, 2000]. Our approach allows to gen-
erate the parameters of the MS-GJR model by blocks which may enhance the
sampler’s eﬃciency. As an application, we ﬁt a single-regime and a Markov-
switching GJR model to the Swiss Market Index log-returns. We use the random
permutation sampler of Fr¨uhwirth-Schnatter [2001b] to ﬁnd suitable identiﬁca-
tion constraints for the MS-GJR model and show the presence of two distinct
volatility regimes in the time series. The generalized residuals are used to test
the models for misspeciﬁcation. By using the Deviance information criterion
of Spiegelhalter, Best, Carlin, and van der Linde [2002] and by estimating the
model likelihoods using the bridge sampling technique of Meng and Wong [1996],
we show the in-sample superiority of the MS-GJR model. To test the predictive
performance of the models, we run a forecasting analysis based on the VaR.
In particular, we compare the MS-GJR model to a single-regime GJR model
estimated on rolling windows and show that both models perform equally well.
However, contrary to the single-regime model, the Markov-switching model is
able to anticipate structural breaks in the conditional variance process and needs
to be estimated only once. Then, we propose a methodology to depict the density
of the one-day ahead VaR by simulation and document how speciﬁc forecasters’
risk perspectives can lead to diﬀerent conclusions on forecasting performance
of the model. A comparison with the traditional ML approach concludes the
chapter.
Finally, we summarize the main results of the book and discuss future av-
enues of research in Chap. 8.

2
Bayesian Statistics and MCMC Methods
“The people who don’t know they are Bayesian are called
non-Bayesian.”
— Irving J. Good
This chapter gives a short introduction to the Bayesian paradigm for inference
and an overview of the Markov chain Monte Carlo (henceforth MCMC) algo-
rithms used in the rest of the book. For a more thorough discussion on Bayesian
statistics, the reader is referred to Koop [2003], for instance. Further details on
MCMC methods can be found in Chib and Greenberg [1996], Smith and Roberts
[1993], Tierney [1994]. The reader who is familiar with these topics can skip this
part of the book and go to the ﬁrst chapter dedicated to the Bayesian estimation
of GARCH models, on page 17.
The plan of this chapter is as follows. The Bayesian paradigm is introduced
in Sect. 2.1. MCMC techniques are presented in Sect. 2.2 where we introduce
the Gibbs sampler as well as the Metropolis-Hastings algorithm. We also brieﬂy
discuss some practical implementation issues.
2.1 Bayesian inference
As in the classical approach to inference, the Bayesian estimation assumes a
T × 1 vector y .= (y1 · · · yT )′ of observations described through a probability
density p(y | θ). The parameter θ ∈Θ serves as an index of the family of
possible distributions for the observations. It represents the characteristics of
interest one would wish to know in order to obtain a complete description of the
generating process for y. It can be a scalar, a vector, a matrix or even a set of
these mathematical objects. For simplicity, we will consider θ as a d-dimensional
vector, hence θ ∈Θ ⊆Rd in what follows.
The diﬀerence between the Bayesian and the classical approach lies in the
mathematical nature of θ. In the classical framework, it is assumed that there
exists a true and ﬁxed value for parameter θ. Conversely, the Bayesian approach

10
2 Bayesian Statistics and MCMC Methods
considers θ as a random variable which is characterized by a prior density de-
noted by p(θ). The prior is speciﬁed with the help of parameters called hyper-
parameters which are initially assumed to be known and constant. Moreover,
depending on the researcher’s prior information, this density can be more or less
informative. Then, by coupling the likelihood function of the model parameters,
L(θ | y) ≡p(y | θ), with the prior density, we can invert the probability density
using Bayes’ rule to get the posterior density p(θ | y) as follows:
p(θ | y) =
L(θ | y)p(θ)
R
Θ L(θ | y)p(θ)dθ .
(2.1)
This posterior is a quantitative, probabilistic description of the knowledge about
the parameter θ after observing the data.
It is often convenient to choose a prior density which is conjugate to the like-
lihood. That is, a density that leads to a posterior which belongs to the same
distributional family as the prior. In eﬀect, conjugate priors permit posterior
densities to emerge without numerical integration. However, the easy calcula-
tions of this speciﬁcation comes with a price due to the restrictions they impose
on the form of the prior. In many cases, it is unlikely that the conjugate prior
is an adequate representation of the prior state of knowledge. In such cases, the
evaluation of (2.1) is analytically intractable, so asymptotic approximations or
Monte Carlo methods are required. Deterministic techniques can provide good
results for low dimensional models. However, when the dimension of the model
becomes large, simulation is the only way to approximate the posterior density.
2.2 MCMC methods
The idea of MCMC sampling was ﬁrst introduced by Metropolis, Rosenbluth,
Rosenbluth, Teller, and Teller [1953] and was subsequently generalized by Hast-
ings [1970]. For ease of exposition, we will restrict the presentation to the context
of Bayesian inference. A general and detailed statistical theory of MCMC meth-
ods can be found in Tierney [1994].
The MCMC sampling strategy relies on the construction of a Markov chain
with realizations θ[0], θ[1], . . . , θ[j], . . . in the parameter space Θ. Under appropri-
ate regularity conditions [see Tierney 1994], asymptotic results guarantee that
as j tends to inﬁnity, then θ[j] tends in distribution to a random variable whose
density is p(θ | y). Hence, the realized values of the chain can be used to make
inference about the joint posterior. All we require are algorithms for construct-
ing appropriately behaved chains. The best known MCMC algorithms are the

2.2 MCMC methods
11
Gibbs sampler and the Metropolis-Hastings (henceforth M-H) algorithm. These
samplers are nowadays essential tools to perform realistic Bayesian inference.
2.2.1 The Gibbs sampler
The Gibbs sampler is possibly the MCMC sampling technique which is used
most frequently. In the statistical physics literature, it is known as the heat bath
algorithm. Geman and Geman [1984] christened it in the mainstream statisti-
cal literature as the Gibbs sampler. An elementary exposition can be found in
Casella and George [1992]. See also Gelfand and Smith [1990], Tanner and Wong
[1987] for practical examples.
The Gibbs sampler is an algorithm based on successive generations from
the full conditional densities p(θi | θ̸=i, y), i.e., the posterior density of the ith
element of θ .= (θ1 · · · θd)′, given all other elements, where elements of θ can be
scalars or sub-vectors. In practice the sampler works as follows:
1. Initialize the iteration counter of the chain to j = 1 and
set an initial value θ[0] .= (θ[0]
1 · · · θ[0]
d )′;
2. Generate a new value θ[j] from θ[j−1] through successive
generation values:
θ[j]
1 ∼p(θ1 | θ[j−1]
̸=1
, y)
θ[j]
2 ∼p(θ2 | θ[j]
1 , θ[j−1]
3
, . . . , θ[j−1]
d
, y)
...
θ[j]
d ∼p(θd | θ[j]
̸=d, y);
3. Change counter j to j + 1 and go back to step 2 until
convergence is reached.
As the number of iterations increases, the chain approaches its stationary dis-
tribution and convergence is then assumed to hold approximately [see Tierney
1994]. Suﬃcient conditions for the convergence of the Gibbs sampler are given
in Roberts and Smith [1994, Sect.4]. As noted in Chib and Greenberg [1996,
p.414], these conditions ensure that each full conditional density is well deﬁned
and that the support of the joint posterior is not separated into disjoint regions
since this would prevent exploration of the full parameter space. Although these
are only suﬃcient conditions for the convergence of the Gibbs sampler, they are
extremely weak and are satisﬁed in most applications.
The Gibbs sampler is the most natural choice of MCMC sampling strategy
when it is easy to write down full conditionals from which we can easily generate

12
2 Bayesian Statistics and MCMC Methods
draws. When the expression of p(θi | θ̸=i, y) is nonstandard, we might consider
rejection methods [see, e.g., Ripley 1987], the Griddy-Gibbs sampler when θi is
univariate [see Ritter and Tanner 1992], adaptive rejection sampling [see Gilks
and Wild 1992] or M-H sampling as shown in the next section.
2.2.2 The Metropolis-Hastings algorithm
Some complicated Bayesian problems cannot be solved by using the Gibbs sam-
pler. This is the case when it is not easy to break down the joint density into
full conditionals or when the full conditional densities are of unknown form.
The M-H algorithm is a simulation scheme which allows to generate draws from
any density of interest whose normalizing constant is unknown. The algorithm
consists of the following steps.
1. Initialize the iteration counter to j = 1 and set an initial
value θ[0];
2. Move the chain to a new value θ⋆generated from a pro-
posal (candidate) density q(• | θ[j−1]);
3. Evaluate the acceptance probability of the move from
θ[j−1] to θ⋆given by:
min

p(θ⋆| y)
p(θ[j−1] | y)
q(θ[j−1] | θ⋆)
q(θ⋆| θ[j−1]), 1

.
If the move is accepted, set θ[j] .= θ⋆, if not, set θ[j] .= θ[j−1]
so that the chain does not move;
4. Change counter from j to j+1 and go back to step 2 until
convergence is reached.
As in the Gibbs sampler, the chain approaches its equilibrium distribution as
the number of iterations increases [see Tierney 1994]. The power of the M-H
algorithm stems from the fact that the convergence of the chain is obtained
for any proposal q whose support includes the support of the joint posterior
[see Roberts and Smith 1994, Sect.5]. It is however crucial that q approximates
closely the posterior to guarantee an acceptance rate which is reasonable.
With no intention of being exhaustive, some comments are in order here.
If we choose a symmetric proposal density, i.e., q(θ[j] | θ⋆) = q(θ⋆| θ[j]), the
acceptance probability of the M-H algorithm reduces to:
min
 p(θ⋆| y)
p(θ[j] | y), 1


2.2 MCMC methods
13
so that the proposal does not need to be evaluated. This simpler version of
the M-H algorithm is known as the Metropolis algorithm because it is the orig-
inal algorithm by Metropolis et al. [1953]. A special case consists of a pro-
posal density which only depends on the distance between θ⋆and θ[j−1], i.e.,
q(θ⋆| θ[j−1]) = q(θ⋆−θ[j−1]). The resulting algorithm is referred to as the
random walk Metropolis algorithm. For instance, q could be a multivariate Nor-
mal density centered at previous draw θ[j−1] and whose covariance matrix is
calibrated to take steps which are reasonably close to θ[j−1] such that the prob-
ability of accepting the candidate is not too low, but with a step size large
enough to ensure a suﬃcient exploration of the parameter space. The drawback
of this method is that it is not fully automatic since the covariance matrix needs
to be chosen carefully; thus preliminary runs are required.
Another special case of the M-H sampler is the independence M-H algorithm,
in which proposal draws are generated independently of the current position of
the chain, i.e., q(θ⋆| θ[j−1]) = q(θ⋆). This algorithm is often used with a Normal
or a Student-t proposal density whose moments are estimated from previous runs
of the MCMC sampler. This approach works well for well-behaved unimodal
posterior densities but may be very ineﬃcient if the posterior is asymmetric or
multimodal.
Finally, we note that in the form of the M-H algorithm we have presented,
the vector θ is updated in a single block at each iteration so that all elements
are changed simultaneously. However, we could also consider componentwise
algorithms where each component is generated by its own proposal density [see
Chib and Greenberg 1995, Tierney 1994]. In fact, the Gibbs belongs to this class
of samplers where each component is updated sequentially, and where proposal
densities are the full conditionals. In this case, new draws are always accepted
[see Chib and Greenberg 1995]. The M-H algorithm is often used in conjunction
with the Gibbs sampler for those components of θ that have a conditional density
that cannot be sampled from directly, typically because the density is known
only up to a scale factor [see Tierney 1994].
2.2.3 Dealing with the MCMC output
Having examined the building-blocks for the standard MCMC samplers, we now
discuss some issues associated with their practical implementation. In particular,
we comment on the manner we can assess their convergence, the way we can
account for autocorrelation in the chains and how we can obtain characteristics
of the joint posterior from the MCMC output. Further details can be found in
Kass, Carlin, Gelman, and Neal [1998], Smith and Roberts [1993].

14
2 Bayesian Statistics and MCMC Methods
Several statistics have been devised for assessing convergence of MCMC out-
puts. The basic idea behind most of them is to compare moments of the sampled
parameters at diﬀerent parts of the chain. Alternatively, we can compare several
sequences drawn from diﬀerent starting points and check that they are indistin-
guishable as the number of iterations increases. We refer the reader to Cowles
and Carlin [1996], Gelman [1995] for a comparative review of these techniques.
In the rest of the book, we will use a methodology based on the analysis of
variance developed by Gelman and Rubin [1992]. More precisely, the approxi-
mate convergence is diagnosed when the variance between diﬀerent sequences
is no larger than the variance within each individual sequence. Apart from for-
mal diagnostic tests, it is also often convenient to check convergence by plotting
the parameters’ draws over iterations (trace plots) as well as the cumulative or
running mean of the drawings.
Regarding the Monte Carlo (simulation) error, it is crucial to understand
that the draws generated by a MCMC method are not independent. The auto-
correlation either comes from the fact that the new draw depends on the past
value of the chain or that the old element is duplicated. When assessing the pre-
cision of an estimator, we must therefore rely on estimation techniques which
account for this autocorrelation [see, e.g., Geweke 1992, Newey and West 1987].
In the rest of the book, we will estimate the numerical standard errors, that is
the variation of the estimates that can be expected if the simulations were to be
repeated, by the method of Andrews [1991], using a Parzen kernel and AR(1)
pre-whitening as presented in Andrews and Monahan [1992]. As noted by De-
schamps [2006], this ensures easy, optimal, and automatic bandwidth selection.
After the run of a Markov chain and its convergence to the stationary distri-
bution, a sample {θ[j]}J
j=1 from the joint posterior density p(θ | y) is available.
We can thus approximate the posterior expectation of any function ξ(θ) of the
model parameters:
Eθ|y

ξ(θ)

=
Z
Θ
ξ(θ)p(θ | y)dθ
(2.2)
by averaging over the draws from the posterior distribution in the following
manner:
ξ .= 1
J
J
X
j=1
ξ(θ[j]) .
Under mild conditions, the sample average ξ converges to the posterior expec-
tation by the law of large numbers, even if the draws are generated by a MCMC
sampler [see Tierney 1994]. Some particular cases of (2.2) allow to obtain char-
acteristics of the joint posterior. For instance, when ξ(θ) .= θ we obtain the

2.2 MCMC methods
15
posterior mean vector θ; for ξ(θ) .= (θ −θ)(θ −θ)′ we obtain the posterior co-
variance matrix; for ξ(θ) .= I{θ∈C}, where I{•} denotes the indicator function
which is equal to one if the constraint holds and zero otherwise, we obtain the
posterior probability of a set C. Finally, if we are interested in the marginal
posterior density of a single component of θ, we can estimate it through a his-
togram or a kernel density estimate of sampled values [see Silverman 1986]. By
contrast, deterministic numerical integration is often intractable.

3
Bayesian Estimation of the GARCH(1, 1) Model
with Normal Innovations
“Large changes tend to be followed by large changes (of
either sign) and small changes tend to be followed by
small changes.”
— Benoˆıt Mandelbrot
(...) “it is remarkable how large a sample is required for
the Normal distribution to be an accurate approximation.”
— Robert McCulloch and Peter E. Rossi
In this chapter, we propose the Bayesian estimation of the parsimonious but ef-
fective GARCH(1, 1) model with Normal innovations. We sample the joint poste-
rior distribution of the parameters using the approach suggested by Nakatsuma
[1998, 2000]. As a ﬁrst step, we ﬁt the model to foreign exchange log-returns
and compare the Bayesian and the Maximum Likelihood estimates. Next, we
analyze the sensitivity of our results with respect to the choice of the priors
and test the residuals for misspeciﬁcation. Finally, we illustrate some appealing
aspects of the Bayesian approach through probabilistic statements made on the
parameters.
The plan of this chapter is as follows. We set up the model in Sect. 3.1.
The MCMC scheme is detailed in Sect. 3.2. The empirical results are presented
in Sect. 3.3. We conclude with some illustrative applications of the Bayesian
approach in Sect. 3.4.
3.1 The model and the priors
A GARCH(1, 1) model with Normal innovations may be written as follows:
yt = εth1/2
t
for t = 1, . . . , T
εt
iid
∼N(0, 1)
ht .= α0 + α1y2
t−1 + βht−1
(3.1)

18
3 The GARCH(1, 1) Model with Normal Innovations
where α0 > 0, α1 ⩾0 and β ⩾0 to ensure a positive conditional variance
and h0 = y0 .= 0 for convenience; N(0, 1) is the standard Normal density. In
this setting, the conditional variance ht is a linear function of the squared past
observation and the past variance.
In order to write the likelihood function, we deﬁne the vectors y .= (y1 · · · yT )′
and α .= (α0 α1)′ and we regroup the model parameters into ψ .= (α, β) for
notational purposes. In addition, we deﬁne the T × T diagonal matrix:
Σ .= Σ(ψ) = diag
 {ht(ψ)}T
t=1

where:
ht(ψ) .= α0 + α1y2
t−1 + βht−1(ψ) .
From there, the likelihood function of ψ can be expressed as follows:
L(ψ | y) ∝(det Σ)−1/2 exp

−1
2y′Σ−1y

.
We propose the following proper priors on the parameters α and β of the
preceding model:
p(α) ∝N2(α | µα, Σα)I{α>0}
p(β) ∝N(β | µβ, Σβ)I{β>0}
where µ• and Σ• are the hyperparameters, I{•} is the indicator function which
equals unity if the constraint holds and zero otherwise, 0 is a 2 × 1 vector of
zeros and Nd is the d-dimensional Normal density (d > 1). In addition, we
assume prior independence between parameters α and β which implies that
p(ψ) = p(α)p(β). Then, we construct the joint posterior density via Bayes’ rule:
p(ψ | y) ∝L(ψ | y)p(ψ) .
(3.2)
3.2 Simulating the joint posterior
The recursive nature of the variance equation in model (3.1) does not allow
for conjugacy between the likelihood function and the prior density in (3.2).
Therefore, we rely on the M-H algorithm to draw samples from the joint posterior
distribution. The algorithm in this section is a special case of the algorithm
described by Nakatsuma [1998, 2000]. We draw an initial value ψ[0] .= (α[0], β[0])
from the joint prior and we generate iteratively J passes for ψ. A single pass is

3.2 Simulating the joint posterior
19
decomposed as follows:
α[j] ∼p(α | β[j−1], y)
β[j] ∼p(β | α[j], y) .
Since no full conditional density is known analytically, we sample parameters
α and β from two proposal densities. These densities are obtained by noting
that the GARCH(1, 1) model can be written as an ARMA(1, 1) model for {y2
t }.
Indeed, by deﬁning wt .= y2
t −ht, we can transform the expression of the condi-
tional variance as follows:
ht = α0 + α1y2
t−1 + βht−1
⇔y2
t = α0 + (α1 + β)y2
t−1 −βwt−1 + wt
(3.3)
where wt can be written as:
wt .= y2
t −ht =
y2
t
ht
−1

ht =
 χ2
1 −1

ht
with χ2
1 denoting a Chi-squared variable with one degree of freedom. Hence, by
construction, {wt} is a Martingale Diﬀerence process with a conditional mean
of zero and a conditional variance of 2h2
t since a χ2
1 variable has a unit mean
and a variance equal to two.
Following Nakatsuma [1998, 2000], we construct an approximate likelihood
for parameters α and β from expression (3.3). The procedure consists in ap-
proximating ﬁrst the variable wt by a variable zt which is Normally distributed
with a mean of zero and a variance of 2h2
t. This leads to the following auxiliary
model:
y2
t = α0 + (α1 + β)y2
t−1 −βzt−1 + zt .
Then, by noting that zt is a function of ψ given by:
zt(ψ) = y2
t −α0 −(α1 + β)y2
t−1 + βzt−1(ψ)
(3.4)
and by deﬁning the T × 1 vector z .= (z1 · · · zT )′ as well as the T × T diagonal
matrix:
Λ .= Λ(ψ) = diag
 {2h2
t(ψ)}T
t=1

we can approximate the likelihood function of ψ from the auxiliary model as
follows:
L(ψ | y) ∝(det Λ)−1/2 exp

−1
2z′Λ−1z

.
(3.5)

20
3 The GARCH(1, 1) Model with Normal Innovations
As will be shown hereafter, the construction of the proposal densities for pa-
rameters α and β is based on this approximate likelihood function.
3.2.1 Generating vector α
Recursive transformations initially proposed by Chib and Greenberg [1994] allow
to express the function zt(ψ) in (3.4) as a linear function of the 2 × 1 vector α.
Let us deﬁne vt .= y2
t for notational convenience. The recursive transformations
are deﬁned as follows:
l∗
t
.= 1 + β l∗
t−1
v∗
t
.= vt−1 + β v∗
t−1
where l∗
0 = v∗
0
.= 0. As shown in Prop. A.1 (see App. A), upon deﬁning the
2×1 vector ct .= (l∗
t v∗
t )′, the function zt can be expressed as zt = vt−c′
tα. Then,
by considering the T × 1 vector v .= (v1 · · · vT )′ and the T × 2 matrix C whose
tth row is c′
t, we get z = v −Cα. Therefore, we can express the approximate
likelihood function of parameter α as follows:
L(α | β, y) ∝(det Λ)−1/2 exp

−1
2(v −Cα)′Λ−1(v −Cα)

.
The proposal density to sample vector α is obtained by combining this likelihood
function and the prior density by the usual Bayes update:
qα(α | eα, β, y) ∝N2(α | bµα, bΣα)I{α>0}
with:
bΣ−1
α
.= C′eΛ−1C + Σ−1
α
bµα
.= bΣα(C′eΛ−1v + Σ−1
α µα)
where the T ×T diagonal matrix eΛ .= diag
 {2h2
t(eα, β)}T
t=1

and eα is the previous
draw of α in the M-H sampler. A candidate α⋆is sampled from this proposal
density and accepted with probability:
min
p(α⋆, β | y)
p(eα, β | y)
qα(eα | α⋆, β, y)
qα(α⋆| eα, β, y), 1

.
3.2.2 Generating parameter β
The function zt(ψ) in (3.4) could be expressed, in the previous section, as a
linear function of parameter α but cannot be expressed as a linear function of

3.2 Simulating the joint posterior
21
β. To overcome this problem, we linearize zt(β) by a ﬁrst order Taylor expansion
at point eβ:
zt(β) ≃zt(eβ) + dzt
dβ

β=eβ
× (β −eβ)
where eβ is the previous draw of parameter β in the M-H sampler. Furthermore,
let us deﬁne the following:
rt .= zt(eβ) + eβ∇t
,
∇t .= −dzt
dβ

β=eβ
where the terms ∇t can be computed by the following recursion:
∇t .= y2
t−1 −zt−1(eβ) + eβ∇t−1
with ∇0 .= 0. This recursion is simply obtained by diﬀerentiating (3.4) with
respect to β. Then, we regroup these terms into the T ×1 vectors r .= (r1 · · · rT )′
and ∇.= (∇1 · · · ∇T )′ and we approximate the term within the exponential
in (3.5) by z ≃r−β∇. This yields the following approximate likelihood function
for parameter β:
L(β | α, y) ∝(det Λ)−1/2 exp

−1
2(r −β∇)′Λ−1(r −β∇)

.
The proposal density to sample β is obtained by combining this likelihood and
the prior density by Bayes’ update:
qβ(β | α, eβ, y) ∝N(β | bµβ, bΣβ)I{β>0}
with:
bΣ−1
β
.= ∇′eΛ−1∇+ Σ−1
β
bµβ
.= bΣβ(∇′eΛ−1r + Σ−1
β µβ)
where the T × T diagonal matrix eΛ .= diag
 {2h2
t(α, eβ)}T
t=1

. A candidate β⋆is
sampled from this proposal density and accepted with probability:
min
(
p(α, β⋆| y)
p(α, eβ | y)
qβ(eβ | α, β⋆, y)
qβ(β⋆| α, eβ, y)
, 1
)
.
We end this section with some comments regarding the implementation of the
MCMC scheme. The program is written in the R language [see R Development
Core Team 2007] with some subroutines implemented in C in order to speed up

22
3 The GARCH(1, 1) Model with Normal Innovations
the simulation procedure. The validity of the algorithm as well as the correctness
of the computer code are veriﬁed by a variant of the method proposed by Geweke
[2004]. We sample ψ from a proper joint prior and generate some passes of the
M-H algorithm. At each pass, we simulate the dependent variable y from the
full conditional p(y | ψ) which is given by the conditional likelihood. This way,
we draw a sample from the joint density p(y, ψ). If the algorithm is correct, the
resulting replications of ψ should reproduce the prior. The Kolmogorov-Smirnov
empirical distribution test does not reject this hypothesis at the 1% signiﬁcance
level.
3.3 Empirical analysis
We apply our Bayesian estimation method to daily observations of the Deutsch-
mark vs British Pound (henceforth DEM/GBP) foreign exchange log-returns.
The sample period is from January 3, 1985, to December 31, 1991, for a total of
1’974 observations. The nominal returns are expressed in percent as in Bollerslev
and Ghysels [1996]. This data set has been proposed as an informal benchmark
for GARCH time series software validation and is available from the Journal of
Business and Economic Statistics at ftp://www.amstat.org/. From this time
series, the ﬁrst 750 observations, which is about three ﬁnancial years, are used
to illustrate the Bayesian approach. The data set is large enough to perform
classical Maximum Likelihood (henceforth ML) estimation and apply asymp-
totic justiﬁcations. Hence, we have an interesting point of view from which to
compare classical and Bayesian approaches. The remaining data set will be used
in an empirical analysis proposed in Chap. 6.
The observation window excerpt from our data set is plotted in the upper
part of Fig. 3.1. We test for autocorrelation in the time series by testing the
joint nullity of autoregressive coeﬃcients for {yt}. We estimate the regression
with autoregressive coeﬃcients up to lag 20 and compute the covariance matrix
using the White estimate. The p-values of the Wald test is 0.377 which does
not support the presence of autocorrelation. However, from Fig. 3.1, we clearly
observe clusters of high and low variability in the time series. This phenomenon
is well known in ﬁnancial data and is referred to as volatility clustering. This
eﬀect is emphasized in the lower part of the ﬁgure where the sample autocorrelo-
gram of squared observations is displayed. In this case, the ﬁrst autocorrelations
are large and signiﬁcant, indicating GARCH eﬀects; the Wald test strongly re-
jects the null hypothesis of the absence of autocorrelation in the squares. As an
additional data analysis, we test for unit root using the test by Phillips and Per-
ron [1988]. The test strongly rejects the I(1) hypothesis. From this preliminary

3.3 Empirical analysis
23
1
250
500
750
−3
−2
−1
0
1
2
3
time
index
Daily log−returns
(in percent)
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
Sample autocorrelogram
time
lag
Fig. 3.1. DEM/GBP foreign exchange daily log-returns (upper graph) and sample
autocorrelogram of the squared log-returns (lower graph).
analysis, we conclude that the time series is not integrated and does not exhibit
autocorrelation. However, we strongly suspect the presence of GARCH eﬀects
in the data.

24
3 The GARCH(1, 1) Model with Normal Innovations
3.3.1 Model estimation
We ﬁt the parsimonious GARCH(1, 1) model to the data for this observation
window. As prior densities for the Bayesian estimation, we choose truncated
Normal densities with zero mean vectors and diagonal covariance matrices. The
variances are set to 10’000 so we do not introduce tight prior information in our
estimation (see Sect. 3.3.2 for a formal check). Finally, we recall that the joint
prior is constructed by assuming prior independence between α and β. We run
two chains for 10’000 passes each. We emphasize the fact that only positivity
constraints are implemented in the M-H algorithm, through the prior densities;
no stationarity conditions are imposed in the simulation procedure. In addition,
we estimate the model by the usual ML technique for comparison purposes.
In Fig. 3.2, the running means are plotted over iterations. For all param-
eters, we notice a convergence of the two chains toward a constant value after
something like 5’000 iterations. As a formal check, we follow Gelman and Rubin
[1992] where the authors elaborated the idea that the chain trajectories should
be the same after convergence using analysis of variance techniques. Considering
m parallel chains and a real function ξ .= ξ(ψ) of the model parameters, there
are m trajectories of length J given by {ξ[j]
i }J
j=1, i = 1, . . . , m. The variances
between chains and within chains, respectively, denoted by B and W, are then
deﬁned as follows:
B .=
J
m −1
m
X
i=1
(ξi −ξ)2
W .=
1
m(J −1)
m
X
i=1
J
X
j=1
(ξ[j]
i
−ξi)2
where ξi is the average of observations of the ith chain and ξ is the average of
these averages. After convergence, all these mJ values for ξi are drawn from the
posterior distribution, and σ2
ξ, the variance of ξ, can be consistently estimated
by W, B as well as the following weighted average:
bσ2
ξ
.=

1 −1
J

W + 1
J B .
If the chains have not yet converged, then initial values will still be inﬂuencing
the trajectories and, due to their overdispersion, will force bσ2
ξ to overestimate
σ2
ξ until stationarity is reached. On the other hand, before convergence, W will
tend to underestimate σ2
ξ because each chain will not have adequately traversed
the complete state space. Following this reasoning, Gelman and Rubin [1992]
construct an indicator of convergence; this is the estimator of potential scale

3.3 Empirical analysis
25
reduction factor given by:
bR .=
s
bσ2
ξ
W .
As the simulation converges, the potential scale reduction declines to one, mean-
ing that the m parallel chains are essentially overlapping. Gelman and Rubin
[1992] suggests accepting convergence when the value of bR is below 1.2. Since
this indicator is subject to estimation error, asymptotic conﬁdence bands can be
constructed and the 97.5th percentile is used as a conservative point estimate.
In our context, we test the convergence of the chains by using the following
functions:
ξ(ψ) = α0
,
ξ(ψ) = α1
and
ξ(ψ) = β .
For these three functions, the diagnostic test by Gelman and Rubin [1992] does
not lead to the rejection of the convergence if we consider the second half of the
simulated values; the 97.5th percentile values for bR indeed belong to the interval
[1.04, 1.05]. We can therefore be conﬁdent that the generated parameters are
drawn from the joint posterior distribution.
Complementary analyses of the MCMC output are also worth mentioning at
this point. In particular, we note that the one-lag autocorrelations in the chains
range from 0.75 for parameter α1 to 0.95 for β which is reasonable. Moreover,
the sampling algorithm allows to reach very high acceptance rates ranging from
89% for vector α to 95% for β, suggesting that the proposal densities are close
to the full conditionals. On the basis of these results, we discard the ﬁrst 5’000
draws from the overall MCMC output as a burn-in period and merge the two
chains to get a ﬁnal sample of length 10’000.
The posterior statistics as well as the ML results are reported in Table 3.1.
First, we note that even though the number of observations is large, the ML es-
timates and the Bayesian posterior means are diﬀerent; the ML point estimate
is lower for components of vector α and higher for parameter β. We also notice
a diﬀerence between the 95% conﬁdence intervals. Whereas the conﬁdence band
is symmetric in the ML case due to the asymptotic Normality assumption, this
is not true for the posterior conﬁdence intervals. The reason can be explained
through Fig. 3.3 where the marginal posterior densities of the parameters are
displayed. We clearly notice the asymmetric shape of the histograms for parame-
ters α0 and α1; the skewness values are 0.46 and 0.39, both signiﬁcantly diﬀerent
from zero at the 1% signiﬁcance level. Therefore the ML conﬁdence band has
a tendency to underestimate the right boundary of the 95% conﬁdence interval
for these parameters. In the case of parameter β, the skewness is −0.09, also
signiﬁcant; in this case, the Maximum Likelihood approach overestimates the

26
3 The GARCH(1, 1) Model with Normal Innovations
Table 3.1. Estimation results for the GARCH(1, 1) model with Normal
innovations.⋆
ψ
ψMLE
ψ
ψ0.5
ψ0.025
ψ0.975
min
max
IF
α0
0.039
0.048
0.047
0.022
0.080
0.011
0.119
9.79
[0.014,0.064]
(0.448)
α1
0.198
0.226
0.223
0.128
0.337
0.083
0.499
5.85
[0.102,0.294]
(1.284)
β
0.686
0.636
0.636
0.476
0.795
0.338
0.849
40.79
[0.538,0.833]
(5.021)
⋆ψMLE: Maximum Likelihood estimate; ψ: posterior mean; ψφ: estimated pos-
terior quantile at probability φ; min: minimum value; max: maximum value; IF:
ineﬃciency factor (i.e., ratio of the squared numerical standard error and the
variance of the sample mean from a hypothetical iid sampler); [•]: Maximum
Likelihood 95% conﬁdence interval; (•): numerical standard error (×103). The
posterior statistics are based on 10’000 draws from the joint posterior sample.
left boundary of the 95% conﬁdence band. Moreover, as shown in the bottom
right-hand side of the ﬁgure, the joint density of parameters α0 and β is slightly
diﬀerent from the ellipsoid obtained with the asymptotic Normal approximation.
Therefore, these results warn us against the abusive use of asymptotic justiﬁ-
cations. In the present case, even 750 observations do not suﬃce to justify the
asymptotic Normal approximation for the parameters estimates.
The last column of Table 3.1 reports the ineﬃciency factors (IF) for the
diﬀerent parameters. Their values are computed as the ratio of the squared nu-
merical standard error of the posterior sample and the variance estimate divided
by the number of iterations (i.e., the variance of the sample mean from a hy-
pothetical iid sequence). The numerical standard errors are estimated by the
method of Andrews [1991], using a Parzen kernel and AR(1) pre-whitening as
presented in Andrews and Monahan [1992]. As noted by Deschamps [2006], this
ensures easy, optimal, and automatic bandwidth selection. In our estimation,
using 10’000 simulations out of the posterior distribution seems appropriate if
we require that the Monte Carlo error in estimating the mean is smaller than
0.4% of the variation of the error due to the data. The larger ineﬃciency factor
reported for parameter β is reﬂected in a larger autocorrelation in the simulated
values.

3.3 Empirical analysis
27
Parameter α0
0
2
4
6
8
10
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
iterations
(x 1000)
Parameter α1
0
2
4
6
8
10
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
iterations
(x 1000)
Parameter β
0
2
4
6
8
10
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
iterations
(x 1000)
Fig. 3.2. Running means of the chains over iterations (up to 10’000). The acceptance rate ranges from 89% for vector α to 95% for
parameter β. The autocorrelations range from 0.75 for α1 to 0.95 for β. The convergence diagnostic test by Gelman and Rubin [1992]
indicates convergence of the chains from iteration 5’000; the 97.5th percentile of the potential reduction factor ranges from 1.04 to 1.05.

28
3 The GARCH(1, 1) Model with Normal Innovations
Parameter α0
0.02
0.04
0.06
0.08
0.10
0.12
0
500
1000
1500
Parameter α1
0.1
0.2
0.3
0.4
0.5
0
500
1000
1500
Fig. 3.3. Marginal posterior densities of the GARCH(1, 1) parameters; upper graph:
parameter α0; lower graph: parameter α1. The histograms are based on 10’000 draws
from the joint posterior sample.

3.3 Empirical analysis
29
Parameter β
0.3
0.4
0.5
0.6
0.7
0.8
0
200
400
600
800
1000
0.02
0.04
0.06
0.08
0.10
0.12
0.4
0.5
0.6
0.7
0.8
Parameter α0
Parameter β
Fig. 3.3. (cont.) Marginal posterior densities of the GARCH(1, 1) parameters; upper
graph: parameter β; lower graph: scatter plot of (α0, β). Both graphs are based on
10’000 draws from the joint posterior sample.

30
3 The GARCH(1, 1) Model with Normal Innovations
3.3.2 Sensitivity analysis
The Bayesian approach is often criticized on the grounds that the choice of the
prior density may have a non negligible impact on the posterior density and,
consequently, bias the posterior results. It is therefore important to determine
the extent of this impact through a sensitivity analysis. To that aim, we follow
Geweke [1999] who proposes a methodology to estimate the Bayes factors for
the initial model against a model with an alternative prior. While the Bayes
factor is a quantity which is often diﬃcult to estimate, Geweke [1999, Sect.2]
shows that it is possible to approximate the Bayes factor between two models
diﬀering only by their prior densities using the posterior simulation output from
just one of the models. This approach provides an attractive way of performing
sensitivity analysis since it does not require the estimation of the alternative
model.
More precisely, let us denote by pI(ψ) the initial prior density for ψ .= (α, β)
and by pA(ψ) the alternative prior used to test the sensitivity of the posterior
density. Based on the T ×1 vector of observations y .= (y1 · · · yT )′, the Bayes fac-
tor in favor of the alternative model A over the initial model I can be expressed
as follows:
BFA≻I = p(y | A)
p(y | I)
where the marginal densities are found by integrating out the parameters:
p(y | •) =
Z
L(ψ | y)p•(ψ)dψ .
Developing the Bayes factor using the expression of the marginal densities yields:
BFA≻I =
R
L(ψ | y)pA(ψ)dψ
R
L(ψ | y)pI(ψ)dψ
=
Z
L(ψ | y)pI(ψ) pA(ψ)
pI(ψ) dψ
R
L(ψ | y)pI(ψ)dψ
=
Z pA(ψ)
pI(ψ)

L(ψ | y)pI(ψ)
R
L(ψ | y)pI(ψ)dψ

dψ
=
Z pA(ψ)
pI(ψ) p(ψ | y, I)dψ
= Eψ|(y,I)
pA(ψ)
pI(ψ)

where the notation Eψ|(y,I) emphasizes the fact that the posterior expectation
is calculated with respect to the initial prior pI. In this simple context, we
thus notice that the Bayes factor is nothing else than the posterior expectation

3.3 Empirical analysis
31
under the initial prior of the ratio of prior densities. The posterior expectation
can therefore be estimated using the joint posterior sample {ψ[j]}J
j=1 as follows:
BFA≻I = Eψ|(y,I)
pA(ψ)
pI(ψ)

≈1
J
J
X
j=1
pA(ψ[j])
pI(ψ[j]) .
(3.6)
We test the sensitivity of our posterior results by considering three alterna-
tive priors which are truncated Normal densities as the initial prior. We choose
however diﬀerent hyperparameters, in particular larger variances in the covari-
ance matrices. Formally, the alternative priors may be expressed as follows:
p(α) ∝N2(α | µ ι2, σ2I2)I{α>0}
p(β) ∝N(β | µ, σ2)I{β>0}
where ι2 is a 2 × 1 vector of ones, I2 is a 2 × 2 identity matrix, µ is the prior
mean and σ2 the prior variance; their values are given in the ﬁrst two columns
of Table 3.2. The Bayes factors are estimated using approximation (3.6) based
on 10’000 draws from the joint posterior sample. The discrimination between
models is then based on the Jeﬀrey’s scale of evidence [see Kass and Raftery
1995, Sect.3.2] which can be summarized as follows:
• Strong evidence in favor of the initial prior compared to the alter-
native prior:
BFA≻I < 0.1
• Moderate evidence in favor of the initial prior compared to the
alternative prior:
0.1 ⩽BFA≻I < 0.3125
• Weak evidence in favor of the initial prior compared to the alter-
native prior:
0.3125 ⩽BFA≻I < 1 .
Estimated BF are reported in the last column of Table 3.2. The numerical
standard errors are not shown since their values are negligible. First, we note
that a change in the prior mean has no impact on the BF. On the contrary,
larger variances in the alternative covariance matrices diminishes the value of
Bayes factors to 0.866; this indicates a weak evidence for the initial speciﬁca-
tion relative to the alternative priors. Therefore, for each alternative prior, the
estimated BF conﬁrms that our initial choice is vague enough and does not
introduce signiﬁcant information in our estimation.

32
3 The GARCH(1, 1) Model with Normal Innovations
Table 3.2. Results of the sensitivity
analysis.⋆
Alternative priors
µ
σ2
BF
1.00
10’000
1.000
0.00
11’000
0.866
1.00
11’000
0.866
⋆The alternative priors are truncated
Normal densities; µ: prior mean; σ2 prior
variance; BF: Bayes factor.
3.3.3 Model diagnostics
We test the residuals for possible misspeciﬁcation. The standardized residuals
are deﬁned by:
bεt .=
yt
bh1/2
t
for t = 1, . . . , 750, where bht is the conditional variance computed with ψ0.5, the
median of the posterior sample. If the statistical assumptions in (3.1) are sat-
isﬁed, these residuals should be independent and Normally distributed asymp-
totically.
In the upper part of Fig. 3.4, we display the residuals over time. No au-
tocorrelation or heteroscedasticity are visually apparent. We test for autocor-
relation using the Ljung-Box test up to lag 20 [see Ljung and Box 1978]. The
test does not reject the null hypothesis of absence of autocorrelation at the 5%
signiﬁcance level (p-value = 0.652). This is also true for the squared residuals
(p-value = 0.961). Therefore, the GARCH(1, 1) process has been able to ﬁlter
the heteroscedastic nature of the data. We form a quantile-quantile plot of the
residuals against the Normal distribution in the lower graph of the ﬁgure. The
distribution is almost Normal at its center whereas the tails are slightly fatter,
especially the left one. The Kolmogorov-Smirnov Normality test rejects the null
hypothesis at the 5% signiﬁcance level (p-value = 0.008). The tails of the innova-
tions’ distribution are not fat enough to fully capture the distributional nature
of the data. This point will be addressed in Chap. 5 with the introduction of
Student-t disturbances in the modeling.

3.3 Empirical analysis
33
−6
−4
−2
0
2
4
6
1
250
500
750
time
index
Residuals
Normal quantiles
Sample quantiles
−3
−2
−1
0
1
2
3
−4
−3
−2
−1
0
1
2
3
4
Quantile−quantile plot
Fig. 3.4. Residuals time series (upper graph) and Normal quantile-quantile plot (lower
graph).

34
3 The GARCH(1, 1) Model with Normal Innovations
3.4 Illustrative applications
In this section, we illustrate some probabilistic statements made possible under
the Bayesian framework. The joint posterior sample is used to simulate nonlinear
functions of the model parameters.
3.4.1 Persistence
As pointed out in Sect. 3.2, a GARCH(1, 1) process for {yt} is equivalent
to an ARMA(1, 1) process for {y2
t } with an autoregressive coeﬃcient (α1 + β)
and a moving average coeﬃcient −β. Consequently, the autocorrelation function
(henceforth ACF) of the squared observations comes from the standard formulae
for the ARMA(1, 1) model. It is recursively given by:
ρi .= (α1 + β) × ρi−1
for i > 1, where the ﬁrst order autocorrelation is:
ρ1 .= α1(1 −β2 −α1β)
1 −β2 −2α1β
.
The term (α1 + β) is the degree of persistence in the autocorrelation of the
squares which controls the intensity of the clustering in the variance process.
With a value close to one, past shocks and past variances will have a longer im-
pact on the future conditional variance. An autoregressive coeﬃcient (α1+β) = 1
corresponds to a unit root process for squared observations.
To make inference on the persistence and ACF of the squared process, we
simply use the posterior sample and generate (α[j]
1 + β[j]) as well as ρ[j]
i
for
j = 1, . . . , 10’000 and i = 1, . . . , 20. The posterior density of the persistence
(α1 + β) is plotted in the upper part of Fig. 3.5. The histogram is left-skewed
with a median value of 0.865 and a maximum value of 0.992. In this case, the
integration for the variance process is not supported by the data. In the lower
part of the ﬁgure, we display the posterior ACF with its 95% and 99% conﬁdence
bands together with the sample autocorrelations of the squared observations.
Although a single observation, at lag 11, lies outside the conﬁdence bands, the
autocorrelation structure of the estimated GARCH(1, 1) model is in line with
the data.

3.4 Illustrative applications
35
α1 + β
0.7
0.8
0.9
1.0
0
500
1000
1500
5
10
15
20
0.0
0.2
0.4
0.6
0.8
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1
5
10
15
20
Theoretical and
sample autocorrelograms
time
lag
 
 
 
+
median
95% confidence band
99% confidence band
sample autocorrelation
Fig. 3.5. Posterior density of the persistence (upper graph) and posterior autocor-
relogram (lower graph) of the squared observations. Both graphs are based on 10’000
draws from the joint posterior sample.

36
3 The GARCH(1, 1) Model with Normal Innovations
3.4.2 Stationarity
In the case of the GARCH(1, 1) model with Normal innovations, Bollerslev [1986,
Thm.1, p.310] and Nelson [1990, Thm.2, p.320] gave the conditions for covari-
ance stationarity (CSC) and strict stationarity (SSC), respectively. These con-
ditions are given by:
CSC .= α1 + β −1 < 0
SSC .= E

ln(α1ε2
t + β)

< 0
where the error term εt is Normally distributed. As pointed out in Sect. 3.3,
no stationarity condition has been imposed in the M-H algorithm. The joint
posterior sample can therefore be used to estimate the posterior density of these
functions by generating:
CSC[j] .= α[j]
1 + β[j] −1
SSC[j] .= 1
K
K
X
k=1
ln
 α[j]
1 (η[k])2 + β[j]
for j = 1, . . . , 10’000, where η[k] is a draw from a standard Normal distribution
and K is set large enough (we choose K = 1’000 in our application). In Fig. 3.6,
we present the Gaussian kernel density estimates of the posterior densities for
CSC and SSC. As we can notice, none of these values exceed zero in our sim-
ulation study. Thus, the estimated model is covariance stationary and strictly
stationary.
We conclude this section by noting that other probabilistic statements on
interesting functions of the model parameters can be obtained using the joint
posterior sample. For instance, the posterior median is 0.341 for the uncon-
ditional variance and 4.54 for the unconditional kurtosis. They approximately
correspond to the sample estimations of 0.323 and 4.63.

3.4 Illustrative applications
37
−0.5
−0.4
−0.3
−0.2
−0.1
0.0
0
2
4
6
8
10
−0.5
−0.4
−0.3
−0.2
−0.1
0.0
0
2
4
6
8
10
Covariance stationarity
Strict stationarity
Fig. 3.6. Posterior densities of the covariance stationarity and strict stationarity con-
ditions. Gaussian kernel density estimates with bandwidth selected by the“Silverman’s
rule of thumb” criterion [see Silverman 1986, p.48]. Both kernel density estimates are
based on 10’000 draws from the joint posterior sample.

4
Bayesian Estimation of the Linear Regression
Model with Normal-GJR(1, 1) Errors
“Overall, these results show a greater impact on volatility
of negative, rather than positive return shocks.”
— Robert F. Engle and Victor K. Ng
In this chapter, we propose the Bayesian estimation of the linear regression
model with conditionally heteroscedastic errors. In the context of time series
regressions, the regression part can include exogenous or lagged dependent vari-
ables. Moreover, we extend the traditional GARCH speciﬁcation of the errors
to account for asymmetric movements between the conditional variance and the
underlying process. The volatility tends to rise more in response to bad news
than to good news and this phenomenon is especially true on equity markets.
This eﬀect was ﬁrst observed by Black [1976] and is referred to as the lever-
age eﬀect in the ﬁnancial literature. One explanation of this empirical fact is
that negative returns increase ﬁnancial leverage which extends the company’s
risk and therefore the variance. To cope with this stylized fact, we use the GJR
model of Glosten et al. [1993]. In this setting, the conditional variance can react
asymmetrically depending on the sign of the past shocks due to the introduction
of dummy variables. The appealing aspect of this model is that it encompasses
the symmetric GARCH. In addition, the MCMC scheme presented in Sect. 3.2
can easily be extended for this asymmetric model in order to ﬁnd proposal den-
sities for the parameters. As a ﬁrst illustration, we ﬁt the model to the S&P100
index log-returns and compare the Bayesian and the Maximum Likelihood es-
timates. Next, we perform a prior sensitivity analysis and test the residuals for
misspeciﬁcation. Finally, we estimate the density of the unconditional variance
of the process.
The plan of this chapter is as follows. We set up the model in Sect. 4.1.
The MCMC scheme is detailed in Sect. 4.2. The empirical results are presented
in Sect. 4.3. We conclude with some illustrations of the Bayesian approach in
Sect. 4.4.

40
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
4.1 The model and the priors
A linear regression model with Normal-GJR(1,1) errors may be written as fol-
lows:
yt = x′
tγ + ut
for t = 1, . . . , T
ut = εth1/2
t
εt
iid
∼N(0, 1)
ht .= α0 + (α1I{ut−1⩾0} + α2I{ut−1<0})u2
t−1 + βht−1
(4.1)
where α0 > 0, α1 ⩾0, α2 ⩾0 and β ⩾0 to ensure a positive conditional
variance and h0 = y0 .= 0 for convenience; yt is a scalar dependent variable; xt
is a m×1 vector of exogenous or lagged dependent variables; γ is a m×1 vector
of regression coeﬃcients; N(0, 1) is the standard Normal density. In this setting,
the conditional variance ht is a linear function of the squared past shock and
the past variance but contrary to the GARCH model, the conditional variance
can react asymmetrically to past shocks depending on their signs. The leverage
eﬀect is present if α2 > α1 so that the conditional variance is higher after a
negative shock than a positive shock.
In order to write the likelihood function, we deﬁne the vectors y .= (y1 · · · yT )′
and α .= (α0 α1 α2)′ as well as the T × m matrix X whose tth row is given by
x′
t. We regroup the model parameters into ψ .= (γ, α, β) for notational purposes
and deﬁne the T × T diagonal matrix:
Σ .= Σ(ψ) = diag
 {ht(ψ)}T
t=1

where:
ht(ψ) .= α0 + (α1I{ut−1(γ)⩾0} + α2I{ut−1(γ)<0})u2
t−1(γ) + βht−1(ψ)
ut(γ) .= yt −x′
tγ .
Then, we regroup the error terms ut(γ) into the T × 1 vector u .= (u1 · · · uT )′
and express the likelihood function of ψ as follows:
L(ψ | y, X) ∝(det Σ)−1/2 exp

−1
2u′Σ−1u

.
(4.2)
We propose the following proper priors on the parameters γ, α and β of the
preceding model:

4.2 Simulating the joint posterior
41
p(γ) = Nm(γ | µγ, Σγ)
p(α) ∝N3(α | µα, Σα)I{α>0}
p(β) ∝N(β | µβ, Σβ)I{β>0} .
where µ• and Σ• are the hyperparameters, 0 is a 3 × 1 vector of zeros, I{•} is
the indicator function and Nd is the d-dimensional Normal density (d > 1). In
addition, we assume prior independence between parameters γ, α and β which
yields the following joint prior:
p(ψ) = p(γ)p(α)p(β) .
Then, we construct the joint posterior density via Bayes’ rule:
p(ψ | y, X) ∝L(ψ | y, X)p(ψ) .
4.2 Simulating the joint posterior
As in the GARCH model of Chap. 3, the recursive nature of the variance equa-
tion does not allow for conjugacy between the likelihood function and the joint
prior density. Hence, we rely again on the M-H algorithm to draw samples from
the joint posterior distribution. We draw an initial value ψ[0] .= (γ[0], α[0], β[0])
from the joint prior and we generate iteratively J passes for ψ. A single pass is
decomposed as follows:
γ[j] ∼p(γ | α[j−1], β[j−1], y, X)
α[j] ∼p(α | γ[j], β[j−1], y, X)
β[j] ∼p(β | γ[j], α[j], y, X) .
Since no full conditional density is known analytically, we sample the parameters
γ, α and β from three proposal densities.
4.2.1 Generating vector γ
The proposal density to sample the m × 1 vector γ is obtained by combining
the likelihood function (4.2) and the prior density by the usual Bayes update:
qγ(γ | eγ, α, β, y, X) = Nm(γ | bµγ, bΣγ)
with:

42
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
bΣ−1
γ
.= X′eΣ−1X + Σ−1
γ
bµγ
.= bΣγ(X′eΣ−1y + Σ−1
γ µγ)
where the T × T diagonal matrix eΣ .= diag
 {ht(eγ, α, β)}T
t=1

and eγ is the
previous draw of γ in the M-H sampler. A candidate γ⋆is sampled from this
proposal density and accepted with probability:
min
p(γ⋆, α, β | y, X)
p(eγ, α, β | y, X)
qγ(eγ | γ⋆, α, β, y, X)
qγ(γ⋆| eγ, α, β, y, X), 1

.
4.2.2 Generating the GJR parameters
The proposal densities to generate the parameters α and β are obtained in
the same manner as in Sect. 3.2. However, since we have a regression term
which appears in the model, we estimate the GJR parameters from the errors
ut .= yt −x′
tγ instead of yt. An approximate likelihood function for (α, β) is
then constructed from the process {u2
t}. Note that in the case of a GJR model
for {ut}, we do not end up with an ARMA process for {u2
t} as in the GARCH
model since we have two dummy variables which appear in the expression of the
conditional variance. Indeed, by deﬁning wt .= u2
t −ht, we can transform the
expression of the conditional variance as follows:
ht = α0 + (α1I{ut−1⩾0} + α2I{ut−1<0})u2
t−1 + βht−1
⇔u2
t = α0 + (α1I{ut−1⩾0} + α2I{ut−1<0} + β)u2
t−1 −βwt−1 + wt
where wt can be written as wt = (χ2
1 −1)ht, χ2
1 denoting a Chi-squared variable
with one degree of freedom. As in the GARCH case, the sequence {wt} is a
Martingale Diﬀerence process where the variable wt has a conditional mean of
zero and a conditional variance of 2h2
t.
Following the methodology of Sect. 3.2, we approximate the variable wt by
a variable zt which is Normally distributed with a mean of zero and a variance
of 2h2
t. This leads to the following auxiliary model:
u2
t = α0 + (α1I{ut−1⩾0} + α2I{ut−1<0} + β)u2
t−1 −βzt−1 + zt .
Then, by noting that zt is a function of (α, β) given by:
zt(α, β) = u2
t −α0 −(α1I{ut−1⩾0} + α2I{ut−1<0} + β)u2
t−1
+ βzt−1(α, β)
(4.3)

4.2 Simulating the joint posterior
43
and by deﬁning the the T ×1 vector z .= (z1 · · · zT )′ as well as the T ×T diagonal
matrix:
Λ .= Λ(α, β) = diag
 {2h2
t(α, β)}T
t=1

we can express the approximate likelihood function of (α, β) as follows:
L(α, β | γ, y, X) ∝(det Λ)−1/2 exp

−1
2z′Λ−1z

.
As will be shown hereafter, the construction of the proposal densities for pa-
rameters α and β is based on this approximate likelihood function.
Generating vector α
We extend the recursive transformations presented in Sect. 3.2.1 to express
the function zt(α, β) in (4.3) as a linear function of the 3×1 vector α. Since the
conditional variance is an asymmetric function of past errors, we have to dis-
tinguish between positive and negative shocks in the recursive transformations.
Let us deﬁne vt
.= u2
t for notational convenience. The appropriate recursive
transformations are then deﬁned as follows:
l∗
t
.= 1 + β l∗
t−1
v∗
t
.= vt−1I{ut−1⩾0} + β v∗
t−1
v∗∗
t
.= vt−1I{ut−1<0} + β v∗∗
t−1
where l∗
0 = v∗
0 = v∗∗
0
.= 0. As shown in Prop. A.2 (see App. A), upon deﬁning
the 3×1 vector ct .= (l∗
t v∗
t v∗∗
t )′, the function zt can be expressed as zt = vt−c′
tα.
Then, by considering the T ×1 vector v .= (v1 · · · vT )′ as well as the T ×3 matrix
C whose tth row is c′
t, it turns out that z = v −Cα. Therefore, we can express
the approximate likelihood function of parameter α as follows:
L(α | γ, β, y, X) ∝(det Λ)−1/2 exp

−1
2(v −Cα)′Λ−1(v −Cα)

.
The proposal density to sample vector α is obtained by combining this likelihood
function and the prior density by Bayes’ update:
qα(α | γ, eα, β, y, X) ∝N3(α | bµα, bΣα)I{α>0}
with:
bΣ−1
α
.= C′eΛ−1C + Σ−1
α
bµα
.= bΣα(C′eΛ−1v + Σ−1
α µα)

44
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
where the T × T diagonal matrix eΛ .= diag
 {2h2
t(γ, eα, β)}T
t=1

and eα is the
previous draw of α in the M-H sampler. A candidate α⋆is sampled from this
proposal density and accepted with probability:
min
p(γ, α⋆, β | y, X)
p(γ, eα, β | y, X)
qα(eα | γ, α⋆, β, y, X)
qα(α⋆| γ, eα, β, y, X), 1

.
Generating parameter β
The methodology is the same as the one presented in Sect. 3.2.2. The function
zt(β) given by:
zt(β) = u2
t −α0 −(α1I{ut−1⩾0} + α2I{ut−1<0} + β)u2
t−1 + βzt−1(β)
is approximated by a ﬁrst order Taylor expansion at point eβ, the previous draw
of parameter β in the M-H sampler. The proposal density qβ(β | •) is then
obtained by combining the approximate likelihood of β and the prior density
via Bayes’ update. A candidate β⋆is generated from this density and accepted
with probability:
min
(
p(γ, α, β⋆| y, X)
p(γ, α, eβ | y, X)
qβ(eβ | γ, α, β⋆, y, X)
qβ(β⋆| γ, α, eβ, y, X)
, 1
)
.
Finally, we conclude this section by noting that the validity of the algo-
rithm and the correctness of the computer code are veriﬁed by the methodology
detailed at the end of Sect. 3.2.2.
4.3 Empirical analysis
We apply our Bayesian estimation method to daily observations of the Standard
& Poors 100 (henceforth S&P100) index log-returns. The sample period is from
January 2, 1990, to December 17, 1992, for a total of 750 observations. The
log-returns are expressed in percent. The S&P100 index is one of the major
benchmarks of U.S. equity performance. It consists of one hundred stocks in a
representative sample of leading companies chosen for market size, liquidity and
industry group. We choose this data set since it is an equity index and is therefore
susceptible to exhibit leverage eﬀects. Moreover, a volatility index of the S&P100
index, the VIX, is computed by the Chicago Board of Exchange. This index
aims to give a fear gauge to investors and can be viewed as a proxy for the
conditional variance. This volatility index gained particular attention in recent
years as it provides an interesting asset for hedging downside market movements.

4.3 Empirical analysis
45
The two data sets are freely available from http://www.finance.yahoo.com.
Note that in 2003, the formula for the VIX’s calculation has been modiﬁed and
the underlying index has changed from the S&P100 index to the S&P500 index.
This is of no consequence here since we consider the old VIX deﬁnition which
is based on the S&P100 index.
The S&P100 index log-returns are displayed in the upper part of Fig. 4.1. In
the lower part, the VIX index is plotted for the same time period. The correlation
between log-returns and gross rates of squared VIX, which can be viewed as a
proxy for the variance, is -0.52, indeed suggesting the presence of the leverage
eﬀect.

46
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
Daily log−returns
(in percent)
1
250
500
750
−2
−1
0
1
2
time
index
VIX level
(in percent)
1
250
500
750
10
20
30
40
time
index
Fig. 4.1. S&P100 index log-returns (upper graph) and VIX level (lower graph).
4.3.1 Model estimation
As noted by Campbell, Lo, and MacKinlay [1996, p.104] for instance, ﬁnancial
time series such as equity indices sometimes present positive ﬁrst order auto-
correlation. This eﬀect is stronger for high-frequency data such as intra-daily

4.3 Empirical analysis
47
or even daily time series due to market micro-structures. Based on that evi-
dence, we estimate the model (4.1) with a constant term and an autoregressive
parameter:
yt = γ0 + γ1yt−1 + ut
(4.4)
where the errors {ut} are modeled by the Normal-GJR(1, 1) process intro-
duced in (4.1). As a prior density for the regression parameters, we choose a
bi-dimensional Normal density. In the case of the GJR parameters, the priors
are truncated Normal densities. Both priors have zero mean vectors and diago-
nal covariance matrices whose variances are set to 10’000 so we do not introduce
tight prior information in our estimation (see Sect. 4.3.2 for a formal check).
Finally, we recall that the joint prior is constructed by assuming prior indepen-
dence between γ, α and β.
We run two chains for 10’000 passes each where only positivity constraints
for the GJR parameters are implemented in the M-H algorithm, through the
prior densities. We test the convergence of the chains using the diagnostic test
by Gelman and Rubin [1992]. The convergence diagnostic based on the two
chains shows no evidence against convergence of the sampler for the last 5’000
iterations (the values of the 97.5th percentile of the potential scale reduction
factor ranges from 1.01 to 1.09). The one-lag autocorrelations in the chains
range from 0.30 for parameter γ1 to 0.96 for β. The sampling algorithm allows
to reach acceptance rates of 66% for vector α, 77% for γ and 95% for β. From
the overall MCMC output, we discard the ﬁrst 5’000 draws and merge the two
chains to get a ﬁnal sample of length 10’000. In addition, we estimate the model
by the usual ML technique for comparison purposes.
The posterior statistics as well as the ML results are reported in Table 4.1.
First, we note a diﬀerence between the Bayesian and ML point estimates for
the GJR parameters; the ML estimates are lower for the components of vector
α and higher for parameter β. In the case of vector γ, the diﬀerence between
the Bayesian and the classical approaches is more pronounced for the compo-
nent γ1. For both components of γ, the posterior 95% conﬁdence bands are
centered around zero which suggests a zero expectation for yt and no need to
model an autoregressive component for this data set. Second, we note strong
asymmetries in the marginal posterior densities for the parameters α and β,
as shown in Fig. 4.2. The marginal posteriors for components of vector α are
right-skewed while left-skewed for parameter β. In the ML case, the asymptotic
Normal approximation of the parameter estimates leads to negative values for
the left boundary of α0 and α1. Caution is therefore in order when applying
the asymptotic Normal approximation in this context. Finally, the values of the

48
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
ineﬃciency factor (IF) reported in the last column of Table 4.1 indicate that
using 10’000 simulations is appropriate if we require that the Monte Carlo error
in estimating the mean is smaller than 0.33% of the variation of the error due
to the data. The larger ineﬃciency factor reported for parameter β is reﬂected
in a larger autocorrelation in the simulated values.
Table 4.1. Estimation results for the linear regression model with Normal-
GJR(1, 1) errors.⋆
ψ
ψMLE
ψ
ψ0.5
ψ0.025
ψ0.975
min
max
IF
γ0
0.001
0.002
0.002
-0.025
0.028
-0.048
0.047
1.92
[-0.025,0.028]
(0.190)
γ1
0.003
0.004
0.004
-0.075
0.084
-0.160
0.163
1.84
[-0.076,0.082]
(0.546)
α0
0.018
0.023
0.022
0.012
0.041
0.006
0.071
16.52
[-0.008,0.028]
(0.299)
α1
0.022
0.038
0.034
0.002
0.096
0.000
0.193
3.45
[-0.020,0.064]
(0.463)
α2
0.155
0.180
0.175
0.101
0.286
0.058
0.393
3.96
[0.077,0.234]
(0.939)
β
0.799
0.750
0.754
0.620
0.853
0.423
0.918
33.62
[0.713,0.885]
(3.45)
⋆ψMLE: Maximum Likelihood estimate; ψ: posterior mean; ψφ: estimated pos-
terior quantile at probability φ; min: minimum value; max: maximum value; IF:
ineﬃciency factor (i.e., ratio of the squared numerical standard error and the
variance of the sample mean from a hypothetical iid sampler); [•]: Maximum
Likelihood 95% conﬁdence interval; (•): numerical standard error (×103). The
posterior statistics are based on 10’000 draws from the joint posterior sample.
Given the expression of the scedastic function in (4.1), the leverage eﬀect can
be measured by △α .= (α2 −α1). The posterior density of △α is displayed in
Fig. 4.3. The mean value is 0.142 and the median is 0.139, indicating a stronger
impact of negative shocks to the conditional variance, as expected. Using the
posterior sample, we can also estimate the probability of the presence of the
leverage eﬀect, i.e., P(△α > 0 | y, X). The estimation gives a probability value
of 0.998 with a 95% conﬁdence band of [0.9976,0.9996]. Therefore, the data
strongly support the presence of the leverage eﬀect for the S&P100 index.

4.3 Empirical analysis
49
Parameter α0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0
500
1000
1500
2000
2500
Parameter α1
0.00
0.05
0.10
0.15
0.20
0
500
1000
1500
Fig. 4.2. Marginal posterior densities of the GJR(1, 1) parameters; upper graph: pa-
rameter α0; lower graph: parameter α1. The histograms are based on 10’000 draws
from the joint posterior sample.

50
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
Parameter α2
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0
500
1000
1500
Parameter β
0.4
0.5
0.6
0.7
0.8
0.9
0
200
400
600
800
1000
1200
1400
Fig. 4.2. (cont.) Marginal posterior densities of the GJR(1, 1) parameters; upper
graph: parameter α2; lower graph: parameter β. The histograms are based on 10’000
draws from the joint posterior sample.

4.3 Empirical analysis
51
−0.1
0.0
0.1
0.2
0.3
0.4
0
500
1000
1500
∆α
Fig. 4.3. Posterior density of the leverage eﬀect parameter △α .= (α2 −α1). The
vertical line is set at △α = 0. The histogram is based on 10’000 draws from the joint
posterior sample.

52
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
4.3.2 Sensitivity analysis
As in Sect. 3.3.2, we test the sensitivity of our posterior results with respect
to the choice of the prior density. We consider three alternative priors by either
modifying the mean and/or increasing the variance relative to our initial prior.
Formally, the alternative prior densities can be expressed as follows:
p(γ) ∝N2(γ | µ ι2, σ2I2)
p(α) ∝N3(α | µ ι3, σ2I3)I{α>0}
p(β) ∝N(β | µ, σ2)I{β>0}
where ιd is a d × 1 vector of ones, Id is a d × d identity matrix, µ is the prior
mean and σ2 the prior variance.
The sensitivity results are reported in Table 4.2; the ﬁrst two columns give
the hyperparameters’ values of the alternative priors while the last column report
the estimated Bayes factors. In all cases, the Bayes factors belong to the interval
[0.3125, 1] which implies a weak evidence in favor of our initial speciﬁcation
relative to the alternative priors. This indicates that our initial prior is vague
enough and does not introduce signiﬁcant information in our estimation.
Table 4.2. Results of the sensitivity
analysis.⋆
Alternative priors
µ
σ2
BF
1.00
10’000
0.999
0.00
11’000
0.751
1.00
11’000
0.751
⋆The alternative priors are (truncated)
Normal densities; µ prior mean; σ2 prior
variance; BF: Bayes factor.
4.3.3 Model diagnostics
We test the residuals for possible misspeciﬁcation. The standardized residuals
are deﬁned by:
bεt .= yt −x′
tbγ
bh1/2
t
for t = 1, . . . , 750, where bγ is the posterior median of vector γ and bht is the
conditional variance computed with the median of the posterior sample. If the

4.4 Illustrative applications
53
statistical assumptions in (4.1) are satisﬁed, these residuals should be indepen-
dent and Normally distributed asymptotically.
We test the residuals for autocorrelation using the Ljung-Box test up to lag
20 [see Ljung and Box 1978]. The test does not reject the null hypothesis of
the absence of autocorrelation at the 5% signiﬁcance level (p-value = 0.365).
This is also true for the squared residuals (p-value = 0.780). The Kolmogorov-
Smirnov Normality test does not reject the null hypothesis at the 5% signiﬁcance
level with a p-value of 0.0514. On the contrary, the Jarque-Bera Normality test
strongly rejects the null. Hence, while the model is able to ﬁlter the hetero-
scedasticity, it is not ﬂexible enough to account for the high kurtosis of the
residuals. This point will be addressed in Chap. 5 with the introduction of
Student-t errors in the modeling.
4.4 Illustrative applications
We end this chapter with the estimation of the unconditional variance of the
underlying process. Under model speciﬁcation (4.4), the process is covariance
stationary if the following conditions are satisﬁed:
CSC1 .= γ2
1 −1 < 0
CSC2 .= (α + β) −1 < 0
where we deﬁne α .=
α1+α2
2
for notational purposes. If both conditions are
satisﬁed, the unconditional variance hy exists and is given by:
hy .=
α0
CSC1 × CSC2
.
The joint posterior sample can be used to estimate the posterior density of these
functions by generating:
CSC[j]
1
.=
 γ[j]
1
2 −1
CSC[j]
2
.= (α[j] + β[j]) −1
and then:
h[j]
y
.=
α[j]
0
CSC[j]
1 × CSC[j]
2
for j = 1, . . . , 10’000. In our simulation study, none of the values CSC1 and CSC2
exceed zero, thus indicating that the process is covariance stationary and that

54
4 The Linear Regression Model with Normal-GJR(1, 1) Errors
the unconditional variance exists. The posterior density of the unconditional
variance is displayed in Fig. 4.4 together with the ML asymptotic Normal
approximation. The posterior mean and the posterior median are respectively
0.169 and 0.1653. The value of the unconditional variance computed from the
ML point estimates is slightly lower with a value of 0.1608. The 95% conﬁdence
band given by the Bayesian approach is [0.1373,0.2197]. In the classical ap-
proach, the conﬁdence band computed via the delta method is [0.0725,0.2491].
In this case, the asymptotic Normal approximation highly overestimates the
size of the conﬁdence band, especially the left part of the interval. As shown
in Fig. 4.4, the asymptotic approximation is ﬂat and symmetric whereas the
posterior density is more peaked and exhibits a positive skewness (the skewness
is 2.01 and signiﬁcantly diﬀerent from zero).
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0
5
10
15
20
hy
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0
5
10
15
20
Delta approximation
Posterior density
Fig. 4.4. Posterior density of the unconditional variance and asymptotic Normal ap-
proximation. The histogram is based on 10’000 draws from the joint posterior sample.

5
Bayesian Estimation of the Linear Regression
Model with Student-t-GJR(1, 1) Errors
“This
development
(i.e.,
the
Student-t
distribution)
permits
a
distinction
between
conditional
heteroskedasticity
and
a
conditional
leptokurtic
distribution,
either
of
which
could
account
for
the
observed unconditional kurtosis in the data.”
— Tim Bollerslev
In this chapter, we extend the linear regression model with conditionally hetero-
scedastic errors. The conditional variance is again described by the GJR process
introduced in Chap. 4. However, in the new speciﬁcation, the errors are no
longer Normally distributed but follow a Student-t distribution. Therefore, the
model incorporates the possibility of heavy-tailed disturbances. Indeed, while
the Normal distribution is used routinely, it has been widely recognized that
ﬁnancial markets exhibit signiﬁcant non-Normalities, in particular asset returns
exhibit heavy tails. A distribution with fat tails makes extreme outcomes such
as crashes relatively more likely than does a Normal distribution which assigns
virtually zero probability to events that are greater than three standard devi-
ations. Since one of the objectives of ﬁnancial risk management models is to
measure severe losses, i.e., events appearing in the tails of the distribution, this
is a serious shortcoming and the alternative of the Student-t distribution is a
parsimonious way to incorporate fat tails in the modeling. In the Bayesian ap-
proach, the heavy-tails eﬀect is created by the introduction of latent variables
in the variance process as proposed by Geweke [1993]; this approach allows the
Bayesian estimation of the degrees of freedom parameter in a convenient man-
ner. As a ﬁrst illustration, we ﬁt the model to the S&P100 index log-returns
and compare the Bayesian and the Maximum Likelihood estimations. Next, we
perform a prior analysis and test the residuals for misspeciﬁcation. Finally, we
estimate the conditional and unconditional kurtosis of the underlying time se-
ries.

56
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
The plan of this chapter is as follows. We set up the model in Sect. 5.1.
The MCMC scheme is detailed in Sect. 5.2. The empirical results are presented
in Sect. 5.3. We conclude with some illustrations of the Bayesian approach in
Sect. 5.4.
5.1 The model and the priors
A linear regression model with Student-t-GJR(1, 1) errors may be written as
follows:
yt = x′
tγ + ut
for t = 1, . . . , T
ut = εt(ϱht)1/2
εt
iid
∼S(0, 1, ν)
ϱ .= ν −2
ν
ht .= α0 + (α1I{ut−1⩾0} + α2I{ut−1<0})u2
t−1 + βht−1
(5.1)
where α0 > 0, α1 ⩾0, α2 ⩾0, β ⩾0, ν > 2 and h0 = y0 .= 0 for convenience;
yt is a scalar dependent variable; xt is a m × 1 vector of exogenous or lagged
dependent variables; γ is a m × 1 vector of regression coeﬃcients; S(0, 1, ν) is
the standard Student-t density with ν degrees of freedom, i.e., its variance is
ν
ν−2. From model speciﬁcation (5.1) we note that ϱ is a scaling factor which
normalizes the variance of the Student-t density so that ht is the variance of yt
given by the GJR scedastic function. The restriction on the degrees of freedom
parameter ensures the conditional variance to be ﬁnite and the restrictions on
the GJR parameters guarantee its positivity.
In order to write the likelihood function, we deﬁne the vectors y .= (y1 · · · yT )′
and α .= (α0 α1 α2)′ as well as the T × m matrix X of observations whose
tth row is x′
t. For notational purposes, we regroup the model parameters into
ψ .= (γ, α, β, ν). In addition, we deﬁne the T × T diagonal matrix:
Σ .= Σ(ψ) = diag
 {ϱht(γ, α, β)}T
t=1

where:
ht(γ, α, β) .= α0 + (α1I{ut−1(γ)⩾0} + α2I{ut−1(γ)<0})u2
t−1(γ)
+ βht−1(γ, α, β)
ut(γ) .= yt −x′
tγ .
(5.2)

5.1 The model and the priors
57
Then, we regroup the error terms ut(γ) into the T × 1 vector u .= (u1 · · · uT )′
and express the likelihood function of ψ as follows:
LS(ψ | y, X) ∝
"
Γ
  ν+1
2

Γ
  ν
2

ν1/2
#T
(det Σ)−1/2
T
Y
t=1

1 +
u2
t
νϱht
−(ν+1)
2
.
(5.3)
The notation LS emphasizes the fact that the likelihood function is constructed
from the Student-t density.
While the likelihood function (5.3) can be used in classical inference, it is
not convenient in the Bayesian framework. It is indeed diﬃcult to ﬁnd proposal
densities for the parameters, especially if we aim to sample the degrees of free-
dom ν as well. To overcome this problem, we express the innovations process
{ut} in another speciﬁcation as proposed by Geweke [1993]. In this new setting,
the variable ut is expressed as follows:
ut = εt(ϖtϱht)1/2
for t = 1, . . . , T
εt
iid
∼N(0, 1)
ϖt
iid
∼IG
ν
2, ν
2

.
(5.4)
Hence, the error term ut, conditional on ϖt, follows a Normal distribution with
a mean of zero and a variance of ϖtϱht; the scedastic function ϱht is multiplied
by a latent variable ϖt which is Inverted Gamma distributed. The degrees of
freedom parameter ν characterizes the density of ϖt as follows:
p(ϖt | ν) =
ν
2
 ν
2 h
Γ
ν
2
i−1
ϖ
−ν
2 −1
t
exp

−ν
2ϖt

.
(5.5)
Let us now regroup the latent variables into the T×1 vector ϖ .= (ϖ1 · · · ϖT )′
and deﬁne the augmented set of parameters Θ .= (ψ, ϖ). Upon deﬁning the T ×T
diagonal matrix:
Σ .= Σ(Θ) = diag
 {ϖtϱht(γ, α, β)}T
t=1

where ht is given in expression (5.2), we can express the likelihood function of
Θ as follows:
L(Θ | y, X) ∝(det Σ)−1/2 exp

−1
2u′Σ−1u

.
(5.6)
This speciﬁcation is equivalent to (5.3). However, in that case, the Bayesian
estimation can be handled for all parameters in a convenient manner.

58
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
We propose the following proper priors on the parameters γ, α, β of the
preceding model:
p(γ) = Nm(γ | µγ, Σγ)
p(α) ∝N3(α | µα, Σα)I{α>0}
p(β) ∝N(β | µβ, Σβ)I{β>0}
where we recall that µ• and Σ• are the hyperparameters, I{•} is the indicator
function, 0 is a 3×1 vector of zeros and Nd is the d-dimensional Normal density
(d > 1).
The prior density of vector ϖ conditional on ν is found by noting that the
components ϖt are independent and identically distributed from (5.5), which
yields:
p(ϖ | ν) =
ν
2
 T ν
2 h
Γ
ν
2
i−T
 T
Y
t=1
ϖt
!−ν
2 −1
exp
"
−1
2
T
X
t=1
ν
ϖt
#
.
We follow Deschamps [2006] in the choice of the prior density on the degrees
of freedom parameter. The density is a translated Exponential with parameters
λ > 0 and δ ⩾2:
p(ν) = λ exp

−λ(ν −δ)

I{ν>δ} .
For large values of λ, the mass of the prior is concentrated in the neighborhood
of δ and a constraint on the degrees of freedom can be imposed in this manner.
The Normality of the errors is obtained when δ becomes large. As pointed out
by Deschamps [2006], this prior density is useful for two reasons. First, it is
potentially important, for numerical reasons, to bound the degrees of freedom
parameter away from two to avoid explosion of the conditional variance. Second,
we can approximate the Normality of the errors while maintaining a reasonably
tight prior which can improve the convergence of the MCMC sampler.
Finally, we assume prior independence between γ, α, β and (ϖ, ν) which
yields the following joint prior:
p(Θ) = p(γ)p(α)p(β)p(ϖ | ν)p(ν)
and, by combining the likelihood function (5.6) and the joint prior, we construct
the posterior density via Bayes’ rule:
p(Θ | y, X) ∝L(Θ | y, X)p(Θ) .

5.2 Simulating the joint posterior
59
5.2 Simulating the joint posterior
Once again, we rely on the M-H algorithm to draw samples from the joint
posterior distribution. We draw an initial value:
Θ[0] .= (γ[0], α[0], β[0], ϖ[0], ν[0])
from the joint prior and we generate iteratively J passes for Θ. A single pass is
decomposed as follows:
γ[j] ∼p(γ | α[j−1], β[j−1], ϖ[j−1], ν[j−1], y, X)
α[j] ∼p(α | γ[j], β[j−1], ϖ[j−1], ν[j−1], y, X)
β[j] ∼p(β | γ[j], α[j], ϖ[j−1], ν[j−1], y, X)
ϖ[j] ∼p(ϖ | γ[j], α[j], β[j], ν[j−1], y, X)
ν[j] ∼p(ν | ϖ[j]) .
Only vector ϖ can be simulated from a known expression. Draws of parameters
γ, α and β are made using a method similar to the one presented in Sect. 4.2.
Sampling parameter ν is more technical and relies on an optimized rejection
technique.
5.2.1 Generating vector γ
The proposal density to sample the m × 1 vector γ is obtained by combining
the likelihood function (5.3) and the prior density by Bayes’ update:
qγ(γ | eγ, α, β, ϖ, ν, y, X) = Nm(γ | bµγ, bΣγ)
with:
bΣ−1
γ
.= X′eΣ−1X + Σ−1
γ
bµγ
.= bΣγ(X′eΣ−1y + Σ−1
γ µγ)
where the T × T diagonal matrix eΣ .= diag
 {ϖtϱht(eγ, α, β)}T
t=1

, eγ is the pre-
vious draw of γ in the M-H sampler and ϱ .= ν−2
ν . A candidate γ⋆is sampled
from this proposal density and accepted with probability:
min
p(γ⋆, α, β, ϖ, ν | y, X)
p(eγ, α, β, ϖ, ν | y, X)
qγ(eγ | γ⋆, α, β, ϖ, ν, y, X)
qγ(γ⋆| eγ, α, β, ϖ, ν, y, X), 1

.

60
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
5.2.2 Generating the GJR parameters
The methodology is similar to the one exposed in Sect. 4.2.2. Let us deﬁne:
wt .= u2
t
τt
−ht
where τt .= ϖtϱ for convenience. From there, we can transform the expression
of the conditional variance as follows:
ht = α0 + (α1I{ut−1⩾0} + α2I{ut−1<0})u2
t−1 + βht−1
⇔u2
t
τt
= α0 + (α1I{ut−1⩾0} + α2I{ut−1<0})u2
t−1
+ β u2
t−1
τt−1
−βwt−1 + wt
⇔vt = α0 + (α1I{ut−1⩾0} + α2I{ut−1<0})τt−1vt−1
+ βvt−1 −βwt−1 + wt
where we deﬁne vt .= u2
t
τt for notational purposes. Moreover, we note that the
variable wt can also be expressed as follows:
wt .= u2
t
τt
−ht =
 u2
t
τtht
−1

ht =
 χ2
1 −1

ht
where χ2
1 denotes a Chi-squared variable with one degree of freedom. The last
equality results from the expression for ut in (5.4). Therefore, the variable wt
has a conditional mean of zero and a conditional variance of 2h2
t. In addition,
we note that the sequence {wt} is again a Martingale Diﬀerence process.
Approximating the variable wt by a variable zt which is Normally distributed
with a mean of zero and a variance of 2h2
t yields the following auxiliary model
for vt:
vt = α0 + (α1I{ut−1⩾0} + α2I{ut−1<0})τt−1vt−1 + βvt−1
−βzt−1 + zt .
Then, by noting that zt is a function of (α, β) given by:
zt(α, β) = vt −α0 −

(α1I{ut−1⩾0} + α2I{ut−1<0})τt−1 + β

vt−1
+ βzt−1(α, β)
(5.7)
and by deﬁning the T × 1 vector z .= (z1 · · · zT )′ as well as the T × T diagonal
matrix:

5.2 Simulating the joint posterior
61
Λ .= Λ(α, β) = diag
 {2h2
t(α, β)}T
t=1

we can approximate the likelihood function of (α, β) as follows:
L(α, β | γ, ϖ, y, X) ∝(det Λ)−1/2 exp

−1
2z′Λ−1z

.
(5.8)
As will be shown hereafter, the construction of the proposal densities for pa-
rameters α and β is based on this approximate likelihood function.
Generating vector α
Our aim is to express the function zt(α, β) in (5.7) as a linear function of the
3×1 vector α. To that aim, let us deﬁne the following recursive transformations:
l∗
t
.= 1 + β l∗
t−1
v∗
t
.= u2
t−1I{ut−1⩾0} + β v∗
t−1
v∗∗
t
.= u2
t−1I{ut−1<0} + β v∗∗
t−1
where l∗
0 = v∗
0 = v∗∗
0
.= 0. As shown in Prop. A.3 (see App. A), upon deﬁning
the 3 × 1 vector ct .= (l∗
t v∗
t v∗∗
t )′, it turns out that the function zt can be
expressed as zt = vt −c′
tα. Hence, by deﬁning the T × 1 vectors z .= (z1 · · · zT )′
and v .= (v1 · · · vT )′ as well as the T × 3 matrix C whose tth row is c′
t, we get
z = v −Cα. Therefore, we can express the approximate likelihood function of
parameter α as follows:
L(α | γ, β, ϖ, ν, y, X) ∝(det Λ)−1/2 exp

−1
2(v −Cα)′Λ−1(v −Cα)

.
The proposal density to sample vector α is obtained by combining this likelihood
function and the prior density by Bayes’ update:
qα(α | eγ, α, β, ϖ, ν, y, X) ∝N3(α | bµα, bΣα)I{α>0}
with:
bΣ−1
α
.= C′eΛ−1C + Σ−1
α
bµα
.= bΣα(C′eΛ−1v + Σ−1
α µα)
where the T × T diagonal matrix eΛ .= diag
 {2h2
t(γ, eα, β)}T
t=1

and eα is the
previous draw of α in the M-H sampler. A candidate α⋆is sampled from this
proposal density and accepted with probability:

62
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
min
p(α⋆, γ, β, ϖ, ν | y, X)
p(eα, γ, β, ϖ, ν | y, X)
qα(eα | γ, α⋆, β, ϖ, ν, y, X)
qα(α⋆| γ, eα, β, ϖ, ν, y, X), 1

.
Generating parameter β
Contrary to the parameter α, we cannot express the function zt(α, β) in (5.7)
as a linear function of β. To bypass this problem, we approximate the function
zt(β) by a ﬁrst order Taylor expansion at point eβ:
zt(β) ≃zt(eβ) + dzt
dβ

β=eβ
× (β −eβ)
where eβ is the previous draw of β in the M-H sampler. From there, we deﬁne
the following:
rt .= zt(eβ) + eβ∇t
,
∇t .= −dzt
dβ

β=eβ
where the terms ∇t can be computed by the following recursion:
∇t .= v2
t−1 −zt−1(eβ) + eβ∇t−1
with ∇0 .= 0. This recursion is simply obtained by diﬀerentiating (5.7) with
respect to β. Then, we regroup these terms into the T ×1 vectors r .= (r1 · · · rT )′
and ∇.= (∇1 · · · ∇T )′ and we approximate the term within the exponential
in (5.8) by z ≃r−β∇. This yields the following approximate likelihood function
for parameter β:
L(β | γ, α, ϖ, ν, y, X) ∝(det Λ)−1/2 exp

−1
2(r −β∇)′Λ−1(r −β∇)

.
This likelihood function is combined with the prior density by Bayes’ update to
construct the proposal qβ(β | •). A candidate β⋆is sampled from this proposal
density and accepted with probability:
min
(
p(γ, α, β⋆, ϖ, ν | y, X)
p(γ, α, eβ, ϖ, ν | y, X)
qβ(eβ | γ, α, β⋆, ϖ, ν, y, X)
qβ(β⋆| γ, α, eβ, ϖ, ν, y, X)
, 1
)
.
5.2.3 Generating vector ϖ
The components of ϖ are independent a posteriori and the full conditional
posterior of ϖt is obtained as follows:

5.2 Simulating the joint posterior
63
p(ϖt | γ, α, β, ν, y, X) ∝L(Θ | y, X)p(ϖt | ν)
∝ϖ
−(ν+3)
2
t
exp

−bt
ϖt

(5.9)
with:
bt .= 1
2
(yt −x′
tγ)2
ϱht
+ ν

where we recall that ht .= ht(γ, α, β) and ϱ .= ν−2
ν . Expression (5.9) is the kernel
of an Inverted Gamma density with parameters ν+1
2
and bt.
5.2.4 Generating parameter ν
Draws from p(ν | ϖ) are made by optimized rejection sampling from a translated
Exponential source density. The target density is:
p(ν | ϖ) ∝p(ϖ | ν)p(ν)
∝
ν
2
 T ν
2 h
Γ
ν
2
i−T
exp [−ϕν] I{ν>δ}
with:
ϕ .= 1
2
T
X
t=1
 ln ϖt + ϖ−1
t

+ λ .
Following Deschamps [2006], we sample a candidate ν⋆from a translated Expo-
nential source density:
g(ν; ¯µ, δ) .= ¯µ exp

−¯µ(ν −δ)

I{ν>δ}
where ¯µ maximizes the acceptance probability. The choice of ¯µ is found by
solving:
T
2

ln
1 + µδ
2µ

+ 1 −Ψ
1 + µδ
2µ

+ µ −ϕ = 0
for µ, where Ψ(z) .=
d ln Γ(z)
dz
is the Digamma function. The candidate ν⋆is
accepted with probability:
p⋆.=
k(ν⋆)
s(¯µ, δ)g(ν⋆; ¯µ, δ)
(5.10)
where k(ν) is the kernel of the target density:
k(ν) .=
ν
2
 T ν
2 h
Γ
ν
2
i−T
exp [−ϕν]

64
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
and s(µ, δ) is given by:
s(µ, δ) .= k
1 + µδ
µ
 
g
1 + µδ
µ
; µ, δ
−1
=
1 + µδ
2µ
 T (1+µδ)
2µ

Γ
1 + µδ
2µ
−T
µ−1 exp

1 −ϕ(1 + µδ)
µ

.
Substituting for k(ν⋆), s(¯µ, δ) and g(ν⋆; ¯µ, δ) in expression (5.10) yields:
p⋆=


Γ

1+¯µδ
2¯µ

Γ
  ν⋆
2



T ν⋆
2
 T ν⋆
2
1 + ¯µδ
2¯µ
 −T (1+¯
µδ)
2¯
µ
× exp

(ν⋆−δ)(¯µ −ϕ) + ϕ
¯µ −1

.
To end this section, we note that a slight modiﬁcation of Geweke [1993]
allows to generate draws from a Student-t distribution with conditional variance
ht without requiring the introduction of a scaling parameter ϱ .= ν−2
ν . This is
done by replacing the speciﬁcation for the latent variable ϖt in (5.4) by:
ϖt
iid
∼IG
ν
2, ν −2
2

.
The use of this new speciﬁcation requires some modiﬁcations of the eﬃcient
rejection scheme. We refer the reader to App. B for further details.
Finally, we note that the validity of the algorithm and the correctness of
the computer code are veriﬁed by the methodology detailed at the end of
Sect. 3.2.2.
5.3 Empirical analysis
To illustrate our Bayesian estimation method, we ﬁt the Student-t-GJR(1, 1)
model to the data set used in the empirical analysis of Chap. 4. Based on
previous results, we do not include the regression part in the current estimation.
5.3.1 Model estimation
As prior densities for the GJR parameters, we choose truncated Normal densities
with zero mean vectors and diagonal covariance matrices whose variances are
set to 10’000. For the prior on the degrees of freedom parameter, we set the
hyperparameters to λ = 0.01 and δ = 2; the prior mean is therefore 102 and the

5.3 Empirical analysis
65
prior variance 10’000. Note that the value of the hyperparameter δ is determined
so that the conditional variance exists. Moreover, we recall that the joint prior
is constructed by assuming prior independence between α, β and (ϖ, ν).
We run two chains for 10’000 passes each and control the convergence of the
sampler using the diagnostic test by Gelman and Rubin [1992]. The convergence
diagnostic shows no evidence against convergence for the last 5’000 iterations
(the value of the 97.5th percentile of the potential scale reduction factor ranges
from 1.001 to 1.1). The one-lag autocorrelations in the chains range from 0.59
for parameter α1 to 0.97 for parameter ν. The acceptance rate is 73% for vector
α and 95% for parameter β. The optimized rejection technique allows to draw
a new value of ν at each pass in the M-H algorithm. From the overall MCMC
output, we discard the ﬁrst 5’000 draws and merge the two chains to get a ﬁnal
sample of length 10’000.
The posterior statistics as well as the ML results are reported in Table 5.1.
First, we note that results for the GJR parameters are close to the results
of Table 4.1 (see p.48). The posterior means of the parameters are slightly
higher in the Student-t case (except for parameter α0) as well as the numerical
standard errors. Second, the marginal posterior densities (not shown) are still
clearly skewed and the 95% conﬁdence band of the parameters obtained through
the asymptotic Normal approximation leads to a negative left boundary for
component α1. The ML point estimate for the degrees of freedom parameter
is 9.9 while the posterior mean is 7.15 and the posterior median is 7.11. This
low value indicates a departure from Normality for the errors. In addition, the
95% conﬁdence band given by the ML approach is much wider than the one
estimated via the Bayesian approach. The left boundary is 0.14 which rejects the
existence of the conditional variance. In the case of the Bayesian estimation, the
minimum value for the degrees of freedom is 3.84, which supports the existence
of the conditional variance. The values of the ineﬃciency factor (IF) range from
3.65 for parameter α1 to 111.98 for parameter ν, indicating that in the worst
case, the numerical errors represent about 1.12% of the variation of the errors
due to the data.
In Fig. 5.1, we present a comparison between the classical and the Bayesian
approaches. The upper graphs show a scatter plot of the draws from the asymp-
totic Normal approximation of the model parameters; the Normal density is
centered at the ML estimates ψMLE and its covariance matrix is estimated as
the inverse of the Hessian matrix evaluated at ψMLE. The lower graphs present a
scatter plot of draws from the joint posterior sample. In both cases, the number
of draws is 10’000. The ﬁrst part of the ﬁgure depicts the draws for (α0, β). By
comparing the ML and Bayesian outputs, we can notice a clear diﬀerence in the

66
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
Table 5.1. Estimation results for the Student-t-GJR(1, 1) model.⋆
ψ
ψMLE
ψ
ψ0.5
ψ0.025
ψ0.975
min
max
IF
α0
0.012
0.018
0.017
0.008
0.036
0.003
0.060
18.15
[0.002,0.021]
(0.314)
α1
0.026
0.041
0.037
0.008
0.107
0.000
0.194
3.65
[-0.015,0.067]
(0.516)
α2
0.153
0.203
0.194
0.105
0.350
0.055
0.533
6.93
[0.063,0.242]
(1.641)
β
0.834
0.776
0.785
0.634
0.870
0.408
0.908
54.98
[0.740,0.929]
(4.636)
ν
9.90
7.549
7.11
4.54
13.60
3.84
18.85
111.98
[0.14,19.65]
(236.50)
⋆ψMLE: Maximum Likelihood estimate; ψ: posterior mean; ψφ: estimated posterior
quantile at probability φ; min: minimum value; max: maximum value; IF: ineﬃ-
ciency factor (i.e., ratio of the squared numerical standard error and the variance of
the sample mean from a hypothetical iid sampler); [•]: Maximum Likelihood 95%
conﬁdence interval; (•): numerical standard error (×103). The posterior statistics
are based on 10’000 draws from the joint posterior sample.
tails of the joint density. Indeed, the Bayesian posterior exhibits larger values
for parameter α0 together with lower values for parameter β. In addition, when
drawing a vertical line at α0 = 0, we note that some draws are negative with
the asymptotic Normal approximation. In the posterior sample, the draws are
positive as this is required by the prior density. In the second part of Fig. 5.1,
the two graphs show the draws for (α2, β). For these parameters, the posterior
sample exhibits a clear departure from the ellipsoid shape obtained with the
Normal approximation.
In Fig. 5.2, we display the prior and the posterior densities of the degrees
of freedom parameter. While the prior density is almost ﬂat (we recall that
the hyperparameters are set to λ = 0.01 and δ = 2 so that the prior mean is
102 and the prior variance 10’000), the shape of the posterior density is peaked
and concentrated around its mean value. In addition, the density is signiﬁcantly
right-skewed.

5.3 Empirical analysis
67
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Parameter α0
Parameter β
Normal approximation
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Parameter α0
Parameter β
Bayesian approach
Fig. 5.1. Comparison between the ML (upper graph) and the Bayesian (lower graph)
approaches. For both graphs, the number of draws is 10’000.

68
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
0.0
0.1
0.2
0.3
0.4
0.5
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Parameter α2
Parameter β
Normal approximation
0.0
0.1
0.2
0.3
0.4
0.5
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Parameter α2
Parameter β
Bayesian approach
Fig. 5.1. (cont.) Comparison between the ML (upper graph) and the Bayesian (lower
graph) approaches. For both graphs, the number of draws is 10’000.

5.3 Empirical analysis
69
5
10
15
20
25
30
35
0.00
0.05
0.10
0.15
0.20
Parameter ν
5
10
15
20
25
30
35
0.00
0.05
0.10
0.15
0.20
Prior density
Posterior density
Fig. 5.2. Prior and posterior densities of the degrees of freedom parameter. The
vertical line is centered at ν = 4, the value required for the conditional kurtosis of the
errors to exist. The histogram is based on 10’000 draws from the posterior sample.

70
5 The Linear Regression Model with Student-t-GJR(1, 1) Errors
5.3.2 Sensitivity analysis
As in previous chapters, we test the robustness of our results with respect to
the choice of the prior density. To that aim, we consider the same alternative
priors of Sect. 4.3.2 for parameters α and β:
p(α) ∝N3(α | µ ι3, σ2I3)I{α>0}
p(β) ∝N(β | µ, σ2)I{β>0}
where we recall that ι3 is a 3×1 vector of ones, I3 is a 3×3 identity matrix, µ is
the prior mean and σ2 the prior variance. For the alternative prior on the degrees
of freedom parameter, we consider a translated Exponential with λ = 0.008 and
δ = 2, which implies a prior mean of 127 and a prior variance of 15’625.
The results of Table 5.2 indicate that the prior on the degrees of freedom
has the largest impact on Bayes factors. Moreover, in all cases we conclude to a
weak evidence in favor of the initial speciﬁcation relative to the alternative priors
since the Bayes factors belong to the interval [0.3125, 1]. This indicates that our
initial prior is vague enough and does not introduce signiﬁcant information in
our estimation.
Table 5.2. Results of the sensitivity analysis.⋆
Alternative priors
µ
σ2
λ
BF
1.00
10’000
0.01
1.000
0.00
11’000
0.01
0.826
0.00
10’000
0.008
0.809
1.00
11’000
0.008
0.668
⋆The alternative priors on the parameters α and β
are truncated Normal densities; µ prior mean; σ2 prior
variance; The alternative prior on the parameter ν is
a translated Exponential with hyperparameters λ and
δ = 2; BF: Bayes factor.
5.3.3 Model diagnostics
We test the standardized residuals for possible model misspeciﬁcation. The
Ljung-Box test does not reject the absence of autocorrelation in the residuals
at the 5% signiﬁcance level (p-value = 0.4727). This is also true for the squared
residuals (p-value = 0.8724). The Kolmogorov-Smirnov Normality test slightly

5.4 Illustrative applications
71
rejects the Normality assumption at the 5% signiﬁcance level with a p-value
of 0.0437. However, when comparing the standardized residuals to a Student-t
distribution whose degrees of freedom parameter is set to the posterior median
bν = 7.11, the Kolmogorov-Smirnov empirical distribution test does not reject
the null hypothesis at the 5% signiﬁcance level (p-value = 0.4296). Hence, the
model accounts for the conditional heteroscedasticity and for the high kurtosis
in the residuals.
5.4 Illustrative applications
We end this chapter by illustrating some probabilistic statements made on the
conditional and unconditional kurtosis of the underlying process. Under speci-
ﬁcation (5.1), the conditional kurtosis κε is deﬁned as follows:
κε .= 3(ν −2)
ν −4
provided that ν > 4. Using the joint posterior sample, we estimate the posterior
probability of the existence for the conditional kurtosis, P(ν > 4 | y, X), to 0.999.
Therefore, the existence is clearly supported by the data. The posterior mean
of the kurtosis is 6.82 and the 95% conﬁdence interval is [3.72,13.8], indicating
heavier tails than for the Normal distribution.
Finally, we extend the analysis to the unconditional kurtosis of the process.
Let us deﬁne α .= α1+α2
2
for notational convenience. As demonstrated by He
and Ter¨asvirta [1999], the expression of the unconditional kurtosis κy is given
by:
κy .= κε(1 + α + β)(1 −α −β)
1 −κε
(α2
1+α2
2)
2
−2β(α + β)
provided that κε is ﬁnite and:
κε(α2
1 + α2
2)
2
+ 2β(α + β) < 1 .
The posterior probability of the latter condition is 0.007, meaning that there is
a 0.7% chance that the unconditional kurtosis exists.

6
Value at Risk and Decision Theory
“Density forecasting is fast becoming an important tool
for decision makers in situations where loss functions
are asymmetric and forecast errors follow non-Gaussian
distributions.”
— Allan Timmermann
6.1 Introduction
Since the Group of Thirty report in 1996, the Value at Risk (henceforth VaR)
has become the corner-stone in any risk management framework and is essential
in allocating capital as a cushion for market risk exposures. This measure gives,
for a given time horizon and a given conﬁdence level φ, the portfolio’s loss that
is expected to be exceeded with probability φc .= (1 −φ). The VaR is in many
aspects an attractive measure of risk, being relatively easy to implement and
easy to explain to non-expert audiences. While primarily designed for market
risk exposures, the VaR methodology now underpins the credit and operational
risk recommendations. From the internal models approach endorsed by the Basel
Committee on Banking and Supervision of Banks for Internal Settlement [see
Basel Committee on Banking Supervision 1995] and later adopted by US bank
regulators, banks are allowed to use their own models to estimate the VaR and
keep aside regulatory capital.
From a statistical viewpoint, the VaR is nothing else than a given percentile
of the proﬁt and loss (henceforth P&L) distribution over a ﬁxed horizon. To
be acceptable by regulators, the conﬁdence level must be 99% and the holding
period must be two weeks (i.e., ten trading days). This is motivated by the fear
of a liquidity crisis where a ﬁnancial institution might not be able to liquidate
its holdings for ten days straights. However, market participants consider the
99% conﬁdence level and the two weeks horizon to be too conservative. As an
additional tool for internal risk controlling, both the holding period and the
conﬁdence level can be selected to ﬁt the needs of analysts; in practice, it is
common to limit the conﬁdence level to 95% and the holding period to one day.

74
6 Value at Risk and Decision Theory
Evidently VaR can only be constructed by statistical methods. But in most
applications, the true P&L distribution is not known and VaR can only be
estimated from sample data. The underlying assumption of all the VaR estima-
tion methods is that the risk associated with a particular portfolio for a ﬁxed
time horizon is encapsulated within the P&L distribution. If this distribution is
known, the VaR can be obtained directly by reading the appropriate percentile
value from this distribution. If the P&L is unknown, it must be estimated. As
noted by McNeil and Frey [2000, p.272]:
(...) “the existing approaches for estimating the P&L distribution can
be divided into three groups: the non-parametric historical simulation
method; fully parametric methods based on an econometric model for
volatility dynamics and the assumption of the conditional distribution,
e.g., GARCH models; and ﬁnally methods based on extreme value the-
ory.”
We focus on the second approach in the current application.
Within the fully parametric literature, many papers either attempt to fore-
cast the VaR at diﬀerent time horizons or use the VaR to assess the forecasting
performance of a particular model. In both cases, the methodology is the same.
First, a statistical model which describes the P&L dynamics is determined. The
model parameters are estimated by the Maximum Likelihood technique for a
given estimation window. Then, based on these estimations, a VaR point fore-
cast is determined for a given horizon. The procedure is repeated again over a
testing window by rolling the estimation window; in this manner, we obtain a
time series of VaR forecasts. Then, the model is backtested, i.e., the predictions
are compared with the realized P&Ls. Often, a statistical test is used to assess
the performance of the model, i.e., to determine whether the model captures
the true VaR [see, e.g., Christoﬀersen 1998, Kupiec 1995].
While this methodology is accepted by academics and is widely implemented
in practice, we note that few empirical studies account for the uncertainty in
the VaR predictions. Nevertheless, this issue is important in a risk management
framework where some measure of the forecasts’ accuracy is also needed; assess-
ing the uncertainty of the VaR will allow the portfolio managers to make more
informed decisions when dictating a portfolio re-balance, for instance. We may
distinguish two sources of uncertainty which can inﬂuence the VaR accuracy:
• The parameter uncertainty within the context of a given model;
• The model uncertainty; via a probability function deﬁned on a class
of M possibly non-nested models Mi (i = 1, . . . , M).

6.1 Introduction
75
The former source of uncertainty, also referred to as estimation risk, is straight-
forwardly handled in Bayesian inference since the complete characterization of
the parameter uncertainty is contained in the joint posterior. The latter, known
as model risk, is a natural concept in the Bayesian framework. However, in
practice, its estimation involves many diﬃculties. In eﬀect, the methodology re-
quires the estimation of the model likelihood p(y | Mi) which can be diﬃcult
to estimate. Several estimation methods have been proposed but their cost is
far from negligible. In addition, the method may not work properly and can be
sensitive to the choice of the prior density. For these reasons, we concentrate
our attention on the estimation risk where the parameter uncertainty is used to
determine the VaR density instead of a single VaR point estimate.
Some approaches have been proposed to quantify the VaR uncertainty when
the P&L dynamics is described by GARCH models. Basically, these techniques
rely on the bootstrap methodology as in Christoﬀersen and Gon¸calves [2004] or
on some asymptotic justiﬁcations as in Bams, Lehnert, and Wolﬀ[2005]. The
former approach is computationally very demanding since at each step in the
procedure, a GARCH model is ﬁtted to the bootstrapped data. While technically
more convenient, the latter approach relies on an asymptotic approximation of
the distribution of the parameter estimates. The Bayesian approach gives a nat-
ural answer to these problems, as noted by Miazhynskaia and Aussenegg [2006].
As will be shown hereafter, the s-day ahead VaR (s ⩾1) can be expressed as a
function of the model parameters; hence, for each parameter in the joint poste-
rior sample, we can obtain a VaR point forecast. By repeating the estimation for
each draw in the posterior sample, we obtain an estimation for the VaR density
itself. When the forecast horizon is one day, the Bayesian approach gives the ex-
act VaR density. For forecasting horizons larger than one day, an approximation
based on the ﬁrst four moments of the future P&L density can be obtained.
In the Bayesian framework, we can either integrate out the parameter un-
certainty or choose a Bayes point estimate within the VaR density. The former
case is achieved by simulating from the predictive density and estimating the
VaR from empirical percentiles. The latter case yields an interesting problem
of decision theory: the choice of a Bayes point estimate which is optimal given
a particular loss function. In decision theory, the common practice is to use a
symmetric squared error loss function. While this loss function is appropriate in
many statistical applications, it may however not be ﬂexible enough for ﬁnan-
cial purposes, where over- and underestimation may have diﬀerent consequences.
Hence a ﬂexible asymmetric loss function is required.
The contributions of this chapter to the existing literature are as follows.
First, we provide a manner to approximate the multi-day ahead VaR density

76
6 Value at Risk and Decision Theory
when the underlying process is described by a GARCH model. Since this class
of models is a workhorse in ﬁnancial risk management, we therefore give the
possibility to determine the VaR term structure and to characterize the uncer-
tainty coming from the parameters. Second, we give a rational justiﬁcation to
the choice of a point estimate within the VaR density based on the decision
theory framework. We document how agents facing diﬀerent risk perspectives
can select their optimal VaR point estimate and show that the diﬀerences across
agents (e.g., fund and risk managers) can be substantial in terms of regulatory
capital. Lastly, we extend our methodology to the Expected Shortfall alternative
risk measure, and show that our simulation procedure can also be applied in a
straightforward manner.
The plan of this chapter is as follows. In Sect. 6.2, we formally deﬁne the
concept of VaR and derive the s-day ahead VaR expression under GARCH
dynamics. In Sect. 6.3, we review some fundamentals of Bayesian decision
theory and introduce the asymmetric Linex loss function. In Sect. 6.4, we
propose an empirical application with the estimation of the VaR term structure.
Finally, we extend the methodology to the Expected Shortfall risk measure in
Sect. 6.5.
6.2 The concept of Value at Risk
In this section, we formally deﬁne the concept of VaR and determine the density
of the one-day ahead VaR under the GARCH(1, 1) dynamics with both Normal
and Student-t disturbances. The density of the VaR for time horizons larger
than one day is obtained by explicitly estimating the ﬁrst four moments of the
conditional P&L density and approximating the percentile of interest by either
using the Cornish-Fisher expansion [see Cornish and Fisher 1937] or a Student-t
approximation. We consider the GARCH(1, 1) model for ease of exposition but
the methodology can be extended, upon modiﬁcations, to higher order GARCH
models as well as asymmetric speciﬁcations.
Deﬁnition 6.1 (Value at Risk). Let Y be a univariate random variable (not
necessarily continuous) with a distribution function FY . For a given risk level
φ, which belongs for risk management purposes to the interval [0.90, 0.995], the
VaR of Y is deﬁned by:
VaRφ .= inf

y ∈R | FY (y) ⩾φc	
where φc .= (1 −φ) for notational purposes.

6.2 The concept of Value at Risk
77
Hence, the VaR is nothing else than a percentile of the distribution of Y . When
the variable Y follows a standard Normal distribution, the VaR with conﬁdence
level φ is the φcth percentile denoted by zφc; e.g., z0.95 = −1.64. When the
variable Y follows a standard Student-t distribution with ν degrees of freedom,
the φcth percentile is denoted by tφc(ν); e.g., t0.95(5) = −2.01. We emphasize
the notation in the Student-t case where the VaR depends on the parameter ν.
6.2.1 The one-day ahead VaR under the GARCH(1, 1) dynamics
Under a GARCH(1, 1) model with Normal disturbances, the one-day ahead VaR
at risk level φ, estimated at time t, is given by:
VaRφ
t (ψ) = h1/2
t+1(α, β) × zφc
where ψ .= (α, β) and ht+1 is the conditional variance which is computed by
recursion given Ft, the information set at time t. Hence, under Normal distur-
bances, the one-day ahead VaR is nothing else than a given percentile of the
standard Normal distribution scaled by the conditional standard deviation.
In the case of Student-t disturbances, the one-day ahead VaR at risk level
φ, estimated at time t, is given by:
VaRφ
t (ψ) =

ϱ(ν) × ht+1(α, β)
1/2 × tφc(ν)
where in this case ψ .= (α, β, ν). In addition to the scale factor ϱ(ν) .= ν−2
ν ,
the φcth percentile of the standard Student-t distribution depends on the model
parameter ν. For both Normal and Student-t cases, the joint posterior sample
can be used to simulate the density of the one-day ahead VaR at any conﬁdence
level φ.
6.2.2 The s-day ahead VaR under the GARCH(1, 1) dynamics
If the horizon is larger than one day, predictions for the cumulative returns
are needed, which in turn requires multi-step predictions. The cumulative re-
turns over an s-day horizon (starting at time t) henceforth denoted as yt,s, are
straightforwardly calculated from the single period log-returns yt+i (i = 1, . . . , s)
as:
yt,s .= yt+1 + yt+2 + . . . + yt+s .
This follows from the deﬁnition of the one-day log-return, calculated as the
logarithmic diﬀerence of asset prices.
As in the one-day ahead case, our aim is to express the VaR of the variable
yt,s as a function of the model parameters ψ. However, it is well known that

78
6 Value at Risk and Decision Theory
under GARCH dynamics, no expression in closed form exists for the density of
yt+i when i > 1; hence no closed form expression is available for the density of
yt,s either. To overcome this problem, we might use Monte Carlo simulations
to generate the density of interest. That is, for a given set of parameters ψ and
information set Ft, we could simulate B paths for the P&L over s days. The
VaR would then be approximated by the empirical percentile of the distribution.
In order to obtain a density for the VaR itself, this evaluation would have to be
handled for each ψ in the joint posterior sample. However, since the quantity
of interest describes the tail of the distribution, a large amount of simulations
B would be required to get an accurate VaR estimate, which might lead to an
extremely costly simulation scheme.
Therefore, in order to simplify and accelerate the estimation procedure, we
propose an approximation of the VaR based on the ﬁrst four conditional mo-
ments of the variable yt,s which can be calculated analytically when ψ is known.
To that aim, let us deﬁne the pth conditional moment of yt,s as follows:
κp(ψ) .= Et,ψ
 yp
t,s

where Et,ψ(•) .= E(• | ψ, Ft) is the conditional expectation given ψ and Ft.
The notation κp(ψ) emphasizes the fact that the pth conditional moment is a
function of ψ and the time index is suppressed to simplify the notation. The
explicit calculation of the ﬁrst four moments is possible using the multinomial
formula which gives the pth power of the cumulative return as follows:
yp
t,s =
 s
X
i=1
yt+i
!p
=
X
i1, ... ,is
i1+ ... +is=p
p !
i1! · · · is! × yi1
t+1 · · · yis
t+s .
Calculations given in Props. C.1 and C.3 (see App. C) show that under the
GARCH(1, 1) speciﬁcation, the ﬁrst and third conditional moments are zero
which implies that the conditional density of yt,s is symmetric around zero. Cal-
culations for the second conditional moment of yt,s in Prop. C.2 (see App. C)
yield:
κ2(ψ) =
s
X
i=1
Et,ψ(ht+i)
where:

6.2 The concept of Value at Risk
79
Et,ψ(ht+i) = α0 + ρ1 Et,ψ(ht+i−1)
(6.1)
and ρ1
.= (α1 + β). Expression (6.1) can be evaluated recursively from
Et,ψ(ht+1) = ht+1(ψ) since this value is known given ψ and Ft. For the fourth
conditional moment, the calculations in Prop. C.4 (see App. C) yield the
following expression:
κ4(ψ) = κε
s
X
i=1
Et,ψ(h2
t+i) + 6
s−1
X
i=1
s
X
j=i+1
Et,ψ(y2
t+i y2
t+j)
(6.2)
where:
Et,ψ(h2
t+i) = α2
0 + τ1 Et,ψ(ht+i−1) + τ2 Et,ψ(h2
t+i−1)
(6.3)
and:
Et,ψ(y2
t+i y2
t+j) = α0
 
1 −ρj−i
1
1 −ρ1
!
Et,ψ(ht+i) + ρj−i−1
1
ρ2 Et,ψ(h2
t+i) .
(6.4)
In expression (6.2), the parameter κε denotes the fourth moment of the distur-
bances in the GARCH(1, 1) process; in the case of Normal disturbances, κε = 3,
while for (scaled) Student-t disturbances, κε = 3(ν−2)
ν−4 . The parameters τ1, τ2
and ρ2 are functions of the set of parameters ψ, respectively given by:
τ1 .= 2α0(α1 + β)
τ2 .= κεα2
1 + β(2α1 + β)
and:
ρ2 .= κεα1 + β .
The conditional expectations of expressions (6.3) and (6.4) can be evaluated
recursively from Et,ψ(ht+1) = ht+1(ψ) and Et,ψ(h2
t+1) = h2
t+1(ψ) since these
values are known given ψ and Ft.
As previously stated, the conditional moments κi (i = 1, . . . , 4) are used to
estimate the percentile of the conditional density of the cumulative return over
an s-day horizon. We propose two approaches to determine this percentile. The
ﬁrst method is the well-known Cornish-Fisher expansion by [see Cornish and
Fisher 1937] which consists in a transformation of the percentile of the standard
Normal density to account for non-zero skewness (i.e., asymmetry) and excess

80
6 Value at Risk and Decision Theory
kurtosis (i.e., fat tails). In our context, κ1 and κ3 are zero so that the Cornish-
Fisher formula simpliﬁes. We thus obtain the following approximation for the
s-day ahead VaR (s > 1) at risk level φ, estimated at time t:
VaRφ
t,s(ψ) ≈κ1/2
2
(ψ) ×

zφc + 1
24(z3
φc −3zφc)
κ4(ψ)
κ2
2(ψ) −3

(6.5)
where we recall that φc .= (1 −φ) and zφc is the φcth percentile of the standard
Normal distribution. From expression (6.5), we can notice the impact of the
excess kurtosis
  κ4
κ2
2 −3

on the VaR. Since conditional moments are functions
ψ, so is the VaR.
The Cornish-Fisher expansion is widely used in practice due to its simplicity,
and its accuracy is suﬃcient in many situations, especially when the distribution
of interest is close to the Normal. In this case, the Cornish-Fisher expansion pro-
vides a small correction for the non-zero skewness and excess kurtosis. However,
as pointed out by Jaschke [2002], the Cornish-Fisher expansion may suﬀer from
important deﬁciencies in pathological situations, for instance, when the kurtosis
of the distribution we aim to approximate is high. We note in particular that:
• The approximation may yield a distribution which is not necessar-
ily monotone. Hence, we may be faced with situations where the
risk capital allocated for a 1% chance event would be lower than
the capital allocated for a 5% chance event!
• The approximation has the wrong tail behavior, i.e., the Cornish-
Fisher approximation for the VaR at risk level φ becomes less and
less reliable for φ →{0, 1}.
These drawbacks can have serious consequences for risk management systems
and we propose therefore a second method to approximate the percentiles of
interest. Since the density we aim to approximate is symmetric around zero, we
simply ﬁt a Student-t density to the second and fourth conditional moments κ2
and κ4. First, we determine the conditional kurtosis of yt,s, denoted by bκ, as
follows:
bκ(ψ) = κ4(ψ)
κ2
2(ψ) .
From there, we estimate the degrees of freedom parameter bν of the Student-t
density. The relation between bκ and bν is given by:
bν(ψ) = 6 −4 bκ(ψ)
3 −bκ(ψ)
.

6.2 The concept of Value at Risk
81
Finally, the VaR is estimated by the appropriate percentile of the standard
Student-t density scaled by the conditional standard deviation κ1/2
2
. This yields
the following approximation for the s-day ahead VaR (s > 1) at risk level φ,
estimated at time t:
VaRφ
t,s(ψ) ≈
bν(ψ) −2
bν(ψ)
1/2
× tφc bν(ψ)

× κ1/2
2
(ψ) .
(6.6)
This approximation is a function of the set of parameters ψ. Hence, as with the
Cornish-Fisher approximation, the density of the VaR can be estimated at low
cost by simulating from the joint posterior sample.
In the Bayesian context, we can integrate out the parameter uncertainty
to end up with a single VaR point estimate. This problem is solved by the
estimation of the predictive density which is deﬁned as the density of future
s-day ahead observations, yt:s
.= (yt+1 · · · yt+s)′ for s ⩾1, conditioned on past
observations y0:t
.= (y1 · · · yt)′ (also denoted by Ft), but marginalized over ψ.
More formally, the predictive density is deﬁned as follows:
p(yt:s | y0:t) =
Z
p(yt:s | ψ, y0:t)p(ψ | y0:t)dψ
(6.7)
where p(yt:s | ψ, y0:t) is the conditional density of yt:s given (ψ, y0:t) and the
marginalization is with respect to the posterior density p(ψ | y0:t). In general,
the predictive density is not available in closed form. However, one can use the
posterior sample in conjunction with the method of composition to produce a
sample of draws from the predictive density. We simulate a draw y[j]
t:s from the
density p(yt:s, ψ | y0:t) as follows:
ψ[j] ∼p(ψ | y0:t)
y[j]
t:s ∼p(yt:s | ψ[j], y0:t)
(6.8)
where the second step in the simulation process is possible by using the method
of composition:
p(yt:s | ψ, y0:t) =
s
Y
i=1
p(yt+i | ψ, y0:(t+i−1)) .
(6.9)
The collection of simulated values {y[j]
t:s}J
j=1 is generated from the predictive
density in (6.7) and the predictive VaR is a percentile of this density. For the
one-day ahead VaR, we only need to consider the ﬁrst component of vector y[j]
t:s
whereas for the s-day ahead VaR, we must sum the components of y[j]
t:s to sim-
ulate the predictive density for yt,s. From (6.7), we notice that the predictive

82
6 Value at Risk and Decision Theory
VaR is a quantile of a mixture density. Therefore, it can be viewed as an exten-
sion of the case where there is no parameter uncertainty (i.e., ψ is constant);
in this case, the predictive VaR would simply be estimated by a percentile of
p(yt:s | ψ, y0:t).
To end this section, we illustrate the quality of the Cornish-Fisher and
Student-t approximations through a simulation study. To that aim, we estimate
the GARCH(1, 1) model with Student-t disturbances for the 750 Deutschmark
vs British Pound foreign exchange log-returns used in the empirical analysis of
Chap. 3. We arbitrarily select a set of parameters ψ .= (α, β, ν) in the joint
posterior sample:
α =
 
0.036
0.297
!
,
β = 0.626
and
ν = 5.4
and estimate the VaR using formulae (6.5) and (6.6) for s = 10 and φ ranging
from 0.001 to 0.999 with a step size of 0.001. This procedure allows to draw two
approximations for the distribution of y750,10. These distributions are compared
with the distribution obtained by simulating 10’000 paths of the process over ten
days, using (6.9). Under the model speciﬁcation and the selected ψ, we obtain
κ2 = 3.8 and κ4 = 492, implying a conditional kurtosis bκ = 34 and a degrees of
freedom parameter bν = 4.2. The simulated distribution clearly exhibits heavier
tails than the Normal distribution.
On the left-hand side of Fig. 6.1, we display the two approximations to-
gether with the distribution obtained by simulation. The Cornish-Fisher ap-
proximation is shown in dotted line, the Student-t approximation in dashed line
and the empirical distribution in solid line. From this ﬁgure, it is almost im-
possible to distinguish the approximation based on the Student-t distribution
from the simulated distribution. In contrast to this, the Cornish-Fisher expan-
sion produces a S-shaped, non-monotone distribution. The four shaded regions
delimit the extreme quantiles, at risk level φ ∈{0.01, 0.05, 0.95, 0.99}. We notice
that the approximations for φ ∈{0.05, 0.95} are quite similar for the Cornish-
Fisher and the Student-t approaches. However, the diﬀerence is substantial in
the case where φ ∈{0.01, 0.99}. In the middle graph of Fig. 6.1, we show a
zoom of the previous graph over the domain [2, 6] × [0.94, 1]. We can see that
the Student-t approximation ﬁts the distribution of interest well. Hence, in this
particular example, the graphical comparison indicates that the Cornish-Fisher
expansion fails in approximating the distribution of interest. On the other hand,
the approximation based on the Student-t distribution seems to provide an ad-
equate approximation of the whole distribution. To complete the simulation
study, we display, on the right-hand side of Fig. 6.1, the diﬀerence between

6.2 The concept of Value at Risk
83
the simulated distribution and the Student-t approximation as a function of φc.
The dotted lines delimit the 95% conﬁdence band for the simulation, estimated
by replicating 500 times the empirical distribution. We note that the diﬀerence
lies within the [−0.1, 0.1] interval for risk levels ranging from 0.05 to 0.95. For
other percentiles, the error increases together with the width of the conﬁdence
band. However, the conﬁdence interval still contains the value of zero, indicating
a good approximation in the tails too.
Finally, we note that other approximation methods of the whole density
for yt,s can be obtained [see, e.g., Highﬁeld and Zellner 1988]. This is of inter-
est when the density we aim to approximate is skewed, since in this case, the
Student-t approximation would fail. Such asymmetric densities arise, e.g., with
asymmetric GARCH models [see Engle 2004, p.415].

84
6 Value at Risk and Decision Theory
Approximations
VaRφ
φc
−8
−6
−4
−2
0
2
4
6
8
0.0
0.2
0.4
0.6
0.8
1.0
Predictive distribution
Cornish−Fisher
Student−t
VaRφ
φc
2
3
4
5
6
0.94
0.95
0.96
0.97
0.98
0.99
1.00
Zoom
0.0
0.2
0.4
0.6
0.8
1.0
−0.4
−0.2
0.0
0.2
0.4
0.0
0.2
0.4
0.6
0.8
1.0
−0.4
−0.2
0.0
0.2
0.4
0.0
0.2
0.4
0.6
0.8
1.0
−0.4
−0.2
0.0
0.2
0.4
φc
Prediction minus Student−t approximation
Approximation error
Fig. 6.1. Cornish-Fisher and Student-t approximations. On the left-hand side, we display the distribution given by the Cornish-Fisher (in
dotted line) and the Student-t (in dashed line) approximations together with the simulated distribution based on 10’000 paths (in solid
line). In the middle graph, we zoom the plot over the [2, 6] × [0.94, 1] domain. On the right-hand side, we plot the diﬀerence between
the simulated distribution and the Student-t approximation. The dotted lines delimit the conﬁdence band obtained by bootstrapping the
simulated distribution 500 times. The shaded regions indicate the 1st, 5th, 95th and 99th percentiles.

6.3 Decision theory
85
6.3 Decision theory
Using the Bayesian approach leads to an interesting problem of decision theory:
the choice of a Bayes point estimate within the whole VaR posterior density.
In this section, we present a short review of decision theory and introduce the
asymmetric Linex loss function. The use of asymmetric loss functions better
characterizes the views of market participants where the impact of underesti-
mation and overestimation can be signiﬁcantly diﬀerent. The Linex loss function
has proved to be advantageous in many ﬁelds, especially for ﬁnancial applica-
tions.
To keep the notation as general as possible, the decisions are formulated in
terms of ω which can either be viewed as a one-dimensional parameter or a point
forecast.
6.3.1 Bayes point estimate
As is the case in economics, statistical decisions are made based on expected
ranking. In economics this ranking is achieved with the help of a utility func-
tion while we use a loss function in statistics. The Bayesian statistical decision
consists in the choice of a point estimate over the posterior density of the pa-
rameters.
Let us assume that a decision maker needs to choose a point estimate bω
and the true state of nature is ω. In the Bayesian framework, the parameter
ω is random and its uncertainty is fully characterized by its posterior density
p(ω | y). Furthermore, we deﬁne the loss function L (bω, ω) which is the loss
incurred when ω is the true state of nature and bω is a point estimate. Then, the
Bayes estimate, also referred to as the optimal point estimate, denoted by bωL ,
is the parameter which minimizes the posterior risk RL (bω | y). Formally, the
Bayes estimate is deﬁned as follows:
bωL .= arg min
bω RL (bω | y)
(6.10)
where:
RL (bω | y) .=
Z
L (bω, ω)p(ω | y)dω .
The problem of point estimation of a location parameter or forecast is most
often treated as a symmetric problem in which positive and negative estimation
errors of the same magnitude are considered to be equally serious; thus, the loss
function L is symmetric. The most used loss functions are the squared error loss

86
6 Value at Risk and Decision Theory
(henceforth SEL), L (bω, ω) = (bω −ω)2, and the absolute error loss (henceforth
AEL), L (bω, ω) = |bω −ω|. Indeed most of the existing VaR literature ignores
the asymmetric loss relevant for diﬀerent economic agents. However, the impact
of overestimating or underestimating VaR can be quite diﬀerent. As quoted by
Knight, Satchell, and Wang [2003, p.335]:
“From the perspective of the fund manager in a bank, the loss of over-
estimating is usually much greater than that of underestimating, as the
reserve capital exceeds the capital required by regulation and earns little
or no return at all. On the other hand, from the regulator’s perspective,
systematic failure would be increasing in the degree to which each bank’s
losses actually exceed their capital reserves. So underestimating will re-
sult in more loss for the regulator.”
This suggests the need of an appropriate asymmetric loss function when choosing
a point estimate within the VaR density.
We point out that, in light of the capital structure theory, the relevance of
an asymmetric loss function for banks is questionable since capital reserves do
not have to be held in cash. In this case, if regulators demand a higher capital
reserve, the bank will just have to rearrange its capital structure, which does
not necessarily increase capital cost. This suggests that bank managers should
in fact not be interested in minimizing regulatory capital. While this argument
is valid for the bank as a whole, it does not hold at the trading desk level since
it is common that traders and fund managers need a buﬀer in cash for facing
market risk exposures.
6.3.2 The Linex loss function
The Linex loss function is employed in the analysis of several central statistical
estimation and prediction problems. Varian [1974] motivates the use of the Linex
loss function on the basis of an example in which there is a natural imbalance
in the economic results of estimation errors of the same magnitude. Varian
argues that the Linex loss is a rational manner to formulate the consequences
of estimation errors in real estate assessment. Christoﬀersen and Diebold [1996,
1997] use the Linex loss function in a study of optimal point prediction where
diﬀerent asymmetric loss functions are tested. More recently, Hwang, Knight,
and Satchell [1999, 2001] derive the Linex one-day ahead volatility forecast for
various volatility models. The empirical results of these authors suggest the
Linex loss function to be particularly well-suited in ﬁnancial applications. In
addition, we note that other ﬁelds than quantitative ﬁnance make use of the
Linex loss function. An example is given in the ﬁeld of hydrology with the

6.3 Decision theory
87
estimation of peak water level in the construction of dams or levies. In that
case, overestimation represents a conservative error which increases construction
costs, while underestimation corresponds to the much more serious error in
which overﬂows might lead to huge damages in the adjacent communities.
In its reduced form, the Linex loss function is given by:
L (bω, ω) = exp[a∆] −a∆−1
(6.11)
where a ∈R∗and ∆.= (bω −ω) denotes the scalar estimation error in using bω
when estimating ω. From expression (6.11), we note that:
• L is a convex function of ∆;
• L is decreasing for ∆∈] −∞, 0[ and increasing for ∆∈]0, ∞[;
• for a > 0, L grows exponentially in positive ∆but behaves ap-
proximately linearly for negative values of ∆. In this case, the Linex
loss function imposes a substantial penalty for overestimation, i.e.,
when bω > ω;
• for |a| ≃0, L is almost symmetric and not far from a squared
error loss function. Indeed, on expanding:
exp[a∆] ≃1 + a∆+ (a∆)2
2
and replacing it in expression (6.11), the loss function becomes
proportional to the SEL function. Thus for small values of a, the
SEL function is approximately nested within the Linex function.
In the upper graph of Fig. 6.2, we display the Linex loss function for parameter
a = 0.5 in dotted line, a = 1 in dashed line and a = 2 in solid line. We can
notice the impact on the shape of the loss function of larger values of parameter
a. Indeed, as a increases, the asymmetry accentuates. In the lower part of the
ﬁgure, we show the Linex loss function for parameter a = 0.1 together with the
appropriately scaled SEL function. The loss functions are almost the same on
the interval.
As veriﬁed by Zellner [1986], the derivation of the Bayes estimator of ω is
straightforward under the Linex loss function (6.11). The optimization prob-
lem (6.10) yields:
bωL = −1
a ln
Z
exp[−aω]p(ω | y)dω


88
6 Value at Risk and Decision Theory
provided that the integral is ﬁnite. Hence, the key to Linex estimation is to ﬁnd
the moment generating function of p(ω | y), which can be estimated using the
posterior sample.

6.3 Decision theory
89
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
Linex loss function
∆
a=0.5
a=1
a=2
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0.000
0.005
0.010
0.015
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0.000
0.005
0.010
0.015
Linex and approximation
∆
Linex (a=0.1)
Approximation
Fig. 6.2. Linex loss function. In the upper graph we plot the Linex function for
diﬀerent values of parameter a. In the lower part, we show the Linex loss function for
parameter a = 0.1 in solid line together with the SEL function in dashed line. We
recall that ∆.= (bω −ω) where bω is the point estimate and ω is the true parameter
value.

90
6 Value at Risk and Decision Theory
6.3.3 The Monomial loss function
We end this section by noting that other interesting asymmetric loss functions
are readily available in the statistical literature. A general class of asymmetric
loss functions, referred to as Monomial-splined functions by Thompson and Basu
[1995], is deﬁned as follows:
L (∆) .=
(
a1 × |∆|p
if
∆⩾0
a2 × |∆|p
if
∆< 0
(6.12)
where ∆.= (bω−ω), ai ∈R+ (i = 1, 2) and p ∈N∗. This class provides asymmet-
ric loss functions for a1 ̸= a2; when a1 > a2, an overestimation incurs more loss
than an underestimation and inversely when a1 < a2. From expression (6.12),
we note the two following special cases:
• p = 1: linear-linear loss function; when a1 = a2 we obtain the AEL
function;
• p = 2: quadratic-quadratic loss function; when a1 = a2 we obtain
the SEL function.
It is often easier to work with a reparametrization of expression (6.12). Let us
deﬁne q .=
a1
a1+a2 and make use of the homogeneity property so that we obtain
the following loss function:
L (∆) =
 q + (1 −2q)I{∆<0}

|∆|p .
(6.13)
This function is particularly interesting when p = 1. In this case, it turns out
that the optimal point estimate ˆωL is nothing else than the qth percentile
of the posterior density p(ω | y). Hence, expression (6.13) gives a statistical
justiﬁcation to the choice of a posterior percentile as Bayes point estimate.
E.g., choosing the 95th percentile of the VaR density would be optimal for an
agent whose loss function is given by (6.13) with q = 0.95. Finally, we note
that numerical methods are needed to ﬁnd Bayes point estimates for the loss
function (6.13) when p > 1.
In Fig. 6.3, we display the loss function given in expression (6.13) for pa-
rameter q = 0.5 in dotted line, q = 0.75 in dashed line and q = 0.95 in solid
line. As parameter q increases, the asymmetry becomes more pronounced and
the impact of an overestimation, i.e., ∆> 0, is larger compared to the impact
of an underestimation. The function is symmetric for q = 0.5.

6.4 Empirical application: the VaR term structure
91
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
Monomial loss function
∆
q=0.5
q=0.75
q=0.95
Fig. 6.3. Monomial loss function for diﬀerent values of parameter q. We recall that
∆.= (bω −ω) where bω is the point estimate and ω is the true parameter value.
6.4 Empirical application: the VaR term structure
In this section, we estimate the term structure of the VaR when the P&L dy-
namics is described by a GARCH(1, 1) model with Normal and Student-t dis-
turbances. Our analysis is inspired by the paper of Guidolin and Timmermann
[2006] which considers the impact of diﬀerent econometric speciﬁcations to the
shape of the VaR term structure. While the authors report signiﬁcant diﬀer-
ences between the models, they do not account for parameter uncertainty in
their analysis. This is indeed a weakness of their approach, as recognized by the
authors [see Guidolin and Timmermann 2006, p.307]:
“We ignored parameter estimation uncertainty in our analysis, but this
could have important eﬀects on the results.”
The Bayesian approach provides a natural framework for investigating this
point. As shown in Sects. 6.2.1 and 6.2.2, the VaR can be expressed as a
function of the GARCH(1, 1) parameters under both Normal and Student-t

92
6 Value at Risk and Decision Theory
speciﬁcations. Consequently, the parameter uncertainty estimated by the joint
posterior sample can be used to estimate the density of the VaR in a convenient
manner.
An additional justiﬁcation for the use of the Bayesian approach in our con-
text is given by Miazhynskaia and Aussenegg [2006] who compare the Bayesian
and traditional techniques for estimating GARCH models. In particular, they
conclude that the Bayesian approach is an adequate framework with less un-
certainty in VaR estimates compared to other VaR methods such as resampling
technique and asymptotic Normal approximation. They also mention the inter-
esting issue of determining a single VaR point estimate [see Miazhynskaia and
Aussenegg 2006, p.262]:
“Open questions for future research are how the total VaR distribution
can be used in market risk management and how to account for VaR
uncertainty in choosing traditional VaR point estimates used to calculate
capital requirements for ﬁnancial institution.”
This is precisely what we aim to achieve in a rational manner through the
decision theory framework.
6.4.1 Data set and estimation design
Our empirical analysis uses the Deutschmark vs British Pound exchange rate
daily log-returns over a sample period ranging from January 3, 1985, to Decem-
ber 31, 1991, for a total of 1’974 observations. This data set was used in the
empirical analysis of Chap. 3.
We consider daily log-returns so that the VaR term structure focuses on
short-term horizons. This is of primary interest for traders and risk managers
who adjust the bank’s portfolios on a daily basis. The methodology can also be
applied to longer time span log-returns as this is done in Guidolin and Timmer-
mann [2006]. In this manner, a term structure for mid- and long-term horizons is
obtained. Note however that modeling monthly or quarterly ﬁnancial data would
probably require more complicated models than the GARCH(1, 1) speciﬁcation,
to account for structural breaks in the time series, for instance. This could dras-
tically complicate the approximation methodology developed in Sect. 6.2.2, in
particular to ﬁnd the ﬁrst four moments conditioned on the model parameters
and information set.
The estimation of the GARCH(1, 1) models is achieved by using the rolling
window methodology. This procedure is heavily used in ﬁnance and ﬁnancial
risk management. The rationale behind it is to act as if we were moving over
time, using past observations to estimate the model and test the performance

6.4 Empirical application: the VaR term structure
93
over a prediction window. This is based on the assumption that older data are
not available or are irrelevant due to structural breaks, which are so complicated
that they cannot be modeled. Conceptually, this method aims to take account
for more recent information in a simpliﬁed framework and it has proved to be
eﬀective in many ﬁnancial applications.
We structure the estimation procedure as follows: 750 log-returns, which is
about three trading years, are used to estimate the models. Then, the next 50
log-returns, which is slightly less than one quarter, are used as a forecasting
window. In the next step, the estimation and forecasting windows are moved
together by 50 days ahead, so that the forecasting windows do not overlap. In
this manner, the model parameters are updated every quarter and the estimation
methodology fulﬁlls the recommendations of the Basel Committee in the use of
internal models [see Basel Committee on Banking Supervision 1996b]. When
applied to our data set, the estimation design leads to the generation of 24
estimation windows. The non-overlapping forecasting windows represent a total
of 24 × 50 = 1’200 observations. An illustration of the methodology is shown
in Fig. 6.4 where we plot the ﬁrst three observation windows excerpt from our
data set; the vertical lines separate the estimation and the forecasting windows.
Note that the standard practice when using the rolling window methodology
in the context of GARCH models consists in moving the window by a single
day ahead. While this procedure can be achieved quite rapidly when estimating
the model by the Maximum Likelihood technique, this can become a computa-
tional burden with the Bayesian approach, since at each step, we need to run
the MCMC scheme again. This problem is however only relevant in an ex-post
framework; a portfolio or risk manager could run the Bayesian estimation of the
model every day, without encountering these computational diﬃculties.

94
6 Value at Risk and Decision Theory
0
200
400
600
800
1000
−2
−1
0
1
2
0
200
400
600
800
1000
−2
−1
0
1
2
Observations windows
Daily log−returns
(in percent)
time
index
0
200
400
600
800
1000
−2
−1
0
1
2
0
200
400
600
800
1000
−2
−1
0
1
2
0
200
400
600
800
1000
−2
−1
0
1
2
0
200
400
600
800
1000
−2
−1
0
1
2
Fig. 6.4. Estimation and forecasting windows (ﬁrst 3 windows out of 24). The 750
log-returns used for the estimation are shown in solid line and the 50 out-of-sample
log-returns are shown in dotted line. The vertical line separate the estimation and the
forecasting windows. At each step in the procedure, both windows are moved together
by 50 days ahead.
6.4.2 Bayesian estimation
As prior densities for the scedastic function’s parameters α and β, we choose
truncated Normal densities with zero mean vectors and diagonal covariance
matrices whose variances are set to 10’000. In the case of Student-t disturbances,
we use the translated Exponential as a prior density for the degrees of freedom
parameter; the hyperparameters are set to λ = 0.01 and δ = 4; the prior mean
is therefore 104 and the prior variance 10’000. The parameter δ is set so that the
conditional variance and conditional fourth moment exist, which allows the use
of the approximation for the predictive density based on the ﬁrst four moments
(see Sect. 6.2.2 for details). For each estimation window, two chains are run for
10’000 passes each and the convergence diagnostic test by Gelman and Rubin
[1992] is applied to guarantee a good convergence of the algorithm. From the

6.4 Empirical application: the VaR term structure
95
overall MCMC output, we discard the ﬁrst 5’000 draws and merge the two chains
to get a ﬁnal sample of length 10’000.
6.4.3 The term structure of the VaR density
As a preliminary analysis, we consider the ﬁrst observation window excerpt from
our data set and estimate the (conditional) term structure of the VaR at risk level
φ = 0.95. We consider risk horizons ranging from one day to ﬁfteen days for the
VaR density estimated under both GARCH(1, 1) Normal and Student-t models
using approximation (6.6). The two term structures are depicted in Fig. 6.5; the
lines give the median point estimates while the shaded regions depict the 95%
conﬁdence intervals of the densities. From this graph, we note that the VaR is a
monotone decreasing function of the time horizon for both models. The Student-t
speciﬁcation leads to lower median point estimates for time horizons larger than
ﬁve days while the diﬀerences for smaller horizons are less pronounced. We also
notice that the VaR uncertainty increases for both models with respect to the
time horizon. Furthermore, the GARCH(1, 1) model with Student-t disturbances
leads to higher uncertainty in VaR at each horizon compared to the Normal
speciﬁcation. Finally, we note that the VaR density is almost symmetric for all
horizons in the Normal case while the density is left-skewed for the Student-t
model.
This ﬁrst static analysis indicates that the density of the VaR is inﬂuenced by
the time horizon as well as the speciﬁcation of the model disturbances; leptokur-
tic disturbances lead to a larger uncertainty in the VaR as well as a left-skewed
density.

96
6 Value at Risk and Decision Theory
VaR term structure
(in percent)
time horizon
(in days)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
−4.5
−4.0
−3.5
−3.0
−2.5
−2.0
−1.5
−1.0
 
 
 
 
Posterior median (Normal disturbances)
Posterior median (Student−t disturbances)
95% CI (Normal disturbances)
95% CI (Student−t disturbances)
Fig. 6.5. Term structures of the VaR density at risk level φ = 0.95 for the
GARCH(1, 1) model with Normal and Student-t disturbances. Both densities are based
on 10’000 draws from the joint posterior sample of the models’ parameters.
6.4.4 VaR point estimates
We now investigate the diﬀerences in VaR point estimates under diﬀerent loss
functions of the forecasters. The comparison of point estimates over the out-of-
sample window has two purposes. First, it will provide a statistical counterpart
to the graphical ﬁndings observed in the preceding section. Second, since the
VaR point estimates are used to calculate capital requirements for ﬁnancial in-
stitutions, this analysis will give a ﬁrst idea on how large the diﬀerences in risk
capital are between agents facing diﬀerent risk perspectives. In what follows, we
concentrate the analysis on horizon s ∈{1, 5, 10} and risk level φ ∈{0.95, 0.99}.
Note that the particular case (s = 1, φ = 0.95) corresponds to the criterion
employed by the popular RiskMetrics benchmark [see RiskMetrics Group 1996].
The case (s = 10, φ = 0.99) is recommended by the Basel Committee on Bank-
ing Supervision [1996a] and aims to take in consideration liquidity constraints
encountered by the bank.

6.4 Empirical application: the VaR term structure
97
To compare the VaR point estimates resulting from the use of diﬀerent loss
functions, we use the following methodology: for each point in the out-of-sample
data set, we estimate the density of the VaR for the three diﬀerent time hori-
zons and the two risk levels using approximation (6.6). Then, for each den-
sity, we determine a point estimate for the VaR which solves the optimization
problem (6.10) for a given loss function L ; this point estimate is denoted by
d
VaR
φ
L ,t,s. In what follows, we use the asymmetric Linex loss, the absolute error
loss (AEL) as well as the squared error loss (SEL) functions for L , the latter
being considered as the benchmark in our analysis. As an additional point es-
timate, we use the predictive VaR deﬁned in (6.7). In this case, for each draw
in the joint posterior sample, we generate a draw from the predictive density
using (6.8) and the predictive VaR is obtained by calculating the appropriate
percentile of the simulated density.
Some comments regarding the diﬀerent perspectives of VaR point estimation
are in order here. In the ﬁrst approach, we estimate the density of the VaR by
simulating from the joint posterior sample and choose a point estimate which
is optimal for a given loss function (Linex, SEL and AEL). The parameter
uncertainty is integrated out in the second step of the procedure, when the
posterior risk is minimized (see Sect. 6.3.1). This methodology is natural in
combining estimation and decision making, and gives therefore an additional
ﬂexibility to the user. In the second approach, the parameter uncertainty is
integrated out by averaging the conditional densities of the cumulated returns
over the joint posterior density of the model parameters. In this case, the VaR
point estimate (i.e., the predictive VaR) is not related to the risk preferences of
an agent and is the same for both regulators and fund managers. Hence, agents
diﬀer not in the estimation of the VaR, but in the way they would make use of
the point estimate afterwards. This approach is natural in Bayesian statistics
but it is not the most sophisticated. It can be viewed as an extension of the
case where there is no parameter uncertainty (i.e., ψ is constant); in this case,
the predictive VaR would simply be estimated by a percentile of the conditional
density of future observations. The justiﬁcation of the predictive VaR on the
grounds of the decision theory still needs to be established.
To ﬁnd an optimal VaR point estimate under the Linex loss function, we
need ﬁrst to choose a value for the parameter a in expression (6.11). The most
direct way is to elicit the parameter through detailed discussion with a fund
or risk manager. Due to the unavailability of this type of data, we will thus
rely on the estimation of Knight et al. [2003] where the authors found a ≈3
based on Standard and Poors 500 index data. In our framework, since the VaR
estimates are negative percentages, this positive parameter a implies a larger

98
6 Value at Risk and Decision Theory
penalty when the estimated VaR is underestimated (in absolute value) compared
to the true VaR, i.e., d
VaR > VaR. Hence, the Linex optimal point estimate
will be conservative in the sense that it will be located in the left tail of the
density to avoid underestimation. Such loss function can thus be attributed to
a regulator or a risk manager whose aim is to avoid systematic failure in VaR
estimation. For comparison purposes, we also consider the Linex function with
parameter a = −3. In that case, overestimating (in absolute value) the VaR,
i.e., d
VaR < VaR, leads to a larger penalty so that the Linex point estimate will
be located in the right tail of the VaR density. This is the loss perspective of
a trader or fund manager whose aim is to save regulatory capital since it earns
little or no return at all (as pointed out previously, traders hold a buﬀer in
cash for facing market risk exposures). The AEL and SEL functions correspond
to the perspective of an agent for which under- and overestimation are equally
serious; the SEL leads however to a larger penalty for large deviations from the
true VaR, compared to the AEL function.
Once the time series of VaR point estimates for a given loss function has
been obtained, we compare its values to the SEL benchmark. More precisely,
for a given risk level φ and time horizon s we compute a time series of diﬀerences
between the VaR point estimates obtained with the loss function L and the SEL
benchmark. Then, we estimate the average of deviations over the N = 1’200
out-of-sample observations as follows:
1
N
N
X
t=1

d
VaR
φ
L ,t,s −d
VaR
φ
SEL,t,s

.
Results of the average deviations are reported in Table 6.1 where the table
entries are given in hundredth percent, i.e., multiplied by 100, for convenience.
The upper panel gives the results for the GARCH(1, 1) model with Normal
disturbances while the lower panel presents the results for the Student-t case.
From this table, we ﬁrst note that the average deviations vary considerably
between table entries; the minimum value is 0.0001% in the case of the predictive
VaR for Normal disturbances with (s = 1, φ = 0.95) while the maximum is
0.496% in the case of the Linex VaR (a = 3) for Student-t disturbances with
(s = 10, φ = 0.99). Moreover, we note that the average size of deviations increase
with respect to the forecasting horizon. The deviations are also larger for risk
level φ = 0.99 and Student-t disturbances (except for the predictive VaR at risk
level φ = 0.99).
Deviations from the SEL benchmark are expected for the two Linex func-
tions. Indeed, the asymmetric nature of the function leads to point estimates
which are necessarily lower or larger than the mean point estimate. In the case

6.4 Empirical application: the VaR term structure
99
of the AEL function, departure from the SEL indicates asymmetric shapes for
the VaR densities over the out-of-sample window. More precisely, the AEL point
estimates are less conservative than the SEL on average, indicating left-skewed
densities for the VaR. This asymmetry can also be captured by comparing the
average deviations for the Linex loss functions. While a symmetric density for
the VaR would imply similar values (of opposite sign) for the two Linex func-
tions, this is clearly not the case here, especially for the Student-t density at
time horizons s = 5 and s = 10. Finally, we can notice that the predictive VaR
point estimates are close to the SEL point estimates for almost all risk levels
and time horizons; for these cases, choosing a quantile of the predictive distri-
bution or choosing the posterior mean of the VaR leads to the same VaR point
estimates, on average.
In summary, the VaR is left-skewed and the asymmetry as well as the un-
certainty increase with respect to the time horizon and the risk level. The av-
erage deviations are also larger when the GARCH(1, 1) model disturbances are
Student-t distributed. At ﬁrst sight, the deviations seem negligible (we recall
that the maximum deviation is half a percent). As will be shown later in this
chapter, the common testing methodology for assessing the performance of the
VaR is unable to discriminate between the point estimates but the deviations
are large enough to imply substantial diﬀerences in terms of regulatory capital.
This therefore gives an additional ﬂexibility to the user when allocating risk
capital.

100
6 Value at Risk and Decision Theory
Table 6.1. Average deviations of the VaR point estimates from the SEL
benchmark.⋆
GARCH(1, 1) with Normal disturbances
φ = 0.95
φ = 0.99
Loss L
s = 1
s = 5
s = 10
s = 1
s = 5
s = 10
Linex (a = 3)
−0.233
−1.385
−3.605
−0.467
−4.041
−12.790
Linex (a = −3)
0.230
1.332
3.226
0.459
3.602
9.470
AELa
0.066
0.256
0.604
0.093
0.637
1.628
Predictiveb
0.011
−0.148
−0.145
−0.344
−1.603
−2.309
GARCH(1, 1) with Student-t disturbances
φ = 0.95
φ = 0.99
Loss L
s = 1
s = 5
s = 10
s = 1
s = 5
s = 10
Linex (a = 3)
−0.318
−2.496
−11.598
−1.087
−9.911
−49.603
Linex (a = −3)
0.312
2.185
6.695
1.043
7.739
22.548
AELa
0.091
0.570
1.863
0.241
1.204
3.498
Predictiveb
0.013
−0.104
1.353
−0.027
0.645
0.847
⋆The tables entries are given in hundredth percent, i.e., multiplied by 100. L :
loss function; s: time horizon (in days); φ: risk level.
a Absolute error loss.
b In the case of the predictive VaR, the point estimate is the φcth percentile of
the predictive density for the s-day ahead cumulative return; φc .= 1 −φ.
6.4.5 Regulatory capital
In this section, we assess the ﬁnancial consequences resulting from the use of a
particular loss function when determining a VaR point estimate. To that aim,
we will base our analysis on the notion of the regulatory capital as deﬁned by the
Basel II approach for market risk [see Basel Committee on Banking Supervision
1996b]. This capital is a cushion for market risk exposures and its value is based
on the ten-day ahead VaR at risk level φ = 0.99. Formally, the regulatory capital
allocated at time t by an agent facing a loss function L can be expressed as
follows:
c
RCL ,t .= min
(
d
VaR
0.99
L ,t,10, ζ
60
59
X
i=0
d
VaR
0.99
L ,t−i,10
)
(6.14)
where d
VaR
0.99
L ,t,10 denotes the ten-day ahead VaR point estimate at time t, for
risk level φ = 0.99 and loss function L . The value ζ ∈[3, 4] is a stress factor
determined by the quality of the model; it is ﬁxed by the regulators and is based
on the forecasting performance of the model. We will set ζ to 3.5 for simplicity
in what follows. From formula (6.14), we note that the regulatory capital is

6.4 Empirical application: the VaR term structure
101
smoothed over time in order to avoid frequent adjustments of the balance sheet
(which is costly for the bank) but can also react quickly enough to market news
such as crashes.
As this was done in Sect. 6.4.4 for the VaR point estimates, we calculate the
time series of diﬀerences between the regulatory capital obtained under loss L
and the SEL benchmark and then compute the average of the deviations over the
out-of-sample window. Results are reported in Table 6.2 where the table entries
are given in percent. First, we can notice that the deviations are much larger
than for the VaR point estimates; the average deviations range from 0.023% in
the case of the predictive VaR to 1.741% in the case of the Linex (a = 3), both
for the GARCH(1, 1) model with Student-t disturbances. In general, deviations
from the SEL are larger when the disturbances are Student-t distributed. The
percentage of capital obtained with the AEL function is, on average, lower than
with the SEL, indicating a left-skewed density for the regulatory capital. The
largest deviation is obtained for the Linex function with parameter a = 3; in this
case, a risk manager or regulator will keep aside a capital which is 1.741% larger
than the SEL agent, for which under- or overestimation are equally serious. In
contrast to this, a fund manager will be able to invest 0.79% more capital on
ﬁnancial markets. For this special case, there is a diﬀerential of about 2.5%
in risk capital allocation between a risk manager and a fund manager; this is
substantial if we imagine the amounts invested on ﬁnancial markets by ﬁnancial
institutions.
Table 6.2. Average deviations of the regulatory capital
point estimates from the SEL benchmark.⋆
GARCH(1, 1) disturbances
Loss L
Normal
Student-t
Linex (a = 3)
−0.446
−1.741
Linex (a = −3)
0.331
0.790
AELa
0.056
0.122
Predictiveb
−0.084
0.023
⋆The tables entries are given in percent. L : loss function.
a Absolute error loss.
b In the case of the predictive VaR, the point estimate is the
φcth percentile of the predictive density for the s-day ahead
cumulative return; φc .= 1 −φ.

102
6 Value at Risk and Decision Theory
6.4.6 Forecasting performance analysis
To test the ability of our models to capture the true VaR, we compare the real-
ization of the cumulated returns {yt,s}t with our VaR estimates for time horizon
s ∈{1, 5, 10} and risk level φ ∈{0.95, 0.99}. To that aim, we adopt the (back-
testing) methodology proposed by Christoﬀersen [1998] which has become the
standard practice in ﬁnancial risk management. When the forecasting horizon
is one day, this approach is based on the study of the random sequence {V φ
t }
where:
V φ
t
.=



1
if
yt+1 < VaRφ
t
0
else .
A sequence of VaR forecasts at risk level φ has correct conditional coverage if
{V φ
t } is an independent and identically distributed sequence of Bernoulli random
variables with parameter φc .= (1−φ). In practice, this hypothesis can be veriﬁed
by testing jointly the independence on the series and the unconditional coverage
of the VaR forecasts, i.e., E(V φ
t ) = φc.
In order to test the performance of the s-day ahead VaR (s > 1), we use
a similar methodology, based now on the study of the random sequence {V φ
t,s}t
where:
V φ
t,s
.=



1
if
yt,s < VaRφ
t,s
0
else .
In this case however, since the cumulative returns yt,s and yτ,s overlap for
|t −τ| ⩽s, the variables V φ
t,s and V φ
τ,s are not independent and the usual test
by Christoﬀersen [1998] cannot be applied directly. However, we can exploit the
structure of dependence between yt,s to yτ,s to get rid of this diﬃculty. Indeed,
the construction of cumulative returns leads to the creation of spurious moving
average eﬀects of order (s −1) in the time series {yt,s}t. We can therefore follow
Diebold and Mariano [1995] and correct the test by Christoﬀersen [1998] for
serial correlation via Bonferroni bounds. To that aim, we partition the series
{yt,s}t into groups for which we expect independence and correct unconditional
coverage. Under the assumption that the series {V φ
t,s}t is (s−1) dependent, each
of the following s sub-series:

6.4 Empirical application: the VaR term structure
103
{V φ
1,s, V φ
1+s,s, V φ
1+2s,s, . . .}
{V φ
2,s, V φ
2+s,s, V φ
2+2s,s, . . .}
...
{V φ
s,s, V φ
2s,s, V φ
3s,s, . . .}
will be iid Bernoulli distributed if the model for the underlying process is correct.
Thus, a formal test with size bounded by α can be obtained by performing
s tests, each of size α/s, on each of the s sub-series, and rejecting the null
hypothesis if the null is rejected for any of the s sub-series.
Forecasting results are reported in Table 6.3 where we give the p-values
of the unconditional coverage (UC), independence (IND) as well as conditional
coverage (CC) tests; for time horizons s = 5 and s = 10, we report the lowest
p-value computed from the s series of VaR forecasts. Our results indicate that
Table 6.3. Forecasting results of the VaR point estimates.⋆
GARCH(1, 1) model with Normal disturbances
φ = 0.95
φ = 0.99
s
UC
IND
CC
UC
IND
CC
1
0.026
0.761
0.081
0.018
0.052
0.009
5
0.392
0.062
0.121
0.306
NA
NA
10
0.412
0.544
0.594
0.162
NA
NA
GARCH(1, 1) model with Student-t disturbances
φ = 0.95
φ = 0.99
s
UC
IND
CC
UC
IND
CC
1
0.222
0.903
0.470
0.572
NA
NA
5
0.392
0.050
0.101
0.344
NA
NA
10
0.983
0.280
0.558
0.162
NA
NA
⋆Forecasting test by Christoﬀersen [1998] based on the SEL point
estimates. φ: risk level; s time horizon (in days); UC: p-value for the
correct uncoverage test; IND: p-value for the independence test; CC:
p-value for the correct conditional coverage test; NA: not applicable.
the GARCH(1, 1) model with Normal disturbances fails, at the 5% signiﬁcance
level, in forecasting the one-day ahead VaR for both risk levels. Indeed, the un-
conditional coverage gives a p-value of 0.026 for φ = 0.95 and 0.018 for φ = 0.99.
The joint test of correct unconditional coverage and independence is however
only rejected for risk level φ = 0.99. In contrast to this, the GARCH(1, 1) model
with Student-t disturbances performs well. For longer time horizons, the models
behave similarly well and neither the unconditional coverage nor the indepen-

104
6 Value at Risk and Decision Theory
dence tests are rejected at the 5% signiﬁcance level. We point out, however, that
the test by Christoﬀersen [1998] is powerful when the number of observations is
large. In our context of 1’200 observations, the test of the ten-day ahead VaR
is based on ten sequences of (only) 120 observations. At risk level φ = 0.99,
a single violation is thus expected. The forecasting results should therefore be
taken with caution in this case.
We emphasize the fact that the test has been applied to the time series of SEL
point estimates. For comparison purposes, we have also analyzed the forecasting
performance of the alternative VaR point estimates, obtained with the Linex and
AEL functions. In all cases, the testing methodology gave similar p-values for
the diﬀerent risk levels and time horizons. This is not surprising. Indeed, the
diﬀerences between the VaR point estimates are small (we recall that the largest
deviation is -0.496%) and the test by Christoﬀersen [1998] focuses on the number
of times the VaR is exceeded instead of testing the size of discrepancy between
predictions and realizations. In addition, the case where the diﬀerences between
point estimates are important was observed for a forecasting horizon of ten days
at risk level φ = 0.99, precisely the case where the power of the test is weak.
Therefore, alternative (more powerful) tests should be developed, as recently
pursued by Zumbach [2006]. In Sect. 7.6 of the next chapter, we will document
that the diﬀerences between the one-day ahead VaR point estimates are large
when the P&L dynamics is described by a Markov-switching GJR model. In this
context, the loss function of the forecaster leads to diﬀerent conclusions on the
forecasting performance of the model, even when relying on the common testing
methodology of Christoﬀersen [1998].
6.5 The Expected Shortfall risk measure
While being now a standard tool in ﬁnancial risk management, the VaR has
been criticized in the research literature for several reasons, in particular:
• the VaR does not tell anything about the potential size of loss that
exceeds its level and, as a result, it is ﬂawed;
• the VaR is not a coherent measure of risk in the sense of Artzner,
Delbaen, Eber, and Heath [1999]. In particular, it lacks the prop-
erty of sub-additivity.
To circumvent these problems, the concept of Expected Shortfall (henceforth
ES) also known as Conditional VaR or CVaR has been introduced by Artzner
et al. [1999].

6.5 The Expected Shortfall risk measure
105
Deﬁnition 6.2 (Expected Shortfall). Let Y be a univariate random vari-
able with distribution FY , assumed continuous for simplicity. Then the Expected
Shortfall at risk level φ is deﬁned as the expected value of Y below the VaRφ
level. Formally:
ESφ .= E(Y | Y ⩽VaRφ) =
R VaRφ
−∞
y dFY (y)
φc
(6.15)
where we recall that φc .= (1−φ) for convenience and VaRφ is given in Def. 6.1
(see p.76).
Basically, the ES risk measure is the expectation of the P&L below the VaR
level.
In the case of the GARCH(1, 1) model with Normal and Student-t distribu-
tions, the integral on the right-hand side of expression (6.15) can be calculated
explicitly given the set of parameters ψ. Indeed, when the model disturbances
are Normally distributed, the one-day ahead ES at risk level φ, estimated at
time t, is given by:
ESφ
t (ψ) = h1/2
t+1(α, β) ×
−exp
h
−
z2
φc
2
i
√
2πφc
(6.16)
where we recall that ψ .= (α, β), ht+1 is the conditional variance which is com-
puted by recursion given the information set Ft and zφc is the φc percentile of
the standard Normal distribution. In the case of a Student-t distribution with
ν degrees of freedom, the one-day ahead ES at risk level φ, estimated at time t,
can be expressed as follows:
ESφ
t (ψ) =

ϱ(ν) × ht+1(α, β)
1/2 × Ψφc(ν)
(6.17)
with:
Ψφc(ν) .=
Γ( ν+1
2 )
Γ( ν
2)
√νπ

ν
ν−1
 
1 +
t2
φc(ν)
ν
 1−ν
2
φc
where in this case ψ .= (α, β, ν), ϱ(ν) .= ν−2
ν
and tφc is the φcth percentile of the
Student-t distribution. Once a joint posterior sample of the model parameters
is obtained, expressions (6.16) and (6.17) can be used to simulate the density of
the one-day ahead ES risk measure at any risk level φ.

106
6 Value at Risk and Decision Theory
In order to ﬁnd the expression for the s-day ahead ESφ (s > 1), we note that
the Expected Shortfall can also be viewed as the average of VaR below the risk
level φc.
Proposition 6.3. Assuming that E(|Y |) < ∞and FY is continuous, we can
express the Expected Shortfall as follows:
E(Y | Y ⩽VaRφ) =
R φc
0
VaRu du
φc
.
Proof. The integral in (6.15) is transformed by the change of variable
y 7→u .= FY (y), so that:
Z VaRφ
−∞
y dFY (y) =
Z φc
0
VaRu du
since FY (−∞) = 0, FY (VaRφ) = φc, du = dFY (y) and y = VaRu by Def. 6.1
(see p.76).
⊓⊔
Using Prop. 6.3, we can estimate the s-day ahead ES at any risk level φ by
integrating the s-day ahead VaR over the (0, φc] interval. Formally:
ESφ
t,s(ψ) =
R φc
0
VaRu
t,s(ψ)du
φc
(6.18)
where VaRu
t,s(ψ) is calculated using approximation (6.6) on page 81. The integral
in expression (6.18) can be estimated by conventional quadrature methods. As
in the one-day ahead VaR, the joint posterior sample can be used to simulate
the density for the s-day ahead ES using formula (6.18).
In Fig. 6.6, we display the (conditional) term structure of the ES density
at risk level φ = 0.95 for the ﬁrst observation window excerpt from our data
set. As in the VaR illustration of Sect. 6.4.3, the lines give the median point
estimates and the shaded regions depict the 95% conﬁdence intervals of the ES
density. From this graph, we note that the GARCH(1, 1) model with Student-t
disturbances leads to lower median point estimates for every time horizon. In
addition, the ES uncertainty increases for both models with respect to the time
horizon. The Student-t model leads to higher uncertainty in ES at each horizon
compared to the Normal speciﬁcation. Finally, the left asymmetry of the density
is visually apparent for horizons larger than ﬁve days for both models.
A comparison with the VaR term structure displayed in Fig. 6.5 (see p.96)
indicates that the ES density has heavier tails and a skewness which is more

6.5 The Expected Shortfall risk measure
107
ES term structure
(in percent)
time horizon
(in days)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
−7.0
−6.5
−6.0
−5.5
−5.0
−4.5
−4.0
−3.5
−3.0
−2.5
−2.0
−1.5
−1.0
 
 
 
 
Posterior median (Normal disturbances)
Posterior median (Student−t disturbances)
95% CI (Normal disturbances)
95% CI (Student−t disturbances)
Fig. 6.6. Term structures of the ES density at risk level φ = 0.95 for the GARCH(1, 1)
model with Normal and Student-t disturbances. The density is based on 10’000 draws
from the joint posterior sample of the models’ parameters.
pronounced. Therefore, given preferences in risk perspectives lead to larger dif-
ferences in ES point estimates.

7
Bayesian Estimation of the Markov-Switching
GJR(1, 1) Model with Student-t Innovations
(...) “the application of GARCH to long time series of
stock-return data will yield a high measure of persistence
because of the presence of deterministic shifts in the
unconditional variance and the subsequent failure of the
econometrician to model these shifts.”
— Christopher G. Lamoureux and William D. Lastrapes
In this chapter, we address the problem of estimating GARCH models subject
to structural changes in the parameters; namely, the Markov-switching GARCH
models (henceforth MS-GARCH). In this framework, a hidden Markov sequence
{st} with state space {1, . . . , K} allows discrete changes in the model param-
eters. Such processes have received a lot of attention in recent years as they
provide an explanation of the high persistence in volatility (i.e., nearly unit root
process for the conditional variance) observed with single-regime GARCH mod-
els [see, e.g., Lamoureux and Lastrapes 1990]. Furthermore, the MS-GARCH
models allow for a quick change in the volatility level which leads to signiﬁcant
improvements in volatility forecasts, as shown by Dueker [1997], Klaassen [2002],
Marcucci [2005].
Following the seminal work of Hamilton and Susmel [1994], diﬀerent para-
metrizations have been proposed to account for discrete changes in the GARCH
parameters [see, e.g., Dueker 1997, Gray 1996, Klaassen 2002]. However, these
parametrizations for the conditional variance process lead to computational dif-
ﬁculties. Indeed, the evaluation of the likelihood function for a sample of length
T requires the integration over all KT possible paths, rendering the estima-
tion infeasible. As a remedy, approximation schemes have been proposed to
shorten the dependence on the state variable’s history. While this diﬃculty is
not present in ARCH type models, lower order GARCH speciﬁcation of the con-
ditional variance oﬀers a more parsimonious representation than higher order
ARCH models.

110
7 MS-GJR(1, 1) Model with Student-t Innovations
In order to avoid any diﬃculties related to the past inﬁnite history of the
state variable, we adopt a recent parametrization due to Haas et al. [2004]. In
their model, the authors hypothesize K separate GARCH(1, 1) processes for the
conditional variance of the MS-GARCH process {yt}. The conditional variances
at time t can be written in vector form as follows:






h1
t
h2
t
...
hK
t






.=






α1
0
α2
0
...
αK
0






+






α1
1
α2
1
...
αK
1






y2
t−1 +






β1
β2
...
βK






⊙






h1
t−1
h2
t−1
...
hK
t−1






(7.1)
where ⊙denotes the Hadamard product, i.e., element-by-element multiplication.
The MS-GARCH process {yt} is then simply obtained by setting:
yt = εt(hst
t )1/2
where εt is an error term with zero mean and unit variance. The parameters αk
0,
αk
1 and βk are therefore the GARCH(1, 1) parameters related to the kth state of
the nature. Under this speciﬁcation, the conditional variance is solely a function
of the past data and current state st, which avoids the problem of inﬁnite history.
In the context of the Bayesian estimation, this allows to simulate the state
process in a multi-move manner which enhances the sampler’s eﬃciency.
In addition to its appealing computational aspects, the MS-GARCH model
of Haas et al. [2004] has conceptual advantages. In eﬀect, one reason for spec-
ifying Markov-switching models that allow for diﬀerent GARCH behavior in
each regime is to capture the diﬀerence in the variance dynamics in low- and
high-volatility periods. As pointed out by Haas et al. [2004, p.498]:
(...) “a relatively large value of αk
1 and relatively low values of βk in
high-volatility regimes may indicate a tendency to over-react to news,
compared to regular periods, while there is less memory in these sub-
processes. Such an interpretation requires a parametrization of Markov-
switching GARCH models that implies a clear association between the
GARCH parameters within regime k, that is αk
0, αk
1 and βk and the
corresponding {hk
t } process.”
The speciﬁcation of the conditional variance in equation (7.1) allows for a clear-
cut interpretation of the variance dynamics in each regime. Moreover, Haas
et al. [2004] show that results on the single-regime GARCH(1, 1) model can be
extended to their speciﬁcation; in particular, they derive explicit formulae for
the covariance stationarity condition, the unconditional variance as well as the
dependence structure of the squared process {y2
t }.

7.1 The model and the priors
111
To account for additional stylized facts observed in ﬁnancial time series, es-
pecially for stock indices (see Chap. 4), we will consider an asymmetric exten-
sion of (7.1) in which the GARCH(1, 1) processes are replaced by GJR(1, 1)
processes. More precisely, in this Markov-switching GJR model (henceforth
MS-GJR), the conditional variances at time t can be written in vector form
as follows:






h1
t
h2
t
...
hK
t






.=






α1
0
α2
0
...
αK
0






+








α1
1
α2
1
...
αK
1






I{yt−1⩾0} +






α1
2
α2
2
...
αK
2






I{yt−1<0}


y2
t−1
+






β1
β2
...
βK






⊙






h1
t−1
h2
t−1
...
hK
t−1






(7.2)
where I{•} denotes the indicator function. In this setting, the conditional vari-
ance in every regime can react asymmetrically depending on the sign of the
past shocks due to the introduction of dummy variables. The leverage eﬀect is
present for a given state k as soon as αk
2 > αk
1. An interesting feature of the
parametrization (7.2) lies in the fact that we can estimate whether the response
to past negative shock on the conditional variance is diﬀerent across regimes.
The plan of this chapter is as follows. We set up the model in Sect. 7.1.
The MCMC scheme is detailed in Sect. 7.2. The MS-GJR model as well as a
single-regime GJR model are applied to the Swiss Market Index log-returns in
Sect. 7.3. In Sect. 7.4, we test the models for misspeciﬁcation by using the
generalized residuals and assess the goodness-of-ﬁt through the calculation of
the Deviance information criterion and the model likelihoods. In Sect. 7.5, we
test the predictive performance of the models by running a forecasting analysis
based on the VaR. In Sect. 7.6, we propose a methodology to depict the one-
day ahead VaR density and document how speciﬁc forecasters’ risk perspectives
can lead to diﬀerent conclusions in terms of the forecasting performance of the
model. We conclude with some comments regarding the ML estimation of the
MS-GJR model in Sect. 7.7.
7.1 The model and the priors
A Markov-switching GJR(1, 1) model with Student-t innovations may be written
as follows:

112
7 MS-GJR(1, 1) Model with Student-t Innovations
yt = εt(ϱht)1/2
for t = 1, . . . , T
εt
iid
∼S(0, 1, ν)
ϱ .= ν −2
ν
ht .= e′
tht
(7.3)
where et is a K×1 vector deﬁned by et .=
 I{st=1} · · · I{st=K}
′, I{•} is the indica-
tor function; the sequence {st} is assumed to be a stationary, irreducible Markov
process with discrete state space {1, . . . , K} and transition matrix P
.= [Pij]
where Pij
.= P(st+1 = j | st = i); S(0, 1, ν) denotes the standard Student-t
density with ν degrees of freedom and ϱ is a scaling factor which ensures that
the conditional variance is given by ht. Moreover, we deﬁne the K × 1 vector of
GJR(1, 1) conditional variances in a compact form as follows:
ht .= α0 + (α1I{yt−1⩾0} + α2I{yt−1<0})y2
t−1 + β ⊙ht−1
where ht .= (h1
t · · · hK
t )′, αj .= (α1
j · · · αK
j )′ for j = 0, 1, 2 and β .= (β1 · · · βK)′.
In addition, we require that α0 > 0, α1 ⩾0, α2 ⩾0 and β ⩾0, where 0
is a K × 1 vector of zeros, in order to ensure the positivity of the conditional
variance in every regime and set h0 .= 0 and y0 .= 0 for convenience.
The use of a Student-t instead of a Normal distribution is quite popu-
lar in standard single-regime GARCH literature. For regime-switching mod-
els, a Student-t distribution might be seen as superﬂuous since the switching
regime can account for large unconditional kurtosis in the data [see, e.g., Haas
et al. 2004]. However, as empirically observed by Klaassen [2002], allowing for
Student-t innovations within regimes can enhance the stability of the states and
allows to focus on the conditional variance’s behavior instead of capturing some
outliers. Moreover, the Student-t distribution includes the Normal distribution
as the limiting case where the degrees of freedom parameter goes to inﬁnity. We
have therefore an additional ﬂexibility in the modeling and can impose Normal-
ity by constraining the lower boundary for the degrees of freedom parameter
through the prior distribution.
As pointed out in Sect. 5.1, the Student-t speciﬁcation (7.3) needs to be
re-expressed to perform a convenient Bayesian estimation. This is achieved as
follows:
yt = εt(ϖtϱht)1/2
for t = 1, . . . , T
εt
iid
∼N(0, 1)
ϖt
iid
∼IG
ν
2, ν
2


7.1 The model and the priors
113
where N(0, 1) is the standard Normal and IG denotes the Inverted Gamma
density. The degrees of freedom parameter ν characterizes the density of ϖt as
follows:
p(ϖt | ν) =
ν
2
 ν
2 h
Γ
ν
2
i−1
ϖ
−ν
2 −1
t
exp

−ν
2ϖt

.
(7.4)
For a parsimonious expression of the likelihood function, we deﬁne the T ×1
vectors y .= (y1 · · · yT )′, ϖ .= (ϖ1 · · · ϖT )′ as well as s .= (s1 · · · sT )′ and regroup
the ARCH parameters into the 3K × 1 vector α .= (α′
0 α′
1 α′
2)′. The model pa-
rameters are then regrouped into the augmented set of parameters Θ .= (ψ, ϖ, s)
where ψ .= (α, β, ν, P). Finally, we deﬁne the T × T diagonal matrix:
Σ .= Σ(Θ) = diag
 {ϖtϱ e′
tht}T
t=1

where we recall that ϱ, et and ht are both functions of the model parameters,
respectively given by:
ϱ(ν) .= ν −2
ν
et(st) .=
 I{st=1} · · · I{st=K}
′
and:
ht(α, β) .= α0 + (α1I{yt−1⩾0} + α2I{yt−1<0})y2
t−1 + β ⊙ht−1(α, β) .
We can now express the likelihood function of Θ as follows:
L(Θ | y) ∝(det Σ)−1/2 exp

−1
2y′Σ−1y

.
(7.5)
In the Bayesian approach, the vector of hidden states is considered as a param-
eter as implied by expression (7.5).
The likelihood function (7.5) is invariant with respect to relabeling the states
(i.e., the labeling of the states can be interchanged without aﬀecting the like-
lihood value), which leads to a lack of identiﬁcation of the state-speciﬁc pa-
rameters. So, without a prior inequality restriction on some state-speciﬁc pa-
rameters, a multimodal posterior is obtained and is diﬃcult to interpret and
summarize. To overcome this problem, we make use of the permutation sampler
of Fr¨uhwirth-Schnatter [2001b] to ﬁnd suitable identiﬁcation constraints. The
permutation sampler requires priors that are labeling invariant. Furthermore,
we cannot be completely non-informative about the state speciﬁc parameters
since, from a theoretical viewpoint, this would result in improper posteriors [see

114
7 MS-GJR(1, 1) Model with Student-t Innovations
Diebolt and Robert 1994]. These points have therefore to be taken into account
when choosing the prior densities.
Conditionally on the K ×K transition probabilities matrix P .= [Pij] where:
Pij .= P(st+1 = j | st = i)
the prior on vector s is Markov:
p(s | P) = π(s1)
K
Y
i=1
K
Y
j=1
P Nij
ij
where Nij .= #{st+1 = j | st = i} is the number of one-step transitions from
state i to j in the T × 1 vector s. The mass function for the initial state, π(s1),
is obtained by calculating the ergodic probabilities of the Markov chain. The
vector of ergodic probabilities can be obtained as the sum of the columns of
matrix (A′A)−1, where the matrix A is deﬁned as follows:
A .=
 
IK −P ′
ι′
K
!
where IK is a K×K identity matrix and ιK a K×1 vector of ones [see Hamilton
1994, Sect.22.2].
The prior density for the K ×K transition matrix P is obtained by assuming
that the K rows are independent and that the density of the ith row is Dirichlet
with parameter ηi
.= (ηi1 · · · ηiK):
p(P) =
K
Y
i=1
D(ηi)
∝
K
Y
i=1
K
Y
j=1
P ηij−1
ij
.
Due to the labeling invariance assumption, we require that ηii
.= ηp for
i = 1, . . . , K and ηij
.= ηq for i, j ∈{1, . . . , K; i ̸= j}. A prior density with
ηp > ηq could be used to model the belief that the probability of persistence is
bigger than the probability of transition.
For the scedastic function’s parameters α and β, we use truncated Normal
densities:
p(α) ∝N3K(α | µα, Σα)I{α>0}
p(β) ∝NK(β | µβ, Σβ)I{β>0}

7.2 Simulating the joint posterior
115
where we recall that µ• and Σ• are the hyperparameters, 0 is a vector of ze-
ros of appropriate size and Nd is the d-dimensional Normal density (d > 1).
The assumption of labeling invariance is fulﬁlled if we assume further that the
hyperparameters are the same for all states. In particular, we set:
[µα]i
.= µα0 ,
[Σα]ii
.= σ2
α0
,

µβ

i
.= µβ
,
[Σβ]ii
.= σ2
β
for i = 1, . . . , K, and:
[µα]i
.= µα1
,
[Σα]ii
.= σ2
α1
for i = K + 1, . . . , 2K, and:
[µα]i
.= µα2
,
[Σα]ii
.= σ2
α2
for i = 2K + 1, . . . , 3K, where µαj, σ2
αj (j = 0, 1, 2), and µβ, σ2
β are ﬁxed
hyperparameters. We note that matrices Σα and Σβ are diagonal in this case.
The prior density of the T × 1 vector ϖ conditional on ν is found by noting
that ϖt are independent and identically distributed from (7.4), which yields:
p(ϖ | ν) =
ν
2
 T ν
2 h
Γ
ν
2
i−T
 T
Y
t=1
ϖt
!−ν
2 −1
exp
"
−1
2
T
X
t=1
ν
ϖt
#
.
Following Deschamps [2006], we choose a translated Exponential with pa-
rameters λ > 0 and δ ⩾2 for the degrees of freedom parameter:
p(ν) = λ exp[−λ(ν −δ)]I{δ<ν<∞} .
Finally, we form the joint prior by assuming prior independence between α,
β, (ϖ, ν) and (s, P) as follows:
p(Θ) = p(α)p(β)p(ϖ | ν)p(ν)p(s | P)p(P)
and by combining the likelihood function (7.5) with the joint prior above, we
obtain the posterior density via Bayes’ rule:
p(Θ | y) ∝L(Θ | y)p(Θ) .
(7.6)
7.2 Simulating the joint posterior
We draw an initial value:

116
7 MS-GJR(1, 1) Model with Student-t Innovations
Θ[0] .= (α[0], β[0], ϖ[0], ν[0], s[0], P [0])
from the joint prior and we generate iteratively J passes for Θ. A single pass is
decomposed as follows:
s[j] ∼p(s | α[j−1], β[j−1], ϖ[j−1], ν[j−1], P [j−1], y)
P [j] ∼p(P | s[j])
α[j] ∼p(α | β[j−1], ϖ[j−1], ν[j−1], s[j], y)
β[j] ∼p(β | α[j], ϖ[j−1], ν[j−1], s[j], y)
ϖ[j] ∼p(ϖ | α[j], β[j], ν[j−1], s[j], y)
ν[j] ∼p(ν | ϖ[j]) .
(7.7)
In (7.7), only ϖ and P can be simulated from known expressions. Draws of α
and β are achieved by a multivariate extension of the methodology proposed by
Nakatsuma [1998, 2000]. The generation of state vector s is made by using the
Forward Filtering Backward Sampling (henceforth FFBS) algorithm described
in Chib [1996]. Finally, sampling ν is achieved by an eﬃcient rejection technique.
As pointed out previously, the likelihood function and the joint prior are
labeling invariant. Consequently, the joint posterior density in (7.6) will also be
invariant and hence exhibit, at least theoretically, K! diﬀerent modes. Therefore,
it is important to carefully select constraints to identify the model. In eﬀect, a
constraint that ignores the geometry of the posterior density will not lead to
a unique labeling and can introduce a bias toward the constraint, as shown in
Fr¨uhwirth-Schnatter [2001b]. If a suitable identifying restriction is not available
or is not known a priori, an elegant solution to determine these constraints
is to use the random permutation sampler proposed by Fr¨uhwirth-Schnatter
[2001b]. In this version of the permutation sampler, each pass of the MCMC
scheme is followed by a random permutation of the regime deﬁnitions. Formally,
a random permutation {Π1, . . . ΠK} of {1, . . . , K} is selected with probability
1
K!. Then, for i, j ∈{1, . . . , K}, the element (i, j) of P is replaced by the element
with indices (Πi, Πj). The hidden states process {st} is substituted by {Πst}.
Finally, for k = 1, . . . , K, parameter αk
0 is replaced by αΠk
0 , parameter αk
1 by
αΠk
1 , parameter αk
2 by αΠk
2
and parameter βk by βΠk. Hence, relabeling only
aﬀects the scedastic function’s parameters, the state process and the transition
probabilities while the vector ϖ and the degrees of freedom parameter ν remain
unchanged.
The random permutation sampler by Fr¨uhwirth-Schnatter [2001b] is used to
improve the mixing of the MCMC sampler and to explore the full unconstrained
parameter space. Then post-processing the MCMC output of the random per-

7.2 Simulating the joint posterior
117
mutation sampler in an exploratory way, by plotting scatter plots for instance,
can suggest an appropriate identiﬁcation constraint, such as:
β1 < . . . < βK
(7.8)
meaning, in this particular case, that the MS-GJR model can be identiﬁed
through inequalities on the parameter β between regimes. At this stage, the
model is estimated again under the constraint (7.8) by enforcing the corre-
sponding permutation of the regimes. This version of the permutation sampler
is referred to as the constrained permutation sampler. At each sweep of the
sampler, we test whether the constraint is fulﬁlled. If not, we order the pairs
{1, β1}, . . . , {K, βK} with respect to the second component. The ﬁrst component
{Π1, . . . , ΠK} of the ordered pairs deﬁnes the correct permutation of reordering
the state parameters and this permutation is applied to the state-speciﬁc com-
ponents, as this was done for the random permutation sampler. If the model is
identiﬁable up to permutations of the states and satisﬁes certain regularity con-
ditions, the constrained posterior density will exhibit a single mode. Note that
the selection of the constraint (7.8) is arbitrary, because there exist K! diﬀerent
ways of formulating constraints which render the model identiﬁed, namely:
βΠ1 < . . . < βΠK
for all permutations {Π1, . . . , ΠK} of {1, . . . , K}. At this stage, if label switch-
ing still occurs, this might indicate that the inequality restriction (7.8) is not
well suited or that the number K of chosen regimes is too large [see Fr¨uhwirth-
Schnatter 2006, Sect.4.2]. We will now present the derivation for the full condi-
tionals appearing in the MCMC scheme (7.7).
7.2.1 Generating vector s
The generation of posterior samples for the T × 1 vector s is carried out in
a multi-move manner by using the FFBS algorithm. We refer the reader to
Chib [1996] and Fr¨uhwirth-Schnatter [2006, Chap.11] for a detailed presenta-
tion of this procedure. We mention however that the FFBS approach can be
used since the conditional density of yt only depends on the current regime
which is a consequence of the deﬁnition for the conditional variance ht .= e′
tht.
Other speciﬁcations for the conditional variance in Gray [1996] or Klaassen
[2002] for instance, do not allow for such an approach, as noted in Kaufmann
and Fr¨uhwirth-Schnatter [2002, Sect.6.3]. The application of the FFBS algo-
rithm has the potential advantage that the states are updated as a single block,

118
7 MS-GJR(1, 1) Model with Student-t Innovations
which avoids superﬂuous correlation in the vector’s components, and therefore
enhances the sampler’s eﬃciency [see Fr¨uhwirth-Schnatter 2006, Sect.11.5.6].
7.2.2 Generating matrix P
The full conditional density of the transition matrix can be derived without
regard to the sampling model since P becomes independent of Θ and y given
the vector of states. Indeed, the posterior density is obtained as follows:
p(P | s) ∝p(s | P)p(P)
∝

π(s1)
K
Y
i=1
K
Y
j=1
P Nij
ij

×


K
Y
i=1
K
Y
j=1
P ηij−1
ij


∝
K
Y
i=1
K
Y
j=1
P Nij+ηij−1
ij
∝
K
Y
i=1
D(bηi)
(7.9)
where bηi
.= (Ni1 +ηi1 · · · NiK +ηiK) and Nij .= #{st+1 = j | st = i} is the total
number of one-step transitions from state i to state j in the vector s. The rows
of matrix P are independent a posteriori and the ith row follows a Dirichlet
density with parameter bηi.
7.2.3 Generating the GJR parameters
The methodology used to draw vectors α and β can be viewed as a multivariate
extension of the approach proposed in Chap. 4 for the single-regime GJR model.
Let us consider the following K × 1 vector:
wt .= y2
t ιK
τt
−ht
where we deﬁne τt .= ϖtρ for convenience and recall that ιK is a K ×1 vector of
ones. In order to simplify the notations further, we deﬁne vt .= y2
t
τt which yields
wt = vtιK −ht. From there, we can transform the expression for the vector of
conditional variances as follows:

7.2 Simulating the joint posterior
119
ht = α0 + (α1I{yt−1⩾0} + α2I{yt−1<0})y2
t−1 + β ⊙ht−1
⇔(vtιK −wt) = α0 + (α1I{yt−1⩾0} + α2I{yt−1<0})y2
t−1
+ β ⊙(vt−1ιK −wt−1)
⇔vtιK = α0 +

τt−1(α1I{yt−1⩾0} + α2I{yt−1<0}) + β

⊙vt−1ιK
−β ⊙wt−1 + wt
⇔wt = vtιK −α0 −

τt−1(α1I{yt−1⩾0} + α2I{yt−1<0}) + β

⊙vt−1ιK
+ β ⊙wt−1 .
Moreover, let us deﬁne wt .= e′
twt and note that wt can be written as follows:
wt .= e′
twt
= vt −ht =

y2
t
ϖtϱht
−1

ht
=
 χ2
1 −1

ht
where χ2
1 denotes a Chi-squared variable with one degree of freedom. This comes
from the fact that the conditional distribution of yt is Normal with zero mean
and variance ϖtϱht. Therefore, the conditional mean of wt is zero and the con-
ditional variance is 2h2
t. As in the single-regime GJR model, this variable can
be approximated by zt, a Normal variable with a mean of zero and a variance of
2h2
t. The variable zt can be further expressed as zt .= e′
tzt where zt is a function
of vectors α and β given by:
zt(α, β) = vtιK −α0 −

τt−1(α1I{yt−1⩾0} + α2I{yt−1<0}) + β

⊙vt−1ιK
+ β ⊙zt−1(α, β) .
(7.10)
Then, we construct the T × 1 vector z .= (z1 · · · zT )′ where zt .= e′
tzt as well as
the T × T diagonal matrix:
Λ .= Λ(α, β) = diag
 {2e′
th2
t(α, β)}T
t=1

and express the approximate likelihood function of (α, β) as follows:
L(α, β | ϖ, ν, s, y) ∝(det Λ)−1/2 exp

−1
2z′Λ−1z

.
(7.11)
As will be shown hereafter, the construction of the proposal densities for pa-
rameters α and β is based on this likelihood function.

120
7 MS-GJR(1, 1) Model with Student-t Innovations
Generating vector α
First, we note that the function zt(α, β) in (7.10) can be expressed as a lin-
ear function of vector α. To show this, we simply extend the argument of the
single-regime GJR model by using appropriate recursive transformations. More
precisely, the ith component of the K × 1 vector zt can be written as follows:
[zt]i = vt −
 l∗
t (βi) v∗
t (βi) v∗∗
t (βi)




αi
0
αi
1
αi
2



with the recursive transformations l∗
t , v∗
t and v∗∗
t
given by:
l∗
t (βi) .= 1 + βil∗
t−1(βi)
v∗
t (βi) .= y2
t−1I{yt−1⩾0} + βiv∗
t−1(βi)
v∗∗
t (βi) .= y2
t−1I{yt−1<0} + βiv∗∗
t−1(βi)
(7.12)
where l∗
0 = v∗
0 = v∗∗
0
.= 0. We notice that l∗
t (•), v∗
t (•) and v∗∗
t (•) in (7.12) are
similar to the recursive transformations used for the single-regime GJR model.
Let us now regroup the recursive values into a K × 3K matrix Ct as follows:
Ct .=





l∗
t (β1)
0
···
0
v∗
t (β1)
0
···
0
v∗∗
t (β1)
0
···
0
0
l∗
t (β2)
0
...
0
v∗
t (β2)
0
...
0
v∗∗
t (β2)
0
...
...
0
...
0
...
0
...
0
...
0
...
0
0
···
0
l∗
t (βK)
0
···
0
v∗
t (βK)
0
···
0
v∗∗
t (βK)




.
It is straightforward to show that zt = vtιK −Ctα, and since zt .= e′
tzt we
get zt = vt −e′
tCtα. Then, by deﬁning the T × 1 vectors z .= (z1 · · · zT )′ and
v .= (v1 · · · vT )′ as well as the T × 3K matrix C whose tth row is e′
tCt, we
end up with z = v −Cα which is the desired linear expression for z. The
proposal density to sample vector α is obtained by combining the approximate
likelihood (7.11) and the prior density by Bayes’ update:
qα(α | eα, β, ϖ, ν, s, y) ∝N3K(α | bµα, bΣα)I{α>0}
(7.13)
with:
bΣ−1
α
.= C′eΛ−1C + Σ−1
α
bµα
.= bΣα(C′eΛ−1v + Σ−1
α µα)

7.2 Simulating the joint posterior
121
where the T × T diagonal matrix eΛ .= diag
 {2e′
th2
t(eα, β)}T
t=1

and eα is the
previous draw of α in the M-H sampler. A candidate α⋆is sampled from this
proposal density and accepted with probability:
min
p(α⋆, β, ϖ, ν, s, P | y)
p(eα, β, ϖ, ν, s, P | y)
qα(eα | α⋆, β, ϖ, ν, s, y)
qα(α⋆| eα, β, ϖ, ν, s, y), 1

.
Generating vector β
The function zt(α, β) in (7.10) could be expressed, in the previous section, as
a linear function of α but cannot be expressed as a linear function of vector β.
To overcome this problem, we linearize the K × 1 vector zt(β) by a ﬁrst order
Taylor expansion at point eβ:
zt(β) ≃zt(eβ) + dzt
dβ′

β=eβ
× (β −eβ)
where eβ is the previous draw of β in the M-H sampler. Furthermore, let us
deﬁne the following:
rt .= zt(eβ) + Gteβ
,
Gt .= −dzt
dβ′

β=eβ
(7.14)
where the K × K matrix Gt can be computed by the following recursion:
Gt .= vt−1IK −Zt−1 + Gt−1eβ
where Zt−1 is a K × K diagonal matrix with zt−1(eβ) in its diagonal, IK is a
K × K identity matrix and G0 is a K × K matrix of zeros. This recursion is
simply obtained by diﬀerentiating (7.10) with respect to β. From the deﬁnitions
in (7.14) we get zt ≃rt −Gtβ and the approximation for zt is obtained as
zt ≃rt−e′
tGtβ where rt .= e′
trt. Let us now deﬁne the T×1 vector r .= (r1 · · · rT )′
as well as the T ×K matrix G whose tth row is e′
tGt. It turns out that z ≃r−Gβ,
thus we can approximate the exponential of the approximate likelihood (7.11)
with:
exp

−1
2(r −Gβ)′Λ−1(r −Gβ)

.
The proposal density to sample vector β is obtained by combining this approx-
imation with the prior density by Bayes’ update:
qβ(β | α, eβ, ϖ, ν, s, y) ∝NK(β | bµβ, bΣβ)I{β>0}
(7.15)
with:

122
7 MS-GJR(1, 1) Model with Student-t Innovations
bΣ−1
β
.= G′eΛ−1G + Σ−1
β
bµβ .= bΣβ(G′eΛ−1r + Σ−1
β µβ)
where the T × T diagonal matrix eΛ .= diag
 {2e′
th2
t(α, eβ)}T
t=1

. A candidate β⋆
is sampled from this proposal density and accepted with probability:
min
(
p(α, β⋆, ϖ, ν, s | y)
p(α, eβ, ϖ, ν, s | y)
qβ(eβ | α, β⋆, ϖ, ν, s, y)
qβ(β⋆| α, eβ, ϖ, ν, s, y)
, 1
)
.
7.2.4 Generating vector ϖ
The components of ϖ are independent a posteriori and the full conditional
posterior of ϖt is obtained as follows:
p(ϖt | α, β, ν, s, y) ∝L(Θ | y)p(ϖt | ν)
∝ϖ
−(ν+3)
2
t
exp

−bt
ϖt

(7.16)
with:
bt .= 1
2
 y2
t
ϱht
+ ν

where we recall that ht .= e′
tht(α, β) and ϱ .=
ν−2
ν . Expression (7.16) is the
kernel of an Inverted Gamma density with parameters ν+1
2
and bt.
7.2.5 Generating parameter ν
Draws from p(ν | ϖ) are made by optimized rejection sampling from a trans-
lated Exponential source density. This is achieved by following the lines of
Sect. 5.2.4.
Finally, we note that the computer code and the correctness of the algorithm
are tested as in previous chapters; the testing methodology is applicable to the
constrained as well as unconstrained versions of the permutation sampler.
7.3 An application to the Swiss Market Index
We apply our Bayesian estimation method to demeaned daily log-returns {yt} of
the Swiss Market Index (henceforth SMI). The sample period is from November
12, 1990, to December 16, 2005 for a total of 3’800 observations and the log-
returns are expressed in percent. The data set is freely available from the website

7.3 An application to the Swiss Market Index
123
http://www.finance.yahoo.com. Note that September 11, 2001, has not been
recorded by the data provider since the stock markets closed after the terrorist
attacks for a few days. From this time series, the ﬁrst 2’500 observations (up
to November 2001), which represent slightly less than two third of the data set,
are used to estimate the model while the remaining 1’300 log-returns are used
in a forecasting performance analysis.
The time series under investigation is plotted in the upper part of Fig. 7.1
where the vertical line delimits the in- and out-of-sample observation windows.
We test for autocorrelation in the times series by testing the joint nullity of au-
toregressive coeﬃcients for {yt}. We estimate the regression with autoregressive
coeﬃcients up to lag 15 and compute the covariance matrix using the White
estimate. The p-value of the Wald test is 0.5299 which does not support the
presence of autocorrelation. When testing for the autocorrelation in the series
of squared observations {y2
t }, we strongly reject the absence of autocorrelation.
This is in line with the autocorrelogram of {y2
t } plotted in the lower part of
Fig. 7.1. The autocorrelations are large and signiﬁcantly diﬀerent from zero up
to lag 70. As an additional data analysis, we test for unit root using the test by
Phillips and Perron [1988]. The test strongly rejects the I(1) hypothesis.
We estimate the single-regime GJR(1, 1) model as well as the two-state
Markov-switching GJR(1, 1) model henceforth referred to as GJR and MS-GJR
for convenience. Both models are estimated using the MCMC scheme presented
in Sect. 7.2. The estimation of the GJR model is obtained as a simpliﬁed ver-
sion of the algorithm when K = 1 by setting the T ×1 vector s to a vector of ones
and omitting the generation of the transition matrix. For the hyperparameters
on priors p(α) and p(β), we set µαi(i = 0, 1, 2) and µβ to zero mean vectors and
choose diagonal covariance matrices for Σαi(i = 0, 1, 2) and Σβ. The variances
are set to σ2
αi = σ2
β = 10’000 (i = 0, 1, 2) so we do not introduce tight prior
information in our estimation. In the case of the prior on the degrees of freedom
parameter, we choose λ = 0.01 and δ = 2; this therefore ensures the existence
for the conditional variance. Finally, the hyperparameters for the prior on the
transition probabilities are set to ηii = 2 and ηij = ηji = 1 for i, j ∈{1, 2} so
that we have a prior belief that the probabilities of persistence are bigger than
the probabilities of transition.
For both models, we run two chains for 50’000 iterations each and assess the
convergence of the sampler by using the diagnostic test by Gelman and Rubin
[1992]. The convergence appears rather quickly, but we nevertheless consider the
ﬁrst half of the iterations as a burn-in phase for precaution. For the GJR model,
the acceptance rates range from 88% for vector α to 97% for β indicating that
the proposal densities are close to the exact conditional posteriors. The one-

124
7 MS-GJR(1, 1) Model with Student-t Innovations
−7.5
−5.0
−2.5
0.0
2.5
5.0
7.5
1991
1993
1995
1997
1999
2001
2003
2005
Daily log−returns
(in percent)
year
0
20
40
60
80
100
0.0
0.1
0.2
0.3
0.4
Sample autocorrelogram
lag
Fig. 7.1. SMI daily log-returns (upper graph) and sample autocorrelogram of the
squared log-returns up to lag 100 (lower graph). The vertical line in the upper graph
delimits the in-sample and out-of-sample observation windows.
lag autocorrelations in the chain range from 0.52 for α1 to 0.96 for β which is
reasonable. For the MS-GJR model, the random permutation sampler is run ﬁrst
to determine suitable identiﬁcation constraints. In Fig. 7.2, we show the contour

7.3 An application to the Swiss Market Index
125
plots of the posterior density for (βk, αk
0), (βk, αk
1) and (βk, αk
2), respectively.
Note that the state value k is arbitrary since all marginal densities contain
the same information [see Fr¨uhwirth-Schnatter 2001b]. As we can notice, the
bimodality of the posterior density is clear for the parameter βk on the three
graphs, suggesting a constraint of the type β1 < β2 for identiﬁcation. Therefore,
the model is estimated again by imposing this constraint at each sweep in the
sampler and the deﬁnition of the states is permuted if the constraint is violated.
In that case, label switching only appeared 16 times after the burn-in phase thus
conﬁrming the suitability of the identiﬁcation constraint. The acceptance rates
obtained with the constrained version of the permutation sampler range from
22% for the vector α to 93% for β. The one-lag autocorrelations range from 0.82
for α2
1 to 0.97 for β2. We keep every ﬁfth draw from the MCMC output for both
models in order to diminish the autocorrelation in the chains. The two chains
are then merged to get a ﬁnal sample of length 10’000. Finally, we note that
a three-state Markov-switching GJR model has also been estimated. However,
post-processing the MCMC output has not allowed to ﬁnd a clear identiﬁcation
constraint.
The posterior statistics for both models are reported in Table 7.1. In the
case of the GJR model (upper panel), we note the high persistence for the con-
ditional variance process, measured by α + β where α .= α1+α2
2
, as well as the
presence of the leverage eﬀect. The estimation of the probability P(α2 > α1 | y)
is 0.999, supporting the asymmetric behavior of the conditional variance. The
low value for the estimated degrees of freedom parameter indicates conditional
leptokurtosis in the data set. In the MS-GJR case (lower panel), we note also
the presence of the leverage eﬀect in both states. A comparison of the scedastic
function’s parameters between regimes indicates similar 95% conﬁdence intervals
for the components of the vectors α1 and α2 while the diﬀerence for compo-
nents of the α0 vector is more pronounced. Indeed, for i = 0, 1, 2, the estimated
probabilities P(α1
i > α2
i | y) are respectively 0.774, 0.397 and 0.543. As in the
single-regime model, the posterior density for the degrees of freedom parameter
indicates conditional leptokurtosis. We note however that the posterior mean
and median are larger than for the GJR model. The posterior means for proba-
bilities p11 and p22 are respectively 0.997 and 0.995 indicating infrequent mixing
between states. Finally, the ineﬃciency factors (IF) reported in the last column
of Table 7.1 indicate that using 10’000 draws out of the MCMC sampler seems
appropriate if we require that the Monte Carlo error in estimating the mean
is smaller than one percent of the variation of the error due to the data. We
recall that the IF are computed as the ratio of the squared numerical standard
error (NSE) of the MCMC simulations and the variance estimate divided by the

126
7 MS-GJR(1, 1) Model with Student-t Innovations
number of iterations (i.e., the variance of the sample mean from a hypothetical
iid sampler). The NSE are estimated by the method of Andrews [1991], using a
Parzen kernel and AR(1) pre-whitening as presented in Andrews and Monahan
[1992]. As noted by Deschamps [2006], this ensures easy, optimal, and automatic
bandwidth selection.
In Fig. 7.3, we display the marginal posterior densities for the MS-GJR
model parameters. First, we note that the use of the constrained permutation
sampler leads to marginal densities which are unimodal. Furthermore, we clearly
notice that most of these densities are skewed. More precisely, the densities for
the components of vector α are right-skewed while components of β are left-
skewed. In the case of parameters α1
1 and α2
1, the modes of the densities are close
to the lower boundary of the parameter’s space, suggesting that the parameters
are close to zero. Finally, we can notice that the posterior densities for p11 and
p22 are strongly left-skewed.
Some probabilistic statements on nonlinear functions of the parameters can
be straightforwardly obtained by simulation from the joint posterior sample
{ψ[j]}J
j=1. In particular, we can test the covariance stationarity condition and es-
timate the density of the unconditional variance when this condition is satisﬁed.
Under the GJR speciﬁcation, the process is covariance stationary if α + β < 1
where we recall that α .= α1+α2
2
for notational purposes. The estimated prob-
ability P(α + β < 1 | y) is one. Hence, the unconditional variance exists and
is given by
α0
1−α−β ; the estimation of its posterior mean is 1.179 with a 95%
conﬁdence interval given by [1.173,1.189]. These estimations can be compared
with the empirical variance of the process which is 1.136. In this case, the single-
regime model slightly overestimates the variability of the underlying time series.
For the Markov-switching model, our simulation study indicates that the process
is covariance stationary in each state. The posterior mean of the unconditional
variances is 0.56 in state 1 and 2.00 in state 2 with 95% conﬁdence intervals
respectively given by [0.557,0.563] and [1.992,2.012]. The unconditional vari-
ance of the process in state 1 is about four times lower than the one in state
2; we will therefore refer state 1 as the low-volatility regime and state 2 as the
high-volatility regime. As found by Haas et al. [2004, Eq.11, p.500], the Markov-
switching GARCH process is covariance stationary if ξ(M) < 1, where ξ(M)
denotes the largest eigenvalue in modulus of matrix M. This matrix is con-
structed from the model parameters and, in the case of the MS-GJR model, it
is given by:

7.3 An application to the Swiss Market Index
127
Table 7.1. Estimation results for the GJR model (upper panel) and
MS-GJR model (lower panel).⋆
GJR model
ψ
ψ
ψ0.5
ψ0.025
ψ0.975
min
max
NSE
IF
α0
0.066
0.065
0.041
0.099
0.021
0.156
0.356
5.58
α1
0.060
0.059
0.028
0.098
0.005
0.162
0.237
1.81
α2
0.207
0.205
0.148
0.278
0.097
0.359
0.690
4.33
β
0.809
0.809
0.750
0.861
0.656
0.911
1.163
16.22
ν
8.083
7.954
6.258
10.580
4.871
13.930
34.643
9.79
MS-GJR model
ψ
ψ
ψ0.5
ψ0.025
ψ0.975
min
max
NSE
IF
α1
0
0.245
0.241
0.149
0.362
0.100
0.518
2.407
19.26
α2
0
0.184
0.178
0.089
0.327
0.046
0.518
1.939
10.45
α1
1
0.020
0.015
0.001
0.063
0.000
0.145
0.276
2.61
α2
1
0.027
0.023
0.001
0.073
0.000
0.135
0.302
2.33
α1
2
0.229
0.224
0.123
0.361
0.074
0.534
1.278
4.21
α2
2
0.220
0.215
0.136
0.332
0.090
0.462
1.140
5.21
β1
0.436
0.440
0.212
0.642
0.004
0.746
4.454
16.80
β2
0.782
0.785
0.670
0.866
0.582
0.907
2.090
18.33
ν
9.459
9.264
7.051
12.880
5.881
23.740
55.931
13.45
p11
0.997
0.997
0.992
0.999
0.982
1.000
0.022
1.23
p12
0.003
0.003
0.001
0.008
0.001
0.018
0.022
1.23
p21
0.005
0.004
0.001
0.011
0.001
0.023
0.027
1.13
p22
0.995
0.996
0.989
0.999
0.978
1.000
0.027
1.13
⋆ψ: posterior mean; ψφ: estimated posterior quantile at probability φ;
min: minimum value; max: maximum value; NSE: numerical standard
error (×103); IF: ineﬃciency factor (i.e., ratio of the squared numerical
standard error and the variance of the sample mean from a hypotheti-
cal iid sampler). The posterior statistics are based on 10’000 draws from
the constrained posterior sample.
M .=





p11(α1 + β1)
0
p21(α1 + β1)
0
p11α2
1
p11β2
p21α2
1
p21β2
p12β1
p12α1
1
p22β1
p22α1
1
0
p12(α2 + β2)
0
p22(α2 + β2)





(7.17)
where αk .= αk
1+αk
2
2
. Using the posterior sample we can thus estimate the density
of ξ(M) by substituting the values of the draws for the model parameters in the
deﬁnition (7.17). In the upper part of Fig. 7.4, we present the posterior density
for ξ(M). As we can notice, none of the values exceed one in our simulation.
Thus, the model is covariance stationary. Therefore, the unconditional variance
of the MS-GJR process exists and is given by:

128
7 MS-GJR(1, 1) Model with Student-t Innovations
hy .= (vec P)′ × (I4 −M)−1 × (π ⊗α0)
(7.18)
where π is the 2 × 1 vector of ergodic probabilities of the Markov chain, I4
is a 4 × 4 identity matrix, vec denotes the vectorization operator which stacks
the columns of a matrix one underneath the other and ⊗denotes the Kro-
necker product. Derivation of formula (7.18) can be found in Haas et al. [2004,
p.501]. The posterior density of the unconditional variance is shown in the lower
part of Fig. 7.4. Its posterior mean is 1.134 with a 95% conﬁdence interval of
[1.128,1.139]. In this case, the conﬁdence band for the mean contains the em-
pirical variance of 1.136 contrary to the one in the GJR model. This suggests
that the Markov-switching model is more apt to reproduce the variability of the
data.
Finally, since the states vector s .= (s1 · · · sT )′ is considered as a parameter in
the MCMC procedure, the draws {s[j]}J
j=1 can also be stored and used to make
inference about the smoothed probabilities. Theses probabilities are estimated
as the percentage of replications of st corresponding to regime k:
P(st = k | y) ≈1
J
J
X
j=1
I{s[j]
t =k} .
In Fig. 7.5, we present the smoothed probabilities for the high-volatility regime
(solid line, left axis) together with the in-sample daily log-returns (circles, right
axis). The 95% conﬁdence bands are shown in dashed lines but are almost indis-
tinguishable from the point estimates. The beginning of year 1991 is associated
with the high-volatility state. Then, from the second half of 1991 to 1997, the
returns are clearly associated with the low-volatility regime, with the exception
of 1994. From 1997 to 2000, the model remains in the high-volatility regime with
a transition during the second semester 2000 to the low-volatility state.

7.3 An application to the Swiss Market Index
129
Parameter βk
Parameter α0
k
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
Parameter βk
Parameter α1
k
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
Parameter βk
Parameter α2
k
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Fig. 7.2. Contour plots for (βk, αk
0), (βk, αk
1) and (βk, αk
2), respectively. The choice of k is arbitrary since all marginal densities contain
the same information [see Fr¨uhwirth-Schnatter 2001b]. The graphs are based on 10’000 draws from the joint posterior sample.

130
7 MS-GJR(1, 1) Model with Student-t Innovations
Parameter α0
1
0.1
0.2
0.3
0.4
0.5
0
200
400
600
800
1000
1200
1400
Parameter α0
2
0.1
0.2
0.3
0.4
0.5
0
200
400
600
800
1000
1200
1400
Parameter α1
1
0.00
0.05
0.10
0.15
0
500
1000
1500
2000
2500
3000
3500
Parameter α1
2
0.00
0.04
0.08
0.12
0
200
400
600
800
1000
1200
Parameter α2
1
0.1
0.2
0.3
0.4
0.5
0
200
400
600
800
1000
1200
Parameter α2
2
0.1
0.2
0.3
0.4
0
500
1000
1500
Parameter β1
0.0
0.2
0.4
0.6
0
500
1000
1500
Parameter β2
0.60
0.70
0.80
0.90
0
500
1000
1500
Parameter ν
5
10
15
20
0
500
1000
1500
2000
2500
Parameter p11
0.985
0.990
0.995
1.000
0
500
1000
1500
2000
Parameter p12
0.000
0.005
0.010
0.015
0
500
1000
1500
2000
Parameter p21
0.000
0.010
0.020
0
500
1000
1500
Parameter p22
0.980
0.990
1.000
0
500
1000
1500
Fig. 7.3. Marginal posterior densities of the MS-GJR parameters. The histograms are based on 10’000 draws from the constrained posterior
sample.

7.3 An application to the Swiss Market Index
131
ξ(M)
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0
200
400
600
800
1000
1200
hy
0.5
1.0
1.5
2.0
2.5
3.0
0
500
1000
1500
Fig. 7.4. Posterior densities of the covariance stationarity condition (upper graph)
and the unconditional variance (lower graph) of the MS-GJR process. The histograms
are based on 10’000 draws from the constrained posterior sample.

132
7 MS-GJR(1, 1) Model with Student-t Innovations
0.00
0.25
0.50
0.75
1.00
−7.5
−5.0
−2.5
0.0
2.5
5.0
7.5
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
Pr(st=2 | y)
Daily log−returns
(in percent)
year
Smoothed probabilities
0.00
0.25
0.50
0.75
1.00
−7.5
−5.0
−2.5
0.0
2.5
5.0
7.5
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
Pr(st=2 | y)
Daily log−returns
(in percent)
year
Smoothed probabilities
Fig. 7.5. Smoothed probabilities of the high-volatility state (solid line, left axis) together with the in-sample log-returns (circles, right
axis). The 95% conﬁdence bands are shown in dashed lines (they are almost indistinguishable from the point estimates).

7.4 In-sample performance analysis
133
7.4 In-sample performance analysis
7.4.1 Model diagnostics
We check for model misspeciﬁcation by analyzing the predictive probabilities re-
ferred to as probability integral transforms or p-scores in the literature [see, e.g.,
Diebold, Gunther, and Tay 1998, Kaufmann and Fr¨uhwirth-Schnatter 2002]. We
make use of a simpler version of this method, as proposed by Kim, Shephard,
and Chib [1998], which consists in conditioning on point estimates of ψ. To
be meaningful, the point estimate has to be chosen when the identiﬁcation is
imposed. Hence, we consider the posterior mean ψ of the constrained poste-
rior sample. Upon deﬁning Ft−1 as the information set up to time t −1, the
(approximate) p-scores are deﬁned as follows:
zt .=
K
X
k=1
P(Yt ⩽yt | st = k, ψ, Ft−1) P(st = k | ψ, Ft−1) .
The probability P(Yt ⩽yt | st = k, ψ, Ft−1) can be estimated by the Student-t
integral and the ﬁltered probability P(st = k | ψ, Ft−1) is obtained as a byprod-
uct from the FFBS algorithm [see Chib 1996, p.83]. Under a correct speciﬁcation,
the p-scores should have independent uniform distributions asymptotically [see
Rosenblatt 1952]. A further transformation through the Normal integral is often
applied for convenience. In this case, we consider ut .= Φ−1(zt) where Φ−1(•)
denotes the inverse cumulative standard Normal function. If the model is cor-
rect, these generalized residuals {ut} should be independent standard Normal
and common tests can be used to check these features. In particular, we test the
presence of autocorrelation in the series {ut} and {u2
t} using a Wald test. We also
report the results of a joint test for zero mean, unit variance, zero skewness, and
the absence of excess kurtosis, employing the likelihood ratio framework pro-
posed by Berkowitz [2001]. For precisions on the testing methodology, we refer
the reader to Haas et al. [2004, p.516].
In the case of the GJR model, the Wald statistic for testing the joint nullity
of autoregressive coeﬃcients, up to lag 15, for ut has a p-value of 0.0868 and for
u2
t, a p-value of 0.399. In the case of the MS-GJR model, the p-values are 0.0745
and 0.464, respectively. Therefore, both models seem adequate in removing the
volatility clustering present in the data set. The likelihood ratio framework for
testing the ﬁrst four moments of the transformed residuals yields p-values of
0.125 for the GJR model and 0.0635 for the MS-GJR model. Overall, these
results indicate no evidence of misspeciﬁcation at the 5% signiﬁcance level for
both models.

134
7 MS-GJR(1, 1) Model with Student-t Innovations
7.4.2 Deviance information criterion
In order to evaluate the goodness-of-ﬁt of the models, we use ﬁrst the Deviance
information criterion (henceforth DIC) introduced by Spiegelhalter et al. [2002].
The DIC is not intended for identiﬁcation of the correct model, but rather
merely as a method of comparing a collection of alternative formulations (all of
which may be incorrect) and determining the most appropriate. This criterion
follows from an extension of the Deviance proposed by Dempster [1997]. A recent
article by Berg, Meyer, and Yu [2004] has illustrated the potential advantages
of this information criterion in determining the appropriate stochastic volatility
model. This criterion presents an interesting alternative to the Bayes factor
which is often diﬃcult to calculate, especially for models that involve many
random eﬀects, large number of unknowns or improper priors.
Let us denote the model parameters by θ for the moment. Based on the
posterior density of the Deviance D(θ) .= −2 ln L(θ | y) where L(θ | y) is the
likelihood function, the DIC consists of two terms: a component that measures
the goodness-of-ﬁt and a penalty term for any increase in model complexity. The
measure of ﬁt is obtained by taking the posterior expectation of the Deviance:
D .= Eθ|y

D(θ)

(7.19)
where Eθ|y[•] denotes the expectation with respect to the joint posterior p(θ | y).
Provided that D(θ) is available in closed form, D can easily be approximated
using the posterior sample by estimating the sample mean of the simulated
values of D(θ). The second component measures the complexity of the model
using the eﬀective number of parameters, denoted by pD, and is deﬁned as
the diﬀerence between the posterior mean of the Deviance and the Deviance
evaluated at a point estimate eθ:
pD .= D −D(eθ) .
(7.20)
A natural candidate for eθ is the posterior mean Eθ|y(θ), as suggested by Spiegel-
halter et al. [2002]. When the density is log-concave, this point estimate ensures
a positive pD due to Jensen’s inequality. The DIC is then simply deﬁned as
DIC .= D + pD and, given a set of models, the one with the smallest DIC has
the best balance between goodness-of-ﬁt and model complexity.
As noted in Celeux, Forbes, Robert, and Titterington [2006], the deﬁnition
eθ .= Eθ|y(θ) is not appropriate in mixture models when no identiﬁcation is im-
posed. Furthermore, when the state variable is discrete and considered as a
parameter in θ, the posterior expectation usually fails to take one of the dis-
crete values. To overcome these diﬃculties, we integrate out the state vector by

7.4 In-sample performance analysis
135
considering the observed likelihood instead [see Celeux et al. 2006, Sect.3.1] and
make use of the constrained posterior sample in the estimation. In the context
of MS-GARCH models, the observed likelihood, also referred to as the marginal
likelihood in Kaufmann and Fr¨uhwirth-Schnatter [2002, p.457] is obtained as
follows:
L(ψ | y) =
T
Y
t=1
" K
X
k=1
p(yt | ψ, st = k, Ft−1) P(st = k | ψ, Ft−1)
#
(7.21)
where p(yt | ψ, st = k, Ft−1) can be estimated by the Student-t density and
the ﬁltered probability P(st = k | ψ, Ft−1) is obtained as a byproduct from the
FFBS algorithm [see Chib 1996, p.83]. The DIC is then deﬁned as the sum of
components (7.19) and (7.20), which yields:
DIC .= 2
n
ln L
 ψ | y

−2Eψ|y

ln L(ψ | y)
o
where we recall that ψ .= Eψ|y(ψ) with ψ .= (α, β, ν, P).
In order to make statements about the goodness-of-ﬁt of one model relative
to another, it is important to consider the uncertainty in the DIC. While the
conﬁdence interval for D can be easily obtained from the MCMC output by
using spectral methods as this is done for the posterior mean, the task is more
tedious in the case of pD and hence for the DIC itself. Approximation methods
have been experimented in Zhu and Carlin [2000] but the brute force approach
is still the most accurate. In this method, the variability of the DIC is estimated
by running several MCMC chains and calculating the DIC’s variance from the
diﬀerent runs. Obviously, this is extremely costly. A simpler alternative consists
in running few MCMC runs and reporting the minimum and maximum DIC
obtained. This gives however a crude idea of DIC’s variability. In what follows,
we make use of a methodology which estimates the whole distribution for the
DIC based on a resampling technique. More precisely, from the joint posterior
sample {ψ[j]}J
j=1, we generate randomly B new posterior samples of size J
by using the block bootstrap technique and estimate DIC’s components for
these samples. By comparing the 95% conﬁdence interval of the diﬀerent DICs,
we can ﬁnd statistical evidence of diﬀerences in the ﬁtting quality. With this
methodology, the MCMC procedure does not need to be re-run which strongly
diminishes the computing time. The choice of the block length is an important
issue in the block bootstrap technique. For the block bootstrap to be eﬀective,
the length should be large enough so that it includes most of the dependence
structure, but not too large so that the number of blocks becomes insuﬃcient.
In our analysis, we use the stationary bootstrap of Politis and Romano [1994]

136
7 MS-GJR(1, 1) Model with Student-t Innovations
and select the block length following the algorithm based on the spectral density
estimation, as proposed by Politis and White [2004]. We apply the block length
selection algorithm to each parameter’s output. The maximum value is then
deﬁned as the optimal block length used for block bootstrapping the constrained
posterior sample. This ad-hoc procedure allows to keep the autocorrelation in
the chains as well as the cross-dependence structure between the parameters.
Results for the DIC and its components are reported in Table 7.2. They are
based on 10’000 draws from the constrained posterior distribution. In squared
brackets we give the 95% conﬁdence interval obtained by the resampling tech-
nique using B = 100 replications. We keep every tenth draw from the joint
posterior sample in the resampling technique in order to speed up the calcula-
tions and diminish the autocorrelation in the chains. For comparison purposes,
we also consider the Bayesian information criterion introduced by Schwarz [1978]
which is deﬁned as follows:
BIC(ψ) .= 2 ln L(ψ | y) −n ln T
where n is the number of parameters and T the number of observations. In our
context, T = 2’500, n = 5 for the GJR model and n = 11 for the MS-GJR model
(since parameters p12 and p21 are redundant due to the summability constraint).
This criterion promotes model parsimony by penalizing models with increased
model complexity (larger n) and sample size T. Hence, a model with the largest
BIC is preferred. The computation of the Bayesian information criterion is based
on the posterior mean Eψ|y

BIC(ψ)

obtained over the 10’000 draws of the
constrained posterior sample.
Table 7.2. Results of the DIC and BIC criteria.⋆
Model
DIC
D
pD
Eψ|y(BIC)
GJR
6770.4
6765.6
4.76
-6806.07
[6769.9,6770.8]
[6765.3,6765.8]
[4.49,4.93]
(7.12)
MS-GJR
6713.3
6704.4
8.84
-6804.73
[6712.6,6713.8]
[6793.9,6794.9]
[8.49,9.04]
(12.55)
⋆DIC: Deviance information criterion; D: Deviance evaluated at the
posterior mean ψ (see Table 7.1, p.127); pD: eﬀective number of pa-
rameters; Eψ|y(BIC): posterior mean of BIC(ψ) obtained over the 10’000
draws of the constrained posterior sample; [•]: 95% conﬁdence interval
based on B = 100 replications of the constrained posterior sample; (•):
numerical standard error (×102).

7.4 In-sample performance analysis
137
From Table 7.2, we can notice that both DIC and BIC criteria favor the
MS-GJR model. Indeed, the DIC estimates based on the initial joint posterior
sample is 6770.4 for the GJR model and 6713.3 for the MS-GJR model. Both
95% conﬁdence intervals do not overlap which suggests signiﬁcant improvement
of the Markov-switching model. In the case of BIC, the diﬀerences between the
criterion’s values are less pronounced but still the Markov-switching model is
favored compared to the single-regime model. If we consider now the estimations
of pD, we note that the estimated value is somewhat lower than ﬁve in the GJR
model while about nine in the MS-GJR case. Hence, in the single-regime model,
every parameter seems to be eﬀective (or informative) when ﬁtting the model
to the data set. In the Markov-switching model however, about two third of the
13 parameters are eﬀective. This is in line with the estimation results where
it was shown that parameters α1 and α2 are almost identical across regimes.
Furthermore, the 2 × 2 transition matrix only contains two free parameters due
to the summability constraint. This suggests that the nine eﬀective parameters
of the MS-GJR model are α1
0, α2
0, α1, α2, β1, β2, ν, p11 and p22.
Finally, we point out that we have also considered the posterior mode:
eψ .= arg max
ψ
L(ψ | y)
in the deﬁnition of pD, as suggested by Celeux et al. [2006, Sect.3.1]. It is argued
that such a point estimate is more relevant since it depends on the posterior
distribution of the whole parameter ψ, rather than on the marginal posterior
distributions of its elements. The values of pD obtained with this new deﬁnition
are larger for both models with 95% conﬁdence intervals respectively given by
[5.17,5.66] and [10.06,11.12] for the single-regime and Markov-switching models.
While the preferred model remains the MS-GJR, the interpretation of parameter
pD is now questionable in the GJR case since the value of pD exceeds the total
number of parameters.
7.4.3 Model likelihood
As a second criterion to discriminate between the models under study, we con-
sider the model likelihood which may be expressed as follows:
p(y) =
Z
L(ψ | y)p(ψ)dψ
where L(ψ | y) is the marginal likelihood given in (7.21) and p(ψ) is the joint
prior density on ψ .= (α, β, ν, P). It is clear that the model likelihood is equal
to the normalizing constant of the posterior density:

138
7 MS-GJR(1, 1) Model with Student-t Innovations
p(ψ | y) = L(ψ | y)p(ψ)
p(y)
.
The estimation of p(y) requires the integration over the whole set of parameters
ψ, which is a diﬃcult task in practice, especially for complex statistical models
such as ours. A full investigation of the various approaches available to esti-
mate the model likelihood for ﬁnite mixture models can be found in Fr¨uhwirth-
Schnatter [2004]. In particular, the author documents that the bridge sampling
technique using the MCMC output of the random permutation sampler and
an iid sample from an importance density q(ψ) which approximates the un-
constrained posterior yields the best estimator of the model likelihood (i.e.,
the estimator with the lowest variance). Moreover, the variance of the bridge
sampling estimator depends on a ratio that is bounded regardless of the tail be-
haviour of the importance density. This renders the estimator robust and gives
more ﬂexibility in the choice of the importance density.
First, let us recall that the bridge sampling technique of Meng and Wong
[1996] is based on the following result:
1 =
R
a(ψ)p(ψ | y)q(ψ)dψ
R
a(ψ)q(ψ)p(ψ | y)dψ = Eq

a(ψ)p(ψ | y)

Eψ|y

a(ψ)q(ψ)

(7.22)
where a(ψ) is an arbitrary function such that
R
a(ψ)p(ψ | y)q(ψ)dψ > 0 and Eq
denotes the expectation with respect to the importance density q(ψ). Replacing
p(ψ | y) by
L(ψ|y)p(ψ)
p(y)
in expression (7.22) yields the key identity for bridge
sampling:
p(y) = Eq

a(ψ)L(ψ | y)p(ψ)

Eψ|y

a(ψ)q(ψ)

.
We can estimate the model likelihood for a given function a(ψ) by replacing the
expectations on the right-hand side of the latter expression by sample averages.
More precisely, we use MCMC draws {ψ[m]}M
m=1 from the joint posterior p(ψ | y)
and iid draws {ψ[l]}L
l=1 from the importance sampling density q(ψ) to get the
following approximation:
p(y) ≈
1
L
PL
l=1 a(ψ[l])L(ψ[l] | y)p(ψ[l])
1
M
PM
m=1 a(ψ[m])q(ψ[m])
.
(7.23)
Meng and Wong [1996] discuss an asymptotically optimal choice for a(ψ), which
minimizes the expected relative error of the p(y) estimator for iid draws from
p(ψ | y) and q(ψ). This function is given by:

7.4 In-sample performance analysis
139
a(ψ) ∝
1
Lq(ψ) + Mp(ψ | y) .
This special case of bridge sampling estimator is referred to as the optimal bridge
sampling estimator by Fr¨uhwirth-Schnatter [2001a] and will be used in what
follows. As the optimal choice depends on the normalized posterior p(ψ | y),
Meng and Wong [1996] use an iterative procedure to estimate p(y) as a limit of
a sequence {pt(y)}. Based on an estimate pt−1(y) of the normalizing constant,
the posterior is normalized as follows:
pt−1(ψ | y) .= L(ψ | y)p(ψ)
pt−1(y)
and a new estimate pt(y) is computed using approximation (7.23). This leads
to the following recursion:
pt(y) .= pt−1(y) ×
1
L
PL
l=1
pt−1(ψ[l]|y)
Lq(ψ[l])+Mpt−1(ψ[l]|y)
1
M
PM
m=1
q(ψ[m])
Lq(ψ[m])+Mpt−1(ψ[m]|y)
which can be initialized, e.g., with the reciprocal importance sampling estimator
of Gelfand and Dey [1994] given by:
p0(y) =
"
1
M
M
X
m=1
q(ψ[m])
L(ψ[m] | y)p(ψ[m])
#−1
.
Note that this latter estimator is only based on MCMC draws from the joint
posterior. Convergence of the bridge sampling technique is typically very fast in
practice. In our case, the estimates converged after 3–4 iterations.
The remaining task consists in choosing an appropriate importance density
to apply the bridge sampling technique. To that aim, we follow Kaufmann and
Fr¨uhwirth-Schnatter [2002, pp.438–439] and Kaufmann and Scheicher [2006,
pp.9–10]. The importance density is constructed in an unsupervised manner
from the MCMC output of the random permutation sampler using a mixture of
the proposal and conditional densities. Its construction is fully automatic and
is easily incorporated in the MCMC sampler [see Fr¨uhwirth-Schnatter 2001a,
p.39]. Formally, the importance density is deﬁned as follows:
q(ψ) .=
 1
R
R
X
r=1
qα(α | α[r], β[r], ϖ[r], ν[r], s[r], y)
× qβ(β | α[r], β[r], ϖ[r], ν[r], s[r], y) × p(P | s[r])

× qν(ν)
(7.24)

140
7 MS-GJR(1, 1) Model with Student-t Innovations
where:
α[r]
,
β[r]
,
ϖ[r]
,
ν[r]
,
s[r]
for r = 1, . . . , R
are draws from the unconstrained posterior sample, qα(α | •) is the proposal
density for parameter α given in (7.13), qβ(β | •) is the proposal density for
parameter β given in (7.15) (the normalizing constants are easily obtained as
the proposals are truncated multivariate Normal densities), p(P | •) is the prod-
uct of Dirichlet posterior densities for the transition probabilities given in (7.9).
For the degrees of freedom parameter ν, the optimized rejection technique of
Sect. 7.2.5 does not lead to a known expression for the marginal posterior
on ν. To tackle this problem, we approximate the marginal posterior by us-
ing a truncated skewed Student-t density whose parameters are estimated by
Maximum Likelihood from the posterior sample {ν[j]}J
j=1. More precisely, the
approximation may be written as follows:
qν(ν) ∝SS(ν | bµ, bσ2, bτ, bγ)I{ν>δ}
where:
SS(ν | µ, σ2, τ, γ) .=
2
γ + 1
γ
Γ
  τ+1
2

Γ
  τ
2

(πτσ2)1/2
×

1 + (ν −µ)2
τσ2
 1
γ I{ν−µ⩾0} + γ2I{−∞<ν−µ}
−τ+1
2
(7.25)
is the skewed Student-t density as deﬁned in Fern´andez and Steel [1998, Eq.13,
p.363]. The parameters of the density deﬁned in (7.25) are: the location pa-
rameter µ, the scale factor σ2 > 0, the degrees of freedom parameter τ ⩾1
and the asymmetry coeﬃcient γ > 0. For γ = 1, the density coincides with the
symmetric Student-t density. In cases where γ > 1, the density is right-skewed
while it is left-skewed when γ < 1. Therefore, parametrization (7.25) allows for a
wide range of asymmetric and heavy-tailed densities. Moreover, the normalizing
constant for qν(ν) is easily obtained by conventional quadrature methods.
Some comments are in order here. First, the generation of draws from the
proposal densities qα(α | •) and qβ(β | •) is achieved by the rejection technique.
While we obtain good acceptance rates in our case, this method can become
very ineﬃcient if the mass of the density is close to the domain of truncation.
For these cases, we would need a more sophisticated algorithm, as proposed in
Philippe and Robert [2003], Robert [1995], to draw eﬃciently from a truncated

7.4 In-sample performance analysis
141
multivariate Normal distribution. Second, the density qν(ν) is constructed in two
steps. The parameters of the the skewed Student-t are ﬁrst estimated by ML
from the MCMC output and then the density is truncated to construct qν(ν).
An alternative approach would be to ﬁt directly the truncated skewed Student-t
density by ML. This is however not necessary in our case since the mass of the
posterior on the degrees of freedom is far from the truncation domain. Finally,
generating draws from qν(ν) is achieved by the rejection technique. In cases
where the boundary is close to the high probability mass, alternative approaches,
such as the inversion technique, are required [see, e.g., Geweke 1991].
As indicated previously, the parameters of the skewed Student-t density are
estimated by ML using the posterior sample of ν. In the case of the MS-GJR
model, we obtain the following ML estimates:
bµ = 9.49
,
bσ2 = 1.50
,
bτ = 16.67
and
bγ = 1.53 .
In the upper part of Fig. 7.6, we display the ﬁtted truncated skewed Student-t
density (in dashed line) together with the density of the posterior sample for
ν (in solid line) obtained through Gaussian kernel density estimates [see Sil-
verman 1986]. We can notice that the truncated skewed Student-t density ap-
proximates the marginal closely. In the lower part of the ﬁgure, we show the
marginal posterior for parameter β1 together with the importance density com-
puted with R = 1’000. As the construction of the mixture (7.24) is based on
averaging over proposal densities, where the state process is sampled from the
unconstrained posterior with balanced label switching, the mixture importance
density is multimodal. We also notice that the importance density provides a
good approximation of the marginal posterior.
In Table 7.3, we report the natural logarithm of the model likelihoods
obtained using the reciprocal sampling estimator (second column) and the bridge
sampling estimator (last column) for M = L = 1’000 draws. From this table, we
can notice that both estimators are higher for the MS-GJR model, indicating
a better in-sample ﬁt for the regime-switching speciﬁcation. As an additional
discrimination criterion, we compute the (transformed) Bayes factor in favor of
the MS-GJR model [see Kass and Raftery 1995, Sect.3.2]. The estimated value
is 2 × ln BF = 2 × (−3389.66 −(−3408.04)) = 36.76, which strongly supports
the in-sample evidence in favor of the regime-switching model.
A ﬁnal word about the robustness of these results is in order. It is indeed
recognized that the model likelihood is sensitive to the choice of the prior density.
We must therefore test whether an alternative joint prior speciﬁcation would
have modiﬁed the conclusion of our analysis. To answer this question, we modify
the hyperparameters’ values and run the sampler again. This time, we consider

142
7 MS-GJR(1, 1) Model with Student-t Innovations
Table 7.3. Results of the model likelihood estimators.⋆
Model
ln p0(y)
ln p(y)
GJR
-3405.33 (2.979)
-3408.04 (2.644)
MS-GJR
-3386.14 (3.109)
-3389.66 (3.191)
⋆ln p0(y): natural logarithm of the model likelihood esti-
mate using reciprocal sampling; ln p(y): natural logarithm
of the model likelihood estimate using bridge sampling; (•)
numerical standard error of the estimators (×102).
slightly more informative priors for the vectors α and β by choosing diagonal
covariance matrices whose variances are set to σ2
αi = σ2
β = 1’000 (i = 0, 1, 2). As
an alternative prior on the degrees of freedom parameter, we choose λ = 0.02
and δ = 2, which implies a prior mean of 52. Finally, the hyperparameters for
the prior on the transition probabilities are set to ηii = 3 and ηij = ηji = 1 for
i, j ∈{1, 2}. We recall that the hyperparameters of the initial joint prior were
set to σ2
αi = σ2
β = 10’000, λ = 0.01, δ = 2, ηii = 2 and ηij = ηji = 1. In this
case, the results are similar to those obtained previously. The natural logarithm
of the bridge sampling estimator is -3402.11 for the GJR model and -3388.09
for the MS-GJR model, implying a (transformed) Bayes factor of 28.04. These
results are in line with the conclusion of the previous section and conﬁrm the
better ﬁt of the Markov-switching model.

7.4 In-sample performance analysis
143
6
8
10
12
14
16
18
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Parameter ν
Posterior density
Truncated skewed Student−t density
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
Parameter β1
Posterior density
Importance density
Fig. 7.6. Importance density (in dashed line) and marginal posterior density (in solid
line) comparison. Gaussian kernel density estimates with bandwidth selected by the
“Silverman’s rule of thumb”criterion [see Silverman 1986, p.48]. Both graphs are based
on 10’000 draws from the unconstrained posterior sample.

144
7 MS-GJR(1, 1) Model with Student-t Innovations
7.5 Forecasting performance analysis
In order to evaluate the ability of the competing models to predict the future
behavior of the volatility process, we study the forecasted one-day ahead Value
at Risk (henceforth VaR), which is a common tool to measure ﬁnancial and
market risks. The one-day ahead VaR at risk level φ ∈(0, 1), denoted by VaRφ,
is estimated by calculating the φcth percentile of the one-day ahead predictive
distribution, where φc
.= (1 −φ) for convenience. The predictive density is
obtained by simulation from the joint posterior sample {ψ[j]}J
j=1 as follows:
s[j]
t+1 ∼p(st+1 | ψ[j], Ft)
y[j]
t+1 ∼p(yt+1 | ψ[j], s[j]
t+1, Ft)
and VaRφ is then simply estimated by calculating the φcth percentile of the
empirical distribution {y[j]
t+1}J
j=1.
In order to simulate from the predictive density over the out-of-sample obser-
vation window, the posterior sample {ψ[j]}J
j=1 should be updated using the most
recent information. Consequently, forecasting the one-day ahead VaR would ne-
cessitate the estimation of the joint posterior sample at each time point in the
out-of-sample observation window. However, such an approach is computation-
ally impractical for a large data set such as ours. Combination of MCMC and
importance sampling to estimate eﬃciently this predictive density is proposed
by Gerlach, Carter, and Kohn [1999]. Nevertheless, for the sake of simplicity, we
will consider the same joint posterior sample, based on the in-sample data set,
when forecasting the VaR.
In addition to the static GJR and MS-GJR models, we consider a GJR
model estimated on rolling windows which is the standard practice in ﬁnancial
risk management. This methodology relies on the assumption that older data are
not available or are irrelevant due to structural breaks, which are so complicated
that they cannot be modeled. We refer the reader to Sect. 6.4.1 for a detailed
presentation of this procedure. For this approach, we use 750 log-returns to es-
timate the model and the next 50 log-returns are used as a forecasting window.
Then, the estimation and forecasting windows are moved together by 50 days
ahead, so that the forecasting windows do not overlap. In this manner, the es-
timation methodology fulﬁlls the recommendations of the Basel Committee in
the use of internal models [see Basel Committee on Banking Supervision 1996b].
When applied to our data set, this estimation design leads to the generation of
26 estimation windows for a total of 26×50 = 1’300 out-of-sample observations.
In the case of the static GJR and MS-GJR models, the ﬁrst 2’500 observations
of our data set are used to estimate the models while the remaining 1’300 obser-

7.5 Forecasting performance analysis
145
vations are used to test their predictive performance. For the three models, the
VaR predictions are obtained for the same 1’300 out-of-sample daily log-returns.
To verify the accuracy of the VaR estimates for the analyzed models, we
adopt the testing methodology proposed by Christoﬀersen [1998]. This approach
is based on the study of the random sequence {V φ
t } where:
V φ
t
.=



1
if
yt+1 < VaRφ
t
0
else .
A sequence of VaR forecasts at risk level φ has correct conditional coverage if
{V φ
t } is an independent and identically distributed sequence of Bernoulli ran-
dom variables with parameter φc. This hypothesis can be veriﬁed by testing
jointly the independence on the series and the unconditional coverage of the
VaR forecasts, i.e., E(V φ
t ) = φc, as proposed by Christoﬀersen [1998].
Forecasting
results
for
the
VaR
are
reported
in
Table
7.4
for
φ ∈{0.90, 0.95, 0.99} which are typical risk levels used in ﬁnancial risk manage-
ment. The second and third columns give the expected and observed number
of violations. The last three columns report the p-values for the tests of correct
unconditional coverage (UC), independence (IND) and correct conditional cov-
erage (CC). From this table, we ﬁrst note that the observed number of violations
for the MS-GJR model are closer to the expected values than for the static GJR
model. Indeed, at the 1% signiﬁcance level, the test of correct unconditional
coverage is not rejected for the Markov-switching model while it is strongly re-
jected for the GJR model at risk level φ = 0.95. The test of independence is
not rejected for both models at the 1% signiﬁcance level. We can notice that
for risk level φ = 0.99 this test is not applicable since no consecutive violations
have been observed. The joint hypothesis of correct unconditional coverage and
independent sequence is obtained via the test of correct conditional coverage. In
the case of the MS-GJR model, p-values are close to 0.10 for risk levels φ = 0.9
and φ = 0.95 while it is 0.030 and 0.013 in the GJR case. We therefore reject
the correct conditional coverage hypothesis for the static GJR model at the 5%
signiﬁcance level. These results indicate the better out-of-sample performance
of the Markov-switching model compared to the static GJR model.
When comparing the MS-GJR model with the rolling GJR model, we can
notice that both approaches perform equally well. Indeed, for both models, the
test of independence is rejected at risk level φ = 0.90 while the correct condi-
tional coverage hypothesis is not rejected at the 5% signiﬁcance level. Although
the two models are successful in forecasting the conditional variance of the SMI
log-returns, the MS-GJR model has two advantages over the rolling window

146
7 MS-GJR(1, 1) Model with Student-t Innovations
Table 7.4. Forecasting results of the VaR.⋆
GJR model (static approach)
φ
E(V φ
t )
#
UC
IND
CC
0.99
13
14
0.783
NA
NA
0.95
65
89
0.004
0.624
0.013
0.90
130
143
0.236
0.018
0.030
GJR model (rolling windows approach)
φ
E(V φ
t )
#
UC
IND
CC
0.99
13
15
0.586
NA
NA
0.95
65
73
0.318
0.547
0.506
0.90
130
126
0.710
0.032
0.093
MS-GJR model (static approach)
φ
E(V φ
t )
#
UC
IND
CC
0.99
13
13
1.000
NA
NA
0.95
65
80
0.065
0.323
0.112
0.90
130
132
0.854
0.035
0.107
⋆φ: risk level; E(V φ
t ): expected number of violations;
#: observed number of violations; UC: p-value for the
correct unconditional coverage test; IND: p-value for
the independence test; CC: p-value for the correct con-
ditional coverage test; NA: not applicable.
approach. First, it is able to anticipate structural breaks in the conditional vari-
ance process. This is achieved through the estimation of the ﬁltered probabilities
P(st = k | ψ, Ft−1), as shown in Fig. 7.7. On the contrary, the rolling window
methodology is merely an ad-hoc approach which is unable to forecast structural
breaks. The updating frequency as well as the length of the rolling window are
subjective quantities, albeit some ranges are recommended by regulators, so that
diﬀerent choices might lead to signiﬁcant diﬀerences in the model’s performance.
Second, the MS-GJR model needs only to be estimated once. On the contrary,
the parameters of the GJR model must be updated frequently to account for
structural breaks in the time series and this can have practical consequences for
risk management systems of ﬁnancial institutions. This is a deﬁnite advantage
of the regime-switching approach compared to the traditional rolling window
methodology.

7.5 Forecasting performance analysis
147
−5.0
−2.5
0.0
2.5
5.0
0.00
0.25
0.50
0.75
1.00
1
101
201
301
401
501
601
701
801
901
1001
1101
1201
Daily log−returns
(in percent)
Pr(st=2 | ψ, Ft−1)
time index
Filtered probabilities
Fig. 7.7. Filtered probabilities of the high-volatility state (solid line, left axis) together with the out-of-sample log-returns (circles, right
axis). The 95% conﬁdence bands are shown in dashed lines.

148
7 MS-GJR(1, 1) Model with Student-t Innovations
7.6 One-day ahead VaR density
As emphasized in Chap. 6, the one-day ahead VaR risk measure can be ex-
pressed as a function of the model parameters when the underlying time series
is described by a single-regime GARCH(1, 1) model. It turns out that this is also
the case in the context of Markov-switching GARCH models. In eﬀect, the one-
day ahead VaR at risk level φ, estimated at time t, can be explicitly calculated
for given ψ and future state st+1 as follows:
VaRφ
t (ψ, st+1) .=

ϱ(ν) × e′
t+1(st+1)ht+1(α, β)
1/2 × tφc(ν)
(7.26)
where we recall that ϱ(ν) .= ν−2
ν
and tφc(ν) denotes the φcth percentile of a
Student-t distribution with ν degrees of freedom. Hence, the VaR risk measure
can be simulated from the joint posterior sample {ψ[j]}J
j=1 by ﬁrst generating
s[j]
t+1 from the ﬁltered probability density p(st+1 | ψ[j], Ft), and then inputting
the joint draw (ψ[j], s[j]
t+1) in expression (7.26).
The result of this procedure is shown in Fig. 7.8 where we plot the one-
day ahead VaR density of the MS-GJR model for two distinct time points in
the out-of-sample observation window. We can notice that both densities are bi-
modal, which is a consequence of the Markov-switching nature of the conditional
variance process. At time t = 2’501, the VaR density gives a higher probability
to larger (in absolute value) VaR values. This suggests that, at that particular
point in time, the probability of being in the high volatility state is higher than
being in the low-volatility regime. At time t = 3’500, the bimodality of the den-
sity is slightly less pronounced. In this case, the VaR density puts more mass on
smaller VaR values (in absolute value). This graph shows that the density of the
VaR has a particular shape in the case of the MS-GJR model. In this context, it
would be interesting to determine if the loss function of an agent, and therefore
the location of his optimal Bayes estimate within the VaR density, would have
any inﬂuence on the forecasting performance of the model.
In order to address this question, we consider diﬀerent loss functions and
determine the Bayes point estimates for the VaR by solving the optimization
problem (6.10) of page 85. The loss functions we consider are the Linex with a
parameter a ∈{−3, 3}, the absolute error loss (AEL) as well as the squared error
loss (SEL); the reader is referred to Sect. 6.4.4 for further details. We recall
however that the Linex function with a positive parameter could be attributed
to a regulator or risk manager whose aim is to avoid systematic failure in risk
measure estimation. On the contrary, a negative parameter could be attributed
to a fund manager who seeks to save risk capital since it earns little or no
return at all (see Sect. 6.3.1 for details). The AEL and SEL correspond to the

7.6 One-day ahead VaR density
149
perspective of an agent for whom under- and overestimation are equally serious.
The SEL leads, however, to a larger penalty for larger deviations from the true
value compared to the AEL function.
The VaR risk measure obtained with the diﬀerent loss functions are then
tested over the 1’300 out-of-sample observations. To test the adequacy of the
point estimates to reproduce the true VaR, we rely on the forecasting method-
ology of Christoﬀersen [1998] as this was done in the preceding section. The
results are reported in Table 7.5 whose second column gives the observed num-
ber of violations and the third, fourth and ﬁfth columns report the p-values for
the tests of correct unconditional coverage (UC), independence (IND) and cor-
rect conditional coverage (CC), respectively. From this table, we note ﬁrst that
the observed number of violations is close to the expected value for the Linex
function with parameter a = 3. In this case, the test of correct unconditional
coverage, at the 5% signiﬁcance level, is never rejected. On the contrary, the
Linex function with parameters a = −3 leads to the rejection of the null for risk
levels φ = 0.95 and φ = 0.99. The null hypothesis is also rejected for the AEL
and SEL point estimates at risk level φ = 0.95, where the estimates systemat-
ically underestimate (in absolute value) the true VaR. The joint hypothesis of
correct unconditional coverage and independence is rejected at the 5% signiﬁ-
cance level for all functions, except the Linex with a = 3 and the SEL at risk
level φ = 0.9.
From what precedes, we can thus conclude that parameter uncertainty has to
be taken seriously in the context of MS-GARCH models. In particular, the choice
of a given point estimate within the VaR density has a signiﬁcant impact on
the forecasting performance of the model. A regulator (Linex a = 3) whose VaR
point estimate are conservative, would conclude to a good performance of the
model while a fund manager (Linex a = −3) would systematically underestimate
(in absolute value) the true VaR.

150
7 MS-GJR(1, 1) Model with Student-t Innovations
Table 7.5. Forecasting results of the VaR point esti-
mates for the MS-GJR model.⋆
φ = 0.90, E(V φ
t ) = 130;
Loss L
#
UC
IND
CC
Linex (a = 3)
130
1.000
0.018
0.061
Linex (a = −3)
140
0.361
0.011
0.025
AELa
133
0.782
0.011
0.039
SELb
131
0.926
0.015
0.053
φ = 0.95, E(V φ
t ) = 65;
Loss L
#
UC
IND
CC
Linex (a = 3)
71
0.452
0.270
0.410
Linex (a = −3)
87
0.008
0.171
0.011
AELa
84
0.020
0.228
0.033
SELb
83
0.028
0.249
0.046
φ = 0.99, E(V φ
t ) = 13;
Loss L
#
UC
IND
CC
Linex (a = 3)
11
0.567
NA
NA
Linex (a = −3)
21
0.041
NA
NA
AELa
17
0.287
NA
NA
SELb
14
0.783
NA
NA
⋆φ: risk level; E(V φ
t ): expected number of violations;
#: observed number of violations; UC: p-value for the
correct uncoverage test; IND: p-value for the indepen-
dence test; CC: p-value for the correct conditional cov-
erage test; NA: not applicable.
a Absolute error loss function.
b Squared error loss function.

7.6 One-day ahead VaR density
151
−2.2
−2.0
−1.8
−1.6
−1.4
−1.2
−1.0
0
1
2
3
4
−2.2
−2.0
−1.8
−1.6
−1.4
−1.2
−1.0
0
1
2
3
4
VaR at t = 2501
VaR at t = 3500
One−day ahead 95% VaR
Fig. 7.8. Density of the one-day ahead VaR at risk level φ = 0.95 for the MS-GJR
model at two time points in the out-of-sample observation window. Gaussian kernel
density estimates with bandwidth selected by the “Silverman’s rule of thumb” criterion
[see Silverman 1986, p.48]. Both graphs are based on 10’000 draws from the joint
posterior density of the MS-GJR model parameters.

152
7 MS-GJR(1, 1) Model with Student-t Innovations
7.7 Maximum Likelihood estimation
We conclude this chapter with some comments regarding the Maximum Likeli-
hood (henceforth ML) estimation of Markov-switching GARCH models. In this
case, the estimation is handled as in Hamilton [1994, p.692], where the algorithm
turns out to be a special case of the Expectation Maximization (henceforth EM)
algorithm developed by Dempster, Laird, and Rubin [1977]. The classical ML
approach cannot be applied directly, as the marginal likelihood where the latent
process {st} is integrated out, is not available in closed form. The estimation
procedure is therefore decomposed into two stages. The ﬁrst step consists in
estimating the sequence of ﬁltered probabilities {P(st = k | ψ, Ft−1)}T
t=1 for a
ﬁxed set of of parameters ψ. The second step maximizes the observed likelihood
L(ψ | y) in expression (7.21) given this sequence of probabilities. The procedure
is iterated until a given convergence criterion is satisﬁed. General results avail-
able for the EM algorithm indicate that the likelihood function increases in the
number of iterations.
While apparently straightforward to handle, the ML estimation has practical
drawbacks. Indeed, the EM algorithm guarantees a convergence to a local maxi-
mum of the likelihood, but not necessarily to the global optimum. As reported in
Hamilton and Susmel [1994], many starting points are required to end up with
a global maximum. Furthermore, the covariance matrix at the optimum can be
extremely tedious to obtain and ad-hoc procedures are often required to get re-
liable results. E.g., Hamilton and Susmel [1994] ﬁx some transition probabilities
to zero in order to determine the variance estimates for some model parameters.
Finally, testing the null of K versus K′ states is not possible within the ML
framework since the regularity conditions for justifying the χ2 approximation of
the likelihood ratio statistic do not hold.
For comparison purposes, we estimate the MS-GJR model via the ML tech-
nique. The iterative procedure described previously has been run using 20 ran-
dom starting values. In all cases, the optimizer has been trapped in a local
maximum or even did not converge. The convergence has only been achieved
by starting the ML optimizer at the posterior mean ψ (see Table 7.1, p.127)
obtained with the Bayesian approach.
In Fig. 7.9, we display the marginal densities obtained via Gaussian kernel
density estimates, for the model parameters obtained through the Bayesian ap-
proach (in solid lines) and the ML approach (in dashed lines). From these graphs,
we note that the ML estimation leads to more peaked density estimates and
therefore underestimates the parameter uncertainty. Furthermore, compared to
the Bayesian approach, the ML approach underestimates the values of the com-
ponents of vector α whereas the components of β are overestimated.

7.7 Maximum Likelihood estimation
153
0.0
0.2
0.4
0
2
4
6
8
Parameter α0
1
0.0
0.2
0.4
0
2
4
6
8
Parameter α0
2
0.00
0.05
0.10
0.15
0
10
20
30
40
50
Parameter α1
1
0.00
0.05
0.10
0
5
10
15
20
25
30
Parameter α1
2
0.0
0.2
0.4
0
1
2
3
4
5
6
7
Parameter α2
1
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
Parameter α2
2
0.0
0.2
0.4
0.6
0.8
0
1
2
3
4
Parameter β1
0.6
0.7
0.8
0.9
0
2
4
6
8
10
Parameter β2
5
10
15
20
25
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Parameter ν
0.985
0.995
0
100
200
300
400
Parameter p11
0.000
0.010
0
100
200
300
400
Parameter p12
0.000
0.010
0.020
0
50
100
150
200
250
Parameter p21
0.980
0.990
1.000
0
50
100
150
200
250
Parameter p22
Fig. 7.9. Marginal posterior densities of the MS-GJR model parameters and comparison with the asymptotic Normal approximation.
Results obtained via the Bayesian approach are given in solid lines while the ML estimates are shown in dashed lines. Gaussian kernel
density estimates with bandwidth selected by the “Silverman’s rule of thumb” criterion [see Silverman 1986, p.48]. The graphs are based
on 10’000 draws from the constrained posterior sample.

8
Conclusion
Single-regime and regime-switching GARCH models are widespread and essen-
tial tools in ﬁnancial econometrics and have, until recently, mainly been es-
timated using the Maximum Likelihood (henceforth ML) technique. However,
the Bayesian estimation of these models has several advantages over the classical
approach. First, computational methods based on Markov chain Monte Carlo
(henceforth MCMC) procedures avoid the common problem of local maxima
encountered in the ML estimation of these models. Second, the exploration of
the joint posterior distribution gives a complete picture of the parameter un-
certainty and this cannot be achieved via the classical approach. Third, exact
distributions of nonlinear functions of the model parameters can be obtained at
low cost by simulating from the joint posterior distribution. Fourth, constraints
on the model parameters can be incorporated through appropriate prior speciﬁ-
cations; in such a setting, imposing the constraint of covariance stationarity for
the regime-switching GARCH model, for instance, is straightforward. Finally,
discrimination between models can be achieved through the calculation of model
likelihoods and Bayes factors. All these reasons strongly motivate the use of the
Bayesian approach when estimating GARCH models.
The choice of the algorithm is the ﬁrst issue when dealing with MCMC
methods and it depends on the nature of the problem under study. In the case
of GARCH models, due to the recursive nature of the conditional variance, the
joint posterior and the full conditional densities are of unknown forms, whatever
distributional assumptions are made on the model disturbances. Therefore, we
cannot use the simple Gibbs sampler and need more elaborate estimation proce-
dures. The sampling schemes adopted in this book are based on the approach of
Nakatsuma [1998, 2000] which has the advantage of being fully automatic and
thus avoids the time-consuming and diﬃcult task of tuning a sampling algo-
rithm. In addition, this approach is easy to extend to regime-switching GARCH

156
8 Conclusion
models. In this case, the parameters in each regime can be regrouped and up-
dated by blocks which may enhance the sampler’s eﬃciency.
This book presented in detail methodologies for the Bayesian estimation of
single-regime and regime-switching GARCH models. It proposed empirical ap-
plications to real data sets and illustrated some interesting probabilistic state-
ments on nonlinear functions of the model parameters made possible under the
Bayesian framework.
The work was introduced in Chap. 1 with a review of GARCH modeling
and a presentation of the advantages of the Bayesian approach compared to
the traditional ML technique. In Chap. 2, we proposed a short introduction to
the Bayesian paradigm for inference and gave an overview of the basic MCMC
algorithms used in the rest of the book.
In Chap. 3, we considered the Bayesian estimation of the parsimonious but
eﬀective GARCH(1, 1) model with Normal innovations. We detailed the MCMC
scheme based on the methodology of Nakatsuma [1998, 2000]. An empirical
application to a foreign exchange rate time series was presented where we com-
pared the Bayesian and the ML point estimates. In particular, we showed that
even for a fairly large data set, the estimates and conﬁdence intervals are dif-
ferent between the methods. Caution is therefore in order when applying the
asymptotic Normal approximation for the model parameters in this case. We
performed a sensitivity analysis to check the robustness of our results with re-
spect to the choice of the priors and tested the residuals for misspeciﬁcation.
Finally, we compared the theoretical and sample autocorrelograms of the process
and tested the covariance and strict stationarity conditions.
In Chap. 4, we analyzed the linear regression model with conditionally
heteroscedastic errors which allowed the introduction of lagged dependent vari-
ables in the modeling; moreover, we considered the GJR(1, 1) model to account
for asymmetric responses to past shocks in the conditional error variance pro-
cess. We ﬁtted the model to the Standard and Poors 100 (henceforth S&P100)
index log-returns and compared the Bayesian and the ML estimations. We per-
formed a prior sensitivity analysis and tested the residuals for misspeciﬁcation.
Finally, we tested the covariance stationarity condition and illustrated the dif-
ferences between the unconditional variance of the process obtained through
the Bayesian approach and the delta method. In particular, we showed that the
Bayesian framework leads to a more precise estimate.
In Chap. 5, we extended the linear regression model further with the in-
troduction of Student-t-GJR(1, 1) errors. An empirical application based on the
S&P100 index log-returns was proposed with a comparison between the joint
posterior and the asymptotic Normal approximation of the parameter estimates.

8 Conclusion
157
We performed a prior sensitivity analysis and tested the residuals for misspeci-
ﬁcation. Finally, we analyzed the conditional and unconditional kurtosis of the
underlying time series.
In Chap. 6, we presented some ﬁnancial applications of the Bayesian esti-
mation of GARCH models. We introduced the concept of Value at Risk (hence-
forth VaR) risk measure and proposed a methodology to estimate the density
of this quantity for diﬀerent risk levels and time horizons. We reviewed some
basics in decision theory and used this framework as a rational justiﬁcation for
choosing a point estimate of the VaR. We showed how agents facing diﬀerent
risk perspectives could select their optimal VaR point estimate and documented
substantial diﬀerences in terms of regulatory capital between individuals. Fi-
nally, we extended our methodology to the Expected Shortfall (henceforth ES)
risk measure.
In Chap. 7, we extended the single-regime GJR model to the regime-
switching GJR model (henceforth MS-GJR); more precisely, we considered an
asymmetric version of the Markov-switching GARCH(1, 1) speciﬁcation of Haas
et al. [2004]. We introduced a novel MCMC scheme which can be viewed as a
multivariate extension of the sampler proposed by Nakatsuma [1998, 2000]. As
an application, we ﬁtted a single-regime and a Markov-switching GJR model to
the Swiss Market Index log-returns. We used the random permutation sampler
of Fr¨uhwirth-Schnatter [2001b] to ﬁnd suitable identiﬁcation constraints for the
MS-GJR model and showed the presence of two distinct volatility regimes in
the time series. By using the Deviance information criterion of Spiegelhalter
et al. [2002] and by estimating the model likelihood using the bridge sampling
technique of Meng and Wong [1996], we showed the in-sample superiority of the
MS-GJR model. To test the predictive performance of the models, we ran a fore-
casting performance analysis based on the VaR. In particular, we compared the
MS-GJR model to a single-regime GJR model estimated on rolling windows and
concluded to the superiority of the MS-GJR speciﬁcation. Finally, we proposed
a methodology to depict the density of the one-day ahead VaR and presented a
comparison with the traditional ML approach.
This book proposed two main contributions which are of practical relevance
for both market participants and academics. First, we proposed a novel MCMC
scheme to perform the Bayesian estimation of a Markov-switching model with
Student-t innovations and asymmetric GJR speciﬁcations for the conditional
variance in each regime. It allows to reproduce many stylized facts observed
in ﬁnancial time series, such as volatility clustering, conditional leptokurticity
and Markov-switching dynamics. Furthermore, it helps to identify whether the
leverage eﬀect is diﬀerent across regimes. Our multivariate extension of the ap-

158
8 Conclusion
proach proposed by Nakatsuma [1998, 2000] leads to a fast, fully automatic and
eﬃcient estimation procedure compared to alternative approaches such as the
Griddy-Gibbs sampler. Practitioners who need to run the estimation frequently
and/or for a large number of time series should ﬁnd the procedure helpful. Sec-
ond, we provided a manner to approximate the multi-day ahead VaR and ES
densities when the underlying process is described by a GARCH model. Our
methodology gives the possibility to determine the term structure of these risk
measures and to characterize the uncertainty coming from the model parame-
ters. In our empirical application, we documented that the choice of the model
disturbances has a signiﬁcant impact on the shape of both risk measures’ den-
sities and this eﬀect gets larger as the time horizon increases. Moreover, the
densities are strongly left-skewed, which implies substantial diﬀerences in risk
capital allocation for agents facing diﬀerent risk perspectives (e.g., risk and fund
managers).
Suggestions for further work
This study has raised many questions and suggests interesting further avenues
of research.
First, in light of the results obtained in Chap. 6, additional work is required
to assess the performance of multi-day ahead VaR models. This is essential for
risk management purposes since the multi-day ahead VaR lies at the heart of the
risk capital allocation’s framework. The development of powerful methodologies
for testing the VaR is the subject of current researches and we refer the reader to
Berkowitz, Christoﬀersen, and Pelletier [2006], Kaufmann [2004], Seiler [2006],
Zumbach [2006] for details. A natural extension of our analyses would consider
the model uncertainty in addition to the parameter uncertainty. The Bayesian
approach provides a natural framework for tackling this issue.
Second, regime-switching GARCH models might be compared to the class
of stochastic volatility (henceforth SV) models [see, e.g., Jacquier, Polson, and
Rossi 1994, Kim et al. 1998]. While SV models are highly ﬂexible (two diﬀerent
processes drive the dynamics of the underlying time series and the dynamics
of the volatility), they are more diﬃcult to estimate eﬃciently. Determining
whether this additional ﬂexibility results in a superior predictive ability would
therefore be of interest.
Finally, in our study of the Markov-switching GJR model, we have consid-
ered a ﬁxed transition matrix for the state process. Consequently, the expected
persistence of the regimes is constant over time, which is questionable. In a
more general formulation, we could allow the transition probabilities to change

8 Conclusion
159
over time depending on some observables [see, e.g., Bauwens et al. 2006, Gray
1996]. This would allow determining whether some exogenous variables trigger
the switching mechanism of the volatility process. The transition probabilities
could also depend on the past level of the volatility. In this case, we could re-
produce an additional feature of the volatility behavior, namely the fact that
the probability of returning to a normal (i.e., low or medium) volatility regime
increases after a high upward jump in the volatility level [see, e.g., Bauwens
et al. 2006, Dueker 1997].

A
Recursive Transformations
In this appendix, we demonstrate the recursive transformations introduced in
Chaps. 3, 4 and 5 which are used to express the function zt(α) as a linear
function of parameter α. The process for the conditional variance is based on
observations {yt}. Results can be straightforwardly extended when a linear com-
ponent is included in the model by considering instead the process {ut} where
ut .= yt −x′γ.
A.1 The GARCH(1, 1) model with Normal innovations
First, let us recall that, in the case of the GARCH(1, 1) process, the expression
for the conditional variance of yt is given by:
ht .= α0 + α1y2
t−1 + βht−1
where h0 = y0 .= 0 for convenience. As shown in Sect. 3.2, the GARCH(1, 1)
model with Normal innovations can be expressed as an ARMA(1, 1) model for
the squared observations {y2
t } and approximated as follows:
y2
t = α0 + (α1 + β)y2
t−1 −βzt−1 + zt
where {zt} is a Martingale Diﬀerence process. Let us deﬁne vt .= y2
t for notational
purposes. The variable zt can then be written as:
zt = vt −α0 −(α1 + β)vt−1 + βzt−1
(A.1)
where v0 = z0 = 0.
Proposition A.1. Upon deﬁning the following recursive transformations:
l∗
t
.= 1 + β l∗
t−1
v∗
t
.= vt−1 + β v∗
t−1
(A.2)
where l∗
0 = v∗
0
.= 0, expression (A.1) can be written as follows:

162
A Recursive Transformations
zt = vt −(l∗
t v∗
t )α
(A.3)
where α .= (α0 α1)′. The function zt(α) in (A.1) can therefore be expressed as
a linear function of the 2 × 1 vector α.
Proof. By induction:
Beginning step:
For t = 1, we have:
v1 −(l∗
1 v∗
1)α
(A.2)
=
v1 −(1 0)α = v1 −α0
(A.1)
=
z1 .
Assumption step:
Let us assume that expression (A.3) is satisﬁed for t = k.
Induction step:
For t = k + 1 we have:
vk+1 −(l∗
k+1 v∗
k+1)α
(A.2)
=
vk+1 −(1 + βl∗
k
vk + βv∗
k)α
=
vk+1 −α0 −α1vk −β(α0l∗
k + α1v∗
k)
=
vk+1 −α0 −α1vk −β(l∗
k v∗
k)α
(A.3)
=
vk+1 −α0 −α1vk −β(vk −zk)
=
vk+1 −α0 −(α1 + β)vk + βzk
(A.1)
=
zk+1 .
⊓⊔
A.2 The GJR(1, 1) model with Normal innovations
First, let us recall that, in the case of the GJR(1, 1) model, the expression for
the conditional variance of yt is given by:
ht .= α0 + (α1I{yt−1⩾0} + α2I{yt−1<0})y2
t−1 + βht−1
where h0 = y0 .= 0 for convenience. As shown in Sect. 4.2.2, the GJR(1, 1)
model with Normal innovations can be transformed for the squared observations
{y2
t } and approximated as follows:
y2
t = α0 + (α1I{yt−1⩾0} + α2I{yt−1<0} + β)y2
t−1 −βzt−1 + zt
where {zt} is a Martingale Diﬀerence process. Let us deﬁne vt .= y2
t for notational
purposes. Then, the variable zt can be written as:
zt = vt −α0 −(α1I{yt−1⩾0} + α2I{yt−1<0} + β)vt−1 + βzt−1
(A.4)
where v0 = z0 = 0.

A.3 The GJR(1, 1) model with Student-t innovations
163
Proposition A.2. Upon deﬁning the following recursive transformations:
l∗
t
.= 1 + βl∗
t−1
v∗
t
.= vt−1I{yt−1⩾0} + β v∗
t−1
v∗∗
t
.= vt−1I{yt−1<0} + β v∗∗
t−1
(A.5)
where l∗
0 = v∗
0 = v∗∗
0
.= 0, expression (A.4) can be written as follows:
zt = vt −(l∗
t v∗
t v∗∗
t )α
(A.6)
where α .= (α0 α1 α2)′. The function zt(α) in (A.4) can therefore be expressed
as a linear function of the 3 × 1 vector α.
Proof. By induction:
Beginning step:
For t = 1, we have:
v1 −(l∗
1 v∗
1 v∗∗
1 )α
(A.5)
=
v1 −(1 0 0)α = v1 −α0
(A.4)
=
z1 .
Assumption step:
Let us assume that expression (A.6) is satisﬁed for t = k.
Induction step:
For t = k + 1, we have:
vk+1 −(l∗
k+1 v∗
k+1 v∗∗
k+1)α
(A.5)
=
vk+1 −(1 + βl∗
k
vkI{yk⩾0} + βv∗
k
vkI{yk<0} + βv∗∗
k )α
=
vk+1 −α0 −α1vkI{yk⩾0} −α2vkI{yk<0}
−β(α0l∗
k + α1v∗
k + α2v∗∗
k )
=
vk+1 −α0 −α1vkI{yk⩾0} −α2vkI{yk<0} −β(l∗
k v∗
k v∗∗
k )α
(A.6)
=
vk+1 −α0 −α1vkI{yk⩾0} −α2vkI{yk<0} −β(vk −zk)
=
vk+1 −α0 −(α1I{yk⩾0} + α2I{yk<0} + β)vk + βzk
(A.4)
=
zk+1 .
⊓⊔
A.3 The GJR(1, 1) model with Student-t innovations
As shown in Sect. 5.2.2, the GJR(1, 1) model with Student-t innovations, de-
noted by {yt}, can be transformed in a new sequence {vt}, where vt .= y2
t
τt with
τt .= ϖtϱ and ϱ .= ν−2
ν . The process {vt} can then be approximated as follows:
vt = α0 + (α1I{yt−1⩾0} + α2I{yt−1<0})τt−1vt−1 + βvt−1 −βzt−1 + zt

164
A Recursive Transformations
where {zt} is a Martingale Diﬀerence process. From this expression, the variable
zt can be written as:
zt = vt −α0 −

(α1I{yt−1⩾0} + α2I{yt−1<0})τt−1 + β

vt−1 + βzt−1
(A.7)
where v0 = z0 = 0.
Proposition A.3. Upon deﬁning the following recursive transformations:
l∗
t
.= 1 + βl∗
t−1
v∗
t
.= y2
t−1I{yt−1⩾0} + β v∗
t−1
v∗∗
t
.= y2
t−1I{yt−1<0} + β v∗∗
t−1
(A.8)
where l∗
0 = v∗
0 = v∗∗
0
.= 0, expression (A.7) can be written as follows:
zt = vt −(l∗
t v∗
t v∗∗
t )α
(A.9)
where α .= (α0 α1 α2)′. The function zt(α) in (A.7) can therefore be expressed
as a linear function of the 3 × 1 vector α.
Proof. By induction:
Beginning step:
For t = 1, we have:
v1 −(l∗
1 v∗
1 v∗∗
1 )α
(A.8)
=
v1 −(1 0 0)α = v1 −α0
(A.7)
=
z1 .
Assumption step:
Let us assume that expression (A.9) is satisﬁed for t = k.
Induction step:
For t = k + 1, we have:
vk+1 −(l∗
k+1 v∗
k+1 v∗∗
k+1)α
(A.8)
=
vk+1 −(1 + βl∗
k
y2
kI{yk⩾0} + βv∗
k
y2
kI{yk<0} + βv∗∗
k )α
=
vk+1 −α0 −α1y2
kI{yk⩾0} −α2y2
kI{yk<0}
−β(α0l∗
k + α1v∗
k + α2v∗∗
k )
=
vk+1 −α0 −α1y2
kI{yk⩾0} −α2y2
kI{yk<0} −β(l∗
k v∗
k v∗∗
k )α
(A.9)
=
vk+1 −α0 −α1y2
kI{yk⩾0} −α2y2
kI{yk<0} −β(vk −zk)
=
vk+1 −α0 −α1y2
kI{yk⩾0} −α2y2
kI{yk<0} −βvk + βzk
=
vk+1 −α0 −

(α1I{yk⩾0} + α2I{yk<0})τk + β

vk + βzk
(A.7)
=
zk+1 .
⊓⊔

B
Equivalent Speciﬁcation
In this appendix, we demonstrate the equivalent speciﬁcation introduced at the
end of Sect. 5.2.
Proposition B.1. The following model:
yt = εt(ϖtht)1/2
for t = 1, . . . , T
εt
iid
∼N(0, 1)
ϖt
iid
∼IG
ν
2, ν −2
2

(B.1)
where ht .= ht(α, β) is a GARCH scedastic function, is equivalent to:
yt = εth1/2
t
for t = 1, . . . , T
εt
iid
∼S∗(0, 1)
where S∗(0, 1) denotes the standardized Student-t density, i.e., its variance is
one.
Proof. First, let us regroup the model parameters into ψ .= (α, β, ν) for nota-
tional purposes. In speciﬁcation (B.1), the variables ϖt are independent and
identically distributed from an Inverted Gamma density with parameters ν
2 and
ν−2
2 :
p(ϖt | ν) =
ν −2
2
 ν
2 h
Γ
ν
2
i−1
ϖ
−ν
2 −1
t
exp

−(ν −2)
2ϖt

and the joint density of the T ×1 vector ϖ .= (ϖ1 · · · ϖT )′ is therefore given by:
p(ϖ | ν) =
ν −2
2
 T ν
2 h
Γ
ν
2
i−T
T
Y
t=1
ϖ
−ν
2 −1
t
exp

−(ν −2)
2ϖt

.
(B.2)
Based on the T × 1 vector of observations y .= (y1 · · · yT )′, we can express the
likelihood function of (ψ, ϖ) as follows:

166
B Equivalent Speciﬁcation
L(ψ, ϖ | y) ∝
T
Y
t=1
(ϖtht)−1/2 exp

−1
2
y2
t
ϖtht

and, by using the Bayes theorem, we obtain the following joint posterior:
p(ψ, ϖ | y) ∝
ν −2
2
 T ν
2 h
Γ
ν
2
i−T
 T
Y
t=1
h−1/2
t
!
×
T
Y
t=1
ω
−(ν+3)
2
t
exp

−1
2
y2
t
ωtht

.
(B.3)
Now, by using the following result:
Z ∞
0
x−a/2 exp

−b
2x

dx = Γ
a −2
2

×
2
b
 a−2
2
we can integrate (B.3) with respect to vector ϖ to get the following expression:
p(ψ | y) ∝
ν −2
2
 T ν
2 h
Γ
ν
2
i−T 
Γ
ν + 1
2
T
× 2
T (ν+1)
2
T
Y
t=1
h−1/2
t
b
−(ν+1)
2
t
(B.4)
where:
bt .= y2
t
ht
+ (ν −2) = (ν −2) ×

1 +
y2
t
(ν −2)ht

.
Some simpliﬁcations of expression (B.4) yield:
"
Γ
  ν+1
2

Γ
  ν
2

(ν −2)1/2
#T
T
Y
t=1
h−1/2
t

1 +
y2
t
(ν −2)ht
−(ν+1)
2
which if proportional to the likelihood function of parameters ψ when observa-
tions yt
iid
∼S∗(0, ht) where ht .= ht(α, β).
⊓⊔
The speciﬁcation (B.1) gives an additional way of handling the Bayesian estima-
tion of GARCH models with Student-t innovations. It has the appealing aspect
that no additional scale factor ϱ .= ν−2
ν
needs to be included in the modeling.
However, the simulation scheme presented in Deschamps [2006], Geweke [1993]
needs to be slightly modiﬁed, as shown hereafter.
In our application, we aim to generate eﬃciently draws for the degrees of
freedom parameter ν. The target density is obtained as follows:

B Equivalent Speciﬁcation
167
p(ν | ϖ) ∝p(ϖ | ν)p(ν)
=
ν −2
2
 T ν
2 h
Γ
ν
2
i−T
 T
Y
t=1
ϖ
−(ν+2)
2
t
!
× exp
"
−(ν −2)
2
T
X
t=1
ϖ−1
t
#
λ exp

−λ(ν −δ)

I{ν>δ}
where we can express QT
t=1 ϖ
−(ν+2)
2
t
as:
T
Y
t=1
ϖ
−(ν+2)
2
t
=
T
Y
t=1
exp

ln ϖ
−(ν+2)
2
t

=
T
Y
t=1
exp

−(ν + 2)
2
ln ϖt

= exp
"
−(ν + 2)
2
T
X
t=1
ln ϖt
#
= exp
"
−ν
2
T
X
t=1
ln ϖt −
T
X
t=1
ln ϖt
#
∝exp
"
−ν
2
T
X
t=1
ln ϖt
#
.
This allows to express the kernel of the target density as follows:
k(ν) .=
ν −2
2
 T ν
2 h
Γ
ν
2
i−T
exp[−ϕν]I{ν>δ}
where:
ϕ .= 1
2
T
X
t=1

ln ϖt + ϖ−1
t

+ λ .
Note that, since the function ln ϖ + ϖ−1 is minimized at ϖ = 1, we have that
ϕ ⩾T
2 + λ > T
2 .
Following Deschamps [2006], the sampling density is a translated Exponen-
tial with kernel density function given by:
g(ν; µ, δ) .= µ exp [−µ(ν −δ)] I{ν>δ}
(B.5)
where the parameter µ is chosen to maximize the acceptance probability. Fol-
lowing Geweke [1993], we can determine the value for this parameter. Given the
usual regularity conditions, a necessary condition is that µ is part of a solution
of the following system:

168
B Equivalent Speciﬁcation
∂
∂ν

ln k(ν) −ln g(ν; µ, δ)

= 0
(B.6a)
∂
∂µ ln g(ν; µ, δ) = 0 .
(B.6b)
Expliciting (B.6a) yields:
T
2

ln
ν −2
2

+

ν
ν −2

−Ψ
ν
2

−ϕ + µ = 0
(B.7)
where Ψ(z)
.=
d ln Γ(z)
dz
denotes the Digamma function, while solving (B.6b)
yields:
ν = 1
µ + δ = 1 + µδ
µ
.
(B.8)
Furthermore, we note that in expression (B.7), the function:
ln
ν −2
2

+

ν
ν −2

−Ψ
ν
2

is monotone decreasing from ∞to 1 on the ]2, ∞[ interval. Hence, since ϕ > T
2 ,
there exists an unique µ satisfying (B.7). Now, inserting (B.8) in expression (B.7)
yields:
T
2

ln
1 + µ(δ −2)
2µ

+
1 + µδ
1 + µ(δ −2) + Ψ
1 + µδ
2µ

+ µ −ϕ = 0
and solving for µ gives the optimal parameter ¯µ for the eﬃcient sampling scheme.
The value ¯µ can be found by standard iterative methods. Then, a candidate ν⋆
is sampled from (B.5) with parameter ¯µ and accepted with probability:
p⋆.=
k(ν⋆)
s(¯µ, δ)g(ν⋆; ¯µ, δ)
where s(µ, δ) is given by:
s(µ, δ) .= k
1 + µδ
µ
 
g
1 + µδ
µ
; µ, δ
−1
=
1 + µ(δ −2)
2µ
 T (1+µδ)
2µ

Γ
1 + µδ
2µ
−T exp
h
1 −ϕ(1+µδ)
µ
i
µ
.
Substituting for k(ν⋆), s(µ, δ) and g(ν⋆; µ, δ) in the expression of the acceptance
probability yields:

B Equivalent Speciﬁcation
169
p⋆=
ν⋆−2
2
 T ν⋆
2

Γ
ν⋆
2
−T
exp[−ϕν⋆]
1 + ¯µ(δ −2)
2¯µ
−T (1+¯
µδ)
2¯
µ
×

Γ
1 + ¯µδ
2¯µ
T
¯µ exp
ϕ(1 + ¯µδ)
µ
−1
 exp

¯µ(ν⋆−δ)

¯µ
=


Γ

1+¯µδ
2¯µ

Γ
  ν⋆
2



T ν⋆−2
2
 T ν⋆
2
1 + ¯µ(δ −2)
2¯µ
−T (1+¯
µδ)
2¯
µ
× exp

(ν⋆−δ)(¯µ −ϕ) + ϕ
¯µ −1

.

C
Conditional Moments
In this appendix, we demonstrate the propositions for the conditional moments
of the cumulative return yt,s .= Ps
i=1 yt+i used in Sect. 6.2.2. We consider the
case where the process {yt} is described by a GARCH(1, 1) model for ease of
exposition but the methodology can be extended, upon modiﬁcations, to higher
order GARCH models as well as asymmetric speciﬁcations. We recall that the
scedastic function of the GARCH(1, 1) model is given by:
ht .= α0 + α1y2
t−1 + βht−1
where h0 = y0 .= 0 for convenience. For the model disturbances, we consider
standardized Normal and Student-t innovations. We deﬁne Et(•) .= E(• | Ft)
and suppress the dependence of the model parameters for notational pur-
poses. The GARCH(1, 1) parameters α .= (α0 α1)′ and β are regrouped into
ψ .= (α, β). In the case of Student-t innovations, ψ .= (α, β, ν). Moreover, the
p-th conditional moment of yt,s is denoted by κp.
The following properties will be used henceforth:
A. the errors εt are iid (i.e., independent and identically distributed);
B. Et(εt+i) = 0 for i ⩾1 (i.e., centered distribution);
C. Et(ε2
t+i) = 1 for i ⩾1 (i.e., unit variance);
D. Et(ε3
t+i) = 0 for i ⩾1 (i.e., symmetric distribution);
E. the conditional variance ht is known given Ft−1, i.e., it is pre-
dictable with respect to the natural ﬁltration of the process {yt}.
To keep the calculations similar for both Normal and Student-t innovations,
we assume unit variance for the disturbances εt as emphasized in property C.
This has an implication for the fourth conditional moment of the innovations
κε
.= Et(ε4
t+i) for i ⩾1. Indeed, in the Normal case, κε = 3 while in the
normalized Student-t case, κε = 3(ν −2)/(ν −4). In comparison, the fourth
moment of the usual Student-t is 3ν2/(ν −4)(ν −2).

172
C Conditional Moments
Proposition C.1 (First conditional moment). For horizon s ⩾1, the value
of the ﬁrst conditional moment κ1 is zero.
Proof.
κ1 .= Et(yt,s) = Et
 s
X
i=1
yt+i
!
=
s
X
i=1
Et(yt+i) = 0
since for 1 ⩽i ⩽s we have:
Et(yt+i) = Et(εt+ih1/2
t+i)
A= Et(εt+i)Et(h1/2
t+i)
B= 0 .
⊓⊔
Proposition C.2 (Second conditional moment).
For horizon s ⩾2, the
value of the second conditional moment κ2 is:
κ2 =
s
X
i=1
Et(ht+i)
where Et(ht+i) = α0 + ρ1Et(ht+i−1) with ρ1 .= (α1 + β).
Proof. For horizon s ⩾2, the second power of the cumulative log-returns yt,s is
given by:
y2
t+s =
X
i1, ... ,is
i1+ ... +is=2
2 !
i1! . . . is! × yi1
t+1 · · · yis
t+s
=
s
X
i=1
y2
t+i + 2
X
1⩽i,j⩽s
i<j
yt+i yt+j
and the second conditional moment κ2 is obtained by taking the conditional
expectation as follows:
κ2 =
s
X
i=1
Et(y2
t+i) + 2
X
1⩽i,j⩽s
i<j
Et(yt+i yt+j) .
(C.1)
Let us consider the ﬁrst term in expression (C.1). For 1 ⩽i ⩽s, we have:
Et(y2
t+i) = Et(ε2
t+iht+i)
A= Et(ε2
t+i)Et(ht+i)
C= Et(ht+i) .
For the second term in expression (C.1), since i < j, we have:
Et(yt+i yt+j) = Et(yt+i εt+jh1/2
t+j)
A= Et(yt+ih1/2
t+j)Et(εt+j)
B= 0 .
Hence, expression (C.1) simpliﬁes to κ2 = Ps
i=1 Et(ht+i) where Et(ht+i) can be
expressed recursively as follows:

C Conditional Moments
173
Et(ht+i) = α0 + ρ1Et(ht+i−1)
with Et(ht+1) = ht+1 since this value is known given Ft. The parameter ρ1 is a
function of the model parameters ψ. Its expression is found by noting ﬁrst that
the conditional variance at time t can be written as follows:
ht+i = α0 + α1y2
t+i−1 + βht+i−1
= α0 + (α1ε2
t+i−1 + β) ht+i−1
= α0 + ϕt+i−1ht+i−1
(C.2)
where ϕt
.= (α1ε2
t + β). By taking the conditional expectation of (C.2), we
obtain:
Et(ht+i) = α0 + Et(ϕt+i−1ht+i−1)
A= α0 + Et(ϕt+i−1)Et(ht+i−1)
= α0 + ρ1Et(ht+i−1)
where ρ1 .= Et(ϕt+i−1) = Et(α1ε2
t+i−1 + β) = α1 Et(ε2
t+i−1) + β
C= α1 + β.
⊓⊔
Proposition C.3 (Third conditional moment).
For horizon s ⩾3, the
value of the third conditional moment κ3 is zero.
Proof. For s ⩾3, the third power of the cumulative log-returns yt,s is given by:
y3
t,s =
X
i1, ... ,is
i1+ ... +is=3
3 !
i1! . . . is! × yi1
t+1 · · · yis
t+s
=
s
X
i=1
y3
t+i + 3
X
1⩽i,j⩽s
i̸=j
y2
t+i yt+j + 6
X
1⩽i,j,k⩽s
i<j<k
yt+i yt+j yt+k
and the third conditional moment κ3 is obtained by taking the conditional ex-
pectation as follows:
κ3 =
s
X
i=1
Et(y3
t+i) + 3
X
1⩽i,j⩽s
i̸=j
Et(y2
t+i yt+j) + 6
X
1⩽i,j,k⩽s
i<j<k
Et(yt+i yt+j yt+k) .
(C.3)
Let us consider the ﬁrst term in expression (C.3). For 1 ⩽i ⩽s, we have:
Et(y3
t+i) = Et(ε3
t+ih3/2
t+i)
A= Et(ε3
t+i)Et(h3/2
t+i)
D= 0 .
For the second term, we need to distinguish two cases. First, when i < j, we
obtain:
Et(y2
t+i yt+j) = Et(y2
t+i εt+jh1/2
t+j)
A= Et(y2
t+ih1/2
t+j)Et(εt+j)
B= 0 .

174
C Conditional Moments
In the case where j < i, we have:
Et(y2
t+i yt+j) = Et(ε2
t+iht+i yt+j)
A= Et(ε2
t+i)Et(ht+iyt+j)
C= Et(ht+iyt+j)
= Et(ht+iεt+jh1/2
t+j)
= Et
 Et+j−1[ht+iεt+jh1/2
t+j]

E= Et
 h1/2
t+j Et+j−1[ht+iεt+j]

(C.4)
where the conditional expectation Et+j−1[•] in (C.4) can be expressed as:
Et+j−1[εt+jht+i]
(C.2)
=
Et+j−1

εt+j(α0 + ϕt+i−1ht+i−1)

B=
Et+j−1[εt+jϕt+i−1ht+i−1]
A=
Et+j−1[εt+jht+i−1]Et+j−1[ϕt+i−1]
=
ρ1Et+j−1[εt+jht+i−1]
...
=
ρi−j
1
Et+j−1[εt+jϕt+jht+j]
E=
ρi−j
1
ht+jEt+j−1[εt+jϕt+j]
=
0 .
(C.5)
The last equality follows from:
Et+j−1

εt+jϕt+j

= Et+j−1

εt+j(α1ε2
t+j + β)

= α1 Et+j−1[ε3
t+j] + β Et+j−1[εt+j]
B,D
= 0 .
Finally, let us consider the last term in expression (C.3). Since i < j < k, we
have:
Et(yt+i yt+j yt+k)
A= Et(yt+i yt+jh1/2
t+k)Et(εt+k)
B= 0 .
Each term in expression (C.3) vanishes so that the third conditional moment is
zero.
⊓⊔
Proposition C.4 (Fourth conditional moment).
For horizon s ⩾4, the
value of the fourth conditional moment κ4 is:
κ4 = κε
s
X
i=1
Et(h2
t+i) + 6
s−1
X
i=1
s
X
j=i+1
Et(y2
t+i y2
t+j)

C Conditional Moments
175
where:
Et(h2
t+i) = α2
0 + τ1 Et(ht+i−1) + τ2 Et(h2
t+i−1)
Et(y2
t+i y2
t+j) = α0
 
1 −ρj−i
1
1 −ρ1
!
Et(ht+i) + ρj−i−1
1
ρ2 Et(h2
t+i)
with ρ1 .= (α1 + β), ρ2 .= (κεα1 + β), τ1 .= 2α0ρ1 and τ2 .= κεα2
1 + β(2α1 + β).
Proof. For s ⩾4, the fourth power of the cumulative log-returns yt,s is given
by:
y4
t,s =
X
i1, ... ,is
i1+ ... +is=4
4 !
i1! . . . is! × yi1
t+1 · · · yis
t+s
=
s
X
i=1
y4
t+i + 4
X
1⩽i,j⩽s
i̸=j
y3
t+i yt+j + 6
X
1⩽i,j⩽s
i<j
y2
t+i y2
t+j
+ 6
X
1⩽i,j,k⩽s
i̸=j̸=k
y2
t+i yt+j yt+k
+ 24
X
1⩽i,j,k,l⩽s
i<j<k<l
yt+i yt+j yt+k yt+l .
and the fourth conditional moment κ4 is obtained by taking the conditional
expectation as follows:
κ4 =
s
X
i=1
Et(y4
t+i) + 4
X
1⩽i,j⩽s
i̸=j
Et(y3
t+i yt+j) + 6
X
1⩽i,j⩽s
i<j
Et(y2
t+i y2
t+j)
+ 6
X
1⩽i,j,k⩽s
i̸=j̸=k
Et(y2
t+i yt+j yt+k)
+ 24
X
1⩽i,j,k,l⩽s
i<j<k<l
Et(yt+i yt+j yt+k yt+l)
(C.6)
Let us ﬁrst consider the terms in (C.6) which vanish. For the second term in
expression (C.6), when i < j, we have:
Et(y3
t+i yt+j) = Et(y3
t+i εt+jh1/2
t+j)
A= Et(y3
t+i h1/2
t+j)Et(εt+j)
B= 0
while in the case where j < i, we obtain:
Et(y3
t+i yt+j) = Et(ε3
t+ih3/2
t+i yt+j)
A= Et(ε3
t+i)Et(h3/2
t+i yt+j)
D= 0 .

176
C Conditional Moments
Let us now consider the fourth term in expression (C.6). Assuming that i < j < k
yields:
Et(y2
t+i yt+j yt+k)
A= Et(y2
t+i yt+j h1/2
t+k)Et(εt+k)
B= 0 .
The same holds for i < k < j, j < i < k and k < i < j. It remains to consider
the cases where i is the greatest integer, i.e., k < j < i and j < k < i. By
assuming (without loss of generality) that j < k < i we have:
Et(y2
t+i yt+j yt+k)
=
Et(ε2
t+iht+i yt+j yt+k)
A=
Et(ε2
t+i)Et(ht+i yt+j yt+k)
C=
Et(ht+i yt+j yt+k)
=
Et
 Et+j−1[ht+i εt+jh1/2
t+j yt+k]

F=
Et
 h1/2
t+j Et+j−1[ht+i εt+j yt+k]

=
Et

h1/2
t+j Et+j−1

Et+k−1{ht+i εt+j εt+kh1/2
t+k}

F=
Et

h1/2
t+j Et+j−1

h1/2
t+k εt+j Et+k−1{ht+i εt+k}

(C.5)
=
0 .
Finally, let us consider the last term in expression (C.6). By assuming (without
loss of generality) that i < j < k < l, we have:
Et(yt+i yt+j yt+k yt+l)
A= Et(yt+i yt+j yt+k h1/2
t+l)Et(εt+l)
B= 0 .
With these intermediate results, expression (C.6) can be simpliﬁed as follows:
κ4 =
s
X
i=1
Et(y4
t+i) + 6
X
1⩽i,j⩽s
i<j
Et(y2
t+i y2
t+j)
=
s
X
i=1
Et(y4
t+i) + 6
s−1
X
i=1
s
X
j=i+1
Et(y2
t+i y2
t+j) .
(C.7)
Let us explicit the ﬁrst term in expression (C.7). We have:
Et(y4
t+i) = Et(ε4
t+ih2
t+i)
A= Et(ε4
t+i)Et(h2
t+i) = κε Et(h2
t+i) .
The term Et(h2
t+i) can be estimated by recursion as follows:

C Conditional Moments
177
Et(h2
t+i)
(C.2)
=
Et
 [α0 + ϕt+i−1ht+i−1]2
=
α2
0 + 2α0 Et(ϕt+i−1ht+i−1) + Et(ϕ2
t+i−1h2
t+i−1)
A=
α2
0 + 2α0 Et(ϕt+i−1)Et(ht+i−1) + Et(ϕ2
t+i−1)Et(h2
t+i−1)
=
α2
0 + τ1 Et(ht+i−1) + τ2 Et(h2
t+i−1)
(C.8)
where τ1 .= 2α0ρ1, ρ1 .= Et(ϕt+i−1) and τ2 .= Et(ϕ2
t+i−1). Expression (C.8) can
be computed recursively since ht+1 is known given Ft. Furthermore, we can
explicit the term τ2 as follows:
τ2
.= Et(ϕ2
t+i−1)
= Et(α2
1ε4
t+i−1 + 2α1βε2
t+i−1 + β2)
= α2
1 Et(ε4
t+i−1) + 2α1β Et(ε2
t+i−1) + β2
C,E
= κεα2
1 + 2α1β + β2
= κεα2
1 + β(2α1 + β) .
It remains to consider the last term in expression (C.6). Since i < j, we have:
Et(y2
t+i y2
t+j) = Et(y2
t+i ε2
t+jht+j)
A= Et(y2
t+i ht+j)Et(ε2
t+j)
C= Et(y2
t+i ht+j)
= Et
 Et+i−1[ε2
t+iht+i ht+j]

F= Et
 ht+i Et+i−1[ε2
t+i ht+j]

(C.9)
where the conditional expectation Et+i−1[•] in the last equality can be developed
as follows:

178
C Conditional Moments
Et+i−1[ε2
t+i ht+j]
(C.2)
=
Et+i−1

ε2
t+i (α0 + ϕt+j−1ht+j−1)

=
α0 Et+i−1[ε2
t+i] + Et+i−1[ε2
t+iϕt+j−1ht+j−1]
A,C
=
α0 + Et+i−1[ε2
t+iht+j−1]Et+i−1[ϕt+j−1]
=
α0 + ρ1Et+i−1[ε2
t+iht+j−1]
...
=
α0(1 + . . . + ρj−i−2
1
) + ρj−i−1
1
Et+i−1[ε2
t+iht+i+1]
=
α0
j−i−2
X
k=0
ρk
1 + ρj−i−1
1
Et+i−1

ε2
t+i (α0 + ϕt+iht+i)

C=
α0
j−i−2
X
k=0
ρk
1 + α0ρj−i−1
1
+ ρj−i−1
1
Et+i−1[ε2
t+iϕt+iht+i]
E=
α0
j−i−1
X
k=0
ρk
1 + ρj−i−1
1
ht+i Et+i−1[ε2
t+iϕt+i]
=
α0
 
1 −ρj−i
1
1 −ρ1
!
+ ρj−i−1
1
ρ2 ht+i
(C.10)
with ρ2 .= Et+i−1[ε2
t+iϕt+i]. The parameter ρ2 is a function of the model pa-
rameters:
ρ2
.= Et+i−1[ε2
t+iϕt+i]
= Et+i−1

ε2
t+i(α1ε2
t+i + β)

= α1 Et+i−1[ε4
t+i] + β Et+i−1[ε2
t+i]
C,E
= κεα1 + β .
Replacing (C.10) in expression (C.9) yields:
Et(y2
t+i y2
t+j) = Et
 ht+i Et+i−1[ε2
t+i ht+j]

= α0
 
1 −ρj−i
1
1 −ρ1
!
Et(ht+i) + ρj−i−1
1
ρ2 Et(h2
t+i) .
which can be computed recursively since ht+1 is known given Ft.
⊓⊔

Computational Details
The algorithms have been written in the R language, version 2.4.1 [see R De-
velopment Core Team 2007], with some subroutines implemented in C in order
to speed up the simulation procedure; this is required in the case of GARCH
models due to the recursive nature of the conditional process which drastically
slows down the computations with high level interpreted languages. Moreover,
the validity of the algorithms as well as the correctness of the computer code
were veriﬁed by a variant of the method proposed by Geweke [2004]. We refer
the reader to the end of Sect. 3.2.2 for further details. The R packages boa
0-1.3, mvtnorm 0-7.5, coda 0.10-7, sandwich 2.0-0, lmtest 0.9-18, MASS
7.2-32 have also been used to perform speciﬁc tasks. The R program itself and
packages are available from CRAN at http://CRAN.R-project.org.
Regarding the ML estimation of the models, the likelihood functions were
maximized using the R function nlminb which performs unconstrained and con-
strained optimization using PORT routines. In some cases, the procedure was
initialized using the DEoptim function provided by the R package DEoptim
1.01-2 [see Ardia 2007b]. This function performs a global (robust) optimization
based on the Diﬀerential Evolution algorithm. The reader is referred to Price,
Storn, and Lampinen [2006] for further details.
Finally, we note that the R package bayesGARCH will soon be available
from CRAN [see Ardia 2007a]. This package allows the Bayesian estimation of
the GARCH(1, 1) model with Student-t innovations. The underlying algorithm is
based on Nakatsuma [1998, 2000] for the generation of the scedastic function’s
parameters. The generation of the degrees of freedom parameter is achieved
following Deschamps [2006], Geweke [1993]. By using a translated Exponential as
a prior on the degrees of freedom parameter, Normal innovations can be obtained
as a special case of the sampler. Moreover, the function addPriorConditions
allows to add any constraints on the model parameters in the M-H sampler.

Abbreviations and Notations
Most of the notation as well as dimensions of vectors and matrices are clearly
deﬁned in the text where it is used. Occasionally, a mathematical symbol has
been assigned to a diﬀerent object.
Abbreviations
ACF
Autocorrelation function
AEL
Absolute error loss
ARCH
AutoRegressive Conditional Heteroscedasticity
ARDS
Adaptive Radial-Based Direction Sampling
AR
AutoRegressive
ARMA
AutoRegressive Moving Average
BIC
Bayesian information criterion
BF
Bayes factor
BUGS
Bayesian analysis Using Gibbs Software
cont.
Continued
CC
Conditional coverage
CSC
Covariance stationarity condition
DEM/GBP
Deutschmark vs British Pounds
DIC
Deviance information criterion
EM
Expectation Maximization
ES
Expected Shortfall
FFBS
Forward Filtering Backward Sampling
GARCH
Generalized ARCH
GJR
Asymmetric GARCH
IF
Ineﬃciency factor
IND
Independence
Linex
Linex loss
MCMC
Markov Chain Monte Carlo
M-H
Metropolis-Hastings
continued on the next page

182
Abbreviations and Notations
Abbreviations (cont.)
ML
Maximum Likelihood
MS-GARCH
Markov-switching GARCH
MS-GJR
Markov-switching GJR
NA
Not applicable
NSE
Numerical standard error
P&L
Proﬁt and loss
SEL
Squared error loss
SMI
Swiss Market Index
S&P100
Standard & Poors 100
S&P500
Standard & Poors 500
SSC
Strict stationarity condition
UC
Unconditional Coverage
VaR
Value at Risk
VIX
Volatility index of the S&P100 index
Densities
N(0, 1)
Univariate standard Normal density
Nd(µ, Σ)
d-dimensional Normal density with mean vector µ and
covariance matrix Σ
S(µ, σ2, ν)
Univariate standard Student-t density with mean µ, scale
parameter σ2 and degrees of freedom ν
SS(µ, σ2, τ, γ)
Univariate skewed Student-t density with mean µ, scale
parameter σ2, degrees of freedom τ and asymmetry pa-
rameter γ
IG(a, b)
Inverted Gamma density with shape parameter a and rate
parameter b
D(η)
Dirichlet density with parameter η
χ2
k
Chi-squared density with k degrees of freedom
Notations used throughout the book
∼
“is generated from”
∝
“is proportional to”
≻
“is preferred over”
≈
“is approximately equal to”
.=
“is deﬁned to”
≡
“is equivalent to”
∈
“belongs to”
iid
Independent and identically distributed
R
Set of real numbers
R∗
Set of non-zero real numbers
R+
Set of real positive numbers
continued on the next page

Abbreviations and Notations
183
Notations used throughout the book (cont.)
N∗
Set of non-zero natural numbers
•
A scalar, a vector, a matrix, a parameter or a set of pa-
rameters (depending on the context)
exp[•]
Exponential
ln[•]
Natural logarithm
•!
Factorial
| • |
Absolute value
P
Summation operator
Q
Product operator
×
Multiplication operator or Cartesian product
R
Integral
•̸=i
Vector without its ith component
{•}
Sequence (or set) of variables (or observations)
max{•}
Maximum value of a set
min{•}
Minimum value of a set
inf{•}
Inﬁmum of a set
#{•}
Number of elements in a set
det(•)
Determinant of a square matrix
•−1
Inverse of a number or a square matrix
Γ(•)
Gamma function
Ψ(•)
Digamma function
Id
Identity matrix of size d
Ft
Information set up to time t
I(d)
Integrated of order d
I{•}
Indicator function
•′
Transpose of a vector or a matrix
•MLE
Maximum Likelihood estimate
•
Sample average
•φ
φth percentile of a sample
L(•)
Likelihood function
L(• | y)
Marginal (observed) likelihood function
f•(•)
Density function
F•(•)
Distribution function
p(• | y)
Posterior density
P(•)
Probability
P(• | •)
Conditional probability
E(•)
Expectation
E(• | •), E•(•)
Conditional expectations
bR
Potential scale reduction factor
0
Vector of zeros
α0 α1 (α2)
ARCH parameters
α
Vector of ARCH parameters
β
GARCH parameter
γ
Vector of linear regression’s parameters
continued on the next page

184
Abbreviations and Notations
Notations used throughout the book (cont.)
ν
Degrees of freedom parameter of the Student-t density
ϖt
Latent scale variable at time t
ϖ
Vector of latent scale variables
θ ψ Θ
Sets (or vectors) of the model parameters
α
α .= (α1 + α2)/2
△α
Leverage eﬀect coeﬃcient: △α .= (α2 −α1)
T
Length of the underlying time series (number of observa-
tions)
yt
Dependent variable at time t
y
Vector containing the observations of yt
εt
Model innovation at time t
bεt
Model residual at time t
ut
Linear regression’s error at time t
u
Vector of linear regression’s errors
m
Number of exogenous or lagged dependent variables
xt
Vector of exogenous or lagged dependent variables at time
t
X
Matrix whose tth row is x′
t
ht
Conditional variance at time t
hy
Unconditional variance of the underlying process
Σ Λ
Diagonal matrices of conditional variances
ϱ
A scaling factor: ϱ .= ν−2
ν
J
Number of draws in the posterior sample
q•(•)
Proposal density
e•
Previous draw in the sampler
•[j]
jth draw in the sampler
•⋆
New draw in the sampler
µ• Σ•
Hyperparameters of the (truncated) Normal priors
bµ• bΣ•
Parameters of the (truncated) Normal proposals
λ δ
Hyperparameters of the translated Exponential prior
κε
Kurtosis of the innovations
κy
Unconditional kurtosis of the underlying process
l∗
t v∗
t v∗∗
t
Recursive transformations
ct
Vector of recursive transformations
C
Matrix whose tth row is c′
t
V φ
t
Indicator variable at time t for the violation of the one-
day ahead VaR at risk level φ
Notations speciﬁc to Chap. 6
φ φc
Risk level of the VaR and φc .= 1 −φ
s
Time horizon (in days)
yt,s
Cumulated return over a s-day ahead horizon at time t
continued on the next page

Abbreviations and Notations
185
Notations speciﬁc to Chap. 6 (cont.)
yt:s
Vector of s-day ahead log-returns at time t
κp(•)
pth conditional moment
zφ
φth percentile of the N(0, 1) density
tφ(ν)
φth percentile of the S(0, 1, ν) density
L
Loss function
ω
True value of the forecast (or one-dimensional parameter)
bω
Point estimate of ω
bωL
Optimal (Bayes) point estimate under loss function L
RL (• | y)
Posterior risk function under loss L
a
Parameter of the Linex loss function
a1 a2 q
Parameters of the Monomial loss function
∆
Statistical error: ∆.= (bω −ω)
N
Number of out-of-sample observations
d
VaR
VaR point estimate
VaRφ
t
One-day ahead VaR at time t for risk level φ
VaRφ
t,s
s-day ahead VaR at time t for risk level φ
d
VaR
φ
L ,t,s
s-day ahead VaR point estimate at time t for risk level φ
and loss function L
c
RCL ,t
Regulatory capital point estimate at time t for loss func-
tion L
V φ
t,s
Indicator variable at time t for the violation of the s-day
ahead VaR at risk level φ
ESφ
t
One-day ahead ES at time t for risk level φ
ESφ
t,s
s-day ahead ES at time t for risk level φ
Notations speciﬁc to Chap. 7
⊗
Kronecker product
⊙
Hadamard product, i.e., element-by-element multiplica-
tion
vec(•)
Vectorization (column stacking) of a matrix
tr(•)
Trace of a square matrix
K
Number of regimes
αk
0 αk
1 αk
2
ARCH parameters in state k
α0 α1 α2
Vectors of ARCH parameters
α
Vector containing the vectors of ARCH parameters
βk
GARCH parameter in state k
β
Vector of GARCH parameters
et
stth column of the matrix IK
ιK
Vector of ones of size K
ht
Vector of conditional variances at time t
ηi
Parameter’s vector of the ith Dirichlet density
continued on the next page

186
Abbreviations and Notations
Notations speciﬁc to Chap. 7 (cont.)
[•]i
ith component of a vector
π
Vector of ergodic probabilities
P
Matrix of transition probabilities
st
Discrete state variable estimated at time t
s
Vector of state variables
zt
Approximate p-score at time t
ut
Generalized residual at time t
D(•)
Deviance function
pD
Eﬀective number of parameters
B
Number of replications for the block bootstrap
q(ψ)
Importance density for parameter ψ
p(y)
Model likelihood
pt(y)
tth estimate of the model likelihood in the bridge sampling
p0(y)
Reciprocal sampling estimator of p(y)

List of Tables
3.1
Estimation results for the GARCH(1, 1) model with Normal
innovations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.2
Results of the sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
4.1
Estimation results for the linear regression model with
Normal-GJR(1, 1) errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
4.2
Results of the sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
5.1
Estimation results for the Student-t-GJR(1, 1) model . . . . . . . . . . . 66
5.2
Results of the sensitivity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.1
Average deviations of the VaR point estimates from the SEL
benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
6.2
Average deviations of the regulatory capital point estimates
from the SEL benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.3
Forecasting results of the VaR point estimates . . . . . . . . . . . . . . . . . 103
7.1
Estimation results for the GJR and MS-GJR models . . . . . . . . . . . . 127
7.2
Results of the DIC and BIC criteria . . . . . . . . . . . . . . . . . . . . . . . . . . 136
7.3
Results of the model likelihood estimators . . . . . . . . . . . . . . . . . . . . . 142
7.4
Forecasting results of the VaR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
7.5
Forecasting results of the VaR point estimates for the MS-GJR
model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150

List of Figures
3.1
DEM/GBP foreign exchange daily log-returns and sample
autocorrelogram of the squared log-returns . . . . . . . . . . . . . . . . . . . . 23
3.2
Running means of the chains over iterations . . . . . . . . . . . . . . . . . . . 27
3.3
Marginal posterior densities of the GARCH(1, 1) parameters . . . . . 28
3.3
Marginal posterior densities of the GARCH(1, 1) parameters
(cont.) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.4
Residuals time series and Normal quantile-quantile plot . . . . . . . . . 33
3.5
Posterior density of the persistence and posterior
autocorrelogram of the squared observations . . . . . . . . . . . . . . . . . . . 35
3.6
Posterior densities of the covariance stationarity and strict
stationarity conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.1
S&P100 index log-returns and VIX level . . . . . . . . . . . . . . . . . . . . . . 46
4.2
Marginal posterior densities of the GJR(1, 1) parameters . . . . . . . . 49
4.2
Marginal posterior densities of the GJR(1, 1) parameters (cont.) . . 50
4.3
Posterior density of the leverage eﬀect parameter . . . . . . . . . . . . . . . 51
4.4
Posterior density of the unconditional variance and asymptotic
Normal approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
5.1
Comparison between the ML and the Bayesian approaches . . . . . . 67
5.1
Comparison between the ML and the Bayesian approaches (cont.)
68
5.2
Prior and posterior densities of the degrees of freedom parameter . 69
6.1
Cornish-Fisher and Student-t approximations . . . . . . . . . . . . . . . . . . 84
6.2
Linex loss function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
6.3
Monomial loss function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
6.4
Estimation and forecasting windows . . . . . . . . . . . . . . . . . . . . . . . . . . 94
6.5
Term structures of the VaR density . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
6.6
Term structures of the ES density . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
7.1
SMI daily log-returns and sample autocorrelogram of the
squared log-returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124

190
List of Figures
7.2
Contour plots for (βk, αk
0), (βk, αk
1) and (βk, αk
2) . . . . . . . . . . . . . . . 129
7.3
Marginal posterior densities of the MS-GJR parameters . . . . . . . . . 130
7.4
Posterior densities of the covariance stationarity condition and
the unconditional variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
7.5
Smoothed probabilities of the high-volatility state together with
the in-sample log-returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
7.6
Importance density and marginal posterior density comparison . . . 143
7.7
Filtered probabilities of the high-volatility state together with
the out-of-sample log-returns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
7.8
Density of the one-day ahead VaR . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
7.9
Marginal posterior densities of the MS-GJR model parameters
and comparison with the asymptotic Normal approximation . . . . . 153

References
Andrews DWK (1991).
“Heteroskedasticity and Autocorrelation Consistent
Covariance Matrix Estimation.” Econometrica, 59(3), 817–858.
Cited on
pages 14, 26, and 126.
Andrews DWK, Monahan JC (1992). “An Improved Heteroskedasticity and Au-
tocorrelation Consistent Covariance Matrix Estimator.” Econometrica, 60(4),
953–966. Cited on pages 14, 26, and 126.
Ardia D (2007a). bayesGARCH: Bayesian Estimation of the GARCH(1,1) Model
with Student-t Innovations in R. R Foundation for Statistical Computing,
Vienna, Austria.
In preparation, URL http://stat.ethz.ch/CRAN/doc/
packages/bayesGARCH.PDF. Cited on page 179.
Ardia D (2007b). DEoptim: Diﬀerential Evolution Optimization. R Foundation
for Statistical Computing, Vienna, Austria. Version 1.1-8, URL http://stat.
ethz.ch/CRAN/doc/packages/DEoptim.PDF. Cited on page 179.
Artzner P, Delbaen F, Eber JM, Heath D (1999). “Coherent Measures of Risk.”
Quantitative Finance, 9(3), 203–228. Cited on page 104.
Aus´ın MC, Galeano P (2007). “Bayesian Estimation of the Gaussian Mixture
GARCH Model.” Computational Statistics and Data Analysis, 51(5), 2636–
2652. doi:10.1016/j.csda.2006.01.006. Cited on page 4.
Bams D, Lehnert T, WolﬀCCP (2005). “An Evaluation Framework for Alter-
native VaR Models.” Journal of International Money and Finance, 24(6),
944–958. doi:10.1016/j.jimonﬁn.2005.05.004. Cited on page 75.
Basel Committee on Banking Supervision (1995).
An Internal Model-Based
Approach to Market Risk Capital Requirements. The Bank for International
Settlements, Basel, Switzerland. URL http://www.bis.org/publ/bcbs17.
htm. Cited on page 73.
Basel Committee on Banking Supervision (1996a). Amendment to the Capital
Accord to Incorporate Market Risk. The Bank for International Settlements,
Basel, Switzerland. URL http://www.bis.org/publ/bcbs24.htm. Cited on
page 96.
Basel Committee on Banking Supervision (1996b).
Supervisory Framework
for the Use of Backtesting in Conjunction with the Internal Approach to
Market Risk Capital Requirement. The Bank for International Settlements,

192
References
Basel, Switzerland. URL http://www.bis.org/publ/bcbs22.htm. Cited on
pages 93, 100, and 144.
Bauwens L, Bos CS, van Dijk HK, van Oest RD (2004). “Adaptive Radial-
Based Direction Sampling: Some Flexible and Robust Monte Carlo Integration
Methods.” Journal of Econometrics, 123(2), 201–225. doi:10.1016/j.jeconom.
2003.12.002. Special issue on recent advances in Bayesian econometrics. Cited
on pages 4 and 5.
Bauwens L, Lubrano M (1998). “Bayesian Inference on GARCH Models Using
the Gibbs Sampler.” The Econometrics Journal, 1(1), C23–C46. doi:10.1111/
1368-423X.11003. Cited on pages 4 and 5.
Bauwens L, Preminger A, Rombouts JVK (2006). “Regime Switching GARCH
Models.” Discussion paper 2006/11, CORE and Department of Economics,
Universit´e catholique de Louvain. Cited on pages 4 and 159.
Bauwens L, Rombouts JVK (2007). “Bayesian Inference for the Mixed Condi-
tional Heteroskedasticity Model.” The Econometrics Journal, 10(2), 408–425.
doi:10.1111/j.1368-423X.2007.00213.x. Cited on page 4.
Berg A, Meyer R, Yu J (2004). “Deviance Information Criterion for Comparing
Stochastic Volatility Models.” Journal of Business and Economic Statistics,
22(1), 107–120. Cited on page 134.
Berkowitz J (2001). “Testing Density Forecasts, with Applications to Risk Man-
agement.” Journal of Business and Economic Statistics, 19(4), 465–474. Cited
on page 133.
Berkowitz J, Christoﬀersen PF, Pelletier D (2006). “Evaluating Value-at-Risk
Model with Desk-Level Data.” Working Paper Series 010, Department of
Economics, North Carolina State University.
URL http://ideas.repec.
org/p/ncs/wpaper/010.html. Cited on page 158.
Black F (1976). “The Pricing of Commodity Contracts.” Journal of Financial
Economics, 3(1–2), 167–179. doi:10.1016/0304-405X(76)90024-6. Cited on
pages 2 and 39.
Bollerslev T (1986). “Generalized Autoregressive Conditional Heteroskedastic-
ity.” Journal of Econometrics, 31(3), 307–327.
doi:10.1016/0304-4076(86)
90063-1. Cited on pages 1, 3, and 36.
Bollerslev T, Chou RY, Kroner K (1992). “ARCH Modeling in Finance: A
Review of the Theory and Empirical Evidence.” Journal of Econometrics,
52(1–2), 5–59. doi:10.1016/0304-4076(92)90064-X. Cited on page 2.
Bollerslev T, Engle RF, Nelson DB (1994). “ARCH Models.” In RF Engle,
DL McFadden (eds.), “Handbook of Econometrics,” volume 4, chapter 49, pp.
2959–3038. North Holland, Amsterdam, NL, ﬁrst edition. ISBN 0444887660.
doi:10.1016/S1573-4412(05)80019-4. Cited on page 2.
Bollerslev T, Ghysels E (1996). “Periodic Autoregressive Conditional Hetero-
scedasticity.” Journal of Business and Economic Statistics, 14(2), 139–151.
Cited on page 22.
Campbell JY, Lo AW, MacKinlay AC (1996). The Econometrics of Financial
Markets. Princeton University Press, Princeton, USA, ﬁrst edition. ISBN
0691043019. Cited on page 46.

References
193
Casella G, George EI (1992). “Explaining the Gibbs Sampler.” The American
Statistician, 46(3), 167–174. Cited on page 11.
Celeux G, Forbes F, Robert CP, Titterington M (2006). “Deviance Information
Criterion for Missing Data Models.” Bayesian Analysis, 1(4), 651–706. With
discussion and rejoinder. Cited on pages 134, 135, and 137.
Chib S (1996). “Calculating Posterior Distributions and Modal Estimates in
Markov Mixture Models.” Journal of Econometrics, 75(1), 79–97. doi:10.
1016/0304-4076(95)01770-4. Cited on pages 116, 117, 133, and 135.
Chib S, Greenberg E (1994).
“Bayes Inference in Regression Models with
ARMA(p,q) Errors.”
Journal of Econometrics, 64(1–2), 183–206.
doi:
10.1016/0304-4076(94)90063-9. Cited on page 20.
Chib S, Greenberg E (1995).
“Understanding the Metropolis-Hasting Algo-
rithm.” The American Statistician, 49(4), 327–335. Cited on page 13.
Chib S, Greenberg E (1996). “Markov Chain Monte Carlo Simulation Methods
in Econometrics.” Econometric Theory, 12(3), 409–431. Cited on pages 9
and 11.
Christoﬀersen PF (1998). “Evaluating Interval Forecasts.” International Eco-
nomic Review, 39(4), 841–862. Cited on pages 74, 102, 103, 104, 145, and 149.
Christoﬀersen PF, Diebold FX (1996). “Further Results on Forecasting and
Model Selection Under Asymmetric Loss.”
Journal of Applied Econo-
metrics, 11(5), 561–571.
doi:10.1002/(SICI)1099-1255(199609)11:5<561::
AID-JAE406>3.0.CO;2-S. Special issue in econometric forecasting. Cited
on page 86.
Christoﬀersen PF, Diebold FX (1997). “Optimal Prediction under Asymmetric
Loss.” Econometric Theory, 13(6), 808–817. Cited on page 86.
Christoﬀersen PF, Gon¸calves S (2004).
“Estimation Risk in Financial Risk
Management.” CIRANO Working Papers 2004s–15, CIRANO. URL http:
//ideas.repec.org/p/cir/cirwor/2004s-15.html. Cited on page 75.
Cornish EA, Fisher RA (1937). “Moments and Cumulants in the Speciﬁcation
of Distributions.” Revue de l’Institut International de Statistique, 4, 1–14.
Reprinted in: Contributions to Mathematical Statistics, Fisher, R. A. (Ed.),
New York: Whiley, 1950. Cited on pages 76 and 79.
Cowles MK, Carlin BP (1996). “Markov Chain Monte Carlo Convergence Diag-
nostics: A Comparative Review.” Journal of the American Statistical Associ-
ation, 91(434), 883–904. Cited on page 14.
Dempster AP (1997).
“The Direct Use of Likelihood for Signiﬁcance Test-
ing.” Statistics and Computing, 7(4), 247–252. doi:10.1023/A:1018598421607.
Cited on page 134.
Dempster AP, Laird NM, Rubin DB (1977). “Maximum Likelihood from Incom-
plete Data via the EM Algorithm.” Journal of the Royal Statistical Society,
39(1), 1–38. Cited on page 152.
Deschamps PJ (2006). “A Flexible Prior Distribution for Markov Switching
Autoregressions with Student-t Errors.” Journal of Econometrics, 133(1),
153–190. doi:10.1016/j.jeconom.2005.03.012. Cited on pages 14, 26, 58, 63,
115, 126, 166, 167, and 179.

194
References
Diebold FX, Gunther TA, Tay AS (1998). “Evaluating Density Forecasts with
Applications to Financial Risk Management.” International Economic Re-
view, 39(4), 863–883. Cited on page 133.
Diebold FX, Mariano RS (1995). “Comparing Predictive Accuracy.” Journal of
Business and Economic Statistics, 13(3), 253–263. Cited on page 102.
Diebolt J, Robert CP (1994).
“Estimation of Finite Mixture Distributions
through Bayesian Sampling.” Journal of the Royal Statistical Society, 56(2),
363–375. Cited on page 114.
Dueker MJ (1997).
“Markov Switching in GARCH Processes and Mean-
Reverting Stock-Market Volatility.” Journal of Business and Economic Statis-
tics, 15(1), 26–34. Cited on pages 2, 109, and 159.
Engle RF (1982).“Autoregressive Conditional Heteroscedasticity with Estimates
of the Variance of United Kingdom Inﬂation.” Econometrica, 50(4), 987–1008.
Cited on page 1.
Engle RF (2004).
“Risk and Volatility: Econometric Models and Financial
Practice.” The American Economic Review, 94(3), 405–420.
doi:10.1257/
0002828041464597. Cited on pages 2 and 83.
Fern´andez C, Steel MF (1998). “On Bayesian Modeling of Fat Tails and Skew-
ness.” Journal of the American Statistical Association, 93(441), 359–371.
Cited on page 140.
Fr¨uhwirth-Schnatter S (2001a). “Fully Bayesian Analysis of Switching Gaussian
State Space Models.” Annals of the Institute of Statistical Mathematics, 53(1),
31–49. Special issue on nonlinear non-Gaussian models and related ﬁltering
methods. Cited on page 139.
Fr¨uhwirth-Schnatter S (2001b). “Markov Chain Monte Carlo Estimation of Clas-
sical and Dynamic Switching and Mixture Models.” Journal of the American
Statistical Association, 96(453), 194–209. Cited on pages 7, 113, 116, 125,
129, and 157.
Fr¨uhwirth-Schnatter S (2004). “Estimating Marginal Likelihoods for Mixture
and Markov Switching Models Using Bridge Sampling Techniques.”
The
Econometrics Journal, 7(1), 143–167. doi:10.1111/j.1368-423X.2004.00125.x.
Cited on page 138.
Fr¨uhwirth-Schnatter S (2006). Finite Mixture and Markov Switching Models.
Springer Series in Statistics. Springer Verlag, New York, USA, ﬁrst edition.
ISBN 0387329099. Cited on pages 3, 117, and 118.
Gallant AR, Tauchen G (1989).
“Seminonparametric Estimation of Condi-
tionally Constrained Heterogeneous Processes: Asset Pricing Applications.”
Econometrica, 57(5), 1091–1120. Cited on page 2.
Gelfand AE, Dey DK (1994). “Bayesian Model Choice: Asymptotics and Exact
Calculations.” Journal of the Royal Statistical Society, 56(3), 501–514. Cited
on page 139.
Gelfand AE, Smith AFM (1990). “Sampling-Based Approaches to Calculating
Marginal Densities.” Journal of the American Statistical Association, 85(410),
398–409. Cited on page 11.
Gelman A (1995). “Inference and Monitoring Convergence.” In WR Gilks,
S Richardson, DJ Spiegelhalter (eds.), “Markov Chain Monte Carlo in Prac-

References
195
tice,” chapter 8, pp. 131–143. Chapman and Hall, London, UK, ﬁrst edition.
ISBN 0412055511. Cited on page 14.
Gelman A, Rubin DB (1992). “Inference from Iterative Simulation Using Multi-
ple Sequences.” Statistical Science, 7(4), 457–472. Cited on pages 14, 24, 25,
27, 47, 65, 94, and 123.
Geman S, Geman D (1984). “Stochastic Relaxation, Gibbs Distributions, and
the Bayesian Restoration of Images.” IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6, 721–741. Cited on page 11.
Gerlach RH, Carter C, Kohn R (1999). “Diagnostics for Time Series Analy-
sis.” Journal of Time Series Analysis, 20(3), 309–330. doi:10.1111/1467-9892.
00139. Cited on page 144.
Geweke JF (1988).
“Exact Inference in Models with Autoregressive Condi-
tional Heteroscedasticity.” In ER Berndt, HL White, WA Barnett (eds.),
“Dynamic Econometric Modeling,” Number 3 in International Symposium in
Economic Theory and Econometrics, pp. 73–103. Cambridge University Press,
New York, USA. ISBN 0521333954. Cited on pages 3 and 4.
Geweke JF (1989).
“Exact Predictive Densities in Linear Models with
ARCH Disturbances.” Journal of Econometrics, 40(1), 63–86. doi:10.1016/
0304-4076(89)90030-4. Cited on page 4.
Geweke JF (1991). “Eﬃcient Simulation From the Multivariate Normal and
Student-t Distributions Subject to Linear Constraints and the Evaluation of
Constraint Probabilities.” In EM Keramidas, SM Kaufman (eds.), “Com-
puting Science and Statistics: The 23rd Symposium on the Interface,” pp.
571–578. Seattle, Washington, USA. Special issue on critical applications of
scientiﬁc computing. Cited on page 141.
Geweke JF (1992). “Evaluating the Accuracy of Sampling-Based Approaches
to the Calculation of Posterior Moments.”
In JO Berger, JM Bernardo,
AP Dawid, AFM Smith (eds.), “Bayesian Statistics,” volume 4, pp. 169–194.
Oxford University Press, Oxford, UK. ISBN 0198522665. Fourth Valencia
international meeting. Cited on page 14.
Geweke JF (1993). “Bayesian Treatment of the Independent Student-t Linear
Model.” Journal of Applied Econometrics, 8(S1), S19–S40. doi:10.1002/jae.
3950080504.
Special issue on econometric inference using simulation tech-
niques. Cited on pages 6, 55, 57, 64, 166, 167, and 179.
Geweke JF (1999). “Using Simulation Methods for Bayesian Econometric Mod-
els: Inference, Development and Communication.”
Econometric Reviews,
18(1), 1–73. doi:10.1080/07474939908800428. Cited on page 30.
Geweke JF (2004). “Getting it Right: Joint Distribution Tests of Posterior
Simulators.” Journal of the American Statistical Association, 99(467), 799–
804. Cited on pages 22 and 179.
Gilks WR, Wild P (1992). “Adaptive Rejection Sampling for Gibbs Sampling.”
Applied Statistics, 41(2), 337–348. Cited on page 12.
Glosten LR, Jaganathan R, Runkle DE (1993). “On the Relation Between the
Expected Value and the Volatility of the Nominal Excess Return on Stocks.”
Journal of Finance, 48(5), 1779–1801. Cited on pages 2, 6, and 39.

196
References
Gray SF (1996). “Modeling the Conditional Distribution of Interest Rates as a
Regime-Switching Process.” Journal of Financial Economics, 42(1), 27–62.
doi:10.1016/0304-405X(96)00875-6. Cited on pages 109, 117, and 159.
Guidolin M, Timmermann A (2006). “Term Stucture of Risk under Alternative
Econometric Speciﬁcations.” Journal of Econometrics, 131(1–2), 285–308.
doi:10.1016/j.jeconom.2005.01.033. Cited on pages 91 and 92.
Haas M, Mittnik S, Paolella MS (2004). “A New Approach to Markov-Switching
GARCH Models.” Journal of Financial Econometrics, 2(4), 493–530. doi:
10.1093/jjﬁnec/nbh020. Cited on pages 2, 7, 110, 112, 126, 128, 133, and 157.
Hamilton JD (1994). Time Series Analysis. Princeton University Press, Prince-
ton, USA, ﬁrst edition. ISBN 0691042896. Cited on pages 114 and 152.
Hamilton JD, Susmel R (1994). “Autoregressive Conditional Heteroskedasticity
and Changes in Regime.” Journal of Econometrics, 64(1–2), 307–333. doi:
10.1016/0304-4076(94)90067-1. Cited on pages 2, 3, 109, and 152.
Hastings WK (1970). “Monte Carlo Sampling Methods Using Markov Chains
and their Applications.” Biometrika, 57(1), 97–109. doi:10.1093/biomet/57.
1.97. Cited on page 10.
He C, Ter¨asvirta T (1999). “Properties of Moments of a Family of GARCH Pro-
cesses.”Journal of Econometrics, 92(1), 173–192. doi:10.1016/S0304-4076(98)
00089-X. Cited on page 71.
Highﬁeld RA, Zellner A (1988). “Calculation of Maximum Entropy Distribu-
tions and Approximation of Marginal Posterior Distributions.” Journal of
Econometrics, 37(2), 195–209. doi:10.1016/0304-4076(88)90002-4. Cited on
page 83.
Hwang S, Knight J, Satchell SE (1999). “Forecasting Volatility Using Linex Loss
Functions.” Working paper 99-07, Financial Econometrics Research Center,
Warwick Business School. Cited on page 86.
Hwang S, Knight J, Satchell SE (2001). “Forecasting Nonlinear Functions of
Returns Using Linex Loss Functions.” Annals of Economics and Finance,
2(1), 187–213. Cited on page 86.
Jacquier E, Polson NG, Rossi PE (1994). “Bayesian Analysis of Stochastic
Volatility Models.” Journal of Business and Economic Statistics, 12(4), 371–
389. Cited on page 158.
Jaschke SR (2002). “The Cornish-Fisher Expansion in the Context of Delta-
Gamma-Normal Approximations.” Journal of Risk, 4(4), 33–52. Cited on
page 80.
Kass RE, Carlin BP, Gelman A, Neal RM (1998). “Markov Chain Monte Carlo
in Practice: A Roundtable Discussion.” The American Statistician, 52(2),
93–100. Cited on page 13.
Kass RE, Raftery AE (1995). “Bayes Factors.” Journal of the American Statis-
tical Association, 90(430), 773–795. Cited on pages 31 and 141.
Kaufmann R (2004).
Long-Term Risk Management.
Phd thesis, Swiss Fed-
eral Institute of Technology Z¨urich. Dissertation ETH No. 15595. Cited on
page 158.

References
197
Kaufmann S, Fr¨uhwirth-Schnatter S (2002). “Bayesian Analysis of Switching
ARCH Models.” Journal of Time Series Analysis, 23(4), 425–458. doi:10.
1111/1467-9892.00271. Cited on pages 4, 117, 133, 135, and 139.
Kaufmann S, Scheicher M (2006). “A Switching ARCH Model for the German
DAX Index.” Studies in Nonlinear Dynamics and Econometrics, 10(4), 1–
35. Article nr. 3, URL http://www.bepress.com/snde/vol10/iss4/art3/.
Cited on pages 4 and 139.
Kim S, Shephard N, Chib S (1998). “Stochastic Volatility: Likelihood Inference
and Comparison with ARCH Models.” Review of Economic Studies, 65(3),
361–393. doi:10.1111/1467-937X.00050. Cited on pages 133 and 158.
Klaassen F (2002).
“Improving GARCH Volatility Forecasts with Regime-
Switching GARCH.”
Empirical Economics, 27(2), 363–394.
doi:10.1007/
s001810100100. Cited on pages 2, 109, 112, and 117.
Kleibergen F, van Dijk HK (1993). “Non-Stationarity in GARCH Models: A
Bayesian Analysis.” Journal of Applied Econometrics, 8(S1), S41–S61. doi:
10.1002/jae.3950080505. Cited on page 4.
Knight J, Satchell S, Wang G (2003). “Value at Risk Linear Exponent Forecasts
-VARLINEX-.” Quantitative Finance, 3, 332–344. Cited on pages 86 and 97.
Koop G (2003). Bayesian Econometrics. Wiley-Interscience, London, UK, ﬁrst
edition. ISBN 0470845678. Cited on page 9.
Kupiec PH (1995). “Techniques for Verifying the Accuracy of Risk Measurement
Models.” Journal of Derivatives, 3, 73–84. Cited on page 74.
Lamoureux CG, Lastrapes WD (1990). “Persistence in Variance, Structural
Change, and the GARCH Model.” Journal of Business and Economic Statis-
tics, 8(2), 225–243. Cited on pages 2 and 109.
Lee SW, Hansen BE (1994). “Asymptotic Theory for the GARCH(1,1) Quasi-
Maximum Likelihood Estimator.” Econometric Theory, 10(1), 29–52. Cited
on page 2.
Ljung GM, Box GEP (1978). “On a Measure of Lack of Fit in Time Series
Models.” Biometrika, 65(2), 297–303. doi:10.1093/biomet/65.2.297. Cited on
pages 32 and 53.
Marcucci J (2005). “Forecasting Stock Market Volatility with Regime-Switching
GARCH Models.” Studies in Nonlinear Dynamics and Econometrics, 9(4),
1–53. Article nr. 6, URL http://www.bepress.com/snde/vol9/iss4/art6/.
Cited on pages 2 and 109.
McNeil AJ, Frey R (2000).
“Estimation of Tail-Related Risk Measures for
Heteroscedastic Financial Time Series: an Extreme Value Approach.” Journal
of Empirical Finance, 7(3–4), 271–300. doi:10.1016/S0927-5398(00)00012-8.
Cited on page 74.
Meng XL, Wong WH (1996). “Simulating Ratios of Normalizing Constants via
a Simple Identity: A Theoretical Exploration.” Statistica Sinica, 6, 831–860.
Cited on pages 7, 138, 139, and 157.
Metropolis N, Rosenbluth AW, Rosenbluth MN, Teller AH, Teller E (1953).
“Equations of State Calculations by Fast Computing Machines.” Journal of
Chemical Physics, 21(6), 1087–1092. Cited on pages 10 and 13.

198
References
Miazhynskaia T, Aussenegg W (2006). “Uncertainty in Value at Risk Estimates
under Parametric and Non-Parametric Modeling.” Financial Markets and
Portfolio Management, 20(3), 243–264. doi:10.1007/s11408-006-0020-8. Cited
on pages 75 and 92.
M¨uller P, Pole A (1998). “Monte Carlo Posterior Integration in GARCH Mod-
els.” Sankhya: The Indian Journal of Statistics, 60, 127–144. Cited on page 4.
Nakatsuma T (1998). “A Markov-Chain Sampling Algorithm for GARCH Mod-
els.” Studies in Nonlinear Dynamics and Econometrics, 3(2), 107–117. Algo-
rithm nr.1, URL http://www.bepress.com/snde/vol3/iss2/algorithm1/.
Cited on pages 4, 5, 6, 7, 17, 18, 19, 116, 155, 156, 157, 158, and 179.
Nakatsuma T (2000). “Bayesian Analysis of ARMA-GARCH Models: A Markov
Chain Sampling Approach.” Journal of Econometrics, 95(1), 57–69.
doi:
10.1016/S0304-4076(99)00029-9. Cited on pages 4, 5, 6, 7, 17, 18, 19, 116,
155, 156, 157, 158, and 179.
Nelson DB (1990). “Stationarity and Persistence in the GARCH(1,1) Model.”
Econometric Theory, 6(3), 318–334. Cited on page 36.
Nelson DB (1991). “Conditional Heteroskedasticity in Asset Returns: A New
Approach.” Econometrica, 59(2), 347–370. Cited on page 2.
Newey WK, West KD (1987). “A Simple, Positive Semi-Deﬁnite, Heteroskedas-
ticity and Autocorrelation Consistent Covariance Matrix.”
Econometrica,
55(3), 703–708. Cited on page 14.
Pagan AR, Schwert GW (1990). “Alternative Models for Conditional Stock
Variability.”
Journal of Econometrics, 45(1–2), 267–290.
doi:10.1016/
0304-4076(90)90101-X. Cited on page 2.
Philippe A, Robert CP (2003). “Perfect Simulation of Positive Gaussian Distri-
butions.” Computer Science and Mathematics and Statistics, 13(2), 179–186.
doi:10.1023/A:1023264710933. Cited on page 140.
Phillips PCB, Perron P (1988). “Testing for a Unit Root in Time Series Re-
gression.” Biometrika, 75(2), 335–346. doi:10.1093/biomet/75.2.335. Cited
on pages 22 and 123.
Politis DN, Romano JP (1994). “The Stationary Bootstrap.” Journal of the
American Statistical Association, 89(428), 1303–1313. Cited on page 135.
Politis DN, White H (2004).
“Automatic Block-Length Selection for the
Dependent Bootstrap.” Econometric Reviews, 23(1), 53–70.
doi:10.1081/
ETC-120028836. Cited on page 136.
Price KV, Storn RM, Lampinen JA (2006). Diﬀerential Evolution: A Practical
Approach to Global Optimization. Springer Verlag, Berlin, Germany. ISBN
3540209506. Cited on page 179.
R Development Core Team (2007). R: A Language and Environment for Statis-
tical Computing. R Foundation for Statistical Computing, Vienna, Austria.
Version 2.4.1, URL http://www.R-project.org/. Cited on pages 21 and 179.
Ripley B (1987). Stochastic Simulation. Probability and Mathematical Statis-
tics. Whiley, New York, USA, ﬁrst edition.
ISBN 0471818844.
Cited on
page 12.

References
199
RiskMetrics Group (1996).
RiskMetrics Technical Document.
J. P. Mor-
gan/Reuters, fourth edition. URL http://www.riskmetrics.com/rmcovv.
html. Cited on page 96.
Ritter C, Tanner MA (1992). “Facilitating the Gibbs Sampler: the Gibbs Stopper
and the Griddy-Gibbs Sampler.” Journal of the American Statistical Associ-
ation, 87(419), 861–868. Cited on pages 5 and 12.
Robert CP (1995). “Simulation of Truncated Normal Variables.” Statistics and
Computing, 5(2), 121–125. doi:10.1007/BF00143942. Cited on page 140.
Roberts GO, Smith AFM (1994). “Simple Conditions for the Convergence of
the Gibbs Sampler and Metropolis-Hastings Algorithm.” Stochastic Processes
and their Applications, 49(2), 207–216. doi:10.1016/0304-4149(94)90134-1.
Cited on pages 11 and 12.
Rosenblatt M (1952). “Remarks on a Multivariate Transformation.” Annals of
Mathematical Statistics, 23, 470–472. Cited on page 133.
Schwarz G (1978). “Estimating the Dimension of a Model.” The Annals of
Statistics, 6(2), 461–464. Cited on page 136.
Seiler D (2006). Backtesting Multiple-Period Forecasting Models. Master thesis,
Swiss Federal Institute of Technology Z¨urich. Cited on page 158.
Silverman BW (1986). Density Estimation for Statistics and Data Analysis.
Chapman and Hall, New York, USA, ﬁrst edition. ISBN 0412246201. Cited
on pages 15, 37, 141, 143, 151, and 153.
Smith AFM, Roberts GO (1993). “Bayesian Computation via the Gibbs Sampler
and Related Markov Chain Monte Carlo Methods.” Journal of the Royal
Statistical Society, 55(1), 3–24. Cited on pages 9 and 13.
Spiegelhalter DJ, Best NG, Carlin BP, van der Linde A (2002).
“Bayesian
Measures of Model Complexity and Fit.” Journal of the Royal Statistical
Society, 64(4), 583–639. doi:10.1111/1467-9868.00353. Cited on pages 7, 134,
and 157.
Spiegelhalter DJ, Thomas A, Best NG, Gilks WR (1995).
BUGS: Bayesian
Inference Using Gibbs Sampling. The BUGS project. Version 0.50. Cited on
page 5.
Spiegelhalter DJ, Thomas A, Best NG, Lunn D (2007). WinBUGS User’s man-
ual. The BUGS project. Version 1.4.1, URL http://www.mrc-bsu.cam.ac.
uk/bugs/. Cited on page 5.
Tanner MA, Wong WH (1987). “The Calculation of Posterior Distributions
by Data Augmentation.” Journal of the American Statistical Association,
82(398), 528–540. Cited on page 11.
Thompson RD, Basu AP (1995). “Asymmetric Loss Functions for Estimat-
ing System Reliability.”
In DA Berry, KM Chaloner, JF Geweke (eds.),
“Bayesian Analysis in Statistics and Econometrics,” Probability and Statis-
tics, pp. 471–482. John Wiley and Sons Ltd., New York, USA, ﬁrst edition.
ISBN 0471118567. Essays in honor of Arnold Zellner. Cited on page 90.
Tierney L (1994). “Markov Chains for Exploring Posterior Distributions.” The
Annals of Statistics, 22(4), 1701–1762. With discussion. Cited on pages 9,
10, 11, 12, 13, and 14.

200
References
Varian HR (1974). “A Bayesian Approach to Real Estate Assessment.” In
SE Fienberg, A Zellner (eds.), “Studies in Bayesian Econometrics and Statis-
tics,” Number 86 in Contributions to economic analysis, pp. 195–208. North
Holland, Amsterdam, NL, ﬁrst edition. Cited on page 86.
Vrontos ID, Dellaportas P, Politis DN (2000).
“Full Bayesian Inference for
GARCH and EGARCH Models.” Journal of Business and Economic Statis-
tics, 18(2), 187–198. Cited on page 4.
Zellner A (1986). “Bayesian Estimation and Prediction Using Asymmetric Loss
Functions.” Journal of the American Statistical Association, 81(394), 446–
451. Cited on page 87.
Zhu L, Carlin BP (2000).
“Comparing Hierarchical Models for Spatio-
Temporally Misaligned Data Using the Deviance Information Criterion.”
Statistics in Medicine, 19, 2265–2278. doi:10.1002/1097-0258(20000915/30)
19:17/18<2265::AID-SIM568>3.0.CO;2-6. Cited on page 135.
Zumbach G (2006). “Backtesing Risk Methodologies from one Day to one Year.”
The Journal of Risk, 9(2), 0–36. Cited on pages 104 and 158.

Index
A
absolute error loss (AEL)
see loss functions
Adaptive Radial-Based
Direction Sampling
(ARDS) . . . . . . . . . see
algorithms
algorithms
Adaptive Radial-Based
Direction Sampling
(ARDS) . . . . . . . . . . . 4
componentwise . . . . . . 13
Expectation Maxi-
mization (EM)
152
Forward Filtering
Backward Sampling
(FFBS). . . . .116, 117
Gibbs . . . . . . . . . . . . 11–13
Griddy-Gibbs . . 4, 5, 158
importance sampling . 4,
144
independence M-H. . .13
Markov chain Monte
Carlo (MCMC) . . . 3,
10–15, 155
Metropolis-Hastings
(M-H) . . . . . . 4, 12–13
permutation sampler
constrained . . 116, 117,
138, 139
random. . . . . . . . .7, 157
random walk Metropolis
13
ARCH . . . . . . . . . . . . . 1, 109
asymmetric. . . . . .see GJR
autocorrelogram. . .22, 123
B
backtesting . . see Value at
Risk
Bayes
(optimal) point
estimate. . . . . . . . . .85
factor (BF) 4, 30, 52, 70,
134, 141, 142, 155
rule. . .10, 18, 41, 58, 115
Bayesian
information (BIC) . . see
information criteria
statistics . . . . . . . . . . 9–10
bootstrap. . . . . . . . . . . .3, 75
block. . . . . . . . . . . . . . .135
stationary . . . . . . . . . . 135
bridge sampling see model
likelihood
BUGS . . . . . . see softwares
burn-in. . . . . . . . . . . .25, 123
C
clustering . . . see volatility
clustering
componentwise . . . . . . . .see
algorithms
conditional moments
78–80, 171–178
conjugate prior . . . . . . . . 10
constrained permutation
sampler. . . . . . . . . .see
algorithms
convergence . . . . . see tests
Cornish-Fisher. .see Value
at Risk
covariance stationary. .see
stationarity
cumulative returns. . . . .77
D
data sets
Deutschmark vs British
Pound (DEM/GBP)
22, 82, 92
Standard & Poors 100
(S&P100) . . . . . . . . 44
Swiss Market Index
(SMI) . . . . . . . . . . . 122
decision theory . . . . . 85–86
Deutschmark vs British
Pound (DEM/GBP)
see data sets
Deviance information
(DIC) . . . . . . . . . . . see
information criteria
diagnostics . . . . . . see tests
disturbances . . . . . . . . . . see
innovations
E
eﬀective number of
parameters . . . . . . 134
Expectation Maximiza-
tion (EM) . . . . . . . see
algorithms
Expected Shortfall (ES)
104–107

202
Index
term structure. . . . . .106
Exponential GARCH . . . 2
F
factor . . . . . . . . . . see Bayes
ﬁltered probabilities. .133,
146, 152
forecasting performance
102–104, 144–149,
see also backtesting
Forward Filtering Back-
ward Sampling
(FFBS). . . . . . . . . .see
algorithms
G
GARCH . . . . . . . 1, 109, 155
Normal innovations
17–36, 77
Gaussian . . . . . see Normal
generalized residuals . . see
residuals
Gibbs. . . . . .see algorithms
GJR . . . . . . . . . . . . . . . . . . . . 2
leverage eﬀect 39, 40, 44,
48, 111, 125, 157
linear regression. .39–54
Normal errors . . . . 39–54
Student-t errors . . 55–71
Griddy-Gibbs . . . . . . . . . see
algorithms
H
hyperparameters. . . . . . .10
I
identiﬁcation . . . . . . . . . . see
Markov-switching
importance
density. .4, 138, 139, 141
sampling see algorithms
independence M-H . . . . see
algorithms
ineﬃciency factor (IF) . 26
information criteria
Bayesian (BIC). . . . .136
Deviance (DIC)134–137
innovations
Normal . . . . . . . . . . . . . see
GARCH,GJR
Student-t. . . . . .see GJR
invariance . . . . . . . . . . . . .see
Markov-switching
J
Jarque-Bera . . . . . see tests
Jeﬀrey’s scale of evidence
31
K
Kolmogorov-Smirnov . . see
tests
kurtosis.see leptokurtosis,
unconditional
L
label invariance . . . . . . . see
invariance
latent variable . . 55, 57, 64
leptokurtosis. .95, 125, 157
leverage eﬀect . . . see GJR
likelihood
approximated 20, 42, 43,
61, 62, 119, 120
function . . 10, 18, 40, 57,
113, 165, 179
maximum (ML) . . 2, 152
model. . . . . . . .see model
likelihood
likelihood ratio statistic 3,
152
linear regression . see GJR
Linex loss . . . . . . . . see loss
functions
Ljung-Box . . . . . . . see tests
loss functions
absolute error (AEL)86,
97, 148
Linex . . . . 86–88, 97, 148
Monomial . . . . . . . . . . . 90
squared error (SEL).85,
97, 148
M
marginalsee unconditional
Markov chain Monte
Carlo (MCMC) . . see
algorithms
Markov process . . . . . . . 112
Markov-switching
GARCH (MS-GARCH)
109–110
GJR (MS-GJR)109–152
identiﬁcation. . .113, 117
invariance. . . . . .114, 115
multimodality . . 3, 4, 13,
113, 141
state variable . . 109, 134
Maximum Likelihood
(ML). .see likelihood
Metropolis-Hastings
(M-H)see algorithms
model likelihood . 137–142
bridge sampling138, 139
reciprocal importance
sampling . . . . . . . . 139
model uncertainty . 74, 158
Monomial . . . . . . . . see loss
functions
Monte Carlo . . . . . . . . . . see
algorithms
multi-day ahead Value at
Risk. . . .see Value at
Risk
multimodality. . . . . . . . .see
Markov-switching
N
Normal see GARCH, GJR
numerical standard error
(NSE). . . . . . . . . . . .14
O
one-day ahead Value at
Risk. . . .see Value at
Risk
optimal point estimate see
Bayes
P
p-scores . . . . . see residuals
permutation sampler . . see
algorithms
persistence.2, 34, 109, 114,
158
point estimate . . see Bayes
posterior
density . . . . . . . . . . . . . . 10
risk. . . . . . . . . . . . . . . . . .85
potential scale reduction
factor . . . . . . . . . . . . 25
predictive density . . . . . . 81
prior density. . . . . . . . . . .10

Index
203
probability integral
transforms. . . . . . .see
residuals
R
R . . . . . . . . . . . see softwares
random
permutation samplersee
algorithms
walk Metropolis. . . . .see
algorithms
reciprocal importance
sampling. .see model
likelihood
recursive transformations
20, 43, 61, 120,
161–164
regime-switching . . . . . . . 2,
155, 158, see also
Markov-switching
regression . . . . . . . see GJR
regulatory capital 100–101
residuals . . . . . . . . 32, 52, 70
generalized . . . . . . . . . 133
p-scores . . . . . . . . . . . . 133
probability integral
transforms . . . . . . 133
risk measure . . . . . . . . . . see
Expected Shortfall,
Value at Risk
rolling window . 74, 92, 144
S
sampler. . . .see algorithms
sensitivity analysis.30–31,
52, 70, 141
smoothed probabilities128
softwares
BUGS . . . . . . . . . . . . . . . . 5
R . . . . . . . . . . . . . . . 21, 179
Standard & Poors 100
(S&P100) . . see data
sets
squared error loss (SEL)
see loss functions
state variable . . . . . . . . . see
Markov-switching
stationarity
covariance 3, 36, 53, 110,
126
strict . . . . . . . . . . . . . . . . 36
stochastic volatility . . 134,
158
strict stationary. . . . . . .see
stationarity
Student-t . . . . . . . . see GJR
sub-additivitysee Value at
Risk
Swiss Market Index (SMI)
see data sets
T
term structure . . . . . . . . see
Expected Shortfall,
Value at Risk
tests
convergence. . .14, 24–25
Jarque-Bera Normality
53
Kolmogorov-Smirnov
empirical distribution
22, 71
Normality . . . 32, 53, 70
Ljung-Box . . . . 32, 53, 70
Wald . . . . . . . 22, 123, 133
transition matrix 112, 114,
118, 158
U
unconditional
kurtosis . . . . . . . . . . 36, 71
variance.36, 53, 126, 127
V
Value at Risk (VaR)73–83
backtesting 74, 102–104,
144–149
Cornish-Fisher expan-
sion . . . . . . . . . . 79, 80,
82
multi-day ahead . 77–83,
158
one-day ahead . . . . . . 77,
144–149
sub-additivity . . . . . . 104
term structure. . . . . . .95
volatility clustering . . . . 22
W
Wald . . . . . . . . . . . . see tests
White . . . . . . . . . . . . . 22, 123

Lecture Notes in Economics
and Mathematical Systems
please contact your bookseller or Springer-Verlag
Vol. 525: S. Spinler, Capacity Reservation for Capital-
Intensive Technologies. XVI, 139 pages. 2003.
Vol. 526: C. F. Daganzo, A Theory of Supply Chains. VIII,
123 pages. 2003.
Vol. 527: C. E. Metz, Information Dissemination in
Currency Crises. XI, 231 pages. 2003.
Vol.
528:
R.
Stolletz,
Performance
Analysis
and
Optimization of InboundCallCenters. X, 219 pages. 2003.
Vol. 529: W. Krabs, S. W. Pickl, Analysis, Controllability
and
Optimization
of
Time-Discrete
Systems
and
Dynamical Games. XII, 187 pages. 2003.
Vol. 530: R. Wapler, Unemployment, Market Structure
and Growth. XXVII, 207 pages. 2003.
Vol. 531: M. Gallegati, A. Kirman, M. Marsili (Eds.), The
Complex Dynamics of Economic Interaction. XV, 402
pages, 2004.
Vol. 532: K. Marti, Y. Ermoliev, G. Pﬂug (Eds.), Dynamic
Stochastic Optimization. VIII, 336 pages. 2004.
Vol. 533: G. Dudek, Collaborative Planning in Supply
Chains. X, 234 pages. 2004.
Vol. 534: M. Runkel, Environmental and Resource Policy
for Consumer Durables. X, 197 pages. 2004.
Vol.535:X.Gandibleux,M.Sevaux,K.Sörensen,V.T’kindt
(Eds.), Metaheuristics for Multiobjective Optimisation.
IX, 249 pages. 2004.
Vol. 536: R. Brüggemann, Model Reduction Methods for
Vector Autoregressive Processes. X, 218 pages. 2004.
Vol. 537: A. Esser, Pricing in (In)Complete Markets. XI,
122 pages, 2004.
Vol. 538: S. Kokot, The Econometrics of Sequential Trade
Models. XI, 193 pages. 2004.
Vol. 539: N. Hautsch, Modelling Irregularly Spaced
Financial Data. XII, 291 pages. 2004.
Vol. 540: H. Kraft, Optimal Portfolios with Stochastic
2004.
Vol. 541: G.-y. Chen, X. Huang, X. Yang, Vector
Optimization. X, 306 pages. 2005.
Vol. 542: J. Lingens, Union Wage Bargaining and
Economic Growth. XIII, 199 pages. 2004.
Vol. 543: C. Benkert, Default Risk in Bond and Credit
Derivatives Markets. IX, 135 pages. 2004.
Vol. 544: B. Fleischmann, A. Klose, Distribution Logistics.
X, 284 pages. 2004.
Vol. 547: M. Wildi, Signal Extraction. XI, 279 pages. 2005.
Vol. 548: D. Kuhn, Generalized Bounds for Convex
Multistage Stochastic Programs. XI, 190 pages. 2005.
Vol. 549: G. N. Krieg, Kanban-Controlled Manufacturing
Systems. IX, 236 pages. 2005.
Vol. 550: T. Lux, S. Reitz, E. Samanidou, Nonlinear
Dynamics and Heterogeneous Interacting Agents. XIII,
327 pages. 2005.
Vol. 551: J. Leskow, M. Puchet Anyul, L. F. Punzo, New
Tools of Economic Dynamics. XIX, 392 pages. 2005.
Vol. 552: C. Suerie, Time Continuity in Discrete Time
Models. XVIII, 229 pages. 2005.
Vol. 553: B. Mönch, Strategic Trading in Illiquid Markets.
XIII, 116 pages. 2005.
Vol. 554: R. Foellmi, Consumption Structure and Macro-
economics. IX, 152 pages. 2005.
Vol. 555: J. Wenzelburger, Learning in Economic Systems
with Expectations Feedback (planned) 2005.
Vol. 556: R. Branzei, D. Dimitrov, S. Tijs, Models in
Cooperative Game Theory. VIII, 135 pages. 2005.
Vol. 557: S. Barbaro, Equity and Efﬁciency Considerations
of Public Higer Education. XII, 128 pages. 2005.
Vol. 558: M. Faliva, M. G. Zoia, Topics in Dynamic Model
Analysis. X, 144 pages. 2005.
Vol. 559: M. Schulmerich, Real Options Valuation. XVI,
357 pages. 2005.
Vol. 560: A. von Schemde, Index and Stability in Bimatrix
Games. X, 151 pages. 2005.
Vol. 561: H. Bobzin, Principles of Network Economics.
XX, 390 pages. 2006.
Vol.
562:
T.
Langenberg,
Standardization
and
Expectations. IX, 132 pages. 2006.
Vol.
563:
A.
Seeger
(Ed.),
Recent
Advances
in
Optimization. XI, 455 pages. 2006.
Vol. 564: P. Mathieu, B. Beauﬁls, O. Brandouy (Eds.),
Artiﬁcial Economics. XIII, 237 pages. 2005.
Vol. 565: W. Lemke, Term Structure Modeling and
Estimation in a State Space Framework. IX, 224 pages.
2006.
Vol. 566: M. Genser, A Structural Framework for the
Pricing of Corporate Securities. XIX, 176 pages. 2006.
Vol. 545: R. Hafner, Stochastic Implied Volatility. XI, 229
pages. 2004.
Vol. 567: A. Namatame, T. Kaizouji, Y. Aruga (Eds.), The
Complex Networks of Economic Interactions. XI, 343
pages. 2006.
Interest Rates and
Assets. X, 173 pages.
Defaultable
Vol. 546: D. Quadt, Lot-Sizing and Scheduling for Flexible
Flow Lines. XVIII, 227 pages. 2004.
Vol. 568: M. Caliendo, Microeconometric Evaluation of
Labour Market Policies. XVII, 258 pages. 2006.
For information about Vols. 1–524

Vol.
569: L.
Neubecker, Strategic
Competition
in
Vol. 570: J. Woo, The Political Economy of Fiscal Policy.
X, 169 pages. 2006.
Vol. 571: T. Herwig, Market-Conform Valuation of
Options. VIII, 104 pages. 2006.
Vol. 572: M. F. Jäkel, Pensionomics. XII, 316 pages. 2006
Vol. 573: J. Emami Namini, International Trade and
Multinational Activity. X, 159 pages, 2006.
Vol. 574: R. Kleber, Dynamic Inventory Management in
Reverse Logistics. XII, 181 pages, 2006.
Vol. 575: R. Hellermann, Capacity Options for Revenue
Management. XV, 199 pages, 2006.
Vol. 576: J. Zajac, Economics Dynamics, Information and
Equilibnum. X, 284 pages, 2006.
Vol. 577: K. Rudolph, Bargaining Power Effects in
Financial Contracting. XVIII, 330 pages, 2006.
Vol. 578: J. Kühn, Optimal Risk-Return Trade-Offs of
Commercial Banks. IX, 149 pages, 2006.
Vol. 579: D. Sondermann, Introduction to Stochastic
Calculus for Finance. X, 136 pages, 2006.
Vol. 580: S. Seifert, PostedPrice Offers in InternetAuction
Markets. IX, 186 pages, 2006.
Vol. 581: K. Marti; Y. Ermoliev; M. Makowsk; G. Pﬂug
(Eds.), Coping with Uncertainty. XIII, 330 pages, 2006.
Vol. 582: J. Andritzky, Sovereign Default Risks Valuation:
Implications of Debt Crises and Bond Restructurings.
VIII, 251 pages, 2006.
Vol. 583: I.V. Konnov, D.T. Luc, A.M. Rubinov† (Eds.),
Generalized Convexity and Related Topics. IX, 469 pages,
2006.
Vol. 584: C. Bruun, Adances in Artiﬁcial Economics: The
Economy as a Complex Dynamic System. XVI, 296 pages,
2006.
Vol. 585: R. Pope, J. Leitner, U. Leopold-Wildburger, The
Knowledge AheadApproachtoRisk. XVI, 218 pages, 2007
(planned).
Vol. 586: B.Lebreton, Strategic Closed-Loop Supply Chain
Management. X, 150 pages, 2007 (planned).
Vol. 587: P. N. Baecker, Real Options and Intellectual
Property: Capital Budgeting Under Imperfect Patent
Protection. X, 276 pages , 2007.
Vol. 588: D. Grundel, R. Murphey, P. Panos , O. Prokopyev
(Eds.), Cooperative Systems: Control and Optimization.
IX, 401 pages , 2007.
Vol. 589: M. Schwind, Dynamic Pricing and Automated
Resource
Allocation
for
Information
Services:
Reinforcement Learning and Combinatorial Auctions.
XII, 293 pages , 2007.
Vol. 592: A. C.-L. Chian, Complex Systems Approach to
Economic Dynamics. X, 95 pages, 2007.
Vol.
593:
J.
Rubart,
The
Employment
Effects
of
Technological
Change:
Heterogenous
Labor,
Wage
Vol. 594: R. Hübner, Strategic Supply Chain Management
in Process Industries: An Application to Specialty
Chemicals Production Network Design. XII, 243 pages,
Vol. 595: H. Gimpel, Preferences in Negotiations: The
Vol. 596: M. Müller-Bungart, Revenue Management
with Flexible Products: Models and Methods for the
Vol. 597: C. Barz, Risk-Averse Capacity Control in
Vol. 598: A. Ule, Partner Choice and Cooperation in
Networks: Theory and Experimental Evidence. Approx.
Vol. 599: A. Consiglio, Artiﬁcial Markets Modeling:
Vol.
600:
M.
Hickman,
P.
Mirchandani,
S.
Voss
(Eds.): Computer-Aided Scheduling of Public Transport.
Vol. 601: D. Radulescu, CGE Models and Capital Income
TaxReforms:TheCaseofaDualIncomeTaxforGermany.
Vol. 602: N. Ehrentreich, Agent-Based Modeling: The
Santa
Fe Institute
Artiﬁcial
Stock
Market
Model
Vol. 603: D. Briskorn, Sports Leagues Scheduling: Models,
Combinatorial Properties, andOptimization Algorithms.
Vol. 604: D. Brown, F. Kubler, Computational Aspects
of General Equilibrium Theory: Refutable Theories of
Vol. 606: S. von Widekind, Evolution of Non-Expected
Vol. 608: P. Nicola, Experimenting with Dynamic
Vol. 609: X. Fang, K.K. Lai, S. Wang, Fuzzy Portfolio
Vol. 610: M. Hillebrand, Pension Systems, Demographic
Vol. 590: S. H. Oda, Developments on Experimental
Economics: New Approaches to Solving Real-World
Problems. XVI, 262 pages, 2007.
Vol. 607: M. Bouziane, Pricing Interest Rate Derivatives:
A Fourier-Transform Based Approach. XII, 191 pages,
 
 
Vol. 611: R. Brosch, Portfolios of Real Options. XVI, 174
Vol. 605: M. Puhle, Bond Portfolio Optimization. XIV, 137
 
 
 
 
 
 
 
 
 
 
Vol. 612:
Risk Management with
Vol.
591:
M.
Lehmann-Waffenschmidt,
Economic
Evolution and Equilibrium: Bridging the Gap. VIII, 272
pages, 2007.
Oligopolies with Fluctuating Demand. IX, 233 pages,  2006.
Bayesian Estimation of GARCH Models. XII, 203 pages,
D. Ardia, Financial
Inequality and Unemployment. XII, 209 pages, 2007.
2007.
Attachment Effect. XIV, 268 pages, 2007.
Broadcasting Industry. XXI, 297 pages, 2007.
Revenue Management. XIV, 163 pages, 2007.
200 pages, 2007.
Methods and Applications. XV, 277 pages, 2007.
Approx. 424 pages, 2007.
XVI, 168 pages, 2007.
Revisited. XVI, 225 pages, 2007.
XII, 164 pages, 2008.
Value. XII, 202 pages, 2008.
pages, 2008.
Utility Preferences. X, 130 pages, 2008.
2008.
Macromodels: Growth and Cycles. XIII, 241 pages, 2008.
Optimization: Theory and Models. IX, 173 pages, 2008.
Change, and the Stock Market. X, 176 pages, 2008 .
pages, 2008.
2008.


An Introduction to
Statistical Computing

WILEY SERIES IN COMPUTATIONAL STATISTICS
Consulting Editors:
Paolo Giudici
University of Pavia, Italy
Geof H. Givens
Colorado State University, USA
Bani K. Mallick
Texas A & M University, USA
Wiley Series in Computational Statistics is comprised of practical guides and cutting
edge research books on new developments in computational statistics. It features
quality authors with a strong applications focus. The texts in the series provide
detailed coverage of statistical concepts, methods and case studies in areas at the
interface of statistics, computing, and numerics.
With sound motivation and a wealth of practical examples, the books show in
concrete terms how to select and to use appropriate ranges of statistical comput-
ing techniques in particular ﬁelds of study. Readers are assumed to have a basic
understanding of introductory terminology.
The series concentrates on applications of computational methods in statistics to
ﬁelds of bioinformatics, genomics, epidemiology, business, engineering, ﬁnance and
applied statistics.
Titles in the Series
Biegler, Biros, Ghattas, Heinkenschloss, Keyes, Mallick, Marzouk, Tenorio,
Waanders, Willcox – Large-Scale Inverse Problems and Quantiﬁcation of Uncertainty
Billard and Diday – Symbolic Data Analysis: Conceptual Statistics and Data Mining
Bolstad – Understanding Computational Bayesian Statistics
Borgelt, Steinbrecher and Kruse – Graphical Models, 2e
Dunne – A Statistical Approach to Neutral Networks for Pattern Recognition
Liang, Liu and Carroll – Advanced Markov Chain Monte Carlo Methods
Ntzoufras – Bayesian Modeling Using WinBUGS
Tuff´ery – Data Mining and Statistics for Decision Making

An Introduction to
Statistical Computing
A Simulation-based Approach
Jochen Voss
School of Mathematics, University of Leeds, UK

This edition ﬁrst published 2014
C⃝2014 John Wiley & Sons, Ltd
Registered ofﬁce
John Wiley & Sons, Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ,
United Kingdom
For details of our global editorial ofﬁces, for customer services and for information about how to apply
for permission to reuse the copyright material in this book please see our website at www.wiley.com.
The right of the author to be identiﬁed as the author of this work has been asserted in accordance with the
Copyright, Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or
transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or otherwise,
except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of
the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may
not be available in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand
names and product names used in this book are trade names, service marks, trademarks or registered
trademarks of their respective owners. The publisher is not associated with any product or vendor
mentioned in this book.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in
preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and speciﬁcally disclaim any implied warranties of
merchantability or ﬁtness for a particular purpose. It is sold on the understanding that the publisher is not
engaged in rendering professional services and neither the publisher nor the author shall be liable for
damages arising herefrom. If professional advice or other expert assistance is required, the services of a
competent professional should be sought.
Library of Congress Cataloging-in-Publication Data
Voss, Jochen.
An introduction to statistical computing : a simulation-based approach / Jochen Voss. – First edition.
pages cm. – (Wiley series in computational statistics)
Includes bibliographical references and index.
ISBN 978-1-118-35772-9 (hardback)
1. Mathematical statistics–Data processing.
I. Title.
QA276.4.V66 2013
519.501′13–dc23
2013019321
A catalogue record for this book is available from the British Library.
ISBN: 978-1-118-35772-9
Typeset in 10/12pt Times by Aptara Inc., New Delhi, India
1
2014

Contents
List of algorithms
ix
Preface
xi
Nomenclature
xiii
1
Random number generation
1
1.1
Pseudo random number generators
2
1.1.1
The linear congruential generator
2
1.1.2
Quality of pseudo random number generators
4
1.1.3
Pseudo random number generators in practice
8
1.2
Discrete distributions
8
1.3
The inverse transform method
11
1.4
Rejection sampling
15
1.4.1
Basic rejection sampling
15
1.4.2
Envelope rejection sampling
18
1.4.3
Conditional distributions
22
1.4.4
Geometric interpretation
26
1.5
Transformation of random variables
30
1.6
Special-purpose methods
36
1.7
Summary and further reading
36
Exercises
37
2
Simulating statistical models
41
2.1
Multivariate normal distributions
41
2.2
Hierarchical models
45
2.3
Markov chains
50
2.3.1
Discrete state space
51
2.3.2
Continuous state space
56
2.4
Poisson processes
58
2.5
Summary and further reading
67
Exercises
67

vi
CONTENTS
3
Monte Carlo methods
69
3.1
Studying models via simulation
69
3.2
Monte Carlo estimates
74
3.2.1
Computing Monte Carlo estimates
75
3.2.2
Monte Carlo error
76
3.2.3
Choice of sample size
80
3.2.4
Reﬁned error bounds
82
3.3
Variance reduction methods
84
3.3.1
Importance sampling
84
3.3.2
Antithetic variables
88
3.3.3
Control variates
93
3.4
Applications to statistical inference
96
3.4.1
Point estimators
97
3.4.2
Conﬁdence intervals
100
3.4.3
Hypothesis tests
103
3.5
Summary and further reading
106
Exercises
106
4
Markov Chain Monte Carlo methods
109
4.1
The Metropolis–Hastings method
110
4.1.1
Continuous state space
110
4.1.2
Discrete state space
113
4.1.3
Random walk Metropolis sampling
116
4.1.4
The independence sampler
119
4.1.5
Metropolis–Hastings with different move types
120
4.2
Convergence of Markov Chain Monte Carlo methods
125
4.2.1
Theoretical results
125
4.2.2
Practical considerations
129
4.3
Applications to Bayesian inference
137
4.4
The Gibbs sampler
141
4.4.1
Description of the method
141
4.4.2
Application to parameter estimation
146
4.4.3
Applications to image processing
151
4.5
Reversible Jump Markov Chain Monte Carlo
158
4.5.1
Description of the method
160
4.5.2
Bayesian inference for mixture distributions
171
4.6
Summary and further reading
178
4.6
Exercises
178
5
Beyond Monte Carlo
181
5.1
Approximate Bayesian Computation
181
5.1.1
Basic Approximate Bayesian Computation
182
5.1.2
Approximate Bayesian Computation with regression
188
5.2
Resampling methods
192

CONTENTS
vii
5.2.1
Bootstrap estimates
192
5.2.2
Applications to statistical inference
197
5.3
Summary and further reading
209
Exercises
209
6
Continuous-time models
213
6.1
Time discretisation
213
6.2
Brownian motion
214
6.2.1
Properties
216
6.2.2
Direct simulation
217
6.2.3
Interpolation and Brownian bridges
218
6.3
Geometric Brownian motion
221
6.4
Stochastic differential equations
224
6.4.1
Introduction
224
6.4.2
Stochastic analysis
226
6.4.3
Discretisation schemes
231
6.4.4
Discretisation error
236
6.5
Monte Carlo estimates
243
6.5.1
Basic Monte Carlo
243
6.5.2
Variance reduction methods
247
6.5.3
Multilevel Monte Carlo estimates
250
6.6
Application to option pricing
255
6.7
Summary and further reading
259
Exercises
260
Appendix A
Probability reminders
263
A.1 Events and probability
263
A.2 Conditional probability
266
A.3 Expectation
268
A.4 Limit theorems
269
A.5 Further reading
270
Appendix B
Programming in R
271
B.1
General advice
271
B.2
R as a Calculator
272
B.2.1
Mathematical operations
273
B.2.2
Variables
273
B.2.3
Data types
275
B.3
Programming principles
282
B.3.1
Don’t repeat yourself!
283
B.3.2
Divide and conquer!
286
B.3.3
Test your code!
290
B.4
Random number generation
292
B.5
Summary and further reading
294
Exercises
294

viii
CONTENTS
Appendix C
Answers to the exercises
299
C.1
Answers for Chapter 1
299
C.2
Answers for Chapter 2
315
C.3
Answers for Chapter 3
319
C.4
Answers for Chapter 4
328
C.5
Answers for Chapter 5
342
C.6
Answers for Chapter 6
350
C.7
Answers for Appendix B
366
References
375
Index
379

List of algorithms
Random number generation
alg. 1.2
linear congruential generator
2
alg. 1.13
inverse transform method
12
alg. 1.19
basic rejection sampling
15
alg. 1.22
envelope rejection sampling
19
alg. 1.25
rejection sampling for conditional distributions
22
Simulating statistical models
alg. 2.9
mixture distributions
47
alg. 2.11
componentwise simulation
49
alg. 2.22
Markov chains with discrete state space
53
alg. 2.31
Markov chains with continuous state space
58
alg. 2.36
Poisson process
61
alg. 2.41
thinning method for Poisson processes
65
Monte Carlo methods
alg. 3.8
Monte Carlo estimate
75
alg. 3.22
importance sampling
85
alg. 3.26
antithetic variables
89
alg. 3.31
control variates
93
Markov Chain Monte Carlo methods
alg. 4.2
Metropolis–Hastings method for continuous state space
110
alg. 4.4
Metropolis–Hastings method for discrete state space
113
alg. 4.9
random walk Metropolis
117
alg. 4.11
independence sampler
119
alg. 4.12
Metropolis–Hastings method with different move types
121
alg. 4.27
Gibbs sampler
142
alg. 4.31
Gibbs sampler for the Ising model
155
alg. 4.32
Gibbs sampler in image processing
158
alg. 4.36
reversible jump Markov Chain Monte Carlo
165

x
LIST OF ALGORITHMS
Beyond Monte Carlo
alg. 5.1
basic Approximate Bayesian Computation
182
alg. 5.6
Approximate Bayesian Computation with regression
191
alg. 5.11
general bootstrap estimate
196
alg. 5.15
bootstrap estimate of the bias
200
alg. 5.18
bootstrap estimate of the standard error
202
alg. 5.20
simple bootstrap conﬁdence interval
205
alg. 5.21
BCa bootstrap conﬁdence interval
207
Continuous-time models
alg. 6.6
Brownian motion
217
alg. 6.12
Euler–Maruyama scheme
232
alg. 6.15
Milstein scheme
235
alg. 6.26
multilevel Monte Carlo estimates
251
alg. 6.29
Euler–Maruyama scheme for the Heston model
256

Preface
This is a book about exploring random systems using computer simulation and thus,
this book combines two different topic areas which have always fascinated me:
the mathematical theory of probability and the art of programming computers. The
method of using computer simulations to study a system is very different from the
more traditional, purely mathematical approach. On the one hand, computer exper-
iments normally can only provide approximate answers to quantitative questions,
but on the other hand, results can be obtained for a much wider class of systems,
including large and complex systems where a purely theoretical approach becomes
difﬁcult.
In this text we will focus on three different types of questions. The ﬁrst, easiest
question is about the normal behaviour of the system: what is a typical state of the sys-
tem? Such questions can be easily answered using computer experiments: simulating
a few random samples of the system gives examples of typical behaviour. The second
kind of question is about variability: how large are the random ﬂuctuations? This
type of question can be answered statistically by analysing large samples, generated
using repeated computer simulations. A ﬁnal, more complicated class of questions is
about exceptional behaviour: how small is the probability of the system behaving in
a speciﬁed untypical way? Often, advanced methods are required to answer this third
type of question. The purpose of this book is to explain how such questions can be
answered. My hope is that, after reading this book, the reader will not only be able
to conﬁdently use methods from statistical computing for answering such questions,
but also to adjust existing methods to the requirements of a given problem and, for
use in more complex situations, to develop new specialised variants of the existing
methods.
This text originated as a set of handwritten notes which I used for teaching
the ‘Statistical Computing’ module at the University of Leeds, but now is greatly
extended by the addition of many examples and more advanced topics. The material
we managed to cover in the ‘Statistical Computing’ course during one semester is less
than half of what is now the contents of the book! This book is aimed at postgraduate
students and their lecturers; it can be used both for self-study and as the basis of
taught courses. With the inclusion of many examples and exercises, the text should
also be accessible to interested undergraduate students and to mathematically inclined
researchers from areas outside mathematics.

xii
PREFACE
Only very few prerequisites are required for this book. On the mathematical side,
the text assumes that the reader is familiar with basic probability, up to and including
the law of large numbers; Appendix A summarises the required results. As a con-
sequence of the decision to require so little mathematical background, some of the
ﬁner mathematical subtleties are not discussed in this book. Results are presented in a
way which makes them easily accessible to readers with limited mathematical back-
ground, but the statements are given in a form which allows the mathematically more
knowledgeable reader to easily add the required detail on his/her own. (For example,
I often use phrases such as ‘every set A ⊆Rd’ where full mathematical rigour would
require us to write ‘every measurable set A ⊆Rd’.) On the computational side, basic
programming skills are required to make use of the numerical methods introduced
in this book. While the text is written independent of any speciﬁc programming
language, the reader will need to choose a language when implementing methods
from this book on a computer. Possible choices of programming language include
Python, Matlab and C/C++. For my own implementations, provided as part of the
solutions to the exercises in Appendix C, I used the R programming language; a short
introduction to programming with R is provided in Appendix B.
Writing this book has been a big adventure for me. When I started this project,
more than a year ago, my aim was to cover enough material so that I could discuss
the topics of multilevel Monte Carlo and reversible jump Markov Chain Monte Carlo
methods. I estimated that 350 pages would be enough to cover this material but it
quickly transpired that I had been much too optimistic: my estimates for the ﬁnal
page count kept rising and even after several rounds of throwing out side-topics and
generally tightening the text, the book is still stretching this limit! Nevertheless, the
text now covers most of the originally planned topics, including multilevel Monte
Carlo methods near the very end of the book. Due to my travel during the last year,
parts of this book have been written on a laptop in exciting places. For example, the
initial draft of section 1.5 was written on a coach travelling through the beautiful
island of Kyushu, halfway around the world from where I live! All in all, I greatly
enjoyed writing this book and I hope that the result is useful to the reader.
This book contains an accompanying website. Please visit www.wiley.com/
go/statistical_computing
Jochen Voss
Leeds, March 2013

Nomenclature
For reference, the following list summarises some of the notation used throughout
this book.
ø
the empty set
N
the natural numbers: N = {1, 2, 3, . . .}
N0
the non-negative integers: N = {0, 1, 2, . . .}
Z
the integers: Z = {. . . , −2, −1, 0, 1, 2, . . .}
n mod m
the remainder of the division of n by m, in the range 0, 1, . . . , m −1
δkl
the Kronecker delta: δkl = 1 if k = l and δkl = 0 otherwise
R
the real numbers
⌈x⌉
the number x ∈R ‘rounded up’, that is the smallest integer greater than
or equal to x
(an)n∈N
a sequence of (possibly random) numbers: (an)n∈N = (a1, a2, . . .)
O(·)
the big O notation, introduced in deﬁnition 3.16
[a, b]
an interval of real numbers: [a, b] =

x ∈R
 a ≤x ≤b

{a, b}
the set containing a and b
A∁
the complement of a set: A∁=

x
 x /∈A

.
A × B
the Cartesian product of the sets A and B:
A × B =

(a, b)
 a ∈A, b ∈B

1A(x)
the indicator function of the set A: 1A(x) = 1 if x ∈A and 0 otherwise
(see section A.3)
U[0, 1]
the uniform distribution on the interval [0, 1]
U{−1, 1}
the uniform distribution on the two-element set {−1, 1}
Pois(λ)
the Poisson distribution with parameter λ
X ∼μ
indicates that a random variable X is distributed according to a probability
distribution μ
|S|
the number of elements in a ﬁnite set S; in section 1.4 also the volume of
a subsets S ⊆Rd
RS
space of vectors where the components are indexed by elements of S (see
section 2.3.2)
RS×S
space of matrices where rows and columns are indexed by elements of S
(see section 2.3.2)

1
Random number generation
The topic of this book is the study of statistical models using computer simulations.
Here we use the term ‘statistical models’ to mean any mathematical models which
include a random component. Our interest in this chapter and the next is in simu-
lation of the random component of these models. The basic building block of such
simulations is the ability to generate random numbers on a computer, and this is the
topic of the present chapter. Later, in Chapter 2, we will see how the methods from
Chapter 1 can be combined to simulate more complicated models.
Generation of random numbers, or more general random objects, on a computer
is complicated by the fact that computer programs are inherently deterministic: while
the output of computer program may look random, it is obtained by executing the
steps of some algorithm and thus is totally predictable. For example the output of a
program computing the decimal digits of the number
π = 3.14159265358979323846264338327950288419716939937510 · · ·
(the ratio between the perimeter and diameter of a circle) looks random at ﬁrst sight,
but of course π is not random at all! The output can only start with the string of digits
given above and running the program twice will give the same output twice.
We will split the problem of generating random numbers into two distinct sub-
problems: ﬁrst we will study the problem of generating any randomness at all, con-
centrating on the simple case of generating independent random numbers, uniformly
distributed on the interval [0, 1]. This problem and related concerns will be discussed
in Section 1.1. In the following sections, starting with Section 1.2, we will study the
generation of random numbers from different distributions, using the independent,
uniformly distributed random numbers obtained in the previous step as a basis.
An Introduction to Statistical Computing: A Simulation-based Approach, First Edition. Jochen Voss.
© 2014 John Wiley & Sons, Ltd. Published 2014 by John Wiley & Sons, Ltd.

2
AN INTRODUCTION TO STATISTICAL COMPUTING
1.1
Pseudo random number generators
There are two fundamentally different classes of methods to generate random
numbers:
(a) True random numbers are generated using some physical phenomenon which
is random. Generating such numbers requires specialised hardware and can
be expensive and slow. Classical examples of this include tossing a coin or
throwing dice. Modern methods utilise quantum effects, thermal noise in
electric circuits, the timing of radioactive decay, etc.
(b) Pseudo random numbers are generated by computer programs. While these
methods are normally fast and resource effective, a challenge with this
approach is that computer programs are inherently deterministic and therefore
cannot produce ‘truly random’ output.
In this text we will only consider pseudo random number generators.
Deﬁnition 1.1
A pseudo random number generator (PRNG) is an algorithm which
outputs a sequence of numbers that can be used as a replacement for an independent
and identically distributed (i.i.d.) sequence of ‘true random numbers’.
1.1.1
The linear congruential generator
This section introduces the linear congruential generator (LCG), a simple example of
a PRNG. While this random number generator is no longer of practical importance,
it shares important characteristics with the more complicated generators used in
practice today and we study it here as an accessible example. The LCG is given by
the following algorithm.
Algorithm 1.2
(linear congruential generator)
input:
m > 1 (the modulus)
a ∈{1, 2, . . . , m −1} (the multiplier)
c ∈{0, 1, . . . , m −1} (the increment)
X0 ∈{0, 1, . . . , m −1} (the seed)
output:
a sequence X1, X2, X3, . . . of pseud random numbers
1: for n = 1, 2, 3, . . . do
2:
Xn ←(aXn−1 + c) mod m
3:
output Xn
4: end for

RANDOM NUMBER GENERATION
3
In the algorithm, ‘mod’ denotes the modulus for integer division, that is the value
n mod m is the remainder of the division of n by m, in the range 0, 1, . . . , m −
1. Thus the sequence generated by algorithm 1.2 consists of integers Xn from the
range {0, 1, 2, . . . , m −1}. The output depends on the parameters m, a, c and on the
seed X0. We will see that, if m, a and c are carefully chosen, the resulting sequence
behaves ‘similar’ to a sequence of independent, uniformly distributed random vari-
ables. By choosing different values for the seed X0, different sequences of pseudo
random numbers can be obtained.
Example 1.3
For parameters m = 8, a = 5, c = 1 and seed X0 = 0, algorithm 1.2
gives the following output:
n
5Xn−1 + 1
Xn
1
1
1
2
6
6
3
31
7
4
36
4
5
21
5
6
26
2
7
11
3
8
16
0
9
1
1
10
6
6
The output 1, 6, 7, 4, 5, 2, 3, 0, 1, 6, . . . shows no obvious pattern and could be con-
sidered to be a sample of a random sequence.
While the output of the LCG looks random, from the way it is generated it is
clear that the output has several properties which make it different from truly random
sequences. For example, since each new value of Xn is computed from Xn−1, once the
generated series reaches a value Xn which has been generated before, the output starts
to repeat. In example 1.3 this happens for X8 = X0 and we get X9 = X1, X10 = X2
and so on. Since Xn can take only m different values, the output of a LCG starts
repeating itself after at most m steps; the generated sequence is eventually periodic.
Sometimes the periodicity of a sequence of pseudo random numbers can cause
problems, but on the other hand, if the period length is longer than the amount of
random numbers we use, periodicity cannot affect our result. For this reason, one
needs to carefully choose the parameters m, a and c in order to achieve a long enough
period. In particular m, since it is an upper bound for the period length, needs to be
chosen large. In practice, typical values of m are on the order of m = 232 ≈4 · 109
and a and c are then chosen such that the generator actually achieves the maximally
possible period length of m. A criterion for the choice of m, a and c is given in the
following theorem (Knuth, 1981, Section 3.2.1.2).

4
AN INTRODUCTION TO STATISTICAL COMPUTING
Theorem 1.4
The LCG has period m if and only if the following three conditions
are satisﬁed:
(a) m and c are relatively prime;
(b) a −1 is divisible by every prime factor of m;
(c) if m is a multiple of 4, then a −1 is a multiple of 4.
In the situation of the theorem, the period length does not depend on the seed X0
and usually this parameter is left to be chosen by the user of the PRNG.
Example 1.5
Let m = 232, a = 1 103 515 245 and c = 12 345. Since the only
prime factor of m is 2 and c is odd, the values m and c are relatively prime and condition
(a) of the theorem is satisﬁed. Similarly, condition (b) is satisﬁed, since a −1 is
even and thus divisible by 2. Finally, since m is a multiple of 4, we have to check
condition (c) but, since a −1 = 1 103 515 244 = 275 878 811 · 4, this condition also
holds. Therefore the LCG with these parameters m, a and c has period 232 for every
seed X0.
1.1.2
Quality of pseudo random number generators
PRNGs used in modern software packages such as R or Matlab are more sophisticated
(and more complicated) than the LCG presented in Section 1.1.1, but they still share
many characteristics of the LCG. We will see that no PRNG can produce a perfect
result, but the random number generators used in practice, for example the Mersenne
Twister algorithm (Matsumoto and Nishimura, 1998), are good enough for most
purposes. In this section we will discuss criteria for the quality of the output of
general PRNGs, and will illustrate these criteria using the LCG as an example.
1.1.2.1
Period length of the output
We have seen that the output of the LCG is eventually periodic, with a period length
of at most m. This property that the output is eventually periodic is shared by all
PRNGs implemented in software. Most PRNGs used in practice have a period length
which is much larger than the amount of random numbers a computer program could
ever use in a reasonable time. For this reason, periodicity of the output is not a big
problem in practical applications of PRNGs. The period length is a measure for the
quality of a PRNG.
1.1.2.2
Distribution of samples
The output of almost all PRNGs is constructed so that it can be used as a replacement
for an i.i.d. sample of uniformly distributed random numbers. Since the output takes

RANDOM NUMBER GENERATION
5
values in a ﬁnite set S = {0, 1, . . . , m −1}, in the long run, for every set A ⊆S we
should have
#

i
 1 ≤i ≤N, Xi ∈A

N
≈#A
#S ,
(1.1)
where #A stands for the number of elements in a ﬁnite set A.
Uniformity of the output can be tested using statistical tests like the chi-
squared test or the Kolmogorov–Smirnov test (see e.g. Lehmann and Romano, 2005,
Chapter 14).
One peculiarity when applying statistical tests for the distribution of samples to the
output of a PRNG is that the test may fail in two different ways: The output could either
have the wrong distribution (i.e. not every value appears with the same probability),
or the output could be too regular. For example, the sequence Xn = n mod m hits
every value equally often in the long run, but it shows none of the ﬂuctuations which
are typical for a sequence of real random numbers. For this reason, statistical tests
should be performed as two-sided tests when the distribution of the output of a PRNG
is being tested.
Example 1.6
Assume that we have a PRNG with m = 1024 possible output values
and that we perform a chi-squared test for the hypothesis
P (Xi ∈{64 j, 64 j + 1, . . . , 64 j + 63}) = 1/16
for j = 0, 1, . . . , 15.
If we consider a sample X1, X2, . . . , X N, the test statistic of the chi-squared test
is computed from the observed numbers of samples in each block, given by
O j = #

i
 64 j ≤Xi < 64( j + 1)

.
The expected count for block j, assuming that (1.1) holds, is
E j = N · 64/1024 = N/16
for j = 0, 1, . . . , 15 and the test statistic of the corresponding chi-squared test is
Q =
15

j=0
(O j −E j)2
E j
.

6
AN INTRODUCTION TO STATISTICAL COMPUTING
For large sample size N, and under the hypothesis (1.1), the value Q follows a
χ2-distribution with 15 degrees of freedom. Some quantiles of this distribution are:
q
6.262
7.261
· · ·
24.996
27.488
P(Q ≤q)
0.025
0.05
· · ·
0.95
0.975
Thus, for a one-sided test with signiﬁcance level 1 −α = 95% we would reject the
hypothesis if Q > 24.996. In contrast, for a two-sided test with signiﬁcance level
1 −α = 95%, we would reject the hypothesis if either Q < 6.262 or Q > 27.488.
We consider two different test cases: ﬁrst, if Xn = n mod 1024 for n =
1, 2, . . . , N = 106, we ﬁnd Q = 0.244368. Since the series is very regular, the value
of Q is very low. The one-sided test would accept this sequence as being uniformly
distributed, whereas the two-sided test would reject the sequence.
Secondly, we consider Xn = n mod 1020 for n = 1, 2, . . . , N = 106. Since this
series never takes the values 1021 to 1023, the distribution is wrong and we expect a
large value of Q. Indeed, for this case we get Q = 232.5864 and thus both versions
of the test reject this sequence.
Random number generators used in practice, and even the LCG for large enough
values of m, pass statistical tests for the distribution of the output samples without
problems.
1.1.2.3
Independence of samples
Another aspect of the quality of PRNGs is the possibility of statistical dependence
between consecutive samples. For example, in the LCG each output sample is a
deterministic function of the previous sample and thus consecutive samples are clearly
dependent. To some extent this problem is shared by all PRNGs.
An easy way to visualise the dependence between pairs of consecutive samples
is a scatter plot of the points (Xi, Xi+1) for i = 1, 2, . . . , N −1. A selection of such
plots is shown in Figure 1.1. Figure 1.1(a) illustrates what kind of plot one would
expect if Xi ∼U[0, 1] was a true i.i.d. sequence. The remaining panels correspond
to different variants of the LCG. Figure 1.1(b) (using m = 81) clearly illustrates that
each Xi can only be followed by exactly one value Xi+1. While the same is true for
Figure 1.1(c) and (d) (using m = 1024 and m = 232, respectively), the dependence
is much convoluted there and in particular the structure of Figure 1.1(d) is visually
indistinguishable from the structure of Figure 1.1(a).
One method for constructing PRNGs where Xi+1 is not a function of Xi is to
use a function f (Xi) of the state, instead of the state Xi itself, as the output of
the PRNG. Here, f : {0, 1, . . . , m −1} →{0, 1, . . . , ˜m −1} is a map where ˜m < m
and where the same number of pre-images is mapped to each output value. Then a
uniform distribution of Xi will be mapped to a uniform distribution for f (Xi) but
the output f (Xi+1) is not a function of the previous output f (Xi). This allows to
construct random number generators with some degree of independence between
consecutive values.

RANDOM NUMBER GENERATION
7
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Xi
Xi+1
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Xi
Xi+1
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Xi
Xi+1
Xi+1
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Xi
(a)
(b)
(c)
(d)
Figure 1.1
Scatter plots to illustrate the correlation between consecutive outputs
Xi and Xi+1 of different pseudo random number generators. The random number
generators used are the runif function in R (a), the LCG with m = 81, a = 1 and
c = 8 (b), the LCG with m = 1024, a = 401, c = 101 (c) and ﬁnally the LCG with
parameters m = 232, a = 1 664 525, c = 1 013 904 223 (d). Clearly the output in the
second and third example does not behave like a sequence of independent random
variables.
One way to quantify the independence of the output samples of a PRNG is the
following criterion.
Deﬁnition 1.7
A periodic sequence (Xn)n∈N with values in a ﬁnite set S and
period length P is k-dimensionally equidistributed, if every possible subsequence
x = (x1, . . . , xk) ∈Sk of length k occurs equally often in the sequence X, that is if
Nx = #

i
 0 ≤i < P, Xi+1 = xi, . . . , Xi+k = xk

does not depend on x.
A random number generator is good, if the output is k-dimensionally equidis-
tributed for large values of k.

8
AN INTRODUCTION TO STATISTICAL COMPUTING
1.1.3
Pseudo random number generators in practice
This section contains advice on using PRNGs in practice.
First, it is normally a bad idea to implement your own PRNG: ﬁnding a good
algorithm for pseudo random number generation is a difﬁcult problem, and even
when an algorithm is available, given the nature of the generated output, it can be
a challenge to spot and remove all mistakes in the implementation. Therefore, it is
advisable to use a well-established method for random number generation, typically
the random number generator built into a well-known software package or provided
by a well-established library.
A second consideration concerns the rˆole of the seed. While different PRNGs
differ greatly in implementation details, they all use a seed (like the value X0 in
algorithm 1.2) to initialise the state of the random number generator. Often, when
non-predictability is required, it is useful to set the seed to some volatile quantity
(like the current time) to get a different sequence of random numbers for different
runs of the program. At other times it can be more useful to get reproducible results,
for example to aid debugging or to ensure repeatability of published results. In these
cases, the seed should be set to a known, ﬁxed value.
Finally, PRNGs like the LCG described above often generate a sequence which
behaves like a sequence of independent random numbers, uniformly distributed on a
ﬁnite set {0, 1, . . . , m −1} for a big value of m. In contrast, most applications require
a sequence of independent, U[0, 1]-distributed random variables, that is a sequence
of i.i.d. values which are uniformly distributed on the real interval [0, 1]. We can
obtain a sequence (Un)n∈N of pseudo random numbers to replace an i.i.d. sequence
of U[0, 1] random variables by setting
Un = Xn + 1
m + 1 ,
where (Xn)n∈N is the output of the PRNG. The output Un can only take the m different
values
1
m + 1,
2
m + 1, . . . ,
m
m + 1
and thus Un is not exactly uniformly distributed on the continuous interval [0, 1].
But, since the possible values are evenly spaced inside the interval [0, 1] and since
each of these values has the same probability, the distribution of Un is a reasonable
approximation to a uniform distribution on [0, 1]. This is particularly true since
computers can only represent ﬁnitely many real numbers exactly.
This concludes our discussion of how a replacement for an i.i.d. sequence of
U[0, 1]-distributed random numbers can be generated on a computer.
1.2
Discrete distributions
Building on the methods from Section 1.1, in this and the following sections we will
study methods to transform an i.i.d. sequence of U[0, 1]-distributed random variables

RANDOM NUMBER GENERATION
9
into an i.i.d. sequence from a prescribed target distribution. The methods from the
previous section were inexact, since the output of a PRNG is not ‘truly random’.
In contrast, the transformations described in this and the following sections can be
carried out with complete mathematical rigour. We will discuss different methods for
generating samples from a given distribution, applicable to different classes of target
distributions. In this section we concentrate on the simplest case where the target
distribution only takes ﬁnitely or countably inﬁnitely many values.
As a ﬁrst example, we consider the uniform distribution on the set A =
{0, 1, . . . , n −1}, denoted by U{0, 1, . . . , n −1}. Since the set A has n elements,
a random variable X with X ∼U{0, 1, . . . , n −1} satisﬁes
P(X = k) = 1
n
for all k ∈A. To generate samples from such a random variable X, at ﬁrst it may
seem like a good idea to just use a PRNG with state space A, for example the LCG
with modulus m = n. But considering the fact that the maximal period length of a
PRNG is restricted to the size of the state space, it becomes clear that this is not a
good idea. Instead we will follow the approach to ﬁrst generate a continuous sample
U ∼U[0, 1] and then to transform this sample into the required discrete uniform
distribution. A method to implement this idea is described in the following lemma.
Lemma 1.8
Let U ∼U[0, 1] and n ∈N. Deﬁne a random variable X by X = ⌊nU⌋,
where ⌊·⌋denotes rounding down. Then X ∼U{0, 1, . . . , n −1}.
Proof
By the deﬁnition of X we have
P (X = k) = P (⌊nU⌋= k) = P (nU ∈[k, k + 1)) = P

U ∈
k
n , k + 1
n

for all k = 0, 1, . . . , n −1.
The uniform distribution U[0, 1] is characterised by the fact that U ∼U[0, 1]
satisﬁes
P (U ∈[a, b]) = b −a
for all 0 ≤a ≤b ≤1. Also, since U is a continuous distribution, we have P(U =
x) = 0 for all x ∈[0, 1] and thus the boundary points of the interval [a, b] can be
included or excluded without changing the probability. Using these results, we ﬁnd
P (X = k) = P

U ∈
k
n , k + 1
n

= k + 1
n
−k
n = 1
n
for all k = 0, 1, . . . , n −1. This completes the proof.
Another common problem related to discrete distributions is the problem of
constructing random events which occur with a given probability p. Such events will,

10
AN INTRODUCTION TO STATISTICAL COMPUTING
for example, be needed in the rejection algorithms considered in Section 1.4. There
are many fascinating aspects to this problem, but here we will restrict ourselves to the
simplest case where the probability p is known explicitly and where we have access to
U[0, 1]-distributed random variables. This case is considered in the following lemma.
Lemma 1.9
Let p ∈[0, 1] andU ∼U[0, 1] and deﬁne the event E as E = {U ≤p}.
Then P(E) = p.
Proof
We have
P(E) = P(U ≤p) = P (U ∈[0, p]) = p −0 = p.
This completes the proof.
The idea underlying lemmas 1.8 and 1.9 can be generalised to sample from
arbitrary distributions on a ﬁnite set A. Let A = {a1, . . . , an} where ai ̸= a j for
i ̸= j and let p1, . . . , pn ≥0 be given with 	n
i=1 pi = 1. Assume that we want to
generate random values X ∈A with P(X = ai) = pi for i = 1, 2, . . . , n. Since the
pi sum up to 1, we can split the unit interval [0, 1] into disjoint sub-intervals lengths
p1, . . . , pn.
0
p1
p2
p3
· · ·
pn
1
U
With this arrangement, if we choose U ∈[0, 1] uniformly, the value of U lies in the
ith subinterval with probability pi. Thus, we can choose X to be the ai corresponding
to the subinterval which contains U. This idea is formalised in the following lemma.
Lemma 1.10
Assume A = {ai | i ∈I} where either I = {1, 2, . . . , n} for some
n ∈N or I = N, and where ai ̸= a j whenever i ̸= j. Let pi ≥0 be given for i ∈I
with 	
i∈I pi = 1. Finally let U ∼U[0, 1] and deﬁne
K = min

k ∈I

k

i=1
pi ≥U

.
(1.2)
Then X = aK ∈A satisﬁes P(X = ak) = pk for all k ∈I.
Proof
We have
P(X = ak) = P(K = k) = P
k−1

i=1
pi < U,
k

i=1
pi ≥U

= P

U ∈
k−1

i=1
pi,
k

i=1
pi

=
k

i=1
pi −
k−1

i=1
pi = pk

RANDOM NUMBER GENERATION
11
for all k ∈I, where we interpret the sum 	0
i=1 pi for k = 1 as 0. This completes the
proof.
The numerical method described by lemma 1.10 requires that we ﬁnd the index K
of the subinterval which contains U. The most efﬁcient way to do this is to ﬁnd a
function ϕ which maps the boundaries of the subintervals to consecutive integers and
then to consider the rounded value ⌊ϕ(I)⌋. This approach is taken in lemma 1.8 and
also in the following example.
Example 1.11
The geometric distribution, describing the number X of individual
trials with probability p until the ﬁrst success, has probability weights P(X = i) =
pi−1(1 −p) = pi for i ∈N. We can use lemma 1.10 with ai = i for all i ∈N to
generate samples from this distribution.
For the weights pi, the value sum in equation (1.2) can be determined explicitly:
using the formula for geometric sums we ﬁnd
k

i=1
pi = (1 −p)
k

i=1
pi−1 = (1 −p)1 −pk
1 −p = 1 −pk.
Thus, we can rewrite the event 	k
i=1 pi ≥U as follows:

U ≤
k

i=1
pi

=

U ≤1 −pk
=

pk ≤1 −U

= {k log(p) ≤log(1 −U)}
=

k ≥log(1 −U)
log(p)

.
In the last expression, we had to change the ≤sign into a ≥sign, since we divided by
the negative number log(p). By deﬁnition, the K from equation (1.2) is the smallest
integer such that 	k
i=1 pi ≥U is satisﬁed and thus the smallest integer greater than
or equal to log(1 −U)/ log(p). Thus, the value
X = aK = K =
log(1 −U)
log(p)

,
where ⌈·⌉denotes the operation of rounding up a number to the nearest integer, is
geometrically distributed with parameter p.
1.3
The inverse transform method
The inverse transform method is a method which can be applied when the target
distribution is one-dimensional, that is to generate samples from a prescribed target

12
AN INTRODUCTION TO STATISTICAL COMPUTING
x
F(x)
u
F −1(u)
v
F −1(v)
w
F −1(w)
a
Figure 1.2
Illustration of the inverse F−1 of a CDF F. At level u the function F
is continuous and injective; here F−1 coincides with the usual inverse of a function.
The value v falls in the middle of a jump of F and thus has no preimage; F−1(v) is
the preimage of the right-hand limit of F and F(F−1(v)) ̸= v. At level w the function
F is not injective, several points map to w; the preimage F−1(w) is the left-most of
these points and we have, for example, F−1(F(a)) ̸= a.
distribution on the real numbers R. The method uses the cumulative distribution
function (CDF) (see Section A.1) to specify the target distribution and can be applied
for distributions which have no density.
Deﬁnition 1.12
Let F be a distribution function. Then the inverse of F is
deﬁned by
F−1(u) = inf

x ∈R
 F(x) ≥u

for all u ∈(0, 1).
The deﬁnition of the inverse of a distribution function is illustrated in Figure 1.2.
In the case where F is bijective, that is when F is strictly monotonically increasing
and has no jumps, F−1 is just the usual inverse of a function. In this case we can
ﬁnd F−1(u) by solving the equation F(x) = u for x. The following algorithm can be
used to generate samples from a given distribution, whenever the inverse F−1 of the
distribution function can be determined.
Algorithm 1.13
(inverse transform method)
input:
the inverse F−1 of a CDF F
randomness used:
U ∼U[0, 1]

RANDOM NUMBER GENERATION
13
output:
X ∼F
1: generate U ∼U[0, 1]
2: return X = F−1(U)
This algorithm is very simple and it directly transforms U[0, 1]-distributed sam-
ples into samples with distribution function F. The following proposition shows that
the samples X generated by algorithm 1.13 have the correct distribution.
Proposition 1.14
Let F: R →[0, 1] be a distribution function and U ∼U[0, 1].
Deﬁne X = F−1(U). Then X has distribution function F.
Proof
Using the deﬁnitions of X and F−1 we ﬁnd
P(X ≤a) = P

F−1(U) ≤a

= P (inf{ x | F(x) ≥U } ≤a) .
Since inf{ x | F(x) ≥U } ≤a holds if and only if F(a) ≥U, we can conclude
P(X ≤a) = P (F(a) ≥U) = F(a)
where the ﬁnal equality comes from the deﬁnition of the uniform distribution on the
interval [0, 1].
Example 1.15
The exponential distribution Exp(λ) has density
f (x) =
λe−λx
if x ≥0 and
0
otherwise.
Using integration, we ﬁnd the corresponding CDF as
F(a) =
 a
−∞
f (x) dx =
 a
0
λe−λx dx = −e−λxa
x=0 = 1 −e−λa
for all a ≥0. Since this function is strictly monotonically increasing and continuous,
F−1 is the usual inverse of F. We have
1 −e−λx = u
⇐⇒
−λx = log(1 −u)
⇐⇒
x = −log(1 −u)
λ
and thus F−1(u) = −log(1 −u)/λ for all u ∈(0, 1). Now assume U ∼U[0, 1]. Then
proposition 1.14 gives that X = −log(1 −U)/λ is Exp(λ)-distributed. Thus we have
found a method to transform U[0, 1] random variables into Exp(λ)-distributed random
variables. The method can be further simpliﬁed by using the observation that U and
1 −U have the same distribution: if U ∼U[0, 1], then −log(U)/λ ∼Exp(λ).

14
AN INTRODUCTION TO STATISTICAL COMPUTING
Example 1.16
The Rayleigh distribution with parameter σ > 0 has density
f (x) =
⎧
⎨
⎩
x
σ 2 e−x2/2σ 2
if x ≥0 and
0
otherwise.
For this distribution we ﬁnd
F(a) =
 a
0
x
σ 2 e−x2/2σ 2 dx = −e−x2/2σ 2
a
x=0 = 1 −e−a2/2σ 2
for all a ≥0. Solving the equation u = F(x) = 1 −e−a2/2σ 2 for x we ﬁnd the
inverse F−1(u) = x =

−2σ 2 log(1 −u). By proposition 1.14 we know that X =

−2σ 2 log(1 −U) has density f if we choose U ∼U[0, 1]. As in the previous
example, we can also write U instead of 1 −U.
Example 1.17
Let X have density
f (x) =
3x2
for x ∈[0, 1] and
0
otherwise.
Then
F(a) =
 a
−∞
f (x) dx =
⎧
⎨
⎩
0
if a < 0
a3
if 0 ≤a < 1 and
1
for 1 ≤a.
Since F maps (0, 1) into (0, 1) bijectively, F−1 is given by the usual inverse function
and consequently F−1(u) = u1/3 for all u ∈(0, 1). Thus, by proposition 1.14, if
U ∼U[0, 1], the cubic root U 1/3 has the same distribution as X.
Example 1.18
Let X be discrete with P(X = 0) = 0.6 and P(X = 1) = 0.4. Then
F(a) =
⎧
⎨
⎩
0
if a < 0
0.6
if 0 ≤a < 1 and
1
if 1 ≤a.
Using the deﬁnition of F−1 we ﬁnd
F−1(u) =
0
if 0 < u ≤0.6 and
1
if 0.6 < u < 1.

RANDOM NUMBER GENERATION
15
By proposition 1.14 we can construct a random variable X with the correct distribution
from U ∼U[0, 1], by setting
X =
0
if U ≤0.6 and
1
if U > 0.6.
The inverse transform method can always be applied when the inverse F−1 is
easy to evaluate. For some distributions like the normal distribution this is not the
case, and the inverse transform method cannot be applied directly. The method can be
applied (but may not be very useful) for discrete distributions such as in example 1.18.
The main restriction of the inverse transform method is that distribution functions
only exist in the one-dimensional case. For distributions on Rd where d > 1, more
sophisticated methods are required.
1.4
Rejection sampling
The rejection sampling method is a more advanced, and very popular, method for
random number generation. Several aspects make this method different from basic
methods such as inverse transform method discussed in the previous section. First,
rejection sampling is not restricted to U[0, 1]-distributed input samples. The method
is often used in multi-stage approaches where different methods are used to generate
samples of approximately the correct distribution and then rejection sampling is used
to convert these samples to follow the target distribution exactly. Secondly, while we
state the method here only for distributions on the Euclidean space Rd, the rejection
sampling method can be generalised to work on very general spaces. Finally, a
random and potentially large number of input samples is required to generate one
output sample in the rejection method. As a consequence, the efﬁciency of the method
becomes a concern.
1.4.1
Basic rejection sampling
In this section we introduce the fundamental idea that all rejection algorithms are
based on. We start by presenting the basic algorithm which forms the prototype of
the methods presented later.
Algorithm 1.19
(basic rejection sampling)
input:
a probability density g (the proposal density),
a function p with values in [0,1] (the acceptance probability)
randomness used:
Xn i.i.d. with density g (the proposals),
Un ∼U[0, 1] i.i.d.

16
AN INTRODUCTION TO STATISTICAL COMPUTING
output:
a sequence of i.i.d. random variables with density
f (x) = 1
Z p(x)g(x)
where
Z =

p(x)g(x) dx.
(1.3)
1: for n = 1, 2, 3, . . . do
2:
generate Xn with density g
3:
generate Un ∼U[0, 1]
4:
if Un ≤p(Xn) then
5:
output Xn
6:
end if
7: end for
The effect of the random variables Un in the algorithm is to randomly decide
whether to output or to ignore the value Xn: the value Xn is output with probability
p(Xn), and using the trick from lemma 1.9 we use the event {U ≤p(Xn)} to decide
whether or not to output the value. In the context of rejection sampling, the random
variables Xn are called proposals. If the proposal Xn is chosen for output, that is if
Un ≤p(Xn), we say that Xn is accepted, otherwise we say that Xn is rejected.
Proposition 1.20
For k ∈N, let X Nk denote the kth output of algorithm 1.19. Then
the following statements hold:
(a) The elements of the sequence (X Nk)k∈N are i.i.d. with density f given by
(1.3).
(b) Each proposal is accepted with probability Z; the number of proposals
required to generate each X Nk is geometrically distributed with mean 1/Z.
Proof
For ﬁxed n, the probability of accepting Xn is
P (Un ≤p(Xn)) =

p(x)g(x) dx = Z,
(1.4)
where Z is the constant deﬁned in equation (1.3). Since the decisions whether to accept
Xn for different n are independent, the time until the ﬁrst success is geometrically
distributed with mean 1/Z as required. This completes the proof of the second
statement.
For the proof of the ﬁrst statement, ﬁrst note that the indices N1, N2, N3, . . . of
the accepted Xn are random. If we let N0 = 0, we can write
Nk = min

n ∈N
 n > Nk-1,Un ≤p(Xn)


RANDOM NUMBER GENERATION
17
for all k ∈N. If we consider the distribution of X Nk conditional on the value of Nk−1,
we ﬁnd
P

X Nk ∈A
 Nk−1 = n

=
∞

m=1
P

Nk = n + m, Xn+m ∈A
 Nk−1 = n

=
∞

m=1
P (Un+1 > p(Xn+1), . . . ,Un+m−1 > p(Xn+m−1),
Un+m ≤p(Xn+m), Xn+m ∈A
 Nk−1 = n

=
∞

m=1
P (Un+1 > p(Xn+1)) · · · P (Un+m-1 > p(Xn+m-1)) ·
P (Un+m ≤p(Xn+m), Xn+m ∈A) .
Here we used the fact that all the probabilities considered in the last expression are
independent of the value of Nk−1. Similar to (1.4) we ﬁnd
P (Un ≤p(Xn), Xn ∈A) =

A
p(x)g(x) dx
and consequently we have
P

X Nk ∈A
 Nk−1 = n

=
∞

m=1
(1 −Z)m−1

A
p(x)g(x) dx
= 1
Z

A
p(x)g(x) dx
by the geometric series formula. Since the right-hand side does not depend on n, we
can conclude
P

X Nk ∈A

= 1
Z

A
p(x)g(x) dx
and thus we ﬁnd that X Nk has density pg/Z.
To see that the X Nk are independent we need to show that
P

X N1 ∈A1, . . . , X Nk ∈Ak

=
k
i=1
P

X Ni ∈Ai


18
AN INTRODUCTION TO STATISTICAL COMPUTING
for all sets A1, . . . , Ak and for all k ∈N. This can be done by summing up the
probabilities for the cases N1 = n1, . . . , Nk = nk, similar to the ﬁrst part of the
proof, but we omit this tedious calculation here.
Example 1.21
Let X ∼U[−1, +1] and accept X with probability
p(X) =

1 −X2.
Then, by proposition 1.20, the accepted samples have density
f (x) = 1
Z p(x)g(x) = 1
Z ·

1 −x2 · 1
21[−1,+1](x),
where we use the indicator function notation from equation (A.7) to get the
abbreviation
1[−1,+1](x) =
1
if x ∈[−1, +1] and
0
otherwise
and
Z =

R

1 −x2 · 1
21[−1,+1](x) dx = 1
2
 1
−1

1 −x2 dx = 1
2 · π
2 = π
4 .
Combining these results, we ﬁnd that the density f of accepted samples is given by
f (x) = 2
π

1 −x2 1[−1,+1](x).
The graph of the density f forms a semicircle and the resulting distribution is known
as Wigner’s semicircle distribution.
One important property of the rejection algorithm 1.19 is that none of the steps in
the algorithm makes any reference to the normalisation constant Z. Thus, we do not
need to compute the value of Z in order to apply this algorithm. We will see that this
fact, while looking like a small detail at ﬁrst glance, is extremely useful in practical
applications.
1.4.2
Envelope rejection sampling
The basic rejection sampling algorithm 1.19 from the previous section is usually
applied by choosing the acceptance probabilities p so that the density f of the
output values, given by (1.3), coincides with a given target distribution. The resulting
algorithm can be written as in the following.

RANDOM NUMBER GENERATION
19
Algorithm 1.22
(envelope rejection sampling)
input:
a function f with values in [0, ∞) (the non-normalised target density),
a probability density g (the proposal density),
a constant c > 0 such that f (x) ≤c g(x) for all x
randomness used:
Xn i.i.d. with density g (the proposals),
Un ∼U[0, 1] i.i.d.
output:
a sequence of i.i.d. random variables with density
˜f (x) = 1
Z f
f (x)
where
Z f =

f (x) dx
1: for n = 1, 2, 3, . . . do
2:
generate Xn with density g
3:
generate Un ∼U[0, 1]
4:
if cg(Xn)Un ≤f (Xn) then
5:
output Xn
6:
end if
7: end for
The assumption in the algorithm is that we can already sample from the distri-
bution with probability density g, but we would like to generate samples from the
distribution with density ˜f instead. Normally, f will be chosen to be a probability
density and in this case we have ˜f = f , but in some situations the normalising con-
stant Z f is difﬁcult to obtain and due to the distinction between f and ˜f , in these
situations the algorithm can still be applied. The rejection mechanism employed in
algorithm 1.22 is illustrated in Figure 1.3. The function cg is sometimes called an
‘envelope’ for f .
Proposition 1.23
Let X Nk for k ∈N denote the kth output value of algorithm 1.22
with (non-normalised) target density f . Then the following statements hold:
(a) The elements of the sequence (X Nk)k∈N are i.i.d. with density ˜f .
(b) Each proposal is accepted with probability Z f /c; the number Mk = Nk −
Nk−1 of proposals required to generate each X Nk is geometrically distributed
with mean E(Mk) = c/Z f .
Proof
Algorithm 1.22 coincides with algorithm 1.19 where the acceptance proba-
bility p is chosen as
p(x) =
 f (x)
cg(x)
if g(x) > 0 and
1
otherwise.

20
AN INTRODUCTION TO STATISTICAL COMPUTING
x
Xk
c · g(Xk)
cg(Xk)Uk
f
cg
Figure 1.3
Illustration of the envelope rejection sampling method from algorithm
1.22. The proposal (Xk, cg(Xk) Uk) is accepted, if it falls into the area underneath the
graph of f . In Section 1.4.4 we will see that the proposal is distributed uniformly on
the area under the graph of cg.
In this situation, the normalisation constant Z from (1.3) is given by:
Z =

p(x)g(x) dx =

f (x)
cg(x) g(x) dx = 1
c

f (x) dx = Z f /c.
From proposition 1.20 we then know that the output of algorithm 1.19 is an i.i.d.
sequence with density
1
Z pg = c
Z f
f
cg g = 1
Z f
f
and that the required number of proposals to generate one output sample is geomet-
rically distributed with mean 1/Z = c/Z f .
Example 1.24
We can use rejection sampling to generate samples from the half-
normal distribution with density
f (x) =

2
√
2π exp

−x2
2

if x ≥0 and
0
otherwise.
(1.5)

RANDOM NUMBER GENERATION
21
If we assume that the proposals are Exp(λ)-distributed, then the density of the pro-
posals is
g(x) =
λ exp(−λx)
if x ≥0 and
0
otherwise.
In order to apply algorithm 1.22 we need to determine a constant c > 0 such
that f (x) ≤cg(x) for all x ∈R. For x < 0 we have f (x) = g(x) = 0. For x ≥0 we
have
f (x)
g(x) =
2
√
2πλ
exp

−x2
2 + λx

.
It is easy to check that the quadratic function −x2/2 + λx attains its maximum at
x = λ. Thus we have
f (x)
g(x) ≤c∗
for all x ≥0, where
c∗=
2
√
2πλ
exp

−λ2
2 + λ · λ

=

2
πλ2 exp

λ2/2

.
Consequently, any c ≥c∗satisﬁes the condition f ≤cg. From proposition 1.23 we
know that the average number of proposals required for generating one sample, and
thus the computational cost, is proportional to c. Thus we should choose c as small
as possible and c = c∗is the optimal choice.
Given our choice of g and c, the acceptance criterion from algorithm 1.22 can be
simpliﬁed as follows:
cg(x) U ≤f (x)
⇐⇒

2
πλ2 exp
λ2
2

λ exp(−λx) U ≤
2
√
2π
exp

−x2
2

⇐⇒
U ≤exp

−x2
2 + λx −λ2
2

⇐⇒
U ≤exp

−1
2(x −λ)2

.
This leads to the following algorithm for generating samples from the half-normal
distribution:
1: for n = 1, 2, 3, . . . do
2:
generate Xn ∼Exp(λ)
3:
generate Un ∼U[0, 1]

22
AN INTRODUCTION TO STATISTICAL COMPUTING
4:
if Un ≤exp(−1
2(Xn −λ)2) then
5:
output Xn
6:
end if
7: end for
Finally, since the density f is the density of a standard-normal distribution con-
ditioned on being positive, and since the normal distribution is symmetric, we can
generate standard normal distributed values by randomly choosing Xn or −Xn, both
with probability 1/2, for each accepted sample.
In algorithm 1.22, we can choose the density g of the proposal distribution in
order to maximise efﬁciency of the method. The only constraint is that we need to
be able to ﬁnd the constant c. This condition implies, for example, that the support
of g cannot be smaller than the support of f , that is we need g(x) > 0 whenever
f (x) > 0. The average cost of generating one sample is given by the average number
of proposals required times the cost for generating each proposal. Therefore the
algorithm is efﬁcient, if the following two conditions are satisﬁed:
(a) There is an efﬁcient method to generate the proposals Xi. This affects the
choice of the proposal density g.
(b) The average number c/Z f of proposals required to generate one sample is
small. This number is inﬂuenced by the value of c and, since the possible
choices of c depend on g, also by the proposal density g.
1.4.3
Conditional distributions
The conditional distribution PX|X∈A corresponds to the remaining randomness in X
when we already know that X ∈A occurred (see equation (A.4) for details). Sampling
from a conditional distribution can be easily done by rejection sampling. The basic
result is the following.
Algorithm 1.25
(rejection sampling for conditional distributions)
input:
a set A with P(X ∈A) > 0
randomness used:
a sequence Xn of i.i.d. copies of X (the proposals)
output:
a sequence of i.i.d. random variables with distribution PX|X∈A
1: for n = 1, 2, 3, . . . do
2:
generate Xn
3:
if Xn ∈A then
4:
output Xn
5:
end if
6: end for

RANDOM NUMBER GENERATION
23
Proposition 1.26
Let X be a random variable and let A be a set. Furthermore, let
X Nk for k ∈N denote the kth output value of algorithm 1.25. Then the following
statements hold:
(a) The elements of the sequence (X Nk)k∈N are i.i.d. and satisfy
P(X Nk ∈B) = P(X ∈B|X ∈A)
for all k ∈N and all sets B.
(b) The number Mk = Nk −Nk−1 of proposals required to generate each X Nk is
geometrically distributed with mean E(Mk) = 1/P(X ∈A).
Proof
Algorithm 1.25 is a special case of algorithm 1.19 where the acceptance
probability is chosen as p(x) = 1A(x). For this choice of p, the decision whether or
not to accept the proposal given the value of Xn is deterministic and thus we can omit
generation of the auxiliary random variables Un in the algorithm.
Now assume that the distribution of X has a density g. Using equation (1.3) we
then ﬁnd
Z =

1A(x)g(x) dx = P(X ∈A)
and by proposition 1.20 we have
P(X Nk ∈B) =

B 1A(x)g(x) dx
Z
= P(X ∈B ∩A)
P(X ∈A)
= P(X ∈B|X ∈A).
A similar proof gives the result in the case where X does not have a density. This
completes the proof of the ﬁrst statement of the proposition. The second statement is
a direct consequence of proposition 1.20.
The method presented in algorithm 1.25 works well if p = P(X ∈A) is not too
small; the time required for producing a single output sample is proportional to 1/p.
Example 1.27
We can use algorithm 1.25 to generate samples X ∼N(0, 1), con-
ditioned on X ≥a. We simply have to repeat the following two steps until enough
samples are output:
(a) generate X ∼N(0, 1);
(b) if X ≥a, output X.
The efﬁciency of this method depends on the value of a. The following table
shows the average number E(Na) of samples required to generate one output sample
for different values of a, rounded to the nearest integer:

24
AN INTRODUCTION TO STATISTICAL COMPUTING
a
1
2
3
4
5
6
E(Na)
6
44
741
31 574
3 488 556
1 013 594 692
The table shows that the method will be slow even for moderate values of a. For
a ≥5 the required number of samples is so large that the method will likely be no
longer practical.
For conditions with very small probabilities, rejection sampling can still be used
to generate samples from the conditional distribution, but we have to use the full
rejection sampling algorithm 1.22 instead of the simpliﬁed version from algorithm
1.25. This is illustrated in the following example.
Example 1.28
We can use algorithm 1.22 to generate samples from the condi-
tional distribution of X ∼N(0, 1), conditioned on X ≥a > 0. The density of the
conditional distribution is
˜f (x) = 1
Z exp(−x2/2)1[a,∞](x) = 1
Z f (x),
where Z is the normalising constant (we have included the pre-factor 1/
√
2π into Z
to simplify notation).
We can sample from this distribution using proposals of the form X = ˜X + a
where ˜X ∼Exp(λ). This proposal distribution has density
g(x) = λ exp (−λ(x −a)) 1[a,∞](x)
and we need to ﬁnd a constant c > 0 such that f (x) ≤cg(x) for all x ≥a. Also, we
can still choose the parameter λ and, in order to maximise efﬁciency of the method,
we should choose a value of λ such that the shape of g is as similar to the shape of f
as possible. In order to achieve this, we choose c and λ so that at x = a both the values
and the derivatives of f and cg coincide (see Figure 1.4 for illustration). This leads to
the conditions e−a2/2 = f (a) = cg(a) = cλ and −ae−a2/2 = f ′(a) = cg′(a) = −cλ2
and solving these two equations for the two unknowns c and λ gives λ = a and
c = e−a2/2/a.
Figure 1.4 indicates that for this choice of λ and c, condition f ≤cg will be
satisﬁed. Indeed we ﬁnd
f (x)
cg(x) =
exp(−x2/2)
1/a exp(−a2/2) · a exp (−a(x −a))
= exp

−x2/2 + ax −a2/2

= exp

−(x −a)2
2

≤1

RANDOM NUMBER GENERATION
25
x
a
f(a) = cg(a), f (a) = cg (a)
f(x)
cg(x)
Figure 1.4
Illustration of the rejection mechanism from example 1.28. The graph
shows the (scaled) proposal density cg, enveloping the (non-normalised) target den-
sity f .
and thus f (x) ≤cg(x) for all x ≥a. Thus, we can apply algorithm 1.22 with proposal
density g to generate samples from the distribution with density ˜f . The resulting
method consists of the following steps:
(a) generate ˜X ∼Exp(a) and U ∼U[0, 1];
(b) let X = ˜X + a;
(c) if U ≤exp(−(X −a)2/2), output X.
From proposition 1.23 we know that the average number Ma of proposals required
to generate one sample is
E(Ma) =
c

R f (x) dx =
exp(−a2/2)/a
 ∞
a exp(−x2/2) dx =
exp(−a2/2)
a
√
2π (1 −(a))
,
where
(a) =
1
√
2π
 a
−∞
exp(−x2/2) dx
is the CDF of the standard normal distribution. The following table lists the value of
E(Ma), rounded to three signiﬁcant digits, for different values of a:
a
1
2
3
4
5
6
E(Ma)
1.53
1.19
1.09
1.06
1.04
1.03
The table clearly shows that the resulting algorithm works well for large values of a:
the steps required to generate one proposal are more complicated than for the method
from example 1.27, but signiﬁcantly fewer proposals are required.

26
AN INTRODUCTION TO STATISTICAL COMPUTING
1.4.4
Geometric interpretation
The rejection sampling method can be applied not only to the generation of random
numbers, but also to the generation of random objects in arbitrary spaces. To illustrate
this, in this section we consider the problem of sampling from the uniform distribution
on subsets of the Euclidean space Rd. We then use the resulting techniques to give
an alternative proof of proposition 1.23, based on geometric arguments.
We write |A| for the d-dimensional volume of a set A ⊆Rd. Then the cube Q =
[a, b]3 ⊆R3 has volume |Q| = (b −a)3, the unit circle C = {x ∈R2  x2
1 + x2
2 ≤1}
has two-dimensional ‘volume’ π (area) and the line segment [a, b] ⊆R has one-
dimensional ‘volume’ b −a (length). For more general sets A, the volume can be
found by integration: we have
|A| =

Rd 1A(x) dx =

· · ·

1A(x1, . . . , xd) dxd · · · dx1.
Deﬁnition 1.29
A random variable X with values in Rd is uniformly distributed on
a set A ⊆Rd with 0 < |A| < ∞, if
P(X ∈B) = |A ∩B|
|A|
for all B ⊆Rd. As for real intervals, we use the notation X ∼U(A) to indicate that
X is uniformly distributed on A.
The intuitive meaning of X being uniformly distributed on a set A is that X is
a random element of A, and that all regions of A are hit by X equally likely. The
probability of X falling into a subset of A only depends on the volume of this subset,
but not on the location inside A.
Let X ∼U(A). From the deﬁnition we can derive simple properties of the uniform
distribution: ﬁrst we have
P(X ∈A) = |A ∩A|
|A|
= 1
and if A and B are disjoint we ﬁnd
P(X ∈B) = |A ∩B|
|A|
= |∅|
|A| = 0.
For general B ⊆Rd we get
P(X /∈B) = P(X ∈Rd \ B)
= |A ∩(Rd \ B)|
|A|
= |A \ B|
|A|
= |A| −|A ∩B|
|A|
= 1 −|A ∩B|
|A|
.

RANDOM NUMBER GENERATION
27
Lemma 1.30
Let A ⊆Rd be a set with volume 0 < |A| < ∞. Then the uniform
distribution U(A) has probability density f = 1A/|A| on Rd.
Proof
Let X ∼U(A). For B ⊆Rd we have
P(X ∈B) = |A ∩B|
|A|
= 1
|A|

Rd 1A∩B(x) dx =

Rd 1B(x)1A(X)
|A|
dx
and thus X has the given density f .
Lemma 1.31
Let X be uniformly distributed on a set A, and let B be a set with
|A ∩B| > 0. Then the conditional distribution PX|X∈B of X conditioned on the event
X ∈B coincides with the uniform distribution on A ∩B.
Proof
From the deﬁnition of the uniform distribution we get
P(X ∈C|X ∈B) = P(X ∈B ∩C)
P(X ∈B)
= |A ∩B ∩C|/|A|
|A ∩B|/|A|
= |(A ∩B) ∩C|
|A ∩B|
.
Since this is the probability of a U(A ∩B)-distributed random variable to hit the
set C, the statement is proved.
By combining the result of lemma 1.31 with the method given in algorithm 1.25,
we can sample from the uniform distribution of every set which can be covered by a
(union of) rectangles. This is illustrated in the following example.
Example 1.32
(uniform distribution on the circle) Let Xn, Yn ∼U[−1, +1] be i.i.d.
By exercise E1.10 the pairs (Xn, Yn) are then uniformly distributed on the square
A = [0, 1] × [0, 1]. Now let (Zk)k∈N be the subsequence of all pairs (Xnk, Ynk) which
satisfy the condition
X2
n + Y 2
n ≤1.
Then (Zk)k∈N is an i.i.d. sequence, uniformly distributed on the unit circle
B =

x ∈R2  |x| ≤1

. The probability p to accept each sample is given by
p = P((Xn, Vn) ∈B) = |B|
|A| = π12
22 = π
4 ≈78.5%
and the number of proposals required to generate one sample is, on average,
1/p ≈1.27.

28
AN INTRODUCTION TO STATISTICAL COMPUTING
To conclude this section, we give an alternative proof of proposition 1.23. This
proof is based on the geometric approach taken in this section and uses a connection
between distributions with general densities on Rd and uniform distributions on Rd+1,
given in the following result.
Lemma 1.33
Let f : Rd →[0, ∞) be a probability density and let
A =

(x, y) ∈Rd × [0, ∞)
 0 ≤y < f (x)

⊆Rd+1.
Then |A| = 1 and the following two statements are equivalent:
(a) (X, Y) is uniformly distributed on A.
(b) X is distributed with density f on Rd and Y = f (X)U where U ∼U[0, 1],
independently of X.
Proof
The volume of the set A can be found by integrating the ‘height’ f (x) over
all of Rd. Since f is a probability density, we get
|A| =

Rd f (x) dx = 1.
Assume ﬁrst that (X, Y) is uniformly distributed on A and deﬁne U = Y/f (X).
Since (X, Y) ∈A, we have f (X) > 0 with probability 1 and thus there is no problem
in dividing by f (X). Given sets C ⊆Rd and D ⊆R we ﬁnd
P (X ∈C,U ∈D) = P

(X, Y) ∈

(x, y)
 x ∈C, y/f (x) ∈D

=
A ∩

(x, y)
 x ∈C, y/f (x) ∈D

=

Rd

f (x)
0
1C(x)1D (y/f (x)) dy dx.
Using the substitution u = y/f (x) in the inner integral we get
P (X ∈C,U ∈D) =

Rd
 1
0
1C(x)1D(u) f (x) du dx
=

C
f (x) dx ·

D
1[0,1](u) du.
Therefore X and U are independent with densities f and 1[0,1], respectively.

RANDOM NUMBER GENERATION
29
For the converse statement assume now that the random variables X with density
f and U ∼U[0, 1] are independent, and let Y = f (X)U. Furthermore let C ⊆Rd,
D ⊆[0, ∞) and B = C × D. Then we get
P ((X, Y) ∈B) = P (X ∈C, Y ∈D)
=

C
P(Y ∈D|X = x) f (x) dx
=

C
P ( f (x)U ∈D) f (x) dx
=

C
|D ∩[0, f (x)]|
f (x)
f (x) dx
=

C
|D ∩[0, f (x)]| dx.
On the other hand we have
|A ∩B| =

Rd

f (x)
0
1B(x, y) dy dx
=

Rd 1C(x)

f (x)
0
1D(y) dy dx
=

C
|D ∩[0, f (x)]| dx
and thus P((X, Y) ∈B) = |A ∩B|. This shows that (X, Y) is uniformly distributed
on A.
An easy application of lemma 1.33 is to convert a uniform distribution of a subset
of R2 to a distribution on R with a given density f : [a, b] →R. For simplicity, we
assume ﬁrst that f lives on a bounded interval [a, b] and satisﬁes f (x) ≤M for all
x ∈[a, b]. We can generate samples from the distribution with density f as follows:
(a) Let (Xk, Yk) are be i.i.d., uniformly distributed on the rectangle R = [a, b] ×
[0, M].
(b) Consider the set A = {(x, y) ∈R
 y ≤f (x)} and let N = min{ k ∈N |
Xk ∈B }. By lemma 1.31, (X N, YN) is uniformly distributed on A.
(c) By lemma 1.33, the value X N is distributed with density f .
This procedure is illustrated in Figure 1.5.
In the situation of algorithm 1.22, that is when f is deﬁned on an unbounded
set, we cannot use proposals which are uniformly distributed on a rectangle any-
more. A solution to this problem is to use lemma 1.33 a second time to obtain a
suitable proposal distribution. This approach provides an alternative way of under-
standing algorithm 1.22: in the situation of algorithm 1.22, Xk has density g and
Uk is uniformly distributed on [0, 1]. Then we know from lemma 1.33 that the

30
AN INTRODUCTION TO STATISTICAL COMPUTING
a
b
0
M
R
f
A
f
Yk
Xk
Figure 1.5
Illustration of the rejection sampling method where the graph of the target
density is contained in a rectangle R = [a, b] × [0, M]. In this case the proposals
are uniformly distributed on the rectangle R and a proposal is accepted if it falls into
the shaded region.
pair (Xk, g(Xk)Uk) is uniformly distributed on the set {(x, v)
 0 ≤v < g(x)}. Con-
sequently, Zk = (Xk, cg(Xk)Uk) is uniformly distributed on A = {(x, y)
 0 ≤y <
cg(x)}. By lemma 1.31 and proposition 1.26, the accepted values are uniformly
distributed on the set B = {(x, y)
 0 ≤y < f (x)} ⊆A and, applying lemma 1.33
again, we ﬁnd that the Xk, conditional on being accepted, have density f . This
argument can be made into an alternative proof of proposition 1.23.
1.5
Transformation of random variables
Samples from a wide variety of distributions can be generated by considering
deterministic transformations of random variables. The inverse transform method,
introduced in Section 1.3, is a special case of this technique where we transform a
uniformly distributed random variable using the inverse of a CDF. In this section, we
consider more general transformations.
The fundamental question we have to answer in order to generate samples by
transforming a random variable is the following: if X is a random variable with
values in Rd and a given distribution, and if ϕ : Rd →Rd is a function, what is the
distribution of ϕ(X)? This question is answered in the following theorem.
Theorem 1.34
(transformation of random variables) Let A, B ⊆Rd be open sets,
ϕ : A →B be bijective and differentiable with continuous partial derivatives, and

RANDOM NUMBER GENERATION
31
let X be a random variable with values in A. Furthermore let g : B →[0, ∞) be a
probability density and deﬁne f : Rd →R by
f (x) =
g(ϕ(x)) · |det Dϕ(x)|
if x ∈A and
0
otherwise.
(1.6)
Then f is a probability density and the random variable X has density f if and only
if ϕ(X) has density g.
The matrix Dϕ used in the theorem is the Jacobian of ϕ, as given in the following
deﬁnition.
Deﬁnition 1.35
Let ϕ : Rd →Rd be differentiable. Then the Jacobian matrix Dϕ
is the d × d matrix consisting of the partial derivatives of ϕ: for i, j = 1, 2, . . . , d
we have Dϕ(x)i j = ∂ϕi
∂x j (x).
Theorem 1.34 is a consequence of the substitution rule for integrals. Before we
give the proof of theorem 1.34, we ﬁrst state the substitution rule in the required form.
Lemma 1.36
(substitution rule for integrals) Let A, B ⊆Rd be open sets, f : B →
R integrable, and ϕ: A →B be bijective and differentiable with continuous partial
derivatives. Then

B
f (y) dy =

A
f (ϕ(x)) |det Dϕ(x)| dx
where Dϕ denotes the Jacobian matrix of ϕ.
A proof of lemma 1.36 can, for example, be found in the book by Rudin (1987,
theorem 7.26). Using lemma 1.36 we can now give the proof of the transformation
rule for random variables.
Proof
(of theorem 1.34). By deﬁnition, the function f is positive and using lemma
1.36, we get

Rd f (x) dx =

A
g (ϕ(x)) · |det Dϕ(x)| dx =

B
g(y) dy = 1.
Thus f is a probability density.
Now assume that X is distributed with density f and let C ⊆B. Then, by equation
(A.8):
P (ϕ(X) ∈C) =

A
1C (ϕ(x)) f (x) dx
=

A
1C (ϕ(x)) g (ϕ(x)) · |det Dϕ(x)| dx.

32
AN INTRODUCTION TO STATISTICAL COMPUTING
Now we can apply lemma 1.36, again, to transform the integral over A into an integral
over the set B: we ﬁnd
P (ϕ(X) ∈C) =

B
1C(y)g(y) dy.
Since this equality holds for all sets C, the random variable ϕ(X) has density g. The
converse statement follows by reversing the steps in this argument.
While theorem 1.34 is most powerful in the multidimensional case, it can be
applied in the one-dimensional case, too. In this case the Jacobian matrix is a 1 × 1
matrix, that is a number, and we have |det Dϕ(x)| = |ϕ′(x)|.
Example 1.37
(two-dimensional normal distribution) Assume that we want to
sample from the two-dimensional standard normal distribution, that is from the
distribution with density
g(x, y) = 1
2π exp

−x2 + y2
2

.
Since g depends on (x, y) only via the squared length x2 + y2 of this vector, we
try to simplify g using polar coordinates. The corresponding transformation ϕ is
given by
ϕ(r, θ) = (r cos(θ),r sin(θ))
for all r > 0, ϕ ∈(0, 2π). Note that we deﬁne ϕ only on the open set A = (0, ∞) ×
(0, 2π) in order to satisfy the requirement from theorem 1.34 that ϕ must be bijective.
The resulting image set is B = ϕ(A) = R2 \ {(x, y)
 x ≥0, y = 0}, that is B is
strictly smaller than R2 since it does not include the positive x-axis. This is not a
problem, since the two-dimensional standard normal distribution hits the positive
x-axis only with probability 0 and thus takes values in the set B with probability 1.
The Jacobian matrix of ϕ is given by
Dϕ(r, θ) =
 ∂
∂r ϕ1
∂
∂θ ϕ1
∂
∂r ϕ2
∂
∂θ ϕ2

=
 cos(θ) −r sin(θ)
sin(θ)
r cos(θ)

and thus we get |det Dϕ(r, θ)| =
r cos(θ)2 + r sin(θ)2 = r. Using theorem 1.34 we
have reduced the problem of sampling from a two-dimensional normal distribution
to the problem of sampling from the density
f (r, θ) = g (ϕ(r, θ)) · |det Dϕ(r, θ)| = 1
2π exp(−r2/2) · r
on (0, ∞) × (0, 2π).

RANDOM NUMBER GENERATION
33
The density f (r, θ) does not depend on θ and we can rewrite it as the product
f (r, θ) = f1(θ) f2(r) where f1(θ) = 1/2π is the density of U[0, 2π] and f2(r) =
r exp(−r2/2). From example 1.16 we know how to sample from the density f2: if
U ∼U[0, 1], then R = √−2 log(U) has density f2. Consequently, we can use the
following steps to sample from the density g:
(a) Generate  ∼U[0, 2π] and U ∼U[0, 1] independently.
(b) Let R = √−2 log(U).
(c) Let (X, Y) = ϕ(R, ) = (R cos(), R sin()).
Then (R, ) has density f and, by theorem 1.34, the vector (X, Y) is standard
normally distributed in R2. This method for converting pairs of uniformly distributed
samples into pairs of normally distributed samples is called the Box–Muller transform
(Box and Muller, 1958).
When the lemma is used to ﬁnd sampling methods, usually g will be the given
density of the distribution we want to sample from. Our task is then to ﬁnd a trans-
formation ϕ so that the density f described by (1.6) corresponds to a distribution we
can already sample from. In this situation, ϕ should be chosen so that it ‘simpliﬁes’
the given density g. In practice, ﬁnding a useful transformation ϕ often needs some
experimentation.
Example 1.38
Assume we want to sample from the distribution with density
g(y) = 3
2
√y · 1[0,1](y). We can cancel the square root from the deﬁnition of g by
choosing ϕ(x) = x2. Then we can apply theorem 1.34 with A = B = [0, 1] and,
since |det Dϕ(x)| =
ϕ′(x)
 = 2x, we get
f (x) = g (ϕ(x)) · |det Dϕ(x)| = 3
2x · 2x = 3x2
for all x ∈[0, 1]. From example 1.17 we already know how to generate samples from
this density: If U ∼U[0, 1], then X = U 1/3 has density f and, by theorem 1.34,
Y = ϕ(X) = X2 = U 2/3 has density g.
An important application of the transformation rule from theorem 1.34 is the case
where X and ϕ(X) are both uniformly distributed. From the relation (1.6) we see that
if X is uniformly distributed and if |det Dϕ| is constant, then ϕ(X) is also uniformly
distributed. For example, using this idea we can sometimes transform the problem of
sampling from the uniform distribution on an unbounded set to the easier problem
of sampling from the uniform distribution on a bounded set. This idea is illustrated
in Figure 1.6. Combining this approach with lemma 1.33 results in the following
general sampling method.

34
AN INTRODUCTION TO STATISTICAL COMPUTING
y0
y1
B
ϕ
x0
x1
A
(a)
(b)
Figure 1.6
Illustration of the transformation used in the ratio-of-uniforms method.
The map ϕ from equation (1.7) maps the bounded set shown in (b) into the unbounded
set in (a). The areas shown in grey in (b) map into the tails in (a) (not displayed).
Since ϕ preserves area (up to a constant), the uniform distribution on the set in (b) is
mapped into the uniform distribution on the set in (a).
Theorem 1.39
(ratio-of-uniforms method) Let f : Rd →R+ be such that Z =

Rd f (x) dx < ∞and let X be uniformly distributed on the set
A =

(x0, x1, . . . , xd)
 x0 > 0, xd+1
0
d + 1 < f
x1
x0
, . . . , xd
x0

⊆R+ × Rd.
Then the vector
Y =
 X1
X0
, . . . , Xd
X0

has density 1
Z f on Rd.
Proof
The proof is an application of the transformation rule for random variables.
To see this, consider the set
B =

(y0, y1, . . . , yd)
 0 < y0 < f (y1, . . . , yd) /Z

⊆R+ × Rd

RANDOM NUMBER GENERATION
35
and deﬁne a transformation ϕ : R+ × Rd →R+ × Rd by
ϕ(x0, x1, . . . , xd) =

Zxd+1
0
d + 1 , x1
x0
, . . . , xd
x0

.
(1.7)
We have x ∈A if and only if ϕ(x) ∈B and thus ϕ maps A onto B bijectively. Since
the determinant of a triagonal matrix is the product of the diagonal elements, the
Jacobian determinant of ϕ is given by
det Dϕ(x) = det
⎛
⎜⎜⎜⎜⎝
Zxd
0
−x1
x2
0
1
x0
...
...
−xd
x2
0
1
x0
⎞
⎟⎟⎟⎟⎠
= Zxd
0
1
x0
· · · 1
x0
= Z
for all x ∈A.
Since X is uniformly distributed on A, the density h of X satisﬁes
h(x) = 1
|A|1A(x) =
1
Z|A|1B (ϕ(x)) · |det Dϕ(x)|
and by theorem 1.34 the random variable ϕ(X) then has density
g(y) =
1
Z|A|1B(y)
for all y ∈R+ × Rd. This density is constant on B and thus the random variable
ϕ(X) is uniformly distributed on B.
To complete the proof we note that the vector Y given in the statement of the
theorem consists of the last d components of ϕ(X). Using this observation, the claim
now follows from lemma 1.33.
Example 1.40
The Cauchy distribution has density
f (x) =
1
π(1 + x2).
For this case, the set A from theorem 1.39 is
A =
⎧
⎨
⎩(x0, x1)
 x0 > 0, x2
0
2 ≤
1
π

1 + ( x1
x0 )2

⎫
⎬
⎭
=

(x0, x1)
 x0 > 0, π
2 x2
0 ≤
x2
0
x2
0 + x2
1

=

(x0, x1)
 x0 > 0, x2
0 + x2
1 ≤2
π

,

36
AN INTRODUCTION TO STATISTICAL COMPUTING
that is A is a semicircle in the x0/x1-plane. Since we can sample from the uniform
distribution on the semicircle (see exercises E1.13 and E1.14), we can use the ratio-
of-uniforms method from theorem 1.39 to sample from the Cauchy distribution. The
following steps are required:
(a) Generate (X0, X1) uniformly on the semicircle A.
(b) Return Y = X1/X0.
Note that, since only the ratio between X1 and X0 is returned, A can be replaced by
a semicircle with arbitrary radius instead of the radius √2/π found above.
1.6
Special-purpose methods
There are many specialised methods to generate samples from speciﬁc distributions.
These are often faster than the generic methods described in the previous sections,
but can typically only be used for a single distribution. These specialised methods
(optimised for speed and often quite complex) form the basis of the random number
generators built into software packages. In contrast, the methods discussed in the
previous sections are general purpose methods which can be used for a wide range
of distributions when no pre-existing method is available.
1.7
Summary and further reading
In this chapter we have learned about various aspects of random number generation
on a computer. The chapter started by considering the differences between ‘pseudo
random number generators’ (the ones considered in this book) and ‘real random
number generators’ (which we will not consider further). Using the LCG as an
example, we have learned about properties of pseudo number generators. In particular
we considered the rˆole of the ‘seed’ to control reproducability of the generated
numbers. Going beyond the scope of this book, a lot of information about LCGs
and about testing of random number generators can be found in Knuth (1981).
The Mersenne Twister, a popular modern PRNG, is described in Matsumoto and
Nishimura (1998).
Building on the output of pseudo number generators, the following sections
considered various general purpose methods for generating samples from different
distributions. The methods we discussed here are the inverse transform method,
the rejection sampling method, and the ratio-of-uniforms method (a special case
of the transformation method). More information about rejection sampling and its
extensions can be found in Robert and Casella (2004, Section 2.3). A specialised
method for generating normally distributed random variables can, for example, be
found in Marsaglia and Tsang (2000). Specialised methods for generating random
numbers from various distributions are, for example, covered in Dagpunar (2007,
Chapter 4) and Kennedy and Gentle (1980, Section 6.5).
An expository presentation of random number generation and many more refer-
ences can be found in Gentle et al. (2004, Chapter II.2).

RANDOM NUMBER GENERATION
37
Exercises
E1.1
Write a function to implement the LCG. The function should take a length n,
the parameters m, a and c as well as the seed X0 as input and should return
a vector X = (X1, X2, . . . , Xn). Test your function by calling it with the
parameters m = 8, a = 5 and c = 1 and by comparing the output with the
result from example 1.3.
E1.2
Given a sequence X1, X2, . . . of U[0, 1]-distributed pseudo random
numbers, we can use a scatter plot of (Xi, Xi+1) for i = 1, . . . , n −1 in
order to try to assess whether the Xi are independent.
(a)
Create such a plot using the built-in random number generator of R:
X <- runif(1000)
plot(X[1:999], X[2:1000], asp=1)
Can you explain the resulting plot?
(b)
Create a similar plot, using your function LCG from exercise E1.1:
m <- 81
a <- 1
c <- 8
seed <- 0
X <- LCG(1000, m, a, c, seed)/m
plot(X[1:999], X[2:1000], asp=1)
Discuss the resulting plot.
(c)
Repeat the experiment from (b) using the parameters m = 1024, a =
401, c = 101 and m = 232, a = 1 664 525, c = 1 013 904 223. Discuss
the results.
E1.3
One (very early) method for pseudo random number generation is von Neu-
mann’s middle square method (von Neumann, 1951). The method works as
follows: starting with X0 ∈{0, 1, . . . , 99}, deﬁne Xn for n ∈N to be the
middle two digits of the four-digit number X2
n−1. If X2
n−1 does not have four
digits, it is padded with leading zeros. For example, if X0 = 64, we have
X2
0 = 4096 and thus X1 = 09 = 9. In the next step, we ﬁnd X2
1 = 81 = 0081
and thus X2 = 08 = 8.
(a)
Write a function which computes Xn from Xn−1.
(b)
The output of the middle square method has loops. For example, once
we have X N = 0, we will have Xn = 0 for all n ≥N. Write a program
to ﬁnd all cycles of the middle square method.
(c)
Comment
on
the
quality
of
the
middle
square
method
as
a PRNG.

38
AN INTRODUCTION TO STATISTICAL COMPUTING
E1.4
Write a program which uses the inverse transform method to generate random
numbers with the following density:
f (x) =
1/x2
if x ≥1 and
0
otherwise.
To test your program, plot a histogram of 10 000 random numbers together
with the density f .
E1.5
For n ∈N, let Kn denote the (random) number of accepted proposals
among the ﬁrst n generated proposals in algorithm 1.19. Show that, with
probability 1, we have
lim
n→∞
1
n Kn = Z.
E1.6
Implement the rejection method from example 1.24 to generate samples
from a half-normal distribution from Exp(1)-distributed proposals. Test your
program by generating a histogram of the output and by comparing the
histogram with the theoretical density of the half-normal distribution.
E1.7
In example 1.24 we have learned how rejection sampling can be used to con-
vert Exp(λ)-distributed proposals into standard normally distributed samples.
(a)
Extend the method to convert Exp(λ)-distributed proposals into
N(0, σ 2)-distributed samples.
(b)
For given σ 2, determine the optimal value of the parameter λ.
E1.8
Consider algorithm 1.22 where the target distribution has density f/Z f with
f (x) =
1
√x exp

−y2/2x −x

and Z f =
 ∞
0
f (˜x) d ˜x, and where the proposals are Exp(1)-distributed. Find
the optimal value for the constant c from algorithm 1.22 for this example.
E1.9
Let f and g be two probability densities and c ∈R with f (x) ≤cg(x) for
all x. Show that c ≥1 and that c = 1 is only possible for f = g (except
possibly on sets with volume 0).
E1.10
Let X ∼U[a, b] and Y ∼U[c, d] be independent. Using the deﬁnition of
the uniform distribution on a set, show that (X, Y) is uniformly distributed
on the rectangle R = [a, b] × [c, d].
E1.11
Without using rejection sampling, propose a method to sample from the
uniform distribution on the set
A = ([0, 1] × [0, 1]) ∪([2, 4] × [0, 1]) .
Write a program implementing your method.

RANDOM NUMBER GENERATION
39
E1.12
Without using rejection sampling, propose a method to sample from the
uniform distribution on the set
B = ([0, 2] × [0, 2]) ∪([1, 3] × [1, 3]) .
Write a program implementing your method.
E1.13
Consider the uniform distribution on a semicircle.
(a)
Explain how rejection sampling can be used to convert i.i.d. proposals
Un ∼U([−1, 1] × [0, 1]) into an i.i.d. sequence (Vk)k∈N which is uni-
formly distributed on the semicircle {(x, y) ∈R2  x2 + y2 ≤1, y ≥
0}. Compute the acceptance probability of the method.
(b)
Write a computer program which generates 1000 samples from the
uniform distribution on the semicircle, using the method from (a).
Create a scatter plot showing the random points. How many proposals
were needed to generate 1000 samples?
E1.14
Propose a rejection method to sample from the uniform distribution on the
semicircle

(x, y) ∈R2  x2 + y2 ≤1, y ≥0

which has an acceptance probability of greater than 80%. Implement your
method.
E1.15
Let (X, Y) be uniformly distributed on the semicircle

(x, y) ∈R2  x2 + y2 ≤1, y ≥0

.
Find the densities of X and Y, respectively.
E1.16
Let X be a random variable on Rd with density f : Rd →[0, ∞) and let
c ̸= 0 be a constant. Determine the density of cX.
E1.17
Let X ∼N(0, 1). Determine the density of Y = (X2 −1)/2.
E1.18
Write a program to implement the ratio-of-uniforms method to sample from
the Cauchy distribution with density
f (x) =
1
π(1 + x2).

2
Simulating statistical models
The output of the methods for random number generation considered in Chapter 1
is a series of independent random samples from a given distribution. In contrast,
most real-world statistical models of interest will involve random samples with a
non-trivial dependence structure and often samples will consist not just of a sequence
of numbers, but will feature a more complicated structure. In this chapter, we will
discuss some examples to show how the methods from Chapter 1 can be used as a
building block to generate samples from more complex statistical models.
2.1
Multivariate normal distributions
One of the most important multivariate distributions is the multivariate normal distri-
bution. In this section, we will derive the basic properties of the multivariate normal
distribution and will discuss how to generate samples from this distribution.
Deﬁnition 2.1
Let μ ∈Rd be a vector and  ∈Rd×d be a symmetric, positive
deﬁnite matrix. Then a random vector X ∈Rd is normally distributed with mean μ
and covariance matrix , if the distribution of X has density f : Rd →R given by
f (x) =
1
(2π)d/2 |det |1/2 exp

−1
2(x −μ)⊤−1(x −μ)

(2.1)
for all x ∈Rd.
An Introduction to Statistical Computing: A Simulation-based Approach, First Edition. Jochen Voss.
© 2014 John Wiley & Sons, Ltd. Published 2014 by John Wiley & Sons, Ltd.

42
AN INTRODUCTION TO STATISTICAL COMPUTING
In this deﬁnition we consider the vector x −μ ∈Rd to be a d × 1 matrix, and
the expression (x −μ)⊤denotes the transpose of this vector, that is the vector x −μ
interpreted as an 1 × d matrix. Using this interpretation we have
(x −μ)⊤−1(x −μ) =
d

i, j=1
(xi −μi)(−1)ij(x j −μ j).
The multivariate normal distribution from deﬁnition 2.1 is a generalisation of the
one-dimensional normal distribution: If  is a diagonal matrix, say
 =
⎛
⎜⎜⎜⎝
σ 2
1
0
. . .
0
0
σ 2
2
. . .
0
...
...
...
...
0
0
. . .
σ 2
d
⎞
⎟⎟⎟⎠,
then | det | = d
i=1 σ 2
i and
−1 =
⎛
⎜⎜⎜⎝
1/σ 2
1
0
. . .
0
0
1/σ 2
2
. . .
0
...
...
...
...
0
0
. . .
1/σ 2
d
⎞
⎟⎟⎟⎠
and thus the density f from (2.1) can be written as
f (x) =
1
(2π)d/2

d
i=1 σ 2
i

1/2 exp

−1
2
d

i=1
(xi −μi) 1
σ 2
i
(xi −μi)

=
d
i=1
1

2πσ 2
i
1/2 exp

−(xi −μi)2
2σ 2
i

=
d
i=1
fi (xi) ,
where the function fi, given by
fi(x) =
1

2πσ 2
i
1/2 exp

−(x −μi)2
2σ 2
i

for all x ∈R, is the density of the one-dimensional normal distribution with mean
μi and variance σ 2
i . This shows that X is normally distributed on Rd with diag-
onal covariance matrix, if and only if the components Xi for i = 1, 2, . . . , d are
independent and normally distributed on R.

SIMULATING STATISTICAL MODELS
43
A sample X from a d-dimensional normal distribution with diagonal covariance
matrix can be generated by generating the individual components Xi ∼N(μi, σ 2
i )
independently. The following lemma gives a method to transform such samples into
samples from arbitrary d-dimensional normal distributions.
Lemma 2.2
Let μ ∈Rd and A ∈Rd×d be invertible. Deﬁne  = AA⊤∈
Rd. Furthermore, let X = (X1, X2, . . . , Xd) ∈Rd be a random vector such that
X1, X2, . . . , Xd ∼N(0, 1) are independent. Then
AX + μ ∼N(μ, )
on Rd.
Proof
The result is a direct consequence of theorem 1.34: let f be the density of X
and g be the density of N(μ, ), that is
f (x) =
1
(2π)d/2 exp

−1
2x⊤x

and
g(x) =
1
(2π)d/2 |det |1/2 exp

−1
2(x −μ)⊤−1(x −μ)

for all x ∈Rd. Consider the transformation ϕ(x) = Ax + μ. Then
g (ϕ(x)) =
1
(2π)d/2 det(AA⊤)
1/2 exp

−1
2(Ax)⊤(AA⊤)−1(Ax)

=
1
(2π)d/2 det A det A⊤1/2 exp

−1
2x⊤A⊤(A⊤)−1A−1Ax

=
1
(2π)d/2 |det A| exp

−1
2x⊤x

for all x ∈Rd, since det A = det A⊤. Assume A = (aij)i, j=1,...,d. Then the Jacobian
Dϕ(x) has components
(Dϕ(x))ij =
∂
∂x j
 d

k=1
aikxk

= aij
and thus Dϕ(x) = A and | det Dϕ(x)| = | det A| for all x ∈Rd. Consequently,
f (x) = g(ϕ(x))| det Dϕ(x)| and, by theorem 1.34, the distribution of ϕ(X)
has density g.

44
AN INTRODUCTION TO STATISTICAL COMPUTING
Lemma 2.2 can be used to generate samples from a multivariate normal distribu-
tion N(μ, ). This can be done by performing the following steps:
(a) Find a matrix A such that  = AAT , for example using the Cholesky decom-
position of A.
(b) Generate independent random values X1, X2, . . . , Xd ∼N(0, 1).
(c) Return AX + μ.
In cases where many N(μ, σ)-distributed random values need to be generated for the
same covariance matrix , the algorithm can be sped up by computing the matrix A
only once and then storing it for later use.
For completeness, we now verify that the distribution N(μ, ) given by
deﬁnition 2.1 indeed has mean μ and covariance matrix .
Lemma 2.3
Let X ∼N(μ, ) where μ ∈Rd and  = (σij)i, j=1,...,n ∈Rd×d. Then
E(Xi) = μi
and
Cov(Xi, X j) = σij
for all i, j = 1, 2, . . . , d.
Proof
By lemma 2.2 we can write X as X = AZ + μ where A = (aij)i, j=1,...,d
is a matrix such that AA⊤=  and Z1, Z2, . . . , Zd are independent and standard-
normally distributed. For the components of X we ﬁnd
Xi =
d

j=1
aijZ j + μi
and thus
E(Xi) =
d

j=1
aijE(Z j) + μi = μi
and
Cov(Xi, X j) = Cov
 d

k=1
aikZk,
d

l=1
ajlZl

=
d

k,l=1
aikajlCov (Zk, Zl)
=
d

k=1
aikajk = (AA⊤)ij = σij.
This completes the proof.

SIMULATING STATISTICAL MODELS
45
Example 2.4
Let εi ∼N(0, σ 2) be i.i.d. and deﬁne
Xi =
i
k=1
εk
for all i ∈N. Then E(Xi) = 0 and
Cov(Xi, X j) = Cov

i
k=1
εk,
j

l=1
εl

=
i
k=1
j

l=1
Cov (εk, εl) .
Since Cov(εk, εl) = σ 2 if k = l and Cov(εk, εl) = 0 otherwise, we ﬁnd
Cov(Xi, X j) = min(i, j) σ 2
for all i, j ∈N. Consequently, for d ∈N, we can use lemma 2.3 to conclude that the
vector X = (X1, X2, . . . , Xd) satisﬁes X ∼N(0, ) where
 = σ 2
⎛
⎜⎜⎜⎜⎜⎝
1
1
1
. . .
1
1
2
2
. . .
2
1
2
3
. . .
3
...
...
...
...
...
1
2
3
. . .
d
⎞
⎟⎟⎟⎟⎟⎠
.
2.2
Hierarchical models
Many important statistical models have a hierarchical structure: not all random vari-
ables in the model are deﬁned simultaneously, but instead there are several ‘levels’ of
randomness and the distribution of the random variables in the later levels depends
on the values of random variables in earlier levels. This structure is, for example,
present in the following situations.
r In Bayesian models (discussed in Section 4.3) the distribution of the data
depends on the value of one or more random parameters.
r In mixture models the distribution of samples depends on the random choice
of mixture component.
r In Markov chains (discussed in Section 2.3) the distribution of the value at
time t depends on the value of the Markov chain at time t −1.
Simulating hierarchical models is often easy: the simulation procedure will be per-
formed in steps, closely following the structure of the model. We illustrate this
approach here using examples.

46
AN INTRODUCTION TO STATISTICAL COMPUTING
Example 2.5
Consider the Bayesian model where the data are described as i.i.d.
samples X1, . . . , Xn ∼N(μ, σ 2), and where the mean μ and the variance σ 2
are themselves assumed to be random with distributions σ 2 ∼Exp(λ) and μ ∼
N(μ0, ασ 2). Since the variance σ 2 occurs in the distribution of μ, the model has
the following dependence structure:
σ 2
−→
μ
−→
X1, . . . , Xn.
To generate samples from this model, we use steps corresponding to the levels in the
model:
1: generate σ 2 ∼Exp(λ)
2: generate μ ∼N(μ0, ασ 2)
3: for i = 1, . . . , n do
4:
generate Xi ∼N(μ, σ 2)
5: end for
Sometimes, the hierarchical structure of a model is not immediately clear. This
is for example the case for mixture distributions as given in the following deﬁnition,
but we will see that for generating samples from a mixture distribution it is beneﬁcial
to introduce a hierarchical structure.
Deﬁnition 2.6
Let P1, . . . , Pk be probability distributions on Rd and let θ1, . . . , θk >
0 such that k
a=1 θa = 1. Then the mixture Pθ of the distributions P1, . . . , Pk with
weights θ1, . . . , θk is given by
Pθ(A) =
k

a=1
θa Pa(A)
for all A ⊆Rd.
It is important to note that a mixture is a weighted sum of distributions, not the
distribution of a weighted sum of random variables. The difference is illustrated in
the following example.
Example 2.7
Consider the three normal distributions P1 = N(1, 0.012), P2 =
N(2, 0.52) and P3 = N(5, 0.022), together with the weights θ1 = 0.1, θ2 = 0.7
and θ3 = 0.02. If Xi ∼Pi for i = 1, 2, 3 are independent, then X = 3
i=1 θi Xi
is normally distributed. In contrast, the mixture distribution Pθ = 3
i=1 θi Pi has
the density shown in Figure 2.1. The ﬁgure shows clearly that Pθ is not a nor-
mal distribution. The density shown in Figure 2.1 was determined using the
following lemma.

SIMULATING STATISTICAL MODELS
47
X
Density
−1
0
1
2
3
4
5
0.0
0.1
0.2
0.3
0.4
0.5
Figure 2.1
A histogram generated from 10 000 samples of the mixture model from
example 2.7.
Lemma 2.8
Assume that P1, . . . , Pk have densities f1, . . . , fk. Then the mixture
distribution Pθ also has a density which is given by
fθ =
k

a=1
θa fa.
Proof
Let A ⊆Rd. Then
Pθ(A) =
k

a=1
θa Pa(A) =
k

a=1
θa

A
fa(x) dx =

A
k

a=1
θa fa(x) dx =

A
fθ(x) dx.
Thus, fθ is the density of Pθ.
At ﬁrst glance, the problem of generating samples from a mixture distribution is
straightforward: since we know the distribution, we could just use the methods from
Chapter 1 to generate such samples. But, as Figure 2.1 shows, mixture distributions
can have a complicated density and it transpires that often the easiest way to generate
samples from a mixture distribution is to artiﬁcially introduce a hierarchical struc-
ture, used only for the generation of samples. This method is used in the following
algorithm.
Algorithm 2.9
(mixture distributions)
input:
probability distributions P1, . . . , Pk
weights θ1, . . . , θk > 0 with k
a=1 θa = 1

48
AN INTRODUCTION TO STATISTICAL COMPUTING
randomness used:
Y ∈{1, 2, . . . , k} with P(Y = a) = θa for all a
samples X ∼Pa for different a ∈{1, . . . , k}
output:
X ∼Pθ
1: generate Y ∈{1, 2, . . . , k} with P(Y = a) = θa for all a
2: generate X ∼PY
3: return X
Lemma 2.10
The sample X constructed by algorithm 2.9 is distributed according
to the mixture distribution from deﬁnition 2.6, that is X ∼Pθ.
Proof
Let A ⊆Rd. By splitting the event {X ∈A} according to the possible values
of Y and using Bayes’ rule (see Section A.2), we ﬁnd
P(X ∈A) =
k

a=1
P(X ∈A, Y = a) =
k

a=1
P(Y = a)P(X ∈A|Y = a).
From step 1 of algorithm 2.9 we know P(Y = a) = θa and from step 2 we see that,
conditional on Y = a, we have X ∼Pa. Thus we get
P(X ∈A) =
k

a=1
θa Pa(A) = Pθ(A).
Thus, X ∼Pθ as required.
Algorithm 2.9 showed the idea of artiﬁcially introducing a hierarchical structure
to simplify sampling. We will reuse the idea, again in the context of mixture dis-
tributions, in Section 4.4.2. To conclude the present section, we will rephrase this
idea in a more general (and more abstract) form: one method for generating sam-
ples from a multivariate distribution P is to simulate the components one by one.
Instead of directly generating a sample X = (X1, X2, . . . , Xn) ∈Rn we can ﬁrst sam-
ple X1 from the corresponding marginal distribution and then, for i = 2, 3, . . . , n,
sampling Xi from the conditional distribution given the (already sampled) values
X1, . . . , Xi−1.
In the following algorithm we assume (for simplicity) that the distribution P has
a density p: Rd →[0, ∞) and we use the marginal density pX1 of X1, given by
pX1(x1) =

R
· · ·

R
p(x1, x2, . . . , xn) dxn · · · dx2,
as well as the conditional densities pXi|X1,...,Xi−1 as deﬁned in Section A.2.

SIMULATING STATISTICAL MODELS
49
Algorithm 2.11
(componentwise simulation)
input:
marginal density pX1
conditional densities pXi|X1,...,Xi−1 for i = 2, 3, . . . , n
randomness used:
samples from pX1 and pXi|X1,...,Xi−1
output:
a sample (X1, . . . , Xn) ∼p
1: generate X1 ∼pX1
2: for i = 2, 3, . . . , n do
3:
generate Xi ∼pXi|X1,...,Xi−1( · |X1, . . . , Xi−1)
4: end for
5: return (X1, . . . , Xn)
Lemma 2.12
The random vector (X1, . . . , Xn) constructed by algorithm 2.11 has
density p.
Proof
For i = 1, 2, . . . , n, denote the marginal density of P for the ﬁrst i coordi-
nates by pX1,...,Xi, that is let
pX1,...,Xi(x1, . . . , xi) =

R
· · ·

R
p(x1, . . . , xi, xi+1, . . . , xn) dxn · · · dxi+1
for all x1, . . . , xi ∈R. We will use induction to prove that (X1, . . . , Xi), as constructed
by the ﬁrst i iterations of the loop in algorithm 2.11, has density pX1,...,Xi for all
i = 1, 2, . . . , n.
For i = 1 we have P(X1 ∈A1) = pX1(A1) by construction of X1 in line 1 of the
algorithm. Thus the statement holds for i = 1. Let i > 1 and assume we have already
shown that (X1, . . . , Xi−1) has density pX1,...,Xi−1. The construction of Xi in line 3 of
the algorithm depends on the values of X1, . . . , Xi−1 constructed in previous steps
of the algorithm. We can integrate over all possible values of X1, . . . , Xi−1 to ﬁnd
P(X1 ∈A1, . . . , Xi ∈Ai)
=
 
1A1×···×Ai(x1, . . . , xi)
· pXi|X1,...,Xi−1( · |x1, . . . , xi−1) dxi
· pX1,...,Xi−1(x1, . . . , xi−1) dxi−1 · · · dx1
=
 
1A1×···×Ai(x1, . . . , xi) pX1,...,Xi(x1, . . . , xi) dxi · · · dx1.
This shows that (X1, . . . , Xi) has density pX1,...,Xi. Using induction, we get this
statement for all i = 1, . . . , n. Since pX1,...,Xn = p, this completes the proof.

50
AN INTRODUCTION TO STATISTICAL COMPUTING
2.3
Markov chains
Markov chains are stochastic processes, which play an important rˆole in many areas
of probability, both as objects of independent mathematical interest and as a tool
in other areas of mathematics. In this section we will give a very brief introduction
to Markov chains, concentrating on basic properties of Markov chains and how to
simulate Markov chains on a computer. Later in this book, in Chapter 4, we will see
how Markov chains can be used as a tool in random number generation.
Throughout this section, we restrict ourselves to the case of discrete-time Markov
chains and we omit some of the technical details.
Deﬁnition 2.13
A stochastic process X = (X j) j∈N0 with values in a set S is a
Markov chain, if
P

X j ∈A j
 X j−1 ∈A j−1, X j−2 ∈A j−2, . . . , X0 ∈A0

= P

X j ∈A j
 X j−1 ∈A j−1

(2.2)
for all A0, A1, . . . , A j ⊆S and all j ∈N, that is if the distribution of X j depends
on X0, . . . , X j−2 only through X j−1. The set S is called the state space of X. The
distribution of X0 is called the initial distribution of X.
Often X0 is deterministic, that is P(X0 = x) = 1 for some x ∈S; in this case x
is called the initial value or starting point of X. The index j is typically interpreted
as time.
Example 2.14
Let (ε j) j∈N be an i.i.d. sequence of random variables. The process
X given by X0 = 0 and X j = X j−1 + ε j for all j ∈N is a Markov chain. We can
write X j as
X j =
j

i=1
εi.
A Markov chain of this type is called a random walk. Important special cases are
ε j ∼U({−1, 1}) (the symmetric, simple random walk on Z) and ε j ∼N(0, 1).
Example 2.15
Let (ε j) j∈N be an i.i.d. sequence of random variables with variance
Var(ε j) = 1. Then the process X given by X0 = X1 = 0 and
X j = X j−1 + X j−2
2
+ ε j
for all j = 2, 3, . . . is not a Markov chain.

SIMULATING STATISTICAL MODELS
51
Deﬁnition 2.16
If the transition probabilities given by the right-hand side of (2.2)
do not depend on the time j, the Markov chain X is called time-homogeneous.
For the rest of this chapter (and nearly all of the book) we restrict ourselves to
the case of time-homogeneous Markov chains.
2.3.1
Discrete state space
If the state space S is ﬁnite, for example S = {1, 2, . . . , N}, then the transition
probabilities P(Xn ∈An
 Xn−1 ∈An−1) in (2.2) can be described by giving the
probabilities
pxy = P

X j = y
 X j−1 = x

of the transitions between all pairs of elements x, y ∈S. The resulting matrix P =
(pxy)x,y∈S is called the transition matrix of the Markov chain X.
When considering transition matrices, it is often convenient to label the rows
and columns of the matrix P using elements of S instead of using the usual indices
{1, 2, . . . , n}. Thus, if S is the alphabet S = {A, B, . . . , Z} we write pAZ instead of
p1,26 to denote the probability of transitions from A to Z. We write RS×S for the set
of all matrices where the columns and rows are indexed by elements of S. Similarly,
for vectors consisting of probability weights for the elements of S, for example the
initial distribution of a Markov chain, it is convenient to label the components of the
vector by elements of S. We write RS for the set of all such vectors.
At this stage, collecting the transition probabilities into a matrix could be seen
to be only a method for bookkeeping, but in the rest of this section we will see the
surprising fact that the connection between Markov chains and matrices is much
deeper. Many of the concepts from linear algebra, including matrix multiplica-
tion and eigenvectors, have a probabilistic interpretation in the context of transition
matrices!
So far we have considered Markov chains with ﬁnite state space S, but we can also
consider transition matrices for Markov chains with countably inﬁnite state space. In
these cases, the transition matrix P is an ‘inﬁnite matrix’
P =
⎛
⎜⎝
p11
p12
p13
· · ·
p21
p22
p23
· · ·
...
...
...
...
⎞
⎟⎠.
Markov chains with ﬁnite or countably inﬁnite state space are called Markov chains
with discrete state space.
Example 2.17
Consider the state space S = {1, 2, 3} and let X be the Markov chain
with X0 = 1 where transitions between states have the probabilities:

52
AN INTRODUCTION TO STATISTICAL COMPUTING
1
2
3
1/2
1/2
1/2
1/2
1
This Markov chain has transition matrix
P =
⎛
⎜⎜⎝
1
2
1
2
0
0
1
2
1
2
1
0
0
⎞
⎟⎟⎠.
Row x of this matrix, for x = 1, 2, 3, consists of the probabilities px,1, px,2, px,3 for
going from state x to states 1, 2 and 3, respectively.
Example 2.18
The symmetric simple random walk X, given by
X j =
j

i=1
εi
for all j ∈N0 with εi ∼U{−1, +1} i.i.d., is a Markov chain with state space S = Z.
Lemma 2.19
Let P S×S be the transition matrix of a Markov chain with state space S.
Then P = (pxy)x,y∈S has the following properties:
(a) pxy ≥0 for all x, y ∈S.
(b) 
y∈S pxy = 1 for all x ∈S.
Proof
The claim follows directly from the deﬁnition of P.
Deﬁnition 2.20
A vector π ∈RS is called a probability vector, if πx ≥0 for all
x ∈S and 
x∈S πx = 1.
Deﬁnition 2.21
A matrix which satisﬁes the two conditions from lemma 2.19 is
called a stochastic matrix.
In order to simulate paths of a Markov chain with discrete state space on a
computer, we have to provide a probability vector π to specify the initial distribution
and a transition matrix P to specify the transition probabilities. As in Section 2.2, the
simulation of the random variables X j is done one at a time, starting with X0.

SIMULATING STATISTICAL MODELS
53
Algorithm 2.22
(Markov chains with discrete state space)
input:
a ﬁnite or countable state space S
a probability vector π ∈RS
a stochastic matrix P = (pxy)x,y∈S ∈RS×S
randomness used:
samples from discrete distributions on S
output:
a path of a Markov chain with initial distribution π and transition matrix P
1: generate X0 ∈S with P(X0 = x) = πx for all x ∈S
2: output X0
3: for j = 1, 2, 3, . . . do
4:
generate X j ∈S with P(X j = x) = pX j−1,x for all x ∈S
5:
output X j
6: end for
Lemma 2.23
Let X be a time-homogeneous Markov chain with ﬁnite state space
and transition matrix P. Then
P

X j+k = y
 X j = x

= (Pk)xy
for all j, k ∈N0 and x, y ∈S, where Pk = P · P · · · P is the kth power of the
transition matrix P.
Proof
For k = 0, the matrix P0 is by deﬁnition the identity matrix and the statement
holds. Also, for k = 1 we have
P

X j+1 = y
 X j = x

= pxy = (P1)xy
by the deﬁnition of the transition matrix.
Now let k > 1 and assume that the statement holds for k −1. Then we have
P

X j+k = y
 X j = x

= P

X j+k = y, X j = x

P

X j = x

=
1
P

X j = x


z∈S
P

X j+k = y, X j+k−1 = z, X j = x

=
1
P

X j = x


z∈S
P

X j+k−1 = z, X j = x

·P

X j+k = y
 X j+k−1 = z, X j = x

.

54
AN INTRODUCTION TO STATISTICAL COMPUTING
Using the Markov property (2.2) we get
P

X j+k = y
 X j = x

=

z∈S
P

X j+k−1 = z, X j = x

P

X j = x

P

X j+k = y
 X j+k−1 = z

=

z∈S
P

X j+k−1 = z
 X j = x

pzy
=

z∈S
(Pk−1)xz pzy.
The last expression can be read as a matrix-matrix multiplication of Pk−1 with P and
thus we get
P

X j+k = y
 X j = x

= (Pk−1 · P)xy = (Pk)xy
as required.
Lemma 2.24
Let X be a time-homogeneous Markov chain with ﬁnite state space
and transition matrix P and initial distribution π. Then we have
P(X j = y) = (π⊤P j)y
(2.3)
for all y ∈S.
Proof
At time 0 we have
P(X0 = x) = πx
for all x ∈S. For time k > 0 we can use Bayes’ formula to ﬁnd
P(X j = y) =

x∈S
P

X j = y, X0 = x

=

x∈S
P (X0 = x) P

X j = y
 X0 = x

=

x∈S
πx(P j)xy
for all y ∈S. If we consider the transposed vector π⊤as a matrix with one row and
|S| columns, we can write the last expression as a matrix-matrix multiplication. This
gives the required expression (2.3).

SIMULATING STATISTICAL MODELS
55
Deﬁnition 2.25
Let X be a time-homogeneous Markov chain with transition
matrix P. A probability vector π is called a stationary distribution of X, if
π⊤P = π⊤, that is if

x∈S
πx pxy = πy
(2.4)
for all y ∈S.
To understand the signiﬁcance of this deﬁnition, we have to recall relation (2.3):
we know that
P(X j = y) = (π⊤P j)y
for all y ∈S. If π is a stationary distribution, we have π⊤P = π⊤and then
π⊤P j = π⊤for all j ∈N. Consequently, if we start the Markov chain with initial
distribution π, the distribution of X j does not change in time: we have X j ∼π for all
j ∈N.
In general, a Markov chain may have more than one stationary distribution, but
for the cases which will be of interest in this chapter, there will only be one stationary
distribution.
Example 2.26
On the state space S = {1, 2, 3}, consider the Markov chain with
transition matrix
P =
⎛
⎝
1/2
1/2
0
0
1/2
1/2
1/5
0
4/5
⎞
⎠
and initial distribution α = (1, 0, 0).
We can use equation (2.3) to get the distribution after one step: P(X1 = x) =
(α⊤P)x where
α⊤P =
1
0
0 
·
⎛
⎝
1/2
1/2
0
0
1/2
1/2
1/5
0
4/5
⎞
⎠=
1/2
1/2
0 
.
Similarly, for X2 we ﬁnd P(X2 = x) = (α⊤P2)x where
α⊤P2 =
1/2
1/2
0
·
⎛
⎝
1/2
1/2
0
0
1/2
1/2
1/5
0
4/5
⎞
⎠=
1/4
1/2
1/4 
.

56
AN INTRODUCTION TO STATISTICAL COMPUTING
Continuing to X2, X3, . . . we get
α⊤P3 = (0.175
0.375
0.450)
α⊤P4 = (0.178
0.275
0.548)
...
α⊤P10 = (0.223
0.222
0.555).
Experimenting shows that the value of α⊤P j does not change signiﬁcantly when
j is increased further, so we can guess that this value is close to the stationary
distribution of the Markov chain. Indeed, we can use equation (2.4) to verify that
π = (2/9, 2/9, 5/9) is a stationary distribution of X:
π⊤P =
2/9
2/9
5/9 
·
⎛
⎝
1/2
1/2
0
0
1/2
1/2
1/5
0
4/5
⎞
⎠=
2/9
2/9
5/9 
= π⊤.
Thus, we have seen that for the Markov chain considered in this example, as n →∞,
the probabilities P(X j = x) converge to the stationary probabilities πx. Convergence
results like this often, but not always, hold.
The condition for π being a stationary distribution can be rewritten by taking
the transpose of the equation π⊤P = π⊤: a probability vector π is a stationary
distribution for P if and only if
P⊤π = π,
that is if π is an eigenvector of P⊤with eigenvalue 1. Since computing eigenvectors
is a well-studied problem, and many software packages provide built-in functions for
this purpose, this property can be used to ﬁnd a stationary distribution π for a given
transition matrix P.
2.3.2
Continuous state space
In this section we will brieﬂy discuss Markov chains with continuous state space.
Markov chains can be considered on very general state spaces, but here we restrict
ourselves to the case S = Rd. Most of the results from the previous section formally
carry over to the case of continuous state space, with only changes in notation.
The concept of transition matrices in the case of continuous state space is replaced
by transition kernels, as given in the following deﬁnition.
Deﬁnition 2.27
A transition kernel is a map P(·, ·) such that:
(a) P(x, A) ≥0 for all x ∈Rd and all A ⊆Rd; and
(b) P(x, ·) is a probability distribution on Rd for all x ∈Rd.

SIMULATING STATISTICAL MODELS
57
This deﬁnition hides some of the technical complications associated with the
study of Markov chains on a continuous state space. If full mathematical rigour is
required, an additional condition, relating to ‘measurability’, must be included.
The idea in this deﬁnition is that x takes the rˆole of the current state of the Markov
chain and, for given x, the map A 	→P(x, A) is the distribution of the next value of
the Markov chain. Thus, the transition kernel of a time-homogeneous Markov chain
is deﬁned by
P(x, A) = P(X j ∈A|X j−1 = x).
Often, the conditional distribution of X j, given X j−1 = x, has a density. In this
case, instead of giving a transition kernel, we can describe the transitions of a Markov
chain by giving a transition density. In analogy to lemma 2.19, a transition density is
deﬁned in the following.
Deﬁnition 2.28
A transition density is a map p: Rd × Rd →R such that:
(a) p(x, y) ≥0 for all x, y ∈Rd; and
(b)

Rd p(x, y) dy = 1 for all x ∈Rd.
If the Markov chain X can be described by a transition density, then we have
P(X j ∈A|X j−1 = x) =

A
p(x, y) dy
for all x ∈Rd.
Example 2.29
On S = R, let X0 = 0 and
X j = 1
2 X j−1 + ε j
for all j ∈N, where ε j ∼N(0, 1) i.i.d. is a Markov chain with state space S = R.
Given X j−1 = x, we have X j = 1
2x + ε j ∼N(x/2, 1). Thus, the transition kernel
for this Markov chain satisﬁes P(x, ·) = N(x/2, 1) for all x ∈Rd. Since the normal
distribution has a density, the Markov chain X has a transition density p, given by
p(x, y) =
1
√
2π
exp

−1
2(y −x/2)2

for all x, y ∈R.
As already remarked, most of the results from the previous section carry over with
only changes in notation required, but the resulting notation is often cumbersome.
Here we restrict ourselves to state the deﬁnition of a stationary density in analogy to
deﬁnition 2.25 we have the following deﬁnition.

58
AN INTRODUCTION TO STATISTICAL COMPUTING
Deﬁnition 2.30
A probability density π: Rd →[0, ∞) is a stationary density for a
Markov chain on the state space Rd with transition density p, if it satisﬁes

S
π(x)p(x, y) dx = π(y)
for all y ∈Rd.
To conclude this section, we state the algorithm for generating paths from a
time-homogeneous Markov chain with a transition density on Rd.
Algorithm 2.31
(Markov chains with continuous state space)
input:
a probability density π: Rd →[0, ∞)
a transition density p: Rd × Rd →[0, ∞)
randomness used:
one sample X0 ∼π
samples from the densities p(x, ·) for x ∈Rd
output:
a path of a Markov chain with initial distribution π and transition matrix P
1: generate X0 ∼π
2: output X0
3: for j = 1, 2, 3, . . . do
4:
generate X j ∼p(X j−1, ·)
5:
output X j
6: end for
2.4
Poisson processes
A sample of a Poisson process consists not of a single number, but of a random number
of random points in a given set. Poisson processes are used to model the occurrence of
events in space and/or time, when the individual events are random and independent
of each other. For example, a Poisson process on a time interval [t1, t2] could be used
to model the times where individual telephone calls arrive at a call centre.
The deﬁnition of the Poisson process builds on the Poisson distribution, as given
in the following deﬁnition.
Deﬁnition 2.32
A random variable X follows a Poisson distribution with para-
meter λ, if X ∈N0 and
P(X = k) = e−λ λk
k!
for all k ∈N0. The Poisson distribution with parameter λ is denoted by Pois(λ).

SIMULATING STATISTICAL MODELS
59
For reference, we state the following basic results about Poisson distributions.
Lemma 2.33
Let X ∼Pois(λ). Then the following statements hold:
(a) E(X) = λ.
(b) Var(X) = λ.
(c) If Y ∼Pois(μ), independent of X, then X + Y ∼Pois(λ + μ).
In the mathematical formalism, the points of a Poisson process are usually col-
lected into a set, so that a sample of a Poisson process can be seen as a random set.
This idea leads to the following deﬁnition of a Poisson process.
Deﬁnition 2.34
A Poisson process on a set D ⊆Rd with intensity function λ: Rd →
[0, ∞) is a random set  ⊆D such that the following two conditions hold:
(a) If A ⊆D, then | ∩A| ∼Pois((A)) where | ∩A| is the number of points
of  in A and
(A) =

A
λ(x) dx.
(2.5)
(b) If A, B ⊆D are disjoint, then | ∩A| and | ∩B| are independent.
The intensity function λ in the deﬁnition speciﬁes how many points of the Poisson
process, on average, are located in a given region. The process will have many points
where λ is large and will have only few points where λ is small. This is illustrated in
Figure 2.2. More speciﬁcally, Since the expectation of the Pois(λ)-distribution equals
λ, the expected number of points in a set A is
E (| ∩A|) = (A).
A consequence of the independence statement in deﬁnition 2.34 is the following
lemma, which allows to build up a Poisson process from smaller building blocks.
Lemma 2.35
(a) Let  be a Poisson process on D with intensity λ and let A ⊆D. Then the
restriction  ∩A of the process  to the set A is a Poisson process on A with
intensity λ.
(b) Let 1 and 2 be independent Poisson processes on D1 and D2, respectively,
and let D1 ∩D2 = ∅. Assume that both processes have the same intensity
λ: Rd →[0, ∞). Then  = 1 ∪2 is a Poisson process on D = D1 ∪D2
with intensity λ.

60
AN INTRODUCTION TO STATISTICAL COMPUTING
−6
−4
−2
0
2
4
6
−4
−2
0
2
4
X1
X2
Figure 2.2
One sample of a two-dimensional Poisson process with a non-
homogeneous intensity. The intensity λ is high at the location of the two visi-
ble clusters and low elsewhere. The exact setup for this ﬁgure is described in
example 2.40.
Proof
The ﬁrst statement is a direct consequence of deﬁnition 2.34. For the second
statement, let A ⊆D and deﬁne Ai = A ∩Di for i = 1, 2. Since A1 and A2 are
disjoint, the random variables | ∩A1| ∼Pois((A1)) and | ∩A2| ∼Pois((A2))
are independent. Thus, by lemma 2.33:
| ∩A| = | ∩A1| + | ∩A2| ∼Pois ((A1) + (A2)) = Pois ((A)) .
This shows that  satisﬁes the ﬁrst condition from deﬁnition 2.34. For the second
condition, let A and B be disjoint. Then, the four sets Ai = A ∩Di and Bi = B ∩
Di for i = 1, 2 are disjoint. Since 1 and 2 are Poisson processes, |1 ∩A1| is
independent of |1 ∩B1| and |2 ∩A2| is independent of |2 ∩B2|. By assumption,
|1 ∩A1| is independent of |2 ∩B2| and |2 ∩A2| is independent of |1 ∩B1|.
Thus, the two values
| ∩A| = |1 ∩A1| + |2 ∩A2|
and
| ∩B| = |1 ∩B1| + |2 ∩B2|
are independent as required.

SIMULATING STATISTICAL MODELS
61
Since the deﬁnition of a Poisson process only speciﬁes the behaviour of the
number of points in a given set, but does not mention the points of  individually,
the deﬁnition cannot be directly used to simulate samples from a Poisson process.
Instead, most methods for simulating a Poisson process are based on the construction
described in the following algorithm and in proposition 2.37.
Algorithm 2.36
(Poisson process)
input:
an intensity function λ: Rd →R
a set D ⊆Rd with (D) < ∞where  is given by (2.5)
randomness used:
N ∼Pois((D))
i.i.d. samples Xi ∼1Dλ(·)/(D) for i = 1, 2, . . . , N
output:
a sample from the Poisson process on D with intensity λ
1: generate N ∼Pois((D))
2:  ←∅
3: for i = 1, 2, . . . , N do
4:
generate Xi ∼
1
(D)1Dλ(·)
5:
 ← ∪{Xi}
6: end for
7: return 
The function 1Dλ/(D) in the algorithm, is given by
1Dλ(x)
(D) =
⎧
⎨
⎩
λ(x)
(D)
if x ∈D and
0
otherwise.
This function is a probability density whenever (D) > 0. For (D) = 0 this
expression is undeﬁned but, since in this case we always have N = 0, we will
never need to generate any Xi when the density is not deﬁned, so there is no
problem.
Proposition 2.37
Let D ⊆Rd be a set and λ: Rd →[0, ∞) a function such that 
deﬁned by (2.5) satisﬁes (D) < ∞. Then the following statements hold:
(a) The output  of algorithm 2.36 is a sample of a Poisson process on D with
intensity λ.
(b) The number of iterations of the loop in algorithm 2.36 is random with expec-
tation (D).

62
AN INTRODUCTION TO STATISTICAL COMPUTING
Proof
To show that the output  is a Poisson process, we have to verify that 
satisﬁes the two conditions from deﬁnition 2.34: let A ⊆D and k ∈N. Then
P(| ∩A| = k) = P
 ∞

n=k
{| ∩A| = k, N = n}

=
∞

n=k
P(N = n)P(| ∩A| = k|N = n).
By construction of , each of the Xi independently takes a value in A with probability
p = P(Xi ∈A) =
1
(D)

A
λ(x) dx = (A)
(D).
Consequently, the probability that k out of the n values Xi are in A is given by the
binomial distribution B(n, p) and we ﬁnd
P (| ∩A| = k)
=
∞

n=k
e−(D) (D)n
n!
·
n!
k!(n −k)!
 (A)
(D)
k (D) −(A)
(D)
n−k
= e−(D) (A)k
k!
∞

n=k
((D) −(A))n−k
(n −k)!
= e−(D) (A)k
k!
e(D)−(A)
= e−(A) (A)k
k!
.
Thus, | ∩A| is Poisson-distributed with parameter (A) as required.
For the second part of the deﬁnition, let A, B ⊆D be disjoint. Similar to the
previous calculation, we ﬁnd
P (| ∩A| = k, | ∩B| = l)
=
∞

n=k+l
P(N = n)P

| ∩A| = k, | ∩B| = l
 N = n

.
Since A and B are disjoint, each of the Xi is either in A or in B or in D \ (A ∪B).
Consequently, | ∩A| = k and | ∩B| = l follow a multinomial distribution:
P (| ∩A| = k, | ∩B| = l)
=
∞

n=k+l
e−(D) (D)n
n!
n!
k!l! (n −k −l)!

SIMULATING STATISTICAL MODELS
63
·
 (A)
(D)
k  (B)
(D)
l (D) −(A) −(B)
(D)
n−k−l
= e−(D) (A)k
k!
(B)l
l!
∞

n=k+l
((D) −(A) −(B))n−k−l
(n −k −l)!
= e−(D) (A)k
k!
(B)l
l!
e(D)−(A)−(B)
= e−(A) (A)k
k!
· e−(B) (B)l
l!
= P (| ∩A| = k) · P (| ∩B| = l) ,
for all k,l ∈N0. Thus, the two random variables | ∩A| = k and | ∩B| = l are
independent. This completes the proof of the ﬁrst statement.
The second statement, about computational cost, is clear from the ﬁrst property
in lemma 2.33.
Example 2.38
Assume that we want to sample from a Poisson process with constant
intensity λ ∈R on an interval D = [a, b] ⊆R. Then we have (D) =
 b
a λ dx =
λ(b −a) and thus λ/(D) = 1/(b −a) is the density of the uniform distribution on
[a, b]. Consequently, by proposition 2.37, we can generate a sample of a Poisson
process on [a, b] by the following procedure:
(a) Generate N ∼Pois(λ(b −a)).
(b) Generate X1, X2, . . . , X N ∼U[a, b] i.i.d.
(c) Let  = {X1, X2, . . . , X N}.
Example 2.39
We can use lemma 2.35 to simulate a Poisson process with a
piecewise constant intensity function λ as shown, for example, in Figure 2.3. Let
a = t0 < t1 < · · · < tn = b be given and assume that λ satisﬁes
λ(t) =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
λ1
if t ∈[t0, t1]
λ2
if t ∈[t1, t2]
...
λn
if t ∈[tn−1, tn].
We can use algorithm 2.36 to generate a sample of a Poisson process on each of
the intervals [ti−1, ti). Since λ is constant on these intervals, we can just generate
Ni ∼Pois(λi) and then let i = {X(i)
1 , . . . , X(i)
Ni } where X(i)
1 , . . . , X(i)
Ni ∼U[ti−1, ti)
are i.i.d. Finally, by lemma 2.35 the set  = 1 ∪2 ∪· · · ∪n is a sample of a
Poisson process on [a, b] with intensity λ.

64
AN INTRODUCTION TO STATISTICAL COMPUTING
t
λ(t)
λ1
λ2
λ3
. . .
λn
a = t0
t1
t2
t3
tn−1
tn = b
Figure 2.3
A Poisson process on an interval [a, b] ⊆R with piecewise constant
intensity function λ. The function λ satisﬁes λ(t) = λi whenever t ∈[ti−1, ti). The
marked points on the t-axis form one sample of the corresponding Poisson process.
Example 2.40
Assume that we want to sample a Poisson process on the domain
D = [−6, 6] × [−4, 4] ⊆R2 with intensity function
λ(x) = 100 exp

−x2
1 + x2
2
2

+ 100 exp

−(x1 −4)2 −(x2 −2)2
for all x ∈R2. In this case, (D) is difﬁcult to determine and we cannot directly
apply algorithm 2.36. Instead, we can get samples from this Poisson process by ﬁrst
simulating the Poisson process on R2 with intensity λ, and the use lemma 2.35 to
restrict the resulting process to the given domain D.
By rewriting the intensity-function λ as
λ(x) = 200π 1
2π exp

−x2
1 + x2
2
2

+ 100π
1
2π 1
2
exp

−(x1 −4)2 + (x2 −2)2
2 · 1
2

,
we see that λ is 300π times the density of the mixture distribution
2
3 N(0, I2) + 1
3 N(μ, I2/2)
where μ = (4, 2) and I2 is the identity matrix in R2. Consequently, we have
(R2) =

R2 λ(x) dx = 200 π + 100 π = 300 π.
Thus, if we set N ∼Pois(300 π) and deﬁne ˜ = {X1, . . . , X N} where the Xi are
independent samples from the mixture distribution with density λ/300π, then ˜ is

SIMULATING STATISTICAL MODELS
65
a Poisson process on R2 with intensity λ. Finally we deﬁne  = ˜ ∩D. Then, by
lemma 2.35,  is a Poisson process on D with intensity λ. The result of a simulation
using the method described in this example is shown in Figure 2.2.
To conclude this section, we describe a rejection algorithm for converting a
Poisson process with intensity λ into a Poisson process with intensity ˜λ ≤λ by
randomly omitting some of the points.
Algorithm 2.41
(thinning method for Poisson processes)
input:
intensity functions λ, ˜λ: Rd →R with ˜λ < λ
a set D ⊆Rd with (D) < ∞where  is given by (2.5)
randomness used:
N ∼Pois((D))
i.i.d. samples Xi ∼1Dλ(·)/(D) for i = 1, 2, . . . , N
Ui ∼U[0, 1] i.i.d.
output:
a sample from the Poisson process on D with intensity ˜λ
1: generate N ∼Pois((D))
2: ˜ ←∅
3: for i = 1, 2, . . . , N do
4:
generate Xi ∼
1
(D)1Dλ(·)
5:
generate Ui ∼U[0, 1]
6:
if Ui ≤˜λ(Xi)/λ(Xi) then
7:
˜ ←˜ ∪{Xi}
8:
end if
9: end for
10: return ˜
Proposition 2.42 shows that this algorithm returns a sample from a Poisson
process with intensity ˜λ. While the result does not depend on the choice of the
auxiliary density λ, the choice of λ affects efﬁciency. Under the constraint λ ≥˜λ, the
intensity λ should be chosen such that the samples Xi in step 4 of the algorithm can
be generated efﬁciently and that (D) is as small as possible.
Proposition 2.42
Let  be a Poisson process on D ⊆Rd with intensity λ: Rd →
[0, ∞). Let ˜λ : Rd →[0, ∞) such that ˜λ(x) ≤λ(x) for all x ∈D and deﬁne a random
subset ˜ ⊆ by randomly including each point x ∈ into ˜ with probability
˜λ(x)/λ(x), independently of each other and of . Then ˜ is a Poisson process with
intensity function ˜λ.
Proof
We prove the statement of the proposition by verifying that ˜ satisﬁes
deﬁnition 2.34: let A ⊆Rd. Then, to check the ﬁrst statement from deﬁnition 2.34,
we have to show that |A ∩| is Poisson-distributed with the correct expectation.

66
AN INTRODUCTION TO STATISTICAL COMPUTING
We start the proof by ﬁrst considering the conditional distribution of , condi-
tioned on |A ∩| = n ∈N0. Using the construction of  given in algorithm 2.36 and
proposition 2.37 we see that, under this condition, the set A ∩ consists of n random
points X1, . . . , Xn ∈A, independently distributed with density λ/(A). Depending
on its location in A, each point Xi is included in ˜ with probability ˜λ(Xi)/λ(Xi)
and averaging over the possible locations for Xi, we ﬁnd the probability of Xi being
included in ˜ as
p =

A
˜λ(x)
λ(x) · λ(x)
(A) dx =
1
(A)

A
˜λ(x) dx =
˜(A)
(A),
where we used the abbreviation
˜(A) =

A
˜λ(x) dx.
Since this probability does not depend on i, the total number of points in | ˜ ∩A|,
conditioned on |A ∩| = n, is B(n, p)-distributed. Thus we have
P
 ˜ ∩A
 = k
 |A ∩| = n

=
n
k
  ˜(A)
(A)
k 
1 −
˜(A)
(A)
n−k
(2.6)
=
n
k
 ˜(A)k 
(A) −˜(A)
n−k
(A)n
.
To ﬁnd the distribution of | ˜ ∩A|, we have to average the result from equation
(2.6) over the possible values of | ∩A|. Since |A ∩| ∼Pois((A)), we get
P
 ˜ ∩A
 = k

=
∞

n=k
P
 ˜ ∩A
 = k
 |A ∩| = n

P (|A ∩| = n)
=
∞

n=k
n!
k!(n −k)!
˜(A)k 
(A) −˜(A)
n−k
(A)n
e−(A) (A)n
n!
=
˜(A)k
k!
∞

n=k

(A) −˜(A)
n−k
(n −k)!
e−(A)
=
˜(A)k
k!
∞

l=0

(A) −˜(A)
l
l!
e−(A)
=
˜(A)k
k!
e(A)−˜(A)e−(A)
= e−˜(A) ˜(A)k
k!
for all k ∈N0. Thus | ˜ ∩A| ∼Pois( ˜(A)) as required.

SIMULATING STATISTICAL MODELS
67
For the second condition from deﬁnition 2.34, let A, B ⊆D be disjoint. Then,
since  is a Poisson process, the numbers | ∩A| and | ∩B| are independent. Also,
given , the choices whether to include any of the x ∈ into the subset ˜ ⊆
are independent, and thus | ˜ ∩A| and | ˜ ∩B| are independent. This completes
the proof.
2.5
Summary and further reading
In this chapter we illustrated with the help of examples, how sequences of independent
random numbers can be used as building blocks to simulate more complex statistical
models. The key aspect of such simulations is that the random components of statis-
tical models are often no longer independent but can feature a complex dependence
structure.
Among the models discussed here were Markov chains (Section 2.3) and Poisson
processes (Section 2.4). For both classes of processes, we restricted discussion of the-
oretical aspects to a minimum. More detail can be found in the literature, for example
in the books by Norris (1997) for Markov chains and Kingman (1993) for Poisson
processes. Space restrictions force us to leave out many popular classes of models,
for example we did not discuss the autoregressive and moving average models for
time series (see e.g. Chatﬁeld, 2004), but often simulation of such processes is either
straightforward or can be performed building on the ideas illustrated in this chapter.
This concludes our study of methods for simulating statistical models. In the
following chapters we will learn how the samples by such simulations can be used
to study the underlying models. We will return to the question of how to simulate
statistical models in a slightly different context in Chapter 6, where we will consider
stochastic processes in continuous time.
Exercises
E2.1
Write a program which generates samples from the mixture of P1 =
N(1, 0.012), P2 = N(2, 0.52) and P3 = N(5, 0.022) with weights θ1 = 0.1,
θ2 = 0.7 and θ3 = 0.2. Generate a plot showing a histogram of 10 000 samples
of this distribution, together with the density of the mixture distribution.
E2.2
Let ε j ∼N(0, 1) i.i.d. for j ∈N and let X = (X j) j∈N be deﬁned by
X0 = 0, X1 = ε1 and X j = X j−1 + X j−2 + ε j for all j ≥2. Show that the
process X is not a Markov chain.
E2.3
Let X be the Markov chain with transition matrix
P =
⎛
⎜⎜⎝
2/3
1/3
0
0
1/10
9/10
0
0
1/10
0
9/10
0
1/10
0
0
9/10
⎞
⎟⎟⎠
and initial distribution μ = (1/4, 1/4, 1/4, 1/4).

68
AN INTRODUCTION TO STATISTICAL COMPUTING
(a)
Write a program which generates a random path X0, X1, X2, . . . , X N
from this Markov chain.
(b)
Use the program from (a) and Monte Carlo integration to numerically
determine the distribution of X10 (i.e. you have to estimate the proba-
bilities P(X10 = x) for x = 1, 2, 3, 4).
(c)
Analytically compute the distribution of X10. You may use a computer
to obtain your answer.
E2.4
(a)
Let P be a stochastic matrix. Show that the vector v = (1, 1, . . . , 1) is
an eigenvector of P and determine the corresponding eigenvalue.
(b)
Let v be an eigenvector of a matrix A with eigenvalue λ. Show that for
α ̸= 0 the vector αv is also an eigenvector of A.
(c)
Find a stationary distribution for the stochastic matrix P from exercise
E2.3 by computing the eigenvectors of P⊤. You can use the R functions
t and eigen for this exercise. Some care is needed because the entries
of the computed eigenvector may not be positive and may not sum up
to 1 (see (b) for an explanation). How can we solve this problem?
E2.5
Write a program which simulates a Poisson process with constant intensity
λ > 0 on an interval [a, b] ⊆R.
E2.6
Write a program which simulates a Poisson process with piecewise con-
stant intensity function λ. The program should take t0 < t1 < · · · < tn and
λ1, . . . , λn ≥0 as inputs (see Figure 2.3 and example 2.39) and should return
one sample of the Poisson process.

3
Monte Carlo methods
So far, in the ﬁrst two chapters of this book, we have learned how to simulate statistical
models on a computer. In this and the following two chapters we will discuss how such
simulations can be used to study properties of the underlying statistical model. In this
chapter we will concentrate on the approach to directly generate a large number of
samples from the given model. The idea is then that the samples reﬂect the statistical
behaviour of the model; questions about the model can then be answered by studying
statistical properties of the samples. The resulting methods are called Monte Carlo
methods.
3.1
Studying models via simulation
When studying statistical models, analytical calculations often are only possible
under assumptions such as independence of samples, normality of samples or large
sample size. For this reason, many problems occurring in ‘real life’ situations are
only approximately covered by the available analytical results. This chapter presents
an alternative approach to such problems, based on estimates derived from computer
simulations instead of analytical calculations.
The fundamental observation underlying the methods discussed in this and the
following chapters is the following: if we can simulate a statistical model on a
computer, then we can generate a large set of samples from the model and then we
can learn about the behaviour of the model by studying the computer-generated set
of samples instead of the model itself. We give three examples for this approach:
r As a consequence of the law of large numbers (see theorem A.8), the expected
value of a random variable can be approximated by generating a large number
of samples of the random variable and then considering the average value.
An Introduction to Statistical Computing: A Simulation-based Approach, First Edition. Jochen Voss.
© 2014 John Wiley & Sons, Ltd. Published 2014 by John Wiley & Sons, Ltd.

70
AN INTRODUCTION TO STATISTICAL COMPUTING
r The probability of an event can be approximated by generating a large number
of samples and then considering the proportion of samples where the event
occurs.
r The quality of a method for statistical inference can be assessed by repeatedly
generating synthetic data with a known distribution and then analysing how
well the inference method recovers the (known) properties of the underlying
distribution from the synthetic data sets.
Since many interesting questions can be reduced to computing the expectation
of some random variable, we will mostly restrict our attention to the problem of
computing expectations of the form E( f (X)) where X is a random sample from
the system under consideration and f is a real-valued function, determining some
quantity of interest in the system. There are several different methods to compute
such an expectation:
(a) Sometimes we can ﬁnd the answer analytically. For example, if the distribu-
tion of X has a density ϕ, we can use the relation
E( f (X)) =

f (x) ϕ(x) dx
(3.1)
to obtain the value of the expectation (see Section A.3). This method only
works if we can solve the resulting integral.
(b) If the integral in (3.1) cannot be solved analytically, we can try to use numer-
ical integration to get an approximation to the value of the integral. When
X takes values in a low-dimensional space, this method often works well,
but for higher dimensional spaces numerical approximation can become very
expensive and the resulting method may no longer be efﬁcient. Since numeri-
cal integration is outside the topic of statistical computing, we will not follow
this approach here.
(c) The approach we will study in this chapter is called Monte Carlo estimation
or Monte Carlo integration. This technique is based on the strong law of large
numbers: if (X j) j∈N is a sequence of i.i.d. random variables with the same
distribution as X, then
E( f (X)) = lim
N→∞
1
N
N

j=1
f (X j)
(3.2)
with probability 1.
Our aim for this chapter is to study approximations for E( f (X)) based on Equation
(3.2). While the exact equality in (3.2) holds only in the limit N →∞, we can use
the approximation with ﬁxed, large N to get the following approximation method.

MONTE CARLO METHODS
71
Deﬁnition 3.1
A Monte Carlo method for estimating the expectation E( f (X)) is a
numerical method based on the approximation
E( f (X)) ≈1
N
N

j=1
f (X j),
(3.3)
where (X j) j∈N are i.i.d. with the same distribution as X.
In order to compute a numerical value for this approximation we will need to
generate a large number of samples X j from the model. This can be done using the
techniques from Chapters 1 and 2. In the present chapter we will assume that we have
solved the problem of generating these samples and we will concentrate on the esti-
mate (3.3) itself: we will consider how the error in the approximation (3.3) depends
on the computational cost and we will derive improved variants of the basic approxi-
mation (3.3).
Example 3.2
For f (x) = x, the Monte Carlo estimate (3.3) reduces to
E(X) ≈1
N
N

j=1
X j = ¯X.
This is just the usual estimator for the mean.
Example 3.3
Assume that we want to compute the expectation E(sin(X)2) where
X ∼N(μ, σ 2). Obtaining the exact value analytically will be difﬁcult, but we can
easily get an approximation using Monte Carlo estimation: if we choose the number
of samples N large and generate independent, N(μ, σ)-distributed random variables
X1, X2, . . . , X N, then, by the strong law of large numbers, we have
E (sin(X)) ≈1
N
N

j=1
sin(X j)2.
The right-hand side of this approximation can be easily evaluated numerically, giving
an estimate for the required expectation.
While the approximation (3.3) is only stated for the problem of estimating an
expectation, the same technique applies to a wider class of problems. To illustrate
this, we consider two different problems, which can be reduced to the problem of
computing an expectation.
As a ﬁrst application of Monte Carlo estimates we consider here the problem of
estimating probabilities. While computing expectations and computing probabilities
at ﬁrst look like different problems, the latter can be reduced to the former: if X

72
AN INTRODUCTION TO STATISTICAL COMPUTING
is a random variable, we have P(X ∈A) = E(1A(X)); this relation is, for example,
explained in Section A.3. Using this equality, we can estimate P(X ∈A) by
P(X ∈A) = E (1A(X)) ≈1
N
N

j=1
1A(X j)
(3.4)
for sufﬁciently large N.
Example 3.4
Let X ∼N(0, 1) and a ∈R. Then the probability p = P(X ≤a)
cannot be computed analytically in an explicit form, but
pN = 1
N
N

j=1
1(−∞,a](X j)
can be used as an approximation for p.
As a second application of Monte Carlo estimation we consider the problem of
approximating the value of integrals such as
 b
a f (x) dx. Again, this problem at ﬁrst
looks like it is distinct from the problem of computing expectations, but it transpires
that the two problems are closely related. To see this, we utilise the relation (3.1)
from the beginning of this chapter as follows: let X, X j ∼U[a, b] be i.i.d. Then the
density of X j is ϕ(x) =
1
b−a 1[a,b]. We get
 b
a
f (x) dx = (b −a)
 b
a
f (x)ϕ(x) dx
= (b −a)E( f (X))
(3.5)
≈b −a
N
N

j=1
f (X j)
for sufﬁciently large N.
Example 3.5
To estimate the integral
 2π
0
eκ cos(x) dx we can generate X j ∼U[0, 2π]
and then use the approximation
 2π
0
eκ cos(x) dx ≈2π
N
N

j=1
eκ cos(X j).
To conclude this section, we give a longer example showing how the methods
from Chapter 1 can be used in Monte Carlo estimates.
Example 3.6
Consider a simple Bayesian inference problem, where we want to
make inference about X ∼Exp(1) using a single observation of Y ∼N(0, X). To

MONTE CARLO METHODS
73
solve this problem, we have to ﬁnd the posterior distribution, that is the conditional
distribution of X given the observation Y = y.
We can ﬁnd the density of the conditional distribution of X using Bayes’ rule
pX|Y(x|y) = pY|X(y|x)pX(x)
pY(y)
as found, for example, using equation (A.5). The value X ∼Exp(1) has density
pX(x) = exp(−x) and, given X = x, the conditional density of Y ∼N(0, X) is
pY|X(y|x) =
1
√
2πx
exp

−y2/2x

.
By averaging over the possible values of X we ﬁnd the unconditional density of Y as
pY(y) =
 ∞
0
pY|X(y|˜x) pX(˜x) d ˜x =
 ∞
0
1
√
2π ˜x
exp

−y2/2˜x −˜x

d ˜x.
Thus, the conditional density of X given Y = y is
pX|Y(x|y) = pY|X(y|x)pX(x)
pY(y)
=
1
√
2πx exp

−y2/2x −x

 ∞
0
1
√
2π ˜x exp

−y2/2˜x −˜x

d ˜x
(3.6)
= 1
Z f
f (x)
for all x ≥0, where
f (x) =
1
√x exp

−y2/2x −x

and Z f =
 ∞
0
f (˜x) d ˜x. This is the required density of the conditional distribution of
X given Y.
To derive estimates about X using Monte Carlo estimation, we need to be able
to generate samples from the posterior distribution of X, that is we need to be able
to generate samples from the distribution with the density pX|Y given in equation
(3.6). Generating samples from this distribution is complicated by the fact that we
do not know the value of the integral Z f . But, since the rejection sampling algorithm
from Section 1.4 can still be applied when the normalising constant Z f is unknown,
we can use algorithm 1.22 for this purpose. Since f is bounded near 0 and since

74
AN INTRODUCTION TO STATISTICAL COMPUTING
f (x) ≈exp(−x) when x is large, we try to use an Exp(1)-distribution for the propos-
als; the corresponding density is
g(x) = exp(−x)
for all x ≥0. For this to work, we need to ﬁnd a constant c > 0 with f (x) ≤cg(x)
for all x ≥0. A straightforward calculation (see exercise E1.8) shows that this bound
is satisﬁed for the value
c = 1
|y| exp(−1/2)
and thus the rejection sampling algorithm 1.22 can be applied to generate the required
samples.
Using samples generated by the rejection algorithm, we can now answer questions
about the posterior distribution. If we generate samples X1, X2, . . . , X N for large N,
for example for N = 10 000, we can use estimates such as
E(X|Y = y) ≈1
N
N

j=1
X j = ¯X
and
Var(X|Y = y) ≈
1
N −1
N

j=1

X j −¯X
2 .
Without the use of Monte Carlo estimation, the posterior mean E(X|Y = y) and the
posterior variance Var(X|Y = y) would be difﬁcult to determine.
3.2
Monte Carlo estimates
In this section we study the basic properties of Monte Carlo estimates as introduced
in the previous section.
Deﬁnition 3.7
Let X be a random variable and f be a function such that f (X) ∈R.
Then the Monte Carlo estimate for E( f (X)) is given by
ZMC
N
= 1
N
N

j=1
f (X j),
(3.7)
where X1, . . . , X N are i.i.d. with the same distribution as X.

MONTE CARLO METHODS
75
Since the estimate ZMC
N
is constructed from random samples X j, it is a random
quantity itself. The random variables X j in (3.7) are sometimes called i.i.d. copies
of X.
3.2.1
Computing Monte Carlo estimates
We can rewrite equation (3.7) as the following very simple algorithm.
Algorithm 3.8
(Monte Carlo estimate)
input:
a function f with values in R
N ∈N
randomness used:
i.i.d. copies (X j) j∈N of X
output:
an estimate ZMC
N
for E( f (X))
1: s ←0
2: for j = 1, 2, . . . , N do
3:
generate X j, with the same distribution as X has
4:
s ←s + f (X j)
5: end for
6: return s/N
Ignoring the negligible computational cost for the initial assignment of the vari-
able s and of the ﬁnal division by N, the execution time of algorithm 3.8 is proportional
to N. On the other hand, by the law of large numbers, we know that the estimate
ZMC
N
converges to the correct value E( f (X)) only as N increases, and thus the error
decreases as N increases. For this reason, choosing the sample size N in algorithm
3.8 involves a trade-off between computational cost and accuracy of the result. We
will discuss this trade-off in more detail in the following section.
An alternative to algorithm 3.8 would be to ﬁrst generate all samples f (X1),
f (X2), . . . , f (Xn) and then, in a second step, to compute and return the average of
the samples. The advantage of using the temporary variable s in algorithm 3.8 to
accumulate the results instead is, that the samples f (X j) do not need to be stored
in the computer’s memory, thus greatly reducing the memory requirements of the
algorithm: the memory required to execute algorithm 3.8 is independent of N.
Deﬁnition 3.7 and algorithm 3.8 do not explicitly state the space the random
variables X and X j take their values in. The reason for this omission is that algorithm
3.8 poses no restrictions on the random variable X except for the requirement that
the expectation E( f (X)) must exist. In simple cases, the random variable X could
take values in R or in Rd, but many practical applications require the values of X to
have a more complicated structure. As an example, in Section 6.5 we will consider
the case where X is a random function. In contrast, we will assume throughout this
text that f (X) ∈R so that there are no complications to deﬁne the sum in equation
(3.7) or to deﬁne the expectation E( f (X)).

76
AN INTRODUCTION TO STATISTICAL COMPUTING
3.2.2
Monte Carlo error
Since the estimate ZMC
N
from deﬁnition 3.7 and algorithm 3.8 is random, the Monte
Carlo error ZMC
N
−E( f (X)) is also random. To quantify the magnitude of this random
error, we use the concepts of bias and mean squared error from statistics.
Deﬁnition 3.9
The bias of an estimator ˆθ = ˆθ(X) for a parameter θ is given by
bias( ˆθ) = Eθ
 ˆθ(X) −θ

= Eθ
 ˆθ(X)

−θ,
where the subscript θ in the expectations on the right-hand side indicates that the
sample X in the expectation comes from the distribution with true parameter θ.
Since the bias as given in deﬁnition 3.9 depends on the value of θ, sometimes the
notation biasθ( ˆθ) is used to indicate the dependence on θ. While the bias measures
how far off the estimate is on average, a small value of the bias does not necessarily
indicate a useful estimator: even when the estimator is correct on average, the actual
values of the estimator may ﬂuctuate so wildly around θ that they are not useful in
practice. For this reason, the size of ﬂuctuations of an estimator is considered.
Deﬁnition 3.10
The standard error of an estimator ˆθ = ˆθ(X) is given by
se( ˆθ) = stdevθ
 ˆθ(X)

,
where the subscript θ on the standard deviation indicates that the sample X comes
from the distribution with true parameter θ.
Finally, the mean squared error, introduced in the following deﬁnition, combines
both kinds of error: it measures the ﬂuctuations of the estimator around the true value
of the parameter.
Deﬁnition 3.11
The mean squared error (MSE) of an estimator ˆθ = ˆθ(X) for a
parameter θ is given by
MSE( ˆθ) = Eθ
 ˆθ(X) −θ
2
.
As for the bias, the expressions for the standard error and for the mean squared
error depend on the value of θ and sometimes the notations seθ( ˆθ) and MSEθ( ˆθ) are
used to indicate this dependence. The following lemma shows that the mean squared
error can be computed from the bias and the standard error.
Lemma 3.12
Let ˆθ = ˆθ(X1, . . . , Xn) be an estimator for a parameter θ ∈R. Then
the mean squared error of ˆθ satisﬁes
MSE( ˆθ) = Var( ˆθ) + bias( ˆθ)2 = se( ˆθ)2 + bias( ˆθ)2.

MONTE CARLO METHODS
77
Proof
We have
MSE( ˆθ) = Eθ

( ˆθ −θ)2
= Eθ( ˆθ2) −2θEθ( ˆθ) + θ2
= Eθ( ˆθ2) −Eθ( ˆθ)2 + Eθ( ˆθ)2 −2θEθ( ˆθ) + θ2
= Eθ( ˆθ2) −Eθ( ˆθ)2 +

Eθ( ˆθ) −θ
2
= Var( ˆθ) + bias( ˆθ)2.
This completes the proof.
Example 3.13
Let X1, . . . , X N ∼N(μ, σ 2) be i.i.d. with a known variance σ 2 and
unknown mean μ ∈R. Then
ˆμ(X) = 1
n
n

i=1
Xi
is an estimator for μ with
bias( ˆμ) = Eμ
	
1
n
n

i=1
Xi

−μ = 1
n
n

i=1
Eμ(Xi) −μ = 1
n
n

i=1
μ −μ = 0
and, using the independence of the Xi,
se( ˆμ) =



Varμ
	
1
n
n

i=1
Xi

=



 1
n2
n

i=1
Varμ(Xi) =

nσ 2
n2 = σ
√n
for all μ ∈R. Finally, using lemma 3.12, we ﬁnd MSE( ˆθ) = (σ/√n)2 + 02 = σ 2/n.
Proposition 3.14
The Monte Carlo estimate ZMC
N
for E( f (X)), as computed in
algorithm 3.8, has
bias

ZMC
N

= 0
and
MSE

ZMC
N

= Var

ZMC
N

= 1
N Var( f (X)).
(3.8)

78
AN INTRODUCTION TO STATISTICAL COMPUTING
Proof
The expectation of ZMC
N
is given by
E

ZMC
N

= E
⎛
⎝1
N
N

j=1
f (X j)
⎞
⎠= 1
N
N

j=1
E

f (X j)

= E( f (X))
and thus we have
bias

ZMC
N

= E

ZMC
N

−E( f (X)) = 0.
Since the X j are independent, the variance of the Monte Carlo estimate can be
found as
Var

ZMC
N

= Var
⎛
⎝1
N
N

j=1
f (X j)
⎞
⎠= 1
N 2
N

j=1
Var

f (X j)

= 1
N Var( f (X)).
Finally, using lemma 3.12, we get
MSE

ZMC
N

= Var

ZMC
N

+ bias

ZMC
N
2 = 1
N Var( f (X)) + 0.
This completes the proof.
Example 3.15
Let X ∼N(0, 1) and assume that we want to estimate the expectation
E(sin(X)2). The Monte Carlo estimate for this expectation is given by
ZMC
N
=
N

j=1
sin(X j)2,
where X1, . . . , X N ∼N(0, 1) are independent. From proposition 3.14 we know that
ZMC
N
is random and that, for every value of N, we have E(ZMC
N ) = E(sin(X)2).
But due to random ﬂuctuations, individual Monte Carlo estimates ZMC
N
will not
be equal to this value, but instead will ﬂuctuate around this value with variance
proportional to 1/N. This is illustrated in Figure 3.1 which shows the spread of the
distribution of ZMC
N
for two different values of N. The ﬁgure shows histograms of
a large number of Monte Carlo estimates with sample size N = 1000 (a) and N =
10 000 (b). As expected from proposition 3.14, the distribution of the estimates with
N = 10 000 has noticeably smaller variance than the distribution of the estimates with
N = 1000.

MONTE CARLO METHODS
79
Z1000
Frequency
0.40
0.42
0.44
0.46
0
500
1000
2000
Z10 000
Frequency
0.40
0.42
0.44
0.46
0
500
1000
2000
(a)
(b)
Figure 3.1
Histograms of Monte Carlo estimates for the expectation E(sin(X)2)
where X ∼N(0, 1). The plots clearly show that the estimates with (a) N = 1000
and (b) N = 10 000 are clustered around the same mean. (b) shows that the estimates
using N = 10 000 samples have much smaller variance, and thus smaller error, than
the estimates for N = 1000. This conﬁrms the result of proposition 3.14.
Since the mean squared error as considered in proposition 3.14 is a squared
quantity, its magnitude is not directly comparable with the magnitude of the estimate
ZMC
N
itself. For this reason, we occasionally consider the root-mean-square error,
given by
RMSE

ZMC
N

=

MSE

ZMC
N

= stdev( f (X))
√
N
.
From this we see that the error of a Monte Carlo estimate decays proportionally to
1/
√
N. As a consequence of this result, in practice huge numbers of samples can
be required to achieve a reasonable level of error. For example, to increase accuracy
by a factor of 10, that is to get one more signiﬁcant digit of the result, one needs to
increase the number of samples, and thus the computational cost of the method, by a
factor of 100.
An alternative way to write the results from this section is using the so-called ‘big
O notation’, as described in the following deﬁnition.
Deﬁnition 3.16
Given two function f : N →R and g: N →R we say that f is
of order O(g), written symbolically as f (N) = O(g(N)), as N →∞if there are
constants N0 ∈N and c > 0 such that
| f (N)| ≤c |g(N)|
for all N ≥N0.

80
AN INTRODUCTION TO STATISTICAL COMPUTING
Similar deﬁnitions exist for other limits, for example for f (δ) = O(g(δ)) as δ ↓0.
Using this notation we can summarise the result of proposition 3.14 as
MSE

ZMC
N

= O
 1
N

and for the root-mean-square error we get
RMSE

ZMC
N

= O
 1
√
N

.
3.2.3
Choice of sample size
The error bound from proposition 3.14 can be used to guide the choice of sample size
N in the Monte Carlo method from algorithm 3.8. The most direct way to use the
proposition in this context is to use equation (3.8) to determine the error of the result
after a run of algorithm 3.8 has completed. If the error is too large, another run with
a larger value of N can be started, until the required precision is reached. The bound
of equation (3.8) can only be applied directly, if the sample variance Var( f (X)) is
known. If the sample variance is unknown, it can be estimated together during the
computation of the Monte Carlo estimate: The estimate ZMC
N
has mean squared error
MSE

ZMC
N

= Var( f (X))
N
≈ˆσ 2
N ,
where
ˆσ 2 =
1
N −1
N

j=1

f (X j) −ZMC
N
2
(3.9)
is the sample variance of the generated values f (X1), . . . , f (X N).
An alternative way to use proposition 3.14 is to determine the required sample
size N for a run of algorithm 3.8 in advance: by solving equation (3.8) for N we see
that, in order to achieve error MSE(ZMC
N ) ≤ε2, the sample size N must satisfy
N ≥Var( f (X))
ε2
.
(3.10)
If the value of Var( f (X)) is known, equation (3.10) can be directly used to ﬁnd an
appropriate sample size N for algorithm 3.8. Normally, the variance Var( f (X)) in
a Monte Carlo estimate is not explicitly known, but there are various ways to work
around this problem. For example, if an upper bound for Var( f (X)) is known, this
bound can be used in place of the true variance.

MONTE CARLO METHODS
81
Example 3.17
Assume Var( f (X)) = 1. In order to estimate E( f (X)) so that the
error satisﬁes MSE(ZMC
N ) = ε2 for ε = 0.01, we can use a Monte Carlo estimate with
N ≥Var( f (X))
ε2
=
1
(0.01)2 = 10 000
samples.
Example 3.18
Let X be a real-valued random variable and A ⊆R. As we have
seen in equation (3.4), we can estimate p = P(X ∈A) by
ZMC
N
= 1
N
N

j=1
1A(X j),
where the X j are i.i.d. copies of X. The variance of the Monte Carlo samples is
Var (1A(X)) = E

1A(X)2
−E (1A(X))2 = p −p2 = p(1 −p).
Thus, from equation (3.10) we know that we can achieve MSE(ZMC
N ) ≤ε2 by choos-
ing
N ≥p(1 −p)
ε2
.
(3.11)
This bound depends on the unknown probability p but, since p(1 −p) ≤1/4 for all
p ∈[0, 1], choosing
N ≥
1
4ε2
is always sufﬁcient to reduce the mean squared error to ε2.
Another approach to using the bound (3.10) in cases where the variance of
the Monte Carlo samples is not known is the following: one can try a two-step
procedure where ﬁrst Monte Carlo integration with a ﬁxed number N0 of samples
(say N0 = 10 000) is used to estimate Var( f (X)). Then, in a second step, one can use
estimates as above to determine the required value of N to achieve a mean squared
error of less than ε2. In this approach, new samples X1, . . . , X N should be generated
for the ﬁnal estimate instead of reusing the initial N0 samples.
Finally, sequential methods can be used to control the error of a Monte Carlo
estimate. In these methods, one generates samples X j one-by-one and estimates
the standard deviation of the generated f (X j) from time to time. The procedure is
continued until the estimated standard deviation falls below a prescribed limit. This
approach should be used with care, since a bias can be introduced by the fact that N
now depends on the samples used in the estimate.

82
AN INTRODUCTION TO STATISTICAL COMPUTING
So far we have seen how the description of the mean squared error from proposi-
tion 3.14 can be used to determine the error of a given Monte Carlo method. Another
application of the mean squared error is to compare different Monte Carlo estimates
for the same quantity. The smaller the variance Var( f (X)) of the samples is, the better
is the resulting Monte Carlo method. We will discuss this idea in detail in Section 3.3.
3.2.4
Reﬁned error bounds
We will conclude this section by showing how the central limit theorem can be used
to obtain reﬁned bounds for the Monte Carlo error.
Lemma 3.19
Let α ∈(0, 1) and qα = 
−1(1 −α/2) where 
 is the CDF of the
standard normal distribution N(0, 1). Furthermore let σ 2 = Var( f (X)) and
N ≥q2
ασ 2
ε2 .
Then, approximately for large N, the Monte Carlo estimate ZMC
N
for E( f (X)), given
by equation (3.7), satisﬁes
P
ZMC
N
−E( f (X))
 ≤ε

≥1 −α.
Proof
As an abbreviation, write
eMC
N
= ZMC
N
−E( f (X)) = 1
N
N

j=1
f (X j) −E( f (X)).
Using this notation, the results of proposition 3.14 state that E(eMC
N ) = 0 and
Var(eMC
N ) = σ 2/N. By the central limit theorem (see theorem A.9), for large val-
ues of N we ﬁnd, approximately,
√
N
σ eMC
N
∼N(0, 1)
and consequently
P

|eMC
N | ≤ε

= P
	
|eMC|
σ/
√
N
≤ε
√
N
σ

≈
	
ε
√
N
σ

−
	
−ε
√
N
σ

= 2
	
ε
√
N
σ

−1.
Thus we have
P

|eMC
N | ≤ε

≥1 −α

MONTE CARLO METHODS
83
if and only if
2
	
ε
√
N
σ

−1 ≥1 −α.
The latter inequality is equivalent to
ε
√
N
σ
≥
−1 
1 −α
2

= qα
and solving this inequality for N gives the required lower bound on N.
As a special case of lemma 3.19, for α = 5% we have q0.05 = 
−1(0.975) ≈1.96
and thus we need
N ≥1.962σ 2
ε2
samples in order to have an absolute error of at most ε with 95% probability. Compar-
ing this inequality with (3.10), we see that approximately 4 times as many samples
are required as for the condition MSE(ZMC
N ) ≤ε2.
Example 3.20
Assume Var( f (X)) = 1. In order to estimate E( f (X)) so that the
error |ZMC
N
−E( f (X))| is at most ε = 0.01 with probability at least 1 −α = 95%,
we can use a Monte Carlo estimate with
N ≥1.962Var( f (X))
ε2
= 1.962
(0.01)2 = 38 416
samples.
An alternative way to express the result of lemma 3.19 is to replace the point esti-
mator ZMC
N
by a conﬁdence interval for E( f (X)): we ﬁnd that ZMC
N
(approximately)
satisﬁes
P

E

f (X)

∈

ZMC
N
−σqα
√
N
, ZMC
N
+ σqα
√
N

≥1 −α
(3.12)
for sufﬁciently large N, where qα and σ 2 are as in lemma 3.19.
If the standard deviation σ in (3.12) is unknown, it can be replaced by the estimate
ˆσ from equation (3.9). A standard result from statistics shows that in this case the
value qα should be replaced by the corresponding quantile of Student’s t-distribution
with N −1 degrees of freedom. These quantiles converge to qα as N →∞and for
the values of N used in a Monte Carlo estimate, qα can be used without problems.

84
AN INTRODUCTION TO STATISTICAL COMPUTING
In all the error estimates of this section, we used the fact that f (X) has ﬁnite
variance. As Var( f (X)) gets bigger, convergence to the correct result gets slower
and slower, and the required number of samples to obtain a given error increases to
inﬁnity. Since the strong law of large numbers does not require the random variables
to have ﬁnite variance, equation (3.2) still holds for Var( f (X)) = ∞, but in this case
the convergence in (3.2) will be extremely slow and the resulting method will not be
useful in practice.
3.3
Variance reduction methods
As we have seen, the efﬁciency of Monte Carlo estimation is determined by the
variance of the estimate: the higher the variance, the more samples required to obtain
a given accuracy. This chapter describes methods to improve efﬁciency by considering
modiﬁed Monte Carlo methods. Compared with the basic Monte Carlo method from
Section 3.2, the improved methods considered here produce estimates with a lower
variance and thus with smaller error.
3.3.1
Importance sampling
The importance sampling method is based on the following argument. Assume that
X is a random variable with density ϕ, that f is a real-valued function and that ψ is
another probability density with ψ(x) > 0 whenever f (x)ϕ(x) > 0. Then we have
E( f (X)) =

f (x) ϕ(x) dx =

f (x) ϕ(x)
ψ(x) ψ(x) dx,
where we deﬁne the fraction to be 0 whenever the denominator (and thus the numer-
ator) equals 0. Since ψ is a probability density, the integral on the right can be written
as an expectation again: if Y has density ψ, we have
E( f (X)) = E

f (Y) ϕ(Y)
ψ(Y)

.
(3.13)
Now we can use a basic Monte Carlo estimate for the expectation on the right-hand
side to get the following estimate.
Deﬁnition 3.21
Let X be a random variable with density ϕ and let f be a function
such that f (X) ∈R. Furthermore, let ψ be another probability density, on the same
space as ϕ. Then the importance sampling estimate for E( f (X)) is given by
ZIS
N = 1
N
N

j=1
f (Y j) ϕ(Y j)
ψ(Y j)
(3.14)
where the Y j are i.i.d. with density ψ.

MONTE CARLO METHODS
85
The estimator ZIS
N can be used as an alternative to the basic Monte Carlo estimator
ZMC
N
from (3.7). Instead of the arithmetic average used in the basic Monte Carlo
method, the importance sampling method uses a weighted average where each sample
Y j is assigned the weight ϕ(Y j)/ψ(Y j). We can write the resulting estimation method
as the following algorithm.
Algorithm 3.22
(importance sampling)
input:
a function f
the density ϕ of X
an auxiliary density ψ
N ∈N
randomness used:
an i.i.d. sequence (Y j) j∈N with density ψ
output:
an estimate ZIS
N for E( f (X))
1: s ←0
2: for j = 1, 2, . . . , N do
3:
generate Y j ∼ψ
4:
s ←s + f (Y j)ϕ(Y j)/ψ(Y j)
5: end for
6: return s/N
This method is a generalisation of the basic Monte Carlo method: if we choose
ψ = ϕ, the two densities in (3.14) cancel and the Y j are i.i.d. copies of X; for this
case, the method is identical to basic Monte Carlo estimation. The usefulness of
importance sampling lies in the fact that we can choose the density ψ (and thus the
distribution of the Y j) in order to maximise efﬁciency.
As for the basic Monte Carlo method, the sample size N can be used to control
the balance between error and computational cost. As N increases, the computational
cost increases but the error of the method decreases.
Proposition 3.23
The importance sampling estimate ZIS
N from (3.14), as computed
by algorithm 3.22, has
bias

ZIS
N

= 0
and
MSE

ZIS
N

= 1
N Var

f (Y) ϕ(Y)
ψ(Y)

= 1
N

Var( f (X)) −E

f (X)2

1 −ϕ(X)
ψ(X)

.
(3.15)

86
AN INTRODUCTION TO STATISTICAL COMPUTING
Proof
Using the deﬁnition of ZIS
N and equation (3.13) we ﬁnd
E

ZIS
N

= E
⎛
⎝1
N
N

j=1
f (Y j) ϕ(Y j)
ψ(Y j)
⎞
⎠= E

f (Y) ϕ(Y)
ψ(Y)

= E( f (X))
and thus
bias

ZMC
N

= E

ZIS
N

−E( f (X)) = 0.
For the variance we have
Var

ZIS
N

= Var
⎛
⎝1
N
N

j=1
f (Y j) ϕ(Y j)
ψ(Y j)
⎞
⎠= 1
N Var

f (Y) ϕ(Y)
ψ(Y)

.
(3.16)
Rewriting the right-hand side of this equation, we get
Var
 f (Y)ϕ(Y)
ψ(Y)

= E
 f (Y)2ϕ(Y)2
ψ(Y)2

−E
 f (Y)ϕ(Y)
ψ(Y)
2
=

f (y)2ϕ(y)2
ψ(y)2
ψ(y) dy −E( f (X))2
=

f (x)2ϕ(x)
ψ(x)
ϕ(x) dx −E( f (X))2,
where the integration variable is changed from y to x in the last line. Transforming
the integral back to an expectation, we get
Var
 f (Y)ϕ(Y)
ψ(Y)

= E

f 2(X) ϕ(X)
ψ(X)

−E( f (X))2
= E

f (X)2
−E( f (X))2 −E

f (X)2
+ E

f 2(X) ϕ(X)
ψ(X)

= Var( f (X)) −E

f (X)2

1 −ϕ(X)
ψ(X)

,
and thus, substituting this expression into (3.16), we get
MSE

ZIS
N

= Var

ZIS
N

+ bias

ZIS
N
2
= 1
N Var

f (Y) ϕ(Y)
ψ(Y)

+ 02
= 1
N

Var( f (X)) −E

f (X)2

1 −ϕ(X)
ψ(X)

.
This completes the proof.

MONTE CARLO METHODS
87
By comparing the mean squared error (3.15) for the importance sampling estimate
to the mean squared error (3.8) of the basic Monte Carlo estimate, we see that the
importance sampling method has smaller error than the basic Monte Carlo method
whenever the density ψ satisﬁes the condition
cψ = E

f (X)2

1 −ϕ(X)
ψ(X)

> 0.
The importance sampling method is efﬁcient, if both of the following criteria are
satisﬁed:
(a) The samples Y j can be generated efﬁciently.
(b) Var( f (Y)ϕ(Y)/ψ(Y)) is small or, equivalently, the constant cψ is large.
To understand how ψ and thus the distribution of Y can be chosen to minimise
the variance in the second criterion, we ﬁrst consider the extreme case where f ϕ/ψ
is constant: for this choice of ψ, the variance of the Monte Carlo estimate is 0 and
therefore there is no error at all! In this case we have
ψ(x) = 1
a f (x)ϕ(x)
(3.17)
for all x where a is the constant value of f ϕ/ψ. We can ﬁnd the value of a by using
the fact that ψ is a probability density:
a = a

ψ(x) dx =

f (x) ϕ(x) dx = E( f (X)).
Therefore, in order to choose ψ as in (3.17) we have to already have solved the
problem of computing the expectation E( f (X)) and thus we do not get a useful
method for this case. Still, this boundary case offers some guidance: since we get
optimal efﬁciency if ψ is chosen proportional to f ϕ, we can expect, at least for f ≥0,
that we will get good efﬁciency if we choose ψ to be approximately proportional to
the function f ϕ or even if we choose a distribution such that ψ is big wherever | f |
is big.
Example 3.24
Let X be a real-valued random variable and A ⊆R. Then the
importance sampling estimate for the probability P(X ∈A) = E(1A(X)) is given by
ZIS
N = 1
N
N

j=1
1A(Y j)ϕ(Y j)
ψ(Y j)
where ψ is a probability density, satisfying ψ(x) > 0 for all points x ∈A with
ϕ(x) > 0, and (Y j) j∈N is a sequence of i.i.d. random variables with density ψ. Then,

88
AN INTRODUCTION TO STATISTICAL COMPUTING
by proposition 3.23, the mean squared error of the importance sampling estimate for
P(X ∈A) is given by
MSE

ZIS
N

= 1
N Var (1A(X)) −1
N E

1A(X)

1 −ϕ(X)
ψ(X)

= MSE

ZMC
N

−1
N E

1A(X)

1 −ϕ(X)
ψ(X)

.
We see that the importance sampling method will have a smaller mean squared error
than basic Monte Carlo estimation, if we can choose the density ψ such that ψ > ϕ
on the set A.
For the choice of the sample size N in algorithm 3.22, the same considerations
apply as for basic Monte Carlo estimation: an estimate for the mean squared error
MSE

ZIS
N

can be computed together with ZIS
N itself, by using the following relation
from proposition 3.23. The estimate ZIS
N has mean squared error
MSE

ZIS
N

= 1
N Var
 f (Y)ϕ(Y)
ψ(Y)

≈ˆσ 2
N ,
(3.18)
where
ˆσ 2 =
1
N −1
N

j=1
 f (Y j)ϕ(Y j)
ψ(Y j)
−ZIS
N
2
(3.19)
is the sample variance of the weighted samples generated to compute ZIS
N . As for the
basic Monte Carlo method, the relation (3.18) can also be solved for N to determine
the value of N required to achieve a given level of error.
3.3.2
Antithetic variables
The antithetic variables method (also called antithetic variates method) reduces the
variance and thus the error of Monte Carlo estimates by using pairwise dependent
samples X j instead of the independent samples used in basic Monte Carlo estimation.
For illustration, we ﬁrst consider the case N = 2: assume that X and X′ are identi-
cally distributed random variables, which are not independent. As for the independent
case we have
E
 f (X) + f (X′)
2

= E( f (X)) + E

f (X′)

2
= E( f (X)),
but for the variance we get
Var
 f (X) + f (X′)
2

= Var( f (X)) + 2 Cov

f (X), f (X′)

+ Var

f (X′)

4
= 1
2Var( f (X)) + 1
2Cov

f (X), f (X′)

.

MONTE CARLO METHODS
89
Compared with the expression for the variance in the independent case, an additional
covariance term 1
2Cov( f (X), f (X′)) is present. The idea of the antithetic variables
method is to construct X and X′ such that Cov( f (X), f (X′)) is negative, thereby
reducing the total variance.
There are many methods available to construct pairs of samples X, X′ which
have the required properties for use in the antithetic variables method (one possible
construction is given below). Once we have found a way to construct such pairs of a
given distribution, we can use the following approximation.
Deﬁnition 3.25
Let X be a random variable and let f be a function such that
f (X) ∈R. Furthermore, let X′ be another random variable, with the same distribution
as X. Then the antithetic variables estimate for E( f (X)) with sample size N ∈2N
is given by
ZAV
N = 1
N
N/2

k=1

f (Xk) + f (X′
k)

(3.20)
where (Xk, X′
k) are i.i.d. copies of (X, X′).
In the deﬁnition, 2N denotes the set of even integers. Since each term in the sum
contributes two terms, f (Xk) and f (X′
k), to the estimate ZAV
N , the sum in (3.20) ranges
only from 1 to N/2. When applying the algorithm, X and X′ will be dependent. For
ﬁxed k, the pair (Xk, X′
k) has the same dependence structure as (X, X′), but Xk and
X′
k are independent of X j and X′
j when j ̸= k. The pairs (X, X′) and (Xk, X′
k) are
called antithetic pairs. We can write the resulting method as the following algorithm.
Algorithm 3.26
(antithetic variables)
input:
a function f
N ∈N even
randomness used:
i.i.d. copies (Xk, X′
k) of (X, X′)
output:
the estimate ZAV
N for E( f (X))
1: s ←0
2: for k = 1, 2, . . . , N/2 do
3:
generate (Xk, X′
k)
4:
s ←s + f (Xk) + f (X′
k)
5: end for
6: return s/N
As for the Monte Carlo methods discussed so far, the running time of algorithm
3.26 is proportional to N. The loop in line 2 of the algorithm has only N/2 iterations,
compared with N iterations for the basic Monte Carlo method, but in each iteration
two terms are added to the cumulative sum s and thus the ﬁnal result is still the sum

90
AN INTRODUCTION TO STATISTICAL COMPUTING
of N terms. For this reason, the computational cost of the antithetic variables method
is usually very similar to the computational cost of the corresponding basic Monte
Carlo algorithm.
Proposition 3.27
Let X and X′ be two random variables with the same distribu-
tion and let ρ = Corr( f (X), f (X′)). Then the antithetic variables estimate ZAV
N for
E( f (X)) satisﬁes
bias

ZAV
N

= 0
and
MSE(ZAV
N ) = 1
N Var( f (X))(1 + ρ).
(3.21)
Proof
The expectation of ZAV
N is given by
E

ZAV
N

= 1
N
N/2

k=1
E

f (Xk) + f (X′
k)

= 1
N · N
2 · 2E( f (X)) = E( f (X)).
Thus the estimate ZAV
N is unbiased and the mean squared error of ZAV
N coincides with
the variance.
For the variance we can use the fact that the pairs (X j, X′
j) and (Xk, X′
k) are
independent for j ̸= k. This gives
Var

ZAV
N

= 1
N 2
N/2

j,k=1
Cov

f (X j) + f (X′
j)

,

f (Xk) + f (X′
k)

= 1
N 2
N/2

k=1

Var ( f (Xk)) + Var

f (X′
k)

+ 2 Cov

f (Xk), f (X′
k)

= 1
N 2 · N
2 ·

Var( f (X)) + Var

f (X′)

+ 2 Cov

f (X), f (X′)

= 1
N

Var( f (X)) + Cov

f (X), f (X′)

.
Finally, since
ρ = Corr

f (X), f (X′)

=
Cov

f (X), f (X′)

√Var( f (X))Var ( f (X′)) = Cov

f (X), f (X′)

Var( f (X))
,
we ﬁnd
MSE

eAV
N

= Var

eAV
N

= 1
N (Var( f (X)) + Var( f (X))ρ) .

MONTE CARLO METHODS
91
This completes the proof.
Comparing equation (3.8) and equation (3.21) for the mean squared error of the
basic Monte Carlo estimate and of the antithetic variables estimate, respectively, we
see that the antithetic variables method has smaller mean squared error than the basic
Monte Carlo method if and only if ρ < 0.
In order to apply the antithetic variables method, we need to construct the pairs
(X, X′) of samples such that both values have the correct distribution but, at the
same time, f (X) and f (X′) are negatively correlated. There is no generic method to
construct such antithetic pairs; here will restrict ourselves to discussing ideas which
can help to construct antithetic pairs in speciﬁc cases.
A ﬁrst idea for constructing antithetic pairs, if the distribution of X is symmetric,
is to use X′ = −X. The correlation Corr( f (X), f (X′)) depends on the function f ,
but in many cases we have Corr( f (X), f (−X)) < 0. This idea is illustrated in the
following example and, in a much more complex situation, in Section 6.5.2.
Example 3.28
Let X ∼N(0, 1) and consider the problem of estimating the prob-
ability p = P(X ∈[1, 3]) = E(1[1,3](X)). Since the distribution of X is symmetric,
we can try to use (X, X′) with X′ = −X as an antithetic pair. For this choice we ﬁnd
Cov

1[1,3](X), 1[1,3](−X)

= E

1[1,3](X)1[1,3](−X)

−E

1[1,3](X)

· E

1[1,3](−X)

= 0 −p · p
= −p2,
since the two values 1[1,3](X) and 1[1,3](−X) cannot be non-zero simultaneously. As
in example 3.18 we ﬁnd
Var

1[1,3](X)

= Var

1[1,3](−X)

= p(1 −p)
and thus
ρ = Corr

1[1,3](X), 1[1,3](X′)

=
Cov

1[1,3](X), 1[1,3](X′)


Var

1[1,3](X)

Var

1[1,3](X′)

= −
p
1 −p .

92
AN INTRODUCTION TO STATISTICAL COMPUTING
Thus, by the result of proposition 3.27, the mean squared error of the antithetic
variables estimate ZAV
N for this example, compared with the error for the basic Monte
Carlo method, is
MSE

ZAV
N

= (1 + ρ)MSE

ZMC
N

=

1 −
p
1 −p

MSE

ZMC
N

.
By computing an estimate for p we ﬁnd p ≈0.16 and ρ = −p/(1 −p) ≈−0.19.
Thus, in this example the mean squared error of ZAV
N is only about 81% of the error
for ZMC
N .
Another method for generating antithetic pairs can be applied if X is one-
dimensional and if we can generate samples of X using the inverse transform method
from Section 1.3: let F be the distribution function of X and let U ∼U[0, 1]. Then
1 −U ∼U[0, 1] and we can use X = F−1(U) and X′ = F−1(1 −U) to generate an
antithetic pair. Since the inverse F−1 of the distribution function is monotonically
increasing, X increases as U increases while X′ decreases as U increases. For this
reason we expect X and X′ to be negatively correlated.
Lemma 3.29
Let g: R →R be monotonically increasing and U ∼U[0, 1]. Then
Cov (g(U), g(1 −U)) ≤0.
Proof
The proof uses a trick, which is based on the idea of introducing a new
random variable V ∼U[0, 1], independent of U. We distinguish two cases: if U ≤V
we have g(U) ≤g(V ) and g(1 −U) ≥g(1 −V ). Otherwise, if U > V , we have
g(U) ≥g(V ) and g(1 −U) ≤g(1 −V ). Thus, in both cases we have
(g(U) −g(V )) (g(1 −U) −g(1 −V )) ≤0
and consequently
Cov (g(U), g(1 −U))
= E (g(U)g(1 −U)) −E (g(U)) E (g(1 −U))
= 1
2E (g(U)g(1 −U) + g(V )g(1 −V ) −g(U)g(1 −V ) −g(V )g(1 −U))
= 1
2E ((g(U) −g(V )) (g(1 −U) −g(1 −V ))) ≤0.
This completes the proof.
If the function f is monotonically increasing, we can apply the lemma to g(u) =
f (F−1(u)). In this case we have
f (X) = f (F−1(U)) = g(U)

MONTE CARLO METHODS
93
and
f (X′) = f

F−1(1 −U)

= g(1 −U).
The lemma allows then to conclude Cov( f (X1), f (X2)) ≤0. While lemma 3.29 only
guarantees that the variance for the antithetic variables method is smaller or equal
to the variance of standard Monte Carlo estimation, in practice often a signiﬁcant
reduction of variance is observed.
3.3.3
Control variates
The control variates method is another method to reduce the variance of Monte Carlo
estimates for expectations of the form E( f (X)). The method is based on the following
idea: if we can ﬁnd a ‘simpler’ function g ≈f such that E(g(X)) can be computed
analytically, then we can use our knowledge of E(g(X)) to assist with the estimation
of E( f (X)). In the control variates methods, this is done by rewriting the expectation
of interest as
E( f (X)) = E ( f (X) −g(X)) + E(g(X)).
Since we know E(g(X)), the Monte Carlo estimation can now be restricted to the
term E( f (X) −g(X)) and since f (X) ≈g(X), the random quantity f (X) −g(X)
has smaller variance and thus smaller Monte Carlo error than f (X) has on its own.
In this context, the random variable g(X) is called a control variate for f (X).
Deﬁnition 3.30
Let g be a function such that E(g(X)) is known. Then the control
variates estimate for E( f (X)) is given by
ZCV
N
= 1
N
N

j=1

f (X j) −g(X j)

+ E(g(X))
where the X j are i.i.d. copies of X.
The algorithm for computing the estimate ZCV
N
is a trivial modiﬁcation of the
basic Monte Carlo algorithm.
Algorithm 3.31
(control variates)
input:
a function f
a function g ≈f such that E(g(X)) is known
N ∈N
randomness used:
a sequence (X j) j∈N of i.i.d. copies of X

94
AN INTRODUCTION TO STATISTICAL COMPUTING
output:
the estimate ZCV
N for E( f (X))
1: s ←0
2: for j = 1, 2, . . . , N do
3:
generate X j with the same distribution as X has
4:
s ←s + f (X j) −g(X j)
5: end for
6: return s/N + E(g(X))
As for the other Monte Carlo algorithms, the sample size N controls the trade-
off between error and computational cost. The computational cost of this algorithm
increases with N, but at the same time the error decreases when N gets larger.
Proposition 3.32
The control variates estimate ZCV
N for E( f (X)) satisﬁes
bias

ZCV
N

= 0
and
MSE

ZCV
N

= 1
N Var ( f (X) −g(X)) .
Proof
This result is a direct consequence of proposition 3.14. We have
E

ZCV
N

= E
⎛
⎝1
N
N

j=1

f (X j) −g(X j)

+ E(g(X))
⎞
⎠
= E ( f (X) −g(X)) + E(g(X))
= E( f (X))
and thus
bias

ZCV
N

= E

ZCV
N

−E( f (X)) = 0.
Using proposition 3.14 again, we ﬁnd the mean squared error as
MSE

ZCV
N

= Var
⎛
⎝1
N
N

j=1

f (X j) −g(X j)

+ E(g(X))
⎞
⎠
= Var
⎛
⎝1
N
N

j=1

f (X j) −g(X j)

⎞
⎠
= 1
N Var ( f (X) −g(X)) .
This completes the proof.

MONTE CARLO METHODS
95
The control variates method described above is a special case of a more general
method. Using a correlated control variate Y, every random variable Z can be trans-
formed into a new random variable ˜Z with the same mean but smaller variance. This
technique is described in the following lemma.
Lemma 3.33
Let Z be a random variable with E(Z) = μ and Var(Z) = σ 2. Fur-
thermore, let Y be a random variable with Corr(Y, Z) = ρ and deﬁne
˜Z = Z −Cov(Y, Z)
Var(Y)
(Y −E(Y)) .
Then the random variable ˜Z satisﬁes E( ˜Z) = μ and
Var( ˜Z) =

1 −ρ2
σ 2 ≤σ 2.
Proof
For c ∈R deﬁne
Zc = Z −c (Y −E(Y)) .
Then the random variable Zc has expectation
E(Zc) = E(Z) −c (E(Y) −E(Y)) = E(Z)
and the variance of Zc is given by
Var(Zc) = Var(Z) −2cCov(Y, Z) + c2Var(Y).
(3.22)
The value ˜Z in the statement satisﬁes ˜Z = Zc for c = Cov(Y, Z)/Var(Y) and thus
we get
Var( ˜Z) = Var(Z) −2Cov(Y, Z)
Var(Y)
Cov(Y, Z) + Cov(Y, Z)2
Var(Y)2
Var(Y)
= Var(Z) −Cov(Y, Z)2
Var(Y)
=

1 −Corr(Y, Z)2
Var(Z).
This completes the proof.
Lemma 3.33 can, for example, be used to construct improved versions of an
unbiased estimator: if Z is an unbiased estimator for a parameter θ, then
bias( ˜Z) = E( ˜Z) −θ = E(Z) −θ = 0

96
AN INTRODUCTION TO STATISTICAL COMPUTING
and
MSE( ˜Z) = Var( ˜Z) < Var(Z) = MSE(Z),
that is ˜Z is then also an unbiased estimator for θ, but has smaller mean squared error
than Z.
It is easy to check that the value of c used in (3.22) is optimal: the c which
minimises the variance satisﬁes the condition
0 = d
dcVar(Zc) = −2 Cov(Y, Z) + 2cVar(Y)
and thus the optimal value of c is given by
c∗= Cov(Y, Z)
Var(Y)
.
In practice, the exact covariance between Z and Y is often unknown. From
equation (3.22) we know that it sufﬁces to have
2c Cov(Y, Z) > c2 Var(Y)
for variance reduction to occur. It is easy to check that this condition is satisﬁed for
all values of c between 0 and 2 Cov(Z, Y)/Var(Y). Therefore we can replace the
estimator ˜Z from lemma 3.33 by
Zc = Z −c (Y −E(Y))
where c is only an approximation to Cov(Y, Z)/Var(Y), and still expect the estimator
Zc to have smaller variance than Z.
3.4
Applications to statistical inference
In this section we will use a series of examples to illustrate how Monte Carlo methods
can be used to study methods from statistical inference. We will consider point
estimates in Section 3.4.1, conﬁdence intervals in Section 3.4.2 and hypothesis tests
in Section 3.4.3.
In statistical inference problems, we have observed data x = (x1, . . . , xn) and
our aim is to decide which statistical model, typically chosen from a family of
models under consideration, the observed data could have been generated by. More
speciﬁcally, we consider a family (Pθ)θ∈ of probability distributions, where θ is the
parameter vector of the models and  is the set of all possible parameter values, and we
assume that the observed data are a sample of a random variable X = (X1, . . . , Xn) ∼
Pθ, for an unknown parameter value θ ∈. In this context, the random variable X is
called the random sample.

MONTE CARLO METHODS
97
3.4.1
Point estimators
A point estimator (or an estimator in short) for the parameter θ is any function of the
random sample X with values in . Typically, we write ˆθ = ˆθ(X) = ˆθ(X1, . . . , Xn)
to denote an estimator for a parameter θ. The value of the estimator for the observed
data x, that is ˆθ(x) is called a point estimate (or an estimate) for θ.
While the deﬁnition of an estimator does not refer to the ‘true’ value θ, useful
estimators will have the property that ˆθ is close to θ. How close the estimate is to
the exact value determines the quality of an estimator. This is measured by quantities
like the bias and the standard error. In simple cases, for example when the data
consist of independent, normally distributed values or in the limit n →∞, it is
possible to determine the bias and standard error of estimators analytically. In more
complicated cases, exact expressions for the bias and the standard error are no longer
available. Here we will illustrate how Monte Carlo estimation can be used in these
cases, to obtain numerical approximations for the bias and the standard error of an
estimator.
3.4.1.1
Monte Carlo estimates of the bias
From deﬁnition 3.9 we know that the bias of an estimator ˆθ = ˆθ(X) for a parameter θ
is given by
biasθ( ˆθ) = E
 ˆθ(X)

−θ
for all θ ∈. For given values of θ, we can use basic Monte Carlo estimation to
approximate the expection E( ˆθ(X)). The resulting Monte Carlo estimate for the bias
is given by

biasθ( ˆθ) = 1
N
N

j=1
ˆθ(X( j)) −θ,
where the samples X( j) = (X( j)
1 , . . . , X( j)
n ) are i.i.d. copies of X, using the given
parameter value θ.
While this procedure is a simple application of Monte Carlo estimation as
described in proposition 3.14, some care is needed to not confuse the two conceptual
levels characterised by the size n of samples, and the number N of samples used
in the Monte Carlo estimate: while ˆθ can be computed from a sample of size n, we
need to generate N samples, that is n · N random values, to compute the estimate ˆμ.
Similarly, while ˆθ is an estimator for θ, the value 
biasθ( ˆθ) is an estimate for the bias
of an estimator.
Of course, the true value of the parameter θ is normally not known, but we can
systematically compute 
bias(θ) for a range of different θ to get, for example, an
approximate upper bound for the bias of an estimator. This approach is illustrated in
the following example.

98
AN INTRODUCTION TO STATISTICAL COMPUTING
Example 3.34
Let ρ ∈[−1, 1] and X, η ∼N(0, 1) and deﬁne
Y = ρX +

1 −ρ2 η.
Then
Cov(X, Y) = Cov(X, ρX +

1 −ρ2 η)
= ρCov(X, X) +

1 −ρ2Cov(X, η)
= ρVar(X)
= ρ,
since X and η are independent. Using independence again, we ﬁnd
Var(Y) = Var

ρX +

1 −ρ2 η

= ρ2Var(X) + (1 −ρ2)Var(η)
= ρ2 + 1 −ρ2
= 1.
Consequently,
Corr(X, Y) =
Cov(X, Y)
√Var(X)Var(Y) =
ρ
√
1 · 1
= ρ.
The correlation between any two random variables X and Y can be estimated by the
sample correlation
ˆρ(X, Y) =
n
i=1(Xi −¯X)(Yi −¯Y)
n
i=1(Xi −¯X)2 n
i=1(Yi −¯Y)2
(3.23)
where ¯X = 1
n
n
i=1 Xi, ¯Y = 1
n
n
i=1 Yi and (Xi, Yi), i = 1, 2, . . . , n is a sequence of
i.i.d. copies of (X, Y). Thus, we can use ˆρ(X, Y) as an estimator for ρ. Our aim in
this example is to determine the bias of this estimator.
For given ρ, the estimator 
biasρ( ˆρ) for the bias of the estimator ˆρ from equation
(3.23) can be computed using the following steps:
1: s ←0
2: for j = 1, 2, . . . , N do
3:
generate X( j)
1 , . . . , X( j)
n
∼N(0, 1)
4:
generate η( j)
1 , . . . , η( j)
n
∼N(0, 1)
5:
let Y ( j)
i
= ρX( j)
i
+

1 −ρ2 η( j)
i
for i = 1, 2, . . . , n
6:
compute ˆρ( j) = ˆρ(X( j), Y ( j)) using (3.23)

MONTE CARLO METHODS
99
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
−1.0
−0.5
0.0
0.5
1.0
−0.02
0.00
0.01
0.02
ρ
Estimated bias
Figure 3.2
The estimated bias of the sample correlation (3.23) when estimating the
correlation ρ. The circles denote Monte Carlo estimates for the bias, for different
values of ρ, obtained using the method described in example 3.34. The solid line is
obtained by ﬁtting a smooth curve to the Monte Carlo estimates.
7:
s ←s + ˆρ( j)
8: end for
9: return s/N −ρ
Finally, this code can be run repeatedly for different values of ρ ∈[−1, +1] to get
the dependence of the bias on the parameter ρ. The result of one run of this procedure
is shown in Figure 3.2.
3.4.1.2
Monte Carlo estimates of the standard error
From deﬁnition 3.10 we know that the standard error of an estimator ˆθ = ˆθ(X) is
given by
seθ( ˆθ) = stdevθ
 ˆθ(X)

=

Varθ
 ˆθ(X)

for all θ ∈. While this expression does not exactly have the form E( f (x)) consid-
ered for Monte Carlo estimates in this chapter, we can still us a Monte Carlo approach
to estimating the standard error: for given θ, we can estimate seθ( ˆθ) as
seθ( ˆθ) =




1
N −1
N

j=1

ˆθ(X( j)) −ˆθ(·)
2
,

100
AN INTRODUCTION TO STATISTICAL COMPUTING
where
ˆθ(·) = 1
N
N

j=1
ˆθ(X( j))
and X( j) for j = 1, 2, . . . , N are i.i.d. copies of X and in the formula for seθ( ˆθ) we
use the standard estimate for the variance.
3.4.2
Conﬁdence intervals
If the set  of all possible parameter values is one-dimensional, that is if  ⊆R,
we can draw inference about the unknown parameter θ using conﬁdence intervals.
Conﬁdence intervals serve a similar purpose as point estimators but, instead of return-
ing just one ‘plausible’ value of the parameter, they determine a range of possible
parameter values, chosen large enough so that the true parameter value lies inside the
range with high probability.
Deﬁnition 3.35
A conﬁdence interval with conﬁdence coefﬁcient 1 −α for a
parameter θ is a random interval [U, V ] ⊂R where U = U(X) and V = V (X) are
functions of the random sample X = (X1, . . . , Xn), such that
Pθ (θ ∈[U(X), V (X)]) ≥1 −α
(3.24)
for all θ ∈. The subscript θ on the probability P indicates that the random sam-
ple X = (X1, . . . , Xn), for the purpose of computing the probability in (3.24), is
distributed according to the distribution with parameter θ.
It is important to note that in equation (3.24) the interval [U, V ] is random,
since it depends on the random sample X, but the value θ is not. The usefulness of
conﬁdence intervals lies in the fact that equation (3.24) holds for all possible values
of θ simultaneously. Thus, even without knowing the true value of θ, we can be
certain that the relation (3.24) holds and for given data x we can use [U(x), V (x)] as
an interval estimate for θ.
In many cases, a conﬁdence interval for a parameter θ can be constructed by
considering a point estimator ˆθ(X) for θ and then choosing the boundaries of the
conﬁdence interval as U(X) = ˆθ(X) −ε and V (X) = ˆθ(X) + ε for an appropriate
value ε > 0. In this case, the condition (3.24) can be written as
Pθ
 ˆθ −θ ∈[−ε, ε]

≥1 −α
(3.25)
and, if the distribution of ˆθ −θ is known, this relation can be used to choose the
value ε. This approach is illustrated in the following example.

MONTE CARLO METHODS
101
Example 3.36
Let X1, . . . , X N ∼N(μ, σ 2) be i.i.d. with a known variance σ 2 and
assume that we want to construct the conﬁdence interval for the unknown mean μ.
As the centre of the conﬁdence interval we choose the estimator
ˆμ = ˆμ(X) =
n

i=1
Xi/n
from example 3.13. As a sum of independent, normally distributed random variables,
ˆμ is itself normally distributed, and computing the mean and variance as in exam-
ple 3.13, we ﬁnd ˆμ −μ ∼N(0, σ 2/n). Denoting the CDF of the standard normal
distribution by 
, we get
Pμ
 ˆθ −θ ∈[−ε, ε]

= Pμ
	 ˆθ −θ
σ/√n ∈

−
ε
σ/√n ,
ε
σ/√n

= 
ε√n
σ

−

−ε√n
σ

= 2
ε√n
σ

−1.
In order for the condition (3.25) to be satisﬁed, we need to choose ε such that
2
ε√n
σ

−1 ≥1 −α
or, equivalently,
ε ≥σ
√n · 
−1 
1 −α
2

= σqα
√n
holds, where we use qα = 
−1(1 −α/2). Using the smallest valid choice of ε, we
ﬁnd
I(X) =

ˆμ(X) −qασ
√n , ˆμ(X) + qασ
√n

as a conﬁdence interval for μ. Commonly used values for qα are q0.1 ≈1.64, q0.05 ≈
1.96 and q0.01 ≈2.58, corresponding to conﬁdence coefﬁcients of 90%, 95% and
99%, respectively.
Example 3.37
If, in the situation of example 3.36, the variance σ is not known, the
interval
I(X) =

ˆμ(X) −pn,α ˆσ
√n , ˆμ(X) + pn,α ˆσ
√n

,
(3.26)

102
AN INTRODUCTION TO STATISTICAL COMPUTING
where ˆσ is given by
ˆσ 2 =
1
n −1
n

i=1
(Xi −ˆμ(X))2 ,
can be used as a conﬁdence interval for the mean. A standard result from statistics
shows that pn,α can be chosen as the (1 −α/2)-quantile of Student’s t-distribution
with n −1 degrees of freedom, in order for this conﬁdence interval to be exact.
If the Xi are not normally distributed, theoretical analysis becomes difﬁcult and,
in particular for small n, often only approximate conﬁdence intervals can be derived.
Monte Carlo estimates can be used, both to assist with the construction of conﬁdence
intervals and to assess the conﬁdence coefﬁcient of a given conﬁdence interval.
The underlying idea for both of these approaches is, following equation (3.4), to
approximate the expectation in (3.24) as
Pθ (θ ∈[U, V ]) ≈1
N
N

j=1
1[U ( j),V ( j)](θ),
(3.27)
where U ( j) = U(X( j)), V ( j) = V (X( j)), and the vectors X( j) = (X( j)
1 , . . . , X( j)
n ) for
j = 1, 2, . . . , N are i.i.d. copies of X = (X1, . . . , Xn). Since equation (3.24) is
required to hold for all θ ∈, we have to compute the value of the approxima-
tion (3.27) for a representative sample of parameter values θ.
Example 3.38
If the data X do not consist of independent, normally distributed
samples, the conﬁdence interval for the mean given by (3.26) in example 3.37 is no
longer exact. In such a situation, we can use Monte Carlo estimation to estimate the
resulting conﬁdence coefﬁcient.
Here, we consider the case where X1, . . . , Xn are independent and Poisson-
distributed with parameter λ. Since the mean of the Pois(λ) distribution is λ, we need
to estimate the probability Pλ(U ≤λ ≤V ), where U and V are the boundaries of the
conﬁdence interval (3.26). A Monte Carlo estimate for this probability, for given λ,
can be obtained by the following algorithm.
1: k ←0
2: for j = 1, 2, . . . , N do
3:
generate X1, . . . , Xn ∼Pois(λ)
4:
ˆμ ←N
i=1 Xi/n
5:
ˆσ ←
n
i=1(Xi −ˆμ)2/(n −1)
6:
U ←ˆμ −pn,α ˆσ/√n
7:
V ←ˆμ + pn,α ˆσ/√n
8:
if U ≤λ ≤V then
9:
k ←k + 1

MONTE CARLO METHODS
103
●
●
●
●
●
●
●●●●●●●
●●●●●●●
●●●●●●●●●●●●●●●●●●●●
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
λ
Confidence coefficient
Figure 3.3
Estimated conﬁdence coefﬁcients for the conﬁdence interval (3.26) with
α = 5%, when the data are Pois(λ) distributed, for different values of λ. The con-
ﬁdence coefﬁcients are estimated using Monte Carlo estimates, as described in
example 3.38. The plot shows that for Poisson-distributed data, in particular when λ
is small, the conﬁdence coefﬁcient is much smaller than 95%. This indicates that the
conﬁdence interval (3.26), designed for normally distributed data, is too small for
data following a Poisson distribution.
10:
end if
11: end for
12: return k/N
The above algorithm determines the probability that the conﬁdence interval covers
the mean for ﬁxed λ. Running this algorithm repeatedly, we can systematically
estimate Pλ(U ≤λ ≤V ) for a range of λ. The result of such an estimation, for
α = 5%, is shown in Figure 3.3. The ﬁgure clearly shows that the interval given in
(3.26) is not a 95% conﬁdence interval for Poisson-distributed values. In particular, for
small parameter values, the probability of the interval covering the actual parameter
value is very small. Thus, to derive an acceptable conﬁdence interval for the parameter
of a Poisson distribution, the interval (3.26) needs to be enlarged. A more detailed
Monte Carlo analysis could be used to derive approximate bounds for such improved
conﬁdence intervals.
3.4.3
Hypothesis tests
In hypothesis testing, inference about an unknown parameter is restricted to the
question of whether or not the parameter satisﬁes a given ‘hypothesis’ H0. Such a
hypothesis about the parameter θ could, for example, be a statement like ‘θ = 0’ or
‘θ > 0’. The alternative hypothesis, that is the hypothesis that θ does not satisfy H0,
is denoted by H1.

104
AN INTRODUCTION TO STATISTICAL COMPUTING
While the dichotomy between H0 and H1 is symmetric, it transpires that in most
situations any given statistical test can only determine for one of the hypotheses
whether it is likely to be true, whereas the other hypothesis can only be shown to be
likely to be wrong. Traditionally the names are chosen such that H0 is the hypothesis
which can only be disproved (called the null hypothesis) and H1 is the hypothesis
which can be proved. The two possible outcomes of a statistical test are then ‘H0 has
been rejected’ and ‘H0 has not been rejected’.
Example 3.39
Assume that X1, . . . , Xn ∼N(μ, σ 2) are i.i.d. with unknown
mean μ. Furthermore assume that, given one instance x1, . . . , xn ∈R of the ran-
dom sample X1, . . . , Xn, we want to test the hypothesis that μ = 0. In this case, for
example if all the xi are concentrated far away from 0, it is possible to conclude
that the observations xi are not compatible with the hypothesis μ = 0. In contrast,
even if the observations are clustered around 0, we will never be able to exclude the
possibility that μ has a value which is only very close to but not identical with 0.
Thus, for this example, we should choose H0 to be the hypothesis μ = 0 and we
should choose H1 to be the alternative hypothesis μ ̸= 0.
Since the null-hypothesis H0 is a statement about the parameter θ, we can describe
the hypothesis H0 by a subset 0 of the set  of all possible parameter values where
H0 holds.
Deﬁnition 3.40
A statistical hypothesis test (or statistical test) of size α ∈(0, 1)
for the hypothesis H0 = {θ ∈0} for 0 ⊆ is given by a function T = T (X) of
the random sample X = (X1, . . . , Xn) together with a set C, such that
Pθ

T (X) ∈C

≤α
(3.28)
for all θ ∈0. The test rejects the hypothesis H0 if and only if T (X) ∈C. The
function T is called the test statistic and the set C is called the critical region of
the test.
A statistical test can fail in two different ways: if θ ∈0, the random sample X
could take a value such that T (X) ∈C. In this case, the hypothesis H0 is wrongly
rejected despite being true. This is called a type I error. From equation (3.28) we
know Pθ(T (X) ∈C) ≤α for all θ ∈0 and thus the probability of type I errors is
bounded from above by the size α of the test. Conversely, if θ /∈0, it can happen
that T (X) /∈C. In this case, the test fails to reject the hypothesis H0, despite it being
wrong. The probability of the corresponding type II error is given by Pθ(T (X) /∈C)
for all θ ∈ \ 0. This probability depends on θ and can only be expected to be
small if the parameter value θ is ‘sufﬁciently far away’ from 0.
The rˆole of the parameter α is to allow for rare, untypical behaviour of the random
sample X. If H0 holds and X shows typical behaviour, the null hypothesis H0 will
not be wrongly rejected. But in rare cases, with probability less than α, the random
behaviour of X will be such that H0 is wrongly rejected, that is such that a type I error

MONTE CARLO METHODS
105
occurs. By choosing the value of α, a trade-off between accuracy and sensitivity can
be made: smaller values of α lead to reduced probability of type I errors, but at the
same time the test will become less and less likely to reject H0 and the probability of
type II errors increases.
In many cases the relation (3.28) can only be proven (or only holds) for large
values of n while for small values of n the real signiﬁcance level of the test will not
be known exactly.
Example 3.41
The skewness
γ = E
	 X −μ
σ
3
= E

(X −μ)3
σ 3
of a random variable with mean μ and standard deviation σ can be estimated by
ˆγn =
1
n
n
i=1(Xi −¯X)3
 1
n
n
i=1(Xi −¯X)23/2 ,
where X1, X2, . . . , Xn are i.i.d. copies of X and ¯X is the average of the Xi. If
X ∼N(μ, σ 2), then γ = 0 and one can show that
 n
σ ˆγn −→N(0, 1)
(3.29)
as n →∞.
Assume that we want to construct a test for the null hypothesis H0 that X is
normally distributed with variance σ 2. As a consequence of (3.29), for large n, we
can use the test statistic T = √n/σ | ˆγn| and the critical region
C = (1.96, ∞) ⊆R
to construct a test of size α = 5%: We reject H0 if T ∈C, that is if | ˆγn| ≥1.96√σ/n.
One problem with the test constructed in the preceding example is, that the
convergence of the distribution of √n/σ ˆγn to N(0, 1) is very slow. For small or
moderate n the probability of wrongly rejecting H0 (type I error) may be bigger
than α.
We can use Monte Carlo estimation to estimate the probability of type I errors of
statistical tests. For simple hypotheses (i.e. if H0 speciﬁes the distribution of the test
statistic completely), this can be done as follows:
(a) For j = 1, 2, . . . , N, generate samples (X( j)
1 , . . . , X( j)
n ) according to the dis-
tribution given by the null hypothesis.
(b) Compute T ( j) = T (X( j)
1 , . . . , X( j)
n ) for j = 1, 2, . . . , N.

106
AN INTRODUCTION TO STATISTICAL COMPUTING
(c) Check for which percentage of samples H0 is (wrongly) rejected:
P (T ∈C) ≈1
N
N

j=1
1C(T ( j)).
3.5
Summary and further reading
In this chapter we have learned how Monte Carlo methods can be used to study
statistical models via simulation. We have given special consideration to the relation
between computational cost and accuracy of the results: the error of Monte Carlo
estimates decays proportional to 1/
√
N where N is the number of samples used. We
have also studied several ‘variance reduction methods’ which allow to reduce the
constant of proportionality in this relation, thus allowing for more efﬁcient estimates.
Some information about Monte Carlo methods and variance reduction can be found
in Ripley (1987). A more extensive discussion of Monte Carlo methods and variance
reduction is contained in Chapters 3–5 of Robert and Casella (2004). The historical
origins of the Monte Carlo method are described in Metropolis (1987).
Finally, we illustrated the methods introduced in this chapter by studying problems
in statistical inference using Monte Carlo estimates. While we only touched the basics
of statistical inference, there are many textbooks about statistics available, ranging
from application oriented texts to more theoretical treatments. Details about statistical
inference can, for example, be found in the books Garthwaite et al. (2002) and Casella
and Berger (2001).
Exercises
E3.1
Assume X ∼Exp(1) and Y ∼N(0, X), that is Y is normally distributed with
a random variance. Use Monte Carlo estimation to estimate E(X|Y = 4) and
Var(X|Y = 4).
E3.2
Write a program which, for given N, computes the Monte Carlo estimate from
algorithm 3.8 for the expectation z = E(sin(X)2). For N = 1000, use this
program to repeatedly generate estimates for z and then use these estimates
to assess the accuracy of the Monte Carlo method for the given problem.
Repeat this experiment for N = 10 000 and use the results to illustrate that
the resulting estimates for z are more accurate than the estimates obtained for
N = 1000.
E3.3
Let X ∼N(0, 1). Use Monte Carlo estimation to obtain an estimate for
E(cos(X)) to three digits of accuracy.
E3.4
Let X ∼N(0, 1) and a ≥0. Consider the estimates
pN = 1
N
N

j=1
1(−∞,a](X j)

MONTE CARLO METHODS
107
and
˜pN = 1
2 +
a
√
2π N
N

j=1
e−a2U 2
j /2
for p = P(X ≤a), where X j ∼N(0, 1) and U j ∼U[0, 1] for j ∈N are
i.i.d.
(a)
Using equation (3.5) or otherwise, show that ˜pN →p as N →∞.
(b)
For a = 1, perform a numerical experiment to determine which of the
two estimates has smaller mean squared error.
E3.5
The na¨ıve approach to compute the Monte Carlo sample variance ˆσ 2 from
equation (3.9) is to store all generated samples f (X j) in memory and to
compute the sample variance (3.9) only after all samples are generated. A
disadvantage of this approach is that this requires an amount of memory
which is proportional to N. Describe a way to compute ˆσ 2 which requires
only a constant (i.e. independent of N) amount of memory.
E3.6
Let X ∼N(0, 1) and A = [3, 4] and consider importance sampling estimates
for the probability P(X ∈A), using samples Y j from the following sample
distributions:
r Y j ∼N(1, 1);
r Y j ∼N(2, 1);
r Y j ∼N(3.5, 1);
r Y j ∼Exp(1) + 3.
Each of these four distributions gives rise to a different importance sampling
method. Our aim is to compare the resulting estimates.
(a)
For each of the four methods, determine the sample variance ˆσ 2 for the
weighted samples, as given in (3.19). Which of these four methods gives
the best results?
(b)
Determine a good estimate for P(X ∈A) and discuss the accuracy of
your estimate.
(c)
For each of the four methods, approximately how many samples Y j are
required to reduce the error of the estimate of P(X ∈A) to 1%?
E3.7
In this question we study the Monte Carlo estimate 
biasρ( ˆρ) for the bias of
the estimator ˆρ from (3.23) for the correlation ρ = Corr(X, Y).
(a)
Implement a function which computes, for given n ∈N and ρ ∈[−1, 1],
the estimate 
biasρ( ˆρ) as in example 3.34. Comment on your choice of
the sample size N for the Monte Carlo estimate.

108
AN INTRODUCTION TO STATISTICAL COMPUTING
(b)
For n = 10, create a plot which shows the bias of r(X, Y) as a function
of ρ.
(c)
Use your results to give an (approximately) unbiased estimate of the
correlation for the following data.
i
1
2
3
4
5
. . .
Xi
0.218
0.0826
0.091
0.095
−0.826
. . .
Yi
0.369
0.715
−1.027
−1.499
1.291
. . .
i
. . .
6
7
8
9
10
Xi
. . .
0.208
0.600
−0.058
0.602
0.620
Yi
. . .
−0.213
−2.400
1.064
−0.367
−1.490
E3.8
Write a program to compute the Monte Carlo estimate (3.27) for the conﬁ-
dence coefﬁcient of the conﬁdence interval (3.26). For n = 10 and α = 5%,
using the value p10,0.05 = 2.262157, estimate the conﬁdence coefﬁcient when
X1, . . . , X10 ∼Pois(λ) for a range of λ and create a plot of the results. Is
(3.26) a useful conﬁdence interval for Poisson-distributed data?

4
Markov Chain Monte
Carlo methods
Monte Carlo methods, as discussed in Chapter 3, use a sequence (X j) j∈N of i.i.d.
samples as a tool to explore the behaviour of a statistical model. Large numbers
of samples are required and thus these methods depend on our ability to generate
the samples from a given distribution efﬁciently. Sometimes, when generating these
samples is difﬁcult, it can be easier to replace the i.i.d. sequence (X j) j∈N by a Markov
chain instead. The resulting methods are called Markov Chain Monte Carlo methods.
Deﬁnition 4.1
A Markov Chain Monte Carlo (MCMC) method for estimating the
expectation E ( f (X)) is a numerical method based on the approximation
E ( f (X)) ≈1
N
N

j=1
f (X j),
(4.1)
where (X j) j∈N is a Markov chain with the distribution of X as its stationary distri-
bution.
MCMC methods form a generalisation of the Monte Carlo methods discussed in
the previous chapter. Where Monte Carlo methods use independent samples, MCMC
methods use samples which can have direct dependence on the immediately preceding
value. Since the dependence structure of samples is restricted in this way, the samples
in MCMC methods are in some sense still ‘close to being independent’.
Compared with the situation of Monte Carlo methods, the theory behind MCMC
methods is more challenging but, at the same time, the resulting methods are often
An Introduction to Statistical Computing: A Simulation-based Approach, First Edition. Jochen Voss.
© 2014 John Wiley & Sons, Ltd. Published 2014 by John Wiley & Sons, Ltd.

110
AN INTRODUCTION TO STATISTICAL COMPUTING
easier to apply since no extra knowledge about random number generation is required
for MCMC methods.
4.1
The Metropolis–Hastings method
In order to apply MCMC methods as in deﬁnition 4.1, we need to be able to construct
Markov chains with a prescribed stationary distribution. The Metropolis–Hastings
method, presented in this section, is a popular method to solve this problem. The
resulting algorithm is similar to the rejection sampling algorithm: starting from a
nearly arbitrary Markov chain, the Metropolis–Hastings algorithm generates a new
Markov chain with the required stationary distribution by ‘rejecting’ some of the state
transitions of the original Markov chain.
Here we concentrate solely on the problem of constructing a Markov chain with
a given stationary distribution. Use of the resulting Markov chain in Monte Carlo
estimates will be discussed in Section 4.2. The discussion in this and the following
sections requires basic knowledge about Markov chains; we refer to Section 2.3 for
a short introduction.
4.1.1
Continuous state space
In its general form, the Metropolis–Hastings method can be used on nearly arbitrary
state spaces. In order to avoid technical complications, we do not give the general
form of the algorithm here but, instead, consider the most important special cases
separately. In this section, we will discuss the case where the state space is S = Rd.
The following section considers the case of ﬁnite or countable state space.
Algorithm 4.2
(Metropolis–Hastings method for continuous state space)
input:
a probability density π (the target density)
a transition density p: S × S →[0, ∞)
X0 ∈

x ∈S
 π(x) > 0

randomness used:
independent samples Y j from the transition density p (the proposals)
U j ∼U[0, 1] i.i.d.
output:
a sample of a Markov chain X with stationary density π.
As an abbreviation we deﬁne a function α: S × S →[0, 1] by
α(x, y) = min
π(y)p(y, x)
π(x)p(x, y), 1

for all x, y ∈S with π(x)p(x, y) > 0.
1: for j = 1, 2, 3, . . . do
2:
generate Y j with density p(X j−1, ·)
3:
generate U j ∼U[0, 1]

MARKOV CHAIN MONTE CARLO METHODS
111
4:
if U j ≤α(X j−1, Y j) then
5:
X j ←Y j
6:
else
7:
X j ←X j−1
8:
end if
9:
output X j
10: end for
The acceptance probability α(x, y) is only deﬁned for elements x, y ∈S which
satisfy π(x)p(x, y) > 0. At a ﬁrst glance it seems that the algorithm could fail by
hitting a proposal Y j such that α(X j−1, Y j) is undeﬁned, but it transpires that this
cannot happen: Assume that we have already found X0, X1, . . . , X j−1. Then we have
π(X j−1) > 0 (it would not have been accepted otherwise in a previous step) and,
given X j−1, the proposal Y j satisﬁes p(X j−1, Y j) > 0 with probability 1 (it would
not have been proposed otherwise). Consequently, α(X j−1, Y j) is deﬁned and we can
compute the next value X j.
The purpose of algorithm 4.2 is to generate a Markov chain with stationary
density π, that is ideally we want to achieve X j ∼π for all j ∈N. The following
results shows that, assuming we have X0 ∼π, the algorithm achieves this aim. In
practice, if we could generate samples with density π, we would be able to use basic
Monte Carlo estimation and there would be no need to employ MCMC methods.
Thus, realistically we will not exactly have X0 ∼π but the results of Section 4.2
will show that, under mild additional assumptions, the Markov chains generated by
algorithm 4.2 can be used as the basis for MCMC methods, even if the algorithm is
not started with X0 ∼π.
Proposition 4.3
The process (X j) j∈N constructed in the Metropolis–Hastings algo-
rithm 4.2 is a Markov chain with stationary density π.
Proof
Assume that X j−1 has density π. Then we have
P(X j ∈A) = E

1A(X j)
	
=

S
π(x)

S
p(x, y) (α(x, y)1A(y) + (1 −α(x, y)) 1A(x)) dy dx
=

S
π(x)

S
p(x, y) α(x, y)1A(y) dy dx
+

S
π(x)

S
p(x, y) (1 −α(x, y)) 1A(x) dy dx.
From the deﬁnition of the acceptance probability α we know
π(x)p(x, y)α(x, y) = min (π(x)p(x, y), π(y)p(y, x)) = π(y)p(y, x)α(y, x)

112
AN INTRODUCTION TO STATISTICAL COMPUTING
and thus
P(X j ∈A) =

S

S
π(y)p(y, x)α(y, x) 1A(y) dy dx
+

S
π(x)

S
p(x, y) (1 −α(x, y)) 1A(x) dy dx.
Finally, interchanging the variables x and y in the ﬁrst double integral, we get
P(X j ∈A) =

S

S
π(x)p(x, y)α(x, y)1A(x) dx dy
+

S
π(x)

S
p(x, y) (1 −α(x, y)) 1A(x) dy dx
=

S
π(x)

S
p(x, y) (α(x, y)1A(x) + (1 −α(x, y)) 1A(x)) dy dx
=

S
π(x)

S
p(x, y)1A(x) dy dx
=

S
π(x)1A(x) dx
= P(X j−1 ∈A)
for all sets A. This shows that the distributions of X j−1 and X j are the same and thus
that π is a stationary density of the Markov chain.
The result of proposition 4.3 can be slightly improved: it transpires that the
Markov chain constructed by algorithm 4.2, when started with initial distribution π,
is balanced in the sense that
P(X j ∈A, X j−1 ∈B) = P(X j ∈B, X j−1 ∈A)
(4.2)
for all sets A, B ⊆Rd. The condition (4.2), for B = Rd, implies
P(X j ∈A) = P(X j ∈A, X j−1 ∈Rd)
= P(X j ∈Rd, X j−1 ∈A)
= P(X j−1 ∈A)
for all j ∈N and by induction we ﬁnd P(X j ∈A) = P(X0 ∈A) = π(A). Thus,
condition (4.2) for X0 ∼π implies the statement of proposition 4.3. A process which
satisﬁes condition (4.2) for X j ∼π is called π-reversible. Since we do not require
reversibility in the following, we restrict proofs of reversibility to the simpler, discrete
case and only prove stationarity as in proposition 4.3 for the continuous case.

MARKOV CHAIN MONTE CARLO METHODS
113
One big advantage of the Metropolis–Hastings algorithm is that, as for the rejec-
tion sampling algorithm, the target density π only needs to be known up to a constant:
the only place where π occurs in the algorithm is in the acceptance probability α;
if we only know the product c · π but not the value of the constant c, we can still
evaluate the function α, since
α(x, y) = min
π(y)p(y, x)
π(x)p(x, y), 1

= min
c π(y)p(y, x)
c π(x)p(x, y), 1

.
This property of the Metropolis–Hastings algorithm can, for example, be used in
situations such as the one from Section 4.3 where we need to sample from the density
p(θ | x) =
p(x | θ)p(θ)

p

x
 ˜θ
	
p( ˜θ) d ˜θ
with an unknown normalisation constant

p

x
 ˜θ
	
p( ˜θ) d ˜θ.
4.1.2
Discrete state space
In this section, we state the Metropolis–Hastings algorithm for discrete state spaces.
The algorithm for this case is obtained from algorithm 4.2 by replacing densities
with probability weights. Since the situation of discrete state is less technically
challenging, here we prove a slightly better result than we did for the continuous case
in proposition 4.3.
Algorithm 4.4
(Metropolis–Hastings method for discrete state space)
input:
a probability vector π ∈RS (the target distribution)
a transition matrix P = (pxy)x,y∈S
X0 ∈S with πX0 > 0
randomness used:
independent samples Y j from the transition matrix P (the proposals)
U j ∼U[0, 1] i.i.d.
output:
a sample of a Markov chain X with stationary distribution π.
As an abbreviation we deﬁne a function α: S × S →[0, 1] by
α(x, y) = min
πy pyx
πx pxy
, 1

(4.3)
for all x, y ∈S with πx pxy > 0.
1: for j = 1, 2, 3, . . . do
2:
generate Y j with P(Y j = y) = pX j−1,y for all y ∈S
3:
generate U j ∼U[0, 1]
4:
if U j ≤α(X j−1, Y j) then

114
AN INTRODUCTION TO STATISTICAL COMPUTING
5:
X j ←Y j
6:
else
7:
X j ←X j−1
8:
end if
9:
output X j
10: end for
While we could follow the structure of Section 4.1.1 and prove a result in analogy
to proposition 4.3, we will prove a slightly better result in this section, by showing
that the Markov chain constructed by algorithm 4.4 is reversible in the sense of the
following deﬁnition.
Deﬁnition 4.5
A time-homogeneous Markov chain X with state space S and
transition matrix P = (pxy)x,y∈S satisﬁes the detailed balance condition, if there is a
probability vector π ∈RS with
πx pxy = πy pyx
for all x, y ∈S. In this case we say that the Markov chain X is π-reversible.
The following lemma shows that being π-reversible is indeed a stronger condition
for a Markov chain than having stationary distribution π.
Lemma 4.6
Let X be a π-reversible Markov chain. Then π is a stationary distribu-
tion of X.
Proof
If X satisﬁes the detailed balance condition for a probability vector π, we
have

x∈S
πx pxy =

x∈S
πy pyx = πy

x∈S
pyx = πy
for all y ∈S. Thus, π satisﬁes the condition from equation (2.4) and consequently is
a stationary distribution.
Proposition 4.7
The process (X j) j∈N constructed in the Metropolis–Hastings algo-
rithm 4.4 is a π-reversible Markov chain. In particular, X has stationary distribution π.
Proof
Since X j in the algorithm depends only on X j−1 and on the additional,
independent randomness from Y j and U j, the process (X j) j∈N is a Markov chain.
Let Q = (qxy)x,y∈S be the transition matrix of this Markov chain, that is
qxy = P(X j = y | X j−1 = x)

MARKOV CHAIN MONTE CARLO METHODS
115
for all x, y ∈S. Then we have to show that Q satisﬁes the detailed balance condition
πxqxy = πyqyx
(4.4)
for all x, y ∈S.
In the case x = y, the relation (4.4) is trivially true; thus we can assume x ̸= y.
For the process X to jump from x to y, the proposal Yn must equal y and then the
proposal must be accepted. Since these two events are independent, we ﬁnd
qxy = pxy · α(x, y)
and thus
πxqxy = πx pxy min
πy pyx
πx pxy
, 1

= min

πy pyx, πx pxy
	
= πy pyx min

1, πx pxy
πy pyx

= πyqyx
for all x, y ∈S with πx pxy > 0 and πy pyx > 0. There are various cases with πx pxy =
0 or πy pyx = 0; using the that fact the pxy = 0 implies qxy = 0 (since transitions
from x to y are never proposed) it is easy to check that in all of these cases πxqxy =
0 = πyqyx holds. Thus, equation (4.4) is satisﬁed for all x, y ∈S, the process X is
π-reversible and, by lemma 4.6, π is a stationary distribution of X.
Example 4.8
Let πx = 2−|x|/3 for all x ∈Z. Then

x∈Z
πx = 1
3

· · · + 1
4 + 1
2 + 1 + 1
2 + 1
4 + · · ·

= 1
3

1 + 2
∞

x=1
2−x

= 1,
that is the inﬁnite vector π is a probability vector on S = Z. Using algorithm 4.4,
we can easily ﬁnd a Markov chain which has π as a stationary distribution: for the
algorithm, we can choose the transition matrix P for the proposals. For this example
we consider
P(Y j = x + 1 | X j−1 = x) = P(Y j = x −1 | X j−1 = x) = 1
2

116
AN INTRODUCTION TO STATISTICAL COMPUTING
for all x ∈S and all n ∈N. This corresponds to pxy = 1/2 if y = x + 1 or y =
x −1 and pxy = 0 otherwise. From this we can compute the acceptance probabilities
α(x, y):
α(x, y) = min
πy pyx
πx pxy
, 1

= min
 2−|y|
3
pyx
2−|x|
3
pxy
, 1

= min

2|x|−|y| pyx
pxy
, 1

.
In the Metropolis–Hastings algorithm, the function α(x, y) is only evaluated for
x = y + 1 and x = y −1. In either of these cases we have pxy = 1/2 and thus
α(x, y) =

2|x|−|y|
if |y| > |x| and
1
otherwise.
Finally, substituting this transition matrix P and the corresponding function α into
algorithm 4.4 we get the following:
1: for j = 1, 2, 3, . . . do
2:
Let Y j ←X j−1 + ε j where ε j ∼U{−1, 1}.
3:
generate U j ∼U[0, 1]
4:
if U j ≤2|X j−1|−|Y j| then
5:
X j ←Y j
6:
else
7:
X j ←X j−1
8:
end if
9: end for
By proposition 4.7, the resulting process X is a Markov chain with stationary
distribution π.
4.1.3
Random walk Metropolis sampling
The random walk Metropolis algorithm discussed in this section is an important
special case of the Metropolis–Hastings algorithm. It can be considered both for the
continuous and for the discrete case; here we restrict ourselves to the continuous case
and refer to example 4.8 for an illustration of the corresponding discrete case.
The Metropolis–Hastings method for the case p(x, y) = p(y, x) is called the
Metropolis method. In this case, the expression for the acceptance probability α
simpliﬁes to
α(x, y) = min
π(y)p(y, x)
π(x)p(x, y), 1

= min
π(y)
π(x), 1


MARKOV CHAIN MONTE CARLO METHODS
117
for all x, y ∈S with π(x) > 0 (or πy/πx for discrete state space). The condition
p(x, y) = p(y, x) is, for example, satisﬁed when the proposals Y j are constructed as
Y j = X j−1 + ε j
where the ε j are i.i.d. with a symmetric distribution (i.e. ε j has the same distribution
as −ε j). We only state the version of the resulting algorithm for continuous state
space S, the discrete version is found by using a probability vector instead of a
density for the target distribution.
Algorithm 4.9
(random walk Metropolis)
input:
a probability density π: S →[0, ∞) (the target density)
X0 ∈S with π(X0) > 0
randomness used:
an i.i.d. sequence (ε j) j∈N with a symmetric distribution (the increments)
U j ∼U[0, 1] i.i.d.
output:
a sample of a Markov chain X with stationary density π.
As an abbreviation we deﬁne a function α: S × S →[0, 1] by
α(x, y) = min
π(y)
π(x), 1

(4.5)
for all x, y ∈S with π(x) > 0.
1: for j = 1, 2, 3, . . . do
2:
generate ε j
3:
let Y j ←X j−1 + ε j
4:
generate U j ∼U[0, 1]
5:
if U j ≤α(X j−1, Y j) then
6:
X j ←Y j
7:
else
8:
X j ←X j−1
9:
end if
10: end for
Since algorithm 4.9 is a special case of the general Metropolis–Hastings algo-
rithm 4.2, we can use proposition 4.3 to see that the output of algorithm 4.9 is a
Markov chain with stationary distribution π, independently of the choice of incre-
ments ε j. The results of Section 4.2 can be used to prove convergence of MCMC
methods based on algorithm 4.9. For state space S = Rd, the most common choice
of increments is ε j ∼N(0, σ 2); convergence of the MCMC methods resulting from
this choice of increments is proved in example 4.24.
The proposal variance σ 2 can be chosen to maximise efﬁciency of the method:
if σ 2 is small, we have Y j ≈X j−1, that is π(Y j) ≈π(X j−1) and consequently

118
AN INTRODUCTION TO STATISTICAL COMPUTING
α(X j−1, Y j) ≈1. In this case, almost all proposals are accepted, but the algorithm
moves slowly since the increments in each step are small. On the other hand, if σ 2
is large, the proposals Y j are widely dispersed and often π(Y j) will be small. In this
case, many proposals are rejected; the process X does not move very often but when
it does, the increment is typically large. The optimal choice of σ 2 will be between
these two extremes. This is illustrated in Figure 4.1.
−10
−5
0
5
10
Xj
Xj
Xj
−10
−5
0
5
10
0
1000
2000
3000
4000
5000
−10
−5
0
5
10
j
(a)
(b)
(c)
Figure 4.1
Paths of the random walk Metropolis process from example 4.10, with
N(0, σ 2)-distributed increments, for different values of the proposal variance σ: (a)
σ = 1; (b) σ = 6; (c) σ = 36.

MARKOV CHAIN MONTE CARLO METHODS
119
Example 4.10
We can use the random walk Metropolis algorithm to generate
samples from the density
π(x) = 1
Z ·
 sin(x)2
x2
if x ∈[−3π, 3π] and
0
otherwise,
(4.6)
where Z is the normalisation constant which makes π a probability density. For this
target distribution, the acceptance probabilities α(x, y) from equation (4.5) are found
as
α(x, y) = min
π(y)
π(x), 1

= min
⎛
⎝
1
Z · sin(y)2
y2
1[−3π,3π](y)
1
Z · sin(x)2
x2
1[−3π,3π](x)
, 1
⎞
⎠
= min
x sin(y)
y sin(x)
2
1[−3π,3π](y), 1

for all x, y ∈R with π(x) > 0.
In this section we have only argued that the given target density π is a stationary
density for the Markov chain generated by the random walk Metropolis algorithm.
This result is a consequence of proposition 4.3 and does not make any assumptions
on π or on the increments ε j. Later, when using the generated Markov chain as
part of a Monte Carlo method in Section 4.2, we will require additional assumptions
to guarantee that the average in the estimate (4.1) converges to the correct value,
even if the Markov chain is not started in stationarity. This will require that the
increments ε j are large enough for the process to move freely within the region of
value typically taken by the target distribution. For this reason, the random walk
Metropolis–Hastings method will often be inefﬁcient if π has several isolated max-
ima, and the method works best if the target density has just one connected region of
high probability.
4.1.4
The independence sampler
Another special case of the Metropolis–Hastings algorithm is obtained by choosing
the proposals Y j independently of X j−1, that is by using a transition density of the
form p(x, y) = p(y).
Algorithm 4.11
(independence sampler)
input:
a probability density π ∈RS (the target density)
X0 ∈S

120
AN INTRODUCTION TO STATISTICAL COMPUTING
randomness used:
an i.i.d. sequence (Y j) j∈N with density p (the proposals)
U j ∼U[0, 1] i.i.d.
output:
a sample of a Markov chain X with stationary density π.
As an abbreviation we deﬁne a function α: S × S →[0, 1] by
α(x, y) = min
π(y)p(x)
π(x)p(y), 1

for all x, y ∈S with π(x) > 0.
1: for j = 1, 2, 3, . . . do
2:
generate Y j ∼p
3:
generate U j ∼U[0, 1]
4:
if U j ≤α(X j−1, Y j) then
5:
X j ←Y j
6:
else
7:
X j ←X j−1
8:
end if
9: end for
While the proposals Y j in this algorithm are independent, the X j are still depen-
dent, since the acceptance probability α(X j−1, Y j) for X j depends on the value of
X j−1. The algorithm is a special case of the general Metropolis–Hastings method
from algorithm 4.2 and by proposition 4.3 the generated Markov chain has stationary
density π.
The resulting method resembles the rejection sampling method from algorithm
1.22. When we use rejection sampling to generate (independent) samples with
density π from proposals with density p, we have the condition π(x)/p(x) ≤c
for all x and we accept proposals, if the condition
cp(Y j)U j ≤π(Y j)
is satisﬁed. In contrast, the independence sampler generates dependent samples, but
does not require boundedness of π(x)/p(x). For the independence sampler, proposals
are accepted whenever
π(X j)
p(X j) p(Y j)U j ≤π(Y j)
holds.
4.1.5
Metropolis–Hastings with different move types
As a simple generalisation of the Metropolis–Hastings algorithm, we can split the
transition mechanism into different move types. This generalisation allows more
ﬂexibility in the design of MCMC algorithms. There are two different situations

MARKOV CHAIN MONTE CARLO METHODS
121
where this approach can be employed. First, sometimes an existing MCMC method
can be improved by adding additional move types which allow the algorithm to
explore the state space faster. An example of this approach can be found in example
4.14. Secondly, some MCMC methods are constructed from the ground up as a
combination of different move types, each of which serves a different purpose. We
will see examples of this approach in Sections 4.4 and 4.5.
A ﬁnite or countable set M is used to denote the possible types of move. Instead of
directly generating a proposal Y j with density p(X j−1, ·) as in the basic Metropolis–
Hastings algorithm, we ﬁrst randomly choose a move type m ∈M and then generate
Y j with a move-dependent density pm(X j−1, ·). Let γm(x) denote the probability of
choosing move m when the current state is X j−1 = x. Then, given the value of X j−1,
the distribution of the proposal Y j is given by
P(Y j ∈A | X j−1) =

m∈M
γm(X j−1)

A
pm(X j−1, y) dy.
The resulting algorithm follows.
Algorithm 4.12
(Metropolis–Hastings method with different move types)
input:
a probability density π (the target density)
(γm(x))m∈M,x∈S with 
m∈M γm(x) = 1 for all x ∈S
transition densities pm: S × S →[0, ∞) for m ∈M
X0 ∈

x ∈S
 π(x) > 0

randomness used:
independent m j ∈M with P(m j = m) = γm(x) for all m ∈M
independent Y j from the transition densities pm (the proposals)
U j ∼U[0, 1] i.i.d.
output:
a sample of a Markov chain X with stationary density π.
As an abbreviation we deﬁne αm: S × S →[0, 1] by
αm(x, y) = min
π(y)γm(y)pm(y, x)
π(x)γm(x)pm(x, y), 1

(4.7)
for all x, y ∈S and all m ∈M.
1: for j = 1, 2, 3, . . . do
2:
generate m j ∈M with P(m j = m) = γm(X j−1) for all m ∈M.
3:
generate Y j with density pm j(X j−1, ·)
4:
generate U j ∼U[0, 1]
5:
if U j ≤αm j(X j−1, Y j) then
6:
X j ←Y j
7:
else
8:
X j ←X j−1
9:
end if
10: end for

122
AN INTRODUCTION TO STATISTICAL COMPUTING
Proposition 4.13
The process (X j) j∈N constructed by algorithm 4.12 is a Markov
chain with stationary density π.
Proof
Assume that X j−1 has density π. Then we have
P(X j ∈A) =

S
π(x)

m∈M
γm(x)

S
pm(x, y)
(αm(x, y)1A(y) + (1 −αm(x, y)) 1A(x)) dy dx
=

m∈M

S

S
π(x)γm(x)pm(x, y)
(αm(x, y)1A(y) + (1 −αm(x, y)) 1A(x)) dy dx.
From the deﬁnition of the acceptance probabilities αm we get
π(x)γm(x)pm(x, y)αm(x, y)
= min (π(x)γm(x)pm(x, y), π(y)γm(y)pm(y, x))
= π(y)γm(y)pm(y, x)αm(y, x)
for all x, y ∈S and all m ∈M. Thus, following the same steps as in the proof of
proposition 4.3, we get
P(X j ∈A) =

m∈M

S
π(x)γm(x)

S
pm(x, y)
(αm(x, y)1A(x) + (1 −αm(x, y)) 1A(x)) dy dx
=

S
π(x)

m∈M
γm(x)

S
pm(x, y) 1A(x) dy dx
=

S
π(x) 1A(x) dx
= P(X j−1 ∈A)
for all sets A. This shows that the distributions of X j−1 and X j are the same and thus
π is a stationary density of the Markov chain.
Since the proof of proposition 4.13 considers each transition X j−1 →X j sepa-
rately, the statement stays true if the probabilities γm(x) for choosing move m depend
on the time j as well as on the location x. In particular, it is possible to choose the
moves solely based on time, for example by applying a pair of moves alternatingly
or, more generally, by having move types M = {0, 1, . . . , k −1} and then always
choosing move type ( j mod k) at time j. Since proposition 4.13 still applies, π is still
a stationary density even if the move probabilities are time dependent. It transpires

MARKOV CHAIN MONTE CARLO METHODS
123
that, even if every move type on its own would result in a reversible Markov chain,
the combined Markov chain is no longer necessarily reversible.
Example 4.14
Assume that the target distribution π is known to have several
disjoint regions A1, . . . , An of high probability and that π is very small outside these
regions. Such a situation could, for example, occur when π is a mixture distribution
(see deﬁnition 2.6). In this situation we can augment a random walk Metropolis
algorithm tuned for moving inside these regions by adding a separate move type
for moving between different regions. A very simple example of this approach is as
follows: let M = {1, 2}. For moves of type m = 1 construct proposals as
Y j = X j−1 + ε j,
where ε j ∼N(0, σ 2
1 ) and σ1 is comparable with the diameter of the regions Ai. For
moves of type m = 2 construct proposals as
Y j = X j−1 + η j,
where η j ∼N(0, σ 2
2 ) and σ2 is comparable with the distances between the regions
Ai.
Some subtleties occur in the context of the Metropolis–Hastings method with
different move types: a common choice for move types is to construct moves which
only change a single coordinate of the state X j−1 ∈Rd. In such cases, the values
Y j are concentrated on a lower dimensional subset of Rd and the distribution of
Y j cannot be described by a density on Rd. Thus, the densities pm(x, ·) from the
deﬁnition of αm do not exist in this situation. Nevertheless, inspection of the proof of
proposition 4.13 reveals that a slight generalisation of algorithm 4.12 still works in
this case. The resulting generalisation is described in the following lemma.
Lemma 4.15
Assume that for each move m ∈M in algorithm 4.12 there is a set
Cm ⊆Rd × Rd such that
(x, y) ∈Cm
⇐⇒
(y, x) ∈Cm
(4.8)
for all x, y ∈Rd and a density ϕm: Cm →[0, ∞) such that, if a proposal Y j is
constructed from a previous state X j−1 ∼π, we have
P(Y j ∈B, X j−1 ∈A) =

Cm
1A(x)1B(y) ϕm(x, y) dy dx
(4.9)
for all A, B ⊆Rd. Consider algorithm 4.12, but using acceptance probabilities
αm(x, y) = min
γm(y)ϕm(y, x)
γm(x)ϕm(x, y), 1

(4.10)

124
AN INTRODUCTION TO STATISTICAL COMPUTING
instead of the expression from (4.7). The process generated by this algorithm is a
Markov chain with stationary density π.
While the pair (x, y) in (4.9) lies in the 2d-dimensional space Rd × Rd, the
integral in (4.9) is an integral over the subset Cm only and in many applications we
have dim(Cm) < 2d. For example, if Cm is (d + 1)-dimensional, the integral will only
be a (d + 1)-dimensional integral, even if the state space S of the Markov chain Xn
has dimension d > 1 and thus 2d > d + 1. Similarly, if Cm contains isolated points,
the integral is interpreted as a sum over these points.
Proof
(of lemma 4.15, outline only). We ﬁrst observe that (4.10) can be obtained
from (4.7) by replacing π(y)pm(y, x)/π(x)pm(x, y) with ϕm(y, x)/ϕm(x, y). Assume
that X j−1 ∼π. If the conditional distribution of Y j given X j = x for move type
m has a density pm(x, ·) then, by (4.9), we have ϕm(x, y) = π(x)pm(x, y) and
we are in the situation of algorithm 4.12. In this case we get X j ∼π from proposi-
tion 4.13.
If the conditional distribution of Y j has no density, as discussed above, a more
advanced concept of integration is required to make sense of the integrals in (4.9)
and we omit the required technical details. Once the meaning of the integrals with
density ϕm(x, y) has been clariﬁed, a proof can be constructed by following the lines
of the proof of proposition 4.13 while replacing occurrences of π(x)pm(x, y) with
ϕm(x, y) and using the modiﬁed deﬁnition of α(x, y).
Example 4.16
If the move m ∈M maps the current state X j−1 ∈Rd to the proposal
Y j ∈Rd, constructed such that the component Y j,1 is random with density q: R →
[0, ∞) and (Y j,2, . . . , Y j,n) = (X j−1,2, . . . , X j−1,n), then we can apply lemma 4.15
with
C =

(x, y) ∈Rd × Rd  x2 = y2, . . . , xd = yd
 ∼= Rd × R.
Since we have
P(Y j ∈B, X j−1 ∈A) =

Rd π(x) 1A(x)

R
q(y1) 1B(y1, x2, . . . , xd) dy1 dx,
the pair (X j−1, Y j) has density ϕ(x, y) = π(x)q(y1) on the (d + 1)-dimensional set
C and the required acceptance probability for move m is
αm(x, y) = min
γm(y)ϕ(y, x)
γm(x)ϕ(x, y), 1

= min
γm(y)π(y)q(x1)
γm(x)π(x)q(y1), 1

.
The resulting moves only change the ﬁrst component of the state vector, but similar
moves can be introduced to change any of the remaining components.

MARKOV CHAIN MONTE CARLO METHODS
125
Example 4.17
If the move m ∈M maps the current state X ∈Rd to the proposal
Y = X + ε, where the distribution of ε ∈Rd is symmetric, that is where P(ε ∈A) =
P(−ε ∈A), then lemma 4.15 can be applied with
C =

(x, y) ∈Rd × Rd  x −y ∈supp(ε j)
 ∼= Rd × supp(ε).
Here supp(ε) is the support of the distribution of ε and, since ε is symmetric, the
support of ε satisﬁes z ∈supp(ε) if and only if −z ∈supp(ε). Thus condition (4.8) is
satisﬁed. Also, the probabilities for going from x to y = x + ε and for going from y
to x = y + (−ε) are the same and thus, for this example, we have ϕ(y, x)/ϕ(x, y) =
π(y)/π(x). Consequently, the acceptance probability for this move type is
αm(x, y) = min
γm(y)ϕ(y, x)
γm(x)ϕ(x, y), 1

= min
γm(y)π(y)
γm(x)π(x), 1

.
In the case dim (supp(ε)) < d, this result is a generalisation of the random walk
Metropolis sampling method from Section 4.1.3.
4.2
Convergence of Markov Chain Monte
Carlo methods
MCMC methods are based on the following deﬁnition.
Deﬁnition 4.18
Let X be a random variable and f be a function such that f (X) ∈R.
Then the MCMC estimate for E ( f (X)) is given by
ZMCMC
N
= 1
N
N

j=1
f (X j)
(4.11)
where (X j) j∈N is a Markov chain with the distribution of X as a stationary distribution.
We have discussed methods for constructing such Markov chains in Section 4.1.
In this section we will discuss conditions for the estimate ZMCMC
N
to converge to
the expectation E ( f (X)) and we will also discuss factors affecting the error of this
estimate.
4.2.1
Theoretical results
Whether or not the MCMC estimate ZMCMC
N
from (4.11) converges to the exact value
E ( f (X)) as N →∞, depends on the Markov chain X: if the Markov chain does not
reach all parts of the state space, or if it moves too slowly, the estimator ZMCMC
N
will
only correspond to an average of f over the visited part of the state space instead

126
AN INTRODUCTION TO STATISTICAL COMPUTING
of to the full expectation E ( f (X)). Thus, for the approximation (4.1) to work, the
Markov chain (X j) j∈N must move through the state space quickly enough. In this
section we will discuss criteria for the Markov chain to explore the state space fast
enough.
Deﬁnition 4.19 and deﬁnition 4.20, as well as the result from theorem 4.21, use
the concept of ‘σ-ﬁnite measures’ from mathematical analysis. A measure μ on S
assigns a ‘size’ μ(A) to subsets A ⊆S. In order to avoid technical complications, we
do not give the formal deﬁnition of a ‘σ-ﬁnite measure’ here but instead give a few
examples:
r If S = Rd, then the d-dimensional volume, that is μ(A) = |A| for all A ⊆S,
is a σ-ﬁnite measure on S.
r If S is ﬁnite or countable, the number of elements in a subset, that is μ(A) = #A
for all A ⊆S, is a σ-ﬁnite measure on S.
r Every probability measure is a σ-ﬁnite measure on S, that is if X is a random
variable we can use μ(A) = P(X ∈A). In particular, the stationary distribution
π of a Markov chain X is a σ-ﬁnite measure.
Deﬁnition 4.19
Let (X j) j∈N be a Markov chain with state space S and let μ be
a σ-ﬁnite measure on S with μ(S) > 0. Then the Markov chain (X j) j∈N is called
μ-irreducible, if for every A ⊆S with μ(A) > 0 and for every x ∈S there is a j ∈N
such that
P(X j ∈A | X0 = x) > 0.
This deﬁnition formalises the idea that there are regions A in the state space
which can always be reached by the process, independently of where it is started,
thus preventing situations where the state space consists of different parts between
which no transitions are possible. The following extension of the concept of μ-
irreducibility makes sure that the times between returns to A are small enough so that
inﬁnitely many visits to A happen for every individual sample path of the Markov
chain.
Deﬁnition 4.20
Let (X j) j∈N be a Markov chain with state space S. The Markov
chain (X j) j∈N is called Harris recurrent, if there exists a σ-ﬁnite measure μ on S
with μ(S) > 0 such that the following conditions hold:
(a) (X j) j∈N is μ-irreducible.
(b) For every A ⊆S with μ(A) > 0 and for every x ∈A we have
P
⎛
⎝
∞

j=1
1A(X j) = ∞

X0 = x
⎞
⎠> 0.

MARKOV CHAIN MONTE CARLO METHODS
127
Using the concept of Harris recurrence we can state the main result of this
section, guaranteeing convergence of the estimate ZMCMC
N
from (4.11) to the required
limit.
Theorem 4.21
(law of large numbers for Markov chains) Let (X j) j∈N be a time
homogeneous, Harris recurrent Markov chain on a state space S with stationary
distribution π and let X ∼π. Let f : S →R be a function such that the expectation
E ( f (X)) exists. Then, for every initial distribution, we have
lim
N→∞
1
N
N

j=1
f (X j) = E ( f (X))
with probability 1.
Proofs of this result can, for example, be found in the books by Robert and Casella
(2004, theorem 6.63) and Meyn and Tweedie (2009, theorem 17.0.1).
In order to apply theorem 4.21 to the Markov chains generated by the
Metropolis–Hastings algorithms 4.2 and 4.4, we need to ﬁnd conditions for the
resulting Markov chains to be Harris recurrent. While irreducibility as described
in deﬁnition 4.19 is often relatively easy to check, the second condition in the
deﬁnition 4.20 of Harris recurrence is less obvious. In many cases, verifying the
conditions of theorem 4.21 for Markov chains generated by the Metropolis–Hastings
algorithm is greatly simpliﬁed by the following result (Robert and Casella, 2004,
lemma 7.3).
Lemma 4.22
Let (X j) j∈N be the Markov chain generated by the Metropolis–
Hastings algorithms 4.2 or 4.4, with stationary distribution π. Assume that (X j) j∈N
is π-irreducible. Then (X j) j∈N is Harris recurrent.
This lemma, together with theorem 4.21 allows the convergence of the
Metropolis–Hastings method to be proved in many situations. For reference, we
state the combined result as a corollary.
Corollary 4.23
Let (X j) j∈N be the Markov chain generated by the Metropolis–
Hastings algorithms 4.2 or 4.4, with state space S, initial value X0 ∈S (either random
or deterministic) and stationary distribution π. Assume that (X j) j∈N is π-irreducible.
Let X ∼π, let f : S →R be a function such that the expectation E ( f (X)) exists
and let ZMCMC
N
be deﬁned by (4.11). Then we have
lim
N→∞ZMCMC
N
= E ( f (X))
with probability 1.

128
AN INTRODUCTION TO STATISTICAL COMPUTING
Proof
The statement is a direct consequence of lemma 4.22 and theorem 4.21.
The usefulness of corollary 4.23 is illustrated by the following example.
Example 4.24
Assume that the target distribution is given by a density π on S = Rd.
Let (X j) j∈N be generated by the random walk Metropolis method from algorithm
4.9 with increments ε j ∼N(0, σ 2). Assume that A ⊆Rd with
π(A) =

A
π(x) dx > 0
and x ∈Rd. The proposal Y1 satisﬁes
P(Y1 ∈A | X0 = x) =

A
ψ0,σ 2(y −x) dy,
where ψ0,σ 2 is the density of the N(0, σ 2)-distribution. Similarly, the probability p
of the proposal Y1 falling in the set A and being accepted, conditional on X0 = x, is
given by
p =

A
α(x, y) ψ0,σ 2(y −x) dy =

A
min
π(y)
π(x), 1

ψ0,σ 2(y −x) dy.
(4.12)
Our aim is to show p > 0, thus proving π-irreducibility of the generated Markov
chain in one time step.
Assume ﬁrst that π(x) > 0. To show that p > 0 in this case, we will use
the fact that for every function h ≥0 we have

A h(x) dx > 0 if and only if

y ∈A
 h(y) > 0
 > 0 where | · | denotes the d-dimensional volume. Thus, since

A π(y) dy > 0, we have

y ∈A
 π(y) > 0
 > 0.
Since π(x) > 0 and since the density ψ0,σ 2 is strictly positive, we ﬁnd


y ∈A
 min
π(y)
π(x), 1

ψ0,σ 2(y −x) > 0
 > 0
and thus, using (4.12), we get p > 0. For the case π(x) = 0, the value α(x, y) and
thus the next step in algorithm 4.9 is not deﬁned. There are two ways to deal with
this case. Either we can restrict the state space S to only include points x ∈Rd
with π(x) > 0. In this case, corollary 4.23 guarantees convergence of the random
walk Metropolis algorithm for all initial points X0 with π(X0) > 0. Alternatively,
we can modify the algorithm to always accept the proposal whenever π(x) = 0

MARKOV CHAIN MONTE CARLO METHODS
129
causes α(x, y) to be undeﬁned. For the modiﬁed algorithm we get p > 0 for all
initial values x ∈Rd. In this case, corollary 4.23 gives convergence of ZMCMC
N
for
all X0 ∈Rd.
The result from theorem 4.21 only states convergence of the averages ZMCMC
N
to
the limit E ( f (X)), but does not give convergence of the distributions of the X j to the
distribution of X. In fact, it transpires that for convergence of the distributions to hold,
the additional assumption of ‘aperiodicity’ of (X j) j∈N is required. In practice, the
Markov chains generated by the Metropolis–Hastings method are normally aperiodic
and thus, in situations where theorem 4.21 applies we normally also have convergence
of the distribution of X j to π as j →∞. For discussion of this topic we refer again
to the books by Robert and Casella (2004, theorem 6.63) and Meyn and Tweedie
(2009, theorem 17.0.1).
When applying the results from this section, it is important to keep in mind that
theorem 4.21 and corollary 4.23 only make statements about the behaviour of the
estimator ZMCMC
N
in the limit as N →∞. The results do not quantify the speed
of convergence and convergence can sometimes be extremely slow (we will see an
example of this at the end of Section 4.4.2).
4.2.2
Practical considerations
In the previous section we have seen that the Markov chain constructed for an MCMC
method must be irreducible in order for the MCMC estimate to converge to the correct
value E ( f (X)). In this section we will discuss different aspects of the speed of this
convergence. As before, we consider the mean squared error
MSE(ZMCMC
N
) = Var(ZMCMC
N
) + bias(ZMCMC
N
)2,
(4.13)
where the estimator ZMCMC
N
is given by equation (4.11) and we use lemma 3.12 to
get the representation (4.13) for the mean squared error.
4.2.2.1
Convergence to stationarity
Since π is a stationary density of the Markov chain X, if we start the Markov chain
with X0 ∼π, we have X j ∼π for every j ∈N. While this is true in theory, in
practice we usually cannot easily generate samples from the density π (otherwise we
would be using Monte Carlo estimation instead of MCMC) and thus we need to start
the Markov chain with a different distribution or with a ﬁxed value. Consequently,
for ﬁnite j, the distribution of X j will not have density π and normally we will have
E

f (X j)
	
̸= E ( f (X)). Thus, the argument from proposition 3.14 will no longer
apply and we will have bias(ZMCMC
N
) ̸= 0.
On the other hand, assuming aperiodicity of X, the distribution of X j will
converge to the correct distribution as j →∞and thus we can normally expect

130
AN INTRODUCTION TO STATISTICAL COMPUTING
bias(ZMCMC
N
) →0 as N →∞. A typical approach to reduce the effect of bias is to
omit the ﬁrst samples from the Monte Carlo estimate in order to give the Markov
chain time to get close to stationarity. In practice one often uses
E ( f (X)) ≈
1
N −M
N

j=M+1
f (X j)
(4.14)
instead of the estimate ZMCMC
N
from (4.11). The value M is chosen large enough
that the process can be assumed close to stationarity, but small enough that enough
samples remain for the estimate. Even if the samples X1, X2, . . . , X M are not directly
used in the estimate (4.14), the corresponding values still need to be computed in
order to get the following values X M+1, X M+2, . . . In the context of the estimate
(4.14), the time interval 1, 2, . . . , M is called the burn-in period.
A very simple illustration of the effect of a burn-in period can be found in
Figure 4.2. In the situation depicted there, the ﬁrst values of the process do not
resemble typical samples from the target distribution. The samples become useful
only after the region of high probability is reached. The same kind of behaviour,
a directional motion towards a region of high probability followed by ﬂuctuations
inside this region, can sometimes be observed in more realistic applications of MCMC
and can the guide the choice of burn-in period.
From the results in Section 4.2.1 we know that a burn-in period is not required
for convergence of the method. The main effect of a burn-in period is to remove
0
200
400
600
800
1000
0
20
40
60
80
100
j
X j
Figure 4.2
Path of a Metropolis–Hastings Markov chain, started far away from
equilibrium: the displayed path corresponds to a random walk Metropolis method
with target distribution N(100, 1), started at X0 = 0. The ﬁrst values of X j, until the
vertical line, do not behave like a sample from the target distribution. A good choice
of the burn-in period for this Markov chain would be the time interval before the
vertical line.

MARKOV CHAIN MONTE CARLO METHODS
131
samples which would be extremely unlikely under the target distribution from the
computation, thus reducing the sample size N required to get good estimates. An
alternative approach to the use of a burn-in phase is to start the Markov chain with a
‘typical value’ for the target distribution, for example at the maximum of the target
density π.
4.2.2.2
Effective sample size
The mean squared error given in equation (4.13) is determined by the bias (considered
above) as well as the variance of the estimator. Since X forms a Markov chain, the
samples X j are not independent, and we have
Var

ZMCMC
N
	
= Var
⎛
⎝1
N
N

j=1
f (X j)
⎞
⎠
= Cov
⎛
⎝1
N
N

j=1
f (X j), 1
N
N

l=1
f (Xl)
⎞
⎠
= 1
N 2
N

j,l=1
Cov

f (X j), f (Xl)
	
.
(4.15)
If we assume that the Markov chain is close to stationarity, then we have
Var(X j) ≈Var(X), that is the variance of X j is approximately constant in time.
Similarly, the joint distribution of X j and Xl (approximately) only depends on the
lag k = l −j but not on the individual values of j and l. Thus, the covariance between
f (X j) and f (Xl) only (approximately) depends on l −j and we get
Cov

f (X j), f (Xl)
	
≈Cov

f (X0), f (Xl−j)
	
.
Similarly, if the Markov chain is close to stationarity, the correlation between f (X j)
and f (Xl) satisﬁes
Corr

f (X j), f (Xl)
	
=
Cov

f (X j), f (Xl)
	

Var

f (X j)
	
Var ( f (Xl))
≈Cov

f (X j), f (Xl)
	
Var ( f (X))
(4.16)
and (approximately) only depends on l −j. This correlation is called the lag-(l −j)
autocorrelation of the Markov chain X.

132
AN INTRODUCTION TO STATISTICAL COMPUTING
Combining equation (4.15) and equation (4.16) and using the symmetry of the
covariance we ﬁnd
Var

ZMCMC
N
	
= 1
N 2
N

j,l=1
Cov

f (X j), f (Xl)
	
≈1
N Var ( f (X)) + 2 1
N 2
N

j=1
N

l= j+1
Cov

f (X j), f (Xl)
	
≈1
N Var ( f (X))
⎛
⎝1 + 2 1
N
N

j=1
N

l= j+1
Corr

f (X0), f (Xl−j)
	
⎞
⎠
≈1
N Var ( f (X))
⎛
⎝1 + 2 1
N
N

j=1
N−j

k=1
Corr ( f (X0), f (Xk))
⎞
⎠.
Markov chains used in MCMC methods (under the assumption of aperiodicity)
have the property that the distribution of a Markov chain is guaranteed to converge
to the same distribution π for every initial distribution. Thus, the Markov chain
‘forgets’ the initial distribution over time and we expect the lag-k autocorrelations
ρk = Corr ( f (X0), f (Xk)) to converge to 0 as k →∞(see Figure 4.3 for illustration).
For sufﬁciently large N we have
N−j

k=1
Corr ( f (X0), f (Xk)) ≈
∞

k=1
Corr ( f (X0), f (Xk))
and, since the right-hand side no longer depends on j we ﬁnd
Var

ZMCMC
N
	
≈1
N Var ( f (X))
⎛
⎝1 + 2 1
N
N

j=1
∞

k=1
Corr ( f (X0), f (Xk))
⎞
⎠
= 1
N Var ( f (X))

1 + 2
∞

k=1
Corr ( f (X0), f (Xk))

.
(4.17)
Comparing Var

ZMCMC
N
	
from equation (4.17) to the corresponding variance
Var(eMC) from (3.8) for the basic Monte Carlo estimate we see that the variance,
and thus the error, of the MCMC estimate differs by the presence of the additional
correlation term on the right-hand side of (4.17). Typically this term is positive and
thus MCMC methods are less efﬁcient than a basic Monte Carlo method would be.
While the above derivation is heuristic, an exact result such as (4.17) can be
derived under conditions very similar to the conditions required for theorem 4.21.

MARKOV CHAIN MONTE CARLO METHODS
133
0.0 0.2 0.4 0.6 0.8 1.0
ρk
ρk
ρk
0.0 0.2 0.4 0.6 0.8 1.0
0
50
100
150
200
0.0 0.2 0.4 0.6 0.8 1.0
Lag
(a)
(b)
(c)
Figure 4.3
The autocorrelation functions for the Markov chain from example 4.10
and example 4.25 for different values of σ: (a) σ = 1; (b) σ = 6; (c) σ = 36. The
corresponding estimation problem is to ﬁnd E(X2). The values of σ considered here
are the same as for the paths shown in Figure 4.1. The autocorrelation function in (b)
decays fastest, indicating that this is the most efﬁcient of the three MCMC methods
considered here.
Such a result can, for example, be found as part of theorem 17.0.1 in the book by
Meyn and Tweedie (2009).
In analogy to equation (3.8) we write
Var

ZMCMC
N
	
≈
1
Neff
Var ( f (X))
where
Neff =
N
1 + 2 ∞
k=1 Corr ( f (X0), f (Xk))

134
AN INTRODUCTION TO STATISTICAL COMPUTING
is called the effective sample size. The effective sample size does not only depend
on N and on the Markov chain X, but it also depends on the function f used
in the estimation problem. The value Neff can be used to quantify the increase in
computational cost caused by the dependence of samples in an MCMC method.
Compared with a Monte Carlo method for the same problem, the number of samples
generated needs to be increased by a factor N/Neff to achieve a comparable level of
error. The empirical sample size can be used to compare the efﬁciency of different
MCMC methods: the larger Neff/N is, the more efﬁcient the resulting method.
Example 4.25
In example 4.10 and the corresponding Figure 4.1 we considered
a family of random walk Metropolis methods for the target distribution π given by
equation (4.6). In the methods considered there, we can choose the variance σ of
the increments of the random walk. To compare the efﬁciency of the random walk
Metropolis algorithm for different values of σ, we can consider the effective sample
sizes Neff.
The autocorrelations and thus the effective sample size depend on the speciﬁc
estimation problem, that is on the choice of the function f in (4.1). Here we consider
the problem of estimating the variance of X. Since π is symmetric, we have E(X) = 0
and thus Var(X) = E(X2). Thus, we consider f (x) = x2.
The autocorrelations Corr

f (X j), f (X j+k)
	
for this choice of f are shown in
Figure 4.3. Using these autocorrelations, we ﬁnd the following effective samples
sizes.
σ = 1
σ = 6
σ = 36
Neff
0.0119 · N
0.1317 · N
0.0337 · N
N/Neff
84.0
7.6
29.7
Thus, even ignoring the additional error introduced by bias, the MCMC method
from example 4.10 needs for σ = 1 approximately 84 times more samples than a
basic Monte Carlo method with the same error would need. The efﬁciency can be
greatly improved by tuning the parameter σ, for example for σ = 6 the MCMC
method only needs approximately 7.6 times more samples than a basic Monte Carlo
method. Thus we see that σ = 6 is a much better choice for the given problem than
either σ = 1 or σ = 36.
4.2.2.3
Acceptance rates
By comparing Figure 4.1 and Figure 4.3 we see that the effective sample size in
example 4.25 is inﬂuenced by two different effects. In Figure 4.1(a) and Figure 4.3(a),
where the variance σ 2 of the increments is smallest, the effective sample size is large
because the small increments allow the process only slow movement through the state
space; reaching different regions of state space takes a long time and thus samples
close in time have a tendency to also be close in space. In contrast, the constant

MARKOV CHAIN MONTE CARLO METHODS
135
1e−04
1e−01
1e+02
1e+05
0.0
0.2
0.4
0.6
0.8
1.0
σ2
Average acceptance probability
Figure 4.4
The average acceptance probability E(α(X j−1, Y j)) of the random walk
Metropolis sampler from example 4.10 and example 4.25, as a function of σ 2. The
three vertical lines (from left to right) correspond to the values σ 2 = 12, σ 2 = 62,
and σ 2 = 362 from Figure 4.3.
stretches of the path in Figure 4.1(c) indicate that the large σ 2 considered there leads
to very low acceptance rates and this, in turn causes the small effective sample size
for large values of σ.
The observation that the acceptance rate affects the efﬁciency of MCMC methods
can be used as a simple, heuristic tuning criterion: in a random walk Metropolis
method, we expect the average acceptance rate to decrease as the variance σ 2 of the
increments increases. (This decrease is not necessarily monotonically, though.) The
variance σ 2 should be chosen as large as possible under the constraint that the average
acceptance probability should not be too small. To illustrate this, Figure 4.4 shows
the average acceptance rate for the random walk Metropolis methods considered in
example 4.10 and example 4.25, as a function of σ 2.
4.2.2.4
Convergence diagnostics
Even in cases where convergence of an MCMC method is formally proved conver-
gence to equilibrium can be extremely slow, resulting in inaccurate and misleading
estimates. Situations where this problem occurs include:
r In multimodal distributions the Markov chain can get ‘stuck’ in a mode for a
very long time. This will be the case if transitions between modes require
a very unlikely sequence of steps, for example if the states between the
modes are very unlikely and if direct transitions between the modes are never
proposed.
If the positions of modes of the target distribution are not known in advance,
this problem can be very difﬁcult to detect since, before the ﬁrst transition

136
AN INTRODUCTION TO STATISTICAL COMPUTING
between modes happens, the path of the Markov chain ‘looks stationary’. If
the problem is not detected, the resulting estimate ZMCMC
N
will estimate the
conditional expectation E

f (X)
 X ∈A
	
where A is the mode in question,
instead of the full expectation E ( f (X)).
One way to resolve this issue is to use an MCMC method with different
move types (see Section 4.1.5) and to introduce special move types which
perform transitions between modes.
r In high-dimensional situations it takes a long time for a Markov chain to explore
all of the space. In cases where most of the mass of the target distribution is
concentrated in a small region of the state space, the Markov chain may take
an extremely long time to ﬁnd this region or even may never reach the relevant
region of space at all.
This kind of problem can be recognised by the fact that the Markov chain
moves through space ‘aimlessly’, possibly behaving like a random walk, or
diverging to inﬁnity. If enough information about the target distribution is
available, this problem can be resolved by starting the Markov chain in the
region of space where the target distribution is concentrated.
To recognise problems caused by slow convergence, so called convergence diag-
nostics can be used. Here we restrict ourselves to a few very simple cases. First,
it is always a good idea to plot some one-dimensional functions of the state of the
Markov chain over time. If the Markov chain is one-dimensional, this could be the
state X j itself. In the case of multidimensional Markov chains this could be, for
example, the coordinate with the highest variance. The resulting plots (see for exam-
ple Figure 4.1 and Figure 4.2) allow to visually assess whether the process is close to
stationarity.
To detect the presence of different modes, one can run several copies of the
Markov chain, using different starting values. The idea here is then to assess whether
all of these Markov chains move into the same mode of the distribution, or whether
they end up in different modes. This question can either be answered visually by using
plots as above, or by analytical methods, for example based on the idea of comparing
the sample variance of the samples from each instance of the Markov chain to the
sample variance of all generated samples combined. A commonly used convergence
diagnostic based on this idea is introduced in Gelman and Rubin (1992).
Convergence diagnostics can be used both to determine a good burn-in period
and to determine whether a given value of N is large enough to give useful estimates.
4.2.2.5
Numerical errors
A ﬁnal consideration when using MCMC methods in practice is the effect of rounding
error caused by the number representation in the computer. While stochastic methods
are often quite robust in this regard, the effect of rounding errors often causes problems
when evaluating the acceptance probability α(X j−1, Y j) in the Metropolis–Hastings
algorithm.

MARKOV CHAIN MONTE CARLO METHODS
137
As an example we consider here the case of algorithm 4.2, where the acceptance
probability has the form
α(x, y) = min
π(y)p(y, x)
π(x)p(x, y), 1

.
(4.18)
Due to the restrictions of number representation in the computer, the numerical values
for probabilities π(x) and p(x, y) in the program will not exactly coincide with the
exact values and problems can occur in cases where π gets very small. To avoid the
resulting problems, equation (4.18) can be rewritten as
α(x, y) = min

exp

log π(y)p(y, x)
π(x)p(x, y)

, 1

= min (exp (log π(y) + log p(y, x) −log π(x) −log p(x, y)) , 1) .
(4.19)
This representation avoids numerical problems by performing all arithmetic on the
logarithmic terms, which are of much smaller magnitude, and only exponentiating
the result after all possible cancellations have taken place.
4.3
Applications to Bayesian inference
In a Bayesian model, one assumes that the parameter θ of a statistical model is itself
random. We consider the following setting:
r Data x = (x1, . . . , xn) is given, where x forms a sample of i.i.d. random vari-
ables X1, . . . , Xn ∈Rd. Here we assume that the distribution of the Xi has a
density ϕX|θ(x | θ), so that the distribution of X = (X1, . . . , Xn) is given by
pX|θ(x | θ) =
n
i=1
ϕX|θ(xi | θ)
(4.20)
for all x = (x1, . . . , xn) ∈(Rd)n and all θ ∈Rp.
r The distribution of the data depends on an unknown parameter θ ∈Rp where θ
is assumed to be random with density pθ(θ). The distribution pθ of θ is called
the prior distribution.
Thus, the data are assumed to be generated by a two-step procedure where ﬁrst θ
is chosen randomly and then, given the value of θ, samples X1, . . . , Xn ∼ϕX|θ(· | θ)
are generated to obtain the observations x1, . . . , xn.
Here, symbols such as pX|θ denote densities and the subscript indicates that this
is the conditional density of X, given the value of θ. The variables used as arguments
of these densities are mostly denoted by the corresponding lower case letters, that is
pX(x) is the density of X at the point x ∈(Rd)n. By a slight abuse of notation, we
use lower case θ both for the random variable and for the corresponding values.

138
AN INTRODUCTION TO STATISTICAL COMPUTING
Our aim is to gather as much information as possible about the unknown para-
meter θ from given data x. Typically, the data x do not completely determine the
value of θ. Instead, since θ is assumed to be random, the solution of this parameter
estimation problem will be given by the conditional distribution of θ, given the
observation X = x. This distribution is called the posterior distribution; it depends
both on the data x and on the prior distribution for θ. By Bayes’ rule (see Section
A.2), we ﬁnd the density of the posterior distribution as
pθ|X(θ | x) =
pX|θ(x | θ)pθ(θ)

pX|θ(x | t)pθ(t) dt = 1
Z pX|θ(x | θ)pθ(θ)
(4.21)
for all θ ∈Rp, where Z =

pX|θ(x | t)pθ(t) dt is the normalisation constant. The
value x in equation (4.21) is the given data and thus a constant and we will consider
the given expression as a function of θ.
Denote the true value of θ which was used to generate the observations x by θ∗.
If sufﬁcient data are given and if the dependence of the distribution of the Xi on the
parameter θ is sensitive enough, the posterior distribution pθ|X will be concentrated
around the true value θ∗and the more data are given the more concentrated the
posterior will be. Given the posterior distribution from equation (4.21), we can obtain
results such as:
r An estimates for the unknown parameter value can be found by either taking
the conditional expectation E(θ | X = x) or by considering the mode or median
of the posterior distribution.
r The uncertainty remaining in the estimate can be quantiﬁed by considering the
conditional variance Var(θ | X = x) or the corresponding standard deviation.
r More speciﬁc questions about the possible behaviour of the parameter, given the
available observations, can be obtained by studying the posterior distribution.
For example the probability that the parameter is inside a region A ⊆Rp can
be found as P(θ ∈A | X = x).
While the posterior distribution is known ‘explicitly’ from equation (4.21), eval-
uating expectations or probabilities with respect to the posterior distribution can be
challenging in practice. Reasons for this include the fact that often the value of the
normalisation constant Z in (4.21) cannot be found, and also the general problems
involved in evaluating expectations for distributions with complicated densities.
Following the approach described in Section 3.1, a widely used strategy is to
employ Monte Carlo and MCMC methods to evaluate expectations with respect
to the posterior distribution. This allows us to evaluate expectations of the form
E( f (θ) | X = x), if we are able to generate samples from the distribution with density
π(θ) = pθ|X(θ | x) = 1
Z pX|θ(x | θ)pθ(θ)
(4.22)
for all θ ∈Rp, where x is the given data. In some simple situations, for example in
example 3.6, it is possible to generate the required samples using rejection sampling,

MARKOV CHAIN MONTE CARLO METHODS
139
but often it is cumbersome to ﬁnd a feasible proposal distribution and thus samples
from the density (4.22) are more commonly generated using MCMC methods. This
is the approach we will consider here.
Since the posterior distribution is typically concentrated around the true parameter
value θ∗, we do not expect any problems arising from multimodality and we can use
the random walk Metropolis algorithm described in Section 4.1.3. For this algorithm,
the probability of accepting a proposal ˜θ if the previous state was θ is given by
α(θ, ˜θ) = min
π( ˜θ)
π(θ), 1

= min
 pX|θ(x | ˜θ)pθ( ˜θ)
pX|θ(x | θ)pθ(θ), 1

(4.23)
where x is the given observations and where we use the fact that the normalisation
constant Z from (4.22) in the numerator and denominator cancels. The right-hand
side in (4.23) can be further expanded using the deﬁnition of pX|θ from equation
(4.20). Since, for large n, the product in (4.20) can be very small, we also use the
trick from equation (4.19) to avoid potential problems caused by rounding errors.
The resulting expression is
pX|θ(x | ˜θ)pθ( ˜θ)
pX|θ(x | θ)pθ(θ) =
n
i=1 ϕX|θ(xi | ˜θ)pθ( ˜θ)
n
i=1 ϕX|θ(xi | θ)pθ(θ)
= exp
 n

i=1
log ϕX|θ(xi | ˜θ) + log pθ( ˜θ)
−
n

i=1
log ϕX|θ(xi | θ) −log pθ(θ)

.
(4.24)
Substituting this expression into (4.23) gives the form in which α(θ, ˜θ) should be
used in an implementation of algorithm 4.9.
Example 4.26
For the prior let μ ∼U[−10, 10] and σ ∼Exp(1), independently,
and set θ = (μ, σ) ∈R2. Assume that the data consist of i.i.d. values X1, . . . , Xn ∼
N(μ, σ 2) and that we have observed values x = (x1, . . . , xn) for the data. In this
case, the prior density is
pμ,σ(μ, σ) = 1
201[−10,10](μ) · exp(−σ)1[0,∞)(σ)
and the conditional density of the observations given the parameters is
pX|μ,σ(x | μ, σ) =
n
i=1
1
√
2πσ 2 exp

−(xi −μ)2
2σ 2

=
1
(2π)n/2 · 1
σ n exp

−1
2σ 2
n

i=1
(xi −μ)2

.

140
AN INTRODUCTION TO STATISTICAL COMPUTING
The constants 1/20 and 1/(2π)n/2 cancel in the numerator and denominator of (4.23)
and using (4.24) we ﬁnd
pX|μ,σ(x | ˜μ, ˜σ)pμ,σ( ˜μ, ˜σ)
pX|μ,σ(x | μ, σ)pμ,σ(μ, σ)
= 1[−10,10]( ˜μ) · 1[0,∞)(˜σ) · σ n
˜σ n
· exp

−1
2˜σ 2
n

i=1
(xi −˜μ)2 −˜σ +
1
2σ 2
n

i=1
(xi −μ)2 + σ

for all ˜μ, ˜σ ∈R and all μ ∈[−10, 10] and σ ≥0. Here we can assume that μ and σ
are inside the valid parameter range (and thus can omit the corresponding indicator
functions in the denominator), because α(μ, σ, ˜μ, ˜σ) only needs to be deﬁned for
(μ, σ) with π(μ, σ) > 0. Finally, substituting these expressions into equation (4.23)
we get the acceptance probability for the random walk Metropolis algorithm for this
example:
α(μ, σ, ˜μ, ˜σ)
= min

1[−10,10]( ˜μ) · 1[0,∞)(˜σ) · σ n
˜σ n
· exp

−1
2˜σ 2
n

i=1
(xi −˜μ)2 −˜σ +
1
2σ 2
n

i=1
(xi −μ)2 + σ

, 1

In order to apply the random walk Metropolis algorithm as given in algorithm
4.9, we have to choose a distribution for the increments ε j. This choice affects
the efﬁciency of the resulting MCMC method. For simplicity we choose here ε j ∼
N(0, σ 2Ip) where Ip is the p-dimensional identity matrix. The parameter σ 2 can be
used to tune the method for efﬁciency as described in Section 4.2.2. This leads to the
following algorithm:
1: choose a θ0 with π(θ0) > 0
2: for j = 1, 2, 3, . . . do
3:
generate ε j ∼N(0, σ 2Ip)
4:
˜θ j ←θ j−1 + ε j
5:
compute a = α(θ j−1, ˜θ j) using (4.23) and (4.24)
6:
generate U j ∼U[0, 1]
7:
if U j ≤a then
8:
θ j ←˜θ j
9:
else
10:
θ j ←θ j−1
11:
end if
12: end for

MARKOV CHAIN MONTE CARLO METHODS
141
From proposition 4.3 we know that the output of this algorithm is a Markov chain
with stationary distribution π = pθ|X(· | x), that is that it generates samples from the
posterior distribution in our Bayesian parameter estimation problem. Using corollary
4.23 and example 4.24 we the ﬁnd that
E

f (θ)
 X = x
	
= lim
N→∞
1
N
N

j=1
f

θ j
	
.
This convergence holds for every initial value θ0 with π(θ0) > 0. Thus we have solved
the given estimation problem.
While the MCMC method works for every initial value θ0, we have seen in
Section 4.2.2 that convergence of the results can be improved if θ0 is chosen inside
a region where the target distribution π is large. If a classical point estimate ˆθn =
ˆθn(x1, . . . , xn) for θ is available, θ0 = ˆθn will often be a good choice.
Many generalisations of the methodology presented in this section are possible:
if some of the parameters or the observations are discrete instead of continuous,
very similar methods can be derived by replacing the corresponding densities with
probability weights. Also, MCMC methods can be used for Bayesian inference where
the models have a more complex structure than in the situation of the present section.
Examples of such situations can be found in Sections 4.4.2 (mixture model with
known number of components) and 4.5.2 (mixture model with unknown number of
components).
4.4
The Gibbs sampler
The Gibbs sampler is an MCMC method which can be used if the state space has a
product structure. It is based on the idea of updating the different components of the
state one at a time, thus potentially reducing a high-dimensional sampling problem
to a sequence of more manageable, low-dimensional sampling problems.
4.4.1
Description of the method
The Gibbs sampler is applicable in situations where the state space S can be written
as a ﬁnite product of spaces, that is where we have
S = S1 × S2 × · · · × Sn.
Elements of this space are vectors x = (xi)i=1,2,...,n where xi ∈Si for every i. Situa-
tions where the Gibbs sampler can be applied include:
r In Bayesian parameter estimation problems, n is typically small, say 2 or 3,
and often the spaces Si have different dimensions. We will study examples of
this type in Section 4.4.2.

142
AN INTRODUCTION TO STATISTICAL COMPUTING
r In applications from statistical mechanics or image processing, n is typically
large but all spaces Si are the same. In this case we write
S = C I
where I = {1, 2, . . . , n} and C is the state space for the individual components.
We will see examples of such situations in Section 4.4.3.
As with other MCMC methods, our aim is to construct a Markov chain
with a given distribution π on S as its stationary distribution. In algorithm 4.27
we use the conditional distribution of a single component Xi of X ∈S, condi-
tioned on the values of all other components and we denote this distribution by
πXi|X¬i(· | x1, . . . , xi−1, xi+1, . . . , xn). Here, X¬i stands for X with the component Xi
left out. In the case of discrete state space Si, the conditional distribution of Xi is given
by probability weights πXi|X¬i whereas, if Si is continuous, πXi|X¬i is normally given
as a probability density. To simplify notation, in this section we write the time of the
Markov chain as an upper index, so that X( j)
i
corresponds to the value of component
i ∈I of the Markov chain X at time j ∈N0. Using this notation, the general Gibbs
sampler algorithm takes the following form.
Algorithm 4.27
(Gibbs sampler)
input:
a distribution π = πX1,...,Xn
initial state X0 ∈S = S1 × S2 × · · · × Sn
randomness used:
samples from the distributions πXi|X¬i(· | x1, . . . , xi−1, xi+1, . . . , xn)
output:
a sample of a Markov chain (X( j)) j∈N with stationary distribution π
1: for j = 1, 2, 3, . . . do
2:
let m ←( j −1 mod n) + 1
3:
generate
ξ ( j) ∼πXm|X¬m(· | X( j−1)
1
, . . . , X( j−1)
m−1 , X( j−1)
m+1 , . . . , X( j−1)
n
)
4:
deﬁne X( j) ∈S by
X( j)
i
=

X( j−1)
i
if i ̸= m and
ξ ( j)
otherwise
for all i = 1, 2, . . . , n.
5: end for

MARKOV CHAIN MONTE CARLO METHODS
143
The index m = ( j −1 mod n) + 1 constructed in step 2 of algorithm 4.27
periodically cycles through the components 1, 2, . . . , n as j increases. Alternative
versions of the Gibbs sampler algorithm can be constructed where the index m ∈
{1, . . . , n} is chosen randomly in each iteration of the loop in algorithm 4.27
instead.
Proposition 4.28
The process (X( j)) j∈N constructed by algorithm 4.27 is a Markov
chain with stationary density π.
Proof
Since in each step of the algorithm X( j) is constructed from X( j−1) and
additional randomness, the process X = (X( j)) j∈N is a Markov chain starting in X0.
Assume X j−1 ∼π. Then
X( j−1)
¬m
= (X( j−1)
1
, . . . , X( j−1)
m−1 , X( j−1)
m+1 , . . . , X( j−1)
n
) ∼π¬m.
The construction of X( j)
m = ξ ( j) in line 3 of the algorithm depends on the values of
the components of X( j−1)
¬m
. Assuming the distribution of the components X( j)
i
has a
density, we can integrate over all possible values of X( j−1)
¬m
to ﬁnd
P(X( j)
1
∈A1, . . . , X( j)
n
∈An)
=

 
1A1×···×An(x1, . . . , xn)
· πXm|X¬m(xm | x¬m) dxm
· πX¬m(x¬m) dx1 · · · dxm−1 dxm+1 · · · dxn
=

 
1A1×···×An(x1, . . . , xn) · πX1,...,Xm(x1, . . . , xm) dx1 · · · dxn.
This shows that (X( j)
1 , . . . , X( j)
n ) has density π = πX1,...,Xm as required. If the distri-
bution of the X( j)
i
is given by probability weights instead of densities, the same result
can be obtained by replacing the integrals over densities with sums over probability
weights.
Example 4.29
Let A ⊆R2 be a set with area |A| < ∞and let X be uniformly
distributed on A, that is X ∼π where the density π is given by
π(x) = 1
|A|1A(x)

144
AN INTRODUCTION TO STATISTICAL COMPUTING
for all x ∈R2. Writing the state space R2 as R × R, we can apply the Gibbs sampler
from algorithm 4.27 to generate samples from the distribution of X. Using equation
(A.5), we can ﬁnd the conditional density of X1 given X2 = x2 ∈R as
πX1|¬X1(x1 | x2) = πX1|X2(x1 | x2)
=
π(x1, x2)

R π(˜x1, x2) d ˜x1
=
1
|A|1A(x1, x2)

R
1
|A|1A(˜x1, x2) d ˜x1
=
1
˜x1
 (˜x1, x2) ∈A
1{˜x1|(˜x1,x2)∈A}(x1)
for all x1 ∈R. Thus, πX1|¬X1 is the density of the uniform distribution on the set
˜x1
 (˜x1, x2) ∈A

, that is on the ‘slice’ of A where the second coordinate is ﬁxed to
x2. Similarly, the density πX2|¬X2 = πX2|X1 transpires to be the density of the uniform
distribution on the ‘slice’
˜x2
 (x1, ˜x2) ∈A

, where the ﬁrst coordinate is ﬁxed to
the value x1. Thus, the Gibbs sampler for π takes the following form:
1: for j = 1, 2, 3, . . . do
2:
if j is odd then
3:
generate ξ ( j) ∼U

x1
 (x1, X( j−1)
2
) ∈A

4:
X( j) ←

ξ ( j), X( j−1)
2

5:
else
6:
generate ξ ( j) ∼U

x2
 (X( j−1)
1
, x2) ∈A

7:
X( j) ←

X( j−1)
1
, ξ ( j)
8:
end if
9: end for
From Lemma 1.33 we know that we can generate samples X with density f by
sampling (X, Y) from the uniform distribution on the set
A =

(x, y) ∈R × [0, ∞)
 0 ≤y < f (x)

⊆R2.
Such samples can be obtained using the Gibbs sampler as explained in the ﬁrst part
of this example. The resulting method and its higher dimensional variants are known
as the slice sampler.
Despite the fact that the Gibbs sampler from algorithm 4.27 does not contain
a rejection mechanism, the algorithm is a special case of the Metropolis–Hastings
algorithm with different move types as introduced in Section 4.1.5. To see this,
consider the Metropolis–Hastings algorithm with move types M = {0, 1, . . . , n −1}.

MARKOV CHAIN MONTE CARLO METHODS
145
Assume that the algorithm, at time j, always performs a move of type m = j mod n
by constructing the proposal Y from the current state X as
Yi =

Xi
for i ̸= m and
ξ
for i = m
for i = 1, 2, . . . , n, where
ξ ∼πXm|X¬m(· | X1, . . . , Xm−1, Xm+1, . . . , Xn).
The random variable Y, given X, is concentrated on the subspace {X1} × · · · ×
{Xm−1} × Sm × {Xm+1} × · · · × {Xn}. Thus, Y has no density but instead we are in
the situation of lemma 4.15. If we let
Cm =

(x, y) ∈S × S
 xi = yi for all i ̸= m
 ∼= S × Sm,
we have (X, Y) ∈Cm with probability 1 and, assuming X ∼π,
P(Y ∈B, X ∈A)
=

S
π(x)1A(x)

Sm
πXm|X¬m(ym | x¬m)
· 1B (x1, . . . , xm−1, ym, xm+1, . . . , xn) dym dx1 · · · dxn.
Thus, the pair (X, Y) has density ϕ(x, ym) = π(x) πXm|X¬m(ym | x¬m) on Cm. From
lemma 4.15 we know now that we should choose the acceptance probability αm(x, y)
for this move type as
αm(x, y) = min
γm(y) ϕ(y, x)
γm(x) ϕ(x, y), 1

= min
γm(y) π(y) πXm|X¬m(xm | y¬m)
γm(x) π(x) πXm|X¬m(ym | x¬m), 1

Since all possible transitions from x to y satisfy x¬m = y¬m, the acceptance proba-
bilities can be rewritten as
αm(x, y) = min
γm(y) π(y) πXm|X¬m(xm | x¬m)
γm(x) π(x) πXm|X¬m(ym | y¬m), 1

= min
π(y) πXm|X¬m(xm | x¬m) πX¬m(x¬m)
π(x) πXm|X¬m(ym | y¬m) πX¬m(y¬m), 1

= min
π(y) πXm,X¬m(xm, x¬m)
π(x) πXm,X¬m(ym, y¬m), 1

= min
π(y) π(x)
π(x) π(y), 1

= 1.

146
AN INTRODUCTION TO STATISTICAL COMPUTING
Thus, moves of type m will always be accepted and no rejection step is necessary.
The resulting algorithm is identical with the Gibbs sampler from algorithm 4.27.
4.4.2
Application to parameter estimation
In this section we illustrate the use of the Gibbs sampler in Bayesian inference, by
considering the problem of sampling from the posterior distribution of the parameters
in a mixture distribution, given a random sample from the mixture. The difference
compared with the more generic Bayesian parameter estimation problems considered
in Section 4.3 is that here we can make use of the hierarchical structure of the
mixture distribution to derive a specialised MCMC algorithm for the problem under
consideration.
Let k ∈N and σ > 0 be ﬁxed and let μ1, . . . , μk ∼ϕμ be independent random
vectors in Rd, where we assume the distribution ϕμ to have a density. Given μ =
(μ1, . . . , μk), let X1, . . . , Xn be an i.i.d. sample from the mixture distribution
ϕX|μ = 1
k
k

a=1
N(μa, σ Id),
where the mixture components are normal distribution with mean μa and covariance
matrix σ Id and where Id denotes the d-dimensional identity matrix. In the later parts
of this example we will restrict ourselves to the case d = 2 and we will assume that,
under the prior distribution, μ1, . . . , μk ∼U ([−10, +10] × [−10, +10]) i.i.d., that
is we will assume that the prior density for the cluster means μa is given by
ϕμ(μa) =
1
202 1[−10,10]×[−10,10](μa).
(4.25)
for all μa ∈R2.
Our aim is to generate samples from the posterior distribution of the μa, that is
from the conditional distribution of μ1, . . . , μk given observations Xi = xi ∈Rd for
i = 1, 2, . . . , n. This target distribution has density
pμ|X(μ | x) = pX|μ(x | μ) pμ(μ)
pX(x)
.
(4.26)
In this section we will explain how the Gibbs sampling algorithm 4.27 can be used
to generate samples from this density.
We can extend the state space of the model by introducing new variables Yi ∈
{1, 2, . . . , k} to indicate which component the values Xi belongs to: for this we
generate Xi in a two-step procedure by ﬁrst generating Yi ∼U{1, 2, . . . , k} and

MARKOV CHAIN MONTE CARLO METHODS
147
then generating Xi ∼N(μYi, σ Id). Using this approach, the joint distribution of the
vectors μ = (μ1, . . . , μk), Y = (Y1, . . . , Yn) and X = (X1, . . . , Xn) is given by
pμ,Y,X(μ, y, x) =
k
a=1
ϕμ(μa) ·
n
i=1
1
k ψ(xi; μyi, σ Id)
for all μ ∈(Rd)k, y ∈{1, . . . , k}n and x ∈(Rd)n, where the term 1/k gives the
probability P(Yi = yi) and ψ(·; μyi, σ Id), given by
ψ(ξ; μ, σ Id) =
1
(2πσ 2)d/2 exp

−|ξ −μ|2
2σ 2

(4.27)
for all ξ ∈Rd, is the density of the normal distribution N(μ, σ Id) and | · | denotes
the Euclidean norm in Rd. In this extended model, both μ and Y are unknown and
we want to sample from the conditional distribution of (μ, Y) ∈(Rd)k × {1, . . . , k}n,
given a sample x ∈(Rd)n of X.
To solve this extended sampling problem we can use the Gibbs sampler with state
space S = S1 × S2 where S1 = (Rd)k and S2 = {1, . . . , k}n. The resulting version of
algorithm 4.27 requires us to alternatingly generate samples
μ( j) ∼pμ|Y,X

·
Y ( j−1), x
	
and
Y ( j) ∼pY|μ,X

·
μ( j−1), x
	
.
The densities pμ|Y,X and pY|μ,X can be found using Bayes’ rule: for pμ|Y,X we have
pμ|Y,X(μ | y, x) = pμ,Y,X(μ, y, x)
pY,X(y, x)
=
k
a=1 ϕμ(μa) · n
i=1
1
k ψ(xi; μyi , σ Id)
pY,X(y, x)
=
1
Z y,x
k
a=1
⎛
⎜⎝ϕμ(μa) ·
n
i=1
yi =a
ψ(xi; μa, σ Id)
⎞
⎟⎠
(4.28)
for all μ ∈(Rd)k, where we grouped the factors ψ(xi; μyi , σ Id) according to the
values of yi and Z y,x is the normalisation constant which makes the function
pμ|Y,X(· | y, x) a probability density. Since the right-hand side of (4.28) is the product
of individual functions of μa for a = 1, 2, . . . , k, under the conditional distribution

148
AN INTRODUCTION TO STATISTICAL COMPUTING
pμ|Y,X the components μ1, . . . , μk are independent with densities given by the cor-
responding factors in the product. Using a similar calculation, we ﬁnd the density
pY|μ,X as
pY|μ,X(y | μ, x) = pμ,Y,X(μ, y, x)
pμ,X(μ, x)
=
k
a=1 ϕμ(μa) · n
i=1
1
k ψ(xi; μyi, σ Id)
pμ,X(μ, x)
=
1
Zμ,x
n
i=1
ψ(xi; μyi, σ Id)
for all y ∈{1, 2, . . . , k}n, where Zμ,k is the normalisation constant. Since the right-
hand side is a product where each factor only depends on one of the components yi,
under the conditional distribution pY|μ,X the components Y1, . . . , Yn are independent
with weights
P(Yi = a | μ, X) =
ψ(Xi; μa, σ Id)
k
˜a=1 ψ(Xi; μ˜a, σ Id)
(4.29)
for a = 1, 2, . . . , k and for all i = 1, 2, . . . , n, where the density ψ is given by (4.27).
To be more speciﬁc, we now restrict ourselves to the case d = 2 and we consider
the prior density ϕμ given by (4.25). Then, using equation (4.28) and the deﬁnition
of ψ from (4.27), we ﬁnd the conditional density of μa given Y and X as
pμa|Y,X(μa | y, x) = 1
Z1
ϕμ(μa)
n
i=1
yi =a
ψ(xi; μa, σ I2)
= 1
Z1
ϕμ(μa)
n
i=1
yi =a
1
2πσ 2 exp

−|xi −μa|2
2σ 2

= 1
Z2
ϕμ(μa) exp
⎛
⎜⎝−
n

i=1
yi =a
|xi −μa|2
2σ 2
⎞
⎟⎠
= 1
Z3
ϕμ(μa) exp
⎛
⎜⎝−na
2σ 2 · |μa|2 + na
2σ 2 · 2⟨μa, 1
na
n

i=1
yi =a
xi⟩
⎞
⎟⎠,
where Z1, Z2 and Z3 are normalisation constants, na = n
i=1 1{a}(yi), and | · | and
⟨·, ·⟩denote the Euclidean norm and inner product on R2, respectively. Completing

MARKOV CHAIN MONTE CARLO METHODS
149
the square in the exponent and substituting the deﬁnition of ϕμ from Equation (4.25)
we ﬁnd
pμa|Y,X(μa | y, x)
= 1
Z4
1[−10,+10]×[−10,+10](μa) exp
⎛
⎜⎝−
1
2σ 2/na

μa −1
na
n

i=1
yi =a
xi

2⎞
⎟⎠,
(4.30)
where Z4 is the resulting normalisation constant. Thus, pμa|Y,X is the density of the
two-dimensional normal distribution N

1
na
n
i=1 1{a}(yi)xi, σ 2
na I2

, conditioned on
the value lying in the square [−10, +10] × [−10, +10]. Samples from this condi-
tional distribution can be easily generated using the rejection sampling algorithm
1.25 for conditional distributions, that is by repeatedly sampling from the uncondi-
tioned normal distribution and rejecting all values which fall outside the square. The
representation from (4.30) for pμa|Y,X is only deﬁned for na > 0. In the case na = 0
we have no observations corresponding to the mean μa and consequently for this
case pμa|Y,X coincides with the prior distribution ϕμ.
As we have already seen, the components of the vector Y under the distribution
pY|μ,X are independent. Their distribution is found by substituting (4.27) for d = 2
into equation (4.29).
Now that we have identiﬁed the conditional distributions pμ|Y,X and pY|μ,X, we
can implement the Gibbs sampler from algorithm 4.27 to sample from the posterior
distribution of μ and Y as follows:
1: for a = 1, 2, . . . , k do
2:
generate μ(0)
a ∼U ([−10, 10] × [−10, 10])
3: end for
4: for i = 1, 2, . . . , n do
5:
generate Y (0)
i
∼U{1, 2, . . . , k}
6: end for
7: for j = 1, 2, 3, . . . do
8:
if j is odd then
9:
for a = 1, 2, . . . , k do
10:
na ←n
i=1 1{a}(Y ( j−1)
i
)
11:
repeatedly generate ξ ( j) ∼N

1
na
n
i=1 1{a}(yi)xi, σ 2
na I2

,
until ξ ( j) ∈[−10, +10] × [−10, +10]
12:
μ( j)
a
←ξ ( j)
13:
end for
14:
Y ( j) ←Y ( j−1)
15:
else
16:
μ( j) ←μ( j−1)
17:
for i = 1, 2, . . . , n do
18:
for a = 1, 2, . . . , k do

150
AN INTRODUCTION TO STATISTICAL COMPUTING
19:
qa ←
1
2πσ 2 exp

−|ξi−μ( j−1)
a
|2
2σ 2

20:
end for
21:
generate Y ( j)
i
∈{1, 2, . . . , k} with P(Y ( j)
i
= a) = qa/ k
b=1 qb
22:
end for
23:
end if
24: end for
The implementation of the Gibbs sampler presented here is based on the following
considerations:
r We need to decide how to generate the initial values for μ(0) and Y (0). In the
implementation given above we generate μ(0) from the prior distribution. Since
there is no prior distribution for the auxiliary variables Y (0), we use the uniform
distribution on the set {1, 2, . . . , k} of allowed values instead.
r We need to alternatingly update μ and Y. In the implementation above we
update μ when j is odd and we update Y when j is even.
r In step 11 of the algorithm some care is needed when na = 0, that is when
there are no observations corresponding to cluster a. In this case the normal
distribution must be replaced by the prior distribution ϕμ.
r Since the distribution of μ( j) only converges to the posterior distribution pμ|X
from (4.26) as j →∞, a burn-in period should be implemented by discarding
the ﬁrst samples.
The result of a run of the Gibbs sampler presented above is shown in Figure 4.5.
In proposition 4.28 we have seen that the target distribution π = pμ|X from
equation (4.26) is indeed a stationary distribution of the Markov chain constructed
by the algorithm described in this section, and often the distribution of the resulting
Markov chain converges to a plausible posterior sample quickly. This is illustrated
in Figure 4.5 where after only 100 steps of burn-in period the samples for the ﬁve
cluster means μa are all centred well inside the ﬁve clusters. Nevertheless, it takes an
extremely long time for the Markov chain to explore the complete state space. This
can be seen by the fact that the posterior distribution is invariant under relabelling
the cluster means and thus, in stationarity, each of the ﬁve cluster means should be
found in any of the ﬁve clusters with equal probability. In the simulation depicted in
Figure 4.5, no transition of cluster means between clusters is observed and indeed
such transitions will not be observed in any runs of realistically achievable length.
The Markov chain generated by the algorithm converges to its stationary distri-
bution very slowly. As a consequence, it is possible for the algorithm to get ‘stuck’
in states which have low probability but where all ‘close by’ states have even lower
probability. Such states are called metastable states and, while the algorithm will
eventually escape from such states, this can take an extremely long time and thus
may not happen in practice. Such a problem is illustrated by Figure 4.6, where during
the burn-in period two components of μ moved into the same cluster, leaving a third
component to ‘cover’ two clusters. For typical samples from the posterior distribution

MARKOV CHAIN MONTE CARLO METHODS
151
−10
−5
0
5
10
−10
−5
0
5
10
Xi, 2
Xi, 1
Figure 4.5
A typical sample of size 100 from the posterior distribution (4.26) of
the cluster means for the parameter estimation problem described in Section 4.4.2.
The grey circles give the positions of the observations X1, . . . , X100, the ﬁve different
kinds of smaller, black symbols give the positions of μ( j)
a
for j = 1, 2, . . . , 100 and
a = 1, . . . , 5. A burn-in period of length 1000 has been used for this ﬁgure.
we would expect the components of μ to be distributed such that each cluster contains
exactly one of them, but numerical experiments show that the Markov chain needs
an extremely (and unachievably) long time to reach states resembling this situation.
Thus, as for any MCMC method, the output of the Gibbs sampler needs to be carefully
checked to verify that the distribution of the output is sufﬁciently close to stationarity.
4.4.3
Applications to image processing
In this section we illustrate the Gibbs sampler using an application to a simple image
processing problem. Here we will represent images as a square grid of ‘pixels’ (short
for ‘picture elements’), each of which takes values in the two-element set {−1, +1}
where −1 stands for a white pixel and the value +1 stands for a black pixel. Thus,
images in this setup are considered to be elements of the product space
S = {−1, +1}I,
(4.31)

152
AN INTRODUCTION TO STATISTICAL COMPUTING
−10
−5
0
5
10
−10
−5
0
5
10
Xi, 2
Xi, 1
Figure 4.6
A sample of size 100, similar to Figure 4.5 but using a different seed
of the random number generator. Different from Figure 4.5, the Markov chain here
has not yet converged to the stationary distribution, despite the presence of a burn-in
period of length 10 000: two components of μ, represented by the symbols × and ⋄,
are concentrated in one cluster while the component + is situated in the gap between
two clusters. This ﬁgure was created by selecting one of a large number of runs of
the algorithm.
where I is the lattice
I = {1, 2, . . . , L} × {1, 2, . . . , L}.
The elements of S are vectors of the form x = (xi)i∈I. States x ∈S can be visualised
as square grids of small black and white dots, where the colour at location i ∈I in
the grid encodes the possible values −1 and +1 for xi.
Deﬁnition 4.30
A probability measure on S where the weights are written in the
form
π(x) = 1
Z exp (−H(x))
(4.32)

MARKOV CHAIN MONTE CARLO METHODS
153
for all x ∈S is called a Gibbs measure. The function H: S →R is called the energy
and the normalisation constant
Z =

x∈S
exp (−H(x))
is called the partition function.
4.4.3.1
The Ising model
The Ising model is a model from statistical mechanics which describes the distribution
of random elements X ∈S where the state space S is of the form given by (4.31).
Motivated by the situation of image processing, we restrict ourselves to the two-
dimensional case I ⊆Z2, but we note that everything in this section can be easily
generalised to the case I ⊆Zd for d ∈N. For the Ising model, the distribution of X
is assumed to be a Gibbs measure, as described in equation (4.32), where the energy
H is given by
H(x) = −β

i, j∈I
i∼j
xix j
(4.33)
for all x ∈S. Here we write i ∼j to indicate that i, j ∈I are nearest neighbours
and the sum in the deﬁnition of H is taken over all pairs of nearest neighbours
in the grid. The constant β > 0 is called the inverse temperature. The model can
be considered for different deﬁnitions of ‘neighbouring’ pixels. Here we consider
only nearest neighbours, that is we consider a pixel i = (i1, i2) in the interior of the
grid to have neighbours (i1 + 1, i2), (i1, i2 + 1), (i1 −1, i2) and (i1, i2 −1). Pixels
on the edges of the grid are considered to have only three neighbours and the four
corner pixels have only two neighbours. An alternative approach, often chosen to
avoid complications in theoretical analysis, would be to extend the grid periodically
or, equivalently, to consider the left-most column to be adjacent to the right-most
column and the top-most row to be adjacent to the bottom-most row.
Samples X from the Ising model for different values of β are depicted in Figure
4.7. The ﬁgure clearly shows that with increasing β the tendency of neighbouring
pixels to have the same colour increases.
To apply the Gibbs sampler from algorithm 4.27 in this situation, we need to ﬁnd
the conditional distributions πXm|X¬m, that is we need to ﬁnd the distribution of the
state Xm of one pixel, given the state X¬m of all the other pixels. Using Bayes’ rule
we get
πXm|X¬m(xm | x¬m) = πXm,X¬m(xm, x¬m)
πX¬m(x¬m)
=
πXm,X¬m(xm, x¬m)
πXm,X¬m(−1, x¬m) + πXm,X¬m(+1, x¬m).
(4.34)

154
AN INTRODUCTION TO STATISTICAL COMPUTING
β = 0.3
β = 0.34
β = 0.38
β = 0.42
β = 0.46
β = 0.5
(a)
(b)
(c)
(d)
(e)
(f)
Figure 4.7
Samples from the Ising model given by equation (4.32) and equation
(4.33), on the grid I = {1, 2, . . . , 150}2, for different values of β. The panels show
the ﬁnal state X(N) of the Gibbs sampling algorithm 4.31, for N = 10 000 × 1502.

MARKOV CHAIN MONTE CARLO METHODS
155
Here, πXm,X¬m(xm, x¬m) = πX(x) is the probability of the state x. Using equation
(4.32) and equation (4.33) we can write this probability as
π(x) = 1
Z exp
⎛
⎜⎝β

i, j∈I
i∼j
xix j
⎞
⎟⎠
= 1
Z exp
⎛
⎝β xm

i∈I
i∼m
xi
⎞
⎠exp
⎛
⎜⎝β

i, j∈I
i∼j,i̸=m, j̸=m
xix j
⎞
⎟⎠.
Since both Z and the ﬁnal factor do not depend on the value of xm, the corresponding
terms in the numerator and denominator of equation (4.34) cancel and we get
πXm|X¬m(xm | x¬m) =
exp

xm β 
i∼m xi
	
exp

β 
i∼m xi
	
+ exp

−β 
i∼m xi
	.
(4.35)
These are the required conditional probabilities to generate the new value ξ j for
location j ∈I in step 3 of the Gibbs sampler algorithm 4.27. The probabilities can
be computed using only the values of the image at the (up to) four neighbours of j.
Thus, the update step in the Gibbs algorithm for the Ising model can be performed
very efﬁciently.
Algorithm 4.31
(Gibbs sampler for the Ising model)
input:
β ≥0 (the inverse temperature)
X0 ∈{−1, +1}I (the initial state)
output:
a path of a Markov chain with the Gibbs measure from (4.32) and (4.33) as its
stationary distribution
randomness used:
independent samples ξ ( j) ∈{−1, +1} for j ∈N
1: for j = 1, 2, 3, . . . do
2:
m1 ←(( j −1) mod L) + 1
3:
m2 ←(⌊( j −1)/L⌋mod L) + 1
4:
d ←X( j−1)
m1−1,m2 + X( j−1)
m1+1,m2 + X( j−1)
m1,m2−1 + X( j−1)
m1,m2+1
5:
p ←
1
1+exp(−2βd)
6:
Generate ξ ( j) ∈{−1, +1} such that P

ξ ( j) = +1
	
= p.
7:
Deﬁne X( j) ∈S by
X( j)
i
=

X( j−1)
i
if i ̸= (m1, m2) and
ξ ( j)
otherwise
for all i ∈I.
8: end for

156
AN INTRODUCTION TO STATISTICAL COMPUTING
In steps 2 and 3 of the algorithm, a pixel position m = (m1, m2) is constructed
such that m cycles through all possible pixel positions cyclically. When implementing
step 4 of the algorithm, some care is needed when m is on the edge of the grid I:
we set Xi = 0 for all i /∈I. The result of different simulations using this algorithm
is shown in Figure 4.7.
As a direct consequence of proposition 4.28, the distribution of the Ising model,
given by equation (4.32) and equation (4.33) is a stationary distribution of the Markov
chain X constructed by algorithm 4.31. Using the results from Section 4.2.1, it is
easy to check that the Markov chain constructed in algorithm 4.31 is irreducible and
aperiodic and thus the distribution of X( j) converges to the exact distribution of the
Ising model for every initial condition X(0).
The Ising model is well-studied in statistical mechanics and nearly everything
about the behaviour of the model is known. For example, a well-known result is that
for large grid sizes the behaviour of the system changes when the inverse temperature
crosses the critical value
β∗= log(1 +
√
2)
2
≈0.44
(see e.g. Pathria, 1996, Section 12.3). For β < β∗typical states consist of a‘patchwork
pattern’ of mostly white and mostly black regions. This pattern is visible in Figure
4.7(a–d). For β > β∗typical states are dominated by one colour, showing only
isolated ‘specks’ of the opposite colour. This pattern is shown by Figure 4.7(e)
and (f). By symmetry, the Markov chain will be in predominantly white states and
predominantly black states for approximately equal amounts of time. Transitions
between these two patterns take an extremely long time and will not be observed in
practice but, for ﬁnite grid size, it is easy to show that the Markov chain is irreducible
and thus, by the results from Section 4.2.1, transitions will happen on long time-scales.
In contrast, for the extension of the Ising model to an inﬁnite grid such transitions no
longer happen. This change of behaviour of the system when β crosses the critical
point β∗is an example of a phase transition and is of great interest in statistical
physics. In particular, many interesting and difﬁcult results concern the behaviour of
the system at the critical point β = β∗.
4.4.3.2
Bayesian image analysis
We will now consider an application of Bayesian inference to the denoising of images.
For this setup we assume that the original image X is described by a probability
distribution on the space of all possible images, and that we have observed a noisy
version Y of this image. Our aim is to reconstruct the original image X from the
observation Y.
For simplicity we assume that we are in the situation of the preceding section, that
is that the original image is square and consists only of black and white pixels. Thus
we have X ∈S where S is the state space given in Equation (4.31) at the beginning of
this section. We also assume that the prior distribution of the original image X is given

MARKOV CHAIN MONTE CARLO METHODS
157
by the Ising model from equation (4.32) and equation (4.33). Finally, for the noisy
observation Y of the image X we assume that independent, N(0, σ 2)-distributed
random variables are added to every pixel of X, that is we have Y ∈RI and the
conditional distribution of Yi given the value of X is
Yi ∼N(Xi, σ 2)
for all i ∈I, independently.
Using this model, the posterior distribution of X given the observations Y is found
using Bayes’ rule as
pX|Y(x | y) = pY|X(y | x) pX(x)
pY(y)
=
1
pY(y)

i∈I
1
√
2πσ 2 · exp

−(yi −xi)2
2σ 2

· 1
Z exp (−H(x)) ,
(4.36)
where the energy function H is given in equation (4.33). Since xi ∈{−1, +1} for all
i ∈I, we have (yi −xi)2 = −2yi xi + (y2
i + 1) and thus we can write the posterior
distribution as the Gibbs measure
pX|Y(x | y) = 1
Z y
exp

−Hy(x)
	
,
with energy Hy given by
Hy(x) = −1
σ 2

i∈I
yixi + H(x) = −1
σ 2

i∈I
yi xi + β

i, j∈I
i∼j
xix j
for all x ∈I, where β is the inverse temperature from the Ising model.
To generate samples from the posterior distribution, we can employ the Gibbs
sampler from algorithm 4.27 with target distribution π = pX|Y. As in equation (4.34)
we have
πXm|X¬m(xm | x¬m) =
πXm,X¬m(xm, x¬m)
πXm,X¬m(−1, x¬m) + πXm,X¬m(+1, x¬m)
and introducing the additional term 
i∈I yixi/σ 2 into the derivation of equation
(4.35) gives
πXm|X¬m(xm | x¬m)
=
exp

xm

β 
i∼m xi −ym
σ 2
		
exp

β 
i∼m xi −ym
σ 2
	
+ exp

−

β 
i∼m xi −ym
σ 2
		

158
AN INTRODUCTION TO STATISTICAL COMPUTING
and thus
πXm|X¬m(+1 | x¬m) =
1
1 + exp

−2

β(
i∼m xi) −ym/σ 2		.
Substituting these conditional probabilities into the general Gibbs sampler algorithm
4.27 leads to the following variant of algorithm 4.31.
Algorithm 4.32
(Gibbs sampler in image processing)
input:
β ≥0 (the inverse temperature)
y ∈RI (the observed, noisy image)
X0 ∈{−1, +1}I (the initial state)
output:
a path of a Markov chain with the Gibbs measure from (4.32) and (4.33) as its
stationary distribution
randomness used:
independent samples ξ ( j) ∈{−1, +1} for j ∈N
1: for j = 1, 2, 3, . . . do
2:
m1 ←(( j −1) mod L) + 1
3:
m2 ←(⌊( j −1)/L⌋mod L) + 1
4:
d ←X( j−1)
m1−1,m2 + X( j−1)
m1+1,m2 + X( j−1)
m1,m2−1 + X( j−1)
m1,m2+1
5:
p ←
1
1+exp(−2(βd+ym/σ 2))
6:
Generate ξ ( j) ∈{−1, +1} such that P

ξ ( j) = +1
	
= p.
7:
Deﬁne X( j) ∈S by
X( j)
i
=

X( j−1)
i
if i ̸= (m1, m2) and
ξ ( j)
otherwise,
for all i ∈I.
8: end for
In steps 2 and 3 of the algorithm, a pixel position m = (m1, m2) is constructed
such that m cycles through all possible pixel positions cyclically. When implementing
step 4 of the algorithm, we set Xi = 0 for all i /∈I. The result of three simulations
using this algorithm, for different values of σ, is shown in Figure 4.8.
4.5
Reversible Jump Markov Chain Monte Carlo
While MCMC methods like the Metropolis–Hastings method can be used in arbitrary
spaces, so far we only have considered the case of generating samples in the Euclidean
space Rd. The Reversible Jump Markov Chain Monte Carlo (RJMCMC) method,
described in this section, is concerned with a more general case, where the target

MARKOV CHAIN MONTE CARLO METHODS
159
σ = 0.5
Y
P(X = 1|Y)
σ = 1
σ = 2
Figure 4.8
Pairs of noisy images Y together with reconstructions generated using
algorithm 4.32 with inverse temperature β = 0.5. The grey values in the reconstructed
images give the posterior probability that the corresponding pixel in the original
image was black, ranging from black for probability 1 to white for probability 0.
distribution lives on the disjoint union of several spaces Rdk. This setup, while
seeming artiﬁcial at ﬁrst glance, is useful in various classes of practical applications:
r Bayesian inference can be used to make simultaneous inference about the
choice of model as well as the model parameters, by assigning prior proba-
bilities to different models. If the models under consideration have different

160
AN INTRODUCTION TO STATISTICAL COMPUTING
numbers of parameters, the basic MCMC approach becomes difﬁcult, but
RJMCMC can still be used. This approach is illustrated in Section 4.5.2.
r Many types of random geometric objects have a variable number of parameters.
These include, for example random trees and random graphs. The RJMCMC
meethod can be used to generate samples from the distribution of such random
geometrical objects.
r Intensity functions, describing the rate of random events in time, can be mod-
elled as piecewise constant functions, parameterised by change point positions
and function values between the change points. The number of parameters in
such a model depends on the number of change points. In a Bayesian setting,
such an intensity function will have a random number of change points and
thus the model will have a random number of parameters. Application of the
RJMCMC method in this context is, for example, discussed in the article by
Green (1995).
4.5.1
Description of the method
Due to the complex structure of the state space used in RJMCMC methods, more
mathematical formalism is required to state the RJMCMC algorithm than was neces-
sary in the previous sections. This section introduces the required notation and states
the general RJMCMC algorithm.
We start the exposition by giving a mathematical description of the state space:
Let I be a ﬁnite or countable set and let dk ∈N0 for all k ∈I be given. Deﬁne
Sk = {k} × Rdk
for all k ∈I and
S =

k∈I
Sk.
Then the elements z of the space S have the form z = (k, x), where x ∈Rdk and
k ∈I. Since the index k is included as the ﬁrst component of all elements in Sk, the
spaces Sk are disjoint and each z ∈S is contained in exactly one of the subspaces
Sk. For a value (k, x) ∈S, the ﬁrst component, k, indicates which of the spaces Rdk
a point is in while the second component, x, gives the position in this space. The
space S is the state space our target distribution will live on and the Markov chain
constructed by the RJMCMC algorithm will move in.
Next, we specify the target distribution π on the space S. If Z ∼π, then Z can
be written as Z = (K, X) and we need to specify the joint distribution of K ∈I and
X ∈RdK . To describe such a distribution we can use a density π which is split between

MARKOV CHAIN MONTE CARLO METHODS
161
the different subspaces Sk, that is a function π(·, ·) such that π(k, ·) : Rdk →[0, ∞)
for every k ∈I and

k∈I

Rdk
π(k, x) dx = 1.
(4.37)
Then we have
P(K = k) =

Rdk
π(k, x) dx
and
P(K = k, X ∈A) =

A
π(k, x) dx
for all A ⊆Rdk and all k ∈I.
Example 4.33
We can use the above formalism to construct a simple model for the
number and diameter of craters on a given area of moon surface. Assume that the
number K of craters is Poisson-distributed with parameter λ, and that each crater,
independently, has a random diameter X given by a Pareto distribution with density
f (x) =

α
xα+1
if x ≥1 and
0
otherwise.
(We ignore craters with size less than 1 for this example.) Then, using the notation
introduced above, we have
I = N0
the possible numbers of craters,
dk = k
one parameter per crater (the diameter),
P(K = k) = e−λ λk
k!
the probability of having exactly k craters,
π(k, x) = e−λ λk
k!
k
i=1 f (xi)
the joint density of k diameters X1, . . . , Xk
for all x ∈Rdk and all k ∈I.
Our aim is to construct a Markov chain with stationary distribution π, using
the Metropolis–Hastings algorithm on the state space S. Since this Markov chain
moves in S, the state at time j is described by a pair (K j, X j). To describe the
transition probabilities of such a Markov chain, for each (k, x) ∈S we need to
specify the distribution of (K j, X j) when (K j−1, X j−1) = (k, x). It transpires that in
the RJMCMC algorithm it is advantageous to ﬁrst determine the value of K j and only
then, in a second step, to determine the value of X j from the conditional distribution,

162
AN INTRODUCTION TO STATISTICAL COMPUTING
conditioned on the value of K j. Thus, the transitions of the Markov chain from
(k, x) ∈S to (l, y) ∈S will be described by probability weights b(k, x;l) with

l∈I
b(k, x;l) = 1
and probability densities p(k, x;l, ·) on Rdl for all (k, x) ∈S. If (K j, X j) j∈N is
described by b and p, then
P

K j = l, X j ∈A
 K j−1 = k, X j−1 = x
	
= b(k, x;l)

A
p(k, x;l, y) dy
for all k,l ∈I, x ∈Rdk and A ⊆Rdl.
The next ingredient used in the RJMCMC algorithm is the idea of splitting the
transition mechanism into different move types, as described in Section 4.1.5. We
denote the set of all possible move types by M and, for a given state (k, x) ∈Sk, the
probability of choosing move m ∈M will be denoted by γm(k, x). The probabilities
γm(k, x) satisfy

m∈M
γm(k, x) = 1
for all (k, x) ∈S. In the presence of different move types, the transition probabilities
given by b and p depend on the move type m, that is instead of b and p we consider
probability weights bm and probability densities pm for all m ∈M.
When computing the corresponding acceptance probabilities for the Metropolis–
Hastings algorithm, there are two different cases to consider: the simpler of the two
cases is the case when the proposal (l, y) lies in the same space as the previous
state (k, x) does, that is when we have l = k. In this case, occurring with with
probability bm(k, x; k), the distribution of the new location X j is given by a density
pm(k, x; ·) : Rdk →[0, ∞). We will see in proposition 4.37, that the corresponding
acceptance probabilities for this case can be chosen as
αm(k, x; y) = min
π(k, y)γm(k, y)bm(k, y; k)pm(k, y; x)
π(k, x)γm(k, x)bm(k, x; k)pm(k, x; y), 1

(4.38)
for all x, y ∈Rdk and all k ∈I. This expression is very similar to the form of the
acceptance probability we found for algorithm 4.12; only the probabilities bm(k, ·; k)
for staying in the space k ∈I need to be included. Many variations of this type
of move are possible: for example, if the proposal can take only a discrete set of
values, we can replace the densities pm(k, x; ·) by probability weights. Alternatively,
in case the distribution of proposals is continuous but restricted to a lower dimensional
subset of Rdk, we can replace π(k, x)pm(k, x; y) by a density ϕm(k, x; y) on a lower
dimensional subspace as described in lemma 4.15.

MARKOV CHAIN MONTE CARLO METHODS
163
The case where the proposal (l, y) falls into a space Sl ̸= Sk is more complicated:
with probability bm(k, x;l) a proposal in the space Sl = {l} × Rdl needs to be con-
structed. In the RJMCMC algorithm, instead of directly specifying the density of the
proposal on the space Rdl, the following mechanism is used. For a transition from Sk
to Sl, both Rdk and Rdl are temporarily extended to spaces of matching dimension,
that is instead of Rdk and Rdl we consider Rdk × Rnk and Rdl × Rnl, where
dk + nk = dl + nl.
(4.39)
To construct the proposal Y, we proceed as follows:
(a) Use a probability density ψm(k, x, ·;l) : Rnk →[0, ∞) to generate an auxil-
iary random variable U ∈Rnk.
(b) Use a map ϕk→l
m
: Rdk × Rnk →Rdl × Rnl to obtain
(Y, V ) = ϕk→l
m
(x,U).
While the value V ∈Rnl is not part of the proposal itself, we will see in equation
(4.40), that the value V is required to compute the acceptance probability for this
transition. The densities ψm and the maps ϕk→l
m
can be chosen as part of designing
the algorithm, subject to the following conditions.
Assumption 4.34
The maps ϕk→l
m
: Rdk × Rnk →Rdl × Rnl are bijective with
continuous partial derivatives. If the move type m allows transitions from Sk to Sl, it
also allows the reverse transition and the corresponding transition maps satisfy the
condition
ϕl→k
m
= (ϕk→l
m
)−1.
We will see in proposition 4.37, that we can choose the acceptance probabilities
for transitions from Sk to Sl as
αm(k, x, u;l, y, v)
= min
 π(l, y)γm(l, y)bm(l, y; k)ψm(l, y, v; k)
π(k, x)γm(k, x)bm(k, x;l)ψm(k, x, u;l)
det Dϕk→l
m
(x, u)
 , 1

,
(4.40)
where Dϕk→l
m
(x, u) is the Jacobian matrix of ϕk→l
m
as given in deﬁnition 1.35.
Example 4.35
Consider the case d1 = 1 and d2 = 2. To construct a move between
the corresponding spaces Rd1 and Rd2, we need to ﬁrst extend the dimensions of
theses spaces to make them equal. For simplicity we can choose n1 = 1 and n2 = 0,

164
AN INTRODUCTION TO STATISTICAL COMPUTING
so that d1 + n1 = 1 + 1 = 2 + 0 = d2 + n2. The corresponding transitions are then
described by a bijective, differentiable map ϕ1→2
m
: R × R →R2. If we choose
ϕ1→2
m
(x, u) =

x + u
x −u

(4.41)
for all (x, u) ∈R × R we ﬁnd
det Dϕ1→2
m
(x, u) = det
 ∂
∂x ϕ1→2
m,1 (x, u)
∂
∂u ϕ1→2
m,1 (x, u)
∂
∂x ϕ1→2
m,2 (x, u)
∂
∂x ϕ1→2
m,2 (x, u)

= det

1
1
1
−1

= 1 · (−1) −1 · 1
= −2.
Thus, for a move starting from (1, x) ∈S1 into S2, the proposal is constructed by ﬁrst
generating U ∈R with density ψm(1, x, ·; 2) and then letting Y = (x + U, x −U).
The move is accepted with probability αm(1, x,U; 2, Y), where
αm(1, x, u; 2, y) = min

π(2, y)γm(2, y)bm(2, y; 1)
π(1, x)γm(1, x)bm(1, x; 2)ψm(1, x, u; 2) · 2, 1

.
The argument v and the term ψm(2, y, v; 1) are omitted since n2 = 0.
Assumption 4.34 requires us to also include a corresponding transition from Rd2
and Rd1 into the same move type. This transition is described by a function ϕ2→1
m
:
R2 →R1 × R1 and, following assumption 4.34, ϕ2→1
m
has to satisfy the condition
ϕ2→1
m
= (ϕ1→2
m
)−1. Solving the equations y1 = x + u and y2 = x −u for x and u we
ﬁnd
ϕ2→1
m
(y) =
 y1+y2
2
y1−y2
2

for all y ∈R2. The Jacobian determinant for this function can either be found directly,
or by using the rules for the derivative of inverse functions:
det Dϕ2→1
m
(y) = det

Dϕ1→2
m
(x, u)
	−1
=
1
det Dϕ1→2
m
(x, u) = −1
2.
To describe the resulting reverse moves from S2 to S1 we switch notation by swapping
x, u with y, v: let (2, x) ∈S2 be the current state and assume that a move to S1

MARKOV CHAIN MONTE CARLO METHODS
165
Table 4.1
An overview of the notation used in the RJMCMC algorithm.
I
index set for the state spaces (ﬁnite or countable)
Sk
state space with index k ∈I: Sk = {k} × Rdk
S
state space of the Markov chain: S = 
k∈I Sk
π(·, ·)
split density of the stationary distribution, satisfying (4.37)
M
index set for the moves (ﬁnite or countable)
γm(k, x)
probability of choosing move m ∈M while in state (k, x) ∈Sk
bm(k, x;l)
probability for move m to move into space l ∈I when the
current state is (k, x) ∈Sk
pm(k, x; ·)
density of the proposal when moving inside space k ∈I with
move m
ψm(k, x, ·;l)
density of the auxiliary random variable U ∈Rnk, when
moving from space k into space l using move m
ϕk→l
m
map describing moves from Rdk × Rnk to Rdl × Rnl: the
proposal y ∈Rdl is given by (y, v) = ϕk→l
m
(x, u)
was selected. Then, using ϕ2→1, the proposal (1, Y) ∈S1 is constructed as Y =
(x1 + x2)/2 and the move is accepted with probability αm(2, x; 1, Y, V ) where V =
(x1 −x2)/2 and
αm(2, x; 1, y, v) = min
π(1, y)γm(1, y)bm(1, y; 2)ψm(1, y, v; 2)
π(2, x)γm(2, x)bm(2, x; 1)
· 1
2, 1

.
For this direction of move, the auxiliary value U and and the corresponding density
ψm(2, x, u; 1) are omitted since n2 = 0. To conclude this example we note that the
only choices made in this example were to use two as the dimension of the extended
spaces and the choice of the map ϕ1→2 in equation (4.41). All remaining expressions
arise as a consequence of equation (4.40) and assumption 4.34.
Table 4.1 summarises the notation introduced for the RJMCMC method. Normally
the state space S, together with the stationary distribution described by π, will be
given as part of the problem whereas the set M of moves and all quantities depending
on the move (indicated by a subscript m in Table 4.1) need to be chosen as part of
designing the method. The resulting Metropolis–Hastings algorithm has the following
form.
Algorithm 4.36
(reversible jump Markov Chain Monte Carlo)
input:
target distribution π
parameters γm, bm, pm ψm and ϕk→l
m
as in Table 4.1
initial values (K0, X0) ∈

(k, x) ∈S
 π(k, x) > 0


166
AN INTRODUCTION TO STATISTICAL COMPUTING
randomness used:
continuous samples with densities pm and ψm
discrete samples with weights γm and bm
W j ∼U[0, 1] i.i.d.
output:
a sample of a Markov chain (K j, X j) j∈N on S with stationary distribution
given by π
1: for j = 1, 2, 3, . . . do
2:
generate m j ∈M with P(m j = m) = γm(K j−1, X j−1) for all m ∈M
3:
generate L j ∈I with P(L j = l) = bm j(K j−1, X j−1;l) for all l ∈I
4:
if L j = K j−1 then
5:
generate Y j with density pm j(K j−1, X j−1; ·)
6:
let α( j) ←αm j(K j−1, X j−1; Y j), using αm from (4.38)
7:
else
8:
generate U j−1 with density ψm j(K j−1, X j−1, ·; L j)
9:
let (Y j, Vj) ←ϕ
K j−1→L j
m j
(X j−1,U j−1)
10:
let α( j) ←αm j(K j−1, X j−1,U j−1; L j, Y j, Vj), using αm from (4.40)
11:
end if
12:
generate W j ∼U[0, 1]
13:
if W j ≤α( j) then
14:
K j ←L j
15:
X j ←Y j
16:
else
17:
K j ←K j−1
18:
X j ←X j−1
19:
end if
20: end for
This algorithm can be used to generate samples from the target distribution π for
use in a MCMC method, as described in Section 4.2. As for the basic Metropolis–
Hastings method from Section 4.1.1, the resulting Markov chain is π-reversible but, to
avoid technical complications, we restrict ourselves to showing that π is a stationary
distribution of the process X. This is the result of the following proposition.
Proposition 4.37
Let π, γm, bm, pm ψm and ϕk→l
m
be as described in Table 4.1 and
let assumption 4.34 be satisﬁed. Then the Markov chain (K j, X j) j∈N generated by
algorithm 4.36 has a stationary distribution with
P(K j = k, X j ∈A) =

A
π(k, x) dx
for all j ∈N, A ⊆Rdk and k ∈I.
Proof
Let j ∈N and assume that
P(K j−1 = k, X j−1 ∈A) =

A
π(k, x) dx

MARKOV CHAIN MONTE CARLO METHODS
167
for all A ⊆Rdk and all k ∈I. To prove the proposition, we have to show that the
probability P(K j = k, X j ∈A) equals this expression.
To determine the distribution of (K j, X j), we have to consider all possible values
(k, x) of (K j−1, X j−1), all possible move types m ∈M and then the possible target
spaces i ∈I for the proposed move. Systematically listing all combinations, we ﬁnd
P

K j = l, X j ∈B
	
=

k∈I

Rdk
π(k, x)

m∈M
γm(k, x)

i∈I
bm(k, x; i) Qm,i(k, x;l, B) dx
(4.42)
for all l ∈I and B ⊆Rdl, where Qm,i(k, x;l, B) is the probability of the event
X j ∈B ⊆Rdl, conditioned on the proposal moving from space Sk into space Si
using move type m. Most terms in the ﬁnal sum will be zero, nonzero contributions
occur only for i = l if the proposal is accepted and for l = k if the proposal is rejected.
We discuss the two resulting cases separately.
For the ﬁrst case, if i = k, we always stay in space Sk and the process can only
reach B ⊆Rdl if l = k. In this case we have
Qm,k(k, x;l, B)
= δkl

Rdk
pm(k, x; y) (αm(k, x; y)1B(y) + (1 −αm(k, x; y)) 1B(x)) dy
where δkl denotes the Kronecker delta. For this case, the acceptance probability αm
is given by (4.38) and using the deﬁnition of αm we ﬁnd
π(k, x)γm(k, x)bm(k, x; k)pm(k, x; y) αm(k, x; y)
= π(k, y)γm(k, y)bm(k, y; k)pm(k, y; x) αm(k, y; x).
As in the proof of proposition 4.13, we can use this symmetry to deduce

Rdk
π(k, x)γm(k, x) bm(k, x; k) Qm,k(k, x;l, B) dx
= δkl

B
π(k, x)γm(k, x)bm(k, x; k) dx
and thus, by summing both sides over k,

k∈I

Rdk
π(k, x)γm(k, x) bm(k, x; k) Qm,k(k, x;l, B) dx
=

B
π(l, x)γm(l, x) bm(l, x;l) dx.
(4.43)
For the second case, if i ̸= k in (4.42), there are two possible ways to achieve
X j ∈B: for l = i the process can reach the space Sl by accepting the proposal,

168
AN INTRODUCTION TO STATISTICAL COMPUTING
whereas for l = k the process can stay in the space Sl by rejecting the proposal. For
all other cases, the probability Qm,i(k, x;l, B) in (4.42) equals 0. Thus we have
Qm,i(k, x;l, B) = Q(1)
m,i(k, x;l, B) + Q(2)
m,i(k, x;l, B)
where, deﬁning y and v by (y, v) = ϕk→i
m
(x, u) again,
Q(1)
m,i(k, x;l, B) = δli

Rnk
ψm(k, x, u; i)αm(k, x, u; i, y, v)1B(y) du
is the probability of reaching B with an accepted jump from Sk to Sl and
Q(2)
m,i(k, x;l, B) = δkl

Rnk
ψm(k, x, u; i) (1 −αm(k, x, u; i, y, v)) du · 1B(x)
is the probability of staying in space l = k by rejecting a move, both conditional on
the proposal using move type m and it being in space Si.
Since ϕi→k
m
= (ϕk→i
m
)−1 by assumption 4.34, the Jacobian of ϕi→k
m
satisﬁes
Dϕi→k
m
(y, v) =

Dϕk→i
m
(x, u)
	−1 and using the rule for the determinant of the inverse
of a matrix, we have
det Dϕi→k
m
(y, v)
 = 1/
det Dϕk→i
m
(x, u)
. From this relation
and the deﬁnition (4.40) of αm we ﬁnd
π(k, x)γm(k, x)bm(k, x; i)ψm(k, x, u; i) αm(k, x, u; i, y, v)
= π(i, y)γm(i, y)bm(i, y; k)ψm(i, y, v; k) αm(i, y, v; k, x, u)
·
det Dϕk→i
m
(x, u)
 .
Thus, we have

Rdk
π(k, x) γm(k, x)bm(k, x; i)Q(1)
m,i(k, x;l, B) dx
= δli

Rdk

Rnk
π(k, x)γm(k, x)bm(k, x; i)ψm(k, x, u; i)
· αm(k, x, u; i, y, v) 1B(y) du dx
= δli

Rdk

Rnk
π(i, y)γm(i, y)bm(i, y; k)ψm(i, y, v; k)
· αm(i, y, v; k, x, u) 1B(y)
det Dϕk→i
m
(x, u)
 du dx
= δli

Rdl

Rnl
π(i, y)γm(i, y)bm(i, y; k)ψm(i, y, v; k)
· αm(i, y, v; k, x, u) 1B(y) dv dy
= δli

B
π(i, y) γm(i, y)bm(i, y; k)
·

Rnl
ψm(i, y, v; k)αm(i, y, v; k, x, u) dv dy,

MARKOV CHAIN MONTE CARLO METHODS
169
where the last equality uses the substitution rule for integrals (see lemma 1.36) and
on the last line of the equation x and u are deﬁned by (x, u) = ϕi→k
m
(y, v). Summing
this expression over k ∈I and i ̸= k we ﬁnd

k∈I

Rdk
π(k, x) γm(k, x)

i̸=k
bm(k, x; i)Q(1)
m,i(k, x;l, B) dx
=

B
π(l, y) γm(l, y)

k̸=l
bm(l, y; k)
·

Rnl
ψm(l, y, v; k)αm(l, y, v; k, x, u) dv dy.
(4.44)
For the terms involving Q(2)
m,i we ﬁnd

Rdk
π(k, x) γm(k, x)bm(k, x; i)Q(2)
m,i(k, x;l, B) dx
= δkl

B
π(k, x) γm(k, x)bm(k, x; i)

Rnk
ψm(k, x, u; i)
· (1 −αm(k, x, u; i, y, v)) du dx
= δkl

B
π(k, x) γm(k, x)bm(k, x; i) dx
−δkl

B
π(k, x) γm(k, x)bm(k, x; i)
·

Rnk
ψm(k, x, u; i)αm(k, x, u; i, y, v) du dx
and summing this expression over k ∈I and i ̸= k we get

k∈I

Rdk
π(k, x) γm(k, x)

i̸=k
bm(k, x; i)Q(2)
m,i(k, x;l, B) dx
=

B
π(l, x) γm(l, x)

i̸=l
bm(l, x; i) dx
−

B
π(l, x) γm(l, x)

i̸=l
bm(l, x; i)
·

Rnl
ψm(l, x, u; i)αm(l, x, u; i, y, v) du dx.
(4.45)

170
AN INTRODUCTION TO STATISTICAL COMPUTING
So far we have considered the terms involving Q(1)
m,i and Q(2)
m,i separately. From these
results we can get the corresponding expressions for Qm,i. Combining equation (4.44)
and equation (4.45) we ﬁnd

k∈I

Rdk
π(k, x) γm(k, x)

i̸=k
bm(k, x; i)Qm,i(k, x;l, B) dx
=

B
π(l, x) γm(l, x)

i̸=l
bm(l, x; i) dx.
(4.46)
This is the result for the second case.
Finally, we can add the result from equation (4.43) and equation (4.46) to get

k∈I

Rdk
π(k, x) γm(k, x)

i∈I
bm(k, x; i)Qm,i(k, x;l, B) dx
=

B
π(l, x) γm(l, x)

i∈I
bm(l, x; i) dx
=

B
π(l, x) γm(l, x) dx
and summing this formula over m ∈M gives the right-hand side in (4.42). Thus we
ﬁnd
P

K j = l, X j ∈B
	
=

B
π(l, x)

m∈M
γm(l, x) dx
=

B
π(l, x) dx.
This shows that K j and X j have the correct distribution and that thus π is a stationary
density of (K j, X j) j∈N.
As for the original Metropolis–Hastings algorithm, the target density π enters
the RJMCMC algorithm only via the acceptance probabilities given by equation
(4.38) and equation (4.40). Since both forms of the acceptance probabilities contain
only the ratio of the two values of π, for the proposal and the current state, the
algorithm can still be applied when π is only known up to a multiplicative constant.
This is particularly useful for applications in Bayesian inference, where the constant
probability of the observations, for example the constant Z in equation (4.21), can
be omitted from the deﬁnition of π. Since the ratio of π(l, y)/π(k, x) in equation
(4.40) contains values of π corresponding to different subspaces Sl ̸= Sk, the above
argument only applies to global constants: if the subdensities π(k, ·) contain unknown

MARKOV CHAIN MONTE CARLO METHODS
171
multiplicative constants which depend on the space k and where the ratio of these
constants between different spaces is not known, the RJMCMC algorithm cannot be
applied directly.
4.5.2
Bayesian inference for mixture distributions
In this section we illustrate the RJMCMC algorithm 4.36 with the help of an example:
we consider a Bayesian inference problem for mixture distributions. For the example,
we assume the following model: observations Y1, . . . , Yn are given from a two-
dimensional mixture distribution
μ = 1
k
k

a=1
N

μa,r2
a I2
	
,
(4.47)
where I2 is the two-dimensional identity matrix. We assume that the number k of
modes, the means μa and the standard deviations ra are all random, with distributions
given by
k ∼Pois(3) + 1
(4.48)
as well as
μa ∼U ([−10, +10] × [−10, +10])
(4.49)
and
ra ∼U
1
2, 5
2
 
(4.50)
for all a ∈{1, . . . , k}. Our aim is to generate samples from the posterior distribution
of k, μa and ra, given the data Y1, . . . , Yn.
In this section we will use the RJMCMC algorithm to generate the required
samples. In order to do so we ﬁrst have to determine the state space S and the target
distribution on this state space, and then we have to choose a set of moves which
allows the algorithm to efﬁciently explore all of the state space.
4.5.2.1
State space
Since we are interested in the posterior distribution of the parameters, the state space
consists of all possible parameter values. The parameters for each component of the
mixture are μa ∈R2 and ra ∈R and thus we can choose I = N and
Sk =

R2	k × Rk ∼= R3k

172
AN INTRODUCTION TO STATISTICAL COMPUTING
for all k ∈I. Here, the parameter vector (μ,r) = (μ1, . . . , μk,r1, . . . ,rk) plays the
role of the state vector x in the general description of the method in Section 4.5.1 and
we have dk = 3k for all k ∈I.
4.5.2.2
Target distribution
The target distribution is the posterior distribution of the parameters k, μ =
(μ1, . . . , μk) and r = (r1, . . . ,rk), given observations y = (y1, . . . , yn) for Y =
(Y1, . . . , Yn). Using Bayes’ rule and the prior distributions of k, μa and ra given
by equation (4.48), equation (4.49) and equation (4.50), we ﬁnd the target distribu-
tion to be
π(k, μ,r)
= pk,μ,r|Y(k, μ,r | y)
= pY|k,μ,r(y | k, μ,r) pk,μ,r(k, μ,r)
pY(y)
= 1
Z pY|k,μ,r(y | k, μ,r) pμ|k(μ | k) pr|k(r | k) pk(k)
(4.51)
for all (μ,r) ∈Sk and all k ∈I, where Z = pY(y) is constant, the observations have
density
pY|k,μ,r(y | k, μ,r) =
n
i=1
1
k
k

a=1
1
2πr2a
exp

−(yi,1 −μa,1)2 + (yi,2 −μa,2)2
2r2a

for all y ∈(R2)n and the prior distribution is given by the densities
pμ|k(μ | k) =
k
a=1
1
202 1[−10,10]×[−10,10](μa)
pr|k(r | k) =
k
a=1
1
21[1/2,5/2](ra)
pk(k) = e−3
3k−1
(k −1)!
for all μ ∈(R2)k, r ∈Rk and k ∈N.
4.5.2.3
Move types
In order to allow the process to explore all of the state space, we need moves to
change the means μa and variances ra of the mixture components, and to change the
number k of components. Here we use a four-element set M = {mμ, mr, m±, m↔} to
enumerate the move types, where mμ denotes a move which changes the means, mr
denotes a move which changes the variances and m± denotes a move which increases

MARKOV CHAIN MONTE CARLO METHODS
173
or decreases the number of mixture components by one. For simplicity we assume
that m± only removes or adds a component at the end of the list of components. To
compensate for this and to allow for arbitrary mixture components to be removed,
we introduce an additional move type m↔, which only changes the numbering of the
mixture components, and otherwise leaves the state unchanged. We will now consider
these four move types in detail.
We need to specify the move type probabilities γm(k, μ,r) for all move types
m ∈M, such that

m∈M
γm(k, μ,r) = 1.
Since k is a discrete variable, fewer moves may be necessary to explore the possible
values for k than for the variables μ and r. To reﬂect this we choose the move
probabilities as
γmμ = γmμ(k, μ,r) = 4/10,
γmσ = γmσ (k, μ,r) = 4/10,
γm± = γm±(k, μ,r) = 1/10,
γm↔= γm±(k, μ,r) = 1/10
for all μ ∈R2k, r ∈Rk and k ∈N.
4.5.2.4
Move details
Moves of type mμ only change the mixture component means μ and thus always
propose values which stay inside the current subspace: we have bmμ(k, y; k) = 1 and
bmμ(k, y;l) = 0 for all l ̸= k. To perform a move of type mμ, we ﬁrst randomly
choose an index
a ∼U{1, 2, . . . , k}
and then replace μa by
˜μa ∼μa + N(0, σ 2
μI2);
all other components of μ stay unchanged. Since the increments ˜μ −μ are symmetric,
we can use use equation (4.38) together with example 4.17 to determine the acceptance
probability for this move type: we get
αmμ(k, μ,r; ˜μ,r) = min

π(k, ˜μ,r) · 4
10 · 1
π(k, μ,r) · 4
10 · 1, 1

= min
π(k, ˜μ,r)
π(k, μ,r), 1

,
(4.52)

174
AN INTRODUCTION TO STATISTICAL COMPUTING
where π is the posterior density given in equation (4.51). Since the unknown con-
stant Z in (4.51) appears both in the numerator and denominator of (4.52), both
occurrences of Z cancel and the value of Z is not required to evaluate the acceptance
probability αmμ. Similar considerations apply to the acceptance probabilities for the
remaining move types.
Moves of type mr change the mixture component variances r2
a. This move type
leaves the subspaces invariant and thus we have bmr (k, y; k) = 1 and bmr (k, y;l) = 0
for all l ̸= k. To perform a move of type mr, we ﬁrst randomly choose an index
a ∼U{1, 2, . . . , k}
and then replace ra by
˜ra ∼ra + N(0, σ 2
r );
all other components of r stay unchanged. Again, we can use use example 4.17 to
determine the acceptance probability for this move type: we get
αmr (k, μ,r; μ, ˜r) = min

π(k, μ, ˜r) · 4
10 · 1
π(k, μ,r) · 4
10 · 1, 1

= min
π(k, μ, ˜r)
π(k, μ,r), 1

.
(4.53)
Moves of types m± change the number k of mixture components by exactly one.
Thus, this move type always changes subspaces and we set
bm±(k, μ,r; k + 1) = bm±(k, μ,r; k −1) = 1/2
for all k > 1. For the case k = 1 we cannot decrease the number of mixture compo-
nents any further and so we set
bm±(1, μ,r; 2) = 1.
Finally, for all other combinations of k,l ∈I we set bm±(k, μ,r;l) = 0.
Since dk = 3k and dk+1 = 3k + 3, we can satisfy the dimension matching crite-
rion (4.39) for moves from k to l = k + 1 by choosing nk = 3 and nk+1 = 0. The
transition maps ϕk→k+1
m±
and ϕk+1→k
m±
must satisfy assumption 4.34, so we need to
specify only one of the two maps and the second map is then found as the inverse of
the ﬁrst one. For simplicity, we choose
ϕk→k+1
m±
(μ,r, u) = (μ1, . . . , μk, (u1, u2),r1, . . . ,rk, u3) ,
that is we just construct a new mean and and new standard deviation from the
auxiliary sample u ∈Rnk as μk+1 = (u1, u2) and rk+1 = u3, and leave the remaining

MARKOV CHAIN MONTE CARLO METHODS
175
mixture components unchanged. To get an appropriate distribution for the newly
added mixture component, we use the prior distribution to construct the density ψm±
of the auxiliary sample U j: we set
ψm±(u) = pμ|k

(u1, u2)
1
	
pr|k(u3 | 1)
=
1
202 1[−10,10](u1)1[−10,10](u2) 1
21[1/2,5/2](u3)
(4.54)
for all u ∈Rnk. This density does not depend on k and x, and thus we write ψm±(u)
instead of ψm±(k, x, u; k + 1). Since ϕk→k+1
m±
just returns a permutation of its argu-
ments, we have
det Dϕk→k+1
m±
 = 1. Putting everything together, we can now get the
corresponding acceptance probabilities from (4.40): for k ≥2 we ﬁnd
αm±(k, μ,r, u; k + 1, ˜μ, ˜r)
= min

π(k + 1, ˜μ, ˜r) · 1
10 · 1
2 · 1
π(k, μ,r) · 1
10 · 1
2 · ψm±(u), 1

= min

π(k + 1, ˜μ, ˜r)
π(k, μ,r) ψm±(u), 1

(4.55)
and for k = 1 we have
αm±(1, μ,r, u; 2, ˜μ, ˜r)
= min

π(2, ˜μ, ˜r) · 1
10 · 1
2 · 1
π(1, μ,r) · 1
10 · 1 · ψm±(u), 1

= min

π(2, ˜μ, ˜r)
2 · π(1, μ,r) ψm±(u), 1

,
(4.56)
where π is again given by (4.51) and ψm± is deﬁned in equation (4.54). Since
the auxiliary sample Vj is in Rnk+1 with nk+1 = 0, we can omit v and its density
ψm±(k + 1, μ,r, ·; k) from the formula.
The reverse transition, from k to k −1 mixture components, is now completely
speciﬁed by assumption 4.34: we ﬁnd
ϕk→k−1
m±
(μ1, . . . , μk,r1, . . . ,rk)
=

(μ1, . . . , μk−1,r1, . . . ,rk−1), (μk,1, μk,2,rk)
	
for all (μ, σ) ∈R2k × Rk, that is ϕk→k−1
m±
performs a permutation and again we
have
det Dϕk→k−1
m±
 = 1. This time we can omit the argument u ∈R0 and the

176
AN INTRODUCTION TO STATISTICAL COMPUTING
corresponding density ψm±(k, μ,r, ·; k −1). The resulting form of the acceptance
probability for k ≥3 is
αm±(k, μ,r; k −1, ˜μ, ˜r, v)
= min

π(k −1, ˜μ, ˜r) · 1
10 · 1
2 · ψm±(v)
π(k, μ,r) · 1
10 · 1
2 · 1
, 1

= min
π(k −1, ˜μ, ˜r) ψm±(v)
π(k, μ,r)
, 1

(4.57)
and for k = 2 we get
αm±(2, μ,r; 1, ˜μ, ˜r, v)
= min

π(1, ˜μ, ˜r) · 1
10 · 1 · ψm±(v)
π(2, μ,r) · 1
10 · 1
2 · 1
, 1

= min
2 · π(1, ˜μ, ˜r) ψm±(v)
π(2, μ,r)
, 1

.
(4.58)
Finally, moves of type m↔change the numbering of the components only. If
k = 1, this move type does nothing: the proposal equals the current state, and the
corresponding acceptance probability is 1. For k > 1, we randomly choose an index
a ∼U{1, 2, . . . , k −1}
and the construct the proposal as
˜μ = (μk−a+1, . . . , μk, μ1, . . . , μk−a) ,
˜r = (rk−a+1, . . . ,rk,r1, . . . ,rk−a) .
Using equation (4.38) and the fact that the target distribution π does not change when
the mixture components are interchanged, we ﬁnd the corresponding acceptance
probability as
αm↔(k, μ,r; ˜μ, ˜r) = min

π(k, ˜μ, ˜r) · 1
10 · 1 ·
1
k−1
π(k, μ,r) · 1
10 · 1 ·
1
k−1
, 1

= 1.
This completes the description of the different move types.
4.5.2.5
Implementation
Using the state space, moves and acceptance probabilities described above, we can
now implement the RJMCMC algorithm 4.36 for the Bayesian parameter estimation
problem described in this section. Once the algorithm is implemented we still need

MARKOV CHAIN MONTE CARLO METHODS
177
to choose the standard deviations σμ and σr used in the moves of type mμ and mr,
respectively. The parameters σμ and σr control the size of the increments in the corre-
sponding moves and as in Section 4.1.3 we expect the acceptance rates to decrease as
σμ and σr are increased. The methods from Section 4.2.2 can be used to guide tuning
of these parameters. For example, the values could be chosen so that the acceptance
rates are not too close to either 0 or 1. In our experiments it was also required to
write the acceptance probabilities in the numerically more robust form (4.19).
Figure 4.9 shows the ﬁnal state of one run of the RJMCMC algorithm for the
setup considered in this section. As an example of the kind of questions which can
−10
−5
0
5
10
−10
−5
0
5
10
K
Probability
0
2
4
6
8
0.0
0.2
0.4
(a)
(b)
Figure 4.9
(a) The ﬁnal state of a run of the RJMCMC algorithm described in Section
4.5.2. The small circles give the locations of n = 80 observations from the mixture
distribution (4.47). The ﬁve sets of concentric circles give the locations and standard
deviations of the ﬁve mixture components present in a sample from the posterior
distribution (the radii are ra, 1.5ra and 2ra), obtained by running N = 50 000 steps
of the RJMCMC algorithm. (b) A histogram of the values (K100 j) j=1,...,N/100 observed
during the run.

178
AN INTRODUCTION TO STATISTICAL COMPUTING
be answered using the RJMCMC algorithm, Figure 4.9(b) gives a histogram of the
distribution of the number k of mixture components observed during the run. For the
given set of observations, showing signiﬁcant overlap between mixture components,
all values k ∈{4, 5, 6, 7} can be observed with non-negligible probability.
4.6
Summary and further reading
In this chapter we have learned how Markov chains can be used as the basis of
Monte Carlo methods. The foundation of all such methods is the Metropolis–Hastings
algorithm which we have discussed in several variants. An overview over the area of
MCMC methods and more pointers to the literature can be found in Gentle et al. (2004,
Chapter II.3). Markov chains and the Metropolis–Hastings method are discussed in
Chapter 7 of Robert and Casella (2004). Applications and extensions to the methods
can also be found in Gilks et al. (1996) and Kroese et al. (2011). We have also covered
the Gibbs sampler and the RJMCMC method; both of these methods technically still
fall under the umbrella of the Metropolis–Hastings framework, but these methods are
so specialised that they are often treated as separate approaches. More information
about the Gibbs sampler can be found in Chapters 8–10 of Robert and Casella (2004)
and the RJMCMC method, introduced in Green (1995), is for example discussed in
Waagepetersen and Sorensen (2001) and Chapter 11 of Robert and Casella (2004).
In Section 4.2 we have discussed both theoretical and practical aspects related to
the convergence of MCMC methods: Section 4.2.1 summarises some of the theoret-
ical results which underpin the mathematical analysis of the resulting methods and
Section 4.2.2 discusses aspects relevant to using MCMC methods in practice. Further
discussion of convergence of MCMC methods can be found in Chapter 6 of Robert
and Casella (2004), a survey is given in Roberts and Rosenthal (2004) and many of
the technical details can be found in the monograph by Meyn and Tweedie (2009).
To illustrate the different methods introduced in this chapter we have considered
a variety of Bayesian inference problems, ranging from simple Bayesian models
(Section 4.3) over parameter estimation for mixture distributions (Sections 4.4.2 and
4.5.2) to applications in image processing (Section 4.4.3). More details about the use
of Monte Carlo methods in Bayesian statistics can be found in Gentle et al. (2004,
Chapter III.11) and Bayesian image analysis is, for example, discussed in Winkler
(1995). In passing we also discussed the Ising model from statistical mechanics;
this model is treated in many advanced texts, for example in Plischke and Bergersen
(2006) and Privman (1990).
Exercises
E4.1
Implement the random walk Metropolis algorithm with target density π given
by equation (4.6). Plot paths of the generated Markov chains for different
values of σ.

MARKOV CHAIN MONTE CARLO METHODS
179
E4.2
Implement the random walk Metropolis sampler for sampling from the target
distribution N(100, 1) on R, using proposals Y j = X j−1 + ε j where ε j ∼
N(0, 1). Experiment with different starting values X0 ∈R and create a plot
of a path X0, X1, . . . , X N which illustrates the need for a burn-in period.
E4.3
Implement the Metropolis–Hastings algorithm with target density given
by equation (4.6) and proposals Y j ∼N(X j, σ 2). For σ = 1, 6, 36, use
the output of the algorithm to estimate the lag k autocorrelations ρk =
Corr(X j, X j+k) for k = 1, 2, . . . , 100. Create plots of the autocorrelations
ρk as a function of k.
E4.4
For the situation of example 4.25, write a program which estimates, for a given
value of σ, the average acceptance probability in the Metropolis–Hastings
algorithm. Create a plot of this acceptance probability as a function of σ.
E4.5
Implement the random walk Metropolis method with N(0, η2)-distributed
increments for the posterior distribution from example 4.26. The input of your
program should be the variance η2 and a list x = (x1, . . . , xn) of observations.
The output of your program should be a path (μ j, σ j) of a Markov chain
which has pμ,σ(· | X = x) as its stationary distribution. Test your program
using randomly generated data.
E4.6
Implement the Gibbs sampler for sampling from the posterior distribution
(4.26), using the method described in Section 4.4.2.
E4.7
Write a program to implement the Gibbs sampler for the Ising model from
algorithm 4.31. Test your program by generating samples from the Ising
model for β = 0.30, β = 0.34, β = 0.38, β = 0.42, β = 0.46 and β = 0.50.
E4.8
Write a program to generate samples from the posterior distribution pX|Y
in the Bayesian image denoising problem from equation (4.36), given a
noisy input image y ∈RI. The program should follow the steps described in
algorithm 4.32.
E4.9
Write a program to implement the RJMCMC method described in Section
4.5.2.

5
Beyond Monte Carlo
In this chapter we present two methods which can be used instead of Monte Carlo
methods if either no statistical model is available or if the available models are too
complicated to easily apply Monte Carlo methods. The ﬁrst of these two methods,
called Approximate Bayesian Computation (ABC), is tailored towards the case where
a computer model of the studied system is available, which can be used instead of a
mathematical model. The second method, Bootstrap sampling, is used in cases where
only a set of observations but no model at all is available.
5.1
Approximate Bayesian Computation
ABC is a computational technique to obtain approximate parameter estimates in a
Bayesian setting. The ABC approach, described in this section, is less efﬁcient than
MCMC methods are, but ABC is easier to implement than MCMC and the method
requires very little theoretical knowledge about the underlying model.
Our aim in this section is to generate samples from the posterior distribution of
the parameter θ in a Bayesian model, given observations X = x. From Section 4.3
we know that the posterior density of θ is given by
pθ|X(θ |x) = 1
Z pX|θ

x
θ

pθ(θ),
(5.1)
where Z is a normalising constant. The problem that ABC is aiming to solve is that
in many situations the density pX|θ(·|θ) can be difﬁcult to obtain in closed form. The
ABC method can be applied in situations where generating samples from this density
is easier than working with the density itself. Since the resulting method avoids use
of any explicit form of this density, ABC is called a likelihood free method.
An Introduction to Statistical Computing: A Simulation-based Approach, First Edition. Jochen Voss.
© 2014 John Wiley & Sons, Ltd. Published 2014 by John Wiley & Sons, Ltd.

182
AN INTRODUCTION TO STATISTICAL COMPUTING
The ABC method is based on the basic rejection sampling algorithm 1.19: if
proposals are generated with density g and if each proposal X is accepted with prob-
ability p(X), then the accepted proposals are distributed with density proportional to
p · g. In the context of Bayesian parameter estimation, we can apply this algorithm
as follows:
(a) Generate samples θ j ∼pθ for j = 1, 2, 3, . . . .
(b) Accept each sample θ j with probability proportional to pX|θ(x|θ j), where x
is the observed data.
By Proposition 1.20, the accepted samples are distributed with density p(θ |x)
given by (5.1). Methods based on this idea are called ABC methods. Since we assume
that we do not have access to an explicit formula for p(x|θ), our aim is to ﬁnd a
method implementing the rejection procedure described above, using only samples
from the density p(x|θ) but not the density itself.
5.1.1
Basic Approximate Bayesian Computation
In this section we describe a basic version of the ABC method. We start the presenta-
tion by describing the method as an algorithm, and then give the required explanations
to understand why this algorithm gives the desired result.
Algorithm 5.1
(basic Approximate Bayesian Computation)
input:
data x∗∈Rn
the prior density π for the unknown parameter θ ∈Rp
a summary statistic S: Rn →Rq
an approximation parameter δ > 0
randomness used:
samples θ j ∼pθ and X j ∼pX|θ(·|θ j) for j ∈N
output:
θ j1, θ j2, . . . approximately distributed with density pθ|X(θ |x∗)
1: s∗←S(x∗)
2: for j = 1, 2, 3, . . . do
3:
sample θ j ∼pθ(·)
4:
sample X j ∼pX|θ(·|θ j)
5:
Sj ←S(X j)
6:
if |Sj −s∗| ≤δ then
7:
output θ j
8: end if
9: end for

BEYOND MONTE CARLO
183
In the algorithm, the summary statistic S is assumed to take values in Rq. The
dimension q is typically much smaller than the dimension n of the data, and often q
equals the number p of parameters. The distance |Sj −s∗| in line 6 of the algorithm
is the Euclidean norm in Rq. Since the algorithm considers the summary statistic
s∗= S(x∗) instead of the full data, the method can only be expected to work if S(θ)
contains ‘enough’ information about θ. The optimal case for this is if S is a sufﬁcient
statistic, as described in the following deﬁnition.
Deﬁnition 5.2
A statistic S = S(X) is a sufﬁcient statistic for θ, if the conditional
distribution of X given the value S does not depend on the parameter θ.
If S is a sufﬁcient statistic, then the value S(X) contains all information from X
about θ: once the value S(X) is known, the sample does not contain any additional
information about θ. More information about sufﬁcient statistics can be found in the
literature, for example in Section 6.2 of the book by Casella and Berger (2001). If
no summary statistic is available, the algorithm can be applied with a nonsufﬁcient
statistic S, but this will introduce an additional error. To minimise this error, S should
be chosen so that it contains as much information about θ as possible.
Algorithm 5.1 involves a trade-off between speed and accuracy: if the approxi-
mation parameter δ > 0 is chosen small, the distribution of the generated samples is
closer to the exact posterior. On the other hand, if δ is chosen larger, generation of
the samples is faster. This is described in the following proposition.
Proposition 5.3
Let (θ jk)k∈N be the accepted output samples of Algorithm 5.1 for
given data x∗∈Rn and s∗= S(x∗). Then the following statements hold.
(a) Let pS|θ be the density of S(X) where X ∼pX|θ. Assume that for every θ the
function s →pS|θ(s|θ) is continuous in a neighbourhood of s∗and that pS|θ
is uniformly bounded in a neighbourhood of s∗, that is
sup
s∈Rq
|s−s∗|<ε
sup
θ∈Rp pS|θ(s|θ) < ∞
for all sufﬁciently small ε > 0. Then we have
lim
δ↓0 P

θ jk ∈A

=

Rp 1A(θ) pABC
θ|S (θ |s∗) dθ
(5.2)
where pABC
θ|S
is the probability density given by
pABC
θ|S (θ |s) ∝pS|θ(s|θ) pθ(θ)
(5.3)
for all θ ∈Rp. Thus, in the limit δ ↓0, the samples θ jk have density pABC
θ|S .

184
AN INTRODUCTION TO STATISTICAL COMPUTING
(b) If S is a sufﬁcient statistic, the limiting density pABC
θ|S
from (5.3) satisﬁes
pABC
θ|S (θ |s∗) = pθ|X(θ |x∗),
that is in the limit δ ↓0 the distribution of the output samples coincides with
the posterior distribution (5.1).
(c) The average number of proposals required to generate each output sample is
of order O(δ−q).
Proof
The proposal θ j in the algorithm has density pθ. The probability of accepting
the proposal θ j is given by
P

|s j −s∗| ≤δ
θ j

= P

|S(X j) −s∗| ≤δ
θ j

=

Bδ(s∗)
pS|θ(s j |θ j) ds j,
where Bδ(s∗) is the ball around s∗with radius δ in Rq. Thus, by proposition 1.20, the
accepted proposals θ jk have density
pδ(θ) ∝

Bδ(s∗)
pS|θ(s j |θ) ds j · pθ(θ).
As an abbreviation, we write
rδ(θ) =
1
Bδ(s∗)


Bδ(s∗)
pS|θ(s j |θ) ds j · pθ(θ)
(5.4)
where
Bδ(s∗)
 denotes the q-dimensional volume of the ball Bδ(s∗). Using this
notation we have pδ(θ) = rδ(θ)/Zδ. Similarly, we write
r(θ) = pS|θ(s∗|θ) pθ(θ)
to get
pABC
θ|S (θ |s∗) = r(θ)/Z
where Z
is the normalisation constant. Since
s →pS|θ(s|θ) is continuous, we have
1
Bδ(s∗)


Bδ(s∗)
pS|θ(s j |θ j) ds j −→pS|θ(s∗|θ j)
and thus rδ(θ) →r(θ) as δ ↓0 for all θ. By the dominated convergence theorem
from analysis (see e.g. Rudin, 1987, theorem 1.34) we then have
lim
δ↓0 Zδ = lim
δ↓0

Rp rδ(θ) dθ =

Rp r(θ) dθ = Z
and thus
pδ(θ) = 1
Zδ
rδ(θ) −→1
Z r(θ) = pABC
θ|S (θ |s∗).

BEYOND MONTE CARLO
185
Consequently, the density of the accepted samples θ jk converges to pABC
θ|S (θ |s∗) as
δ ↓0 for every θ ∈Rp. By Scheff´e’s lemma (Scheff´e, 1947), this implies the con-
vergence of probabilities in (5.2) and thus the ﬁrst statement of the proposition is
proved.
For the second part of the proposition, we assume that S is a sufﬁcient statistic.
By deﬁnition 5.2 of sufﬁciency we have pX|S,θ(x|s, θ) = pX|S(x|s). Using this fact,
and the fact that S(X) is a deterministic function of X, we get
pX|θ(x|θ) = pX,S|θ

x, S(x)
θ

= pX|S,θ

x
 S(x), θ

· pS|θ

S(x)
θ

= pX|S

x
 S(x)

· pS|θ

S(x)
θ

.
Similarly, we ﬁnd
pX(x) = pX,S

x, S(x)

= pX|S

x
 S(x)

· pS

S(x)

.
Using these two relations, we get
pθ|X(θ |x) = pX|θ(x|θ) · pθ(θ)
pX(x)
= pX|S

x
 S(x)

pS|θ

S(x)
θ

· pθ(θ)
pX|S

x
 S(x)

pS

S(x)

= pS|θ

S(x)
θ

pθ(θ)
pS

S(x)

= pABC
θ|S

θ
 S(x)

.
This completes the proof of the second statement.
Finally, using equation (5.4) and the deﬁnition of Zδ from the ﬁrst part of the
proof, we have
P

|Sj −s∗| ≤δ

=

Rp

Bδ(s∗)
pS|θ(s j |θ) ds j pθ(θ) dθ
=
Bδ(s∗)


Rp rδ(θ) dθ
=
Bδ(s∗)
Zδ.
Thus, the number Nδ of proposals required to generate one sample is geometrically
distributed with mean
E(Nδ) =
1
P

|Sj −s∗| ≤δ
 =
1
Bδ(s∗)
Zδ
=
1
δqB1(s∗)
Zδ
.

186
AN INTRODUCTION TO STATISTICAL COMPUTING
Since Zδ →Z as δ ↓0, we ﬁnd
lim
δ↓0
E(Nδ)
δ−q
=
1
B1(s∗)
Z
and consequently E(Nδ) = O(δ−q). This completes the proof.
To illustrate use of the ABC method, we consider a very simple example. In the
example, use of ABC is not really necessary, but the simplicity of the situation helps
to illustrate the method.
Example 5.4
For the prior let μ ∼U[−10, 10] and σ ∼Exp(1), independently,
and set θ = (μ, σ) ∈R2. Assume that the data consist of i.i.d. values X1, . . . , Xn ∼
N(μ, σ 2) and that we have observed values x = (x1, . . . , xn) for the data. Our aim
is to generate samples from the posterior distribution of θ for given observations x.
To apply the ABC method, we ﬁrst have to choose a summary statistic S. From
the second statement of proposition 5.3 we know that it is best to choose a sufﬁcient
statistic. In the simple case considered here, the statistic
S(x) =

1
n
n

i=1
xi, 1
n
n

i=1
x2
i

.
is known to be sufﬁcient for θ = (μ, σ) (see, e.g. Casella and Berger, 2001, example
6.2.9). Denote the components of S(x) where x is the observed data by s∗
1 and s∗
2.
Then we can use ABC with this summary statistic, to generate posterior samples as
follows:
(a) Sample μ j ∼U[−10, 10] and σ j ∼Exp(1) independently.
(b) Sample X j,1, . . . , X j,n ∼N(μ j, σ 2
j ) i.i.d.
(c) Let s j,1 = 1
n
	n
i=1 X j,i and s j,2 = 1
n
	n
i=1 X2
j,i.
(d) Accept θ j = (μ j, σ 2
j ) if (si,1 −s∗
1)2 + (s j,2 −s∗
2)2 ≤δ2.
In this example, the posterior density can easily be computed explicitly, and thus
methods like MCMC could be applied in this situation. But even here, implementation
of ABC is easier than implementation of MCMC, since we do not need to perform
the calculation to obtain the formula for the posterior density. The power of ABC
lies in the fact that we can use the method without evaluating the posterior density
(except possibly to verify the assumptions of proposition 5.3) and thus ABC can be
applied in more general situations than MCMC.
The results of a simulation with n = 20 and s∗= (6.989, 52.247) are shown in
Figure 5.1. Figure 5.1(a) corresponds to δ = 0.05 (generating one sample on average
took 60 549 proposals), Figure 5.1(b) corresponds to δ = 0.15 (6022 proposals per
sample) and Figure 5.1(c) corresponds to δ = 0.25 (1059 proposals per sample).

BEYOND MONTE CARLO
187
From algorithm 5.1 is is clear that in the limit δ →∞the distribution of the
output converges to the prior distribution: in the algorithm, the proposals θ j are
generated from the prior density pθ and if δ is large most or even all of the generated
samples are output without any change. This effect is clearly visible in the right-hand
column of Figure 5.1: as δ increases, the histograms get closer and closer to the
Exp(1)-distribution used as the prior for σ.
5
6
7
8
9
0.0
0.5
1.0
1.5
0
1
2
3
4
0.0
0.5
1.0
1.5
5
6
7
8
9
0.0
0.5
1.0
1.5
0
1
2
3
4
0.0
0.5
1.0
1.5
μ
5
6
7
8
9
0.0
0.5
1.0
1.5
σ
0
1
2
3
4
0.0
0.5
1.0
1.5
(a)
(b)
(c)
Figure 5.1
The effect of the approximation parameter δ in the ABC method from
example 5.4. The histograms show the distribution of the ABC samples, the solid
lines give the exact posterior density of μ and σ. The rows correspond to different
values of δ: (a) δ = 0.05; (b) δ = 0.15; δ = 0.25; The ﬁgure clearly shows that the
distribution of the ABC samples gets more accurate as δ decreases (from bottom to
top). On the other hand, computational cost increases with accuracy.

188
AN INTRODUCTION TO STATISTICAL COMPUTING
In practical applications it is important to scale the components of the summary
statistic S so that they are all approximately of the same order of magnitude. For the
generated proposals, the range of values observed for each of the components of S
must have width bigger than δ, otherwise a situation can arise where for a given δ
only the largest component of the summary statistic has an appreciable effect on the
decision whether to accept a sample and the other components of S are effectively
ignored. The required scaling causes no problems, since a rescaled version (and
indeed any bijective image) of a sufﬁcient statistic is again sufﬁcient.
5.1.2
Approximate Bayesian Computation with regression
The basic ABC method as described in the previous section can be computationally
very expensive. Many variants of ABC, aiming to reduce the computational cost, are
used in application areas. In this section we describe one approach to constructing such
improved variants of ABC. This approach is based on the idea of accepting a larger
proportion of the samples and then to numerically compensate for the systematic
error introduced by the discrepancy between the sampled values s j = S(X j) and the
observed value s∗= S(x∗).
The method discussed here is based on the assumption that the samples θ j can be
written as
θ j ≈f (Sj) + ε j
for all accepted j, where f (s) = E(θ |S = s) and the ε j are independent of each other
and of the Sj. If this relation holds at least approximately, we can use the modiﬁed
samples
˜θ j = f (s∗) + ε j
= f (Sj) + ε j + f (s∗) −f (Sj)
(5.5)
= θ j + f (s∗) −f (Sj)
instead of θ j in order to transform samples corresponding to S = Sj into samples
corresponding to the required value S = s∗. This idea is made more rigorous by the
following result.
Lemma 5.5
Let ε ∈Rp and X ∈Rn be independent random variables. Furthermore,
let S: Rn →Rq and f : Rq →Rp be functions and deﬁne θ ∈Rp by
θ = f

S(X)

+ ε.
(5.6)
Then the following statements hold:
(a) S is a sufﬁcient statistic for θ.
(b) Let s∗∈Rq. Then the conditional distribution of θ, given S(X) = s∗, coin-
cides with the distribution of ˜θ = θ + f (s∗) −f (S(X)).

BEYOND MONTE CARLO
189
Proof
For the proof we restrict ourselves to the case where the distributions of X
and ε have densities. In this case, since θ can be written in the form (5.6), the pair
(X, θ) has joint density
pX,θ(x, θ) = pX(x) · pε

θ −f

S(x)

.
Consequently, the conditional density of X given θ is
pX|θ(x|θ) = pX,θ(x, θ)
pθ(θ)
= pX(x) ·
pε

θ −f

S(x)

pθ(θ)
.
Thus, since the ﬁrst term is independent of θ and the second term depends on x only
via S(x), we can use the factorisation theorem for sufﬁcient statistics (see e.g. Casella
and Berger, 2001, theorem 6.2.6) to conclude that S is sufﬁcient for θ. This proves
the ﬁrst statement of the lemma.
The second statement is a consequence of the independence between X and ε: the
expression f (S(X)), conditioned on S(X) = s∗is the constant value f (s∗) and, since
ε is independent of X, conditioning on S(X) does not change the distribution of ε.
Thus, the conditional distribution of θ = f (S(X)) + ε coincides with the distribution
of ˜θ = f (s∗) + ε. Substituting ε = θ −f (S(X)) into this expression completes the
proof.
Since we apply the correction only to samples which are accepted in the basic
ABC algorithm, that is where s j is close to s∗, we only need to consider f in a
neighbourhood of s. If f is smooth and δ is chosen small enough, then f will be
approximately afﬁne. For this reason, the usual approach to modelling f is to assume
that near s∗the function f is an afﬁne function of the form
f (s) = α + s⊤β
for all s ∈Rq, where α ∈Rp and β ∈Rq×p. In this case, estimates ˆα and ˆβ for the
parameters α and β can be computed from the values (s j, θ j), using the least squares
method, that is by ﬁnding ˆα and ˆβ which minimise the residual sum of squares
r(α, β) =
N

j=1
θ j −f (s j)
2 =
N

j=1
θ j −α −s⊤
j β
2.
To compute the solution of the multidimensional least-squares problem, we
rewritetheproblem inmatrixnotation. Usingthevectors s j ∈Rq for j = 1, 2, . . . , N,
we deﬁne a matrix S ∈RN×(q+1) by
Sji =
(s j)i
if i ∈{1, 2, . . . , q} and
1
if i = q + 1
(5.7)

190
AN INTRODUCTION TO STATISTICAL COMPUTING
for all j ∈{1, 2, . . . , N} and i ∈{1, 2, . . . , q + 1}. Using the coefﬁcients α ∈Rp
and β ∈Rq×p, we deﬁne a matrix B ∈R(q+1)×p by
Bik =
βik
if i ∈{1, 2, . . . , q} and
αk
if i = q + 1
(5.8)
for all i ∈{1, 2, . . . , q + 1} and k ∈{1, 2, . . . , p}. Finally, we collect the vectors θ j
for j = 1, 2, . . . , N in a matrix 	 ∈RN×p, given by
	 jk = (θ j)k
(5.9)
for all j ∈{1, 2, . . . , N} and k ∈{1, 2, . . . , p}. Using this notation, we have f (s j)k =
(SB) jk where SB is the matrix product of S and B (Figure 5.2), and the residual sum
of squares can be rewritten as
r(α, β) =
n

j=1
p

k=1
(	 −SB)2
jk.
The matrices 	 and S only depend on the given data while the matrix B collects the
entries of the unknowns α and β as described in (5.8). Using the standard theory for
multidimensional least squares, the least squares estimate for B can be found as
ˆB = (S⊤S)−1S⊤	,
θ1
θ2
...
θN
Θ
=
s1
1
s2
1
...
...
sN
1
S
·
α
β
B
+
ε1
ε2
...
εN
ε
Figure 5.2
Illustration of the matrices 	, S and B introduced in Section 5.1.2. The
matrices 	 and S are given, and the aim is to estimate B (and thus α and β) using
the least squares method.

BEYOND MONTE CARLO
191
where S⊤is the transposed matrix and (S⊤S)−1 is the inverse of S⊤S. Finally, the
estimates for α and β can be extracted from ˆB using (5.8), that is by deﬁning
ˆαk = ˆBq+1,k and
ˆβik = ˆBik
(5.10)
for all j ∈{1, 2, . . . , N} and k ∈{1, 2, . . . , p}. Substituting the result of the least
squares estimation into (5.5), the modiﬁed samples are then
˜θ j = θ j + (s∗−s j)⊤ˆβ.
Since f (s) ≈ˆα + s⊤ˆβ, we can use lemma 5.5 to conclude that the distribution of the
resulting samples ˜θ j approximately coincides with the conditional distribution of θ
given S = s∗. The resulting algorithm follows.
Algorithm 5.6
(Approximate Bayesian Computation with regression)
input:
data x ∈Rn
the prior density π for the unknown parameter θ ∈Rp
a summary statistic S: Rn →Rq
an approximation parameter δ > 0
the output sample size N
randomness used:
samples θ j ∼pθ and X j ∼pX|θ(·|θ j) for j ∈N
output:
˜θ1, . . . , ˜θN, approximately distributed with density pθ|X(θ |x)
1: s∗←S(x∗)
2: Use basic ABC to get samples θ1, . . . , θN and s1, . . . , sN.
3: Compute the matrix S ∈RN×(q+1) given by (5.7).
4: Compute C = S⊤	 ∈R(q+1)×p, where 	 is given by (5.9).
5: Compute the matrix S⊤S ∈R(q+1)×(q+1).
6: Solve the system of linear equations (S⊤S) ˆB = C, to ﬁnd the matrix
ˆB ∈R(q+1)×p.
7: Construct ˆβ from B, using (5.10).
8: for j = 1, 2, . . . , N do
9:
˜θ j ←θ j + (s∗−s j)⊤ˆβ
10: end for
The correction of samples implemented by this algorithm allows to use larger
values of δ, resulting in a more efﬁcient method, while still getting reason-
ably accurately distributed samples. This is illustrated by the difference between
Figure 5.1 (for basic ABC) and Figure 5.3 (for ABC with regression): while the
distribution of samples generated by basic ABC for δ = 0.25 in Figure 5.1(c) is very
different from the exact distribution (shown as solid lines), the distribution of samples

192
AN INTRODUCTION TO STATISTICAL COMPUTING
μ
5
6
7
8
9
0.0
0.5
1.0
1.5
σ
0
1
2
3
4
0.0
0.5
1.0
1.5
Figure 5.3
The improvements in the distribution of ABC samples which can be
achieved by using algorithm 5.6 instead of algorithm 5.1 in the situation of example
5.4. The value of δ = 0.25 used for this ﬁgure is the same as for Figure 5.1(c).
Comparing this ﬁgure with Figure 5.1(c) shows that ABC with regression gives
samples with a much improved distribution.
obtained from ABC with regression for δ = 0.25 in Figure 5.3 is reasonably close to
the exact distribution.
5.2
Resampling methods
Resampling methods are applied in situations where a sample of data is available for a
stochastic system, but where no adequate model is available to describe the data. This
can be the case, for example when the data are obtained by physical measurements
of the state of a complex system. In such situations, Monte Carlo methods cannot be
applied directly, because generation of samples requires a statistical model.
The methods described in this section are based on the idea of replacing the sam-
ples in Monte Carlo methods with values picked at random from the available sample
of data. This procedure of reusing the data for sampling is called resampling and the
resulting methods are called resampling methods. While such methods will typically
be less accurate than Monte Carlo methods, their advantages are the simplicity of the
resulting algorithms and the wide applicability of these methods.
5.2.1
Bootstrap estimates
The basis of all resampling methods is to replace the distribution given by a model
with the ‘empirical distribution’ of the given data, as described in the following
deﬁnition.
Deﬁnition 5.7
Given a sequence x = (x1, x2, . . . , xM), the distribution of X∗= xK,
where the index K is random and uniformly distributed on the set {1, 2, . . . , M}, is
called the empirical distribution of the xi. In this chapter we denote the empirical
distribution of x by P∗
x .

BEYOND MONTE CARLO
193
In the deﬁnition, the vector x is assumed to be ﬁxed. The randomness in X∗stems
from the choice of a random element, with index K ∼U{1, , 2, . . . , M}, of this ﬁxed
sequence. Computational methods which are based on the idea of approximating an
unknown ‘true’ distribution by an empirical distribution are called bootstrap methods.
Assume that X∗is distributed according to the empirical distribution P∗
x . Then
we have
P(X∗= a) = 1
M
M

i=1
1{a}(xi),
that is under the empirical distribution, the probability that X∗equals a is given by
the relative frequency of occurrences of a in the given data. Similarly, we have the
relations
P

X∗∈A

= 1
M
M

i=1
1A(xi)
and
E

f (X∗)

=

a∈{x1,...,xM}
f (a)P

X∗= a

=

a∈{x1,...,xM}
f (a) 1
M
M

i=1
1{a}(xi) = 1
M
M

i=1
f (xi).
(5.11)
Some care is needed when verifying this relation: the sums where the index a runs
over the set {x1, . . . , xM} have only one term for each element of the set, even if the
corresponding value occurs repeatedly in the given data.
Example 5.8
Let the data x = (1, 2, 1, 4) be given and let X∗be distributed accord-
ing to the empirical distribution of x. Then we have the probabilities P(X∗= 1) =
2/4 = 1/2 and P(X∗= 2) = P(X∗= 4) = 1/4. The expectation of X∗is
E(X∗) = 1 · 2
4 + 2 · 1
4 + 4 · 1
4 = 2 + 2 + 4
4
= 2.
In the context of resampling methods, the data X = (X1, X2, . . . , X M) is assumed
to be an i.i.d. sample from some distribution P unknown to us. When considering
X∗∼P∗
X in this situation, there are two different levels of randomness involved:
ﬁrst, the data X and thus the empirical distribution P∗
X is random. And secondly,
for every instance of the data X, the sample X∗contains the additional randomness
from the random choice of the index K in deﬁnition 5.7. In this section we write
P∗
X(·) and E∗
X(·) for probabilities and expectations which take X as being ﬁxed and
which only consider the randomness from generating X∗∼P∗
X. Technically, P∗
X is

194
AN INTRODUCTION TO STATISTICAL COMPUTING
the conditional probability and E∗
X is the conditional expectation, both conditioned
on the value of X. The following lemma shows how these two different levels of
randomness combine to create the total variance.
Lemma 5.9
Let X = (X1, . . . , X M) where the Xi ∼P are i.i.d. Furthermore, let
X∗
1, . . . , X∗
n ∼P∗
X be i.i.d. and Y ∗= f (X∗
1, . . . , X∗
n) for some function f . Then
E(Y ∗) = E

E∗
X(Y ∗)

(5.12)
and
Var(Y ∗) = E

Var∗
X(Y ∗)

+ Var

E∗
X(Y ∗)

.
(5.13)
Proof
Since we have E∗
X(Y ∗) = E(Y ∗|X), we can use the tower property
E(E(Y |X)) = E(Y) of the conditional expectation to get (5.12). For the variance
(5.13) we ﬁnd
Var(Y ∗) = E

(Y ∗)2
−E(Y ∗)2
= E

E((Y ∗)2|X)

−E

E(Y ∗|X)
2
= E

E((Y ∗)2|X) −E(Y ∗|X)2
+ E

E(Y ∗|X)2
−E

E(Y ∗|X)
2
= E

Var(Y ∗|X)

+ Var

E(Y ∗|X)

= E

Var∗
X(Y ∗)

+ Var

E∗
X(Y ∗)

.
This completes the proof.
If we consider the data x = (x1, . . . , xM) to be an instance of an i.i.d. sample
from the distribution P, and if we let X∗∼P∗
x as well as X ∼P, then the law of
large numbers (theorem A.8) guarantees
P∗
x (X∗∈A) = 1
M
M

i=1
1A(xi) ≈P(X ∈A)
(5.14)
for every set A and sufﬁciently large M. Similarly, we ﬁnd
E∗
x

f (X∗)

= 1
M
M

i=1
f (Xi) ≈E

f (X)

for all functions f . These two equations show that the distribution of the random
variable X∗(i.e. the empirical distribution P∗
x ) can be used as an approximation to
the distribution of X (i.e. the unknown distribution P).

BEYOND MONTE CARLO
195
In typical applications of the bootstrap method, the function f depends on
n independent values, that is we want to compute an expectation of the form
E( f (X1, . . . , Xn)) where X1, . . . , Xn are i.i.d., distributed with an unknown dis-
tribution P. In these cases, we can use the approximation
E∗
x

f (X∗
1, . . . , X∗
n)

≈E

f (X1, . . . , Xn)

,
(5.15)
where X∗
1, . . . , X∗
n are independent samples from the empirical distribution P∗
x .
Finally, if we are interested in any property θ = θ(P) of the unknown distribution
P we can approximate the result by considering the same property of the empirical
distribution P∗
x :
θ(P) ≈θ(P∗
x ).
(5.16)
For example, if σ 2(P) is the variance of the distribution P (or equivalently, of X ∼P),
then we can use the approximation
σ 2(P) ≈σ 2(P∗
x ) = Var∗
x(X∗)
where X∗∼P∗
x . Since the distribution P∗
x of X∗is known, the value Var∗
x(X∗) can
be computed from the given data x.
Equation (5.14), equation (5.15) and equation (5.16) illustrate a general method
for constructing estimators from given data x. The method is based on two different
(but related) substitutions:
(a) Every direct occurrence of the unknown distribution P is replaced with the
empirical distribution P∗
x .
(b) All instances of random variables X ∼P are replaced with bootstrap samples
X∗∼P∗
x .
The examples above shows only one of the two substitutions at a time, but
in the next section we will see an application where both substitutions occur
simultaneously. Estimates based on this methodology are called bootstrap esti-
mates and samples (X∗
1, . . . , X∗
n) from the empirical distribution are called bootstrap
samples.
Example 5.10
Let X1, X2, . . . , Xn be i.i.d. with P(Xi = 1) = p and P(Xi = 0) =
1 −p. Assume that the value of p is unknown to us, but that we have a sample x =
(x1, x2, . . . , xn) from the distribution of the Xi. Since we do not know the value of p,
we do not know the distribution P of the random variables Xi. As we have seen above,
the empirical distribution P∗
x can be used to approximate the unknown distribution P:
let X∗∼P∗
x and k = 	n
i=1 xi. Since X∗is chosen uniformly from the entries of x, we
have P(X∗= 1) = k/n and P(X∗= 0) = 1 −k/n. A bootstrap sample X∗
1, . . . , X∗
n
can be constructed by generating n i.i.d. samples from the distribution P∗
x . Expressions

196
AN INTRODUCTION TO STATISTICAL COMPUTING
involving Xi can be approximated using the corresponding expression for X∗
i . For
example, using (5.14), we ﬁnd the bootstrap estimate for P(	n
i=1 Xi ≤a) as
P
 n

i=1
Xi ≤a

≈P∗
x
 n

i=1
X∗
i ≤a

=
a

j=1
P∗
x
 n

i=1
X∗
i = j

=
a

j=1
n
j
 k
n
 j 
1 −k
n
n−j
for all a ∈N. This result is not surprising: the bootstrap estimate equals the exact
probability where p is replaced by the estimate k/n.
The difﬁculty in bootstrap methods lies in evaluating the expressions involving
the empirical distribution P∗
x in (5.14), (5.15) and (5.16). In simple cases, such as in
example 5.10, this can be done analytically but in most situations an analytical solution
will not be available. Since generating samples from the empirical distribution is easy,
Monte Carlo methods can be used instead. For example, in the case of approximation
(5.15) we can generate independent samples X∗( j)
i
∼P∗
x for i = 1, 2, . . . , n and j =
1, 2, . . . , N, and then compute the approximation
E∗
x

f (X∗
1, . . . , X∗
n)

≈1
N
N

j=1
f

X∗( j)
1
, . . . , X∗( j)
n

.
(5.17)
This method leads to the following algorithm.
Algorithm 5.11
(general bootstrap estimate)
input:
data x1, x2, . . . , xM ∈A with values in some set A
f : An →R
N ∈N
randomness used:
a sequence (K ( j)
i ) j∈N, i=1,...,n with K ( j)
i
∼U{1, 2, . . . , M} i.i.d.
output:
an estimate for E( f (X1, . . . , Xn)) where X1, . . . , Xn are i.i.d. from the
distribution of the data xi
distribution of the data xi
1: s ←0
2: for j = 1, 2, . . . , N do
3:
generate K ( j)
1 , . . . , K ( j)
n
∼U{1, 2, . . . , M} i.i.d.

BEYOND MONTE CARLO
197
4:
let X∗( j)
i
←xK ( j)
i
for i = 1, 2, . . . , n
5:
s ←s + f (X∗( j)
1
, . . . , X∗( j)
n
)
6: end for
7: return s/N
As explained above, this algorithm is based on the following sequence of approx-
imations:
1
N
N

j=1
f

X∗( j)
1
, . . . , X∗( j)
n

≈E∗
x

f (X∗
1, . . . , X∗
n)

≈E

f (X1, . . . , Xn)

.
The error in the ﬁrst of these two approximations goes to 0 as N increases. Since we
control generation of the samples X∗( j), we can make this error arbitrarily small at
the expense of additional computation time. The error in the second approximation
decreases as M →∞. Normally, the given data set x1, x2, . . . , xM is ﬁxed and cannot
be easily extended; in these cases there is no way to reduce the error in the second
approximation.
Proving the convergence
E∗
x

f (X∗
1, . . . , X∗
n)

→E

f (X1, . . . , Xn)

for M →∞is challenging in the general setting and results depend on regularity
properties of the function f . The ususal approach is to prove convergence only for the
speciﬁc forms of f arising in applications, instead of relying on general theorems.
In most applications, the number M of available samples equals the number n
of arguments of the function f , that is enough data are given to compute one value
f (X1, . . . , Xn). The bootstrap estimate computed by algorithm 5.11 allows to reuse
the available information to also get an estimate for the expectation of this value.
Some care is needed when using bootstrap estimates in practice: while
the arguments X∗
1, . . . , X∗
n of f in Equation (5.15) have (approximately) the correct
distribution, with large probability the bootstrap sample contains duplicate values and
thus the arguments are not independent. On the other hand, the arguments X1, . . . , Xn
on the right-hand side of (5.15) are independent. For this reason, the bootstrap esti-
mate (5.17) is usually biased (this is in contrast to the Monte Carlo estimate, which
by proposition 3.14 is always unbiased) and sometimes it is not even clear that the
total error decreases to 0 as the value of n increases.
5.2.2
Applications to statistical inference
The main application of the bootstrap method in statistical inference is to quantify
the accuracy of parameter estimates.

198
AN INTRODUCTION TO STATISTICAL COMPUTING
In this section, we will consider parameters as a function of the corresponding
distribution: if θ is a parameter, for example the mean or the variance, then we
write θ(P) for the corresponding parameter. In statistics, there are many ways of
constructing estimators for a parameter θ. One general method for constructing
parameter estimators, the plug-in principle, is given in the following deﬁnition.
Deﬁnition 5.12
Consider an estimator ˆθn = ˆθn(X1, . . . , Xn) for a parameter θ(P).
The estimator ˆθn satisﬁes the plug-in principle, if it satisﬁes the relation
ˆθn(x1, . . . , xn) = θ(P∗
x ),
(5.18)
for all x = (x1, . . . , xn), where P∗
x is the empirical distribution of x. In this case, ˆθn
is called the plug-in estimator for θ.
Since the idea of bootstrap methods is to approximate the distribution P by the
empirical distribution P∗
x , plug-in estimators are particularly useful in conjunction
with bootstrap methods.
Example 5.13
The plug-in estimator for the mean μ is found by taking the mean
of the empirical distribution. Taking X∗∼P∗
x we get
ˆμ(x1, . . . , xn) = μ

P∗
x

= E

X∗
and using equation (5.11), with M = n and f (x) = x, we ﬁnd
ˆμ(x1, . . . , xn) = 1
n
n

i=1
xi.
Thus, the plug-in estimator for the mean is just the sample average.
Example 5.14
The plug-in estimator for the variance σ 2 is the variance of the
empirical distribution. Taking X∗∼P∗
x again, we get
ˆσ 2(x1, . . . , xn) = σ 2
P∗
x

= Var

X∗
= E

(X∗)2
−

E(X∗)
2.
From equation (5.11) we ﬁnd
ˆσ 2(x1, . . . , xn) = 1
n
n

i=1
x2
i −

1
n
n

i=1
xi
2
,

BEYOND MONTE CARLO
199
and using the abbreviation ¯x = 1
n
	n
i=1 xi we can rewrite this as
ˆσ 2(x1, . . . , xn) = 1
n
n

i=1
x2
i −¯x2
= 1
n
 n

i=1
x2
i −2
n

i=1
xi ¯x +
n

i=1
¯x2

= 1
n
n

i=1
(xi −¯x)2.
This is the plug-in estimator for the variance.
5.2.2.1
Bootstrap estimates of the bias
The bias, as given in deﬁnition 3.9, is a measure for the systematic error of an
estimator. In the notation of this section, the bias of an estimator ˆθn for a parameter θ
is given by
bias( ˆθn) = EP( ˆθn) −θ(P),
where EP(·) denotes the expectation with respect to the distribution P. If the distri-
bution P is not known, we cannot compute the bias analytically but for a given i.i.d.
sample X = (X1, . . . , Xn) from the distribution P we can ﬁnd the bootstrap estimate
for the bias by applying the bootstrap principle. This results in the estimator

bias
∗( ˆθn) = E∗
X( ˆθ∗
n ) −θ(P∗
X)
(5.19)
for the bias, where
ˆθ∗
n = ˆθn

X∗
1, . . . , X∗
n

(5.20)
and X∗
1, . . . , X∗
n ∼P∗
X are i.i.d. As before, we write E∗
X(·) to indicate that the expec-
tation takes X1, . . . , Xn as ﬁxed and only considers the additional randomness in
X∗
1, . . . , X∗
n.
Since, for given X1, . . . , Xn, the distribution P∗
X is known, we can evaluate
the right-hand side of (5.19): If we assume that ˆθn satisﬁes the plug-in principle,
then, by equation (5.18), we have θ(P∗
X) = ˆθn(X1, . . . , Xn). For the evaluation of
E∗( ˆθ∗
n ) we can use a Monte Carlo estimate: we generate samples X∗( j)
i
∼P∗
x i.i.d.
for i = 1, . . . , n and j = 1, 2, . . . , N and let
ˆθ∗( j)
n
= ˆθn

X∗( j)
1
, . . . , X∗( j)
n

(5.21)

200
AN INTRODUCTION TO STATISTICAL COMPUTING
as well as
ˆθ∗n = 1
N
N

j=1
ˆθ∗( j)
n
.
(5.22)
Finally, for sufﬁciently large N, we can use the approximation ˆθ∗n ≈E∗( ˆθ∗
n ). Substi-
tuting these expressions into equation (5.19), we get

bias
∗( ˆθn) ≈ˆθ∗n −ˆθn(X1, . . . , Xn).
This is the usual form for the bootstrap estimate of the bias of the estimator ˆθn.
Some care is needed here, since there are two different levels of estimation
involved: ˆθn is an estimator for θ(P) while 
bias
∗is an estimate for the bias of the
estimate ˆθn.
Algorithm 5.15
(bootstrap estimate of the bias)
input:
data x1, x2, . . . , xn
the plug-in estimator ˆθn for a parameter θ
N ∈N
randomness used:
K ( j)
i
∼U{1, 2, . . . , n} i.i.d. for i = 1, . . . , n and j = 1, . . . , N
output:
an estimate for the bias of ˆθn
1: for j=1, 2, . . . , N do
2:
generate K ( j)
1 , . . . , K ( j)
n
∼U{1, 2, . . . , n} i.i.d.
3:
let X∗( j)
i
←xK ( j)
i
for i = 1, 2, . . . , n
4:
let ˆθ∗( j)
n
←ˆθn(X∗( j)
1
, . . . , X∗( j)
n
)
5: end for
6: let ˆθ∗n = 1
N
N

j=1
ˆθ∗( j)
n
7: return ˆθ∗n −ˆθn(X1, . . . , Xn)
Both, the accuracy and the computational cost of this estimate increase when the
size n of the given data set and the Monte Carlo sample size N increase.
If ˆθn satisﬁes the plug-in principle, an elegant alternative formula for 
bias∗can
be derived. From (5.18) we know
ˆθn

X∗
1, . . . , X∗
n

= θ

P∗
X∗


BEYOND MONTE CARLO
201
and substituting this relation into Equation (5.19) we get

bias
∗( ˆθn) = E∗
X

θ

P∗
X∗

−θ

P∗
X

.
(5.23)
On the other hand, substituting the deﬁnition (5.18) of a plug-in estimate into the
deﬁnition of the bias gives
bias( ˆθn) = EP

θ(P∗
X)) −θ(P).
(5.24)
Comparing the expressions in (5.23) and (5.24) we ﬁnd that the bootstrap estimate of
the bias makes use of the fact that, in some sense, P∗
X∗relates to P∗
X, as P∗
X does to P.
5.2.2.2
Bootstrap estimates of the standard error
The standard error of an estimator ˆθn for a parameter θ(P), introduced in deﬁ-
nition 3.10, is the standard deviation of ˆθn(X1, . . . , Xn) when the random sample
X1, . . . , Xn is distributed according to the distribution P:
se( ˆθn) = stdev
 ˆθn(X1, . . . , Xn)

.
(5.25)
Together with the bias, the standard error is a measure for the accuracy of an estimator.
In this section we will see how the bootstrap method can be used to obtain an estimate
of the standard error of a given estimator.
Example 5.16
Consider the estimator ˆμ(x) = 1
n
	n
i=1 xi for the mean μ of a
distribution. The standard error of this estimator is given by
se( ˆμn) =



Var

1
n
n

i=1
Xi

= σ(P)
√n ,
where X1, . . . , Xn ∼P are i.i.d. and σ(P) denotes the standard deviation of the
distribution P.
The value se( ˆθn) does not directly depend on P, but only on the values Xi.
Consequently, for ﬁnding the bootstrap estimate of the standard error, it is sufﬁ-
cient to replace the random variables X1, . . . , Xn ∼P in (5.25) with i.i.d. samples
X∗
1, . . . , X∗
n ∼P∗
X from the empirical distribution. The resulting estimate is
se∗( ˆθn) = stdev∗ ˆθ∗
n

,
(5.26)
where
ˆθ∗
n = ˆθn

X∗
1, . . . , X∗
n


202
AN INTRODUCTION TO STATISTICAL COMPUTING
and the symbol stdev∗(·) indicates the standard deviation for ﬁxed values of
X1, . . . , Xn, taking only the randomness in the X∗
i into account. In contrast to the
bootstrap estimate of the bias from the previous section, for the bootstrap estimate of
the standard error we do not need to assume that the estimator ˆθn satisﬁes the plug-in
principle.
Example 5.17
Consider the estimate ˆμn(x) = 	n
i=1 xi/n for the mean. From
example 5.16 we know that the standard error of the mean is se( ˆμn) = σ(P)/√n.
Instead of using the general formula (5.26), we can apply the bootstrap principle to
se( ˆμn) and ﬁnd the bootstrap estimate for the standard error of ˆμn as
se∗( ˆμn) = σ(P∗
X)
√n .
Using the result from example 5.14 we get
se∗( ˆμn) =

1
n
	n
i=1

Xi −¯X
2
√n
=



 1
n2
n

i=1

Xi −¯X
2.
This is the bootstrap estimate for the standard error of the mean.
Only in simple situations such as the one in example 5.17 is it possible to
compute the bootstrap estimate of the standard error analytically. Typically, the
standard deviation on the right-hand side of equation (5.26) cannot be evaluated
exactly and is approximated using a Monte Carlo estimate instead. This leads to the
following expression for the bootstrap estimate of the standard error:
se∗( ˆθn) ≈




1
N −1
N

j=1

ˆθ∗( j)
n
−ˆθ∗n
2
where ˆθ∗n and the ˆθ∗( j)
n
are given by equation (5.21) and equation (5.22), respectively.
This leads to the following algorithm.
Algorithm 5.18
(bootstrap estimate of the standard error)
input:
data x1, x2, . . . , xn
an estimator ˆθn for a parameter θ
N ∈N
randomness used:
K ( j)
i
∼U{1, 2, . . . , n} i.i.d. for i = 1, . . . , n and j = 1, . . . , N
output:
an estimate for the standard error of ˆθn

BEYOND MONTE CARLO
203
1: for j=1, 2, . . . , N do
2:
generate K ( j)
1 , . . . , K ( j)
n
∼U{1, 2, . . . , n} i.i.d.
3:
let X∗( j)
i
←xK ( j)
i
for i = 1, 2, . . . , n
4:
let ˆθ∗( j)
n
←ˆθn(X∗( j)
1
, . . . , X∗( j)
n
)
5: end for
6: let ˆθ∗n = 1
N
N

j=1
ˆθ∗( j)
n
7: return




1
N −1
N

j=1

ˆθ∗( j)
n
−ˆθ∗n
2
Both the accuracy and the computational cost of this estimate increase when the
size n of the given data set and the Monte Carlo sample size N increase.
5.2.2.3
Bootstrap conﬁdence intervals
Conﬁdence intervals, as introduced in Section 3.4.2, are often difﬁcult to construct
exactly. There are many different approaches to construct bootstrap approximations
for conﬁdence intervals. Here we restrict ourselves to ﬁrst derive one simple bootstrap
conﬁdence interval and to then give a very short description of a more complicated
bootstrap conﬁdence interval.
A conﬁdence interval is an interval [U, V ], computed from the given data
(X1, . . . , Xn) ∼Pθ, such that
Pθ

θ ∈[U, V ]

≥1 −α.
(5.27)
While we only require this relation to hold for the true value of θ, this value is not
known and thus U = U(X1, . . . , Xn) and V = V (X1, . . . , Xn) are constructed such
that the relation (5.27) holds for all possible values of θ simultaneously. To construct
exact conﬁdence intervals, detailed knowledge of the family of distributions Pθ is
required. In contrast, the bootstrap methods presented in this section allow to construct
approximate conﬁdence intervals, based on just the given data X1, . . . , Xn. These
methods use exactly the same information as is required for computing the point
estimate ˆθn. Thus, using bootstrap conﬁdence intervals allows information about
accuracy to be attached to any parameter estimate.
Let ˆθn be the plug-in estimate for the parameter θ = θ(Pθ). To construct a conﬁ-
dence interval, we can try to ﬁnd values a and b such that
Pθ

ˆθn −a ≤θ ≤ˆθn + b

= 1 −α.
(5.28)

204
AN INTRODUCTION TO STATISTICAL COMPUTING
Direct application of the bootstrap principle, that is replacing Pθ with P∗
X and
X1, . . . , Xn ∼Pθ with X∗
1, . . . , X∗
n ∼P∗
X, yields the condition
P∗
X

ˆθ∗
n −a ≤θ(P∗
X) ≤ˆθ∗
n + b

= 1 −α,
(5.29)
where ˆθ∗
n is deﬁned in equation (5.20) and we write P∗
X to indicate that, when
computing the probability, the data X1, . . . , Xn are assumed to be ﬁxed. We use the
relation (5.29) to construct values a and b for an approximate conﬁdence interval
for θ.
Lemma 5.19
Let ˆθn be the plug-in estimator for θ. Let θ∗
α/2 and θ∗
1−α/2 be the α/2
and 1 −α/2 quantiles of the distribution of ˆθ∗
n given by (5.20). Deﬁne
a = θ∗
1−α/2 −ˆθn

X1, . . . , Xn

and
b = ˆθn

X1, . . . , Xn

−θ∗
α/2.
Then (5.29) is satisﬁed.
Proof
Since ˆθn is the plug-in estimator for θ, we have ˆθn(X1, . . . , Xn) = θ(P∗
X).
Substituting this relation into the deﬁnitions of a and b, we get
P∗
ˆθ∗
n −a ≤θ(P∗
X) ≤ˆθ∗
n + b

= P∗
−θ∗
1−α/2 + θ(P∗
X) ≤θ(P∗
X) −ˆθ∗
n ≤θ(P∗
X) −θ∗
α/2

= P∗
θ∗
1−α/2 ≥ˆθ∗
n ≥θ∗
α/2

= 1 −α.
This completes the proof.
An approximate conﬁdence interval [U ∗, V ∗] can now be found by substituting
the values a and b from the lemma into (5.28). The resulting boundaries are
U ∗= ˆθn

X1, . . . , Xn

−a = 2 ˆθn

X1, . . . , Xn

−θ∗
1−α/2
and
V ∗= ˆθn

X1, . . . , Xn

+ b = 2 ˆθn

X1, . . . , Xn

−θ∗
α/2.
Normally, the quantiles θ∗
α/2 and θ∗
1−α/2 cannot be computed analytically and are
estimated using Monte Carlo instead. In order to obtain such estimates, we generate

BEYOND MONTE CARLO
205
N samples, sort them in increasing order, and then pick out the entries with indices
Nα/2 and N(1 −α/2), with appropriate rounding, from the sorted list. This technique
is used in the following algorithm.
Algorithm 5.20
(simple bootstrap conﬁdence interval)
input:
data x1, x2, . . . , xn
the plug-in estimator ˆθn for a one-dimensional parameter θ
N ∈N
α ∈(0, 1)
randomness used:
K ( j)
i
∼U{1, 2, . . . , n} i.i.d. for i = 1, . . . , n and j = 1, . . . , N
output:
an approximate conﬁdence interval [U ∗, V ∗] for θ
1: for j=1, 2, . . . , N do
2:
generate K ( j)
1 , . . . , K ( j)
n
∼U{1, 2, . . . , n} i.i.d.
3:
let X∗( j)
i
←xK ( j)
i
for i = 1, 2, . . . , n
4:
let ˆθ∗( j)
n
←ˆθn(X∗( j)
1
, . . . , X∗( j)
n
)
5: end for
6: let t ←ˆθn

X1, . . . , Xn)
7: let l ←
 α
2 N

8: let u ←

(1 −α
2 )N

9: let θ∗
(1), . . . , θ∗
(N) be θ∗
1 , . . . , θ∗
N, sorted in increasing order
10: return (2t −θ∗
(u), 2t −θ∗
(l))
In the algorithm, the symbol ⌈·⌉denotes the operation of rounding up a number,
that is for x ∈R the value ⌈x⌉is the smallest integer greater than or equal to x.
The parameter N in algorithm 5.20 controls the accuracy of the bootstrap esti-
mate for the quantiles of the distribution of ˆθ∗
n . For bigger values of N the result
gets more accurate but, at the same time, the computation gets slower. The situation
is slightly different from the case of Monte Carlo estimates. For the bootstrap esti-
mates considered here, the total error does not converge to 0 even as N increases
to inﬁnity, since an additional error is introduced by the ﬁnite size n of the given
data set.
One easy method of ﬁnding good values for N is to repeatedly compute a con-
ﬁdence interval for the same data set and to consider the standard deviation of
the bounds between runs of the algorithm. This standard deviation is a measure
for the error contributed by the ﬁniteness of N. The value of N should be chosen
large enough that this standard deviation is small, compared with the size of the
conﬁdence interval. Numerical experiments indicate the N should be chosen to be
relatively large, for example in the situation of exercise E5.6 with N = 4000 the sam-
pling error is still a noticeable percentage of the width of the estimated conﬁdence
interval.

206
AN INTRODUCTION TO STATISTICAL COMPUTING
Many variants of and improvements to the simple bootstrap conﬁdence interval
from algorithm 5.20 are possible. Here we restrict ourselves to describing one of the
many variants, known as the BCa method (BCa stands for ‘bias corrected and acceler-
ated’), but without giving any derivation. Using a bootstrap sample ˆθ∗(1)
n
, . . . , ˆθ∗(N)
n
,
constructed as in (5.21), the BCa conﬁdence interval is deﬁned using the following
steps.
First, let 
: R →(0, 1) be the distribution function of the standard normal distri-
bution and deﬁne a value ˆz by
ˆz = 
−1

#

j
 ˆθ∗( j)
n
≤ˆθn

N

.
Next, let x(i) = (x1, . . . , xi−1, xi+1, . . . , xn), that is the data x with xi left out, and
deﬁne θ(i) = ˆθn−1(x(i)) for i = 1, 2, . . . , n, as well as
θ(·) = 1
n
n

i=1
θ(i).
From this, a value ˆa is computed as
ˆa = 1
6 ·
	n
i=1

θ(·) −ˆθ(i)
3

	n
i=1

θ(·) −ˆθ(i)
23/2 .
Finally, using ˆz and ˆa, we deﬁne a map q: (0, 1) →(0, 1) by
q(α) = 

ˆz +
ˆz + 
−1(α)
1 −ˆa
ˆz + 
−1(α)


for all α ∈(0, 1). Then the BCa bootstrap conﬁdence interval [U ∗, V ∗] is given by
the boundaries
U ∗= θ∗
q(α/2)
and
V ∗= θ∗
q(1−α/2),
where θ∗
α denotes the α-quantile of the distribution of the ˆθ∗
n . As before, these quantiles
can be approximated by the corresponding quantiles of the empirical distribution of
ˆθ∗(1)
n
, . . . , ˆθ∗(N)
n
. This leads to the following algorithm.

BEYOND MONTE CARLO
207
Algorithm 5.21
(BCa bootstrap conﬁdence interval)
input:
data x1, x2, . . . , xn
an estimator ˆθn for a one-dimensional parameter θ
N ∈N
α ∈(0, 1)
randomness used:
K ( j)
i
∼U{1, 2, . . . , n} i.i.d. for i = 1, . . . , n and j = 1, . . . , N
output:
an approximate conﬁdence interval [U ∗, V ∗] for θ
1: for j=1, 2, . . . , N do
2:
generate K ( j)
1 , . . . , K ( j)
n
∼U{1, 2, . . . , n} i.i.d.
3:
let X∗( j)
i
←xK ( j)
i
for i = 1, 2, . . . , n
4:
let ˆθ∗( j)
n
←ˆθn(X∗( j)
1
, . . . , X∗( j)
n
)
5: end for
6: let ˆz = 
−1

#

j
 ˆθ∗( j) ≤ˆθn

N

7: for i=1, 2, . . . , n do
8:
let θ(i) ←ˆθn−1(x1, . . . , xi−1, xi+1, . . . , xn)
9: end for
10: let θ(·) ←1
n
	n
i=1 θ(i)
11: let ˆa ←1
6 ·
	n
i=1( θ(·) −ˆθ(i))3
	n
i=1( θ(·) −ˆθ(i))23/2
12: let ql ←

ˆz +
ˆz + 
−1(α/2)
1 −ˆa(ˆz + 
−1(α/2))

13: let qu ←

ˆz +
ˆz + 
−1(1 −α/2)
1 −ˆa(ˆz + 
−1(1 −α/2))

14: let l ←⌈ql N⌉
15: let u ←⌈qu N⌉
16: let θ∗
(1), . . . , θ∗
(N) be θ∗
1 , . . . , θ∗
N, sorted in increasing order
17: return (θ∗
(l), θ∗
(u))
The derivation of the speciﬁc form of bounds in the BCa method is beyond the
scope of this text; the reader is referred to Efron and Tibshirani (1993, Chapter 22)
and DiCiccio and Efron (1996) for discussion of the details. Instead, we study the
quality of the conﬁdence intervals from algorithm 5.20 and algorithm 5.21 with the
help of a numerical experiment.
By deﬁnition, a conﬁdence interval [U, V ] with conﬁdence coefﬁcient 1 −α for
a parameter θ satisﬁes
P

U(X1, . . . , Xn) ≤θ ≤V (X1, . . . , Xn)

≥1 −α.

208
AN INTRODUCTION TO STATISTICAL COMPUTING
Typically, the interval is constructed symmetrically in the sense that it satisﬁes P(θ <
U) = α/2 and P(θ > V ) = α/2. The remaining probability 1 −α/2 −α/2 = 1 −α
corresponds to the case U ≤θ ≤V . For a ﬁxed distribution P with known parameter
θ = θ(P), and for a given conﬁdence interval, we can estimate the probabilities
P(θ < U), P(U ≤θ ≤V ) and P(θ < U) using the following Monte Carlo approach:
(a) Generate X( j)
1 , . . . , X( j)
n
∼P, i.i.d., for j = 1, 2, . . . , N.
(b) Compute U ( j) = U ( j)(X( j)
1 , . . . , X( j)
n ) and V ( j) = V ( j)(X( j)
1 , . . . , X( j)
n ) for
j = 1, 2, . . . , N.
(c) Let
pinside = #

j
 U ( j) ≤θ(P) ≤V ( j)
N
as well as
pleft = #

j
 θ(P) < U ( j)
N
and
pright = #

j
 θ(P) > V ( j)
N
.
This allows to quantify how well the conﬁdence intervals perform for different
models.
Example 5.22
We apply the above procedure to three different families of models:
r the mean μ of standard normally distributed values;
r the mean μ of Exp(1)-distributed values; and
r the variance σ 2 of standard normally distributed values.
Each of these three families is considered for three different sample sizes (n =
10, 50, 100), resulting in nine different models being considered in total. Approximate
conﬁdence intervals were computed using both algorithm 5.20 and algorithm 5.21.
The resulting Monte Carlo estimates for the probabilities pleft, pinside and pright, using
α = 0.05, are shown in Table 5.1.
The results show, as expected, that the conﬁdence intervals improve as the sample
size increases. Both methods perform better for the symmetric problem of estimating
the mean of a normal distribution than they do for the more skewed cases of the last
two problems. For the last two problems, both methods result in conﬁdence intervals
which extend too far to the left and not far enough to the right, resulting in very small
probabilities for the event θ < U and too large probabilities for the event θ > V . For
the three test problems considered here, there is no clear difference in quality of the
conﬁdence intervals constructed by the two methods.

BEYOND MONTE CARLO
209
Table 5.1
The results of a numerical experiment to measure the empirical coverage
of the simple and BCa bootstrap conﬁdence intervals, for three different test
problems and different sample sizes n. The exact setup is described in example 5.22.
The experimentally determined coverage is shown in the last six columns of this
table. The theoretical values are 0.025 (left), 0.95 (inside) and 0.025 (right); results
close to the theoretical values are displayed in bold face.
Simple
BCa
P
θ
n
Left
Inside
Right
Left
Inside
Right
10
0.042
0.91
0.049
0.062
0.89
0.048
N(0, 1)
μ
50
0.032
0.94
0.031
0.029
0.94
0.035
100
0.027
0.95
0.022
0.024
0.95
0.027
10
0.011
0.85
0.14
0.021
0.86
0.12
Exp(1)
μ
50
0.008
0.92
0.072
0.015
0.93
0.055
100
0.014
0.94
0.048
0.022
0.94
0.038
10
0.021
0.86
0.12
0.006
0.77
0.23
N(0, 1)
σ 2
50
0.014
0.92
0.063
0.007
0.91
0.083
100
0.018
0.94
0.038
0.011
0.92
0.069
5.3
Summary and further reading
In this chapter we have studied two different methods which can be applied when
not enough mathematical structure is available to employ Monte Carlo methods. The
ﬁrst part of this chapter introduced the ABC method which can be used in Bayesian
problems when the posterior density is not explicitly known (or is too complicated),
but when a method for generating samples is still available. The second part of
this chapter introduced bootstrap methods, which can be used when even generating
samples is problematic.
One of the earliest papers about the ABC method, covering applications in pop-
ulation genetics, was by Tavar´e et al. (1997). Newer developments and extensions of
the method can be found, for example in Beaumont et al. (2002), Blum and Franc¸ois
(2010) and Fearnhead and Prangle (2012).
The bootstrap method is described in various textbooks, for example in Efron and
Tibshirani (1993). A wealth of practical information about bootstrap methods can be
found in the monograph by Davison and Hinkley (1997).
Exercises
E5.1
Implement the ABC method described in example 5.4. Test your implemen-
tation by generating samples for n = 20 and s∗= (6.989, 52.247).

210
AN INTRODUCTION TO STATISTICAL COMPUTING
E5.2
Implement ABC with regression from algorithm 5.6 for the problem described
in example 5.4. Test your implementation by generating samples for n = 20
and s∗= (6.989, 52.247).
E5.3
Consider the estimator
ˆσ 2(x1, . . . , xn) = 1
n
n

i=1

xi −¯x
2
for the variance, where ¯x is the average of the xi. Write a program that
computes, for given data x = (x1, . . . , xn), the bootstrap estimate of the bias
as given in algorithm 5.15. Test your program on a data set consisting of 100
standard normally distributed values. Justify your choice of the Monte Carlo
sample size N.
E5.4
Assume that we observe 2500 tosses of a biased coin and that we get the
following sequence of heads and tails:
H H T H H H H T H T T T H T T H T H T T T H H H H T T H T . . . T H T H,
containing 1600 heads and 900 tails in total. From these observations, we
estimate the probability p of head as ˆp = 1600/2500 = 16/25. Compute the
bootstrap estimate of the standard error of ˆp.
E5.5
Implement algorithm 5.20 for computing bootstrap conﬁdence intervals. Test
your program by computing bootstrap conﬁdence intervals for the mean of
a normal distribution and by comparing the output of your program with the
theoretically exact conﬁdence interval for this case.
E5.6
(a)
For n = 25, create a sample of n independent standard normally dis-
tributed random variables X1, . . . , Xn.
(b)
For ﬁxed N, repeatedly apply algorithm 5.20 to this data set to get dif-
ferent estimates for a conﬁdence interval for the variance. Numerically
determine the standard deviation of both bounds as well as the average
width of the interval.
(c)
Repeat the previous step for N = 1000, N = 2000 and N = 4000 and
comment on your results.
E5.7
Implement the BCa method from algorithm 5.21.
E5.8
Write a program which numerically determines the coverage probabilities
for the simple bootstrap conﬁdence interval (algorithm 5.20) and for the BCa
bootstrap conﬁdence interval (algorithm 5.21), as described in example 5.22.
Use this program to estimate the probability P(θ ∈[U, V ]) for the following
three problems:
(a)
Estimating conﬁdence intervals for the mean μ of standard normally
distributed values.
(b)
Estimating conﬁdence intervals for the mean μ of Exp(1)-distributed
values.

BEYOND MONTE CARLO
211
(c)
Estimating conﬁdence intervals for the variance σ 2 of standard normally
distributed values.
Compare the quality of the two different kinds of conﬁdence intervals.
Determine how the quality of the conﬁdence intervals depends on the sample
size n.

6
Continuous-time models
In this chapter we will pick up the thread we left in Chapter 2 and will return to the topic
of simulating statistical models. The models we are interested in here are described by
continuous-time processes, that is by stochastic processes where time is represented
by a bounded or unbounded interval of real numbers. There are two main motivations
for using continuous-time processes. First, physical time is continuous: physical quan-
tities like the temperature at a point or the location of a particle in space are in principle
deﬁned for all times. Thus, continuous-time models are appropriate for describing
physical processes. Secondly, in mathematical models continuous-time processes can
arise as the limit of discrete-time processes when the distance between the time steps
converges to zero. In such cases, analysis of the limiting continuous-time process is
typically much easier than analysis of the underlying discrete-time process.
In this chapter we will introduce the most important classes of continuous-time
stochastic processes and we will study how these processes can be simulated on a
computer. In the second part of this chapter we will revisit the Monte Carlo techniques
introduced in Chapter 3 and we will study how statistical continuous-time models
can be studied by simulation.
6.1
Time discretisation
Compared with the situation of discrete-time processes, for example the Markov
chains considered in Section 2.3, simulation in continuous-time introduces new chal-
lenges. Consider a stochastic process (Xt)t∈I where I ⊆R is a time interval, for
example I = [0, ∞) or I = [0, T ] for some time horizon T > 0. Even if the time
interval I is bounded, the trajectory (Xt)t∈I consists of uncountably many values.
Since computers only have ﬁnite storage capacity, it is impossible to store the whole
trajectory of a continuous-time process on a computer. Even computing values for
all Xt would take an inﬁnite amount of time. For these reasons, we restrict ourselves
An Introduction to Statistical Computing: A Simulation-based Approach, First Edition. Jochen Voss.
© 2014 John Wiley & Sons, Ltd. Published 2014 by John Wiley & Sons, Ltd.

214
AN INTRODUCTION TO STATISTICAL COMPUTING
to simulate X only for times t ∈In where In = {t1, t2, . . . , tn} ⊂I is ﬁnite. This
procedure is called time discretisation.
In many cases we can simulate the process X by iterating through the times
t1, t2, . . . , tn ∈In: we ﬁrst simulate Xt1, next we use the value of Xt1 to simulate
Xt2, then we use the values Xt1 and Xt2 to simulate Xt3 and so on. The ﬁnal step in
this procedure is to use the values Xt1, . . . , Xtn−1 to simulate Xtn. One problem with
this approach is that often the distribution of Xtk does not only depend on Xti for
i = 1, 2, . . . , k −1, but also on (unknown to us) values Xt where t /∈In. For this
reason, most continuous-time processes cannot be simulated exactly on a computer
and we have to resort to approximate solutions instead. The error introduced by these
approximations is called discretisation error.
6.2
Brownian motion
Brownian motion forms a basic building block for many kinds of continuous-time
stochastic processes.
Deﬁnition 6.1
An Rd-valued stochastic process (Bt)t≥0 is a Brownian motion (also
called a Wiener process), if it satisﬁes the following three conditions:
(a) B0 = 0;
(b) for every t ≥0 and h > 0, the increment Bt+h −Bt is N(0, hId)-distributed
and independent of (Bs)0≤s≤t; and
(c) the map t →Bt is continuous.
In the deﬁnition, N(0, hId) denotes the d-dimensional normal distribution with
covariance matrix hId (see Section 2.1) and Id is the d-dimensional identity matrix.
For a one-dimensional Brownian motion, the distribution of the increments of B
simpliﬁes to Bt+h −Bt ∼N(0, h).
In this book we denote Brownian motion by B. Some authors prefer the name
‘Wiener process’ over ‘Brownian motion’, and then use W to denote this process.
Both conventions are widely used in the literature.
From the deﬁnition of a Brownian motion B we see that t →Bt is a random
continuous function of time with values in Rd. One path of a one-dimensional
Brownian motion is shown in Figure 6.1.
In many cases, models based on Brownian motion are a good ﬁt for processes
which are, on a microscopic scale, driven by small, random, independent, additive
contributions. For such models, there are two possible categories:
r If the variance of the individual contributions is small (compared with the
inverse of their number), then the macroscopic behaviour of the system will be
deterministic.
r If the variance of the individual contributions is larger (comparable with the
inverse of the number of contributions), the resulting system will show random

CONTINUOUS-TIME MODELS
215
0
20
40
60
80
100
−4
−2
0
2
4
6
t
Bt
Figure 6.1
One instance of a one-dimensional Brownian motion, simulated until
time T = 100.
behaviour on a macroscopic scale. In these cases often the central limit theorem
(theorem A.9) applies and macroscopic increments are approximately normally
distributed.
Models based on Brownian motion are used for situations falling into the second
category.
Example 6.2
A tiny particle, like a grain of dust, suspended in a liquid is sub-
ject to ‘impulses’ by the individual water molecules hitting the particle. These
impacts happen with very high frequency and due to the complicated dynamics
of the water molecules can be considered to be random. This effect causes a visible
motion of the suspended particle which can be described by a three-dimensional
Brownian motion.
Example 6.3
The price of a share of stock of a company is determined by individual
trades of this stock. For stocks with a high enough volume of trading, the stock price
can be described as a (transformed) Brownian motion.
For both of these examples it is important to keep in mind that models based on
Brownian motion only form approximations. Use of these approximations is justiﬁed
by the fact that the resulting models describe the systems well on a wide range of
timescales. Nevertheless, since normally distributed increments are only observed
when sufﬁciently many of the microscopic contributions are combined, the models
will break down on very short timescales. Similarly, on very long timescales new
effects can occur; for example the physical particle considered in example 6.2 will
eventually hit the boundary of the enclosing container, whereas the Brownian motion
used as a model here does not include information about the container boundaries.

216
AN INTRODUCTION TO STATISTICAL COMPUTING
6.2.1
Properties
In this section we brieﬂy state some of the most important properties of Brownian
motion. Using deﬁnition 6.1, we can immediately derive the following results:
r We have Bt = Bt −B0 ∼N(0, t Id) and in particular the expectation of Bt is
E

Bt

= 0.
r For the one-dimensional case we ﬁnd Bt ∼N(0, t) and thus the standard
deviation of Bt is √t.
r Similarly, for any dimension, we ﬁnd that Bt has the same distribution as √t B1.
Thus, the magnitude of |Bt| only grows like √t as t increases.
Another basic result, relating the one-dimensional case to the d-dimensional case,
is given in the following lemma.
Lemma 6.4
The d-dimensional process B = (B(1)
t , . . . , B(d)
t )t≥0 is a Brownian
motion if and only if the components B(i) for i = 1, 2, . . . , d are independent, one-
dimensional Brownian motions.
Proof
This follows directly from the deﬁnition of a Brownian motion. We check
the three conditions from deﬁnition 6.1 one by one. First, the vector B0 is zero if
and only if all d of its components are zero. Secondly, Bt+h −Bt ∼N(0, hId) if and
only if B(i)
t+h −B(i)
t
∼N(0, h) for all i = 1, 2, . . . , d. And, ﬁnally, the map t →Bt
is continuous if and only if all of its components are continuous.
To conclude this section, in the following lemma we collect three invariance
properties of Brownian motion.
Lemma 6.5
Let B be a Brownian motion. Then the following statements hold.
(a) The process (−Bt)t≥0 is a Brownian motion.
(b) For every s ≥0, the process (Bs+t −Bs)t≥0 is a Brownian motion, indepen-
dent of (Br)0≤r≤s. (This is called the Markov property of Brownian motion.)
(c) For every c > 0, the process (√cBt/c)t≥0 is a Brownian motion. (This is called
the scaling property of Brownian motion.)
Proof
All three statements follow directly from the deﬁnition of a Brownian motion.
For the ﬁrst statement, let Xt = −Bt for all t ≥0. Then we have X0 = −B0 =
−0 = 0, the increments Xt+h −Xt = −(Bt+h −Bt) are N(0, hId) distributed since
Bt+h −Bt ∼N(0, hId), and t →Xt = −Bt is continuous since t →Bt is contin-
uous. Thus X = −B satisﬁes the conditions of deﬁnition 6.1 and consequently is a
Brownian motion.

CONTINUOUS-TIME MODELS
217
For the second statement, let s ≥0 and deﬁne Yt = Bs+t −Bs for all t ≥0. Then
Y0 = Bs+0 −Bs = 0 and
Yt+h −Yt = (Bs+t+h −Bs) −(Bs+t −Bs)
= Bs+t+h −Bs+t
∼N(0, hId),
since B is a Brownian motion. Furthermore, t →Yt = Bs+t −Bs is continuous and
thus Y is a Brownian motion. Again, since B is a Brownian motion, the random
variables Yt = Bs+t −Bs are independent of (Br)0≤r≤s.
Finally, for the third statement, let Zt = √cBt/c for all t ≥0. Clearly Z0 =
√cB0/c = 0. For the increments we have
Zt+h −Zt = √cB(t+h)/c −√cBt/c = √c

Bt/c+h/c −Bt/c

and, since Bt/c+h/c −Bt/c ∼N(0, h/cId), we ﬁnd Zt+h −Zt ∼N(0, hId). Again,
the continuity of B implies continuity of Z and thus the process Z is a Brownian
motion. This completes the proof of the lemma.
6.2.2
Direct simulation
As we have seen at the start of this chapter, we cannot hope to simulate all of the
inﬁnitely many values (Bt)t≥0 on a computer simultaneously. Instead, we restrict our-
selves to simulating the values of B for times 0 = t0 < t1 < · · · < tn. For a Brownian
motion, this can be easily done by using the ﬁrst two conditions from deﬁnition 6.1:
we have B0 = 0 and Bti can be computed from Bti−1 by adding an N(0, ti −ti−1)-
distributed random value, independent of all values computed so far. This method is
described in the following algorithm.
Algorithm 6.6
(Brownian motion)
input:
sample times 0 = t0 < t1 < · · · < tn
randomness used:
an i.i.d. sequence (εi)i=1,2,...,n with distribution N(0, 1)
output:
a sample of Bt0, . . . , Btn, that is a discretised path of a Brownian motion
1: B0 ←0
2: for i = 1, 2, . . . , n do
3:
generate εi ∼N(0, 1)
4:
Bi ←√ti −ti−1 εi
5:
Bti ←Bti−1 + Bi
6: end for
7: return (Bti)i=0,1,...,n

218
AN INTRODUCTION TO STATISTICAL COMPUTING
−1
0
1
2
3
4
5
−3
−2
−1
0
1
2
3
Bt
(2)
Bt
(1)
Figure 6.2
Path of a two-dimensional Brownian motion, simulated until time T =
10. The shades indicate time, ranging from t = 0 (light grey) to t = T (black).
While algorithm 6.6 only covers the one-dimensional case, it can also be used
to simulate a d-dimensional Brownian motion B = (B(1), . . . , B(d)) for d > 1: by
lemma 6.4 it sufﬁces to simulate the individual components B(i) for i = 1, 2, . . . , d,
independently.
Since the paths of B are continuous, for large n we can assume that the values
of B in between grid points are close to the values at the neighbouring grid points.
By using small grid spacing and connecting the values Bti using straight lines, we
can obtain a good approximation to a path of a Brownian motion. Results of such
simulations are shown in Figure 6.1 and Figure 6.2.
6.2.3
Interpolation and Brownian bridges
If we have already simulated values of a Brownian motion B for a set of times,
it is possible to reﬁne the simulated path afterwards by simulating values of B for
additional times. This method is called interpolation of the Brownian path. Since
these additional simulations need to be compatible with the already sampled values,
some care is needed when implementing this method.
Assume that we know the values of B at times 0 = t0 < t1 < · · · < tn and that
we want to simulate an additional value for B at time s with ti−1 < s < ti for
some i ∈{2, 3, . . . , n}. As an abbreviation we write r = ti−1 and t = ti. By the

CONTINUOUS-TIME MODELS
219
Markov property of Brownian motion (lemma 6.5, part (b)), the increment Bs −Br
is independent of (Bu)0≤u≤r. Similarly, (Bu −Bt)u≥t is independent of (Bu)0≤u≤t and
thus of Bs −Br. Consequently, we only need to take the value Bt = Bti into account
when sampling the increment Bs −Br; by independence the remaining Bt j with j ̸= i
do not affect the distribution of Bs −Br.
Assume that B is one-dimensional with Br = a. Since (Bu+r −Br)u≥0 is a
Brownian motion independent of Br, we have Bs ∼N(a, s −r) and Bt −Bs ∼
N(0, t −s), independently of each other. Thus, the joint density of (Bs, Bt) is
fBs,Bt(x, y) =
1
√2π(s −r) exp

−(x −a)2
2(s −r)

1
√2π(t −s) exp

−(y −x)2
2(t −s)

for all x, y ∈R. We need to condition this distribution on the, already sampled, value
of Bt. By deﬁnition of the conditional density fBs|Bt from (A.5) we have
fBs|Bt(x|b) = c1 fBs,Bt(x, b)
= c1
1
√2π(s −r) exp

−(x −a)2
2(s −r)

1
√2π(t −s) exp

−(b −x)2
2(t −s)

= c2 exp

−(x −a)2
2(s −r) −(b −x)2
2(t −s)

,
where c1 and c2 are constants. By expanding the expressions inside the exponential
and then completing the square we ﬁnd
fBs|Bt(x|b) = c3 exp

−(x −μ)2
2σ 2

,
where c3 is a constant,
μ = t −s
t −r a + s −r
t −r b
and
σ 2 = (t −s)(s −r)
t −r
.
Since fBs|Bt is a probability density, we know c3 = 1/
√
2πσ 2 and thus fBs|Bt is the
density of a N(μ, σ 2)-distribution.
The argument presented above shows that, given Br = a and Bt = b, the value
Bs for r < s < t satisﬁes
Bs ∼N
t −s
t −r a + s −r
t −r b, (t −s)(s −r)
t −r

.
(6.1)
The expectation of Bs for r < s < t is found by linear interpolation between Br and
Bt (corresponding to the dashed line in Figure 6.3). The variance of Bs is biggest at

220
AN INTRODUCTION TO STATISTICAL COMPUTING
s
Bs
r
t
a
b
Figure 6.3
Path of a Brownian bridge (Bs)s∈[r,t], interpolated between the points
(r, a) and (t, b). The dashed line gives the mean of the Brownian bridge, the shaded
region has a width of one standard deviation around the mean.
the centre of the interval [r, t] and converges to 0 when s approaches either of the
boundary points; the corresponding standard deviation is represented by the shaded
region in Figure 6.3.
Using equation (6.1), the procedure for interpolating a Brownian path is as fol-
lows: let 0 = t0 < t1 < · · · < tn be the times where the values Bti are already known
and let t > 0.
(a) If t = ti for an index i ∈{0, 1, . . . , n}, we return the already sampled value
Bt = Bti.
(b) If t > tn, return Bt ∼N(Btn, t −tn).
(c) If ti−1 < t < ti, return
Bt ∼N
 ti −t
ti −ti−1
Bti−1 + t −ti−1
ti −ti−1
Bti, (ti −t)(t −ti−1)
ti −ti−1

.
For the last two cases, if the procedure is repeated, we need to add the newly sampled
value to the list of already known values for B. The result of a simulation using this
method is shown in Figure 6.4.
The method we used to interpolate a Brownian path between already sampled
points describes a continuous-time stochastic process on the time interval [r, t].
Deﬁnition 6.7
A Brownian bridge between (r, a) and (t, b), where r < t are times
and a, b ∈Rd, is the continuous-time stochastic process on the time interval [r, t],
obtained by conditioning a Brownian motion B on the events Br = a and Bt = b.

CONTINUOUS-TIME MODELS
221
0
5
10
15
20
−2
−1
0
1
2
3
t
Bt
Figure 6.4
One instance of a one-dimensional Brownian motion, simulated using
the interpolation method described in Section 6.2.3. The grey ‘skeleton’ of the path is
simulated ﬁrst. The reﬁned path, shown in black, is sampled later using interpolation.
A Brownian bridge can be simulated by interpolating a Brownian path between
Br = a and Bt = b, as described above. The distribution of a Brownian bridge at a
ﬁxed time s ∈[r, t] is described by equation (6.1).
A second method for sampling a path of a Brownian bridge is given by the
following procedure: we can ﬁrst sample a path B of an ordinary Brownian motion,
and then deﬁne a process X by
Xs = Bs−r + a −s −r
t −r (Bt−r −b + a).
(6.2)
One can prove that the resulting process (Xs)s∈[r,t] is again a Brownian bridge. This
method is often more efﬁcient than the interpolation method, when many samples
from the path of a Brownian bridge are required.
6.3
Geometric Brownian motion
Geometric Brownian motion is a continuous-time stochastic process which is derived
from Brownian motion and can, for example, be used as a simple model for stock
prices.
Deﬁnition 6.8
A continuous-time stochastic process X = (Xt)t≥0 is a geometric
Brownian motion, if
Xt = X0 exp

αBt + βt

(6.3)
for all t ≥0, where α > 0 and β ∈R are constants and B is a one-dimensional
Brownian motion, independent of X0.

222
AN INTRODUCTION TO STATISTICAL COMPUTING
0
2
4
6
8
10
0 2 4 6 8 10
t
Xt
Figure 6.5
One path of a geometrical Brownian motion with X0 = 1, α = 1 and
β = −0.1, simulated until time T = 10.
Since we know how to generate samples of a Brownian motion from section 6.2.2,
generating samples of a geometric Brownian motion is easy: we can simply simulate
Bt, for example using algorithm 6.6, and then separately compute X0 exp(αBt + βt)
for every t-value used in the simulation (the result of one such simulation is shown
in Figure 6.5). Nevertheless, we will see below that some care is needed when using
the simulated values for Monte Carlo estimates.
Lemma 6.9
Let X be a geometric Brownian motion with parameters α and β. Then
E(Xt) = E(X0) exp

(α2/2 + β)t

for all t ≥0.
Proof
Since we know Bt ∼N(0, t), we can compute the expectation of Xt using
integration:
E(Xt) = E(X0) E

exp

αBt + βt

= E(X0)

R
exp

αy + βt

1
√
2πt
exp

−y2
2t

dy
= E(X0) eβt
√
2πt

R
exp

−y2
2t + αy

dy.
By completing the square we ﬁnd
E(Xt) = E(X0)exp

(α2/2 + β)t

√
2πt

R
exp

−1
2t

y2 −2yαt + α2t2
dy
= E(X0) exp

(α2/2 + β)t

1
√
2πt

R
exp

−(y −αt)2
2t

dy
= E(X0) exp

(α2/2 + β)t

.
(6.4)
This completes the proof.
As a consequence of lemma 6.9 we see that the conditions for Xt to have constant
expectation on the one hand and for the exponent αBt + βt in (6.3) to have constant

CONTINUOUS-TIME MODELS
223
expectation on the other hand are different. For β = −α2/2 we ﬁnd that the process
Xt satisﬁes
E(Xt) = E(X0) exp
α2
2 −α2
2

t

= E(X0),
that is for β = −α2/2 the geometric Brownian motion has constant expectation. On
the other hand, the exponent αBt + βt satisﬁes for this case
E(αBt + βt) = E

αBt −α2
2 t

= −α2
2 t
and thus E(αBt + βt) is not constant but converges to −∞as t →∞.
The difference in behaviour of E(Xt) and E(αBt + βt) is caused by the fact
that positive ﬂuctuations of αBt + βt are greatly ampliﬁed by the exponential in the
deﬁnition (6.3) whereas negative ﬂuctuations are damped down. A more quantitative
result can be obtained by studying the integral in equation (6.4): the integration
variable y runs over all possible values of Bt and the integrand exp

−(y −αt)2/2t

in the last integral combined the map which transforms B into X with the density
for Bt. It is easy to check that the main contribution of this integral comes from
the region y ≈αt ± √t. Thus, the main contribution to E(Xt) comes from values
of Bt ≈αt ± √t. This corresponds to very unlikely events which, when they occur,
make huge a contribution to the expectation: The probability of Bt ≈αt ± √t can
be estimated as
P

αt −√t ≤Bt ≤αt + √t

= P

α√t −1 ≤Bt
√t ≤α
√
t + 1

=
1
√
2π
 1
−1
exp

−1
2(x + α
√
t)2

dx
≈
2
√
2π
exp

−α2t
2

.
(6.5)
As a consequence, Monte Carlo methods cannot be used to estimate E(Xt) for large
values of t. In order to get a reasonable estimate for E(Xt), at least a few Monte
Carlo samples need to fall into the region Bt ≈αt ± √t. From equation (6.5) we
see that this requires sample size N ≫exp(α2t/2) and for large values of α2t such
sample sizes will no longer be practical. This effect is illustrated in Figure 6.6. A
solution to this problem would be to use importance sampling and to replace the i.i.d.
copies Bt ∼N(0, t) used in the basic Monte Carlo method with N(αt, t)-distributed
proposals.

224
AN INTRODUCTION TO STATISTICAL COMPUTING
●●●●●●●●●●●●
●
●
●●
●
●
●
●
●
●
●
●
●●●●
●●
●●
●
●
●
●●●●
●
●●●
●
●●●●●
●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
0
20
40
60
80
100
0.0
0.5
1.0
1.5
2.0
2.5
3.0
t
E(X t)
Figure 6.6
Monte Carlo estimates for the expectation of the geometric Brownian
motion Xt = exp(Bt −t/2), where B is a Brownian motion, using N = 106 Monte
Carlo samples for each value of t. The estimates for different t are displayed as
circles whereas the exact value of the expectation, E(Xt) = 1 by lemma 6.9, is given
by the horizontal line. The ﬁgure shows that good estimates are only obtained for
t < 15.
6.4
Stochastic differential equations
A wide class of continuous-time stochastic processes X = (Xt)t≥0 can be described
as solutions to stochastic differential equations (SDEs) of the form
dXt = μ(t, Xt) dt + σ(t, Xt) dBt
for all t > 0,
X0 = x0,
(6.6)
where B = (Bt)t≥0 is a Brownian motion with values in Rm and μ: [0, ∞) × Rd →
Rd as well as σ: [0, ∞) × Rd →Rd×m are functions, and the initial condition x0 ∈
Rd can be random or deterministic.
6.4.1
Introduction
In this section we will give a very short introduction to SDEs, mainly by giving an
intuitive idea about the properties of processes described by equations such as (6.6).
We start by giving an informal explanation of different aspects of equation (6.6).
r The stochastic process X = (Xt)t≥0 is the ‘unknown’ in equation (6.6). Solving
the SDE means to ﬁnd a stochastic process X such that (6.6) is satisﬁed. (We
will discuss below what this means.) Since the Brownian motion B on the
right-hand side of (6.6) is random, the solution X is random, too.
r x0 ∈Rd is called the initial value of the SDE.

CONTINUOUS-TIME MODELS
225
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
−1.0
0.0
0.5
−0.5
1.0
Xt
(2)
Xt
(1)
●x0
Figure 6.7
One path of a two-dimensional SDE. The drift μ is indicated by the grey
arrows; the diffusion coefﬁcient is large inside the grey rectangle and small outside
it.
r The function μ: [0, ∞) × Rd →Rd is called the drift of the SDE. Given the
current time t and the current value Xt, it determines the direction of mean
change of the process just after time t:
E

Xt+h
 Xt

≈Xt + μ(t, Xt)h
as h ↓0. The effect of the drift is illustrated in Figure 6.7.
r The matrix-valued function σ: [0, ∞) × Rd →Rd×m is called the diffusion
coefﬁcient of the SDE. It determines the amount of random ﬂuctuations X is
subject to at any given time and place. Conditioned on the value of Xt, the
covariance matrix of Xt+h satisﬁes
Cov

Xt+h
 Xt

≈σ(t, Xt)σ(t, Xt)⊤h
as h ↓0.
r The stochastic differentials dXt, dt and dBt describe the increments of the
processes X, t and B. Formally, we have dXt = Xt −X0, dt = t −0 = t and
dBt = Bt −B0 = Bt.
r The ‘products’ μ(t, Xt) dt and σ(t, Xt) dBt are shorthand notations for inte-
grals. The ﬁrst of these terms, μ(t, Xt) dt, stands for the vector with compo-
nents

μ(t, Xt) dt)i =
 t
0
μi(s, Xs) ds

226
AN INTRODUCTION TO STATISTICAL COMPUTING
for i = 1, 2, . . . , d where μi is the ith component of the vector μ. Similarly,
σ(t, Xt) dBt stands for the random vector where the components are given by
the sum of the stochastic integrals

σ(t, Xt) dBt

i =
m

j=1
 t
0
σi j(s, Xs) dB( j)
s
(6.7)
for i = 1, 2, . . . , d. Here, σi j stands for the element in row i, column j of
the matrix σ, the values B(1), . . . , B(m) are the components of the Brownian
motion B, and the sum over j is part of a matrix-vector product between
the matrix-valued function σ and the vector-valued process B. We will defer
explanation of the stochastic integrals in (6.7) until Section 6.4.2.
Using the notation introduced, a continuous-time stochastic process X is deﬁned to
be a solution of the SDE (6.6), if X = (X(1), . . . , X(d)) satisﬁes
X(i)
t
= x(i)
0 +
 t
0
μi(s, Xs) ds +
m

j=1
 t
0
σi j(s, Xs) dB( j)
s
(6.8)
for all t ≥0 and all i = 1, 2, . . . , d.
In order for the SDE (6.6) to have a solution, that is for a process X which satisﬁes
(6.8) to exist, assumptions on the drift μ and the diffusion coefﬁcient σ are required.
We will not consider these conditions here and instead refer to the literature given
at the end of this chapter, for example the books by Mao (2007, Section 5.2) and
Kloeden and Platen (1999, Section 4.5).
6.4.2
Stochastic analysis
Some technical detail is required to give a mathematically rigorous deﬁnition of the
stochastic integrals in equation (6.7). We omit the rigorous deﬁnition here and refer to
the references given at the end of this chapter for details. Instead, we restrict ourselves
to heuristic explanations of the most important aspects.
6.4.2.1
Ito integrals
The stochastic integral, also called the Ito integral, of the integrand Y with a integrator
X is given by the limit
 T
0
Yt dXt = lim
n→∞
n−1

i=0
Yt(n)
i

Xt(n)
i+1 −Xt(n)
i

(6.9)
where t(n)
i
= iT/n for i = 0, 1, . . . , n. Here, X and Y are stochastic processes. In
(6.7) this relation is used with the integrand σi j(s, Xs) instead of Y and with the
integrator B instead of X. Since X and Y are random, the value of the stochastic

CONTINUOUS-TIME MODELS
227
integral (6.9) is a random variable. Equation (6.9) is in analogy to the approximation
of the ordinary Riemann integral by Riemann sums:
 T
0
f (t) dt = lim
n→∞
n−1

i=0
f (ti)

ti+1 −ti

.
(6.10)
While the characterisation of a stochastic integral given in Equation (6.9) is not
enough to be able to give mathematically rigorous proofs of statements involving
stochastic integrals, it sufﬁces to motivate the numerical methods introduced in this
chapter. Also, equation (6.9) can be used as the basis of numerical methods to compute
stochastic integrals.
An important special case of equation (6.9) is when Y is constant, that is Yt = c
for all t ∈[0, T ]. In this case, we have
 T
0
c dXt = c lim
n→∞
n−1

i=0

Xt(n)
i+1 −Xt(n)
i

= c

Xtn −Xt0

= c

XT −X0

.
6.4.2.2
Ito’s formula
An important tool from stochastic analysis is Ito’s formula: This formula allows
to evaluate the stochastic differentials of functions of a stochastic process. If X is
a stochastic process with values in Rd and f : [0, ∞) × Rd →R is differentiable
with respect to the ﬁrst argument and differentiable twice with respect to the second
argument, then the stochastic differentials of the process f (t, Xt) satisfy
d

f (t, Xt)

= ∂
∂t f (t, Xt) dt +
d

i=1
∂
∂xi
f (t, Xt) dX(i)
t
+1
2
d

i, j=1
∂2
∂xi∂x j
f (t, Xt) dX(i)
t dX( j)
t ,
(6.11)
where the product dX(i)
t dX( j)
t
is determined by the rules
dB(i)
t dB( j)
t
=
	dt
if i = j and
0
otherwise,
dB(i)
t dt = 0.
In particular, if X solves the SDE (6.6), we have
dX(i)
t
= μi(t, Xt) dt +
d

k=1
σik(t, Xt) dB(k)
t

228
AN INTRODUCTION TO STATISTICAL COMPUTING
and thus
dX(i)
t dX( j)
t
= μi(t, Xt)μ j(t, Xt) dt dt + μi(t, Xt)
d

l=1
σ jl(t, Xt) dB(l)
t dt
+
d

k=1
σik(t, Xt)μ j(t, Xt) dB(k)
t
dt
+
d

k,l=1
σik(t, Xt)σ jl(t, Xt) dB(k)
t
dB(l)
t
=
d

k=1
σik(t, Xt)σ jk(t, Xt) dt
= (σσ ⊤)i j(t, Xt) dt.
These expressions can be substituted into (6.11).
One important special case is the case of a one-dimensional stochastic process X.
In this case, equation (6.11) simpliﬁes to
d

f (t, Xt)

= ∂
∂t f (t, Xt) dt + ∂
∂x f (t, Xt) dXt
+ 1
2
∂2
∂x2 f (t, Xt) dXt dXt.
(6.12)
Furthermore, if f does not depend on t, we get
d

f (Xt)

= f ′(Xt) dXt + 1
2 f ′′(Xt) dXt dXt.
(6.13)
Finally, if X solves a one-dimensional SDE of the form (6.6), the product dXt dXt
can be written as
dXt dXt = σ(t, Xt)2 dt
and we get
d

f (Xt)

= f ′(Xt) dXt + 1
2 f ′′(Xt)σ(t, Xt)2 dt
= f ′(Xt) (μ(t, Xt) dt + σ(t, Xt) dBt) + 1
2 f ′′(Xt)σ(t, Xt)2 dt
=

f ′(Xt)μ(t, Xt) + 1
2 f ′′(Xt)σ(t, Xt)2

dt + f ′(Xt)σ(t, Xt) dBt.
(6.14)

CONTINUOUS-TIME MODELS
229
Example 6.10
Let X = B be a one-dimensional Brownian motion and f (x) = x2.
In this case, since f ′(x) = 2x and f ′′(x) = 2, we can use Ito’s formula in the form
of equation (6.13) to ﬁnd
d

B2
t

= 2Bt dBt + 1
22 dB dB = 2Bt dBt + dt.
This equation can be written in integral notation as
B2
t −B2
0 = 2
 t
0
Bs dBs + (t −0)
and, since B0 = 0, we can rewrite this as
 t
0
Bs dBs = 1
2

B2
t −t

.
Thus, with the help of Ito’s formula we have found the exact value of the stochastic
integral

 t
0 Bs dBs.
Example 6.11
Let Xt = x0 exp

αBt + βt

be a geometric Brownian motion with
x0 ∈R. Then we can write Xt = f (t, Bt) for all t ≥0, where
f (t, x) = x0 exp

αx + βt

.
In order to ﬁnd the stochastic differential dX, we need to compute the derivatives of
f : we get
∂
∂t f (t, x) = x0β exp

αx + βt

= β f (t, x),
∂
∂x f (t, x) = x0α exp

αx + βt

= αf (t, x),
∂2
∂x2 f (t, x) = x0α2 exp

αx + βt

= α2 f (t, x).
Now we can apply Ito’s formula in the form of equation (6.12) to get
dXt = d

f (t, Bt)

= ∂
∂t f (t, Bt) dt + ∂
∂x f (t, Bt) dBt + 1
2
∂2
∂x2 f (t, Bt) dBt dBt
= β f (t, Bt) dt + αf (t, Bt) dBt + 1
2α2 f (t, Bt) dt

230
AN INTRODUCTION TO STATISTICAL COMPUTING
=

β + α2
2

f (t, Bt) dt + αf (t, Bt) dBt
=

β + α2
2

Xt dt + αXt dBt.
Thus, we have found that the geometric Brownian motion X is a solution of the SDE
dXt =
α2
2 + β

Xt dt + αXt dBt
X0 = x0.
6.4.2.3
Stratonovich integrals
Looking back at the deﬁnition of the Ito integral in equation (6.9) we can see that
we had a choice when we chose the analogy to the Riemann integral from equation
(6.10): The approximation in (6.10) still works if f (ti) is replaced with f (ti+1), or
with any f (˜ti) where ˜ti ∈[ti, ti+1]. It transpires that the stochastic integral in (6.9) is
much less robust: the integrand Y must be evaluated at the left-most point ti of each
discretisation interval [ti, ti+1]. If Yti in (6.9) is, for example, replaced with Yti+1, one
can show that for many integrators X the approximation converges to a different limit
as n →∞. This is caused by the irregular paths featured by typical integrators, for
example, for X = B as before.
By choosing different times to evaluate the integrand of a stochastic integral,
different kinds of stochastic integrals can be obtained. The most common choice,
after the Ito integral, is called the Stratonovich integral. This integral is obtained by
evaluating the integrand at the centre of each discretisation interval, and it is denoted
by an additional ◦in front of the integrand dX:
 T
0
f (t) ◦dXt = lim
n→∞
n−1

i=0
f

t(n)
i
+ t(n)
i+1
2


Xt(n)
i+1 −Xt(n)
i

.
(6.15)
Correspondingly, by replacing the Ito integral in (6.8) with a Stratonovich integral,
Stratonovich SDEs of the form
dXt = μ(t, Xt) dt + σ(t, Xt) ◦dBt
X0 = x0
(6.16)
can be deﬁned.
Studying the difference between the Ito integral (6.9) and the Stratonovich inte-
gral (6.15) more closely, one can show that a process X solves the Stratonovich

CONTINUOUS-TIME MODELS
231
SDE (6.16) with differentiable diffusion coefﬁcient σ, if and only if it solves the
Ito SDE
dXt = ˜μ(t, Xt) dt + σ(t, Xt) dBt
X0 = x0,
where the drift vector ˜μ: [0, ∞) × Rd →Rd has components
˜μi(t, x) = μi(t, x) + 1
2
m

j=1
d

k=1
σkj(t, x) ∂
∂xk
σi j(t, x)
for all t ≥0 and x ∈Rd and for all i = 1, 2, . . . , d. This allows to convert any
Stratonovich SDE into an Ito SDE with the same solutions. For this reason, in the
rest of this chapter we will only discuss Ito SDEs.
6.4.3
Discretisation schemes
In this section we will discuss methods to simulate solutions of an SDE using a
computer. We consider the SDE
dXt = μ(t, Xt) dt + σ(t, Xt) dBt
X0 = x0
(6.17)
where B = (Bt)t≥0 is an m-dimensional Brownian motion, μ: [0, ∞) × Rd →Rd is
the drift, and the diffusion coefﬁcient is given by σ: [0, ∞) × Rd →Rd×m. Our aim
is to simulate values of X at times 0 = t0 < t1 < · · · < tn. We will proceed, starting
with X0 = x0, by successively computing Xt1, Xt2, . . . until time tn is reached.
The amount of change of X over the time interval [ti, ti+1] can be found from the
deﬁnition (6.8) of a solution: subtracting the expressions for Xti from the expression
for Xti+1, we ﬁnd
X( j)
ti+1 −X( j)
ti
=
 ti+1
ti
μ j(t, Xt) dt +
m

k=1
 ti+1
ti
σ jk(t, Xt) dB(k)
t
(6.18)
for j = 1, 2, . . . , d. Assume that we have already computed X0, Xt1, . . . , Xti and we
want to compute Xti+1. Then we have to solve the following two problems:
r The integrals on the right-hand side of (6.18) depend on the values Xt for
t ∈[ti, ti+1]. This is a problem, since we only know the (approximate) value
of Xti and the values of Xt for t > ti are unknown.
r Even if X was known, it is still not clear how the integrals can be solved for
nontrivial functions μ and σ.

232
AN INTRODUCTION TO STATISTICAL COMPUTING
There are different approaches to solving these problems, leading to different
numerical schemes.
6.4.3.1
The Euler–Maruyama scheme
The idea behind the Euler–Maruyama scheme (sometimes called the stochastic Euler
scheme or just the Euler scheme) is to evaluate the drift and diffusion coefﬁcient in
(6.18) at time ti instead of time t for all t ∈[ti, ti+1]. Then the integrals can be
approximated by
 ti+1
ti
μ j(t, Xt) dt ≈
 ti+1
ti
μ j(ti, Xti ) dt = μ j(ti, Xti ) (ti+1 −ti)
and
m

k=1
 ti+1
ti
σ jk(t, Xt) dB(k)
t
≈
m

k=1
 ti+1
ti
σ jk(ti, Xti ) dB(k)
t
=
m

k=1
σ jk(ti, Xti )

B(k)
ti+1 −B(k)
ti

=

σ(ti, Xti )

Bti+1 −Bti

j .
where the last expression uses matrix-vector multiplication to simplify notation. Sub-
stituting these approximations into equation (6.18) allows us to compute approximate
values
Xti+1 −Xti ≈μ j(ti, Xti ) (ti+1 −ti) + σ(ti, Xti )

Bti+1 −Bti

(6.19)
for i = 0, 1, . . . , n −1. This leads to the following algorithm for computing approx-
imate solutions to SDEs.
Algorithm 6.12
(Euler–Maruyama scheme)
input:
the drift μ: [0, ∞) × Rd →Rd
the diffusion coefﬁcient σ: [0, ∞) × Rd →Rm×d
the initial value x0 ∈Rd
the time horizon T ≥0
the discretisation parameter n ∈N
randomness used:
samples B0, Bh, B2h, . . . , Bnh from a d-dimensional Brownian motion,
where h = T/n
output:
an approximation (XEM
ih )i=0,...,n to a solution of (6.17)
1: h ←T/n
2: XEM
0
←x0

CONTINUOUS-TIME MODELS
233
3: for i = 0, 1, . . . , n −1 do
4:
Bi ←B(i+1)h −Bih
5:
XEM
(i+1)h ←XEM
ih + μ(ih, XEM
ih ) h + σ(ih, XEM
ih ) Bi
6: end for
7: return (XEM
ih )i=0,1,...,n
The algorithm can either use a given path of a Brownian motion, or alternatively
the increments Bi can be directly sampled in the algorithm by generating Bi ∼
N(0, hId) independently in each iteration of the loop.
Lemma 6.13
The computational cost for computing a path using the Euler–
Maruyama scheme from algorithm 6.12 is of order C(n) = O(n).
Proof
Each of the iterations of the loop has the same cost, so the total cost is
C(n) = a + bn, where a is the cost of the assignments at the start of the algorithm
and b is the cost per iteration of the loop.
Since the approximations underlying the Euler method get more accurate when
the size of the time step decreases, we expect the ‘error’ of the Euler–Maruyama
approximation to go to 0 as the discretisation parameter n increases. This convergence
is discussed in Section 6.4.4.
Example 6.14
For α, β ∈R, consider the one-dimensional SDE
dXt =
α2
2 + β

Xt dt + αXt dBt.
(6.20)
The Euler–Maruyama scheme for this SDE is
XEM
(i+1)h = XEM
ih +
α2
2 + β

XEM
ih h + α XEM
ih Bi
for i = 0, 1, . . . , n −1, where XEM
0
= x0 and Bi = B(i+1)h −Bih. We can obtain
numerical solutions of SDE (6.20) by iteratively computing XEM
ih
for i =
0, 1, . . . , n −1.
When applying algorithm 6.12, there is a trade-off between speed and accuracy to
be made: larger values of the discretisation parameter n lead to more accurate results,
smaller values of n lead to faster execution.
6.4.3.2
The Milstein scheme
The Milstein scheme is an improved version of the Euler–Maruyama scheme. It
uses a slightly more complicated discretisation mechanism to achieve more accurate

234
AN INTRODUCTION TO STATISTICAL COMPUTING
results. In this section we only discuss the one-dimensional case. More speciﬁcally,
we consider
dXt = μ(Xt) dt + σ(Xt) dBt
for all t >0,
X0 = x0,
(6.21)
where B = (Bt)t≥0 is a one-dimensional Brownian motion, μ: R →R is the drift
and σ: R →R is the diffusion coefﬁcient.
Like the Euler–Maruyama scheme, the Milstein scheme is based on equation
(6.18), but instead of the approximation σ(Xt) ≈σ(Xti) for all t ∈[ti, ti+1], the
Milstein scheme uses an improved approximation: since the paths of the Brownian
motion B are very rough, for t ≈ti the Brownian increment |Bt −Bti| is much bigger
than |t −ti|. Consequently, the second term in the approximation (6.19) dominates
and we have
Xt −Xti ≈σ(Xti) ·

Bt −Bti

.
Using this approximation, together with ﬁrst-order Taylor approximation for σ, we
ﬁnd
σ(Xt) = σ(Xti + Xt −Xti)
≈σ(Xti) + σ ′(Xti)(Xt −Xti)
≈σ(Xti) + σ ′(Xti)σ(Xti) ·

Bt −Bti

.
Using this expression, the second integral in (6.18) can be approximated as
 ti+1
ti
σ(Xt) dBt ≈
 ti+1
ti
σ(Xti ) + σ ′(Xti)σ(Xti ) ·

Bt −Bti

dBt
= σ(Xti )

Bti+1 −Bti

+ σ ′(Xti )σ(Xti )
 ti+1
ti

Bt −Bti

dBt
= σ(Xti )

Bti+1 −Bti

+ σ ′(Xti )σ(Xti )1
2

Bti+1 −Bti
2 −(ti+1 −ti)

,
where the value of the last stochastic integral is found similar to the one in example
6.10. Based on this argument, the Milstein scheme replaces (6.19) by
Xti+1 −Xti ≈μ j(Xti) hi + σ(Xti ) Bi
+1
2σ(Xih)σ ′(Xih)

B2
i −hi

,
(6.22)
where σ ′ is the derivative of σ, Bi = Bti+1 −Bti , and hi = ti+1 −ti for i =
0, 1, . . . , n −1. Comparing this expression with equation (6.19) shows that the only

CONTINUOUS-TIME MODELS
235
difference between the Euler–Maruyama and Milstein methods is the term in the sec-
ond line of equation (6.22). In the special case where σ is constant, we have σ ′ = 0
and both methods coincide.
Algorithm 6.15
(Milstein scheme)
input:
the drift μ: R →R
the diffusion coefﬁcient σ: R →R and its derivative σ ′
the initial value X0 ∈R
the time horizon T ≥0
the discretisation parameter n ∈N
randomness used:
samples B0, Bh, B2h, . . . , Bnh from a one-dimensional Brownian motion,
where h = T/n
output:
an approximation (XMIL
ih )i=0,...,n to a solution of (6.21)
1: h ←T/n
2: for i = 0, 1, . . . , n −1 do
3:
generate Bi = B(i+1)h −Bih
4:
XMIL
(i+1)h = XMIL
ih
+ μ(XMIL
ih ) h + σ(XMIL
ih ) Bi
+ 1
2σ(XMIL
ih )σ ′(XMIL
ih )

B2
i −h

5: end for
6: return (XMIL
ih )i=0,1,...,n
As before, there is a trade-off between speed of the algorithm and accuracy of the
obtained approximation: larger values of the discretisation parameter n lead to more
accurate results, smaller values of n lead to faster execution.
Lemma 6.16
The computational cost for computing a path from the Milstein
scheme in algorithm 6.1 is of order C(n) = O(n).
Proof
Each of the iterations of the loop has the same cost, so the total cost is
C(n) = a + bn where a is the cost of the assignments at the start of the algorithm
and b is the cost per iteration of the loop.
We defer discussion of the error of the Milstein method until Section 6.4.4.
Example 6.17
Continuing from example 6.14, we consider again the SDE
dXt =
α2
2 + β

Xt dt + αXt dBt.

236
AN INTRODUCTION TO STATISTICAL COMPUTING
Since the diffusion coefﬁcient is σ(x) = αx, we have σ ′(x) = α and σ(x)σ ′(x) =
α2x. Thus the Milstein scheme for this SDE is
XMIL
(i+1)h = XMIL
ih
+
α2
2 + β

XMIL
ih
h + α XMIL
ih
Bi
+ α2
2 XMIL
ih

B2
i −h

for i = 0, 1, . . . , n −1, where XMIL
0
= x0 and Bi = B(i+1)h −Bih.
6.4.4
Discretisation error
Since numerical methods for SDEs replace the increments from equation (6.18) by
approximations, a numerically obtained solution will not exactly coincide with the
exact solution. In this section we will discuss the resulting discretisation error.
Since both the exact solution and the numerical approximation are random, dif-
ferent ways of quantifying the error can be considered.
6.4.4.1
Strong error
Let X be the exact solution of the SDE (6.17) and let ˆX be a numerical approximation
to X, using the same Brownian motion as X does. Then the strong error of the
approximation ˆX at time T is given by
estrong = E
 ˆXT −XT

(6.23)
for all sufﬁciently large n. The quantity estrong measures the average distance between
the exact and the approximate solution. The strong error is small, if the values of the
numerical solution are close to the values of the exact solution, that is if the paths of
the SDE are approximated well.
Different numerical methods can be compared by studying how the strong error
decreases when the discretisation parameter increases. This needs to be balanced to
the corresponding increase in computational cost.
Let XEM be the Euler–Maruyama approximation computed by algorithm 6.12
with discretisation parameter n. Then, under appropriate assumptions on the drift μ
and the diffusion coefﬁcient σ, there is a constant c > 0 such that
eEM
strong = E
XEM
T
−XT
 ≤
c
√n .
Let XMIL be the Milstein approximation, computed by algorithm 6.15 with dis-
cretisation parameter n. Then, under appropriate assumptions on the drift μ and the

CONTINUOUS-TIME MODELS
237
diffusion coefﬁcient σ, there is a constant c > 0 such that the strong error of the
Milstein scheme satisﬁes
eMIL
strong = E
XMIL
T
−XT
 ≤c
n
for all sufﬁciently large n. As n gets large, this bound decays faster than the bound
c/√n for the Euler–Maruyama method. Thus, for large n, the Milstein scheme has a
signiﬁcantly smaller strong error than the Euler–Maruyama scheme.
Example 6.18
SDE (6.20) from example 6.14 is simple enough that it can be
solved explicitly: from example 6.11 we know that the exact solution of (6.20) is the
geometric Brownian motion
Xt = X0 exp (αBt + βt) .
This knowledge of the exact solution allows us to ‘measure’ the error of different
discretisation schemes: comparing a numerical solution ˆXT to the exact value XT
allows to determine | ˆXT −XT |. By repeating this experiment for different paths of
the Brownian motion B and taking averages, we can obtain a Monte Carlo estimate
for the error estrong = E
 ˆXT −XT
. Repeating this estimation procedure for different
values of n allows then to determine the dependence of eEM
strong on the discretisation
parameter n.
Figure 6.8 shows the result of such simulations, for ˆX = XEM and ˆX = XMIL,
respectively. The ﬁgure conﬁrms that we have indeed eEM
strong ≈c/√n and eMIL
strong ≈
c/n.
6.4.4.2
Weak error
In cases where we solve the SDE for use in a Monte Carlo estimate, it is only important
that the distribution of XEM
T
is accurate, whereas it is less important that the values
of XEM
T
(as a function of B) are accurate. For this reason, a second error criterion is
considered in this section.
As before, let X be the exact solution of an SDE and let ˆX be a numerical
approximation to X. Furthermore, let A be a class of functions from Rd to R (e.g.
all bounded, twice differentiable functions). Then the error in the distribution of ˆXT
can be quantiﬁed by considering
eweak( f ) =
E

f ( ˆXT )

−E ( f (XT ))

(6.24)
for all f ∈A and all sufﬁciently large n. The quantity
eweak = sup
f ∈A
eweak( f )

238
AN INTRODUCTION TO STATISTICAL COMPUTING
x
x
x
x
x
x
x
x
x
0.005
0.010
0.020
0.050
1
2
5
10
20
h
Strong error
o
o
o
o
o
o
o
o
o
x
o
Euler−Maruyama
Milstein
Figure 6.8
Strong error of the Euler–Maruyama and Milstein schemes, for the SDE
(6.20) with α = 1, β = 0.4 and T = 5. The plot shows the estimated strong error for
both methods as a function of the step size h = T/n, for different values of n. The points
marked with ‘x’ are Monte Carlo estimates for eEM
strong, the points marked with ‘o’ are
Monte Carlo estimates for eMIL
strong, and the bars give 95% conﬁdence intervals for the
Monte Carlo estimates. Finally, the solid lines are ﬁtted curves of the forms c/√n
(for the Euler–Maruyama method) and c/n (for the Milstein method), respectively.
is called the weak error of ˆX at time T . In this context, the functions f ∈A are called
test functions. The weak error is small, if the distribution of the numerical solution is
close to the distribution of the exact solution.
Let XEM be the Euler–Maruyama approximation with discretisation parameter n.
Then, under appropriate assumptions on the drift μ, the diffusion coefﬁcient σ and
the class A, there is a constant c > 0 such that
eEM
weak = sup
f ∈A
E

f (XEM
T )

−E ( f (XT ))
 ≤c
n
(6.25)
for all f ∈A and all sufﬁciently large n. Similarly, let XMIL be the Milstein approx-
imation to X. Then, under appropriate assumptions on the drift μ, the diffusion
coefﬁcient σ and the class A, there is a constant c > 0 such that
eMIL
weak( f ) = sup
f ∈A
E

f (XMIL
T
)

−E ( f (XT ))
 ≤c
n
(6.26)
for all f ∈A and all sufﬁciently large n. This is the same rate of convergence as
for the Euler–Maruyama scheme. Thus we would expect the weak errors for the two
schemes to stay comparable as n increases.

CONTINUOUS-TIME MODELS
239
Example 6.19
Continuing from example 6.18, it is possible to numerically estimate
eEM
weak( f ) for solutions of (6.20). The exact expectation is given by
E ( f (XT )) = E ( f (X0 exp(αBT + βT ))) .
Since BT ∼N(0, T ), for simple functions f and deterministic X0 this expectation
can be calculated analytically. On the other hand, the expectation E

f (XEM
T )

can
be estimated using Monte Carlo integration.
For this example we consider the case of X0 = 1 and f (x) = 1(−∞,a](x) for some
constant a. Then we have
E ( f (XT )) = P (exp (αBT + βt) ≤a) = P

BT ≤log(a) −βt
α

.
(6.27)
Figure 6.9 shows a simulation where a is chosen such that the expectation in (6.27)
equals 0.6.
Some care is needed when performing numerical experiments of this kind: since
eweak( f ) is typically much smaller than either of the values E

f ( ˆXT )

and E ( f (XT )),
the Monte Carlo estimates for the expectation need to be performed with high accuracy
and thus require huge sample sizes. In addition, for unbounded test functions f , the
x
x
x
x
x
x
x
x
x
0.005
0.010
0.020
0.050
5e−04
2e−03 5e−03
2e−02
h
Weak error
o
o
o
o
o
o
o
o
o
x
o
Euler−Maruyama
Milstein
Figure 6.9
Weak error for the Euler–Maruyama and Milstein schemes, for the SDE
(6.20) with α = 1, β = 0.4 and T = 5. The plot shows the estimated weak error for
both methods as a function of the step size h = T/n, for different values of n. The
points marked ‘x’ are Monte Carlo estimates for eEM
weak( f ), the points marked ‘o’
are Monte Carlo estimates for eMIL
weak( f ), and the bars give 95% conﬁdence intervals
for the Monte Carlo estimates. The test function used is f (x) = 1(−∞,a](x) for some
constant a. The solid lines correspond to 0.6275/n (for the Euler–Maruyama method)
and 1.584/n (for the Milstein method), respectively.

240
AN INTRODUCTION TO STATISTICAL COMPUTING
SDE (6.20) is susceptible to problems such as the one illustrated in Figure 6.6,
potentially making Monte Carlo estimation very difﬁcult.
6.4.4.3
Qualitative behaviour of solutions
The strong and weak errors discussed so far describe convergence of numerical
approximations to the exact solutions as the discretisation parameter increases. In
this section we will discuss two effects which, for ﬁxed discretisation parameter, can
affect the qualitative behaviour of numerical solutions.
One case where numerical solutions can be qualitatively different from the
exact solution is when the exact solution is known to be positive whereas the
numerical solution may become negative. This problem could, for example, occur
when the SDE in question models a stock price or an interest rate. We illustrate
this problem here for the case of the one-dimensional Euler–Maruyama scheme:
assume that the current value of the simulation is XEM
ih = x. Then the next value is
given by
XEM
(i+1)h = x + μ(x) h + σ(x) Bi
∼N

x + μ(x)h, σ(x)2h

.
Since XEM
(i+1)h is normally distributed, it takes negative values with positive probability.
Usually, the probability of this happening is negligible, but in cases where the current
state x is close to 0 or when μ or σ is large, the problem can appear with high enough
probability to be seen in practice. This effect is illustrated in Figure 6.10.
t
(i −1)h
ih
(i + 1)h
ˆX(i−1)h
ˆXih
ˆX(i+1)h
Figure 6.10
Illustration of a case where the numerical solution of an SDE becomes
negative while the exact solution stays positive. The dashed line shows the solution of
the equation with the noise term removed, starting at (ih, ˆXih). A method such as the
Euler–Maruyama scheme will choose the next approximation point ((i + 1)h, ˆX(i+1)h)
by following the tangent of this line and then adding the random term coming from
the Brownian increment. If h is large enough, this can lead to ˆX(i+1)h < 0. The thin,
rough line gives a path of the exact SDE, starting (ih, ˆXih), for comparison.

CONTINUOUS-TIME MODELS
241
If this effect causes problems, for example in cases when μ or σ are only deﬁned
for x > 0, there are several ad hoc ways to force the numerical solution to be positive:
r One can replace negative values of ˆXih with | ˆXih| throughout the algorithm.
For example, for the Euler method, one could replace the update step by
ˆX(i+1)h =
 ˆXih + μ(ih, ˆXih) h + σ(ih, ˆXih) Bi
,
with the modulus added to keep the solution positive. This method may intro-
duce a bias, because solutions are only ever modiﬁed to take larger values.
r One can consider Yt = log(Xt) instead of X. Since the function f (x) = log(x)
has derivatives f ′(x) = 1/x and f ′′(x) = −1/x2, Ito’s formula in the form of
equation (6.14) can be used to derive and SDE for Y:
dYt = d (log(Xt))
=
μ(t, Xt)
Xt
−σ(t, Xt)2
2X2
t

dt + σ(t, Xt)
Xt
dBt
=
μ(t, eYt)
eYt
−σ(t, eYt)2
2e2Yt

dt + σ(t, eYt)
eYt
dBt
= ˜μ(t, Yt) dt + ˜σ(t, Yt) dBt,
where
˜μ(t, y) = μ(t, ey)
ey
−σ(t, ey)2
2e2y
and
˜σ(t, y) = σ(t, ey)
ey
.
Another class of problems of numerical solutions of SDEs comes from the lack
of stability of some numerical methods: it can happen that the numerical solution
‘explodes’, while the exact solution of the SDE stays bounded. This effect is illustrated
in Figure 6.11.
We illustrate the problem of numerical instability here with the help of an example.
Consider the one-dimensional SDE
dXt = −X3 dt + dBt.
The Euler–Maruyama scheme with step size h > 0 for this SDE is given by
ˆX(i+1)h = ˆXih −ˆX3
ih h + Bi = ˆXih

1 −ˆX2
ih h

+ Bi.
Assume now that the discretised solution reaches by chance a state ˆXih = x with
|x| ≥

2 + ε
h

242
AN INTRODUCTION TO STATISTICAL COMPUTING
0
200
400
600
800
1000
−4 −2
0
2
4
Xt
950
960
970
980
990
1000
−20 −10
0
10
20
t
Xt
(a)
(b)
Figure 6.11
Numerical solution of an SDE where the Euler scheme is unstable. (a)
A simulation of the SDE until the method becomes unstable. (b) A ‘zoomed in’ view
of the last few steps before the numerical solution explodes.
for some ε > 0. Then we have ˆX2
ihh ≥2 + ε. If h is small, for this x the drift −x3
will be much bigger than the diffusion term Bi and thus we ﬁnd
 ˆX(i+1)h
 ≈
 ˆXih
1 −ˆX2
ihh
 ≥(1 + ε)
 ˆXih
,
where ˆX(i+1)h and ˆXih have opposite signs. Since this implies ˆX(i+1)h > ˆXih, the
same argument applies again for the next step of the discretisation, and we ﬁnd
 ˆX(i+k)h
 ⪆(1 + ε)k ˆXih

and consequently ˆX(i+k)h diverges exponentially fast. While in theory the values
ˆX(i+k)h stay ﬁnite (but very big) numbers, in a numerical simulation the resulting
numbers will quickly leave the range of values which can be represented on a com-
puter.
In cases where numerical instability is a problem, the best solution is often
to decrease the step size h of the discretisation scheme. With decreasing h, the
probability that the approximation hits an unstable state decays normally very quickly,
so that the instability, even when present theoretically, is not seen in practice for small
enough h.

CONTINUOUS-TIME MODELS
243
6.5
Monte Carlo estimates
One situation where numerical solutions of SDEs are employed is the computation
of Monte Carlo estimates.
6.5.1
Basic Monte Carlo
Assume that X = (Xt)t∈[0,T ] is given as the solution of an SDE and that we want to
compute E ( f (X)). In order to estimate this quantity using Monte Carlo integration,
we generate independent samples X(n,1), X(n,2), . . . , X(n,N), using numerical approx-
imations to X, obtained by repeatedly solving a discretised SDE with discretisation
parameter n. Then we can use the approximation
E ( f (X)) ≈1
N
N

j=1
f (X(n, j)) = Zn,N.
(6.28)
Here we allow for the function f to depend on the whole path of X until time T . We
can choose, for example,
f (X) = sup
t∈[0,T ]
Xt
to get the maximum of a one-dimensional path, or f (X) = |XT |2 to get the second
moment of the ﬁnal point of the path.
As for all Monte Carlo methods, there is a trade-off between accuracy of the result
and computational cost. One notable feature of Monte Carlo estimation for SDEs is
that the result is not only affected by the Monte Carlo error, but also by discretisation
error.
Lemma 6.20
The computational cost of computing the Monte Carlo estimate Zn,N
from (6.28) is of order C = O(nN). The mean squared error of the estimate is
MSE(Zn,N) = σ 2
N + eweak( f )2,
where σ 2 = Var

f (X(n)
T )

is the sample variance corresponding to the endpoint of
the numerical solution of the SDE with discretisation parameter n and eweak( f ) is the
weak error of the approximation scheme used.
Proof
The cost of computing a single solution X(n, j) with discretisation parameter
n is of order O(n). For Zn,N we have to compute N such solutions and add them up,
leading to a total cost of order O(nN).

244
AN INTRODUCTION TO STATISTICAL COMPUTING
By lemma 3.12, the mean squared error of the estimator Zn,N is
MSE(Zn,N) = E

Zn,N −E ( f (XT ))
2
= Var(Zn,N) + bias(Zn,N)2.
The variance of Zn,N is given by
Var(Zn,N) =
Var

f (X(n,1)
T
)

N
= σ 2
N .
The bias of Zn,N can be found as
bias(Zn,N)
 =
E(Zn,N) −E ( f (XT ))

=
E

f (X(n,1)
T
)

−E ( f (XT ))

= eweak( f ).
Substituting these two expressions into the formula for the mean squared error com-
pletes the proof.
The variance of Zn,N, given by the term σ 2/N, corresponds to Monte Carlo error.
It decreases to 0 as the Monte Carlo sample size N increases. The bias of Zn,N,
corresponding to eweak( f )2 in the lemma, decreases to 0 as the grid parameter n
increases.
Example 6.21
Consider the process
dXt = −Xt dt + dBt
X0 = 0.
(6.29)
Assume that we want to estimate the probability that the process X exceeds the level
c > 0 before time T , that is we want to estimate
p = P

sup
t∈[0,T ]
Xt > c

.
A basic Monte Carlo estimate for this probability is obtained as follows:
(a) Choose a discretisation parameter n and let h = T/n.
(b) Simulate solutions (X( j)
ih )i=0,1,...,n of (6.29) for j = 1, 2, . . . , N. The Euler–
Maruyama scheme (algorithm 6.12) can be used for this purpose.

CONTINUOUS-TIME MODELS
245
(c) For each path X( j), determine
1(c,∞)

sup
i
X( j)
ih

=
	
1
ifX( j)
ih > c for at least one i ∈{0, 1, 2, . . . , n}
0
otherwise.
(d) Compute the estimate pMC for p as
pMC = 1
N
N

j=1
1(c,∞)(sup
i
X( j)
ih ).
A numerical experiment, using 200000 simulated paths of SDE (6.29), results in
the estimate
P

sup
t∈[0,5]
Xt > 3.2

≈1.45 · 10−4.
An estimated conﬁdence interval for this probability, obtained ignoring the bias
and only considering the standard deviation of the Monte Carlo samples, is [0.92 ·
10−4, 1.98 · 10−4].
We can optimise the parameters N and n to minimise the computational cost
required to achieve a given error. For the Euler–Maruyama scheme and the Milstein
scheme the rate of decay of the weak error is given in equation (6.25) and equation
(6.26), respectively. The weak error typically decays as a/n for some constant a > 0.
Thus, the mean squared error of the estimator Zn,N satisﬁes
MSE(Zn,N) ≈σ 2
N + a2
n2
(6.30)
where n is the grid parameter used for discretising the SDE and N is the sample size
for the Monte Carlo estimate. This error needs to be balanced against the cost of
computing the estimate Zn,N, given by
C(n, N) = bnN
(6.31)
for some constant b > 0.
Our aim is now to tune the parameters N and n to minimise the computational
cost while, at the same time, keeping the mean squared error below a speciﬁed level.
For this, we will use the following general result.
Theorem 6.22
(optimisation under constraints). Let f, g1, . . . , gn: Rd →R be
continuously differentiable functions. Deﬁne C ⊆Rd by
C =

x ∈Rd  g1(x) = g2(x) = · · · = gn(x) = 0


246
AN INTRODUCTION TO STATISTICAL COMPUTING
and let x∗∈C be a global maximum or minimum of the restriction of f to the set C,
that is f (x∗) ≥f (x) for all x ∈C. Then x∗satisﬁes
∇f (x∗) = λ1∇g1(x∗) + λ2∇g2(x∗) + · · · + λn∇gn(x∗)
(6.32)
for some λ1, . . . , λn ∈R. The numbers λi are called Lagrange multipliers.
While the criterion given in theorem 6.22 is only necessary but not sufﬁcient, it can
be used to identify candidates for maxima or minima. The relation (6.32) is a system
of d equations and the constraints gi(x∗) = 0 provide another n equations, giving
d + n equations in total. On the other hand, the unknown vector x∗has d components
and we also have to identify the Lagrange multipliers λ1, . . . , λn. Thus the total
number of unknowns, d + n, matches the number of equations. As a consequence,
in many cases the resulting set of equations has only ﬁnitely many solutions and by
a systematic search we can determine which of these solutions corresponds to the
maximum or minimum of f .
Taking n and N to be continuous variables for simplicity, we can apply the-
orem 6.22 to ﬁnd the minimum of the cost function (6.31) under the constraint
MSE(Zn,N) = ε2, where the mean squared error is given by equation (6.30). For the
partial derivatives, comprising the gradients in (6.32) we ﬁnd
∂
∂n MSE(Zn,N) = −2a2
n3 ,
∂
∂n C(n, N) = bN
and
∂
∂N MSE(Zn,N) = −σ 2
N 2 ,
∂
∂N C(n, N) = bn.
Thus, the minimum of C under the constraint MSE(Zn,N) = ε2 satisﬁes the equations
−2a2
(n∗)3 = λ · bN ∗,
−σ 2
(N ∗)2 = λ · bn∗,
where λ is the Lagrange multiplier, as well as the constraint
σ 2
N ∗+
a2
(n∗)2 = ε2.
The unique solution of this system of equations is given by
n∗=
√
3 a · 1
ε ,
N ∗= 3 σ 2
2
· 1
ε2 ,
(6.33)

CONTINUOUS-TIME MODELS
247
where λ = −4 ε5/
√
243 abσ 2, and substituting these values into (6.31) gives
C(n∗, N ∗) =
√
27 abσ 2
ε3
= O

1/ε3
.
(6.34)
This is the optimal cost for Monte Carlo estimates of E ( f (XT )) with mean squared
error ε2, where XT is the end-point of the solution of an SDE.
From equation (6.33), assuming that σ 2 and a are both of order 1, we ﬁnd that
the optimal way to balance the parameters N and n is to choose n approximately
equal to
√
N. While the variance, controlled by N, is easy to determine numerically,
the bias, controlled by n, is difﬁcult to measure. For this reason, in practice it might
make sense to choose n bigger than
√
N in order to avoid the risk of an unnoticed
bias affecting the Monte Carlo estimates for SDEs.
6.5.2
Variance reduction methods
The error in Monte Carlo estimates such as (6.28) can be signiﬁcantly reduced by
employing the variance reduction techniques from Section 3.3.
6.5.2.1
Antithetic paths
Pairs of antithetic paths can be easily generated for SDEs by using the fact that, if B
is a Brownian motion, −B is also a Brownian motion (see lemma 6.5): thus, solving
the SDE using the Brownian motions B and B′ = −B gives rise to two paths X and
X′ of the SDE. Since Bt and B′
t are negatively correlated, typically Xt and X′
t will
also be negatively correlated for all t ∈[0, T ].
Example 6.23
For illustration, Figure 6.12 (a) shows two paths of the SDE
dXt = (0.8 −Xt) dt + Xt(1 −Xt) dBt
X0 = 0,
(6.35)
(a)
(b)
0
1
2
3
4
5
0.0
0.4
0.8
t
Xt
0.3
0.5
0.7
0.9
0.3
0.5
0.7
0.9
XT
XT
'
Figure 6.12
Illustration of the antithetic variables method for SDEs. (a) Two paths
X and X′ of SDE (6.35), computed using Brownian paths B and −B. (b) A scatter
plot of 1000 pairs (XT , X′
T ), illustrating that the values are negatively correlated.

248
AN INTRODUCTION TO STATISTICAL COMPUTING
one path X computed from a Brownian path B and one path X′ computed from −B.
One can clearly see that often, if one path moves up, the other path moves down.
Figure 6.12(b) shows a scatter plot of 1000 pairs (XT , X′
T ), clearly illustrating that
the values are negatively correlated. In this example, the numerical value for the
correlation coefﬁcient is −0.787. From proposition 3.14 and proposition 3.27 we see
that for ﬁxed sample size N, the root-mean squared error of this antithetic variables
method satisﬁes
RMSEAV =
√
1 −0.787 RMSEMC = 0.461 RMSEMC.
Thus, for the same number of SDEs solved, the antithetic variables method in this
example has less than half the error than the corresponding basic Monte Carlo
estimate.
6.5.2.2
Importance sampling
In this section we will study how to apply the importance sampling method from
Section 3.3.1 to Monte Carlo estimates for SDEs. At ﬁrst this seems an impossible
task, since the solution of an SDE is a random function and not just a random number,
but it transpires that the method still works in many cases.
Consider solutions to the two SDEs
dXt = μ(t, Xt) dt + σ(t, Xt) dBt
(6.36)
and
dYt = ˜μ(t, Yt) dt + σ(t, Yt) dBt,
(6.37)
on the time interval t ∈[0, T ], where B = (Bt)t≥0 is a d-dimensional Brownian
motion, μ, ˜μ: [0, ∞) × Rd →Rd are different drift functions and the common diffu-
sion coefﬁcient σ: [0, ∞) × Rd →Rd×d is invertible. We assume that both processes
start at the same initial value X0 = Y0.
Importance sampling for SDEs is based on the following consequence of Gir-
sanov’s theorem (see e.g. Mao, 2007, theorem 2.2). Here we state the result without
proof.
Theorem 6.24
Assume that the SDEs (6.36) and (6.37) have solutions X and Y,
respectively, up to time T > 0. Let f be a function which maps paths X: [0, T ] →Rd
to real numbers. Then, under additional technical assumptions, we have
E ( f (X)) = E

f (Y) ϕ(Y)
ψ(Y)

,
(6.38)

CONTINUOUS-TIME MODELS
249
where
ϕ(Y) = exp
 T
0
a(t, Yt)−1μ(t, Yt) dYt −1
2
 T
0
μ(t, Yt)⊤a(t, Yt)−1μ(t, Yt) dt

and
ψ(Y) = exp
 T
0
a(t, Yt)−1 ˜μ(t, Yt) dYt −1
2
 T
0
˜μ(t, Yt)⊤a(t, Yt)−1 ˜μ(t, Yt) dt

with
a(t, x) = σ(t, x)σ(t, x)⊤∈Rd×d.
The relation (6.38) from the theorem replaces (3.13) in the basic importance
sampling method. When the theorem is used for importance sampling, the paths
Y will be generated using an approximation scheme for the SDE (6.37), and the
integrals in the expressions for ϕ and ψ are evaluated using the approximations (6.9)
and (6.10).
An important special case is the situation where the SDEs for X and Y are one-
dimensional, μ and ˜μ do not depend on time, and σ is constant. In this case, the
expression for ϕ simpliﬁes to
ϕ(Y) = exp
 1
σ 2
 T
0
μ(Yt) dYt −
1
2σ 2
 T
0
μ(Yt)2 dt

and ψ is the corresponding expression obtained by replacing μ with ˜μ. Thus, the
factor ϕ(Y)/ψ(Y) from (6.38) can be found as
ϕ(Y)
ψ(Y) = exp
 1
σ 2
 T
0
(μ(Yt) −˜μ(Yt)) dYt −
1
2σ 2
 T
0

μ(Yt)2 −˜μ(Yt)2
dt

.
(6.39)
Example 6.25
In example 6.21, we considered the probability that the solution of
the SDE (6.29) exceeds level c before time T . Since the drift μ(x) = −x of this SDE
drives the process towards 0, large values of Xt are very unlikely. Thus, when c is
big, a large number of Monte Carlo samples is required to see a sufﬁcient number of
cases where 1(c,∞)(supi X( j)
ih ) = 1.
To reduce the required number of samples, we can use importance sampling with
samples from a modiﬁed process Y. The process Y should be chosen so that Y is
‘similar’ to X but that the event supt∈[0,1] Yt > c happens with probability higher than
p. For this example we will use solutions of the SDE
dYt = −qYt dt + dBt
Y0
= 0
(6.40)

250
AN INTRODUCTION TO STATISTICAL COMPUTING
with constant q ∈[0, 1). Since the drift is weaker, this process will stay less close
to 0 and thus is more likely to exceed level c. The drift of Y is ˜μ(x) = −qx and
consequently we ﬁnd μ(x) −˜μ(x) = −x −(−qx) = −(1 −q)x as well as μ(x)2 −
˜μ(x)2 = x2 −q2x2 = (1 −q2)x2. Substituting these expressions into (6.39), we ﬁnd
ϕ(Y)
ψ(Y) = exp

−(1 −q)
 T
0
Yt dYt −1 −q2
2
 T
0
Y 2
t dt

.
A numerical experiment, using 200000 simulated paths of SDE (6.40), results in
the estimate
P

sup
t∈[0,5]
Xt > 3.2

≈1.55 · 10−4.
An estimated conﬁdence interval, obtained by considering the standard deviation of
the Monte Carlo samples, for this probability is [1.41 · 10−4, 1.70 · 10−4]. Comparing
these estimates with the corresponding results from example 6.21 shows that the
importance sampling estimate, for the same number of samples, has signiﬁcantly
smaller error.
Another important variance reduction technique for Monte Carlo estimates for
solutions of SDEs is described in the next section.
6.5.3
Multilevel Monte Carlo estimates
The variance reduction methods we have discussed so far are applications of the
general methods from Section 3.3 to the problem of estimating expectations for paths
of SDEs. In contrast, the multilevel Monte Carlo approach, discussed in the rest of
this section, is speciﬁc to situations where discretisation error is involved.
Multilevel Monte Carlo methods, by cleverly balancing the effects of discretisa-
tion error and Monte Carlo error, allow us to reduce the computational cost required
to compute an estimate with a given level of error. Let X be the solution of an SDE
and let ε > 0. In equation (6.34) we have seen that the basic Monte Carlo estimate
for E ( f (XT )) requires computational cost of order O(1/ε3) in order to bring the
root-mean squared error down to ε > 0. We will see that multilevel Monte Carlo
methods can reduce this cost to nearly O(1/ε2).
Let
Yi = f

X(ni)
T

where X(ni)
T
is the approximation for XT with discretisation parameter ni. We
assume that n1 < n2 < . . . < nk so that the approximations Yi get more accurate
as i increases. Also, except for the smallest values of i we expect Yi ≈Yi−1, when
computed from the same path of the underlying Brownian motion. Thus the two

CONTINUOUS-TIME MODELS
251
values will be strongly correlated. We will exploit this correlation by using a variant
of the control variates method (see Section 3.3.3): we can expand Yk in a telescopic
sum as
Yk =
k

i=1
(Yi −Yi−1)
where Y0 = 0. For large enough k we have
E ( f (XT )) ≈E (Yk) =
k

i=1
E (Yi −Yi−1) .
Since we can obtain values of a Brownian path on the coarse and on the ﬁne grid
simultaneously, each of the terms on the right-hand side can be estimated using Monte
Carlo integration: we get
E (Yi −Yi−1) ≈1
Ni
Ni

j=1

Y (i, j)
i
−Y (i, j)
i−1

.
The resulting multilevel Monte Carlo estimate for E ( f (XT )) is then
Z N1,...,Nk =
k

i=1
1
Ni
Ni

j=1

Y (i, j)
i
−Y (i, j)
i−1

.
(6.41)
Here, Y (i, j)
i
= f

X(ni,i, j)
T

and Y (i, j)
i−1 = f

X(ni−1,i, j)
T

where X(ni,i, j) and X(ni−1,i, j)
are numerical solutions of the SDE with discretisation parameters ni and ni−1,
respectively. The two solutions X(ni,i, j) and X(ni−1,i, j) are both computed using the
same Brownian motion B(i, j). The Brownian motions B(i, j) for different (i, j) are
independent.
Algorithm 6.26
(multilevel Monte Carlo estimate)
input:
a function f
an SDE on the time interval t ∈[0, T ]
k ∈N, n1 < · · · < nk and N1, . . . , Nk ∈N
randomness used:
independent Brownian paths B(i, j) for i = 1, . . . , k, j = 1, . . . , Ni
output:
an estimate for E ( f (XT )) where X solves the given SDE
1: s ←0
2: for i = 1, 2, . . . , k do
3:
si ←0

252
AN INTRODUCTION TO STATISTICAL COMPUTING
4:
for j = 1, 2, . . . , Ni do
5:
Generate a Brownian path B(i, j).
6:
Compute a solution (X(ni,i, j)
t
)t∈[0,T ] of the SDE, using discretisation
parameter ni and the Brownian path B(i, j).
7:
Y (i, j)
i
←f

X(ni,i, j)
T

8:
if i > 1 then
9:
Compute a solution (X(ni−1,i, j)
t
)t∈[0,T ] of the SDE, using discretisation
parameter ni−1 and the Brownian path B(i, j).
10:
Y (i, j)
i−1 ←f

X(ni−1,i, j)
T

11:
else
12:
Y (i, j)
i−1 ←0
13:
end if
14:
si ←si + Y (i, j)
i
−Y (i, j)
i−1
15: end for
16: s ←s + si/Ni
17: end for
18: return s
A convenient choice for the discretisation parameters ni in the algorithm is
ni = mi. Then X(ni,i, j) and X(ni−1,i, j) can be simulated jointly, where X(ni,i, j) uses
time steps of length T/ni and m of these time steps together form one time step for
the simulation of X(ni−1,i, j).
Lemma 6.27
The computational cost for obtaining the multilevel Monte Carlo
estimate Z N1,...,Nk from (6.41) is
C(N1, . . . , Nk) ∝
k

i=1
ni Ni.
The mean squared error of the estimate satisﬁes
MSE(Z N1,...,Nk) =
k

i=1
σ 2
i
Ni
+

e(nk)
weak( f )
2
,
where σ 2
i = Var(Yi −Yi−1) and e(nk)
weak( f ) is the weak error from (6.24) for discretisa-
tion parameter nk.
Proof
The statement about the computational cost follows from the fact that
the cost of simulating one path at level i is proportional to the discretisation
parameter ni.

CONTINUOUS-TIME MODELS
253
For the statement about the mean squared error we ﬁrst note that, by lemma 3.12,
the mean squared error of the estimate is
MSE(Z N1,...,Nk) = Var(Z N1,...,Nk) + bias(Z N1,...,Nk)2.
For the variance we ﬁnd
Var(Z N1,...,Nk) =
k

i=1
1
N 2
i
Ni

j=1
Var

Y (i, j)
i
−Y (i, j)
i−1

=
k

i=1
σ 2
i
Ni
and the bias is given by
bias(Z N1,...,Nk) = E

Z N1,...,Nk

−E ( f (XT ))
=
k

i=1
1
Ni
Ni

j=1

E

Y (i, j)
i

−E

Y (i, j)
i−1

−E ( f (XT ))
=
k

i=1
(E (Yi) −E (Yi−1)) −E ( f (XT ))
= E

f (X(nk)
T
)

−E ( f (XT )) .
Substituting the deﬁnition of e(nk)
weak( f ) completes the proof.
The parameters k ∈N and N1, . . . , Nk can be chosen to minimise the numerical
error for a given computational cost C. Since the weak error e(ni)
weak( f ) is independent
of the values N1, . . . , Nk, it sufﬁces to minimise
V (N1, . . . , Nk) =
k

i=1
σ 2
i
Ni
under the constraint of ﬁxed cost C(N1, . . . , Nk) = C. For simplicity, we take
N1, . . . , Nk to be continuous variables. Then, using minimisation under constraints
(theorem 6.22), we ﬁnd that the minimum satisﬁes the equations
0 = ∂V
∂Ni
(N1, . . . , Nk) −λ ∂C
∂Ni
(N1, . . . , Nk) = −σ 2
i
N 2
i
−λcni
fori = 1, 2, . . . , k, where λ is the Lagrange multiplier. The solution of these equations
is
Ni ∼

σ 2
i /ni
(6.42)

254
AN INTRODUCTION TO STATISTICAL COMPUTING
for all i = 1, 2, . . . , k, where the common constant is chosen so that the condition
C(N1, . . . , Nk) = C is satisﬁed.
Since
Yi −Yi−1 =

f (X(ni)
T ) −f (XT )

−

f (X(ni−1)
T
) −f (XT )

,
the variances σ 2
i depend on the path-wise accuracy of the approximations X(ni)
T
and
X(ni−1)
T
for XT (i.e. on the strong error of the numerical scheme for solving the SDE),
as well as on the regularity of the function f . For the Euler–Maruyama scheme and
Lipschitz continuous f , one can show σ 2
i = O(1/ni).
Example 6.28
Let ni = mi for some m ∈N and assume σ 2
i ∼1/ni as well as
e(n)
weak ∼1/n. The condition on the weak error is, for example, satisﬁed for the Euler–
Maruyama scheme and the Milstein scheme. In this case, the optimality condition
(6.42) turns into Ni ∼1/ni.
For ε > 0, set
k ≈log(ε−1)
log(m)
and
Ni ∼
k
niε2 .
Here, k is chosen in order to get a bias of order O(ε2) and the form of the sample
sizes Ni is dictated by (6.42). Then we have
k

i=1
σ 2
i
Ni
∼
k

i=1
1
ni
· niε2
k
= ε2
and

e(nk)
weak( f )
2
∼1
n2
k
= m−2k ≈exp

−2log(ε−1)
log(m) log(m)

= ε2
and thus, by lemma 6.27, the mean squared error satisﬁes
MSE(Z N1,...,Nk) ∼ε2,
On the other hand, the computational cost of the resulting estimate is
C(N1, . . . , Nk) ∼
k

i=1
ni Ni ∼
k

i=1
ni
k
niε2 = k2
ε2 ∼log(ε)2
ε2
.
Since log(ε)2 grows only very slowly as ε ↓0, the cost of the method is nearly
O(1/ε2). For small values of ε, this computational cost will be much lower than the
cost O(1/ε3), from (6.34), for the basic Monte Carlo estimate with the same error.

CONTINUOUS-TIME MODELS
255
The results from this section suggest the following procedure for choosing the
parameters of a multilevel Monte Carlo method: ﬁrst, choose nk large enough that the
weak error of solutions with discretisation parameter nk is expected to be smaller than
the level of error we are willing to tolerate. Next, choose discretisation parameters
n1 < n2 < · · · < nk for the intermediate levels. Normally, these value are chosen
so that ni ≈mni−1 for all i and some constant m. Experiments (e.g. example 6.30)
indicate that it is best not to use too many intermediate levels. Finally, let Ni ≈L/ni
where the constant L ≥nk is chosen large enough to keep the Monte Carlo variance
of the samples small. The execution time of the program will be proportional to L,
the Monte Carlo variance will decay as 1/L.
6.6
Application to option pricing
In this section we illustrate the techniques from Chapter 6 with the help of an example
from ﬁnancial mathematics. In the Heston model (Heston, 1993), the joint evolution
of a stock price St and of the corresponding volatility √Vt at time t is described by a
system of two stochastic differential equations:
dSt = r St dt + St
√Vt dB(1)
t ,
dVt = λ(σ 2 −Vt) dt + ξ√Vt

ρ dB(1)
t
+

1 −ρ2 dB(2)
t

.
(6.43)
Here, r ≥1 (the interest rate), λ > 0, σ 2 > 0, ξ > 0 and the correlation ρ ∈(−1, 1)
are ﬁxed parameters and B(1) and B(2) are two independent Brownian motions. The
SDEs start at time t = 0 (assumed to be the current time) with given values S0 and
V0. Our aim is to numerically estimate the price of a call option with expiry time
T > 0 and strike price K > 0. This price is given by the expectation
C = E

e−rT max(ST −K, 0)

,
(6.44)
where ST is the solution of (6.43) at time T .
We start our analysis by rewriting (6.43) as a two-dimensional SDE. By
lemma 6.4, the process Bt = (B(1)
t , B(2)
t ) is a two-dimensional Brownian motion.
Deﬁning Xt = (St, Vt), we have
dX = μ(Xt) dt + σ(Xt) dBt
with
μ(x) =

rx1
λ(σ 2 −x2)

and
σ(x) =
 x1
√x2
0
ρξ√x2
(1 −ρ2)ξ√x2


256
AN INTRODUCTION TO STATISTICAL COMPUTING
for all x ∈R2
+. Thus, the system (6.43) of SDEs ﬁts into the framework discussed in
this chapter. To simulate solutions of this SDE numerically, we can use the Euler–
Maruyama scheme described in algorithm 6.12. Specialised to (6.43), the algorithm
has the following form.
Algorithm 6.29
(Euler–Maruyama scheme for the Heston model)
input:
interest rate r ≥1
parameters λ > 0, σ 2 > 0, ξ > 0 and ρ ∈(−1, 1)
initial values S0, V0 > 0
time horizon T ≥0
discretisation parameter n ∈N
randomness used:
B( j)
i
∼N(0, h) i.i.d. for i = 0, . . . , n −1 and j ∈{1, 2}, where
h = T/n
output:
approximate solutions (S(n)
ih )i=0,...,n and (V (n)
ih )i=0,...,n to (6.43)
1: h ←T/n
2: S(n)
0
←S0
3: V (n)
0
←V0
4: for i = 0, 1, . . . , n −1 do
5:
S(n)
(i+1)h ←S(n)
ih + r S(n)
ih h + S(n)
ih

V (n)
ih B(1)
i
6:
V (n)
(i+1)h ←V (n)
ih + λ

σ 2 −V (n)
ih

h + ξ

V (n)
ih

ρB(1)
i
+

1 −ρ2B(2)
i

6: end for
7: return (S(n)
ih )i=0,1,...,n and (V (n)
ih )i=0,1,...,n
This algorithm can easily be implemented on a computer. One problem with this
approach is that there is the possibility that the numerical approximations for S and
V may take negative values. Details of this problem are discussed near the end of
Section 6.4.4. Once negative values occur for Vt, the algorithm cannot continue since
the next iteration will involve computing the value √Vt, which is not deﬁned for
Vt < 0. One solution to this problem is to replace

V (n)
ih with

|V (n)
ih | throughout the
algorithm. Then the solution for V can still take negative values, but the algorithm can
continue and due to the drift term the process V will return to positive values soon.
A pair (S, V ) of paths of a simulation using this algorithm is shown in Figure 6.13.
Now that we are able to simulate paths from the Heston model, our next aim
is to use these paths to estimate the expectation C from (6.44). For this task, we
employ Monte Carlo estimation as described in Section 6.5. Given a discretisation
parameter n and a Monte Carlo sample size N, the basic Monte Carlo estimate can
be computed as follows:
(a) Simulate solutions (S( j)
ih )i=0,1,...,n to (6.43) for j = 1, 2, . . . , N, indepen-
dently, using algorithm 6.29 with step size h = T/n.

CONTINUOUS-TIME MODELS
257
1 2 3 4 5 6 7
St
Vt
0
1
2
3
4
5
0
1
2
3
t
(a)
(b)
Figure 6.13
A pair (St, Vt) of paths from the Heston model (6.43). The evolution
of (a) the stock price St and (b) the instantaneous variance Vt is given by the
SDEs (6.43). The solutions were simulated using the Euler–Maruyama scheme from
algorithm 6.29.
(b) Compute the estimate CMC for C as
CMC = 1
N
N

j=1
f

S( j)
T

,
where f (s) = e−rT max(s −K, 0) for all s ∈R.
As an example, the histogram in Figure 6.14 shows the distribution of 10000 samples
of S( j)
T . A Monte Carlo estimate for C can be obtained by applying f to each of these
samples and then averaging the results.
From Section 6.5.1 we know that the computational cost of computing CMC is
proportional to nN and the mean squared error satisﬁes
MSE(CMC) ≈σ 2
N 2 +

e(n)
weak( f )
2
,
(6.45)
where σ 2 = Var

f (S(n)
T )

and e(n)
weak( f ) is the weak error for the Euler–Maruyama
discretisation with discretisation parameter n. From (6.33) we know that the optimal
balance of N and n is approximately to choose N ≈n2. While the weak error is
difﬁcult to determine, the mean squared Monte Carlo error σ 2/N 2 can be easily
estimated together with C, by taking the sample variance of the Monte Carlo sample.

258
AN INTRODUCTION TO STATISTICAL COMPUTING
ST
Density
0
2
4
6
8
10
12
0.0
0.1
0.2
0.3
0.4
Figure 6.14
A histogram showing the distribution of the stock price ST at time
T = 1, obtained by generating 10 000 samples with initial values S0 = 1, V0 = 0.16
and parameters r = 1.02, λ = 1, σ = 0.5, ξ = 1 and ρ = −0.5.
In order to compute estimates faster or more accurately, we can use the variance
reduction methods from Section 6.5.2. As an example, here we consider the multi-
level Monte Carlo method (see Section 6.5.3). From lemma 6.27 we know that the
computational cost of this method is proportional to k
i=1 ni Ni and the mean squared
error of this method satisﬁes
MSE(ZMLMC) =
k

i=1
σ 2
i
Ni
+

e(nk)
weak( f )
2
,
(6.46)
where ni is the discretisation parameter on level i, Ni is the number of Monte Carlo
samples on level i and σ 2
i is the variance of these samples. From example 6.28 we
know that for the Euler method the optimal balance between Ni and ni is to choose
Ni proportional to 1/ni.
As for the mean squared error of the basic Monte Carlo estimate, the bias,
controlled by the weak error e(nk)
weak( f ), is difﬁcult to estimate while the Monte Carlo
variance k
i=1 σ 2
i /Ni can be estimated together with C at nearly no extra cost.
If the discretisation parameter n for the Monte Carlo estimate is the same as the
discretisation parameter nk for the ﬁnest grid in the multilevel estimate, the two bias
terms in (6.45) and (6.46) are identical. Thus, for n = nk the difference in mean
squared error can easily be estimated numerically.
Example 6.30
We compare the Monte Carlo estimate with n = 256 and N = 2562
to different multilevel Monte Carlo estimates, with ni = mi. The multilevel estimates
we consider correspond to m = 2, m = 4 and m = 16. The corresponding numbers
of levels are chosen so that nk = n, that is we consider k = 8, k = 4 and k = 2.

CONTINUOUS-TIME MODELS
259
The Monte Carlo sample sizes Ni are taken to be Ni = L/ni, where L is chosen so
that the computation time for the corresponding multilevel estimate is close to the
computation time for the basic Monte Carlo estimate. The results are summarised as:
Method
m
Estimate
Error
Time
MC
—
0.1138
0.7965 · 10−6
1.000
MLMC
2
0.1137
1.1570 · 10−6
1.011
MLMC
4
0.1145
0.5315 · 10−6
1.004
MLMC
16
0.1133
0.4532 · 10−6
1.013
The column ‘Time’ gives the computation time (measured CPU time), relative to
the time for the Monte Carlo estimate and ‘Error’ lists the Monte Carlo variance of
the estimate.
The table clearly shows that, in this example, the mean squared error for m = 4
and m = 16 is smaller than the mean squared error for the basic Monte Carlo estimate,
indicating that the estimate from these two methods will be more accurate than the
basic Monte Carlo estimate, while taking the same amount of time to compute. The
theory in Section 6.5.2 shows that the advantage of multilevel methods will become
more pronounced as n increases; for larger n, the multilevel method with m = 2 will
also become more accurate than the corresponding Monte Carlo method.
6.7
Summary and further reading
In this chapter we have discussed continuous-time stochastic processes, including
Brownian motion and the solutions of SDEs. Following the topics of interest for this
book, we have focused on computational aspects, leaving out most of the underlying
theory. Many texts about continuous-time stochastic processes are available to provide
more theoretical detail. Brownian motion is, for example, covered in Rogers and
Williams (2000), Karatzas and Shreve (1991) and, in great detail, in M¨orters and
Peres (2010) and Borodin and Salminen (1996). SDEs are covered in textbooks
such as Mao (2007), Karatzas and Shreve (1991) and Øksendal (2003), numerical
methods are discussed in Kloeden and Platen (1999) and in the very accessible review
by Higham (2001).
In the second part of the chapter, simulation of continuous-time processes gave
the opportunity to review the Monte Carlo methods from Chapter 3 in a different
context: we considered both basic Monte Carlo estimates and the variance reduction
techiques from Section 3.3 for continuous-time models. Finally, we have studied the
multilevel Monte Carlo method which is well-adapted to the problem of computing
Monte Carlo estimates in the presence of discretisation errors. More details about
multilevel Monte Carlo estimates can be found in the publications by Giles (2008a,b).

260
AN INTRODUCTION TO STATISTICAL COMPUTING
Exercises
E6.1
Write a program to simulate the values of a one-dimensional Brownian
motion at times t1 < t2 < . . . < tn. Test your program by plotting the graph
of one path of a Brownian motion.
E6.2
Write a program to simulate a path of a two-dimensional Brownian motion.
Test your program by generating a plot of one such path, similar to the one
in Figure 6.2.
E6.3
Write a function which takes time t as an argument and computes the value
Bt of a Brownian path. Repeated calls to the function should return samples
for the same Brownian path.
E6.4
Write a program to sample paths of a one-dimensional Brownian bridge
between (r, a) and (t, b), where r < t are times and a, b ∈R.
E6.5
Write a program to simulate paths of a geometric Brownian motion X until a
time T > 0, for given parameters α, β, initial value X0, and time horizon T .
Test your program by generating plots of X for different values of α and β.
E6.6
Use Monte Carlo integration with a sample size of N = 106 to estimate the
expectation of Xt = exp(Bt −t/2) for t ≥0. By experiment, determine the
range of t-values where this method results in reasonable estimates.
E6.7
Implement the Euler–Maruyama scheme for computing an approximate solu-
tion of the SDE (6.17). Test your program by generating plots of solutions X
with drift
μ(t, x1, x2) =

x2
x1(1 −x2
1)

and diffusion coefﬁcient
σ(t, x1, x2) = s(x1, x2)
1
0
0
1

where
s(x1, x2) =
	0.2
if x1, x2 > 0 and
0.03
otherwise.
E6.8
Implement the Milstein scheme for computing approximate solutions to
SDEs of the form (6.21). Test your program by simulating solutions of
dXt = 0.1 dt + Xt dB
with initial condition X0 = 1.

CONTINUOUS-TIME MODELS
261
E6.9
Implement a Monte Carlo method to estimate the strong error for the
Euler–Maruyama and Milstein schemes, when simulating paths of a geo-
metric Brownian motion. Use the resulting program to recreate the plot in
Figure 6.8.
E6.10
Implement a program to use Monte Carlo estimation to estimate the
weak error of the Euler–Maruyama and Milstein methods, as described in
example 6.19. Use your program to recreate the plot in Figure 6.9. Note, the
resulting program will probably take a long time to run!
E6.11
Use Monte Carlo estimation to estimate the probability that the solution X
of the SDE (6.29) exceeds the level c = 2.5 before time T = 5. Determine
the approximate root-mean squared error of your estimate.
E6.12
Use importance sampling to estimate the probability that the solution X of
the SDE (6.29) exceeds the level c = 2.5 before time T = 5. Determine an
approximate conﬁdence interval for the probability in question.
E6.13
Write a program to simulate paths (St)t∈[0,T ] and (Vt)t∈[0,T ] from the Heston
model (6.43).
E6.14
Write a program to compute Monte Carlo estimates for the price C
of a call option with expiry time T > 0 and strike price K > 0, where
the underlying stock is described by the Heston model (6.43).
Test your program by determining an estimate for C for the case where
S0 = 1 and V0 = 0.16, and where the parameters in the Heston model are
r = 1.02, λ = 1, σ = 0.5, ξ = 1 and ρ = −0.5. Ignoring the bias, estimate
the mean squared Monte Carlo error of your estimate.
E6.15
Write a program to compute multilevel Monte Carlo estimates for the price C
of a call option with expiry time T > 0 and strike price K > 0, where the
underlying stock is described by the Heston model (6.43). Test your program
by comparing the result with the result from exercise E6.14.

Appendix A
Probability reminders
In this text we assume that the reader is familiar with the basic results of probability.
For reference, and to ﬁx notation, this chapter summarises some important concepts
and results.
A.1
Events and probability
The basic objects in probability theory are events and random variables. Typically
events are denoted by capital letters such as A, B and C and we write P(A) for the
probability that an event A occurs. Random variables are typically denoted by upper
case letters such as X, Y, Z and they can take values either in the real numbers R, in
the Euclidean space Rd or even in more general spaces. Random variables are often
used to construct events; for example {X < 3} denotes the event that X takes a value
which is smaller than 3 and we write P(X < 3) for the probability of this event. If X
is a random variable taking values in some set, and if f is a function on this set, then
f (X) is again a random variable.
Each random variable has a distribution or probability distribution, which com-
pletely describes its probabilistic behaviour. Special probability distributions are
often designated by calligraphic upper case letters, sometimes with parameters given
in brackets, for example N(μ, σ 2) for the normal distribution with mean μ and
variance σ 2 or U[a, b] for the uniform distribution on the interval [a, b]. General
distributions are often designated by P or μ. We write
X ∼P
to state that a random variable X has distribution P. For real-valued random variables,
the distribution can always be completely described by a distribution function.
An Introduction to Statistical Computing: A Simulation-based Approach, First Edition. Jochen Voss.
© 2014 John Wiley & Sons, Ltd. Published 2014 by John Wiley & Sons, Ltd.

264
APPENDIX A: PROBABILITY REMINDERS
Deﬁnition A.1
The cumulative distribution function (CDF) of a random variable
X on R is given by
F(a) = P(X ≤a)
for all a ∈R.
Often the CDF of a random variable is simply referred to as the ‘distribution func-
tion’, omitting the term ‘cumulative’ for brevity. Distribution functions are normally
denoted by capital letters such as F or FX (where the subscript denotes which ran-
dom variable this is the CDF of), occasionally Greek letters such as  are used. We
sometimes write X ∼F (slightly abusing notation) to indicate that the distribution
of X has distribution function F.
Deﬁnition A.2
A random variable X has probability density f , if
P

X ∈A

=

A
f (x) dx
(A.1)
for every set A.
Often the probability density of a random variable X is referred to just as the
density of X. While every random variable on R has a distribution function F, a
density f may or may not exist. If f exists, then it can be found as the derivative
of the CDF, that is f = F′. Probability densities are typically denoted by Roman or
Greek letters such as f, g, p, ϕ and ψ.
One important property of probability densities is that they are not uniquely
deﬁned. If the density ϕ is only changed on sets which are small enough to not affect
the value of the integral in (A.1), for example in a ﬁnite number of points, the changed
density will still describe the same distribution.
Example A.3
The uniform distribution U[0, 1] on the interval [0, 1] has density
f (x) =
1
if x ∈[0, 1] and
0
otherwise.
Since we can change f at individual points without changing the distribution, we can
equivalently use densities such as
˜f (x) =
1
if x ∈(0, 1) and
0
otherwise
to describe the distribution U[0, 1]. The corresponding distribution function is
F(a) =
 a
−∞
f (x) dx =
⎧
⎨
⎩
0
if a < 0
a
if 0 ≤a < 1 and
1
otherwise.

APPENDIX A: PROBABILITY REMINDERS
265
Note that the CDF F is uniquely determined; we get the same result if we compute
F using the alternative density ˜f .
Example A.4
The standard normal distribution N(0, 1) has density
f (x) =
1
√
2π
e−x2/2
and distribution function
F(x) =
1
√
2π
 x
−∞
e−y2/2 dy.
The integral in the CDF cannot be evaluated explicitly, but many programming
languages provide functions to evaluate F numerically.
Example A.5
The exponential distribution Exp(λ) has density
f (x) =
λe−λx
if x ≥0 and
0
if x < 0.
The distribution function is
F(x) =
1 −e−λx
if x ≥0 and
0
if x < 0.
Example A.6
The value X ∈{1, 2, 3, 4, 5, 6} of a single dice throw has no density.
Its distribution function is
F(x) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
0
for x < 1
1/6
for 1 ≤x < 2
2/6
for 2 ≤x < 3
3/6
for 3 ≤x < 4
4/6
for 4 ≤x < 5
5/6
for 5 ≤x < 6 and
1
for 6 ≤x.
The most important properties of densities are given by the following charac-
terisation: a function f is a probability density if and only if it satisﬁes the two
properties:
(a) f ≥0; and
(b) f is integrable with

f (x) dx = 1.

266
APPENDIX A: PROBABILITY REMINDERS
Sometimes a density f is only known up to a constant Z, that is we know the function
g(x) = Z f (x) for all x, but we do not know f and the value of Z. In these cases, the
second property listed can be used to ﬁnd Z: if we let
Z = Z

f (x) dx =

Z f (x) dx =

g(x) dx,
then
f (x) = 1
Z g(x)
is a probability density. In this context, Z is called the normalisation constant for the
unnormalized density g.
Deﬁnition A.7
Two random variables X and Y are independent of each other if
P(X ∈A, Y ∈B) = P(X ∈A)P(Y ∈B)
for all sets A and B. More generally, random variables X1, . . . , Xn are independent
if
P

Xi ∈Ai for i = 1, . . . , n

=
n
i=1
P(Xi ∈Ai)
for all sets A1, . . . , An.
If the random variable Xi has density fi for i = 1, 2, . . . , n, then we can charac-
terise independence of the Xi via their densities: X1, . . . , Xn are independent if and
only if the joint density f of X1, . . . , Xn is of the form
f (x1, . . . , xn) =
n
i=1
f (xi).
A.2
Conditional probability
The conditional probability of an event A, given another event B with P(B) > 0, is
deﬁned as
P(A|B) = P

A ∩B

P(B)
,
(A.2)

APPENDIX A: PROBABILITY REMINDERS
267
where P

A ∩B

is the probability that both A and B occur simultaneously. The
same relation multiplied by P(B) is known as Bayes’ rule:
P

A ∩B

= P(A|B)P(B).
(A.3)
Often, the event and condition in a conditional probability concern the distribution
of a random variable X: if P(X ∈B) > 0 we can consider P(X ∈A|X ∈B). For
ﬁxed B, the conditional distribution PX|X∈B of X given X ∈B, deﬁned by
PX|X∈B(A) = P(X ∈A|X ∈B),
(A.4)
is itself a probability distribution.
The random variable X here can take values on an arbitrary space. By taking X to
be a vector, X = (X1, X2) ∈R2 say, and by choosing A = A1 × R and B = R × B2,
we get
P(X ∈A|X ∈B) = P(X1 ∈A1|X2 ∈B2).
Using this idea, results such as proposition 1.26 can also be applied to the distribution
of a random variable X, conditioned on the values of a different random variable Y.
If the pair (X, Y) ∈Rm × Rn has a joint density f (x, y), it is also possible to
consider X conditioned on the event Y = y. Since the event Y = y has probability 0,
deﬁnition (A.2) can no longer be used; instead, one deﬁnes the conditional density
of X given Y = y as
fX|Y(x|y) =
 f (x,y)
fY (y)
if fY(y) > 0 and
π(x)
otherwise,
(A.5)
where
fY(y) =

f (˜x, y) d ˜x
is the density of Y and π is an arbitrary probability density. The choice of π in the
deﬁnition does not matter, since it is used only for the case fY(y) = 0, that is when
conditioning on cases which never occur. The function fX|Y(x|y) deﬁned in this way
satisﬁes

fX|Y(x|y) dx = 1
for all y and

A

B
fX|Y(x|y) fY(y) dy dx =

A

B
f (x, y) dy dx = P(X ∈A, Y ∈B).

268
APPENDIX A: PROBABILITY REMINDERS
The latter relation is the analogue of Bayes’ rule (A.3) for conditional densities.
A.3
Expectation
Real-valued random variables X often (but not always) have an expectation, which
we denote by E(X). If X only takes ﬁnitely many values, say x1, . . . , xn, then the
expectation of X is given by
E(X) =
n

i=1
xi P(X = xi).
If the distribution of X has a density f , the expectation of X can be computed as
E(X) =

x f (x) dx.
Similarly, if ϕ is a function, the expectation of the random variable ϕ(X) can be
computed as
E

ϕ(X)

=

ϕ(x) f (x) dx.
(A.6)
In cases where the integral on the right-hand side can be solved explicitly, this formula
allows expectations to be computed analytically.
Probabilities of events involving X can be rewritten as expectations using the
indicator function of the event: the indicator function of the event {X ∈A} is the
random variable 1A(X) given by
1A(x) =
1
if x ∈A and
0
otherwise.
(A.7)
Using the deﬁnition of the expectation, we ﬁnd
E(1A(X)) = 1 · P(X ∈A) + 0 · P(X /∈A) = P(X ∈A)
and from (A.6) we get
P(X ∈A) = E

1A(X)

=

1A(x) f (x) dx,
where f is the density of X. Similarly, if ϕ(X) is a function of X, we have
P

ϕ(X) ∈A

= E

1A

ϕ(X)

=

1A

ϕ(x)

f (x) dx.
(A.8)

APPENDIX A: PROBABILITY REMINDERS
269
A.4
Limit theorems
In this section we cite, for reference, two well-known limit theorems which all Monte
Carlo methods rely on heavily: the law of large numbers and the central limit theorem.
The strong law of large numbers allows to approximate an expectation by the
average of a large i.i.d. sample. This approximation forms the basis of the Monte
Carlo methods discussed in Chapter 3.
Theorem A.8
(strong law of large numbers). Let (Xn)n∈N be a sequence of i.i.d.
random variables with expectation μ. Then
lim
n→∞
1
n
n

i=1
Xi = μ
with probability 1.
This theorem is due to Kolmogorov and it can be found in most textbooks about
probability, for example as theorem 20.2 in Jacod and Protter (2000) or as 4.3S in
Williams (2001).
From the law of large numbers we know that
1
n
n

i=1

Xi −μ

−→0
as n →∞. The central limit theorem, given below, is a reﬁnement of this result: it
describes the ﬂuctuations around this limit. The central limit theorem can for example
be used to analyse the error of numerical methods based on the law of large numbers.
Theorem A.9
(central limit theorem). Let (Xn)n∈N be a sequence of independent
random variables with expectation μ and ﬁnite variance σ 2 > 0. Then we have
1
√n
n

i=1

Xi −μ

d
−−→N(0, σ 2),
(A.9)
where
d
−−→denotes convergence in law.
In the central limit theorem, the ‘convergence in law’ in equation (A.9) is equiv-
alent to the statement
P
 1
√n
n

i=1

Xi −μ

∈[a, b]

−→
1
√
2πσ 2
 b
a
exp

−x2
2σ 2

dx

270
APPENDIX A: PROBABILITY REMINDERS
as n →∞for all a, b ∈R with a ≤b. Again, this result can be found in most
textbooks about probability, for example as theorem 21.1 in Jacod and Protter (2000)
or in Section 5.4 of Williams (2001).
A.5
Further reading
A concise and rigorous exposition of basic probability is given in Jacod and Protter
(2000). A longer introduction, with a view towards applications in statistics, can be
found in Williams (2001) and a classical text is the book by Feller (1968).

Appendix B
Programming in R
This appendix contains a short introduction to programming with the R program-
ming language (R Development Core Team, 2011). R can be downloaded from the
R homepage at http://www.r-project.org/ and there are many online tutorials
available which describe how to install and use the R system. Therefore, we assume
that the reader has access to a working R installation and already knows how start the
program and how to use the built-in help system. The presentation here will focus on
aspects of the language which are relevant for statistical computing.
B.1
General advice
One of the most important points to understand when learning to program is the
distinction between learning to program and learning a programming language.
Learning to program involves understanding how to approach and structure a problem
in order to turn it into an algorithm which a computer can execute. This process often
requires that the problem is rephrased or broken down into smaller subproblems.
Becoming a proﬁcient programmer is a slow process which requires a lot of practice
and which can take a long time. In contrast, once you already know how to program,
learning a new programming language is relatively easy. Typically this just requires
learning a small number (normally much less than 100) of commands and rules.
This appendix gives an introduction to both, the basics of programming and of the
programming language R.
While it may be possible to learn the mathematical contents of this book just by
reading and understanding the text, programming can only be learned by practising
a lot (just like learning to juggle or learning to play a musical instrument). One of
the main reasons for this is that attention to detail is crucially important in computer
programming. While a typo in a mathematical argument will normally just make the
argument a bit more difﬁcult to follow, a single typo in a computer program will
typically cause the program to abort with an error or to return nonsensical results.
An Introduction to Statistical Computing: A Simulation-based Approach, First Edition. Jochen Voss.
© 2014 John Wiley & Sons, Ltd. Published 2014 by John Wiley & Sons, Ltd.

272
APPENDIX B: PROGRAMMING IN R
The text includes numerous exercises which allow you to practise your program-
ming skills. Another way to train your programming skills is to read and understand
other people’s programs; but only after you have written the corresponding program
yourself! For this purpose, the text includes answers for all of the exercises. But,
again, it is not possible to learn to program by just reading programs (just as it is
impossible to become a good violinist only by watching other people play the violin),
so it is important that you try the exercises yourself before looking at the answers.
Finally, and as already remarked, learning to program takes a long time, so make sure
that you commit enough time to learning and practising.
B.2
R as a Calculator
In this section we discuss basic use of R and introduce some fundamental concepts.
Many of the topics discussed here will be covered in more detail in the following
sections.
You interact with the R system by entering textual commands, and the system
then reacts on these commands. A simple example of a command is
3 + 4
This command asks R to add the numbers 3 and 4, the result 7 is printed to the screen.
After you try the above command in R, you should see the following two lines:
> 3 + 4
[1] 7
Your command is preceded by > and the result is preceded by [1]. We use this
distinction throughout the appendix: listings starting with the character > show both
your commands and the resulting output. Listings not starting with the character >
only show the commands without giving the output. You can (and should!) always
run the commands yourself to see the result.
An example of a more complicated command is
plot(c(1,2,3,4), c(5,2,1,6), xlab="x", ylab="y", type="o")
(B.1)
Try this example yourself! If everything worked, a graph such as the one in Figure B.1
will appear on the screen.
R processes commands one at a time. Commands can be split over more than one
line, for example a plot command could look as follows:
plot(c(1,2,3,4), c(5,2,1,6), type="o",
xlab="really long label for the x-axis", ylab="y")
When you enter this command into R, nothing will happen when you enter the ﬁrst
line (since R notices that the ﬁrst bracket is not yet closed, so the command cannot be

APPENDIX B: PROGRAMMING IN R
273
●
●
●
●
1.0
1.5
2.0
2.5
3.0
3.5
4.0
1
2
3
4
5
6
x
y
Figure B.1
The graph generated by the command in listing (B.1). The circles cor-
respond to the x and y coordinates provided in the command, the axis labels are as
speciﬁed.
complete). Only after the command is completed by the second line, the plot appears.
For programming, we need to split the solution to a problem into steps which are
small enough that each step can be performed in a single command.
B.2.1
Mathematical operations
Some of the simplest commands available are the ones which correspond directly
to mathematical operations. In the ﬁrst example of this section, we could just type
3 + 4 to compute the corresponding sum. Table B.1 lists the R equivalent of the most
important mathematical operations.
B.2.2
Variables
In R you can use variables to store intermediate results of computations. The following
transscript illustrates the use of variables:
> a <- 1
> b <- 7
> c <- 2
> root1 <- (-b + sqrt(b^2 - 4*a*c)) / (2*a)
> root2 <- (-b - sqrt(b^2 - 4*a*c)) / (2*a)
> root1
[1] -0.2984379
> root2
[1] -6.701562
> root1*root2
[1] 2

274
APPENDIX B: PROGRAMMING IN R
Table B.1
List of some commonly used mathematical operations in R.
Operation
Example
R code
Addition
8 + 3
8 + 3
Subtraction
8 −3
8 - 3
Multiplication
8 · 3
8 * 3
Division
8/3
8 / 3
Power
83
8 ^ 3
Modulus
8 mod 3
8 %% 3
Absolute value
|x|
abs(x)
Square root
√x
sqrt(x)
Exponential
ex
exp(x)
Natural logarithm
log(x)
log(x)
Sine
sin(2πx)
sin(2 * pi * x)
Cosine
cos(2πx)
cos(2 * pi * x)
You can freely choose names for your variables, consisting of letters, digits and the
dot character (but starting with a letter), and you can assign values to variables using
the assignment operator <-. After a value is assigned to a variable, the name of the
variable can be used as a shorthand for the assigned value.
Since variable names in R cannot contain accented or greek characters, some
creativity is required when translating a mathematical formula into R code: instead
of Xk we could, for example, write Xk and instead of ˜α we could, for example, write
alpha.tilde.
There is a subtle difference between the use of variables in mathematics and in
programming. While in mathematics expressions like x = x + 1 are not very useful,
the corresponding expression x <- x+1 in R has a useful meaning:
> x <- 6
> x <- x + 1
> x
[1] 7
What happens here is that in the assignment x <- x+1, the right-hand side x + 1
is evaluated ﬁrst: by the rules for the use of variables, x is replaced by its value 6,
and then 6 + 1 is evaluated to 7. Once the value to be assigned is determined, this
value (the 7) is assigned to x. Consequently, the effect of the command x <- x+1 is
to increase the value stored in the variable x by 1.
An alternative notation for assigning a value to a variable in R is using the operator
=. The R statements x <- 1 and x = 1 are equivalent. To avoid confusion with the
equality sign from mathematics, we use the <- operator to assign values to variables
throughout. One consequence of the fact that <- indicates an assignment, that is some

APPENDIX B: PROGRAMMING IN R
275
action the computer will perform, is that the order of commands can matter when
assignments are involved:
x <- 3
y <- x + 1
sets x to 3 and y to 4, whereas
y <- x + 1
x <- 3
also sets x to 3 but sets y to whatever value x previously had, plus 1. If the value of
x has not been previously set, an error message to this effect will be shown when the
line y <- x + 1 is executed.
The main use of variables is to store data and results of computations for later
reference. This has several advantages: ﬁrst, once a value is stored, the variable name
can be used to refer to this value, potentially saving much typing and making the
program shorter. If a descriptive name is chosen for the variable, this can also make
the intention of a command much clearer to human readers. Secondly, if the result of
a time-consuming computation is stored in a variable, the result from the variable can
be reused without performing the computation again. Thus, sometimes the speed of
a program can be greatly improved by storing results in variables instead of redoing
the same computation over and over again.
B.2.3
Data types
Every object in R represents one of a handful of possible ‘data types’. In the examples
above we have already seen numbers and strings (short for ‘character strings’).
B.2.3.1
Numbers
The basic data type in R are numbers. Most of the time, numbers in R programs are
written in the same way as that used in mathematics, but there are a few aspects to
be aware of. These mostly relate to very small or very large numbers.
A special notation is used in R (and in many other programming languages) to
denote multiplication with a power of 10: the R code xey, where x and y are ordinary
numbers, stands for x · 10y. Thus, 1e6 is a shorthand notation for one million and
1.3e-5 stands for 0.000013. The number before the symbol e is allowed to be a
decimal fraction, but the number after the e must be an integer. This notation is
widely used and R uses it, for example, when it needs to print very large or very
small numbers. For example the output of the following command tells us that e100
is approximately equal to 2.69 · 1043:
> exp(100)
[1] 2.688117e+43

276
APPENDIX B: PROGRAMMING IN R
An important issue to keep in mind is the fact that R, like any programming
language, has only limited precision when storing numbers: the internal representation
of a number may be affected by truncation error, slightly changing the value of the
number stored. As a result, very small positive numbers are rounded to 0 in the
internal representation, and very large values are represented as ∞:
> exp(-10000)
[1] 0
> exp(10000)
[1] Inf
The number system in R can explicitly represent the numbers +∞and −∞. The
corresponding symbols in R are Inf and -Inf.
Truncation error in particular causes problems in situations where two large but
nearly equal values are subtracted and nearly cancel. For example, if we subtract 1015
from 1015 + 0.2 in R, we do not get the expected result 0.2, but 0.25 instead.
> (1e15 + 0.2) - 1e15
[1] 0.25
Similar problems can occur when very small, but approximately equal, positive
numbers are divided. This type of cancellation can, for example, be encountered when
evaluating the acceptance probability (4.3) of a Metropolis-Hastings algorithm. The
best solution to problems caused by such cancellations is to rewrite the underlying
mathematical expressions so that the cancellations in the program no longer occur. For
the problem of evaluating the acceptance probability (4.3), this approach is illustrated
in equation (4.19).
Finally, the special value NaN, short for ‘not a number’, is used to represent the
outcome of calculation where the result is not mathematically deﬁned.
> 0 / 0
[1] NaN
> log(-1)
[1] NaN
B.2.3.2
Vectors
Vector objects in R are useful to represent mathematical vectors in a program; they can
also be used as a way to store data for later processing. An easy way to create vector
objects is the function c (short for ‘concatenate’) which collects all its arguments into
a vector. The vector
x =
⎛
⎝
1
2
3
⎞
⎠

APPENDIX B: PROGRAMMING IN R
277
can be represented in R as follows:
> c(1, 2, 3)
[1] 1 2 3
The elements of a vector can be accessed by using square brackets: if x is a vector,
x[1] is the ﬁrst element, x[2] the second element and so on:
> x <- c(7, 6, 5, 4)
> x[1]
[1] 7
> x[1] + x[2]
[1] 13
> x[1] <- 99
> x
[1] 99
6
5
4
The function c can also be used to append elements to the end of an existing vector,
thus increasing its length:
> x <- c(1, 2, 3)
> x <- c(x, 4)
> x
[1] 1 2 3 4
In addition to the function c, there are several ways of constructing vectors: one
can start with an empty vector and add elements one-by-one:
> x <- c()
> x[1] <- 1
> x[2] <- 1
> x[3] <- x[2] + x[1]
> x[4] <- x[3] + x[2]
> x
[1] 1 1 2 3
Vectors consisting of consecutive, increasing numbers can be created using the colon
operator:
> 1:15
[1]
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15
> 10:20
[1] 10 11 12 13 14 15 16 17 18 19 20

278
APPENDIX B: PROGRAMMING IN R
More complicated vectors can be generated using the function seq:
> seq(from=1, to=15, by=2)
[1]
1
3
5
7
9 11 13 15
> seq(from=15, to=1, by=-2)
[1] 15 13 11
9
7
5
3
1
Vectors in R programs are mostly used to store data sets, but they can also be used
to store the components of mathematical vectors, that is of elements of the space Rd.
Mathematical operations on vectors work as expected:
> c(1, 2, 3) * 2
[1] 2 4 6
> c(1, 2, 3) + c(3, 2, 1)
[1] 4 4 4
Other useful functions on vectors include sum (to compute the sum of the vector
elements), mean (the average), var (the sample variance), sd (the sample standard
deviation), and length (the length of the vector):
> x <- c(1, 2, 3)
> (x[1] + x[2] + x[3]) / 3
[1] 2
> sum(x) / length(x)
[1] 2
> mean(x)
[1] 2
The operators and functions from Table B.1, when applied to a vector, operate on
the individual elements:
> x <- c(-1, 0, 1, 2, 3)
> abs(x)
[1] 1 0 1 2 3
> x^2
[1] 1 0 1 4 9
This allows to efﬁciently operate on a whole data set with a single instruction.
B.2.3.3
Matrices
Matrices can be constructed in R using the function matrix. To represent the matrix
A =
 1
2
3
4
5
6


APPENDIX B: PROGRAMMING IN R
279
in R, the following command can be used:
> A <- matrix(c(1, 2, 3,
+
4, 5, 6),
+
nrow=2, ncol=3, byrow=TRUE)
> A
[,1] [,2] [,3]
[1,]
1
2
3
[2,]
4
5
6
The ﬁrst argument to matrix is a vector giving the numbers to be stored in the matrix.
The following two arguments set the number of rows/columns of the matrix, and the
last argument states that we gave the entries row-by-row. The whole command can be
given on one line, the line breaks are only inserted to increase readability. The n × n
identity matrix can be generated by diag(n) and an m × n zero matrix by matrix(0,
nrow=m, ncol=n).
Individual elements of a matrix can be accessed using square brackets, just as
for vectors: if A is a matrix, A[1,1] denotes the top-left element of the matrix, A[1,]
denotes the ﬁrst row of the matrix (as a vector), and A[,1] denotes the ﬁrst column
of A:
> A <- matrix(c(1,2,3,4), nrow=2, ncol=2, byrow=TRUE)
> A[1,1] <- 9
> A
[,1] [,2]
[1,]
9
2
[2,]
3
4
> A[1,]
[1]
9 2
> A[,1]
[1]
9 3
For complicated matrices it is sometimes useful to ﬁrst create an empty matrix,
and then use a short program to ﬁll in the values. This can be achieved by using
a command such as matrix(nrow=m, ncol=n), without specifying values for the
matrix elements:
> A <- matrix(nrow=2, ncol=10)
> A
[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
[2,]
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
> A[1,] <- 1
> A[2,] <- 1:10
> A

280
APPENDIX B: PROGRAMMING IN R
[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]
1
1
1
1
1
1
1
1
1
1
[2,]
1
2
3
4
5
6
7
8
9
10
The entries of the empty matrix are originally displayed as NA (for ‘not available’),
and we have to assign values to the elements of A before the matrix can be used.
The sum of matrices and the product of a matrix with a number can be computed
using + and *, the matrix-matrix and matrix-vector products from linear algebra are
given by %*%. (Note, A * A is not the matrix product, but the element-wise product!)
> A <- matrix(c(1, 2,
+
2, 3),
+
nrow=2, ncol=2, byrow=TRUE)
> A
[,1] [,2]
[1,]
1
2
[2,]
2
3
> A %*% A
[,1] [,2]
[1,]
5
8
[2,]
8
13
> x <- c(0, 1)
> A %*% x
[,1]
[1,]
2
[2,]
3
> x %*% A %*% x
[,1]
[1,]
3
The last command shows that vectors are automatically interpreted as row vectors
or column vectors as needed: in R it is not required to transpose the vector x when
evaluating expressions such as x⊤Ax.
Many matrix operations are available, for example the transpose of a matrix A can
be computed using t(A), the inverse by solve(A), the functions rowSums and colSums
return the row and column sums as vectors, and rowMeans and colMeans return the
row and column averages, respectively. The solution x to a system Ax = b of linear
equations can be computed using the command solve(A, b).
B.2.3.4
Strings
Strings are used in R to represent short texts, represented as a sequence of characters.
Strings are used, for example, to specify the text used for the axis labels in a plot.
Strings in R are enclosed in quotation marks.

APPENDIX B: PROGRAMMING IN R
281
Sometimes a bit of care is needed when using strings: the string "12" represents
the text consisting of the digits one and two; this is different from the number 12:
> 12 + 1
[1] 13
> "12" + "1"
Error in "12" + "1" : non-numeric argument to binary operator
R complains that it cannot add "12" and "1", because these values are not numbers.
Strings can be stored in variables just as for numbers and the function paste can
be used to concatenate strings:
> s <- paste("this", "is", "a", "test")
> paste(s, ": ", "a", "bra", "ca", "da", "bra", sep="")
[1] "this is a test: abracadabra"
> paste("x=", 12, sep="")
[1] "x=12"
The argument sep for the function paste speciﬁes a ‘separator’ which is put between
the individual strings. The default value is a single space character. If the arguments
of paste are not strings, they are converted to a string before the concatenation:
> paste("x=", 12, sep="")
[1] "x=12"
B.2.3.5
Truth values
To represent truth values (also known as boolean values), R uses the special values
TRUE and FALSE. The shorthand versions T for TRUE and F for FALSE can also be used.
Truth values are most commonly encountered as the results of comparisons, using
the operators from Table B.2:
> 1 < 2
[1] TRUE
> 3 < 2
[1] FALSE
Truth values can be stored in variables:
> result <- 1 < 2
> result
[1] TRUE
When used in a context where a numeric value is expected, truth values are
automatically converted to numbers: TRUE is interpreted as 1 and FALSE is interpreted
as 0. This behaviour is, for example, useful to count how many elements of a list

282
APPENDIX B: PROGRAMMING IN R
Table B.2
List of comparison operators in R. The values
textttx and y can be replaced by arbitrary numeric expressions,
the result of the comparison is either TRUE or FALSE.
Comparison
Example
R code
Strictly smaller
x < y
x < y
Smaller or equal
x ≤y
x <= y
Strictly bigger
x > y
x > y
Bigger or equal
x ≥y
x >= y
Equal
x = y
x == y
Not equal
x ̸= y
x != y
satisfy a given condition; adding the truth values using sum gives the number of TRUE
entries in a vector of booleans:
> x <- rnorm(10)
> x
[1] -1.1497316
0.9251614 0.2357447 0.7422293
1.8712479
[6] -0.7683445 -0.4054793 0.9313800 1.1917342 -0.7275102
> x > 0
[1] FALSE TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE FALSE
> sum(x > 0)
[1] 6
B.3
Programming principles
In the previous section we have seen many examples of R commands. An R program
is a sequence of commands, designed to solve a speciﬁc problem when executed
in order. As an example, consider the Fibonacci sequence deﬁned by x1 = x2 = 1
and xk = xk−1 + xk−2 for all n > 2. The following commands form an R program to
compute x6:
x <- c()
x[1] <- 1
x[2] <- 1
x[3] <- x[2] + x[1]
x[4] <- x[3] + x[2]
x[5] <- x[4] + x[3]
x[6] <- x[5] + x[4]
cat("the 6th Fibonacci number is", x[6], "\n")
(B.2)

APPENDIX B: PROGRAMMING IN R
283
When these commands are executed in R, one after another, the last command prints
‘the 6th Fibonacci number is 8’ to the screen (the \n starts a new line in the
output). While this program works, it still has a number of shortcomings which we
will address in the following sections.
B.3.1
Don’t repeat yourself!
A fundamental principle in programming is to avoid duplication wherever possible.
This principle applies on many different levels and in many different situations. It is
sometimes called the don’t repeat yourself (DRY) principle. In this section we will
discuss some aspects of the DRY principle, using the program (B.2) as an example.
B.3.1.1
Loops
The way we compute x[6] in the program (B.2) involves a lot of repetition: to
represent the equation xn = xn−1 + xn−2, we used four different lines in our program!
Because of this, it will be impractical to use a similar program to compute x[100]. A
second problem is that it can be difﬁcult to spot mistakes caused by this repetition.
Another complication appears if we want to modify the program to use the
equation xn = xn−1 −xn−2 instead: a single change to the mathematical formula
requires multiple changes to our program.
The problem of this speciﬁc repetition can be solved using a ‘loop’ in the R
program. Such a loop instructs R to execute a command repeatedly. We can use such
a loop to write the command for the relation xk = xk−1 + xk−2 only once, and then
let R repeat this command as needed:
n <- 10
x <- c()
x[1] <- 1
x[2] <- 1
for (k in 3:n) {
x[k] <- x[k-1] + x[k-2]
}
cat("the ", n, "th Fibonacci number is ", x[n], "\n", sep="")
(B.3)
The loop is implemented by the for statement: for is followed by a group of one or
more commands, enclosed in curly brackets { and }, which are executed repeatedly.
The number of repetitions is determined by the vector 3:n, the commands are executed
once for each element of this vector, that is n −2 times in total. The variable k is set
to the corresponding element of the vector before each iteration starts: the ﬁrst time
the loop is executed, k is set to 3. The executed statement is then x[3] <- x[3-1] +
x[3-2]. Before the next iteration, k is set to 4 and, after substituting k, the executed
statement is x[4] <- x[4-1] + x[4-2]. This is repeated until, in the last iteration of
the loop, k is set to the value of n. Once the loop is completed, the program continues
with the ﬁrst instruction after the loop and the program outputs ‘the 10th Fibonacci
number is 55’.

284
APPENDIX B: PROGRAMMING IN R
The choice of the name k for the loop variable in the example above was arbitrary
(it was chosen to match the index in the mathematical formula), any other variable
name could have been used instead. Similarly, the vector 3:n can be replaced by any
other vector, the elements are not required to be adjacent or increasing.
A further improvement of this program, compared with the ﬁrst version, is the
introduction of the variable n: with the new program, we can compute a different
Fibonacci number by changing only a single line. Similarly, if we want to implement
a different recursion relation, we can just replace the command x[k] <- x[k-1] +
x[k-2] with a different one. By avoiding repetition in the program text, mistakes
such as the one in exercise EB.5 where the required change to the output string was
forgotten, are no longer possible.
There is a second kind of loop available, the while loop, which can be used when
the required number of iterations is not known in advance. For example, the following
program determines the ﬁrst Fibonacci number which is bigger than or equal to 1000:
x <- c()
x[1] <- 1
x[2] <- 1
n <- 2
while (x[n] < 1000) {
n <- n + 1
x[n] <- x[n-1] + x[n-2]
}
cat("the ", n, "th Fibonacci number is ", x[n], "\n", sep="")
The while loop repeats its commands while the condition in the round brackets is
satisﬁed. In the condition, all the usual comparison operators can be used (Table B.2).
For this type of loop, no automatic assignment to a loop variable takes place, so we
have to increment n ourselves using the command n <- n + 1. As soon as the
condition is false, the loop ends and the program continues with the ﬁrst command
after the loop.
B.3.1.2
Conditional statements
Inside loops it is often useful to be able to execute different commands in different
iterations of the loop. For example, to print all numbers n = 1, 2, . . . , 100 satisfying
sin(n) > 1
2 to the screen, we can use the following program:
for (n in 1:100) {
if (sin(n) > 0.5) {
cat(n, "\n")
}
}
The if statement is followed by a block of one or more commands, enclosed in curly
brackets { and }. These commands are only executed if the condition given inside

APPENDIX B: PROGRAMMING IN R
285
the round brackets is true, otherwise the whole if block has no effect. As with the
while statement, all the usual comparison operators (Table B.2) can be used in the
condition. There is a second form of the if statement:
for (n in 1:10) {
if (n %% 2 == 1) {
cat(n, "is odd\n")
} else {
cat(n, "is even\n")
}
}
(B.4)
Here, the commands in the ﬁrst block of curly brackets are executed if the condi-
tion is true, and otherwise the commands in the second block are executed. Since
n mod 2 equals 0 for even numbers and 1 for odd numbers, this program gives the
correct output.
Finally, for cases where a result is computed using one of two alternative formulas,
the ifelse statement can be used. Using ifelse, the loop from listing (B.4) can be
rewritten as follows:
for (n in 1:10) {
cat(n, ifelse(n %% 2 == 1, "is odd", "is even"), "\n")
}
The ﬁrst argument of ifelse must be a truth value which determines which of the
following two values should be used: if the ﬁrst argument is true, the result is taken
from the second argument, otherwise it is taken from the third argument. While the
ifelse function can always be replaced by an if statement, use of ifelse sometimes
allows to simplify programs.
B.3.1.3
Command scripts
The DRY principle applies also to repetition between different programs: if you have
solved a problem once, it is better to reuse the old program, instead of writing a new
one. Reuse of old programs does not only avoid unnecessary work, it also reduces
the risk of mistakes being introduced in the code and allows to improve the program
over time. To facilitate this, the following guidelines are useful:
r Save all your R programs in ﬁles (ﬁle names for such ﬁles typically end in .R),
store these ﬁles somewhere safe, and keep the programs organised in a way
which allows you to ﬁnd them again when needed at a later time.
r When writing programs, take care to write them in a way which makes the
code reusable. As we have seen, the DRY principle can help with this. Also, it
is useful to write the program as clearly as possible, to use descriptive names

286
APPENDIX B: PROGRAMMING IN R
for your variables, and to use indentation to make the structure of loops and if
statements easy to follow.
r If the program uses any nonobvious constructions or clever tricks, it is often
helpful to add comments with explanations to the program. Such comments
can be very helpful when trying to reuse programs written more than a few
weeks ago. Since R ignores every line of input which starts with the character
#, such comments can be stored directly in the program. For example, when
reading the (slightly cryptic) program
# compute Fibonacci numbers x[n]
# y = (x[n], x[n-1])
y <- c(1,1)
for (n in 3:17) {
y <- c(sum(y), y[1])
}
print(y[1])
the comments make it a lot easier to ﬁgure out what the program does.
B.3.2
Divide and conquer!
In this section we will discuss a second fundamental programming principle, some-
times called the ‘divide and conquer paradigm’. This principle is to break down a
problem into smaller subproblems which can be solved individually. After solving the
individual subproblems, the individual solutions can be combined to obtain a solution
to the full problem. As for the DRY principle discussed above, the divide and conquer
principle applies on many different levels and in many different situations. In this
text we will focus on the most basic aspects of this principle.
The basic tool for isolating individual building blocks of a program is a ‘function’
which allows to use a simple name as an abbreviation for a list of commands. We
will illustrate the concept of functions in R with the help of examples.
Example B.1
The program from listing (B.3) can be turned into an R function as
follows:
fibonacci <- function(n) {
x <- c()
x[1] <- 1
x[2] <- 1
for (k in 3:n) {
x[k] <- x[k-1] + x[k-2]
}
return(x[n])
}
(B.5)

APPENDIX B: PROGRAMMING IN R
287
When we execute the above lines in R, nothing seems to happen. The commands only
deﬁne the name fibonacci as an abbreviation for the commands on the following
lines, the commands are not yet executed. This step of assigning a name to the
function is referred to as ‘deﬁning the function’.
After the function is deﬁned, we can execute the commands in the function by
using the assigned name fibonacci:
> fibonacci(5)
[1] 5
> res <- fibonacci(10)
> res
[1] 55
(B.6)
This step is referred to as ‘calling the function’. The second call to fibonacci shows
that we can assign the result of a function call to a variable and thus store it for
later use.
The ﬁrst line of listing (B.5) not only gives the name of the function but also,
in the brackets after the keyword function, it indicates that the function has exactly
one argument, called n. Every time the function is called, we have to specify a value
for this argument n, for the ﬁrst call of fibonacci in (B.6) we write fibonacci(5)
to indicate that the value 5 should be used for n, for the second call n equals 10.
The next six lines in listing (B.5), copied from listing (B.3), then compute the ﬁrst n
elements of the Fibonacci sequence, and the return statement indicates the result of
the computation and ends the function.
Example B.2
The R equivalent of a mathematical function is often straightforward
to implement. For example, to deﬁne an R function for computing the value of
f (x) = e−x2,
for a given x, we can use the following R code:
f <- function(x) {
return(exp(-x^2))
}
The idea of a function like fibonacci in listing (B.5) is that, following the DRY
principle, we write the function only once; from then on we can use the function
by calling it without having to think about the details of how Fibonacci numbers
are computed. The function can be used as one of the building blocks for a bigger
program, alongside the built-in R functions such as sqrt and plot.
To allow for easy reuse of existing functions, some of the effect of commands
inside the function is hidden from the caller: while the variables n and x are used and

288
APPENDIX B: PROGRAMMING IN R
modiﬁed inside the function, R takes care not to disturb any variables which may be
used by the caller:
> n <- 3
> fibonacci(10)
[1] 55
> n
[1] 3
> x
Error: object ’x’ not found
As we can see, the assignment n=10 used inside fibonacci does not affect the
value 3 we stored in n before the call and, similarly, the variable x used inside
the function is not visible to the caller. This isolation of the variables inside the
function is the reason that we need to use return to pass the result of a function to
the caller.
Sometimes, it is required to pass several argument values into a function. For
example, the built-in function rnorm to generate normally distributed random vari-
ables could be deﬁned as follows:
rnorm <- function(n, mean, sd) {
# code for generating random numbers
...
}
This declares three arguments: n speciﬁes how many random numbers should be
generated, mean speciﬁes the mean, and sd speciﬁes the standard deviation. We can
use the command rnorm(10, 0, 1) to get 10 standard normally distributed random
variables. Since the special case of mean 0 and standard deviation 1 is very common,
these value are used as ‘default values’. The true declaration of rnorm is
rnorm <- function(n, mean=0, sd=1) {
...
}
The mean=0 tells R to use 0 for the mean, if no other value is given, and similarly for
the standard deviation. Thus, rnorm(10, 5) generates 10 random values with mean 5
(since we speciﬁed the mean in the second argument), and standard deviation 1 (the
default value is used because we did not give a third argument).
When calling a function which uses default values for some or all of its arguments,
and if we want to specify only some of the arguments, we can give values for individual
function arguments as in the following example:
X <- rnorm(10, sd=3)

APPENDIX B: PROGRAMMING IN R
289
This will generate 10 values from a normal distribution with mean 0 (the default
value is used because we did not specify this argument) and standard deviation 3 (as
speciﬁed by sd=3).
Example B.3
Consider the following function:
test <- function(a=1, b=2, c=3, d=4) {
cat("a=", a, ", b=", b, ", c=", c, ", d=", d, "\n", sep="")
}
When experimenting with this function, we get the following output:
> test()
a=1, b=2, c=3, d=4
> test(5, 6)
a=5, b=6, c=3, d=4
> test(a=5, d=6)
a=5, b=2, c=3, d=6
> n <- 3
> test(a=n, b=n+1, c=n+2, d=n+3)
a=3, b=4, c=5, d=6
As we have seen, functions can be used to encapsulate parts of your program as a
single unit. Functions help to follow both the ‘divide and conquer’ principle (because
they allow to easily break down a program into smaller building blocks) and the
DRY principle (because they are easily reused). We conclude this section with some
general guidelines for writing functions:
r Sometimes there is a choice about which parts of a program could be split into
functions. A good idea in such situations is to aim for functions whose purpose
is easily explained: ‘compute the nth Fibonacci number’ is a task which will
make a good function. In contrast, it seems less clear whether implementing a
formula such as:
xn =
xn−1/2
if xn−1 is even and
3xn−1 + 1
if xn−1 is odd
(B.7)
in a function is a good idea: the description of this function will likely be as
long as the function itself, and reuse of the resulting function seems not very
likely.
r Using descriptive names for functions is a good idea. Examples of well-chosen
function names include mean, sin and plot; in all three cases it is easy to guess
from the name what the function will do.

290
APPENDIX B: PROGRAMMING IN R
r The ‘divide and conquer’ principle becomes most powerful when functions
make use of previously deﬁned functions which implement more fundamental
building blocks. The extreme case of this is when the deﬁnition of a function
includes calls to itself (for example when sorting of a list of length n is
reduced to the problem of sorting lists of length n −1); this technique is called
‘recursion’.
B.3.3
Test your code!
The ﬁnal programming principle we will discuss in this text is the importance of
testing programs: even for experienced programmers, it is almost impossible to get a
non-trivial program right at the ﬁrst attempt. Thus, the usual procedure is to complete
a program bit by bit (following the approach in the previous section), and every time
a part of the program is completed to systematically test for the presence of errors.
There are different kinds of errors which can occur in computer programs:
(a) ‘Syntax errors’ are errors caused by not following the restrictions and require-
ments of the programming language. In this case, R does not understand the
program at all and complains immediately with an error message. Examples
include excess commas and brackets:
> mean(c(1,2,,3))
Error in c(1, 2, , 3) : argument 3 is empty
> mean(c(1,2,3)))
Error: unexpected ’)’ in "mean(c(1,2,3)))"
Normally, the cause of these errors can be spotted immediately, and the
problem is usually easy to resolve by following the hints given in the error
message.
(b) ‘Run-time errors’ occur when the program is syntactically correct, but when
R encounters an operation which cannot be performed during the execution
of the program. Examples include programs which try to compute the sample
variance of an empty data set. In this case, the program is stopped immediately
and an error message is printed:
> print.var <- function(x) {
+
cat("the variance is", var(x), "\n")
+ }
> print.var(c(1,2,3))
the variance is 1
> print.var(c())
Error in var(x) : ’x’ is NULL
This kind of error is often more difﬁcult to detect and ﬁx, because the error
may not occur in every run of the program and because it may be difﬁcult to
ﬁnd the exact location of the error in the program.

APPENDIX B: PROGRAMMING IN R
291
‘Arithmetic errors’ are special cases of run-time errors, where the pro-
gram tries to evaluate expressions like 0/0 or to compute the square root
of a negative number. In these cases, R just sets the result to NaN (short
for ‘not a number’) and sometimes (but not always) produces a warning
message:
> 0/0
[1] NaN
> 2/2 + 1/1 + 0/0
[1] NaN
> sqrt(-1)
[1] NaN
Warning message:
In sqrt(-1) : NaNs produced
(c) ‘Semantic errors’ are the errors caused by a program being a valid program,
but one which does not implement the intended algorithm. These errors
are cases where the program does what you told it to do instead of what
you meant it to do. Such errors can be very difﬁcult to spot and ﬁx. As a
simple example, consider the following function for computing the mean of
a vector:
# something is wrong here!
average <- function(x) {
n <- length(x)
s <- sum(x)
return(x/n)
}
(B.8)
This function does not work as intended (since the return statement erro-
neously uses x instead of s), but there is no error message. The best way to
ﬁnd this mistake is to test the function by trying to call it.
Errors in programs are sometimes called ‘bugs’, and the process of locating
and ﬁxing errors in a program is called ‘debugging’. There are various aspects to
debugging:
r A good start is to try the code in question for a few cases where the correct
result is known. Make sure to try some boundary cases (such as very short data
sets) as well as some typical cases. Test each function of a program separately,
starting with the most fundamental ones.
r Carefully look out for any error messages or warnings. These messages often
contain useful hints about what went wrong.
r To locate the position of an error, it is often helpful to temporarily insert a
print or cat statement into a program to check whether the program actually

292
APPENDIX B: PROGRAMMING IN R
executes certain lines of code and to see whether variables at these locations
still have the expected values.
r The functions debug and undebug can be used to watch the execution of a func-
tionstepbystep.SeetheRhelptextfor debug [obtainedbytyping help(debug)]
for usage instructions.
Example B.4
Assume we want to debug the function average given in listing (B.8).
The ﬁrst step is to try a few values:
> average(c(1, 2, 3))
[1] 0.3333333 0.6666667 1.0000000
Since the mean of (1, 2, 3) is 2, we see immediately that something is wrong. Assum-
ing that we do not spot the typo yet, we can try to modify the function by inserting a
cat statement as follows:
average <- function(x) {
n <- length(x)
s <- sum(x)
cat("n =", n, " s =", s, "\n")
return(x/n)
}
If we rerun the function, we now get the following output:
> average(c(1, 2, 3))
n = 3
s = 6
[1] 0.3333333 0.6666667 1.0000000
Since the printed values are what we expect (the list has length n = 3, and the sum of
the elements should be s = 1 + 2 + 3 = 6), we know that the mistake must be after
the cat statement. Thus, we have narrowed down the location of the problem to a
single line (the return statement), and looking at this line it is now easy to spot that
x should be replaced with s.
B.4
Random number generation
R contains an extensive set of built-in functions for generating (pseudo-)random
numbers of many of the standard probability distributions. There are also functions
available to compute densities, CDFs and quantiles of these distributions. Since these
functions will be used extensively for the exercises in the main text, we give a short
introduction here.

APPENDIX B: PROGRAMMING IN R
293
Table B.3
Some probability distributions supported by R.
Distribution
Name in R
Binomial distribution
binom
χ2 distribution
chisq
Exponential distribution
exp
Gamma distribution
gamma
Normal distribution
norm
Poisson distribution
pois
Uniform distribution
unif
The names of all these functions are constructed using the following scheme: the
ﬁrst letter is
r
for random number generators,
d
for densities (weights for the discrete case),
p
for the CDFs and
q
for quantiles.
The rest of the name determines the distribution; some possible distributions are
given in Table B.3.
Example B.5
The function to generate normal distributed random numbers is rnorm
and the density of the exponential distribution is dexp.
The functions starting with r, examples include runif and rnorm, can be used
to generate random numbers. The ﬁrst argument for each of these functions is the
number n of random values required; the output is a vector of length n. The following
arguments give parameters of the underlying distribution; often these arguments have
the most commonly used parameter values as their default values. Details about how
to use these functions and how to set the distribution parameters can be found in the R
online help. Finally, the function set.seed is available to set the seed of the random
number generator (see the discussion in section 1.1.3).
Example B.6
A vector of 10 independent, standard normally distributed random
numbers can be obtained using the command rnorm(10). A single sample from a
N(2, 9) distribution can be obtained using rnorm(1, 2, 3), where the last argument
gives the standard deviation (not the variance) of the distribution.
An exception to the naming scheme for random number generators is the discrete
uniform distribution: a sample X1, . . . , Xn ∼U{1, 2, . . . , a} can be generated with
the R command sample.int(a, n, replace=TRUE).

294
APPENDIX B: PROGRAMMING IN R
B.5
Summary and further reading
The best way to learn programming is to gain a lot of practice. For this reason, I
recommend trying to solve as many of the exercises provided in this text as possi-
ble. Other interesting sources of programming challenges can be found online, for
example on the web page of Project Euler.1
An extensive introduction to all aspects of R can be found online in Venables et al.
(2011). A more in-depth description can be found in Venables and Ripley (2000).
R source code for many of the methods discussed here can be found in Rizzo (2008).
Information about speciﬁc R functions can be found using the built-in help system
(accessed using the function help).
Exercises
EB.1
Use R to compute the following values:
(a)
1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10
(b)
216
(c)
2100
(d)
2
1 +
√
5
EB.2
What are the values of x and y after the following commands are executed
in R?
x <- 1
y <- 2
x <- x + y
y <- x + y
EB.3
Use R to compute the value of 1 + 2 + · · · + 100.
EB.4
Write an R function to compute the sample excess kurtosis
g2 =
1
n
	n
i=1(xi −¯x)4

 1
n
	n
i=1(xi −¯x)22 −3,
for a given vector x = (x1, . . . , xn), where ¯x is the average of the elements
of x.
1 Available from http://projecteuler.net/.

APPENDIX B: PROGRAMMING IN R
295
EB.5
The following code is an extension of the program from listing (B.2),
changed to compute the Fibbonacci numbers until x[8] (instead of x[6]).
x <- c()
x[1] <- 1
x[2] <- 1
x[3] <- x[2] + x[1]
x[4] <- x[3] + x[2]
x[6] <- x[5] + x[4]
x[7] <- x[6] + x[5]
x[8] <- x[7] + x[6]
cat("the 6th Fibonacci number is", x[8], "\n")
Can you spot the mistake?
EB.6
Let x0 = 0 and xn = cos(xn−1) for all n ∈N. Use R to compute the value of
x20.
EB.7
Write an R program which uses a for loop to print the elements of the
decreasing sequence 100, 99, . . . , 0 to the screen, one per line.
EB.8
Write an R program which uses a while loop to determine the smallest
square number bigger than 5000.
EB.9
Write an R program which uses a while loop to determine the biggest square
number smaller than 5000.
EB.10
Given x0 ∈N, a sequence (xn)n∈N of integers can be deﬁned by equation
(B.7). Once this sequence reaches 1, it starts to cycle through the values
3 · 1 + 1 = 4, 4/2 = 2, and 2/2 = 1, but it is unknown whether this cycle
is reached from every starting point x0. Write an R program which prints
the values of xn, starting with x0 = 27. The program should stop when the
value 1 is reached for the ﬁrst time.
EB.11
What does the following function compute?
f <- function(x) {
n <- length(x)
m <- mean(x)
s <- 0
for (i in 1:n) {
s <- s + (x[i] - m)^2
}
return(s/(n-1))
}
EB.12
Continuing example B.3, feed the following commands into R:
test <- function(a=1, b=2, c=3, d=4) {
cat("a=", a, ", b=", b, ", c=", c, ", d=", d, "\n", sep="")
}

296
APPENDIX B: PROGRAMMING IN R
b <- "a"
test(c=b)
Explain the result R prints to the screen.
EB.13
Section B.3.3 introduces three categories of programming errors. Which
categories do the two (!) errors from exercise EB.5 fall into?
EB.14
When trying to solve exercise E3.7, one could try to implement the function
ˆρ(X, Y) from equation (3.23) as follows:
# something is wrong here!
rxy <- function(X,Y) {
mX <- mean(X)
mY <- mean(Y)
numerator <- sum((X - mX) * (Y - mY))
denominator <- sqrt(sum(X-mX)^2 * sum(Y-mY)^2)
return(numerator / denominator)
}
When testing this function, we quickly discover that something must be
wrong:
> X <- c(1.6, -1.1, -1.2)
> Y <- c(0.1, 0.2, 0.1)
> rxy(X, Y)
[1] -Inf
One can check that −1 ≤ˆρ(X, Y) ≤1, so the value −∞clearly cannot be
the correct result! Use the techniques explained in section B.3.3 to ﬁnd the
mistake.
EB.15
The following function is a (failed) attempt to compute
n−1

i=1
(xi+1 −xi)2,
that is the sum of squared increments, in R:
SomethingWrong <- function(x) {
n <- length(x)
sum <- 0
for (i in 1:n-1) {
sum <- sum + (x[i+1] - x[i])^2
}
return(sum)
}

APPENDIX B: PROGRAMMING IN R
297
When we apply this function to the vector (1, 2, 3), we do not get the correct
answer 2, but numeric(0) instead.
> SomethingWrong(c(1,2,3))
numeric(0)
What is the mistake in the function SomethingWrong?
EB.16
Use R to create plots of the densities and CDFs of the following distributions:
(a)
N(0, 1) — the standard normal distribution;
(b)
N(3, 4) — the normal distribution with mean μ = 3 and variance
σ 2 = 4;
(c)
Exp(1) — the exponential distribution with rate 1;
(d)
(9, 0.5) — the gamma distribution with shape parameter k = 9 and
scale parameter θ = 0.5.

Appendix C
Answers to the exercises
This appendix contains solutions to the exercises found throughout the book. Most
of the answers require use of a computer. Any programs used in the solutions here
are written using the R programming language (R Development Core Team, 2011);
of course, similar solutions can be written in different programming languages. For
reference, a short introduction to programming in R can be found in Appendix B.
All R programs used for this book, both to generate the ﬁgures and for the
answers of the exercises, can be downloaded from the accompanying web page at
www.wiley.com/go/statistical computing
C.1
Answers for Chapter 1
Solution E1.1
To implement the LCG in R, we can execute the commands from
algorithm 1.2 in a loop.
LCG <- function(n, m, a, c, X0) {
X <- numeric(length=n)
Xn <- X0
for (i in 1:n) {
Xn <- (a*Xn + c) %% m
X[i] <- Xn
}
return(X)
}
The resulting function LCG can be called as follows:
> LCG(10, 8, 5, 1, 0)
[1] 1 6 7 4 5 2 3 0 1 6
An Introduction to Statistical Computing: A Simulation-based Approach, First Edition. Jochen Voss.
© 2014 John Wiley & Sons, Ltd. Published 2014 by John Wiley & Sons, Ltd.

300
APPENDIX C: ANSWERS TO THE EXERCISES
The output matches the manually computed result from Example 1.3, so we can
assume that the program is correct.
Solution E1.2
Using the function LCG from exercise E1.1, we can generate the
graphs as follows:
par(mfrow=c(2,2))
X <- runif(1000)
plot(X[1:999], X[2:1000], asp=1, cex=.5,
xlab=expression(X[i]), ylab=expression(X[i+1]))
m <- 81
a <- 1
c <- 8
seed <- 0
X <- LCG(1000, m, a, c, seed)/m
plot(X[1:999], X[2:1000], asp=1, cex=.5,
xlab=expression(X[i]), ylab=expression(X[i+1]))
m <- 1024
a <- 401
c <- 101
seed <- 0
X <- LCG(1000, m, a, c, seed)/m
plot(X[1:999], X[2:1000], asp=1, cex=.5,
xlab=expression(X[i]), ylab=expression(X[i+1]))
m <- 2^32
a <- 1664525
c <- 1013904223
seed <- 0
X <- LCG(1000, m, a, c, seed)/m
plot(X[1:999], X[2:1000], asp=1, cex=.5,
xlab=expression(X[i]), ylab=expression(X[i+1]))
The output is shown in Figure 1.1.
Solution E1.3
(a) There are different approaches to picking out the middle two digits of a
number. One method is to use algebraic operations:
middle.square <- function (x) {
square <- x^2
middle <- floor(square / 10) %% 100
return(middle);
}

APPENDIX C: ANSWERS TO THE EXERCISES
301
Another method is to convert between numbers and strings:
middle.square2 <- function(x) {
square <- x^2
padded <- formatC(square, width=4, flag="0")
middle <- substring(padded, 2, 3)
return(as.numeric(middle))
}
(b) The easiest method to ﬁnd a loop is to generate elements of the sequence Xn,
until a previously seen element is reached:
find.loop <- function (start) {
seen <- c()
# follow the sequence until it runs back into itself
x <- start
while (! is.element(x, seen)) {
seen <- c(seen, x)
x <- middle.square(x)
}
# get the complete loop
loop <- c(x)
y <- middle.square(x)
while (y != x) {
loop <- c(loop, y)
y <- middle.square(y)
}
return(loop)
}
Using this function, we can systematically look for loops:
all.loops <- c()
for (x in 0:99) {
loop <- find.loop(x)
# To see whether we’ve found this loop already,
# we keep track of the smallest element in each loop.
smallest <- min(loop)
if (! is.element(smallest, all.loops)) {
cat("found a loop: ", loop, "\n");
all.loops <- c(all.loops, smallest)
}
}

302
APPENDIX C: ANSWERS TO THE EXERCISES
This program generates the following output:
found a loop:
0
found a loop:
10
found a loop:
60
found a loop:
24 57
found a loop:
50
The function middle.square is illustrated in Figure C.1.
(c) Figure C.1 shows that the output starts repeating itself after at most 15 steps
and that all loops are very short (length 1 or 2). Therefore, the method forms
a rather poor PRNG.
42
69
34
66
76
15
35
65
85
26
86
94
79
50
24
83
88
74
47
99
53
49
80
97
40
98
51
60
20
57
39
23
59
67
22
89
77
92
48
48
46
31
63
30
70
90
96
11
87
81
56
12
21
38
10
44
54
91
71
93
58
37
19
41
16
25
68
75
36
27
61
82
64
28
33
78
9
72
43
29
62
7
71
84
18
73
8
6
95
55
45
32
5
4
1
2
3
0
14
13
Figure C.1
The transition graph of the middle square method for two-digit numbers.

APPENDIX C: ANSWERS TO THE EXERCISES
303
Solution E1.4
The ﬁrst step in solving this question is to ﬁnd the CDF of the given
distribution. For x < 1 we have f (x) = 0 and thus F(x) = 0. For x ≥1 we have
f (x) = 1/x2 and thus
F(a) =
 a
1
1
x2 dx =

−1
x

a
x=1 = 1
1 −1
a = 1 −1
a .
Next, we have to ﬁnd the inverse of F. Since F is continuous, we can just compute
the ordinary inverse:
u = F(x) = 1 −1
x
⇐⇒
1
x = 1 −u
⇐⇒
x =
1
1 −u .
By proposition 1.14, if U ∼U[0, 1], then X = 1/(1 −U) has density f . The result-
ing method can be implemented in R as follows:
GenerateSample <- function(n) {
U <- runif(n)
return(1/(1-U))
}
To generate a histogram of 10 000 values, we can then use the following
commands:
X <- GenerateSample(10000)
hist(X, freq=FALSE, breaks=seq(0, max(X)+1, 0.1),
xlim=c(0,10), ylim=c(0,1),
main=NULL, col="gray80", border="gray20")
Some care is needed, because X occasionally takes very big values:
> summary(X)
Min.
1st Qu.
Median
Mean
3rd Qu.
Max.
1.000
1.336
1.971
8.073
3.899 2746.000
To make the histogram useful we have to choose sufﬁciently narrow bars (using the
breaks argument) and to restrict the displayed horizontal coordinate range (using
the xlim argument). Also, since we want to compare the output with the density,
we have to switch to a density plot instead of a frequency plot (using the argument
freq=FALSE). The density can be added to the plot using the lines command:
x <- seq(0, 10, 0.01)
f <- ifelse(x <= 1, 0, 1/x^2)
lines(x, f, lw=1)
The resulting plot is shown in Figure C.2. The plot shows a good match between the
density function and the histogram.

304
APPENDIX C: ANSWERS TO THE EXERCISES
X
Density
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Figure C.2
A histogram of 10 000 samples from the distribution given in exer-
cise E1.4, together with the corresponding density function (solid line).
Solution E1.5
Let Yn = 1, if the proposal Xn is accepted and Yn = 0 otherwise.
Then we can write Kn = n
i=1 Yi. From equation (1.4) in the proof of proposition
1.20 we know that each of the proposals is accepted with probability Z and thus we
have
E(Yn) = 1 · P(Yn = 1) + 0 · P(Yn = 0) = Z
for all n ∈N. Finally, by the strong law of large numbers (theorem A.8), we ﬁnd
lim
n→∞
1
n Kn = lim
n→∞
1
n
n

i=1
Yi = E(Y1) = Z.
This completes the proof.
Solution E1.6
We can implement the rejection algorithm as follows:
f <- function(x) {
return((x> 0) * 2 * dnorm(x,0,1))
}
g <- function(x) { return(dexp(x,1)) }
c <- sqrt(2 * exp(1) / pi)
rhalfnormal <- function(n) {
res <- numeric(length=n)
i <- 0

APPENDIX C: ANSWERS TO THE EXERCISES
305
X
Density
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
Figure C.3
A histogram of the output of the algorithm from exercise E1.6. The
histogram is generated from 10 000 samples from the half-normal distribution. For
comparison, the solid line gives the exact density from equation (1.5).
while (i<n) {
U <- runif(1, 0, 1)
X <- rexp(1, 1)
if (c * g(X) * U <= f(X)) {
i <- i+1
res[i] <- X;
}
}
return(res)
}
To test the function rhalfnormal, we generate a histogram as follows:
X <- rhalfnormal(10000)
hist(X, breaks=50, prob=TRUE, ylim=c(0,1),
main=NULL, col="gray80", border="gray20")
curve(f, min(X), max(X), n=500,
ylim=c(0,1), ylab="f", add=TRUE)
The output of this program is shown in Figure C.3.
Solution E1.7
In this question we are asked to construct a rejection algorithm to
convert the Exp(λ)-distributed proposal into N(0, 1)-distributed samples. We con-
struct the algorithm in two steps: ﬁrst we generate half-normal distributed samples
with density
f (x) =
2
√
2πσ 2 exp

−x2
2σ 2


306
APPENDIX C: ANSWERS TO THE EXERCISES
for all x ≥0. Since the corresponding samples are positive, we can use the density
of the Exp(λ)-distribution, that is
g(x) = λ exp (−λx)
for all x ≥0 as the proposal density. In a second step, we then attach a random sign
to get the full normal distribution.
For the rejection sampling algorithm, we need to ﬁnd a constant c with
f (x)
≤
c · g(x)
⇐⇒
2
√
2πσ 2 exp

−x2
2σ 2

≤c · λ exp (−λx)
⇐⇒
2
λ
√
2πσ 2 exp

−x2
2σ 2 + λx

≤c
⇐⇒
2
λ
√
2πσ 2 exp

−1
2σ 2 (x −λσ 2)2

exp

λ2σ 2/2
	
≤c
for all x ∈R. Since (x −λσ 2)2 ≥0, this condition is satisﬁed for all values of c with
c ≥
2
λ
√
2πσ 2 exp

λ2σ 2/2
	
.
Since the method is most efﬁcient for small values of c, the best choice is to choose
c equal to this bound. The resulting acceptance condition is
c · g (X)U ≤f (X)
⇐⇒
2
λ
√
2πσ 2 exp

λ2σ 4	
λ exp (−λx) U ≤
2
√
2πσ 2 exp

−x2
2σ 2

⇐⇒
U ≤exp

−1
2σ 2 (x −λσ 2)2

.
This leads to the following algorithm:
(a) Generate X ∼Exp(λ).
(b) Generate U ∼U[0, 1].
(c) Accept X if U ≤exp

−(X −λσ 2)2/2σ 2	
, otherwise reject X.
(d) For accepted samples, return −X or X randomly, each with probability 1/2.
We can optimise the method by choosing the value of λ which minimises
c(λ) = 2 exp

λ2σ 2/2
	
λ
√
2πσ 2
.

APPENDIX C: ANSWERS TO THE EXERCISES
307
Setting the derivative of c(λ) equal to zero, we ﬁnd the following necessary condition
for an extremum:
0 = c′(λ)
=
2
√
2πσ 2 · λσ 2 exp

λ2σ 2/2
	
· λ −1 · exp

λ2σ 2/2
	
λ2
=

λ2σ 2 −1
	 2 exp

−λ2σ 4	
λ2√
2πσ 2
.
This condition is only satisﬁed for λ = λ∗= 1/σ and, since c(λ) converges to ∞for
λ ↓0 and λ →∞, the value λ∗is a minimum of c(λ). Thus, the optimal value of λ
is λ∗and the corresponding, optimal value of c is
c(λ∗) = σ 2 exp(1/2)
√
2πσ 2
=

2e
π ≈1.315.
This shows that, when the algorithm is optimally tuned, we need on average 1.315
proposals to generate one output sample.
Solution E1.8
Since the proposals are Exp(1)-distributed, the proposal density is
given by
g(x) = exp(−x)
and we need to ﬁnd a constant c > 0 with f (x) ≤cg(x) for all x ≥0. We have
f (x)
≤
c · g(x)
⇐⇒
1
√x exp

−y2/2x −x
	
≤c exp(−x)
⇐⇒
1
√x exp

−y2/2x
	
≤c,
for all x ≥0 and thus we need to ﬁnd the maximum of the left-hand side of this
equation. Setting the derivative equal to zero, gives the condition
0 =
 1
√x exp

−y2/2x
	′
=
y2
2x2 exp(−y2/2x)√x −
1
2√x exp(−y2/2x)
x
=
 y2
x −1
 exp(−y2/2x)
2x3/2
,

308
APPENDIX C: ANSWERS TO THE EXERCISES
which is satisﬁed for x = y2; it is easy to check that this value corresponds to a
maximum. Thus, the optimal choice for c is
c =
1

y2 exp

−y2/2y2	
= 1
|y| exp(−1/2).
This is the required result.
Solution E1.9
The inequality c ≥1 follows from
1 =

f (x) dx ≤

cg(x) dx = c

g(x) dx = c.
Now assume c = 1. Then we have f ≤g. For ε > 0 let Aε =

x
 f (x) ≤g(x) −ε

.
Then we have
1 =

f (x) dx =

Aε
f (x) dx +

A∁ε
f (x) dx
≤

Aε
(g(x) −ε) dx +

A∁ε
g(x) dx =

g(x) dx −ε|Aε| = 1 −ε|Aε|
and thus ε|Aε| ≤0. Since ε > 0, this is only possible if |Aε| = 0. Therefore we ﬁnd

x
 f (x) < g(x)
 =


ε>0
Aε
 = lim
ε↓0 |Aε| = 0.
This completes the proof.
Solution E1.10
Since we have X ∼U[a, b] and Y ∼U[c, d], the density of X is
given by 1[a,b](x)/(b −a) and the density of Y is 1[c,d](y)/(d −c), where
1[a,b](x) =
1
if x ∈[a, b] and
0
otherwise.
Since X and Y are independent, the joint density of (X, Y) is
f (x, y) = 1[a,b](x)
b −a
· 1[c,d](y)
d −c
= 1[a,b](x)1[c,d](y)
(b −a)(d −c) .
Consequently,
P

(X, Y) ∈A
	
=
 
1A(x, y) f (x, y) dy dx
=
 
1A(x, y)1[a,b](x)1[c,d](y)
(b −a)(d −c) dy dx
=
1
(b −a)(d −c)
 b
a
 d
c
1A(x, y) dy dx

APPENDIX C: ANSWERS TO THE EXERCISES
309
where
1A(x, y) =
1
if (x, y) ∈A and
0
otherwise.
Since the rectangle R = [a, b] × [c, d] satisﬁes |R| = (b −a)(d −c) and any set A
satisﬁes
A ∩R
 =
 b
a
 d
c
1A(x, y) dy dx
we ﬁnd
P

(X, Y) ∈A
	
=
A ∩R

|R|
.
This is the probability from the deﬁnition of the uniform distribution on R and
therefore the proof is complete.
Solution E1.11
The set A consists of two disjoint components. The probability for
a sample to be in the left component is 1/3 and the probability of being in the right
component is 2/3. Using lemma 1.31 we know that, conditioned on being in one
of the components, the sample should be distributed uniformly on this component.
Consequently we can generate samples Xi ∼U(A) as follows:
(a) Let I ∈{1, 2} with P(I = 1) = 1/3 and P(I = 2) = 2/3.
(b) If
I = 1
generate
X ∼U

[0, 1] × [0, 1]
	
.
Otherwise
generate
X ∼
U

[2, 4] × [0, 1]
	
.
The method can be implemented in R as follows:
r.two.squares <- function(n) {
U <- runif(n)
X <- ifelse(U < 1/3, runif(n), runif(n, 2, 4))
Y <- runif(n)
return(cbind(X, Y))
}
Z <- r.two.squares(1000)
plot(Z[,1], Z[,2], xlab="X", ylab="Y", cex=.5, asp=1)
The resulting plot is shown in Figure C.4.
Solution E1.12
We can proceed as in exercise E1.11, but some care is needed
since the two squares composing the set B overlap. To avoid this problem,
we write B = B1 ∪B2 ∪B3 where B1 = [0, 2] × [0, 1], B2 = [0, 3] × [1, 2] and

310
APPENDIX C: ANSWERS TO THE EXERCISES
0
1
2
3
4
0.0
0.5
1.0
X
Y
Figure C.4
A sample of 1000 points from the uniform distribution on set A from
exercise E1.11.
B3 = [1, 3] × [2, 3]. The corresponding probabilities are P(X ∈B1) = |B1|/|B| =
2/7, P(X ∈B2) = |B2|/|B| = 3/7 and P(X ∈B3) = |B3|/|B| = 2/7.
The method can be implemented in R as follows:
r.three.rectangles <- function(n) {
U <- runif(n)
X <- ifelse(U < 2/7, runif(n, 0, 2),
ifelse(U < 5/7, runif(n, 0, 3), runif(n, 1, 3)))
Y <- ifelse(U < 2/7, runif(n, 0, 1),
ifelse(U < 5/7, runif(n, 1, 2), runif(n, 2, 3)))
return(cbind(X, Y))
}
A plot of a sample generated by this function is shown in Figure C.5.
Solution E1.13
(a) We can use the following procedure: let Xn ∼U[−1, 1] and Yn ∼
U[0, 1] be independent. Accept Un = (Xn, Yn) if X2
n + Y 2
n ≤1. Then the
accepted points are uniformly distributed on the semicircle. The acceptance
probability is
P

(Xn, Yn) accepted
	
= P

(Xn, Yn) in semicircle
	
=
semicircle

rectangle
 = π/2
2
= π
4 ≈0.7854.

APPENDIX C: ANSWERS TO THE EXERCISES
311
−1
0
1
2
3
4
0.0
0.5
1.0
1.5
2.0
2.5
3.0
X
Y
Figure C.5
A sample of 1000 points from the uniform distribution on set B from
exercise E1.12.
(b) Since we do not know in advance how many proposals are required to generate
a given number of samples, we use a while loop in our program:
rsemicircle <- function(n) {
res = c()
k <- 0
while (length(res) < 2*n) {
k <- k + 1
X <- runif(1, -1, 1)
Y <- runif(1, 0, 1)
if (X^2 + Y^2 < 1) {
res <- rbind(res, c(X, Y))
}
}
cat(k, "proposals ->", n, "samples\n")
return(res)
}
Z <- rsemicircle(1000)
plot(Z[,1], Z[,2], xlab="X", ylab="Y", cex=.5, asp=1)
The resulting plot is shown in Figure C.6.
Solution E1.14
To obtain a rejection method with acceptance probability of at
least 80% we need to ﬁnd a region for the proposals which contains the semicircle,
but which has an area of at most π/(2 · 0.8). Here we use the area depicted in
Figure C.7.

312
APPENDIX C: ANSWERS TO THE EXERCISES
−1.0
−0.5
0.0
0.5
1.0
0.0
0.2
0.4
0.6
0.8
1.0
X
Y
Figure C.6
A sample of 1000 points from the uniform distribution on the semicircle
(see exercise E1.13). A total of 1274 proposals was used in the rejection algorithm
to generate the 1000 samples.
The proposal region A has area |A| = 2 −2 · 1/42 = 30/16 (6/16 for the upper
region and 24/16 for the lower region) and thus the acceptance probability is π/2
|A| ≈
0.8378. We can sample from the uniform distribution on A as in exercise E1.12:
proposal <- function() {
U <- runif(1)
X <- ifelse(U < 6/30, runif(1, -0.75, 0.75), runif(1, -1, 1))
Y <- ifelse(U < 6/30, runif(1, 0.75, 1), runif(1, 0, 0.75))
return(c(X,Y))
}
0.75
0.75
0.75
−0.75
Figure C.7
Proposal region for the rejection sampling algorithm proposed in the
solution to exercise E1.14.

APPENDIX C: ANSWERS TO THE EXERCISES
313
Using rejection sampling to obtain the uniform distribution on the semicircle is
now straightforward:
rsemicircle <- function(n) {
res = c()
k <- 0
while (length(res) < 2*n) {
k <- k + 1
Z <- proposal()
if (sum(Z^2) < 1) {
res <- rbind(res, Z)
}
}
return(res)
}
Solution E1.15
To ﬁnd the density of X we can use lemma 1.33: since X is
distributed uniformly on the area between the x-axis and the graph of the function
√
1 −x2, the vector (X, 2
π Y) is uniformly distributed on
A =

(x, y) ∈R2  0 ≤y < f (x)

where
f (x) =
 2
π
√
1 −x2
if x ∈[−1, 1] and
0
otherwise.
Since f ≥0 and

f (x) dx = 2
π · π
2 = 1, the function f is a probability density, and
by lemma 1.33, the random variable X has density f .
To ﬁnd the density of Y we use a direct calculation:
P

Y ∈A
	
=
 1
0 1A(y)⟨width of semicircle at level y⟩dy
⟨area of semicircle⟩
=
 1
0
1A(y)2

1 −y2
π/2
dy
=

R
1A(y) g(y) dy
where
g(y) = 4
π

1 −y2 1[0,1](y).
Thus, Y is distributed with density g.

314
APPENDIX C: ANSWERS TO THE EXERCISES
Solution E1.16
Denote the density of cX by g and deﬁne ϕ(x) = cx for all x ∈Rd.
By theorem 1.34, if X has density
f (x) = g

ϕ(x)
	
·
det Dϕ(x)
,
then ϕ(X) has density g. Since ϕ(x)i = cxi for all i = 1, 2, . . . , d, we ﬁnd
det Dϕ(x) = det
⎛
⎜⎜⎜⎝
c
0
· · ·
0
0
c
· · ·
0
...
...
...
...
0
0
· · ·
c
⎞
⎟⎟⎟⎠= cd
and thus f (x) = g(cx)|cd| for all x ∈Rd. We can use the substitution y = cx to
solve this equation for g, the result is
g(y) =
1
|cd| f (y/c).
This is the required density of cX.
Solution E1.17
We have Y = ϕ(X) with ϕ(x) = (x2 −1)/2 for all x ∈R. We
would like to apply theorem 1.34 but, since ϕ is not bijective, the theorem does not
apply directly. To work around this problem we ﬁrst consider the random variable
Z = |X|. Since X2 = Z2, Y can also be written as Y = ϕ(Z) where ϕ: [0, ∞) →R
is bijective. We have
P(Z ∈A) = P(X ∈A) + P(−X ∈A) =
2
√
2π

A
exp(−x2/2) dx
for all A ⊆[0, ∞) and thus Z has density
f (z) =

2
√
2π exp(−z2/2)
if z ≥0 and
0
otherwise.
Now we can apply theorem 1.34 by considering ϕ to be a map from A = (0, ∞) to
B = (−1/2, ∞). Denoting the density of Y by g we get
2
√
2π
exp(−z2/2) = g

ϕ(z)
	ϕ′(z)
 = g

ϕ(z)
	
z
for all z > 0. Choosing z = ϕ−1(y) = √2y + 1 we get
2
√
2π
exp(−y −1/2) = g(y)

2y + 1

APPENDIX C: ANSWERS TO THE EXERCISES
315
and thus
g(y) =

exp(−y−1/2)
√π(y+1/2)
if y ≥−1/2 and
0
otherwise.
This is the required density of Y.
Solution E1.18
Following the steps presented in example 1.40, we can use the
following R program:
rcauchy.ratio <- function(n) {
X0 <- c()
X1 <- c()
while (length(X0) < 2*n) {
x <- runif(1, 0, 1)
y <- runif(1, -1, 1)
if (x^2 + y^2 < 1) {
X0 <- c(X0, x)
X1 <- c(X1, y)
}
}
return(X1 / X0)
}
C.2
Answers for Chapter 2
Solution E2.1
Following algorithm 2.9 we can sample from the mixture distribution
as follows:
mu <- c(1, 2, 5)
sigma <- sqrt(c(0.01, 0.5, 0.02))
theta <- c(0.1, 0.7, 0.2)
Y <- sample(3, 10000, replace=TRUE, prob=theta)
X <- rnorm(10000, mu[Y], sigma[Y])
The required histogram can be plotted using the hist command.
hist(X, breaks=50, prob=TRUE,
main=NULL, col="gray80", border="gray20")
Finally, from lemma 2.8 we know that the density of the mixture distribution is
the mixture of the component densities. Using the function rnorm for the components,
we can overlay the mixture density over the plot as follows:

316
APPENDIX C: ANSWERS TO THE EXERCISES
curve(theta[1] * dnorm(x, mu[1], sigma[1])
+ theta[2] * dnorm(x, mu[2], sigma[2])
+ theta[3] * dnorm(x, mu[3], sigma[3]),
min(X), max(X), n=1000, add=TRUE)
The resulting plot is shown in Figure 2.1.
Solution E2.2
To show that X is not a Markov chain, we need to ﬁnd a case where
the Markov chain condition (2.2) is violated. Example: if X j−1 = a and X j−2 = b
then X j ∼N(a + b, 1). Thus, for small ε > 0,
P

X j ∈[−3, 3]
 X j−1 ∈[−ε, +ε], X j−2 ∈[10 −ε, 10 + ε]

≈0
but
P

X j ∈[−3, 3]
 X j−1 ∈[−ε, +ε], X j−2 ∈[−ε, +ε]

≈1.
If X was a Markov chain, the two probabilities would be equal, but here they are not.
Thus, X is not a Markov chain.
Solution E2.3
(a) To sample paths from the Markov chain we can use a program
such as:
MC <- function(N, initial, P) {
Xj <- sample(length(initial), 1, prob=initial)
res <- c(Xj)
for (j in 1:N) {
p <- P[Xj,]
Xj <- sample(length(p), 1, prob=p)
res <- c(res, Xj)
}
return(res)
}
# transition matrix
P <- matrix(c(2/3, 1/3, 0, 0,
.1, .9, 0, 0,
.1, 0, .9, 0,
.1, 0, 0, .9),
nrow = 4, ncol = 4, byrow=TRUE)

APPENDIX C: ANSWERS TO THE EXERCISES
317
# initial distribution
mu <- c(.25, .25, .25, .25)
MC(10, mu, P)
(b) To numerically estimate the distribution of X10 we can use a histogram:
data <- c()
for (k in 1:10000) {
X <- MC(10, mu, P)
data <- c(data, X[11])
}
hist(data, breaks=seq(0.5,4.5), probability=TRUE,
main=NULL, col="gray80", border="gray20")
The histogram allows us to ﬁnd estimated probabilities for the different states of the
Markov chain: from the plot (not shown here) we read off the values P(X10 = 1) ≈
0.22, P(X10 = 2) ≈0.6, P(X10 = 3) ≈0.09, and P(X10 = 4) ≈0.09.
(c) We have to compute μ⊤P10:
> mu %*% P %*% P %*% P %*% P %*% P %*% P %*% P %*% P %*% P %*% P
[,1]
[,2]
[,3]
[,4]
[1,] 0.2308349 0.5948259 0.08716961 0.08716961
The result coincides well with the values we read off the histogram.
Solution E2.4
(a) This follows directly from the properties of a stochastic matrix:
(Pv)y =

x∈S
pxyvy =

x∈S
pxy1 = 1 = 1 · vy
for all y ∈S. Thus, v is an eigenvector with eigenvalue 1.
(b) Assume Av = λv. Then A(αv) = αAv = α(λv) = λ(αv).
(c) We ﬁnd the eigenvalues of P as follows:
> E <- eigen(t(P))
$values
[1] 1.0000000 0.9000000 0.9000000 0.5666667
$vectors
[,1]
[,2]
[,3]
[,4]
[1,] -0.2873479 -3.925231e-17 -3.925231e-17 -0.7071068
[2,] -0.9578263 -7.071068e-01 -7.071068e-01
0.7071068
[3,]
0.0000000
7.071068e-01
0.000000e+00
0.0000000
[4,]
0.0000000
0.000000e+00
7.071068e-01
0.0000000

318
APPENDIX C: ANSWERS TO THE EXERCISES
Looking at the ﬁrst column of this matrix (corresponding to the eigenvalue
1.0000000), we see that
v = (−0.2873479, −0.9578263, 0.0000000, 0.0000000)
is the (only) eigenvector of P⊤with eigenvalue 1. Thus, by normalising this vector
to be a probability vector, we ﬁnd the stationary distribution of P:
> v <- E$vectors[,1]
> v / sum(v)
[1] 0.2307692 0.7692308 0.0000000 0.0000000
Solution E2.5
To simulate a Poisson process with constant intensity, we can
directly follow the steps described in example 2.38. In R, we can use the following
function:
PoissonProcessConst <- function(a, b, lambda) {
N <- rpois(1, lambda * (b-a))
X <- runif(N, a, b)
return(X)
}
If we want the function to return the points in increasing order, we could replace the
return statement by return(sort(X)).
Solution E2.6
To implement the required function in R, we can directly follow the
steps described in example 2.39:
PoissonProcessStep <- function(t, lambda) {
stopifnot(length(t) == length(lambda)+1)
X <- c()
for (i in 1:length(lambda)) {
N <- rpois(1, lambda * (t[i+1]-t[i]))
X <- c(X, runif(N, t[i], t[i+1]))
}
return(X)
}
The function can be used as follows:
t <- c(1.4, 3,
4, 6,
6.2, 7.9,
8,
9.5)
lambda <- c(1, 4.5, 2, 1.4,
0,
1.1, 0.8)
X <- PoissonProcessStep(t, lambda)
The output of a call such as this has been used to determine the position of the marked
points in Figure 2.3.

APPENDIX C: ANSWERS TO THE EXERCISES
319
C.3
Answers for Chapter 3
Solution E3.1
We can use the rejection algorithm suggested in example 3.6,
that is we apply the rejection sampling algorithm 1.22 with target density f (x) =
exp(−y2/2x −x)/√x, proposal density g(x) = exp(−x) and c = exp(−1/2)/|y|.
The condition cg(X)U ≤f (X) from the algorithm takes the following form:
cg (X)U ≤f (X)
⇐⇒
1
|y| exp(−1/2) exp(−x)U ≤
1
√x exp

−y2/2x −x
	
⇐⇒
U ≤
|y|
√x exp

−y2/2x + 1/2
	
.
This gives the following algorithm for generating samples from the conditional
distribution of X given Y = y:
(a) Generate X ∼Exp(1).
(b) Generate U ∼U[0, 1].
(c) Accept X if U ≤|y|
√x exp

−y2/2x + 1/2
	
.
We can implement this rejection algorithm in R as follows:
GeneratePosteriorSamples <- function(n, y) {
res <- c()
while (length(res) < n) {
X <- rexp(1)
U <- runif(1)
if (U <= abs(y) * exp(-y^2/(2*X) + 0.5) / sqrt(X)) {
res <- c(res, X)
}
}
return(res)
};
Finally, Monte Carlo estimation can be used to get approximations for the mean
E(X |Y = 4) and the variance Var(X |Y = 4):
> X <- GeneratePosteriorSamples(10000, 4)
> mean(X)
[1] 3.329493
> var(X)
[1] 1.934953
This shows that the posterior expectation is approximately 3.33 and the posterior
variance is approximately 1.93.

320
APPENDIX C: ANSWERS TO THE EXERCISES
Solution E3.2
To compute a single Monte Carlo estimate for E

sin(X)2	
we can
use the following function:
GetMCEstimate <- function(N) {
X <- rnorm(N)
return(mean(sin(X)^2))
}
This function takes N as its only parameter and computes one instance of the Monte
Carlo estimate ZMC
N . An estimate for E(sin(X)2) can be obtained with the following
command:
> GetMCEstimate(1000000)
[1] 0.4318852
We want to compare the variances of the Monte Carlo estimates for N = 1000 and
N = 10 000. To do so, we ﬁrst generate estimates repeatedly and then plot a histogram
of the resulting values:
estimates <- replicate(10000, GetMCEstimate(1000))
range <- c(min(estimates), max(estimates))
hist(estimates, breaks=seq(range[1], range[2], length.out=50),
xlab=expression(Z[1000]), xlim=range, ylim=c(0,2100),
main=NULL, col="gray80", border="gray20")
By taking a huge value of N we can generate a single estimate which is close to
the true value of the expectation. This value can be marked in the histogram using
the following commands:
good.estimate <- GetMCEstimate(1000000)
abline(v=good.estimate)
The resulting plot is shown in Figure 3.1(a). The ﬁgure shows that the exact value for
the expectation is approximately 0.43, the estimates for N = 1000 range from about
0.40 to about 0.46. The width of this range is more than 10% of the exact value and
thus a single estimate for N = 1000 would not be a very accurate approximation for
the mean.
Finally, for the second part of the question, we repeat the experiment with Monte
Carlo sample size N = 10 000:
estimates2 <- replicate(10000, GetMCEstimate(10000))
hist(estimates2, breaks=seq(range[1], range[2], length.out=50),
xlab=expression(Z[10000]), xlim=range, ylim=c(0,2100),
main=NULL, col="gray80", border="gray20")
abline(v=good.estimate)

APPENDIX C: ANSWERS TO THE EXERCISES
321
The new histogram is shown in Figure 3.1(b). As expected from proposition 3.14,
the variance of the samples in the second histogram is signiﬁcantly lower than in the
ﬁrst histogram.
Solution E3.3
The Monte Carlo estimate for E

cos(X)
	
is given by
ZMC
N
= 1
N
N

j=1
cos(X j),
where X1, . . . , X N ∼N(0, 1) are i.i.d. The difﬁculty lies in the choice of N.
The question asks us to obtain and estimate with ‘three digits of accuracy’. This
statement is open to interpretation and to answer the question, we need to choose
which interpretation of ‘three digits of accuracy’ we choose. Here we interpret ‘three
digits of accuracy’ to mean MSE(ZMC
N ) ≤0.0012. (The square on the right-hand side
is needed, since the mean squared error is a squared quantity.) Since
Var

cos(X)
	
= E

cos(X)2	
−E

cos(X)
	2 ≤1 −0 = 1,
we can use the estimate
MSE

ZMC
N
	
≤Var (cos(X))
N
≤1
N .
Using this estimate we see that N = 106 is enough to achieve MSE(ZMC
N ) ≤10−6 =
0.0012. A computer experiment can be used to get a numerical value for the expec-
tation:
> N <- 1e6
> X <- rnorm(N)
> mean(cos(X))
[1] 0.6064109
Thus, our estimate is E(cos(X)) ≈0.606.
Solution E3.4
From equation (3.5) we get
P(X ≤a) =
 a
−∞
1
√
2π
e−x2/2 dx
= 1
2 +
1
√
2π
 a
0
e−x2/2 dx
= 1
2 +
a
√
2π
E

e−a2U 2/2	

322
APPENDIX C: ANSWERS TO THE EXERCISES
where U ∼U[0, 1]. Since the estimate ˜pN is obtained by replacing the expectation
on the right-hand side with the corresponding Monte Carlo estimate, we know what
˜pN →p as N →∞and that ˜pN is unbiased.
Since both estimates are unbiased, we have
MSE(pN) = Var(pN) = 1
N Var

1[a,∞)(X)
	
and
MSE( ˜pN) = Var( ˜pN) = Var
1
2 +
a
√
2π N
N

j=1
e−a2U 2/2
= 1
N · a2
2π Var

e−a2U 2/2	
.
To estimate the MSE of the estimator pN we can use the following R
commands:
> a <- 1
> X <- rnorm(1000000)
> var(X >= a)
[1] 0.1332084
Thus, we have MSE(pN) ≈0.1332/N. For the estimator ˜pN we get:
> U <- runif(1000000)
> var(exp(-a^2*U^2/2)) * a^2 / (2*pi)
[1] 0.002345373
that is we have MSE( ˜pN) ≈0.0023/N. Thus the estimator ˜pN has much smaller
error than the estimator pN does.
Solution E3.5
The key observation which allows to compute ˆσ 2 using only a
constant amount of memory is that ˆσ 2 can be written as
ˆσ 2 =
1
N −1
N

j=1

f (X j)2 −2 f (X j)ZMC
N
+

ZMC
N
	2
=
1
N −1
N

j=1
f (X j)2 −2 ZMC
N
N −1
N

j=1
f (X j) +
N
N −1

ZMC
N
	2
=
1
N −1
N

j=1
f (X j)2 −
N
N −1(ZMC
N )2
=
1
N −1
N

j=1
f (X j)2 −
1
N(N −1)
 N

j=1
f (X j)
2
.

APPENDIX C: ANSWERS TO THE EXERCISES
323
Using this formula, we can compute ZMC
N
and ˆσ 2 simultaneously, using the following
steps:
1: s ←0
2: t ←0
3: for j = 1, 2, . . . , N do
4:
generate X j
5:
s ←s + f (X j)
6:
t ←t + f (X j)2
7: end for
8: return ZMC
N
= s/N and ˆσ 2 = (t −s2/N)/(N −1).
This algorithm requires a constant amount of memory, since only the variables s and
t need to be stored between iterations of the loop.
Solution E3.6
We consider importance sampling estimates of the probability
p = P(X ∈A), where X ∼N(0, 1) and A = [3, 4]. If samples Y j ∼ψ are used, the
corresponding importance sampling estimate for p is given by
ˆp = 1
N
N

j=1
1A(Y j) ϕ(Y j)
ψ(Y j),
where
ϕ(x) =
1
√
2π
exp

−x2
2

is the density of X.
(a) An importance sampling estimate ˆp, together with the sample variance ˆσ 2 can
be computed using the following R code
GetISEstimate <- function(N, gen.Y, psi) {
Y <- gen.Y(N)
phi <- function(x) dnorm(x, 0, 1);
weighted.samples <- (Y >= 3 & Y <= 4) * phi(Y) / psi(Y)
return(list(p=mean(weighted.samples),
var=var(weighted.samples)))
}
The R function GetISEstimate takes three arguments: N is the sample size, gen.Y is
an R function to generate the samples Y1, . . . , YN and psi is an R function to compute
the density ψ. The function GetISEstimate can be called as follows:
> gen.Y1 <- function(N) { rnorm(N, 1, 1) }
> psi1 <- function(x) { dnorm(x, 1, 1) }
> GetISEstimate(10000, gen.Y1, psi1)
$p
[1]
0.001243597

324
APPENDIX C: ANSWERS TO THE EXERCISES
The output of GetISEstimate is a list which contains the estimate for p in the ﬁeld
$p and the estimated variance of the weighted samples in $var.
To get a list of estimated variances for all four methods, we have to call GetISEs-
timate repeatedly. One way of doing so is to deﬁne the following helper functions:
PrintISVariance <- function(name, gen.Y, psi) {
est <- GetISEstimate(10000, gen.Y, psi)
cat(name, "
-->
Var = ", est$var, "\n", sep="")
return(est$var)
}
gen.Y2 <- function(N) { rnorm(N, 2, 1) }
psi2 <- function(x) { dnorm(x, 2, 1) }
gen.Y3 <- function(N) { rnorm(N, 3.5, 1) }
psi3 <- function(x) { dnorm(x, 3.5, 1) }
gen.Y4 <- function(N) { rexp(N) + 3 }
psi4 <- function(x) { dexp(x-3) }
PrintAllVariances <- function() {
v1 <- PrintISVariance("N(1,1)
", gen.Y1, psi1)
v2 <- PrintISVariance("N(2,1)
", gen.Y2, psi2)
v3 <- PrintISVariance("N(3.5,1)", gen.Y3, psi3)
v4 <- PrintISVariance("Exp(1)+3", gen.Y4, psi4)
return(c(v1, v2, v3, v4))
}
These functions determine the variances for all four estimators. The result is as
follows:
> res <- PrintAllVariances()
N(1,1)
-->
Var = 7.882264e-05
N(2,1)
-->
Var = 1.400159e-05
N(3.5,1)
-->
Var = 6.666196e-06
EXP(1)+3
-->
Var = 1.897206e-06
The results show that the method which uses Y j ∼Exp(1) + 3 has the smallest
variance and thus gives the most accurate results for given sample size N. We store
the list of variances in the variable res for use in (c).
(b) We use the most efﬁcient method, identiﬁed in the previous part of the question,
to obtain a good estimate for p:
> N <- 10000
> est <- GetISEstimate(N, gen.Y4, psi4)
> est$p
[1] 0.001329137
> 1.96 * sqrt(est$var) / sqrt(N)
[1] 2.70939e-05

APPENDIX C: ANSWERS TO THE EXERCISES
325
The output shows that the computed value for the estimate is ˆp = 0.001329137
and that a 95% conﬁdence interval for p around ˆp has a width of approximately
2 · 2.7 · 10−5 = 0.000054. Since the width of the conﬁdence interval is much smaller
than the value itself, we expect ˆp to be an accurate estimate.
(c) To get an estimate with an error of 1% we choose N such that MSE( ˆp) = ε2,
where ε = 0.01 ˆp. Solving (3.18) for N we ﬁnd that the required condition for N is
given by
N ≥ˆσ 2
ε2 =
ˆσ 2
(0.01 ˆp)2 .
Using the variances from (a) we get the following results:
> eps <- 0.01 * est$p
> ceiling(res / eps^2)
[1]
446182
79257
37735
10740
For comparison we note that from equation (3.11) we know that the corresponding
lower bound for a basic Monte Carlo estimate is N ≥p(1 −p)/ε2:
> est$p * (1 - est$p) / eps^2
[1]
7513676
We see that computation of the basic Monte Carlo estimate will take approximately
7513676/10740 ≈700 times as long as computing the best of the importance sam-
pling estimates considered here.
Solution E3.7
We study how Monte Carlo estimation can be used to estimate the
bias of the estimator ˆρ(X, Y) from equation (3.23).
(a) The estimator ˆρ(X, Y) can be implemented as an R function as follows:
rxy <- function(x, y) {
x.bar <- mean(x)
y.bar <- mean(y)
numerator <- mean((x-x.bar)*(y-y.bar))
denominator <- sqrt(mean((x-x.bar)^2)*mean((y-y.bar)^2))
return(numerator/denominator)
}
Using this function, the bias of ˆρ(X, Y) can be estimated by the following
program: we generate random pairs (X, Y) with given length n and given
correlation ρ as in the previous question. The following function generates
samples ˆρ(X j, Y j) for j = 1, 2, . . . , N:
rhohat.sample <- function(N, n, rho) {
res <- c()

326
APPENDIX C: ANSWERS TO THE EXERCISES
for (j in 1:N) {
X <- rnorm(n)
eta <- rnorm(n)
Y <- rho*X + sqrt(1-rho^2)*eta
res[j] <- rxy(X, Y)
}
return(res)
}
Using the output of this function, a Monte Carlo estimate for the bias can be
computed as follows:
EstimateBias <- function(N, n, rho) {
return(mean(rhohat.sample(N, n, rho)) - rho)
}
Since ˆρ(X, Y) ∈[−1, 1], we have the upper bound Var

ˆρ(X, Y)
	
≤1 and
thus the variance of the estimate for the bias is smaller than 1/N. Conse-
quently, for N = 10 000, the standard deviation of the estimate is guaranteed
to be smaller than 0.01 (and in reality will be much smaller than this). Since
correlations lie in the range [−1, +1], bringing the error below 0.01 seems
sufﬁcient; thus we will use N = 10 000 in (b) and (c).
(b) We can call the function EstimateBias in a loop in order to plot the estimated
bias as a function of ρ. Assuming n = 10, we can use the following program:
n <- 10
N <- 10000
rho.values <- seq(-1, +1, by=0.05)
bias.values <- c()
for (rho in rho.values) {
cat("rho =", rho, "\n")
bias.values <- c(bias.values, EstimateBias(N, n, rho))
}
plot(rho.values, bias.values,
xlab=expression(rho), ylab="estimated bias")
The resulting data points in the plot are still affected by random noise, caused
by the Monte Carlo error for ﬁnite N. To make the picture clearer, we can
add a smoothed curve through the estimated values:
s <- smooth.spline(rho.values, bias.values, spar=0.6)
lines(s)
The resulting plot is shown in Figure 3.2.
(c) We can compute ˆρ(X, Y) for the given data as follows:
X <- c(0.218, 0.0826, 0.091, 0.095, -0.826,
0.208, 0.600, -0.058, 0.602, 0.620)

APPENDIX C: ANSWERS TO THE EXERCISES
327
Y <- c(0.369, 0.715, -1.027, -1.499, 1.291,
-0.213, -2.400, 1.064, -0.367, -1.490)
print(rxy(X,Y))
The result is -0.6917216. From Figure 3.2 we know that the bias for ρ-values
of this magnitude is about 0.02, that is on average ˆρ(X, Y) will over-estimate
ρ by about 0.02. Consequently, −0.69 −0.02 is an approximately unbiased
estimate for ρ.
Solution E3.8
We can use the following R code to get the Monte Carlo estimate
of the conﬁdence coefﬁcients:
EstimateConfidenceCoefficient <- function(n, lambda, N=100000){
p.alpha <- qt(0.975, df=n-1)
k <- 0
for (j in 1:N) {
X <- rpois(n, lambda)
m <- mean(X)
sigma.hat <- sd(X)
d <- p.alpha * sigma.hat / sqrt(n)
if (m - d <= lambda && lambda <= m + d) {
k <- k + 1
}
}
return(k / N)
}
To call this function for a range of different λ, we can use a loop such as the
following:
lambda.list <- c()
prob.list <- c()
width.list <- c()
for (lambda in seq(0.025, 1, length.out=40)) {
prob <- EstimateConfidenceCoefficient(10, lambda)
print(c(lambda, prob))
lambda.list <- c(lambda.list, lambda)
prob.list <- c(prob.list, prob)
}
The results of the estimation are stored in the vector prob.list, the corresponding
values of λ are stored in lambda.list. Finally, we create a plot of the results:
plot(lambda.list, prob.list, ylim=c(0,1),
xlab=expression(lambda), ylab="confidence coefficient")

328
APPENDIX C: ANSWERS TO THE EXERCISES
The plot, shown in Figure 3.3, clearly shows that for Poisson-distributed data the
interval (3.26), at least when λ is allowed to be arbitrarily small, has an extremely
low conﬁdence coefﬁcient and thus is not a useful conﬁdence interval.
C.4
Answers for Chapter 4
Solution E4.1
To implement the random walk Metropolis method described in
example 4.10, we can use the following R function:
GenerateMCMCSample <- function(N, sigma) {
X <- numeric(length=N)
Xj <- runif(1, -1, 1)
for (j in 1:N) {
Yj <- rnorm(1, Xj, sigma)
if (abs(Yj) <= 3 * pi) {
alpha <- (Xj * sin(Yj) / (Yj * sin(Xj))) ^ 2
} else {
alpha <- 0
}
U <- runif(1)
if (U < alpha) {
Xj <- Yj
}
X[j] <- Xj
}
return(X)
}
Some care is needed with the choice of the initial value X0: since, at least in our
implementation, α(x, y) is undeﬁned for x = 0, we cannot start the process with
X0 = 0. Instead we start with a random initial point X0 ∼U[−1, 1]. This choice
avoids the problem at 0 and, at the same time, guarantees that X0 falls into a region
where π is large. Figure 4.1 shows paths resulting from three runs of the algorithm,
for different values of σ.
Solution E4.2
To implement the random walk Metropolis method in R we can use
the following functions:
pi <- function(x, log=FALSE) {
dnorm(x, 100, 1, log=log)
}
p <- function(x, y, log=FALSE) {
dnorm(y, x, log=log)
}

APPENDIX C: ANSWERS TO THE EXERCISES
329
alpha <- function(x, y) {
return(exp(pi(y, log=TRUE) + p(y, x, log=TRUE)
- pi(x, log=TRUE) - p(x, y, log=TRUE)))
}
MCMC <- function(X0, N) {
X <- numeric(length=N)
Xj <- X0
for (j in 1:N) {
Yj <- rnorm(1, Xj)
U <- runif(1)
if (U < alpha(Xj, Yj)) {
Xj <- Yj
}
X[j] <- Xj
}
return(c(X0, X))
}
These functions can be used as follows:
N <- 1000
j <- 0:N
X <- MCMC(0, N)
plot(j, X, type="l", ylab=expression(X[j]))
The resulting plot is shown in Figure 4.2. Experiments show that the process always
moves towards the value 100 with approximately constant speed. Once the process
is close to 100, it starts ﬂuctuating around this value. A good choice for the burn-in
period is, for example the time until the process reaches the interval [99, 101]. In
this simple example it would be even better to always start with X0 = 100 instead of
using a burn-in period.
Solution E4.3
In exercise E4.1 we have already written an R-function to generate
paths from the random walk Metropolis algorithm with target density π. Using
this function, named GenerateMCMCSample we can generate plots of the estimated
autocorrelations as follows:
PlotAutocorrelation <- function(N, sigma, ...) {
X <- GenerateMCMCSample(N, sigma)
a <- acf(X^2, lag.max=200, ci=0, ylab=expression(rho[k]), ...)
text(195, 0.92, bquote(sigma==.(sigma)))
# a$acf stores the autocorrelation coefficients,
# a$acf[1] = rho_0 = 1, a$acf[2] = rho_1, ...

330
APPENDIX C: ANSWERS TO THE EXERCISES
eff <- 1/(1 + 2 * sum(a$acf[-1]))
print(c(eff, 1/eff))
}
Here we use the built-in R function acf to estimate the autocorrelation of the samples.
The function PlotAutocorrelation can be used as follows:
> PlotAutocorrelation(1000000, 6)
[1] 0.1520343
Solution E4.4
There are two different ways we can estimate the required acceptance
probability. We can either use the ratio of accepted samples to proposed samples in
the algorithm, or we can take the average of the acceptance probabilities α(X j−1, Y j)
for j = 1, 2, . . . , N. Here we use the second of these methods:
AverageAcceptanceRate <- function(N, sigma) {
asum <- 0;
Xj <- runif(1, -1, 1)
for (j in 1:N) {
Yj <- rnorm(1, Xj, sigma)
if (abs(Yj) <= 3 * pi) {
alpha <- min((Xj * sin(Yj) / (Yj * sin(Xj))) ^ 2, 1)
} else {
alpha <- 0
}
U <- runif(1)
if (U < alpha) {
Xj <- Yj
}
asum <- asum + alpha
}
return(asum / N)
}
xx <- c()
yy <- c()
for (p in seq(-2, 3, 0.2)) {
sigma <- 10^p
cat("sigma = ", sigma, "\n", sep="")
xx <- c(xx, sigma^2)
yy <- c(yy, AverageAcceptanceRate(100000, sigma))
}
plot(xx, yy, type="l", log="x",
xlab=expression(sigma^2), ylab="average acceptance probability")

APPENDIX C: ANSWERS TO THE EXERCISES
331
abline(v=1, lty=3)
abline(v=6^2, lty=3)
abline(v=36^2, lty=3)
The output of this program is shown in Figure 4.4.
Solution E4.5
The random walk Metropolis algorithm for the posterior distribution
can be implemented as follows:
GeneratePosteriorSample <- function(N, x, eta) {
n <- length(x)
mu <- mean(x)
sigma <- sd(x)
theta <- matrix(nrow=N, ncol=2)
for (j in 1:N) {
eps <- rnorm(2, 0, eta)
mu.tilde <- mu + eps[1]
sigma.tilde <- sigma + eps[2]
if (-10 <= mu.tilde && mu.tilde <= 10 && sigma.tilde > 0) {
a <- n * (log(sigma) - log(sigma.tilde))
b <- sum((x - mu.tilde)^2) / (2 * sigma.tilde) + sigma.tilde
c <- sum((x - mu)^2) / (2 * sigma) + sigma
alpha <- min(exp(a - b + c), 1)
} else {
alpha <- 0
}
U <- runif(1)
if (U < alpha) {
mu <- mu.tilde
sigma <- sigma.tilde
}
theta[j,1] <- mu
theta[j,2] <- sigma
}
return(theta)
}
The function can be used as follows:
n <- 100
x <- rnorm(n, 5, 2)
N <- 10000
theta <- GeneratePosteriorSample(N, x, 0.1)

332
APPENDIX C: ANSWERS TO THE EXERCISES
5.0
5.4
5.8
μj
0
2000
4000
6000
8000
1.4 1.8 2.2 2.6
j
σj
Figure C.8
Output of the program from exercise E4.5.
The path of the Markov chain is now stored in theta. The value theta[j,1] corre-
sponds to μ j and the entry theta[j,1] stores σ j. These data can now be used for
Monte Carlo estimates. Here we restrict ourselves to plotting the data:
j = 1:N
plot(j, theta[j,1], type="l", ylab=expression(mu[j]))
plot(j, theta[j,2], type="l", ylab=expression(sigma[j]))
The resulting plot is shown in Figure C.8.
Solution E4.6
The Gibbs sampler described in section 4.4.2 can be implemented
in R as follows:
GeneratePosteriorSample <- function(N, burn.in=1000) {
muj <- matrix(runif(2*k, -10, 10), nrow=k, ncol=2)
Yj <- sample(1:k, n, replace=TRUE)
for (j in 1:(burn.in + N)) {
if (j %% 2 == 1) {
for (a in 1:k) {
ind <- Yj == a
na <- sum(ind)
if (na > 0) {
m <- c(mean(x[ind,1]), mean(x[ind,2]))
s <- sigma / sqrt(na)
xi <- c(999, 999)
# this is re-set in the loop below
while (xi[1] < -10 || xi[1] > 10
|| xi[2] < -10 || xi[2] > 10) {

APPENDIX C: ANSWERS TO THE EXERCISES
333
xi = rnorm(2, m, s)
}
if (j > burn.in) {
points(xi[1], xi[2], pch=a+1, cex=0.6)
}
} else {
xi <- runif(2, -10, 10)
}
muj[a,] <- xi
}
} else {
for (i in 1:n) {
q <- numeric(length=k)
for (a in 1:k) {
q[a] <- (dnorm(x[i,1], muj[a,1], sigma)
* dnorm(x[i,2], muj[a,2], sigma))
}
Yj[i] <- sample(1:k, 1, prob=q)
}
}
}
}
The function assumes that the number of modes is stored in the global variable k,
the number of observations is stored in n and the observations are stored in an n × 2
matrix x. For simplicity we plot the generated samples inside this function (using the
points command) instead of returning the values.
To test the function we generate a synthetic data set from the model:
k <- 5
n <- 100
sigma <- 1
true.mu <- matrix(runif(2*k, -10, 10), nrow=k, ncol=2)
true.y <- sample(1:k, n, replace=TRUE)
x <- matrix(nrow=n, ncol=2)
x[,1] <- rnorm(n, true.mu[true.y, 1], sigma)
x[,2] <- rnorm(n, true.mu[true.y, 2], sigma)
A sample from the posterior distribution of the cluster means μ1, . . . , μn given
the observations stored in x can now be obtained by calling the function Gener-
atePosteriorSample as follows:
plot(x[,1], x[,2], xlim=c(-10,10), ylim=c(-10,10),
xlab=expression(X[list(i,1)]), ylab=expression(X[list(i,2)]),

334
APPENDIX C: ANSWERS TO THE EXERCISES
col="gray60", asp=1)
GeneratePosteriorSample(100)
The results of two different runs of this program are shown in Figure 4.5 (a typical
output) and Figure 4.6 (an exceptional output, where the Markov chain has not yet
converged).
Solution E4.7
Since the Gibbs sampler for the Ising model, as given by algorithm
4.31, can require a large number of iterations to reach equilibrium, we take some care
to provide an efﬁcient implementation:
r To avoid special-casing the pixels on the boundaries of the grid in step 4, we
add extra rows/columns of zeros around the image, resulting in an extended
grid size of (L + 2) × (L + 2).
r Since the variable d computed in step 4 can only take the ﬁve possible values
−4, −2, 0, 2, and 4, we can pre-compute the resulting probability weights
πXm|X¬m(+1|x¬m) =
exp(+βd)
exp(+βd) + exp(−βd) =
1
1 + exp(−2βd),
to avoid evaluating the exponential function inside the main loop of the algo-
rithm.
The resulting method can be implemented in R as follows:
RunGibbs <- function(M, beta, L) {
X <- matrix(0, nrow=L+2, ncol=L+2)
pixels <- sample(c(-1,+1), size=L*L, replace=TRUE)
X[2:(L+1),2:(L+1)] <- matrix(pixels, nrow=L, ncol=L)
p.table <- numeric(length=5)
for (d in seq(-4, 4, by=2)) {
p.table[(d+6)/2] <- 1 / (1 + exp(-2 * beta * d))
}
for (sweep in 1:M) {
n <- matrix(0, nrow=L, ncol=L)
for(ix in 2:(L+1)) {
for (iy in 2:(L+1)) {
d <- X[iy-1, ix] + X[iy+1, ix] + X[iy, ix-1] + X[iy, ix+1]
p <- p.table[(d+6)/2]
X[iy, ix] <- sample(c(-1, +1), size=1, prob=c(1-p, p))
}
}
}
return(X[2:(L+1),2:(L+1)])
}

APPENDIX C: ANSWERS TO THE EXERCISES
335
For given inverse temperature β and given M, the program performs N = M · L2
steps of algorithm 4.31, that is it performs M ‘sweeps‘ over the complete picture,
and returns the ﬁnal state X(N) ∈S. This state can then be plotted using R commands
such as image(1:L, 1:L, X). The results of different runs of this function are shown
in Figure 4.7.
It is tempting to attempt to implement a much faster version of the program
which would update all pixels in parallel. Unfortunately step 4 of algorithm 4.31
references pixels which have only been updated during the current ‘sweep’ over the
image (X( j−1)
m1−1,m2 and X( j−1)
m1,m2−1), making parallel updates difﬁcult.
Solution E4.8
For implementing algorithm 4.32 in R, we follow the steps described
in the solution to exercise E4.7, and only modify the expression for sampling from
the posterior distribution pXm|X¬m to include the given observation Y of the original
image:
RunGibbs <- function(M, beta, Y, sigma, burn.in=100) {
L <- ncol(Y)
count <- matrix(0, nrow=L, ncol=L)
X <- matrix(0, nrow=L+2, ncol=L+2)
pixels <- sample(c(-1,+1), size=L*L, replace=TRUE)
X[2:(L+1),2:(L+1)] <- matrix(pixels, nrow=L, ncol=L)
for (sweep in 1:(burn.in + M)) {
for (ix in 2:(L+1)) {
for (iy in 2:(L+1)) {
d <- X[iy-1, ix] + X[iy+1, ix] + X[iy, ix-1] + X[iy, ix+1]
p <- 1 / (1 + exp(-2 * (beta * d + Y[iy-1, ix-1] / sigma^2)))
X[iy, ix] <- sample(c(-1, +1), size=1, prob=c(1-p, p))
}
}
if (sweep > burn.in) {
count <- count + (X[2:(L+1),2:(L+1)] == 1)
}
}
return(count / M)
}
The return value of this function is a matrix of the same size as Y, giving the posterior
probability that the corresponding pixel in the original image was black. The output
of this function can be plotted using commands such as the following.
prob <- RunGibbs(N, beta, Y, sigma)
image(1:L, 1:L, prob)
The result of three different runs of this function is shown in Figure 4.8.

336
APPENDIX C: ANSWERS TO THE EXERCISES
Solution E4.9
We start by implementing the mixture model from equation (4.47)
in R. Samples from the mixture distribution can be generated using the following
function:
model.clusters.mean <- 4
# the average number of clusters
model.r.min <- 0.5
# smallest possible stddev for a cluster
model.r.max <- 2.5
# largest possible stddev for a cluster
GenerateMixtureSample <- function(n) {
# Sample the parameters:
k <- rpois(1, lambda=model.clusters.mean-1) + 1
mu <- matrix(nrow=2, ncol=k)
mu[1,] <- runif(k, -10, 10)
mu[2,] <- runif(k, -10, 10)
sigma <- runif(k, model.r.min, model.r.max)
# Generate a sample of size n from the mixture distribution:
a <- sample.int(k, n, replace=TRUE)
x_1 <- rnorm(n, mu[1,a], sigma[a])
x_2 <- rnorm(n, mu[2,a], sigma[a])
return(rbind(x_1, x_2))
}
We will need a function to evaluate the posterior distribution as given in equation
(4.51). This is implemented in the function GetLogPosterior:
GetLogPrior <- function(mu, r) {
# Since both, x and y coordinates of the mean have the same
# distribution, we can merge them into one list:
log.p.mu <- sum(dunif(as.vector(mu), -10, 10, log=TRUE))
log.p.r <- sum(dunif(r, model.r.min, model.r.max, log=TRUE))
k <- length(r)
log.p.k <- dpois(k-1, lambda=model.clusters.mean-
1, log=TRUE)
return(log.p.mu + log.p.r + log.p.k)
}
GetLogMixDens <- function(mu, r, x) {
mix.dens <- mean(dnorm(mu[1,], x[1], r) * dnorm(mu[2,], x[2], r))
return(log(mix.dens))
}
GetLogPosterior <- function(mu, r, obs) {

APPENDIX C: ANSWERS TO THE EXERCISES
337
log.prior <- GetLogPrior(mu, r)
# Avoid calling GetLogMixDens for negative variances:
if (log.prior == -Inf) return(-Inf)
log.p.obs <- 0
for (i in 1:ncol(obs)) {
tmp <- GetLogMixDens(mu, r, obs[,i])
log.p.obs <- log.p.obs + tmp
}
return(log.p.obs + log.prior)
}
To avoid problems caused by rounding errors, this function computes log π(k, x)
instead of π(k, x). Some care is needed, since the function GetLogPosterior may be
called with ‘impossible’ parameter values, for example some components of r may
be negative; in these cases, log 0 = −∞must be returned.
Using the function GetLogPosterior to deﬁne the target density and using
the different move types described in section 4.5.2, we can now implement
algorithm 4.36.
The RJMCMC method can be implemented by the following R code. We start by
introducing names for the different move probabilities:
MCMC.prob.mu.move <- 4 / 10
MCMC.prob.r.move <- 4 / 10
MCMC.prob.k.move <- 1 / 10
MCMC.prob.rot.move <- 1 / 10
MCMC.move.names = c("mu", "r", "k", "rot")
MCMC.move.probabilities <- c(MCMC.prob.mu.move,
MCMC.prob.r.move,
MCMC.prob.k.move,
MCMC.prob.rot.move)
names(MCMC.move.probabilities) <- MCMC.move.names
Moves of types mμ and mr depend on the parameters σμ and σr, respectively.
Experimentation shows that the following values lead to reasonable acceptance rates:
MCMC.sigma.mu <- 0.28
MCMC.sigma.r <- 0.22
Next we deﬁne functions to compute the acceptance probabilities given in equa-
tion (4.52), equation (4.53), equation (4.55), equation (4.56), equation (4.57) and
equation (4.58).
GetMuMoveAlpha <- function(GetLogPi, mu1, mu2, r) {

338
APPENDIX C: ANSWERS TO THE EXERCISES
return(exp(GetLogPi(mu2, r) - GetLogPi(mu1, r)))
}
GetRadiusMoveAlpha <- function(GetLogPi, mu, r1, r2) {
return(exp(GetLogPi(mu, r2) - GetLogPi(mu, r1)))
}
GetLogPsi <- function(mu, r) {
log.p.mu <- sum(dunif(mu, -10, 10, log=TRUE))
log.p.r <- dunif(r, model.r.min, model.r.max, log=TRUE)
return(log.p.mu + log.p.r)
}
GetAddMoveAlpha <- function(GetLogPi, mu1, mu2, r1, r2) {
k.new <- length(r2)
a <- exp(GetLogPi(mu2, r2)
- GetLogPi(mu1, r1)
- GetLogPsi(mu2[,k.new], r2[k.new]))
return(ifelse(k.new == 2, a / 2, a))
}
GetRemoveMoveAlpha <- function(GetLogPi, mu1, mu2, r1, r2) {
k.old <- length(r1)
a <- exp(GetLogPi(mu2, r2)
+ GetLogPsi(mu1[,k.old], r1[k.old])
- GetLogPi(mu1, r1))
return(ifelse(k.old == 2, 2 * a, a))
}
With all these preparations in place, we can now implement algorithm 4.36:
RunRJMCMC <- function(GetLogPi, N, count.every=1) {
types <- length(MCMC.move.probabilities)
count.proposals <- rep(0, types)
names(count.proposals) <- MCMC.move.names
count.accepts <- rep(0, types)
names(count.accepts) <- MCMC.move.names
path.k <<- c()
for (j in 1:N) {
move.type = sample.int(types, 1,
prob=MCMC.move.probabilities)
count.proposals[move.type] <- count.proposals[move.type] + 1
k2 <- k

APPENDIX C: ANSWERS TO THE EXERCISES
339
Mu2 <- Mu
R2 <- R
if (move.type == 1) {
# try a mean move
a <- sample.int(k, 1)
Mu2[,a] <- rnorm(2, Mu[,a], MCMC.sigma.mu)
alpha <- GetMuMoveAlpha(GetLogPi, Mu, Mu2, R)
} else if (move.type == 2) {
# try a radius move
a <- sample.int(length(R), 1)
R2[a] <- rnorm(1, R[a], MCMC.sigma.r)
alpha <- GetRadiusMoveAlpha(GetLogPi, Mu, R, R2)
} else if (move.type == 3) {
if (k == 1 || runif(1) < 0.5) { # try to add a component
k2 <- k + 1
Mu2 <- cbind(Mu, runif(2, -10, 10))
R2 <- c(R, runif(1, model.r.min, model.r.max))
alpha <- GetAddMoveAlpha(GetLogPi, Mu, Mu2, R, R2)
} else {
# try to remove a component
k2 <- k - 1
# prevent R from converting Mu2 to a vector if k2==1:
Mu2 <- as.matrix(Mu[,-k], nrow=k2, ncol=2)
R2 <- R[-k]
alpha <- GetRemoveMoveAlpha(GetLogPi, Mu, Mu2, R, R2)
}
} else {
# cyclic right shift of components by a:
if (k > 1) {
a <- sample.int(length(R)-1, 1)
Mu2[,1:a] <- Mu[,(k-a+1):k]
Mu2[,(a+1):k] <- Mu[,1:(k-a)]
R2[1:a] <- R[(k-a+1):k]
R2[(a+1):k] <- R[1:(k-a)]
alpha <- 1
}
}
if (runif(1) < alpha) {
# accept the move
count.accepts[move.type] <- count.accepts[move.type] + 1
# store the updated values in the global namespace
k <<- k2
Mu <<- Mu2
R <<- R2
}
if (j %% count.every == 0) {
path.k <<- c(path.k, k)

340
APPENDIX C: ANSWERS TO THE EXERCISES
}
}
cat("acceptance rates for each move type:\n")
print(count.accepts / count.proposals, digits=3)
cat("\n")
}
The argument GetLogPi to the function RunRJMCMC must be a function which returns
the logarithm log π(k, μ,r). Since k can be found by inspecting μ, the function
GetLogPi just takes Mu and R as arguments. The function uses the global variables k,
Mu and R to store the current state of the Markov chain. These variables need to be
set before RunRJMCMC is called, and they contain the ﬁnal state of the Markov chain
on return from this function.
To test the function RunRJMCMC, we ﬁrst run the RJMCMC algorithm without any
observations, that is in the case where π is the prior distribution:
k <- rpois(1, lambda=model.clusters.mean-1) + 1
Mu <- matrix(runif(2*k, -10, 10), nrow=2, ncol=k)
R <- runif(k, model.r.min, model.r.max)
RunRJMCMC(GetLogPrior, 10000)
# burn-in
RunRJMCMC(GetLogPrior, 1000000, count.every=100)
h <- hist(path.k, breaks=seq(-0.5, max(path.k) + 1, by=1),
prob=TRUE, xlab="k", ylab="probability",
main=NULL, col="gray80", border="gray20")
p <- c(0, dpois(0:9, lambda=model.clusters.mean-1))
points(0:10, p)
The resulting histogram is shown in Figure C.9.
Finally, we can run the function RunRJMCMC on data generated from the model:
k <- rpois(1, lambda=model.clusters.mean-1) + 1
Mu <- matrix(runif(2*k, -10, 10), nrow=2, ncol=k)
R <- runif(k, model.r.min, model.r.max)
obs <- GenerateMixtureSample(80)
GetLogPi <- function(mu, r) {
return(GetLogPosterior(mu, r, obs))
}
RunRJMCMC(GetLogPi, 5000)
# burn-in
RunRJMCMC(GetLogPi, 50000, count.every=100)
To visualise the results of the run, we deﬁne two auxiliary functions for plotting:
PlotObservations <- function(obs) {
plot(obs[1,], obs[2,], xlim=c(-13,13), ylim=c(-11,11),

APPENDIX C: ANSWERS TO THE EXERCISES
341
k
Probability
0
2
4
6
8
10
12
14
0.00
0.10
0.20
Figure C.9
A histogram showing the distribution of k in the RJMCMC algorithm
from exercise E4.9, in the case where there are no observations and the target
distribution coincides with the prior distribution π. The bars in the histogram give the
frequencies observed in one run of the algorithm, the circles denote exact probabilities
expected for the stationary distribution. Both sets of values coincide well, giving
conﬁdence that the implementation of the algorithm works correctly.
asp=1, cex=0.5, xlab="", ylab="")
rect(-10, -10, 10, 10)
}
PlotState <- function(mu, r) {
symbols(mu[1,], mu[2,], circles=r,
inches=FALSE, add=TRUE)
symbols(mu[1,], mu[2,], circles=1.5*r,
inches=FALSE, add=TRUE)
symbols(mu[1,], mu[2,], circles=2*r,
inches=FALSE, add=TRUE)
}
Using these functions, we can plot the ﬁnal state of the RJMCMC Markov chain,
together with a histogram of K j.
par(mai=c(0.35, 0.35, 0.05, 0.05))
PlotObservations(obs)
PlotState(Mu, R)
par(mai=c(0.65, 0.75, 0.15, 0.05))
hist(path.k, breaks=seq(-0.5, max(path.k) + 0.5, by=1), prob=TRUE,
xlab="K", ylab="probability",
main=NULL, col="gray80", border="gray20")
The resulting plot is shown in Figure 4.9.

342
APPENDIX C: ANSWERS TO THE EXERCISES
C.5
Answers for Chapter 5
Solution E5.1
The function to generate the ABC samples is modelled directly
after the steps described in example 5.4. These steps are executed in a loop until N
samples are accumulated.
S <- function(x) {
return(c(sum(x), sum(x^2)) / n)
}
ABC.basic <- function(N, n, s, delta) {
res.theta <- matrix(nrow=N, ncol=2) # col. 1: mu, col. 2: sigma
res.s <- matrix(nrow=N, ncol=2) # row j = s_j
j <- 1
k <- 1
while (k <= N) {
mu <- runif(1, -10, 10)
sigma <- rexp(1)
X <- rnorm(n, mu, sigma)
sj <- S(X)
if (sum((sj-s)^2) <= delta^2) {
res.theta[k,] <- c(mu, sigma)
res.s[k,] <- sj
k <- k + 1
}
j <- j + 1
}
cat("delta=", delta, ": ", j/N, " proposals/sample\n",
sep="")
return(list(theta=res.theta, s=res.s))
}
The newly deﬁned function ABC.basic does not only return the samples θ jk, but also
the values s jk. The values s j are sometimes useful, for example in methods which
weight the samples according to the discrepancy s j −s∗. The function ABC.basic
can be used as follows:
n <- 20
s.obs <- c(6.989, 52.247)
X <- ABC.basic(100, n, s.obs, 0.1)
Figure 5.1 shows histograms of the output values X$theta[,1] (corresponding to μ)
and X$theta[,2] (corresponding to σ) for different values of δ.

APPENDIX C: ANSWERS TO THE EXERCISES
343
Solution E5.2
Using the function ABC.basic from exercise E5.1, we can implement
algorithm 5.6 as follows:
ABC.regression <- function(N, n, s, delta) {
samples <- ABC.basic(N, n, s, delta)
S <- cbind(samples$s, 1)
C <- t(S) %*% samples$theta
B.hat <- solve(t(S) %*% S, C)
beta.hat <- B.hat[1:2, 1:2]
correction <- t((s - t(samples$s))) %*% beta.hat
theta.tilde <- samples$theta + correction
return(list(theta=theta.tilde))
}
Here, we return a list with just one element theta so that ABC.regression can be used
as a drop-in replacement for ABC.basic. Different from the function in exercise E5.1,
we do not return the s j, since the correction makes these values largely meaningless
for the returned values. The function ABC.regression can be called as follows:
n <- 20
s.obs <- c(6.989, 52.247)
samples <- ABC.basic(100, n, s.obs, 0.1)
Solution E5.3
Algorithm 5.15 can be implemented in R as follows:
bias.boot <- function(theta.hat, x, N=10000) {
n <- length(x)
theta.star <- c()
for (j in 1:N) {
X.star <- sample(x, n, replace=TRUE)
theta.star[j] <- theta.hat(X.star)
}
cat("RMSE* = ", sd(theta.star)/sqrt(N), "\n", sep="")
return(mean(theta.star) - theta.hat(x))
}
The ﬁrst argument of this function must be an implementation of the estimator in
question. For the estimator ˆσ 2 from the question we can use the following function.
sigma.squared <- function(x) {
x.bar <- mean(x)
return(mean((x - x.bar)^2))
}

344
APPENDIX C: ANSWERS TO THE EXERCISES
The bootstrap estimate for the bias can then be computed with calls such as:
bias.boot(sigma.squared, x)
To test the function, we use a generated data set x:
x <- rnorm(100)
The resulting estimate of the bias is then
> bias.boot(sigma.squared, x)
RMSE* = 0.001194376
[1] -0.009388906
Since this result is close to the theoretical value −Var(X)/√n = −0.1 for the bias,
we can assume that our implementation works correctly.
The Monte Carlo sample size N affects the size of error in the approximation
(5.22). To justify our choice of N we ﬁrst note that, by proposition 3.14, the magnitude
of this error is given by the root-mean-square error
RMSE = stdev∗
f (X∗)
	
√
N
.
The function bias.boot prints an estimate for the RMSE; for the example the value
is approximately 0.0012. With 95% probability the magnitude of the Monte Carlo
error is less than 1.96 · RMSE ≈0.0023. Thus, we expect the error introduced by the
Monte Carlo integration to be approximately 25%. Since this ratio is still quite large,
we increase the value of N from 10 000 to 100 000 by specifying the optional, third
argument to bias.boot:
> bias.boot(sigma.squared, x, 100000)
RMSE* = 0.0003801055
[1] -0.0107199
This brings the Monte Carlo error down to about 7% which seems acceptable, since
the result is only approximate even in the absence of Monte Carlo error.
Solution E5.4
If we interpret each head as X = 1 and each tail as X = 0, then
the probability p of heads can be written as p = E(X), that is p is the mean of the
distribution of X, and our estimate is ˆp = n
i=1 Xi/n = ¯X. The bootstrap estimate
of the standard error of the mean is
se∗( ˆp) =



 1
n2
n

i=1

Xi −¯X
	2.

APPENDIX C: ANSWERS TO THE EXERCISES
345
Each of the 1600 terms in the sum where Xi = 1 contributes (1 −16/25)2 = 92/252.
Each of the 900 terms in the sum where Xi = 0 contributes (0 −16/25)2 = 162/252.
Thus we get
se∗( ˆp) =

1
n2

1600 · 92
252 + 900 · 162
252

=

1
n2
100 · 16 · 9 · (9 + 16)
(16 + 9)2
= 10 · 4 · 3
n · 5
=
24
2500.
This is the bootstrap estimate of the standard error of ˆp.
Solution E5.5
We can implement algorithm 5.20 in R as follows:
GetSimpleBootstrapCI <- function(x, theta.hat, N, alpha=0.05) {
# generate the bootstrap samples
n <- length(x)
theta.star <- c()
for (j in 1:N) {
X.star <- sample(x, n, replace=TRUE)
theta.star[j] <- theta.hat(X.star)
}
# construct the confidence interval
t <- theta.hat(x)
l <- ceiling(0.5 * alpha * N)
u <- ceiling((1 - 0.5 * alpha) * N)
theta.star <- sort(theta.star)
return(c(2 * t - theta.star[u], 2 * t - theta.star[l]))
}
In order to test GetSimpleBootstrapCI, we consider conﬁdence intervals for the
mean. Algorithm 5.20 requires the plug-in estimator for the parameter in question;
from example 5.13 we know that the plug-in estimator for the mean is the average
¯x of the sample x. This estimator, in form of the built-in R function mean, can be
used as the parameter theta.hat of GetSimpleBootstrapCI. To check our function,
we compare the output to the exact conﬁdence interval. Assuming the variance is not

346
APPENDIX C: ANSWERS TO THE EXERCISES
known, the exact conﬁdence interval is given by
[U, V ] =

¯x −t1−α/2,n−1 ˆσ
√n
, ¯x + t1−α/2,n−1 ˆσ
√n
 
,
where ˆσ 2 = n
i=1(xi −¯x)2/(n −1) and t1−α/2,n−1 denotes the 1 −α/2 quantile of
the t-distribution with n −1 degrees of freedom. We can compute the exact conﬁ-
dence interval using the following R code.
GetExactCI <- function(x, alpha=0.05) {
n <- length(x)
m <- mean(x)
s <- sd(x)
c <- qt(1 - 0.5*alpha, n-1)
return(c(m - c * s / sqrt(n), m + c * s / sqrt(n)))
}
Finally, in order to easily perform repeated comparisons, we write a function
which, for a given sample size, prints the output of both GetSimpleBootstrapCI and
GetExactCI to the screen.
Test <- function(n, alpha=0.05) {
X <- rnorm(n)
ci.bootstrap <- GetSimpleBootstrapCI(X, mean, 1000)
cat("bootstrap CI: [", ci.bootstrap[1], ", ",
ci.bootstrap[2], "]\n", sep="")
ci.exact <- GetExactCI(X)
cat("exact CI:
[", ci.exact[1], ", ",
ci.exact[2], "]\n", sep="")
}
Using this function, we can perform tests as follows:
> Test(10)
bootstrap CI: [-0.5593357, 0.206685]
exact CI:
[-0.6447165, 0.3395043]
> Test(50)
bootstrap CI: [-0.08583986, 0.448177]
exact CI:
[-0.1023353, 0.4421098]
> Test(100)
bootstrap CI: [-0.1195222, 0.2811508]
exact CI:
[-0.1205862, 0.2828386]
The output shows that there is reasonably good agreement between the bootstrap
conﬁdence intervals and the theoretically exact conﬁdence intervals. Thus, we can
assume that our implementation of algorithm 5.20 works correctly.

APPENDIX C: ANSWERS TO THE EXERCISES
347
Solution E5.6
First we generate the test data set:
X <- rnorm(25)
From example 5.14 we know that the plug-in estimate for the variance is given
by ˆσn(x) = n
i=1(xi −¯x)2/n. Since the built-in R function var uses n −1 instead of
n, we replace var by our own version:
PlugInVar <- function(x) {
return(mean((x - mean(x))^2))
}
Next, we write a function to estimate the standard deviation of the boundary
points and the average width of the conﬁdence interval for the variance, always using
the data set X we created above:
PrintStdDevAndMean <- function(N) {
lower <- c()
upper <- c()
width <- c()
for (i in 1:1000) {
ci <- GetSimpleBootstrapCI(X, PlugInVar, N);
lower[i] <- ci[1]
upper[i] <- ci[2]
width[i] <- ci[2] - ci[1]
}
cat("lower bound stddev: ", sd(lower), "\n", sep="")
cat("upper bound stddev: ", sd(upper), "\n", sep="")
cat("average width: ", mean(width), "\n", sep="")
}
Finally, we call this function for different values of N:
> PrintStdDevAndMean(1000)
lower bound stddev: 0.01664069
upper bound stddev: 0.0102032
average width: 0.6364756
> PrintStdDevAndMean(2000)
lower bound stddev: 0.01174421
upper bound stddev: 0.007007113
average width: 0.6358301
> PrintStdDevAndMean(4000)
lower bound stddev: 0.008348285
upper bound stddev: 0.005002864
average width: 0.6371888

348
APPENDIX C: ANSWERS TO THE EXERCISES
As expected, the standard deviation of and thus the error in the boundaries decreases.
But even for N = 4000 the standard deviation of either boundary is still approximately
1% of the width of the interval, so the error is still signiﬁcant.
Solution E5.7
The BCa algorithm 5.21 can be implemented in R as follows:
GetBCaBootstrapCI <- function(x, theta.hat, N, alpha=0.05) {
# generate the bootstrap samples
n <- length(x)
theta.star <- c()
for (j in 1:N) {
X.star <- sample(x, n, replace=TRUE)
theta.star[j] <- theta.hat(X.star)
}
# compute z.hat and a.hat
z.hat <- qnorm(sum(theta.star < theta.hat(x)) / N)
theta.jack <- c()
for (i in 1:n) {
theta.jack[i] <- theta.hat(x[-i])
}
y <- theta.jack - mean(theta.jack)
a.hat <- (sum(y^3) / (6 * sum(y^2)^1.5))
# pick out the BCa quantiles
p <- c(0.5 * alpha, 1 - 0.5 * alpha)
w <- z.hat + qnorm(p)
q <- pnorm(z.hat + w / (1 - a.hat * w))
idx <- ceiling(q * N)
theta.star <- sort(theta.star)
return(theta.star[idx])
}
Solution E5.8
The Monte Carlo estimate for the coverage probabilities can be
implemented in R as follows:
TestConfInt <- function(get.ci, gen.sample, n, theta, theta.hat) {
res <- c(0, 0, 0)
names(res) <- c("left", "inside", "right")
for (i in 1:1000) {
x <- gen.sample(n, theta)
ci <- get.ci(x, theta.hat, 5000)
if (theta < ci[1]) {
res[1] <- res[1] + 1

APPENDIX C: ANSWERS TO THE EXERCISES
349
} else if (theta > ci[2]) {
res[3] <- res[3] + 1
} else {
res[2] <- res[2] + 1
}
}
return(res / sum(res))
}
Using this function, we can now compare the two different methods for estimating
conﬁdence intervals.
CompareConfInts <- function(gen.sample, n, theta, theta.hat) {
res.simple <- TestConfInt(GetSimpleBootstrapCI, gen.sample,
n, theta, theta.hat)
res.BCa <- TestConfInt(GetBCaBootstrapCI, gen.sample,
n, theta, theta.hat)
res <- rbind(res.simple, res.BCa)
rownames(res) <- c("simple", "BCa")
return(res)
}
# test 1: mean of standard normal random variables
print(CompareConfInts(rnorm, 10, 0, mean))
print(CompareConfInts(rnorm, 50, 0, mean))
print(CompareConfInts(rnorm, 100, 0, mean))
# test 2: mean of exponentially distributed random variables
gen2 <- function(n, mu) {
return(rexp(n, 1/mu))
}
print(CompareConfInts(gen2, 10, 1, mean))
print(CompareConfInts(gen2, 50, 1, mean))
print(CompareConfInts(gen2, 100, 1, mean))
# test 3: standard deviation of normal random variables
gen3 <- function(n, sigma) {
return(rnorm(n, 0, sigma))
}
theta.hat3 <- function(x) {
x.bar <- mean(x)
return(sqrt(sum((x - x.bar)^2)/length(x)))
}
print(CompareConfInts(gen3, 10, 1, theta.hat3))
print(CompareConfInts(gen3, 50, 1, theta.hat3))
print(CompareConfInts(gen3, 100, 1, theta.hat3))

350
APPENDIX C: ANSWERS TO THE EXERCISES
The resulting output of one run of this program is summarised in Table 5.1. The
results shown in the table indicate that for the test problems considered here, the
performance of both methods is comparable. The data also show that the quality of
the conﬁdence intervals improves as the sample size n increases.
C.6
Answers for Chapter 6
Solution E6.1
We construct the solution iteratively, starting at time t = 0 and
then computing Bih from B(i−1)h for i = 1, 2, . . . , n by adding N(0, h)-distributed
random increments as described in algorithm 6.6. In R, this can be implemented as
follows:
BrownianMotion1d <- function(t) {
s.prev <- 0
# initial time
B <- 0
# initial value
res <- c()
for (s in t) {
h <- s - s.prev
dB <- rnorm(1, 0, sqrt(h))
B <- B + dB
res <- c(res, B)
s.prev <- s
}
return(res)
}
Since R has a built-in function cumsum for computing cumulative sums, we can use this
function to write a shorter and more efﬁcient implementation of the same algorithm:
BrownianMotion1d <- function(t) {
n <- length(t)
h <- t - c(0, t[-n])
dB <- rnorm(n, 0, sqrt(h))
B <- cumsum(dB)
return(B)
}
(C.1)
We test the new function BrownianMotion1d by creating a plot of one path of a
Brownian motion until time T = 10:
t <- seq(0, 100, by=0.01)
B <- BrownianMotion1d(t)
plot(t, B, type="l", xlab="t", ylab=expression(B[t]))
The resulting plot is shown in Figure 6.1.

APPENDIX C: ANSWERS TO THE EXERCISES
351
Solution E6.2
We construct the solution iteratively, starting at time t = 0 and
then computing Bih from B(i−1)h for i = 1, 2, . . . , n by adding N(0, h)-distributed
random increments as described in algorithm 6.6. In R, this can be implemented as
follows:
BrownianMotion <- function(t, d=1) {
n <- length(t)
sqrt.h <- sqrt(t - c(0, t[-n]))
B <- c()
for (i in 1:d) {
dB <- rnorm(n, 0, sqrt.h)
B <- cbind(B,cumsum(dB))
}
return(B)
}
We test the new function BM by creating a plot of one path of a two-dimensional
Brownian motion until time T = 10:
n <- 32000
t <- seq(0, 10, length.out=n+1)
B <- BrownianMotion(t, d=2)
plot(B[,1], B[,2], asp=1, type="l", lwd=0.1,
xlab=expression(B[t]^(1)), ylab=expression(B[t]^(2)))
col <- gray(n:0/n*0.8)
segments(B[1:(n-1),1], B[1:(n-1),2], B[2:n,1], B[2:n,2],
asp=1, col=col)
The resulting plot is shown in Figure 6.2.
Solution E6.3
A function B to return samples from a single Brownian path can be
implemented in R as follows:
t.known <- 0;
# times sampled so far
B.known <- 0;
# values of B corresponding to the times
in t.known
B <- function(times) {
res <- c()
for (s in times) {
i <- max(c(which(t.known < s), 1))
r <- t.known[i]
if (s == r) {
Bs <- B.known[i]
} else {

352
APPENDIX C: ANSWERS TO THE EXERCISES
if (i < length(t.known)) {
t <- t.known[i+1]
mu <- B.known[i]*(t-s)/(t-r)+B.known[i+1]*(s-r)/(t-r)
sigma <- sqrt((t-s)*(s-r)/(t-r))
} else {
mu <- B.known[i]
sigma <- sqrt(s-r)
}
Bs <- rnorm(1, mu, sigma)
t.known <<- append(t.known, s, after=i)
B.known <<- append(B.known, Bs, after=i)
}
res <- c(res, Bs)
}
return(res)
}
The function works by storing all values simulated so far in the global variable
B.known and the corresponding times in the global variable t.known.
The function B can be used like an ordinary function, the argument can be
either a single time or a vector of times. The following commands illustrate how
to use B:
t <- seq(0, 20, by=2)
plot(t, B(t), type="l", ylim=c(min(B(t))-0.5, max(B(t))+0.5),
ylab=expression(B[t]), col="gray", lwd=1.4)
t <- seq(0, 20, by=0.02)
lines(t, B(t))
The plot created by these commands is shown in Figure 6.4.
Solution E6.4
To generate a Brownian bridge, we ﬁrst generate a Brownian
motion and then use equation (6.2) to convert this into a Brownian bridge. Using the
function BrownianMotion1d from listing (C.1), we can implement this method in R as
follows:
BrownianBridge <- function(s, r=0, a=0, t=1, b=0) {
s.shifted <- c(s-r, t-r)
# make sure we sample B_{t-r}
B <- BrownianMotion1d(s.shifted)
Bt <- B[length(B)]
X <- B + a - s.shifted / (t - r) * (Bt - b + a)
return(X[-length(X)])
}

APPENDIX C: ANSWERS TO THE EXERCISES
353
Solution E6.5
We can use the following function to simulate a path of a geometric
Brownian motion, based on the function BrownianMotion1d from listing (C.1):
GeometricBM <- function(t, x0, alpha, beta, B) {
if (missing(B)) {
B <- BrownianMotion1d(t)
}
X <- x0 * exp(alpha * B + beta * t)
return(X)
}
As in the solution to exercise E6.1, we use the cumsum function to efﬁciently simulate
the Brownian motion B. We can plot paths of the simulated geometric Brownian
motion with commands such as:
t <- seq(0, 10, by=0.01)
X <- GeometricBM(t, x0=1, alpha=1, beta=-0.1)
plot(t, X, type="l", xlab="t", ylab=expression(X[t]))
The plot resulting from one run of this program is shown in Figure 6.5.
Solution E6.6
From lemma 6.9 we know that Xt = exp(Bt −t/2) has expectation
E(Xt) = 1 · exp

(1/2 −1/2)t
	
= exp(0) = 1.
We can use the following R code to generate Monte Carlo estimates for this value
with t = 0, 1, 2, . . . , 100 and to generate a plot of the resulting estimates:
N <- 1e6
tt <- seq(0, 100, by=1)
xx <- c()
for (t in tt) {
Bt <- rnorm(N, 0, sqrt(t))
Xt <- exp(Bt - 0.5*t)
xx <- c(xx, mean(Xt))
}
plot(tt, xx, ylim=c(0,3), xlab="t", ylab=expression(E(X[t])))
abline(h=1)
The plot resulting from one run of this program is shown in Figure 6.6. The ﬁgure
shows that for t ≤15 the Monte Carlo estimates are accurate and for 15 < t ≤40
the estimates have high variance but may be still acceptable. For t > 40 most of the
estimates are close to 0 instead of being close to the exact value 1, and thus for this
range of times t the sample size N = 106 is much too small.

354
APPENDIX C: ANSWERS TO THE EXERCISES
Solution E6.7
To solve SDE (6.17) with the given drift and diffusion coefﬁcient,
we implement the Euler-Maruyama method from algorithm 6.12. This can be done
by using an R function such as:
EulerMaruyama1d <- function(t, x0, mu, sigma, B) {
n <- length(t)
h <- t - c(0, t[-n])
if (missing(B)) {
dB <- rnorm(n, 0, sqrt(h))
} else {
if (length(B) != n) {
stop("lengths of t and B don’t match (",
n, " vs. ", length(B), ")")
}
dB <- B - c(0, B[-n])
}
path <- c()
s = 0
X = x0
for (i in 1:n) {
X <- X + mu(s, X) * h[i] + sigma(s, X) * dB[i]
s <- t[i]
path[i] <- X
}
return(path)
}
A function which works for d > 1 or m > 1 can be implemented as follows:
EulerMaruyama <- function(t, x0, mu, sigma, B) {
n <- length(t)
## dimension of the solution (from the initial condition)
d <- length(x0)
## dimension of the noise (from the columns of sigma)
m <- ncol(sigma(0, x0))
if (! is.numeric(m)) m <- 1;
h <- t - c(0, t[-n])
if (missing(B)) {
dB <- matrix(rnorm(n*m, 0, sqrt(h)), nrow=n, ncol=m)
} else {
B <- as.matrix(B)

APPENDIX C: ANSWERS TO THE EXERCISES
355
if (nrow(B) != n) {
stop("lengths of t and B don’t match (",
n, " vs. ", nrow(B), ")")
}
if (ncol(B) != m) {
stop("dimensions of sigma and B don’t match (",
m, " vs. ", ncol(B), ")")
}
dB = B - rbind(rep(0, d), as.matrix(B[-n,]))
}
path <- matrix(nrow=n, ncol=d)
s = 0
X = x0
for (i in 1:n) {
X <- X + mu(s, X) * h[i] + sigma(s, X) %*% dB[i,]
s <- t[i]
path[i,] <- X
}
return(path)
}
For one-dimensional SDEs, EulerMaruyama1d takes about half the time that
EulerMaruyama does to solve the same SDE.
In order to use the function EulerMaruyama, we have to provide implementations
of the drift μ and of the diffusion coefﬁcient σ as R functions. For the given functions,
this can be done as follows:
mu <- function(t, x) { c(x[2], x[1]*(1-x[1]^2)) }
sigma <- function(t, x) {
ifelse(x[1]> 0 && x[2]> 0, 0.2, 0.03) * diag(2)
}
x0 <- c(-0.03, -0.49)
t <- seq(0, 15, by=0.001)
X <- EulerMaruyama(t, x0, mu, sigma)
Then the path of X in R2 can be plotted as follows:
plot(X[,1], X[,2], type="l")
The output, together with arrows visualising the drift μ, is shown in Figure 6.7.
Solution E6.8
The Milstein scheme from algorithm 6.15 can be implemented in
R as shown in the following. Here we allow the functions mu and sigma to depend on

356
APPENDIX C: ANSWERS TO THE EXERCISES
the time t, in order to make the function more compatible with the EulerMaruyama1d
function from exercise E6.7.
Milstein1d <- function(t, x0, mu, sigma, sigma.prime, B) {
n <- length(t)
h <- t - c(0, t[-n])
if (missing(B)) {
dB <- rnorm(n, 0, sqrt(h))
} else {
if (length(B) != n) {
stop("lengths of t and B don’t match (",
n, " vs. ", length(B), ")")
}
dB <- B - c(0, B[-n])
}
path <- c()
s = 0
X = x0
for (i in 1:n) {
sig <- sigma(t, X)
X <- (X
+ mu(s, X) * h[i]
+ sigma(s, X) * dB[i]
+ 0.5 * sig * sigma.prime(s, X) * (dB[i]^2 - h[i]))
s <- t[i]
path[i] <- X
}
return(path)
}
The function can be used as follows:
mu <- function(t, x) { 0.1 }
sigma <- function(t, x) { x }
sigma.prime <- function(t, x) { 1 }
t <- seq(0, 20, length.out=10001)
X <- Milstein1d(t, 1, mu, sigma, sigma.prime)
plot(t, X, type="l")
Solution E6.9
In order to determine the strong error using Monte Carlo integration,
we will need to solve SDE (6.20) many times, the and efﬁciency of the method
becomes important. Since the strong error in (6.23) only depends on the ﬁnal point XT

APPENDIX C: ANSWERS TO THE EXERCISES
357
of the path, we introduce a specialised, faster version of the function EulerMaruyama1d
(see Exercise 6.7), which returns only the endpoint instead of the whole path.
EulerMaruyama1dEndpoint <- function(t, x0, mu, sigma, B) {
n <- length(t)
h <- t - c(0, t[-n])
if (missing(B)) {
dB <- rnorm(n, 0, sqrt(h))
} else {
if (length(B) != n) {
stop("lengths of t and B don’t match (",
n, " vs. ", length(B), ")")
}
dB <- B - c(0, B[-n])
}
s = 0
X = x0
for (i in 1:n) {
X <- X + mu(s, X) * h[i] + sigma(s, X) * dB[i]
s <- t[i]
}
return(X)
}
As a result of the simpliﬁed code, a call to EulerMaruyama1dEndpoint takes only
approximately 30% of the time EulerMaruyama1d does. A similar simpliﬁcation can
be applied to Milstein1d from exercise E6.8:
Milstein1dEndpoint <- function(t, x0, mu, sigma, sigma.prime, B) {
n <- length(t)
h <- t - c(0, t[-n])
if (missing(B)) {
dB <- rnorm(n, 0, sqrt(h))
} else {
if (length(B) != n) {
stop("lengths of t and B don’t match (",
n, " vs. ", length(B), ")")
}
dB <- B - c(0, B[-n])
}
s = 0
X = x0
for (i in 1:n) {

358
APPENDIX C: ANSWERS TO THE EXERCISES
sig <- sigma(t, X)
X <- (X
+ mu(s, X) * h[i]
+ sigma(s, X) * dB[i]
+ 0.5 * sig * sigma.prime(s, X) * (dB[i]^2 - h[i]))
s <- t[i]
}
return(X)
}
Next, we deﬁne the drift and diffusion coefﬁcient.
alpha <- 1
beta <- 0.4
mu <- function(t,x) { (0.5*alpha^2 + beta) * x }
sigma <- function(t,x) { alpha * x }
sigma.prime <- function(t,x) { alpha }
x0 <- 1
# initial value
T <- 5
# time horizon
Now we have to simulate solutions of the SDE repeatedly, for different grid sizes,
and to compute the strong error from the results. This can be done as follows:
n.max <- 1024
# largest discretisation parameter to try
n.min <- 64
# smallest discretisation parameter to try
K <- 9
# number of resolution steps
n <- round(exp(seq(log(n.min), log(n.max), length.out=K)))
N <- 20000
# number of runs to average over
samples.euler <- matrix(nrow=N, ncol=K)
samples.milstein <- matrix(nrow=N, ncol=K)
progress <- txtProgressBar(min=0, max=N, style=3)
t <- seq(0, T, length.out=n.max+1)
for (j in 1:N) {
B <- c(0, cumsum(rnorm(n.max, 0, sqrt(t[2:(n.max+1)]-
t[1:n.max]))))
X.exact <- x0 * exp(alpha*B + beta*t)
Xt.exact <- X.exact[n.max+1]
for (k in 1:K) {
I <- round(seq(1, n.max+1, length.out=n[k]+1))
Xt.euler <- EulerMaruyama1dEndpoint(t[I], x0, mu, sigma, B[I])
samples.euler[j,k] <- abs(Xt.euler - Xt.exact)

APPENDIX C: ANSWERS TO THE EXERCISES
359
Xt.milstein <- Milstein1dEndpoint(t[I], x0, mu, sigma,
sigma.prime, B[I])
samples.milstein[j,k] <- abs(Xt.milstein - Xt.exact)
}
setTxtProgressBar(progress, j)
}
close(progress)
error.euler <- colMeans(samples.euler)
error.milstein <- colMeans(samples.milstein)
Finally, we use the resulting data to create a plot:
h <- T / n
plot(h, error.euler, pch="x",
xlab="h", ylab="strong error",
ylim=range(c(error.euler, error.milstein)),
log="xy")
points(h, error.milstein, pch="o")
The resulting plot, together with conﬁdence intervals for each measurement, is
shown in Figure 6.8.
Solution E6.10
First, we have to implement the Euler-Maruyama scheme and
the Milstein scheme for SDE (6.20). To speed up the computation, we perform N
simulations at the same time, thus reducing the required number of loops in the R
program:
alpha <- 1
beta <- 0.4
x0 <- 1
# initial value
T <- 5
# time horizon
EulerMaruyama <- function(n, N) {
h <- T / n
X <- rep(x0, times=N)
for (i in 1:n) {
dB <- rnorm(N, 0, sqrt(h))
X <- (X
+ (0.5 * alpha^2 + beta) * X * h
+ alpha * X * dB)
}
return(X)

360
APPENDIX C: ANSWERS TO THE EXERCISES
}
Milstein <- function(n, N) {
h <- T / n
X <- rep(x0, times=N)
for (i in 1:n) {
dB <- rnorm(N, 0, sqrt(h))
X <- (X
+ (0.5 * alpha^2 + beta) * X * h
+ alpha * X * dB
+ 0.5 * alpha^2 * X * (dB^2 - h))
}
return(X)
}
Next, we implement the test function f :
prob <- 0.6
threshold <- x0 * exp(alpha*qnorm(prob, 0, sqrt(T)) + beta*T)
f <- function(x) {
as.numeric(x <= threshold)
}
Now we have to solve the SDE repeatedly for different grid sizes and to compute the
weak error. This can be done as follows:
n.max <- 1024
# largest discretisation parameter to try
n.min <- 64
# smallest discretisation parameter to try
K <- 9
# number of resolution steps
n <- round(exp(seq(log(n.min), log(n.max), length.out=K)))
N <- 10000000
# number of runs to average over
mean.euler <- c()
mean.milstein <- c()
sd.euler <- c()
sd.milstein <- c()
progress <- txtProgressBar(min=0, max=2*K, style=3)
for (k in 1:K) {
Xt.euler <- f(Euler--Maruyama(n[k], N))
mean.euler[k] <- mean(Xt.euler)
sd.euler[k] <- sd(Xt.euler)
setTxtProgressBar(progress, 2*k-1)
Xt.milstein <- f(Milstein(n[k], N))
mean.milstein[k] <- mean(Xt.milstein)
sd.milstein[k] <- sd(Xt.milstein)

APPENDIX C: ANSWERS TO THE EXERCISES
361
setTxtProgressBar(progress, 2*k)
}
close(progress)
exact <- prob
error.euler <- abs(mean.euler - exact)
error.milstein <- abs(mean.milstein - exact)
Finally, we use the resulting data to create a plot:
h <- T / n
plot(h, error.euler, pch="x",
xlab="h", ylab="weak error",
ylim=range(c(error.euler, error.milstein)),
log="xy")
points(h, error.milstein, pch="o")
This completes the answers.
Solution E6.11
A basic Monte Carlo estimate for the probability that X exceeds a
given level c can be based on the following R code.
f <- function(X, c) {
return(as.numeric(max(X) > c))
}
x0 <- 0
T <- 5
t <- seq(0, T, by=0.01)
mu <- function(t, x) { -x }
sigma <- function(t, x) { 1.0 }
MC.sample <- function(N, c) {
res <- c()
for (i in 1:N) {
X <- EulerMaruyama1d(t, x0, mu, sigma)
res[i] <- f(X, c)
}
return(res)
}
For c = 2.5, we can use the function MC.sample as follows:
N <- 10000

362
APPENDIX C: ANSWERS TO THE EXERCISES
Z <- MC.sample(N, 2.5)
m <- mean(Z)
s <- sd(Z)/sqrt(N)
cat("estimate for p: ", m, "\n", sep="")
cat("estimate for RMSE: ", sd(Z)/sqrt(N), "\n", sep="")
cat("estimate for CI: [", m - 1.96*s, ", ", m + 1.96*s, "]\n",
sep="")
In the estimation of the root-mean squared error we neglected the discretisation error
of the Euler-Maruyama method and only use the Monte Carlo error. The true error
will be slightly bigger than the obtained estimate, due to the bias introduced by the
discretisation error. The output of one run of the program is as follows:
estimate for p: 0.007
estimate for RMSE: 0.0008337683
CI = [0.005365814, 0.008634186]
This completes our solution of exercise E6.11.
Solution E6.12
An importance sampling estimate for the probability that X exceeds
a given level c can be based on the following R code.
q <- 0.5
mu.tilde <- function(t, x) { -q*x }
phi.over.psi <- function(t, Y) {
n <- length(t)
h <- t[-1] - t[-n]
dY <- Y[-1] - Y[-n]
stoch.int <- sum(Y[-n] * dY)
int <- sum(Y[-n]^2 * h)
return(exp(-(1-q)*stoch.int - .5*(1-q*q)*int))
}
IS.sample <- function(N, c) {
res <- c()
for (i in 1:N) {
Y <- EulerMaruyama1d(t, x0, mu.tilde, sigma)
res[i] <- f(Y, c) * phi.over.psi(t, Y)
}
return(res)
}
For c = 3.4 and N = 200 000, we can use the function IS.sample as follows:
N <- 200000

APPENDIX C: ANSWERS TO THE EXERCISES
363
Z <- IS.sample(N, 3.2)
m <- mean(Z)
s <- sd(Z)/sqrt(N)
cat("estimate for p: ", m, "\n", sep="")
cat("estimate for RMSE: ", sd(Z)/sqrt(N), "\n", sep="")
cat("estimate for CI: [", m - 1.96*s, ", ", m + 1.96*s, "]\n",
sep="")
The output of one run of the program is as follows:
estimate for p: 4.241444e-05
estimate for RMSE: 3.296227e-06
estimate for CI: [3.595384e-05, 4.887505e-05]
Solution E6.13
Algorithm 6.29 can be implemented as shown in the following. In
the program we use the absolute value inside the square roots √|Vt| in order to avoid
problems when Vt temporarily takes negative values caused by discretisation error.
Heston <- function(t, S0, V0, r, lambda, sigma, xi, rho) {
n <- length(t)
h <- t - c(0, t[-n])
dB <- matrix(rnorm(2*n, 0, sqrt(h)), nrow=n, ncol=2)
sigma.square <- sigma^2
rho.prime <- sqrt(1 - rho^2)
path <- matrix(nrow=n, ncol=2)
S <- S0
V <- V0
for (i in 1:n) {
sqrt.V = sqrt(abs(V))
S <- S + r * S * h[i] + S * sqrt.V * dB[i,1]
V <- (V + lambda * (sigma.square - V) * h[i]
+ xi * sqrt.V * (rho * dB[i,1] + rho.prime * dB[i,2]))
path[i,1] <- S
path[i,2] <- V
}
return(path)
}
We pre-compute sigma^2 and sqrt(1 - rho^2) outside the loop in order to avoid
unnecessary, repeated evaluations of the squares and the square root. The function
can be used as follows:
t <- seq(0, 5, by=0.001)

364
APPENDIX C: ANSWERS TO THE EXERCISES
X <- Heston(t, S0=1, V0=0.16, r=1.02,
lambda=1, sigma=1, xi=1, rho=-0.5)
plot(t, X[,1], type="l")
plot(t, X[,2], type="l")
The resulting plot is shown in Figure 6.13.
Solution E6.14
The Monte Carlo estimate requires us to solve SDE (6.43) many
times in a row. To reduce the time required for computing these estimates, we slightly
modify the code from exercise E6.13 to make it faster: since we are only interested
in the ﬁnal value ST , there is no need to store the whole paths of S and V , and we can
also assume a ﬁxed step size h = T/n for the Euler scheme. The resulting, simpliﬁed
function is as follows:
HestonEndpointS <- function(n) {
h <- T/n
dB <- matrix(rnorm(2*n, 0, sqrt(h)), nrow=n, ncol=2)
sigma.square <- sigma^2
rho.prime <- sqrt(1 - rho^2)
S <- S0
V <- V0
for (i in 1:n) {
sqrt.V = sqrt(abs(V))
S <- S + r * S * h + S * sqrt.V * dB[i,1]
V <- (V + lambda * (sigma.square - V) * h
+ xi * sqrt.V * (rho * dB[i,1] + rho.prime * dB[i,2]))
}
return(S)
}
To compute the Monte Carlo estimate, we have to call this function repeatedly, with
the given parameters, and we have to apply f to the results.
r <- 1.02
lambda <- 1
sigma <- 0.5
xi <- 1
rho <- -0.5
S0 <- 1
V0 <- 0.16
T <- 1
K <- 3

APPENDIX C: ANSWERS TO THE EXERCISES
365
f <- function(s) {
exp(-r*T) * max(s - K, 0)
}
MC.estimate <- function(n, N) {
C <- replicate(N, f(HestonEndpointS(n)))
est <- mean(C)
mse <- var(C) / N
return(list(est=est, mse=mse))
}
Estimates can now be obtained using calls such as:
> MC.estimate(100, 10000)
C=0.1147109, MSE=5.295385e-06
For the speciﬁed parameters, our estimate for the option price is C = 0.1139534,
with an estimated mean squared error of 5.3 · 10−6.
Solution E6.15
To implement the multilevel algorithm 6.26, we modify the func-
tion HestonEndpointS from exercise E6.14 to compute the solution of the SDE for
discretisation parameters n and n/m simultaneously.
MLMCSample <- function(n, m) {
if (n == m) return(f(HestonEndpointS(n)))
if (n %% m != 0 || n <= m) stop("invalid grid size")
h <- T/n
dB <- matrix(rnorm(2*n, 0, sqrt(h)), nrow=n, ncol=2)
sigma.square <- sigma^2
rho.prime <- sqrt(1 - rho^2)
S <- S0
V <- V0
Sm <- S0
Vm <- V0
dBm <- c(0, 0)
for (i in 1:n) {
sqrt.V = sqrt(abs(V))
S <- S + r * S * h + S * sqrt.V * dB[i,1]
V <- (V + lambda * (sigma.square - V) * h
+ xi * sqrt.V * (rho * dB[i,1] + rho.prime * dB[i,2]))
dBm <- dBm + dB[i,]
if (i %% m == 0) {
sqrt.Vm = sqrt(abs(Vm))
Sm <- Sm + r * Sm * m * h + Sm * sqrt.Vm * dBm[1]

366
APPENDIX C: ANSWERS TO THE EXERCISES
Vm <- (Vm + lambda * (sigma.square - Vm) * m * h
+ xi * sqrt.Vm * (rho * dBm[1] + rho.prime * dBm[2]))
dBm <- c(0, 0)
}
}
return(f(S)-f(Sm))
}
This function gives us the difference Y (i, j)
i
−Y (i, j)
i−1 in the multilevel Monte Carlo
estimate (6.41). The full estimate can be computed by combining the individual steps
as follows:
MLMC.estimate <- function(n, m, L) {
k = round(log(n) / log(m))
if (n != m^k) stop("n=", n, " is not a power of k=", k)
est <- 0
mse <- 0
n <- 1
N <- L * m^k
for (i in 1:k) {
n <- n * m
N <- N / m
Ci <- replicate(N, MLMCSample(n, m))
est <- est + mean(Ci)
mse <- mse + var(Ci) / N
}
return(list(est=est, mse=mse))
}
C.7
Answers for Appendix B
Solution EB.1
As the following transcript of an R session shows, the ﬁrst three
expressions are straightforward:
> 1+2+3+4+5+6+7+8+9+10
[1] 55
> 2^16
[1] 65536
> 2^100
[1] 1.267651e+30
The sum of the ﬁrst 10 natural numbers is 55 and 216 = 65536. The ﬁnal answer
1.267651e+30 uses a shorthand notation: the returned result stands for 1.267651 · 1030

APPENDIX C: ANSWERS TO THE EXERCISES
367
(e is short for ‘exponent’). R chooses this representation because the result is very
big and would be difﬁcult to read when written in standard notation.
For the ﬁnal part of the exercise, computing 2/(1 +
√
5), we can use the fact that
brackets in R can be used exactly as they are used in mathematical expressions: using
the function sqrt (see Table B.1), we can write
> 2/(1+sqrt(5))
[1] 0.618034
to get the correct (up to six decimal digits) result 0.618034.
Solution EB.2
The value of x is 3, the value of y is 5.
Solution EB.3
The sum can be computed by ﬁrst constructing a vector of the
numbers 1, 2, . . . , 100, and then using the R function sum to add all elements of the
vector:
> x <- 1:100
> x
[1]
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
[16]
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
[31]
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
[46]
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
[61]
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
[76]
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
[91]
91
92
93
94
95
96
97
98
99 100
> sum(x)
[1] 5050
Thus, the required sum is 5050. A shorter solution is to merge the two steps into one
command by just typing sum(1:100). This results in the same answer.
Solution EB.4
There are various ways to implement expressions such as the one
for g2 in R. To get a readable and short solution, it is important to make good use
of the built-in R functions such as sum and mean. To compute g2, we can use the
following R code:
excessKurtosis <- function(x) {
x.bar <- mean(x)
numerator <- mean((x - x.bar)^4)
denominator <- mean((x - x.bar)^2)^2
return(numerator / denominator - 3)
}
This function computes the value g2 for any sample x.

368
APPENDIX C: ANSWERS TO THE EXERCISES
To test the function, we try large samples of distributions with a known kurtosis.
First, the theoretical excess kurtosis of normal distributed values is 0. If we use the
estimator g2 for large samples, we should get results close to 0:
> excessKurtosis(rnorm(1000000))
[1] -0.00452515
> excessKurtosis(rnorm(1000000))
[1] 0.0002580271
> excessKurtosis(rnorm(1000000))
[1] -7.535562e-05
As a second test, we estimate the kurtosis of an exponential distribution; in this case,
the theoretical excess kurtosis equals 6:
> excessKurtosis(rexp(1000000))
[1] 5.945269
> excessKurtosis(rexp(1000000))
[1] 5.991549
> excessKurtosis(rexp(1000000))
[1] 6.09649
Both sets of tests give the results close to the theoretical values, so we have reason to
assume that the function excessKurtosis works.
Solution EB.5
The program contains at least two (!) mistakes: the line to assign
a value to x[5] is missing, and the output claims wrongly that x[8] is the sixth (not
eighth) Fibonacci number.
Solution EB.6
Since we need to repeat the operation of computing the cosine 20
times, we use a for loop in R:
X <- 0
for (i in 1:20) X <- cos(X)
print(X)
The output of these commands, and thus the value of x20, is 0.7389378.
Solution EB.7
We ﬁrst use seq to construct a vector which contains the numbers
from 100 to 0, and then use this vector to control the for loop:
for (i in seq(from=100, to=0, by=-1)) {
cat(i, "\n")
}

APPENDIX C: ANSWERS TO THE EXERCISES
369
The command print(i) can be used instead of cat(i, "\n"), but the output is not
quite as tidy (try this yourself!).
Solution EB.8
We can use the following program:
n <- 1
while (n^2 <= 5000) {
n <- n + 1
}
print(n)
The result is n = 71.
Solution EB.9
One solution is the following program:
n <- 1
while ((n+1)^2 < 5000) {
n <- n + 1
}
print(n)
Alternatively we can use the program from EB.8, and print n-1 instead of n [we also
need to check that (n-1)^2 is not exactly equal to 5000; otherwise we would have to
use n-2 instead]. Using either method, the result is 70.
Solution EB.10
One solution is given in the following program.
x <- 27
n <- 0
while (x != 1) {
cat("x[", n, "] = ", x, "\n", sep="")
n = n + 1
if (x %% 2 == 0) {
x = x / 2
} else {
x = 3*x + 1
}
}
cat("x[", n, "] = ", x, "\n", sep="")
This program violates the DRY principle: the cat command is duplicated to output
the ﬁnal value. One way to ﬁx this problem is to stop the loop one iteration later,

370
APPENDIX C: ANSWERS TO THE EXERCISES
that is when the previous value equals one. This idea is implemented in the following
version of the program:
n <- 0
x <- 27
old <- 0
while (old != 1) {
old <- x
cat("x[", n, "] = ", x, "\n", sep="")
n = n + 1
if (x %% 2 == 0) {
x = x / 2
} else {
x = 3*x + 1
}
}
Solution EB.11
The function computes the sample variance of the elements of x.
Solution EB.12
We set the variable b to the string "a". Since variables inside the
function are independent of the names used by the caller, the function call test(c=b)
is equivalent to test(c="a"), that is inside the function the value of the variable c
is the string "a". Finally, the function cat does not include the enclosing quotation
marks into the output when processing strings and thus the output is ‘a=1, b=2,
c=a, d=4’.
Solution EB.13
The missing line, where x[5] should have been set, causes a run-
time error: the command x[6] <- x[5] + x[4] tries to read the uninitialised value
x[5] and consequently x[6] and all the following values are set to NA. The mistake
in the message printed via cat is a semantic error.
Solution EB.15
To ﬁnd the mistake in the given R function, we ﬁrst add a series
of print commands to the function, to see at which step the function deviates from
our expectations:
SomethingWrong <- function(x) {
n <- length(x)
cat("x =", x, ", n =", n, "\n")
sum <- 0
for (i in 1:n-1) {
cat("i =", i, ", sum =", sum, "\n")
sum <- sum + (x[i+1] - x[i])^2

APPENDIX C: ANSWERS TO THE EXERCISES
371
}
return(sum)
}
This results in the following output:
> SomethingWrong(c(1,2,3))
x = 1 2 3 ,n = 3
i = 0 , sum = 0
i = 1 , sum =
i = 2 , sum =
numeric(0)
The ﬁrst line of the output is what we expect: the input data are 1 2 3 and the length
of the input data are 3. The second line already shows a problem: we asked R to use
the values 1, . . . , n −1 for i, but the ﬁrst iteration of the loop has i = 0. This is
the cause of the problem, since the loop evaluates x[0] which is not the value we
intended to use.
To ﬁnd out why i took the value 0, we inspect the loop statement: we have i in
1:n-1 and we know that at this point n equals 3. Thus the range of i is 1:3-1:
> 1:3-1
[1] 0 1 2
> 1:2
[1] 1 2
> 1:(3-1)
[1] 1 2
The experiments shown above clearly point out the problem: R interprets 1:n-1 as
(1, 2, . . . , n) −1, that is the computer subtracts 1 from every value in the sequence
1:n. This is not what we intended, and we can ﬁx this by introducing brackets around
n-1. The following version of the function ﬁxes the error:
SomethingWrong <- function(x) {
n <- length(x)
sum <- 0
for (i in 1:(n-1)) {
sum <- sum + (x[i+1] - x[i])^2
}
return(sum)
}
Finally, we test the new version of the function:
> SomethingWrong(c(1,2,3))
[1] 2

372
APPENDIX C: ANSWERS TO THE EXERCISES
−2
0
2
4
6
8
0.0
0.1
0.2
0.3
0.4
dnorm(x, 0, 1)
−2
0
2
4
6
8
0.0
0.4
0.8
pnorm(x, 0, 1)
−2
0
2
4
6
8
0.00
0.10
0.20
dnorm(x, 3, sqrt(4))
−2
0
2
4
6
8
0.0
0.4
0.8
pnorm(x, 3, sqrt(4))
−2
0
2
4
6
8
0.0
0.4
0.8
dexp(x, 1)
−2
0
2
4
6
8
0.0
0.4
0.8
pexp(x, 1)
−2
0
2
4
6
8
0.00
0.10
0.20
x
dgamma(x, 9, scale = 0.5)
−2
0
2
4
6
8
0.0
0.4
0.8
x
pgamma(x, 9, scale = 0.5)
Figure C.10
Output of the R script from exercise EB.16. The graphs show the
densities (left column) and distribution functions (right column) of the distributions
N(0, 1), N(3, 4), Exp(1) and (9, 0.5) (top to bottom).

APPENDIX C: ANSWERS TO THE EXERCISES
373
Since this is the expected result, we can assume that we have correctly identiﬁed and
ﬁxed the problem.
Solution EB.16
The function dnorm gives the density and pnorm gives the CDF
of the normal distribution. Similarly, we can use the functions dexp and pexp for the
exponential distribution and dgamma and pgamma for the gamma distribution. Thus,
we can plot the required graphs as follows:
par(mfrow=c(4,2))
curve(dnorm(x,0,1), -3, 9)
curve(pnorm(x,0,1), -3, 9, ylim=c(0,1))
curve(dnorm(x,3,sqrt(4)), -3, 9)
curve(pnorm(x,3,sqrt(4)), -3, 9, ylim=c(0,1))
curve(dexp(x,1), -3, 9)
curve(pexp(x,1), -3, 9, ylim=c(0,1))
curve(dgamma(x,9,scale=.5), -3, 9)
curve(pgamma(x,9,scale=.5), -3, 9, ylim=c(0,1))
The resulting plot is shown in Figure C.10.

References
M. A. Beaumont, W. Zhang, and D. J. Balding. Approximate Bayesian Computation in popu-
lation genetics. Genetics, 162(4):2025–2035, 2002.
M. Blum and O. Franc¸ois. Non-linear regression models for Approximate Bayesian Compu-
tation. Statistics and Computing, 20:63–73, 2010.
A. N. Borodin and P. Salminen. Handbook of Brownian Motion — Facts and Formulae.
Probability and its Applications. Birkh¨auser, 1996.
G. E. P. Box and M. E. Muller. A note on the generation of random normal deviates. Annals of
Mathematical Statistics, 29(2):610–611, 1958.
G. Casella and R. L. Berger. Statistical Inference. Duxbury Press, second edition, 2001.
C. Chatﬁeld. The Analysis of Time Series. Chapman & Hall/CRC Texts in Statistical Science
Series. Chapman & Hall/CRC, sixth edition, 2004.
J. S. Dagpunar. Simulation and Monte Carlo, with Applications in Finance and MCMC. John
Wiley & Sons, Ltd, 2007.
A. C. Davison and D. V. Hinkley. Bootstrap Methods and their Application. Cambridge
University Press, 1997.
T. J. DiCiccio and B. Efron. Bootstrap conﬁdence intervals. Statistical Science, 11(3):189–212,
1996.
B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall, 1993.
P. Fearnhead and D. Prangle. Constructing summary statistics for approximate Bayesian com-
putation: semi-automatic approximate Bayesian computation. Journal of the Royal Statisti-
cal Society: Series B, 74(3):419–474, 2012.
W. Feller. An Introduction to Probability Theory and Its Applications, volume I. John Wiley
& Sons, Ltd, third edition, 1968.
P. H. Garthwaite, I. T. Jolliffe, and B. Jones. Statistical Inference. Oxford University Press,
2002.
A. Gelman and D. B. Rubin. Inference from iterative simulation using multiple sequences.
Statistical Science, 7(4):457–472, 1992.
J. E. Gentle, W. H¨ardle, and Y. Mori, editors. Handbook of Computational Statistics. Springer,
2004.
M. B. Giles. Multilevel Monte Carlo path simulation. Operations Research, 56(3):607–617,
2008a.
An Introduction to Statistical Computing: A Simulation-based Approach, First Edition. Jochen Voss.
© 2014 John Wiley & Sons, Ltd. Published 2014 by John Wiley & Sons, Ltd.

376
REFERENCES
M. B. Giles. Improved multilevel Monte Carlo convergence using the Milstein scheme. In
A. Keller, S. Heinrich, and H. Niederreiter, editors, Monte Carlo and Quasi-Monte Carlo
Methods 2006, pages 343–358. Springer, 2008b.
W. R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors. Markov Chain Monte Carlo in
Practice. Chapman & Hall/CRC, 1996.
P. J. Green. Reversible jump Markov Chain Monte Carlo computation and Bayesian model
determination. Biometrika, 82(4):711–732, 1995.
S. L. Heston. A closed-form solution for options with stochastic volatility with applications to
bond and currency options. The Review of Financial Studies, 6(2):327–343, 1993.
D. J. Higham. An algorithmic introduction to numerical simulation of stochastic differential
equations. SIAM Review, 43(3):525–546, 2001.
J. Jacod and P. Protter. Probability Essentials. Springer, 2000.
I. Karatzas and S. E. Shreve. Brownian Motion and Stochastic Calculus, volume 113 of
Graduate Texts in Mathematics. Springer, second edition, 1991.
W. J. Kennedy Jr and J. E. Gentle. Statistical Computing. Marcel Dekker, Inc., 1980.
J. F. C. Kingman. Poisson Processes, volume 3 of Oxford Studies in Probability. Clarendon
Press, 1993.
P. E. Kloeden and E. Platen. Numerical Solution of Stochastic Differential Equations. Number
23 in Applications of Mathematics. Springer, 1999. Corrected Third Printing.
D. E. Knuth. Seminumerical Algorithms, volume 2 of The Art of Computer Programming.
Addison-Wesley, second edition, 1981.
D. P. Kroese, T. Taimre, and Z. I. Botev. Handbook of Monte Carlo Methods. John Wiley &
Sons, Ltd, 2011.
E. L. Lehmann and J. P. Romano. Testing Statistical Hypotheses. Springer, third edition, 2005.
X. Mao. Stochastic Differential Equations and Applications. Woodhead Publishing, second
edition, 2007.
G. Marsaglia and W. W. Tsang. The ziggurat method for generating random variables. Journal
of Statistical Software, 5(8):1–7, 2000.
M. Matsumoto and T. Nishimura. Mersenne twister: a 623-dimensionally equidistributed uni-
form pseudo-random number generator. ACM Transactions on Modeling and Computer
Simulation, 8(1):3–30, 1998.
N. Metropolis. The beginning of the Monte Carlo method. Los Alamos Science, 125–130,
1987.
S. P. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Cambridge University
Press, second edition, 2009.
P. M¨orters and Y. Peres. Brownian Motion. Cambridge University Press, 2010.
J. R. Norris. Markov Chains. Cambridge University Press, 1997.
B. K. Øksendal. Stochastic Differential Equations. Springer, sixth edition, 2003.
R. K. Pathria. Statistical Mechanics. Elsevier, second edition, 1996.
M. Plischke and B. Bergersen. Equilibrium Statistical Physics. World Scientiﬁc, third edition,
2006.
V. Privman, editor. Finite Size Scaling and Numerical Simulation of Statistical Systems. World
Scientiﬁc, 1990.
R Development Core Team. R: A Language and Environment for Statistical Computing. 2011.

REFERENCES
377
B. D. Ripley. Stochastic Simulation. John Wiley & Sons, Ltd, 1987.
M. L. Rizzo. Statistical Computing with R. Chapman & Hall/CRC, 2008.
C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer Texts in Statistics.
Springer, second edition, 2004.
G. O. Roberts and J. S. Rosenthal. General state space Markov chains and MCMC algorithms.
Probability Surveys, 1:20–71, 2004.
L. C. G. Rogers and D. Williams. Diffusions, Markov Processes, and Martingales, volume 2 of
Cambridge Mathematical Library. Cambridge University Press, 2000. Ito calculus, Reprint
of the second (1994) edition.
W. Rudin. Real and Complex Analysis. McGraw-Hill, third edition, 1987.
H. Scheff´e. A useful convergence theorem for probability distributions. Annals of Mathematical
Statistics, 18(3):434–438, 1947.
S. Tavar´e, D. J. Balding, R. C. Grifﬁths, and P. Donnelly. Inferring coalescence times from
DNA sequence data. Genetics, 145(2):505–518, 1997.
W. N. Venables and B. D. Ripley. S Programming. Springer, 2000.
W. N. Venables, D. M. Smith, and the R Development Core Team. An introduction to R.
Available from http://cran.r-project.org/doc/manuals/R-intro.html, 2011 (accessed
14 May 2013).
J. von Neumann. Various techniques used in connection with random digits. In A. S. House-
holder, G. E. Forsythe and H. H. Germond, editors, Monte Carlo Method, volume 12 of
National Bureau of Standards Applied Mathematics Series, pages 36–38. US Government
Printing Ofﬁce, 1951.
R. Waagepetersen and D. Sorensen. A tutorial on reversible jump MCMC with a view toward
applications in QTL-mapping. International Statistical Review, 69:49–61, 2001.
D. Williams. Weighing the Odds: A Course in Probability and Statistics. Cambridge University
Press, 2001.
G. Winkler. Image Analysis, Random Fields and Dynamic Monte Carlo Methods. Springer,
1995.

Index
ABC, see Approximate Bayesian
Computation
acceptance probability
independence sampler, 120
Metropolis–Hastings method, 110,
113
random walk Metropolis, 117
rejection sampling, 15
RJMCMC, 163, 164
thinning method, 65
antithetic paths, 247–248
antithetic variables, 88–93
for SDEs, 247–248
Approximate Bayesian Computation,
182–188
with regression, 188–192
autocorrelation, 132–135
Bayes’ rule, 139, 267
Bayesian inference, 72, 138–142,
147–152, 172–179
bias, 76
bootstrap estimates, 199–201
Monte Carlo estimation, 97–99
of discretised SDEs, 243–244
binomial distribution, 293
boolean values, 281
bootstrap conﬁdence intervals, 203–208
BCa, 207
simple, 205
bootstrap estimates, 192–208
general, 196
of conﬁdence intervals, 203–208
of the bias, 199–201
of the standard error, 201–203
Box–Muller transform, 34
Brownian bridge, 220
Brownian motion, 214–221
geometric, 221–223
interpolation, 218–221
Markov property, 216
scaling property, 216
simulation, 217–218
burn-in period, 130–132, 137, 151
Cauchy distribution, 37
CDF, see cumulative distribution
function
change of variables, 33
χ2-distribution, 6, 293
componentwise simulation, 48–50, 142
computational cost
ABC, 184
MCMC, 135
Monte Carlo estimation, 75
rejection sampling, 20, 22
SDEs, 233, 235, 247
conditional density, 267
conditional distribution, 23–27, 183
conditional expectation, 194
An Introduction to Statistical Computing: A Simulation-based Approach, First Edition. Jochen Voss.
© 2014 John Wiley & Sons, Ltd. Published 2014 by John Wiley & Sons, Ltd.

380
INDEX
conﬁdence intervals, 83, 100–103, 245
bootstrap, 203–208
continuous-time processes, 213–261
control variates, 93–96, 251
convergence diagnostics, 136–137
correlation, 90, 91–93, 95, 132
auto-, 132–135
of a sample, 97–99
critical region, 104
cumulative distribution function, 12,
264
detailed balance condition, 114
diffusion coefﬁcient, 225
discretisation error, 214
for SDEs, 236–242
don’t repeat yourself
drift, 225
DRY, see don’t repeat yourself
effective sample size, 134
empirical distribution, 192
energy, 154
Euler–Maruyama scheme, 232–233
for the Heston model, 256
events, 263
exponential distribution, 13, 265, 293
gamma distribution, 293
geometric Brownian motion, 221–223
geometric distribution, 11, 17, 20, 185
Gibbs measure, 154
Gibbs sampler, 142–159
image processing, 157–159
Ising model, 154–157
parameter estimation, 147–152
half-normal distribution, 21
Harris recurrence, 127
Heston model, 255–259
hierarchical models, 45–50, 52, 147
hypothesis tests, 5, 103–106
i.i.d., 2
i.i.d. copies, 75
importance sampling, 84–88, 223
for SDEs, 248–250
independence, 109, 266
independence sampler, 120–121
indicator function, 268
initial distribution
for Markov chains, 50, 128
for SDEs, 224
intensity function, 59, 161
inverse temperature, 154
inverse transform method, 12–15, 92
irreducible Markov chain, 127
Ising model, 154
Ito integral, 226–227
Ito’s formula, 227–230
Jacobian matrix, 33, 34, 37, 164, 165
Kronecker delta, 168
kurtosis, 294
Lagrange multipliers, 246
law of large numbers, 269
for Markov chains, 128–130
LCG, see linear congruential generator
Linear Congruential Generator, 2–4
marginal distribution, 48–50
Markov chain, 50–58, 109, 126–130
continuous state space, 56–58
discrete state space, 51–56
Harris recurrent, 127
initial distribution, 50, 128
irreducible, 127
law of large numbers, 128–130
reversible, 112, 114
time-homogeneous, 51
Markov Chain Monte Carlo, 109–180
convergence, 126–138
reversible jump, 159–179
Markov property
of Brownian motion, 216
MCMC, see Markov Chain Monte
Carlo

INDEX
381
mean squared error, 76
for antithetic variables, 90
for control variates, 94
for importance sampling, 85
for MCMC, 130
for Monte Carlo estimates, 77
for SDEs, 243, 245–247
Metropolis–Hastings method, 110–126
continuous state space, 110–113
discrete state space, 113–116
independence sampler, 120–121
move types, 121–126, 163, 173–177
random walk Metropolis, 116–119,
129, 140–142
Milstein scheme, 234, 233–236
mixture distributions, 46–48, 147, 172
models
continuous-time, 213–261
hierarchical, 45–50, 52, 147
statistical, 1, 41–68
Monte Carlo estimates, 69–108
choice of sample size, 80–82
error, 76–80, 82–84
for integrals, 72
for probabilities, 71, 81, 87, 91, 102,
105, 244, 249
for SDEs, 243–255
multi-level, 250–255
variance reduction, 84–96, 247–255
MSE, see mean squared error
multi-level Monte Carlo estimates,
250–255
normal distribution, 265, 293
generation, 23, 34
half, 21
multivariate, 41–45, 214
normalisation constant, 266
optimisation under constraints, 245
option pricing, 255–259
Pareto distribution, 162
partition function, 154
pixels, 153
plug-in estimator, 198
plug-in principle, 198
point estimators, 83, 97–100, 197–203
Poisson distribution, 58, 102, 162, 293
Poisson process, 58–67
intensity function, 59
thinning method, 65
posterior distribution, 73, 138–141,
147, 158, 181
prior distribution, 138
PRNG, see pseudo random number
generator
probability, 263–270
probability density, 264
probability distribution, 263
probability vector, 52
pseudo random number generator, 2
R programming, 271–297
random number generators, 2–8
random variables, 263, 292
transformation of, 32–38
random walk, 50
random walk Metropolis, 116–119
ratio-of-uniforms method, 35–38
Rayleigh distribution, 14
rejection sampling, 15–32, 73, 120
basic, 15–19, 182
envelope, 19–23
for conditional distributions, 23–27,
150
resampling methods, 192–208
reversible jump Markov Chain Monte
Carlo, 159–179
dimension matching, 163–166, 175
state space, 161, 172
target distribution, 161, 173
transitions, 162, 173
reversible Markov chain, 112, 114
RJMCMC, see reversible jump Markov
Chain Monte Carlo
RMSE, see root-mean-square error
root-mean-square error, 79, 344

382
INDEX
sample correlation, 97–99
scaling property
of Brownian motion, 216
SDE, see stochastic differential
equations
seed, 2, 8, 153, 293
semicircle distribution, 18
skewness, 105
slice sampler, 145
stability of discretisation schemes,
241
standard error, 76, 201
bootstrap estimates, 201–203
standard normal distribution, see
normal distribution
state space
Gibbs sampler, 142
Markov chain, 50, 51, 56
RJMCMC, 161
stationary density, 58
stationary distribution, 55
statistical computing, 1–262
statistical hypothesis tests, 5, 103–106
statistical inference, 96–106
Bayesian, 138–142, 147–152,
172–179
bootstrap methods, 197–208
statistical models, 1, 41–68
stochastic analysis, 226–231
stochastic differential equations,
224–242
Euler–Maruyama scheme, 232–233
initial distribution, 224
Milstein scheme, 233–236
Monte Carlo estimates, 243–255
strong error, 236–237
weak error, 237–240
stochastic integrals, 226–231
time discretisation, 226
stochastic matrix, 52
Stratonovich integral, 230, 230–231
strong error, 236, 236–237
substitution rule, 33, 170
sufﬁcient statistic, 183
test functions, 238
tests
statistical, 5, 103–106
time discretisation, 214
Brownian motion, 217
for SDEs, 231–242
stochastic integrals, 226
transformation
of random variables, 32–38, 43
of U[0, 1], 13
transition density, 57
transition kernel, 56
transition matrix, 51
truncation error, 276
uniform distribution, 4, 8, 27, 293
discrete, 9, 293
ratio-of-uniforms, 35
variance reduction methods, 84–96,
247–255
weak error, 237–240
Wiener process, see Brownian motion
Wigner’s semicircle distribution, 18



 
 
Advanced 
Engineering 
Mathematics  
 
 

 
Advances in Applied Mathematics 
 
Series Editors:  
Daniel Zwillinger, H. T. Banks 
 
 
Advanced Mathematical Modeling with Technology 
William P. Fox, Robert E. Burks 
 
Introduction to Quantum Control and Dynamics 
Domenico D’Alessandro 
 
Handbook of Radar Signal Analysis 
Bassem R. Mahafza, Scott C. Winton, Atef Z. Elsherbeni 
 
Separation of Variables and Exact Solutions to Nonlinear PDEs 
Andrei D. Polyanin, Alexei I. Zhurov 
 
Boundary Value Problems on Time Scales, Volume I 
Svetlin Georgiev, Khaled Zennir 
 
Boundary Value Problems on Time Scales, Volume II 
Svetlin Georgiev, Khaled Zennir 
 
Observability and Mathematics 
Fluid Mechanics, Solutions of Navier-Stokes Equations, and Modeling 
Boris Khots 
 
Handbook of Differential Equations, 4th Edition 
Daniel Zwillinger, Vladimir Dobrushkin 
Experimental Statistics and Data Analysis for Mechanical and Aerospace Engineers 
James Middleton 
 
Advanced Engineering Mathematics with MATLAB 
Dean G. Duffy 
 
Handbook of Fractional Calculus for Engineering and Science 
Harendra Singh, H. M. Srivastava, Juan J Nieto 
 
Advanced Engineering Mathematics 
A Second Course  
Dean G. Duffy 
 
https://www.routledge.com/Advances-in-Applied-Mathematics/book-
series/CRCADVAPPMTH?pd=published,forthcoming&pg=1&pp=12&so=pub&view=list 

 
Advances in Applied Mathematics 
 
Advanced 
Engineering 
Mathematics  
A Second Course 
 
 
 
Dean G. Duffy 
 
 
 
 
 
 
 
 

 
MATLAB® is a trademark of The MathWorks, Inc. and is used with permission. The MathWorks does not 
warrant the accuracy of the text or exercises in this book. This book’s use or discussion of MATLAB® 
software or related products does not constitute endorsement or sponsorship by The MathWorks of a 
particular pedagogical approach or particular use of the MATLAB® software. 
First edition published 2022  
by CRC Press 
6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742 
and by CRC Press 
2 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN 
© 2022 Dean G. Duffy 
CRC Press is an imprint of Taylor & Francis Group, LLC 
Reasonable efforts have been made to publish reliable data and information, but the author and publisher 
cannot assume responsibility for the validity of all materials or the consequences of their use. The authors 
and publishers have attempted to trace the copyright holders of all material reproduced in this publication 
and apologize to copyright holders if permission to publish in this form has not been obtained. If any 
copyright material has not been acknowledged please write and let us know so we may rectify in any future 
reprint. 
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter 
invented, including photocopying, microfilming, and recording, or in any information storage or retrieval 
system, without written permission from the publishers. 
For permission to photocopy or use material electronically from this work, access www.copyright.com or 
contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-
8400. For works that are not available on CCC please contact mpkbookspermissions@tandf.co.uk 
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are used 
only for identification and explanation without intent to infringe. 
ISBN: 9781032133423 (hbk) 
ISBN: 9781032223452 (pbk)  
ISBN: 9781003272205 (ebk) 
 
DOI: 10.1201/9781003272205 
 
Publisher's note: This book has been prepared from camera-ready copy provided by the authors 

Dedicated to the Brigade of Midshipmen
and the Corps of Cadets
v


Contents
Dedication
v
Contents
vii
Acknowledgments
xiii
Author
xv
Introduction
xvii
List of Deﬁnitions
xix
x
π/3
1
C
πi
e
/6
3
y
C2
C
Chapter 1:
Complex Variables
1
1.1
Complex Numbers
1
1.2
Finding Roots
5
1.3
The Derivative in the Complex Plane: The Cauchy-Riemann Equations
8
1.4
Line Integrals
16
1.5
The Cauchy-Goursat Theorem
20
vii

viii
Advanced Engineering Mathematics: A Second Course
1.6
Cauchy’s Integral Formula
23
1.7
Taylor and Laurent Expansions and Singularities
27
1.8
Theory of Residues
33
1.9
Evaluation of Real Deﬁnite Integrals
37
1.10 Cauchy’s Principal Value Integral
50
1.11 Conformal Mapping
59
t > 3
(c,0)
t < 3
Chapter 2: Advanced
Transform Methods
77
2.1 Inversion of Fourier Transforms by Contour Integration
77
2.2 Inversion of Laplace Transforms by Contour Integration
92
2.3 Integral Equations
100
2.4 The Solution of the Wave Equation by Using Laplace Transforms
105
2.5 The Solution of the Heat Equation by Using Laplace Transforms
129
2.6 The Solution of Laplace’s Equation by Using Laplace Transforms
154
0.0
1.0
2.0
3.0
ωΤ
0.1
1.0
10.0
Ratio of quadrature amplitudes to ideal integration      
Trapezoidal
Simpson’s
3/8−rule
Rule
Simpson’s
1/3−rule
Ideal Rule
Chapter 3:
The Z-Transform
159
3.1 The Relationship of the Z-Transform to the Laplace Transform
160
3.2 Some Useful Properties
164
3.3 Inverse Z-Transforms
173
3.4 Solution of Diﬀerence Equations
183
3.5 Stability of Discrete-Time Systems
189

Table of Contents
ix
−15
−10
−5
0
5
10
15
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
t
exact Hilbert transform
computed Hilbert transform
Chapter 4:
The Hilbert Transform
195
4.1 Deﬁnition
195
4.2 Some Useful Properties
205
4.3 Analytic Signals
211
4.4 Causality: The Kramers-Kronig Relationship
213
Chapter 5:
Green’s Functions
217
5.1 What Is a Green’s Function?
217
5.2 Ordinary Diﬀerential Equations
223
5.3 Joint Transform Method
243
5.4 Wave Equation
247
5.5 Heat Equation
256
5.6 Helmholtz’s Equation
266
5.7 Galerkin Method
285
−4
−3
−2
−1
0
1
2
3
4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
 x
Estimated and true PDF
Chapter 6:
Probability
293
6.1 Review of Set Theory
294
6.2 Classic Probability
295
6.3 Discrete Random Variables
308

x
Advanced Engineering Mathematics: A Second Course
6.4 Continuous Random Variables
313
6.5 Mean and Variance
318
6.6 Some Commonly Used Distributions
325
6.7 Joint Distributions
333
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
1.5
2
2.5
x
y
Chapter 7:
Random Processes
345
7.1 Fundamental Concepts
349
7.2 Power Spectrum
354
7.3 Two-State Markov Chains
357
7.4 Birth and Death Processes
366
7.5 Poisson Processes
377
Rt/L
0
0.5
1
1.5
2
2.5
3
3.5
4
I(t)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Chapter 8:
Itˆo’s Stochastic Calculus
385
8.1 Random Diﬀerential Equations
386
8.2 Random Walk and Brownian Motion
395
8.3 Itˆo’s Stochastic Integral
406
8.4 Itˆo’s Lemma
410
8.5 Stochastic Diﬀerential Equations
419
8.6 Numerical Solution of Stochastic Diﬀerential Equations
427
Answers to the Odd-Numbered Problems
435
Index
443

Acknowledgments
I would like to thank the many midshipmen and cadets who have taken engineering
mathematics from me. They have been willing or unwilling guinea pigs in testing out many
of the ideas and problems in this book. Special thanks go to Prof. William S. Price of
the University of Western Sydney (Australia) for his suggestions concerning a chapter on
Green’s functions. Most of the plots and calculations were done using MATLAB R
⃝.
MATLAB is a registered trademark of
The MathWorks Inc.
24 Prime Park Way
Natick, MA 01760-1500
Phone: (508) 647-7000
Email: info@mathworks.com
www.mathworks.com
xi


Author
Dean G. Duﬀy received his bachelor of science in geophysics from Case Institute of
Technology (Cleveland, Ohio) and his doctorate of science in meteorology from the Mas-
sachusetts Institute of Technology (Cambridge, Massachusetts). He served in the United
States Air Force from September 1975 to December 1979 as a numerical weather prediction
oﬃcer. After his military service, he began a twenty-ﬁve-year (1980 to 2005) association
with NASA at the Goddard Space Flight Center (Greenbelt, Maryland) where he focused on
numerical weather prediction, oceanic wave modeling, and dynamical meteorology. He also
wrote papers in the areas of Laplace transforms, antenna theory, railroad tracks, and heat
conduction. In addition to his NASA duties, he taught engineering mathematics, diﬀerential
equations, and calculus at the United States Naval Academy (Annapolis, Maryland) and
the United States Military Academy (West Point, New York). Drawing from his teaching
experience, he has written several books on transform methods, engineering mathematics,
Green’s functions, and mixed-boundary-value problems.
xiii


Introduction
For the last twenty-ﬁve years I have written a series of engineering mathematics books.
When it came to revising my fourth edition, I realized that something radical must be
done. The encyclopedic engineering mathematics tome is dead, killed by the growth of
the Internet and students’ unwillingness to buy these books. When I surveyed the current
variety of engineering mathematics courses, I realized that I needed to write two books.
The companion to the present volume (Advanced Engineering Mathematics with MATLAB)
would only focus on those topics that are currently taught in most advanced engineering
mathematics courses. When one had studied that volume, he/she could feel conﬁdent that
he/she had a solid knowledge of those mathematical techniques used in current engineering
and scientiﬁc courses.
This volume (Advanced Engineering Mathematics: A Second Course) is my attempt to
look into the future of advanced engineering mathematics courses. Some of the material,
such as complex variables and probability, is currently taught to engineers, although not
usually in courses entitled advanced engineering mathematics. I have included these topics
because they are required for transform methods and random processes.
One trend that I see is that entering freshmen are increasingly likely to have had
calculus in high school. This means that they will probably place out of the traditional
diﬀerential and integral calculus courses in their freshman year, allowing them to take
multivariable calculus and diﬀerential equations during their freshman year and taking
advanced engineering mathematics courses during their sophomore year. Therefore, the
question arises as to nature of these courses. The answer appears to be that the current
traditional engineering mathematics course will occur during the fall semester and some
other mathematics course will occur during the spring semester. The present volume is
designed to meet this need, as well as stand as the advanced engineering mathematics
text on its own. For those past formal education, this book provides the professional with
powerful mathematical techniques.
The ﬁrst ﬁve chapters are aimed at the systems, communications and electrical en-
gineering crowd: those involved in the digital revolution. First, that portion of complex
xv

xvi
Advanced Engineering Mathematics: A Second Course
variable theory is presented so that the reader will feel prepared in dealing with transform
methods. For example, Chapter 2 shows how complex variables can be used to invert par-
ticularly complicated Fourier and Laplace transforms. This chapter also illustrates how
transform methods can solve the heat, wave and Laplace’s equations.
In Chapters 3 and 4 we study two transforms, the z- and Hilbert transforms, that are
currently important in the digital revolution. Chapter 3 introduces the z-transform by ﬁrst
giving its deﬁnition and then developing some of its general properties. We also illustrate
how to compute the inverse by long division, partial fractions, and contour integration.
Finally, we use z-transforms to solve diﬀerence equations, especially with respect to the
stability of the system.
The Hilbert transform is important in the explosion of interest in communications. The
Hilbert transform is introduced in Section 4.1 and its properties are explored in Section 4.2.
Two important applications of Hilbert transforms are introduced in Sections 4.3 and 4.4,
namely the concept of analytic signals and the Kramers-Kronig relationship.
To round out this area we present Green’s function in Chapter 5. Green’s function
gives the response of a system to impulse forcing without the clouding eﬀects of a particular
forcing function or initial conditions. Each successive section deals with ordinary, wave, heat
and Helmholtz’s equations. The solution to general problems follows from the superposition
integral.
The book concludes by turning to the future. It is now recognized that random pro-
cesses are useful in describing many physical systems. We begin by introducing the fun-
damental concepts behind probability in Chapter 6 and random processes in Chapter 7.
Chapter 6 introduces the student to the concepts of probability distributions, mean, and
variance because these topics appear so frequently in random processes. Chapter 7 explores
common random processes such as Poisson processes and birth and death.
A unique aspect of this book appears in Chapter 8, which is devoted to stochastic
calculus. We start by exploring deterministic diﬀerential equations with a stochastic forcing.
Next, the important stochastic process of Brownian motion is developed in depth. Using this
Brownian motion, we introduce the concept of (Itˆo) stochastic integration, Itˆo’s lemma, and
stochastic diﬀerential equations. The chapter concludes with various numerical methods to
integrate stochastic diﬀerential equations.
MATLAB is still employed to reinforce the concepts that are taught. Of course, this
book still continues my principle of including a wealth of examples from the scientiﬁc and
engineering literature. Worked solutions to all of the problems are given at the end.

List of Deﬁnitions
Function
Deﬁnition
δ(t −a)
=
 ∞,
t = a,
0,
t ̸= a,
Z ∞
−∞
δ(t −a) dt = 1
erf(x)
=
2
√π
Z x
0
e−y2 dy
Γ(x)
gamma function
H(t −a)
=
 1,
t > a,
0,
t < a.
ℑ(z)
imaginary part of the complex variable z
In(x)
modiﬁed Bessel function of the ﬁrst kind and order n
Jn(x)
Bessel function of the ﬁrst kind and order n
Kn(x)
modiﬁed Bessel function of the second kind and order n
Pn(x)
Legendre polynomial of order n
ℜ(z)
real part of the complex variable z
sgn(t −a)
=
 −1,
t < a,
1,
t > a.
Yn(x)
Bessel function of the second kind and order n
xvii


x
π/3
1
C
πi
e
/6
3
y
C2
C
Chapter 1
Complex Variables
The theory of complex variables was originally developed by mathematicians as an aid
in understanding functions. Functions of a complex variable enjoy many powerful properties
that their real counterparts do not. That is not why we will study them. For us they provide
the keys for the complete mastery of transform methods and diﬀerential equations.
In this chapter all of our work points to one objective: integration on the complex
plane by the method of residues. For this reason we minimize discussions of limits and
continuity, which play such an important role in conventional complex variables, in favor
of the computational aspects. We begin by introducing some simple facts about complex
variables. Then we progress to diﬀerential and integral calculus on the complex plane.
1.1 COMPLEX NUMBERS
A complex number is any number of the form a+bi, where a and b are real and i = √−1.
We denote any member of a set of complex numbers by the complex variable z = x + iy.
The real part of z, usually denoted by ℜ(z), is x while the imaginary part of z, ℑ(z), is y.
The complex conjugate, z or z∗, of the complex number a + bi is a −bi.
Complex numbers obey the fundamental rules of algebra. Thus, two complex numbers
a + bi and c + di are equal if and only if a = c and b = d. Just as real numbers have
the fundamental operations of addition, subtraction, multiplication, and division, so too do
complex numbers. These operations are deﬁned:
Addition
(a + bi) + (c + di) = (a + c) + (b + d)i
(1.1.1)
Subtraction
(a + bi) −(c + di) = (a −c) + (b −d)i
(1.1.2)
1

2
Advanced Engineering Mathematics: A Second Course
Multiplication
(a + bi)(c + di) = ac + bci + adi + i2bd = (ac −bd) + (ad + bc)i
(1.1.3)
Division
a + bi
c + di = a + bi
c + di
c −di
c −di = ac −adi + bci −bdi2
c2 + d2
= ac + bd + (bc −ad)i
c2 + d2
.
(1.1.4)
The absolute value or modulus
of a complex number a + bi, written |a + bi|, equals
√
a2 + b2. Additional properties include:
|z1z2z3 · · · zn| = |z1||z2||z3| · · · |zn|
(1.1.5)
|z1/z2| = |z1|/|z2|
if
z2 ̸= 0
(1.1.6)
|z1 + z2 + z3 + · · · + zn| ≤|z1| + |z2| + |z3| + · · · + |zn|
(1.1.7)
and
|z1 + z2| ≥|z1| −|z2|.
(1.1.8)
The use of inequalities with complex variables has meaning only when they involve absolute
values.
It is often useful to plot the complex number x + iy as a point (x, y) in the xy-plane,
now called the complex plane. Figure 1.1.1 illustrates this representation.
This geometrical interpretation of a complex number suggests an alternative method
of expressing a complex number: the polar form. From the polar representation of x and y,
x = r cos(θ)
and
y = r sin(θ),
(1.1.9)
where r =
p
x2 + y2 is the modulus, amplitude, or absolute value of z and θ is the argument
or phase, we have that
z = x + iy = r[cos(θ) + i sin(θ)].
(1.1.10)
However, from the Taylor expansion of the exponential in the real case,
eiθ =
∞
X
k=0
(θi)k
k! .
(1.1.11)
Expanding Equation 1.1.11,
eiθ = 1 −θ2
2! + θ4
4! −θ6
6! + · · · + i

θ −θ3
3! + θ5
5! −θ7
7! + · · ·

(1.1.12)
= cos(θ) + i sin(θ).
(1.1.13)
Equation 1.1.13 is Euler’s formula. Consequently, we may express Equation 1.1.10 as
z = reiθ,
(1.1.14)
which is the polar form of a complex number. Furthermore, because
zn = rneinθ
(1.1.15)

Complex Variables
3
θ
(x,y)
r
y
x
Figure 1.1.1: The complex plane.
by the law of exponents,
zn = rn[cos(nθ) + i sin(nθ)].
(1.1.16)
Equation 1.1.16 is De Moivre’s theorem.
• Example 1.1.1
Let us simplify the following complex number:
3 −2i
−1 + i = 3 −2i
−1 + i × −1 −i
−1 −i = −3 −3i + 2i + 2i2
1 + 1
= −5 −i
2
= −5
2 −i
2. (1.1.17)
⊓⊔
• Example 1.1.2
Let us reexpress the complex number −
√
6 −i
√
2 in polar form. From Equation 1.1.9
r = √6 + 2 and θ = tan−1(b/a) = tan−1(1/
√
3) = π/6 or 7π/6. Because −
√
6 −i
√
2 lies in
the third quadrant of the complex plane, θ = 7π/6 and
−
√
6 −i
√
2 = 2
√
2e7πi/6.
(1.1.18)
Note that Equation 1.1.18 is not a unique representation because ±2nπ may be added to
7π/6 and we still have the same complex number since
ei(θ±2nπ) = cos(θ ± 2nπ) + i sin(θ ± 2nπ) = cos(θ) + i sin(θ) = eiθ.
(1.1.19)
For uniqueness we often choose n = 0 and deﬁne this choice as the principal branch. Other
branches correspond to diﬀerent values of n.
⊓⊔
• Example 1.1.3
Find the curve described by the equation |z −z0| = a.
From the deﬁnition of the absolute value,
p
(x −x0)2 + (y −y0)2 = a
(1.1.20)

4
Advanced Engineering Mathematics: A Second Course
or
(x −x0)2 + (y −y0)2 = a2.
(1.1.21)
Equation 1.1.21, and hence |z −z0| = a, describes a circle of radius a with its center located
at (x0, y0). Later on, we shall use equations such as this to describe curves in the complex
plane.
⊓⊔
• Example 1.1.4
As an example in manipulating complex numbers, let us show that

a + bi
b + ai
 = 1.
(1.1.22)
We begin by simplifying
a + bi
b + ai = a + bi
b + ai × b −ai
b −ai =
2ab
a2 + b2 + b2 −a2
a2 + b2 i.
(1.1.23)
Therefore,

a + bi
b + ai
 =
s
4a2b2
(a2 + b2)2 + b4 −2a2b2 + a4
(a2 + b2)2
=
s
a4 + 2a2b2 + b4
(a2 + b2)2
= 1.
(1.1.24)
MATLAB can also be used to solve this problem. Typing the commands
>> syms a b real
>> abs((a+b*i)/(b+a*i))
yields
ans =
1
Note that you must declare a and b real in order to get the ﬁnal result.
Problems
Simplify the following complex numbers. Represent the solution in the Cartesian form a+bi.
Check your answers using MATLAB.
1.
5i
2 + i
2. 5 + 5i
3 −4i +
20
4 + 3i
3. 1 + 2i
3 −4i + 2 −i
5i
4. (1 −i)4
5. i(1 −i
√
3)(
√
3 + i)
6. (7 + i)(1 −5i)
(4 −i)(6 + i)
Represent the following complex numbers in polar form:
7.
−i
8.
−4
9.
2 + 2
√
3 i
10.
−5 + 5i
11.
2 −2i
12.
−1 +
√
3 i
13. By the law of exponents, ei(α+β) = eiαeiβ. Use Euler’s formula to obtain expressions
for cos(α + β) and sin(α + β) in terms of sines and cosines of α and β.

Complex Variables
5
14. Use De Moivre’s theorem with r = 1 to express cos(4θ) and sin(4θ) in terms of cos(θ)
and sin(θ).
15.
Using the property that PN
n=0 qn = (1 −qN+1)/(1 −q) and the geometric series
PN
n=0 eint, obtain the following sums of trigonometric functions:
N
X
n=0
cos(nt) = cos
Nt
2
 sin[(N + 1)t/2]
sin(t/2)
and
N
X
n=1
sin(nt) = sin
Nt
2
 sin[(N + 1)t/2]
sin(t/2)
.
These results are often called Lagrange’s trigonometric identities.
16. (a) Using the property that P∞
n=0 qn = 1/(1 −q), if |q| < 1, and the geometric series
P∞
n=0 ǫneint, |ǫ| < 1, show that
∞
X
n=0
ǫn cos(nt) =
1 −ǫ cos(t)
1 + ǫ2 −2ǫ cos(t)
and
∞
X
n=1
ǫn sin(nt) =
ǫ sin(t)
1 + ǫ2 −2ǫ cos(t).
(b) Let ǫ = e−a, where a > 0. Show that
2
∞
X
n=1
e−na sin(nt) =
sin(t)
cosh(a) −cos(t).
1.2 FINDING ROOTS
The concept of ﬁnding roots of a number, which is rather straightforward in the case of
real numbers, becomes more diﬃcult in the case of complex numbers. By ﬁnding the roots
of a complex number, we wish to ﬁnd all of the solutions w of the equation wn = z, where
n is a positive integer for a given z.
We begin by writing z in the polar form:
z = reiϕ,
(1.2.1)
while we write
w = ReiΦ
(1.2.2)
for the unknown. Consequently,
wn = RneinΦ = reiϕ = z.
(1.2.3)
We satisfy Equation 1.2.3 if
Rn = r
and
nΦ = ϕ + 2kπ,
k = 0, ±1, ±2, . . . ,
(1.2.4)
because the addition of any multiple of 2π to the argument is also a solution.
Thus,
R = r1/n, where R is the uniquely determined real positive root, and
Φk = ϕ
n + 2πk
n ,
k = 0, ±1, ±2, . . . .
(1.2.5)

6
Advanced Engineering Mathematics: A Second Course
Because wk = wk±n, it is suﬃcient to take k = 0, 1, 2, . . . , n−1. Therefore, there are exactly
n solutions:
wk = ReΦki = r1/n exp

i
ϕ
n + 2πk
n

(1.2.6)
with k = 0, 1, 2, . . . , n −1. They are the n roots of z. Geometrically we can locate these
solutions wk on a circle, centered at the point (0, 0), with radius R and separated from each
other by 2π/n radians. These roots also form the vertices of a regular polygon of n sides
inscribed inside a circle of radius R. (See Example 1.2.1.)
In summary, the method for ﬁnding the n roots of a complex number z0 is as follows.
First, write z0 in its polar form: z0 = reiϕ. Then multiply the polar form by e2iπk. Using
the law of exponents, take the 1/n power of both sides of the equation.
Finally, using
Euler’s formula, evaluate the roots for k = 0, 1, . . . , n −1.
• Example 1.2.1
Let us ﬁnd all of the values of z for which z5 = −32 and locate these values on the
complex plane.
Because
−32 = 32eπi = 25eπi,
(1.2.7)
zk = 2 exp
πi
5 + 2πik
5

,
k = 0, 1, 2, 3, 4,
(1.2.8)
or
z0 = 2 exp
πi
5

= 2
h
cos
π
5

+ i sin
π
5
i
,
(1.2.9)
z1 = 2 exp
3πi
5

= 2

cos
3π
5

+ i sin
3π
5

,
(1.2.10)
z2 = 2eπi = −2,
(1.2.11)
z3 = 2 exp
7πi
5

= 2

cos
7π
5

+ i sin
7π
5

(1.2.12)
and
z4 = 2 exp
9πi
5

= 2

cos
9π
5

+ i sin
9π
5

.
(1.2.13)
Figure 1.2.1 shows the location of these roots in the complex plane.
⊓⊔
• Example 1.2.2
Let us ﬁnd the cube roots of −1 + i and locate them graphically.
Because −1 + i =
√
2 exp(3πi/4),
zk = 21/6 exp
πi
4 + 2iπk
3

,
k = 0, 1, 2,
(1.2.14)
or
z0 = 21/6 exp
πi
4

= 21/6 h
cos
π
4

+ i sin
π
4
i
,
(1.2.15)

Complex Variables
7
x
y
1
z
z
z
z
z
0
4
3
2
Figure 1.2.1: The zeros of z5 = −32.
z1 = 21/6 exp
11πi
12

= 21/6

cos
11π
12

+ i sin
11π
12

,
(1.2.16)
and
z2 = 21/6 exp
19πi
12

= 21/6

cos
19π
12

+ i sin
19π
12

.
(1.2.17)
Figure 1.2.2 gives the location of these zeros on the complex plane.
⊓⊔
• Example 1.2.3
The routine solve in MATLAB can also be used to compute the roots of complex
numbers. For example, let us ﬁnd all of the roots of z4 = −a4.
The MATLAB commands are as follows:
>> syms a z
>> solve(z^4+a^4)
This yields the solution
ans=
[
(1/2*2^(1/2)+1/2*i*2^(1/2))*a]
[ (-1/2*2^(1/2)+1/2*i*2^(1/2))*a]
[
(1/2*2^(1/2)-1/2*i*2^(1/2))*a]
[ (-1/2*2^(1/2)-1/2*i*2^(1/2))*a]
Problems
Extract all of the possible roots of the following complex numbers. Verify your answer using
MATLAB.
1.
81/6
2.
(−1)1/3
3.
(−i)1/3
4.
(−27i)1/6
5. Find algebraic expressions for the square roots of a −bi, where a > 0 and b > 0.
6. Find all of the roots for the algebraic equation z4 −3iz2 −2 = 0. Then check your answer
using solve in MATLAB.

8
Advanced Engineering Mathematics: A Second Course
y
0
x
z
z
z
1
2
Figure 1.2.2: The zeros of z3 = −1 + i.
7. Find all of the roots for the algebraic equation z4 + 6iz2 + 16 = 0. Then check your
answer using solve in MATLAB.
1.3 THE DERIVATIVE IN THE COMPLEX PLANE: THE CAUCHY-RIEMANN EQUATIONS
In the previous two sections, we introduced complex arithmetic. We are now ready for
the concept of function as it applies to complex variables.
We already deﬁned the complex variable z = x+iy, where x and y are variable. We now
introduce another complex variable w = u+iv so that for each value of z there corresponds
a value of w = f(z). From all of the possible complex functions that we might invent, we
focus on those functions where for each z there is one, and only one, value of w. These
functions are single-valued. They diﬀer from functions such as the square root, logarithm,
and inverse sine and cosine, where there are multiple answers for each z. These multivalued
functions do arise in various problems. However, they are beyond the scope of this book
and we shall always assume that we are dealing with single-valued functions.
A popular method for representing a complex function involves drawing some closed
domain in the z-plane and then showing the corresponding domain in the w-plane. This
procedure is called mapping and the z-plane illustrates the domain of the function while
the w-plane illustrates its image or range. Figure 1.3.1 shows the z-plane and w-plane for
w = z2; a pie-shaped wedge in the z-plane maps into a semicircle on the w-plane.
• Example 1.3.1
Given the complex function w = e−z2, let us ﬁnd the corresponding u(x, y) and v(x, y).
From Euler’s formula,
w = e−z2 = e−(x+iy)2 = ey2−x2e−2ixy = ey2−x2[cos(2xy) −i sin(2xy)].
(1.3.1)
Therefore, by inspection,
u(x, y) = ey2−x2 cos(2xy),
and
v(x, y) = −ey2−x2 sin(2xy).
(1.3.2)

Complex Variables
9
2
x
y
v
u
z-plane 
w-plane
2
1
3
3
1
Figure 1.3.1: The complex function w = z2.
Note that there is no i in the expression for v(x, y). The function w = f(z) is single-valued
because for each distinct value of z, there is a unique value of u(x, y) and v(x, y).
⊓⊔
• Example 1.3.2
As counterpoint, let us show that w = √z is a multivalued function.
We begin by writing z = reiθ+2πik, where r =
p
x2 + y2 and θ = tan−1(y/x). Then,
wk = √reiθ/2+πik,
k = 0, 1,
(1.3.3)
or
w0 = √r [cos(θ/2) + i sin(θ/2)]
and
w1 = −w0.
(1.3.4)
Therefore,
u0(x, y) = √r cos(θ/2),
v0(x, y) = √r sin(θ/2),
(1.3.5)
and
u1(x, y) = −√r cos(θ/2),
v1(x, y) = −√r sin(θ/2).
(1.3.6)
Each solution w0 or w1 is a branch of the multivalued function √z. We can make √z single-
valued by restricting ourselves to a single branch, say w0. In that case, the ℜ(w) > 0 if we
restrict −π < θ < π. Although this is not the only choice that we could have made, it is a
popular one. For example, most digital computers use this deﬁnition in their complex square
root function. The point here is our ability to make a multivalued function single-valued
by deﬁning a particular branch.
⊓⊔
Although the requirement that a complex function be single-valued is important, it is
still too general and would cover all functions of two real variables. To have a useful theory,
we must introduce additional constraints. Because an important property associated with
most functions is the ability to take their derivative, let us examine the derivative in the
complex plane.
Following the deﬁnition of a derivative for a single real variable, the derivative of a
complex function w = f(z) is deﬁned as
dw
dz = lim
∆z→0
∆w
∆z = lim
∆z→0
f(z + ∆z) −f(z)
∆z
.
(1.3.7)
A function of a complex variable that has a derivative at every point within a region of the
complex plane is said to be analytic (or regular or holomorphic) over that region. If the
function is analytic everywhere in the complex plane, it is entire.

10
Advanced Engineering Mathematics: A Second Course
Because the derivative is deﬁned as a limit and limits are well behaved with respect
to elementary algebraic operations, the following operations carry over from elementary
calculus:
d
dz

cf(z)

= cf ′(z),
c a constant
(1.3.8)
d
dz

f(z) ± g(z)

= f ′(z) ± g′(z)
(1.3.9)
d
dz

f(z)g(z)

= f ′(z)g(z) + f(z)g′(z)
(1.3.10)
d
dz
f(z)
g(z)

= g(z)f ′(z) −g′(z)f(z)
g2(z)
(1.3.11)
d
dz

f[g(z)]

= f ′[g(z)]g′(z),
the chain rule.
(1.3.12)
Another important property that carries over from real variables is l’Hˆopital’s rule: Let
f(z) and g(z) be analytic at z0, where f(z) has a zero1 of order m and g(z) has a zero of
order n. Then, if m > n,
lim
z→z0
f(z)
g(z) = 0;
(1.3.13)
if m = n,
lim
z→z0
f(z)
g(z) = f (m)(z0)
g(m)(z0) ;
(1.3.14)
and if m < n,
lim
z→z0
f(z)
g(z) = ∞.
(1.3.15)
• Example 1.3.3
Let us evaluate limz→i(z10 + 1)/(z6 + 1). From l’Hˆopital’s rule,
lim
z→i
z10 + 1
z6 + 1 = lim
z→i
10z9
6z5 = 5
3 lim
z→i z4 = 5
3.
(1.3.16)
⊓⊔
So far, we introduced the derivative and some of its properties. But how do we actually
know whether a function is analytic or how do we compute its derivative? At this point we
must develop some relationships involving the known quantities u(x, y) and v(x, y).
We begin by returning to the deﬁnition of the derivative. Because ∆z = ∆x + i∆y,
there is an inﬁnite number of diﬀerent ways of approaching the limit ∆z →0. Uniqueness
of that limit requires that Equation 1.3.7 must be independent of the manner in which ∆z
approaches zero. A simple example is to take ∆z in the x-direction so that ∆z = ∆x;
another is to take ∆z in the y-direction so that ∆z = i∆y. These examples yield
dw
dz = lim
∆z→0
∆w
∆z = lim
∆x→0
∆u + i∆v
∆x
= ∂u
∂x + i∂v
∂x
(1.3.17)
1 An analytic function f(z) has a zero of order m at z0 if and only if f(z0) = f′(z0) = · · · = f(m−1)(z0) =
0 and f(m)(z0) ̸= 0.

Complex Variables
11
Although educated as an engineer, Augustin-Louis Cauchy (1789–1857) would become a mathe-
matician’s mathematician, publishing 789 papers and 7 books in the ﬁelds of pure and applied
mathematics. His greatest writings established the discipline of mathematical analysis as he reﬁned
the notions of limit, continuity, function, and convergence. It was this work on analysis that led him
to develop complex function theory via the concept of residues. (Portrait courtesy of the Archives
de l’Acad´emie des sciences, Paris.)
and
dw
dz = lim
∆z→0
∆w
∆z = lim
∆y→0
∆u + i∆v
i∆y
= ∂v
∂y −i∂u
∂y .
(1.3.18)
In both cases we are approaching zero from the positive side. For the limit to be unique
and independent of path, Equation 1.3.17 must equal Equation 1.3.18, or
∂u
∂x = ∂v
∂y
and
∂u
∂y = −∂v
∂x.
(1.3.19)
These equations that u and v must both satisfy are the Cauchy-Riemann equations.
They are necessary but not suﬃcient to ensure that a function is diﬀerentiable. The follow-
ing example illustrates this.
• Example 1.3.4
Consider the complex function
w =

z5/|z|4,
z ̸= 0
0,
z = 0.
(1.3.20)
The derivative at z = 0 is given by
dw
dz = lim
∆z→0
(∆z)5/|∆z|4 −0
∆z
= lim
∆z→0
(∆z)4
|∆z|4 ,
(1.3.21)

12
Advanced Engineering Mathematics: A Second Course
Despite his short life, (Georg Friedrich) Bernhard Riemann’s (1826–1866) mathematical work con-
tained many imaginative and profound concepts. It was in his doctoral thesis on complex function
theory (1851) that he introduced the Cauchy-Riemann diﬀerential equations. Riemann’s later work
dealt with the deﬁnition of the integral and the foundations of geometry and non-Euclidean (elliptic)
geometry. (Portrait courtesy of Photo AKG, London, with permission.)
provided that this limit exists. However, this limit does not exist because, in general, the
numerator depends upon the path used to approach zero. For example, if ∆z = reπi/4 with
r →0, dw/dz = −1. On the other hand, if ∆z = reπi/2 with r →0, dw/dz = 1.
Are the Cauchy-Riemann equations satisﬁed in this case? To check this, we ﬁrst com-
pute
ux(0, 0) = lim
∆x→0
 ∆x
|∆x|
4
= 1,
vy(0, 0) = lim
∆y→0
 i∆y
|∆y|
4
= 1,
(1.3.22)
uy(0, 0) = lim
∆y→0 ℜ
 (i∆y)5
∆y|∆y|4

= 0,
and
vx(0, 0) = lim
∆x→0 ℑ
" ∆x
|∆x|
4#
= 0.
(1.3.23)
Hence, the Cauchy-Riemann equations are satisﬁed at the origin. Thus, even though the
derivative is not uniquely deﬁned, Equation 1.3.21 happens to have the same value for paths
taken along the coordinate axes so that the Cauchy-Riemann equations are satisﬁed.
⊓⊔
In summary, if a function is diﬀerentiable at a point, the Cauchy-Riemann equations
hold. Similarly, if the Cauchy-Riemann equations are not satisﬁed at a point, then the
function is not diﬀerentiable at that point. This is one of the important uses of the Cauchy-
Riemann equations: the location of nonanalytic points. Isolated nonanalytic points of an
otherwise analytic function are called isolated singularities.
Functions that contain isolated
singularities are called meromorphic.
The Cauchy-Riemann condition can be modiﬁed so that it is suﬃcient for the derivative
to exist. Let us require that ux, uy, vx, and vy be continuous in some region surrounding a

Complex Variables
13
point z0 and satisfy the Cauchy-Riemann equations there. Then
f(z) −f(z0) = [u(z) −u(z0)] + i[v(z) −v(z0)]
(1.3.24)
= [ux(z0)(x −x0) + uy(z0)(y −y0) + ǫ1(x −x0) + ǫ2(y −y0)]
+ i[vx(z0)(x −x0) + vy(z0)(y −y0) + ǫ3(x −x0) + ǫ4(y −y0)]
(1.3.25)
= [ux(z0) + ivx(z0)](z −z0) + (ǫ1 + iǫ3)(x −x0)
+ (ǫ2 + iǫ4)(y −y0),
(1.3.26)
where we used the Cauchy-Riemann equations and ǫ1, ǫ2, ǫ3, ǫ4 →0 as ∆x, ∆y →0. Hence,
f ′(z0) = lim
∆z→0
f(z) −f(z0)
∆z
= ux(z0) + ivx(z0),
(1.3.27)
because |∆x| ≤|∆z| and |∆y| ≤|∆z|. Using Equation 1.3.27 and the Cauchy-Riemann
equations, we can obtain the derivative from any of the following formulas:
dw
dz = ∂u
∂x + i∂v
∂x = ∂v
∂y −i∂u
∂y ,
(1.3.28)
and
dw
dz = ∂v
∂y + i∂v
∂x = ∂u
∂x −i∂u
∂y .
(1.3.29)
Furthermore, f ′(z0) is continuous because the partial derivatives are.
• Example 1.3.5
Let us show that sin(z) is an entire function.
w = sin(z)
(1.3.30)
u + iv = sin(x + iy) = sin(x) cos(iy) + cos(x) sin(iy)
(1.3.31)
= sin(x) cosh(y) + i cos(x) sinh(y),
(1.3.32)
because
cos(iy) = 1
2

ei(iy) + e−i(iy)
= 1
2

ey + e−y
= cosh(y),
(1.3.33)
and
sin(iy) = 1
2i

ei(iy) −e−i(iy)
= −1
2i

ey −e−y
= i sinh(y),
(1.3.34)
so that
u(x, y) = sin(x) cosh(y),
and
v(x, y) = cos(x) sinh(y).
(1.3.35)
Diﬀerentiating both u(x, y) and v(x, y) with respect to x and y, we have that
∂u
∂x = cos(x) cosh(y),
∂u
∂y = sin(x) sinh(y),
(1.3.36)

14
Advanced Engineering Mathematics: A Second Course
∂v
∂x = −sin(x) sinh(y),
∂v
∂y = cos(x) cosh(y),
(1.3.37)
and u(x, y) and v(x, y) satisfy the Cauchy-Riemann equations for all values of x and y.
Furthermore, ux, uy, vx, and vy are continuous for all x and y. Therefore, the function
w = sin(z) is an entire function.
⊓⊔
• Example 1.3.6
Consider the function w = 1/z. Then
w = u + iv =
1
x + iy =
x
x2 + y2 −
iy
x2 + y2 .
(1.3.38)
Therefore,
u(x, y) =
x
x2 + y2 ,
and
v(x, y) = −
y
x2 + y2 .
(1.3.39)
Now
∂u
∂x = (x2 + y2) −2x2
(x2 + y2)2
=
y2 −x2
(x2 + y2)2 ,
(1.3.40)
∂v
∂y = −(x2 + y2) −2y2
(x2 + y2)2
=
y2 −x2
(x2 + y2)2 = ∂u
∂x,
(1.3.41)
∂v
∂x = −0 −2xy
(x2 + y2)2 =
2xy
(x2 + y2)2 ,
(1.3.42)
and
∂u
∂y =
0 −2xy
(x2 + y2)2 = −
2xy
(x2 + y2)2 = −∂v
∂x.
(1.3.43)
The function is analytic at all points except the origin because the function itself ceases to
exist when both x and y are zero and the modulus of w becomes inﬁnite.
⊓⊔
• Example 1.3.7
Let us ﬁnd the derivative of sin(z).
Using Equation 1.3.28 and Equation 1.3.32,
d
dz

sin(z)

= ∂u
∂x +i∂v
∂x = cos(x) cosh(y)−i sin(x) sinh(y) = cos(x+iy) = cos(z). (1.3.44)
Similarly,
d
dz
1
z

=
y2 −x2
(x2 + y2)2 +
2ixy
(x2 + y2)2 = −
1
(x + iy)2 = −1
z2 .
(1.3.45)
⊓⊔
The results in the above examples are identical to those for z real.
As we showed
earlier, the fundamental rules of elementary calculus apply to complex diﬀerentiation. Con-
sequently, it is usually simpler to apply those rules to ﬁnd the derivative rather than breaking
f(z) down into its real and imaginary parts, applying either Equation 1.3.28 or Equation
1.3.29, and then putting everything back together.

Complex Variables
15
An additional property of analytic functions follows by cross diﬀerentiating the Cauchy-
Riemann equations, or
∂2u
∂x2 = ∂2v
∂x∂y = −∂2u
∂y2 ,
or
∂2u
∂x2 + ∂2u
∂y2 = 0,
(1.3.46)
and
∂2v
∂x2 = −∂2u
∂x∂y = −∂2v
∂y2 ,
or
∂2v
∂x2 + ∂2v
∂y2 = 0.
(1.3.47)
Any function that has continuous partial derivatives of second order and satisﬁes Laplace’s
equation, Equation 1.3.46 or Equation 1.3.47, is called a harmonic function. Because both
u(x, y) and v(x, y) satisfy Laplace’s equation if f(z) = u + iv is analytic, u(x, y) and v(x, y)
are called conjugate harmonic functions.
• Example 1.3.8
Given that u(x, y) = e−x[x sin(y) −y cos(y)], let us show that u is harmonic and ﬁnd
a conjugate harmonic function v(x, y) such that f(z) = u + iv is analytic.
Because
∂2u
∂x2 = −2e−x sin(y) + xe−x sin(y) −ye−x cos(y),
(1.3.48)
and
∂2u
∂y2 = −xe−x sin(y) + 2e−x sin(y) + ye−x cos(y),
(1.3.49)
it follows that uxx + uyy = 0. Therefore, u(x, y) is harmonic. From the Cauchy-Riemann
equations,
∂v
∂y = ∂u
∂x = e−x sin(y) −xe−x sin(y) + ye−x cos(y),
(1.3.50)
and
∂v
∂x = −∂u
∂y = e−x cos(y) −xe−x cos(y) −ye−x sin(y).
(1.3.51)
Integrating Equation 1.3.50 with respect to y,
v(x, y) = ye−x sin(y) + xe−x cos(y) + g(x).
(1.3.52)
Using Equation 1.3.51,
vx = −ye−x sin(y) −xe−x cos(y) + e−x cos(y) + g′(x)
= e−x cos(y) −xe−x cos(y) −ye−x sin(x).
(1.3.53
Therefore, g′(x) = 0 or g(x) = constant. Consequently,
v(x, y) = e−x[y sin(y) + x cos(y)] + constant.
(1.3.54)
Hence, for our real harmonic function u(x, y), there are inﬁnitely many harmonic conjugates
v(x, y), which diﬀer from each other by an additive constant.

16
Advanced Engineering Mathematics: A Second Course
Problems
Show that the following functions are entire:
1. f(z) = iz + 2
2. f(z) = e−z
3. f(z) = z3
4. f(z) = cosh(z)
Find the derivative of the following functions:
5. f(z) = (1 + z2)3/2
6. f(z) = (z + 2z1/2)1/3
7. f(z) = (1 + 4i)z2 −3z −2
8. f(z) = (2z −i)/(z + 2i)
9.
f(z) = (iz −1)−3
10. f(z) = z/(z3 + 1)
Evaluate the following limits:
11.
lim
z→i
z2 −2iz −1
z4 + 2z2 + 1
12.
lim
z→0
z −sin(z)
z3
13.
lim
z→n
z −n
sin(πz)
Here, n is an integer.
14. Show that the function f(z) = z∗is nowhere diﬀerentiable.
For each of the following u(x, y), show that it is harmonic and then ﬁnd a corresponding
v(x, y) such that f(z) = u + iv is analytic.
15. u(x, y) = x2 −y2
16. u(x, y) = x4 −6x2y2 + y4 + x
17. u(x, y) = x cos(x)e−y −y sin(x)e−y
18. u(x, y) = (x2 −y2) cos(y)ex −2xy sin(y)ex
1.4 LINE INTEGRALS
So far, we discussed complex numbers, complex functions, and complex diﬀerentiation.
We are now ready for integration.
Just as we have integrals involving real variables, we can deﬁne an integral that involves
complex variables. Because the z-plane is two-dimensional, there is clearly greater freedom
in what we mean by a complex integral. For example, we might ask whether the integral of
some function between points A and B depends upon the curve along which we integrate.
(In general it does.) Consequently, an important ingredient in any complex integration is
the contour that we follow during the integration.
The result of a line integral is a complex number or expression. Unlike its counterpart
in real variables, there is no physical interpretation for this quantity, such as area under
a curve.
Generally, integration in the complex plane is an intermediate process with a
physically realizable quantity occurring only after we take its real or imaginary part. For
example, in potential ﬂuid ﬂow, the lift and drag are found by taking the real and imaginary
parts of a complex integral, respectively.
How do we compute
R
C f(z) dz? Let us deal with the deﬁnition; we illustrate the actual
method by examples.
A popular method for evaluating complex line integrals consists of breaking everything
up into real and imaginary parts. This reduces the integral to line integrals of real-valued
functions, which we know how to handle. Thus, we write f(z) = u(x, y) + iv(x, y) as usual,
and because z = x + iy, formally dz = dx + i dy. Therefore,
Z
C
f(z) dz =
Z
C
[u(x, y) + iv(x, y)][dx + i dy]
(1.4.1)
=
Z
C
u(x, y) dx −v(x, y) dy + i
Z
C
v(x, y) dx + u(x, y) dy.
(1.4.2)

Complex Variables
17
y
C
x
2
4
2
2a
C b
C
2
1
Figure 1.4.1: Contour used in Example 1.4.1.
The exact method used to evaluate Equation 1.4.2 depends upon the exact path speciﬁed.
From the deﬁnition of the line integral, we have the following self-evident properties:
Z
C
f(z) dz = −
Z
C′ f(z) dz,
(1.4.3)
where C′ is the contour C taken in the opposite direction of C and
Z
C1+C2
f(z) dz =
Z
C1
f(z) dz +
Z
C2
f(z) dz.
(1.4.4)
• Example 1.4.1
Let us evaluate
R
C z∗dz from z = 0 to z = 4+2i along two diﬀerent contours. The ﬁrst
consists of the parametric equation z = t2 + it. The second consists of two “dog legs”: the
ﬁrst leg runs along the imaginary axis from z = 0 to z = 2i and then along a line parallel
to the x-axis from z = 2i to z = 4 + 2i. See Figure 1.4.1.
For the ﬁrst case, the points z = 0 and z = 4 + 2i on C1 correspond to t = 0 and t = 2,
respectively. Then the line integral equals
Z
C1
z∗dz =
Z 2
0
(t2 + it)∗d(t2 + it) =
Z 2
0
(2t3 −it2 + t) dt = 10 −8i
3 .
(1.4.5)
The line integral for the second contour C2 equals
Z
C2
z∗dz =
Z
C2a
z∗dz +
Z
C2b
z∗dz,
(1.4.6)
where C2a denotes the integration from z = 0 to z = 2i while C2b denotes the integration
from z = 2i to z = 4 + 2i. For the ﬁrst integral,
Z
C2a
z∗dz =
Z
C2a
(x −iy)(dx + i dy) =
Z 2
0
y dy = 2,
(1.4.7)
because x = 0 and dx = 0 along C2a. On the other hand, along C2b, y = 2 and dy = 0 so
that
Z
C2b
z∗dz =
Z
C2b
(x −iy)(dx + i dy) =
Z 4
0
x dx + i
Z 4
0
−2 dx = 8 −8i.
(1.4.8)

18
Advanced Engineering Mathematics: A Second Course
C
1
C
C
y
1
x
2 b
2 a
1
2
Figure 1.4.2: Contour used in Example 1.4.2.
Thus the value of the entire C2 contour integral equals the sum of the two parts, or 10−8i.
The point here is that integration along two diﬀerent paths has given us diﬀerent results
even though we integrated from z = 0 to z = 4 + 2i both times. This result foreshadows
a general result that is extremely important. Because the integrand contains nonanalytic
points along and inside the region enclosed by our two curves, as shown by the Cauchy-
Riemann equations, the results depend upon the path taken. Since complex integrations
often involve integrands that have nonanalytic points, many line integrations depend upon
the contour taken.
⊓⊔
• Example 1.4.2
Let us integrate the entire function f(z) = z2 along the two paths from z = 0 to
z = 2 + i shown in Figure 1.4.2. For the ﬁrst integration, x = 2y, while along the second
path we have two straight paths: z = 0 to z = 2 and z = 2 to z = 2 + i.
For the ﬁrst contour integration,
Z
C1
z2dz =
Z 1
0
(2y + iy)2(2 dy + i dy) =
Z 1
0
(3y2 + 4y2i)(2 dy + i dy)
(1.4.9)
=
Z 1
0
6y2 dy + 8y2i dy + 3y2i dy −4y2 dy =
Z 1
0
2y2 dy + 11y2i dy
(1.4.10)
= 2
3y3|1
0 + 11
3 iy3|1
0 = 2
3 + 11i
3 .
(1.4.11)
For our second integration,
Z
C2
z2 dz =
Z
C2a
z2 dz +
Z
C2b
z2 dz.
(1.4.12)
Along C2a we ﬁnd that y = dy = 0 so that
Z
C2a
z2 dz =
Z 2
0
x2 dx = 1
3x3|2
0 = 8
3,
(1.4.13)
and
Z
C2b
z2 dz =
Z 1
0
(2 + iy)2i dy = i

4y + 2iy2 −y3
3

1
0
= 4i −2 −i
3,
(1.4.14)
because x = 2 and dx = 0. Consequently,
Z
C2
z2 dz = 2
3 + 11i
3 .
(1.4.15)

Complex Variables
19
y
x
(0,−1)
C
C
C 
2
1
3
(−1,0)
(0,1)
(1,0)
Figure 1.4.3: Contour used in Example 1.4.3.
In this problem we obtained the same results from two diﬀerent contours of integration.
Exploring other contours, we would ﬁnd that the results are always the same; the integration
is path independent. But what makes these results path independent while the integration
in Example 1.4.1 was not? Perhaps it is the fact that the integrand is analytic everywhere
on the complex plane and there are no nonanalytic points. We will explore this later.
⊓⊔
Finally, an important class of line integrals involves closed contours. We denote this
special subclass of line integrals by placing a circle on the integral sign:
H
. Consider now
the following examples:
• Example 1.4.3
Let us integrate f(z) = z around the closed contour shown in Figure 1.4.3.
From Figure 1.4.3,
I
C
z dz =
Z
C1
z dz +
Z
C2
z dz +
Z
C3
z dz.
(1.4.16)
Now
Z
C1
z dz =
Z 0
1
iy (i dy) = −
Z 0
1
y dy = −y2
2

0
1
= 1
2,
(1.4.17)
Z
C2
z dz =
Z −1
0
x dx = x2
2

−1
0
= 1
2,
(1.4.18)
and
Z
C3
z dz =
Z π/2
−π
eθiieθidθ = e2θi
2

π/2
−π
= −1,
(1.4.19)
where we used z = eθi around the portion of the unit circle. Therefore, the closed line
integral equals zero.
⊓⊔
• Example 1.4.4
Let us integrate f(z) = 1/(z −a) around any circle centered on z = a. The Cauchy-
Riemann equations show that f(z) is a meromorphic function. It is analytic everywhere
except at the isolated singularity z = a.

20
Advanced Engineering Mathematics: A Second Course
If we introduce polar coordinates by letting z −a = reθi and dz = ireθidθ,
I
C
dz
z −a =
Z 2π
0
ireθi
reθi dθ = i
Z 2π
0
dθ = 2πi.
(1.4.20)
Note that the integrand becomes undeﬁned at z = a. Furthermore, the answer is indepen-
dent of the size of the circle. Our example suggests that when we have a closed contour
integration, it is the behavior of the function within the contour rather than the exact shape
of the closed contour that is of importance. We will return to this point in later sections.
Problems
1. Evaluate
H
C(z∗)2 dz around the circle |z| = 1 taken in the counterclockwise direction.
2. Evaluate
H
C |z|2 dz around the square with vertices at (0,0), (1,0), (1,1), and (0,1) taken
in the counterclockwise direction.
3. Evaluate
R
C |z| dz along the right half of the circle |z| = 1 from z = −i to z = i.
4. Evaluate
R
C ez dz along the line y = x from (−1, −1) to (1, 1).
5. Evaluate
R
C(z∗)2 dz along the line y = x2 from (0, 0) to (1, 1).
6. Evaluate
R
C z−1/2 dz, where C is (a) the upper semicircle |z| = 1 and (b) the lower semi-
circle |z| = 1. If z = reθi, restrict −π < θ < π. Take both contours in the counterclockwise
direction.
1.5 THE CAUCHY-GOURSAT THEOREM
In the previous section we showed how to evaluate line integrations by brute-force
reduction to real-valued integrals. In general, this direct approach is quite diﬃcult and we
would like to apply some of the deeper properties of complex analysis to work smarter. In
the remaining portions of this chapter, we introduce several theorems that will do just that.
If we scan over the examples worked in the previous section, we see considerable diﬀer-
ences when the function was analytic inside and on the contour and when it was not. We
may formalize this anecdotal evidence into the following theorem:
Cauchy-Goursat theorem:2 Let f(z) be analytic in a domain D and let C be a simple
Jordan curve3 inside D so that f(z) is analytic on and inside of C. Then
H
C f(z) dz = 0.
Proof : Let C denote the contour around which we will integrate w = f(z). We divide the
region within C into a series of inﬁnitesimal rectangles. See Figure 1.5.1. The integration
2 Goursat, E., 1900: Sur la d´eﬁnition g´en´erale des fonctions analytiques, d’apr`es Cauchy. Trans. Am.
Math. Soc., 1, 14–16.
3 A Jordan curve is a simply closed curve. It looks like a closed loop that does not cross itself. See
Figure 1.5.2.

Complex Variables
21
C
x
y
Figure 1.5.1: Diagram used in proving the Cauchy-Goursat theorem.
around each rectangle equals the product of the average value of w on each side and its
length,

w + ∂w
∂x
dx
2

dx +

w + ∂w
∂x dx + ∂w
∂(iy)
d(iy)
2

d(iy)
+

w + ∂w
∂x
dx
2 + ∂w
∂(iy) d(iy)

(−dx) +

w + ∂w
∂(iy)
d(iy)
2

d(−iy)
=
∂w
∂x −∂w
i∂y

(i dx dy).
(1.5.1)
Substituting w = u + iv into Equation 1.5.1,
∂w
∂x −∂w
i ∂y =
∂u
∂x −∂v
∂y

+ i
∂v
∂x + ∂u
∂y

.
(1.5.2)
Because the function is analytic, the right side of Equation 1.5.1 and Equation 1.5.2 equals
zero. Thus, the integration around each of these rectangles also equals zero.
We note next that in integrating around adjoining rectangles, we transverse each side
in opposite directions, the net result being equivalent to integrating around the outer curve
C. We therefore arrive at the result
H
C f(z) dz = 0, where f(z) is analytic within and on
the closed contour.
⊓⊔
The Cauchy-Goursat theorem has several useful implications. Suppose that we have a
domain where f(z) is analytic. Within this domain, let us evaluate a line integral from point
A to B along two diﬀerent contours C1 and C2. Then, the integral around the closed contour
formed by integrating along C1 and then back along C2, only in the opposite direction, is
I
C
f(z) dz =
Z
C1
f(z) dz −
Z
C2
f(z) dz = 0
(1.5.3)
or
Z
C1
f(z) dz =
Z
C2
f(z) dz.
(1.5.4)

22
Advanced Engineering Mathematics: A Second Course
x
y
(a)
(b)
Figure 1.5.2: Examples of a (a) simply closed curve and (b) not simply closed curve.
Because C1 and C2 are completely arbitrary, we have the result that if, in a domain, f(z)
is analytic, the integral between any two points within the domain is path independent.
One obvious advantage of path independence is the ability to choose the contour so that
the computations are made easier. This obvious choice immediately leads to the following
principle:
The principle of deformation of contours: The value of a line integral of an analytic
function around any simple closed contour remains unchanged if we deform the contour in
such a manner that we do not pass over a nonanalytic point.
⊓⊔
• Example 1.5.1
Let us integrate f(z) = z−1 around the closed contour C in the counterclockwise
direction. This contour consists of a square, centered on the origin, with vertices at (1, 1),
(1, −1), (−1, 1), and (−1, −1).
The direct integration of
H
C z−1dz around the original contour is very cumbersome.
However, because the integrand is analytic everywhere except at the origin, we may deform
the origin contour into a circle of radius r, centered on the origin. Then, z = reθi and
dz = rieθidθ so that
I
C
dz
z =
Z 2π
0
rieθi
reθi dθ = i
Z 2π
0
dθ = 2πi.
(1.5.5)
The point here is that no matter how bizarre the contour is, as long as it encircles the origin
and is a simply closed contour, we can deform it into a circle and we get the same answer
for the contour integral. This suggests that it is not the shape of the closed contour that
makes the diﬀerence but whether we enclose any singularities (points where f(z) becomes
undeﬁned) that matters. We shall return to this idea many times in the next few sections.⊓⊔
Finally, suppose that we have a function f(z) such that f(z) is analytic in some domain.
Furthermore, let us introduce the analytic function F(z) such that f(z) = F ′(z). We would
like to evaluate
R b
a f(z) dz in terms of F(z).
We begin by noting that we can represent F, f as F(z) = U + iV and f(z) = u + iv.
From Example 1.3.28 we have that u = Ux and v = Vx. Therefore,
Z b
a
f(z) dz =
Z b
a
(u + iv)(dx + i dy) =
Z b
a
Ux dx −Vx dy + i
Z b
a
Vx dx + Ux dy
(1.5.6)

Complex Variables
23
=
Z b
a
Ux dx + Uy dy + i
Z b
a
Vx dx + Vy dy =
Z b
a
dU + i
Z b
a
dV = F(b) −F(a)
(1.5.7)
or
Z b
a
f(z) dz = F(b) −F(a).
(1.5.8)
Equation 1.5.8 is the complex variable form of the fundamental theorem of calculus. Thus,
if we can ﬁnd the antiderivative of a function f(z) that is analytic within a speciﬁc region,
we can evaluate the integral by evaluating the antiderivative at the endpoints for any curves
within that region.
• Example 1.5.2
Let us evaluate
R πi
0 z sin(z2) dz.
The integrand f(z) = z sin(z2) is an entire function and its antiderivative equals
−1
2 cos(z2). Therefore,
Z πi
0
z sin(z2) dz = −1
2 cos(z2)
πi
0 = 1
2[cos(0) −cos(−π2)] = 1
2[1 −cos(π2)].
(1.5.9)
Problems
For the following integrals, show that they are path independent and determine the value
of the integral:
1.
Z 2+3πi
1−πi
e−2z dz
2.
Z 2π
0
[ez −cos(z)] dz
3.
Z π
0
sin2(z) dz
4.
Z 2i
−i
(z + 1) dz
5.
Z 2+2i
1
(z2 −z + 8) dz 6.
Z 2i
1
[(1 −i)z2 + 2iz −4] dz
7.
Z i
0
z2 cos(z3) dz
8.
Z 1+i
i
ze−z2 dz
1.6 CAUCHY’S INTEGRAL FORMULA
In the previous section, our examples suggested that the presence of a singularity
within a contour really determines the value of a closed contour integral. Continuing with
this idea, let us consider a class of closed contour integrals that explicitly contains a single
singularity within the contour, namely
H
C g(z) dz, where g(z) = f(z)/(z −z0), and f(z) is
analytic within and on the contour C. We closed the contour in the positive sense where
the enclosed area lies to your left as you move along the contour.
We begin by examining a closed contour integral where the closed contour consists of
the C1, C2, C3, and C4 as shown in Figure 1.6.1. The gap or cut between C2 and C4 is very
small. Because g(z) is analytic within and on the closed integral, we have that
Z
C1
f(z)
z −z0
dz +
Z
C2
f(z)
z −z0
dz +
Z
C3
f(z)
z −z0
dz +
Z
C4
f(z)
z −z0
dz = 0.
(1.6.1)
It can be shown that the contribution to the integral from the path C2 going into the
singularity cancels the contribution from the path C4 going away from the singularity as
the gap between them vanishes. Because f(z) is analytic at z0, we can approximate its

24
Advanced Engineering Mathematics: A Second Course
2
C1
x
y
C
C
C
3
4
Figure 1.6.1: Diagram used to prove Cauchy’s integral formula.
value on C3 by f(z) = f(z0) + δ(z), where δ is a small quantity. Substituting into Equation
1.6.1,
I
C1
f(z)
z −z0
dz = −f(z0)
Z
C3
1
z −z0
dz −
Z
C3
δ(z)
z −z0
dz.
(1.6.2)
Consequently, as the gap between C2 and C4 vanishes, the contour C1 becomes the closed
contour C so that Equation 1.6.2 may be written
I
C
f(z)
z −z0
dz = 2πif(z0) + i
Z 2π
0
δ dθ,
(1.6.3)
where we set z −z0 = ǫeθi and dz = iǫeθidθ.
Let M denote the value of the integral on the right side of Equation 1.6.3 and ∆equal
the greatest value of the modulus of δ along the circle. Then
|M| <
Z 2π
0
|δ| dθ ≤
Z 2π
0
∆dθ = 2π∆.
(1.6.4)
As the radius of the circle diminishes to zero, ∆also diminishes to zero. Therefore, |M|,
which is positive, becomes less than any ﬁnite quantity, however small, and M itself equals
zero. Thus, we have that
f(z0) =
1
2πi
I
C
f(z)
z −z0
dz.
(1.6.5)
This equation is Cauchy’s integral formula. By taking n derivatives of Equation 1.6.5, we
can extend Cauchy’s integral formula4 to
f (n)(z0) = n!
2πi
I
C
f(z)
(z −z0)n+1 dz
(1.6.6)
4 See Carrier, G. F., M. Krook, and C. E. Pearson, 1966: Functions of a Complex Variable: Theory
and Technique. McGraw-Hill, pp. 39–40 for the proof.

Complex Variables
25
for n = 1, 2, 3, . . .. For computing integrals, it is convenient to rewrite Equation 1.6.6 as
I
C
f(z)
(z −z0)n+1 dz = 2πi
n! f (n)(z0).
(1.6.7)
• Example 1.6.1
Let us ﬁnd the value of the integral
I
C
cos(πz)
(z −1)(z −2) dz,
(1.6.8)
where C is the circle |z| = 5. Using partial fractions,
1
(z −1)(z −2) =
1
z −2 −
1
z −1,
(1.6.9)
and
I
C
cos(πz)
(z −1)(z −2) dz =
I
C
cos(πz)
z −2 dz −
I
C
cos(πz)
z −1 dz.
(1.6.10)
By Cauchy’s integral formula with z0 = 2 and z0 = 1,
I
C
cos(πz)
z −2 dz = 2πi cos(2π) = 2πi,
(1.6.11)
and
I
C
cos(πz)
z −1 dz = 2πi cos(π) = −2πi,
(1.6.12)
because z0 = 1 and z0 = 2 lie inside C and cos(πz) is analytic there. Thus the required
integral has the value
I
C
cos(πz)
(z −1)(z −2) dz = 4πi.
(1.6.13)
⊓⊔
• Example 1.6.2
Let us use Cauchy’s integral formula to evaluate
I =
I
|z|=2
ez
(z −1)2(z −3) dz.
(1.6.14)
We need to convert Equation 1.6.14 into the form Equation 1.6.7. To do this, we rewrite
Equation 1.6.14 as
I
|z|=2
ez
(z −1)2(z −3) dz =
I
|z|=2
ez/(z −3)
(z −1)2
dz.
(1.6.15)

26
Advanced Engineering Mathematics: A Second Course
Therefore, f(z) = ez/(z −3), n = 1, and z0 = 1. The function f(z) is analytic within
the closed contour because the point z = 3 lies outside of the contour. Applying Cauchy’s
integral formula,
I
|z|=2
ez
(z −1)2(z −3) dz = 2πi
1!
d
dz
 ez
z −3

z=1
= 2πi
 ez
z −3 −
ez
(z −3)2

z=1
= −3πie
2
.
(1.6.16)
Project: Computing Derivatives of Any Order of a Complex or Real Function
The most common technique for computing a derivative is ﬁnite diﬀerencing. Recently
Mahajerin and Burgess5 showed how Cauchy’s integral formula can be used to compute the
derivatives of any order of a complex or real function via numerical quadrature. In this
project you will derive the algorithm, write code implementing it, and ﬁnally test it.
Step 1: Consider the complex function f(z) = u + iv, which is analytic inside the closed
circular contour C of radius R centered at z0. Using Cauchy’s integral formula, show that
f (n)(z0) =
n!
2πRn
Z 2π
0
[u(x, y) + iv(x, y)][cos(nθ) −i sin(nθ)] dθ,
where x = x0 + R cos(θ), and y = y0 + R sin(θ).
Step 2: Using ﬁve-point Gaussian quadrature, write code to implement the results from
Step 1.
Step 3: Test out this scheme by ﬁnding the ﬁrst, sixth, and eleventh derivative of f(x) =
8x/(x2 + 4) for x = 2. The exact answers are 0, 2.8125, and 1218.164, respectively. What
is the maximum value of R? How does the accuracy vary with the number of subdivisions
used in the numerical integration? Is the algorithm sensitive to the value of R and the
number of subdivisions? For a ﬁxed number of subdivisions, is there an optimal R?
Problems
Use Cauchy’s integral formula to evaluate the following integrals. Assume all of the contours
are in the positive sense.
1.
I
|z|=1
sin6(z)
z −π/6 dz
2.
I
|z|=1
sin6(z)
(z −π/6)3 dz
3.
I
|z|=1
1
z(z2 + 4) dz
4.
I
|z|=1
tan(z)
z
dz
5.
I
|z−1|=1/2
1
(z −1)(z −2) dz
6.
I
|z|=5
exp(z2)
z3
dz
7.
I
|z−1|=1
z2 + 1
z2 −1 dz
8.
I
|z|=2
z2
(z −1)4 dz
9.
I
|z|=2
z3
(z + i)3 dz
10.
I
|z|=1
cos(z)
z2n+1 dz
11.
I
|z|=1
z2 + 3z −1
z(z2 −3) dz
12.
I
|z|=3
iez
(z −2 + i)4 dz
5 Mahajerin, E., and G. Burgess, 1993: An algorithm for computing derivatives of any order of a complex
or real function. Computers & Struct., 49, 385–387.

Complex Variables
27
1.7 TAYLOR AND LAURENT EXPANSIONS AND SINGULARITIES
In the previous section we showed what a crucial role singularities play in complex
integration. Before we can ﬁnd the most general way of computing a closed complex integral,
our understanding of singularities must deepen. For this, we employ power series.
One reason why power series are so important is their ability to provide locally a general
representation of a function even when its arguments are complex. For example, when we
were introduced to trigonometric functions in high school, it was in the context of a right
triangle and a real angle. However, when the argument becomes complex, this geometrical
description disappears and power series provide a formalism for deﬁning the trigonometric
functions, regardless of the nature of the argument.
Let us begin our analysis by considering the complex function f(z), which is analytic
everywhere on the boundary, and the interior of a circle whose center is at z = z0. Then, if
z denotes any point within the circle, we have from Cauchy’s integral formula that
f(z) =
1
2πi
I
C
f(ζ)
ζ −z dζ =
1
2πi
I
C
f(ζ)
ζ −z0

1
1 −(z −z0)/(ζ −z0)

dζ,
(1.7.1)
where C denotes the closed contour. Expanding the bracketed term as a geometric series,
we ﬁnd that
f(z) =
1
2πi
I
C
f(ζ)
ζ −z0
dζ +(z−z0)
I
C
f(ζ)
(ζ −z0)2 dζ +· · ·+(z−z0)n
I
C
f(ζ)
(ζ −z0)n+1 dζ +· · ·

.
(1.7.2)
Applying Cauchy’s integral formula to each integral in Equation 1.7.2, we ﬁnally obtain
f(z) = f(z0) + (z −z0)
1!
f ′(z0) + · · · + (z −z0)n
n!
f (n)(z0) + · · ·
(1.7.3)
or the familiar formula for a Taylor expansion. Consequently, we can expand any analytic
function into a Taylor series. Interestingly, the radius of convergence6 of this series may be
shown to be the distance between z0 and the nearest nonanalytic point of f(z).
• Example 1.7.1
Let us ﬁnd the expansion of f(z) = sin(z) about the point z0 = 0.
Because f(z) is an entire function, we can construct a Taylor expansion anywhere on
the complex plane. For z0 = 0,
f(z) = f(0) + 1
1!f ′(0)z + 1
2!f ′′(0)z2 + 1
3!f ′′′(0)z3 + · · · .
(1.7.4)
Because f(0) = 0, f ′(0) = 1, f ′′(0) = 0, f ′′′(0) = −1 and so forth,
f(z) = z −z3
3! + z5
5! −z7
7! + · · · .
(1.7.5)
Because sin(z) is an entire function, the radius of convergence is |z −0| < ∞, i.e., all z. ⊓⊔
6 A positive number h such that the series diverges for |z −z0| > h but converges absolutely for
|z −z0| < h.

28
Advanced Engineering Mathematics: A Second Course
0
C
C
C
z
y
x
2
1
z
Figure 1.7.1: Contour used in deriving the Laurent expansion.
• Example 1.7.2
Let us ﬁnd the expansion of f(z) = 1/(1 −z) about the point z0 = 0.
From the formula for a Taylor expansion,
f(z) = f(0) + 1
1!f ′(0)z + 1
2!f ′′(0)z2 + 1
3!f ′′′(0)z3 + · · · .
(1.7.6)
Because f (n)(0) = n!, we ﬁnd that
f(z) = 1 + z + z2 + z3 + z4 + · · · =
1
1 −z .
(1.7.7)
Equation 1.7.7 is the familiar result for a geometric series. Because the only nonanalytic
point is at z = 1, the radius of convergence is |z −0| < 1, the unit circle centered at z = 0.⊓⊔
Consider now the situation where we draw two concentric circles about some arbitrary
point z0; we denote the outer circle by C while we denote the inner circle by C1.
See
Figure 1.7.1. Let us assume that f(z) is analytic inside the annulus between the two circles.
Outside of this area, the function may or may not be analytic. Within the annulus we pick
a point z and construct a small circle around it, denoting the circle by C2. As the gap or
cut in the annulus becomes inﬁnitesimally small, the line integrals that connect the circle
C2 to C1 and C sum to zero, leaving
I
C
f(ζ)
ζ −z dζ =
I
C1
f(ζ)
ζ −z dζ +
I
C2
f(ζ)
ζ −z dζ.
(1.7.8)
Because f(ζ) is analytic everywhere within C2,
2πif(z) =
I
C2
f(ζ)
ζ −z dζ.
(1.7.9)
Using the relationship:
I
C1
f(ζ)
ζ −z dζ = −
I
C1
f(ζ)
z −ζ dζ,
(1.7.10)

Complex Variables
29
Equation 1.7.8 becomes
f(z) =
1
2πi
I
C
f(ζ)
ζ −z dζ +
1
2πi
I
C1
f(ζ)
z −ζ dζ.
(1.7.11)
Now,
1
ζ −z =
1
ζ −z0 −z + z0
=
1
ζ −z0
1
1 −(z −z0)/(ζ −z0)
(1.7.12)
=
1
ζ −z0
"
1 +
z −z0
ζ −z0

+
z −z0
ζ −z0
2
+ · · · +
z −z0
ζ −z0
n
+ · · ·
#
,
(1.7.13)
where |z −z0|/|ζ −z0| < 1 and
1
z −ζ =
1
z −z0 −ζ + z0
=
1
z −z0
1
1 −(ζ −z0)/(z −z0)
(1.7.14)
=
1
z −z0
"
1 +
ζ −z0
z −z0

+
ζ −z0
z −z0
2
+ · · · +
ζ −z0
z −z0
n
+ · · ·
#
,
(1.7.15)
where |ζ −z0|/|z −z0| < 1. Upon substituting these expressions into Equation 1.7.11,
f(z) =
 1
2πi
I
C
f(ζ)
ζ −z0
dζ + z −z0
2πi
I
C
f(ζ)
(ζ −z0)2 dζ + · · ·
+ (z −z0)n
2πi
I
C
f(ζ)
(ζ −z0)n+1 dζ + · · ·

+

1
z −z0
1
2πi
I
C1
f(ζ) dζ +
1
(z −z0)2
1
2πi
I
C1
f(ζ)(ζ −z0) dζ + · · ·
+
1
(z −z0)n
1
2πi
I
C1
f(ζ)(ζ −z0)n−1 dζ + · · ·

(1.7.16)
or
f(z) =
a1
z −z0
+
a2
(z −z0)2 + · · · +
an
(z −z0)n + · · · + b0 + b1(z −z0) + · · · + bn(z −z0)n + · · · .
(1.7.17)
Equation 1.7.17 is a Laurent expansion.7 If f(z) is analytic at z0, then a1 = a2 = · · · = an =
· · · = 0 and the Laurent expansion reduces to a Taylor expansion. If z0 is a singularity of
f(z), then the Laurent expansion includes both positive and negative powers. The coeﬃcient
of the (z −z0)−1 term, a1, is the residue, for reasons that will appear in the next section.
Unlike the Taylor series, a Laurent series provides no straightforward method for ob-
taining the coeﬃcients. For the remaining portions of this section we illustrate their con-
struction. These techniques include replacing a function by its appropriate power series,
the use of geometric series to expand the denominator, and the use of algebraic tricks to
assist in applying the ﬁrst two methods.
7 Laurent, M., 1843: Extension du th´eor`eme de M. Cauchy relatif `a la convergence du d´eveloppement
d’une fonction suivant les puissances ascendantes de la variable x. C. R. l’Acad. Sci., 17, 938–942.

30
Advanced Engineering Mathematics: A Second Course
• Example 1.7.3
Laurent expansions provide a formalism for the classiﬁcation of singularities of a func-
tion. Isolated singularities fall into three types; they are as follows:
• Essential Singularity: Consider the function f(z) = cos(1/z). Using the expansion for
cosine,
cos
1
z

= 1 −
1
2!z2 +
1
4!z4 −
1
6!z6 + · · ·
(1.7.18)
for 0 < |z| < ∞. Note that this series never truncates in the inverse powers of z. Essential
singularities have Laurent expansions, which have an inﬁnite number of inverse powers of
z −z0. The value of the residue for this essential singularity at z = 0 is zero.
• Removable Singularity: Consider the function f(z) = sin(z)/z.
This function has a
singularity at z = 0. Upon applying the expansion for sine,
sin(z)
z
= 1
z

z −z3
3! + z5
5! −z7
7! + z9
9! −. . .

= 1 −z2
3! + z4
5! −z6
7! + z8
9! −. . .
(1.7.19)
for all z, if the division is permissible. We made f(z) analytic by deﬁning it by Equation
1.7.19 and, in the process, removed the singularity. The residue for a removable singularity
always equals zero.
• Pole of order n: Consider the function
f(z) =
1
(z −1)3(z + 1).
(1.7.20)
This function has two singularities: one at z = 1 and the other at z = −1. We shall only
consider the case z = 1. After a little algebra,
f(z) =
1
(z −1)3
1
2 + (z −1) = 1
2
1
(z −1)3
1
1 + (z −1)/2
(1.7.21)
= 1
2
1
(z −1)3

1 −z −1
2
+ (z −1)2
4
−(z −1)3
8
+ · · ·

(1.7.22)
=
1
2(z −1)3 −
1
4(z −1)2 +
1
8(z −1) −1
16 + · · ·
(1.7.23)
for 0 < |z −1| < 2. Because the largest inverse (negative) power is three, the singularity
at z = 1 is a third-order pole; the value of the residue is 1/8. Generally, we refer to a
ﬁrst-order pole as a simple pole.
⊓⊔
• Example 1.7.4
Let us ﬁnd the Laurent expansion for
f(z) =
z
(z −1)(z −3)
(1.7.24)
about the point z = 1.

Complex Variables
31
We begin by rewriting f(z) as
f(z) =
1 + (z −1)
(z −1)[−2 + (z −1)] = −1
2
1 + (z −1)
(z −1)[1 −1
2(z −1)]
(1.7.25)
= −1
2
1 + (z −1)
(z −1)
[1 + 1
2(z −1) + 1
4(z −1)2 + · · ·]
(1.7.26)
= −1
2
1
z −1 −3
4 −3
8 (z −1) −3
16 (z −1)2 −· · ·
(1.7.27)
provided 0 < |z −1| < 2. Therefore we have a simple pole at z = 1 and the value of the
residue is −1/2. A similar procedure would yield the Laurent expansion about z = 3.
⊓⊔
• Example 1.7.5
Let us ﬁnd the Laurent expansion for
f(z) =
zn + z−n
z2 −2z cosh(α) + 1,
α > 0,
n ≥0,
(1.7.28)
about the point z = 0.
We begin by rewriting f(z) as
f(z) =
zn + z−n
(z −eα)(z −e−α) =
1
2 sinh(α)
zn + z−n
z −eα
−zn + z−n
z −e−α

.
(1.7.29)
Because
1
z −eα = −
e−α
1 −ze−α = −e−α  1 + ze−α + z2e−2α + · · ·

(1.7.30)
if |z| < eα and
1
z −e−α = −
eα
1 −zeα = −eα  1 + zeα + z2e2α + · · ·

(1.7.31)
if |z| < e−α,
f(z) =
eα
2 sinh(α)
 zn + zn+1eα + zn+2e2α + · · · + z−n + z1−neα + z2−ne2α + · · ·

(1.7.32)
−
e−α
2 sinh(α)
 zn + zn+1e−α + zn+2e−2α + · · · + z−n + z1−ne−α + z2−ne−2α + · · ·

,
if |z| < e−α. Clearly we have an nth-order pole at z = 0. The residue, the coeﬃcient of all
of the z−1 terms in Equation 1.7.32, is found directly and equals
Res[f(z); 0] = sinh(nα)
sinh(α) .
(1.7.33)
⊓⊔
For complicated complex functions, it is very diﬃcult to determine the nature of the
singularities by ﬁnding the complete Laurent expansion, and we must try another method.
We shall call it “a poor man’s Laurent expansion.” The idea behind this method is the
fact that we generally need only the ﬁrst few terms of the Laurent expansion to discover

32
Advanced Engineering Mathematics: A Second Course
its nature. Consequently, we compute these terms through the application of power series
where we retain only the leading terms. Consider the following example.
• Example 1.7.6
Let us discover the nature of the singularity at z = 0 of the function
f(z) =
etz
z sinh(az),
(1.7.34)
where a and t are real.
We begin by replacing the exponential and hyperbolic sine by their Taylor expansion
about z = 0. Then
f(z) = 1 + tz + t2z2/2 + · · ·
z(az + a3z3/6 + · · ·) .
(1.7.35)
Factoring out az in the denominator,
f(z) = 1 + tz + t2z2/2 + · · ·
az2(1 + a2z2/6 + · · ·).
(1.7.36)
Within the parentheses, all of the terms except the leading one are small. Therefore, by
long division, we formally have that
f(z) =
1
az2 (1 + tz + t2z2/2 + · · ·)(1 −a2z2/6 + · · ·)
(1.7.37)
=
1
az2 (1 + tz + t2z2/2 −a2z2/6 + · · ·) =
1
az2 + t
az + 3t2 −a2
6a
+ · · · .
(1.7.38)
Thus, we have a second-order pole at z = 0 and the residue equals t/a.
Problems
1. Find the Taylor expansion of f(z) = (1 −z)−2 about the point z = 0.
2. Find the Taylor expansion of f(z) = (z −1)ez about the point z = 1. (Hint: Don’t ﬁnd
the expansion by taking derivatives.)
By constructing a Laurent expansion, describe the type of singularity and give the residue
at z0 for each of the following functions:
3. f(z) = z10e−1/z;
z0 = 0
4. f(z) = z−3 sin2(z);
z0 = 0
5. f(z) = cosh(z) −1
z2
;
z0 = 0
6. f(z) =
z
(z + 2)2 ;
z0 = −2
7. f(z) = ez + 1
e−z −1;
z0 = 0
8. f(z) =
eiz
z2 + b2 ;
z0 = bi
9. f(z) =
1
z(z −2);
z0 = 2
10. f(z) = exp(z2)
z4
;
z0 = 0

Complex Variables
33
1
x
y
C
n
2
1
z
z
z
Cn
C
C   
2
Figure 1.8.1: Contour used in deriving the residue theorem.
1.8 THEORY OF RESIDUES
Having shown that around any singularity we may construct a Laurent expansion,
we now use this result in the integration of closed complex integrals. Consider a closed
contour in which the function f(z) has a number of isolated singularities. As we did in the
case of Cauchy’s integral formula, we introduce a new contour C′ that excludes all of the
singularities because they are isolated. See Figure 1.8.1. Therefore,
I
C
f(z) dz −
I
C1
f(z) dz −· · · −
I
Cn
f(z) dz =
I
C′ f(z) dz = 0.
(1.8.1)
Consider now the mth integral, where 1 ≤m ≤n. Constructing a Laurent expansion for
the function f(z) at the isolated singularity z = zm, this integral equals
I
Cm
f(z) dz =
∞
X
k=1
ak
I
Cm
1
(z −zm)k dz +
∞
X
k=0
bk
I
Cm
(z −zm)k dz.
(1.8.2)
Because (z −zm)k is an entire function if k ≥0, the integrals equal zero for each term in
the second summation. We use Cauchy’s integral formula to evaluate the remaining terms.
The analytic function in the numerator is 1. Because dk−1(1)/dzk−1 = 0 if k > 1, all of
the terms vanish except for k = 1. In that case, the integral equals 2πia1, where a1 is the
value of the residue for that particular singularity. Applying this approach to each of the
singularities, we obtain the following:
Cauchy’s residue theorem:8 If f(z) is analytic inside and on a closed contour C (taken
in the positive sense) except at points z1, z2, . . ., zn where f(z) has singularities, then
I
C
f(z) dz = 2πi
n
X
j=1
Res[f(z); zj],
(1.8.3)
8 See Mitrinovi´c, D. S., and J. D. Ke˘cki´c, 1984: The Cauchy Method of Residues: Theory and Ap-
plications.
D. Reidel Publishing, 361 pp.
Section 10.3 gives the historical development of the residue
theorem.

34
Advanced Engineering Mathematics: A Second Course
where Res[f(z); zj] denotes the residue of the jth isolated singularity of f(z) located at
z = zj.
⊓⊔
• Example 1.8.1
Let us compute
H
|z|=2 z2/(z + 1) dz by the residue theorem, assuming that we take the
contour in the positive sense.
Because the contour is a circle of radius 2, centered on the origin, the singularity at
z = −1 lies within the contour. If the singularity were not inside the contour, then the
integrand would have been analytic inside and on the contour C. In this case, the answer
would then be zero by the Cauchy-Goursat theorem.
Returning to the original problem, we construct the Laurent expansion for the integrand
around the point z = 1 by noting that
z2
z + 1 = [(z + 1) −1]2
z + 1
=
1
z + 1 −2 + (z + 1).
(1.8.4)
The singularity at z = −1 is a simple pole and by inspection, the value of the residue equals
1. Therefore,
I
|z|=2
z2
z + 1 dz = 2πi.
(1.8.5)
⊓⊔
As it presently stands, it would appear that we must always construct a Laurent expan-
sion for each singularity if we wish to use the residue theorem. This becomes increasingly
diﬃcult as the structure of the integrand becomes more complicated.
In the following
paragraphs we show several techniques that avoid this problem in practice.
We begin by noting that many functions which we will encounter consist of the ratio of
two polynomials, i.e., rational functions: f(z) = g(z)/h(z). Generally, we can write h(z) as
(z −z1)m1(z −z2)m2 · · ·. Here we assumed that we divided out any common factors between
g(z) and h(z) so that g(z) does not vanish at z1, z2, . . .. Clearly z1, z2, . . ., are singularities
of f(z). Further analysis shows that the nature of the singularities are a pole of order m1
at z = z1, a pole of order m2 at z = z2, and so forth.
Having found the nature and location of the singularity, we compute the residue as
follows. Suppose that we have a pole of order n. Then we know that its Laurent expansion
is
f(z) =
an
(z −z0)n +
an−1
(z −z0)n−1 + · · · + b0 + b1(z −z0) + · · · .
(1.8.6)
Multiplying both sides of Equation 1.8.6 by (z −z0)n,
F(z) = (z −z0)nf(z) = an + an−1(z −z0) + · · · + b0(z −z0)n + b1(z −z0)n+1 + · · · . (1.8.7)
Because F(z) is analytic at z = z0, it has the Taylor expansion
F(z) = F(z0) + F ′(z0)(z −z0) + · · · + F (n−1)(z0)
(n −1)! (z −z0)n−1 + · · · .
(1.8.8)
Matching powers of z −z0 in Equation 1.8.7 and Equation 1.8.8, the residue equals
Res[f(z); z0] = a1 = F (n−1)(z0)
(n −1)! .
(1.8.9)

Complex Variables
35
Substituting in F(z) = (z −z0)nf(z), we can compute the residue of a pole of order n by
Res[f(z); zj] =
1
(n −1)! lim
z→zj
dn−1
dzn−1

(z −zj)nf(z)

.
(1.8.10)
For a simple pole, Equation 1.8.10 simpliﬁes to
Res[f(z); zj] = lim
z→zj(z −zj)f(z).
(1.8.11)
Quite often, f(z) = p(z)/q(z). From l’Hˆopital’s rule, it follows that Equation 1.8.11 be-
comes
Res[f(z); zj] = p(zj)
q′(zj).
(1.8.12)
Recall that these formulas work only for ﬁnite-order poles. For an essential singularity we
must compute the residue from its Laurent expansion; however, essential singularities are
very rare in applications.
• Example 1.8.2
Let us evaluate
I
C
eiz
z2 + a2 dz,
(1.8.13)
where C is any contour that includes both poles at z = ±ai and is in the positive sense.
From Cauchy’s residue theorem,
I
C
eiz
z2 + a2 dz = 2πi

Res

eiz
z2 + a2 ; ai

+ Res

eiz
z2 + a2 ; −ai

.
(1.8.14)
The singularities at z = ±ai are simple poles. The corresponding residues are
Res

eiz
z2 + a2 ; ai

= lim
z→ai(z −ai)
eiz
(z −ai)(z + ai) = e−a
2ia
(1.8.15)
and
Res

eiz
z2 + a2 ; −ai

=
lim
z→−ai(z + ai)
eiz
(z −ai)(z + ai) = −ea
2ia.
(1.8.16)
Consequently,
I
C
eiz
z2 + a2 dz = −2π
2a
 ea −e−a
= −2π
a sinh(a).
(1.8.17)
⊓⊔

36
Advanced Engineering Mathematics: A Second Course
• Example 1.8.3
Let us evaluate
1
2πi
I
C
etz
z2(z2 + 2z + 2) dz,
(1.8.18)
where C includes all of the singularities and is in the positive sense.
The integrand has a second-order pole at z = 0 and two simple poles at z = −1 ± i,
which are the roots of z2 + 2z + 2 = 0. Therefore, the residue at z = 0 is
Res

etz
z2(z2 + 2z + 2); 0

= lim
z→0
1
1!
d
dz

(z −0)2

etz
z2(z2 + 2z + 2)

(1.8.19)
= lim
z→0

tetz
z2 + 2z + 2 −
(2z + 2)etz
(z2 + 2z + 2)2

= t −1
2
.
(1.8.20)
The residue at z = −1 + i is
Res

etz
z2(z2 + 2z + 2); −1 + i

=
lim
z→−1+i[z −(−1 + i)]
etz
z2(z2 + 2z + 2)
(1.8.21)
=

lim
z→−1+i
etz
z2
 
lim
z→−1+i
z + 1 −i
z2 + 2z + 2

(1.8.22)
= exp[(−1 + i)t]
2i(−1 + i)2
= exp[(−1 + i)t]
4
.
(1.8.23)
Similarly, the residue at z = −1 −i is
Res

etz
z2(z2 + 2z + 2); −1 −i

=
lim
z→−1−i[z −(−1 −i)]
etz
z2(z2 + 2z + 2)
(1.8.24)
=

lim
z→−1−i
etz
z2
 
lim
z→−1−i
z + 1 + i
z2 + 2z + 2

(1.8.25)
= exp[(−1 −i)t]
(−2i)(−1 −i)2 = exp[(−1 −i)t]
4
.
(1.8.26)
Then by the residue theorem,
1
2πi
I
C
etz
z2(z2 + 2z + 2) dz = Res

etz
z2(z2 + 2z + 2); 0

+ Res

etz
z2(z2 + 2z + 2); −1 + i

+ Res

etz
z2(z2 + 2z + 2); −1 −i

(1.8.27)
= t −1
2
+ exp[(−1 + i)t]
4
+ exp[(−1 −i)t]
4
(1.8.28)
= 1
2 [t −1 + e−t cos(t)] .
(1.8.29)

Complex Variables
37
Problems
Assuming that all of the following closed contours are in the positive sense, use the residue
theorem to evaluate the following integrals:
1.
I
|z|=1
z + 1
z4 −2z3 dz
2.
I
|z|=1
(z + 4)3
z4 + 5z3 + 6z2 dz
3.
I
|z|=1
1
1 −ez dz
4.
I
|z|=2
z2 −4
(z −1)4 dz
5.
I
|z|=2
z3
z4 −1 dz
6.
I
|z|=1
zne2/z dz,
n > 0
7.
I
|z|=1
e1/z cos(1/z) dz
8.
I
|z|=2
2 + 4 cos(πz)
z(z −1)2
dz
9.
I
|z−1|= 1
2
z + 1
z −1
dz
sin(πz)
Hint for Problem 9: sin(πz) = −sin[π(z −1)] and z + 1 = (z −1) + 2.
1.9 EVALUATION OF REAL DEFINITE INTEGRALS
One of the important applications of the theory of residues consists of the evaluation of
certain types of real deﬁnite integrals. Similar techniques apply when the integrand contains
a sine or cosine.
• Example 1.9.1
Let us evaluate the integral
Z ∞
0
dx
x2 + 1 = 1
2
Z ∞
−∞
dx
x2 + 1.
(1.9.1)
This integration occurs along the real axis. In terms of complex variables, we can rewrite
Equation 1.9.1 as
Z ∞
0
dx
x2 + 1 = 1
2
Z
C1
dz
z2 + 1,
(1.9.2)
where the contour C1 is the line ℑ(z) = 0. However, the use of the residue theorem requires
an integration along a closed contour. Let us choose the one pictured in Figure 1.9.1. Then
I
C
dz
z2 + 1 =
Z
C1
dz
z2 + 1 +
Z
C2
dz
z2 + 1,
(1.9.3)
where C denotes the complete closed contour and C2 denotes the integration path along
a semicircle at inﬁnity. Clearly we want the second integral on the right side of Equation
1.9.3 to vanish; otherwise, our choice of the contour C2 is poor. Because z = Reθi and
dz = iReθi dθ,

Z
C2
dz
z2 + 1
 =

Z π
0
iR exp(θi)
1 + R2 exp(2θi) dθ
 ≤
Z π
0
R
R2 −1 dθ,
(1.9.4)
which tends to zero as R →∞. On the other hand, the residue theorem gives
I
C
dz
z2 + 1 = 2πi Res

1
z2 + 1; i

= 2πi lim
z→i
z −i
z2 + 1 = 2πi × 1
2i = π.
(1.9.5)

38
Advanced Engineering Mathematics: A Second Course
1
x
y
C
C
2
Figure 1.9.1: Contour used in evaluating the integral, Equation 1.9.1.
Therefore,
Z ∞
0
dx
x2 + 1 = π
2 .
(1.9.6)
Note that we only evaluated the residue in the upper half-plane because it is the only one
inside the contour.
⊓⊔
This example illustrates the basic concepts of evaluating deﬁnite integrals by the residue
theorem.
We introduce a closed contour that includes the real axis and an additional
contour. We must then evaluate the integral along this additional contour as well as the
closed contour integral. If we properly choose our closed contour, this additional integral
vanishes. For certain classes of general integrals, we shall now show that this additional
contour is a circular arc at inﬁnity.
Theorem: If, on a circular arc CR with a radius R and center at the origin, zf(z) →0
uniformly with |z| ∈CR and as R →∞, then
lim
R→∞
Z
CR
f(z) dz = 0.
(1.9.7)
The proof is as follows: If |zf(z)| ≤MR, then |f(z)| ≤MR/R. Because the length of
CR is αR, where α is the subtended angle,

Z
CR
f(z) dz
 ≤MR
R αR = αMR →0,
(1.9.8)
because MR →0 as R →∞.
⊓⊔
• Example 1.9.2
A simple illustration of this theorem is the integral
Z ∞
−∞
dx
x2 + x + 1 =
Z
C1
dz
z2 + z + 1.
(1.9.9)
A quick check shows that z/(z2 + z + 1) tends to zero uniformly as R →∞. Therefore, if
we use the contour pictured in Figure 1.9.1,
Z ∞
−∞
dx
x2 + x + 1 =
I
C
dz
z2 + z + 1 = 2πi Res

1
z2 + z + 1; −1
2 +
√
3
2 i

(1.9.10)
= 2πi
lim
z→−1
2 +
√
3
2 i

1
2z + 1

= 2π
√
3.
(1.9.11)
⊓⊔

Complex Variables
39
x
π/3
1
C
πi
e
/6
3
y
C2
C
Figure 1.9.2: Contour used in evaluating the integral, Equation 1.9.13.
• Example 1.9.3
Let us evaluate
Z ∞
0
dx
x6 + 1.
(1.9.12)
In place of an inﬁnite semicircle in the upper half-plane, consider the following integral
I
C
dz
z6 + 1,
(1.9.13)
where we show the closed contour in Figure 1.9.2. We chose this contour for two reasons.
First, we only have to evaluate one residue rather than the three enclosed in a traditional
upper half-plane contour. Second, the contour integral along C3 simpliﬁes to a particularly
simple and useful form.
Because the only enclosed singularity lies at z = eπi/6,
I
C
dz
z6 + 1 = 2πi Res

1
z6 + 1; eπi/6

= 2πi
lim
z→eπi/6
z −eπi/6
z6 + 1
(1.9.14)
= 2πi
lim
z→eπi/6
1
6z5 = −πi
3 eπi/6.
(1.9.15)
Let us now evaluate Equation 1.9.12 along each of the legs of the contour:
Z
C1
dz
z6 + 1 =
Z ∞
0
dx
x6 + 1,
(1.9.16)
Z
C2
dz
z6 + 1 = 0,
(1.9.17)
because of Equation 1.9.7 and
Z
C3
dz
z6 + 1 =
Z 0
∞
eπi/3 dr
r6 + 1 = −eπi/3
Z ∞
0
dx
x6 + 1,
(1.9.18)
since z = reπi/3.

40
Advanced Engineering Mathematics: A Second Course
Substituting into Equation 1.9.15,

1 −eπi/3 Z ∞
0
dx
x6 + 1 = −πi
3 eπi/6
(1.9.19)
or
Z ∞
0
dx
x6 + 1 = πi
6
2ieπi/6
eπi/6  eπi/6 −e−πi/6 =
π
6 sin(π/6) = π
3 .
(1.9.20)
⊓⊔
• Example 1.9.4
Rectangular closed contours are best for the evaluation of integrals that involve hyper-
bolic sines and cosines. To illustrate9 this, let us evaluate the integral
2
Z ∞
0
sin(ax) sinh(x)
[b + cosh(x)]2 dx =
Z ∞
−∞
sin(ax) sinh(x)
[b + cosh(x)]2 dx = ℑ
Z ∞
−∞
sinh(x)eiax
[b + cosh(x)]2 dx

,
(1.9.21)
where a > 0 and b > 1.
We begin by determining the value of
I
C
sinh(z)eiaz
[b + cosh(z)]2 dz
about the closed contour shown in Figure 1.9.3. Writing this contour integral in terms of
the four line segments that constitute the closed contour, we have
I
C
sinh(z)eiaz
[b + cosh(z)]2 dz =
Z
C1
sinh(z)eiaz
[b + cosh(z)]2 dz +
Z
C2
sinh(z)eiaz
[b + cosh(z)]2 dz
+
Z
C3
sinh(z)eiaz
[b + cosh(z)]2 dz +
Z
C4
sinh(z)eiaz
[b + cosh(z)]2 dz.
(1.9.22)
Because the integrand behaves as e−R as R →∞, the integrals along C2 and C4 vanish.
On the other hand,
Z
C1
sinh(z)eiaz
[b + cosh(z)]2 dz =
Z ∞
−∞
sinh(x)eiax
[b + cosh(x)]2 dx,
(1.9.23)
and
Z
C3
sinh(z)eiaz
[b + cosh(z)]2 dz = −e−2πa
Z ∞
−∞
sinh(x)eiax
[b + cosh(x)]2 dx,
(1.9.24)
9 This is a slight variation on a problem solved by Spyrou, K. J., B. Cotton, and B. Gurd, 2002:
Analytical expressions of capsize boundary for a ship with roll bias in beam waves.
J. Ship Res., 46,
167–174.

Complex Variables
41
  

C
C
C
C
1
2
3
4
(−R,0)
(R,0)
(R,2  i)
(−R,2  i)
π
π
y
x
zs
Figure 1.9.3: Rectangular closed contour used to obtain Equation 1.9.31.
because cosh(x + 2πi) = cosh(x) and sinh(x + 2πi) = sinh(x).
Within the closed contour C, we have a single singularity where b + cosh(zs) = 0 or
ezs = −b −
√
b2 −1 or zs = ln(b +
√
b2 −1 ) + πi. To discover the nature of this singularity,
we expand b + cosh(z) in a Taylor expansion and ﬁnd that
b + cosh(z) = sinh(zs)(z −zs) + 1
2 cosh(zs)(z −zs)2 + · · · .
(1.9.25)
Therefore, we have a second-order pole at z = zs. Therefore, the value of the residue there
is
Res
 sinh(z)eiaz
[b + cosh(z)]2 ; zs

= lim
z→zs
d
dz

sinh(z)eiaz
sinh2(zs) + sinh(zs) cosh(zs)(z −zs) + · · ·

(1.9.26)
= ia e−πa
sinh(zs) exp[ia cosh−1(b)].
(1.9.27)
Therefore,
Z ∞
−∞
sinh(x)eiax
[b + cosh(x)]2 dx = −2πa exp[−πa + ai cosh−1(b)]
(1 −e−2πa) sinh(zs)
= πa exp[ai cosh−1(b)]
√
b2 −1 sinh(πa)
, (1.9.28)
because
sinh(zs) = 1
2

−b −
p
b2 −1 +
1
b +
√
b2 −1

= −
p
b2 −1.
(1.9.29)
Substituting Equation 1.9.28 into Equation 1.9.21 yields
Z ∞
0
sin(ax) sinh(x)
[b + cosh(x)]2 dx = πa sin[a cosh−1(b)]
2
√
b2 −1 sinh(πa)
.
(1.9.30)
⊓⊔

42
Advanced Engineering Mathematics: A Second Course
• Example 1.9.5
The method of residues is also useful in the evaluation of deﬁnite integrals of the form
R 2π
0
F[sin(θ), cos(θ)] dθ, where F is a quotient of polynomials in sin(θ) and cos(θ).
For
example, let us evaluate the integral10
I =
Z 2π
0
cos3(θ)
cos2(θ) −a2 dθ,
a > 1.
(1.9.31)
We begin by introducing the complex variable z = eiθ. This substitution yields the
closed contour integral
I = 1
2i
I
C
(z2 + 1)3
(z2 + 1)2 −4a2z2
dz
z2 ,
(1.9.32)
where C is a circle of radius 1 taken in the positive sense. The integrand of Equation 1.9.32
has ﬁve singularities: a second-order pole at z5 = 0 and simple poles located at
z1 = −a −
p
a2 −1,
z2 = −a +
p
a2 −1,
(1.9.33)
z3 = a −
p
a2 −1,
and
z4 = a +
p
a2 −1.
(1.9.34)
Only the singularities z2, z3, and z5 lie within C. Consequently, the value of I equals 2πi
times the sum of the residues at these three singularities. The residues equal
Res

(z2 + 1)3
z2[(z2 + 1)2 −4a2z2]; −a +
p
a2 −1

=
lim
z→−a+
√
a2−1
(z2 + 1)3
z2
lim
z→−a+
√
a2−1
z + a −
√
a2 −1
(z2 + 1)2 −4a2z2
(1.9.35)
=
lim
z→−a+
√
a2−1
(z2 + 1)3
4z3(z2 + 1 −2a2)
(1.9.36)
= −
a2(a −
√
a2 −1 )3
(2a2 −1 −2a
√
a2 −1 )(a2 −1 −a
√
a2 −1 )
,
(1.9.37)
Res

(z2 + 1)3
z2[(z2 + 1)2 −4a2z2]; a −
p
a2 −1

=
lim
z→a−
√
a2−1
(z2 + 1)3
z2
lim
z→a−
√
a2−1
z −a +
√
a2 −1
(z2 + 1)2 −4a2z2
(1.9.38)
=
lim
z→a−
√
a2−1
(z2 + 1)3
4z3(z2 + 1 −2a2)
(1.9.39)
=
a2(a −
√
a2 −1 )3
(2a2 −1 −2a
√
a2 −1 )(a2 −1 −a
√
a2 −1 )
,
(1.9.40)
10 Simpliﬁed version of an integral presented by Jiang, Q. F., and R. B. Smith, 2000: V-waves, bow
shocks, and wakes in supercritical hydrostatic ﬂow. J. Fluid Mech., 406, 27–53.

Complex Variables
43
and
Res

(z2 + 1)3
z2[(z2 + 1)2 −4a2z2]; 0

= lim
z→0
d
dz

(z2 + 1)3
(z2 + 1)2 −4a2z2

(1.9.41)
= lim
z→0
6z[(z2 + 1)4 −4a2z2(z2 + 1)2] −4z(z2 + 1)3(z2 + 1 −2a2)
[(z2 + 1)2 −4a2z2]2
(1.9.42)
= 0.
(1.9.43)
Summing the residues, we obtain 0. Therefore,
Z 2π
0
cos3(θ)
cos2(θ) −a2 dθ = 0,
a > 1.
(1.9.44)
Problems
Use the residue theorem to verify the following integrals:
1.
Z ∞
0
dx
x4 + 1 = π
√
2
4
2.
Z ∞
−∞
dx
(x2 + 4x + 5)2 = π
2
3.
Z ∞
−∞
x dx
(x2 + 1)(x2 + 2x + 2) = −π
5
4.
Z ∞
0
x2
x6 + 1 dx = π
6
5.
Z ∞
0
dx
(x2 + 1)2 = π
4
6.
Z ∞
0
dx
(x2 + 1)(x2 + 4)2 = 5π
288
7.
Z ∞
−∞
x2 dx
(x2 + a2)(x2 + b2)2 =
π
2b(a + b)2 ,
a, b > 0
8.
Z ∞
0
t2
(t2 + 1)[t2(a/h + 1) + (a/h −1)] dt = π
4
"
1 −
r
a −h
a + h
#
,
a > h
9. Show that
Z π/2
0
dθ
a + sin2(θ) =
π
2
√
a + a2 ,
a > 0.
Step 1: Convert the real integral into a closed contour integration:
Z π/2
0
dθ
a + sin2(θ) = i
I
|z|=1
z
(z2 −1)2 −4az2 dz,
where z = eθi.
Step 2: Show that the integrand has four poles: z = ±√a ± √1 + a. Only two are located
inside the contour: z1 = −√a + √1 + a and z2 = √a −√1 + a.

44
Advanced Engineering Mathematics: A Second Course
Step 3: Show that the corresponding residues are
Res

z
(z2 −1)2 −4az2 ; z1

= Res

z
(z2 −1)2 −4az2 ; z2

= −
1
8
√
a + a2 .
Step 4: Obtain the ﬁnal result by applying the residue theorem and the results from Step
1 through Step 3.
10. Show that
Z π/2
0
dθ
a2 cos2(θ) + b2 sin2(θ) =
π
2ab,
b ≥a > 0.
Step 1: Convert the real integral into a closed contour integration:
Z π/2
0
dθ
a2 cos2(θ) + b2 sin2(θ) = −i
I
|z|=1
z
a2(z2 + 1)2 −b2(z2 −1)2 dz,
where z = eθi.
Step 2: Show that the integrand has four simple poles located at z2
+ = (b + a)/(b −a), and
z2
−= (b−a)/(b+a). Only two are located inside the contour: z(1)
−=
p
(b −a)/(b + a), and
z(2)
−= −
p
(b −a)/(b + a).
Step 3: Show that the corresponding residues are
Res

z
a2(z2 + 1)2 −b2(z2 −1)2 ; z(1)
−

= Res

z
a2(z2 + 1)2 −b2(z2 −1)2 ; z(2)
−

=
1
8ab.
Step 4: Obtain the ﬁnal result by employing the residue theorem and the results from Step
1 through Step 3.
11. Show that
Z π
0
sin2(θ)
a + b cos(θ) dθ = π
b2

a −
p
a2 −b2

,
a > b > 0.
Step 1: Convert the real integral into a closed contour integration:
Z π
0
sin2(θ)
a + b cos(θ) dθ = i
4
I
|z|=1
(z2 −1)2
[b(z2 + 1) + 2az]z2 dz,
where z = eθi.
Step 2: Show that the integrand has a second-order pole at z = 0 and simple poles at
z1,2 =
 −a ±
√
a2 −b2 
/b. Only the poles located at z = 0 and z1 =
 −a +
√
a2 −b2 
/b
lie within the closed contour.
Step 3: Show that the corresponding residues are
Res

(z2 −1)2
[b(z2 + 1) + 2az]z2 ; 0

= −2a
b2 ,
and Res

(z2 −1)2
[b(z2 + 1) + 2az]z2 ; z1

= 2
√
a2 −b2
b2
.

Complex Variables
45
Step 4: Obtain the ﬁnal results by employing the residue theorem and Step 1 through Step
3.
12. Show that
Z 2π
0
einθ
1 + 2r cos(θ) + r2 dθ = 2π (−r)n
1 −r2 ,
1 > |r|,
n = 0, 1, 2, . . . .
Step 1: Convert the real integral into a closed contour integration:
Z 2π
0
einθ
1 + 2r cos(θ) + r2 dθ = −i
I
|z|=1
zn
r(z2 + 1) + (1 + r2)z dz,
where z = eθi.
Step 2: Show that the integrand has two simple poles: z+ = −r, and z−= −1/r. Why is
the z+ pole the only one inside the contour?
Step 3: Show that the corresponding residue is
Res

zn
r(z2 + 1) + (1 + r2)z ; z+

= (−r)n
1 −r2 .
Step 4: Obtain the ﬁnal result by using the residue theorem and Step 1 through Step 3.
13. Show that
Z 2π
0
sin2n(θ) dθ = 2π(2n)!
(2nn!)2 .
Step 1: Convert the real integral into a closed contour integration:
Z 2π
0
sin2n(θ) dθ =
−i
(−1)n22n
I
|z|=1
(z2 −1)2n
z2n+1
dz,
where z = eθi.
Step 2: Show that the integrand has a pole of order 2n + 1 at z = 0.
Step 3: Because
(z2 −1)2n = z4n −2nz4n−1 + · · · + (2n)!(−1)n
n!n!
z2n + · · · ,
show that
Res
(z2 −1)2n
z2n+1
; 0

= (2n)!(−1)n
n!n!
.
Step 4: Obtain the ﬁnal result by using the residue theorem and Step 1 through Step 3.
14. Show that
Z π
−π
cos(nθ)
cos(θ) + α dθ = 2π
 −α +
√
α2 −1
n
√
α2 −1
,
α > 1,
n ≥0.

46
Advanced Engineering Mathematics: A Second Course
Step 1: Convert the real integral into a closed contour integration:
Z π
−π
cos(nθ)
cos(θ) + α dθ = 1
i
I
|z|=1
zn + z−n
z2 + 2αz + 1 dz,
where z = eθi.
Step 2: Assume that n ̸= 0. Show that the integrand has an n-order pole at z = 0 and
simple poles at z1,2 = −α ±
√
α2 −1. Why is z1 = −α +
√
α2 −1 the only simple pole that
lies inside the contour?
Step 3: Because
zn + z−n
z2 + 2αz + 1 =
1
2
√
α2 −1
zn + z−n
z −z1
−zn + z−n
z −z2

,
show that
Res
zn + z−n
z −z1
; z1

= z2n
1
+ 1
zn
1
.
Step 4: Because
zn + z−n
z −z1
= −
 zn + z−n 1
z1
"
1 +
 z
z1

+
 z
z1
2
+
 z
z1
3
+ · · ·
#
,
show that
Res
zn + z−n
z −z1
; 0

= −1
zn
1
,
and
Res
zn + z−n
z −z2
; 0

= −1
zn
2
= −zn
1 .
Step 5: Use the residue theorem plus Steps 1 through 4 to obtain the ﬁnal result when
n ̸= 0.
Step 6: Redo the problem when n = 0. In this case we only have the pole at z = z1.
15. Show that
Z π
0
cos(nθ)
cosh(α) −cos(θ) dθ =
π
sinh(α)e−nα,
α ̸= 0,
n ≥0.
Step 1: Convert the real integral into a closed contour integration:
Z π
0
cos(nθ)
cosh(α) −cos(θ) dθ = −1
i
I
|z|=1
zn + z−n
z2 −2z cosh(α) + 1 dz,
where z = eθi.
Step 2: Show that the integrand has an n-order pole at z = 0 and simple poles at z1,2 =
eα, e−α. Because α can be taken as positive without loss of generality, then only the poles
located at z = 0 and z = e−α lie within the closed contour.

Complex Variables
47
Step 3: Because
zn + z−n
z2 −2z cosh(α) + 1 =
1
2 sinh(α)
zn + z−n
z −eα
−zn + z−n
z −eα

,
show that the corresponding residues are
Res

zn + z−n
z2 −2z cosh(α) + 1; 0

= sinh(nα)
sinh(α) ,
from Example 1.7.5 and
Res

zn + z−n
z2 −2z cosh(α) + 1; e−α

= −cosh(nα)
sinh(α) .
Step 4: Use the residue theorem plus Steps 1 through 3 to ﬁnish the problem.
16. Show that
Z ∞
0
x2
(1 −x2)2 + a2x2 dx =
π
2|a|,
where a is real and not equal to zero.
Step 1: Show that
Z ∞
0
x2
(1 −x2)2 + a2x2 dx = 1
2
I
C
z2
(1 −z2)2 + a2z2 dz,
where C denotes a semicircle of inﬁnite radius in the upper half of the complex plane. Along
the real axis, the contour slightly above y = 0 when x < 0 and slightly below y = 0 when
x > 0.
Step 2: Show that the poles of the integrand are simple and equal
zn =



± 1
2
 ±
√
4 −a2 + |a|i

,
if
0 < |a| < 2,
± i
2
 |a| ±
√
a2 −4

,
if
2 < |a|.
If |a| = 2, we have second-order poles at zn = ±i.
Step 3: Show that the residues for the poles in the upper half plane are
Res

f(z); 1
2
 ±
p
4 −a2 + |a|i

= ±(±
√
4 −a2 + |a|i)/2
2|a|i
√
4 −a2
,
Res

f(z); i
2
 |a| ±
p
a2 −4

= ∓i(|a| ±
√
a2 −4 )/2
2|a|
√
a2 −4
,
and
Res[f(z); i ] = −i
4.
Step 4: Show that when you sum the residues for the cases for 0 < |a| < 2 and 2 < |a|, you
obtain −i/(2|a|)

48
Advanced Engineering Mathematics: A Second Course
Step 5: Redo the calculation when |a| = 2.
17. Evaluating the closed contour integral
I
C
eiaz
cosh2(bz) dz,
around the rectangular contour with vertices at (∞, 0), (−∞, 0), (∞, π/b), and (−∞, π/b),
show that
Z ∞
0
cos(ax)
cosh2(bx) dx =
πa
2b2 sinh[aπ/(2b)],
a, b > 0.
Step 1: Show that
I
C
eiaz
cosh2(bz) dz =
Z
C1
eiaz
cosh2(bz) dz +
Z
C2
eiaz
cosh2(bz) dz
+
Z
C3
eiaz
cosh2(bz) dz +
Z
C4
eiaz
cosh2(bz) dz,
where C1, C2, C3 and C4 are the contours along the bottom, right side, top, and left side
of the rectangle.
Step 2: Show that the integrals along C2 and C4 vanish. Why?
Step 3: Show that
Z
C1
eiaz
cosh2(bz) dz =
Z ∞
−∞
eiax
cosh2(bx) dx,
and
Z
C3
eiaz
cosh2(bz) dz = −e−πa/b
Z ∞
−∞
eiax
cosh2(bx) dx.
Step 4: Setting zs = πi/(2b), show that the Laurent expansion for eiaz/ cosh2(bz) at zs is
eiaz
cosh2(bz) = −
eiazs
b2(z −zs)2 −
ia eiazs
b2(z −zs) + · · · .
Hence, we have a second-order pole there.
Step 5: Show that
(1 −e−πa/b)
Z ∞
−∞
eiax
cosh2(bx) dx = 2πae−πa/(2b)
b2
.
Step 6: Simplify Step 5 to obtain the desired result.
18. Using the closed contour integral
I
C
z
cosh(z) cosh(z + a) dz,

Complex Variables
49
where C is a rectangular contour with vertices at (∞, 0), (−∞, 0), (∞, π), and (−∞, π),
show11 that
Z ∞
0
dx
cosh(x) cosh(x + a) =

2a/ sinh(a),
if
a ̸= 0,
2,
if
a = 2.
Step 1: If a ̸= 0, show that
I
C
f(z) dz = 2πi Res[f(z); z1] + 2πi Res[f(z); z2],
where z1 and z2 are simple poles with z1 = πi/2 and z2 = −a + πi/2, respectively.
Step 2: Show that in this case,
Res[f(z); z1] = −
πi
2 sinh(a),
and
Res[f(z); z2] =
πi
2 −a

1
sinh(a).
Step 3: If a = 0, show that we a second-order pole located at z1 = πi/2 within the closed
contour with
Res[f(z); z1] =
lim
z→πi/2
d
dz
z(z −πi/2)2
cosh2(z)

= −lim
η→0
d
dη
η2(η + πi/2)
sinh2(η)

= −lim
η→0
d
dη

η + πi/2
(1 + η2/6 + · · ·)2

= −lim
η→0
(1 + η2/6 + · · ·) −(η + πi/2)(3η + · · ·)
(1 + η2/6 + · · ·)4

= −1.
Step 4: Denoting the contour along and parallel to the y-axis at x = ∞as C2 and the
contour along and parallel to the y-axis at x = −∞as C4, show that
Z
C2
f(z) dz →0
as
x →∞,
and
Z
C4
f(z) dz →0
as
x →−∞.
Step 5: Along the real axis, call it C1, show that
Z
C1
f(z) dz =
Z ∞
−∞
x
cosh(x) cosh(x + a) dx,
while along the contour C3 (which runs parallel to the real axis but π units above it),
Z
C3
f(z) dz = −
Z ∞
−∞
x + πi
cosh(x) cosh(x + a) dx.
Step 6: If a ̸= 0, show that
−πi
Z ∞
−∞
dx
cosh(x) cosh(x + a) = −2πai
sinh(a),
11 See Yan, J. R., X. H. Yan, J. Q. You, and J. X. Zhong, 1993: On the interaction between two
nonpropagating hydrodynamic solitons. Phys. Fluids A, 5, 1651–1656.

50
Advanced Engineering Mathematics: A Second Course
while for a = 0, show that
−πi
Z ∞
−∞
dx
cosh2(x) = −2πi.
19. During an electromagnetic calculation, Strutt12 needed to prove that
π sinh(σx)
cosh(σπ) = 2σ
∞
X
n=0
cos
 n + 1
2

(x −π)

σ2 +
 n + 1
2
2
,
|x| ≤π.
Verify his proof by doing the following:
Step 1: Using the residue theorem, show that
1
2πi
I
CN
π sinh(xz)
cosh(πz)
dz
z −σ = π sinh(σx)
cosh(σπ) −
N
X
n=−N−1
(−1)n sin
 n + 1
2

x

σ −i
 n + 1
2

,
where CN is a circular contour that includes the poles z = σ and zn = ±i
 n + 1
2

, n =
0, 1, 2, . . . , N.
Step 2: Show that in the limit of N →∞, the contour integral vanishes. Hint: Examine
the behavior of z sinh(xz)/[(z −σ) cosh(πz)] as |z| →∞. Use Equation 1.9.7 where CR is
the circular contour.
Step 3: Break the inﬁnite series in Step 1 into two parts and simplify.
You would obtain the same series by computing the Fourier series of sinh(σx)/ cosh(σπ)
and using direct integration.
1.10 CAUCHY’S PRINCIPAL VALUE INTEGRAL
The conventional deﬁnition of the integral of a function f(x) of the real variable x over
a ﬁnite interval a ≤x ≤b assumes that f(x) has a deﬁnite ﬁnite value at each point within
the interval. We shall now extend this deﬁnition to cover cases when f(x) is inﬁnite at a
ﬁnite number of points within the interval.
Consider the case when there is only one point c at which f(x) becomes inﬁnite. If c
is not an endpoint of the interval, we take two small positive numbers ǫ and η and examine
the expression
Z c−ǫ
a
f(x) dx +
Z b
c+η
f(x) dx.
(1.10.1)
If Equation 1.10.1 exists and tends to a unique limit as ǫ and η tend to zero independently,
we say that the improper integral of f(x) over the interval exists, its value being deﬁned by
Z b
a
f(x) dx = lim
ǫ→0
Z c−ǫ
a
f(x) dx + lim
η→0
Z b
c+η
f(x) dx.
(1.10.2)
12 Strutt, M. J. O., 1934: Berechnung des hochfrequenten Feldes einer Kreiszylinderspule in einer konzen-
trischen leitenden Schirmh¨ulle mit ebenen Deckeln. Hochfrequenztechn. Elecktroak., 43, 121–123.

Complex Variables
51
If, however, the expression does not tend to a limit as ǫ and η tend to zero independently,
it may still happen that
lim
ǫ→0
(Z c−ǫ
a
f(x) dx +
Z b
c+ǫ
f(x) dx
)
(1.10.3)
exists. When this is the case, we call this limit the Cauchy principal value of the improper
integral and denote it by
PV
Z b
a
f(x) dx.
(1.10.4)
Finally, if f(x) becomes inﬁnite at an endpoint, say a, of the range of integration, we say
that f(x) is integrable over a ≤x ≤b if
lim
ǫ→0+
Z b
a+ǫ
f(x) dx
(1.10.5)
exists.
• Example 1.10.1
Consider the integral
R 2
−1 dx/x.
This integral does not exist in the ordinary sense
because of the strong singularity at the origin. However, the integral would exist if
lim
ǫ→0
Z ǫ
−1
dx
x + lim
δ→0
Z 2
δ
dx
x
(1.10.6)
existed and had a unique value as ǫ and δ independently approach zero. Because this limit
equals
lim
ǫ,δ→0 [ ln(ǫ) + ln(2) −ln(δ)] = lim
ǫ,δ→0 [ ln(2) −ln(δ/ǫ)] ,
(1.10.7)
our integral would have the value of ln(2) if δ = ǫ. This particular limit is the Cauchy
principal value of the improper integral, which we express as
PV
Z 2
−1
dx
x = ln(2).
(1.10.8)
⊓⊔
We can extend these ideas to complex integrals used to determine the value or prin-
cipal value of an improper integral by Cauchy’s residue theorem when the integrand has a
singularity on the contour of integration. We avoid this diﬃculty by deleting from the area
within the contour, that portion which also lies within a small circle |z −c| = ǫ, and then
integrating around the boundary of the remaining region. This process is called indenting
the contour.
The integral around the indented contour is calculated by the theorem of residues
and then the radius of each indentation is made to tend to zero. This process gives the
Cauchy principal value of the improper integral. The details of this method are shown in
the following examples.

52
Advanced Engineering Mathematics: A Second Course
C
a
-a
C
-R
1
2
x
x
R
η
ε
Figure 1.10.1: Contour C used in Example 1.10.2.
• Example 1.10.2
Let us show that
PV
Z ∞
−∞
cos(x)
a2 −x2 dx = π sin(a)
a
,
a > 0.
(1.10.9)
Consider the integral
I
C
eiz
a2 −z2 dz,
(1.10.10)
where the closed contour C consists of the real axis from −R to R and a semicircle in the
upper half of the z-plane where this segment is its diameter. See Figure 1.10.1. Because
the integrand has poles at z = ±a, which lie on this contour, we modify C by making an
indentation of radius ǫ at a and another of radius η at −a. The integrand is now analytic
within and on C and Equation 1.10.10 equals zero by the Cauchy-Goursat theorem.
Evaluating each part of the integral, Equation 1.10.10, we have that
Z π
0
eiR cos(θ)−R sin(θ)
a2 −R2e2θi
iReθi dθ +
Z
C1
eiz
a2 −z2 dz +
Z
C2
eiz
a2 −z2 dz
+
Z −a−η
−R
eix
a2 −x2 dx +
Z a−ǫ
−a+η
eix
a2 −x2 dx +
Z R
a−ǫ
eix
a2 −x2 dx = 0,
(1.10.11)
where C1 and C2 denote the integrals around the indentations at a and −a, respectively.
The modulus of the ﬁrst term on the left side of Equation 1.10.11 is less than πR/(R2 −a2),
so this term tends to zero as R →∞. To evaluate C1, we observe that z = a + ǫeθi along
C1, where θ decreases from π to 0. Hence,
Z
C1
eiz
a2 −z2 dz = lim
ǫ→0
Z 0
π
exp
 ia + iǫeθi
ǫieθi
−2aǫeθi −ǫ2e2θi dθ
(1.10.12)
= lim
ǫ→0
Z π
0
exp
 ia + iǫeθi
i
2a + ǫeθi dθ = πieia
2a .
(1.10.13)
Similarly,
Z
C2
eiz
a2 −z2 dz = −πie−ia
2a
,
(1.10.14)
as η tends to zero.

Complex Variables
53
-R
x
1
ε
C
R
Figure 1.10.2: Contour C used in Example 1.10.3.
Upon letting R →∞, ǫ →0, and η →0, we ﬁnd that
PV
Z ∞
−∞
eix
a2 −x2 dx = −πi
2a
 eia −e−ia
= π sin(a)
a
.
(1.10.15)
Finally, equating the real and imaginary parts, we obtain
PV
Z ∞
−∞
cos(x)
a2 −x2 dx = π sin(a)
a
,
PV
Z ∞
−∞
sin(x)
a2 −x2 dx = 0.
(1.10.16)
⊓⊔
• Example 1.10.3
Let us show that
Z ∞
−∞
sin(x)
x
dx = π.
(1.10.17)
Consider the integral
I
C
eiz
z dz,
(1.10.18)
where the closed contour C consists of the real axis from −R to R and a semicircle in the
upper half of the z-plane where this segment is its diameter. Because the integrand has a
pole at z = 0, which lies on the contour, we modify C by making an indentation of radius
ǫ at z = 0. See Figure 1.10.2. Because eiz/z is analytic along C,
Z π
0
eiR cos(θ)−R sin(θ)i dθ +
Z −ǫ
−R
eix
x dx +
Z
C1
eiz
z dz +
Z R
ǫ
eix
x dx = 0.
(1.10.19)
Since e−R sin(θ) < e−Rθ for 0 < θ < π,

Z π
0
eiR cos(θ)−R sin(θ)i dθ
 ≤
Z π
0
e−Rθ dθ = 1 −e−πR
R
,
(1.10.20)
which tends to zero as R →∞. Therefore,
Z −ǫ
−∞
eix
x dx +
Z ∞
ǫ
eix
x dx = −
Z
C1
eiz
z dz.
(1.10.21)

54
Advanced Engineering Mathematics: A Second Course
Now,
Z
C1
eiz
z dz =
Z
C1
dz
z + i
Z
C1
dz −
Z
C1
z
2 dz + · · · = −πi
(1.10.22)
in the limit ǫ →0 because z = ǫeθi. Consequently, in the limit of ǫ →0,
PV
Z ∞
−∞
eix
x dx = π.
(1.10.23)
Upon separating the real and imaginary parts, we obtain
PV
Z ∞
−∞
cos(x)
x
dx = 0,
Z ∞
−∞
sin(x)
x
dx = π.
(1.10.24)
Problems
1. Noting that
Z θ−ǫ
0
dϕ
cos(ϕ) −cos(θ) =
1
sin(θ) ln

sin
 1
2(θ + ϕ)

sin
 1
2(θ −ϕ)


θ−ǫ
0
,
and
Z π
θ+ǫ
dϕ
cos(ϕ) −cos(θ) =
1
sin(θ) ln

sin
 1
2(θ + ϕ)

sin
 1
2(θ −ϕ)


π
θ+ǫ
,
show that
PV
Z π
0
dϕ
cos(ϕ) −cos(θ) = 0,
0 < θ < π.
2. Show that
Z ∞
−∞
cos(πx/2)
x2 −1
dx = −π.
Step 1: Show that
Z ∞
−∞
cos(πx/2)
x2 −1
dx = ℜ

PV
Z ∞
−∞
eiπx/2
x2 −1 dx

.
Step 2: Consider now the integral
H
C eiπz/2 dz/(z2 −1), where the closed contour C consists
of the real axis from −R to R plus a semicircle of radius R in the upper half of the z-plane.
See Figure 1.10.1. Because the integrand has poles at z = ±1 which lie on the contour, we
modify C by making an indentation of radius η above z = −1 and another indentation of
radius ǫ above z = 1. Why is this closed contour integral equal to zero?
Step 3: Show that this contour integral is given by
lim
R→∞
Z π
0
eiπR cos(θ)/2−Rπ sin(θ)/2
R2e2θi −1
iReθi dθ +
Z
C1
eiπz/2
z2 −1 dz +
Z
C2
eiπz/2
z2 −1 dz
+ lim
η→0
Z −1−η
−R
eiπx/2
x2 −1 dx + lim
ǫ,η→0
Z 1−ǫ
−1+η
eiπx/2
x2 −1 dx + lim
ǫ→0
Z R
1+ǫ
eiπx/2
x2 −1 dx = 0,

Complex Variables
55
where C1 and C2 denote the integrals around the indentations at −1 and 1, respectively.
Step 4: Show that the ﬁrst term on the left side tends to zero as R →∞. Why?
Step 5: Taking z = −1 + ηeθi along C1, where θ decreases from π to 0, show that
Z
C1
eiπz/2
z2 −1 dz = lim
η→0
Z 0
π
exp

iπ(−1 + ηeθi)/2

−2ηeθi + η2e2θi
iηeθi dθ = π
2 .
Step 6: Similarly, show that along C2,
Z
C2
eiπz/2
z2 −1 dz = lim
ǫ→0
Z 0
π
exp

iπ(1 + ǫeθi)/2

2ǫeθi + ǫ2e2θi
iǫeθi dθ = π
2 .
Step 7: Using Steps 1 through 6, obtain the ﬁnal result.
3. Show that
Z ∞
−∞
eax −ebx
1 −ex
dx = π[cot(aπ) −cot(bπ)],
0 < a, b < 1.
Step 1: Consider the integral
H
C
 eaz−ebz
dz/(1−ez), where the closed contour C consists of
the rectangular box with vertices at (−R, 0), (R, 0), (−R, π) and (R, π), and a semicircular
indentation Cǫ at the origin. Show that this closed integral equals zero. Why?
Step 2: Show that this closed integral may be rewritten,
lim
R→∞
Z −R+πi
R+πi
eaz −ebz
1 −ez
dz + lim
R→∞
Z −R
−R+πi
eaz −ebz
1 −ez
dz +
lim
R→∞,ǫ→0
Z −ǫ
−R
eaz −ebz
1 −ez
dz
+
Z
Cǫ
eaz −ebz
1 −ez
dz +
lim
R→∞,ǫ→0
Z R
ǫ
eaz −ebz
1 −ez
dz + lim
R→∞
Z R+πi
R
eaz −ebz
1 −ez
dz = 0.
Step 3: Show that
Z
Cǫ
eaz −ebz
1 −ez
dz = lim
ǫ→0
Z 0
π
1 + aǫeθi + a2ǫ2e2θi/2 + · · · −1 −bǫeθi −b2ǫ2e2θi/2 −· · ·
1 −1 −ǫeθi −ǫ2e2θi/2 −· · ·
iǫeθi dθ
= 0.
Step 4: Show that
lim
R→∞
Z −R
−R+πi
eaz −ebz
1 −ez
dz = lim
R→∞
Z 0
π
e−aReayi −e−bRebyi
1 −e−Reyi
i dy = 0,
and
lim
R→∞
Z R+πi
R
eaz −ebz
1 −ez
dz = lim
R→∞
Z π
0
eaReayi −ebRebyi
1 −eReyi
i dy = 0,
if 0 < a, b < 1.

56
Advanced Engineering Mathematics: A Second Course
Step 5: Show that
Z ∞
−∞
eax −ebx
1 −ex
dx =
Z ∞
−∞
eaxeaπi
1 + ex dx −
Z ∞
−∞
ebxebπi
1 + ex dx
=
πeaπi
sin(aπ) −πebπi
sin(bπ) = π [cot(aπ) + i] −π [cot(bπ) + i] .
4. Show13 that
Z ∞
−∞
1 −cos[2a(x + ζ)]
(x + ζ)2(x2 + α2) dx =
π
α(ζ2 + α2)2

2aα(ζ2 + α2) + (ζ2 −α2)
−e−2aα 
(ζ2 −α2) cos(2aζ) + 2αζ sin(2aζ)
	
,
where a, α, and ζ are real.
Step 1: Show that
 Z
C∞
+
Z −ζ−ǫ
−R
+
Z
Cǫ
+
Z R
−ζ+ǫ
!
f(z) dz = 2πi Res[f(z); iα],
where C∞denotes the semicircular contour of inﬁnite radius, Cǫ is the semicircular inden-
tation above z = −ζ and
f(z) =
1 −e2ia(z+ζ)
(z + ζ)2(z2 + α2).
Step 2: Taking the limit of R →∞and ǫ →0, show that
Z ∞
−∞
f(x) dx = 2πi Res[f(z); iα] + πi Res[f(z); −ζ].
Step 3: Show that
Res[f(z); iα] = ζ2 −α2 −e−2aα[(ζ2 −α2) cos(2aζ) + 2αζ sin(2aζ)]
2iα(ζ2 + α2)2
−2αζ + e−2aα[(ζ2 −α2) sin(2aζ) −2αζ cos(2aζ)]
2α(ζ2 + α2)2
and
Res[f(z); −ζ] = lim
z→−ζ
d
dz
1 −e2ia(z+ζ)
z2 + α2

= −
2ia
ζ2 + α2 .
Step 4: Use the results from Steps 1 through 3 to obtain the desired result.
5. Show that
PV
Z ∞
−∞
cos(mx)
x −a
dx = −π sin(ma),
and
PV
Z ∞
−∞
sin(mx)
x −a
dx = π cos(ma),
13 Ko, S. H., and A. H. Nuttall, 1991: Analytical evaluation of ﬂush-mounted hydrophone array response
to the Corcos turbulent wall pressure spectrum. J. Acoust. Soc. Am., 90, 579–588.

Complex Variables
57
where m > 0 and a is real.
Step 1: Using the complex function eimz/(z −a) and a closed contour similar to that shown
in Figure 1.10.2, show that
 Z
C∞
+
Z a−ǫ
−R
+
Z
Cǫ
+
Z R
a+ǫ
!
eimz
z −a dz = 0.
Why? Here C∞denotes the semicircular contour of inﬁnite radius and Cǫ is the semicircular
indentation above z = a.
Step 2: Taking the limit of R →∞and ǫ →0, show that
PV
Z ∞
−∞
eimx
x −a dx = πi Res
 eimz
z −a; a

= πieima.
Step 3: Complete the derivation by taking the real and imaginary parts of the equation in
Step 2.
6. Show that
PV
Z ∞
−∞
xexi
x2 −π2 dx = −πi,
and
PV
Z ∞
−∞
eimx
(x −1)(x −3) dx = πi
2
 e3mi −emi
,
where m > 0.
Step 1: Show that
 Z
C∞
+
Z −π−ǫ
−R
+
Z
Cǫ1
+
Z π−ǫ
−π+ǫ
+
Z
Cǫ2
+
Z R
π+ǫ
!
zeiz
z2 −π2 dz = 0
Why? Here C∞denotes the semicircular contour of inﬁnite radius and Cǫ1 and Cǫ2 are
semicircular indentations above z = −π and z = π.
Step 2: Taking the limit of R →∞and ǫ →0, show that
PV
Z ∞
−∞
xeix
x2 −π2 dx = πi Res
 z eiz
z2 −π2 ; −π

+ πi Res
 z eiz
z2 −π2 ; π

= 1
2πie−πi + 1
2πieπi = −πi.
Step 3: To prove the second relationship, show that
 Z
C∞
+
Z 1−ǫ
−R
+
Z
Cǫ1
+
Z 3−ǫ
1+ǫ
+
Z
Cǫ2
+
Z R
3+ǫ
!
eimz
(z −1)(z −3) dz = 0.
Why? Here C∞denotes the semicircular contour of inﬁnite radius and Cǫ1 and Cǫ2 are
semicircular indentations above z = 1 and z = 3.
Step 4: Taking the limit of R →∞and ǫ →0, show that
PV
Z ∞
−∞
eimx
(x −1)(x −3) dx = πi Res

eimz
(z −1)(z −3); 1

+ πi Res

eimz
(z −1)(z −3); 3

= −1
2πiemi + 1
2πie3mi = πi
2
 e3mi −emi
.

58
Advanced Engineering Mathematics: A Second Course
7. Redo Example 1.10.3, except the contour is now a rectangle with vertices at ±R and
±R + Ri indented at the origin.
Step 1: Show that along the left side,

Z −R+Ri
−R
eiz
z dz
 ≤
Z R
0
e−y
p
R2 + y2 dy < 1
R
Z R
0
e−y dy = 1
R
 1 −e−R
,
which tends to zero as R →∞.
Step 2: Show that along the top,

Z R+Ri
R
eiz
z dz
 ≤
Z R
0
e−y
p
R2 + y2 dy,
which also tends to zero as R →∞. Why?
Step 3: Show that along the right side,

Z R+Ri
−R+Ri
eiz
z dz
 ≤2e−R
Z R
0
dx
√
R2 + x2 = 2 ln

1 +
√
2

e−R,
which tends to zero as R →∞. Why?
Step 4: Just as in the case of the semicircle close contour, we only have an integration along
the real axis. Do this to complete the problem.
8. Let us show14 that
G(α) = PV
Z 1
−1
dx
(x + α)
√
1 −x2 =
(
απ
|α|
√
α2 −1
,
|α| > 1,
0,
|α| < 1.
Step 1: Using the transformation 2ix = z −z−1, show that
2i dx = z2 + 1
z2
dz,
1 −x2 = 1
4

z + 1
z
2
,
p
1 −x2 = 1
2
z2 + 1
z

,
and
x + α = 1
2i

z −1
z

+ α = 1
2i
z2 + 2iαz −1
z

.
Substitute these results into the original integral to ﬁnd G(α) as a contour integration on
the unit circle.
Step 2: For |α| < 1, we have two singularities within the contours located at z = ±
√
1 −α2−
αi. In that case, show that G(α) = 0.
Step 3: If α > 1, there is a single singularity within the contours and it is located at
z = i
√
α2 −1 −αi. Show that G(α) = π/
√
α2 −1.
14 Ott, E., T. M. Antonsen, and R. V. Lovelace, 1977: Theory of foil-less diode generation of intense
relativistic electron beams. Phys. Fluids, 20, 1180–1184.

Complex Variables
59
Step 4: Finally, if α < −1, there is a single singularity within the contours and it is located
at z = −i
√
α2 −1 −αi. Show that G(α) = −π/
√
α2 −1.
9. Let the function f(z) possess a simple pole with a residue Res[f(z); c] on a simply closed
contour C. If C is indented at c, show that the integral of f(z) around the indentation
tends to −Res[f(z); c]αi as the radius of the indentation tends to zero, α being the internal
angle between the two parts of C meeting at c.
1.11 CONFORMAL MAPPING
Conformal mapping is a powerful technique for ﬁnding solutions, or for simplifying
the process of ﬁnding solutions, to Laplace’s diﬀerential equation in two dimensions. This
method involves introducing two complex variables: z = x + iy and τ = ρ + iσ. These
two complex variables are related to each other via the mapping z = f(τ). Under this
mapping the Argand diagram for the z-variable is mapped into one for the τ-variable. In
certain cases, for example τ = √z, the complex z-plane may only map into a portion of the
τ-plane. In other cases, say τ = z + 3i, the complete z-plane would be mapped into the
complete τ-plane.
Once we map the original domain into a simpler geometry (a half-plane, circle or
square), how do we ﬁnd the solution? There are several techniques available. One method,
for example, recalls that the real and imaginary parts of an analytic function satisfy
Laplace’s equation. Therefore, if we could construct an analytic function whose real or
imaginary parts satisfy the boundary conditions in the new domain, we would have the
solution in the τ-plane. Then we could use the transformation to obtain the solution in the
original z-plane.
What types of functions f(z) are useful? Consider an arbitrary point z0 in the complex
z-plane. Assuming that f ′(z0) ̸= 0, a straightforward transformation yields
∂2u
∂x2 + ∂2u
∂y2

z0
= |f ′(z0)|2
∂2v
∂ρ2 + ∂2v
∂σ2

τ0
,
(1.11.1)
where u(x, y) and v(ρ, σ) are solutions to Laplace’s equation in the z and τ planes, respec-
tively. Thus, f(z) must be analytic.
• Example 1.11.1
In their study of magnetic recording, Curland and Judy15 modeled the ring heads as
two semi-inﬁnite regions located below the x-axis and running to the right of x = a/2 and
to the left of x = −a/2. See Figure 1.11.1.
From symmetry we need only consider the half-space x > 0. Consequently, the new
boundary consists of the four line segments: AB, BC, CD and DE. If we require that
the point D in the τ-plane lies at τ = 1, we shall show in Example 1.11.7 that the desired
conformal mapping is
z = a
π
√
τ −1 −i
2 log
1 −i√τ −1
1 + i√τ −1

+ a
2.
(1.11.2)
15 Curland, N., and J. H. Judy, 1986: Calculation of exact ring head ﬁelds using conformal mapping.
IEEE Trans. Magnet., MAG-22, 1901–1903.

60
Advanced Engineering Mathematics: A Second Course
                                                                                                                  


                                                                                                                                                                                                                                                














                                                                                                                                                                                                                                                














               














z−plane
σ
τ−
ρ
plane
x
a/2
y
τ = 1
                                          
                                          
              













                     




















      


      


              













              













A
D
E
B C
C
B
A
D
E
Figure 1.11.1: The conformal mapping used to ﬁnd the ﬁelds of a semi-inﬁnite ring head with a ﬁnite gap
of width a. The potential on the right pole face equals 1 while the potential of the left pole face equals −1.
In the z-plane the point A is located at (0, ∞) while point B is located at (0, −∞). Because of symmetry
the potential along the center of the gap AB equals 0.
A useful method for illustrating this conformal mapping is to draw lines of constant ρ and
σ in the z-plane. See Figure 1.11.2. This ﬁgure shows the local orthogonality between lines
of constant ρ and σ.
The greatest diﬃculty in creating this ﬁgure was computing τ for a given z. This was
done using the Newton-Raphson method. Starting at the top of the domain, the ﬁrst guess
there was given by τ = 1 + π2z2. Marching downward, the τ from the previous grid point
was used for the initial guess. The corresponding MATLAB script is as follows:
clear; delta = 0.01; % resolution of the grid
for jj = 1:201
for ii = 1:201
XX(jj,ii) = delta*ii; YY(jj,ii) = delta*(jj-101);
RHO(jj,ii) = NaN; SIGMA(jj,ii) = NaN;
end; end
% code for the domain x, y > 0
for jj = 1:100
y = 1 - delta*(jj-1);
for ii = 1:201
x = delta*ii; z = complex(x,y);
if (jj == 1) tau = 1+pi*pi*z*z; else tau = TAU(ii); end
for icount = 1:10
temp1 = sqrt(tau-1);
temp2 = temp1 - 0.5*i*log(1-i*temp1) + 0.5*i*log(1+i*temp1);
ff = temp2/pi + 0.5 - z; deriv = temp1 /(2*pi*tau);
temp3 = ff/deriv; tau = tau - temp3; % Newton-Raphson method
end
TAU(ii) = tau; RHO(202-jj,ii) = real(tau);
SIGMA(202-jj,ii) = imag(tau);
end; end

Complex Variables
61
0.002
0.05
0.5
0.5
0.5
0.5
5
5
5
10
10
20
20
30
-8
-4
0
0
0
4
4
10
20
30
x/a
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
y/a
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Figure 1.11.2: Lines of constant ρ (dashed lines) and σ (solid lines) given by the conformal mapping
expressed by Equation 1.11.2.
% code for the domain 0 < x < 1
2 and y < 0
for jj = 1:101
y = delta - delta*jj;
for ii = 1:49
x = delta*ii; z = complex(x,y);
tau = TAU(ii); % first guess
for icount = 1:10
temp1 = sqrt(tau-1);
temp2 = temp1 - 0.5*i*log(1-i*temp1) + 0.5*i*log(1+i*temp1);
ff = temp2/pi + 0.5 - z; deriv = temp1 /(2*pi*tau);
temp3 = ff/deriv; tau = tau - temp3; % Newton-Raphson method
end
TAU(ii) = tau; RHO(102-jj,ii) = real(tau);
SIGMA(102-jj,ii) = imag(tau);
end; end
% plot the conformal mapping Equation 1.11.2
figure
[C,h] = contour(XX,YY,SIGMA,[0.002,0.05,0.5,5,10,20,30],’k’);
clabel(C,h,’FontSize’,10,’Color’,’k’,’Rotation’,0)
xlabel(’x/a’,’FontSize’,20); ylabel(’y/a’,’FontSize’,20);
hold on
v = [-8,-4,0,4,10,20,30];
[C,h] = contour(XX,YY,RHO,v,’--b’);
clabel(C,h,’FontSize’,10,’Color’,’b’,’Rotation’,0)

62
Advanced Engineering Mathematics: A Second Course
0.1
0.1
0.3
0.3
0.5
0.5
0.5
0.7
0.7
0.7
0.9
0.9
0.9
x/a
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
y/a
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Figure 1.11.3: The solution to Laplace’s equation when the left boundary is held at 0 while the left and
top sides of the shaded rectangle are held at 1. This ﬁgure shows only a portion of the domain x > 0 and
|y| < ∞.
Now that we can transform between the z-plane and the τ-plane, and vice versa, let us
turn our attention to ﬁnding the solution to Laplace’s equation in the τ-plane. There the
solution equals 1 for ρ > 0 and 0 for ρ < 0 along σ = 0.
Consider now the analytic function (except at the branch point τ = 0)
f(τ) = i −log(τ)/π.
(1.11.3)
A quick check (using τ = reiθ) shows that the imaginary part of f(τ), v(r, θ) = 1 −θ/π,
satisﬁes Laplace’s equation and the boundary conditions. Thus, constructing the solution is
as follows: For a given x and y, we use our MATLAB code to compute τ. Substituting that
τ into Equation 1.11.3 we compute f(τ). Taking the imaginary part, we have the solution
at x and y. Figure 1.11.3 illustrates the solution for the domain 0 < x < 2 and −1 < y < 1.
⊓⊔
In summary, conformal mapping allowed us to transform the original domain into one
(an upper half-plane) where we could construct another analytic function whose imaginary
part satisﬁed Laplace’s equation and the boundary conditions. A natural question is what
do we do if we cannot ﬁnd this analytic function in the τ-plane? The next example shows
an alternative approach.
• Example 1.11.2
For our second example of conformal mapping, consider τ =
√
z2 + a2. To illustrate
this mapping we have constructed two Argand diagrams; one is for the z-plane while the
second is for the τ-plane. Figure 1.11.4 shows how a particular boundary in the z-plane
maps into the τ-plane. The advantage here is that the inﬁnitely thin ﬁlament or peg located
at z = 0 is completely eliminated in the τ-plane.
One source of concern is the presence of the square root; for any value of z we would
have two possible solutions. We make the mapping unique by requiring that ℑ(τ) ≥0.

Complex Variables
63
                                                                                                                  


                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        













x
z−plane
σ
τ−plane
y
ρ
−a
a
0
z2= ai
α
α
α
1 
3
2
= 0
= 0
z1 
z3
+
_
ρ  = 
ρ  = 
ρ  = 
1 
2
3
                                          
                                          
                     




















      


      


      


      


      


      


Figure 1.11.4: The conformal mapping between the z-plane and τ-plane achieved by the conformal map-
ping τ =
√
z2 + a2.
To better understand this transformation, Figure 1.11.5 illustrates various lines of
constant ℜ(τ/a) and ℑ(τ/a) as a function of x/a and y/a. This ﬁgure was constructed
using the MATLAB code:
clear;
% compute τ for various values of z
for jj = 1:40
y = 0.05 * jj;
for ii = 1:42
x = 0.05 * (ii-21.5); z = x + i*y; tau(ii,jj) = sqrt(z*z+a*a);
if (imag(tau(ii,jj)) <= 0) tau(ii,jj) = -tau(ii,jj); end
X(ii,jj) = x; Y(ii,jj) = y;
IM(ii,jj) = imag(tau(ii,jj)); REAL(ii,jj) = real(tau(ii,jj));
end; end
% plot the conformal mapping Equation τ =
√
z2 + a2
figure
[C,h] = contour(X,Y,IM,[0.1,0.25,0.5,0.75,1,1.5,2],’k’);
clabel(C,’FontSize’,10,’Color’,’k’,’Rotation’,0)
xlabel(’x’,’FontSize’,20); ylabel(’y’,’FontSize’,20);
hold on
v = [-1,-0.5,-0.25,-0.01,0.01,0.25,0.5,1];
[C,h] = contour(X,Y,REAL,v,’--b’);
clabel(C,’manual’,’FontSize’,10,’Color’,’b’,’Rotation’,0)
As y →∞, lines of constant ℑ(τ/a) become parallel to the boundary y = 0. Only for
smaller values of y, and as we approach the peg at x = 0, do these lines deviate strongly

64
Advanced Engineering Mathematics: A Second Course
0.1
0.25
0.5
0.75
1
1.5
-1
-0.5
-0.25
0.25
0.5
1
x/a
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
y/a
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Figure 1.11.5: Lines of constant ℜ(τ/a) (dashed line) and ℑ(τ/a) (solid lines) as a function of x and y
for the conformal mapping τ =
√
z2 + a2.
from the horizontal as they pass over the obstacle. The smaller the value of ℑ(τ/a) the
more they conform to the shape of the obstacle.
The behavior of lines of constant ℜ(τ/a) are more diﬃcult to understand. There are
two general classes, depending upon whether the absolute value of ℜ(τ/a) is less or greater
than 1. When |ℜ(τ/a)| > 1 they are clearly orthogonal to constant lines of ℑ(τ/a). Positive
values of ℜ(τ/a) exist for x > 0 while negative values occur when x < 0. |ℜ(τ/a)| < 1 for
y ≥a.
This example has two interesting aspects to it. The ﬁrst is the presence of the square
root. The second involves how we will ﬁnd the solution to Laplace’s equation in the τ-plane.
Let us assume that in the original z-plane the solution equals zero along the entire
boundary except along the “peg.” There, the solution equals 1. In the τ-plane the solution
equals zero along the entire boundary except for the segment −a < ρ < a, where σ = 0,
along which the solution equals 1. Instead of ﬁnding an analytic function whose real or
imaginary part satisﬁes this boundary condition, we employ Poisson’s integral formula16
for the half-plane y > 0 or Schwarz integral formula:17
u(x, y) = 1
π
Z ∞
−∞
yf(t)
(x −t)2 + y2 dt.
In the present case, we ﬁnd that
u(ρ, σ) = 1
π
Z a
−a
σ
σ2 + (ξ −ρ)2 dξ
(1.11.4)
= 1
π

tan−1
a −ρ
σ

+ tan−1
a + ρ
σ

.
(1.11.5)
16
Poisson, S. D., 1823: Suite du m´emoire sur les int´egrales d´eﬁnies et sur la sommation des s´eries. J.
´Ecole Polytech., 19, 404–509. See pg. 462.
17 Schwarz, H. A., 1870: ¨Uber die Integration der partiellen Diﬀerentialgleichung∂2u/∂x2 +∂2u/∂y2 = 0
f¨ur die Fl¨ache eines Kreises. Vierteljahrsschr. Naturforsch. Ges. Z¨urich, 15, 113–128.

Complex Variables
65
0.05
0.05
0.2
0.2
0.4
0.6
0.8
x
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
y
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Figure 1.11.6: The solution of Laplace’s equation when the solution (potential) along the boundary equals
zero except along the peg located at x = 0. There the solution (potential) equals one.
Given Equation 1.11.5 we can compute the solution as follows: For a speciﬁc value of
x and y, we ﬁnd the corresponding value of ρ and σ. Equation 1.11.5 gives us the solution
to Laplace’s equation at that point and the corresponding x and y. The MATLAB code is:
clear; a = 1;
for jj = 1:100
y = 0.02 * jj;
for ii = 1:202
x = 0.02 * (ii-101.5); z = x + i*y; tau = sqrt(z*z+a*a);
if (imag(tau) <= 0) tau = -tau; end
sigma = imag(tau); rho = real(tau);
X(ii,jj) = x; Y(ii,jj) = y;
% Equation 1.11.5
arg1 = (a-rho)/sigma; arg2 = (a+rho)/sigma;
T(ii,jj) = (atan(arg1)+atan(arg2)) / pi;
end; end
% plot the solution to Laplace’s equation
figure
[C,h] = contourf(X,Y,T,[0,0.05,0.2,0.4,0.6,0.8],’k’);
colormap autumn
clabel(C,’FontSize’,10,’Color’,’k’,’Rotation’,0)
xlabel(’x’,’FontSize’,20); ylabel(’y’,’FontSize’,20);
Figure 1.11.6 illustrates this solution.
⊓⊔
So far we have not presented a strategy for ﬁnding our conformal mappings.
One
method would be to simply experiment with transforms that had been used in similar

66
Advanced Engineering Mathematics: A Second Course
problems. Fortunately, during the 1860s, two German mathematicians, E. B. Christoﬀel18
(1829–1900) and H. A. Schwarz19 (1843–1921), developed a very popular method of mapping
a polygon into a half plane. Example 1.11.1 illustrated one of their transforms. Indeed, if
we imagine that the boundary of the polygon is constructed from a thin wire, the purpose
of the Schwarz-Christoﬀel transformation is to unbend the corners so that the wire becomes
straight.
Our derivation begins by considering a mapping z = f(τ) where
dz
dτ = C(τ −ρ1)k1(τ −ρ2)k2 · · · (τ −ρn)kn,
(1.11.6)
and ρ1, ρ2, . . . , ρn are any n points arranged in order along the real axis in the τ-plane such
that ρ1 < ρ2 < . . . < ρn. Here the ki’s are real constants and C is a real or complex
constant. By taking the logarithm of both sides of Equation 1.11.6 we ﬁnd that
log
dz
dτ

= log(C) + k1 log(τ −ρ1) + k2 log(τ −ρ2) + · · · + kn log(τ −ρn).
(1.11.7)
We have assumed that the principal value20 of each logarithm is taken. The local magniﬁ-
cation factor of the mapping from the τ-plane to the z-plane equals dz/dτ, while the angle
of dz/dτ gives the angle through which a small portion of the mapped curve in the τ-plane
is rotated by the mapping. This angle is given by
̸ )
dz
dτ

= ̸ ) (C) + k1̸ ) (τ −ρ1) + k2̸ ) (τ −ρ2) + · · · + kn̸ ) (τ −ρn).
(1.11.8)
Equation 1.11.8 follows by ﬁrst taking the imaginary part of Equation 1.11.7 and then
noting that ̸ ) (C) = ℑ[log(C)].
Let the point (ρ, σ) = (−∞, 0) in the τ-plane be mapped into the point z∗in the
z-plane. See Figure 1.11.7. If we consider the image of a point ρ as it moves to the right
along the negative real axis in the τ-plane, then all of the ρ −ρi are real and negative as
long as ρ < ρ1. Hence the angles for all of the ρ−ρi are constant and equal to π in Equation
1.11.8. Therefore, this equation simpliﬁes to
̸ )
dz
dτ

= ̸ ) (C) + (k1 + k2 + · · · + kn)π.
(1.11.9)
Thus the portion of the ρ axis to the left of the point ρ1 is mapped into a straight line
segment, making the angle deﬁned by Equation 1.11.9 with the real axis in the z-plane, and
extending from z∗to z1 the image of ρ −ρ1.
Now as the point ρ crosses the point ρ1 on the real axis, the real number ρ−ρ1 becomes
positive so that its angle abruptly changes from π to 0. Hence ̸ ) (dz/dτ) abruptly decreases
by an amount k1π and then remains constant as τ travels from ρ1 to ρ2. It follows that
18 Christoﬀel, E. B., 1868: Sul problema delle temperature stazionarie e la rappresentazione di una data
superﬁcie.
Ann.
Mat.
Pura Appl., Series 2, 1, 89–103; Christoﬀel, E. B., 1870: Sopra un problema
proposto da Dirichlet. Ann. Mat. Pura Appl., Series 2, 4, 1–9.
19 Schwarz, H. A., 1868: ¨Uber einige Abbildungsaufgaben. J. Reine Angew. Math., 70, 105–120.
20 For the complex number z = reθi, r ̸= 0, the principal value of the logarithm is log(z) = ln(r) + θi,
where θ must lie between 0 and 2π.

Complex Variables
67
                                                                                                                                                                                                                                                                                                                                                               












                                                                            

 
x
z−plane
σ
τ−
ρ
plane
y
1 
2
n
α
α
1 
2
z
z1 
2z
*
ρ
ρ
ρ
              













                     




















      


      


      


      


      


      


              













              













Figure 1.11.7: Diagram used in the derivation of the Schwarz-Christoﬀel method.
the image of the segment (ρ1ρ2) in the z-plane makes an angle of −k1π with the segment
(z∗z1).
Proceeding in this way, we see that each segment (ρn, ρn+1) is mapped into a line
segment (zn, zn+1) in the z-plane, making the angle of −knπ with the segment previously
mapped. Thus, if the interior angle of the resultant polynomial contour at the point zn is
to have the magnitude αn, we must set π −αn = −knπ, or kn = αn/π −1 in Equation
1.11.6. After an integration, we then conclude that the mapping
z = C
Z τ
(η −ρ1)k1(η −ρ2)k2 · · · (η −ρn)kn dη + K,
(1.11.10)
where the arbitrary complex constants C and K map the real axis σ = 0 of the τ-plane into
a polynomial boundary in the z-plane in such a way that the vertices z1, z2, . . . , zn with
interior angles α1, α2, . . . , αn are the images of the points ρ1, ρ2, . . . , ρn.
For the ﬁnal segment τ −ρ > ρn the numbers τ −ρi are all real, positive, and equal to
zero, so that this segment is rotated through the angle
̸ )(dz/dτ) = ̸ ) (C),
ρ > ρn.
(1.11.11)
For a closed polynomial the sum of the interior angles is
α1 + α2 + · · · + αn = (n −2)π.
(1.11.12)
Therefore,
k1 + k2 + · · · + kn = (n −2)π
π
−n = −2.
(1.11.13)
Thus, according to Equations 1.11.8 and 1.11.11, the two inﬁnite segments of the line σ = 0
are rotated through the angle ̸ ) (C) −2π and ̸ ) (C), as is clearly necessary for a closed
ﬁgure.
What roles do C and K play? Because C is often complex, this constant introduces
any necessary magniﬁcation and rotation of the transformation so that any prescribed
polynomial in the z-plane is made to correspond point by point to the real axis σ = 0 in

68
Advanced Engineering Mathematics: A Second Course
                                                                                    


                                                                                                                                                                                                                                                                                                                           














                                                                                                                                                      














  

 
 
  

                                                                                                                  


x
z−plane
σ
τ−
ρ
plane
1
y
α
β
C
A
B
C
A
B
z1 
= 1
2
z
0
C
= 0
ρ  = 
ρ  = 
1 
2
                                         
                                         
              













                     




















      


      


      


      


              













              













Figure 1.11.8: The complex z- and τ-planes used in Example 1.11.4.
the τ-plane. In fact, this correspondence can be set up in inﬁnitely many ways, in that
three of the numbers ρ1, ρ2, . . . , ρn can be determined arbitrarily. Finally, the mapping can
be shown to establish a one-to-one correspondence between points in the interior of the
polygon in the z-plane and points in the upper half of the τ-plane.
• Example 1.11.3
Let us derive the conformal mapping used in Example 1.11.2. Referring back to Figure
1.11.4, we see that α1 = π/2, k1 = −1/2, and ρ1 = −a at z1 = 0−; α2 = 2π, k2 = 1, and
ρ2 = 0 at z2 = ai; and α3 = π/2, k3 = −1/2, and ρ3 = a at z3 = 0+. Therefore, from
Equation 1.11.6,
dz
dτ = C(τ + a)−1/2τ(τ −a)−1/2 = C
τ
√
τ 2 −a2 .
(1.11.14)
Integrating this diﬀerential equation,
z = C
p
τ 2 −a2 + K.
(1.11.15)
Because the point ρ1 = −a corresponds to z = 0−, K = 0. Similarly, at ρ2 = 0, we have
that
ai = C
p
−a2,
or
C = 1.
(1.11.16)
Therefore, the conformal mapping is given by z =
√
τ 2 −a2, or τ =
√
z2 + a2.
⊓⊔
• Example 1.11.4
Consider the triangle ABC located in the z-plane as shown on Figure 1.11.8. Here
we desire to map the interior space of this triangle into the upper half of the τ-plane. At
point C, points along the boundary and to the left of C are to be mapped out to −∞in
the τ-plane while points along the boundary and to the right of C are mapped to +∞.
From Equation 1.11.6 we have that
dz
dτ = C′τ α/π−1(τ −1)β/π−1 = Cτ α/π−1(1 −τ)β/π−1.
(1.11.17)

Complex Variables
69
                        


                                                                                                                                                                                                                                                                                                                                                               












                        


A
E
                                                                            

 
 
  

  

 
x
z−plane
σ
τ−plane
y
α
α
z
z1 
z2
3
= −a
= 0
C
B
D
1
−1
E
ρ
C
D
B
A
0
= a
ρ  = 
ρ  = 
ρ  = 
1 
2
3
                                          
                                          
                     




















      


      


      


      


      


      


              













              













Figure 1.11.9: The complex z- and τ-planes used in Example 1.11.5.
Integrating this diﬀerential equation,
z = C
Z τ
ηα/π−1(1 −η)β/π−1 dη + K.
(1.11.18)
Because we want the points τ = 0 and z = 0 to correspond to each other, K = 0. On the
other hand, if we wish τ = 1 and z = 1 to correspond, Equation 1.11.18 yields
C
Z 1
0
ηα/π−1(1 −η)β/π−1 dη = C Γ(α/π)Γ(β/π)
Γ[(α + β)/π] = 1,
(1.11.19)
where Γ(·) is the gamma function deﬁned by
Γ(x) =
Z ∞
0
tx−1e−t dt.
(1.11.20)
Consequently,
C = Γ[(α + β)/π]
Γ(α/π)Γ(β/π),
(1.11.21)
and
z = Γ[(α + β)/π]
Γ(α/π)Γ(β/π)
Z τ
0
ηα/π−1(1 −η)β/π−1 dη.
(1.11.22)
A noteworthy aspect of this example is that the conformal mapping is given by an integral
and not some analytic expression.
⊓⊔
• Example 1.11.5
Consider the domain lying in the upper half of the z-plane except for a triangular section
BCD shown in Figure 1.11.9. We wish to construct the Schwarz-Christoﬀel transformation
that maps this domain into the upper half of the τ-plane. From Equation 1.11.6 we have
that
dz
dτ = C′(τ + 1)(π−α)/π−1τ (π+2α)/π−1(τ −1)(π−α)/π−1
(1.11.23)
= C′
τ 2α/π
(τ 2 −1)α/π = C
τ 2α/π
(1 −τ 2)α/π .
(1.11.24)

70
Advanced Engineering Mathematics: A Second Course
                                      


















                                                                                 


                      










                                          


            





                                                


F
F
A
E
                                                                                                                  


 
 
 
 
 
 
  

x
z−plane
σ
τ−plane
y
ρ
1 
−1
−a
a
1
B
C
D
C
D
E
A
B
A
0
ρ  = 
ρ  = 
ρ  = 
ρ  = 
2
3
4
5
ρ  = 
                                          
                                           
      


      


      


      


      


      


      


      


      


      


              













Figure 1.11.10: The complex z- and τ-planes used in Example 1.11.6 with a < 1.
Integrating this diﬀerential equation,
z = C
Z τ
0
η2α/π
(1 −η2)α/π dη + K.
(1.11.25)
If we want the point τ = 0 to correspond to the point z = ki, then K = ki. On the other
hand, if the point τ = 1 corresponds to z = a, then
a = C
Z 1
0
η2α/π
(1 −η2)α/π dη + ki.
(1.11.26)
Solving for C,
C =
√π(a −ki)
Γ
 α/π + 1
2

Γ(1 −α/π).
(1.11.27)
Therefore, the ﬁnal answer is
z =
√π(a −ki)
Γ
 α/π + 1
2

Γ(1 −α/π)
Z τ
0
η2α/π
(1 −η2)α/π dη + ki.
(1.11.28)
⊓⊔
• Example 1.11.6
Consider the domain within the L-shaped boundary shown in Figure 1.11.10. We wish
to construct the Schwarz-Christoﬀel transform that maps the interior into the upper half
of the τ-plane. Note that we broke the boundary in such a manner that points slightly to
the left of point A are mapped to −∞while points slightly below the point A are mapped
to +∞.
Because a < 1, Equation 1.11.6 gives
dz
dτ = C(τ + 1)−1/2(τ + a)−1/2τ −1/2(τ −a)−1/2(τ −1)1/2.
(1.11.29)

Complex Variables
71
                    









                                                      


























                        

                    

ai
                                      

σ
plane
τ−
ρ
1
y
x
plane
z−
                      
                      





















      


      


      


                       






















A
E
D
C
B
A
D
E
B
C
Figure 1.11.11: The complex z- and τ-planes used in Example 1.11.7.
Integrating this diﬀerential equation,
z = C
Z τ
0
(η −1) dη
η
p
(η2 −1)(η2 −a2)
+ K = C
a
Z τ
0
(η −1) dη
η
p
(1 −η2)(1 −p2η2)
+ K,
(1.11.30)
where p2 = 1/a2. To compute C and K, we would need further information.
⊓⊔
• Example 1.11.7
Let us derive the conformal mapping, Equation 1.11.2, used in Example 1.11.1. The
z−and τ−planes are shown in Figure 1.11.11. From this ﬁgure we see that α1 = 3π/2,
α2 = π/2, α3 = π/2, ρ1 = 1, ρ2 = 0−, and ρ3 = 0+. This yields
dz
dτ = K(τ −1)(3π)/(2π)−1(τ −0−)(π)/(2π)−1(τ −0+)(π)/(2π)−1 = K
√τ −1
τ
.
(1.11.31)
Integrating Equation 1.11.31, we ﬁnd that
z = 2K
√
τ −1 −arctan
 √
τ −1

+ C = 2K
√
τ −1 + i
2 log
1 + i√τ −1
1 −i√τ −1

+ C.
(1.11.32)
Because at τ = 1, z = a/2, we have C = a/2.
The computation of K is more complicated. Referring to Figure 1.11.11, we note that
Z C
B
dz =
Z C′
B′ K
√τ −1
τ
dτ.
(1.11.33)
Setting τ = r eθi with r →0, Equation 1.11.34 becomes
a
2 = K lim
r→0
Z 0
π
√
r eθi −1
r eθi
ir eθi dθ = Kπ.
(1.11.34)
Thus K = a/(2π) and we recover Equation 1.11.2.

72
Advanced Engineering Mathematics: A Second Course
                                      

                            













                                                               


σ
plane
τ−
ρ
x
y
plane
z−
1 
2
3
(−π,π)
ρ
ρ
ρ
                     
                     
                      





















      


      


        



      


                       






















A
C
B
C
A
B
Problem 3
                                                                                                                                                                                                                                                            

















                                      

                                                


σ
plane
τ−
ρ
1 
2
3
y
x
plane
z−
ρ
ρ
ρ
                      
                      
                      





















      


      


        



B
C
A
B
C
A
Problem 4
                                                                                                                                                                                                   












                    

                                        

σ
plane
τ−
ρ
y
x
1
1
plane
z−
7π/4
                      
      


      


C
D
B
A
A
B
C
D
Problem 5
Problems
1. Verify that the function τ = ez maps the strip 0 < ℑ(z) < π into the half-plane ℑ(τ) > 0.
2. Verify that the function τ 2 = 1 −ez maps the strip −π < ℑ(z) < π, except for the
negative real axis, into the upper half of the τ-plane.
3. Use the Schwarz-Christoﬀel method to ﬁnd the conformal mapping that maps the quarter
plane x > −π, y < π into the upper half of the τ-plane. We require that the point (−π, π)
in the z-plane maps to the point (0, 0) in the τ-plane.
4. Use the Schwarz-Christoﬀel method to ﬁnd the conformal mapping that maps the sector
lying between the x-axis and the line θ = π/3 into the upper half of the τ-plane. We require
that the point (0, 0) in the z-plane maps to the point (0, 0) in the τ-plane.
5. Use the Schwarz-Christoﬀel method to ﬁnd the conformal mapping that maps the portion
of the z-plane deﬁned by 0 < r < ∞, 0 < θ < 7π/4 into the upper half of the τ-plane. We

Complex Variables
73
                                      


















                                      

                                                         


                                      


















σ
1
−1
plane
τ−
ρ
x
y
plane
z−
ρ  = 
ρ  = 
2
1 
                     
                     
                   


















      


      


                       






















B
C
B
C
a
−a
Problem 6
                        











                                                               


                                                         


                                      

y
z−plane
x
σ
1 
1
2
−1
plane
τ−
ρ
ρ  = 
ρ  = 
                     
                     
                      





















      


      


                       






















a B
C
B
C
Problem 7
                    









                        

                              


ai
                                                         


          
σ
plane
τ−
ρ
1
y
x
plane
z−
                     
                     
                      





















      


      


C
D
A
B
C
D
A
B
Problem 8
require that the points (0, 0) and (1, 0) in the z-plane map to the points (0, 0) and (1, 0) in
the τ-plane, respectively.
6. Use the Schwarz-Christoﬀel method to ﬁnd the conformal mapping that maps the domain
|x| < a, 0 < y into the upper half of the τ-plane. Let the point (−a, 0) become the point
(−1, 0) while the point (a, 0) becomes the point (1, 0).
7. Use the Schwarz-Christoﬀel method to ﬁnd the conformal mapping that maps the region
x > 0, 0 < y < a into the upper half of the τ-plane. We require that the point (0, a) maps
to (−1, 0) in the τ-plane while the point (0, 0) maps to (1, 0) in the τ-plane.
8. Use the Schwarz-Christoﬀel method to ﬁnd the conformal mapping that maps the region
shown in the ﬁgure into the upper half of the τ-plane. We require that the points (0, 0) and
(0, a) in the z-plane map to the points (0, 0) and (1, 0) in the τ-plane, respectively.
9. Construct a transform between a z-plane which has a barrier that runs parallel to the
x-axis from z = L + πL i to ∞+ πL i and a τ-plane that has no barrier.

74
Advanced Engineering Mathematics: A Second Course
                                                                                                                                                        







                                                                                                                              






                                                                            

                                                                                             


  

  

  

  

x
z−plane
σ
τ−
ρ
plane
y
A
B
C
D
A
B
C
D
α
              













              













                                           
                     




















              













              













1
Problem 9a
Step 1: Begin by using the Schwarz-Christoffel method to show that the conformal mapping
pictured in Figure 9a is given by
dz
dτ = Cτ k1(τ −1)k2,
where k1 = −α/(2π) and k2 = α/π −1.
Step 2: Next, consider the limit as the points B and D in the z-plane in Figure 9a move
out to inﬁnity (so that α →2π) and we obtain Figure 9b. Consequently, the transform
approaches
dz
dτ = C τ −1
τ
,
or
z = C[τ −log(τ)] + K.
Here we have taken the principal branch of the logarithm so that log(z) = ln(|z|)+iθ where
0 ≤θ ≤π. (We do not require that 0 ≤θ < 2π because we are always in the upper
half-plane.)
Step 3: Following Example 1.11.7, consider the area around τ = 0. Show that
dz ≈−C dτ
τ = −iC dθ,
where τ = r eθi. Integrating from point B′
1 to point B′
2, show that C = L.
Step 4: To compute K, note that if the point C, located at z = L + πLi, corresponds to
the point C′, located at τ = 1, then K = πLi.
10. Use conformal mapping to solve Laplace’s equation for the inﬁnite strip −∞< x < ∞,
0 ≤y ≤π. The solution equals zero everywhere along the boundary except for x > 0,
y = 0, where u(x, 0) = 1.
Step 1: Consider the mapping τ = ez. Show that ρ = ex cos(y) and σ = ex sin(y). In
particular, (∞, π) →(−∞, 0), (0, π) →(−1, 0), (−∞, y) →(0, 0), (0, 0) →(1, 0), and
(∞, 0) →(∞, 0).

Complex Variables
75
                                                                            

  

  

                                                                                                                                                                



 
  

  

x
z−plane
σ
τ−
ρ
plane
y
A
C
D
A
B
B
D
B
B
C
              













                                           
                     




















              













              













1
1
2
1
2
Problem 9b
                                                                                             


                                                                                                                                                                                                                                                                                                                                                         














                                                                                                                                                                                                                                                                                             














                                                                            

x
z−plane
σ
τ−
ρ
plane
0
1
y
= 0
z1 
z2 = 1
α
ρ  = 
ρ  = 
1 
2
                                          
                                          
              













                     




















      


      


      


              













              













Figure 1.11.12: The conformal mapping between the z-plane and τ-plane achieved by the conformal
mapping τ = zπ/α.
Step 2: Using Poisson’s integral formula for the upper half-plane, show that
u(ρ, σ) = 1
π
π
2 −tan−1
1 −x
y

= 1 −1
π tan−1

y
x −1

.
Step 3: Show that
u(x, y) = 1 −1
π tan−1

ex sin(y)
ex cos(y) −1

.
11. Use conformal mapping to solve Laplace’s equation for a pie-shaped sector in the ﬁrst
quadrant. See Figure 1.11.12. The solution equals zero along the entire boundary except
for 0 < x < 1 where it equals one.
Step 1: Show that the mapping z = τ α/π or τ = zπ/α maps the pie-shaped sector into the
half-plane ℑ(τ) > 0. See Figure 1.11.12.

76
Advanced Engineering Mathematics: A Second Course
Step 2: Using Poisson’s integral formula for the upper half-plane, show that
u(ρ, σ) = 1
π cot−1
ρ2 + σ2 −ρ
σ

.
Step 3: Show that
u(r, θ) = 1
π cot−1
rπ/α −cos(πθ/α)
sin(πθ/α)

,
where x = r cos(θ) and y = r sin(θ).
12. Use conformal mapping to solve Laplace’s equation for the semi-inﬁnite strip 0 ≤x ≤a,
0 ≤y < ∞, where u(x, 0) = 1, 0 ≤x ≤a, and u(0, y) = u(a, y) = 0, 0 ≤y < ∞.
Step 1: Consider the mapping τ = −cos(πz/a). Show that
ρ = −cos(πx/a) cosh(πy/a),
and
σ = sin(πx/a) sinh(πy/a).
In particular, (0, ∞) →(−∞, 0), (0, 0) →(−1, 0), (a/2, 0) →(0, 0), (a, 0) →(1, 0), and
(a, ∞) →(∞, 0).
Step 2: Using Poisson’s integral formula for the upper half-plane, show that
u(ρ, σ) = 1
π cot−1
ρ2 + σ2 −1
2σ

.
Step 3: Show that
u(x, y) = 1
π cot−1
h
sinh2πy
a

−sin2πx
a
i 
2 sin
πx
a

sinh
πy
a

= 2
π tan−1

sin
πx
a
 
sinh
πy
a

.
Step 4: In the case that boundary conditions read u(0, y) = u(a, y) = 1 for 0 ≤y < ∞
and u(x, 0) = 0 for 0 ≤x ≤a, how could you use the solution in Step 3 to solve this new
problem?
Further Readings
Ablowitz, M. J., and A. S. Fokas, 2003: Complex Variables: Introduction and Applications.
Cambridge University Press, 660 pp. Covers a wide variety of topics, including complex
numbers, analytic functions, singularities, conformal mapping and the Riemann-Hilbert
problem.
Carrier, G. F., M. Krook, and C. E. Pearson, 1966: Functions of a Complex Variable:
Theory and Technique. McGraw-Hill Book Co., 438 pp. Graduate-level textbook.
Churchill, R. V., 1960: Complex Variables and Applications. McGraw-Hill Book Co., 297
pp. Classic textbook.
Flanigan, F. J., 1983: Complex Variables. Dover, 364 pp. A crystal clear exposition and
emphasis on an intuitive understanding of complex analysis.

t > 3
(c,0)
t < 3
Chapter 2
Advanced Transform Methods
In their course work, most engineering students are introduced to the concept of the
Fourier and Laplace transforms. The presentations are limited because the student has not
studied complex variables. Having presented this topic in the previous chapter, the reader
is ready to deepen his/her ability to use these transform methods.
This chapter deals with two important aspects of transform methods. In the past you
may have inverted Fourier and Laplace transforms using partial fractions, tables and some
general properties of the transform. Often these techniques fail and here we show how the
power of complex variables can overcome these diﬃculties.
The reason that Laplace transforms are taught to engineers is their ability to solve
ordinary diﬀerential equations. When it comes to partial diﬀerential equations the student
is only taught one method: separation of variables. In Sections 2.4 through 2.6 we show
how Laplace transforms can be used to solve the wave, heat, and Laplace equations.
2.1 INVERSION OF FOURIER TRANSFORMS BY CONTOUR INTEGRATION
Although we may ﬁnd the inverse by direct integration or partial fractions, in many
instances the Fourier transform does not lend itself to these techniques. On the other hand,
if we view the inverse Fourier transform as a line integral along the real axis in the complex
ω-plane, then some of the techniques that we developed in Chapter 1 can be applied to this
problem. To this end, we rewrite the inversion integral for the Fourier transform as
f(t) = 1
2π
Z ∞
−∞
F(ω)eitω dω = 1
2π
I
C
F(z)eitz dz −1
2π
Z
CR
F(z)eitz dz,
(2.1.1)
77

78
Advanced Engineering Mathematics: A Second Course
where C denotes a closed contour consisting of the entire real axis plus a new contour
CR that joins the point (∞, 0) to (−∞, 0). There are countless possibilities for CR. For
example, it could be the loop (∞, 0) to (∞, R) to (−∞, R) to (−∞, 0) with R > 0. However,
any choice of CR must be such that we can compute
R
CR F(z)eitz dz. When we take that
constraint into account, the number of acceptable contours decreases to just a few. The
best is given by Jordan’s lemma.1
Jordan’s lemma: Suppose that, on a circular arc CR with radius R and center at the
origin, f(z) →0 uniformly as R →∞. Then
(1)
lim
R→∞
Z
CR
f(z)eimz dz = 0,
(m > 0)
(2.1.2)
if CR lies in the ﬁrst and/or second quadrant;
(2)
lim
R→∞
Z
CR
f(z)e−imz dz = 0,
(m > 0)
(2.1.3)
if CR lies in the third and/or fourth quadrant;
(3)
lim
R→∞
Z
CR
f(z)emz dz = 0,
(m > 0)
(2.1.4)
if CR lies in the second and/or third quadrant; and
(4)
lim
R→∞
Z
CR
f(z)e−mz dz = 0,
(m > 0)
(2.1.5)
if CR lies in the ﬁrst and/or fourth quadrant.
Technically, only (1) is actually Jordan’s lemma, while the remaining points are varia-
tions.
Proof : We shall prove the ﬁrst part; the remaining portions follow by analog. We begin by
noting that
|IR| =

Z
CR
f(z)eimz dz
 ≤
Z
CR
|f(z)|
eimz |dz|.
(2.1.6)
Now
|dz| = R dθ,
|f(z)| ≤MR,
(2.1.7)
eimz =
exp(imReθi)
 = |exp{imR[cos(θ) + i sin(θ)]}| = e−mR sin(θ).
(2.1.8)
Therefore,
|IR| ≤RMR
Z θ1
θ0
exp[−mR sin(θ)] dθ,
(2.1.9)
1 Jordan, C., 1894: Cours D’Analyse de l’ ´Ecole Polytechnique. Vol. 2. Gauthier-Villars, pp. 285–286.
See also Whittaker, E. T., and G. N. Watson, 1963: A Course of Modern Analysis. Cambridge University
Press, p. 115.

Advanced Transform Methods
79
where 0 ≤θ0 < θ1 ≤π. Because the integrand is positive, the right side of Equation 2.1.9
is largest if we take θ0 = 0 and θ1 = π. Then
|IR| ≤RMR
Z π
0
e−mR sin(θ) dθ = 2RMR
Z π/2
0
e−mR sin(θ) dθ.
(2.1.10)
We cannot evaluate the integrals in Equation 2.1.10 as they stand.
However, because
sin(θ) ≥2θ/π if 0 ≤θ ≤π/2, we can bound the value of the integral by
|IR| ≤2RMR
Z π/2
0
e−2mRθ/π dθ = π
mMR
 1 −e−mR
.
(2.1.11)
If m > 0, |IR| tends to zero with MR as R →∞.
⊓⊔
Consider now the following inversions of Fourier transforms:
• Example 2.1.1
For our ﬁrst example we ﬁnd the inverse for
F(ω) =
1
ω2 −2ibω −a2 −b2 ,
a, b > 0.
(2.1.12)
From the inversion integral,
f(t) = 1
2π
Z ∞
−∞
eitω
ω2 −2ibω −a2 −b2 dω,
(2.1.13)
or
f(t) = 1
2π
I
C
eitz
z2 −2ibz −a2 −b2 dz −1
2π
Z
CR
eitz
z2 −2ibz −a2 −b2 dz,
(2.1.14)
where C denotes a closed contour consisting of the entire real axis plus CR.
Because
f(z) = 1/(z2 −2ibz −a2 −b2) tends to zero uniformly as |z| →∞and m = t, the second
integral in Equation 2.1.14 vanishes by Jordan’s lemma if CR is a semicircle of inﬁnite radius
in the upper half of the z-plane when t > 0 and a semicircle in the lower half of the z-plane
when t < 0.
Next we must ﬁnd the location and nature of the singularities. They are located at
z2 −2ibz −a2 −b2 = 0,
or
z = ±a + bi.
(2.1.15)
Therefore we can rewrite Equation 2.1.14 as
f(t) = 1
2π
I
C
eitz
(z −a −bi)(z + a −bi) dz.
(2.1.16)
Thus, all of the singularities are simple poles.
Consider now t > 0.
As stated earlier, we close the line integral with an inﬁnite
semicircle in the upper half-plane. See Figure 2.1.1. Inside this closed contour there are
two singularities: z = ±a + bi. For these poles,
Res

eitz
z2 −2ibz −a2 −b2 ; a + bi

=
lim
z→a+bi
(z −a −bi)eitz
(z −a −bi)(z + a −bi)
(2.1.17)
= eiate−bt
2a
= e−bt
2a [cos(at) + i sin(at)],
(2.1.18)

80
Advanced Engineering Mathematics: A Second Course
                                 
original   contour
x
y
a+bi
-a+bi
R
R
C     for t > 0
C     for t < 0
Figure 2.1.1: Contour used to ﬁnd the inverse of the Fourier transform, Equation 2.1.12. The contour C
consists of the line integral along the real axis plus CR.
where we used Euler’s formula to eliminate eiat. Similarly,
Res

eitz
z2 −2ibz −a2 −b2 ; −a + bi

= −e−bt
2a [cos(at) −i sin(at)].
(2.1.19)
Consequently, the inverse Fourier transform follows from Equation 2.1.16 after applying the
residue theorem, and equals
f(t) = −e−bt
2a sin(at)
(2.1.20)
for t > 0.
For t < 0, the semicircle is in the lower half-plane because the contribution from the
semicircle vanishes as R →∞. Because there are no singularities within the closed contour,
f(t) = 0. Therefore, we can write in general that
f(t) = −e−bt
2a sin(at)H(t).
(2.1.21)
⊓⊔
• Example 2.1.2
Let us ﬁnd the inverse of the Fourier transform
F(ω) =
e−ωi
ω2 + a2 ,
(2.1.22)
where a is real and positive.
From the inversion integral,
f(t) = 1
2π
Z ∞
−∞
ei(t−1)ω
ω2 + a2 dω = 1
2π
I
C
ei(t−1)z
z2 + a2 dz −1
2π
Z
CR
ei(t−1)z
z2 + a2 dz,
(2.1.23)

Advanced Transform Methods
81
where C denotes a closed contour consisting of the entire real axis plus CR. The contour
CR is determined by Jordan’s lemma because 1/(z2 + a2) →0 uniformly as |z| →∞. Since
m = t −1, the semicircle CR of inﬁnite radius lies in the upper half-plane if t > 1 and in
the lower half-plane if t < 1. Thus, if t > 1,
f(t) = 1
2π (2πi)Res
ei(t−1)z
z2 + a2 ; ai

= e−a(t−1)
2a
,
(2.1.24)
whereas for t < 1,
f(t) = 1
2π (−2πi)Res
ei(t−1)z
z2 + a2 ; −ai

= ea(t−1)
2a
.
(2.1.25)
The minus sign in front of the 2πi arises from the clockwise direction or negative sense of
the contour. We can write the inverse as the single expression
f(t) = e−a|t−1|
2a
.
(2.1.26)
⊓⊔
• Example 2.1.3
Let us evaluate the integral
Z ∞
−∞
cos(kx)
x2 + a2 dx,
(2.1.27)
where a, k > 0.
We begin by noting that
Z ∞
−∞
cos(kx)
x2 + a2 dx = ℜ
Z ∞
−∞
eikx
x2 + a2 dx

= ℜ
Z
C1
eikz
z2 + a2 dz

,
(2.1.28)
where C1 denotes a line integral along the real axis from −∞to ∞. A quick check shows
that the integrand of the right side of Equation 2.1.28 satisﬁes Jordan’s lemma. Therefore,
Z ∞
−∞
eikx
x2 + a2 dx =
I
C
eikz
z2 + a2 dz = 2πi Res

eikz
z2 + a2 ; ai

(2.1.29)
= 2πi lim
z→ai
(z −ai)eikz
z2 + a2
= π
a e−ka,
(2.1.30)
where C denotes the closed inﬁnite semicircle in the upper half-plane. Taking the real and
imaginary parts of Equation 2.1.30,
Z ∞
−∞
cos(kx)
x2 + a2 dx = π
a e−ka
and
Z ∞
−∞
sin(kx)
x2 + a2 dx = 0.
(2.1.31)
⊓⊔

82
Advanced Engineering Mathematics: A Second Course
                



                



                    

              

                    

 
x
y
R
R
C     for t > 0
C     for t < 0
original   contour
a
-a
Figure 2.1.2: Contour used in Example 2.1.32.
• Example 2.1.4
Let us now invert the Fourier transform F(ω) = 2a/(a2 −ω2), where a is real. The
interesting aspect of this problem is the presence of singularities at ω = ±a that lie along
the contour of integration. How do we use contour integration to compute
f(t) = a
π
Z ∞
−∞
eitω
a2 −ω2 dω?
(2.1.32)
The answer to this question involves the concept of Cauchy principal value integrals,
which allows us to extend the conventional deﬁnition of integrals to include integrands
that become inﬁnite at a ﬁnite number of points.
See Section 1.10.
Thus, by treating
Equation 2.1.32 as a Cauchy principal value integral, we again convert it into a closed
contour integration by closing the line integration along the real axis as shown in Figure
2.1.2. The semicircles at inﬁnity vanish by Jordan’s lemma and
f(t) = a
π
I
C
eitz
a2 −z2 dz.
(2.1.33)
For t > 0,
f(t) = −2πia
π
1
2Res

eitz
z2 −a2 ; −a

−2πia
π
1
2Res

eitz
z2 −a2 ; a

.
(2.1.34)
We have the factor 1
2 because we are only passing over the “top” of the singularity at z = a
and z = −a. Computing the residues and simplifying the results, we obtain f(t) = sin(at).
Similarly, when t < 0,
f(t) = 2πia
π
1
2Res

eitz
z2 −a2 ; −a

+ 2πia
π
1
2Res

eitz
z2 −a2 ; a

= −sin(at).
(2.1.35)

Advanced Transform Methods
83
These results can be collapsed down to the single expression f(t) = sgn(t) sin(at).
⊓⊔
• Example 2.1.5
An additional beneﬁt of understanding inversion by the residue method is the ability
to qualitatively anticipate the inverse by knowing the location of the poles of F(ω). This
intuition is important because many engineering analyses discuss stability and performance
entirely in terms of the properties of the system’s Fourier transform. In Figure 2.1.3 we
graphed the location of the poles of F(ω) and the corresponding f(t). The student should
go through the mental exercise of connecting the two pictures.
⊓⊔
• Example 2.1.6
So far, we used only the ﬁrst two points of Jordan’s lemma.
In this example2 we
illustrate how the remaining two points may be applied.
Consider the contour integral
I
C
cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

dz,
where c > 0 and β, τ are real. Let us evaluate this contour integral where the contour is
shown in Figure 2.1.4.
From the residue theorem,
I
C
cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

dz
= 2πi
∞
X
n=1
Res

cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

; n

+ 2πi Res

cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

; |τ| + βi
2π

+ 2πi Res

cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

; |τ| −βi
2π

.
(2.1.36)
Now
Res

cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

; n

= lim
z→n
(z −n) cos(πz)
sin(πz)
lim
z→n

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

(2.1.37)
= 1
π

e−nc
(τ + 2nπ)2 + β2 +
e−nc
(τ −2nπ)2 + β2

,
(2.1.38)
Res

cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

; |τ| + βi
2π

=
lim
z→(|τ|+βi)/2π
cot(πz)
4π2

(z −|τ| −βi)e−cz
(z + τ/2π)2 + β2/4π2 +
(z −|τ| −βi)e−cz
(z −τ/2π)2 + β2/4π2

(2.1.39)
= cot(|τ|/2 + βi/2) exp(−c|τ|/2π)[cos(cβ/2π) −i sin(cβ/2π)]
4πβi
,
(2.1.40)
2 See Hsieh, T. C., and R. Greif, 1972: Theoretical determination of the absorption coeﬃcient and the
total band absorptance including a speciﬁc application to carbon monoxide. Int. J. Heat Mass Transfer,
15, 1477–1487.

84
Advanced Engineering Mathematics: A Second Course
-plane
f(t)
t
f(t)
t
ω -plane
ω
-plane
f(t)
t
f(t)
t
ω -plane
ω
-plane
f(t)
t
f(t)
t
ω -plane
ω
Figure 2.1.3: The correspondence between the location of the simple poles of the Fourier transform F(ω)
and the behavior of f(t).

Advanced Transform Methods
85
2π
ε
|τ|−ιβ
|τ|+ιβ
2π
Figure 2.1.4: Contour used in Example 2.1.6.
and
Res

cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

; |τ| −βi
2π

=
lim
z→(|τ|−βi)/2π
cot(πz)
4π2

(z −|τ| + βi)e−cz
(z + τ/2π)2 + β2/4π2 +
(z −|τ| + βi)e−cz
(z −τ/2π)2 + β2/4π2

(2.1.41)
= cot(|τ|/2 −βi/2) exp(−c|τ|/2π)[cos(cβ/2π) + i sin(cβ/2π)]
−4πβi
.
(2.1.42)
Therefore,
I
C
cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

dz
= 2i
∞
X
n=1

e−nc
(τ + 2nπ)2 + β2 +
e−nc
(τ −2nπ)2 + β2

+ i
2β
ei|τ| + eβ
ei|τ| −eβ e−c|τ|/2π[cos(cβ/2π) −i sin(cβ/2π)]
−i
2β
ei|τ| + e−β
ei|τ| −e−β e−c|τ|/2π[cos(cβ/2π) + i sin(cβ/2π)]
(2.1.43)
= 2i
∞
X
n=1

e−nc
(τ + 2nπ)2 + β2 +
e−nc
(τ −2nπ)2 + β2

−i
β
sinh(β) cos(cβ/2π) + sin(|τ|) sin(cβ/2π)
cosh(β) −cos(τ)
e−c|τ|/2π,
(2.1.44)
where cot(α) = i(e2iα + 1)/(e2iα −1), and we made extensive use of Euler’s formula.
Let us now evaluate the contour integral by direct integration. The contribution from
the integration along the semicircle at inﬁnity vanishes according to Jordan’s lemma. In-
deed, that is why this particular contour was chosen. Therefore,

86
Advanced Engineering Mathematics: A Second Course
I
C
cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

dz
=
Z iǫ
i∞
cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

dz
+
Z
Cǫ
cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

dz
+
Z −i∞
−iǫ
cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

dz.
(2.1.45)
Now, because z = iy,
Z iǫ
i∞
cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

dz
=
Z ǫ
∞
coth(πy)

e−icy
(τ + 2πiy)2 + β2 +
e−icy
(τ −2πiy)2 + β2

dy
(2.1.46)
= −2
Z ∞
ǫ
coth(πy)(τ 2 + β2 −4π2y2)e−icy
(τ 2 + β2 −4π2y2)2 + 16π2τ 2y2 dy,
(2.1.47)
Z −i∞
−iǫ
cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

dz
=
Z −∞
−ǫ
coth(πy)

e−icy
(τ + 2πiy)2 + β2 +
e−icy
(τ −2πiy)2 + β2

dy
(2.1.48)
= 2
Z ∞
ǫ
coth(πy)(τ 2 + β2 −4π2y2)eicy
(τ 2 + β2 −4π2y2)2 + 16π2τ 2y2 dy,
(2.1.49)
and
Z
Cǫ
cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

dz
=
Z −π/2
π/2

1
πǫeθi −πǫeθi
3
−· · ·

ǫieθi dθ
×

exp(−cǫeθi)
(τ + 2πǫeθi)2 + β2 +
exp(−cǫeθi)
(τ −2πǫeθi)2 + β2

.
(2.1.50)
In the limit of ǫ →0,
I
C
cot(πz)

e−cz
(τ + 2πz)2 + β2 +
e−cz
(τ −2πz)2 + β2

dz
= 4i
Z ∞
0
coth(πy)(τ 2 + β2 −4π2y2) sin(cy)
(τ 2 + β2 −4π2y2)2 + 16π2τ 2y2
dy −
2i
τ 2 + β2
(2.1.51)
= 2i
∞
X
n=1

e−nc
(τ + 2nπ)2 + β2 +
e−nc
(τ −2nπ)2 + β2

−i
β
sinh(β) cos(cβ/2π) + sin(|τ|) sin(cβ/2π)
cosh(β) −cos(τ)
e−c|τ|/2π,
(2.1.52)

Advanced Transform Methods
87
or
4
Z ∞
0
coth(πy)(τ 2 + β2 −4π2y2) sin(cy)
(τ 2 + β2 −4π2y2)2 + 16π2τ 2y2
dy
= 2
∞
X
n=1

e−nc
(τ + 2nπ)2 + β2 +
e−nc
(τ −2nπ)2 + β2

(2.1.53)
−1
β
sinh(β) cos(cβ/2π) + sin(|τ|) sin(cβ/2π)
cosh(β) −cos(τ)
e−c|τ|/2π +
2
τ 2 + β2 .
If we let y = x/2π,
β
π
Z ∞
0
coth(x/2)(τ 2 + β2 −x2) sin(cx/2π)
(τ 2 + β2 −x2)2 + 4τ 2x2
dx
= 2β
∞
X
n=1

e−nc
(τ + 2nπ)2 + β2 +
e−nc
(τ −2nπ)2 + β2

(2.1.54)
−sinh(β) cos(cβ/2π) + sin(|τ|) sin(cβ/2π)
cosh(β) −cos(τ)
e−c|τ|/2π +
2β
τ 2 + β2 .
Problems
By taking the appropriate closed contour, ﬁnd the inverse of the following Fourier transforms
F(ω) by contour integration. The parameter a is real and positive.
1.
1
ω2 + a2
2.
ω
ω2 + a2
3.
ω
(ω2 + a2)2
4.
ω2
(ω2 + a2)2
5.
1
ω2 −3iω −3
6.
1
(ω −ia)2n+2
7.
ω2
(ω2 −1)2 + 4a2ω2
8.
3
(2 −ωi)(1 + ωi)
Then check your answer using MATLAB.
9. Find the inverse of F(ω) = cos(ω)/(ω2 + a2), a > 0, by ﬁrst rewriting the transform as
F(ω) =
eiω
2(ω2 + a2) +
e−iω
2(ω2 + a2)
and then using the residue theorem on each term.
10. Find3 the inverse Fourier transform for
F±(ω) =
e±iω
(ω −ai) (R2eωi −e−ωi) =
e±iω−iω
(ω −ai) (R2 −e−2ωi),
where a > 0 and R > 1.
3 See Scharstein, R. W., 1992: Transient electromagnetic plane wave reﬂection from a dielectric slab.
IEEE Trans. Educ., 35, 170–175.

88
Advanced Engineering Mathematics: A Second Course
Step 1: Show that
f±(t) = 1
2π
Z ∞
−∞
ei(t−1±1)ω
(ω −ai)(R2 −e−2ωi) dω.
Step 2: Show that the singularities consist of simple poles at z = ai and zn = ±nπ+i ln(R),
where n = 0, ±1, ±2, ±3, . . ..
Step 3: For t > 0 show that
f+(t) = 1
2π
Z ∞
−∞
eitω
(ω −ai)(R2 −e−2ωi) dω.
and we must close the contour with an inﬁnite semi-circle in the top half-plane.
Step 4: Show that
Z ∞
−∞
eitω
(ω −ai)(R2 −e−2ωi) dω = 2πi Res

eitz
(z −ai)(R2 −e−2zi); ai

+ 2πi
∞
X
n=−∞
Res

eitz
(z −ai)(R2 −e−2zi); zn

.
where
Res

eitz
(z −ai)(R2 −e−2zi); ai

=
e−at
R2 −e2a ,
and
Res

eitz
(z −ai)(R2 −e−2zi); zn

=
R−te±inπt
2iR2 {±nπ + [ln(R) −a] i},
so that
f+(t) =
i e−at
R2 −e2a +
1
2Rt+2
∞
X
n=−∞
einπt
nπ + [ln(R) −a] i.
Step 5: For the case t < 0, show that we close the contour with an inﬁnite semi-circle in
the bottom half-plane to compute f+(t).
Step 6: Compute the residues of the enclosed singularities in Step 5 and show that f+(t) = 0.
Why?
Step 7: Show that f+(t) equals
f+(t) =
i e−at
R2 −e2a H(t) + H(t)
2Rt+2
∞
X
n=−∞
einπt
nπ + [ln(R) −a] i
at any time t.
Step 8: For F−(ω), show that
f−(t) = 1
2π
Z ∞
−∞
ei(t−2)ω
(ω −ai)(R2 −e−2ωi) dω.
Step 9: For t > 2, show that we close the contour with an inﬁnite semi-circle in the top
half-plane.

Advanced Transform Methods
89
Step 10: Compute the residue of the enclosed singularities and show that
Z ∞
−∞
ei(t−2)ω
(ω −ai)(R2 −e−2ωi) dω = 2πi Res

ei(t−2)z
(z −ai)(R2 −e−2zi); ai

+ 2πi
∞
X
n=−∞
Res

ei(t−2)z
(z −ai)(R2 −e−2zi); zn

.
where
Res

ei(t−2)z
(z −ai)(R2 −e−2zi); ai

=
e−at
R2e−2a −1,
and
Res

ei(t−2)z
(z −ai)(R2 −e−2zi); zn

=
R−t+2e±inπt
2iR2 {±nπ + [ln(R) −a] i}.
so that
f−(t) =
i e−at
R2e−2a −1 +
1
2Rt
∞
X
n=−∞
einπt
nπ + [ln(R) −a] i.
Step 11: For t < 2 we must close the contour with an inﬁnite semi-circle in the bottom
half-plane. Compute the residue of the enclosed singularities and show that f−(t) = 0.
Why?
Step 12: Show that the ﬁnal answer for f−(t) at any time t is
f−(t) =
i e−at
R2e−2a −1H(t −2) + H(t −2)
2Rt
∞
X
n=−∞
einπt
nπ + [ln(R) −a] i.
11. During the solution of the heat equation, Taitel et al.4 inverted the Fourier transform
F(ω) =
cosh(y
√
ω2 + 1 )
√
ω2 + 1 sinh(p
√
ω2 + 1/2)
,
where y and p are real.
Step 1: From the deﬁnition of the Fourier transform, show that
f(t) = 1
2π
I
C
cosh
 y
√
z2 + 1

eizt
√
z2 + 1 sinh
 p
√
z2 + 1/2
dz,
where we have closed the line integral with an inﬁnite semicircle in the upper half-plane if
t > 0. For t < 0 we close the contour in the lower half-plane.
Step 2: For t > 0, show that the enclosed singularities are simple poles that are located at
z = i and p
√
z2 + 1 = 2nπi, n = 0, 1, 2, . . ., or zn = i
p
1 + 4n2π2/p2.
Step 3: Show that
Res

cosh
 y
√
z2 + 1

eizt
√
z2 + 1 sinh
 p
√
z2 + 1/2
; i

= e−t
ip ,
4 Taitel, Y., M. Bentwich, and A. Tamir, 1973: Eﬀects of upstream and downstream boundary conditions
on heat (mass) transfer with axial diﬀusion. Int. J. Heat Mass Transfer, 16, 359–369.

90
Advanced Engineering Mathematics: A Second Course
and
Res

cosh
 y
√
z2 + 1

eizt
√
z2 + 1 sinh
 p
√
z2 + 1/2
; zn

= 2 cos(2nπy/p) exp(−
p
1 + 4n2π2/p2 t)
ip(−1)np
1 + 4n2π2/p2
.
Step 4: For t > 0, show that
f(t) = e−t
p
+ 2
p
∞
X
n=1
(−1)n cos(2nπy/p) exp(−
p
1 + 4n2π2/p2 t)
p
1 + 4n2π2/p2
.
Step 5: For t < 0, show that the enclosed singularities are simple poles located at z = −i
and zn = −i
p
1 + 4n2π2/p2.
Step 6: Show that
Res

cosh
 y
√
z2 + 1

eizt
√
z2 + 1 sinh
 p
√
z2 + 1/2
; −i

= −et
ip,
and
Res

cosh
 y
√
z2 + 1

eizt
√
z2 + 1 sinh
 p
√
z2 + 1/2
; zn

= −2 cos(2nπy/p) exp(
p
1 + 4n2π2/p2 t)
ip(−1)np
1 + 4n2π2/p2
.
Step 7: For t < 0, show that
f(t) = et
p + 2
p
∞
X
n=1
(−1)n cos(2nπy/p) exp(
p
1 + 4n2π2/p2 t)
p
1 + 4n2π2/p2
.
Step 8: Show that we write the results from Step 4 and Step 7 as
f(t) = e−|t|
p
+ 2
p
∞
X
n=1
(−1)n
p
1 + 4n2π2/p2 cos
2nπy
p

e−√
1+4n2π2/p2 |t|.
In this case, our time variable t was their spatial variable x −ξ.
12. Find the inverse of the Fourier transform
F(ω) =

cos

ωL
β[1 + iγ sgn(ω)]
−1
,
where L, β, and γ are real and positive and sgn(z) = 1 if ℜ(z) > 0 and −1 if ℜ(z) < 0.
Step 1: From the deﬁnition of the Fourier transform, show that
f(t) = 1
2π
Z ∞
−∞
eitω
cos
n
ωL
β[1+iγsgn(ω)]
odω = 1
2π
I
C
eitz
cos
n
zL
β[1+iγsgn(z)]
o dz.
Step 2: Show that the integral has simple poles at zn± = ±(2n −1)βπ/(2L) + (2n −
1)iβγπ/(2L), where n = 1, 2, 3, . . .

Advanced Transform Methods
91
Step 3: For t > 0, use the residue theorem and show that
f(t) = i
" ∞
X
n=1
Res
 
eitz
cos
n
zL
β[1+iγsgn(z)]
o; zn+
!
+ Res
 
eitz
cos
n
zL
β[1+iγsgn(z)]
o; zn−
!#
,
where
Res

eitz
cos
n
zL
β[1+iγsgn(z)]
o; zn±

= ±(−1)nβ
L
[1 + iγsgn(zn)]e−(2n−1)βγπt/(2L)±(2n−1)βπit/(2L).
Step 4: For t < 0, show that f(t) = 0. Why?
Step 5: Show that we can summarize the results from Step 3 and Step 4 by
f(t) = 2β
L H(t)
∞
X
n=1
(−1)n+1e−(2n−1)βγπt/2L {γ cos[(2n −1)βπt/2L] + sin[(2n −1)βπt/2L]} .
Use the residue theorem to verify the following integrals:
13.
Z ∞
−∞
sin(x)
x2 + 4x + 5 dx = −π
e sin(2)
14.
Z ∞
0
cos(x)
(x2 + 1)2 dx = π
2e
15
Z ∞
−∞
x sin(ax)
x2 + 4
dx = πe−2a
16.
Z ∞
0
x2 cos(ax)
(x2 + b2)2 dx = π
4b(1 −ab)e−ab
where a, b > 0.
17. The concept of forced convection is normally associated with heat streaming through
a duct or past an obstacle.
Bentwich5 showed that a similar transport can exist when
convection results from a wave traveling through an essentially stagnant ﬂuid. In the process
of computing the amount of heating, he proved the following identity:
Z ∞
−∞
cosh(hx) −1
x sinh(hx)
cos(ax) dx = ln[coth(|a|π/h)],
h > 0.
Conﬁrm his result.
Step 1:
Z ∞
−∞
cosh(hx) −1
x sinh(hx)
cos(ax) dx = ℜ
I
C
cosh(hz) −1
z sinh(hz) eaiz dz

,
if a > 0 and C is a semicircle of inﬁnite radius in the upper half-plane.
Step 2: Within the contour, show that there is a removal singularity at z = 0 and simple
poles at hzn = nπi with n = 1, 2, 3, . . ..
5 Bentwich, M., 1966: Convection enforced by surface and tidal waves. Int. J. Heat Mass Transfer, 9,
663–670.

92
Advanced Engineering Mathematics: A Second Course
Step 3: Show that
I
C
cosh(hz) −1
z sinh(hz) eaiz dz = 2πi
∞
X
n=1
Res
cosh(hz) −1
z sinh(hz) eaiz; nπi
h

with
Res
cosh(hz) −1
z sinh(hz) eaiz; nπi
h

= 1 −(−1)n
nπi
e−nπa/h.
Step 4: Show that
Z ∞
−∞
cosh(hx) −1
x sinh(hx)
cos(ax) dx = 4
∞
X
m=1
exp[−(2m −1)πa/h]
2m −1
= ln[coth(πa/h)].
Step 5: Redo the analysis if we replace a by −a. Reconcile your results with those given
by Bentwich.
2.2 INVERSION OF LAPLACE TRANSFORMS BY CONTOUR INTEGRATION
Partial fractions and convolution are two common methods for ﬁnding the inverse of
the Laplace transform F(s). In many instances these methods fail simply because of the
complexity of the transform to be inverted. In this section we shall show how we can invert
transforms through the powerful method of contour integration. Of course, the student
must be proﬁcient in the use of complex variables.
Consider the piece-wise diﬀerentiable function f(x), which vanishes for x < 0. We can
express the function e−cxf(x) by the complex Fourier representation of
f(x)e−cx = 1
2π
Z ∞
−∞
eiωx
Z ∞
0
e−ctf(t)e−iωt dt

dω,
(2.2.1)
for any value of the real constant c, where the integral
I =
Z ∞
0
e−ct|f(t)| dt
(2.2.2)
exists. By multiplying both sides of Equation 2.2.1 by ecx and bringing it inside the ﬁrst
integral,
f(x) = 1
2π
Z ∞
−∞
e(c+ωi)x
Z ∞
0
f(t)e−(c+ωi)t dt

dω.
(2.2.3)
With the substitution z = c + ωi, where z is a new, complex variable of integration,
f(x) =
1
2πi
Z c+∞i
c−∞i
ezx
Z ∞
0
f(t)e−zt dt

dz.
(2.2.4)
The quantity inside the square brackets is the Laplace transform F(z). Therefore, we can
express f(t) in terms of its transform by the complex contour integral
f(t) =
1
2πi
Z c+∞i
c−∞i
F(z)etzdz.
(2.2.5)

Advanced Transform Methods
93
An outstanding mathematician at Cambridge University at the turn of the twentieth century,
Thomas John I’Anson Bromwich (1875–1929) came to Heaviside’s operational calculus through
his interest in divergent series. Beginning a correspondence with Heaviside, Bromwich was able to
justify operational calculus through the use of contour integrals by 1915. After his premature death,
individuals such as J. R. Carson and Sir H. Jeﬀreys brought Laplace transforms to the increasing
attention of scientists and engineers. (Portrait courtesy of the Royal Society of London.)
This line integral, the Bromwich integral,6 runs along the line x = c parallel to the imaginary
axis and c units to the right of it, the so-called Bromwich contour. We select the value of c
suﬃciently large so that the integral, Equation 2.2.2, exists; subsequent analysis shows that
this occurs when c is larger than the real part of any of the singularities of F(z).
We must now evaluate the contour integral.
Because of the power of the residue
theorem in complex variables, the contour integral is usually transformed into a closed
contour through the use of Jordan’s lemma. See Section 2.1, Equations 2.1.4 and Equation
2.1.5. The following examples will illustrate the proper use of Equation 2.2.5.
• Example 2.2.1
Let us invert
F(s) =
e−3s
s2(s −1).
(2.2.6)
6 Bromwich, T. J. I’A., 1916: Normal coordinates in dynamical systems. Proc. London Math. Soc.,
Ser. 2, 15, 401–448.

94
Advanced Engineering Mathematics: A Second Course
t > 3
(c,0)
t < 3
Figure 2.2.1: Contours used in the inversion of Equation 2.2.6.
From Bromwich’s integral,
f(t) =
1
2πi
Z c+∞i
c−∞i
e(t−3)z
z2(z −1) dz =
1
2πi
I
C
e(t−3)z
z2(z −1) dz −
1
2πi
Z
CR
e(t−3)z
z2(z −1) dz,
(2.2.7)
where CR is a semicircle of inﬁnite radius in either the right or left half of the z-plane and
C is the closed contour that includes CR and Bromwich’s contour. See Figure 2.2.1.
Our ﬁrst task is to choose an appropriate contour so that the integral along CR vanishes.
By Jordan’s lemma, this requires a semicircle in the right half-plane if t −3 < 0 and
a semicircle in the left half-plane if t −3 > 0.
Consequently, by considering these two
separate cases, we force the second integral in Equation 2.2.7 to zero and the inversion
simply equals the closed contour.
Consider the case t < 3 ﬁrst. Because Bromwich’s contour lies to the right of any
singularities, there are no singularities within the closed contour and f(t) = 0.
Consider now the case t > 3. Within the closed contour in the left half-plane, there is
a second-order pole at z = 0 and a simple pole at z = 1. Therefore,
f(t) = Res
 e(t−3)z
z2(z −1); 0

+ Res
 e(t−3)z
z2(z −1); 1

,
(2.2.8)
where
Res
 e(t−3)z
z2(z −1); 0

= lim
z→0
d
dz

z2 e(t−3)z
z2(z −1)

= lim
z→0
(t −3)e(t−3)z
z −1
−e(t−3)z
(z −1)2

= 2 −t,
(2.2.9)
and
Res
 e(t−3)z
z2(z −1); 1

= lim
z→1 (z −1) e(t−3)z
z2(z −1) = et−3.
(2.2.10)
Taking our earlier results into account, the inverse equals
f(t) =

et−3 −(t −3) −1

H(t −3),
(2.2.11)

Advanced Transform Methods
95
-
(c,0)
πi/a
πi/a
2πi/a
2πi/a
3πi/a
3πi/a
-
-
Figure 2.2.2: Contours used in the inversion of Equation 2.2.12.
which we would have obtained from the second shifting theorem and tables.
⊓⊔
• Example 2.2.2
For our second example of the inversion of Laplace transforms by complex integration,
let us ﬁnd the inverse of
F(s) =
1
s sinh(as),
(2.2.12)
where a is real. From Bromwich’s integral,
f(t) =
1
2πi
Z c+∞i
c−∞i
etz
z sinh(az) dz.
(2.2.13)
Here c is greater than the real part of any of the singularities in Equation 2.2.12. Using the
inﬁnite product for the hyperbolic sine,7
etz
z sinh(az) =
etz
az2[1 + a2z2/π2][1 + a2z2/(4π2)][1 + a2z2/(9π2)] · · ·.
(2.2.14)
Thus, we have a second-order pole at z = 0 and simple poles at zn = ±nπi/a, where
n = 1, 2, 3, . . ..
We can convert the line integral Equation 2.2.13, with the Bromwich contour lying
parallel and slightly to the right of the imaginary axis, into a closed contour using Jordan’s
lemma through the addition of an inﬁnite semicircle joining i∞to −i∞, as shown in Figure
2.2.2. We now apply the residue theorem. For the second-order pole at z = 0,
7 Gradshteyn, I. S., and I. M. Ryzhik, 1965: Table of Integrals, Series and Products. Academic Press,
Section 1.431, Formula 2.

96
Advanced Engineering Mathematics: A Second Course
Res

etz
z sinh(az); 0

= 1
1! lim
z→0
d
dz
(z −0)2etz
z sinh(az)

= lim
z→0
d
dz

zetz
sinh(az)

(2.2.15)
= lim
z→0

etz
sinh(az) +
ztetz
sinh(az) −az cosh(az)etz
sinh2(az)

= t
a
(2.2.16)
after using sinh(az) = az + O(z3). For the simple poles zn = ±nπi/a,
Res

etz
z sinh(az); zn

= lim
z→zn
(z −zn)etz
z sinh(az) = lim
z→zn
etz
sinh(az) + az cosh(az)
(2.2.17)
= exp(±nπit/a)
(−1)n(±nπi) ,
(2.2.18)
because cosh(±nπi) = cos(nπ) = (−1)n. Thus, summing up all of the residues gives
f(t) = t
a +
∞
X
n=1
(−1)n exp(nπit/a)
nπi
−
∞
X
n=1
(−1)n exp(−nπit/a)
nπi
(2.2.19)
= t
a + 2
π
∞
X
n=1
(−1)n
n
sin(nπt/a).
(2.2.20)
⊓⊔
In addition to computing the inverse of Laplace transforms, Bromwich’s integral places
certain restrictions on F(s) in order that an inverse exists. If α denotes the minimum value
that c may possess, the restrictions are threefold.8
First, F(z) must be analytic in the
half-plane x ≥α, where z = x + iy. Second, in the same half-plane it must behave as z−k,
where k > 1. Finally, F(x) must be real when x ≥α.
• Example 2.2.3
Is the function sin(s)/(s2 + 4) a proper Laplace transform? Although the function
satisﬁes the ﬁrst and third criteria listed in the previous paragraph on the half-plane x > 2,
the function becomes unbounded as y →±∞for any ﬁxed x > 2. Thus, sin(s)/(s2 + 4)
cannot be a Laplace transform.
⊓⊔
• Example 2.2.4
An additional beneﬁt of understanding inversion by the residue method is the ability
to qualitatively anticipate the inverse by knowing the location of the poles of F(s). This
intuition is important because many engineering analyses discuss stability and performance
entirely in terms of the properties of the system’s Laplace transform. In Figure 2.2.3 we
have graphed the location of the poles of F(s) and the corresponding f(t). The student
should go through the mental exercise of connecting the two pictures.
8 For the proof, see Churchill, R. V., 1972: Operational Mathematics. McGraw-Hill, Section 67.

Advanced Transform Methods
97
t
f(t)
f(t)
s-plane
s-plane
t
t
f(t)
f(t)
s-plane
s-plane
t
t
f(t)
f(t)
s-plane
s-plane
t
Figure 2.2.3: The correspondence between the location of the simple poles of the Laplace transform F(s)
and the behavior of f(t).

98
Advanced Engineering Mathematics: A Second Course
Problems
Use Bromwich’s integral to invert the following Laplace transforms F(s):
1.
s + 1
(s + 2)2(s + 3)
2.
1
s2(s + a)2
3.
1
s(s −2)3
4.
1
s(s + a)2(s2 + b2)
5.
e−s
s2(s + 2)
6. Use Bromwich’s integral to invert
F(s) =
1
s(1 + e−as).
Step 1: Show that the singularities are all simple poles and are located at z = 0 and
zn = ±(2n −1)πi/a, where n = 1, 2, 3, . . ..
Step 2: Show that the corresponding residues are
Res

etz
z(1 + e−az); 0

= 1
2,
and
Res

etz
z(1 + e−az); zn

= ±exp[±(2n −1)πit/a]
(2n −1)πi
.
Step 3: Show that the inverse Laplace transform equals
f(t) = 1
2 + 2
π
∞
X
n=1
sin[(2n −1)πt/a]
(2n −1)π
.
7. Use Bromwich’s integral to invert
F(s) =
1
(s + b) cosh(as).
Step 1: Show that the singularities are all simple poles and are located at z = −b and
zn = ±(2n −1)πi/(2a), where n = 1, 2, 3, . . . because cosh(az) = cos(iaz) = 0.
Step 2: Show that the corresponding residues are
Res

etz
(z + b) cosh(az); −b

=
e−bt
cosh(ab),
and
Res

etz
(z + b) cosh(az); zn

= ±
exp[±(2n −1)πit/(2a)]
ai[b ± (2n −1)πi/(2a)] sin[(2n −1)π/2].
Step 3: Show that the inverse Laplace transform equals
f(t) =
e−bt
cosh(ab) −8ab
∞
X
n=1
(−1)n sin[(2n −1)πt/(2a)]
4a2b2 + (2n −1)2π2
+ 4
∞
X
n=1
(−1)n (2n −1)π cos[(2n −1)πt/(2a)]
4a2b2 + (2n −1)2π2
.

Advanced Transform Methods
99
8. Use Bromwich’s integral to invert
F(s) =
1
s(1 −e−as).
Step 1: Show that the singularities are all simple poles and are located at z = 0 and
zn = ±2nπi/a, where n = 1, 2, 3, . . ..
Step 2: Show that the corresponding residues are
Res

etz
z(1 −e−az); 0

= t
a + 1
2,
and
Res

etz
z(1 −e−az); zn

= ±exp(±2nπit/a)
2nπi
.
Hint: Near z = 0, show that
etz
z(1 −e−az) =
1 + tz + · · ·
az2(1 −az/2 + · · ·) =
1
az2

1 + tz + az
2 + · · ·

and read oﬀthe residue from the Laurent expansion.
Step 3: Show that the inverse Laplace transform equals
f(t) = t
a + 1
2 + 1
π
∞
X
n=1
sin(2nπt/a)
n
.
9. Consider a function f(t) that has the Laplace transform F(z), which is analytic in the
half-plane ℜ(z) > s0. Can we use this knowledge to ﬁnd g(t), whose Laplace transform
G(z) equals F[ϕ(z)], where ϕ(z) is also analytic for ℜ(z) > s0? The answer to this question
leads to the Schouten9–Van der Pol10 theorem.
Step 1: Show that the following relationships hold true:
G(z) = F[ϕ(z)] =
Z ∞
0
f(τ)e−ϕ(z)τ dτ,
and
g(t) =
1
2πi
Z c+∞i
c−∞i
F[ϕ(z)]etz dz.
Step 2: Using the results from Step 1, show that
g(t) =
Z ∞
0
f(τ)
 1
2πi
Z c+∞i
c−∞i
e−ϕ(z)τetz dz

dτ.
This is the Schouten-Van der Pol theorem.
Step 3: If G(z) = F(√z ) show that
g(t) =
1
2
√
πt3
Z ∞
0
τf(τ) exp

−τ 2
4t

dτ.
9 Schouten, J. P., 1935: A new theorem in operational calculus together with an application of it.
Physica, 2, 75–80.
10 Van der Pol, B., 1934: A theorem on electrical networks with applications to ﬁlters.
Physica, 1,
521–530.

100
Advanced Engineering Mathematics: A Second Course
Hint: Do not evaluate the contour integral. Instead, ask yourself: What function of time
has a Laplace transform that equals e−ϕ(z)τ, where τ is a parameter? Then use tables.
2.3 INTEGRAL EQUATIONS
An integral equation contains the dependent variable under an integral sign. The convo-
lution theorem provides an excellent tool for solving a very special class of these equations,
the Volterra equation of the second kind:11
f(t) −
Z t
0
K[t, x, f(x)] dx = g(t),
0 ≤t ≤T.
(2.3.1)
These equations appear in history-dependent problems, such as epidemics,12 vibration prob-
lems,13 and viscoelasticity.14
• Example 2.3.1
Let us ﬁnd f(t) from the integral equation
f(t) = 4t −3
Z t
0
f(x) sin(t −x) dx.
(2.3.2)
The integral in Equation 2.3.2 is such that we can use the convolution theorem to ﬁnd
its Laplace transform. Then, because L[sin(t)] = 1/(s2 +1), the convolution theorem yields
L
Z t
0
f(x) sin(t −x) dx

= F(s)
s2 + 1.
(2.3.3)
Therefore, the Laplace transform converts Equation 2.3.2 into
F(s) = 4
s2 −3F(s)
s2 + 1.
(2.3.4)
Solving for F(s),
F(s) = 4(s2 + 1)
s2(s2 + 4).
(2.3.5)
By partial fractions, or by inspection,
F(s) = 1
s2 +
3
s2 + 4.
(2.3.6)
11 Fock, V., 1924: ¨Uber eine Klasse von Integralgleichungen. Math. Z., 21, 161–173; Koizumi, S., 1931:
On Heaviside’s operational solution of a Volterra’s integral equation when its nucleus is a function of (x−ξ).
Philos. Mag., Ser. 7, 11, 432–441.
12 Wang, F. J. S., 1978: Asymptotic behavior of some deterministic epidemic models. SIAM J. Math.
Anal., 9, 529–534.
13 Lin, S. P., 1975: Damped vibration of a string. J. Fluid Mech., 72, 787–797.
14 Rogers, T. G., and E. H. Lee, 1964: The cylinder problem in viscoelastic stress analysis. Q. Appl.
Math., 22, 117–131.

Advanced Transform Methods
101
Therefore, inverting term by term,
f(t) = t + 3
2 sin(2t).
(2.3.7)
Note that the integral equation
f(t) = 4t −3
Z t
0
f(t −x) sin(x) dx
(2.3.8)
also has the same solution.
⊓⊔
• Example 2.3.2
Let us solve the equation
f ′(t) + α2
Z t
0
f(τ) dτ = B −C cos(ωt),
f(0) = 0.
(2.3.9)
Again the integral is one of the convolution type; it diﬀers from the previous example
in that it includes a derivative. Taking the Laplace transform of Equation 2.3.9,
sF(s) −f(0) + α2F(s)
s
= B
s −
sC
s2 + ω2 .
(2.3.10)
Because f(0) = 0, Equation 2.3.10 simpliﬁes to
(s2 + α2)F(s) = B −
Cs2
s2 + ω2 .
(2.3.11)
Solving for F(s),
F(s) =
B
s2 + α2 −
Cs2
(s2 + α2)(s2 + ω2).
(2.3.12)
Using partial fractions to invert Equation 2.3.12,
f(t) =
B
α +
αC
ω2 −α2

sin(αt) −
ωC
ω2 −α2 sin(ωt).
(2.3.13)
⊓⊔
• Example 2.3.3
Let us solve15 the integral equation
f(t) =
a
2(1 + 2a)
Z t
0
f(t −x)f(x) dx + e−t.
(2.3.14)
15 Hounslow, M. J., 1990: A discretized population balance for continuous systems at steady state.
AICHE J., 36, 106–116.

102
Advanced Engineering Mathematics: A Second Course
Taking the Laplace transform of Equation 2.3.14, we obtain
F(s) =
a F 2(s)
2(1 + 2a) +
1
s + 1.
(2.3.15)
Solving for F(s) so that F(s) →0 as s →∞, we have
F(s) = 2a + 1
a
−2a + 1
a
s
(2a + 1)(s + 1) −2a
(2a + 1)(s + 1)
= 2a + 1
a
−
√2a + 1
a
r
(2a + 1)s + 1
s + 1
.
(2.3.16)
Taking the inverse of Equation 2.3.16,
f(t) = 2a + 1
a
δ(t) −
√2a + 1
a
g(t),
(2.3.17)
where g(t) is the inverse of the Laplace transform G(s),
G(s) =
r
(2a + 1)s + 1
s + 1
=
√
2a + 1
s + 1/(1 + 2a)
√s + 1
p
s + 1/(2a + 1)
(2.3.18)
=
√
2a + 1 sH(s) +
H(s)
√2a + 1
(2.3.19)
and
H(s) =
1
√s + 1
p
s + 1/(2a + 1)
.
(2.3.20)
Taking the inverse of H(s), we ﬁnd that
h(t) = exp

−a + 1
2a + 1t

I0

at
2a + 1

(2.3.21)
and
h′(t) = −a + 1
2a + 1 exp

−a + 1
2a + 1t

I0

at
2a + 1

+
a
2a + 1 exp

−a + 1
2a + 1t

I1

at
2a + 1

,
(2.3.22)
where I0(·) and I1(·) are modiﬁed Bessel functions of the ﬁrst kind.
Because sH(s) = L[h′(t)]+h(0) and h(0) = 1, h′(t) = L−1[sH(s)]−δ(t) or L−1[sH(s)]
= h′(t) + δ(t). Then,
g(t) =
√
2a + 1

h′(t) +
h(t)
2a + 1 + δ(t)

(2.3.23)
=
√
2a + 1

δ(t) +
a
2a + 1 exp

−a + 1
2a + 1t

I1

at
2a + 1

−
a
2a + 1 exp

−a + 1
2a + 1t

I0

at
2a + 1

.
(2.3.24)
Finally, substituting Equation 2.3.24 into Equation 2.3.17,
f(t) = exp

−a + 1
2a + 1t
 
I0

at
2a + 1

−I1

at
2a + 1

.
(2.3.25)

Advanced Transform Methods
103
Problems
Solve the following integral equations:
1. f(t) = 1 + 2
Z t
0
f(t −x)e−2x dx
2. f(t) = 1 +
Z t
0
f(x) sin(t −x) dx
3. f(t) = t +
Z t
0
f(t −x)e−x dx
4. f(t) = 4t2 −
Z t
0
f(t −x)e−x dx
5. f(t) = t3 +
Z t
0
f(x) sin(t −x) dx
6. f(t) = 8t2 −3
Z t
0
f(x) sin(t −x) dx
7. f(t) = t2 −2
Z t
0
f(t −x) sinh(2x) dx
8. f(t) = 1 + 2
Z t
0
f(t −x) cos(x) dx
9. f(t) = e2t + 2
Z t
0
f(t −x) cos(x) dx
10. f(t) = t2 +
Z t
0
f(x) sin(t −x) dx
11. f(t) = e−t −2
Z t
0
f(x) cos(t −x) dx
12. f(t) +
Z t
0
f(x)(t −x) dx = t
13. f(t) = 6t + 4
Z t
0
f(x)(t −x)2 dx
14. f(t) = a
√
t −
Z t
0
f(t −x)
√x
dx
15. Solve the following equation for f(t) with the condition that f(0) = 4:
f ′(t) = t +
Z t
0
f(t −x) cos(x) dx.
16. Solve the following equation for f(t) with the condition that f(0) = 0:
f ′(t) = sin(t) +
Z t
0
f(t −x) cos(x) dx.
17. During a study of nucleation involving idealized active sites along a boiling surface,
Marto and Rohsenow16 solved the integral equation
A = B
√
t + C
Z t
0
x′(τ)
√t −τ dτ
to ﬁnd the position x(t) of the liquid/vapor interface. If A, B, and C are constants and
x(0) = 0, ﬁnd the solution for them.
18. Solve the following equation for x(t) with the condition that x(0) = 0:
x(t) + t =
1
c√π
Z t
0
x′(τ)
√t −τ dτ,
where c is constant.
Step 1: Show that
X(s) = −
c
s2(c −√s ) = −c(c + √s )
s2(c2 −s) .
16 Marto, P. J., and W. M. Rohsenow, 1966: Nucleate boiling instability of alkali metals.
J. Heat
Transfer, 88, 183–193.

104
Advanced Engineering Mathematics: A Second Course
Step 2: Use partial fractions to show that
X(s) = −1
c2

1 +
√s
c
 c2
s2 + 1
s −
1
s −c2

.
Step 3: Show that
x(t) = 1
c2
(
ec2t h
1 + erf

c
√
t
i
−c2t −1 −2c
r
t
π
)
.
19. During a study of the temperature f(t) of a heat reservoir attached to a semi-inﬁnite
heat-conducting rod, Huber17 solved the integral equation
f ′(t) = α −β
√π
Z t
0
f ′(τ)
√t −τ dτ,
where α and β are constants and f(0) = 0. Find f(t) for him.
Step 1: Show that
F(s) =
α
s3/2  s1/2 + β
 =
α
s (s −β2) −
αβ
s3/2 (s −β2)
=
α
β2(s −β2) −α
β2s −
αβ
s3/2 (s −β2).
Step 2: Taking the inverse term by term, show that
f(t) = α
β2
 
eβ2t −1 −4eβ2t
√π
Z β
√
t
0
e−x2x2 dx
!
= α
β2
 
eβ2t −1 + 2β
√
t
√π
−2eβ2t
√π
Z β
√
t
0
e−x2 dx
!
.
20.
During the solution of a diﬀusion problem, Zhdanov, Chikhachev, and Yavlinskii18
solved an integral equation similar to
Z t
0
f(τ)

1 −erf
 a
√
t −τ

dτ = at,
where erf(x) =
2
√π
Z x
0
e−y2 dy is the error function. What should they have found?
Step 1: Show that
F(s) = a
s + a3
s2 +
a2
s
√
s + a2 +
a4
s2 √
s + a2 .
17 Huber, A., 1934: Eine Methode zur Bestimmung der W¨arme- und Temperaturleitf¨ahigkeit. Monatsh.
Math. Phys., 41, 35–42.
18 Zhdanov, S. K., A. S. Chikhachev, and Yu. N. Yavlinskii, 1976: Diﬀusion boundary-value problem for
regions with moving boundaries and conservation of particles. Sov. Phys. Tech. Phys., 21, 883–884.

Advanced Transform Methods
105
Step 2: Show that
L

t erf(a
√
t ) −
1
2a2 erf(a
√
t ) +
√
t
a √π e−a2t

= −d
ds

a
s
√
s + a2

−
1
2as
√
s + a2 +
1
2a (s + a2)3/2 =
a
s2 √
s + a2 .
Step 3: Taking the inverse of Step 1 term by term, show that
f(t) = a + a2t + 1
2a erf(
√
at ) + a3t erf(
√
at ) + a2√
t
√π e−a2t.
21. The Laguerre polynomial19
y(t) = Ln(t) = et
n!
dn
dtn
 tne−t
,
n = 0, 1, 2, 3, . . .
satisﬁes the ordinary diﬀerential equation
ty′′ + (1 −t)y′ + ny = (ty′)′ −ty′ + ny = 0,
with y(0) = 1 and y′(0) = −n.
Step 1: Using the properties that L[f (n)(t)] = snF(s) −sn−1f(0) −· · · −sf (n−2)(0) −
f (n−1)(0) and Equation L[t f(t)] = −F ′(s), show that the Laplace transformed version of
this diﬀerential equation is
Y ′(s) = n + 1 −s
s(s −1) Y (s) =
n
s −1Y (s) −n + 1
s
Y (s),
where Y (s) is the Laplace transform of y(t).
Step 2: Using the property that L[t f(t)] = −F ′(s) and the convolution theorem, show that
Laguerre polynomials are the solution to the integral equation
ty(t) = (n + 1)
Z t
0
y(τ) dτ −net
Z t
0
y(τ) e−τ dτ.
2.4 THE SOLUTION OF THE WAVE EQUATION BY USING LAPLACE TRANSFORMS
The solution of linear partial diﬀerential equations by Laplace transforms is the most
commonly employed analytic technique after separation of variables. Because the transform
consists solely of an integration with respect to time, the transform U(x, s) of the solution
of the wave equation u(x, t) is
U(x, s) =
Z ∞
0
u(x, t)e−st dt,
(2.4.1)
19 See Section 5.3 in Andrews, L. C., 1985: Special Functions for Engineers and Applied Mathematicians.
MacMillan, 357 pp.

106
Advanced Engineering Mathematics: A Second Course
assuming that the wave equation only varies in a single spatial variable x and time t.
Partial derivatives involving time have transforms similar to those that we encountered
in the case of functions of a single variable. They include
L[ut(x, t)] = sU(x, s) −u(x, 0),
(2.4.2)
and
L[utt(x, t)] = s2U(x, s) −su(x, 0) −ut(x, 0).
(2.4.3)
These transforms introduce the initial conditions via u(x, 0) and ut(x, 0). On the other
hand, derivatives involving x become
L[ux(x, t)] = d
dx{L[u(x, t)]} = dU(x, s)
dx
,
(2.4.4)
and
L[uxx(x, t)] = d2
dx2 {L[u(x, t)]} = d2U(x, s)
dx2
.
(2.4.5)
Because the transformation eliminates the time variable, only U(x, s) and its derivatives
remain in the equation. Consequently, we transform the partial diﬀerential equation into a
boundary-value problem involving an ordinary diﬀerential equation. Because this equation
is often easier to solve than a partial diﬀerential equation, the use of Laplace transforms
considerably simpliﬁes the original problem. Of course, the Laplace transforms must exist
for this technique to work.
The following schematic summarizes the Laplace transform method:
In the following examples, we illustrate transform methods by solving the classic equa-
tion of telegraphy as it applies to a uniform transmission line. The line has a resistance R,
an inductance L, a capacitance C, and a leakage conductance G per unit length. We denote
the current in the direction of positive x by I; V is the voltage drop across the transmission
line at the point x. The dependent variables I and V are functions of both distance x along
the line and time t.
To derive the diﬀerential equations that govern the current and voltage in the line,
consider the points A at x and B at x + ∆x in Figure 2.4.1. The current and voltage at
A are I(x, t) and V (x, t); at B, I + ∂I
∂x∆x and V + ∂V
∂x ∆x. Therefore, the voltage drop
from A to B is −∂V
∂x ∆x and the current in the line is I + ∂I
∂x∆x. Neglecting terms that are
proportional to (∆x)2,

L∂I
∂t + RI

∆x = −∂V
∂x ∆x.
(2.4.6)
The voltage drop over the parallel portion HK of the line is V while the current in this
portion of the line is −∂I
∂x∆x. Thus,

C ∂V
∂t + GV

∆x = −∂I
∂x∆x.
(2.4.7)
Therefore, the diﬀerential equations for I and V are
L∂I
∂t + RI = −∂V
∂x ,
(2.4.8)
and
C ∂V
∂t + GV = −∂I
∂x.
(2.4.9)

Advanced Transform Methods
107
x
K
H
L
R
B
V+
C
A
x
x
∆x
1 ∆x
∆x
∆
I
+∆
x
δ I
δ x
δ V
δ x
+
I
δ
δ x
I
x
∆x
∆
G
V
Figure 2.4.1: Schematic of a uniform transmission line.
Turning to the initial conditions, we solve these simultaneous partial diﬀerential equa-
tions with the initial conditions
I(x, 0) = I0(x),
(2.4.10)
and
V (x, 0) = V0(x)
(2.4.11)
for 0 < t. There are also boundary conditions at the ends of the line; we will introduce
them for each speciﬁc problem. For example, if the line is short-circuited at x = a, V = 0
at x = a; if there is an open circuit at x = a, I = 0 at x = a.
To solve Equation 2.4.8 and Equation 2.4.9 by Laplace transforms, we take the Laplace
transform of both sides of these equations, which yields
(Ls + R)I(x, s) = −dV (x, s)
dx
+ LI0(x),
(2.4.12)
and
(Cs + G)V (x, s) = −dI(x, s)
dx
+ CV0(x).
(2.4.13)
Eliminating I gives an ordinary diﬀerential equation in V
d2V
dx2 −q2V = LdI0(x)
dx
−C(Ls + R)V0(x),
(2.4.14)
where q2 = (Ls + R)(Cs + G). After ﬁnding V , we may compute I from
I = −
1
Ls + R
dV
dx + LI0(x)
Ls + R.
(2.4.15)
At this point we treat several classic cases.
• Example 2.4.1: The semi-inﬁnite transmission line
We consider the problem of a semi-inﬁnite line 0 < x with no initial current and charge.
The end x = 0 has a constant voltage E for 0 < t.
In this case,
d2V
dx2 −q2V = 0,
0 < x.
(2.4.16)

108
Advanced Engineering Mathematics: A Second Course
The boundary conditions at the ends of the line are
V (0, t) = E,
0 < t,
(2.4.17)
and V (x, t) is ﬁnite as x →∞. The transform of these boundary conditions is
V (0, s) = E/s,
and
lim
x→∞V (x, s) →0.
(2.4.18)
The general solution of Equation 2.4.16 is
V (x, s) = Ae−qx + Beqx.
(2.4.19)
The requirement that V remains ﬁnite as x →∞forces B = 0. The boundary condition at
x = 0 gives A = E/s. Thus,
V (x, s) = E
s exp
h
−
p
(Ls + R)(Cs + G) x
i
.
(2.4.20)
We discuss the general case later. However, for the so-called “lossless” line, where R = G =
0,
V (x, s) = E
s exp(−sx/c),
(2.4.21)
where c = 1/
√
LC. Consequently,
V (x, t) = EH

t −x
c

,
(2.4.22)
where H(t) is Heaviside’s step function. The physical interpretation of this solution is as
follows: V (x, t) is zero up to the time x/c, at which time a wave traveling with speed c from
x = 0 would arrive at the point x. V (x, t) has the constant value E afterwards.
For the so-called “distortionless” line,20 R/L = G/C = ρ,
V (x, t) = Ee−ρx/cH

t −x
c

.
(2.4.23)
In this case, the disturbance not only propagates with velocity c but also attenuates as we
move along the line.
Suppose now, that instead of applying a constant voltage E at x = 0, we apply a
time-dependent voltage, f(t). The only modiﬁcation is that in place of Equation 2.4.20,
V (x, s) = F(s)e−qx.
(2.4.24)
In the case of the distortionless line, q = (s + ρ)/c, this becomes
V (x, s) = F(s)e−(s+ρ)x/c
(2.4.25)
and
V (x, t) = e−ρx/cf

t −x
c

H

t −x
c

.
(2.4.26)
20 Prechtl and Sch¨urhuber (Prechtl, A., and R. Sch¨urhuber, 2000: Nonuniform distortionless transmission
lines. Electr. Eng. [Berlin], 82, 127–134) generalized this problem to nonuniform transmission lines.

Advanced Transform Methods
109
(l-x)/c
t
E
V(x,t)
(3l-x)/c
(3l+x)/c
direct
once
twice
thrice reflected
(l+x)/c
Figure 2.4.2: The voltage within a lossless, ﬁnite transmission line of length l as a function of time t.
Thus, our solution shows that the voltage at x is zero up to the time x/c. Afterwards V (x, t)
follows the voltage at x = 0 with a time lag of x/c and decreases in magnitude by e−ρx/c.⊓⊔
• Example 2.4.2: The ﬁnite transmission line
We now discuss the problem of a ﬁnite transmission line 0 < x < l with zero initial
current and charge. We ground the end x = 0 and maintain the end x = l at constant
voltage E for 0 < t.
The transformed partial diﬀerential equation becomes
d2V
dx2 −q2V = 0,
0 < x < l.
(2.4.27)
The boundary conditions are
V (0, t) = 0,
and
V (l, t) = E,
0 < t.
(2.4.28)
The Laplace transform of these boundary conditions is
V (0, s) = 0,
and
V (l, s) = E/s.
(2.4.29)
The solution of Equation 2.4.27 that satisﬁes the boundary conditions is
V (x, s) = E sinh(qx)
s sinh(ql) .
(2.4.30)
Let us rewrite Equation 2.4.30 in a form involving negative exponentials and expand the
denominator by the binomial theorem,
V (x, s) = E
s e−q(l−x) 1 −e−2qx
1 −e−2ql
(2.4.31)
= E
s e−q(l−x)(1 −e−2qx)
 1 + e−2ql + e−4ql + · · ·

(2.4.32)
= E
s

e−q(l−x) −e−q(l+x) + e−q(3l−x) −e−q(3l+x) + · · ·

.
(2.4.33)
In the special case of the lossless line where q = s/c,
V (x, s) = E
s

e−s(l−x)/c −e−s(l+x)/c + e−s(3l−x)/c −e−s(3l+x)/c + · · ·

,
(2.4.34)
or
V (x, t) = E

H

t −l −x
c

−H

t −l + x
c

+ H

t −3l −x
c

−H

t −3l + x
c

+ · · ·

.
(2.4.35)

110
Advanced Engineering Mathematics: A Second Course
1
10
100
1000
x
0.0
0.2
0.4
0.6
0.8
1.0
V(x,t)/E
κt=10
100
1000
10000
Figure 2.4.3: The voltage within a submarine cable as a function of distance for various values of κt.
We illustrate Equation 2.4.35 in Figure 2.4.2. The voltage at x is zero up to the time
(l −x)/c, at which time a wave traveling directly from the end x = l would reach the point
x. The voltage then has the constant value E up to the time (l+x)/c, at which time a wave
traveling from the end x = l and reﬂected back from the end x = 0 would arrive. From this
time up to the time of arrival of a twice-reﬂected wave, it has the value zero, and so on. ⊓⊔
• Example 2.4.3: The semi-inﬁnite transmission line reconsidered
In the ﬁrst example, we showed that the transform of the solution for the semi-inﬁnite
line is
V (x, s) = E
s e−qx,
(2.4.36)
where q2 = (Ls + R)(Cs + G). In the case of a lossless line (R = G = 0), we found traveling
wave solutions.
In this example, we shall examine the case of a submarine cable,21 where L = G = 0.
In this special case,
V (x, s) = E
s e−x√
s/κ,
(2.4.37)
where κ = 1/(RC).
From a table of Laplace transforms,22 we can immediately invert
Equation 2.4.37 and ﬁnd that
V (x, t) = E erfc

x
2
√
κt

,
(2.4.38)
where erfc(·) is the complementary error function. Unlike the traveling wave solution, the
voltage diﬀuses into the cable as time increases. We illustrate Equation 2.4.38 in Figure
2.4.3.
⊓⊔
21 First solved by Thomson, W., 1855: On the theory of the electric telegraph. Proc. R. Soc. London,
Ser. A, 7, 382–399.
22 See Churchill, R. V., 1972: Operational Mathematics. McGraw-Hill Book, Section 27.

Advanced Transform Methods
111
• Example 2.4.4: A short-circuited, ﬁnite transmission line
Let us ﬁnd the voltage of a lossless transmission line of length l that initially has the
constant voltage E. At t = 0, we ground the line at x = 0 while we leave the end x = l
insulated.
The transformed partial diﬀerential equation now becomes
d2V
dx2 −s2
c2 V = −sE
c2 ,
(2.4.39)
where c = 1/
√
LC. The boundary conditions are
V (0, s) = 0,
(2.4.40)
and
I(l, s) = −1
Ls
dV (l, s)
dx
= 0
(2.4.41)
from Equation 2.4.15.
The solution to this boundary-value problem is
V (x, s) = E
s −E cosh[s(l −x)/c]
s cosh(sl/c)
.
(2.4.42)
The ﬁrst term on the right side of Equation 2.4.42 is easy to invert and the inversion equals
E. The second term is much more diﬃcult to handle. We will use Bromwich’s integral.
In Section 2.2 we showed that
L−1
cosh[s(l −x)/c]
s cosh(sl/c)

=
1
2πi
Z c+∞i
c−∞i
cosh[z(l −x)/c]etz
z cosh(zl/c)
dz.
(2.4.43)
To evaluate this integral, we must ﬁrst locate and then classify the singularities. Using the
product formula for the hyperbolic cosine,
cosh[z(l −x)/c]
z cosh(zl/c)
= [1 + 4z2(l−x)2
c2π2
][1 + 4z2(l−x)2
9c2π2
] · · ·
z[1 + 4z2l2
c2π2 ][1 + 4z2l2
9c2π2 ] · · ·
.
(2.4.44)
This shows that we have an inﬁnite number of simple poles located at z = 0, and zn =
±(2n −1)πci/(2l), where n = 1, 2, 3, . . .. Therefore, Bromwich’s contour can lie along, and
just to the right of, the imaginary axis. By Jordan’s lemma we close the contour with a
semicircle of inﬁnite radius in the left half of the complex plane. Computing the residues,
Res
cosh[z(l −x)/c]etz
z cosh(zl/c)
; 0

= lim
z→0
cosh[z(l −x)/c]etz
cosh(zl/c)
= 1,
(2.4.45)
and
Res
cosh[z(l −x)/c]etz
z cosh(zl/c)
; zn

= lim
z→zn
(z −zn) cosh[z(l −x)/c]etz
z cosh(zl/c)
(2.4.46)
= cosh[(2n −1)π(l −x)i/(2l)] exp[±(2n −1)πcti/(2l)]
[(2n −1)πi/2] sinh[(2n −1)πi/2]
(2.4.47)
=
2(−1)n
(2n −1)π cos
(2n −1)π(l −x)
2l

exp

±(2n −1)πcti
2l

.
(2.4.48)

112
Advanced Engineering Mathematics: A Second Course
Summing the residues and using the relationship that cos(t) = (eti + e−ti)/2,
V (x, t) = E −E

1 −4
π
∞
X
n=1
(−1)n+1
2n −1 cos
(2n −1)π(l −x)
2l

cos
(2n −1)πct
2l

(2.4.49)
= 4E
π
∞
X
n=1
(−1)n+1
2n −1 cos
(2n −1)π(l −x)
2l

cos
(2n −1)πct
2l

.
(2.4.50)
An alternative to contour integration is to rewrite Equation 2.4.42 as
V (x, s) = E
s
(
1 −e−sx/c 
1 + e−2s(l−x)/c
1 + e−2sl/c
#
(2.4.51)
= E
s
h
1 −e−sx/c −e−s(2l−x)/c + e−s(2l+x)/c + · · ·
i
(2.4.52)
so that
V (x, t) = E

1 −H

t −x
c

−H

t −2l −x
c

+ H

t −2l + x
c

+ · · ·

. (2.4.53)
⊓⊔
• Example 2.4.5: The general solution of the equation of telegraphy
In this example we solve the equation of telegraphy without any restrictions on R, C,
G, or L. We begin by eliminating the dependent variable I(x, t) from the set of equations,
Equation 2.4.8 and Equation 2.4.9. This yields
CL∂2V
∂t2 + (GL + RC)∂V
∂t + RG V = ∂2V
∂x2 .
(2.4.54)
We next take the Laplace transform of Equation 2.4.54 assuming that V (x, 0) = f(x), and
Vt(x, 0) = g(x). The transformed version of Equation 2.4.54 is
d2V
dx2 −[CLs2 + (GL + RC)s + RG]V = −CLg(x) −(CLs + GL + RC)f(x),
(2.4.55)
or
d2V
dx2 −(s + ρ)2 −σ2
c2
V = −g(x)
c2
−
 s
c2 + 2ρ
c2

f(x),
(2.4.56)
where c2 = 1/LC, ρ = c2(RC + GL)/2, and σ = c2(RC −GL)/2.
We solve Equation 2.4.56 by Fourier transforms with the requirement that the solution
dies away as |x| →∞. The most convenient way of expressing this solution is the convolution
product
V (x, s) =
g(x)
c
+
s
c + 2ρ
c

f(x)

∗exp[−|x|
p
(s + ρ)2 −σ2/c]
2
p
(s + ρ)2 −σ2
.
(2.4.57)
From a table of Laplace transforms,
L−1
"
exp
 −b
√
s2 −a2 
√
s2 −a2
#
= I0

a
p
t2 −b2

H(t −b),
(2.4.58)

Advanced Transform Methods
113
where b > 0 and I0(·) is the zeroth-order modiﬁed Bessel function of the ﬁrst kind. There-
fore, by the ﬁrst shifting theorem,
L−1



exp
h
−|x|
p
(s + ρ)2 −σ2/c
i
p
(s + ρ)2 −σ2


= e−ρtI0
h
σ
p
t2 −(x/c)2
i
H

t −|x|
c

.
(2.4.59)
Using Equation 2.4.59 to invert Equation 2.4.57, we have that
V (x, t) =
1
2ce−ρtg(x) ∗I0
h
σ
p
t2 −(x/c)2
i
H(t −|x|/c)
+ 1
2ce−ρtf(x) ∗∂
∂t
n
I0[σ
p
t2 −(x/c)2]
o
H(t −|x|/c)
+ ρ
ce−ρtf(x) ∗I0
h
σ
p
t2 −(x/c)2
i
H(t −|x|/c)
+ 1
2e−ρt[f(x + ct) + f(x −ct)].
(2.4.60)
The last term in Equation 2.4.60 arises from noting that sF(s) = L[f(t)] + f(0). If we
explicitly write out the convolution, the ﬁnal form of the solution is
V (x, t) = 1
2e−ρt[f(x + ct) + f(x −ct)]
+ 1
2ce−ρt
Z x+ct
x−ct
[g(η) + 2ρf(η)]I0

σ
p
c2t2 −(x −η)2

c

dη
+ 1
2ce−ρt
Z x+ct
x−ct
f(η) ∂
∂t

I0

σ
p
c2t2 −(x −η)2

c

dη.
(2.4.61)
There is a straightforward physical interpretation of the ﬁrst line of Equation 2.4.61.
It represents damped progressive waves; one is propagating to the right and the other to
the left. In addition to these progressive waves, there is a contribution from the integrals,
even after the waves pass. These integrals include all of the points where f(x) and g(x)
are nonzero within a distance ct from the point in question. This eﬀect persists through all
time, although dying away, and constitutes a residue or tail. Figure 2.4.4 illustrates this for
ρ = 0.1, σ = 0.2, and c = 1. This ﬁgure was obtained using the MATLAB script:
% initialize parameters in calculation
clear; dx = 0.1; dt = 0.5; rho over c = 0.1; sigma over c = 0.2;
X=[-10:dx:10]; T = [0:dt:10]; % compute locations of x and t
for j=1:length(T); t = T(j);
for i=1:length(X); x = X(i);
XX(i,j) = x; TT(i,j) = t; deta i = 0.05 % set up grid
% compute characteristics x+ct and x-ct
characteristic 1 = x - t; characteristic 2 = x + t;
% compute first term in Equation 2.4.61
F = inline(’stepfun(x,-1.0001)-stepfun(x,1.0001)’);
u(i,j ) = F(characteristic 1) + F(characteristic 2);
% find the upper and lower limits of the integration
upper = characteristic 2; lower = characteristic 1;
if t > 0 & upper > -1 & lower < 1
if upper > 1 upper = 1; end
if lower < -1 lower = -1; end

114
Advanced Engineering Mathematics: A Second Course
−10
−5
0
5
10
0
5
10
0
0.2
0.4
0.6
0.8
1
1.2
TIME
DISTANCE
SOLUTION
Figure 2.4.4: The evolution of the voltage with time given by the general equation of telegraphy for initial
conditions and parameters stated in the text.
% set up parameters needed for integration
interval = upper-lower;
NN = interval / deta i;
if mod(NN,2) > 0 NN = NN + 1; end;
deta = interval / NN;
% compute integrals in Equation 2.4.61 by Simpson’s rule
% sum1 deals with the first integral while sum2 is the second
sum1 = 0; sum2 = 0; eta = lower;
for k = 0:2:NN-2
arg = sigma over c * sqrt(t*t-(x-eta)*(x-eta));
sum1 = sum1 + besseli(0,arg);
if (arg == 0)
sum2 = sum2 + 0.5 * sigma over c * t;
else
sum2 = sum2 + t * besseli(1,arg) / arg; end
eta = eta + deta;
arg = sigma over c * sqrt(t*t-(x-eta)*(x-eta));
sum1 = sum1 + 4*besseli(0,arg);
if (arg == 0)
sum2 = sum2 + 4 * 0.5 * sigma over c * t;
else
sum2 = sum2 + 4 * t * besseli(1,arg) / arg; end
eta = eta + deta;
arg = sigma over c * sqrt(t*t-(x-eta)*(x-eta));
sum1 = sum1 + besseli(0,arg);
if (arg == 0)
sum2 = sum2 + 0.5 * sigma over c * t;

Advanced Transform Methods
115
else
sum2 = sum2 + t * besseli(1,arg) / arg; end
end
u(i,j) = u(i,j) + 2 * rho over c * deta * sum1 / 3 ...
+ sigma over c * deta * sum2 / 3;
end
% multiply final answer by damping coefficient
u(i,j) = 0.5 * exp(-rho over c * t) * u(i,j);
end;end;
% plot results
mesh(XX,TT,real(u)); colormap spring;
xlabel(’DISTANCE’,’Fontsize’,20); ylabel(’TIME’,’Fontsize’,20)
zlabel(’SOLUTION’,’Fontsize’,20)
We evaluated the integrals by Simpson’s rule for the initial conditions f(x) = H(x + 1) −
H(x −1), and g(x) = 0. If there was no loss, then two pulses would propagate to the left
and right. However, with resistance and leakage the waves leave a residue after their leading
edge has passed.
Problems
1. Use transform methods to solve the wave equation
∂2u
∂t2 = ∂2u
∂x2 ,
0 < x < 1,
0 < t,
with the boundary conditions u(0, t) = u(1, t) = 0, 0 < t, and the initial conditions u(x, 0) =
0, ut(x, 0) = 1, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s2U(x, s) = −1,
0 < x < 1,
subject to the boundary conditions U(0, s) = U(1, s) = 1.
Step 2: Show that the solution to the previous step is
U(x, s) = 1 −cosh(sx)
s2
+ [cosh(s) −1] sinh(sx)
s2 sinh(s)
.
Step 3: Show that U(x, s) has simple poles at sn = ±nπi with n = 1, 2, 3, . . . and a
removable pole at s = 0.
Step 4: Use Bromwich’s integral and show that
u(x, t) = 4
π2
∞
X
m=1
sin[(2m −1)πx] sin[(2m −1)πt]
(2m −1)2
.
2. Use transform methods to solve the wave equation
∂2u
∂t2 = ∂2u
∂x2 ,
0 < x < 1,
0 < t,

116
Advanced Engineering Mathematics: A Second Course
with the boundary conditions u(0, t) = ux(1, t) = 0, 0 < t, and the initial conditions
u(x, 0) = 0, ut(x, 0) = x, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s2U(x, s) = −x,
0 < x < 1,
with the boundary condition U(0, s) = U ′(1, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = xs cosh(s) −sinh(sx)
s3 cosh(s)
.
Step 3: Show that U(x, s) has simple poles at sn = ±(2n −1)πi/2 with n = 1, 2, 3, . . . and
a removable pole at s = 0.
Step 4: Use Bromwich’s integral and show that
u(x, t) = 16
π3
∞
X
n=1
(−1)n+1
(2n −1)3 sin
(2n −1)πx
2

sin
(2n −1)πt
2

.
3. Use transform methods to solve the wave equation
∂2u
∂t2 = ∂2u
∂x2 ,
0 < x < 1,
0 < t,
with the boundary conditions u(0, t) = u(1, t) = 0, 0 < t, and the initial conditions u(x, 0) =
sin(πx), ut(x, 0) = −sin(πx), 0 < x < 1.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s2U(x, s) = −s sin(πx) + sin(πx),
0 < x < 1,
with the boundary conditions U(0, s) = U(1, s) = 0.
Step 2: Show that the solution to the previous step is U(x, s) = (s −1) sin(πx)/(s2 + π2).
Step 3: Inverting by inspection, show that u(x, t) = sin(πx) cos(πt) −sin(πx) sin(πt)/π.
4. Use transform methods to solve the wave equation
∂2u
∂t2 = c2 ∂2u
∂x2 ,
0 < x < a,
0 < t,
with the boundary conditions u(0, t) = sin(ωt), u(a, t) = 0, 0 < t, and the initial conditions
u(x, 0) = ut(x, 0) = 0, 0 < x < a. Assume that ωa/c is not an integer multiple of π. Why?
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s2
c2 U(x, s) = 0,
0 < x < a,

Advanced Transform Methods
117
with the boundary condition U(0, s) = ω/(s2 + ω2) and U(a, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) =
ω sinh[s(a −x)/c]
(s2 + ω2) sinh(sa/c).
Step 3: Show that U(x, s) has simple poles at s = ±ωi and sn = ±nπci/a with n =
1, 2, 3, . . . and a removable pole at s = 0.
Step 4: Use Bromwich’s integral and show that
u(x, t) = sin[ω(a −x)/c]
sin(ωa/c)
sin(ωt) −2ωa
c
∞
X
n=1
sin(nπx/a)
n2π2 −a2ω2/c2 sin
nπct
a

.
5. Use transform methods to solve the wave equation
∂2u
∂t2 = c2 ∂2u
∂x2 ,
0 < x < L,
0 < t,
with the boundary conditions ux(0, t) = −f(t), ux(L, t) = 0, 0 < t, and the initial conditions
u(x, 0) = ut(x, 0) = 0, 0 < x < L.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s2
c2 U(x, s) = 0,
0 < x < L,
with the boundary conditions U ′(0, s) = −F(s) and U ′(L, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = cF(s) cosh[s(L −x)/c]
s sinh(sL/c)
.
Step 3: Replacing sinh and cosh by their deﬁnitions and expanding the denominator as a
geometric series, show that
U(x, s) = cF(s)
s
h
e−sx/c + e−s(2L−x)/ci 
1 + e−2sL/c + e−4sL/c + · · ·

.
Step 4: Multiplying everything out and inverting term by term, show that
u(x, t) = c
∞
X
n=0
f(t −x/c −2nL/c)H(t −x/c −2nL/c)
+ c
∞
X
m=1
f(t + x/c −2mL/c)H(t + x/c −2mL/c).

118
Advanced Engineering Mathematics: A Second Course
6. Use transform methods to solve the wave equation
∂2u
∂t2 = c2 ∂2u
∂x2 −q′(t),
a < x < b,
0 < t,
with the boundary conditions u(a, t) = ux(b, t) = 0, 0 < t, and the initial conditions
u(x, 0) = 0, ut(x, 0) = −q(0), a < x < b.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
c2 d2U(x, s)
dx2
−s2U(x, s) = sQ(s),
a < x < b,
with the boundary conditions U(a, s) = U ′(b, s) = 0.
Step 2: Show that the eigenfunctions sin[kn(x −a)], where kn = (2n + 1)π/[2(b −a)] and
n = 0, 1, 2, . . ., satisfy the boundary conditions.
Step 3: Expand the right side of the diﬀerential equation using an eigenfunction expansion
consisting of sin[kn(x −a)]. Show that
sQ(s) = 4sQ(s)
π
∞
X
n=0
1
2n + 1 sin
(2n + 1)π(x −a)
2(b −a)

.
Step 4: Assuming that
U(x, s) =
∞
X
n=0
An sin
(2n + 1)π(x −a)
2(b −a)

,
show by direct substitution that
An = −4sc2Q(s)
π(2n + 1)

s2 + (2n + 1)2π2c2
4(b −a)2
−1
.
Step 5: Invert U(x, s) term by term and show that
u(x, t) = −4c2
π
∞
X
n=0
1
2n + 1 sin
(2n + 1)π(x −a)
2(b −a)
 Z t
0
q(τ) cos
(2n + 1)πc(t −τ)
2(b −a)

dτ.
7. Use transform methods to solve the wave equation
∂2u
∂t2 −∂2u
∂x2 = te−x,
0 < x < ∞,
0 < t,
with the boundary conditions u(0, t) = 1 −e−t, limx→∞|u(x, t)| ∼xn, n ﬁnite, 0 < t, and
the initial conditions u(x, 0) = 0, ut(x, 0) = x, 0 < x < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s2U(x, s) = −x −e−x
s2 ,
0 < x < ∞,

Advanced Transform Methods
119
with the boundary conditions
U(0, s) = 1
s −
1
s + 1
and
lim
x→∞|U(x, s)| ∼xn.
Step 2: Show that the solution to the previous step is
U(x, s) =
1
s −
1
s + 1 + 1
s2 −
1
s2 −1

e−sx + x
s2 −e−x
s2 +
e−x
s2 −1.
Step 3: Inverting term by term, show that
u(x, t) = xt −te−x + sinh(t)e−x +
h
1 −e−(t−x) + t −x −sinh(t −x)
i
H(t −x).
8. Use transform methods to solve the wave equation
∂2u
∂t2 −∂2u
∂x2 = xe−t,
0 < x < ∞,
0 < t,
with the boundary conditions u(0, t) = cos(t), limx→∞|u(x, t)| ∼xn, n ﬁnite, 0 < t, and
the initial conditions u(x, 0) = 1, ut(x, 0) = 0, 0 < x < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s2U(x, s) = −s −
x
s + 1,
0 < x < ∞,
with the boundary conditions U(0, s) = s/(s2 + 1) and limx→∞|U(x, s)| ∼xn.
Step 2: Show that the solution to the previous step is
U(x, s) =

s
s2 + 1 −1
s

e−sx + 1
s + x
s2 −x
s +
x
s + 1.
Step 3: Inverting term by term, show that u(x, t) = 1+xt−x+xe−t+[cos(t−x)−1]H(t−x).
9. Use transform methods to solve the wave equation
∂2u
∂t2 = ∂2u
∂x2 ,
0 < x < L,
0 < t,
with the boundary conditions
u(0, t) = 0,
∂2u(L, t)
∂t2
+ k
m
∂u(L, t)
∂x
= g,
0 < t,
and the initial conditions u(x, 0) = ut(x, 0) = 0, 0 < x < L, where k, m, and g are constants.

120
Advanced Engineering Mathematics: A Second Course
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s2U(x, s) = 0,
0 < x < L,
with the boundary conditions U(0, s) = 0 and s2U(L, s) + ω2U ′(L, s) = g/s, where ω2 =
k/m.
Step 2: Show that the solution to the previous step is
g sinh(sx)
U(x, s) =
.
s2[s sinh(sL) + ω2 cosh(sL)]
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = ±λni, where λn = ω2 cot(λnL)
with n = 1, 2, 3, . . ..
Step 4: Using Bromwich’s integral, show that
gx
2gω2
∞
sin(λnx) cos(λnt)
u(x, t) = ω2 −
L
X
.
λ2n(ω4 + ω2/L + λ2n) sin(λnL)
n=1
10. Use transform methods23 to solve the wave equation
∂2u
∂
= c2
 ∂u
x

,
0 < x < 1,
0 < t,
∂t2
∂x
∂x
with the boundary conditions limx
0 |u(x, t)| < ∞and u(1, t) = A sin(ωt), 0 < t, and
→
the initial conditions u(x, 0) = ut(x, 0) = 0, 0 < x < 1. Assume that 2ω = cβn, where
J0(βn) = 0.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d  dU(x, s)
x

s2
−
U(x, s) = 0,
0 < x < 1.
dx
dx
c2
with the boundary conditions lim
2
2
x→0 |U(x, s)| < ∞and U(1, s) = Aω/(s + ω ).
Step 2: Show that the solution to the previous step is
Aω
I0(2s√x/c)
U(x, s) =
.
s2 + ω2
I0(2s/c)
Step 3: Show that U(x, s) has simple poles at s = ±ωi and sn = ±cβni/2, where J0(βn) = 0,
n = 1, 2, 3, ...
23 Suggested by a problem solved by Brown, J., 1975: Stresses in towed cables during re-entry. J. Spacecr.
Rockets, 12, 524–527.
̸

Advanced Transform Methods
121
Step 4: Using Bromwich’s integral, show that
J0(2ω√x/c)
X
∞J0(βn
√x ) sin(βnct/2)
u(x, t) = A
sin(ωt) + Acω
.
J0(2ω/c)
(ω2
c
=
−
2β2n/4)J1(βn)
n
1
11. A lossless transmission line of length ℓhas a constant voltage E applied to the end
x = 0 while we insulate the other end [Vx(ℓ, t) = 0]. Find the voltage at any point on the
line if the initial current and charge are zero.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
s2
−
U(x, s) = 0,
0 < x < ℓ,
dx2
c2
with the boundary conditions U(0, s) = E/s and U ′(ℓ, s) = 0.
Step 2: Show that the solution to the previous step is
E cosh[s(ℓ−x)/c]
U(x, s) =
.
s cosh(sℓ/c)
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = ±(2n −1)cπi/(2ℓ) with
n = 1, 2, 3, . . .
Step 4: Using Bromwich’s integral, show that
4E
∞
1
(2n
1)πx
(2n
1)cπt
u(x, t) = E
in

−
−
X
s

cos
−
π
2n
1
2ℓ
n=1
−

2ℓ

.
Step 5: An alternative approach is to replace the hyperbolic functions with their exponential
deﬁnitions. Then,
E
U(x, s) =
h
e−sx/c −e−s(x+2ℓ)/c + e−s(x+4ℓ)/c
s
−· · ·
E
+
h
e−s(2ℓ−x)/c −e−s(4ℓ−x)/c + e−s(6ℓ−x)/c
i
s
−· · ·
i
after using the summation rule for the geometric series. Take the inverse by inspection and
show that
∞
u(x, t) = E
n
X
x + 2nℓ
(
=0
−1)nH

t −
c

+ E
n
X
∞
n

[(2n + 2)ℓ
(
=0
−1) H
t
−x]
−
c

.
12. Solve the equation of telegraphy without leakage
∂2u
∂u
∂2u
= CR
+ CL
,
0 < x < ℓ,
0 < t,
∂x2
∂t
∂t2

122
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions u(0, t) = 0, u(ℓ, t) = E, 0 < t, and the initial conditions
u(x, 0) = ut(x, 0) = 0, 0 < x < ℓ. Assume that 4π2L/CR2ℓ2 > 1. Why?
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
s2
−
U(x, s) = 0,
0 < x < ℓ,
dx2
c2
with the boundary conditions U(0, s) = E/s and U ′(ℓ, s) = 0.
Step 2: Show that the solution to the previous step is
E cosh[s(ℓ−x)/c]
U(x, s) =
.
s cosh(sℓ/c)
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = ±(2n −1)cπi/(2ℓ) with
n = 1, 2, 3, . . .
Step 4: Using Bromwich’s integral, show that
u(x, t)
x
2
=
E
ℓ−
e−t/2T
π
n
X
∞(−1)n
i
2T )
s n
nπx s n(t
√
n2δ2
i
−1/
√
+cos
n
ℓ
n2δ2
1
−1
=

t
p
n2δ2 −1/2T

.
13. The pressure and velocity oscillations from water hammer in a pipe without friction24
are given by the equations
∂p
2 ∂u
∂u
1 ∂p
=
c
t
−ρ
,
and
=
∂
∂x
∂t
−
,
ρ ∂x
where p(x, t) denotes the pressure perturbation, u(x, t) is the velocity perturbation, c is
the speed of sound in water, and ρ is the density of water. These two ﬁrst-order partial
diﬀerential equations can be combined to yield
∂2p
p
= c2 ∂2
,
0 < x < L,
0 < t.
∂t2
∂x2
Find the solution to this partial diﬀerential equation if p(0, t) = p0, and u(L, t) = 0, and
the initial conditions are p(x, 0) = p0, pt(x, 0) = 0, and u(x, 0) = u0.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2P(x, s)
s2
s
−
P(x, s) = −
p0,
0 < x < L,
dx2
c2
c2
with the boundary conditions P(0, s) = p0/s and P ′(L, s) = ρu0.
Step 2: Show that the solution to the previous step is
p0
ρu0c sinh(sx/c)
P(x, s) =
+
.
s
s cosh(sL/c)
24 See Rich, G. R., 1945: Water-hammer analysis by the Laplace-Mellin transformation. Trans. ASME,
67, 361–376.

Advanced Transform Methods
123
Step 3: Show that P(x, s) has simple poles at sn = ±(2n −1)cπi/(2L) with n = 1, 2, 3, . . .
and a removable singularity at s = 0.
Step 4: Using Bromwich’s integral, show that
p(x, t) = p0 −4ρu0c
π
∞
X
n=1
(−1)n
2n −1 sin
(2n −1)πx
2L

sin
(2n −1)cπt
2L

.
14. Use Laplace transforms to solve the wave equation25
∂2u
∂t2 = c2
∂2u
∂r2 + 2
r
∂u
∂r −2u
r2

,
a < r < ∞,
0 < t,
subject to the boundary conditions that
u(a, t) = A

1 −e−ct/a
H(t),
lim
r→∞u(r, t) →0,
0 < t,
and the initial conditions that u(r, 0) = ut(r, 0) = 0, a < r < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(r, s)
dr2
+ 2
r
dU(r, s)
dr
−2
r2 U(r, s) −s2
c2 U(r, s) = 0,
a < r < ∞,
with the boundary condition
U(a, s) = A
s −
A
s + c/a,
lim
r→∞|U(r, s)| < ∞.
Step 2: Show that the solution to the previous step is
U(r, s) = A
 a2
sr2 −
a2
r2(s + c/a) −c
a
a2
r2 −a
r

1
(s + c/a)2

e−s(r−a)/c.
Step 3: Use tables and the second shifting theorem to show that
u(r, t) = A
a2
r2 −
a2
r2 + cτ
a
a2
r2 −a
r

e−cτ/a

H(τ),
where τ = t −(r −a)/c.
15. Use Laplace transforms to solve the wave equation26
∂2(ru)
∂t2
= c2 ∂2(ru)
∂r2
,
a < r < ∞,
0 < t,
25 Wolf, J. P., and G. R. Darbre, 1986: Time-domain boundary element method in visco-elasticity with
application to a spherical cavity. Soil Dynam. Earthq. Eng., 5, 138–148.
26 Originally solved using Fourier transforms by Sharpe, J. A., 1942: The production of elastic waves by
explosion pressures. I. Theory and empirical ﬁeld observations. Geophysics, 7, 144–154.

124
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions that
−ρc2
∂2u
∂r2 + 2
3r
∂u
∂r

r=a
= p0e−αtH(t),
lim
r→∞u(r, t) →0,
0 < t,
where α > 0, and the initial conditions that u(r, 0) = ut(r, 0) = 0, a < r < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2[rU(r, s)]
dr2
−s2
c2 [rU(r, s)] = 0,
a < r < ∞.
with the boundary condition
−ρc2
d2U(a, s)
dr2
+ 2
3a
dU(a, s)
dr

=
p0
s + α
and
lim
r→∞|U(r, s)| < ∞.
Step 2: Show that the solution to the previous step is
U(r, s) = −
ap0 exp[−s(r −a)/c]
ρr(s + α)[s2 + 4sc/(3a) + 4c2/(3a2)].
Step 3: Show that U(r, s) has three simple poles, s = −α and s = −β/
√
2 ± βi, where
β = 2
√
2c/(3a).
Step 4: Use Bromwich’s integral and show that
u(r, t) =
ap0
ρr[(β/
√
2 −α)2 + β2]

e−βτ/
√
2
 1
√
2 −α
β

sin(βτ) + cos(βτ)

−e−ατ

H(τ),
where τ = t −(r −a)/c.
16. Consider a vertical rod or column of length L that is supported at both ends. The
elastic waves that arise when the support at the bottom is suddenly removed are governed
by the wave equation27
∂2u
∂t2 = c2 ∂2u
∂x2 + g,
0 < x < L,
0 < t,
where g denotes the gravitational acceleration, c2 = E/ρ, E is Young’s modulus, and ρ is the
mass density. Find the wave solution if the boundary conditions are ux(0, t) = ux(L, t) = 0,
0 < t, and the initial conditions are
u(x, 0) = −gx2
2c2 ,
∂u(x, 0)
∂t
= 0,
0 < x < L.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s2
c2 U(x, s) = sgx2
2c4 −g
sc2 ,
0 < x < L,
27 See Hall, L. H., 1953: Longitudinal vibrations of a vertical column by the method of Laplace transform.
Am. J. Phys., 21, 287–292.

Advanced Transform Methods
125
with U ′(0, s) = U ′(L, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = gL cosh(sx/c)
cs2 sinh(sL/c) −gx2
2sc2 .
Step 3: Show that U(x, s) has poles that are located at s = 0 and sn = ±nπci/L, where
n = 1, 2, 3, . . ..
Step 4: Use Bromwich’s integral and show that
u(x, t) = gt2
2 −gL2
6c2 −2gL2
c2π2
∞
X
n=1
(−1)n
n2
cos
nπx
L

cos
nπct
L

.
17. Use Laplace transforms to solve the hyperbolic equation
∂2u
∂t2 −∂2u
∂x2 + 1 = 0,
0 < x < 1,
0 < t,
subject to the boundary conditions that ux(0, t) = 0, ux(1, t) = 1, 0 < t, and the initial
conditions that u(x, 0) = ut(x, 0) = 0, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s2U(x, s) = 1
s + x2 −1,
0 < x < 1,
with U ′(0, s) = 0 and U ′(1, s) = 1/s.
Step 2: Show that the solution to the previous step is
U(x, s) = 1 −x2
s2
−1
s3 −2
s4 + cosh(sx)
s2 sinh(s) + 2 cosh(sx)
s3 sinh(s) .
Step 3: Show that U(x, s) has poles that are located at s = 0 and zn = ±nπi, where
n = 1, 2, 3, . . ..
Step 4: Use Bromwich’s integral and show that
u(x, t) = 2t
3 + x2
2 −1
6 −2
∞
X
n=1
(−1)n cos(nπx)
cos(nπt)
n2π2
+ 2 sin(nπt)
n3π3

.
18. Solve the telegraph-like equation28
∂2u
∂t2 + k ∂u
∂t = c2
∂2u
∂x2 + α∂u
∂x

,
0 < x < ∞,
0 ≤t,
28 See Abbott, M. R., 1959: The downstream eﬀect of closing a barrier across an estuary with particular
reference to the Thames. Proc. R. Soc. London, Ser. A, 251, 426–439.

126
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions ux(0, t) = −u0δ(t), limx→∞u(x, t) →0, 0 ≤t, and the
initial conditions u(x, 0) = u0, ut(x, 0) = 0, 0 < x < ∞, with αc > k. Here δ(t) denotes the
Dirac delta function.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
+ αdU(x, s)
dx
−
s2 + ks
c2

U(x, s) = −
s + k
c2

u0,
0 < x < ∞,
with U ′(0, s) = −u0, and limx→∞U(x, s) →0.
Step 2: Show that the solution to the previous step is
U(x, s) = u0
s + u0e−αx/2
exp

−x
q s + k
2
2 + a2/c

α
2 +
q
(s + k
2)2 + a2/c
,
where 4a2 = α2c2 −k2 > 0.
Step 3: Using the ﬁrst and second shifting theorems and the property that
F
p
s2 + a2

= L
"
f(t) −a
Z t
0
J1
 a
√
t2 −τ 2 
√
t2 −τ 2
τf(τ) dτ
#
,
show that
u(x, t) = u0 + u0ce−kt/2H(t −x/c)
"
e−αct/2 −a
Z t
x/c
J1
 a
√
t2 −τ 2 
√
t2 −τ 2
τe−αcτ/2dτ
#
.
19. As an electric locomotive travels down a track at the speed V , the pantograph (the
metallic framework that connects the overhead power lines to the locomotive) pushes up
the line with a force P. Let us ﬁnd the behavior29 of the overhead wire as a pantograph
passes between two supports of the electrical cable that are located a distance L apart. We
model this system as a vibrating string with a point load:
∂2u
∂t2 = c2 ∂2u
∂x2 + P
ρV δ

t −x
V

,
0 < x < L,
0 < t.
Let us assume that the wire is initially at rest [u(x, 0) = ut(x, 0) = 0 for 0 < x < L] and
ﬁxed at both ends [u(0, t) = u(L, t) = 0 for 0 < t].
Step 1: Take the Laplace transform of the partial diﬀerential equation and show that
s2U(x, s) = c2 d2U(x, s)
dx2
+ P
ρV e−xs/V ,
0 < x < L.
29 See Oda, O., and Y. Ooura, 1976: Vibrations of catenary overhead wire. Q. Rep., (Tokyo) Railway
Tech. Res. Inst., 17, 134–135.

Advanced Transform Methods
127
Step 2: Solve the ordinary diﬀerential equation in Step 1 as a Fourier half-range sine series
U(x, s) =
∞
X
n=1
Bn(s) sin
nπx
L

,
where
Bn(s) =
2Pβn
ρL(β2n −α2n)

1
s2 + α2n
−
1
s2 + β2n
 h
1 −(−1)ne−Ls/V i
,
αn = nπc/L and βn = nπV/L. This solution satisﬁes the boundary conditions.
Step 3: By inverting the solution in Step 2, show that
u(x, t) = 2P
ρL
∞
X
n=1
sin(βnt)
α2n −β2n
−V
c
sin(αnt)
α2n −β2n

sin
nπx
L

−2P
ρL H

t −L
V
 ∞
X
n=1
(−1)n sin
nπx
L
 sin[βn(t −L/V )]
α2n −β2n
−V
c
sin[αn(t −L/V )]
α2n −β2n

,
or
u(x, t) = 2P
ρL
∞
X
n=1
sin(βnt)
α2n −β2n
−V
c
sin(αnt)
α2n −β2n

sin
nπx
L

−2P
ρL H

t −L
V
 ∞
X
n=1
sin
nπx
L
 sin(βnt)
α2n −β2n
−V
c (−1)n sin[αn(t −L/V )]
α2n −β2n

.
The ﬁrst term in both summations represents the static uplift on the line; this term dis-
appears after the pantograph passes. The second term in both summations represents the
vibrations excited by the traveling force. Even after the pantograph passes, they continue
to exist.
20. Solve the wave equation
1
c2
∂2u
∂t2 −∂2u
∂r2 −1
r
∂u
∂r + u
r2 = δ(r −α)
α2
,
0 ≤r < a,
0 < t,
where 0 < α < a, subject to the boundary conditions limr→0 |u(r, t)| < ∞, ur(a, t) +
h u(a, t)/a = 0, 0 < t, and the initial conditions u(r, 0) = ut(r, 0) = 0, 0 ≤r < a.
Step 1: Take the Laplace transform of the partial diﬀerential equation and show that
d2U(r, s)
dr2
+ 1
r
dU(r, s)
dr
−
s2
c2 + 1
r2

U(r, s) = −δ(r −α)
sα2
,
0 ≤r < a,
with limr→0 |U(r, s)| < ∞and U ′(a, s) + h
aU(a, s) = 0.
Step 2: Show that the Dirac delta function can be reexpressed as the Fourier-Bessel series
δ(r −α) = 2α
a2
∞
X
n=1
β2
n J1(βnα/a)
(β2n + h2 −1)J2
1(βn)J1(βnr/a),
0 ≤r < a,
where βn is the nth root of βJ′
1(β) + h J1(β) = βJ0(β) + (h −1)J1(β) = 0 and J0(·), J1(·)
are the zeroth and ﬁrst-order Bessel functions of the ﬁrst kind, respectively.

128
Advanced Engineering Mathematics: A Second Course
Step 3: Show that the solution to the ordinary diﬀerential equation in Step 1 is
U(r, s) = 2
α
∞
X
n=1
J1(βnα/a)J1(βnr/a)
(β2n + h2 −1) J2
1(βn)
1
s −
s
s2 + c2β2n/a2

.
Note that this solution satisﬁes the boundary conditions.
Step 4: Taking the inverse of the Laplace transform in Step 3, show that the solution to
the partial diﬀerential equation is
u(r, t) = 2
α
∞
X
n=1
J1(βnα/a)J1(βnr/a)
(β2n + h2 −1) J2
1(βn)

1 −cos
cβnt
a

.
21. Solve the hyperbolic equation
∂2u
∂x∂t + u = 0,
0 < x, t,
subject to the boundary conditions u(0, t) = e−t, limx→∞u(x, t) →0, 0 < t, and u(x, 0) = 1,
limt→∞|u(x, t)| < Mekt, 0 < k, M, x, t.
Step 1: Take the Laplace transform of the partial diﬀerential equation and show that
sdU(x, s)
dx
+ U = 0,
0 < x < ∞,
with U(0, s) = 1/(s + 1) and limx→∞U(x, s) →0.
Step 2: Show that
U(x, s) = e−x/s
s + 1 = e−x/s
s
−
e−x/s
s(s + 1).
Step 3: Using tables and the convolution theorem, show that the solution is
u(x, t) = J0(2
√
xt ) −e−t
Z t
0
eτJ0(2√xτ ) dτ,
where J0(·) is the Bessel function of the ﬁrst kind and order zero.
22. Solve the hyperbolic equation
∂2u
∂x∂t + a∂u
∂t + b∂u
∂x = 0,
0 < a, b, x, t,
subject to the boundary conditions u(0, t) = ect, limx→∞u(x, t) →0, 0 < t, and the initial
conditions u(x, 0) = 1, limt→∞|u(x, t)| < Mekt, 0 < k, M, t, x.
Step 1: Take the Laplace transform of the partial diﬀerential equation and show that
(s + b)dU(x, s)
dx
+ asU = a,
0 < x < ∞,
with U(0, s) = 1/(s −c) and limx→∞U(x, s) →0.

Advanced Transform Methods
129
Step 2: Show that
U(x, s) = 1
s + c e−ax
s(s −c) exp
 bx
s + b

.
Step 3: Using tables, the ﬁrst shifting theorem, and the convolution theorem, show that
the solution is
u(x, t) = 1 + c ect−ax
Z t
0
e−(b+c)τI0

2
√
bxτ

dτ,
where I0(·) is the modiﬁed Bessel function of the ﬁrst kind and order zero.
2.5 THE SOLUTION OF THE HEAT EQUATION BY USING LAPLACE TRANSFORMS
In the previous section we showed that we can solve the wave equation by the method
of Laplace transforms. This is also true for the heat equation. Once again, we take the
Laplace transform with respect to time. From the deﬁnition of Laplace transforms,
L[u(x, t)] = U(x, s),
(2.5.1)
L[ut(x, t)] = sU(x, s) −u(x, 0),
(2.5.2)
and
L[uxx(x, t)] = d2U(x, s)
dx2
.
(2.5.3)
We next solve the resulting ordinary diﬀerential equation, known as the auxiliary equation,
along with the corresponding Laplace transformed boundary conditions. The initial condi-
tion gives us the value of u(x, 0). The ﬁnal step is the inversion of the Laplace transform
U(x, s). We typically use the inversion integral.
• Example 2.5.1
To illustrate these concepts, we solve a heat conduction problem30 in a plane slab of
thickness 2L. Initially the slab has a constant temperature of unity. For 0 < t, we allow
both faces of the slab to radiatively cool in a medium that has a temperature of zero.
If u(x, t) denotes the temperature, a2 is the thermal diﬀusivity, h is the relative emis-
sivity, t is the time, and x is the distance perpendicular to the face of the slab and measured
from the middle of the slab, then the governing equation is
∂u
∂t = a2 ∂2u
∂x2 ,
−L < x < L,
0 < t,
(2.5.4)
with the initial condition
u(x, 0) = 1,
−L < x < L,
(2.5.5)
and boundary conditions
∂u(L, t)
∂x
+ hu(L, t) = 0,
∂u(−L, t)
∂x
+ hu(−L, t) = 0,
0 < t.
(2.5.6)
30 Goldstein, S., 1932: The application of Heaviside’s operational method to the solution of a problem
in heat conduction. Z. Angew. Math. Mech., 12, 234–243.

130
Advanced Engineering Mathematics: A Second Course
Taking the Laplace transform of Equation 2.5.4 and substituting the initial condition,
a2 d2U(x, s)
dx2
−sU(x, s) = −1.
(2.5.7)
If we write s = a2q2, Equation 2.5.7 becomes
d2U(x, s)
dx2
−q2U(x, s) = −1
a2 .
(2.5.8)
From the boundary conditions, U(x, s) is an even function in x and we may conveniently
write the solution as
U(x, s) = 1
s + A cosh(qx).
(2.5.9)
From Equation 2.5.6,
qA sinh(qL) + h
s + hA cosh(qL) = 0,
(2.5.10)
and
U(x, s) = 1
s −
h cosh(qx)
s[q sinh(qL) + h cosh(qL)].
(2.5.11)
The inverse of U(x, s) consists of two terms. The ﬁrst term is simply unity. We will
invert the second term by contour integration.
We begin by examining the nature and location of the singularities in the second term.
Using the product formulas for the hyperbolic cosine and sine functions, the second term
equals
h

1 + 4q2x2
π2
 
1 + 4q2x2
9π2

· · ·
s

q2L

1 + q2L2
π2
 
1 + q2L2
4π2

· · · + h

1 + 4q2L2
π2
 
1 + 4q2L2
9π2

· · ·
.
(2.5.12)
Because q2 = s/a2, Equation 2.5.12 shows that we do not have any √s in the transform
and we need not concern ourselves with branch points and cuts. Furthermore, we have only
simple poles: one located at s = 0 and the others where
q sinh(qL) + h cosh(qL) = 0.
(2.5.13)
If we set q = iλ, Equation 2.5.13 becomes
h cos(λL) −λ sin(λL) = 0,
or
λL tan(λL) = hL.
(2.5.14)
From Bromwich’s integral,
L−1

h cosh(qx)
s[q sinh(qL) + h cosh(qL)]

=
1
2πi
I
C
h cosh(qx)etz
z[q sinh(qL) + h cosh(qL)] dz,
(2.5.15)

Advanced Transform Methods
131
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1
1.2
distance
time
U(X,T)
Figure 2.5.1: The temperature within the portion of a slab 0 < x/L < 1 at various times a2t/L2 if the
faces of the slab radiate to free space at temperature zero and the slab initially has the temperature 1. The
parameter hL = 1.
where q = z1/2/a and the closed contour C consists of Bromwich’s contour plus a semicircle
of inﬁnite radius in the left half of the z-plane. The residue at z = 0 is 1 while at zn = −a2λ2
n,
Res

h cosh(qx)etz
z[q sinh(qL) + h cosh(qL)]; zn

= lim
z→zn
h(z + a2λ2
n) cosh(qx)etz
z[q sinh(qL) + h cosh(qL)]
(2.5.16)
= lim
z→zn
h cosh(qx)etz
z[(1 + hL) sinh(qL) + qL cosh(qL)]/(2a2q)
(2.5.17)
=
2ha2λni cosh(iλnx) exp(−λ2
na2t)
(−a2λ2n)[(1 + hL)i sin(λnL) + iλnL cos(λnL)]
(2.5.18)
= −
2h cos(λnx) exp(−a2λ2
nt)
λn[(1 + hL) sin(λnL) + λnL cos(λnL)].
(2.5.19)
Therefore, the inversion of U(x, s) is
u(x, t) = 1 −

1 −2h
∞
X
n=1
cos(λnx) exp(−a2λ2
nt)
λn[(1 + hL) sin(λnL) + λnL cos(λnL)]

,
(2.5.20)
or
u(x, t) = 2h
∞
X
n=1
cos(λnx) exp(−a2λ2
nt)
λn[(1 + hL) sin(λnL) + λnL cos(λnL)].
(2.5.21)
We can further simplify Equation 2.5.21 by using h/λn = tan(λnL). This yields hL =
λnL tan(λnL). Substituting these relationships into Equation 2.5.21 and simplifying,
u(x, t) = 2
∞
X
n=1
sin(λnL) cos(λnx) exp(−a2λ2
nt)
λnL + sin(λnL) cos(λnL)
.
(2.5.22)
Figure 2.5.1 illustrates Equation 2.5.22. It was created using the MATLAB script

132
Advanced Engineering Mathematics: A Second Course
clear
hL = 1; m = 0; M = 100; dx = 0.05; dt = 0.05;
% create initial guess at zero n
zero = zeros(length(M));
for n = 1:10000
k1 = 0.1*n; k2 = 0.1*(n+1);
prod = k1 * tan(k1); y1 = hL - prod; y2 = hL - k2 * tan(k2);
if (y1*y2 <= 0 & prod < 2 & m < M) m = m+1; zero(m) = k1; end;
end;
% use Newton-Raphson method to improve values of zero n
for n = 1:M; for k = 1:10
f = hL - zero(n) * tan(zero(n));
fp = - tan(zero(n)) - zero(n) * sec(zero(n))^2;
zero(n) = zero(n) - f / fp;
end; end;
% compute Fourier coefficients
for m = 1:M
a(m) = 2 * sin(zero(m)) / (zero(m) + sin(zero(m))*cos(zero(m)));
end
% compute grid and initialize solution
X = [0:dx:1]; T = [0:dt:2];
u = zeros(length(T),length(X));
XX = repmat(X,[length(T) 1]); TT = repmat(T’,[1 length(X)]);
% compute solution from Equation 2.5.22
for m = 1:M
u = u + a(m) * cos(zero(m)*XX) .* exp(-zero(m)*zero(m)*TT);
end
surf(XX,TT,u)
xlabel(’distance’,’Fontsize’,20); ylabel(’time’,’Fontsize’,20)
zlabel(’U(X,T)’,’Fontsize’,20)
⊓⊔
• Example 2.5.2: Heat dissipation in disc brakes
Disc brakes consist of two blocks of frictional material known as pads that press against
each side of a rotating annulus, usually made of a ferrous material. In this problem we deter-
mine the transient temperatures reached in a disc brake during a single brake application.31
If we ignore the errors introduced by replacing the cylindrical portion of the drum by a
rectangular plate, we can model our disc brakes as a one-dimensional solid, which friction
heats at both ends. Assuming symmetry about x = 0, the boundary condition there is
ux(0, t) = 0. To model the heat ﬂux from the pads, we assume a uniform disc deceleration
that generates heat from the frictional surfaces at the rate N(1 −Mt), where M and N are
experimentally determined constants.
If u(x, t), κ, and a2 denote the temperature, thermal conductivity, and diﬀusivity of
the rotating annulus, respectively, then the heat equation is
∂u
∂t = a2 ∂2u
∂x2 ,
0 < x < L,
0 < t,
(2.5.23)
31 From Newcomb, T. P., 1958: The ﬂow of heat in a parallel-faced inﬁnite solid. Brit. J. Appl. Phys.,
9, 370–372. See also Newcomb, T. P., 1958/59: Transient temperatures in brake drums and linings. Proc.
Inst. Mech. Eng., Auto. Div., 227–237; Newcomb, T. P., 1959: Transient temperatures attained in disk
brakes. Brit. J. Appl. Phys., 10, 339–340.

Advanced Transform Methods
133
with the boundary conditions
∂u(0, t)
∂x
= 0,
κ∂u(L, t)
∂x
= N(1 −Mt),
0 < t.
(2.5.24)
The boundary condition at x = L gives the frictional heating of the disc pads.
Introducing the Laplace transform of u(x, t), deﬁned as
U(x, s) =
Z ∞
0
u(x, t)e−st dt,
(2.5.25)
the equation to be solved becomes
d2U
dx2 −s
a2 U = 0,
(2.5.26)
subject to the boundary conditions that
dU(0, s)
dx
= 0,
and
dU(L, s)
dx
= N
κ
1
s −M
s2

.
(2.5.27)
The solution of Equation 2.5.26 is
U(x, s) = A cosh(qx) + B sinh(qx),
(2.5.28)
where q = s1/2/a. Using the boundary conditions, the solution becomes
U(x, s) = N
κ
1
s −M
s2
 cosh(qx)
q sinh(qL).
(2.5.29)
It now remains to invert the transform, Equation 2.5.29.
We will invert cosh(qx)/
[sq sinh(qL)]; the inversion of the second term follows by analog.
Our ﬁrst concern is the presence of s1/2 because this is a multivalued function. However,
when we replace the hyperbolic cosine and sine functions with their Taylor expansions,
cosh(qx)/[sq sinh(qL)] contains only powers of s and is, in fact, a single-valued function.
From Bromwich’s integral,
L−1
 cosh(qx)
sq sinh(qL)

=
1
2πi
Z c+∞i
c−∞i
cosh(qx)etz
zq sinh(qL) dz,
(2.5.30)
where q = z1/2/a. Just as in the previous example, we replace the hyperbolic cosine and
sine with their product expansion to determine the nature of the singularities. The point
z = 0 is a second-order pole. The remaining poles are located where z1/2
n
L/a = nπi, or
zn = −n2π2a2/L2, where n = 1, 2, 3, . . .. We have chosen the positive sign because z1/2
must be single-valued; if we had chosen the negative sign, the answer would have been the
same. Our expansion also shows that the poles are simple.
Having classiﬁed the poles, we now close Bromwich’s contour, which lies slightly to the
right of the imaginary axis, with an inﬁnite semicircle in the left half-plane, and use the

134
Advanced Engineering Mathematics: A Second Course
residue theorem. The values of the residues are
Res
cosh(qx)etz
zq sinh(qL) ; 0

= 1
1! lim
z→0
d
dz
(z −0)2 cosh(qx)etz
zq sinh(qL)

(2.5.31)
= lim
z→0
d
dz
z cosh(qx)etz
q sinh(qL)

(2.5.32)
= a2
L lim
z→0
d
dz
z
h
1 + zx2
2!a2 + · · ·
i h
1 + tz + t2z2
2! + · · ·
i
z + L2z2
3!a2 + · · ·

(2.5.33)
= a2
L lim
z→0
d
dz

1 + tz + zx2
2a2 −zL2
3!a2 + · · ·

(2.5.34)
= a2
L

t + x2
2a2 −L2
6a2

,
(2.5.35)
and
Res
cosh(qx)etz
zq sinh(qL) ; zn

=

lim
z→zn
cosh(qx)
zq
etz

lim
z→zn
z −zn
sinh(qL)

(2.5.36)
= lim
z→zn
cosh(qx)etz
zq cosh(qL)L/(2a2q)
(2.5.37)
= cosh(nπxi/L) exp(−n2π2a2t/L2)
(−n2π2a2/L2) cosh(nπi)L/(2a2)
(2.5.38)
= −2L(−1)n
n2π2
cos(nπx/L)e−n2π2a2t/L2.
(2.5.39)
When we sum all of the residues from both inversions, the solution is
u(x, t) = a2N
κL

t + x2
2a2 −L2
6a2

−2LN
κπ2
∞
X
n=1
(−1)n
n2
cos(nπx/L)e−n2π2a2t/L2
−a2NM
κL
t2
2 + tx2
2a2 −tL2
6a2 +
x4
24a4 −x2L2
12a4 + 7L4
360a4

−2L3NM
a2κπ4
∞
X
n=1
(−1)n
n4
cos(nπx/L)e−n2π2a2t/L2.
(2.5.40)
Figure 2.3.2 shows the temperature in the brake lining at various places within the
lining [x′ = x/L] if a2 = 3.3 × 10−3 cm2/sec, κ = 1.8 × 10−3 cal/(cm sec◦C), L = 0.48 cm,
and N = 1.96 cal/(cm2 sec). Initially the frictional heating results in an increase in the disc
brake’s temperature. As time increases, the heating rate decreases and radiative cooling
becomes suﬃciently large that the temperature begins to fall.
⊓⊔
• Example 2.5.3
In the previous example we showed that Laplace transforms are particularly useful
when the boundary conditions are time dependent. Consider now the case when one of the
boundaries is moving.

Advanced Transform Methods
135
Figure 2.5.2: Typical curves of transient temperature at diﬀerent locations in a brake lining.
Circles
denote computed values while squares are experimental measurements. (From Newcomb, T. P., 1958: The
ﬂow of heat in a parallel-faced inﬁnite solid. Brit. J. Appl. Phys., 9, 372 with permission.)
We wish to solve the heat equation
∂u
∂t = a2 ∂2u
∂x2 ,
βt < x < ∞,
0 < t,
(2.5.41)
subject to the boundary conditions
u(x, t)

x=βt = f(t),
and
lim
x→∞u(x, t) →0,
0 < t,
(2.5.42)
and the initial condition
u(x, 0) = 0,
0 < x < ∞.
(2.5.43)
This type of problem arises in combustion problems where the boundary moves due to the
burning of the fuel.
We begin by introducing the coordinate η = x −βt. Then the problem can be refor-
mulated as
∂u
∂t −β ∂u
∂η = a2 ∂2u
∂η2 ,
0 < η < ∞,
0 < t,
(2.5.44)
subject to the boundary conditions
u(0, t) = f(t),
lim
η→∞u(η, t) →0,
0 < t,
(2.5.45)
and the initial condition
u(η, 0) = 0,
0 < η < ∞.
(2.5.46)

136
Advanced Engineering Mathematics: A Second Course
Taking the Laplace transform of Equation 2.5.44, we have that
d2U(η, s)
dη2
+ β
a2
dU(η, s)
dη
−s
a2 U(η, s) = 0,
(2.5.47)
with
U(0, s) = F(s),
and
lim
η→∞U(η, s) →0.
(2.5.48)
The solution to Equation 2.5.47 and Equation 2.5.48 is
U(η, s) = F(s) exp
 
−βη
2a2 −η
a
r
s + β2
4a2
!
.
(2.5.49)
Because
L[Φ(η, t)] = exp
 
−η
a
r
s + β2
4a2
!
,
(2.5.50)
where
Φ(η, t) = 1
2

e−βη/2a2erfc

η
2a
√
t −β
√
t
2a

+ eβη/2a2erfc

η
2a
√
t + β
√
t
2a

,
(2.5.51)
and
erfc(x) = 1 −
2
√π
Z x
0
e−η2 dη,
(2.5.52)
we have by the convolution theorem that
u(η, t) = e−βη/2a2 Z t
0
f(t −τ)Φ(η, τ) dτ,
(2.5.53)
or
u(x, t) = e−β(x−βt)/2a2 Z t
0
f(t −τ)Φ(x −βτ, τ) dτ.
(2.5.54)
Problems
1. Solve the heat equation
∂u
∂t = ∂2u
∂x2 −a2(u −T0),
0 < x < 1,
0 < t,
subject to the boundary conditions ux(0, t) = ux(1, t) = 0, 0 < t, and the initial condition
u(x, 0) = 0, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−(s + a2)U(x, s) = −a2T0
s
,
0 < x < 1,
subject to the boundary conditions U ′(0, s) = U ′(1, s) = 0.

Advanced Transform Methods
137
Step 2: Show that the solution to the previous step is
U(x, s) = T0
1
s −
1
s + a2

.
Step 3: Invert U(x, s) and show that u(x, t) = T0

1 −e−a2t
.
2. Solve
∂u
∂t = ∂2u
∂x2 ,
0 < x < 1,
0 < t,
subject to the boundary conditions ux(0, t) = 0, u(1, t) = t, 0 < t, and the initial condition
u(x, 0) = 0, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−sU(x, s) = 0,
0 < x < 1,
subject to the boundary conditions U ′(0, s) = 0 and U(1, s) = 1/s2.
Step 2: Show that the solution to the previous step is
U(x, s) = cosh(x√s )
s2 cosh(√s ).
Step 3: Show that U(x, s) has a second-order pole at s = 0 and simple poles at sn =
−(2n −1)2π2/4 with √zn = (2n −1)πi/2, where n = 1, 2, 3, . . ..
Step 4: Use Bromwich’s integral and show that
u(x, t) = t + 1
2(x2 −1) −16
π3
∞
X
n=1
(−1)n
(2n −1)3 cos
(2n −1)πx
2

exp

−(2n −1)2π2t
4

.
3. Solve
∂u
∂t = ∂2u
∂x2 ,
0 < x < 1,
0 < t,
subject to the boundary conditions u(0, t) = 0, u(1, t) = 1, 0 < t, and the initial condition
u(x, 0) = 0, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−sU(x, s) = 0,
0 < x < 1,
subject to the boundary conditions U(0, s) = 0 and U(1, s) = 1/s.
Step 2: Show that the solution to the previous step is U(x, s) = sinh(x√s ) /[s sinh(√s )].
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = −n2π2 with √zn = nπi,
where n = 1, 2, 3, . . ..

138
Advanced Engineering Mathematics: A Second Course
Step 4: Use Bromwich’s integral and show that
u(x, t) = x + 2
π
∞
X
n=1
(−1)n
n
sin(nπx)e−n2π2t.
4. Solve
∂u
∂t = ∂2u
∂x2 ,
−1
2 < x < 1
2,
0 ≤t,
subject to the boundary conditions ux
 −1
2, t

= 0, ux
  1
2, t

= δ(t), 0 ≤t, and the initial
condition u(x, 0) = 0, −1
2 < x < 1
2. Here δ(t) is the Dirac delta function.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−sU(x, s) = 0,
−1
2 < x < 1
2,
subject to the boundary conditions U ′ −1
2, s

= 0 and U ′  1
2, s

= 1.
Step 2: Show that the solution to the previous step is
U(x, s) = cosh

(x + 1
2)√s

√s sinh(√s )
.
Step 3: Replacing the hyperbolic functions by their exponential deﬁnition, show that
U(x, s) = 1
√s

exp
 x −1
2
 √s

+ exp

−
 x + 3
2
 √s
	 
1 + e−2√s + e−4√s + · · ·

.
Step 4: Taking the inverse of U(x, s) term by term, show that
u(x, t) =
1
√
π t
∞
X
n=0
(
exp
"
−
 2n + 1
2 −x
2
4t
#
+ exp
"
−
 2n + 3
2 + x
2
4t
#)
.
Step 5: Show that U(x, s) has simple poles at s = 0 and sn = −n2π2 with √zn = nπi,
where n = 1, 2, 3, . . ..
Step 6: Use Bromwich’s integral and show that
u(x, t) = 1 + 2
∞
X
n=1
(−1)n cos

nπ
 x + 1
2

e−n2π2t.
5. Solve
∂u
∂t −∂2u
∂x2 = 1,
0 < x < 1,
0 < t,
subject to the boundary conditions u(0, t) = u(1, t) = 0, 0 < t, and the initial condition
u(x, 0) = 0, 0 < x < 1.

Advanced Transform Methods
139
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−sU(x, s) = −1
s,
0 < x < 1,
subject to the boundary conditions U(0, s) = U(1, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = 1 −cosh(x√s )
s2
−[1 −cosh(√s )] sinh(x√s )
s2 sinh(√s )
.
Step 3: Show that U(x, s) has a second-order pole at s = 0 and simple poles at sn = −n2π2
with √zn = nπi, where n = 1, 2, 3, . . ..
Step 4: Use Bromwich’s integral and show that
u(x, t) = x(1 −x)
2
−4
π3
∞
X
m=1
sin[(2m −1)πx]
(2m −1)3
e−(2m−1)2π2t.
6. Solve32
∂u
∂t = a2 ∂2u
∂x2 ,
0 < x < ∞,
0 < t,
subject to the boundary conditions u(0, t) = 1, limx→∞u(x, t) →0, 0 < t, and the initial
condition u(x, 0) = 0, 0 < x < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s
a2 U(x, s) = 0,
0 < x < ∞,
subject to the boundary conditions U(0, s) = 1/s and limx→∞|U(x, s)| < ∞.
Step 2: Show that the solution to the previous step is U(x, s) = e−x√s/a/s.
Step 3: From an extensive table of inverses, show that u(x, t) = erfc

x/(2a
√
t )

, where
erfc(·) is the complementary error function.
7. Solve
∂u
∂t = ∂2u
∂x2 ,
0 < x < ∞,
0 < t,
subject to the boundary conditions ux(0, t) = 1, limx→∞u(x, t) →0, 0 < t, and the initial
condition u(x, 0) = 0, 0 < x < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−sU(x, s) = 0,
0 < x < ∞,
32 If u(x, t) denotes the Eulerian velocity of a viscous ﬂuid in the half space x > 0 and parallel to the wall
located at x = 0, then this problem was ﬁrst solved by Stokes, G. G., 1850: On the eﬀect of the internal
friction of ﬂuids on the motions of pendulums. Proc. Cambridge Philos. Soc., 9, Part II, [8]–[106].

140
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions U ′(0, s) = 1/s and limx→∞|U(x, s)| < ∞.
Step 2: Show that the solution to the previous step is U(x, s) = −e−x√s/s3/2.
Step 3: From an extensive table of inverses, show that
u(x, t) = x erfc
 x
2
√
t

−2
r
t
π exp

−x2
4t

,
where erfc(·) is the complementary error function.
8. Solve
∂u
∂t = ∂2u
∂x2 ,
0 < x < ∞,
0 < t,
subject to the boundary conditions u(0, t) = 1, limx→∞u(x, t) →0, 0 < t, and the initial
condition u(x, 0) = e−x, 0 < x < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−sU(x, s) = −e−x,
0 < x < ∞,
subject to the boundary conditions U(0, s) = 1/s and limx→∞|U(x, s)| < ∞.
Step 2: Show that the solution to the previous step is
U(x, s) = e−x
s −1 +
1
s −
1
s −1

e−x√s.
Step 3: From an extensive table of inverses, show that
u(x, t) = et−x + erfc
 x
2
√
t

−1
2et

e−xerfc
 x
2
√
t −
√
t

+ exerfc
 x
2
√
t +
√
t

,
where erfc(·) is the complementary error function.
9. Solve
∂u
∂t = a2
∂2u
∂x2 + (1 + δ)∂u
∂x + δu

,
0 < x < ∞,
0 < t,
where δ is a constant, subject to the boundary conditions u(0, t) = u0, limx→∞u(x, t) →0,
0 < t, and the initial condition u(x, 0) = 0, 0 < x < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
+ (1 + δ)dU(x, s)
dx
+

δ −s
a2

U(x, s) = 0,
0 < x < ∞,
subject to the boundary conditions U(0, s) = u0/s and limx→∞|U(x, s)| < ∞.
Step 2: Show that the solution to the previous step is
U(x, s) = u0
s exp
"
−(1 + δ)x
2
−x
a
r
a2(1 −δ)2
4
+ s
#
.

Advanced Transform Methods
141
Step 3: From an extensive table of inverses, show that
u(x, t) = u0
2 e−δxerfc

x
2a
√
t + a(1 −δ)
√
t
2

+ u0
2 e−xerfc

x
2a
√
t −a(1 −δ)
√
t
2

.
10. During their modeling of a chemical reaction with a back reaction, Agmon et al.33
solved
∂u
∂t = a2 ∂2u
∂x2 ,
0 < x < ∞,
0 < t,
subject to the boundary conditions
κd + a2ux(0, t) + a2κd
Z t
0
ux(0, τ) dτ = κru(0, t),
lim
x→∞u(x, t) →0,
0 < t,
and the initial condition u(x, 0) = 0, 0 < x < ∞, where κd and κr denote the intrinsic
dissociation and recombination rate coeﬃcients, respectively. What should they have found?
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s
a2 U(x, s) = 0,
0 < x < ∞,
subject to the boundary conditions limx→∞|U(x, s)| < ∞and
κd + (s + κd)a2U ′(0, s) = sκrU(0, s).
Step 2: Show that the solution to the previous step is
U(x, s) = 2κd exp(−x√s/a)
a∆√s

1
2a√s + κr −∆−
1
2a√s + κr + ∆

,
where ∆≡
p
κ2r −4a2κd.
Step 3: From an extensive table of inverses, show that
u(x, t) = κd
∆e−x2/(4a2t) h
ex2
−erfc(x−) −ex2
+erfc(x+)
i
,
where x± = [x + (κr ± ∆)a2t]/(2a
√
t ) and erfc(·) is the complementary error function.
11. Solve34
∂u
∂t = ∂2u
∂x2 −βu,
0 < x < ∞,
0 < t,
subject to the boundary conditions ρu(0, t)−ux(0, t) = e(σ2−β)t, limx→∞u(x, t) →0, 0 < t,
and the initial condition u(x, 0) = 0, 0 < x < ∞, where β, ρ, and σ are constants and
σ ̸= ρ.
33 Agmon, N., E. Pines, and D. Huppert, 1988: Germinate recombination in proton-transfer reactions.
II. Comparison of diﬀusional and kinetic schemes. J. Chem. Phys., 88, 5631–5638.
34 Saidel, G. M., E. D. Morris, and G. M. Chisolm, 1987: Transport of macromolecules in arterial wall
in vivo: A mathematical model and analytic solutions. Bull. Math. Biol., 49, 153–169.

142
Advanced Engineering Mathematics: A Second Course
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−(s + β)U(x, s) = 0,
0 < x < ∞,
subject to the boundary conditions
ρU(0, s) −U ′(0, s) =
1
s + β −σ2 ,
lim
x→∞|U(x, s)| < ∞.
Step 2: Show that the solution to the previous step is
U(x, s) =
exp(−x√s + β )
(s + β −σ2)(ρ + √s + β ).
Step 3: Using partial fractions, show that
U(x, s) =
e−x
√
s′
(s′ + σ2)(
√
s′ + ρ)
=
e−x
√
s′
(
√
s′ + σ)(
√
s′ −σ)(
√
s′ + ρ)
=
e−x
√
s′
(ρ2 −σ2)(
√
s′ + ρ)
+
e−x
√
s′
2σ(ρ + σ)(
√
s′ −σ)
−
e−x
√
s′
2σ(ρ −σ)(
√
s′ + σ)
,
where s′ = s + β.
Step 4: Using the ﬁrst shifting theorem and the fact that
L−1
 
e−k√s
a + √s
!
=
1
√
πt exp

−k2
4t

−aeakea2terfc

a
√
t +
k
2
√
t

,
show that
u(x, t) = 1
2eσ2t−βt
 e−σx
ρ + σ erfc
 x
2
√
t −σ
√
t

+ eσx
ρ −σ erfc
 x
2
√
t + σ
√
t

−
ρ
ρ2 −σ2 eρx+ρ2t−βterfc
 x
2
√
t + ρ
√
t

.
12. Solve
∂u
∂t = a2 ∂2u
∂x2 + Ae−kx,
0 < x < ∞,
0 < t,
subject to the boundary conditions ux(0, t) = 0, limx→∞u(x, t) = u0, 0 < t, and the initial
condition u(x, 0) = u0, 0 < x < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s
a2 U(x, s) = −u0
a2 −A
a2 se−kx,
0 < x < ∞,
subject to the boundary conditions U ′(0, s) = 0 and limx→∞U(x, s) = u0/s.

Advanced Transform Methods
143
Step 2: Show that the solution to the previous step is
U(x, s) = u0
s + Ae−kx
a2k2

1
s −a2k2 −1
s

+ Ae−qx
aks√s −
Ae−qx
ak√s(s −a2k2),
where q = √s/a.
Step 3: Using the convolution theorem,
u(x, t) = u0 + Ae−kx
a2k2

ea2k2t −1

+ A
ak
"
2
r
t
π exp

−x2
4a2t

−x
aerfc

x
2a
√
t
#
−Aea2k2t
ak
Z t
0
e−a2k2τ exp

−x2
4a2τ
 dτ
√πτ .
13. Solve
∂u
∂t = a2 ∂2u
∂x2 −P,
0 < x < L,
0 < t,
subject to the boundary conditions u(0, t) = t, u(L, t) = 0, 0 < t, and the initial condition
u(x, 0) = 0, 0 < x < L.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s
a2 U(x, s) = P
sa2 ,
0 < x < L,
subject to the boundary conditions U(0, s) = 1/s2 and U(L, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = P
s2
 sinh(qx)
sinh(qL) −1

+ (P + 1)sinh[q(L −x)]
s2 sinh(qL) ,
where q = √s/a.
Step 3: Show that U(x, s) has a second-order pole at s = 0 and simple poles at sn =
−n2π2a2/L2 and √sn = nπai/L, where n = 1, 2, 3, . . ..
Step 4: Use Bromwich’s integral and show that
u(x, t) = t(L −x)
L
+ Px(x −L)
2a2
−x(x −L)(x −2L)
6a2L
−2PL2
a2π3
∞
X
n=1
(−1)n
n3
sin
nπx
L

exp

−a2n2π2t
L2

+ 2(P + 1)L2
a2π3
∞
X
n=1
1
n3 sin
nπx
L

exp

−a2n2π2t
L2

.
14. Solve
∂u
∂t = a2 ∂2u
∂x2 + ku,
0 < x < L,
0 < k, t,

144
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions u(0, t) = u(L, t) = T0, 0 < t, and the initial condition
u(x, 0) = T0, 0 < x < L.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s −k
a2
U(x, s) = −T0
a2 ,
0 < x < L,
subject to the boundary conditions U(0, s) = U(L, s) = T0/s.
Step 2: Show that the solution to the previous step is
U(x, s) =
T0
s −k −
kT0
s(s −k)
sinh(qx) + sinh[q(L −x)]
sinh(qL)
.
where q =
√
s −k/a.
Step 3: Show that U(x, s) has simple poles at s = 0, s = k, and sn = k −n2π2a2/L2, where
n = 1, 2, 3, . . .. Note here that qn = nπi/L.
Step 4: Use Bromwich’s integral and show that
u(x, t) = T0 cos[(L/2 −x)
p
k/a2]
cos(L
p
k/a2/2)
+ 4kT0
π
∞
X
m=1
sin[(2m −1)πx/L]
(2m −1)[k −(2m −1)2π2a2/L2]ekt−(2m−1)2π2a2t/L2
= 4T0
π
∞
X
m=1
1
2m −1

κm
κm −k −

k
κm −k

ekt−κmt

sin
(2m −1)πx
L

,
where κm = (2m −1)2π2a2/L2.
15. An electric fuse protects electrical devices by using resistance heating to melt an en-
closed wire when excessive current passes through it. A knowledge of the distribution of
temperature along the wire is important in the design of the fuse. If the temperature rises
to the melting point only over a small interval of the element, the melt will produce a
small gap, resulting in an unnecessary prolongation of the fault and a considerable release
of energy. Therefore, the desirable temperature distribution should melt most of the wire.
For this reason, Guile and Carne35 solved the heat conduction equation
∂u
∂t = a2 ∂2u
∂x2 + q(1 + αu),
−L < x < L,
0 < t,
to understand the temperature structure within the fuse just before meltdown. The second
term on the right side of the heat conduction equation gives the resistance heating, which is
assumed to vary linearly with temperature. If the terminals at x = ±L remain at a constant
temperature, which we can take to be zero, the boundary conditions are u(−L, t) = u(L, t) =
0, 0 < t. The initial condition is u(x, 0) = 0, −L < x < L. Find the temperature ﬁeld as a
function of the parameters a, q, and α.
35 Guile, A. E., and E. B. Carne, 1954: An analysis of an analogue solution applied to the heat conduction
problem in a cartridge fuse. AIEE Trans., Part 1, 72, 861–868.

Advanced Transform Methods
145
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
+ αq −s
a2
U(x, s) = −q
a2s,
0 < x < L,
subject to the boundary conditions U(−L, s) = U(L, s) = 0.
Step 2: Show that the solution to the previous step is
sU(x, s) =
q
s −αq −
q cosh(x√s −αq/a)
(s −αq) cosh(L√s −αq/a).
Step 3: Show that U(x, s) has a removable singularity at s = αq and simple poles at
sn = αq −(2n −1)2π2a2/4L2 with √sn −αq = (2n −1)πai/2L, where n = 1, 2, 3, . . ..
Step 4: Use Bromwich’s integral and show that
ut(x, t) = −4q
π
∞
X
n=1
(−1)n
2n −1 cos[(2n −1)πx/2L] exp[αqt −(2n −1)2π2a2t/4L2].
Step 5: Integrate ut(x, t) with respect to time and obtain
u(x, t) = 4q
π
∞
X
n=1
(−1)n cos[(2n −1)πx/2L]
(2n −1)[αq −(2n −1)2π2a2/4L2]

1 −exp[αqt −(2n −1)2π2a2t/4L2]
	
,
where the constant of integration ensures that u(x, 0) = 0.
16. Solve36
∂u
∂t = ∂2u
∂r2 + 2
r
∂u
∂r ,
0 ≤r < 1,
0 < t,
subject to the boundary conditions limr→0 |u(r, t)| < ∞, ur(1, t) = 1, 0 < t, and the initial
condition u(r, 0) = 0, 0 ≤r < 1.
Step 1: Introduce the new variable v(r, t) = r u(r, t) and show that the problem becomes
∂v
∂t = ∂2v
∂r2 ,
0 ≤r < 1, 0 < t,
with the boundary conditions limr→0 v(r, t) →0 and
∂v(1, t)
∂r
−v(1, t) = 1,
t > 0
and the initial condition v(r, 0) = 0, 0 ≤r < 1.
Step 2: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2V (r, s)
dr2
−sV (r, s) = 0,
0 ≤r < 1,
36 See Reismann, H., 1962: Temperature distribution in a spinning sphere during atmospheric entry. J.
Aerosp. Sci., 29, 151–159.

146
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions limr→0 V (r, s) →0 and V ′(1, s) −V (1, s) = 1/s.
Step 3: Show that the solution to the previous step is
V (r, s) =
sinh(r√s )
s [√s cosh(√s ) −sinh(√s )].
Step 4: Show that V (x, s) has a second-order pole at s = 0 and simple poles where √sn =
iλn, sn = −λ2
n and tan(λn) = λn, n = 1, 2, 3, . . ..
Step 5: Use Bromwich’s integral and show that
u(r, t) = r2
2 + 3t −3
10 −2
r
∞
X
n=1
sin(λnr)
λ2n sin(λn)e−λ2
nt,
where tan(λn) = λn.
17. Solve37
∂u
∂t = a2
∂2u
∂r2 + 2
r
∂u
∂r

+ q(t) = a2
r
∂2(ru)
∂r2
+ q(t),
b < r < ∞,
0 < t,
subject to the boundary conditions
∂u(b, t)
∂r
= u(b, t),
lim
r→∞u(r, t) = u0 +
Z t
0
q(τ) dτ,
0 < t,
and the initial condition u(r, 0) = u0, b < r < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2[rU(r, s)]
dr2
−s
a2 rU(r, s) = −r[Q(s) + u0]
a2
,
b < r < ∞,
subject to the boundary conditions U ′(b, s) = U(b, s) and limr→∞U(r, s) = [u0 + Q(s)]/s.
Step 2: Show that the solution to the previous step is
U(r, s) = u0 + Q(s)
s
−
b Q(s)
s(q + 1/β)
e−q(r−b)
r
−
b u0
s(q + 1/β)
e−q(r−b)
r
,
where q = √s/a and β = b/(1 + b).
Step 3: Because
L−1
 e−√s (r−b)/a
s(√s/a + 1/β)

= β

erfc
 r −b
2a
√
t

−exp
r −b
β
+ a2t
β2

erfc
a
√
t
β
+ r −b
2a
√
t

,
37 See Frisch, H. L, and F. C. Collins, 1952: Diﬀusional processes in the growth of aerosol particles. J.
Chem. Phys., 20, 1797–1803.

Advanced Transform Methods
147
show that
u(r, t) = u0

1 −b −β
r
f(r, t)

+
Z t
0

1 −b −β
r
f(r, t −τ)

q(τ) dτ,
where
f(r, t) = erfc
 r −b
2a
√
t

−exp
r −b
β
+ a2t
β2

erfc
a
√
t
β
+ r −b
2a
√
t

.
18. Consider38 a viscous ﬂuid located between two ﬁxed walls x = ±L. At x = 0 we
introduce a thin, inﬁnitely long rigid barrier of mass m per unit area and let it fall under
the force of gravity, which points in the direction of positive x. We wish to ﬁnd the velocity
of the ﬂuid u(x, t). The ﬂuid is governed by the partial diﬀerential equation
∂u
∂t = ν ∂2u
∂x2 ,
0 < x < L,
0 < t,
subject to the boundary conditions u(L, t) = 0 and ut(0, t) −2µux(0, t)/m = g, 0 < t, and
the initial condition u(x, 0) = 0, 0 < x < L.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s
ν U(x, s) = 0,
0 < x < L,
subject to the boundary conditions U(L, s) = 0 and sU(0, s) −2µU ′(0, s)/m = g/s.
Step 2: Show that the solution to the previous step is
U(x, s) =
g sinh[(L −x)
p
s/ν ]
s[s sinh(L
p
s/ν ) + 2µ√s cosh(L
p
s/ν )/(m√ν )]
.
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = −νλ2
n/L2, where λn tan(λn) =
2µL/(mν) ≡k and n = 1, 2, 3, . . ..
Step 4: Use Bromwich’s integral and show that
u(x, t) = mg(L −x)
2µ
−4gµL3
mν2
∞
X
n=1
sin[λn(L −x)/L] exp(−νλ2
nt/L2)
λ2n[λ2n + k(1 + k)] sin(λn)
.
19. Consider39 a viscous ﬂuid located between two ﬁxed walls x = ±L. At x = 0 we
introduce a thin, inﬁnitely long rigid barrier of mass m per unit area. The barrier is acted
upon by an elastic force in such a manner that it would vibrate with a frequency ω if the
38 See Havelock, T. H., 1921: The solution of an integral equation occurring in certain problems of viscous
ﬂuid motion. Philos. Mag., Ser. 6, 42, 620–628.
39 See Havelock, T. H., 1921: On the decay of oscillation of a solid body in a viscous ﬂuid. Philos. Mag.,
Ser. 6, 42, 628–634.

148
Advanced Engineering Mathematics: A Second Course
liquid were absent. We wish to ﬁnd the barrier’s deviation from equilibrium, y(t). The ﬂuid
is governed by the partial diﬀerential equation
∂u
∂t = ν ∂2u
∂x2 ,
0 < x < L,
0 < t.
The boundary conditions are u(L, t) = my′′(t) −2µux(0, t) + mω2y(t) = 0, y′(t) = u(0, t),
0 < t, and the initial conditions are u(x, 0) = 0, 0 < x < L, y(0) = A, and y′(0) = 0.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s
ν U(x, s) = 0,
0 < x < L,
subject to the boundary conditions U(L, s) = 0 and ms2Y (s) −2µU ′(0, s) + mω2Y (s) =
−msA and sY (s) −A = U(0, s).
Step 2: Show that U(x, s) = B sinh
hp
s/ν(L −x)
i
.
Step 3: Show that at x = 0,
ms2Y (s) + 2µB
r s
ν cosh

L
r s
ν

+ mω2Y (s) = msA
and
sY (s) −A = B sinh

L
r s
ν

.
Step 4: Eliminating B in Step 3, show that
Y (s) = A
ms + 2µ
p
s/ν coth

L
p
s/ν

ms2 + 2µs
p
s/ν coth

L
p
s/ν

+ mω2 .
Step 5: Show that Y (s) has simple poles at λn which are the roots of
λ2
n + 2µλ3/2
n
coth

L
p
λn/ν

/(m√ν ) + ω2 = 0,
n = 1, 2, 3, . . . .
Step 6: Use Bromwich’s integral and show that
y(t) = 4µAω2
mL
∞
X
n=1
λneλnt
λ4n −( 2µ
mL)(1 + 2µL
mν )λ3n + 2ω2λ2n + 6ω2µ
mL λn + ω4 .
20. Solve40
∂u
∂t = a2 ∂2u
∂x2 ,
0 < x < L,
0 < t,
40 See McCarthy, T. A., and H. J. Goldsmid, 1970: Electro-deposited copper in bismuth telluride. J.
Phys. D, 3, 697–706.

Advanced Transform Methods
149
subject to the boundary conditions ux(0, t) = 0, a2ux(L, t) + αu(L, t) = F, 0 < t, and the
initial condition u(x, 0) = 0, 0 < x < L.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−s
a2 U(x, s) = 0,
0 < x < L,
subject to the boundary conditions U ′(0, s) = 0 and a2U ′(L, s) + αU(L, s) = F/s.
Step 2: Show that the solution to the previous step is
U(x, s) =
F cosh(qx)
s[a2q sinh(qL) + α cosh(qL)],
where q = √s/a.
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = −a2λ2
n/L2, where λn is the
nth root of λ tan(λ) = αL/a2, qn = iλn/L, and n = 1, 2, 3, . . ..
Step 4: Use Bromwich’s integral and show that
u(x, t) = F
α
(
1 −2hL
∞
X
n=1
cos(λnx/L) exp(−a2λ2
nt/L2)
[hL(1 + hL) + λ2n) cos(λn)
)
,
where h = α/a2.
21. Solve
∂u
∂t = ∂2u
∂x2 ,
0 ≤x < 1,
0 ≤t,
subject to the boundary conditions u(0, t) = 0 and 3a [ux(1, t) −u(1, t)] + ut(1, t) = δ(t),
0 ≤t, and the initial condition u(x, 0) = 0, 0 ≤x < 1. Here δ(t) denotes the Dirac delta
function.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−sU(x, s) = 0,
0 < x < 1,
subject to the boundary conditions U(0, s) = 0 and 3a[U ′(1, s) −U(1, s)] + sU(1, s) = 1.
Step 2: Show that the solution to the previous step is
U(x, s) =
sinh(x√s )
3a [√s cosh(√s ) −sinh(√s )] + s sinh(√s ).
Step 3: Show that U(x, s) has simple poles at s = 0 and sn = −λ2
n where λn cot(λn) =
(3a + λ2
n)/3a, n = 1, 2, 3, . . ..
Step 4: Use Bromwich’s integral and show that
u(x, t) =
x
a + 1 + 2
∞
X
n=1
sin(λnx) exp(−λ2
nt)
[3a + 3 + λ2n/(3a)] sin(λn).

150
Advanced Engineering Mathematics: A Second Course
22. Solve41 the partial diﬀerential equation
∂u
∂t + V ∂u
∂x = ∂2u
∂x2 ,
0 < x < 1,
0 < t,
where V is a constant, subject to the boundary conditions u(0, t) = 1, ux(1, t) = 0, 0 < t,
and the initial condition u(x, 0) = 0, 0 < x < 1.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U(x, s)
dx2
−V dU(x, s)
dx
−sU(x, s) = 0,
0 < x < 1,
subject to the boundary conditions U(0, s) = 1/s and U ′(1, s) = 0.
Step 2: Show that the solution to the previous step is
U(x, s) = eV x/2 µ cosh[µ(1 −x)] + (V/2) sinh[µ(1 −x)]
s [µ cosh(µ) + (V/2) sinh(µ)]
= eV x/2
√
s′ cosh
h
(1 −x)
√
s′
i
+ (V/2) sinh
h
(1 −x)
√
s′
i
(s′ −V 2/4)
h√
s′ cosh
√
s′

+ (V/2) sinh
√
s′
i,
where µ =
p
s + V 2/4 and s′ = s + V 2/4.
Step 3: Show that U(x, s) has simple poles at s′ = V 2/4 and s′
n = −λ2
n with λn cot(λn) =
−V/2, where n = 1, 2, 3, . . ..
Step 4: Using Bromwich’s integral, show that
u(x, t) = 1 −2eV x/2−V 2t/4
×
∞
X
n=1
λn{(V/2) sin[λn(1 −x)] + λn cos[λn(1 −x)]}e−λ2
nt
(λ2n + V 2/4)[λn sin(λn) −(1 + V/2) cos(λn)]
.
23. Solve42 the partial diﬀerential equation
∂2u
∂x∂t + a∂u
∂t + b∂u
∂x = 0,
0 < x < ∞,
0 < a, b, t,
subject to the boundary conditions u(0, t) = 1, limx→∞u(x, t) →0, 0 < t, and the initial
condition ux(x, 0) + au(x, 0) = 0, 0 < x < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and
(s + b)U ′(x, s) + asU(x, s) = 0,
0 < x < ∞,
41 See Yoo, H., and E.-T. Pak, 1996: Analytical solutions to a one-dimensional ﬁnite-domain model for
stratiﬁed thermal storage tanks. Sol. Energy, 56, 315–322.
42 See Liaw, C. H., J. S. P. Wang, R. A. Greenhorn, and K. C. Chao, 1979: Kinetics of ﬁxed-bed
absorption: A new solution. AICHE J., 25, 376–381.

Advanced Transform Methods
151
subject to the boundary conditions limx→0 |U(x, s)| < ∞and U(0, s) = 1/s.
Step 2: Show that the solution to the previous step is
U(x, s) = 1
s exp

−asx
s + b

.
Step 3: Because
e−cξ = 1 −c
Z ξ
0
e−cη dη,
show that
U(x, s) = 1
s −
Z ax
0
e−η exp
 bη
s + b

dη
s + b.
Step 4: By inverting U(x, s) term by term and using the ﬁrst shifting theorem,
u(x, t) = 1 −e−bt
Z ax
0
e−ηI0

2
p
btη

dη.
24. Solve
1
r
∂
∂r

r∂u
∂r

−∂u
∂t = δ(t),
0 ≤r < a,
0 ≤t,
subject to the boundary conditions limr→0 |u(r, t)| < ∞, u(a, t) = 0, 0 < t, and the initial
condition u(r, 0) = 0, 0 ≤r < a, where δ(t) is the Dirac delta function.
Note that
Jn(iz) = inIn(z) and In(iz) = inJn(z) for all complex z.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
1
r
d
dr

rdU(r, s)
dr

−sU(r, s) = 1,
0 ≤r < a,
subject to the boundary conditions limr→0 |U(r, s)| < ∞and U(a, s) = 0.
Step 2: Show that the solution to the previous step is
U(r, s) = I0(r√s ) −I0(a√s )
s I0(a√s )
.
Step 3: Show that U(r, s) has a removable singularity at s = 0 and simple poles at sn =
−k2
n/a2, where J0(kn) = 0 and n = 1, 2, 3, . . ..
Step 4: Using Bromwich’s integral, show that
u(r, t) = −2
∞
X
n=1
J0(knr/a)
kn J1(kn) e−k2
nt/a2.
25. Solve
∂u
∂t = 1
r
∂
∂r

r∂u
∂r

+ H(t),
0 ≤r < a,
0 < t,

152
Advanced Engineering Mathematics: A Second Course
subject to the boundary conditions limr→0 |u(r, t)| < ∞, u(a, t) = 0, 0 < t, and the initial
condition u(r, 0) = 0, 0 ≤r < a. Note that Jn(iz) = inIn(z) and In(iz) = inJn(z) for all
complex z.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
1
r
d
dr

rdU(r, s)
dr

−sU(r, s) = −1
s,
0 ≤r < a,
subject to the boundary conditions limr→0 |U(r, s)| < ∞and U(a, s) = 0.
Step 2: Show that the solution to the previous step is
U(r, s) = I0(a√s ) −I0(r√s )
s2 I0(a√s)
.
Step 3: Show that U(r, s) has simple poles at s = 0 and sn = −k2
n/a2 where J0(kn) = 0
and n = 1, 2, 3, . . ..
Step 4: Using Bromwich’s integral, show that
u(r, t) = a2 −r2
4
−2a2
∞
X
n=1
J0(knr/a)
k3n J1(kn) e−k2
nt/a2.
26. Solve
∂u
∂t = 1
r
∂
∂r

r∂u
∂r

,
0 ≤r < a,
0 < t,
subject to the boundary conditions limr→0 |u(r, t)| < ∞, u(a, t) = e−t/τ0, 0 < t, and the
initial condition u(r, 0) = 1, 0 ≤r < a. Note that Jn(iz) = inIn(z) and In(iz) = inJn(z)
for all complex z.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
1
r
d
dr

rdU(r, s)
dr

−sU(r, s) = −1,
0 ≤r < a,
subject to the boundary conditions limr→0 |U(r, s)| < ∞and U(a, s) = 1/(s + 1/τ0).
Step 2: Show that the solution to the previous step is
U(r, s) = 1
s +

1
s + 1/τ0
−1
s
 I0(r√s )
I0(a√s ).
Step 3: Show that U(r, s) has simple poles at s = 0, s = −1/τ0, and sn = −k2
n/a2, where
J0(kn) = 0 and n = 1, 2, 3, . . ..
Step 4: Using Bromwich’s integral, show that
u(r, t) = J0(r
p
1/τ0 )
J0(a
p
1/τ0 )
e−t/τ0 + 2a2
∞
X
n=1
J0(knr/a)
kn(a2 −k2nτ0) J1(kn)e−k2
nt/a2
= e−t/τ0 + 2a2
∞
X
n=1
J0(knr/a)
kn(a2 −k2nτ0) J1(kn)

e−k2
nt/a2 −e−t/τ0
.

Advanced Transform Methods
153
27. Solve
∂u
∂t = a2
∂2u
∂r2 + 1
r
∂u
∂r

,
0 ≤r < b,
0 < t,
subject to the boundary conditions
lim
r→0 |u(r, t)| < ∞,
u(b, t) = kt,
0 < t,
and the initial condition u(r, 0) = 0, 0 ≤r < b. Note that Jn(iz) = inIn(z) and In(iz) =
inJn(z) for all complex z.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
1
r
d
dr

rdU(r, s)
dr

−s
a2 U(r, s) = 0,
0 ≤r < b,
subject to the boundary conditions limr→0 |U(r, s)| < ∞and U(b, s) = k/s2.
Step 2: Show that the solution to the previous step is
U(r, s) = k I0(r√s/a)
s2 I0(b√s/a).
Step 3: Show that U(r, s) has a second-order pole at z = 0 and simple poles at iκn = √zn/a
or zn = −a2κ2
n, where J0(κnb) = 0 and n = 1, 2, 3, . . ..
Step 4: Using Bromwich’s integral, show that
u(r, t) = k
"
t −b2 −r2
4a2
+ 2
a2b
∞
X
n=1
J0(κnr)
κ3nJ1(κnb)
#
.
28. Solve the nonhomogeneous heat equation for the spherical shell43
∂u
∂t = a2
∂2u
∂r2 + 2
r
∂u
∂r + A
r4

,
α < r < β,
0 < t,
subject to the boundary conditions ur(α, t) = u(β, t) = 0, 0 < t, and the initial condition
u(r, 0) = 0, α < r < β.
Step 1: By introducing v(r, t) = r u(r, t), show that the problem simpliﬁes to
∂v
∂t = a2
∂2v
∂r2 + A
r3

,
α < r < β,
0 < t,
subject to the boundary conditions vr(α, t) −v(α, t)/α = v(β, t) = 0, 0 < t, and the initial
condition v(r, 0) = 0, α < r < β.
43 See Malkovich, R. Sh., 1977: Heating of a spherical shell by a radial current. Sov. Phys. Tech. Phys.,
22, 636.

154
Advanced Engineering Mathematics: A Second Course
Step 2: Taking the Laplace transform of the diﬀerential equation and boundary conditions
in Step 1, show that
d2V (r, s)
dr2
−s
a2 V (r, s) = −A
sr3 ,
α < r < β,
along with V ′(α, s) + V (α, s)/α = V (β, s) = 0.
Step 3: Using the method of variation of parameters, show that the particular solution to
Step 2 is Vp(r, s) = u1(r, s) cosh(qr) + u2(r, s) sinh(qr), where
u1(r, s) = A
sq
Z r
β
sinh(qτ)
τ 3
dτ,
u2(r, s) = −A
sq
Z r
β
cosh(qτ)
τ 3
dτ,
and
q = √s/a.
Step 4: Show that the general solution to Step 2 is
V (r, s) = C sinh[q(r −β)] −A
sq
Z r
β
sinh[q(r −τ)]
τ 3
dτ.
This solution satisﬁes V (β, s) = 0.
Step 5: Use the remaining boundary condition and show that
U(r, s) = A
srq

sinh[q(β −r)]
αq cosh(qℓ) + sinh(qℓ)
Z ℓ
0
αq cosh(qη) + sinh(qη)
(α + η)3
dη−
Z β−r
0
sinh(qη)
(r + η)3 dη

,
where ℓ= β −α. Note: V (r, s) = r U(r, s).
Step 6: Show that U(r, s) has simple poles at s = 0 and sn = −a2γ2
n, where γn is the nth
root of αγn cos(γnℓ) + sin(γnℓ) = 0, n = 1, 2, 3, . . ..
Step 7: Use Bromwich’s integral and show that
u(r, t) = A
1
r −1
β
  1
α −1
2
1
r + 1
β

−2α2
rℓ2
∞
X
n=0
sin[γn(β −r)] exp(−a2γ2
nt)
sin2(γnℓ)(β + α2ℓγ2n)
Z 1
0
sin(γnℓη)
(δ −η)3 dη

,
where γn is the nth root of αγ + tan(ℓγ) = 0, and δ = 1 + α/ℓ.
2.6 THE SOLUTION OF LAPLACE’S EQUATION BY LAPLACE TRANSFORMS
Laplace transforms are useful in solving Laplace’s or Poisson’s equation over a semi-
inﬁnite strip. The following problem illustrates this technique.
Let us solve Poisson’s equation within a semi-inﬁnite circular cylinder
1
r
∂
∂r

r∂u
∂r

+ ∂2u
∂z2 = 2
b n(z)δ(r −b),
0 ≤r < a,
0 < z < ∞,
(2.6.1)
subject to the boundary conditions
u(r, 0) = 0,
lim
z→∞|u(r, z)| < ∞,
0 ≤r < a,
(2.6.2)

Advanced Transform Methods
155
and
u(a, z) = 0,
0 < z < ∞,
(2.6.3)
where 0 < b < a. This problem gives the electrostatic potential within a semi-inﬁnite cylin-
der of radius a that is grounded and has the charge density of n(z) within an inﬁnitesimally
thin shell located at r = b.
Because the domain is semi-inﬁnite in the z direction, we introduce the Laplace trans-
form
U(r, s) =
Z ∞
0
u(r, z) e−sz dz.
(2.6.4)
Thus, taking the Laplace transform of Equation 2.6.1, we have that
1
r
d
dr

rdU(r, s)
dr

+ s2U(r, s) −su(r, 0) −uz(r, 0) = 2
b N(s)δ(r −b).
(2.6.5)
Although u(r, 0) = 0, uz(r, 0) is unknown and we denote its value by f(r).
Therefore,
Equation 2.6.5 becomes
1
r
d
dr

rdU(r, s)
dr

+ s2U(r, s) = f(r) + 2
b N(s)δ(r −b),
0 ≤r < a,
(2.6.6)
with limr→0 |U(r, s)| < ∞, and U(a, s) = 0.
To solve Equation 2.6.6 we ﬁrst assume that we can rewrite f(r) as the Fourier-Bessel
series
f(r) =
∞
X
n=1
AnJ0(knr/a),
(2.6.7)
where kn is the nth root of the J0(k) = 0, and
An =
2
a2J2
1(kn)
Z a
0
f(r) J0(knr/a) r dr.
(2.6.8)
Similarly, the expansion for the delta function is
δ(r −b) = 2b
a2
∞
X
n=1
J0(knb/a)J0(knr/a)
J2
1(kn)
,
(2.6.9)
because
Z a
0
δ(r −b)J0(knr/a) r dr = b J0(knb/a).
(2.6.10)
Why we chose this particular expansion will become apparent shortly.
Thus, Equation 2.6.6 may be rewritten as
1
r
d
dr

rdU(r, s)
dr

+ s2U(r, s) = 2
a2
∞
X
n=1
2N(s)J0(knb/a) + ak
J2
1(kn)
J0(knr/a),
(2.6.11)
where ak =
R a
0 f(r) J0(knr/a) r dr.
The form of the right side of Equation 2.6.11 suggests that we seek solutions of the
form
U(r, s) =
∞
X
n=1
BnJ0(knr/a),
0 ≤r < a.
(2.6.12)

156
Advanced Engineering Mathematics: A Second Course
We now understand why we rewrote the right side of Equation 2.6.6 as a Fourier-Bessel
series; the solution U(r, s) automatically satisﬁes the boundary condition U(a, s) = 0. Sub-
stituting Equation 2.6.12 into Equation 2.6.11, we ﬁnd that
U(r, s) = 2
a2
∞
X
n=1
2N(s)J0(knb/a) + ak
(s2 −k2n/a2)J2
1(kn) J0(knr/a),
0 ≤r < a.
(2.6.13)
We have not yet determined ak. Note, however, that in order for the inverse of Equation
2.6.13 not to grow as eknz/a, the numerator must vanish when s = kn/a and s = kn/a is a
removable pole. Thus,
ak = −2N(kn/a)J0(knb/a),
(2.6.14)
and
U(r, s) = 4
a2
∞
X
n=1
[N(s) −N(kn/a)]J0(knb/a)
(s2 −k2n/a2)J2
1(kn)
J0(knr/a),
0 ≤r < a.
(2.6.15)
The inverse of U(r, s) then follows directly from simple inversions, the convolution theorem,
and the deﬁnition of the Laplace transform. The complete solution is
u(r, z) = 2
a
∞
X
n=1
J0(knb/a)J0(knr/a)
kn J2
1(kn)
×
Z z
0
n(τ)ekn(z−τ)/a dτ −
Z z
0
n(τ)e−kn(z−τ)/a dτ
−
Z ∞
0
n(τ)e−knτ/aeknz/a dτ +
Z ∞
0
n(τ)e−knτ/ae−knz/a dτ

(2.6.16)
= 2
a
∞
X
n=1
J0(knb/a)J0(knr/a)
kn J2
1(kn)
(2.6.17)
×
Z ∞
0
n(τ)e−kn(z+τ)/a dτ −
Z z
0
n(τ)e−kn(z−τ)/a dτ −
Z ∞
z
n(τ)e−kn(τ−z)/a dτ

.
Problems
1. Use Laplace transforms to solve
∂2u
∂x2 + ∂2u
∂y2 = 0,
0 < x < ∞,
0 < y < a,
subject to the boundary conditions u(0, y) = 1, limx→∞|u(x, y)| < ∞, 0 < y < a, and
u(x, 0) = u(x, a) = 0, 0 < x < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
d2U
dy2 + s2U = s + f(s, y),
subject to the boundary conditions U(s, 0) = U(s, a) = 0.

Advanced Transform Methods
157
Step 2: Because
1 = 2
π
∞
X
n=1
[1 −(−1)n]
n
sin
nπy
a

and expanding f(s, y) in a half-range sine expansion:
f(s, y) =
∞
X
n=1
An sin
nπy
a

, where An = 2
a
Z a
0
f(s, y) sin
nπy
a

dy,
show that the diﬀerential equations in Step 1 can be rewritten
d2U
dy2 + s2U =
∞
X
n=1
2s[1 −(−1)n]
nπ
+ An

sin
nπy
a

,
Step 3: Show that the solution of the diﬀerential equation in Step 2 is
U(s, y) =
∞
X
n=1
2s[1 −(−1)n] + nπAn
nπ(s2 −n2π2/a2)
sin
nπy
a

.
Step 4: For the solution to remain ﬁnite as x →∞, s = nπ/a cannot be a pole of the
transform U(s, y). Show that An = −2[1 −(−1)n]/a and
U(s, y) =
∞
X
n=1
2[1 −(−1)n]
nπ(s + nπ/a) sin
nπy
a

.
Step 5: Take the inverse of U(s, y) term by term and show that
u(x, y) = 4
π
∞
X
m=1
1
2m −1 exp

−(2m −1)πx
a

sin
(2m −1)πy
a

.
2. Use Laplace transforms to solve
1
r
∂
∂r

r∂u
∂r

+ ∂2u
∂z2 = 0,
0 ≤r < a,
0 < z < ∞,
subject to the boundary conditions u(r, 0) = 1, limz→∞|u(r, z)| < ∞0 ≤r < a, and
limr→0 |u(r, z)| < ∞, u(a, z) = 0, 0 < z < ∞.
Step 1: Take the Laplace transform of the partial diﬀerential equation and boundary con-
ditions and show that
1
r
d
dr

rdU(r, s)
dr

+ s2U(r, s) = s + f(r),
0 ≤r < a,
with |U(0, s)| < ∞and U(a, s) = 0.
Step 2: Rewrite f(r) as the Fourier-Bessel series:
f(r) =
∞
X
n=1
AnJ0(knr/a),

158
Advanced Engineering Mathematics: A Second Course
where kn is the nth root of the J0(k) = 0 and
An =
2
a2J2
1(kn)
Z a
0
f(r) J0(knr/a) r dr.
Step 3: Because
1 = 2
∞
X
n=1
J0(knr/a)
kn J1(kn) ,
0 ≤r < a,
show that the diﬀerential equation in Step 1 becomes
1
r
d
dr

rdU(r, s)
dr

+ s2U(r, s) = 2
a2
∞
X
n=1
sa2J1(kn) −knak
kn J2
1(kn)
J0
knr
a

,
where ak =
R a
0 f(r)J0(knr/a) r dr.
Step 4: Show that the solution to the diﬀerential equation is
U(r, s) = 2
a2
∞
X
n=1
sa2J1(kn) −knak
(s2 −k2n/a2)knJ2
1(kn)J0(knr/a),
0 ≤r < a.
Step 5: Because s = kn/a cannot be a pole of U(r, s), ak = aJ1(kn). Therefore,
U(r, s) = 2
∞
X
n=1
J0(knr/a)
kn(s + kn/a)J1(kn),
0 ≤r < a.
Step 6: Find the inverse of U(r, s) and show that
u(r, z) = 2
∞
X
n=1
J0(knr/a)
kn J1(kn) e−knz/a.
Further Readings
Debnath, L., and D. Bhatta, 2015: Integral Transforms and Their Applications.
CRC
Press, 792 pp. A book that covers Laplace, Fourier, z-, Hankel, Mellin, Hilbert and Stieltjes
transforms and their application.
Duﬀy, D. G., 2015: Transform Methods for Solving Partial Diﬀerential Equations. CRC
Press, 728 pp. This book covers the material of this chapter in greater depth.

0.0
1.0
2.0
3.0
ωΤ
0.1
1.0
10.0
Ratio of quadrature amplitudes to ideal integration      
Trapezoidal
Simpson’s
3/8−rule
Rule
Simpson’s
1/3−rule
Ideal Rule
Chapter 3
The Z-Transform
Since the Second World War, the rise of digital technology has resulted in a corre-
sponding demand for designing and understanding discrete-time (data sampled) systems.
These systems are governed by diﬀerence equations in which members of the sequence yn
are coupled to each other.
One source of diﬀerence equations is the numerical evaluation of integrals on a digital
computer.
Because we can only have values at discrete time points tk = kT for k =
0, 1, 2, . . ., the value of the integral y(t) =
R t
0 f(τ) dτ is
y(kT) =
Z kT
0
f(τ) dτ =
Z (k−1)T
0
f(τ) dτ +
Z kT
(k−1)T
f(τ) dτ
(3.0.1)
= y[(k −1)T] +
Z kT
(k−1)T
f(τ) dτ = y[(k −1)T] + Tf(kT),
(3.0.2)
because
R kT
(k−1)T f(τ) dτ ≈Tf(kT). The right side of Equation 3.0.2 is an example of a
ﬁrst-order diﬀerence equation because the numerical scheme couples the sequence value
y(kT) directly to the previous sequence value y[(k −1)T]. If Equation 3.0.2 had contained
y[(k −2)T], then it would have been a second-order diﬀerence equation, and so forth.
Although we could use the conventional Laplace transform to solve these diﬀerence
equations, the use of z-transforms can greatly facilitate the analysis, especially when we
only desire responses at the sampling instants. Often the entire analysis can be done using
only the transforms and the analyst does not actually ﬁnd the sequence y(kT).
In this chapter we will ﬁrst deﬁne the z-transform and discuss its properties. Then we
will show how to ﬁnd its inverse. Finally, we shall use them to solve diﬀerence equations.
159

160
Advanced Engineering Mathematics: A Second Course
f(t)
t
t
t
T
2T
3T
T
2T
3T
0
0
0
ε
f  (t)
S
f  (t)
S*
Figure 3.1.1: Schematic of how a continuous function f(t) is sampled by a narrow-width pulse sampler
f∗
S(t) and an ideal sampler fS(t).
3.1 THE RELATIONSHIP OF THE Z-TRANSFORM TO THE LAPLACE TRANSFORM1
Let f(t) be a continuous function that an instrument samples every T units of time.
We denote this data-sampled function by f ∗
S(t). See Figure 3.1.1. Taking ǫ, the duration of
an individual sampling event, to be small, we may approximate the narrow-width pulse in
Figure 3.1.1 by ﬂat-topped pulses. Then f ∗
S(t) approximately equals
f ∗
S(t) ≈1
ǫ
∞
X
n=0
f(nT) [H(t −nT + ǫ/2) −H(t −nT −ǫ/2)] ,
(3.1.1)
if ǫ ≪T.
Clearly the presence of ǫ is troublesome in Equation 3.1.1; it adds one more parameter
to our problem. For this reason we introduce the concept of the ideal sampler, where the
sampling time becomes inﬁnitesimally small so that
fS(t) = lim
ǫ→0
∞
X
n=0
f(nT)
H(t −nT + ǫ/2) −H(t −nT −ǫ/2)
ǫ

=
∞
X
n=0
f(nT)δ(t −nT).
(3.1.2)
Let us now ﬁnd the Laplace transform of this data-sampled function. From the linearity
property of Laplace transforms,
FS(s) = L[fS(t)] = L
"
∞
X
n=0
f(nT)δ(t −nT)
#
=
∞
X
n=0
f(nT)L[δ(t −nT)].
(3.1.3)
1 Gera (Gera, A. E., 1999: The relationship between the z-transform and the discrete-time Fourier
transform.
IEEE Trans.
Auto.
Control, AC-44, 370–371) explored the general relationship between
the one-sided discrete-time Fourier transform and the one-sided z-transform. See also Naumovi´c, M. B.,
2001: Interrelationship between the one-sided discrete-time Fourier transform and one-sided delta transform.
Electr. Engng., 83, 99–101.

The Z-Transform
161
Because L[δ(t −nT)] = e−nsT , Equation 3.1.3 simpliﬁes to
FS(s) =
∞
X
n=0
f(nT)e−nsT .
(3.1.4)
If we now make the substitution that z = esT , then FS(s) becomes
F(z) = Z(fn) =
∞
X
n=0
fnz−n,
(3.1.5)
where F(z) is the one-sided z-transform2 of the sequence f(nT), which we shall now denote
by fn. Here Z denotes the operation of taking the z-transform while Z−1 represents the
inverse z-transformation. We will consider methods for ﬁnding the inverse z-transform in
Section 3.3.
Just as the Laplace transform was deﬁned by an integration in t, the z-transform is
deﬁned by a power series (Laurent series) in z. Consequently, every z-transform has a region
of convergence that must be implicitly understood if not explicitly stated. Furthermore,
just as the Laplace integral diverged for certain functions, there are sequences where the
associated power series diverges and its z-transform does not exist.
Consider now the following examples of how to ﬁnd the z-transform.
• Example 3.1.1
Given the unit sequence fn = 1, n ≥0, let us ﬁnd F(z). Substituting fn into the
deﬁnition of the z-transform leads to
F(z) =
∞
X
n=0
z−n =
z
z −1,
(3.1.6)
because P∞
n=0 z−n is a complex-valued geometric series with common ratio z−1. This series
converges if |z−1| < 1 or |z| > 1, which gives the region of convergence of F(z).
MATLAB’s symbolic toolbox provides an alternative to the hand computation of the
z-transform. In the present case, the command
>> syms z; syms n positive
>> ztrans(1,n,z)
yields
ans =
z/(z-1)
⊓⊔
• Example 3.1.2
Let us ﬁnd the z-transform of the sequence
fn = e−anT ,
n ≥0,
(3.1.7)
2 The standard reference is Jury, E. I., 1964: Theory and Application of the z-Transform Method. John
Wiley & Sons, 330 pp.

162
Advanced Engineering Mathematics: A Second Course
for a real and a imaginary.
For a real, substitution of the sequence into the deﬁnition of the z-transform yields
F(z) =
∞
X
n=0
e−anT z−n =
∞
X
n=0
 e−aT z−1n.
(3.1.8)
If u = e−aT z−1, then Equation 3.1.8 is a geometric series so that
F(z) =
∞
X
n=0
un =
1
1 −u.
(3.1.9)
Because |u| = e−aT |z−1|, the condition for convergence is that |z| > e−aT . Thus,
F(z) =
z
z −e−aT ,
|z| > e−aT .
(3.1.10)
For imaginary a, the inﬁnite series in Equation 3.1.8 converges if |z| > 1, because
|u| = |z−1| when a is imaginary. Thus,
F(z) =
z
z −e−aT ,
|z| > 1.
(3.1.11)
Although the z-transforms in Equation 3.1.10 and Equation 3.1.11 are the same in these
two cases, the corresponding regions of convergence are diﬀerent. If a is a complex number,
then
F(z) =
z
z −e−aT ,
|z| > |e−aT |.
(3.1.12)
Checking our work using MATLAB, we type the commands:
>> syms a z; syms n T positive
>> ztrans(exp(-a*n*T),n,z);
>> simplify(ans)
which yields
ans =
z*exp(a*T)/(z*exp(a*T)-1)
⊓⊔
• Example 3.1.3
Let us ﬁnd the z-transform of the sinusoidal sequence
fn = cos(nωT),
n ≥0.
(3.1.13)
Substituting Equation 3.1.13 into the deﬁnition of the z-transform results in
F(z) =
∞
X
n=0
cos(nωT)z−n.
(3.1.14)
From Euler’s formula,
cos(nωT) = 1
2(einωT + e−inωT ),
(3.1.15)

The Z-Transform
163
so that Equation 3.1.14 becomes
F(z) = 1
2
∞
X
n=0

einωT z−n + e−inωT z−n

,
(3.1.16)
or
F(z) = 1
2

Z(einωT ) + Z(e−inωT )

.
(3.1.17)
From Equation 3.1.11,
Z(e±inωT ) =
z
z −e±iωT ,
|z| > 1.
(3.1.18)
Substituting Equation 3.1.18 into Equation 3.1.17 and simplifying yields
F(z) =
z[z −cos(ωT)]
z2 −2z cos(ωT) + 1,
|z| > 1.
(3.1.19)
⊓⊔
• Example 3.1.4
Let us ﬁnd the z-transform for the sequence
fn =
 1,
0 ≤n ≤5,
( 1
2)n,
6 ≤n.
(3.1.20)
From the deﬁnition of the z-transform,
Z(fn) = F(z) =
5
X
n=0
z−n +
∞
X
n=6
 1
2z
n
.
(3.1.21)
= 1 + 1
z + 1
z2 + 1
z3 + 1
z4 + 1
z5 +
2z
2z −1
−1 −1
2z −
1
4z2 −
1
8z3 −
1
16z4 −
1
32z5
(3.1.22)
=
2z
2z −1 + 1
2z +
3
4z2 +
7
8z3 +
15
16z4 +
31
32z5 .
(3.1.23)
We could also have obtained Equation 3.1.23 via MATLAB by typing the commands:
>> syms z; syms n positive
>> ztrans(’1+((1/2)^n-1)*Heaviside(n-6)’,n,z)
which yields
ans =
2*z/(2*z-1)+1/2/z+3/4/z^2+7/8/z^3+15/16/z^4+31/32/z^5
⊓⊔

164
Advanced Engineering Mathematics: A Second Course
We summarize some of the more commonly encountered sequences and their transforms
in Table 3.1.1 along with their regions of convergence.
• Example 3.1.5
In many engineering studies, the analysis is done entirely using transforms without
actually ﬁnding any inverses. Consequently, it is useful to compare and contrast how various
transforms behave in very simple test problems.
Consider the time function f(t) = ae−atH(t), a > 0. Its Laplace and Fourier transform
are identical, namely a/(a + iω), if we set s = iω. In Figure 3.1.2 we illustrate its behavior
as a function of positive ω.
Let us now generate the sequence of observations that we would measure if we sampled
f(t) every T units of time apart: fn = ae−anT . Taking the z-transform of this sequence, it
equals az/
 z −e−aT 
. Recalling that z = esT = eiωT , we can also plot this transform as
a function of positive ω. For small ω, the transforms agree, but as ω becomes larger they
diverge markedly. Why does this occur?
Recall that the z-transform is computed from a sequence comprised of samples from a
continuous signal. One very important ﬂaw in sampled data is the possible misrepresenta-
tion of high-frequency eﬀects as lower-frequency phenomena. It is this aliasing or folding
eﬀect that we are observing here. Consequently, the z-transform of a sampled record can
diﬀer markedly from the corresponding Laplace or Fourier transforms of the continuous
record at frequencies above one half of the sampling frequency. This also suggests that care
should be exercised in interpolating between sampling instants. Indeed, in those applica-
tions where the output between sampling instants is very important, such as in a hybrid
mixture of digital and analog systems, we must apply the so-called “modiﬁed z-transform.”
Problems
From the fundamental deﬁnition of the z-transform, ﬁnd the transform of the following
sequences, where n ≥0. Then check your answer using MATLAB.
1. fn =
1
2
n
2. fn = einθ
3. fn =
 1,
0 ≤n ≤5,
0,
5 < n
4. fn =
(   1
2
n ,
n = 0, 1, . . . , 10
  1
4
n ,
n ≥11
5. fn =
( 0,
n = 0,
−1,
n = 1,
an,
n ≥2
3.2 SOME USEFUL PROPERTIES
In principle we could construct any desired transform from the deﬁnition of the z-
transform. However, there are several general theorems that are much more eﬀective in
ﬁnding new transforms.

The Z-Transform
165
Table 3.1.1: Z-Transforms of Some Commonly Used Sequences
fn, n ≥0
F(z)
Region of
convergence
1.
f0 = k = const.
k
|z| > 0
fn = 0, n ≥1
2.
fm = k = const.
kz−m
|z| > 0
fn = 0, for all values
of n ̸= m
3.
k = constant
kz/(z −1)
|z| > 1
4.
kn
kz/(z −1)2
|z| > 1
5.
kn2
kz(z + 1)/(z −1)3
|z| > 1
6. ke−anT , a complex
kz/
 z −e−aT 
|z| > |e−aT |
7. kne−anT , a complex
kze−aT
(z−e−aT )2
|z| > |e−aT |
8.
sin(ω0nT)
z sin(ω0T )
z2−2z cos(ω0T )+1
|z| > 1
9.
cos(ω0nT)
z[z−cos(ω0T )]
z2−2z cos(ω0T )+1
|z| > 1
10.
e−anT sin(ω0nT)
ze−aT sin(ω0T )
z2−2ze−aT cos(ω0T )+e−2aT
|z| > e−aT
11.
e−anT cos(ω0nT)
ze−aT [zeaT −cos(ω0T )]
z2−2ze−aT cos(ω0T )+e−2aT
|z| > e−aT
12.
αn , α constant
z/(z −α)
|z| > |α|
13.
nαn
αz/(z −α)2
|z| > |α|
14.
n2αn
αz(z + α)/(z −α)3
|z| > |α|
15.
sinh(ω0nT)
z sinh(ω0T )
z2−2z cosh(ω0T )+1
|z| > cosh(ω0T)
16.
cosh(ω0nT)
z[z−cosh(ω0T )]
z2−2z cosh(ω0T )+1
|z| > sinh(ω0T)
17.
an/n!
ea/z
|z| > 0
18.
[ln(a)]n/n!
a1/z
|z| > 0

166
Advanced Engineering Mathematics: A Second Course
0.0
2.5
5.0
7.5
10.0
12.5
ω
10
−4
10
−3
10
−2
10
−1
10
0
amplitude of transform
a = 0.1
a = 0.01
a = 0.001
Figure 3.1.2: The amplitude of the Laplace or Fourier transform (solid line) for the function ae−atH(t)
and the z-transform (dashed line) for the sequence fn = ae−anT as a function of frequency ω for various
positive values of a and T = 1.
Linearity
From the deﬁnition of the z-transform, it immediately follows that
if
hn = c1fn + c2gn,
then
H(z) = c1F(z) + c2G(z),
(3.2.1)
where F(z) = Z(fn), G(z) = Z(gn), H(z) = Z(hn), and c1, c2 are arbitrary constants.
Multiplication by an expo-
nential sequence
If
gn = e−anT fn,
n ≥0,
then
G(z) = F(zeaT ).
(3.2.2)
This follows from
G(z) = Z(gn) =
∞
X
n=0
gn z−n =
∞
X
n=0
e−anT fn z−n =
∞
X
n=0
fn(zeaT )−n = F(zeaT ).
(3.2.3)
This is the z-transform analog to the ﬁrst shifting theorem in Laplace transforms.
Shifting
The eﬀect of shifting depends upon whether it is to the right or to the left, as Table
3.2.1 illustrates. For the sequence fn−2, no values from the sequence fn are lost; thus,
we anticipate that the z-transform of fn−2 only involves F(z). However, in forming the

The Z-Transform
167
Table 3.2.1: Examples of Shifting Involving Sequences
n
fn
fn−2
fn+2
0
1
0
4
1
2
0
8
2
4
1
16
3
8
2
64
4
16
4
128
...
...
...
...
sequence fn+2, the ﬁrst two values of fn are lost, and we anticipate that the z-transform of
fn+2 cannot be expressed solely in terms of F(z) but must include those two lost pieces of
information.
Let us now conﬁrm these conjectures by ﬁnding the z-transform of fn+1, which is a
sequence that has been shifted one step to the left. From the deﬁnition of the z-transform,
it follows that
Z(fn+1) =
∞
X
n=0
fn+1z−n = z
∞
X
n=0
fn+1z−(n+1)
(3.2.4)
or
Z(fn+1) = z
∞
X
k=1
fkz−k + zf0 −zf0,
(3.2.5)
where we added zero in Equation 3.2.5. This algebraic trick allows us to collapse the ﬁrst
two terms on the right side of Equation 3.2.5 into one and
Z(fn+1) = zF(z) −zf0.
(3.2.6)
In a similar manner, repeated applications of Equation 3.2.6 yield
Z(fn+m) = zmF(z) −zmf0 −zm−1f1 −. . . −zfm−1,
(3.2.7)
where m > 0. This shifting operation transforms fn+m into an algebraic expression involv-
ing m. Furthermore, we introduced initial sequence values, just as we introduced initial
conditions when we took the Laplace transform of the nth derivative of f(t). We will make
frequent use of this property in solving diﬀerence equations in Section 3.4.
Consider now shifting to the right by the positive integer k,
gn = fn−kHn−k,
n ≥0,
(3.2.8)
where Hn−k = 0 for n < k and 1 for n ≥k. Then the z-transform of Equation 3.2.8 is
G(z) = z−kF(z),
(3.2.9)
where G(z) = Z(gn), and F(z) = Z(fn). This follows from
G(z) =
∞
X
n=0
gnz−n =
∞
X
n=0
fn−kHn−kz−n = z−k
∞
X
n=k
fn−kz−(n−k) = z−k
∞
X
m=0
fmz−m = z−kF(z).
(3.2.10)

168
Advanced Engineering Mathematics: A Second Course
This result is the z-transform analog to the second shifting theorem in Laplace transforms.
In symbolic calculations involving MATLAB, the operator Hn−k can be expressed by
Heaviside(n-k).
Initial-value theorem
The initial value of the sequence fn, f0, can be computed from F(z) using the initial-
value theorem:
f0 = lim
z→∞F(z).
(3.2.11)
From the deﬁnition of the z-transform,
F(z) =
∞
X
n=0
fnz−n = f0 + f1z−1 + f2z−2 + . . . .
(3.2.12)
In the limit of z →∞, we obtain the desired result.
Final-value theorem
The value of fn, as n →∞, is given by the ﬁnal-value theorem:
f∞= lim
z→1 (z −1)F(z),
(3.2.13)
where F(z) is the z-transform of fn.
We begin by noting that
Z(fn+1 −fn) = lim
n→∞
n
X
k=0
(fk+1 −fk)z−k.
(3.2.14)
Using the shifting theorem on the left side of Equation 3.2.14,
zF(z) −zf0 −F(z) = lim
n→∞
n
X
k=0
(fk+1 −fk)z−k.
(3.2.15)
Applying the limit as z approaches 1 to both sides of Equation 3.2.15:
lim
z→1 (z −1)F(z) −f0 = lim
n→∞
n
X
k=0
(fk+1 −fk)
(3.2.16)
= lim
n→∞

(f1 −f0) + (f2 −f1) + . . . + (fn −fn−1) + (fn+1 −fn) + . . .

(3.2.17)
= lim
n→∞(−f0 + fn+1) = −f0 + f∞.
(3.2.18)
Consequently,
f∞= lim
z→1 (z −1)F(z).
(3.2.19)

The Z-Transform
169
Note that this limit has meaning only if f∞exists. This occurs if F(z) has no second-order
or higher poles on the unit circle and no poles outside the unit circle.
Multiplication by n
Given
gn = nfn,
n ≥0,
(3.2.20)
this theorem states that
G(z) = −z dF(z)
dz
,
(3.2.21)
where G(z) = Z(gn), and F(z) = Z(fn).
This follows from
G(z) =
∞
X
n=0
gnz−n =
∞
X
n=0
nfnz−n = z
∞
X
n=0
nfnz−n−1 = −z dF(z)
dz
.
(3.2.22)
Periodic sequence theorem
Consider the N-periodic sequence:
fn = {f0f1f2 . . . fN−1
|
{z
}
ﬁrst period
f0f1 . . .},
(3.2.23)
and the related sequence:
xn =

fn,
0 ≤n ≤N −1,
0,
N ≤n.
(3.2.24)
This theorem allows us to ﬁnd the z-transform of fn if we can ﬁnd the z-transform of xn
via the relationship
F(z) =
X(z)
1 −z−N ,
|zN| > 1,
(3.2.25)
where X(z) = Z(xn).
This follows from
F(z) =
∞
X
n=0
fnz−n =
N−1
X
n=0
xnz−n +
2N−1
X
n=N
xn−Nz−n +
3N−1
X
n=2N
xn−2Nz−n + · · · .
(3.2.26)
Application of the shifting theorem in Equation 3.2.26 leads to
F(z) = X(z) + z−NX(z) + z−2NX(z)+ = X(z)

1 + z−N + z−2N + · · ·

.
(3.2.27)

170
Advanced Engineering Mathematics: A Second Course
Equation 3.2.27 contains an inﬁnite geometric series with common ratio z−N, which con-
verges if |z−N| < 1. Thus,
F(z) =
X(z)
1 −z−N ,
|zN| > 1.
(3.2.28)
Convolution
Given the sequences fn and gn, the convolution product of these two sequences is
wn = fn ∗gn =
n
X
k=0
fkgn−k =
n
X
k=0
fn−kgk.
(3.2.29)
Given F(z) and G(z), we then have that W(z) = F(z)G(z).
This follows from
W(z) =
∞
X
n=0
"
n
X
k=0
fkgn−k
#
z−n =
∞
X
n=0
∞
X
k=0
fkgn−kz−n,
(3.2.30)
because gn−k = 0 for k > n. Reversing the order of summation and letting m = n −k,
W(z) =
∞
X
k=0
∞
X
m=−k
fkgmz−(m+k) =
" ∞
X
k=0
fkz−k
# "
∞
X
m=0
gmz−m
#
= F(z)G(z).
(3.2.31)
We can use MATLAB’s command conv( ), which multiplies two polynomials to perform
discrete convolution as follows:
>>x = [1 1 1 1 1 1 1];
>>y = [1 2 4 8 16 32 64];
>>z = conv(x,y)
produces
z =
1 3 7 15 31 63 127 126 124 120 112 96 64
The ﬁrst seven values of z contain the convolution of the sequence x with the sequence y.
Consider now the following examples of the properties discussed in this section.
• Example 3.2.1
From
Z(an) =
1
1 −az−1 ,
(3.2.32)
for n ≥0 and |z| > |a| , we have that
Z
 einx
=
1
1 −eixz−1 ,
(3.2.33)

The Z-Transform
171
and
Z
 e−inx
=
1
1 −e−ixz−1 ,
(3.2.34)
if n ≥0 and |z| > 1. Therefore, the sequence fn = cos(nx) has the z-transform
F(z) = Z[cos(nx)] = 1
2Z
 einx
+ 1
2Z
 e−inx
(3.2.35)
= 1
2
1
1 −eixz−1 + 1
2
1
1 −e−ixz−1 =
1 −cos(x)z−1
1 −2 cos(x)z−1 + z−2 .
(3.2.36)
⊓⊔
• Example 3.2.2
Using the z-transform,
Z(an) =
1
1 −az−1 ,
n ≥0,
(3.2.37)
we ﬁnd that
Z(nan) = −z d
dz
h 1 −az−1−1i
= (−z)(−1)
 1 −az−1−2 (−a)(−1)z−2
(3.2.38)
=
az−1
(1 −az−1)2 =
az
(z −a)2 .
(3.2.39)
⊓⊔
• Example 3.2.3
Consider F(z) = 2az−1/(1 −az−1)3, where |a| < |z| and |a| < 1. Here we have that
f0 = lim
z→∞F(z) = lim
z→∞
2az−1
(1 −az−1)3 = 0
(3.2.40)
from the initial-value theorem. This agrees with the inverse of F(z):
fn = n(n + 1)an,
n ≥0.
(3.2.41)
If the z-transform consists of the ratio of two polynomials, we can use MATLAB to ﬁnd
f0. For example, if F(z) = 2z2/(z −1)3, we can ﬁnd f0 as follows:
>>num = [2 0 0];
>>den = conv([1 -1],[1 -1]);
>>den = conv(den,[1 -1]);
>>initialvalue = polyval(num,1e20) / polyval(den,1e20)
initialvalue =
2.0000e-20
Therefore, f0 = 0.
⊓⊔

172
Advanced Engineering Mathematics: A Second Course
• Example 3.2.4
Given the z-transform F(z) = (1 −a)z/[(z −1)(z −a)], where |z| > 1 > a > 0, then
from the ﬁnal-value theorem we have that
lim
n→∞fn = lim
z→1(z −1)F(z) = lim
z→1
1 −a
1 −az−1 = 1.
(3.2.42)
This is consistent with the inverse transform fn = 1 −an with n ≥0.
⊓⊔
• Example 3.2.5
Using the sequences fn = 1 and gn = an, where a is real, verify the convolution
theorem.
We ﬁrst compute the convolution of fn with gn, namely
wn = fn ∗gn =
n
X
k=0
ak =
1
1 −a −an+1
1 −a.
(3.2.43)
Taking the z-transform of wn,
W(z) =
z
(1 −a)(z −1) −
az
(1 −a)(z −a) =
z2
(z −1)(z −a) = F(z)G(z),
(3.2.44)
and the convolution theorem holds true for this special case.
Problems
Use the properties of z-transforms and Table 3.1.1 to ﬁnd the z-transform of the following
sequences. Then check your answer using MATLAB.
1. fn = nTe−anT
2. fn =

0,
n = 0
nan−1,
n ≥1
3. fn =

0,
n = 0
n2an−1,
n ≥1
4. fn = an cos(n)
[Hint : Use cos(n) = 1
2(ein + e−in)]
5. fn = cos(n −2)Hn−2
6. fn = 3 + e−2nT
7. fn = sin(nω0T + θ),
8. fn =





0,
n = 0
1,
n = 1
2,
n = 2
1,
n = 3,
fn+4 = fn
9. fn = (−1)n
(Hint: fn is a periodic sequence.)
10. Using the property stated in Equation 3.2.20 and Equation 3.2.21 twice, ﬁnd the z-
transform of n2 = n[n(1)n]. Then verify your result using MATLAB.

The Z-Transform
173
11. Verify the convolution theorem using the sequences fn = gn = 1. Then check your
results using MATLAB.
12. Verify the convolution theorem using the sequences fn = 1 and gn = n. Then check
your results using MATLAB.
13. Verify the convolution theorem using the sequences fn = gn = 1/(n!). Then check
your results using MATLAB. Hint: Use the binomial theorem with x = 1 to evaluate the
summation.
14. If a is a real number, show that Z(anfn) = F(z/a), where Z(fn) = F(z).
3.3 INVERSE Z-TRANSFORMS
In the previous two sections we dealt with ﬁnding the z-transform. In this section we
ﬁnd fn by inverting the z-transform F(z). There are four methods for ﬁnding the inverse:
(1) power series, (2) recursion, (3) partial fractions, and (4) the residue method. We will
discuss each technique individually. The ﬁrst three apply only to those functions F(z) that
are rational functions while the residue method is more general. For symbolic computations
with MATLAB, you can use iztrans.
Power series
By means of the long-division process, we can always rewrite F(z) as the Laurent
expansion:
F(z) = a0 + a1z−1 + a2z−2 + · · · .
(3.3.1)
From the deﬁnition of the z-transform,
F(z) =
∞
X
n=0
fnz−n = f0 + f1z−1 + f2z−2 + · · · ,
(3.3.2)
the desired sequence fn is given by an.
• Example 3.3.1
Let
F(z) = z + 1
2z −2 = N(z)
D(z) .
(3.3.3)
Using long division, N(z) is divided by D(z) and we obtain
F(z) = 1
2 + z−1 + z−2 + z−3 + z−4 + · · · .
(3.3.4)
Therefore,
a0 = 1
2, a1 = 1, a2 = 1, a3 = 1, a4 = 1, etc.,
(3.3.5)
which suggests that f0 = 1
2 and fn = 1 for n ≥1 is the inverse of F(z).
⊓⊔

174
Advanced Engineering Mathematics: A Second Course
• Example 3.3.2
Let us ﬁnd the inverse of the z-transform:
F(z) =
2z2 −1.5z
z2 −1.5z + 0.5.
(3.3.6)
By the long-division process, we have that
2
+
1.5z−1
+
1.25z−2
+
1.125z−3
+
· · ·
z2 −1.5z + 0.5
2z2
−
1.5z
2z2
−
3z
+
1
1.5z
−
1
1.5z
−
2.25
+
0.750z−1
1.25
−
0.750z−1
1.25
−
1.870z−1
+
· · ·
1.125z−1
+
· · ·
Thus, f0 = 2, f1 = 1.5, f2 = 1.25, f3 = 1.125, and so forth, or fn = 1 + ( 1
2)n. In general,
this technique only produces numerical values for some of the elements of the sequence.
Note also that our long division must always yield the power series Equation 3.3.1 in order
for this method to be of any use.
To check our answer using MATLAB, we type the commands:
syms z; syms n positive
iztrans((2*z^2 - 1.5*z)/(z^2 - 1.5*z + 0.5),z,n)
which yields
ans =
1 + (1/2)^n
⊓⊔
Recursive method
An alternative to long division was suggested3 several years ago. It obtains the inverse
recursively.
We begin by assuming that the z-transform is of the form
F(z) = a0zm + a1zm−1 + a2zm−2 + · · · + am−1z + am
b0zm + b1zm−1 + b2zm−2 + · · · + bm−1z + bm
,
(3.3.7)
where some of the coeﬃcients ai and bi may be zero and b0 ̸= 0. Applying the initial-value
theorem,
f0 = lim
z→∞F(z) = a0/b0.
(3.3.8)
3 Jury, E. I., 1964: Theory and Application of the z-Transform Method. John Wiley & Sons, p. 41;
Pierre, D. A., 1963:
A tabular algorithm for z-transform inversion.
Control Engng., 10(9), 110–111;
Jenkins, L. B., 1967: A useful recursive form for obtaining inverse z-transforms. Proc. IEEE, 55, 574–575.

The Z-Transform
175
Next, we apply the initial-value theorem to z[F(z) −f0] and ﬁnd that
f1 = lim
z→∞z[F(z) −f0]
(3.3.9)
= lim
z→∞z (a0 −b0f0)zm + (a1 −b1f0)zm−1 + · · · + (am −bmf0)
b0zm + b1zm−1 + b2zm−2 + · · · + bm−1z + bm
(3.3.10)
= (a1 −b1f0)/b0.
(3.3.11)
Note that the coeﬃcient a0 −b0f0 = 0 from Equation 3.3.8. Similarly,
f2 = lim
z→∞z[zF(z) −zf0 −f1]
(3.3.12)
= lim
z→∞z (a0 −b0f0)zm+1 + (a1 −b1f0 −b0f1)zm + (a2 −b2f0 −b1f1)zm−1 + · · · −bmf1
b0zm + b1zm−1 + b2zm−2 + · · · + bm−1z + bm
(3.3.13)
= (a2 −b2f0 −b1f1)/b0
(3.3.14)
because a0 −b0f0 = a1 −b1f0 −f1b0 = 0. Continuing this process, we ﬁnally have that
fn = (an −bnf0 −bn−1f1 −· · · −b1fn−1) /b0,
(3.3.15)
where an = bn ≡0 for n > m.
• Example 3.3.3
Let us redo Example 3.3.2 using the recursive method. Comparing Equation 3.3.7 to
Equation 3.3.6, a0 = 2, a1 = −1.5, a2 = 0, b0 = 1, b1 = −1.5, b2 = 0.5, and an = bn = 0 if
n ≥3. From Equation 3.3.15,
f0 = a0/b0 = 2/1 = 2,
(3.3.16)
f1 = (a1 −b1f0)/b0 = [−1.5 −(−1.5)(2)]/1 = 1.5,
(3.3.17)
f2 = (a2 −b2f0 −b1f1)/b0
(3.3.18)
= [0 −(0.5)(2) −(−1.5)(1.5)]/1 = 1.25,
(3.3.19)
and
f3 = (a3 −b3f0 −b2f1 −b1f2)/b0
(3.3.20)
= [0 −(0)(2) −(0.5)(1.5) −(−1.5)(1.25)]/1 = 1.125.
(3.3.21)
⊓⊔
Partial fraction expansion
One of the popular methods for inverting Laplace transforms is partial fractions. A
similar, but slightly diﬀerent, scheme works here.

176
Advanced Engineering Mathematics: A Second Course
• Example 3.3.4
Given F(z) = z/
 z2 −1

, let us ﬁnd fn. The ﬁrst step is to obtain the partial fraction
expansion of F(z)/z.
Why we want F(z)/z rather than F(z) will be made clear in a
moment. Thus,
F(z)
z
=
1
(z −1)(z + 1) =
A
z −1 +
B
z + 1,
(3.3.22)
where
A = (z −1) F(z)
z

z=1
= 1
2,
and
B = (z + 1) F(z)
z

z=−1
= −1
2.
(3.3.23)
Multiplying Equation 3.3.22 by z,
F(z) = 1
2

z
z −1 −
z
z + 1

.
(3.3.24)
Next, we ﬁnd the inverse z-transform of z/(z −1) and z/(z + 1) in Table 3.1.1. This
yields
Z−1

z
z −1

= 1,
and
Z−1

z
z + 1

= (−1)n.
(3.3.25)
Thus, the inverse is
fn = 1
2 [1 −(−1)n] , n ≥0.
(3.3.26)
⊓⊔
From this example it is clear that there are two steps: (1) obtain the partial fraction
expansion of F(z)/z, and (2) ﬁnd the inverse z-transform by referring to Table 3.1.1.
• Example 3.3.5
Given F(z) = 2z2/[(z + 2)(z + 1)2], let us ﬁnd fn. We begin by expanding F(z)/z as
F(z)
z
=
2z
(z + 2)(z + 1)2 =
A
z + 2 +
B
z + 1 +
C
(z + 1)2 ,
(3.3.27)
where
A = (z + 2) F(z)
z

z=−2
= −4,
B = d
dz

(z + 1)2 F(z)
z

z=−1
= 4,
(3.3.28)
and
C = (z + 1)2 F(z)
z

z=−1
= −2,
(3.3.29)
so that
F(z) =
4z
z + 1 −
4z
z + 2 −
2z
(z + 1)2 ,
(3.3.30)
or
fn = Z−1
 4z
z + 1

−Z−1
 4z
z + 2

−Z−1

2z
(z + 1)2

.
(3.3.31)

The Z-Transform
177
From Table 3.1.1,
Z−1

z
z + 1

= (−1)n,
Z−1

z
z + 2

= (−2)n,
(3.3.32)
and
Z−1

z
(z + 1)2

= −Z−1

−z
(z + 1)2

= −n(−1)n = n(−1)n+1.
(3.3.33)
Applying Equation 3.3.32 and Equation 3.3.33 to Equation 3.3.31,
fn = 4(−1)n −4(−2)n + 2n(−1)n, n ≥0.
(3.3.34)
⊓⊔
• Example 3.3.6
Given F(z) = (z2 + z)/(z −2)2, let us determine fn. Because
F(z)
z
=
z + 1
(z −2)2 =
1
z −2 +
3
(z −2)2 ,
(3.3.35)
fn = Z−1

z
z −2

+ Z−1

3z
(z −2)2

.
(3.3.36)
Referring to Table 3.1.1,
Z−1

z
z −2

= 2n,
and
Z−1

3z
(z −2)2

= 3
2n2n.
(3.3.37)
Substituting Equation 3.3.37 into Equation 3.3.36 yields
fn =
  3
2n + 1

2n, n ≥0.
(3.3.38)
⊓⊔
Residue method
The power series, recursive, and partial fraction expansion methods are rather limited.
We now prove that fn may be computed from the following inverse integral formula:
fn =
1
2πi
I
C
zn−1F(z) dz,
n ≥0,
(3.3.39)
where C is any simple curve, taken in the positive sense, that encloses all of the singularities
of F(z). It is readily shown that the power series and partial fraction methods are special
cases of the residue method.

178
Advanced Engineering Mathematics: A Second Course
Proof : Starting with the deﬁnition of the z-transform
F(z) =
∞
X
n=0
fnz−n,
|z| > R1,
(3.3.40)
we multiply Equation 3.3.40 by zn−1 and integrating both sides around any contour C that
includes all of the singularities:
1
2πi
I
C
zn−1F(z) dz =
∞
X
m=0
fm
1
2πi
I
C
zn−m dz
z .
(3.3.41)
Let C be a circle of radius R, where R > R1. Then, changing variables to z = R eiθ, and
dz = iz dθ,
1
2πi
I
C
zn−m dz
z = Rn−m
2π
Z 2π
0
ei(n−m)θdθ =
 1,
m = n,
0,
otherwise.
(3.3.42)
Substituting Equation 3.3.42 into Equation 3.3.41 yields the desired result that
1
2πi
I
C
zn−1F(z) dz = fn.
(3.3.43)
⊓⊔
We can easily evaluate the inversion integral, Equation 3.3.39, using Cauchy’s residue
theorem.
• Example 3.3.7
Let us ﬁnd the inverse z-transform of
F(z) =
1
(z −1)(z −2).
(3.3.44)
From the inversion integral,
fn =
1
2πi
I
C
zn−1
(z −1)(z −2) dz.
(3.3.45)
Clearly the integral has simple poles at z = 1 and z = 2. However, when n = 0 we also have
a simple pole at z = 0. Thus the cases n = 0 and n > 0 must be considered separately.
Case 1: n = 0. The residue theorem yields
f0 = Res

1
z(z −1)(z −2); 0

+ Res

1
z(z −1)(z −2); 1

+ Res

1
z(z −1)(z −2); 2

.
(3.3.46)
Evaluating these residues,
Res

1
z(z −1)(z −2); 0

=
1
(z −1)(z −2)

z=0
= 1
2,
(3.3.47)

The Z-Transform
179
Res

1
z(z −1)(z −2); 1

=
1
z(z −2)

z=1
= −1,
(3.3.48)
and
Res

1
z(z −1)(z −2); 2

=
1
z(z −1)

z=2
= 1
2.
(3.3.49)
Substituting Equation 3.3.47 through Equation 3.3.49 into Equation 3.3.46 yields f0 = 0.
Case 2: n > 0. Here we only have contributions from z = 1 and z = 2.
fn = Res

zn−1
(z −1)(z −2); 1

+ Res

zn−1
(z −1)(z −2); 2

,
n > 0,
(3.3.50)
where
Res

zn−1
(z −1)(z −2); 1

= zn−1
z −2

z=1
= −1,
(3.3.51)
and
Res

zn−1
(z −1)(z −2); 2

= zn−1
z −1

z=2
= 2n−1, n > 0.
(3.3.52)
Thus,
fn = 2n−1 −1,
n > 0.
(3.3.53)
Combining our results,
fn =

0,
n = 0,
1
2 (2n −2) ,
n > 0.
(3.3.54)
⊓⊔
• Example 3.3.8
Let us use the inversion integral to ﬁnd the inverse of
F(z) = z2 + 2z
(z −1)2 .
(3.3.55)
The inversion theorem gives
fn =
1
2πi
I
C
zn+1 + 2zn
(z −1)2
dz = Res
zn+1 + 2zn
(z −1)2
; 1

,
(3.3.56)
where the pole at z = 1 is second order. Consequently, the corresponding residue is
Res
zn+1 + 2zn
(z −1)2
; 1

= d
dz

zn+1 + 2zn

z=1
= 3n + 1.
(3.3.57)
Thus, the inverse z-transform of Equation 3.3.55 is
fn = 3n + 1,
n ≥0.
(3.3.58)
⊓⊔

180
Advanced Engineering Mathematics: A Second Course
• Example 3.3.9
Let F(z) be a z-transform whose poles lie within the unit circle |z| = 1. Then
F(z) =
∞
X
n=0
fnz−n,
|z| > 1,
(3.3.59)
and
F(z)F(z−1) =
∞
X
n=0
f 2
n +
∞
X
n=0
∞
X
m=0
n̸=m
fmfnzm−n.
(3.3.60)
We now multiply both sides of Equation 3.3.60 by z−1 and integrate around the unit circle
C. Therefore,
I
|z|=1
F(z)F(z−1)z−1 dz =
∞
X
n=0
I
|z|=1
f 2
nz−1 dz +
∞
X
n=0
∞
X
m=0
n̸=m
fmfn
I
|z|=1
zm−n−1 dz, (3.3.61)
after interchanging the order of integration and summation. Performing the integration,
∞
X
n=0
f 2
n =
1
2πi
I
|z|=1
F(z)F(z−1)z−1 dz,
(3.3.62)
which is Parseval’s theorem for one-sided z-transforms. Recall that there are similar theo-
rems for Fourier series and transforms.
⊓⊔
• Example 3.3.10: Evaluation of partial summations4
We begin by noting that
SN =
N
X
n=1
fn =
1
2πi
I
C
F(z)
N
X
n=1
zn−1 dz.
(3.3.63)
Here we employed the inversion integral to replace fn and reversed the order of integration
and summation. This interchange is permissible since we only have a partial summation.
Because the summation in Equation 3.3.63 is a geometric series, we have the ﬁnal result
that
SN =
1
2πi
I
C
F(z)(zN −1)
z −1
dz.
(3.3.64)
Therefore, we can use the residue theorem and z-transforms to evaluate partial summations.
4 See Bunch, K. J., W. N. Cain, and R. W. Grow, 1990: The z-transform method of evaluating partial
summations in closed form. J. Phys. A, 23, L1213–L1215.

The Z-Transform
181
Let us ﬁnd SN = PN
n=1 n3.
Because fn = n3, F(z) = z(z2 + 4z + 1)/(z −1)4.
Consequently
SN = Res
z(z2 + 4z + 1)(zN −1)
(z −1)5
; 1

= 1
4!
d4
dz4

z(z2 + 4z + 1)(zN −1)

z=1
(3.3.65)
= 1
4!
d4
dz4
 zN+3 + 4zN+2 + zN+1 −z3 −4z2 −z

z=1
= 1
4(N + 1)2N 2.
(3.3.66)
⊓⊔
• Example 3.3.11
An additional beneﬁt of understanding inversion by the residue method is the ability
to qualitatively anticipate the inverse by knowing the location of the poles of F(z). This
intuition is important because many engineering analyses discuss stability and performance
entirely in terms of the properties of the system’s z-transform. In Figure 3.3.1 we graphed
the location of the poles of F(z) and the corresponding fn. The student should go through
the mental exercise of connecting the two pictures.
Problems
Use the power series or recursive method to compute the ﬁrst few values of fn of the
following z-transforms. Then check your answers with MATLAB.
1. F(z) = 0.09z2 + 0.9z + 0.09
12.6z2 −24z + 11.4
2. F(z) =
z + 1
2z4 −2z3 + 2z −2
3. F(z) =
1.5z2 + 1.5z
15.25z2 −36.75z + 30.75
4. F(z) =
6z2 + 6z
19z3 −33z2 + 21z −7
Use partial fractions to ﬁnd the inverse of the following z-transforms. Then verify your
answers with MATLAB.
5. F(z) =
z(z + 1)
(z −1)(z2 −z + 1/4)
6. F(z) =
(1 −e−aT )z
(z −1)(z −e−aT )
7. F(z) =
z2
(z −1)(z −α)
8. F(z) = (2z −a −b)z
(z −a)(z −b)
9. Using the property that the z-transform of gn = fn−kHn−k if n ≥0 is G(z) = z−kF(z),
ﬁnd the inverse of
F(z) =
z + 1
z10(z −1/2).
Then check your answer with MATLAB.
Use the residue method to ﬁnd the inverse z-transform of the following z-transforms. Then
verify your answer with MATLAB.
10. F(z) =
z2 + 3z
(z −1/2)3
11. F(z) =
z
(z + 1)2(z −2)
12. F(z) =
z
(z + 1)2(z −1)2
13. F(z) = ea/z

182
Advanced Engineering Mathematics: A Second Course
n
|z|=1
n
n
|z|=1
n
f
f
n
|z|=1
n
n
|z|=1
n
f
f
n
|z|=1
n
n
|z|=1
n
f
f
Figure 3.3.1: The correspondence between the location of the simple poles of the z-transform F(z) and
the behavior of fn.

The Z-Transform
183
3.4 SOLUTION OF DIFFERENCE EQUATIONS
Having reached the point where we can take a z-transform and then ﬁnd its inverse,
we are ready to use it to solve diﬀerence equations. The procedure parallels that of solving
ordinary diﬀerential equations by Laplace transforms. Essentially we reduce the diﬀerence
equation to an algebraic problem. We then ﬁnd the solution by inverting Y (z).
• Example 3.4.1
Let us solve the second-order diﬀerence equation
2yn+2 −3yn+1 + yn = 5 3n, n ≥0,
(3.4.1)
where y0 = 0 and y1 = 1.
Taking the z-transform of both sides of Equation 3.4.1, we obtain
2Z(yn+2) −3Z(yn+1) + Z(yn) = 5 Z(3n).
(3.4.2)
From the shifting theorem and Table 3.1.1,
2z2Y (z) −2z2y0 −2zy1 −3[zY (z) −zy0] + Y (z) =
5z
z −3.
(3.4.3)
Substituting y0 = 0 and y1 = 1 into Equation 3.4.3 and simplifying yields
(2z −1)(z −1)Y (z) = z(2z −1)
z −3
,
or
Y (z) =
z
(z −3)(z −1).
(3.4.4)
To obtain yn from Y (z) we can employ partial fractions or the residue method. Applying
partial fractions gives
Y (z)
z
=
A
z −1 +
B
z −3,
(3.4.5)
where
A = (z −1) Y (z)
z

z=1
= −1
2,
and
B = (z −3) Y (z)
z

z=3
= 1
2.
(3.4.6)
Thus,
Y (z) = −1
2
z
z −1 + 1
2
z
z −3,
or
yn = −1
2Z−1

z
z −1

+ 1
2Z−1

z
z −3

. (3.4.7)
From Equation 3.4.7 and Table 3.1.1,
yn = 1
2 (3n −1) ,
n ≥0.
(3.4.8)
An alternative to this hand calculation is to use MATLAB’s ztrans and iztrans to
solve diﬀerence equations. In the present case, the MATLAB script would read
clear
% define symbolic variables
syms z Y; syms n positive

184
Advanced Engineering Mathematics: A Second Course
% take z-transform of left side of difference equation
LHS = ztrans(2*sym(’y(n+2)’)-3*sym(’y(n+1)’)+sym(’y(n)’),n,z);
% take z-transform of right side of difference equation
RHS = 5 * ztrans(3^n,n,z);
% set Y for z-transform of y and introduce initial conditions
newLHS = subs(LHS,’ztrans(y(n),n,z)’,’y(0)’,’y(1)’,Y,0,1);
% solve for Y
Y = solve(newLHS-RHS,Y);
% invert z-transform and find y(n)
y = iztrans(Y,z,n)
This script produced
y =
-1/2+1/2*3^n
Two checks conﬁrm that we have the correct solution. First, our solution must satisfy
the initial values of the sequence. Computing y0 and y1,
y0 = 1
2(30 −1) = 1
2(1 −1) = 0,
and
y1 = 1
2(31 −1) = 1
2(3 −1) = 1.
(3.4.9)
Thus, our solution gives the correct initial values.
Our sequence yn must also satisfy the diﬀerence equation. Now
yn+2 = 1
2(3n+2−1) = 1
2(9 3n−1),
and
yn+1 = 1
2(3n+1−1) = 1
2(3 3n−1). (3.4.10)
Therefore,
2yn+2 −3yn+1 + yn =
 9 −9
2 + 1
2

3n −1 + 3
2 −1
2 = 5 3n
(3.4.11)
and our solution is correct.
Finally, we note that the term 3n/2 is necessary to give the right side of Equation 3.4.1;
it is the particular solution. The −1/2 term is necessary so that the sequence satisﬁes the
initial values; it is the complementary solution.
⊓⊔
• Example 3.4.2
Let us ﬁnd the yn in the diﬀerence equation
yn+2 −2yn+1 + yn = 1,
n ≥0
(3.4.12)
with the initial conditions y0 = 0 and y1 = 3/2.
From Equation 3.4.12,
Z(yn+2) −2Z(yn+1) + Z(yn) = Z(1).
(3.4.13)
The z-transform of the left side of Equation 3.4.13 is obtained from the shifting theorem,
and Table 3.1.1 yields Z(1). Thus,
z2Y (z) −z2y0 −zy1 −2zY (z) + 2zy0 + Y (z) =
z
z −1.
(3.4.14)
Substituting y0 = 0 and y1 = 3/2 in Equation 3.4.14 and simplifying gives
Y (z) = 3z2 −z
2(z −1)3
or
yn = Z−1
 3z2 −z
2(z −1)3

.
(3.4.15)

The Z-Transform
185
We ﬁnd the inverse z-transform of Equation 3.4.15 by the residue method, or
yn =
1
2πi
I
C
3zn+1 −zn
2(z −1)3 dz = 1
2!
d2
dz2
3zn+1
2
−zn
2

z=1
= 1
2n2 + n.
(3.4.16)
Thus,
yn = 1
2n2 + n,
n ≥0.
(3.4.17)
Note that n2/2 gives the particular solution to Equation 3.4.12, while n is there so that
yn satisﬁes the initial conditions.
This problem is particularly interesting because our
constant forcing produces a response that grows as n2, just as in the case of resonance in
a time-continuous system when a ﬁnite forcing such as sin(ω0t) results in a response whose
amplitude grows as tm.
⊓⊔
• Example 3.4.3
Let us solve the diﬀerence equation
b2yn + yn+2 = 0,
(3.4.18)
where the initial conditions are y0 = b2 and y1 = 0.
We begin by taking the z-transform of each term in Equation 3.4.18. This yields
b2Z(yn) + Z(yn+2) = 0.
(3.4.19)
From the shifting theorem, it follows that
b2Y (z) + z2Y (z) −z2y0 −zy1 = 0.
(3.4.20)
Substituting y0 = b2 and y1 = 0 into Equation 3.4.20,
b2Y (z) + z2Y (z) −b2z2 = 0,
or
Y (z) =
b2z2
z2 + b2 .
(3.4.21)
To ﬁnd yn we employ the residue method or
yn =
1
2πi
I
C
b2zn+1
(z −ib)(z + ib) dz.
(3.4.22)
Thus,
yn = b2zn+1
z + ib

z=ib
+ b2zn+1
z −ib

z=−ib
= bn+2in
2
+ bn+2(−i)n
2
(3.4.23)
= bn+2einπ/2
2
+ bn+2e−inπ/2
2
= bn+2 cos
nπ
2

,
(3.4.24)
because cos(x) = 1
2
 eix + e−ix
. Consequently, we obtain the desired result that
yn = bn+2 cos
nπ
2

for n ≥0.
(3.4.25)
⊓⊔

186
Advanced Engineering Mathematics: A Second Course
0
5
10
15
20
25
30
0
1
2
3
4
5
6
7
8
9
10
number of conversion periods
amount left in account (K$)
Figure 3.4.1: The amount in a savings account as a function of an annual conversion period when interest
is compounded at the annual rate of 12% and $1000 is taken from the account every period starting with
period 10.
• Example 3.4.4: Compound interest
Diﬀerence equations arise in ﬁnance because the increase or decrease in an account
occurs in discrete steps. For example, the amount of money in a compound interest savings
account after n + 1 conversion periods (the time period between interest payments) is
yn+1 = yn + ryn,
(3.4.26)
where r is the interest rate per conversion period. The second term on the right side of
Equation 3.4.32 is the amount of interest paid at the end of each period.
Let us ask a somewhat more diﬃcult question of how much money we will have if we
withdraw the amount A at the end of every period starting after the period ℓ. Now the
diﬀerence equation reads
yn+1 = yn + ryn −AHn−ℓ−1.
(3.4.27)
Taking the z-transform of Equation 3.4.27,
zY (z) −zy0 = (1 + r)Y (z) −Az2−ℓ
z −1
(3.4.28)
after using Equation 3.2.9 or
Y (z) =
y0z
z −(1 + r) −
Az2−ℓ
(z −1)[z −(1 + r)].
(3.4.29)
Taking the inverse of Equation 3.4.29,
yn = y0(1 + r)n −A
r

(1 + r)n−ℓ+1 −1

Hn−ℓ.
(3.4.30)
The ﬁrst term in Equation 3.4.30 represents the growth of money by compound interest
while the second term gives the depletion of the account by withdrawals.
Figure 3.4.1 gives the values of yn for various starting amounts assuming an annual
conversion period with r = 0.12, ℓ= 10 years, and A = $1000. These computations were
done in two ways using MATLAB as follows:

The Z-Transform
187
% load in parameters
clear; r = 0.12; A = 1; k = 0:30;
y = zeros(length(k),3); yanswer = zeros(length(k),3);
% set initial condition
for m=1:3
y(1,m) = m;
% compute other y values
for n = 1:30
y(n+1,m) = y(n,m)+r*y(n,m);
y(n+1,m) = y(n+1,m)-A*stepfun(n,11);
end
% now use Equation 3.4.30
for n = 1:31
yanswer(n,m) = y(1,m)*(1+r)^(n-1);
yanswer(n,m) = yanswer(n,m)-A*((1+r)^(n-10)-1)
*stepfun(n,11)/r;
end; end;
plot(k,y,’o’); hold; plot(k,yanswer,’s’);
axis([0 30 0 10])
xlabel(’number of conversion periods’,’Fontsize’,20)
ylabel(’amount left in account (K$)’,’Fontsize’,20)
Figure 3.4.1 shows that if an investor places an initial amount of $3000 in an account
bearing 12% annually, after 10 years he can withdraw $1000 annually, essentially forever.
This is because the amount that he removes every year is replaced by the interest on the
funds that remain in the account.
⊓⊔
• Example 3.4.5
Let us solve the following system of diﬀerence equations:
xn+1 = 4xn + 2yn,
and
yn+1 = 3xn + 3yn,
(3.4.31)
with the initial values of x0 = 0 and y0 = 5.
Taking the z-transform of Equation 3.4.31,
zX(z) −x0z = 4X(z) + 2Y (z),
zY (z) −y0z = 3X(z) + 3Y (z),
(3.4.32)
or
(z −4)X(z) −2Y (z) = 0,
3X(z) −(z −3)Y (z) = −5z.
(3.4.33)
Solving for X(z) and Y (z),
X(z) =
10z
(z −6)(z −1) =
2z
z −6 −
2z
z −1,
(3.4.34)
and
Y (z) =
5z(z −4)
(z −6)(z −1) =
2z
z −6 +
3z
z −1.
(3.4.35)
Taking the inverse of Equation 3.4.34 and Equation 3.4.35 term by term,
xn = −2 + 2 6n,
and
yn = 3 + 2 6n.
(3.4.36)

188
Advanced Engineering Mathematics: A Second Course
We can also check our work using the MATLAB script
clear
% define symbolic variables
syms X Y z; syms n positive
% take z-transform of left side of differential equations
LHS1 = ztrans(sym(’x(n+1)’)-4*sym(’x(n)’)-2*sym(’y(n)’),n,z);
LHS2 = ztrans(sym(’y(n+1)’)-3*sym(’x(n)’)-3*sym(’y(n)’),n,z);
% set X and Y for the z-transform of x and y
%
and introduce initial conditions
newLHS1 = subs(LHS1,’ztrans(x(n),n,z)’,’ztrans(y(n),n,z)’,...
’x(0)’,’y(0)’,X,Y,0,5);
newLHS2 = subs(LHS2,’ztrans(x(n),n,z)’,’ztrans(y(n),n,z)’,...
’x(0)’,’y(0)’,X,Y,0,5);
% solve for X and Y
[X,Y] = solve(newLHS1,newLHS2,X,Y);
% invert z-transform and find x(n) and y(n)
x = iztrans(X,z,n)
y = iztrans(Y,z,n)
This script yields
x =
2*6^n-2
y =
2*6^n+3
Problems
Solve the following diﬀerence equations using z-transforms, where n ≥0. Check your answer
using MATLAB.
1. yn+1 −yn = n2,
y0 = 1.
2. yn+2 −2yn+1 + yn = 0,
y0 = y1 = 1.
3. yn+2 −2yn+1 + yn = 1,
y0 = y1 = 0.
4. yn+1 + 3yn = n,
y0 = 0.
5. yn+1 −5yn = cos(nπ),
y0 = 0.
6. yn+2 −4yn = 1,
y0 = 1, y1 = 0.
7. yn+2 −1
4yn = ( 1
2)n, y0 = y1 = 0.
8. yn+2 −5yn+1 + 6yn = 0, y0 = y1 = 1.
9. yn+2 −3yn+1 + 2yn = 1,
y0 = y1 = 0.
10. yn+2 −2yn+1 + yn = 2,
y0 = 0, y1 = 2.
11. xn+1 = 3xn −4yn, yn+1 = 2xn −3yn,
x0 = 3, y0 = 2.
12. xn+1 = 2xn −10yn, yn+1 = −xn −yn,
x0 = 3, y0 = −2.
13. xn+1 = xn −2yn, yn+1 = −6yn,
x0 = −1, y0 = −7.
14. xn+1 = 4xn −5yn, yn+1 = xn −2yn,
x0 = 6, y0 = 2.

The Z-Transform
189
3.5 STABILITY OF DISCRETE-TIME SYSTEMS
When we discussed the solution of ordinary diﬀerential equations by Laplace trans-
forms, we introduced the concept of transfer function and impulse response. In the case of
discrete-time systems, similar considerations come into play.
Consider the recursive system
yn = a1yn−1Hn−1 + a2yn−2Hn−2 + xn,
n ≥0,
(3.5.1)
where Hn−k is the unit step function. It equals 0 for n < k and 1 for n ≥k. Equation
3.5.1 is called a recursive system because future values of the sequence depend upon all of
the previous values. At present, a1 and a2 are free parameters that we shall vary.
Using Equation 3.2.7,
z2Y (z) −a1zY (z) −a2Y (z) = z2X(z),
(3.5.2)
or
G(z) = Y (z)
X(z) =
z2
z2 −a1z −a2
.
(3.5.3)
As in the case of Laplace transforms, the ratio Y (z)/X(z) is the transfer function. The
inverse of the transfer function gives the impulse response for our discrete-time system.
This particular transfer function has two poles, namely
z1,2 = a1
2 ±
r
a2
1
4 + a2.
(3.5.4)
At this point, we consider three cases.
Case 1: a2
1/4 + a2 < 0. In this case z1 and z2 are complex conjugates. Let us write them
as z1,2 = re±iω0T . Then
G(z) =
z2
(z −reiω0T )(z −re−iω0T ) =
z2
z2 −2r cos(ω0T)z + r2 ,
(3.5.5)
where r2 = −a2, and ω0T = cos−1(a1/2r). From the inversion integral,
gn = Res

zn+1
z2 −2r cos(ω0T)z + r2 ; z1

+ Res

zn+1
z2 −2r cos(ω0T)z + r2 ; z2

,
(3.5.6)
where gn denotes the impulse response. Now
Res

zn+1
z2 −2r cos(ω0T)z + r2 ; z1

= lim
z→z1
(z −z1)zn+1
(z −z1)(z −z2)
(3.5.7)
= rn exp[i(n + 1)ω0T]
eiω0T −e−iω0T
= rn exp[i(n + 1)ω0T]
2i sin(ω0T)
.
(3.5.8)
Similarly,
Res

zn+1
z2 −2r cos(ω0T)z + r2 ; z2

= −rn exp[−i(n + 1)ω0T]
2i sin(ω0T)
,
(3.5.9)

190
Advanced Engineering Mathematics: A Second Course
and
gn = rn sin[(n + 1)ω0T]
sin(ω0T)
.
(3.5.10)
A graph of sin[(n + 1)ω0T]/ sin(ω0T) with respect to n gives a sinusoidal envelope.
More importantly, if |r| < 1 these oscillations vanish as n →∞and the system is stable.
On the other hand, if |r| > 1 the oscillations grow without bound as n →∞and the system
is unstable.
Recall that |r| > 1 corresponds to poles that lie outside the unit circle while |r| < 1 is
exactly the opposite. Our example suggests that for discrete-time systems to be stable, all
of the poles of the transfer function must lie within the unit circle while an unstable system
has at least one pole that lies outside of this circle.
Case 2: a2
1/4 + a2 > 0. This case leads to two real roots, z1 and z2. From the inversion
integral, the sum of the residues gives the impulse response
gn = zn+1
1
−zn+1
2
z1 −z2
.
(3.5.11)
Once again, if the poles lie within the unit circle, |z1| < 1 and |z2| < 1, the system is stable.
Case 3: a2
1/4 + a2 = 0. This case yields z1 = z2,
G(z) =
z2
(z −a1/2)2
and
gn =
1
2πi
I
C
zn+1
(z −a1/2)2 dz =
a1
2
n
(n + 1).
(3.5.12)
This system is obviously stable if |a1/2| < 1 and the pole of the transfer function lies within
the unit circle.
In summary, ﬁnding the transfer function of a discrete-time system is important in
determining its stability. Because the location of the poles of G(z) determines the response
of the system, a stable system has all of its poles within the unit circle. Conversely, if
any of the poles of G(z) lie outside of the unit circle, the system is unstable. Finally, if
limn→∞gn = c, the system is marginally stable. For example, if G(z) has simple poles,
some of the poles must lie on the unit circle.
• Example 3.5.1
Numerical methods of integration provide some of the simplest, yet most important,
diﬀerence equations in the literature. In this example,5 we show how z-transforms can be
used to highlight the strengths and weaknesses of such schemes.
Consider the trapezoidal integration rule in numerical analysis.
The integral yn is
updated by adding the latest trapezoidal approximation of the continuous curve. Thus, the
integral is computed by
yn = 1
2T(xn + xn−1Hn−1) + yn−1Hn−1,
(3.5.13)
where T is the interval between evaluations of the integrand.
5 See Salzer, J. M., 1954: Frequency analysis of digital computers operating in real time. Proc. IRE,
42, 457–466.

The Z-Transform
191
We ﬁrst determine the stability of this rule because it is of little value if it is not stable.
Using Equation 3.2.7, the transfer function is
G(z) = Y (z)
X(z) = T
2
z + 1
z −1

.
(3.5.14)
To ﬁnd the impulse response, we use the inversion integral and ﬁnd that
gn = T
4πi
I
C
zn−1 z + 1
z −1 dz.
(3.5.15)
At this point, we must consider two cases: n = 0 and n > 0. For n = 0,
g0 = T
2 Res
 z + 1
z(z −1); 0

+ T
2 Res
 z + 1
z(z −1); 1

= T
2 .
(3.5.16)
For n > 0,
g0 = T
2 Res
zn−1(z + 1)
z −1
; 1

= T.
(3.5.17)
Therefore, the impulse response for this numerical scheme is g0 = T
2 and gn = T for n > 0.
Note that this is a marginally stable system (the solution neither grows nor decays with n)
because the pole associated with the transfer function lies on the unit circle.
Having discovered that the system is not unstable, let us continue and explore some
of its properties. Recall now that z = esT = eiωT if s = iω. Then the transfer function
becomes
G(ω) = T
2
1 + e−iωT
1 −e−iωT = −iT
2 cot
ωT
2

.
(3.5.18)
On the other hand, the transfer function of an ideal integrator is 1/s or −i/ω. Thus, the
trapezoidal rule has ideal phase but its shortcoming lies in its amplitude characteristic; it
lies below the ideal integrator for 0 < ωT < π. We show this behavior, along with that for
Simpson’s one-third rule and Simpson’s three-eighths rule, in Figure 3.5.1.
Figure 3.5.1 conﬁrms the superiority of Simpson’s one-third rule over his three-eighths
rule. The ﬁgure also shows that certain schemes are better at suppressing noise at higher
frequencies, an eﬀect not generally emphasized in numerical calculus but often important in
system design. For example, the trapezoidal rule is inferior to all others at low frequencies
but only to Simpson’s one-third rule at higher frequencies. Furthermore, the trapezoidal
rule might actually be preferred, not only because of its simplicity but also because it
attenuates at higher frequencies, thereby counteracting the eﬀect of noise.
⊓⊔
• Example 3.5.2
Given the transfer function
G(z) =
z2
(z −1)(z −1/2),
(3.5.19)
is this discrete-time system stable or marginally stable?
This transfer function has two simple poles. The pole at z = 1/2 gives rise to a term
that varies as ( 1
2)n in the impulse response, while the z = 1 pole gives a constant. Because
this constant neither grows nor decays with n, the system is marginally stable.
⊓⊔

192
Advanced Engineering Mathematics: A Second Course
0.0
1.0
2.0
3.0
ωΤ
0.1
1.0
10.0
Ratio of quadrature amplitudes to ideal integration      
Trapezoidal
Simpson’s
3/8−rule
Rule
Simpson’s
1/3−rule
Ideal Rule
Figure 3.5.1: Comparison of various quadrature formulas by ratios of their amplitudes to that of an ideal
integrator. (From Salzer, J. M., 1954: Frequency analysis of digital computers operating in real time. Proc.
IRE, 42, p. 463.)
• Example 3.5.3
In most cases the transfer function consists of a ratio of two polynomials.
In this
case we can use the MATLAB function filter to compute the impulse response as follows:
Consider the Kronecker delta sequence, x0 = 1, and xn = 0 for n > 0. From the deﬁnition
of the z-transform, X(z) = 1. Therefore, if our input into filter is the Kronecker delta
sequence, the output yn will be the impulse response since Y (z) = G(z). If the impulse
response grows without bound as n increases, the system is unstable. If it goes to zero as
n increases, the system is stable. If it remains constant, it is marginally stable.
To illustrate this concept, the following MATLAB script ﬁnds the impulse response
corresponding to the transfer function, Equation 3.5.19:
% enter the coefficients of the numerator
%
of the transfer function, Equation 3.5.19
num = [1 0 0];
% enter the coefficients of the denominator
%
of the transfer function, Equation 3.5.19
den = [1 -1.5 0.5];
% create the Kronecker delta sequence
x = [1 zeros(1,20)];
% find the impulse response
y = filter(num,den,x);
% plot impulse response
plot(y,’o’), axis([0 20 0.5 2.5])
xlabel(’n+1’,’Fontsize’,20)
ylabel(’impulse response’,’Fontsize’,20)
Figure 3.5.2 shows the computed impulse response. The asymptotic limit is two, so the
system is marginally stable, as we found before.
We note in closing that the same procedure can be used to ﬁnd the inverse of any
z-transform that consists of a ratio of two polynomials. Here we simply set G(z) equal to
the given z-transform and perform the same analysis.

The Z-Transform
193
0
5
10
15
20
0.5
1
1.5
2
2.5
n+1
impulse response
Figure 3.5.2: The impulse response for a discrete system with a transform function given by Equation
3.5.19.
Problems
For the following time-discrete systems, ﬁnd the transfer function and determine whether
the systems are unstable, marginally stable, or stable. Check your answer by graphing the
impulse response using MATLAB.
1. yn = yn−1Hn−1 + xn
2. yn = 2yn−1Hn−1 −yn−2Hn−2 + xn
3. yn = 3yn−1Hn−1 + xn
4. yn = 1
4yn−2Hn−2 + xn
Further Readings
Jury, E. I., 1964: Theory and Application of the z-Transform Method. John Wiley & Sons,
330 pp. The classic text on z-transforms.
LePage, W. R., 1980: Complex Variables and the Laplace Transform for Engineers. Dover,
483 pp. Chapter 16 is on z-transforms.


−15
−10
−5
0
5
10
15
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
t
exact Hilbert transform
computed Hilbert transform
Chapter 4
The Hilbert Transform
In addition to the Fourier, Laplace, and z-transforms, there are many other linear
transforms that have their own special niche in engineering.
Examples include Hankel,
Walsh, Radon, and Hartley transforms. In this chapter we consider the Hilbert transform,
which is a commonly used technique for relating the real and imaginary parts of a spectral
response, particularly in communication theory.
We begin our study of Hilbert transforms by ﬁrst deﬁning them and then exploring
their properties. Next, we develop the concept of the analytic signal. Finally, we explore
a property of Hilbert transforms that is frequently applied to data analysis: the Kramers-
Kronig relationship.
4.1 DEFINITION
In Chapter 3 we motivated the development of z-transforms by exploring the concept
of the ideal sampler. In the case of Hilbert transforms, we introduce another fundamental
operation, namely quadrature phase shifting or the ideal Hilbert transformer. This proce-
dure does nothing more than shift the phase of all input frequency components by −π/2.
Hilbert transformers are frequently used in communication systems and signal processing;
examples include the generation of single-sideband modulated signals and radar and speech
signal processing.
Because a −π/2 phase shift is equivalent to multiplying the Fourier transform of a
signal by e−iπ/2 = −i, and because phase shifting must be an odd function of frequency,1
1 For a real function the phase of its Fourier transform must be an odd function of ω.
195

196
Advanced Engineering Mathematics: A Second Course
the transfer function of the phase shifter is G(ω) = −i sgn(ω), where sgn(·) is deﬁned by
sgn(t) =
( 1,
t > 0,
0,
t = 0,
−1,
t < 0.
In other words, if X(ω) denotes the input spectrum to the phase shifter, the output spec-
trum must be −i sgn(ω)X(ω). If the process is repeated, the total phase shift is −π, a
complete phase reversal of all frequency components. The output spectrum then equals
[−i sgn(ω)]2X(ω) = −X(ω). This agrees with the notion of phase reversal because the
output function is −x(t).
Consider now the impulse response of the quadrature phase shifter, g(t) = F−1[G(ω)].
From the deﬁnition of Fourier transforms,
dG
dω = −i
Z ∞
−∞
t g(t)e−iωt dt,
(4.1.1)
and
g(t) = i
tF−1
dG
dω

.
(4.1.2)
Since G′(ω) = −2iδ(ω), the corresponding impulse response is
g(t) = i
tF−1[−2iδ(ω)] = 1
πt.
(4.1.3)
Consequently, if x(t) is the input to a quadrature phase shifter, the superposition integral
gives the output time function as
bx(t) = x(t) ∗1
πt = 1
π
Z ∞
−∞
x(τ)
t −τ dτ.
(4.1.4)
We shall deﬁne bx(t) as the Hilbert transform of x(t), although some authors use the negative
of Equation 4.1.4 corresponding to a +π/2 phase shift. The transform bx(t) is also called
the harmonic conjugate of x(t).
In similar fashion, bbx(t) is the Hilbert transform of the Hilbert transform of x(t) and
corresponds to the output of two cascaded phase shifters. However, this output is known
to be −x(t), so bbx(t) = −x(t), and we arrive at the inverse Hilbert transform relationship
that
x(t) = −bx(t) ∗1
πt = −1
π
Z ∞
−∞
bx(τ)
t −τ dτ.
(4.1.5)
Taken together, x(t) and bx(t) are called a Hilbert pair.
Hilbert pairs enjoy the unique
property that x(t) + ibx(t) is an analytic function.2
2 For the proof, see Titchmarsh, E. C., 1948: Introduction to the Theory of Fourier Integrals. Oxford
University Press, p. 125.

The Hilbert Transform
197
Descended from a Prussian middle-class family, David Hilbert (1862–1943) would make signiﬁcant
contributions in the ﬁelds of algebraic form, algebraic number theory, foundations of geometry,
analysis, mathematical physics, and the foundations of mathematics.
Hilbert transforms arose
during his study of integral equations (Hilbert, D., 1912: Grundz¨uge einer allgemeinen Theorie der
linearen Integralgleichungen. Teubner, p. 75). (Portrait courtesy of Photo AKG, London, with
permission.)
Because of the singularity at τ = t, the integrals in Equation 4.1.4 and Equation 4.1.5
must be taken in the Cauchy principal value sense by approaching the singularity point
from both sides, namely
Z ∞
−∞
f(τ) dτ = lim
ǫ→0
Z t−ǫ
−∞
f(τ) dτ +
Z ∞
t+ǫ
f(τ) dτ

,
(4.1.6)
so that the inﬁnities to the right and left of τ = t cancel each other. See Section 1.10.
We also note that the Hilbert transform is basically a convolution and does not produce a
change of domain; if x is a function of time, then bx is also a function of time. This is quite
diﬀerent from what we encountered with Laplace or Fourier transforms.
From its origin in phase shifting, Hilbert transforms of sinusoidal functions are trivial.
Some examples are
d
cos(ωt + ϕ) = cos
 ωt + ϕ −π
2

= sgn(ω) sin(ωt + ϕ).
(4.1.7)
Similarly,
d
sin(ωt + ϕ) = −sgn(ω) cos(ωt + ϕ),
(4.1.8)

198
Advanced Engineering Mathematics: A Second Course
and
d
eiωt+iϕ = −i sgn(ω)eiωt+iϕ.
(4.1.9)
Thus, Hilbert transformation does not change the amplitude of sine or cosine but does
change their phase by ±π/2.
• Example 4.1.1
Let us apply the integral deﬁnition of the Hilbert transform, Equation 4.1.4, to ﬁnd
the Hilbert transform of sin(ωt), ω ̸= 0.
From the deﬁnition,
H[sin(ωt)] = 1
π
Z ∞
−∞
sin(ωτ)
t −τ
dτ.
(4.1.10)
If x = t −τ, then
H[sin(ωt)] = −cos(ωt)
π
Z ∞
−∞
sin(ωx)
x
dx = −cos(ωt) sgn(ω).
(4.1.11)
⊓⊔
• Example 4.1.2
Let us compute the Hilbert transform of x(t) = sin(t)/(t2 + 1) from the deﬁnition of
the Hilbert transform, Equation 4.1.4.
From the deﬁnition,
bx(t) = 1
π PV
Z ∞
−∞
sin(τ)
(t −τ)(τ 2 + 1) dτ = 1
π ℑ

PV
Z ∞
−∞
eiτ
(t −τ)(τ 2 + 1) dτ

.
(4.1.12)
Because of the singularity on the real axis at τ = t, we treat the integrals in Equation 4.1.12
in the sense of Cauchy principal value.
To evaluate Equation 4.1.12, we convert it into a closed contour integration by in-
troducing a semicircle CR of inﬁnite radius in the upper half-plane. This yields a closed
contour C, which consists of the real line plus this semicircle. Therefore, Equation 4.1.12
can be rewritten
PV
Z ∞
−∞
eiτ
(t −τ)(τ 2 + 1) dτ = PV
I
C
eiz
(t −z)(z2 + 1) dz−
Z
CR
eiz
(t −z)(z2 + 1) dz. (4.1.13)
The second integral on the right side of Equation 4.1.13 vanishes by Equation 1.9.7.
The evaluation of the closed integral in Equation 4.1.13 follows from the residue theo-
rem. We have that
Res

eiz
(t −z)(z2 + 1); t

= lim
z→t
(z −t) eiz
(t −z)(z2 + 1) = −
eit
t2 + 1,
(4.1.14)
and
Res

eiz
(t −z)(z2 + 1); i

= lim
z→i
(z −i) eiz
(t −z)(z2 + 1) =
e−1
2i(t −i).
(4.1.15)
We do not have a contribution from z = −i because it lies outside of the closed contour.

The Hilbert Transform
199
The Hilbert Transform of Some Common Functions
function, x(t)
Hilbert transform, bx(t)
1.

1,
a < t < b
0,
otherwise
1
π ln

t −a
t −b

2.
sin(ωt + ϕ)
−sgn(ω) cos(ωt + ϕ)
3.
cos(ωt + ϕ)
sgn(ω) sin(ωt + ϕ)
4.
eiωt+ϕi
−i sgn(ω)eiωt+ϕi
5.
1
t
−πδ(t)
6.
1
t2 + a2 ,
0 < ℜ(a)
t
a(t2 + a2)
7.
λt + µa
t2 + a2 ,
0 < ℜ(a)
µt −λa
t2 + a2
8.
1
1 + t4
t(1 + t2)
√
2 (1 + t4)
9.
sin(at)
t
,
0 < a
1 −cos(at)
t
10.
sin(t)
1 + t2
e−1 −cos(t)
1 + t2
11.
sin(at)J1(at),
0 < a
−cos(at)J1(at)
12.
sin(at)Jn(bt),
0 < b < a
−cos(at)Jn(bt)
13.
cos(at)J1(at),
0 < a
sin(at)J1(at)
14.
cos(at)Jn(bt),
0 < b < a
sin(at)Jn(at)
15.
 √
a2 −t2,
−a < t < a
0,
otherwise



t +
√
t2 −a2,
−∞< t < −a
t,
−a < t < a
t −
√
t2 −a2,
a < t < ∞
16.
sin
 a
√
t

H(t),
0 < a
(
−e−a√
|t|,
−∞< t < 0
−cos
 a
√
t

,
0 < t < ∞

200
Advanced Engineering Mathematics: A Second Course
Therefore,
PV
Z ∞
−∞
eiτ
(t −τ)(τ 2 + 1) dτ = −πi eit
t2 + 1 + π e−1(t + i)
t2 + 1
.
(4.1.16)
Only one half of the value of the residue at z = t was included; this reﬂects the semicircular
indentation around the singularity there. Substituting Equation 4.1.16 into Equation 4.1.12,
we obtain the ﬁnal result that
H
 sin(t)
t2 + 1

= e−1 −cos(t)
t2 + 1
.
(4.1.17)
⊓⊔
• Example 4.1.3
Let us employ the relationship that the Fourier transform of bx(t) equals −i sgn(ω) times
the Fourier transform of x(t) to ﬁnd the Hilbert transform of x(t) = e−t2.
Because F(e−t2) = √πe−ω2/4,
b
X(ω) = −i√π sgn(ω)e−ω2/4.
(4.1.18)
Therefore,
bx(t) =
i
2√π
Z 0
−∞
eitω−ω2/4 dω −
i
2√π
Z ∞
0
eitω−ω2/4 dω
(4.1.19)
=
i
√π
Z 0
−∞
e2itη−η2 dη −
i
√π
Z ∞
0
e2itη−η2 dη
(4.1.20)
= e−t2
√π
Z t
−i∞
e−s2 ds −e−t2
√π
Z i∞
t
e−s2 ds = 2e−t2
√π
Z t
0
e−s2 ds,
(4.1.21)
where s = t + ηi. The integral in Equation 4.1.21 is the well-known Dawson’s integral.3 See
Gautschi and Waldvogel4 for an alternative derivation.
⊓⊔
• Example 4.1.4: Numerical computation of the Hilbert transform
Recently Andr´e Weideman5 devised a particularly eﬃcient method for numerically
computing the Hilbert transform when x(t) is known exactly for any real t and enjoys the
property that
Z ∞
−∞
|x(t)|2 dt < ∞.
(4.1.22)
3 Press, W. H., S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, 1992: Numerical Recipes in
Fortran: The Art of Scientiﬁc Computing. Cambridge University Press, Section 6.10.
4 Gautschi, W., and J. Waldvogel, 2000: Computing the Hilbert transform of the generalized Laguerre
and Hermite weight functions. BIT, 41, 490–503.
5 Weideman, J. A. C., 1995: Computing the Hilbert transform on the real line. Math. Comput., 64,
745–762.

The Hilbert Transform
201
Given Equation 4.1.22, the function x(t) can be represented by the rational expansion
x(t) =
∞
X
n=−∞
anρn(t),
(4.1.23)
where ρn(t) is the set of rational functions
ρn(t) =
(1 + it)n
(1 −it)n+1 ,
n = 0, ±1, ±2, · · · ,
(4.1.24)
and
an = 1
π
Z ∞
−∞
x(t)ρ∗
n(t) dt
(4.1.25)
or
an = 1
2π
Z π
−π

1 −i tan
  1
2θ

x

tan
  1
2θ

e−inθ dθ,
(4.1.26)
if we introduce the substitution t = tan(θ/2).
Why is Equation 4.1.23 useful? Taking the Hilbert transform of both sides of this
equation,
bx(t) =
∞
X
n=−∞
anbρn(t).
(4.1.27)
Using contour integration, we ﬁnd that
bρn(t) = 1
π PV
Z ∞
−∞
(1 + iτ)n
(1 −iτ)n+1(t −τ) dτ = −i sgn(n)ρn(t),
(4.1.28)
where sgn(t) is the signum function with sgn(0) = 1. Therefore,
bx(t) = −i
∞
X
n=−∞
sgn(n) an ρn(t).
(4.1.29)
We must now approximate Equation 4.1.29 so that we can evaluate it numerically. We
do this by introducing the following truncated version:
bxN(t) = −i
N−1
X
n=−N
sgn(n) An ρn(t).
(4.1.30)
This particular truncation was chosen because ρn(t) and ρ−n−1(t) are a conjugate pair. The
coeﬃcient an has become An, which equals
An = 1
N
N−1
X
j=−N+1

1 −i tan
  1
2θj

x

tan
  1
2θj

e−inθj,
(4.1.31)
where θj = πj/N. The terms corresponding to j = ±N have been set to zero because it
is assumed that x(t) vanishes rapidly with t →±∞. Finally, we substitute θ for t and
transform Equation 4.1.30 into
bxN(tj) = −
i
1 −i tan(θj)
N−1
X
n=−N
sgn(n)Aneinθj.
(4.1.32)

202
Advanced Engineering Mathematics: A Second Course
−15
−10
−5
0
5
10
15
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
t
exact Hilbert transform
computed Hilbert transform
Figure 4.1.1: The Hilbert transform for x(t) = 1/(1 + t4) computed from Weideman’s algorithm.
The advantage of Equation 4.1.31 and Equation 4.1.32 is that they can be evaluated
using fast Fourier transforms. For example, the following MATLAB script devised by Wei-
deman illustrates his methods for x(t) = 1/(1 + t4):
% initialize parameters used in computation
b = 1; N = 8; n = [-N:N-1]’;
% set up collocation points and evaluate function there
t = b * tan(pi*(n+1/2)/(2*N)); F = 1./(1+t.^4);
% evaluate Equation 4.1.31
an = fftshift(fft(F.*(b-i*t)));
% compute Hilbert transform via Equation 4.1.32
hilbert = ifft(fftshift(i*(sign(n+1/2).*an)))./(b-i*t);
hilbert = -real(hilbert);
% find points at which we will compute exact answer
tt = [-12:0.02:12];
% compute exact answer
answer = tt.*(1+tt.^2)./(1+tt.^4)./sqrt(2);
fzero = zeros(size(tt));
% plot both computed Hilbert transform and exact answer
plot(tt,answer,’-’,t,hilbert,’o’,tt,fzero,’--’)
xlabel(’t’,’Fontsize’,20)
legend(’exact Hilbert transform’,’computed Hilbert transform’)
legend boxoff
Figure 4.1.1 illustrates Weideman’s algorithm for numerically computing the Hilbert
transform of 1/(1 + t4).
There are two important points concerning Weideman’s implementation of his algo-
rithm. First, the collocation points originally given by tj = tan[πj/(2N)], j = −N, . . . , N−1
have changed to tj = tan[(j + 1
2)π/(2N)], j = −N, . . . , N −1. This change replaces the
trapezoidal rule discretization for the Fourier coeﬃcients with a midpoint rule. The advan-
tages are twofold: First, it avoids the nuisance of dealing with a collocation point at inﬁnity.
Second, it actually yields more accurate results in many cases.

The Hilbert Transform
203
The discerning student will also notice that Weideman introduced a free parameter b,
which we set to one. This rescaling parameter can have a major inﬂuence on the accuracy.
The interested student is referred to the bottom of page 756 in Weideman’s paper for further
details.
⊓⊔
• Example 4.1.5: Discrete Hilbert transform
Quite often the function is given as discrete data points. How do we ﬁnd the Hilbert
transform in this case? We will now prove6 that the equivalent discrete Hilbert transform
is
H(fn) = bfk =









2
π
X
n odd
fn
k −n,
k even,
2
π
X
n even
fn
k −n,
k odd,
(4.1.33)
where fn denotes a set of discrete data values that are sampled at t = nT and both k and
n run from −∞to ∞. The corresponding inverse is
fn =











2
π
X
k odd
bfk
k −n,
n even,
2
π
X
k even
bfk
k −n,
n odd.
(4.1.34)
We begin our proof by inserting Equation 4.1.33 into Equation 4.1.34. For n even,
fn = 2
π
X
k odd
1
k −n
 
2
π
X
p even
fp
k −p
!
= 4
π2
X
p even
X
k odd
fp
(k −p)(k −n)
(4.1.35)
= 4
π2
X
k odd
fn
(k −n)2 + 4
π2
X
p even,p̸=n
X
k odd
(n −p)fp

1
k −n −
1
k −p

.
(4.1.36)
The term within the curly brackets equals zero as k runs through all of its values. Therefore,
Equation 4.1.36 reduces to
fn = 8
π2 fn

1 + 1
32 + 1
52 + + 1
72 + · · ·

.
(4.1.37)
However, the term in the brackets of Equation 4.1.37 equals π2/8. Therefore, Equation
4.1.33 and Equation 4.1.34 is proved for n even. An identical proof follows for n odd.
A popular alternative7 to Equation 4.1.33 involves the (fast) Fourier transform and
the relationship that b
X(ω) = −i sgn(ω)X(ω), where X(ω) and b
X(ω) denote the Fourier
transform of x(t) and bx(t), respectively. In this technique, a fast Fourier transform is taken
of the data.
This transformed dataset is then multiplied by −i sgn(ω) and then back
transformed to give the Hilbert transform.
6 See Kak, S. C., 1970: The discrete Hilbert transform. Proc. IEEE, 58, 585–586. For an alternative
derivation, see Kress, R., and E. Martensen, 1970: Anwendung der Rechteckregel auf die reelle Hilbert-
transformation mit unendlichem Intervall. Z. Angew. Math. Mech., 50, T61–T64.
7 ˇC´ıˇzek, V., 1970: Discrete Hilbert transform. IEEE Trans. Audio Electroacoust., AU-18, 340–343.

204
Advanced Engineering Mathematics: A Second Course
Let x(t) be a real, even function. Then X(ω), the Fourier transform of x(t), is also an
even function. Consequently,
bx(t) = 1
2π
Z ∞
−∞
b
X(ω)eiωt dω = 1
2π
Z ∞
−∞
−i sgn(ω)X(ω) [cos(ωt) + i sin(ωt)] dω
(4.1.38)
= −i
2π
Z ∞
−∞
sgn(ω)X(ω) cos(ωt) dω + 1
2π
Z ∞
−∞
sgn(ω)X(ω) sin(ωt) dω
(4.1.39)
= 1
π
Z ∞
0
X(ω) sin(ωt) dω.
(4.1.40)
Note that the Hilbert transform in this case is an odd function. Similarly, if x(t) is a real,
odd function,
bx(t) = −i
π
Z ∞
0
X(ω) cos(ωt) dω,
(4.1.41)
and the Hilbert transform is an even function.
Problems
1. Show that the Hilbert transform of a constant function is zero.
2. Use Equation 4.1.4 to compute the Hilbert transform of cos(ωt), ω ̸= 0.
3. Use Equation 4.1.4 to show that the Hilbert transform of the Dirac delta function δ(t)
is 1/(πt).
4. Use Equation 4.1.4 to show that the Hilbert transform of 1/(t2 + 1) is t/(t2 + 1).
5. The output y(t) from an ideal lowpass ﬁlter can be expressed by the convolution integral
y(t) = x(t) ∗sin(2πωt)
πt
,
where x(t) is the input signal. Show that this expression can also be expressed in terms of
Hilbert transforms as
y(t) = H[x(t) cos(2πωt)] sin(2πωt) −H[x(t) sin(2πωt)] cos(2πωt).
Following Example 4.1.3, ﬁnd the Hilbert transforms of
6. x(t) =
1
1 + t2
7. x(t) =
 1,
−a < t < a
0,
otherwise
8. Using the commutative and associate properties of convolution, f(t) ∗g(t) = g(t) ∗f(t)
and [f(t) ∗g(t)] ∗v(t) = f(t) ∗[g(t) ∗v(t)], respectively, and the deﬁnition of the Hilbert
transform, Equation 4.1.4, show8 that
H[f(t) ∗g(t)] = bf(t) ∗g(t) = f(t) ∗bg(t).
8 For an application, see Sakai, H., and G. A. Vanasse, 1966: Hilbert transform in Fourier spectroscopy.
J. Opt. Soc. Am., 56, 131–132.

The Hilbert Transform
205
Using MATLAB, test Weideman’s algorithm for the following cases. Why does the algorithm
do well or not?
9.
 1,
−1 < t < 1
0,
otherwise
10. sin(t)
11.
1
t2 + 1
12. sin(t)
1 + t4
For Problem 12, you will need
H
 sin(t)
t4 + 1

= e−1/
√
2[cos(1/
√
2 ) + sin(1/
√
2 )t2] −cos(t)
t4 + 1
.
4.2 SOME USEFUL PROPERTIES
In principle, we could construct any desired transform from the deﬁnition of the Hilbert
transform. However, there are several general theorems that are much more eﬀective in
ﬁnding new transforms.
Linearity
From the deﬁnition of the Hilbert transform, it immediately follows that if z(t) =
c1x(t) + c2y(t), where c1 and c2 are arbitrary constants, then bz(t) = c1bx(t) + c2by(t).
The energy in a signal and its Hilbert
transform are the same.
Consider the energy spectral densities at input and output of a quadrature phase shifter.
The output equals
| b
X(ω)|2 =
F[bx(t)]
2 = | −i sgn(ω)|2|X(ω)|2 = |X(ω)|2.
(4.2.1)
Because the energy spectral density at input and output are the same, so are the total
energies.
A signal and its Hilbert transform
are orthogonal.
From Parseval’s theorem,
Z ∞
−∞
x(t)bx(t) dt =
Z ∞
−∞
X(ω) b
X∗(ω) dω,
(4.2.2)
where b
X(ω) = F[bx(t)]. Then,
Z ∞
−∞
X(ω) b
X∗(ω) dω =
Z ∞
−∞
i sgn(ω)|X(ω)|2 dω = 0,
(4.2.3)

206
Advanced Engineering Mathematics: A Second Course
because the integrand in the middle expression of Equation 4.2.3 is odd. Thus,
Z ∞
−∞
x(t)bx(t) dt = 0.
(4.2.4)
The reason why a function and its Hilbert transform are orthogonal to each other follows
from the fact that a Hilbert transformation of a function shifts the phase of each Fourier
component of the function forward by π/2 for positive frequencies and backward for negative
frequencies.
• Example 4.2.1
Let us verify the orthogonality condition for Hilbert transforms using x(t) = 1/(1+t2).
Because bx(t) = t/(1 + t2),
Z ∞
−∞
x(t)bx(t) dt =
Z ∞
−∞
t
(1 + t2)2 dt = 0,
(4.2.5)
since the integrand is an odd function.
⊓⊔
Shifting
Let us ﬁnd the Hilbert transform of x(t + a) if we know bx(t). From the deﬁnition of
Hilbert transforms,
H[x(t + a)] = 1
π
Z ∞
−∞
x(η + a)
t −η
dη = 1
π
Z ∞
−∞
x(τ)
(t + a) −τ dτ = bx(t + a)
(4.2.6)
or H[x(t + a)] = bx(t + a).
Time scaling
Let a > 0. Then,
H[x(at)] = 1
π
Z ∞
−∞
x(aη)
t −η dη = 1
π
Z ∞
−∞
x(τ)
at −τ dτ = bx(at).
(4.2.7)
On the other hand, if a < 0,
H[x(at)] = 1
π
Z ∞
−∞
x(aη)
t −η dη = −1
π
Z ∞
−∞
x(τ)
at −τ dτ = −bx(at).
(4.2.8)
Thus, we have that H[x(at)] = sgn(a) bx(at).

The Hilbert Transform
207
Some General Properties of Hilbert Transforms
function, x(t)
Hilbert transform, bx(t)
1.
bx(t)
−x(t)
2.
x(t) + y(t)
bx(t) + by(t)
3.
x(t + a),
a real
bx(t + a)
4.
dnx(t)
dtn
dnbx(t)
dtn
5.
x(at)
sgn(a) bx(at)
6.
tx(t)
tbx(t) + 1
π
R ∞
−∞x(τ) dτ
7.
(t + a)x(t)
(t + a)bx(t) + 1
π
R ∞
−∞x(τ) dτ
Derivatives
Let us ﬁnd the relationship between the nth derivative of x(t) and its Hilbert transform.
Using the derivative rule as it applies to Fourier transforms,
H

F
dnx
dtn

= −i sgn(ω)(iω)nX(ω) = (iω)n[−i sgn(ω)X(ω)] = (iω)n b
X(ω) = F
dnbx
dtn

.
(4.2.9)
Taking the inverse Fourier transforms, we have that
H
dnx
dtn

= dnbx
dtn .
(4.2.10)
Convolution
Hilbert transforms enjoy a similar, but not identical, property with Fourier transforms
with respect to convolution. If
w(t) = u(t) ∗v(t) =
Z ∞
−∞
u(τ)v(t −τ) dτ =
Z ∞
−∞
u(t −τ)v(τ) dτ,
(4.2.11)
then
bw(t) = v(t) ∗bu(t).
(4.2.12)
Proof : From the convolution theorem for Fourier transforms, W(ω) = V (ω)U(ω). Multi-
plying both sides of the equation by −i sgn(ω),
c
W(ω) = −i sgn(ω)W(ω) = V (ω)[−i sgn(ω)U(ω)] = V (ω)bU(ω).
(4.2.13)

208
Advanced Engineering Mathematics: A Second Course
Again, using the convolution theorem as it applies to Fourier transforms, we arrive at the
ﬁnal result.
⊓⊔
• Example 4.2.2
Given the functions u(t) = cos(t) and v(t) = 1/(1 + t4), let us verify the convolution
theorem as it applies to Hilbert transforms.
With u(t) = cos(t) and v(t) = 1/(1 + t4),
w(t) = u(t) ∗v(t) =
Z ∞
−∞
cos(t −x)
1 + x4
dx
(4.2.14)
=
Z ∞
−∞
cos(t) cos(x)
1 + x4
dx +
Z ∞
−∞
sin(t) sin(x)
1 + x4
dx
(4.2.15)
= π
√
2e−1/
√
2

cos
 1
√
2

+ sin
 1
√
2

cos(t)
(4.2.16)
so that
bw(t) = π
√
2e−1/
√
2

cos
 1
√
2

+ sin
 1
√
2

sin(t).
(4.2.17)
Because bv(t) = t(1 + t2)/[
√
2 (1 + t4)],
u(t) ∗bv(t) =
1
√
2
Z ∞
−∞
cos(t −x)x(1 + x2)
1 + x4
dx
(4.2.18)
=
1
√
2
Z ∞
−∞
cos(t) cos(x)x(1 + x2)
1 + x4
dx + 1
√
2
Z ∞
−∞
sin(t) sin(x)x(1 + x2)
1 + x4
dx
(4.2.19)
=
1
√
2 sin(t)
Z ∞
−∞
x(1 + x2) sin(x)
1 + x4
dx
(4.2.20)
= π
√
2e−1/
√
2

cos
 1
√
2

+ sin
 1
√
2

sin(t),
(4.2.21)
and the convolution theorem for Hilbert transforms holds true in this case.
⊓⊔
Product theorem
Let f(t) and g(t) denote complex functions with Fourier transforms F(ω) and G(ω),
respectively. If
1) F(ω) vanishes for |ω| > a, and G(ω) vanishes for |ω| < a, where a > 0,
or
2) f(t) and g(t) are analytic functions (their real and imaginary parts are Hilbert pairs),
then the Hilbert transform of the product of f(t) and g(t) is
H[f(t)g(t)] = f(t)bg(t).
(4.2.22)

The Hilbert Transform
209
F(u) = 0, u<0
(b)
(a) 
G(u) = 0, |v| < a
G(u) = 0, v<0
v
v
u+v = 0
u+v = 0
F(u) = 0, |u| > a
u
u
a
-a
a
-a
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      




























                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    





























Figure 4.2.1: Region of integration in the proof of the product theorem.
Proof :9 The product f(t)g(t) can be expressed as
f(t)g(t) =
1
4π2
Z ∞
−∞
Z ∞
−∞
F(u)G(v)ei(u+v)t dv du.
(4.2.23)
Because H(eibt) = i sgn(b)eibt,
H[f(t)g(t)] =
i
4π2
Z ∞
−∞
Z ∞
−∞
F(u)G(v) sgn(u + v)ei(u+v)t dv du.
(4.2.24)
The shaded regions of Figure 4.2.1 are those in which the product F(u)G(v) is nonvanishing
for the conditions of the theorem. In Figure 4.2.1(a) the nonoverlapping Fourier transforms
yield two semi-inﬁnite strips in which the product is nonvanishing.
In Figure 4.2.1(b),
for analytic functions, the Fourier transforms vanish for negative arguments10 so that the
product is nonvanishing only in the ﬁrst quadrant. In both cases sgn(u + v) = sgn(v) over
the regions of integration in which the integrand is nonvanishing. Thus,
H[f(t)g(t)] =
i
4π2
Z ∞
−∞
Z ∞
−∞
F(u)G(v) sgn(v)ei(u+v)t dv du
(4.2.25)
= f(t) i
2π
Z ∞
−∞
G(v) sgn(v)eivt dv = f(t)bg(t).
(4.2.26)
⊓⊔
• Example 4.2.3: Hilbert transforms of band-pass functions
In communications, we have the double-sideband, amplitude-modulated signal given
by a(t) cos(ωt + ϕ), where ϕ is constant. From the product theorem, its Hilbert transform
equals a(t) sin(ωt + ϕ), ω > 0, provided that the highest frequency component in a(t) is
less than ω. Paradoxically, the Hilbert transform of more general a(t) cos[ωt + ϕ(t)], which
equals a(t) sin[ωt + ϕ(t)], has no such restriction.
9 See Bedrosian, E., 1963: A product theorem for Hilbert transforms. Proc. IEEE, 51, 868–869. This
theorem has been extended to functions of n-dimensional real vectors by Stark, H., 1971: An extension of
the Hilbert transform product theorem. Proc. IEEE, 59, 1359–1360.
10 Titchmarsh, E. C., 1948: Introduction to the Theory of Fourier Integrals. Oxford University Press,
p. 128.

210
Advanced Engineering Mathematics: A Second Course
Problems
Verify the orthogonality property of Hilbert transforms using
1. x(t) = 1/(1 + t4)
2. x(t) = sin(t)/(1 + t2)
3. x(t) =
 1,
0 < t < a
0,
otherwise
Verify the convolution theorem for Hilbert transforms using
4. u(t) =
 1,
0 < t < a,
0,
otherwise,
v(t) = sin(t)
5. u(t) = cos(t),
v(t) =
1
1 + t2
6. Use the product theorem to show that
H[sin(at)Jn(bt)] = −cos(at)Jn(bt),
0 < b < a,
if n = 0, 1, 2, 3, . . . .
Hint:
F[Jn(bt)] = 2(−1)m
√
b2 −ω2 Tn
|ω|
b

H(b −|ω|),
where Tn(·) is a Chebyshev polynomial of the ﬁrst kind and m = n/2 or (n−1)/2, depending
upon which deﬁnition gives an integer.
7. Given cosine and sine integrals:
Ci(x) = −
Z ∞
x
cos(t)
t
dt,
Si(x) = −
Z ∞
x
sin(t)
t
dt,
and
H[Ci(a|t|)] = −sgn(t)Si(a|t|),
0 < a,
use the product rule to show that
H[sin(bt)Ci(a|t|)] = −sgn(t) sin(bt)Si(a|t|),
0 < b < a.
Hint:
F[Ci(a|t|)] =

0,
0 < |ω| < a,
−π/|ω|,
a < |ω| < ∞,
0 < a.
8. Prove that
H[tx(t)] = tbx(t) −1
π
Z ∞
−∞
x(τ) dτ.
Hint:
τx(τ)
t −τ = tx(τ)
t −τ −x(τ).

The Hilbert Transform
211
4.3 ANALYTIC SIGNALS
The monochromatic signal A cos(ω0t + ϕ) appears in many physical and engineering
applications. It is common to represent this signal by the complex representation Aei(ω0t+ϕ).
These two representations are related to each other by
A cos(ω0t + ϕ) = ℜ
h
Aei(ω0t+ϕ)i
= 1
2

Aei(ω0t+ϕ) + Ae−i(ω0t+ϕ)
.
(4.3.1)
Furthermore, the Fourier transform of A cos(ω0t + ϕ) is
F[A cos(ω0t + ϕ)] = 1
2

Aeiϕδ(ω −ω0) + Ae−iϕδ(ω + ω0)

,
(4.3.2)
while the Fourier transform of Aei(ω0t+ϕ) is
F
h
Aei(ω0t+ϕ)i
= Aeiϕδ(ω −ω0).
(4.3.3)
As Equation 4.3.2 and Equation 4.3.3 clearly show, in passing from the real signal to its
complex representation, we double the strength of the positive frequencies and remove
entirely the negative frequencies.
Let us generalize these concepts to nonmonochromatic signals. For the real signal x(t)
with Fourier transform X(ω) and the complex signal z(t) with Fourier transform Z(ω), the
previous paragraph shows that our generalization must have the property:
Z(ω) = X(ω) + sgn(ω)X(ω)
(4.3.4)
or
Z(ω) =



2X(ω),
ω > 0,
X(ω),
ω = 0,
0,
ω < 0.
(4.3.5)
Taking the inverse of Equation 4.3.4, we have the deﬁnition of an analytic signal as
z(t) = x(t) + ibx(t),
(4.3.6)
where x(t) is a real signal and bx(t) is its Hilbert transform.
• Example 4.3.1
In Figure 4.3.1 the amplitude spectrum of the analytic signal is graphed when x(t) is
the rectangular pulse,
x(t) =

1,
|t| < a,
0,
|t| > a.
Note that the amplitude spectrum equals zero for ω < 0 and twice the amplitude spectrum
for ω > 0.
⊓⊔
• Example 4.3.2
Let us ﬁnd the energy of an analytic signal.
The energy of an analytic signal is
Z ∞
−∞
|z(t)|2 dt =
Z ∞
−∞
x2(t) dt +
Z ∞
−∞
bx2(t) dt = 2
Z ∞
−∞
x2(t) dt = 2
Z ∞
−∞
|X(ω)|2 dω (4.3.7)

212
Advanced Engineering Mathematics: A Second Course
−20.0
−10.0
0.0
10.0
20.0
 a
−1.0
0.0
1.0
2.0
3.0
4.0
phase (radians)
−1.0
0.0
1.0
2.0
3.0
4.0
5.0
amplitude / a
ω
Figure 4.3.1: The spectrum of the analytic signal when x(t) is the rectangular pulse given in Example
4.3.1.
(b)
(d)
(a)
(c)
X(  )
ω
ω
ω
−ω
ω
−ω
Z(  )
0
0
ω
0
0
ω
ω
−ω
max
max
ωmax
ω
ω
Figure 4.3.2: Given a function x(t) with an amplitude spectrum shown in (a), frame (b) shows the
amplitude spectrum of the amplitude-modulated signal x(t) cos(ω0t) while frames (c) and (d) give the
amplitude spectrum of the analytic signal z(t) and x(t) cos(ω0t) −bx(t) sin(ω0t), respectively.
by Parseval’s theorem. Thus, the analytic signal has twice the energy of the corresponding
real signal.
⊓⊔
Consider the function x(t) whose amplitude spectrum is shown in Figure 4.3.2(a). If
we were to amplitude modulate x(t) with cos(ω0t), then the amplitude spectrum of this
modulated signal would appear as pictured in Figure 4.3.2(b).
Consider now the signal
y(t) = x(t) cos(ω0t) −bx(t) sin(ω0t) = ℜ

[x(t) + ibx(t)]eiω0t	
(4.3.8)
= ℜ

z(t)eiω0t	
= 1
2

z(t)eiω0t + z∗(t)e−iω0t
,
(4.3.9)
where z(t) is the analytic signal of x(t). We have plotted the amplitude spectrum |Z(ω)| in
Figure 4.3.2(c). If we computed the amplitude spectrum of y(t), we would ﬁnd that
Y (ω) = 1
2Z(ω −ω0) + 1
2Z(−ω −ω0)
(4.3.10)

The Hilbert Transform
213
Y (ω) =



X(ω −ω0),
ω0 ≤ω ≤ω0 + ωmax,
X∗(−ω −ω0),
−ω0 −ωmax ≤ω ≤ω0,
0,
otherwise.
(4.3.11)
We have sketched this amplitude spectrum |Y (ω)| in Figure 4.3.2(d). Each triangular part
is called the single sideband signal because it contains the upper frequencies (|ω| > ω0) of
the modulated signal x(t) cos(ω0t). Similarly, if we had used x(t) cos(ω0t) + bx(t) sin(ω0t),
we would only have obtained the lower sidebands. Consequently, a communication system
using x(t) cos(ω0t)−bx(t) sin(ω0t) or x(t) cos(ω0t)+bx(t) sin(ω0t) would realize a 50% savings
in its frequency bandwidth over one transmitting x(t) cos(ω0t).
Problems
1. Find the analytic signal corresponding to x(t) = cos(ωt), ω > 0.
2. Show that the polar form of an analytic signal can be written
z(t) = |z(t)|eiϕ(t),
where
|z(t)|2 = x2(t) + bx2(t),
ϕ(t) = tan−1
 bx(t)
x(t)

.
3. Analytic signals are often used with narrow-band waveforms with carrier frequency ω0.
If ϕ(t) = ω0t + ϕ′(t), show that the analytic signal can be written z(t) = r(t)eiω0t, where
r(t) = |z(t)|eiϕ′(t). The function r(t) is called the complex envelope or the phasor amplitude;
this is a generalization of the phasor idea beyond pure alternating currents.
4.4 CAUSALITY: THE KRAMERS-KRONIG RELATIONSHIP
Causality is the physical principle which states that an event cannot proceed its cause.
In this section we explore what eﬀect this principle has on Hilbert transforms.
We begin by introducing the concept of causal functions. A causal function is a function
that equals zero for all t < 0. As with all functions we can write it in terms of an even xe(t)
and an odd xo(t) part as x(t) = xe(t) + xo(t). Because x(t) is causal, xo(t) = sgn(t)xe(t)
and
x(t) = xe(t) + sgn(t)xe(t).
(4.4.1)
Taking the Fourier transform of Equation 4.4.1, we ﬁnd that the Fourier transform of all
causal functions are of the form
X(ω) = Xe(ω) −i b
Xe(ω),
(4.4.2)
where
b
Xe(ω) = 1
π
Z ∞
−∞
Xe(τ)
ω −τ dτ,
and
Xe(ω) = −1
π
Z ∞
−∞
b
Xe(τ)
ω −τ dτ,
(4.4.3)
because
2πF[xe(t)sgn(t)] = 2
iω ∗Xe(ω) = 2
i
Z ∞
−∞
Xe(τ)
ω −τ dτ.
(4.4.4)

214
Advanced Engineering Mathematics: A Second Course
Equation 4.4.3 ﬁrst arose in dielectric theory and, taken together, are called the Kramers11
and Kronig12 relation after their discoverers, who derived these relationships during their
work on the dispersion of light by gaseous atoms or molecules.
• Example 4.4.1
Let us verify the Kramers-Kronig relation using the causal time function x(t) = H(t).
Because xe(t) = 1
2 and Xe(ω) = πδ(ω),
b
Xe(ω) = 1
π
Z ∞
−∞
π δ(τ)
ω −τ dτ = −1
ω .
(4.4.5)
Consequently, by the Kramers-Kronig relation,
F[H(t)] = Xe(ω) −i b
Xe(ω) = πδ(ω) + i
ω .
(4.4.6)
⊓⊔
• Example 4.4.2
A simple example of a causal function is the impulse response or Green’s function intro-
duced in earlier chapters. From Equation 4.4.2 we have the result that the transfer function
G(ω), the Fourier transform of the impulse response, must yield the Hilbert transform pair
Ge(ω) −i bGe(ω).
For example, if g(t) = e−tH(t), then G(ω) = 1/(1 + iω). Because
1
1 + iω =
1
ω2 + 1 −i
ω
ω2 + 1,
(4.4.7)
we have the Hilbert transform pair of
x(t) =
1
t2 + 1
and
bx(t) =
t
t2 + 1.
(4.4.8)
⊓⊔
• Example 4.4.3
Let us verify the Kramers-Kronig relation for the Hilbert transform pair
x(t) =
1
t4 + 1
and
bx(t) =
t(t2 + 1)
√
2(t4 + 1)
(4.4.9)
by direct integration.
11 Kramers, H. A., 1929: Die Dispersion und Absorption von R¨ontgenstrahlen. Phys. Z., 30, 522–523.
12 Kronig, R. de L., 1926: On the theory of dispersion of x-rays. J. Opt. Soc. Am., 12, 547–551.

The Hilbert Transform
215
From Equation 4.4.3, we have that
ω(ω2 + 1)
√
2(ω4 + 1) = 1
π
Z ∞
−∞
dτ
(τ 4 + 1)(ω −τ).
(4.4.10)
Applying the residue theorem to the right side of Equation 4.4.10, we obtain
ω(ω2 + 1)
√
2(ω4 + 1) = i Res

1
(z4 + 1)(ω −z); ω

+ 2i Res

1
(z4 + 1)(ω −z); eπi/4

+ 2i Res

1
(z4 + 1)(ω −z); e3πi/4

.
(4.4.11)
We only include one half of the value of the residue at τ = ω because the singularity lies
on the path of integration and we must treat this integration along the lines of a Cauchy
principal value. Evaluating the residues, we ﬁnd
Res

1
(z4 + 1)(ω −z); ω

= −
1
ω4 + 1,
(4.4.12)
Res

1
(z4 + 1)(ω −z); eπi/4

=
√
2 −(1 + i)ω
4
√
2

ω −
1
√
2
2
+ 1
2
,
(4.4.13)
and
Res

1
(z4 + 1)(ω −z); e3πi/4

=
√
2 + (1 −i)ω
4
√
2

ω +
1
√
2
2
+ 1
2
.
(4.4.14)
Substituting Equation 4.4.12 through Equation 4.4.14 into the right side of Equation 4.4.11,
we obtain the left side.
Problems
1. For a causal function x(t), prove that xo(t) = sgn(t)xe(t) and xe(t) = sgn(t)xo(t).
2. Redo our analysis if x(t) is a negative time function, i.e., x(t) = 0 if t > 0. Verify your
result using x(t) = etH(−t).
3. Using g(t) = te−tH(t), ﬁnd the corresponding Hilbert transform pairs.
4. Using g(t) = e−t cos(ωt)H(t), ﬁnd the corresponding Hilbert transform pairs.
5. Verify the Kramers-Kronig relation for the Hilbert transform pair:
x(t) =
1
t2 + 1
and
bx(t) =
t
t2 + 1
by direct integration.
Further Reading
Hahn, S. L., 1996: Hilbert Transforms in Signal Processing. Artech House, 442 pp. Covers
the basic theory and gives some practical applications.


Chapter 5
Green’s Functions
An important aspect of engineering mathematics is the solution of linear ordinary and
partial diﬀerential equations. As an undergraduate you were probably introduced to the
method of separation of variables, which leads to a solution in terms of an eigenfunction
expansion. However, this method is not the only one; there is Duhamel’s principle which
uses the superposition integral. Here we expand upon this idea and illustrate how a solution,
called a Green’s function, to a diﬀerential equation forced by the Dirac delta function can
be used in an integral representation of a solution when the forcing is arbitrary.
5.1 WHAT IS A GREEN’S FUNCTION?
The following examples taken from engineering show how Green’s functions naturally
appear during the solution of initial-value and boundary-value problems. We also show that
the solution u(x) can be expressed as an integral involving the Green’s function and f(x).
Circuit theory
In electrical engineering, one of the simplest electrical devices consists of a voltage
source v(t) connected to a resistor with resistance R and an inductor with inductance L.
See Figure 5.1.1. Denoting the current by i(t), the equation that governs this circuit is
Ldi
dt + Ri = v(t).
(5.1.1)
Consider now the following experiment: With the circuit initially dead, we allow the
voltage to suddenly become V0/∆τ during a very short duration ∆τ starting at t = τ.
217

218
Advanced Engineering Mathematics: A Second Course
v(t)
-
+
L
i(t)
R
Figure 5.1.1: The RL electrical circuit driven by the voltage v(t).
Then, at t = τ + ∆τ, we again turn oﬀthe voltage supply. Mathematically, for t > τ + ∆τ,
the circuit’s performance obeys the homogeneous diﬀerential equation:
Ldi
dt + Ri = 0,
t > τ + ∆τ,
(5.1.2)
whose solution is
i(t) = I0e−Rt/L,
t > τ + ∆τ,
(5.1.3)
where I0 is a constant and L/R is the time constant of the circuit. Because the voltage v(t)
during τ < t < τ + ∆τ is V0/∆τ, then
Z τ+∆τ
τ
v(t) dt = V0.
(5.1.4)
Therefore, over the interval τ < t < τ + ∆τ, Equation 5.1.1 can be integrated to yield
L
Z τ+∆τ
τ
di + R
Z τ+∆τ
τ
i(t) dt =
Z τ+∆τ
τ
v(t) dt,
(5.1.5)
or
L [i(τ + ∆τ) −i(τ)] + R
Z τ+∆τ
τ
i(t) dt = V0.
(5.1.6)
If i(t) remains continuous as ∆τ becomes small, then
R
Z τ+∆τ
τ
i(t) dt ≈0.
(5.1.7)
Finally, because
i(τ) = 0
and
i(τ + ∆τ) = I0e−R(τ+∆τ)/L ≈I0e−Rτ/L,
(5.1.8)
for small ∆τ, Equation 5.1.6 reduces to
LI0e−Rτ/L = V0,
or
I0 = V0
L eRτ/L.
(5.1.9)
Therefore, Equation 5.1.3 can be written as
i(t) =
(
0,
t < τ,
V0e−R(t−τ)/L/L,
τ ≤t,
(5.1.10)

Green’s Functions
219
t
τ+∆τ
τ
V  /L
0
i(t)
Figure 5.1.2: The current i(t) within an RL circuit when the voltage V0/∆τ is introduced between the
times τ < t < τ + ∆τ.
after using Equation 5.1.9. Equation 5.1.10 is plotted in Figure 5.1.2.
Consider now a new experiment with the same circuit where we subject the circuit
to N voltage impulses, each of duration ∆τ and amplitude Vi/∆τ with i = 0, 1, . . . , N,
occurring at t = τi. See Figure 5.1.3. The current response is then
i(t) =

































0,
t < τ0,
V0e−R(t−τ0)/L/L,
τ0 < t < τ1,
V0e−R(t−τ0)/L/L + V1e−R(t−τ1)/L/L,
τ1 < t < τ2,
...
...
N
X
i=0
Vie−R(t−τi)/L/L,
τN < t < τN+1.
(5.1.11)
Finally, consider our circuit subjected to a continuous voltage source v(t). Over each
successive interval dτ, the step change in voltage is v(τ) dτ. Consequently, from Equation
5.1.11 the response i(t) is now given by the superposition integral
i(t) =
Z t
τ
v(τ)
L e−R(t−τ)/L dτ,
or
i(t) =
Z t
τ
v(τ)g(t|τ) dτ,
(5.1.12)
where
g(t|τ) = e−R(t−τ)/L
L
,
τ < t.
(5.1.13)
Here we have assumed that i(t) = v(t) = 0 for t < τ. In Equation 5.1.13, g(t|τ) is called the
Green’s function. As this equation shows, given the Green’s function to Equation 5.1.1, the
response i(t) to any voltage source v(t) can be obtained by convolving the voltage source
with the Green’s function.
We now show that we could have found the Green’s function, Equation 5.1.13, by
solving Equation 5.1.1 subject to an impulse- or delta-forcing function. Mathematically,
this corresponds to solving the following initial-value problem:
Ldg
dt + Rg = δ(t −τ),
g(0|τ) = 0.
(5.1.14)

220
Advanced Engineering Mathematics: A Second Course
τ
τ
τ2
1
0
τN
t
i(t)
Figure 5.1.3: The current i(t) within an RL circuit when the voltage is changed at t = τ0, t = τ1, and so
forth.
Taking the Laplace transform of Equation 5.1.14, we ﬁnd that
G(s|τ) =
e−sτ
Ls + R,
or
g(t|τ) = e−R(t−τ)/L
L
H(t −τ),
(5.1.15)
where H(·) is the Heaviside step function. As our short derivation showed, the most direct
route to ﬁnding a Green’s function is solving the diﬀerential equation when its forcing
equals the impulse or delta function. This is the technique that we will use throughout this
chapter.
Statics
Consider a string of length L that is connected at both ends to supports and is subjected
to a load (external force per unit length) of f(x). We wish to ﬁnd the displacement u(x) of
the string. If the load f(x) acts downward (negative direction), the displacement u(x) of
the string is given by the diﬀerential equation:
T d2u
dx2 = f(x),
(5.1.16)
where T denotes the uniform tensile force of the string. Because the string is stationary at
both ends, the displacement u(x) satisﬁes the boundary conditions u(0) = u(L) = 0.
Instead of directly solving for the displacement u(x) of the string subject to the load
f(x), let us ﬁnd the displacement that results from a load δ(x−ξ) concentrated at the point
x = ξ. See Figure 5.1.4. For this load, the diﬀerential equation, Equation 5.1.16, becomes
T d2g
dx2 = δ(x −ξ),
(5.1.17)
subject to the boundary conditions g(0|ξ) = g(L|ξ) = 0.
In Equation 5.1.17, g(x|ξ) denotes the displacement of the string when it is subjected
to an impulse load at x = ξ. In line with our circuit theory example, it gives the Green’s
function for our statics problem. Once found, the displacement u(x) of the string subject to

Green’s Functions
221
ξ
f(
ξ )
ξ)
g(x
x
L
Figure 5.1.4: The response, commonly called a Green’s function, of a string ﬁxed at both ends to a point
load at x = ξ.
any arbitrary load f(x) can be found by convolving the load f(x) with the Green’s function
g(x|ξ) as we did earlier.
Let us now ﬁnd this Green’s function. At any point x ̸= ξ, Equation 5.1.17 reduces to
the homogeneous diﬀerential equation:
d2g
dx2 = 0,
(5.1.18)
which has the solution
g(x|ξ) =

ax + b,
0 ≤x < ξ,
cx + d,
ξ < x ≤L.
(5.1.19)
Applying the boundary conditions, Equation 5.1.19, we ﬁnd that
g(0|ξ) = a · 0 + b = b = 0,
and
g(L|ξ) = cL + d = 0,
or
d = −cL.
(5.1.20)
Therefore, we can rewrite Equation 5.1.19 as
g(x|ξ) =

ax,
0 ≤x < ξ,
c(x −L),
ξ < x ≤L,
(5.1.21)
where a and c are undetermined constants.
At x = ξ, the displacement u(x) of the string must be continuous; otherwise, the string
would be broken. Therefore, the Green’s function given by Equation 5.1.21 must also be
continuous there. Thus,
aξ = c(ξ −L),
or
c =
aξ
ξ −L.
(5.1.22)
From Equation 5.1.13 the second derivative of g(x|ξ) must equal the impulse function.
Therefore, the ﬁrst derivative of g(x|ξ), obtained by integrating this equation, must be
discontinuous by the amount 1/T or
lim
ǫ→0
dg(ξ + ǫ|ξ)
dx
−dg(ξ −ǫ|ξ)
dx

= 1
T ,
(5.1.23)
in which case
dg(ξ+|ξ)
dx
−dg(ξ−|ξ)
dx
= 1
T ,
(5.1.24)

222
Advanced Engineering Mathematics: A Second Course
where ξ+ and ξ−denote points lying just above or below ξ, respectively. Using Equation
5.1.24, we ﬁnd that
dg(ξ−|ξ)
dx
= a,
and
dg(ξ+|ξ)
dx
= c =
aξ
ξ −L.
(5.1.25)
Thus, Equation 5.1.25 leads to
aξ
ξ −L −a = 1
T =
aL
ξ −L,
or
a = ξ −L
LT ,
(5.1.26)
and the Green’s function is
g(x|ξ) =
1
TL(x> −L)x<,
(5.1.27)
where x< = min(x, ξ) and x> = max(x, ξ). To ﬁnd the displacement u(x) subject to the
load f(x), we proceed as we did in the previous example. The result of this analysis is
u(x) =
Z L
0
f(ξ)g(x|ξ) dξ = x −L
TL
Z x
0
f(ξ) ξ dξ + x
TL
Z L
x
f(ξ) (ξ −L) dξ,
(5.1.28)
since ξ < x in the ﬁrst integral and x < ξ in the second integral of Equation 5.1.28.
Integral Equations
Consider the Sturm-Liouville problem
y′′ + λy = 0,
y(0) = y(L) = 0.
(5.1.29)
From its general theory, nontrivial solutions exist only if
λn = n2π2
L2 ,
yn(x) = sin
nπx
L

,
(5.1.30)
where n = 1, 2, 3, . . ..
Consider now a new boundary-value problem:
d2y
dx2 = −f(x),
y(0) = y(L) = 0.
(5.1.31)
In the next section (Equation 5.2.76), we will show that we can write its solution by
y(x) =
Z L
0
f(ξ)g(x|ξ) dξ,
(5.1.32)
where the Green’s function g(x|ξ) is given by
d2g
dx2 = −δ(x −ξ),
g(0|ξ) = g(L|ξ) = 0,
or
g(x|ξ) = (L −x>)x</L,
(5.1.33)
where x> = max(x, ξ) and x< = min(x, ξ).

Green’s Functions
223
We can now use Equation 5.1.29 to rewrite Equation 5.1.31 as λy(ξ) = f(ξ). Multi-
plying this equation by g(x|ξ) and integrating from 0 to L, we ﬁnd that
Z L
0
f(ξ)g(x|ξ) dξ = λ
Z L
0
y(ξ)g(x|ξ) dξ,
(5.1.34)
or
y(x) −λ
Z L
0
y(ξ)g(x|ξ) dξ = 0.
(5.1.35)
Because of the equivalence of Equation 5.1.29 and Equation 5.1.35, the solutions to the
integral equation, Equation 5.1.35, are λn = n2π2/L2 with yn(x) = sin(nπx/L). Direct
substitution veriﬁes this result. Thus, we can use Green’s functions to construct integral
equations that have known solutions. Indeed, it was the use of Green’s functions to solve
Fredholm integral equations that drew the attention of mathematicians at the turn of the
twentieth century.1
5.2 ORDINARY DIFFERENTIAL EQUATIONS
Second-order diﬀerential equations are ubiquitous in engineering. In electrical engi-
neering, many electrical circuits are governed by second-order, linear ordinary diﬀerential
equations. In mechanical engineering they arise during the application of Newton’s second
law.
One of the drawbacks of solving ordinary diﬀerential equations with a forcing term is
its lack of generality. Each new forcing function requires a repetition of the entire process.
In this section we give some methods for ﬁnding the solution in a somewhat more gen-
eral manner for stationary systems where the forcing, not any initially stored energy (i.e.,
nonzero initial conditions), produces the total output. Unfortunately, the solution must be
written as an integral.
Consider the linear diﬀerential equation
y′′ + 2y′ + y = f(t),
(5.2.1)
subject to the initial conditions y(0) = y′(0) = 0. Solving this equation by Laplace trans-
forms, we can write the Laplace transform of y(t), Y (s), as the product of two Laplace
transforms:
Y (s) =
1
(s + 1)2 F(s).
(5.2.2)
One drawback in using Equation 5.2.2 is its dependence upon an unspeciﬁed Laplace trans-
form F(s). Is there a way to eliminate this dependence and yet retain the essence of the
solution?
One way of obtaining a quantity that is independent of the forcing is to consider the
ratio:
Y (s)
F(s) = G(s) =
1
(s + 1)2 .
(5.2.3)
This ratio is called the transfer function because we can transfer the input F(s) into the
output Y (s) by multiplying F(s) by G(s).
It depends only upon the properties of the
system.
1 See Section 36 in Kneser, A., 1911: Integralgleichungen und ihre Anwendungen in der mathematischen
Physik. Braunschweig, 293 pp.

224
Advanced Engineering Mathematics: A Second Course
Let us now consider a problem related to Equation 5.2.1, namely
g′′ + 2g′ + g = δ(t),
t > 0,
(5.2.4)
with g(0) = g′(0) = 0. Because the forcing equals the Dirac delta function, g(t) is called
the impulse response or Green’s function.2 Computing G(s),
G(s) =
1
(s + 1)2 .
(5.2.5)
From Equation 5.2.3 we see that G(s) is also the transfer function. Thus, an alternative
method for computing the transfer function is to subject the system to impulse forcing and
the Laplace transform of the response is the transfer function.
From Equation 5.2.3,
Y (s) = G(s)F(s),
(5.2.6)
or
y(t) = g(t) ∗f(t).
(5.2.7)
That is, the convolution of the impulse response with the particular forcing gives the re-
sponse of the system. Thus, we may describe a stationary system in one of two ways: (1)
in the transform domain we have the transfer function, and (2) in the time domain there is
the impulse response.
Despite the fundamental importance of the impulse response or Green’s function for a
given linear system, it is often quite diﬃcult to determine, especially experimentally, and a
more convenient practice is to deal with the response to the unit step H(t). This response
is called the indicial admittance or step response, which we shall denote by a(t). Because
L[H(t)] = 1/s, we can determine the transfer function from the indicial admittance because
L[a(t)] = G(s)L[H(t)] or sA(s) = G(s). Furthermore, because
L[g(t)] = G(s) = L[a(t)]
L[H(t)],
(5.2.8)
then
g(t) = da(t)
dt ,
(5.2.9)
since L[f ′(t)] = sF(s) −f(0+).
• Example 5.2.1
Let us ﬁnd the transfer function, impulse response, and step response for the system
y′′ −3y′ + 2y = f(t),
(5.2.10)
with y(0) = y′(0) = 0. To ﬁnd the impulse response, we solve
g′′ −3g′ + 2g = δ(t −τ),
(5.2.11)
2 For the origin of the Green’s function, see Farina, J. E. G., 1976: The work and signiﬁcance of George
Green, the miller mathematician, 1793–1841. Bull. Inst. Math. Appl., 12, 98–105.

Green’s Functions
225
with g(0) = g′(0) = 0. We have generalized the problem to an arbitrary forcing at t = τ
and now denote the Green’s function by g(t|τ). We have done this so that our discussion
will be consistent with the other sections in the chapter.
Taking the Laplace transform of Equation 5.2.11, we ﬁnd that
G(s|τ) =
e−sτ
s2 −3s + 2,
(5.2.12)
which is the transfer function for this system when τ = 0. The impulse response or Green’s
function equals the inverse of G(s|τ) or
g(t|τ) =
h
e2(t−τ) −et−τi
H(t −τ).
(5.2.13)
To ﬁnd the step response, we solve
a′′ −3a′ + 2a = H(t),
(5.2.14)
with a(0) = a′(0) = 0. Taking the Laplace transform of Equation 5.2.14,
A(s) =
1
s(s −1)(s −2),
(5.2.15)
and the indicial admittance is given by the inverse of Equation 5.2.15, or
a(t) = 1
2 + 1
2e2t −et.
(5.2.16)
Note that a′(t) = g(t|0).
⊓⊔
• Example 5.2.2
MATLAB’s control toolbox contains several routines for the numerical computation
of impulse and step responses if the transfer function can be written as the ratio of two
polynomials. To illustrate this capacity, let us redo the previous example where the transfer
function is given by Equation 5.2.12 with τ = 0. The transfer function is introduced by
loading in the polynomial in the numerator num and in the denominator den followed by
calling tf. The MATLAB script
clear
% load in coefficients of the numerator and denominator
%
of the transfer function
num = [0 0 1]; den = [1 -3 2];
% create the transfer function
sys = tf(num,den);
% find the step response, a
[a,t] = step(sys);
% plot the indicial admittance
subplot(2,1,1), plot(t, a, ’o’)
ylabel(’indicial response’,’Fontsize’,20)
% find the impulse response, g
[g,t] = impulse(sys);
% plot the impulse response

226
Advanced Engineering Mathematics: A Second Course
0
0.2
0.4
0.6
0.8
1
1.2
0
1
2
3
indicial response
0
0.2
0.4
0.6
0.8
1
1.2
0
2
4
6
8
impulse response
time
Figure 5.2.1: The impulse and step responses corresponding to the transfer function, Equation 5.2.12,
with τ = 0.
subplot(2,1,2), plot(t, g, ’o’)
ylabel(’impulse response’,’Fontsize’,20)
xlabel(’time’,’Fontsize’,20)
shows how the impulse and step responses are found. Both of them are shown in Figure
5.2.1.
⊓⊔
• Example 5.2.3
There is an old joke about a man who took his car into a garage because of a terrible
knocking sound. Upon his arrival the mechanic took one look at it and gave it a hefty
kick.3 Then, without a moment’s hesitation he opened the hood, bent over, and tightened
up a loose bolt. Turning to the owner, he said, “Your car is ﬁne. That’ll be $50.” The
owner felt that the charge was somewhat excessive, and demanded an itemized account.
The mechanic said, “The kicking of the car and tightening one bolt, cost you a buck. The
remaining $49 comes from knowing where to kick the car and ﬁnding the loose bolt.”
Although the moral of the story may be about expertise as a marketable commodity,
it also illustrates the concept of transfer function.4 Let us model the car as a linear system
where the equation
an
dny
dtn + an−1
dn−1y
dtn−1 + · · · + a1
dy
dt + a0y = f(t)
(5.2.17)
governs the response y(t) to a forcing f(t). Assuming that the car has been sitting still, the
initial conditions are zero and the Laplace transform of Equation 5.2.17 is
K(s)Y (s) = F(s),
(5.2.18)
where
K(s) = ansn + an−1sn−1 + · · · + a1s + a0.
(5.2.19)
3 This is obviously a very old joke.
4 Originally suggested by Stern, M. D., 1987: Why the mechanic kicked the car - A teaching aid for
transfer functions. Math. Gaz., 71, 62–64.

Green’s Functions
227
Hence,
Y (s) = F(s)
K(s) = G(s)F(s),
(5.2.20)
where the transfer function G(s) clearly depends only on the internal workings of the car.
So if we know the transfer function, we understand how the car vibrates because
y(t) =
Z t
0
g(t −x)f(x) dx.
(5.2.21)
But what does this have to do with our mechanic? He realized that a short sharp kick
mimics an impulse forcing with f(t) = δ(t) and y(t) = g(t). Therefore, by observing the
response of the car to his kick, he diagnosed the loose bolt and ﬁxed the car.
⊓⊔
In the previous examples, we used Laplace transforms to solve for the Green’s functions.
However, there is a rich tradition of using Fourier transforms rather than Laplace transforms.
In these particular cases, the Fourier transform of the Green’s function is called frequency
response or steady-state transfer function of our system when τ = 0. Consider the following
examples.
• Example 5.2.4: Spectrum of a damped harmonic oscillator
In mechanics the damped oscillations of a mass m attached to a spring with a spring
constant k and damped with a velocity-dependent resistance are governed by the equation
my′′ + cy′ + ky = f(t),
(5.2.22)
where y(t) denotes the displacement of the oscillator from its equilibrium position, c denotes
the damping coeﬃcient, and f(t) denotes the forcing.
Assuming that both f(t) and y(t) have Fourier transforms, let us analyze this system
by ﬁnding its frequency response.
We begin by solving for the Green’s function g(t|τ),
which is given by
mg′′ + cg′ + kg = δ(t −τ),
(5.2.23)
because the Green’s function is the response of a system to a delta function forcing. Taking
the Fourier transform of both sides of Equation 5.2.23, the frequency response is
G(ω|τ) =
e−iωτ
k + icω −mω2 =
e−iωτ/m
ω2
0 + icω/m −ω2 ,
(5.2.24)
where ω2
0 = k/m is the natural frequency of the system. The most useful quantity to plot
is the frequency response or
|G(ω|τ)| =
ω2
0
k
p
(ω2 −ω2
0)2 + ω2ω2
0(c2/km)
(5.2.25)
=
1
k
p
[(ω/ω0)2 −1]2 + (c2/km)(ω/ω0)2 .
(5.2.26)
In Figure 5.2.2 we plotted the frequency response as a function of c2/(km). Note that as the
damping becomes larger, the sharp peak at ω = ω0 essentially vanishes. As c2/(km) →0,

228
Advanced Engineering Mathematics: A Second Course
ω/ω 0
k
0.0
0.5
1.0
1.5
2.0
0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
9.0
10.0
11.0
c /km = 0.01
2
c /km = 0.1
c /km = 1
2
2
|
)
G(ω |
Figure 5.2.2: The variation of the frequency response for a damped harmonic oscillator as a function of
driving frequency ω. See the text for the deﬁnition of the parameters.
we obtain a very ﬁnely tuned response curve. Let us now ﬁnd the Green’s function. From
the deﬁnition of the inverse Fourier transform,
mg(t|τ) = −1
2π
Z ∞
−∞
eiωt
ω2 −icω/m −ω2
0
dω = −1
2π
Z ∞
−∞
eiωt
(ω −ω1)(ω −ω2) dω,
(5.2.27)
where
ω1,2 = ±
q
ω2
0 −γ2 + γi,
(5.2.28)
and γ = c/(2m) > 0. We can evaluate Equation 5.2.27 by residues. Clearly the poles always
lie in the upper half of the ω-plane. Thus, if t < τ in Equation 5.2.27 we can close the line
integration along the real axis with a semicircle of inﬁnite radius in the lower half of the
ω-plane by Jordan’s lemma. Because the integrand is analytic within the closed contour,
g(t|τ) = 0 for t < τ. This is simply the causality condition,5 the impulse forcing being the
cause of the excitation. Clearly, causality is closely connected with the analyticity of the
frequency response in the lower half of the ω-plane.
If t > τ, we close the line integration along the real axis with a semicircle of inﬁnite
radius in the upper half of the ω-plane and obtain
mg(t|τ) = 2πi

−1
2π
 
Res

eiz(t−τ)
(z −ω1)(z −ω2); ω1

+ Res

eiz(t−τ)
(z −ω1)(z −ω2); ω2

(5.2.29)
=
−i
ω1 −ω2
h
eiω1(t−τ) −eiω2(t−τ)i
=
e−γ(t−τ) sin
h
(t −τ)
p
ω2
0 −γ2
i
p
ω2
0 −γ2
H(t −τ).
(5.2.30)
Let us now examine the damped harmonic oscillator by describing the migration of
the poles ω1,2 in the complex ω-plane as γ increases from 0 to ∞. See Figure 5.2.3. For
γ ≪ω0 (weak damping), the poles ω1,2 are very near to the real axis, above the points
±ω0, respectively. This corresponds to the narrow resonance band discussed earlier and
we have an underdamped harmonic oscillator.
As γ increases from 0 to ω0, the poles
5 The principle stating that an event cannot precede its cause.

Green’s Functions
229
τ
τ
x
y
−ω0
0
ω
t > 
t < 
Figure 5.2.3: The migration of the poles of the frequency response of a damped harmonic oscillator as a
function of γ.
approach the positive imaginary axis, moving along a semicircle of radius ω0 centered at
the origin. They coalesce at the point iω0 for γ = ω0, yielding repeated roots, and we have
a critically damped oscillator. For γ > ω0, the poles move in opposite directions along
the positive imaginary axis; one of them approaches the origin, while the other tends to
i∞as γ →∞. The solution then has two purely decaying, overdamped solutions. During
the early 1950s, a similar diagram was invented by Evans6 where the movement of closed-
loop poles is plotted for all values of a system parameter, usually the gain. This root-locus
method is very popular in system control theory for two reasons. First, the investigator can
easily determine the contribution of a particular closed-loop pole to the transient response.
Second, he can determine the manner in which open-loop poles or zeros should be introduced
or their location modiﬁed so that he will achieve a desired performance characteristic for
his system.
⊓⊔
• Example 5.2.5: Low-frequency ﬁlter
Consider the ordinary diﬀerential equation
Ry′ + y
C = f(t),
(5.2.31)
where R and C are real, positive constants. If y(t) denotes current, then Equation 5.2.31
would be the equation that gives the voltage across a capacitor in an RC circuit. Let us ﬁnd
the frequency response and Green’s function for this system. We begin by writing Equation
5.2.31 as
Rg′ + g
C = δ(t −τ),
(5.2.32)
where g(t|τ) denotes the Green’s function. If the Fourier transform of g(t|τ) is G(ω|τ), the
frequency response G(ω|τ) is given by
iωRG(ω|τ) + G(ω|τ)
C
= e−iωτ,
(5.2.33)
6 Evans, W. R., 1948: Graphical analysis of control systems. Trans. AIEE, 67, 547–551; Evans, W.
R., 1954: Control-System Dynamics. McGraw-Hill, 282 pp.

230
Advanced Engineering Mathematics: A Second Course
p
ω/ω
|
0.0
1.0
2.0
3.0
4.0
5.0
0.0
0.2
0.4
0.6
0.8
1.0
G(ω
/C
|)
Figure 5.2.4: The variation of the frequency response, Equation 5.2.35, as a function of driving frequency
ω. See the text for the deﬁnition of the parameters.
or
G(ω|τ) =
e−iωτ
iωR + 1/C =
Ce−iωτ
1 + iωRC ,
(5.2.34)
and
|G(ω|τ)| =
C
√
1 + ω2R2C2 =
C
q
1 + ω2/ω2p
,
(5.2.35)
where ωp = 1/(RC) is an intrinsic constant of the system.
In Figure 5.2.4 we plotted
|G(ω|τ)| as a function of ω. From this ﬁgure, we see that the response is largest for small
ω and decreases as ω increases.
This is an example of a low-frequency ﬁlter because relatively more signal passes through
at lower frequencies than at higher frequencies. To understand this, let us drive the system
with a forcing function that has the Fourier transform F(ω). The response of the system
will be G(ω, 0)F(ω). Thus, that portion of the forcing function’s spectrum at the lower
frequencies is relatively unaﬀected because |G(ω, 0)| is near unity.
However, at higher
frequencies where |G(ω, 0)| is smaller, the magnitude of the output is greatly reduced.
⊓⊔
• Example 5.2.6
During his study of tumor growth, Adam7 found the particular solution to an ordinary
diﬀerential equation which, in its simplest form, is
y′′ −α2y =

|x|/L −1,
|x| < L,
0,
|x| > L,
(5.2.36)
by the method of Green’s functions. Let us retrace his steps and see how he did it.
The ﬁrst step is ﬁnding the Green’s function. We do this by solving
g′′ −α2g = δ(x),
(5.2.37)
subject to the boundary conditions lim|x|→∞g(x) →0. Taking the Fourier transform of
Equation 5.2.37, we obtain
G(ω) = −
1
ω2 + α2 .
(5.2.38)
7 Adam, J. A., 1986: A simpliﬁed mathematical model of tumor growth. Math. Biosci., 81, 229–244.

Green’s Functions
231
The function G(ω) is the frequency response for our problem. Straightforward inversion
yields the Green’s function
g(x) = −e−α|x|
2α
.
(5.2.39)
Therefore, by the convolution integral, y(x) = g(x) ∗f(x),
y(x) =
Z L
−L
g(x −ξ) (|ξ|/L −1) dξ = 1
2α
Z L
−L
(1 −|ξ|/L) e−α|x−ξ| dξ.
(5.2.40)
To evaluate Equation 5.2.40 we must consider four separate cases: −∞< x < −L,
−L < x < 0, 0 < x < L, and L < x < ∞. Turning to the −∞< x < −L case ﬁrst, we have
y(x) = 1
2α
Z L
−L
(1 −|ξ|/L) eα(x−ξ) dξ
(5.2.41)
= eαx
2α
Z 0
−L
(1 + ξ/L) e−αξ dξ + eαx
2α
Z L
0
(1 −ξ/L) e−αξ dξ
(5.2.42)
= eαx
2α3L
 eαL + e−αL −2

.
(5.2.43)
Similarly, for x > L,
y(x) = 1
2α
Z L
−L
(1 −|ξ|/L) e−α(x−ξ) dξ
(5.2.44)
= e−αx
2α
Z 0
−L
(1 + ξ/L) eαξ dξ + e−αx
2α
Z L
0
(1 −ξ/L) eαξ dξ
(5.2.45)
= e−αx
2α3L
 eαL + e−αL −2

.
(5.2.46)
On the other hand, for −L < x < 0, we ﬁnd that
y(x) = 1
2α
Z x
−L
(1 −|ξ|/L) e−α(x−ξ) dξ + 1
2α
Z L
x
(1 −|ξ|/L) eα(x−ξ) dξ
(5.2.47)
= e−αx
2α
Z x
−L
(1 + ξ/L) eαξ dξ + eαx
2α
Z 0
x
(1 + ξ/L) e−αξ dξ + eαx
2α
Z L
0
(1 −ξ/L) e−αξ dξ
(5.2.48)
=
1
α3L

e−αL cosh(αx) + α(x + L) −eαx 
.
(5.2.49)
Finally, for 0 < x < L, we have that
y(x) = 1
2α
Z x
−L
(1 −|ξ|/L) e−α(x−ξ) dξ + 1
2α
Z L
x
(1 −|ξ|/L) eα(x−ξ) dξ
(5.2.50)
= e−αx
2α
Z 0
−L
(1 + ξ/L) eαξ dξ + e−αx
2α
Z x
0
(1 −ξ/L) eαξ dξ + eαx
2α
Z L
x
(1 −ξ/L) e−αξ dξ
(5.2.51)
=
1
α3L

e−αL cosh(αx) + α(L −x) −e−αx 
.
(5.2.52)

232
Advanced Engineering Mathematics: A Second Course
These results can be collapsed down into
y(x) =
1
α3L
h
e−αL cosh(αx) + α(L −|x|) −e−α|x| i
(5.2.53)
if |x| < L, and
y(x) = e−α|x|
2α3L
 eαL + e−αL −2

(5.2.54)
if |x| > L.
⊓⊔
Superposition integral
So far we showed how the response of any system can be expressed in terms of its
Green’s function and the arbitrary forcing. Can we also determine the response using the
indicial admittance a(t)?
Consider ﬁrst a system that is dormant until a certain time t = τ1. At that instant we
subject the system to a forcing H(t −τ1). Then the response will be zero if t < τ1 and will
equal the indicial admittance a(t −τ1) when t > τ1 because the indicial admittance is the
response of a system to the step function. Here t−τ1 is the time measured from the instant
of change.
Next, suppose that we now force the system with the value f(0) when t = 0 and hold
that value until t = τ1. We then abruptly change the forcing by an amount f(τ1) −f(0)
to the value f(τ1) at the time τ1 and hold it at that value until t = τ2. Then we again
abruptly change the forcing by an amount f(τ2) −f(τ1) at the time τ2, and so forth (see
Figure 5.2.5). From the linearity of the problem, the response after the instant t = τn equals
the sum
y(t) = f(0)a(t) + [f(τ1) −f(0)]a(t −τ1) + [f(τ2) −f(τ1)]a(t −τ2)
+ · · · + [f(τn) −f(τn−1)]a(t −τn).
(5.2.55)
If we write f(τk) −f(τk−1) = ∆fk and τk −τk−1 = ∆τk, Equation 5.2.55 becomes
y(t) = f(0)a(t) +
n
X
k=1
a(t −τk)∆fk
∆τk
∆τk.
(5.2.56)
Finally, proceeding to the limit as the number n of jumps becomes inﬁnite, in such a manner
that all jumps and intervals between successive jumps tend to zero, this sum has the limit
y(t) = f(0)a(t) +
Z t
0
f ′(τ)a(t −τ) dτ.
(5.2.57)
Because the total response of the system equals the weighted sum (the weights being a(t))
of the forcing from the initial moment up to the time t, we refer to Equation 5.2.57 as
the superposition integral, or Duhamel’s integral,8 named after the French mathematical
8 Duhamel, J.-M.-C., 1833: M´emoire sur la m´ethode g´en´erale relative au mouvement de la chaleur dans
les corps solides plong´es dans des milieux dont la temp´erature varie avec le temps. J. ´Ecole Polytech., 22,
20–77.

Green’s Functions
233
t
τ
τ
τ
1
2
3
∆τ
f(t)
τn
Figure 5.2.5: Diagram used in the derivation of Duhamel’s integral.
physicist Jean-Marie-Constant Duhamel (1797–1872), who ﬁrst derived it in conjunction
with heat conduction.
We can also express Equation 5.2.57 in several diﬀerent forms. Integration by parts
yields
y(t) = f(t)a(0) +
Z t
0
f(τ)a′(t −τ) dτ = d
dt
Z t
0
f(τ)a(t −τ) dτ

.
(5.2.58)
• Example 5.2.7
Suppose that a system has the step response of a(t) = A[1 −e−t/T ], where A and T
are positive constants. Let us ﬁnd the response if we force this system by f(t) = kt, where
k is a constant.
From the superposition integral, Equation 5.2.57,
y(t) = 0 +
Z t
0
kA[1 −e−(t−τ)/T ] dτ = kA[t −T(1 −e−t/T )].
(5.2.59)
⊓⊔
Boundary-value problem
One of the purposes of this book is the solution of a wide class of nonhomogeneous
ordinary diﬀerential equations of the form
d
dx

p(x)dy
dx

+ s(x)y = −f(x),
a ≤x ≤b,
(5.2.60)
with
α1y(a) + α2y′(a) = 0,
β1y(b) + β2y′(b) = 0.
(5.2.61)
This is an example of a Sturm-Liouville-like equation
d
dx

p(x)dy
dx

+ [q(x) + λr(x)]y = −f(x),
a ≤x ≤b,
(5.2.62)

234
Advanced Engineering Mathematics: A Second Course
where λ is a parameter. Here we wish to develop the Green’s function for this class of
boundary-value problems.
We begin by determining the Green’s function for the equation
d
dx

p(x)dg
dx

+ s(x)g = −δ(x −ξ),
(5.2.63)
subject to yet undetermined boundary conditions. We know that such a function exists for
the special case p(x) = 1 and s(x) = 0, and we now show that this is almost always true
in the general case. Presently we construct Green’s functions by requiring that they satisfy
the following conditions:
• g(x|ξ) satisﬁes the homogeneous equation f(x) = 0 except at x = ξ,
• g(x|ξ) satisﬁes certain homogeneous conditions, and
• g(x|ξ) is continuous at x = ξ.
These homogeneous boundary conditions for a ﬁnite interval (a, b) will be
α1g(a|ξ) + α2g′(a|ξ) = 0,
β1g(b|ξ) + β2g′(b|ξ) = 0,
(5.2.64)
where g′ denotes the x derivative of g(x|ξ) and neither a nor b equals ξ. The coeﬃcients
α1 and α2 cannot both be zero; this also holds for β1 and β2. These conditions include the
commonly encountered Dirichlet, Neumann, and Robin boundary conditions.
What about the value of g′(x|ξ) at x = ξ? Because g(x|ξ) is a continuous function of
x, Equation 5.2.63 dictates that there must be a discontinuity in g′(x|ξ) at x = ξ. We now
show that this discontinuity consists of a jump in the value g′(x|ξ) at x = ξ. To prove this,
we begin by integrating Equation 5.2.63 from ξ −ǫ to ξ + ǫ, which yields
p(x)dg(x|ξ)
dx

ξ+ǫ
ξ−ǫ
+
Z ξ+ǫ
ξ−ǫ
s(x)g(x|ξ) dx = −1.
(5.2.65)
Because g(x|ξ) and s(x) are both continuous at x = ξ,
lim
ǫ→0
Z ξ+ǫ
ξ−ǫ
s(x)g(x|ξ) dx = 0.
(5.2.66)
Applying the limit ǫ →0 to Equation 5.2.65, we have that
p(ξ)
dg(ξ+|ξ)
dx
−dg(ξ−|ξ)
dx

= −1,
(5.2.67)
where ξ+ and ξ−denote points just above and below x = ξ, respectively. Consequently,
our last requirement on g(x|ξ) will be that
• dg/dx must have a jump discontinuity of magnitude −1/p(ξ) at x = ξ.

Green’s Functions
235
Similar conditions hold for higher-order ordinary diﬀerential equations.9
Consider now the region a ≤x < ξ. Let y1(x) be a nontrivial solution of the homo-
geneous diﬀerential equation satisfying the boundary condition at x = a; then α1y1(a) +
α2y′
1(a) = 0. Because g(x|ξ) must satisfy the same boundary condition, α1g(a|ξ)+α2g′(a|ξ)
= 0. Since the set α1, α2 is nontrivial, then the Wronskian of y1 and g must vanish at x = a
or y1(a)g′(a|ξ) −y′
1(a)g(a|ξ) = 0. However, for a ≤x < ξ, both y1(x) and g(x|ξ) satisfy
the same diﬀerential equation, the homogeneous one. Therefore, their Wronskian is zero
at all points and g(x|ξ) = c1y1(x) for a ≤x < ξ, where c1 is an arbitrary constant. In
a similar manner, if the nontrivial function y2(x) satisﬁes the homogeneous equation and
the boundary conditions at x = b, then g(x|ξ) = c2y2(x) for ξ < x ≤b. The continuity
condition of g and the jump discontinuity of g′ at x = ξ imply
c1y1(ξ) −c2y2(ξ) = 0,
c1y′
1(ξ) −c2y′
2(ξ) = 1/p(ξ).
(5.2.68)
We can solve Equation 5.2.68 for c1 and c2 provided the Wronskian of y1 and y2 does not
vanish at x = ξ, or
y1(ξ)y′
2(ξ) −y2(ξ)y′
1(ξ) ̸= 0.
(5.2.69)
In other words, y1(x) must not be a multiple of y2(x). Is this always true? The answer is
“generally yes.” If the homogeneous equation admits no nontrivial solutions satisfying both
boundary conditions at the same time,10 then y1(x) and y2(x) must be linearly independent.
On the other hand, if the homogeneous equation possesses a single solution, say y0(x), which
also satisﬁes α1y0(a)+ α2y′
0(a) = 0 and β1y0(b)+ β2y′
0(b) = 0, then y1(x) will be a multiple
of y0(x) and so is y2(x). Then they are multiples of each other and their Wronskian vanishes.
This would occur, for example, if the diﬀerential equation is a Sturm-Liouville equation, λ
equals the eigenvalue, and y0(x) is the corresponding eigenfunction. No Green’s function
exists in this case.
• Example 5.2.8
Consider the problem of ﬁnding the Green’s function for g′′ +k2g = −δ(x−ξ), 0 < x <
L, subject to the boundary conditions g(0|ξ) = g(L|ξ) = 0 with k ̸= 0. The corresponding
homogeneous equation is y′′ + k2y = 0. Consequently, g(x|ξ) = c1y1(x) = c1 sin(kx) for
0 ≤x ≤ξ, while g(x|ξ) = c2y2(x) = c2 sin[k(L −x)] for ξ ≤x ≤L.
Let us compute the Wronskian. For our particular problem,
W(x) = y1(x)y′
2(x) −y′
1(x)y2(x)
(5.2.70)
= −k sin(kx) cos[k(L −x)] −k cos(kx) sin[k(L −x)]
(5.2.71)
= −k sin[k(x + L −x)] = −k sin(kL),
(5.2.72)
and W(ξ) = −k sin(kL). Therefore, the Green’s function will exist as long as kL ̸= nπ. If
kL = nπ, y1(x) and y2(x) are linearly dependent with y0(x) = c3 sin(nπx/L), the solution
to the regular Sturm-Liouville problem y′′ + λy = 0, and y(0) = y(L) = 0.
⊓⊔
9 Ince, E. L., 1956: Ordinary Diﬀerential Equations. Dover Publications, Inc. See Section 11.1.
10 In the theory of diﬀerential equations, this system would be called incompatible: one that admits no
solution, save y = 0, which is also continuous for all x in the interval (a, b) and satisﬁes the homogeneous
boundary conditions.

236
Advanced Engineering Mathematics: A Second Course
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
 x/L
 ξ/L
 L g(x|ξ)
Figure 5.2.6: The Green’s function, Equation 5.2.75, divided by L, as functions of x and ξ when kL = 10.
Let us now proceed to ﬁnd g(x|ξ) when it does exist. The system, Equation 5.2.68, has
the unique solution
c1 = −
y2(ξ)
p(ξ)W(ξ),
and
c2 = −
y1(ξ)
p(ξ)W(ξ),
(5.2.73)
where W(ξ) is the Wronskian of y1(x) and y2(x) at x = ξ. Therefore,
g(x|ξ) = −y1(x<)y2(x>)
p(ξ)W(ξ)
.
(5.2.74)
Clearly g(x|ξ) is symmetric in x and ξ. It is also unique. The proof of the uniqueness is as
follows: We can always choose a diﬀerent y1(x), but it will be a multiple of the “old” y1(x),
and the Wronskian will be multiplied by the same factor, leaving g(x|ξ) the same. This is
also true if we modify y2(x) in a similar manner.
• Example 5.2.9
Let us ﬁnd the Green’s function for g′′ + k2g = −δ(x −ξ), 0 < x < L, subject to
the boundary conditions g(0|ξ) = g(L|ξ) = 0.
As we showed in the previous example,
y1(x) = c1 sin(kx), y2(x) = c2 sin[k(L −x)], and W(ξ) = −k sin(kL). Substituting into
Equation 5.2.74, we have that
g(x|ξ) = sin(kx<) sin[k(L −x>)]
k sin(kL)
,
(5.2.75)
where x< = min(x, ξ) and x> = max(x, ξ). Figure 5.2.6 illustrates Equation 5.2.75.
⊓⊔
So far, we showed that the Green’s function for Equation 5.2.63 exists, is symmetric,
and enjoys certain properties (see the material in the boxes after Equation 5.2.63 and
Equation 5.2.67). But how does this help us solve Equation 5.2.63? We now prove that
y(x) =
Z b
a
g(x|ξ)f(ξ) dξ
(5.2.76)

Green’s Functions
237
is the solution to the nonhomogeneous diﬀerential equation, Equation 5.2.63, and the ho-
mogeneous boundary conditions, Equation 5.2.64.
We begin by noting that in Equation 5.2.76 x is a parameter while ξ is the dummy
variable. As we perform the integration, we must switch from the form for g(x|ξ) for ξ ≤x
to the second form for ξ ≥x when ξ equals x; thus,
y(x) =
Z x
a
g(x|ξ)f(ξ) dξ +
Z b
x
g(x|ξ)f(ξ) dξ.
(5.2.77)
Diﬀerentiation yields
d
dx
Z x
a
g(x|ξ)f(ξ) dξ =
Z x
a
dg(x|ξ)
dx
f(ξ) dξ + g(x|x−)f(x),
(5.2.78)
and
d
dx
Z b
x
g(x|ξ)f(ξ) dξ =
Z b
x
dg(x|ξ)
dx
f(ξ) dξ −g(x|x+)f(x).
(5.2.79)
Because g(x|ξ) is continuous everywhere, we have that g(x|x+) = g(x|x−) so that
dy
dx =
Z x
a
dg(x|ξ)
dx
f(ξ) dξ +
Z b
x
dg(x|ξ)
dx
f(ξ) dξ.
(5.2.80)
Diﬀerentiating once more gives
d2y
dx2 =
Z x
a
d2g(x|ξ)
dx2
f(ξ) dξ+dg(x|x−)
dx
f(x)+
Z b
x
d2g(x|ξ)
dx2
f(ξ) dξ−dg(x|x+)
dx
f(x). (5.2.81)
The second and fourth terms on the right side of Equation 5.2.81 will not cancel in this
case; on the contrary,
dg(x|x−)
dx
−dg(x|x+)
dx
= −1
p(x).
(5.2.82)
To show this, we note that the term dg(x|x−)/dx denotes a diﬀerentiation of g(x|ξ) with
respect to x using the x > ξ form and then letting ξ →x. Thus,
dg(x|x−)
dx
= −lim
ξ→x
ξ<x
y′
2(x)y1(ξ)
p(ξ)W(ξ) = −y′
2(x)y1(x)
p(x)W(x) ,
(5.2.83)
while for dg(x|x+)/dx we use the x < ξ form or
dg(x|x+)
dx
= −lim
ξ→x
ξ>x
y′
1(x)y2(ξ)
p(ξ)W(ξ) = −y′
1(x)y2(x)
p(x)W(x) .
(5.2.84)
Upon introducing these results into the diﬀerential equation
p(x)d2y
dx2 + p′(x)dy
dx + s(x)y = −f(x),
(5.2.85)
we have
Z x
a
[p(x)g′′(x|ξ) + p′(x)g′(x|ξ) + s(x)g(x|ξ)]f(ξ) dξ
(5.2.86)
+
Z b
x
[p(x)g′′(x|ξ) + p′(x)g′(x|ξ) + s(x)g(x|ξ)]f(ξ) dξ −p(x)f(x)
p(x) = −f(x).

238
Advanced Engineering Mathematics: A Second Course
Because
p(x)g′′(x|ξ) + p′(x)g′(x|ξ) + s(x)g(x|ξ) = 0,
(5.2.87)
except for x = ξ, Equation 5.2.86, and thus Equation 5.2.63, is satisﬁed. Although Equation
5.2.87 does not hold at the point x = ξ, the results are still valid because that one point
does not aﬀect the values of the integrals. As for the boundary conditions,
y(a) =
Z b
a
g(a|ξ)f(ξ) dξ,
y′(a) =
Z b
a
dg(a|ξ)
dx
f(ξ) dξ,
(5.2.88)
and α1y(a) + α2y′(a) = 0 from Equation 5.2.64. A similar proof holds for x = b.
Finally, let us consider the solution for the nonhomogeneous boundary conditions
α1y(a) + α2y′(a) = α, and β1y(b) + β2y′(b) = β. The solution in this case is
y(x) =
αy2(x)
α1y2(a) + α2y′
2(a) +
βy1(x)
β1y1(b) + β2y′
1(b) +
Z b
a
g(x|ξ)f(ξ) dξ.
(5.2.89)
A quick check shows that Equation 5.2.89 satisﬁes the diﬀerential equation and both non-
homogeneous boundary conditions.
Eigenfunction expansion
We just showed how Green’s functions can be used to solve the nonhomogeneous linear
diﬀerential equation. The next question is how do you ﬁnd the Green’s function? Here we
present the most common method: series expansion. This is not surprising given its success
in solving the Sturm-Liouville problem.
Consider the nonhomogeneous problem
y′′ = −f(x),
with
y(0) = y(L) = 0.
(5.2.90)
The Green’s function g(x|ξ) must therefore satisfy
g′′ = −δ(x −ξ),
with
g(0|ξ) = g(L|ξ) = 0.
(5.2.91)
Because g(x|ξ) vanishes at the ends of the interval (0, L), this suggests that it can be
expanded in a series of suitably chosen orthogonal functions such as, for instance, the
Fourier sine series
g(x|ξ) =
∞
X
n=1
Gn(ξ) sin
nπx
L

,
(5.2.92)
where the expansion coeﬃcients Gn are dependent on the parameter ξ. Although we chose
the orthogonal set of functions sin(nπx/L), we could have used other orthogonal functions
as long as they vanish at the endpoints.

Green’s Functions
239
Because
g′′(x|ξ) =
∞
X
n=1

−n2π2
L2

Gn(ξ) sin
nπx
L

,
(5.2.93)
and
δ(x −ξ) =
∞
X
n=1
An(ξ) sin
nπx
L

,
(5.2.94)
where
An(ξ) = 2
L
Z L
0
δ(x −ξ) sin
nπx
L

dx = 2
L sin
nπξ
L

,
(5.2.95)
we have that
−
∞
X
n=1
n2π2
L2

Gn(ξ) sin
nπx
L

= −2
L
∞
X
n=1
sin
nπξ
L

sin
nπx
L

,
(5.2.96)
after substituting Equation 5.2.93 through Equation 5.2.95 into the diﬀerential equation,
Equation 5.2.91. Since Equation 5.2.96 must hold for any arbitrary x,
n2π2
L2

Gn(ξ) = 2
L sin
nπξ
L

.
(5.2.97)
Thus, the Green’s function is
g(x|ξ) = 2L
π2
∞
X
n=1
1
n2 sin
nπξ
L

sin
nπx
L

.
(5.2.98)
How might we use Equation 5.2.98? We can use this series to construct the solution of
the nonhomogeneous equation, Equation 5.2.90, via the formula
y(x) =
Z L
0
g(x|ξ) f(ξ) dξ.
(5.2.99)
This leads to
y(x) = 2L
π2
∞
X
n=1
1
n2 sin
nπx
L
 Z L
0
f(ξ) sin
nπξ
L

dξ,
(5.2.100)
or
y(x) = L2
π2
∞
X
n=1
an
n2 sin
nπx
L

,
(5.2.101)
where an are the Fourier sine coeﬃcients of f(x).
• Example 5.2.10
Consider now the more complicated boundary-value problem
y′′ + k2y = −f(x),
with
y(0) = y(L) = 0.
(5.2.102)
The Green’s function g(x|ξ) must now satisfy
g′′ + k2g = −δ(x −ξ),
and
g(0|ξ) = g(L|ξ) = 0.
(5.2.103)

240
Advanced Engineering Mathematics: A Second Course
Once again, we use the Fourier sine expansion
g(x|ξ) =
∞
X
n=1
Gn(ξ) sin
nπx
L

.
(5.2.104)
Direct substitution of Equation 5.2.104 and Equation 5.2.94 into Equation 5.2.103 and
grouping by corresponding harmonics yields
−n2π2
L2 Gn(ξ) + k2Gn(ξ) = −2
L sin
nπξ
L

,
(5.2.105)
or
Gn(ξ) = 2
L
sin(nπξ/L)
n2π2/L2 −k2 .
(5.2.106)
Thus, the Green’s function is
g(x|ξ) = 2
L
∞
X
n=1
sin(nπξ/L) sin(nπx/L)
n2π2/L2 −k2
.
(5.2.107)
Examining Equation 5.2.107 more closely, we note that it enjoys the symmetry property
that g(x|ξ) = g(ξ|x).
⊓⊔
• Example 5.2.11
Let us ﬁnd the series expansion for the Green’s function for
xg′′ + g′ +

k2x −m2
x

g = −δ(x −ξ),
0 < x < L,
(5.2.108)
where m ≥0 and is an integer. The boundary conditions are
lim
x→0 |g(x|ξ)| < ∞,
and
g(L|ξ) = 0.
(5.2.109)
To ﬁnd this series, consider the Fourier-Bessel series
g(x|ξ) =
∞
X
n=1
Gn(ξ)Jm(knmx),
(5.2.110)
where knm is the nth root of Jm(knmL) = 0. This series enjoys the advantage that it satisﬁes
the boundary conditions and we will not have to introduce any homogeneous solutions so
that g(x|ξ) satisﬁes the boundary conditions.
Substituting Equation 5.2.110 into Equation 5.2.108 after we divide by x and using the
Fourier-Bessel expansion for the delta function, we have that
(k2 −k2
nm)Gn(ξ) = −2k2
nmJm(knmξ)
L2[Jm+1(knmL)]2 = −
2Jm(knmξ)
L2[J′m(knmL)]2 ,
(5.2.111)
so that
g(x|ξ) = 2
L2
∞
X
n=1
Jm(knmξ)Jm(knmx)
(k2nm −k2)[J′m(knmL)]2 .
(5.2.112)

Green’s Functions
241
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
−2
−1
0
1
2
3
4
 x/L
 ξ/L
 L g(x|ξ)
Figure 5.2.7: The Green’s function, Equation 5.2.112, as functions of x/L and ξ/L when kL = 10 and
m = 1.
Equation 5.2.112 is plotted in Figure 5.2.7.
⊓⊔
We summarize the expansion technique as follows: Suppose that we want to solve the
diﬀerential equation
Ly(x) = −f(x),
(5.2.113)
with some condition By(x) = 0 along the boundary, where L now denotes the Sturm-
Liouville diﬀerential operator
L = d
dx

p(x) d
dx

+ [q(x) + λr(x)],
(5.2.114)
and B is the boundary condition operator
B =







α1 + α2
d
dx,
at x = a,
β1 + β2
d
dx,
at x = b.
(5.2.115)
We begin by seeking a Green’s function g(x|ξ), which satisﬁes
Lg = −δ(x −ξ),
Bg = 0.
(5.2.116)
To ﬁnd the Green’s function, we utilize the set of eigenfunctions ϕn(x) associated with the
regular Sturm-Liouville problem
d
dx

p(x)dϕn
dx

+ [q(x) + λnr(x)]ϕn = 0,
(5.2.117)

242
Advanced Engineering Mathematics: A Second Course
where ϕn(x) satisﬁes the same boundary conditions as y(x). If g exists and if the set {ϕn}
is complete, then g(x|ξ) can be represented by the series
g(x|ξ) =
∞
X
n=1
Gn(ξ)ϕn(x).
(5.2.118)
Applying L to Equation 5.2.118,
Lg(x|ξ) =
∞
X
n=1
Gn(ξ)L[ϕn(x)] =
∞
X
n=1
Gn(ξ)(λ −λn)r(x)ϕn(x) = −δ(x −ξ),
(5.2.119)
if λ does not equal any of the eigenvalues λn. Multiplying both sides of Equation 5.2.119
by ϕm(x) and integrating over x,
∞
X
n=1
Gn(ξ)(λ −λn)
Z b
a
r(x)ϕn(x)ϕm(x) dx = −ϕm(ξ).
(5.2.120)
If the eigenfunctions are orthonormal,
Z b
a
r(x)ϕn(x)ϕm(x) dx =
 1,
n = m,
0,
n ̸= m,
and
Gn(ξ) = ϕn(ξ)
λn −λ.
(5.2.121)
This leads directly to the bilinear formula:
g(x|ξ) =
∞
X
n=1
ϕn(ξ)ϕn(x)
λn −λ
,
(5.2.122)
which permits us to write the Green’s function at once if the eigenvalues and eigenfunctions
of L are known.
Problems
For the following initial-value problems, ﬁnd the transfer function, impulse response, Green’s
function, and step response. Assume that all of the necessary initial conditions are zero and
τ > 0. If you have MATLAB’s control toolbox, use MATLAB to check your work.
1. g′ + kg = δ(t −τ)
2. g′′ −2g′ −3g = δ(t −τ)
3. g′′ + 4g′ + 3g = δ(t −τ)
4. g′′ −2g′ + 5g = δ(t −τ)
5. g′′ −3g′ + 2g = δ(t −τ)
6. g′′ + 4g′ + 4g = δ(t −τ)
7. g′′ −9g = δ(t −τ)
8. g′′ + g = δ(t −τ)
9. g′′ −g′ = δ(t −τ)
Find the Green’s function and the corresponding bilinear expansion using eigenfunctions
from the regular Sturm-Liouville problem ϕ′′
n + k2
nϕn = 0 for
g′′ = −δ(x −ξ),
0 < x, ξ < L,

Green’s Functions
243
which satisfy the following boundary conditions:
10. g(0|ξ) −αg′(0|ξ) = 0, α ̸= 0, −L,
g(L|ξ) = 0,
11. g(0|ξ) −g′(0|ξ) = 0,
g(L|ξ) −g′(L|ξ) = 0,
12. g(0|ξ) −g′(0|ξ) = 0,
g(L|ξ) + g′(L|ξ) = 0.
Find the Green’s function11 and the corresponding bilinear expansion using eigenfunctions
from the regular Sturm-Liouville problem ϕ′′
n + k2
nϕn = 0 for
g′′ −k2g = −δ(x −ξ),
0 < x, ξ < L,
which satisfy the following boundary conditions:
13. g(0|ξ) = 0,
g(L|ξ) = 0,
14. g′(0|ξ) = 0,
g′(L|ξ) = 0,
15. g(0|ξ) = 0,
g(L|ξ) + g′(L|ξ) = 0,
16. g(0|ξ) = 0,
g(L|ξ) −g′(L|ξ) = 0,
17. a g(0|ξ) + g′(0|ξ) = 0,
g′(L|ξ) = 0,
18. g(0|ξ) + g′(0|ξ) = 0,
g(L|ξ) −g′(L|ξ) = 0.
5.3 JOINT TRANSFORM METHOD
In the previous section an important method for ﬁnding Green’s function involved either
Laplace or Fourier transforms. In the following sections we wish to ﬁnd Green’s functions
for partial diﬀerential equations. Again, transform methods play an important role. We will
always use the Laplace transform to eliminate the temporal dependence. However, for the
spatial dimension we will use either a Fourier series or Fourier transform. Our choice will
be dictated by the domain: If it reaches to inﬁnity, then we will employ Fourier transforms.
On the other hand, a domain of ﬁnite length calls for an eigenfunction expansion. The
following two examples illustrate our solution technique for domains of inﬁnite and ﬁnite
extent.
• Example 5.3.1: One-dimensional Klein-Gordon equation
The Klein-Gordon equation is a form of the wave equation that arose in particle physics
as the relativistic scalar wave equation describing particles with nonzero rest mass. In this
example, we ﬁnd its Green’s function when there is only one spatial dimension:
∂2g
∂x2 −1
c2
∂2g
∂t2 + a2g

= −δ(x −ξ)δ(t −τ),
(5.3.1)
11 Problem 18 was used by Chakrabarti, A., and T. Sahoo, 1996: Reﬂection of water waves by a nearly
vertical porous wall. J. Austral. Math. Soc., Ser. B, 37, 417–429.

244
Advanced Engineering Mathematics: A Second Course
where −∞< x, ξ < ∞, 0 < t, τ, c is a real, positive constant (the wave speed), and a is a
real, nonnegative constant. The corresponding boundary conditions are
lim
|x|→∞g(x, t|ξ, τ) →0,
(5.3.2)
and the initial conditions are
g(x, 0|ξ, τ) = gt(x, 0|ξ, τ) = 0.
(5.3.3)
We begin by taking the Laplace transform of Equation 5.3.1 and ﬁnd that
d2G
dx2 −
s2 + a2
c2

G = −δ(x −ξ)e−sτ.
(5.3.4)
Applying Fourier transforms to Equation 5.3.4, we obtain
G(x, s|ξ, τ) = c2
2π e−sτ
Z ∞
−∞
eik(x−ξ)
s2 + a2 + k2c2 dk = c2
π e−sτ
Z ∞
0
cos[k(x −ξ)]
s2 + a2 + k2c2 dk.
(5.3.5)
Inverting the Laplace transform and employing the second shifting theorem,
g(x, t|ξ, τ) = c2
π H(t −τ)
Z ∞
0
sin

(t −τ)
√
a2 + k2c2 
cos[k(x −ξ)]
√
a2 + k2c2
dk.
(5.3.6)
Equation 5.3.6 represents a superposition of homogeneous solutions (normal modes) to
Equation 5.3.1. An intriguing aspect of Equation 5.3.6 is that this solution occurs every-
where after t > τ. If |x −ξ| > c(t −τ), these wave solutions destructively interfere so that
we have zero there while they constructively interfere at those times and places where the
physical waves are present.
Applying integral tables to Equation 5.3.6, the ﬁnal result is
g(x, t|ξ, τ) = c
2J0
h
a
p
(t −τ)2 −(x −ξ)2/c2
i
H[c(t −τ) −|x −ξ|].
(5.3.7)
Figure 5.3.1 illustrates this Green’s function. Thus, the Green’s function for the Klein-
Gordon equation yields waves that propagate to the right and left from x = 0 with the
wave front located at x = ±ct. At a given point, after the passage of the wave front, the
solution vibrates with an ever-decreasing amplitude and at a frequency that approaches a,
the so-called cutoﬀfrequency, at t →∞.
Why is a called a cutoﬀfrequency? From Equation 5.3.5, we see that, although the
spectral representation includes all of the wavenumbers k running from −∞to ∞, the
frequency ω =
√
c2k2 + a2 is restricted to the range ω ≥a from Equation 5.3.6. Thus, a is
the lowest possible frequency that a wave solution to the Klein-Gordon equation may have
for a real value of k.
⊓⊔

Green’s Functions
245
−10
−5
0
5
10
0
2
4
6
8
10
−0.4
−0.2
0
0.2
0.4
0.6
 a(x−ξ)/c
 a(t−τ)
 g(x,t|ξ,τ)/c
Figure 5.3.1: The free-space Green’s function g(x, t|ξ, τ)/c for the one-dimensional Klein-Gordon equation
at diﬀerent distances a(x −ξ)/c and times a(t −τ).
• Example 5.3.2: One-dimensional wave equation on the interval 0 < x < L
One of the classic problems of mathematical physics involves ﬁnding the displacement
of a taut string between two supports when an external force is applied. The governing
equation is
∂2u
∂t2 −c2 ∂2u
∂x2 = f(x, t),
0 < x < L,
0 < t,
(5.3.8)
where c is the constant phase speed.
In this example, we ﬁnd the Green’s function for this problem by considering the
following problem:
∂2g
∂t2 −c2 ∂2g
∂x2 = δ(x −ξ)δ(t −τ),
0 < x, ξ < L,
0 < t, τ,
(5.3.9)
with the boundary conditions
α1g(0, t|ξ, τ) + β1gx(0, t|ξ, τ) = 0,
0 < t,
(5.3.10)
and
α2g(L, t|ξ, τ) + β2gx(L, t|ξ, τ) = 0,
0 < t,
(5.3.11)
and the initial conditions
g(x, 0|ξ, τ) = gt(x, 0|ξ, τ) = 0,
0 < x < L.
(5.3.12)
We start by taking the Laplace transform of Equation 5.3.9 and ﬁnd that
d2G
dx2 −s2
c2 G = −δ(x −ξ)
c2
e−sτ,
0 < x < L,
(5.3.13)
with
α1G(0, s|ξ, τ) + β1G′(0, s|ξ, τ) = 0,
(5.3.14)

246
Advanced Engineering Mathematics: A Second Course
and
α2G(L, s|ξ, τ) + β2G′(L, s|ξ, τ) = 0.
(5.3.15)
Problems similar to Equation 5.3.13 through Equation 5.3.15 were considered in the previous
section. There, solutions were developed in terms of an eigenfunction expansion. Applying
the same technique here,
G(x, s|ξ, τ) = e−sτ
∞
X
n=1
ϕn(ξ)ϕn(x)
s2 + c2k2n
,
(5.3.16)
where ϕn(x) is the nth orthonormal eigenfunction to the regular Sturm-Liouville problem
ϕ
′′(x) + k2ϕ(x) = 0,
0 < x < L,
(5.3.17)
subject to the boundary conditions
α1ϕ(0) + β1ϕ′(0) = 0,
(5.3.18)
and
α2ϕ(L) + β2ϕ′(L) = 0.
(5.3.19)
Taking the inverse of Equation 5.3.16, we have that the Green’s function is
g(x, t|ξ, τ) =
( ∞
X
n=1
ϕn(ξ)ϕn(x)sin[knc(t −τ)]
knc
)
H(t −τ).
(5.3.20)
Let us illustrate our results to ﬁnd the Green’s function for
∂2g
∂t2 −c2 ∂2g
∂x2 = δ(x −ξ)δ(t −τ),
(5.3.21)
with the boundary conditions
g(0, t|ξ, τ) = g(L, t|ξ, τ) = 0,
0 < t,
(5.3.22)
and the initial conditions
g(x, 0|ξ, τ) = gt(x, 0|ξ, τ) = 0,
0 < x < L.
(5.3.23)
For this example, the Sturm-Liouville problem is
ϕ′′(x) + k2ϕ(x) = 0,
0 < x < L,
(5.3.24)
with the boundary conditions ϕ(0) = ϕ(L) = 0. The nth orthonormal eigenfunction for
this problem is
ϕn(x) =
r
2
L sin
nπx
L

.
(5.3.25)

Green’s Functions
247
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
 x/L
 c(t−τ)/L
 c g(x,t|ξ,τ)
Figure 5.3.2: The Green’s function cg(x, t|ξ, τ) given by Equation 5.3.26 for the one-dimensional wave
equation over the interval 0 < x < L as a function of location x/L and time c(t −τ)/L with ξ/L = 0.2.
The boundary conditions are g(0, t|ξ, τ) = g(L, t|ξ, τ) = 0.
Consequently, from Equation 5.3.20, the Green’s function is
g(x, t|ξ, τ) = 2
πc
( ∞
X
n=1
1
n sin
nπξ
L

sin
nπx
L

sin
nπc(t −τ)
L
)
H(t −τ).
(5.3.26)
See Figure 5.3.2.
5.4 WAVE EQUATION
In Section 5.2, we showed how Green’s functions could be used to solve initial- and
boundary-value problems involving ordinary diﬀerential equations. When we approach par-
tial diﬀerential equations, similar considerations hold, although the complexity increases.
In the next three sections, we work through the classic groupings of the wave, heat, and
Helmholtz’s equations in one spatial dimension. All of these results can be generalized to
three dimensions.
Of these three groups, we start with the wave equation
∂2u
∂x2 −1
c2
∂2u
∂t2 = −q(x, t),
(5.4.1)
where t denotes time, x is the position, c is the phase velocity of the wave, and q(x, t) is
the source density. In addition to Equation 5.4.1 it is necessary to state boundary and
initial conditions to obtain a unique solution. The condition on the boundary can be either
Dirichlet or Neumann or a linear combination of both (Robin condition). The conditions
in time must be Cauchy, that is, we must specify the value of u(x, t) and its time derivative
at t = t0 for each point of the region under consideration.
We begin by proving that we can express the solution to Equation 5.4.1 in terms of
boundary conditions, initial conditions, and the Green’s function, which is found by solving
∂2g
∂x2 −1
c2
∂2g
∂t2 = −δ(x −ξ)δ(t −τ),
(5.4.2)

248
Advanced Engineering Mathematics: A Second Course
where ξ denotes the position of a source that is excited at t = τ. Equation 5.4.2 expresses
the eﬀect of an impulse as it propagates from x = ξ as time increases from t = τ. For
t < τ, causality requires that g(x, t|ξ, τ) = gt(x, t|ξ, τ) = 0 if the impulse is the sole source
of the disturbance. We also require that g satisﬁes the homogeneous form of the boundary
condition satisﬁed by u.
Our derivation starts with the equations
∂2u(ξ, τ)
∂ξ2
−1
c2
∂2u(ξ, τ)
∂τ 2
= −q(ξ, τ),
(5.4.3)
and
∂2g(x, t|ξ, τ)
∂ξ2
−1
c2
∂2g(x, t|ξ, τ)
∂τ 2
= −δ(x −ξ)δ(t −τ),
(5.4.4)
where we obtain Equation 5.4.4 from a combination of Equation 5.4.2 plus reciprocity,
namely g(x, t|ξ, τ) = g(ξ, −τ|x, −t). Next we multiply Equation 5.4.3 by g(x, t|ξ, τ) and
Equation 5.4.4 by u(ξ, τ) and subtract. Integrating over ξ from a to b, where a and b are
the endpoints of the spatial domain, and over τ from 0 to t+, where t+ denotes a time
slightly later than t so that we avoid ending the integration exactly at the peak of the delta
function, we obtain
Z t+
0
Z b
a

g(x, t|ξ, τ)∂2u(ξ, τ)
∂ξ2
−u(ξ, τ)∂2g(x, t|ξ, τ)
∂ξ2
+ 1
c2

u(ξ, τ)∂2g(x, t|ξ, τ)
∂τ 2
−g(x, t|ξ, τ)∂2u(ξ, τ)
∂τ 2

dξ dτ
= u(x, t) −
Z t+
0
Z b
a
q(ξ, τ) g(x, t|ξ, τ) dξ dτ.
(5.4.5)
Because
g(x, t|ξ, τ)∂2u(ξ, τ)
∂ξ2
−u(ξ, τ)∂2g(x, t|ξ, τ)
∂ξ2
= ∂
∂ξ

g(x, t|ξ, τ)∂u(ξ, τ)
∂ξ

−∂
∂ξ

u(ξ, τ)∂g(x, t|ξ, τ)
∂ξ

,
(5.4.6)
and
g(x, t|ξ, τ)∂2u(ξ, τ)
∂τ 2
−u(ξ, τ)∂2g(x, t|ξ, τ)
∂τ 2
= ∂
∂τ

g(x, t|ξ, τ)∂u(ξ, τ)
∂τ

−∂
∂τ

u(ξ, τ)∂g(x, t|ξ, τ)
∂τ

,
(5.4.7)
we ﬁnd that
Z t+
0

g(x, t|ξ, τ)∂u(ξ, τ)
∂ξ
−u(ξ, τ)∂g(x, t|ξ, τ)
∂ξ
ξ=b
ξ=a
dτ
+ 1
c2
Z b
a

u(ξ, τ)∂g(x, t|ξ, τ)
∂τ
−g(x, t|ξ, τ)∂u(ξ, τ)
∂τ
τ=t+
τ=0
dξ
+
Z t+
0
Z b
a
q(ξ, τ) g(x, t|ξ, τ) dξ dτ = u(x, t).
(5.4.8)

Green’s Functions
249
The integrand in the ﬁrst integral is speciﬁed by the boundary conditions. In the second
integral, the integrand vanishes at t = t+ from the initial conditions on g(x, t|ξ, τ). The
limit at t = 0 is determined by the initial conditions. Hence,
u(x, t) =
Z t+
0
Z b
a
q(ξ, τ)g(x, t|ξ, τ) dξ dτ
+
Z t+
0

g(x, t|ξ, τ)∂u(ξ, τ)
∂ξ
−u(ξ, τ)∂g(x, t|ξ, τ)
∂ξ
ξ=b
ξ=a
dτ
−1
c2
Z b
a

u(ξ, 0)∂g(x, t|ξ, 0)
∂τ
−g(x, t|ξ, 0)∂u(ξ, 0)
∂τ

dξ.
(5.4.9)
Equation 5.4.9 gives the complete solution of the nonhomogeneous problem. The ﬁrst two
integrals on the right side of this equation represent the eﬀect of the source and the boundary
conditions, respectively. The last term involves the initial conditions; it can be interpreted
as asking what sort of source is needed so that the function u(x, t) starts in the desired
manner.
• Example 5.4.1
Let us apply the Green’s function technique to solve
∂2u
∂t2 = ∂2u
∂x2 ,
0 < x < 1,
0 < t,
(5.4.10)
subject to the boundary conditions u(0, t) = 0 and u(1, t) = t, 0 < t, and the initial
conditions u(x, 0) = x and ut(x, 0) = 0, 0 < x < 1.
Because there is no source term and c = 1, Equation 5.4.9 becomes
u(x, t) =
Z t
0
[g(x, t|1, τ)uξ(1, τ) −u(1, τ)gξ(x, t|1, τ)] dτ
−
Z t
0
[g(x, t|0, τ)uξ(0, τ) −u(0, τ)gξ(x, t|0, τ)] dτ
−
Z 1
0
[u(ξ, 0)gτ(x, t|ξ, 0) −g(x, t|ξ, 0)uξ(ξ, 0)] dξ.
(5.4.11)
Therefore we must ﬁrst compute the Green’s function for this problem. However, we have
already done this in Example 5.3.2 and it is given by Equation 5.3.26 with c = L = 1.
Next, we note that g(x, t|1, τ) = g(x, t|0, τ) = 0 and u(0, τ) = uτ(ξ, 0) = 0. Consequently,
Equation 5.4.11 reduces to only two nonvanishing integrals:
u(x, t) = −
Z t
0
u(1, τ)gξ(x, t|1, τ) dτ −
Z 1
0
u(ξ, 0)gτ(x, t|ξ, 0) dξ.
(5.4.12)
If we now substitute for g(x, t|ξ, τ) and reverse the order of integration and summation,
Z t
0
u(1, τ)gξ(x, t|1, τ) dτ = 2
∞
X
n=1
(−1)n sin(nπx)
Z t
0
τ sin[nπ(t −τ)] dτ
(5.4.13)
= 2t
∞
X
n=1
(−1)n sin(nπx)
Z t
0
sin[nπ(t −τ)] d(t −τ)

250
Advanced Engineering Mathematics: A Second Course
−2
∞
X
n=1
(−1)n sin(nπx)
Z t
0
(t −τ) sin[nπ(t −τ)] d(t −τ)
(5.4.14)
Z t
0
u(1, τ)gξ(x, t|1, τ) dτ = −2t
∞
X
n=1
(−1)n sin(nπx) cos[nπ(t −τ)]
nπ

t
0
(5.4.15)
−2
∞
X
n=1
(−1)n sin(nπx)
sin[nπ(t −τ)]
n2π2
−(t −τ)cos[nπ(t −τ)]
nπ

t
0
= −2t
π
∞
X
n=1
(−1)n
n
sin(nπx) + 2
π2
∞
X
n=1
(−1)n
n2
sin(nπx) sin(nπt),
(5.4.16)
and
Z 1
0
u(ξ, 0)gτ(x, t|ξ, 0) dξ = −2
∞
X
n=1
sin(nπx) cos(nπt)
Z 1
0
ξ sin(nπξ) dξ
(5.4.17)
= −2
∞
X
n=1
sin(nπx) cos(nπt)
sin(nπξ)
n2π2
−ξ cos(nπξ)
nπ

1
0
(5.4.18)
= 2
π
∞
X
n=1
(−1)n
n
sin(nπx) cos(nπt).
(5.4.19)
Substituting Equation 5.4.16 and Equation 5.4.19 into Equation 5.4.12, we ﬁnally obtain
u(x, t) = −2t
π
∞
X
n=1
(−1)n
n
sin(nπx) −2
π
∞
X
n=1
(−1)n
n
sin(nπx) cos(nπt)
+ 2
π2
∞
X
n=1
(−1)n
n2
sin(nπx) sin(nπt).
(5.4.20)
The ﬁrst summation in Equation 5.4.20 is the Fourier sine expansion for f(x) = x over the
interval 0 < x < 1. Indeed, a quick check shows that the particular solution up(x, t) = xt
satisﬁes the partial diﬀerential equation and boundary conditions.
The remaining two
summations are necessary so that u(x, 0) = x and ut(x, 0) = 0.
⊓⊔
To apply Equation 5.4.9 to other problems, we must now ﬁnd the Green’s function for
a speciﬁc domain. In the following examples we illustrate how this is done using the joint
transform method introduced in the previous section. Note that both examples given there
were for the wave equation.
• Example 5.4.2: One-dimensional wave equation in an unlimited domain
The simplest possible example of Green’s functions for the wave equation is the one-
dimensional vibrating string problem.12 In this problem the Green’s function is given by
the equation
∂2g
∂t2 −c2 ∂2g
∂x2 = c2δ(x −ξ)δ(t −τ),
(5.4.21)
12 See also Graﬀ, K. F., 1991: Wave Motion in Elastic Solids. Dover Publications, Inc., Section 1.1.8.

Green’s Functions
251
where −∞< x, ξ < ∞, and 0 < t, τ.
If the initial conditions equal zero, the Laplace
transform of Equation 5.4.21 is
d2G
dx2 −s2
c2 G = −δ(x −ξ)e−sτ,
(5.4.22)
where G(x, s|ξ, τ) is the Laplace transform of g(x, t|ξ, τ). To solve Equation 5.4.22 we take
its Fourier transform and obtain the algebraic equation
G(k, s|ξ, τ) = exp(−ikξ −sτ)
k2 + s2/c2
.
(5.4.23)
Having found the joint Laplace-Fourier transform of g(x, t|ξ, τ), we must work our way
back to the Green’s function. From the deﬁnition of the Fourier transform, we have that
G(x, s|ξ, τ) = e−sτ
2π
Z ∞
−∞
eik(x−ξ)
k2 + s2/c2 dk.
(5.4.24)
To evaluate the Fourier-type integral, Equation 5.4.24, we apply the residue theorem. See
Section 2.1. Performing the calculation,
G(x, s|ξ, τ) = c exp(−sτ −s|x −ξ|/c)
2s
.
(5.4.25)
Finally, taking the inverse Laplace transform of Equation 5.4.25,
g(x, t|ξ, τ) = c
2H(t −τ −|x −ξ|/c) ,
(5.4.26)
or
g(x, t|ξ, τ) = c
2H[c(t −τ) + (x −ξ)] H[c(t −τ) −(x −ξ)] .
(5.4.27)
We can use Equation 5.4.26 and the method of images to obtain the Green’s function
for
∂2g
∂x2 −1
c2
∂2g
∂t2 = δ(x −ξ)δ(t −τ),
0 < x, t, ξ, τ,
(5.4.28)
subject to the boundary condition g(0, t|ξ, τ) = 0.
We begin by noting that the free-space Green’s function,13 Equation 5.4.26, is the
particular solution to Equation 5.4.28. Therefore, we need only ﬁnd a homogeneous solution
f(x, t|ξ, τ) so that
g(x, t|ξ, τ) = c
2H(t −τ −|x −ξ|/c) + f(x, t|ξ, τ)
(5.4.29)
13 In electromagnetic theory, a free-space Green’s function is the particular solution of the diﬀerential
equation valid over a domain of inﬁnite extent, where the Green’s function remains bounded as we approach
inﬁnity, or satisﬁes a radiation condition there.

252
Advanced Engineering Mathematics: A Second Course
0
2
4
6
8
10
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
 x/ξ
 c(t−τ)/ξ
 g(x,t|ξ,τ)/c
Figure 5.4.1: The Green’s function g(x, t|ξ, τ)/c given by Equation 5.4.30 for the one-dimensional wave
equation for x > 0 at diﬀerent distances x/ξ and times c(t−τ) subject to the boundary condition g(0, t|ξ, τ) =
0.
satisﬁes the boundary condition at x = 0.
To ﬁnd f(x, t|ξ, τ), let us introduce a source at x = −ξ at t = τ. The corresponding
free-space Green’s function is H(t −τ −|x + ξ|/c). If, along the boundary x = 0 for any
time t, this Green’s function destructively interferes with the free-space Green’s function
associated with the source at x = ξ, then we have our solution. This will occur if our new
source has a negative sign, resulting in the combined Green’s function
g(x, t|ξ, τ) = c
2 [H(t −τ −|x −ξ|/c) −H(t −τ −|x + ξ|/c)] .
(5.4.30)
See Figure 5.4.1. Because Equation 5.4.30 satisﬁes the boundary condition, we need no
further sources.
In a similar manner, we can use Equation 5.4.26 and the method of images to ﬁnd the
Green’s function for
∂2g
∂x2 −1
c2
∂2g
∂t2 = δ(x −ξ)δ(t −τ),
0 < x, t, ξ, τ,
(5.4.31)
subject to the boundary condition gx(0, t|ξ, τ) = 0.
We begin by examining the related problem
∂2g
∂x2 −1
c2
∂2g
∂t2 = δ(x −ξ)δ(t −τ) + δ(x + ξ)δ(t −τ),
(5.4.32)
where −∞< x, ξ < ∞, and 0 < t, τ. In this particular case, we have chosen an image that
is the mirror reﬂection of δ(x −ξ). This was dictated by the fact that the Green’s function
must be an even function of x along x = 0 for any time t. In line with this argument,
g(x, t|ξ, τ) = c
2 [H(t −τ −|x −ξ|/c) + H(t −τ −|x + ξ|/c)] .
(5.4.33)
⊓⊔

Green’s Functions
253
• Example 5.4.3: Equation of telegraphy
When the vibrating string problem includes the eﬀect of air resistance, Equation 5.4.21
becomes
∂2g
∂t2 + 2γ ∂g
∂t −c2 ∂2g
∂x2 = c2δ(x −ξ)δ(t −τ),
(5.4.34)
where −∞< x, ξ < ∞, and 0 < t, τ, with the boundary conditions
lim
|x|→∞g(x, t|ξ, τ) →0
(5.4.35)
and the initial conditions
g(x, 0|ξ, τ) = gt(x, 0|ξ, τ) = 0.
(5.4.36)
Let us ﬁnd the Green’s function.
Our analysis begins by introducing an intermediate dependent variable w(x, t|ξ, τ),
where g(x, t|ξ, τ) = e−γtw(x, t|ξ, τ). Substituting for g(x, t|ξ, τ), we now have
∂2w
∂t2 −γ2w −c2 ∂2w
∂x2 = c2δ(x −ξ)δ(t −τ)eγτ.
(5.4.37)
Taking the Laplace transform of Equation 5.4.37, we obtain
d2W
dx2 −
s2 −γ2
c2

W = −δ(x −ξ)eγτ−sτ.
(5.4.38)
Using Fourier transforms as in Example 5.3.1, the solution to Equation 5.4.38 is
W(x, s|ξ, τ) = exp[−|x −ξ|
p
(s2 −γ2)/c2 + γτ −sτ]
2
p
(s2 −γ2)/c2
.
(5.4.39)
Employing tables to invert the Laplace transform and the second shifting theorem, we have
that
w(x, t|ξ, τ) = c
2eγτI0
h
γ
p
(t −τ)2 −(x −ξ)2/c2
i
H[c(t −τ) −|x −ξ|],
(5.4.40)
or
g(x, t|ξ, τ) = c
2e−γ(t−τ)I0
h
γ
p
(t −τ)2 −(x −ξ)2/c2
i
H[c(t −τ) −|x −ξ|].
(5.4.41)
Figure 5.4.2 illustrates Equation 5.4.41 when γ = 1.
⊓⊔
• Example 5.4.4
Let us solve14 the one-dimensional wave equation on an inﬁnite domain:
∂2u
∂t2 −∂2u
∂x2 = cos(ωt)δ[x −X(t)],
(5.4.42)
14 See Knowles, J. K., 1968: Propagation of one-dimensional waves from a source in random motion. J.
Acoust. Soc. Am., 43, 948–957.

254
Advanced Engineering Mathematics: A Second Course
−10
−5
0
5
10
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
0.6
 (x−ξ)/c
 t−τ
 g(x,t|ξ,τ)/c
Figure 5.4.2: The free-space Green’s function g(x, t|ξ, τ)/c for the one-dimensional equation of telegraphy
with γ = 1 at diﬀerent distances (x −ξ)/c and times t −τ.
subject to the boundary conditions
lim
|x|→∞u(x, t) →0,
0 < t,
(5.4.43)
and initial conditions
u(x, 0) = ut(x, 0) = 0,
−∞< x < ∞.
(5.4.44)
Here ω is a constant and X(t) is some function of time.
With the given boundary and initial conditions, only the ﬁrst integral in Equation 5.4.9
does not vanish. Substituting the source term q(x, t) = cos(ωt)δ[x −X(t)] and the Green’s
function given by Equation 5.4.26, we have that
u(x, t) =
Z t
0
Z ∞
−∞
q(ξ, τ)g(x, t|ξ, τ) dξ dτ
(5.4.45)
= 1
2
Z t
0
Z ∞
−∞
cos(ωτ)δ[ξ −X(τ)]H(t −τ) −|x −ξ|) dξ dτ
(5.4.46)
= 1
2
Z t
0
H[t −τ −|X(τ) −x|] cos(ωτ) dτ,
(5.4.47)
since c = 1.
Problems
1. By direct substitution, show15 that
g(x, t|0, 0) = J0(
√
xt )H(x)H(t)
15 First proven by Picard, ´E., 1894: Sur une ´equation aux d´eriv´ees partielles de la th´eorie de la propa-
gation de l’´electricit´e. Bull. Soc. Math., 22, 2–8.

Green’s Functions
255
is the free-space Green’s function governed by
∂2g
∂x∂t + 1
4g = δ(x)δ(t),
−∞< x, t < ∞.
2.
Use Equation 5.3.20 to construct the Green’s function for the one-dimensional wave
equation
∂2g
∂t2 −∂2g
∂x2 = δ(x −ξ)δ(t −τ),
0 < x, ξ < L,
0 < t, τ,
subject to the boundary conditions g(0, t|ξ, τ) = gx(L, t|ξ, τ) = 0, 0 < t, and the initial
conditions that g(x, 0|ξ, τ) = gt(x, 0|ξ, τ) = 0, 0 < x < L.
3.
Use Equation 5.3.20 to construct the Green’s function for the one-dimensional wave
equation
∂2g
∂t2 −∂2g
∂x2 = δ(x −ξ)δ(t −τ),
0 < x, ξ < L,
0 < t, τ,
subject to the boundary conditions gx(0, t|ξ, τ) = gx(L, t|ξ, τ) = 0, 0 < t, and the initial
conditions that g(x, 0|ξ, τ) = gt(x, 0|ξ, τ) = 0, 0 < x < L.
4. Use the Green’s function given by Equation 5.3.26 to write down the solution to the
wave equation utt = uxx on the interval 0 < x < L with the boundary conditions u(0, t) =
u(L, t) = 0, 0 < t, and the initial conditions u(x, 0) = cos(πx/L) and ut(x, 0) = 0, 0 < x <
L.
5. Use the Green’s function given by Equation 5.3.26 to write down the solution to the wave
equation utt = uxx on the interval 0 < x < L with the boundary conditions u(0, t) = e−t
and u(L, t) = 0, 0 < t, and the initial conditions u(x, 0) = sin(πx/L) and ut(x, 0) = 1,
0 < x < L.
6. Use the Green’s function that you found in Problem 2 to write down the solution to the
wave equation utt = uxx on the interval 0 < x < L with the boundary conditions u(0, t) = 0
and ux(L, t) = 1, 0 < t, and the initial conditions u(x, 0) = x and ut(x, 0) = 1, 0 < x < L.
7. Use the Green’s function that you found in Problem 3 to write down the solution to
the wave equation utt = uxx on the interval 0 < x < L with the boundary conditions
ux(0, t) = 1 and ux(L, t) = 0, 0 < t, and the initial conditions u(x, 0) = 1 and ut(x, 0) = 0,
0 < x < L.
8. Find the Green’s function16 governed by
∂2g
∂t2 + 2∂g
∂t −∂2g
∂x2 = δ(x −ξ)δ(t −τ),
0 < x, ξ < L,
0 < t, τ,
subject to the boundary conditions
gx(0, t|ξ, τ) = gx(L, t|ξ, τ) = 0,
0 < t,
16 ¨Ozi¸sik, M. N., and B. Vick, 1984: Propagation and reﬂection of thermal waves in a ﬁnite medium.
Int.
J. Heat Mass Transfer, 27, 1845–1854; Tang, D.-W., and N. Araki, 1996:
Propagation of non-
Fourier temperature wave in ﬁnite medium under laser-pulse heating (in Japanese). Nihon Kikai Gakkai
Rombumshu (Trans. Japan Soc. Mech. Engrs.), Ser. B, 62, 1136–1141.

256
Advanced Engineering Mathematics: A Second Course
and the initial conditions
g(x, 0|ξ, τ) = gt(x, 0|ξ, τ) = 0,
0 < x < L.
Step 1: If the Green’s function can be written as the Fourier half-range cosine series
g(x, t|ξ, τ) = 1
LG0(t|τ) + 2
L
∞
X
n=1
Gn(t|τ) cos
nπx
L

,
so that it satisﬁes the boundary conditions, show that Gn(t|τ) is governed by
G′′
n + 2G′
n + n2π2
L2 Gn = cos
nπξ
L

δ(t −τ),
0 ≤n.
Step 2: Show that
G0(t|τ) = e−(t−τ) sinh(t −τ)H(t −τ),
and
Gn(t|τ) = cos
nπξ
L

e−(t−τ) sin[βn(t −τ)]
βn
H(t −τ),
1 ≤n,
where βn =
p
(nπ/L)2 −1.
Step 3: Combine the results from Steps 1 and 2 and show that
g(x, t|ξ, τ) = e−(t−τ) sinh(t −τ)H(t −τ)/L
+ 2e−(t−τ)H(t −τ)/L
×
∞
X
n=1
sin[βn(t −τ)]
βn
cos
nπξ
L

cos
nπx
L

.
5.5 HEAT EQUATION
In this section we present the Green’s function17 for the heat equation
∂u
∂t −a2 ∂2u
∂x2 = q(x, t),
(5.5.1)
where t denotes time, x is the position, a2 is the diﬀusivity, and q(x, t) is the source den-
sity. In addition to Equation 5.5.1, boundary conditions must be speciﬁed to ensure the
uniqueness of solution; the most common ones are Dirichlet, Neumann, and Robin (a linear
combination of the ﬁrst two). An initial condition u(x, t = t0) is also needed.
The heat equation diﬀers in many ways from the wave equation and the Green’s function
must, of course, manifest these diﬀerences. The most notable one is the asymmetry of the
17 See also Carslaw, H. S., and J. C. Jaeger, 1959: Conduction of Heat in Solids.
Clarendon Press,
Chapter 14; Beck, J. V., K. D. Cole, A. Haji-Sheikh, and B. Litkouhi, 1992: Heat Conduction Using
Green’s Functions. Hemisphere Publishing Corp., 523 pp.; ¨Ozi¸sik, M. N., 1993: Heat Conduction. John
Wiley & Sons, Inc., Chapter 6.

Green’s Functions
257
heat equation with respect to time. This merely reﬂects the fact that the heat equation
diﬀerentiates between past and future as entropy continually increases.
We begin by proving that we can express the solution to Equation 5.5.1 in terms of
boundary conditions, the initial condition, and the Green’s function, which is found by
solving
∂g
∂t −a2 ∂2g
∂x2 = δ(x −ξ)δ(t −τ),
(5.5.2)
where ξ denotes the position of the source. From causality18 we know that g(x, t|ξ, τ) = 0
if t < τ. We again require that the Green’s function g(x, t|ξ, τ) satisﬁes the homogeneous
form of the boundary condition on u(x, t). For example, if u satisﬁes a homogeneous or
nonhomogeneous Dirichlet condition, then the Green’s function will satisfy the correspond-
ing homogeneous Dirichlet condition. Although we will focus on the mathematical aspects
of the problem, Equation 5.5.2 can be given the physical interpretation of the temperature
distribution within a medium when a unit of heat is introduced at ξ at time τ.
We now establish that the solution to the nonhomogeneous heat equation can be ex-
pressed in terms of the Green’s function, boundary conditions, and the initial condition.
We begin with the equations
a2 ∂2u(ξ, τ)
∂ξ2
−∂u(ξ, τ)
∂τ
= −q(ξ, τ),
(5.5.3)
and
a2 ∂2g(x, t|ξ, τ)
∂ξ2
+ ∂g(x, t|ξ, τ)
∂τ
= −δ(x −ξ)δ(t −τ).
(5.5.4)
As we did in the previous section, we multiply Equation 5.5.3 by g(x, t|ξ, τ) and Equation
5.5.4 by u(ξ, τ) and subtract. Integrating over ξ from a to b, where a and b are the endpoints
of the spatial domain, and over τ from 0 to t+, where t+ denotes a time slightly later than
t so that we avoid ending the integration exactly at the peak of the delta function, we ﬁnd
a2
Z t+
0
Z b
a

u(ξ, τ)∂2g(x, t|ξ, τ)
∂ξ2
−g(x, t|ξ, τ)∂2u(ξ, τ)
∂ξ2

dξ dτ
+
Z t+
0
Z b
a

u(ξ, τ)∂g(x, t|ξ, τ)
∂τ
+ g(x, t|ξ, τ)∂u(ξ, τ)
∂τ

dξ dτ
=
Z t+
0
Z b
a
q(ξ, τ)g(x, t|ξ, τ) dξ dτ −u(x, t).
(5.5.5)
Applying Equation 5.4.6 and performing the time integration in the second integral, we
ﬁnally obtain
u(x, t) =
Z t+
0
Z b
a
q(ξ, τ)g(x, t|ξ, τ) dξ dτ
+ a2
Z t+
0

g(x, t|ξ, τ)∂u(ξ, τ)
∂ξ
−u(ξ, τ)∂g(x, t|ξ, τ)
∂ξ
ξ=b
ξ=a
dτ
+
Z b
a
u(ξ, 0)g(x, t|ξ, 0) dξ,
(5.5.6)
18 The principle stating that an event cannot precede its cause.

258
Advanced Engineering Mathematics: A Second Course
where we used g(x, t|ξ, t+) = 0. The ﬁrst two terms in Equation 5.5.6 represent the familiar
eﬀects of volume sources and boundary conditions, while the third term includes the eﬀects
of the initial data.
• Example 5.5.1: One-dimensional heat equation in an unlimited domain
The Green’s function for the one-dimensional heat equation is governed by
∂g
∂t −a2 ∂2g
∂x2 = δ(x −ξ)δ(t −τ),
−∞< x, ξ < ∞,
0 < t, τ,
(5.5.7)
subject to the boundary conditions lim|x|→∞g(x, t|ξ, τ) →0, and the initial condition
g(x, 0|ξ, τ) = 0. Let us ﬁnd g(x, t|ξ, τ).
We begin by taking the Laplace transform of Equation 5.5.7 and ﬁnd that
d2G
dx2 −s
a2 G = −δ(x −ξ)
a2
e−sτ.
(5.5.8)
Next, we take the Fourier transform of Equation 5.5.8 so that
(k2 + b2)G(k, s|ξ, τ) = e−ikξe−sτ
a2
,
(5.5.9)
where G(k, s|ξ, τ) is the Fourier transform of G(x, s|ξ, τ) and b2 = s/a2.
To ﬁnd G(x, s|ξ, τ), we use the inversion integral
G(x, s|ξ, τ) = e−sτ
2πa2
Z ∞
−∞
ei(x−ξ)k
k2 + b2 dk.
(5.5.10)
Transforming Equation 5.5.10 into a closed contour via Jordan’s lemma, we evaluate it by
the residue theorem and ﬁnd that
G(x, s|ξ, τ) = e−|x−ξ|√s/a−sτ
2a √s
.
(5.5.11)
From a table of Laplace transforms we ﬁnally obtain
g(x, t|ξ, τ) =
H(t −τ)
p
4πa2(t −τ)
exp

−(x −ξ)2
4a2(t −τ)

,
(5.5.12)
after applying the second shifting theorem.
⊓⊔
The primary use of the fundamental or free-space Green’s function19 is as a particular
solution to the Green’s function problem. For this reason, it is often called the fundamental
19 In electromagnetic theory, a free-space Green’s function is the particular solution of the diﬀerential
equation valid over a domain of inﬁnite extent, where the Green’s function remains bounded as we approach
inﬁnity, or satisﬁes a radiation condition there.

Green’s Functions
259
0
0.5
1
1.5
2
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
 x
 a2(t−τ)
 [a2(t−τ)]1/2 g(x,t|ξ,τ)
Figure 5.5.1: The Green’s function, Equation 5.5.17, for the one-dimensional heat equation on the semi-
inﬁnite domain 0 < x < ∞, and 0 ≤t −τ, when the left boundary condition is gx(0, t|ξ, τ) = 0 and
ξ = 0.5.
heat conduction solution. Consequently, we usually must ﬁnd a homogeneous solution so
that the sum of the free-space Green’s function plus the homogeneous solution satisﬁes any
boundary conditions. The following examples show some commonly employed techniques.
• Example 5.5.2
Let us ﬁnd the Green’s function for the following problem:
∂g
∂t −a2 ∂2g
∂x2 = δ(x −ξ)δ(t −τ),
0 < x, ξ < ∞,
0 < t, τ,
(5.5.13)
subject to the boundary conditions g(0, t|ξ, τ) = 0, limx→∞g(x, t|ξ, τ) →0, and the initial
condition g(x, 0|ξ, τ) = 0. From the boundary condition g(0, t|ξ, τ) = 0, we deduce that
g(x, t|ξ, τ) must be an odd function in x over the open interval (−∞, ∞). We ﬁnd this
Green’s function by introducing an image source of −δ(x + ξ) and resolving Equation 5.5.7
with the source δ(x−ξ)δ(t−τ)−δ(x+ξ)δ(t−τ). Because Equation 5.5.7 is linear, Equation
5.5.12 gives the solution for each delta function and the Green’s function for Equation 5.5.13
is
g(x, t|ξ, τ) =
H(t −τ)
p
4πa2(t −τ)

exp

−(x −ξ)2
4a2(t −τ)

−exp

−(x + ξ)2
4a2(t −τ)

(5.5.14)
=
H(t −τ)
p
πa2(t −τ)
exp

−x2 + ξ2
4a2(t −τ)

sinh

xξ
2a2(t −τ)

.
(5.5.15)
In a similar manner, if the boundary condition at x = 0 changes to gx(0, t|ξ, τ) = 0,
then Equation 5.5.14 through Equation 5.5.15 become
g(x, t|ξ, τ) =
H(t −τ)
p
4πa2(t −τ)

exp

−(x −ξ)2
4a2(t −τ)

+ exp

−(x + ξ)2
4a2(t −τ)

(5.5.16)
=
H(t −τ)
p
πa2(t −τ)
exp

−x2 + ξ2
4a2(t −τ)

cosh

xξ
2a2(t −τ)

.
(5.5.17)
Figure 5.5.1 illustrates Equation 5.5.17 for the special case when ξ = 0.5.
⊓⊔

260
Advanced Engineering Mathematics: A Second Course
• Example 5.5.3: One-dimensional heat equation on the interval 0 < x < L
Here we ﬁnd the Green’s function for the one-dimensional heat equation over the in-
terval 0 < x < L associated with the problem
∂u
∂t −a2 ∂2u
∂x2 = f(x, t),
0 < x < L,
0 < t,
(5.5.18)
where a2 is the diﬀusivity constant.
To ﬁnd the Green’s function for this problem, consider the following problem:
∂g
∂t −a2 ∂2g
∂x2 = δ(x −ξ)δ(t −τ),
0 < x, ξ < L,
0 < t, τ,
(5.5.19)
with the boundary conditions
α1g(0, t|ξ, τ) + β1gx(0, t|ξ, τ) = 0,
0 < t,
(5.5.20)
and
α2g(L, t|ξ, τ) + β2gx(L, t|ξ, τ) = 0,
0 < t,
(5.5.21)
and the initial condition
g(x, 0|ξ, τ) = 0,
0 < x < L.
(5.5.22)
We begin by taking the Laplace transform of Equation 5.5.19 and ﬁnd that
d2G
dx2 −s
a2 G = −δ(x −ξ)
a2
e−sτ,
0 < x < L,
(5.5.23)
with
α1G(0, s|ξ, τ) + β1G′(0, s|ξ, τ) = 0,
(5.5.24)
and
α2G(L, s|ξ, τ) + β2G′(L, s|ξ, τ) = 0.
(5.5.25)
Problems similar to Equation 5.5.23 through Equation 5.5.25 were considered in Section
5.2. Applying this technique of eigenfunction expansions, we have that
G(x, s|ξ, τ) = e−sτ
∞
X
n=1
ϕn(ξ)ϕn(x)
s + a2k2n
,
(5.5.26)
where ϕn(x) is the nth orthonormal eigenfunction to the regular Sturm-Liouville problem
ϕ
′′(x) + k2ϕ(x) = 0,
0 < x < L,
(5.5.27)
subject to the boundary conditions
α1ϕ(0) + β1ϕ′(0) = 0,
(5.5.28)
and
α2ϕ(L) + β2ϕ′(L) = 0.
(5.5.29)

Green’s Functions
261
Taking the inverse of Equation 5.5.26, we have that
g(x, t|ξ, τ) =
" ∞
X
n=1
ϕn(ξ)ϕn(x)e−k2
na2(t−τ)
#
H(t −τ).
(5.5.30)
For example, let us ﬁnd the Green’s function for the heat equation on a ﬁnite domain
∂g
∂t −a2 ∂2g
∂x2 = δ(x −ξ)δ(t −τ),
0 < x, ξ < L,
0 < t, τ,
(5.5.31)
with the boundary conditions g(0, t|ξ, τ) = g(L, t|ξ, τ) = 0, 0 < t, and the initial condition
g(x, 0|ξ, τ) = 0, 0 < x < L.
The Sturm-Liouville problem is
ϕ′′(x) + k2ϕ(x) = 0,
0 < x < L,
(5.5.32)
with the boundary conditions ϕ(0) = ϕ(L) = 0. The nth orthonormal eigenfunction to
Equation 5.5.32 is
ϕn(x) =
r
2
L sin
nπx
L

.
(5.5.33)
Substituting Equation 5.5.33 into Equation 5.5.30, we ﬁnd that
g(x, t|ξ, τ) = 2
L
( ∞
X
n=1
sin
nπξ
L

sin
nπx
L

e−a2n2π2(t−τ)/L2
)
H(t −τ).
(5.5.34)
On the other hand, the Green’s function for the heat equation on a ﬁnite domain
governed by
∂g
∂t −a2 ∂2g
∂x2 = δ(x −ξ)δ(t −τ),
0 < x, ξ < L,
0 < t, τ,
(5.5.35)
with the boundary conditions
gx(0, t|ξ, τ) = 0,
gx(L, t|ξ, τ) + hg(L, t|ξ, τ) = 0,
0 < t,
(5.5.36)
and the initial condition g(x, 0|ξ, τ) = 0, 0 < x < L, yields the Sturm-Liouville problem
that we must now solve:
ϕ′′(x) + λϕ(x) = 0,
ϕ′(0) = 0,
ϕ′(L) + hϕ(L) = 0.
(5.5.37)
The nth orthonormal eigenfunction for Equation 5.5.37 is
ϕn(x) =
s
2(k2n + h2)
L(k2n + h2) + h cos(knx),
(5.5.38)

262
Advanced Engineering Mathematics: A Second Course
where kn is the nth root of k tan(kL) = h. We also used the identity that (k2
n+h2) sin2(knh)
= h2. Substituting Equation 5.5.38 into Equation 5.5.30, we ﬁnally obtain
g(x, t|ξ, τ) = 2
L
( ∞
X
n=1
[(knL)2 + (hL)2] cos(knξ) cos(knx)
(knL)2 + (hL)2 + hL
e−a2k2
n(t−τ)
)
H(t −τ).
(5.5.39)
⊓⊔
• Example 5.5.4
Let us use Green’s functions to solve the heat equation
∂u
∂t = a2 ∂2u
∂x2 ,
0 < x < L,
0 < t,
(5.5.40)
subject to the boundary conditions
u(0, t) = 0,
u(L, t) = t,
0 < t,
(5.5.41)
and the initial condition
u(x, 0) = 0,
0 < x < L.
(5.5.42)
Because there is no source term, Equation 5.5.6 simpliﬁes to
u(x, t) = a2
Z t
0
[g(x, t|L, τ)uξ(L, τ) −u(L, τ)gξ(x, t|L, τ)] dτ
(5.5.43)
−a2
Z t
0
[g(x, t|0, τ)uξ(0, τ) −u(0, τ)gξ(x, t|0, τ)] dτ +
Z L
0
u(ξ, 0)g(x, t|ξ, 0) dξ.
The Green’s function that should be used here is the one given by Equation 5.5.34. Further
simpliﬁcation occurs by noting that g(x, t|0, τ) = g(x, t|L, τ) = 0 as well as u(0, τ) =
u(ξ, 0) = 0. Therefore we are left with the single integral
u(x, t) = −a2
Z t
0
u(L, τ)gξ(x, t|L, τ) dτ.
(5.5.44)
Upon substituting for g(x, t|L, τ) and reversing the order of integration and summation,
u(x, t) = −2πa2
L2
∞
X
n=1
(−1)nn sin
nπx
L
 Z t
0
τ exp
a2n2π2
L2
(τ −t)

dτ
(5.5.45)
= −2L2
a2π3
∞
X
n=1
(−1)n
n3
sin
nπx
L

exp
a2n2π2
L2
(τ −t)
 a2n2π2τ
L2
−1

t
0
(5.5.46)
= −2L2
a2π3
∞
X
n=1
(−1)n
n3
sin
nπx
L
 a2n2π2t
L2
−1 + exp

−a2n2π2t
L2

.
(5.5.47)
Figure 5.5.2 illustrates Equation 5.5.47. This solution could also have been found using
Duhamel’s integral.
⊓⊔

Green’s Functions
263
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.5
1
 x/L
 a2t/L2
 a2u(x,t)/L2
Figure 5.5.2: The temperature distribution within a bar when the temperature is initially at zero and
then the ends are held at zero at x = 0 and t at x = L.
• Example 5.5.5: Heat equation within a cylinder
In this example, we ﬁnd the Green’s function for the heat equation in cylindrical
coordinates
∂g
∂t −a2
r
∂
∂r

r∂g
∂r

= δ(r −ρ)δ(t −τ)
2πr
,
0 < r, ρ < b,
0 < t, τ,
(5.5.48)
subject to the boundary conditions limr→0 |g(r, t|ρ, τ)| < ∞, g(b, t|ρ, τ) = 0, and the initial
condition g(r, 0|ρ, τ) = 0.
As usual, we begin by taking the Laplace transform of Equation 5.5.48, or
1
r
d
dr

rdG
dr

−s
a2 G = −e−sτ
2πa2rδ(r −ρ).
(5.5.49)
Next we re-express δ(r −ρ)/r as the Fourier-Bessel expansion
δ(r −ρ)
2πr
=
∞
X
n=1
AnJ0(knr/b),
(5.5.50)
where kn is the nth root of J0(k) = 0, and
An =
2
b2J2
1(kn)
Z b
0
δ(r −ρ)
2πr
J0(knr/b) r dr = J0(knρ/b)
πb2J2
1(kn)
(5.5.51)
so that
1
r
d
dr

rdG
dr

−s
a2 G = −e−sτ
πa2b2
∞
X
n=1
J0(knρ/b)J0(knr/b)
J2
1(kn)
.
(5.5.52)

264
Advanced Engineering Mathematics: A Second Course
0
0.5
1
0
0.05
0.1
0.15
0.2
0
1
2
3
4
5
 r/b
 a2(t−τ)/b2
 b2 g(r,t|ρ,τ)
Figure 5.5.3: The Green’s function, Equation 5.5.54, for the axisymmetric heat equation, Equation 5.5.48,
with a Dirichlet boundary condition at r = b. Here ρ/b = 0.3 and the graph starts at a2(t −τ)/b2 = 0.001
to avoid the delta function at t −τ = 0.
The solution to Equation 5.5.52 is
G(r, s|ρ, τ) = e−sτ
π
∞
X
n=1
J0(knρ/b)J0(knr/b)
(sb2 + a2k2n)J2
1(kn) .
(5.5.53)
Taking the inverse of Equation 5.5.53 and applying the second shifting theorem,
g(r, t|ρ, τ) = H(t −τ)
πb2
∞
X
n=1
J0(knρ/b)J0(knr/b)
J2
1(kn)
e−a2k2
n(t−τ)/b2.
(5.5.54)
See Figure 5.5.3.
If we modify the boundary condition at r = b so that it now reads
gr(b, t|ρ, τ) + hg(b, t|ρ, τ) = 0,
(5.5.55)
where h ≥0, our analysis now leads to
g(r, t|ρ, τ) = H(t −τ)
πb2
∞
X
n=1
J0(knρ/b)J0(knr/b)
J2
0(kn) + J2
1(kn)
e−a2k2
n(t−τ)/b2,
(5.5.56)
where kn are the positive roots of k J1(k) −hb J0(k) = 0. If h = 0, we must add 1/(πb2) to
Equation 5.5.56.
Problems
1. Find the free-space Green’s function for the linearized Ginzburg-Landau equation20
∂g
∂t + v ∂g
∂x −ag −b∂2g
∂x2 = δ(x −ξ)δ(t −τ), −∞< x, ξ < ∞, 0 < t, τ,
20 See Deissler, R. J., 1985: Noise-sustained structure, intermittency, and the Ginzburg-Landau equation.
J. Stat. Phys., 40, 371–395.

Green’s Functions
265
with b > 0.
Step 1: Taking the Laplace transform of the partial diﬀerential equation, show that it
reduces to the ordinary diﬀerential equation
bd2G
dx2 −v dG
dx + aG −sG = −δ(x −ξ)e−sτ.
Step 2: Using Fourier transforms, show that
G(x, s|ξ, τ) = e−sτ
2π
Z ∞
−∞
eik(x−ξ)
s + ikv + bk2 −a dk,
or
g(x, t|ξ, τ) = ea(t−τ)
π
H(t −τ)
Z ∞
0
e−b(t−τ)k2 cos{k[x −ξ −v(t −τ)]} dk.
Step 3: Evaluate the second integral and show that
g(x, t|ξ, τ) = ea(t−τ)H(t −τ)
2
p
πb(t −τ)
exp

−[x −ξ −v(t −τ)]2
4b(t −τ)

.
2. Use Green’s functions to show that the solution21 to
∂u
∂t = a2 ∂2u
∂x2 ,
0 < x, t,
subject to the boundary conditions
u(0, t) = 0,
lim
x→∞u(x, t) →0,
0 < t,
and the initial condition
u(x, 0) = f(x),
0 < x < ∞,
is
u(x, t) = e−x2/(4a2t)
a
√
πt
Z ∞
0
f(τ) sinh
 xτ
2a2t

e−τ 2/(4a2t) dτ.
3.
Use Equation 5.5.30 to construct the Green’s function for the one-dimensional heat
equation gt −gxx = δ(x −ξ)δ(t −τ) for 0 < x < L, 0 < t, with the initial condition that
g(x, 0|ξ, τ) = 0, 0 < x < L, and the boundary conditions that g(0, t|ξ, τ) = gx(L, t|ξ, τ) = 0
for 0 < t. Assume that L ̸= π.
4.
Use Equation 5.5.30 to construct the Green’s function for the one-dimensional heat
equation gt −gxx = δ(x −ξ)δ(t −τ) for 0 < x < L, 0 < t, with the initial condition that
21 See Gilev, S. D., and T. Yu. Mikha˘ılova, 1996: Current wave in shock compression of matter in a
magnetic ﬁeld. Tech. Phys., 41, 407–411.

266
Advanced Engineering Mathematics: A Second Course
g(x, 0|ξ, τ) = 0, 0 < x < L, and the boundary conditions that gx(0, t|ξ, τ) = gx(L, t|ξ, τ) = 0
for 0 < t.
5.
Use Equation 5.5.43 and the Green’s function given by Equation 5.5.34 to ﬁnd the
solution to the heat equation ut = uxx for 0 < x < L, 0 < t, with the initial data
u(x, 0) = 1, 0 < x < L, and the boundary conditions u(0, t) = e−t and u(L, t) = 0 when
0 < t.
6. Use Equation 5.5.43 and the Green’s function that you found in Problem 3 to ﬁnd the
solution to the heat equation ut = uxx for 0 < x < L, 0 < t, with the initial data u(x, 0) = 1,
0 < x < L, and the boundary conditions u(0, t) = sin(t) and ux(L, t) = 0 when 0 < t.
7. Use Equation 5.5.43 and the Green’s function that you found in Problem 4 to ﬁnd the
solution to the heat equation ut = uxx for 0 < x < L, 0 < t, with the initial data u(x, 0) = 1,
0 < x < L, and the boundary conditions ux(0, t) = 1 and ux(L, t) = 0 when 0 < t.
8. Find the Green’s function for
∂g
∂t −a2 ∂2g
∂x2 + a2k2g = δ(x −ξ)δ(t −τ),
0 < x, ξ < L,
0 < t, τ,
subject to the boundary conditions
g(0, t|ξ, τ) = gx(L, t|ξ, τ) = 0,
0 < t,
and the initial condition
g(x, 0|ξ, τ) = 0,
0 < x < L,
where a and k are real constants.
5.6 HELMHOLTZ’S EQUATION
In the previous sections, we sought solutions to the heat and wave equations via Green’s
functions. In this section, we turn to the reduced wave equation
∂2u
∂x2 + ∂2u
∂y2 + λu = −f(x, y).
(5.6.1)
Equation 5.6.1, generally known as Helmholtz’s equation, includes the special case of Pois-
son’s equation when λ = 0. Poisson’s equation has a special place in the theory of Green’s
functions because George Green (1793–1841) invented his technique for its solution.
The reduced wave equation arises during the solution of the harmonically forced wave
equation22 by separation of variables. In one spatial dimension, the problem is
∂2u
∂x2 −1
c2
∂2u
∂t2 = −f(x)e−iωt.
(5.6.2)
Equation 5.6.2 occurs, for example, in the mathematical analysis of a stretched string over
some interval subject to an external, harmonic forcing. Assuming that u(x, t) is bounded
22 See, for example, Graﬀ, K. F., 1991: Wave Motion in Elastic Solids. Dover Publications, Inc., Section
1.4.

Green’s Functions
267
everywhere, we seek solutions of the form u(x, t) = y(x)e−iωt.
Upon substituting this
solution into Equation 5.6.2 we obtain the ordinary diﬀerential equation
y′′ + k2
0y = −f(x),
(5.6.3)
where k2
0 = ω2/c2. This is an example of the one-dimensional Helmholtz equation.
Let us now use Green’s functions to solve the Helmholtz equation, Equation 5.6.1,
where the Green’s function is given by the Helmholtz equation
∂2g
∂x2 + ∂2g
∂y2 + λg = −δ(x −ξ)δ(y −η).
(5.6.4)
The most commonly encountered boundary conditions are
• the Dirichlet boundary condition, where g vanishes on the boundary,
• the Neumann boundary condition, where the normal gradient of g vanishes on the bound-
ary, and
• the Robin boundary condition, which is the linear combination of the Dirichlet and Neu-
mann conditions.
We begin by multiplying Equation 5.6.1 by g(x, y|ξ, η) and Equation 5.6.4 by u(x, y), sub-
tract and integrate over the region a < x < b, c < y < d. We ﬁnd that
u(ξ, η) =
Z d
c
Z b
a

g(x, y|ξ, η)
∂2u(x, y)
∂x2
+ ∂2u(x, y)
∂y2

−u(x, y)
∂2g(x, y|ξ, η)
∂x2
+ ∂2g(x, y|ξ, η)
∂y2

dx dy
+
Z d
c
Z b
a
f(x, y)g(x, y|ξ, η) dx dy
(5.6.5)
=
Z d
c
Z b
a
 ∂
∂x

g(x, y|ξ, η)∂u(x, y)
∂x

−∂
∂x

u(x, y)∂g(x, y|ξ, η)
∂x

dx dy
+
Z d
c
Z b
a
 ∂
∂y

g(x, y|ξ, η)∂u(x, y)
∂y

−∂
∂y

u(x, y)∂g(x, y|ξ, η)
∂y

dx dy
+
Z d
c
Z b
a
f(x, y)g(x, y|ξ, η) dx dy
(5.6.6)
=
Z d
c

g(x, y|ξ, η)∂u(x, y)
∂x
−u(x, y)∂g(x, y|ξ, η)
∂x
x=b
x=a
dy
+
Z b
a

g(x, y|ξ, η)∂u(x, y)
∂y
−u(x, y)∂g(x, y|ξ, η)
∂y
y=d
y=c
dx
+
Z d
c
Z b
a
f(x, y)g(x, y|ξ, η) dx dy.
(5.6.7)
Because (ξ, η) is an arbitrary point inside the rectangle, we denote it in general by (x, y).
Furthermore, the variable (x, y) is now merely a dummy integration variable that we now
denote by (ξ, η).
Upon making these substitutions and using the symmetry condition

268
Advanced Engineering Mathematics: A Second Course
g(x, y|ξ, η) = g(ξ, η|x, y), we have that
u(x, y) =
Z d
c

g(x, y|ξ, η)∂u(ξ, η)
∂ξ
−u(ξ, η)∂g(x, y|ξ, η)
∂ξ
ξ=b
ξ=a
dη
+
Z b
a

g(x, y|ξ, η)∂u(ξ, η)
∂η
−u(ξ, η)∂g(x, y|ξ, η)
∂η
η=d
η=c
dξ
+
Z d
c
Z b
a
f(ξ, η)g(x, y|ξ, η) dξ dη.
(5.6.8)
Equation 5.6.8 shows that the solution of Helmholtz’s equation depends upon the sources
inside the rectangle and values of u(x, y) and (∂u/∂x, ∂u/∂y) along the boundary. On the
other hand, we must still ﬁnd the particular Green’s function for a given problem; this
Green’s function depends directly upon the boundary conditions. At this point, we work
out several special cases.
1. Nonhomogeneous Helmholtz equation
and homogeneous Dirichlet boundary conditions
In this case, let us assume that we can ﬁnd a Green’s function that also satisﬁes the
same Dirichlet boundary conditions as u(x, y). Once the Green’s function is found, then
Equation 5.6.8 reduces to
u(x, y) =
Z d
c
Z b
a
f(ξ, η)g(x, y|ξ, η) dξdη.
(5.6.9)
A possible source of diﬃculty would be the nonexistence of the Green’s function. From
our experience in Section 5.2, we know that this will occur if λ equals one of the eigenvalues
of the corresponding homogeneous problem. An example of this occurs in acoustics when
the Green’s function for the Helmholtz equation does not exist at resonance.
2. Homogeneous Helmholtz equation
and nonhomogeneous Dirichlet boundary conditions
In this particular case, f(x, y) = 0. For convenience, let us use the Green’s function
from the previous example so that g(x, y|ξ, η) = 0 along all of the boundaries. Under these
conditions, Equation 5.6.8 becomes
u(x, y) = −
Z b
a
u(ξ, η)∂g(x, y|ξ, η)
∂η

η=d
η=c
dξ −
Z d
c
u(ξ, η)∂g(x, y|ξ, η)
∂ξ

ξ=b
ξ=a
dη.
(5.6.10)
Consequently, the solution is determined once we compute the normal gradient of the
Green’s function along the boundary.
3. Nonhomogeneous Helmholtz equation
and homogeneous Neumann boundary conditions
If we require that u(x, y) satisﬁes the nonhomogeneous Helmholtz equation with homo-
geneous Neumann boundary conditions, then the governing equations are Equation 5.6.1

Green’s Functions
269
and the boundary conditions ux = 0 along x = a and x = b, and uy = 0 along y = c and
y = d. Integrating Equation 5.6.1, we have that
Z d
c
∂u(b, y)
∂x
−∂u(a, y)
∂x

dy +
Z b
a
∂u(x, d)
∂y
−∂u(x, c)
∂y

dx
+ λ
Z d
c
Z b
a
u(x, y) dx dy = −
Z d
c
Z b
a
f(x, y) dx dy.
(5.6.11)
Because the ﬁrst two integrals in Equation 5.6.11 must vanish in the case of homogeneous
Neumann boundary conditions, this equation cannot be satisﬁed if λ = 0 unless
Z d
c
Z b
a
f(x, y) dx dy = 0.
(5.6.12)
A physical interpretation of Equation 5.6.12 is as follows: Consider the physical process
of steady-state heat conduction within a rectangular region. The temperature u(x, y) is
given by Poisson’s equation
∂2u
∂x2 + ∂2u
∂y2 = −f(x, y),
(5.6.13)
where f(x, y) is proportional to the density of the heat sources and sinks. The boundary
conditions ux(a, y) = ux(b, y) = 0 and uy(x, c) = uy(x, d) = 0 imply that there is no heat
exchange across the boundary. Consequently, no steady-state temperature distribution can
exist unless the heat sources are balanced by heat sinks. This balance of heat sources and
sinks is given by Equation 5.6.12.
Having provided an overview of how Green’s functions can be used to solve Poisson
and Helmholtz equations, let us now determine several of them for commonly encountered
domains.
• Example 5.6.1: Free-space Green’s function for the one-dimensional Helmholtz equation
Let us ﬁnd the Green’s function for the one-dimensional Helmholtz equation
g′′ + k2
0g = −δ(x −ξ),
−∞< x, ξ < ∞.
(5.6.14)
If we solve Equation 5.6.14 by piecing together homogeneous solutions, then
g(x|ξ) = Ae−ik0(x−ξ) + Beik0(x−ξ),
(5.6.15)
for x < ξ, while
g(x|ξ) = Ce−ik0(x−ξ) + Deik0(x−ξ),
(5.6.16)
for ξ < x.
Let us examine Equation 5.6.15 more closely. The solution represents two propagating
waves. Because x < ξ, the ﬁrst term is a wave propagating out to inﬁnity, while the second
term gives a wave propagating in from inﬁnity. This is seen most clearly by including the
e−iωt term into Equation 5.6.15, or
g(x|ξ)e−iωt = Ae−ik0(x−ξ)−iωt + Beik0(x−ξ)−iωt.
(5.6.17)
Because we have a source only at x = ξ, solutions that represent waves originating at inﬁnity
are nonphysical and we must discard them. This requirement that there are only outwardly

270
Advanced Engineering Mathematics: A Second Course
0
x
x
iy
k
-k
-k
k
iy
(a) Contour for x >
(b) Contour for x < 
0
0
ξ
ξ
0
Figure 5.6.1: Contour used to evaluate Equation 5.6.21.
propagating wave solutions is commonly called Sommerfeld’s radiation condition.23 Similar
considerations hold for Equation 5.6.16 and we must take C = 0.
To evaluate A and D, we use the continuity conditions on the Green’s function:
g(ξ+|ξ) = g(ξ−|ξ),
and
g′(ξ+|ξ) −g′(ξ−|ξ) = −1,
(5.6.18)
or
A = D,
and
ik0D + ik0A = −1.
(5.6.19)
Therefore,
g(x|ξ) =
i
2k0
eik0|x−ξ|.
(5.6.20)
We can also solve Equation 5.6.14 by Fourier transforms. Assuming that the Fourier
transform of g(x|ξ) exists and denoting it by G(k|ξ), we ﬁnd that
G(k|ξ) =
e−ikξ
k2 −k2
0
,
and
g(x|ξ) = 1
2π
Z ∞
−∞
eik(x−ξ)
k2 −k2
0
dk.
(5.6.21)
Immediately we see that there is a problem with the singularities lying on the path of
integration at k = ±k0. How do we avoid them?
There are four possible ways that we might circumvent the singularities. One of them
is shown in Figure 5.6.1. Applying Jordan’s lemma to close the line integral along the real
axis (as shown in Figure 5.6.1),
g(x|ξ) = 1
2π
I
C
eiz(x−ξ)
z2 −k2
0
dz.
(5.6.22)
23 Sommerfeld, A., 1912: Die Greensche Funktion der Schwingungsgleichung.
Jahresber.
Deutschen
Math.- Vereinung, 21, 309–353.

Green’s Functions
271
Free-Space Green’s Function for the Poisson and Helmholtz Equations
Dimension
Poisson Equation
Helmholtz Equation
One
no solution
g(x|ξ) =
i
2k0
eik0|x−ξ|
Two
g(x, y|ξ, η) = −ln(r)
2π
g(x, y|ξ, η) = i
4H(1)
0 (k0r)
r =
p
(x −ξ)2 + (y −η)2
Note: For the Helmholtz equation, we have taken the temporal forcing to be e−iωt and k0 = ω/c.
For x < ξ,
g(x|ξ) = −i Res
eiz(x−ξ)
z2 −k2
0
; −k0

=
i
2k0
e−ik0(x−ξ),
(5.6.23)
while
g(x|ξ) = i Res
eiz(x−ξ)
z2 −k2
0
; k0

=
i
2k0
eik0(x−ξ),
(5.6.24)
for x > ξ. A quick check shows that these solutions agree with Equation 5.6.20. If we try
the three other possible paths around the singularities, we obtain incorrect solutions.
⊓⊔
• Example 5.6.2: Free-space Green’s function for the two-dimensional Helmholtz equation
At this point, we have found two forms of the free-space Green’s function for the one-
dimensional Helmholtz equation. The ﬁrst form is the analytic solution, Equation 5.6.20,
while the second is the integral representation, Equation 5.6.21, where the line integration
along the real axis is shown in Figure 5.6.1.
In the case of two dimensions, the Green’s function24 for the Helmholtz equation sym-
metric about the point (ξ, η) is the solution of the equation
d2g
dr2 + 1
r
dg
dr + k2
0g = −δ(r)
2πr ,
(5.6.25)
where r =
p
(x −ξ)2 + (y −η)2.
The homogeneous form of Equation 5.6.25 is Bessel’s
diﬀerential equation of order zero. Consequently, the general solution in terms of Hankel
functions is
g(r|r0) = A H(1)
0 (k0r) + B H(2)
0 (k0r).
(5.6.26)
Why have we chosen to use Hankel functions rather than J0(·) and Y0(·)? As we argued
earlier, solutions to the Helmholtz equation must represent outwardly propagating waves
(the Sommerfeld radiation condition).
If we again assume that the temporal behavior
is e−iωt and use the asymptotic expressions for Hankel functions, we see that H(1)
0 (k0r)
represents outwardly propagating waves and B = 0.
24 For an alternative derivation, see Graﬀ, K. F., 1991: Wave Motion in Elastic Solids. Dover Publica-
tions, Inc., pp. 284–285.

272
Advanced Engineering Mathematics: A Second Course
What is the value of A? Integrating Equation 5.6.26 over a small circle around the
point r = 0 and taking the limit as the radius of the circle vanishes, A = i/4 and
g(r|r0) = i
4H(1)
0 (k0r).
(5.6.27)
If a real function is needed, then the free-space Green’s function equals the Neumann
function Y0(k0r) divided by −4.
⊓⊔
• Example 5.6.3: Free-space Green’s function for the two-dimensional Laplace equation
In this subsection, we ﬁnd the free-space Green’s function for Poisson’s equation in two
dimensions. This Green’s function is governed by
1
r
∂
∂r

r∂g
∂r

+ 1
r2
∂2g
∂θ2 = −δ(r −ρ)δ(θ −θ′)
r
.
(5.6.28)
If we now choose our coordinate system so that the origin is located at the point source,
r =
p
(x −ξ)2 + (y −η)2 and ρ = 0. Multiplying both sides of this simpliﬁed Equation
5.6.28 by r dr dθ and integrating over a circle of radius ǫ, we obtain −1 on the right side
from the surface integration over the delta functions. On the left side,
Z 2π
0
r∂g
∂r

r=ǫ
dθ = −1.
(5.6.29)
The Green’s function g(r, θ|0, θ′) = −ln(r)/(2π) satisﬁes Equation 5.6.29.
To ﬁnd an alternative form of the free-space Green’s function when the point of exci-
tation and the origin of the coordinate system do not coincide, we ﬁrst note that
δ(θ −θ′) = 1
2π
∞
X
n=−∞
ein(θ−θ′).
(5.6.30)
This suggests that the Green’s function should be of the form
g(r, θ|ρ, θ′) =
∞
X
n=−∞
gn(r|ρ)ein(θ−θ′).
(5.6.31)
Substituting Equation 5.6.30 and Equation 5.6.31 into Equation 5.6.29, we obtain the or-
dinary diﬀerential equation
1
r
d
dr

rdgn
dr

−n2
r2 gn = −δ(r −ρ)
2πr
.
(5.6.32)
The homogeneous solution to Equation 5.6.32 is
g0(r|ρ) =

a,
0 ≤r ≤ρ ,
b ln(r),
ρ ≤r < ∞,
(5.6.33)

Green’s Functions
273
and
gn(r|ρ) =

c (r/ρ)n,
0 ≤r ≤ρ ,
d (ρ/r)n,
ρ ≤r < ∞,
(5.6.34)
if n ̸= 0.
At r = ρ, the gn’s must be continuous, in which case,
a = b ln(ρ),
and
c = d.
(5.6.35)
On the other hand,
ρdgn
dr

r=ρ+
r=ρ−
= −1
2π ,
(5.6.36)
or
a = −ln(ρ)
2π ,
b = −1
2π ,
and
c = d =
1
4πn.
(5.6.37)
Therefore,
g(r, θ|ρ, θ′) = −ln(r>)
2π
+ 1
2π
∞
X
n=1
1
n
r<
r>
n
cos[n(θ −θ′)],
(5.6.38)
where r> = max(r, ρ) and r< = min(r, ρ).
We can simplify Equation 5.6.38 by noting that
ln

1 + ρ2 −2ρ cos(θ −θ′)

= −2
∞
X
n=1
ρn cos[n(θ −θ′)]
n
,
(5.6.39)
if |ρ| < 1. Applying this relationship to Equation 5.6.38, we ﬁnd that
g(r, θ|ρ, θ′) = −1
4π ln

r2 + ρ2 −2rρ cos(θ −θ′)

.
(5.6.40)
Note that when ρ = 0 we recover g(r, θ|0, θ′) = −ln(r)/(2π).
⊓⊔
• Example 5.6.4: Two-dimensional Poisson equation over a rectangular domain
Consider the two-dimensional Poisson equation
∂2u
∂x2 + ∂2u
∂y2 = −f(x, y).
(5.6.41)
This equation arises in equilibrium problems, such as the static deﬂection of a rectangular
membrane. In that case, f(x, y) represents the external load per unit area, divided by the
tension in the membrane. The solution u(x, y) must satisfy certain boundary conditions.
For the present, let us choose u(0, y) = u(a, y) = 0, and u(x, 0) = u(x, b) = 0.
To ﬁnd the Green’s function for Equation 5.6.41 we must solve the partial diﬀerential
equation
∂2g
∂x2 + ∂2g
∂y2 = −δ(x −ξ)δ(y −η),
0 < x, ξ < a,
0 < y, η < b,
(5.6.42)
subject to the boundary conditions
g(0, y|ξ, η) = g(a, y|ξ, η) = g(x, 0|ξ, η) = g(x, b|ξ, η) = 0.
(5.6.43)

274
Advanced Engineering Mathematics: A Second Course
From Equation 5.6.9,
u(x, y) =
Z a
0
Z b
0
g(x, y|ξ, η)f(ξ, η) dη dξ.
(5.6.44)
One approach to ﬁnding the Green’s function is to expand it in terms of the eigenfunc-
tions ϕ(x, y) of the diﬀerential equation
∂2ϕ
∂x2 + ∂2ϕ
∂y2 = −λϕ,
(5.6.45)
and the boundary conditions, Equation 5.6.43. The eigenvalues are
λnm = n2π2
a2
+ m2π2
b2
,
(5.6.46)
where n = 1, 2, 3, . . ., m = 1, 2, 3, . . ., and the corresponding eigenfunctions are
ϕnm(x, y) = sin
nπx
a

sin
mπy
b

.
(5.6.47)
Therefore, we seek g(x, y|ξ, η) in the form
g(x, y|ξ, η) =
∞
X
n=1
∞
X
m=1
Anm sin
nπx
a

sin
mπy
b

.
(5.6.48)
Because the delta functions can be written
δ(x −ξ)δ(y −η) = 4
ab
∞
X
n=1
∞
X
m=1
sin
nπξ
a

sin
mπη
b

sin
nπx
a

sin
mπy
b

,
(5.6.49)
we ﬁnd that
n2π2
a2
+ m2π2
b2

Anm = 4
ab sin
nπξ
a

sin
mπη
b

,
(5.6.50)
after substituting Equations 5.6.48 and 5.6.49 into Equation 5.6.42, and setting the corre-
sponding harmonics equal to each other. Therefore, the bilinear formula for the Green’s
function of Poisson’s equation is
g(x, y|ξ, η) = 4
ab
∞
X
n=1
∞
X
m=1
sin
nπx
a

sin
nπξ
a

sin
mπy
b

sin
mπη
b

n2π2/a2 + m2π2/b2
.
(5.6.51)
Thus, solutions to Poisson’s equation can now be written as
u(x, y) =
∞
X
n=1
∞
X
m=1
anm
n2π2/a2 + m2π2/b2 sin
nπx
a

sin
mπy
b

,
(5.6.52)

Green’s Functions
275
where anm are the Fourier coeﬃcients for the function f(x, y):
anm = 4
ab
Z a
0
Z b
0
f(x, y) sin
nπx
a

sin
mπy
b

dy dx.
(5.6.53)
Another form of the Green’s function can be obtained by considering each direction
separately. To satisfy the boundary conditions along the edges y = 0 and y = b, we write
the Green’s function as the Fourier series
g(x, y|ξ, η) =
∞
X
m=1
Gm(x|ξ) sin
mπy
b

,
(5.6.54)
where the coeﬃcients Gm(x|ξ) are left as undetermined functions of x, ξ, and m. Substi-
tuting this series into the partial diﬀerential equation for g, multiplying by 2 sin(nπy/b)/b,
and integrating over y, we ﬁnd that
d2Gn
dx2 −n2π2
b2 Gn = −2
b sin
nπη
b

δ(x −ξ).
(5.6.55)
This diﬀerential equation shows that the expansion coeﬃcients Gn(x|ξ) are one-dimensional
Green’s functions; we can ﬁnd them, as we did in Section 5.2, by piecing together homo-
geneous solutions to Equation 5.6.55 that are valid over various intervals. For the region
0 ≤x ≤ξ, the solution to this equation that vanishes at x = 0 is
Gn(x|ξ) = An sinh
nπx
b

,
(5.6.56)
where An is presently arbitrary. The corresponding solution for ξ ≤x ≤a is
Gn(x|ξ) = Bn sinh
nπ(a −x)
b

.
(5.6.57)
Note that this solution vanishes at x = a. Because the Green’s function must be continuous
at x = ξ,
An sinh
nπξ
b

= Bn sinh
nπ(a −ξ)
b

.
(5.6.58)
On the other hand, the appropriate jump discontinuity of G′
n(x|ξ) yields
−nπ
b Bn cosh
nπ(a −ξ)
b

−nπ
b An cosh
nπξ
b

= −2
b sin
nπη
b

.
(5.6.59)
Solving for An and Bn,
An = 2
nπ sin
nπη
b
 sinh[nπ(a −ξ)/b]
sinh(nπa/b)
,
(5.6.60)
and
Bn = 2
nπ sin
nπη
b
 sinh(nπξ/b)
sinh(nπa/b).
(5.6.61)
This yields the Green’s function
g(x, y|ξ, η) = 2
π
∞
X
n=1
sinh[nπ(a −x>)/b] sinh(nπx</b)
n sinh(nπa/b)
sin
nπη
b

sin
nπy
b

,
(5.6.62)

276
Advanced Engineering Mathematics: A Second Course
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
 x/b
 y/b
 g(x,y|ξ,η)
Figure 5.6.2: The Green’s function, Equation 5.6.62 or Equation 5.6.63, for the planar Poisson equation
over a rectangular area with Dirichlet boundary conditions on all sides when a = b and ξ/b = η/b = 0.3.
where x> = max(x, ξ) and x< = min(x, ξ). Figure 5.6.2 illustrates Equation 5.6.62 in the
case of a square domain with ξ/b = η/b = 0.3.
If we began with a Fourier expansion in the y-direction, we would have obtained
g(x, y|ξ, η) = 2
π
∞
X
m=1
sinh[mπ(b −y>)/a] sinh(mπy</a)
m sinh(mπb/a)
sin
mπξ
a

sin
mπx
a

,
(5.6.63)
where y> = max(y, η) and y< = min(y, η).
⊓⊔
• Example 5.6.5: Two-dimensional Helmholtz equation over a rectangular domain
The problem to be solved is
∂2g
∂x2 + ∂2g
∂y2 + k2
0g = −δ(x −ξ)δ(y −η),
(5.6.64)
where 0 < x, ξ < a, and 0 < y, η < b, subject to the boundary conditions that
g(0, y|ξ, η) = g(a, y|ξ, η) = g(x, 0|ξ, η) = g(x, b|ξ, η) = 0.
(5.6.65)
We use the same technique to solve Equation 5.6.64 as we did in the previous example
by assuming that the Green’s function has the form
g(x, y|ξ, η) =
∞
X
m=1
Gm(x|ξ) sin
mπy
b

,
(5.6.66)
where the coeﬃcients Gm(x|ξ) are undetermined functions of x, ξ, and η. Substituting this
series into Equation 5.6.64, multiplying by 2 sin(nπy/b)/b, and integrating over y, we ﬁnd
that
d2Gn
dx2 −
n2π2
b2
−k2
0

Gn = −2
b sin
nπη
b

δ(x −ξ).
(5.6.67)

Green’s Functions
277
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
−1
−0.5
0
0.5
1
 x/a
 y/a
 g(x,y|ξ,η)
Figure 5.6.3: The Green’s function, Equation 5.6.72, for Helmholtz’s equation over a rectangular region
with a Dirichlet boundary condition on the sides when a = b, k0a = 10, and ξ/a = η/a = 0.35.
The ﬁrst method for solving Equation 5.6.67 involves writing
δ(x −ξ) = 2
a
∞
X
m=1
sin
mπξ
a

sin
mπx
a

,
(5.6.68)
and
Gn(x|ξ) = 2
a
∞
X
m=1
anm sin
mπx
a

.
(5.6.69)
Upon substituting Equations 5.6.68 and 5.6.69 into Equation 5.6.67, we obtain
∞
X
m=1

k2
0−m2π2
a2
−n2π2
b2

anm sin
mπx
a

= −4
ab
∞
X
m=1
sin
nπη
b

sin
mπξ
a

sin
mπx
a

.
(5.6.70)
Matching similar harmonics,
anm =
4 sin(mπξ/a) sin(nπη/b)
ab(m2π2/a2 + n2π2/b2 −k2
0),
(5.6.71)
and the bilinear form of the Green’s function is
g(x, y|ξ, η) = 4
ab
∞
X
n=1
∞
X
m=1
sin(mπξ/a) sin(nπη/b) sin(mπx/a) sin(nπy/b)
m2π2/a2 + n2π2/b2 −k2
0
.
(5.6.72)
See Figure 5.6.3. The bilinear form of the Green’s function for the two-dimensional Helm-
holtz equation with Neumann boundary conditions is left as Problem 8.

278
Advanced Engineering Mathematics: A Second Course
As in the previous example, we can construct an alternative to the bilinear form of the
Green’s function, Equation 5.6.72, by writing Equation 5.6.67 as
d2Gn
dx2 −k2
nGn = −2
b sin
nπη
b

δ(x −ξ),
(5.6.73)
where k2
n = n2π2/b2 −k2
0. The homogeneous solution to Equation 5.6.73 is now
Gn(x|ξ) =

An sinh(knx),
0 ≤x ≤ξ,
Bn sinh[kn(a −x)],
ξ ≤x ≤a.
(5.6.74)
This solution satisﬁes the boundary conditions at both endpoints.
Because Gn(x|ξ) must be continuous at x = ξ,
An sinh(knξ) = Bn sinh[kn(a −ξ)].
(5.6.75)
On the other hand, the jump discontinuity involving G′
n(x|ξ) yields
−knBn cosh[kn(a −ξ)] −knAn cosh(knξ) = −2
b sin
nπη
b

.
(5.6.76)
Solving for An and Bn,
An =
2
bkn
sin
nπη
b
 sinh[kn(a −ξ)]
sinh(kna)
,
(5.6.77)
and
Bn =
2
bkn
sin
nπη
b
 sinh(knξ)
sinh(kna).
(5.6.78)
This yields the Green’s function
g(x, y|ξ, η) = 2
b
N
X
n=1
sin[κn(a −x>)] sin(κnx<)
κn sin(κna)
sin
nπη
b

sin
nπy
b

+ 2
b
∞
X
n=N+1
sinh[kn(a −x>)] sinh(knx<)
kn sinh(kna)
sin
nπη
b

sin
nπy
b

,
(5.6.79)
where x> = max(x, ξ) and x< = min(x, ξ). Here N denotes the largest value of n such that
k2
n < 0, and κ2
n = k2
0 −n2π2/b2. If we began with a Fourier expansion in the y direction,
we would have obtained
g(x, y|ξ, η) = 2
a
M
X
m=1
sin[κm(b −y>)] sin(κmy<)
κm sin(κmb)
sin
mπξ
a

sin
mπx
a

+ 2
a
∞
X
m=M+1
sinh[km(b −y>)] sinh(kmy<)
km sinh(kmb)
sin
mπξ
a

sin
mπx
a

,
(5.6.80)
where M denotes the largest value of m such that k2
m < 0, k2
m = m2π2/a2 −k2
0, κ2
m =
k2
0 −m2π2/a2, y< = min(y, η), and y> = max(y, η).
⊓⊔

Green’s Functions
279
• Example 5.6.6: Two-dimensional Helmholtz equation over a circular disk
In this example, we ﬁnd the Green’s function for the Helmholtz equation when the
domain consists of the circular region 0 < r < a. The Green’s function is governed by the
partial diﬀerential equation
1
r
∂
∂r

r∂g
∂r

+ 1
r2
∂2g
∂θ2 + k2
0g = −δ(r −ρ)δ(θ −θ′)
r
,
(5.6.81)
where 0 < r, ρ < a, and 0 ≤θ, θ′ ≤2π, with the boundary conditions
lim
r→0 |g(r, θ|ρ, θ′)| < ∞,
g(a, θ|ρ, θ′) = 0,
0 ≤θ, θ′ ≤2π.
(5.6.82)
The Green’s function must be periodic in θ.
We begin by noting that
δ(θ −θ′) = 1
2π + 1
π
∞
X
n=1
cos[n(θ −θ′)] = 1
2π
∞
X
n=−∞
cos[n(θ −θ′)].
(5.6.83)
Therefore, the solution has the form
g(r, θ|ρ, θ′) =
∞
X
n=−∞
gn(r|ρ) cos[n(θ −θ′)].
(5.6.84)
Substituting Equation 5.6.83 and Equation 5.6.84 into Equation 5.6.81 and simplifying, we
ﬁnd that
1
r
d
dr

rdgn
dr

−n2
r2 gn + k2
0gn = −δ(r −ρ)
2πr
.
(5.6.85)
The solution to Equation 5.6.85 is the Fourier-Bessel series
gn(r|ρ) =
∞
X
m=1
AnmJn
knmr
a

,
(5.6.86)
where knm is the mth root of Jn(k) = 0. Upon substituting Equation 5.6.86 into Equation
5.6.85 and solving for Anm, we have that
(k2
0 −k2
nm/a2)Anm = −
1
πa2J′2
n(knm)
Z a
0
δ(r −ρ)Jn
knmr
a

dr,
(5.6.87)
or
Anm =
Jn(knmρ/a)
π(k2nm −k2
0a2)J′2
n(knm).
(5.6.88)
Thus, the Green’s function25 is
g(r, θ|ρ, θ′) = 1
π
∞
X
n=−∞
∞
X
m=1
Jn(knmρ/a)Jn(knmr/a)
(k2nm −k2
0a2)J′2
n(knm) cos[n(θ −θ′)].
(5.6.89)
25 For an example of its use, see Zhang, D. R., and C. F. Foo, 1999: Fields analysis in a solid magnetic
toroidal core with circular cross section based on Green’s function. IEEE Trans. Magnetics, 35, 3760–3762.

280
Advanced Engineering Mathematics: A Second Course
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
 x/a
 y/a
 g(r,θ|ρ,θ′)
Figure 5.6.4: The Green’s function, Equation 5.6.89, for Helmholtz’s equation within a circular disk with
a Dirichlet boundary condition on the rim when k0a = 10, ρ/a = 0.35
√
2, and θ′ = π/4.
See Figure 5.6.4.
Problems
1. Using a Fourier sine expansion in the x-direction, construct the Green’s function governed
by the planar Poisson equation
∂2g
∂x2 + ∂2g
∂y2 = −δ(x −ξ)δ(y −η),
0 < x, ξ < a,
−∞< y, η < ∞,
with the Dirichlet boundary conditions
g(0, y|ξ, η) = g(a, y|ξ, η) = 0,
−∞< y < ∞,
and the conditions at inﬁnity that
lim
|y|→∞g(x, y|ξ, η) →0,
0 < x < a.
2. Using a generalized Fourier expansion in the x-direction, construct the Green’s function
governed by the planar Poisson equation
∂2g
∂x2 + ∂2g
∂y2 = −δ(x −ξ)δ(y −η),
0 < x, ξ < a,
−∞< y, η < ∞,
with the Neumann and Dirichlet boundary conditions
gx(0, y|ξ, η) = g(a, y|ξ, η) = 0,
−∞< y < ∞,
and the conditions at inﬁnity that
lim
|y|→∞g(x, y|ξ, η) →0,
0 < x < a.

Green’s Functions
281
3. Using a Fourier sine expansion in the y-direction, show that the Green’s function governed
by the planar Poisson equation
∂2g
∂x2 + ∂2g
∂y2 = −δ(x −ξ)δ(y −η),
0 < x, ξ < a,
0 < y, η < b,
with the boundary conditions
g(x, 0|ξ, η) = g(x, b|ξ, η) = 0,
and
g(0, y|ξ, η) = gx(a, y|ξ, η) + β g(a, y|ξ, η) = 0,
β ≥0,
is
g(x, y|ξ, η) =
∞
X
n=1
sinh(νx<) {ν cosh[ν(a −x>)] + β sinh[ν(a −x>)]}
ν2 cosh(νa) + βν sinh(νa)
sin
nπη
b

sin
nπy
b

,
where ν = nπ/b, x> = max(x, ξ), and x< = min(x, ξ).
4. Using the Fourier series representation of the delta function in a circular domain:
δ(θ −θ′) = 1
2π + 1
π
∞
X
n=1
cos[n(θ −θ′)],
0 ≤θ, θ′ ≤2π,
construct the Green’s function governed by the planar Poisson equation
1
r
∂
∂r

r∂g
∂r

+ 1
r2
∂2g
∂θ2 = −δ(r −ρ)δ(θ −θ′)
r
,
where a < r, ρ < b, and 0 ≤θ, θ′ ≤2π, subject to the boundary conditions g(a, θ|ρ, θ′) =
g(b, θ|ρ, θ′) = 0 and periodicity in θ.
5. Construct the Green’s function governed by the planar Poisson equation
1
r
∂
∂r

r∂g
∂r

+ 1
r2
∂2g
∂θ2 = −δ(r −ρ)δ(θ −θ′)
r
,
where 0 < r, ρ < ∞, and 0 < θ, θ′ < β, subject to the boundary conditions that g(r, 0|ρ, θ′)
= g(r, β|ρ, θ′) = 0 for all r. Hint:
δ(θ −θ′) = 2
β
∞
X
n=1
sin
nπθ′
β

sin
nπθ
β

.
6. Construct the Green’s function governed by the planar Poisson equation
1
r
∂
∂r

r∂g
∂r

+ 1
r2
∂2g
∂θ2 = −δ(r −ρ)δ(θ −θ′)
r
,

282
Advanced Engineering Mathematics: A Second Course
where 0 < r, ρ < a, and 0 < θ, θ′ < β, subject to the boundary conditions g(r, 0|ρ, θ′) =
g(r, β|ρ, θ′) = g(a, θ|ρ, θ′) = 0. Hint:
δ(θ −θ′) = 2
β
∞
X
n=1
sin
nπθ′
β

sin
nπθ
β

.
7. Using a Fourier sine series in the z-direction and the fact that
δ(x −b) = 2b
a2
∞
X
k=1
J0(µkb/a)J0(µkx/a)
J2
1(µk)
,
0 < x, b < a,
where µk is the kth positive root of J0(µ) = 0, ﬁnd the Green’s function governed by the
axisymmetric Poisson equation
∂2g
∂r2 + 1
r
∂g
∂r + ∂2g
∂z2 = −δ(r −ρ)δ(z −ζ)
2πr
,
where 0 < r, ρ < a, and 0 < z, ζ < L, subject to the boundary conditions
g(r, 0|ρ, ζ) = g(r, L|ρ, ζ) = 0,
0 < r < a,
and
lim
r→0 |g(r, z|ρ, ζ)| < ∞,
g(a, z|ρ, ζ) = 0,
0 < z < L.
8. Following Example 5.6.5 except for using Fourier cosine series, construct the Green’s
function26 governed by the planar Helmholtz equation
∂2g
∂x2 + ∂2g
∂y2 + k2
0g = −δ(x −ξ)δ(y −η),
0 < x, ξ < a,
0 < y, η < b,
subject to the Neumann boundary conditions
gx(0, y|ξ, η) = gx(a, y|ξ, η) = 0,
0 < y < b,
and
gy(x, 0|ξ, η) = gy(x, b|ξ, η) = 0,
0 < x < a.
9. Using Fourier sine transforms,
g(x, y|ξ, η) = 2
π
Z ∞
0
G(k, y|ξ, η) sin(kx) dk,
where
G(k, y|ξ, η) =
Z ∞
0
g(x, y|ξ, η) sin(kx) dx,
26 Kulkarni et al. (Kulkarni, S., F. G. Leppington, and E. G. Broadbent, 2001: Vibrations in several
interconnected regions: A comparison of SEA, ray theory and numerical results. Wave Motion, 33, 79–96)
solved this problem when the domain has two diﬀerent, constant k2
0’s.

Green’s Functions
283
ﬁnd the Green’s function governed by
∂2g
∂x2 + ∂2g
∂y2 = −δ(x −ξ)δ(y −η),
for the quarter space 0 < x, y, with the boundary conditions
g(0, y|ξ, η) = g(x, 0|ξ, η) = 0,
and
lim
x,y→∞g(x, y|ξ, η) →0.
Step 1: Taking the Fourier sine transform in the x direction, show that the partial diﬀer-
ential equation reduces to the ordinary diﬀerential equation
d2G
dy2 −k2G = −sin(kξ)δ(y −η),
with the boundary conditions
G(k, 0|ξ, η) = 0,
and
lim
y→∞G(k, y|ξ, η) →0.
Step 2: Show that the particular solution to the ordinary diﬀerential equation in Step 1 is
Gp(k, y|ξ, η) = sin(kξ)
2k
e−k|y−η|.
You may want to review Example 5.2.8.
Step 3: Find the homogeneous solution to the ordinary diﬀerential equation in Step 1 so
that the general solution satisﬁes the boundary conditions. Show that the general solution
is
G(k, y|ξ, η) = sin(kξ)
2k
h
e−k|y−η| −e−k(y+η)i
.
Step 4: Taking the inverse, show that
g(x, y|ξ, η) = 1
π
Z ∞
0
h
e−k|y−η| −e−k(y+η)i
sin(kξ) sin(kx) dk
k .
Step 5: Performing the integration,27 show that
g(x, y|ξ, η) = −1
4π ln
[(x −ξ)2 + (y −η)2][(x + ξ)2 + (y + η)2]
[(x −ξ)2 + (y + η)2][(x + ξ)2 + (y −η)2]

.
27 Gradshteyn, I. S., and I. M. Ryzhik, 1965: Table of Integrals, Series, and Products. Academic Press,
Section 3.947, Formula 1.

284
Advanced Engineering Mathematics: A Second Course
10. Find the free-space Green’s function28 governed by
∂2g
∂x2 + ∂2g
∂y2 −g = −δ(x −ξ)δ(y −η),
−∞< x, y, ξ, η < ∞.
Step 1: Introducing the Fourier transform
g(x, y|ξ, η) = 1
2π
Z ∞
−∞
G(k, y|ξ, η)eikx dk,
where
G(k, y|ξ, η) =
Z ∞
−∞
g(x, y|ξ, η)e−ikx dx,
show that the governing partial diﬀerential equation can be transformed into the ordinary
diﬀerential equation
d2G
dy2 −
 k2 + 1

G = −e−ikξδ(y −η).
Step 2: Introducing the Fourier transform in the y-direction,
G(k, y|ξ, η) = 1
2π
Z ∞
−∞
G(k, ℓ|ξ, η)eiℓy dℓ,
where
G(k, ℓ|ξ, η) =
Z ∞
−∞
G(k, y|ξ, η)e−iℓy dy,
solve the ordinary diﬀerential equation in Step 1 and show that
G(k, y|ξ, η) = e−ikξ
2π
Z ∞
−∞
eiℓ(y−η)
k2 + ℓ2 + 1 dℓ.
Step 3: Complete the problem by showing that
g(x, y|ξ, η) =
1
4π2
Z ∞
−∞
Z ∞
−∞
eik(x−ξ)eiℓ(y−η)
k2 + ℓ2 + 1
dℓdk
=
1
4π2
Z ∞
0
Z 2π
0
eirκ cos(θ−ϕ)
κ2 + 1
κ dθ dκ
= 1
2π
Z ∞
0
J0(rκ)
κ2 + 1 κ dκ = K0(r)
2π
,
where r =
p
(x −ξ)2 + (y −η)2, k = κ cos(θ), ℓ= κ sin(θ), x −ξ = r cos(ϕ), and y −η =
r sin(ϕ). You will need to use integral tables29 to obtain the ﬁnal result.
28 For its use, see Geisler, J. E., 1970: Linear theory of the response of a two layer ocean to a moving
hurricane. Geophys. Fluid Dyn., 1, 249–272.
29 Gradshteyn and Ryzhik, op. cit., Section 6.532, Formula 6.

Green’s Functions
285
11. Find the free-space Green’s function governed by
∂2g
∂x2 + ∂2g
∂y2 −∂g
∂x = −δ(x −ξ)δ(y −η),
−∞< x, y, ξ, η < ∞.
Step 1: By introducing ϕ(x, y|ξ, η) such that
g(x, y|ξ, η) = ex/2ϕ(x, y|ξ, η),
show that the partial diﬀerential equation for g(x, y|ξ, η) becomes
∂2ϕ
∂x2 + ∂2ϕ
∂y2 −ϕ
4 = −e−ξ/2δ(x −ξ)δ(y −η).
Step 2: After taking the Fourier transform with respect to x of the partial diﬀerential
equation in Step 1, show that it becomes the ordinary diﬀerential equation
d2Φ
dy2 −
 k2 + 1
4

Φ = −e−ξ/2−ikξδ(y −η).
Step 3: Introducing the same transformation as in Step 3 of the previous problem, show
that
Φ(k, y|ξ, η) = e−ξ/2−ikξ
2π
Z ∞
−∞
eiℓ(y−η)
k2 + ℓ2 + 1
4
dℓ,
and
ϕ(x, y|ξ, η) = e−ξ/2
2π K0( 1
2r),
where r =
p
(x −ξ)2 + (y −η)2.
Step 4: Using the transformation introduced in Step 1, show that
g(x, y|ξ, η) = e(x−ξ)/2
2π
K0( 1
2r).
5.7 GALERKIN METHOD
In the previous sections we developed various analytic expressions for Green’s functions.
We close this chapter by showing how to construct a numerical approximation.
Finite elements can be used to solve diﬀerential equations by introducing subdomains
known as ﬁnite elements rather than a grid of nodal points. The solution is then represented
within each element by an interpolating polynomial. Unlike ﬁnite diﬀerence schemes that are
constructed from Taylor expansions, the theory behind ﬁnite elements introduces concepts
from functional analysis and variational methods to formulate the algebraic equations.
There are several paths that lead to the same ﬁnite element formulation. The two most
common techniques are the Galerkin and collocation methods. In this section we focus on
the Galerkin method. This method employs a rational polynomial, called a basis function,
that satisﬁes the boundary conditions.

286
Advanced Engineering Mathematics: A Second Course
We begin by considering the Sturm-Liouville problem governed by
d2ψn
dx2 + λnψn = 0,
0 < x < L,
(5.7.1)
subject to the boundary conditions ψn(0) = ψn(L) = 0.
Although we could solve this
problem exactly, we will pretend that we cannot.
Rather, we will assume that we can
express it by
ψn(x) =
N
X
j=1
αnjfj(x),
(5.7.2)
where fj(x) is our basis function. Clearly, it is desirable that fj(0) = fj(L) = 0.
How do we compute αnj? We begin by multiplying Equation 5.7.1 by fi(x) and inte-
grating the resulting equation from 0 and L. This yields
Z L
0
fi(x)d2ψn
dx2 dx + λn
Z L
0
fi(x)ψn(x) dx = 0,
(5.7.3)
where i = 1, 2, 3, . . . , N. Next, we substitute Equation 5.7.2 and ﬁnd that
N
X
j=1
"Z L
0
fi(x)f ′′
j (x) dx + λn
Z L
0
fi(x)fj(x) dx
#
αnj = 0.
(5.7.4)
We can write Equation 5.7.4 as
(A + λnB)d = 0,
(5.7.5)
where
aij =
Z L
0
fi(x)f ′′
j (x) dx = −
Z L
0
f ′
i(x)f ′
j(x) dx,
(5.7.6)
bij =
Z L
0
fi(x)fj(x) dx,
(5.7.7)
and the vector d contains the elements αnj.
There are several obvious choices for fj(x):
• Example 5.7.1
The simplest choice for fj(x) = sin(jπx/L). If we select N = 2, Equation 5.7.2 becomes
ψn(x) = αn1 sin
πx
L

+ αn2 sin
2πx
L

.
(5.7.8)
From Equation 5.7.6 and Equation 5.7.7,
aij = −
jπ
L
2 Z L
0
sin
iπx
L

sin
jπx
L

dx,
i = 1, 2, j = 1, 2;
(5.7.9)
and
bij =
Z L
0
sin
iπx
L

sin
jπx
L

dx,
i = 1, 2, j = 1, 2.
(5.7.10)

Green’s Functions
287
Performing the integrations, a12 = a21 = b12 = b21 = 0, a11 = −π2/(2L), a22 = −2π2/L,
and b11 = b22 = L/2.
Returning to Equation 5.7.5, it becomes

−π2/2 + λnL2/2
0
0
−2π2 + λnL2/2
 
αn1
αn2

=

0
0

.
(5.7.11)
In order for Equation 5.7.11 to have a unique solution,

−π2/2 + λnL2/2
0
0
−2π2 + λnL2/2
 = 0.
(5.7.12)
Equation 5.7.12 yields 4λ1 = λ2 = 4π2/L2.
In summary,
ψ1(x) = sin(πx/L),
λ1 = π2/L2;
(5.7.13)
and
ψ2(x) = sin(2πx/L),
λ2 = 4π2/L2,
(5.7.14)
with α12 = α21 = 0. Here we have chosen that α11 = α22 = 1.
⊓⊔
• Example 5.7.2
Another possible choice for fj(x) involves polynomials of the form (1 −x/L)(x/L)j
with j = 1, 2. Unlike the previous example, we have nonorthogonal basis functions here.
Note that fj(0) = fj(L) = 0. Therefore, Equation 5.7.2 becomes
ψn(x) = αn1(1 −x/L)(x/L) + αn2(1 −x/L)(x/L)2.
(5.7.15)
From Equation 5.7.6 and Equation 5.7.7,
aij = −1
L2
Z L
0

1 −x
L
  x
L
i 
j(j −1)
 x
L
j−2
−j(j + 1)
 x
L
j−1
dx
(5.7.16)
= 1
L
 j(j −1)
i + j −1 −j(j −1)
i + j
−j(j + 1)
i + j
+ j(j + 1)
i + j + 1

,
(5.7.17)
with i = 1, 2 and j = 1, 2. Similarly,
bij =
Z L
0

1 −x
L
  x
L
i 
1 −x
L
  x
L
j
dx
(5.7.18)
= L

1
i + j + 1 −
2
i + j + 2 +
1
i + j + 3

.
(5.7.19)
Performing the computations, a11 = −1/(3L), a12 = a21 = −1/(6L), a22 = −2/(15L),
b11 = L/30, b12 = b21 = L/60, and b22 = L/105.
Returning to Equation 5.7.5, it becomes

−1/3 + λnL2/30
−1/6 + λnL2/60
−1/6 + λnL2/60
−2/15 + λnL2/105
 
αn1
αn2

=

0
0

.
(5.7.20)

288
Advanced Engineering Mathematics: A Second Course
In order for Equation 5.7.20 to have a unique solution,

−1/3 + λnL2/30
−1/6 + λnL2/60
−1/6 + λnL2/60
−2/15 + λnL2/105
 = 0.
(5.7.21)
Equation 5.7.21 yields λ1L2 = 10 and λ2L2 = 42. Note how close these values of λ are to
those found in the previous example. Returning to Equation 5.7.20, we ﬁnd that α11 = 1,
α12 = 0, α22 = 1, and α21 = −1/2.
In summary,
ψ1(x) =

1 −x
L
 x
L,
λ1 = 10
L2 ;
(5.7.22)
and
ψ2(x) = −1
2

1 −x
L
 x
L +

1 −x
L
  x
L
2
,
λ2 = 42
L2 .
(5.7.23)
Because fj(x) are linearly independent, their use in the Galerkin expansion is quite ac-
ceptable. However, because these functions are not particularly orthogonal, their usefulness
will become more diﬃcult as N increases. Consequently, the choice of orthogonal functions
is often best.
⊓⊔
How do we employ the Galerkin technique to approximate Green’s functions? We begin
by considering the inhomogeneous heat conduction problem:
∂u
∂t −∂2u
∂x2 = F(x, t),
0 < x < L,
0 < t,
(5.7.24)
with the boundary conditions
u(0, t) = u(L, t) = 0,
0 < t,
(5.7.25)
and the initial condition u(x, 0) = 0, 0 < x < L.
Let us write the solution to this problem as
u(x, t) =
N
X
n=1
cn(t)ψn(x)e−λnt.
(5.7.26)
Direct substitution of Equation 5.7.26 into Equation 5.7.24, followed by multiplying the
resulting equation by fi(x) and integrating from 0 to L, gives
N
X
n=1
cn(t)e−λnt
Z L
0
fi(x)d2ψn
dx2 dx −
N
X
n=1
dcn
dt −λncn

e−λnt
Z L
0
fi(x)ψn(x) dx
= −
Z L
0
fi(x)F(x, t) dx.
(5.7.27)
Because
d2ψn
dx2 + λnψn = 0,
(5.7.28)
Equation 5.7.27 simpliﬁes to
N
X
n=1
dcn
dt e−λnt
Z L
0
fi(x)ψn(x) dx =
Z L
0
fi(x)F(x, t) dx = F ∗
i (t),
(5.7.29)

Green’s Functions
289
where i = 1, 2, . . . , N.
We must now ﬁnd cn. We can write Equation 5.7.29 as
N
X
n=1
eine−λnt dcn
dt dx = F ∗
i (t),
(5.7.30)
where
ein =
N
X
j=1
αnjbji.
(5.7.31)
Using linear algebra, we ﬁnd that
e−λnt dcn
dt =
N
X
i=1
pniF ∗
i (t),
(5.7.32)
where pni are the elements of an array P = E−1 and E = (DB)T . The arrays D and B
consist of elements αij and bij, respectively. Solving Equation 5.7.32, we ﬁnd that
cn(t) = An +
N
X
i=1
pni
Z t
0
F ∗
i (η)eλnη dη.
(5.7.33)
Because u(x, 0) = 0, cn(0) = 0 and An = 0.
We are now ready to ﬁnd our Green’s function. Let us set F(x, t) = δ(x −ξ)δ(t −τ).
Then F ∗
i (t) = fi(ξ)δ(t −τ) and
cn(t) = H(t −τ)
N
X
i=1
pnifi(ξ)eλnτ.
(5.7.34)
From Equation 5.7.2, Equation 5.7.26, and Equation 5.7.34, we obtain the ﬁnal result that
g(x, t|ξ, τ) = H(t −τ)
N
X
n=1
N
X
j=1
N
X
i=1
αnjpnifi(ξ)fj(x)e−λn(t−τ).
(5.7.35)
⊓⊔
• Example 5.7.3
In Example 5.5.2, we solved the Green’s function problem:
∂g
∂t −∂2g
∂x2 = δ(x −ξ)δ(t −τ),
(5.7.36)
with the boundary condition
g(0, t|ξ, τ) = g(L, t|ξ, τ) = 0,
(5.7.37)
and the initial condition g(x, 0|ξ, τ) = 0. There we found the solution (Equation 5.5.34):
g(x, t|ξ, τ) = H(t −τ)
∞
X
n=1
ψn(ξ)ψn(x)e−k2
n(t−τ),
(5.7.38)

290
Advanced Engineering Mathematics: A Second Course
where we have the orthonormal eigenfunctions
ψn(x) =
p
2/L sin(knx),
kn = nπ/L.
(5.7.39)
Let us use the basis function fj(x) = (1 −x/L)(x/L)j to ﬁnd the approximate Green’s
function to Equation 5.7.36. Here j = 1, 2, 3, . . . , N,
For N = 2, we showed in Example 5.7.2 that
B = L

1/30
1/60
1/60
1/105

,
D =

1
0
−1/2
1

.
(5.7.40)
Consequently,
BD =
L
840

28
14
0
1

,
E =
L
840

28
0
14
1

.
(5.7.41)
Using Gaussian elimination,
P = E−1 = 1
L

30
0
−420
840

.
(5.7.42)
Therefore, the two-term approximation to the Green’s function, Equation 5.7.38, is
g(x, t|ξ, τ) = 30
L
x
L

1 −x
L
 ξ
L

1 −ξ
L

exp

−10(t −τ)
L2

H(t −τ)
+
210
L
x
L

1 −x
L
 ξ
L

1 −ξ
L

−420
L
x
L

1 −x
L
  ξ
L
2 
1 −ξ
L

−420
L
 x
L
2 
1 −x
L
 ξ
L

1 −ξ
L

+ 840
L
 x
L
2 
1 −x
L
  ξ
L
2 
1 −ξ
L

× exp

−42(t −τ)
L2

H(t −τ).
(5.7.43)
For N > 2, hand computations are very cumbersome and numerical computations are
necessary. For a speciﬁc N, we ﬁrst compute the arrays A and B via Equation 5.7.17 and
Equation 5.7.19.
for j = 1:N
for i = 1:N
A(i,j) = j*(j-1)/(i+j-1) - j*(j-1)/(i+j) ...
+ j*(j+1)/(i+j+1) - j*(j+1)/(i+j) ;
B(i,j) = 1/(i+j+1) - 2/(i+j+2) + 1/(i+j+3);
end; end
Next we compute the λn’s and corresponding eigenfunctions: [v,d] = eig(A,-B).
Table 5.7.1 gives L2λn for several values of N.
Once we found the eigenvalues and eigenvectors, we now compute the matrices D, E,
and P. For convenience we have reordered the eigenvalues so that their numerical value
increases with n. Furthermore, we have set αnn equal to one for n = 1, 2, . . . , N.
[lambda,ix] = sort(temp);
for i = 1:N
for j = 1:N
D(i,j) = v(j,ix(i));

Green’s Functions
291
Table 5.7.1: The Value of L2λn for n = 1, 2, . . . , N as a Function of N.
n
Exact
N = 2
N = 3
N = 4
N = 6
N = 8
N = 10
1
9.87
10.00
9.87
9.87
9.87
9.87
9.87
2
39.48
42.00
42.00
39.50
39.48
39.48
39.48
3
88.83
102.13
102.13
89.17
88.83
88.83
4
157.91
200.50
159.99
157.96
157.91
5
246.74
350.96
254.42
247.04
6
355.31
570.53
376.47
356.65
7
483.61
878.88
531.55
8
631.65
1298.03
725.34
9
799.44
1850.98
10
986.96
2548.73
end; end
for i = 1:N
denom = D(i,i);
for j = 1:N
D(i,j) = D(i,j) / denom;
end; end
E = transpose(D*B);
P = inv(E);
Having computed the matrices D and P, our ﬁnal task is the computation of the
Green’s function using Equation 5.7.35. The MATLAB code is:
phi i(1) = (1-xi)*xi;
for i = 2:N
phi i(i) = xi*phi i(i-1);
end
for ii = 1:idim
x = (ii-1)*dx;
phi j(1) = (1-x)*x;
for j = 2:N
phi j(j) = x*phi j(j-1);
end
for n = 1:N
for j = 1:N
for i = 1:N
g(ii) = g(ii) + D(n,j).*P(n,i).*phi j(j).*phi i(i) ...
.*exp(-lambda(n)*time);
end; end; end
end
In this code the parameter time denotes the quantity (t−τ)/L2. Figure 5.7.1 compares
this approximate Green’s function for various N against the exact solution. One of the

292
Advanced Engineering Mathematics: A Second Course
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−2
0
2
4
6
8
10
x/L
L g(x,t|ξ,τ)
exact
N = 3
N = 6
N = 9
N = 12
Figure 5.7.1: Comparison of the exact Green’s function Lg(x, t|ξ, τ) for a one-dimensional heat equation
given by Equation 5.7.38 (solid line) and approximate Green’s functions found by the Galerkin method for
diﬀerent values of N. Here (t −τ)/L2 = 0.001 and ξ = 0.4.
problems with this method is ﬁnding the inverse of the array E.
As N increases, the
accuracy of the inverse becomes poorer.
Further Readings
Beck, J. V., K. D. Cole, A. Haji-Sheikh, and B. Litkouhi, 1992: Heat Conduction Using
Green’s Functions. Hemisphere Publishing Corp., 523 pp. Detailed study of solving heat
conduction problems via Green’s functions.
Carslaw, H. S., and J. C. Jaeger, 1959: Conduction of Heat in Solids. Oxford University
Press, Chapter 14. An early classic for ﬁnding the Green’s function for the heat equation.
Duﬀy, D. G., 2015: Green’s Functions with Applications. Chapman & Hall/ CRC, 464 pp.
A source book.
¨Ozi¸sik, M. N., 1993: Heat Conduction. John Wiley & Sons, Chapter 6. A book of how to
solve partial diﬀerential equations of heat conduction.
Stakgold, I., 1997: Green’s Functions and Boundary Value Problems. Wiley-Interscience,
720 pp. A systematic approach to boundary-value problems.

−4
−3
−2
−1
0
1
2
3
4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
 x
Estimated and true PDF
Chapter 6
Probability
So far in this book we presented mathematical techniques that are used to solve deter-
ministic problems—problems in which the underlying physical processes are known exactly.
In this and the next chapter we turn to problems in which uncertainty is key.
Although probability theory was ﬁrst developed to explain the behavior of games of
chance,1 its usefulness in the physical sciences and engineering became apparent by the late
nineteenth century. Consider, for example, the biological process of birth and death. If b
denotes the birth rate and d is the death rate, the size of the population P(t) at time t is
P(t) = P(0)e(b−d)t.
(6.0.1)
Let us examine the situation when P(0) = 1 and b = 2d so that a birth is twice as likely
to occur as a death. Then, Equation 6.0.1 predicts exponential growth with P(t) = edt.
But the ﬁrst event may be a death, a one-in-three chance since d/(b + d) = 1/3, and this
would result in the population immediately becoming extinct. Consequently we see that
for small populations, chance ﬂuctuations become important and a deterministic model is
inadequate.
The purpose of this and the next chapter is to introduce mathematical techniques that
will lead to realistic models where chance plays an important role, and show under what
conditions deterministic models will work. In this chapter we present those concepts that
we will need in the next chapter to explain random processes.
1 Todhunter, I., 1949: A History of the Mathematical Theory of Probability from the Time of Pascal
to That of Laplace. Chelsea, 624 pp.; Hald, A., 1990: A History of Probability and Statistics and Their
Applications before 1750. John Wiley & Sons, 586 pp.
293

294
Advanced Engineering Mathematics: A Second Course
6.1 REVIEW OF SET THEORY
Often we must count various objects in order to compute a probability. Sets provide a
formal method to aid in these computations. Here we review important concepts from set
theory.
Sets are collections of objects, such as the number of undergraduate students at a
college. We deﬁne a set A either by naming the objects or describing the objects. For
example, the set of natural numbers can be either enumerated:
A = {1, 2, 3, 4, . . .},
(6.1.1)
or described:
A = {I : I is an integer and I ≥1}.
(6.1.2)
Each object in set A is called an element and each element is distinct. Furthermore, the
ordering of the elements within the set is not important.
Two sets are said to be equal if they contain the same elements and are written A = B.
An element x of a set A is denoted by x ∈A. A set with no elements is called a empty
or null set and denoted by ∅. On the other hand, a universal set is the set of all elements
under consideration.
A set B is subset of a set A, written B ⊂A, if every element in B is also an element
of A. For example, if A = {x : 0 ≤x < ∞} and S = {x : −∞< x < ∞}, then A ⊂S.
We can also use this concept to deﬁne the equality of sets A and B. Equality occurs when
A ⊂B and B ⊂A.
The complement of A, written A, is the set of elements in S but not in A. For example,
if A = {x : 0 ≤x < ∞} and S = {x : −∞< x < ∞}, then A = {x : −∞< x < 0}.
Two sets can be combined together to form a new set. This union of A and B, written
A ∪B, creates a new set that contains elements that belong to A and/or B. This deﬁnition
can be extended to multiple sets A1, A2, . . . , AN so that the union is the set of elements for
which each element belongs to at least one of these sets. It is written
A1 ∪A2 ∪A3 ∪· · · ∪AN =
N
[
i=1
Ai.
(6.1.3)
The intersection of sets A and B, written A ∩B, is deﬁned as the set of elements that
belong to both A and B. This deﬁnition can also be extended to multiple sets A1, A2, . . . , AN
so that the intersection is the set of elements for which each element belongs to all of these
sets. It is written
A1 ∩A2 ∩A3 ∩· · · ∩AN =
N
\
i=1
Ai.
(6.1.4)
If two sets A and B have no elements in common, they are said to be disjoint and A∩B = ∅.
A popular tool for visualizing set operations is the Venn diagram.2 For sets A and B
Figure 6.1.1 pictorially illustrates A ∪B, A ∩B, A, and A ∩B.
With these deﬁnitions a number of results follow: A = A, A ∪A = S, A ∩A = ∅,
A ∪∅= A, A ∩∅= ∅, A ∪S = S, A ∩S = A, S = ∅, and ∅= S. Here S denotes the
universal set.
Sets obey various rules similar to those encountered in algebra. They include:
2 Venn, J., 2008: Symbolic Logic. Kessinger, 492 pp.

Probability
295
                                                                                                                                                         
















                                                                                                                                















                                                                                                                                                                                              


















                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                























                        











S
S
S
S
A
B
A
B
A
B
Shaded area:
Shaded area:
B
U
A
Shaded area:
B
A
Shaded area:
A
B
U
U
A
A
Figure 6.1.1: Examples of Venn diagrams for various conﬁgurations of sets A and B. Note that in the
case of the lower right diagram, B ⊂A.
1. Commutative properties: A ∪B = B ∪A,
A ∩B = B ∩A.
2. Associate properties: A ∪(B ∪C) = (A ∪B) ∪C,
A ∩(B ∩C) = (A ∩B) ∩C.
3. Distributive properties: A∩(B∪C) = (A∩B)∪(A∩C),
A∪(B∩C) = (A∪B)∩(A∪C).
4. De Morgan’s law: A ∪B = A ∩B.
Finally we deﬁne the size of a set.
Discrete sets may have a ﬁnite number of el-
ements or countably inﬁnite number of elements.
By countably inﬁnite we mean that
we could in theory count the number of elements in the sets. Two simple examples are
A = {1, 2, 3, 4, 5, 6} and A = {1, 4, 16, 64, . . .}. Discrete sets lie in opposition to continuous
sets where the elements are inﬁnite in number and cannot be counted. A simple example
is A = {x : 0 ≤x ≤2}.
Problems
1. If B ⊂A, use Venn diagrams to show that A = B ∪(B ∩A) and B ∩(B ∩A) = ∅. Hint:
Use the Venn diagram in the lower right frame of Figure 6.1.1.
2. Using Venn diagrams, show that A ∪B = A ∪(A ∩B) and B = (A ∩B) ∪(A ∩B). Hint:
For A ∩B, use the upper right frame from Figure 6.1.1.
6.2 CLASSIC PROBABILITY
All questions of probability begin with the concept of an experiment where the governing
principle is chance. The set of all possible outcomes of a random experiment is called the
sample space (or universal set); we shall denote it by S. An element of S is called a sample
point. The number of elements in S can be ﬁnite as in the ﬂipping of a coin twice, inﬁnite
but countable such as repeatedly tossing a coin and counting the number of heads, or inﬁnite
and uncountable, as measuring the lifetime of a light bulb.

296
Advanced Engineering Mathematics: A Second Course
Any subset of the sample set S is called an event. If this event contains a single point,
then the event is elementary or simple.
• Example 6.2.1
Consider an experiment that consists of two steps. In the ﬁrst step, a die is tossed. If
the number of dots on the top of the die is even, a coin is ﬂipped; if the number of dots on
the die is odd, a ball is selected from a box containing blue and green balls. The sample
space is S = {1B, 1G, 2H, 2T, 3B, 3G, 4H, 4T, 5B, 5G, 6H, 6T}. The event A of obtaining a
blue ball is A = {1B, 3B, 5B}, of obtaining a green ball is B = {1G, 3G, 5G}, and obtaining
an even number of dots when the die is tossed is C = {2H, 2T, 4H, 4T, 6H, 6T}. The simple
event of obtaining a 1 on the die and a blue ball is D = {1B}.
⊓⊔
Equally likely outcomes
An important class of probability problems consists of those whose outcomes are equally
likely. The expression “equally likely” is essentially an intuitive one. For example, if a coin
is tossed it seems reasonable that the coin is just as likely to fall “heads” as to fall “tails.”
Probability seeks to quantify this common sense.
Consider a sample space S of an experiment that consists of ﬁnitely many outcomes
that are equally likely. Then the probability of an event A is
P(A) = Number of points in A
Number of points in S .
(6.2.1)
With this simple deﬁnition we are ready to do some simple problems. An important aid
in our counting is whether we can count a particular sample only once (sampling without
replacement) or repeatedly (sampling with replacement). The following examples illustrate
both cases.
• Example 6.2.2: Balls drawn from urns with replacement
Imagine the situation where we have an urn that has k red balls and N −k black balls.
A classic problem asks: What is the chance of two balls being drawn, one after another
with replacement, where the ﬁrst ball is red and the second one is black?
We begin by labeling the k red balls with 1, 2, 3, . . . , k and black balls are numbered
k + 1, k + 2, . . . , N. The possible outcomes of the experiment can be written as a 2-tuple
(z1, z2), where z1 ∈1, 2, 3, . . . , N and z2 ∈1, 2, 3, . . . , N. A successful outcome is a red ball
followed by a black one; we can express this case by E = {(z1, z2) : z1 = 1, 2, . . . , k; z2 =
k + 1, k + 2, . . . , N}. Now the total number of 2-tuples in the sample space is N 2 while the
total number of 2-tuples in E is k(N −k). Therefore, the probability is
P(E) = k(N −k)
N 2
= p(1 −p),
(6.2.2)
where p = k/N.
⊓⊔

Probability
297
0
10
20
30
40
50
60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of people in crowd
Probability of shared birthdays
Figure 6.2.1: The probability that a pair of individuals in a crowd of n people share the same birthday.
• Example 6.2.3: Balls drawn from urns without replacement
Let us redo the previous example where the same ball now cannot be chosen twice. We
can express this mathematically by the condition z1 ̸= z2. The sample space has N(N −1)
balls and the number of successful 2-tuples is again k(N −k). The probability is therefore
given by
P(E) = k(N −k)
N(N −1) = k
N
N −k
N
N
N −1 = p(1 −p)
N
N −1.
(6.2.3)
The restriction that we cannot replace the original ball has resulted in a higher probability.
Why? We have reduced the number of red balls and thereby reduced the chance that we
again selected another red ball while the situation with the black balls remains unchanged.
⊓⊔
• Example 6.2.4: The birthday problem3
A classic problem in probability is: What is the chance that at least two individuals
share the same birthday in a crowd of n people? Actually it is easier to solve the comple-
mentary problem: What is the chance that no one in a crowd of n individuals shares the
same birthday?
For simplicity let us assume that there are only 365 days in the year. Each individual
then has a birthday on one of these 365 days. Therefore, there are a total of (365)n possible
outcomes in a given crowd.
Consider now each individual separately. The ﬁrst person has a birthday on one of 365
days. The second person, who cannot have the same birthday, has one of the remaining
364 days. Therefore, if A denotes the event that no two people have the same birthday and
each outcome is equally likely, then
P(A) = n(A)
n(S) = (365)(364) · · · (365 −n + 1)
(365)n
.
(6.2.4)
To solve the original question, we note that P(A) = 1 −P(A) where P(A) denotes the
probability that at least two individuals share the same birthday.
3 First posed by von Mises, R., 1939: ¨Uber Aufteilungs- und Besetzungswahrscheinlichkeiten. Rev. Fac.
Sci. Istambul, 4, 145–163.

298
Advanced Engineering Mathematics: A Second Course
t
t
T
T
t
τ
2
1
Figure 6.2.2: The graphical solution of whether two fellows can chat online between noon and T minutes
after noon. The shaded area denotes the cases when the two will both be online whereas the rectangle gives
the sample space.
If n = 50, P(A) ≈0.03 and P(A) ≈0.97. On the other hand, if n = 23, P(A) ≈0.493
and P(A) ≈0.507. Figure 6.2.1 illustrates P(A) as a function of n. Nymann4 computed
the probability that in a group of n people, at least one pair will have the same birthday
with at least one such pair among the ﬁrst k people.
⊓⊔
In the previous examples we counted the objects in sets A and S. Sometimes we can
deﬁne these sets only as areas on a graph. This graphical deﬁnition of probability is
P(A) = Area covered by set A
Area covered by set S .
(6.2.5)
The following example illustrates this deﬁnition.
• Example 6.2.5
Two friends, Joe and Dave, want to chat online but they will log on independently
between noon and T minutes after noon. Because of their schedules, Joe can only wait
t minutes after his log-on while Dave can only spare τ minutes. Neither fellow can stay
beyond T minutes after noon. What is the chance that they will chat?
Let us denote Joe’s log-on time by t1 and Dave’s log-on time by t2. Joe and Dave will
chat if 0 < t2 −t1 < t and 0 < t1 −t2 < τ. In Figure 6.2.2 we show the situation where
these inequalities are both satisﬁed. The area of the sample space is T 2. Therefore, from
the geometrical deﬁnition of probability, the probability P(A) that they will chat is
P(A) = T 2 −(T −t)2/2 −(T −τ)2/2
T 2
.
(6.2.6)
⊓⊔
So far there has been a single event that interests us and we have only had to compute
P(A). Suppose we now have two events that we wish to follow. How are the probabilities
P(A) and P(B) related?
4 Nymann, J. E., 1975: Another generalization of the birthday problem. Math. Mag., 53, 111–125.

Probability
299
                                                                                                                                















                        











S
B
A
B
S
B
A
Shaded area:
B
A
Shaded area:
U
U
A
Figure 6.2.3: The Venn diagram used in the derivation of Property 5.
Consider the case of ﬂipping a coin. We could deﬁne event A as obtaining a head, A =
{head}. Event B could be obtaining a tail, B = {tail}. Clearly A ∪B = {head, tail} = S,
the sample space. Furthermore, A∩B = ∅and A and B are mutually exclusive. We already
know that P(A) = P(B) = 1
2. But what happens if A ∩B is not an empty set?
From our deﬁnition of probability and previous examples, we introduce the following
three basic axions:
Axion 1: P(A), P(B) ≥0,
Axion 2: P(S) = 1,
Axion 3: P(A ∪B) = P(A) + P(B) if A ∩B = ∅.
The ﬁrst two axions are clearly true from the deﬁnition of probability and sample space.
It is the third axion that needs some attention. Here we have two mutually exclusive events
A and B in the sample space S. Because the number of points in A ∪B equals the number
of points in A plus the number of points in B, n(A∪B) = n(A)+n(B). Dividing both sides
of this equation by the number of sample points and applying Equation 6.2.1, we obtain
Axion 3 when A ∩B = ∅.
From these three axioms, the following properties can be written down:
1. P(A) = 1 −P(A)
2. P(∅) = 0
3. P(A) ≤P(B) if A ⊂B
4. P(A) ≤1
5. P(A ∪B) + P(A ∩B) = P(A) + P(B).

300
Advanced Engineering Mathematics: A Second Course
                                                                                                                                                                                                                                                                                                                                                                                                                                    





























S
A
B
A
B
U
U
A
B
Figure 6.2.4: The Venn diagram that shows that A = (A ∩B) ∪(A ∩B).
All of these properties follow readily from our deﬁnition of probability except for Prop-
erty 5 and this is an important one. To prove this property from Axion 3, consider the
Venn diagram shown in Figure 6.2.3. From this ﬁgure we see that
A ∪B = A ∪(A ∩B)
and
B = (A ∩B) ∪(A ∩B).
(6.2.7)
From Axion 3, we have that
P(A ∪B) = P(A) + P(A ∩B),
(6.2.8)
and
P(B) = P(A ∩B) + P(A ∩B).
(6.2.9)
Eliminating P(A ∩B) between Equation 6.2.8 and Equation 6.2.9, we obtain Property 5.
The following example illustrates a probability problem with two events A and B.
• Example 6.2.6
Consider Figure 6.2.4. From this ﬁgure, we see that A = (A ∩B) ∪(A ∩B). Because
A ∩B and A ∩B are mutually exclusive, then from Axion 3 we have that
P(A) = P(A ∩B) + P(A ∩B).
(6.2.10)
⊓⊔
• Example 6.2.7
A company has 400 employees. Every quarter, 100 of them are tested for drugs. The
company’s policy is to test everyone at random, whether they have been previously tested
or not. What is the chance that someone is not tested?
The chance that someone will be tested is 1/4. Therefore, the chance that someone
will not be tested is 1 −1/4 = 3/4.
⊓⊔
Permutations and combinations
By now it should be evident that your success at computing probabilities lies in correctly
counting the objects in a given set. Here we examine two important concepts for systemic
counting: permutations and combinations.

Probability
301
A permutation consists of ordering n objects without any regard to their order. For
example, the six permutations of the three letters a, b, and c are abc, acb, bac, bca, cab, and
cba. The number of permutations equals n!.
In a combination of given objects, we select one or more objects without regard to
their order. There are two types of combinations: (1) n diﬀerent objects, taken k at a time,
without repetition, and (2) n diﬀerent objects, taken k at a time, with repetitions. In the
ﬁrst case, the number of sets that can be made up from n objects, each set containing k
diﬀerent objects and no two sets containing exactly the same k things, equals
number of diﬀerent combinations =

n
k

≡
n!
k!(n −k)!.
(6.2.11)
Using the three letters a, b, and c, there are three combinations, taken two letters at a time,
without repetition: ab, ac, and bc.
In the second case, the number of sets, consisting of k objects chosen from the n objects
and each being used as often as desired, is
number of diﬀerent combinations =

n + k −1
k

.
(6.2.12)
Returning to our example using three letters, there are six combinations with repetitions:
ab, ac, bc, aa, bb, and cc.
• Example 6.2.8
An urn contains r red balls and b blue balls. If a random sample of size m is chosen,
what is the probability that it contains exactly k red balls?
If we choose a random sample of size m, we obtain

r + b
m

possible outcomes. The
number of samples that includes k red balls and m−k blue balls is

r
k
 
b
m −k

. There-
fore, the probability that a sample of size m contains exactly k red balls is

r
k
 
b
m −k


r + b
m

.
⊓⊔
• Example 6.2.9
A dog kennel has 50 dogs, including 5 German shepherds. (a) What is the probability of
choosing 3 German shepherds if 10 dogs are randomly selected? (b) What is the probability
of choosing all of the German shepherds in a group of 10 dogs that is chosen at random?
Let S denote the sample space of groups of 10 dogs. The number of those groups
is n(S) = 50!/(10!40!).
Let Ai denote the set of 10 dogs that contain i German shep-
herds. Then the number of groups of 10 dogs that contain i German shepherds is n(Ai) =
10!/[i!(10 −i)!]. Therefore, the probability that out of 50 dogs, we can select at random 10
dogs that include i German shepherds is
P(Ai) = n(Ai)
n(S) =
10!10!40!
i!(10 −i)!50!.
(6.2.13)

302
Advanced Engineering Mathematics: A Second Course
Thus, P(A3) = 1.1682 × 10−8 and P(A5) = 2.453 × 10−8.
⊓⊔
• Example 6.2.10
Consider an urn with n red balls and n blue balls inside. Let R = {r1, r2, . . . , rn} and
B = {b1, b2, . . . , bn}. Then the number of subsets of R ∪B with n elements is

2n
n

. On
the other hand, any subset of R∪B with n elements can be written as the union of a subset
of R with i elements and a subset of B with n −i elements for some 0 ≤i ≤n. Because,
for each i, there are

n
i
 
n
n −i

such subsets, the total number of subsets of red and
blue balls with n elements equals Pn
i=0

n
i
 
n
n −i

. Since both approaches must be
equivalent,

2n
n

=
n
X
i=0

n
i
 
n
n −i

=
n
X
i=0

n
i
2
(6.2.14)
because

n
n −i

=

n
i

.
⊓⊔
Conditional probability
Often we are interested in the probability of an event A provided event B occurs.
Denoting this conditional probability by P(A|B), its probability is given by
P(A|B) = P(A ∩B)
P(B)
,
P(B) > 0,
(6.2.15)
where P(A ∩B) is the joint probability of A and B. Similarly,
P(B|A) = P(A ∩B)
P(A)
,
P(A) > 0.
(6.2.16)
Therefore,
P(A ∩B) = P(A|B)P(B) = P(B|A)P(A),
(6.2.17)
and we obtain the famous Bayes’ rule
P(A|B) = P(B|A)P(A)
P(B)
.
(6.2.18)
• Example 6.2.11
Consider a box containing 10 pencils. Three of the pencils are defective with broken
lead. If we draw 2 pencils out at random, what is the chance that we will have selected
nondefective pencils?
There are two possible ways of selecting our two pencils: with and without replacement.
Let Event A be that the ﬁrst pencil is not defective and Event B be that the second pencil
is not defective. Regardless of whether we replace the ﬁrst pencil or not, P(A) =
7
10 because

Probability
303
each pencil is equally likely to be picked. If we then replace the ﬁrst pencil, we have the
same situation before any selection was made and P(B|A) = P(A) =
7
10. Therefore,
P(A ∩B) = P(A)P(B|A) = 0.49.
(6.2.19)
On the other hand, if we do not replace the ﬁrst selected pencil, P(B|A) = 6
9 because
there is one fewer nondefective pencils. Consequently,
P(A ∩B) = P(A)P(B|A) = 7
10 × 6
9 = 14
30 < 0.49.
(6.2.20)
Why do we have a better chance of obtaining defective pencils if we don’t replace the
ﬁrst one? Our removal of that ﬁrst, nondefective pencil has reduced the uncertainty because
we know that there are relatively more defective pencils in the remaining 9 pencils. This
reduction in uncertainty must be reﬂected in a reduction in the chances that both selected
pencils will be nondefective.
⊓⊔
Law of total probability
Conditional probabilities are useful because they allow us to simplify probability cal-
culations. Suppose we have n mutually exclusive events A1, A2, . . . , An whose probabilities
sum to unity, then
P(B) = P(B|A1)P(A1) + P(B|A2)P(A2) + · · · + P(B|An)P(An),
(6.2.21)
where B is an arbitrary event, and P(B|Ai) is the conditional probability of B assuming
Ai. In other words, the law (or formula) of total probability expresses the total probability
of an outcome that can be realized via several distinct events.
• Example 6.2.12
There are three boxes, each containing a diﬀerent number of light bulbs. The ﬁrst box
has 10 bulbs, of which 4 are dead. The second has 6 bulbs, of which one is dead. Finally,
there is a third box of eight bulbs, of which 3 bulbs are dead. What is the probability of
choosing a dead bulb if a bulb is randomly chosen from one of the three boxes?
The probability of choosing a dead bulb is
P(D) = P(D|B1)P(B1) + P(D|B2)P(B2) + P(D|B3)P(B3)
(6.2.22)
=
1
3
  4
10

+
1
3
 1
6

+
1
3
 3
8

= 113
360.
(6.2.23)
If we had only one box with a total 24 bulbs, of which 8 were dead, then our chance of
choosing a dead bulb would be 1/3 > 113/360.
⊓⊔
Independent events
If events A and B satisfy the equation
P(A ∩B) = P(A)P(B),
(6.2.24)

304
Advanced Engineering Mathematics: A Second Course
they are called independent events. From Equation 6.2.15 and Equation 6.2.16, we see that
if Equation 6.2.24 holds, then
P(A|B) = P(A),
P(B|A) = P(B),
(6.2.25)
assuming that P(A) ̸= 0 and P(B) ̸= 0. Therefore, the term “independent” refers to the
fact that the probability of A does not depend on the occurrence or non-occurrence of B,
and vice versa.
• Example 6.2.13
Imagine some activity where you get two chances to be successful (for example, jumping
for fruit still on a tree or shooting basketballs). If each attempt is independent and the
probability of success 0.6 is the same for each trial, what is the probability of success after
(at most) two tries?
There are two ways of achieving success. We can be successful in the ﬁrst attempt
with P(S1) = 0.6 or we can fail and then be successful on the second attempt: P(F1 ∩
S2) = P(F1)P(S2) = (0.4)(0.6) = 0.24, since each attempt is independent.
Therefore,
the probability of achieving success in two tries is 0.6 + 0.24 = 0.84. Alternatively, we
can compute the probability of failure in two attempts: P(F1 ∩F2) = 0.16.
Then the
probability of success with two tries would be the complement of the probability of two
failures: 1 −0.16 = 0.84.
⊓⊔
• Example 6.2.14
Consider the tossing of a fair die. Let event A denote the tossing of a 2 or 3. Then
P(A) = P({2, 3}) = 1
3. Let event B denote tossing an odd number, B = {1, 3, 5}. Then
P(B) = 1
2.
Now A ∩B = {3} and P(A ∩B) = 1
6. Because P(A ∩B) = P(A)P(B), events A and
B are independent.
⊓⊔
Often we can characterize each outcome of an experiment consisting of n experiments
as either a “success” or a “failure.”
If the probability of each individual success is p,
then the probability of k successes and n −k failures is pk(1 −p)n−k. Because there are
n!/[k!(n −k)!] ways of achieving these k successes, the probability of an event having k
successes in n independent trials is
Pn(k) =
n!
k!(n −k)!pk(1 −p)n−k,
(6.2.26)
where p is the probability of a success during one of the independent trials.
• Example 6.2.15
What is the probability of having two boys in a four-child family?
Let us assume that the probability of having a male is 0.5. Taking the birth of one
child as a single trial,
P4(2) = 4!
2!2!
1
2
4
= 3
8.
(6.2.27)
Note that this is not 0.5, as one might initially guess.

Probability
305
Problems
1. For the following experiments, describe the sample space:
(a) ﬂipping a coin twice
(b) selecting two items out of three items {a, b, c} without replacement
(c) selecting two items out of three items {a, b, c} with replacement
(d) selecting three balls, one by one, from a box that contains four blue balls and ﬁve green
balls without replacement
(e) selecting three balls, one by one, from a box that contains four blue balls and ﬁve green
balls with replacement.
2. Consider two fair dice. What is the probability of throwing them so that the dots sum
to seven?
3. In throwing a fair die, what is the probability of obtaining a one or two on the top side
of the cube?
4. What is the probability of getting heads exactly (a) twice or (b) thrice if you ﬂip a fair
coin 6 times?
5.
An urn contains six red balls, three blue balls, and two green balls.
Two balls are
randomly selected.
What is the sample space for this experiment?
Let X denote the
number of green balls selected. What are the possible values of X? Calculate P(X = 1).
6. Consider an urn with 30 blue balls and 50 red balls in it. These balls are identical except
for their color. If they are well mixed and you draw 3 balls without replacement, what is
the probability that the balls are all of the same color?
7. A deck of cards has 52 cards, including 4 jacks and 4 ten’s. What is the probability of
selecting a jack or ten?
8. Two boys and two girls take their place on a stage to receive an award. What is the
probability that the boys take the two end seats?
9. A lottery consists of posting a 3-digit number given by selecting 3 balls from 10 balls,
each ball having the number from 1 to 10. The balls are not replaced after they are drawn.
What are your chances of winning the lottery if the order does not matter? What are your
chances of winning the lottery if the order does matter? Write a short MATLAB code and
verify your results. You may want to read about the MATLAB intrinsic function randperm.
10. A circle of radius 1 is inscribed in a square with sides of length 2. A point is selected
at random in the square in such a manner that all the subsets of equal area of the square
are equally likely to contain the point. What is the probability that it is inside the circle?
11. In a rural high school, 20% of the students play football and 10% of them play football
and wrestle. If Ed, a randomly selected student of this high school, played football, what
is the probability that he also wrestles for his high school?

306
Advanced Engineering Mathematics: A Second Course
12. You have a well-shuﬄed card deck. What is the probability the second card in the deck
is an ace?
13. We have two urns: One has 4 red balls and 6 green balls, the other has 6 red and 4
green. We toss a fair coin. If heads, we pick a random ball from the ﬁrst urn, if tails from
the second. What is the probability of getting a red ball? How do your results compare
with the probability of getting a red ball if all of the red and green balls had been placed
into a single urn?
14. A customer decides between two dinners: a “cheap” one and an “expensive” one. The
probability that the customer chooses the expensive meal is P(E) = 0.2. A customer who
chooses the expensive meal likes it with a 80% probability P(L|E) = 0.8. A customer who
chooses the cheap meal dislikes it with 70% probability P(D|C) = 0.7.
(a) Compute the probability that a customer (1) will choose a cheap meal, (2) will be
disappointed with an expensive meal, and (3) will like the cheap meal.
(b) Use the law of total probability to compute the probability that a customer will be
disappointed.
(c) If a customer found his dinner to his liking, what is the probability that he or she chose
the expensive meal? Hint: Use Bayes’ theorem.
15. Suppose that two points are randomly and independently selected from the interval
(0, 1). What is the probability the ﬁrst one is greater than 1/4, and the second one is less
than 3/4? Check your result using rand in MATLAB.
16. A certain brand of electronics chip is found to fail prematurely in 1% of all cases. If
three of these chips are used in three independent sets of equipment, what is the probability
that (a) all three will fail prematurely, (b) that two will fail prematurely, (c) that one will
fail prematurely, and (d) that none will fail?
Project: Experimenting with MATLAB’s Intrinsic Function rand
The MATLAB function rand can be used in simulations where sampling occurs with
replacement. If we write X = rand(1,100), the vector X contains 100 elements whose values
vary between 0 and 1. Therefore, if you wish to simulate a fair die, then we can set up the
following table:
0 < X < 1/6
die with one dot showing
1/6 < X < 1/3
die with two dots showing
1/3 < X < 1/2
die with three dots showing
1/2 < X < 2/3
die with four dots showing
2/3 < X < 5/6
die with ﬁve dots showing
5/6 < X < 1
die with six dots showing.
We can then write MATLAB code that counts the number of times that we obtain a one or
two. Call this number n. Then the probability that we would obtain one or two dots on a
fair die is n/100. Carry out this experiment and compare your answer with the result from
Problem 2. What occurs as you do more and more experiments?

Probability
307
Table 6.2.1: The Probability of a Male (Female) Freshman Having Always Had New Male
(Female) Roommates from a Pool of m Other Male (Female) Freshmen after n Random
Reassignments during His (Her) Freshman Year. The Numerator Is the Probability for a
Two-Person Room; the Denominator Is the Probability for a Three-Person Room.
Total Number of Freshmen
n
6
12
18
24
30
36
42
48
2
0.8000
0.3000
0.9091
0.6545
0.9412
0.7721
0.9565
0.8300
0.9655
0.8645
0.9714
0.8874
0.9756
0.9037
0.9787
0.9158
4
0.1920
0.0000
0.5409
0.4550
0.6839
0.1792
0.7594
0.3015
0.8059
0.3981
0.8374
0.4729
0.8601
0.5325
0.8773
0.5801
6
0.0000
0.0000
0.1878
0.0000
0.3692
0.0073
0.4910
0.0385
0.5750
0.0867
0.6357
0.1407
0.6815
0.1943
0.7172
0.2450
8
0.0000
0.0000
0.0310
0.0000
0.1405
0.0000
0.2524
0.0012
0.3459
0.0075
0.4214
0.0212
0.4825
0.0411
0.5325
0.0658
Project: Experimenting with
MATLAB’s Intrinsic Function randperm
MATLAB’s intrinsic function randperm(m) creates a random ordering of the numbers
from 1 to m. If you execute perm = randperm(365), this would produce a vector of length
365 and each element has a value lying between 1 and 365. If you repeat the process, you
would obtain another list of 365 numbers but they would be in a diﬀerent order.
Let us simulate the birthday problem. Invoking the randperm command, use the ﬁrst
element to simulate the birthday of student 1 in a class of N students. Repeatedly invoking
this command, create vector birthdays that contains the birthdays of the N students. Then
ﬁnd out if any of the days are duplicates of another. (Hint: You might want to explore the
MATLAB command unique.) Repeating this experiment many times, compute the chance
that a class of size N has at least two students that have the same birthday. Compare your
results with Equation 6.2.4. What occurs as the number of experiments increases?
Project: The Roommate Problem
You are a freshman at a small all-male (all-female) college with m other freshmen. For
esprit de corps, the administration requires that n times during your freshman year, you
are randomly (with equal probability) assigned new roommates. The administration does
not, however, require that you have never roomed with any of them previously.
(a) Assuming that there are 2 freshmen per room (so that m + 1 is even), what is the
probability that all of your roommates during the year have never roomed with you before?
Verify your answer by writing a MATLAB script that simulates this housing practice. I used
the MATLAB intrinsic functions randi(m,1,n), unique and length and ran the simulation
10 million times.
(b) Assuming that there are 3 freshmen per room (so that m + 1 is a multiple of 3),
what is the probability that all of your roommates during the year have never roomed with

308
Advanced Engineering Mathematics: A Second Course
you before? Verify your answer by writing a MATLAB script that simulates this housing
practice. I used the MATLAB intrinsic functions randperm, unique and length and ran the
simulation 10 million times.
6.3 DISCRETE RANDOM VARIABLES
In the previous section we presented the basic concepts of probability. In high school
algebra you were introduced to the concept of a variable—a quantity that could vary unlike
constants and parameters. Here we extend this idea to situations where the variations are
due to randomness.
A random variable is a single-valued real function that assigns a real number, the value,
to each sample point t of S. The variable can be discrete, such as the ﬂipping of a coin,
or continuous, such as the lifetime of a light bulb. The sample space S is the domain of
the random variable X(t), and the collection of all numbers X(t) is the range. Two or
more sample points can give the same value of X(t), but we will never allow two diﬀerent
numbers in the range of X(t) for a given t.
The term “random variable” is probably a poor one. Consider the simple example of
tossing a coin. A random variable that describes this experiment is
X[si] =

1,
s1 = head,
0,
s2 = tail.
(6.3.1)
An obvious question is: What is random about Equation 6.3.1? If a head is tossed, we
obtain the answer one; if a tail is tossed, we obtain a zero. Everything is well deﬁned;
there is no element of chance here. The randomness arises from the tossing of the coin.
Until the experiment (tossing of the coin) is performed, we do not know the outcome of
the experiment and the value of the random variable. Therefore, a random variable is a
variable that may take diﬀerent values if a random experiment is conducted and its value is
not known in advance.
We begin our study of random variables by focusing on those arising from discrete
events. If X is discrete, X assumes only ﬁnitely many or countably many values: x1, x2, x3,
. . .. For each possible value of xi, there is a corresponding positive probability pX[x1] =
P(X = x1), pX[x2] = P(X = x2), . . . given by the probability mass function. For values of
x diﬀerent from xi, say x1 < x < x2, the probability mass function equals zero. Therefore,
we have that
pX[xi] =
n pi,
x = xi,
0,
otherwise,
(6.3.2)
where i = 1, 2, 3, . . ..
At this point it is convenient to introduce several special classes or types of random
variables. First we have independent random variables where the realization of one does
not aﬀect the probability distribution of the other. Of equal importance are identically
distributed random variables where the random variables have the same probability dis-
tribution. Finally we can combine both properties into independent identically distributed
(i.i.d.) random variables. This last class occurs repeatedly in common applications.
• Example 6.3.1
Consider a fair die. We can describe the results from rolling this fair die via the discrete
random variable X, which has the possible values xi = 1, 2, 3, 4, 5, 6 with the probability
pX[xi] = 1
6 each. Note that 0 ≤pX[xi] < 1 here. Furthermore,
6
X
i=1
pX[xi] = 1.
(6.3.3)

Probability
309
1
2
3
4
5
6
1
2
3
4
5
6
x
x
F  (x)
X
X
1/6
1/6
1/3
1/2
2/3
5/6
1
p  [x]
Figure 6.3.1: The probability mass function for a fair die.
Figure 6.3.1 illustrates the probability mass function.
⊓⊔
• Example 6.3.2
Let us now modify Example 6.3.1 so that
X[si] =
( 1,
si = 1, 2,
2,
si = 3, 4,
3,
si = 5, 6.
(6.3.4)
The probability mass function becomes
pX[1] = pX[2] = pX[3] = 1
3.
(6.3.5)
⊓⊔
• Example 6.3.3
Consider the probability mass function:
pX[xn] =

k(1/2)n,
n = 0, 1, 2, . . . ,
0,
otherwise.
(6.3.6)
Let us (a) ﬁnd the value of k, (b) ﬁnd P(X = 2), (c) ﬁnd P(X ≤2), and (d) P(X ≥1).
From the properties of probability mass function,
k
∞
X
n=0
1
2
n
= k
1
1 −1
2
= 2k = 1.
(6.3.7)
Therefore, k = 1
2. Note that 0 ≤pX[xn] ≤1.
Having found k, we immediately have
P(X = 2) = pX[x2] = 1
8,
(6.3.8)
P(X ≤2) = pX[x0] + pX[x1] + pX[x2] = 7
8,
(6.3.9)
and
P(X ≥1) = 1 −P(X = 0) = 1
2.
(6.3.10)
⊓⊔

310
Advanced Engineering Mathematics: A Second Course
Some Properties of the Probability Mass Function pX[xi]
0 ≤pX[xk] < 1,
pX[x] = 0
if
x ̸= xk,
k = 1, 2, . . .
X
n
pX[xn] = 1
FX(x) = P(X ≤x) =
X
xk≤x
pX[xk]
P(a < x ≤b) =
X
a<xk≤b
pX[xk]
Having introduced the probability mass function, an alternative means of describing the
probabilities of a discrete random variable is the cumulative distribution function.
It is
deﬁned as
FX(x) = P(X ≤x),
−∞< x < ∞.
(6.3.11)
It is computed via
FX(x) =
X
xi≤x
pX[xi] =
X
xi≤x
pi.
(6.3.12)
Consequently, combining Equation 6.3.11 and Equation 6.3.12, we obtain
P(a < x ≤b) =
X
a<xi≤b
pi.
(6.3.13)
Equation 6.3.13 gives the probability over the interval (a, b].
• Example 6.3.4
A Bernoulli experiment is a random experiment, the outcome of which is a success
or failure. Consider now a sequence of independent Bernoulli trials with probability p of
success from trial to trial. This sequence is observed until the ﬁrst success occurs. Let X
denote a random variable that equals the trial number on which the ﬁrst success occurs.
The probability mass function is then
pX[xn] = (1 −p)n−1p,
n = 1, 2, 3, . . . .
(6.3.14)
Let us compute the cumulative distribution function.
For geometric series, we begin by noting that
∞
X
n=0
arn =
∞
X
n=1
arn−1 =
a
1 −r,
|r| < 1.
(6.3.15)
Next we check Equation 6.3.14 and determine whether it is a valid probability mass function.
It is because
∞
X
n=1
pX[xn] =
∞
X
n=1
(1 −p)n−1p =
p
1 −(1 −p) = 1,
(6.3.16)

Probability
311
1
F  (x)
1−p
x
p
X
1
Figure 6.3.2: The cumulative distribution function for a Bernoulli random variable.
where we used Equation 6.3.15. Next, we note that
P(X > m) =
∞
X
n=m+1
(1 −p)n−1p = (1 −p)mp
1 −(1 −p) = (1 −p)m.
(6.3.17)
Therefore,
FX(x) = P(X ≤m) = 1 −P(X > m) = 1 −(1 −p)m,
(6.3.18)
where m = 1, 2, 3, . . . .
⊓⊔
• Example 6.3.5: Generating discrete random variables via MATLAB
In this example we show how to generate a discrete random variable using MATLAB’s
intrinsic function rand. This MATLAB command produces random, uniformly distributed
(equally probable) reals over the interval (0, 1). How can we use this function, when in the
case of discrete random variables, we have only integer values, such as k = 1, 2, 3, 4, 5, 6, in
the case of tossing a die?5
Consider the Bernoulli random variable X = k, k = 0, 1. As you will show in your
homework, it has the cumulative distribution function of
FX(x) =
( 0,
x < 0,
1 −p,
0 ≤x < 1,
1,
1 ≤x.
(6.3.19)
See Figure 6.3.2.
Imagine now a program that includes the MATLAB function rand, which yields the
value t. Then, if 0 < t ≤1 −p, Figure 6.3.2 gives us that X = 0. On the other hand, if
1 −p < t < 1, then X = 1. Thus, to obtain M realizations of the Bernoulli random variable
X, the MATLAB code would read for a given p:
clear;
5 This technique is known as the inverse transform sampling method. See pages 85–102 in Devroye, L.,
1986: Non-Uniform Random Variable Generation. Springer-Verlag, 843 pp.

312
Advanced Engineering Mathematics: A Second Course
0
5
10
15
20
25
30
35
40
45
50
−0.5
0
0.5
1
1.5
 k
 xk
(a)
0
20
40
60
80
100
120
140
160
180
200
0
0.2
0.4
0.6
0.8
1
 M
 est. pX[1]
(b)
Figure 6.3.3: (a) Outcomes of the Bernoulli random variable generated by the MATLAB function rand. (b)
The computed value of the probability mass function pX[1] as a function of M realization of the Bernoulli
random variable. The dashed line is the line for the exact answer p = 0.4.
for i = 1:M
t = rand(1,1);
if (t <= 1-p) X(i,1) = 0;
else
X(i,1) = 1;
end; end
The end product of this code creates a vector X of length M consisting of a random variable
with either zeros or ones. This is shown in Figure 6.3.3(a) when p = 0.4.
Once we have generated this random variable, we can use its relative frequency to
compute its probability mass function and cumulative distribution function from
ˆpX[xk] = Number of outcomes equal to k
M
,
(6.3.20)
and
ˆFX(x) = Number of outcomes ≤x
M
.
(6.3.21)
In Figure 6.3.3(b) we have computed the value of ˆpX[1]. Clearly it should equal p. As this
ﬁgure shows, we obtain poor results when M is small, with ˆpX[1] moving randomly above
and below the correct answer. As M becomes larger, our estimate improves.
Problems
1. The Bernoulli distribution has the probability mass function
pX[xk] = P(X = k) = pk(1 −p)1−k,
k = 0, 1,
where 0 ≤p ≤1. (a) Show that this distribution is a valid probability mass function. (b)
Find its cumulative distribution function.

Probability
313
x
x
p  (x)
X
dx
Figure 6.4.1: A probability density function.
2.
An experiment is performed where a digit, ranging from 0 to 9, is repeatedly and
randomly chosen. If X denotes the times that this experiment must be repeated until the
digit 0 is selected, ﬁnd P(X).
3. A scientiﬁc company needs a programmer who knows an unusual programming language.
If only 5% of programmers know this language, how many programmers should the company
interview to have a 75% chance of ﬁnding such a programmer?
6.4 CONTINUOUS RANDOM VARIABLES
In the previous section we examined random variables that can assume only certain
discrete values. Here we extend the concept of random variables so that they can take on
values over a continuous interval. Typical examples of continuous random variables include
the noisy portion of the voltage within an ampliﬁer, the phase of a propagating wave, and
the amount of precipitation.
An important quantity that we introduced in the previous section was the probability
mass function. What is the corresponding function for continuous random variables? From
the fundamental concepts of probability, we know that the probability of a continuous
variable assuming one speciﬁc value out of its possible range values equals zero; it is merely
one point out of an inﬁnite number of points in the sample space. On the other hand, there
is a ﬁnite probability that the value assumed by the random variable X will lie within an
arbitrarily small interval dx and this probability will depend on the length of the interval.
Another factor that should inﬂuence the probability is the value of x. There is no
reason why the probability of X should be independent of x. Consequently, an equation
for probability in the interval x < X ≤x + dx requires a function pX(x), which acts as a
weighting function and models the relative frequency behavior of X. For these reasons, the
probability that a continuous random variable X will assume a value lying between x and
x + dx is given by
P(x < X ≤x + dx) = pX(x) dx.
(6.4.1)
Figure 6.4.1 illustrates a possible example of pX(x) where the shaded area equals the prob-
ability P(x < X ≤x + dx). Clearly the function pX(x) = P(x < X ≤x + dx)/dx has the
dimension of probability per inﬁnitesimal interval dx and is called, for that reason, the prob-
ability density. Furthermore, although pX(x) dx ≤1, this does not mean that pX(x) ≤1.
A family of random variables having the same probability density is identically distributed.
The function pX(x) must also satisfy several additional conditions. Because probability
cannot be negative, pX(x) ≥0 of all x. Furthermore, as Figure 6.4.1 suggests, if we add

314
Advanced Engineering Mathematics: A Second Course
Some Properties of the Probability Density Function pX(x)
pX(x) ≥0,
Z ∞
−∞
pX(x) dx = 1
P(a < X ≤b) =
Z b
a
pX(x) dx
up all of the possible values of x, then we have a certain event.
We can express this
mathematically by
Z ∞
−∞
pX(x) dx = 1.
(6.4.2)
Thus, a probability density has the properties given by Equation 6.4.1 and Equation 6.4.2.
It must also be a single-valued function of x. Note that these conditions do not require that
pX(x) is a continuous function of x.
Let us now consider the probability P(a < X ≤b) where a and b are constants. If
we subdivide the range of x between a and b into inﬁnitesimal intervals (x, x + dx), the
probability that the random variable will assume a value from one such interval is given by
Equation 6.4.1. The probability that the variable will assume a value in the interval (a, b)
equals the sum of the probabilities from each subinterval between a and b and is given by
the area under the curve p(x) between x = a and x = b. Therefore,
P(a < X ≤b) =
Z b
a
pX(x) dx.
(6.4.3)
If a = −∞, we have that
P(X ≤b) =
Z b
−∞
pX(x) dx.
(6.4.4)
Alternatively, setting b = ∞,
P(a < X) =
Z ∞
a
pX(x) dx.
(6.4.5)
From Equation 6.4.3 we also have
P(X > a) = 1 −P(X < a) = 1 −
Z a
−∞
pX(x) dx =
Z ∞
a
pX(x) dx.
(6.4.6)
From Equation 6.4.4 we now deﬁne
FX(x) = P(X ≤x) =
Z x
−∞
pX(ξ) dξ.
(6.4.7)
This function FX(x) is called the cumulative distribution function, or simply the distribution
function, of the random variable X. Clearly,
pX(x) = F ′
X(x).
(6.4.8)
Therefore, from the properties of pX(x), we have that (1) FX(x) is a nondecreasing function
of x, (2) FX(−∞) = 0, (3) FX(∞) = 1, and (4) P(a < X ≤b) = FX(b) −FX(a).

Probability
315
• Example 6.4.1
The continuous random variable X has the probability density function
pX(x) =

k
 x −x2
,
0 < x < 1,
0,
otherwise.
(6.4.9)
What must be the value of k? What is the cumulative distribution function? What is
P(X < 1/2)?
From Equation 6.4.2, we have that
Z ∞
−∞
pX(x) dx = k
Z 1
0
 x −x2
dx = k x2
2 −x3
3

1
0
= k
6.
(6.4.10)
Therefore, k must equal 6.
Next, we note that
FX(x) = P(X ≤x) =
Z x
−∞
pX(ξ) dξ.
(6.4.11)
If x < 0, FX(x) = 0. For 0 < x < 1, then
FX(x) = 6
Z x
0
 ξ −ξ2
dξ = 6
ξ2
2 −ξ3
3

x
0
= 3x2 −2x3.
(6.4.12)
Finally, if x > 1,
FX(x) = 6
Z 1
0
 ξ −ξ2
dξ = 1.
(6.4.13)
In summary,
FX(x) =
(
0,
0 ≤x,
3x2 −2x3,
0 < x ≤1,
1,
1 < x.
(6.4.14)
Because P(X ≤x) = FX(x), we have that P(X < 1
2) = 1
2 and P(X > 1
2) = 1 −P(X <
1
2) = 1
2.
⊓⊔
• Example 6.4.2: Generating continuous random variables via MATLAB6
In the previous section we showed how the MATLAB function rand can be used to gen-
erate outcomes for a discrete random variable. Similar considerations hold for a continuous
random variable.
Consider the exponential random variable X. Its probability density function is
pX(x) =

0,
x < 0,
λe−λx,
0 < x,
(6.4.15)
where λ > 0. For homework you will show that the corresponding cumulative distribution
function is
FX(x) =

0,
x ≤0,
1 −e−λx,
0 < x.
(6.4.16)
6 This technique is known as the inverse transform sampling method. See pages 27–39 in Devroye, L.,
1986: Non-Uniform Random Variable Generation. Springer-Verlag, 843 pp.

316
Advanced Engineering Mathematics: A Second Course
F  (x)
X
1
2
1
x
3
Figure 6.4.2: The cumulative distribution function for an exponential random variable.
Figure 6.4.2 illustrates this cumulative density function when λ = 1. How can we use these
results to generate a MATLAB code that produces an exponential random variable?
Recall that both MATLAB function rand and the cumulative distribution function pro-
duce values that vary between 0 and 1. Given a value from rand, we can compute the
corresponding X = x, which would give the same value from the cumulative distribution
function. In short, we are creating random values for the cumulative distribution function
and using those values to give the exponential random variable via
X = x = −ln(1 −rand) /λ,
(6.4.17)
where we have set FX(x) = rand. Therefore, the MATLAB code to generate exponential
random variables for a particular lambda is
clear;
for i = 1:M
t = rand(1,1);
X(i,1) = -log(1-t) / lambda;
end
where M is the number of experiments that we run. In Figure 6.4.3(a) we illustrate the ﬁrst
200 outcomes from our numerical experiment to generate an exponential random variable.
To compute the probability density function we use the ﬁnite diﬀerence approximation
of Equation 6.4.1, or
ˆp(x0) = Number of outcomes in [x0 −∆x/2, x0 + ∆x/2]
M∆x
,
(6.4.18)
where ∆x is the size of the bins into which we collect the various outcomes. Figure 6.4.3(b)
illustrates this numerical estimation of the probability density function in the case of an
exponential random variable. The function ˆpX(x) was created from the MATLAB code:
clear;
delta x = 0.2; lambda = 1; M = 1000; % Initialize ∆x, λ and M
% sample M outcomes from the uniformly distributed distribution
t = rand(M,1);
% generate the exponential random variable
x = - log(1-t)/lambda;

Probability
317
0
5
10
15
20
25
30
35
40
45
50
0
1
2
3
4
5
6
 k
 xk
(a)
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.2
0.4
0.6
0.8
1
 x
 est. pX(x)
(b)
Figure 6.4.3: (a) Outcomes of a numerical experiment to generate an exponential random variable using
the MATLAB function rand. (b) The function ˆpX(x) given by Equation 6.4.18 as a function of x for an
exponential random variable with M = 1000. The dashed black line is the exact probability density function.
% create the various bins [x0 −∆x/2, x0 + ∆x/2]
bincenters=[delta x/2:delta x:5];
bins=length(bincenters); % count the number of bins
% now bin the M outcomes into the various bins
[n,x out] = hist(x,bincenters);
n = n / (delta x*M); % compute the probability per bin
bar h = bar(x out,n); % create the bar graph
bar child = get(bar h,’Children’);
set(bar child,’CData’,n);
colormap(Autumn);
Problems
1. The probability density function for the exponential random variable is
pX(x) =

0,
x < 0,
λe−λx,
0 < x,
with λ > 0. Find its cumulative distribution function.
2. Given the probability density function
pX(x) =

kx,
0 < x < 2,
0,
otherwise,

318
Advanced Engineering Mathematics: A Second Course
S
0.35
0.4
0.45
0.5
0.55
0.6
0.65
probability density
0
2
4
6
8
10
12
14
Figure 6.4.4: Computed probability density function for the sum S = (X1 + X2 + X3 + · · · + X100)/100,
where Xi is the ith sample from a uniform distribution.
where k is a constant, (a) compute the value of k, (b) ﬁnd the cumulative density function
FX(x), and (c) ﬁnd the P(1 < X ≤2).
3. Given the probability density function
pX(x) =

k (1 −|x|) ,
|x| < 1,
0,
|x| > 1,
where k is a constant, (a) compute the value of k and (b) ﬁnd the cumulative density
function FX(x).
Project: Central Limit Theorem
Consider the sum S = (X1 + X2 + X3 + · · · + X100)/100, where Xi is the ith sample
from a uniform distribution.
Step 1: Write a MATLAB program to compute the probability density function of S. See
Figure 6.4.4.
Step 2: The central limit theorem states the distribution of the sum (or average) of a
large number of independent, identically distributed random variables will be approximately
normal, regardless of the underlying distribution. Do your numerical results agree with this
theorem?
6.5 MEAN AND VARIANCE
In the previous two sections we explored the concepts of the random variable and
distribution.
Here we introduce two parameters, mean and variance, that are useful in
characterizing a distribution.

Probability
319
The mean µX is deﬁned by
µX = E(X) =







X
k
xk pX[xk],
X discrete,
Z ∞
−∞
x pX(x) dx,
X continuous.
(6.5.1)
The mean provides the position of the center of the distribution. The operator E(X), which
is called the expectation of X, gives the average value of X that one should expect after many
trials.
Two important properties involve the expectation of the sum and product of two ran-
dom variables X and Y . The ﬁrst one is
E(X + Y ) = E(X) + E(Y ).
(6.5.2)
Second, if X and Y are independent random variables, then
E(XY ) = E(X)E(Y ).
(6.5.3)
The proofs can be found elsewhere.7
The variance provides the spread of a distribution. It is computed via
σ2
X = Var(X) = E{[X −E(X)]2},
(6.5.4)
or
σ2
X =







X
k
(xk −µX)2pX[xk],
X discrete,
Z ∞
−∞
(x −µX)2pX(x) dx,
X continuous.
(6.5.5)
If we expand the right side of Equation 6.5.4, an alternative method for ﬁnding the variance
is
σ2
X = Var(X) = E(X2) −[E(X)]2,
(6.5.6)
where
E(Xn) =







X
k
xn
k pX[xk],
X discrete,
Z ∞
−∞
xnpX(x) dx,
X continuous.
(6.5.7)
• Example 6.5.1: Mean and variance of M equally likely outcomes
Consider the random variable X = k where k = 1, 2, . . . , M. If each event has an
equally likely outcome, pX[xk] = 1/M. Then the expected or average or mean value is
µX = 1
M
M
X
k=1
xk = M(M + 1)
2M
= M + 1
2
.
(6.5.8)
7 For example, Kay, S. M., 2006: Intuitive Probability and Random Processes Using MATLAB. Springer,
833 pp. See Sections 7.7 and 12.7.

320
Advanced Engineering Mathematics: A Second Course
Note that the mean does not equal any of the possible values of X. Therefore, the expected
value need not equal a value that will be actually observed.
Turning to the variance,
Var(X) = (M + 1) [(2M + 1)/6 −(M + 1)/4]
(6.5.9)
= (M + 1) [4M + 2 −3M −3] /12
(6.5.10)
= (M + 1)(M −1)/12 = (M 2 −1)/12,
(6.5.11)
because
E(X2) = 1
M
M
X
k=1
x2
k = M(M + 1)(2M + 1)
6M
= (M + 1)(2M + 1)
6
.
(6.5.12)
We used Equation 6.5.6 to compute the variance.
⊓⊔
• Example 6.5.2
Let us ﬁnd the mean and variance of the random variable X whose probability density
function is
pX(x) =

kx,
0 < x < 1,
0,
otherwise.
(6.5.13)
From Equation 6.5.1, we have that
µX = E(X) =
Z 1
0
x(kx) dx = kx3
3

1
0
= k
3.
(6.5.14)
From Equation 6.5.6, the variance of X is
σ2
X = Var(X) = E(X2) −[E(X)]2 =
Z 1
0
x2(kx) dx −k2
9 = kx4
4

1
0
−k2
9 = k
4 −k2
9 .(6.5.15)
⊓⊔
• Example 6.5.3: Characteristic functions
The characteristic function of a random variable is deﬁned by
φX(ω) = E[exp(iωX)].
(6.5.16)
If X is a discrete random variable, then
φX(ω) =
∞
X
k=−∞
pX[xk]eikω.
(6.5.17)
On the other hand, if X is a continuous random variable,
φX(ω) =
Z ∞
−∞
pX(x)eiωx dx,
(6.5.18)
the inverse Fourier transform (times 2π) of the Fourier transform, pX(x).

Probability
321
Characteristic functions are useful for computing various moments of a random variable
via
E(Xn) = 1
in
dnϕX(ω)
dωn

ω=0
.
(6.5.19)
This follows by taking repeated diﬀerentiation of Equation 6.5.16 and then evaluating the
diﬀerentiation at ω = 0.
Consider, for example, the exponential probability density function pX(x) = λe−λx
with x, λ > 0. A straightforward calculation gives
φX(ω) =
λ
λ −ωi.
(6.5.20)
Substituting Equation 6.5.20 into Equation 6.5.19 yields
E(Xn) = n!
λn .
(6.5.21)
In particular,
E(X) = 1
λ
and
E(X2) = 2
λ2 .
(6.5.22)
Consequently, µX = 1/λ and Var(X) = E(X2) −µ2
X = 1/λ2.
⊓⊔
• Example 6.5.4: Characteristic function for a Gaussian distribution
Let us ﬁnd the characteristic function for the Gaussian distribution and then use that
characteristic function to compute the mean and variance.
Because
pX(x) =
1
√
2π σ e−(x−µ)2/(2σ2),
(6.5.23)
the characteristic function equals
φX(ω) =
1
√
2π σ
Z ∞
−∞
e−(x−µ)2/(2σ2)+iωx dx
(6.5.24)
= eiωµ−σ2ω2/2

1
√
2π σ
Z ∞
−∞
exp

−(x −µ −iωσ2)2
2σ2

dx

(6.5.25)
= eiωµ−σ2ω2/2
(6.5.26)
because the quantity within the wavy brackets equals one.
Given this characteristic function, Equation 6.5.26, we have that
φ′
X(ω) = (iµ −σ2ω)eiωµ−σ2ω2/2.
(6.5.27)
Therefore, φ′
X(0) = iµ and from Equation 6.5.19, µX = E(X) = µ. Furthermore,
φ′′
X(ω) = (iµ −σ2ω)2eiωµ−σ2ω2/2 −σ2eiωµ−σ2ω2/2.
(6.5.28)
Consequently, φ′′
X(0) = −µ2 −σ2 and Var(X) = E(X2) −µ2
X = σ2.

322
Advanced Engineering Mathematics: A Second Course
• Example 6.5.5: (Weak) law of large numbers
One of the reasons why independent identically distributed (i.i.d.) random variables
play such a large role in probability and statistics lies in the (weak) law of large numbers. If
X1, X2, X3, . . . , Xn denote i.i.d. random variables and An = 1
n
Pn
i=1 Xi, then P(|An −µ| ≥
ǫ) →0, as n →∞for any ǫ > 0. How is this law useful in daily life? Let us observe some
random variable many times and take the average of these observations. The law of large
numbers predicts that this average will converges to a single value, namely the mean.
Problems
1. Let X(s) denote a discrete random variable associated with a fair coin toss. Then
X(s) =

0,
s = tail,
1,
s = head.
Find the expected value and variance of this random variable.
2. The geometric random variable X has the probability mass function:
pX[xk] = P(X = k) = p(1 −p)k−1,
k = 1, 2, 3, . . . .
Find its mean and variance. Hint:
∞
X
k=1
krk−1 =
1
(1 −r)2 ,
∞
X
k=2
k(k −1)rk−2 =
2
(1 −r)3 ,
|r| < 1,
and E(X2) = E[X(X −1)] + E(X).
3. Given
pX(x) =

kx(2 −x)
0 < x < 2,
0,
otherwise,
(a) ﬁnd k and (b) its mean and variance.
4. Given the probability density
pX(x) = (a2 −x2)ν−1
2 ,
ν > −1
2,
ﬁnd its characteristic function using integral tables.
For the following distributions, ﬁrst ﬁnd their characteristic functions. Then compute the
mean and variance using Equation 6.5.19.
5. Binomial distribution:
pX[xk] =

n
k

pkqn−k,
0 < p < 1,
where q = 1 −p. Hint: Use the binomial theorem to simplify Equation 6.5.17.

Probability
323
6. Poisson distribution:
pX[xk] = e−λ λk
k! ,
0 < λ.
7. Geometric distribution:
pX[xk] = qkp,
0 < p < 1,
where q = 1 −p.
8. Uniform distribution:
pX(x) = H(x −a) −H(x −b)
b −a
,
b > a > 0.
Project: MATLAB’s Intrinsic Function mean and var
MATLAB has the special commands mean and var to compute the mean and variance,
respectively, of the random variable X.
Use the MATLAB command randn to create a
random variable X(n) of length N. Then, ﬁnd the mean and variance of X(n). How do these
parameters vary with N?
Project: Monte Carlo Integration and Importance Sampling
Consider the integral I =
R 1
0
√
1 −x2 dx = π/4. If we were to compute it numerically
by the conventional midpoint rule, the approximate value is given by
IN = 1
N
N
X
n=1
f(xn),
(1)
where f(x) =
√
1 −x2 and xn = (n −1/2)/N. For N = 10, 50, 100, and 500, the absolute
value of the relative error is 2.7 × 10−3, 2.4 × 10−4, 8.6 × 10−5, and 7.7 × 10−6, respectively.
Monte Carlo integration is a simple alternative method for doing the numerical inte-
gration using random sampling. It is a particularly powerful technique for approximating
complicated integrals. Here you will explore a simple one-dimensional version of this scheme.
Consider the random variable:
IM = 1
M
M
X
m=1
f(xm),
(2)
where xm is the mth sample point taken from the uniform distribution. IM is a random
variable because it is a function of the random variable xm. Therefore,
E(IM) = 1
M
M
X
m=1
E[f(xm)] = 1
M
M
X
m=1
Z 1
0
f(x)p(x) dx
= 1
M
M
X
m=1
Z 1
0
f(x) dx =
Z 1
0
f(x) dx = I,
because p(x), the probability of the uniform distribution, equals 1. Furthermore, as we
increase the number of samples M, IM approaches I. By the strong law of large numbers,

324
Advanced Engineering Mathematics: A Second Course
−0.3
−0.2
−0.1
0
0.1
0.2
0
1
2
3
4
5
6
M = 10
probability density
−0.1
−0.05
0
0.05
0.1
0
2
4
6
8
10
12
14
M = 50
−0.08
−0.06
−0.04
−0.02
0
0.02
0.04
0.06
0.08
0
5
10
15
20
M = 100
approximate − exact value of the integral
probability density
−0.04
−0.03
−0.02
−0.01
0
0.01
0.02
0.03
0.04
0
10
20
30
40
50
M = 500
approximate − exact value of the integral
Figure 6.5.1: The probability density function arising from using Monte Carlo integration to compute
R 1
0
√
1 −x2 dx for various values of M.
this limit is guaranteed to converge to the exact solution: P(limM→∞IM −I) = 1. Equation
(2) is not the midpoint rule because the uniform grid xn has been replaced by randomly
spaced grid points.
Step 1: Write a MATLAB program that computes IM for various values of M when xm is
selected from a uniform distribution. By running your code thousands of times, ﬁnd the
probability density as a function of the diﬀerence between IM and I. Compute the mean
and variance of IM. How does the variance vary with M? See Figure 6.5.1.
The reason why standard Monte Carlo integration is not particularly good is the fact
that we used a uniform distribution. A better idea would be to sample from regions where
the integrand is larger. This is the essence of the concept of importance sampling: That
certain values of the input random variable xm in a simulation have more impact on the
parameters being estimated than others.
We begin by noting that
I =
Z 1
0
f(x) dx =
Z 1
0
f(x)
p1(x)p1(x) dx,
where p1(x) is a new probability density function that replaces the uniform probability
distribution and is relatively larger when f(x) is larger and relatively smaller when f(x) is
smaller.
The question now becomes how to compute p1(x). We shall use the VEGAS algo-
rithm, which constructs p1(x) by sampling f(x) K times, where K < M. Within each kth
subinterval we assume that there are M/K uniformly distributed points. Therefore,
p1(xm) =
K f(sm)
PK
k=1 f(sk)
,
where sk is the center point of the kth subinterval within which the mth point is located.
For each m, we must ﬁnd xm. This is done in two steps: First we randomly choose the kth

Probability
325
−0.04
−0.03
−0.02
−0.01
0
0.01
0.02
0.03
0
10
20
30
40
50
K = 5
probability density
−0.03
−0.02
−0.01
0
0.01
0.02
0
10
20
30
40
50
60
70
80
K = 10
−0.02 −0.015 −0.01 −0.005
0
0.005
0.01
0.015
0
20
40
60
80
100
120
140
K = 20
approximate − exact value of the integral
probability density
−0.015
−0.01
−0.005
0
0.005
0.01
0
50
100
150
200
250
300
350
K = 50
approximate − exact value of the integral
Figure 6.5.2: The probability density function arising from using importance sampling with Monte Carlo
integration to compute R 1
0
√
1 −x2 dx for various values of K and M = 100.
subinterval using a uniform distribution. Then we randomly choose the point xm within
that subinterval using a uniform distribution. Therefore, our modiﬁed integration scheme
becomes
IM = 1
M
M
X
m=1
f(xm)
p1(xm).
(3)
Now,
E(IM) = 1
M
M
X
m=1
E
 f(xm)
p1(xm)

= 1
M
M
X
m=1
Z 1
0
f(x)
p1(x)p1(x)p2(x) dx
= 1
M
M
X
m=1
Z 1
0
f(x) dx =
Z 1
0
f(x) dx = I,
because p2(x) = 1.
Step 2: Write a MATLAB program that computes IM for various values of K for a ﬁxed value
of M. Recall that you must ﬁrst select the subdivision using the MATLAB function rand
and then the value of xm within the subdivision using a uniform distribution. By running
your code thousands of times, ﬁnd the probability density as a function of the diﬀerence
between IM and I. Compute the mean and variance of IM. How does the variance vary
with M? See Figure 6.5.2.
6.6 SOME COMMONLY USED DISTRIBUTIONS
In the previous sections we introduced the concept of probability distributions and their
description via mean and variance. In this section we focus on some special distributions,
both discrete and continuous, that appear often in engineering.

326
Advanced Engineering Mathematics: A Second Course
Bernoulli distribution
Consider an experiment where the outcome can be classiﬁed as either a success or
failure. The probability of a success is p and the probability of a failure is 1−p. Then these
“Bernoulli trials” have a random variable X associated with them where the probability
mass function is given by
pX[xk] = P(X = k) = pk(1 −p)1−k,
k = 0, 1,
(6.6.1)
where 0 ≤p ≤1. From Equation 6.3.12 the cumulative density function of the Bernoulli
random variable X is
FX(x) =
(
0,
x < 0,
1 −p,
0 ≤x < 1,
1,
1 ≤x.
(6.6.2)
The mean and variance of the Bernoulli random variable X are
µX = E(X) = p,
and
σ2
X = Var(X) = p(1 −p).
(6.6.3)
• Example 6.6.1
A simple pass and fail process is taking a ﬁnal exam, which can be modeled by a
Bernoulli distribution. Suppose a class passed a ﬁnal exam with the probability of 0.75. If
X denotes the random variable that someone passed the exam, then
E(X) = p = 0.75,
and
Var(X) = p(1 −p) = (0.75)(0.25) = 0.1875. (6.6.4)
⊓⊔
Geometric distribution
Consider again an experiment where we either have success with probability p or failure
with probability 1−p. This experiment is repeated until the ﬁrst success occurs. Let random
variable X denote the trial number on which this ﬁrst success occurs. Its probability mass
function is
pX[xk] = P(X = k) = p(1 −p)k−1,
k = 1, 2, 3, . . . .
(6.6.5)
From Equation 6.3.12 the cumulative density function of this geometric random variable X
is
FX(x) = P(X ≤x) = 1 −(1 −p)k.
(6.6.6)
The mean and variance of the geometric random variable X are
µX = E(X) = 1
p,
and
σ2
X = Var(X) = 1 −p
p2
.
(6.6.7)

Probability
327
• Example 6.6.2
A particle within an accelerator has the probability 0.01 of hitting a target material.
(a) What is the probability that the ﬁrst particle to hit the target is the 50th? (b) What is
the probability that the target will be hit by any particle?
P(ﬁrst particle to hit is the 50th) = 0.01(0.99)49 = 0.0061.
(6.6.8)
P(target hit by any of ﬁrst 50th particles) =
50
X
n=1
0.01(0.99)n−1 = 0.3950. (6.6.9)
⊓⊔
• Example 6.6.3
The police ticket 5% of parked cars. Assuming that the cars are ticketed independently,
ﬁnd the probability of 1 ticket on a block with 7 parked cars.
Each car is a Bernoulli trial with P(ticket) = 0.05. Therefore,
P(1 ticket on block) = P(1 ticket in 7 trials) =

7
1

(0.95)6(0.05) = 0.2573.
(6.6.10)
⊓⊔
Binomial distribution
Consider now an experiment in which n independent Bernoulli trials are performed and
X represents the number of successes that occur in the n trials. In this case the random
variable X is called binomial with parameters (n, p) with a probability mass function given
by
pX[xk] = P(X = k) =

n
k

pk(1 −p)n−k,
k = 0, 1, . . . , n,
(6.6.11)
where 0 ≤p ≤1, and

n
k

=
n!
k!(n −k)!,
(6.6.12)
the binomial coeﬃcient. The term pk arises from the k successes while (1 −p)n−k is due
to the failures. The binomial coeﬃcient gives the number of ways that we pick those k
successes from the n trials.
The corresponding cumulative density function of X is
FX(x) =
n
X
k=0

n
k

pk(1 −p)n−k,
n ≤x < n + 1.
(6.6.13)
The mean and variance of the binomial random variable X are
µX = E(X) = np,
and
σ2
X = Var(X) = np(1 −p).
(6.6.14)

328
Advanced Engineering Mathematics: A Second Course
A Bernoulli random variable is the same as a binomial random variable when the parameters
are (1, n).
• Example 6.6.4
Let us ﬁnd the probability of rolling the same side of a die (say, the side with N dots
on it) at least 3 times when a fair die is rolled 4 times.
During our 4 tosses, we could obtain no rolls with N dots on the side (k = 0), one roll
with N dots (k = 1), two rolls with N dots (k = 2), three rolls with N dots (k = 3), or
four rolls with N dots (k = 4). If we deﬁne A as the event of rolling a die so that the side
with N dots appears at least three times, then we must add the probabilities for k = 3 and
k = 4. Therefore,
P(A) = pX[x3] + pX[x4] =

4
3

p3(1 −p)1 +

4
4

p4(1 −p)0
(6.6.15)
= 4!
3!1!p3(1 −p)1 + 4!
4!0!p4(1 −p)0 = 0.0162
(6.6.16)
because p = 1
6.
⊓⊔
• Example 6.6.5
If 10 random binary digits are transmitted, what is the probability that more than
seven 1’s are included among them?
Let X denote the number of 1’s among the 10 digits. Then
P(X > 7) = P(X = 8) + P(X = 9) + P(X = 10) = pX[x8] + pX[x9] + pX[x10]
(6.6.17)
=

10
8
 1
2
8 1
2
2
+

10
9
 1
2
9 1
2
1
+

10
10
 1
2
10 1
2
0
(6.6.18)
= (45 + 10 + 1)
1
2
10
=
56
1024.
(6.6.19)
⊓⊔
Poisson distribution
The Poisson probability distribution arises as an approximation for the binomial dis-
tribution as n →∞and p →0 such that np remains ﬁnite. To see this, let us rewrite the
binomial distribution as follows:
P(X = k) =
n!
k!(n −k)!
λ
n
k 
1 −λ
n
n−k
= n(n −1)(n −2) · · · (n −k + 1)
nk
λn
n!
(1 −λ/n)n
(1 −λ/n)k ,
(6.6.20)
if λ = np. For ﬁnite λ,
lim
n→∞

1 −λ
n
k
→1,
lim
n→∞

1 −λ
n
n
→e−λ,
(6.6.21)
and
lim
n→∞
n(n −1)(n −2) · · · (n −k + 1)
nk
→1.
(6.6.22)

Probability
329
Therefore, for large n, small p and moderate λ, we can approximate the binomial distribution
by the Poisson distribution:
pX[xk] = P(X = k) = e−λ λk
k! ,
k = 0, 1, . . . .
(6.6.23)
The corresponding cumulative density function of X is
FX(x) = e−λ
n
X
k=0
λk
k! ,
n ≤x < n + 1.
(6.6.24)
The mean and variance of the Poisson random variable X are
µX = E(X) = λ,
and
σ2
X = Var(X) = λ.
(6.6.25)
In addition to this approximation, the Poisson distribution is the probability distribution
for a Poisson process. But that has to wait for the next chapter.
• Example 6.6.6
Consider a student union on a campus. On average 3 persons enter the union per
minute. What is the probability that, during any given minute, 3 or more persons will
enter the union?
To make use of Poisson’s distribution to solve this problem, we must have both a large
n and a small p with the average λ = np = 3. Therefore, we divide time into a large number
of small intervals so that n is large while the probability that someone will enter the union
is small. Assuming independence of events, we have a binomial distribution with large n.
Let A denote the event that 3 or more persons will enter the union, then
P(A) = pX[0] + pX[1] + pX[2] = e−3
30
0! + 31
1! + 32
2!

= 0.423.
(6.6.26)
Therefore, P(A) = 1 −P(A) = 0.577.
⊓⊔
Uniform distribution
The continuous random variable X is called uniform if its probability density function
is
pX(x) =

1/(b −a),
a < x < b,
0,
otherwise.
(6.6.27)
The corresponding cumulative density function of X is
FX(x) =
(
0,
x ≤a,
(x −a)/(b −a),
a < x < b,
1,
b ≤x.
(6.6.28)
The mean and variance of a uniform random variable X are
µX = E(X) = 1
2(a + b),
and
σ2
X = Var(X) = (b −a)2
12
.
(6.6.29)

330
Advanced Engineering Mathematics: A Second Course
Uniform distributions are used when we have no prior knowledge of the actual probability
density function and all continuous values in some range appear equally likely.
Exponential distribution
The continuous random variable X is called exponential with parameter λ > 0 if its
probability density function is
pX(x) =

λe−λx,
x > 0,
0,
x < 0.
(6.6.30)
The corresponding cumulative density function of X is
FX(x) =

1 −e−λx,
x ≥0,
0,
x < 0.
(6.6.31)
The mean and variance of an exponential random variable X are
µX = E(X) = 1/λ,
and
σ2
X = Var(X) = 1/λ2.
(6.6.32)
This distribution has the interesting property that is “memoryless.” By memoryless,
we mean that for a nonnegative random variable X, then
P(X > s + t|X > t) = P(X > s),
(6.6.33)
where x, t ≥0. For example, if the lifetime of a light bulb is exponentially distributed, then
the light bulb that has been in use for some hours is as good as a new light bulb with regard
to the amount of time remaining until it fails.
To prove this, from Equation 6.2.4, Equation 6.6.33 becomes
P(X > s + t and X > t)
P(X > t)
= P(X > s),
(6.6.34)
or
P(X > s + t and X > t) = P(X > t)P(X > s),
(6.6.35)
since P(X > s + t and X > t) = P(X > s + t). Now, because
P(X > s + t) = 1 −
h
1 −e−λ(s+t)i
= e−λ(s+t),
(6.6.36)
P(X > s) = 1 −
 1 −e−λs
= e−λs,
(6.6.37)
and
P(X > t) = 1 −
 1 −e−λt
= e−λt.
(6.6.38)
Therefore, Equation 6.6.35 is satisﬁed and X is memoryless.
• Example 6.6.7
A component in an electrical circuit has an exponentially distributed failure time with
a mean of 1000 hours. Calculate the time so that the probability of the time to failure is
less than 10−3.

Probability
331
Let the exponential random variable X = k have the units of hours. Then λ = 10−3.
From the deﬁnition of the cumulative density function,
FX(xt) = P(X ≤xt) = 0.001,
and
1 −exp(−λxt) = 0.001.
(6.6.39)
Solving for xt,
xt = −ln(0.999)/λ = 1.
(6.6.40)
⊓⊔
• Example 6.6.8
A computer contains a certain component whose time (in years) to failure is given by
the random variable T distributed exponentially with λ = 1/5. If 5 of these components
are installed in diﬀerent computers, what is the probability that at least 2 of them will still
work at the end of 8 years?
The probability that a component will last 8 years or longer is
P(T > 8) = e−8/5 = 0.2019,
(6.6.41)
because λ = 1/5.
Let X denote the number of components functioning after 8 years. Then,
P(X ≥2) = 1 −P(X = 0) −P(X = 1)
(6.6.42)
= 1 −

5
0

(0.2019)0(0.7981)5 −

5
1

(0.2019)1(0.7981)4
(6.6.43)
= 0.2666.
(6.6.44)
⊓⊔
Normal (or Gaussian) distribution
The normal distribution is the most important continuous distribution. It occurs in
many applications and plays a key role in the study of random phenomena in nature.
A random variable X is called a normal random variable if its probability density
function is
pX(x) = e−(x−µ)2/(2σ2)
√
2π σ
,
(6.6.45)
where the mean and variance of a normal random variable X are
µX = E(X) = µ,
and
σ2
X = Var(X) = σ2.
(6.6.46)
The distribution is symmetric with respect to x = µ and its shape is sometimes called
“bell shaped.” For small σ2 we obtain a high peak and steep slope while with increasing σ2
the curve becomes ﬂatter and ﬂatter.
The corresponding cumulative density function of X is
FX(x) =
1
√
2π σ
Z x
−∞
e−(ξ−µ)2/(2σ2) dξ =
1
√
2π
Z (x−µ)/σ
−∞
e−ξ2/2 dξ.
(6.6.47)

332
Advanced Engineering Mathematics: A Second Course
The integral in Equation 6.6.46 must be evaluated numerically. It is convenient to introduce
the probability integral:
Φ(z) =
1
√
2π
Z z
−∞
e−ξ2/2 dξ.
(6.6.48)
Note that Φ(−z) = 1 −Φ(z). Therefore,
FX(x) = Φ
x −µ
σ

.
(6.6.49)
and
P(a < X ≤b) = FX(b) −FX(a).
(6.6.50)
Consider now the intervals consisting of one σ, two σ, and three σ around the mean µ.
Then, from Equation 6.6.50,
P(µ −σ < X ≤µ + σ) = 0.68,
(6.6.51)
P(µ −2σ < X ≤µ + 2σ) = 0.955,
(6.6.52)
and
P(µ −3σ < X ≤µ + 3σ) = 0.997.
(6.6.53)
Therefore, approximately
2
3 of the values will be distributed between µ −σ and µ + σ,
approximately 95% of the values will be distributed between µ−2σ and µ+2σ, and almost
all values will be distributed between µ −3σ and µ + 3σ. For most uses, then, all values
will lie between µ −3σ and µ + 3σ, the so-called “three-sigma limits.”
As stated earlier, the mean and variance of a normal random variable X are
µX = E(X) = µ,
and
σ2
X = Var(X) = σ2.
(6.6.54)
The notation N(µ; σ) commonly denotes that X is normal with mean µ and variance
σ2. The special case of a normal random variable Z with zero mean and unit variance,
N(0, 1), is called a standard normal random variable.
Problems
1.
Four coins are tossed simultaneously.
Find the probability function for the random
variable X that gives the number of heads. Then compute the probabilities of (a) obtaining
no heads, (b) exactly one head, (c) at least one head, and (d) not less than four heads.
2. A binary source generates the digits 1 and 0 randomly with equal probability. (a) What
is the probability that three 1’s and three 0’s will occur in a six-digit sequence? (b) What
is the probability that at least three 1’s will occur in a six-digit sequence?
3. Show that the probability of exactly n heads in 2n tosses of a fair coin is
pX[xn] = 1 · 3 · 5 · · · 2n −1
2 · 4 · 6 · · · 2n
.
4. If your cell phone rings, on average, 3 times between noon and 3 P.M., what is the
probability that during that time period you will receive (a) no calls, (b) 6 or more calls, and
(c) not more than 2 calls? Assume that the probability is given by a Poisson distribution.

Probability
333
5. A company sells blank DVDs in packages of 10. If the probability of a defective DVD is
0.001, (a) what is the probability that a package contains a defective DVD? (b) what is the
probability that a package has two or more defective DVDs?
6. A school plans to oﬀer a course on probability in a classroom that contains 20 seats.
From experience they know that 95% of the students who enroll actually show up. If the
school allows 22 students to enroll before the session is closed, what is the probability of
the class being oversubscribed?
7. The lifetime (in hours) of a certain electronic device is a random variable T having a
probability density function pT (t) = 100H(t−100)/t2. What is the probability that exactly
3 of 5 such devices must be replaced within the ﬁrst 150 hours of operation? Assume that
the events that the ith device must be replaced within this time are independent.
6.7 JOINT DISTRIBUTIONS
In the previous sections we introduced distributions that depended upon a single ran-
dom variable. Here we generalize these techniques for two random variables. The range of
the two-dimensional random variable (X, Y ) is RXY = {(x, y); ξ ∈S and X(ξ) = x, Y (ξ) =
y}.
Discrete joint distribution
Let X and Y denote two discrete random variables deﬁned on the same sample space
(jointly distributed). The function pXY [xi, yj] = P[X = xi, Y = yj] is the joint probability
mass function of X and Y . As one might expect, pXY [xi, yj] ≥0.
Let the sets of possible values of X and Y be A and B. If xi ̸∈A or yj ̸∈B, then
pXY [xi, yj] = 0. Furthermore,
X
xi∈A,yj∈B
pXY [xi, yj] = 1.
(6.7.1)
The marginal probability functions of X and Y are deﬁned by
pX[xi] =
X
yj∈B
pXY [xi, yj],
(6.7.2)
and
pY [yj] =
X
xi∈A
pXY [xi, yj].
(6.7.3)
If X and Y are independent random variables, then pXY [xi, yj] = pX[xi]· pY [yj].
• Example 6.7.1
A joint probability mass function is given by
pXY [xi, yj] =

k(xi + 2yj),
xi = 1, 2, 3, yj = 1, 2;
0,
otherwise.
(6.7.4)

334
Advanced Engineering Mathematics: A Second Course
Let us ﬁnd the value of k, pX[xi], and pY [yj].
From Equation 6.7.1, we have that
k
3
X
xi=1
2
X
yj=1
(xi + 2yj) = 1,
(6.7.5)
or
k

(1 + 2) + (1 + 4) + (2 + 2) + (2 + 4) + (3 + 2) + (3 + 4)

= 1.
(6.7.6)
Therefore, k = 1/30.
Turning to pX[xi] and pY [yj],
pX[xi] = k
2
X
yj=1
(xi + 2yj) = k(xi + 2) + k(xi + 4) = k(2xi + 6) = (xi + 3)/15,
(6.7.7)
where xi = 1, 2, 3, and
pY [yj] = k
3
X
xi=1
(xi + 2yj) = k(1 + 2yj) + k(2 + 2yj) + k(3 + 2yj) = k(6 + 6yj) = (1 + yj)/5,
(6.7.8)
where yj = 1, 2.
⊓⊔
• Example 6.7.2
Consider an urn that contains 1 red ball, 2 blue balls, and 2 green balls. Let (X, Y )
be a bivariate random variable where X and Y denote the number of red and blue balls,
respectively, chosen from the urn. There are 18 possible ways that three balls can be drawn
from the urn: rbb, rbg, rgb, rgg, brb, brg, bbr, bbg, bgr, bgb, bgg, grb, grg, gbr, gbb, gbg, ggr,
and ggb.
The range of X and Y in the present problem is RXY ={(0, 1), (0, 2), (1, 0), (1, 1), (1, 2)}.
The joint probability mass function of (X, Y ) is given by pXY [xi, yj] = P(X = i, Y = j),
where xi = 0, 1 and yj = 0, 1, 2. From our list of possible drawings, we ﬁnd that pXY [0, 0] =
0, pXY [0, 1] = 1/6, pXY [0, 2] = 1/6, pXY [1, 0] = 1/6, pXY [1, 1] = 1/3, and pXY [1, 2] = 1/6.
Note that all of these probabilities sum to one.
Given these probabilities, the marginal probabilities are pX[0] = 1/3, pX[1] = 2/3,
pY [0] = 1/3, pY [1] = 1/2, and pY [2] = 1/3. Because pXY [0, 0] ̸= pX[0]pY [0], X and Y are
not independent variables.
⊓⊔
• Example 6.7.3
Consider a community where 50% of the families have a pet. Of these families, 60%
have one pet, 30% have 2 pets, and 10% have 3 pets. Furthermore, each pet is equally
likely (and independently) to be a male or female. If a family is chosen at random from
the community, then we want to compute the joint probability that his family has M male
pets and F female pets.
These probabilities are as follows:
P{F = 0, M = 0} = P{no pets} = 0.5,
(6.7.9)

Probability
335
P{F = 1, M = 0} = P{1 female and total of 1 pet}
(6.7.10)
= P{1 pet}P{1 female|1 pet}
(6.7.11)
= (0.5)(0.6) × 1
2 = 0.15,
(6.7.12)
P{F = 2, M = 0} = P{2 females and total of 2 pets}
(6.7.13)
= P{2 pets}P{2 females|2 pets}
(6.7.14)
= (0.5)(0.3) ×
1
2
2
= 0.0375,
(6.7.15)
and
P{F = 3, M = 0} = P{3 females and total of 3 pets}
(6.7.16)
= P{3 pets}P{3 females|3 pets}
(6.7.17)
= (0.5)(0.1) ×
1
2
3
= 0.00625.
(6.7.18)
The remaining probabilities can be obtained in a similar manner.
⊓⊔
Continuous joint distribution
Let us now turn to the case when we have two continuous random variables. In analog
with the deﬁnition given in Section 6.4, we deﬁne the two-dimensional probability density
pXY (x, y) by
P(x < X ≤x + dx, y < Y ≤y + dy) = pXY (x, y) dx dy.
(6.7.19)
Here, the comma in the probability parentheses means “and also.”
Repeating the same analysis as in Section 6.4, we ﬁnd that pXY (x, y) must be a single-
valued function with pXY (x, y) ≥0, and
Z ∞
−∞
Z ∞
−∞
pXY (x, y) dx dy = 1.
(6.7.20)
The joint distribution function of X and Y is
FXY (x, y) = P(X ≤x, Y ≤y) =
Z x
−∞
Z y
−∞
pXY (ξ, η) dξ dη.
(6.7.21)
Therefore,
P(a < X ≤b, c < Y ≤d) =
Z b
a
Z d
c
pXY (ξ, η) dξ dη.
(6.7.22)
The marginal probability density functions are deﬁned by
pX(x) =
Z ∞
−∞
pXY (x, y) dy,
and
pY (y) =
Z ∞
−∞
pXY (x, y) dx.
(6.7.23)

336
Advanced Engineering Mathematics: A Second Course
An important distinction exists upon whether the random variables are independent or not.
Two variables X and Y are independent if and only if
pXY (x, y) = pX(x)pY (y),
(6.7.24)
and conversely.
• Example 6.7.4
The joint probability density function of bivariate random variables (X, Y ) is
pXY (x, y) =

kxy,
0 < y < x < 1,
0,
otherwise,
(6.7.25)
where k is a constant. (a) Find the value of k. (b) Are X and Y independent?
The range RXY for this problem is a right triangle with its sides given by x = 1, y = 0,
and y = x. From Equation 6.7.20,
Z ∞
−∞
Z ∞
−∞
pXY (x, y) dx dy = k
Z 1
0
x
Z x
0
y dy

dx = k
Z 1
0
x y2
2

x
0
dx
(6.7.26)
= k
2
Z 1
0
x3 dx = k
8 x41
0 = k
8.
(6.7.27)
Therefore, k = 8.
To check for independence we must ﬁrst compute pX(x) and pY (y). From Equation
6.7.23 and holding x constant,
pX(x) = 8x
Z x
0
y dy = 4x3,
0 < x < 1;
(6.7.28)
pX(x) = 0 otherwise. From Equation 6.7.23 and holding y constant,
pY (y) = 8y
Z 1
y
x dx = 4y(1 −y2),
0 < y < 1.
(6.7.29)
Because pXY (x, y) ̸= pX(x)pY (y), X and Y are not independent.
⊓⊔
• Example 6.7.5: Buﬀon’s needle problem
A classic application of joint probability distributions is the solution of Buﬀon’s needle
problem:8 Consider an inﬁnite plane with an inﬁnite series of parallel lines spaced a unit
distance apart. A needle of length L < 1 is thrown upward and we want to compute the
probability that the stick will land so that it intersects one of these lines. See Figure 6.7.1.
There are two random variables that determine the needle’s orientation: X, the distance
from the lower end O of the needle to the nearest line above and Θ, the angle from the
vertical to the needle. Of course, we assume that the position where the needle lands is
random; otherwise, it would not be a probability problem.
8 First posed in 1733, its solution is given on pages 100–104 of Buﬀon, G., 1777: Essai d’arithm´etique
morale. Histoire naturelle, g´en´erale et particuli`ere, Suppl´ement, 4, 46–123.

Probability
337
L
cos
Θ
L 
(θ)
(b) no intersection
Θ
LL cos(θ)
O
X
X
O
(a) intersection
Figure 6.7.1: Schematic of Buﬀon’s needle problem showing the random variables X and Θ.
Let us deﬁne X ﬁrst. Its possible values lie between 0 and 1. Second, X is uniformly
distributed on (0, 1) with the probability density
pX(x) =
 1,
0 ≤x ≤1,
0,
otherwise.
(6.7.30)
Turning to Θ, its value lies between −π/2 to π/2 and is uniformly distributed between these
values. Therefore, the probability density is
pΘ(θ) =

1/π,
−π/2 < θ < π/2,
0,
otherwise.
(6.7.31)
The probability p that we seek is
p = P{needle intersects line} = P{X < L cos(Θ)}.
(6.7.32)
Because X and Θ are independent, their joint density equals the product of the densities
for X and Θ: pXΘ(x, θ) = pX(x)pΘ(θ).
The ﬁnal challenge is to use pXΘ(x, θ) to compute p. In Section 6.2 we gave a geometric
deﬁnition of probability. The area of the sample space is π because it consists of a rectangle
in (X, Θ) space with 0 < x < 1 and −π/2 < θ < π/2.
The values of X and Θ that
lead to the intersection with a parallel line is 0 < x < L cos(θ) where −π/2 < θ < π/2.
Consequently, from Equation 6.2.5,
p =
Z π/2
−π/2
Z L cos(θ)
0
pXΘ(x, θ) dx dθ =
Z π/2
−π/2
Z L cos(θ)
0
pX(x)pΘ(θ) dx dθ
(6.7.33)
= 1
π
Z π/2
−π/2
Z L cos(θ)
0
dx dθ = 1
π
Z π/2
−π/2
L cos(θ) dθ = 2L
π .
(6.7.34)
Consequently, given L, we can perform the tossing either physically or numerically, measure
p, and compute the value of π.
⊓⊔

338
Advanced Engineering Mathematics: A Second Course
Convolution
It is often important to calculate the distribution of X + Y from the distribution of
X and Y when X and Y are independent. We shall derive the relationship for continuous
random variables and then state the result for X and Y discrete.
Let X have a probability density function pX(x) and Y has the probability density
pY (y). Then the cumulative distribution function of X + Y is
GX+Y (a) = P(x + y ≤a) =
Z Z
x+y≤a
pX(x)pY (y) dx dy
(6.7.35)
=
Z ∞
−∞
Z a−y
−∞
pX(x)pY (y) dx dy =
Z ∞
−∞
Z a−y
−∞
pX(x) dx

pY (y) dy
(6.7.36)
=
Z ∞
−∞
FX(a −y)pY (y) dx.
(6.7.37)
Therefore,
pX+Y (a) = d
da
Z ∞
−∞
FX(a −y)pY (y) dy

=
Z ∞
−∞
pX(a −y)pY (y) dy.
(6.7.38)
In the case when X and Y are discrete,
pX+Y [ak] =
∞
X
i=−∞
pX[xi]pY [ak −xi].
(6.7.39)
Covariance
In Section 6.5 we introduced the concept of variance of a random variable X. There
we showed that this quantity measures the dispersion, or spread, of the distribution of X
about its expectation. What about the case of two jointly distributed random numbers?
Our ﬁrst attempt might be to look at Var(X) and Var(Y ). But this would simply
display the dispersions of X and Y independently rather than jointly.
Indeed, Var(X)
would give the spread along the x-direction while Var(Y ) would measure the dispersion
along the y-direction.
Consider now Var(aX +bY ), the joint spread of X and Y along the (ax+by)-direction
for two arbitrary real numbers a and b. Then
Var(aX + bY ) = E[(aX + bY ) −E(aX + bY )]2
(6.7.40)
= E[(aX + bY ) −E(aX) −E(bY )]2
(6.7.41)
= E{a[X −E(X)] + b[Y −E(Y )]}2
(6.7.42)
= E{a2[X −E(X)]2 + b2[Y −E(Y )]2 + 2ab[X −E(X)][Y −E(Y )]}
(6.7.43)
= a2Var(X) + b2Var(Y ) + 2abE{[X −E(X)][Y −E(Y )]}.
(6.7.44)

Probability
339
Thus, the joint spread or dispersion of X and Y in any arbitrary direction ax + by depends
upon three parameters: Var(X), Var(Y ), and E{[X −E(X)][Y −E(Y )]}. Because Var(X)
and Var(Y ) give the dispersion of X and Y separately, it is the quantity E{[X −E(X)][Y −
E(Y )]} that measures the joint spread of X and Y . This last quantity,
Cov(X, Y ) = E{[X −E(X)][Y −E(Y )]},
(6.7.45)
is called the covariance and is usually denoted by Cov(X, Y ) because it determines how X
and Y covary jointly. It only makes sense when we have two diﬀerent random variables
because in the case of a single random variable, Cov(X, X) = σ2
X = Var(X). Furthermore,
Cov(X, Y ) ≤σXσY . In summary,
Var(aX + bY ) = a2 Var(X) + b2 Var(Y ) + 2ab Cov(X, Y ).
(6.7.46)
An alternative method for computing the covariance occurs if we recall that µX = E(X)
and µY = E(Y ). Then
Cov(X, Y ) = E[(X −µX)(Y −µY )] = E(XY −µXY −µY X + µXµY )
(6.7.47)
= E(XY ) −µXE(Y ) −µY E(X) + µXµY
(6.7.48)
= E(XY ) −µXµY −µY µX + µXµY
(6.7.49)
= E(XY ) −µXµY = E(XY ) −E(X)E(Y ),
(6.7.50)
where
E(XY ) =







X
xi∈A,yj∈B
xiyj pXY [xi, yj],
X discrete,
Z ∞
−∞
Z ∞
−∞
xy pXY (x, y) dx dy,
X continuous.
(6.7.51)
Therefore,
Cov(X, Y ) = E(XY ) −E(X)E(Y ).
(6.7.52)
It is left as a homework assignment to show that
Cov(aX + b, cY + d) = ac Cov(X, Y ).
(6.7.53)
In general, Cov(X, Y ) can be positive, negative, or zero.
For it to be positive, X
and Y decrease together or increase together.
For a negative value, X would increase
while Y decreases, or vice versa. If Cov(X, Y ) > 0, X and Y are positively correlated. If
Cov(X, Y ) < 0, X and Y are negatively correlated. Finally, if Cov(X, Y ) = 0, X and Y are
uncorrelated.
• Example 6.7.6
The following table gives a discrete joint density function:

340
Advanced Engineering Mathematics: A Second Course
xi
pXY [xi, yj]
0
1
2
pY [yj]
0
3
28
9
28
3
28
15
28
yj
1
3
14
3
14
0
3
7
2
1
28
0
0
1
28
pX[xi]
5
14
15
28
3
28
Because
E(XY ) =
2
X
i=0
2
X
j=0
xi yj pXY [xi, yj] = 3
14,
(6.7.54)
µX = E(X) =
2
X
i=0
xi pX[xi] = 3
4,
and
µY = E(Y ) =
2
X
j=0
yj pY [yj] = 1
2,
(6.7.55)
then
Cov(X, Y ) = E(XY ) −E(X)E(Y ) = 3
14 −3
4 · 1
2 = −9
56.
(6.7.56)
Therefore, X and Y are negatively correlated.
⊓⊔
• Example 6.7.7
The random variables X and Y have the joint probability density function
pXY (x, y) =
 x + y,
0 < x < 1, 0 < y < 1,
0,
otherwise.
(6.7.57)
Let us compute the covariance.
First, we must compute pX(x) and pY (y). We ﬁnd that
pX(x) =
Z 1
0
pXY (x, y) dy =
Z 1
0
(x + y) dy = x + 1
2
(6.7.58)
for 0 < x < 1, and
pY (y) =
Z 1
0
pXY (x, y) dx =
Z 1
0
(x + y) dx = y + 1
2
(6.7.59)
for 0 < y < 1.
Because
E(XY ) =
Z 1
0
Z 1
0
xy(x + y) dx dy =
Z 1
0
 
y x3
3

1
0
+ y2 x2
2

1
0
!
dy
(6.7.60)
=
Z 1
0
y
3 + y2
2

dy = y2
6 + y2
6

1
0
= 1
3,
(6.7.61)

Probability
341
µX = E(X) =
Z 1
0
x pX(x) dx =
Z 1
0

x2 + x
2

dx = 7
12,
(6.7.62)
µY = E(Y ) =
Z 1
0
y pY (y) dy =
Z 1
0

y2 + y
2

dy = 7
12,
(6.7.63)
then
Cov(X, Y ) = E(XY ) −E(X)E(Y ) = 1
3 −49
144 = −1
144.
(6.7.64)
Therefore, X and Y are negatively correlated.
⊓⊔
Correlation
Although the covariance tells us how X and Y vary jointly, it depends upon the same
units in which X and Y are measured. It is often better if we free ourselves of this nuisance,
and we now introduce the concept of correlation.
Let X and Y be two random variables with 0 < σ2
X < ∞and 0 < σ2
Y < ∞. The
correlation coeﬃcient ρ(X, Y ) between X and Y is given by
ρ(X, Y ) = Cov
X −E(X)
σX
, Y −E(Y )
σY

= Cov(X, Y )
σXσY
.
(6.7.65)
It is noteworthy that |ρ(X, Y )| ≤1.
Random Vectors
It is often useful to express our two random variables X and Y as a two-dimensional
random vector V = (X Y )T . Then, the covariance can be written as a 2 × 2 covariance
matrix, given by

cov(X, X)
cov(X, Y )
cov(Y, X)
cov(Y, Y )

.
These considerations can be generalized into the n-dimensional random vector consisting of
n random variables that are all associated with the same events.
• Example 6.7.7
Using MATLAB, let us create two random variables by invoking X = randn(N,1) and Y
= randn(N,2), where N is the sample size. If N = 10, we would ﬁnd that using the MATLAB
command cov(X,Y) would yield
>> ans =
3.1325
0.9748
0.9748
1.4862
.
(If you do this experiment, you will also obtain a symmetric matrix but with diﬀerent
elements.) On the other hand, if N = 1000, we ﬁnd that cov(X,Y) equals
>> ans =

342
Advanced Engineering Mathematics: A Second Course
X
-4
-2
0
2
4
Y
-4
-2
0
2
4
Figure 6.7.2: Scatter plot of points (Xi, Yi) given by the random vector V in Example 6.7.7 when N =
1000.
0.9793
−0.0100
−0.0100
0.9927 .
The interpretation of the covariance matrix is as follows: The variance (or spread)
of data given by X and Y is (essentially) unity.
The correlation between X and Y is
(essentially) zero. These results are conﬁrmed in Figure 6.7.2 where we have plotted X and
Y as the data points (Xi, Yi) when N = 1000. We can see the symmetric distribution of
data points.
Problems
1. A search committee of 5 is selected from a science department that has 7 mathematics
professors, 8 physics professors, and 5 chemistry professors. If X and Y denote the number
of mathematics and physics professors, respectively, that are selected, calculate the joint
probability function.
2. In an experiment of rolling a fair die twice, let Z denote a random variable that equals
the sum of the results. What is pZ[zi]? Hint: Let X denote the result from the ﬁrst toss
and Y denote the result from the second toss. What you must ﬁnd is Z = X + Y .
3. Show that Cov(aX + b, cY + d) = ac Cov(X, Y ).
Project: Convolution
Consider two independent, uniformly distributed random variables (X, Y ) that are summed
to give Z = X + Y with
pX(x) =
 1,
0 < x < 1,
0,
otherwise,
and
pY (y) =
 1,
0 < y < 1,
0,
otherwise.

Probability
343
−0.5
0
0.5
1
1.5
2
2.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 z
Estimated  pZ(z)
Convolution Project
Show that
pZ(z) =
(
z,
0 < z ≤1,
2 −z,
1 < z ≤2,
0,
otherwise.
Then conﬁrm your results using MATLAB’s intrinsic function rand to generate {xi} and {yj}
and computing pZ(z). You may want to review Example 6.5.1 in my Advanced Engineering
Mathematics with MATLAB to see how to compute a convolution analytically.
Further Readings
Beckmann, P., 1967: Probability in Communication Engineering. Harcourt, Brace & World,
511 pp. A presentation of probability as it applies to problems in communication engineer-
ing.
Ghahramani, S., 2000: Fundamentals of Probability. Prentice Hall, 511 pp. Nice introduc-
tory text on probability with a wealth of examples.
Hsu, H., 1997: Probability, Random Variables, & Random Processes. McGraw-Hill, 306 pp.
Summary of results plus many worked problems.
Kay, S. M., 2006: Intuitive Probability and Random Processes Using MATLAB. Springer,
833 pp. A well-paced book designed for the electrical engineering crowd.
Ross, S. M., 2007: Introduction to Probability Models. Academic Press, 782 pp. An intro-
ductory undergraduate book in applied probability and stochastic processes.
Tuckwell, H. C., 1995: Elementary Applications of Probability Theory. Chapman & Hall,
292 pp. This book presents applications using probability theory, primarily from biology.


−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
1.5
2
2.5
x
y
Chapter 7
Random Processes
In the previous chapter we introduced the concept of a random variable X. There X
assumed various values of x according to a probability mass function pX[k] or probability
density function pX(x). In this chapter we generalize the random variable so that it is also
a function of time t. As before, the values of x assumed by the random variable X(t) at a
certain time is still unknown beforehand and unpredictable.
Our random, time-varying variable X(t; ξ) is often used to describe a stochastic or
random process. In that case, X(t) is the state of the process at time t. The process can
be either discrete or continuous in t.
A random process is not one function but a collection or family of functions, called
sample functions, with some probability assigned to each. When we perform an experiment,
we observe only one of these functions that is called a realization or sample path
of the
process. To observe more than a single function, we must repeat the experiment.
The state space of a random process is the set of all possible values that the random
variable X(t) can assume.
We can view random processes from many perspectives. First, it is a random function of
time. This perspective is useful when we wish to relate an evolutionary physical phenomenon
to its probabilistic model. Second, we can focus on its aspect as a random variable. This is
useful in developing mathematical methods and tools to analyze random processes.
Another method for characterizing a random process examines its behavior as t and ξ
vary or are kept constant. For example, if we allow t and ξ to vary, we obtain a family or
ensemble of X(t). If we allow t to vary while ξ is ﬁxed, then X(t) is simply a function of
time and gives a sample function or realization for this particular random process. On the
other hand, if we ﬁx t and allow ξ to vary, X(t) is a random variable equal to the state of
the random process at time t. Finally, if we ﬁx both t and ξ, then X(t) is a number.
345

346
Advanced Engineering Mathematics: A Second Course
0
20
40
60
80
100
−1
−0.5
0
0.5
1
 t
 X(t)/a
Figure 7.0.1: A realization of the random telegraph signal.
• Example 7.0.1
Consider a random process X(t) = A, where A is uniformly distributed in the interval
[0, 1]. A plot of sample functions of X(t) (a plot of X(t) as a function of t) consists of
horizontal straight lines that would cross the ordinate somewhere between 0 and 1.
⊓⊔
• Example 7.0.2
Consider the coin tossing experiment where the outcomes are either heads H or tails
T. We can introduce the random process deﬁned by
X(t; H) = sin(t),
and
X(t; T) = cos(t).
(7.0.1)
Note that the sample functions here are continuous functions of time.
⊓⊔
• Example 7.0.3: Random telegraph signal
Consider a signal that switches between −a and +a at random times. Suppose the
process starts (at time t = 0) in the −a state. It then remains in that state for a time
interval T1 at which point it switches to the state X(t) = a. The process remains in that
state until t = T2, then switches back to X(t) = −a. The switching time is given by a
Poisson process, a random process that we discuss in Section 7.6. Figure 7.0.1 illustrates
the random telegraph signal.
⊓⊔
Of all the possible random processes, a few are so useful in engineering and the physical
sciences that they warrant special names. Some of them are:
• Bernoulli process
Imagine an electronics ﬁrm that produces electronic devices that either work (a success
denoted by “S” ) or do not work (a failure or denoted “F”). We can model the production
line as a series of independent, repeated events where p denotes the probability of producing
a working device and q = 1 −p is the probability of producing a faulty device. Thus, the
production line can be modeled as a random process, called a Bernoulli process, which has
discrete states and parameter space.

Random Processes
347
0
5
10
15
20
25
30
−0.5
0
0.5
1
1.5
 x[k,s]
Realization 1
0
5
10
15
20
25
30
−0.5
0
0.5
1
1.5
 x[k,s]
Realization 2
0
5
10
15
20
25
30
−0.5
0
0.5
1
1.5
 k
 x[k,s]
Realization 3
Figure 7.0.2: Three realization or sample functions of a Bernoulli random process with p = 0.4. The
realization starts at k = 0 and continues forever.
The dashed box highlights the values of the random
variable X[21, s].
If we denote each discrete trial by the integer k, a Bernoulli process generates successive
outcomes at times k = 0, 1, 2, . . .. Mathematically we can express this discrete random
process by X[k, s] where k denotes the time and s denotes the number of the realization
or sample function.
Furthermore, this random process maps the original experimental
sample space {(F, F, S, . . .), (S, F, F, . . .), (F, F, F, . . .), . . .} to the numerical sample space
{(0, 0, 1, . . .), (1, 0, 0, . . .), (0, 0, 0, . . .), . . .}. Unlike the Bernoulli trial that we examined in
the previous chapter, each simple event now becomes an inﬁnite sequence of S’s and F’s.
Figure 7.0.2 illustrates three realizations or sample functions for a Bernoulli random
variable when p = 0.4. In each realizations s = 1, 2, . . ., the abscissa denotes time where
each successive trial occurs at times k = 0, 1, 2, . . .. When we ﬁx the value of k, the quantity
X[k, s] is a random variable with a probability mass function of a Bernoulli random variable.
• Markov process
Communication systems transmit either the digits 0 or 1. Each transmitted digit often
must pass through several stages. At each stage there is a chance that the digit that enters
one stage will be changed by the time when it leaves.
A Markov process is a stochastic process that describes the probability that the digit
will or will not be changed. It does this by computing the conditional distribution of any
future state Xn+1 by considering only the past states X0, X1, . . . , Xn−1 and the present
state Xn.
In Section 7.4 we examine the simplest possible discrete Markov process, a
Markov chain, when only the present and previous stages are involved. An example is the
probabilistic description of birth and death, which is given in Section 7.5.
• Poisson process
The prediction of the total number of “events” that occur by time t is important to
such diverse ﬁelds as telecommunications and banking. The most popular of these counting
processes is the Poisson process. It occurs when:

348
Advanced Engineering Mathematics: A Second Course
1. the events occur “rarely,”
2. the events occur in nonoverlapping intervals of time that are independent of each other,
3. the events occur at a constant rate λ.
In Section 7.6 we explore this random process.
• Wiener process
A Wiener process Wt is a random process that is continuous in time and possesses the
following three properties:
1. W0 = 0,
2. Wt is almost surely continuous, and
3. Wt has independent increments with a distribution Wt−Ws ∼N(0, t−s) for 0 ≤s ≤t.
As a result of these properties, we have that
1. the expectation is zero, E(Wt) = 0,
2. the variance is E(W 2
t ) −E2(Wt) = t, and
3. the covariance is cov(Ws, Wt) = min(s, t).
Norbert Wiener (1894–1964) developed this process to rigorously describe the physical
phenomena of Brownian motion—the apparent random motion of particles suspended in
a ﬂuid. In a Wiener process the distances traveled in Brownian motion are distributed
according to a Gaussian distribution and the path is continuous but consists entirely of
sharp corners.
Project: Gambler’s Ruin Problem
Pete and John decide to play a coin-tossing game. Pete agrees to pay John 10 cents
whenever the coin yields a “head” and John agrees to pay Pete 10 cents whenever it
is a “tail.”
Let Sn denote the amount that John earns in n tosses of a coin.
This
game is a stochastic process with discrete time (number of tosses).
The state space is
{0, ±10, ±20, · · ·} cents. A realization occurs each time that they play a new game.
Step 1: Create a MATLAB code to compute a realization of Sn. Plot several realizations
(sample functions) of this random process. See Figure 7.0.3.
Step 2: Suppose Pete has 10 dimes. Therefore, there is a chance he will run out of dimes at
some n = N. Modify your MATLAB code to construct a probability density function that
gives the probability Pete will run out of money at time n = N. See Figure 7.0.3.
This problem is often formulated in terms of a gambler versus casino and called the
gambler’s ruin problem: A gambler enters a casino with $n in cash and starts playing a
game where he wins with probability p and loses with probability 1 −q. The gambler plays
the game repeatedly, betting $1 in each round. He leaves the casino if his total fortune
reaches $N or he runs out of money.
The gambler’s ruin problem is also particularly popular because it a simple exam-
ple of a important stochastic process called a martingale.
In discrete time the martin-
gale requires that the sequence X1, X2, X3, . . . satisﬁes two conditions: E(|Xn|) < ∞and
E(Xn+1|X1, X2, . . . , Xn) = Xn for any time n, where E(·) denotes the expectation opera-
tor. If Xn is an observation, then we have a martingale if the conditional expected value
of the next observation, given all the past observations, equals the most recent observation.
To see that the gambler’s run problem is a martingale, we compute
E(Xn+1|xn) = 1
2(Xn + 1) + 1
2(Xn −1) = Xn,

Random Processes
349
0
200
400
600
800
1000
−50
0
50
100
number of tosses
gain/loss (in dimes)
0
200
400
600
800
1000
0
1
2
3
4x 10
−3
N
Estimated PDF
Figure 7.0.3: (a) Top frame: John’s gains or losses as the result of the three diﬀerent coin tossing games.
(b) The probability density function for John’s winning 10 dimes as a function of the number of tosses that
are necessary to win 10 dimes.
where we denote the gambler’s bankroll by Xn.
7.1 FUNDAMENTAL CONCEPTS
In Section 6.5 we introduced the concepts of mean (or expectation) and variance as
they apply to discrete and continuous random variables. These parameters provide useful
characterizations of a probability mass function or probability density function. Similar
considerations hold in the case of random processes and we introduce them here.
Mean and variance
We deﬁne the mean of the random process X(t) as the expected value of the process—
that is, the expected value of the random variable deﬁned by X(t) for a ﬁxed instant of
time. Note that when we take the expectation, we hold the time as a nonrandom parameter
and average only over the random quantities. We denote this mean of the random process
by µX(t), since, in general, it may depend on time. The deﬁnition of the mean is just the
expectation of X(t):
µX(t) = E[X(t)] =
Z ∞
−∞
x pX(t)(t; x) dx.
(7.1.1)
In a similar vein, we can generalize the concept of variance so that it applies to random
processes. Here variance also becomes a time-dependent function deﬁned by
σ2
X(t) = Var[X(t)] = E
n
[X(t) −µX(t)]2o
.
(7.1.2)

350
Advanced Engineering Mathematics: A Second Course
• Example 7.1.1: Random linear trajectories
Consider the random process deﬁned by
X(t) = A + Bt,
(7.1.3)
where A and B are uncorrelated random variables with means µA and µB. Let us ﬁnd the
mean of this random process.
From the linearity property of expectation, we have that
µX(t) = E[X(t)] = E(A + Bt) = E(A) + E(B)t = µA + µBt.
(7.1.4)
⊓⊔
• Example 7.1.2: Random sinusoidal signal
A random sinusoidal signal is one governed by X(t) = A cos(ω0t + Θ), where A and
Θ are independent random variables, A has a mean µA and variance σ2
A, and Θ has the
probability density function pΘ(x) that is nonzero only over the interval (0, 2π).
The mean of X(t) is given by
µX(t) = E[X(t)] = E[A cos(ω0t + Θ)] = E[A]E[cos(ω0t + Θ)].
(7.1.5)
We have used the property that the expectation of two independent random variables equals
the product of the expectation of each of the random variables. Simplifying Equation 7.1.5,
µX(t) = µA
Z 2π
0
cos(ω0t + x)pΘ(x) dx.
(7.1.6)
A common assumption is that pΘ(x) is uniformly distributed in the interval (0, 2π), namely
pΘ(x) = 1
2π ,
0 < x < 2π.
(7.1.7)
Substituting Equation 7.1.7 into Equation 7.1.6, we ﬁnd that
µX(t) = µA
2π
Z 2π
0
cos(ω0t + x) dx = 0.
(7.1.8)
⊓⊔
• Example 7.1.3: Wiener random process or Brownian motion
A Wiener (random) process is deﬁned by
X(t) =
Z t
0
U(ξ) dξ,
t ≥0,
(7.1.9)
where U(t) denotes white Gaussian noise. It is often used to model Brownian motion. To
ﬁnd its mean, we have that
E[X(t)] = E
Z t
0
U(ξ) dξ

=
Z t
0
E[U(ξ)] dξ = 0,
(7.1.10)

Random Processes
351
because the mean of white Gaussian noise equals zero.
⊓⊔
Autocorrelation function
When a random process is examined at two time instants t = t1 and t = t2, we obtain
two random variables X(t1) and X(t2). A useful relationship between these two random
variables is found by computing their correlation as a function of time instants t1 and t2.
Because it is a correlation between the values of the same process sampled at two diﬀerent
instants of time, we shall call it the autocorrelation function of the process X(t) and denote
it by RX(t1, t2). It is deﬁned in the usual way for expectations by
RX(t1, t2) = E[X(t1)X(t2)].
(7.1.11)
Just as in the two random variables case, we can deﬁne the covariance and correlation
coeﬃcient, but here the name is slightly diﬀerent. We deﬁne the autocovariance function
as
CX(t1, t2) = E{[X(t1) −µX(t1)][X(t2) −µX(t2)]}
(7.1.12)
= RX(t1, t2) −µX(t1)µX(t2).
(7.1.13)
Note that the variance of the process and its average power (the names used for the average
of [X(t)−µX(t)]2 and [X(t)]2, respectively) can be directly obtained for the autocorrelation
and the autocovariance functions, by simply using the same time instants for both t1 and
t2:
E{[X(t)]2} = RX(t, t),
(7.1.14)
and
σ2
X(t) = E{[X(t) −µX(t)]2} = CX(t, t) = RX(t, t) −µ2
X(t).
(7.1.15)
Therefore, the average power, Equation 7.1.14, and the variance, Equation 7.1.15, of the
process follows directly from the deﬁnition of the autocorrelation and autocovariance func-
tions.
• Example 7.1.4: Random linear trajectories
Let us continue Example 7.1.1 and ﬁnd the autocorrelation of a random linear trajectory
given by X(t) = A + Bt. From the deﬁnition of the autocorrelation,
RX(t1, t2) = E[X(t1)X(t2)] = E{[A + Bt1][A + Bt2]}
(7.1.16)
= E(A2) + E(AB)(t1 + t2) + E(B2)t1t2
(7.1.17)
= (σ2
A + µ2
A) + µAµB(t1 + t2) + (σ2
B + µ2
B)t1t2,
(7.1.18)
where σ2
A and σ2
B are the variances of the random variables A and B. We can easily ﬁnd
the autocovariance by
CX(t1, t2) = RX(t1, t2) −µX(t1)µX(t2) = σ2
A + σ2
Bt1t2.
(7.1.19)
⊓⊔

352
Advanced Engineering Mathematics: A Second Course
• Example 7.1.5: Random sinusoidal signal
We continue to examine the random sinusoidal signal given by X(t) = A cos(ω0t + Θ).
The autocorrelation function is
RX(t1, t2) = E[X(t1)X(t2)] = E[A cos(ω0t1 + Θ)A cos(ω0t2 + Θ)]
(7.1.20)
= 1
2E(A2)E[cos(ω0t2 −ω0t1) + cos(ω0t2 + ω0t1 + 2Θ)]
(7.1.21)
= 1
2(σ2
A + µ2
A)

cos[ω0(t2 −t1)] +
Z 2π
0
cos[ω0(t2 + t1) + 2x]pΘ(x) dx

. (7.1.22)
In our derivation we used (1) the property that the expectation of A2 equals the sum of
the variance and the square of the mean, and (2) the ﬁrst term involving the cosine is
not random because it is a function of only the time instants and the frequency. From
Equation 7.1.22 we see that autocorrelation function may depend on both time instants if
the probability density function of the phase angle is arbitrary. On the other hand, if pΘ(x)
is uniformly distributed, then the last term in Equation 7.1.22 vanishes because integrating
the cosine function over the interval of one period is zero. In this case we can write the
autocorrelation function as a function of only the time diﬀerence. The process also becomes
wide-sense stationary with
RX(τ) = E[X(t)X(t + τ)] = 1
2(σ2
A + µ2
A) cos(ω0τ).
(7.1.23)
⊓⊔
Wide-sense stationary processes
The mathematical analysis of a random or stochastic process would appear to be hope-
less because of the uncertainty of its time-dependent behavior at any instant of time. To
circumvent this diﬃculty we will examine only those processes that have certain statistical
properties at any instant. A wide-sense stationary process is one of the most popular.
A process is strictly stationary if its distribution and density functions do not depend
on the absolute values of the time instants t1 and t2, but only on the diﬀerence of the time
instants, |t1 −t2|. However, this is a very rigorous condition. If we are concerned only with
the mean and autocorrelation function, then we can soften our deﬁnition of a stationary
process to a limited form, and we call such processes wide-sense stationary processes. A
wide-sense stationary process has a constant mean, and its autocorrelation function depends
only on the time diﬀerence:
µX(t) = E[X(t)] = µX,
(7.1.24)
and
RX(t1, t2) = E[X(t1)X(t2)] = RX(t2 −t1).
(7.1.25)
Because time does not appear in the mean, we simply write it as a constant mean value µX.
Similarly, because the autocorrelation function is a function only of the time diﬀerence, we
can write it as a function of a single variable, the time diﬀerence τ:
RX(τ) = E[X(t)X(t + τ)].
(7.1.26)

Random Processes
353
We can obtain similar expressions for the autocovariance function, which in this case de-
pends only on the time diﬀerence as well:
CX(τ) = E{[X(t) −µX][X(t + τ) −µX]} = RX(τ) −µ2
X.
(7.1.27)
Finally, the average power and variance for a wide-sense stationary process are
E{[X(t)]2} = RX(0),
and
σ2
X = CX(0) = RX(0) −µ2
X,
(7.1.28)
respectively. Therefore, a wide-sense stationary process has a constant average power and
constant variance.
Problems
1. Find µX(t) and σ2
X(t) for the random process given by X(t) = A cos(ωt), where ω is
a constant and A is a random variable with the Gaussian (or normal) probability density
function
pX(x) =
1
√
2π e−x2/2.
2. Consider a sine-wave random process X(t) = A cos(ωt + Θ), −π < t < π, where A and
ω are constants with A > 0. The phase function Θ is a random, uniform variable on the
interval [−π, π]. Find the mean, variance and autocorrelation for this random function. Is
this process wide-sense stationary?
3.
Consider a countably inﬁnite sequence {Xn, n = 0, 1, 2, 3, . . .} of a random variable
deﬁned by
Xn =

1,
for success in the nth trial,
0,
for failure in the nth trial,
with the probabilities P(Xn = 0) = 1 −p and P(Xn = 1) = p. Thus, Xn is a Bernoulli
process. For this process, E(Xn) = p and Var(Xn) = p(1−p). Show that the autocorrelation
is
RX(t1, t2) =
 p,
t1 = t2,
p2,
t1 ̸= t2;
and the autocovariance is
CX(t1, t2) =

p(1 −p),
t1 = t2,
0,
t1 ̸= t2.
Project: Computing the Autocorrelation Function
In most instances you must compute the autocorrelation function numerically. The
purpose of this project is to explore this computation using the random telegraph signal.
The exact solution is given by Equation 7.2.24. You will compute the autocorrelation two
ways:
Step 1: Using Example 7.6.1, create MATLAB code that generates 500 realizations of the
random telegraph signal.

354
Advanced Engineering Mathematics: A Second Course
0
0.2
0.4
0.6
0.8
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
τ
 RX
0
0.2
0.4
0.6
0.8
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
τ
(a)
(b)
Figure 7.1.1: The autocorrelation function RX(τ) for the random telegraph signal as a function of τ when
λ = 2. The dashed line gives the exact solution. In frame (a) Xk(tS)Xk(tS + τ) has been averaged over
500 realizations when tS = 2. In frame (b) X200(m∆t)X200(m∆t + τ) has been averaged with M = 1200
and ∆t = 0.01.
Step 2: Choosing an arbitrary time tS, compute Xk(tS)Xk(tS + τ) for 0 ≤0 ≤τmax and
k = 1, 2, 3, . . . , 500. Then ﬁnd the average value of Xk(tS)Xk(tS + τ). Plot RX(τ) as a
function of τ and include the exact answer for comparison. Does it matter how many sample
functions you use?
Step 3: Now introduce a number of times tm = m∆t, where m = 0, 1, 2, . . . , M. Using only
a single realization k = K of your choice, compute XK(m∆t) × XK(m∆t + τ). Then ﬁnd
the average value of XK(m∆t)XK(m∆t + τ) and plot this result as a function of τ. On the
same plot, include the exact solution. Does the value of ∆t matter? See Figure 7.1.1
7.2 POWER SPECTRUM
In earlier chapters we provided two alternative descriptions of signals, either in the
time domain, which provides information on the shape of the waveform, or in the frequency
domain, which provides information on the frequency content. Because random signals do
not behave in any predictable fashion nor are they represented by a single function, it is
unlikely that we can deﬁne the spectrum of a random signal by taking its Fourier transform.
On the other hand, the autocorrelation of random signals describes in some sense whether
the signal changes rapidly or slowly. In this section we explain and illustrate the concept
of power spectrum of random signals.
For a wide-sense stationary random signal X(t) with autocorrelation function RX(τ),
the power spectrum SX(ω) of the random signal is the Fourier transform of the autocorre-
lation function:
SX(ω) =
Z ∞
−∞
RX(τ)e−iωτ dτ.
(7.2.1)
Consequently, the autocorrelation can be obtained from inverse Fourier transform of the
power spectrum, or
RX(τ) = 1
2π
Z ∞
−∞
SX(ω)eiωτ dω.
(7.2.2)

Random Processes
355
As with any Fourier transform, it enjoys certain properties. They are:
1. The power spectrum is real and even: SX(−ω) = SX(ω) and S∗
X(ω) = SX(ω), where
S∗
X(ω) denotes the complex conjugate value of SX(ω).
2. The power spectrum is nonnegative: SX(ω) ≥0.
3. The average power of the random signal is equal to the integral of the power spectrum:
E{[X(t)]2} = RX(0) = 1
π
Z ∞
0
SX(ω) dω.
(7.2.3)
4. If the random signal has nonzero mean µX, then its power spectrum contains an impulse
at zero frequency of magnitude 2πµ2
X.
5. The Fourier transform of the autocovariance function of the random process is itself
also a power spectrum and usually does not contain an impulse component in zero
frequency.
Consider the following examples of the power spectrum:
• Example 7.2.1: Random sinusoidal signal
The sinusoidal signal is deﬁned by
X(t) = A cos(ω0t + Θ),
(7.2.4)
where the phase is uniformly distributed in the interval [0, 2π]. If the amplitude A has a
mean of zero and a variance of σ2, then the autocorrelation function is
RX(τ) = 1
2σ2 cos(ω0τ) = RX(0) cos(ω0τ).
(7.2.5)
The power spectrum of this signal is then
SX(ω) =
Z ∞
−∞
RX(0) cos(ω0τ)e−iωτ dτ = RX(0)π [δ(ω −ω0) + δ(ω + ω0)] .
(7.2.6)
Because this signal contains only one frequency ω0, its power spectrum is just two impulses,
one at ω0 and one at −ω0. Since the negative frequency appears due only to the even
property of the power spectrum, it is clear that all power is concentrated at the frequency
of the sinusoidal signal. While this is a very simple example, it does illustrate that the
power spectrum indeed represents the way the power in the random signal is distributed
among the various frequencies.
We shall see later that if we also use linear systems in
order to amplify or attenuate certain frequencies, the results mirror what we expect in the
deterministic case.
⊓⊔
• Example 7.2.2: Modulated signal
Let us now examine a sinusoidal signal modulated by another random signal that
contains low frequencies. This random process is described by
Y (t) = X(t) cos(ω0t + Θ),
(7.2.7)

356
Advanced Engineering Mathematics: A Second Course
where the phase angle in Equation 7.2.7 is a random variable that is uniformly distributed
in the interval [0, 2π] and is independent of X(t). Then the autocorrelation function of Y (t)
is given by
RY (τ) = E[Y (t)Y (t + τ)] = E{X(t) cos(ω0t + Θ)X(t + τ) cos[ω0(t + τ) + Θ]}
(7.2.8)
= E[X(t)X(t + τ)]E{cos(ω0t + Θ) cos[ω0(t + τ) + Θ]} = 1
2RX(τ) cos(ω0t).
(7.2.9)
Let us take RX(τ) = RX(0)e−2λ|τ|, the autocorrelation function for a random telegraph
signal (see Equation 7.2.22). In this case,
RY (τ) = 1
2RX(0)e−2λ|τ| cos(ω0t).
(7.2.10)
Turning to the power spectrum, the deﬁnition gives
SY (ω) =
Z ∞
−∞
1
2RX(τ) cos(ω0t)e−iωτ dτ
(7.2.11)
= 1
4
Z ∞
−∞
RX(τ)
 eiω0τ + e−iω0τ
e−iωτ dτ
(7.2.12)
= 1
4 [SX(ω −ω0) + SX(ω + ω0)] .
(7.2.13)
Thus, the resulting power spectrum is shifted to the modulating frequency ω0 and its
negative value, with peak values located at both ω = ω0 and ω = −ω0.
⊓⊔
• Example 7.2.3: White noise
There are instances when we want to approximate random signals where the autocor-
relation function is very narrow and very large about τ = 0. In those cases we construct an
idealization of the autocorrelation function by using the impulse or delta function δ(τ).
In the present case when RX(τ) = C δ(τ), the power spectrum is
SX(ω) =
Z ∞
−∞
C δ(τ)e−iωτ dτ = C.
(7.2.14)
Thus, the power spectrum here is a ﬂat spectrum whose value is equal to C. Because the
power spectrum is ﬂat for all frequencies, it is often called “white noise” since it contains
all frequencies with equal weight.
An alternative derivation involves the random telegraph that we introduced in Example
7.0.3. As the switching rate becomes large and the rate λ approaches inﬁnity, its amplitude
increases as
√
λ.
Because RX(0) increases linearly with λ, the autocorrelation function
becomes
RX(τ) = Cλ exp(−2λ|τ|).
(7.2.15)
The resulting power spectrum equals
SX(ω) = lim
λ→∞
4Cλ2
ω2 + 4λ2 = lim
λ→∞
C
1 + [ω/(2λ)]2 = C.
(7.2.16)
The power spectrum is again ﬂat for all frequencies.
The autocorrelation for white noise is an idealization because it has inﬁnite average
power. Obviously no real signal has inﬁnite power since in practice the power spectrum

Random Processes
357
decays eventually.
Nevertheless, white noise is still quite useful because the decay usu-
ally occurs at such high frequencies that we can tolerate the errors of introducing a ﬂat
spectrum.
⊓⊔
• Example 7.2.4: Random telegraph signal
In Example 7.0.3 we introduced the random telegraph signal: X(t) equals either +h or
−h, changing its value from one to the other in Poisson-distributed moments of time. The
probability of n changes in a time interval τ is
Pτ(n) = (λτ)n
n!
e−λτ,
(7.2.17)
where λ denotes the average frequency of changes.
To compute the power spectrum, we must ﬁrst compute the correlation function via
the product X(t)X(t + τ). This product equals h2 or −h2, depending on whether X(t) =
X(t + τ) or X(t) = −X(t + τ), respectively.
These latter relationships depend on the
number of changes during the time interval. Now,
P[X(t) = X(t + τ)] = Pτ(n even) = e−λτ
∞
X
n=1
(λτ)2n
(2n)! = e−λτ cosh(λτ),
(7.2.18)
and
P[X(t) = −X(t + τ)] = Pτ(n odd) = e−λτ
∞
X
n=1
(λτ)2n+1
(2n + 1)! = e−λτ sinh(λτ).
(7.2.19)
Therefore,
E[X(t)X(t + τ)] = h2Pτ(n even) −h2Pτ(n odd)
(7.2.20)
= h2e−λτ[cosh(λτ) −sinh(λτ)]
(7.2.21)
= h2e−2λ|τ|.
(7.2.22)
We have introduced the absolute value sign in Equation 7.2.24 because our derivation was
based on t2 > t1 and the absolute value sign takes care of the case t2 < t1.
Using Problem 1, we have that
SX(ω) = 2h2
Z ∞
0
e−2λτ cos(λτ) dτ =
4h2λ
ω2 + 4λ2 .
(7.2.23)
Problems
1. Show that
SX(ω) = 2
Z ∞
0
RX(τ) cos(ωτ) dτ.
7.3 TWO-STATE MARKOV CHAINS
A Markov chain is a probabilistic model in which the outcomes of successive trials
depend only on its immediate predecessors. The mathematical description of a Markov

358
Advanced Engineering Mathematics: A Second Course
chain involves the concepts of states and state transition. If Xn = i, then we have a process
with state i and time n. Given a process in state i, there is a ﬁxed probability Pij that state
i will transition into state j. In this section we focus on the situation of just two states.
Imagine that you want to predict the chance of rainfall tomorrow.1 From close obser-
vation you note that the chance of rain tomorrow depends only on whether it is raining
today and not on past weather conditions. From your observations you ﬁnd that if it rains
today, then it will rain tomorrow with probability α, and if it does not rain today, then
the chance it will rain tomorrow is β. Assuming that these probabilities of changes are
stationary (unchanging), you would like to answer the following questions:
1. Given that it is raining (or not raining), what are the chances of it raining in eight
days?
2. Suppose the day is rainy (or dry). How long will the current weather remain before it
changes for the ﬁrst time?
3. Suppose it begins to rain during the week. How long does it take before it stops?
If the weather observation takes place at noon, we have a discrete parameter process;
the two possible states of the process are rain and no rain. Let these be denoted by 0 for no
rain and 1 for rain. The four possible transitions are (0 →0), (0 →1), (1 →0), and (1 →1).
Let Xn be the state of the process at the nth time point. We have Xn = 0, 1. Clearly,
{Xn, n = 0, 1, 2, . . .} is a two-state Markov chain. Therefore, questions about precipitation
can be answered if all the properties of the two-state Markov chains are known. Let
P (m,n)
i,j
= P(Xn = j|Xm = i),
i, j = 0, 1;
m ≤n.
(7.3.1)
P (m,n)
i,j
denotes the probability that the state of the process at the nth time point is j given
that it was at state i at the mth time point. Furthermore, if this probability is larger for
i = j than when i ̸= j, the system prefers to stay or persist in whatever state it is. When
n = m + 1, we have that
P (m,m+1)
i,j
= P(Xm+1 = j|Xm = i).
(7.3.2)
This is known as the one-step transition probability, given that the process is at i at time
m.
There are two possibilities: either P (m,m+1)
ij
depends on m or P (m,m+1)
ij
is independent
of m, where m is the initial value of the time parameter. Our precipitation model is an
example of a second type of process in which the one-step transition probabilities do not
change with time.
Such processes are known as time homogeneous.
Presently we shall
restrict ourselves only to these processes. Consequently, without loss of generality we can
use the following notation for the probabilities:
Pij = P(Xm+1 = j|Xm = i)
for all m,
(7.3.3)
and
P (n)
ij
= P(Xm+n = j|Xm = i)
for all m.
(7.3.4)
1 See, for example, Gabriel, K. R., and J. Neumann, 1962: A Markov chain model for daily rainfall
occurrence at Tel Aviv. Quart. J. R. Met. Soc., 88, 90–95.

Random Processes
359
Chapman-Kolmogorov equation
The Chapman2-Kolmogorov3 equations provide a mechanism for computing the tran-
sition probabilities after n steps. The n-step transition probabilities P (n)
ij
denote the prob-
ability that a process in state i will be in state j after n transitions, or
P (n)
ij
= P[Xn+k = j|Xk = i],
n ≥0,
i, j ≥0.
(7.3.5)
Therefore, P (1)
ij
= Pij. The Chapman-Kolmogorov equations give a method for computing
these n-step transition probabilities via
P (n+m)
ij
=
∞
X
k=0
P (n)
ik P (m)
kj ,
n, m ≥0,
(7.3.6)
for all i and j. Here P (n)
ik P (m)
kj
represents the probability that the ith starting process will
go to state j in n + m transitions via a path that takes it into state k at the nth transition.
Equation 7.3.6 follows from
P (n+m)
ij
= P[Xn+m = j|X0 = i] =
∞
X
k=0
P[Xn+m = j, Xn = k|X0 = i]
(7.3.7)
=
∞
X
k=0
P[Xn+m = j|Xn = k, X0 = i]P[Xn = k|X0 = i] =
∞
X
k=0
P (n)
ik P (m)
kj .
(7.3.8)
Transmission probability matrix
Returning to the task at hand, we have that
P (2) = P (1+1) = P · P = P 2,
(7.3.9)
and by induction
P (n) = P (n−1+1) = P (n−1) · P = P n,
(7.3.10)
where P (n) denotes the transition matrix after n steps.
From our derivation, we see the following: (1) The one-step transition probability
matrix completely deﬁnes the time-homogeneous two-state Markov chain. (2) All transition
probability matrices show the important property that the elements in any of their rows add
up to one. This follows from the fact that the elements of a row represent the probabilities
of mutually exclusive and exhaustive events on a sample space.
2 Chapman, S., 1928: On the Brownian displacements and thermal diﬀusion of grains suspended via
non-uniform ﬂuid. Proc. R. Soc. London, Ser. A, 119, 34–54.
3 Kolmogorov, A. N., 1931: ¨Uber die analytischen Methoden in der Wahrscheinlichkeitsrechnung. Math.
Ann., 104, 415–458.

360
Advanced Engineering Mathematics: A Second Course
Table 7.3.1: The Probability of Rain on the nth Day.
n
P00
P10
P01
P11
1
0.7000
0.2000
0.3000
0.8000
2
0.5500
0.3000
0.4500
0.7000
3
0.4750
0.3500
0.5250
0.6500
4
0.4375
0.3750
0.5625
0.6250
5
0.4187
0.3875
0.5813
0.6125
6
0.4094
0.3938
0.5906
0.6063
7
0.4047
0.3969
0.5953
0.6031
8
0.4023
0.3984
0.5977
0.6016
9
0.4012
0.3992
0.5988
0.6008
10
0.4006
0.3996
0.5994
0.6004
∞
0.4000
0.4000
0.6000
0.6000
For two-state Markov processes, this means that
P (n)
00 + P (n)
01 = 1,
and
P (n)
10 + P (n)
11 = 1.
(7.3.11)
Furthermore, with the one-step transmission probability matrix:
P =

1 −a
a
b
1 −b

,
0 ≤a, b ≤1,
|1 −a −b| < 1,
(7.3.12)
then the n-step transmission probability matrix is

P (n)
00
P (n)
01
P (n)
10
P (n)
11

=
 
b
a+b + a (1−a−b)n
a+b
a
a+b −a (1−a−b)n
a+b
b
a+b −b (1−a−b)n
a+b
a
a+b + b (1−a−b)n
a+b
!
.
(7.3.13)
This follows from the Chapman-Kolmogorov equation that
P (1)
00 = 1 −a,
(7.3.14)
and
P (n)
00 = (1 −a)P (n−1)
00
+ bP (n−1)
01
,
n > 1,
(7.3.15)
= b + (1 −a −b)P (n−1)
00
,
(7.3.16)
since P (n)
01 = 1−P (n)
00 . Solving these equations recursively for n = 1, 2, 3, . . . and simplifying,
we obtain Equation 7.3.13 as long as both a and b do not equal zero.
• Example 7.3.1
Consider a precipitation model where the chance for rain depends only on whether it
rained yesterday. If we denote the occurrence of rain by state 0 and state 1 denotes no rain,
then observations might give you a transition probability that looks like:
P =

0.7
0.3
0.2
0.8

.
(7.3.17)
Given that the atmosphere starts today with any one of these states, the probability of
ﬁnding that it is raining on the nth day is given by P n. Table 7.3.1 illustrates the results
as a function of n. Thus, regardless of whether it rains today or not, in ten days the chance
for rain is 0.4 while the chance for no rain is 0.6.
⊓⊔

Random Processes
361
Limiting behavior
As Table 7.3.1 suggests, as our Markov chain evolves, it reaches some steady state. Let
us explore this limit of n →∞because it often provides a simple and insightful represen-
tation of a Markov process.
For large values of n it is possible to show that the limiting probability distribution of
states is independent of the initial value. In particular, for |1 −a −b| < 1, we have that
lim
n→∞P (n) =

b
a+b
a
a+b
b
a+b
a
a+b

.
(7.3.18)
This follows from limn→∞(1 −a −b)n →0 since |1 −a −b| < 1. From Equation 7.3.13 the
second term in each of the elements of the matrix tends to zero as n →∞.
Let us denote these limiting probabilities by πj = limn→∞P (n)
ij . Then, from Equation
7.3.18,
π00 = π10 =
b
a + b = π0,
and
π01 = π11 =
a
a + b = π1,
(7.3.19)
and these limiting distributions are independent of the initial state.
Number of visits to a certain state
When a random process visits several states, we would like to know the number of
visits to a certain state. Let N (n)
ij
denote the number of visits the two-state Markov chain
{Xn} makes to state j, starting initially at state i, in n time periods. If µ(n)
ij
denotes the
expected number of visits that the process makes to state j in n steps after it originally
started at state i, and the transition probability matrix P of the two-state Markov chain is
P =

1 −a
a
b
1 −b

(7.3.20)
with |1 −a −b| < 1, then
||µ(n)
ij || =
 
nb
a+b + a(1−a−b)[1−(1−a−b)n]
(a+b)2
na
a+b −a(1−a−b)[1−(1−a−b)n]
(a+b)2
nb
a+b −b(1−a−b)[1−(1−a−b)n]
(a+b)2
na
a+b + b(1−a−b)[1−(1−a−b)n]
(a+b)2
!
(7.3.21)
To prove Equation 7.3.21, we introduce a random variable Y (k)
ij , where
Y (n)
ij
=

1,
if Xk = j and X0 = i,
0,
otherwise,
(7.3.22)
for i, j = 0, 1. This random variable Y (n)
ij
gives the time at which the process visits state j.
The probability distribution of Y (n)
ij
for ﬁxed k is
Y (n)
ij
0
1

362
Advanced Engineering Mathematics: A Second Course
Probability
1 −P (n)
ij
P (n)
ij
Thus, we have that
E[Y (k)
ij ] = P (k)
ij ,
i, j = 0, 1;
k = 1, 2, . . . , n.
(7.3.23)
Because Y (k)
ij
equals 1 whenever the process is in state j and 0 when it is not in j, the
number of visits to j, starting originally from i, in n steps is
N (n)
ij
= Y (1)
ij
+ Y (2)
ij
+ · · · + Y (n)
ij .
(7.3.24)
Taking the expected values and using the property that the expectation of a sum is the sum
of expectations,
µ(n)
ij
= E
h
N (n)
ij
i
= P (1)
ij
+ P (2)
ij
+ · · · + P (n)
ij
=
n
X
k=1
P (k)
ij .
(7.3.25)
From Equation 7.3.13, we substitute for each P (k)
ij
and ﬁnd
µ(n)
00 =
n
X
k=1
P (k)
00 =
n
X
k=1

b
a + b + a(1 −a −b)k
a + b

,
(7.3.26)
µ(n)
01 =
n
X
k=1
P (k)
01 =
n
X
k=1

a
a + b −a(1 −a −b)k
a + b

,
(7.3.27)
µ(n)
10 =
n
X
k=1
P (k)
10 =
n
X
k=1

b
a + b −b(1 −a −b)k
a + b

,
(7.3.28)
and
µ(n)
11 =
n
X
k=1
P (k)
11 =
n
X
k=1

a
a + b + b(1 −a −b)k
a + b

,
(7.3.29)
ﬁnally, noting that
n
X
k=1
b
a + b =
nb
a + b,
(7.3.30)
and
n
X
k=1
a(1 −a −b)k
a + b
=
a
a + b
n
X
k=1
(1 −a −b)k
(7.3.31)
=
a
a + b

(1 −a −b) + (1 −a −b)2 + · · · + (1 −a −b)n
(7.3.32)
= a(1 −a −b)
a + b

1 + (1 −a −b) + · · · + (1 −a −b)n−1
(7.3.33)
= a(1 −a −b) [1 −(1 −a −b)n]
(a + b) [1 −(1 −a −b)]
.
(7.3.34)
Here we used the property of a geometric series that
n−1
X
k=0
xk = 1 −xn
1 −x ,
|x| < 1.
(7.3.35)

Random Processes
363
• Example 7.3.2
Let us continue with our precipitation model that we introduced in Example 7.3.1. If
we wish to know the expected number of days within a week that the atmosphere will be
in a given state, we have from Equation 7.3.21 that
µ(7)
00 =
7b
a + b + a(1 −a −b)[1 −(1 −a −b)7]
(a + b)2
= 3.3953,
(7.3.36)
µ(7)
10 =
7b
a + b −b(1 −a −b)[1 −(1 −a −b)7]
(a + b)2
= 2.4031,
(7.3.37)
µ(7)
01 =
7a
a + b −a(1 −a −b)[1 −(1 −a −b)7]
(a + b)2
= 3.6047,
(7.3.38)
and
µ(7)
11 =
7a
a + b + b(1 −a −b)[1 −(1 −a −b)7]
(a + b)2
= 4.5969,
(7.3.39)
since a = 0.3 and b = 0.2.
⊓⊔
Duration of stay
In addition to computing the number of visits to a certain state, it would also be useful
to know the fraction of the discrete time that a process stays in state j out of n when the
process started in state i. These fractions are:
lim
n→∞
µ(n)
00
n
= lim
n→∞
µ(n)
10
n
= π0,
(7.3.40)
and
lim
n→∞
µ(n)
01
n
= lim
n→∞
µ(n)
11
n
= π1.
(7.3.41)
Thus, the limiting probabilities also give the fraction of time that the process spends in the
two states in the long run.
If the process is in state i (i = 0, 1) at some time, let us compute the number of
additional time periods it stays in state i until it moves out of that state. We now want to
show that this probability distribution αi, i = 0, 1, is
P(α0 = n) = a(1 −a)n,
(7.3.42)
and
P(α1 = n) = b(1 −b)n,
(7.3.43)
where n = 1, 2, 3, . . .. Furthermore,
E(α0) = (1 −a)/a,
E(α1) = (1 −b)/b,
(7.3.44)
and
Var(α0) = (1 −a)/a2,
Var(α1) = (1 −b)/b2,
(7.3.45)

364
Advanced Engineering Mathematics: A Second Course
where the transition probability matrix P of the Markov chain {Xn} equals
P =

1 −a
a
b
1 −b

(7.3.46)
with |1 −a −b| < 1. Clearly a or b cannot equal zero.
To prove this we note that at every step, the process has two choices: either to stay in
the same state or to move to the other state. Suppose the process is in state 0 at some time.
The probability of a sequence of outcomes of the type {0 0 · · · 0
| {z }
n
1} is required. Because of the
property of Markov-dependence, we therefore have the realization of a Bernoulli process with
n consecutive outcomes of one type followed by an outcome of the other type. Therefore,
the probability distribution of α0 is geometric with (1 −a) as the probability of “failure,”
and the distribution of α1 is geometric with (1−b) as the probability of failure. Thus, from
Equation 6.6.5, we have that
P(α0 = n) = a(1 −a)n,
(7.3.47)
and
P(α1 = n) = b(1 −b)n,
(7.3.48)
where n = 0, 1, 2, . . .. The expressions for the mathematical expectation and variance of α0
and α1 easily follow from the corresponding expressions for the geometric distribution.
• Example 7.3.3
Let us illustrate our expectation and variance expressions for our precipitation model.
From Equation 7.3.44 and Equation 7.3.45, we have that
E(α0) = (1 −a)/a = 2.3333,
E(α1) = (1 −b)/b = 4,
(7.3.49)
and
Var(α0) = (1 −a)/a2 = 7.7777,
Var(α1) = (1 −b)/b2 = 20,
(7.3.50)
since a = 0.3 and b = 0.2.
⊓⊔
• Example 7.3.4: Gambler’s ruin problem
At the beginning of this chapter we introduced the gambler’s ruin problem as an ex-
ample of a random process. Here we wish to redo that problem as a Markov chain
Our particular version of the game is as follows: A gambler plays a game involving the
ﬂipping of a coin. The probability of the coin coming up heads is p while the probability of
the coin coming up tails is q = 1−p. He enters the game with some initial amount of money
and plays until (1) he has lost all of his money or (2) he has gained N units of money. We
would like to describe this game at the j ﬂip of the coin.
Let xi denote the probability that the gambler has i units of money. At the j ﬂip,
these probabilities will be aﬀected by the states xi+1 and xi−1 according to
xj+1
i
= pxj
i+1 + qxj
i−1,
i = 1, 2, . . . , N −1.
(7.3.51)
Equation 7.3.51 does not describe the states i = 0 and i = N, the absorbing states. State
i = 0 corresponds to the gambler losing all of his money and quitting the game while state
i = N corresponds to the gambler winning N units of money and calling it a night. Once

Random Processes
365
q
p
p
p
q
q
0
1
N
2
N−2
N−1
1
1
Figure 7.3.1: Markov chain diagram for the gambler’s ruin problem.
these absorbing states are attained, there is no way of going to another state: xj+1
0
= xj
0
and xj+1
N
= xj
N. Since these absorbing states can be eventually reached from any other
state, the game will eventually reach a steady state. We have illustrated this Markov chain
in Figure 7.3.1.
The most convenient way of computing x is via matrix algebra. Using matrix notation,
we can compute the probabilities from:
xj+1 = xjP,
(7.3.52)
where
P =










1
0
0
0
0
· · ·
0
0
0
0
q
0
p
0
0
· · ·
0
0
0
0
0
q
0
p
0
· · ·
0
0
0
0
...
...
...
...
...
...
...
...
...
...
0
0
0
0
0
· · ·
q
0
p
0
0
0
0
0
0
· · ·
0
q
0
p
0
0
0
0
0
· · ·
0
0
0
1










(7.3.53)
and x is the row vector [x0 x1 · · · xN−1xN].
To illustrate the evolution of the gambler’s ruin problem, let us set N = 3, p = q = 0.5,
and x0 = [0 1 0 0]T . Then.
x1 = x0P = [0.5 0 0.5 0]
(7.3.54)
x2 = x0P 2 = [0.5 0.25 0 0.25]
(7.3.55)
x3 = x0P 3 = [0.625 0 0.125 0.25]
(7.3.56)
x10 = x0P 10 = [0.66601562 0.00097656 0 0.33300781]
(7.3.57)
x100 = x0P 100 = [0.6660666 0.00000000 0.00000000 0.3333333].
(7.3.58)
The interpretation of these results is straightforward. After 100 games, the probability
that the gambler, with an initial bankroll of one unit of money, will lose all his money is
2/3 while the chance that he will go home with 3 units of money is 1/3. There are no other
outcomes to the game.
Problems
1. Given
P =

3/4
1/4
1/2
1/2

,
(a) compute P n and (b) ﬁnd limn→∞P n.
2. Suppose you want to model how your dog learns a new trick. Let Fido be in state 0 if
he learns the new trick and in state 1 if he fails to learn the trick. Suppose that if he learns

366
Advanced Engineering Mathematics: A Second Course
the trick, he will retain the trick. If he has yet to learn the trick, there is a probability α
of him learning it with each training session. (a) Write down the transition matrix. (b)
Compute P (n) where n is the number of training sessions. (c) What is the steady-state
solution? Interpret your result. (d) Compute the expected amount of time that Fido will
spend in each state during n training sessions.
7.4 BIRTH AND DEATH PROCESSES
In the previous section we considered two-state Markov chains that undergo n steps.
As the time interval between steps tends to zero, the Markov process becomes continuous
in time. In this section and the next, we consider two independent examples of continuous
Markov processes.
We began Chapter 6 by showing that the deterministic description of birth and death
is inadequate to explain the extinction of species. Here we will ﬁll out the details of our
analysis and extend them to population dynamics and chemical kinetics.
Deterministic
models lead to ﬁrst-order ordinary diﬀerential equations, and this description fails when
the system initially contains a small number of particles.
Consider a population of organisms that multiply by the following rules:
1. The sub-populations generated by two co-existing individuals develop completely in-
dependently of one another;
2. an individual existing at time t has a chance λ dt+o(dt) of multiplying by binary ﬁssion
during the following time interval of length dt;
3. the “birth rate” λ is the same for all individuals in the population at any time t;
4. an individual existing at time t has a chance µ dt + o(dt) of dying in the following time
interval of length dt; and
5. the “death rate” µ is the same for all individuals at any time t.
Rule 3 is usually interpreted in the sense that in each birth, just one new member is added
to the population, but of course mathematically (and because the age structure of the
population is being ignored) it is not possible to distinguish between this and an alternative
interpretation in which one of the parents dies when the birth occurs and is replaced by
two children.
Let n0 be the number of individuals at the initial time t = 0 and let pn(t) denote the
probability that the population size N(t) has the value n at the time t. Then
dpn
dt = (n −1)λpn−1 −n(λ + µ)pn + µ(n + 1)pn+1,
n ≥1,
(7.4.1)
and
dp0(t)
dt
= µp1(t),
(7.4.2)
subject to the initial condition that
pn(0) =
 1,
n = n0,
0,
n ̸= n0.
(7.4.3)
Equation 7.4.1 through Equation 7.4.3 constitute a system of linear ordinary equations.
The question now turns on how to solve them most eﬃciently. To this end we introduce a
probability-generating function:
φ(z, t) =
∞
X
n=0
znpn(t).
(7.4.4)

Random Processes
367
Summing Equation 7.4.1 from n = 1 to ∞after we multiplied it by zn and using Equation
7.4.2, we obtain
∞
X
n=0
zn dpn
dt = λ
∞
X
n=1
(n−1)znpn−1(t)−(λ+µ)
∞
X
n=1
nznpn(t)+µ
∞
X
n=0
(n+1)znpn+1(t). (7.4.5)
Because
∞
X
n=0
zn dpn
dt = ∂φ
∂t ,
(7.4.6)
∞
X
n=1
nznpn(t) = z
∞
X
n=0
nzn−1pn(t) = z ∂φ
∂z ,
(7.4.7)
∞
X
n=1
(n −1)znpn−1(t) =
∞
X
k=0
kzk+1pk(t) = z2
∞
X
k=0
kzk−1pk(t) = z2 ∂φ
∂z ,
(7.4.8)
and
∞
X
n=0
(n + 1)znpn+1(t) =
∞
X
k=1
kzk−1pk(t) =
∞
X
k=0
kzk−1pk(t) = ∂φ
∂z ,
(7.4.9)
Equation 7.4.5 becomes the ﬁrst-order partial diﬀerential equation
∂φ
∂t = (λz −µ)(z −1)∂φ
∂z ,
(7.4.10)
subject to the initial condition
φ(z, 0) = zn0.
(7.4.11)
Equation 7.4.10 is an example of a ﬁrst-order partial diﬀerential equation of the general
form
P(x, y)∂u
∂x + Q(x, y)∂u
∂y = 0.
(7.4.12)
This equation has solutions4 of the form u(x, y) = f(ξ) where f(·) is an arbitrary function
that is diﬀerentiable and ξ(x, y) = constant are solutions to
dx
P(x, y) =
dy
Q(x, y).
(7.4.13)
In the present case,
dt
1 = −
dz
(λz −µ)(z −1) = −
dz
(λ −µ)(z −1) +
dz
(λ −µ)(z −µ/λ).
(7.4.14)
Integrating Equation 7.4.14,
−(λ −µ)t + ln[ψ(z)] = ln(ξ),
(7.4.15)
or
ξ(z, t) = ψ(z)e−(λ−µ)t,
(7.4.16)
4 See Webster, A. G., 1966: Partial Diﬀerential Equations of Mathematical Physics. Dover, 446 pp.
See Section 22.

368
Advanced Engineering Mathematics: A Second Course
where
ψ(z) = λz −µ
z −1 .
(7.4.17)
Therefore, the general solution is
φ(z, t) = f
h
ψ(z)e−(λ−µ)ti
.
(7.4.18)
Our remaining task is to ﬁnd f(·). From the initial condition, Equation 7.4.11, we have
that
φ(z, 0) = f[ψ(z)] = zn0.
(7.4.19)
Because z = [µ −ψ(z)]/[λ −ψ(z)], then
f(ψ) =
µ −ψ
λ −ψ
n0
.
(7.4.20)
Therefore,
φ(z, t) =
µ −ψ(z)e−(λ−µ)t
λ −ψ(z)e−(λ−µ)t
n0
.
(7.4.21)
Once we ﬁnd φ(z, t), we can compute the probabilities of each of the species from the
probability generating function. For example,
P{N(t) = 0|N(0) = n0} = p0(t) = φ(0, t).
(7.4.22)
From Equation 7.4.17 we have ψ(0) = µ and
φ(0, t) =
(
µ

1 −e−(λ−µ)t
λ −µe−(λ−µ)t
)n0
,
λ ̸= µ,
(7.4.23)
and
φ(0, t) =

λt
1 + λt
n0
,
λ = µ.
(7.4.24)
An important observation from Equation 7.4.23 and Equation 7.4.24 is that
lim
t→∞p0(t) = 1,
λ ≤µ,
(7.4.25)
and
lim
t→∞p0(t) =
µ
λ
n0
,
λ > µ.
(7.4.26)
This limit can be interpreted as the probability of extinction of the population in a ﬁnite
time.
Consequently, there will be “almost certain” extinction whenever λ ≤µ.
These
results, which are true whatever the initial number of individuals may be, show very clearly
the inadequacy of the deterministic description of population dynamics.
Finally, let us compute the mean and variance for the birth and death process. The
expected number of individuals at time t is
m(t) = E[N(t)] =
∞
X
n=0
n pn(t) =
∞
X
n=1
n pn(t).
(7.4.27)

Random Processes
369
Now
dm
dt =
∞
X
n=1
ndpn
dt =
∞
X
n=1
n [(n −1)λpn−1 −n(λ + µ)pn + µ(n + 1)pn+1]
(7.4.28)
= λ
∞
X
n=1
(n −1)2pn−1 + λ
∞
X
n=1
(n −1)pn−1 −(λ + µ)
∞
X
n=1
n2pn + µ
∞
X
n=1
(n + 1)2pn+1
−µ
∞
X
n=1
(n + 1)pn+1
(7.4.29)
= −(λ + µ)
∞
X
n=1
n2pn + λ
∞
X
i=0
i2pm + µ
∞
X
k=2
k2pk + λ
∞
X
i=0
i pi −µ
∞
X
k=2
k pk.
(7.4.30)
In the ﬁrst three sums in Equation 7.4.30, terms from i, k, n = 2, and onward cancel and
leave −(λ + µ)p1 + λp1 = −µp1. Therefore,
dm
dt = −µp1 + λ
∞
X
i=0
i pi −µ
∞
X
k=2
k pk = (λ −µ)
∞
X
n=0
n pn, = (λ −µ)m.
(7.4.31)
If we choose the initial condition m(0) = n0, the solution is
m(t) = n0e(λ−µ)t.
(7.4.32)
This is the same as the deterministic result with the birth rate b replaced by λ and the death
rate d replaced by µ. Furthermore, if λ = µ, the mean size of the population is constant.
The second moment of N(t) is
M(t) =
∞
X
n=0
n2pn(t).
(7.4.33)
Proceeding as before, we have that
dM
dt =
∞
X
n=1
n2 dpn
dt =
∞
X
n=1
n2 [λ(n −1)pn−1 −(λ + µ)n pn + µ(n + 1)pn+1]
(7.4.34)
= λ
∞
X
n=1
(n −1)3pn−1 + 2λ
∞
X
n=1
(n −1)2pn−1 + λ
∞
X
n=1
(n −1)pn−1 −(λ + µ)
∞
X
n=1
n3pn
+ µ
∞
X
n=1
(n + 1)3pn+1 −2µ
∞
X
n=1
(n + 1)2pn+1 + µ
∞
X
n=1
(n + 1)pn+1
(7.4.35)
= λ
∞
X
k=1
k3pk + 2λ
∞
X
k=1
k2pk + λ
∞
X
k=1
k pk −(λ + µ)
∞
X
n=1
n3pn
+ µ
∞
X
i=2
i3pi −2µ
∞
X
i=2
i2pi + µ
∞
X
i=2
i pi.
(7.4.36)
The three sums, which contain either i3 or k3 or n3 in them, cancel when i, k, n = 2 and
onward; these three sums reduce to −µp1. The sums that involve i2 or k2 can be written

370
Advanced Engineering Mathematics: A Second Course
in terms of M(t). Finally, the sums involving i and k can be expressed in terms of m(t).
Therefore, Equation 7.4.36 becomes the ﬁrst-order ordinary diﬀerential equation
dM
dt −2(λ −µ)M = (λ + µ)m(t) = (λ + µ)n0e(λ−µ)t,
(7.4.37)
with M(0) = n2
0.
Equation 7.4.37 can be solved exactly using the technique of integrating factors. Its
solution is
M(t) = n2
0e2(λ−µ)t + λ + µ
λ −µn0e(λ−µ)t h
e(λ−µ)t −1
i
.
(7.4.38)
From the deﬁnition of variance, Equation 6.6.5, the variance of the population in the birth
and death process equals
Var[N(t)] = n0
(λ + µ)
(λ −µ)e(λ−µ)t h
e(λ−µ)t −1
i
,
λ ̸= µ,
(7.4.39)
or
Var[N(t)] = 2λn0t,
λ = µ.
(7.4.40)
• Example 7.4.1: Chemical kinetics
The use of Markov processes to describe birth and death has become quite popular.
Indeed, it can be applied to any phenomena where something is being created or destroyed.
Here we illustrate its application in chemical kinetics.
Let the random variable X(t) be the number of A molecules in a unimolecular reaction
A →B (such as radioactive decay) at time t. A stochastic model that describes the decrease
of A can be constructed from the following assumptions:
1. The probability of transition from n to n −1 in the time interval (t, t + ∆t) is nλ∆t +
o(∆t) where λ is a constant and o(∆t) denotes that o(∆t)/∆t →0 as ∆t →0.
2. The probability of a transition from n to n −j, j > 1, in the time interval (t, t + ∆t) is
at least o(∆t) because the time interval is so small that only one molecule undergoes
a transition.
3. The reverse reaction occurs with probability zero.
The equation that governs the probability that X(t) = n is
pn(t + ∆t) = (n + 1)λ∆tpn+1(t) + (1 −λn∆t)pn(t) + o(∆t).
(7.4.41)
Transposing pn(t) from the right side, dividing by ∆t, and taking the limit ∆t →0, we
obtain the diﬀerential-diﬀerence equation5
dpn
dt = (n + 1)λpn+1(t) −nλpn(t).
(7.4.42)
Equation 7.4.42 is frequently called the stochastic master equation. The ﬁrst term on the
right side of this equation vanishes when n = n0.
5 McQuarrie, D. A., 1963: Kinetics of small systems. I. J. Chem. Phys., 38, 433–436.

Random Processes
371
The solution of Equation 7.4.42 once again involves introducing a generating function
for pn(t), namely
F(z, t) =
n0
X
n=0
pn(t)zn,
|z| < 1.
(7.4.43)
Summing Equation 7.4.42 from n = 0 to n0 after multiplying it by zn, we ﬁnd
n0
X
n=0
zn dpn
dt = λ
n0−1
X
n=0
(n + 1)znpn+1(t) −λ
n0
X
n=1
nznpn(t).
(7.4.44)
Because
n0
X
n=0
zn dpn
dt = ∂F
∂t ,
(7.4.45)
n0
X
n=0
nznpn(t) = z
n0
X
n=0
nzn−1pn(t) = z ∂F
∂z ,
(7.4.46)
and
n0−1
X
n=0
(n + 1)znpn+1(t) =
n0
X
k=1
kzk−1pk(t) =
n0
X
k=0
kzk−1pk(t) = ∂F
∂z ,
(7.4.47)
Equation 7.4.44 becomes the ﬁrst-order partial diﬀerential equation
∂F
∂t = λ(1 −z)∂F
∂z .
(7.4.48)
The solution of Equation 7.4.48 follows the method used to solve Equation 7.4.10. Here
we ﬁnd ξ(z, t) via
dt
1 =
dz
λ(z −1),
(7.4.49)
or
ξ(z, t) = (z −1)e−λt.
(7.4.50)
Therefore,
F(z, t) = f

(z −1)e−λt
.
(7.4.51)
To ﬁnd f(·), we use the initial condition that F(z, 0) = zn0. This yields f(y) = (1+y)n0
and
F(z, t) =

1 + (z −1)e−λtn0 .
(7.4.52)
Once again, we can compute the mean and variance of this process. Because
∂F
∂z

z=1
=
n0
X
n=0
npn(t),
(7.4.53)
the mean is given by
E[X(t)] = ∂F(1, t)
∂z
.
(7.4.54)
To compute the variance, we ﬁrst compute the second moment. Since
z ∂F
∂z =
n0
X
n=0
nznpn(t),
(7.4.55)

372
Advanced Engineering Mathematics: A Second Course
and
∂
∂z

z ∂F
∂z

=
n0
X
n=0
n2zn−1pn(t),
(7.4.56)
we have that
n0
X
n=0
n2pn(t) = ∂2F(1, t)
∂z2
+ ∂F(1, t)
∂z
.
(7.4.57)
From Equation 6.6.5, the ﬁnal result is
Var[X(t)] = ∂2F(1, t)
∂z2
+ ∂F(1, t)
∂z
−
∂F(1, t)
∂z
2
.
(7.4.58)
Upon substituting Equation 7.4.52 into Equations 7.4.54 and 7.4.58, the mean and variance
for this process are
E[X(t)] = n0e−λt,
and
Var[X(t)] = n0e−λt  1 −e−λt
.
(7.4.59)
Because the expected value of the stochastic representation also equals the deterministic
result, the two representations are “consistent in the mean.” Further study shows that this
is true only for unimolecular reactions. Upon expanding Equation 7.4.52, we ﬁnd that
pn(t) =

n0
n

e−nλt  1 −e−λtn0−n .
(7.4.60)
An alternative method to the generating function involves Laplace transforms.6 To
illustrate this method, we again examine the reaction A →B.
The stochastic master
equation is
dpn
dt = (n −1)λpn−1(t) −nλpn(t),
n0 ≤n < ∞,
(7.4.61)
pn(t) = 0 for 0 < n < n0, where pn(t) denotes the probability that we have n particles of
B at time t. The initial condition is that pn0(0) = 1 and pm(0) = 0 for m ̸= n0 where n0
denotes the initial number of molecules of B.
Taking the Laplace transform of Equation 7.4.61, we ﬁnd that
sPn(s) = (n −1)λPn−1(s) −nλPn(s),
n0 < n < ∞,
(7.4.62)
and
sPn0(s) −1 = −nλPn0(s).
(7.4.63)
Therefore, solving for Pn(s),
Pn(s) = (n −1)λ
s + nλ Pn−1(s) = λn−n0(n −1)!
(n0 −1)!
n
Y
k=n0
(s + kλ)−1.
(7.4.64)
From partial fractions,
Pn(s) = (n −1)!
(n0 −1)!
n
X
k=n0
(−1)k−n0
(k −n0)!(n −k)!(s + kλ).
(7.4.65)
6 Ishida, K., 1969: Stochastic model for autocatalytic reaction. Bull. Chem. Soc. Japan, 42, 564–565.

Random Processes
373
Taking the inverse Laplace transform,
pn(t) =
(n −1)!
(n0 −1)!(n −n0)!
n
X
k=n0
(−1)k−n0(n −n0)!
(k −n0)!(n −k)! e−λkt
(7.4.66)
=
(n −1)!e−λn0t
(n0 −1)!(n −n0)!
n−n0
X
j=0
(−1)j(n −n0)!
j!(n −n0 −j)!e−λjt
(7.4.67)
=
(n −1)!e−λn0t
(n0 −1)!(n −n0)!
 1 −e−λtn−n0 ,
(7.4.68)
where we introduced j = k −n0 and eliminated the summation via the binomial theorem.
Equation 7.4.68 is identical to results7 given by Delbr¨uck using another technique.
⊓⊔
• Example 7.4.2
In the chemical reaction rA
λ
→
←
µ B, r molecules of A combine to form one molecule of B.
If X(t) = n is the number of B molecules, then the probability pn(t) = P{X(t) = n} of
having n molecules of B is given by
dpn
dt = −[nµ + (N −n)λ] pn + (N −n + 1)λpn−1 + (n + 1)µpn+1,
(7.4.69)
where 0 ≤n ≤N, rN is the total number of molecules of A, λ is the rate at which r
molecules of A combine to produce B, and µ is the rate at which B decomposes into A.
Multiplying Equation 7.4.69 by zn and summing from n = −1 to N + 1,
N+1
X
n=−1
zn dpn
dt = −Nλ
N+1
X
n=−1
znpn + (λ −µ)
N+1
X
n=−1
nznpn + Nλ
N+1
X
n=−1
znpn−1
−λ
N+1
X
n=−1
(n −1)znpn−1 + µ
N+1
X
n=−1
(n + 1)znpn+1.
(7.4.70)
Deﬁning
F(z, t) =
N+1
X
n=−1
pn(t)zn,
|z| < 1,
(7.4.71)
with p−1 = pN+1 = 0, we have that
∂F
∂t =
N+1
X
n=−1
zn dpn
dt ,
(7.4.72)
∂F
∂z =
N+1
X
n=−1
nzn−1pn =
N
X
i=−2
(i + 1)zipi+1 =
N+1
X
i=−1
(i + 1)zipi+1,
(7.4.73)
7 Delbr¨uck, M., 1940: Statistical ﬂuctuations in autocatalytic reactions. J. Chem. Phys., 8, 120–124.
See his Equation 7.

374
Advanced Engineering Mathematics: A Second Course
z ∂F
∂z =
N+1
X
n=−1
nznpn,
(7.4.74)
z2 ∂F
∂z =
N+1
X
n=−1
nzn+1pn =
N+2
X
i=0
(i −1)zipi−1 =
N+1
X
i=−1
(i −1)zipi−1,
(7.4.75)
and
F =
N+1
X
n=−1
zn+1pn =
N+2
X
i=0
zipi−1 =
N+1
X
i=−1
zipi−1.
(7.4.76)
Therefore, the diﬀerential-diﬀerence equation, Equation 7.4.70, can be replaced by
∂F
∂t = Nλ(z −1)F +

µ −(µ −λ)z −λz2 ∂F
∂z .
(7.4.77)
Using the same technique as above, this partial diﬀerential equation can be written as
dt
−1 =
dz
(1 −z)(µ + λz) =
dF
−Nλ(z −1).
(7.4.78)
Equation 7.4.78 yields the independent solutions
1 −z
µ + λz e−(µ+λ)t = ξ(z, t) = constant,
(7.4.79)
and
(µ + λ)−NF(z, t) = η(z, t) = another constant,
(7.4.80)
where f(·) is an arbitrary, diﬀerentiable function.
If there are m units of B at t = 0,
0 ≤m ≤N, the initial condition is F(z, 0) = zm. Then,
f
 1 −z
µ + λz

=
zm
(µ + λz)N ,
(7.4.81)
or
f(x) = (1 −µx)m
(µ + λ)N (1 + λx)N−m.
(7.4.82)
After some algebra, we ﬁnally ﬁnd that
F(z, t) =
1
(µ + λ)N
n
µ
h
1 −e−(µ+λ)ti
+ z
h
λ + µe−(µ+λ)tiom
×
n
µ + λe−(µ+λ)t + λz
h
1 −e−(µ+λ)tioN−m
.
(7.4.83)
Computing the mean and variance, we obtain
E(X) =
m
µ + λ
h
λ + µe−(µ+λ)ti
+ (N −m)λ
µ + λ
h
1 −e−(µ+λ)ti
,
(7.4.84)
and
Var(X) =
mµ
(µ + λ)2
h
λ + µe−(µ+λ)ti h
1 −e−(µ+λ)ti
+ (N −m)λ
(µ + λ)2
h
µ + λe−(µ+λ)ti h
1 −e−(µ+λ)ti
.
(7.4.85)

Random Processes
375
Problems
1. During their study of growing cancerous cells (with growth rate α), Bartoszy´nski et
al.8 developed a probabilistic model of a tumor that has not yet metastasized. In their
mathematical derivation a predictive model gives the probability pn(t) that certain nth
type of cells (out of N) will develop. This probability can change in two ways: (1) Each of
the existing cells has the probability λn∆t + o(∆t) of mutating to another type between t
and t+∆t. (2) The probability that cells in state n at time t will shed a metastasis between
t and t + ∆t is µncet/α∆t + o(∆t), where µ is a constant and c is the size of a single cell.
Setting ρ = λc/N and ν = µc, the governing equations for pn(t) are
dpn
dt = −(ρ + ν)net/αpn + ρ(n + 1)et/αpn+1,
n = 0, 1, 2, . . . , N −1,
and
dpN
dt
= −(ρ + ν)Net/αpN,
with the initial conditions pN(0) = 1 and pn(0) = 0 if n ̸= N.
Step 1: Introducing the generating function
φ(z, t) =
N
X
n=0
znpn(t),
0 ≤z ≤1,
show that our system of linear diﬀerential-diﬀerence equations can be written as the ﬁrst-
order partial diﬀerential equation
∂φ
∂t = [ρ −(ρ + ν)z]et/α ∂φ
∂z
with φ(z, 0) = zN.
Step 2: Solve the partial diﬀerential equation in Step 1 and show that
φ(z, t) =

ρ
ρ + ν
N 
1 −

1 −ρ + ν
ρ
z

exp
h
−α(ρ + ν)

et/a −1
iN
.
Project: Stochastic Simulation of Chemical Reactions
Most stochastic descriptions of chemical reactions cannot be attacked analytically and
numerical simulation is necessary. The purpose of this project is to familiarize you with
some methods used in the stochastic simulation of chemical reactions. In particular, we will
use the Lokta reactions given by the reaction equations:
A + X
k1
−→2X,
(1)
X + Y
k2
−→2Y,
(2)
Y
k3
−→Z.
(3)
8 Bartoszy´nski, R., B. F. Jones, and J. P. Klein, 1985: Some stochastic models of cancer metastases.
Commun. Statist.-Stochastic Models, 1, 317–339.

376
Advanced Engineering Mathematics: A Second Course
0
5
10
15
20
25
30
0
500
1000
1500
2000
2500
3000
 time
 number of x molecules
0
5
10
15
20
25
30
0
500
1000
1500
2000
2500
3000
 time
 number of y molecules
Figure 7.4.1: The temporal variation of the molecules in a Lokta reaction when ∆t = 10−5, k1a = 10,
k2 = 0.01, k3 = 10, and x(0) = y(0) = 1000.
Surprisingly, simple numerical integration of the master equation is not fruitful. This occurs
because of the number and nature of the independent variables; there is only one master
equation but N reactants and time for independent variables.
An alternative to integrating the master equation is a direct stochastic simulation. In
this approach, the (transition) probability for each reaction is computed: p1 = k1ax∆t,
p2 = k2xy∆t, and p3 = k3y∆t, where ∆t is the time between each consecutive state and
a is the constant number of molecules of A.
The obvious question is: Which of these
probabilities should we use?
Our ﬁrst attempt follows Nakanishi:9 Assume that ∆t is suﬃciently small so that
p1 + p2 + p3 < 1.
Using a normalized uniform distribution, such as MATLAB’s rand,
compute a random variable r for each time step. Then march forward in time. At each
time step, there are four possibilities. If 0 < r ≤p1, then the ﬁrst reaction occurs and
x(t + ∆t) = x(t) + 1, y(t + ∆t) = y(t). If p1 < r ≤p1 + p2, then the second reaction occurs
and x(t + ∆t) = x(t) −1, y(t + ∆t) = y(t) + 1. If p1 + p2 < r ≤p1 + p2 + p3, then the third
reaction occurs and x(t + ∆t) = x(t), y(t + ∆t) = y(t) −1. Finally, if p1 + p2 + p3 < r ≤1,
then no reaction occurs and x(t + ∆t) = x(t), y(t + ∆t) = y(t).
For the ﬁrst portion of this project, create MATLAB code to simulate our chemical
reaction using this simulation technique. Explore how your results behave as you vary x(0),
y(0) and especially ∆t. See Figure 7.4.1.
One of the diﬃculties in using Nakanishi’s method is the introduction of ∆t. What
value should we choose to ensure that p1 + p2 + p3 < 1? Several years later, Gillespie10
9 This is the technique used by Nakanishi, T., 1972: Stochastic analysis of an oscillating chemical
reaction. J. Phys. Soc. Japan, 32, 1313–1322.
10 Gillespie, D. T., 1976: A general method for numerically simulating the stochastic time evolution
of coupled chemical reactions. J. Comput. Phys., 22, 403–434; Gillespie, D. T., 1977: Exact stochastic

Random Processes
377
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
 time
 number of x molecules
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
 time
 number of y molecules
Figure 7.4.2: Same as Figure 7.4.1 except that Gillespie’s method has been used.
developed a similar algorithm. He introduced three parameters, a1 = k1ax, a2 = k2xy, and
a3 = k3y, along with a0 = a1 + a2 + a3. These parameters a1, a2, and a3 are similar to the
probabilities p1, p2, and p3. Similarly, he introduced a random number r2 that is chosen
from a normalized uniform distribution. Then, if 0 < r2a0 ≤a1, the ﬁrst reaction occurs
and x(t + ∆t) = x(t) + 1, y(t + ∆t) = y(t). If a1 < r2a0 ≤a1 + a2, then the second reaction
occurs and x(t + ∆t) = x(t) −1, y(t + ∆t) = y(t) + 1. If a1 + a2 < r2a0 ≤a0, then the third
reaction occurs and x(t + ∆t) = x(t), y(t + ∆t) = y(t) −1. Because of his selection criteria
for the reaction that occurs during a time step, one of the three reactions must take place.
See Figure 7.4.2.
The most radical diﬀerence between the Nakanishi and Gillespie schemes involves the
time step. It is no longer constant but varies with time and equals ∆t = ln(1/r1)/a0, where
r1 is a random variable selected from a normalized uniform distribution. The theoretical
justiﬁcation for this choice is given in Section III of his paper.
For the second portion of this project, create MATLAB code to simulate our chemical
reaction using Gillespie’s technique. You might like to plot x(t) vs y(t) and observe the
patterns that you obtain.
Finally, for a speciﬁc time, compute the probability density function that gives the
probability that x and y molecules exist. See Figure 7.4.3.
7.5 POISSON PROCESSES
The Poisson random process is a counting process that counts the number of occur-
rences of some particular event as time increases. In other words, for each value of t, there
simulation of coupled chemical reactions. J. Phys. Chem., 81, 2340–2361

378
Advanced Engineering Mathematics: A Second Course
0
1000
2000
3000
0
0.2
0.4
0.6
0.8
1
1.2x 10
−3
number of  x  molecules
Estimated PDF
0
1000
2000
3000
0
0.2
0.4
0.6
0.8
1
1.2x 10
−3
number of  y  molecules
Figure 7.4.3: The estimated probability density function for the chemical reactions given by Equations
(1) through (3) (for X on the left, Y on the right) at time t = 10. Five thousand realizations were used in
these computations.
is a number N(t), which gives the number of events that occurred during the interval [0, t].
For this reason N(t) is a discrete random variable with the set of possible values {0, 1, 2, . . .}.
Figure 7.5.1 illustrates a sample function. We can express this process mathematically by
N(t) =
∞
X
n=0
H(t −T[n]),
(7.5.1)
where T[n] is the time to the nth arrival, a random sequence of times. The question now
becomes how to determine the values of T[n]. The answer involves three rather physical
assumptions. They are:
1. N(0) = 0.
2. N(t) has independent and stationary increments. By stationary we mean that for any
two equal time intervals ∆t1 and ∆t2, the probability of n events in ∆t1 equals the
probability of n events in ∆t2. By independent we mean that for any time interval
(t, t + ∆t) the probability of n events in (t, t + ∆t) is independent of how many events
have occurred earlier or how they have occurred.
3.
P[N(t + ∆t) −N(t) = k ] =
( 1 −λ∆t,
k = 0,
λ∆t,
k = 1,
0,
k > 1,
(7.5.2)
for all t. Here λ equals the expected number of events in an interval of unit length of
time. Because E[N(t)] = λ, it is the average number of events that occur in one unit
of time and in practice it can be measured experimentally.
We begin our analysis of Poisson processes by ﬁnding P[N(t) = 0 ] for any t > 0. If
there are no arrivals in [0, t], then there must be no arrivals in [0, t−∆t] and also no arrivals

Random Processes
379
4
3
2
1
N(t)
t
t
t
t
t
t
1
2
4
5
3
Figure 7.5.1: Schematic of a Poisson process.
in (t −∆t, t]. Therefore,
P[N(t) = 0 ] = P[N(t −∆t) = 0, N(t) −N(t −∆t) = 0 ].
(7.5.3)
Because N(t) is independent,
P[N(t) = 0 ] = P[N(t −∆t) = 0 ]P[N(t) −N(t −∆t) = 0 ].
(7.5.4)
Furthermore, since N(t) is stationary,
P[N(t) = 0 ] = P[N(t −∆t) = 0 ]P[N(t + ∆t) −N(t) = 0 ].
(7.5.5)
Finally, from Equation 7.5.2,
P[N(t) = 0 ] = P[N(t −∆t) = 0 ](1 −λ∆t).
(7.5.6)
Let us denote P[N(t) = 0 ] by P0(t). Then,
P0(t) = P0(t −∆t)(1 −λ∆t),
(7.5.7)
or
P0(t) −P0(t −∆t)
∆t
= −λP0(t −∆t).
(7.5.8)
Taking the limit as ∆t →0, we obtain the (linear) diﬀerential equation
dP0(t)
dt
= −λP0(t).
(7.5.9)
The solution of Equation 7.5.9 is
P0(t) = Ce−λt,
(7.5.10)
where C is an arbitrary constant. To evaluate C, we have the initial condition P0(0) =
P[N(0) = 0 ] = 1 from Axion 1. Therefore,
P[N(t) = 0 ] = P0(t) = e−λt.
(7.5.11)

380
Advanced Engineering Mathematics: A Second Course
Next, let us ﬁnd P1(t) = P[N(t) = 1 ]. We either have no arrivals in [0, t −∆t] and one
arrival in (t −∆t, t] or one arrival in [0, t −∆t] and no arrivals in (t −∆t, t]. These are the
only two possibilities because there can be at most one arrival in a time interval ∆t. The
two events are mutually exclusive. Therefore,
P[N(t) = 1 ] = P[N(t −∆t) = 0, N(t) −N(t −∆t) = 1 ]
+ P[N(t −∆t) = 0, N(t) −N(t −∆t) = 0 ]
(7.5.12)
= P[N(t −∆t) = 0 ]P[N(t) −N(t −∆t) = 1 ]
+ P[N(t −∆t) = 1 ]P[N(t) −N(t −∆t) = 0 ]
(7.5.13)
= P[N(t −∆t) = 0 ]P[N(t + ∆t) −N(t) = 1 ]
+ P[N(t −∆t) = 1 ]P[N(t + ∆t) −N(t) = 0 ].
(7.5.14)
Equation 7.5.13 follows from independence while Equation 7.5.14 follows from stationarity.
Introducing P1(t) in Equation 7.5.14 and using Axion 3,
P1(t) = P0(t −∆t)λ∆t + P1(t −∆t)(1 −λ∆t),
(7.5.15)
or
P1(t) −P1(t −∆t)
∆t
= −λP1(t −∆t) + λP0(t −∆t).
(7.5.16)
Taking the limit as ∆t →0, we obtain
dP1(t)
dt
+ λP1(t) = λP0(t).
(7.5.17)
In a similar manner, we can prove that
dPk(t)
dt
+ λPk(t) = λPk−1(t),
(7.5.18)
where k = 1, 2, 3, . . . and Pk(t) = P[N(t) = k].
This set of simultaneous linear equations can be solved recursively. Its solution is
Pk(t) = exp(−λt)(λt)k
k!
,
k = 0, 1, 2, . . . ,
(7.5.19)
which is the Poisson probability mass function. Here λ is the average number of arrivals
per second.
In the realization of a Poisson process, one of the important quantities is the arrival
time, tn, shown in Figure 7.5.1. Of course, the arrival time is also a random process and
will change with each new realization. A related quantity Zi = ti −ti−1, the time intervals
between two successive occurrences (interoccurrence times) of Poisson events. We will now
show that the random variables Z1, Z2, etc., are independent and identically distributed
with
P(Zn ≤x) = 1 −e−λx,
x ≥0,
n = 1, 2, 3, . . .
(7.5.20)
We begin by noting that
P(Z1 > t) = P[N(t) = 0] = e−λt
(7.5.21)

Random Processes
381
from Equation 7.5.19. Therefore, Z1 has an exponential distribution.
Let us denote its probability density by pZ1(z1). From the joint conditional density
function,
P(Z2 > t) =
Z ξ1
0
P(Z2 > t|Z1 = z1)pZ1(z1) dz1,
(7.5.22)
where 0 < ξ1 < t. If Z1 = z1, then Z2 > t if and only if N(t + z1) −N(z1) = 0. Therefore,
using the independence and stationary properties,
P{Z2 > t|Z1 = P[N(t + z1) −N(z1) = 0]} = P[N(t) = 0] = e−λt.
(7.5.23)
Consequently,
P(Z2 > t) = e−λt,
(7.5.24)
showing that Z2 is also exponential. Also, Z2 is independent of Z1. Now, let us introduce
pZ2(z2) as the probability density of Z1 + Z2. By similar arguments we can show that Z3
is also exponential. The ﬁnal result follows by induction.
• Example 7.5.1: Random telegraph signal
We can use the fact that interoccurrence times are independent and identically dis-
tributed to realize the Poisson process. An important application of this is in the generation
of the random telegraph signal: X(t) = (−1)N(t). However, no one uses this deﬁnition to
compute the signal; they use the arrival times to change the signal from +1 to −1 or vice
versa.
We begin by noting that Ti = Ti−1 + Zi, with i = 1, 2, . . ., T0 = 0, and Ti is the ith
arrival time. Each Zi has the same exponent probability density function. From Equation
6.4.17,
Zi = 1
λ ln

1
1 −Ui

,
(7.5.25)
where the Ui’s are from a uniform distribution. The realization of a random telegraphic
signal is given by the MATLAB code:
clear
N = 100; % number of switches in realization
lambda = 0.15; % switching rate
X = [ ];
% generate N uniformly distributed random variables
S = rand(1,N);
% transform S into an exponential random variable
T = - log(S)/lambda;
V = cumsum(T); % compute switching times
t = [0.01:0.01:100]; % create time array
icount = 1; amplitude = -1; % initialize X(t)
for k = 1:10000
if ( t(k) >= V(icount) ) % at each switching point
icount = icount + 1;
amplitude = - amplitude; % switch sign
end
X(k) = amplitude; % generate X(t)
end

382
Advanced Engineering Mathematics: A Second Course
plot(t,X) % plot results
xlabel(’\it t’,’FontSize’,25);
ylabel(’\it X(t)/a’,’FontSize’,25);
axis([0 max(t) -1.1 1.1])
This was the MATLAB code that was used to generate Figure 7.5.2.
⊓⊔
• Example 7.0.2
It takes a workman an average of one hour to put a widget together. Assuming that
the task can be modeled as a Poisson process, what is the probability that a workman can
build 12 widgets during an eight-hour shift?
The probability that n widgets can be constructed by time t is
P[N(t) = n] = e−λt (λt)n
n!
.
(7.5.26)
Therefore, the probability that 12 or more widgets can be constructed in eight hours is
P[N(t) ≥12] = e−8
∞
X
n=12
8n
n! = 0.1119,
(7.5.27)
since λ = 1.
We could have also obtained our results by creating 12 exponentially distributed time
periods and summed them together using MATLAB:
t uniform = rand(1,12);
T = - log(1-t uniform);
total time = sum(T);
Then, by executing this code a large number N of times and counting the number icount
of times that total time <= 8, the probability equals icount / N.
Problems
1. Use the generating function
F(z, t) =
∞
X
n=0
pn(t)zn,
|z| < 1,
with F(z, 0) = 1 to solve Equation 7.5.18 by showing that F(z, t) = eλt(z−1). Then, by
expanding F(z, t), recover Equation 7.5.19.

Random Processes
383
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
 y
 P(y)
Figure 7.5.1: The probability density P(y) of the output from an ideal integrator with ﬁnite memory
when the input is a random telegraphic signal when ∆t = 0.01, λ = 2, and τ1 = 10.
Project: Output from a Filter
When the Input Is a Random Telegraphic Signal 11
In the study of many systems, such as linear ﬁlters, the output y(·) can be written as
y(t) =
Z t
−∞
W(t −τ)x(τ) dτ,
where W(·) is the weight function and x(·) is the input. The purpose of this project is
to explore the probability density P(y) of the output when x(t) is the random telegraphic
signal, a Poisson random process. You will ﬁlter this input two ways: (1) ideal integrator
with ﬁnite memory: W(t) = H(t) −H(t −τ1), τ1 > 0, and (2) simple RC = 1 low-pass
ﬁlter W(t) = e−tH(t).
Step 1: Use MATLAB to code x(t) where the expected time between the zeros is λ.
Step 2: Develop MATLAB code to compute y(t) for each of the weight functions W(t).
Step 3: Compute P(y) for both ﬁlters. How do your results vary as λ varies?
Further Readings
Beckmann, P., 1967: Probability in Communication Engineering. Harcourt, Brace & World,
511 pp. A presentation of probability as it applies to problems in communication engineer-
ing.
Gillespie, D. T., 1991: Markov Processes: An Introduction for Physical Scientists. Academic
Press, 592 pp. For the scientist who needs an introduction to the details of the subject.
Hsu, H., 1997: Probability, Random Variables, & Random Processes. McGraw-Hill, 306 pp.
Summary of results plus many worked problems.
11 Suggested by a paper by McFadden, J. A., 1959: The probability density of the output of a ﬁlter when
the input is a random telegraphic signal: Diﬀerential-equation approach. IRE Trans. Circuit Theory, 6,
228–233.

384
Advanced Engineering Mathematics: A Second Course
−1
−0.5
0
0.5
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
 y
 P(y)
Figure 7.5.2: The probability density P(y) of the output from a simple RC = 1 ﬁlter, when the input is
a random telegraphic signal, when λ = 1, and ∆t = 0.05.
Kay, S. M., 2006: Intuitive Probability and Random Processes Using MATLAB. Springer,
833 pp. A well-paced book designed for the electrical engineering crowd.
Ross, S. M., 2007: Introduction to Probability Models. Academic Press, 782 pp. An intro-
ductory undergraduate book in applied probability and stochastic processes.
Tuckwell, H. C., 1995: Elementary Applications of Probability Theory. Chapman & Hall,
292 pp. This book presents applications using probability theory, primarily from biology.

Rt/L
0
0.5
1
1.5
2
2.5
3
3.5
4
I(t)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Chapter 8
Itˆo’s Stochastic Calculus
In elementary diﬀerential equation classes, students study the solution to ﬁrst-order
ordinary diﬀerential equations
dx
dt = a(t, x),
x(0) = x0.
(8.0.1)
There we showed that Equation 8.0.1 has the solution
x(t) = x(0) +
Z t
0
a[η, x(η)] dη.
(8.0.2)
Consider now the analogous stochastic diﬀerential equation:
dX(t) = a[t, X(t)] dt,
X(0) = X0.
(8.0.3)
Although Equations 8.0.1 and 8.0.3 formally appear the same, an immediate question is
what is meant by dX(t). In elementary calculus, the concept of the inﬁnitesimal involves
limits, continuity, and so forth. As we shall see in Section 8.2, Brownian motion, a very
common stochastic process, is nowhere diﬀerentiable. Here we can merely say that dX(t) =
X(t + dt) −X(t).
Consider now a modiﬁcation of Equation 8.0.3 where we introduce a random forcing:
dX(t) = a[t, X(t)] dt + b[t, X(t)] dB(t),
X(0) = X0.
(8.0.4)
Here dB(t) = B(t + dt) −B(t), B(t) denotes Brownian motion and a[t, X(t)] and b[t, X(t)]
are deterministic functions. Consequently, changes to X(t) result from (1) the eﬀects of the
385

386
Advanced Engineering Mathematics: A Second Course
initial conditions and (2) noise generated by Brownian motion (the driving force). Stochastic
processes governed by Equation 8.0.4 are referred to as Itˆo processes.
Following the methods used to derive 8.0.2, we can formally write the solution to
Equation 8.0.4 as
X(t) = X0 +
Z t
0
a[η, X(η)] dη +
Z t
0
b[η, X(η)] dB(η).
(8.0.5)
The ﬁrst integral in Equation 8.0.5 is the conventional Riemann integral from elementary
calculus and is well understood. The second integral, however, is new and must be treated
with care. It is called Itˆo’s stochastic integral and treated in Section 8.3.
In summary, a simple analog to ﬁrst-order ordinary diﬀerential equations for a single
random variable X(t) raises several important questions. What is meant by the inﬁnitesimal
and the integral in stochastic calculus? In this chapter we will focus on Itˆo processes and
the associated calculus. Although Itˆo’s calculus is an important discipline, it is not the only
form of stochastic calculus. The interested student is referred elsewhere for further study.
Problems
1. The Poisson random process N(t) is deﬁned by
N(t) =
∞
X
n=1
H(t −tn),
where tn is a sequence of independent and identically distributed inter-arrival times tn.
A graphical representation of N(t) would consist of ever-increasing steps with the edges
located at t = tn. Use the deﬁnition of dN(t) = N(t + dt) −N(t) to show that
dN(t) =

1,
for t = tn,
0,
otherwise.
2. The telegraph signal is deﬁned by X(t) = (−1)N(t), where N(t) is given by the Poisson
random distribution in Problem 1. Show1 that
dX(t) = X(t + dt) −X(t) = (−1)N(t) h
(−1)dN(t) −1
i
= −2X(t) dN(t).
Hint: Consider dN(t) at various times.
3. If X(t) and Y (t) denote two stochastic processes, use the deﬁnition of the derivative to
show that (a) d[cX(t)] = c dX(t), where c is a constant, (b) d[X(t)±Y (t)] = dX(t)±dY (t),
and (c) d[X(t)Y (t)] = X(t) dY (t) + Y (t) dX(t) + dX(t) dY (t).
8.1 RANDOM DIFFERENTIAL EQUATIONS
A large portion of this book has been devoted to solving diﬀerential equations. Here
we examine the response of diﬀerential equations to random forcing where the diﬀerential
1 Taken from Janaswamy, R., 2013: On random time and on the relation between wave and telegraph
equation. IEEE Trans. Antennas Propag., 61, 2735–2744.

Itˆo’s Stochastic Calculus
387
equation describes a nonrandom process. This is an important question in the sciences and
engineering because noise, a random phenomenon, is ubiquitous in nature.
Because the solution to random diﬀerential equations can be found by conventional
techniques, we can use them to study the eﬀect of randomness on the robustness of a
solution to a diﬀerential equation subject to small changes of the initial condition. Although
this may be of considerable engineering interest, it is really too simple to develop a deep
understanding of stochastic diﬀerential equations.
• Example 8.1.1: LR circuit
One of the simplest diﬀerential equations involves the mathematical model for an LR
electrical circuit:
LdI
dt + RI = E(t),
(8.1.1)
where I(t) denotes the current within an electrical circuit with inductance L and resistance
R, and E(t) is the mean electromotive force. If we solve this ﬁrst-order ordinary diﬀerential
equation using an integrating factor, its solution is
I(t) = I(0) exp

−Rt
L

+ 1
L exp

−Rt
L
 Z t
0
F(τ) exp
Rτ
L

dτ.
(8.1.2)
Clearly, if the electromotive forcing is random, so is the current.
In the previous chapter we showed that the mean and variance were useful parameters
in characterizing a random variable. This will also be true here. If we ﬁnd the mean of the
solution,
E[I(t)] = I(0) exp

−Rt
L

(8.1.3)
provided E[F(t)] = 0. Thus, the mean of the current is the same as that for an ideal LR
circuit.
Turning to the variance,
σ2
X(t) = E[I2(t)] −{E[I(t)]}2
(8.1.4)
= E

I2(0) exp

−2Rt
L

+ 2I(0)
L
exp

−2Rt
L
 Z t
0
E[F(τ)] exp
Rτ
L

dτ
+ 1
L2 exp

−2Rt
L
 Z t
0
Z t
0
E[F(τ)F(τ ′)] exp
R(τ + τ ′)
L

dτ ′ dτ
(8.1.5)
−I2(0) exp

−2Rt
L

= 1
L2 exp

−2Rt
L
 Z t
0
Z t
0
E[F(τ)F(τ ′)] exp
R(τ + ξ)
L

dτ ′ dτ.
(8.1.6)
To proceed further we need the autocorrelation E[F(τ)F(τ ′)]. In papers by Ornstein
et al.2 and Jones and McCombie,3 they adopted a random process with the autocorrelation
2 Ornstein, L. S., H. C. Burger, J. Taylor, and W. Clarkson, 1927: The Brownian movement of a
galvanometer and the inﬂuence of the temperature of the outer circuit. Proc. Roy. Soc. London, Ser. A,
115, 391–406.
3 Jones, R. V., and C. W. McCombie, 1952: Brownian ﬂuctuations in galvanometer and galvanometer
ampliﬁers. Phil. Trans. Roy. Soc. London, Ser. A, 244, 205–230.

388
Advanced Engineering Mathematics: A Second Course
function
E[F(τ)F(τ ′)] = 2Dδ(τ −τ ′).
(8.1.7)
The advantage of this process is that it is mathematically the simplest because it possesses a
white power spectrum. Unfortunately this random process can never be physically realized
because it would possess inﬁnite mean square power. All physically realizable processes
involve a power spectrum that tends to zero at suﬃciently high frequencies. If Φ(ω) denotes
the power spectrum, this condition can be expressed as
Z ∞
0
Φ(ω) dω < ∞.
(8.1.8)
In view of these considerations, let us adopt the autocorrelation
RX(τ −τ ′) =
Z ∞
0
Φ(ω) cos[ω(τ −τ ′)] dω,
(8.1.9)
where Φ(ω) is the power spectrum of F(τ). Therefore, the variance becomes
σ2
X(t) = 1
L2
Z t
0
Z t
0
Z ∞
0
Φ(ω) exp

−R(t −τ)
L

exp

−R(t −τ ′)
L

cos[ω(τ −τ ′)] dω dτ dτ ′.
(8.1.10)
Reversing the ordering of integration,
σ2
X(t) = 1
L2
Z ∞
0
Φ(ω)
Z t
0
Z t
0
exp

−R(2t −τ −τ ′)
L

cos[ω(τ −τ ′)] dτ dτ ′ dω.
(8.1.11)
We can evaluate the integrals involving τ and τ ′ exactly. Equation 8.1.11 then becomes
σ2
X(t) =
Z ∞
0
Φ(ω)
ω2 + R2/L2
h
1 + e−2Rt/L −2e−Rt/L cos(ωt)
i
dω.
(8.1.12)
Let us now consider some special cases. As t →0, σ2
X(t) →0 and the variance is
initially small. On the other hand, as t →∞,
σ2
X(t) =
Z ∞
0
Φ(ω)
ω2 + R2/L2 dω.
(8.1.13)
Thus, the variance grows to a constant value, which we would have found by using Fourier
transforms to solve the diﬀerential equation.
Consider now the special case Φ(ω) = 2D/π, a forcing by white noise. Ignoring the
defects in this model, we can evaluate the integrals in Equation 8.1.13 exactly and ﬁnd that
σ2
X(t) = DL
R

1 −e−2Rt/L
.
(8.1.14)
These results are identical to those found by Uhlenbeck and Ornstein4 in their study of a
free particle in Brownian motion.
⊓⊔
4 Uhlenbeck, G. E., and L. S. Ornstein, 1930: On the theory of the Brownian motion. Phys. Review,
36, 823–841. See the top of their page 828.

Itˆo’s Stochastic Calculus
389
• Example 8.1.2: Damped harmonic motion
Another classic diﬀerential equation that we can excite with a random process is the
damped harmonic oscillator:
y′′ + 2ξω0y′ + ω2
0y = F(t),
(8.1.15)
where 0 ≤ξ < 1, y denotes the displacement, t is time, ω2
0 = k/m, 2ξω0 = β/m, m is
the mass of the oscillator, k is the linear spring constant, and β denotes the constant of a
viscous damper. The solution to this second-order ordinary diﬀerential equation is
y(t) = y(0)e−ξω0t

cos(ω1t) + ξω0
ω1
sin(ω1t)

+ y′(0)
ω1
e−ξωt sin(ω1t) +
Z t
0
h(t −τ)F(τ) dτ,
(8.1.16)
where ω1 = ω0
p
1 −ξ2, and
h(t) = e−ξω0t
ω1
sin(ω1t)H(t).
(8.1.17)
Again we begin by ﬁnding the mean of Equation 8.1.16. It is
E[y(t)] = y(0)e−ξω0t

cos(ω1t) + ξω0
ω1
sin(ω1t)

+y′(0)
ω1
e−ξωt sin(ω1t)+
Z t
0
h(t−τ)E[F(τ)] dτ.
(8.1.18)
If we again choose a random process where E[F(t)] = 0, the integral vanishes and the
stochastic mean of the motion only depends on the initial conditions.
Turning to the variance,
σ2
X(t) = E[y2(t)] −{E[y(t)]}2 =
Z t
0
Z t
0
h(t −τ)h(t −τ ′)E[F(τ)F(τ ′)] dτ dτ ′.
(8.1.19)
If we again adopt the autocorrelation function
RX(τ −τ ′) =
Z ∞
0
Φ(ω) cos[ω(τ −τ ′)] dω,
(8.1.20)
where Φ(ω) is the power spectrum of F(τ), then
σ2
X(t) =
Z ∞
0
Φ(ω)
ω2
1
Z t
0
Z t
0
e−ξω0(2t−τ−τ ′) sin[ω1(t−τ)] sin[ω1(t−τ ′)] cos[ω(τ −τ ′)] dτ dτ ′ dω.
(8.1.21)
Carrying out the integrations in τ and τ ′, we ﬁnally obtain
σ2
X(t) =
Z ∞
0
Φ(ω)
|Ω(ω)|2

1 + e−2ξω0t

1 + 2ξω0
ω1
sin(ω1t) cos(ω1t)
−eω0ξt

2 cos(ω1t) + 2ξω0
ω1
sin(ω1t)

cos(ωt) −eξω0t 2ω
ω1
sin(ω1t) sin(ωt)
+ ξ2ω2
0 −ω2
1 + ω2
ω2
1
sin2(ω1t)

dω,
(8.1.22)
where |Ω(ω)|2 = (ω2
0 −ω2)2 + 4ω2ω2
0ξ2.

390
Advanced Engineering Mathematics: A Second Course
0
1
2
3
4
−0.02
−0.01
0
0.01
0.02
0.03
mean of forcing
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
mean of response
0
1
2
3
4
0.95
1
1.05
t
variance of forcing
0
1
2
3
4
0
2
4
6x 10
−3
t
variance of response
Figure 8.1.1: The mean and variance of the response for the diﬀerential equation y′ + y = f(t) when
forced by Gaussian random noise. The parameters used are y(0) = 1 and ∆τ = 0.01.
As in the previous example, σ2
X(t) →0 as t →0 and the variance is initially small.
The steady-state variance now becomes
σ2
X(t) =
Z ∞
0
Φ(ω)
|Ω(ω)|2 dω.
(8.1.23)
Finally, for the special case Φ(ω) = 2D/π, the variance is
σ2
X(t) =
D
2ξω2
0

1 −e−2ξω0t
ω2
1

ω2
1 + ω0ω1ξ sin(2ω1t) + 2ξ2ω2
0 sin2(ω1t)

.
(8.1.24)
These results are identical to those found by Uhlenbeck and Ornstein5 in their study of a
harmonically bound particle in Brownian motion.
Project: Low-Pass Filter with Random Input
Consider the initial-value problem
y′ + y = f(t),
y(0) = y0.
It has the solution
y(t) = y0e−t + e−t
Z t
0
eτf(τ) dτ.
This diﬀerential equation is identical to that governing an RC electrical circuit. This circuit
has the property that it ﬁlters out high-frequency disturbances. Here we explore the case
when f(t) is a random process.
5 Ibid. See their pages 834 and 835.

Itˆo’s Stochastic Calculus
391
−0.2 −0.1
0
0.1
0.2
0
5
10
15
 t = 0.1
Estimated PDF
−0.2 −0.1
0
0.1
0.2
0
2
4
6
8
 t = 0.5
Estimated PDF
−0.2 −0.1
0
0.1
0.2
0
2
4
6
8
 t = 1.5
 y
Estimated PDF
−0.2 −0.1
0
0.1
0.2
0
2
4
6
 t = 3
Estimated PDF
y
Figure 8.1.2: The probability density function for the response to the diﬀerential equation y′ + y = f(t)
when f(t) is a Gaussian distribution.
Twenty thousand realizations were used to compute the density
function. Here the parameters used are y(0) = 0 and ∆τ = 0.01.
Step 1: Using the MATLAB intrinsic function randn, generate a stationary white noise
excitation of length N. Let deltat denote the time interval ∆t between each new forcing so
that n = 1 corresponds to t = 0 and n = N corresponds to the end of the record t = T.
Step 2: Using the Gaussian random forcing that you created in Step 1, develop a MATLAB
code to compute y(t) given y(0) and f(t).
Step 3: Once you have conﬁdence in your code, modify it so that you can generate many
realizations of y(t). Save your solution as a function of t and realization. Use MATLAB’s
intrinsic functions mean and var to compute the mean and variance as a function of time.
Figure 8.1.1 shows the results when 2000 realizations were used. For comparison the mean
and variance of the forcing have also been included. Ideally this mean and variance should
be zero and one, respectively. We have also included the exact mean and variance, given
by Equation 8.1.3 and Equation 8.1.14, when we set L = R = 1 and D = ∆t/2.
Step 4: Now generalize your MATLAB code so that you can compute the probability density
function of ﬁnding y(t) lying between y and y + dy at various times. Figure 8.1.2 illustrates
four times when y(0) = 0 and ∆τ = 0.01.
Step 5: Modify your MATLAB code so that you can compute the autocovariance. See Figure
8.1.3.
Project: First-Passage Problem with Random Vibrations 6
In the design of devices, it is often important to know the chance that the device will
exceed its design criteria. In this project you will examine how often the amplitude of a
6 Based on a paper by Crandall, S. H., K. L. Chandiramani, and R. G. Cook, 1966: Some ﬁrst-passage
problems in random vibration. J. Appl. Mech., 33, 532–538.

392
Advanced Engineering Mathematics: A Second Course
0
1
2
3
0
1
2
3
0
2
4
6
x 10
−3
 t1
 t2
autocovariance
Figure 8.1.3: The autocovariance function for the diﬀerential equation y′ + y = f(t) when f(t) is a
Gaussian distribution. Twenty thousand realizations were used. The parameters used here are y(0) = 0
and ∆τ = 0.01.
simple, slightly damped harmonic oscillator
y′′ + 2ζω0y′ + ω2
0y = f(t),
0 < ζ ≪1,
(8.1.25)
will exceed a certain magnitude when forced by white noise. In the physical world this
transcending of a barrier or passage level leads to “bottoming” or “short circuiting.”
Step 1: Using the MATLAB command randn, generate a stationary white noise excitation
of length N. Let deltat denote the time interval ∆t between each new forcing so that n =
1 corresponds to t = 0 and n = N corresponds to the end of the record t = T.
Step 2: The exact solution to Equation 8.1.25 is
y(t) = y(0)e−ζω0t
"
cos
p
1 −ζ2 ω0t

+
ζ
p
1 −ζ2 sin
p
1 −ζ2 ω0t
#
+
y′(0)
ω0
p
1 −ζ2 e−ζω0t sin
p
1 −ζ2 ω0t

(8.1.26)
+
Z t
0
e−ζω0(t−τ) sin
hp
1 −ζ2 ω0(t −τ)
i
p
1 −ζ2
f(τ)
ω2
0
d(ω0τ)
= y(0)e−ζω0t
"
cos
p
1 −ζ2 ω0t

+
ζ
p
1 −ζ2 sin
p
1 −ζ2 ω0t
#
+
y′(0)
ω0
p
1 −ζ2 e−ζω0t sin
p
1 −ζ2 ω0t

(8.1.27)
+ e−ζω0t sin
p
1 −ζ2 ω0t

p
1 −ζ2
Z t
0
eζω0τ cos
p
1 −ζ2 ω0τ
 f(τ)
ω2
0
d(ω0τ)

Itˆo’s Stochastic Calculus
393
0
20
40
60
80
100
−4
−2
0
2
4
forcing
0
20
40
60
80
100
−2
−1
0
1
2
ω0t
y(t)
Figure 8.1.4: A realization of the random function y(t) governed by Equation (1) when forced by the
Gaussian random forcing shown in the top frame. The parameters used here are y(0) = 1, y′(0) = 0.5,
ζ = 0.1, and ω0∆τ = 0.02.
−e−ζω0t cos
p
1 −ζ2 ω0t

p
1 −ζ2
Z t
0
eζω0τ sin
p
1 −ζ2 ω0τ
 f(τ)
ω2
0
d(ω0τ).
Because you will be computing numerous realizations of y(t) for diﬀerent f(t)’s, an eﬃcient
method for evaluating the integrals must be employed. Equation 8.1.27 is more eﬃcient
than Equation 8.1.26.
Using the Gaussian random forcing that you created in Step 1, develop a MATLAB code
to compute y(t) given y(0), y′(0), ζ and f(t). Figure 8.1.4 illustrates a realization where
the trapezoidal rule was used to evaluate the integrals in Equation 8.1.27.
Step 3: Now that you can compute y(t) or y(n) for a given Gaussian random forcing,
generalize your code so that you can compute irun realizations and store them in y(n,m)
where m = 1:irun. For a speciﬁc n or ω0t, you can use MATLAB’s commands mean and var
to compute the mean µX(t) and the variance σ2
X(t). Figure 8.1.5 shows the results when
1000 realizations were used. For comparison the mean and variance of the forcing have also
been included. Ideally this mean and variance should be zero and one, respectively. The
crosses give the exact results that
µX(t) = y(0)e−ζω0t
"
cos
p
1 −ζ2 ω0t

+
ζ
p
1 −ζ2 sin
p
1 −ζ2 ω0t
#
+
y′(0)
ω0
p
1 −ζ2 e−ζω0t sin
p
1 −ζ2 ω0t

and Equation 8.1.24 when D = ω0∆t/2.
Step 4: Finally, generalize your MATLAB code so that you store the time T(m) that the
solution y(n) exceeds a certain amplitude b > 0 for the ﬁrst time during the realization m.

394
Advanced Engineering Mathematics: A Second Course
0
10
20
30
40
50
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
mean of forcing
0
10
20
30
40
50
−1
−0.5
0
0.5
1
1.5
mean of response
0
10
20
30
40
50
0.8
0.9
1
1.1
1.2
ω0t
variance of forcing
0
10
20
30
40
50
0
0.02
0.04
0.06
0.08
0.1
0.12
ω0t
variance of response
Figure 8.1.5: The mean µX(t) and variance σ2
X(t) of a slightly damped simple harmonic oscillator when
forced by the Gaussian random noise. The parameters used here are y(0) = 1, y′(0) = 0.5, ζ = 0.1, and
ω0∆τ = 0.02.
Of course, you can do this for several diﬀerent b’s during a particular realization. Once you
have this data you can estimate the probability density function using histc. Figure 8.1.6
illustrates four probability density functions for b = 0.4, b = 0.8, b = 1.2, and b = 1.6.
Project: Wave Motion Generated by Random Forcing 7
In the previous projects we examined ordinary diﬀerential equations that we forced
with a random process. Here we wish to extend our investigation to the one-dimensional
wave equation
∂2u
∂t2 −∂2u
∂x2 = cos(ωt)δ[x −X(t)],
subject to the boundary conditions
lim
|x|→∞u(x, t) →0,
0 < t,
and initial conditions
u(x, 0) = ut(x, 0) = 0,
−∞< x < ∞.
Here ω is a constant and X(t) is a stochastic process.
In Example 5.4.4 we show that the solution to this problem is
u(x, t) = 1
2
Z t
0
H[t −τ −|X(τ) −x|] cos(ωτ) dτ.
7 Based on a paper by Knowles, J. K., 1968: Propagation of one-dimensional waves from a source in
random motion. J. Acoust. Soc. Am., 43, 948–957.

Itˆo’s Stochastic Calculus
395
0
10
20
30
40
50
0
0.02
0.04
0.06
0.08
0.1
 b = 0.4
Estimated PDF
0  
50
100
150
200
0
0.005
0.01
0.015
0.02
0.025
 b = 0.8
Estimated PDF
0   
250
500 
750
1000
0
1
2
3
4
5x 10
−3
 b = 1.2
ω0T
Estimated PDF
0   
500
1000
1500
2000
0
2
4
6
8x 10
−4
 b = 1.6
ω0T
Estimated PDF
Figure 8.1.6: The probability density function that a slightly damped oscillator exceeds b at the time
ω0T. Fifty thousand realizations were used to compute the density function. The parameters used here are
y(0) = 0, y′(0) = 0, ζ = 0.05, and ω0∆τ = 0.05. The mean value of ω0T is 10.7 when b = 0.4, 41.93 when
b = 0.8, 188.19 when b = 1.2, and 1406.8 when b = 1.6.
When the stochastic forcing is absent X(t) = 0, we can evaluate the integral and ﬁnd that
u(x, t) = 1
2ω H(t −|x|) sin[ω(t −|x|)].
Step 1: Invoking the MATLAB command randn, use this Gaussian distribution to numeri-
cally generate an excitation X(t).
Step 2: Using the Gaussian distribution from Step 1, develop a MATLAB code to com-
pute u(x, t). Figure 8.1.7 illustrates one realization where the trapezoidal rule was used to
evaluate the integral.
Step 3: Now that you can compute u(x, t) for a particular random forcing, generalize your
code so that you can compute irun realizations. Then, for particular values of x and t, you
can compute the corresponding mean and variance from the irun realizations. Figure 8.1.8
shows the results when 10,000 realizations were used.
Step 4: Redo your calculations but use a sine wave with random phase: X(t) = A sin(Ωt+ξ),
where A and Ωare constants and ξ is a random variable with a uniform distribution on
[0, 2π].
8.2 RANDOM WALK AND BROWNIAN MOTION
In 1827 the Scottish botanist Robert Brown (1773–1858) investigated the fertilization
process in a newly discovered species of ﬂower. Brown observed under the microscope that
when the pollen grains from the ﬂower were suspended in water, they performed a “rapid

396
Advanced Engineering Mathematics: A Second Course
−6
−4
−2
0
2
4
6
0
1
2
3
4
5
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
 t
 x
 u(x,t)
−6
−4
−2
0
2
4
6
0
1
2
3
4
5
−0.25
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
 t
 x
 u(x,t)
(a)
(b)
Figure 8.1.7: The solution (realization) of the wave equation when forced by a Gaussian distribution and
ω = 2. In frame (a), there is no stochastic forcing X(t) = 0. Frame (b) shows one realization.
−6
−4
−2
0
2
4
60
1
2
3
4
5
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
0.2
 t
 x
 mean
−6
−4
−2
0
2
4
60
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
x 10
−3
 t
 x
 variance
Figure 8.1.8: The mean and variance when the wave equation is forced by the stochastic forcing cos(ωt)δ[x
−X(t)], where ω = 2 and X(t) is a Gaussian distribution.
oscillation motion.” This motion, now known as Brownian motion, results from the random
kinetic strikes on the pollen grain by water molecules. Brownian motion is an example of
a random process known as random walk. This process has now been discovered in diverse
disciplines, from biology8 to ﬁnance. In this section we examine its nature.
Consider a particle that moves along a straight line in a series of steps of equal length.
Each step is taken, either forwards or backwards, with equal probability 1
2. After taking
N steps, the particle could be at any one (let us denote it m) of the following points:
8 Codling, E. A., M. J. Plank, and S. Benhamou, 2008: Random walk models in biology. J. R. Soc.
Interface, 5, 813–834.

Itˆo’s Stochastic Calculus
397
0
10
20
30
40
50
−10
−5
0
5
10
15
 N
 m
Figure 8.2.1: Three realizations of a one-dimensional random walk where N = 50.
−N, −N + 1, . . . , −1, 0, 1, . . . , N −1 and N. Here m is a random variable.
We can generate realizations of one-dimensional Brownian motion using the MATLAB
code:
clear
NN = 50; % select the number of steps for the particle to take
t = (0:1:NN); % create ‘‘time’’ as the particle moves
% create an array to give the position at each time step
m = zeros(size(t));
m(1) = 0; % initialize the position of particle
for N = 1:NN % now move the particle
x = rand(1); % generate a random variable lying between [0, 1]
if (x <= 0.5) step = 1; % if less then 0.5, make it a ‘‘head’’
else step = -1; end % otherwise it is a ‘‘tail’’
% move the particle one step to the right or left
m(N+1) = m(N) + step;
end
%
plot the results
hold on
plot(t,m,’--ko’,’LineWidth’,2,’MarkerSize’,8)
xlabel(’N’,’FontSize’,25); ylabel(’m’,’FontSize’,25)
grid on % add a grid to axes
Figure 8.2.1 illustrates three such realizations.
A natural question would now be: What are the quantitative properties of random
walk? In particular, what is the probability P(m, N) that the particle is at point m after N
displacements? We begin by noting the probability of any given sequence of N steps is
  1
2
N.
The desired probability P(m, N) equals
  1
2
N times the number of distinct sequences of steps

398
Advanced Engineering Mathematics: A Second Course
that will lead to the point m after N steps. To reach m, we must take (N +m)/2 steps in the
positive direction and (N −m)/2 in the negative direction since (N +m)/2−(N −m)/2 = m.
(Note both m and N must be even or odd.) The number of these distinct sequences is
N!
 1
2(N + m)

!
 1
2(N −m)

!.
(8.2.1)
Therefore,
P(m, N) =
N!
 1
2(N + m)

!
 1
2(N −m)

!
1
2
N
.
(8.2.2)
Comparing these results with Equation 6.6.14, we see that P(m, N) is simply a binomial
distribution. For this reason, we immediately know E(m) = 0 and Var(m) = N. That
is, the average position is the origin and the spread of the Brownian motion occurs as the
square root of steps taken increases.
The case of greatest interest arises when N is large and m ≪N. Then we can approx-
imate P(m, N) by the Poisson distribution,
P(m, N) ≈
r
2
πN exp

−m2
2N

.
(8.2.3)
Let us reexpress Equation 8.2.3 in terms of x and t where x = m∆x and t = N∆t. Using
these deﬁnitions, our equation becomes
P(x, t) =
1
2
√
πDt
exp

−x2
4Dt

,
(8.2.4)
if we deﬁne D = (∆x)2/(2∆t). The attentive student will note that P(x, t) is the Green’s
function for the heat function, Example 5.5.1.
An alternative approach to this problem would be to compute many random walks and
then calculate the probability density function from these computations. We can construct
a MATLAB code to do this. First we would realize many random walks (here 2000) and
count the number of times that they end at position m:
clear
NN = 100; % set the end point of the random walks
% introduce intermediate positions along the random walk
t = (0:1:NN);
% initialize array ‘‘m’’ which gives the position at any time
m = zeros(size(t));
for icount = 1:2000 % now perform many random walks
m(1) = 0; % initial position of particle in each walk
for N = 1:NN
x = rand(1); % create a random variable between [0, 1]
% if ’’x’’ less than 0.5, we have a ’’heads’’
if (x <= 0.5) step = 1;
else step = -1; end % otherwise we have a ’’tail’’

Itˆo’s Stochastic Calculus
399
−40
−30
−20
−10
0
10
20
30
40
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
 m
Estimated  P(m,100)
Figure 8.2.2: Numerical computation of P(m, 100) using 2000 random walks. The black line gives Equation
8.2.3.
m(N+1) = m(N) + step; % now take a step forward or backward
end
% set up array that tracks of the final position of the particle
location(icount) = m(N+1);
end
xx = -40:1:40;
% now count the particles that ended somewhere
%
between -40 and 40
[n,xout] = hist(location,xx)
% for comparison, compute Equation 8.2.3
w exact = sqrt(2/(pi*NN))*exp(-xout.*xout/(2*NN));
n = n / 2000; % now compute the mass probability function
%
plot the results
bar h = bar(xout,n)
bar child = get(bar h,’Children’)
set(bar child,’CData’,n)
colormap(Autumn)
hold on
plot(xout,w exact,’-k’,’LineWidth’,3)
xlabel(’\it m’,’FontSize’,25)
ylabel(’Estimated \it P(m,100)’,’FontSize’,25)
Figure 8.2.2 illustrates the results of simulating random walk.
• Example 8.2.1: On the probability of striking a barrier
An important question in engineering is what is the probability that a given random sys-
tem will exceed its design constraints. Here we ask a similar question about one-dimensional

400
Advanced Engineering Mathematics: A Second Course
t
ξ
N
m
Figure 8.2.3: Several random walks from the origin to point (ξ, N). All of these walks would be excluded
from our calculations because they either cross or touch the line m = ξ before the ﬁnal step.
Brownian motion: What is the probability that after taking N steps the particle arrives at
ξ without ever having touched or crossed the line m = ξ at any earlier step? We will do it
exactly and then conﬁrm our results using MATLAB.
The arrival of the particle at ξ after N steps implies that its position after N −1 steps
must have been either ξ −1 or ξ + 1. However, a trajectory from (ξ + 1, N −1) to (ξ, N) is
not allowed because it must have crossed the line m = ξ earlier. On the other hand, not all
trajectories arriving at (ξ, N) from (ξ −1, N −1) are acceptable because a certain number
will have touched or crossed the line m = ξ earlier than its last step. See Figure 8.2.3. Thus
the number of permitted ways of arriving at ξ for the ﬁrst time after N steps equals all
possible ways of arriving at ξ minus any arrivals from (ξ −1, N −1) and any arrivals that
crossed or touched the line m = ξ earlier than the N −1.
From our previous work, the number of possible ways from the origin to (ξ, N) is
N!
 1
2(N + ξ)

!
 1
2(N −ξ)

!.
(8.2.5)
The number of possible ways from the origin to (ξ + 1, N −1) is
(N −1)!
 1
2(N + ξ)

!
 1
2(N −ξ −2)

!.
(8.2.6)
Finally, the number of trajectories arriving at (ξ −1, N −1) but having an earlier contact
with, or a crossing of, the line m = ξ is also
(N −1)!
 1
2(N + ξ)

!
 1
2(N −ξ −2)

!,
(8.2.7)
since it equals the number of trajectories that arrive at (ξ + 1, N −1). From Figure 8.2.3
we see that, due to symmetry, the trajectory that leads to (ξ + 1, N −1) also leads to

Itˆo’s Stochastic Calculus
401
One of the great mathematicians of the twentieth century, Norbert Wiener (1894–1964) graduated
from high school at the age of 11 and Tufts at 14. Obtaining a doctorate in mathematical logic
at 18, he repeatedly traveled to Europe for further education. His work extends over an extremely
wide range from stochastic processes to harmonic analysis to cybernetics. (Photo courtesy of the
MIT Museum with permission.)
(ξ −1, N −1). Consequently the number of trajectories from the origin to (ξ, N) that have
never touched or crossed m = ξ is
N!
 1
2(N + ξ)

!
 1
2(N −ξ)

! −2
(N −1)!
 1
2(N + ξ)

!
 1
2(N −ξ −2)

!,
(8.2.8)
or
ξ
N
N!
 1
2(N + ξ)

!
 1
2(N −ξ)

!.
(8.2.9)
The probability P(ξ, N) that we are seeking is
P(ξ, N) = ξ
N
N!
 1
2(N + ξ)

!
 1
2(N −ξ)

!
1
2
N
.
(8.2.10)
For large N, P(ξ, N) is approximately given by
P(ξ, N) = ξ
N
r
2
πN exp

−ξ2
2N

.
(8.2.11)
We can also compute this probability using the MATLAB code given above. In this
code we replace the counting process location(icount) = m(N+1); by
b = sort(m);
if ( (m(NN+1) == b(NN+1)) & (b(NN+1)>b(NN)) )
jcount = jcount + 1;
location(jcount) = m(NN+1);
end
where we initialize jcount = 0 at the beginning. The idea behind this code is as follows:
For each of the icount trajectories, we use the MATLAB routine sort to arrange them from

402
Advanced Engineering Mathematics: A Second Course
−5
0
5
10
15
20
25
30
35
40
45
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5x 10
−3
ξ
Estimated  P(ξ,100)
Figure 8.2.4: The probability P(ξ, N) that a particle will reach the point m = ξ without the particle ever
crossing or touching the line m = ξ earlier than N = 100. The solid line is the theoretical probability given
by Equation 8.2.11. Here 50, 000 random walks were taken.
the left-most to the right-most position. To be included in the count of particles reaching
m = ξ at step N, the last position of the particle must be (ξ, N) and it may never have
reached or crossed m = ξ.
The if condition ensures that both conditions are met.
If
they are, that particular walk is accepted. Once again, the various right-most positions are
binned and the probability is computed. Figure 8.2.4 illustrates this process using 5000
random walks and this result is compared with the probability given by Equation 8.2.11. ⊓⊔
• Example 8.2.2: Wiener process
Consider the time interval (0, t] and let us subdivide it into subintervals of length ∆t
so that there are t/∆t subintervals. Suppose now that a particle, initially at x = 0, takes a
step (in one space dimension) at the times ∆t, 2∆t, . . . and that the size of the step is either
∆x or −∆x, with a probability of 1
2 that the step is to the left or right. The position of
the particle X(t) at time t is a random walk, which has executed t/∆t steps. Because the
position depends on the choice of ∆t and ∆x, X(t) depends upon t, ∆t and ∆x.
Mathematically we can describe this random process by
X(t) =
t/∆t
X
n=1
Zi,
(8.2.12)
where the Zi’s are independent and identically distributed with
P(Zi = ∆x) = P(Zi = −∆x) = 1
2,
(8.2.13)
and n = 1, 2, . . .. For each Zi,
E(Zi) = 0,
and
Var(Zi) = E(Z2
i ) = (∆x)2.
(8.2.14)
From Equation 8.2.12, we see that
E[X(t)] = 0,
and
Var[X(t)] = E(Z2
i ) =
t
∆tVar(Zi) = t(∆x)2
∆t
.
(8.2.15)

Itˆo’s Stochastic Calculus
403
Presently we have said nothing about the relationship between ∆t and ∆x except
that both are small. However, we cannot have just any relationship between them because
the variance would be either zero or inﬁnite. The only reasonable choice is ∆x =
√
∆t,
which makes Var[X(t)] = t for all values of ∆t. In the limit ∆t →0 the random variable
X(t) converges into a random variable, hereafter denoted by B(t), with the properties that
E[B(t)] = 0 and Var[B(t)] = t.
The collection of random variables {B(t), t > 0} is a
continuous process in time and called a Wiener process.
⊓⊔
Our previous example shows that Brownian motion and the Wiener process are very
closely linked. Because Brownian motion occurs in so many physical and biological pro-
cesses, we shall focus on that motion (and the corresponding Wiener process) exclusively
from now on.
We deﬁne the standard Brownian motion (or Wiener process) B(t) as a
stochastic process that has the following properties:
1. It starts at zero: B(0) = 0.
2. Noting that B(t)−B(s) ∼N(0, t−s), E{[B(t)−B(s)]2} = t−s and Var{[B(t)−
B(s)]2} = 2(t −s)2.
Replacing t with t + dt and s with t, we ﬁnd that
E{[dB(t)]2} = dt.
3. It has stationary and independent increments.
Stationary increments means
that B(t + h) −B(η + h) = B(t) −B(η) for all h. An independent increment
means B(t2) −B(t1), . . . , B(tn) −B(tn−1) are independent random variables.
4. Because increments of Brownian motion on adjacent intervals are independent
regardless of the length of the interval, the derivative will oscillate wildly as
∆x →0 and never converge. Consequently, Brownian motion is nowhere diﬀer-
entiable.
5. It has continuous sample paths, i.e., “no jumps.”
6. The expectation values for the moments are given by
E[B2n(t)] = (2n)!tn
n!2n ,
and
E[B2n−1(t)] = 0,
(8.2.16)
where n > 0. See Problem 1 at the end of Section 8.4.
Problems
1. Show that E{sin[aB(t)]} = 0, where a is a real.
2. Show that
E{cos[aB(t)]} =
∞
X
n=0
(−1)n
2nn! (a2t)n,
where a is a real.

404
Advanced Engineering Mathematics: A Second Course
3. Show that E{exp[aB(t)]} = exp(a2t/2), where a is a real.
Project: Probabilistic Solutions of Laplace’s Equation
Laplace’s equation can be solved using ﬁnite diﬀerence or ﬁnite element methods, re-
spectively.
During the 1940s, the apparently unrelated ﬁelds of random processes and
potential theory were shown to be in some sense mathematically equivalent.9 As a result, it
is possible to use Brownian motion to solve Laplace’s equation, as you will discover in this
project. The present numerical method is useful for the following reasons: (1) the entire
region need not be solved in order to determine potentials at relatively few points, (2) com-
putation time is not lengthened by complex geometries, and (3) a probabilistic potential
theory computation is more topologically eﬃcient than matrix manipulations for problems
in two and three spatial dimensions.
To understand this technique,10 consider the following potential problem:
∂2u
∂x2 + ∂2u
∂y2 = 0,
0 < x < 1,
0 < y < 1,
(8.2.17)
subject to the boundary conditions
u(x, 0) = 0,
u(x, 1) = x,
0 < x < 1,
(8.2.18)
and
u(0, y) = u(1, y) = 0,
0 < y < 1.
(8.2.19)
If we introduce a uniform grid with ∆x = ∆y = ∆s, then the ﬁnite diﬀerence method yields
the diﬀerence equation:
4u(i, j) = u(i + 1, j) + u(i −i, j) + u(i, j + 1) + u(i, j −1),
(8.2.20)
with i, j = 1, N −1 and ∆s = 1/N.
Consider now a random process of the Markov type in which a large number N1 of non-
interacting particles are released at some point (x1, y1) and subsequently perform Brownian
motion in steps of length ∆s each unit of time. At some later time, when a few arrive at
point (x, y), we deﬁne a probability P(i, j) of any of them reaching the boundary y = 1
with potential uk at any subsequent time in the future. Whenever one of these particles
does (later) arrive on y = 1, it is counted and removed from the system. Because P(i, j) is
deﬁned over an inﬁnite time interval of the diﬀusion process, the probability of any parti-
cles leaving (x, y) and arriving along some other boundary (where the potential equals 0)
at some future time is 1 −P(i, j). Whenever a particle arrives along these boundaries it is
also removed from the square.
Having deﬁned P(i, j) for an arbitrary (x, y), we now compute it in terms of the proba-
bilities of the neighboring points. Because the process is Markovian, where a particle jumps
from a point to a neighbor with no memory of the past,
P(i, j) = p(i + 1, j|i, j)P(i + 1, j) + p(i −1, j|i, j)P(i −1, j)
+ p(i, j + 1|i, j)P(i, j + 1) + p(i, j −1|i, j)P(i, j −1),
(8.2.21)
9 See Hersh, R., and R. J. Griego, 1969: Brownian motion and potential theory. Sci. Amer., 220,
67–74.
10 For the general case, see Bevensee, R. M., 1973: Probabilistic potential theory applied to electrical
engineering problems. Proc. IEEE, 61, 423–437.

Itˆo’s Stochastic Calculus
405
0
2
4
6
8
10
0
1
2
3
4
5
6
7
8
9
10
 i
 j
Figure 8.2.5: Four Brownian motions within a square domain with ∆x = ∆y. All of the random walks
begin at grid point i = 4 and j = 6.
where p(i + 1, j|i, j) is the conditional probability of jumping to (x + ∆s, y), given that the
particle is at (x, y). Equation 8.2.21 evaluates P(i, j) as the sum of the probabilities of
reaching y = 1 at some future time by various routes through the four neighboring points
around (x, y). The sum of all the p’s is exactly one because a particle at (x, y) must jump
to a neighboring point during the next time interval.
Let us now compare Equation 8.2.20 and Equation 8.2.21. The potential u(i, j) in
Equation 8.2.20 and P(i, j) becomes an identity if we take the conditional probabilities as
p(i + 1, j|i, j) = p(i −1, j|i, j) = p(i, j + 1|i, j) = p(i, j −1|i, j) = 1
4,
and if we also force u(i, N) = P(i, N) = i, u(i, 0) = P(i, 0) = 0, u(0, j) = P(0, j) = 0,
and u(N, j) = P(N, j) = 0. Both the potential u and the probability P become continuous
functions in the space as ∆s →0, and both are well behaved as (x, y) approaches a boundary
point. A particle starting along y = 1, where the potential is uk, has a probability uk of
arriving there; a particle starting on the remaining boundaries, where the potential is zero,
is immediately removed with no chance of arriving along y = 1. From these considerations,
we have
u(i, j) ≡P(i, j) = lim
N→∞
1
N
X
k
Nkuk,
where N is the number of particles starting at (x, y) and Nk equals the number of particles
that eventually—after inﬁnite time—arrive along the entire boundary at potential uk. This
sum includes the boundary y = 1 and (trivially) the remaining boundaries.
Step 1: Develop a MATLAB code to perform two-dimensional Brownian motion. Let U
be a uniformly distributed random variable lying between 0 and 1. You can use rand. If
0 < U ≤1
4, take one step to the right; 1
4 < U ≤1
2, take one step to the left; if 1
2 < U ≤3
4,
take one step downward; and if 3
4 < U ≤1, take one step upward. For the arbitrary point
i,j located on a grid of N ×N points with 2 ≤i, j ≤N −1, repeatedly take a random step

406
Advanced Engineering Mathematics: A Second Course
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
 x
 50 realizations 
 y
 u(x,y)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
 x
 200 realizations 
 y
 u(x,y)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
 x
 800 realizations 
 y
 u(x,y)
0
0.5
1
0
0.5
1
0
0.2
0.4
0.6
0.8
1
 x
 3200 realizations 
 y
 u(x,y)
Figure 8.2.6: Solution to Equation 8.2.17 through Equation 8.2.19 using the probabilistic solution method.
until you reach one of the boundaries. Record the value of the potential at the boundary
point. Let us call this result u k(1). Figure 8.2.5 illustrates four of these two-dimensional
Brownian motions.
Step 2: Once you have conﬁdence in your two-dimensional Brownian motion code, generalize
it to solve Equation 8.2.17 through Equation 8.2.19 using runs realizations at some interior
grid point. Then the solution u(i,j) is given by
u(i, j) =
1
runs
runs
X
n=1
u k(n).
Step 3: Finally, plot your results. Figure 8.2.6 illustrates the potential ﬁeld for diﬀerent
values of runs. What are the possible sources of error in using this method?
8.3 ITˆO’S STOCHASTIC INTEGRAL
In the previous section we noted that Brownian motion (the Wiener process) is nowhere
diﬀerentiable. An obvious question is what is meant by the integral of a stochastic variable.
Consider the interval [a, b], which we subdivide so that a = t0 < t1 < t2 < · · · < tn = b.
The earliest and simplest deﬁnition of the integral is
Z b
a
f(t) dt = lim
∆t→0
n
X
i=1
f(τi) ∆ti,
(8.3.1)

Itˆo’s Stochastic Calculus
407
where ti−1 ≤τi ≤ti and ∆ti = ti −ti−1. In the case of the classic integral, the integration
is with regards to the increment dt.
Itˆo’s integral is an integral where the inﬁnitesimal increment involves Brownian motion
dB(t), which is a random variable. Before we can deﬁne this integral, we must introduce
two important concepts. The ﬁrst one is nonanticipating processes: A process F(t) is a
nonanticipating process if F(t) is independent of any future increment B(s) −B(t) for any
s and t where s > t. Nonanticipating processes are important because Itˆo’s integral applies
only to them.
The second important concept is convergence in the mean square sense. It is deﬁned
by
lim
n→∞E



"
Sn −
Z b
a
F(t) dB(t)
#2

= 0,
(8.3.2)
where Sn is the partial sum
Sn =
n
X
i=1
F(ti−1) [B(ti) −B(ti−1)] .
(8.3.3)
We are now ready to deﬁne the Itˆo integral: It is the limit of the partial sum Sn:
ms−lim
n→∞
Sn =
Z b
a
F(t) dB(t),
(8.3.4)
where we denoted the limit in the mean square sense by ms−lim. Combining Equation 8.3.3
and Equation 8.3.4 together, we ﬁnd that
Z b
a
f[t, B(t)] dB(t) = lim
∆t→0
n
X
i=1
f[ti−1, B(ti−1)] [B(ti) −B(ti−1)] ,
(8.3.5)
where ti = i∆t and ∆t = (b −a)/N. As one might suspect,
Z b
a
dB(t) = B(b) −B(a).
(8.3.6)
Because F(t) and dB(t) are random variables, so is Itˆo’s integral.
The results from Equation 8.3.6 would be misunderstood if we think about them as we
do in conventional calculus. We cannot evaluate the right side of Equation 8.3.6 by looking
up B(t) in some book entitled “Tables of Brownian Motion.” This equation only holds true
for a particular realization (sample path).
• Example 8.3.1
Let us use the deﬁnition of the Itˆo integral to evaluate Itˆo integral
R t
0 B(x) dB(x). In
the present case,
Sn =
n
X
i=1
B(xi−1) [B(xi) −B(xi−1)] ,
(8.3.7)

408
Advanced Engineering Mathematics: A Second Course
where xi = it/n. Because 2a(b −a) = b2 −a2 −(b −a)2,
Sn = 1
2
n
X
i=1
B2(xi) −1
2
n
X
i=1
B2(xi−1) −1
2
n
X
i=1
[B(xi) −B(xi−1)]2
(8.3.8)
= 1
2B2(t) −1
2
n
X
i=1
[B(xi) −B(xi−1)]2 .
(8.3.9)
Therefore,
ms−lim
n→∞
Sn = 1
2B2(t) −1
2 ms−lim
n→∞
n
X
i=1
[B(xi) −B(xi−1)]2
(8.3.10)
= 1
2B2(t) −t
2.
(8.3.11)
As a consequence,
Z t
0
B(η) dB(η) = 1
2B2(t) −t
2,
(8.3.12)
or
Z b
a
B(t) dB(t) = 1
2[B2(b) −B2(a)] −b −a
2
.
(8.3.13)
Consider now the derivative of B2(t),
d[B2(t)] = [B(t + dt) −B(t)]2 = 2B(t) dB(t) + dB(t) dB(t).
(8.3.14)
In order for Equation 8.3.12 and Equation 8.3.14 to be consistent, we arrive at the very
important result that
[dB(t)]2 = dt
(8.3.15)
in the mean square sense. We will repeatedly use this result in the remaining portions of
the chapter.
⊓⊔
Because the Itˆo integral is a random variable, two important quantities are its mean
and variance. Let us turn ﬁrst to the computation of the expectation of
R b
a f[t, B(t)] dB(t).
From Equation 8.3.5 we ﬁnd that
E
(Z b
a
f[t, B(t)] dB(t)
)
= lim
∆t→0 E
( n
X
i=1
f[ti−1, B(ti−1)]∆Bi
)
(8.3.16)
= lim
∆t→0
n
X
i=1
E{f[ti−1, B(ti−1)]}E[∆Bi] = 0.
(8.3.17)
Therefore
E
(Z b
a
f[t, B(t)] dB(t)
)
= 0.
(8.3.18)

Itˆo’s Stochastic Calculus
409
To compute the variance, we begin by noting that
(Z b
a
f[t, B(t)] dB(t)
)2
= lim
∆t→0
( n
X
i=1
f[ti−1, B(ti−1)]∆Bi
)2
(8.3.19)
= lim
∆t→0
n
X
i=1
f 2[ti−1, B(ti−1)](∆Bi)2
(8.3.20)
+ 2
n
X
i=1
n
X
j=1
i̸=j
f[ti−1, B(ti−1)]∆Bi f[tj−1, B(tj−1)]∆Bj.
Taking the expectation of both sides of Equation 8.3.20, we have that
E
Z b
a
f[t, B(t)] dB(t)
2
= lim
∆t→0
n
X
i=1
E{f 2[ti−1, B(ti−1)]}E[(∆Bi)2]
(8.3.21)
+ 2 lim
∆t→0
n
X
i=1
n
X
j=1
i̸=j
E{f(ti−1, B(ti−1)]}E[∆Bi] E{f(tj−1, B(tj−1)]}E[∆Bj]
= lim
∆t→0
n
X
i=1
E{f 2[ti−1, B(ti−1)]}(ti+1 −ti).
(8.3.22)
The double summation vanishes because of the independence of Brownian motion. There-
fore, the ﬁnal result is
E
(Z b
a
f[t, B(t)] dB(t)
)2
=
Z b
a
E{f 2[t, B(t)]} dt.
(8.3.23)
⊓⊔
• Example 8.3.2
Consider the random number X =
R b
a
√
t sin[B(t)] dB(t). Let us ﬁnd E(X) and E(X2).
From Equation 8.3.18, we have that E(X) = 0. For that reason, var(X) = E(X2) and
Var(X) = E(X2) =
Z b
a
E
n√
t sin[B(t)]
2o
dt =
Z b
a
t E

sin2[B(t)]
	
dt
(8.3.24)
=
Z b
a
(t/2)E{1 −cos[2B(t)]} dt =
Z b
a
t
2
"
1 −
∞
X
n=0
(−1)n22ntn
2nn!
#
dt
(8.3.25)
= −1
2
Z b
a
∞
X
n=1
(−1)n2n
n!
tn+1 dt = 1
2
∞
X
n=1
(−1)n+12n
(n + 2)n!
 bn+2 −an+2
.
(8.3.26)
The value of E{cos[2B(t)]} follows from Problem 2 at the end of the last section.
⊓⊔

410
Advanced Engineering Mathematics: A Second Course
Table 8.3.1 gives a list of Itˆo stochastic integrals. Most of these results were not derived
from the deﬁnition of the Itˆo stochastic integral but from Itˆo lemma, to which we now turn.
Problems
Consider the random variable X =
R b
a f[t, B(t)] dB. Find E(X) and Var(X) for the follow-
ing f[t, B(t)]:
1. f[t, B(t)] = t
2. f[t, B(t)] = tB(t)
3. f[t, B(t)] = |B(t)|
4. f[f, B(t)] =
√
t exp[B(t)]
5. If X =
R b
a f(t){sin[B(t)] + cos[B(t)]} dB(t), show that var(X) =
R b
a f 2(t) dt, if f(t) is a
real function.
Project: Numerical Integration of Itˆo’s Integral
Equation 8.3.5 is useful for numerically integrating the Itˆo integral
Z t
0
f[x, B(x)] dB(x).
Write a MATLAB script to check Example 8.3.1 for various values of n when t = 1. How
does the error vary with n?
Project: Numerical Check of Equations 8.3.18 and 8.3.23
Using the script from the previous project, develop MATLAB code to compute Equation
8.3.18 and Equation 8.3.23.
Using a million realizations (sample paths), compare your
numerical results with the exact answer when a = 1, b = 1, and f[t, B(t)] =
√
t sin[B(t)].
8.4 ITˆO’S LEMMA
Before we can solve stochastic diﬀerential equations, we must derive a key result in
stochastic calculus: Itˆo’s formula or lemma. This is stochastic calculus’s version of the
chain rule.
Consider a function f(t) that is twice diﬀerentiable. Using Taylor’s expansion,
df(B) = f(B + dB) −f(B) = f ′(B) dB + 1
2f ′′(B) (dB)2 + · · · ,
(8.4.1)
where B(t) denotes Brownian motion. Integrating Equation 8.4.1 from s to t, we ﬁnd that
Z t
s
df(B) = f[B(t)] −f[B(s)] =
Z t
s
f ′(B) dB + 1
2
Z t
s
f ′′(B) dx + · · · ,
(8.4.2)
because [dB(x)]2 = dx. The ﬁrst integral on the right side of Equation 8.4.2 is an Itˆo’s
stochastic integral while the second one can be interpreted as the Riemann integral of f ′′(B).
Therefore, Itˆo’s lemma or formula is
f[B(t)] −f[B(s)] =
Z t
s
f ′(B) dB + 1
2
Z t
s
f ′′(B) dx
(8.4.3)

Itˆo’s Stochastic Calculus
411
Table 8.3.1: A Table of Itˆo Stochastic Integrals with t > 0 and b > a > 0
1.
Z b
a
dB(t) = B(b) −B(a)
2.
Z t
0
B(η) dB(η) = 1
2[B2(t) −t]
3.
Z t
0
[B2(η) −η] dB(η) = 1
3B2(t) −tB(t)
4.
Z t
0
η dB(η) = tB(t) −
Z t
0
B(η) dη
5.
Z t
0
B2(η) dB(η) = 1
3B3(t) −
Z t
0
B(η) dη
6.
Z t
0
eλ2η/2 cos[λB(η)] dB(η) = 1
λeλ2t/2 sin[λB(t)]
7.
Z t
0
eλ2η/2 sin[λB(η)] dB(η) = 1
λ
n
1 −eλ2t/2 cos[λB(t)]
o
8.
Z t
0
exp

−1
2λ2η ± λB(η)

dB(η) = ± 1
λ

exp

−1
2λ2t ± λB(t)

−1
	
9.
Z b
a
B(η) exp
B2(η)
2η
 dB(η)
η3/2
= b−1/2 exp
B2(b)
2b

−a−1/2 exp
B2(a)
2a

10.
Z b
a
f(η) dB(η) = f(t)B(t)

b
a
−
Z b
a
f ′(η)B(η) dη
11.
Z b
a
g′[B(η)] dB(η) = g[B(t)]

b
a
−1
2
Z b
a
g′′[B(η)] dη

412
Advanced Engineering Mathematics: A Second Course
for t > s.
• Example 8.4.1
Consider the case when f(t) = t2 and s = 0. Then, Itˆo’s formula yields
B2(t) −B2(0) = 2
Z t
0
B(x) dB(x) −
Z t
0
dx.
(8.4.4)
Evaluating the second integral and noting that B(0) = 0, we again obtain Equation 8.3.12,
that
Z t
0
B(x) dB(x) = 1
2[B2(t) −t].
(8.4.5)
⊓⊔
• Example 8.4.2
Consider the case when f(t) = eat and s = 0. Then, Itˆo’s formula yields
eaB(t) −1 = a
Z t
0
eaB(x) dB(x) + a2
2
Z t
0
eaB(x) dx.
(8.4.6)
Computing the expectation of both sides,
E
h
eaB(t)i
−1 = a2
2
Z t
0
E
h
eaB(x)i
dx.
(8.4.7)
Solving this integral equation, we ﬁnd that E[eaB(t)] = ea2t/2, a result that we found earlier
in Problem 2, Section 8.2.
⊓⊔
• Example 8.4.3
If f(t) = sin(λt), λ > 0, then Itˆo’s formula gives
sin[λB(t)] = λ
Z t
0
cos[λB(η)] dB(η) −1
2λ2
Z t
0
sin[λB(η)] dη.
(8.4.8)
Taking the expectation of both sides of Equation 8.4.8, we ﬁnd that
E{sin[λB(t)]} = −1
2λ2
Z t
0
E{sin[λB(η)]} dη.
(8.4.9)
Setting g(t) = E{sin[λB(t)]}, then
g(t) = −1
2λ2
Z t
0
g(η) dη.
(8.4.10)
The solution to this integral equation is g(t) = 0. Therefore, E{sin[λB(t)]} = 0.
⊓⊔

Itˆo’s Stochastic Calculus
413
Educated at the Imperial University of Tokyo, Kiyoshi Itˆo (1915–2008) applied the techniques of
diﬀerential and integral to stochastic processes. Much of Itˆo’s original work from 1938 to 1945 was
done while he worked for the Japanese National Statistical Bureau. After receiving his doctorate,
Itˆo became a professor at the University of Kyoto from 1952 to 1979. (Author: Konrad Jacobs,
Source: Archives of the Mathematisches Forschungsinstitut Oberwolfach.)
The second version of Itˆo’s lemma begins with the second-order Taylor expansion of
the function f(t, x):
f[t + dt, B(t + dt)] −f[t, B(t)] = ft[t, B(t)] dt + fx[t, B(t)] dB(t)
+ 1
2

ftt[t, B(t)] (dt)2 + fxt[t, B(t)] dt dB(t)
(8.4.11)
+ fxx[t, B(t)] [dB(t)]2	
+ · · · .
Here we assume that f[t, B(t)] has continuous partial derivatives of at least second order.
Neglecting higher-order terms in Equation 8.4.11, which include the terms with factors such
as (dt)2 and dt dB(t) but not [dB(t)]2 because [dB(t)]2 = dt, our second version of Itˆo’s
lemma is
f[t, B(t)] −f[s, B(s)] =
Z t
s

ft[η, B(η)] + 1
2fxx[η, B(η)]
	
dη +
Z t
s
fx[η, B(η)] dB(η)
(8.4.12)
if t > s.

414
Advanced Engineering Mathematics: A Second Course
• Example 8.4.4
Consider the function f(t, x) = ex−t/2. Then,
ft(t, x) = −1
2ex−t/2,
fx(t, x) = ex−t/2,
and
fxx(t, x) = ex−t/2.
(8.4.13)
Therefore, from Itˆo’s lemma, we have that
eB(t)−t/2 −eB(s)−s/2 =
Z t
s
e−η/2eB(η) dB(η).
(8.4.14)
⊓⊔
• Example 8.4.5: Integration by parts
Consider the case when F(t, x) = f(t)g(x). The Itˆo formula gives
d[f(t)g(x)] =

f ′(t)g[B(t)] + 1
2f(t)g′′[B(t)]
	
dt + f(t)g′[B(t)] dB(t).
(8.4.15)
Integrating both sides of Equation 8.4.15, we ﬁnd that
Z b
a
f(t)g′[B(t)] dB(t) = f(t)g[B(t)]

b
a
−
Z b
a
f ′(t)g[B(t)] dt −1
2
Z b
a
f(t)g′′[B(t)] dt,
(8.4.16)
which is the stochastic version of integration by parts.
For example, let us choose f(t) = eαt and g(x) = sin(x). Equation 8.4.16 yields
Z t
0
eαη cos[B(η)] dB(η) = eαη sin[B(η)]

t
0
−α
Z t
0
eαη sin[B(η)] dη + 1
2
Z t
0
eαη sin[B(η)] dη
(8.4.17)
= eαt sin[B(t)] −
 α −1
2
 Z t
0
eαη sin[B(η)] dη.
(8.4.18)
In the special case of α = 1
2, Equation 8.4.18 simpliﬁes to
Z t
0
eαη cos[B(η)] dB(η) = et/2 sin[B(t)].
(8.4.19)
⊓⊔
An important extension of Itˆo’s lemma involves the function f[t, X(t)] where X(t) is no
longer simply Brownian motion but is given by the ﬁrst-order stochastic diﬀerential equation
dX(t) = cX(t) dt + σX(t) dB(t),
(8.4.20)
where c and σ are real. The second-order Taylor expansion of the function f[t, X(t)] becomes
f[t + dt,X(t + dt)] −f[t, X(t)] = ft[t, X(t)] dt + fx[t, X(t)] dX(t)
(8.4.21)
+ 1
2

ftt[t, X(t)] (dt)2 + fxt[t, X(t)] dt dX(t) + fxx[t, X(t)] [dX(t)]2	
+ · · · .

Itˆo’s Stochastic Calculus
415
Next, we substitute for dX(t) using Equation 8.4.20, neglect terms involving (dt)2 and
dt dB(t), and substitute [dB(t)]2 = dt. Consequently,
df = f[t + dt, X(t + dt)] −f[t, X(t)]
(8.4.22)
= σX(t)fx[t, X(t)] dB(t) +

ft[t, X(t)] + cX(t)fx[t, X(t)] + 1
2σ2X2(t)fxx[t, X(t)]

dt.
(8.4.23)
The present extension of Itˆo’s lemma reads
f[t, X(t)] −f[s, X(s)] =
Z t
s

ft[η, X(η)] + cX(η)fx[η, X(η)] + 1
2σ2X2(η)fxx[η, X(η)]

dη
+
Z t
s
σX(η)fx[η, X(η)] dB(η)
(8.4.24)
=
Z t
s

ft[η, X(η)] + 1
2σ2X2(η)fxx[η, X(η)]

dη
+
Z t
s
fx[η, X(η)] dX(η),
(8.4.25)
where
dX(η) = cX(η) dη + σX(η) dB(η)
(8.4.26)
and t > s.
We can ﬁnally generalize Itˆo’s formula to the case of several Itˆo processes with respect
to the same Brownian motion. For example, let X(t) and Y (t) denote two Itˆo processes
governed by
dX(t) = A(1,1)(t) dt + A(2,1)(t) dB(t),
(8.4.27)
and
dY (t) = A(1,2)(t) dt + A(2,2)(t) dB(t).
(8.4.28)
For stochastic process f[t, X(t), Y (t)], the Taylor expansion is
df[t, X(t),Y (t)] = ft[t, X(t), Y (t)] dt + fx[t, X(t), Y (t)] dX(t)
+ fy[t, X(t), Y (t)] dY (t)
(8.4.29)
+ 1
2fxx[t, X(t), Y (t)]A(2,1)(t)A(2,1)(t) dt + 1
2fxy[t, X(t), Y (t)]A(2,1)(t)A(2,2)(t) dt
+ 1
2fyx]t, X(t), Y (t)]A(2,2)(t)A(2,1)(t) dt + 1
2fyy[t, X(t), Y (t)]A(2,2)(t)A(2,2)(t) dt.
• Example 8.4.6: Product rule
Consider the special case f(t, x, y) = xy. Then ft = 0, fx = y, fy = x, fxx = fyy = 0,
and fxy = fyx = 1. In this case, Equation 8.4.29 simpliﬁes to
d[X(t)Y (t)] = Y (t) dX(t) + X(t) dY (t) + A(2,1)[t, X(t), Y (t)]A(2,2)[t, X(t), Y (t)] dt.
(8.4.30)
A very important case occurs when A(2,1)[t, X(t), Y (t)] = 0 and X(t) = g(t) is purely
deterministic. In this case,
d[g(t)Y (t)] = Y (t) dg(t) + g(t) dY (t).
(8.4.31)

416
Advanced Engineering Mathematics: A Second Course
This is exactly the product rule from calculus.
Problems
1. (a) Use Equation 8.4.3 and f(t) = tn to show that
Bn(t) = n
Z t
0
Bn−1(x) dB(x) + n(n −1)
2
Z t
0
Bn−2(x) dx.
(b) Show that
E[Bn(t)] = n(n −1)
2
Z t
0
E[Bn−2(x)] dx.
(c) Because E[B(t)] = 0 and E[B2(t)] = t, show that
E

B2k+1(t)

= 0,
and
E

B2k(t)

= (2k)!
2kk! tk.
2. Let f(t, x) = x2t and use Itˆo’s formula to show that
Z t
0
B2(η) dt + 2
Z t
0
ηB(η) dB(η) = tB2(t) −t2/2.
3. Let f(t, x) = x3/2 and use Itˆo’s formula to show that
Z t
0
B1/2(η) dB(η) = 2
3B3/2(t) −1
4
Z t
0
B−1/2(η) dt.
4. Let f(t, x) = x3/3 −tx and use Itˆo’s formula to show that
Z t
0
[B2(η) −η] dB(η) = 1
3B3(t) −t B(t).
5. If f(x) is any continuously diﬀerentiable function, use Equation 8.4.29 to show that
Z t
0
f(η) dB(η) = f(t)B(t) −
Z t
0
f ′(η)B(η) dη.
6. If f(t) = et, use the previous problem to show that
Z t
0
eη dB(η) = etB(t) −
Z t
0
eηB(η) dη.
7. Let G(x) denote the antiderivative of g(x). Use Equation 8.4.3 to show that
Z b
a
g[B(t)] dB(t) = G[B(t)]

b
a
−1
2
Z b
a
g′[B(t)] dt.

Itˆo’s Stochastic Calculus
417
8. (a) If g(x) = xex, use Problem 7 to show that
Z t
0
B(η)eB(η) dB(η) = [B(t) −1]eB(t) + 1 −1
2
Z t
0
[B(η) + 1]eB(η) dη.
(b) Use Equation 8.3.18 to show that
E
h
B(t)eB(t)i
= E
h
eB(t)i
−1 + 1
2
Z t
0
n
E
h
B(η)eB(η)i
+ E
h
eB(η)io
dη
= et/2 −1 + 1
2
Z t
0
n
eη/2 + E
h
B(η)eB(η)io
dη.
(c) Setting g(t) = E

B(t)eB(t)
, use Laplace transforms to show that
E
h
B(t)eB(t)i
= tet/2.
9. (a) If g(x) = 1/(1 + x2), use Problem 7 to show that
Z t
0
dB(η)
1 + B2(η) = arctan[B(t)] +
Z t
0
B(η)
[1 + B2(η)]2 dη.
(b) Use Equation 8.3.18 to show that
Z t
0
E

B(η)
[1 + B2(η)]2

dη = −E{arctan[B(t)]} .
(c) Because
−3
√
3
16
≤
x
(1 + x2)2 ≤3
√
3
16 ,
or
−3
√
3
16 t ≤
Z t
0
B(η)
[1 + B2(η)]2 dη ≤3
√
3
16 t,
show that
−3
√
3
16 t ≤E{arctan[B(t)]} ≤3
√
3
16 t.
10. If g(x) = x/(1 + x2), use Problem 7 to show that
Z t
0
B(η)
1 + B2(η) dB(η) = 1
2 log[1 + B2(t)] −1
2
Z t
0
1 −B2(η)
[1 + B2(η)]2 dη.
11. Use integration by parts with f(t) = eβt and g(x) = −cos(x) to show that
Z t
0
eβη sin[B(η)] dB(η) = 1 −eβt cos[B(t)] +
 β −1
2
 Z t
0
eβη cos[B(η)] dη.

418
Advanced Engineering Mathematics: A Second Course
Then, take β = 1
2 and show that
Z t
0
eη/2 sin[B(η)] dB(η) = 1 −et/2 cos[B(t)].
12. Redo Example 8.4.3 and show that E{cos[λB(t)]} = e−λ2t/2, λ > 0.
13. Use trigonometric double angle formulas to show that
(a)
E{sin[t + λB(t)]} = e−λ2t/2 sin(t),
and
(b)
E{cos[t + λB(t)]} = e−λ2t/2 cos(t),
when λ > 0.
14. Following Example 8.4.4 with f(t, x) = ±λ exp
 ±λx −λ2t/2

, λ > 0, show that
Z t
0
exp

±λB(η) −λ2η
2

dB(η) = ± 1
λ

exp

±λB(t) −λ2t
2

−1

.
15. Following Example 8.4.4 with f(t, x) = exp(λ2t/2) sin(λx), λ > 0, show that
Z t
0
exp
λ2η
2

cos[λB(η)] dB(η) = 1
λ exp
λ2t
2

sin[λB(t)].
16. Following Example 8.4.4 with f(t, x) = −exp(λ2t/2) cos(λx), λ > 0, show that
Z t
0
exp
λ2η
2

sin[λB(η)] dB(η) = 1
λ

1 −exp
λ2t
2

cos[λB(t)]

.
17. Following Example 8.4.4 with f(t, x) = t−1/2 exp[x2/(2t)], show that
Z b
a
B(t) exp
B2(t)
2t
 dB(t)
t3/2
= b−1/2 exp
B2(b)
2b

−a−1/2 exp
B2(a)
2a

.
18. The average of geometric Brownian motion on [0, t] is deﬁned by
G(t) = 1
t
Z t
0
eB(η) dη.
Use the product rule to ﬁnd dG(t). Hint: Take the time derivative of tG(t) =
R t
0 eB(η) dη.

Itˆo’s Stochastic Calculus
419
8.5 STOCHASTIC DIFFERENTIAL EQUATIONS
We have reached the point where we can examine stochastic diﬀerential equations. Of
all the possible stochastic diﬀerential equations, we will focus on Langevin’s equation11—a
model of the velocity of Brownian particles. We will employ this model in a manner similar
to that played by simple harmonic motion in the study of ordinary diﬀerential equations.
It illustrates many of the aspects of stochastic diﬀerential equations without being overly
complicated.
• Example 8.5.1
Before we consider the general stochastic diﬀerential equation, consider the following
cases where we can make clever use of the product rule. For example, let us solve
dX(t) = [t + B2(t)] dt + 2tB(t) dB(t),
X(0) = X0.
(8.5.1)
In the present case, we can ﬁnd the solution by noting that
dX(t) = B2(t) dt + t[2B(t) dB(t) + dt] = B2(t) dt + t d[B2(t)] = d[tB2(t)].
(8.5.2)
Integrating both sides of Equation 8.5.2, we ﬁnd that the solution to Equation 8.5.1 is
X(t) = tB2(t) + X0.
(8.5.3)
Similarly, let us solve the stochastic diﬀerential equation
dX(t) = b −X(t)
1 −t
dt + dB(t),
0 ≤t < 1,
(8.5.4)
with X(0) = X0.
We begin by writing Equation 8.5.4 as
d[b −X(t)]
1 −t
+ b −X(t)
(1 −t)2 dt = −dB(t)
1 −t .
(8.5.5)
Running the product rule backwards,
d
b −X(t)
1 −t

= −dB(t)
1 −t .
(8.5.6)
Integrating both sides of Equation 8.5.6 from 0 to t, we ﬁnd that
b −X(t)
1 −t
= b −X(0) −
Z t
0
dB(η)
1 −η .
(8.5.7)
Solving for X(t), we obtain the ﬁnal result that
X(t) = b −[b −X(0)](1 −t) + (1 −t)
Z t
0
dB(η)
1 −η .
(8.5.8)
11 Langevin, P., 1908: Sur la th´eorie du mouvement brownien. C. R. Acad. Sci. Paris, 146, 530–530.
English translation: Langevin, P., 1997: On the theory of Brownian motion. Am. J. Phys., 65, 1079–1081.

420
Advanced Engineering Mathematics: A Second Course
In the present case we cannot simplify the integral in Equation 8.5.8 and must apply nu-
merical quadrature if we wish to have numerical values.
⊓⊔
In the introduction we showed that the solution to Langevin’s equation:
dX(t) = cX(t) dt + σ dB(t),
X(0) = X0,
(8.5.9)
is
X(t) = X0 + c
Z t
0
X(η) dη + σ
Z t
0
dB(η).
(8.5.10)
An obvious diﬃculty in understanding this solution is the presence of X(s) in the ﬁrst
integral on the right side of Equation 8.5.21.
Let us approach its solution by considering the function f(t, x) = e−ctx. Then, by Itˆo’s
lemma, Equation 8.4.16,
f[t, X(t)] −X(0) =
Z t
0

ft[η, X(η)] + cX(η)fx[η, X(η)] + 1
2σ2fxx[η, X(η)]
	
dη
+
Z t
0
σfx[η, X(η)] dB(η),
(8.5.11)
because f[0, X(0)] = X(0). Direct substitution of f(t, x) into Equation 8.5.11 yields
e−ctX(t) −X0 = σ
Z t
0
e−cη dB(η).
(8.5.12)
Finally, solving for X(t), we obtain
X(t) = X(0)ect + σect
Z t
0
e−cη dB(η),
(8.5.13)
an explicit expression for X(t). For the special case when X0 is constant, X(t) is known as
an Ornstein-Uhlenbeck process.12
An alternative derivation begins by multiplying Equation 8.5.9 by the integrating factor
e−ct so that the equation now reads
e−ct dX(t) −ce−ctX(t) dt = σe−ct dB(t).
(8.5.14)
Running the product rule, Equation 8.4.23, backwards, we have that
d

e−ctX(t)

= σe−ct dB(t).
(8.5.15)
Integrating both sides of Equation 8.5.15, we obtain Equation 8.5.12.
• Example 8.5.2: Exact stochastic diﬀerential equation
Consider the stochastic diﬀerential equation
X(t) = X(0) + c
Z t
0
X(s) ds + σ
Z t
0
X(s) dB(s),
(8.5.16)
12 Uhlenbeck and Ornstein, op. cit.

Itˆo’s Stochastic Calculus
421
time
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
X(t)
0.5
1
1.5
2
2.5
3
3.5
Figure 8.5.1: Ten realizations (sample paths) of geometric Brownian motion when c = 0.1, σ = 0.5, and
X(0) = 1. The heavy line is the mean of X(t).
with c, σ > 0.
If X(t) = f[t, B(t)], then by Itˆo’s lemma, Equation 8.4.9,
X(t) = X(0) +
Z t
0

ft[s, B(s)] + 1
2fxx[s, B(s)]
	
ds +
Z t
0
fx[s, B(s)] dB(s).
(8.5.17)
Comparing Equation 8.5.16 and Equation 8.5.17, we ﬁnd that
cf(t, x) = ft(t, x) + 1
2fxx(t, x),
(8.5.18)
and
σf(t, x) = fx(t, x).
(8.5.19)
From Equation 8.5.19,
fxx(t, x) = σfx(t, x) = σ2f(t, x).
(8.5.20)
Therefore, Equation 8.5.18 can be replaced by
 c −1
2σ2
f(t, x) = ft(t, x).
(8.5.21)
Equation 8.5.19 and Equation 8.5.21 can be solved using separation of variables, which
yields
f(t, x) = f(0, 0) exp
 c −1
2σ2
t + σx

,
(8.5.22)
or
X(t) = f[t, B(t)] = X(0) exp
 c −1
2σ2
t + σB(t)

.
(8.5.23)
Thus, a stochastic diﬀerential equation can sometimes be solved as the solution of a deter-
ministic partial diﬀerential equation. In the present case, this solution is called geometric
Brownian motion. For its solution numerically, see Example 8.6.1. See Figure 8.5.1.
⊓⊔

422
Advanced Engineering Mathematics: A Second Course
• Example 8.5.3: Homogeneous linear equation
Consider the homogeneous linear stochastic diﬀerential equation
dX(t) = c1(t)X(t) dt + σ1(t)X(t) dB(t).
(8.5.24)
Let us introduce f(t, x) = ln(x). Then by Itˆo’s lemma, Equation 8.4.21,
df = d[ln(X)] =

c1(t) −1
2σ2
1(t)

dt + σ1(t) dB(t),
(8.5.25)
because ft = 0, fx = 1/x and fxx = −1/x2. Integrating both sides of Equation 8.5.25 and
exponentiating the resulting expression, we obtain
X(t) = X(0) exp
Z t
0

c1(η) −1
2σ2
1(η)

dη +
Z t
0
σ1(η) dB(η)

.
(8.5.26)
⊓⊔
• Example 8.5.4: General case
Consider the homogeneous linear stochastic diﬀerential equation
dX(t) = [c1(t)X(t) + c2(t)] dt + [σ1(t)X(t) + σ2(t)] dB(t).
(8.5.27)
Our analysis begins by considering the homogeneous linear stochastic diﬀerential equa-
tion
dY (t) = c1(t)Y (t) dt + σ1(t)Y (t) dB(t),
Y (0) = 1.
(8.5.28)
From the previous example,
Y (t) = exp
Z t
0

c1(η) −1
2σ2
1(η)

dη +
Z t
0
σ1(η) dB(η)

.
(8.5.29)
Next, let us introduce two random variables, X1 = 1/Y and X2 = X. Using Itˆo lemma
f(t, x) = 1/x, then
dX1 = df(t, Y ) = d
 1
Y

=

σ2
1(t) −c1(t)
 dt
Y −σ1(t)dB(t)
Y
(8.5.30)
=

σ2
1(t) −c1(t)

X1(t) dt −σ1(t)X1(t) dB(t),
(8.5.31)
since ft = 0, fx = −1/x2 and fxx = 2/x3.
Using Equation 8.4.30, where X1 is governed by Equation 8.5.31 and X2 is governed
by 8.5.27 because X2 = X,
d(X1X2) = [c2(t) −σ1(t)σ2(t)] X1(t) dt + σ2(t)X1(t) dB(t).
(8.5.32)
Upon integrating both sides of Equation 8.5.32, we have
X1X2 −X1(0) =
Z t
0
[c2(η) −σ1(η)σ2(η)]
dη
Y (η) +
Z t
0
σ2(η) dB(η)
Y (η) .
(8.5.33)

Itˆo’s Stochastic Calculus
423
Consequently, our ﬁnal result is
X(t) = Y (t)

X(0) +
Z t
0
[c2(η) −σ1(η)σ2(η)]
dη
Y (η) +
Z t
0
σ2(η) dB(η)
Y (η)

,
(8.5.34)
where Y (t) is given by Equation 8.5.29.
⊓⊔
• Example 8.5.5: Stochastic Verhulst equation
The stochastic Verhulst equation is
dX(t) = aX(t)[M −X(t)] dt + bX(t) dB(t),
X(0) = X0.
(8.5.35)
We begin its solution by introducing Φ(t) = 1/X(t). Then by Itˆo’s lemma, Equation
8.4.21 with f(x) = 1/x,
dΦ(t) = −Φ(t)[(aM −b2) dt + b dB(t)] + a dt,
Φ(0) = 1/X0.
(8.5.36)
To solve Equation 8.5.36, we use the results from Example 8.5.4 with c1(t) = b2 −aM,
c2(t) = a, σ1(t) = −b, and σ2(t) = 0. Denoting ǫ(t) = (aM −b2/2)t + bB(t), we can write
Equation 8.5.34 as
Φ(t)eξ(t) −Φ(0) = a
Z t
0
eξ(η) dη,
(8.5.37)
or
eξ(t)
X(t) −1
X0
= a
Z t
0
eξ(η) dη.
(8.5.38)
Solving for X(t), we obtain the ﬁnal result that
X(t) =
X0 exp[ξ(t)]
1 + aX0
R t
0 exp[ξ(η)] dη
.
(8.5.39)
Problems
1. Solve the stochastic diﬀerential equation
dX(t) = 1
2et/2B(t) dt + et/2 dB(t),
X(0) = X0,
by running the product rule backwards.
2. Solve the stochastic diﬀerential equation
dX(t) = e2t[1 + 2B2(t)] dt + 2e2tB(t) dB(t),
X(0) = X0,
by running the product rule backwards. Hint: Rewrite the diﬀerential equation dX(t) =
e2t[2B(t) dB(t) + dt] + (2e2t dt)B2(t).
3. Solve the stochastic diﬀerential equation
dX(t) = [1 + B(t)] dt + [t + 2B(t)] dB(t),
X(0) = X0,

424
Advanced Engineering Mathematics: A Second Course
by running the product rule backwards. Hint: Rewrite the diﬀerential equation dX(t) =
2B(t) dB(t) + dt + B(t) dt + t dB(t).
4. Solve the stochastic diﬀerential equation
dX(t) = [3t2 + B(t)] dt + t dB(t),
X(0) = X0,
by running the product rule backwards. Hint: Rewrite the diﬀerential equation dX(t) =
3t2 dt + [B(t) dt + t dB(t)].
5. Solve the stochastic diﬀerential equation
dX(t) = B2(t) dt + 2tB(t) dB(t),
X(0) = X0,
by running the product rule backwards. Hint: Rewrite the diﬀerential equation dX(t) =
t[2B(t) dB(t) + dt] + B2(t) dt −t dt.
6. Find the integrating factor and solution to the stochastic diﬀerential equation
dX(t) = [β −αX(t)] dt + σ dB(t),
X(0) = X0,
where B(t) is Brownian motion and α, β and σ are constants.
7. Find the integrating factor and solution to the stochastic diﬀerential equation
dX(t) = [1 + 2X(t)] dt + e2t dB(t),
X(0) = X0,
where B(t) is Brownian motion.
8. Find the integrating factor and solution to the stochastic diﬀerential equation
dQ(t) + Q(t)
RC dt = V (t)
R
dt + α(t)
R
dB(t),
Q(0) = Q0,
where R and C are real, positive constants, and B(t) is Brownian motion.
9. Find the integrating factor and solution to the stochastic diﬀerential equation13
dX(t) = 2tX(t) dt + e−t dt + dB(t),
t ∈[0, 1],
with X(0) = X0, and B(t) is Brownian motion.
10. Find the integration factor and solution to the stochastic diﬀerential equation
dX(t) = [4X(t) −1] dt + 2 dB(t),
X(0) = X0,
where B(t) is Brownian motion.
13 Khodabin, M., and M. Rostami, 2015: Mean square numerical solution of stochastic diﬀerential equa-
tions by fourth order Runge-Kutta method and its applications in the electric circuits with noise. Adv.
Diﬀ. Eq., 2015, 62.

Itˆo’s Stochastic Calculus
425
11. Find the integration factor and solution to the stochastic diﬀerential equation
dX(t) = [2 −X(t)] dt + e−tB(t) dB(t),
X(0) = X0,
where B(t) is Brownian motion.
12. Find the integration factor and solution to the stochastic diﬀerential equation
dX(t) = [1 + X(t)] dt + etB(t) dB(t),
X(0) = X0,
where B(t) is Brownian motion.
13. Find the integration factor and solution to the stochastic diﬀerential equation
dX(t) =
 1
2X(t) + 1

dt + et cos[B(t)] dB(t),
X(0) = X0,
where B(t) is Brownian motion.
14. Find the integration factor and solution to the stochastic diﬀerential equation
dX(t) =

t + 1
2X(t)

dt + et sin[B(t)] dB(t),
X(0) = X0,
where B(t) is Brownian motion.
15. Following Example 8.5.2, solve the exact stochastic diﬀerential equation:
dX(t) = et 
1 + B2(t)

dt +

1 + 2etB(t)

dB(t),
X(0) = X0.
Step 1: Show that ft + 1
2fxx = et(1 + x2), and fx = 1 + 2etx.
Step 2: Show that f(t, x) = x + etx2 + g(t).
Step 3: Show that g(t) = X0 and X(t) = B(t) + etB2(t) + X0.
16. Following Example 8.5.2, solve the exact stochastic diﬀerential equation:
dX(t) =

2tB2(t) + 3t2 [1 + B(t)]
	
dt +

1 + 3t2B2(t)

dB,
X(0) = X0.
Step 1: Show that ft + 1
2fxx = 2tx3 + 3t2(1 + x), and fx = 3t2x2 + 1.
Step 2: Show that f(t, x) = t2x3 + x + g(t).
Step 3: Show that g′(t) = 3t2.
Step 4: Show that X(t) = t2[B3(t) + t] + B(t) + X0.
Using Equation 8.5.26, solve the following stochastic diﬀerential equations:
17. dX(t) = t2X(t) dt + tX(t) dB(t),
X(0) = X0
18. dX(t) = cos(t)X(t) dt + sin(t)X(t) dB(t),
X(0) = X0
19. dX(t) = ln(t + 1)X(t) dt +
p
ln(t + 1) X(t) dB(t),
X(0) = X0

426
Advanced Engineering Mathematics: A Second Course
20. dX(t) = ln(t + 1)X(t) dt + tX(t) dB(t),
X(0) = X0
21. Following Example 8.5.5, solve the stochastic diﬀerential equation
dX(t) = [aXn(t) + bX(t)] dt + cX(t) dB(t),
X(0) = X0,
where n > 1.
Step 1: Setting Φ(t) = X1−n(t), use Itˆo’s lemma Equation 8.4.21 with f(x) = 1/xn−1 to
show that
dΦ(t) = (1 −n)Φ(t)
 b −1
2nc2
dt + c dB(t)

+ (1 −n)a dt.
Step 2: Setting c1(t) = (1 −n)b −n(1 −n)c2/2, c2(t) = (1 −n)a, σ1(t) = (1 −n)c, and
σ2(t) = 0, show that
exp[(n −1)ξ(t)]
Xn−1(t)
−
1
Xn−1
0
= (1 −n)a
Z t
0
exp[(n −1)ξ(η)] dη,
or
exp[(n −1)ξ(t)]
Xn−1(t)
=
1
Xn−1
0
+ (1 −n)a
Z t
0
exp[(n −1)ξ(η)] dη,
where ξ(t) = (b −c2/2)t + cB(t).
22. Following Example 8.5.5, solve the stochastic Ginzburg-Landau equation:
dX(t) =
h
aecX(t) + b
i
dt + σ dB(t),
X(0) = X0.
Step 1: Setting Φ(t) = exp[−cX(t)], use Itˆo’s lemma Equation 8.4.21 with f(x) = e−cx to
show that
dΦ(t) = −
 bc −1
2σ2c2
Φ(t) dt −σcΦ(t) dB(t) −ac dt.
Step 2: Setting c1(t) = σ2c2/2 −bc, c2(t) = −ac, σ1(t) = −σc, and σ2(t) = 0, show that
X(t) = X0 + bt + σB(t) −1
c ln

1 −ac
Z t
0
exp[cX0 + bcξ + σcB(ξ)] dξ

.
23. Following Example 8.5.5, solve the stochastic diﬀerential equation:
dX(t) =

[1 + X(t)][1 + X2(t)]

dt + [1 + X2(t)] dB(t),
X(0) = X0.
Step 1: Setting Φ(t) = tan−1[X(t)], use Itˆo’s lemma Equation 8.4.21 with f(x) = tan−1(x)
to show that dΦ(t) = dt + dB(t).
Step 2: Solving the stochastic diﬀerential equation in Step 1, show that
X(t) = tan[tan−1(X0) + t + B(t)].

Itˆo’s Stochastic Calculus
427
8.6 NUMERICAL SOLUTION OF STOCHASTIC DIFFERENTIAL EQUATIONS
In this section we construct numerical schemes for integrating the stochastic diﬀerential
equation
dX(t) = a[X(t), t] dt + b[X(t), t] dB(t)
(8.6.1)
on t0 ≤t ≤T with the initial-value X(t0) = X0.
Our derivation begins by introducing the grid t0 < t1 < t2 < · · · < tn < · · · < tN = T.
For simplicity we assume that all of the time increments are the same and equal to 0 <
∆t < 1 although our results can be easily generalized when this is not true. Now
Xn+1 = Xn +
Z tn+1
tn
a[X(η), η] dη +
Z tn+1
tn
b[X(η), η] dB(η).
(8.6.2)
The crudest approximation to the integrals in Equation 8.6.2 is
Z tn+1
tn
a[X(η), η] dη ≈a[X(tn), tn]∆tn,
(8.6.3)
and
Z tn+1
tn
b[X(η), η] dB(η) ≈b[X(tn), tn]∆Bn.
(8.6.4)
Substituting these approximations into Equation 8.6.2 yields the Euler-Marugama approx-
imation.14 For the Itˆo process X(t) = {X(t), t0 ≤t ≤T}:
Xn+1 = Xn + a(tn, Xn) (tn+1 −tn) + b(tn, Xn)
 Btn+1 −Btn

(8.6.5)
for n = 0, 1, 2, . . . , N −1 with the initial value X0.
When b = 0, the stochastic iterative scheme reduces to the conventional Euler scheme
for ordinary diﬀerential equations. When b ̸= 0, we have an extra term generated by the
random increment ∆Bn = B(tn+1) −B(tn) where n = 0, 1, 2, . . . , N −1 for Brownian
motion (the Wiener process) B(t) = B(t), t ≥0. Because these increments are independent
Gaussian random variables, the mean equals E(∆Bn) = 0 while the variance is E[(∆Bn)2] =
∆t. We can generate ∆Bn using the MATLAB function randn.
An important consideration in the use of any numerical scheme is the rate of conver-
gence. During the numerical simulation of a realization, at time t there will be a diﬀerence
between the exact solution X(t) and the numerical approximation Y (t). This diﬀerence
e(t) = X(t)−Y (t) will also be a random variable. A stochastic diﬀerential equation scheme
converges strongly with order m, if for any time t, E(|e(t)|) = O[(∆t)m] for suﬃciently small
time step ∆t. The strong order for the Euler-Marugama method can be proven to be 1
2.
To construct a strong order 1 approximation to Equation 8.6.1, we return to Equation
8.6.2. Using Equation 8.4.12, we have
Xn+1 −Xn =
Z tn+1
tn

a[Xn(η), η] +
Z η
tn
 aax + 1
2b2axx

dξ +
Z η
tn
bax dB(ξ)

dη
+
Z tn+1
tn

b[Xn(η), η] +
Z η
tn
 abx + 1
2b2bxx

dξ +
Z η
tn
bbx dB(ξ)

dη
(8.6.6)
= a[X(tn), tn]∆t + b[X(tn), tn)∆Bn + Rn,
(8.6.7)
14 Maruyama, G., 1955: Continuous Markov processes and stochastic equations. Rend. Circ. Math.
Palermo, Ser. 2,, 4, 48–90.

428
Advanced Engineering Mathematics: A Second Course
where
Rn =
Z tn+1
tn
Z η
tn
bbx dB(ξ)

dB(η) + higher-order terms.
(8.6.8)
Dropping the higher-order terms,
Rn ≈b[X(tn), tn]bx[X(tn), tn]
Z tn+1
tn
Z η
tn
dB(ξ)

dB(η).
(8.6.9)
Consider now the double integrals
(∆Bn)2 =
Z tn+1
tn
dB(η)
 Z tn+1
tn
dB(η)

=
Z tn+1
tn
Z tn+1
tn
dB(ξ)

dB(η).
(8.6.10)
Now,
Z tn+1
tn
Z tn+1
tn
dB(ξ)

dB(η) =
Z tn+1
tn
Z η
tn
dB(ξ)

dB(η) +
Z tn+1
tn
Z tn+1
η
dB(ξ)

dB(η)
+
Z tn+1
tn
[dB(η)]2
(8.6.11)
= 2
Z tn+1
tn
Z η
tn
dB(ξ)

dB(η) +
Z tn+1
tn
[dB(η)]2
(8.6.12)
= 2
Z tn+1
tn
Z η
tn
dB(ξ)

dB(η) + ∆t,
(8.6.13)
because
Z tn+1
tn
[dB(η)]2 =
Z tn+1
tn
dη = ∆t.
(8.6.14)
Combining Equation 8.6.9, Equation 8.6.10, and Equation 8.6.13 yields
Rn ≈b[X(tn), tn]bx[X(tn), tn]

(∆Bn)2 −∆t

.
(8.6.15)
Finally, substituting Equation 8.6.15 into Equation 8.6.7 gives the ﬁnal result, the Milstein
method:15
Xn+1 = Xn + a(Xn, tn) ∆tn + b(Xn, tn) ∆Bn + 1
2b(Xn, tn)∂b(Xn, tn)
∂x

(∆Bn)2 −∆t

.
(8.6.16)
• Example 8.6.1
Consider the Itˆo process X(t) deﬁned by the linear stochastic diﬀerential equation
dX(t) = aX(t) dt + bX(t) dB(t),
(8.6.17)
15 Milstein, G., 1974: Approximate integration of stochastic diﬀerential equations. Theory Prob. Applic.,
19, 557–562.

Itˆo’s Stochastic Calculus
429
0
0.25
0.5
0.75
1
X(t n),Yn
0
1
2
3
4
5
6
h = 0.2
0
0.25
0.5
0.75
1
0
5
10
15
20
25
30
h = 0.1
t
0
0.25
0.5
0.75
1
X(t n),Yn
0
2.5
5
7.5
10
12.5
15
h = 0.05
t
0
0.25
0.5
0.75
1
0
1
2
3
4
5
6
h = 0.02
Figure 8.6.1: The numerical solution of the stochastic diﬀerential equation, Equation 8.6.17, using the
Euler-Marugama (crosses) and the Milstein (circles) methods for various time steps h. The dashed line
gives the exact solution.
for t ∈[0, T]. If this Itˆo process has the drift a(x, t) = ax and the diﬀusion coeﬃcient
b(x, t) = bx, the exact solution (see Equation 8.5.16) is
X(t) = X0 exp

a −b2
2

t + bB(t)

(8.6.18)
for t ∈[0, T]. Figure 8.6.1 compares the numerical solution of this stochastic diﬀerential
equation using the Euler-Marugama and Milstein method against the exact solution. Note
that each frame has a diﬀerent solution because the Brownian forcing changes with each
realization.
⊓⊔
Although a plot of various realizations can give an idea of how the stochastic processes
aﬀect the solution, two more useful parameters are the sample mean and standard deviation
at time tn:
X(tn) = 1
J
J
X
j=1
Xj(tn),
(8.6.19)
and
σ2(tn) =
1
J −1
J
X
j=1

Xj(tn) −X(tn)
2 ,
(8.6.20)
where J are the number of realizations and Xj(tn) is the value of the random variable at
time tn of the jth realization.
In many physical problems, “noise” is the origin of the stochastic process and we suspect
that we have a normal distribution N(µ, σ2) where µ and σ are the population mean and
standard deviation, respectively. Then, using the sample statistics, Equations 8.6.20 and

430
Advanced Engineering Mathematics: A Second Course
Rt/L
0
0.5
1
1.5
2
2.5
3
3.5
4
I(t)
-2
-1
0
1
2
3
4
5
Figure 8.6.2: Eleven realizations as a function of the nondimensional time Rt/L of the numerical solution
of Equation 8.6.24 using the Euler-Marugama method when h = 0.02, α/L = 1, β/L = 0, I0 = 0, and
v(t) = R. The mean and 95% conﬁdence interval (here tstudent = 2.228) are given by the heavy solid and
dashed lines, respectively. Finally, the crosses (+) give the deterministic solution.
8.6.21, a two-sided conﬁdence interval can be determined as

X(tn) −τstudent
σ(tn)
√
J
, X(tn) + τstudent
σ(tn)
√
J

based on the student-τ distribution with J −1 degrees of freedom.
Project: RL Electrical Circuit with Noise
An important component of contemporary modeling is the mixture of deterministic
and stochastic aspects of a physical system. In this project you will see how this is done
using a simple electrical system.16
Consider a simple electrical circuit consisting of a resistor with resistance R and an
inductor with inductance L. If the circuit is driven by a voltage source v(t), the current
I(t) at a given time t is given by the ﬁrst-order ordinary diﬀerential equation
LdI
dt + RI = v(t),
I(0) = I0.
(8.6.21)
Step 1: Using classical methods, show that the deterministic solution to Equation 8.6.21 is
I(t) = I0e−Rt/L + 1
L
Z t
0
exp
R
L (τ −t)

v(τ) dτ.
(8.6.22)
16 See Kol´aˇrov´a, E., 2005: Modeling RL electrical circuits by stochastic diﬀerential equations. Proc. Int.
Conf. Computers as Tool, Belgrade (Serbia and Montenegro), IEEE R8, 1236–1238.

Itˆo’s Stochastic Calculus
431
time
0
100
200
300
400
500
600
700
800
900
1000
E[x(t)]
-1.5
-1
-0.5
0
0.5
1
σ = 0
σ = 0.01
σ = 0.1
Figure 8.6.3: Plot of E[x(t)] versus time for the FitzHugh-Nagamo model for three values of σ. The value
of the parameters are a = 0.8, m = 1.2, and τ = 100. The Euler method was used with a time step of 0.1.
There are two possible ways that randomness can enter this problem. First, the power
supply could introduce some randomness so that the right side of Equation 8.6.21 could
read v(t) + α dB2(t)/dt.
Second, some physical process within the resistor could cause
randomness so that the resistance would now equal R + β dB1(t)/dt. Here B1(t) and B2(t)
denote two independent white noise processes and α, β are nonnegative constants. In this
case the governing diﬀerential equation would now read
dI
dt + 1
L

R + αdB1
dt

= 1
L

v(t) + β dB2
dt

,
I(0) = I0.
(8.6.23)
Converting Equation 8.6.23 into the standard form of a stochastic ordinary diﬀerential
equation, we have that
dI = 1
L [v(t) −RI(t)] dt −α
LI(t) dB1(t) + β
L dB2(t),
I(0) = I0.
(8.6.24)
Step 2: Using MATLAB, create a script to numerically integrate Equation 8.6.24 for a given
set of α, β, I0 = 0, R, L, and v(t). Plot I(t) as a function of the nondimensional time Rt/L
for many realizations (say 20). See Figure 8.6.2.
Step 3: Although some idea of the eﬀect of randomness is achieved by plotting several
realizations, a better way would be to compute the mean and standard deviation at a given
time. On the plot from the previous step, plot the mean and standard deviation of your
solution as a function of nondimensional time. How does it compare to the deterministic
solution?
Project: Relaxation Oscillator with Brownian Motion Forcing
The FitzHugh-Nagamo17 model describes excitable systems such as a neuron. We will
modify it so that the forcing is due to Brown motion. The governing equations are
dx = −x(x2 −a2) dt −y dt + σ dB1(t),
17 FitzHugh, R., 1961:
Impulses and physiological states in theoretical models of nerve membrane.
Biophys. J., 1, 445-466; Nagumo, J., S. Arimoto, and S. Yoshizawa, 1962: An active pulse transmission
line simulating nerve axon. Proc. IRE, 50, 2061–2070.

432
Advanced Engineering Mathematics: A Second Course
time
0
5
10
15
20
25
30
E[x(t)]
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Euler method
Heun method
leapfrog method
Figure 8.6.4: Plot of E[x(t)] versus time for the damped harmonic oscillator forced by Brownian motion.
The value of the parameters are k = 1, γ = 0.25, and alpha = ∆t = 0.1. Five thousand realization were
performed.
and
dy = (x −my) dt/τ + σ dB2(t),
where a, m, σ, and τ are parameters.
Write a MATLAB script to numerically integrate this modiﬁed FitzHugh-Nagamo model
for various values of σ. Using many simulations, compute E[x(t)] as a function of time t.
See Figure 8.6.3. What is the eﬀect of the Brownian motion forcing?
Project: Stochastically Damped Harmonic Oscillator
The damped stochastic harmonic oscillator is governed by the stochastic diﬀerential
equations:
dv(t) = −γv(t) dt −k2x(t) dt −αx(t) dB(t),
and
dx(t) = v(t) dt,
where k, α and γ are real constants. This system of equations is of interest for two reasons:
(1) The system is forced by Brownian motion. (2) The noise is multiplicative rather than
additive because the forcing term is x(t) dB(t) rather than just dB(t).
We could solve both equations numerically using Euler’s method.18 The purpose of
this project is to introduce you to the Heun method. In the Heun method we ﬁrst compute
an estimate of the solution x∗and v∗by taking a Euler-like time step:
x∗= xi + vi∆t,
and
v∗= vi −γvi∆t −k2xi∆t −αxi∆Bi,
where xi and vi denote the displacement and velocity at time ti = i∆t, ∆t is the time step,
and i = 0, 1, 2, . . .. With these estimates we compute the value for xi+1 and vi+1 using
xi+1 = xi + 1
2(vi + v∗)∆t,
and
vi+1 = vi −1
2γ(vi + v∗)∆t −1
2k2(xi + x∗)∆t −αxi∆Bi.
18 For further details, see Greiner, A., W. Strittmatter, and J. Honerkamp, 1988: Numerical integration
of stochastic diﬀerential equations. J. Stat. Phys., 51, 95–108.

Itˆo’s Stochastic Calculus
433
h1/2
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
mean time
0.7
0.8
0.9
1
1.1
1.2
1.3
Figure 8.6.5: The mean time that it takes a particle to travel from X(0) = −1 to X = 0 in the double-well
potential stated in the text. Sixty thousand realizations were used with a time step h. Two diﬀerential
numerical schemes were used: the Euler-Marugama (crosses) and the Milstein (circles) methods. The curves
are linear least-squares ﬁts through the results.
Qiang and Habib19 developed a leapfrog algorithm to solve this problem. Because the
algorithm is rather complicated, the interested student is referred to their paper.
Write a MATLAB script to use the Euler and Huen methods to numerically integrate
the stochastic harmonic oscillator when 10α = 4γ = k = 1 and x(0) = v(0) = 0. Using
many simulations, compute E[x(t)] as a function of time t. See Figure 8.6.4. What happens
to the accuracy of the solution for larger values of ∆t?
Project: Mean First Passage Time
The stochastic diﬀerential equation
dX(t) = [X(t) −X3(t)] dt +
4
X2(t) + 1 dB(t)
describes the motion of a particle in a double-well potential V (x) = x4/4 −x2/2, subject
to a spatially dependent random forcing when the acceleration X′′(t) can be neglected.
An important question is what is the average (mean) time that it takes a particle initially
located at a minimum X(0) = −1 to reach the local maximum X(t) = 0.
Write MATLAB code that computes X(t) as a function of time t.
Using this code
and creating N realizations, compute the length of time that it takes the particle to reach
X(t) = 0 in each realization. Then compute the mean from those times and plot the results
as a function
√
h, the square root of the time step. See Figure 8.6.5. We used
√
h rather
than h following the suggestions of Seeßelberg and Petruccione.20
19 Qiang, J., and S. Habib, 2000: Second-order stochastic leapfrog algorithm for multiplicative noise
Brownian motion. Phys. Review, 62, 7430–7437.
20 Seeßelberg, M., and F. Petruccione, 1993: An improved algorithm for the estimation of the mean ﬁrst
passage of ordinary stochastic diﬀerential equations. Comput. Phys. Commun., 74, 247–255.

434
Advanced Engineering Mathematics: A Second Course
interest (%/yr)
0
1
2
3
4
5
6
7
8
9
10
probability of bankruptcy
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
σ = 1.5
σ = 2.0
σ = 2.5
Figure 8.6.6: The probability of bankruptcy over a three-year period as a function of interest rate of a
ﬁrm with initial wealth X0 = 500 and debt D = 100. Other parameters are h = 0.01 yr and µ = 1.001/year.
The units on σ is year−1/2. Five hundred thousand realizations were used to compute the probability.
Project: Bankruptcy of a Company
The stochastic diﬀerential equation21
dX(t) = [µX(t) −iD] dt + σX(t) dB(t),
0 < t < T,
with X(0) = X0, describes the evolution with time t of the wealth X(t) of a ﬁrm. Here µ
and σ denote the deterministic and stochastic evolution of the ﬁrm’s wealth, respectively,
X0 is the initial wealth of the ﬁrm, and iD gives the amount of payment to a ﬁnancier (bank)
who initially loaned the ﬁrm the amount D at the interested rate i. Write a MATLAB code
to simulate the wealth of a ﬁrm during its lifetime T given a known D, i and X0 with
µ = 1.001/year, and various values of σ.
During the simulation there is a chance that the ﬁrm goes bankrupt at time t = τ <
T. This occurs when the stochastic process hits the barrier X(τ) = 0. If n denotes the
number of times that bankruptcy occurs in N simulations, the probability of bankruptcy
is P[X(τ) = 0] = n/N.
Using your code for simulating a ﬁrm’s wealth, compute the
probability of bankruptcy as a function of interest rate for a small (D = 20, X0 = 100),
medium (D = 100, X0 = 500), and large (D = 200, X0 = 1000) ﬁrm. See Figure 8.6.6.
How does the average value of τ vary with interest rate?
Further Readings
Kloeden, P. E., and E. Platen, 1992: Numerical Solution of Stochastic Diﬀerential Equa-
tions. Springer-Verlag, 632 pp. A solid book covering numerical schemes for solving stochas-
tic diﬀerential equations.
Mikosch, T., 1998: Elementary Stochastic Calculus with Finance in View. World Scientiﬁc,
212 pp. Very well-crafted book on stochastic calculus.
21 See Cerqueti, R., and A. G. Quaranta, 2012:
The perspective of a bank in granting credit:
An
optimization model. Optim. Lett., 6, 867–882.

Answers
to the Odd-Numbered Problems
Section 1.1
1. 1 + 2i
3. −2/5
5. 2 + 2i
√
3
7. z = e3πi/2
9. z = 4eπi/3
11. z = 2
√
2e7πi/4
Section 1.2
1. ±
√
2,
±
√
2
 
1
2 +
√
3i
2
!
,
±
√
2
 
−1
2 +
√
3i
2
!
3. i,
±
√
3
2 −i
2
5. ± 1
√
2

−
qp
a2 + b2 + a + i
qp
a2 + b2 + a

7. ±(1 + i),
±2(1 −i)
Section 1.3
1. u = 2 −y, v = x
3. u = x3 −3xy2, v = 3x2y −y3
5. f ′(z) = 3z(1 + z2)1/2
7. f ′(z) = 2(1 + 4i)z −3
9. f ′(z) = −3i(iz −1)−4
11. −1/4
13. (−1)n/π
15. v(x, y) = 2xy + constant
17. v(x, y) = x sin(x)e−y + ye−y cos(x) + constant
Section 1.4
1. 0
3. 2i
5. 14/15 −i/3
435

436
Advanced Engineering Mathematics: A Second Course
Section 1.5
1. (e−2 −e−4)/2
3. π/2
5. 17/6 + 52i/3
7. −sinh(1)i/3
Section 1.6
1. πi/32
3. πi/2
5. −2πi
7. 2πi
9. −6π
11. 2πi/3
Section 1.7
1.
∞
X
n=0
(n + 1)zn
3. f(z) = z10 −z9 + z8
2 −z7
6 + · · · −
1
11!z + · · ·
We have an essential singularity and the residue equals −1/11!
5. f(z) = 1
2! + z2
4! + z4
6! + · · ·
We have a removable singularity where the value of the residue equals zero.
7. f(z) = −2
z −2 −7z
6 −z2
2 −· · ·
We have a simple pole and the residue equals −2.
9. f(z) = 1
2
1
z −2 −1
4 + z −2
8
−· · ·
We have a simple pole and the residue equals 1/2.
Section 1.8
1. −3πi/4
3. −2πi.
5. 2πi
7. 2πi
9. −2i
Section 1.11
3. z = C√τ −π + π i
5. z = τ 7/4 or τ = z4/7
7. z = a cosh−1(τ)/π,
0 ≤ℑ[cosh−1(τ)] ≤π
Section 2.1
1. f(t) = e−a|t|/(2a)
3. f(t) = ite−a|t|/(4a)
5. f(t) = −2e−3t/2 sin
 √
3 t/2

H(t)/
√
3
7.
f(t) =



e−a|t| cosh(
√
a2−1|t|)
4a
−
e−a|t| sinh(
√
a2−1|t|)
4
√
a2−1
,
a > 1,
e−a|t| cos(
√
1−a2|t|)
4a
−
e−a|t| sin(
√
1−a2|t|)
4
√
1−a2
,
0 < a < 1.
Section 2.2
1. f(t) = (2 −t)e−2t −2e−3t
3. f(t) =
 t2/4 −t/4 + 1/8

e2t −1/8
5. f(t) =

(t −1)/2 −1/4 + e−2(t−1)/4

H(t −1)

Answers to the Odd-Numbered Problems
437
Section 2.3
1. f(t) = 1 + 2t
3. f(t) = t + t2/2
5. f(t) = t3 + t5/20
7. f(t) = t2 −t4/3
9. f(t) = 5e2t −4et −2tet
11. f(t) = (1 −t)2e−t
13. f(t) = e2t −e−t 
cos
 √
3 t

) +
√
3 sin
 √
3 t

15. f(t) = 4 + 5t2/2 + t4/24
17. x(t) = 2A
√
t/(πC) −Bt/(2C)
Section 3.1
1. F(z) = 2z/(2z −1) if |z| > 1/2
3. F(z) = (z6 −1)/(z6 −z5) if |z| > 0
5. F(z) = (a2 + a −z)/[z(z −a)] if |z| > a.
Section 3.2
1. F(z) = zTeaT /(zeaT −1)2
3. F(z) = z(z + a)/(z −a)3
5. F(z) = [z −cos(1)]/{z[z2 −2z cos(1) + 1]}
7. F(z) = z[z sin(θ) + sin(ω0T −θ)]/[z2 −2z cos(ω0T) + 1]
9. F(z) = z/(z + 1)
11. fn ∗gn = n + 1
13. fn ∗gn = 2n/n!
Section 3.3
1. f0 = 0.007143, f1 = 0.08503, f2 = 0.1626, f3 = 0.2328
3. f0 = 0.09836, f1 = 0.3345, f2 = 0.6099, f3 = 0.7935
5. fn = 8 −8
  1
2
n −6n
  1
2
n
7. fn = (1 −αn+1)/(1 −α)
9. fn =
  1
2
n−10 Hn−10 +
  1
2
n−11 Hn−11
11. fn = 1
9(6n −4)(−1)n + 4
9
  1
2
n
13. fn = an/n!
Section 3.4
1. yn = 1 + 1
6n(n −1)(2n −1)
3. yn = 1
2n(n −1)
5. yn = 1
6 [5n −(−1)n]
7. yn = (2n −1)
  1
2
n +
 −1
2
n
9. yn = 2n −n −1
11. xn = 2 + (−1)n; yn = 1 + (−1)n
13. xn = 1 −2(−6)n; yn = −7(−6)n
Section 3.5
1. marginally stable
3. unstable
Section 4.1
7. bx(t) = 1
π ln

t + a
t −a


438
Advanced Engineering Mathematics: A Second Course
Section 4.2
5. w(t) = u(t) ∗v(t) = πe−1 sin(t)
Section 4.3
1. z(t) = eiωt
Section 4.4
3. x(t) =
1 −t2
(1 + t2)2 ;
bx(t) =
2t
(1 + t2)2
Section 5.2
1. G(s) = 1/(s + k)
g(t|0) = e−kt
g(t|τ) = e−k(t−τ)H(t −τ)
a(t) =
 1 −e−kt
/k
3. G(s) = 1/(s2 + 4s + 3)
g(t|0) = 1
2(e−t −e−3t)
g(t|τ) = 1
2

e−(t−τ) −e−3(t−τ)
H(t −τ)
a(t) = 1
6e−3t −1
2e−t + 1
3
5. G(s) = 1/[(s −2)(s −1)]
g(t|0) = e2t −et
g(t|τ) =

e2(t−τ) −et−τ
H(t −τ)
a(t) = 1
2 + 1
2e2t −et
7. G(s) = 1/(s −9)2
g(t|0) = 1
3 sinh(3t)
g(t|τ) = 1
3 sinh[3(t −τ)]H(t −τ)
a(t) = 1
9 [cosh(3t) −1]
9. G(s) = 1/[s(s −1)]
g(t|0) = et −1
g(t|τ) = [et−τ −1] H(t −τ)
a(t) = et −t −1
11.
g(x|ξ) = (1 + x<)(L −1 −x>)
L
,
and
g(x|ξ) = −2ex+ξ
e2L −1 + 2L3
π2
∞
X
n=1
ϕn(ξ)ϕn(x)
n2(n2π2 + L2),
where ϕn(x) = sin(nπx/L) + nπ cos(nπx/L)/L.
13.
g(x|ξ) = sinh(kx<) sinh[k(L −x>)]
k sinh(kL)
,
and
g(x|ξ) = 2L
∞
X
n=1
sin(nπξ/L) sin(nπx/L)
n2π2 + k2L2
.
15.
g(x|ξ) = sinh(kx<){k cosh[k(x> −L)] −sinh[k(x> −L)]}
k sinh(kL) + k2 cosh(kL)
,
and
g(x|ξ) = 2
∞
X
n=1
(1 + k2
n) sin(knξ) sin(knx)
[1 + (1 + k2n)L](k2n + k2) ,

Answers to the Odd-Numbered Problems
439
where kn is the nth root of tan(kL) = −k.
17.
g(x|ξ) = [a sinh(kx<) −k cosh(kx<)] cosh[k(L −x>)]
k[a cosh(kL) −k sinh(kL)]
,
and
g(x|ξ) = 2
∞
X
n=1
(a2 + k2
n) cos[kn(ξ −L)] cos[kn(x −L)]
[(a2 + k2n)L −a](k2n + k2)
,
where kn is the nth root of k tan(kL) = −a.
Section 5.4
3.
g(x, t|ξ, τ) = t −τ
L
H(t −τ) + 2
π H(t −τ)
∞
X
n=1
1
n cos
nπξ
L

cos
nπx
L

sin
nπ(t −τ)
L

5.
u(x, t) = 2
∞
X
n=1
sin
nπx
L
 
nπ
L2 + n2π2

e−t −cos
nπt
L

+
L
L2 + n2π2 sin
nπt
L

+ 2 sin
πx
L

cos
πt
L

+ 4L
π2
∞
X
m=1
1
(2m −1)2 sin
(2m −1)πx
L

sin
(2m −1)πt
L

7.
u(x, t) = 1 −t2
2L −2L
π2
∞
X
n=1
1
n2 cos
nπx
L
 
1 −cos
nπt
L

Section 5.5
3.
g(x, t|ξ, τ) = 2
L
 ∞
X
n=1
sin
(2n −1)πξ
2L

sin
(2n −1)πx
2L

exp

−(2n −1)2π2(t −τ)
4L2

× H(t −τ)
5.
u(x, t) = 2π
∞
X
n=1
n
n2π2 −L2 sin
nπx
L
 
e−t −exp

−n2π2t
L2

+ 4
π
∞
X
m=1
1
2m −1 sin
(2m −1)πx
L

exp

−(2m −1)2π2t
L2


440
Advanced Engineering Mathematics: A Second Course
7.
u(x, t) = 1 −t
L −2L
π2
∞
X
n=1
1
n2 cos
nπx
L
 
1 −exp

−n2π2t
L2

Section 5.6
1.
g(x, y|ξ, η) = 1
π
∞
X
n=1
1
n exp

−nπ
a |y −η|

sin
nπξ
a

sin
nπx
a

5.
g(r, θ|ρ, θ′) = 1
π
∞
X
n=1
1
nrnπ/β
<
r−nπ/β
>
sin
nπθ′
β

sin
nπθ
β

7.
g(r, z|ρ, ζ) =
2
πa2L
∞
X
n=1
∞
X
m=1
J0(kmρ/a)J0(kmr/a)
πa2LJ2
1(km)(k2m/a2 + n2π2/L2) sin
nπζ
L

sin
nπz
L

Section 6.2
1. (a) S = {HH, HT, TH, TT}
(b) S = {ab, ac, ba, bc, ca, cb}
(c) S = {aa, ab, ac, ba, bb, bc, ca, cb, cc}
(d) S = {bbb, bbg, bgb, bgg, ggb, ggg, gbb, gbg}
(e) S = {bbb, bbg, bgb, bgg, ggb, ggg, gbb, gbg}
3. 1/3
5. 1/3
7. 2/13
9. 1/720, 1/120
11. 1/2
13. 1/2
15. 9/16
Section 6.3
1.
FX(x) =
( 0,
x < 0,
1 −p,
0 ≤x < 1,
1,
1 ≤x.
3.
27
Section 6.4
1. FX(x) =

0,
x ≤0,
1 −e−λx,
0 < x.
3. FX(x) =





0,
x < −1,
(1 + x)2/2,
−1 ≤x < 0,
1 −(x −1)2/2,
0 ≤x < 1,
1,
1 ≤x.
Section 6.5
1. E(X) = 1
2, and Var(X) = 1
4
3. k = 3/4, E(X) = 1, and Var(X) = 1
5

Answers to the Odd-Numbered Problems
441
5. φX(ω) =
 peiω + q
n, µX = np, Var(X) = npq
7. φX(ω) = p/(1 −qeωi), µX = q/p, Var(X) = q/p2
Section 6.6
1. (a) 1/16, (b) 1/4, (c) 15/16, (d) 1/16
5. P(X > 0) = 0.01, and P(X > 1) = 9 × 10−5
7. P(T < 150) = 1
3, and P(X = 3) = 0.1646
Section 6.7
1.
pXY [xi, yj] =

7
xi
 
8
yj
 
5
5 −xi −yj


20
5

,
where xi = 0, 1, 2, 3, 4, 5, yj = 0, 1, 2, 3, 4, 5 and 0 ≤xi + yj ≤5.
Section 7.1
1. µX(t) = 0, and σ2
X(t) = cos(ωt)
3. For t1 = t2, RX(t1, t2) = p; for t1 ̸= t2, RX(t1, t2) = p2. For t1 = t2, CX(t1, t2) = p(1−p);
for t1 ̸= t2, CX(t1, t2) = 0.
Section 7.4
1.
P n =

2/3 + (1/3)(1/4)n
1/3 −(1/3)(1/4)n
2/3 −(2/3)(1/4)n
1/3 + (2/3)(1/4)n

.
P ∞=

2/3
1/3
2/3
1/3

.
Section 8.3
1. E(X) = 0,
Var(X) = E(X2) = (b3 −a3)/3
3. E(X) = 0,
Var(X) = E(X2) = (b2 −a2)/2
Section 8.5
1. X(t) = et/2B(t) + X0
3. X(t) = B2(t) + tB(t) + X0
5. X(t) = tB2(t) −t2/2 + X0
7. X(t) = X(0)e2t + 1
2
 e2t −1

+ e2tB(t)
9. X(t) = et2X(0) + et2 R t
0 e−η2−η dη + et2 R t
0 e−η2 dB(η)
11. X(t) = X0e−t + 2(1 −e−t) + 1
2e−t[B2(t) −t]
13. X(t) = X0et/2 + 2
 et/2 −1

+ et sin[B(t)]
17. X(t) = X0 exp
h
t3
6 +
R t
0 η dB(η)
i
19. X(t) = X0 exp
h
1
2t ln(t + 1) + 1
2 ln(t + 1) −1
2t +
R t
0
p
ln(η + 1) dB(η)
i


Index
absolute value of a complex number, 2
addition of a complex numbers, 1
amplitude of a complex number, 2
analytic complex function, 9
derivative of, 10
analytic signals, 211–212
argument of a complex number, 2
band-pass functions, 209
basis function, 285
Bayes’ rule, 302
branches
of a complex function, 9
principal, 3
Bromwich contour, 93
Bromwich integral, 93
Bromwich, Thomas John I’Anson, 93
Brownian motion, 396–403
Buﬀon’s needle problem, 336–337
Cauchy
integral formula, 24–26
principal value, 51–53
residue theorem, 34–36
Cauchy, Augustin-Louis, 11
Cauchy-Goursat theorem, 20
Cauchy-Riemann equations, 11
central limit theorem, 318
Chapman-Kolmogorov equation, 359
characteristic function, 320–321
chemical reaction, 373–375
closed contour integral, 19
combinations, 301
complex
-valued function, 8–10
conjugate, 1
number, 1
plane, 2
variable, 1
compound interest, 186
conformal mapping, 59–75
contour integrals, 16–20
convolution theorem
for Hilbert transforms, 207–209
for z-transforms, 170
curve, simply closed, 20
cutoﬀfrequency, 244
damped harmonic motion, 389
de Moivre’s theorem, 3
deformation principle, 22
diﬀerence equations, 159
diﬀerential equations, stochastic, 419–423
division of complex numbers, 1
Duhamel’s theorem
for ordinary diﬀerential equation, 233
entire complex function, 9
essential singularity, 30
443

444
Advanced Engineering Mathematics: A Second Course
Euler’s formula, 2
evaluation of partial sums
using z-transform, 180
ﬁnal-value theorem
for z-transforms, 168
ﬁnite element, 285
ﬁrst-passage problem, 391–393
Fourier transform, 78–91, 228–232
inverse of, 78–87
frequency response, 227
frequency spectrum,
for a damped harmonic
oscillator, 227–229
for low-frequency ﬁlter, 229–230
function
multiplied complex, 8
single-valued complex, 8
Galerkin method, 285–292
gambler’s ruin problem, 348, 364
Green’s function, 217–233
for a damped harmonic oscillator, 228
for heat equation, 256–266
for Helmholtz’s equation, 266–285
for low-frequency ﬁlter, 230
for ordinary diﬀerential eqn, 223–243
for wave equation, 247–256
harmonic functions, complex, 15
heat dissipation in disc brakes, 132–134
heat equation, 129–153
Hilbert pair, 196
Hilbert transform, 195–215
and convolution, 207–208
and derivatives, 206–207
and shifting, 206
and time scaling, 206
discrete, 203–204
linearity of, 205
product theorem, 208–209
Hilbert, David, 197
holomorphic complex function, 9
ideal Hilbert transformer, 195
ideal sampler, 160
imaginary part of a complex number, 1
importance sampling, 324
impulse function
see (Dirac) delta function
impulse response, 224
indicial admittance
for ordinary diﬀerential eqns, 224–225
initial-value theorem
for z-transforms, 168
integral equation, 222–223
of convolution type, 100–101
integrals
complex contour, 16–20
Fourier type, evaluation of, 81
real, evaluation of, 37–43
interest rate, 186
inverse
Fourier transform, 78–87
Hilbert transform, 196
Laplace transform, 92–96
z-transform, 173–181
inversion formula
for the Hilbert transform, 196
for the Laplace transform, 92–96
for the z-transform, 173–181
inversion of Fourier transform
by contour integration, 77–87
inversion of Laplace transform
by contour integration, 93–96
inversion of z-transform
by contour integration, 177–181
by partial fractions, 175–177
by power series, 173–175
by recursion, 174–175
isolated singularities, 12
Itˆo process, 386
Itˆo’s integral, 406–411
Itˆo’s lemma, 410–418
Itˆo, Kiyhosi, 413
joint transform method, 243
Jordan curve, 20
Jordan’s lemma, 78
Kramers-Kronig relationship, 213–215
Lagrange’s trigonometric identities, 5
Laguerre polynomial, 105
Laplace transform, 92–101
in solving
heat equation, 129–136
integral equations, 100–101
Laplace equation, 154–156
wave equation, 105–115

Index
445
Laplace transform (contd.)
inverse of, 92–96
Schouten-van der Pol theorem for, 99
Laplace’s equation,
solution by Laplace transforms, 154–156
Laurent expansion, 29
law of large numbers, 322
line integral, 16–20
linearity
of Hilbert transform, 205
of z-transform, 166
low-frequency ﬁlter, 229–230
low-pass ﬁlter, 390–392
Markov chain
state, 358
state transition, 358
time homogeneous, 358
martingale, 349
mean, 318–230
meromorphic function, 12
method of partial fractions
for Fourier transform, 77
for z-transform, 175–177
modulus of a complex number, 2
Monte Carlo integration, 324
multiplication of complex numbers, 1
multivalued complex function, 8
not simply connected, 20
numerical solution
of stochastic diﬀerential eqn, 427–434
order of a pole, 30
Parseval’s identity
for z-transform, 180
partial fraction expansion
for z-transform, 175–177
path in complex integrals, 17
path independence in complex integrals, 22
permutation, 301
phase of the complex number, 2
phasor amplitude, 213
Poisson process, 377–382
arrival time, 380
polar form of a complex number, 2
pole of order n, 30
population growth and decay, 366–375
positively oriented curve, 23
power spectrum, 354–357
principal branch, 3
probability
Bernoulli distribution, 326
Bernoulli trials, 310
binomial distribution, 327
characteristic function, 320
combinations, 301
conditional, 302
continuous joint distribution, 335
correlation, 341
covariance, 339
cumulative distribution, 310
distribution function, 314
event, 296
elementary, 296
simple, 296
expectation, 319
experiment, 295
exponential distribution, 330
Gaussian distribution, 331
geometric distribution, 326
independent events, 303–304
joint probability mass function, 333
law of total probability, 303
marginal probability functions, 333
mean, 319
normal distribution, 331
permutation, 301
Poisson distribution, 328
probability integral, 332
probability mass function, 308
random variable, 308
sample point, 295
sample space, 295
standard normal distribution, 332
uniform distribution, 329
variance, 319
quadrature phase shifting, 195
radius of convergence, 27
random diﬀerential equation, 387–388
random process, 345–383
autocorrelation function, 351
Bernoulli process, 346
Brownian motion, 395–403
chemical kinetics, 370
counting process, 347
mean, 349

446
Advanced Engineering Mathematics: A Second Course
random process (contd.)
power spectrum, 354
realization, 345
sample function, 345
sample path, 345
state, 345
state space, 345
variance, 349
wide-sense stationary process, 352
Wiener process, 402–403
random variable, 308
discrete, 308
domain, 308
identically distributed, 308
independent, 308
independent identically distributed, 308
range, 308
real deﬁnite integrals
evaluation of, 37–43
real part of a complex number, 1
regular complex function, 9
removable singularity, 30
residue, 29
residue theorem, 33–36
Riemann, Georg Friedrich Bernhard, 12
root locus method, 229
roots of a complex number, 5–7
Schouten-Van der Pol theorem, 99
Schwarz-Christoﬀel transformation, 66–75
set, 294
complement, 294
disjoint, 294
element, 294
empty, 294
intersection, 294
null, 294
subset, 294
union, 294
universal, 294
simple pole, 30
simply closed curve, 20
single side-band signal, 213
single-valued complex function, 8
singularity
essential, 30
isolated, 30
pole of order n, 30
removable, 30
solution of ordinary diﬀerential equations
by Fourier transform, 227–232
steady-state transfer function, 227
step response, 224–225
stochastic calculus, 385–431
Brownian motion, 396–403
damped harmonic motion, 389
derivative, 385
diﬀerential equations, 419–423
ﬁrst-passage problem, 391–393
integrating factor, 420
Itˆo process, 385
Itˆo’s integral, 406–411
Itˆo’s lemma, 410–418
low-pass ﬁlter, 390–392
nonlinear oscillator, 431
numerical solution, 427–430
Euler-Marugama method, 427
Milstein method, 428
product rule, 416, 419
random diﬀerential equations, 387–388
RL electrical circuit with noise, 430
wave motion due to random
forcing, 394–396
Wiener process, 403
stochastic process, 345
subtraction of complex numbers, 1
superposition integral, 219
for ordinary diﬀerential equations, 233
Taylor expansion, 27
telegraph equation, 106–115
telegraph signal, 346, 353
transfer function, 223
transform
Fourier, 78–91, 228–232
Hilbert, 195–215
Laplace, 92–101
z-, 159–193
transmission line, 106–115
transmission probability matrix, 360
variance, 318–320
Venn diagram, 294
Volterra equation of the second kind, 100
wave equation, 100–127
wave motion due to random forcing, 394–396
Wiener process, 350, 403
Wiener, Norbert, 401

Index
447
z-transform, 159–193
basic properties of, 164–172
convolution for, 170
ﬁnal-value theorem for, 168–169
for solving diﬀerence equations, 183–188
initial-value theorem for, 168
inverse of, 173–181
linearity of, 166
multiplication by n, 169
of a sequence multiplied by an
exponential sequence, 166
of a shifted sequence, 166–168
of periodic sequences, 169–170
their use in determining
stability, 189–192

